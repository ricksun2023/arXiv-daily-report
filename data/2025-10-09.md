<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 53]
- [cs.CV](#cs.CV) [Total: 50]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 51]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [OpenStaxQA: A multilingual dataset based on open-source college textbooks](https://arxiv.org/abs/2510.06239)
*Pranav Gupta*

Main category: cs.CL

TL;DR: 本文介绍了一个名为 OpenStaxQA 的评估基准，该基准专门用于大学水平的教育应用程序。


<details>
  <summary>Details</summary>
Motivation: 创建该基准以评估大型语言模型在大学水平教育材料上的表现。

Method: 使用量化的低秩适配器 (QLoRa) 在此数据集上对具有大约 70 亿个参数的大型语言模型 (LLM) 进行微调和评估。此外，我们还在 AI2 推理挑战开发数据集上进行了零样本评估，以检查 OpenStaxQA 是否可以提高其他任务的性能。

Result: 对大型语言模型进行了微调和评估，并在 AI2 推理挑战开发数据集上进行了零样本评估。

Conclusion: 本文讨论了与 OpenStaxQA 等数据集相关的更广泛的影响。

Abstract: We present OpenStaxQA, an evaluation benchmark specific to college-level
educational applications based on 43 open-source college textbooks in English,
Spanish, and Polish, available under a permissive Creative Commons license. We
finetune and evaluate large language models (LLMs) with approximately 7 billion
parameters on this dataset using quantized low rank adapters (QLoRa).
Additionally we also perform a zero-shot evaluation on the AI2 reasoning
challenge dev dataset in order to check if OpenStaxQA can lead to an improved
performance on other tasks. We also discuss broader impacts relevant to
datasets such as OpenStaxQA.

</details>


### [2] [Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets](https://arxiv.org/abs/2510.06240)
*Jiqun Pan,Zhenke Duan,Jiani Tu,Anzhi Cheng,Yanqing Wang*

Main category: cs.CL

TL;DR: 提出了一种名为知识图引导的多智能体系统蒸馏 (KG-MASD) 的方法，用于提高工业问答系统的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 工业问答系统需要在高风险场景中更高的安全性和可靠性，而现有的多智能体大型语言模型存在迭代不可控和输出不可验证的问题，传统的蒸馏方法难以将协作推理能力转移到轻量级模型。

Method: 将蒸馏过程建模为马尔可夫决策过程，并结合知识图谱作为可验证的结构化先验，以丰富状态表示并确保收敛。

Result: 在工业问答数据集上的实验表明，KG-MASD 的准确率比基线提高了 2.4% 到 20.1%，并显着提高了可靠性。

Conclusion: KG-MASD 能够将推理深度和可验证性共同提炼到适合边缘部署的紧凑型学生模型中，从而在安全关键的工业场景中实现可信赖的 AI 部署。

Abstract: Industrial question-answering (QA) systems require higher safety and
reliability than general-purpose dialogue models, as errors in high-risk
scenarios such as equipment fault diagnosis can have severe consequences.
Although multi-agent large language models enhance reasoning depth, they suffer
from uncontrolled iterations and unverifiable outputs, and conventional
distillation methods struggle to transfer collaborative reasoning capabilities
to lightweight, deployable student models. To address these challenges, we
propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our
approach formulates distillation as a Markov Decision Process and incorporates
a knowledge graph as a verifiable structured prior to enrich state
representation and ensure convergence. By integrating collaborative reasoning
with knowledge grounding, KG-MASD generates high-confidence instruction-tuning
data and jointly distills reasoning depth and verifiability into compact
student models suitable for edge deployment. Experiments on an industrial QA
dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent
over baselines and significantly enhances reliability, enabling trustworthy AI
deployment in safety-critical industrial scenarios. Code and data are available
at https://github.com/erwinmsmith/KG-MAD/.

</details>


### [3] [Transparent Reference-free Automated Evaluation of Open-Ended User Survey Responses](https://arxiv.org/abs/2510.06242)
*Subin An,Yugyeong Ji,Junyoung Kim,Heejin Kook,Yang Lu,Josh Seltzer*

Main category: cs.CL

TL;DR: 这篇论文提出了一种专门为人工调查回复设计的两阶段评估框架，以解决现有方法不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的自动评估方法不适用于评估人工撰写的回复，因为它们有其独特的特征。低质量的回复会给研究人员带来手动过滤的负担，并且可能导致误导性的结论，因此需要有效的评估方法。

Method: 该框架包括两个阶段：首先，使用乱语过滤来移除无意义的回复；然后，基于对真实世界调查数据的实证分析，使用LLM能力评估三个维度：努力程度、相关性和完整性。

Result: 在英语和韩语数据集上的验证表明，该框架不仅优于现有的指标，而且在诸如回复质量预测和回复拒绝等实际应用中也显示出很高的实用性，与专家评估显示出很强的相关性。

Conclusion: 该研究提出了一个有效且实用的框架，用于评估人工调查回复的质量，可以帮助研究人员过滤低质量回复并提高研究结果的可靠性。

Abstract: Open-ended survey responses provide valuable insights in marketing research,
but low-quality responses not only burden researchers with manual filtering but
also risk leading to misleading conclusions, underscoring the need for
effective evaluation. Existing automatic evaluation methods target
LLM-generated text and inadequately assess human-written responses with their
distinct characteristics. To address such characteristics, we propose a
two-stage evaluation framework specifically designed for human survey
responses. First, gibberish filtering removes nonsensical responses. Then,
three dimensions-effort, relevance, and completeness-are evaluated using LLM
capabilities, grounded in empirical analysis of real-world survey data.
Validation on English and Korean datasets shows that our framework not only
outperforms existing metrics but also demonstrates high practical applicability
for real-world applications such as response quality prediction and response
rejection, showing strong correlations with expert assessment.

</details>


### [4] [Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization](https://arxiv.org/abs/2510.06732)
*Tiancheng Xing,Jerry Li,Yixuan Du,Xiyang Hu*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）在信息检索中越来越多地被用作重排序器，但它们排序行为容易受到提示词的影响。本文提出了一种名为 Rank Anything First (RAF) 的方法，通过优化文本扰动来提升目标项目在 LLM 生成的排名中的位置，并且难以被检测。


<details>
  <summary>Details</summary>
Motivation: 揭示大型语言模型（LLM）作为重排序器时，其排序行为容易受到对抗性提示词的影响这一安全漏洞。

Method: 提出 Rank Anything First (RAF) 方法，该方法通过双阶段令牌优化，生成既能有效提升目标项目排名，又能保持语言自然性的提示词。第一阶段使用贪婪坐标梯度来选择候选令牌，第二阶段使用基于熵的动态加权方案评估候选令牌。

Result: 实验表明，RAF 能够使用自然语言显著提升目标项目的排名，并且比现有方法更具鲁棒性。

Conclusion: 研究结果表明，基于 LLM 的重排序本质上容易受到对抗性操纵，这对现代检索系统的可信度和鲁棒性提出了新的挑战。

Abstract: Large language models (LLMs) are increasingly used as rerankers in
information retrieval, yet their ranking behavior can be steered by small,
natural-sounding prompts. To expose this vulnerability, we present Rank
Anything First (RAF), a two-stage token optimization method that crafts concise
textual perturbations to consistently promote a target item in LLM-generated
rankings while remaining hard to detect. Stage 1 uses Greedy Coordinate
Gradient to shortlist candidate tokens at the current position by combining the
gradient of the rank-target with a readability score; Stage 2 evaluates those
candidates under exact ranking and readability losses using an entropy-based
dynamic weighting scheme, and selects a token via temperature-controlled
sampling. RAF generates ranking-promoting prompts token-by-token, guided by
dual objectives: maximizing ranking effectiveness and preserving linguistic
naturalness. Experiments across multiple LLMs show that RAF significantly
boosts the rank of target items using naturalistic language, with greater
robustness than existing methods in both promoting target items and maintaining
naturalness. These findings underscore a critical security implication:
LLM-based reranking is inherently susceptible to adversarial manipulation,
raising new challenges for the trustworthiness and robustness of modern
retrieval systems. Our code is available at: https://github.com/glad-lab/RAF.

</details>


### [5] [CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning](https://arxiv.org/abs/2510.06243)
*Qihua Dong,Luis Figueroa,Handong Zhao,Kushal Kafle,Jason Kuen,Zhihong Ding,Scott Cohen,Yun Fu*

Main category: cs.CL

TL;DR: 本文提出了一种名为CoT Referring的新策略，旨在提高多模态大型语言模型在指代表达理解和分割任务中的性能。该策略通过结构化的思维链训练数据，增强模型在不同模态之间的推理能力。


<details>
  <summary>Details</summary>
Motivation: 评估多模态大型语言模型整合语言理解和图像理解的能力，特别是在指代表达理解和分割任务中。

Method: 系统地解析文本结构到连续的指代步骤，识别关系并确保一致的参考对齐。同时，重构训练数据，强制新的输出形式，并整合检测和分割能力到一个统一的MLLM框架中，使用自适应加权损失进行训练。

Result: 在自建的benchmark和RefCOCO/+/g数据集上，实验结果表明该方法的有效性，相比基线模型有2.5%+的显著提升。

Conclusion: CoT Referring策略能够有效提高模型在复杂指代情况下的准确性，并在指代表达理解和分割任务中取得了显著的性能提升。

Abstract: Referring Expression Comprehension and Segmentation are critical tasks for
assessing the integration of language understanding and image comprehension,
serving as benchmarks for Multimodal Large Language Models (MLLMs)
capabilities. To address these challenges, we propose a new strategy, CoT
Referring, which enhances model reasoning across modalities through a
structured, chain-of-thought training data structure. Our approach
systematically parses textual structures to a sequential referring step, where
in each step it identifies relationships and ensures consistent reference
alignment, thereby improving accuracy in complex query scenarios. We
restructure the training data to enforce a new output form, providing new
annotations for existing datasets and compiling an evaluation benchmark from
existing resources. This benchmark is designed explicitly for complex referring
cases. We also integrate detection and segmentation capabilities into a unified
MLLM framework, training it with a novel adaptive weighted loss to optimize
performance. Experimental results on our curated benchmark and RefCOCO/+/g
demonstrate the effectiveness of our approach, with a notable increase of 2.5%+
over baseline models.

</details>


### [6] [Overview of the Plagiarism Detection Task at PAN 2025](https://arxiv.org/abs/2510.06805)
*André Greiner-Petter,Maik Fröbe,Jan Philip Wahle,Terry Ruas,Bela Gipp,Akiko Aizawa,Martin Potthast*

Main category: cs.CL

TL;DR: PAN 2025 旨在检测科学文章中自动生成的抄袭文本，并将其与来源对齐。他们使用 Llama、DeepSeek-R1 和 Mistral 等大型语言模型创建了一个新的大规模自动生成抄袭数据集。


<details>
  <summary>Details</summary>
Motivation: 为了识别科学文章中自动生成的抄袭文本，并将其与各自的来源对齐。

Method: 使用 Llama、DeepSeek-R1 和 Mistral 等大型语言模型创建了一个新的大规模自动生成抄袭数据集。概述了数据集的创建，总结并比较了所有参与者和四个基线的结果，并在 PAN 2015 的最后一项抄袭检测任务中评估了结果，以解释所提出方法的稳健性。

Result: 基于嵌入向量的朴素语义相似度方法提供了高达 0.8 的召回率和 0.5 的精确率。大多数这些方法在 2015 年的数据集上的表现明显不佳，表明缺乏泛化性。

Conclusion: 当前的方法没有引入各种各样的方法，因为基于嵌入向量的朴素语义相似度方法提供了有希望的结果，但缺乏泛化性。

Abstract: The generative plagiarism detection task at PAN 2025 aims at identifying
automatically generated textual plagiarism in scientific articles and aligning
them with their respective sources. We created a novel large-scale dataset of
automatically generated plagiarism using three large language models: Llama,
DeepSeek-R1, and Mistral. In this task overview paper, we outline the creation
of this dataset, summarize and compare the results of all participants and four
baselines, and evaluate the results on the last plagiarism detection task from
PAN 2015 in order to interpret the robustness of the proposed approaches. We
found that the current iteration does not invite a large variety of approaches
as naive semantic similarity approaches based on embedding vectors provide
promising results of up to 0.8 recall and 0.5 precision. In contrast, most of
these approaches underperform significantly on the 2015 dataset, indicating a
lack in generalizability.

</details>


### [7] [Evaluating Embedding Frameworks for Scientific Domain](https://arxiv.org/abs/2510.06244)
*Nouman Ahmed,Ronin Wu,Victor Botev*

Main category: cs.CL

TL;DR: 这篇论文致力于为科学领域寻找最佳的词表示算法和分词方法。


<details>
  <summary>Details</summary>
Motivation: 在特定领域中，词语具有不同的含义和表示，因此找到适用于科学领域的词表示方法至关重要。虽然生成式AI和Transformer架构在生成上下文嵌入方面表现出色，但它们耗时且计算成本高昂，尤其是在从头开始预训练模型时。

Method: 构建一个综合评估套件，用于评估科学领域中各种词表示和分词算法。

Result: 使用构建的评估套件来测试各种词表示和分词算法。

Conclusion: 目标是找到最佳的词表示和分词方法，并构建一个全面的评估套件，以评估科学领域中的各种词表示和分词算法。

Abstract: Finding an optimal word representation algorithm is particularly important in
terms of domain specific data, as the same word can have different meanings and
hence, different representations depending on the domain and context. While
Generative AI and transformer architecture does a great job at generating
contextualized embeddings for any given work, they are quite time and compute
extensive, especially if we were to pre-train such a model from scratch. In
this work, we focus on the scientific domain and finding the optimal word
representation algorithm along with the tokenization method that could be used
to represent words in the scientific domain. The goal of this research is two
fold: 1) finding the optimal word representation and tokenization methods that
can be used in downstream scientific domain NLP tasks, and 2) building a
comprehensive evaluation suite that could be used to evaluate various word
representation and tokenization algorithms (even as new ones are introduced) in
the scientific domain. To this end, we build an evaluation suite consisting of
several downstream tasks and relevant datasets for each task. Furthermore, we
use the constructed evaluation suite to test various word representation and
tokenization algorithms.

</details>


### [8] [TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B](https://arxiv.org/abs/2510.06249)
*Toshiki Nakai,Ravi Kiran Chikkala,Lena Sophie Oberkircher,Nicholas Jennings,Natalia Skachkova,Tatiana Anikina,Jesujoba Oluwadara Alabi*

Main category: cs.CL

TL;DR: 本研究针对印度低资源语言的资源匮乏问题， 探讨了在decoder-only多语种大语言模型(LLM)的特定内部层中强制执行跨语言相似性是否可以提高从低资源语言到高资源语言的翻译质量。


<details>
  <summary>Details</summary>
Motivation: 印度缺乏针对其多样化的低资源语言(LRL)的资源，这是其面临的最紧迫的语言差距之一。

Method: 结合了中心核对齐(CKA) (一种鼓励不同语言的表示对齐的相似性度量) 和REPINA (一种约束参数更新以保持接近预训练模型的一种正则化方法)， 形成了一种名为TRepLiNa的联合方法。 使用Aya-23 8B with QLoRA在MMLoSo共享任务语言对(Mundari, Santali, Bhili)上，以印地语/英语为轴进行零样本、少样本和微调设置的实验。

Result: 结果表明，使用TRepLiNa (CKA+REPINA)对齐中间层是一种低成本、实用的改进LRL翻译的方法，尤其是在数据稀缺的情况下。

Conclusion: 在数据稀缺的情况下，使用TRepLiNa (CKA+REPINA)对齐中间层是一种低成本、实用的改进LRL翻译的方法

Abstract: The 2025 Multimodal Models for Low-Resource Contexts and Social Impact
(MMLoSo) Language Challenge addresses one of India's most pressing linguistic
gaps: the lack of resources for its diverse low-resource languages (LRLs). In
this study, we investigate whether enforcing cross-lingual similarity in
specific internal layers of a decoder-only multilingual large language model
(LLM) can improve translation quality from LRL to high-resource language (HRL).
Specifically, we combine Centered Kernel Alignment (CKA), a similarity metric
that encourages representations of different languages to align, with REPINA, a
regularization method that constrains parameter updates to remain close to the
pretrained model, into a joint method we call TRepLiNa. In this research
project, we experiment with zero-shot, few-shot, and fine-tuning settings using
Aya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari,
Santali, Bhili) with Hindi/English pivots. Our results show that aligning
mid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach
to improving LRL translation, especially in data-scarce settings.

</details>


### [9] [Towards Reliable Retrieval in RAG Systems for Large Legal Datasets](https://arxiv.org/abs/2510.06999)
*Markus Reuter,Tobias Lingenberg,Rūta Liepiņa,Francesca Lagioia,Marco Lippi,Giovanni Sartor,Andrea Passerini,Burcu Sayin*

Main category: cs.CL

TL;DR: 本文提出了一种名为 Summary-Augmented Chunking (SAC) 的方法，以解决法律领域检索增强生成 (RAG) 系统中由于文档级检索不匹配 (DRM) 导致的检索不准确问题。SAC 通过在文本块中加入文档级摘要来增强检索效果。


<details>
  <summary>Details</summary>
Motivation: 法律领域的大量结构相似文档导致检索系统失效，尤其是在检索步骤的准确性至关重要的情况下，RAG 系统容易出现幻觉。

Method: 本文首先识别并量化了一个关键的失败模式，即文档级检索不匹配 (DRM)。然后，提出了一种简单且计算高效的技术，即 Summary-Augmented Chunking (SAC)，通过文档级摘要增强文本块。

Result: 实验表明，SAC 显著降低了 DRM，并提高了文本级检索的准确率和召回率。通用摘要策略优于结合法律专家领域知识的方法。

Conclusion: 本文证明了 SAC 技术可以提高 RAG 系统在大型法律文档数据集上的可靠性，且该技术具有实用性、可扩展性和易集成性。

Abstract: Retrieval-Augmented Generation (RAG) is a promising approach to mitigate
hallucinations in Large Language Models (LLMs) for legal applications, but its
reliability is critically dependent on the accuracy of the retrieval step. This
is particularly challenging in the legal domain, where large databases of
structurally similar documents often cause retrieval systems to fail. In this
paper, we address this challenge by first identifying and quantifying a
critical failure mode we term Document-Level Retrieval Mismatch (DRM), where
the retriever selects information from entirely incorrect source documents. To
mitigate DRM, we investigate a simple and computationally efficient technique
which we refer to as Summary-Augmented Chunking (SAC). This method enhances
each text chunk with a document-level synthetic summary, thereby injecting
crucial global context that would otherwise be lost during a standard chunking
process. Our experiments on a diverse set of legal information retrieval tasks
show that SAC greatly reduces DRM and, consequently, also improves text-level
retrieval precision and recall. Interestingly, we find that a generic
summarization strategy outperforms an approach that incorporates legal expert
domain knowledge to target specific legal elements. Our work provides evidence
that this practical, scalable, and easily integrable technique enhances the
reliability of RAG systems when applied to large-scale legal document datasets.

</details>


### [10] [Scalable multilingual PII annotation for responsible AI in LLMs](https://arxiv.org/abs/2510.06250)
*Bharti Meena,Joanna Skubisz,Harshit Rajgarhia,Nand Dave,Kiran Ganesh,Shivali Dalmia,Abhishek Mukherji,Vasudevan Sundarababu,Olga Pospelova*

Main category: cs.CL

TL;DR: 本文介绍了一个可扩展的多语言数据管理框架，专为高质量的PII注释而设计，覆盖13种代表性不足的语言环境。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型(llm)在不同的监管环境中可靠地处理个人身份信息(PII)至关重要。

Method: 一种分阶段的、人工参与的注释方法，将语言专业知识与严格的质量保证相结合。

Result: 从试点、培训和生产阶段来看，召回率和假阳性率都有了显著提高。通过利用注释者间一致性指标和根本原因分析，该框架系统地发现并解决了注释不一致问题，从而生成了适用于监督LLM微调的高保真数据集。

Conclusion: 迭代的、分析驱动的管道可以提高注释质量和下游模型的可靠性。

Abstract: As Large Language Models (LLMs) gain wider adoption, ensuring their reliable
handling of Personally Identifiable Information (PII) across diverse regulatory
contexts has become essential. This work introduces a scalable multilingual
data curation framework designed for high-quality PII annotation across 13
underrepresented locales, covering approximately 336 locale-specific PII types.
Our phased, human-in-the-loop annotation methodology combines linguistic
expertise with rigorous quality assurance, leading to substantial improvements
in recall and false positive rates from pilot, training, and production phases.
By leveraging inter-annotator agreement metrics and root-cause analysis, the
framework systematically uncovers and resolves annotation inconsistencies,
resulting in high-fidelity datasets suitable for supervised LLM fine-tuning.
Beyond reporting empirical gains, we highlight common annotator challenges in
multilingual PII labeling and demonstrate how iterative, analytics-driven
pipelines can enhance both annotation quality and downstream model reliability.

</details>


### [11] [Prakriti200: A Questionnaire-Based Dataset of 200 Ayurvedic Prakriti Assessments](https://arxiv.org/abs/2510.06262)
*Aryan Kumar Singh,Janvi Singh*

Main category: cs.CL

TL;DR: 本数据集包含对一份标准化、双语（英语-印地语）的Prakriti评估问卷的回复，该问卷旨在根据古典阿育吠陀原则评估个体的身体、生理和心理特征。


<details>
  <summary>Details</summary>
Motivation: 为了评估个体的身体、生理和心理特征，为计算智能、阿育吠陀研究和个性化健康分析提供结构化平台。

Method: 使用包含24个多项选择题的标准化双语问卷，通过Google Forms收集数据，自动评分以将个体特征映射到dosha特定分数。

Result: 生成了一个数据集，可以分析特征分布、相关性和预测建模。

Conclusion: 该数据集为未来基于Prakriti的研究和智能健康应用程序的开发提供参考。

Abstract: This dataset provides responses to a standardized, bilingual (English-Hindi)
Prakriti Assessment Questionnaire designed to evaluate the physical,
physiological, and psychological characteristics of individuals according to
classical Ayurvedic principles. The questionnaire consists of 24
multiple-choice items covering body features, appetite, sleep patterns, energy
levels, and temperament. It was developed following AYUSH/CCRAS guidelines to
ensure comprehensive and accurate data collection. All questions are mandatory
and neutrally phrased to minimize bias, and dosha labels (Vata, Pitta, Kapha)
are hidden from participants. Data were collected via a Google Forms
deployment, enabling automated scoring of responses to map individual traits to
dosha-specific scores. The resulting dataset provides a structured platform for
research in computational intelligence, Ayurvedic studies, and personalized
health analytics, supporting analysis of trait distributions, correlations, and
predictive modeling. It can also serve as a reference for future Prakriti-based
studies and the development of intelligent health applications.

</details>


### [12] [Dual-stage and Lightweight Patient Chart Summarization for Emergency Physicians](https://arxiv.org/abs/2510.06263)
*Jiajun Wu,Swaleh Zaidi,Braden Teitge,Henry Leung,Jiayu Zhou,Jessalyn Holodinsky,Steve Drew*

Main category: cs.CL

TL;DR: 本文提出了一种完全在嵌入式设备上运行的两阶段总结系统，可以在保护患者隐私的同时进行离线临床总结。


<details>
  <summary>Details</summary>
Motivation: 急诊医生需要快速从大量的非结构化电子健康记录（EHR）中识别关键信息，这给他们带来了很大的负担。

Method: 该系统采用双设备架构，首先使用Jetson Nano-R检索相关患者记录部分，然后使用Jetson Nano-S生成结构化摘要，并通过轻量级套接字链接进行通信。检索阶段使用本地存储的EHR，将长笔记拆分为语义连贯的部分，并搜索每个查询最相关的部分。生成阶段使用本地托管的小型语言模型（SLM）从检索到的文本生成摘要。

Result: 在MIMIC-IV和去识别化的真实EHR上的初步结果表明，该完全离线系统可以在30秒内有效地生成有用的摘要。

Conclusion: 该研究证明了在资源受限的嵌入式设备上实现离线临床总结的可行性，这有助于保护患者隐私并提高急诊医生的工作效率。

Abstract: Electronic health records (EHRs) contain extensive unstructured clinical data
that can overwhelm emergency physicians trying to identify critical
information. We present a two-stage summarization system that runs entirely on
embedded devices, enabling offline clinical summarization while preserving
patient privacy. In our approach, a dual-device architecture first retrieves
relevant patient record sections using the Jetson Nano-R (Retrieve), then
generates a structured summary on another Jetson Nano-S (Summarize),
communicating via a lightweight socket link. The summarization output is
two-fold: (1) a fixed-format list of critical findings, and (2) a
context-specific narrative focused on the clinician's query. The retrieval
stage uses locally stored EHRs, splits long notes into semantically coherent
sections, and searches for the most relevant sections per query. The generation
stage uses a locally hosted small language model (SLM) to produce the summary
from the retrieved text, operating within the constraints of two NVIDIA Jetson
devices. We first benchmarked six open-source SLMs under 7B parameters to
identify viable models. We incorporated an LLM-as-Judge evaluation mechanism to
assess summary quality in terms of factual accuracy, completeness, and clarity.
Preliminary results on MIMIC-IV and de-identified real EHRs demonstrate that
our fully offline system can effectively produce useful summaries in under 30
seconds.

</details>


### [13] [A Comprehensive Survey of Hallucination in Large Language Models: Causes, Detection, and Mitigation](https://arxiv.org/abs/2510.06265)
*Aisha Alansari,Hamzah Luqman*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在各种任务中表现出色，但也存在幻觉问题，产生不准确或捏造的信息。


<details>
  <summary>Details</summary>
Motivation: LLMs的幻觉问题损害了其可靠性和可信度，尤其是在需要事实准确性的领域。

Method: 本文全面回顾了LLMs中幻觉的研究，重点关注原因、检测和缓解。提出了幻觉类型的分类，并分析了其在LLM开发生命周期中的根本原因。构建了检测方法和缓解策略的结构化分类。

Result: 分析了当前检测和缓解方法的优缺点，并回顾了用于量化LLMs幻觉的现有评估基准和指标。

Conclusion: 概述了未来的关键开放挑战和有希望的方向，为开发更真实和值得信赖的LLMs奠定了基础。

Abstract: Large language models (LLMs) have transformed natural language processing,
achieving remarkable performance across diverse tasks. However, their
impressive fluency often comes at the cost of producing false or fabricated
information, a phenomenon known as hallucination. Hallucination refers to the
generation of content by an LLM that is fluent and syntactically correct but
factually inaccurate or unsupported by external evidence. Hallucinations
undermine the reliability and trustworthiness of LLMs, especially in domains
requiring factual accuracy. This survey provides a comprehensive review of
research on hallucination in LLMs, with a focus on causes, detection, and
mitigation. We first present a taxonomy of hallucination types and analyze
their root causes across the entire LLM development lifecycle, from data
collection and architecture design to inference. We further examine how
hallucinations emerge in key natural language generation tasks. Building on
this foundation, we introduce a structured taxonomy of detection approaches and
another taxonomy of mitigation strategies. We also analyze the strengths and
limitations of current detection and mitigation approaches and review existing
evaluation benchmarks and metrics used to quantify LLMs hallucinations.
Finally, we outline key open challenges and promising directions for future
research, providing a foundation for the development of more truthful and
trustworthy LLMs.

</details>


### [14] [Language models for longitudinal analysis of abusive content in Billboard Music Charts](https://arxiv.org/abs/2510.06266)
*Rohitash Chandra,Yathin Suresh,Divyansh Raj Sinha,Sanchit Jindal*

Main category: cs.CL

TL;DR: 本研究调查了公告牌音乐排行榜中辱骂性和性露骨内容的趋势。


<details>
  <summary>Details</summary>
Motivation: 缺乏验证音乐中此类内容趋势的研究，而这些内容会对儿童和青少年产生有害的行为变化。

Method: 利用深度学习方法分析了过去七十年美国公告牌排行榜上的歌曲歌词，使用情感分析和滥用检测回顾了内容的发展。

Result: 从 1990 年代开始，热门音乐中的露骨内容显着增加。包含亵渎、性露骨和不当语言的歌曲越来越普遍。

Conclusion: 语言模型能够捕捉歌词内容中细微的模式，反映了社会规范和语言使用的转变。

Abstract: There is no doubt that there has been a drastic increase in abusive and
sexually explicit content in music, particularly in Billboard Music Charts.
However, there is a lack of studies that validate the trend for effective
policy development, as such content has harmful behavioural changes in children
and youths. In this study, we utilise deep learning methods to analyse songs
(lyrics) from Billboard Charts of the United States in the last seven decades.
We provide a longitudinal study using deep learning and language models and
review the evolution of content using sentiment analysis and abuse detection,
including sexually explicit content. Our results show a significant rise in
explicit content in popular music from 1990 onwards. Furthermore, we find an
increasing prevalence of songs with lyrics containing profane, sexually
explicit, and otherwise inappropriate language. The longitudinal analysis of
the ability of language models to capture nuanced patterns in lyrical content,
reflecting shifts in societal norms and language use over time.

</details>


### [15] [Reproducibility Study of "XRec: Large Language Models for Explainable Recommendation"](https://arxiv.org/abs/2510.06275)
*Ranjan Mishra,Julian I. Bibo,Quinten van Engelen,Henk Schaapman*

Main category: cs.CL

TL;DR: 本文重现了 Ma et al. (2024) 的论文“XRec: 用于可解释推荐的大型语言模型”中的工作，并使用 Llama 3 代替 GPT-3.5-turbo 进行了评估。


<details>
  <summary>Details</summary>
Motivation: 重现 XRec 论文的结果，并使用 Llama 3 作为 LLM 进行评估。

Method: 基于 Ma et al. (2024) 提供的源代码，修改了 XRec 的 Mixture of Experts 模块的输入或删除了输出嵌入。

Result: XRec 有效地生成个性化解释，并通过整合协作信息提高了其稳定性。然而，XRec 在每个指标上并未始终优于所有基线模型。

Conclusion: 强调了 Mixture of Experts 嵌入在塑造解释结构中的重要性，展示了协作信号如何与语言建模互动。提供了一个开源评估实现，增强了研究人员和从业者的可访问性。

Abstract: In this study, we reproduced the work done in the paper "XRec: Large Language
Models for Explainable Recommendation" by Ma et al. (2024). The original
authors introduced XRec, a model-agnostic collaborative instruction-tuning
framework that enables large language models (LLMs) to provide users with
comprehensive explanations of generated recommendations. Our objective was to
replicate the results of the original paper, albeit using Llama 3 as the LLM
for evaluation instead of GPT-3.5-turbo. We built on the source code provided
by Ma et al. (2024) to achieve our goal. Our work extends the original paper by
modifying the input embeddings or deleting the output embeddings of XRec's
Mixture of Experts module. Based on our results, XRec effectively generates
personalized explanations and its stability is improved by incorporating
collaborative information. However, XRec did not consistently outperform all
baseline models in every metric. Our extended analysis further highlights the
importance of the Mixture of Experts embeddings in shaping the explanation
structures, showcasing how collaborative signals interact with language
modeling. Through our work, we provide an open-source evaluation implementation
that enhances accessibility for researchers and practitioners alike. Our
complete code repository can be found at
https://github.com/julianbibo/xrec-reproducibility.

</details>


### [16] [Type and Complexity Signals in Multilingual Question Representations](https://arxiv.org/abs/2510.06304)
*Robin Kokot,Wessel Poelman*

Main category: cs.CL

TL;DR: 本研究探讨了多语言Transformer模型如何表示问题的形态句法属性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是了解多语言模型如何处理不同语言中问题的结构和复杂性。

Method: 作者构建了一个包含七种语言的QTC数据集，并标注了问题类型和复杂度指标，然后使用选择性控制的探测方法来评估模型在回归标签上的表现。

Result: 结果表明，在具有显式标记的语言中，统计特征可以有效地分类问题，而神经探针可以更好地捕捉细粒度的结构复杂性模式。

Conclusion: 研究结果用于评估上下文表示何时优于统计基线，以及参数更新是否会降低预训练语言信息的可用性。

Abstract: This work investigates how a multilingual transformer model represents
morphosyntactic properties of questions. We introduce the Question Type and
Complexity (QTC) dataset with sentences across seven languages, annotated with
type information and complexity metrics including dependency length, tree
depth, and lexical density. Our evaluation extends probing methods to
regression labels with selectivity controls to quantify gains in
generalizability. We compare layer-wise probes on frozen Glot500-m (Imani et
al., 2023) representations against subword TF-IDF baselines, and a fine-tuned
model. Results show that statistical features classify questions effectively in
languages with explicit marking, while neural probes capture fine-grained
structural complexity patterns better. We use these results to evaluate when
contextual representations outperform statistical baselines and whether
parameter updates reduce the availability of pre-trained linguistic
information.

</details>


### [17] [LLM Bias Detection and Mitigation through the Lens of Desired Distributions](https://arxiv.org/abs/2510.06354)
*Ingroj Shrestha,Padmini Srinivasan*

Main category: cs.CL

TL;DR: 论文提出了一种基于加权自适应损失的微调方法，旨在使大型语言模型的输出分布与期望的分布对齐，从而减轻偏差。


<details>
  <summary>Details</summary>
Motivation: 以往的偏差缓解工作主要关注促进社会公平和人口均等，较少关注使LLM的输出与期望的分布对齐。例如，我们可能希望使模型与现实世界的分布对齐，以支持事实基础。

Method: 提出了一种基于加权自适应损失的微调方法，该方法使LLM的性别-职业输出分布与期望的分布对齐，同时保留了语言建模能力。

Result: 在三种掩码语言模型中观察到两种分布下的偏差。在平等条件下，我们实现了接近完全的缓解，在现实世界设置下实现了30-75%的减少。自回归LLM在平等条件下没有表现出偏差，但在现实世界设置下表现出明显的偏差，Llama Instruct模型（3.2-3B，3.1-8B）实现了50-62%的减少。

Conclusion: 该研究表明，通过调整LLM的输出分布，可以有效减轻偏差，并使其更符合实际应用的需求。

Abstract: Although prior work on bias mitigation has focused on promoting social
equality and demographic parity, less attention has been given to aligning
LLM's outputs to desired distributions. For example, we might want to align a
model with real-world distributions to support factual grounding. Thus, we
define bias as deviation from a desired distribution, which may be an equal or
real-world distribution, depending on application goals. We propose a weighted
adaptive loss based fine-tuning method that aligns LLM's gender-profession
output distribution with the desired distribution, while preserving language
modeling capability. Using 3 profession sets -- male-dominated,
female-dominated, and gender-balanced -- derived from U.S. labor statistics
(2024), we assess both our adaptive method for reflecting reality and a
non-adaptive variant for equality. Across three masked language models, bias is
observed under both distributions. We achieve near-complete mitigation under
equality and 30-75% reduction under real-world settings. Autoregressive LLMs
show no bias under equality but notable bias under real-world settings, with
the Llama Instruct models (3.2-3B, 3.1-8B) achieving a 50-62% reduction.

</details>


### [18] [EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preference](https://arxiv.org/abs/2510.06370)
*Kshitish Ghate,Andy Liu,Devansh Jain,Taylor Sorensen,Atoosa Kasirzadeh,Aylin Caliskan,Mona T. Diab,Maarten Sap*

Main category: cs.CL

TL;DR: EVALUESTEER是一个用于评估大型语言模型（LLMs）和奖励模型（RMs）在多大程度上能够根据用户的价值观和风格偏好进行调整的基准。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在全球范围内的部署，创建能够适应全球用户多样化偏好和价值观的多元化系统至关重要。现有的数据集不支持对奖励模型（RM）指导的可控评估。

Method: 该研究合成了包含165,888个偏好对的数据集，这些偏好对在4个价值观维度（传统、世俗理性、生存和自我表达）和4个风格维度（冗长、可读性、自信和热情）上系统地变化。使用EVALUESTEER来评估，给定一个用户资料和一对候选的带有价值观和风格的回应，LLM和RM是否能够选择与用户偏好相符的输出。

Result: 结果表明，当提供完整的价值观和风格偏好用户资料时，最佳模型在选择正确回应方面的准确率低于75%，而仅提供相关的风格和价值观偏好时，准确率高于99%。

Conclusion: EVALUESTEER突出了当前RM在识别和适应相关用户资料信息方面的局限性，并为开发能够根据不同人类价值观和偏好进行指导的RM提供了一个具有挑战性的试验平台。

Abstract: As large language models (LLMs) are deployed globally, creating pluralistic
systems that can accommodate the diverse preferences and values of users
worldwide becomes essential. We introduce EVALUESTEER, a benchmark to measure
LLMs' and reward models' (RMs) steerability towards users' value and stylistic
preference profiles grounded in psychology and human-LLM interaction
literature. To address the gap in existing datasets that do not support
controlled evaluations of RM steering, we synthetically generated 165,888
preference pairs -- systematically varying pairs along 4 value dimensions
(traditional, secular-rational, survival, and self-expression) and 4 style
dimensions (verbosity, readability, confidence, and warmth). We use EVALUESTEER
to evaluate whether, given a user profile and a pair of candidate value-laden
and style-laden responses, LLMs and RMs are able to select the output that
aligns with the user's preferences. We evaluate six open-source and proprietary
LLMs and RMs under sixteen systematic prompting conditions and six preference
comparison scenarios. Notably, our results show that, when given the user's
full profile of values and stylistic preferences, the best models achieve <75%
accuracy at choosing the correct response, in contrast to >99% accuracy when
only relevant style and value preferences are provided. EVALUESTEER thus
highlights the limitations of current RMs at identifying and adapting to
relevant user profile information, and provides a challenging testbed for
developing RMs that can be steered towards diverse human values and
preferences.

</details>


### [19] [EverydayMMQA: A Multilingual and Multimodal Framework for Culturally Grounded Spoken Visual QA](https://arxiv.org/abs/2510.06371)
*Firoj Alam,Ali Ezzat Shahroor,Md. Arid Hasan,Zien Sheikh Ali,Hunzalah Hassan Bhatti,Mohamed Bayan Kmainasi,Shammur Absar Chowdhury,Basel Mousi,Fahim Dalvi,Nadir Durrani,Natasa Milic-Frayling*

Main category: cs.CL

TL;DR: 本文介绍了一个名为 Everyday Multimodal and Multilingual QA (EverydayMMQA) 的框架，用于创建大规模、具有文化背景的口语和视觉问答 (SVQA) 数据集。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型在视觉问答 (VQA) 等任务上取得了显著成果，但当查询需要具有文化基础的日常知识时，它们常常会失败，尤其是在低资源和代表性不足的语言中。

Method: 使用该框架，我们开发了 OASIS，这是一个整合语音、图像和文本的多模态数据集。OASIS 包含超过约 0.92M 张图像和 14.8M 个 QA 对，包含 3.7M 个口语问题，支持四种独特的输入组合：仅语音、仅文本、语音+图像和文本+图像。

Result: 专注于英语和阿拉伯语的各种变体，覆盖 18 个国家，数据集内容经过精心策划，旨在反映多样化的现实世界情境。OASIS 在涉及实用、常识和文化意识推理的超出对象识别的任务上对模型进行测试。我们对四个闭源模型、三个开源模型和一个微调模型进行了基准测试。

Conclusion: EverydayMMQA 和 OASIS 共同为构建多模态 LLM 提供了一个基准和训练数据集，用于文化背景下的一整套日常任务。该框架和数据集将向社区公开。

Abstract: Large-scale multimodal models achieve strong results on tasks like Visual
Question Answering (VQA), but they often fail when queries require culturally
grounded, everyday knowledge, particularly in low-resource and underrepresented
languages. To bridge this gap, we introduce Everyday Multimodal and
Multilingual QA (EverydayMMQA), a framework for creating large-scale,
culturally-grounded datasets for spoken and visual question answering (SVQA).
Using this framework, we developed OASIS, a multimodal dataset integrating
speech, images, and text. With over ~0.92M images and 14.8M QA pairs, OASIS
contains 3.7M spoken questions, enabling four unique input combinations:
speech-only, text-only, speech+image, and text+image. Focused on English and
Arabic varieties, 18 countries, the dataset content is curated to reflect
diverse, real-world situations. OASIS tests models on tasks beyond object
recognition that involve pragmatic, commonsense, and culturally aware
reasoning. We benchmarked four closed-source models, three open-source models,
and one fine-tuned model. EverydayMMQA and OASIS together provide a benchmark
and training dataset for building multimodal LLMs for a comprehensive set of
everyday tasks within cultural contexts. The framework and dataset will be made
publicly available to the community.

</details>


### [20] [Semantic Regexes: Auto-Interpreting LLM Features with a Structured Language](https://arxiv.org/abs/2510.06378)
*Angie Boggust,Donghao Ren,Yannick Assogba,Dominik Moritz,Arvind Satyanarayan,Fred Hohman*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种新的LLM特征描述方法，称为语义正则表达式，旨在解决自然语言描述的模糊性和不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的自然语言特征描述通常模糊且不一致，需要手动重新标记。

Method: 结合语言和语义特征模式的原语，以及用于上下文、组合和量化的修饰符，生成精确且具有表现力的特征描述。

Result: 语义正则表达式在定量基准测试和定性分析中与自然语言的准确性相匹配，同时产生更简洁和一致的特征描述。此外，其固有的结构允许新型分析，包括量化跨层特征复杂性，将自动可解释性从对单个特征的洞察扩展到模型范围的模式。

Conclusion: 语义正则表达式描述可以帮助人们建立LLM特征激活的准确心理模型。

Abstract: Automated interpretability aims to translate large language model (LLM)
features into human understandable descriptions. However, these natural
language feature descriptions are often vague, inconsistent, and require manual
relabeling. In response, we introduce semantic regexes, structured language
descriptions of LLM features. By combining primitives that capture linguistic
and semantic feature patterns with modifiers for contextualization,
composition, and quantification, semantic regexes produce precise and
expressive feature descriptions. Across quantitative benchmarks and qualitative
analyses, we find that semantic regexes match the accuracy of natural language
while yielding more concise and consistent feature descriptions. Moreover,
their inherent structure affords new types of analyses, including quantifying
feature complexity across layers, scaling automated interpretability from
insights into individual features to model-wide patterns. Finally, in user
studies, we find that semantic regex descriptions help people build accurate
mental models of LLM feature activations.

</details>


### [21] [Protecting De-identified Documents from Search-based Linkage Attacks](https://arxiv.org/abs/2510.06383)
*Pierre Lison,Mark Anderson*

Main category: cs.CL

TL;DR: 提出了一种防止基于搜索的链接攻击的方法，同时保持文本的语义完整性。


<details>
  <summary>Details</summary>
Motivation: 去标识化模型无法解决链接风险，即将去标识化的文本映射回其来源的风险。一种执行此类链接的简单方法是从去标识化的文档中提取短语，然后检查它们是否存在于原始数据集中。

Method: 该方法分两步进行。首先，构建文档集合中出现的 N-gram 的倒排索引，从而可以有效地确定哪些 N-gram 出现在少于 k 个文档中（单独或与其他 N-gram 组合）。然后，迭代查询基于 LLM 的重写器以重新表达这些跨度，直到不再可能进行链接。

Result: 在法院案件集合上的实验结果表明，该方法能够有效地防止基于搜索的链接，同时保持对原始内容的忠实。

Conclusion: 该方法能够有效地防止基于搜索的链接，同时保持对原始内容的忠实。

Abstract: While de-identification models can help conceal the identity of the
individual(s) mentioned in a document, they fail to address linkage risks,
defined as the potential to map the de-identified text back to its source. One
straightforward way to perform such linkages is to extract phrases from the
de-identified document and then check their presence in the original dataset.
This paper presents a method to counter search-based linkage attacks while
preserving the semantic integrity of the text. The method proceeds in two
steps. We first construct an inverted index of the N-grams occurring in the
document collection, making it possible to efficiently determine which N-grams
appear in less than $k$ documents (either alone or in combination with other
N-grams). An LLM-based rewriter is then iteratively queried to reformulate
those spans until linkage is no longer possible. Experimental results on a
collection of court cases show that the method is able to effectively prevent
search-based linkages while remaining faithful to the original content.

</details>


### [22] [Controllable Stylistic Text Generation with Train-Time Attribute-Regularized Diffusion](https://arxiv.org/abs/2510.06386)
*Fan Zhou,Chang Tian,Tim Van de Cruys*

Main category: cs.CL

TL;DR: RegDiff: 通过正则化扩散框架，利用属性特征实现可控文本生成，无需预训练分类器，降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法在风格文本生成中，要么属性控制不佳（CFG），要么计算成本高且泛化性差（CG）。

Method: 提出 RegDiff，一个正则化扩散框架，使用 VAE 编码器-解码器保证重建保真度，并使用潜在扩散模型和属性监督实现可控文本生成。属性信息仅在训练期间注入。

Result: 在五个数据集上的实验表明，RegDiff 在生成风格文本方面优于强大的基线。

Conclusion: RegDiff 是一种用于属性可控文本扩散的有效解决方案。

Abstract: Generating stylistic text with specific attributes is a key problem in
controllable text generation. Recently, diffusion models have emerged as a
powerful paradigm for both visual and textual generation. Existing approaches
can be broadly categorized into classifier-free guidance (CFG) and classifier
guidance (CG) methods. While CFG effectively preserves semantic content, it
often fails to provide effective attribute control. In contrast, CG modifies
the denoising trajectory using classifier gradients, enabling better attribute
alignment but incurring high computational costs during sampling and suffering
from classifier generalization issues. In this work, we propose RegDiff, a
regularized diffusion framework that leverages attribute features without
requiring a pretrained classifier during sampling, thereby achieving
controllable generation with reduced computational costs. Specifically, RegDiff
employs a VAE-based encoder--decoder architecture to ensure reconstruction
fidelity and a latent diffusion model trained with attribute supervision to
enable controllable text generation. Attribute information is injected only
during training. Experiments on five datasets spanning multiple stylistic
attributes demonstrate that RegDiff outperforms strong baselines in generating
stylistic texts. These results validate the effectiveness of RegDiff as an
efficient solution for attribute-controllable text diffusion. Our code,
datasets, and resources will be released upon publication at
https://github.com/xxxx.

</details>


### [23] [Reward Model Perspectives: Whose Opinions Do Reward Models Reward?](https://arxiv.org/abs/2510.06391)
*Elle*

Main category: cs.CL

TL;DR: 奖励模型（RM）对于语言模型（LM）的对齐至关重要，但我们对其行为的理解有限。本文研究了RM在多大程度上表现出社会人口统计学偏差，以及提示对将奖励导向目标群体偏好的影响。研究表明，RM与某些人口群体对齐较差，并且会系统性地奖励有害的刻板印象，并且仅靠引导不足以克服这些限制。


<details>
  <summary>Details</summary>
Motivation: 了解奖励模型（RM）的行为对于防止语言技术中不必要的社会偏见至关重要。当前的理解有限，需要深入研究。

Method: 1. 形式化一个框架，用于测量RM捕获的意见的对齐程度。2. 调查RM表现出社会人口统计学偏差的程度。3. 探讨提示对将奖励导向目标群体偏好的影响。研究主题是关于有争议话题的主观和多样化观点。

Result: RM与某些人口群体对齐较差，并且会系统性地奖励有害的刻板印象。单独的引导不足以克服这些限制。

Conclusion: 在偏好学习期间，需要更仔细地考虑模型对齐中RM的行为，以防止在我们使用的语言技术中传播不必要的社会偏见。

Abstract: Reward models (RMs) are central to the alignment of language models (LMs). An
RM often serves as a proxy for human preferences to guide downstream LM
behavior. However, our understanding of RM behavior is limited. Our work (i)
formalizes a framework for measuring the alignment of opinions captured by RMs,
(ii) investigates the extent to which RMs demonstrate sociodemographic biases,
and (iii) explores the effects of prompting to steer rewards towards the
preferences of a target group. We study the subjective and diverse perspectives
on controversial topics, which allows us to quantify RM perspectives in terms
of their opinions, attitudes, and values. We show that RMs are poorly aligned
with several demographic groups and can systematically reward harmful
stereotypes, and steering alone is not enough to overcome these limitations.
Our findings underscore the need for more careful consideration of RM behavior
in model alignment during preference learning to prevent the propagation of
unwanted social biases in the language technologies that we use.

</details>


### [24] [Instructional Goal-Aligned Question Generation for Student Evaluation in Virtual Lab Settings: How Closely Do LLMs Actually Align?](https://arxiv.org/abs/2510.06411)
*R. Alexander Knipper,Indrani Dey,Souvika Sarkar,Hari Narayanan,Sadhana Puntambekar,Santu Karmaker*

Main category: cs.CL

TL;DR: 本研究提出了一种新的教学目标对齐问题生成框架，该框架利用大型语言模型（LLM）通过自然语言交互生成与模拟对齐的、具有教学意义的问题。


<details>
  <summary>Details</summary>
Motivation: 教师很难调整虚拟实验室以适应他们的教学目标。第三方材料可能与课堂需求不符，并且开发定制资源可能非常耗时且难以扩展。大型语言模型（LLM）的最新进展为解决这些限制提供了一条有希望的途径。

Method: 该框架集成了四个组件：通过教师-LLM对话进行教学目标理解，通过知识单元和关系分析进行实验室理解，用于构建认知和教学意图的问题分类法，以及用于控制提示细节的TELeR分类法。

Result: 目标和实验室理解将问题建立在教师意图和模拟背景中，问题分类法提升了认知需求（开放式格式和关系类型将质量提高了0.29-0.39分），并且优化的TELeR提示增强了格式依从性（80%的可解析性，>90%的依从性）。

Conclusion: 更大的模型产生了最强的增益：可解析性+37.1%，依从性+25.7%，平均质量+0.8 Likert点。

Abstract: Virtual Labs offer valuable opportunities for hands-on, inquiry-based science
learning, yet teachers often struggle to adapt them to fit their instructional
goals. Third-party materials may not align with classroom needs, and developing
custom resources can be time-consuming and difficult to scale. Recent advances
in Large Language Models (LLMs) offer a promising avenue for addressing these
limitations. In this paper, we introduce a novel alignment framework for
instructional goal-aligned question generation, enabling teachers to leverage
LLMs to produce simulation-aligned, pedagogically meaningful questions through
natural language interaction. The framework integrates four components:
instructional goal understanding via teacher-LLM dialogue, lab understanding
via knowledge unit and relationship analysis, a question taxonomy for
structuring cognitive and pedagogical intent, and the TELeR taxonomy for
controlling prompt detail. Early design choices were informed by a small
teacher-assisted case study, while our final evaluation analyzed over 1,100
questions from 19 open-source LLMs. With goal and lab understanding grounding
questions in teacher intent and simulation context, the question taxonomy
elevates cognitive demand (open-ended formats and relational types raise
quality by 0.29-0.39 points), and optimized TELeR prompts enhance format
adherence (80% parsability, >90% adherence). Larger models yield the strongest
gains: parsability +37.1%, adherence +25.7%, and average quality +0.8 Likert
points.

</details>


### [25] [FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial Long-Form Question Answering](https://arxiv.org/abs/2510.06426)
*Yitao Long,Tiansheng Hu,Yilun Zhao,Arman Cohan,Chen Zhao*

Main category: cs.CL

TL;DR: 这篇论文介绍了一个新的金融长文本问答基准测试 FinLFQA，旨在评估大型语言模型在生成答案时提供可靠和细致的归因的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注检索支持文本证据作为参考的简单归因，但在实际金融应用中，归因需要更进一步。因此，本文旨在设计一个能够评估LLM在复杂金融问题上生成长文本答案，并进行可靠和细致归因能力的基准测试。

Method: 本文构建了一个名为FinLFQA的基准测试，并通过人工标注评估归因的三个关键方面：从财务报告中提取的支持证据、中间数值推理步骤以及为推理过程提供信息的领域特定财务知识。此外，还提供了一个自动评估框架，涵盖答案质量和归因质量。

Result: 通过在八个LLM上进行的大量实验，发现细粒度指标对于区分模型能力非常重要，端到端生成与事后方法取得了相当的性能，并且迭代改进只有在外部反馈的指导下才能有所帮助。

Conclusion: 本文提出了FinLFQA基准测试，可以有效评估LLM在金融领域的长文本问答中生成可靠和细致归因的能力，并通过实验验证了该基准测试的有效性，为未来研究提供了有价值的工具和发现。

Abstract: Large Language Models (LLMs) frequently hallucinate to long-form questions,
producing plausible yet factually incorrect answers. A common mitigation
strategy is to provide attribution to LLM outputs. However, existing benchmarks
primarily focus on simple attribution that retrieves supporting textual
evidence as references. We argue that in real-world scenarios such as financial
applications, attribution goes beyond reference retrieval. We introduce
FinLFQA, a benchmark designed to evaluate the ability of LLMs to generate
long-form answers to complex financial questions with reliable and nuanced
attributions. FinLFQA evaluates three critical aspects of attribution through
human annotations: (1) supporting evidence extracted from financial reports,
(2) intermediate numerical reasoning steps, and (3) domain-specific financial
knowledge that informs the reasoning process. We further provide an automatic
evaluation framework covering both answer quality and attribution quality.
Through extensive experiments on eight LLMs across multiple
attribution-generation paradigms, we find that fine-grained metrics are
important to distinguish model capabilities, that end-to-end generation
achieves comparable performance to post-hoc approaches, and that iterative
refinement only helps when guided by external feedback.

</details>


### [26] [Bridging Discourse Treebanks with a Unified Rhetorical Structure Parser](https://arxiv.org/abs/2510.06427)
*Elena Chistova*

Main category: cs.CL

TL;DR: UniRST is a unified RST discourse parser for multiple treebanks and languages.


<details>
  <summary>Details</summary>
Motivation: To create a single discourse parser that can handle multiple treebanks and languages without modifying relation inventories.

Method: Two training strategies: Multi-Head and Masked-Union.

Result: Masked-Union is the strongest approach. UniRST outperforms 16 of 18 mono-treebank baselines.

Conclusion: UniRST demonstrates the advantages of a single-model, multilingual end-to-end discourse parsing across diverse resources.

Abstract: We introduce UniRST, the first unified RST-style discourse parser capable of
handling 18 treebanks in 11 languages without modifying their relation
inventories. To overcome inventory incompatibilities, we propose and evaluate
two training strategies: Multi-Head, which assigns separate relation
classification layer per inventory, and Masked-Union, which enables shared
parameter training through selective label masking. We first benchmark
monotreebank parsing with a simple yet effective augmentation technique for
low-resource settings. We then train a unified model and show that (1) the
parameter efficient Masked-Union approach is also the strongest, and (2) UniRST
outperforms 16 of 18 mono-treebank baselines, demonstrating the advantages of a
single-model, multilingual end-to-end discourse parsing across diverse
resources.

</details>


### [27] [MathRobust-LV: Evaluation of Large Language Models' Robustness to Linguistic Variations in Mathematical Reasoning](https://arxiv.org/abs/2510.06430)
*Neeraja Kirtane,Yuvraj Khanna,Peter Relan*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型在面对语言变体时，解决高中数学问题的鲁棒性问题。作者构建了一个名为 MathRobust-LV 的测试集，该测试集通过改变问题中的表面细节（如名称、上下文、变量），同时保持数值结构和答案不变来评估模型的鲁棒性。实验结果表明，现有模型在面对语言变体时，准确率会显著下降，尤其是在较小的模型上。这表明语言模型的鲁棒性仍然是一个重要的挑战。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在真实教育场景中解决高中数学问题的鲁棒性，特别是在面对语言变体时。现有研究主要集中在高难度竞赛题上，而忽略了实际教育应用中常见的语言表达变化。论文旨在填补这一空白，并强调语言鲁棒性对于模型在辅导和评估系统中的可靠部署至关重要。

Method: 1. 构建 MathRobust-LV 测试集，该测试集包含高中水平的数学问题，并通过改变问题的表面细节来创建变体，同时保持数值结构和答案不变。
2. 在 34 个不同的语言模型上进行实验，比较模型在原始问题和变体问题上的准确率。
3. 分析实验结果，评估模型在面对语言变体时的鲁棒性。

Result: 实验结果表明，现有模型在面对语言变体时，准确率会显著下降。较小的模型下降幅度为 9-11%，即使是更强大的模型也表现出可衡量的性能下降。GPT-5 和 Gemini-2.5pro 等前沿模型相对稳定，但仍然受到影响。

Conclusion: 论文强调了语言变体鲁棒性是一个根本性的挑战，并揭示了模型在推理方面的脆弱性。研究结果表明，即使在数学数据基准测试被认为是饱和的情况下，语言模型的鲁棒性仍然有很大的改进空间。

Abstract: Large language models excel on math benchmarks, but their math reasoning
robustness to linguistic variation is underexplored. While recent work
increasingly treats high-difficulty competitions like the IMO as the gold
standard for evaluating reasoning, we believe in comprehensive benchmarking of
high school-level math problems in real educational settings. We introduce
MathRobust-LV, a test set and evaluation methodology that mirrors how
instructors rephrase problems across assessments while keeping difficulty
constant: we change surface details (names, contexts, variables) while
preserving numerical structure and answers. In contrast to prior efforts that
alter problem content or emphasize IMO-level tasks, we focus on
high-school-level dataset problems at the difficulty level where models are
currently deployed in educational settings: tutoring and assessment systems. In
these applications, instructors rephrase identical concepts in varied ways,
making linguistic robustness essential for reliable deployment. Although MATH
data benchmarking is often regarded as saturated, our experiment on 34 models
reveals that accuracy declines when moving from the baseline to the variants.
These drops are severe for smaller models (9-11%) while stronger models also
show measurable degradation. Frontier models like GPT-5, Gemini-2.5pro remain
comparatively stable. Our results highlight that robustness to linguistic
variation is a fundamental challenge, exposing reasoning vulnerabilities in
models.

</details>


### [28] [A Survey on Agentic Security: Applications, Threats and Defenses](https://arxiv.org/abs/2510.06445)
*Asif Shahriar,Md Nafiu Rahman,Sadif Ahmed,Farig Sadeque,Md Rizwan Parvez*

Main category: cs.CL

TL;DR: 本文全面综述了 LLM 智能体安全领域，从应用、威胁和防御三个相互依存的支柱构建了该领域。


<details>
  <summary>Details</summary>
Motivation: 自主 LLM 智能体的出现标志着网络安全领域的新范例，但也引入了一类新的安全风险。

Method: 对 150 多篇论文进行了全面的分类，解释了智能体的用途、漏洞以及旨在保护它们的对策。

Result: 详细的交叉分析显示了智能体架构的新兴趋势，同时揭示了模型和模态覆盖范围的关键研究差距。

Conclusion: 本文首次全面地展示了智能体安全态势，并为未来的研究方向提供了指导。

Abstract: The rapid shift from passive LLMs to autonomous LLM-agents marks a new
paradigm in cybersecurity. While these agents can act as powerful tools for
both offensive and defensive operations, the very agentic context introduces a
new class of inherent security risks. In this work we present the first
holistic survey of the agentic security landscape, structuring the field around
three interdependent pillars: Applications, Threats, and Defenses. We provide a
comprehensive taxonomy of over 150 papers, explaining how agents are used, the
vulnerabilities they possess, and the countermeasures designed to protect them.
A detailed cross-cutting analysis shows emerging trends in agent architecture
while revealing critical research gaps in model and modality coverage.

</details>


### [29] [Linguistically Informed Tokenization Improves ASR for Underresourced Languages](https://arxiv.org/abs/2510.06461)
*Massimo Daul,Alessio Tosolini,Claire Bowern*

Main category: cs.CL

TL;DR: 本研究探索了自动语音识别（ASR）在资源匮乏语言中的应用，特别是在Yan-nhangu语上。


<details>
  <summary>Details</summary>
Motivation: 现代ASR系统需要大量数据，这使得它们通常无法用于资源匮乏的语言。本研究旨在探索ASR在语言文档记录中的可行性。

Method: 本研究在Yan-nhangu语上微调了一个wav2vec2 ASR模型，并比较了音素和正字法分词策略对性能的影响。

Result: 研究发现，与基线正字法分词方案相比，语言学上的音素分词系统大大提高了WER和CER。

Conclusion: 研究表明，手动校正ASR模型的输出比从头开始手动转录音频要快得多，证明ASR可以用于资源匮乏的语言。

Abstract: Automatic speech recognition (ASR) is a crucial tool for linguists aiming to
perform a variety of language documentation tasks. However, modern ASR systems
use data-hungry transformer architectures, rendering them generally unusable
for underresourced languages. We fine-tune a wav2vec2 ASR model on Yan-nhangu,
a dormant Indigenous Australian language, comparing the effects of phonemic and
orthographic tokenization strategies on performance. In parallel, we explore
ASR's viability as a tool in a language documentation pipeline. We find that a
linguistically informed phonemic tokenization system substantially improves WER
and CER compared to a baseline orthographic tokenization scheme. Finally, we
show that hand-correcting the output of an ASR model is much faster than
hand-transcribing audio from scratch, demonstrating that ASR can work for
underresourced languages.

</details>


### [30] [Test-Time Scaling of Reasoning Models for Machine Translation](https://arxiv.org/abs/2510.06471)
*Zihao Li,Shaoxiong Ji,Jörg Tiedemann*

Main category: cs.CL

TL;DR: 本文研究了测试时缩放（TTS）在机器翻译（MT）中的效果，发现通用RM的TTS效果有限，但领域特定微调可以解锁TTS的潜力，在后编辑中TTS非常有效。


<details>
  <summary>Details</summary>
Motivation: 探讨增加推理时间计算是否能提高翻译质量，以及TTS在机器翻译中的应用潜力。

Method: 在多个MT基准上评估了12个RM，涵盖直接翻译、强制推理外推和后编辑三种场景。

Result: 通用RM的TTS对直接翻译的提升有限且不稳定；领域特定微调能有效提升TTS效果；强制模型过度推理会降低翻译质量；TTS在后编辑中非常有效。

Conclusion: 推理时间计算的价值在于多步骤自校正工作流程和任务专用模型，而不是增强通用模型的单次翻译。

Abstract: Test-time scaling (TTS) has enhanced the performance of Reasoning Models
(RMs) on various tasks such as math and coding, yet its efficacy in machine
translation (MT) remains underexplored. This paper investigates whether
increased inference-time computation improves translation quality. We evaluate
12 RMs across a diverse suite of MT benchmarks spanning multiple domains,
examining three scenarios: direct translation, forced-reasoning extrapolation,
and post-editing. Our findings show that for general-purpose RMs, TTS provides
limited and inconsistent benefits for direct translation, with performance
quickly plateauing. However, the effectiveness of TTS is unlocked by
domain-specific fine-tuning, which aligns a model's reasoning process with task
requirements, leading to consistent improvements up to an optimal,
self-determined reasoning depth. We also find that forcing a model to reason
beyond its natural stopping point consistently degrades translation quality. In
contrast, TTS proves highly effective in a post-editing context, reliably
turning self-correction into a beneficial process. These results indicate that
the value of inference-time computation in MT lies not in enhancing single-pass
translation with general models, but in targeted applications like multi-step,
self-correction workflows and in conjunction with task-specialized models.

</details>


### [31] [Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels](https://arxiv.org/abs/2510.06499)
*Zhepeng Cen,Haolin Chen,Shiyu Wang,Zuxin Liu,Zhiwei Liu,Ding Zhao,Silvio Savarese,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: 提出了一种新的数据引擎 Webscale-RL，用于将大规模预训练文档转换为强化学习 (RL) 的问答对。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习数据集比网络规模的预训练语料库小几个数量级且多样性不足，限制了强化学习的应用。

Method: 构建了一个名为 Webscale-RL 的数据管道，它可以系统地将大规模预训练文档转换为数百万个不同的、可验证的问答对，从而构建包含超过 9 个领域 120 万个示例的 Webscale-RL 数据集。

Result: 使用该数据集训练的模型在一系列基准测试中明显优于持续预训练和强数据细化基线，并且效率更高，只需少 100 倍的 tokens 即可达到持续预训练的性能。

Conclusion: 这项工作为将强化学习扩展到预训练水平提供了一条可行的途径，从而能够实现更强大、更高效的语言模型。

Abstract: Large Language Models (LLMs) have achieved remarkable success through
imitation learning on vast text corpora, but this paradigm creates a
training-generation gap and limits robust reasoning. Reinforcement learning
(RL) offers a more data-efficient solution capable of bridging this gap, yet
its application has been constrained by a critical data bottleneck: existing RL
datasets are orders of magnitude smaller and less diverse than web-scale
pre-training corpora. To address this, we introduce the Webscale-RL pipeline, a
scalable data engine that systematically converts large-scale pre-training
documents into millions of diverse, verifiable question-answer pairs for RL.
Using this pipeline, we construct the Webscale-RL dataset, containing 1.2
million examples across more than 9 domains. Our experiments show that the
model trained on this dataset significantly outperforms continual pretraining
and strong data refinement baselines across a suite of benchmarks. Notably, RL
training with our dataset proves substantially more efficient, achieving the
performance of continual pre-training with up to 100$\times$ fewer tokens. Our
work presents a viable path toward scaling RL to pre-training levels, enabling
more capable and efficient language models.

</details>


### [32] [From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language Model Pretraining](https://arxiv.org/abs/2510.06548)
*Seng Pei Liew,Takuya Kato*

Main category: cs.CL

TL;DR: 研究了bootstrap预训练的扩展行为，发现其扩展效率会降低。


<details>
  <summary>Details</summary>
Motivation: 重用预训练的基础模型进行进一步的预训练，例如持续预训练或模型增长，有望降低从头开始训练语言模型的成本。然而，其有效性仍不清楚，特别是当应用于过度训练的基础模型时。

Method: 通过经验研究bootstrap预训练的扩展行为。

Result: 发现bootstrap预训练的扩展效率以可预测的方式降低：关于第二阶段预训练tokens的扩展指数随着用于预训练基础模型的tokens数量呈对数递减。第一阶段和第二阶段tokens的联合依赖关系可以通过一个简单的扩展定律来准确建模。

Conclusion: 多阶段预训练策略存在一个根本的权衡：模型经过越广泛的预训练，bootstrap提供的额外好处就越少。研究结果为高效的语言模型训练提供了实践见解，并为过度训练模型的重用提出了重要的考虑因素。

Abstract: Bootstrapped pretraining, i.e., the reuse of a pretrained base model for
further pretraining, such as continual pretraining or model growth, is
promising at reducing the cost of training language models from scratch.
However, its effectiveness remains unclear, especially when applied to
overtrained base models. In this work, we empirically study the scaling
behavior of bootstrapped pretraining and find that its scaling efficiency
diminishes in a predictable manner: The scaling exponent with respect to
second-stage pretraining tokens decreases logarithmically with the number of
tokens used to pretrain the base model. The joint dependence on first- and
second-stage tokens is accurately modeled by a simple scaling law. Such
saturation effect reveals a fundamental trade-off in multi-stage pretraining
strategies: the more extensively a model is pretrained, the less additional
benefit bootstrapping provides. Our findings provide practical insights for
efficient language model training and raise important considerations for the
reuse of overtrained models.

</details>


### [33] [Flipping the Dialogue: Training and Evaluating User Language Models](https://arxiv.org/abs/2510.06552)
*Tarek Naous,Philippe Laban,Wei Xu,Jennifer Neville*

Main category: cs.CL

TL;DR: 助理语言模型（LM）在多轮对话中作为用户模拟器表现不佳，更好的助理反而导致更差的模拟效果。


<details>
  <summary>Details</summary>
Motivation: 为了评估语言模型在真实场景中的性能，现有研究通常模拟多轮对话中的用户，但直接使用训练成助理的语言模型来模拟用户存在问题。

Method: 提出专门构建的用户语言模型（User LM），该模型经过专门训练以模拟多轮对话中的人类用户。

Result: 用户语言模型与人类行为更吻合，并且比现有的模拟方法具有更好的鲁棒性。在模拟编码和数学对话时，更真实的环境会导致助理模型（GPT-4o）的性能下降。

Conclusion: 更真实的模拟环境揭示了助理模型在多轮对话中难以应对用户细微差别的缺陷。

Abstract: Conversations with LMs involve two participants: a human user leading the
conversation, and an LM assistant responding to the user's request. To satisfy
this specific role, LMs are post-trained to be helpful assistants -- optimized
to produce exhaustive and well-structured responses, free of ambiguity and
grammar errors. User utterances, on the other hand, are rarely perfected, with
each user phrasing requests in unique ways, sometimes putting in partial effort
at each turn and refining on the fly. To evaluate LM performance in realistic
settings, prior work simulated users in multi-turn conversations, often
prompting an LLM originally trained to be a helpful assistant to act as a user.
However, we show that assistant LMs make for poor user simulators, with the
surprising finding that better assistants yield worse simulators. Instead, we
introduce purpose-built User Language Models (User LMs) - models post-trained
to simulate human users in multi-turn conversations. Through various
evaluations, we show how User LMs align better with human behavior and achieve
better simulation robustness than existing simulation methods. When leveraging
User LMs to simulate coding and math conversations, the performance of a strong
assistant (GPT-4o) drops from 74.6% to 57.4%, confirming that more realistic
simulation environments lead to assistant struggles as they fail to cope with
the nuances of users in multi-turn setups.

</details>


### [34] [The Algebra of Meaning: Why Machines Need Montague More Than Moore's Law](https://arxiv.org/abs/2510.06559)
*Cheonkam Jeong,Sungdo Kim,Jewoo Park*

Main category: cs.CL

TL;DR: 本文提出了一种新的神经符号架构Savassan，它将自然语言编译成Montague风格的逻辑形式，并将其映射到扩展了义务算子和管辖范围上下文的类型本体，以实现合规感知指导。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型虽然流畅，但经常错误地处理其输出所蕴含的意义类型。幻觉、脆弱的适度和不透明的合规结果是缺少类型理论语义的症状，而不是数据或规模的限制。

Method: 提出了一种神经符号架构Savassan，它将话语编译成Montague风格的逻辑形式，并将其映射到扩展了义务算子和管辖范围上下文的类型本体。神经组件从非结构化输入中提取候选结构；符号组件执行类型检查、约束推理和跨管辖范围映射，以产生合规感知指导。

Result: Savassan系统能够将结果投影到多个法律本体中，并将结果组合成一个可解释的决策。

Conclusion: 可信的自主性需要对意义进行组合类型化，使系统能够在统一的意义代数中推理所描述的内容、所规定的内容以及所产生的责任。

Abstract: Contemporary language models are fluent yet routinely mis-handle the types of
meaning their outputs entail. We argue that hallucination, brittle moderation,
and opaque compliance outcomes are symptoms of missing type-theoretic semantics
rather than data or scale limitations. Building on Montague's view of language
as typed, compositional algebra, we recast alignment as a parsing problem:
natural-language inputs must be compiled into structures that make explicit
their descriptive, normative, and legal dimensions under context.
  We present Savassan, a neuro-symbolic architecture that compiles utterances
into Montague-style logical forms and maps them to typed ontologies extended
with deontic operators and jurisdictional contexts. Neural components extract
candidate structures from unstructured inputs; symbolic components perform type
checking, constraint reasoning, and cross-jurisdiction mapping to produce
compliance-aware guidance rather than binary censorship. In cross-border
scenarios, the system "parses once" (e.g., defect claim(product x, company y))
and projects the result into multiple legal ontologies (e.g., defamation risk
in KR/JP, protected opinion in US, GDPR checks in EU), composing outcomes into
a single, explainable decision.
  This paper contributes: (i) a diagnosis of hallucination as a type error;
(ii) a formal Montague-ontology bridge for business/legal reasoning; and (iii)
a production-oriented design that embeds typed interfaces across the pipeline.
We outline an evaluation plan using legal reasoning benchmarks and synthetic
multi-jurisdiction suites. Our position is that trustworthy autonomy requires
compositional typing of meaning, enabling systems to reason about what is
described, what is prescribed, and what incurs liability within a unified
algebra of meaning.

</details>


### [35] [WeatherArchive-Bench: Benchmarking Retrieval-Augmented Reasoning for Historical Weather Archives](https://arxiv.org/abs/2510.05336)
*Yongan Yu,Xianda Du,Qingchen Hu,Jiahao Liang,Jingwei Ni,Dan Qiang,Kaiyu Huang,Grant McKenzie,Renee Sieber,Fengran Mo*

Main category: cs.CL

TL;DR: 提出了一个名为 WeatherArchive-Bench 的基准，用于评估检索增强生成 (RAG) 系统在历史天气档案上的性能。


<details>
  <summary>Details</summary>
Motivation: 历史天气档案提供了关于社会如何经历和应对极端天气事件的丰富叙述，但由于其规模庞大、数字化质量差和语言古老，难以转化为气候研究的结构化知识。

Method: 构建了 WeatherArchive-Bench，包含两个任务：WeatherArchive-Retrieval（评估系统定位历史相关段落的能力）和 WeatherArchive-Assessment（评估大型语言模型 (LLM) 对极端天气叙述中社会脆弱性和复原力指标进行分类的能力）。

Result: 实验表明，稠密检索器在历史术语上常常失败，而 LLM 经常误解脆弱性和复原力概念。

Conclusion: 研究结果突出了在推理复杂社会指标方面的关键限制，并为从档案背景设计更强大的、以气候为中心的 RAG 系统提供了见解。

Abstract: Historical archives on weather events are collections of enduring primary
source records that offer rich, untapped narratives of how societies have
experienced and responded to extreme weather events. These qualitative accounts
provide insights into societal vulnerability and resilience that are largely
absent from meteorological records, making them valuable for climate scientists
to understand societal responses. However, their vast scale, noisy digitized
quality, and archaic language make it difficult to transform them into
structured knowledge for climate research. To address this challenge, we
introduce WeatherArchive-Bench, the first benchmark for evaluating
retrieval-augmented generation (RAG) systems on historical weather archives.
WeatherArchive-Bench comprises two tasks: WeatherArchive-Retrieval, which
measures a system's ability to locate historically relevant passages from over
one million archival news segments, and WeatherArchive-Assessment, which
evaluates whether Large Language Models (LLMs) can classify societal
vulnerability and resilience indicators from extreme weather narratives.
Extensive experiments across sparse, dense, and re-ranking retrievers, as well
as a diverse set of LLMs, reveal that dense retrievers often fail on historical
terminology, while LLMs frequently misinterpret vulnerability and resilience
concepts. These findings highlight key limitations in reasoning about complex
societal indicators and provide insights for designing more robust
climate-focused RAG systems from archival contexts. The constructed dataset and
evaluation framework are publicly available at
https://anonymous.4open.science/r/WeatherArchive-Bench/.

</details>


### [36] [TinyScientist: An Interactive, Extensible, and Controllable Framework for Building Research Agents](https://arxiv.org/abs/2510.06579)
*Haofei Yu,Keyang Xuan,Fenghai Li,Kunlun Zhu,Zijie Lei,Jiaxun Zhang,Ziheng Qi,Kyle Richardson,Jiaxuan You*

Main category: cs.CL

TL;DR: TinyScientist提出了一种交互式、可扩展且可控的框架，以应对自动研究工作流程的复杂性，该框架易于适应新工具并支持迭代增长。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的研究人员和开发人员开始使用和构建这些工具和平台，扩展和维护此类代理工作流程的复杂性和难度已成为一项重大挑战，尤其是在算法和架构不断发展的情况下。

Method: TinyScientist 识别了自动研究工作流程的基本组件，并提出了一个交互式、可扩展且可控的框架，该框架易于适应新工具并支持迭代增长。

Result: 我们提供了一个开源代码库、一个交互式网络演示和一个 PyPI Python 包，以使最先进的自动研究管道广泛可供每个研究人员和开发人员使用。

Conclusion: TinyScientist 旨在通过提供一个易于使用、可扩展且可控的框架来应对自动研究工作流程的复杂性。

Abstract: Automatic research with Large Language Models (LLMs) is rapidly gaining
importance, driving the development of increasingly complex workflows involving
multi-agent systems, planning, tool usage, code execution, and human-agent
interaction to accelerate research processes. However, as more researchers and
developers begin to use and build upon these tools and platforms, the
complexity and difficulty of extending and maintaining such agentic workflows
have become a significant challenge, particularly as algorithms and
architectures continue to advance. To address this growing complexity,
TinyScientist identifies the essential components of the automatic research
workflow and proposes an interactive, extensible, and controllable framework
that easily adapts to new tools and supports iterative growth. We provide an
open-source codebase, an interactive web demonstration, and a PyPI Python
package to make state-of-the-art auto-research pipelines broadly accessible to
every researcher and developer.

</details>


### [37] [Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?](https://arxiv.org/abs/2510.06594)
*Sri Durga Sai Sowmya Kadali,Evangelos E. Papalexakis*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）的越狱现象，通过检查LLM的内部表示，重点关注隐藏层如何响应越狱提示与良性提示。


<details>
  <summary>Details</summary>
Motivation: 随着对话式LLM的日益普及和可访问性，越狱大型语言模型（LLM）已成为一个紧迫的问题。攻击者不断开发新的提示技术，并且没有现有模型可以被认为是完全抗攻击的。

Method: 我们分析了开源LLM GPT-J和状态空间模型Mamba2，展示了突出不同层行为的初步发现。

Result: 我们的结果为进一步研究利用内部模型动态进行鲁棒的越狱检测和防御提供了有希望的方向。

Conclusion: 这项研究为利用内部模型动态进行鲁棒的越狱检测和防御提供了有希望的方向。

Abstract: Jailbreaking large language models (LLMs) has emerged as a pressing concern
with the increasing prevalence and accessibility of conversational LLMs.
Adversarial users often exploit these models through carefully engineered
prompts to elicit restricted or sensitive outputs, a strategy widely referred
to as jailbreaking. While numerous defense mechanisms have been proposed,
attackers continuously develop novel prompting techniques, and no existing
model can be considered fully resistant. In this study, we investigate the
jailbreak phenomenon by examining the internal representations of LLMs, with a
focus on how hidden layers respond to jailbreak versus benign prompts.
Specifically, we analyze the open-source LLM GPT-J and the state-space model
Mamba2, presenting preliminary findings that highlight distinct layer-wise
behaviors. Our results suggest promising directions for further research on
leveraging internal model dynamics for robust jailbreak detection and defense.

</details>


### [38] [A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures](https://arxiv.org/abs/2510.06640)
*Nhat M. Hoang,Do Xuan Long,Cong-Duy Nguyen,Min-Yen Kan,Luu Anh Tuan*

Main category: cs.CL

TL;DR: 本文对SSM和TBM在长序列处理中的表征传播进行了token和layer级别的统一分析，揭示了它们在信息流动上的关键差异。


<details>
  <summary>Details</summary>
Motivation: 尽管SSM在长序列处理中表现出高效性，但对其内部的上下文信息流动方式尚缺乏深入理解。

Method: 使用中心核对齐、稳定性指标和探针技术，来描述表征在层内和层间的演变。

Result: TBM迅速同质化token表征，多样性仅在后期层重新出现，而SSM在早期保持token独特性，但更深层趋于同质化。TBM的过度平滑源于架构设计，而SSM的过度平滑主要来自训练动态。

Conclusion: 这些发现阐明了两种架构的归纳偏置，并为未来长上下文推理的模型和训练设计提供了信息。

Abstract: State Space Models (SSMs) have recently emerged as efficient alternatives to
Transformer-Based Models (TBMs) for long-sequence processing, offering linear
scaling and lower memory use. Yet, how contextual information flows across
layers and tokens in these architectures remains understudied. We present the
first unified, token- and layer-level analysis of representation propagation in
SSMs and TBMs. Using centered kernel alignment, stability metrics, and probing,
we characterize how representations evolve within and across layers. We find a
key divergence: TBMs rapidly homogenize token representations, with diversity
reemerging only in later layers, while SSMs preserve token uniqueness early but
converge to homogenization deeper. Theoretical analysis and parameter
randomization further reveal that oversmoothing in TBMs stems from
architectural design, whereas in SSMs it arises mainly from training dynamics.
These insights clarify the inductive biases of both architectures and inform
future model and training designs for long-context reasoning.

</details>


### [39] [Aligning Large Language Models via Fully Self-Synthetic Data](https://arxiv.org/abs/2510.06652)
*Shangjian Yin,Zhepei Wei,Xinyu Zhu,Wei-Lin Chen,Yu Meng*

Main category: cs.CL

TL;DR: 提出了一种名为自对齐优化 (SAO) 的全自合成框架，用于大型语言模型 (LLM) 对齐，其中所有训练数据均由模型自身生成。


<details>
  <summary>Details</summary>
Motivation: 传统的人工反馈强化学习 (RLHF) 依赖于昂贵的人工标注数据集，而来自 AI 反馈的强化学习 (RLAIF) 也需要大量成本，需要收集各种提示和相应的响应，通常需要外部奖励模型或专有模型（如 GPT-4）来注释偏好对。

Method: SAO 首先指示 LLM 进行角色扮演并生成各种提示和响应，然后对这些提示和响应进行自我评估以进行偏好优化。

Result: SAO 有效地增强了模型在 AlpacaEval~2.0 等标准基准上的聊天能力，同时保持了在下游目标任务（例如，问答、数学推理）方面的强大性能。

Conclusion: SAO 为 LLM 对齐的自我改进提供了一种实用的解决方案。

Abstract: Traditional reinforcement learning from human feedback (RLHF) for large
language models (LLMs) relies on expensive human-annotated datasets, while
Reinforcement Learning from AI Feedback (RLAIF) also incurs significant costs,
requiring the collection of diverse prompts and corresponding responses, often
necessitating external reward models or proprietary models like GPT-4 to
annotate preference pairs. In this work, we introduce Self-Alignment
Optimization (SAO), a fully self-synthetic framework for LLM alignment, where
all training data, including prompts (i.e., user queries), responses, and
preferences, are generated by the model itself. Specifically, SAO first
instructs the LLM to engage in persona role-play and generate diverse prompts
and responses, which are then self-evaluated for preference optimization.
Extensive experiments demonstrate that SAO effectively enhances the model's
chat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining
strong performance on downstream objective tasks (e.g., question-answering,
math reasoning). Our work provides a practical solution for self-improvement in
aligning LLMs, and the code for reproducing our results is available at:
https://github.com/SJY8460/SAO.

</details>


### [40] [ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory](https://arxiv.org/abs/2510.06664)
*Yunzhong Xiao,Yangmin Li,Hewei Wang,Yunlong Tang,Zora Zhiruo Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为 ToolMem 的方法，该方法使 agents 能够通过总结不同工具的优缺点并将其存储在内存中，从而开发工具能力的记忆。在推理时，agent 可以从 ToolMem 中检索相关条目，并选择最佳工具来更准确地解决单个任务。


<details>
  <summary>Details</summary>
Motivation: 现有agents通常依赖于固定工具，限制了为特定任务选择最合适工具的灵活性。为了解决这个问题，本文旨在构建能够从与工具的先前交互中获益的agents。

Method: 本文提出了 ToolMem，它允许agents通过总结工具的优缺点并将其存储在内存中来开发工具能力的记忆。在推理时，agent 可以从 ToolMem 中检索相关条目，并选择最佳工具来更准确地解决单个任务。

Result: 与无记忆的通用agents相比，使用 ToolMem 的agents在文本和多模态生成场景中预测工具性能的准确率分别提高了 14.8% 和 28.7%。此外，ToolMem 在各种场景中分别实现了 21% 和 24% 的绝对增长，从而促进了多个选择中的最佳工具选择。

Conclusion: 本文提出了一种名为 ToolMem 的新方法，该方法使agents能够通过总结不同工具的优缺点并将其存储在内存中，从而开发工具能力的记忆。实验结果表明，ToolMem 可以提高agents预测工具性能的准确率，并促进最佳工具的选择。

Abstract: Agents utilizing tools powered by large language models (LLMs) or
vision-language models (VLMs) have demonstrated remarkable progress in diverse
tasks across text and visual modalities. Unlike traditional tools such as
calculators, which give deterministic outputs, neural tools perform uncertainly
across task scenarios. While different tools for a task may excel in varied
scenarios, existing agents typically rely on fixed tools, thus limiting the
flexibility in selecting the most suitable tool for specific tasks. In
contrast, humans snowball their understanding of the capabilities of different
tools by interacting with them, and apply this knowledge to select the optimal
tool when solving a future task. To build agents that similarly benefit from
this process, we propose ToolMem that enables agents to develop memories of
tool capabilities from previous interactions, by summarizing their strengths
and weaknesses and storing them in memory; at inference, the agent can retrieve
relevant entries from ToolMem, and select the best tool to solve individual
tasks more accurately. We evaluate ToolMem on learning varied text generation
and text-to-image generation neural tools. Compared to no-memory, generic
agents, we find ToolMem-augmented agents predict tool performance 14.8% and
28.7% more accurately across text and multimodal generation scenarios.
Moreover, ToolMem facilitates optimal tool selection among multiple choices by
21% and 24% absolute increases in respective scenarios.

</details>


### [41] [PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from Scratch](https://arxiv.org/abs/2510.06670)
*Shangjian Yin,Shining Liang,Wenbiao Ding,Yuli Qian,Zhouxing Shi,Hongzhi Li,Yutao Xie*

Main category: cs.CL

TL;DR: PiKa: A data-efficient alignment dataset family, requiring only 30k SFT examples, outperforming models trained on larger datasets.


<details>
  <summary>Details</summary>
Motivation: Current alignment datasets are either private, costly, or require large amounts of data, limiting reproducibility and scalability for academic and resource-limited communities.

Method: Introduce PiKa-SFT, a data-efficient alignment dataset with only 30k SFT examples. Fine-tune Llama-3-8B-Base and Qwen2.5 series (0.5B to 7B) on PiKa-SFT and evaluate the performance.

Result: PiKa-SFT outperforms models trained on much larger datasets, even surpassing the official Llama-3-8B-Instruct model. Qwen2.5 series also achieved consistent gains when trained on PiKa-SFT.

Conclusion: High-quality alignment can be achieved with significantly less data, offering a scalable path for open-source LLM alignment.

Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone
for aligning large language models (LLMs). However, its effectiveness depends
on high-quality instruction data. Most existing alignment datasets are either
private or require costly human annotation, which limits reproducibility and
scalability. Even with Reinforcement Learning from AI Feedback (RLAIF),
concerns about data quality remain. Moreover, it is unclear how much data is
actually required to fine-tune a base model into a strong instruction-following
model. Current approaches often rely on over 300k examples even at the
supervised fine-tuning (SFT) stage, yet they still underperform compared to
proprietary models, creating barriers for academic and resource-limited
communities. To address this gap, we introduce PiKa, a data-efficient family of
expert-level alignment datasets. In particular, the PiKa-SFT dataset uses only
30k SFT examples, far fewer than state-of-the-art datasets like Magpie. Through
evaluations by fine-tuning Llama-3-8B-Base on PiKa and other public datasets,
we show that PiKa-SFT outperforms models trained on much larger data. On
AlpacaEval 2.0 and Arena-Hard benchmarks, PiKa-SFT fine-tuning even surpasses
the official Llama-3-8B-Instruct model trained on over 10 million proprietary
examples. We further extend our study by training the Qwen2.5 series (0.5B to
7B) on PiKa-SFT, achieving consistent gains. These findings demonstrate that
high-quality alignment can be achieved with significantly less data, offering a
scalable path for open-source LLM alignment. Code and data:
https://github.com/SJY8460/PiKa.

</details>


### [42] [Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback](https://arxiv.org/abs/2510.06677)
*Yisha Wu,Cen,Zhao,Yuanpei Cao,Xiaoqing Su,Yashar Mehdad,Mindy Ji,Claire Na Cheng*

Main category: cs.CL

TL;DR: 本文介绍了一个为客户支持坐席设计的增量摘要系统，该系统能够在对话过程中智能地生成简洁的要点笔记。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在减少客服人员的上下文切换工作和冗余审查。

Method: 该方法结合了一个微调的 Mixtral-8x7B 模型，用于连续生成笔记，以及一个基于 DeBERTa 的分类器，用于过滤琐碎的内容。坐席的编辑会改进在线笔记的生成，并定期通知离线模型重新训练，从而形成坐席编辑反馈闭环。

Result: 在生产环境中部署后，与批量摘要相比，案件处理时间减少了 3%（在高度复杂的案件中减少高达 9%），同时坐席满意度调查的评分也很高。

Conclusion: 结果表明，具有持续反馈的增量摘要可以有效地提高摘要质量和大规模的坐席生产力。

Abstract: We introduce an incremental summarization system for customer support agents
that intelligently determines when to generate concise bullet notes during
conversations, reducing agents' context-switching effort and redundant review.
Our approach combines a fine-tuned Mixtral-8x7B model for continuous note
generation with a DeBERTa-based classifier to filter trivial content. Agent
edits refine the online notes generation and regularly inform offline model
retraining, closing the agent edits feedback loop. Deployed in production, our
system achieved a 3% reduction in case handling time compared to bulk
summarization (with reductions of up to 9% in highly complex cases), alongside
high agent satisfaction ratings from surveys. These results demonstrate that
incremental summarization with continuous feedback effectively enhances summary
quality and agent productivity at scale.

</details>


### [43] [Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks](https://arxiv.org/abs/2510.06695)
*Qinhao Zhou,Xiang Xiang,Kun He,John E. Hopcroft*

Main category: cs.CL

TL;DR: 本论文提出了一种新的prompt优化方法，专门针对机器翻译任务，使用back-translation-based策略训练的小参数模型，降低了单任务优化的训练开销，同时提供了高效的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的prompt工程方法主要集中于优化通用任务的instruction组件，通常需要大型参数LLM作为辅助工具。然而，这些方法对于像机器翻译这样的任务的适用性有限，在机器翻译中，input组件起着更关键的作用。

Method: 提出了一种新的prompt优化方法，专门针对机器翻译任务。该方法采用了一种基于back-translation-based策略训练的小参数模型，

Result: 该方法显著降低了单任务优化的训练开销，同时提供了高效的性能。

Conclusion: 通过一定的调整，该方法也可以扩展到其他下游任务。

Abstract: In recent years, the growing interest in Large Language Models (LLMs) has
significantly advanced prompt engineering, transitioning from manual design to
model-based optimization. Prompts for LLMs generally comprise two components:
the \textit{instruction}, which defines the task or objective, and the
\textit{input}, which is tailored to the instruction type. In natural language
generation (NLG) tasks such as machine translation, the \textit{input}
component is particularly critical, while the \textit{instruction} component
tends to be concise. Existing prompt engineering methods primarily focus on
optimizing the \textit{instruction} component for general tasks, often
requiring large-parameter LLMs as auxiliary tools. However, these approaches
exhibit limited applicability for tasks like machine translation, where the
\textit{input} component plays a more pivotal role. To address this limitation,
this paper introduces a novel prompt optimization method specifically designed
for machine translation tasks. The proposed approach employs a small-parameter
model trained using a back-translation-based strategy, significantly reducing
training overhead for single-task optimization while delivering highly
effective performance. With certain adaptations, this method can also be
extended to other downstream tasks.

</details>


### [44] [How Language Models Conflate Logical Validity with Plausibility: A Representational Analysis of Content Effects](https://arxiv.org/abs/2510.06700)
*Leonardo Bertolazzi,Sandro Pezzelle,Raffaelle Bernardi*

Main category: cs.CL

TL;DR: 大型语言模型和人类一样，都会受到内容效应的影响，即推理问题中语义内容的可信度会影响对其逻辑有效性的判断。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究大型语言模型中内容效应背后的机制，探讨模型如何在内部表征中编码有效性和可信度这两个概念。

Method: 通过研究大型语言模型的内部表征，分析有效性和可信度概念的线性表征和对齐情况，并使用 steering vectors 来验证可信度对有效性判断的因果影响。

Result: 研究表明，有效性和可信度在线性表征中强对齐，导致模型混淆两者。通过 steering vectors 可以发现，可信度向量会影响有效性判断，反之亦然。并且，这两个概念的对齐程度可以预测模型行为内容效应的大小。构建 debiasing vectors 可以解耦这两个概念，减少内容效应并提高推理准确性。

Conclusion: 研究结果加深了对大型语言模型中抽象逻辑概念表征方式的理解，并强调了表征干预是实现更具逻辑性的系统的一种途径。

Abstract: Both humans and large language models (LLMs) exhibit content effects: biases
in which the plausibility of the semantic content of a reasoning problem
influences judgments regarding its logical validity. While this phenomenon in
humans is best explained by the dual-process theory of reasoning, the
mechanisms behind content effects in LLMs remain unclear. In this work, we
address this issue by investigating how LLMs encode the concepts of validity
and plausibility within their internal representations. We show that both
concepts are linearly represented and strongly aligned in representational
geometry, leading models to conflate plausibility with validity. Using steering
vectors, we demonstrate that plausibility vectors can causally bias validity
judgements, and vice versa, and that the degree of alignment between these two
concepts predicts the magnitude of behavioral content effects across models.
Finally, we construct debiasing vectors that disentangle these concepts,
reducing content effects and improving reasoning accuracy. Our findings advance
understanding of how abstract logical concepts are represented in LLMs and
highlight representational interventions as a path toward more logical systems.

</details>


### [45] [Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management](https://arxiv.org/abs/2510.06727)
*Miao Lu,Weiwei Sun,Weihua Du,Zhan Ling,Xuesong Yao,Kang Liu,Jiecao Chen*

Main category: cs.CL

TL;DR: 本文提出了一种基于总结的上下文管理方法，用于训练能够进行长程多轮工具使用的LLM Agent。


<details>
  <summary>Details</summary>
Motivation: 现有的RL pipeline存在指令遵循能力下降、rollout成本过高以及严格的上下文限制等问题。

Method: 该方法通过LLM生成的总结定期压缩工具使用历史，以保持上下文的紧凑性，并使Agent能够扩展到固定的上下文窗口之外。同时，推导出一种策略梯度表示，可以无缝地实现标准LLM RL基础设施来优化工具使用行为和总结策略。

Result: 在交互式函数调用和搜索任务上的实验表明，SUPO显著提高了成功率，同时保持了相同甚至更低的工作上下文长度。对于复杂的搜索任务，当测试时的最大总结轮数超过训练时，SUPO可以进一步提高评估性能。

Conclusion: 基于总结的上下文管理是一种原则上可扩展的方法，用于训练超出固定上下文长度限制的RL Agent。

Abstract: We study reinforcement learning (RL) fine-tuning of large language model
(LLM) agents for long-horizon multi-turn tool use, where context length quickly
becomes a fundamental bottleneck. Existing RL pipelines can suffer from
degraded instruction following, excessive rollout costs, and most importantly,
strict context limits. To address these challenges, we introduce
summarization-based context management to training. In specific, it
periodically compresses the tool using history by LLM-generated summaries that
retain task-relevant information to keep a compact context while enabling the
agent to scale beyond the fixed context window. Building on this formulation,
we derive a policy gradient representation that seamlessly enables standard LLM
RL infrastructures to optimize both tool-use behaviors as well as summarization
strategies in an end-to-end fashion. We instantiate this framework with
\underline{SU}mmarization augmented \underline{P}olicy \underline{O}ptimization
(\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond
a fixed context limit. Experiments on interactive function calling and
searching tasks demonstrate that \texttt{SUPO} significantly improves the
success rate while maintaining the same or even lower working context length
compared to baselines. We also demonstrate that for complex searching tasks,
\texttt{SUPO} can further improve the evaluation performance when scaling
test-time maximum round of summarization beyond that of training time. Our
results establish summarization-based context management as a principled and
scalable approach for training RL agents beyond a fixed context length limit.

</details>


### [46] [PTEB: Towards Robust Text Embedding Evaluation via Stochastic Paraphrasing at Evaluation Time with LLMs](https://arxiv.org/abs/2510.06730)
*Manuel Frank,Haithem Afli*

Main category: cs.CL

TL;DR: 提出了一个动态的句子嵌入模型评估协议 PTEB，该协议在评估时随机生成释义，并在多次运行中聚合结果。


<details>
  <summary>Details</summary>
Motivation: 当前的句子嵌入模型评估依赖于静态测试平台，这可能会夸大报告的性能并掩盖现实世界的鲁棒性。

Method: 使用基于语义文本相似性黄金评级的、具有成本效益的 LLM 方法生成释义。

Result: 验证了句子编码器的性能对token空间的变化敏感，即使语义保持不变。较小的模型并没有受到相对于较大模型不成比例的影响。结果在多次运行中具有统计上的稳健性，并将实验扩展到涵盖 10 种语言的 3 个多语言数据集。

Conclusion: 提出了一个新的 NLP 评估范例，该范例减少了对静态、预定义的基准的依赖，而是转向利用评估时间计算的动态、随机评估。

Abstract: Current evaluations of sentence embedding models typically rely on static
test beds such as the Massive Text Embedding Benchmark (MTEB). While
invaluable, repeated tuning on a fixed suite can inflate reported performance
and obscure real-world robustness. We introduce the Paraphrasing Text Embedding
Benchmark (PTEB), a dynamic protocol that stochastically generates
meaning-preserving paraphrases at evaluation time and aggregates results across
multiple runs. Using a cost-efficient LLM-based method grounded in semantic
textual similarity gold ratings, we show that LLMs generate token-diverse but
semantically preserving, paraphrases. Across 7 MTEB tasks, we validate our
hypothesis that the performance of sentence encoders is sensitive to changes in
token space even when semantics remain fixed. We also observe that smaller
models are not disproportionately affected relative to larger ones. Our results
are statistically robust over multiple runs and we extended our experiments to
3 multilingual datasets covering 10 languages. More generally, we aim to
propose a new evaluation paradigm in NLP that relies less on static,
pre-defined benchmarks but shifts towards dynamic, stochastic evaluation
leveraging eval-time compute.

</details>


### [47] [AWM: Accurate Weight-Matrix Fingerprint for Large Language Models](https://arxiv.org/abs/2510.06738)
*Boyi Zeng,Lin Chen,Ziwei He,Xinbing Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: 提出了一种基于权重矩阵的免训练指纹识别方法，用于确定可疑LLM是否从头开始训练或衍生自现有基础模型。


<details>
  <summary>Details</summary>
Motivation: 保护大型语言模型（LLM）的知识产权至关重要，因为它们的训练需要大量资源。因此，模型所有者和第三方迫切需要确定可疑的LLM是从头开始训练还是从现有基础模型派生的。然而，模型通常经过的强化学习等密集的后训练过程对可靠识别构成了重大挑战。

Method: 利用线性分配问题（LAP）和无偏中心核对齐（CKA）相似性来消除参数操作的影响，从而产生高度鲁棒和高保真的相似性度量。

Result: 在包含60个阳性和90个阴性模型对的综合测试平台上，该方法在所有六个上述后训练类别中均表现出卓越的鲁棒性，同时表现出接近于零的假阳性风险。通过在所有分类指标上获得满分，该方法为可靠的模型谱系验证奠定了坚实的基础。

Conclusion: 该方法为可靠的模型谱系验证奠定了坚实的基础，并且在NVIDIA 3090 GPU上，整个计算在30秒内完成。

Abstract: Protecting the intellectual property of large language models (LLMs) is
crucial, given the substantial resources required for their training.
Consequently, there is an urgent need for both model owners and third parties
to determine whether a suspect LLM is trained from scratch or derived from an
existing base model. However, the intensive post-training processes that models
typically undergo-such as supervised fine-tuning, extensive continued
pretraining, reinforcement learning, multi-modal extension, pruning, and
upcycling-pose significant challenges to reliable identification. In this work,
we propose a training-free fingerprinting method based on weight matrices. We
leverage the Linear Assignment Problem (LAP) and an unbiased Centered Kernel
Alignment (CKA) similarity to neutralize the effects of parameter
manipulations, yielding a highly robust and high-fidelity similarity metric. On
a comprehensive testbed of 60 positive and 90 negative model pairs, our method
demonstrates exceptional robustness against all six aforementioned
post-training categories while exhibiting a near-zero risk of false positives.
By achieving perfect scores on all classification metrics, our approach
establishes a strong basis for reliable model lineage verification. Moreover,
the entire computation completes within 30s on an NVIDIA 3090 GPU. The code is
available at https://github.com/LUMIA-Group/AWM.

</details>


### [48] [TWIST: Training-free and Label-free Short Text Clustering through Iterative Vector Updating with LLMs](https://arxiv.org/abs/2510.06747)
*I-Fan Lin,Faegheh Hasibi,Suzan Verberne*

Main category: cs.CL

TL;DR: 提出了一种无需训练和标签的短文本聚类方法，可用于任何现有的嵌入器。


<details>
  <summary>Details</summary>
Motivation: 公司需要根据用户的意图对大量用户话语进行聚类，但通常没有可用的标记数据，并且聚类的数量是未知的。

Method: 该方法基于迭代向量更新：它构建基于代表性文本的稀疏向量，然后通过 LLM 指导迭代地细化它们。

Result: 该方法实现了与使用对比学习的最新方法相当或更好的结果，但没有假设集群或标签的先验知识。在不同的数据集和较小的 LLM 上的实验表明，该方法与模型无关，可以应用于任何嵌入器，具有相对较小的 LLM 和不同的聚类方法。该方法可以扩展到大型数据集，从而降低 LLM 的计算成本。

Conclusion: 这些低资源、适应性强的设置和该方法的可扩展性使其比现有的聚类方法更符合实际场景。

Abstract: In this paper, we propose a training-free and label-free method for short
text clustering that can be used on top of any existing embedder. In the
context of customer-facing chatbots, companies are dealing with large amounts
of user utterances that need to be clustered according to their intent. In
these commercial settings, no labeled data is typically available, and the
number of clusters is not known. Our method is based on iterative vector
updating: it constructs sparse vectors based on representative texts, and then
iteratively refines them through LLM guidance. Our method achieves comparable
or superior results to state-of-the-art methods that use contrastive learning,
but without assuming prior knowledge of clusters or labels. Experiments on
diverse datasets and smaller LLMs show that our method is model agnostic and
can be applied to any embedder, with relatively small LLMs, and different
clustering methods. We also show that our method scales to large datasets,
reducing the computational cost of the LLM. These low-resource, adaptable
settings and the scalability of our method make it more aligned with real-world
scenarios than existing clustering methods.

</details>


### [49] [A Formal Framework for Fluency-based Multi-Reference Evaluation in Grammatical Error Correction](https://arxiv.org/abs/2510.06749)
*Eitan Klinger,Zihao Huang,Tran Minh Nguyen,Emma Jayeon Park,Yige Chen,Yang Gu,Qingyu Gao,Siliang Liu,Mengyang Qiu,Jungyeul Park*

Main category: cs.CL

TL;DR: 提出了一个基于流畅度的多参考评估的正式框架，解决了现有语法纠错评估指标的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的评估框架主要基于编辑，以英语为中心，依赖于系统和参考编辑之间的严格对齐，限制了它们在多语言和生成环境中的适用性。

Method: 将n-gram相似性构建为一个在多个合理修正上进行的聚合问题。通过四种聚合策略实例化GLEU：select-best，simple-average，weighted-average和merged-counts，并分析它们的有界性、单调性和对参考变异的敏感性。

Result: 在捷克语、爱沙尼亚语、乌克兰语和汉语语料库上的实证结果表明，这些策略捕捉到了流畅性和覆盖率的互补方面。

Conclusion: 该框架将多参考评估统一为一个基于原则的、面向流畅性的方法，该方法结合了语言多样性，而不会惩罚合法的变异。

Abstract: Evaluating grammatical error correction requires metrics that reflect the
diversity of valid human corrections rather than privileging a single
reference. Existing frameworks, largely edit-based and English-centric, rely on
rigid alignments between system and reference edits, limiting their
applicability in multilingual and generative settings. This paper introduces a
formal framework for \textit{fluency-based multi-reference evaluation}, framing
$n$-gram similarity as an aggregation problem over multiple legitimate
corrections. Within this formulation, we instantiate GLEU through four
aggregation strategies--\textsc{select-best}, \textsc{simple-average},
\textsc{weighted-average}, and \textsc{merged-counts}--and analyze their
properties of boundedness, monotonicity, and sensitivity to reference
variation. Empirical results on Czech, Estonian, Ukrainian, and Chinese corpora
show that these strategies capture complementary aspects of fluency and
coverage. The framework unifies multi-reference evaluation into a principled,
fluency-oriented approach that incorporates linguistic diversity without
penalizing legitimate variation.

</details>


### [50] [Gold-Switch: Training-Free Superposition of Slow- and Fast- Thinking LLMs](https://arxiv.org/abs/2510.06750)
*Jaeseong Lee,Dayoung Kwon,seung-won hwang*

Main category: cs.CL

TL;DR: 提出了一种新的推理优化策略，通过选择性地从LRM中“unlearn”来减少计算量，同时保留推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRM）在结构化任务中表现出色，但常常过度思考，导致性能下降和资源浪费。部署多个模型成本高昂或不切实际。

Method: 通过分析奇异值的累积能量，确定最佳的低秩投影，以恰当地调整推理。

Result: 通过选择性地从LRM中“unlearn”，在推理时减少计算量，同时保留推理能力。

Conclusion: 提出了一种轻量级的、无需训练的调节方法，通过开关一个模型来优化推理。

Abstract: Large Reasoning Models (LRMs) excel in structured tasks by emulating
deliberate human reasoning but often suffer from overthinking, degrading
performance and wasting resources. One possible baseline is to deploy both LLM
and LRM, then route input by predicting whether it requires reasoning and may
cause overthinking. However, deploying multiple models can be costly or
impractical. We propose a superposed deployment strategy with a lightweight,
training-free regulation to optimize inference by switching one model on and
off. Instead of routing, we selectively unlearn from LRM at inference, scaling
down computation while preserving reasoning. By analyzing the cumulative energy
of singular values, we identify optimal low-rank projections to adjust
reasoning just right.

</details>


### [51] [Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition](https://arxiv.org/abs/2510.06774)
*Lei Xu,Pierre Beckmann,Marco Valentino,André Freitas*

Main category: cs.CL

TL;DR: 提出了一种自适应的神经符号推理框架，可以自动识别形式推理策略并动态选择和应用逻辑求解器。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号方法大多是静态的，无法利用多样化的形式推理策略。

Method: 该框架通过自动形式化接口动态选择和应用专门的形式逻辑求解器。

Result: 该框架在多个推理任务上优于现有基线，并且自适应推理可以积极影响纯LLM方法。

Conclusion: 该工作为自适应LLM符号推理奠定了基础，为统一异构推理挑战中的物质和形式推理提供了一条前进的道路。

Abstract: Neuro-symbolic NLP methods aim to leverage the complementary strengths of
large language models and formal logical solvers. However, current approaches
are mostly static in nature, i.e., the integration of a target solver is
predetermined at design time, hindering the ability to employ diverse formal
inference strategies. To address this, we introduce an adaptive,
multi-paradigm, neuro-symbolic inference framework that: (1) automatically
identifies formal reasoning strategies from problems expressed in natural
language; and (2) dynamically selects and applies specialized formal logical
solvers via autoformalization interfaces. Extensive experiments on individual
and multi-paradigm reasoning tasks support the following conclusions: LLMs are
effective at predicting the necessary formal reasoning strategies with an
accuracy above 90 percent. This enables flexible integration with formal
logical solvers, resulting in our framework outperforming competing baselines
by 27 percent and 6 percent compared to GPT-4o and DeepSeek-V3.1, respectively.
Moreover, adaptive reasoning can even positively impact pure LLM methods,
yielding gains of 10, 5, and 6 percent on zero-shot, CoT, and symbolic CoT
settings with GPT-4o. Finally, although smaller models struggle with adaptive
neuro-symbolic reasoning, post-training offers a viable path to improvement.
Overall, this work establishes the foundations for adaptive LLM-symbolic
reasoning, offering a path forward for unifying material and formal inferences
on heterogeneous reasoning challenges.

</details>


### [52] [Foundations of LLM Knowledge Materialization: Termination, Reproducibility, Robustness](https://arxiv.org/abs/2510.06780)
*Luca Giordano,Simon Razniewski*

Main category: cs.CL

TL;DR: 这篇论文研究了如何将大型语言模型(LLM)中的知识转化为结构化格式，例如通过递归提取方法(GPTKB)。


<details>
  <summary>Details</summary>
Motivation: 衡量和系统化LLM中编码的大量事实知识仍然具有挑战性。研究了知识提取方法是否可以终止，其输出是否可重现，以及它们对变化的鲁棒性。

Method: 使用miniGPTKBs（特定领域的，易于处理的子爬取）系统地研究LLM知识的物化，分析了三个类别的指标：产量、词汇相似性和语义相似性。使用了四种变体（种子，语言，随机性，模型）和三个示例领域（来自历史，娱乐和金融）。

Result: 研究结果表明：(i) 终止率高，但取决于模型；(ii) 可重复性混合；(iii) 鲁棒性随扰动类型而变化：种子和温度高，语言和模型低。

Conclusion: LLM知识物化可以可靠地呈现核心知识，同时也揭示了重要的局限性。

Abstract: Large Language Models (LLMs) encode substantial factual knowledge, yet
measuring and systematizing this knowledge remains challenging. Converting it
into structured format, for example through recursive extraction approaches
such as the GPTKB methodology (Hu et al., 2025b), is still underexplored. Key
open questions include whether such extraction can terminate, whether its
outputs are reproducible, and how robust they are to variations. We
systematically study LLM knowledge materialization using miniGPTKBs
(domain-specific, tractable subcrawls), analyzing termination, reproducibility,
and robustness across three categories of metrics: yield, lexical similarity,
and semantic similarity. We experiment with four variations (seed, language,
randomness, model) and three illustrative domains (from history, entertainment,
and finance). Our findings show (i) high termination rates, though
model-dependent; (ii) mixed reproducibility; and (iii) robustness that varies
by perturbation type: high for seeds and temperature, lower for languages and
models. These results suggest that LLM knowledge materialization can reliably
surface core knowledge, while also revealing important limitations.

</details>


### [53] [FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline](https://arxiv.org/abs/2510.06800)
*Haotian Wu,Shufan Jiang,Chios Chen,Yiyang Feng,Hehai Lin,Heqing Zou,Yao Shu,Yanran Li,Chengwei Qin*

Main category: cs.CL

TL;DR: 提出了FURINA-Builder，一个自动构建角色扮演（RP）基准的多智能体协作流程，以解决现有基准的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有角色扮演基准范围狭窄、交互模式过时，且在不同应用场景中的适应性有限。

Method: 通过模拟测试角色与其他角色的对话，并使用LLM裁判选择细粒度的评估维度，将测试角色的反应调整为最终的测试话语。

Result: 构建了一个新的综合性角色扮演基准FURINA-Bench，对领先的LLM进行了广泛评估，发现最佳模型在英语和中文RP任务中表现出色。推理能力会放大已建立角色和合成角色之间的差距，并且推理的LLM存在新的权衡：推理提高了RP性能，但同时也增加了RP幻觉。

Conclusion: FURINA-Builder有效，FURINA-Bench具有挑战性，并揭示了角色扮演性能和可靠性之间更广泛的帕累托前沿。

Abstract: As large language models (LLMs) advance in role-playing (RP) tasks, existing
benchmarks quickly become obsolete due to their narrow scope, outdated
interaction paradigms, and limited adaptability across diverse application
scenarios. To address this gap, we introduce FURINA-Builder, a novel
multi-agent collaboration pipeline that automatically constructs fully
customizable RP benchmarks at any scale. It enables evaluation of arbitrary
characters across diverse scenarios and prompt formats, as the first benchmark
builder in RP area for adaptable assessment. FURINA-Builder simulates dialogues
between a test character and other characters drawn from a well-constructed
character-scene pool, while an LLM judge selects fine-grained evaluation
dimensions and adjusts the test character's responses into final test
utterances. Using this pipeline, we build FURINA-Bench, a new comprehensive
role-playing benchmark featuring both established and synthesized test
characters, each assessed with dimension-specific evaluation criteria. Human
evaluation and preliminary separability analysis justify our pipeline and
benchmark design. We conduct extensive evaluations of cutting-edge LLMs and
find that o3 and DeepSeek-R1 achieve the best performance on English and
Chinese RP tasks, respectively. Across all models, established characters
consistently outperform synthesized ones, with reasoning capabilities further
amplifying this disparity. Interestingly, we observe that model scale does not
monotonically reduce hallucinations. More critically, for reasoning LLMs, we
uncover a novel trade-off: reasoning improves RP performance but simultaneously
increases RP hallucinations. This trade-off extends to a broader Pareto
frontier between RP performance and reliability for all LLMs. These findings
demonstrate the effectiveness of FURINA-Builder and the challenge posed by
FURINA-Bench.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [54] [Milestone Determination for Autonomous Railway Operation](https://arxiv.org/abs/2510.06229)
*Josh Hunter,John McDermid,Simon Burton,Poppy Fynes,Mia Dempster*

Main category: cs.CV

TL;DR: 传统铁路自动化计算机视觉系统受限于高质量时序数据的稀缺。本文提出了一种基于特定路线、关联上下文线索生成丰富时序数据集的方法，并利用里程碑确定简化学习过程，从而在可控环境中训练视觉代理，提高铁路自动化系统的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 高质量时序数据在铁路自动化计算机视觉系统中的稀缺性是研究动机。

Method: 通过关注特定路线、关联上下文的线索，生成丰富的时序数据集。利用里程碑确定，开发有针对性的、基于规则的模型，从而简化学习过程。

Result: 在可控环境中训练视觉代理，提高铁路自动化系统的安全性和效率。

Conclusion: 本文提出的方法为在可控、可预测的环境中训练视觉代理提供了一个实用的框架，从而促进更安全、更高效的铁路自动化机器学习系统。

Abstract: In the field of railway automation, one of the key challenges has been the
development of effective computer vision systems due to the limited
availability of high-quality, sequential data. Traditional datasets are
restricted in scope, lacking the spatio temporal context necessary for
real-time decision-making, while alternative solutions introduce issues related
to realism and applicability. By focusing on route-specific, contextually
relevant cues, we can generate rich, sequential datasets that align more
closely with real-world operational logic. The concept of milestone
determination allows for the development of targeted, rule-based models that
simplify the learning process by eliminating the need for generalized
recognition of dynamic components, focusing instead on the critical decision
points along a route. We argue that this approach provides a practical
framework for training vision agents in controlled, predictable environments,
facilitating safer and more efficient machine learning systems for railway
automation.

</details>


### [55] [CML-Bench: A Framework for Evaluating and Enhancing LLM-Powered Movie Scripts Generation](https://arxiv.org/abs/2510.06231)
*Mingzhe Zheng,Dingjie Song,Guanyu Zhou,Jun You,Jiahao Zhan,Xuran Ma,Xinyuan Song,Ser-Nam Lim,Qifeng Chen,Harry Yang*

Main category: cs.CV

TL;DR: LLMs在生成结构化文本方面表现出色，但在电影剧本的情感深度方面有所欠缺。为了解决这个问题，作者提出了CML-Dataset和CML-Bench，并设计了CML-Instruction提示策略来指导LLMs生成更高质量的剧本。


<details>
  <summary>Details</summary>
Motivation: LLMs难以捕捉电影剧本中的细致叙事和情感深度。

Method: 1. 创建CML-Dataset数据集，包含电影剧本片段和摘要；2. 确定了对话连贯性(DC)、角色一致性(CC)和情节合理性(PR)三个质量评估维度；3. 提出了CML-Bench，用于评估剧本质量；4. 设计了CML-Instruction提示策略，指导LLMs生成更结构化和电影化的剧本。

Result: CML-Bench能有效区分高质量的人工剧本和LLMs生成的剧本的弱点。CML-Instruction能指导LLMs生成更高质量的剧本，结果更符合人类偏好。

Conclusion: 论文验证了CML-Bench的有效性，并证明了CML-Instruction能提升LLMs生成剧本的质量。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in
generating highly structured texts. However, while exhibiting a high degree of
structural organization, movie scripts demand an additional layer of nuanced
storytelling and emotional depth-the 'soul' of compelling cinema-that LLMs
often fail to capture. To investigate this deficiency, we first curated
CML-Dataset, a dataset comprising (summary, content) pairs for Cinematic Markup
Language (CML), where 'content' consists of segments from esteemed,
high-quality movie scripts and 'summary' is a concise description of the
content. Through an in-depth analysis of the intrinsic multi-shot continuity
and narrative structures within these authentic scripts, we identified three
pivotal dimensions for quality assessment: Dialogue Coherence (DC), Character
Consistency (CC), and Plot Reasonableness (PR). Informed by these findings, we
propose the CML-Bench, featuring quantitative metrics across these dimensions.
CML-Bench effectively assigns high scores to well-crafted, human-written
scripts while concurrently pinpointing the weaknesses in screenplays generated
by LLMs. To further validate our benchmark, we introduce CML-Instruction, a
prompting strategy with detailed instructions on character dialogue and event
logic, to guide LLMs to generate more structured and cinematically sound
scripts. Extensive experiments validate the effectiveness of our benchmark and
demonstrate that LLMs guided by CML-Instruction generate higher-quality
screenplays, with results aligned with human preferences.

</details>


### [56] [User to Video: A Model for Spammer Detection Inspired by Video Classification Technology](https://arxiv.org/abs/2510.06233)
*Haoyang Zhang,Zhou Yang,Yucai Pang*

Main category: cs.CV

TL;DR: 提出了一种基于用户视频化的垃圾信息检测模型（UVSD），该模型将用户行为子空间视为帧图像，连续的帧图像视为视频，然后利用视频分类算法来识别垃圾信息发送者。


<details>
  <summary>Details</summary>
Motivation: 受视频分类技术的启发，将用户行为子空间视为帧图像，连续帧图像视为视频，从而进行垃圾信息检测。

Method: 1) 提出了用户像素化算法（user2piexl），将用户视为像素，立场量化为像素的RGB值。2) 提出了行为图像化算法（behavior2image），使用表示学习对子空间用户关系进行低秩稠密向量化，并引入切割和扩散算法完成帧图像化。3) 基于时间特征构建用户行为视频，并结合视频分类算法识别垃圾信息发送者。

Result: 在WEIBO和TWITTER公开数据集上的实验表明，UVSD模型优于现有方法。

Conclusion: 该论文提出了一种新颖的基于用户视频化的垃圾信息检测模型，并在实验中验证了其有效性。

Abstract: This article is inspired by video classification technology. If the user
behavior subspace is viewed as a frame image, consecutive frame images are
viewed as a video. Following this novel idea, a model for spammer detection
based on user videoization, called UVSD, is proposed. Firstly, a user2piexl
algorithm for user pixelization is proposed. Considering the adversarial
behavior of user stances, the user is viewed as a pixel, and the stance is
quantified as the pixel's RGB. Secondly, a behavior2image algorithm is proposed
for transforming user behavior subspace into frame images. Low-rank dense
vectorization of subspace user relations is performed using representation
learning, while cutting and diffusion algorithms are introduced to complete the
frame imageization. Finally, user behavior videos are constructed based on
temporal features. Subsequently, a video classification algorithm is combined
to identify the spammers. Experiments using publicly available datasets, i.e.,
WEIBO and TWITTER, show an advantage of the UVSD model over state-of-the-art
methods.

</details>


### [57] [Uncertainty Quantification In Surface Landmines and UXO Classification Using MC Dropout](https://arxiv.org/abs/2510.06238)
*Sagar Lekhak,Emmett J. Ientilucci,Dimah Dera,Susmita Ghosh*

Main category: cs.CV

TL;DR: 本研究探讨了使用 Monte Carlo (MC) Dropout 结合 ResNet-50 架构进行表面地雷和未爆弹药 (UXO) 分类，以量化不确定性并提高预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法在地雷检测中容易受到噪声和对抗攻击的影响，导致漏检或误分类。

Method: 将 MC Dropout 集成到微调的 ResNet-50 架构中，用于表面地雷和 UXO 分类，并在模拟数据集上进行测试。

Result: 实验结果表明，该模型能够标记具有挑战性条件下不可靠的预测。

Conclusion: 该研究强调了地雷清除中不确定性量化的必要性，并强调了开发更鲁棒和可靠模型的重要性。

Abstract: Detecting surface landmines and unexploded ordnances (UXOs) using deep
learning has shown promise in humanitarian demining. However, deterministic
neural networks can be vulnerable to noisy conditions and adversarial attacks,
leading to missed detection or misclassification. This study introduces the
idea of uncertainty quantification through Monte Carlo (MC) Dropout, integrated
into a fine-tuned ResNet-50 architecture for surface landmine and UXO
classification, which was tested on a simulated dataset. Integrating the MC
Dropout approach helps quantify epistemic uncertainty, providing an additional
metric for prediction reliability, which could be helpful to make more informed
decisions in demining operations. Experimental results on clean, adversarially
perturbed, and noisy test images demonstrate the model's ability to flag
unreliable predictions under challenging conditions. This proof-of-concept
study highlights the need for uncertainty quantification in demining, raises
awareness about the vulnerability of existing neural networks in demining to
adversarial threats, and emphasizes the importance of developing more robust
and reliable models for practical applications.

</details>


### [58] [multimodars: A Rust-powered toolkit for multi-modality cardiac image fusion and registration](https://arxiv.org/abs/2510.06241)
*Anselm W. Stark,Marc Ilic,Ali Mokhtari,Pooya Mohammadi Kazaj,Christoph Graeni,Isaac Shiri*

Main category: cs.CV

TL;DR: multimodars is an open, flexible toolkit for multi-state coronary artery analysis using intravascular imaging and CCTA fusion.


<details>
  <summary>Details</summary>
Motivation: Combining intravascular imaging and CCTA is critical for reliable 3D coronary models, but existing solutions lack open, flexible toolkits for multi-state analysis.

Method: Deterministic alignment algorithms, a compact NumPy-centred data model, and an optimised Rust backend.

Result: The package accepts CSV/NumPy inputs including data formats produced by the AIVUS-CAA software.

Conclusion: multimodars addresses the gap in multi-state coronary artery analysis by offering deterministic behavior, high performance, and easy pipeline integration.

Abstract: Combining complementary imaging modalities is critical to build reliable 3D
coronary models: intravascular imaging gives sub-millimetre resolution but
limited whole-vessel context, while CCTA supplies 3D geometry but suffers from
limited spatial resolution and artefacts (e.g., blooming). Prior work
demonstrated intravascular/CCTA fusion, yet no open, flexible toolkit is
tailored for multi-state analysis (rest/stress, pre-/post-stenting) while
offering deterministic behaviour, high performance, and easy pipeline
integration. multimodars addresses this gap with deterministic alignment
algorithms, a compact NumPy-centred data model, and an optimised Rust backend
suitable for scalable, reproducible experiments. The package accepts CSV/NumPy
inputs including data formats produced by the AIVUS-CAA software

</details>


### [59] [Does Physics Knowledge Emerge in Frontier Models?](https://arxiv.org/abs/2510.06251)
*Ieva Bagdonaviciute,Vibhav Vineet*

Main category: cs.CV

TL;DR: 评估前沿视觉语言模型在物理动态理解和预测方面的能力，发现其在感知和物理推理方面存在割裂。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在视觉感知和通用推理方面表现出色，但其理解和预测物理动态的能力尚不清楚。

Method: 在三个物理模拟数据集上对六个前沿视觉语言模型进行基准测试，并设计诊断子测试来隔离感知和物理推理。

Result: 模型在感知或物理推理方面的优势并不一定能转化为预测或反事实评估方面的更好表现，两者之间相关性较弱。

Conclusion: 当前视觉语言模型的感知和物理技能仍然是割裂的，未能结合成因果理解，需要更紧密地结合感知和推理的架构。

Abstract: Leading Vision-Language Models (VLMs) show strong results in visual
perception and general reasoning, but their ability to understand and predict
physical dynamics remains unclear. We benchmark six frontier VLMs on three
physical simulation datasets - CLEVRER, Physion, and Physion++ - where the
evaluation tasks test whether a model can predict outcomes or hypothesize about
alternative situations. To probe deeper, we design diagnostic subtests that
isolate perception (objects, colors, occluders) from physics reasoning (motion
prediction, spatial relations). Intuitively, stronger diagnostic performance
should support higher evaluation accuracy. Yet our analysis reveals weak
correlations: models that excel at perception or physics reasoning do not
consistently perform better on predictive or counterfactual evaluation. This
counterintuitive gap exposes a central limitation of current VLMs: perceptual
and physics skills remain fragmented and fail to combine into causal
understanding, underscoring the need for architectures that bind perception and
reasoning more tightly.

</details>


### [60] [Enhanced Self-Distillation Framework for Efficient Spiking Neural Network Training](https://arxiv.org/abs/2510.06254)
*Xiaochen Zhao,Chengting Yu,Kairong Yu,Lei Liu,Aili Wang*

Main category: cs.CV

TL;DR: 提出了一种增强的自蒸馏框架，以在有限的计算资源下实现高性能SNN训练。


<details>
  <summary>Details</summary>
Motivation: 传统的基于替代梯度和BPTT的SNN训练方法不仅在性能上落后于ANN，而且还会产生显著的计算和内存开销，这些开销随时间维度线性增长。

Method: 将中间SNN层的激发率投影到轻量级ANN分支上，并使用模型本身生成的高质量知识，通过ANN途径优化子结构。将教师信号解耦为可靠和不可靠的组成部分，确保只有可靠的知识被用来指导模型的优化。

Result: 在CIFAR-10、CIFAR-100、CIFAR10-DVS和ImageNet上的大量实验表明，该方法降低了训练复杂性，同时实现了高性能的SNN训练。

Conclusion: 该方法能够在有限的计算资源下实现高性能SNN训练。

Abstract: Spiking Neural Networks (SNNs) exhibit exceptional energy efficiency on
neuromorphic hardware due to their sparse activation patterns. However,
conventional training methods based on surrogate gradients and Backpropagation
Through Time (BPTT) not only lag behind Artificial Neural Networks (ANNs) in
performance, but also incur significant computational and memory overheads that
grow linearly with the temporal dimension. To enable high-performance SNN
training under limited computational resources, we propose an enhanced
self-distillation framework, jointly optimized with rate-based backpropagation.
Specifically, the firing rates of intermediate SNN layers are projected onto
lightweight ANN branches, and high-quality knowledge generated by the model
itself is used to optimize substructures through the ANN pathways. Unlike
traditional self-distillation paradigms, we observe that low-quality
self-generated knowledge may hinder convergence. To address this, we decouple
the teacher signal into reliable and unreliable components, ensuring that only
reliable knowledge is used to guide the optimization of the model. Extensive
experiments on CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet demonstrate that
our method reduces training complexity while achieving high-performance SNN
training. Our code is available at
https://github.com/Intelli-Chip-Lab/enhanced-self-distillation-framework-for-snn.

</details>


### [61] [Ensemble Deep Learning and LLM-Assisted Reporting for Automated Skin Lesion Diagnosis](https://arxiv.org/abs/2510.06260)
*Sher Khan,Raz Muhammad,Adil Hussain,Muhammad Sajjad,Muhammad Rashid*

Main category: cs.CV

TL;DR: 本研究提出了一个统一的框架，通过异构卷积神经网络集成和大型语言模型，改进皮肤科恶性肿瘤的早期检测。


<details>
  <summary>Details</summary>
Motivation: 现有皮肤科人工智能系统受限于同构架构、数据集偏差和NLP与临床决策分离。

Method: 构建异构卷积神经网络集成，并嵌入大型语言模型到诊断工作流程中。

Result: 生成包含精确病灶特征、诊断推理和可行监测指导的结构化报告。

Conclusion: 该框架在提高诊断精度的同时，支持从初步检测到患者教育的连续护理，从而提高皮肤病变的早期干预率。

Abstract: Cutaneous malignancies demand early detection for favorable outcomes, yet
current diagnostics suffer from inter-observer variability and access
disparities. While AI shows promise, existing dermatological systems are
limited by homogeneous architectures, dataset biases across skin tones, and
fragmented approaches that treat natural language processing as separate
post-hoc explanations rather than integral to clinical decision-making. We
introduce a unified framework that fundamentally reimagines AI integration for
dermatological diagnostics through two synergistic innovations. First, a
purposefully heterogeneous ensemble of architecturally diverse convolutional
neural networks provides complementary diagnostic perspectives, with an
intrinsic uncertainty mechanism flagging discordant cases for specialist review
-- mimicking clinical best practices. Second, we embed large language model
capabilities directly into the diagnostic workflow, transforming classification
outputs into clinically meaningful assessments that simultaneously fulfill
medical documentation requirements and deliver patient-centered education. This
seamless integration generates structured reports featuring precise lesion
characterization, accessible diagnostic reasoning, and actionable monitoring
guidance -- empowering patients to recognize early warning signs between
visits. By addressing both diagnostic reliability and communication barriers
within a single cohesive system, our approach bridges the critical
translational gap that has prevented previous AI implementations from achieving
clinical impact. The framework represents a significant advancement toward
deployable dermatological AI that enhances diagnostic precision while actively
supporting the continuum of care from initial detection through patient
education, ultimately improving early intervention rates for skin lesions.

</details>


### [62] [Vision Transformer for Transient Noise Classification](https://arxiv.org/abs/2510.06273)
*Divyansh Srivastava,Andrzej Niedzielski*

Main category: cs.CV

TL;DR: 本研究使用Vision Transformer模型对LIGO数据中的瞬态噪声进行分类，以提高引力波的探测精度。


<details>
  <summary>Details</summary>
Motivation: LIGO数据中的瞬态噪声会阻碍引力波的探测，需要对其进行有效分类。

Method: 使用Vision Transformer (ViT) 模型，在包含Gravity Spy数据集和LIGO O3a运行中新增的两个噪声类别的数据集上进行训练。

Result: 分类效率达到92.26%，表明Vision Transformer在区分瞬态噪声方面具有潜力。

Conclusion: Vision Transformer可以有效地区分瞬态噪声，从而提高引力波的探测精度。

Abstract: Transient noise (glitches) in LIGO data hinders the detection of
gravitational waves (GW). The Gravity Spy project has categorized these noise
events into various classes. With the O3 run, there is the inclusion of two
additional noise classes and thus a need to train new models for effective
classification. We aim to classify glitches in LIGO data into 22 existing
classes from the first run plus 2 additional noise classes from O3a using the
Vision Transformer (ViT) model. We train a pre-trained Vision Transformer
(ViT-B/32) model on a combined dataset consisting of the Gravity Spy dataset
with the additional two classes from the LIGO O3a run. We achieve a
classification efficiency of 92.26%, demonstrating the potential of Vision
Transformer to improve the accuracy of gravitational wave detection by
effectively distinguishing transient noise.
  Key words: gravitational waves --vision transformer --machine learning

</details>


### [63] [General and Efficient Visual Goal-Conditioned Reinforcement Learning using Object-Agnostic Masks](https://arxiv.org/abs/2510.06277)
*Fahim Shahriar,Cheryl Wang,Alireza Azimi,Gautham Vasan,Hany Hamed Elanwar,A. Rupam Mahmood,Colin Bellinger*

Main category: cs.CV

TL;DR: 本文提出了一种基于掩码的目标表示系统，该系统为智能体提供与对象无关的视觉线索，从而实现高效学习和卓越的泛化。


<details>
  <summary>Details</summary>
Motivation: 现有目标表示方法（如目标状态图像、3D 坐标和 one-hot 向量）存在泛化能力差、收敛速度慢以及需要专用摄像头等问题。

Method: 该方法使用掩码生成密集奖励，无需容易出错的距离计算。在模拟中使用 ground truth 掩码进行学习，在训练和未见过的测试对象上实现了 99.9% 的到达精度。

Result: 该方法无需目标位置信息即可高精度地执行拾取任务。此外，还展示了使用两个不同的物理机器人从头开始学习和 sim-to-real 迁移应用，利用预训练的开放词汇对象检测模型生成掩码。

Conclusion: 基于掩码的目标表示系统可以提高 GCRL 的效率和泛化能力，并且可以应用于实际机器人任务。

Abstract: Goal-conditioned reinforcement learning (GCRL) allows agents to learn diverse
objectives using a unified policy. The success of GCRL, however, is contingent
on the choice of goal representation. In this work, we propose a mask-based
goal representation system that provides object-agnostic visual cues to the
agent, enabling efficient learning and superior generalization. In contrast,
existing goal representation methods, such as target state images, 3D
coordinates, and one-hot vectors, face issues of poor generalization to unseen
objects, slow convergence, and the need for special cameras. Masks can be
processed to generate dense rewards without requiring error-prone distance
calculations. Learning with ground truth masks in simulation, we achieved 99.9%
reaching accuracy on training and unseen test objects. Our proposed method can
be utilized to perform pick-up tasks with high accuracy, without using any
positional information of the target. Moreover, we demonstrate learning from
scratch and sim-to-real transfer applications using two different physical
robots, utilizing pretrained open vocabulary object detection models for mask
generation.

</details>


### [64] [Improving the Spatial Resolution of GONG Solar Images to GST Quality Using Deep Learning](https://arxiv.org/abs/2510.06281)
*Chenyang Li,Qin Li,Haimin Wang,Bo Shen*

Main category: cs.CV

TL;DR: 本研究提出了一种基于GAN的超分辨率方法，可以将低分辨率的全日面Hα图像增强到与高分辨率观测相当的质量。


<details>
  <summary>Details</summary>
Motivation: 全日面Hα图像的空间分辨率有限，不足以分辨小尺度结构，而高分辨率的太阳成像对于捕捉精细尺度的动态特征至关重要。

Method: 采用具有残差密集块和相对论判别器的Real-ESRGAN。

Result: 该模型有效地恢复了太阳黑子半影内的精细细节，并解决了细丝和纤维中的精细细节，平均均方误差（MSE）为467.15，均方根误差（RMSE）为21.59，互相关（CC）为0.7794。

Conclusion: 图像对之间的轻微错位限制了定量性能，我们计划在未来的工作中解决这个问题，同时扩大数据集以进一步提高重建质量。

Abstract: High-resolution (HR) solar imaging is crucial for capturing fine-scale
dynamic features such as filaments and fibrils. However, the spatial resolution
of the full-disk H$\alpha$ images is limited and insufficient to resolve these
small-scale structures. To address this, we propose a GAN-based superresolution
approach to enhance low-resolution (LR) full-disk H$\alpha$ images from the
Global Oscillation Network Group (GONG) to a quality comparable with HR
observations from the Big Bear Solar Observatory/Goode Solar Telescope
(BBSO/GST). We employ Real-ESRGAN with Residual-in-Residual Dense Blocks and a
relativistic discriminator. We carefully aligned GONG-GST pairs. The model
effectively recovers fine details within sunspot penumbrae and resolves fine
details in filaments and fibrils, achieving an average mean squared error (MSE)
of 467.15, root mean squared error (RMSE) of 21.59, and cross-correlation (CC)
of 0.7794. Slight misalignments between image pairs limit quantitative
performance, which we plan to address in future work alongside dataset
expansion to further improve reconstruction quality.

</details>


### [65] [ChainMPQ: Interleaved Text-Image Reasoning Chains for Mitigating Relation Hallucinations](https://arxiv.org/abs/2510.06292)
*Yike Wu,Yiwei Wang,Yujun Cai*

Main category: cs.CV

TL;DR: 本文提出了一种名为 ChainMPQ 的无需训练的方法，旨在减少大型视觉语言模型 (LVLM) 中的关系幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型 (LVLM) 在多模态任务中表现出色，但幻觉问题仍然存在，尤其以关系幻觉最为突出，但受到的关注最少。

Method: ChainMPQ 首先从问题中提取主语和宾语关键词，以增强相应的图像区域。然后，它构建多角度问题，关注关系的三要素：主语、宾语以及连接它们的联系。这些问题被依次输入模型，早期步骤的文本和视觉记忆为后续步骤提供支持上下文，从而形成图像和文本交错的链条，引导渐进的关系推理。

Result: 在多个 LVLM 和基准测试上的实验表明，ChainMPQ 大幅减少了关系幻觉。

Conclusion: 消融研究进一步验证了 ChainMPQ 三个核心模块的有效性。

Abstract: While Large Vision-Language Models (LVLMs) achieve strong performance in
multimodal tasks, hallucinations continue to hinder their reliability. Among
the three categories of hallucinations, which include object, attribute, and
relation, relation hallucinations account for the largest proportion but have
received the least attention. To address this issue, we propose ChainMPQ
(Multi-Perspective Questions guided Interleaved Chain of Image and Text), a
training-free method that improves relational inference in LVLMs by utilizing
accumulated textual and visual memories. ChainMPQ first extracts subject and
object keywords from the question to enhance the corresponding image regions.
It then constructs multi-perspective questions that focus on the three core
components of a relationship: the subject, the object, and the relation that
links them. These questions are sequentially input to the model, with textual
and visual memories from earlier steps providing supporting context for
subsequent ones, thereby forming an interleaved chain of images and text that
guides progressive relational reasoning. Experiments on multiple LVLMs and
benchmarks show that ChainMPQ substantially reduces relation hallucinations,
while ablation studies further validate the effectiveness of its three core
modules.

</details>


### [66] [Efficient High-Resolution Image Editing with Hallucination-Aware Loss and Adaptive Tiling](https://arxiv.org/abs/2510.06295)
*Young D. Kwon,Abhinav Mehrotra,Malcolm Chadwick,Alberto Gil Ramos,Sourav Bhattacharya*

Main category: cs.CV

TL;DR: MobilePicasso是一个用于在移动设备上高效进行高分辨率图像编辑的新系统，它在计算成本和内存使用方面都进行了优化。


<details>
  <summary>Details</summary>
Motivation: 现有的图像编辑扩散模型在资源受限的设备上部署时，在高分辨率下会面临内存和图像质量方面的挑战。

Method: MobilePicasso包含三个阶段：（i）使用感知幻觉损失在标准分辨率下执行图像编辑；（ii）应用潜在投影以避免进入像素空间；（iii）使用自适应上下文保持平铺将编辑后的图像潜在地上采样到更高的分辨率。

Result: 用户研究表明，MobilePicasso不仅将图像质量提高了18-48％，而且与现有方法相比，幻觉减少了14-51％。MobilePicasso的延迟显着降低，例如，速度提高了高达55.8倍，而运行时内存仅略有增加，例如，比先前的工作仅增加了9％。令人惊讶的是，MobilePicasso的设备上运行时间比在A100 GPU上运行的基于服务器的高分辨率图像编辑模型更快。

Conclusion: MobilePicasso能够在移动设备上高效地进行高分辨率图像编辑，同时最大限度地降低计算成本和内存使用。

Abstract: High-resolution (4K) image-to-image synthesis has become increasingly
important for mobile applications. Existing diffusion models for image editing
face significant challenges, in terms of memory and image quality, when
deployed on resource-constrained devices. In this paper, we present
MobilePicasso, a novel system that enables efficient image editing at high
resolutions, while minimising computational cost and memory usage.
MobilePicasso comprises three stages: (i) performing image editing at a
standard resolution with hallucination-aware loss, (ii) applying latent
projection to overcome going to the pixel space, and (iii) upscaling the edited
image latent to a higher resolution with adaptive context-preserving tiling.
Our user study with 46 participants reveals that MobilePicasso not only
improves image quality by 18-48% but reduces hallucinations by 14-51% over
existing methods. MobilePicasso demonstrates significantly lower latency, e.g.,
up to 55.8$\times$ speed-up, yet with a small increase in runtime memory, e.g.,
a mere 9% increase over prior work. Surprisingly, the on-device runtime of
MobilePicasso is observed to be faster than a server-based high-resolution
image editing model running on an A100 GPU.

</details>


### [67] [RGBD Gaze Tracking Using Transformer for Feature Fusion](https://arxiv.org/abs/2510.06298)
*Tobias J. Bauer*

Main category: cs.CV

TL;DR: 本研究实现了基于RGBD图像和Transformer架构的注视追踪系统，并创建了一个新的数据集。


<details>
  <summary>Details</summary>
Motivation: 结合RGBD图像和Transformer架构的方法尚未被研究；现有数据集不包含深度信息或不适用于注视角度估计任务。

Method: 使用Transformer架构融合从RGBD图像中提取的特征，并使用GAN去除深度图伪影和提取头部姿势特征。

Result: 在ShanghaiTechGaze+数据集上，使用Transformer模块的模型平均欧几里得误差为55.3mm，不使用预训练GAN模块误差为30.1mm，替换为MLP后误差降至26.9mm。在ETH-XGaze数据集上，使用Transformer模块的模型平均角度误差为3.59度，不使用Transformer模块为3.26度。

Conclusion: 通过实验验证了不同模型配置在三个数据集上的性能，并表明使用MLP替代Transformer模块可以提高注视追踪的准确性。

Abstract: Subject of this thesis is the implementation of an AI-based Gaze Tracking
system using RGBD images that contain both color (RGB) and depth (D)
information. To fuse the features extracted from the images, a module based on
the Transformer architecture is used. The combination of RGBD input images and
Transformers was chosen because it has not yet been investigated. Furthermore,
a new dataset is created for training the AI models as existing datasets either
do not contain depth information or only contain labels for Gaze Point
Estimation that are not suitable for the task of Gaze Angle Estimation. Various
model configurations are trained, validated and evaluated on a total of three
different datasets. The trained models are then to be used in a real-time
pipeline to estimate the gaze direction and thus the gaze point of a person in
front of a computer screen. The AI model architecture used in this thesis is
based on an earlier work by Lian et al. It uses a Generative Adversarial
Network (GAN) to simultaneously remove depth map artifacts and extract head
pose features. Lian et al. achieve a mean Euclidean error of 38.7mm on their
own dataset ShanghaiTechGaze+. In this thesis, a model architecture with a
Transformer module for feature fusion achieves a mean Euclidean error of 55.3mm
on the same dataset, but we show that using no pre-trained GAN module leads to
a mean Euclidean error of 30.1mm. Replacing the Transformer module with a
Multilayer Perceptron (MLP) improves the error to 26.9mm. These results are
coherent with the ones on the other two datasets. On the ETH-XGaze dataset, the
model with Transformer module achieves a mean angular error of 3.59{\deg} and
without Transformer module 3.26{\deg}, whereas the fundamentally different
model architecture used by the dataset authors Zhang et al. achieves a mean
angular error of 2.04{\deg}. On the OTH-Gaze-Estimation dataset created for...

</details>


### [68] [Scalable deep fusion of spaceborne lidar and synthetic aperture radar for global forest structural complexity mapping](https://arxiv.org/abs/2510.06299)
*Tiago de Conto,John Armston,Ralph Dubayah*

Main category: cs.CV

TL;DR: 本研究利用GEDI激光雷达数据结合多模态SAR数据，通过深度学习框架生成了全球高分辨率（25米）的森林结构复杂度地图。


<details>
  <summary>Details</summary>
Motivation: 现有的GEDI数据虽然可以mapping温带和热带森林的结构复杂度，但是其稀疏的采样限制了连续的高分辨率mapping。

Method: 本研究使用改进的EfficientNetV2架构，使用超过1.3亿个GEDI footprints进行训练。

Result: 该模型实现了高性能（全球R2 = 0.82），参数少于400,000个，并且可以生成2015年至2022年的全球多时相森林结构复杂度数据集。

Conclusion: 该方法支持对全球森林结构动态进行连续的多时相监测，并为生物多样性保护和气候变化中的生态系统管理工作提供工具。

Abstract: Forest structural complexity metrics integrate multiple canopy attributes
into a single value that reflects habitat quality and ecosystem function.
Spaceborne lidar from the Global Ecosystem Dynamics Investigation (GEDI) has
enabled mapping of structural complexity in temperate and tropical forests, but
its sparse sampling limits continuous high-resolution mapping. We present a
scalable, deep learning framework fusing GEDI observations with multimodal
Synthetic Aperture Radar (SAR) datasets to produce global, high-resolution (25
m) wall-to-wall maps of forest structural complexity. Our adapted
EfficientNetV2 architecture, trained on over 130 million GEDI footprints,
achieves high performance (global R2 = 0.82) with fewer than 400,000
parameters, making it an accessible tool that enables researchers to process
datasets at any scale without requiring specialized computing infrastructure.
The model produces accurate predictions with calibrated uncertainty estimates
across biomes and time periods, preserving fine-scale spatial patterns. It has
been used to generate a global, multi-temporal dataset of forest structural
complexity from 2015 to 2022. Through transfer learning, this framework can be
extended to predict additional forest structural variables with minimal
computational cost. This approach supports continuous, multi-temporal
monitoring of global forest structural dynamics and provides tools for
biodiversity conservation and ecosystem management efforts in a changing
climate.

</details>


### [69] [Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding](https://arxiv.org/abs/2510.06308)
*Yi Xin,Qi Qin,Siqi Luo,Kaiwen Zhu,Juncheng Yan,Yan Tai,Jiayi Lei,Yuewen Cao,Keqi Wang,Yibin Wang,Jinbin Bai,Qian Yu,Dengyang Jiang,Yuandong Pu,Haoxing Chen,Le Zhuo,Junjun He,Gen Luo,Tianbin Li,Ming Hu,Jin Ye,Shenglong Ye,Bo Zhang,Chang Xu,Wenhai Wang,Hongsheng Li,Guangtao Zhai,Tianfan Xue,Bin Fu,Xiaohong Liu,Yu Qiao,Yihao Liu*

Main category: cs.CV

TL;DR: Lumina-DiMOO是一个开源多模态生成和理解的基础模型。


<details>
  <summary>Details</summary>
Motivation: Lumina-DiMOO旨在通过利用完全离散的扩散建模来处理跨各种模态的输入和输出，从而区别于先前的统一模型。

Method: Lumina-DiMOO采用完全离散的扩散建模。

Result: Lumina-DiMOO在多个基准测试中实现了最先进的性能，超过了现有的开源统一多模态模型。

Conclusion: Lumina-DiMOO的代码和检查点已发布，以促进多模态和离散扩散模型研究的进一步发展。

Abstract: We introduce Lumina-DiMOO, an open-source foundational model for seamless
multi-modal generation and understanding. Lumina-DiMOO sets itself apart from
prior unified models by utilizing a fully discrete diffusion modeling to handle
inputs and outputs across various modalities. This innovative approach allows
Lumina-DiMOO to achieve higher sampling efficiency compared to previous
autoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a
broad spectrum of multi-modal tasks, including text-to-image generation,
image-to-image generation (e.g., image editing, subject-driven generation, and
image inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves
state-of-the-art performance on multiple benchmarks, surpassing existing
open-source unified multi-modal models. To foster further advancements in
multi-modal and discrete diffusion model research, we release our code and
checkpoints to the community. Project Page:
https://synbol.github.io/Lumina-DiMOO.

</details>


### [70] [TransFIRA: Transfer Learning for Face Image Recognizability Assessment](https://arxiv.org/abs/2510.06353)
*Allen Tu,Kartik Narayan,Joshua Gleason,Jennifer Xu,Matthew Meyn,Tom Goldstein,Vishal M. Patel*

Main category: cs.CV

TL;DR: TransFIRA: A lightweight, annotation-free framework for face image recognizability assessment grounded in embedding space.


<details>
  <summary>Details</summary>
Motivation: Conventional visual quality metrics fail to predict whether inputs are truly recognizable to the deployed encoder in unconstrained environments. Existing FIQA methods rely on visual heuristics, curated annotations, or computationally intensive generative pipelines, leaving their predictions detached from the encoder's decision geometry.

Method: Defines recognizability via class-center similarity (CCS) and class-center angular separation (CCAS). Introduces a recognizability-informed aggregation strategy.

Result: Achieves state-of-the-art verification accuracy on BRIAR and IJB-C while nearly doubling correlation with true recognizability. Demonstrates strong performance on body recognition and robustness under cross-dataset shifts.

Conclusion: TransFIRA is a unified, geometry-driven framework for recognizability assessment that is encoder-specific, accurate, interpretable, and extensible across modalities.

Abstract: Face recognition in unconstrained environments such as surveillance, video,
and web imagery must contend with extreme variation in pose, blur,
illumination, and occlusion, where conventional visual quality metrics fail to
predict whether inputs are truly recognizable to the deployed encoder. Existing
FIQA methods typically rely on visual heuristics, curated annotations, or
computationally intensive generative pipelines, leaving their predictions
detached from the encoder's decision geometry. We introduce TransFIRA (Transfer
Learning for Face Image Recognizability Assessment), a lightweight and
annotation-free framework that grounds recognizability directly in embedding
space. TransFIRA delivers three advances: (i) a definition of recognizability
via class-center similarity (CCS) and class-center angular separation (CCAS),
yielding the first natural, decision-boundary--aligned criterion for filtering
and weighting; (ii) a recognizability-informed aggregation strategy that
achieves state-of-the-art verification accuracy on BRIAR and IJB-C while nearly
doubling correlation with true recognizability, all without external labels,
heuristics, or backbone-specific training; and (iii) new extensions beyond
faces, including encoder-grounded explainability that reveals how degradations
and subject-specific factors affect recognizability, and the first
recognizability-aware body recognition assessment. Experiments confirm
state-of-the-art results on faces, strong performance on body recognition, and
robustness under cross-dataset shifts. Together, these contributions establish
TransFIRA as a unified, geometry-driven framework for recognizability
assessment -- encoder-specific, accurate, interpretable, and extensible across
modalities -- significantly advancing FIQA in accuracy, explainability, and
scope.

</details>


### [71] [Road Surface Condition Detection with Machine Learning using New York State Department of Transportation Camera Images and Weather Forecast Data](https://arxiv.org/abs/2510.06440)
*Carly Sutter,Kara J. Sulia,Nick P. Bassill,Christopher D. Wirz,Christopher D. Thorncroft,Jay C. Rothenberger,Vanessa Przybylo,Mariana G. Cains,Jacob Radford,David Aaron Evans*

Main category: cs.CV

TL;DR: 纽约州交通部使用交通摄像头来观察路况。机器学习模型可以通过自动分类路况来为交通部提供额外的支持。


<details>
  <summary>Details</summary>
Motivation: 纽约州交通部需要评估路况，以便在冬季天气事件期间做出关键的运营决策。目前他们通过在道路上行驶和观察实时摄像头来评估路况，但这些任务非常耗费人力。

Method: 使用卷积神经网络和随机森林在摄像头图像和天气数据上进行训练，以预测路面状况。使用了一个人工标注的包含约22,000张摄像头图像的数据集进行训练，每张图像被人工标注为六种路面状况之一。

Result: 该研究中的与天气相关的路面状况模型在完全未见过的摄像头上实现了81.5%的准确率。

Conclusion: 机器学习模型可以有效地预测路面状况，从而为纽约州交通部提供决策支持。

Abstract: The New York State Department of Transportation (NYSDOT) has a network of
roadside traffic cameras that are used by both the NYSDOT and the public to
observe road conditions. The NYSDOT evaluates road conditions by driving on
roads and observing live cameras, tasks which are labor-intensive but necessary
for making critical operational decisions during winter weather events.
However, machine learning models can provide additional support for the NYSDOT
by automatically classifying current road conditions across the state. In this
study, convolutional neural networks and random forests are trained on camera
images and weather data to predict road surface conditions. Models are trained
on a hand-labeled dataset of ~22,000 camera images, each classified by human
labelers into one of six road surface conditions: severe snow, snow, wet, dry,
poor visibility, or obstructed. Model generalizability is prioritized to meet
the operational needs of the NYSDOT decision makers, and the weather-related
road surface condition model in this study achieves an accuracy of 81.5% on
completely unseen cameras.

</details>


### [72] [TDiff: Thermal Plug-And-Play Prior with Patch-Based Diffusion](https://arxiv.org/abs/2510.06460)
*Piyush Dashpute,Niki Nezakati,Wolfgang Heidrich,Vishwanath Saragadam*

Main category: cs.CV

TL;DR: 提出了一种基于patch的扩散框架(TDiff)，用于解决低成本相机热图像的分辨率低、固定模式噪声和其他局部退化问题。


<details>
  <summary>Details</summary>
Motivation: 低成本相机的热图像通常分辨率较低，存在固定模式噪声和其他局部退化。可用的热成像数据集在大小和多样性方面也受到限制。

Method: 通过在小的热图像patch上进行训练，利用这些失真的局部性质，提出了一种基于patch的扩散框架(TDiff)。通过去噪重叠的patch并使用平滑的空间窗口混合它们来恢复全分辨率图像。

Result: 在去噪、超分辨率和去模糊的实验中，在模拟和真实的热数据上都表现出强大的结果，将该方法确立为统一的恢复管道。

Conclusion: TDiff是第一个基于patch的扩散框架，它为跨多个任务的热图像恢复建模了学习先验。

Abstract: Thermal images from low-cost cameras often suffer from low resolution, fixed
pattern noise, and other localized degradations. Available datasets for thermal
imaging are also limited in both size and diversity. To address these
challenges, we propose a patch-based diffusion framework (TDiff) that leverages
the local nature of these distortions by training on small thermal patches. In
this approach, full-resolution images are restored by denoising overlapping
patches and blending them using smooth spatial windowing. To our knowledge,
this is the first patch-based diffusion framework that models a learned prior
for thermal image restoration across multiple tasks. Experiments on denoising,
super-resolution, and deblurring demonstrate strong results on both simulated
and real thermal data, establishing our method as a unified restoration
pipeline.

</details>


### [73] [SIGMA-GEN: Structure and Identity Guided Multi-subject Assembly for Image Generation](https://arxiv.org/abs/2510.06469)
*Oindrila Saha,Vojtech Krs,Radomir Mech,Subhransu Maji,Kevin Blackburn-Matzen,Matheus Gadelha*

Main category: cs.CV

TL;DR: SIGMA-GEN: A unified framework for multi-identity preserving image generation.


<details>
  <summary>Details</summary>
Motivation: The paper introduces SIGMA-GEN to address the need for single-pass multi-subject identity-preserved generation guided by structural and spatial constraints, which prior approaches couldn't achieve.

Method: The method uses a novel synthetic dataset called SIGMA-SET27K, which provides identity, structure, and spatial information for over 100k unique subjects across 27k images, to train a model that supports user guidance at various levels of precision.

Result: SIGMA-GEN achieves state-of-the-art performance in identity preservation, image generation quality, and speed.

Conclusion: SIGMA-GEN is a novel framework that enables single-pass multi-subject identity-preserved generation with user guidance, achieving state-of-the-art results.

Abstract: We present SIGMA-GEN, a unified framework for multi-identity preserving image
generation. Unlike prior approaches, SIGMA-GEN is the first to enable
single-pass multi-subject identity-preserved generation guided by both
structural and spatial constraints. A key strength of our method is its ability
to support user guidance at various levels of precision -- from coarse 2D or 3D
boxes to pixel-level segmentations and depth -- with a single model. To enable
this, we introduce SIGMA-SET27K, a novel synthetic dataset that provides
identity, structure, and spatial information for over 100k unique subjects
across 27k images. Through extensive evaluation we demonstrate that SIGMA-GEN
achieves state-of-the-art performance in identity preservation, image
generation quality, and speed. Code and visualizations at
https://oindrilasaha.github.io/SIGMA-Gen/

</details>


### [74] [Superpixel Integrated Grids for Fast Image Segmentation](https://arxiv.org/abs/2510.06487)
*Jack Roberts,Jeova Farias Sales Rocha Neto*

Main category: cs.CV

TL;DR: 这篇论文介绍了一种新的基于超像素的数据结构，称为 SIGRID，用于图像分割任务。


<details>
  <summary>Details</summary>
Motivation: 现有的超像素方法空间分布不规则，限制了深度学习的应用，需要专门的训练算法和架构。

Method: 利用经典形状描述符对超像素的颜色和形状信息进行编码，从而降低输入维度。

Result: 在四个基准数据集上，SIGRID 在匹配甚至超过像素级表示的性能的同时，显著加速了模型训练。

Conclusion: SIGRID 在准确性和计算效率之间取得了良好的平衡。

Abstract: Superpixels have long been used in image simplification to enable more
efficient data processing and storage. However, despite their computational
potential, their irregular spatial distribution has often forced deep learning
approaches to rely on specialized training algorithms and architectures,
undermining the original motivation for superpixelations. In this work, we
introduce a new superpixel-based data structure, SIGRID (Superpixel-Integrated
Grid), as an alternative to full-resolution images in segmentation tasks. By
leveraging classical shape descriptors, SIGRID encodes both color and shape
information of superpixels while substantially reducing input dimensionality.
We evaluate SIGRIDs on four benchmark datasets using two popular convolutional
segmentation architectures. Our results show that, despite compressing the
original data, SIGRIDs not only match but in some cases surpass the performance
of pixel-level representations, all while significantly accelerating model
training. This demonstrates that SIGRIDs achieve a favorable balance between
accuracy and computational efficiency.

</details>


### [75] [Text2Interact: High-Fidelity and Diverse Text-to-Two-Person Interaction Generation](https://arxiv.org/abs/2510.06504)
*Qingxuan Wu,Zhiyang Dou,Chuan Guo,Yiming Huang,Qiao Feng,Bing Zhou,Jian Wang,Lingjie Liu*

Main category: cs.CV

TL;DR: 提出了Text2Interact框架，用于生成逼真且与文本对齐的人与人交互。


<details>
  <summary>Details</summary>
Motivation: 现有的从文本建模人与人交互的方法存在两个局限性：双人训练数据有限，以及文本到交互建模不够精细。

Method: 1) InterCompose：可扩展的合成方法，将LLM生成的交互描述与单人运动先验对齐。2) InterActor：具有词级条件作用的文本到交互模型，保留token级别的线索，并使用自适应交互损失。

Result: 在运动多样性、保真度和泛化性方面均表现出持续的提升，包括超出分布的场景和用户研究。

Conclusion: Text2Interact框架有效地解决了现有方法的局限性，并在人与人交互建模方面取得了显著进展。

Abstract: Modeling human-human interactions from text remains challenging because it
requires not only realistic individual dynamics but also precise,
text-consistent spatiotemporal coupling between agents. Currently, progress is
hindered by 1) limited two-person training data, inadequate to capture the
diverse intricacies of two-person interactions; and 2) insufficiently
fine-grained text-to-interaction modeling, where language conditioning
collapses rich, structured prompts into a single sentence embedding. To address
these limitations, we propose our Text2Interact framework, designed to generate
realistic, text-aligned human-human interactions through a scalable
high-fidelity interaction data synthesizer and an effective spatiotemporal
coordination pipeline. First, we present InterCompose, a scalable
synthesis-by-composition pipeline that aligns LLM-generated interaction
descriptions with strong single-person motion priors. Given a prompt and a
motion for an agent, InterCompose retrieves candidate single-person motions,
trains a conditional reaction generator for another agent, and uses a neural
motion evaluator to filter weak or misaligned samples-expanding interaction
coverage without extra capture. Second, we propose InterActor, a
text-to-interaction model with word-level conditioning that preserves
token-level cues (initiation, response, contact ordering) and an adaptive
interaction loss that emphasizes contextually relevant inter-person joint
pairs, improving coupling and physical plausibility for fine-grained
interaction modeling. Extensive experiments show consistent gains in motion
diversity, fidelity, and generalization, including out-of-distribution
scenarios and user studies. We will release code and models to facilitate
reproducibility.

</details>


### [76] [From Captions to Keyframes: Efficient Video Summarization via Caption- and Context-Aware Frame Scoring](https://arxiv.org/abs/2510.06509)
*Shih-Yao Lin,Sibendu Paul,Caren Chen*

Main category: cs.CV

TL;DR: 提出了一种多模态框架 KeyScore，用于评估帧级别的重要性，并结合 STACFP 实现了高效的视频理解。


<details>
  <summary>Details</summary>
Motivation: 为了高效地理解视频语言，需要选择一小组能保留长视频语义和上下文信息的帧。

Method: KeyScore 结合了语义相似性、时间多样性和上下文丢弃影响来识别信息量最大的帧。STACFP 用于生成紧凑且多样化的帧候选。

Result: 与完整帧推理相比，帧减少率高达 99%，并且在 MSRVTT、MSVD 和 DiDeMo 上大幅优于标准 8 帧编码器。

Conclusion: 强调视觉和文本信号之间的多模态对齐能够实现可扩展、高效且基于字幕的视频理解，而无需显式的视频摘要。

Abstract: Efficient video-language understanding requires selecting a small set of
frames that retain semantic and contextual information from long videos. We
propose KeyScore, a multimodal frame scoring framework that jointly leverages
captions and visual context to estimate frame-level importance. By combining
semantic similarity, temporal diversity, and contextual drop impact, KeyScore
identifies the most informative frames for downstream tasks such as retrieval,
captioning, and video-language reasoning. To complement KeyScore, we introduce
STACFP (Spatio-Temporal Adaptive Clustering for Frame Proposals), which
generates compact and diverse frame candidates for long-form videos. Together,
these modules achieve up to 99\% frame reduction compared to full-frame
inference and substantially outperform standard 8-frame encoders on MSRVTT,
MSVD, and DiDeMo. Our results demonstrate that emphasizing multimodal alignment
between visual and textual signals enables scalable, efficient, and
caption-grounded video understanding -- without explicit video summarization.

</details>


### [77] [LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval](https://arxiv.org/abs/2510.06512)
*Avishree Khare,Hideki Okamoto,Bardh Hoxha,Georgios Fainekos,Rajeev Alur*

Main category: cs.CV

TL;DR: 本文提出了LogSTOP，一种用于计算序列时间属性得分的评分函数，优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 从视频和音频片段的单帧中检测局部属性（如物体和情绪）是很有用的。将这些检测结果提升到序列的时间属性，可用于查询匹配和排序检索等下游应用。

Method: 提出了一种名为LogSTOP的评分函数，可以有效地计算以线性时序逻辑表示的时间属性的得分。

Result: LogSTOP在使用YOLO和HuBERT时，在对象视频和语音情感的时间属性查询匹配方面，比大型视觉/音频语言模型和其他基于时序逻辑的基线至少高出16%。在使用Grounding DINO和SlowR50时，LogSTOP在视频中对象和动作的时间属性排序检索方面，平均精度和召回率分别比zero-shot文本到视频检索基线至少高出19%和16%。

Conclusion: LogSTOP在时间属性的查询匹配和排序检索任务中表现出色。

Abstract: Neural models such as YOLO and HuBERT can be used to detect local properties
such as objects ("car") and emotions ("angry") in individual frames of videos
and audio clips respectively. The likelihood of these detections is indicated
by scores in [0, 1]. Lifting these scores to temporal properties over sequences
can be useful for several downstream applications such as query matching (e.g.,
"does the speaker eventually sound happy in this audio clip?"), and ranked
retrieval (e.g., "retrieve top 5 videos with a 10 second scene where a car is
detected until a pedestrian is detected"). In this work, we formalize this
problem of assigning Scores for TempOral Properties (STOPs) over sequences,
given potentially noisy score predictors for local properties. We then propose
a scoring function called LogSTOP that can efficiently compute these scores for
temporal properties represented in Linear Temporal Logic. Empirically, LogSTOP,
with YOLO and HuBERT, outperforms Large Vision / Audio Language Models and
other Temporal Logic-based baselines by at least 16% on query matching with
temporal properties over objects-in-videos and emotions-in-speech respectively.
Similarly, on ranked retrieval with temporal properties over objects and
actions in videos, LogSTOP with Grounding DINO and SlowR50 reports at least a
19% and 16% increase in mean average precision and recall over zero-shot
text-to-video retrieval baselines respectively.

</details>


### [78] [Limited-Angle Tomography Reconstruction via Projector Guided 3D Diffusion](https://arxiv.org/abs/2510.06516)
*Zhantao Deng,Mériem Er-Rafik,Anna Sushko,Cécile Hébert,Pascal Fua*

Main category: cs.CV

TL;DR: 提出了一种基于3D扩散的迭代重建框架TEMDiff，用于解决Limited-angle electron tomography中的缺失楔形问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法需要大量高质量的训练数据，但在电子显微镜中难以获得。为了解决这个问题。

Method: 使用模拟器将FIB-SEM数据映射到TEM倾斜序列，使模型能够学习真实的结构先验，而无需干净的TEM ground truth。直接在3D体积上操作，隐式地在切片之间强制一致性。

Result: 在模拟电子断层扫描数据集上，TEMDiff优于现有方法。经过训练的TEMDiff模型可以很好地推广到在不同条件下获得的真实TEM倾斜，并且可以从窄至8度的倾斜范围（以2度为增量）恢复准确的结构，而无需任何再训练或微调。

Conclusion: TEMDiff是一种有效的Limited-angle electron tomography重建方法，可以在不需要大量高质量训练数据的情况下，从有限的倾斜角度恢复准确的结构。

Abstract: Limited-angle electron tomography aims to reconstruct 3D shapes from 2D
projections of Transmission Electron Microscopy (TEM) within a restricted range
and number of tilting angles, but it suffers from the missing-wedge problem
that causes severe reconstruction artifacts. Deep learning approaches have
shown promising results in alleviating these artifacts, yet they typically
require large high-quality training datasets with known 3D ground truth which
are difficult to obtain in electron microscopy. To address these challenges, we
propose TEMDiff, a novel 3D diffusion-based iterative reconstruction framework.
Our method is trained on readily available volumetric FIB-SEM data using a
simulator that maps them to TEM tilt series, enabling the model to learn
realistic structural priors without requiring clean TEM ground truth. By
operating directly on 3D volumes, TEMDiff implicitly enforces consistency
across slices without the need for additional regularization. On simulated
electron tomography datasets with limited angular coverage, TEMDiff outperforms
state-of-the-art methods in reconstruction quality. We further demonstrate that
a trained TEMDiff model generalizes well to real-world TEM tilts obtained under
different conditions and can recover accurate structures from tilt ranges as
narrow as 8 degrees, with 2-degree increments, without any retraining or
fine-tuning.

</details>


### [79] [VUGEN: Visual Understanding priors for GENeration](https://arxiv.org/abs/2510.06529)
*Xiangyi Chen,Théophane Vallaeys,Maha Elbayad,John Nguyen,Jakob Verbeek*

Main category: cs.CV

TL;DR: VUGEN利用视觉语言模型(VLM)的预训练视觉理解先验知识进行高效和高质量的图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常依赖于面向重建的自编码器或复杂的桥接机制，导致理解和生成表示之间的不对齐，或架构复杂性。

Method: 该方法首先将VLM原生视觉编码器的高维潜在空间转换为低维、易于处理的分布，从而最大限度地保留视觉信息。然后训练VLM在这个缩减的潜在空间中进行采样，确保与其视觉理解能力对齐。最后，一个专用的像素解码器将这些生成的潜在因素映射回图像空间。

Result: VUGEN实现了卓越的图像生成性能，在COCO上将DPG Bench从71.17提高到74.32，FID从11.86提高到9.06。

Conclusion: VUGEN在图像生成方面优于现有技术，同时完全保留了VLM的原始理解能力。

Abstract: Recent advances in Vision-Language Models (VLMs) have enabled unified
understanding across text and images, yet equipping these models with robust
image generation capabilities remains challenging. Existing approaches often
rely on reconstruction-oriented autoencoders or complex bridging mechanisms,
leading to misalignment between understanding and generation representations,
or architectural complexity. In this work, we propose VUGEN, a novel framework
that explicitly leverages VLM's pretrained visual understanding priors for
efficient and high-quality image generation. Our approach first transforms the
high-dimensional latent space of the VLM's native vision encoder into a
lower-dimensional, tractable distribution that maximally preserves visual
information. The VLM is then trained to sample within this reduced latent
space, ensuring alignment with its visual understanding capabilities. Finally,
a dedicated pixel decoder maps these generated latents back to the image space.
We find that a VAE-free pixel diffusion decoder to be on par or better than
commonly used complex latent diffusion decoders that internally rely on VAE
latents. Extensive experiments demonstrate that VUGEN achieves superior image
generation performance, improving DPG Bench from 71.17 to 74.32 and FID from
11.86 to 9.06 on COCO, while fully preserving the VLM's original understanding
capabilities.

</details>


### [80] [Cluster Paths: Navigating Interpretability in Neural Networks](https://arxiv.org/abs/2510.06541)
*Nicholas M. Kroeger,Vincent Bindschaedler*

Main category: cs.CV

TL;DR: 本文提出了一种名为“cluster paths”的后验可解释性方法，用于理解深度神经网络的决策过程。


<details>
  <summary>Details</summary>
Motivation: 现代深度神经网络在视觉任务中表现出色，但其决策过程不透明，存在风险。

Method: 该方法通过聚类选定层的激活，并将每个输入表示为其聚类ID序列。为了评估这些聚类路径，引入了四个指标：路径复杂性、加权路径纯度、决策对齐忠实度和路径一致性。

Result: 在实验中，聚类路径能够识别基于颜色的捷径，并在去除提示时崩溃。在CelebA头发颜色任务中，实现了90%的忠实度，并在高斯噪声下保持96%的一致性，且不牺牲准确性。此外，聚类路径可以作为有效的OOD检测器。

Conclusion: 聚类路径能够揭示视觉概念，并且可以扩展到大型视觉模型，同时生成简洁且人类可读的解释。

Abstract: While modern deep neural networks achieve impressive performance in vision
tasks, they remain opaque in their decision processes, risking unwarranted
trust, undetected biases and unexpected failures. We propose cluster paths, a
post-hoc interpretability method that clusters activations at selected layers
and represents each input as its sequence of cluster IDs. To assess these
cluster paths, we introduce four metrics: path complexity (cognitive load),
weighted-path purity (class alignment), decision-alignment faithfulness
(predictive fidelity), and path agreement (stability under perturbations). In a
spurious-cue CIFAR-10 experiment, cluster paths identify color-based shortcuts
and collapse when the cue is removed. On a five-class CelebA hair-color task,
they achieve 90% faithfulness and maintain 96% agreement under Gaussian noise
without sacrificing accuracy. Scaling to a Vision Transformer pretrained on
ImageNet, we extend cluster paths to concept paths derived from prompting a
large language model on minimal path divergences. Finally, we show that cluster
paths can serve as an effective out-of-distribution (OOD) detector, reliably
flagging anomalous samples before the model generates over-confident
predictions. Cluster paths uncover visual concepts, such as color palettes,
textures, or object contexts, at multiple network depths, demonstrating that
cluster paths scale to large vision models while generating concise and
human-readable explanations.

</details>


### [81] [HSNet: Heterogeneous Subgraph Network for Single Image Super-resolution](https://arxiv.org/abs/2510.06564)
*Qiongyang Hu,Wenyang Liu,Wenbin Zou,Yuejiao Su,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: 提出了一种新的图像超分辨率框架，名为异构子图网络 (HSNet)，该框架有效地利用了图建模，同时保持了计算可行性。


<details>
  <summary>Details</summary>
Motivation: 现有的图像超分辨率深度学习方法，特别是基于 CNN 和注意力机制的方法，通常存在结构不灵活的问题；虽然基于图的方法提供了更大的表征适应性，但它们经常受到过高的计算复杂性的阻碍。

Method: 该方法将全局图分解为可管理的子组件。首先，引入了建设性子图集块 (CSSB)，它生成了一组不同的互补子图；其次，子图聚合块 (SAB) 集成了嵌入在这些子图中的表示；此外，节点采样策略 (NSS) 旨在选择性地保留最显着的功能，从而在提高准确性的同时减少计算开销。

Result: HSNet 实现了最先进的性能，有效地平衡了重建质量和计算效率。

Conclusion: 该论文提出了 HSNet，一种用于图像超分辨率的新框架，它通过分解全局图、生成互补子图、聚合子图表示和选择性地保留最显着的功能，有效地利用了图建模，同时保持了计算可行性。

Abstract: Existing deep learning approaches for image super-resolution, particularly
those based on CNNs and attention mechanisms, often suffer from structural
inflexibility. Although graph-based methods offer greater representational
adaptability, they are frequently impeded by excessive computational
complexity. To overcome these limitations, this paper proposes the
Heterogeneous Subgraph Network (HSNet), a novel framework that efficiently
leverages graph modeling while maintaining computational feasibility. The core
idea of HSNet is to decompose the global graph into manageable sub-components.
First, we introduce the Constructive Subgraph Set Block (CSSB), which generates
a diverse set of complementary subgraphs. Rather than relying on a single
monolithic graph, CSSB captures heterogeneous characteristics of the image by
modeling different relational patterns and feature interactions, producing a
rich ensemble of both local and global graph structures. Subsequently, the
Subgraph Aggregation Block (SAB) integrates the representations embedded across
these subgraphs. Through adaptive weighting and fusion of multi-graph features,
SAB constructs a comprehensive and discriminative representation that captures
intricate interdependencies. Furthermore, a Node Sampling Strategy (NSS) is
designed to selectively retain the most salient features, thereby enhancing
accuracy while reducing computational overhead. Extensive experiments
demonstrate that HSNet achieves state-of-the-art performance, effectively
balancing reconstruction quality with computational efficiency. The code will
be made publicly available.

</details>


### [82] [Through the Perspective of LiDAR: A Feature-Enriched and Uncertainty-Aware Annotation Pipeline for Terrestrial Point Cloud Segmentation](https://arxiv.org/abs/2510.06582)
*Fei Zhang,Rob Chancia,Josie Clapp,Amirhossein Hassanzadeh,Dimah Dera,Richard MacKenzie,Jan van Aardt*

Main category: cs.CV

TL;DR: 提出了一种半自动、不确定性感知的流水线，用于减少TLS点云语义分割的标注工作量，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 精确的地面激光扫描(TLS)点云语义分割受到昂贵的手动标注的限制。

Method: 该方法将三维点投影到二维球面网格，用多源特征丰富像素，并训练一个分割网络集成来生成伪标签和不确定性图，后者指导标注模糊区域。

Result: 结果表明，性能在约12次标注扫描后达到饱和，几何特征贡献最大，紧凑的9通道堆栈几乎捕获了所有判别能力，平均交并比(mIoU)稳定在0.76左右。通过对ForestSemantic和Semantic3D的跨数据集测试，证实了特征丰富策略的泛化性。

Conclusion: 主要贡献包括：(i)一个具有可视化工具的鲁棒的、不确定性感知的TLS标注流水线；(ii)Mangrove3D数据集；(iii)关于数据效率和特征重要性的经验指导，从而能够对TLS点云进行可扩展的、高质量的分割，用于生态监测等。

Abstract: Accurate semantic segmentation of terrestrial laser scanning (TLS) point
clouds is limited by costly manual annotation. We propose a semi-automated,
uncertainty-aware pipeline that integrates spherical projection, feature
enrichment, ensemble learning, and targeted annotation to reduce labeling
effort, while sustaining high accuracy. Our approach projects 3D points to a 2D
spherical grid, enriches pixels with multi-source features, and trains an
ensemble of segmentation networks to produce pseudo-labels and uncertainty
maps, the latter guiding annotation of ambiguous regions. The 2D outputs are
back-projected to 3D, yielding densely annotated point clouds supported by a
three-tier visualization suite (2D feature maps, 3D colorized point clouds, and
compact virtual spheres) for rapid triage and reviewer guidance. Using this
pipeline, we build Mangrove3D, a semantic segmentation TLS dataset for mangrove
forests. We further evaluate data efficiency and feature importance to address
two key questions: (1) how much annotated data are needed and (2) which
features matter most. Results show that performance saturates after ~12
annotated scans, geometric features contribute the most, and compact
nine-channel stacks capture nearly all discriminative power, with the mean
Intersection over Union (mIoU) plateauing at around 0.76. Finally, we confirm
the generalization of our feature-enrichment strategy through cross-dataset
tests on ForestSemantic and Semantic3D.
  Our contributions include: (i) a robust, uncertainty-aware TLS annotation
pipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii)
empirical guidance on data efficiency and feature importance, thus enabling
scalable, high-quality segmentation of TLS point clouds for ecological
monitoring and beyond. The dataset and processing scripts are publicly
available at https://fz-rit.github.io/through-the-lidars-eye/.

</details>


### [83] [Improving Artifact Robustness for CT Deep Learning Models Without Labeled Artifact Images via Domain Adaptation](https://arxiv.org/abs/2510.06584)
*Justin Cheung,Samuel Savine,Calvin Nguyen,Lin Lu,Alhassan S. Yasin*

Main category: cs.CV

TL;DR: 这篇研究探讨了如何利用领域自适应来提高深度学习模型在医学图像中对新伪影的鲁棒性，避免昂贵的人工标注。


<details>
  <summary>Details</summary>
Motivation: 当CT扫描仪引入训练标签中没有的新伪影时，深度学习模型的性能会显著下降。直接标注这些新分布的图像成本高昂，因此需要更经济的方法。

Method: 该研究通过模拟正弦图空间中的环状伪影，并在OrganAMNIST腹部CT数据集上评估了领域对抗神经网络（DANN）相对于基线和基于增强的方法的效果。

Result: 结果表明，仅在干净图像上训练的基线模型无法推广到具有环状伪影的图像，而传统的增强方法没有改善。相反，DANN方法仅使用未标记的伪影数据，成功地保持了环状伪影图像的高分类准确率。

Conclusion: 领域自适应可以有效解决医学图像中的分布偏移问题，无需对新的伪影分布进行昂贵的专家标注，因此在临床环境中具有应用前景。

Abstract: Deep learning models which perform well on images from their training
distribution can degrade substantially when applied to new distributions. If a
CT scanner introduces a new artifact not present in the training labels, the
model may misclassify the images. Although modern CT scanners include design
features which mitigate these artifacts, unanticipated or difficult-to-mitigate
artifacts can still appear in practice. The direct solution of labeling images
from this new distribution can be costly. As a more accessible alternative,
this study evaluates domain adaptation as an approach for training models that
maintain classification performance despite new artifacts, even without
corresponding labels. We simulate ring artifacts from detector gain error in
sinogram space and evaluate domain adversarial neural networks (DANN) against
baseline and augmentation-based approaches on the OrganAMNIST abdominal CT
dataset. Our results demonstrate that baseline models trained only on clean
images fail to generalize to images with ring artifacts, and traditional
augmentation with other distortion types provides no improvement on unseen
artifact domains. In contrast, the DANN approach successfully maintains high
classification accuracy on ring artifact images using only unlabeled artifact
data during training, demonstrating the viability of domain adaptation for
artifact robustness. The domain-adapted model achieved classification
performance on ring artifact test data comparable to models explicitly trained
with labeled artifact images, while also showing unexpected generalization to
uniform noise. These findings provide empirical evidence that domain adaptation
can effectively address distribution shift in medical imaging without requiring
expensive expert labeling of new artifact distributions, suggesting promise for
deployment in clinical settings where novel artifacts may emerge.

</details>


### [84] [Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer](https://arxiv.org/abs/2510.06590)
*Ziyuan Huang,DanDan Zheng,Cheng Zou,Rui Liu,Xiaolong Wang,Kaixiang Ji,Weilong Chai,Jianxin Sun,Libin Wang,Yongjie Lv,Taozhi Huang,Jiajia Liu,Qingpei Guo,Ming Yang,Jingdong Chen,Jun Zhou*

Main category: cs.CV

TL;DR: 提出了一种新的视觉 tokenizer，名为 MingTok，它使用连续潜在空间来实现统一的自回归生成和理解。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常在离散潜在空间中使用 tokenizer 来与大型语言模型的 token 对齐，但量化误差会限制语义表达并降低视觉语言理解能力。

Method: MingTok 采用三阶段顺序架构，包括低级编码、语义扩展和视觉重建。基于此，Ming-UniVision 消除了对特定于任务的视觉表示的需求，并在单个自回归预测范例下统一了各种视觉语言任务。

Result: 使用统一的连续视觉表示可以协调理解和生成任务对 tokenizer 的竞争需求，从而在两个领域都实现了最先进的性能。

Conclusion: 我们的发现将有助于在连续域中统一视觉 tokenization。

Abstract: Visual tokenization remains a core challenge in unifying visual understanding
and generation within the autoregressive paradigm. Existing methods typically
employ tokenizers in discrete latent spaces to align with the tokens from large
language models, where the quantization errors can limit semantic
expressiveness and degrade the capability of vision-language understanding. To
address this, we introduce MingTok, a new family of visual tokenizers with a
continuous latent space, for unified autoregressive generation and
understanding. While understanding tasks favor discriminative high-dimensional
features, generation tasks prefer compact low-level codes. Thus, to reconcile
these competing demands, MingTok adopts a three-stage sequential architecture
involving low-level encoding, semantic expansion, and visual reconstruction.
Built on top of it, Ming-UniVision eliminates the need for task-specific visual
representations, and unifies diverse vision-language tasks under a single
autoregrsssive prediction paradigm. By formulating both understanding and
generation as next-token prediction in a shared continuous space, it seamlessly
supports multi-round, in-context tasks such as iterative understanding,
generation and editing. Empirically, we find that using a unified continuous
visual representation reconciles the competing requirements on the tokenizers
by the understanding and generation tasks, thereby leading to state-of-the-art
level performance across both domains. We hope our findings will facilitate
unified visual tokenization in the continuous domain. Inference code and model
weights are released to benefit community.

</details>


### [85] [Adaptive Stain Normalization for Cross-Domain Medical Histology](https://arxiv.org/abs/2510.06592)
*Tianyue Xu,Yanlin Wu,Abhai K. Tripathi,Matthew M. Ippolito,Benjamin D. Haeffele*

Main category: cs.CV

TL;DR: 该论文提出了一种可训练的颜色归一化模型，以解决病理图像分析中由于染色和成像条件差异引起的颜色变化问题，该模型基于Beer-Lambert定律和非负矩阵分解，可以在跨域对象检测和分类任务中优于现有的染色归一化方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习在数字病理分析中取得了显著进展，但染色协议和成像条件的差异导致颜色变化，从而降低了模型在不同条件下数据的性能。

Method: 该方法基于Beer-Lambert定律，通过非负矩阵分解模型的算法展开来提取与染色无关的结构信息，并将其用作后续处理的输入。

Result: 在公开的病理数据集和疟疾血涂片数据集上，该方法在跨域对象检测和分类任务中优于许多最先进的染色归一化方法。

Conclusion: 该研究提出了一种有效的颜色归一化方法，可以提高深度学习模型在病理图像分析中的跨域泛化能力。

Abstract: Deep learning advances have revolutionized automated digital pathology
analysis. However, differences in staining protocols and imaging conditions can
introduce significant color variability. In deep learning, such color
inconsistency often reduces performance when deploying models on data acquired
under different conditions from the training data, a challenge known as domain
shift. Many existing methods attempt to address this problem via color
normalization but suffer from several notable drawbacks such as introducing
artifacts or requiring careful choice of a template image for stain mapping. To
address these limitations, we propose a trainable color normalization model
that can be integrated with any backbone network for downstream tasks such as
object detection and classification. Based on the physics of the imaging
process per the Beer-Lambert law, our model architecture is derived via
algorithmic unrolling of a nonnegative matrix factorization (NMF) model to
extract stain-invariant structural information from the original pathology
images, which serves as input for further processing. Experimentally, we
evaluate the method on publicly available pathology datasets and an internally
curated collection of malaria blood smears for cross-domain object detection
and classification, where our method outperforms many state-of-the-art stain
normalization methods. Our code is available at
https://github.com/xutianyue/BeerLaNet.

</details>


### [86] [SDQM: Synthetic Data Quality Metric for Object Detection Dataset Evaluation](https://arxiv.org/abs/2510.06596)
*Ayush Zenith,Arnold Zumbrun,Neel Raut,Jing Lin*

Main category: cs.CV

TL;DR: 提出了一种新的合成数据集质量评估指标（SDQM），用于评估目标检测任务中合成数据的质量，无需模型训练收敛。


<details>
  <summary>Details</summary>
Motivation: 大规模、良好标注的数据集稀缺，对创建鲁棒的模型提出了重大挑战。合成数据通过增强数据集多样性，提高模型的性能、可靠性和弹性，成为一种有前景的解决方案。然而，评估生成数据的质量需要有效的指标。

Method: 引入了合成数据集质量指标（SDQM），用于评估目标检测任务的数据质量，无需模型训练即可收敛。

Result: SDQM与YOLOv11（一种领先的目标检测模型）的平均精度均值（mAP）得分表现出很强的相关性，而之前的指标仅表现出中等或较弱的相关性。此外，它还为提高数据集质量提供了可操作的见解，最大限度地减少了对昂贵的迭代训练的需求。

Conclusion: SDQM是一种可扩展且高效的指标，为评估合成数据设定了新标准。

Abstract: The performance of machine learning models depends heavily on training data.
The scarcity of large-scale, well-annotated datasets poses significant
challenges in creating robust models. To address this, synthetic data generated
through simulations and generative models has emerged as a promising solution,
enhancing dataset diversity and improving the performance, reliability, and
resilience of models. However, evaluating the quality of this generated data
requires an effective metric. This paper introduces the Synthetic Dataset
Quality Metric (SDQM) to assess data quality for object detection tasks without
requiring model training to converge. This metric enables more efficient
generation and selection of synthetic datasets, addressing a key challenge in
resource-constrained object detection tasks. In our experiments, SDQM
demonstrated a strong correlation with the mean Average Precision (mAP) scores
of YOLOv11, a leading object detection model, while previous metrics only
exhibited moderate or weak correlations. Additionally, it provides actionable
insights for improving dataset quality, minimizing the need for costly
iterative training. This scalable and efficient metric sets a new standard for
evaluating synthetic data. The code for SDQM is available at
https://github.com/ayushzenith/SDQM

</details>


### [87] [AIM 2025 Challenge on Real-World RAW Image Denoising](https://arxiv.org/abs/2510.06601)
*Feiran Li,Jiacheng Li,Marcos V. Conde,Beril Besbinar,Vlad Hosu,Daisuke Iso,Radu Timofte*

Main category: cs.CV

TL;DR: 提出了一个真实世界RAW图像去噪挑战赛 (AIM 2025)，旨在推进基于数据合成的高效去噪技术。


<details>
  <summary>Details</summary>
Motivation: 旨在推进高效的图像去噪技术，特别是在真实场景和低光照条件下。

Method: 建立了一个新的评估基准，其中包含使用五种不同的单反相机在野外拍摄的具有挑战性的低光噪声图像。要求参与者开发新的噪声合成管道、网络架构和训练方法，以在不同的相机模型上实现高性能。通过全参考指标（PSNR、SSIM、LPIPS）和非参考指标（ARNIQA、TOPIQ）来确定获胜者。

Result: 推动在合成数据上训练的、与相机无关的低光RAW图像去噪技术的发展。

Conclusion: 比赛结果预计将影响图像恢复和夜间自动驾驶等多个领域。

Abstract: We introduce the AIM 2025 Real-World RAW Image Denoising Challenge, aiming to
advance efficient and effective denoising techniques grounded in data
synthesis. The competition is built upon a newly established evaluation
benchmark featuring challenging low-light noisy images captured in the wild
using five different DSLR cameras. Participants are tasked with developing
novel noise synthesis pipelines, network architectures, and training
methodologies to achieve high performance across different camera models.
Winners are determined based on a combination of performance metrics, including
full-reference measures (PSNR, SSIM, LPIPS), and non-reference ones (ARNIQA,
TOPIQ). By pushing the boundaries of camera-agnostic low-light RAW image
denoising trained on synthetic data, the competition promotes the development
of robust and practical models aligned with the rapid progress in digital
photography. We expect the competition outcomes to influence multiple domains,
from image restoration to night-time autonomous driving.

</details>


### [88] [Self-supervised Physics-guided Model with Implicit Representation Regularization for Fast MRI Reconstruction](https://arxiv.org/abs/2510.06611)
*Jingran Xu,Yuanyuan Liu,Yanjie Zhu*

Main category: cs.CV

TL;DR: 提出了一种名为UnrollINR的新型零样本自监督重建框架，用于快速MRI重建，无需外部训练数据。


<details>
  <summary>Details</summary>
Motivation: 快速MRI重建技术可以通过从欠采样的k空间数据重建高保真MR图像来有效减少采集时间。深度学习方法，特别是自监督和无监督学习方法，在难以获得完全采样数据的情况下显示出显著的价值。

Method: 采用物理引导的展开迭代重建架构，并引入隐式神经表示（INR）作为正则化先验，以有效约束解空间。

Result: 即使在10倍的高加速率下，UnrollINR也能实现优于监督学习方法的重建性能。

Conclusion: UnrollINR在快速MRI重建方面表现出优越性，验证了该方法的优越性。

Abstract: Magnetic Resonance Imaging (MRI) is a vital clinical diagnostic tool, yet its
widespread application is limited by prolonged scan times. Fast MRI
reconstruction techniques effectively reduce acquisition duration by
reconstructing high-fidelity MR images from undersampled k-space data. In
recent years, deep learning-based methods have demonstrated remarkable progress
in this field, with self-supervised and unsupervised learning approaches
proving particularly valuable in scenarios where fully sampled data are
difficult to obtain. This paper proposes a novel zero-shot self-supervised
reconstruction framework named UnrollINR, which enables scan-specific MRI
reconstruction without relying on external training data. The method adopts a
physics-guided unrolled iterative reconstruction architecture and introduces
Implicit Neural Representation (INR) as a regularization prior to effectively
constrain the solution space. By combining a deep unrolled structure with the
powerful implicit representation capability of INR, the model's
interpretability and reconstruction performance are enhanced. Experimental
results demonstrate that even at a high acceleration rate of 10, UnrollINR
achieves superior reconstruction performance compared to the supervised
learning method, validating the superiority of the proposed method.

</details>


### [89] [A Bridge from Audio to Video: Phoneme-Viseme Alignment Allows Every Face to Speak Multiple Languages](https://arxiv.org/abs/2510.06612)
*Zibo Su,Kun Wei,Jiahua Li,Xu Yang,Cheng Deng*

Main category: cs.CV

TL;DR: 提出了一种名为Multilingual Experts (MuEx) 的新型框架，用于生成逼真的多语言说话人脸动画。


<details>
  <summary>Details</summary>
Motivation: 现有的说话人脸合成模型在非英语语言中表现不佳，产生错误的口型和僵硬的面部表情，这是由于以英语为主的训练数据集和缺乏跨语言的泛化能力造成的。

Method: 该框架采用Phoneme-Guided Mixture-of-Experts (PG-MoE) 架构，利用音素和viseme作为通用媒介来桥接音频和视频模态。通过提取音素和viseme作为音频和视频特征，并引入Phoneme-Viseme Alignment Mechanism (PV-Align) 来解决视听同步问题。此外，构建了一个包含12种不同语言的高质量视频的多语言说话人脸基准 (MTFB)。

Result: MuEx在MTFB的所有语言中都取得了优异的性能，并表现出对未见语言的有效零样本泛化能力，无需额外训练。

Conclusion: MuEx框架能够有效提升多语言说话人脸合成的性能，并在跨语言泛化方面表现出色。

Abstract: Speech-driven talking face synthesis (TFS) focuses on generating lifelike
facial animations from audio input. Current TFS models perform well in English
but unsatisfactorily in non-English languages, producing wrong mouth shapes and
rigid facial expressions. The terrible performance is caused by the
English-dominated training datasets and the lack of cross-language
generalization abilities. Thus, we propose Multilingual Experts (MuEx), a novel
framework featuring a Phoneme-Guided Mixture-of-Experts (PG-MoE) architecture
that employs phonemes and visemes as universal intermediaries to bridge audio
and video modalities, achieving lifelike multilingual TFS. To alleviate the
influence of linguistic differences and dataset bias, we extract audio and
video features as phonemes and visemes respectively, which are the basic units
of speech sounds and mouth movements. To address audiovisual synchronization
issues, we introduce the Phoneme-Viseme Alignment Mechanism (PV-Align), which
establishes robust cross-modal correspondences between phonemes and visemes. In
addition, we build a Multilingual Talking Face Benchmark (MTFB) comprising 12
diverse languages with 95.04 hours of high-quality videos for training and
evaluating multilingual TFS performance. Extensive experiments demonstrate that
MuEx achieves superior performance across all languages in MTFB and exhibits
effective zero-shot generalization to unseen languages without additional
training.

</details>


### [90] [MSITrack: A Challenging Benchmark for Multispectral Single Object Tracking](https://arxiv.org/abs/2510.06619)
*Tao Feng,Tingfa Xu,Haolin Qin,Tianhao Li,Shuaihao Han,Xuyang Zou,Zhan Lv,Jianan Li*

Main category: cs.CV

TL;DR: 提出了一个名为MSITrack的大型多光谱单目标跟踪数据集，旨在提升复杂场景下的目标区分度，并促进该领域的发展。


<details>
  <summary>Details</summary>
Motivation: 基于RGB的跟踪器在真实场景中面临遮挡、相似物体干扰和复杂背景等挑战，而多光谱图像能增强目标区分度，但多光谱跟踪数据集有限。

Method: 构建了一个包含300个视频，超过12.9万帧多光谱图像的数据集，并进行了细致的处理、手动标注和多阶段验证。

Result: 通过代表性跟踪器的广泛评估表明，MSITrack中的多光谱数据显著提高了性能。

Conclusion: MSITrack数据集的发布，能够推动多光谱单目标跟踪领域未来的发展。

Abstract: Visual object tracking in real-world scenarios presents numerous challenges
including occlusion, interference from similar objects and complex
backgrounds-all of which limit the effectiveness of RGB-based trackers.
Multispectral imagery, which captures pixel-level spectral reflectance,
enhances target discriminability. However, the availability of multispectral
tracking datasets remains limited. To bridge this gap, we introduce MSITrack,
the largest and most diverse multispectral single object tracking dataset to
date. MSITrack offers the following key features: (i) More Challenging
Attributes-including interference from similar objects and similarity in color
and texture between targets and backgrounds in natural scenarios, along with a
wide range of real-world tracking challenges; (ii) Richer and More Natural
Scenes-spanning 55 object categories and 300 distinct natural scenes, MSITrack
far exceeds the scope of existing benchmarks. Many of these scenes and
categories are introduced to the multispectral tracking domain for the first
time; (iii) Larger Scale-300 videos comprising over 129k frames of
multispectral imagery. To ensure annotation precision, each frame has undergone
meticulous processing, manual labeling and multi-stage verification. Extensive
evaluations using representative trackers demonstrate that the multispectral
data in MSITrack significantly improves performance over RGB-only baselines,
highlighting its potential to drive future advancements in the field. The
MSITrack dataset is publicly available at:
https://github.com/Fengtao191/MSITrack.

</details>


### [91] [StaR-KVQA: Structured Reasoning Traces for Implicit-Knowledge Visual Question Answering](https://arxiv.org/abs/2510.06638)
*Zhihao Wen,Wenkang Wei,Yuan Fang,Xingtong Yu,Hui Zhang,Weicheng Zhu,Xin Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为StaR-KVQA的方法，用于解决隐式知识视觉问答（IK-KVQA）问题，该方法通过监督结构化推理轨迹，提高模型准确性和可解释性，并在多个基准测试中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLM）在解决IK-KVQA问题时，缺乏明确的推理监督，产生不一致的理由，并且在标准监督微调（SFT）后泛化能力较差。

Method: 该方法通过构建和选择路径相关的推理轨迹来形成一个trace-enriched数据集，然后通过结构化的自我蒸馏进行微调，以使生成与监督对齐。整个过程不使用外部检索器、验证器或curated知识库（KB），轨迹是离线构建的，推理是一个单一的自回归过程。

Result: StaR-KVQA在多个基准测试中提高了准确性和可解释性，在OK-VQA上实现了高达+11.3%的答案准确率提升，并表现出强大的跨领域泛化能力。

Conclusion: StaR-KVQA方法通过监督结构化推理轨迹，有效地提高了MLLM在IK-KVQA任务中的性能，并且具有良好的可解释性和泛化能力。

Abstract: Knowledge-based Visual Question Answering (KVQA) requires models to ground
entities in images and reason over factual knowledge. We study its
implicit-knowledge variant, IK-KVQA, where a multimodal large language model
(MLLM) is the sole knowledge source, without external retrieval. Yet, MLLMs
lack explicit reasoning supervision and produce inconsistent justifications,
and generalize poorly after standard supervised fine-tuning (SFT). We present
StaR-KVQA (Structured Reasoning Traces for IK-KVQA), which supervises
structured traces - dual symbolic relation paths plus path-grounded
natural-language explanations - so that reasoning becomes transparent and
verifiable. With one open-source MLLM, StaR-KVQA constructs and selects
path-grounded reasoning traces to form a trace-enriched dataset, then
fine-tunes via structured self-distillation to align generation with
supervision; no external retrievers, verifiers, or curated knowledge bases
(KBs) are used, traces are built offline, and inference is a single
autoregressive pass. Across benchmarks, StaR-KVQA improves both accuracy and
interpretability, achieving up to +11.3% higher answer accuracy on OK-VQA over
the strongest baseline while exhibiting robust cross-domain generalization.

</details>


### [92] [Automated Neural Architecture Design for Industrial Defect Detection](https://arxiv.org/abs/2510.06669)
*Yuxi Liu,Yunfeng Ma,Yi Tang,Min Liu,Shuai Jiang,Yaonan Wang*

Main category: cs.CV

TL;DR: 提出了一种自动神经架构设计框架AutoNAD，用于工业表面缺陷检测（SDD）。


<details>
  <summary>Details</summary>
Motivation: 现有的表面缺陷检测方法主要使用手动设计的模型，需要大量的试验和错误，并且通常难以有效地解决类内差异和类间相似性这两个主要挑战。

Method: AutoNAD 联合搜索卷积、transformer 和多层感知器。此外，AutoNAD 引入了一种交叉权重共享策略，以加速 supernet 的收敛并提高 subnet 的性能。此外，还集成了一个可搜索的多级特征聚合模块 (MFAM) 以增强多尺度特征学习。AutoNAD 结合了延迟感知先验来指导高效架构的选择。

Result: 在三个工业缺陷数据集上验证了 AutoNAD 的有效性，并进一步应用于缺陷成像和检测平台。

Conclusion: AutoNAD 能够捕获细粒度的局部变化和长程语义上下文，在降低手动网络设计成本的同时，解决了两个关键挑战。

Abstract: Industrial surface defect detection (SDD) is critical for ensuring product
quality and manufacturing reliability. Due to the diverse shapes and sizes of
surface defects, SDD faces two main challenges: intraclass difference and
interclass similarity. Existing methods primarily utilize manually designed
models, which require extensive trial and error and often struggle to address
both challenges effectively. To overcome this, we propose AutoNAD, an automated
neural architecture design framework for SDD that jointly searches over
convolutions, transformers, and multi-layer perceptrons. This hybrid design
enables the model to capture both fine-grained local variations and long-range
semantic context, addressing the two key challenges while reducing the cost of
manual network design. To support efficient training of such a diverse search
space, AutoNAD introduces a cross weight sharing strategy, which accelerates
supernet convergence and improves subnet performance. Additionally, a
searchable multi-level feature aggregation module (MFAM) is integrated to
enhance multi-scale feature learning. Beyond detection accuracy, runtime
efficiency is essential for industrial deployment. To this end, AutoNAD
incorporates a latency-aware prior to guide the selection of efficient
architectures. The effectiveness of AutoNAD is validated on three industrial
defect datasets and further applied within a defect imaging and detection
platform. Code will be available at https://github.com/Yuxi104/AutoNAD.

</details>


### [93] [Heptapod: Language Modeling on Visual Signals](https://arxiv.org/abs/2510.06673)
*Yongxin Zhu,Jiawei Chen,Yuanzhe Chen,Zhuo Chen,Dongya Jia,Jian Cong,Xiaobin Zhuang,Yuping Wang,Yuxuan Wang*

Main category: cs.CV

TL;DR: Heptapod是一个图像自回归模型，它遵循语言建模的基本原则，采用因果注意力，不依赖CFG，避免使用语义标记器。通过预测下一个2D分布，模型能够通过生成式训练捕捉全面的图像语义。


<details>
  <summary>Details</summary>
Motivation: 作者旨在探索一种新的图像建模方法，该方法遵循语言建模的原则，并且能够克服传统方法的局限性。

Method: Heptapod使用带有重建聚焦视觉标记器的因果Transformer，学习预测图像在每个时间步的整个2D空间网格上的分布。

Result: 在ImageNet生成基准测试中，Heptapod的FID为2.70，显著优于之前的因果自回归方法。

Conclusion: 这项工作为视觉信号及其他领域的语言建模提供了一个新的思路。

Abstract: We introduce Heptapod, an image autoregressive model that adheres to the
foundational principles of language modeling. Heptapod employs \textbf{causal
attention}, \textbf{eliminates reliance on CFG}, and \textbf{eschews the trend
of semantic tokenizers}. Our key innovation is \textit{next 2D distribution
prediction}: a causal Transformer with reconstruction-focused visual tokenizer,
learns to predict the distribution over the entire 2D spatial grid of images at
each timestep. This learning objective unifies the sequential modeling of
autoregressive framework with the holistic self-supervised learning of masked
autoencoding, enabling the model to capture comprehensive image semantics via
generative training. On the ImageNet generation benchmark, Heptapod achieves an
FID of $2.70$, significantly outperforming previous causal autoregressive
approaches. We hope our work inspires a principled rethinking of language
modeling on visual signals and beyond.

</details>


### [94] [DreamOmni2: Multimodal Instruction-based Editing and Generation](https://arxiv.org/abs/2510.06679)
*Bin Xia,Bohao Peng,Yuechen Zhang,Junjia Huang,Jiyang Liu,Jingyao Li,Haoru Tan,Sitong Wu,Chengyao Wang,Yitong Wang,Xinglong Wu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: 提出多模态指令图像编辑和生成任务，以支持文本和图像指令，并扩展到包括具体和抽象概念


<details>
  <summary>Details</summary>
Motivation: 基于指令的图像编辑依赖于语言指令，通常无法捕捉到具体的编辑细节，需要参考图像。同时，主题驱动的生成仅限于组合具体的对象或人，忽略了更广泛的抽象概念。

Method: 1) 使用特征混合方法创建抽象和具体概念的提取数据; 2) 使用编辑和提取模型生成多模态指令编辑训练数据; 3) 应用提取模型创建多模态指令编辑训练数据。为了处理多图像输入，我们提出了一种索引编码和位置编码移位方案，以帮助模型区分图像并避免像素混淆。此外，我们还引入了与 VLM 和我们的生成/编辑模型的联合训练，以更好地处理复杂指令。

Result: DreamOmni2 取得了令人印象深刻的结果。

Conclusion: 提出了两个新的任务：多模态指令图像编辑和生成，并为这两个新任务提出了综合基准，以推动其发展。

Abstract: Recent advancements in instruction-based image editing and subject-driven
generation have garnered significant attention, yet both tasks still face
limitations in meeting practical user needs. Instruction-based editing relies
solely on language instructions, which often fail to capture specific editing
details, making reference images necessary. Meanwhile, subject-driven
generation is limited to combining concrete objects or people, overlooking
broader, abstract concepts. To address these challenges, we propose two novel
tasks: multimodal instruction-based editing and generation. These tasks support
both text and image instructions and extend the scope to include both concrete
and abstract concepts, greatly enhancing their practical applications. We
introduce DreamOmni2, tackling two primary challenges: data creation and model
framework design. Our data synthesis pipeline consists of three steps: (1)
using a feature mixing method to create extraction data for both abstract and
concrete concepts, (2) generating multimodal instruction-based editing training
data using the editing and extraction models, and (3) further applying the
extraction model to create training data for multimodal instruction-based
editing. For the framework, to handle multi-image input, we propose an index
encoding and position encoding shift scheme, which helps the model distinguish
images and avoid pixel confusion. Additionally, we introduce joint training
with the VLM and our generation/editing model to better process complex
instructions. In addition, we have proposed comprehensive benchmarks for these
two new tasks to drive their development. Experiments show that DreamOmni2 has
achieved impressive results. Models and codes will be released.

</details>


### [95] [Semantic Segmentation Algorithm Based on Light Field and LiDAR Fusion](https://arxiv.org/abs/2510.06687)
*Jie Luo,Yuxuan Jiang,Xin Jin,Mingyu Liu,Yihui Fan*

Main category: cs.CV

TL;DR: 本文提出了一种多模态语义分割方法，集成了光场数据和点云数据，以应对自动驾驶中遮挡等复杂条件下的场景理解挑战。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，语义分割是场景理解的基石，但在遮挡等复杂条件下仍面临挑战。光场和激光雷达模态提供互补的视觉和空间线索，有利于鲁棒感知，但它们的有效集成受到有限的视点多样性和固有的模态差异的阻碍。

Method: 本文提出了一个多模态光场点云融合分割网络(Mlpfseg)，结合了特征补全和深度感知，以同时分割相机图像和激光雷达点云。特征补全模块通过对点云特征图进行差分重建，解决了点云和图像像素之间的密度不匹配问题，增强了这些模态的融合。深度感知模块通过加强注意评分来提高遮挡物体的分割效果，从而更好地感知遮挡。

Result: 该方法优于仅图像分割1.71 mIoU，优于仅点云分割2.38 mIoU，证明了其有效性。

Conclusion: 本文提出的多模态语义分割方法有效地集成了光场数据和点云数据，提高了在复杂条件下场景理解的准确性和鲁棒性。

Abstract: Semantic segmentation serves as a cornerstone of scene understanding in
autonomous driving but continues to face significant challenges under complex
conditions such as occlusion. Light field and LiDAR modalities provide
complementary visual and spatial cues that are beneficial for robust
perception; however, their effective integration is hindered by limited
viewpoint diversity and inherent modality discrepancies. To address these
challenges, the first multimodal semantic segmentation dataset integrating
light field data and point cloud data is proposed. Based on this dataset, we
proposed a multi-modal light field point-cloud fusion segmentation
network(Mlpfseg), incorporating feature completion and depth perception to
segment both camera images and LiDAR point clouds simultaneously. The feature
completion module addresses the density mismatch between point clouds and image
pixels by performing differential reconstruction of point-cloud feature maps,
enhancing the fusion of these modalities. The depth perception module improves
the segmentation of occluded objects by reinforcing attention scores for better
occlusion awareness. Our method outperforms image-only segmentation by 1.71
Mean Intersection over Union(mIoU) and point cloud-only segmentation by 2.38
mIoU, demonstrating its effectiveness.

</details>


### [96] [SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis](https://arxiv.org/abs/2510.06694)
*Jipeng Lyu,Jiahua Dong,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: 提出了一种名为SCas4D的级联优化框架，利用3D高斯溅射中的结构模式进行动态场景建模，实现跟踪和新视角合成。


<details>
  <summary>Details</summary>
Motivation: 捕捉精确的形变同时保持计算效率具有挑战性。

Method: 通过从粗略的部件级别到精细的点级别逐步细化形变，利用高斯组共享相似变换的层级模式。

Result: 在每个时间帧内100次迭代内实现收敛，并以现有方法二十分之一的训练迭代次数产生可比较的结果。在自监督铰接对象分割、新视角合成和密集点跟踪任务中表现出有效性。

Conclusion: SCas4D通过利用动态场景中的结构模式，实现了高效且精确的动态场景建模。

Abstract: Persistent dynamic scene modeling for tracking and novel-view synthesis
remains challenging due to the difficulty of capturing accurate deformations
while maintaining computational efficiency. We propose SCas4D, a cascaded
optimization framework that leverages structural patterns in 3D Gaussian
Splatting for dynamic scenes. The key idea is that real-world deformations
often exhibit hierarchical patterns, where groups of Gaussians share similar
transformations. By progressively refining deformations from coarse part-level
to fine point-level, SCas4D achieves convergence within 100 iterations per time
frame and produces results comparable to existing methods with only
one-twentieth of the training iterations. The approach also demonstrates
effectiveness in self-supervised articulated object segmentation, novel view
synthesis, and dense point tracking tasks.

</details>


### [97] [Evaluating LLMs for Historical Document OCR: A Methodological Framework for Digital Humanities](https://arxiv.org/abs/2510.06743)
*Maria Levchenko*

Main category: cs.CV

TL;DR: 本研究针对LLM在历史文献数字化中的应用，提出了新的评估方法，以解决传统OCR评估指标无法捕捉的时间偏差和特定时期错误的问题。


<details>
  <summary>Details</summary>
Motivation: 当前数字人文研究学者越来越多地使用大型语言模型进行历史文献数字化，但是缺乏合适的基于LLM的OCR评估框架。传统的评估指标无法捕捉时间偏差和特定时期错误，这对于创建历史语料库至关重要。

Method: 我们提出了一种基于LLM的历史OCR评估方法，解决了外交转录中的污染风险和系统性偏差。使用18世纪的俄罗斯 гражданский шрифт 文本，我们引入了包括历史字符保留率 (HCPR) 和古语插入率 (AIR) 在内的新指标，以及污染控制和稳定性测试的协议。

Result: 我们评估了 12 个多模态 LLM，发现 Gemini 和 Qwen 模型优于传统 OCR，但也表现出过度历史化的问题，即插入了来自不正确的历史时期的古体字符。OCR 后的校正反而降低了性能。

Conclusion: 我们的方法论为数字人文研究从业者在历史语料库数字化中选择模型和进行质量评估提供了指导。

Abstract: Digital humanities scholars increasingly use Large Language Models for
historical document digitization, yet lack appropriate evaluation frameworks
for LLM-based OCR. Traditional metrics fail to capture temporal biases and
period-specific errors crucial for historical corpus creation. We present an
evaluation methodology for LLM-based historical OCR, addressing contamination
risks and systematic biases in diplomatic transcription. Using 18th-century
Russian Civil font texts, we introduce novel metrics including Historical
Character Preservation Rate (HCPR) and Archaic Insertion Rate (AIR), alongside
protocols for contamination control and stability testing. We evaluate 12
multimodal LLMs, finding that Gemini and Qwen models outperform traditional OCR
while exhibiting over-historicization: inserting archaic characters from
incorrect historical periods. Post-OCR correction degrades rather than improves
performance. Our methodology provides digital humanities practitioners with
guidelines for model selection and quality assessment in historical corpus
digitization.

</details>


### [98] [DeRainMamba: A Frequency-Aware State Space Model with Detail Enhancement for Image Deraining](https://arxiv.org/abs/2510.06746)
*Zhiliang Zhu,Tao Zeng,Tao Yang,Guoliang Luo,Jiyong Zeng*

Main category: cs.CV

TL;DR: DeRainMamba: Combines frequency-domain modeling and spatial detail enhancement within a state-space framework for single image deraining.


<details>
  <summary>Details</summary>
Motivation: Mamba-based models have limited ability to capture fine-grained details and lack frequency-domain awareness in image deraining.

Method: Proposes DeRainMamba, integrating a Frequency-Aware State-Space Module (FASSM) and Multi-Directional Perception Convolution (MDPConv).

Result: Outperforms state-of-the-art methods in PSNR and SSIM on four public benchmarks with fewer parameters and lower computational costs.

Conclusion: Combining frequency-domain modeling and spatial detail enhancement within a state-space framework is effective for single image deraining.

Abstract: Image deraining is crucial for improving visual quality and supporting
reliable downstream vision tasks. Although Mamba-based models provide efficient
sequence modeling, their limited ability to capture fine-grained details and
lack of frequency-domain awareness restrict further improvements. To address
these issues, we propose DeRainMamba, which integrates a Frequency-Aware
State-Space Module (FASSM) and Multi-Directional Perception Convolution
(MDPConv). FASSM leverages Fourier transform to distinguish rain streaks from
high-frequency image details, balancing rain removal and detail preservation.
MDPConv further restores local structures by capturing anisotropic gradient
features and efficiently fusing multiple convolution branches. Extensive
experiments on four public benchmarks demonstrate that DeRainMamba consistently
outperforms state-of-the-art methods in PSNR and SSIM, while requiring fewer
parameters and lower computational costs. These results validate the
effectiveness of combining frequency-domain modeling and spatial detail
enhancement within a state-space framework for single image deraining.

</details>


### [99] [OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot](https://arxiv.org/abs/2510.06751)
*Junhan Zhu,Hesong Wang,Mingluo Su,Zefang Wang,Huan Wang*

Main category: cs.CV

TL;DR: 提出了一种新的单次剪枝框架OBS-Diff，用于压缩大型文本到图像的扩散模型，且无需训练。


<details>
  <summary>Details</summary>
Motivation: 现有的一次性网络剪枝方法很难直接应用于扩散模型，因为扩散模型具有迭代去噪的特性。

Method: 该方法包括：(1) 改进了经典的最优脑外科手术(OBS)；(2) 提出了一种新的时间步感知Hessian构造，它结合了对数递减加权方案；(3) 提出了一种计算效率高的分组顺序剪枝策略。

Result: OBS-Diff实现了最先进的扩散模型单次剪枝，在视觉质量 минимальном 降低的情况下实现了推理加速。

Conclusion: OBS-Diff 是一种有效的扩散模型压缩方法。

Abstract: Large-scale text-to-image diffusion models, while powerful, suffer from
prohibitive computational cost. Existing one-shot network pruning methods can
hardly be directly applied to them due to the iterative denoising nature of
diffusion models. To bridge the gap, this paper presents OBS-Diff, a novel
one-shot pruning framework that enables accurate and training-free compression
of large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff
revitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex
architectures of modern diffusion models and supporting diverse pruning
granularity, including unstructured, N:M semi-structured, and structured (MHA
heads and FFN neurons) sparsity; (ii) To align the pruning criteria with the
iterative dynamics of the diffusion process, by examining the problem from an
error-accumulation perspective, we propose a novel timestep-aware Hessian
construction that incorporates a logarithmic-decrease weighting scheme,
assigning greater importance to earlier timesteps to mitigate potential error
accumulation; (iii) Furthermore, a computationally efficient group-wise
sequential pruning strategy is proposed to amortize the expensive calibration
process. Extensive experiments show that OBS-Diff achieves state-of-the-art
one-shot pruning for diffusion models, delivering inference acceleration with
minimal degradation in visual quality.

</details>


### [100] [Transforming Noise Distributions with Histogram Matching: Towards a Single Denoiser for All](https://arxiv.org/abs/2510.06757)
*Sheng Fu,Junchao Zhang,Kailun Yang*

Main category: cs.CV

TL;DR: 提出了一种直方图匹配方法，将任意噪声转换为具有已知强度的目标高斯分布，从而提高去噪性能。


<details>
  <summary>Details</summary>
Motivation: 监督高斯去噪器在面对分布外噪声时泛化能力有限，因为不同噪声类型的分布特征各不相同。

Method: 在噪声转换和后续去噪之间建立一个相互加强的循环，逐步完善要转换的噪声，使其接近真实噪声。局部直方图匹配处理信号相关噪声，patch内置换处理通道相关噪声，频域直方图匹配与像素-shuffle下采样相结合，打破空间相关性。

Result: 通过应用这些转换，单个高斯去噪器获得了处理各种分布外噪声的卓越能力，包括泊松噪声、椒盐噪声和重复模式噪声等合成噪声，以及复杂的真实世界噪声。

Conclusion: 大量实验证明了该方法具有优越的泛化性和有效性。

Abstract: Supervised Gaussian denoisers exhibit limited generalization when confronted
with out-of-distribution noise, due to the diverse distributional
characteristics of different noise types. To bridge this gap, we propose a
histogram matching approach that transforms arbitrary noise towards a target
Gaussian distribution with known intensity. Moreover, a mutually reinforcing
cycle is established between noise transformation and subsequent denoising.
This cycle progressively refines the noise to be converted, making it
approximate the real noise, thereby enhancing the noise transformation effect
and further improving the denoising performance. We tackle specific noise
complexities: local histogram matching handles signal-dependent noise,
intrapatch permutation processes channel-related noise, and frequency-domain
histogram matching coupled with pixel-shuffle down-sampling breaks spatial
correlation. By applying these transformations, a single Gaussian denoiser
gains remarkable capability to handle various out-of-distribution noises,
including synthetic noises such as Poisson, salt-and-pepper and repeating
pattern noises, as well as complex real-world noises. Extensive experiments
demonstrate the superior generalization and effectiveness of our method.

</details>


### [101] [A deep multiple instance learning approach based on coarse labels for high-resolution land-cover mapping](https://arxiv.org/abs/2510.06769)
*Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 本文提出了一种使用高分辨率图像和弱低分辨率参考数据训练土地覆盖分类器的方法。


<details>
  <summary>Details</summary>
Motivation: 在高分辨率土地覆盖制图中，训练标签的数量和质量是核心问题。弱标签可以通过利用现有的低分辨率或过时的产品大量收集。

Method: 该方法基于深度多示例学习（DMIL），训练像素级多类分类器并预测低分辨率标签（即，patch-level分类），其中实际的高分辨率标签是在没有直接监督的情况下隐式学习的。通过灵活的池化层将高分辨率图像中像素的语义与低分辨率参考标签联系起来。多示例学习（MIL）问题在多类和多标签设置中重新构建。分类器采用Positive-Unlabeled Learning（PUL）策略进行训练。

Result: 在2020年IEEE GRSS数据融合竞赛数据集上的实验结果表明，与标准训练策略相比，该框架是有效的。

Conclusion: 该论文提出了一种有效的利用弱标签进行高分辨率土地覆盖分类的方法。

Abstract: The quantity and the quality of the training labels are central problems in
high-resolution land-cover mapping with machine-learning-based solutions. In
this context, weak labels can be gathered in large quantities by leveraging on
existing low-resolution or obsolete products. In this paper, we address the
problem of training land-cover classifiers using high-resolution imagery (e.g.,
Sentinel-2) and weak low-resolution reference data (e.g., MODIS -derived
land-cover maps). Inspired by recent works in Deep Multiple Instance Learning
(DMIL), we propose a method that trains pixel-level multi-class classifiers and
predicts low-resolution labels (i.e., patch-level classification), where the
actual high-resolution labels are learned implicitly without direct
supervision. This is achieved with flexible pooling layers that are able to
link the semantics of the pixels in the high-resolution imagery to the
low-resolution reference labels. Then, the Multiple Instance Learning (MIL)
problem is re-framed in a multi-class and in a multi-label setting. In the
former, the low-resolution annotation represents the majority of the pixels in
the patch. In the latter, the annotation only provides us information on the
presence of one of the land-cover classes in the patch and thus multiple labels
can be considered valid for a patch at a time, whereas the low-resolution
labels provide us only one label. Therefore, the classifier is trained with a
Positive-Unlabeled Learning (PUL) strategy. Experimental results on the 2020
IEEE GRSS Data Fusion Contest dataset show the effectiveness of the proposed
framework compared to standard training strategies.

</details>


### [102] [TTRV: Test-Time Reinforcement Learning for Vision Language Models](https://arxiv.org/abs/2510.06783)
*Akshit Singh,Shyam Marjit,Wei Lin,Paul Gavrikov,Serena Yeung-Levy,Hilde Kuehne,Rogerio Feris,Sivan Doveh,James Glass,M. Jehanzeb Mirza*

Main category: cs.CV

TL;DR: TTRV通过在推理时进行模型调整来增强视觉语言理解，无需任何标签数据。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习中提取奖励信号的方法通常依赖于标签数据和专门的训练集，这与人类直接从环境中学习的方式形成对比。

Method: 通过设计基于基础模型输出频率的奖励来增强Group Relative Policy Optimization (GRPO)框架，同时多次推断每个测试样本。此外，通过同时奖励模型以获得输出经验分布的低熵来控制模型输出的多样性。

Result: 在对象识别和视觉问题回答 (VQA) 方面均获得了一致的收益，分别提高了高达 52.4% 和 29.8%，并且在 16 个数据集上的平均提升分别为 24.6% 和 10.0%。在图像识别方面，应用于 InternVL 8B 的 TTRV 在 8 个基准测试中平均超过 GPT-4o 2.3%，同时在 VQA 方面保持高度竞争力。

Conclusion: 测试时强化学习可以匹配或超过最强大的专有模型。即使在极端数据受限的情况下，即在单个随机选择的未标记测试示例上执行适应时，TTRV 仍然可以在识别任务中产生高达 5.5% 的非平凡改进。

Abstract: Existing methods for extracting reward signals in Reinforcement Learning
typically rely on labeled data and dedicated training splits, a setup that
contrasts with how humans learn directly from their environment. In this work,
we propose TTRV to enhance vision language understanding by adapting the model
on the fly at inference time, without the need for any labeled data.
Concretely, we enhance the Group Relative Policy Optimization (GRPO) framework
by designing rewards based on the frequency of the base model's output, while
inferring on each test sample multiple times. Further, we also propose to
control the diversity of the model's output by simultaneously rewarding the
model for obtaining low entropy of the output empirical distribution. Our
approach delivers consistent gains across both object recognition and visual
question answering (VQA), with improvements of up to 52.4% and 29.8%,
respectively, and average boosts of 24.6% and 10.0% across 16
datasets.Remarkably, on image recognition, TTRV applied to InternVL 8B
surpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining
highly competitive on VQA, demonstrating that test-time reinforcement learning
can match or exceed the strongest proprietary models. Finally, we find many
interesting properties of test-time RL for VLMs: for example, even in extremely
data-constrained scenarios, where adaptation is performed on a single randomly
chosen unlabeled test example, TTRV still yields non-trivial improvements of up
to 5.5% in recognition tasks.

</details>


### [103] [Extreme Amodal Face Detection](https://arxiv.org/abs/2510.06791)
*Changlin Song,Yunzhong Hou,Michael Randall Barnes,Rahul Shome,Dylan Campbell*

Main category: cs.CV

TL;DR: 本文提出了一种新的单图像极端非模态人脸检测方法，该方法利用图像中的上下文线索来推断不可见人脸的存在。


<details>
  <summary>Details</summary>
Motivation: 现有的方法依赖于图像序列或生成模型，效率较低。本文旨在解决单图像极端非模态人脸检测问题，应用于安全和隐私领域。

Method: 设计了一个基于热图的极端非模态对象检测器，使用选择性的由粗到精的解码器。

Result: 该方法在该任务上取得了很好的结果，甚至优于效率较低的生成方法。

Conclusion: 本文提出的方法能够有效地预测大量（超出帧区域）的信息。

Abstract: Extreme amodal detection is the task of inferring the 2D location of objects
that are not fully visible in the input image but are visible within an
expanded field-of-view. This differs from amodal detection, where the object is
partially visible within the input image, but is occluded. In this paper, we
consider the sub-problem of face detection, since this class provides
motivating applications involving safety and privacy, but do not tailor our
method specifically to this class. Existing approaches rely on image sequences
so that missing detections may be interpolated from surrounding frames or make
use of generative models to sample possible completions. In contrast, we
consider the single-image task and propose a more efficient, sample-free
approach that makes use of the contextual cues from the image to infer the
presence of unseen faces. We design a heatmap-based extreme amodal object
detector that addresses the problem of efficiently predicting a lot (the
out-of-frame region) from a little (the image) with a selective coarse-to-fine
decoder. Our method establishes strong results for this new task, even
outperforming less efficient generative approaches.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [104] [AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning](https://arxiv.org/abs/2510.06261)
*Zhanke Zhou,Chentao Cao,Xiao Feng,Xuan Li,Zongze Li,Xiangyu Lu,Jiangchao Yao,Weikai Huang,Linrui Xu,Tian Cheng,Guanyu Jiang,Yiming Zheng,Brando Miranda,Tongliang Liu,Sanmi Koyejo,Masashi Sugiyama,Bo Han*

Main category: cs.AI

TL;DR: AlphaApollo是一个自进化的agentic推理系统，旨在解决基础模型（FM）推理中的两个瓶颈：有限的模型内在能力和不可靠的测试时迭代。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型推理能力有限和测试时迭代不可靠的问题。

Method: 通过集成计算工具（Python）和检索工具（外部信息），并利用共享状态图支持多轮、多模型解决方案的演进。

Result: 在AIME 2024/2025的评估中，AlphaApollo在多个模型上取得了持续的提升。

Conclusion: AlphaApollo能够有效提升基础模型的推理能力，超过了非工具基线。

Abstract: We present AlphaApollo, a self-evolving agentic reasoning system that aims to
address two bottlenecks in foundation model (FM) reasoning-limited
model-intrinsic capacity and unreliable test-time iteration. AlphaApollo
orchestrates multiple models with professional tools to enable deliberate,
verifiable reasoning. It couples (i) a computation tool (Python with numerical
and symbolic libraries) and (ii) a retrieval tool (task-relevant external
information) to execute exact calculations and ground decisions. The system
further supports multi-round, multi-model solution evolution via a shared state
map that records candidates, executable checks, and feedback for iterative
refinement. In evaluations on AIME 2024/2025 across multiple models,
AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32
for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for
Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool
calls are successfully executed, with consistent outperformance of non-tool
baselines, thereby lifting the capability ceiling of FMs. More empirical
results and implementation details will be updated at
https://github.com/tmlr-group/AlphaApollo.

</details>


### [105] [Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization](https://arxiv.org/abs/2510.06274)
*Mohammad Mahdi Samiei Paqaleh,Arash Marioriyad,Arman Tahmasebi-Zadeh,Mohamadreza Fereydooni,Mahdi Ghaznavai,Mahdieh Soleymani Baghshah*

Main category: cs.AI

TL;DR: 论文提出了“复杂性分布外泛化 (Complexity OoD)” 框架，用于定义和衡量推理能力。当模型在测试实例上保持性能时，它会表现出复杂性 OoD 泛化，其中测试实例的最小所需解决方案复杂性超过所有训练示例。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在需要逐步推理的问题上取得了进展，但缺乏对推理能力明确且一致的定义或指标。

Method: 论文通过解决方案描述 Kolmogorov 复杂性和操作代理（例如，对象/关系计数；推理步骤计数）来形式化复杂性，并阐明了复杂性 OoD 与长度和组合 OoD 的不同之处。还提出了在基准和评估指标设计中结合复杂性，重新思考针对解决方案跟踪的监督，寻找和设计复杂性 OoD 泛化的归纳偏差，解决学习推理溢出等实践建议。

Result: 论文统一了学习和推理：许多可以用类似 System1 的处理方式解决的低复杂性案例在复杂性压力下变成了类似 System2 的处理方式，而 System2 可以被看作是对解决方案结构的泛化。

Conclusion: 复杂性 OoD 无法通过单独缩放数据来解决，因此，要实现稳健的推理，需要明确建模并根据复杂性分配计算的架构和训练方案。

Abstract: Recent progress has pushed AI frontiers from pattern recognition tasks toward
problems that require step by step, System2 style reasoning, especially with
large language models. Yet, unlike learning, where generalization and out of
distribution (OoD) evaluation concepts are well formalized, there is no clear,
consistent definition or metric for reasoning ability. We propose Complexity
Out of Distribution (Complexity OoD) generalization as a framework and problem
setting to define and measure reasoning. A model exhibits Complexity OoD
generalization when it maintains performance on test instances whose minimal
required solution complexity, either representational (richer solution
structure) or computational (more reasoning steps/program length), exceeds that
of all training examples. We formalize complexity via solution description
Kolmogorov complexity and operational proxies (e.g., object/relation counts;
reasoning step counts), clarifying how Complexity OoD differs from length and
compositional OoD. This lens unifies learning and reasoning: many cases
solvable with System1 like processing at low complexity become System2 like
under complexity pressure, while System2 can be viewed as generalization over
solution structures. We translate this perspective into practice with
recommendations for operationalizing Complexity OoD across the stack:
incorporating complexity into benchmark and evaluation metric design,
rethinking supervision to target solution traces, seeking and designing
inductive biases for Complexity OoD generalization, addressing learning to
reason spillovers such as spurious shortcuts, semantic robustness, catastrophic
forgetting, and step wise calibration. Because Complexity OoD cannot be solved
by scaling data alone, progress toward robust reasoning will require
architectures and training regimes that explicitly model and allocate
computation with respect to complexity.

</details>


### [106] [BuilderBench -- A benchmark for generalist agents](https://arxiv.org/abs/2510.06288)
*Raj Ghugare,Catherine Ji,Kathryn Wantlin,Jin Schofield,Benjamin Eysenbach*

Main category: cs.AI

TL;DR: BuilderBench是一个用于加速智能体预训练研究的基准，专注于开放式探索，要求智能体学习如何使用积木构建任何结构。


<details>
  <summary>Details</summary>
Motivation: 当前的AI模型主要通过模仿和强化学习，难以解决超出已有数据范围的问题。为了解决新问题，智能体应该获得通过经验探索和学习的技能。为开发通过交互学习的智能体寻找可扩展的学习机制仍然是一个主要的开放问题。

Method: 引入BuilderBench基准，它配备了机器人智能体与各种物理积木交互的硬件加速模拟器，以及包含42个多样化目标结构的task-suite，这些结构经过精心设计，用于测试对物理、数学和长期规划的理解。在训练期间，智能体必须探索和学习关于环境的通用原则，而没有任何外部监督。在评估期间，智能体必须从task suite构建看不见的目标结构。

Result: 实验表明，许多任务对当前迭代的算法提出了挑战。因此，我们还提供了一个“training wheels”协议，其中智能体被训练和评估以构建来自task suite的单个目标结构。

Conclusion: 我们提供了六种不同算法的单文件实现，作为研究人员的参考点。

Abstract: Today's AI models learn primarily through mimicry and sharpening, so it is
not surprising that they struggle to solve problems beyond the limits set by
existing data. To solve novel problems, agents should acquire skills for
exploring and learning through experience. Finding a scalable learning
mechanism for developing agents that learn through interaction remains a major
open problem. In this work, we introduce BuilderBench, a benchmark to
accelerate research into agent pre-training that centers open-ended
exploration. BuilderBench requires agents to learn how to build any structure
using blocks. BuilderBench is equipped with $(1)$ a hardware accelerated
simulator of a robotic agent interacting with various physical blocks, and
$(2)$ a task-suite with over 42 diverse target structures that are carefully
curated to test an understanding of physics, mathematics, and long-horizon
planning. During training, agents have to explore and learn general principles
about the environment without any external supervision. During evaluation,
agents have to build the unseen target structures from the task suite. Solving
these tasks requires a sort of \emph{embodied reasoning} that is not reflected
in words but rather in actions, experimenting with different strategies and
piecing them together. Our experiments show that many of these tasks challenge
the current iteration of algorithms. Hence, we also provide a ``training
wheels'' protocol, in which agents are trained and evaluated to build a single
target structure from the task suite. Finally, we provide single-file
implementations of six different algorithms as a reference point for
researchers.

</details>


### [107] [Requirements for Game-Based Learning Design Framework for Information System Integration in the Context of Post-Merger Integration](https://arxiv.org/abs/2510.06302)
*Ksenija Lace,Marite Kirikova*

Main category: cs.AI

TL;DR: 本研究探讨了如何利用游戏化学习设计来改进信息系统集成培训，以解决传统方法学习曲线高和学习动机低的问题。


<details>
  <summary>Details</summary>
Motivation: 并购后的信息系统集成面临独特的挑战，但相关培训不足。现有方法AMILI和AMILP存在学习曲线高和学习动机低的问题。

Method: 通过分析学习理论、认知负荷和动机模型以及严肃游戏设计框架，确定了针对并购后信息系统集成的游戏化学习设计框架的基本要求。要求分为两个部分：转型过程和最终学习体验。

Result: 研究结果为游戏化学习设计框架提供了基本要求，并将其构建为转型过程和学习体验两个组成部分。

Conclusion: 论文最后提出了通过迭代设计和实际验证来开发和评估所提出的框架的计划。

Abstract: Post-merger integration states unique challenges for professionals
responsible for information system integration aimed on alignment and
combination diverse system architectures of merging organizations. Although the
theoretical and practical guidance exists for post-merger integration on the
business level, there is a significant gap in training for information system
integration in this context. In prior research specific methods AMILI (Support
method for informed decision identification) and AMILP (Support method for
informed decision-making) were introduced for the support of information system
integration decisions in the post-merger integration. But during the practical
application was reported high learning curve and low learner motivation. This
paper explores how game-based learning design can address these limitations by
transforming static method training into engaging learning experience. The
study analyzes foundational learning theories, cognitive load and motivation
models, and serious game design frameworks to identify the essential
requirements for a game-based learning design framework tailored to information
system integration in post-merger integration. Requirements are structured in
two components: the transformation process and resulting learning experience.
The paper concludes with a plan for developing and evaluating the proposed
framework through iterative design and real-world validation.

</details>


### [108] [Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks](https://arxiv.org/abs/2510.06307)
*Wentao Deng,Jiahuan Pei,Zhiwei Xu,Zhaochun Ren,Zhumin Chen,Pengjie Ren*

Main category: cs.AI

TL;DR: 提出了一种新的共识寻求框架，通过选择最优合作者和校准系统内部信念来促进稳定共识。


<details>
  <summary>Details</summary>
Motivation: 现有的共识寻求方法通常依赖投票机制来判断共识，忽略了系统内部信念中的矛盾，并且通常涉及代理通过与每个其他代理进行无差别的协作来更新其结果，未能识别每个代理的最佳合作者，阻碍了稳定共识的出现。

Method: 提供了一个选择最大化共识稳定性的最优合作者的理论框架，并提出了信念校准共识寻求（BCCS）框架。

Result: 在MATH和MMLU基准数据集上的实验结果表明，所提出的BCCS框架在具有挑战性的任务上的准确性分别优于现有最佳结果2.23%和3.95%。

Conclusion: 所提出的BCCS框架能够有效地提高多智能体系统在复杂自然语言处理任务中的性能。

Abstract: A multi-agent system (MAS) enhances its capacity to solve complex natural
language processing (NLP) tasks through collaboration among multiple agents,
where consensus-seeking serves as a fundamental mechanism. However, existing
consensus-seeking approaches typically rely on voting mechanisms to judge
consensus, overlooking contradictions in system-internal beliefs that
destabilize the consensus. Moreover, these methods often involve agents
updating their results through indiscriminate collaboration with every other
agent. Such uniform interaction fails to identify the optimal collaborators for
each agent, hindering the emergence of a stable consensus. To address these
challenges, we provide a theoretical framework for selecting optimal
collaborators that maximize consensus stability. Based on the theorems, we
propose the Belief-Calibrated Consensus Seeking (BCCS) framework to facilitate
stable consensus via selecting optimal collaborators and calibrating the
consensus judgment by system-internal beliefs. Experimental results on the MATH
and MMLU benchmark datasets demonstrate that the proposed BCCS framework
outperforms the best existing results by 2.23% and 3.95% of accuracy on
challenging tasks, respectively. Our code and data are available at
https://github.com/dengwentao99/BCCS.

</details>


### [109] [Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?](https://arxiv.org/abs/2510.06410)
*Aochong Oliver Li,Tanya Goyal*

Main category: cs.AI

TL;DR: This paper investigates whether current reasoning LLMs can effectively collaborate and build upon each other's reasoning, even with distractions or guidance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore collaborative reasoning in LLMs to improve inference efficiency and exploration.

Method: The authors propose twin tests, Recoverability and Guidability, to evaluate off-trajectory reasoning behaviors. They evaluate 15 open-weight LLMs (1.5B-32B).

Result: Stronger LLMs are often more fragile under distraction, and all models struggle to leverage guidance from collaborators effectively. The study also isolates the effects of distillation teacher, RL, and data selection strategy.

Conclusion: The work highlights the limitations of off-the-shelf reasoning LLMs and provides insights for training natively strong reasoning collaborators.

Abstract: Reasoning LLMs are trained to verbalize their reasoning process, yielding
strong gains on complex tasks. This transparency also opens a promising
direction: multiple reasoners can directly collaborate on each other's thinking
within a shared trajectory, yielding better inference efficiency and
exploration. A key prerequisite, however, is the ability to assess the
usefulness and build on another model's partial thinking -- we call this
off-trajectory reasoning. Our paper investigates a critical question: can
standard solo-reasoning training pipelines deliver desired off-trajectory
behaviors? We propose twin tests that capture the two extremes of the
off-trajectory spectrum, namely Recoverability, which tests whether LLMs can
backtrack from "distractions" induced by misleading reasoning traces, and
Guidability, which tests their ability to build upon correct reasoning from
stronger collaborators. Our study evaluates 15 open-weight LLMs (1.5B-32B) and
reveals a counterintuitive finding -- "stronger" LLMs on benchmarks are often
more fragile under distraction. Moreover, all models tested fail to effectively
leverage guiding steps from collaborators on problems beyond their inherent
capabilities with solve rates remaining under 9.2%. Finally, we conduct control
studies to isolate the effects of three factors in post-training on these
behaviors: the choice of distillation teacher, the use of RL, and data
selection strategy. Our results provide actionable insights for training
natively strong reasoning collaborators; e.g., we find that suboptimal
recoverability behaviors of teacher models are transferred to distilled
students even if the distillation trajectories are correct. Taken together,
this work lays the groundwork for evaluating multi-model collaborations in
shared reasoning trajectories and highlights the limitations of off-the-shelf
reasoning LLMs.

</details>


### [110] [Flavonoid Fusion: Creating a Knowledge Graph to Unveil the Interplay Between Food and Health](https://arxiv.org/abs/2510.06433)
*Aryan Singh Dalal,Yinglun Zhang,Duru Doğan,Atalay Mert İleri,Hande Küçük McGinty*

Main category: cs.AI

TL;DR: 本研究旨在构建一个知识图谱，连接食物与健康，以弥补食物与健康之间关系在标准化、机器可读格式方面的研究空白。


<details>
  <summary>Details</summary>
Motivation: 过去几年，越来越多的研究关注“食物即药物”，但很少有研究以标准化的机器可读格式表示食物与健康之间的关系。

Method: 利用KNARM方法，从美国农业部数据库中食物的类黄酮含量和文献中癌症的关联信息构建知识图谱。

Result: 构建了一个知识图谱，展示了膳食选择和疾病管理之间复杂的相互作用。

Conclusion: 该知识图谱为研究人员提供了一个示例，未来将扩展知识图谱的范围，捕捉细微差别，添加更多相关数据，并对所获取的知识进行推断，以发现隐藏的关系。

Abstract: The focus on "food as medicine" is gaining traction in the field of health
and several studies conducted in the past few years discussed this aspect of
food in the literature. However, very little research has been done on
representing the relationship between food and health in a standardized,
machine-readable format using a semantic web that can help us leverage this
knowledge effectively. To address this gap, this study aims to create a
knowledge graph to link food and health through the knowledge graph's ability
to combine information from various platforms focusing on flavonoid contents of
food found in the USDA databases and cancer connections found in the
literature. We looked closely at these relationships using KNARM methodology
and represented them in machine-operable format. The proposed knowledge graph
serves as an example for researchers, enabling them to explore the complex
interplay between dietary choices and disease management. Future work for this
study involves expanding the scope of the knowledge graph by capturing nuances,
adding more related data, and performing inferences on the acquired knowledge
to uncover hidden relationships.

</details>


### [111] [PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles](https://arxiv.org/abs/2510.06475)
*Yitao Long,Yuru Jiang,Hongjun Liu,Yilun Zhao,Jingchen Sun,Yiqiu Shen,Chen Zhao,Arman Cohan,Dennis Shasha*

Main category: cs.AI

TL;DR: 本文介绍了一个名为PuzzlePlex的基准测试，用于评估基础模型在复杂动态环境中推理和规划能力的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 研究基础模型在复杂动态环境中的推理和规划能力。

Method: 通过PuzzlePlex基准测试评估不同类型的谜题，并定制游戏策略进行比较。使用细粒度指标衡量性能，并深入分析前沿基础模型在指令和代码设置下的表现。

Result: 推理模型在指令设置中表现优于其他模型，而基于代码的执行更具挑战性，但也提供了一种可扩展且高效的替代方案。

Conclusion: PuzzlePlex能够进行有针对性的评估，并指导未来在推理、规划和基础模型泛化方面的改进。

Abstract: This work investigates the reasoning and planning capabilities of foundation
models and their scalability in complex, dynamic environments. We introduce
PuzzlePlex, a benchmark designed to assess these capabilities through a diverse
set of puzzles. PuzzlePlex consists of 15 types of puzzles, including
deterministic and stochastic games of varying difficulty, as well as
single-player and two-player scenarios. The PuzzlePlex framework provides a
comprehensive environment for each game, and supports extensibility to generate
more challenging instances as foundation models evolve. Additionally, we
implement customized game-playing strategies for comparison. Building on this
benchmark, we develop fine-grained metrics to measure performance and conduct
an in-depth analysis of frontier foundation models across two settings:
instruction-based and code-based. Furthermore, we systematically investigate
their scaling limits. Our findings show that reasoning models outperform others
in instruction-based settings, while code-based execution presents greater
challenges but offers a scalable and efficient alternative. PuzzlePlex enables
targeted evaluation and guides future improvements in reasoning, planning, and
generalization for foundation models.

</details>


### [112] [Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them](https://arxiv.org/abs/2510.06534)
*Jiahe Jin,Abhijay Paladugu,Chenyan Xiong*

Main category: cs.AI

TL;DR: 这篇论文提出了一种基于LLM的agentic搜索方法，通过模仿人类的推理行为来提高搜索效率。


<details>
  <summary>Details</summary>
Motivation: 现有的agentic搜索在处理复杂的用户需求时，LLM的推理和agentic能力面临挑战。

Method: 该论文提出了一个基于推理的LLM pipeline，用于分析agentic搜索中的有效推理行为模式，并提出了一种名为Behavior Priming的技术，通过监督微调（SFT）和强化学习（RL）来训练更有效的agentic搜索模型。

Result: 在三个benchmark上的实验表明，与直接使用RL训练agentic搜索模型相比，behavior priming在Llama3.2-3B和Qwen3-1.7B上产生了超过35%的收益。重要的是，SFT数据中期望的推理行为，而不是最终答案的正确性，是实现RL后强大最终性能的关键因素。

Conclusion: 论文分析表明，引入的推理行为使模型具有更有效的探索和测试时扩展能力，为RL提供了坚实的基础。

Abstract: Agentic search leverages large language models (LLMs) to interpret complex
user information needs and execute a multi-step process of planning, searching,
and synthesizing information to provide answers. This paradigm introduces
unique challenges for LLMs' reasoning and agentic capabilities when interacting
with retrieval systems and the broader web. In this paper, we propose a
reasoning-driven LLM-based pipeline to study effective reasoning behavior
patterns in agentic search. Using this pipeline, we analyze successful agentic
search trajectories and identify four beneficial reasoning behaviors:
Information Verification, Authority Evaluation, Adaptive Search, and Error
Recovery. Based on these findings, we propose a technique called Behavior
Priming to train more effective agentic search models. It synthesizes agentic
search trajectories that exhibit these four behaviors and integrates them into
the agentic search model through supervised fine-tuning (SFT), followed by
standard reinforcement learning (RL). Experiments on three benchmarks (GAIA,
WebWalker, and HLE) demonstrate that behavior priming yields over 35% gains in
Llama3.2-3B and Qwen3-1.7B compared to directly training agentic search models
with RL. Crucially, we demonstrate that the desired reasoning behaviors in the
SFT data, rather than the correctness of the final answer, is the critical
factor for achieving strong final performance after RL: fine-tuning on
trajectories with desirable reasoning behaviors but incorrect answers leads to
better performance than fine-tuning on trajectories with correct answers. Our
analysis further reveals the underlying mechanism: the introduced reasoning
behaviors endow models with more effective exploration (higher pass@k and
entropy) and test-time scaling (longer trajectories) capabilities, providing a
strong foundation for RL. Our code will be released as open source.

</details>


### [113] [Auto-Prompt Ensemble for LLM Judge](https://arxiv.org/abs/2510.06538)
*Jiajie Li,Huayi Zhang,Peng Lin,Jinjun Xiong,Wei Xu*

Main category: cs.AI

TL;DR: 提出了一种新的框架，通过选择性地用辅助评估维度增强LLM来提高LLM判断的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM判断通常会遗漏关键的评估维度，因为它们无法识别人类评估中潜在的隐式标准。

Method: 提出了Auto-Prompt Ensemble (APE)，一个自适应框架，可以从失败案例中自动学习评估维度。APE结合了一种基于置信度的集成机制，通过一种名为Collective Confidence的新型置信度估计方法来决定何时采用来自额外评估维度的判断。

Result: 大量的实验表明，APE提高了LLM Judge在各种标准基准测试中的可靠性。例如，在zero-shot设置中，APE将GPT-4o在Reward Bench上的协议率从87.2%提高到90.5%。

Conclusion: APE为LLM Judge提供了一种利用测试时计算的原则性方法，并弥合了人类和LLM判断之间的评估差距。

Abstract: We present a novel framework that improves the reliability of LLM judges by
selectively augmenting LLM with auxiliary evaluation dimensions. Existing LLM
judges often miss crucial evaluation dimensions because they fail to recognize
the implicit standards underlying human assessments. To address this challenge,
we propose the Auto-Prompt Ensemble (APE), an adaptive framework that
automatically learns evaluation dimensions from its failure cases. APE
incorporates a confidence-based ensemble mechanism to decide when to adopt the
judgments from additional evaluation dimensions through a novel confidence
estimation approach called Collective Confidence. Extensive experiments
demonstrate that APE improves the reliability of LLM Judge across diverse
standard benchmarks. For instance, APE enhances GPT-4o agreement rate on Reward
Bench from 87.2% to 90.5% in the zero-shot setting. Overall, APE provides a
principled approach for LLM Judge to leverage test-time computation, and bridge
the evaluation gap between human and LLM judges.

</details>


### [114] [WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks](https://arxiv.org/abs/2510.06587)
*Jingbo Yang,Bairu Hou,Wei Wei,Shiyu Chang,Yujia Bao*

Main category: cs.AI

TL;DR: WebDART框架通过动态分解任务和持续重新规划来提升LLM在复杂web任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM agent在需要长程导航、大规模信息提取和约束推理的复杂web任务中表现不佳。

Method: 提出WebDART框架，将任务分解为导航、信息提取和执行三个子任务，并根据新页面信息持续重新规划。

Result: 在WebChoreArena上，WebDART的成功率比之前的SOTA agent 提高了13.7个百分点，同时在WebArena上保持了相同的性能，并减少了14.7个导航步骤。

Conclusion: WebDART框架显著提升了LLM agent在复杂web任务中的性能。

Abstract: Large language model (LLM) agents are becoming competent at straightforward
web tasks, such as opening an item page or submitting a form, but still
struggle with objectives that require long horizon navigation, large scale
information extraction, and reasoning under constraints. We present WebDART, a
general framework that enables a single LLM to handle such complex chores.
WebDART (i) dynamically decomposes each objective into three focused subtasks:
navigation, information extraction, and execution, so the model concentrates on
one skill at a time, and (ii) continuously replans the decomposition as new
webpages are revealed, taking advantage of newly discovered filters or
shortcuts and avoiding redundant exploration. Evaluated on WebChoreArena,
WebDART lifts success rates by up to 13.7 percentage points over previous SOTA
agents, while matching their performance on the easier WebArena suite and
completing tasks with up to 14.7 fewer navigation steps.

</details>


### [115] [Fine-Grained Emotion Recognition via In-Context Learning](https://arxiv.org/abs/2510.06600)
*Zhaochun Ren,Zhou Yang,Chenglong Ye,Haizhou Sun,Chao Chen,Xiaofei Zhu,Xiangwen Liao*

Main category: cs.AI

TL;DR: 本文提出了一种名为情感上下文学习 (EICL) 的新方法，用于细粒度情感识别，该方法通过引入情感上相似的示例和动态软标签策略来改进查询表示，并通过两阶段排除策略来优化决策过程。


<details>
  <summary>Details</summary>
Motivation: 现有方法侧重于通过语义相似的例子增强推理过程中的查询表示，但忽略了决策过程。语义相似的例子会引入情感差异，阻碍准确的表示并导致错误。

Method: 本文通过原型理论研究了细粒度情感识别中的决策过程。提出了情感上下文学习 (EICL)，它引入情感上相似的例子，并使用动态软标签策略来改进情感推理过程中的查询表示。然后，采用两阶段排除策略从多个角度评估相似性，进一步优化决策过程。

Result: 大量实验表明，EICL 在多个数据集上显着优于 ICL。

Conclusion: EICL 通过引入情感相似的例子和动态软标签策略，以及两阶段排除策略，显著提高了细粒度情感识别的性能。

Abstract: Fine-grained emotion recognition aims to identify the emotional type in
queries through reasoning and decision-making processes, playing a crucial role
in various systems. Recent methods use In-Context Learning (ICL), enhancing the
representation of queries in the reasoning process through semantically similar
examples, while further improving emotion recognition by explaining the
reasoning mechanisms. However, these methods enhance the reasoning process but
overlook the decision-making process. This paper investigates decision-making
in fine-grained emotion recognition through prototype theory. We show that ICL
relies on similarity matching between query representations and emotional
prototypes within the model, where emotion-accurate representations are
critical. However, semantically similar examples often introduce emotional
discrepancies, hindering accurate representations and causing errors. To
address this, we propose Emotion In-Context Learning (EICL), which introduces
emotionally similar examples and uses a dynamic soft-label strategy to improve
query representations in the emotion reasoning process. A two-stage exclusion
strategy is then employed to assess similarity from multiple angles, further
optimizing the decision-making process. Extensive experiments show that EICL
significantly outperforms ICL on multiple datasets.

</details>


### [116] [Agent-in-the-Loop: A Data Flywheel for Continuous Improvement in LLM-based Customer Support](https://arxiv.org/abs/2510.06674)
*Cen,Zhao,Tiantian Zhang,Hanchen Su,Yufeng,Zhang,Shaowei Su,Mingzhi Xu,Yu,Liu,Wei Han,Jeremy Werner,Claire Na Cheng,Yashar Mehdad*

Main category: cs.AI

TL;DR: 提出了一个Agent-in-the-Loop (AITL) 框架，用于迭代改进基于LLM的客户支持系统。


<details>
  <summary>Details</summary>
Motivation: 传统的离线方法依赖于批量注释，效率低下。

Method: AITL 将四种关键类型的注释直接集成到实时客户运营中：(1) 成对响应偏好，(2) 代理采用和理由，(3) 知识相关性检查，(4) 识别缺失的知识。

Result: 在美国客户支持代理的生产试点中，检索准确率（+11.7% recall@75，+14.8% precision@8），生成质量（+8.4% 帮助性）和代理采用率（+4.5%）得到了显著提高。

Conclusion: 将人工反馈循环直接嵌入到运营工作流程中，可以持续改进基于LLM的客户支持系统。

Abstract: We introduce an Agent-in-the-Loop (AITL) framework that implements a
continuous data flywheel for iteratively improving an LLM-based customer
support system. Unlike standard offline approaches that rely on batch
annotations, AITL integrates four key types of annotations directly into live
customer operations: (1) pairwise response preferences, (2) agent adoption and
rationales, (3) knowledge relevance checks, and (4) identification of missing
knowledge. These feedback signals seamlessly feed back into models' updates,
reducing retraining cycles from months to weeks. Our production pilot involving
US-based customer support agents demonstrated significant improvements in
retrieval accuracy (+11.7% recall@75, +14.8% precision@8), generation quality
(+8.4% helpfulness) and agent adoption rates (+4.5%). These results underscore
the effectiveness of embedding human feedback loops directly into operational
workflows to continuously refine LLM-based customer support system.

</details>


### [117] [Inefficiencies of Meta Agents for Agent Design](https://arxiv.org/abs/2510.06711)
*Batu El,Mert Yuksekgonul,James Zou*

Main category: cs.AI

TL;DR: 本文研究了使用元代理自动设计代理系统时面临的三个关键挑战：跨迭代学习、行为多样性低以及经济可行性。


<details>
  <summary>Details</summary>
Motivation: 现有工作使用元代理来自动化设计代理系统，但存在一些挑战。

Method: 本文通过实验研究了元代理在跨迭代学习、行为多样性和经济可行性方面的问题，并提出了一些改进方法，例如使用进化方法来提高性能。

Result: 本文发现，简单的扩展上下文进行跨迭代学习的效果不如完全忽略先前的设计；设计的代理行为多样性较低；只有在少数情况下，自动设计和部署代理的总成本低于人工设计的代理。

Conclusion: 本文评估了自动设计的经济可行性，并发现其仅在少数情况下优于人工设计，强调了在自动化设计代理系统时需要考虑成本效益。

Abstract: Recent works began to automate the design of agentic systems using
meta-agents that propose and iteratively refine new agent architectures. In
this paper, we examine three key challenges in a common class of meta-agents.
First, we investigate how a meta-agent learns across iterations and find that
simply expanding the context with all previous agents, as proposed by previous
works, performs worse than ignoring prior designs entirely. We show that the
performance improves with an evolutionary approach. Second, although the
meta-agent designs multiple agents during training, it typically commits to a
single agent at test time. We find that the designed agents have low behavioral
diversity, limiting the potential for their complementary use. Third, we assess
when automated design is economically viable. We find that only in a few
cases--specifically, two datasets--the overall cost of designing and deploying
the agents is lower than that of human-designed agents when deployed on over
15,000 examples. In contrast, the performance gains for other datasets do not
justify the design cost, regardless of scale.

</details>


### [118] [MultiCNKG: Integrating Cognitive Neuroscience, Gene, and Disease Knowledge Graphs Using Large Language Models](https://arxiv.org/abs/2510.06742)
*Ali Sarabadani,Kheirolah Rahsepar Fard*

Main category: cs.AI

TL;DR: MultiCNKG: A novel knowledge graph integrating cognitive, genetic, and disease data using LLMs.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle to capture complex relationships in biomedical and cognitive domains. This paper aims to create a cohesive knowledge graph linking genetic mechanisms, neurological disorders, and cognitive functions.

Method: The authors merge three knowledge sources (CNKG, GO, DO) using LLMs (GPT-4) for entity alignment, semantic similarity computation, and graph augmentation.

Result: The resulting MultiCNKG encompasses 6.9K nodes and 11.3K edges, demonstrating high precision, recall, coverage, graph consistency, novelty detection, and expert validation. Link prediction shows competitive performance.

Conclusion: MultiCNKG advances personalized medicine, cognitive disorder diagnostics, and hypothesis formulation in cognitive neuroscience.

Abstract: The advent of large language models (LLMs) has revolutionized the integration
of knowledge graphs (KGs) in biomedical and cognitive sciences, overcoming
limitations in traditional machine learning methods for capturing intricate
semantic links among genes, diseases, and cognitive processes. We introduce
MultiCNKG, an innovative framework that merges three key knowledge sources: the
Cognitive Neuroscience Knowledge Graph (CNKG) with 2.9K nodes and 4.3K edges
across 9 node types and 20 edge types; Gene Ontology (GO) featuring 43K nodes
and 75K edges in 3 node types and 4 edge types; and Disease Ontology (DO)
comprising 11.2K nodes and 8.8K edges with 1 node type and 2 edge types.
Leveraging LLMs like GPT-4, we conduct entity alignment, semantic similarity
computation, and graph augmentation to create a cohesive KG that interconnects
genetic mechanisms, neurological disorders, and cognitive functions. The
resulting MultiCNKG encompasses 6.9K nodes across 5 types (e.g., Genes,
Diseases, Cognitive Processes) and 11.3K edges spanning 7 types (e.g., Causes,
Associated with, Regulates), facilitating a multi-layered view from molecular
to behavioral domains. Assessments using metrics such as precision (85.20%),
recall (87.30%), coverage (92.18%), graph consistency (82.50%), novelty
detection (40.28%), and expert validation (89.50%) affirm its robustness and
coherence. Link prediction evaluations with models like TransE (MR: 391, MRR:
0.411) and RotatE (MR: 263, MRR: 0.395) show competitive performance against
benchmarks like FB15k-237 and WN18RR. This KG advances applications in
personalized medicine, cognitive disorder diagnostics, and hypothesis
formulation in cognitive neuroscience.

</details>


### [119] [Verifying Memoryless Sequential Decision-making of Large Language Models](https://arxiv.org/abs/2510.06756)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb*

Main category: cs.AI

TL;DR: 本文介绍了一种用于验证基于大型语言模型（LLM）策略的工具，该工具用于验证无记忆序列决策任务。


<details>
  <summary>Details</summary>
Motivation: 验证大型语言模型在序列决策任务中的安全性。

Method: 该方法通过LLM选择的动作，增量构建MDP的可达部分，并使用Storm检查生成的模型是否满足安全属性。

Result: 实验表明，确定性播种时，通过Ollama访问的开源LLM可以被验证，但通常不如深度强化学习基线。

Conclusion: 该工具与Ollama原生集成，支持PRISM指定的任务，为正式验证日益强大的LLM奠定了实践基础。

Abstract: We introduce a tool for rigorous and automated verification of large language
model (LLM)- based policies in memoryless sequential decision-making tasks.
Given a Markov decision process (MDP) representing the sequential
decision-making task, an LLM policy, and a safety requirement expressed as a
PCTL formula, our approach incrementally constructs only the reachable portion
of the MDP guided by the LLM's chosen actions. Each state is encoded as a
natural language prompt, the LLM's response is parsed into an action, and
reachable successor states by the policy are expanded. The resulting formal
model is checked with Storm to determine whether the policy satisfies the
specified safety property. In experiments on standard grid world benchmarks, we
show that open source LLMs accessed via Ollama can be verified when
deterministically seeded, but generally underperform deep reinforcement
learning baselines. Our tool natively integrates with Ollama and supports
PRISM-specified tasks, enabling continuous benchmarking in user-specified
sequential decision-making tasks and laying a practical foundation for formally
verifying increasingly capable LLMs.

</details>


### [120] [Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration](https://arxiv.org/abs/2510.06761)
*Zhi Zhang,Yan Liu,Zhejing Hu,Gong Chen,Sheng-hua Zhong,Jiannong Cao*

Main category: cs.AI

TL;DR: 提出了一种双环多智能体（DLMA）框架，用于自动解决科研问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决自动端到端科研流程中，既要发展新颖可靠的高级计划，又要动态不确定条件下正确执行这些计划的双重挑战。

Method: 该框架包含一个由教授智能体组成的领导环，负责发展研究计划，采用进化算法迭代生成和完善研究方案；以及一个由博士生智能体组成的跟随环，负责执行最佳计划，通过会前和会后会议动态调整计划。

Result: 在ACLAward和Laboratory等基准测试中，DLMA生成的论文在自动评估中取得了最先进的分数，显著优于强大的基线。

Conclusion: 消融研究证实了两个环的关键作用，进化驱动创新，执行确保可靠性。

Abstract: Automating the end-to-end scientific research process poses a fundamental
challenge: it requires both evolving high-level plans that are novel and sound,
and executing these plans correctly amidst dynamic and uncertain conditions. To
address this bilevel challenge, we propose a novel Double-Loop Multi-Agent
(DLMA) framework to solve the given research problem automatically. The leader
loop, composed of professor agents, is responsible for evolving research plans.
It employs an evolutionary algorithm through involvement, improvement, and
integration meetings to iteratively generate and refine a pool of research
proposals, exploring the solution space effectively. The follower loop,
composed of doctoral student agents, is responsible for executing the
best-evolved plan. It dynamically adjusts the plan during implementation via
pre-hoc and post-hoc meetings, ensuring each step (e.g., drafting, coding) is
well-supported by contextual and external observations. Extensive experiments
on benchmarks like ACLAward and Laboratory show that DLMA generates research
papers that achieve state-of-the-art scores in automated evaluation,
significantly outperforming strong baselines. Ablation studies confirm the
critical roles of both loops, with evolution driving novelty and execution
ensuring soundness.

</details>


### [121] [Autoformalizer with Tool Feedback](https://arxiv.org/abs/2510.06857)
*Qi Guo,Jianing Wang,Jianfei Zhang,Deyang Kong,Xiangzhou Huang,Xiangyu Xi,Wei Wang,Jingang Wang,Xunliang Cai,Shikun Zhang,Wei Ye*

Main category: cs.AI

TL;DR: 这篇论文提出了一种名为Autoformalizer with Tool Feedback (ATF) 的新方法，通过整合语法和一致性信息作为工具来改进自动形式化过程，从而提高句法有效性和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有形式化工具难以持续生成满足句法有效性和语义一致性的有效语句。

Method: 该方法结合了Lean 4编译器进行语法纠正，并采用多LLM作为判断器进行一致性验证，模型能够根据工具反馈自适应地改进生成的语句。训练过程包括冷启动阶段、专家迭代阶段和直接偏好优化。

Result: 实验结果表明，ATF明显优于一系列基线形式化模型，并通过人工评估进一步验证了其卓越性能。后续分析表明，ATF展示了出色的推理扩展属性。

Conclusion: ATF方法通过引入工具反馈，显著提升了自动形式化的性能，并开源了一个包含750K合成形式语句的数据集Numina-ATF，以促进自动形式化和ATP研究的进展。

Abstract: Autoformalization addresses the scarcity of data for Automated Theorem
Proving (ATP) by translating mathematical problems from natural language into
formal statements. Efforts in recent work shift from directly prompting large
language models to training an end-to-end formalizer model from scratch,
achieving remarkable advancements. However, existing formalizer still struggles
to consistently generate valid statements that meet syntactic validity and
semantic consistency. To address this issue, we propose the Autoformalizer with
Tool Feedback (ATF), a novel approach that incorporates syntactic and
consistency information as tools into the formalization process. By integrating
Lean 4 compilers for syntax corrections and employing a multi-LLMs-as-judge
approach for consistency validation, the model is able to adaptively refine
generated statements according to the tool feedback, enhancing both syntactic
validity and semantic consistency. The training of ATF involves a cold-start
phase on synthetic tool-calling data, an expert iteration phase to improve
formalization capabilities, and Direct Preference Optimization to alleviate
ineffective revisions. Experimental results show that ATF markedly outperforms
a range of baseline formalizer models, with its superior performance further
validated by human evaluations. Subsequent analysis reveals that ATF
demonstrates excellent inference scaling properties. Moreover, we open-source
Numina-ATF, a dataset containing 750K synthetic formal statements to facilitate
advancements in autoformalization and ATP research.

</details>


### [122] [TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs](https://arxiv.org/abs/2510.06878)
*Daria Ozerova,Ekaterina Trofimova*

Main category: cs.AI

TL;DR: 提出了一种新的框架，结合 GRPO 和基于 Thompson 采样的树搜索，以解决 LLM 中迭代改进的搜索空间问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于预定义的启发式方法，受困于探索-利用的两难问题，并且无法根据过去的改进结果进行调整。

Method: 结合 GRPO 和基于 Thompson 采样的树搜索，积极探索失败和成功的改进路径，具有更密集的训练轨迹和更自适应的策略。

Result: 在 HumanEval、MBPP 和 APPS 基准测试中，pass@1 最高提升 4.2 个百分点（在 MBPP 上），pass@10 最高提升 12.51 个百分点（在 APPS 上）。

Conclusion: TGPR 专注于将学习策略与结构化搜索方法相结合，为增强 LLM 中的迭代改进和状态推理提供了一个通用框架。

Abstract: Iterative refinement has been a promising paradigm to enable large language
models (LLMs) to resolve difficult reasoning and problem-solving tasks. One of
the key challenges, however, is how to effectively search through the enormous
search space of possible refinements. Existing methods typically fall back on
predefined heuristics, which are troubled by the exploration-exploitation
dilemma and cannot adapt based on past refinement outcomes. We introduce
Tree-Guided Policy Refinement (TGPR), a novel framework that combines GRPO with
a Thompson-Sampling-based tree search. TGPR explores both failed and successful
refinement paths actively, with denser training trajectories and more adaptive
policies. On HumanEval, MBPP, and APPS benchmarks, our method achieves up to
+4.2 percentage points absolute improvement in pass@1 (on MBPP) and up to
+12.51 percentage points absolute improvement in pass@10 (on APPS) compared to
a competitive GRPO baseline. Apart from debugging code, TGPR focuses on a
principled approach to combining learned policies with structured search
methods, offering a general framework for enhancing iterative refinement and
stateful reasoning in LLMs.

</details>


### [123] [LLM-Assisted Modeling of Semantic Web-Enabled Multi-Agents Systems with AJAN](https://arxiv.org/abs/2510.06911)
*Hacane Hechehouche,Andre Antakli,Matthias Klusch*

Main category: cs.AI

TL;DR: AJAN框架使用多种语义Web标准构建多智能体系统，但RDF/RDFS和SPARQL的定义仍然是一个挑战。


<details>
  <summary>Details</summary>
Motivation: 实践中，基于RDF/RDFS和SPARQL的智能体行为建模存在困难，例如URI容易出现拼写错误，以及在大型环境中编写复杂的SPARQL查询学习曲线高。

Method: 提出了一个集成开发环境，以克服AJAN智能体建模的障碍。

Result: 该集成开发环境通过利用大型语言模型进行智能体工程，扩展了AJAN的用户社区。

Conclusion: 该论文提出了一个集成的开发环境，旨在降低AJAN智能体建模的门槛，并利用大型语言模型扩大用户群体。

Abstract: There are many established semantic Web standards for implementing
multi-agent driven applications. The AJAN framework allows to engineer
multi-agent systems based on these standards. In particular, agent knowledge is
represented in RDF/RDFS and OWL, while agent behavior models are defined with
Behavior Trees and SPARQL to access and manipulate this knowledge. However, the
appropriate definition of RDF/RDFS and SPARQL-based agent behaviors still
remains a major hurdle not only for agent modelers in practice. For example,
dealing with URIs is very error-prone regarding typos and dealing with complex
SPARQL queries in large-scale environments requires a high learning curve. In
this paper, we present an integrated development environment to overcome such
hurdles of modeling AJAN agents and at the same time to extend the user
community for AJAN by the possibility to leverage Large Language Models for
agent engineering.

</details>


### [124] [Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces](https://arxiv.org/abs/2510.06953)
*Minju Gwak,Guijin Son,Jaehyung Kim*

Main category: cs.AI

TL;DR: 这篇论文研究了大型语言模型(LLM)推理过程中的信息密度均匀性，并发现均匀的信息密度与更好的推理质量相关。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨均匀信息密度(UID)原则是否适用于大型语言模型的推理过程，以及步骤层面的均匀性是否反映了推理质量。

Method: 该研究提出了一种基于熵的逐步信息密度指标，并引入了局部和全局均匀性评分两种互补的均匀性度量方法。

Result: 实验结果表明，步骤层面的均匀性不仅提供了强大的理论视角，而且带来了实际的性能提升。例如，选择信息密度更均匀的推理轨迹，在AIME2025上可以提高10-32%的准确率。正确的推理轨迹倾向于避免急剧的信息密度峰值，而错误的轨迹则表现出不规则的信息爆发。

Conclusion: 研究结果表明，受UID启发的的信息密度度量方法优于其他内部信号，可以作为推理质量的预测指标。信息密度均匀性可以作为构建更可靠和准确的推理系统的诊断和选择标准。

Abstract: The Uniform Information Density (UID) hypothesis suggests that effective
communication maintains a stable flow of information. In this work, we revisit
this principle in the context of large language model (LLM) reasoning traces,
asking whether step-level uniformity reflects reasoning quality. To this end,
we propose an entropy-based stepwise information density metric and introduce
two complementary measures of uniformity, local and global uniformity scores.
Across the experiments on six different reasoning benchmarks, we find that
step-level uniformity not only provides a strong theoretical lens but also
yields practical performance benefits; for example, selecting reasoning traces
with more uniform information density at the step-level improves accuracy by
10-32\% relative gains over baselines at AIME2025. Our analysis further reveals
that correct reasoning traces tend to avoid sharp information density spikes,
while incorrect traces exhibit irregular information bursts. These results
demonstrate that UID-inspired information density measures outperform
alternative internal signals as predictors of reasoning quality. Results
highlight the uniformity of the information density as a robust diagnostic and
selection criterion for building more reliable and accurate reasoning systems.

</details>


### [125] [Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning](https://arxiv.org/abs/2510.07038)
*Wenxun Wu,Yuanyang Li,Guhan Chen,Linyue Wang,Hongyang Chen*

Main category: cs.AI

TL;DR: 本文提出了一种名为Tool-Augmented Policy Optimization (TAPO) 的新框架，它结合了多跳推理和自适应工具调用能力，以增强大型语言模型在需要最新知识或计算工具的任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在处理需要最新知识或复杂计算工具的任务时存在局限性。

Method: 本文采用了一种改进的动态采样策略优化（DAPO）方法，并将其调整为工具调用场景，使模型能够动态地将复杂推理与按需工具使用（包括搜索API和Python解释器）交错进行。此外，还构建了两个新的数据集TAPO-easy-60K和TAPO-hard-18K，用于训练和评估。

Result: 实验结果表明，TAPO方法在Qwen2.5-3B和Qwen2.5-7B模型上取得了最先进的性能，尤其是在需要外部知识和数学计算的任务中，并且工具利用效率更高，同时防止了过度调用。

Conclusion: TAPO框架通过结合高级推理和工具使用，显著提升了模型在知识密集型和计算密集型任务中的性能潜力。

Abstract: Recent advances in large language models (LLMs) have popularized test-time
scaling, where models generate additional reasoning tokens before producing
final answers. These approaches have demonstrated significant performance
improvements on benchmarks involving mathematical reasoning. However, language
models relying solely on direct inference still struggle with tasks demanding
up-to-date knowledge or computational tools such as calculators and code
interpreters for complex arithmetic operations. To overcome these limitations,
we propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement
learning framework that systematically integrates multi-hop reasoning with
adaptive tool-calling capabilities. Our approach employs a modified version of
Dynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm,
which we adapt specifically for tool invocation scenarios, enabling models to
dynamically interleave complex reasoning with on-demand tool usage (including
search APIs and Python interpreters).
  To support this research, we introduce two new datasets: TAPO-easy-60K and
TAPO-hard-18K, specifically designed to train and evaluate both fact-based
reasoning and mathematical calculation capabilities. Our experiments on
Qwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach,
with both models achieving state-of-the-art performance on tasks requiring
external knowledge and mathematical computation among methods with comparable
parameters. Notably, TAPO achieves more efficient tool utilization than
baseline methods while preventing excessive calls caused by reward hacking.
These results highlight the significant potential of combining advanced
reasoning with tool usage to enhance model performance in knowledge-intensive
and computationally demanding tasks.

</details>


### [126] [Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations](https://arxiv.org/abs/2510.07064)
*Manh Hung Nguyen,Sebastian Tschiatschek,Adish Singla*

Main category: cs.AI

TL;DR: 提出一个新框架，构建一组能够集体捕捉人类群体多样性的LLM Agent。


<details>
  <summary>Details</summary>
Motivation: 现有LLM无法捕捉人类行为的多样性。

Method: 通过上下文学习，利用少量人类演示来指导LLM Agent的行为，并使用submodular优化方法选择LLM Agent。

Result: 实验表明，该方法构建的Agent比基线方法更有效地代表人类群体。

Conclusion: 该Agent能够重现学生和注释者的行为模式和观点。

Abstract: The difficulty and expense of obtaining large-scale human responses make
Large Language Models (LLMs) an attractive alternative and a promising proxy
for human behavior. However, prior work shows that LLMs often produce
homogeneous outputs that fail to capture the rich diversity of human
perspectives and behaviors. Thus, rather than trying to capture this diversity
with a single LLM agent, we propose a novel framework to construct a set of
agents that collectively capture the diversity of a given human population.
Each agent is an LLM whose behavior is steered by conditioning on a small set
of human demonstrations (task-response pairs) through in-context learning. The
central challenge is therefore to select a representative set of LLM agents
from the exponentially large space of possible agents. We tackle this selection
problem from the lens of submodular optimization. In particular, we develop
methods that offer different trade-offs regarding time complexity and
performance guarantees. Extensive experiments in crowdsourcing and educational
domains demonstrate that our approach constructs agents that more effectively
represent human populations compared to baselines. Moreover, behavioral
analyses on new tasks show that these agents reproduce the behavior patterns
and perspectives of the students and annotators they are designed to represent.

</details>


### [127] [Inductive Learning for Possibilistic Logic Programs Under Stable Models](https://arxiv.org/abs/2510.07069)
*Hongbo Hu,Yisong Wang,Yi Huang,Kewen Wang*

Main category: cs.AI

TL;DR: 本文研究了稳定模型下可能性逻辑程序 (poss-programs) 的归纳推理问题，这是答案集编程 (ASP) 的一个主要变体。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要集中在可能性稳定模型的语义和性质上，而对归纳推理问题尚未进行研究。

Method: 本文提出了一种从背景程序和例子中提取 poss-programs 的方法，并定义了归纳任务的概念，研究了其性质，并提出了两种计算归纳解的算法 ilpsm 和 ilpsmmin。

Result: 实验结果表明，当输入为普通逻辑程序时，该原型在随机生成的数据集上优于一个主要的用于从稳定模型中学习正规逻辑程序的归纳学习系统。

Conclusion: 本文为可能性逻辑程序的归纳推理问题提供了一种有效的解决方案，并通过实验验证了其在特定条件下的优越性。

Abstract: Possibilistic logic programs (poss-programs) under stable models are a major
variant of answer set programming (ASP). While its semantics (possibilistic
stable models) and properties have been well investigated, the problem of
inductive reasoning has not been investigated yet. This paper presents an
approach to extracting poss-programs from a background program and examples
(parts of intended possibilistic stable models). To this end, the notion of
induction tasks is first formally defined, its properties are investigated and
two algorithms ilpsm and ilpsmmin for computing induction solutions are
presented. An implementation of ilpsmmin is also provided and experimental
results show that when inputs are ordinary logic programs, the prototype
outperforms a major inductive learning system for normal logic programs from
stable models on the datasets that are randomly generated.

</details>


### [128] [VRPAgent: LLM-Driven Discovery of Heuristic Operators for Vehicle Routing Problems](https://arxiv.org/abs/2510.07073)
*André Hottung,Federico Berto,Chuanbo Hua,Nayeli Gast Zepeda,Daniel Wetzel,Michael Römer,Haoran Ye,Davide Zago,Michael Poli,Stefano Massaroli,Jinkyoo Park,Kevin Tierney*

Main category: cs.AI

TL;DR: VRPAgent利用大型语言模型生成车辆路径问题(VRP)的启发式算法，并通过遗传搜索进行优化，性能超越了手工方法。


<details>
  <summary>Details</summary>
Motivation: 设计高性能的VRP启发式算法复杂且依赖领域知识；大型语言模型在代码生成方面有潜力但不如专家。

Method: 提出VRPAgent框架，集成LLM生成组件到元启发式算法中，并通过遗传搜索改进。

Result: VRPAgent在多种VRP问题上发现了超越手工方法和学习方法的启发式算子。

Conclusion: VRPAgent是首个基于LLM且在VRP问题上达到state-of-the-art的范例，为自动启发式算法发现开辟了前景。

Abstract: Designing high-performing heuristics for vehicle routing problems (VRPs) is a
complex task that requires both intuition and deep domain knowledge. Large
language model (LLM)-based code generation has recently shown promise across
many domains, but it still falls short of producing heuristics that rival those
crafted by human experts. In this paper, we propose VRPAgent, a framework that
integrates LLM-generated components into a metaheuristic and refines them
through a novel genetic search. By using the LLM to generate problem-specific
operators, embedded within a generic metaheuristic framework, VRPAgent keeps
tasks manageable, guarantees correctness, and still enables the discovery of
novel and powerful strategies. Across multiple problems, including the
capacitated VRP, the VRP with time windows, and the prize-collecting VRP, our
method discovers heuristic operators that outperform handcrafted methods and
recent learning-based approaches while requiring only a single CPU core. To our
knowledge, \VRPAgent is the first LLM-based paradigm to advance the
state-of-the-art in VRPs, highlighting a promising future for automated
heuristics discovery.

</details>


### [129] [The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas](https://arxiv.org/abs/2510.07091)
*Baixuan Xu,Tianshi Zheng,Zhaowei Wang,Hong Ting Tsang,Weiqi Wang,Tianqing Fang,Yangqiu Song*

Main category: cs.AI

TL;DR: 论文研究了长时程任务中大型语言模型的行动表示方法，探讨了传统的使用行动进行规划（PwA）和使用模式进行规划（PwS）两种方法的有效性，并提出了认知带宽视角来理解它们之间的差异。


<details>
  <summary>Details</summary>
Motivation: 在开放世界中，大型语言模型需要执行长时程任务，而当环境行动空间变得非常大时，传统的行动表示方法变得不切实际。因此，研究在行动空间扩大时，哪种行动表示方法对于长时程智能体是最优的。

Method: 论文系统地研究了两种不同的行动表示方法：传统的PwA和PwS。通过实验观察ALFWorld和SciWorld两个环境中的表现，并进行受控实验，研究模型能力与拐点位置之间的关系。

Result: 观察到在ALFWorld（约35个行动）和SciWorld（约500个行动）之间存在一个表示选择的拐点，这证明了对可扩展表示的需求。更强的规划能力使拐点右移，而更好的模式实例化使其左移。

Conclusion: 论文为构建更强大的PwS智能体提供了一个可操作的指南，以实现更好的可扩展自主性。

Abstract: Enabling LLMs to effectively operate long-horizon task which requires
long-term planning and multiple interactions is essential for open-world
autonomy. Conventional methods adopt planning with actions where a executable
action list would be provided as reference. However, this action representation
choice would be impractical when the environment action space is combinatorial
exploded (e.g., open-ended real world). This naturally leads to a question: As
environmental action space scales, what is the optimal action representation
for long-horizon agents? In this paper, we systematically study the
effectiveness of two different action representations. The first one is
conventional planning with actions (PwA) which is predominantly adopted for its
effectiveness on existing benchmarks. The other one is planning with schemas
(PwS) which instantiate an action schema into action lists (e.g., "move [OBJ]
to [OBJ]" -> "move apple to desk") to ensure concise action space and reliable
scalability. This alternative is motivated by its alignment with human
cognition and its compliance with environment-imposed action format
restriction. We propose cognitive bandwidth perspective as a conceptual
framework to qualitatively understand the differences between these two action
representations and empirically observe a representation-choice inflection
point between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve
as evidence of the need for scalable representations. We further conduct
controlled experiments to study how the location of this inflection point
interacts with different model capacities: stronger planning proficiency shifts
the inflection rightward, whereas better schema instantiation shifts it
leftward. Finally, noting the suboptimal performance of PwS agents, we provide
an actionable guide for building more capable PwS agents for better scalable
autonomy.

</details>


### [130] [The Contingencies of Physical Embodiment Allow for Open-Endedness and Care](https://arxiv.org/abs/2510.07117)
*Leonardo Christov-Moore,Arthur Juliani,Alex Kiefer,Nicco Reggente,B. Scott Rousse,Adam Safron,Nicol'as Hinrichs,Daniel Polani,Antonio Damasio*

Main category: cs.AI

TL;DR: 探讨了物理脆弱性和死亡率在人工agent发展中的作用，并提出了从存在主义现象学中获得的两个最小物理体现条件：在世界中存在和趋向死亡。在此基础上，agent可以通过最大化对未来状态的控制来增强其维持物理完整性的能力。


<details>
  <summary>Details</summary>
Motivation: 旨在理解生命条件在人工agent与生物有机体适应开放环境能力差异中的作用，从而开发更强大、适应性更强和更有爱心的人工agent。

Method: 从马丁·海德格尔的存在主义现象学中获得灵感，定义了两个物理体现的最小条件，并结合尼采的权力意志概念，使用强化学习框架对这些概念进行形式化。

Result: 通过强化学习框架，研究了在开放式多agent环境中学习的、内驱的具身agent如何培养开放性和关怀能力。

Conclusion: 从物理体现的两个最小条件出发，agent可以通过最大化对未来状态的控制来增强其维持物理完整性的能力，从而培养开放性和关怀能力。

Abstract: Physical vulnerability and mortality are often seen as obstacles to be
avoided in the development of artificial agents, which struggle to adapt to
open-ended environments and provide aligned care. Meanwhile, biological
organisms survive, thrive, and care for each other in an open-ended physical
world with relative ease and efficiency. Understanding the role of the
conditions of life in this disparity can aid in developing more robust,
adaptive, and caring artificial agents. Here we define two minimal conditions
for physical embodiment inspired by the existentialist phenomenology of Martin
Heidegger: being-in-the-world (the agent is a part of the environment) and
being-towards-death (unless counteracted, the agent drifts toward terminal
states due to the second law of thermodynamics). We propose that from these
conditions we can obtain both a homeostatic drive - aimed at maintaining
integrity and avoiding death by expending energy to learn and act - and an
intrinsic drive to continue to do so in as many ways as possible. Drawing
inspiration from Friedrich Nietzsche's existentialist concept of will-to-power,
we examine how intrinsic drives to maximize control over future states, e.g.,
empowerment, allow agents to increase the probability that they will be able to
meet their future homeostatic needs, thereby enhancing their capacity to
maintain physical integrity. We formalize these concepts within a reinforcement
learning framework, which enables us to examine how intrinsically driven
embodied agents learning in open-ended multi-agent environments may cultivate
the capacities for open-endedness and care.ov

</details>


### [131] [Integrating Domain Knowledge into Process Discovery Using Large Language Models](https://arxiv.org/abs/2510.07161)
*Ali Norouzifar,Humam Kourani,Marcus Dees,Wil van der Aalst*

Main category: cs.AI

TL;DR: 提出了一种交互式框架，该框架利用大型语言模型（LLM）将领域知识（以自然语言表达）整合到流程发现流程中。


<details>
  <summary>Details</summary>
Motivation: 从事件日志中导出的流程模型可能无法准确反映实际流程，因为事件日志通常不完整或受噪声影响，并且领域知识这一重要的补充资源通常被忽略。因此，发现的模型可能缺乏下游任务的可靠性。

Method: 利用LLM从领域专家提供的文本描述中提取声明性规则。这些规则用于指导IMr发现算法，该算法通过结合来自事件日志和提取的规则的见解来递归地构建流程模型，从而避免与领域知识相矛盾的问题流程结构。

Result: 提出了一个完全实现的工具，支持此工作流程，并对多个LLM和提示工程策略进行了广泛的评估。实证研究包括一个基于真实事件日志的案例研究，领域专家参与评估了该框架的可用性和有效性。

Conclusion: 该框架能够整合领域知识到流程发现流程中，从而提高流程模型的可靠性。

Abstract: Process discovery aims to derive process models from event logs, providing
insights into operational behavior and forming a foundation for conformance
checking and process improvement. However, models derived solely from event
data may not accurately reflect the real process, as event logs are often
incomplete or affected by noise, and domain knowledge, an important
complementary resource, is typically disregarded. As a result, the discovered
models may lack reliability for downstream tasks. We propose an interactive
framework that incorporates domain knowledge, expressed in natural language,
into the process discovery pipeline using Large Language Models (LLMs). Our
approach leverages LLMs to extract declarative rules from textual descriptions
provided by domain experts. These rules are used to guide the IMr discovery
algorithm, which recursively constructs process models by combining insights
from both the event log and the extracted rules, helping to avoid problematic
process structures that contradict domain knowledge. The framework coordinates
interactions among the LLM, domain experts, and a set of backend services. We
present a fully implemented tool that supports this workflow and conduct an
extensive evaluation of multiple LLMs and prompt engineering strategies. Our
empirical study includes a case study based on a real-life event log with the
involvement of domain experts, who assessed the usability and effectiveness of
the framework.

</details>


### [132] [NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents](https://arxiv.org/abs/2510.07172)
*Tianshi Zheng,Kelvin Kiu-Wai Tam,Newt Hue-Nam K. Nguyen,Baixuan Xu,Zhaowei Wang,Jiayang Cheng,Hong Ting Tsang,Weiqi Wang,Jiaxin Bai,Tianqing Fang,Yangqiu Song,Ginny Y. Wong,Simon See*

Main category: cs.AI

TL;DR: 介绍了NewtonBench，一个用于评估大型语言模型在科学定律发现方面的基准，它通过形而上学的转变来生成大量可扩展、科学相关且抗记忆的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准在科学相关性、可扩展性和抗记忆性之间存在三难困境，并且过于简化了发现过程，无法捕捉到通过复杂模型系统的交互探索来揭示嵌入式定律的真实科学过程。

Method: 引入NewtonBench，包含12个物理领域的324个科学定律发现任务，利用形而上学的转变来缓解评估困境，并提升评估标准到交互式模型发现。

Result: 实验表明，前沿LLM的发现能力明显但脆弱，随着系统复杂性的增加而迅速下降，并且对观测噪声非常敏感。工具辅助会产生矛盾的影响：代码解释器会阻碍更有能力的模型，导致它们过早地从探索转向利用，从而满足于次优解。

Conclusion: 在复杂、交互式环境中进行稳健、可推广的发现仍然是核心挑战。NewtonBench提供了一个可扩展、稳健且科学真实的测试平台，用于衡量真正的进展，并指导下一代能够真正进行科学发现的AI代理的开发。

Abstract: Large language models are emerging as powerful tools for scientific law
discovery, a foundational challenge in AI-driven science. However, existing
benchmarks for this task suffer from a fundamental methodological trilemma,
forcing a trade-off between scientific relevance, scalability, and resistance
to memorization. Furthermore, they oversimplify discovery as static function
fitting, failing to capture the authentic scientific process of uncovering
embedded laws through the interactive exploration of complex model systems. To
address these critical gaps, we introduce NewtonBench, a benchmark comprising
324 scientific law discovery tasks across 12 physics domains. Our design
mitigates the evaluation trilemma by using metaphysical shifts - systematic
alterations of canonical laws - to generate a vast suite of problems that are
scalable, scientifically relevant, and memorization-resistant. Moreover, we
elevate the evaluation from static function fitting to interactive model
discovery, requiring agents to experimentally probe simulated complex systems
to uncover hidden principles. Our extensive experiment reveals a clear but
fragile capability for discovery in frontier LLMs: this ability degrades
precipitously with increasing system complexity and exhibits extreme
sensitivity to observational noise. Notably, we uncover a paradoxical effect of
tool assistance: providing a code interpreter can hinder more capable models by
inducing a premature shift from exploration to exploitation, causing them to
satisfice on suboptimal solutions. These results demonstrate that robust,
generalizable discovery in complex, interactive environments remains the core
challenge. By providing a scalable, robust, and scientifically authentic
testbed, NewtonBench offers a crucial tool for measuring true progress and
guiding the development of next-generation AI agents capable of genuine
scientific discovery.

</details>


### [133] [Multi-Objective Multi-Agent Path Finding with Lexicographic Cost Preferences](https://arxiv.org/abs/2510.07276)
*Pulkit Rustagi,Kyle Hollins Wray,Sandhya Saisubramanian*

Main category: cs.AI

TL;DR: 提出了一种新的用于解决多目标多智能体路径规划 (MO-MAPF) 问题的词典框架和算法 LCBS，该算法直接计算与目标上的词典偏好对齐的单个解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的 MO-MAPF 算法通常通过计算 Pareto 前沿来生成无冲突的计划，但没有明确地针对用户定义的偏好进行优化，并且随着目标数量的增加，其性能会下降。

Method: LCBS 算法集成了优先级感知的低级 A* 搜索与基于冲突的搜索，避免了 Pareto 前沿构建，并实现了由目标偏好引导的有效规划。

Result: LCBS 算法可以计算最优解，并且可以扩展到具有多达十个目标的实例，远远超出了现有 MO-MAPF 方法的限制。在标准和随机 MAPF 基准上的评估表明，相对于最先进的基线，LCBS 算法具有始终更高的成功率，尤其是在目标数量增加的情况下。

Conclusion: LCBS 算法在解决多目标多智能体路径规划问题方面具有优势，特别是在目标数量较多且存在用户偏好的情况下。

Abstract: Many real-world scenarios require multiple agents to coordinate in shared
environments, while balancing trade-offs between multiple, potentially
competing objectives. Current multi-objective multi-agent path finding
(MO-MAPF) algorithms typically produce conflict-free plans by computing Pareto
frontiers. They do not explicitly optimize for user-defined preferences, even
when the preferences are available, and scale poorly with the number of
objectives. We propose a lexicographic framework for modeling MO-MAPF, along
with an algorithm \textit{Lexicographic Conflict-Based Search} (LCBS) that
directly computes a single solution aligned with a lexicographic preference
over objectives. LCBS integrates a priority-aware low-level $A^*$ search with
conflict-based search, avoiding Pareto frontier construction and enabling
efficient planning guided by preference over objectives. We provide insights
into optimality and scalability, and empirically demonstrate that LCBS computes
optimal solutions while scaling to instances with up to ten objectives -- far
beyond the limits of existing MO-MAPF methods. Evaluations on standard and
randomized MAPF benchmarks show consistently higher success rates against
state-of-the-art baselines, especially with increasing number of objectives.

</details>


### [134] [Agentic generative AI for media content discovery at the national football league](https://arxiv.org/abs/2510.07297)
*Henry Wang,Md Sirajus Salekin,Jake Lee,Ross Claytor,Shinan Zhang,Michael Chi*

Main category: cs.AI

TL;DR: 这篇论文介绍了一种基于生成式人工智能的工作流程，该流程使媒体研究人员和分析师能够使用自然语言查询相关的历史剧本。


<details>
  <summary>Details</summary>
Motivation: 为了解决媒体研究人员和分析师使用传统过滤和点击界面查询相关历史剧本效率低下的问题。

Method: 该方法采用一种基于生成式人工智能的工作流程，该流程将用户查询作为输入，将其分解为多个元素，并将其转换为底层数据库查询语言。通过精心设计的语义缓存，进一步提高了准确性和延迟。

Result: 该解决方案实现了超过 95% 的准确率，并将查找相关视频的平均时间从 10 分钟缩短到 30 秒。

Conclusion: 该解决方案显著提高了 NFL 的运营效率，并允许用户专注于制作创意内容和引人入胜的故事情节。

Abstract: Generative AI has unlocked new possibilities in content discovery and
management. Through collaboration with the National Football League (NFL), we
demonstrate how a generative-AI based workflow enables media researchers and
analysts to query relevant historical plays using natural language rather than
traditional filter-and-click interfaces. The agentic workflow takes a user
query as input, breaks it into elements, and translates them into the
underlying database query language. Accuracy and latency are further improved
through carefully designed semantic caching. The solution achieves over 95
percent accuracy and reduces the average time to find relevant videos from 10
minutes to 30 seconds, significantly increasing the NFL's operational
efficiency and allowing users to focus on producing creative content and
engaging storylines.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [135] [Bridging Imperative Process Models and Process Data Queries-Translation and Relaxation](https://arxiv.org/abs/2510.06414)
*Abdur Rehman Anwar Qureshi,Adrian Rebmann,Timotheus Kampik,Matthias Weidlich,Mathias Weske*

Main category: cs.DB

TL;DR: 本文提出了一种将命令式模型转换为松弛过程数据查询的方法，以弥合传统过程建模与数据驱动过程分析之间的差距。


<details>
  <summary>Details</summary>
Motivation: 传统的过程模型（通常使用 Petri 网形式化）不能直接应用于包含大量可用结构化过程执行数据的关系数据库，导致过程模型未被充分利用。

Method: 将命令式模型转换为松弛过程数据查询，特别是可在关系数据库上执行的 SQL 查询，以进行一致性检查。

Result: 结果表明，命令式过程模型对数据驱动的过程管理仍然具有相关性，并且行为足迹和其他声明式方法对于集成基于模型和数据驱动的过程管理非常重要。

Conclusion: 本文弥合了传统过程建模与数据驱动过程分析之间的差距，强调了命令式模型和声明式方法在数据驱动过程管理中的持续相关性和重要性。

Abstract: Business process management is increasingly practiced using data-driven
approaches. Still, classical imperative process models, which are typically
formalized using Petri nets, are not straightforwardly applicable to the
relational databases that contain much of the available structured process
execution data. This creates a gap between the traditional world of process
modeling and recent developments around data-driven process analysis,
ultimately leading to the under-utilization of often readily available process
models. In this paper, we close this gap by providing an approach for
translating imperative models into relaxed process data queries, specifically
SQL queries executable on relational databases, for conformance checking. Our
results show the continued relevance of imperative process models to
data-driven process management, as well as the importance of behavioral
footprints and other declarative approaches for integrating model-based and
data-driven process management.

</details>


### [136] [Automated Discovery of Test Oracles for Database Management Systems Using LLMs](https://arxiv.org/abs/2510.06663)
*Qiuyang Mang,Runyuan He,Suyang Zhong,Xiaoxuan Liu,Huanchen Zhang,Alvin Cheung*

Main category: cs.DB

TL;DR: 这篇论文提出了一种名为Argus的新框架，该框架利用大型语言模型（LLM）来自动化数据库管理系统（DBMS）的测试预言机的发现和实例化。


<details>
  <summary>Details</summary>
Motivation: 现有的DBMS自动测试技术依赖于手动设计的测试预言机，这限制了自动化程度。这篇论文旨在利用LLM来自动化测试预言机的发现和实例化，解决DBMS测试中的瓶颈问题。

Method: Argus框架基于约束抽象查询的核心概念，使用LLM生成语义等价的SQL骨架对，并通过SQL等价求解器进行形式化验证，最后使用LLM生成具体的SQL代码片段来实例化这些骨架。

Result: Argus在五个广泛测试的DBMS上发现了40个以前未知的错误，其中35个是逻辑错误，36个已确认，26个已被开发人员修复。

Conclusion: Argus框架成功地利用LLM自动化了DBMS测试预言机的生成，并发现了大量新的错误，证明了该方法的有效性。

Abstract: Since 2020, automated testing for Database Management Systems (DBMSs) has
flourished, uncovering hundreds of bugs in widely-used systems. A cornerstone
of these techniques is test oracle, which typically implements a mechanism to
generate equivalent query pairs, thereby identifying bugs by checking the
consistency between their results. However, while applying these oracles can be
automated, their design remains a fundamentally manual endeavor. This paper
explores the use of large language models (LLMs) to automate the discovery and
instantiation of test oracles, addressing a long-standing bottleneck towards
fully automated DBMS testing. Although LLMs demonstrate impressive creativity,
they are prone to hallucinations that can produce numerous false positive bug
reports. Furthermore, their significant monetary cost and latency mean that LLM
invocations should be limited to ensure that bug detection is efficient and
economical.
  To this end, we introduce Argus, a novel framework built upon the core
concept of the Constrained Abstract Query - a SQL skeleton containing
placeholders and their associated instantiation conditions (e.g., requiring a
placeholder to be filled by a boolean column). Argus uses LLMs to generate
pairs of these skeletons that are asserted to be semantically equivalent. This
equivalence is then formally proven using a SQL equivalence solver to ensure
soundness. Finally, the placeholders within the verified skeletons are
instantiated with concrete, reusable SQL snippets that are also synthesized by
LLMs to efficiently produce complex test cases. We implemented Argus and
evaluated it on five extensively tested DBMSs, discovering 40 previously
unknown bugs, 35 of which are logic bugs, with 36 confirmed and 26 already
fixed by the developers.

</details>


### [137] [Relational Database Distillation: From Structured Tables to Condensed Graph Data](https://arxiv.org/abs/2510.06980)
*Xinyi Gao,Jingxi Zhang,Lijian Chen,Tong Chen,Lizhen Cui,Hongzhi Yin*

Main category: cs.DB

TL;DR: 这篇论文提出了一种关系数据库蒸馏（RDD）方法，旨在将大型关系数据库提炼成紧凑的异构图，同时保留预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法由于数据库规模庞大和跨互连表的密集消息传递的计算负担，导致存储开销过高和训练时间过长。

Method: 该方法通过节点特征保留多模态列信息，并通过异构边编码主外键关系，并设计了一种基于核岭回归引导的目标函数与伪标签，以生成高质量的蒸馏图特征。

Result: 在多个真实关系数据库上的大量实验表明，该解决方案在分类和回归任务上保持竞争性能的同时，显著减少了数据大小。

Conclusion: 该方法为使用关系数据库进行可扩展学习创建了一条有效途径。

Abstract: Relational databases (RDBs) underpin the majority of global data management
systems, where information is structured into multiple interdependent tables.
To effectively use the knowledge within RDBs for predictive tasks, recent
advances leverage graph representation learning to capture complex inter-table
relations as multi-hop dependencies. Despite achieving state-of-the-art
performance, these methods remain hindered by the prohibitive storage overhead
and excessive training time, due to the massive scale of the database and the
computational burden of intensive message passing across interconnected tables.
To alleviate these concerns, we propose and study the problem of Relational
Database Distillation (RDD). Specifically, we aim to distill large-scale RDBs
into compact heterogeneous graphs while retaining the predictive power (i.e.,
utility) required for training graph-based models. Multi-modal column
information is preserved through node features, and primary-foreign key
relations are encoded via heterogeneous edges, thereby maintaining both data
fidelity and relational structure. To ensure adaptability across diverse
downstream tasks without engaging the traditional, inefficient bi-level
distillation framework, we further design a kernel ridge regression-guided
objective with pseudo-labels, which produces quality features for the distilled
graph. Extensive experiments on multiple real-world RDBs demonstrate that our
solution substantially reduces the data size while maintaining competitive
performance on classification and regression tasks, creating an effective
pathway for scalable learning with RDBs.

</details>


### [138] [On the Expressiveness of Languages for Querying Property Graphs in Relational Databases](https://arxiv.org/abs/2510.07062)
*Hadar Rotschield,Liat Peterfreund*

Main category: cs.DB

TL;DR: SQL/PGQ是查询关系数据视图上定义的属性图的新兴ISO标准。本文研究了其在三个部分中的表达能力：只读核心、读写扩展和具有更丰富的视图定义的扩展变体。


<details>
  <summary>Details</summary>
Motivation: 图创建在决定表达能力方面起着核心作用。

Method: 形式化了SQL/PGQ在三个部分中的表达能力。

Result: 只读片段严格弱于读写片段，后者仍低于复杂性类别NL。使用任意arity标识符扩展视图定义缩小了这一差距：扩展的片段精确地捕获了NL。这产生了一个SQL/PGQ片段的严格层次结构，其联合覆盖了所有NL查询。在有序结构上，层次结构崩溃：一旦允许arity-2标识符，更高的arity不会增加任何能力，这反映了经典的传递闭包崩溃，并强调了视图构建在属性图查询中的核心作用。

Conclusion: SQL/PGQ片段存在一个严格的层次结构，其联合覆盖了所有NL查询。

Abstract: SQL/PGQ is the emerging ISO standard for querying property graphs defined as
views over relational data. We formalize its expressive power across three
fragments: the read-only core, the read-write extension, and an extended
variant with richer view definitions. Our results show that graph creation
plays a central role in determining the expressiveness. The read-only fragment
is strictly weaker than the read-write fragment, and the latter is still below
the complexity class NL. Extending view definitions with arbitrary arity
identifiers closes this gap: the extended fragment captures exactly NL. This
yields a strict hierarchy of SQL/PGQ fragments, whose union covers all NL
queries. On ordered structures the hierarchy collapses: once arity-2
identifiers are allowed, higher arities add no power, mirroring the classical
transitive-closure collapse and underscoring the central role of view
construction in property graph querying.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [139] [LLM-Powered Nuanced Video Attribute Annotation for Enhanced Recommendations](https://arxiv.org/abs/2510.06657)
*Boyuan Long,Yueqi Wang,Hiloni Mehta,Mick Zomnir,Omkar Pathak,Changping Meng,Ruolin Jia,Yajun Peng,Dapeng Hong,Xia Wu,Mingyan Gao,Onkar Dalal,Ningren Han*

Main category: cs.IR

TL;DR: 本文介绍了一个使用大型语言模型（LLM）作为高级“注释”机制的案例研究，以在大型工业短视频推荐系统中实现细致的内容理解。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习分类器在内容理解方面面临漫长的开发周期和缺乏深入、细致的理解的问题。因此，提出了“LLM-as-annotators”方法。

Method: 该方法包含一个端到端的工作流程：(1) 迭代定义和稳健评估目标属性，通过离线指标和在线 A/B 测试进行改进；(2) 使用具有多模态特征的 LLM 对视频语料库进行可扩展的离线批量注释，优化推理和知识提炼以实现广泛应用；(3) 将这些丰富的注释集成到在线推荐服务系统中，例如，通过个性化限制检索。

Result: 实验结果表明，LLM 在离线注释质量方面优于人工评估者，并在在线 A/B 测试中显着提高了用户参与度和满意度。

Conclusion: 该研究深入了解了如何设计和扩展用于丰富内容评估的生产级 LLM 管道，强调了 LLM 生成的细致理解在增强内容发现、用户满意度和现代推荐系统整体有效性方面的适应性和优势。

Abstract: This paper presents a case study on deploying Large Language Models (LLMs) as
an advanced "annotation" mechanism to achieve nuanced content understanding
(e.g., discerning content "vibe") at scale within a large-scale industrial
short-form video recommendation system. Traditional machine learning
classifiers for content understanding face protracted development cycles and a
lack of deep, nuanced comprehension. The "LLM-as-annotators" approach addresses
these by significantly shortening development times and enabling the annotation
of subtle attributes. This work details an end-to-end workflow encompassing:
(1) iterative definition and robust evaluation of target attributes, refined by
offline metrics and online A/B testing; (2) scalable offline bulk annotation of
video corpora using LLMs with multimodal features, optimized inference, and
knowledge distillation for broad application; and (3) integration of these rich
annotations into the online recommendation serving system, for example, through
personalized restrict retrieval. Experimental results demonstrate the efficacy
of this approach, with LLMs outperforming human raters in offline annotation
quality for nuanced attributes and yielding significant improvements of user
participation and satisfied consumption in online A/B tests. The study provides
insights into designing and scaling production-level LLM pipelines for rich
content evaluation, highlighting the adaptability and benefits of LLM-generated
nuanced understanding for enhancing content discovery, user satisfaction, and
the overall effectiveness of modern recommendation systems.

</details>


### [140] [Can We Hide Machines in the Crowd? Quantifying Equivalence in LLM-in-the-loop Annotation Tasks](https://arxiv.org/abs/2510.06658)
*Jiaman He,Zikang Leng,Dana McKay,Damiano Spina,Johanne R. Trippas*

Main category: cs.IR

TL;DR: 本研究不仅仅关注大型语言模型（LLM）在文本标注中的正确性，更关注LLM是否能像人类一样做出主观判断，并提出了一种统计评估方法来判断LLM是否能融入人类标注者群体。


<details>
  <summary>Details</summary>
Motivation: 以往的评估主要关注LLM输出的正确性，缺乏对LLM主观判断能力的评估。

Method: 基于Krippendorff's alpha、配对bootstrap和TOST等统计方法，评估LLM是否能融入人类标注者群体。

Result: 在MovieLens 100K数据集上，LLM与人类标注者在统计上无法区分（p=0.004），但在PolitiFact数据集上则不然（p=0.155）。

Conclusion: LLM在不同任务上的表现存在差异，该方法可以通过少量人工数据评估LLM是否适合大规模标注。

Abstract: Many evaluations of large language models (LLMs) in text annotation focus
primarily on the correctness of the output, typically comparing model-generated
labels to human-annotated ``ground truth'' using standard performance metrics.
In contrast, our study moves beyond effectiveness alone. We aim to explore how
labeling decisions -- by both humans and LLMs -- can be statistically evaluated
across individuals. Rather than treating LLMs purely as annotation systems, we
approach LLMs as an alternative annotation mechanism that may be capable of
mimicking the subjective judgments made by humans. To assess this, we develop a
statistical evaluation method based on Krippendorff's $\alpha$, paired
bootstrapping, and the Two One-Sided t-Tests (TOST) equivalence test procedure.
This evaluation method tests whether an LLM can blend into a group of human
annotators without being distinguishable.
  We apply this approach to two datasets -- MovieLens 100K and PolitiFact --
and find that the LLM is statistically indistinguishable from a human annotator
in the former ($p = 0.004$), but not in the latter ($p = 0.155$), highlighting
task-dependent differences. It also enables early evaluation on a small sample
of human data to inform whether LLMs are suitable for large-scale annotation in
a given application.

</details>


### [141] [Reproducing and Extending Causal Insights Into Term Frequency Computation in Neural Rankers](https://arxiv.org/abs/2510.06728)
*Cile van Marken,Roxana Petcu*

Main category: cs.IR

TL;DR: 本文重现了 Chen 等人的研究成果，并进一步探索了神经 IR 模型中预定义的检索原理。


<details>
  <summary>Details</summary>
Motivation: 现有神经排序模型内部决策过程不明确，缺乏因果关系解释。

Method: 采用激活修补的因果可解释性方法，扩展框架以包含额外的词频原理。

Result: 验证了 Chen 等人的主要结论，并成功识别出一组编码词频原理的注意力头。

Conclusion: 神经排序模型不仅捕获词频信息，而且这些表示可以定位到模型的特定组件。

Abstract: Neural ranking models have shown outstanding performance across a variety of
tasks, such as document retrieval, re-ranking, question answering and
conversational retrieval. However, the inner decision process of these models
remains largely unclear, especially as models increase in size. Most
interpretability approaches, such as probing, focus on correlational insights
rather than establishing causal relationships. The paper 'Axiomatic Causal
Interventions for Reverse Engineering Relevance Computation in Neural Retrieval
Models' by Chen et al. addresses this gap by introducing a framework for
activation patching - a causal interpretability method - in the information
retrieval domain, offering insights into how neural retrieval models compute
document relevance. The study demonstrates that neural ranking models not only
capture term-frequency information, but also that these representations can be
localized to specific components of the model, such as individual attention
heads or layers. This paper aims to reproduce the findings by Chen et al. and
to further explore the presence of pre-defined retrieval axioms in neural IR
models. We validate the main claims made by Chen et al., and extend the
framework to include an additional term-frequency axiom, which states that the
impact of increasing query term frequency on document ranking diminishes as the
frequency becomes higher. We successfully identify a group of attention heads
that encode this axiom and analyze their behavior to give insight into the
inner decision-making process of neural ranking models.

</details>


### [142] [Crossing Domains without Labels: Distant Supervision for Term Extraction](https://arxiv.org/abs/2510.06838)
*Elena Senger,Yuri Campbell,Rob van der Goot,Barbara Plank*

Main category: cs.IR

TL;DR: 本文介绍了一种新的自动术语提取(ATE)方法，该方法使用大型语言模型(LLM)在多个领域上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前ATE方法需要大量人工标注，并且难以进行领域迁移，限制了它们的实际应用。

Method: 该方法首先使用LLM生成伪标签，然后在这些数据上微调LLM。为了提高文档级别的一致性，引入了轻量级的事后启发式方法。

Result: 该方法在7个领域中的5个领域上超过了以前的方法，平均提高了10个百分点。

Conclusion: 本文发布了数据集和微调模型，以支持该领域的未来研究。

Abstract: Automatic Term Extraction (ATE) is a critical component in downstream NLP
tasks such as document tagging, ontology construction and patent analysis.
Current state-of-the-art methods require expensive human annotation and
struggle with domain transfer, limiting their practical deployment. This
highlights the need for more robust, scalable solutions and realistic
evaluation settings. To address this, we introduce a comprehensive benchmark
spanning seven diverse domains, enabling performance evaluation at both the
document- and corpus-levels. Furthermore, we propose a robust LLM-based model
that outperforms both supervised cross-domain encoder models and few-shot
learning baselines and performs competitively with its GPT-4o teacher on this
benchmark. The first step of our approach is generating psuedo-labels with this
black-box LLM on general and scientific domains to ensure generalizability.
Building on this data, we fine-tune the first LLMs for ATE. To further enhance
document-level consistency, oftentimes needed for downstream tasks, we
introduce lightweight post-hoc heuristics. Our approach exceeds previous
approaches on 5/7 domains with an average improvement of 10 percentage points.
We release our dataset and fine-tuned models to support future research in this
area.

</details>


### [143] [M3Retrieve: Benchmarking Multimodal Retrieval for Medicine](https://arxiv.org/abs/2510.06888)
*Arkadeep Acharya,Akash Ghosh,Pradeepika Verma,Kitsuchart Pasupa,Sriparna Saha,Priti Singh*

Main category: cs.IR

TL;DR: 提出了一个用于评估医疗多模态检索模型的新基准 M3Retrieve。


<details>
  <summary>Details</summary>
Motivation: 现有的医疗领域缺乏评估多模态检索模型的标准基准，而这种模型在医疗问答、跨模态检索和多模态摘要等任务中至关重要。

Method: 构建了一个包含5个领域、16个医学领域和4个不同任务的 M3Retrieve 基准，包含120万文本和16.4万多模态查询，并评估了领先的多模态检索模型。

Result: 通过在 M3Retrieve 基准上评估模型，探索了不同医学专业的挑战及其对检索性能的影响。

Conclusion: 发布 M3Retrieve 旨在促进系统评估、模型创新，并加速构建更强大和可靠的医疗多模态检索系统的研究。

Abstract: With the increasing use of RetrievalAugmented Generation (RAG), strong
retrieval models have become more important than ever. In healthcare,
multimodal retrieval models that combine information from both text and images
offer major advantages for many downstream tasks such as question answering,
cross-modal retrieval, and multimodal summarization, since medical data often
includes both formats. However, there is currently no standard benchmark to
evaluate how well these models perform in medical settings. To address this
gap, we introduce M3Retrieve, a Multimodal Medical Retrieval Benchmark.
M3Retrieve, spans 5 domains,16 medical fields, and 4 distinct tasks, with over
1.2 Million text documents and 164K multimodal queries, all collected under
approved licenses. We evaluate leading multimodal retrieval models on this
benchmark to explore the challenges specific to different medical specialities
and to understand their impact on retrieval performance. By releasing
M3Retrieve, we aim to enable systematic evaluation, foster model innovation,
and accelerate research toward building more capable and reliable multimodal
retrieval systems for medical applications. The dataset and the baselines code
are available in this github page https://github.com/AkashGhosh/M3Retrieve.

</details>


### [144] [Ethical AI prompt recommendations in large language models using collaborative filtering](https://arxiv.org/abs/2510.06924)
*Jordan Nelson,Almas Baimagambetov,Konstantinos Avgerinakis,Nikolaos Polatidis*

Main category: cs.IR

TL;DR: 本研究探讨了大型语言模型（LLM）中确保合乎道德的提示推荐的重要性，并提出了使用协同过滤来增强道德提示选择的方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在人工智能发展中至关重要，但同时也带来了偏差、公平性和问责制等伦理风险。传统监督方法难以扩展，需要动态解决方案。

Method: 利用推荐系统中的协同过滤技术，通过用户互动促进伦理准则，减少偏差。

Result: 创建了一个用于提示推荐的合成数据集，并应用了协同过滤。

Conclusion: 该研究致力于解决伦理人工智能中的挑战，如减少偏差、提高透明度以及防止不道德的提示工程。

Abstract: As large language models (LLMs) shape AI development, ensuring ethical prompt
recommendations is crucial. LLMs offer innovation but risk bias, fairness
issues, and accountability concerns. Traditional oversight methods struggle
with scalability, necessitating dynamic solutions. This paper proposes using
collaborative filtering, a technique from recommendation systems, to enhance
ethical prompt selection. By leveraging user interactions, it promotes ethical
guidelines while reducing bias. Contributions include a synthetic dataset for
prompt recommendations and the application of collaborative filtering. The work
also tackles challenges in ethical AI, such as bias mitigation, transparency,
and preventing unethical prompt engineering.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [145] [RareGraph-Synth: Knowledge-Guided Diffusion Models for Generating Privacy-Preserving Synthetic Patient Trajectories in Ultra-Rare Diseases](https://arxiv.org/abs/2510.06267)
*Khartik Uppalapati,Shakeel Abdulkareem,Bora Yimenicioglu*

Main category: cs.LG

TL;DR: 提出RareGraph-Synth，一种知识引导的连续时间扩散框架，用于生成用于超罕见疾病的逼真但保护隐私的合成电子健康记录（EHR）轨迹。


<details>
  <summary>Details</summary>
Motivation: 为超罕见疾病生成逼真且保护隐私的合成电子健康记录轨迹，以促进更安全的数据共享，从而促进罕见疾病的研究。

Method: 将Orphanet/Orphadata、人类表型本体（HPO）、GARD罕见疾病知识图谱、PrimeKG和FDA不良事件报告系统（FAERS）五个公共资源整合到一个包含约800万个类型化边的异构知识图谱中。从该知识图谱中提取的元路径评分调节前向随机微分方程中的每个token的噪声计划，引导生成生物学上合理的实验室-药物-不良事件共现，同时保持基于分数的扩散模型稳定性。逆向去噪器然后生成包含时间戳的实验室代码、药物代码和不良事件标志三元组序列，其中不包含受保护的健康信息。

Result: 在模拟的超罕见疾病队列中，相对于无引导的扩散基线，RareGraph-Synth将分类最大平均差异降低了40%，相对于GAN降低了60%以上，且不牺牲下游预测效用。使用DOMIAS攻击者的黑盒成员推断评估产生约0.53的AUROC，远低于0.55的安全发布阈值，并且明显优于非知识图谱基线的约0.61加/减0.03，表明对重新识别具有很强的抵抗力。

Conclusion: 将生物医学知识图谱直接整合到扩散噪声计划中可以同时提高保真度和隐私性，从而为罕见疾病研究实现更安全的数据共享。

Abstract: We propose RareGraph-Synth, a knowledge-guided, continuous-time diffusion
framework that generates realistic yet privacy-preserving synthetic
electronic-health-record (EHR) trajectories for ultra-rare diseases.
RareGraph-Synth unifies five public resources: Orphanet/Orphadata, the Human
Phenotype Ontology (HPO), the GARD rare-disease KG, PrimeKG, and the FDA
Adverse Event Reporting System (FAERS) into a heterogeneous knowledge graph
comprising approximately 8 M typed edges. Meta-path scores extracted from this
8-million-edge KG modulate the per-token noise schedule in the forward
stochastic differential equation, steering generation toward biologically
plausible lab-medication-adverse-event co-occurrences while retaining
score-based diffusion model stability. The reverse denoiser then produces
timestamped sequences of lab-code, medication-code, and adverse-event-flag
triples that contain no protected health information. On simulated
ultra-rare-disease cohorts, RareGraph-Synth lowers categorical Maximum Mean
Discrepancy by 40 percent relative to an unguided diffusion baseline and by
greater than 60 percent versus GAN counterparts, without sacrificing downstream
predictive utility. A black-box membership-inference evaluation using the
DOMIAS attacker yields AUROC approximately 0.53, well below the 0.55
safe-release threshold and substantially better than the approximately 0.61
plus or minus 0.03 observed for non-KG baselines, demonstrating strong
resistance to re-identification. These results suggest that integrating
biomedical knowledge graphs directly into diffusion noise schedules can
simultaneously enhance fidelity and privacy, enabling safer data sharing for
rare-disease research.

</details>


### [146] [MCCE: A Framework for Multi-LLM Collaborative Co-Evolution](https://arxiv.org/abs/2510.06270)
*Nian Ran,Zhongzheng Li,Yue Wang,Qingsong Ran,Xiaoyuan Zhang,Shikun Feng,Richard Allmendinger,Xiaoguang Zhao*

Main category: cs.LG

TL;DR: 提出了一种混合框架，结合了冻结的闭源LLM和轻量级可训练模型，以解决多目标离散优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统进化算法容易陷入局部最优，而专家知识可以为加速收敛提供关键指导。大型语言模型(LLM)提供了强大的先验知识和推理能力，使其成为专家知识重要时的自然优化器。然而，闭源LLM无法更新参数，而小型开放模型缺乏广泛的知识和推理能力。

Method: 引入多LLM协同进化(MCCE)，通过强化学习逐步改进小型模型，两个模型共同支持和补充全局探索。

Result: 在多目标药物设计基准测试中，MCCE实现了最先进的Pareto前沿质量，并始终优于基线。

Conclusion: MCCE展示了一种新的混合LLM系统中持续进化的范例，结合了知识驱动的探索和经验驱动的学习。

Abstract: Multi-objective discrete optimization problems, such as molecular design,
pose significant challenges due to their vast and unstructured combinatorial
spaces. Traditional evolutionary algorithms often get trapped in local optima,
while expert knowledge can provide crucial guidance for accelerating
convergence. Large language models (LLMs) offer powerful priors and reasoning
ability, making them natural optimizers when expert knowledge matters. However,
closed-source LLMs, though strong in exploration, cannot update their
parameters and thus cannot internalize experience. Conversely, smaller open
models can be continually fine-tuned but lack broad knowledge and reasoning
strength. We introduce Multi-LLM Collaborative Co-evolution (MCCE), a hybrid
framework that unites a frozen closed-source LLM with a lightweight trainable
model. The system maintains a trajectory memory of past search processes; the
small model is progressively refined via reinforcement learning, with the two
models jointly supporting and complementing each other in global exploration.
Unlike model distillation, this process enhances the capabilities of both
models through mutual inspiration. Experiments on multi-objective drug design
benchmarks show that MCCE achieves state-of-the-art Pareto front quality and
consistently outperforms baselines. These results highlight a new paradigm for
enabling continual evolution in hybrid LLM systems, combining knowledge-driven
exploration with experience-driven learning.

</details>


### [147] [RVFL-X: A Novel Randomized Network Based on Complex Transformed Real-Valued Tabular Datasets](https://arxiv.org/abs/2510.06278)
*M. Sajid,Mushir Akhtar,A. Quadir,M. Tanveer*

Main category: cs.LG

TL;DR: 本文提出了一种名为RVFL-X的复值随机向量函数链接网络，它扩展了传统的RVFL网络，利用复数表示来处理实值数据，并在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管复数神经网络具有强大的表示能力，但缺乏将实值表格数据转换为复值表示的有效方法，限制了其在随机神经网络中的应用。

Method: 提出了两种从实值数据集生成复值表示的方法：自然变换和自编码器驱动方法。基于这些方法，构建了RVFL-X网络，它将复数变换集成到实值数据中，同时保持了原始RVFL架构的简单性和效率。

Result: 在80个UCI数据集上的综合评估表明，RVFL-X始终优于原始RVFL和最先进的RNN变体。

Conclusion: RVFL-X在各种应用领域中表现出鲁棒性和有效性。

Abstract: Recent advancements in neural networks, supported by foundational theoretical
insights, emphasize the superior representational power of complex numbers.
However, their adoption in randomized neural networks (RNNs) has been limited
due to the lack of effective methods for transforming real-valued tabular
datasets into complex-valued representations. To address this limitation, we
propose two methods for generating complex-valued representations from
real-valued datasets: a natural transformation and an autoencoder-driven
method. Building on these mechanisms, we propose RVFL-X, a complex-valued
extension of the random vector functional link (RVFL) network. RVFL-X
integrates complex transformations into real-valued datasets while maintaining
the simplicity and efficiency of the original RVFL architecture. By leveraging
complex components such as input, weights, and activation functions, RVFL-X
processes complex representations and produces real-valued outputs.
Comprehensive evaluations on 80 real-valued UCI datasets demonstrate that
RVFL-X consistently outperforms both the original RVFL and state-of-the-art
(SOTA) RNN variants, showcasing its robustness and effectiveness across diverse
application domains.

</details>


### [148] [On knot detection via picture recognition](https://arxiv.org/abs/2510.06284)
*Anne Dranowski,Yura Kabkov,Daniel Tubbenhauer*

Main category: cs.LG

TL;DR: 使用机器学习方法（卷积神经网络和transformers）和传统算法（计算琼斯多项式等量子不变量）相结合，以实现从结的照片自动识别结的目标。


<details>
  <summary>Details</summary>
Motivation: 最终目标是拍摄一个结的照片，并让手机自动识别它。这篇解释性文章解释了一种近似实现此目标的策略。

Method: 使用现代机器学习方法（特别是用于图像识别的卷积神经网络和transformers）和传统算法（计算琼斯多项式等量子不变量）。

Result: 提出了直接从图像预测交叉数的简单基线，表明即使是轻量级的CNN和transformer架构也可以恢复有意义的结构信息。

Conclusion: 结合感知模块与平面图（PD）代码的符号重建，实现下游不变计算，从而实现鲁棒的结分类。这种两阶段方法突出了机器学习（处理嘈杂的视觉数据）和不变量（强制执行严格的拓扑区分）之间的互补性。

Abstract: Our goal is to one day take a photo of a knot and have a phone automatically
recognize it. In this expository work, we explain a strategy to approximate
this goal, using a mixture of modern machine learning methods (in particular
convolutional neural networks and transformers for image recognition) and
traditional algorithms (to compute quantum invariants like the Jones
polynomial). We present simple baselines that predict crossing number directly
from images, showing that even lightweight CNN and transformer architectures
can recover meaningful structural information. The longer-term aim is to
combine these perception modules with symbolic reconstruction into planar
diagram (PD) codes, enabling downstream invariant computation for robust knot
classification. This two-stage approach highlights the complementarity between
machine learning, which handles noisy visual data, and invariants, which
enforce rigorous topological distinctions.

</details>


### [149] [Traj-Transformer: Diffusion Models with Transformer for GPS Trajectory Generation](https://arxiv.org/abs/2510.06291)
*Zhiyang Zhang,Ningcong Chen,Xin Zhang,Yanhua Li,Shen Su,Hui Lu,Jun Luo*

Main category: cs.LG

TL;DR: 提出了一种名为轨迹转换器（Trajectory Transformer）的新模型，该模型采用 Transformer 主干进行条件信息嵌入和噪声预测，以提高轨迹生成质量并有效缓解现有方法中存在的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于基于卷积的架构来预测扩散过程中的噪声，由于模型容量有限，通常会导致明显的偏差和细粒度的街道级别细节的丢失。

Method: 提出 Trajectory Transformer 模型，该模型使用 Transformer 主干进行条件信息嵌入和噪声预测，并探索了两种 GPS 坐标嵌入策略：位置嵌入和经纬度嵌入。

Result: 在两个真实世界数据集上的实验表明，Trajectory Transformer 显着提高了生成质量，并有效缓解了先前方法中观察到的偏差问题。

Conclusion: Trajectory Transformer 模型可以提高轨迹生成质量，并有效缓解现有方法中存在的偏差问题。

Abstract: The widespread use of GPS devices has driven advances in spatiotemporal data
mining, enabling machine learning models to simulate human decision making and
generate realistic trajectories, addressing both data collection costs and
privacy concerns. Recent studies have shown the promise of diffusion models for
high-quality trajectory generation. However, most existing methods rely on
convolution based architectures (e.g. UNet) to predict noise during the
diffusion process, which often results in notable deviations and the loss of
fine-grained street-level details due to limited model capacity. In this paper,
we propose Trajectory Transformer, a novel model that employs a transformer
backbone for both conditional information embedding and noise prediction. We
explore two GPS coordinate embedding strategies, location embedding and
longitude-latitude embedding, and analyze model performance at different
scales. Experiments on two real-world datasets demonstrate that Trajectory
Transformer significantly enhances generation quality and effectively
alleviates the deviation issues observed in prior approaches.

</details>


### [150] [Spiral Model Technique For Data Science & Machine Learning Lifecycle](https://arxiv.org/abs/2510.06987)
*Rohith Mahadevan*

Main category: cs.LG

TL;DR: 提出了一种新的数据科学生命周期技术，称为螺旋技术，以强调业务流程的多功能性、敏捷性和迭代方法。


<details>
  <summary>Details</summary>
Motivation: 公司调整数据科学生命周期以适应其文化，从而提高生产力并提高竞争力。数据科学生命周期是启动和结束依赖数据的项目的重要因素。

Method: 提出了一种称为螺旋技术的新技术，将数据科学生命周期融入到具有明确最终目标的业务问题中。

Result: 论文提出了一个新的技术

Conclusion: 论文提出了一个新的螺旋技术

Abstract: Analytics play an important role in modern business. Companies adapt data
science lifecycles to their culture to seek productivity and improve their
competitiveness among others. Data science lifecycles are fairly an important
contributing factor to start and end a project that are data dependent. Data
science and Machine learning life cycles comprises of series of steps that are
involved in a project. A typical life cycle states that it is a linear or
cyclical model that revolves around. It is mostly depicted that it is possible
in a traditional data science life cycle to start the process again after
reaching the end of cycle. This paper suggests a new technique to incorporate
data science life cycle to business problems that have a clear end goal. A new
technique called spiral technique is introduced to emphasize versatility,
agility and iterative approach to business processes.

</details>


### [151] [Relational Transformer: Toward Zero-Shot Foundation Models for Relational Data](https://arxiv.org/abs/2510.06377)
*Rishabh Ranjan,Valter Hudovernik,Mark Znidar,Charilaos Kanatsoulis,Roshan Upendra,Mahmoud Mohammadi,Joe Meyer,Tom Palczewski,Carlos Guestrin,Jure Leskovec*

Main category: cs.LG

TL;DR: 提出了关系Transformer (RT) 架构，用于关系数据的零样本迁移学习。


<details>
  <summary>Details</summary>
Motivation: 关系领域缺乏能够跨数据集和任务迁移的架构，核心挑战是关系数据的多样性。

Method: RT 通过 (i) 使用表/列元数据标记单元格，(ii) 通过掩码token预测进行预训练，以及 (iii) 利用一种新的关系注意力机制来实现。

Result: RT 在 RelBench 数据集上获得了强大的零样本性能，在二元分类任务中平均达到完全监督 AUROC 的 94%。

Conclusion: RT 为关系数据的基础模型提供了一条可行的路径。

Abstract: Pretrained transformers readily adapt to new sequence modeling tasks via
zero-shot prompting, but relational domains still lack architectures that
transfer across datasets and tasks. The core challenge is the diversity of
relational data, with varying heterogeneous schemas, graph structures and
functional dependencies. In this paper, we present the Relational Transformer
(RT) architecture, which can be pretrained on diverse relational databases and
directly applied to unseen datasets and tasks without task- or dataset-specific
fine-tuning, or retrieval of in-context examples. RT (i) tokenizes cells with
table/column metadata, (ii) is pretrained via masked token prediction, and
(iii) utilizes a novel \textit{Relational Attention} mechanism over columns,
rows, and primary-foreign key links. Pretrained on RelBench datasets spanning
tasks such as churn and sales forecasting, RT attains strong zero-shot
performance, averaging 94% of fully supervised AUROC on binary classification
tasks with a single forward pass of a 22M parameter model, as opposed to 84%
for a 27B LLM. Fine-tuning yields state-of-the-art results with high sample
efficiency. Our experiments show that RT's zero-shot transfer harnesses
task-table context, relational attention patterns and schema semantics.
Overall, RT provides a practical path toward foundation models for relational
data.

</details>


### [152] [BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression](https://arxiv.org/abs/2510.06293)
*Cristian Meo,Varun Sarathchandran,Avijit Majhi,Shao Hung,Carlo Saccardi,Ruben Imhoff,Roberto Deidda,Remko Uijlenhoet,Justin Dauwels*

Main category: cs.LG

TL;DR: 提出了BlockGPT，一种用于视频预测的生成式自回归Transformer，通过分批令牌化方法预测完整的二维场，并将其应用于降水临近预报。


<details>
  <summary>Details</summary>
Motivation: 现有的基于令牌的自回归模型存在归纳偏置缺陷和推理速度慢的问题，而扩散模型计算量大。

Method: BlockGPT使用分批令牌化方法，在每个帧内使用自注意力，跨帧使用因果注意力，从而分解时空。

Result: 在KNMI和SEVIR两个降水数据集上，BlockGPT在准确性、事件定位（通过分类指标测量）和推理速度方面优于现有模型，推理速度比同类模型快31倍。

Conclusion: BlockGPT在降水临近预报任务上表现出色，具有更高的准确性和更快的推理速度。

Abstract: Predicting precipitation maps is a highly complex spatiotemporal modeling
task, critical for mitigating the impacts of extreme weather events. Short-term
precipitation forecasting, or nowcasting, requires models that are not only
accurate but also computationally efficient for real-time applications. Current
methods, such as token-based autoregressive models, often suffer from flawed
inductive biases and slow inference, while diffusion models can be
computationally intensive. To address these limitations, we introduce BlockGPT,
a generative autoregressive transformer using batched tokenization (Block)
method that predicts full two-dimensional fields (frames) at each time step.
Conceived as a model-agnostic paradigm for video prediction, BlockGPT
factorizes space-time by using self-attention within each frame and causal
attention across frames; in this work, we instantiate it for precipitation
nowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI
(Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines
including token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet)
models. The results show that BlockGPT achieves superior accuracy, event
localization as measured by categorical metrics, and inference speeds up to 31x
faster than comparable baselines.

</details>


### [153] [SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation](https://arxiv.org/abs/2510.06303)
*Shuang Cheng,Yihan Bian,Dawei Liu,Yuhua Jiang,Yihao Liu,Linfeng Zhang,Wenhai Wang,Qipeng Guo,Kai Chen,Biqing Qi,Bowen Zhou*

Main category: cs.LG

TL;DR: SDAR: Combines autoregressive (AR) efficiency with diffusion's parallel inference by converting a trained AR model into a blockwise diffusion model. It maintains AR performance, enables parallel generation, and shows enhanced reasoning and adaptability, especially in large models.


<details>
  <summary>Details</summary>
Motivation: To unify the training efficiency of autoregressive models with the parallel inference capability of diffusion models, overcoming the high computational cost of end-to-end diffusion training.

Method: A lightweight paradigm conversion transforms a well-trained AR model into a blockwise diffusion model through brief, data-efficient adaptation. During inference, sequences are generated autoregressively across blocks, with parallel decoding within each block via a discrete diffusion process.

Result: SDAR achieves efficient AR-to-diffusion conversion with minimal cost, preserving AR-level performance while enabling parallel generation. Larger models exhibit stronger robustness to block size and decoding thresholds, yielding greater speedups without accuracy loss. The 30B MoE model surpasses its AR counterpart on scientific reasoning benchmarks.

Conclusion: SDAR is a practical paradigm that combines the strengths of autoregression and diffusion for scalable, high-throughput reasoning. It demonstrates enhanced reasoning and domain adaptability, making it a valuable approach for various applications.

Abstract: We propose SDAR, a Synergistic Diffusion-Autoregression paradigm that unifies
the training efficiency of autoregressive models with the parallel inference
capability of diffusion. Instead of costly end-to-end diffusion training, SDAR
performs a lightweight paradigm conversion that transforms a well-trained
autoregressive (AR) model into a blockwise diffusion model through brief,
data-efficient adaptation. During inference, SDAR generates sequences
autoregressively across blocks for global coherence while decoding all tokens
within each block in parallel via a discrete diffusion process. Extensive
experiments show that AR models remain substantially more compute-efficient
than masked diffusion models, providing a strong foundation for adaptation.
Building on this insight, SDAR achieves efficient AR-to-diffusion conversion
with minimal cost, preserving AR-level performance while enabling parallel
generation. Scaling studies across dense and Mixture-of-Experts architectures
confirm that SDAR scales without compromise: larger models exhibit stronger
robustness to block size and decoding thresholds, yielding greater speedups
without accuracy loss. Beyond efficiency, SDAR demonstrates enhanced reasoning
and domain adaptability. Our 30B MoE model surpasses its AR counterpart on
challenging scientific reasoning benchmarks such as GPQA and ChemBench, and
gains further improvements under test-time scaling methods like majority voting
and pass@k. Together, these results establish SDAR as a practical paradigm that
combines the strengths of autoregression and diffusion for scalable,
high-throughput reasoning.

</details>


### [154] [Flexible Swarm Learning May Outpace Foundation Models in Essential Tasks](https://arxiv.org/abs/2510.06349)
*Moein E. Samadi,Andreas Schuppert*

Main category: cs.LG

TL;DR: 本文探讨了基础模型在现实世界中超越人类决策的可能性，并指出在动态环境中调整复杂系统是关键挑战。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能发展迅速，但许多应用领域，如重症监护中的疾病诊断和治疗，进展有限。在复杂和动态环境中优化系统，需要可靠的自适应建模。

Method: 提出了一种分散式架构，即交互式小代理网络(SANs)，其中每个代理代表系统的专门子结构，仅覆盖部分系统功能。通过swarm-learning，SANs能够在动态环境中实现优于单体基础模型的决策。

Result: 通过数学结果和现有应用证据表明，与单体基础模型相比，自适应SANs能够在动态环境中提供卓越的决策能力，但代价是细节的可重复性降低。

Conclusion: 维度诅咒是高效自适应的根本障碍，单体基础模型在克服这一障碍方面面临概念限制。分散式架构的SANs为解决这一问题提供了一种有前景的替代方案。

Abstract: Foundation models have rapidly advanced AI, raising the question of whether
their decisions will ultimately surpass human strategies in real-world domains.
The exponential, and possibly super-exponential, pace of AI development makes
such analysis elusive. Nevertheless, many application areas that matter for
daily life and society show only modest gains so far; a prominent case is
diagnosing and treating dynamically evolving disease in intensive care.
  The common challenge is adapting complex systems to dynamic environments.
Effective strategies must optimize outcomes in systems composed of strongly
interacting functions while avoiding shared side effects; this requires
reliable, self-adaptive modeling. These tasks align with building digital twins
of highly complex systems whose mechanisms are not fully or quantitatively
understood. It is therefore essential to develop methods for self-adapting AI
models with minimal data and limited mechanistic knowledge. As this challenge
extends beyond medicine, AI should demonstrate clear superiority in these
settings before assuming broader decision-making roles.
  We identify the curse of dimensionality as a fundamental barrier to efficient
self-adaptation and argue that monolithic foundation models face conceptual
limits in overcoming it. As an alternative, we propose a decentralized
architecture of interacting small agent networks (SANs). We focus on agents
representing the specialized substructure of the system, where each agent
covers only a subset of the full system functions. Drawing on mathematical
results on the learning behavior of SANs and evidence from existing
applications, we argue that swarm-learning in diverse swarms can enable
self-adaptive SANs to deliver superior decision-making in dynamic environments
compared with monolithic foundation models, though at the cost of reduced
reproducibility in detail.

</details>


### [155] [PIKAN: Physics-Inspired Kolmogorov-Arnold Networks for Explainable UAV Channel Modelling](https://arxiv.org/abs/2510.06355)
*Kürşat Tekbıyık,Güneş Karabulut Kurt,Antoine Lesage-Landry*

Main category: cs.LG

TL;DR: 提出了一种新的无人机（UAV）空对地（A2G）信道建模方法，该方法兼具准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的确定性模型缺乏灵活性，而深度学习模型缺乏可解释性。

Method: 提出了Physics-Inspired Kolmogorov-Arnold Network (PIKAN)，将物理原理嵌入到学习过程中。

Result: PIKAN在UAV A2G测量数据上实现了与深度学习模型相当的精度，同时提供了与传播规律一致的符号化和可解释的表达式。PIKAN仅用232个参数就实现了这一性能，比具有数千个参数的多层感知器（MLP）基线轻37倍。

Conclusion: PIKAN是beyond-5G和6G网络中UAV信道建模的一种高效、可解释和可扩展的解决方案。

Abstract: Unmanned aerial vehicle (UAV) communications demand accurate yet
interpretable air-to-ground (A2G) channel models that can adapt to
nonstationary propagation environments. While deterministic models offer
interpretability and deep learning (DL) models provide accuracy, both
approaches suffer from either rigidity or a lack of explainability. To bridge
this gap, we propose the Physics-Inspired Kolmogorov-Arnold Network (PIKAN)
that embeds physical principles (e.g., free-space path loss, two-ray
reflections) into the learning process. Unlike physics-informed neural networks
(PINNs), PIKAN is more flexible for applying physical information because it
introduces them as flexible inductive biases. Thus, it enables a more flexible
training process. Experiments on UAV A2G measurement data show that PIKAN
achieves comparable accuracy to DL models while providing symbolic and
explainable expressions aligned with propagation laws. Remarkably, PIKAN
achieves this performance with only 232 parameters, making it up to 37 times
lighter than multilayer perceptron (MLP) baselines with thousands of
parameters, without sacrificing correlation with measurements and also
providing symbolic expressions. These results highlight PIKAN as an efficient,
interpretable, and scalable solution for UAV channel modelling in beyond-5G and
6G networks.

</details>


### [156] [Lagrangian neural ODEs: Measuring the existence of a Lagrangian with Helmholtz metrics](https://arxiv.org/abs/2510.06367)
*Luca Wolf,Tobias Buck,Bjoern Malte Schaefer*

Main category: cs.LG

TL;DR: 提出Helmholtz度量来量化一个给定的ODE与Euler-Lagrange方程的相似性，并将其与二阶神经ODE结合，形成拉格朗日神经ODE，可以直接学习Euler-Lagrange方程，且没有额外的推理成本。


<details>
  <summary>Details</summary>
Motivation: 并非所有解在物理学中都是物理的，因为它是Euler-Lagrange方程。

Method: 提出了Helmholtz度量，并将其与二阶神经ODE结合。

Result: 仅使用位置数据，他们可以区分拉格朗日和非拉格朗日系统，并改进神经ODE解决方案。

Conclusion: 拉格朗日神经ODE可以直接学习Euler-Lagrange方程，且没有额外的推理成本，并且可以使用位置数据来区分拉格朗日和非拉格朗日系统，并改进神经ODE解决方案。

Abstract: Neural ODEs are a widely used, powerful machine learning technique in
particular for physics. However, not every solution is physical in that it is
an Euler-Lagrange equation. We present Helmholtz metrics to quantify this
resemblance for a given ODE and demonstrate their capabilities on several
fundamental systems with noise. We combine them with a second order neural ODE
to form a Lagrangian neural ODE, which allows to learn Euler-Lagrange equations
in a direct fashion and with zero additional inference cost. We demonstrate
that, using only positional data, they can distinguish Lagrangian and
non-Lagrangian systems and improve the neural ODE solutions.

</details>


### [157] [Monte Carlo Permutation Search](https://arxiv.org/abs/2510.06381)
*Tristan Cazenave*

Main category: cs.LG

TL;DR: 提出了蒙特卡洛置换搜索 (MCPS) 算法，该算法改进了 GRAVE 算法，适用于深度强化学习不可行或计算能力有限的情况。


<details>
  <summary>Details</summary>
Motivation: 在深度强化学习不可行或预计算能力不足的情况下，例如在通用游戏博弈中，需要一种更有效的搜索算法。

Method: MCPS 的原理是在节点的探索项中包含所有包含从根到节点路径上所有移动的 playout 的统计信息。

Result: MCPS 在所有双人游戏中都比 GRAVE 算法有更好的结果。对于多人游戏，结果是等效的，因为即使玩家具有不同的优势，这些游戏本质上也是平衡的。使用抽象代码代替精确代码可以改善置换统计和 AMAF 统计。

Conclusion: MCPS 算法通过改进置换统计和消除超参数，在多种游戏类型中表现出优于 GRAVE 算法的性能。

Abstract: We propose Monte Carlo Permutation Search (MCPS), a general-purpose Monte
Carlo Tree Search (MCTS) algorithm that improves upon the GRAVE algorithm. MCPS
is relevant when deep reinforcement learning is not an option, or when the
computing power available before play is not substantial, such as in General
Game Playing, for example. The principle of MCPS is to include in the
exploration term of a node the statistics on all the playouts that contain all
the moves on the path from the root to the node. We extensively test MCPS on a
variety of games: board games, wargame, investment game, video game and
multi-player games. MCPS has better results than GRAVE in all the two-player
games. It has equivalent results for multi-player games because these games are
inherently balanced even when players have different strengths. We also show
that using abstract codes for moves instead of exact codes can be beneficial to
both MCPS and GRAVE, as they improve the permutation statistics and the AMAF
statistics. We also provide a mathematical derivation of the formulas used for
weighting the three sources of statistics. These formulas are an improvement on
the GRAVE formula since they no longer use the bias hyperparameter of GRAVE.
Moreover, MCPS is not sensitive to the ref hyperparameter.

</details>


### [158] [Making and Evaluating Calibrated Forecasts](https://arxiv.org/abs/2510.06388)
*Yuxuan Lu,Yifan Wu,Jason Hartline,Lunjia Hu*

Main category: cs.LG

TL;DR: 本文提出了一种用于多分类预测任务的完全真实的校准度量，并研究了从二元到多类预测扩展校准度量的常用方法，确定了哪些方法可以或不能保持真实性。


<details>
  <summary>Details</summary>
Motivation: 设计合适的校准度量对于评估预测器的错误校准水平至关重要。之前的校准度量都是非真实的，直到 Hartline 等人 [2025] 引入了第一个用于批量设置中二元预测任务的完全真实的校准度量。

Method: 本文提出了一种完全真实的校准度量，推广了 Hartline 等人 [2025] 的工作，使其超越了二元预测。研究了从二元到多类预测扩展校准度量的常用方法，并确定了哪些方法可以或不能保持真实性。

Result: 本文提出的校准度量具有优越的鲁棒性，可以稳健地保持优势和劣势预测器之间的顺序，而与超参数（bin 大小）的选择无关。该结果解决了先前工作中反复观察到的分箱 ECE 的非鲁棒性问题。

Conclusion: 本文成功地为多分类预测任务设计了一种完全真实的校准度量，并证明了其优越的鲁棒性。

Abstract: Calibrated predictions can be reliably interpreted as probabilities. An
important step towards achieving better calibration is to design an appropriate
calibration measure to meaningfully assess the miscalibration level of a
predictor. A recent line of work initiated by Haghtalab et al. [2024] studies
the design of truthful calibration measures: a truthful measure is minimized
when a predictor outputs the true probabilities, whereas a non-truthful measure
incentivizes the predictor to lie so as to appear more calibrated. All previous
calibration measures were non-truthful until Hartline et al. [2025] introduced
the first perfectly truthful calibration measures for binary prediction tasks
in the batch setting.
  We introduce a perfectly truthful calibration measure for multi-class
prediction tasks, generalizing the work of Hartline et al. [2025] beyond binary
prediction. We study common methods of extending calibration measures from
binary to multi-class prediction and identify ones that do or do not preserve
truthfulness. In addition to truthfulness, we mathematically prove and
empirically verify that our calibration measure exhibits superior robustness:
it robustly preserves the ordering between dominant and dominated predictors,
regardless of the choice of hyperparameters (bin sizes). This result addresses
the non-robustness issue of binned ECE, which has been observed repeatedly in
prior work.

</details>


### [159] [Geometry-Aware Backdoor Attacks: Leveraging Curvature in Hyperbolic Embeddings](https://arxiv.org/abs/2510.06397)
*Ali Baheri*

Main category: cs.LG

TL;DR: 非欧几里德基础模型将表示置于弯曲空间中，例如双曲几何。这种几何结构会产生边界驱动的非对称性，后门触发器可以利用它。


<details>
  <summary>Details</summary>
Motivation: 这种几何结构会产生边界驱动的非对称性，后门触发器可以利用它。在边界附近，小的输入变化对于标准输入空间检测器来说显得很微妙，但会在模型的表示空间中产生不成比例的大量移动。

Method: 形式化了这种效应，并揭示了防御的局限性：通过沿半径向内拉点的方法可以抑制这种触发器，但代价是牺牲了同一方向上有用的模型灵敏度。提出了一种简单的几何自适应触发器，并在任务和架构中对其进行了评估。

Result: 攻击成功率向边界增加，而传统检测器减弱，反映了理论趋势。

Conclusion: 这些结果表明了非欧几里德模型中特定于几何结构的漏洞，并为设计和理解防御的局限性提供了分析支持的指导。

Abstract: Non-Euclidean foundation models increasingly place representations in curved
spaces such as hyperbolic geometry. We show that this geometry creates a
boundary-driven asymmetry that backdoor triggers can exploit. Near the
boundary, small input changes appear subtle to standard input-space detectors
but produce disproportionately large shifts in the model's representation
space. Our analysis formalizes this effect and also reveals a limitation for
defenses: methods that act by pulling points inward along the radius can
suppress such triggers, but only by sacrificing useful model sensitivity in
that same direction. Building on these insights, we propose a simple
geometry-adaptive trigger and evaluate it across tasks and architectures.
Empirically, attack success increases toward the boundary, whereas conventional
detectors weaken, mirroring the theoretical trends. Together, these results
surface a geometry-specific vulnerability in non-Euclidean models and offer
analysis-backed guidance for designing and understanding the limits of
defenses.

</details>


### [160] [The Effect of Label Noise on the Information Content of Neural Representations](https://arxiv.org/abs/2510.06401)
*Ali Hussaini Umar,Franky Kevin Nando Tezoh,Jean Barbier,Santiago Acevedo,Alessandro Laio*

Main category: cs.LG

TL;DR: 本文研究了标签噪声对深度学习模型隐藏层表示的影响，发现隐藏层的信息含量随着网络参数数量的变化呈现双重下降趋势。在高参数化状态下，网络表示对标签噪声具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究标签噪声对深度学习模型性能的影响已被广泛研究，但其对网络隐藏层表示的影响仍知之甚少。

Method: 通过信息失衡（一种条件互信息的计算有效代理）系统地比较隐藏层表示。

Result: 观察到隐藏层的信息含量随着网络参数数量的变化呈现双重下降趋势；在欠参数化状态下，用噪声标签学习的表示比用干净标签学习的表示更具信息性，而在过参数化状态下，这些表示的信息量相同；过度参数化网络的表示对标签噪声具有鲁棒性；倒数第二层和 pre-softmax 层之间的信息失衡随着过度参数化状态下的交叉熵损失而降低；随机标签的表现比随机特征差。

Conclusion: 过度参数化网络的表示对标签噪声具有鲁棒性，并且在分类任务中，倒数第二层和 pre-softmax 层之间的信息失衡随着交叉熵损失而降低。此外，用随机标签训练网络会使其超出惰性学习的范围，因为权重会适应以编码标签信息。

Abstract: In supervised classification tasks, models are trained to predict a label for
each data point. In real-world datasets, these labels are often noisy due to
annotation errors. While the impact of label noise on the performance of deep
learning models has been widely studied, its effects on the networks' hidden
representations remain poorly understood. We address this gap by systematically
comparing hidden representations using the Information Imbalance, a
computationally efficient proxy of conditional mutual information. Through this
analysis, we observe that the information content of the hidden representations
follows a double descent as a function of the number of network parameters,
akin to the behavior of the test error. We further demonstrate that in the
underparameterized regime, representations learned with noisy labels are more
informative than those learned with clean labels, while in the
overparameterized regime, these representations are equally informative. Our
results indicate that the representations of overparameterized networks are
robust to label noise. We also found that the information imbalance between the
penultimate and pre-softmax layers decreases with cross-entropy loss in the
overparameterized regime. This offers a new perspective on understanding
generalization in classification tasks. Extending our analysis to
representations learned from random labels, we show that these perform worse
than random features. This indicates that training on random labels drives
networks much beyond lazy learning, as weights adapt to encode labels
information.

</details>


### [161] [Test-Time Efficient Pretrained Model Portfolios for Time Series Forecasting](https://arxiv.org/abs/2510.06419)
*Mert Kayaalp,Caner Turkmen,Oleksandr Shchur,Pedro Mercado,Abdul Fatir Ansari,Michael Bohlke-Schneider,Bernie Wang*

Main category: cs.LG

TL;DR: 探索小型预训练时间序列预测模型组合的有效性，作为大型单一模型的替代方案。


<details>
  <summary>Details</summary>
Motivation: 研究更大的时间序列基础模型是否总是更好，并寻找更有效的替代方案。

Method: 构建小型预训练预测模型组合，并应用集成或模型选择。

Result: 使用更少的参数，在大型基准测试中实现了有竞争力的性能；专家模型集合优于独立训练的通用模型集合；后训练基础模型是创建足够多样化的专家模型的有效计算方法；集成和模型选择比测试时微调更有效。

Conclusion: 小型预训练模型组合在时间序列预测中具有竞争力，且更有效。

Abstract: Is bigger always better for time series foundation models? With the question
in mind, we explore an alternative to training a single, large monolithic
model: building a portfolio of smaller, pretrained forecasting models. By
applying ensembling or model selection over these portfolios, we achieve
competitive performance on large-scale benchmarks using much fewer parameters.
We explore strategies for designing such portfolios and find that collections
of specialist models consistently outperform portfolios of independently
trained generalists. Remarkably, we demonstrate that post-training a base model
is a compute-effective approach for creating sufficiently diverse specialists,
and provide evidences that ensembling and model selection are more
compute-efficient than test-time fine-tuning.

</details>


### [162] [Nearly Instance-Optimal Parameter Recovery from Many Trajectories via Hellinger Localization](https://arxiv.org/abs/2510.06434)
*Eliot Shekhtman,Yichen Zhou,Ingvar Ziemann,Nikolai Matni,Stephen Tu*

Main category: cs.LG

TL;DR: 这篇论文研究了多轨迹设置下的时序数据学习问题，旨在找到在更广泛条件下实现与完整数据预算相匹配的实例最优边界。


<details>
  <summary>Details</summary>
Motivation: 现有的序列学习理解不完整，尤其是在多轨迹设置下。多轨迹设置反映了现代训练流程，并提供了在没有典型混合假设下学习的潜力。但只有最小二乘回归具有已知的实例最优边界；对于更一般的模型或损失函数，现有的保证要么降低到独立同分布学习，要么降低到现有的单轨迹结果。

Method: 该研究通过 Hellinger 局部化框架，一种用于最大似然估计的通用方法，来扩展多轨迹设置中实例最优速率的范围。该方法首先通过降低到独立同分布学习来控制路径度量级别的平方 Hellinger 距离，然后通过轨迹 Fisher 信息加权的参数空间中的二次形式进行局部化。

Result: 该研究在四个不同的案例研究中实例化了该框架：马尔可夫链的简单混合，非高斯噪声下的相关线性回归，具有非单调激活的广义线性模型和线性注意力序列模型。在所有情况下，该研究的边界几乎与来自渐近正态性的实例最优速率相匹配，大大优于标准降低。

Conclusion: 该研究显著拓宽了多轨迹设置中实例最优速率的范围，并在更广泛的条件下实现了与完整数据预算相匹配的实例最优边界，优于标准方法。

Abstract: Learning from temporally-correlated data is a core facet of modern machine
learning. Yet our understanding of sequential learning remains incomplete,
particularly in the multi-trajectory setting where data consists of many
independent realizations of a time-indexed stochastic process. This important
regime both reflects modern training pipelines such as for large foundation
models, and offers the potential for learning without the typical mixing
assumptions made in the single-trajectory case. However, instance-optimal
bounds are known only for least-squares regression with dependent covariates;
for more general models or loss functions, the only broadly applicable
guarantees result from a reduction to either i.i.d. learning, with effective
sample size scaling only in the number of trajectories, or an existing
single-trajectory result when each individual trajectory mixes, with effective
sample size scaling as the full data budget deflated by the mixing-time.
  In this work, we significantly broaden the scope of instance-optimal rates in
multi-trajectory settings via the Hellinger localization framework, a general
approach for maximum likelihood estimation. Our method proceeds by first
controlling the squared Hellinger distance at the path-measure level via a
reduction to i.i.d. learning, followed by localization as a quadratic form in
parameter space weighted by the trajectory Fisher information. This yields
instance-optimal bounds that scale with the full data budget under a broad set
of conditions. We instantiate our framework across four diverse case studies: a
simple mixture of Markov chains, dependent linear regression under non-Gaussian
noise, generalized linear models with non-monotonic activations, and
linear-attention sequence models. In all cases, our bounds nearly match the
instance-optimal rates from asymptotic normality, substantially improving over
standard reductions.

</details>


### [163] [Bayesian Optimization under Uncertainty for Training a Scale Parameter in Stochastic Models](https://arxiv.org/abs/2510.06439)
*Akash Yadav,Ruda Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的贝叶斯优化框架，专门用于不确定性下的超参数调优，重点是优化随机模型中的尺度或精度型参数。


<details>
  <summary>Details</summary>
Motivation: 由于噪声函数评估，不确定性下的优化在计算上可能很昂贵。

Method: 该方法采用底层随机变量的统计替代，从而能够对期望算子进行分析评估。此外，我们推导了随机采集函数优化器的封闭式表达式，从而显着降低了每次迭代的计算成本。

Result: 与传统的基于蒙特卡罗的一维优化方案相比，所提出的方法需要的数据点减少 40 倍，从而使计算成本降低高达 40 倍。

Conclusion: 通过计算工程中的两个数值例子证明了所提出方法的有效性。

Abstract: Hyperparameter tuning is a challenging problem especially when the system
itself involves uncertainty. Due to noisy function evaluations, optimization
under uncertainty can be computationally expensive. In this paper, we present a
novel Bayesian optimization framework tailored for hyperparameter tuning under
uncertainty, with a focus on optimizing a scale- or precision-type parameter in
stochastic models. The proposed method employs a statistical surrogate for the
underlying random variable, enabling analytical evaluation of the expectation
operator. Moreover, we derive a closed-form expression for the optimizer of the
random acquisition function, which significantly reduces computational cost per
iteration. Compared with a conventional one-dimensional Monte Carlo-based
optimization scheme, the proposed approach requires 40 times fewer data points,
resulting in up to a 40-fold reduction in computational cost. We demonstrate
the effectiveness of the proposed method through two numerical examples in
computational engineering.

</details>


### [164] [Context-Aware Inference via Performance Forecasting in Decentralized Learning Networks](https://arxiv.org/abs/2510.06444)
*Joel Pfeffer,J. M. Diederik Kruijssen,Clément Gossart,Mélanie Chevance,Diego Campo Millan,Florian Stecker,Steven N. Longmore*

Main category: cs.LG

TL;DR: 本文提出了一种使用机器学习预测模型在时间序列中每个epoch的预测性能的方法，通过为在给定时间可能更准确的模型分配更高的权重，从而实现“上下文感知”，并提高网络推理的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的使用线性池化方法（从简单平均到动态权重更新）的动态预测组合依赖于历史性能来更新权重，因此是反应式的，并且调整到变化的环境（阶段或状态变化）的速度较慢。

Method: 开发了一个模型，该模型使用机器学习来预测时间序列中每个epoch的模型预测性能。通过预测后悔值（相对于网络推理的性能）或后悔值z-score（相对于其他worker的性能）来提高网络推理的准确性。

Result: 在去中心化学习网络中增加一个性能预测worker，可以提高网络推理的准确性。预测后悔值或后悔值z-score的模型比预测损失的模型表现更好。

Conclusion: 使用性能预测进行预测组合在任何需要预测性而非反应性模型加权的情况下都可能有用。性能预测模型的性能可能对特征集和训练epoch数量的选择敏感，这些属性可能取决于具体问题，应针对每个领域进行定制。

Abstract: In decentralized learning networks, predictions from many participants are
combined to generate a network inference. While many studies have demonstrated
performance benefits of combining multiple model predictions, existing
strategies using linear pooling methods (ranging from simple averaging to
dynamic weight updates) face a key limitation. Dynamic prediction combinations
that rely on historical performance to update weights are necessarily reactive.
Due to the need to average over a reasonable number of epochs (with moving
averages or exponential weighting), they tend to be slow to adjust to changing
circumstances (phase or regime changes). In this work, we develop a model that
uses machine learning to forecast the performance of predictions by models at
each epoch in a time series. This enables `context-awareness' by assigning
higher weight to models that are likely to be more accurate at a given time. We
show that adding a performance forecasting worker in a decentralized learning
network, following a design similar to the Allora network, can improve the
accuracy of network inferences. Specifically, we find forecasting models that
predict regret (performance relative to the network inference) or regret
z-score (performance relative to other workers) show greater improvement than
models predicting losses, which often do not outperform the naive network
inference (historically weighted average of all inferences). Through a series
of optimization tests, we show that the performance of the forecasting model
can be sensitive to choices in the feature set and number of training epochs.
These properties may depend on the exact problem and should be tailored to each
domain. Although initially designed for a decentralized learning network, using
performance forecasting for prediction combination may be useful in any
situation where predictive rather than reactive model weighting is needed.

</details>


### [165] [How NOT to benchmark your SITE metric: Beyond Static Leaderboards and Towards Realistic Evaluation](https://arxiv.org/abs/2510.06448)
*Prabhant Singh,Sibylle Hess,Joaquin Vanschoren*

Main category: cs.LG

TL;DR: 这篇论文主要关注迁移性评估指标，用于在不进行微调和不访问源数据集的情况下，为给定的目标任务找到高性能的预训练模型。


<details>
  <summary>Details</summary>
Motivation: 当前用于评估迁移性评估指标的基准存在缺陷。

Method: 通过实证研究揭示了广泛使用的基准设置的缺点。

Result: 发现这些基准中不切实际的模型空间和静态性能层级会人为地夸大现有指标的性能，甚至简单的、与数据集无关的启发式方法都能胜过复杂的方法。

Conclusion: 目前的评估协议与真实模型选择的复杂性之间存在严重脱节。论文为构建更稳健和真实的基准提供了具体建议，以指导未来的研究朝着更有意义的方向发展。

Abstract: Transferability estimation metrics are used to find a high-performing
pre-trained model for a given target task without fine-tuning models and
without access to the source dataset. Despite the growing interest in
developing such metrics, the benchmarks used to measure their progress have
gone largely unexamined. In this work, we empirically show the shortcomings of
widely used benchmark setups to evaluate transferability estimation metrics. We
argue that the benchmarks on which these metrics are evaluated are
fundamentally flawed. We empirically demonstrate that their unrealistic model
spaces and static performance hierarchies artificially inflate the perceived
performance of existing metrics, to the point where simple, dataset-agnostic
heuristics can outperform sophisticated methods. Our analysis reveals a
critical disconnect between current evaluation protocols and the complexities
of real-world model selection. To address this, we provide concrete
recommendations for constructing more robust and realistic benchmarks to guide
future research in a more meaningful direction.

</details>


### [166] [Attention Sinks and Compression Valleys in LLMs are Two Sides of the Same Coin](https://arxiv.org/abs/2510.06477)
*Enrique Queipo-de-Llano,Álvaro Arroyo,Federico Barbero,Xiaowen Dong,Michael Bronstein,Yann LeCun,Ravid Shwartz-Ziv*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型中的注意力陷阱和压缩谷现象，发现它们都与残差流中大量激活的形成有关。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型中注意力陷阱和压缩谷这两种孤立现象之间的联系。

Method: 通过理论证明和跨多个模型（410M-120B 参数）的实验，以及消融研究验证理论预测。

Result: 研究表明，当序列起始token在中间层产生极端的激活范数时，压缩谷和注意力陷阱同时出现。提出Mix-Compress-Refine信息流理论。

Conclusion: Transformer-based LLM以三个阶段处理tokens：早期层的广泛混合，中间层的压缩计算，以及后期层的选择性优化。该框架解释了为什么嵌入任务在中间层表现最佳，而生成任务受益于全深度处理。

Abstract: Attention sinks and compression valleys have attracted significant attention
as two puzzling phenomena in large language models, but have been studied in
isolation. In this work, we present a surprising connection between attention
sinks and compression valleys, tracing both to the formation of massive
activations in the residual stream. We prove theoretically that massive
activations necessarily produce representational compression and establish
bounds on the resulting entropy reduction. Through experiments across several
models (410M-120B parameters), we confirm that when the beginning-of-sequence
token develops extreme activation norms in the middle layers, both compression
valleys and attention sinks emerge simultaneously. Targeted ablation studies
validate our theoretical predictions. This unified view motivates us to propose
the Mix-Compress-Refine theory of information flow, as an attempt to explain
how LLMs organize their computation in depth by controlling attention and
representational compression via massive activations. Specifically, we posit
that Transformer-based LLMs process tokens in three distinct phases: (1) broad
mixing in the early layers, (2) compressed computation with limited mixing in
the middle layers, and (3) selective refinement in the late layers. Our
framework helps explain why embedding tasks perform best at intermediate
layers, whereas generation tasks benefit from full-depth processing, clarifying
differences in task-dependent representations.

</details>


### [167] [Valid Stopping for LLM Generation via Empirical Dynamic Formal Lift](https://arxiv.org/abs/2510.06478)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.LG

TL;DR: 提出了Sequential-EDFL，一种用于语言模型生成的随时有效的序列测试方法，可减少生成量并保持误差控制。


<details>
  <summary>Details</summary>
Motivation: 旨在解决语言模型生成过程中的停止问题，并提供形式化的误差控制。

Method: 通过自归一化经验伯恩斯坦e-processes跟踪信息增益（完整模型和弱化骨架基线之间的对数似然比），并结合在线均值估计、混合e-processes和自适应重置等技术。

Result: 在六个基准测试中，Sequential-EDFL比序列基线减少了22-28%的生成量，同时保持了delta水平的控制，计算开销增加了12%。

Conclusion: EDFL可作为第一阶段过滤器，减少验证负担，但不能作为安全关键领域的独立解决方案。

Abstract: We introduce Sequential-EDFL (Empirical Dynamic Formal Lift), applying
anytime-valid sequential testing to language model generation stopping. Our
approach tracks information lift -- the log-likelihood ratio between full
models and deliberately weakened "skeleton" baselines -- using self-normalized
empirical-Bernstein e-processes that provide formal delta-level error control
regardless of stopping time. We handle unknown centering through online mean
estimation, combine multiple parameters via mixture e-processes, and support
adaptive resets under distributional drift. On six benchmarks, Sequential-EDFL
reduces generation by 22-28% vs. sequential baselines while maintaining
delta-level control with 12% computational overhead. We introduce automated
skeletons (distilled submodels, randomized logits) and show robustness across
skeleton families. Composing EDFL with a lightweight correctness gate (sentence
boundaries + verifier) improves end-task correctness while preserving
anytime-valid guarantees by only delaying stopping. Our certificates control
information sufficiency, not factual correctness -- 10.9% of stopped sequences
remain incorrect even with the gate (13.2-22.7% without it). EDFL serves as a
first-stage filter reducing verification burden by 83%, not as a standalone
solution for safety-critical domains.

</details>


### [168] [GUIDE: Guided Initialization and Distillation of Embeddings](https://arxiv.org/abs/2510.06502)
*Khoa Trinh,Gaurav Menghani,Erik Vee*

Main category: cs.LG

TL;DR: 本文介绍了一种名为 GUIDE 的新知识蒸馏技术，该技术通过在参数空间中匹配 teacher 和 student 模型，从而缩小 teacher-student 之间的质量差距。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法仅限于让 student 模型匹配 teacher 模型的输出，而忽略了 teacher 模型中更多有用的信息。考虑到训练大型模型的成本，应该从 teacher 模型中提取更多有用的信息。

Method: 提出了一种名为 GUIDE 的知识蒸馏技术，该技术通过在参数空间中强制 student 模型匹配 teacher 模型。

Result: 使用 GUIDE，当使用在 ≈ 20B tokens 上训练的大型 student 模型（400M - 1B 参数）时，teacher-student 之间的质量差距缩小了 25-26%。GUIDE 可以与知识蒸馏相结合，从而获得近似的附加改进。单独应用 GUIDE 可以获得比单独应用知识蒸馏更好的模型质量。GUIDE 不引入训练或推理开销。

Conclusion: GUIDE 是一种有效的知识蒸馏技术，可以提高模型质量，且不引入额外的训练或推理开销。

Abstract: Algorithmic efficiency techniques such as distillation
(\cite{hinton2015distillation}) are useful in improving model quality without
increasing serving costs, provided a larger teacher model is available for a
smaller student model to learn from during training. Standard distillation
methods are limited to only forcing the student to match the teacher's outputs.
Given the costs associated with training a large model, we believe we should be
extracting more useful information from a teacher model than by just making the
student match the teacher's outputs.
  In this paper, we introduce \guide (Guided Initialization and Distillation of
Embeddings). \guide can be considered a distillation technique that forces the
student to match the teacher in the parameter space. Using \guide we show
25-26\% reduction in the teacher-student quality gap when using large student
models (400M - 1B parameters) trained on $\approx$ 20B tokens. We also present
a thorough analysis demonstrating that \guide can be combined with knowledge
distillation with near additive improvements. Furthermore, we show that
applying \guide alone leads to substantially better model quality than applying
knowledge distillation by itself.
  Most importantly, \guide introduces no training or inference overhead and
hence any model quality gains from our method are virtually free.

</details>


### [169] [ATLO-ML: Adaptive Time-Length Optimizer for Machine Learning -- Insights from Air Quality Forecasting](https://arxiv.org/abs/2510.06503)
*I-Hsi Kao,Kanji Uchino*

Main category: cs.LG

TL;DR: ATLO-ML: adaptive time-length optimization system that automatically determines the optimal input time length and sampling rate based on user-defined output time length.


<details>
  <summary>Details</summary>
Motivation: Accurate time-series predictions in machine learning are heavily influenced by the selection of appropriate input time length and sampling rate.

Method: ATLO-ML is validated using air quality datasets, including both GAMS-dataset and proprietary data collected from a data center, both in time series format.

Result: Utilizing the optimized time length and sampling rate significantly improves the accuracy of machine learning models compared to fixed time lengths.

Conclusion: ATLO-ML shows potential for generalization across various time-sensitive applications, offering a robust solution for optimizing temporal input parameters in machine learning workflows.

Abstract: Accurate time-series predictions in machine learning are heavily influenced
by the selection of appropriate input time length and sampling rate. This paper
introduces ATLO-ML, an adaptive time-length optimization system that
automatically determines the optimal input time length and sampling rate based
on user-defined output time length. The system provides a flexible approach to
time-series data pre-processing, dynamically adjusting these parameters to
enhance predictive performance. ATLO-ML is validated using air quality
datasets, including both GAMS-dataset and proprietary data collected from a
data center, both in time series format. Results demonstrate that utilizing the
optimized time length and sampling rate significantly improves the accuracy of
machine learning models compared to fixed time lengths. ATLO-ML shows potential
for generalization across various time-sensitive applications, offering a
robust solution for optimizing temporal input parameters in machine learning
workflows.

</details>


### [170] [A Median Perspective on Unlabeled Data for Out-of-Distribution Detection](https://arxiv.org/abs/2510.06505)
*Momin Abbas,Ali Falahati,Hossein Goli,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: 本文提出了一种名为Medix的新框架，用于从未标记数据中识别潜在的异常值，并使用这些识别出的异常值和标记的InD数据来训练鲁棒的OOD分类器。


<details>
  <summary>Details</summary>
Motivation: 在现实世界的应用中，为了保证机器学习系统的鲁棒性和可靠性，需要进行OOD检测。最近的方法探索了使用未标记数据，显示出增强OOD检测能力的潜力。然而，由于InD和OOD样本的混合性质，有效利用未标记的野外数据仍然具有挑战性。缺乏独特的OOD样本集使得训练最优OOD分类器变得复杂。

Method: 我们使用中值操作，因为它提供了中心趋势的稳定估计，作为OOD检测机制，因为它对噪声和异常值具有鲁棒性。使用这些识别出的异常值，以及标记的InD数据，我们训练了一个鲁棒的OOD分类器。

Result: 从理论的角度来看，我们推导了误差界，表明Medix实现了低错误率。实证结果进一步证实了我们的主张，因为Medix在开放世界设置中全面优于现有方法，证实了我们的理论见解。

Conclusion: Medix框架在开放世界OOD检测中表现出色，理论和实验结果均支持其有效性。

Abstract: Out-of-distribution (OOD) detection plays a crucial role in ensuring the
robustness and reliability of machine learning systems deployed in real-world
applications. Recent approaches have explored the use of unlabeled data,
showing potential for enhancing OOD detection capabilities. However,
effectively utilizing unlabeled in-the-wild data remains challenging due to the
mixed nature of both in-distribution (InD) and OOD samples. The lack of a
distinct set of OOD samples complicates the task of training an optimal OOD
classifier. In this work, we introduce Medix, a novel framework designed to
identify potential outliers from unlabeled data using the median operation. We
use the median because it provides a stable estimate of the central tendency,
as an OOD detection mechanism, due to its robustness against noise and
outliers. Using these identified outliers, along with labeled InD data, we
train a robust OOD classifier. From a theoretical perspective, we derive error
bounds that demonstrate Medix achieves a low error rate. Empirical results
further substantiate our claims, as Medix outperforms existing methods across
the board in open-world settings, confirming the validity of our theoretical
insights.

</details>


### [171] [Text-to-Image Models Leave Identifiable Signatures: Implications for Leaderboard Security](https://arxiv.org/abs/2510.06525)
*Ali Naseh,Anshuman Suri,Yuefeng Peng,Harsh Chaudhari,Alina Oprea,Amir Houmansadr*

Main category: cs.LG

TL;DR: 文本到图像的排行榜比以前认为的更容易受到攻击。研究表明，通过分析图像的 CLIP 嵌入空间，即使没有提示控制或历史数据，也可以高精度地识别生成模型。


<details>
  <summary>Details</summary>
Motivation: 评估生成式AI模型的能力至关重要，但排行榜容易被操纵。攻击者可以通过去匿名化模型来操纵排名。

Method: 使用来自 280 个提示和 19 个不同模型的超过 150,000 张生成的图像，在 CLIP 嵌入空间中进行实时分类，以识别生成模型。引入了提示级别的可分离性指标，并识别出能够实现近乎完美去匿名化的提示。

Result: 即使没有提示控制或历史数据，简单的实时分类也能以高精度识别生成模型。发现某些提示能够实现近乎完美的去匿名化。

Conclusion: 文本到图像排行榜中的排名操纵比以前认为的更容易，因此需要更强大的防御措施。

Abstract: Generative AI leaderboards are central to evaluating model capabilities, but
remain vulnerable to manipulation. Among key adversarial objectives is rank
manipulation, where an attacker must first deanonymize the models behind
displayed outputs -- a threat previously demonstrated and explored for large
language models (LLMs). We show that this problem can be even more severe for
text-to-image leaderboards, where deanonymization is markedly easier. Using
over 150,000 generated images from 280 prompts and 19 diverse models spanning
multiple organizations, architectures, and sizes, we demonstrate that simple
real-time classification in CLIP embedding space identifies the generating
model with high accuracy, even without prompt control or historical data. We
further introduce a prompt-level separability metric and identify prompts that
enable near-perfect deanonymization. Our results indicate that rank
manipulation in text-to-image leaderboards is easier than previously
recognized, underscoring the need for stronger defenses.

</details>


### [172] [Wide Neural Networks as a Baseline for the Computational No-Coincidence Conjecture](https://arxiv.org/abs/2510.06527)
*John Dunbar,Scott Aaronson*

Main category: cs.LG

TL;DR: 大型随机初始化的神经网络，当激活函数是非线性且在 高斯测量下具有零均值时，具有几乎独立的输出。


<details>
  <summary>Details</summary>
Motivation: 提议使用具有零均值激活函数的神经网络，作为 Alignment Research Center 的计算非巧合猜想的一个有希望的候选者——该猜想旨在衡量 AI 可解释性的界限。

Method: 研究了具有较大宽度和超参数自然选择的随机初始化神经网络。

Result: 当激活函数为非线性且在 高斯测量下具有零均值时，大型随机初始化的神经网络具有几乎独立的输出。

Conclusion: 具有零均值激活函数的神经网络可能是衡量 AI 可解释性极限的一个有希望的候选者。

Abstract: We establish that randomly initialized neural networks, with large width and
a natural choice of hyperparameters, have nearly independent outputs exactly
when their activation function is nonlinear with zero mean under the Gaussian
measure: $\mathbb{E}_{z \sim \mathcal{N}(0,1)}[\sigma(z)]=0$. For example, this
includes ReLU and GeLU with an additive shift, as well as tanh, but not ReLU or
GeLU by themselves. Because of their nearly independent outputs, we propose
neural networks with zero-mean activation functions as a promising candidate
for the Alignment Research Center's computational no-coincidence conjecture --
a conjecture that aims to measure the limits of AI interpretability.

</details>


### [173] [Scalable Policy-Based RL Algorithms for POMDPs](https://arxiv.org/abs/2510.06540)
*Ameya Anjarlekar,Rasoul Etesami,R Srikant*

Main category: cs.LG

TL;DR: 本文提出了一种通过将POMDP模型近似为有限状态MDP（称为超状态MDP）来解决部分可观察强化学习（PORL）问题的方法。


<details>
  <summary>Details</summary>
Motivation: 在POMDP中，信念状态的连续性给学习最优策略带来了巨大的计算挑战。

Method: 1. 推导了改进先前工作的理论保证，该理论保证将转换后的超状态MDP的最优价值函数与原始POMDP的最优价值函数相关联。2. 提出了一种基于策略的学习方法，该方法使用线性函数逼近来学习超状态MDP的最优策略。

Result: 证明了近似误差随着历史长度的增加呈指数下降。首次明确量化了在将标准TD学习应用于真实动态非马尔可夫的环境时引入的误差。

Conclusion: POMDP可以通过将其视为MDP，使用TD学习和策略优化来近似解决，其中MDP状态对应于有限的历史。

Abstract: The continuous nature of belief states in POMDPs presents significant
computational challenges in learning the optimal policy. In this paper, we
consider an approach that solves a Partially Observable Reinforcement Learning
(PORL) problem by approximating the corresponding POMDP model into a
finite-state Markov Decision Process (MDP) (called Superstate MDP). We first
derive theoretical guarantees that improve upon prior work that relate the
optimal value function of the transformed Superstate MDP to the optimal value
function of the original POMDP. Next, we propose a policy-based learning
approach with linear function approximation to learn the optimal policy for the
Superstate MDP. Consequently, our approach shows that a POMDP can be
approximately solved using TD-learning followed by Policy Optimization by
treating it as an MDP, where the MDP state corresponds to a finite history. We
show that the approximation error decreases exponentially with the length of
this history. To the best of our knowledge, our finite-time bounds are the
first to explicitly quantify the error introduced when applying standard TD
learning to a setting where the true dynamics are not Markovian.

</details>


### [174] [Incoherence in goal-conditioned autoregressive models](https://arxiv.org/abs/2510.06545)
*Jacek Karwowski,Raymond Douglas*

Main category: cs.LG

TL;DR: 研究了非相干性，这是一种通过自回归模型的朴素目标条件反射得出的强化学习策略的结构性问题。


<details>
  <summary>Details</summary>
Motivation: 专注于用在线强化学习微调离线学习策略的重训练模型的过程。证明了它可以减少非相干性并提高回报。

Method: 通过重构控制即推理和软 Q 学习的标准概念，建立了与其他两种理解迭代重训练过程的三向对应关系：将后验概率折叠到奖励中，以及在确定性情况下，降低温度参数；该对应关系通过训练-推理权衡具有计算内容。

Result: 通过软条件生成模型，讨论了非相干性和有效范围之间的联系。

Conclusion: 重训练模型能减少非相干性并提高回报，通过软条件生成模型，讨论了非相干性和有效范围之间的联系。

Abstract: We investigate mathematically the notion of incoherence: a structural issue
with reinforcement learning policies derived by naive goal-conditioning of
autoregressive models. We focus on the process of re-training models on their
own actions, that is, fine-tuning offline-learned policies with online RL. We
prove that it decreases incoherence and leads to an improvement in return, and
we aim to characterize the resulting trajectory of policies. By re-framing
standard notions of control-as-inference and soft Q learning, we establish a
three-way correspondence with two other ways of understanding the iterative
re-training process: as folding the posterior into the reward and, in the
deterministic case, as decreasing the temperature parameter; the correspondence
has computational content via the training-inference trade-off. Through
soft-conditioning generative models, we discuss the link between incoherence
and the effective horizon.

</details>


### [175] [The Markovian Thinker](https://arxiv.org/abs/2510.06557)
*Milad Aghajohari,Kamran Chitsaz,Amirhossein Kazemnejad,Sarath Chandar,Alessandro Sordoni,Aaron Courville,Siva Reddy*

Main category: cs.LG

TL;DR: 提出 Markovian Thinking 范式，将推理过程分解为固定大小的块，实现线性计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 标准 RL 的“思考环境”状态是无界的，导致计算量随思考长度呈二次方增长。

Method: 设计 Delethink 环境，将推理结构化为固定大小的块，并在块边界重置上下文，通过 RL 学习在每个块末尾写入文本状态，以便在重置后无缝继续推理。

Result: 1.5B 模型在 8K-token 块中推理，性能与 24K 预算的 LongCoT-RL 相媲美，且测试时可扩展性更强。线性计算效果显著，估计在 96K 平均思考长度下，Delethink 的成本远低于 LongCoT-RL。

Conclusion: 重新设计思考环境是一个强大的杠杆，它可以在没有二次方开销的情况下实现非常长的推理，并为高效、可扩展的推理 LLM 开辟了一条道路。

Abstract: Reinforcement learning (RL) has recently become a strong recipe for training
reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard
RL "thinking environment", where the state is the prompt plus all prior
reasoning tokens, makes the state unbounded and forces attention-based policies
to pay quadratic compute as thoughts lengthen. We revisit the environment
itself. We propose Markovian Thinking, a paradigm in which the policy advances
reasoning while conditioning on a constant-size state, decoupling thinking
length from context size. As an immediate consequence this yields linear
compute with constant memory. We instantiate this idea with Delethink, an RL
environment that structures reasoning into fixed-size chunks. Within each
chunk, the model thinks as usual; at the boundary, the environment resets the
context and reinitializes the prompt with a short carryover. Through RL, the
policy learns to write a textual state near the end of each chunk sufficient
for seamless continuation of reasoning after reset. Trained in this
environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up
to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget.
With test-time scaling, Delethink continues to improve where LongCoT plateaus.
The effect of linear compute is substantial: we empirically estimate at 96K
average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink.
Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B)
often sample Markovian traces zero-shot across diverse benchmarks, providing
positive samples that make RL effective at scale. Our results show that
redesigning the thinking environment is a powerful lever: it enables very long
reasoning without quadratic overhead and opens a path toward efficient,
scalable reasoning LLMs.

</details>


### [176] [The Framework That Survives Bad Models: Human-AI Collaboration For Clinical Trials](https://arxiv.org/abs/2510.06567)
*Yao Chen,David Ohlssen,Aimee Readie,Gregory Ligozio,Ruvie Martin,Thibaud Coroller*

Main category: cs.LG

TL;DR: AI 在临床试验中应用前景广阔，但需防范风险。研究比较了两种 AI 框架与纯人工评估在医学图像疾病评估中的表现，包括成本、准确性、稳健性和泛化能力。通过注入不良模型进行压力测试，确保治疗效果的有效性。结果表明，AI 作为辅助阅读器 (AI-SR) 是临床试验的最佳方法，即使在不良模型下也能满足所有标准，提供可靠的疾病估计，保持临床试验的治疗效果和结论。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能（AI）在临床试验中支持患者招募、终点评估和治疗反应预测的潜力，并强调在评估直接影响试验结论的患者终点时，部署 AI 可能存在的风险。

Method: 比较两种 AI 框架与纯人工评估在基于医学图像的疾病评估中的表现，测量成本、准确性、稳健性和泛化能力。通过注入不良模型进行压力测试，评估框架在严重模型退化下的表现。使用两项随机对照试验，终点来自脊柱 X 射线图像。

Result: 研究发现，使用 AI 作为辅助阅读器 (AI-SR) 是最适合临床试验的方法，因为它满足各种模型类型的所有标准，即使是不良模型。该方法始终提供可靠的疾病估计，保留临床试验治疗效果的估计和结论，并在应用于不同人群时保持这些优势。

Conclusion: AI 辅助阅读器 (AI-SR) 是临床试验的最佳方法，即使在不良模型下也能提供可靠的疾病估计，保持临床试验的治疗效果和结论，并在应用于不同人群时保持这些优势。

Abstract: Artificial intelligence (AI) holds great promise for supporting clinical
trials, from patient recruitment and endpoint assessment to treatment response
prediction. However, deploying AI without safeguards poses significant risks,
particularly when evaluating patient endpoints that directly impact trial
conclusions. We compared two AI frameworks against human-only assessment for
medical image-based disease evaluation, measuring cost, accuracy, robustness,
and generalization ability. To stress-test these frameworks, we injected bad
models, ranging from random guesses to naive predictions, to ensure that
observed treatment effects remain valid even under severe model degradation. We
evaluated the frameworks using two randomized controlled trials with endpoints
derived from spinal X-ray images. Our findings indicate that using AI as a
supporting reader (AI-SR) is the most suitable approach for clinical trials, as
it meets all criteria across various model types, even with bad models. This
method consistently provides reliable disease estimation, preserves clinical
trial treatment effect estimates and conclusions, and retains these advantages
when applied to different populations.

</details>


### [177] [DPA-Net: A Dual-Path Attention Neural Network for Inferring Glycemic Control Metrics from Self-Monitored Blood Glucose Data](https://arxiv.org/abs/2510.06623)
*Canyu Lei,Benjamin Lobo,Jianxin Xie*

Main category: cs.LG

TL;DR: 本研究提出了一种名为DPA-Net的双路径注意力神经网络，旨在仅使用SMBG数据估算AGP指标。


<details>
  <summary>Details</summary>
Motivation: 动态葡萄糖监测（CGM）成本高且可及性有限，而SMBG数据虽然便宜但稀疏，难以转化为有意义的血糖指标。

Method: DPA-Net集成了两个互补路径：重建CGM轨迹的空间-通道注意路径和直接预测AGP指标的多尺度ResNet路径。此外，还开发了一个主动点选择器来识别反映患者行为模式的SMBG采样点。

Result: 在大型真实世界数据集上的实验结果表明，DPA-Net实现了鲁棒的准确性，误差较低，同时减少了系统偏差。

Conclusion: 这是第一个从SMBG数据估计AGP指标的监督机器学习框架，在无法使用CGM的情况下，提供了一种实用且具有临床意义的决策支持工具。

Abstract: Continuous glucose monitoring (CGM) provides dense and dynamic glucose
profiles that enable reliable estimation of Ambulatory Glucose Profile (AGP)
metrics, such as Time in Range (TIR), Time Below Range (TBR), and Time Above
Range (TAR). However, the high cost and limited accessibility of CGM restrict
its widespread adoption, particularly in low- and middle-income regions. In
contrast, self-monitoring of blood glucose (SMBG) is inexpensive and widely
available but yields sparse and irregular data that are challenging to
translate into clinically meaningful glycemic metrics.
  In this work, we propose a Dual-Path Attention Neural Network (DPA-Net) to
estimate AGP metrics directly from SMBG data. DPA-Net integrates two
complementary paths: (1) a spatial-channel attention path that reconstructs a
CGM-like trajectory from sparse SMBG observations, and (2) a multi-scale ResNet
path that directly predicts AGP metrics. An alignment mechanism between the two
paths is introduced to reduce bias and mitigate overfitting. In addition, we
develop an active point selector to identify realistic and informative SMBG
sampling points that reflect patient behavioral patterns.
  Experimental results on a large, real-world dataset demonstrate that DPA-Net
achieves robust accuracy with low errors while reducing systematic bias. To the
best of our knowledge, this is the first supervised machine learning framework
for estimating AGP metrics from SMBG data, offering a practical and clinically
relevant decision-support tool in settings where CGM is not accessible.

</details>


### [178] [POME: Post Optimization Model Edit via Muon-style Projection](https://arxiv.org/abs/2510.06627)
*Yong Liu,Di Fu,Yang Luo,Zirui Zhu,Minhao Cheng,Cho-Jui Hsieh,Yang You*

Main category: cs.LG

TL;DR: POME是一种新算法，它仅使用预训练和微调的检查点来提高微调后的语言模型的性能，而无需额外的数据或进一步的优化。


<details>
  <summary>Details</summary>
Motivation: 核心思想是将muon风格的投影应用于ΔW，即微调和预训练权重之间的差异。

Method: 该投影使用截断奇异值分解（SVD）来均衡主要更新方向的影响并修剪小的奇异值，这些奇异值通常代表噪声。

Result: POME提供了持续的增益，在GSM8K上将平均性能提高了+2.5%，在代码生成上提高了+1.0%。

Conclusion: POME的广泛适用性——从7B基础模型到72B RLHF指导模型——使其成为任何微调管道的实用、零成本的增强。

Abstract: We introduce Post-Optimization Model Edit (POME), a new algorithm that
enhances the performance of fine-tuned large language models using only their
pretrained and fine-tuned checkpoints, without requiring extra data or further
optimization. The core idea is to apply a muon-style projection to $\Delta W$,
the difference between the fine-tuned and pretrained weights. This projection
uses truncated singular value decomposition (SVD) to equalize the influence of
dominant update directions and prune small singular values, which often
represent noise. As a simple post-processing step, POME is completely decoupled
from the training pipeline. It requires zero modifications and imposes no
overhead, making it universally compatible with any optimizer or distributed
framework. POME delivers consistent gains, boosting average performance by
+2.5\% on GSM8K and +1.0\% on code generation. Its broad applicability -- from
7B foundation models to 72B RLHF-instructed models -- establishes it as a
practical, zero-cost enhancement for any fine-tuning pipeline. Code is
available at https://github.com/NUS-HPC-AI-Lab/POME.

</details>


### [179] [AI-Driven Forecasting and Monitoring of Urban Water System](https://arxiv.org/abs/2510.06631)
*Qiming Guo,Bishal Khatri,Hua Zhang,Wenlu Wang*

Main category: cs.LG

TL;DR: 本文提出了一种集成了AI和遥感器的框架，用于检测地下水管道的泄漏。


<details>
  <summary>Details</summary>
Motivation: 传统人工检测效率低，密集传感器部署成本高，而地下水和废水管道对于城市运营至关重要，但容易出现泄漏和渗透等异常情况，导致大量的水流失、环境破坏和高昂的维修成本。

Method: 通过部署稀疏的遥感器来捕获实时流量和深度数据，并结合HydroNet（一个利用管道属性的有向图模型）来实现更高精度的建模。

Result: 在真实校园废水网络数据集上的评估表明，该系统收集了有效的时空水力数据，使HydroNet优于先进的基线模型。

Conclusion: 边缘感知消息传递与水力模拟的集成，能够从有限的传感器部署中实现准确的全网络预测。该方法可以有效地扩展到各种地下水管道网络。

Abstract: Underground water and wastewater pipelines are vital for city operations but
plagued by anomalies like leaks and infiltrations, causing substantial water
loss, environmental damage, and high repair costs. Conventional manual
inspections lack efficiency, while dense sensor deployments are prohibitively
expensive. In recent years, artificial intelligence has advanced rapidly and is
increasingly applied to urban infrastructure. In this research, we propose an
integrated AI and remote-sensor framework to address the challenge of leak
detection in underground water pipelines, through deploying a sparse set of
remote sensors to capture real-time flow and depth data, paired with HydroNet -
a dedicated model utilizing pipeline attributes (e.g., material, diameter,
slope) in a directed graph for higher-precision modeling. Evaluations on a
real-world campus wastewater network dataset demonstrate that our system
collects effective spatio-temporal hydraulic data, enabling HydroNet to
outperform advanced baselines. This integration of edge-aware message passing
with hydraulic simulations enables accurate network-wide predictions from
limited sensor deployments. We envision that this approach can be effectively
extended to a wide range of underground water pipeline networks.

</details>


### [180] [Chem-NMF: Multi-layer $α$-divergence Non-Negative Matrix Factorization for Cardiorespiratory Disease Clustering, with Improved Convergence Inspired by Chemical Catalysts and Rigorous Asymptotic Analysis](https://arxiv.org/abs/2510.06632)
*Yasaman Torabi,Shahram Shirani,James P. Reilly*

Main category: cs.LG

TL;DR: 本研究提出了一种新的非负矩阵分解 (NMF) 方法，称为 Chem-NMF，它使用来自物理化学的灵感来稳定收敛。


<details>
  <summary>Details</summary>
Motivation: 现有的基于 α-散度的 NMF 方法在扩展到多层架构时，收敛性存在挑战。

Method: 该方法引入了一个受化学反应中能量垒的玻尔兹曼概率启发的边界因子，以稳定收敛。

Result: 实验结果表明，该算法在生物医学信号上的聚类精度提高了 5.6% ± 2.7%，在人脸图像上的聚类精度提高了 11.1% ± 7.2%。

Conclusion: 该研究首次将物理化学的视角应用于严格分析 NMF 算法的收敛行为，并证明了 Chem-NMF 在聚类精度方面的有效性。

Abstract: Non-Negative Matrix Factorization (NMF) is an unsupervised learning method
offering low-rank representations across various domains such as audio
processing, biomedical signal analysis, and image recognition. The
incorporation of $\alpha$-divergence in NMF formulations enhances flexibility
in optimization, yet extending these methods to multi-layer architectures
presents challenges in ensuring convergence. To address this, we introduce a
novel approach inspired by the Boltzmann probability of the energy barriers in
chemical reactions to theoretically perform convergence analysis. We introduce
a novel method, called Chem-NMF, with a bounding factor which stabilizes
convergence. To our knowledge, this is the first study to apply a physical
chemistry perspective to rigorously analyze the convergence behaviour of the
NMF algorithm. We start from mathematically proven asymptotic convergence
results and then show how they apply to real data. Experimental results
demonstrate that the proposed algorithm improves clustering accuracy by 5.6%
$\pm$ 2.7% on biomedical signals and 11.1% $\pm$ 7.2% on face images (mean
$\pm$ std).

</details>


### [181] [Three Forms of Stochastic Injection for Improved Distribution-to-Distribution Generative Modeling](https://arxiv.org/abs/2510.06634)
*Shiye Su,Yuhui Zhang,Linqi Zhou,Rajesh Ranganath,Serena Yeung-Levy*

Main category: cs.LG

TL;DR: Flow matching在一般分布到分布的转换中由于稀疏监督而失效，通过在训练过程中注入随机性来解决，在五个不同的成像任务中显著提高了生成质量。


<details>
  <summary>Details</summary>
Motivation: 在药物发现和进化模拟等应用中，对任意数据分布之间的转换进行建模是一个根本的科学挑战。Flow matching为这项任务提供了一个自然的框架，但其在一般分布到分布设置中的应用尚未被充分探索。

Method: 提出了一种简单且计算高效的方法，该方法通过扰动源样本和流插值器将随机性注入到训练过程中。

Result: 在跨越生物学、放射学和天文学的五个不同的成像任务中，该方法显著提高了生成质量，比现有基线平均高出 9 个 FID 点。该方法还降低了输入和生成的样本之间的传输成本，从而更好地突出了转换的真实效果。

Conclusion: 该方法使 flow matching 成为模拟科学中出现的各种分布转换的更实用的工具。

Abstract: Modeling transformations between arbitrary data distributions is a
fundamental scientific challenge, arising in applications like drug discovery
and evolutionary simulation. While flow matching offers a natural framework for
this task, its use has thus far primarily focused on the noise-to-data setting,
while its application in the general distribution-to-distribution setting is
underexplored. We find that in the latter case, where the source is also a data
distribution to be learned from limited samples, standard flow matching fails
due to sparse supervision. To address this, we propose a simple and
computationally efficient method that injects stochasticity into the training
process by perturbing source samples and flow interpolants. On five diverse
imaging tasks spanning biology, radiology, and astronomy, our method
significantly improves generation quality, outperforming existing baselines by
an average of 9 FID points. Our approach also reduces the transport cost
between input and generated samples to better highlight the true effect of the
transformation, making flow matching a more practical tool for simulating the
diverse distribution transformations that arise in science.

</details>


### [182] [StruSR: Structure-Aware Symbolic Regression with Physics-Informed Taylor Guidance](https://arxiv.org/abs/2510.06635)
*Yunpeng Gong,Sihan Lan,Can Yang,Kunpeng Xu,Min Jiang*

Main category: cs.LG

TL;DR: 提出了一种结构感知的符号回归框架 (StruSR)，利用物理信息神经网络 (PINN) 从时间序列数据中提取局部结构化的物理先验知识，以指导符号表达式的演化。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归方法缺乏从时间序列观测中提取结构化物理先验的机制，难以捕捉反映系统全局行为的符号表达式，因此需要一种能够有效利用物理先验知识的符号回归方法。

Method: 1. 利用训练好的 PINN 进行局部泰勒展开，获得基于导数的结构信息来指导符号表达式的演化。
2. 引入基于掩码的归因机制，量化每个子树对结构对齐和物理残差减少的贡献。
3. 混合适应度函数，共同最小化物理残差和泰勒系数不匹配。

Result: 在基准 PDE 系统上的实验表明，StruSR 提高了收敛速度、结构保真度和表达式可解释性。

Conclusion: StruSR 是一种基于物理的符号发现的有效范例。

Abstract: Symbolic regression aims to find interpretable analytical expressions by
searching over mathematical formula spaces to capture underlying system
behavior, particularly in scientific modeling governed by physical laws.
However, traditional methods lack mechanisms for extracting structured physical
priors from time series observations, making it difficult to capture symbolic
expressions that reflect the system's global behavior. In this work, we propose
a structure-aware symbolic regression framework, called StruSR, that leverages
trained Physics-Informed Neural Networks (PINNs) to extract locally structured
physical priors from time series data. By performing local Taylor expansions on
the outputs of the trained PINN, we obtain derivative-based structural
information to guide symbolic expression evolution. To assess the importance of
expression components, we introduce a masking-based attribution mechanism that
quantifies each subtree's contribution to structural alignment and physical
residual reduction. These sensitivity scores steer mutation and crossover
operations within genetic programming, preserving substructures with high
physical or structural significance while selectively modifying less
informative components. A hybrid fitness function jointly minimizes physics
residuals and Taylor coefficient mismatch, ensuring consistency with both the
governing equations and the local analytical behavior encoded by the PINN.
Experiments on benchmark PDE systems demonstrate that StruSR improves
convergence speed, structural fidelity, and expression interpretability
compared to conventional baselines, offering a principled paradigm for
physics-grounded symbolic discovery.

</details>


### [183] [Control-Augmented Autoregressive Diffusion for Data Assimilation](https://arxiv.org/abs/2510.06637)
*Prakhar Srivastava,Farrin Marouf Sofian,Francesco Immorlano,Kushagra Pandey,Stephan Mandt*

Main category: cs.LG

TL;DR: 本研究探索了自回归扩散模型（ARDM）中的指导问题，并提出了一个摊销框架，通过轻量级控制器网络增强预训练的ARDM，以预测未来的ARDM rollouts并学习逐步控制。


<details>
  <summary>Details</summary>
Motivation: 现有的数据同化（DA）方法在计算上过于昂贵，并且在稀疏观测下容易产生预测漂移。

Method: 该方法通过离线训练一个轻量级控制器网络，学习逐步控制，以预测未来的ARDM rollouts。在数据同化推理中，该方法将推理简化为单次前向 rollout，避免了昂贵的伴随计算或优化。

Result: 该方法在稳定、准确性和物理保真度方面始终优于四种最先进的基线方法。

Conclusion: 该研究提出的方法能够有效地解决数据同化问题，并在计算效率和预测精度上都取得了显著的提升。

Abstract: Despite recent advances in test-time scaling and finetuning of diffusion
models, guidance in Auto-Regressive Diffusion Models (ARDMs) remains
underexplored. We introduce an amortized framework that augments pretrained
ARDMs with a lightweight controller network, trained offline by previewing
future ARDM rollouts and learning stepwise controls that anticipate upcoming
observations under a terminal cost objective. We evaluate this framework in the
context of data assimilation (DA) for chaotic spatiotemporal partial
differential equations (PDEs), a setting where existing methods are often
computationally prohibitive and prone to forecast drift under sparse
observations. Our approach reduces DA inference to a single forward rollout
with on-the-fly corrections, avoiding expensive adjoint computations and/or
optimizations during inference. We demonstrate that our method consistently
outperforms four state-of-the-art baselines in stability, accuracy, and
physical fidelity across two canonical PDEs and six observation regimes. We
will release code and checkpoints publicly.

</details>


### [184] [The False Promise of Zero-Shot Super-Resolution in Machine-Learned Operators](https://arxiv.org/abs/2510.06646)
*Mansi Sakarvadia,Kareem Hegazy,Amin Totounferoush,Kyle Chard,Yaoqing Yang,Ian Foster,Michael W. Mahoney*

Main category: cs.LG

TL;DR: 这篇论文评估了机器学习算子（MLO）在“零样本超分辨率”方面的表现，即模型在比原始训练数据更高分辨率的数据上进行推理的能力。研究发现MLO在这方面表现不佳，提出了一个多分辨率训练协议来解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习和科学计算中的一个核心挑战是建模连续现象，而这些现象在实践中是离散表示的。机器学习算子（MLO）被引入作为实现这一建模目标的一种手段，因为这类架构可以在任意分辨率下进行推理。

Method: 该研究全面评估了MLO中的零样本子分辨率和超分辨率（即，多分辨率）推理。将多分辨率推理分解为两个关键行为：1）外推到不同的频率信息；2）在不同的分辨率之间进行插值。通过实验证明MLO无法以零样本方式完成这两项任务。

Result: 实验结果表明，MLO无法在与训练分辨率不同的分辨率下进行准确的推理，而且非常脆弱，容易产生混叠。

Conclusion: 为了解决这些失败模式，论文提出了一个简单、计算效率高且数据驱动的多分辨率训练协议，该协议可以克服混叠，并提供强大的多分辨率泛化能力。

Abstract: A core challenge in scientific machine learning, and scientific computing
more generally, is modeling continuous phenomena which (in practice) are
represented discretely. Machine-learned operators (MLOs) have been introduced
as a means to achieve this modeling goal, as this class of architecture can
perform inference at arbitrary resolution. In this work, we evaluate whether
this architectural innovation is sufficient to perform "zero-shot
super-resolution," namely to enable a model to serve inference on
higher-resolution data than that on which it was originally trained. We
comprehensively evaluate both zero-shot sub-resolution and super-resolution
(i.e., multi-resolution) inference in MLOs. We decouple multi-resolution
inference into two key behaviors: 1) extrapolation to varying frequency
information; and 2) interpolating across varying resolutions. We empirically
demonstrate that MLOs fail to do both of these tasks in a zero-shot manner.
Consequently, we find MLOs are not able to perform accurate inference at
resolutions different from those on which they were trained, and instead they
are brittle and susceptible to aliasing. To address these failure modes, we
propose a simple, computationally-efficient, and data-driven multi-resolution
training protocol that overcomes aliasing and that provides robust
multi-resolution generalization.

</details>


### [185] [Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions](https://arxiv.org/abs/2510.06649)
*Frank Wu,Mengye Ren*

Main category: cs.LG

TL;DR: 提出了一种新的基于 Forward-Forward 算法的强化学习方法，称为 Action-conditioned Root mean squared Q-Functions (ARQ)。


<details>
  <summary>Details</summary>
Motivation: Forward-Forward 算法主要应用于监督学习，在强化学习领域存在空白。

Method: 该方法利用 Forward-Forward 算法的优点，结合了时间差分学习和动作条件反射。

Result: 在 MinAtar 和 DeepMind Control Suite 基准测试中，该方法优于目前最好的局部反向传播自由强化学习方法，并且在大多数任务中优于使用反向传播训练的算法。

Conclusion: 提出的 ARQ 方法是一种简单且具有生物学基础的强化学习方法，具有良好的性能。

Abstract: The Forward-Forward (FF) Algorithm is a recently proposed learning procedure
for neural networks that employs two forward passes instead of the traditional
forward and backward passes used in backpropagation. However, FF remains
largely confined to supervised settings, leaving a gap at domains where
learning signals can be yielded more naturally such as RL. In this work,
inspired by FF's goodness function using layer activity statistics, we
introduce Action-conditioned Root mean squared Q-Functions (ARQ), a novel value
estimation method that applies a goodness function and action conditioning for
local RL using temporal difference learning. Despite its simplicity and
biological grounding, our approach achieves superior performance compared to
state-of-the-art local backprop-free RL methods in the MinAtar and the DeepMind
Control Suite benchmarks, while also outperforming algorithms trained with
backpropagation on most tasks. Code can be found at
https://github.com/agentic-learning-ai-lab/arq.

</details>


### [186] [Rethinking Nonlinearity: Trainable Gaussian Mixture Modules for Modern Neural Architectures](https://arxiv.org/abs/2510.06660)
*Weiguo Lu,Gangnan Yuan,Hong-kun Zhang,Shangyang Li*

Main category: cs.LG

TL;DR: 提出了高斯混合模型启发的非线性模块（GMNM），以增强神经网络的非线性能力。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络的非线性能力受限于激活函数的选择。

Method: 利用高斯混合模型（GMM）的通用密度近似和高斯核的距离属性，设计了一种新的可微模块GMNM，并将其集成到各种神经网络架构中。

Result: 将GMNM集成到MLP、CNN、注意力机制和LSTM等架构中，均能持续提升性能。

Conclusion: GMNM是一种强大而灵活的模块，有潜力提高各种机器学习应用的效率和准确性。

Abstract: Neural networks in general, from MLPs and CNNs to attention-based
Transformers, are constructed from layers of linear combinations followed by
nonlinear operations such as ReLU, Sigmoid, or Softmax. Despite their strength,
these conventional designs are often limited in introducing non-linearity by
the choice of activation functions. In this work, we introduce Gaussian
Mixture-Inspired Nonlinear Modules (GMNM), a new class of differentiable
modules that draw on the universal density approximation Gaussian mixture
models (GMMs) and distance properties (metric space) of Gaussian kernal. By
relaxing probabilistic constraints and adopting a flexible parameterization of
Gaussian projections, GMNM can be seamlessly integrated into diverse neural
architectures and trained end-to-end with gradient-based methods. Our
experiments demonstrate that incorporating GMNM into architectures such as
MLPs, CNNs, attention mechanisms, and LSTMs consistently improves performance
over standard baselines. These results highlight GMNM's potential as a powerful
and flexible module for enhancing efficiency and accuracy across a wide range
of machine learning applications.

</details>


### [187] [The Effect of Attention Head Count on Transformer Approximation](https://arxiv.org/abs/2510.06662)
*Penghao Yu,Haotian Jiang,Zeyu Bao,Ruoxi Yu,Qianxiao Li*

Main category: cs.LG

TL;DR: 本文研究了Transformer的逼近性质，重点关注注意力头的数量。


<details>
  <summary>Details</summary>
Motivation: 对Transformer的结构参数如何影响表达能力缺乏详细理解。

Method: 通过引入广义D-检索任务，建立了理论框架，并推导了参数复杂度的上下界。

Result: 证明了具有足够多头的Transformer可以实现有效逼近，而头数过少时，参数数量必须至少按$O(1/\\epsilon^{cT})$的比例缩放。此外，还证明了单头Transformer的嵌入维度为$O(T)$时，可以完全记忆输入。

Conclusion: 实验验证了理论结果的实际意义。

Abstract: Transformer has become the dominant architecture for sequence modeling, yet a
detailed understanding of how its structural parameters influence expressive
power remains limited. In this work, we study the approximation properties of
transformers, with particular emphasis on the role of the number of attention
heads. Our analysis begins with the introduction of a generalized $D$-retrieval
task, which we prove to be dense in the space of continuous functions, thereby
providing the basis for our theoretical framework. We then establish both upper
and lower bounds on the parameter complexity required for
$\epsilon$-approximation. Specifically, we show that transformers with
sufficiently many heads admit efficient approximation, whereas with too few
heads, the number of parameters must scale at least as $O(1/\epsilon^{cT})$,
for some constant $c$ and sequence length $T$. To the best of our knowledge,
this constitutes the first rigorous lower bound of this type in a nonlinear and
practically relevant setting. We further examine the single-head case and
demonstrate that an embedding dimension of order $O(T)$ allows complete
memorization of the input, where approximation is entirely achieved by the
feed-forward block. Finally, we validate our theoretical findings with
experiments on both synthetic data and real-world tasks, illustrating the
practical relevance of our results.

</details>


### [188] [XRPO: Pushing the limits of GRPO with Targeted Exploration and Exploitation](https://arxiv.org/abs/2510.06672)
*Udbhav Bamba,Minghao Fang,Yifan Yu,Haizhong Zheng,Fan Lai*

Main category: cs.LG

TL;DR: XRPO: A reinforcement learning framework that improves large language model reasoning by balancing exploration and exploitation.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning methods for large language models suffer from limited exploration and underexploited feedback signals.

Method: XRPO introduces an adaptive rollout allocator, in-context seeding, and a group-relative, novelty-aware advantage sharpening mechanism.

Result: XRPO outperforms existing methods on math and coding benchmarks, improving performance and accelerating training convergence.

Conclusion: XRPO is a unified framework that enhances policy optimization through rollout exploration-exploitation.

Abstract: Reinforcement learning algorithms such as GRPO have driven recent advances in
large language model (LLM) reasoning. While scaling the number of rollouts
stabilizes training, existing approaches suffer from limited exploration on
challenging prompts and leave informative feedback signals underexploited, due
to context-independent rollout allocation across prompts (e.g., generating 16
rollouts per prompt) and relying heavily on sparse rewards. This paper presents
XRPO(eXplore - eXploit GRPO), a unified framework that recasts policy
optimization through the principled lens of rollout exploration-exploitation.
To enhance exploration, XRPO introduces a mathematically grounded rollout
allocator that adaptively prioritizes prompts with higher potential for
uncertainty reduction. It further addresses stagnation on zero-reward prompts
through an in-context seeding strategy that injects curated exemplars, steering
the model into more difficult reasoning trajectories. To strengthen
exploitation, XRPO develops a group-relative, novelty-aware advantage
sharpening mechanism that leverages sequence likelihoods to amplify
low-probability yet correct responses, thereby extending the policy's reach
beyond sparse rewards. Experiments across diverse math and coding benchmarks on
both reasoning and non-reasoning models demonstrate that XRPO outperforms
existing advances (e.g., GRPO and GSPO) up to 4% pass@1 and 6% cons@32, while
accelerating training convergence by up to 2.7X.

</details>


### [189] [TimeFormer: Transformer with Attention Modulation Empowered by Temporal Characteristics for Time Series Forecasting](https://arxiv.org/abs/2510.06680)
*Zhipeng Liu,Peibo Duan,Xuan Tang,Baixin Li,Yongsheng Huang,Mingyang Geng,Changsheng Zhang,Bin Zhang,Binwu Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为TimeFormer的新型Transformer架构，专门为时间序列数据设计，旨在最大限度地提高其表征能力。


<details>
  <summary>Details</summary>
Motivation: Transformer在自然语言处理中表现出色，但由于对文本和时间模态之间差异的考虑不足，因此扩展到时间序列预测仍然具有挑战性。

Method: TimeFormer的核心创新是一种具有两个调制项（MoSA）的自注意力机制，旨在捕获Hawkes过程和因果掩蔽约束下的时间序列的时间先验。此外，TimeFormer引入了一个基于多尺度和子序列分析的框架，以捕获不同时间尺度的语义依赖关系，丰富时间依赖性。

Result: 在多个真实世界数据集上进行的大量实验表明，TimeFormer显着优于最先进的方法，与最佳基线相比，MSE降低了7.45％，并在94.04％的评估指标上设置了新的基准。

Conclusion: MoSA机制可以广泛应用于增强其他基于Transformer的模型的性能。

Abstract: Although Transformers excel in natural language processing, their extension
to time series forecasting remains challenging due to insufficient
consideration of the differences between textual and temporal modalities. In
this paper, we develop a novel Transformer architecture designed for time
series data, aiming to maximize its representational capacity. We identify two
key but often overlooked characteristics of time series: (1) unidirectional
influence from the past to the future, and (2) the phenomenon of decaying
influence over time. These characteristics are introduced to enhance the
attention mechanism of Transformers. We propose TimeFormer, whose core
innovation is a self-attention mechanism with two modulation terms (MoSA),
designed to capture these temporal priors of time series under the constraints
of the Hawkes process and causal masking. Additionally, TimeFormer introduces a
framework based on multi-scale and subsequence analysis to capture semantic
dependencies at different temporal scales, enriching the temporal dependencies.
Extensive experiments conducted on multiple real-world datasets show that
TimeFormer significantly outperforms state-of-the-art methods, achieving up to
a 7.45% reduction in MSE compared to the best baseline and setting new
benchmarks on 94.04\% of evaluation metrics. Moreover, we demonstrate that the
MoSA mechanism can be broadly applied to enhance the performance of other
Transformer-based models.

</details>


### [190] [Distributed Algorithms for Multi-Agent Multi-Armed Bandits with Collision](https://arxiv.org/abs/2510.06683)
*Daoyuan Zhou,Xuchuang Wang,Lin Yang,Yang Gao*

Main category: cs.LG

TL;DR: 研究随机多人多臂老虎机（MMAB）问题，多个玩家选择臂以最大化累积奖励。当两个或多个玩家选择同一臂时发生碰撞，导致没有奖励，并且被相关玩家观察到。


<details>
  <summary>Details</summary>
Motivation: 在没有中心协调的分布式环境中，每个玩家只能观察到自己的行为和碰撞反馈。

Method: 提出了一种具有自适应、高效通信协议的分布式算法。

Result: 该算法实现了接近最优的群体和个体后悔值，通信成本仅为$\\\mathcal{O}(\log\log T)$。实验表明，与现有基线相比，性能有显着提高。与最先进的（SOTA）方法相比，我们的方法在减少个体后悔值方面取得了显着成果。最后，我们将我们的方法扩展到周期性异步设置，证明了该问题的下界，并提出了一种实现对数后悔值的算法。

Conclusion: 在周期性异步设置下，算法实现了对数后悔值

Abstract: We study the stochastic Multiplayer Multi-Armed Bandit (MMAB) problem, where
multiple players select arms to maximize their cumulative rewards. Collisions
occur when two or more players select the same arm, resulting in no reward, and
are observed by the players involved. We consider a distributed setting without
central coordination, where each player can only observe their own actions and
collision feedback. We propose a distributed algorithm with an adaptive,
efficient communication protocol. The algorithm achieves near-optimal group and
individual regret, with a communication cost of only $\mathcal{O}(\log\log T)$.
Our experiments demonstrate significant performance improvements over existing
baselines. Compared to state-of-the-art (SOTA) methods, our approach achieves a
notable reduction in individual regret. Finally, we extend our approach to a
periodic asynchronous setting, proving the lower bound for this problem and
presenting an algorithm that achieves logarithmic regret.

</details>


### [191] [AutoBalance: An Automatic Balancing Framework for Training Physics-Informed Neural Networks](https://arxiv.org/abs/2510.06684)
*Kang An,Chenhao Si,Ming Yan,Shiqian Ma*

Main category: cs.LG

TL;DR: 本文介绍了一种新的训练范式 AutoBalance，用于解决物理信息神经网络 (PINN) 训练中平衡多个损失项的问题。


<details>
  <summary>Details</summary>
Motivation: PINN 训练困难，因为需要平衡多个损失项（例如 PDE 残差和边界条件），这些损失项通常具有冲突的目标和差异很大的曲率。现有的方法通过在优化之前操纵梯度来解决这个问题（一种“预组合”策略），但这种方法从根本上是有限的。

Method: AutoBalance 为每个损失分量分配一个独立的自适应优化器，并在之后聚合由此产生的预处理更新。

Result: 在具有挑战性的 PDE 基准上的大量实验表明，AutoBalance 始终优于现有的框架，在解决方案误差方面取得了显着的降低。

Conclusion: AutoBalance 与其他流行的 PINN 方法正交且互补，从而在要求苛刻的基准上放大了它们的有效性。

Abstract: Physics-Informed Neural Networks (PINNs) provide a powerful and general
framework for solving Partial Differential Equations (PDEs) by embedding
physical laws into loss functions. However, training PINNs is notoriously
difficult due to the need to balance multiple loss terms, such as PDE residuals
and boundary conditions, which often have conflicting objectives and vastly
different curvatures. Existing methods address this issue by manipulating
gradients before optimization (a "pre-combine" strategy). We argue that this
approach is fundamentally limited, as forcing a single optimizer to process
gradients from spectrally heterogeneous loss landscapes disrupts its internal
preconditioning. In this work, we introduce AutoBalance, a novel "post-combine"
training paradigm. AutoBalance assigns an independent adaptive optimizer to
each loss component and aggregates the resulting preconditioned updates
afterwards. Extensive experiments on challenging PDE benchmarks show that
AutoBalance consistently outperforms existing frameworks, achieving significant
reductions in solution error, as measured by both the MSE and $L^{\infty}$
norms. Moreover, AutoBalance is orthogonal to and complementary with other
popular PINN methodologies, amplifying their effectiveness on demanding
benchmarks.

</details>


### [192] [Is the Hard-Label Cryptanalytic Model Extraction Really Polynomial?](https://arxiv.org/abs/2510.06692)
*Akira Ito,Takayuki Miura,Yosuke Todo*

Main category: cs.LG

TL;DR: 本文提出了一种新的模型提取攻击方法，可以有效降低查询复杂度，缓解现有模型提取方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有模型提取方法在攻击深度增加时，假设变得不切实际，需要指数级的查询次数。

Method: 提出了一种名为CrossLayer Extraction的新攻击方法，利用跨层神经元交互从更深层提取信息。

Result: 该技术显著降低了查询复杂度。

Conclusion: CrossLayer Extraction攻击方法能够有效降低模型提取的查询复杂度，缓解现有方法的局限性。

Abstract: Deep Neural Networks (DNNs) have attracted significant attention, and their
internal models are now considered valuable intellectual assets. Extracting
these internal models through access to a DNN is conceptually similar to
extracting a secret key via oracle access to a block cipher. Consequently,
cryptanalytic techniques, particularly differential-like attacks, have been
actively explored recently. ReLU-based DNNs are the most commonly and widely
deployed architectures. While early works (e.g., Crypto 2020, Eurocrypt 2024)
assume access to exact output logits, which are usually invisible, more recent
works (e.g., Asiacrypt 2024, Eurocrypt 2025) focus on the hard-label setting,
where only the final classification result (e.g., "dog" or "car") is available
to the attacker. Notably, Carlini et al. (Eurocrypt 2025) demonstrated that
model extraction is feasible in polynomial time even under this restricted
setting.
  In this paper, we first show that the assumptions underlying their attack
become increasingly unrealistic as the attack-target depth grows. In practice,
satisfying these assumptions requires an exponential number of queries with
respect to the attack depth, implying that the attack does not always run in
polynomial time. To address this critical limitation, we propose a novel attack
method called CrossLayer Extraction. Instead of directly extracting the secret
parameters (e.g., weights and biases) of a specific neuron, which incurs
exponential cost, we exploit neuron interactions across layers to extract this
information from deeper layers. This technique significantly reduces query
complexity and mitigates the limitations of existing model extraction
approaches.

</details>


### [193] [A Diffusion Model for Regular Time Series Generation from Irregular Data with Completion and Masking](https://arxiv.org/abs/2510.06699)
*Gal Fadlon,Idan Arbiv,Nimrod Berman,Omri Azencot*

Main category: cs.LG

TL;DR: 提出了一种新的两步框架，用于生成具有非规律采样和缺失值的真实时间序列数据。


<details>
  <summary>Details</summary>
Motivation: 生成真实的时间序列数据对于医疗保健、金融和科学等应用至关重要。然而，不规则的采样和缺失值带来了巨大的挑战。

Method: 首先，时间序列转换器完成不规则序列，创建自然邻域；其次，一个基于视觉的扩散模型与掩蔽最小化对完成值的依赖。

Result: 该方法实现了最先进的性能，在判别分数上实现了 70% 的相对提高，在计算成本上实现了 85% 的相对提高。

Conclusion: 该方法结合了补全和掩蔽的优点，能够稳健有效地生成真实的时间序列。

Abstract: Generating realistic time series data is critical for applications in
healthcare, finance, and science. However, irregular sampling and missing
values present significant challenges. While prior methods address these
irregularities, they often yield suboptimal results and incur high
computational costs. Recent advances in regular time series generation, such as
the diffusion-based ImagenTime model, demonstrate strong, fast, and scalable
generative capabilities by transforming time series into image representations,
making them a promising solution. However, extending ImagenTime to irregular
sequences using simple masking introduces "unnatural" neighborhoods, where
missing values replaced by zeros disrupt the learning process. To overcome
this, we propose a novel two-step framework: first, a Time Series Transformer
completes irregular sequences, creating natural neighborhoods; second, a
vision-based diffusion model with masking minimizes dependence on the completed
values. This approach leverages the strengths of both completion and masking,
enabling robust and efficient generation of realistic time series. Our method
achieves state-of-the-art performance, achieving a relative improvement in
discriminative score by $70\%$ and in computational cost by $85\%$. Code is at
https://github.com/azencot-group/ImagenI2R.

</details>


### [194] [Dual Goal Representations](https://arxiv.org/abs/2510.06714)
*Seohong Park,Deepinder Mann,Sergey Levine*

Main category: cs.LG

TL;DR: 本文提出了用于目标条件强化学习 (GCRL) 的双重目标表示。


<details>
  <summary>Details</summary>
Motivation: 双重目标表示通过时间距离来表征状态与其他状态的关系，从而对状态进行编码。这种表示只依赖于环境的内在动态，并且对原始状态表示是不变的，同时能够过滤掉外生噪声。

Method: 基于双重目标表示的概念，我们开发了一种实用的目标表示学习方法，该方法可以与任何现有的 GCRL 算法相结合。

Result: 通过在 OGBench 任务套件上的各种实验，我们通过实验表明，在 20 个基于状态和像素的任务中，双重目标表示始终可以提高离线目标实现性能。

Conclusion: 双重目标表示可以提高离线目标实现性能。

Abstract: In this work, we introduce dual goal representations for goal-conditioned
reinforcement learning (GCRL). A dual goal representation characterizes a state
by "the set of temporal distances from all other states"; in other words, it
encodes a state through its relations to every other state, measured by
temporal distance. This representation provides several appealing theoretical
properties. First, it depends only on the intrinsic dynamics of the environment
and is invariant to the original state representation. Second, it contains
provably sufficient information to recover an optimal goal-reaching policy,
while being able to filter out exogenous noise. Based on this concept, we
develop a practical goal representation learning method that can be combined
with any existing GCRL algorithm. Through diverse experiments on the OGBench
task suite, we empirically show that dual goal representations consistently
improve offline goal-reaching performance across 20 state- and pixel-based
tasks.

</details>


### [195] [Incorporating Expert Knowledge into Bayesian Causal Discovery of Mixtures of Directed Acyclic Graphs](https://arxiv.org/abs/2510.06735)
*Zachris Björkman,Jorge Loría,Sophie Wharrie,Samuel Kaski*

Main category: cs.LG

TL;DR: 提出了一种用于异构环境下的因果推断方法，该方法基于贝叶斯实验设计和变分混合结构学习，能够利用专家先验知识来改进因果贝叶斯网络的学习。


<details>
  <summary>Details</summary>
Motivation: 在异构领域中，因果发现需要领域专家的先验信息，但现有的先验获取方法不适用于异构领域。

Method: 提出了一种基于贝叶斯实验设计的因果获取策略和变分混合结构学习方法（VaMSL），用于迭代推断因果贝叶斯网络（CBNs）的混合。

Result: 该方法成功生成了一组替代因果模型（混合成分或簇），并在模拟专家提供信息的情况下，提高了异构合成数据的结构学习性能。在乳腺癌数据库中，该方法能够捕获复杂的分布。

Conclusion: 该方法在异构领域中，利用专家先验知识，能够有效地学习因果结构。

Abstract: Bayesian causal discovery benefits from prior information elicited from
domain experts, and in heterogeneous domains any prior knowledge would be badly
needed. However, so far prior elicitation approaches have assumed a single
causal graph and hence are not suited to heterogeneous domains. We propose a
causal elicitation strategy for heterogeneous settings, based on Bayesian
experimental design (BED) principles, and a variational mixture structure
learning (VaMSL) method -- extending the earlier differentiable Bayesian
structure learning (DiBS) method -- to iteratively infer mixtures of causal
Bayesian networks (CBNs). We construct an informative graph prior incorporating
elicited expert feedback in the inference of mixtures of CBNs. Our proposed
method successfully produces a set of alternative causal models (mixture
components or clusters), and achieves an improved structure learning
performance on heterogeneous synthetic data when informed by a simulated
expert. Finally, we demonstrate that our approach is capable of capturing
complex distributions in a breast cancer database.

</details>
