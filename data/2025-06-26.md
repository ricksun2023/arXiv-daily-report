<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 21]
- [cs.CV](#cs.CV) [Total: 22]
- [cs.AI](#cs.AI) [Total: 21]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 23]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation](https://arxiv.org/abs/2506.19952)
*Deepon Halder,Thanmay Jayakumar,Raj Dabre*

Main category: cs.CL

TL;DR: CycleDistill是一种自举方法，它利用LLM和few-shot翻译来获得高质量的MT系统，并且只需要少量的few-shot示例。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(llm)，尽管它们具有执行few-shot机器翻译(MT)的能力，但通常落后于在平行语料库上训练的专用MT系统，这对于高质量的机器翻译(MT)至关重要。然而，对于低资源语言来说，平行语料库通常是稀缺的或不存在的。

Method: CycleDistill，一种利用LLM和few-shot翻译的自举方法，以获得高质量的MT系统。CycleDistill涉及通过zero- or few-shot MT从单语语料库中迭代生成合成平行语料库，然后用于微调用于生成所述数据的MT模型。

Result: 仅依赖于单语语料库，它可以实现高质量的机器翻译，在第一次迭代中，在few-shot基线模型的基础上，平均提高了20-30个chrF点。我们还研究了在蒸馏过程中利用softmax激活的影响，并观察到翻译质量的轻微提高。

Conclusion: CycleDistill可以实现高质量的机器翻译，在第一次迭代中，在few-shot基线模型的基础上，平均提高了20-30个chrF点

Abstract: Large language models (LLMs), despite their ability to perform few-shot
machine translation (MT), often lag behind dedicated MT systems trained on
parallel corpora, which are crucial for high quality machine translation (MT).
However, parallel corpora are often scarce or non-existent for low-resource
languages. In this paper, we propose CycleDistill, a bootstrapping approach
leveraging LLMs and few-shot translation to obtain high-quality MT systems.
CycleDistill involves iteratively generating synthetic parallel corpora from
monolingual corpora via zero- or few-shot MT, which is then used to fine-tune
the model that was used for generating said data for MT. CycleDistill does not
need parallel corpora beyond 1 to 4 few-shot examples, and in our experiments
focusing on three Indian languages, by relying solely on monolingual corpora,
it can achieve high-quality machine translation, improving upon a few-shot
baseline model by over 20-30 chrF points on average in the first iteration. We
also study the effect of leveraging softmax activations during the distillation
process and observe mild improvements in translation quality.

</details>


### [2] [Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs](https://arxiv.org/abs/2506.19967)
*Travis Thompson,Seung-Hwan Lim,Paul Liu,Ruoying He,Dongkuan Xu*

Main category: cs.CL

TL;DR: Inference-Scaled GraphRAG enhances LLM-based graph reasoning by applying inference-time compute scaling, improving multi-hop question answering performance.


<details>
  <summary>Details</summary>
Motivation: LLMs underperform on knowledge-intensive reasoning tasks due to limited access to structured context and multi-hop information. Conventional RAG and GraphRAG methods often fail to capture relational structure across nodes in knowledge graphs.

Method: Inference-Scaled GraphRAG, combines sequential scaling with deep chain-of-thought graph traversal, and parallel scaling with majority voting over sampled trajectories within an interleaved reasoning-execution loop.

Result: Achieves substantial gains over both traditional GraphRAG and prior graph traversal baselines on the GRBench benchmark.

Conclusion: Inference-time scaling significantly improves multi-hop question answering performance on structured knowledge reasoning tasks with LLMs.

Abstract: Large Language Models (LLMs) have achieved impressive capabilities in
language understanding and generation, yet they continue to underperform on
knowledge-intensive reasoning tasks due to limited access to structured context
and multi-hop information. Retrieval-Augmented Generation (RAG) partially
mitigates this by grounding generation in retrieved context, but conventional
RAG and GraphRAG methods often fail to capture relational structure across
nodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel
framework that enhances LLM-based graph reasoning by applying inference-time
compute scaling. Our method combines sequential scaling with deep
chain-of-thought graph traversal, and parallel scaling with majority voting
over sampled trajectories within an interleaved reasoning-execution loop.
Experiments on the GRBench benchmark demonstrate that our approach
significantly improves multi-hop question answering performance, achieving
substantial gains over both traditional GraphRAG and prior graph traversal
baselines. These findings suggest that inference-time scaling is a practical
and architecture-agnostic solution for structured knowledge reasoning with LLMs

</details>


### [3] [Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation](https://arxiv.org/abs/2506.19998)
*Xinyi Ni,Haonan Jian,Qiuyang Wang,Vedanshi Chetan Shah,Pengyu Hong*

Main category: cs.CL

TL;DR: Doc2Agent 是一个可扩展的管道，用于构建可以调用从 API 文档生成的基于 Python 的工具的代理。


<details>
  <summary>Details</summary>
Motivation: 大多数基于 API 的代理依赖于精选和统一的工具集，这些工具集不能反映真实世界 API 的复杂性。为任意领域构建工具使用代理仍然是一个主要的挑战，因为它需要阅读非结构化的 API 文档，测试 API 并推断正确的参数。

Method: Doc2Agent 生成可从 API 文档执行的工具，并使用代码代理迭代地改进它们。

Result: 在 WebArena 基准测试中，与直接 API 调用相比，我们实现了 55% 的相对性能提升，成本降低了 90%。为糖材料科学构建的特定领域代理进一步证明了该管道对复杂、知识丰富的任务的适应性。

Conclusion: Doc2Agent 提供了一种通用的解决方案，用于大规模地从非结构化 API 文档构建工具代理。

Abstract: REST APIs play important roles in enriching the action space of web agents,
yet most API-based agents rely on curated and uniform toolsets that do not
reflect the complexity of real-world APIs. Building tool-using agents for
arbitrary domains remains a major challenge, as it requires reading
unstructured API documentation, testing APIs and inferring correct parameters.
We propose Doc2Agent, a scalable pipeline to build agents that can call
Python-based tools generated from API documentation. Doc2Agent generates
executable tools from API documentations and iteratively refines them using a
code agent. We evaluate our approach on real-world APIs, WebArena APIs, and
research APIs, producing validated tools. We achieved a 55\% relative
performance improvement with 90\% lower cost compared to direct API calling on
WebArena benchmark. A domain-specific agent built for glycomaterial science
further demonstrates the pipeline's adaptability to complex, knowledge-rich
tasks. Doc2Agent offers a generalizable solution for building tool agents from
unstructured API documentation at scale.

</details>


### [4] [Knowledge-Aware Diverse Reranking for Cross-Source Question Answering](https://arxiv.org/abs/2506.20476)
*Tong Zhou*

Main category: cs.CL

TL;DR: Team Marikarp's knowledge-aware diverse reranking RAG pipeline won the SIGIR 2025 LiveRAG competition.


<details>
  <summary>Details</summary>
Motivation: fair evaluation of retrieving question-relevant supporting documents from a 15M documents subset of the FineWeb corpus.

Method: knowledge-aware diverse reranking RAG pipeline

Result: achieved first place in the competition.

Conclusion: The knowledge-aware diverse reranking RAG pipeline achieved first place in the competition.

Abstract: This paper presents Team Marikarp's solution for the SIGIR 2025 LiveRAG
competition. The competition's evaluation set, automatically generated by
DataMorgana from internet corpora, encompassed a wide range of target topics,
question types, question formulations, audience types, and knowledge
organization methods. It offered a fair evaluation of retrieving
question-relevant supporting documents from a 15M documents subset of the
FineWeb corpus. Our proposed knowledge-aware diverse reranking RAG pipeline
achieved first place in the competition.

</details>


### [5] [ReCode: Updating Code API Knowledge with Reinforcement Learning](https://arxiv.org/abs/2506.20495)
*Haoze Wu,Yunzhi Yao,Wenhao Yu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: ReCode是一个新框架，通过强化学习提升大型语言模型在动态API场景下的代码生成能力，且对模型通用代码生成能力影响较小。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在适应外部库API的频繁更新时表现不佳，因为它们依赖于训练数据中过时的API知识，即使可以访问最新的文档。

Method: 提出ReCode框架，该框架模仿人类程序员适应API变化，构建包含约2000条数据的数据集以训练LLM执行版本迁移，并引入改进的字符串相似性度量作为强化学习的奖励。

Result: ReCode显著提升了LLM在动态API场景中的代码生成性能，尤其是在未见过的CodeUpdateArena任务上。与监督微调相比，ReCode对LLM的通用代码生成能力影响较小。在各种LLM和强化学习算法上的应用均取得了持续的改进。

Conclusion: ReCode通过强化学习提升大型语言模型在动态API场景下的代码生成性能，且对模型通用代码生成能力影响较小。Qwen2.5-Coder-7B在训练后超过了32B参数的代码指令调优模型。

Abstract: Large Language Models (LLMs) exhibit remarkable code generation capabilities
but falter when adapting to frequent updates in external library APIs. This
critical limitation, stemming from reliance on outdated API knowledge from
their training data, even with access to current documentation, impedes
reliable code generation in dynamic environments. To tackle this issue, we
propose ReCode (rule-based Reinforcement learning for Code Update), a novel
framework that mimics human programmer adaptation to API changes. Specifically,
we construct a dataset of approximately 2,000 data entries to train the LLMs to
perform version migration based on updated information. Then, we introduce a
modified string similarity metric for code evaluation as the reward for
reinforcement learning. Our experiments demonstrate that ReCode substantially
boosts LLMs' code generation performance in dynamic API scenarios, especially
on the unseen CodeUpdateArena task. Crucially, compared to supervised
fine-tuning, ReCode has less impact on LLMs' general code generation abilities.
We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and
DAPO), all achieving consistent improvements. Notably, after training,
Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned
model and the reasoning model with the same architecture. Code is available at
https://github.com/zjunlp/ReCode.

</details>


### [6] [A Modular Multitask Reasoning Framework Integrating Spatio-temporal Models and LLMs](https://arxiv.org/abs/2506.20073)
*Kethmi Hirushini Hettige,Jiahao Ji,Cheng Long,Shili Xiang,Gao Cong,Jingyuan Wang*

Main category: cs.CL

TL;DR: STReason是一个新的框架，它集成了LLM和时空模型，用于多任务推理和执行，优于现有的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的时空数据挖掘模型通常仅限于狭窄的任务，缺乏多任务推理和复杂的长篇推理能力，这限制了它们在现实世界中的适用性。

Method: STReason框架集成了大型语言模型（LLM）的推理能力与时空模型的分析能力，用于多任务推理和执行。它利用上下文学习将复杂的自然语言查询分解为模块化的、可解释的程序，然后系统地执行这些程序以生成解决方案和详细的基本原理，而无需特定于任务的微调。

Result: STReason显著优于先进的LLM基线，尤其是在复杂的、推理密集的时空场景中。人工评估进一步验证了STReason的可信度和实用性。

Conclusion: STReason在复杂时空推理场景中显著优于先进的LLM基线，并通过了人工评估，验证了其可信度和实用性，表明其有潜力减少专家工作量并拓宽其实际应用。

Abstract: Spatio-temporal data mining plays a pivotal role in informed decision making
across diverse domains. However, existing models are often restricted to narrow
tasks, lacking the capacity for multi-task inference and complex long-form
reasoning that require generation of in-depth, explanatory outputs. These
limitations restrict their applicability to real-world, multi-faceted decision
scenarios. In this work, we introduce STReason, a novel framework that
integrates the reasoning strengths of large language models (LLMs) with the
analytical capabilities of spatio-temporal models for multi-task inference and
execution. Without requiring task-specific finetuning, STReason leverages
in-context learning to decompose complex natural language queries into modular,
interpretable programs, which are then systematically executed to generate both
solutions and detailed rationales. To facilitate rigorous evaluation, we
construct a new benchmark dataset and propose a unified evaluation framework
with metrics specifically designed for long-form spatio-temporal reasoning.
Experimental results show that STReason significantly outperforms advanced LLM
baselines across all metrics, particularly excelling in complex,
reasoning-intensive spatio-temporal scenarios. Human evaluations further
validate STReason's credibility and practical utility, demonstrating its
potential to reduce expert workload and broaden the applicability to real-world
spatio-temporal tasks. We believe STReason provides a promising direction for
developing more capable and generalizable spatio-temporal reasoning systems.

</details>


### [7] [SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization](https://arxiv.org/abs/2506.20081)
*Dhruv Gupta,Gayathri Ganesh Lakshmy,Yiqing Xie*

Main category: cs.CL

TL;DR: The paper analyzes code retrieval, finds issues with current methods, and proposes SACL to improve performance.


<details>
  <summary>Details</summary>
Motivation: current retrievers heavily rely on surface-level textual features and exhibit a strong bias towards well-documented code, even if the documentation is irrelevant

Method: a framework that enriches textual information and reduces bias by augmenting code or structural knowledge with semantic information

Result: SACL substantially improves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on HumanEval / MBPP / SWE-Bench-Lite), which also leads to better code generation performance (e.g., by 4.88% Pass@1 on HumanEval).

Conclusion: SACL improves code retrieval and code generation performance.

Abstract: Retrieval-Augmented Code Generation (RACG) is a critical technique for
enhancing code generation by retrieving relevant information. In this work, we
conduct an in-depth analysis of code retrieval by systematically masking
specific features while preserving code functionality. Our discoveries include:
(1) although trained on code, current retrievers heavily rely on surface-level
textual features (e.g., docstrings, identifier names), and (2) they exhibit a
strong bias towards well-documented code, even if the documentation is
irrelevant.Based on our discoveries, we propose SACL, a framework that enriches
textual information and reduces bias by augmenting code or structural knowledge
with semantic information. Extensive experiments show that SACL substantially
improves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on HumanEval /
MBPP / SWE-Bench-Lite), which also leads to better code generation performance
(e.g., by 4.88% Pass@1 on HumanEval).

</details>


### [8] [Bridging Compositional and Distributional Semantics: A Survey on Latent Semantic Geometry via AutoEncoder](https://arxiv.org/abs/2506.20083)
*Yingji Zhang,Danilo S. Carvalho,André Freitas*

Main category: cs.CL

TL;DR: This survey explores how integrating compositional and symbolic properties into language models can improve their performance. It focuses on the latent space geometry and reviews three autoencoder architectures (VAE, VQVAE, SAE).


<details>
  <summary>Details</summary>
Motivation: Integrating compositional and symbolic properties into current distributional semantic spaces can enhance the interpretability, controllability, compositionality, and generalisation capabilities of Transformer-based auto-regressive language models (LMs). Enable a bridge between symbolic and distributional semantics, helping to mitigate the gap between them.

Method: Review and compare three mainstream autoencoder architectures-Variational AutoEncoder (VAE), Vector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE).

Result: a novel perspective on latent space geometry through the lens of compositional semantics, a direction we refer to as semantic representation learning

Conclusion: This survey reviews and compares three mainstream autoencoder architectures (VAE, VQVAE, and SAE) and examines the distinctive latent geometries they induce in relation to semantic structure and interpretability.

Abstract: Integrating compositional and symbolic properties into current distributional
semantic spaces can enhance the interpretability, controllability,
compositionality, and generalisation capabilities of Transformer-based
auto-regressive language models (LMs). In this survey, we offer a novel
perspective on latent space geometry through the lens of compositional
semantics, a direction we refer to as \textit{semantic representation
learning}. This direction enables a bridge between symbolic and distributional
semantics, helping to mitigate the gap between them. We review and compare
three mainstream autoencoder architectures-Variational AutoEncoder (VAE),
Vector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE)-and examine the
distinctive latent geometries they induce in relation to semantic structure and
interpretability.

</details>


### [9] [ITFormer: Bridging Time Series and Natural Language for Multi-Modal QA with Large-Scale Multitask Dataset](https://arxiv.org/abs/2506.20093)
*Yilin Wang,Peixuan Lei,Jie Song,Yuzhe Hao,Tao Chen,Yuxuan Zhang,Lei Jia,Yuanxiang Li,Zhongyu Wei*

Main category: cs.CL

TL;DR: Introduces EngineMT-QA, a large-scale time-series QA dataset, and ITFormer, a framework that bridges time-series encoders with frozen LLMs for improved QA accuracy.


<details>
  <summary>Details</summary>
Motivation: Effectively integrating high-dimensional temporal signals with natural language for dynamic, interactive tasks is a significant challenge.

Method: The paper proposes the Instruct Time Transformer (ITFormer) framework.

Result: ITFormer achieves a strong improvement in QA accuracy over strong baselines with fewer than 1% additional trainable parameters.

Conclusion: The paper introduces ITFormer, a framework that integrates time-series encoders with frozen large language models, achieving improved QA accuracy with minimal additional trainable parameters. This establishes a new paradigm for integrating temporal data with natural language.

Abstract: Time-series data are critical in diverse applications, such as industrial
monitoring, medical diagnostics, and climate research. However, effectively
integrating these high-dimensional temporal signals with natural language for
dynamic, interactive tasks remains a significant challenge. To address this, we
introduce the Time-Series Question Answering (Time-Series QA) task and release
EngineMT-QA, the first large-scale, multi-task, temporal-textual QA dataset
designed to capture complex interactions between time-series signals and
natural language. Building on this resource, we propose the Instruct Time
Transformer (ITFormer), a novel framework that bridges time-series encoders
with frozen large language models (LLMs). ITFormer effectively extracts,
aligns, and fuses temporal and textual features, achieving a strong improvement
in QA accuracy over strong baselines with fewer than 1\% additional trainable
parameters. By combining computational efficiency with robust cross-modal
modeling, our work establishes a adaptable paradigm for integrating temporal
data with natural language, paving the way for new research and applications in
multi-modal AI. More details about the project, including datasets and code,
are available at: https://pandalin98.github.io/itformer_site/

</details>


### [10] [A Multi-Pass Large Language Model Framework for Precise and Efficient Radiology Report Error Detection](https://arxiv.org/abs/2506.20112)
*Songsoo Kim,Seungtae Lee,See Young Lee,Joonho Kim,Keechan Kan,Dukyong Yoon*

Main category: cs.CL

TL;DR: A three-pass LLM framework improves the accuracy and efficiency of AI-assisted radiology report quality assurance.


<details>
  <summary>Details</summary>
Motivation: The positive predictive value (PPV) of large language model (LLM)-based proofreading for radiology reports is limited due to the low error prevalence.

Method: A retrospective analysis was performed on 1,000 consecutive radiology reports (250 each: radiography, ultrasonography, CT, MRI) from the MIMIC-III database. Two external datasets (CheXpert and Open-i) were validation sets. Three LLM frameworks were tested: (1) single-prompt detector; (2) extractor plus detector; and (3) extractor, detector, and false-positive verifier.

Result: Framework PPV increased from 0.063 (95% CI, 0.036-0.101, Framework 1) to 0.079 (0.049-0.118, Framework 2), and significantly to 0.159 (0.090-0.252, Framework 3; P<.001 vs. baselines). aTPR remained stable (0.012-0.014; P>=.84). Operational costs per 1,000 reports dropped to USD 5.58 (Framework 3) from USD 9.72 (Framework 1) and USD 6.85 (Framework 2), reflecting reductions of 42.6% and 18.5%, respectively. Human-reviewed reports decreased from 192 to 88. External validation supported Framework 3's superior PPV (CheXpert 0.133, Open-i 0.105) and stable aTPR (0.007).

Conclusion: A three-pass LLM framework significantly enhanced PPV and reduced operational costs, maintaining detection performance, providing an effective strategy for AI-assisted radiology report quality assurance.

Abstract: Background: The positive predictive value (PPV) of large language model
(LLM)-based proofreading for radiology reports is limited due to the low error
prevalence. Purpose: To assess whether a three-pass LLM framework enhances PPV
and reduces operational costs compared with baseline approaches. Materials and
Methods: A retrospective analysis was performed on 1,000 consecutive radiology
reports (250 each: radiography, ultrasonography, CT, MRI) from the MIMIC-III
database. Two external datasets (CheXpert and Open-i) were validation sets.
Three LLM frameworks were tested: (1) single-prompt detector; (2) extractor
plus detector; and (3) extractor, detector, and false-positive verifier.
Precision was measured by PPV and absolute true positive rate (aTPR).
Efficiency was calculated from model inference charges and reviewer
remuneration. Statistical significance was tested using cluster bootstrap,
exact McNemar tests, and Holm-Bonferroni correction. Results: Framework PPV
increased from 0.063 (95% CI, 0.036-0.101, Framework 1) to 0.079 (0.049-0.118,
Framework 2), and significantly to 0.159 (0.090-0.252, Framework 3; P<.001 vs.
baselines). aTPR remained stable (0.012-0.014; P>=.84). Operational costs per
1,000 reports dropped to USD 5.58 (Framework 3) from USD 9.72 (Framework 1) and
USD 6.85 (Framework 2), reflecting reductions of 42.6% and 18.5%, respectively.
Human-reviewed reports decreased from 192 to 88. External validation supported
Framework 3's superior PPV (CheXpert 0.133, Open-i 0.105) and stable aTPR
(0.007). Conclusion: A three-pass LLM framework significantly enhanced PPV and
reduced operational costs, maintaining detection performance, providing an
effective strategy for AI-assisted radiology report quality assurance.

</details>


### [11] [Leveraging AI Graders for Missing Score Imputation to Achieve Accurate Ability Estimation in Constructed-Response Tests](https://arxiv.org/abs/2506.20119)
*Masaki Uto,Yuma Ito*

Main category: cs.CL

TL;DR: 本研究提出了一种利用自动评分技术进行IRT能力估计的方法，以减少人工评分工作量并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 评估学习者的能力是教育领域的一项基本目标。特别是，越来越需要评估诸如表达能力和逻辑思维等高阶能力。虽然简答题和论文题等构建反应测试是有效的方法，但它们需要大量的人工评分，这使得它们既劳动密集又成本高昂。随着缺失分数比例的增加，能力估计的准确性会下降。为了克服这些挑战。

Method: 提出了一种利用自动评分技术来填补缺失分数的新方法。

Result: 该方法在能力估计中实现了高精度，同时显著减少了人工评分工作量。

Conclusion: 该研究提出了一种新方法，利用自动评分技术来填补缺失分数，以实现准确的基于IRT的能力估计。所提出的方法在显著减少人工评分工作量的同时，实现了高精度的能力估计。

Abstract: Evaluating the abilities of learners is a fundamental objective in the field
of education. In particular, there is an increasing need to assess higher-order
abilities such as expressive skills and logical thinking. Constructed-response
tests such as short-answer and essay-based questions have become widely used as
a method to meet this demand. Although these tests are effective, they require
substantial manual grading, making them both labor-intensive and costly. Item
response theory (IRT) provides a promising solution by enabling the estimation
of ability from incomplete score data, where human raters grade only a subset
of answers provided by learners across multiple test items. However, the
accuracy of ability estimation declines as the proportion of missing scores
increases. Although data augmentation techniques for imputing missing scores
have been explored in order to address this limitation, they often struggle
with inaccuracy for sparse or heterogeneous data. To overcome these challenges,
this study proposes a novel method for imputing missing scores by leveraging
automated scoring technologies for accurate IRT-based ability estimation. The
proposed method achieves high accuracy in ability estimation while markedly
reducing manual grading workload.

</details>


### [12] [CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG Evaluation](https://arxiv.org/abs/2506.20128)
*Aashiq Muhamed*

Main category: cs.CL

TL;DR: CCRS: A novel suite of five metrics that utilizes a single, powerful, pretrained LLM as a zero-shot, end-to-end judge, offering a practical, comprehensive, and efficient framework for evaluating and iteratively improving RAG systems.


<details>
  <summary>Details</summary>
Motivation: Evaluating the multifaceted quality of RAG outputs poses significant challenges. Existing evaluation methods often rely on simple lexical overlap metrics, which are inadequate for capturing these nuances, or involve complex multi-stage pipelines, hindering practical efficiency.

Method: We propose CCRS (Contextual Coherence and Relevance Score), a novel suite of five metrics that utilizes a single, powerful, pretrained LLM as a zero-shot, end-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance (QR), Information Density (ID), Answer Correctness (AC), and Information Recall (IR).

Result: CCRS effectively discriminates between system performances, confirming, for instance, that the Mistral-7B reader outperforms Llama variants. Compared to the complex RAGChecker framework, CCRS offers comparable or superior discriminative power for key aspects like recall and faithfulness, while being significantly more computationally efficient.

Conclusion: CCRS offers a practical, comprehensive, and efficient framework for evaluating and iteratively improving RAG systems.

Abstract: RAG systems enhance LLMs by incorporating external knowledge, which is
crucial for domains that demand factual accuracy and up-to-date information.
However, evaluating the multifaceted quality of RAG outputs, spanning aspects
such as contextual coherence, query relevance, factual correctness, and
informational completeness, poses significant challenges. Existing evaluation
methods often rely on simple lexical overlap metrics, which are inadequate for
capturing these nuances, or involve complex multi-stage pipelines with
intermediate steps like claim extraction or require finetuning specialized
judge models, hindering practical efficiency. To address these limitations, we
propose CCRS (Contextual Coherence and Relevance Score), a novel suite of five
metrics that utilizes a single, powerful, pretrained LLM as a zero-shot,
end-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance
(QR), Information Density (ID), Answer Correctness (AC), and Information Recall
(IR). We apply CCRS to evaluate six diverse RAG system configurations on the
challenging BioASQ dataset. Our analysis demonstrates that CCRS effectively
discriminates between system performances, confirming, for instance, that the
Mistral-7B reader outperforms Llama variants. We provide a detailed analysis of
CCRS metric properties, including score distributions, convergent/discriminant
validity, tie rates, population statistics, and discriminative power. Compared
to the complex RAGChecker framework, CCRS offers comparable or superior
discriminative power for key aspects like recall and faithfulness, while being
significantly more computationally efficient. CCRS thus provides a practical,
comprehensive, and efficient framework for evaluating and iteratively improving
RAG systems.

</details>


### [13] [AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control](https://arxiv.org/abs/2506.20160)
*Ruosen Li,Ziming Luo,Quan Zhang,Ruochen Li,Ben Zhou,Ali Payani,Xinya Du*

Main category: cs.CL

TL;DR: AALC 减少了 LRM 的响应长度，同时保持或提高了准确性，但降低了可解释性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型 (LRM) 通过生成冗长的思维链来实现令人印象深刻的推理能力，但是这种“过度思考”会导致高延迟和高成本，而没有获得相应的准确性。

Method: 引入 AALC，一种轻量级的、精度感知的长度奖励，它被集成到强化学习中，可以在训练期间动态地平衡正确性和简洁性。

Result: 我们的方法可以将响应长度减少 50% 以上，同时保持甚至提高原始精度。此外，定性分析表明，我们的方法可以抑制冗余的推理模式，例如过度的子目标设置和验证，从而产生结构上更精细的输出，而不是简单的截断。

Conclusion: reward-based strategies 可以引导 LRMs 找到更高效、更通用的推理路径。

Abstract: Large reasoning models (LRMs) achieve impressive reasoning capabilities by
generating lengthy chain-of-thoughts, but this "overthinking" incurs high
latency and cost without commensurate accuracy gains. In this work, we
introduce AALC, a lightweight, accuracy-aware length reward integrated into
reinforcement learning that dynamically balances correctness and brevity during
training. By incorporating validation accuracy into the reward and employing a
smooth, dynamically scheduled length penalty, AALC delays length penalty until
target performance is met. Through extensive experiments across standard and
out-of-distribution math benchmarks, we show that our approach reduces response
length by over 50% while maintaining or even improving the original accuracy.
Furthermore, qualitative analysis reveals that our method curbs redundant
reasoning patterns such as excessive subgoal setting and verification, leading
to structurally refined outputs rather than naive truncation. We also identify
that efficiency gains are accompanied by reduced interpretability: models
trained with AALC omit some narrative framing and explanatory context. These
findings highlight the potential of reward-based strategies to guide LRMs
toward more efficient, generalizable reasoning paths.

</details>


### [14] [SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction with LLMs](https://arxiv.org/abs/2506.20167)
*Fengze Li,Yue Wang,Yangle Liu,Ming Huang,Dou Hong,Jieming Ma*

Main category: cs.CL

TL;DR: SEED: a structural encoder for embedding-driven decoding, which integrates four stages: a token-aware encoder for patch extraction, a projection module that aligns patches with language model embeddings, a semantic reprogramming mechanism that maps patches to task-aware prototypes, and a frozen language model for prediction.


<details>
  <summary>Details</summary>
Motivation: Multivariate time series forecasting requires models to simultaneously capture variable-wise structural dependencies and generalize across diverse tasks. Structural encoders lack the capacity to support semantic-level reasoning or task adaptation, while large language models (LLMs) remain incompatible with raw time series inputs.

Method: a structural encoder for embedding-driven decoding, which integrates four stages: a token-aware encoder for patch extraction, a projection module that aligns patches with language model embeddings, a semantic reprogramming mechanism that maps patches to task-aware prototypes, and a frozen language model for prediction

Result: achieves consistent improvements over strong baselines

Conclusion: The proposed SEED method achieves consistent improvements over strong baselines and addresses the structural-semantic modeling gap.

Abstract: Multivariate time series forecasting requires models to simultaneously
capture variable-wise structural dependencies and generalize across diverse
tasks. While structural encoders are effective in modeling feature
interactions, they lack the capacity to support semantic-level reasoning or
task adaptation. Conversely, large language models (LLMs) possess strong
generalization capabilities but remain incompatible with raw time series
inputs. This gap limits the development of unified, transferable prediction
systems. Therefore, we introduce SEED, a structural encoder for
embedding-driven decoding, which integrates four stages: a token-aware encoder
for patch extraction, a projection module that aligns patches with language
model embeddings, a semantic reprogramming mechanism that maps patches to
task-aware prototypes, and a frozen language model for prediction. This modular
architecture decouples representation learning from inference, enabling
efficient alignment between numerical patterns and semantic reasoning.
Empirical results demonstrate that the proposed method achieves consistent
improvements over strong baselines, and comparative studies on various datasets
confirm SEED's role in addressing the structural-semantic modeling gap.

</details>


### [15] [COIN: Uncertainty-Guarding Selective Question Answering for Foundation Models with Provable Risk Guarantees](https://arxiv.org/abs/2506.20178)
*Zhiyuan Wang,Jinhao Duan,Qingni Wang,Xiaofeng Zhu,Tianlong Chen,Xiaoshuang Shi,Kaidi Xu*

Main category: cs.CL

TL;DR: COIN is proposed to control false discovery rate (FDR) in text generation by calibrating uncertainty thresholds. It improves sample retention and can be further boosted by alternative methods.


<details>
  <summary>Details</summary>
Motivation: Uncertainty quantification (UQ) for foundation models is essential to identify and mitigate potential hallucinations in automatically generated text. However, heuristic UQ approaches lack formal guarantees for key metrics such as the false discovery rate (FDR) in selective prediction. Previous work adopts the split conformal prediction (SCP) framework to ensure desired coverage of admissible answers by constructing prediction sets, but these sets often contain incorrect candidates, limiting their practical utility.

Method: We propose COIN, an uncertainty-guarding selection framework that calibrates statistically valid thresholds to filter a single generated answer per question under user-specified FDR constraints. COIN estimates the empirical error rate on a calibration set and applies confidence interval methods such as Clopper-Pearson to establish a high-probability upper bound on the true error rate.

Result: COIN ensures FDR control on test data while significantly increasing sample retention. COIN's power performance can be boosted by employing alternative upper bound constructions and UQ strategies.

Conclusion: COIN is robust in risk control, strong test-time power in retaining admissible answers, and predictive efficiency under limited calibration data. Employing alternative upper bound constructions and UQ strategies can further boost COIN's power performance.

Abstract: Uncertainty quantification (UQ) for foundation models is essential to
identify and mitigate potential hallucinations in automatically generated text.
However, heuristic UQ approaches lack formal guarantees for key metrics such as
the false discovery rate (FDR) in selective prediction. Previous work adopts
the split conformal prediction (SCP) framework to ensure desired coverage of
admissible answers by constructing prediction sets, but these sets often
contain incorrect candidates, limiting their practical utility. To address
this, we propose COIN, an uncertainty-guarding selection framework that
calibrates statistically valid thresholds to filter a single generated answer
per question under user-specified FDR constraints. COIN estimates the empirical
error rate on a calibration set and applies confidence interval methods such as
Clopper-Pearson to establish a high-probability upper bound on the true error
rate (i.e., FDR). This enables the selection of the largest uncertainty
threshold that ensures FDR control on test data while significantly increasing
sample retention. We demonstrate COIN's robustness in risk control, strong
test-time power in retaining admissible answers, and predictive efficiency
under limited calibration data across both general and multimodal text
generation tasks. Furthermore, we show that employing alternative upper bound
constructions and UQ strategies can further boost COIN's power performance,
which underscores its extensibility and adaptability to diverse application
scenarios.

</details>


### [16] [How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?](https://arxiv.org/abs/2506.20199)
*Mengqi Wang,Tiantian Feng,Shrikanth Narayanan*

Main category: cs.CL

TL;DR: This study explores how to retrieve high-quality examples in in-context learning (ICL) to enhance CER. Augmented example retrieval consistently outperforms other techniques.


<details>
  <summary>Details</summary>
Motivation: Creating a high-performing application with high accuracy remains challenging, particularly for subjective tasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this study investigates approaches to improving conversational emotion recognition (CER) by LLMs.

Method: We propose various strategies based on random and augmented example retrieval and also analyze the impact of conversational context on CER accuracy.

Result: Experiments were conducted on the three datasets including IEMOCAP, MELD and EmoryNLP.

Conclusion: Augmented example retrieval consistently outperforms other techniques under investigation across all datasets, highlighting the importance of retrieving coherent targeted examples and enhancing them through paraphrasing.

Abstract: Large language models (LLMs) have enabled a wide variety of real-world
applications in various domains. However, creating a high-performing
application with high accuracy remains challenging, particularly for subjective
tasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this
study investigates approaches to improving conversational emotion recognition
(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples
in in-context learning (ICL) to enhance CER. We propose various strategies
based on random and augmented example retrieval and also analyze the impact of
conversational context on CER accuracy. Experiments were conducted on the three
datasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented
example retrieval consistently outperforms other techniques under investigation
across all datasets, highlighting the importance of retrieving coherent
targeted examples and enhancing them through paraphrasing.

</details>


### [17] [Intrinsic vs. Extrinsic Evaluation of Czech Sentence Embeddings: Semantic Relevance Doesn't Help with MT Evaluation](https://arxiv.org/abs/2506.20203)
*Petra Barančíková,Ondřej Bojar*

Main category: cs.CL

TL;DR: compared Czech-specific and multilingual sentence embedding models through intrinsic and extrinsic evaluation paradigms, found that models that excel in intrinsic semantic similarity tests do not consistently yield superior performance on downstream translation evaluation tasks.


<details>
  <summary>Details</summary>
Motivation: comparing Czech-specific and multilingual sentence embedding models

Method: compare Czech-specific and multilingual sentence embedding models through intrinsic and extrinsic evaluation paradigms. intrinsic evaluation: Costra, a complex sentence transformation dataset, and several Semantic Textual Similarity (STS) benchmarks. extrinsic evaluation: fine-tune each embedding model using COMET-based metrics for machine translation evaluation.

Result: models that excel in intrinsic semantic similarity tests do not consistently yield superior performance on downstream translation evaluation tasks. models with seemingly over-smoothed embedding spaces can, through fine-tuning, achieve excellent results.

Conclusion: Models that excel in intrinsic semantic similarity tests do not consistently yield superior performance on downstream translation evaluation tasks. Models with seemingly over-smoothed embedding spaces can, through fine-tuning, achieve excellent results. The complex relationship between semantic property probes and downstream task, emphasizing the need for more research into 'operationalizable semantics' in sentence embeddings, or more in-depth downstream tasks datasets (here translation evaluation)

Abstract: In this paper, we compare Czech-specific and multilingual sentence embedding
models through intrinsic and extrinsic evaluation paradigms. For intrinsic
evaluation, we employ Costra, a complex sentence transformation dataset, and
several Semantic Textual Similarity (STS) benchmarks to assess the ability of
the embeddings to capture linguistic phenomena such as semantic similarity,
temporal aspects, and stylistic variations. In the extrinsic evaluation, we
fine-tune each embedding model using COMET-based metrics for machine
translation evaluation.
  Our experiments reveal an interesting disconnect: models that excel in
intrinsic semantic similarity tests do not consistently yield superior
performance on downstream translation evaluation tasks. Conversely, models with
seemingly over-smoothed embedding spaces can, through fine-tuning, achieve
excellent results. These findings highlight the complex relationship between
semantic property probes and downstream task, emphasizing the need for more
research into 'operationalizable semantics' in sentence embeddings, or more
in-depth downstream tasks datasets (here translation evaluation)

</details>


### [18] [Perspectives in Play: A Multi-Perspective Approach for More Inclusive NLP Systems](https://arxiv.org/abs/2506.20209)
*Benedetta Muscato,Lucia Passaro,Gizem Gezici,Fosca Giannotti*

Main category: cs.CL

TL;DR: This paper introduces a multi-perspective approach using soft labels to capture human disagreements in subjective NLP tasks. It outperforms traditional methods but shows lower confidence in highly subjective tasks, with XAI providing insights.


<details>
  <summary>Details</summary>
Motivation: Disregarding individual opinions in NLP can underrepresent minority perspectives, especially in subjective tasks.

Method: A new multi-perspective approach using soft labels.

Result: The multi-perspective approach better approximates human label distributions (JSD) and achieves superior classification performance (F1 scores), but shows lower confidence in irony and stance detection. XAI provides insights into model predictions.

Conclusion: The multi-perspective approach, while better approximating human label distributions and achieving superior classification performance, exhibits lower confidence in highly subjective tasks. XAI is used to explore model uncertainty and uncover insights.

Abstract: In the realm of Natural Language Processing (NLP), common approaches for
handling human disagreement consist of aggregating annotators' viewpoints to
establish a single ground truth. However, prior studies show that disregarding
individual opinions can lead can lead to the side effect of underrepresenting
minority perspectives, especially in subjective tasks, where annotators may
systematically disagree because of their preferences. Recognizing that labels
reflect the diverse backgrounds, life experiences, and values of individuals,
this study proposes a new multi-perspective approach using soft labels to
encourage the development of the next generation of perspective aware models,
more inclusive and pluralistic. We conduct an extensive analysis across diverse
subjective text classification tasks, including hate speech, irony, abusive
language, and stance detection, to highlight the importance of capturing human
disagreements, often overlooked by traditional aggregation methods. Results
show that the multi-perspective approach not only better approximates human
label distributions, as measured by Jensen-Shannon Divergence (JSD), but also
achieves superior classification performance (higher F1 scores), outperforming
traditional approaches. However, our approach exhibits lower confidence in
tasks like irony and stance detection, likely due to the inherent subjectivity
present in the texts. Lastly, leveraging Explainable AI (XAI), we explore model
uncertainty and uncover meaningful insights into model predictions.

</details>


### [19] [Enhancing Large Language Models through Structured Reasoning](https://arxiv.org/abs/2506.20241)
*Yubo Dong,Hehe Fan*

Main category: cs.CL

TL;DR: 通过显式结构化推理来增强LLM，从而提高复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 最近的大型语言模型（LLM）在自然语言处理和自动决策方面取得了显著进展。然而，这些模型在执行涉及逻辑推演和系统规划的复杂推理任务时仍然遇到困难，这主要是因为它们依赖于没有结构化知识表示的隐式统计关系。

Method: 通过显式注释推理步骤，将非结构化数据转换为结构化格式，使用结构化数据集通过监督微调（SFT）训练LLM。使用GRPO增强LLM的结构化推理能力，包含MAX-Flow和LCS算法。

Result: 对DeepSeek-R1-Distill-Qwen-1.5B模型进行微调的实验结果表明，其推理简洁，在各种场景中表现出强大的性能，并提高了与优化技术的兼容性。

Conclusion: 通过将结构化推理整合到LLM中，验证了其有效性。

Abstract: Recent Large Language Models (LLMs) have significantly advanced natural
language processing and automated decision-making. However, these models still
encounter difficulties when performing complex reasoning tasks involving
logical deduction and systematic planning, primarily due to their reliance on
implicit statistical relationships without structured knowledge
representation.Inspired by cognitive science and neurosymbolic AI, we introduce
a novel approach to enhance LLMs through explicit structured reasoning. First,
we convert unstructured data into structured formats by explicitly annotating
reasoning steps. We then employ this structured dataset to train LLMs through
Supervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning
capabilities of LLMs using Group Relative Policy Optimization (GRPO),
incorporating two innovative algorithms--MAX-Flow and Longest Common
Subsequence (LCS)--which notably improve reasoning effectiveness and reduce
computational complexity. Experimental results from fine-tuning a
DeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust
performance across various scenarios, and improved compatibility with
optimization techniques, validating the efficacy of structured reasoning
integration in LLMs.

</details>


### [20] [CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment](https://arxiv.org/abs/2506.20243)
*Papa Séga Wade,Mihai Andries,Ioannis Kanellos,Thierry Moudenc*

Main category: cs.CL

TL;DR: A chunk-based multi-SSL fusion method improves automatic fluency assessment (AFA) for non-native speakers.


<details>
  <summary>Details</summary>
Motivation: Automatic fluency assessment (AFA) remains challenging, particularly in capturing speech rhythm, pauses, and disfluencies in non-native speakers.

Method: Chunk-based approach integrating self-supervised learning (SSL) models (Wav2Vec2, HuBERT, and WavLM) with a hierarchical CNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero voice activity detection (Silero-VAD).

Result: Improves F1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines on Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on Avalinguo, surpassing Pyannote.audio-based segmentation baselines.

Conclusion: Chunk-based multi-SSL fusion improves fluency evaluation, but generalization to dialects with irregular prosody needs further exploration.

Abstract: Automatic fluency assessment (AFA) remains challenging, particularly in
capturing speech rhythm, pauses, and disfluencies in non-native speakers. We
introduce a chunk-based approach integrating self-supervised learning (SSL)
models (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths
in phonetic, prosodic, and noisy speech modeling, with a hierarchical
CNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero
voice activity detection (Silero-VAD), enabling fine-grained temporal analysis
while mitigating over-segmentation artifacts. SSL embeddings are fused via a
learnable weighted mechanism, balancing acoustic and linguistic features, and
enriched with chunk-level fluency markers (e.g., speech rate, pause durations,
n-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies
across chunks. Evaluated on Avalinguo and Speechocean762, our approach improves
F1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines
on Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on
Avalinguo, surpassing Pyannote.audio-based segmentation baselines. These
findings highlight chunk-based multi-SSL fusion for robust fluency evaluation,
though future work should explore generalization to dialects with irregular
prosody.

</details>


### [21] [Narrative Shift Detection: A Hybrid Approach of Dynamic Topic Models and Large Language Models](https://arxiv.org/abs/2506.20269)
*Kai-Robin Lange,Tobias Schmidt,Matthias Reccius,Henrik Müller,Michael Roos,Carsten Jentsch*

Main category: cs.CL

TL;DR: This paper proposes a combination of Large Language Models and topic models to dynamically model narrative shifts across time. The findings indicate that a Large Language Model can efficiently extract a narrative shift if one exists at a given point in time, but does not perform as well when having to decide whether a shift in content or a narrative shift took place.


<details>
  <summary>Details</summary>
Motivation: With rapidly evolving media narratives, it has become increasingly critical to not just extract narratives from a given corpus but rather investigate, how they develop over time. While popular narrative extraction methods such as Large Language Models do well in capturing typical narrative elements or even the complex structure of a narrative, applying them to an entire corpus comes with obstacles, such as a high financial or computational cost.

Method: combination of the language understanding capabilities of Large Language Models with the large scale applicability of topic models to dynamically model narrative shifts across time using the Narrative Policy Framework. We apply a topic model and a corresponding change point detection method to find changes that concern a specific topic of interest. Using this model, we filter our corpus for documents that are particularly representative of that change and feed them into a Large Language Model that interprets the change that happened in an automated fashion and distinguishes between content and narrative shifts.

Result: Our findings indicate that a Large Language Model can efficiently extract a narrative shift if one exists at a given point in time, but does not perform as well when having to decide whether a shift in content or a narrative shift took place.

Conclusion: A Large Language Model can efficiently extract a narrative shift if one exists at a given point in time, but does not perform as well when having to decide whether a shift in content or a narrative shift took place.

Abstract: With rapidly evolving media narratives, it has become increasingly critical
to not just extract narratives from a given corpus but rather investigate, how
they develop over time. While popular narrative extraction methods such as
Large Language Models do well in capturing typical narrative elements or even
the complex structure of a narrative, applying them to an entire corpus comes
with obstacles, such as a high financial or computational cost. We propose a
combination of the language understanding capabilities of Large Language Models
with the large scale applicability of topic models to dynamically model
narrative shifts across time using the Narrative Policy Framework. We apply a
topic model and a corresponding change point detection method to find changes
that concern a specific topic of interest. Using this model, we filter our
corpus for documents that are particularly representative of that change and
feed them into a Large Language Model that interprets the change that happened
in an automated fashion and distinguishes between content and narrative shifts.
We employ our pipeline on a corpus of The Wall Street Journal news paper
articles from 2009 to 2023. Our findings indicate that a Large Language Model
can efficiently extract a narrative shift if one exists at a given point in
time, but does not perform as well when having to decide whether a shift in
content or a narrative shift took place.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [22] [Computer Vision based Automated Quantification of Agricultural Sprayers Boom Displacement](https://arxiv.org/abs/2506.19939)
*Aryan Singh Dalal,Sidharth Rai,Rahul Singh,Treman Singh Kaloya,Rahul Harsha Cheppally,Ajay Sharda*

Main category: cs.CV

TL;DR: 本研究开发了一种自动计算机视觉系统来量化各种农业喷雾器的喷杆运动。


<details>
  <summary>Details</summary>
Motivation: 使用自走式农业喷雾器进行农业生产时的施用率误差仍然令人担忧。 在众多因素中，喷杆不稳定是造成施用误差的主要原因之一。 38 米的喷杆宽度，加上 30 公里/小时的行驶速度、变化的地形以及在操纵复杂的田地边界时的机器动力，使得这些喷杆的控制非常复杂。 然而，没有关于喷杆运动程度的定量知识来系统地开发一种解决方案，该解决方案可能包括喷杆设计和响应式喷杆控制系统。

Method: 开发了一种计算机视觉系统来实时跟踪喷杆边缘的目标。 训练了 YOLO V7、V8 和 V11 神经网络模型，以跟踪田间作业中喷杆的运动，从而量化垂直和横向的有效位移。 在喷杆上安装了一个倾角计传感器，以捕获喷杆角度并验证神经网络模型的输出。

Result: 结果表明，该模型可以检测到目标，准确率超过 90%，并且喷杆上目标的距离估计值在倾角计传感器数据的 0.026 米以内。

Conclusion: 该系统可以量化当前喷雾器以及可能经过少量修改的任何其他喷雾器上的喷杆运动。 这些数据可用于进行设计改进，以使喷杆更加稳定并实现更高的施用精度。

Abstract: Application rate errors when using self-propelled agricultural sprayers for
agricultural production remain a concern. Among other factors, spray boom
instability is one of the major contributors to application errors. Spray
booms' width of 38m, combined with 30 kph driving speeds, varying terrain, and
machine dynamics when maneuvering complex field boundaries, make controls of
these booms very complex. However, there is no quantitative knowledge on the
extent of boom movement to systematically develop a solution that might include
boom designs and responsive boom control systems. Therefore, this study was
conducted to develop an automated computer vision system to quantify the boom
movement of various agricultural sprayers. A computer vision system was
developed to track a target on the edge of the sprayer boom in real time. YOLO
V7, V8, and V11 neural network models were trained to track the boom's
movements in field operations to quantify effective displacement in the
vertical and transverse directions. An inclinometer sensor was mounted on the
boom to capture boom angles and validate the neural network model output. The
results showed that the model could detect the target with more than 90 percent
accuracy, and distance estimates of the target on the boom were within 0.026 m
of the inclinometer sensor data. This system can quantify the boom movement on
the current sprayer and potentially on any other sprayer with minor
modifications. The data can be used to make design improvements to make sprayer
booms more stable and achieve greater application accuracy.

</details>


### [23] [EBC-ZIP: Improving Blockwise Crowd Counting with Zero-Inflated Poisson Regression](https://arxiv.org/abs/2506.19955)
*Yiming Ma,Victor Sanchez,Tanaya Guha*

Main category: cs.CV

TL;DR: EBC-ZIP 是一个人群计数框架，它使用零膨胀泊松 (ZIP) 回归来更好地处理零权重分布，同时保持计数准确性，优于 EBC 并达到最先进的水平。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了ground-truth密度图的极端稀疏性，并且大多数密度估计中使用的损失函数主要基于 MSE 并且隐含地假设高斯分布，这不适合建模离散的非负计数数据。

Method: 该方法使用零膨胀泊松 (ZIP) 回归公式对计数空间分布进行建模，并用 ZIP 分布的负对数似然代替传统回归损失。

Result: EBC-ZIP 始终优于 EBC，并在四个人群计数基准测试中实现了最先进的结果。

Conclusion: EBC-ZIP在四个人群计数基准测试中始终优于EBC，并实现了最先进的结果。

Abstract: Density map estimation has become the mainstream paradigm in crowd counting.
However, most existing methods overlook the extreme sparsity of ground-truth
density maps. In real-world crowd scenes, the vast majority of spatial regions
(often over 95%) contain no people, leading to heavily imbalanced count
distributions. Ignoring this imbalance can bias models toward overestimating
dense regions and underperforming in sparse areas. Furthermore, most loss
functions used in density estimation are majorly based on MSE and implicitly
assume Gaussian distributions, which are ill-suited for modeling discrete,
non-negative count data. In this paper, we propose EBC-ZIP, a crowd counting
framework that models the spatial distribution of counts using a Zero-Inflated
Poisson (ZIP) regression formulation. Our approach replaces the traditional
regression loss with the negative log-likelihood of the ZIP distribution,
enabling better handling of zero-heavy distributions while preserving count
accuracy. Built upon the recently proposed Enhanced Block Classification (EBC)
framework, EBC-ZIP inherits EBC's advantages in preserving the discreteness of
targets and ensuring training stability, while further improving performance
through a more principled probabilistic loss. We also evaluate EBC-ZIP with
backbones of varying computational complexity to assess its scalability.
Extensive experiments on four crowd counting benchmarks demonstrate that
EBC-ZIP consistently outperforms EBC and achieves state-of-the-art results.

</details>


### [24] [ToSA: Token Merging with Spatial Awareness](https://arxiv.org/abs/2506.20066)
*Hsiang-Wei Huang,Wenhao Chai,Kuang-Ming Chen,Cheng-Yen Yang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: ToSA: a novel token merging method that combines both semantic and spatial awareness to guide the token merging process.


<details>
  <summary>Details</summary>
Motivation: Existing methods primarily rely on the visual token's feature similarity for token merging, overlooking the potential of integrating spatial information, which can serve as a reliable criterion for token merging in the early layers of ViT, where the visual tokens only possess weak visual information.

Method: ToSA leverages the depth image as input to generate pseudo spatial tokens, which serve as auxiliary spatial information for the visual token merging process. With the introduced spatial awareness, ToSA achieves a more informed merging strategy that better preserves critical scene structure.

Result: ToSA achieves a more informed merging strategy that better preserves critical scene structure.

Conclusion: ToSA outperforms previous token merging methods across multiple benchmarks on visual and embodied question answering while largely reducing the runtime of the ViT, making it an efficient solution for ViT acceleration.

Abstract: Token merging has emerged as an effective strategy to accelerate Vision
Transformers (ViT) by reducing computational costs. However, existing methods
primarily rely on the visual token's feature similarity for token merging,
overlooking the potential of integrating spatial information, which can serve
as a reliable criterion for token merging in the early layers of ViT, where the
visual tokens only possess weak visual information. In this paper, we propose
ToSA, a novel token merging method that combines both semantic and spatial
awareness to guide the token merging process. ToSA leverages the depth image as
input to generate pseudo spatial tokens, which serve as auxiliary spatial
information for the visual token merging process. With the introduced spatial
awareness, ToSA achieves a more informed merging strategy that better preserves
critical scene structure. Experimental results demonstrate that ToSA
outperforms previous token merging methods across multiple benchmarks on visual
and embodied question answering while largely reducing the runtime of the ViT,
making it an efficient solution for ViT acceleration. The code will be
available at: https://github.com/hsiangwei0903/ToSA

</details>


### [25] [From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents](https://arxiv.org/abs/2506.20326)
*Sergio Torres Aguilar*

Main category: cs.CV

TL;DR: Compares object detection architectures for document layout analysis on historical documents, finding that CNN-OBB models outperform Transformers on complex layouts.


<details>
  <summary>Details</summary>
Motivation: Robust Document Layout Analysis (DLA) is critical for the automated processing and understanding of historical documents with complex page organizations.

Method: Benchmarks five state-of-the-art object detection architectures on three annotated datasets: e-NDP, CATMuS, and HORAE. Evaluates two Transformer-based models (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and YOLO-World).

Result: Significant performance variations dependent on model architecture, data set characteristics, and bounding box representation. On the e-NDP dataset, Co-DETR achieves state-of-the-art results, while on the more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB significantly outperforms all other models. Using Oriented Bounding Boxes (OBB) is a fundamental requirement for accurately modeling the non-Cartesian nature of historical manuscripts.

Conclusion: A key trade-off exists between the global context awareness of Transformers, ideal for structured layouts, and the superior generalization of CNN-OBB models for visually diverse and complex documents.

Abstract: Robust Document Layout Analysis (DLA) is critical for the automated
processing and understanding of historical documents with complex page
organizations. This paper benchmarks five state-of-the-art object detection
architectures on three annotated datasets representing a spectrum of
codicological complexity: The e-NDP, a corpus of Parisian medieval registers
(1326-1504); CATMuS, a diverse multiclass dataset derived from various medieval
and modern sources (ca.12th-17th centuries) and HORAE, a corpus of decorated
books of hours (ca.13th-16th centuries). We evaluate two Transformer-based
models (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and
YOLO-World). Our findings reveal significant performance variations dependent
on model architecture, data set characteristics, and bounding box
representation. In the e-NDP dataset, Co-DETR achieves state-of-the-art results
(0.752 mAP@.50:.95), closely followed by YOLOv11X-OBB (0.721). Conversely, on
the more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB
significantly outperforms all other models (0.564 and 0.568, respectively).
This study unequivocally demonstrates that using Oriented Bounding Boxes (OBB)
is not a minor refinement but a fundamental requirement for accurately modeling
the non-Cartesian nature of historical manuscripts. We conclude that a key
trade-off exists between the global context awareness of Transformers, ideal
for structured layouts, and the superior generalization of CNN-OBB models for
visually diverse and complex documents.

</details>


### [26] [BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos](https://arxiv.org/abs/2506.20103)
*Jiahao Lin,Weixuan Peng,Bojia Zi,Yifeng Gao,Xianbiao Qi,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: Introduce BrokenVideos, a benchmark dataset for artifact localization in AI-generated videos, to address the lack of comprehensive benchmark.


<details>
  <summary>Details</summary>
Motivation: Synthesized content often exhibits visual artifacts such as temporally inconsistent motion, physically implausible trajectories, unnatural object deformations, and local blurring that undermine realism and user trust. Accurate detection and spatial localization of these artifacts are crucial for both automated quality control and for guiding the development of improved generative models. However, the research community currently lacks a comprehensive benchmark specifically designed for artifact localization in AI generated videos.

Method: introduce BrokenVideos, a benchmark dataset of 3,254 AI-generated videos with meticulously annotated, pixel-level masks highlighting regions of visual corruption. Each annotation is validated through detailed human inspection

Result: training state of the art artifact detection models and multi modal large language models (MLLMs) on BrokenVideos significantly improves their ability to localize corrupted regions

Conclusion: BrokenVideos establishes a critical foundation for benchmarking and advancing research on artifact localization in generative video models.

Abstract: Recent advances in deep generative models have led to significant progress in
video generation, yet the fidelity of AI-generated videos remains limited.
Synthesized content often exhibits visual artifacts such as temporally
inconsistent motion, physically implausible trajectories, unnatural object
deformations, and local blurring that undermine realism and user trust.
Accurate detection and spatial localization of these artifacts are crucial for
both automated quality control and for guiding the development of improved
generative models. However, the research community currently lacks a
comprehensive benchmark specifically designed for artifact localization in AI
generated videos. Existing datasets either restrict themselves to video or
frame level detection or lack the fine-grained spatial annotations necessary
for evaluating localization methods. To address this gap, we introduce
BrokenVideos, a benchmark dataset of 3,254 AI-generated videos with
meticulously annotated, pixel-level masks highlighting regions of visual
corruption. Each annotation is validated through detailed human inspection to
ensure high quality ground truth. Our experiments show that training state of
the art artifact detection models and multi modal large language models (MLLMs)
on BrokenVideos significantly improves their ability to localize corrupted
regions. Through extensive evaluation, we demonstrate that BrokenVideos
establishes a critical foundation for benchmarking and advancing research on
artifact localization in generative video models. The dataset is available at:
https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.

</details>


### [27] [From 2D to 3D Cognition: A Brief Survey of General World Models](https://arxiv.org/abs/2506.20134)
*Ningwei Xie,Zizi Tian,Lei Yang,Xiao-Ping Zhang,Meng Guo,Jie Li*

Main category: cs.CV

TL;DR: This survey reviews 3D world models, focusing on 3D representations, world knowledge, 3D physical scene generation, 3D spatial reasoning, and 3D spatial interaction. It also examines applications and future directions.


<details>
  <summary>Details</summary>
Motivation: The field lacks systematic analysis to categorize emerging techniques and clarify their roles in advancing 3D cognitive world models.

Method: introduce a conceptual framework, providing a structured and forward-looking review of world models transitioning from 2D perception to 3D cognition. Within this framework, we highlight two key technological drivers, particularly advances in 3D representations and the incorporation of world knowledge, as fundamental pillars. Building on these, we dissect three core cognitive capabilities that underpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning, and 3D spatial interaction.

Result: 3D-aware generative world models have demonstrated the ability to synthesize geometrically consistent, interactive 3D environments, marking a shift toward 3D spatial cognition.

Conclusion: This survey identifies challenges across data, modeling, and deployment, and outline future directions for advancing more robust and generalizable 3D world models.

Abstract: World models have garnered increasing attention in the development of
artificial general intelligence (AGI), serving as computational frameworks for
learning representations of the external world and forecasting future states.
While early efforts focused on 2D visual perception and simulation, recent
3D-aware generative world models have demonstrated the ability to synthesize
geometrically consistent, interactive 3D environments, marking a shift toward
3D spatial cognition. Despite rapid progress, the field lacks systematic
analysis to categorize emerging techniques and clarify their roles in advancing
3D cognitive world models. This survey addresses this need by introducing a
conceptual framework, providing a structured and forward-looking review of
world models transitioning from 2D perception to 3D cognition. Within this
framework, we highlight two key technological drivers, particularly advances in
3D representations and the incorporation of world knowledge, as fundamental
pillars. Building on these, we dissect three core cognitive capabilities that
underpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning,
and 3D spatial interaction. We further examine the deployment of these
capabilities in real-world applications, including embodied AI, autonomous
driving, digital twin, and gaming/VR. Finally, we identify challenges across
data, modeling, and deployment, and outline future directions for advancing
more robust and generalizable 3D world models.

</details>


### [28] [EAR: Erasing Concepts from Unified Autoregressive Models](https://arxiv.org/abs/2506.20151)
*Haipeng Fan,Shiyuan Zhang,Baohunesitu,Zihang Guo,Huaiwen Zhang*

Main category: cs.CV

TL;DR: propose Erasure Autoregressive Model (EAR), a fine-tuning method for effective and utility-preserving concept erasure in AR models, and propose a novel benchmark, Erase Concept Generator and Visual Filter (ECGVF), aim at provide a more rigorous and comprehensive foundation for evaluating concept erasure in AR models


<details>
  <summary>Details</summary>
Motivation: removing undesired concepts from AR models while maintaining overall generation quality remains an open challenge

Method: introduce Windowed Gradient Accumulation (WGA) strategy to align patch-level decoding with erasure objectives, and Thresholded Loss Masking (TLM) strategy to protect content unrelated to the target concept during fine-tuning

Result: Extensive experimental results conducted on the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR achieves marked improvements in both erasure effectiveness and model utility preservation.

Conclusion: EAR achieves marked improvements in both erasure effectiveness and model utility preservation.

Abstract: Autoregressive (AR) models have achieved unified and strong performance
across both visual understanding and image generation tasks. However, removing
undesired concepts from AR models while maintaining overall generation quality
remains an open challenge. In this paper, we propose Erasure Autoregressive
Model (EAR), a fine-tuning method for effective and utility-preserving concept
erasure in AR models. Specifically, we introduce Windowed Gradient Accumulation
(WGA) strategy to align patch-level decoding with erasure objectives, and
Thresholded Loss Masking (TLM) strategy to protect content unrelated to the
target concept during fine-tuning. Furthermore, we propose a novel benchmark,
Erase Concept Generator and Visual Filter (ECGVF), aim at provide a more
rigorous and comprehensive foundation for evaluating concept erasure in AR
models. Specifically, we first employ structured templates across diverse large
language models (LLMs) to pre-generate a large-scale corpus of
target-replacement concept prompt pairs. Subsequently, we generate images from
these prompts and subject them to rigorous filtering via a visual classifier to
ensure concept fidelity and alignment. Extensive experimental results conducted
on the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR
achieves marked improvements in both erasure effectiveness and model utility
preservation. Code is available at: https://github.com/immc-lab/ear/

</details>


### [29] [Loss-Aware Automatic Selection of Structured Pruning Criteria for Deep Neural Network Acceleration](https://arxiv.org/abs/2506.20152)
*Deepak Ghimire,Kilho Lee,Seong-heum Kim*

Main category: cs.CV

TL;DR: This paper presents LAASP, a pruning-while-training method for compressing neural networks. It automatically selects pruning criteria based on loss and achieves good accuracy and FLOPs reduction on CIFAR-10 and ImageNet.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for efficient structured pruning techniques to compress neural networks for deployment on resource-limited edge devices.

Method: The paper introduces Loss-Aware Automatic Selection of Structured Pruning Criteria (LAASP), a pruning-while-training approach that integrates pruning and fine-tuning into a single cycle. It uses the network's loss on a small training subset to automatically select pruning criteria and layers, and retrains the network after reducing a predefined number of FLOPs.

Result: Experiments on VGGNet and ResNet models on CIFAR-10 and ImageNet show that LAASP improves accuracy and reduces FLOPs. Specifically, ResNet56 and ResNet110 on CIFAR-10 improve top-1 accuracy, while ResNet50 on ImageNet reduces FLOPs by over 42% with a minimal accuracy drop.

Conclusion: The proposed LAASP method demonstrates significant improvements in top-1 accuracy on CIFAR-10 with ResNet56 and ResNet110, and reduces FLOPs by over 42% on ImageNet with ResNet50 with a negligible accuracy drop.

Abstract: Structured pruning is a well-established technique for compressing neural
networks, making it suitable for deployment in resource-limited edge devices.
This paper presents an efficient Loss-Aware Automatic Selection of Structured
Pruning Criteria (LAASP) for slimming and accelerating deep neural networks.
The majority of pruning methodologies employ a sequential process consisting of
three stages: 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed
pruning technique adopts a pruning-while-training approach that eliminates the
first stage and integrates the second and third stages into a single cycle. The
automatic selection of magnitude or similarity-based filter pruning criteria
from a specified pool of criteria and the specific pruning layer at each
pruning iteration is guided by the network's overall loss on a small subset of
the training data. To mitigate the abrupt accuracy drop due to pruning, the
network is retrained briefly after each reduction of a predefined number of
floating-point operations (FLOPs). The optimal pruning rates for each layer in
the network are automatically determined, eliminating the need for manual
allocation of fixed or variable pruning rates for each layer. Experiments on
the VGGNet and ResNet models on the CIFAR-10 and ImageNet benchmark datasets
demonstrate the effectiveness of the proposed method. In particular, the
ResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the
top-1 accuracy compared to state-of-the-art methods while reducing the network
FLOPs by 52\%. Furthermore, the ResNet50 model on the ImageNet dataset reduces
FLOPs by more than 42\% with a negligible 0.33\% drop in top-5 accuracy. The
source code of this paper is publicly available online -
https://github.com/ghimiredhikura/laasp.

</details>


### [30] [Towards Efficient Exemplar Based Image Editing with Multimodal VLMs](https://arxiv.org/abs/2506.20155)
*Avadhoot Jadhav,Ashutosh Srivastava,Abhinav Java,Silky Singh,Tarun Ram Menta,Surgan Jandial,Balaji Krishnamurthy*

Main category: cs.CV

TL;DR: This paper tackles exemplar-based image editing by transferring an edit from an exemplar pair to a content image(s), by leveraging pretrained text-to-image diffusion models and multimodal VLMs.


<details>
  <summary>Details</summary>
Motivation: Capturing all types of edits through text alone can be challenging and cumbersome. The ambiguous nature of certain image edits is better expressed through an exemplar pair, i.e., a pair of images depicting an image before and after an edit respectively.

Method: leveraging pretrained text-to-image diffusion models and multimodal VLMs

Result: Experiments demonstrate that it still outperforms baselines on multiple types of edits while being ~4x faster.

Conclusion: The proposed optimization-free pipeline outperforms baselines on multiple types of edits while being ~4x faster.

Abstract: Text-to-Image Diffusion models have enabled a wide array of image editing
applications. However, capturing all types of edits through text alone can be
challenging and cumbersome. The ambiguous nature of certain image edits is
better expressed through an exemplar pair, i.e., a pair of images depicting an
image before and after an edit respectively. In this work, we tackle
exemplar-based image editing -- the task of transferring an edit from an
exemplar pair to a content image(s), by leveraging pretrained text-to-image
diffusion models and multimodal VLMs. Even though our end-to-end pipeline is
optimization-free, our experiments demonstrate that it still outperforms
baselines on multiple types of edits while being ~4x faster.

</details>


### [31] [Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2506.20168)
*Zhentao He,Can Zhang,Ziheng Wu,Zhenghao Chen,Yufei Zhan,Yifan Li,Zhao Zhang,Xian Wang,Minghui Qiu*

Main category: cs.CV

TL;DR: This paper addresses the problem of hallucination in multimodal models when processing degraded documents. It introduces a new benchmark dataset (KIE-HVQA) and a GRPO-based framework to improve accuracy and reduce hallucination, achieving significant improvements over GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal large language models exhibit incompleteness in real-world scenarios, particularly under visual degradation, leading to overreliance on linguistic priors or misaligned visual-textual reasoning and the generation of hallucinatory content.

Method: The paper proposes KIE-HVQA, a benchmark dataset for evaluating OCR hallucination in degraded document understanding. It also introduces a GRPO-based framework featuring a novel reward mechanism with self-awareness of visual uncertainty and a refusal-to-answer strategy.

Result: Experiments on Qwen2.5-VL demonstrate that the proposed 7B-parameter model achieves a 22% absolute improvement in hallucination-free accuracy over GPT-4o on KIE-HVQA, without significant performance drop in standard tasks.

Conclusion: The paper introduces a GRPO-based framework with a novel reward mechanism to mitigate hallucinations in ambiguous regions, achieving a 22% absolute improvement in hallucination-free accuracy over GPT-4o on KIE-HVQA with a 7B-parameter model, while maintaining performance on standard tasks.

Abstract: Recent advancements in multimodal large language models have enhanced
document understanding by integrating textual and visual information. However,
existing models exhibit incompleteness within their paradigm in real-world
scenarios, particularly under visual degradation. In such conditions, the
current response paradigm often fails to adequately perceive visual degradation
and ambiguity, leading to overreliance on linguistic priors or misaligned
visual-textual reasoning. This difficulty in recognizing uncertainty frequently
results in the generation of hallucinatory content, especially when a precise
answer is not feasible. To better demonstrate and analyze this phenomenon and
problem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR
hallucination in degraded document understanding. This dataset includes test
samples spanning identity cards and invoices, with simulated real-world
degradations for OCR reliability. This setup allows for evaluating models'
capacity, under degraded input, to distinguish reliable visual information and
answer accordingly, thereby highlighting the challenge of avoiding
hallucination on uncertain data. To achieve vision-faithful reasoning and
thereby avoid the aforementioned issues, we further introduce a GRPO-based
framework featuring a novel reward mechanism. By incorporating a self-awareness
of visual uncertainty and an analysis method that initiates refusal to answer
to increase task difficulty within our supervised fine-tuning and reinforcement
learning framework, we successfully mitigated hallucinations in ambiguous
regions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model
achieves a 22\% absolute improvement in hallucination-free accuracy over GPT-4o
on KIE-HVQA and there is no significant performance drop in standard tasks,
highlighting both effectiveness and robustness.

</details>


### [32] [Towards Scalable and Generalizable Earth Observation Data Mining via Foundation Model Composition](https://arxiv.org/abs/2506.20174)
*Man Duc Chuc*

Main category: cs.CV

TL;DR: feature-level ensembling of smaller pretrained models can match or exceed the performance of much larger models, while requiring less training time and computational resources


<details>
  <summary>Details</summary>
Motivation: investigate whether foundation models pretrained on remote sensing and general vision datasets can be effectively combined to improve performance across a diverse set of key Earth Observation tasks

Method: evaluate several prominent models, including Prithvi, Hiera, and DOFA, on eleven datasets covering a range of spatial resolutions, sensor modalities, and task types

Result: feature-level ensembling of smaller pretrained models can match or exceed the performance of much larger models

Conclusion: feature-level ensembling of smaller pretrained models can match or exceed the performance of much larger models, while requiring less training time and computational resources. Moreover, the study highlights the potential of applying knowledge distillation to transfer the strengths of ensembles into more compact models, offering a practical path for deploying foundation models in real-world Earth Observation applications.

Abstract: Foundation models are rapidly transforming Earth Observation data mining by
enabling generalizable and scalable solutions for key tasks such as scene
classification and semantic segmentation. While most efforts in the geospatial
domain have focused on developing large models trained from scratch using
massive Earth Observation datasets, an alternative strategy that remains
underexplored is the reuse and combination of existing pretrained models. In
this study, we investigate whether foundation models pretrained on remote
sensing and general vision datasets can be effectively combined to improve
performance across a diverse set of key Earth Observation tasks. Using the
GEO-Bench benchmark, we evaluate several prominent models, including Prithvi,
Hiera, and DOFA, on eleven datasets covering a range of spatial resolutions,
sensor modalities, and task types. The results show that feature-level
ensembling of smaller pretrained models can match or exceed the performance of
much larger models, while requiring less training time and computational
resources. Moreover, the study highlights the potential of applying knowledge
distillation to transfer the strengths of ensembles into more compact models,
offering a practical path for deploying foundation models in real-world Earth
Observation applications.

</details>


### [33] [Progressive Alignment Degradation Learning for Pansharpening](https://arxiv.org/abs/2506.20179)
*Enzhe Zhao,Zhichang Guo,Yao Li,Fanghui Song,Boying Wu*

Main category: cs.CV

TL;DR: 该论文提出了一种新的深度学习 pansharpening 方法，通过学习更真实的降级过程和嵌入高频细节来提高图像质量，实验结果表明该方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的 pansharpening 通常使用 Wald 协议生成的合成数据进行训练，但 Wald 协议对真实世界退化模式的近似不准确，限制了深度 pansharpening 模型的泛化能力。

Method: 该论文提出了渐进对齐退化模块 (PADM) 和 HFreqdiff 框架，PADM 使用两个子网络相互迭代学习精确的退化过程，HFreqdiff 将高频细节嵌入到扩散框架中，并结合 CFB 和 BACM 模块。

Result: 实验和消融研究表明，该方法与最先进的技术相比，具有优越的性能。

Conclusion: 该论文提出了一种新的深度学习 pansharpening 方法，通过 PADM 学习精确的降级过程，并引入 HFreqdiff 嵌入高频细节，从而显著提高空间清晰度和质量。

Abstract: Deep learning-based pansharpening has been shown to effectively generate
high-resolution multispectral (HRMS) images. To create supervised ground-truth
HRMS images, synthetic data generated using the Wald protocol is commonly
employed. This protocol assumes that networks trained on artificial
low-resolution data will perform equally well on high-resolution data. However,
well-trained models typically exhibit a trade-off in performance between
reduced-resolution and full-resolution datasets. In this paper, we delve into
the Wald protocol and find that its inaccurate approximation of real-world
degradation patterns limits the generalization of deep pansharpening models. To
address this issue, we propose the Progressive Alignment Degradation Module
(PADM), which uses mutual iteration between two sub-networks, PAlignNet and
PDegradeNet, to adaptively learn accurate degradation processes without relying
on predefined operators. Building on this, we introduce HFreqdiff, which embeds
high-frequency details into a diffusion framework and incorporates CFB and BACM
modules for frequency-selective detail extraction and precise reverse process
learning. These innovations enable effective integration of high-resolution
panchromatic and multispectral images, significantly enhancing spatial
sharpness and quality. Experiments and ablation studies demonstrate the
proposed method's superior performance compared to state-of-the-art techniques.

</details>


### [34] [UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2506.20214)
*Yanzhe Chen,Huasong Zhong,Yan Li,Zhenheng Yang*

Main category: cs.CV

TL;DR: UniCode^2 是一种级联代码本框架，支持大规模、语义对齐和稳定的视觉标记化。


<details>
  <summary>Details</summary>
Motivation: 现有的基于代码本的方法要么依赖于缺乏细粒度语义的小词汇表（约 16K 条目），要么天真地扩大规模，导致token利用率低和训练不稳定。

Method: 通过对数百万 SigLIP 序列嵌入进行聚类，构建一个 50 万条目的代码本，该代码本保留了视觉-语言对齐，同时扩展了容量。通过级联设计确保稳定性：冻结的代码本锚定嵌入空间，可训练的代码本细化特定于任务的语义。这种解耦提高了利用率和稳健的学习能力。

Result: UniCode^2 提供了强大的性能，支持高质量的视觉合成，并且与预训练的扩散解码器无缝集成。

Conclusion: UniCode^2在各种基准测试中表现出色，证明了在不牺牲稳定性、语义或模块化的情况下扩展视觉标记空间的可行性。

Abstract: Unified multimodal large language models (MLLMs) have shown promise in
jointly advancing multimodal understanding and generation, with visual
codebooks discretizing images into tokens for autoregressive modeling. Existing
codebook-based methods either rely on small vocabularies (~16K entries) that
lack fine-grained semantics or naively scale up, resulting in low token
utilization and unstable training. We propose UniCode$^2$, a cascaded codebook
framework enabling large-scale, semantically aligned, and stable visual
tokenization. By clustering millions of SigLIP sequence embeddings, we build a
500K-entry codebook that preserves vision-language alignment while expanding
capacity. Stability is ensured via a cascaded design: a frozen codebook anchors
the embedding space, and a trainable codebook refines task-specific semantics.
This decoupling promotes high utilization and robust learning. Moreover, the
alignment of our visual tokens with textual semantics enables seamless
integration with pretrained diffusion decoders, supporting high-quality visual
synthesis with minimal adaptation. UniCode^2 delivers strong performance across
diverse benchmarks, demonstrating the viability of scaling visual token spaces
without sacrificing stability, semantics, or modularity.

</details>


### [35] [Dynamic Bandwidth Allocation for Hybrid Event-RGB Transmission](https://arxiv.org/abs/2506.20222)
*Pujing Yang,Guangyi Zhang,Yunlong Cai,Lei Yu,Guanding Yu*

Main category: cs.CV

TL;DR: A joint event and image transmission framework is proposed to eliminate redundancy and optimize channel bandwidth utilization. It adaptively allocates transmission bandwidth based on scene dynamics and achieves superior reconstruction quality and enhanced deblurring performance.


<details>
  <summary>Details</summary>
Motivation: A major challenge in hybrid systems with event cameras and RGB cameras lies in the transmission of the large volume of triggered events and RGB images. Conventional RGB cameras and event cameras typically capture the same scene in different ways, often resulting in significant redundant information across their outputs.

Method: A joint event and image (E-I) transmission framework is developed to eliminate redundancy and thereby optimize channel bandwidth utilization. Bayesian modeling and the information bottleneck method are employed to disentangle the shared and domain-specific information within the E-I inputs. This disentangled information bottleneck framework ensures both the compactness and informativeness of extracted shared and domain-specific information. It adaptively allocates transmission bandwidth based on scene dynamics.

Result: The proposed scheme achieves superior reconstruction quality compared to conventional systems and delivers enhanced deblurring performance.

Conclusion: The proposed scheme achieves superior reconstruction quality compared to conventional systems and delivers enhanced deblurring performance.

Abstract: Event cameras asynchronously capture pixel-level intensity changes with
extremely low latency. They are increasingly used in conjunction with RGB
cameras for a wide range of vision-related applications. However, a major
challenge in these hybrid systems lies in the transmission of the large volume
of triggered events and RGB images. To address this, we propose a transmission
scheme that retains efficient reconstruction performance of both sources while
accomplishing real-time deblurring in parallel. Conventional RGB cameras and
event cameras typically capture the same scene in different ways, often
resulting in significant redundant information across their outputs. To address
this, we develop a joint event and image (E-I) transmission framework to
eliminate redundancy and thereby optimize channel bandwidth utilization. Our
approach employs Bayesian modeling and the information bottleneck method to
disentangle the shared and domain-specific information within the E-I inputs.
This disentangled information bottleneck framework ensures both the compactness
and informativeness of extracted shared and domain-specific information.
Moreover, it adaptively allocates transmission bandwidth based on scene
dynamics, i.e., more symbols are allocated to events for dynamic details or to
images for static information. Simulation results demonstrate that the proposed
scheme not only achieves superior reconstruction quality compared to
conventional systems but also delivers enhanced deblurring performance.

</details>


### [36] [Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and Task-graph Guided Refinement](https://arxiv.org/abs/2506.20254)
*Kun Yuan,Tingxuan Chen,Shi Li,Joel L. Lavanchy,Christian Heiliger,Ege Özsoy,Yiming Huang,Long Bai,Nassir Navab,Vinkle Srivastav,Hongliang Ren,Nicolas Padoy*

Main category: cs.CV

TL;DR: SPA是一个轻量级框架，用于通用手术工作流程理解，通过最少的注释将基础模型调整到机构设置。


<details>
  <summary>Details</summary>
Motivation: 在开发用于跨机构和跨程序手术理解的通用模型时，手术工作流程的复杂性和多样性提出了巨大的挑战。最近在大型视觉语言数据上预训练的手术基础模型提供了有希望的可迁移性，但它们的零样本性能仍然受到域转移的限制，从而限制了它们在看不见的手术环境中的效用。

Method: SPA利用少样本空间自适应来对齐多模态嵌入与特定机构的手术场景和阶段。它还通过扩散建模确保时间一致性，扩散建模编码了来自机构程序协议的任务图先验。最后，SPA采用动态测试时自适应，利用多模态阶段预测流之间的相互协议，以自监督的方式使模型适应给定的测试视频，从而提高测试时分布偏移下的可靠性。

Result: SPA框架在跨机构和程序的少样本手术阶段识别方面实现了最先进的性能，甚至优于具有32个样本数据的全样本模型。

Conclusion: SPA框架在跨机构和程序的少样本手术阶段识别方面实现了最先进的性能，甚至优于具有32个样本数据的全样本模型。

Abstract: The complexity and diversity of surgical workflows, driven by heterogeneous
operating room settings, institutional protocols, and anatomical variability,
present a significant challenge in developing generalizable models for
cross-institutional and cross-procedural surgical understanding. While recent
surgical foundation models pretrained on large-scale vision-language data offer
promising transferability, their zero-shot performance remains constrained by
domain shifts, limiting their utility in unseen surgical environments. To
address this, we introduce Surgical Phase Anywhere (SPA), a lightweight
framework for versatile surgical workflow understanding that adapts foundation
models to institutional settings with minimal annotation. SPA leverages
few-shot spatial adaptation to align multi-modal embeddings with
institution-specific surgical scenes and phases. It also ensures temporal
consistency through diffusion modeling, which encodes task-graph priors derived
from institutional procedure protocols. Finally, SPA employs dynamic test-time
adaptation, exploiting the mutual agreement between multi-modal phase
prediction streams to adapt the model to a given test video in a
self-supervised manner, enhancing the reliability under test-time distribution
shifts. SPA is a lightweight adaptation framework, allowing hospitals to
rapidly customize phase recognition models by defining phases in natural
language text, annotating a few images with the phase labels, and providing a
task graph defining phase transitions. The experimental results show that the
SPA framework achieves state-of-the-art performance in few-shot surgical phase
recognition across multiple institutions and procedures, even outperforming
full-shot models with 32-shot labeled data. Code is available at
https://github.com/CAMMA-public/SPA

</details>


### [37] [A Transformer Based Handwriting Recognition System Jointly Using Online and Offline Features](https://arxiv.org/abs/2506.20255)
*Ayush Lodh,Ritabrata Chakraborty,Shivakumara Palaiahnakote,Umapada Pal*

Main category: cs.CV

TL;DR: early fusion of offline images and online stroke data within a shared latent space to achieve state-of-the-art accuracy in handwriting recognition.


<details>
  <summary>Details</summary>
Motivation: handwriting recognition benefits from complementary cues carried by the rasterized complex glyph and the pen's trajectory, yet most systems exploit only one modality.

Method: an end-to-end network that performs early fusion of offline images and online stroke data within a shared latent space. A patch encoder converts the grayscale crop into fixed-length visual tokens, while a lightweight transformer embeds the $(x, y, 	 ext{pen})$ sequence. Learnable latent queries attend jointly to both token streams, yielding context-enhanced stroke embeddings that are pooled and decoded under a cross-entropy loss objective.

Result: achieves state-of-the-art accuracy, exceeding previous bests by up to 1%. adaptation of this pipeline with gesturification on the ISI-Air dataset.

Conclusion: The approach achieves state-of-the-art accuracy, exceeding previous bests by up to 1%.

Abstract: We posit that handwriting recognition benefits from complementary cues
carried by the rasterized complex glyph and the pen's trajectory, yet most
systems exploit only one modality. We introduce an end-to-end network that
performs early fusion of offline images and online stroke data within a shared
latent space. A patch encoder converts the grayscale crop into fixed-length
visual tokens, while a lightweight transformer embeds the $(x, y, \text{pen})$
sequence. Learnable latent queries attend jointly to both token streams,
yielding context-enhanced stroke embeddings that are pooled and decoded under a
cross-entropy loss objective. Because integration occurs before any high-level
classification, temporal cues reinforce each other during representation
learning, producing stronger writer independence. Comprehensive experiments on
IAMOn-DB and VNOn-DB demonstrate that our approach achieves state-of-the-art
accuracy, exceeding previous bests by up to 1\%. Our study also shows
adaptation of this pipeline with gesturification on the ISI-Air dataset. Our
code can be found here.

</details>


### [38] [Hierarchical Mask-Enhanced Dual Reconstruction Network for Few-Shot Fine-Grained Image Classification](https://arxiv.org/abs/2506.20263)
*Ning Luo,Meiyin Hu,Huan Wan,Yanyan Yang,Zhuohang Jiang,Xin Wei*

Main category: cs.CV

TL;DR: HMDRN integrates dual-layer feature reconstruction with mask-enhanced feature processing to improve fine-grained classification.


<details>
  <summary>Details</summary>
Motivation: Existing methods have critical limitations: metric-based methods lose spatial information and misalign local features, while reconstruction-based methods fail to utilize hierarchical feature information and lack mechanisms to focus on discriminative regions.

Method: Hierarchical Mask-enhanced Dual Reconstruction Network (HMDRN), which integrates dual-layer feature reconstruction with mask-enhanced feature processing

Result: dual-layer reconstruction enhances inter-class discrimination while mask-enhanced transformation reduces intra-class variations.

Conclusion: HMDRN consistently outperforms state-of-the-art methods across Conv-4 and ResNet-12 backbone architectures.

Abstract: Few-shot fine-grained image classification (FS-FGIC) presents a significant
challenge, requiring models to distinguish visually similar subclasses with
limited labeled examples. Existing methods have critical limitations:
metric-based methods lose spatial information and misalign local features,
while reconstruction-based methods fail to utilize hierarchical feature
information and lack mechanisms to focus on discriminative regions. We propose
the Hierarchical Mask-enhanced Dual Reconstruction Network (HMDRN), which
integrates dual-layer feature reconstruction with mask-enhanced feature
processing to improve fine-grained classification. HMDRN incorporates a
dual-layer feature reconstruction and fusion module that leverages
complementary visual information from different network hierarchies. Through
learnable fusion weights, the model balances high-level semantic
representations from the last layer with mid-level structural details from the
penultimate layer. Additionally, we design a spatial binary mask-enhanced
transformer self-reconstruction module that processes query features through
adaptive thresholding while maintaining complete support features, enhancing
focus on discriminative regions while filtering background noise. Extensive
experiments on three challenging fine-grained datasets demonstrate that HMDRN
consistently outperforms state-of-the-art methods across Conv-4 and ResNet-12
backbone architectures. Comprehensive ablation studies validate the
effectiveness of each proposed component, revealing that dual-layer
reconstruction enhances inter-class discrimination while mask-enhanced
transformation reduces intra-class variations. Visualization results provide
evidence of HMDRN's superior feature reconstruction capabilities.

</details>


### [39] [Forensic Study of Paintings Through the Comparison of Fabrics](https://arxiv.org/abs/2506.20272)
*Juan José Murillo-Fuentes,Pablo M. Olmos,Laura Alba-Carcelén*

Main category: cs.CV

TL;DR: A novel deep learning approach is presented to assess the similarity of canvas fabrics in artworks, overcoming limitations of traditional methods and enabling analysis of non-contiguous pieces.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for analyzing canvas fabrics in art rely on thread density map matching, which is ineffective for non-contiguous canvas pieces.

Method: A Siamese deep learning model is designed and trained to compare pairs of images, with a similarity estimation method aggregating predictions from multiple pairs of cloth samples.

Result: The approach is applied to canvases from the Museo Nacional del Prado, corroborating the hypothesis that plain weave canvases can be effectively compared even with similar thread densities.

Conclusion: The proposed deep learning method demonstrates feasibility and accuracy in comparing plain weave canvases, opening new avenues for analyzing masterpieces.

Abstract: The study of canvas fabrics in works of art is a crucial tool for
authentication, attribution and conservation. Traditional methods are based on
thread density map matching, which cannot be applied when canvases do not come
from contiguous positions on a roll. This paper presents a novel approach based
on deep learning to assess the similarity of textiles. We introduce an
automatic tool that evaluates the similarity between canvases without relying
on thread density maps. A Siamese deep learning model is designed and trained
to compare pairs of images by exploiting the feature representations learned
from the scans. In addition, a similarity estimation method is proposed,
aggregating predictions from multiple pairs of cloth samples to provide a
robust similarity score. Our approach is applied to canvases from the Museo
Nacional del Prado, corroborating the hypothesis that plain weave canvases,
widely used in painting, can be effectively compared even when their thread
densities are similar. The results demonstrate the feasibility and accuracy of
the proposed method, opening new avenues for the analysis of masterpieces.

</details>


### [40] [From Ideal to Real: Unified and Data-Efficient Dense Prediction for Real-World Scenarios](https://arxiv.org/abs/2506.20279)
*Changliang Xia,Chengyou Jia,Zhuohang Dang,Minnan Luo*

Main category: cs.CV

TL;DR: The paper introduces DenseWorld, a benchmark for real-world dense prediction tasks, and DenseDiT, a model that leverages generative priors for improved performance with minimal additional parameters and training data.


<details>
  <summary>Details</summary>
Motivation: Existing methods primarily focus on idealized conditions, with limited generalization to real-world scenarios and facing the challenging scarcity of real-world data.

Method: We propose DenseDiT, which maximally exploits generative models' visual priors to perform diverse real-world dense prediction tasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism and two lightweight branches that adaptively integrate multi-scale context, working with less than 0.1% additional parameters.

Result: Evaluations on DenseWorld reveal significant performance drops in existing general and specialized baselines, highlighting their limited real-world generalization.

Conclusion: DenseDiT achieves superior results using less than 0.01% training data of baselines, underscoring its practical value for real-world deployment.

Abstract: Dense prediction tasks hold significant importance of computer vision, aiming
to learn pixel-wise annotated label for an input image. Despite advances in
this field, existing methods primarily focus on idealized conditions, with
limited generalization to real-world scenarios and facing the challenging
scarcity of real-world data. To systematically study this problem, we first
introduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction
tasks that correspond to urgent real-world applications, featuring unified
evaluation across tasks. Then, we propose DenseDiT, which maximally exploits
generative models' visual priors to perform diverse real-world dense prediction
tasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism
and two lightweight branches that adaptively integrate multi-scale context,
working with less than 0.1% additional parameters. Evaluations on DenseWorld
reveal significant performance drops in existing general and specialized
baselines, highlighting their limited real-world generalization. In contrast,
DenseDiT achieves superior results using less than 0.01% training data of
baselines, underscoring its practical value for real-world deployment. Our
data, and checkpoints and codes are available at
https://xcltql666.github.io/DenseDiTProj

</details>


### [41] [Breaking Spatial Boundaries: Spectral-Domain Registration Guided Hyperspectral and Multispectral Blind Fusion](https://arxiv.org/abs/2506.20293)
*Kunjing Yang,Libin Zheng,Minru Bai,Ting Lu,Leyuan Fang*

Main category: cs.CV

TL;DR: This paper proposes a spectral domain approach to address the registration challenge in blind fusion of unregistered hyperspectral and multispectral images. A lightweight Spectral Prior Learning network and a blind sparse fusion method are used. Numerical experiments verify the effectiveness of the method in registration and fusion, and also demonstrate its efficacy in enhancing classification performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods employ spatial transformations on the HSI to achieve alignment with the MSI. However, due to the substantial differences in spatial resolution of the images, the performance of these methods is often unsatisfactory. Moreover, the registration process tends to be time-consuming when dealing with large-sized images in remote sensing.

Method: A lightweight Spectral Prior Learning (SPL) network is developed to extract spectral features from the HSI and enhance the spectral resolution of the MSI. Following this, the obtained image undergoes spatial downsampling to produce the registered HSI. Next, a blind sparse fusion (BSF) method, which utilizes group sparsity regularization to equivalently promote the low-rankness of the image is proposed. Then, the Proximal Alternating Optimization (PAO) algorithm to solve the BSF model is employed.

Result: The proposed method tackles the registration problem from the spectral domain. Subspace representation and cyclic training strategy are employed to improve spectral accuracy of the registered HSI obtained. The approach not only circumvents the need for rank estimation, but also reduces computational complexity.

Conclusion: The effectiveness of the proposed method in registration and fusion is verified through numerical experiments on simulated and real datasets. The efficacy in enhancing classification performance is also demonstrated.

Abstract: The blind fusion of unregistered hyperspectral images (HSIs) and
multispectral images (MSIs) has attracted growing attention recently. To
address the registration challenge, most existing methods employ spatial
transformations on the HSI to achieve alignment with the MSI. However, due to
the substantial differences in spatial resolution of the images, the
performance of these methods is often unsatisfactory. Moreover, the
registration process tends to be time-consuming when dealing with large-sized
images in remote sensing. To address these issues, we propose tackling the
registration problem from the spectral domain. Initially, a lightweight
Spectral Prior Learning (SPL) network is developed to extract spectral features
from the HSI and enhance the spectral resolution of the MSI. Following this,
the obtained image undergoes spatial downsampling to produce the registered
HSI. In this process, subspace representation and cyclic training strategy are
employed to improve spectral accuracy of the registered HSI obtained. Next, we
propose a blind sparse fusion (BSF) method, which utilizes group sparsity
regularization to equivalently promote the low-rankness of the image. This
approach not only circumvents the need for rank estimation, but also reduces
computational complexity. Then, we employ the Proximal Alternating Optimization
(PAO) algorithm to solve the BSF model, and present its convergence analysis.
Finally, extensive numerical experiments on simulated and real datasets are
conducted to verify the effectiveness of our method in registration and fusion.
We also demonstrate its efficacy in enhancing classification performance.

</details>


### [42] [Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag Explorations](https://arxiv.org/abs/2506.20294)
*Shunqi Mao,Wei Guo,Chaoyi Zhang,Weidong Cai*

Main category: cs.CV

TL;DR: 提出了一种名为 Ctrl-Z 采样的新采样策略，通过在局部最大值处注入噪声并回退到之前的状态来逃离局部最优，从而提高条件生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在条件生成方面表现出强大的性能，通过逐步去噪高斯噪声来实现目标数据分布。然而，由于潜在空间的复杂性和次优初始化，扩散模型通常会收敛到局部最优，这些局部最优在局部视觉上是连贯的，但在全局上是不一致的或条件错位的。

Method: 引入了受控随机 Zigzag 采样 (Ctrl-Z 采样)，这是一种新颖的采样策略，旨在检测和逃离条件生成过程中的局部最大值。该方法首先使用奖励模型识别潜在的局部最大值。检测到局部最大值后，它会注入噪声并恢复到之前的、噪声更大的状态，以逃离当前的优化平台。然后，奖励模型评估候选轨迹，仅接受那些提供改进的轨迹，而逐渐加深的撤退可以在附近的替代方案失败时实现更强的逃逸。

Result: Ctrl-Z Sampling 是一种与模型无关的方法，与现有的扩散框架兼容。实验结果表明

Conclusion: Ctrl-Z Sampling 显著提高了生成质量，且函数评估次数仅增加约 7.6 倍。

Abstract: Diffusion models have shown strong performance in conditional generation by
progressively denoising Gaussian noise toward a target data distribution. This
denoising process can be interpreted as a form of hill climbing in a learned
latent space, where the model iteratively refines the sample toward regions of
higher probability. However, diffusion models often converge to local optima
that are locally visually coherent yet globally inconsistent or conditionally
misaligned, due to latent space complexity and suboptimal initialization. Prior
efforts attempted to address this by strengthening guidance signals or
manipulating the initial noise distribution. We introduce Controlled Random
Zigzag Sampling (Ctrl-Z Sampling), a novel sampling strategy designed to detect
and escape such local maxima during conditional generation. The method first
identifies potential local maxima using a reward model. Upon detection, it
injects noise and reverts to a previous, noisier state to escape the current
optimization plateau. The reward model then evaluates candidate trajectories,
accepting only those that offer improvement, while progressively deeper retreat
enables stronger escapes when nearby alternatives fail. This controlled random
zigzag process allows dynamic alternation between forward refinement and
backward exploration, enhancing both alignment and visual quality in the
generated outputs. The proposed Ctrl-Z Sampling is model-agnostic and
compatible with existing diffusion frameworks. Experimental results show that
Ctrl-Z Sampling substantially improves generation quality with only around 7.6X
increase in function evaluations.

</details>


### [43] [TDiR: Transformer based Diffusion for Image Restoration Tasks](https://arxiv.org/abs/2506.20302)
*Abbas Anwar,Mohammad Shullar,Ali Arshad Nasir,Mudassir Masood,Saeed Anwar*

Main category: cs.CV

TL;DR: 开发了一个基于transformer的扩散模型，以解决图像恢复任务，旨在提高退化图像的质量。


<details>
  <summary>Details</summary>
Motivation: 在具有挑战性的环境中拍摄的图像通常会经历各种形式的退化，包括噪声、色彩失真、模糊和光散射。这些影响显著降低了图像质量，阻碍了它们在下游任务（如目标检测、映射和分类）中的适用性。

Method: transformer-based diffusion模型

Result: 该扩散模型结合transformer，在性能上超过了当前的方法。

Conclusion: 扩散模型和transformer的结合在提高退化图像质量方面非常有效，从而扩大了它们在需要高保真视觉数据的下游任务中的效用。

Abstract: Images captured in challenging environments often experience various forms of
degradation, including noise, color cast, blur, and light scattering. These
effects significantly reduce image quality, hindering their applicability in
downstream tasks such as object detection, mapping, and classification. Our
transformer-based diffusion model was developed to address image restoration
tasks, aiming to improve the quality of degraded images. This model was
evaluated against existing deep learning methodologies across multiple quality
metrics for underwater image enhancement, denoising, and deraining on publicly
available datasets. Our findings demonstrate that the diffusion model, combined
with transformers, surpasses current methods in performance. The results of our
model highlight the efficacy of diffusion models and transformers in improving
the quality of degraded images, consequently expanding their utility in
downstream tasks that require high-fidelity visual data.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [44] [Prover Agent: An Agent-based Framework for Formal Mathematical Proofs](https://arxiv.org/abs/2506.19923)
*Kaito Baba,Chaoran Liu,Shuhei Kurita,Akiyoshi Sannai*

Main category: cs.AI

TL;DR: Prover Agent结合LLM和Lean，在MiniF2F基准测试上取得了新的SOTA，且样本预算较低。


<details>
  <summary>Details</summary>
Motivation: 提出了Prover Agent，一种用于自动定理证明的新型AI Agent。

Method: 集成了大型语言模型（LLM）与形式化证明助手Lean。

Result: Prover Agent能够生成辅助引理以帮助发现整体证明策略，并在解决具有挑战性的问题中发挥作用。

Conclusion: Prover Agent实现了在MiniF2F基准测试上86.1%的成功率，在小语言模型（SLM）方法中建立了新的最先进水平。

Abstract: We present Prover Agent, a novel AI agent for automated theorem proving that
integrates large language models (LLMs) with a formal proof assistant, Lean.
Prover Agent coordinates an informal reasoning LLM, a formal prover model, and
feedback from Lean while also generating auxiliary lemmas to assist in
discovering the overall proof strategy. It achieves an 86.1% success rate on
the MiniF2F benchmark, establishing a new state-of-the-art among methods using
small language models (SLMs) with a much lower sample budget than previous
approaches. We also present case studies illustrating how these generated
lemmas contribute to solving challenging problems.

</details>


### [45] [Context Attribution with Multi-Armed Bandit Optimization](https://arxiv.org/abs/2506.19977)
*Deng Pan,Keerthiram Murugesan,Nuno Moniz,Nitesh Chawla*

Main category: cs.AI

TL;DR: This paper presents a novel framework for context attribution in generative QA systems using a combinatorial multi-armed bandit approach, achieving improved query efficiency and high attribution fidelity compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Understanding which parts of the retrieved context contribute to a large language model's generated answer is essential for building interpretable and trustworthy generative QA systems.

Method: The paper formulates context attribution as a combinatorial multi-armed bandit (CMAB) problem and employs Combinatorial Thompson Sampling (CTS).

Result: The method demonstrates substantially improved query efficiency while maintaining high attribution fidelity.

Conclusion: The proposed method achieves competitive attribution quality with fewer model queries.

Abstract: Understanding which parts of the retrieved context contribute to a large
language model's generated answer is essential for building interpretable and
trustworthy generative QA systems. We propose a novel framework that formulates
context attribution as a combinatorial multi-armed bandit (CMAB) problem. Each
context segment is treated as a bandit arm, and we employ Combinatorial
Thompson Sampling (CTS) to efficiently explore the exponentially large space of
context subsets under a limited query budget. Our method defines a reward
function based on normalized token likelihoods, capturing how well a subset of
segments supports the original model response. Unlike traditional
perturbation-based attribution methods such as SHAP, which sample subsets
uniformly and incur high computational costs, our approach adaptively balances
exploration and exploitation by leveraging posterior estimates of segment
relevance. This leads to substantially improved query efficiency while
maintaining high attribution fidelity. Extensive experiments on diverse
datasets and LLMs demonstrate that our method achieves competitive attribution
quality with fewer model queries.

</details>


### [46] [QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges](https://arxiv.org/abs/2506.20008)
*Abdul Basit,Minghao Shao,Haider Asif,Nouhaila Innan,Muhammad Kashif,Alberto Marchisio,Muhammad Shafique*

Main category: cs.AI

TL;DR: This paper benchmarks LLMs for quantum code generation, introduces a new dataset (QHackBench), and finds that RAG-enhanced models perform well. They also introduce a multi-agent evaluation pipeline that improves execution success rates and will release their dataset and evaluation framework.


<details>
  <summary>Details</summary>
Motivation: LLMs have demonstrated strong potential in code generation, yet their effectiveness in quantum computing remains underexplored.

Method: LLMs for PennyLane-based quantum code generation using real-world challenges from the Quantum Hackathon (QHack). QHackBench, a novel benchmark dataset derived from QHack competitions, and evaluate model performance under vanilla prompting and Retrieval-Augmented Generation (RAG). A multi-agent evaluation pipeline that iteratively refines incorrect solutions

Result: RAG-enhanced models, supplemented with an augmented PennyLane dataset, generate similar results as the standard prompting, particularly in complex quantum algorithms.

Conclusion: RAG-enhanced models generate similar results as standard prompting, particularly in complex quantum algorithms. A multi-agent evaluation pipeline iteratively refines incorrect solutions, enhancing execution success rates. The QHackBench dataset, evaluation framework, and experimental results are publicly released.

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated strong
potential in code generation, yet their effectiveness in quantum computing
remains underexplored. This paper benchmarks LLMs for PennyLane-based quantum
code generation using real-world challenges from the Quantum Hackathon (QHack).
We introduce QHackBench, a novel benchmark dataset derived from QHack
competitions, and evaluate model performance under vanilla prompting and
Retrieval-Augmented Generation (RAG). Our structured evaluation framework
assesses functional correctness, syntactic validity, and execution success
across varying challenge difficulties. Results indicate that RAG-enhanced
models, supplemented with an augmented PennyLane dataset, approximately
generate similar results as the standard prompting, particularly in complex
quantum algorithms. Additionally, we introduce a multi-agent evaluation
pipeline that iteratively refines incorrect solutions, further enhancing
execution success rates. To foster further research, we commit to publicly
releasing QHackBench, along with our evaluation framework and experimental
results, enabling continued advancements in AI-assisted quantum programming.

</details>


### [47] [Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks](https://arxiv.org/abs/2506.20009)
*Konstantinos Vrettos,Michail E. Klontzas*

Main category: cs.AI

TL;DR: 本地 LLM 可以构建比商业模型更好、更环保的医疗 RAG。


<details>
  <summary>Details</summary>
Motivation: 医疗保健中越来越多地采用人工智能 (AI) 引发了对其环境和伦理影响的日益关注。ChatGPT 和 DeepSeek 等商业大型语言模型 (LLM) 需要大量资源，而将这些系统用于医疗目的会引发有关患者隐私和安全的关键问题。

Method: 我们开发了一个用于医疗任务的可定制检索增强生成 (RAG) 框架，该框架可监控其能源使用和 CO2 排放。然后，该系统用于创建基于各种开源 LLM 的 RAG。测试的模型包括通用模型（如 llama3.1:8b）和医学领域特定模型（medgemma-4b-it）。我们将最佳 RAG 的性能和能耗与 DeepSeekV3-R1 和 OpenAI 的 o4-mini 模型进行了比较。使用医学问题数据集进行评估。

Result: 定制 RAG 模型在准确性和能耗方面优于商业模型。基于 llama3.1:8B 构建的 RAG 模型实现了最高的准确率 (58.5%)，并且明显优于包括 o4-mini 和 DeepSeekV3-R1 在内的其他模型。llama3.1-RAG 在所有模型中也表现出最低的能耗和 CO2 排放，每千瓦时的性能为 0.52，总 CO2 排放量为 473 克。与 o4-mini 相比，llama3.1-RAG 在保持更高准确率的同时，每千瓦时实现了 2.7 倍以上的准确率，并减少了 172% 的电力使用。

Conclusion: 本地 LLM 可用于开发 RAG，其在医疗任务中的表现优于商业在线 LLM，同时环境影响更小。我们的模块化框架促进了可持续 AI 开发，减少了电力使用，并与联合国可持续发展目标相一致。

Abstract: Background The increasing adoption of Artificial Intelligence (AI) in
healthcare has sparked growing concerns about its environmental and ethical
implications. Commercial Large Language Models (LLMs), such as ChatGPT and
DeepSeek, require substantial resources, while the utilization of these systems
for medical purposes raises critical issues regarding patient privacy and
safety. Methods We developed a customizable Retrieval-Augmented Generation
(RAG) framework for medical tasks, which monitors its energy usage and CO2
emissions. This system was then used to create RAGs based on various
open-source LLMs. The tested models included both general purpose models like
llama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs
performance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs
o4-mini model. A dataset of medical questions was used for the evaluation.
Results Custom RAG models outperformed commercial models in accuracy and energy
consumption. The RAG model built on llama3.1:8B achieved the highest accuracy
(58.5%) and was significantly better than other models, including o4-mini and
DeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption
and CO2 footprint among all models, with a Performance per kWh of 0.52 and a
total CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x
times more accuracy points per kWh and 172% less electricity usage while
maintaining higher accuracy. Conclusion Our study demonstrates that local LLMs
can be leveraged to develop RAGs that outperform commercial, online LLMs in
medical tasks, while having a smaller environmental impact. Our modular
framework promotes sustainable AI development, reducing electricity usage and
aligning with the UNs Sustainable Development Goals.

</details>


### [48] [Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models](https://arxiv.org/abs/2506.20018)
*Zechun Deng,Ziwei Liu,Ziqian Bi,Junhao Song,Chia Xin Liang,Joe Yeong,Junfeng Hao*

Main category: cs.AI

TL;DR: 本文探讨了利用低延迟 AI 模型的实时决策支持系统，着眼于大型语言模型如何在资源有限时协助决策，并为未来的突破铺平道路。


<details>
  <summary>Details</summary>
Motivation: 研究实时决策支持系统，该系统利用低延迟 AI 模型，汇集了整体 AI 驱动的决策工具、与 Edge-IoT 技术的集成以及有效的人工智能团队合作方法的最新进展，着眼于大型语言模型如何在资源有限时协助决策。

Method: 通过详细的回顾，提供了关于开发策略和应用领域的实践观点。

Result: 考察了 DeLLMa 等技术发展的影响、压缩模型的方法以及边缘设备分析的改进，同时解决了资源有限和需要适应性框架等问题。

Conclusion: AI 有可能重塑实时决策支持，为未来突破铺平道路。

Abstract: This paper investigates real-time decision support systems that leverage
low-latency AI models, bringing together recent progress in holistic AI-driven
decision tools, integration with Edge-IoT technologies, and approaches for
effective human-AI teamwork. It looks into how large language models can assist
decision-making, especially when resources are limited. The research also
examines the effects of technical developments such as DeLLMa, methods for
compressing models, and improvements for analytics on edge devices, while also
addressing issues like limited resources and the need for adaptable frameworks.
Through a detailed review, the paper offers practical perspectives on
development strategies and areas of application, adding to the field by
pointing out opportunities for more efficient and flexible AI-supported
systems. The conclusions set the stage for future breakthroughs in this
fast-changing area, highlighting how AI can reshape real-time decision support.

</details>


### [49] [Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning](https://arxiv.org/abs/2506.20020)
*Saloni Dash,Amélie Reymond,Emma S. Spiro,Aylin Caliskan*

Main category: cs.AI

TL;DR: LLMs with assigned personas exhibit human-like motivated reasoning, which is difficult to mitigate and may worsen identity-based reasoning in both LLMs and humans.


<details>
  <summary>Details</summary>
Motivation: Motivated reasoning at a collective level can be detrimental to society, and prior studies have reported that large language models (LLMs) are also susceptible to human-like cognitive biases, however, the extent to which LLMs selectively reason toward identity-congruent conclusions remains largely unexplored.

Method: Assigning 8 personas across 4 political and socio-demographic attributes to induce motivated reasoning in LLMs, and testing 8 LLMs across two reasoning tasks from human-subject studies.

Result: Persona-assigned LLMs have up to 9% reduced veracity discernment relative to models without personas. Political personas specifically, are up to 90% more likely to correctly evaluate scientific evidence on gun control when the ground truth is congruent with their induced political identity. Prompt-based debiasing methods are largely ineffective at mitigating these effects.

Conclusion: Persona-assigned LLMs exhibit human-like motivated reasoning that is hard to mitigate through conventional debiasing prompts, raising concerns of exacerbating identity-congruent reasoning in both LLMs and humans.

Abstract: Reasoning in humans is prone to biases due to underlying motivations like
identity protection, that undermine rational decision-making and judgment. This
motivated reasoning at a collective level can be detrimental to society when
debating critical issues such as human-driven climate change or vaccine safety,
and can further aggravate political polarization. Prior studies have reported
that large language models (LLMs) are also susceptible to human-like cognitive
biases, however, the extent to which LLMs selectively reason toward
identity-congruent conclusions remains largely unexplored. Here, we investigate
whether assigning 8 personas across 4 political and socio-demographic
attributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and
proprietary) across two reasoning tasks from human-subject studies -- veracity
discernment of misinformation headlines and evaluation of numeric scientific
evidence -- we find that persona-assigned LLMs have up to 9% reduced veracity
discernment relative to models without personas. Political personas
specifically, are up to 90% more likely to correctly evaluate scientific
evidence on gun control when the ground truth is congruent with their induced
political identity. Prompt-based debiasing methods are largely ineffective at
mitigating these effects. Taken together, our empirical findings are the first
to suggest that persona-assigned LLMs exhibit human-like motivated reasoning
that is hard to mitigate through conventional debiasing prompts -- raising
concerns of exacerbating identity-congruent reasoning in both LLMs and humans.

</details>


### [50] [DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction](https://arxiv.org/abs/2506.20059)
*Weijieying Ren,Tianxiang Zhao,Lei Wang,Tianchun Wang,Vasant Honavar*

Main category: cs.AI

TL;DR: DiaLLM是一个将EHR数据整合到临床对话中的医疗LLM，用于临床测试推荐和诊断预测。


<details>
  <summary>Details</summary>
Motivation: 现有的医疗LLM忽略了电子健康记录(EHR)的重要作用，主要关注诊断推荐，限制了它们的临床适用性。

Method: 我们设计了一个临床测试参考(CTR)策略，将每个临床代码映射到其对应的描述，并将测试结果分类为“正常”或“异常”。此外，DiaLLM采用强化学习框架进行证据获取和自动诊断。为了处理大的动作空间，我们引入了一种拒绝抽样策略来减少冗余和提高探索效率。此外，确认奖励和类别敏感的诊断奖励旨在指导准确的诊断预测。

Result: DiaLLM是第一个将异构EHR数据集成到临床对话中的医疗LLM，能够进行临床测试推荐、结果解释和诊断预测，以更好地与真实世界的医疗实践相一致。

Conclusion: DiaLLM在临床测试推荐和诊断预测方面优于基线模型。

Abstract: Recent advances in Large Language Models (LLMs) have led to remarkable
progresses in medical consultation. However, existing medical LLMs overlook the
essential role of Electronic Health Records (EHR) and focus primarily on
diagnosis recommendation, limiting their clinical applicability. We propose
DiaLLM, the first medical LLM that integrates heterogeneous EHR data into
clinically grounded dialogues, enabling clinical test recommendation, result
interpretation, and diagnosis prediction to better align with real-world
medical practice. To construct clinically grounded dialogues from EHR, we
design a Clinical Test Reference (CTR) strategy that maps each clinical code to
its corresponding description and classifies test results as "normal" or
"abnormal". Additionally, DiaLLM employs a reinforcement learning framework for
evidence acquisition and automated diagnosis. To handle the large action space,
we introduce a reject sampling strategy to reduce redundancy and improve
exploration efficiency. Furthermore, a confirmation reward and a
class-sensitive diagnosis reward are designed to guide accurate diagnosis
prediction. Extensive experimental results demonstrate that DiaLLM outperforms
baselines in clinical test recommendation and diagnosis prediction.

</details>


### [51] [AI Copilots for Reproducibility in Science: A Case Study](https://arxiv.org/abs/2506.20130)
*Adrien Bibal,Steven N. Minton,Deborah Khider,Yolanda Gil*

Main category: cs.AI

TL;DR: OpenPub, an AI-powered platform, introduces the Reproducibility Copilot, which analyzes research papers to generate structured Jupyter Notebooks and recommendations aimed at facilitating computational reproducibility. It reduces reproduction time from over 30 hours to about 1 hour.


<details>
  <summary>Details</summary>
Motivation: Ensuring that published findings can be independently reproduced remains a persistent challenge.

Method: The Reproducibility Copilot analyzes manuscripts, code, and supplementary materials to generate structured Jupyter Notebooks and recommendations aimed at facilitating computational, or "rote", reproducibility. Feasibility tests were conducted using previously studied research papers with known reproducibility benchmarks.

Result: OpenPub can substantially reduce reproduction time - from over 30 hours to about 1 hour - while achieving high coverage of figures, tables, and results suitable for computational reproduction. The system systematically detects barriers to reproducibility, including missing hyperparameters, undocumented preprocessing steps, and incomplete or inaccessible datasets.

Conclusion: AI-driven tools can reduce the burden of reproducibility efforts and contribute to more transparent and verifiable scientific communication. The modular copilot architecture also provides a foundation for extending AI assistance to additional open science objectives beyond reproducibility.

Abstract: Open science initiatives seek to make research outputs more transparent,
accessible, and reusable, but ensuring that published findings can be
independently reproduced remains a persistent challenge. This paper introduces
OpenPub, an AI-powered platform that supports researchers, reviewers, and
readers through a suite of modular copilots focused on key open science tasks.
In this work, we present the Reproducibility Copilot, which analyzes
manuscripts, code, and supplementary materials to generate structured Jupyter
Notebooks and recommendations aimed at facilitating computational, or "rote",
reproducibility. We conducted feasibility tests using previously studied
research papers with known reproducibility benchmarks. Results indicate that
OpenPub can substantially reduce reproduction time - from over 30 hours to
about 1 hour - while achieving high coverage of figures, tables, and results
suitable for computational reproduction. The system systematically detects
barriers to reproducibility, including missing hyperparameters, undocumented
preprocessing steps, and incomplete or inaccessible datasets. These findings
suggest that AI-driven tools can meaningfully reduce the burden of
reproducibility efforts and contribute to more transparent and verifiable
scientific communication. The modular copilot architecture also provides a
foundation for extending AI assistance to additional open science objectives
beyond reproducibility.

</details>


### [52] [Language Modeling by Language Models](https://arxiv.org/abs/2506.20249)
*Junyan Cheng,Peter Clark,Kyle Richardson*

Main category: cs.AI

TL;DR: Genesys, a multi-agent LLM system, discovers novel LM architectures using genetic programming and scaling laws, achieving competitive performance with known architectures.


<details>
  <summary>Details</summary>
Motivation: Leverage LLMs to model the process of discovering novel language model architectures, inspired by real research.

Method: A multi-agent LLM approach that simulates conventional research stages, using a Ladder of Scales approach and a novel genetic programming backbone.

Result: 1,162 newly discovered designs (1,062 fully verified through pre-training), with significant improvement in successful design generation compared to direct prompt generation workflows.

Conclusion: The best discovered designs are highly competitive with known architectures, outperforming GPT2 and Mamba2 on 6/9 common benchmarks. Comprehensive system-level ablations and formal results give broader insights into the design of effective autonomous discovery systems.

Abstract: Can we leverage LLMs to model the process of discovering novel language model
(LM) architectures? Inspired by real research, we propose a multi-agent LLM
approach that simulates the conventional stages of research, from ideation and
literature search (proposal stage) to design implementation (code generation),
generative pre-training, and downstream evaluation (verification). Using ideas
from scaling laws, our system, Genesys, employs a Ladder of Scales approach;
new designs are proposed, adversarially reviewed, implemented, and selectively
verified at increasingly larger model scales (14M$\sim$350M parameters) with a
narrowing budget (the number of models we can train at each scale). To help
make discovery efficient and factorizable, Genesys uses a novel genetic
programming backbone, which we show has empirical advantages over commonly used
direct prompt generation workflows (e.g., $\sim$86\% percentage point
improvement in successful design generation, a key bottleneck). We report
experiments involving 1,162 newly discovered designs (1,062 fully verified
through pre-training) and find the best designs to be highly competitive with
known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common
benchmarks). We couple these results with comprehensive system-level ablations
and formal results, which give broader insights into the design of effective
autonomous discovery systems.

</details>


### [53] [Enterprise Large Language Model Evaluation Benchmark](https://arxiv.org/abs/2506.20274)
*Liya Wang,David Yi,Damien Jose,John Passarelli,James Gao,Jordan Leventis,Kang Li*

Main category: cs.AI

TL;DR: This paper introduces a new benchmark for evaluating LLMs in enterprise contexts, revealing performance gaps and providing insights for model optimization.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like Massive Multitask Language Understanding (MMLU) inadequately assess enterprise-specific task complexities.

Method: We propose a 14-task framework grounded in Bloom's Taxonomy to holistically evaluate LLM capabilities in enterprise contexts. To address challenges of noisy data and costly annotation, we develop a scalable pipeline combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented generation (CRAG), curating a robust 9,700-sample benchmark.

Result: Evaluation of six leading models shows open-source contenders like DeepSeek R1 rival proprietary models in reasoning tasks but lag in judgment-based scenarios, likely due to overthinking.

Conclusion: The benchmark reveals critical enterprise performance gaps and offers actionable insights for model optimization. This work provides enterprises a blueprint for tailored evaluations and advances practical LLM deployment.

Abstract: Large Language Models (LLMs) ) have demonstrated promise in boosting
productivity across AI-powered tools, yet existing benchmarks like Massive
Multitask Language Understanding (MMLU) inadequately assess enterprise-specific
task complexities. We propose a 14-task framework grounded in Bloom's Taxonomy
to holistically evaluate LLM capabilities in enterprise contexts. To address
challenges of noisy data and costly annotation, we develop a scalable pipeline
combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented
generation (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six
leading models shows open-source contenders like DeepSeek R1 rival proprietary
models in reasoning tasks but lag in judgment-based scenarios, likely due to
overthinking. Our benchmark reveals critical enterprise performance gaps and
offers actionable insights for model optimization. This work provides
enterprises a blueprint for tailored evaluations and advances practical LLM
deployment.

</details>


### [54] [Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards](https://arxiv.org/abs/2506.20332)
*Jihao Gu,Qihang Ai,Yingyao Wang,Pi Bu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Ziming Wang,Yingxiu Zhao,Ming-Liang Zhang,Jun Song,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: Mobile-R1 使用交互式的多轮强化学习和任务级别的奖励来改进移动代理，并通过新的数据集和基准进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有的研究集中于离线强化学习训练或使用动作级别奖励的在线优化，这限制了智能体与环境的动态交互。这通常导致智能体陷入局部最优，从而削弱了它们的探索和错误行动纠正能力。

Method: Mobile-R1，它采用交互式的多轮强化学习与任务级别的奖励。

Result: 收集了一个包含28个中国应用程序的数据集，其中包含24,521个高质量的手动注释，并建立了一个包含500个轨迹的新基准。

Conclusion: Mobile-R1显著提高了性能，通过交互式的多轮强化学习和任务级别的奖励，增强了探索和纠错能力。

Abstract: Vision-language model-based mobile agents have gained the ability to not only
understand complex instructions and mobile screenshots, but also optimize their
action outputs via thinking and reasoning, benefiting from reinforcement
learning, such as Group Relative Policy Optimization (GRPO). However, existing
research centers on offline reinforcement learning training or online
optimization using action-level rewards, which limits the agent's dynamic
interaction with the environment. This often results in agents settling into
local optima, thereby weakening their ability for exploration and error action
correction. To address these challenges, we introduce an approach called
Mobile-R1, which employs interactive multi-turn reinforcement learning with
task-level rewards for mobile agents. Our training framework consists of three
stages: initial format finetuning, single-step online training via action-level
reward, followed by online training via task-level reward based on multi-turn
trajectories. This strategy is designed to enhance the exploration and error
correction capabilities of Mobile-R1, leading to significant performance
improvements. Moreover, we have collected a dataset covering 28 Chinese
applications with 24,521 high-quality manual annotations and established a new
benchmark with 500 trajectories. We will open source all resources, including
the dataset, benchmark, model weight, and codes:
https://mobile-r1.github.io/Mobile-R1/.

</details>


### [55] [Tabular Feature Discovery With Reasoning Type Exploration](https://arxiv.org/abs/2506.20357)
*Sungwon Han,Sungkyu Park,Seungeon Lee*

Main category: cs.AI

TL;DR: This paper introduces REFeat, a new method that guides large language models (LLMs) to discover diverse and informative features for tabular data by leveraging multiple types of reasoning. It achieves higher predictive accuracy and discovers more meaningful features on 59 benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Feature engineering for tabular data remains a critical yet challenging step in machine learning. Existing LLM-based approaches often produce overly simple or repetitive features, partly due to inherent biases in the transformations the LLM chooses and the lack of structured reasoning guidance during generation.

Method: The paper proposes a novel method REFeat, which guides an LLM to discover diverse and informative features by leveraging multiple types of reasoning to steer the feature generation process.

Result: Experiments on 59 benchmark datasets demonstrate that the approach not only achieves higher predictive accuracy on average, but also discovers more diverse and meaningful features.

Conclusion: This paper demonstrates the promise of incorporating rich reasoning paradigms and adaptive strategy selection into LLM-driven feature discovery for tabular data.

Abstract: Feature engineering for tabular data remains a critical yet challenging step
in machine learning. Recently, large language models (LLMs) have been used to
automatically generate new features by leveraging their vast knowledge.
However, existing LLM-based approaches often produce overly simple or
repetitive features, partly due to inherent biases in the transformations the
LLM chooses and the lack of structured reasoning guidance during generation. In
this paper, we propose a novel method REFeat, which guides an LLM to discover
diverse and informative features by leveraging multiple types of reasoning to
steer the feature generation process. Experiments on 59 benchmark datasets
demonstrate that our approach not only achieves higher predictive accuracy on
average, but also discovers more diverse and meaningful features. These results
highlight the promise of incorporating rich reasoning paradigms and adaptive
strategy selection into LLM-driven feature discovery for tabular data.

</details>


### [56] [Paladin-mini: A Compact and Efficient Grounding Model Excelling in Real-World Scenarios](https://arxiv.org/abs/2506.20384)
*Dror Ivry,Oran Nahum*

Main category: cs.AI

TL;DR: 本文提出了一种新的小型的开源模型Paladin-mini和一个新的评估数据集grounding-benchmark，用于解决和评估声明的grounding问题。


<details>
  <summary>Details</summary>
Motivation: 本文介绍了两个重要的贡献，以解决在给定上下文中grounding声明的问题。Grounding意味着给定一个上下文（文档）和一个声明，文档中至少有一个支持该声明的证据。

Method: 介绍了Paladin-mini，一个紧凑的（3.8B参数）开源分类器模型（用于将数据标记为已grounding或未grounding）。

Result: Paladin-mini在现实场景中表现出强大的性能，grounding-benchmark是一个新的评估数据集，旨在评估关键推理任务的性能。

Conclusion: 展示了Paladin-mini在基准测试中与当前最先进技术的对比结果，并分享了清晰且可复现的结果。

Abstract: This paper introduces two significant contributions to address the issue of
grounding claims in a given context. Grounding means that given a context
(document) and a claim, there's at least one supportive evidence for the claim
in the document. We will introduce Paladin-mini, a compact (3.8B parameters)
open-source classifier model (used for labeling data as grounded or ungrounded)
engineered for robust performance in real-world scenarios, and the
grounding-benchmark, a new evaluation dataset designed to assess performance on
critical reasoning tasks. We'll also demonstrate the results of Paladin-mini
with benchmarks against the current State-of-the-art and share clear and
reproducible results.

</details>


### [57] [Smart Ride and Delivery Services with Electric Vehicles: Leveraging Bidirectional Charging for Profit Optimisation](https://arxiv.org/abs/2506.20401)
*Jinchun Du,Bojie Shen,Muhammad Aamir Cheema,Adel N. Toosi*

Main category: cs.AI

TL;DR: This paper introduces the Electric Vehicle Orienteering Problem with V2G (EVOP-V2G): a profit-maximization problem where EV drivers must select customer requests or orders while managing when and where to charge or discharge. Two metaheuristic algorithms are proposed and the experiments on real-world data show that the proposed methods can double driver profits compared to baselines.


<details>
  <summary>Details</summary>
Motivation: With the rising popularity of electric vehicles (EVs), modern service systems, such as ride-hailing delivery services, are increasingly integrating EVs into their operations. Unlike conventional vehicles, EVs often have a shorter driving range, necessitating careful consideration of charging when fulfilling requests. With recent advances in Vehicle-to-Grid (V2G) technology - allowing EVs to also discharge energy back to the grid - new opportunities and complexities emerge.

Method: The problem is formulated as a Mixed Integer Programming (MIP) model and propose two near-optimal metaheuristic algorithms: one evolutionary (EA) and the other based on large neighborhood search (LNS).

Result: The proposed methods can double driver profits compared to baselines, while maintaining near-optimal performance on small instances and excellent scalability on larger ones.

Conclusion: The experiments on real-world data show that the proposed methods can double driver profits compared to baselines, while maintaining near-optimal performance on small instances and excellent scalability on larger ones. The work highlights a promising path toward smarter, more profitable EV-based mobility systems that actively support the energy grid.

Abstract: With the rising popularity of electric vehicles (EVs), modern service
systems, such as ride-hailing delivery services, are increasingly integrating
EVs into their operations. Unlike conventional vehicles, EVs often have a
shorter driving range, necessitating careful consideration of charging when
fulfilling requests. With recent advances in Vehicle-to-Grid (V2G) technology -
allowing EVs to also discharge energy back to the grid - new opportunities and
complexities emerge. We introduce the Electric Vehicle Orienteering Problem
with V2G (EVOP-V2G): a profit-maximization problem where EV drivers must select
customer requests or orders while managing when and where to charge or
discharge. This involves navigating dynamic electricity prices, charging
station selection, and route constraints. We formulate the problem as a Mixed
Integer Programming (MIP) model and propose two near-optimal metaheuristic
algorithms: one evolutionary (EA) and the other based on large neighborhood
search (LNS). Experiments on real-world data show our methods can double driver
profits compared to baselines, while maintaining near-optimal performance on
small instances and excellent scalability on larger ones. Our work highlights a
promising path toward smarter, more profitable EV-based mobility systems that
actively support the energy grid.

</details>


### [58] [GymPN: A Library for Decision-Making in Process Management Systems](https://arxiv.org/abs/2506.20404)
*Riccardo Lo Bianco,Willem van Jaarsveld,Remco Dijkman*

Main category: cs.AI

TL;DR: This paper presents GymPN, a software library that supports optimal decision-making in business processes using Deep Reinforcement Learning. It addresses limitations of previous work by supporting partial process observability and modeling multiple decisions, and demonstrates its effectiveness on typical business process decision-making problems.


<details>
  <summary>Details</summary>
Motivation: Suitable software tools are required to support these decisions in a way that is optimal for the organization.

Method: a software library, called GymPN, that supports optimal decision-making in business processes using Deep Reinforcement Learning

Result: introducing two key novelties: support for partial process observability and the ability to model multiple decisions in a business process

Conclusion: GymPN allows for easy modeling of the desired problems, as well as learning optimal decision policies.

Abstract: Process management systems support key decisions about the way work is
allocated in organizations. This includes decisions on which task to perform
next, when to execute the task, and who to assign the task to. Suitable
software tools are required to support these decisions in a way that is optimal
for the organization. This paper presents a software library, called GymPN,
that supports optimal decision-making in business processes using Deep
Reinforcement Learning. GymPN builds on previous work that supports task
assignment in business processes, introducing two key novelties: support for
partial process observability and the ability to model multiple decisions in a
business process. These novel elements address fundamental limitations of
previous work and thus enable the representation of more realistic process
decisions. We evaluate the library on eight typical business process
decision-making problem patterns, showing that GymPN allows for easy modeling
of the desired problems, as well as learning optimal decision policies.

</details>


### [59] [Mixtures of Neural Cellular Automata: A Stochastic Framework for Growth Modelling and Self-Organization](https://arxiv.org/abs/2506.20486)
*Salvatore Milite,Giulio Caravagna,Andrea Sottoriva*

Main category: cs.AI

TL;DR: MNCAs, a novel framework incorporating the idea of mixture models into the NCA paradigm, can model diverse local behaviors and reproduce the stochastic dynamics observed in biological processes.


<details>
  <summary>Details</summary>
Motivation: NCAs' deterministic nature limits their ability to capture the stochasticity of real-world biological and physical systems.

Method: We propose the Mixture of Neural Cellular Automata (MNCA), a novel framework incorporating the idea of mixture models into the NCA paradigm. By combining probabilistic rule assignments with intrinsic noise, MNCAs can model diverse local behaviors and reproduce the stochastic dynamics observed in biological processes.

Result: MNCAs achieve superior robustness to perturbations, better recapitulate real biological growth patterns, and provide interpretable rule segmentation.

Conclusion: MNCAs are a promising tool for modeling stochastic dynamical systems and studying self-growth processes.

Abstract: Neural Cellular Automata (NCAs) are a promising new approach to model
self-organizing processes, with potential applications in life science.
However, their deterministic nature limits their ability to capture the
stochasticity of real-world biological and physical systems.
  We propose the Mixture of Neural Cellular Automata (MNCA), a novel framework
incorporating the idea of mixture models into the NCA paradigm. By combining
probabilistic rule assignments with intrinsic noise, MNCAs can model diverse
local behaviors and reproduce the stochastic dynamics observed in biological
processes.
  We evaluate the effectiveness of MNCAs in three key domains: (1) synthetic
simulations of tissue growth and differentiation, (2) image morphogenesis
robustness, and (3) microscopy image segmentation. Results show that MNCAs
achieve superior robustness to perturbations, better recapitulate real
biological growth patterns, and provide interpretable rule segmentation. These
findings position MNCAs as a promising tool for modeling stochastic dynamical
systems and studying self-growth processes.

</details>


### [60] [Engineering Sentience](https://arxiv.org/abs/2506.20504)
*Konstantin Demin,Taylor Webb,Eric Elmoznino,Hakwan Lau*

Main category: cs.AI

TL;DR: 我们提出了一个感知的定义，它可能有助于在机器中设计和构建它。


<details>
  <summary>Details</summary>
Motivation: 我们详细阐述了感知的定义，这可能有助于在机器中设计和构建它。

Method: 我们提出，为了使感知对人工智能有意义，它必须以功能性的、计算性的术语来充实，要有足够的细节以允许实施。然而，这种感知概念也必须反映一些本质上“主观”的东西，而不仅仅是具有编码感知内容的一般能力。为了使这种特定的功能性感知概念发生，我们提出某些感官信号需要既是断言性的（持久的）又是定性的。

Result: 为了用更具体的术语来说明这个定义，我们概述了一些在当前技术条件下潜在的实现方法。

Conclusion: 理解人工智能代理在功能上具有感知能力需要什么，也可以帮助我们避免意外地创造它们，或者至少及时意识到我们已经创造了它们。

Abstract: We spell out a definition of sentience that may be useful for designing and
building it in machines. We propose that for sentience to be meaningful for AI,
it must be fleshed out in functional, computational terms, in enough detail to
allow for implementation. Yet, this notion of sentience must also reflect
something essentially 'subjective', beyond just having the general capacity to
encode perceptual content. For this specific functional notion of sentience to
occur, we propose that certain sensory signals need to be both assertoric
(persistent) and qualitative. To illustrate the definition in more concrete
terms, we sketch out some ways for potential implementation, given current
technology. Understanding what it takes for artificial agents to be
functionally sentient can also help us avoid creating them inadvertently, or at
least, realize that we have created them in a timely manner.

</details>


### [61] [Case-based Reasoning Augmented Large Language Model Framework for Decision Making in Realistic Safety-Critical Driving Scenarios](https://arxiv.org/abs/2506.20531)
*Wenbin Gan,Minh-Son Dao,Koji Zettsu*

Main category: cs.AI

TL;DR: 提出了CBR-LLM框架，通过整合语义场景理解和检索相关驾驶案例，提升大型语言模型在复杂风险场景中规避机动决策的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(llm)具有强大的通用推理能力，为决策提供了有希望的基础。然而，由于领域适应、情境基础以及缺乏在动态、高风险环境中做出可靠和可解释决策所需的经验知识，它们在自动驾驶中的直接应用仍然受到限制。

Method: 提出了一个基于案例推理增强的大型语言模型(CBR-LLM)框架，用于在复杂风险场景中进行规避机动决策。

Result: 该框架提高了决策的准确性、理由质量以及与人类专家行为的一致性。基于风险的提示策略进一步提高了各种风险类型的性能，而基于相似性的案例检索始终优于随机抽样。

Conclusion: CBR-LLM框架在复杂风险场景中具有鲁棒性，并有潜力作为智能驾驶系统的自适应和可信赖的决策支持工具。

Abstract: Driving in safety-critical scenarios requires quick, context-aware
decision-making grounded in both situational understanding and experiential
reasoning. Large Language Models (LLMs), with their powerful general-purpose
reasoning capabilities, offer a promising foundation for such decision-making.
However, their direct application to autonomous driving remains limited due to
challenges in domain adaptation, contextual grounding, and the lack of
experiential knowledge needed to make reliable and interpretable decisions in
dynamic, high-risk environments. To address this gap, this paper presents a
Case-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for
evasive maneuver decision-making in complex risk scenarios. Our approach
integrates semantic scene understanding from dashcam video inputs with the
retrieval of relevant past driving cases, enabling LLMs to generate maneuver
recommendations that are both context-sensitive and human-aligned. Experiments
across multiple open-source LLMs show that our framework improves decision
accuracy, justification quality, and alignment with human expert behavior.
Risk-aware prompting strategies further enhance performance across diverse risk
types, while similarity-based case retrieval consistently outperforms random
sampling in guiding in-context learning. Case studies further demonstrate the
framework's robustness in challenging real-world conditions, underscoring its
potential as an adaptive and trustworthy decision-support tool for intelligent
driving systems.

</details>


### [62] [Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges](https://arxiv.org/abs/2506.20598)
*Alexander D. Kalian,Jaewook Lee,Stefan P. Johannesson,Lennart Otte,Christer Hogstrand,Miao Guo*

Main category: cs.AI

TL;DR: A multi-agent AI framework with literature search and information extraction agents was developed for sustainable protein production research, focusing on microbial protein sources. Fine-tuning and prompt engineering were used to optimize the agents.


<details>
  <summary>Details</summary>
Motivation: The increasing global demand for sustainable protein sources requires intelligent tools to process and synthesize scientific knowledge rapidly.

Method: A RAG-oriented system with two GPT-based LLM agents: a literature search agent and an information extraction agent. Agent optimization was explored through fine-tuning and prompt engineering.

Result: Both fine-tuning and prompt engineering improved the information extraction agent's performance, increasing mean cosine similarity scores by up to 25% and universally reaching scores ≥ 0.89. Fine-tuning consistently achieved mean scores ≥ 0.94.

Conclusion: Fine-tuning and prompt engineering enhance the information extraction agent's performance, with fine-tuning achieving higher mean cosine similarity scores (≥ 0.94) compared to prompt engineering, although the latter shows lower statistical uncertainties. A user interface was developed for the multi-agent AI system, and chemical safety search capabilities were explored.

Abstract: The global demand for sustainable protein sources has accelerated the need
for intelligent tools that can rapidly process and synthesise domain-specific
scientific knowledge. In this study, we present a proof-of-concept multi-agent
Artificial Intelligence (AI) framework designed to support sustainable protein
production research, with an initial focus on microbial protein sources. Our
Retrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based
LLM agents: (1) a literature search agent that retrieves relevant scientific
literature on microbial protein production for a specified microbial strain,
and (2) an information extraction agent that processes the retrieved content to
extract relevant biological and chemical information. Two parallel
methodologies, fine-tuning and prompt engineering, were explored for agent
optimisation. Both methods demonstrated effectiveness at improving the
performance of the information extraction agent in terms of transformer-based
cosine similarity scores between obtained and ideal outputs. Mean cosine
similarity scores were increased by up to 25%, while universally reaching mean
scores of $\geq 0.89$ against ideal output text. Fine-tuning overall improved
the mean scores to a greater extent (consistently of $\geq 0.94$) compared to
prompt engineering, although lower statistical uncertainties were observed with
the latter approach. A user interface was developed and published for enabling
the use of the multi-agent AI system, alongside preliminary exploration of
additional chemical safety-based search capabilities

</details>


### [63] [CogGen: A Learner-Centered Generative AI Architecture for Intelligent Tutoring with Programming Video](https://arxiv.org/abs/2506.20600)
*Wengxi Li,Roy Pea,Nick Haber,Hari Subramonyam*

Main category: cs.AI

TL;DR: CogGen: Learner-centered AI architecture transforms programming videos into interactive, adaptive learning experiences.


<details>
  <summary>Details</summary>
Motivation: transform programming videos into interactive, adaptive learning experiences

Method: a learner-centered AI architecture that transforms programming videos into interactive, adaptive learning experiences by integrating student modeling with generative AI tutoring based on the Cognitive Apprenticeship framework. The architecture consists of three components: (1) video segmentation by learning goals, (2) a conversational tutoring engine applying Cognitive Apprenticeship strategies, and (3) a student model using Bayesian Knowledge Tracing to adapt instruction.

Result: demonstrates effective video segmentation accuracy and strong pedagogical alignment across knowledge, method, action, and interaction layers. Ablation studies confirm the necessity of each component in generating effective guidance.

Conclusion: This work advances AI-powered tutoring by bridging structured student modeling with interactive AI conversations, offering a scalable approach to enhancing video-based programming education.

Abstract: We introduce CogGen, a learner-centered AI architecture that transforms
programming videos into interactive, adaptive learning experiences by
integrating student modeling with generative AI tutoring based on the Cognitive
Apprenticeship framework. The architecture consists of three components: (1)
video segmentation by learning goals, (2) a conversational tutoring engine
applying Cognitive Apprenticeship strategies, and (3) a student model using
Bayesian Knowledge Tracing to adapt instruction. Our technical evaluation
demonstrates effective video segmentation accuracy and strong pedagogical
alignment across knowledge, method, action, and interaction layers. Ablation
studies confirm the necessity of each component in generating effective
guidance. This work advances AI-powered tutoring by bridging structured student
modeling with interactive AI conversations, offering a scalable approach to
enhancing video-based programming education.

</details>


### [64] [AI Assistants to Enhance and Exploit the PETSc Knowledge Base](https://arxiv.org/abs/2506.20608)
*Barry Smith,Junchao Zhang,Hong Zhang,Lois Curfman McInnes,Murat Keceli,Archit Vasan,Satish Balay,Toby Isaac,Le Chen,Venkatram Vishwanath*

Main category: cs.AI

TL;DR: The PETSc team is building an LLM-powered system to improve access to and utilization of their knowledge base, aiming to enhance software development and scientific discovery.


<details>
  <summary>Details</summary>
Motivation: Much of PETSc's knowledge remains informal and inaccessible to users and new developers.

Method: The PETSc team is building an LLM-powered system that combines PETSc content with custom LLM tools, including retrieval-augmented generation (RAG), reranking algorithms, and chatbots.

Result: The paper presents initial experiences designing and evaluating these tools, focusing on system architecture, using RAG and reranking for PETSc-specific information, evaluation methodologies, and user interface design. LLM responses can enhance the development and use of numerical software.

Conclusion: The paper outlines directions for expanding the LLM-powered system into a robust platform for advancing software ecosystems and accelerating scientific discovery.

Abstract: Generative AI, especially through large language models (LLMs), is
transforming how technical knowledge can be accessed, reused, and extended.
PETSc, a widely used numerical library for high-performance scientific
computing, has accumulated a rich but fragmented knowledge base over its three
decades of development, spanning source code, documentation, mailing lists,
GitLab issues, Discord conversations, technical papers, and more. Much of this
knowledge remains informal and inaccessible to users and new developers. To
activate and utilize this knowledge base more effectively, the PETSc team has
begun building an LLM-powered system that combines PETSc content with custom
LLM tools -- including retrieval-augmented generation (RAG), reranking
algorithms, and chatbots -- to assist users, support developers, and propose
updates to formal documentation. This paper presents initial experiences
designing and evaluating these tools, focusing on system architecture, using
RAG and reranking for PETSc-specific information, evaluation methodologies for
various LLMs and embedding models, and user interface design. Leveraging the
Argonne Leadership Computing Facility resources, we analyze how LLM responses
can enhance the development and use of numerical software, with an initial
focus on scalable Krylov solvers. Our goal is to establish an extensible
framework for knowledge-centered AI in scientific software, enabling scalable
support, enriched documentation, and enhanced workflows for research and
development. We conclude by outlining directions for expanding this system into
a robust, evolving platform that advances software ecosystems to accelerate
scientific discovery.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [65] [Near Data Processing in Taurus Database](https://arxiv.org/abs/2506.20010)
*Shu Lin,Arunprasad P. Marathe,Per-Ȧke Larson,Chong Chen,Calvin Sun,Paul Lee,Weidong Yu*

Main category: cs.DB

TL;DR: 华为GaussDB for MySQL (Taurus) 通过近数据处理 (NDP) 减少数据传输和CPU使用，从而提高查询性能。


<details>
  <summary>Details</summary>
Motivation: 将数据缩减操作（选择、投影和聚合）推送到靠近存储的位置可以减少网络传输的数据量，释放计算层的CPU容量，并缩短查询运行时间。

Method: 在华为云原生数据库系统GaussDB for MySQL (Taurus) 中设计和实现了近数据处理 (NDP)。

Result: TPCH基准测试（100 GB）表明，22个查询中有18个受益于NDP；数据传输量减少了63％；CPU时间减少了50％。在Q15上，影响甚至更高：数据传输量减少了98％；CPU时间减少了91％；运行时间减少了80％。

Conclusion: NDP可以显著减少数据传输量，降低CPU时间，并缩短查询运行时间，从而提高系统吞吐量。

Abstract: Huawei's cloud-native database system GaussDB for MySQL (also known as
Taurus) stores data in a separate storage layer consisting of a pool of storage
servers. Each server has considerable compute power making it possible to push
data reduction operations (selection, projection, and aggregation) close to
storage. This paper describes the design and implementation of near data
processing (NDP) in Taurus. NDP has several benefits: it reduces the amount of
data shipped over the network; frees up CPU capacity in the compute layer; and
reduces query run time, thereby enabling higher system throughput. Experiments
with the TPCH benchmark (100 GB) showed that 18 out of 22 queries benefited
from NDP; data shipped was reduced by 63 percent; and CPU time by 50 percent.
On Q15 the impact was even higher: data shipped was reduced by 98 percent; CPU
time by 91 percent; and run time by 80 percent.

</details>


### [66] [Piecewise Linear Approximation in Learned Index Structures: Theoretical and Empirical Analysis](https://arxiv.org/abs/2506.20139)
*Jiayong Qin,Xianyu Zhu,Qiyu Liu,Guangyi Zhang,Zhigang Cai,Jianwei Liao,Sha Hu,Jingshu Peng,Yingxia Shao,Lei Chen*

Main category: cs.DB

TL;DR: This paper revisits error-bounded Piecewise Linear Approximation ($\\&epsilon$-PLA) for learned indexes, providing a comprehensive benchmark and actionable guidelines for future designs.


<details>
  <summary>Details</summary>
Motivation: Conventional index structures, such as B+-trees, are being augmented with machine learning (ML) models, and error-bounded Piecewise Linear Approximation ($\\&epsilon$-PLA) has emerged as a popular choice. However, the design and analysis of $\epsilon$-PLA fitting algorithms remain underexplored.

Method: The paper revisits $\epsilon$-PLA from both theoretical and empirical perspectives, establishing a fundamentally improved lower bound of $\Omega(\kappa 
obreakdash \cdot \epsilon^2)$ on the expected segment coverage for existing $\epsilon$-PLA fitting algorithms and presenting a comprehensive benchmark of state-of-the-art $\epsilon$-PLA algorithms.

Result: The paper establishes a fundamentally improved lower bound of $\Omega(\kappa \cdot \epsilon^2)$ on the expected segment coverage for existing $\epsilon$-PLA fitting algorithms and highlights key trade-offs among model accuracy, model size, and query performance.

Conclusion: This paper provides actionable guidelines for the principled design of future learned data structures by highlighting key trade-offs among model accuracy, model size, and query performance.

Abstract: A growing trend in the database and system communities is to augment
conventional index structures, such as B+-trees, with machine learning (ML)
models. Among these, error-bounded Piecewise Linear Approximation
($\epsilon$-PLA) has emerged as a popular choice due to its simplicity and
effectiveness. Despite its central role in many learned indexes, the design and
analysis of $\epsilon$-PLA fitting algorithms remain underexplored. In this
paper, we revisit $\epsilon$-PLA from both theoretical and empirical
perspectives, with a focus on its application in learned index structures. We
first establish a fundamentally improved lower bound of $\Omega(\kappa \cdot
\epsilon^2)$ on the expected segment coverage for existing $\epsilon$-PLA
fitting algorithms, where $\kappa$ is a data-dependent constant. We then
present a comprehensive benchmark of state-of-the-art $\epsilon$-PLA algorithms
when used in different learned data structures. Our results highlight key
trade-offs among model accuracy, model size, and query performance, providing
actionable guidelines for the principled design of future learned data
structures.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [67] [CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender Systems](https://arxiv.org/abs/2506.19993)
*Haochen Zhang,Tianyi Zhang,Junze Yin,Oren Gal,Anshumali Shrivastava,Vladimir Braverman*

Main category: cs.IR

TL;DR: This paper introduces CoVE, a novel recommendation system that leverages the sequence understanding abilities of LLMs by assigning unique IDs to items within an expanded vocabulary and compressing the embedding layer for practical large-scale use.


<details>
  <summary>Details</summary>
Motivation: Existing approaches that focus on aligning LLMs with recommendation tasks do not fully leverage their sequential information processing capabilities, leading to suboptimal performance.

Method: In CoVE, each item is assigned a unique ID within the expanded vocabulary. Our framework effectively capitalizes on sequence understanding abilities of LLMs, significantly enhancing their performance on recommendation tasks. Additionally, we compress the embedding layer, making CoVE practical for large-scale industrial applications.

Result: significantly enhancing their performance on recommendation tasks

Conclusion: The effectiveness and performance of CoVE are demonstrated through comprehensive experiments on multiple recommendation datasets and comparisons with prior works.

Abstract: Recommender systems play a pivotal role in providing relevant content to
users. With the rapid development of large language models (LLMs), researchers
have begun utilizing LLMs to build more powerful recommender systems. However,
existing approaches that focus on aligning LLMs with recommendation tasks do
not fully leverage their sequential information processing capabilities,
leading to suboptimal performance.
  In this paper, we propose a novel system called compressed vocabulary
expansion (CoVE). In CoVE, each item is assigned a unique ID within the
expanded vocabulary. Our framework effectively capitalizes on sequence
understanding abilities of LLMs, significantly enhancing their performance on
recommendation tasks. Additionally, we compress the embedding layer, making
CoVE practical for large-scale industrial applications. The effectiveness and
performance of CoVE are demonstrated through comprehensive experiments on
multiple recommendation datasets and comparisons with prior works. Our code can
be found at https://github.com/HaochenZhang717/CoVE-official-Repo.

</details>


### [68] [Controlled Retrieval-augmented Context Evaluation for Long-form RAG](https://arxiv.org/abs/2506.20051)
*Jia-Huei Ju,Suzan Verberne,Maarten de Rijke,Andrew Yates*

Main category: cs.IR

TL;DR: CRUX是一个用于评估RAG检索模块的框架，它使用人工编写的摘要来控制知识的信息范围，并使用基于问题的评估来细粒度地评估RAG的检索。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）通过结合从外部知识源检索到的上下文来增强大型语言模型。虽然检索模块的有效性通常使用基于相关性的排序指标来评估，但这些指标可能不足以反映检索对最终RAG结果的影响，尤其是在长格式生成场景中。我们认为，提供全面的检索增强上下文对于报告生成等长格式RAG任务非常重要，并提出了用于评估独立于生成的上下文的指标。

Method: 我们引入了CRUX，这是一个旨在直接评估检索增强上下文的受控检索增强上下文评估框架。该框架使用人工编写的摘要来控制知识的信息范围，使我们能够衡量上下文在多大程度上涵盖了长格式生成必不可少的信息。CRUX使用基于问题的评估来细粒度地评估RAG的检索。

Result: 实证结果表明，CRUX提供更具反思性和诊断性的评估。我们的研究结果还表明，当前的检索方法仍有很大的改进空间，为推进RAG的检索指明了有希望的方向。

Conclusion: CRUX提供更具反思性和诊断性的评估。研究结果表明，当前的检索方法仍有很大的改进空间，为推进RAG的检索指明了有希望的方向。我们的数据和代码是公开的，以支持和推进未来对检索的研究。

Abstract: Retrieval-augmented generation (RAG) enhances large language models by
incorporating context retrieved from external knowledge sources. While the
effectiveness of the retrieval module is typically evaluated with
relevance-based ranking metrics, such metrics may be insufficient to reflect
the retrieval's impact on the final RAG result, especially in long-form
generation scenarios. We argue that providing a comprehensive
retrieval-augmented context is important for long-form RAG tasks like report
generation and propose metrics for assessing the context independent of
generation. We introduce CRUX, a \textbf{C}ontrolled
\textbf{R}etrieval-a\textbf{U}gmented conte\textbf{X}t evaluation framework
designed to directly assess retrieval-augmented contexts. This framework uses
human-written summaries to control the information scope of knowledge, enabling
us to measure how well the context covers information essential for long-form
generation. CRUX uses question-based evaluation to assess RAG's retrieval in a
fine-grained manner. Empirical results show that CRUX offers more reflective
and diagnostic evaluation. Our findings also reveal substantial room for
improvement in current retrieval methods, pointing to promising directions for
advancing RAG's retrieval. Our data and code are publicly available to support
and advance future research on retrieval.

</details>


### [69] [Multimodal Information Retrieval for Open World with Edit Distance Weak Supervision](https://arxiv.org/abs/2506.20070)
*KMA Solaiman,Bharat Bhargava*

Main category: cs.IR

TL;DR: FemmIR is a framework to retrieve multimodal results relevant to information needs expressed with multimodal queries by example without any similarity label, re-using the pretrained encoders in large language models and vision tasks. A new dataset called MuQNOL is curated for benchmarking progress on this task. FemmIR performs comparably to similar retrieval systems.


<details>
  <summary>Details</summary>
Motivation: Existing multi-media retrieval models either rely on creating a common subspace with modality-specific representation models or require schema mapping among modalities to measure similarities among multi-media data. The goal is to avoid the annotation overhead incurred from considering retrieval as a supervised classification task and re-use the pretrained encoders in large language models and vision tasks. Identification is necessary for real-world applications where data annotations are scarce and satisfactory performance is required without fine-tuning with a common framework across applications.

Method: FemmIR re-uses the high-level properties and maintains the property value and relationship constraints with a multi-level interaction score between data samples and the query example provided by the user. The technique is based on weak supervision introduced through edit distance between samples: graph edit distance can be modified to consider the cost of replacing a data sample in terms of its properties, and relevance can be measured through the implicit signal from the amount of edit cost among the objects.

Result: FemmIR performs comparably to similar retrieval systems in delivering on-demand retrieval results with exact and approximate similarities while using the existing property identifiers in the system. A new dataset called MuQNOL is curated for benchmarking progress on this task.

Conclusion: FemmIR performs comparably to similar retrieval systems in delivering on-demand retrieval results with exact and approximate similarities while using the existing property identifiers in the system.

Abstract: Existing multi-media retrieval models either rely on creating a common
subspace with modality-specific representation models or require schema mapping
among modalities to measure similarities among multi-media data. Our goal is to
avoid the annotation overhead incurred from considering retrieval as a
supervised classification task and re-use the pretrained encoders in large
language models and vision tasks. We propose "FemmIR", a framework to retrieve
multimodal results relevant to information needs expressed with multimodal
queries by example without any similarity label. Such identification is
necessary for real-world applications where data annotations are scarce and
satisfactory performance is required without fine-tuning with a common
framework across applications. We curate a new dataset called MuQNOL for
benchmarking progress on this task. Our technique is based on weak supervision
introduced through edit distance between samples: graph edit distance can be
modified to consider the cost of replacing a data sample in terms of its
properties, and relevance can be measured through the implicit signal from the
amount of edit cost among the objects. Unlike metric learning or encoding
networks, FemmIR re-uses the high-level properties and maintains the property
value and relationship constraints with a multi-level interaction score between
data samples and the query example provided by the user. We empirically
evaluate FemmIR on a missing person use case with MuQNOL. FemmIR performs
comparably to similar retrieval systems in delivering on-demand retrieval
results with exact and approximate similarities while using the existing
property identifiers in the system.

</details>


### [70] [Semantic-enhanced Modality-asymmetric Retrieval for Online E-commerce Search](https://arxiv.org/abs/2506.20330)
*Zhigong Zhou,Ning Ding,Xiaochuan Fan,Yue Shang,Yiming Qiu,Jingwei Zhuo,Zhiwei Ge,Songlin Wang,Lin Liu,Sulong Xu,Han Zhang*

Main category: cs.IR

TL;DR: This paper introduces SMAR, a new model for multimodal retrieval that improves retrieval accuracy by fusing textual and visual information in an asymmetric scenario. The model outperforms existing methods on an industrial dataset, which has been made publicly available.


<details>
  <summary>Details</summary>
Motivation: Multimodal retrieval remains a non-trivial and unsolved problem especially in the asymmetric scenario where the query is unimodal while the item is multimodal.

Method: A novel model named SMAR (Semantic-enhanced Modality-Asymmetric Retrieval) is proposed to tackle the problem of modality fusion and alignment in an asymmetric scenario where the query is unimodal while the item is multimodal.

Result: Extensive experimental results on an industrial dataset show that the proposed model outperforms baseline models significantly in retrieval accuracy. The industrial dataset is open sourced.

Conclusion: The proposed SMAR model outperforms baseline models significantly in retrieval accuracy on an industrial dataset.

Abstract: Semantic retrieval, which retrieves semantically matched items given a
textual query, has been an essential component to enhance system effectiveness
in e-commerce search. In this paper, we study the multimodal retrieval problem,
where the visual information (e.g, image) of item is leveraged as supplementary
of textual information to enrich item representation and further improve
retrieval performance. Though learning from cross-modality data has been
studied extensively in tasks such as visual question answering or media
summarization, multimodal retrieval remains a non-trivial and unsolved problem
especially in the asymmetric scenario where the query is unimodal while the
item is multimodal. In this paper, we propose a novel model named SMAR, which
stands for Semantic-enhanced Modality-Asymmetric Retrieval, to tackle the
problem of modality fusion and alignment in this kind of asymmetric scenario.
Extensive experimental results on an industrial dataset show that the proposed
model outperforms baseline models significantly in retrieval accuracy. We have
open sourced our industrial dataset for the sake of reproducibility and future
research works.

</details>


### [71] [Unidentified and Confounded? Understanding Two-Tower Models for Unbiased Learning to Rank](https://arxiv.org/abs/2506.20501)
*Philipp Hager,Onno Zoeter,Maarten de Rijke*

Main category: cs.IR

TL;DR: 双塔模型在生产系统中用点击数据训练会导致排名性能下降。本文研究了日志记录策略和模型可识别性问题，并提出了一种样本权重技术来缓解这些影响。


<details>
  <summary>Details</summary>
Motivation: 在工业环境中，加性双塔模型是处理有偏差的用户反馈的流行的学习排序方法。然而，最近的研究报告了一种令人担忧的现象：在表现良好的生产系统收集的点击数据上训练双塔模型会导致排名性能下降。

Method: 理论分析了双塔模型的可识别性条件，表明需要跨位置的文档交换或重叠的特征分布才能从点击中恢复模型参数。我们还研究了日志记录策略对双塔模型的影响，发现当模型完美地捕捉用户行为时，它们不会引入偏差。然而，当模型不完美地捕捉用户行为时，日志记录策略会放大偏差，特别是当预测误差与跨位置的文档放置相关时。

Result: 表明需要跨位置的文档交换或重叠的特征分布才能从点击中恢复模型参数。我们还研究了日志记录策略对双塔模型的影响，发现当模型完美地捕捉用户行为时，它们不会引入偏差。然而，当模型不完美地捕捉用户行为时，日志记录策略会放大偏差，特别是当预测误差与跨位置的文档放置相关时。

Conclusion: 提出了一种样本权重技术来缓解这些影响，并为使用双塔模型的研究人员和从业人员提供了可操作的见解。

Abstract: Additive two-tower models are popular learning-to-rank methods for handling
biased user feedback in industry settings. Recent studies, however, report a
concerning phenomenon: training two-tower models on clicks collected by
well-performing production systems leads to decreased ranking performance. This
paper investigates two recent explanations for this observation: confounding
effects from logging policies and model identifiability issues. We
theoretically analyze the identifiability conditions of two-tower models,
showing that either document swaps across positions or overlapping feature
distributions are required to recover model parameters from clicks. We also
investigate the effect of logging policies on two-tower models, finding that
they introduce no bias when models perfectly capture user behavior. However,
logging policies can amplify biases when models imperfectly capture user
behavior, particularly when prediction errors correlate with document placement
across positions. We propose a sample weighting technique to mitigate these
effects and provide actionable insights for researchers and practitioners using
two-tower models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [72] [Position: Machine Learning Conferences Should Establish a "Refutations and Critiques" Track](https://arxiv.org/abs/2506.19882)
*Rylan Schaeffer,Joshua Kazdan,Yegor Denisov-Blanch,Brando Miranda,Matthias Gerstgrasser,Susan Zhang,Andreas Haupt,Isha Gupta,Elyas Obbad,Jesse Dodge,Jessica Zosa Forde,Koustuv Sinha,Francesco Orabona,Sanmi Koyejo,David Donoho*

Main category: cs.LG

TL;DR: ML conferences should establish a dedicated "Refutations and Critiques" (R & C) Track to foster a dynamic self-correcting research ecosystem.


<details>
  <summary>Details</summary>
Motivation: ML conferences do not offer robust processes to help the field systematically correct when such errors are made.

Method: ML conferences should establish a dedicated "Refutations and Critiques" (R & C) Track.

Result: This R & C Track would provide a high-profile, reputable platform to support vital research that critically challenges prior research, thereby fostering a dynamic self-correcting research ecosystem.

Conclusion: ML conferences should create official, reputable mechanisms to help ML research self-correct.

Abstract: Science progresses by iteratively advancing and correcting humanity's
understanding of the world. In machine learning (ML) research, rapid
advancements have led to an explosion of publications, but have also led to
misleading, incorrect, flawed or perhaps even fraudulent studies being accepted
and sometimes highlighted at ML conferences due to the fallibility of peer
review. While such mistakes are understandable, ML conferences do not offer
robust processes to help the field systematically correct when such errors are
made.This position paper argues that ML conferences should establish a
dedicated "Refutations and Critiques" (R & C) Track. This R & C Track would
provide a high-profile, reputable platform to support vital research that
critically challenges prior research, thereby fostering a dynamic
self-correcting research ecosystem. We discuss key considerations including
track design, review principles, potential pitfalls, and provide an
illustrative example submission concerning a recent ICLR 2025 Oral. We conclude
that ML conferences should create official, reputable mechanisms to help ML
research self-correct.

</details>


### [73] [STIMULUS: Achieving Fast Convergence and Low Sample Complexity in Stochastic Multi-Objective Learning](https://arxiv.org/abs/2506.19883)
*Zhuqing Liu,Chaosheng Dong,Michinari Momma,Simone Shao,Shaoyuan Xu,Yan Gao,Haibo Yang,Jia Liu*

Main category: cs.LG

TL;DR: This paper introduces STIMULUS, a new MOO algorithm with improved convergence and sample complexity, along with enhanced versions STIMULUS-M, STIMULUS+, and STIMULUS-M+.


<details>
  <summary>Details</summary>
Motivation: Existing multi-objective optimization (MOO) methods suffer from unsatisfactory convergence rate and sample complexity performance.

Method: The paper proposes an algorithm called STIMULUS (stochastic path-integrated multi-gradient recursive estimator), a recursive framework for updating stochastic gradient estimates. An enhanced version, STIMULUS-M, incorporates a momentum term. Adaptive batching versions, STIMULUS+ and STIMULUS-M+, are also proposed.

Result: The proposed methods achieve improved convergence performance with low sample complexity. Enhanced versions with adaptive batching alleviate the periodic full gradient evaluation requirement.

Conclusion: This paper establishes convergence rates of O(1/T) for non-convex settings and O(exp{-μT}) for strongly convex settings. It also achieves state-of-the-art sample complexities of O(n+√nϵ^{-1}) for non-convex settings and O(n+ √n ln(μ/ϵ)) for strongly convex settings.

Abstract: Recently, multi-objective optimization (MOO) has gained attention for its
broad applications in ML, operations research, and engineering. However, MOO
algorithm design remains in its infancy and many existing MOO methods suffer
from unsatisfactory convergence rate and sample complexity performance. To
address this challenge, in this paper, we propose an algorithm called STIMULUS(
stochastic path-integrated multi-gradient recursive e\ulstimator), a new and
robust approach for solving MOO problems. Different from the traditional
methods, STIMULUS introduces a simple yet powerful recursive framework for
updating stochastic gradient estimates to improve convergence performance with
low sample complexity. In addition, we introduce an enhanced version of
STIMULUS, termed STIMULUS-M, which incorporates a momentum term to further
expedite convergence. We establish $O(1/T)$ convergence rates of the proposed
methods for non-convex settings and $O (\exp{-\mu T})$ for strongly convex
settings, where $T$ is the total number of iteration rounds. Additionally, we
achieve the state-of-the-art $O \left(n+\sqrt{n}\epsilon^{-1}\right)$ sample
complexities for non-convex settings and $O\left(n+ \sqrt{n} \ln
({\mu/\epsilon})\right)$ for strongly convex settings, where $\epsilon>0$ is a
desired stationarity error. Moreover, to alleviate the periodic full gradient
evaluation requirement in STIMULUS and STIMULUS-M, we further propose enhanced
versions with adaptive batching called STIMULUS+/ STIMULUS-M+ and provide their
theoretical analysis.

</details>


### [74] [LSH-DynED: A Dynamic Ensemble Framework with LSH-Based Undersampling for Evolving Multi-Class Imbalanced Classification](https://arxiv.org/abs/2506.20041)
*Soheil Abadifard,Fazli Can*

Main category: cs.LG

TL;DR: LSH-DynED是一种用于处理多类不平衡数据流的新方法，它通过LSH-RHP欠采样多数类，并在实际环境中表现出良好的性能。


<details>
  <summary>Details</summary>
Motivation: 多类不平衡数据流的分类是机器学习中的一个关键难点，尤其是在处理多个类别时。有效管理动态不平衡比率是该领域的一个关键挑战。

Method: 该研究将局部敏感哈希与随机超平面投影（LSH-RHP）集成到动态集成多样化（DynED）框架中，提出了一种新颖、稳健且有弹性的方法。利用LSH-RHP对多数类进行欠采样，提供平衡的训练集。

Result: LSH-DynED在Kappa和mG-Mean有效性指标方面优于其他方法，证明了其在处理多类不平衡非平稳数据流方面的能力。

Conclusion: LSH-DynED在处理多类不平衡非平稳数据流方面表现出色，尤其是在大规模、高维度、具有显著类不平衡的数据集中，并展示了在实际环境中的适应性和鲁棒性。

Abstract: The classification of imbalanced data streams, which have unequal class
distributions, is a key difficulty in machine learning, especially when dealing
with multiple classes. While binary imbalanced data stream classification tasks
have received considerable attention, only a few studies have focused on
multi-class imbalanced data streams. Effectively managing the dynamic imbalance
ratio is a key challenge in this domain. This study introduces a novel, robust,
and resilient approach to address these challenges by integrating Locality
Sensitive Hashing with Random Hyperplane Projections (LSH-RHP) into the Dynamic
Ensemble Diversification (DynED) framework. To the best of our knowledge, we
present the first application of LSH-RHP for undersampling in the context of
imbalanced non-stationary data streams. The proposed method undersamples the
majority classes by utilizing LSH-RHP, provides a balanced training set, and
improves the ensemble's prediction performance. We conduct comprehensive
experiments on 23 real-world and ten semi-synthetic datasets and compare
LSH-DynED with 15 state-of-the-art methods. The results reveal that LSH-DynED
outperforms other approaches in terms of both Kappa and mG-Mean effectiveness
measures, demonstrating its capability in dealing with multi-class imbalanced
non-stationary data streams. Notably, LSH-DynED performs well in large-scale,
high-dimensional datasets with considerable class imbalances and demonstrates
adaptation and robustness in real-world circumstances. To motivate our design,
we review existing methods for imbalanced data streams, outline key challenges,
and offer guidance for future work. For the reproducibility of our results, we
have made our implementation available on GitHub.

</details>


### [75] [DIM-SUM: Dynamic IMputation for Smart Utility Management](https://arxiv.org/abs/2506.20023)
*Ryan Hildebrant,Rahul Bhope,Sharad Mehrotra,Christopher Tull,Nalini Venkatasubramanian*

Main category: cs.LG

TL;DR: DIM-SUM is a preprocessing framework for training robust imputation models that bridges the gap between artificially masked training data and real missing patterns.


<details>
  <summary>Details</summary>
Motivation: Practitioners often encounter datasets where large amounts of data are missing and follow complex, heterogeneous patterns.

Method: DIM-SUM combines pattern clustering and adaptive masking strategies with theoretical learning guarantees to handle diverse missing patterns actually observed in the data.

Result: DIM-SUM outperforms traditional methods by reaching similar accuracy with lower processing time and significantly less training data. When compared against a large pre-trained model, DIM-SUM averages 2x higher accuracy with significantly less inference time.

Conclusion: DIM-SUM outperforms traditional methods by reaching similar accuracy with lower processing time and significantly less training data. When compared against a large pre-trained model, DIM-SUM averages 2x higher accuracy with significantly less inference time.

Abstract: Time series imputation models have traditionally been developed using
complete datasets with artificial masking patterns to simulate missing values.
However, in real-world infrastructure monitoring, practitioners often encounter
datasets where large amounts of data are missing and follow complex,
heterogeneous patterns. We introduce DIM-SUM, a preprocessing framework for
training robust imputation models that bridges the gap between artificially
masked training data and real missing patterns. DIM-SUM combines pattern
clustering and adaptive masking strategies with theoretical learning guarantees
to handle diverse missing patterns actually observed in the data. Through
extensive experiments on over 2 billion readings from California water
districts, electricity datasets, and benchmarks, we demonstrate that DIM-SUM
outperforms traditional methods by reaching similar accuracy with lower
processing time and significantly less training data. When compared against a
large pre-trained model, DIM-SUM averages 2x higher accuracy with significantly
less inference time.

</details>


### [76] [FlightKooba: A Fast Interpretable FTP Model](https://arxiv.org/abs/2506.19885)
*Jing Lu,Xuan Wu,Yizhun Tian,Songhan Fan,Yali Fang*

Main category: cs.LG

TL;DR: proposes FlightKooba, a new modeling and control framework based on the HIPPO method, the Koopman theory, and state space equations from cybernetics to address the issue that current models applying the Koopman theory to FTP tasks are not very effective, model interpretability is indeed an issue, and the Koopman operators are computationally intensive, resulting in long training times


<details>
  <summary>Details</summary>
Motivation: current models applying the Koopman theory to FTP tasks are not very effective, model interpretability is indeed an issue, and the Koopman operators are computationally intensive, resulting in long training times

Method: a new modeling and control framework based on the HIPPO method, the Koopman theory, and state space equations from cybernetics: FlightKooba. Inspired by the idea of structural state space equations, FlightKooba directly constructs the Koopman operators from data.

Result: Experiments have demonstrated the superiority of the FlightKooba modeling method in terms of time and memory consumption (training time comparable to the Mamba module without using CUDA-level acceleration; memory reduced by more than 50% on most datasets, with a tenfold reduction in the number of parameters), essentially completing the FTP task.

Conclusion: It provides a new method for the fast computation of the Koopman operators, opening up new possibilities for the combination of time series forecasting and control.

Abstract: The Koopman theory is a powerful and effective modeling tool for converting
nonlinear systems into linear representations, and flight trajectory prediction
(FTP) is a complex nonlinear system. However, current models applying the
Koopman theory to FTP tasks are not very effective, model interpretability is
indeed an issue, and the Koopman operators are computationally intensive,
resulting in long training times. To address this issue, this paper proposes a
new modeling and control framework based on the HIPPO method, the Koopman
theory, and state space equations from cybernetics: FlightKooba. Inspired by
the idea of structural state space equations, FlightKooba directly constructs
the Koopman operators from data. This makes the framework highly interpretable
and significantly reduces the number of trainable parameters in the module,
thereby greatly reducing training time. Experiments have demonstrated the
superiority of the FlightKooba modeling method in terms of time and memory
consumption (training time comparable to the Mamba module without using
CUDA-level acceleration; memory reduced by more than 50% on most datasets, with
a tenfold reduction in the number of parameters), essentially completing the
FTP task. It provides a new method for the fast computation of the Koopman
operators, opening up new possibilities for the combination of time series
forecasting and control.

</details>


### [77] [A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior](https://arxiv.org/abs/2506.19999)
*Francesco Ignazio Re,Andreas Opedal,Glib Manaiev,Mario Giulianelli,Ryan Cotterell*

Main category: cs.LG

TL;DR: propose a more general probabilistic model of reading behavior, based on a marked spatio-temporal point process, that captures not only how long fixations last, but also where they land in space and when they take place in time


<details>
  <summary>Details</summary>
Motivation: modeling a reader's fixations and saccades yields insight into their online sentence processing. However, standard approaches to such modeling rely on aggregated eye-tracking measurements and models that impose strong assumptions, ignoring much of the spatio-temporal dynamics that occur during reading

Method: a more general probabilistic model of reading behavior, based on a marked spatio-temporal point process

Result: Hawkes process model exhibits a better fit to human saccades than baselines

Conclusion: incorporating contextual surprisal as a predictor results in only a marginal improvement in the model's predictive accuracy. This finding suggests that surprisal theory struggles to explain fine-grained eye movements.

Abstract: Reading is a process that unfolds across space and time, alternating between
fixations where a reader focuses on a specific point in space, and saccades
where a reader rapidly shifts their focus to a new point. An ansatz of
psycholinguistics is that modeling a reader's fixations and saccades yields
insight into their online sentence processing. However, standard approaches to
such modeling rely on aggregated eye-tracking measurements and models that
impose strong assumptions, ignoring much of the spatio-temporal dynamics that
occur during reading. In this paper, we propose a more general probabilistic
model of reading behavior, based on a marked spatio-temporal point process,
that captures not only how long fixations last, but also where they land in
space and when they take place in time. The saccades are modeled using a Hawkes
process, which captures how each fixation excites the probability of a new
fixation occurring near it in time and space. The duration time of fixation
events is modeled as a function of fixation-specific predictors convolved
across time, thus capturing spillover effects. Empirically, our Hawkes process
model exhibits a better fit to human saccades than baselines. With respect to
fixation durations, we observe that incorporating contextual surprisal as a
predictor results in only a marginal improvement in the model's predictive
accuracy. This finding suggests that surprisal theory struggles to explain
fine-grained eye movements.

</details>


### [78] [Causal-Aware Intelligent QoE Optimization for VR Interaction with Adaptive Keyframe Extraction](https://arxiv.org/abs/2506.19890)
*Ziru Zhang,Jiadong Yu,Danny H. K. Tsang*

Main category: cs.LG

TL;DR: This paper introduces an intelligent framework that maximizes QoE in multi-user VR by integrating adaptive keyframe extraction with causal-aware reinforcement learning. Experiments show it reduces latency, enhances QoE, and maintains fairness.


<details>
  <summary>Details</summary>
Motivation: The optimization of quality of experience (QoE) in multi-user virtual reality (VR) interactions demands a delicate balance between ultra-low latency, high-fidelity motion synchronization, and equitable resource allocation. Existing approaches often overlook the causal relationships among allocated bandwidth, CPU frequency, and user perception, limiting QoE gains.

Method: The paper proposes Partial State Causal Deep Deterministic Policy Gradient (PS-CDDPG), which integrates the Deep Deterministic Policy Gradient (DDPG) method with causal influence detection. The QoE optimization problem is modeled as a mixed integer programming (MIP) task, jointly optimizing keyframe ratios, bandwidth, and computational resources under horizon-fairness constraints.

Result: The framework significantly reduces interactive latency, enhances QoE, and maintains fairness, achieving superior performance compared to benchmark methods.

Conclusion: The proposed framework significantly reduces interactive latency, enhances QoE, and maintains fairness, achieving superior performance compared to benchmark methods.

Abstract: The optimization of quality of experience (QoE) in multi-user virtual reality
(VR) interactions demands a delicate balance between ultra-low latency,
high-fidelity motion synchronization, and equitable resource allocation. While
adaptive keyframe extraction mitigates transmission overhead, existing
approaches often overlook the causal relationships among allocated bandwidth,
CPU frequency, and user perception, limiting QoE gains. This paper proposes an
intelligent framework to maximize QoE by integrating adaptive keyframe
extraction with causal-aware reinforcement learning (RL). First, a novel QoE
metric is formulated using the Weber-Fechner Law, combining perceptual
sensitivity, attention-driven priorities, and motion reconstruction accuracy.
The QoE optimization problem is then modeled as a mixed integer programming
(MIP) task, jointly optimizing keyframe ratios, bandwidth, and computational
resources under horizon-fairness constraints. We propose Partial State Causal
Deep Deterministic Policy Gradient (PS-CDDPG), which integrates the Deep
Deterministic Policy Gradient (DDPG) method with causal influence detection. By
leveraging causal information regarding how QoE is influenced and determined by
various actions, we explore actions guided by weights calculated from causal
inference (CI), which in turn improves training efficiency. Experiments
conducted with the CMU Motion Capture Database demonstrate that our framework
significantly reduces interactive latency, enhances QoE, and maintains
fairness, achieving superior performance compared to benchmark methods.

</details>


### [79] [Orthogonal Soft Pruning for Efficient Class Unlearning](https://arxiv.org/abs/2506.19891)
*Qinghui Gong,Xue Yang,Xiaohu Tang*

Main category: cs.LG

TL;DR: 提出了一种基于正交卷积核正则化的类感知软剪枝框架，用于快速而精确的机器卸载，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 机器卸载旨在有选择地从预训练的神经网络中删除特定于类的知识，以满足 GDPR 等隐私法规。现有方法通常面临卸载速度和预测精度保持之间的权衡，通常会产生高计算开销或保留类别的显着性能下降。

Method: 提出了一种新的类感知软剪枝框架，该框架利用正交卷积核正则化来实现快速而精确的遗忘，响应时间达到毫秒级。

Result: 在多个架构和数据集上进行的大量评估表明，该方法具有稳定的剪枝和接近瞬时的执行速度，可以完全忘记目标类别，并且保留数据的精度损失最小。在 CIFAR-10、CIFAR-100 和 TinyImageNet 上的实验证实，与最先进的基线相比，该方法大大降低了成员推理攻击风险，并将卸载速度提高了几个数量级。

Conclusion: 该框架为机器学习即服务 (MLaaS) 场景中的实时机器卸载提供了一种高效、实用的解决方案。

Abstract: Machine unlearning aims to selectively remove class-specific knowledge from
pretrained neural networks to satisfy privacy regulations such as the GDPR.
Existing methods typically face a trade-off between unlearning speed and
preservation of predictive accuracy, often incurring either high computational
overhead or significant performance degradation on retained classes. In this
paper, we propose a novel class-aware soft pruning framework leveraging
orthogonal convolutional kernel regularization to achieve rapid and precise
forgetting with millisecond-level response times. By enforcing orthogonality
constraints during training, our method decorrelates convolutional filters and
disentangles feature representations, while efficiently identifying
class-specific channels through activation difference analysis. Extensive
evaluations across multiple architectures and datasets demonstrate stable
pruning with near-instant execution, complete forgetting of targeted classes,
and minimal accuracy loss on retained data. Experiments on CIFAR-10, CIFAR-100,
and TinyImageNet confirm that our approach substantially reduces membership
inference attack risks and accelerates unlearning by orders of magnitude
compared to state-of-the-art baselines. This framework provides an efficient,
practical solution for real-time machine unlearning in Machine Learning as a
Service (MLaaS) scenarios.

</details>


### [80] [Distillation-Enabled Knowledge Alignment for Generative Semantic Communications in AIGC Provisioning Tasks](https://arxiv.org/abs/2506.19893)
*Jingzhi Hu,Geoffrey Ye Li*

Main category: cs.LG

TL;DR: This paper proposes DeKA-g, a distillation-enabled knowledge alignment algorithm for GSC systems, which improves the alignment between edge and cloud generated images, adapts to compression rates more efficiently, and enhances performance in low-SNR conditions.


<details>
  <summary>Details</summary>
Motivation: Due to the surging amount of AI-generated content (AIGC), its provisioning to edges and mobile users from the cloud incurs substantial traffic on networks. Generative semantic communication (GSC) offers a promising solution by transmitting highly compact information. However, GSC relies on the alignment between the knowledge in the cloud generative AI (GAI) and that possessed by the edges and users, and between the knowledge for wireless transmission and that of actual channels, which remains challenging.

Method: DeKA-g, a distillation-enabled knowledge alignment algorithm for GSC systems, comprises two novel methods: metaword-aided knowledge distillation (MAKD) and variable-rate grouped SNR adaptation (VGSA).

Result: DeKA-g improves the alignment between the edge-generated images and the cloud-generated ones by 44%. Moreover, it adapts to compression rates with 116% higher efficiency than the baseline and enhances the performance in low-SNR conditions by 28%.

Conclusion: DeKA-g improves the alignment between the edge-generated images and the cloud-generated ones by 44%. Moreover, it adapts to compression rates with 116% higher efficiency than the baseline and enhances the performance in low-SNR conditions by 28%.

Abstract: Due to the surging amount of AI-generated content (AIGC), its provisioning to
edges and mobile users from the cloud incurs substantial traffic on networks.
Generative semantic communication (GSC) offers a promising solution by
transmitting highly compact information, i.e., prompt text and latent
representations, instead of high-dimensional AIGC data. However, GSC relies on
the alignment between the knowledge in the cloud generative AI (GAI) and that
possessed by the edges and users, and between the knowledge for wireless
transmission and that of actual channels, which remains challenging. In this
paper, we propose DeKA-g, a distillation-enabled knowledge alignment algorithm
for GSC systems. The core idea is to distill the generation knowledge from the
cloud-GAI into low-rank matrices, which can be incorporated by the edge and
used to adapt the transmission knowledge to diverse wireless channel
conditions. DeKA-g comprises two novel methods: metaword-aided knowledge
distillation (MAKD) and variable-rate grouped SNR adaptation (VGSA). For MAKD,
an optimized metaword is employed to enhance the efficiency of knowledge
distillation, while VGSA enables efficient adaptation to diverse compression
rates and SNR ranges. From simulation results, DeKA-g improves the alignment
between the edge-generated images and the cloud-generated ones by 44%.
Moreover, it adapts to compression rates with 116% higher efficiency than the
baseline and enhances the performance in low-SNR conditions by 28%.

</details>


### [81] [Explaining deep neural network models for electricity price forecasting with XAI](https://arxiv.org/abs/2506.19894)
*Antoine Pesenti,Aidan OSullivan*

Main category: cs.LG

TL;DR: Use a DNN and XAI methods to understand the factors driving the price dynamics in the market.


<details>
  <summary>Details</summary>
Motivation: The objective is to increase our understanding of how different electricity markets work.

Method: Use a DNN to forecast the price and then use XAI methods

Result: increase our understanding of how different electricity markets work

Conclusion: Use SHAP and Gradient to analyse the behaviour and contributions of various features across five electricity markets, introducing SSHAP values and SSHAP lines.

Abstract: Electricity markets are highly complex, involving lots of interactions and
complex dependencies that make it hard to understand the inner workings of the
market and what is driving prices. Econometric methods have been developed for
this, white-box models, however, they are not as powerful as deep neural
network models (DNN). In this paper, we use a DNN to forecast the price and
then use XAI methods to understand the factors driving the price dynamics in
the market. The objective is to increase our understanding of how different
electricity markets work. To do that, we apply explainable methods such as SHAP
and Gradient, combined with visual techniques like heatmaps (saliency maps) to
analyse the behaviour and contributions of various features across five
electricity markets. We introduce the novel concepts of SSHAP values and SSHAP
lines to enhance the complex representation of high-dimensional tabular models.

</details>


### [82] [A Framework for Uncertainty Quantification Based on Nearest Neighbors Across Layers](https://arxiv.org/abs/2506.19895)
*Miguel N. Font,José L. Jorro-Aragoneses,Carlos M. Alaíz*

Main category: cs.LG

TL;DR: This paper introduces a new way to measure uncertainty in neural networks, which can help detect errors in critical applications. The approach uses similar training cases to assess the reliability of a decision, showing improved performance over existing methods on CIFAR-10 and MNIST datasets.


<details>
  <summary>Details</summary>
Motivation: Neural Networks sometimes return wrong solutions, which become problematic in high-risk domains like medical diagnosis or autonomous driving. One strategy to detect and mitigate these errors is the measurement of the uncertainty over neural network decisions.

Method: a novel post-hoc framework for measuring the uncertainty of a decision based on retrieved training cases that have a similar activation vector to the query for each layer. Based on these retrieved cases, we propose two new metrics: Decision Change and Layer Uncertainty, which capture changes in nearest-neighbor class distributions across layers.

Result: We evaluated our approach in a classification model for two datasets: CIFAR-10 and MNIST.

Conclusion: The proposed metrics enhance uncertainty estimation, especially in challenging classification tasks, outperforming softmax-based confidence.

Abstract: Neural Networks have high accuracy in solving problems where it is difficult
to detect patterns or create a logical model. However, these algorithms
sometimes return wrong solutions, which become problematic in high-risk domains
like medical diagnosis or autonomous driving. One strategy to detect and
mitigate these errors is the measurement of the uncertainty over neural network
decisions. In this paper, we present a novel post-hoc framework for measuring
the uncertainty of a decision based on retrieved training cases that have a
similar activation vector to the query for each layer. Based on these retrieved
cases, we propose two new metrics: Decision Change and Layer Uncertainty, which
capture changes in nearest-neighbor class distributions across layers. We
evaluated our approach in a classification model for two datasets: CIFAR-10 and
MNIST. The results show that these metrics enhance uncertainty estimation,
especially in challenging classification tasks, outperforming softmax-based
confidence.

</details>


### [83] [MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations](https://arxiv.org/abs/2506.20100)
*Vardhan Dongre,Chi Gui,Shubham Garg,Hooshang Nayyeri,Gokhan Tur,Dilek Hakkani-Tür,Vikram S. Adve*

Main category: cs.LG

TL;DR: MIRAGE是一个新的农业领域的多模态专家级推理和决策基准，它结合了自然用户查询、专家响应和图像上下文，以评估模型在实际、知识密集型领域的推理、澄清策略和长文本生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基准依赖于明确的用户输入和封闭的分类，而MIRAGE具有不明确的、上下文丰富的场景与开放世界的设置。

Method: MIRAGE通过结合自然的用户查询、专家编写的响应和基于图像的上下文来捕获专家咨询的完整复杂性。

Result: MIRAGE跨越了多种作物健康、病虫害诊断和作物管理场景，包括7,000多种独特的生物实体。

Conclusion: MIRAGE是一个新的基准，它具有开放世界的设置，需要模型来推断潜在的知识差距，处理罕见的实体，并主动地指导交互或响应。

Abstract: We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning
and decision-making in consultative interaction settings. Designed for the
agriculture domain, MIRAGE captures the full complexity of expert consultations
by combining natural user queries, expert-authored responses, and image-based
context, offering a high-fidelity benchmark for evaluating models on grounded
reasoning, clarification strategies, and long-form generation in a real-world,
knowledge-intensive domain. Grounded in over 35,000 real user-expert
interactions and curated through a carefully designed multi-step pipeline,
MIRAGE spans diverse crop health, pest diagnosis, and crop management
scenarios. The benchmark includes more than 7,000 unique biological entities,
covering plant species, pests, and diseases, making it one of the most
taxonomically diverse benchmarks available for vision-language models, grounded
in the real world. Unlike existing benchmarks that rely on well-specified user
inputs and closed-set taxonomies, MIRAGE features underspecified, context-rich
scenarios with open-world settings, requiring models to infer latent knowledge
gaps, handle rare entities, and either proactively guide the interaction or
respond. Project Page: https://mirage-benchmark.github.io

</details>


### [84] [A Comparative Analysis of Reinforcement Learning and Conventional Deep Learning Approaches for Bearing Fault Diagnosis](https://arxiv.org/abs/2506.19929)
*Efe Çakır,Patrick Dumond*

Main category: cs.LG

TL;DR: 本研究探讨了强化学习在轴承故障分类任务中的可行性，结果表明，强化学习模型在适应性方面表现出色，但计算需求较高。


<details>
  <summary>Details</summary>
Motivation: 旋转机械中的轴承故障可能导致重大的运营中断和维护成本。用于轴承故障诊断的现代方法严重依赖振动分析和机器学习技术，这些方法通常需要大量的标记数据，并且可能无法很好地适应动态环境。

Method: 强化学习 (RL)，特别是深度 Q 网络 (DQN)

Result: 研究表明，虽然本研究中开发的强化学习模型可以在受控条件下与传统的监督学习模型相媲美，但它们在配备优化的奖励结构时，在适应性方面表现出色。

Conclusion: RL模型在受控条件下可以与传统的监督学习模型相媲美，并且在配备优化的奖励结构时，它们在适应性方面表现出色。然而，它们的计算需求突出了进一步改进的领域。这些发现证明了强化学习在补充传统方法方面的潜力，为自适应诊断框架铺平了道路。

Abstract: Bearing faults in rotating machinery can lead to significant operational
disruptions and maintenance costs. Modern methods for bearing fault diagnosis
rely heavily on vibration analysis and machine learning techniques, which often
require extensive labeled data and may not adapt well to dynamic environments.
This study explores the feasibility of reinforcement learning (RL),
specifically Deep Q-Networks (DQNs), for bearing fault classification tasks in
machine condition monitoring to enhance the accuracy and adaptability of
bearing fault diagnosis. The results demonstrate that while RL models developed
in this study can match the performance of traditional supervised learning
models under controlled conditions, they excel in adaptability when equipped
with optimized reward structures. However, their computational demands
highlight areas for further improvement. These findings demonstrate RL's
potential to complement traditional methods, paving the way for adaptive
diagnostic frameworks.

</details>


### [85] [Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture](https://arxiv.org/abs/2506.19935)
*Shuchen Xue,Tianyu Xie,Tianyang Hu,Zijin Feng,Jiacheng Sun,Kenji Kawaguchi,Zhenguo Li,Zhi-Ming Ma*

Main category: cs.LG

TL;DR: This paper fairly compares autoregressive (AR) and masked diffusion models (MDMs) by evaluating MDMs within a decoder-only framework, and investigates architectural influences within MDMs.


<details>
  <summary>Details</summary>
Motivation: A key challenge in comparing AR and MDM paradigms is their typical architectural difference: AR models are often decoder-only, while MDMs have largely been encoder-only. This practice of changing both the modeling paradigm and architecture simultaneously makes direct comparisons unfair.

Method: This research evaluates MDMs within a decoder-only framework to equitably compare MDM (as Any-Order AR, or AO-AR) and standard AR paradigms, and Investigate architectural influences (decoder-only vs. encoder-only) within MDMs.

Result: decoder-only MDMs can achieve dramatic generation speedups and comparable perplexity with temperature annealing despite modeling a vastly larger space, highlighting key trade-offs. The standard AO-AR objective, which averages over all token permutations, may benefit from refinement, as many permutations appear less informative compared to the language's inherent left-to-right structure.

Conclusion: This work decouples core paradigm differences from architectural influences, offering insights for future model design.

Abstract: Large language models (LLMs) predominantly use autoregressive (AR)
approaches, but masked diffusion models (MDMs) are emerging as viable
alternatives. A key challenge in comparing AR and MDM paradigms is their
typical architectural difference: AR models are often decoder-only, while MDMs
have largely been encoder-only. This practice of changing both the modeling
paradigm and architecture simultaneously makes direct comparisons unfair, as
it's hard to distinguish whether observed differences stem from the paradigm
itself or the architectural shift. This research evaluates MDMs within a
decoder-only framework to: (1) equitably compare MDM (as Any-Order AR, or
AO-AR) and standard AR paradigms. Our investigation suggests that the standard
AO-AR objective, which averages over all token permutations, may benefit from
refinement, as many permutations appear less informative compared to the
language's inherent left-to-right structure. (2) Investigate architectural
influences (decoder-only vs. encoder-only) within MDMs. We demonstrate that
while encoder-only MDMs model a simpler conditional probability space,
decoder-only MDMs can achieve dramatic generation speedups ($\sim25\times$) and
comparable perplexity with temperature annealing despite modeling a vastly
larger space, highlighting key trade-offs. This work thus decouples core
paradigm differences from architectural influences, offering insights for
future model design. Code is available at https://github.com/scxue/AO-GPT-MDM.

</details>


### [86] [The Most Important Features in Generalized Additive Models Might Be Groups of Features](https://arxiv.org/abs/2506.19937)
*Tomas M. Bosschieter,Luis Franca,Jessica Wolk,Yiyuan Wu,Bella Mehta,Joseph Dehoney,Orsolya Kiss,Fiona C. Baker,Qingyu Zhao,Rich Caruana,Kilian M. Pohl*

Main category: cs.LG

TL;DR: This paper introduces a novel approach to determine the importance of a group of features for Generalized Additive Models (GAMs) that is efficient and remains meaningful in high-dimensional settings. Analyzing group importance offers a more accurate, holistic view compared to a single-feature analysis.


<details>
  <summary>Details</summary>
Motivation: The joint signal from a group of related features is sometimes overlooked or inadvertently excluded, which could bypass a critical insight that the most significant predictors are not isolated features, but rather the combined effect of groups of features. This can be especially problematic for datasets that contain natural groupings of features, including multimodal datasets.

Method: This paper introduces a novel approach to determine the importance of a group of features for Generalized Additive Models (GAMs) that is efficient, requires no model retraining, allows defining groups posthoc, permits overlapping groups, and remains meaningful in high-dimensional settings.

Result: Showcase properties of our method on three synthetic experiments that illustrate the behavior of group importance across various data regimes. Demonstrate the importance of groups of features in identifying depressive symptoms from a multimodal neuroscience dataset, and study the importance of social determinants of health after total hip arthroplasty.

Conclusion: Analyzing group importance offers a more accurate, holistic view of the medical issues compared to a single-feature analysis.

Abstract: While analyzing the importance of features has become ubiquitous in
interpretable machine learning, the joint signal from a group of related
features is sometimes overlooked or inadvertently excluded. Neglecting the
joint signal could bypass a critical insight: in many instances, the most
significant predictors are not isolated features, but rather the combined
effect of groups of features. This can be especially problematic for datasets
that contain natural groupings of features, including multimodal datasets. This
paper introduces a novel approach to determine the importance of a group of
features for Generalized Additive Models (GAMs) that is efficient, requires no
model retraining, allows defining groups posthoc, permits overlapping groups,
and remains meaningful in high-dimensional settings. Moreover, this definition
offers a parallel with explained variation in statistics. We showcase
properties of our method on three synthetic experiments that illustrate the
behavior of group importance across various data regimes. We then demonstrate
the importance of groups of features in identifying depressive symptoms from a
multimodal neuroscience dataset, and study the importance of social
determinants of health after total hip arthroplasty. These two case studies
reveal that analyzing group importance offers a more accurate, holistic view of
the medical issues compared to a single-feature analysis.

</details>


### [87] [HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization](https://arxiv.org/abs/2506.19992)
*Gabor Petnehazi,Bernadett Aradi*

Main category: cs.LG

TL;DR: HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization, a novel algorithm and Python package designed for hierarchical k-means clustering of diverse data types


<details>
  <summary>Details</summary>
Motivation: The explosive growth of complex datasets across various modalities necessitates advanced analytical tools that not only group data effectively but also provide human-understandable insights into the discovered structures.

Method: a novel algorithm and Python package designed for hierarchical k-means clustering of diverse data types, including text, images, and numeric data (processed one modality per run).

Result: constructs a cluster hierarchy by recursively applying k-means clustering, starting from individual data points at level 0. A key innovation is its deep integration of Large Language Models (LLMs) to generate semantically rich titles and descriptions for clusters at each level of the hierarchy, significantly enhancing interpretability.

Conclusion: demonstrate HERCULES's capabilities and discuss its potential for extracting meaningful, hierarchical knowledge from complex datasets.

Abstract: The explosive growth of complex datasets across various modalities
necessitates advanced analytical tools that not only group data effectively but
also provide human-understandable insights into the discovered structures. We
introduce HERCULES (Hierarchical Embedding-based Recursive Clustering Using
LLMs for Efficient Summarization), a novel algorithm and Python package
designed for hierarchical k-means clustering of diverse data types, including
text, images, and numeric data (processed one modality per run). HERCULES
constructs a cluster hierarchy by recursively applying k-means clustering,
starting from individual data points at level 0. A key innovation is its deep
integration of Large Language Models (LLMs) to generate semantically rich
titles and descriptions for clusters at each level of the hierarchy,
significantly enhancing interpretability. The algorithm supports two main
representation modes: `direct' mode, which clusters based on original data
embeddings or scaled numeric features, and `description' mode, which clusters
based on embeddings derived from LLM-generated summaries. Users can provide a
`topic\_seed' to guide LLM-generated summaries towards specific themes. An
interactive visualization tool facilitates thorough analysis and understanding
of the clustering results. We demonstrate HERCULES's capabilities and discuss
its potential for extracting meaningful, hierarchical knowledge from complex
datasets.

</details>


### [88] [TRACED: Transition-aware Regret Approximation with Co-learnability for Environment Design](https://arxiv.org/abs/2506.19997)
*Geonwoo Cho,Jaegyun Im,Jihwan Lee,Hojun Yi,Sejin Kim,Sundong Kim*

Main category: cs.LG

TL;DR: TRACED是一种新的UED方法，它使用转移预测误差和共同学习性来改进课程设计，从而提高泛化能力和样本效率。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习智能体泛化到未见环境仍然是一个重大挑战。现有的UED方法通常通过regret来衡量学习潜力，而regret仅通过价值函数损失来近似。

Method: TRACED：结合了转移预测误差和共同学习性度量的遗憾近似方法。

Result: TRACED在多个基准测试中提高了零样本泛化能力，并且比强大的基线方法减少了高达2倍的环境交互。消融研究证实，转移预测误差驱动了快速的复杂性提升，并且当与转移预测误差配对时，共同学习性提供了额外的收益。

Conclusion: TRACED通过改进的遗憾近似和显式建模任务关系，实现了UED中样本高效的课程设计。

Abstract: Generalizing deep reinforcement learning agents to unseen environments
remains a significant challenge. One promising solution is Unsupervised
Environment Design (UED), a co-evolutionary framework in which a teacher
adaptively generates tasks with high learning potential, while a student learns
a robust policy from this evolving curriculum. Existing UED methods typically
measure learning potential via regret, the gap between optimal and current
performance, approximated solely by value-function loss. Building on these
approaches, we introduce the transition prediction error as an additional term
in our regret approximation. To capture how training on one task affects
performance on others, we further propose a lightweight metric called
co-learnability. By combining these two measures, we present Transition-aware
Regret Approximation with Co-learnability for Environment Design (TRACED).
Empirical evaluations show that TRACED yields curricula that improve zero-shot
generalization across multiple benchmarks while requiring up to 2x fewer
environment interactions than strong baselines. Ablation studies confirm that
the transition prediction error drives rapid complexity ramp-up and that
co-learnability delivers additional gains when paired with the transition
prediction error. These results demonstrate how refined regret approximation
and explicit modeling of task relationships can be leveraged for
sample-efficient curriculum design in UED.

</details>


### [89] [Neuromorphic Wireless Split Computing with Resonate-and-Fire Neurons](https://arxiv.org/abs/2506.20015)
*Dengyu Wu,Jiechen Chen,H. Vincent Poor,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出了一种基于共振发射(RF)神经元的无线分割计算架构，用于直接处理时域信号，从而降低了能耗，并在音频和调制分类任务上取得了与传统方法相当的精度。


<details>
  <summary>Details</summary>
Motivation: 神经形态计算为实时时间序列处理提供了一种节能的替代传统深度学习加速器的方法。然而，许多边缘应用，如无线传感和音频识别，产生具有丰富频谱特征的流信号，而传统的leaky integrate-and-fire (LIF)脉冲神经元无法有效地捕捉这些特征。

Method: 无线分割计算架构，采用具有振荡动态的共振发射(RF)神经元直接处理时域信号，无需昂贵的频谱预处理。

Result: 所提出的RF-SNN架构实现了与传统LIF-SNN和ANN相当的精度，同时显著降低了脉冲速率和推理及通信过程中的总能耗。

Conclusion: RF-SNN架构在推理和通信过程中，与传统的LIF-SNN和ANN相比，实现了相当的精度，同时显著降低了脉冲速率和总能耗。

Abstract: Neuromorphic computing offers an energy-efficient alternative to conventional
deep learning accelerators for real-time time-series processing. However, many
edge applications, such as wireless sensing and audio recognition, generate
streaming signals with rich spectral features that are not effectively captured
by conventional leaky integrate-and-fire (LIF) spiking neurons. This paper
investigates a wireless split computing architecture that employs
resonate-and-fire (RF) neurons with oscillatory dynamics to process time-domain
signals directly, eliminating the need for costly spectral pre-processing. By
resonating at tunable frequencies, RF neurons extract time-localized spectral
features while maintaining low spiking activity. This temporal sparsity
translates into significant savings in both computation and transmission
energy. Assuming an OFDM-based analog wireless interface for spike
transmission, we present a complete system design and evaluate its performance
on audio classification and modulation classification tasks. Experimental
results show that the proposed RF-SNN architecture achieves comparable accuracy
to conventional LIF-SNNs and ANNs, while substantially reducing spike rates and
total energy consumption during inference and communication.

</details>


### [90] [New Insights on Unfolding and Fine-tuning Quantum Federated Learning](https://arxiv.org/abs/2506.20016)
*Shanika Iroshi Nanayakkara,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: propose a new approach leveraging deep unfolding, which enables clients to autonomously optimize hyperparameters, such as learning rates and regularization factors, based on their specific training behavior to overcome the limitations of Client heterogeneity in Quantum Federated Learning (QFL).


<details>
  <summary>Details</summary>
Motivation: Client heterogeneity poses significant challenges to the performance of Quantum Federated Learning (QFL).

Method: a new approach leveraging deep unfolding, which enables clients to autonomously optimize hyperparameters

Result: achieves approximately 90% accuracy, significantly outperforming traditional methods, which typically yield around 55% accuracy

Conclusion: This study addresses the core limitations of conventional QFL, streamlining its applicability to any complex challenges such as healthcare and genomic research.

Abstract: Client heterogeneity poses significant challenges to the performance of
Quantum Federated Learning (QFL). To overcome these limitations, we propose a
new approach leveraging deep unfolding, which enables clients to autonomously
optimize hyperparameters, such as learning rates and regularization factors,
based on their specific training behavior. This dynamic adaptation mitigates
overfitting and ensures robust optimization in highly heterogeneous
environments where standard aggregation methods often fail. Our framework
achieves approximately 90% accuracy, significantly outperforming traditional
methods, which typically yield around 55% accuracy, as demonstrated through
real-time training on IBM quantum hardware and Qiskit Aer simulators. By
developing self adaptive fine tuning, the proposed method proves particularly
effective in critical applications such as gene expression analysis and cancer
detection, enhancing diagnostic precision and predictive modeling within
quantum systems. Our results are attributed to convergence-aware, learnable
optimization steps intrinsic to the deep unfolded framework, which maintains
the generalization. Hence, this study addresses the core limitations of
conventional QFL, streamlining its applicability to any complex challenges such
as healthcare and genomic research.

</details>


### [91] [Elucidated Rolling Diffusion Models for Probabilistic Weather Forecasting](https://arxiv.org/abs/2506.20024)
*Salva Rühling Cachay,Miika Aittala,Karsten Kreis,Noah Brenowitz,Arash Vahdat,Morteza Mardani,Rose Yu*

Main category: cs.LG

TL;DR: ERDM是一种新的扩散模型框架，它结合了滚动预测和EDM，在序列生成问题上表现出色，尤其是在需要模拟不确定性增长的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在预测高维混沌系统时，难以捕捉复杂的时间依赖性，并且不能显式地解释不确定性的增长。

Method: 提出了Elucidated Rolling Diffusion Models (ERDM)，它将滚动预测结构与Elucidated Diffusion Models (EDM)的设计统一起来，并调整了EDM的核心组件，包括噪声调度、网络预处理和Heun采样器。

Result: ERDM在2D Navier-Stokes模拟和ERA5全球天气预测中，始终优于其他基于扩散的模型。

Conclusion: ERDM在模拟和天气预测中优于其他扩散模型，为模拟不确定性增长的序列生成问题提供了一个灵活而强大的框架。

Abstract: Diffusion models are a powerful tool for probabilistic forecasting, yet most
applications in high-dimensional chaotic systems predict future snapshots
one-by-one. This common approach struggles to model complex temporal
dependencies and fails to explicitly account for the progressive growth of
uncertainty inherent to such systems. While rolling diffusion frameworks, which
apply increasing noise to forecasts at longer lead times, have been proposed to
address this, their integration with state-of-the-art, high-fidelity diffusion
techniques remains a significant challenge. We tackle this problem by
introducing Elucidated Rolling Diffusion Models (ERDM), the first framework to
successfully unify a rolling forecast structure with the principled, performant
design of Elucidated Diffusion Models (EDM). To do this, we adapt the core EDM
components-its noise schedule, network preconditioning, and Heun sampler-to the
rolling forecast setting. The success of this integration is driven by three
key contributions: (i) a novel loss weighting scheme that focuses model
capacity on the mid-range forecast horizons where determinism gives way to
stochasticity; (ii) an efficient initialization strategy using a pre-trained
EDM for the initial window; and (iii) a bespoke hybrid sequence architecture
for robust spatiotemporal feature extraction under progressive denoising. On 2D
Navier-Stokes simulations and ERA5 global weather forecasting at 1.5^\circ
resolution, ERDM consistently outperforms key diffusion-based baselines,
including conditional autoregressive EDM. ERDM offers a flexible and powerful
general framework for tackling diffusion-based sequence generation problems
where modeling escalating uncertainty is paramount. Code is available at:
https://github.com/salvaRC/erdm

</details>


### [92] [Thumb on the Scale: Optimal Loss Weighting in Last Layer Retraining](https://arxiv.org/abs/2506.20025)
*Nathan Stromberg,Christos Thrampoulidis,Lalitha Sankar*

Main category: cs.LG

TL;DR: loss weighting is effective in the last layer retraining regime if weights take into account the relative overparameterization of the model


<details>
  <summary>Details</summary>
Motivation: machine learning models' ability to overcome biases introduced by training data has come under increasing scrutiny. Previous results suggest that there are two extremes of parameterization with very different behaviors

Method: theoretically and empirically explore the regime of last layer retraining

Result: loss weighting is still effective in the last layer retraining regime

Conclusion: loss weighting is still effective in the last layer retraining regime, but that these weights must take into account the relative overparameterization of the model

Abstract: While machine learning models become more capable in discriminative tasks at
scale, their ability to overcome biases introduced by training data has come
under increasing scrutiny. Previous results suggest that there are two extremes
of parameterization with very different behaviors: the population
(underparameterized) setting where loss weighting is optimal and the separable
overparameterized setting where loss weighting is ineffective at ensuring equal
performance across classes. This work explores the regime of last layer
retraining (LLR) in which the unseen limited (retraining) data is frequently
inseparable and the model proportionately sized, falling between the two
aforementioned extremes. We show, in theory and practice, that loss weighting
is still effective in this regime, but that these weights \emph{must} take into
account the relative overparameterization of the model.

</details>


### [93] [Automated Generation of Diverse Courses of Actions for Multi-Agent Operations using Binary Optimization and Graph Learning](https://arxiv.org/abs/2506.20031)
*Prithvi Poddar,Ehsan Tarkesh Esfahani,Karthik Dantu,Souma Chowdhury*

Main category: cs.LG

TL;DR: This paper presents a new theoretical formulation and computational framework to generate diverse pools of COAs for operations with soft variations in agent-task compatibility. A genetic algorithm is used for allocations of tasks to each agent that jointly maximize diversity within the COA pool and overall compatibility of the agent-task mappings. A graph neural network is trained using a policy gradient approach to then perform single agent task sequencing in each COA, which maximizes completion rates adaptive to task features.


<details>
  <summary>Details</summary>
Motivation: Operations in disaster response, search & rescue, and military missions that involve multiple agents demand automated processes to support the planning of the courses of action (COA). Moreover, traverse-affecting changes in the environment (rain, snow, blockades, etc.) may impact the expected performance of a COA, making it desirable to have a pool of COAs that are diverse in task distributions across agents. Further, variations in agent capabilities, which could be human crews and/or autonomous systems, present practical opportunities and computational challenges to the planning process.

Method: Formulating the COAs as a centralized multi-robot task allocation problem, a genetic algorithm is used for (order-ignoring) allocations of tasks to each agent that jointly maximize diversity within the COA pool and overall compatibility of the agent-task mappings. A graph neural network is trained using a policy gradient approach to then perform single agent task sequencing in each COA, which maximizes completion rates adaptive to task features.

Result: significant performance gain over a random walk baseline, small optimality gap in task sequencing, and execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task operations.

Conclusion: Our tests of the COA generation process in a simulated environment demonstrate significant performance gain over a random walk baseline, small optimality gap in task sequencing, and execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task operations.

Abstract: Operations in disaster response, search \& rescue, and military missions that
involve multiple agents demand automated processes to support the planning of
the courses of action (COA). Moreover, traverse-affecting changes in the
environment (rain, snow, blockades, etc.) may impact the expected performance
of a COA, making it desirable to have a pool of COAs that are diverse in task
distributions across agents. Further, variations in agent capabilities, which
could be human crews and/or autonomous systems, present practical opportunities
and computational challenges to the planning process. This paper presents a new
theoretical formulation and computational framework to generate such diverse
pools of COAs for operations with soft variations in agent-task compatibility.
Key to the problem formulation is a graph abstraction of the task space and the
pool of COAs itself to quantify its diversity. Formulating the COAs as a
centralized multi-robot task allocation problem, a genetic algorithm is used
for (order-ignoring) allocations of tasks to each agent that jointly maximize
diversity within the COA pool and overall compatibility of the agent-task
mappings. A graph neural network is trained using a policy gradient approach to
then perform single agent task sequencing in each COA, which maximizes
completion rates adaptive to task features. Our tests of the COA generation
process in a simulated environment demonstrate significant performance gain
over a random walk baseline, small optimality gap in task sequencing, and
execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task
operations.

</details>


### [94] [Verifiable Unlearning on Edge](https://arxiv.org/abs/2506.20037)
*Mohammad M Maheri,Alex Davidson,Hamed Haddadi*

Main category: cs.LG

TL;DR: This paper presents a zk-SNARKs based verification framework for verifiable and privacy-preserving machine unlearning on edge devices, with minimal impact on model performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the need for verifiable removal of data samples across edge devices due to copyright, bias, or regulatory issues.

Method: The authors introduce a verification framework using zero-knowledge proofs (zk-SNARKs) and develop algorithms compatible with efficient zk-SNARK proof generation.

Result: The results affirm the practicality and effectiveness of the framework, showing verifiable unlearning with minimal degradation in personalization-induced performance improvements.

Conclusion: This paper demonstrates a practical and effective verification framework for machine unlearning on edge devices, ensuring minimal performance degradation.

Abstract: Machine learning providers commonly distribute global models to edge devices,
which subsequently personalize these models using local data. However, issues
such as copyright infringements, biases, or regulatory requirements may require
the verifiable removal of certain data samples across all edge devices.
Ensuring that edge devices correctly execute such unlearning operations is
critical to maintaining integrity.
  In this work, we introduce a verification framework leveraging zero-knowledge
proofs, specifically zk-SNARKs, to confirm data unlearning on personalized
edge-device models without compromising privacy. We have developed algorithms
explicitly designed to facilitate unlearning operations that are compatible
with efficient zk-SNARK proof generation, ensuring minimal computational and
memory overhead suitable for constrained edge environments. Furthermore, our
approach carefully preserves personalized enhancements on edge devices,
maintaining model performance post-unlearning.
  Our results affirm the practicality and effectiveness of this verification
framework, demonstrating verifiable unlearning with minimal degradation in
personalization-induced performance improvements. Our methodology ensures
verifiable, privacy-preserving, and effective machine unlearning across edge
devices.

</details>
