{"id": "2511.00290", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.00290", "abs": "https://arxiv.org/abs/2511.00290", "authors": ["Ashwin Gerard Colaco", "Sharad Mehrotra", "Michael J De Lucia", "Kevin Hamlen", "Murat Kantarcioglu", "Latifur Khan", "Ananthram Swami", "Bhavani Thuraisingham"], "title": "NOMAD - Navigating Optimal Model Application to Datastreams", "comment": null, "summary": "NOMAD (Navigating Optimal Model Application for Datastreams) is an\nintelligent framework for data enrichment during ingestion that optimizes\nrealtime multiclass classification by dynamically constructing model chains,\ni.e ,sequences of machine learning models with varying cost-quality tradeoffs,\nselected via a utilitybased criterion. Inspired by predicate ordering\ntechniques from database query processing, NOMAD leverages cheaper models as\ninitial filters, proceeding to more expensive models only when necessary, while\nguaranteeing classification quality remains comparable to a designated role\nmodel through a formal chain safety mechanism. It employs a dynamic belief\nupdate strategy to adapt model selection based on per event predictions and\nshifting data distributions, and extends to scenarios with dependent models\nsuch as earlyexit DNNs and stacking ensembles. Evaluation across multiple\ndatasets demonstrates that NOMAD achieves significant computational savings\ncompared to static and naive approaches while maintaining classification\nquality comparable to that achieved by the most accurate (and often the most\nexpensive) model.", "AI": {"tldr": "NOMAD: \u667a\u80fd\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6444\u53d6\u671f\u95f4\u8fdb\u884c\u6570\u636e\u4e30\u5bcc\uff0c\u901a\u8fc7\u52a8\u6001\u6784\u5efa\u6a21\u578b\u94fe\u6765\u4f18\u5316\u5b9e\u65f6\u591a\u7c7b\u5206\u7c7b\u3002", "motivation": "\u5728\u6444\u53d6\u671f\u95f4\u4f18\u5316\u5b9e\u65f6\u591a\u7c7b\u5206\u7c7b\u7684\u6570\u636e\u4e30\u5bcc", "method": "\u5229\u7528\u66f4\u4fbf\u5b9c\u7684\u6a21\u578b\u4f5c\u4e3a\u521d\u59cb\u8fc7\u6ee4\u5668\uff0c\u4ec5\u5728\u5fc5\u8981\u65f6\u624d\u4f7f\u7528\u66f4\u6602\u8d35\u7684\u6a21\u578b\uff0c\u540c\u65f6\u901a\u8fc7\u6b63\u5f0f\u7684\u94fe\u5b89\u5168\u673a\u5236\u6765\u4fdd\u8bc1\u5206\u7c7b\u8d28\u91cf\u4e0e\u6307\u5b9a\u7684\u89d2\u8272\u6a21\u578b\u76f8\u5f53\u3002\u5b83\u91c7\u7528\u52a8\u6001\u4fe1\u5ff5\u66f4\u65b0\u7b56\u7565\uff0c\u4ee5\u6839\u636e\u6bcf\u4e2a\u4e8b\u4ef6\u7684\u9884\u6d4b\u548c\u53d8\u5316\u7684\u6570\u636e\u5206\u5e03\u6765\u8c03\u6574\u6a21\u578b\u9009\u62e9\uff0c\u5e76\u6269\u5c55\u5230\u5177\u6709\u4f9d\u8d56\u6a21\u578b\u7684\u573a\u666f\uff0c\u4f8b\u5982\u63d0\u524d\u9000\u51fa DNN \u548c\u5806\u53e0\u96c6\u6210\u3002", "result": "\u4e0e\u9759\u6001\u548c\u6734\u7d20\u65b9\u6cd5\u76f8\u6bd4\uff0cNOMAD \u5b9e\u73b0\u4e86\u663e\u7740\u7684\u8ba1\u7b97\u8282\u7701\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u6700\u51c6\u786e\uff08\u4e14\u901a\u5e38\u662f\u6700\u6602\u8d35\uff09\u7684\u6a21\u578b\u76f8\u5f53\u7684\u5206\u7c7b\u8d28\u91cf\u3002", "conclusion": "NOMAD \u662f\u4e00\u79cd\u667a\u80fd\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6a21\u578b\u94fe\u5728\u6570\u636e\u6444\u53d6\u671f\u95f4\u4f18\u5316\u5b9e\u65f6\u591a\u7c7b\u5206\u7c7b\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u5206\u7c7b\u8d28\u91cf\u7684\u540c\u65f6\u663e\u7740\u8282\u7701\u8ba1\u7b97\u3002"}}
{"id": "2511.00414", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.00414", "abs": "https://arxiv.org/abs/2511.00414", "authors": ["Sirintra Vaiwsri", "Thilina Ranbaduge"], "title": "Embedding based Encoding Scheme for Privacy Preserving Record Linkage", "comment": "12 pages", "summary": "To discover new insights from data, there is a growing need to share\ninformation that is often held by different organisations. One key task in data\nintegration is the calculation of similarities between records in different\ndatabases to identify pairs or sets of records that correspond to the same\nreal-world entities. Due to privacy and confidentiality concerns, however, the\nowners of sensitive databases are often not allowed or willing to exchange or\nshare their data with other organisations to allow such similarity\ncalculations. Privacy-preserving record linkage (PPRL) is the process of\nmatching records that refer to the same entity across sensitive databases held\nby different organisations while ensuring no information about the entities is\nrevealed to the participating parties. In this paper, we study how embedding\nbased encoding techniques can be applied in the PPRL context to ensure the\nprivacy of the entities that are being linked. We first convert individual\nq-grams into the embedded space and then convert the embedding of a set of\nq-grams of a given record into a binary representation. The final binary\nrepresentations can be used to link records into matches and non-matches. We\nempirically evaluate our proposed encoding technique against different\nreal-world datasets. The results suggest that our proposed encoding approach\ncan provide better linkage accuracy and protect the privacy of individuals\nagainst attack compared to state-of-the-art techniques for short record values.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5728\u9690\u79c1\u4fdd\u62a4\u8bb0\u5f55\u94fe\u63a5(PPRL)\u4e2d\u4f7f\u7528\u57fa\u4e8e\u5d4c\u5165\u7684\u7f16\u7801\u6280\u672f\uff0c\u4ee5\u786e\u4fdd\u88ab\u94fe\u63a5\u5b9e\u4f53\u7684\u9690\u79c1\u3002", "motivation": "\u6570\u636e\u96c6\u6210\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u4efb\u52a1\u662f\u8ba1\u7b97\u4e0d\u540c\u6570\u636e\u5e93\u4e2d\u8bb0\u5f55\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u4ee5\u8bc6\u522b\u5bf9\u5e94\u4e8e\u540c\u4e00\u73b0\u5b9e\u4e16\u754c\u5b9e\u4f53\u7684\u8bb0\u5f55\u5bf9\u6216\u96c6\u5408\u3002\u7531\u4e8e\u9690\u79c1\u548c\u4fdd\u5bc6\u95ee\u9898\uff0c\u654f\u611f\u6570\u636e\u5e93\u7684\u6240\u6709\u8005\u901a\u5e38\u4e0d\u88ab\u5141\u8bb8\u6216\u4e0d\u613f\u610f\u4e0e\u5176\u4ed6\u7ec4\u7ec7\u4ea4\u6362\u6216\u5171\u4eab\u5176\u6570\u636e\uff0c\u4ee5\u8fdb\u884c\u6b64\u7c7b\u76f8\u4f3c\u6027\u8ba1\u7b97\u3002", "method": "\u9996\u5148\u5c06\u5355\u4e2aq-gram\u8f6c\u6362\u4e3a\u5d4c\u5165\u7a7a\u95f4\uff0c\u7136\u540e\u5c06\u7ed9\u5b9a\u8bb0\u5f55\u7684\u4e00\u7ec4q-gram\u7684\u5d4c\u5165\u8f6c\u6362\u4e3a\u4e8c\u8fdb\u5236\u8868\u793a\u3002\u6700\u7ec8\u7684\u4e8c\u8fdb\u5236\u8868\u793a\u53ef\u4ee5\u7528\u4e8e\u5c06\u8bb0\u5f55\u94fe\u63a5\u5230\u5339\u914d\u548c\u975e\u5339\u914d\u9879\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u4e8e\u77ed\u8bb0\u5f55\u503c\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u6280\u672f\u76f8\u6bd4\uff0c\u8be5\u7f16\u7801\u65b9\u6cd5\u53ef\u4ee5\u63d0\u4f9b\u66f4\u597d\u7684\u94fe\u63a5\u51c6\u786e\u6027\uff0c\u5e76\u4fdd\u62a4\u4e2a\u4eba\u9690\u79c1\u514d\u53d7\u653b\u51fb\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5d4c\u5165\u7684\u7f16\u7801\u6280\u672f\uff0c\u53ef\u4ee5\u5e94\u7528\u4e8ePPRL\u4e0a\u4e0b\u6587\u4e2d\uff0c\u4ee5\u786e\u4fdd\u88ab\u94fe\u63a5\u5b9e\u4f53\u7684\u9690\u79c1\u3002"}}
{"id": "2511.00693", "categories": ["cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.00693", "abs": "https://arxiv.org/abs/2511.00693", "authors": ["Saba Latif", "Huma Latif", "Muhammad Rameez Ur Rahman"], "title": "Object-Centric Analysis of XES Event Logs: Integrating OCED Modeling with SPARQL Queries", "comment": "12 pages, 4 figures, PROFES2025 conference", "summary": "Object Centric Event Data (OCED) has gained attention in recent years within\nthe field of process mining. However, there are still many challenges, such as\nconnecting the XES format to object-centric approaches to enable more\ninsightful analysis. It is important for a process miner to understand the\ninsights and dependencies of events in the event log to see what is going on in\nour processes. In previous standards, the dependencies of event logs are only\nused to show events, but not their dependencies among each other and actions in\ndetail as described in OCEDO. There is more information in the event log when\nit is revealed using the OCEDO model. It becomes more understandable and easier\nto grasp the concepts and deal with the processes. This paper proposes the use\nof Object-Centric Event Data Ontology (OCEDO) to overcome the limitations of\nthe XES standard in event logs for process mining. We demonstrate how the OCEDO\napproach, integrated with SPARQL queries, can be applied to the BPIC 2013\ndataset to make the relationships between events and objects more explicit. It\ndescribes dealing with the meta descriptions of the OCEDO model on a business\nprocess challenge as an event log. It improves the completeness and readability\nof process data, suggesting that object-centric modeling allows for richer\nanalyses than traditional approaches.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u9762\u5411\u5bf9\u8c61\u4e8b\u4ef6\u6570\u636e\u672c\u4f53\uff08OCEDO\uff09\u6765\u514b\u670d XES \u6807\u51c6\u5728\u8fc7\u7a0b\u6316\u6398\u4e8b\u4ef6\u65e5\u5fd7\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u8fc7\u7a0b\u6316\u6398\u9886\u57df\u4e2d\uff0c\u9762\u5411\u5bf9\u8c61\u4e8b\u4ef6\u6570\u636e\uff08OCED\uff09\u8fd1\u5e74\u6765\u5907\u53d7\u5173\u6ce8\u3002\u7136\u800c\uff0c\u4ecd\u7136\u5b58\u5728\u8bb8\u591a\u6311\u6218\uff0c\u4f8b\u5982\u5c06 XES \u683c\u5f0f\u8fde\u63a5\u5230\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u66f4\u6df1\u5165\u7684\u5206\u6790\u3002", "method": "\u5c06 OCEDO \u65b9\u6cd5\u4e0e SPARQL \u67e5\u8be2\u96c6\u6210\uff0c\u5e94\u7528\u4e8e BPIC 2013 \u6570\u636e\u96c6\uff0c\u4ee5\u4f7f\u4e8b\u4ef6\u548c\u5bf9\u8c61\u4e4b\u95f4\u7684\u5173\u7cfb\u66f4\u52a0\u660e\u786e\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u5efa\u6a21\u53ef\u4ee5\u8fdb\u884c\u66f4\u4e30\u5bcc\u7684\u5206\u6790\uff0c\u5e76\u63d0\u9ad8\u4e86\u8fc7\u7a0b\u6570\u636e\u7684\u5b8c\u6574\u6027\u548c\u53ef\u8bfb\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6539\u8fdb\u4e86\u8fc7\u7a0b\u6570\u636e\u7684\u5b8c\u6574\u6027\u548c\u53ef\u8bfb\u6027\uff0c\u8868\u660e\u9762\u5411\u5bf9\u8c61\u7684\u5efa\u6a21\u80fd\u591f\u5b9e\u73b0\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u4e30\u5bcc\u7684\u5206\u6790\u3002"}}
{"id": "2511.00748", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.00748", "abs": "https://arxiv.org/abs/2511.00748", "authors": ["Yi Yang", "Jian Pei", "Jun Yang", "Jichun Xie"], "title": "Finding Non-Redundant Simpson's Paradox from Multidimensional Data", "comment": "20 pages, 7 figures", "summary": "Simpson's paradox, a long-standing statistical phenomenon, describes the\nreversal of an observed association when data are disaggregated into\nsub-populations. It has critical implications across statistics, epidemiology,\neconomics, and causal inference. Existing methods for detecting Simpson's\nparadox overlook a key issue: many paradoxes are redundant, arising from\nequivalent selections of data subsets, identical partitioning of\nsub-populations, and correlated outcome variables, which obscure essential\npatterns and inflate computational cost. In this paper, we present the first\nframework for discovering non-redundant Simpson's paradoxes. We formalize three\ntypes of redundancy - sibling child, separator, and statistic equivalence - and\nshow that redundancy forms an equivalence relation. Leveraging this insight, we\npropose a concise representation framework for systematically organizing\nredundant paradoxes and design efficient algorithms that integrate depth-first\nmaterialization of the base table with redundancy-aware paradox discovery.\nExperiments on real-world datasets and synthetic benchmarks show that redundant\nparadoxes are widespread, on some real datasets constituting over 40% of all\nparadoxes, while our algorithms scale to millions of records, reduce run time\nby up to 60%, and discover paradoxes that are structurally robust under data\nperturbation. These results demonstrate that Simpson's paradoxes can be\nefficiently identified, concisely summarized, and meaningfully interpreted in\nlarge multidimensional datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d1\u73b0\u975e\u5197\u4f59\u8f9b\u666e\u68ee\u6096\u8bba\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u5197\u4f59\u6096\u8bba\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8f9b\u666e\u68ee\u6096\u8bba\u68c0\u6d4b\u65b9\u6cd5\u5ffd\u7565\u4e86\u5197\u4f59\u6096\u8bba\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u589e\u52a0\u5e76\u63a9\u76d6\u4e86\u672c\u8d28\u6a21\u5f0f\u3002", "method": "\u672c\u6587\u5f62\u5f0f\u5316\u4e86\u4e09\u79cd\u5197\u4f59\u7c7b\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u6d01\u7684\u8868\u793a\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u4f18\u5148\u7269\u5316\u548c\u5197\u4f59\u611f\u77e5\u6096\u8bba\u53d1\u73b0\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5197\u4f59\u6096\u8bba\u5e7f\u6cdb\u5b58\u5728\uff0c\u5728\u67d0\u4e9b\u771f\u5b9e\u6570\u636e\u96c6\u4e2d\u5360\u6240\u6709\u6096\u8bba\u768440%\u4ee5\u4e0a\uff0c\u5e76\u4e14\u672c\u6587\u7684\u7b97\u6cd5\u53ef\u4ee5\u6269\u5c55\u5230\u6570\u767e\u4e07\u6761\u8bb0\u5f55\uff0c\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe60%\uff0c\u5e76\u53d1\u73b0\u7ed3\u6784\u4e0a\u5bf9\u6570\u636e\u6270\u52a8\u5177\u6709\u9c81\u68d2\u6027\u7684\u6096\u8bba\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86\u8f9b\u666e\u68ee\u6096\u8bba\u53ef\u4ee5\u5728\u5927\u578b\u591a\u7ef4\u6570\u636e\u96c6\u4e2d\u6709\u6548\u5730\u8bc6\u522b\u3001\u7b80\u6d01\u5730\u603b\u7ed3\u548c\u6709\u610f\u4e49\u5730\u89e3\u91ca\u3002"}}
{"id": "2511.00072", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00072", "abs": "https://arxiv.org/abs/2511.00072", "authors": ["Pradeep M", "Ritesh Pallod", "Satyen Abrol", "Muthu Raman", "Ian Anderson"], "title": "LookSync: Large-Scale Visual Product Search System for AI-Generated Fashion Looks", "comment": "4 pages, 5 figures. Accepted at the International Conference on Data\n  Science (IKDD CODS 2025), Demonstration Track. Demo video:\n  https://youtu.be/DZdlWmTUwjc", "summary": "Generative AI is reshaping fashion by enabling virtual looks and avatars\nmaking it essential to find real products that best match AI-generated styles.\nWe propose an end-to-end product search system that has been deployed in a\nreal-world, internet scale which ensures that AI-generated looks presented to\nusers are matched with the most visually and semantically similar products from\nthe indexed vector space. The search pipeline is composed of four key\ncomponents: query generation, vectorization, candidate retrieval, and reranking\nbased on AI-generated looks. Recommendation quality is evaluated using\nhuman-judged accuracy scores. The system currently serves more than 350,000 AI\nLooks in production per day, covering diverse product categories across global\nmarkets of over 12 million products. In our experiments, we observed that\nacross multiple annotators and categories, CLIP outperformed alternative models\nby a small relative margin of 3--7\\% in mean opinion scores. These\nimprovements, though modest in absolute numbers, resulted in noticeably better\nuser perception matches, establishing CLIP as the most reliable backbone for\nproduction deployment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u4ea7\u54c1\u641c\u7d22\u7cfb\u7edf\uff0c\u7528\u4e8e\u5339\u914dAI\u751f\u6210\u7684\u65f6\u5c1a\u5916\u89c2\u548c\u771f\u5b9e\u4ea7\u54c1\u3002", "motivation": "\u4e3a\u4e86\u627e\u5230\u4e0eAI\u751f\u6210\u7684\u98ce\u683c\u6700\u5339\u914d\u7684\u771f\u5b9e\u4ea7\u54c1\uff0c\u56e0\u4e3a\u751f\u6210\u5f0fAI\u6b63\u5728\u91cd\u5851\u65f6\u5c1a\u3002", "method": "\u8be5\u641c\u7d22\u7ba1\u9053\u7531\u56db\u4e2a\u5173\u952e\u7ec4\u4ef6\u7ec4\u6210\uff1a\u67e5\u8be2\u751f\u6210\u3001\u5411\u91cf\u5316\u3001\u5019\u9009\u68c0\u7d22\u548c\u57fa\u4e8eAI\u751f\u6210\u5916\u89c2\u7684\u91cd\u65b0\u6392\u5e8f\u3002", "result": "CLIP\u5728\u5e73\u5747\u610f\u89c1\u5f97\u5206\u65b9\u9762\u4ee53-7%\u7684\u5c0f\u5e45\u76f8\u5bf9\u4f18\u52bf\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002\u8be5\u7cfb\u7edf\u76ee\u524d\u6bcf\u5929\u5728\u751f\u4ea7\u4e2d\u670d\u52a1\u8d85\u8fc7350,000\u4e2aAI\u5916\u89c2\uff0c\u6db5\u76d6\u5168\u7403\u5e02\u573a\u8d85\u8fc71200\u4e07\u79cd\u4ea7\u54c1\u7684\u5404\u79cd\u4ea7\u54c1\u7c7b\u522b\u3002", "conclusion": "CLIP\u662f\u751f\u4ea7\u90e8\u7f72\u4e2d\u6700\u53ef\u9760\u7684\u9aa8\u5e72\u3002"}}
{"id": "2511.00002", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00002", "abs": "https://arxiv.org/abs/2511.00002", "authors": ["Yurun Wu", "Yousong Sun", "Burkhard Wunsche", "Jia Wang", "Elliott Wen"], "title": "VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games", "comment": null, "summary": "Virtual Reality (VR) has rapidly become a mainstream platform for gaming and\ninteractive experiences, yet ensuring the quality, safety, and appropriateness\nof VR content remains a pressing challenge. Traditional human-based quality\nassurance is labor-intensive and cannot scale with the industry's rapid growth.\nWhile automated testing has been applied to traditional 2D and 3D games,\nextending it to VR introduces unique difficulties due to high-dimensional\nsensory inputs and strict real-time performance requirements. We present\nVRScout, a deep learning-based agent capable of autonomously navigating VR\nenvironments and interacting with virtual objects in a human-like and real-time\nmanner. VRScout learns from human demonstrations using an enhanced Action\nChunking Transformer that predicts multi-step action sequences. This enables\nour agent to capture higher-level strategies and generalize across diverse\nenvironments. To balance responsiveness and precision, we introduce a\ndynamically adjustable sliding horizon that adapts the agent's temporal context\nat runtime. We evaluate VRScout on commercial VR titles and show that it\nachieves expert-level performance with only limited training data, while\nmaintaining real-time inference at 60 FPS on consumer-grade hardware. These\nresults position VRScout as a practical and scalable framework for automated VR\ngame testing, with direct applications in both quality assurance and safety\nauditing.", "AI": {"tldr": "VR\u5185\u5bb9\u8d28\u91cf\u4fdd\u8bc1\u662f\u96be\u9898\uff1b\u4eba\u5de5\u6210\u672c\u9ad8\uff0c\u6269\u5c55\u6027\u5dee\u3002\u672c\u6587\u63d0\u51fa\u4e86VRScout\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u667a\u80fd\u4f53\uff0c\u53ef\u4ee5\u81ea\u52a8\u5bfc\u822aVR\u73af\u5883\u5e76\u4e0e\u865a\u62df\u5bf9\u8c61\u4ea4\u4e92\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u5de5\u8d28\u91cf\u4fdd\u8bc1\u52b3\u52a8\u5bc6\u96c6\uff0c\u65e0\u6cd5\u968f\u7740\u884c\u4e1a\u5feb\u901f\u589e\u957f\u800c\u6269\u5c55\u3002\u867d\u7136\u81ea\u52a8\u5316\u6d4b\u8bd5\u5df2\u5e94\u7528\u4e8e\u4f20\u7edf\u76842D\u548c3D\u6e38\u620f\uff0c\u4f46\u7531\u4e8e\u9ad8\u7ef4\u611f\u5b98\u8f93\u5165\u548c\u4e25\u683c\u7684\u5b9e\u65f6\u6027\u80fd\u8981\u6c42\uff0c\u5c06\u5176\u6269\u5c55\u5230VR\u5f15\u5165\u4e86\u72ec\u7279\u7684\u56f0\u96be\u3002", "method": "VRScout\u4ece\u4eba\u7c7b\u6f14\u793a\u4e2d\u5b66\u4e60\uff0c\u4f7f\u7528\u589e\u5f3a\u7684\u52a8\u4f5c\u5206\u5757Transformer\u9884\u6d4b\u591a\u6b65\u52a8\u4f5c\u5e8f\u5217\u3002\u4e3a\u4e86\u5e73\u8861\u54cd\u5e94\u6027\u548c\u7cbe\u5ea6\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u52a8\u6001\u53ef\u8c03\u7684\u6ed1\u52a8\u8303\u56f4\uff0c\u53ef\u4ee5\u5728\u8fd0\u884c\u65f6\u8c03\u6574\u4ee3\u7406\u7684\u65f6\u95f4\u4e0a\u4e0b\u6587\u3002", "result": "VRScout\u5728\u5546\u4e1aVR\u6e38\u620f\u4e2d\u5b9e\u73b0\u4e86\u4e13\u5bb6\u7ea7\u7684\u6027\u80fd\uff0c\u53ea\u9700\u6709\u9650\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u540c\u65f6\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u4fdd\u630160 FPS\u7684\u5b9e\u65f6\u63a8\u7406\u3002", "conclusion": "VRScout\u662f\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316VR\u6e38\u620f\u6d4b\u8bd5\u6846\u67b6\uff0c\u5728\u8d28\u91cf\u4fdd\u8bc1\u548c\u5b89\u5168\u5ba1\u8ba1\u4e2d\u5177\u6709\u76f4\u63a5\u5e94\u7528\u3002"}}
{"id": "2511.00010", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00010", "abs": "https://arxiv.org/abs/2511.00010", "authors": ["Jiajun Zhang", "Jianke Zhang", "Zeyu Cui", "Jiaxi Yang", "Lei Zhang", "Binyuan Hui", "Qiang Liu", "Zilei Wang", "Liang Wang", "Junyang Lin"], "title": "PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization", "comment": null, "summary": "Recent Large Language Models (LLMs) have demonstrated remarkable profi-\nciency in code generation. However, their ability to create complex visualiza-\ntions for scaled and structured data remains largely unevaluated and\nunderdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark\nfeaturing 1k challenging visualization tasks that cover a wide range of topics,\nsuch as fi- nance, scientific research, and sociology. The benchmark is\nstructured around seven high-level visualization tasks and encompasses 48\ndistinct chart types. Cru- cially, it is the first to systematically evaluate\nboth single-turn generation and multi-turn refinement across a diverse spectrum\nof task complexities. Our com- prehensive evaluation of 23 leading LLMs on\nPlotCraft reveals obvious per- formance deficiencies in handling sophisticated\nvisualization tasks. To bridge this performance gap, we develope SynthVis-30K,\na large-scale, high-quality dataset of complex visualization code synthesized\nvia a collaborative agent frame- work. Building upon this dataset, we develope\nPlotCraftor, a novel code gener- ation model that achieves strong capabilities\nin complex data visualization with a remarkably small size. Across VisEval,\nPandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance\ncomparable to that of leading propri- etary approaches. Especially, on hard\ntask, Our model achieves over 50% per- formance improvement. We will release\nthe benchmark, dataset, and code at\nhttps://github.com/Speakn0w/PlotCraft-Benchmark.", "AI": {"tldr": "PlotCraft\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6570\u636e\u53ef\u89c6\u5316\u65b9\u9762\u7684\u80fd\u529b\u3002\u5b83\u5305\u542b1000\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u53ef\u89c6\u5316\u4efb\u52a1\uff0c\u6db5\u76d6\u91d1\u878d\u3001\u79d1\u5b66\u7814\u7a76\u548c\u793e\u4f1a\u5b66\u7b49\u5e7f\u6cdb\u4e3b\u9898\u3002\u5bf923\u4e2a\u9886\u5148\u7684LLM\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5b83\u4eec\u5728\u5904\u7406\u590d\u6742\u7684\u89c6\u89c9\u4efb\u52a1\u65b9\u9762\u5b58\u5728\u660e\u663e\u7684\u6027\u80fd\u7f3a\u9677\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u6027\u80fd\u5dee\u8ddd\uff0c\u4f5c\u8005\u5f00\u53d1\u4e86SynthVis-30K\uff0c\u8fd9\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u590d\u6742\u53ef\u89c6\u5316\u4ee3\u7801\u6570\u636e\u96c6\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u4f5c\u8005\u5f00\u53d1\u4e86PlotCraftor\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u578b\u7684\u4ee3\u7801\u751f\u6210\u6a21\u578b\uff0c\u5728\u590d\u6742\u7684\u6570\u636e\u53ef\u89c6\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u800c\u4e14\u89c4\u6a21\u975e\u5e38\u5c0f\u3002PlotCraftor\u5728VisEval\u3001PandasPlotBench\u548cPlotCraft\u4e0a\u8868\u73b0\u51fa\u4e0e\u9886\u5148\u7684\u4e13\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002\u7279\u522b\u662f\u5728\u56f0\u96be\u7684\u4efb\u52a1\u4e0a\uff0c\u8be5\u6a21\u578b\u5b9e\u73b0\u4e86\u8d85\u8fc750%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u8bc4\u4f30\u548c\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u521b\u5efa\u590d\u6742\u6570\u636e\u53ef\u89c6\u5316\u7684\u80fd\u529b\u3002", "method": "1. \u5f15\u5165PlotCraft\u57fa\u51c6\uff0c\u5305\u542b1000\u4e2a\u53ef\u89c6\u5316\u4efb\u52a1\uff0c\u6db5\u76d6\u5404\u79cd\u4e3b\u9898\u548c\u56fe\u8868\u7c7b\u578b\u30022. \u8bc4\u4f3023\u4e2a\u9886\u5148\u7684LLM\u5728PlotCraft\u4e0a\u7684\u6027\u80fd\u30023. \u5f00\u53d1SynthVis-30K\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u6a21\u578b\u30024. \u5f00\u53d1PlotCraftor\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u590d\u6742\u7684\u6570\u636e\u53ef\u89c6\u5316\u4ee3\u7801\u3002", "result": "1. \u53d1\u73b0\u73b0\u6709LLM\u5728\u590d\u6742\u6570\u636e\u53ef\u89c6\u5316\u65b9\u9762\u5b58\u5728\u6027\u80fd\u7f3a\u9677\u30022. PlotCraftor\u6a21\u578b\u5728\u590d\u6742\u6570\u636e\u53ef\u89c6\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u5e76\u4e14\u89c4\u6a21\u975e\u5e38\u5c0f\u30023. PlotCraftor\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4e0e\u9886\u5148\u7684\u4e13\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u56f0\u96be\u7684\u4efb\u52a1\u4e0a\uff0c\u6027\u80fd\u63d0\u5347\u8d85\u8fc750%\u3002", "conclusion": "PlotCraft\u57fa\u51c6\u548cPlotCraftor\u6a21\u578b\u4e3a\u8bc4\u4f30\u548c\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6570\u636e\u53ef\u89c6\u5316\u65b9\u9762\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u548c\u65b9\u6cd5\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u8bbe\u8ba1\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8LLM\u5728\u8fd9\u4e00\u9886\u57df\u7684\u6027\u80fd\u3002"}}
{"id": "2511.00011", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.00011", "abs": "https://arxiv.org/abs/2511.00011", "authors": ["Alexander Okupnik", "Johannes Schneider", "Kyriakos Flouris"], "title": "Generative human motion mimicking through feature extraction in denoising diffusion settings", "comment": null, "summary": "Recent success with large language models has sparked a new wave of verbal\nhuman-AI interaction. While such models support users in a variety of creative\ntasks, they lack the embodied nature of human interaction. Dance, as a primal\nform of human expression, is predestined to complement this experience. To\nexplore creative human-AI interaction exemplified by dance, we build an\ninteractive model based on motion capture (MoCap) data. It generates an\nartificial other by partially mimicking and also \"creatively\" enhancing an\nincoming sequence of movement data. It is the first model, which leverages\nsingle-person motion data and high level features in order to do so and, thus,\nit does not rely on low level human-human interaction data. It combines ideas\nof two diffusion models, motion inpainting, and motion style transfer to\ngenerate movement representations that are both temporally coherent and\nresponsive to a chosen movement reference. The success of the model is\ndemonstrated by quantitatively assessing the convergence of the feature\ndistribution of the generated samples and the test set which serves as\nsimulating the human performer. We show that our generations are first steps to\ncreative dancing with AI as they are both diverse showing various deviations\nfrom the human partner while appearing realistic.", "AI": {"tldr": "\u6784\u5efa\u4e86\u4e00\u4e2a\u4ea4\u4e92\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u4eff\u548c\u521b\u9020\u6027\u5730\u589e\u5f3a\u4f20\u5165\u7684\u8fd0\u52a8\u6570\u636e\u5e8f\u5217\u6765\u751f\u6210\u4e00\u4e2a\u865a\u62df\u7684\u821e\u4f34\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u4eba\u7c7b\u4ea4\u4e92\u7684\u5177\u8eab\u6027\uff0c\u800c\u821e\u8e48\u662f\u4eba\u7c7b\u8868\u8fbe\u7684\u539f\u59cb\u5f62\u5f0f\uff0c\u53ef\u4ee5\u8865\u5145\u8fd9\u79cd\u4f53\u9a8c\u3002", "method": "\u8be5\u6a21\u578b\u57fa\u4e8e\u8fd0\u52a8\u6355\u6349(MoCap)\u6570\u636e\uff0c\u7ed3\u5408\u4e86\u4e24\u79cd\u6269\u6563\u6a21\u578b\u3001\u8fd0\u52a8\u4fee\u590d\u548c\u8fd0\u52a8\u98ce\u683c\u8fc1\u79fb\u7684\u601d\u60f3\u3002", "result": "\u901a\u8fc7\u5b9a\u91cf\u8bc4\u4f30\u751f\u6210\u6837\u672c\u7684\u7279\u5f81\u5206\u5e03\u548c\u6d4b\u8bd5\u96c6\u7684\u6536\u655b\u6027\u6765\u8bc1\u660e\u6a21\u578b\u7684\u6210\u529f\u3002", "conclusion": "\u8be5\u6a21\u578b\u751f\u6210\u7684\u821e\u8e48\u65e2\u5177\u6709\u591a\u6837\u6027\uff0c\u53c8\u663e\u793a\u51fa\u4e0e\u4eba\u7c7b\u821e\u4f34\u7684\u5404\u79cd\u504f\u5dee\uff0c\u540c\u65f6\u53c8\u663e\u5f97\u903c\u771f\uff0c\u662f\u4e0e\u4eba\u5de5\u667a\u80fd\u8fdb\u884c\u521b\u9020\u6027\u821e\u8e48\u7684\u7b2c\u4e00\u6b65\u3002"}}
{"id": "2511.00020", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00020", "abs": "https://arxiv.org/abs/2511.00020", "authors": ["Suhasnadh Reddy Veluru", "Sai Teja Erukude", "Viswa Chaitanya Marella"], "title": "Multimodal Detection of Fake Reviews using BERT and ResNet-50", "comment": "Published in IEEE", "summary": "In the current digital commerce landscape, user-generated reviews play a\ncritical role in shaping consumer behavior, product reputation, and platform\ncredibility. However, the proliferation of fake or misleading reviews often\ngenerated by bots, paid agents, or AI models poses a significant threat to\ntrust and transparency within review ecosystems. Existing detection models\nprimarily rely on unimodal, typically textual, data and therefore fail to\ncapture semantic inconsistencies across different modalities. To address this\ngap, a robust multimodal fake review detection framework is proposed,\nintegrating textual features encoded with BERT and visual features extracted\nusing ResNet-50. These representations are fused through a classification head\nto jointly predict review authenticity. To support this approach, a curated\ndataset comprising 21,142 user-uploaded images across food delivery,\nhospitality, and e-commerce domains was utilized. Experimental results indicate\nthat the multimodal model outperforms unimodal baselines, achieving an F1-score\nof 0.934 on the test set. Additionally, the confusion matrix and qualitative\nanalysis highlight the model's ability to detect subtle inconsistencies, such\nas exaggerated textual praise paired with unrelated or low-quality images,\ncommonly found in deceptive content. This study demonstrates the critical role\nof multimodal learning in safeguarding digital trust and offers a scalable\nsolution for content moderation across various online platforms.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u865a\u5047\u8bc4\u8bba\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u6587\u672c\u548c\u89c6\u89c9\u7279\u5f81\u3002", "motivation": "\u52a8\u673a\u662f\u73b0\u6709\u68c0\u6d4b\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5355\u6a21\u6001\u6570\u636e\uff0c\u65e0\u6cd5\u6355\u6349\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u7684\u8bed\u4e49\u4e0d\u4e00\u81f4\u6027\u3002 \u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u5bf9\u4e8e\u7ef4\u62a4\u8bc4\u8bba\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u4fe1\u4efb\u548c\u900f\u660e\u5ea6\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528BERT\u7f16\u7801\u6587\u672c\u7279\u5f81\uff0c\u4f7f\u7528ResNet-50\u63d0\u53d6\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u5206\u7c7b\u5934\u878d\u5408\u8fd9\u4e9b\u8868\u793a\uff0c\u4ee5\u8054\u5408\u9884\u6d4b\u8bc4\u8bba\u7684\u771f\u5b9e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u591a\u6a21\u6001\u6a21\u578b\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e860.934\u7684F1\u5206\u6570\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u5728\u4fdd\u62a4\u6570\u5b57\u4fe1\u4efb\u65b9\u9762\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u4e3a\u5404\u79cd\u5728\u7ebf\u5e73\u53f0\u7684\u5185\u5bb9\u5ba1\u6838\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00772", "categories": ["cs.DB", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.00772", "abs": "https://arxiv.org/abs/2511.00772", "authors": ["Raymond M. Xiong", "Panyu Chen", "Tianze Dong", "Jian Lu", "Benjamin Goldstein", "Danyang Zhuo", "Anru R. Zhang"], "title": "Reliable Curation of EHR Dataset via Large Language Models under Environmental Constraints", "comment": null, "summary": "Electronic health records (EHRs) are central to modern healthcare delivery\nand research; yet, many researchers lack the database expertise necessary to\nwrite complex SQL queries or generate effective visualizations, limiting\nefficient data use and scientific discovery. To address this barrier, we\nintroduce CELEC, a large language model (LLM)-powered framework for automated\nEHR data extraction and analytics. CELEC translates natural language queries\ninto SQL using a prompting strategy that integrates schema information,\nfew-shot demonstrations, and chain-of-thought reasoning, which together improve\naccuracy and robustness. On a subset of the EHRSQL benchmark, CELEC achieves\nexecution accuracy comparable to prior systems while maintaining low latency,\ncost efficiency, and strict privacy by exposing only database metadata to the\nLLM. CELEC also adheres to strict privacy protocols: the LLM accesses only\ndatabase metadata (e.g., table and column names), while all query execution\noccurs securely within the institutional environment, ensuring that no\npatient-level data is ever transmitted to or shared with the LLM. Ablation\nstudies confirm that each component of the SQL generation pipeline,\nparticularly the few-shot demonstrations, plays a critical role in performance.\nBy lowering technical barriers and enabling medical researchers to query EHR\ndatabases directly, CELEC streamlines research workflows and accelerates\nbiomedical discovery.", "AI": {"tldr": "CELEC is an LLM-powered framework for automated EHR data extraction and analytics.", "motivation": "Many researchers lack the database expertise necessary to write complex SQL queries or generate effective visualizations, limiting efficient data use and scientific discovery.", "method": "CELEC translates natural language queries into SQL using a prompting strategy that integrates schema information, few-shot demonstrations, and chain-of-thought reasoning.", "result": "CELEC achieves execution accuracy comparable to prior systems while maintaining low latency, cost efficiency, and strict privacy.", "conclusion": "CELEC streamlines research workflows and accelerates biomedical discovery by lowering technical barriers."}}
{"id": "2511.00176", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00176", "abs": "https://arxiv.org/abs/2511.00176", "authors": ["Milad Sabouri", "Masoud Mansoury", "Kun Lin", "Bamshad Mobasher"], "title": "Effectiveness of LLMs in Temporal User Profiling for Recommendation", "comment": "Accepted to the IEEE International Conference on Data Mining (ICDM\n  2025), Workshop on User Modeling and Recommendation (UMRec). To appear in the\n  IEEE ICDMW 2025 proceedings", "summary": "Effectively modeling the dynamic nature of user preferences is crucial for\nenhancing recommendation accuracy and fostering transparency in recommender\nsystems. Traditional user profiling often overlooks the distinction between\ntransitory short-term interests and stable long-term preferences. This paper\nexamines the capability of leveraging Large Language Models (LLMs) to capture\nthese temporal dynamics, generating richer user representations through\ndistinct short-term and long-term textual summaries of interaction histories.\nOur observations suggest that while LLMs tend to improve recommendation quality\nin domains with more active user engagement, their benefits appear less\npronounced in sparser environments. This disparity likely stems from the\nvarying distinguishability of short-term and long-term preferences across\ndomains; the approach shows greater utility where these temporal interests are\nmore clearly separable (e.g., Movies\\&TV) compared to domains with more stable\nuser profiles (e.g., Video Games). This highlights a critical trade-off between\nenhanced performance and computational costs, suggesting context-dependent LLM\napplication. Beyond predictive capability, this LLM-driven approach inherently\nprovides an intrinsic potential for interpretability through its natural\nlanguage profiles and attention weights. This work contributes insights into\nthe practical capability and inherent interpretability of LLM-driven temporal\nuser profiling, outlining new research directions for developing adaptive and\ntransparent recommender systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6355\u83b7\u7528\u6237\u504f\u597d\u7684\u65f6\u95f4\u52a8\u6001\uff0c\u901a\u8fc7\u751f\u6210\u77ed\u671f\u548c\u957f\u671f\u6587\u672c\u6458\u8981\u6765\u4e30\u5bcc\u7528\u6237\u8868\u793a\uff0c\u4ece\u800c\u63d0\u9ad8\u63a8\u8350\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u3002\u7814\u7a76\u8868\u660e\uff0cLLM\u5728\u7528\u6237\u53c2\u4e0e\u5ea6\u8f83\u9ad8\u7684\u9886\u57df\u80fd\u6709\u6548\u63d0\u9ad8\u63a8\u8350\u8d28\u91cf\uff0c\u4f46\u5728\u7a00\u758f\u73af\u5883\u4e2d\u6548\u679c\u4e0d\u660e\u663e\u3002", "motivation": "\u4f20\u7edf\u7684\u7528\u6237\u753b\u50cf\u5ffd\u7565\u4e86\u77ed\u6682\u7684\u77ed\u671f\u5174\u8da3\u548c\u7a33\u5b9a\u7684\u957f\u671f\u504f\u597d\u4e4b\u95f4\u7684\u533a\u522b\u3002\u672c\u6587\u65e8\u5728\u5229\u7528LLM\u6355\u83b7\u8fd9\u4e9b\u65f6\u95f4\u52a8\u6001\uff0c\u751f\u6210\u66f4\u4e30\u5bcc\u7684\u7528\u6237\u8868\u793a\u3002", "method": "\u5229\u7528LLM\u751f\u6210\u7528\u6237\u4ea4\u4e92\u5386\u53f2\u7684\u77ed\u671f\u548c\u957f\u671f\u6587\u672c\u6458\u8981\uff0c\u4ee5\u6b64\u533a\u5206\u77ed\u671f\u5174\u8da3\u548c\u957f\u671f\u504f\u597d\u3002", "result": "LLM\u5728\u7528\u6237\u53c2\u4e0e\u5ea6\u8f83\u9ad8\u7684\u9886\u57df\uff08\u5982\u7535\u5f71\u548c\u7535\u89c6\uff09\u80fd\u6709\u6548\u63d0\u9ad8\u63a8\u8350\u8d28\u91cf\uff0c\u4f46\u5728\u7528\u6237\u753b\u50cf\u66f4\u7a33\u5b9a\u7684\u7a00\u758f\u73af\u5883\uff08\u5982\u89c6\u9891\u6e38\u620f\uff09\u4e2d\u6548\u679c\u4e0d\u660e\u663e\u3002\u8be5\u65b9\u6cd5\u5728\u65f6\u95f4\u5174\u8da3\u66f4\u6613\u533a\u5206\u7684\u9886\u57df\u8868\u73b0\u51fa\u66f4\u5927\u7684\u6548\u7528\u3002", "conclusion": "\u672c\u6587\u9a8c\u8bc1\u4e86LLM\u9a71\u52a8\u7684\u65f6\u95f4\u7528\u6237\u753b\u50cf\u7684\u5b9e\u7528\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u5f00\u53d1\u81ea\u9002\u5e94\u548c\u900f\u660e\u7684\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002\u540c\u65f6\u5f3a\u8c03\u4e86\u6027\u80fd\u548c\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5efa\u8bae\u6839\u636e\u5177\u4f53\u60c5\u51b5\u5e94\u7528LLM\u3002"}}
{"id": "2511.00029", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00029", "abs": "https://arxiv.org/abs/2511.00029", "authors": ["Samaksh Bhargav", "Zining Zhu"], "title": "Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts", "comment": "12 pages, 6 figures", "summary": "Large Language Model (LLM) deployment requires guiding the LLM to recognize\nand not answer unsafe prompts while complying with safe prompts. Previous\nmethods for achieving this require adjusting model weights along with other\nexpensive procedures. While recent advances in Sparse Autoencoders (SAEs) have\nenabled interpretable feature extraction from LLMs, existing approaches lack\nsystematic feature selection methods and principled evaluation of\nsafety-utility tradeoffs. We explored using different steering features and\nsteering strengths using Sparse Auto Encoders (SAEs) to provide a solution.\nUsing an accurate and innovative contrasting prompt method with the\nAI-Generated Prompts Dataset from teknium/OpenHermes-2p5-Mistral-7B and Air\nBench eu-dataset to efficiently choose the best features in the model to steer,\nwe tested this method on Llama-3 8B. We conclude that using this method, our\napproach achieves an 18.9% improvement in safety performance while\nsimultaneously increasing utility by 11.1%, demonstrating that targeted SAE\nsteering can overcome traditional safety-utility tradeoffs when optimal\nfeatures are identified through principled selection methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u4e00\u79cd\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u5f15\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8LLM\u5728\u5b89\u5168\u6027\u548c\u6548\u7528\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u3002", "motivation": "\u73b0\u6709LLM\u90e8\u7f72\u65b9\u6cd5\u5728\u5904\u7406\u5b89\u5168\u63d0\u793a\u65f6\u9700\u8981\u8c03\u6574\u6a21\u578b\u6743\u91cd\u7b49\u590d\u6742\u64cd\u4f5c\uff0c\u4e14\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u548c\u5b89\u5168-\u6548\u7528\u6743\u8861\u8bc4\u4f30\u3002", "method": "\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u63d0\u53d6\u53ef\u89e3\u91ca\u7684\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u521b\u65b0\u6027\u7684\u5bf9\u6bd4\u63d0\u793a\u65b9\u6cd5\u548cAI\u751f\u6210\u63d0\u793a\u6570\u636e\u96c6\uff0c\u9ad8\u6548\u9009\u62e9\u6700\u4f73\u7279\u5f81\u8fdb\u884c\u5f15\u5bfc\u3002", "result": "\u5728Llama-3 8B\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u80fd\u4e0a\u63d0\u5347\u4e8618.9%\uff0c\u540c\u65f6\u6548\u7528\u6027\u4e5f\u63d0\u5347\u4e8611.1%\u3002", "conclusion": "\u6709\u9488\u5bf9\u6027\u7684SAE\u5f15\u5bfc\u53ef\u4ee5\u514b\u670d\u4f20\u7edf\u7684\u5b89\u5168-\u6548\u7528\u6743\u8861\uff0c\u524d\u63d0\u662f\u901a\u8fc7\u5408\u7406\u7684\u9009\u62e9\u65b9\u6cd5\u786e\u5b9a\u6700\u4f73\u7279\u5f81\u3002"}}
{"id": "2511.00115", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00115", "abs": "https://arxiv.org/abs/2511.00115", "authors": ["Haoyuan Li", "Yuanbo Tong", "Yuchen Li", "Zirui Wang", "Chunhou Liu", "Jiamou Liu"], "title": "Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference", "comment": null, "summary": "Personality recognition from text is typically cast as hard-label\nclassification, which obscures the graded, prototype-like nature of human\npersonality judgments. We present ProtoMBTI, a cognitively aligned framework\nfor MBTI inference that operationalizes prototype theory within an LLM-based\npipeline. First, we construct a balanced, quality-controlled corpus via\nLLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).\nNext, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative\nembeddings and to standardize a bank of personality prototypes. At inference,\nwe retrieve top-k prototypes for a query post and perform a\nretrieve--reuse--revise--retain cycle: the model aggregates prototype evidence\nvia prompt-based voting, revises when inconsistencies arise, and, upon correct\nprediction, retains the sample to continually enrich the prototype library.\nAcross Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both\nthe four MBTI dichotomies and the full 16-type task, and exhibits robust\ncross-dataset generalization. Our results indicate that aligning the inference\nprocess with psychological prototype reasoning yields gains in accuracy,\ninterpretability, and transfer for text-based personality modeling.", "AI": {"tldr": "ProtoMBTI: A new framework for MBTI inference using prototype theory and LLMs.", "motivation": "Traditional personality recognition treats personality as hard-label classification, ignoring the nuanced nature of personality judgments.", "method": "1) Construct a balanced corpus using LLM augmentation. 2) Fine-tune an encoder to learn embeddings and standardize personality prototypes. 3) Implement a retrieve-reuse-revise-retain cycle for inference, aggregating prototype evidence and revising inconsistencies.", "result": "ProtoMBTI outperforms baselines on MBTI benchmarks and demonstrates cross-dataset generalization.", "conclusion": "Aligning the inference process with psychological prototype reasoning improves accuracy, interpretability, and transfer in text-based personality modeling."}}
{"id": "2511.00021", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00021", "abs": "https://arxiv.org/abs/2511.00021", "authors": ["Julio Jerison E. Macrohon", "Gordon Hung"], "title": "Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets", "comment": "15 pages, 10 figures", "summary": "Coral reefs support numerous marine organisms and are an important source of\ncoastal protection from storms and floods, representing a major part of marine\necosystems. However coral reefs face increasing threats from pollution, ocean\nacidification, and sea temperature anomalies, making efficient protection and\nmonitoring heavily urgent. Therefore, this study presents a novel\nmachine-learning-based coral bleaching classification system based on a diverse\nglobal dataset with samples of healthy and bleached corals under varying\nenvironmental conditions, including deep seas, marshes, and coastal zones. We\nbenchmarked and compared three state-of-the-art models: Residual Neural Network\n(ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN).\nAfter comprehensive hyperparameter tuning, the CNN model achieved the highest\naccuracy of 88%, outperforming existing benchmarks. Our findings offer\nimportant insights into autonomous coral monitoring and present a comprehensive\nanalysis of the most widely used computer vision models.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u73ca\u745a\u767d\u5316\u5206\u7c7b\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u4f7f\u7528\u5305\u542b\u5065\u5eb7\u548c\u767d\u5316\u73ca\u745a\u6837\u672c\u7684\u591a\u5143\u5316\u5168\u7403\u6570\u636e\u96c6\u3002", "motivation": "\u73ca\u745a\u7901\u9762\u4e34\u7740\u6765\u81ea\u6c61\u67d3\u3001\u6d77\u6d0b\u9178\u5316\u548c\u6d77\u6c34\u6e29\u5ea6\u5f02\u5e38\u7684\u65e5\u76ca\u4e25\u91cd\u7684\u5a01\u80c1\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u6709\u6548\u7684\u4fdd\u62a4\u548c\u76d1\u6d4b\u3002", "method": "\u8be5\u7814\u7a76\u5bf9\u4e09\u79cd\u6700\u5148\u8fdb\u7684\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u548c\u6bd4\u8f83\uff1a\u6b8b\u5dee\u795e\u7ecf\u7f51\u7edc (ResNet)\u3001\u89c6\u89c9\u8f6c\u6362\u5668 (ViT) \u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN)\u3002", "result": "\u7ecf\u8fc7\u5168\u9762\u7684\u8d85\u53c2\u6570\u8c03\u6574\u540e\uff0cCNN \u6a21\u578b\u5b9e\u73b0\u4e86 88% \u7684\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u81ea\u4e3b\u73ca\u745a\u76d1\u6d4b\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u89c1\u89e3\uff0c\u5e76\u5bf9\u6700\u5e7f\u6cdb\u4f7f\u7528\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\u3002"}}
{"id": "2511.00039", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00039", "abs": "https://arxiv.org/abs/2511.00039", "authors": ["Krishna Kumar Neelakanta Pillai Santha Kumari Amma"], "title": "Graph-Attentive MAPPO for Dynamic Retail Pricing", "comment": null, "summary": "Dynamic pricing in retail requires policies that adapt to shifting demand\nwhile coordinating decisions across related products. We present a systematic\nempirical study of multi-agent reinforcement learning for retail price\noptimization, comparing a strong MAPPO baseline with a\ngraph-attention-augmented variant (MAPPO+GAT) that leverages learned\ninteractions among products. Using a simulated pricing environment derived from\nreal transaction data, we evaluate profit, stability across random seeds,\nfairness across products, and training efficiency under a standardized\nevaluation protocol. The results indicate that MAPPO provides a robust and\nreproducible foundation for portfolio-level price control, and that MAPPO+GAT\nfurther enhances performance by sharing information over the product graph\nwithout inducing excessive price volatility. These results indicate that\ngraph-integrated MARL provides a more scalable and stable solution than\nindependent learners for dynamic retail pricing, offering practical advantages\nin multi-product decision-making.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u96f6\u552e\u52a8\u6001\u5b9a\u4ef7\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u673a\u5236\u7684MAPPO\u6539\u8fdb\u7b97\u6cd5(MAPPO+GAT)\u3002", "motivation": "\u96f6\u552e\u4e2d\u7684\u52a8\u6001\u5b9a\u4ef7\u7b56\u7565\u9700\u8981\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u9700\u6c42\uff0c\u540c\u65f6\u534f\u8c03\u76f8\u5173\u4ea7\u54c1\u4e4b\u95f4\u7684\u51b3\u7b56\u3002", "method": "\u4f7f\u7528\u4ece\u771f\u5b9e\u4ea4\u6613\u6570\u636e\u4e2d\u63d0\u53d6\u7684\u6a21\u62df\u5b9a\u4ef7\u73af\u5883\uff0c\u6bd4\u8f83\u4e86MAPPO\u57fa\u7ebf\u548c\u4e00\u4e2a\u56fe\u6ce8\u610f\u529b\u589e\u5f3a\u53d8\u4f53(MAPPO+GAT)\u3002\u8bc4\u4f30\u4e86\u5229\u6da6\u3001\u968f\u673a\u79cd\u5b50\u4e4b\u95f4\u7684\u7a33\u5b9a\u6027\u3001\u4ea7\u54c1\u4e4b\u95f4\u7684\u516c\u5e73\u6027\u4ee5\u53ca\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\u4e0b\u7684\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cMAPPO\u4e3a\u6295\u8d44\u7ec4\u5408\u5c42\u9762\u7684\u4ef7\u683c\u63a7\u5236\u63d0\u4f9b\u4e86\u7a33\u5065\u548c\u53ef\u91cd\u590d\u7684\u57fa\u7840\uff0c\u800cMAPPO+GAT\u901a\u8fc7\u5728\u4ea7\u54c1\u56fe\u4e0a\u5171\u4eab\u4fe1\u606f\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4e14\u4e0d\u4f1a\u5f15\u8d77\u8fc7\u5ea6\u7684\u4ef7\u683c\u6ce2\u52a8\u3002", "conclusion": "\u56fe\u96c6\u6210\u7684MARL\u4e3a\u52a8\u6001\u96f6\u552e\u5b9a\u4ef7\u63d0\u4f9b\u4e86\u6bd4\u72ec\u7acb\u5b66\u4e60\u8005\u66f4\u5177\u53ef\u6269\u5c55\u6027\u548c\u7a33\u5b9a\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u591a\u4ea7\u54c1\u51b3\u7b56\u63d0\u4f9b\u4e86\u5b9e\u9645\u4f18\u52bf\u3002"}}
{"id": "2511.00826", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.00826", "abs": "https://arxiv.org/abs/2511.00826", "authors": ["Shatha Algarni", "Boris Glavic", "Seokki Lee", "Adriane Chapman"], "title": "Efficient Query Repair for Aggregate Constraints", "comment": "19 pages, 63 figures", "summary": "In many real-world scenarios, query results must satisfy domain-specific\nconstraints. For instance, a minimum percentage of interview candidates\nselected based on their qualifications should be female. These requirements can\nbe expressed as constraints over an arithmetic combination of aggregates\nevaluated on the result of the query. In this work, we study how to repair a\nquery to fulfill such constraints by modifying the filter predicates of the\nquery. We introduce a novel query repair technique that leverages bounds on\nsets of candidate solutions and interval arithmetic to efficiently prune the\nsearch space. We demonstrate experimentally, that our technique significantly\noutperforms baselines that consider a single candidate at a time.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u4fee\u6539\u67e5\u8be2\u7684\u8fc7\u6ee4\u8c13\u8bcd\u4ee5\u6ee1\u8db3\u7ea6\u675f\u6761\u4ef6\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u53ef\u4ee5\u8868\u793a\u4e3a\u5728\u67e5\u8be2\u7ed3\u679c\u4e0a\u8bc4\u4f30\u7684\u805a\u5408\u7b97\u672f\u7ec4\u5408\u7684\u7ea6\u675f\u6761\u4ef6\u3002", "motivation": "\u5728\u8bb8\u591a\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u67e5\u8be2\u7ed3\u679c\u5fc5\u987b\u6ee1\u8db3\u7279\u5b9a\u9886\u57df\u7684\u7ea6\u675f\u3002\u4f8b\u5982\uff0c\u6839\u636e\u8d44\u683c\u9009\u62e9\u7684\u9762\u8bd5\u5019\u9009\u4eba\u4e2d\uff0c\u5973\u6027\u5fc5\u987b\u5360\u6700\u4f4e\u767e\u5206\u6bd4\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u67e5\u8be2\u4fee\u590d\u6280\u672f\uff0c\u8be5\u6280\u672f\u5229\u7528\u5019\u9009\u89e3\u51b3\u65b9\u6848\u96c6\u7684\u754c\u9650\u548c\u533a\u95f4\u7b97\u672f\u6765\u6709\u6548\u5730\u4fee\u526a\u641c\u7d22\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6280\u672f\u660e\u663e\u4f18\u4e8e\u4e00\u6b21\u8003\u8651\u5355\u4e2a\u5019\u9009\u5bf9\u8c61\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u67e5\u8be2\u4fee\u590d\u6280\u672f\uff0c\u53ef\u4ee5\u6ee1\u8db3\u67e5\u8be2\u7ed3\u679c\u4e0a\u7684\u7ea6\u675f\u6761\u4ef6\u3002"}}
{"id": "2511.00436", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.00436", "abs": "https://arxiv.org/abs/2511.00436", "authors": ["Doyun Choi", "Cheonwoo Lee", "Jaemin Yoo"], "title": "Simple and Behavior-Driven Augmentation for Recommendation with Rich Collaborative Signals", "comment": "10 pages. This paper is accepted at IEEE BigData 2025 (Short)", "summary": "Contrastive learning (CL) has been widely used for enhancing the performance\nof graph collaborative filtering (GCF) for personalized recommendation. Since\ndata augmentation plays a crucial role in the success of CL, previous works\nhave designed augmentation methods to remove noisy interactions between users\nand items in order to generate effective augmented views. However, the\nambiguity in defining ''noisiness'' presents a persistent risk of losing core\ninformation and generating unreliable data views, while increasing the overall\ncomplexity of augmentation. In this paper, we propose Simple Collaborative\nAugmentation for Recommendation (SCAR), a novel and intuitive augmentation\nmethod designed to maximize the effectiveness of CL for GCF. Instead of\nremoving information, SCAR leverages collaborative signals extracted from\nuser-item interactions to generate pseudo-interactions, which are then either\nadded to or used to replace existing interactions. This results in more robust\nrepresentations while avoiding the pitfalls of overly complex augmentation\nmodules. We conduct experiments on four benchmark datasets and show that SCAR\noutperforms previous CL-based GCF methods as well as other state-of-the-art\nself-supervised learning approaches across key evaluation metrics. SCAR\nexhibits strong robustness across different hyperparameter settings and is\nparticularly effective in sparse data scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6bd4\u5b66\u4e60\u63a8\u8350\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6dfb\u52a0\u6216\u66ff\u6362\u4f2a\u4ea4\u4e92\u6765\u751f\u6210\u66f4\u9c81\u68d2\u7684\u8868\u793a\uff0c\u907f\u514d\u4e86\u590d\u6742\u589e\u5f3a\u6a21\u5757\u7684\u9677\u9631\u3002", "motivation": "\u4ee5\u5f80\u7684\u5bf9\u6bd4\u5b66\u4e60\u56fe\u534f\u540c\u8fc7\u6ee4\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u53bb\u9664\u566a\u58f0\u4ea4\u4e92\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u4f46\u566a\u58f0\u7684\u5b9a\u4e49\u6a21\u7cca\uff0c\u5bb9\u6613\u4e22\u5931\u6838\u5fc3\u4fe1\u606f\u5e76\u751f\u6210\u4e0d\u53ef\u9760\u7684\u6570\u636e\u89c6\u56fe\uff0c\u540c\u65f6\u589e\u52a0\u589e\u5f3a\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSCAR\u7684\u7b80\u5355\u534f\u540c\u589e\u5f3a\u65b9\u6cd5\uff0c\u5229\u7528\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u4e2d\u63d0\u53d6\u7684\u534f\u540c\u4fe1\u53f7\u751f\u6210\u4f2a\u4ea4\u4e92\uff0c\u5e76\u5c06\u5176\u6dfb\u52a0\u5230\u73b0\u6709\u4ea4\u4e92\u6216\u7528\u4e8e\u66ff\u6362\u73b0\u6709\u4ea4\u4e92\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSCAR\u4f18\u4e8e\u4ee5\u5f80\u7684\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u56fe\u534f\u540c\u8fc7\u6ee4\u65b9\u6cd5\u4ee5\u53ca\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u7684\u8d85\u53c2\u6570\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u662f\u5728\u7a00\u758f\u6570\u636e\u573a\u666f\u4e2d\u3002", "conclusion": "SCAR\u901a\u8fc7\u751f\u6210\u4f2a\u4ea4\u4e92\u6765\u5b9e\u73b0\u6709\u6548\u7684\u5bf9\u6bd4\u5b66\u4e60\u56fe\u534f\u540c\u8fc7\u6ee4\u589e\u5f3a\uff0c\u907f\u514d\u4e86\u590d\u6742\u589e\u5f3a\u6a21\u5757\u7684\u9677\u9631\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2511.00030", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00030", "abs": "https://arxiv.org/abs/2511.00030", "authors": ["Myeongseob Ko", "Hoang Anh Just", "Charles Fleming", "Ming Jin", "Ruoxi Jia"], "title": "Probing Knowledge Holes in Unlearned LLMs", "comment": "The Thirty-ninth Annual Conference on Neural Information Processing\n  Systems", "summary": "Machine unlearning has emerged as a prevalent technical solution for\nselectively removing unwanted knowledge absorbed during pre-training, without\nrequiring full retraining. While recent unlearning techniques can effectively\nremove undesirable content without severely compromising performance on\nstandard benchmarks, we find that they may inadvertently create ``knowledge\nholes'' -- unintended losses of benign knowledge that standard benchmarks fail\nto capture. To probe where unlearned models reveal knowledge holes, we propose\na test case generation framework that explores both immediate neighbors of\nunlearned content and broader areas of potential failures. Our evaluation\ndemonstrates significant hidden costs of unlearning: up to 98.7\\% of the test\ncases yield irrelevant or nonsensical responses from unlearned models, despite\nbeing answerable by the pretrained model. These findings necessitate rethinking\nthe conventional approach to evaluating knowledge preservation in unlearning,\nmoving beyond standard, static benchmarks.", "AI": {"tldr": "\u673a\u5668\u5b66\u4e60\u5378\u8f7d\u6280\u672f\u65e8\u5728\u9009\u62e9\u6027\u5730\u79fb\u9664\u9884\u8bad\u7ec3\u4e2d\u5438\u6536\u7684\u4e0d\u826f\u77e5\u8bc6\uff0c\u800c\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5378\u8f7d\u6280\u672f\u53ef\u80fd\u4f1a\u610f\u5916\u5730\u9020\u6210\u201c\u77e5\u8bc6\u6f0f\u6d1e\u201d\uff0c\u5373\u6807\u51c6\u57fa\u51c6\u65e0\u6cd5\u6355\u6349\u5230\u7684\u826f\u6027\u77e5\u8bc6\u7684\u610f\u5916\u635f\u5931\u3002", "motivation": "\u73b0\u6709\u7684\u5378\u8f7d\u6280\u672f\u5728\u79fb\u9664\u4e0d\u826f\u5185\u5bb9\u7684\u540c\u65f6\uff0c\u53ef\u80fd\u4f1a\u610f\u5916\u5730\u9020\u6210\u826f\u6027\u77e5\u8bc6\u7684\u635f\u5931\uff0c\u800c\u6807\u51c6\u57fa\u51c6\u65e0\u6cd5\u6355\u6349\u5230\u8fd9\u4e9b\u635f\u5931\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u6846\u67b6\uff0c\u63a2\u7d22\u5378\u8f7d\u5185\u5bb9\u7684\u76f4\u63a5\u90bb\u57df\u548c\u6f5c\u5728\u5931\u8d25\u7684\u66f4\u5e7f\u6cdb\u533a\u57df\uff0c\u4ee5\u63a2\u6d4b\u5378\u8f7d\u6a21\u578b\u5728\u4f55\u5904\u66b4\u9732\u77e5\u8bc6\u6f0f\u6d1e\u3002", "result": "\u8bc4\u4f30\u8868\u660e\u5378\u8f7d\u5b58\u5728\u663e\u8457\u7684\u9690\u85cf\u6210\u672c\uff1a\u9ad8\u8fbe 98.7% \u7684\u6d4b\u8bd5\u7528\u4f8b\u4ea7\u751f\u4e0d\u76f8\u5173\u6216\u65e0\u610f\u4e49\u7684\u54cd\u5e94\uff0c\u5c3d\u7ba1\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u4ee5\u56de\u7b54\u8fd9\u4e9b\u95ee\u9898\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\u9700\u8981\u91cd\u65b0\u601d\u8003\u8bc4\u4f30\u5378\u8f7d\u4e2d\u77e5\u8bc6\u4fdd\u7559\u7684\u4f20\u7edf\u65b9\u6cd5\uff0c\u8d85\u8d8a\u6807\u51c6\u3001\u9759\u6001\u7684\u57fa\u51c6\u3002"}}
{"id": "2511.00180", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00180", "abs": "https://arxiv.org/abs/2511.00180", "authors": ["Nicky Pochinkov", "Yulia Volkova", "Anna Vasileva", "Sai V R Chereddy"], "title": "ParaScopes: What do Language Models Activations Encode About Future Text?", "comment": "Main paper: 9 pages, 10 figures. Total 24 pages", "summary": "Interpretability studies in language models often investigate forward-looking\nrepresentations of activations. However, as language models become capable of\ndoing ever longer time horizon tasks, methods for understanding activations\noften remain limited to testing specific concepts or tokens. We develop a\nframework of Residual Stream Decoders as a method of probing model activations\nfor paragraph-scale and document-scale plans. We test several methods and find\ninformation can be decoded equivalent to 5+ tokens of future context in small\nmodels. These results lay the groundwork for better monitoring of language\nmodels and better understanding how they might encode longer-term planning\ninformation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6b8b\u5dee\u6d41\u89e3\u7801\u5668\u6846\u67b6\uff0c\u7528\u4e8e\u63a2\u7a76\u8bed\u8a00\u6a21\u578b\u6fc0\u6d3b\u4e2d\u6bb5\u843d\u7ea7\u548c\u6587\u6863\u7ea7\u8ba1\u5212\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u8a00\u6a21\u578b\u53ef\u6267\u884c\u66f4\u957f\u65f6\u95f4\u8303\u56f4\u7684\u4efb\u52a1\uff0c\u4f46\u7406\u89e3\u6fc0\u6d3b\u7684\u65b9\u6cd5\u901a\u5e38\u4ec5\u9650\u4e8e\u6d4b\u8bd5\u7279\u5b9a\u6982\u5ff5\u6216token\u3002", "method": "\u5f00\u53d1\u6b8b\u5dee\u6d41\u89e3\u7801\u5668\u6846\u67b6\uff0c\u7528\u4e8e\u63a2\u6d4b\u6a21\u578b\u6fc0\u6d3b\u3002", "result": "\u5728\u5c0f\u578b\u6a21\u578b\u4e2d\uff0c\u53ef\u4ee5\u89e3\u7801\u76f8\u5f53\u4e8e\u672a\u6765\u4e0a\u4e0b\u6587\u4e2d 5+ \u4e2a token \u7684\u4fe1\u606f\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u66f4\u597d\u5730\u76d1\u63a7\u8bed\u8a00\u6a21\u578b\u548c\u7406\u89e3\u5b83\u4eec\u5982\u4f55\u7f16\u7801\u957f\u671f\u8ba1\u5212\u4fe1\u606f\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.00022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00022", "abs": "https://arxiv.org/abs/2511.00022", "authors": ["Jules Gerard", "Leandro Di Bella", "Filip Huyghe", "Marc Kochzius"], "title": "Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline", "comment": "Accepted to EUVIP2025, student session", "summary": "Coral reef monitoring in the Western Indian Ocean is limited by the labor\ndemands of underwater visual censuses. This work evaluates a YOLOv8-based deep\nlearning pipeline for automating family-level fish identification from video\ntransects collected in Kenya and Tanzania. A curated dataset of 24 families was\ntested under different configurations, providing the first region-specific\nbenchmark for automated reef fish monitoring in the Western Indian Ocean. The\nbest model achieved mAP@0.5 of 0.52, with high accuracy for abundant families\nbut weaker detection of rare or complex taxa. Results demonstrate the potential\nof deep learning as a scalable complement to traditional monitoring methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLOv8\u7684\u6df1\u5ea6\u5b66\u4e60\u7ba1\u9053\uff0c\u7528\u4e8e\u81ea\u52a8\u8bc6\u522b\u897f\u5370\u5ea6\u6d0b\u7684\u73ca\u745a\u7901\u9c7c\u7c7b\uff0c\u4ee5\u89e3\u51b3\u6c34\u4e0b\u89c6\u89c9\u8c03\u67e5\u7684\u4eba\u529b\u9700\u6c42\u95ee\u9898\u3002", "motivation": "\u897f\u5370\u5ea6\u6d0b\u7684\u73ca\u745a\u7901\u76d1\u6d4b\u53d7\u5230\u6c34\u4e0b\u89c6\u89c9\u8c03\u67e5\u7684\u4eba\u529b\u9700\u6c42\u9650\u5236\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eYOLOv8\u7684\u6df1\u5ea6\u5b66\u4e60\u7ba1\u9053\uff0c\u5bf9\u5728\u80af\u5c3c\u4e9a\u548c\u5766\u6851\u5c3c\u4e9a\u6536\u96c6\u7684\u89c6\u9891\u6837\u5e26\u8fdb\u884c\u5bb6\u5ead\u5c42\u9762\u9c7c\u7c7b\u8bc6\u522b\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5b9e\u73b0\u4e860.52\u7684mAP@0.5\uff0c\u5bf9\u4e30\u5bcc\u9c7c\u7c7b\u7684\u8bc6\u522b\u51c6\u786e\u7387\u8f83\u9ad8\uff0c\u4f46\u5bf9\u7a00\u6709\u6216\u590d\u6742\u7c7b\u7fa4\u7684\u68c0\u6d4b\u8f83\u5f31\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u6df1\u5ea6\u5b66\u4e60\u6709\u6f5c\u529b\u4f5c\u4e3a\u4f20\u7edf\u76d1\u6d4b\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u8865\u5145\u3002"}}
{"id": "2511.00048", "categories": ["cs.AI", "cs.CY", "62-11", "E.5; G.3; I.6.4; I.6.6; J.3; J.4"], "pdf": "https://arxiv.org/pdf/2511.00048", "abs": "https://arxiv.org/abs/2511.00048", "authors": ["Martin Bicher", "Maximilian Viehauser", "Daniele Giannandrea", "Hannah Kastinger", "Dominik Brunmeir", "Claire Rippinger", "Christoph Urach", "Niki Popper"], "title": "GEPOC Parameters - Open Source Parametrisation and Validation for Austria, Version 2.0", "comment": "134 pages, 75 figures, 19 tables", "summary": "GEPOC, short for Generic Population Concept, is a collection of models and\nmethods for analysing population-level research questions. For the valid\napplication of the models for a specific country or region, stable and\nreproducible data processes are necessary, which provide valid and ready-to-use\nmodel parameters. This work contains a complete description of the\ndata-processing methods for computation of model parameters for Austria, based\nexclusively on freely and publicly accessible data. In addition to the\ndescription of the source data used, this includes all algorithms used for\naggregation, disaggregation, fusion, cleansing or scaling of the data, as well\nas a description of the resulting parameter files. The document places\nparticular emphasis on the computation of parameters for the most important\nGEPOC model, GEPOC ABM, a continuous-time agent-based population model. An\nextensive validation study using this particular model was made and is\npresented at the end of this work.", "AI": {"tldr": "\u672c\u6587\u6863\u5168\u9762\u63cf\u8ff0\u4e86\u57fa\u4e8e\u53ef\u81ea\u7531\u516c\u5f00\u83b7\u53d6\u7684\u6570\u636e\u8ba1\u7b97\u5965\u5730\u5229\u6a21\u578b\u53c2\u6570\u7684\u6570\u636e\u5904\u7406\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u9488\u5bf9\u7279\u5b9a\u56fd\u5bb6\u6216\u533a\u57df\u6709\u6548\u5e94\u7528\u6a21\u578b\uff0c\u7a33\u5b9a\u4e14\u53ef\u91cd\u590d\u7684\u6570\u636e\u8fc7\u7a0b\u662f\u5fc5\u8981\u7684\uff0c\u8fd9\u4e9b\u8fc7\u7a0b\u63d0\u4f9b\u6709\u6548\u4e14\u53ef\u7528\u7684\u6a21\u578b\u53c2\u6570\u3002", "method": "\u672c\u6587\u6863\u63cf\u8ff0\u4e86\u7528\u4e8e\u805a\u5408\u3001\u5206\u89e3\u3001\u878d\u5408\u3001\u6e05\u7406\u6216\u7f29\u653e\u6570\u636e\u7684\u6240\u6709\u7b97\u6cd5\uff0c\u4ee5\u53ca\u5bf9\u751f\u6210\u7684\u53c2\u6570\u6587\u4ef6\u7684\u63cf\u8ff0\u3002", "result": "\u4f7f\u7528 GEPOC ABM \u6a21\u578b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u9a8c\u8bc1\u7814\u7a76\uff0c\u5e76\u5728\u672c\u6587\u6863\u672b\u5c3e\u8fdb\u884c\u4e86\u5c55\u793a\u3002", "conclusion": "\u672c\u6587\u6863\u4e3a\u5965\u5730\u5229 GEPOC \u6a21\u578b\u53c2\u6570\u7684\u8ba1\u7b97\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u6570\u636e\u5904\u7406\u65b9\u6cd5\u3002"}}
{"id": "2511.00855", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.00855", "abs": "https://arxiv.org/abs/2511.00855", "authors": ["Zhonggen Li", "Yougen Li", "Yifan Zhu", "Zhaoqiang Chen", "Yunjun Gao"], "title": "All-in-one Graph-based Indexing for Hybrid Search on GPUs", "comment": null, "summary": "Hybrid search has emerged as a promising paradigm to overcome the limitations\nof single-path retrieval, enhancing accuracy for applications like\nrecommendations, information retrieval, and Retrieval-Augmented Generation.\nHowever, existing methods are constrained by a trilemma: they sacrifice\nflexibility for efficiency, suffer from accuracy degradation due to separate\nretrievals, or incur prohibitive storage overhead for flexible combinations of\nretrieval paths. This paper introduces Allan-Poe, a novel All-in-one graph\nindex accelerated by GPUs for efficient hybrid search. We first analyze the\nlimitations of existing retrieval paradigms and distill key design principles\nfor an effective hybrid search index. Guided by these principles, we architect\na unified graph-based index that flexibly integrates four retrieval paths-dense\nvector, sparse vector, full-text, and knowledge graph-within a single, cohesive\nstructure. To enable efficient construction, we design a GPU-accelerated\npipeline featuring a warp-level hybrid distance kernel, RNG-IP joint pruning,\nand keyword-aware neighbor recycling. For query processing, we introduce a\ndynamic fusion framework that supports any combination of retrieval paths and\nweights without index reconstruction, leveraging logical edges from the\nknowledge graph to resolve complex multi-hop queries. Extensive experiments on\n6 real-world datasets demonstrate that Allan-Poe achieves superior end-to-end\nquery accuracy and outperforms state-of-the-art methods by 1.5-186.4x in\nthroughput, while significantly reducing storage overhead.", "AI": {"tldr": "Allan-Poe: A GPU-accelerated graph index for efficient hybrid search, overcoming limitations of existing methods with a unified structure integrating multiple retrieval paths.", "motivation": "Existing hybrid search methods sacrifice flexibility for efficiency, suffer from accuracy degradation, or have prohibitive storage overhead.", "method": "A unified graph-based index integrating dense vector, sparse vector, full-text, and knowledge graph retrieval paths, with a GPU-accelerated pipeline for efficient construction and a dynamic fusion framework for query processing.", "result": "Allan-Poe achieves superior query accuracy and outperforms state-of-the-art methods by 1.5-186.4x in throughput, while significantly reducing storage overhead on 6 real-world datasets.", "conclusion": "Allan-Poe is a novel all-in-one graph index that enables efficient and accurate hybrid search by unifying multiple retrieval paths in a single, cohesive structure."}}
{"id": "2511.00444", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00444", "abs": "https://arxiv.org/abs/2511.00444", "authors": ["Benjamin Clavi\u00e9", "Xianming Li", "Antoine Chaffin", "Omar Khattab", "Tom Aarsen", "Manuel Faysse", "Jing Li"], "title": "LIR: The First Workshop on Late Interaction and Multi Vector Retrieval @ ECIR 2026", "comment": "Accepted workshop at ECIR 2026", "summary": "Late interaction retrieval methods, pioneered by ColBERT, have emerged as a\npowerful alternative to single-vector neural IR. By leveraging fine-grained,\ntoken-level representations, they have been demonstrated to deliver strong\ngeneralisation and robustness, particularly in out-of-domain settings. They\nhave recently been shown to be particularly well-suited for novel use cases,\nsuch as reasoning-based or cross-modality retrieval. At the same time, these\nmodels pose significant challenges of efficiency, usability, and integrations\ninto fully fledged systems; as well as the natural difficulties encountered\nwhile researching novel application domains. Recent years have seen rapid\nadvances across many of these areas, but research efforts remain fragmented\nacross communities and frequently exclude practitioners. The purpose of this\nworkshop is to create an environment where all aspects of late interaction can\nbe discussed, with a focus on early research explorations, real-world outcomes,\nand negative or puzzling results to be freely shared and discussed. The aim of\nLIR is to provide a highly-interactive environment for researchers from various\nbackgrounds and practitioners to freely discuss their experience, fostering\nfurther collaboration.", "AI": {"tldr": "\u672c\u6b21\u7814\u8ba8\u4f1a\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e00\u4e2a\u73af\u5883\uff0c\u8ba8\u8bba\u665a\u671f\u4ea4\u4e92\u7684\u5404\u4e2a\u65b9\u9762\uff0c\u91cd\u70b9\u662f\u65e9\u671f\u7814\u7a76\u63a2\u7d22\u3001\u5b9e\u9645\u6210\u679c\u4ee5\u53ca\u53ef\u4ee5\u81ea\u7531\u5206\u4eab\u548c\u8ba8\u8bba\u7684\u8d1f\u9762\u6216\u4ee4\u4eba\u56f0\u60d1\u7684\u7ed3\u679c\uff0c\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7684\u5408\u4f5c\u3002", "motivation": "\u665a\u671f\u4ea4\u4e92\u68c0\u7d22\u65b9\u6cd5\u5df2\u6210\u4e3a\u5355\u5411\u91cf\u795e\u7ecf\u4fe1\u606f\u68c0\u7d22\u7684\u5f3a\u5927\u66ff\u4ee3\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u9886\u57df\u5916\u8bbe\u7f6e\u4e2d\uff0c\u5b83\u4eec\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u7279\u522b\u9002\u5408\u57fa\u4e8e\u63a8\u7406\u6216\u8de8\u6a21\u6001\u68c0\u7d22\u7b49\u65b0\u9896\u7528\u4f8b\u3002\u4f46\u8fd9\u4e9b\u6a21\u578b\u5728\u6548\u7387\u3001\u53ef\u7528\u6027\u548c\u96c6\u6210\u5230\u5b8c\u5584\u7684\u7cfb\u7edf\u4e2d\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u672c\u6b21\u7814\u8ba8\u4f1a\u65e8\u5728\u521b\u5efa\u4e00\u4e2a\u9ad8\u5ea6\u4e92\u52a8\u7684\u73af\u5883\uff0c\u4f9b\u6765\u81ea\u4e0d\u540c\u80cc\u666f\u7684\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u81ea\u7531\u8ba8\u8bba\u4ed6\u4eec\u7684\u7ecf\u9a8c\u3002", "result": "\u91cd\u70b9\u8ba8\u8bba\u65e9\u671f\u7814\u7a76\u63a2\u7d22\u3001\u5b9e\u9645\u6210\u679c\u4ee5\u53ca\u53ef\u4ee5\u81ea\u7531\u5206\u4eab\u548c\u8ba8\u8bba\u7684\u8d1f\u9762\u6216\u4ee4\u4eba\u56f0\u60d1\u7684\u7ed3\u679c\u3002", "conclusion": "LIR \u7684\u76ee\u6807\u662f\u4e3a\u6765\u81ea\u4e0d\u540c\u80cc\u666f\u7684\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e00\u4e2a\u9ad8\u5ea6\u4e92\u52a8\u7684\u73af\u5883\uff0c\u81ea\u7531\u8ba8\u8bba\u4ed6\u4eec\u7684\u7ecf\u9a8c\uff0c\u4ece\u800c\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7684\u5408\u4f5c\u3002"}}
{"id": "2511.00032", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00032", "abs": "https://arxiv.org/abs/2511.00032", "authors": ["Lei Liu", "Zhongyi Yu", "Hong Wang", "Huanshuo Dong", "Haiyang Xin", "Hongwei Zhao", "Bin Li"], "title": "From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators", "comment": null, "summary": "In recent years, Neural Operators(NO) have gradually emerged as a popular\napproach for solving Partial Differential Equations (PDEs). However, their\napplication to large-scale engineering tasks suffers from significant\ncomputational overhead. And the fact that current models impose a uniform\ncomputational cost while physical fields exhibit vastly different complexities\nconstitutes a fundamental mismatch, which is the root of this inefficiency. For\ninstance, in turbulence flows, intricate vortex regions require deeper network\nprocessing compared to stable flows. To address this, we introduce a framework:\nSkip-Block Routing (SBR), a general framework designed for Transformer-based\nneural operators, capable of being integrated into their multi-layer\narchitectures. First, SBR uses a routing mechanism to learn the complexity and\nranking of tokens, which is then applied during inference. Then, in later\nlayers, it decides how many tokens are passed forward based on this ranking.\nThis way, the model focuses more processing capacity on the tokens that are\nmore complex. Experiments demonstrate that SBR is a general framework that\nseamlessly integrates into various neural operators. Our method reduces\ncomputational cost by approximately 50% in terms of Floating Point Operations\n(FLOPs), while still delivering up to 2x faster inference without sacrificing\naccuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86Skip-Block Routing (SBR) \u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u795e\u7ecf\u7b97\u5b50\u5728\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u65f6\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5bf9\u6240\u6709\u7269\u7406\u573a\u65bd\u52a0\u7edf\u4e00\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u800c\u7269\u7406\u573a\u8868\u73b0\u51fa\u622a\u7136\u4e0d\u540c\u7684\u590d\u6742\u6027\uff0c\u8fd9\u662f\u4e00\u4e2a\u6839\u672c\u7684\u4e0d\u5339\u914d\uff0c\u662f\u6548\u7387\u4f4e\u4e0b\u7684\u6839\u6e90\u3002\u4f8b\u5982\uff0c\u5728\u6e4d\u6d41\u4e2d\uff0c\u4e0e\u7a33\u5b9a\u6d41\u52a8\u76f8\u6bd4\uff0c\u590d\u6742\u7684\u6da1\u6d41\u533a\u57df\u9700\u8981\u66f4\u6df1\u7684\u7f51\u7edc\u5904\u7406\u3002", "method": "SBR \u4f7f\u7528\u8def\u7531\u673a\u5236\u6765\u5b66\u4e60 tokens \u7684\u590d\u6742\u6027\u548c\u6392\u5e8f\uff0c\u7136\u540e\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5e94\u7528\u3002\u7136\u540e\uff0c\u5728\u540e\u9762\u7684\u5c42\u4e2d\uff0c\u5b83\u6839\u636e\u6b64\u6392\u5e8f\u51b3\u5b9a\u4f20\u9012\u591a\u5c11 tokens\u3002", "result": "SBR \u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u5c06\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u4e86\u5927\u7ea6 50%\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u8fbe 2 \u500d\u7684\u66f4\u5feb\u63a8\u7406\u3002", "conclusion": "SBR \u662f\u4e00\u79cd\u901a\u7528\u6846\u67b6\uff0c\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u5404\u79cd\u795e\u7ecf\u7b97\u5b50\u4e2d\u3002"}}
{"id": "2511.00198", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00198", "abs": "https://arxiv.org/abs/2511.00198", "authors": ["Chun-Hao Yang", "Bo-Han Feng", "Tzu-Yuan Lai", "Yan Yu Chen", "Yin-Kai Dean Huang", "Shou-De Lin"], "title": "Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap", "comment": null, "summary": "Optimizing training performance in large language models (LLMs) remains an\nessential challenge, particularly in improving model performance while\nmaintaining computational costs. This work challenges the conventional approach\nof training LLMs using next-token prediction (NTP), arguing that by predicting\ninformation-rich tokens during training, there is a more effective way to train\nLLMs. We investigate the impact of the proposed solution in three kinds of\ntasks for LLMs: arithmetic, multi-label classification of text, and\nnatural-language generation. This work offers a principled approach to\noptimizing LLM training, advancing both model performance and theoretical\nunderstanding of the target-token selection strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLM\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u4fe1\u606f\u4e30\u5bcc\u7684tokens\u6765\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u7684LLM\u8bad\u7ec3\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u7814\u7a76\u4e86\u5728LLM\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9884\u6d4b\u4fe1\u606f\u4e30\u5bcctokens\u7684\u5f71\u54cd\u3002", "result": "\u5728\u7b97\u672f\u3001\u6587\u672c\u591a\u6807\u7b7e\u5206\u7c7b\u548c\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4e09\u4e2a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f18\u5316LLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u9014\u5f84\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u5bf9\u76ee\u6807token\u9009\u62e9\u7b56\u7565\u7684\u7406\u8bba\u7406\u89e3\u3002"}}
{"id": "2511.00028", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00028", "abs": "https://arxiv.org/abs/2511.00028", "authors": ["Hanyang Chen", "Yanchao Yang"], "title": "Mutual Information guided Visual Contrastive Learning", "comment": "Tech Report - Undergraduate Thesis - 2023", "summary": "Representation learning methods utilizing the InfoNCE loss have demonstrated\nconsiderable capacity in reducing human annotation effort by training invariant\nneural feature extractors. Although different variants of the training\nobjective adhere to the information maximization principle between the data and\nlearned features, data selection and augmentation still rely on human\nhypotheses or engineering, which may be suboptimal. For instance, data\naugmentation in contrastive learning primarily focuses on color jittering,\naiming to emulate real-world illumination changes. In this work, we investigate\nthe potential of selecting training data based on their mutual information\ncomputed from real-world distributions, which, in principle, should endow the\nlearned features with better generalization when applied in open environments.\nSpecifically, we consider patches attached to scenes that exhibit high mutual\ninformation under natural perturbations, such as color changes and motion, as\npositive samples for learning with contrastive loss. We evaluate the proposed\nmutual-information-informed data augmentation method on several benchmarks\nacross multiple state-of-the-art representation learning frameworks,\ndemonstrating its effectiveness and establishing it as a promising direction\nfor future research.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u8868\u5f81\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5bf9\u6bd4\u5b66\u4e60\u7684\u6570\u636e\u589e\u5f3a\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\uff0c\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u7684\u3002", "method": "\u8be5\u65b9\u6cd5\u9009\u62e9\u5728\u81ea\u7136\u6270\u52a8\u4e0b\u5177\u6709\u9ad8\u4e92\u4fe1\u606f\u7684\u56fe\u50cf\u5757\u4f5c\u4e3a\u6b63\u6837\u672c\uff0c\u7528\u4e8e\u5bf9\u6bd4\u635f\u5931\u5b66\u4e60\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2511.00092", "categories": ["cs.AI", "cs.CL", "cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.00092", "abs": "https://arxiv.org/abs/2511.00092", "authors": ["Shunya Minami", "Tatsuya Ishigaki", "Ikko Hamamura", "Taku Mikuriya", "Youmi Ma", "Naoaki Okazaki", "Hiroya Takamura", "Yohichi Suzuki", "Tadashi Kadowaki"], "title": "QuantumBench: A Benchmark for Quantum Problem Solving", "comment": "11 pages, 8 figures", "summary": "Large language models are now integrated into many scientific workflows,\naccelerating data analysis, hypothesis generation, and design space\nexploration. In parallel with this growth, there is a growing need to carefully\nevaluate whether models accurately capture domain-specific knowledge and\nnotation, since general-purpose benchmarks rarely reflect these requirements.\nThis gap is especially clear in quantum science, which features non-intuitive\nphenomena and requires advanced mathematics. In this study, we introduce\nQuantumBench, a benchmark for the quantum domain that systematically examine\nhow well LLMs understand and can be applied to this non-intuitive field. Using\npublicly available materials, we compiled approximately 800 questions with\ntheir answers spanning nine areas related to quantum science and organized them\ninto an eight-option multiple-choice dataset. With this benchmark, we evaluate\nseveral existing LLMs and analyze their performance in the quantum domain,\nincluding sensitivity to changes in question format. QuantumBench is the first\nLLM evaluation dataset built for the quantum domain, and it is intended to\nguide the effective use of LLMs in quantum research.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86QuantumBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u91cf\u5b50\u79d1\u5b66\u9886\u57df\u80fd\u529b\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u51c6\u786e\u6355\u6349\u7279\u5b9a\u9886\u57df\u7684\u77e5\u8bc6\u548c\u7b26\u53f7\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u975e\u76f4\u89c2\u73b0\u8c61\u548c\u9700\u8981\u9ad8\u7b49\u6570\u5b66\u7684\u91cf\u5b50\u79d1\u5b66\u9886\u57df\u3002", "method": "\u4f5c\u8005\u5229\u7528\u516c\u5f00\u53ef\u7528\u7684\u6750\u6599\uff0c\u6574\u7406\u4e86\u7ea6800\u4e2a\u95ee\u9898\u53ca\u5176\u7b54\u6848\uff0c\u6db5\u76d6\u91cf\u5b50\u79d1\u5b66\u76f8\u5173\u7684\u4e5d\u4e2a\u9886\u57df\uff0c\u5e76\u5c06\u5b83\u4eec\u7ec4\u7ec7\u6210\u4e00\u4e2a\u516b\u4e2a\u9009\u9879\u7684\u591a\u9879\u9009\u62e9\u9898\u6570\u636e\u96c6\u3002", "result": "\u4f5c\u8005\u4f7f\u7528\u8be5\u57fa\u51c6\u8bc4\u4f30\u4e86\u51e0\u4e2a\u73b0\u6709\u7684LLM\uff0c\u5e76\u5206\u6790\u4e86\u5b83\u4eec\u5728\u91cf\u5b50\u9886\u57df\u7684\u8868\u73b0\uff0c\u5305\u62ec\u5bf9\u95ee\u9898\u5f62\u5f0f\u53d8\u5316\u7684\u654f\u611f\u6027\u3002", "conclusion": "QuantumBench\u662f\u7b2c\u4e00\u4e2a\u4e3a\u91cf\u5b50\u9886\u57df\u6784\u5efa\u7684LLM\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u65e8\u5728\u6307\u5bfcLLM\u5728\u91cf\u5b50\u7814\u7a76\u4e2d\u7684\u6709\u6548\u4f7f\u7528\u3002"}}
{"id": "2511.00865", "categories": ["cs.DB", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.00865", "abs": "https://arxiv.org/abs/2511.00865", "authors": ["Hangdong Zhao", "Zhenghong Yu", "Srinag Rao", "Simon Frisk", "Zhiwei Fan", "Paraschos Koutris"], "title": "FlowLog: Efficient and Extensible Datalog via Incrementality", "comment": "Accepted to VLDB 2026", "summary": "Datalog-based languages are regaining popularity as a powerful abstraction\nfor expressing recursive computations in domains such as program analysis and\ngraph processing. However, existing systems often face a trade-off between\nefficiency and extensibility. Engines like Souffle achieve high efficiency\nthrough domain-specific designs, but lack general-purpose flexibility. Others,\nlike RecStep, offer modularity by layering Datalog on traditional databases,\nbut struggle to integrate Datalog-specific optimizations.\n  This paper bridges this gap by presenting FlowLog, a new Datalog engine that\nuses an explicit relational IR per-rule to cleanly separate recursive control\n(e.g., semi-naive execution) from each rule's logical plan. This boundary lets\nus retain fine-grained, Datalog-aware optimizations at the logical layer, but\nalso reuse off-the-shelf database primitives at execution. At the logical level\n(i.e. IR), we apply proven SQL optimizations, such as logic fusion and subplan\nreuse. To address high volatility in recursive workloads, we adopt a\nrobustness-first approach that pairs a structural optimizer (avoiding\nworst-case joins) with sideways information passing (early filtering). Built\natop Differential Dataflow--a mature framework for streaming analytics--FlowLog\nsupports both batch and incremental Datalog and adds novel recursion-aware\noptimizations called Boolean (or algebraic) specialization. Our evaluation\nshows that FlowLog outperforms state-of-the-art Datalog engines and modern\ndatabases across a broad range of recursive workloads, achieving superior\nscalability while preserving a simple and extensible architecture.", "AI": {"tldr": "FlowLog\u662f\u4e00\u4e2a\u65b0\u7684Datalog\u5f15\u64ce\uff0c\u5b83\u901a\u8fc7\u6bcf\u4e2a\u89c4\u5219\u7684\u663e\u5f0f\u5173\u7cfbIR\u6765\u6e05\u6670\u5730\u5206\u79bb\u9012\u5f52\u63a7\u5236\u548c\u6bcf\u4e2a\u89c4\u5219\u7684\u903b\u8f91\u8ba1\u5212\uff0c\u4ece\u800c\u5f25\u5408\u4e86\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u7684\u7cfb\u7edf\u901a\u5e38\u9762\u4e34\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002\u50cfSouffle\u8fd9\u6837\u7684\u5f15\u64ce\u901a\u8fc7\u7279\u5b9a\u4e8e\u9886\u57df\u7684\u8bbe\u8ba1\u5b9e\u73b0\u9ad8\u6548\u7387\uff0c\u4f46\u7f3a\u4e4f\u901a\u7528\u7684\u7075\u6d3b\u6027\u3002\u5176\u4ed6\u7684\uff0c\u50cfRecStep\uff0c\u901a\u8fc7\u5728\u4f20\u7edf\u6570\u636e\u5e93\u4e0a\u5206\u5c42Datalog\u6765\u63d0\u4f9b\u6a21\u5757\u5316\uff0c\u4f46\u96be\u4ee5\u6574\u5408Datalog\u7279\u5b9a\u7684\u4f18\u5316\u3002", "method": "FlowLog\u4f7f\u7528\u6bcf\u4e2a\u89c4\u5219\u7684\u663e\u5f0f\u5173\u7cfbIR\u6765\u6e05\u6670\u5730\u5206\u79bb\u9012\u5f52\u63a7\u5236\uff08\u4f8b\u5982\uff0c\u534a\u6734\u7d20\u6267\u884c\uff09\u548c\u6bcf\u4e2a\u89c4\u5219\u7684\u903b\u8f91\u8ba1\u5212\u3002\u5728\u903b\u8f91\u5c42\u9762\uff08\u5373IR\uff09\uff0c\u6211\u4eec\u5e94\u7528\u4e86\u7ecf\u8fc7\u9a8c\u8bc1\u7684SQL\u4f18\u5316\uff0c\u4f8b\u5982\u903b\u8f91\u878d\u5408\u548c\u5b50\u8ba1\u5212\u91cd\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u9012\u5f52\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7684\u9ad8\u6ce2\u52a8\u6027\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u4e00\u79cd\u7a33\u5065\u6027\u4f18\u5148\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u7ed3\u6784\u4f18\u5316\u5668\uff08\u907f\u514d\u6700\u574f\u60c5\u51b5\u7684\u8fde\u63a5\uff09\u4e0e\u6a2a\u5411\u4fe1\u606f\u4f20\u9012\uff08\u65e9\u671f\u8fc7\u6ee4\uff09\u76f8\u7ed3\u5408\u3002FlowLog\u6784\u5efa\u5728Differential Dataflow\u4e4b\u4e0a\uff0c\u8fd9\u662f\u4e00\u4e2a\u6210\u719f\u7684\u6d41\u5206\u6790\u6846\u67b6\uff0c\u652f\u6301\u6279\u5904\u7406\u548c\u589e\u91cfDatalog\uff0c\u5e76\u589e\u52a0\u4e86\u65b0\u9896\u7684\u9012\u5f52\u611f\u77e5\u4f18\u5316\uff0c\u79f0\u4e3a\u5e03\u5c14\uff08\u6216\u4ee3\u6570\uff09\u4e13\u4e1a\u5316\u3002", "result": "FlowLog\u5728\u5e7f\u6cdb\u7684\u9012\u5f52\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684Datalog\u5f15\u64ce\u548c\u73b0\u4ee3\u6570\u636e\u5e93\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u53ef\u6269\u5c55\u6027\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u67b6\u6784\u3002", "conclusion": "FlowLog\u662f\u4e00\u4e2a\u5f88\u6709\u524d\u9014\u7684Datalog\u5f15\u64ce\uff0c\u5b83\u5728\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u548c\u7075\u6d3b\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2511.00530", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.00530", "abs": "https://arxiv.org/abs/2511.00530", "authors": ["Hongtao Huang", "Chengkai Huang", "Junda Wu", "Tong Yu", "Julian McAuley", "Lina Yao"], "title": "Listwise Preference Diffusion Optimization for User Behavior Trajectories Prediction", "comment": null, "summary": "Forecasting multi-step user behavior trajectories requires reasoning over\nstructured preferences across future actions, a challenge overlooked by\ntraditional sequential recommendation. This problem is critical for\napplications such as personalized commerce and adaptive content delivery, where\nanticipating a user's complete action sequence enhances both satisfaction and\nbusiness outcomes. We identify an essential limitation of existing paradigms:\ntheir inability to capture global, listwise dependencies among sequence items.\nTo address this, we formulate User Behavior Trajectory Prediction (UBTP) as a\nnew task setting that explicitly models long-term user preferences. We\nintroduce Listwise Preference Diffusion Optimization (LPDO), a diffusion-based\ntraining framework that directly optimizes structured preferences over entire\nitem sequences. LPDO incorporates a Plackett-Luce supervision signal and\nderives a tight variational lower bound aligned with listwise ranking\nlikelihoods, enabling coherent preference generation across denoising steps and\novercoming the independent-token assumption of prior diffusion methods. To\nrigorously evaluate multi-step prediction quality, we propose the task-specific\nmetric Sequential Match (SeqMatch), which measures exact trajectory agreement,\nand adopt Perplexity (PPL), which assesses probabilistic fidelity. Extensive\nexperiments on real-world user behavior benchmarks demonstrate that LPDO\nconsistently outperforms state-of-the-art baselines, establishing a new\nbenchmark for structured preference learning with diffusion models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7528\u6237\u884c\u4e3a\u8f68\u8ff9\u9884\u6d4b\uff08UBTP\uff09\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u8bad\u7ec3\u6846\u67b6LPDO\uff0c\u4ee5\u4f18\u5316\u6574\u4e2a\u9879\u76ee\u5e8f\u5217\u4e2d\u7684\u7ed3\u6784\u5316\u504f\u597d\u3002", "motivation": "\u73b0\u6709\u8303\u4f8b\u65e0\u6cd5\u6355\u6349\u5e8f\u5217\u9879\u76ee\u4e4b\u95f4\u7684\u5168\u5c40\u5217\u8868\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86Plackett-Luce\u76d1\u7763\u4fe1\u53f7\uff0c\u5e76\u63a8\u5bfc\u51fa\u4e00\u4e2a\u4e0e\u5217\u8868\u6392\u5e8f\u4f3c\u7136\u5bf9\u9f50\u7684\u4e25\u683c\u53d8\u5206\u4e0b\u754c\uff0c\u4ece\u800c\u5728\u53bb\u566a\u6b65\u9aa4\u4e2d\u5b9e\u73b0\u8fde\u8d2f\u7684\u504f\u597d\u751f\u6210\uff0c\u5e76\u514b\u670d\u4e86\u5148\u524d\u6269\u6563\u65b9\u6cd5\u7684\u72ec\u7acbtoken\u5047\u8bbe\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7528\u6237\u884c\u4e3a\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLPDO\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4f7f\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u7ed3\u6784\u5316\u504f\u597d\u5b66\u4e60\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2511.00035", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00035", "abs": "https://arxiv.org/abs/2511.00035", "authors": ["Georg Velev", "Stefan Lessmann"], "title": "Neural Architecture Search for global multi-step Forecasting of Energy Production Time Series", "comment": null, "summary": "The dynamic energy sector requires both predictive accuracy and runtime\nefficiency for short-term forecasting of energy generation under operational\nconstraints, where timely and precise predictions are crucial. The manual\nconfiguration of complex methods, which can generate accurate global multi-step\npredictions without suffering from a computational bottleneck, represents a\nprocedure with significant time requirements and high risk for human-made\nerrors. A further intricacy arises from the temporal dynamics present in\nenergy-related data. Additionally, the generalization to unseen data is\nimperative for continuously deploying forecasting techniques over time. To\novercome these challenges, in this research, we design a neural architecture\nsearch (NAS)-based framework for the automated discovery of time series models\nthat strike a balance between computational efficiency, predictive performance,\nand generalization power for the global, multi-step short-term forecasting of\nenergy production time series. In particular, we introduce a search space\nconsisting only of efficient components, which can capture distinctive patterns\nof energy time series. Furthermore, we formulate a novel objective function\nthat accounts for performance generalization in temporal context and the\nmaximal exploration of different regions of our high-dimensional search space.\nThe results obtained on energy production time series show that an ensemble of\nlightweight architectures discovered with NAS outperforms state-of-the-art\ntechniques, such as Transformers, as well as pre-trained forecasting models, in\nterms of both efficiency and accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u53d1\u73b0\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff0c\u4ee5\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u3001\u9884\u6d4b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u7528\u4e8e\u80fd\u6e90\u751f\u4ea7\u65f6\u95f4\u5e8f\u5217\u7684\u5168\u5c40\u591a\u6b65\u77ed\u671f\u9884\u6d4b\u3002", "motivation": "\u80fd\u6e90\u9886\u57df\u7684\u77ed\u671f\u9884\u6d4b\u9700\u8981\u9884\u6d4b\u7cbe\u5ea6\u548c\u8fd0\u884c\u6548\u7387\uff0c\u624b\u52a8\u914d\u7f6e\u590d\u6742\u65b9\u6cd5\u8017\u65f6\u4e14\u5bb9\u6613\u51fa\u9519\uff0c\u80fd\u6e90\u76f8\u5173\u6570\u636e\u5b58\u5728\u65f6\u95f4\u52a8\u6001\u6027\uff0c\u9700\u8981\u63a8\u5e7f\u5230\u672a\u89c1\u6570\u636e\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eNAS\u7684\u6846\u67b6\uff0c\u641c\u7d22\u7a7a\u95f4\u4ec5\u5305\u542b\u9ad8\u6548\u7ec4\u4ef6\uff0c\u53ef\u4ee5\u6355\u83b7\u80fd\u6e90\u65f6\u95f4\u5e8f\u5217\u7684\u72ec\u7279\u6a21\u5f0f\u3002\u6b64\u5916\uff0c\u8fd8\u5236\u5b9a\u4e86\u4e00\u4e2a\u65b0\u7684\u76ee\u6807\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u8003\u8651\u4e86\u65f6\u95f4\u80cc\u666f\u4e0b\u7684\u6027\u80fd\u6cdb\u5316\u548c\u9ad8\u7ef4\u641c\u7d22\u7a7a\u95f4\u4e0d\u540c\u533a\u57df\u7684\u6700\u5927\u63a2\u7d22\u3002", "result": "\u5728\u80fd\u6e90\u751f\u4ea7\u65f6\u95f4\u5e8f\u5217\u4e0a\u83b7\u5f97\u7684\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7NAS\u53d1\u73b0\u7684\u8f7b\u91cf\u7ea7\u67b6\u6784\u96c6\u5408\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6280\u672f\uff0c\u5982Transformers\u4ee5\u53ca\u9884\u8bad\u7ec3\u7684\u9884\u6d4b\u6a21\u578b\u3002", "conclusion": "NAS\u53ef\u4ee5\u81ea\u52a8\u53d1\u73b0\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u80fd\u6e90\u751f\u4ea7\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u3002"}}
{"id": "2511.00222", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00222", "abs": "https://arxiv.org/abs/2511.00222", "authors": ["Marwa Abdulhai", "Ryan Cheng", "Donovan Clay", "Tim Althoff", "Sergey Levine", "Natasha Jaques"], "title": "Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to simulate human users in\ninteractive settings such as therapy, education, and social role-play. While\nthese simulations enable scalable training and evaluation of AI agents,\noff-the-shelf LLMs often drift from their assigned personas, contradict earlier\nstatements, or abandon role-appropriate behavior. We introduce a unified\nframework for evaluating and improving persona consistency in LLM-generated\ndialogue. We define three automatic metrics: prompt-to-line consistency,\nline-to-line consistency, and Q&A consistency, that capture different types of\npersona drift and validate each against human annotations. Using these metrics\nas reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs\nfor three user roles: a patient, a student, and a social chat partner. Our\nmethod reduces inconsistency by over 55%, resulting in more coherent and\nfaithful simulated users.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u63d0\u9ad8LLM\u751f\u6210\u5bf9\u8bdd\u4e2d\u89d2\u8272\u4e00\u81f4\u6027", "motivation": "\u73b0\u6709\u7684LLM\u5728\u4ea4\u4e92\u73af\u5883\u4e2d\u6a21\u62df\u4eba\u7c7b\u7528\u6237\u65f6\uff0c\u7ecf\u5e38\u51fa\u73b0\u89d2\u8272\u6f02\u79fb\u3001\u524d\u540e\u77db\u76fe\u6216\u653e\u5f03\u89d2\u8272\u884c\u4e3a\u7684\u95ee\u9898", "method": "\u5b9a\u4e49\u4e86\u4e09\u79cd\u81ea\u52a8\u6307\u6807\uff08prompt-to-line consistency, line-to-line consistency, Q&A consistency\uff09\u6765\u6355\u6349\u4e0d\u540c\u7c7b\u578b\u7684\u89d2\u8272\u6f02\u79fb\uff0c\u5e76\u4f7f\u7528\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u5bf9LLM\u8fdb\u884c\u5fae\u8c03", "result": "\u8be5\u65b9\u6cd5\u5c06\u4e0d\u4e00\u81f4\u6027\u964d\u4f4e\u4e8655%\u4ee5\u4e0a", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u66f4\u8fde\u8d2f\u548c\u5fe0\u5b9e\u7684\u89d2\u8272\u6a21\u62df\u7528\u6237"}}
{"id": "2511.00037", "categories": ["cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.00037", "abs": "https://arxiv.org/abs/2511.00037", "authors": ["Riya Gupta", "Alexander Chowdhury", "Sahil Nalawade"], "title": "Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra", "comment": null, "summary": "Federated Learning (FL) has emerged as a transformative paradigm in medical\nAI, enabling collaborative model training across institutions without direct\ndata sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE,\nFlower, and Owkin Substra to evaluate their suitability for medical imaging\napplications in real-world settings. Using the PathMNIST dataset, we assess\nmodel performance, convergence efficiency, communication overhead, scalability,\nand developer experience. Results indicate that NVIDIA FLARE offers superior\nproduction scalability, Flower provides flexibility for prototyping and\nacademic research, and Owkin Substra demonstrates exceptional privacy and\ncompliance features. Each framework exhibits strengths optimized for distinct\nuse cases, emphasizing their relevance to practical deployment in healthcare\nenvironments.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9\u4e09\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff08NVIDIA FLARE\u3001Flower \u548c Owkin Substra\uff09\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u8bc4\u4f30\u5b83\u4eec\u5728\u533b\u5b66\u6210\u50cf\u5e94\u7528\u4e2d\u7684\u9002\u7528\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60 (FL) \u5df2\u6210\u4e3a\u533b\u5b66\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u4e00\u79cd\u53d8\u9769\u6027\u8303\u4f8b\uff0c\u53ef\u4ee5\u5728\u4e0d\u76f4\u63a5\u5171\u4eab\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u8de8\u673a\u6784\u7684\u534f\u4f5c\u6a21\u578b\u8bad\u7ec3\u3002", "method": "\u4f7f\u7528 PathMNIST \u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u6a21\u578b\u6027\u80fd\u3001\u6536\u655b\u6548\u7387\u3001\u901a\u4fe1\u5f00\u9500\u3001\u53ef\u6269\u5c55\u6027\u548c\u5f00\u53d1\u8005\u4f53\u9a8c\u3002", "result": "NVIDIA FLARE \u5177\u6709\u5353\u8d8a\u7684\u751f\u4ea7\u53ef\u6269\u5c55\u6027\uff0cFlower \u4e3a\u539f\u578b\u8bbe\u8ba1\u548c\u5b66\u672f\u7814\u7a76\u63d0\u4f9b\u4e86\u7075\u6d3b\u6027\uff0c\u800c Owkin Substra \u5219\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u9690\u79c1\u548c\u5408\u89c4\u6027\u529f\u80fd\u3002", "conclusion": "\u6bcf\u4e2a\u6846\u67b6\u90fd\u5c55\u793a\u4e86\u9488\u5bf9\u4e0d\u540c\u7528\u4f8b\u4f18\u5316\u7684\u4f18\u52bf\uff0c\u5f3a\u8c03\u4e86\u5b83\u4eec\u4e0e\u533b\u7597\u73af\u5883\u4e2d\u5b9e\u9645\u90e8\u7f72\u7684\u76f8\u5173\u6027\u3002"}}
{"id": "2511.00122", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00122", "abs": "https://arxiv.org/abs/2511.00122", "authors": ["Ran Xu", "Yupeng Qi", "Jingsen Feng", "Xu Chu"], "title": "Engineering.ai: A Platform for Teams of AI Engineers in Computational Design", "comment": null, "summary": "In modern engineering practice, human engineers collaborate in specialized\nteams to design complex products, with each expert completing their respective\ntasks while communicating and exchanging results and data with one another.\nWhile this division of expertise is essential for managing multidisciplinary\ncomplexity, it demands substantial development time and cost. Recently, we\nintroduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer\nfor computational fluid dynamics, and turbulence.ai, which can conduct\nend-to-end research in fluid mechanics draft publications and PhD theses.\nBuilding upon these foundations, we present Engineering.ai, a platform for\nteams of AI engineers in computational design. The framework employs a\nhierarchical multi-agent architecture where a Chief Engineer coordinates\nspecialized agents consisting of Aerodynamics, Structural, Acoustic, and\nOptimization Engineers, each powered by LLM with domain-specific knowledge.\nAgent-agent collaboration is achieved through file-mediated communication for\ndata provenance and reproducibility, while a comprehensive memory system\nmaintains project context, execution history, and retrieval-augmented domain\nknowledge to ensure reliable decision-making across the workflow. The system\nintegrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,\nenabling parallel multidisciplinary simulations while maintaining computational\naccuracy. The framework is validated through UAV wing optimization. This work\ndemonstrates that agentic-AI-enabled AI engineers has the potential to perform\ncomplex engineering tasks autonomously. Remarkably, the automated workflow\nachieved a 100% success rate across over 400 parametric configurations, with\nzero mesh generation failures, solver convergence issues, or manual\ninterventions required, validating that the framework is trustworthy.", "AI": {"tldr": "Engineering.ai is a platform for AI engineers to collaboratively design complex products.", "motivation": "Modern engineering design is complex, time-consuming, and costly due to the need for specialized teams and extensive communication.", "method": "A hierarchical multi-agent architecture is used, with a Chief Engineer coordinating specialized agents (Aerodynamics, Structural, Acoustic, and Optimization) powered by LLMs. Communication is file-mediated, and a memory system maintains project context.", "result": "The system achieved a 100% success rate across over 400 parametric configurations in UAV wing optimization, with no failures or manual interventions.", "conclusion": "Agentic-AI-enabled AI engineers have the potential to perform complex engineering tasks autonomously, and the framework is trustworthy."}}
{"id": "2511.00985", "categories": ["cs.DB", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00985", "abs": "https://arxiv.org/abs/2511.00985", "authors": ["Yiwen Jiao", "Tonghui Ren", "Yuche Gao", "Zhenying He", "Yinan Jing", "Kai Zhang", "X. Sean Wang"], "title": "ORANGE: An Online Reflection ANd GEneration framework with Domain Knowledge for Text-to-SQL", "comment": "16 pages, 4 figures, preprint", "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in\ntranslating natural language to SQL, but a significant semantic gap persists\nbetween their general knowledge and domain-specific semantics of databases.\nHistorical translation logs constitute a rich source of this missing in-domain\nknowledge, where SQL queries inherently encapsulate real-world usage patterns\nof database schema. Existing methods primarily enhance the reasoning process\nfor individual translations but fail to accumulate in-domain knowledge from\npast translations. We introduce ORANGE, an online self-evolutionary framework\nthat constructs database-specific knowledge bases by parsing SQL queries from\ntranslation logs. By accumulating in-domain knowledge that contains schema and\ndata semantics, ORANGE progressively reduces the semantic gap and enhances the\naccuracy of subsequent SQL translations. To ensure reliability, we propose a\nnovel nested Chain-of-Thought SQL-to-Text strategy with tuple-semantic\ntracking, which reduces semantic errors during knowledge generation.\nExperiments on multiple benchmarks confirm the practicality of ORANGE,\ndemonstrating its effectiveness for real-world Text-to-SQL deployment,\nparticularly in handling complex and domain-specific queries.", "AI": {"tldr": "ORANGE: \u4f7f\u7528\u5728\u7ebf\u81ea\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u6790\u7ffb\u8bd1\u65e5\u5fd7\u4e2d\u7684SQL\u67e5\u8be2\u6765\u6784\u5efa\u7279\u5b9a\u4e8e\u6570\u636e\u5e93\u7684\u77e5\u8bc6\u5e93\uff0c\u4ee5\u7f29\u5c0f\u8bed\u4e49\u5dee\u8ddd\u5e76\u63d0\u9ad8SQL\u7ffb\u8bd1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5c06\u81ea\u7136\u8bed\u8a00\u7ffb\u8bd1\u6210SQL\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u7684\u901a\u7528\u77e5\u8bc6\u548c\u6570\u636e\u5e93\u7684\u9886\u57df\u7279\u5b9a\u8bed\u4e49\u4e4b\u95f4\u4ecd\u7136\u5b58\u5728\u663e\u8457\u7684\u8bed\u4e49\u5dee\u8ddd\u3002\u5386\u53f2\u7ffb\u8bd1\u65e5\u5fd7\u6784\u6210\u4e86\u8fd9\u79cd\u7f3a\u5931\u7684\u9886\u57df\u5185\u77e5\u8bc6\u7684\u4e30\u5bcc\u6765\u6e90\uff0c\u5176\u4e2dSQL\u67e5\u8be2\u672c\u8d28\u4e0a\u5c01\u88c5\u4e86\u6570\u636e\u5e93\u6a21\u5f0f\u7684\u771f\u5b9e\u4e16\u754c\u4f7f\u7528\u6a21\u5f0f\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u589e\u5f3a\u5355\u4e2a\u7ffb\u8bd1\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f46\u672a\u80fd\u4ece\u8fc7\u53bb\u7684\u7ffb\u8bd1\u4e2d\u79ef\u7d2f\u9886\u57df\u5185\u77e5\u8bc6\u3002", "method": "\u6211\u4eec\u5f15\u5165ORANGE\uff0c\u8fd9\u662f\u4e00\u79cd\u5728\u7ebf\u81ea\u8fdb\u5316\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u89e3\u6790\u7ffb\u8bd1\u65e5\u5fd7\u4e2d\u7684SQL\u67e5\u8be2\u6765\u6784\u5efa\u7279\u5b9a\u4e8e\u6570\u636e\u5e93\u7684\u77e5\u8bc6\u5e93\u3002\u4e3a\u4e86\u786e\u4fdd\u53ef\u9760\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5d4c\u5957Chain-of-Thought SQL-to-Text\u7b56\u7565\uff0c\u5177\u6709\u5143\u7ec4\u8bed\u4e49\u8ddf\u8e2a\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u77e5\u8bc6\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u8bed\u4e49\u9519\u8bef\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u5b9e\u4e86ORANGE\u7684\u5b9e\u7528\u6027\uff0c\u8bc1\u660e\u4e86\u5b83\u5728\u5b9e\u9645\u7684Text-to-SQL\u90e8\u7f72\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u548c\u7279\u5b9a\u9886\u57df\u7684\u67e5\u8be2\u65f6\u3002", "conclusion": "ORANGE\u901a\u8fc7\u79ef\u7d2f\u5305\u542b\u6a21\u5f0f\u548c\u6570\u636e\u8bed\u4e49\u7684\u9886\u57df\u5185\u77e5\u8bc6\uff0c\u9010\u6b65\u7f29\u5c0f\u4e86\u8bed\u4e49\u5dee\u8ddd\uff0c\u63d0\u9ad8\u4e86\u540e\u7eedSQL\u7ffb\u8bd1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2511.00584", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00584", "abs": "https://arxiv.org/abs/2511.00584", "authors": ["Ke Shi", "Yan Zhang", "Miao Zhang", "Lifan Chen", "Jiali Yi", "Kui Xiao", "Xiaoju Hou", "Zhifei Li"], "title": "Structurally Refined Graph Transformer for Multimodal Recommendation", "comment": "Comment: 13 pages, 7 figures, accepted by IEEE Transactions on\n  Multimedia 2025", "summary": "Multimodal recommendation systems utilize various types of information,\nincluding images and text, to enhance the effectiveness of recommendations. The\nkey challenge is predicting user purchasing behavior from the available data.\nCurrent recommendation models prioritize extracting multimodal information\nwhile neglecting the distinction between redundant and valuable data. They also\nrely heavily on a single semantic framework (e.g., local or global semantics),\nresulting in an incomplete or biased representation of user preferences,\nparticularly those less expressed in prior interactions. Furthermore, these\napproaches fail to capture the complex interactions between users and items,\nlimiting the model's ability to meet diverse users. To address these\nchallenges, we present SRGFormer, a structurally optimized multimodal\nrecommendation model. By modifying the transformer for better integration into\nour model, we capture the overall behavior patterns of users. Then, we enhance\nstructural information by embedding multimodal information into a hypergraph\nstructure to aid in learning the local structures between users and items.\nMeanwhile, applying self-supervised tasks to user-item collaborative signals\nenhances the integration of multimodal information, thereby revealing the\nrepresentational features inherent to the data's modality. Extensive\nexperiments on three public datasets reveal that SRGFormer surpasses previous\nbenchmark models, achieving an average performance improvement of 4.47 percent\non the Sports dataset. The code is publicly available online.", "AI": {"tldr": "SRGFormer: A structurally optimized multimodal recommendation model.", "motivation": "Current models neglect the distinction between redundant and valuable data, rely on a single semantic framework, and fail to capture the complex interactions between users and items.", "method": "Modifies the transformer, embeds multimodal information into a hypergraph structure, and applies self-supervised tasks to user-item collaborative signals.", "result": "SRGFormer surpasses previous benchmark models, achieving an average performance improvement of 4.47 percent on the Sports dataset.", "conclusion": "SRGFormer enhances the integration of multimodal information, thereby revealing the representational features inherent to the data's modality."}}
{"id": "2511.00040", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00040", "abs": "https://arxiv.org/abs/2511.00040", "authors": ["Seonggyun Lee", "Sungjun Lim", "Seojin Park", "Soeun Cheon", "Kyungwoo Song"], "title": "Semi-Supervised Preference Optimization with Limited Feedback", "comment": null, "summary": "The field of preference optimization has made outstanding contributions to\nthe alignment of language models with human preferences. Despite these\nadvancements, recent methods still rely heavily on substantial paired (labeled)\nfeedback data, leading to substantial resource expenditures. To address these\nchallenges, we study the problem of Semi-Supervised Preference Optimization\n(SSPO) in which the idea is to learn from both a small number of pairwise\npreference labels and a large pool of unpaired samples simultaneously. Our key\ntheoretical contribution proves the existence of an optimal reward threshold\ncapable of separating winning and losing responses with high probability, which\nenables a principled pseudo-labeling of unpaired data. By leveraging these\npseudo-labels, SSPO effectively distills latent preferences from large-scale\nunpaired data, thus maintaining human alignment while drastically reducing\nacquisition costs. Extensive experiments across datasets validate this\nremarkable data efficiency; for instance, SSPO trained with Llama3-8B-Instruct\non just 1% of UltraFeedback consistently surpasses strong baselines trained on\n10% of UltraFeedback.", "AI": {"tldr": "\u63d0\u51fa\u534a\u76d1\u7763\u504f\u597d\u4f18\u5316\uff08SSPO\uff09\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u5bf9\u5927\u91cf\u914d\u5bf9\u53cd\u9988\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u914d\u5bf9\u6807\u6ce8\u6570\u636e\uff0c\u5bfc\u81f4\u8d44\u6e90\u6d88\u8017\u5de8\u5927\u3002", "method": "\u5229\u7528\u5c11\u91cf\u914d\u5bf9\u504f\u597d\u6807\u7b7e\u548c\u5927\u91cf\u672a\u914d\u5bf9\u6837\u672c\uff0c\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u5b58\u5728\u6700\u4f73\u5956\u52b1\u9608\u503c\uff0c\u5bf9\u672a\u914d\u5bf9\u6570\u636e\u8fdb\u884c\u4f2a\u6807\u7b7e\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSSPO \u5177\u6709\u663e\u8457\u7684\u6570\u636e\u6548\u7387\uff0c\u4f8b\u5982\uff0c\u5728\u4ec5\u4f7f\u7528 1% UltraFeedback \u6570\u636e\u8bad\u7ec3\u7684 Llama3-8B-Instruct \u6a21\u578b\u4e0a\uff0cSSPO \u59cb\u7ec8\u4f18\u4e8e\u5728 10% UltraFeedback \u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u5f3a\u5927\u57fa\u7ebf\u3002", "conclusion": "SSPO \u80fd\u591f\u6709\u6548\u5730\u4ece\u5927\u89c4\u6a21\u672a\u914d\u5bf9\u6570\u636e\u4e2d\u63d0\u53d6\u6f5c\u5728\u504f\u597d\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u4eba\u7c7b\u5bf9\u9f50\u7684\u540c\u65f6\uff0c\u5927\u5e45\u964d\u4f4e\u83b7\u53d6\u6210\u672c\u3002"}}
{"id": "2511.00265", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.00265", "abs": "https://arxiv.org/abs/2511.00265", "authors": ["Arman Anwar", "Zefang Liu"], "title": "AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding", "comment": null, "summary": "Traditional cybersecurity tabletop exercises (TTXs) provide valuable training\nbut are often scripted, resource-intensive, and difficult to scale. We\nintroduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches\ngame that integrates large language model teammates with a Bloom-aligned,\nretrieval-augmented copilot (C2D2). The system expands a curated corpus into\nfactual, conceptual, procedural, and metacognitive snippets, delivering\non-demand, cognitively targeted hints. Prompt-engineered agents employ a\nscaffolding ladder that gradually fades as learner confidence grows. In a\nsolo-player pilot with four graduate students, participants reported greater\nintention to use the agent-based version compared to the physical card deck and\nviewed it as more scalable, though a ceiling effect emerged on a simple\nknowledge quiz. Despite limitations of small sample size, single-player focus,\nand narrow corpus, these early findings suggest that large language model\naugmented TTXs can provide lightweight, repeatable practice without the\nlogistical burden of traditional exercises. Planned extensions include\nmulti-player modes, telemetry-driven coaching, and comparative studies with\nlarger cohorts.", "AI": {"tldr": "AgentBnB is a browser-based cybersecurity tabletop exercise that uses large language models to provide on-demand hints and teammates.", "motivation": "Traditional cybersecurity tabletop exercises are often scripted, resource-intensive, and difficult to scale.", "method": "The system integrates large language model teammates with a retrieval-augmented copilot (C2D2) that expands a curated corpus into factual, conceptual, procedural, and metacognitive snippets, delivering on-demand, cognitively targeted hints. Prompt-engineered agents employ a scaffolding ladder that gradually fades as learner confidence grows.", "result": "In a solo-player pilot with four graduate students, participants reported greater intention to use the agent-based version compared to the physical card deck and viewed it as more scalable, though a ceiling effect emerged on a simple knowledge quiz.", "conclusion": "Large language model augmented TTXs can provide lightweight, repeatable practice without the logistical burden of traditional exercises."}}
{"id": "2511.00046", "categories": ["cs.CV", "68U10, 94A08", "I.4.3; I.4.4; I.5.1; J.3"], "pdf": "https://arxiv.org/pdf/2511.00046", "abs": "https://arxiv.org/abs/2511.00046", "authors": ["Rupjyoti Chutia", "Dibya Jyoti Bora"], "title": "Enhancing rice leaf images: An overview of image denoising techniques", "comment": "18 pages, 6 figures. Research Article published in the International\n  Journal of Agricultural and Natural Sciences (IJANS), Vol. 18, Issue 2, 2025.\n  This paper presents a comparative study of image denoising and CLAHE\n  techniques for enhancing rice leaf images corrupted by Gaussian,\n  Salt-and-pepper, Speckle, and Random noise for agricultural analysis", "summary": "Digital image processing involves the systematic handling of images using\nadvanced computer algorithms, and has gained significant attention in both\nacademic and practical fields. Image enhancement is a crucial preprocessing\nstage in the image-processing chain, improving image quality and emphasizing\nfeatures. This makes subsequent tasks (segmentation, feature extraction,\nclassification) more reliable. Image enhancement is essential for rice leaf\nanalysis, aiding in disease detection, nutrient deficiency evaluation, and\ngrowth analysis. Denoising followed by contrast enhancement are the primary\nsteps. Image filters, generally employed for denoising, transform or enhance\nvisual characteristics like brightness, contrast, and sharpness, playing a\ncrucial role in improving overall image quality and enabling the extraction of\nuseful information. This work provides an extensive comparative study of\nwell-known image-denoising methods combined with CLAHE (Contrast Limited\nAdaptive Histogram Equalization) for efficient denoising of rice leaf images.\nThe experiments were performed on a rice leaf image dataset to ensure the data\nis relevant and representative. Results were examined using various metrics to\ncomprehensively test enhancement methods. This approach provides a strong basis\nfor assessing the effectiveness of methodologies in digital image processing\nand reveals insights useful for future adaptation in agricultural research and\nother domains.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6c34\u7a3b\u53f6\u7247\u56fe\u50cf\u8fdb\u884c\u53bb\u566a\u548c\u589e\u5f3a\uff0c\u4ee5\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\u548c\u540e\u7eed\u5206\u6790\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u56fe\u50cf\u589e\u5f3a\u662f\u56fe\u50cf\u5904\u7406\u4e2d\u7684\u5173\u952e\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u5bf9\u4e8e\u6c34\u7a3b\u53f6\u7247\u5206\u6790\u4e2d\u7684\u75c5\u5bb3\u68c0\u6d4b\u3001\u8425\u517b\u4e0d\u826f\u8bc4\u4f30\u548c\u751f\u957f\u5206\u6790\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u7814\u7a76\u4e86\u7ed3\u5408 CLAHE\uff08\u5bf9\u6bd4\u5ea6\u6709\u9650\u81ea\u9002\u5e94\u76f4\u65b9\u56fe\u5747\u8861\u5316\uff09\u7684\u56fe\u50cf\u53bb\u566a\u65b9\u6cd5\uff0c\u5e76\u8fdb\u884c\u4e86\u6bd4\u8f83\u7814\u7a76\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u4f7f\u7528\u5404\u79cd\u6307\u6807\u8fdb\u884c\u68c0\u67e5\uff0c\u4ee5\u5168\u9762\u6d4b\u8bd5\u589e\u5f3a\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8bc4\u4f30\u6570\u5b57\u56fe\u50cf\u5904\u7406\u65b9\u6cd5\u7684\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u5e76\u63ed\u793a\u4e86\u5bf9\u519c\u4e1a\u7814\u7a76\u548c\u5176\u4ed6\u9886\u57df\u672a\u6765\u9002\u5e94\u6709\u7528\u7684\u89c1\u89e3\u3002"}}
{"id": "2511.00162", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00162", "abs": "https://arxiv.org/abs/2511.00162", "authors": ["Michael D. Moffitt"], "title": "ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus", "comment": null, "summary": "The Abstraction and Reasoning Corpus remains one of the most compelling and\nchallenging benchmarks for tracking progress toward achieving Artificial\nGeneral Intelligence. In contrast to other evaluation datasets designed to\nassess an agent's task-specific skills or accumulated knowledge, the ARC-AGI\nsuite is specifically targeted at measuring skill acquisition efficiency, a\ntrait that has (so far) been lacking in even the most sophisticated machine\nlearning systems. For algorithms that require extensive intra-task exemplars, a\nsignificant constraint imposed by ARC-AGI is the modest cardinality of its\ndemonstration set, comprising a small number of $\\langle$ input, output\n$\\rangle$ grids per task specifying the corresponding transformation. To\nembellish the space of viable sample pairs, this paper introduces ARC-GEN, an\nopen-source procedural generator aimed at extending the original ARC-AGI\ntraining dataset as faithfully as possible. Unlike prior efforts, our generator\nis both exhaustive (covering all four-hundred tasks) and mimetic (more closely\nhonoring the distributional properties and characteristics embodied in the\ninitial ARC-AGI-1 release). We also discuss the use of this generator in\nestablishing a static benchmark suite to verify the correctness of programs\nsubmitted to the 2025 Google Code Golf Championship.", "AI": {"tldr": "ARC-GEN\u662f\u4e00\u4e2a\u5f00\u653e\u6e90\u4ee3\u7801\u7a0b\u5e8f\u751f\u6210\u5668\uff0c\u65e8\u5728\u6269\u5c55\u539f\u59cbARC-AGI\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "motivation": "ARC-AGI\u5957\u4ef6\u4e13\u95e8\u7528\u4e8e\u8861\u91cf\u6280\u80fd\u83b7\u53d6\u6548\u7387\uff0c\u8fd9\u662f\u6700\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u8fc4\u4eca\u4e3a\u6b62\u6240\u7f3a\u4e4f\u7684\u7279\u6027\u3002", "method": "\u5f15\u5165ARC-GEN\uff0c\u4e00\u4e2a\u65e8\u5728\u6269\u5c55\u539f\u59cbARC-AGI\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u7a0b\u5e8f\u751f\u6210\u5668\u3002", "result": "ARC-GEN\u662f\u8be6\u5c3d\u7684\uff08\u8986\u76d6\u6240\u6709\u56db\u767e\u4e2a\u4efb\u52a1\uff09\u548c\u6a21\u4eff\u7684\uff08\u66f4\u5bc6\u5207\u5730\u9075\u5b88\u521d\u59cbARC-AGI-1\u7248\u672c\u4e2d\u5305\u542b\u7684\u5206\u5e03\u7279\u6027\u548c\u7279\u5f81\uff09\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u4f7f\u7528\u6b64\u751f\u6210\u5668\u5efa\u7acb\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u4ee5\u9a8c\u8bc1\u63d0\u4ea4\u7ed92025\u5e74\u8c37\u6b4c\u4ee3\u7801\u9ad8\u5c14\u592b\u9526\u6807\u8d5b\u7684\u7a0b\u5e8f\u7684\u6b63\u786e\u6027\u3002"}}
{"id": "2511.00995", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.00995", "abs": "https://arxiv.org/abs/2511.00995", "authors": ["Tianming Wu", "Dixin Tang"], "title": "PathFinder: Efficiently Supporting Conjunctions and Disjunctions for Filtered Approximate Nearest Neighbor Search", "comment": null, "summary": "Filtered approximate nearest neighbor search (ANNS) restricts the search to\ndata objects whose attributes satisfy a given filter and retrieves the top-$K$\nobjects that are most semantically similar to the query object. Many\ngraph-based ANNS indexes are proposed to enable efficient filtered ANNS but\nremain limited in applicability or performance: indexes optimized for a\nspecific attribute achieve high efficiency for filters on that attribute but\nfail to support complex filters with arbitrary conjunctions and disjunctions\nover multiple attributes. Inspired by the design of relational databases, this\npaper presents PathFinder, a new indexing framework that allows users to\nselectively create ANNS indexes optimized for filters on specific attributes\nand employs a cost-based optimizer to efficiently utilize them for processing\ncomplex filters. PathFinder includes three novel techniques: 1) a new\noptimization metric that captures the tradeoff between query execution time and\naccuracy, 2) a two-phase optimization for handling filters with conjunctions\nand disjunctions, and 3) an index borrowing optimization that uses an\nattribute-specific index to process filters on another attribute. Experiments\non four real-world datasets show that PathFinder outperforms the best baseline\nby up to 9.8x in query throughput at recall 0.95.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7d22\u5f15\u6846\u67b6PathFinder\uff0c\u7528\u4e8e\u652f\u6301\u8fc7\u6ee4\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\uff08ANNS\uff09\uff0c\u5b83\u5141\u8bb8\u7528\u6237\u9009\u62e9\u6027\u5730\u521b\u5efa\u9488\u5bf9\u7279\u5b9a\u5c5e\u6027\u7684ANNS\u7d22\u5f15\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u6210\u672c\u7684\u4f18\u5316\u5668\u6765\u6709\u6548\u5730\u5229\u7528\u5b83\u4eec\u6765\u5904\u7406\u590d\u6742\u7684\u8fc7\u6ee4\u5668\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u7684ANNS\u7d22\u5f15\u5728\u652f\u6301\u590d\u6742\u8fc7\u6ee4\u5668\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u5f88\u597d\u5730\u652f\u6301\u591a\u4e2a\u5c5e\u6027\u4e0a\u7684\u4efb\u610f\u5408\u53d6\u548c\u6790\u53d6\u3002", "method": "PathFinder \u5305\u542b\u4e09\u4e2a\u521b\u65b0\u6280\u672f\uff1a1) \u4e00\u79cd\u65b0\u7684\u4f18\u5316\u6307\u6807\uff0c\u7528\u4e8e\u6355\u6349\u67e5\u8be2\u6267\u884c\u65f6\u95f4\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff1b2) \u4e00\u79cd\u7528\u4e8e\u5904\u7406\u5177\u6709\u5408\u53d6\u548c\u6790\u53d6\u7684\u8fc7\u6ee4\u5668\u7684\u4e24\u9636\u6bb5\u4f18\u5316\uff1b3) \u4e00\u79cd\u7d22\u5f15\u501f\u7528\u4f18\u5316\uff0c\u5b83\u4f7f\u7528\u7279\u5b9a\u4e8e\u5c5e\u6027\u7684\u7d22\u5f15\u6765\u5904\u7406\u53e6\u4e00\u4e2a\u5c5e\u6027\u4e0a\u7684\u8fc7\u6ee4\u5668\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPathFinder \u5728\u53ec\u56de\u7387 0.95 \u65f6\uff0c\u67e5\u8be2\u541e\u5410\u91cf\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u9ad8\u4e86 9.8 \u500d\u3002", "conclusion": "PathFinder \u662f\u4e00\u79cd\u9ad8\u6548\u7684\u8fc7\u6ee4 ANNS \u7d22\u5f15\u6846\u67b6\uff0c\u5b83\u53ef\u4ee5\u901a\u8fc7\u9009\u62e9\u6027\u5730\u521b\u5efa\u9488\u5bf9\u7279\u5b9a\u5c5e\u6027\u7684 ANNS \u7d22\u5f15\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u6210\u672c\u7684\u4f18\u5316\u5668\u6765\u6709\u6548\u5730\u5229\u7528\u5b83\u4eec\u6765\u5904\u7406\u590d\u6742\u7684\u8fc7\u6ee4\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u67e5\u8be2\u541e\u5410\u91cf\u3002"}}
{"id": "2511.00694", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.00694", "abs": "https://arxiv.org/abs/2511.00694", "authors": ["Uthman Jinadu", "Siawpeng Er", "Le Yu", "Chen Liang", "Bingxin Li", "Yi Ding", "Aleksandar Velkoski"], "title": "Taxonomy-based Negative Sampling In Personalized Semantic Search for E-commerce", "comment": "Accepted at 2025 IEEE International Conference on Big Data", "summary": "Large retail outlets offer products that may be domain-specific, and this\nrequires having a model that can understand subtle differences in similar\nitems. Sampling techniques used to train these models are most of the time,\ncomputationally expensive or logistically challenging. These models also do not\nfactor in users' previous purchase patterns or behavior, thereby retrieving\nirrelevant items for them. We present a semantic retrieval model for e-commerce\nsearch that embeds queries and products into a shared vector space and\nleverages a novel taxonomy-based hard-negative sampling(TB-HNS) strategy to\nmine contextually relevant yet challenging negatives. To further tailor\nretrievals, we incorporate user-level personalization by modeling each\ncustomer's past purchase history and behavior. In offline experiments, our\napproach outperforms BM25, ANCE and leading neural baselines on Recall@K, while\nlive A/B testing shows substantial uplifts in conversion rate, add-to-cart\nrate, and average order value. We also demonstrate that our taxonomy-driven\nnegatives reduce training overhead and accelerate convergence, and we share\npractical lessons from deploying this system at scale.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7535\u5b50\u5546\u52a1\u641c\u7d22\u7684\u8bed\u4e49\u68c0\u7d22\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5c06\u67e5\u8be2\u548c\u4ea7\u54c1\u5d4c\u5165\u5230\u5171\u4eab\u5411\u91cf\u7a7a\u95f4\u4e2d\uff0c\u5e76\u5229\u7528\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5206\u7c7b\u7684\u786c\u8d1f\u91c7\u6837 (TB-HNS) \u7b56\u7565\u6765\u6316\u6398\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u8d1f\u6837\u672c\u3002", "motivation": "\u5927\u578b\u96f6\u552e\u5546\u5e97\u63d0\u4f9b\u7684\u4ea7\u54c1\u53ef\u80fd\u662f\u7279\u5b9a\u9886\u57df\u7684\uff0c\u8fd9\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u7406\u89e3\u76f8\u4f3c\u5546\u54c1\u4e4b\u95f4\u7ec6\u5fae\u5dee\u522b\u7684\u6a21\u578b\u3002\u7528\u4e8e\u8bad\u7ec3\u8fd9\u4e9b\u6a21\u578b\u7684\u62bd\u6837\u6280\u672f\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u6216\u540e\u52e4\u5177\u6709\u6311\u6218\u6027\u3002\u8fd9\u4e9b\u6a21\u578b\u4e5f\u6ca1\u6709\u8003\u8651\u7528\u6237\u4e4b\u524d\u7684\u8d2d\u4e70\u6a21\u5f0f\u6216\u884c\u4e3a\uff0c\u56e0\u6b64\u4f1a\u68c0\u7d22\u5230\u4e0e\u4ed6\u4eec\u65e0\u5173\u7684\u5546\u54c1\u3002", "method": "\u8be5\u65b9\u6cd5\u5d4c\u5165\u67e5\u8be2\u548c\u4ea7\u54c1\u5230\u5171\u4eab\u5411\u91cf\u7a7a\u95f4\uff0c\u5e76\u5229\u7528\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5206\u7c7b\u7684\u786c\u8d1f\u91c7\u6837 (TB-HNS) \u7b56\u7565\u6765\u6316\u6398\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u8d1f\u6837\u672c\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u5b9a\u5236\u68c0\u7d22\uff0c\u6211\u4eec\u901a\u8fc7\u5bf9\u6bcf\u4e2a\u5ba2\u6237\u7684\u8fc7\u53bb\u8d2d\u4e70\u5386\u53f2\u548c\u884c\u4e3a\u8fdb\u884c\u5efa\u6a21\uff0c\u4ece\u800c\u6574\u5408\u7528\u6237\u7ea7\u522b\u7684\u4e2a\u6027\u5316\u3002", "result": "\u79bb\u7ebf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728 Recall@K \u4e0a\u4f18\u4e8e BM25\u3001ANCE \u548c\u9886\u5148\u7684\u795e\u7ecf\u57fa\u7ebf\uff0c\u800c\u5b9e\u9645 A/B \u6d4b\u8bd5\u8868\u660e\u8f6c\u5316\u7387\u3001\u52a0\u5165\u8d2d\u7269\u8f66\u7387\u548c\u5e73\u5747\u8ba2\u5355\u4ef7\u503c\u5927\u5e45\u63d0\u5347\u3002\u6211\u4eec\u8fd8\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u5206\u7c7b\u9a71\u52a8\u7684\u8d1f\u6837\u672c\u51cf\u5c11\u4e86\u8bad\u7ec3\u5f00\u9500\u5e76\u52a0\u901f\u4e86\u6536\u655b\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7535\u5b50\u5546\u52a1\u641c\u7d22\u8bed\u4e49\u68c0\u7d22\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u79bb\u7ebf\u548c\u5728\u7ebf\u5b9e\u9a8c\u4e2d\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.00043", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00043", "abs": "https://arxiv.org/abs/2511.00043", "authors": ["Tyrus Whitman", "Andrew Particka", "Christopher Diers", "Ian Griffin", "Charuka Wickramasinghe", "Pradeep Ranaweera"], "title": "Physics-Informed Neural Network Frameworks for the Analysis of Engineering and Biological Dynamical Systems Governed by Ordinary Differential Equations", "comment": "21 pages, 10 figures, 5 tables", "summary": "In this study, we present and validate the predictive capability of the\nPhysics-Informed Neural Networks (PINNs) methodology for solving a variety of\nengineering and biological dynamical systems governed by ordinary differential\nequations (ODEs). While traditional numerical methods a re effective for many\nODEs, they often struggle to achieve convergence in problems involving high\nstiffness, shocks, irregular domains, singular perturbations, high dimensions,\nor boundary discontinuities. Alternatively, PINNs offer a powerful approach for\nhandling challenging numerical scenarios. In this study, classical ODE problems\nare employed as controlled testbeds to systematically evaluate the accuracy,\ntraining efficiency, and generalization capability under controlled conditions\nof the PINNs framework. Although not a universal solution, PINNs can achieve\nsuperior results by embedding physical laws directly into the learning process.\nWe first analyze the existence and uniqueness properties of several benchmark\nproblems and subsequently validate the PINNs methodology on these model\nsystems. Our results demonstrate that for complex problems to converge to\ncorrect solutions, the loss function components data loss, initial condition\nloss, and residual loss must be appropriately balanced through careful\nweighting. We further establish that systematic tuning of hyperparameters,\nincluding network depth, layer width, activation functions, learning rate,\noptimization algorithms, w eight initialization schemes, and collocation point\nsampling, plays a crucial role in achieving accurate solutions. Additionally,\nembedding prior knowledge and imposing hard constraints on the network\narchitecture, without loss the generality of the ODE system, significantly\nenhances the predictive capability of PINNs.", "AI": {"tldr": "\u672c\u7814\u7a76\u9a8c\u8bc1\u4e86\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u5728\u6c42\u89e3\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODE\uff09\u63a7\u5236\u7684\u5de5\u7a0b\u548c\u751f\u7269\u52a8\u529b\u5b66\u7cfb\u7edf\u4e2d\u7684\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u5728\u89e3\u51b3\u9ad8\u521a\u6027\u3001\u51b2\u51fb\u3001\u4e0d\u89c4\u5219\u57df\u3001\u5947\u5f02\u6270\u52a8\u3001\u9ad8\u7ef4\u6216\u8fb9\u754c\u4e0d\u8fde\u7eed\u7b49\u95ee\u9898\u65f6\u96be\u4ee5\u6536\u655b\uff0c\u800cPINNs\u63d0\u4f9b\u4e86\u4e00\u79cd\u5904\u7406\u8fd9\u4e9b\u6311\u6218\u6027\u6570\u503c\u573a\u666f\u7684\u6709\u6548\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7ecf\u5178ODE\u95ee\u9898\u4f5c\u4e3a\u53d7\u63a7\u8bd5\u9a8c\u53f0\uff0c\u7cfb\u7edf\u5730\u8bc4\u4f30PINNs\u6846\u67b6\u5728\u53d7\u63a7\u6761\u4ef6\u4e0b\u7684\u51c6\u786e\u6027\u3001\u8bad\u7ec3\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002\u901a\u8fc7\u5d4c\u5165\u7269\u7406\u5b9a\u5f8b\u5230\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0cPINNs\u53ef\u4ee5\u83b7\u5f97\u4f18\u8d8a\u7684\u7ed3\u679c\u3002\u5206\u6790\u4e86\u51e0\u4e2a\u57fa\u51c6\u95ee\u9898\u7684\u5b58\u5728\u6027\u548c\u552f\u4e00\u6027\uff0c\u5e76\u5728\u8fd9\u4e9b\u6a21\u578b\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86PINNs\u65b9\u6cd5\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u4e8e\u590d\u6742\u95ee\u9898\uff0c\u6570\u636e\u635f\u5931\u3001\u521d\u59cb\u6761\u4ef6\u635f\u5931\u548c\u6b8b\u5dee\u635f\u5931\u5fc5\u987b\u901a\u8fc7\u4ed4\u7ec6\u7684\u52a0\u6743\u6765\u9002\u5f53\u5730\u5e73\u8861\uff0c\u624d\u80fd\u6536\u655b\u5230\u6b63\u786e\u7684\u89e3\u3002\u8d85\u53c2\u6570\uff08\u5305\u62ec\u7f51\u7edc\u6df1\u5ea6\u3001\u5c42\u5bbd\u5ea6\u3001\u6fc0\u6d3b\u51fd\u6570\u3001\u5b66\u4e60\u7387\u3001\u4f18\u5316\u7b97\u6cd5\u3001\u6743\u91cd\u521d\u59cb\u5316\u65b9\u6848\u548c\u914d\u7f6e\u70b9\u91c7\u6837\uff09\u7684\u7cfb\u7edf\u8c03\u6574\u5728\u5b9e\u73b0\u7cbe\u786e\u89e3\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002\u6b64\u5916\uff0c\u5d4c\u5165\u5148\u9a8c\u77e5\u8bc6\u548c\u5bf9\u7f51\u7edc\u67b6\u6784\u65bd\u52a0\u786c\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86PINNs\u7684\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "PINNs\u867d\u7136\u4e0d\u662f\u4e00\u4e2a\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u901a\u8fc7\u5d4c\u5165\u7269\u7406\u5b9a\u5f8b\u76f4\u63a5\u5230\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u53ef\u4ee5\u83b7\u5f97\u4f18\u8d8a\u7684\u7ed3\u679c\u3002"}}
{"id": "2511.00268", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00268", "abs": "https://arxiv.org/abs/2511.00268", "authors": ["Shounak Paul", "Dhananjay Ghumare", "Pawan Goyal", "Saptarshi Ghosh", "Ashutosh Modi"], "title": "IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval", "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Identifying/retrieving relevant statutes and prior cases/precedents for a\ngiven legal situation are common tasks exercised by law practitioners.\nResearchers to date have addressed the two tasks independently, thus developing\ncompletely different datasets and models for each task; however, both retrieval\ntasks are inherently related, e.g., similar cases tend to cite similar statutes\n(due to similar factual situation). In this paper, we address this gap. We\npropose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),\nwhich is a unique corpus that provides a common testbed for developing models\nfor both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit\nthe dependence between the two. We experiment extensively with several baseline\nmodels on the tasks, including lexical models, semantic models and ensemble\nbased on GNNs. Further, to exploit the dependence between the two tasks, we\ndevelop an LLM-based re-ranking approach that gives the best performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aIL-PCR\u7684\u5370\u5ea6\u6cd5\u5f8b\u8bed\u6599\u5e93\uff0c\u7528\u4e8e\u5224\u4f8b\u548c\u6cd5\u89c4\u68c0\u7d22\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u4ee5\u5229\u7528\u8fd9\u4e24\u4e2a\u4efb\u52a1\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u7684\u6cd5\u5f8b\u4ece\u4e1a\u8005\u901a\u5e38\u9700\u8981\u68c0\u7d22\u76f8\u5173\u6cd5\u89c4\u548c\u5224\u4f8b\uff0c\u4f46\u76ee\u524d\u7684\u7814\u7a76\u901a\u5e38\u72ec\u7acb\u5904\u7406\u8fd9\u4e24\u4e2a\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u4e4b\u95f4\u7684\u5185\u5728\u8054\u7cfb\uff0c\u4f8b\u5982\u76f8\u4f3c\u7684\u6848\u4ef6\u503e\u5411\u4e8e\u5f15\u7528\u76f8\u4f3c\u7684\u6cd5\u89c4\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aIL-PCR\u7684\u8bed\u6599\u5e93\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5b9e\u9a8c\u4e86\u591a\u79cd\u57fa\u7ebf\u6a21\u578b\uff0c\u5305\u62ec\u8bcd\u6c47\u6a21\u578b\u3001\u8bed\u4e49\u6a21\u578b\u548c\u57fa\u4e8eGNN\u7684\u96c6\u6210\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u4ee5\u5229\u7528\u5224\u4f8b\u548c\u6cd5\u89c4\u68c0\u7d22\u4efb\u52a1\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u57fa\u4e8eLLM\u7684\u91cd\u6392\u5e8f\u65b9\u6cd5\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u5370\u5ea6\u6cd5\u5f8b\u8bed\u6599\u5e93IL-PCR\uff0c\u5e76\u8bc1\u660e\u4e86\u5229\u7528\u5224\u4f8b\u548c\u6cd5\u89c4\u68c0\u7d22\u4efb\u52a1\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u53ef\u4ee5\u63d0\u9ad8\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2511.00060", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.00060", "abs": "https://arxiv.org/abs/2511.00060", "authors": ["Zhiqi Qi", "Runxin Zhao", "Hanyang Zhuang", "Chunxiang Wang", "Ming Yang"], "title": "Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?", "comment": null, "summary": "LiDAR-based roadside perception is a cornerstone of advanced Intelligent\nTransportation Systems (ITS). While considerable research has addressed optimal\nLiDAR placement for infrastructure, the profound impact of differing LiDAR\nscanning patterns on perceptual performance remains comparatively\nunder-investigated. The inherent nature of various scanning modes - such as\ntraditional repetitive (mechanical/solid-state) versus emerging non-repetitive\n(e.g. prism-based) systems - leads to distinct point cloud distributions at\nvarying distances, critically dictating the efficacy of object detection and\noverall environmental understanding. To systematically investigate these\ndifferences in infrastructure-based contexts, we introduce the \"InfraLiDARs'\nBenchmark,\" a novel dataset meticulously collected in the CARLA simulation\nenvironment using concurrently operating infrastructure-based LiDARs exhibiting\nboth scanning paradigms. Leveraging this benchmark, we conduct a comprehensive\nstatistical analysis of the respective LiDAR scanning abilities and evaluate\nthe impact of these distinct patterns on the performance of various leading 3D\nobject detection algorithms. Our findings reveal that non-repetitive scanning\nLiDAR and the 128-line repetitive LiDAR were found to exhibit comparable\ndetection performance across various scenarios. Despite non-repetitive LiDAR's\nlimited perception range, it's a cost-effective option considering its low\nprice. Ultimately, this study provides insights for setting up roadside\nperception system with optimal LiDAR scanning patterns and compatible\nalgorithms for diverse roadside applications, and publicly releases the\n\"InfraLiDARs' Benchmark\" dataset to foster further research.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e0d\u540c\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u6a21\u5f0f\u5bf9\u8def\u8fb9\u611f\u77e5\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u53d1\u5e03\u4e86InfraLiDARs' Benchmark\u6570\u636e\u96c6\u3002", "motivation": "\u63a2\u8ba8\u4e0d\u540c\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u6a21\u5f0f\u5bf9\u611f\u77e5\u6027\u80fd\u7684\u6df1\u8fdc\u5f71\u54cd\uff0c\u73b0\u6709\u7814\u7a76\u5bf9\u57fa\u7840\u8bbe\u65bd\u4e2d\u6fc0\u5149\u96f7\u8fbe\u7684\u6700\u4f73\u653e\u7f6e\u4f4d\u7f6e\u8fdb\u884c\u4e86\u5927\u91cf\u7814\u7a76\uff0c\u4f46\u5bf9\u4e0d\u540c\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u6a21\u5f0f\u7684\u5f71\u54cd\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u5728CARLA\u6a21\u62df\u73af\u5883\u4e2d\uff0c\u4f7f\u7528\u540c\u65f6\u8fd0\u884c\u7684\u3001\u5177\u6709\u4e24\u79cd\u626b\u63cf\u6a21\u5f0f\u7684\u3001\u57fa\u4e8e\u57fa\u7840\u8bbe\u65bd\u7684\u6fc0\u5149\u96f7\u8fbe\uff0c\u6536\u96c6\u4e86\u4e00\u4e2a\u540d\u4e3a\u201cInfraLiDARs' Benchmark\u201d\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u5404\u79cd\u9886\u5148\u76843D\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u975e\u91cd\u590d\u626b\u63cf\u6fc0\u5149\u96f7\u8fbe\u548c128\u7ebf\u91cd\u590d\u6fc0\u5149\u96f7\u8fbe\u5728\u5404\u79cd\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u76f8\u5f53\u7684\u68c0\u6d4b\u6027\u80fd\u3002\u5c3d\u7ba1\u975e\u91cd\u590d\u626b\u63cf\u6fc0\u5149\u96f7\u8fbe\u7684\u611f\u77e5\u8303\u56f4\u6709\u9650\uff0c\u4f46\u8003\u8651\u5230\u5176\u4f4e\u5ec9\u7684\u4ef7\u683c\uff0c\u5b83\u662f\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u9009\u62e9\u3002", "conclusion": "\u4e3a\u8bbe\u7f6e\u5177\u6709\u6700\u4f73\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u6a21\u5f0f\u548c\u517c\u5bb9\u7b97\u6cd5\u7684\u8def\u8fb9\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u4ee5\u7528\u4e8e\u5404\u79cd\u8def\u8fb9\u5e94\u7528\uff0c\u5e76\u516c\u5f00\u53d1\u5e03\u201cInfraLiDARs' Benchmark\u201d\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u3002"}}
{"id": "2511.00194", "categories": ["cs.AI", "F.2.2, F.4.1"], "pdf": "https://arxiv.org/pdf/2511.00194", "abs": "https://arxiv.org/abs/2511.00194", "authors": ["Jovial Cheukam Ngouonou", "Ramiz Gindullin", "Claude-Guy Quimper", "Nicolas Beldiceanu", "Remi Douence"], "title": "Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures", "comment": null, "summary": "We present an improved incremental selection algorithm of the selection\nalgorithm presented in [1] and prove all the selected conjectures.", "AI": {"tldr": "\u6539\u8fdb\u4e86[1]\u4e2d\u63d0\u51fa\u7684\u9009\u62e9\u7b97\u6cd5\u7684\u589e\u91cf\u9009\u62e9\u7b97\u6cd5", "motivation": "[1]\u4e2d\u63d0\u51fa\u7684\u9009\u62e9\u7b97\u6cd5", "method": "\u6539\u8fdb\u589e\u91cf\u9009\u62e9\u7b97\u6cd5", "result": "\u8bc1\u660e\u4e86\u6240\u6709\u9009\u5b9a\u7684\u731c\u60f3", "conclusion": "NA"}}
{"id": "2511.01025", "categories": ["cs.DB", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.01025", "abs": "https://arxiv.org/abs/2511.01025", "authors": ["Huihui Yang", "Pingpeng Yuan"], "title": "Fast Answering Pattern-Constrained Reachability Queries with Two-Dimensional Reachability Index", "comment": null, "summary": "Reachability queries ask whether there exists a path from the source vertex\nto the target vertex on a graph. Recently, several powerful reachability\nqueries, such as Label-Constrained Reachability (LCR) queries and Regular Path\nQueries (RPQ), have been proposed for emerging complex edge-labeled digraphs.\nHowever, they cannot allow users to describe complex query requirements by\ncomposing query patterns. Here, we introduce composite patterns, a logical\nexpression of patterns that can express complex constraints on the set of\nlabels. Based on pattern, we propose pattern-constrained reachability queries\n(PCR queries). However, answering PCR queries is NP-hard. Thus, to improve the\nperformance to answer PCR queries, we build a two-dimensional reachability (TDR\nfor short) index which consists of a multi-way index (horizontal dimension) and\na path index (vertical dimension). Because the number of combinations of both\nlabels and vertices is exponential, it is very expensive to build full indices\nthat contain all the reachability information. Thus, the reachable vertices of\na vertex are decomposed into blocks, each of which is hashed into the\nhorizontal dimension index and the vertical dimension index, respectively. The\nindices in the horizontal dimension and the vertical dimension serve as a\nglobal filter and a local filter, respectively, to prune the search space.\nExperimental results demonstrate that our index size and indexing time\noutperform the state-of-the-art label-constrained reachability indexing\ntechnique on 16 real datasets. TDR can efficiently answer pattern-constrained\nreachability queries, including label-constrained reachability queries.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6a21\u5f0f\u7ea6\u675f\u53ef\u8fbe\u6027\u67e5\u8be2\uff08PCR\u67e5\u8be2\uff09\uff0c\u4ee5\u652f\u6301\u7528\u6237\u901a\u8fc7\u7ec4\u5408\u67e5\u8be2\u6a21\u5f0f\u6765\u63cf\u8ff0\u590d\u6742\u67e5\u8be2\u9700\u6c42\u3002\u4e3a\u4e86\u63d0\u9ad8\u67e5\u8be2\u6548\u7387\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u4e8c\u7ef4\u53ef\u8fbe\u6027\u7d22\u5f15\uff08TDR\uff09\uff0c\u5305\u542b\u591a\u8def\u7d22\u5f15\uff08\u6c34\u5e73\u7ef4\u5ea6\uff09\u548c\u8def\u5f84\u7d22\u5f15\uff08\u5782\u76f4\u7ef4\u5ea6\uff09\u3002", "motivation": "\u73b0\u6709\u7684\u53ef\u8fbe\u6027\u67e5\u8be2\uff0c\u5982\u6807\u7b7e\u7ea6\u675f\u53ef\u8fbe\u6027\uff08LCR\uff09\u67e5\u8be2\u548c\u6b63\u5219\u8def\u5f84\u67e5\u8be2\uff08RPQ\uff09\uff0c\u65e0\u6cd5\u8ba9\u7528\u6237\u901a\u8fc7\u7ec4\u5408\u67e5\u8be2\u6a21\u5f0f\u6765\u63cf\u8ff0\u590d\u6742\u67e5\u8be2\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u6a21\u5f0f\u7ea6\u675f\u53ef\u8fbe\u6027\u67e5\u8be2\uff08PCR\u67e5\u8be2\uff09\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u4e8c\u7ef4\u53ef\u8fbe\u6027\u7d22\u5f15\uff08TDR\uff09\uff0c\u5b83\u7531\u591a\u8def\u7d22\u5f15\uff08\u6c34\u5e73\u7ef4\u5ea6\uff09\u548c\u8def\u5f84\u7d22\u5f15\uff08\u5782\u76f4\u7ef4\u5ea6\uff09\u7ec4\u6210\u3002\u9876\u70b9\u53ef\u8fbe\u7684\u9876\u70b9\u88ab\u5206\u89e3\u6210\u5757\uff0c\u5206\u522b\u54c8\u5e0c\u5230\u6c34\u5e73\u548c\u5782\u76f4\u7ef4\u5ea6\u7d22\u5f15\u4e2d\uff0c\u4ee5\u4f5c\u4e3a\u5168\u5c40\u548c\u5c40\u90e8\u8fc7\u6ee4\u5668\u6765\u7f29\u5c0f\u641c\u7d22\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u572816\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u6211\u4eec\u7684\u7d22\u5f15\u5927\u5c0f\u548c\u7d22\u5f15\u65f6\u95f4\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6807\u7b7e\u7ea6\u675f\u53ef\u8fbe\u6027\u7d22\u5f15\u6280\u672f\u3002TDR\u53ef\u4ee5\u6709\u6548\u5730\u56de\u7b54\u6a21\u5f0f\u7ea6\u675f\u7684\u53ef\u8fbe\u6027\u67e5\u8be2\uff0c\u5305\u62ec\u6807\u7b7e\u7ea6\u675f\u7684\u53ef\u8fbe\u6027\u67e5\u8be2\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684PCR\u67e5\u8be2\u548cTDR\u7d22\u5f15\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u590d\u6742\u56fe\u7ed3\u6784\u4e0a\u7684\u53ef\u8fbe\u6027\u67e5\u8be2\u95ee\u9898\u3002"}}
{"id": "2511.00805", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.00805", "abs": "https://arxiv.org/abs/2511.00805", "authors": ["Rishita Agarwal", "Himanshu Singhal", "Peter Baile Chen", "Manan Roy Choudhury", "Dan Roth", "Vivek Gupta"], "title": "REaR: Retrieve, Expand and Refine for Effective Multitable Retrieval", "comment": "13 pages, 2 figures, 8 tables", "summary": "Answering natural language queries over relational data often requires\nretrieving and reasoning over multiple tables, yet most retrievers optimize\nonly for query-table relevance and ignore table table compatibility. We\nintroduce REAR (Retrieve, Expand and Refine), a three-stage, LLM-free framework\nthat separates semantic relevance from structural joinability for efficient,\nhigh-fidelity multi-table retrieval. REAR (i) retrieves query-aligned tables,\n(ii) expands these with structurally joinable tables via fast, precomputed\ncolumn-embedding comparisons, and (iii) refines them by pruning noisy or weakly\nrelated candidates. Empirically, REAR is retriever-agnostic and consistently\nimproves dense/sparse retrievers on complex table QA datasets (BIRD, MMQA, and\nSpider) by improving both multi-table retrieval quality and downstream SQL\nexecution. Despite being LLM-free, it delivers performance competitive with\nstate-of-the-art LLM-augmented retrieval systems (e.g.,ARM) while achieving\nmuch lower latency and cost. Ablations confirm complementary gains from\nexpansion and refinement, underscoring REAR as a practical, scalable building\nblock for table-based downstream tasks (e.g., Text-to-SQL).", "AI": {"tldr": "REAR\u662f\u4e00\u4e2a\u4e09\u9636\u6bb5\u7684\u3001\u4e0d\u4f9d\u8d56LLM\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u8868\u68c0\u7d22\uff0c\u5b83\u5206\u79bb\u4e86\u8bed\u4e49\u76f8\u5173\u6027\u548c\u7ed3\u6784\u53ef\u8fde\u63a5\u6027\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u7684\u68c0\u7d22\u3002", "motivation": "\u73b0\u6709\u7684\u68c0\u7d22\u5668\u901a\u5e38\u53ea\u4f18\u5316\u67e5\u8be2-\u8868\u7684\u76f8\u5173\u6027\uff0c\u800c\u5ffd\u7565\u8868-\u8868\u4e4b\u95f4\u7684\u517c\u5bb9\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "REAR\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a(1) \u68c0\u7d22\u4e0e\u67e5\u8be2\u76f8\u5173\u7684\u8868\uff1b(2) \u901a\u8fc7\u5feb\u901f\u7684\u3001\u9884\u5148\u8ba1\u7b97\u7684\u5217\u5d4c\u5165\u6bd4\u8f83\uff0c\u7528\u7ed3\u6784\u4e0a\u53ef\u8fde\u63a5\u7684\u8868\u6269\u5c55\u8fd9\u4e9b\u8868\uff1b(3) \u901a\u8fc7\u4fee\u526a\u566a\u58f0\u6216\u5f31\u76f8\u5173\u7684\u5019\u9009\u8868\u6765\u6539\u8fdb\u5b83\u4eec\u3002", "result": "REAR\u5728\u590d\u6742\u8868\u683cQA\u6570\u636e\u96c6\uff08BIRD\u3001MMQA\u548cSpider\uff09\u4e0a\u4e00\u81f4\u5730\u6539\u8fdb\u4e86\u5bc6\u96c6/\u7a00\u758f\u68c0\u7d22\u5668\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u591a\u8868\u68c0\u7d22\u8d28\u91cf\u548c\u4e0b\u6e38SQL\u6267\u884c\u3002REAR\u7684\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684LLM\u589e\u5f3a\u68c0\u7d22\u7cfb\u7edf\u76f8\u5f53\uff0c\u4f46\u5ef6\u8fdf\u548c\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "REAR\u662f\u4e00\u4e2a\u5b9e\u7528\u7684\u3001\u53ef\u6269\u5c55\u7684\u6784\u5efa\u5757\uff0c\u9002\u7528\u4e8e\u57fa\u4e8e\u8868\u683c\u7684\u4e0b\u6e38\u4efb\u52a1\uff08\u4f8b\u5982\uff0cText-to-SQL\uff09\u3002"}}
{"id": "2511.00044", "categories": ["cs.LG", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2511.00044", "abs": "https://arxiv.org/abs/2511.00044", "authors": ["Kohei Tsuchiyama", "Andre Roehm", "Takatomo Mihana", "Ryoichi Horisaki"], "title": "ReLaX-Net: Reusing Layers for Parameter-Efficient Physical Neural Networks", "comment": null, "summary": "Physical Neural Networks (PNN) are promising platforms for next-generation\ncomputing systems. However, recent advances in digital neural network\nperformance are largely driven by the rapid growth in the number of trainable\nparameters and, so far, demonstrated PNNs are lagging behind by several orders\nof magnitude in terms of scale. This mirrors size and performance constraints\nfound in early digital neural networks. In that period, efficient reuse of\nparameters contributed to the development of parameter-efficient architectures\nsuch as convolutional neural networks.\n  In this work, we numerically investigate hardware-friendly weight-tying for\nPNNs. Crucially, with many PNN systems, there is a time-scale separation\nbetween the fast dynamic active elements of the forward pass and the only\nslowly trainable elements implementing weights and biases. With this in mind,we\npropose the Reuse of Layers for eXpanding a Neural Network (ReLaX-Net)\narchitecture, which employs a simple layer-by-layer time-multiplexing scheme to\nincrease the effective network depth and efficiently use the number of\nparameters. We only require the addition of fast switches for existing PNNs. We\nvalidate ReLaX-Nets via numerical experiments on image classification and\nnatural language processing tasks. Our results show that ReLaX-Net improves\ncomputational performance with only minor modifications to a conventional PNN.\nWe observe a favorable scaling, where ReLaX-Nets exceed the performance of\nequivalent traditional RNNs or DNNs with the same number of parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReLaX-Net\u7684\u65b0\u578b\u7269\u7406\u795e\u7ecf\u7f51\u7edc(PNN)\u67b6\u6784\uff0c\u901a\u8fc7\u65f6\u95f4\u590d\u7528\u589e\u52a0\u7f51\u7edc\u6df1\u5ea6\u5e76\u6709\u6548\u5229\u7528\u53c2\u6570\uff0c\u4ece\u800c\u63d0\u9ad8\u8ba1\u7b97\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u7269\u7406\u795e\u7ecf\u7f51\u7edc\u5728\u89c4\u6a21\u4e0a\u843d\u540e\u4e8e\u6570\u5b57\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u4e14\u53c2\u6570\u6548\u7387\u8f83\u4f4e\u3002", "method": "\u63d0\u51faReLaX-Net\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u91c7\u7528\u9010\u5c42\u65f6\u95f4\u590d\u7528\u65b9\u6848\u6765\u589e\u52a0\u6709\u6548\u7f51\u7edc\u6df1\u5ea6\uff0c\u5e76\u4e14\u53ea\u9700\u8981\u4e3a\u73b0\u6709PNN\u6dfb\u52a0\u5feb\u901f\u5f00\u5173\u3002", "result": "ReLaX-Net\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660eReLaX-Net\u5728\u8ba1\u7b97\u6027\u80fd\u4e0a\u6709\u6240\u63d0\u9ad8\uff0c\u5e76\u4e14\u8d85\u8fc7\u4e86\u5177\u6709\u76f8\u540c\u6570\u91cf\u53c2\u6570\u7684\u4f20\u7edfRNN\u6216DNN\u7684\u6027\u80fd\u3002", "conclusion": "ReLaX-Net\u901a\u8fc7\u5bf9\u4f20\u7edfPNN\u8fdb\u884c\u5c11\u91cf\u4fee\u6539\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6027\u80fd\u7684\u63d0\u5347\uff0c\u5e76\u89c2\u5bdf\u5230\u826f\u597d\u7684\u6269\u5c55\u6027\u3002"}}
{"id": "2511.00270", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00270", "abs": "https://arxiv.org/abs/2511.00270", "authors": ["Abhinav Joshi", "Vaibhav Sharma", "Sanjeet Singh", "Ashutosh Modi"], "title": "POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation", "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Sign language translation remains a challenging task due to the scarcity of\nlarge-scale, sentence-aligned datasets. Prior arts have focused on various\nfeature extraction and architectural changes to support neural machine\ntranslation for sign languages. We propose POSESTITCH-SLT, a novel pre-training\nscheme that is inspired by linguistic-templates-based sentence generation\ntechnique. With translation comparison on two sign language datasets, How2Sign\nand iSign, we show that a simple transformer-based encoder-decoder architecture\noutperforms the prior art when considering template-generated sentence pairs in\ntraining. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign\nand from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for\npose-based gloss-free translation. The results demonstrate the effectiveness of\ntemplate-driven synthetic supervision in low-resource sign language settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9884\u8bad\u7ec3\u65b9\u6848POSESTITCH-SLT\uff0c\u7528\u4e8e\u89e3\u51b3\u56e0\u7f3a\u4e4f\u5927\u89c4\u6a21\u53e5\u5b50\u5bf9\u9f50\u6570\u636e\u96c6\u800c\u5bfc\u81f4\u7684sign language translation \u4efb\u52a1\u7684\u6311\u6218\u3002", "motivation": "\u4ee5\u5f80\u7684\u7814\u7a76\u96c6\u4e2d\u4e8e\u5404\u79cd\u7279\u5f81\u63d0\u53d6\u548c\u67b6\u6784\u53d8\u5316\uff0c\u4ee5\u652f\u6301sign language\u7684\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u3002", "method": "\u8be5\u65b9\u6cd5\u53d7\u5230\u57fa\u4e8e\u8bed\u8a00\u6a21\u677f\u7684\u53e5\u5b50\u751f\u6210\u6280\u672f\u7684\u542f\u53d1\u3002", "result": "\u5728How2Sign\u548ciSign\u4e24\u4e2asign language\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5728How2Sign\u4e0aBLEU-4\u63d0\u9ad8\u4e861.97\u52304.56\uff0c\u5728iSign\u4e0a\u63d0\u9ad8\u4e860.55\u52303.43\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4f4e\u8d44\u6e90sign language\u73af\u5883\u4e2d\uff0c\u6a21\u677f\u9a71\u52a8\u7684\u5408\u6210\u76d1\u7763\u662f\u6709\u6548\u7684\u3002"}}
{"id": "2511.00062", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00062", "abs": "https://arxiv.org/abs/2511.00062", "authors": ["NVIDIA", ":", "Arslan Ali", "Junjie Bai", "Maciej Bala", "Yogesh Balaji", "Aaron Blakeman", "Tiffany Cai", "Jiaxin Cao", "Tianshi Cao", "Elizabeth Cha", "Yu-Wei Chao", "Prithvijit Chattopadhyay", "Mike Chen", "Yongxin Chen", "Yu Chen", "Shuai Cheng", "Yin Cui", "Jenna Diamond", "Yifan Ding", "Jiaojiao Fan", "Linxi Fan", "Liang Feng", "Francesco Ferroni", "Sanja Fidler", "Xiao Fu", "Ruiyuan Gao", "Yunhao Ge", "Jinwei Gu", "Aryaman Gupta", "Siddharth Gururani", "Imad El Hanafi", "Ali Hassani", "Zekun Hao", "Jacob Huffman", "Joel Jang", "Pooya Jannaty", "Jan Kautz", "Grace Lam", "Xuan Li", "Zhaoshuo Li", "Maosheng Liao", "Chen-Hsuan Lin", "Tsung-Yi Lin", "Yen-Chen Lin", "Huan Ling", "Ming-Yu Liu", "Xian Liu", "Yifan Lu", "Alice Luo", "Qianli Ma", "Hanzi Mao", "Kaichun Mo", "Seungjun Nah", "Yashraj Narang", "Abhijeet Panaskar", "Lindsey Pavao", "Trung Pham", "Morteza Ramezanali", "Fitsum Reda", "Scott Reed", "Xuanchi Ren", "Haonan Shao", "Yue Shen", "Stella Shi", "Shuran Song", "Bartosz Stefaniak", "Shangkun Sun", "Shitao Tang", "Sameena Tasmeen", "Lyne Tchapmi", "Wei-Cheng Tseng", "Jibin Varghese", "Andrew Z. Wang", "Hao Wang", "Haoxiang Wang", "Heng Wang", "Ting-Chun Wang", "Fangyin Wei", "Jiashu Xu", "Dinghao Yang", "Xiaodong Yang", "Haotian Ye", "Seonghyeon Ye", "Xiaohui Zeng", "Jing Zhang", "Qinsheng Zhang", "Kaiwen Zheng", "Andrew Zhu", "Yuke Zhu"], "title": "World Simulation with Video Foundation Models for Physical AI", "comment": null, "summary": "We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World\nFoundation Models for Physical AI. Built on a flow-based architecture,\n[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation\nin a single model and leverages [Cosmos-Reason1], a Physical AI vision-language\nmodel, to provide richer text grounding and finer control of world simulation.\nTrained on 200M curated video clips and refined with reinforcement\nlearning-based post-training, [Cosmos-Predict2.5] achieves substantial\nimprovements over [Cosmos-Predict1] in video quality and instruction alignment,\nwith models released at 2B and 14B scales. These capabilities enable more\nreliable synthetic data generation, policy evaluation, and closed-loop\nsimulation for robotics and autonomous systems. We further extend the family\nwith [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and\nReal2Real world translation. Despite being 3.5$\\times$ smaller than\n[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video\ngeneration. Together, these advances establish [Cosmos-Predict2.5] and\n[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To\naccelerate research and deployment in Physical AI, we release source code,\npretrained checkpoints, and curated benchmarks under the NVIDIA Open Model\nLicense at https://github.com/nvidia-cosmos/cosmos-predict2.5 and\nhttps://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open\nresources lower the barrier to adoption and foster innovation in building the\nnext generation of embodied intelligence.", "AI": {"tldr": "Cosmos-Predict2.5 improves video quality and instruction alignment over Cosmos-Predict1 using a flow-based architecture and reinforcement learning. Cosmos-Transfer2.5, a control-net style framework, achieves higher fidelity and robust long-horizon video generation compared to Cosmos-Transfer1. Code, checkpoints, and benchmarks are released to foster innovation in Physical AI.", "motivation": "To improve synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems.", "method": "Uses a flow-based architecture to unify Text2World, Image2World, and Video2World generation, leverages Cosmos-Reason1 for richer text grounding, and is trained on 200M video clips with reinforcement learning-based post-training. Introduces Cosmos-Transfer2.5, a control-net style framework for Sim2Real and Real2Real world translation.", "result": "Cosmos-Predict2.5 achieves substantial improvements over Cosmos-Predict1 in video quality and instruction alignment. Cosmos-Transfer2.5 delivers higher fidelity and robust long-horizon video generation, despite being smaller than Cosmos-Transfer1.", "conclusion": "Cosmos-Predict2.5 and Cosmos-Transfer2.5 are versatile tools for scaling embodied intelligence. Open resources are released to lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence."}}
