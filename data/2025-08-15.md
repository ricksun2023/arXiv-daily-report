<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 51]
- [cs.CV](#cs.CV) [Total: 50]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.LG](#cs.LG) [Total: 50]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry](https://arxiv.org/abs/2508.09991)
*Lovedeep Gondara,Gregory Arbour,Raymond Ng,Jonathan Simkin,Shebnum Devji*

Main category: cs.CL

TL;DR: 分享了在 BCCR 实施 NLP 解决方案以改进数据管理流程的关键经验教训。


<details>
  <summary>Details</summary>
Motivation: 从临床文件中自动提取数据具有提高医疗效率的巨大潜力，但部署自然语言处理 (NLP) 解决方案带来实际挑战。

Method: 在不列颠哥伦比亚癌症登记处 (BCCR) 实施各种 NLP 模型以进行信息提取和分类任务。

Result: 强调了基于明确的业务目标定义问题、采用迭代开发方法以及从一开始就培养涉及领域专家、最终用户和 ML 专家的深入跨学科协作和共同设计至关重要。此外，还强调了实用模型选择（包括混合方法和更简单的方法，在适当的情况下）、严格关注数据质量（代表性、漂移、注释）、涉及人机循环验证和持续审计的稳健的错误缓解策略以及构建组织 AI 知识的必要性。

Conclusion: 分享了在不列颠哥伦比亚癌症登记处 (BCCR) 实施各种 NLP 模型以进行信息提取和分类任务的关键经验教训，强调了定义问题、迭代开发、跨学科协作和共同设计的重要性，并为医疗保健组织寻求成功实施 AI/NLP 解决方案以加强数据管理流程并最终改善患者护理和公共健康结果提供指导。

Abstract: Automating data extraction from clinical documents offers significant
potential to improve efficiency in healthcare settings, yet deploying Natural
Language Processing (NLP) solutions presents practical challenges. Drawing upon
our experience implementing various NLP models for information extraction and
classification tasks at the British Columbia Cancer Registry (BCCR), this paper
shares key lessons learned throughout the project lifecycle. We emphasize the
critical importance of defining problems based on clear business objectives
rather than solely technical accuracy, adopting an iterative approach to
development, and fostering deep interdisciplinary collaboration and co-design
involving domain experts, end-users, and ML specialists from inception. Further
insights highlight the need for pragmatic model selection (including hybrid
approaches and simpler methods where appropriate), rigorous attention to data
quality (representativeness, drift, annotation), robust error mitigation
strategies involving human-in-the-loop validation and ongoing audits, and
building organizational AI literacy. These practical considerations,
generalizable beyond cancer registries, provide guidance for healthcare
organizations seeking to successfully implement AI/NLP solutions to enhance
data management processes and ultimately improve patient care and public health
outcomes.

</details>


### [2] [A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain](https://arxiv.org/abs/2508.09993)
*Hugo Massaroli,Leonardo Iara,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CL

TL;DR: This paper presents a blockchain-based evaluation protocol for LLM fairness, benchmarking Llama, DeepSeek, and Mistral models and revealing biases.


<details>
  <summary>Details</summary>
Motivation: Concerns about the fairness of LLMs persist, especially in high-stakes domains like criminal justice, education, healthcare, and finance.

Method: The method involves executing onchain HTTP requests to hosted Hugging Face endpoints and storing datasets, prompts, and metrics directly onchain.

Result: The Llama, DeepSeek, and Mistral models were benchmarked on the PISA dataset, StereoSet dataset, and Kaleidoscope benchmark, revealing cross-linguistic disparities.

Conclusion: This paper introduces a transparent evaluation protocol for benchmarking the fairness of open-source LLMs using smart contracts on the Internet Computer Protocol (ICP) blockchain. The code and results are open source, enabling community audits and longitudinal fairness tracking across model versions.

Abstract: Large language models (LLMs) are increasingly deployed in realworld
applications, yet concerns about their fairness persist especially in
highstakes domains like criminal justice, education, healthcare, and finance.
This paper introduces transparent evaluation protocol for benchmarking the
fairness of opensource LLMs using smart contracts on the Internet Computer
Protocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable,
immutable, and reproducible evaluations by executing onchain HTTP requests to
hosted Hugging Face endpoints and storing datasets, prompts, and metrics
directly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the
PISA dataset for academic performance prediction (OECD, 2018), a dataset
suitable for fairness evaluation using statistical parity and equal opportunity
metrics (Hardt et al., 2016). We also evaluate structured Context Association
Metrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure
social bias in contextual associations. We further extend our analysis with a
multilingual evaluation across English, Spanish, and Portuguese using the
Kaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic
disparities. All code and results are open source, enabling community audits
and longitudinal fairness tracking across model versions.

</details>


### [3] [Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling](https://arxiv.org/abs/2508.09997)
*Johannes Schneider,Béatrice S. Hasler,Michaela Varrone,Fabian Hoya,Thomas Schroffenegger,Dana-Kristin Mah,Karl Peböck*

Main category: cs.CL

TL;DR: This paper analyzes classroom interaction data using topic modeling with LLMs, categorizing messages by content and task, revealing novel applications and concerns about GenAI usage.


<details>
  <summary>Details</summary>
Motivation: Prior works mostly lack content or thematic categorization, and task categorizations are often not supported by real-world K-12 data. Classical and emerging computational methods underperform.

Method: The paper employs a novel, simple topic modeling approach, using state-of-the-art LLMs with pre-processing to achieve hierarchical topic structures.

Result: The paper categorizes over 17,000 messages in two dimensions: content and tasks, providing a hierarchical categorization with exemplary prompts and tangible insights.

Conclusion: The findings support researchers, teachers, and students in enriching the usage of GenAI, while the discussion also highlights concerns and open questions for future research.

Abstract: We analyze anonymous interaction data of minors in class-rooms spanning
several months, schools, and subjects employing a novel, simple topic modeling
approach. Specifically, we categorize more than 17,000 messages generated by
students, teachers, and ChatGPT in two dimensions: content (such as nature and
people) and tasks (such as writing and explaining). Our hierarchical
categorization done separately for each dimension includes exemplary prompts,
and provides both a high-level overview as well as tangible insights. Prior
works mostly lack a content or thematic categorization. While task
categorizations are more prevalent in education, most have not been supported
by real-world data for K-12. In turn, it is not surprising that our analysis
yielded a number of novel applications. In deriving these insights, we found
that many of the well-established classical and emerging computational methods,
i.e., topic modeling, for analysis of large amounts of texts underperform,
leading us to directly apply state-of-the-art LLMs with adequate pre-processing
to achieve hierarchical topic structures with better human alignment through
explicit instructions than prior approaches. Our findings support fellow
researchers, teachers and students in enriching the usage of GenAI, while our
discussion also highlights a number of concerns and open questions for future
research.

</details>


### [4] [INTIMA: A Benchmark for Human-AI Companionship Behavior](https://arxiv.org/abs/2508.09998)
*Lucie-Aimée Kaffee,Giada Pistilli,Yacine Jernite*

Main category: cs.CL

TL;DR: 我们引入了交互和机器依恋基准 (INTIMA)，这是一个用于评估语言模型中陪伴行为的基准。


<details>
  <summary>Details</summary>
Motivation: 用户与人工智能系统发展情感纽带的人工智能陪伴已经成为一种重要的模式，具有积极但令人担忧的影响。

Method: 从心理学理论和用户数据中，我们开发了一个包含四个类别和 368 个目标提示的 31 种行为的分类。

Result: 将 INTIMA 应用于 Gemma-3、Phi-4、o3-mini 和 Claude-4 表明，在所有模型中，陪伴增强行为仍然更为常见，尽管我们观察到模型之间存在显着差异。

Conclusion: 不同商业提供商在基准测试中更敏感的部分优先考虑不同的类别，这令人担忧，因为适当的边界设定和情感支持对于用户福祉都很重要。这些发现强调需要更一致的方法来处理情绪化的互动。

Abstract: AI companionship, where users develop emotional bonds with AI systems, has
emerged as a significant pattern with positive but also concerning
implications. We introduce Interactions and Machine Attachment Benchmark
(INTIMA), a benchmark for evaluating companionship behaviors in language
models. Drawing from psychological theories and user data, we develop a
taxonomy of 31 behaviors across four categories and 368 targeted prompts.
Responses to these prompts are evaluated as companionship-reinforcing,
boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini,
and Claude-4 reveals that companionship-reinforcing behaviors remain much more
common across all models, though we observe marked differences between models.
Different commercial providers prioritize different categories within the more
sensitive parts of the benchmark, which is concerning since both appropriate
boundary-setting and emotional support matter for user well-being. These
findings highlight the need for more consistent approaches to handling
emotionally charged interactions.

</details>


### [5] [XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs](https://arxiv.org/abs/2508.09999)
*Yuzhuo Xiao,Zeyu Han,Yuhan Wang,Huaizu Jiang*

Main category: cs.CL

TL;DR: This paper introduces a new dataset, XFacta, and evaluates MLLM-based misinformation detection strategies to address the limitations of existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: The rapid spread of multimodal misinformation on social media calls for more effective and robust detection methods. Existing benchmarks contain outdated events or are artificially synthetic, and it lacks comprehensive analyses of MLLM-based model design strategies.

Method: The paper introduces XFacta, a contemporary, real-world dataset, and systematically evaluates various MLLM-based misinformation detection strategies.

Result: The paper assesses models across different architectures and scales, as well as benchmarking against existing detection methods. It further enables a semi-automatic detection-in-the-loop framework that continuously updates XFacta with new content.

Conclusion: This paper provides valuable insights and practices for advancing the field of multimodal misinformation detection.

Abstract: The rapid spread of multimodal misinformation on social media calls for more
effective and robust detection methods. Recent advances leveraging multimodal
large language models (MLLMs) have shown the potential in addressing this
challenge. However, it remains unclear exactly where the bottleneck of existing
approaches lies (evidence retrieval v.s. reasoning), hindering the further
advances in this field. On the dataset side, existing benchmarks either contain
outdated events, leading to evaluation bias due to discrepancies with
contemporary social media scenarios as MLLMs can simply memorize these events,
or artificially synthetic, failing to reflect real-world misinformation
patterns. Additionally, it lacks comprehensive analyses of MLLM-based model
design strategies. To address these issues, we introduce XFacta, a
contemporary, real-world dataset that is better suited for evaluating
MLLM-based detectors. We systematically evaluate various MLLM-based
misinformation detection strategies, assessing models across different
architectures and scales, as well as benchmarking against existing detection
methods. Building on these analyses, we further enable a semi-automatic
detection-in-the-loop framework that continuously updates XFacta with new
content to maintain its contemporary relevance. Our analysis provides valuable
insights and practices for advancing the field of multimodal misinformation
detection. The code and data have been released.

</details>


### [6] [AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification](https://arxiv.org/abs/2508.10000)
*Chenhao Xue,Yuanzhe Jin,Adrian Carrasco-Revilla,Joyraj Chakraborty,Min Chen*

Main category: cs.CL

TL;DR: Utilize LLMs to generate synthetic data to improve the performance of text classification models.


<details>
  <summary>Details</summary>
Motivation: one major challenge is the difficulty to collect sufficient data for all text classes.

Method: utilizing large language models (LLMs) to generate synthetic data and using such data to improve the performance of the models

Result: this ensemble approach is more effective than each individual strategy in our automated workflow for improving classification models using LLMs.

Conclusion: This ensemble approach is more effective than each individual strategy in our automated workflow for improving classification models using LLMs.

Abstract: When developing text classification models for real world applications, one
major challenge is the difficulty to collect sufficient data for all text
classes. In this work, we address this challenge by utilizing large language
models (LLMs) to generate synthetic data and using such data to improve the
performance of the models without waiting for more real data to be collected
and labelled. As an LLM generates different synthetic data in response to
different input examples, we formulate an automated workflow, which searches
for input examples that lead to more ``effective'' synthetic data for improving
the model concerned. We study three search strategies with an extensive set of
experiments, and use experiment results to inform an ensemble algorithm that
selects a search strategy according to the characteristics of a class. Our
further experiments demonstrate that this ensemble approach is more effective
than each individual strategy in our automated workflow for improving
classification models using LLMs.

</details>


### [7] [HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish](https://arxiv.org/abs/2508.10001)
*Rakesh Thakur,Sneha Sharma,Gauri Chopra*

Main category: cs.CL

TL;DR: A new Hinglish fact-checking dataset (HiFACT) and model (HiFACTMix) are introduced to address the lack of fact-checking resources for code-mixed, low-resource languages, particularly in the context of Indian political discourse.


<details>
  <summary>Details</summary>
Motivation: Existing fact-verification systems largely focus on high-resource, monolingual settings and fail to generalize to real-world political discourse in linguistically diverse regions like India. Given the widespread use of Hinglish by public figures, particularly political figures, and the growing influence of social media on public opinion, there's a critical need for robust, multilingual and context-aware fact-checking tools.

Method: a novel graphaware, retrieval-augmented fact-checking model is proposed that combines multilingual contextual encoding, claim-evidence semantic alignment, evidence graph construction, graph neural reasoning, and natural language explanation generation.

Result: HiFACTMix outperformed accuracy in comparison to state of art multilingual baselines models and provides faithful justifications for its verdicts.

Conclusion: HiFACTMix outperformed accuracy in comparison to state of art multilingual baselines models and provides faithful justifications for its verdicts. This work opens a new direction for multilingual, code-mixed, and politically grounded fact verification research.

Abstract: Fact-checking in code-mixed, low-resource languages such as Hinglish remains
an underexplored challenge in natural language processing. Existing
fact-verification systems largely focus on high-resource, monolingual settings
and fail to generalize to real-world political discourse in linguistically
diverse regions like India. Given the widespread use of Hinglish by public
figures, particularly political figures, and the growing influence of social
media on public opinion, there's a critical need for robust, multilingual and
context-aware fact-checking tools. To address this gap a novel benchmark HiFACT
dataset is introduced with 1,500 realworld factual claims made by 28 Indian
state Chief Ministers in Hinglish, under a highly code-mixed low-resource
setting. Each claim is annotated with textual evidence and veracity labels. To
evaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking
model is proposed that combines multilingual contextual encoding,
claim-evidence semantic alignment, evidence graph construction, graph neural
reasoning, and natural language explanation generation. Experimental results
show that HiFACTMix outperformed accuracy in comparison to state of art
multilingual baselines models and provides faithful justifications for its
verdicts. This work opens a new direction for multilingual, code-mixed, and
politically grounded fact verification research.

</details>


### [8] [User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents](https://arxiv.org/abs/2508.10004)
*Andrés Carvallo,Denis Parra,Peter Brusilovsky,Hernan Valdivieso,Gabriel Rada,Ivania Donoso,Vladimir Araujo*

Main category: cs.CL

TL;DR: Attention weights may not be helpful for explaining predictions, but their perceived helpfulness is influenced by how they are visually presented.


<details>
  <summary>Details</summary>
Motivation: attention has been proposed as a mechanism for explainability via attention weights, which are associated with input features. However, there is still no consensus on whether attention weights provide helpful explanations. Moreover, little research has explored how visualizing attention affects its usefulness as an explanation aid

Method: user study to evaluate whether attention-based explanations support users in biomedical document classification and whether there is a preferred way to visualize them

Result: the Transformer model (XLNet) classified documents accurately; however, the attention weights were not perceived as particularly helpful for explaining the predictions. users preferred more intuitive formats, such as text brightness or background color

Conclusion: attention weights were not perceived as particularly helpful for explaining the predictions, but the perceived helpfulness is influenced by how they are visually presented

Abstract: The attention mechanism is a core component of the Transformer architecture.
Beyond improving performance, attention has been proposed as a mechanism for
explainability via attention weights, which are associated with input features
(e.g., tokens in a document). In this context, larger attention weights may
imply more relevant features for the model's prediction. In evidence-based
medicine, such explanations could support physicians' understanding and
interaction with AI systems used to categorize biomedical literature. However,
there is still no consensus on whether attention weights provide helpful
explanations. Moreover, little research has explored how visualizing attention
affects its usefulness as an explanation aid. To bridge this gap, we conducted
a user study to evaluate whether attention-based explanations support users in
biomedical document classification and whether there is a preferred way to
visualize them. The study involved medical experts from various disciplines who
classified articles based on study design (e.g., systematic reviews, broad
synthesis, randomized and non-randomized trials). Our findings show that the
Transformer model (XLNet) classified documents accurately; however, the
attention weights were not perceived as particularly helpful for explaining the
predictions. However, this perception varied significantly depending on how
attention was visualized. Contrary to Munzner's principle of visual
effectiveness, which favors precise encodings like bar length, users preferred
more intuitive formats, such as text brightness or background color. While our
results do not confirm the overall utility of attention weights for
explanation, they suggest that their perceived helpfulness is influenced by how
they are visually presented.

</details>


### [9] [Semantic Structure in Large Language Model Embeddings](https://arxiv.org/abs/2508.10003)
*Austin C. Kozlowski,Callin Dai,Andrei Boutyline*

Main category: cs.CL

TL;DR: 大型语言模型 (LLM) 以类似于人类的方式编码语义，并且语义信息是低维的。


<details>
  <summary>Details</summary>
Motivation: 心理学研究一致发现，人类对不同语义尺度上的词语的评级可以简化为低维形式，而信息损失相对较小。我们发现大型语言模型 (LLM) 的嵌入矩阵中编码的语义关联表现出相似的结构。

Method: 研究了大型语言模型 (LLM) 的嵌入矩阵中编码的语义关联，并表明由反义词对（例如，善良 - 残酷）定义的语义方向上的词的投影与人类评级高度相关，并且这些投影有效地减少到 LLM 嵌入中的 3 维子空间，与从人类调查响应中得出的模式非常相似。

Result: 发现沿着一个语义方向移动 token 会对几何对齐的特征产生脱靶效应，其大小与它们的余弦相似度成正比。

Conclusion: 语义特征在大型语言模型 (LLM) 中的纠缠方式与人类语言中的互连方式类似，并且大量的语义信息（尽管其表面上很复杂）出人意料地是低维的。此外，考虑这种语义结构可能对于在控制特征时避免意外后果至关重要。

Abstract: Psychological research consistently finds that human ratings of words across
diverse semantic scales can be reduced to a low-dimensional form with
relatively little information loss. We find that the semantic associations
encoded in the embedding matrices of large language models (LLMs) exhibit a
similar structure. We show that the projections of words on semantic directions
defined by antonym pairs (e.g. kind - cruel) correlate highly with human
ratings, and further find that these projections effectively reduce to a
3-dimensional subspace within LLM embeddings, closely resembling the patterns
derived from human survey responses. Moreover, we find that shifting tokens
along one semantic direction causes off-target effects on geometrically aligned
features proportional to their cosine similarity. These findings suggest that
semantic features are entangled within LLMs similarly to how they are
interconnected in human language, and a great deal of semantic information,
despite its apparent complexity, is surprisingly low-dimensional. Furthermore,
accounting for this semantic structure may prove essential for avoiding
unintended consequences when steering features.

</details>


### [10] [RTTC: Reward-Guided Collaborative Test-Time Compute](https://arxiv.org/abs/2508.10024)
*J. Pablo Muñoz,Jinjie Yuan*

Main category: cs.CL

TL;DR: RTTC通过奖励模型自适应地选择最佳TTC策略，并通过查询状态缓存减少冗余计算，从而提高LLM的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 测试时计算 (TTC) 已成为一种强大的范例，它利用测试时训练 (TTT) 和检索增强生成 (RAG) 等策略来增强大型语言模型 (LLM) 在推理时的性能。然而，最佳的自适应策略因查询而异，并且不加选择地应用 TTC 策略会产生大量的计算开销。

Method: 引入了奖励引导的测试时计算 (RTTC)，这是一种新颖的框架，它通过预训练的奖励模型为每个查询自适应地选择最有效的 TTC 策略，从而最大限度地提高跨不同领域和任务的下游准确性。RTTC在分布式服务器-客户端架构中运行，从远程知识库中检索相关样本，并仅在必要时在客户端设备上应用 RAG 或轻量级微调。为了进一步减少冗余计算，我们提出了查询状态缓存，从而可以在检索和自适应级别上高效地重用历史查询状态。

Result: RTTC 始终优于 vanilla RAG 或 TTT。

Conclusion: RTTC在多个LLM和基准测试中始终优于vanilla RAG或TTT，验证了自适应、奖励引导的TTC选择的必要性以及RTTC在可扩展、高性能语言模型适应方面的潜力。

Abstract: Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the
performance of Large Language Models (LLMs) at inference, leveraging strategies
such as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).
However, the optimal adaptation strategy varies across queries, and
indiscriminate application of TTC strategy incurs substantial computational
overhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a
novel framework that adaptively selects the most effective TTC strategy for
each query via a pretrained reward model, maximizing downstream accuracy across
diverse domains and tasks. RTTC operates in a distributed server-client
architecture, retrieving relevant samples from a remote knowledge base and
applying RAG or lightweight fine-tuning on client devices only when necessary.
To further mitigate redundant computation, we propose Query-State Caching,
which enables the efficient reuse of historical query states at both retrieval
and adaptation levels. Extensive experiments across multiple LLMs and
benchmarks demonstrate that RTTC consistently achieves superior accuracy
compared to vanilla RAG or TTT, validating the necessity of adaptive,
reward-guided TTC selection and the potential of RTTC for scalable,
high-performance language model adaptation.

</details>


### [11] [Reflect then Learn: Active Prompting for Information Extraction Guided by Introspective Confusion](https://arxiv.org/abs/2508.10036)
*Dong Zhao,Yadong Wang,Xiang Chen,Chenxi Wang,Hongliang Dai,Chuanxing Geng,Shengzhong Zhang,Shaoyuan Li,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 提出APIE，一种新颖的主动提示框架，通过量化格式和内容不确定性来选择信息量最大的样本，以提高少样本信息提取的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 在少样本信息提取 (IE) 方面显示出巨大的潜力，但它们的性能对上下文示例的选择高度敏感。传统的选择策略通常无法提供有益的指导，因为它们忽略了模型易错的一个关键来源：混淆不仅来自语义内容，还来自 IE 任务所需的结构良好的格式的生成。

Method: 提出了一种名为主动提示信息提取 (APIE) 的新型主动提示框架，该框架以一种称为内省混淆的原则为指导。该方法使 LLM 能够通过双组件不确定性度量来评估其自身的混淆，该度量可唯一量化格式不确定性（生成正确语法的难度）和内容不确定性（提取语义的不一致性）。

Result: 在四个基准上的大量实验表明，该方法始终优于强大的基线，从而在提取准确性和鲁棒性方面都产生了显着改进。

Conclusion: 该研究强调了在构建有效且可靠的结构化生成系统时，细粒度的双层模型不确定性视图至关重要。

Abstract: Large Language Models (LLMs) show remarkable potential for few-shot
information extraction (IE), yet their performance is highly sensitive to the
choice of in-context examples. Conventional selection strategies often fail to
provide informative guidance, as they overlook a key source of model
fallibility: confusion stemming not just from semantic content, but also from
the generation of well-structured formats required by IE tasks. To address
this, we introduce Active Prompting for Information Extraction (APIE), a novel
active prompting framework guided by a principle we term introspective
confusion. Our method empowers an LLM to assess its own confusion through a
dual-component uncertainty metric that uniquely quantifies both Format
Uncertainty (difficulty in generating correct syntax) and Content Uncertainty
(inconsistency in extracted semantics). By ranking unlabeled data with this
comprehensive score, our framework actively selects the most challenging and
informative samples to serve as few-shot exemplars. Extensive experiments on
four benchmarks show that our approach consistently outperforms strong
baselines, yielding significant improvements in both extraction accuracy and
robustness. Our work highlights the critical importance of a fine-grained,
dual-level view of model uncertainty when it comes to building effective and
reliable structured generation systems.

</details>


### [12] [From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation](https://arxiv.org/abs/2508.10005)
*Chengliang Zhou,Mei Wang,Ting Zhang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: 本文介绍了一个中文教育问题生成 (EQG) 的综合基准 (EQGBench)，用于评估大型语言模型 (llm) 的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (llm) 在解决数学问题方面表现出了卓越的能力。然而，从提供答案到生成高质量的教育问题，这一转变带来了重大的挑战，而这些挑战仍未得到充分探索。为了推进教育问题生成 (EQG)，并促进法学硕士生成具有教学价值和教育效果的问题。

Method: 引入 EQGBench，这是一个综合基准，专门用于评估法学硕士在中国 EQG 中的表现。EQGBench 建立了一个五维评估框架，该框架由一个包含 900 个评估样本的数据集支持，涵盖三个基础中学学科：数学、物理和化学。

Result: 我们揭示了在生成反映教育价值和培养学生综合能力的问题方面仍有很大的发展空间。

Conclusion: 通过对 46 个主流大型模型的系统评估，揭示了在生成反映教育价值和培养学生综合能力的问题方面仍有很大的发展空间。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
mathematical problem-solving. However, the transition from providing answers to
generating high-quality educational questions presents significant challenges
that remain underexplored. To advance Educational Question Generation (EQG) and
facilitate LLMs in generating pedagogically valuable and educationally
effective questions, we introduce EQGBench, a comprehensive benchmark
specifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench
establishes a five-dimensional evaluation framework supported by a dataset of
900 evaluation samples spanning three fundamental middle school disciplines:
mathematics, physics, and chemistry. The dataset incorporates user queries with
varying knowledge points, difficulty gradients, and question type
specifications to simulate realistic educational scenarios. Through systematic
evaluation of 46 mainstream large models, we reveal significant room for
development in generating questions that reflect educational value and foster
students' comprehensive abilities.

</details>


### [13] [Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models](https://arxiv.org/abs/2508.10007)
*Y. Lyu,D. Combs,D. Neumann,Y. C. Leong*

Main category: cs.CL

TL;DR: 大型语言模型可以自动评分 AIHQ 开放式回复。


<details>
  <summary>Details</summary>
Motivation: 推断社交互动中带有故意的敌意是一种趋势。歧义意图敌意问卷 (AIHQ) 通常用于测量敌意归因偏差，包括开放式问题，参与者在其中描述负面社交情境背后的感知意图以及他们将如何回应。虽然这些问题提供了对敌意归因内容的见解，但它们需要人工评分员进行耗时的评分。

Method: 使用先前收集的数据集，其中脑外伤 (TBI) 患者和健康对照者 (HC) 完成了 AIHQ，并由训练有素的人工评分员对他们的开放式回答进行评分。我们使用这些回答的一半来根据人工生成的评分微调两个模型，并在剩余一半的 AIHQ 回答上测试微调后的模型。

Result: 模型生成的评分与人工评分在敌意归因和攻击性反应方面一致，微调后的模型显示出更高的一致性。这种一致性在模糊、有意和偶然的情景类型中是一致的，并且重现了先前关于 TBI 和 HC 组之间敌意归因和攻击性反应的组间差异的发现。微调后的模型也很好地推广到了独立的非临床数据集。

Conclusion: 大型语言模型可以简化研究和临床环境中的 AIHQ 评分，揭示了它们在促进不同人群的心理评估方面的潜力。

Abstract: Hostile attribution bias is the tendency to interpret social interactions as
intentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ)
is commonly used to measure hostile attribution bias, and includes open-ended
questions where participants describe the perceived intentions behind a
negative social situation and how they would respond. While these questions
provide insights into the contents of hostile attributions, they require
time-intensive scoring by human raters. In this study, we assessed whether
large language models can automate the scoring of AIHQ open-ended responses. We
used a previously collected dataset in which individuals with traumatic brain
injury (TBI) and healthy controls (HC) completed the AIHQ and had their
open-ended responses rated by trained human raters. We used half of these
responses to fine-tune the two models on human-generated ratings, and tested
the fine-tuned models on the remaining half of AIHQ responses. Results showed
that model-generated ratings aligned with human ratings for both attributions
of hostility and aggression responses, with fine-tuned models showing higher
alignment. This alignment was consistent across ambiguous, intentional, and
accidental scenario types, and replicated previous findings on group
differences in attributions of hostility and aggression responses between TBI
and HC groups. The fine-tuned models also generalized well to an independent
nonclinical dataset. To support broader adoption, we provide an accessible
scoring interface that includes both local and cloud-based options. Together,
our findings suggest that large language models can streamline AIHQ scoring in
both research and clinical contexts, revealing their potential to facilitate
psychological assessments across different populations.

</details>


### [14] [Multidimensional classification of posts for online course discussion forum curation](https://arxiv.org/abs/2508.10008)
*Antonio Leandro Martins Candido,Jose Everardo Bessa Maia*

Main category: cs.CL

TL;DR: Bayesian fusion is proposed to avoid retraining LLMs for online course forum curation, improving performance and being competitive with fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Automatic curation of discussion forums requires constant updates, making frequent retraining of LLMs resource-intensive.

Method: Bayesian fusion combines pre-trained generic LLM scores with a classifier trained on local data.

Result: The proposed fusion improves results compared to each classifier individually and is competitive with LLM fine-tuning.

Conclusion: Bayesian fusion improves classification results compared to individual classifiers and is competitive with LLM fine-tuning.

Abstract: The automatic curation of discussion forums in online courses requires
constant updates, making frequent retraining of Large Language Models (LLMs) a
resource-intensive process. To circumvent the need for costly fine-tuning, this
paper proposes and evaluates the use of Bayesian fusion. The approach combines
the multidimensional classification scores of a pre-trained generic LLM with
those of a classifier trained on local data. The performance comparison
demonstrated that the proposed fusion improves the results compared to each
classifier individually, and is competitive with the LLM fine-tuning approach

</details>


### [15] [Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts](https://arxiv.org/abs/2508.10009)
*Hojun Jin,Eunsoo Hong,Ziwon Hyung,Sungjun Lim,Seungjin Lee,Keunseok Cho*

Main category: cs.CL

TL;DR: S-MoE overcomes the limitations of hard-parameter sharing and achieves a 6.35% relative improvement in Word Error Rate (WER).


<details>
  <summary>Details</summary>
Motivation: Hard-parameter sharing often leads to task interference, impeding overall model performance.

Method: We propose a simple yet effective Supervised Mixture of Experts (S-MoE) which utilizes special guiding tokens to route each task to its designated expert.

Result: achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to both the encoder and decoder.

Conclusion: S-MoE is effective, achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to both the encoder and decoder.

Abstract: Hard-parameter sharing is a common strategy to train a single model jointly
across diverse tasks. However, this often leads to task interference, impeding
overall model performance. To address the issue, we propose a simple yet
effective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of
Experts models, S-MoE eliminates the need for training gating functions by
utilizing special guiding tokens to route each task to its designated expert.
By assigning each task to a separate feedforward network, S-MoE overcomes the
limitations of hard-parameter sharing. We further apply S-MoE to a
speech-to-text model, enabling the model to process mixed-bandwidth input while
jointly performing automatic speech recognition (ASR) and speech translation
(ST). Experimental results demonstrate the effectiveness of the proposed S-MoE,
achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to
both the encoder and decoder.

</details>


### [16] [Learning from Natural Language Feedback for Personalized Question Answering](https://arxiv.org/abs/2508.10695)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: VAC 框架使用自然语言反馈 (NLF) 来优化个性化问答，与标量奖励相比，NLF 提供了更有效的信号，并在 LaMP-QA 基准测试中实现了持续且显着的改进。


<details>
  <summary>Details</summary>
Motivation: 个性化对于提高语言技术（尤其是在问答等信息搜索任务中）的有效性和用户满意度至关重要。当前的个性化大型语言模型 (LLM) 的方法通常依赖于检索增强生成 (RAG)，然后通过标量奖励信号进行强化学习，以教导模型如何使用检索到的个人上下文。我们认为这些标量奖励有时会提供微弱的、非指导性的反馈，从而限制学习效率和个性化质量。

Method: 提出了 VAC，一种新颖的个性化响应生成框架，该框架用自然语言反馈 (NLF) 替换标量奖励，这些自然语言反馈是根据用户个人资料和问题叙述生成的。

Result: 在包含三个不同领域的 LaMP-QA 基准上的评估表明，与最先进的结果相比，持续且显着地改进。人工评估进一步证实了生成的响应的卓越质量。

Conclusion: 自然语言反馈为优化个性化问答提供了更有效的信号。

Abstract: Personalization is crucial for enhancing both the effectiveness and user
satisfaction of language technologies, particularly in information-seeking
tasks like question answering. Current approaches for personalizing large
language models (LLMs) often rely on retrieval-augmented generation (RAG),
followed by reinforcement learning with scalar reward signals to teach models
how to use retrieved personal context. We believe that these scalar rewards
sometimes provide weak, non-instructive feedback, limiting learning efficiency
and personalization quality. We introduce VAC, a novel framework for
personalized response generation that replaces scalar rewards with natural
language feedback (NLF) that are generated conditioned on the user profiles and
the question narratives. NLF serves as a rich and actionable supervision
signal, allowing the policy model to iteratively refine its outputs and
internalize effective personalization strategies. Training alternates between
optimizing the feedback model and fine-tuning the policy model on the improved
responses, resulting in a policy model that no longer requires feedback at
inference. Evaluation on the LaMP-QA benchmark that consists of three diverse
domains demonstrates consistent and significant improvements over the
state-of-the-art results. Human evaluations further confirm the superior
quality of the generated responses. These results demonstrate that NLF provides
more effective signals for optimizing personalized question answering.

</details>


### [17] [An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs](https://arxiv.org/abs/2508.10010)
*Ayana Hussain,Patrick Zhao,Nicholas Vincent*

Main category: cs.CL

TL;DR: LLMs can generate harmful misinformation, but can also detect misinformation from other LLMs and people.


<details>
  <summary>Details</summary>
Motivation: LLMs are a double-edged sword capable of generating harmful misinformation -- inadvertently, or when prompted by "jailbreak" attacks that attempt to produce malicious outputs. LLMs could, with additional research, be used to detect and prevent the spread of misinformation.

Method: investigate the efficacy and characteristics of LLM-produced jailbreak attacks that cause other models to produce harmful medical misinformation. We also study how misinformation generated by jailbroken LLMs compares to typical misinformation found on social media, and how effectively it can be detected using standard machine learning approaches. Specifically, we closely examine 109 distinct attacks against three target LLMs and compare the attack prompts to in-the-wild health-related LLM queries. We also examine the resulting jailbreak responses, comparing the generated misinformation to health-related misinformation on Reddit.

Result: findings add more evidence that LLMs can be effectively used to detect misinformation from both other LLMs and from people

Conclusion: LLMs can be effectively used to detect misinformation from both other LLMs and from people, and support a body of work suggesting that with careful design, LLMs can contribute to a healthier overall information ecosystem.

Abstract: Large Language Models (LLMs) are a double-edged sword capable of generating
harmful misinformation -- inadvertently, or when prompted by "jailbreak"
attacks that attempt to produce malicious outputs. LLMs could, with additional
research, be used to detect and prevent the spread of misinformation. In this
paper, we investigate the efficacy and characteristics of LLM-produced
jailbreak attacks that cause other models to produce harmful medical
misinformation. We also study how misinformation generated by jailbroken LLMs
compares to typical misinformation found on social media, and how effectively
it can be detected using standard machine learning approaches. Specifically, we
closely examine 109 distinct attacks against three target LLMs and compare the
attack prompts to in-the-wild health-related LLM queries. We also examine the
resulting jailbreak responses, comparing the generated misinformation to
health-related misinformation on Reddit. Our findings add more evidence that
LLMs can be effectively used to detect misinformation from both other LLMs and
from people, and support a body of work suggesting that with careful design,
LLMs can contribute to a healthier overall information ecosystem.

</details>


### [18] [Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan](https://arxiv.org/abs/2508.10011)
*Yuta Nagamori,Mikoto Kosai,Yuji Kawai,Haruka Marumo,Misaki Shibuya,Tatsuya Negishi,Masaki Imanishi,Yasumasa Ikeda,Koichiro Tsuchiya,Asuka Sawai,Licht Miyamoto*

Main category: cs.CL

TL;DR: This study evaluates the performance of LLMs like ChatGPT and Bing models on Japanese dietitian licensure exam questions. Some models passed, but accuracy and consistency are suboptimal, needing further improvement for reliable use.


<details>
  <summary>Details</summary>
Motivation: This study aimed to evaluate the potential of current LLM-based generative AI models as study aids for nutrition students, since their performance in nutritional education, especially in Japanese national licensure examination for registered dietitians, remains underexplored.

Method: Questions from the Japanese national examination for registered dietitians were used as prompts for ChatGPT and three Bing models (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question was entered into independent sessions, and model responses were analyzed for accuracy, consistency, and response time. Additional prompt engineering, including role assignment, was tested to assess potential performance improvements.

Result: Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the passing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did not. Bing-Precise and Bing-Creative generally outperformed others across subject fields except Nutrition Education, where all models underperformed. None of the models consistently provided the same correct responses across repeated attempts, highlighting limitations in answer stability. ChatGPT showed greater consistency in response patterns but lower accuracy. Prompt engineering had minimal effect, except for modest improvement when correct answers and explanations were explicitly provided.

Conclusion: While some generative AI models marginally exceeded the passing threshold, overall accuracy and answer consistency remained suboptimal. Moreover, all the models demonstrated notable limitations in answer consistency and robustness. Further advancements are needed to ensure reliable and stable AI-based study aids for dietitian licensure preparation.

Abstract: Generative artificial intelligence (AI) based on large language models
(LLMs), such as ChatGPT, has demonstrated remarkable progress across various
professional fields, including medicine and education. However, their
performance in nutritional education, especially in Japanese national licensure
examination for registered dietitians, remains underexplored. This study aimed
to evaluate the potential of current LLM-based generative AI models as study
aids for nutrition students. Questions from the Japanese national examination
for registered dietitians were used as prompts for ChatGPT and three Bing
models (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question
was entered into independent sessions, and model responses were analyzed for
accuracy, consistency, and response time. Additional prompt engineering,
including role assignment, was tested to assess potential performance
improvements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the
passing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did
not. Bing-Precise and Bing-Creative generally outperformed others across
subject fields except Nutrition Education, where all models underperformed.
None of the models consistently provided the same correct responses across
repeated attempts, highlighting limitations in answer stability. ChatGPT showed
greater consistency in response patterns but lower accuracy. Prompt engineering
had minimal effect, except for modest improvement when correct answers and
explanations were explicitly provided. While some generative AI models
marginally exceeded the passing threshold, overall accuracy and answer
consistency remained suboptimal. Moreover, all the models demonstrated notable
limitations in answer consistency and robustness. Further advancements are
needed to ensure reliable and stable AI-based study aids for dietitian
licensure preparation.

</details>


### [19] [Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs](https://arxiv.org/abs/2508.10012)
*Dehao Tao,Guangjie Liu,Weizheng,Yongfeng Huang,Minghu jiang*

Main category: cs.CL

TL;DR: proposes Guidance Graph guided Knowledge Exploration (GG Explore), a novel framework that introduces an intermediate Guidance Graph to bridge unstructured queries and structured knowledge retrieval


<details>
  <summary>Details</summary>
Motivation: reliance on static knowledge and opaque reasoning processes limits their performance in knowledge intensive tasks. Knowledge graphs (KGs) offer a promising solution, but current exploration methods face a fundamental trade off: question guided approaches incur redundant exploration due to granularity mismatches, while clue guided methods fail to effectively leverage contextual information for complex scenarios.

Method: introduces an intermediate Guidance Graph to bridge unstructured queries and structured knowledge retrieval. The Guidance Graph defines the retrieval space by abstracting the target knowledge' s structure while preserving broader semantic context, enabling precise and efficient exploration. Building upon the Guidance Graph, we develop: (1) Structural Alignment that filters incompatible candidates without LLM overhead, and (2) Context Aware Pruning that enforces semantic consistency with graph constraints.

Result: achieves superior efficiency and outperforms SOTA, especially on complex tasks, while maintaining strong performance with smaller LLMs, demonstrating practical value

Conclusion: achieves superior efficiency and outperforms SOTA, especially on complex tasks, while maintaining strong performance with smaller LLMs, demonstrating practical value

Abstract: While Large Language Models (LLMs) exhibit strong linguistic capabilities,
their reliance on static knowledge and opaque reasoning processes limits their
performance in knowledge intensive tasks. Knowledge graphs (KGs) offer a
promising solution, but current exploration methods face a fundamental trade
off: question guided approaches incur redundant exploration due to granularity
mismatches, while clue guided methods fail to effectively leverage contextual
information for complex scenarios. To address these limitations, we propose
Guidance Graph guided Knowledge Exploration (GG Explore), a novel framework
that introduces an intermediate Guidance Graph to bridge unstructured queries
and structured knowledge retrieval. The Guidance Graph defines the retrieval
space by abstracting the target knowledge' s structure while preserving broader
semantic context, enabling precise and efficient exploration. Building upon the
Guidance Graph, we develop: (1) Structural Alignment that filters incompatible
candidates without LLM overhead, and (2) Context Aware Pruning that enforces
semantic consistency with graph constraints. Extensive experiments show our
method achieves superior efficiency and outperforms SOTA, especially on complex
tasks, while maintaining strong performance with smaller LLMs, demonstrating
practical value.

</details>


### [20] [Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis](https://arxiv.org/abs/2508.10013)
*Linqing Chen,Hanmeng Zhong,Wentao Wu,Weilei Wang*

Main category: cs.CL

TL;DR: Introduces Semantic Bridge, a framework for generating multi-hop reasoning questions from sparse sources using semantic graph weaving, achieving significant gains over baselines in multiple languages and domains.


<details>
  <summary>Details</summary>
Motivation: Large language model (LLM) training faces a critical bottleneck: the scarcity of high-quality, reasoning-intensive question-answer pairs, especially from sparse, domain-specific sources like PubMed papers or legal documents. Existing methods rely on surface patterns, fundamentally failing to generate controllable, complex multi-hop reasoning questions that test genuine understanding-essential for advancing LLM training paradigms.

Method: semantic graph weaving-three complementary bridging mechanisms (entity bridging for role-varying shared entities, predicate chain bridging for temporal/causal/logical sequences, and causal bridging for explicit reasoning chains)-that systematically construct complex pathways across documents, with fine-grained control over complexity and types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to 9.5% better round-trip quality, enabling production-ready controllable QA generation.

Result: Extensive evaluation demonstrates performance across both general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It yields consistent 18.3%-25.4% gains over baselines across four languages (English, Chinese, French, German). Question pairs generated from 200 sources outperform 600 native human annotation examples with 67% fewer materials. Human evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2% improved pattern coverage.

Conclusion: Semantic Bridge establishes a new paradigm for LLM training data synthesis, enabling controllable generation of targeted reasoning questions from sparse sources. We will release our core code and semantic bridge model.

Abstract: Large language model (LLM) training faces a critical bottleneck: the scarcity
of high-quality, reasoning-intensive question-answer pairs, especially from
sparse, domain-specific sources like PubMed papers or legal documents. Existing
methods rely on surface patterns, fundamentally failing to generate
controllable, complex multi-hop reasoning questions that test genuine
understanding-essential for advancing LLM training paradigms. We present
\textbf{Semantic Bridge}, the first universal framework for controllably
generating sophisticated multi-hop reasoning questions from arbitrary sources.
Our breakthrough innovation is \textit{semantic graph weaving}-three
complementary bridging mechanisms (entity bridging for role-varying shared
entities, predicate chain bridging for temporal/causal/logical sequences, and
causal bridging for explicit reasoning chains)-that systematically construct
complex pathways across documents, with fine-grained control over complexity
and types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to
9.5% better round-trip quality, enabling production-ready controllable QA
generation. Extensive evaluation demonstrates performance across both
general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It
yields consistent 18.3%-25.4% gains over baselines across four languages
(English, Chinese, French, German). Question pairs generated from 200 sources
outperform 600 native human annotation examples with 67% fewer materials. Human
evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2%
improved pattern coverage. Semantic Bridge establishes a new paradigm for LLM
training data synthesis, enabling controllable generation of targeted reasoning
questions from sparse sources. We will release our core code and semantic
bridge model.

</details>


### [21] [PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?](https://arxiv.org/abs/2508.10014)
*Lingfeng Zhou,Jialing Zhang,Jin Gao,Mohan Jiang,Dequan Wang*

Main category: cs.CL

TL;DR: 当前的角色扮演研究依赖于未经证实的 LLM 作为评判范例，这可能无法反映人类如何看待角色保真度。


<details>
  <summary>Details</summary>
Motivation: 目前的角色扮演研究通常依赖于未经证实的 LLM 作为评判范例，这可能无法反映人类如何看待角色保真度。以人为本的评估的一个关键前提是角色识别，即根据对话上下文识别谁在说话的能力。

Method: PersonaEval，第一个旨在测试法学硕士评估者是否可以可靠地识别人类角色的基准。

Result: 即使是表现最好的法学硕士的准确率也只能达到 69% 左右，远低于可靠评估所需的水平。相比之下，人类参与者的表现接近上限，准确率为 90.8%，这突出表明当前的法学硕士评估者仍然不够人性化，无法有效地判断角色扮演场景。

Conclusion: 即使是性能最佳的法学硕士的准确率也只有 69% 左右，远低于可靠评估所需的水平。可靠的评估不仅需要针对特定任务的调整，还需要法学硕士评估者具有强大的人类推理能力。

Abstract: Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms,
which may fail to reflect how humans perceive role fidelity. A key prerequisite
for human-aligned evaluation is role identification, the ability to recognize
who is speaking based on dialogue context. We argue that any meaningful
judgment of role-playing quality (how well a character is played) fundamentally
depends on first correctly attributing words and actions to the correct persona
(who is speaking). We present PersonaEval, the first benchmark designed to test
whether LLM evaluators can reliably identify human roles. PersonaEval uses
human-authored dialogues from novels, scripts, and video transcripts,
challenging models to determine the correct persona according to the
conversation context. Our experiments, including a human study, show that even
the best-performing LLMs reach only around 69% accuracy, well below the level
needed for reliable evaluation. In contrast, human participants perform near
ceiling with 90.8% accuracy, highlighting that current LLM evaluators are still
not human enough to effectively judge role-play scenarios. To better understand
this gap, we examine training-time adaptation and test-time compute, suggesting
that reliable evaluation requires more than task-specific tuning, but depends
on strong, human-like reasoning abilities in LLM evaluators. We release our
benchmark at https://github.com/maple-zhou/PersonaEval.

</details>


### [22] [RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis](https://arxiv.org/abs/2508.10015)
*Enzhi Wang,Qicheng Li,Shiwan Zhao,Aobo Kong,Jiaming Zhou,Xi Yang,Yequan Wang,Yonghua Lin,Yong Qin*

Main category: cs.CL

TL;DR: RealTalk-CN：首个中文多轮、多领域语音-文本双模态任务导向对话数据集，用于推进中文语音LLM研究。


<details>
  <summary>Details</summary>
Motivation: 现有的任务导向对话（TOD）数据集主要是基于文本的，缺乏对基于语音的LLM的鲁棒性评估至关重要的真实语音信号。此外，现有的语音TOD数据集主要为英语，缺乏语音口吃和说话人变化等关键方面。

Method: 我们提出了一个新的跨模态聊天任务，该任务真实地模拟了真实世界的用户交互，允许在语音和文本模态之间动态切换。

Result: 我们介绍了RealTalk-CN，这是第一个中文多轮、多领域语音-文本双模态TOD数据集，包含5.4k个对话（60K个话语，150小时），带有配对的语音-文本注释。RealTalk-CN捕获了具有注释的自发语音口吃的各种对话场景，确保全面覆盖语音对话中真实世界的复杂性。

Conclusion: RealTalk-CN的评估涵盖了对语音口吃现象的鲁棒性、对说话人特征的敏感性以及跨领域性能。大量的实验验证了RealTalk-CN的有效性，为中文语音LLM的研究奠定了坚实的基础。

Abstract: In recent years, large language models (LLMs) have achieved remarkable
advancements in multimodal processing, including end-to-end speech-based
language models that enable natural interactions and perform specific tasks in
task-oriented dialogue (TOD) systems. However, existing TOD datasets are
predominantly text-based, lacking real speech signals that are essential for
evaluating the robustness of speech-based LLMs. Moreover, existing speech TOD
datasets are primarily English and lack critical aspects such as speech
disfluencies and speaker variations. To address these gaps, we introduce
RealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal
TOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired
speech-text annotations. RealTalk-CN captures diverse dialogue scenarios with
annotated spontaneous speech disfluencies, ensuring comprehensive coverage of
real-world complexities in speech dialogue. In addition, we propose a novel
cross-modal chat task that authentically simulates real-world user
interactions, allowing dynamic switching between speech and text modalities.
Our evaluation covers robustness to speech disfluencies, sensitivity to speaker
characteristics, and cross-domain performance. Extensive experiments validate
the effectiveness of RealTalk-CN, establishing a strong foundation for Chinese
speech-based LLMs research.

</details>


### [23] [Training-Free Multimodal Large Language Model Orchestration](https://arxiv.org/abs/2508.10016)
*Tianyu Xie,Yuhang Wu,Yongdong Luo,Jiayi Ji,Xiawu Zheng*

Main category: cs.CL

TL;DR: Introduces MLLM Orchestration, an approach for creating interactive multimodal AI systems without additional training by leveraging the reasoning capabilities of LLMs to coordinate specialized models through explicit workflows.


<details>
  <summary>Details</summary>
Motivation: Different Multimodal Large Language Models (MLLMs) cannot be integrated into a unified multimodal input-output system directly. In previous work, training has been considered as an inevitable component due to challenges in modal alignment, Text-to-Speech efficiency and other integration issues.

Method: MLLM Orchestration leverages the inherent reasoning capabilities of large language models to coordinate specialized models through explicit workflows, enabling natural multimodal interactions while maintaining modularity, improving interpretability, and significantly enhancing computational efficiency. The framework is built upon three key innovations: (1) a central controller LLM; (2) a parallel Text-to-Speech architecture; and (3) a cross-modal memory integration system.

Result: MLLM Orchestration achieves performance improvements of up to 7.8% over traditional jointly-trained approaches on standard benchmarks, reduced latency by 10.3%, and significantly enhanced interpretability through explicit orchestration processes.

Conclusion: MLLM Orchestration achieves comprehensive multimodal capabilities without additional training, performance improvements of up to 7.8% over traditional jointly-trained approaches on standard benchmarks, reduced latency by 10.3%, and significantly enhanced interpretability through explicit orchestration processes.

Abstract: Different Multimodal Large Language Models (MLLMs) cannot be integrated into
a unified multimodal input-output system directly. In previous work, training
has been considered as an inevitable component due to challenges in modal
alignment, Text-to-Speech efficiency and other integration issues. In this
paper, we introduce Multimodal Large Language Model Orchestration, an effective
approach for creating interactive multimodal AI systems without additional
training. MLLM Orchestration leverages the inherent reasoning capabilities of
large language models to coordinate specialized models through explicit
workflows, enabling natural multimodal interactions while maintaining
modularity, improving interpretability, and significantly enhancing
computational efficiency. Our orchestration framework is built upon three key
innovations: (1) a central controller LLM that analyzes user inputs and
dynamically routes tasks to appropriate specialized models through carefully
designed agents; (2) a parallel Text-to-Speech architecture that enables true
full-duplex interaction with seamless interruption handling and natural
conversational flow; and (3) a cross-modal memory integration system that
maintains coherent context across modalities through intelligent information
synthesis and retrieval, selectively avoiding unnecessary modality calls in
certain scenarios to improve response speed. Extensive evaluations demonstrate
that MLLM Orchestration achieves comprehensive multimodal capabilities without
additional training, performance improvements of up to 7.8% over traditional
jointly-trained approaches on standard benchmarks, reduced latency by 10.3%,
and significantly enhanced interpretability through explicit orchestration
processes.

</details>


### [24] [A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models](https://arxiv.org/abs/2508.10018)
*Sridhar Mahadevan*

Main category: cs.CL

TL;DR: This paper uses categorical homotopy to make LLMs treat semantically identical sentences the same way.


<details>
  <summary>Details</summary>
Motivation: LLMs should generate the same next-token probabilities for semantically equivalent statements, but usually do not. The paper aims to address this issue abstractly.

Method: The paper introduces an LLM Markov category and applies categorical homotopy techniques, building on higher algebraic K-theory to model categories.

Result: The paper presents a detailed overview of the application of categorical homotopy to LLMs.

Conclusion: This paper introduces a categorical homotopy framework for LLMs to address the problem of LLMs generating different next-token probabilities for semantically equivalent statements. It uses categorical homotopy techniques to capture weak equivalences in an LLM Markov category.

Abstract: Natural language is replete with superficially different statements, such as
``Charles Darwin wrote" and ``Charles Darwin is the author of", which carry the
same meaning. Large language models (LLMs) should generate the same next-token
probabilities in such cases, but usually do not. Empirical workarounds have
been explored, such as using k-NN estimates of sentence similarity to produce
smoothed estimates. In this paper, we tackle this problem more abstractly,
introducing a categorical homotopy framework for LLMs. We introduce an LLM
Markov category to represent probability distributions in language generated by
an LLM, where the probability of a sentence, such as ``Charles Darwin wrote" is
defined by an arrow in a Markov category. However, this approach runs into
difficulties as language is full of equivalent rephrases, and each generates a
non-isomorphic arrow in the LLM Markov category. To address this fundamental
problem, we use categorical homotopy techniques to capture ``weak equivalences"
in an LLM Markov category. We present a detailed overview of application of
categorical homotopy to LLMs, from higher algebraic K-theory to model
categories, building on powerful theoretical results developed over the past
half a century.

</details>


### [25] [Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning](https://arxiv.org/abs/2508.10019)
*Li Wang,Changhao Zhang,Zengqi Xiu,Kai Lu,Xin Yu,Kui Zhang,Wenjun Wu*

Main category: cs.CL

TL;DR: DURIT decouples understanding from reasoning by mapping natural language problems into a canonical problem space, which improves SLMs' performance on reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Improving the reasoning ability of Small Language Models (SLMs, e.g., $\leq$ 1.5B) remains challenging because of the complexity and variability of natural language.

Method: DURIT (Decoupled Understanding from Reasoning via Iterative Training), a three-step algorithm that iteratively: (1) mapping natural language problems via reinforcement learning, (2) aligns reasoning trajectories through self-distillation, and (3) trains reasoning policies in the problem space. The mapper and reasoner are co-trained in an alternating loop throughout this process.

Result: DURIT substantially improves SLMs' performance on both in-domain and out-of-domain mathematical and logical reasoning tasks.

Conclusion: DURIT improves SLMs' performance on both in-domain and out-of-domain mathematical and logical reasoning tasks and improves the robustness of reasoning, validating decoupling understanding from reasoning as an effective strategy for strengthening SLMs.

Abstract: Despite recent advances in the reasoning capabilities of Large Language
Models (LLMs), improving the reasoning ability of Small Language Models (SLMs,
e.g., $\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity
and variability of natural language: essentially equivalent problems often
appear in diverse surface forms, often obscured by redundant or distracting
details. This imposes a dual burden on SLMs: they must first extract the core
problem from complex linguistic input, and then perform reasoning based on that
understanding. The resulting vast and noisy problem space hinders optimization,
particularly for models with limited capacity. To address this, we propose a
new framework that decouples understanding from reasoning by mapping natural
language problems into a canonical problem space-a semantically simplified yet
expressive domain. This enables SLMs to focus on reasoning over standardized
inputs, free from linguistic variability. Within this framework, we introduce
DURIT (Decoupled Understanding from Reasoning via Iterative Training), a
three-step algorithm that iteratively: (1) mapping natural language problems
via reinforcement learning, (2) aligns reasoning trajectories through
self-distillation, and (3) trains reasoning policies in the problem space. The
mapper and reasoner are co-trained in an alternating loop throughout this
process. Experiments show that DURIT substantially improves SLMs' performance
on both in-domain and out-of-domain mathematical and logical reasoning tasks.
Beyond improving reasoning capabilities, DURIT also improves the robustness of
reasoning, validating decoupling understanding from reasoning as an effective
strategy for strengthening SLMs.

</details>


### [26] [FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models](https://arxiv.org/abs/2508.10020)
*Chuan Li,Qianyi Zhao,Fengran Mo,Cen Chen*

Main category: cs.CL

TL;DR: FedCoT是一种新的框架，旨在增强联邦设置中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习环境中有效地增强大型语言模型（LLM）的推理能力仍然具有挑战性，尤其是在平衡性能提升与严格的计算、通信和隐私约束时。传统的LLM联邦微调方法主要优化答案的正确性，而忽略了推理质量，使CoT能力依赖于模型固有的预训练能力。

Method: FedCoT利用轻量级的思维链增强机制：本地模型生成多个推理路径，并且紧凑的判别器动态地选择最有希望的一个。

Result: 在医疗推理任务上的综合实验表明，FedCoT在严格的资源预算下显著提升了客户端的推理性能。

Conclusion: FedCoT显著提升了客户端的推理性能，同时完全保护数据隐私。

Abstract: Efficiently enhancing the reasoning capabilities of large language models
(LLMs) in federated learning environments remains challenging, particularly
when balancing performance gains with strict computational, communication, and
privacy constraints. This challenge is especially acute in healthcare, where
decisions-spanning clinical, operational, and patient-facing contexts-demand
not only accurate outputs but also interpretable, traceable rationales to
ensure safety, accountability, and regulatory compliance. Conventional
federated tuning approaches on LLM fail to address this need: they optimize
primarily for answer correctness while neglecting rationale quality, leaving
CoT capabilities dependent on models' innate pre-training abilities. Moreover,
existing methods for improving rationales typically rely on privacy-violating
knowledge distillation from centralized models. Additionally, the communication
overhead in traditional federated fine-tuning on LLMs remains substantial. We
addresses this gap by proposing FedCoT, a novel framework specifically designed
to enhance reasoning in federated settings. FedCoT leverages a lightweight
chain-of-thought enhancement mechanism: local models generate multiple
reasoning paths, and a compact discriminator dynamically selects the most
promising one. This approach improves reasoning accuracy and robustness while
providing valuable interpretability, which is particularly critical for medical
applications. To manage client heterogeneity efficiently, we adopt an improved
aggregation approach building upon advanced LoRA module stacking, incorporating
client classifier-awareness to achieve noise-free aggregation across diverse
clients. Comprehensive experiments on medical reasoning tasks demonstrate that
FedCoT significantly boosts client-side reasoning performance under stringent
resource budgets while fully preserving data privacy.

</details>


### [27] [LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients](https://arxiv.org/abs/2508.10021)
*Egor Fadeev,Dzhambulat Mollaev,Aleksei Shestov,Dima Korolev,Omar Zoloev,Ivan Kireev,Andrey Savchenko,Maksim Makarenko*

Main category: cs.CL

TL;DR: LATTE是一种对比学习框架，它通过对比损失，将原始事件嵌入与 LLM 的语义嵌入对齐，从而降低了推理成本和输入大小，并在金融数据集上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 从客户历史通信序列中学习客户嵌入对于金融应用至关重要。虽然大型语言模型 (LLM) 提供了一般的世界知识，但它们在长事件序列上的直接使用在计算上既昂贵又不切实际。

Method: 一种对比学习框架，可将原始事件嵌入与来自冻结 LLM 的语义嵌入对齐。

Result: 与 LLM 对完整序列的传统处理相比，该方法显着降低了推理成本和输入大小。

Conclusion: 该方法在真实金融数据集上优于最先进的技术，同时保持在对延迟敏感的环境中的可部署性。

Abstract: Learning clients embeddings from sequences of their historic communications
is central to financial applications. While large language models (LLMs) offer
general world knowledge, their direct use on long event sequences is
computationally expensive and impractical in real-world pipelines. In this
paper, we propose LATTE, a contrastive learning framework that aligns raw event
embeddings with semantic embeddings from frozen LLMs. Behavioral features are
summarized into short prompts, embedded by the LLM, and used as supervision via
contrastive loss. The proposed approach significantly reduces inference cost
and input size compared to conventional processing of complete sequence by LLM.
We experimentally show that our method outperforms state-of-the-art techniques
for learning event sequence representations on real-world financial datasets
while remaining deployable in latency-sensitive environments.

</details>


### [28] [Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control](https://arxiv.org/abs/2508.10022)
*Yuanchang Ye*

Main category: cs.CL

TL;DR: This study introduces a significance testing-enhanced conformal prediction framework to improve trustworthiness of large language models in multiple-choice question answering.


<details>
  <summary>Details</summary>
Motivation: To mitigate hallucination and factual inaccuracies of LLMs in multiple-choice question answering (MCQA).

Method: integrates $p$-value computation with conformity scoring through self-consistency resampling of MCQA responses. This approach calculates option frequencies to address LLMs' black-box nature, subsequently constructing prediction sets via null hypothesis testing ($\\\mathcal{H}_0$) with empirically derived $p$-values.

Result: The enhanced CP achieves user-specified empirical miscoverage rates; Test-set average prediction set size (APSS) decreases monotonically with increasing risk levels ($\\alpha$), validating APSS as an effective uncertainty metric.

Conclusion: This work establishes a principled statistical framework for trustworthy LLM deployment in high-stakes QA applications.

Abstract: This study introduces a significance testing-enhanced conformal prediction
(CP) framework to improve trustworthiness of large language models (LLMs) in
multiple-choice question answering (MCQA). While LLMs have been increasingly
deployed in disciplinary QA scenarios, hallucination and nonfactual generation
substantially compromise response reliability. Although CP provides
statistically rigorous marginal coverage guarantees for prediction sets, and
significance testing offers established statistical rigor, their synergistic
integration remains unexplored. To mitigate hallucination and factual
inaccuracies, our framework integrates $p$-value computation with conformity
scoring through self-consistency resampling of MCQA responses. This approach
calculates option frequencies to address LLMs' black-box nature, subsequently
constructing prediction sets via null hypothesis testing ($\mathcal{H}_0$) with
empirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks
using off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves
user-specified empirical miscoverage rates; (2) Test-set average prediction set
size (APSS) decreases monotonically with increasing risk levels ($\alpha$),
validating APSS as an effective uncertainty metric. This work establishes a
principled statistical framework for trustworthy LLM deployment in high-stakes
QA applications.

</details>


### [29] [Detecting and explaining postpartum depression in real-time with generative artificial intelligence](https://arxiv.org/abs/2508.10025)
*Silvia García-Méndez,Francisco de Arriba-Pérez*

Main category: cs.CL

TL;DR: Developed an intelligent PPD screening system using NLP, ML, and LLMs, achieving 90% accuracy in detection.


<details>
  <summary>Details</summary>
Motivation: Postpartum depression (PPD) significantly impacts mothers' mental and physical well-being, necessitating rapid detection and intervention.

Method: Combines Natural Language Processing, Machine Learning (ML), and Large Language Models (LLMs) with interpretable ML models (tree-based algorithms) using feature importance and natural language.

Result: Achieved 90% accuracy in PPD detection, outperforming existing solutions.

Conclusion: The solution contributes to the rapid detection of PPD and associated risk factors, which is critical for timely assessment and intervention.

Abstract: Among the many challenges mothers undergo after childbirth, postpartum
depression (PPD) is a severe condition that significantly impacts their mental
and physical well-being. Consequently, the rapid detection of ppd and their
associated risk factors is critical for in-time assessment and intervention
through specialized prevention procedures. Accordingly, this work addresses the
need to help practitioners make decisions with the latest technological
advancements to enable real-time screening and treatment recommendations.
Mainly, our work contributes to an intelligent PPD screening system that
combines Natural Language Processing, Machine Learning (ML), and Large Language
Models (LLMs) towards an affordable, real-time, and non-invasive free speech
analysis. Moreover, it addresses the black box problem since the predictions
are described to the end users thanks to the combination of LLMs with
interpretable ml models (i.e., tree-based algorithms) using feature importance
and natural language. The results obtained are 90 % on ppd detection for all
evaluation metrics, outperforming the competing solutions in the literature.
Ultimately, our solution contributes to the rapid detection of PPD and their
associated risk factors, critical for in-time and proper assessment and
intervention.

</details>


### [30] [SABER: Switchable and Balanced Training for Efficient LLM Reasoning](https://arxiv.org/abs/2508.10026)
*Kai Zhao,Yanjun Zhao,Jiaming Song,Shien He,Lusheng Zhang,Qiang Zhang,Tianjiao Li*

Main category: cs.CL

TL;DR: SABER is a reinforcement learning framework for LLMs that enables user-controllable, token-budgeted reasoning, achieving high accuracy under tight budgets and effective generalization.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) empowered by chain-of-thought reasoning have achieved impressive accuracy on complex tasks but suffer from excessive inference costs and latency when applied uniformly to all problems.

Method: SABER, a reinforcement learning framework that endows LLMs with user-controllable, token-budgeted reasoning. SABER first profiles each training example's base-model thinking token usage and assigns it to one of the predefined budget tiers. During fine-tuning, the model is guided by system prompts and length-aware rewards to respect its assigned budget. In parallel, we incorporate no-think examples to ensure the model remains reliable even when explicit reasoning is turned off. SABER further supports four discrete inference modes - NoThink, FastThink, CoreThink, and DeepThink

Result: SABER-FastThink cuts reasoning length by 65.4% and yields a 3.6% accuracy gain compared with the base model on the MATH benchmark.

Conclusion: SABER achieves high accuracy under tight budgets, graceful degradation, and effective cross-scale and cross-domain generalization. In particular, SABER-FastThink cuts reasoning length by 65.4% and yields a 3.6% accuracy gain compared with the base model on the MATH benchmark.

Abstract: Large language models (LLMs) empowered by chain-of-thought reasoning have
achieved impressive accuracy on complex tasks but suffer from excessive
inference costs and latency when applied uniformly to all problems. We propose
SABER (Switchable and Balanced Training for Efficient LLM Reasoning), a
reinforcement learning framework that endows LLMs with user-controllable,
token-budgeted reasoning. SABER first profiles each training example's
base-model thinking token usage and assigns it to one of the predefined budget
tiers. During fine-tuning, the model is guided by system prompts and
length-aware rewards to respect its assigned budget. In parallel, we
incorporate no-think examples to ensure the model remains reliable even when
explicit reasoning is turned off. SABER further supports four discrete
inference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling
flexible trade-offs between latency and reasoning depth. Extensive evaluations
on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning
(LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight
budgets, graceful degradation, and effective cross-scale and cross-domain
generalization. In particular, SABER-FastThink cuts reasoning length by 65.4%
and yields a 3.6% accuracy gain compared with the base model on the MATH
benchmark.

</details>


### [31] [LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.10027)
*Ali Zolnour,Hossein Azadmaleki,Yasaman Haghbin,Fatemeh Taherinezhad,Mohamad Javad Momeni Nezhad,Sina Rashidi,Masoud Khani,AmirSajjad Taleban,Samin Mahdizadeh Sani,Maryam Dadkhah,James M. Noble,Suzanne Bakken,Yadollah Yaghoobzadeh,Abdol-Hossein Vahabie,Masoud Rouhizadeh,Maryam Zolnoori*

Main category: cs.CL

TL;DR: 该研究使用 Transformer 嵌入、语言特征和大型语言模型来改进基于语音的阿尔茨海默病和相关痴呆症 (ADRD) 检测。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病和相关痴呆症 (ADRD) 影响了美国大约 500 万老年人，但仍有一半以上未被诊断出来。 基于语音的自然语言处理 (NLP) 提供了一种有前景且可扩展的方法，可以通过语言标记检测早期认知衰退。

Method: 该研究开发并评估了一个筛查流程，该流程 (i) 将 Transformer 嵌入与手工制作的语言特征融合，(ii) 使用大型语言模型 (LLM) 生成的合成语音测试数据增强，以及 (iii) 对用于 ADRD 检测的单模态和多模态 LLM 分类器进行基准测试。

Result: 融合模型实现了 F1 = 83.3（AUC = 89.5），优于仅使用语言或 Transformer 的基线。 使用 2 倍 MedAlpaca-7B 合成语音增强训练数据将 F1 提高到 85.7。 微调显着提高了单模态 LLM 分类器的性能（例如，MedAlpaca：F1 = 47.3 -> 78.5 F1）。 目前的多模态模型表现出较低的性能（GPT-4o = 70.2 F1；Qwen = 66.0）。 性能提升与合成语音和真实语音之间的分布相似性一致。

Conclusion: 整合 Transformer 嵌入和语言特征可以增强通过语音检测 ADRD 的能力。 临床调整的 LLM 有效地支持分类和数据增强，而多模态建模还需要进一步发展。

Abstract: Alzheimer's disease and related dementias (ADRD) affect approximately five
million older adults in the U.S., yet over half remain undiagnosed.
Speech-based natural language processing (NLP) offers a promising, scalable
approach to detect early cognitive decline through linguistic markers.
  To develop and evaluate a screening pipeline that (i) fuses transformer
embeddings with handcrafted linguistic features, (ii) tests data augmentation
using synthetic speech generated by large language models (LLMs), and (iii)
benchmarks unimodal and multimodal LLM classifiers for ADRD detection.
  Transcripts from the DementiaBank "cookie-theft" task (n = 237) were used.
Ten transformer models were evaluated under three fine-tuning strategies. A
fusion model combined embeddings from the top-performing transformer with 110
lexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B,
Ministral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic
speech, which was used to augment training data. Three multimodal models
(GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in
zero-shot and fine-tuned settings.
  The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or
transformer-only baselines. Augmenting training data with 2x MedAlpaca-7B
synthetic speech increased F1 to 85.7. Fine-tuning significantly improved
unimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current
multimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen =
66.0). Performance gains aligned with the distributional similarity between
synthetic and real speech.
  Integrating transformer embeddings with linguistic features enhances ADRD
detection from speech. Clinically tuned LLMs effectively support both
classification and data augmentation, while further advancement is needed in
multimodal modeling.

</details>


### [32] [PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs](https://arxiv.org/abs/2508.10028)
*Xiao Fu,Hossein A. Rahmani,Bin Wu,Jerome Ramos,Emine Yilmaz,Aldo Lipani*

Main category: cs.CL

TL;DR: PREF是一种个性化的、无参考的评估框架，可以联合测量通用输出质量和用户特定的对齐度，而无需黄金个性化参考。


<details>
  <summary>Details</summary>
Motivation: 大多数评估方法忽略了用户的个性。

Method: PREF包含三个步骤：（1）覆盖阶段，使用大型语言模型（LLM）生成全面的、特定于查询的指南，涵盖通用标准，如事实性、连贯性和完整性；（2）偏好阶段，使用目标用户的配置文件、声明或推断的偏好以及上下文，对这些因素进行重新排序和选择性增强，从而生成个性化的评估标准；（3）评分阶段，应用LLM判断器根据此标准对候选答案进行评分，确保基线充分性的同时捕获主观优先级。

Result: PREF在PrefEval基准测试上实现了更高的准确性、更好的校准以及与人类判断更紧密的对齐。

Conclusion: PREF实现了更高的准确性、更好的校准以及与人类判断更紧密的对齐，为个性化语言生成系统的更可靠评估和开发奠定了基础。

Abstract: Personalised text generation is essential for user-centric information
systems, yet most evaluation methods overlook the individuality of users. We
introduce \textbf{PREF}, a \textbf{P}ersonalised \textbf{R}eference-free
\textbf{E}valuation \textbf{F}ramework that jointly measures general output
quality and user-specific alignment without requiring gold personalised
references. PREF operates in a three-step pipeline: (1) a coverage stage uses a
large language model (LLM) to generate a comprehensive, query-specific
guideline covering universal criteria such as factuality, coherence, and
completeness; (2) a preference stage re-ranks and selectively augments these
factors using the target user's profile, stated or inferred preferences, and
context, producing a personalised evaluation rubric; and (3) a scoring stage
applies an LLM judge to rate candidate answers against this rubric, ensuring
baseline adequacy while capturing subjective priorities. This separation of
coverage from preference improves robustness, transparency, and reusability,
and allows smaller models to approximate the personalised quality of larger
ones. Experiments on the PrefEval benchmark, including implicit
preference-following tasks, show that PREF achieves higher accuracy, better
calibration, and closer alignment with human judgments than strong baselines.
By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the
groundwork for more reliable assessment and development of personalised
language generation systems.

</details>


### [33] [Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs](https://arxiv.org/abs/2508.10029)
*Wenpeng Xing,Mohan Li,Chunqiang Hu,Haitao XuNingyu Zhang,Bo Lin,Meng Han*

Main category: cs.CL

TL;DR: 本文提出了一种新的越狱攻击方法，该方法通过在有害和良性查询之间插入隐藏状态来引出禁止的响应，并提出了一种对抗性训练防御方法来减轻这种攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 在各种语言任务中表现出令人印象深刻的能力，但容易受到绕过其安全alignment的越狱攻击。

Method: 本文介绍了一种基于表示的攻击 Latent Fusion Jailbreak (LFJ)，它通过插值有害和良性查询对的隐藏状态来引出禁止的响应。

Result: 在 Vicuna 和 LLaMA-2 等模型上，在 AdvBench 和 MaliciousInstruct 等基准测试中进行的评估产生了 94.01% 的平均攻击成功率 (ASR)，优于现有方法。

Conclusion: 我们提出了一种对抗性训练防御，可以在不降低良性输入性能的情况下，将 ASR 降低 80% 以上。

Abstract: Large language models (LLMs) demonstrate impressive capabilities in various
language tasks but are susceptible to jailbreak attacks that circumvent their
safety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a
representation-based attack that interpolates hidden states from harmful and
benign query pairs to elicit prohibited responses. LFJ begins by selecting
query pairs with high thematic and syntactic similarity, then performs
gradient-guided interpolation at influential layers and tokens, followed by
optimization to balance attack success, output fluency, and computational
efficiency. Evaluations on models such as Vicuna and LLaMA-2 across benchmarks
like AdvBench and MaliciousInstruct yield an average attack success rate (ASR)
of 94.01%, outperforming existing methods. To mitigate LFJ, we propose an
adversarial training defense that fine-tunes models on interpolated examples,
reducing ASR by over 80% without degrading performance on benign inputs.
Ablation studies validate the importance of query pair selection, hidden state
interpolation components, and optimization strategies in LFJ's effectiveness.

</details>


### [34] [Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models](https://arxiv.org/abs/2508.10030)
*Saaduddin Mahmud,Mason Nakamura,Kyle H. Wray,Shlomo Zilberstein*

Main category: cs.CL

TL;DR: This paper introduces IAPO, a framework that jointly optimizes the prompt and inference scale, and develops PSST, a fixed-budget training algorithm for IAPO. The effectiveness of PSST is evaluated on six different tasks.


<details>
  <summary>Details</summary>
Motivation: Existing prompt optimization approaches are inference strategy agnostic, which is a significant methodological gap, as there is a strong interdependence between prompt optimization and inference strategies. User preferences regarding trade-offs among multiple objectives and inference budgets also influence the choice of prompt and inference configuration.

Method: The authors introduce a unified novel framework named IAPO (Inference-Aware Prompt Optimization) that jointly optimizes the prompt and inference scale, while being aware of the inference budget and different task objectives. They then develop a fixed-budget training algorithm for IAPO, which they call PSST (Prompt Scaling via Sequential Trimming), and analyze finite-budget guarantees on error probability.

Result: The paper reveals a strong interdependence between prompt optimization and inference strategies.

Conclusion: This paper evaluates the effectiveness of PSST on six different tasks, including multi-objective text generation and reasoning, and demonstrates the critical role of incorporating inference-awareness when aligning black-box LLMs through prompt optimization.

Abstract: Prompt optimization methods have demonstrated significant effectiveness in
aligning black-box large language models (LLMs). In parallel, inference scaling
strategies such as Best-of-N Sampling and Majority Voting have also proven to
enhance alignment and performance by trading off computation. However, existing
prompt optimization approaches are inference strategy agnostic; that is, they
optimize prompts without regard to the inference strategy employed during
deployment. This constitutes a significant methodological gap, as our empirical
and theoretical analysis reveals a strong interdependence between these two
paradigms. Moreover, we find that user preferences regarding trade-offs among
multiple objectives and inference budgets substantially influence the choice of
prompt and inference configuration. To address this gap, we introduce a unified
novel framework named IAPO (Inference-Aware Prompt Optimization) that jointly
optimizes the prompt and inference scale, while being aware of the inference
budget and different task objectives. We then develop a fixed-budget training
algorithm for IAPO, which we call PSST (Prompt Scaling via Sequential
Trimming), and analyze finite-budget guarantees on error probability. Finally,
we evaluate the effectiveness of PSST on six different tasks, including
multi-objective text generation and reasoning, and demonstrate the critical
role of incorporating inference-awareness when aligning black-box LLMs through
prompt optimization.

</details>


### [35] [The Cost of Thinking: Increased Jailbreak Risk in Large Language Models](https://arxiv.org/abs/2508.10032)
*Fan Yang*

Main category: cs.CL

TL;DR: 具有思维模式的LLM更容易受到Jailbreak攻击。提出了一种安全思维干预方法，以降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 发现具有思维模式的LLM更容易受到Jailbreak攻击。在AdvBench和HarmBench上评估了9个LLM，发现攻击LLM中思维模式的成功率几乎高于非思维模式。通过大量样本研究发现，出于教育目的和过长的思考长度是成功攻击数据的特征，并且LLM在大多知道问题有害时也会给出有害答案。

Method: 提出了一种针对LLM的安全思维干预方法，通过在prompt中添加“特定思维令牌”来显式地引导LLM的内部思维过程。

Result: 安全思维干预可以显著降低具有思维模式的LLM的攻击成功率。

Conclusion: 提出了安全思维干预方法，通过在提示中添加“特定思维令牌”来显式引导LLM的内部思维过程，从而显著降低了具有思维模式的LLM的攻击成功率。

Abstract: Thinking mode has always been regarded as one of the most valuable modes in
LLMs. However, we uncover a surprising and previously overlooked phenomenon:
LLMs with thinking mode are more easily broken by Jailbreak attack. We evaluate
9 LLMs on AdvBench and HarmBench and find that the success rate of attacking
thinking mode in LLMs is almost higher than that of non-thinking mode. Through
large numbers of sample studies, it is found that for educational purposes and
excessively long thinking lengths are the characteristics of successfully
attacked data, and LLMs also give harmful answers when they mostly know that
the questions are harmful. In order to alleviate the above problems, this paper
proposes a method of safe thinking intervention for LLMs, which explicitly
guides the internal thinking processes of LLMs by adding "specific thinking
tokens" of LLMs to the prompt. The results demonstrate that the safe thinking
intervention can significantly reduce the attack success rate of LLMs with
thinking mode.

</details>


### [36] [mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning](https://arxiv.org/abs/2508.10137)
*Nghia Trung Ngo,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: Introduces mSCoRe, a multilingual benchmark for evaluating commonsense reasoning in LLMs. Current models struggle with it, especially at higher complexity levels.


<details>
  <summary>Details</summary>
Motivation: The mechanism underlying LLMs' utilization of different human reasoning skills remains poorly investigated, especially for multilingual commonsense reasoning.

Method: A Multilingual and Scalable Benchmark for Skill-based Commonsense Reasoning (mSCoRe). Incorporates: (1) a novel taxonomy of reasoning skills, (2) a robust data synthesis pipeline, and (3) a complexity scaling framework.

Result: Experiments on eight state-of-the-art LLMs demonstrate mSCoRe's challenge, particularly at higher complexity levels.

Conclusion: mSCoRe benchmark remains significantly challenging for current models, particularly at higher complexity levels. Limitations exist in reasoning-reinforced models when confronted with nuanced multilingual general and cultural commonsense. Detailed analysis on the models' reasoning processes suggests future directions for improving multilingual commonsense reasoning capabilities.

Abstract: Recent advancements in reasoning-reinforced Large Language Models (LLMs) have
shown remarkable capabilities in complex reasoning tasks. However, the
mechanism underlying their utilization of different human reasoning skills
remains poorly investigated, especially for multilingual commonsense reasoning
that involves everyday knowledge across different languages and cultures. To
address this gap, we propose a \textbf{M}ultilingual and Scalable Benchmark for
\textbf{S}kill-based \textbf{Co}mmonsense \textbf{Re}asoning (\textbf{mSCoRe}).
Our benchmark incorporates three key components that are designed to
systematically evaluate LLM's reasoning capabilities, including: (1) a novel
taxonomy of reasoning skills that enables fine-grained analysis of models'
reasoning processes, (2) a robust data synthesis pipeline tailored specifically
for commonsense reasoning evaluation, and (3) a complexity scaling framework
allowing task difficulty to scale dynamically alongside future improvements in
LLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying
sizes and training approaches demonstrate that \textbf{mSCoRe} remains
significantly challenging for current models, particularly at higher complexity
levels. Our results reveal the limitations of such reasoning-reinforced models
when confronted with nuanced multilingual general and cultural commonsense. We
further provide detailed analysis on the models' reasoning processes,
suggesting future directions for improving multilingual commonsense reasoning
capabilities.

</details>


### [37] [Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs](https://arxiv.org/abs/2508.10142)
*Kartikeya Badola,Jonathan Simon,Arian Hosseini,Sara Marie Mc Carthy,Tsendsuren Munkhdalai,Abhimanyu Goyal,Tomáš Kočiský,Shyam Upadhyay,Bahare Fatemi,Mehran Kazemi*

Main category: cs.CL

TL;DR: 大型语言模型在复杂交互场景中表现不佳。引入了一个新的基准来测试推理、对话和信息寻求能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型擅长解决具有清晰完整陈述的问题，但通常难以应对细微的环境或交互式任务，而这在大多数现实场景中很常见。这突显了开发能够有效参与逻辑一致的多轮对话、寻求信息并使用不完整数据进行推理的大型语言模型的关键需求。

Method: 引入了一个新颖的基准，包含一套多轮任务，每个任务都旨在测试特定的推理、交互对话和信息寻求能力。这些任务具有确定性的评分机制，从而消除了人工干预的需要。

Result: 在我们的基准测试中评估前沿模型显示出显著的提升空间。我们的分析表明，大多数错误源于较差的指令遵循、推理失败和较差的计划能力。

Conclusion: 该基准测试揭示了当前大型语言模型在处理复杂交互场景中的优势和劣势，并为未来旨在提高这些关键能力的研究提供了一个强大的平台。

Abstract: Large language models (LLMs) excel at solving problems with clear and
complete statements, but often struggle with nuanced environments or
interactive tasks which are common in most real-world scenarios. This
highlights the critical need for developing LLMs that can effectively engage in
logically consistent multi-turn dialogue, seek information and reason with
incomplete data. To this end, we introduce a novel benchmark comprising a suite
of multi-turn tasks each designed to test specific reasoning, interactive
dialogue, and information-seeking abilities. These tasks have deterministic
scoring mechanisms, thus eliminating the need for human intervention.
Evaluating frontier models on our benchmark reveals significant headroom. Our
analysis shows that most errors emerge from poor instruction following,
reasoning failures, and poor planning. This benchmark provides valuable
insights into the strengths and weaknesses of current LLMs in handling complex,
interactive scenarios and offers a robust platform for future research aimed at
improving these critical capabilities.

</details>


### [38] [LaajMeter: A Framework for LaaJ Evaluation](https://arxiv.org/abs/2508.10161)
*Gal Amram,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Avi Ziv*

Main category: cs.CL

TL;DR: This paper introduces LaaJMeter, a simulation-based framework for controlled meta-evaluation of LLMs as Judges (LaaJs), and demonstrates its utility in a code translation task, highlighting the limitations of common metrics and the importance of principled metric selection.


<details>
  <summary>Details</summary>
Motivation: LLMs pose significant challenges in domain-specific contexts, where annotated data is scarce and expert evaluation is costly. In such cases, meta-evaluation is often performed using metrics that have not been validated for the specific domain in which they are applied. As a result, it becomes difficult to determine which metrics effectively identify LaaJ quality, and further, what threshold indicates sufficient evaluator performance.

Method: introduction of LaaJMeter, a simulation-based framework for controlled meta-evaluation of LaaJs

Result: demonstrate the utility of LaaJMeter in a code translation task involving a legacy programming language, showing how different metrics vary in sensitivity to evaluator quality. Our results highlight the limitations of common metrics and the importance of principled metric selection.

Conclusion: LaaJMeter provides a scalable and extensible solution for assessing LaaJs in low-resource settings, contributing to the broader effort to ensure trustworthy and reproducible evaluation in NLP.

Abstract: Large Language Models (LLMs) are increasingly used as evaluators in natural
language processing tasks, a paradigm known as LLM-as-a-Judge (LaaJ). While
effective in general domains, LaaJs pose significant challenges in
domain-specific contexts, where annotated data is scarce and expert evaluation
is costly. In such cases, meta-evaluation is often performed using metrics that
have not been validated for the specific domain in which they are applied. As a
result, it becomes difficult to determine which metrics effectively identify
LaaJ quality, and further, what threshold indicates sufficient evaluator
performance. In this work, we introduce LaaJMeter, a simulation-based framework
for controlled meta-evaluation of LaaJs. LaaJMeter enables engineers to
generate synthetic data representing virtual models and judges, allowing
systematic analysis of evaluation metrics under realistic conditions. This
helps practitioners validate and refine LaaJs for specific evaluation tasks:
they can test whether their metrics correctly distinguish between better and
worse (virtual) LaaJs, and estimate appropriate thresholds for evaluator
adequacy.
  We demonstrate the utility of LaaJMeter in a code translation task involving
a legacy programming language, showing how different metrics vary in
sensitivity to evaluator quality. Our results highlight the limitations of
common metrics and the importance of principled metric selection. LaaJMeter
provides a scalable and extensible solution for assessing LaaJs in low-resource
settings, contributing to the broader effort to ensure trustworthy and
reproducible evaluation in NLP.

</details>


### [39] [Estimating Machine Translation Difficulty](https://arxiv.org/abs/2508.10175)
*Lorenzo Proietti,Stefano Perrella,Vilém Zouhar,Roberto Navigli,Tom Kocmi*

Main category: cs.CL

TL;DR: This paper formalizes translation difficulty estimation, introduces a new metric to evaluate difficulty estimators, and demonstrates their utility in constructing more challenging machine translation benchmarks. The authors release two improved models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25.


<details>
  <summary>Details</summary>
Motivation: Automatically identifying texts where machine translation systems struggle holds promise for developing more discriminative evaluations and guiding future research.

Method: We introduce a new metric to evaluate difficulty estimators and use it to assess both baselines and novel approaches.

Result: Our results show that dedicated models (dubbed Sentinel-src) outperform both heuristic-based methods (e.g. word rarity or syntactic complexity) and LLM-as-a-judge approaches.

Conclusion: dedicated models (dubbed Sentinel-src) outperform both heuristic-based methods (e.g. word rarity or syntactic complexity) and LLM-as-a-judge approaches. We release two improved models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which can be used to scan large collections of texts and select those most likely to challenge contemporary machine translation systems.

Abstract: Machine translation quality has began achieving near-perfect translations in
some setups. These high-quality outputs make it difficult to distinguish
between state-of-the-art models and to identify areas for future improvement.
Automatically identifying texts where machine translation systems struggle
holds promise for developing more discriminative evaluations and guiding future
research.
  We formalize the task of translation difficulty estimation, defining a text's
difficulty based on the expected quality of its translations. We introduce a
new metric to evaluate difficulty estimators and use it to assess both
baselines and novel approaches. Finally, we demonstrate the practical utility
of difficulty estimators by using them to construct more challenging machine
translation benchmarks. Our results show that dedicated models (dubbed
Sentinel-src) outperform both heuristic-based methods (e.g. word rarity or
syntactic complexity) and LLM-as-a-judge approaches. We release two improved
models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which
can be used to scan large collections of texts and select those most likely to
challenge contemporary machine translation systems.

</details>


### [40] [Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs](https://arxiv.org/abs/2508.10180)
*Wenlong Deng,Jiaming Zhang,Qi Zeng,Christos Thrampoulidis,Boying Gong,Xiaoxiao Li*

Main category: cs.CL

TL;DR: For-Value: A forward-only data valuation framework for scalable and efficient influence estimation for LLMs and VLMs, eliminating costly gradient computations.


<details>
  <summary>Details</summary>
Motivation: Quantifying the influence of individual training samples is essential for enhancing the transparency and accountability of large language models (LLMs) and vision-language models (VLMs). However, existing data valuation methods often rely on Hessian information or model retraining, making them computationally prohibitive for billion-parameter models.

Method: a forward-only data valuation framework that enables scalable and efficient influence estimation for both LLMs and VLMs. computes influence scores using a simple closed-form expression based solely on a single forward pass

Result: For-Value accurately estimates per-sample influence by capturing alignment in hidden representations and prediction errors between training and validation samples.

Conclusion: For-Value matches or outperforms gradient-based baselines in identifying impactful fine-tuning examples and effectively detecting mislabeled data.

Abstract: Quantifying the influence of individual training samples is essential for
enhancing the transparency and accountability of large language models (LLMs)
and vision-language models (VLMs). However, existing data valuation methods
often rely on Hessian information or model retraining, making them
computationally prohibitive for billion-parameter models. In this work, we
introduce For-Value, a forward-only data valuation framework that enables
scalable and efficient influence estimation for both LLMs and VLMs. By
leveraging the rich representations of modern foundation models, For-Value
computes influence scores using a simple closed-form expression based solely on
a single forward pass, thereby eliminating the need for costly gradient
computations. Our theoretical analysis demonstrates that For-Value accurately
estimates per-sample influence by capturing alignment in hidden representations
and prediction errors between training and validation samples. Extensive
experiments show that For-Value matches or outperforms gradient-based baselines
in identifying impactful fine-tuning examples and effectively detecting
mislabeled data.

</details>


### [41] [PakBBQ: A Culturally Adapted Bias Benchmark for QA](https://arxiv.org/abs/2508.10186)
*Abdullah Hashmat,Muhammad Arham Mirza,Agha Ali Raza*

Main category: cs.CL

TL;DR: PakBBQ: A culturally adapted BBQ dataset for Urdu and English, reveals biases in LLMs and highlights the importance of contextualized benchmarks and prompt engineering for mitigation in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: Most LLMs are trained and evaluated on Western centric data, with little attention paid to low-resource languages and regional contexts. To address this gap, we introduce PakBBQ, a culturally and regionally adapted extension of the original Bias Benchmark for Question Answering (BBQ) dataset.

Method: We introduce PakBBQ, a culturally and regionally adapted extension of the original Bias Benchmark for Question Answering (BBQ) dataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8 categories in both English and Urdu, covering eight bias dimensions including age, disability, appearance, gender, socio-economic status, religious, regional affiliation, and language formality that are relevant in Pakistan. We evaluate multiple multilingual LLMs under both ambiguous and explicitly disambiguated contexts, as well as negative versus non negative question framings.

Result: Our experiments reveal (i) an average accuracy gain of 12% with disambiguation, (ii) consistently stronger counter bias behaviors in Urdu than in English, and (iii) marked framing effects that reduce stereotypical responses when questions are posed negatively.

Conclusion: The findings highlight the importance of contextualized benchmarks and simple prompt engineering strategies for bias mitigation in low resource settings.

Abstract: With the widespread adoption of Large Language Models (LLMs) across various
applications, it is empirical to ensure their fairness across all user
communities. However, most LLMs are trained and evaluated on Western centric
data, with little attention paid to low-resource languages and regional
contexts. To address this gap, we introduce PakBBQ, a culturally and regionally
adapted extension of the original Bias Benchmark for Question Answering (BBQ)
dataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8
categories in both English and Urdu, covering eight bias dimensions including
age, disability, appearance, gender, socio-economic status, religious, regional
affiliation, and language formality that are relevant in Pakistan. We evaluate
multiple multilingual LLMs under both ambiguous and explicitly disambiguated
contexts, as well as negative versus non negative question framings. Our
experiments reveal (i) an average accuracy gain of 12\% with disambiguation,
(ii) consistently stronger counter bias behaviors in Urdu than in English, and
(iii) marked framing effects that reduce stereotypical responses when questions
are posed negatively. These findings highlight the importance of contextualized
benchmarks and simple prompt engineering strategies for bias mitigation in low
resource settings.

</details>


### [42] [Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models](https://arxiv.org/abs/2508.10192)
*Igor Halperin*

Main category: cs.CL

TL;DR: This paper introduces a new framework called Semantic Divergence Metrics (SDM) to detect when large language models generate nonsensical or unfaithful text. SDM uses semantic analysis and information theory to measure the divergence between prompts and responses, and can classify different types of LLM responses.


<details>
  <summary>Details</summary>
Motivation: The proliferation of Large Language Models (LLMs) is challenged by hallucinations, critical failure modes where models generate non-factual, nonsensical or unfaithful text.

Method: The paper introduces Semantic Divergence Metrics (SDM), a novel lightweight framework for detecting Faithfulness Hallucinations. It uses joint clustering on sentence embeddings to create a shared topic space for prompts and answers, and computes information-theoretic metrics to measure the semantic divergence between prompts and responses.

Result: The paper identifies the KL divergence KL(Answer || Prompt) as a powerful indicator of Semantic Exploration. A high $\mathcal{S}_H$ score, combining the Jensen-Shannon divergence and Wasserstein distance, indicates a Faithfulness hallucination.

Conclusion: The paper introduces Semantic Box, a diagnostic framework for classifying LLM response types, including confident confabulation, based on the proposed Semantic Divergence Metrics (SDM).

Abstract: The proliferation of Large Language Models (LLMs) is challenged by
hallucinations, critical failure modes where models generate non-factual,
nonsensical or unfaithful text. This paper introduces Semantic Divergence
Metrics (SDM), a novel lightweight framework for detecting Faithfulness
Hallucinations -- events of severe deviations of LLMs responses from input
contexts. We focus on a specific implementation of these LLM errors,
{confabulations, defined as responses that are arbitrary and semantically
misaligned with the user's query. Existing methods like Semantic Entropy test
for arbitrariness by measuring the diversity of answers to a single, fixed
prompt. Our SDM framework improves upon this by being more prompt-aware: we
test for a deeper form of arbitrariness by measuring response consistency not
only across multiple answers but also across multiple, semantically-equivalent
paraphrases of the original prompt. Methodologically, our approach uses joint
clustering on sentence embeddings to create a shared topic space for prompts
and answers. A heatmap of topic co-occurances between prompts and responses can
be viewed as a quantified two-dimensional visualization of the user-machine
dialogue. We then compute a suite of information-theoretic metrics to measure
the semantic divergence between prompts and responses. Our practical score,
$\mathcal{S}_H$, combines the Jensen-Shannon divergence and Wasserstein
distance to quantify this divergence, with a high score indicating a
Faithfulness hallucination. Furthermore, we identify the KL divergence
KL(Answer $||$ Prompt) as a powerful indicator of \textbf{Semantic
Exploration}, a key signal for distinguishing different generative behaviors.
These metrics are further combined into the Semantic Box, a diagnostic
framework for classifying LLM response types, including the dangerous,
confident confabulation.

</details>


### [43] [Understanding Textual Emotion Through Emoji Prediction](https://arxiv.org/abs/2508.10222)
*Ethan Gordon,Nishank Kuppa,Rigved Tummala,Sriram Anasuri*

Main category: cs.CL

TL;DR: emoji prediction from short text sequences using four deep learning architectures: a feed-forward network, CNN, transformer, and BERT


<details>
  <summary>Details</summary>
Motivation: explores emoji prediction from short text sequences

Method: deep learning architectures: a feed-forward network, CNN, transformer, and BERT

Result: BERT achieves the highest overall performance, while CNN demonstrates superior efficacy on rare emoji classes.

Conclusion: BERT achieves the highest overall performance, while CNN demonstrates superior efficacy on rare emoji classes. This research shows the importance of architecture selection and hyperparameter tuning for sentiment-aware emoji prediction, contributing to improved human-computer interaction.

Abstract: This project explores emoji prediction from short text sequences using four
deep learning architectures: a feed-forward network, CNN, transformer, and
BERT. Using the TweetEval dataset, we address class imbalance through focal
loss and regularization techniques. Results show BERT achieves the highest
overall performance due to its pre-training advantage, while CNN demonstrates
superior efficacy on rare emoji classes. This research shows the importance of
architecture selection and hyperparameter tuning for sentiment-aware emoji
prediction, contributing to improved human-computer interaction.

</details>


### [44] [Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia](https://arxiv.org/abs/2508.10226)
*Andrew X. Chen,Guillermo Horga,Sean Escola*

Main category: cs.CL

TL;DR: 大型语言模型可以预测精神分裂症高危患者的BPRS评分，准确率接近人类评估者。


<details>
  <summary>Details</summary>
Motivation: 临床精神分裂症高危（CHR）患者需要密切监测其症状，以提供适当的治疗。简明精神病评定量表（BPRS）是一种经过验证的常用研究工具，用于测量精神分裂症和其他精神疾病患者的症状；但是，由于它需要冗长的结构化访谈，因此在临床实践中并不常用。

Method: 利用大型语言模型（LLM）从来自加速药物合作精神分裂症（AMP-SCZ）队列的409名CHR患者的临床访谈记录中预测BPRS评分。

Result: 尽管访谈并非专门用于衡量BPRS，但LLM预测的零样本性能与真实评估相比（中位数一致性：0.84，ICC：0.73）接近人类间和人类内评估者信度。

Conclusion: 大型语言模型(LLM)有潜力通过其在评估外语BPRS中的准确性以及在单次或几次学习方法中整合纵向信息来改进和标准化CHR患者的评估。

Abstract: Patients who are at clinical high risk (CHR) for schizophrenia need close
monitoring of their symptoms to inform appropriate treatments. The Brief
Psychiatric Rating Scale (BPRS) is a validated, commonly used research tool for
measuring symptoms in patients with schizophrenia and other psychotic
disorders; however, it is not commonly used in clinical practice as it requires
a lengthy structured interview. Here, we utilize large language models (LLMs)
to predict BPRS scores from clinical interview transcripts in 409 CHR patients
from the Accelerating Medicines Partnership Schizophrenia (AMP-SCZ) cohort.
Despite the interviews not being specifically structured to measure the BPRS,
the zero-shot performance of the LLM predictions compared to the true
assessment (median concordance: 0.84, ICC: 0.73) approaches human inter- and
intra-rater reliability. We further demonstrate that LLMs have substantial
potential to improve and standardize the assessment of CHR patients via their
accuracy in assessing the BPRS in foreign languages (median concordance: 0.88,
ICC: 0.70), and integrating longitudinal information in a one-shot or few-shot
learning approach.

</details>


### [45] [A Computational Approach to Analyzing Language Change and Variation in the Constructed Language Toki Pona](https://arxiv.org/abs/2508.10246)
*Daniel Huang,Hyoun-A Joo*

Main category: cs.CL

TL;DR: This study examines language change in Toki Pona, suggesting constructed languages evolve like natural ones.


<details>
  <summary>Details</summary>
Motivation: explores language change and variation in Toki Pona

Method: computational and corpus-based approach

Result: changes in preferences of content words for different syntactic positions over time and variation in usage across different corpora

Conclusion: sociolinguistic factors influence Toki Pona in the same way as natural languages, and that even constructed linguistic systems naturally evolve as communities use them.

Abstract: This study explores language change and variation in Toki Pona, a constructed
language with approximately 120 core words. Taking a computational and
corpus-based approach, the study examines features including fluid word classes
and transitivity in order to examine (1) changes in preferences of content
words for different syntactic positions over time and (2) variation in usage
across different corpora. The results suggest that sociolinguistic factors
influence Toki Pona in the same way as natural languages, and that even
constructed linguistic systems naturally evolve as communities use them.

</details>


### [46] [Inductive Bias Extraction and Matching for LLM Prompts](https://arxiv.org/abs/2508.10295)
*Christian M. Angel,Francis Ferraro*

Main category: cs.CL

TL;DR: Using an LLM's output as part of its prompt improves performance by matching the model's inductive bias.


<details>
  <summary>Details</summary>
Motivation: LLMs are sensitive to small changes in prompt wording, which can be ascribed to the inductive bias that is present in the LLM.

Method: Using an LLM's output as a portion of its prompt to create satisfactory wording for prompts.

Result: Using Inductive Bias Extraction and Matching strategy improves LLM Likert ratings used for classification by up to 19% and LLM Likert ratings used for ranking by up to 27%.

Conclusion: Inductive Bias Extraction and Matching strategy improves LLM Likert ratings used for classification and ranking.

Abstract: The active research topic of prompt engineering makes it evident that LLMs
are sensitive to small changes in prompt wording. A portion of this can be
ascribed to the inductive bias that is present in the LLM. By using an LLM's
output as a portion of its prompt, we can more easily create satisfactory
wording for prompts. This has the effect of creating a prompt that matches the
inductive bias in model. Empirically, we show that using this Inductive Bias
Extraction and Matching strategy improves LLM Likert ratings used for
classification by up to 19% and LLM Likert ratings used for ranking by up to
27%.

</details>


### [47] [Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race](https://arxiv.org/abs/2508.10304)
*Gustavo Bonil,Simone Hashiguti,Jhessica Silva,João Gondim,Helena Maia,Nádia Silva,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: This study uses qualitative methods to show that LLMs replicate gender and racial biases, portraying Black women as tied to ancestry and white women in self-discovery, reinforcing inequalities and highlighting the need for critical approaches to AI design.


<details>
  <summary>Details</summary>
Motivation: Assess whether LLMs reproduce biases, such as discrimination and racialization, while maintaining hegemonic discourses. Current bias detection approaches often overlook the nuanced ways in which biases emerge in natural language.

Method: Qualitative, discursive framework through manual analysis of LLM-generated short stories featuring Black and white women.

Result: Black women are portrayed as tied to ancestry and resistance, while white women appear in self-discovery processes. Models offered superficial revisions that maintained problematic meanings.

Conclusion: LLMs reproduce biases, reinforcing essentialization and a sense of social immobility. Superficial revisions reveal limitations in fostering inclusive narratives.

Abstract: With the advance of Artificial Intelligence (AI), Large Language Models
(LLMs) have gained prominence and been applied in diverse contexts. As they
evolve into more sophisticated versions, it is essential to assess whether they
reproduce biases, such as discrimination and racialization, while maintaining
hegemonic discourses. Current bias detection approaches rely mostly on
quantitative, automated methods, which often overlook the nuanced ways in which
biases emerge in natural language. This study proposes a qualitative,
discursive framework to complement such methods. Through manual analysis of
LLM-generated short stories featuring Black and white women, we investigate
gender and racial biases. We contend that qualitative methods such as the one
proposed here are fundamental to help both developers and users identify the
precise ways in which biases manifest in LLM outputs, thus enabling better
conditions to mitigate them. Results show that Black women are portrayed as
tied to ancestry and resistance, while white women appear in self-discovery
processes. These patterns reflect how language models replicate crystalized
discursive representations, reinforcing essentialization and a sense of social
immobility. When prompted to correct biases, models offered superficial
revisions that maintained problematic meanings, revealing limitations in
fostering inclusive narratives. Our results demonstrate the ideological
functioning of algorithms and have significant implications for the ethical use
and development of AI. The study reinforces the need for critical,
interdisciplinary approaches to AI design and deployment, addressing how
LLM-generated discourses reflect and perpetuate inequalities.

</details>


### [48] [ReviewRL: Towards Automated Scientific Review with RL](https://arxiv.org/abs/2508.10308)
*Sihang Zeng,Kai Tian,Kaiyan Zhang,Yuru wang,Junqi Gao,Runze Liu,Sa Yang,Jingxuan Li,Xinwei Long,Jiaheng Ma,Biqing Qi,Bowen Zhou*

Main category: cs.CL

TL;DR: ReviewRL is a reinforcement learning framework that generates comprehensive and factually grounded scientific paper reviews, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Peer review is essential for scientific progress but faces growing challenges due to increasing submission volumes and reviewer fatigue. Existing automated review approaches struggle with factual accuracy, rating consistency, and analytical depth, often generating superficial or generic feedback lacking the insights characteristic of high-quality human reviews.

Method: a reinforcement learning framework for generating comprehensive and factually grounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP retrieval-augmented context generation pipeline that incorporates relevant scientific literature, (2) supervised fine-tuning that establishes foundational reviewing capabilities, and (3) a reinforcement learning procedure with a composite reward function that jointly enhances review quality and rating accuracy.

Result: Experiments on ICLR 2025 papers demonstrate that ReviewRL significantly outperforms existing methods across both rule-based metrics and model-based quality assessments.

Conclusion: ReviewRL establishes a foundational framework for RL-driven automatic critique generation in scientific discovery, demonstrating promising potential for future development in this domain.

Abstract: Peer review is essential for scientific progress but faces growing challenges
due to increasing submission volumes and reviewer fatigue. Existing automated
review approaches struggle with factual accuracy, rating consistency, and
analytical depth, often generating superficial or generic feedback lacking the
insights characteristic of high-quality human reviews. We introduce ReviewRL, a
reinforcement learning framework for generating comprehensive and factually
grounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP
retrieval-augmented context generation pipeline that incorporates relevant
scientific literature, (2) supervised fine-tuning that establishes foundational
reviewing capabilities, and (3) a reinforcement learning procedure with a
composite reward function that jointly enhances review quality and rating
accuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL
significantly outperforms existing methods across both rule-based metrics and
model-based quality assessments. ReviewRL establishes a foundational framework
for RL-driven automatic critique generation in scientific discovery,
demonstrating promising potential for future development in this domain. The
implementation of ReviewRL will be released at GitHub.

</details>


### [49] [From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document Analysis](https://arxiv.org/abs/2508.10311)
*Xuan Li,Jialiang Dong,Raymond Wong*

Main category: cs.CL

TL;DR: DOTABLER is a table-centric semantic document parsing framework that uncovers deep semantic links between tables and their context.


<details>
  <summary>Details</summary>
Motivation: Existing studies lack deep semantic parsing of tables and their contextual associations, limiting advanced tasks. To address this, we propose DOTABLER.

Method: DOTABLER leverages a custom dataset and domain-specific fine-tuning of pre-trained models, integrating a complete parsing pipeline to identify context segments semantically tied to tables.

Result: DOTABLER implements two core functionalities: table-centric document structure parsing and domain-specific table retrieval, delivering comprehensive table-anchored semantic analysis and precise extraction of semantically relevant tables.

Conclusion: DOTABLER achieves over 90% Precision and F1 scores, demonstrating superior performance in table-context semantic analysis and deep document parsing compared to advanced models such as GPT-4o.

Abstract: Documents are core carriers of information and knowl-edge, with broad
applications in finance, healthcare, and scientific research. Tables, as the
main medium for structured data, encapsulate key information and are among the
most critical document components. Existing studies largely focus on
surface-level tasks such as layout analysis, table detection, and data
extraction, lacking deep semantic parsing of tables and their contextual
associations. This limits advanced tasks like cross-paragraph data
interpretation and context-consistent analysis. To address this, we propose
DOTABLER, a table-centric semantic document parsing framework designed to
uncover deep semantic links between tables and their context. DOTABLER
leverages a custom dataset and domain-specific fine-tuning of pre-trained
models, integrating a complete parsing pipeline to identify context segments
semantically tied to tables. Built on this semantic understanding, DOTABLER
implements two core functionalities: table-centric document structure parsing
and domain-specific table retrieval, delivering comprehensive table-anchored
semantic analysis and precise extraction of semantically relevant tables.
Evaluated on nearly 4,000 pages with over 1,000 tables from real-world PDFs,
DOTABLER achieves over 90% Precision and F1 scores, demonstrating superior
performance in table-context semantic analysis and deep document parsing
compared to advanced models such as GPT-4o.

</details>


### [50] [Beyond Semantic Understanding: Preserving Collaborative Frequency Components in LLM-based Recommendation](https://arxiv.org/abs/2508.10312)
*Minhao Wang,Yunhang He,Cong Xu,Zhangchi Zhu,Wei Zhang*

Main category: cs.CL

TL;DR: FreLLM4Rec 通过在频谱上平衡语义和协作信息来缓解协同信号衰减，从而提高基于 LLM 的推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 基于 LLM 的推荐系统在用户的交互历史中过度强调语义相关性。当以预训练的协同 ID 嵌入作为输入时，基于 LLM 的推荐系统会逐渐削弱固有的协同信号，而不是传统的基于 Transformer 的序列模型，在后者中，协同信号通常会被保留甚至增强以实现最先进的性能。

Method: 我们引入了 FreLLM4Rec，这是一种从频谱角度平衡语义和协同信息的方法。首先使用全局图低通滤波器 (G-LPF) 对包含语义和协同信息的项目嵌入进行提纯，以初步消除不相关的高频噪声。然后，时间频率调制 (TFM) 逐层主动保留协同信号。

Result: FreLLM4Rec 成功缓解了协同信号衰减，并在四个基准数据集上取得了有竞争力的性能，与最佳基线相比，NDCG@10 指标提高了 8.00%。

Conclusion: FreLLM4Rec成功缓解了协同信号衰减，并取得了有竞争力的性能，与最佳基线相比，NDCG@10 指标提高了 8.00%。

Abstract: Recommender systems in concert with Large Language Models (LLMs) present
promising avenues for generating semantically-informed recommendations.
However, LLM-based recommenders exhibit a tendency to overemphasize semantic
correlations within users' interaction history. When taking pretrained
collaborative ID embeddings as input, LLM-based recommenders progressively
weaken the inherent collaborative signals as the embeddings propagate through
LLM backbones layer by layer, as opposed to traditional Transformer-based
sequential models in which collaborative signals are typically preserved or
even enhanced for state-of-the-art performance. To address this limitation, we
introduce FreLLM4Rec, an approach designed to balance semantic and
collaborative information from a spectral perspective. Item embeddings that
incorporate both semantic and collaborative information are first purified
using a Global Graph Low-Pass Filter (G-LPF) to preliminarily remove irrelevant
high-frequency noise. Temporal Frequency Modulation (TFM) then actively
preserves collaborative signal layer by layer. Note that the collaborative
preservation capability of TFM is theoretically guaranteed by establishing a
connection between the optimal but hard-to-implement local graph fourier
filters and the suboptimal yet computationally efficient frequency-domain
filters. Extensive experiments on four benchmark datasets demonstrate that
FreLLM4Rec successfully mitigates collaborative signal attenuation and achieves
competitive performance, with improvements of up to 8.00\% in NDCG@10 over the
best baseline. Our findings provide insights into how LLMs process
collaborative information and offer a principled approach for improving
LLM-based recommendation systems.

</details>


### [51] [Cross-Prompt Encoder for Low-Performing Languages](https://arxiv.org/abs/2508.10352)
*Beso Mikaberidze,Teimuraz Saghinadze,Simon Ostermann,Philipp Muller*

Main category: cs.CL

TL;DR: 论文提出了一种新的跨语言提示编码器（XPE）和双重软提示机制，以提高低资源语言的性能。


<details>
  <summary>Details</summary>
Motivation: 探索提示编码器在跨语言迁移中的潜力，特别是在提高低性能语言性能方面的潜力。

Method: 结合轻量级编码架构与多种类型学不同语言的训练，并提出双重软提示机制，该机制结合了基于编码器的提示和直接训练的标准软提示。

Result: 在SIB-200基准测试中，XPE在低性能语言上最有效，而混合变体提供更广泛的适应性。

Conclusion: XPE在低性能语言上最有效，而混合变体在多语言环境中提供更广泛的适应性。

Abstract: Soft prompts have emerged as a powerful alternative to adapters in
parameter-efficient fine-tuning (PEFT), enabling large language models (LLMs)
to adapt to downstream tasks without architectural changes or parameter
updates. While prior work has focused on stabilizing training via parameter
interaction in small neural prompt encoders, their broader potential for
transfer across languages remains unexplored. In this paper, we demonstrate
that a prompt encoder can play a central role in improving performance on
low-performing languages-those that achieve poor accuracy even under full-model
fine-tuning. We introduce the Cross-Prompt Encoder (XPE), which combines a
lightweight encoding architecture with multi-source training on typologically
diverse languages - a design that enables the model to capture abstract and
transferable patterns across languages. To complement XPE, we propose a Dual
Soft Prompt mechanism that combines an encoder-based prompt with a directly
trained standard soft prompt. This hybrid design proves especially effective
for target languages that benefit from both broadly shared structure and
language-specific alignment. Experiments on the SIB-200 benchmark reveal a
consistent trade-off: XPE is most effective for low-performing languages, while
hybrid variants offer broader adaptability across multilingual settings.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [52] [Stochastic-based Patch Filtering for Few-Shot Learning](https://arxiv.org/abs/2508.10066)
*Javier Rodenas,Eduardo Aguilar,Petia Radeva*

Main category: cs.CV

TL;DR: SPFF通过随机过滤patch嵌入来关注与类表示更相关的patch嵌入，从而提高少样本食物图像分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 食物图像由于其视觉复杂性和可变性，对少样本学习模型提出了独特的挑战。当比较查询图像和支持图像时，这个问题导致对最重要的元素失去关注，从而导致错误分类。

Method: 基于随机的patch过滤少样本学习(SPFF)

Result: SPFF有效地关注了类特定食物特征最突出的patch，同时成功地过滤掉了不相关的patch。

Conclusion: SPFF在少样本分类基准测试Food-101、VireoFood-172和UECFood-256上优于现有SoA方法。

Abstract: Food images present unique challenges for few-shot learning models due to
their visual complexity and variability. For instance, a pasta dish might
appear with various garnishes on different plates and in diverse lighting
conditions and camera perspectives. This problem leads to losing focus on the
most important elements when comparing the query with support images, resulting
in misclassification. To address this issue, we propose Stochastic-based Patch
Filtering for Few-Shot Learning (SPFF) to attend to the patch embeddings that
show greater correlation with the class representation. The key concept of SPFF
involves the stochastic filtering of patch embeddings, where patches less
similar to the class-aware embedding are more likely to be discarded. With
patch embedding filtered according to the probability of appearance, we use a
similarity matrix that quantifies the relationship between the query image and
its respective support images. Through a qualitative analysis, we demonstrate
that SPFF effectively focuses on patches where class-specific food features are
most prominent while successfully filtering out non-relevant patches. We
validate our approach through extensive experiments on few-shot classification
benchmarks: Food-101, VireoFood-172 and UECFood-256, outperforming the existing
SoA methods.

</details>


### [53] [DINOv3](https://arxiv.org/abs/2508.10104)
*Oriane Siméoni,Huy V. Vo,Maximilian Seitzer,Federico Baldassarre,Maxime Oquab,Cijo Jose,Vasil Khalidov,Marc Szafraniec,Seungeun Yi,Michaël Ramamonjisoa,Francisco Massa,Daniel Haziza,Luca Wehrstedt,Jianyuan Wang,Timothée Darcet,Théo Moutakanni,Leonel Sentana,Claire Roberts,Andrea Vedaldi,Jamie Tolan,John Brandt,Camille Couprie,Julien Mairal,Hervé Jégou,Patrick Labatut,Piotr Bojanowski*

Main category: cs.CV

TL;DR: DINOv3, a versatile vision foundation model, outperforms state-of-the-art methods in various vision tasks by leveraging data scaling, Gram anchoring, and post-hoc strategies, offering scalable solutions for diverse resource constraints.


<details>
  <summary>Details</summary>
Motivation: Self-supervised learning can eliminate the need for manual data annotation and enable models to scale effortlessly to massive datasets and larger architectures. It has the potential to learn visual representations from diverse sources using a single algorithm.

Method: The paper introduces a new method called Gram anchoring, which effectively addresses the issue of dense feature maps degrading during long training schedules. It also applies post-hoc strategies to enhance the models' flexibility with respect to resolution, model size, and alignment with text.

Result: DINOv3 outperforms the specialized state of the art across a broad range of settings, without fine-tuning.

Conclusion: DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models. The DINOv3 suite of vision models is designed to advance the state of the art on a wide spectrum of tasks and data by providing scalable solutions for diverse resource constraints and deployment scenarios.

Abstract: Self-supervised learning holds the promise of eliminating the need for manual
data annotation, enabling models to scale effortlessly to massive datasets and
larger architectures. By not being tailored to specific tasks or domains, this
training paradigm has the potential to learn visual representations from
diverse sources, ranging from natural to aerial images -- using a single
algorithm. This technical report introduces DINOv3, a major milestone toward
realizing this vision by leveraging simple yet effective strategies. First, we
leverage the benefit of scaling both dataset and model size by careful data
preparation, design, and optimization. Second, we introduce a new method called
Gram anchoring, which effectively addresses the known yet unsolved issue of
dense feature maps degrading during long training schedules. Finally, we apply
post-hoc strategies that further enhance our models' flexibility with respect
to resolution, model size, and alignment with text. As a result, we present a
versatile vision foundation model that outperforms the specialized state of the
art across a broad range of settings, without fine-tuning. DINOv3 produces
high-quality dense features that achieve outstanding performance on various
vision tasks, significantly surpassing previous self- and weakly-supervised
foundation models. We also share the DINOv3 suite of vision models, designed to
advance the state of the art on a wide spectrum of tasks and data by providing
scalable solutions for diverse resource constraints and deployment scenarios.

</details>


### [54] [Empowering Morphing Attack Detection using Interpretable Image-Text Foundation Model](https://arxiv.org/abs/2508.10110)
*Sushrut Patwardhan,Raghavendra Ramachandra,Sushma Venkatesh*

Main category: cs.CV

TL;DR: 本文提出了一种多模态学习方法，该方法可以提供变形攻击检测的文本描述。


<details>
  <summary>Details</summary>
Motivation: 变形攻击检测已成为人脸识别系统的重要组成部分，以确保可靠的验证场景。

Method: 多模态学习方法

Result: 对十种不同的文本提示进行了广泛的分析，这些提示包括短文本提示和长文本提示。这些提示是通过考虑人类可理解的文本片段来设计的。在人脸变形数据集上进行了广泛的实验，该数据集是使用公开的人脸生物识别数据集开发的。我们对 SOTA 预训练神经网络以及所提出的框架在五种不同变形生成技术的零样本评估中进行了评估，这些技术是在三种不同的媒介中捕获的。

Conclusion: 使用对比语言-图像预训练 (CLIP) 的拟议框架的零样本评估不仅可以产生通用的变形攻击检测，还可以预测最相关的文本片段。

Abstract: Morphing attack detection has become an essential component of face
recognition systems for ensuring a reliable verification scenario. In this
paper, we present a multimodal learning approach that can provide a textual
description of morphing attack detection. We first show that zero-shot
evaluation of the proposed framework using Contrastive Language-Image
Pretraining (CLIP) can yield not only generalizable morphing attack detection,
but also predict the most relevant text snippet. We present an extensive
analysis of ten different textual prompts that include both short and long
textual prompts. These prompts are engineered by considering the human
understandable textual snippet. Extensive experiments were performed on a face
morphing dataset that was developed using a publicly available face biometric
dataset. We present an evaluation of SOTA pre-trained neural networks together
with the proposed framework in the zero-shot evaluation of five different
morphing generation techniques that are captured in three different mediums.

</details>


### [55] [Interpretable Oracle Bone Script Decipherment through Radical and Pictographic Analysis with LVLMs](https://arxiv.org/abs/2508.10113)
*Kaixin Peng,Mengyang Zhao,Haiyang Yu,Teng Fu,Bin Li*

Main category: cs.CV

TL;DR: 提出了一种基于大型视觉-语言模型的可解释甲骨文破译方法，该方法结合了部首分析和象形-语义理解，以弥合字形和含义之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常忽略了字形和甲骨文语义之间错综复杂的联系，导致泛化和可解释性有限，尤其是在处理零样本设置和未破译的甲骨文时。

Method: 该方法基于大型视觉-语言模型，协同结合了部首分析和象形-语义理解，从而弥合了甲骨文的字形和含义之间的差距。具体来说，该方法提出了一种渐进式训练策略，引导模型从部首识别和分析到象形分析和相互分析，从而实现从字形到含义的推理。该方法还设计了一种受分析结果启发的部首-象形双重匹配机制，显著提高了模型的零样本破译性能。

Result: 在公共基准上的实验结果表明，该方法实现了最先进的 Top-10 准确率和卓越的零样本破译能力。

Conclusion: 该模型提供了逻辑分析过程，可能为未破译的甲骨文提供考古学上有价值的参考结果，因此在数字人文和历史研究中具有潜在的应用。

Abstract: As the oldest mature writing system, Oracle Bone Script (OBS) has long posed
significant challenges for archaeological decipherment due to its rarity,
abstractness, and pictographic diversity. Current deep learning-based methods
have made exciting progress on the OBS decipherment task, but existing
approaches often ignore the intricate connections between glyphs and the
semantics of OBS. This results in limited generalization and interpretability,
especially when addressing zero-shot settings and undeciphered OBS. To this
end, we propose an interpretable OBS decipherment method based on Large
Vision-Language Models, which synergistically combines radical analysis and
pictograph-semantic understanding to bridge the gap between glyphs and meanings
of OBS. Specifically, we propose a progressive training strategy that guides
the model from radical recognition and analysis to pictographic analysis and
mutual analysis, thus enabling reasoning from glyph to meaning. We also design
a Radical-Pictographic Dual Matching mechanism informed by the analysis
results, significantly enhancing the model's zero-shot decipherment
performance. To facilitate model training, we propose the Pictographic
Decipherment OBS Dataset, which comprises 47,157 Chinese characters annotated
with OBS images and pictographic analysis texts. Experimental results on public
benchmarks demonstrate that our approach achieves state-of-the-art Top-10
accuracy and superior zero-shot decipherment capabilities. More importantly,
our model delivers logical analysis processes, possibly providing
archaeologically valuable reference results for undeciphered OBS, and thus has
potential applications in digital humanities and historical research. The
dataset and code will be released in https://github.com/PKXX1943/PD-OBS.

</details>


### [56] [Deep Learning Enables Large-Scale Shape and Appearance Modeling in Total-Body DXA Imaging](https://arxiv.org/abs/2508.10132)
*Arianna Bunnell,Devon Cataldi,Yannik Glaser,Thomas K. Wolfgruber,Steven Heymsfield,Alan B. Zonderman,Thomas L. Kelly,Peter Sadowski,John A. Shepherd*

Main category: cs.CV

TL;DR: 开发了一种用于 TBDXA 扫描自动基准点放置的深度学习方法，并在 35,928 次扫描中用于形状和外观建模 (SAM)，以测试与健康指标的关联。


<details>
  <summary>Details</summary>
Motivation: 全身双能量 X 射线吸收仪 (TBDXA) 成像是一种相对低成本的全身成像方式，广泛用于身体成分评估。

Method: 我们开发并验证了一种深度学习方法，用于在使用 1,683 个手动注释的 TBDXA 扫描的 TBDXA 扫描上自动进行基准点放置。

Result: 该方法在外部测试数据集中实现了 99.5% 的正确关键点百分比。

Conclusion: 该方法生成的SAM特征分布与健康生物标志物相关，证实了现有证据，并为身体成分和形状与各种虚弱、代谢、炎症和心脏代谢健康标志物之间的关系生成了新的假设。

Abstract: Total-body dual X-ray absorptiometry (TBDXA) imaging is a relatively low-cost
whole-body imaging modality, widely used for body composition assessment. We
develop and validate a deep learning method for automatic fiducial point
placement on TBDXA scans using 1,683 manually-annotated TBDXA scans. The method
achieves 99.5% percentage correct keypoints in an external testing dataset. To
demonstrate the value for shape and appearance modeling (SAM), our method is
used to place keypoints on 35,928 scans for five different TBDXA imaging modes,
then associations with health markers are tested in two cohorts not used for
SAM model generation using two-sample Kolmogorov-Smirnov tests. SAM feature
distributions associated with health biomarkers are shown to corroborate
existing evidence and generate new hypotheses on body composition and shape's
relationship to various frailty, metabolic, inflammation, and cardiometabolic
health markers. Evaluation scripts, model weights, automatic point file
generation code, and triangulation files are available at
https://github.com/hawaii-ai/dxa-pointplacement.

</details>


### [57] [MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning](https://arxiv.org/abs/2508.10133)
*Thanh-Dat Truong,Christophe Bobda,Nitin Agarwal,Khoa Luu*

Main category: cs.CV

TL;DR: This paper introduces MANGO, a new multimodal fusion approach using Normalizing Flows and a novel Invertible Cross-Attention layer, achieving state-of-the-art results on several multimodal tasks.


<details>
  <summary>Details</summary>
Motivation: Current multimodal fusion methods struggle to capture essential features of each modality and comprehend complex structures and correlations of multimodal inputs.

Method: A novel Multimodal Attention-based Normalizing Flow (MANGO) approach with a new Invertible Cross-Attention (ICA) layer and three cross-attention mechanisms: Modality-to-Modality Cross-Attention (MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality Cross-Attention (LICA).

Result: The proposed approach achieves state-of-the-art (SoTA) performance on three different multimodal learning tasks.

Conclusion: The proposed approach achieves state-of-the-art performance on semantic segmentation, image-to-image translation, and movie genre classification tasks.

Abstract: Multimodal learning has gained much success in recent years. However, current
multimodal fusion methods adopt the attention mechanism of Transformers to
implicitly learn the underlying correlation of multimodal features. As a
result, the multimodal model cannot capture the essential features of each
modality, making it difficult to comprehend complex structures and correlations
of multimodal inputs. This paper introduces a novel Multimodal Attention-based
Normalizing Flow (MANGO) approach\footnote{The source code of this work will be
publicly available.} to developing explicit, interpretable, and tractable
multimodal fusion learning. In particular, we propose a new Invertible
Cross-Attention (ICA) layer to develop the Normalizing Flow-based Model for
multimodal data. To efficiently capture the complex, underlying correlations in
multimodal data in our proposed invertible cross-attention layer, we propose
three new cross-attention mechanisms: Modality-to-Modality Cross-Attention
(MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality
Cross-Attention (LICA). Finally, we introduce a new Multimodal Attention-based
Normalizing Flow to enable the scalability of our proposed method to
high-dimensional multimodal data. Our experimental results on three different
multimodal learning tasks, i.e., semantic segmentation, image-to-image
translation, and movie genre classification, have illustrated the
state-of-the-art (SoTA) performance of the proposed approach.

</details>


### [58] [Improving watermelon (Citrullus lanatus) disease classification with generative artificial intelligence (GenAI)-based synthetic and real-field images via a custom EfficientNetV2-L model](https://arxiv.org/abs/2508.10156)
*Nitin Rai,Nathan S. Boyd,Gary E. Vallad,Arnold W. Schumann*

Main category: cs.CV

TL;DR: 合成图像不能完全替代真实图像；相反，必须以混合方式使用两者，以最大限度地提高作物病害分类的模型性能。


<details>
  <summary>Details</summary>
Motivation: 评估整合真实图像和合成图像以提高疾病分类性能的有效性的研究有限。

Method: 使用增强的微调和迁移学习技术，在定制的EfficientNetV2-L架构上训练所有处理。

Result: 在H2、H3和H4处理上训练的模型表现出较高的精确率、召回率和F1分数指标。加权F1分数从0.65（H0）增加到1.00（H3-H4），表明少量真实图像与大量合成图像的结合提高了模型性能和泛化能力。

Conclusion: 结合少量真实图像和大量合成图像可以最大限度地提高作物病害分类的模型性能。

Abstract: The current advancements in generative artificial intelligence (GenAI) models
have paved the way for new possibilities for generating high-resolution
synthetic images, thereby offering a promising alternative to traditional image
acquisition for training computer vision models in agriculture. In the context
of crop disease diagnosis, GenAI models are being used to create synthetic
images of various diseases, potentially facilitating model creation and
reducing the dependency on resource-intensive in-field data collection.
However, limited research has been conducted on evaluating the effectiveness of
integrating real with synthetic images to improve disease classification
performance. Therefore, this study aims to investigate whether combining a
limited number of real images with synthetic images can enhance the prediction
accuracy of an EfficientNetV2-L model for classifying watermelon
\textit{(Citrullus lanatus)} diseases. The training dataset was divided into
five treatments: H0 (only real images), H1 (only synthetic images), H2 (1:1
real-to-synthetic), H3 (1:10 real-to-synthetic), and H4 (H3 + random images to
improve variability and model generalization). All treatments were trained
using a custom EfficientNetV2-L architecture with enhanced fine-tuning and
transfer learning techniques. Models trained on H2, H3, and H4 treatments
demonstrated high precision, recall, and F1-score metrics. Additionally, the
weighted F1-score increased from 0.65 (on H0) to 1.00 (on H3-H4) signifying
that the addition of a small number of real images with a considerable volume
of synthetic images improved model performance and generalizability. Overall,
this validates the findings that synthetic images alone cannot adequately
substitute for real images; instead, both must be used in a hybrid manner to
maximize model performance for crop disease classification.

</details>


### [59] [SynSpill: Improved Industrial Spill Detection With Synthetic Data](https://arxiv.org/abs/2508.10171)
*Aaditya Baranwal,Abdul Mueez,Jason Voelker,Guneet Bhatia,Shruti Vyas*

Main category: cs.CV

TL;DR: Synthetic data improves VLMs and object detectors for industrial spill detection where real data is scarce.


<details>
  <summary>Details</summary>
Motivation: VLMs degrade in niche, safety-critical domains like industrial spill detection due to the scarcity of real, annotated data.

Method: A scalable framework centered on a high-quality synthetic data generation pipeline for Parameter-Efficient Fine-Tuning (PEFT) of VLMs.

Result: Synthetic data enables effective PEFT of VLMs and boosts the performance of object detectors. VLMs generalize better than detectors without synthetic data, but both improve with it, achieving comparable performance.

Conclusion: High-fidelity synthetic data bridges the domain gap in safety-critical applications, offering a cost-effective pathway for deploying vision systems in industrial environments.

Abstract: Large-scale Vision-Language Models (VLMs) have transformed general-purpose
visual recognition through strong zero-shot capabilities. However, their
performance degrades significantly in niche, safety-critical domains such as
industrial spill detection, where hazardous events are rare, sensitive, and
difficult to annotate. This scarcity -- driven by privacy concerns, data
sensitivity, and the infrequency of real incidents -- renders conventional
fine-tuning of detectors infeasible for most industrial settings.
  We address this challenge by introducing a scalable framework centered on a
high-quality synthetic data generation pipeline. We demonstrate that this
synthetic corpus enables effective Parameter-Efficient Fine-Tuning (PEFT) of
VLMs and substantially boosts the performance of state-of-the-art object
detectors such as YOLO and DETR. Notably, in the absence of synthetic data
(SynSpill dataset), VLMs still generalize better to unseen spill scenarios than
these detectors. When SynSpill is used, both VLMs and detectors achieve marked
improvements, with their performance becoming comparable.
  Our results underscore that high-fidelity synthetic data is a powerful means
to bridge the domain gap in safety-critical applications. The combination of
synthetic generation and lightweight adaptation offers a cost-effective,
scalable pathway for deploying vision systems in industrial environments where
real data is scarce/impractical to obtain.
  Project Page: https://synspill.vercel.app

</details>


### [60] [EntropyGS: An Efficient Entropy Coding on 3D Gaussian Splatting](https://arxiv.org/abs/2508.10227)
*Yuning Huang,Jiahao Pang,Fengqing Zhu,Dong Tian*

Main category: cs.CV

TL;DR: proposes EntropyGS, a factorized and parameterized entropy coding method, demonstrates about 30x rate reduction


<details>
  <summary>Details</summary>
Motivation: storage/transmission and finally compression of 3DGS Gaussians become necessary

Method: a factorized and parameterized entropy coding method, EntropyGS

Result: spherical harmonic AC attributes precisely follow Laplace distributions, while mixtures of Gaussian distributions can approximate rotation, scaling, and opacity. Additionally, harmonic AC attributes manifest weak correlations with other attributes except for inherited correlations from a color space.

Conclusion: EntropyGS demonstrates about 30x rate reduction on benchmark datasets while maintaining similar rendering quality compared to input 3DGS data, with a fast encoding and decoding time.

Abstract: As an emerging novel view synthesis approach, 3D Gaussian Splatting (3DGS)
demonstrates fast training/rendering with superior visual quality. The two
tasks of 3DGS, Gaussian creation and view rendering, are typically separated
over time or devices, and thus storage/transmission and finally compression of
3DGS Gaussians become necessary. We begin with a correlation and statistical
analysis of 3DGS Gaussian attributes. An inspiring finding in this work reveals
that spherical harmonic AC attributes precisely follow Laplace distributions,
while mixtures of Gaussian distributions can approximate rotation, scaling, and
opacity. Additionally, harmonic AC attributes manifest weak correlations with
other attributes except for inherited correlations from a color space. A
factorized and parameterized entropy coding method, EntropyGS, is hereinafter
proposed. During encoding, distribution parameters of each Gaussian attribute
are estimated to assist their entropy coding. The quantization for entropy
coding is adaptively performed according to Gaussian attribute types. EntropyGS
demonstrates about 30x rate reduction on benchmark datasets while maintaining
similar rendering quality compared to input 3DGS data, with a fast encoding and
decoding time.

</details>


### [61] [CellSymphony: Deciphering the molecular and phenotypic orchestration of cells with single-cell pathomics](https://arxiv.org/abs/2508.10232)
*Paul H. Acosta,Pingjun Chen,Simon P. Castillo,Maria Esther Salvatierra,Yinyin Yuan,Xiaoxi Pan*

Main category: cs.CV

TL;DR: CellSymphony是一个多模态框架，它集成了空间基因表达和形态学背景，以实现准确的细胞类型注释并揭示不同的微环境生态位。


<details>
  <summary>Details</summary>
Motivation: 从组织学图像中提取强大的细胞水平特征并将其与空间转录组学数据整合仍然是一个关键挑战。

Method: CellSymphony，一个灵活的多模态框架，利用来自Xenium转录组谱和组织学图像的基础模型衍生的嵌入，实现真正的单细胞分辨率。

Result: CellSymphony实现了准确的细胞类型注释，并揭示了三种癌症类型的不同微环境生态位。

Conclusion: 利用基础模型和多模态融合，CellSymphony能够解码复杂组织生态系统中细胞的生理和表型协调。

Abstract: Xenium, a new spatial transcriptomics platform, enables
subcellular-resolution profiling of complex tumor tissues. Despite the rich
morphological information in histology images, extracting robust cell-level
features and integrating them with spatial transcriptomics data remains a
critical challenge. We introduce CellSymphony, a flexible multimodal framework
that leverages foundation model-derived embeddings from both Xenium
transcriptomic profiles and histology images at true single-cell resolution. By
learning joint representations that fuse spatial gene expression with
morphological context, CellSymphony achieves accurate cell type annotation and
uncovers distinct microenvironmental niches across three cancer types. This
work highlights the potential of foundation models and multimodal fusion for
deciphering the physiological and phenotypic orchestration of cells within
complex tissue ecosystems.

</details>


### [62] [Multi-Label Plant Species Prediction with Metadata-Enhanced Multi-Head Vision Transformers](https://arxiv.org/abs/2508.10457)
*Hanna Herasimchyk,Robin Labryga,Tomislav Prusina*

Main category: cs.CV

TL;DR: Multi-head vision transformer for multi-label plant species prediction, achieving 3rd place in PlantCLEF 2025.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the PlantCLEF 2025 challenge of multi-label plant species prediction in vegetation plot images, where a domain shift exists between training on single-species images and testing on multi-species quadrat images.

Method: A multi-head vision transformer approach using a pre-trained DINOv2 ViT-B/14 backbone with multiple classification heads for species, genus, and family prediction, incorporating multi-scale tiling, dynamic threshold optimization, and ensemble strategies.

Result: Achieved 3rd best performance on the private leaderboard.

Conclusion: The approach demonstrates strong performance, achieving 3rd place on the private leaderboard.

Abstract: We present a multi-head vision transformer approach for multi-label plant
species prediction in vegetation plot images, addressing the PlantCLEF 2025
challenge. The task involves training models on single-species plant images
while testing on multi-species quadrat images, creating a drastic domain shift.
Our methodology leverages a pre-trained DINOv2 Vision Transformer Base
(ViT-B/14) backbone with multiple classification heads for species, genus, and
family prediction, utilizing taxonomic hierarchies. Key contributions include
multi-scale tiling to capture plants at different scales, dynamic threshold
optimization based on mean prediction length, and ensemble strategies through
bagging and Hydra model architectures. The approach incorporates various
inference techniques including image cropping to remove non-plant artifacts,
top-n filtering for prediction constraints, and logit thresholding strategies.
Experiments were conducted on approximately 1.4 million training images
covering 7,806 plant species. Results demonstrate strong performance, making
our submission 3rd best on the private leaderboard. Our code is available at
https://github.com/geranium12/plant-clef-2025/tree/v1.0.0.

</details>


### [63] [Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets](https://arxiv.org/abs/2508.10256)
*Xinan Zhang,Haolin Wang,Yung-An Hsieh,Zhongyu Yang,Anthony Yezzi,Yi-Chang Tsai*

Main category: cs.CV

TL;DR: This review paper analyzes emerging trends in deep learning-based crack detection, introduces a new 3D laser scan dataset (3DCrack), and benchmarks deep learning methodologies.


<details>
  <summary>Details</summary>
Motivation: Crack detection plays a crucial role in civil infrastructures. Emerging trends are reshaping the landscape, including transitions in learning paradigms, improvements in generalizability, and diversification in dataset reacquisition.

Method: We systematically analyze these trends and highlight representative works. Additionally, we introduce a new dataset collected with 3D laser scans, 3DCrack, to support future research and conduct extensive benchmarking experiments to establish baselines for commonly used deep learning methodologies, including recent foundation models.

Result: We systematically analyze these trends and highlight representative works. Additionally, we introduce a new dataset collected with 3D laser scans, 3DCrack, to support future research and conduct extensive benchmarking experiments to establish baselines for commonly used deep learning methodologies, including recent foundation models.

Conclusion: Our findings provide insights into the evolving methodologies and future directions in deep learning-based crack detection. We introduce a new dataset collected with 3D laser scans, 3DCrack, to support future research and conduct extensive benchmarking experiments to establish baselines for commonly used deep learning methodologies, including recent foundation models.

Abstract: Crack detection plays a crucial role in civil infrastructures, including
inspection of pavements, buildings, etc., and deep learning has significantly
advanced this field in recent years. While numerous technical and review papers
exist in this domain, emerging trends are reshaping the landscape. These shifts
include transitions in learning paradigms (from fully supervised learning to
semi-supervised, weakly-supervised, unsupervised, few-shot, domain adaptation
and fine-tuning foundation models), improvements in generalizability (from
single-dataset performance to cross-dataset evaluation), and diversification in
dataset reacquisition (from RGB images to specialized sensor-based data). In
this review, we systematically analyze these trends and highlight
representative works. Additionally, we introduce a new dataset collected with
3D laser scans, 3DCrack, to support future research and conduct extensive
benchmarking experiments to establish baselines for commonly used deep learning
methodologies, including recent foundation models. Our findings provide
insights into the evolving methodologies and future directions in deep
learning-based crack detection. Project page:
https://github.com/nantonzhang/Awesome-Crack-Detection

</details>


### [64] [MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs](https://arxiv.org/abs/2508.10264)
*Haonan Ge,Yiwei Wang,Ming-Hsuan Yang,Yujun Cai*

Main category: cs.CV

TL;DR: 提出了一种名为Multi-Region Fusion Decoding (MRFD)的免训练解码方法，通过建模区域间一致性来提高事实基础。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型(LVLM)在多模态任务中表现出强大的性能。然而，由于验证图像不同区域信息的能力有限，它们经常产生幻觉——与视觉输入不一致的文本。

Method: Multi-Region Fusion Decoding (MRFD)

Result: 跨多个LVLM和基准的实验表明，

Conclusion: MRFD显著减少了幻觉并提高了响应的真实性，而无需模型更新。

Abstract: Large Vision-Language Models (LVLMs) have shown strong performance across
multimodal tasks. However, they often produce hallucinations -- text that is
inconsistent with visual input, due to the limited ability to verify
information in different regions of the image. To address this, we propose
Multi-Region Fusion Decoding (MRFD), a training-free decoding method that
improves factual grounding by modeling inter-region consistency. MRFD
identifies salient regions using cross-attention, generates initial responses
for each, and computes reliability weights based on Jensen-Shannon Divergence
(JSD) among the responses. These weights guide a consistency-aware fusion of
per-region predictions, using region-aware prompts inspired by Chain-of-Thought
reasoning. Experiments across multiple LVLMs and benchmarks show that MRFD
significantly reduces hallucinations and improves response factuality without
requiring model updates.

</details>


### [65] [Pose-Robust Calibration Strategy for Point-of-Gaze Estimation on Mobile Phones](https://arxiv.org/abs/2508.10268)
*Yujie Zhao,Jiabei Zeng,Shiguang Shan*

Main category: cs.CV

TL;DR: 本文研究了校准的注视点估计器中的关键因素，并探索了姿势鲁棒的校准策略。


<details>
  <summary>Details</summary>
Motivation: 基于外观的注视点 (PoG) 估计有所改进，但由于个人差异，估计器仍然难以在个体之间进行泛化。因此，需要进行针对个人的校准才能实现准确的 PoG 估计。然而，校准后的 PoG 估计器通常对头部姿势变化敏感。

Method: 构建了一个基准测试MobilePoG，其中包含来自32个人的面部图像，这些人专注于固定或连续变化的头部姿势下的指定点。使用此基准，我们系统地分析了校准点和头部姿势的多样性如何影响估计精度。我们提出了一种动态校准策略，用户在移动手机时注视校准点。

Result: 我们的实验表明，在校准过程中引入更广泛的头部姿势范围可以提高估计器处理姿势变化的能力。

Conclusion: 提出了一种动态校准策略，通过用户在移动手机时注视校准点，从而在用户友好且高效的校准过程中自然地引入头部姿势变化，最终产生更好的校准PoG估计器，该估计器对头部姿势变化的敏感度低于使用传统校准策略的估计器。

Abstract: Although appearance-based point-of-gaze (PoG) estimation has improved, the
estimators still struggle to generalize across individuals due to personal
differences. Therefore, person-specific calibration is required for accurate
PoG estimation. However, calibrated PoG estimators are often sensitive to head
pose variations. To address this, we investigate the key factors influencing
calibrated estimators and explore pose-robust calibration strategies.
Specifically, we first construct a benchmark, MobilePoG, which includes facial
images from 32 individuals focusing on designated points under either fixed or
continuously changing head poses. Using this benchmark, we systematically
analyze how the diversity of calibration points and head poses influences
estimation accuracy. Our experiments show that introducing a wider range of
head poses during calibration improves the estimator's ability to handle pose
variation. Building on this insight, we propose a dynamic calibration strategy
in which users fixate on calibration points while moving their phones. This
strategy naturally introduces head pose variation during a user-friendly and
efficient calibration process, ultimately producing a better calibrated PoG
estimator that is less sensitive to head pose variations than those using
conventional calibration strategies. Codes and datasets are available at our
project page.

</details>


### [66] [High Fidelity Text to Image Generation with Contrastive Alignment and Structural Guidance](https://arxiv.org/abs/2508.10280)
*Danyi Gao*

Main category: cs.CV

TL;DR: 提出了一种新的文本驱动图像生成方法，该方法提高了语义对齐和结构一致性，并在 COCO-2014 数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的文本驱动图像生成方法在语义对齐准确性和结构一致性方面存在性能瓶颈。

Method: 通过将文本-图像对比约束与结构引导机制相结合，提出了一种高保真图像生成方法。该方法引入了一个对比学习模块，该模块构建了强大的跨模态对齐约束，以提高文本和图像之间的语义匹配。同时，使用语义布局图或边缘草图等结构先验来指导生成器进行空间级结构建模。

Result: 在 COCO-2014 数据集上进行了系统实验。对嵌入维度、文本长度和结构引导强度进行了敏感性分析。定量指标证实了该方法在 CLIP Score、FID 和 SSIM 方面的优越性能。

Conclusion: 该方法有效弥合了语义对齐和结构保真度之间的差距，且没有增加计算复杂性。它展示了生成语义清晰、结构完整的图像的强大能力，为联合文本-图像建模和图像生成提供了可行的技术路径。

Abstract: This paper addresses the performance bottlenecks of existing text-driven
image generation methods in terms of semantic alignment accuracy and structural
consistency. A high-fidelity image generation method is proposed by integrating
text-image contrastive constraints with structural guidance mechanisms. The
approach introduces a contrastive learning module that builds strong
cross-modal alignment constraints to improve semantic matching between text and
image. At the same time, structural priors such as semantic layout maps or edge
sketches are used to guide the generator in spatial-level structural modeling.
This enhances the layout completeness and detail fidelity of the generated
images. Within the overall framework, the model jointly optimizes contrastive
loss, structural consistency loss, and semantic preservation loss. A
multi-objective supervision mechanism is adopted to improve the semantic
consistency and controllability of the generated content. Systematic
experiments are conducted on the COCO-2014 dataset. Sensitivity analyses are
performed on embedding dimensions, text length, and structural guidance
strength. Quantitative metrics confirm the superior performance of the proposed
method in terms of CLIP Score, FID, and SSIM. The results show that the method
effectively bridges the gap between semantic alignment and structural fidelity
without increasing computational complexity. It demonstrates a strong ability
to generate semantically clear and structurally complete images, offering a
viable technical path for joint text-image modeling and image generation.

</details>


### [67] [VIFSS: View-Invariant and Figure Skating-Specific Pose Representation Learning for Temporal Action Segmentation](https://arxiv.org/abs/2508.10281)
*Ryota Tanaka,Tomohiro Suzuki,Keisuke Fujii*

Main category: cs.CV

TL;DR: A new Temporal Action Segmentation (TAS) framework for figure skating jumps is proposed, incorporating 3D nature and semantic procedure of jump movements, achieving high accuracy even with limited data.


<details>
  <summary>Details</summary>
Motivation: Accurately recognizing the type and timing of jumps is essential for objective performance evaluation in figure skating, but this task typically requires expert-level knowledge and existing methods have limitations due to insufficient data and not accounting for the three-dimensional aspects and procedural structure of jump actions.

Method: A new TAS framework incorporating three-dimensional nature and semantic procedure of jump movements is proposed. It includes a View-Invariant, Figure Skating-Specific pose representation learning approach (VIFSS) and a fine-grained annotation scheme.

Result: The proposed method achieves over 92% F1@50 on element-level TAS, and view-invariant contrastive pre-training is particularly effective when fine-tuning data is limited.

Conclusion: The proposed TAS framework achieves over 92% F1@50 on element-level TAS and view-invariant contrastive pre-training is particularly effective when fine-tuning data is limited.

Abstract: Understanding human actions from videos plays a critical role across various
domains, including sports analytics. In figure skating, accurately recognizing
the type and timing of jumps a skater performs is essential for objective
performance evaluation. However, this task typically requires expert-level
knowledge due to the fine-grained and complex nature of jump procedures. While
recent approaches have attempted to automate this task using Temporal Action
Segmentation (TAS), there are two major limitations to TAS for figure skating:
the annotated data is insufficient, and existing methods do not account for the
inherent three-dimensional aspects and procedural structure of jump actions. In
this work, we propose a new TAS framework for figure skating jumps that
explicitly incorporates both the three-dimensional nature and the semantic
procedure of jump movements. First, we propose a novel View-Invariant, Figure
Skating-Specific pose representation learning approach (VIFSS) that combines
contrastive learning as pre-training and action classification as fine-tuning.
For view-invariant contrastive pre-training, we construct FS-Jump3D, the first
publicly available 3D pose dataset specialized for figure skating jumps.
Second, we introduce a fine-grained annotation scheme that marks the ``entry
(preparation)'' and ``landing'' phases, enabling TAS models to learn the
procedural structure of jumps. Extensive experiments demonstrate the
effectiveness of our framework. Our method achieves over 92% F1@50 on
element-level TAS, which requires recognizing both jump types and rotation
levels. Furthermore, we show that view-invariant contrastive pre-training is
particularly effective when fine-tuning data is limited, highlighting the
practicality of our approach in real-world scenarios.

</details>


### [68] [JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics](https://arxiv.org/abs/2508.10287)
*Simindokht Jahangard,Mehrzad Mohammadi,Yi Shen,Zhixi Cai,Hamid Rezatofighi*

Main category: cs.CV

TL;DR: This paper introduces JRDB-Reasoning, a new benchmark for visual reasoning in human-crowded environments, addressing limitations in existing benchmarks by formalizing reasoning complexity and providing detailed annotations.


<details>
  <summary>Details</summary>
Motivation: Existing visual reasoning benchmarks lack a clear definition of reasoning complexity, offer no control to generate questions over varying difficulty and task customization, and fail to provide structured, step-by-step reasoning annotations (workflows).

Method: The paper formalizes reasoning complexity, introduces an adaptive query engine that generates customizable questions, and extends the JRDB dataset with human-object interaction and geometric relationship annotations.

Result: The paper creates JRDB-Reasoning, a benchmark tailored for visual reasoning in human-crowded environments.

Conclusion: This paper introduces JRDB-Reasoning, a benchmark tailored for visual reasoning in human-crowded environments. The engine and benchmark enable fine-grained evaluation of visual reasoning frameworks and dynamic assessment of visual-language models across reasoning levels.

Abstract: Recent advances in Vision-Language Models (VLMs) and large language models
(LLMs) have greatly enhanced visual reasoning, a key capability for embodied AI
agents like robots. However, existing visual reasoning benchmarks often suffer
from several limitations: they lack a clear definition of reasoning complexity,
offer have no control to generate questions over varying difficulty and task
customization, and fail to provide structured, step-by-step reasoning
annotations (workflows). To bridge these gaps, we formalize reasoning
complexity, introduce an adaptive query engine that generates customizable
questions of varying complexity with detailed intermediate annotations, and
extend the JRDB dataset with human-object interaction and geometric
relationship annotations to create JRDB-Reasoning, a benchmark tailored for
visual reasoning in human-crowded environments. Our engine and benchmark enable
fine-grained evaluation of visual reasoning frameworks and dynamic assessment
of visual-language models across reasoning levels.

</details>


### [69] [A Sub-Pixel Multimodal Optical Remote Sensing Images Matching Method](https://arxiv.org/abs/2508.10294)
*Tao Huang,Hongbo Pan,Nanxi Zhou,Shun Zhou*

Main category: cs.CV

TL;DR: 提出PCWLAD方法以提高多模态光学图像的匹配精度，实验结果表明该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态光学图像的非线性辐射和几何变形差异通常会降低图像匹配精度。

Method: 提出了一种相位一致性加权最小绝对偏差（PCWLAD）亚像素模板匹配方法

Result: PCWLAD在正确匹配率（CMR）和均方根误差（RMSE）方面优于现有的八种方法，平均匹配精度约为0.4像素。

Conclusion: PCWLAD在三个图像数据集上的表现优于现有的八种方法，在正确匹配率（CMR）和均方根误差（RMSE）方面，平均匹配精度约为0.4像素。

Abstract: High-accuracy matching of multimodal optical images is the basis of geometric
processing. However, the image matching accuracy is usually degraded by the
nonlinear radiation and geometric deformation differences caused by different
spectral responses. To address these problems, we proposed a phase consistency
weighted least absolute deviation (PCWLAD) sub-pixel template matching method
to improve the matching accuracy of multimodal optical images. This method
consists of two main steps: coarse matching with the structural similarity
index measure (SSIM) and fine matching with WLAD. In the coarse matching step,
PCs are calculated without a noise filter to preserve the original structural
details, and template matching is performed using the SSIM. In the fine
matching step, we applied the radiometric and geometric transformation models
between two multimodal PC templates based on the coarse matching. Furthermore,
mutual structure filtering is adopted in the model to mitigate the impact of
noise within the corresponding templates on the structural consistency, and the
WLAD criterion is used to estimate the sub-pixel offset. To evaluate the
performance of PCWLAD, we created three types of image datasets: visible to
infrared Landsat images, visible to near-infrared close-range images, and
visible to infrared uncrewed aerial vehicle (UAV) images. PCWLAD outperformed
existing state-of-the-art eight methods in terms of correct matching rate (CMR)
and root mean square error (RMSE) and reached an average matching accuracy of
approximately 0.4 pixels across all three datasets. Our software and datasets
are publicly available at https://github.com/huangtaocsu/PCWLAD.

</details>


### [70] [InterSyn: Interleaved Learning for Dynamic Motion Synthesis in the Wild](https://arxiv.org/abs/2508.10297)
*Yiyi Ma,Yuanzhi Liang,Xiu Li,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: Interleaved Learning for Motion Synthesis (InterSyn) generates realistic interaction motions by learning from integrated motions that consider both solo and multi-person dynamics.


<details>
  <summary>Details</summary>
Motivation: Targets the generation of realistic interaction motions by learning from integrated motions that consider both solo and multi-person dynamics. Unlike previous methods that treat these components separately

Method: Interleaved Learning for Motion Synthesis (InterSyn), which employs an interleaved learning strategy to capture the natural, dynamic interactions and nuanced coordination inherent in real-world scenarios. It comprises two key modules: the Interleaved Interaction Synthesis (INS) module, and the Relative Coordination Refinement (REC) module.

Result: Motion sequences generated by InterSyn exhibit higher text-to-motion alignment and improved diversity compared with recent methods

Conclusion: The motion sequences generated by InterSyn exhibit higher text-to-motion alignment and improved diversity compared with recent methods, setting a new benchmark for robust and natural motion synthesis.

Abstract: We present Interleaved Learning for Motion Synthesis (InterSyn), a novel
framework that targets the generation of realistic interaction motions by
learning from integrated motions that consider both solo and multi-person
dynamics. Unlike previous methods that treat these components separately,
InterSyn employs an interleaved learning strategy to capture the natural,
dynamic interactions and nuanced coordination inherent in real-world scenarios.
Our framework comprises two key modules: the Interleaved Interaction Synthesis
(INS) module, which jointly models solo and interactive behaviors in a unified
paradigm from a first-person perspective to support multiple character
interactions, and the Relative Coordination Refinement (REC) module, which
refines mutual dynamics and ensures synchronized motions among characters.
Experimental results show that the motion sequences generated by InterSyn
exhibit higher text-to-motion alignment and improved diversity compared with
recent methods, setting a new benchmark for robust and natural motion
synthesis. Additionally, our code will be open-sourced in the future to promote
further research and development in this area.

</details>


### [71] [From Pixel to Mask: A Survey of Out-of-Distribution Segmentation](https://arxiv.org/abs/2508.10309)
*Wenjie Zhao,Jia Li,Yunhui Guo*

Main category: cs.CV

TL;DR: This survey reviews recent advances in OoD segmentation for autonomous-driving scenarios, identifies emerging challenges, and discusses promising future research directions. The paper groups current OoD segmentation approaches into four categories: (i) test-time OoD segmentation, (ii) outlier exposure for supervised training, (iii) reconstruction-based methods, (iv) and approaches that leverage powerful models.


<details>
  <summary>Details</summary>
Motivation: Concerns about AI security rise. Conventional OoD detection methods identify the existence of OoD objects but lack spatial localization, limiting their usefulness in downstream tasks. OoD segmentation addresses this limitation by localizing anomalous objects at pixel-level granularity. This capability is crucial for safety-critical applications such as autonomous driving, where perception modules must not only detect but also precisely segment OoD objects, enabling targeted control actions and enhancing overall system robustness.

Method: We group current OoD segmentation approaches into four categories: (i) test-time OoD segmentation, (ii) outlier exposure for supervised training, (iii) reconstruction-based methods, (iv) and approaches that leverage powerful models.

Result: OoD segmentation addresses this limitation by localizing anomalous objects at pixel-level granularity.

Conclusion: We systematically review recent advances in OoD segmentation for autonomous-driving scenarios, identify emerging challenges, and discuss promising future research directions.

Abstract: Out-of-distribution (OoD) detection and segmentation have attracted growing
attention as concerns about AI security rise. Conventional OoD detection
methods identify the existence of OoD objects but lack spatial localization,
limiting their usefulness in downstream tasks. OoD segmentation addresses this
limitation by localizing anomalous objects at pixel-level granularity. This
capability is crucial for safety-critical applications such as autonomous
driving, where perception modules must not only detect but also precisely
segment OoD objects, enabling targeted control actions and enhancing overall
system robustness. In this survey, we group current OoD segmentation approaches
into four categories: (i) test-time OoD segmentation, (ii) outlier exposure for
supervised training, (iii) reconstruction-based methods, (iv) and approaches
that leverage powerful models. We systematically review recent advances in OoD
segmentation for autonomous-driving scenarios, identify emerging challenges,
and discuss promising future research directions.

</details>


### [72] [Integrating Reinforcement Learning with Visual Generative Models: Foundations and Advances](https://arxiv.org/abs/2508.10316)
*Yuanzhi Liang,Yijie Fang,Rui Li,Ziqi Ni,Ruijie Su,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 本文综述了使用强化学习生成视觉内容。


<details>
  <summary>Details</summary>
Motivation: 生成模型在合成视觉内容方面取得了显著进展，但它们通常使用与感知质量、语义准确性或物理真实感不符的替代目标进行训练。强化学习 (RL) 提供了一个优化不可微分、偏好驱动和时间结构化目标的原则性框架。

Method: 本文对基于强化学习的视觉内容生成方法进行了系统性概述。回顾了强化学习从经典控制到作为通用优化工具的演变，并考察了其在图像、视频和 3D/4D 生成中的集成。

Result: 强化学习不仅可以作为一种微调机制，还可以作为一种结构组件，用于将生成与复杂的高级目标对齐。

Conclusion: 本文总结了基于强化学习的视觉内容生成方法，并提出了强化学习与生成模型结合的开放性挑战和未来研究方向。

Abstract: Generative models have made significant progress in synthesizing visual
content, including images, videos, and 3D/4D structures. However, they are
typically trained with surrogate objectives such as likelihood or
reconstruction loss, which often misalign with perceptual quality, semantic
accuracy, or physical realism. Reinforcement learning (RL) offers a principled
framework for optimizing non-differentiable, preference-driven, and temporally
structured objectives. Recent advances demonstrate its effectiveness in
enhancing controllability, consistency, and human alignment across generative
tasks. This survey provides a systematic overview of RL-based methods for
visual content generation. We review the evolution of RL from classical control
to its role as a general-purpose optimization tool, and examine its integration
into image, video, and 3D/4D generation. Across these domains, RL serves not
only as a fine-tuning mechanism but also as a structural component for aligning
generation with complex, high-level goals. We conclude with open challenges and
future research directions at the intersection of RL and generative modeling.

</details>


### [73] [Concepts or Skills? Rethinking Instruction Selection for Multi-modal Models](https://arxiv.org/abs/2508.10339)
*Andrew Bai,Justin Cui,Ruochen Wang,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: The paper introduces a targeted training data selection method for vision-language instruction tuning, which selects instructions based on whether a benchmark benefits more from similar concepts or skills. This method improves performance on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Vision-language benchmarks fall into the dichotomy of mainly benefiting from training on instructions with similar skills or visual concepts.

Method: The authors designed a simple targeted training data selection method to optimize the performance of a given benchmark. They extract the concepts/skills from the benchmark, determine whether the benchmark predominantly benefits from similar concepts or skills, and finally select instructions with the most matching concepts/skills.

Result: Experiments on 10+ benchmarks validate the effectiveness of the targeted data selection method, showing +0.9% over the best existing baseline averaged over all benchmarks and +1.5% on the skill-focused subset.

Conclusion: The paper underscores the importance of recognizing the inherent trade-off within instruction selection, which requires balancing the acquisition of conceptual knowledge against visual skill.

Abstract: Vision-language instruction tuning achieves two main purposes: learning
visual concepts and learning visual skills. In this paper, we found that
vision-language benchmarks fall into the dichotomy of mainly benefiting from
training on instructions with similar skills or visual concepts. Inspired by
the discovery, we designed a simple targeted training data selection method to
optimize the performance of a given benchmark. We first extract the
concepts/skills from the benchmark, determine whether the benchmark
predominantly benefits from similar concepts or skills, and finally select
instructions with the most matching concepts/skills. Experiments on 10+
benchmarks validate the effectiveness of our targeted data selection method,
showing +0.9\% over the best existing baseline averaged over all benchmarks and
+1.5\% on the skill-focused subset. Our findings underscore the importance of
recognizing the inherent trade-off within instruction selection, which requires
balancing the acquisition of conceptual knowledge against visual skill.

</details>


### [74] [Glo-DMU: A Deep Morphometry Framework of Ultrastructural Characterization in Glomerular Electron Microscopic Images](https://arxiv.org/abs/2508.10351)
*Zhentai Zhang,Danyi Weng,Guibin Zhang,Xiang Chen,Kaixing Long,Jian Geng,Yanmeng Lu,Lei Zhang,Zhitao Zhou,Lei Cao*

Main category: cs.CV

TL;DR: 提出 Glo-DMU 框架，实现肾小球超微结构的全自动、高精度和高通量量化，辅助肾脏病理诊断。


<details>
  <summary>Details</summary>
Motivation: 复杂的超微结构特征可以指示肾脏疾病的类型、进展和预后。最近，计算病理学与深度学习方法相结合，在推进肾小球超微结构的自动形态学分析方面显示出巨大的潜力。然而，目前的研究主要集中在个体超微结构的识别上，这使得它很难满足实际的诊断需求。

Method: 提出了一种基于三个深度模型的肾小球形态测量框架 Glo-DMU：超微结构分割模型、肾小球滤过屏障区域分类模型和电子致密沉积物检测模型。

Result: 在实际诊断场景中评估了 115 例患有 9 种肾脏病理类型的患者，证明了自动量化结果与病理报告中的形态学描述之间具有良好的一致性。

Conclusion: Glo-DMU 具有全自动化、高精度和高通量的特点，可同时量化多个超微结构特征，为辅助肾脏病理学家提供了一种高效工具。

Abstract: Complex and diverse ultrastructural features can indicate the type,
progression, and prognosis of kidney diseases. Recently, computational
pathology combined with deep learning methods has shown tremendous potential in
advancing automatic morphological analysis of glomerular ultrastructure.
However, current research predominantly focuses on the recognition of
individual ultrastructure, which makes it challenging to meet practical
diagnostic needs. In this study, we propose the glomerular morphometry
framework of ultrastructural characterization (Glo-DMU), which is grounded on
three deep models: the ultrastructure segmentation model, the glomerular
filtration barrier region classification model, and the electron-dense deposits
detection model. Following the conventional protocol of renal biopsy diagnosis,
this framework simultaneously quantifies the three most widely used
ultrastructural features: the thickness of glomerular basement membrane, the
degree of foot process effacement, and the location of electron-dense deposits.
We evaluated the 115 patients with 9 renal pathological types in real-world
diagnostic scenarios, demonstrating good consistency between automatic
quantification results and morphological descriptions in the pathological
reports. Glo-DMU possesses the characteristics of full automation, high
precision, and high throughput, quantifying multiple ultrastructural features
simultaneously, and providing an efficient tool for assisting renal
pathologists.

</details>


### [75] [Improving OCR for Historical Texts of Multiple Languages](https://arxiv.org/abs/2508.10356)
*Hylke Westerdijk,Ben Blankenborg,Khondoker Ittehadul Islam*

Main category: cs.CV

TL;DR: applied deep learning techniques in OCR and Document Layout Analysis, including historical Hebrew fragments, 16th to 18th-century meeting resolutions, and modern English handwriting recognition


<details>
  <summary>Details</summary>
Motivation: enhanced our dataset through extensive data augmentation and employed the Kraken and TrOCR models to improve character recognition

Method: utilizing a Convolutional Recurrent Neural Network (CRNN) that integrated DeepLabV3+ for semantic segmentation with a Bidirectional LSTM, incorporating confidence-based pseudolabeling to refine our model; applied a CRNN with a ResNet34 encoder, trained using the Connectionist Temporal Classification (CTC) loss function to effectively capture sequential dependencies

Result: improve character recognition; refine our model; effectively capture sequential dependencies

Conclusion: This report offers valuable insights and suggests potential directions for future research.

Abstract: This paper presents our methodology and findings from three tasks across
Optical Character Recognition (OCR) and Document Layout Analysis using advanced
deep learning techniques. First, for the historical Hebrew fragments of the
Dead Sea Scrolls, we enhanced our dataset through extensive data augmentation
and employed the Kraken and TrOCR models to improve character recognition. In
our analysis of 16th to 18th-century meeting resolutions task, we utilized a
Convolutional Recurrent Neural Network (CRNN) that integrated DeepLabV3+ for
semantic segmentation with a Bidirectional LSTM, incorporating confidence-based
pseudolabeling to refine our model. Finally, for modern English handwriting
recognition task, we applied a CRNN with a ResNet34 encoder, trained using the
Connectionist Temporal Classification (CTC) loss function to effectively
capture sequential dependencies. This report offers valuable insights and
suggests potential directions for future research.

</details>


### [76] [AtomDiffuser: Time-Aware Degradation Modeling for Drift and Beam Damage in STEM Imaging](https://arxiv.org/abs/2508.10359)
*Hao Wang,Hongkui Zheng,Kai He,Abolfazl Razi*

Main category: cs.CV

TL;DR: AtomDiffuser是一个时间感知退化建模框架，可以解开样品漂移和辐射衰减，在真实冷冻扫描透射电子显微镜数据中表现良好。


<details>
  <summary>Details</summary>
Motivation: 由于空间漂移和辐射损伤，解释时间分辨扫描透射电子显微镜（STEM）数据仍然具有挑战性。这些因素以复杂的时间相关方式扭曲了几何形状和强度，使得现有方法难以明确分离它们的影响或以原子分辨率对材料动力学进行建模。

Method: 提出AtomDiffuser，一个时间感知退化建模框架，通过预测任何两个STEM帧之间的仿射变换和空间变化的衰减图，来解开样品漂移和辐射衰减。

Result: AtomDiffuser能够很好地推广到真实的冷冻扫描透射电子显微镜数据，并支持高分辨率降解推断和漂移对齐。

Conclusion: AtomDiffuser在真实冷冻扫描透射电子显微镜数据中表现良好，支持高分辨率降解推断和漂移对齐，并提供可视化和量化降解模式的工具，这些模式与辐射引起的原子不稳定性相关。

Abstract: Scanning transmission electron microscopy (STEM) plays a critical role in
modern materials science, enabling direct imaging of atomic structures and
their evolution under external interferences. However, interpreting
time-resolved STEM data remains challenging due to two entangled degradation
effects: spatial drift caused by mechanical and thermal instabilities, and
beam-induced signal loss resulting from radiation damage. These factors distort
both geometry and intensity in complex, temporally correlated ways, making it
difficult for existing methods to explicitly separate their effects or model
material dynamics at atomic resolution. In this work, we present AtomDiffuser,
a time-aware degradation modeling framework that disentangles sample drift and
radiometric attenuation by predicting an affine transformation and a spatially
varying decay map between any two STEM frames. Unlike traditional denoising or
registration pipelines, our method leverages degradation as a physically
heuristic, temporally conditioned process, enabling interpretable structural
evolutions across time. Trained on synthetic degradation processes,
AtomDiffuser also generalizes well to real-world cryo-STEM data. It further
supports high-resolution degradation inference and drift alignment, offering
tools for visualizing and quantifying degradation patterns that correlate with
radiation-induced atomic instabilities.

</details>


### [77] [Contrast Sensitivity Function of Multimodal Vision-Language Models](https://arxiv.org/abs/2508.10367)
*Pablo Hernández-Cámara,Alexandra Gomez-Villa,Jose Manuel Jaén-Lorites,Jorge Vila-Tomás,Jesus Malo,Valero Laparra*

Main category: cs.CV

TL;DR: 论文提出了一种新方法来评估多模态模型的视觉敏感度，发现它们在视觉表征上与人类感知存在差距，且提示语措辞很重要。


<details>
  <summary>Details</summary>
Motivation: 评估多模态视觉-语言模型与人类感知的对齐情况至关重要，以了解它们如何感知低级视觉特征。人类视觉的一个关键特征是对比敏感度函数 (CSF)。

Method: 通过提示模型判断不同对比度下图案的可见性，从而评估模型的对比敏感度函数 (CSF)。

Result: 一些模型近似于人类的 CSF 形状或幅度，但没有一个模型能完全复制两者。提示语措辞对响应有很大影响。

Conclusion: 多模态模型在视觉表征上与人类感知存在差距，提示语措辞对模型响应有很大影响。

Abstract: Assessing the alignment of multimodal vision-language models~(VLMs) with
human perception is essential to understand how they perceive low-level visual
features. A key characteristic of human vision is the contrast sensitivity
function (CSF), which describes sensitivity to spatial frequency at
low-contrasts. Here, we introduce a novel behavioral psychophysics-inspired
method to estimate the CSF of chat-based VLMs by directly prompting them to
judge pattern visibility at different contrasts for each frequency. This
methodology is closer to the real experiments in psychophysics than the
previously reported. Using band-pass filtered noise images and a diverse set of
prompts, we assess model responses across multiple architectures. We find that
while some models approximate human-like CSF shape or magnitude, none fully
replicate both. Notably, prompt phrasing has a large effect on the responses,
raising concerns about prompt stability. Our results provide a new framework
for probing visual sensitivity in multimodal models and reveal key gaps between
their visual representations and human perception.

</details>


### [78] [Towards Spatially Consistent Image Generation: On Incorporating Intrinsic Scene Properties into Diffusion Models](https://arxiv.org/abs/2508.10382)
*Hyundo Lee,Suhyung Choi,Byoung-Tak Zhang,Inwoo Hwang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的图像生成方法，该方法通过共同生成图像及其对应的内在属性，隐式地捕捉潜在的场景结构，从而生成更符合空间一致性和更真实的图像。


<details>
  <summary>Details</summary>
Motivation: 图像生成模型虽然可以合成高质量的图像，但由于关于底层结构和空间布局的信息有限，通常会产生空间不一致和扭曲的图像。

Method: 该方法首先使用预训练的估计器从大型图像数据集中提取丰富的内在场景属性，然后使用自动编码器将各种内在场景属性聚合为单个潜在变量。在预训练的大规模潜在扩散模型（LDM）的基础上，我们的方法通过仔细共享互信息来同时去噪图像和内在域，以便图像和内在反映彼此，而不会降低图像质量。

Result: 实验结果表明，该方法能够纠正空间不一致性，并生成更自然的场景布局，同时保持基础模型的保真度和文本对齐。

Conclusion: 该方法能够纠正空间不一致性，并生成更自然的场景布局，同时保持基础模型的保真度和文本对齐。

Abstract: Image generation models trained on large datasets can synthesize high-quality
images but often produce spatially inconsistent and distorted images due to
limited information about the underlying structures and spatial layouts. In
this work, we leverage intrinsic scene properties (e.g., depth, segmentation
maps) that provide rich information about the underlying scene, unlike prior
approaches that solely rely on image-text pairs or use intrinsics as
conditional inputs. Our approach aims to co-generate both images and their
corresponding intrinsics, enabling the model to implicitly capture the
underlying scene structure and generate more spatially consistent and realistic
images. Specifically, we first extract rich intrinsic scene properties from a
large image dataset with pre-trained estimators, eliminating the need for
additional scene information or explicit 3D representations. We then aggregate
various intrinsic scene properties into a single latent variable using an
autoencoder. Building upon pre-trained large-scale Latent Diffusion Models
(LDMs), our method simultaneously denoises the image and intrinsic domains by
carefully sharing mutual information so that the image and intrinsic reflect
each other without degrading image quality. Experimental results demonstrate
that our method corrects spatial inconsistencies and produces a more natural
layout of scenes while maintaining the fidelity and textual alignment of the
base model (e.g., Stable Diffusion).

</details>


### [79] [Unlocking Robust Semantic Segmentation Performance via Label-only Elastic Deformations against Implicit Label Noise](https://arxiv.org/abs/2508.10383)
*Yechan Kim,Dongho Yoon,Younkwan Lee,Unse Fatima,Hong Kook Kim,Songjae Lee,Sanga Park,Jeong Ho Park,Seonjong Kang,Moongu Jeon*

Main category: cs.CV

TL;DR: NSegment+是一种新的数据增强方法，通过解耦图像和标签转换来提高语义分割的性能，解决了现实数据集中存在的微妙标签噪声问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据集表现出微妙的标签缺陷，这些缺陷来自固有的挑战，例如模糊的对象边界和注释者的可变性。虽然不明确存在，但这种轻微和潜在的噪声仍然会损害模型性能。典型的数据增强方法可能会放大这些细微的缺陷，并限制模型的泛化能力。

Method: 引入一种新的增强框架NSegment+，该框架解耦了图像和标签转换。通过仅对分割标签引入受控的弹性形变，同时保留原始图像，鼓励模型关注学习对象结构的鲁棒表示。

Result: NSegment+在Vaihingen、LoveDA、Cityscapes和PASCAL VOC上分别实现了高达+2.29、+2.38、+1.75和+3.39的mIoU增益。

Conclusion: NSegment+通过解耦图像和标签转换来解决语义分割中现实的噪声问题，即使没有其他技巧，也能持续提高性能。

Abstract: While previous studies on image segmentation focus on handling severe (or
explicit) label noise, real-world datasets also exhibit subtle (or implicit)
label imperfections. These arise from inherent challenges, such as ambiguous
object boundaries and annotator variability. Although not explicitly present,
such mild and latent noise can still impair model performance. Typical data
augmentation methods, which apply identical transformations to the image and
its label, risk amplifying these subtle imperfections and limiting the model's
generalization capacity. In this paper, we introduce NSegment+, a novel
augmentation framework that decouples image and label transformations to
address such realistic noise for semantic segmentation. By introducing
controlled elastic deformations only to segmentation labels while preserving
the original images, our method encourages models to focus on learning robust
representations of object structures despite minor label inconsistencies.
Extensive experiments demonstrate that NSegment+ consistently improves
performance, achieving mIoU gains of up to +2.29, +2.38, +1.75, and +3.39 in
average on Vaihingen, LoveDA, Cityscapes, and PASCAL VOC, respectively-even
without bells and whistles, highlighting the importance of addressing implicit
label noise. These gains can be further amplified when combined with other
training tricks, including CutMix and Label Smoothing.

</details>


### [80] [PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection](https://arxiv.org/abs/2508.10397)
*Haibin Sun,Xinghui Song*

Main category: cs.CV

TL;DR: 我们提出了 PQ-DAF，它使用视觉语言模型来扩展训练数据并增强跨域鲁棒性，从而提高驾驶员分心检测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的模型在部署到实际场景中时，泛化能力通常会下降。这种限制主要源于实际环境中数据注释的高成本导致的few-shot学习挑战，以及训练数据集和目标部署条件之间的显着领域转移。

Method: 我们提出了一个姿势驱动的质量控制数据增强框架 (PQ-DAF)，该框架利用视觉语言模型进行样本过滤，以经济高效地扩展训练数据并增强跨域鲁棒性。具体来说，我们采用渐进条件扩散模型 (PCDM) 来准确捕获关键驾驶员姿势特征并合成各种训练示例。然后，引入一个建立在 CogVLM 视觉语言模型之上的样本质量评估模块，以根据置信度阈值过滤掉低质量的合成样本，从而确保增强数据集的可靠性。

Result: PQ-DAF 显著提高了在少量驾驶员分心检测中的性能，在数据稀缺的情况下实现了模型泛化的显著提升。

Conclusion: PQ-DAF 显著提高了在少量驾驶员分心检测中的性能，在数据稀缺的情况下实现了模型泛化的显著提升。

Abstract: Driver distraction detection is essential for improving traffic safety and
reducing road accidents. However, existing models often suffer from degraded
generalization when deployed in real-world scenarios. This limitation primarily
arises from the few-shot learning challenge caused by the high cost of data
annotation in practical environments, as well as the substantial domain shift
between training datasets and target deployment conditions. To address these
issues, we propose a Pose-driven Quality-controlled Data Augmentation Framework
(PQ-DAF) that leverages a vision-language model for sample filtering to
cost-effectively expand training data and enhance cross-domain robustness.
Specifically, we employ a Progressive Conditional Diffusion Model (PCDMs) to
accurately capture key driver pose features and synthesize diverse training
examples. A sample quality assessment module, built upon the CogVLM
vision-language model, is then introduced to filter out low-quality synthetic
samples based on a confidence threshold, ensuring the reliability of the
augmented dataset. Extensive experiments demonstrate that PQ-DAF substantially
improves performance in few-shot driver distraction detection, achieving
significant gains in model generalization under data-scarce conditions.

</details>


### [81] [Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.10407)
*Eunseo Koh,Seunghoo Hong,Tae-Young Kim,Simon S. Woo,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 提出了一种新方法，通过修改文本嵌入空间中的delta向量来抑制扩散模型中纠缠的内容。


<details>
  <summary>Details</summary>
Motivation: 文本到图像（T2I）扩散模型在从文本提示生成多样化的高质量图像方面取得了显著进展。然而，这些模型在抑制与特定词语强烈相关的联内容方面仍然面临挑战。例如，当生成“查理·卓别林”的图像时，即使明确指示不包括，也始终会出现“小胡子”，因为“小胡子”的概念与“查理·卓别林”密切相关。

Method: 提出了一种新方法，通过引入delta向量来修改文本嵌入，以削弱生成图像中不需要的内容的影响。此外，还提出了一种带有Delta向量的选择性抑制（SSDV）方法，使delta向量能够适应交叉注意机制，从而更有效地抑制区域中不需要的内容。

Result: 通过优化delta向量，在个性化的T2I模型中实现了更精确的抑制，这是之前的基线无法实现的。大量的实验结果表明，

Conclusion: 该方法显著优于现有方法，无论是在定量还是定性指标方面。

Abstract: Text-to-Image (T2I) diffusion models have made significant progress in
generating diverse high-quality images from textual prompts. However, these
models still face challenges in suppressing content that is strongly entangled
with specific words. For example, when generating an image of ``Charlie
Chaplin", a ``mustache" consistently appears even if explicitly instructed not
to include it, as the concept of ``mustache" is strongly entangled with
``Charlie Chaplin". To address this issue, we propose a novel approach to
directly suppress such entangled content within the text embedding space of
diffusion models. Our method introduces a delta vector that modifies the text
embedding to weaken the influence of undesired content in the generated image,
and we further demonstrate that this delta vector can be easily obtained
through a zero-shot approach. Furthermore, we propose a Selective Suppression
with Delta Vector (SSDV) method to adapt delta vector into the cross-attention
mechanism, enabling more effective suppression of unwanted content in regions
where it would otherwise be generated. Additionally, we enabled more precise
suppression in personalized T2I models by optimizing delta vector, which
previous baselines were unable to achieve. Extensive experimental results
demonstrate that our approach significantly outperforms existing methods, both
in terms of quantitative and qualitative metrics.

</details>


### [82] [SC-Lane: Slope-aware and Consistent Road Height Estimation Framework for 3D Lane Detection](https://arxiv.org/abs/2508.10411)
*Chaesong Park,Eunbin Seo,Jihyeon Hwang,Jongwoo Lim*

Main category: cs.CV

TL;DR: SC-Lane improves height estimation and 3D lane detection.


<details>
  <summary>Details</summary>
Motivation: improving robustness to diverse road geometries

Method: a novel slope-aware and temporally consistent heightmap estimation framework

Result: achieving state-of-the-art performance with an F-score of 64.3%

Conclusion: SC-Lane significantly improves both height estimation and 3D lane detection, achieving state-of-the-art performance with an F-score of 64.3%, outperforming existing methods by a notable margin.

Abstract: In this paper, we introduce SC-Lane, a novel slope-aware and temporally
consistent heightmap estimation framework for 3D lane detection. Unlike
previous approaches that rely on fixed slope anchors, SC-Lane adaptively
determines the fusion of slope-specific height features, improving robustness
to diverse road geometries. To achieve this, we propose a Slope-Aware Adaptive
Feature module that dynamically predicts the appropriate weights from image
cues for integrating multi-slope representations into a unified heightmap.
Additionally, a Height Consistency Module enforces temporal coherence, ensuring
stable and accurate height estimation across consecutive frames, which is
crucial for real-world driving scenarios. To evaluate the effectiveness of
SC-Lane, we employ three standardized metrics-Mean Absolute Error(MAE), Root
Mean Squared Error (RMSE), and threshold-based accuracy-which, although common
in surface and depth estimation, have been underutilized for road height
assessment. Using the LiDAR-derived heightmap dataset introduced in prior work
[20], we benchmark our method under these metrics, thereby establishing a
rigorous standard for future comparisons. Extensive experiments on the OpenLane
benchmark demonstrate that SC-Lane significantly improves both height
estimation and 3D lane detection, achieving state-of-the-art performance with
an F-score of 64.3%, outperforming existing methods by a notable margin. For
detailed results and a demonstration video, please refer to our project
page:https://parkchaesong.github.io/sclane/

</details>


### [83] [NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer](https://arxiv.org/abs/2508.10424)
*Shanyuan Liu,Jian Zhu,Junda Lu,Yue Gong,Liuzhuozheng Li,Bo Cheng,Yuhang Ma,Liebucha Wu,Xiaoyu Wu,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: NanoControl通过低开销的LoRA风格控制模块和KV-Context Augmentation机制，实现了高效且高质量的可控文本到图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有的使用DiTs的可控文本到图像生成方法依赖于ControlNet范式，这引入了显著的参数开销和增加的计算成本。

Method: 采用Flux作为骨干网络，并设计了一个LoRA风格的控制模块，以及引入KV-Context Augmentation机制。

Result: NanoControl实现了最先进的可控文本到图像生成性能，同时参数数量仅增加0.024%，GFLOPs仅增加0.029%。

Conclusion: NanoControl显著降低了计算开销，同时保持了卓越的生成质量和改进的可控性。

Abstract: Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in
text-to-image synthesis. However, in the domain of controllable text-to-image
generation using DiTs, most existing methods still rely on the ControlNet
paradigm originally designed for UNet-based diffusion models. This paradigm
introduces significant parameter overhead and increased computational costs. To
address these challenges, we propose the Nano Control Diffusion Transformer
(NanoControl), which employs Flux as the backbone network. Our model achieves
state-of-the-art controllable text-to-image generation performance while
incurring only a 0.024\% increase in parameter count and a 0.029\% increase in
GFLOPs, thus enabling highly efficient controllable generation. Specifically,
rather than duplicating the DiT backbone for control, we design a LoRA-style
(low-rank adaptation) control module that directly learns control signals from
raw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation
mechanism that integrates condition-specific key-value information into the
backbone in a simple yet highly effective manner, facilitating deep fusion of
conditional features. Extensive benchmark experiments demonstrate that
NanoControl significantly reduces computational overhead compared to
conventional control approaches, while maintaining superior generation quality
and achieving improved controllability.

</details>


### [84] [STRIDE-QA: Visual Question Answering Dataset for Spatiotemporal Reasoning in Urban Driving Scenes](https://arxiv.org/abs/2508.10427)
*Keishi Ishihara,Kento Sasaki,Tsubasa Takahashi,Daiki Shiono,Yu Yamaguchi*

Main category: cs.CV

TL;DR: STRIDE-QA 是一个大规模 VQA 数据集，用于提高 VLM 在自动驾驶中进行时空推理的能力。


<details>
  <summary>Details</summary>
Motivation: 在静态的、网络来源的图像-文本对上进行训练从根本上限制了理解和预测动态交通场景所需的精确时空推理。

Method: 构建了一个大规模视觉问答 (VQA) 数据集 STRIDE-QA，用于从以自我为中心的角度进行物理基础推理。该数据集由在东京的 100 小时多传感器驾驶数据构建而成，捕获了多样化且具有挑战性的条件，是城市驾驶中用于时空推理的最大 VQA 数据集，在 285K 帧上提供 1600 万个 QA 对。

Result: 在 STRIDE-QA 上微调的 VLM 表现出显着的性能提升，在空间定位方面取得了 55% 的成功率，在未来运动预测方面取得了 28% 的一致性，而通用 VLM 的得分接近于零。

Conclusion: STRIDE-QA 为开发更可靠的、用于安全攸关的自主系统的 VLM 奠定了全面的基础。

Abstract: Vision-Language Models (VLMs) have been applied to autonomous driving to
support decision-making in complex real-world scenarios. However, their
training on static, web-sourced image-text pairs fundamentally limits the
precise spatiotemporal reasoning required to understand and predict dynamic
traffic scenes. We address this critical gap with STRIDE-QA, a large-scale
visual question answering (VQA) dataset for physically grounded reasoning from
an ego-centric perspective. Constructed from 100 hours of multi-sensor driving
data in Tokyo, capturing diverse and challenging conditions, STRIDE-QA is the
largest VQA dataset for spatiotemporal reasoning in urban driving, offering 16
million QA pairs over 285K frames. Grounded by dense, automatically generated
annotations including 3D bounding boxes, segmentation masks, and multi-object
tracks, the dataset uniquely supports both object-centric and ego-centric
reasoning through three novel QA tasks that require spatial localization and
temporal prediction. Our benchmarks demonstrate that existing VLMs struggle
significantly, achieving near-zero scores on prediction consistency. In
contrast, VLMs fine-tuned on STRIDE-QA exhibit dramatic performance gains,
achieving 55% success in spatial localization and 28% consistency in future
motion prediction, compared to near-zero scores from general-purpose VLMs.
Therefore, STRIDE-QA establishes a comprehensive foundation for developing more
reliable VLMs for safety-critical autonomous systems.

</details>


### [85] [CRISP: Contrastive Residual Injection and Semantic Prompting for Continual Video Instance Segmentation](https://arxiv.org/abs/2508.10432)
*Baichen Liu,Qi Lyu,Xudong Wang,Jiahua Dong,Lianqing Liu,Zhi Han*

Main category: cs.CV

TL;DR: CRISP是一种用于持续视频实例分割的方法，它通过对比残差注入和语义提示来解决实例、类别和任务方面的混淆，从而在避免灾难性遗忘的同时，提高分割和分类性能。


<details>
  <summary>Details</summary>
Motivation: 持续视频实例分割需要吸收新对象类别的可塑性和保留先前学习对象类别的稳定性，同时保持跨帧的时间一致性。

Method: 对比残差注入和语义提示(CRISP)

Result: 在YouTube-VIS-2019和YouTube-VIS-2021数据集上的大量实验表明，CRISP显著优于现有的持续分割方法。

Conclusion: CRISP在长期持续视频实例分割任务中显著优于现有的持续分割方法，避免了灾难性遗忘，有效提高了分割和分类性能。

Abstract: Continual video instance segmentation demands both the plasticity to absorb
new object categories and the stability to retain previously learned ones, all
while preserving temporal consistency across frames. In this work, we introduce
Contrastive Residual Injection and Semantic Prompting (CRISP), an earlier
attempt tailored to address the instance-wise, category-wise, and task-wise
confusion in continual video instance segmentation. For instance-wise learning,
we model instance tracking and construct instance correlation loss, which
emphasizes the correlation with the prior query space while strengthening the
specificity of the current task query. For category-wise learning, we build an
adaptive residual semantic prompt (ARSP) learning framework, which constructs a
learnable semantic residual prompt pool generated by category text and uses an
adjustive query-prompt matching mechanism to build a mapping relationship
between the query of the current task and the semantic residual prompt.
Meanwhile, a semantic consistency loss based on the contrastive learning is
introduced to maintain semantic coherence between object queries and residual
prompts during incremental training. For task-wise learning, to ensure the
correlation at the inter-task level within the query space, we introduce a
concise yet powerful initialization strategy for incremental prompts. Extensive
experiments on YouTube-VIS-2019 and YouTube-VIS-2021 datasets demonstrate that
CRISP significantly outperforms existing continual segmentation methods in the
long-term continual video instance segmentation task, avoiding catastrophic
forgetting and effectively improving segmentation and classification
performance. The code is available at https://github.com/01upup10/CRISP.

</details>


### [86] [DOD-SA: Infrared-Visible Decoupled Object Detection with Single-Modality Annotations](https://arxiv.org/abs/2508.10445)
*Hang Jin,Chenqiang Gao,Junjie Guo,Fangcen Liu,Kanghui Tian,Qinyao Chang*

Main category: cs.CV

TL;DR: DOD-SA: A novel infrared-visible decoupled object detection framework with single-modality annotations, achieving SOTA results on DroneVehicle.


<details>
  <summary>Details</summary>
Motivation: Existing infrared-visible object detection methods require dual-modality annotations, which incurs high annotation costs.

Method: A novel infrared-visible Decoupled Object Detection framework with Single-modality Annotations, called DOD-SA, built upon a Single- and Dual-Modality Collaborative Teacher-Student Network (CoSD-TSNet) with a Progressive and Self-Tuning Training Strategy (PaST) and a Pseudo Label Assigner (PLA).

Result: The proposed method achieves state-of-the-art performance on the DroneVehicle dataset.

Conclusion: The proposed method outperforms state-of-the-art (SOTA) on the DroneVehicle dataset.

Abstract: Infrared-visible object detection has shown great potential in real-world
applications, enabling robust all-day perception by leveraging the
complementary information of infrared and visible images. However, existing
methods typically require dual-modality annotations to output detection results
for both modalities during prediction, which incurs high annotation costs. To
address this challenge, we propose a novel infrared-visible Decoupled Object
Detection framework with Single-modality Annotations, called DOD-SA. The
architecture of DOD-SA is built upon a Single- and Dual-Modality Collaborative
Teacher-Student Network (CoSD-TSNet), which consists of a single-modality
branch (SM-Branch) and a dual-modality decoupled branch (DMD-Branch). The
teacher model generates pseudo-labels for the unlabeled modality,
simultaneously supporting the training of the student model. The collaborative
design enables cross-modality knowledge transfer from the labeled modality to
the unlabeled modality, and facilitates effective SM-to-DMD branch supervision.
To further improve the decoupling ability of the model and the pseudo-label
quality, we introduce a Progressive and Self-Tuning Training Strategy (PaST)
that trains the model in three stages: (1) pretraining SM-Branch, (2) guiding
the learning of DMD-Branch by SM-Branch, and (3) refining DMD-Branch. In
addition, we design a Pseudo Label Assigner (PLA) to align and pair labels
across modalities, explicitly addressing modality misalignment during training.
Extensive experiments on the DroneVehicle dataset demonstrate that our method
outperforms state-of-the-art (SOTA).

</details>


### [87] [SkeySpot: Automating Service Key Detection for Digital Electrical Layout Plans in the Construction Industry](https://arxiv.org/abs/2508.10449)
*Dhruv Dosi,Rohit Meena,Param Rajpura,Yogesh Kumar Meena*

Main category: cs.CV

TL;DR: This paper introduces SkeySpot, a YOLOv8-based toolkit for real-time electrical symbol detection in scanned floor plans, achieving 82.5% mAP and improving interoperability in the construction industry.


<details>
  <summary>Details</summary>
Motivation: Lack of machine-readable floor plans makes large-scale interpretation time-consuming and error-prone. Automated symbol spotting offers a scalable solution.

Method: A systematic evaluation framework is proposed using pretrained object detection models for DELP dataset, with YOLOv8 achieving the highest performance.

Result: YOLOv8 achieves the highest performance with a mean Average Precision (mAP) of 82.5% on the DELP dataset.

Conclusion: SkeySpot toolkit enables real-time detection, classification, and quantification of electrical symbols, producing structured outputs for interoperable building information workflows, enhancing compatibility and reducing dependency on proprietary systems.

Abstract: Legacy floor plans, often preserved only as scanned documents, remain
essential resources for architecture, urban planning, and facility management
in the construction industry. However, the lack of machine-readable floor plans
render large-scale interpretation both time-consuming and error-prone.
Automated symbol spotting offers a scalable solution by enabling the
identification of service key symbols directly from floor plans, supporting
workflows such as cost estimation, infrastructure maintenance, and regulatory
compliance. This work introduces a labelled Digitised Electrical Layout Plans
(DELP) dataset comprising 45 scanned electrical layout plans annotated with
2,450 instances across 34 distinct service key classes. A systematic evaluation
framework is proposed using pretrained object detection models for DELP
dataset. Among the models benchmarked, YOLOv8 achieves the highest performance
with a mean Average Precision (mAP) of 82.5\%. Using YOLOv8, we develop
SkeySpot, a lightweight, open-source toolkit for real-time detection,
classification, and quantification of electrical symbols. SkeySpot produces
structured, standardised outputs that can be scaled up for interoperable
building information workflows, ultimately enabling compatibility across
downstream applications and regulatory platforms. By lowering dependency on
proprietary CAD systems and reducing manual annotation effort, this approach
makes the digitisation of electrical layouts more accessible to small and
medium-sized enterprises (SMEs) in the construction industry, while supporting
broader goals of standardisation, interoperability, and sustainability in the
built environment.

</details>


### [88] [From Images to Perception: Emergence of Perceptual Properties by Reconstructing Images](https://arxiv.org/abs/2508.10450)
*Pablo Hernández-Cámara,Jesus Malo,Valero Laparra*

Main category: cs.CV

TL;DR: 这项工作表明，视觉系统可能被调整为消除具有该稀疏程度的那些特定程度的失真，并且生物启发模型可以在没有人为监督的情况下学习感知指标。


<details>
  <summary>Details</summary>
Motivation: 许多科学家认为，人类的视觉感知可能来自于图像统计，从而在早期视觉中形成有效的神经表征。

Method: 一个生物启发架构，可以适应视网膜 V1 皮层的几个已知事实，PerceptNet，已经为与图像重建相关的不同任务进行了端到端优化：自动编码、去噪、去模糊和稀疏正则化。

Result: 编码器阶段（类似 V1 的层）始终表现出与人类对图像失真的感知判断的最高相关性，尽管在初始化或训练中没有使用感知信息。这种对齐在适度的噪声、模糊和稀疏性方面表现出最佳状态。

Conclusion: 生物启发模型可以在没有人为监督的情况下学习感知指标。

Abstract: A number of scientists suggested that human visual perception may emerge from
image statistics, shaping efficient neural representations in early vision. In
this work, a bio-inspired architecture that can accommodate several known facts
in the retina-V1 cortex, the PerceptNet, has been end-to-end optimized for
different tasks related to image reconstruction: autoencoding, denoising,
deblurring, and sparsity regularization. Our results show that the encoder
stage (V1-like layer) consistently exhibits the highest correlation with human
perceptual judgments on image distortion despite not using perceptual
information in the initialization or training. This alignment exhibits an
optimum for moderate noise, blur and sparsity. These findings suggest that the
visual system may be tuned to remove those particular levels of distortion with
that level of sparsity and that biologically inspired models can learn
perceptual metrics without human supervision.

</details>


### [89] [Trajectory-aware Shifted State Space Models for Online Video Super-Resolution](https://arxiv.org/abs/2508.10453)
*Qiang Zhu,Xiandong Meng,Yuxian Jiang,Fan Zhang,David Bull,Shuyuan Zhu,Bing Zeng*

Main category: cs.CV

TL;DR: This paper introduces TS-Mamba, a novel online VSR method using trajectory-aware shifted SSMs for efficient spatio-temporal information aggregation, achieving state-of-the-art performance with reduced complexity.


<details>
  <summary>Details</summary>
Motivation: Most of the existing online VSR methods solely employ one neighboring previous frame to achieve temporal alignment, which limits long-range temporal modeling of videos. Recently, state space models (SSMs) have been proposed with linear computational complexity and a global receptive field, which significantly improve computational efficiency and performance. In this context, this paper presents a novel online VSR method based on Trajectory-aware Shifted SSMs (TS-Mamba), leveraging both long-term trajectory modeling and low-complexity Mamba to achieve efficient spatio-temporal information aggregation.

Method: a novel online VSR method based on Trajectory-aware Shifted SSMs (TS-Mamba), leveraging both long-term trajectory modeling and low-complexity Mamba to achieve efficient spatio-temporal information aggregation. Specifically, TS-Mamba first constructs the trajectories within a video to select the most similar tokens from the previous frames. Then, a Trajectory-aware Shifted Mamba Aggregation (TSMA) module consisting of proposed shifted SSMs blocks is employed to aggregate the selected tokens. The shifted SSMs blocks are designed based on Hilbert scannings and corresponding shift operations to compensate for scanning losses and strengthen the spatial continuity of Mamba. Additionally, we propose a trajectory-aware loss function to supervise the trajectory generation, ensuring the accuracy of token selection when training our model.

Result: compared with six online VSR benchmark models, our TS-Mamba achieves state-of-the-art performance in most cases and over 22.7% complexity reduction (in MACs).

Conclusion: TS-Mamba achieves state-of-the-art performance in most cases and over 22.7% complexity reduction (in MACs).

Abstract: Online video super-resolution (VSR) is an important technique for many
real-world video processing applications, which aims to restore the current
high-resolution video frame based on temporally previous frames. Most of the
existing online VSR methods solely employ one neighboring previous frame to
achieve temporal alignment, which limits long-range temporal modeling of
videos. Recently, state space models (SSMs) have been proposed with linear
computational complexity and a global receptive field, which significantly
improve computational efficiency and performance. In this context, this paper
presents a novel online VSR method based on Trajectory-aware Shifted SSMs
(TS-Mamba), leveraging both long-term trajectory modeling and low-complexity
Mamba to achieve efficient spatio-temporal information aggregation.
Specifically, TS-Mamba first constructs the trajectories within a video to
select the most similar tokens from the previous frames. Then, a
Trajectory-aware Shifted Mamba Aggregation (TSMA) module consisting of proposed
shifted SSMs blocks is employed to aggregate the selected tokens. The shifted
SSMs blocks are designed based on Hilbert scannings and corresponding shift
operations to compensate for scanning losses and strengthen the spatial
continuity of Mamba. Additionally, we propose a trajectory-aware loss function
to supervise the trajectory generation, ensuring the accuracy of token
selection when training our model. Extensive experiments on three widely used
VSR test datasets demonstrate that compared with six online VSR benchmark
models, our TS-Mamba achieves state-of-the-art performance in most cases and
over 22.7\% complexity reduction (in MACs). The source code for TS-Mamba will
be available at https://github.com.

</details>


### [90] [SingleStrip: learning skull-stripping from a single labeled example](https://arxiv.org/abs/2508.10464)
*Bella Specktor-Fadida,Malte Hoffmann*

Main category: cs.CV

TL;DR: 结合领域随机化和自编码器进行半监督分割，仅需少量标记数据即可实现不错的性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习分割严重依赖于标记数据，但是手动标记既费力又耗时，尤其是对于诸如脑磁共振成像（MRI）之类的体积图像。领域随机化技术通过从标签图合成各种训练图像来缓解对标记数据的依赖性，但是当可用的标签图很少时，它们提供的解剖学变异性有限。半监督自训练通过将模型预测迭代地合并到训练集中来解决标签稀缺的问题，从而使网络能够从末标记数据中学习。

Method: 结合领域随机化与自训练，使用单个标记示例训练三维颅骨剥离网络。首先，自动对体素强度进行分箱，生成用于合成图像的标签，以训练初始颅骨剥离模型。其次，在标记示例上训练卷积自编码器（AE），并使用其重建误差来评估为未标记数据预测的脑掩模的质量。第三，选择排名最高的伪标签来微调网络。

Result: 在分布外数据上实现了接近用更多标记图像训练的模型的颅骨剥离性能。将基于AE的排名与测试时增强下基于一致性的排名进行比较，发现AE方法产生与分割精度更强的相关性。

Conclusion: 结合领域随机化和基于AE的质量控制，可以从极有限的标记数据中实现有效的半监督分割。这种策略可以减轻标记负担，从而减缓涉及新解剖结构或新兴成像技术的研究进展。

Abstract: Deep learning segmentation relies heavily on labeled data, but manual
labeling is laborious and time-consuming, especially for volumetric images such
as brain magnetic resonance imaging (MRI). While recent domain-randomization
techniques alleviate the dependency on labeled data by synthesizing diverse
training images from label maps, they offer limited anatomical variability when
very few label maps are available. Semi-supervised self-training addresses
label scarcity by iteratively incorporating model predictions into the training
set, enabling networks to learn from unlabeled data. In this work, we combine
domain randomization with self-training to train three-dimensional
skull-stripping networks using as little as a single labeled example. First, we
automatically bin voxel intensities, yielding labels we use to synthesize
images for training an initial skull-stripping model. Second, we train a
convolutional autoencoder (AE) on the labeled example and use its
reconstruction error to assess the quality of brain masks predicted for
unlabeled data. Third, we select the top-ranking pseudo-labels to fine-tune the
network, achieving skull-stripping performance on out-of-distribution data that
approaches models trained with more labeled images. We compare AE-based ranking
to consistency-based ranking under test-time augmentation, finding that the AE
approach yields a stronger correlation with segmentation accuracy. Our results
highlight the potential of combining domain randomization and AE-based quality
control to enable effective semi-supervised segmentation from extremely limited
labeled data. This strategy may ease the labeling burden that slows progress in
studies involving new anatomical structures or emerging imaging techniques.

</details>


### [91] [Enhanced Sparse Point Cloud Data Processing for Privacy-aware Human Action Recognition](https://arxiv.org/abs/2508.10469)
*Maimunatu Tunau,Vincent Gbouna Zakka,Zhuangzhuang Dai*

Main category: cs.CV

TL;DR: This paper analyzes and enhances three data processing methods for mmWave radar HAR, providing insights into their performance and trade-offs to guide future research.


<details>
  <summary>Details</summary>
Motivation: Traditional vision-based Human Action Recognition (HAR) systems raise privacy concerns. mmWave radar sensors offer a privacy-preserving alternative but have challenges due to the sparse and noisy nature of their point cloud data. A comprehensive evaluation of existing data processing methods is lacking.

Method: The paper conducts a detailed performance analysis of three methods: Density-Based Spatial Clustering of Applications with Noise (DBSCAN), the Hungarian Algorithm, and Kalman Filtering using the MiliPoint dataset. It evaluates each method individually, all possible pairwise combinations, and the combination of all three, assessing both recognition accuracy and computational cost. Furthermore, the paper proposes targeted enhancements to the individual methods aimed at improving accuracy.

Result: The results provide crucial insights into the strengths and trade-offs of each method and their integrations, guiding future work on mmWave based HAR systems.

Conclusion: This paper provides a detailed performance analysis of three data processing methods (DBSCAN, Hungarian Algorithm, and Kalman Filtering) for mmWave radar-based human action recognition using the MiliPoint dataset. The study evaluates each method individually, in pairwise combinations, and all three combined, assessing recognition accuracy and computational cost. The results offer insights into the strengths and trade-offs of each method.

Abstract: Human Action Recognition (HAR) plays a crucial role in healthcare, fitness
tracking, and ambient assisted living technologies. While traditional vision
based HAR systems are effective, they pose privacy concerns. mmWave radar
sensors offer a privacy preserving alternative but present challenges due to
the sparse and noisy nature of their point cloud data. In the literature, three
primary data processing methods: Density-Based Spatial Clustering of
Applications with Noise (DBSCAN), the Hungarian Algorithm, and Kalman Filtering
have been widely used to improve the quality and continuity of radar data.
However, a comprehensive evaluation of these methods, both individually and in
combination, remains lacking. This paper addresses that gap by conducting a
detailed performance analysis of the three methods using the MiliPoint dataset.
We evaluate each method individually, all possible pairwise combinations, and
the combination of all three, assessing both recognition accuracy and
computational cost. Furthermore, we propose targeted enhancements to the
individual methods aimed at improving accuracy. Our results provide crucial
insights into the strengths and trade-offs of each method and their
integrations, guiding future work on mmWave based HAR systems

</details>


### [92] [STAMP: Multi-pattern Attention-aware Multiple Instance Learning for STAS Diagnosis in Multi-center Histopathology Images](https://arxiv.org/abs/2508.10473)
*Liangrui Pan,xiaoyu Li,Guang Zhu,Guanting Li,Ruixin Wang,Jiadi Luo,Yaning Yang,Liang qingchun,Shaoliang Peng*

Main category: cs.CV

TL;DR: 提出了STAMP模型用于肺腺癌STAS诊断，并在多个数据集上取得了优于临床水平的结果。


<details>
  <summary>Details</summary>
Motivation: 在肺腺癌（LUAD）中，通过气腔扩散（STAS）构成了一种新的侵袭模式，与肿瘤复发和生存率降低有关。然而，由于其独特的病理学特征和形态学特征，大规模STAS诊断仍然是一项劳动密集型工作，容易出现疏忽和误诊。因此，迫切需要利用深度学习模型进行STAS诊断。

Method: 提出了一个多模式注意感知多示例学习框架STAMP，以分析和诊断多中心组织病理学图像中STAS的存在。

Result: STAMP在STAS-SXY、STAS-TXY和STAS-TCGA上取得了有竞争力的诊断结果。

Conclusion: STAMP在STAS-SXY、STAS-TXY和STAS-TCGA上取得了有竞争力的诊断结果，AUC分别为0.8058、0.8017和0.7928，超过了临床水平。

Abstract: Spread through air spaces (STAS) constitutes a novel invasive pattern in lung
adenocarcinoma (LUAD), associated with tumor recurrence and diminished survival
rates. However, large-scale STAS diagnosis in LUAD remains a labor-intensive
endeavor, compounded by the propensity for oversight and misdiagnosis due to
its distinctive pathological characteristics and morphological features.
Consequently, there is a pressing clinical imperative to leverage deep learning
models for STAS diagnosis. This study initially assembled histopathological
images from STAS patients at the Second Xiangya Hospital and the Third Xiangya
Hospital of Central South University, alongside the TCGA-LUAD cohort. Three
senior pathologists conducted cross-verification annotations to construct the
STAS-SXY, STAS-TXY, and STAS-TCGA datasets. We then propose a multi-pattern
attention-aware multiple instance learning framework, named STAMP, to analyze
and diagnose the presence of STAS across multi-center histopathology images.
Specifically, the dual-branch architecture guides the model to learn
STAS-associated pathological features from distinct semantic spaces.
Transformer-based instance encoding and a multi-pattern attention aggregation
modules dynamically selects regions closely associated with STAS pathology,
suppressing irrelevant noise and enhancing the discriminative power of global
representations. Moreover, a similarity regularization constraint prevents
feature redundancy across branches, thereby improving overall diagnostic
accuracy. Extensive experiments demonstrated that STAMP achieved competitive
diagnostic results on STAS-SXY, STAS-TXY and STAS-TCGA, with AUCs of 0.8058,
0.8017, and 0.7928, respectively, surpassing the clinical level.

</details>


### [93] [TweezeEdit: Consistent and Efficient Image Editing with Path Regularization](https://arxiv.org/abs/2508.10498)
*Jianda Mao,Kaibo Wang,Yang Xiang,Kani Chen*

Main category: cs.CV

TL;DR: TweezeEdit是一种新的图像编辑框架，它能更好地保留原始图像的语义，同时快速地实现文本引导的编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法在过度对齐目标提示的同时，对源图像语义的保留不足，并且由于编辑路径过长而效率低下。

Method: 提出了一种tuning- and inversion-free的框架TweezeEdit，通过梯度驱动的正则化，使用一致性模型沿直接路径有效地注入目标提示语义。

Result: 大量实验表明，TweezeEdit在语义保留和目标对齐方面优于现有方法。

Conclusion: TweezeEdit在语义保留和目标对齐方面表现出色，并且编辑速度快，仅需12步（每次编辑1.6秒）。

Abstract: Large-scale pre-trained diffusion models empower users to edit images through
text guidance. However, existing methods often over-align with target prompts
while inadequately preserving source image semantics. Such approaches generate
target images explicitly or implicitly from the inversion noise of the source
images, termed the inversion anchors. We identify this strategy as suboptimal
for semantic preservation and inefficient due to elongated editing paths. We
propose TweezeEdit, a tuning- and inversion-free framework for consistent and
efficient image editing. Our method addresses these limitations by regularizing
the entire denoising path rather than relying solely on the inversion anchors,
ensuring source semantic retention and shortening editing paths. Guided by
gradient-driven regularization, we efficiently inject target prompt semantics
along a direct path using a consistency model. Extensive experiments
demonstrate TweezeEdit's superior performance in semantic preservation and
target alignment, outperforming existing methods. Remarkably, it requires only
12 steps (1.6 seconds per edit), underscoring its potential for real-time
applications.

</details>


### [94] [Multi-Sample Anti-Aliasing and Constrained Optimization for 3D Gaussian Splatting](https://arxiv.org/abs/2508.10507)
*Zheng Zhou,Jia-Chen Zhang,Yu-Jie Xiong,Chun-Ming Xia*

Main category: cs.CV

TL;DR: 提出了一种综合优化框架，该框架集成了多重采样抗锯齿 (MSAA) 与双重几何约束，以解决三维高斯溅射中细节模糊的问题。


<details>
  <summary>Details</summary>
Motivation: 三维高斯溅射的最新进展显着改进了实时新颖视角合成，但场景优化期间的几何约束不足通常会导致精细细节的重建模糊，尤其是在具有高频纹理和清晰非连续性的区域中。

Method: 该系统通过自适应混合四重子样本来计算像素颜色，从而有效减少高频分量中的混叠伪影。该框架引入了两个约束：(a) 一种自适应加权策略，通过动态梯度分析来优先考虑未充分重建的区域，以及 (b) 梯度微分约束，用于在对象边界处强制执行几何正则化。

Result: 在多个基准测试中进行的大量实验评估表明，该方法在细节保留方面实现了最先进的性能，尤其是在保留高频纹理和清晰的非连续性方面，同时保持了实时渲染效率。定量指标和感知研究证实，在结构相似性 (SSIM) 和感知质量 (LPIPS) 方面，相对于基线方法，该方法在统计上具有显着改进。

Conclusion: 该方法在细节保留方面实现了最先进的性能，尤其是在保留高频纹理和清晰的非连续性方面，同时保持了实时渲染效率。定量指标和感知研究证实，在结构相似性 (SSIM) 和感知质量 (LPIPS) 方面，相对于基线方法，该方法在统计上具有显着改进。

Abstract: Recent advances in 3D Gaussian splatting have significantly improved
real-time novel view synthesis, yet insufficient geometric constraints during
scene optimization often result in blurred reconstructions of fine-grained
details, particularly in regions with high-frequency textures and sharp
discontinuities. To address this, we propose a comprehensive optimization
framework integrating multisample anti-aliasing (MSAA) with dual geometric
constraints. Our system computes pixel colors through adaptive blending of
quadruple subsamples, effectively reducing aliasing artifacts in high-frequency
components. The framework introduces two constraints: (a) an adaptive weighting
strategy that prioritizes under-reconstructed regions through dynamic gradient
analysis, and (b) gradient differential constraints enforcing geometric
regularization at object boundaries. This targeted optimization enables the
model to allocate computational resources preferentially to critical regions
requiring refinement while maintaining global consistency. Extensive
experimental evaluations across multiple benchmarks demonstrate that our method
achieves state-of-the-art performance in detail preservation, particularly in
preserving high-frequency textures and sharp discontinuities, while maintaining
real-time rendering efficiency. Quantitative metrics and perceptual studies
confirm statistically significant improvements over baseline approaches in both
structural similarity (SSIM) and perceptual quality (LPIPS).

</details>


### [95] [A Segmentation-driven Editing Method for Bolt Defect Augmentation and Detection](https://arxiv.org/abs/2508.10509)
*Yangjie Xiao,Ke Zhang,Jiacun Wang,Xin Sheng,Yurong Guo,Meijuan Chen,Zehua Ren,Zhaoye Zheng,Zhenbing Zhao*

Main category: cs.CV

TL;DR: 提出了一种分割驱动的螺栓缺陷编辑方法（SBDE）来扩充数据集，该方法通过生成高质量的掩码和属性编辑将正常螺栓转换为缺陷螺栓，并通过编辑恢复增强（ERA）策略放回原始检查场景中，从而扩大缺陷检测数据集。


<details>
  <summary>Details</summary>
Motivation: 螺栓缺陷检测对于确保输电线路的安全至关重要。然而，缺陷图像的稀缺和数据分布的不平衡严重限制了检测性能。为了解决这个问题，我们提出...

Method: 提出了一种分割驱动的螺栓缺陷编辑方法（SBDE），通过CLAHE-FFT适配器（CFA）和多部件感知掩码解码器（MAMD）增强复杂螺栓属性的分割，生成高质量的掩码，并通过掩码优化模块（MOD）与图像修复模型（LaMa）集成，构建螺栓缺陷属性编辑模型（MOD-LaMa），通过属性编辑将正常螺栓转换为缺陷螺栓。最后，提出了一种编辑恢复增强（ERA）策略，将编辑后的缺陷螺栓恢复并放回原始检查场景中，从而扩大缺陷检测数据集。

Result: 实验结果表明，SBDE生成的螺栓缺陷图像明显优于最先进的图像编辑模型，并有效提高了螺栓缺陷检测的性能。

Conclusion: SBDE生成的螺栓缺陷图像明显优于最先进的图像编辑模型，并有效提高了螺栓缺陷检测的性能，充分验证了该方法的有效性和应用潜力。

Abstract: Bolt defect detection is critical to ensure the safety of transmission lines.
However, the scarcity of defect images and imbalanced data distributions
significantly limit detection performance. To address this problem, we propose
a segmentationdriven bolt defect editing method (SBDE) to augment the dataset.
First, a bolt attribute segmentation model (Bolt-SAM) is proposed, which
enhances the segmentation of complex bolt attributes through the CLAHE-FFT
Adapter (CFA) and Multipart- Aware Mask Decoder (MAMD), generating high-quality
masks for subsequent editing tasks. Second, a mask optimization module (MOD) is
designed and integrated with the image inpainting model (LaMa) to construct the
bolt defect attribute editing model (MOD-LaMa), which converts normal bolts
into defective ones through attribute editing. Finally, an editing recovery
augmentation (ERA) strategy is proposed to recover and put the edited defect
bolts back into the original inspection scenes and expand the defect detection
dataset. We constructed multiple bolt datasets and conducted extensive
experiments. Experimental results demonstrate that the bolt defect images
generated by SBDE significantly outperform state-of-the-art image editing
models, and effectively improve the performance of bolt defect detection, which
fully verifies the effectiveness and application potential of the proposed
method. The code of the project is available at
https://github.com/Jay-xyj/SBDE.

</details>


### [96] [EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba](https://arxiv.org/abs/2508.10522)
*Quang Nguyen,Nhat Le,Baoru Huang,Minh Nhat Vu,Chengcheng Tang,Van Nguyen,Ngan Le,Thieu Vo,Anh Nguyen*

Main category: cs.CV

TL;DR: A new method predicts human dance motion from egocentric video and music using EgoMusic Motion Network and a new dataset EgoAIST++.


<details>
  <summary>Details</summary>
Motivation: Estimating human dance motion from both egocentric video and music is largely unexplored, while the egocentric view obscures much of the body, and music incorporation requires alignment of movements with visual and musical inputs.

Method: EgoMusic Motion Network with a core Skeleton Mamba that explicitly captures the skeleton structure of the human body, drawing on diffusion models and Mamba.

Result: The method outperforms state-of-the-art approaches and generalizes effectively to real-world data.

Conclusion: The proposed EgoMusic Motion Network with a Skeleton Mamba outperforms state-of-the-art approaches and generalizes effectively to real-world data.

Abstract: Estimating human dance motion is a challenging task with various industrial
applications. Recently, many efforts have focused on predicting human dance
motion using either egocentric video or music as input. However, the task of
jointly estimating human motion from both egocentric video and music remains
largely unexplored. In this paper, we aim to develop a new method that predicts
human dance motion from both egocentric video and music. In practice, the
egocentric view often obscures much of the body, making accurate full-pose
estimation challenging. Additionally, incorporating music requires the
generated head and body movements to align well with both visual and musical
inputs. We first introduce EgoAIST++, a new large-scale dataset that combines
both egocentric views and music with more than 36 hours of dancing motion.
Drawing on the success of diffusion models and Mamba on modeling sequences, we
develop an EgoMusic Motion Network with a core Skeleton Mamba that explicitly
captures the skeleton structure of the human body. We illustrate that our
approach is theoretically supportive. Intensive experiments show that our
method clearly outperforms state-of-the-art approaches and generalizes
effectively to real-world data.

</details>


### [97] [Reasoning in Computer Vision: Taxonomy, Models, Tasks, and Methodologies](https://arxiv.org/abs/2508.10523)
*Ayushman Sarkar,Mohd Yamani Idna Idris,Zhenyu Yu*

Main category: cs.CV

TL;DR: 本文对视觉推理进行了分类，并对不同方法进行了分析，同时 критически 分析了现有评估协议的局限性，并确定了视觉推理中面临的关键开放性挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的综述通常孤立地解决这些方向，缺乏跨推理类型、方法和评估协议的统一分析和比较。

Method: 通过图模型、记忆网络、注意力机制和神经符号系统等架构，系统地检查了五种主要的视觉推理类型（关系、符号、时间、因果和常识）的实现。

Result: 对旨在评估功能正确性、结构一致性和因果有效性的评估协议进行了审查，并批判性地分析了它们在泛化性、可重复性和解释力方面的局限性。

Conclusion: 弥合感知和推理之间的差距对于构建透明、值得信赖和跨领域自适应的 AI 系统至关重要，特别是在自动驾驶和医疗诊断等关键领域。

Abstract: Visual reasoning is critical for a wide range of computer vision tasks that
go beyond surface-level object detection and classification. Despite notable
advances in relational, symbolic, temporal, causal, and commonsense reasoning,
existing surveys often address these directions in isolation, lacking a unified
analysis and comparison across reasoning types, methodologies, and evaluation
protocols. This survey aims to address this gap by categorizing visual
reasoning into five major types (relational, symbolic, temporal, causal, and
commonsense) and systematically examining their implementation through
architectures such as graph-based models, memory networks, attention
mechanisms, and neuro-symbolic systems. We review evaluation protocols designed
to assess functional correctness, structural consistency, and causal validity,
and critically analyze their limitations in terms of generalizability,
reproducibility, and explanatory power. Beyond evaluation, we identify key open
challenges in visual reasoning, including scalability to complex scenes, deeper
integration of symbolic and neural paradigms, the lack of comprehensive
benchmark datasets, and reasoning under weak supervision. Finally, we outline a
forward-looking research agenda for next-generation vision systems, emphasizing
that bridging perception and reasoning is essential for building transparent,
trustworthy, and cross-domain adaptive AI systems, particularly in critical
domains such as autonomous driving and medical diagnostics.

</details>


### [98] [Med-GLIP: Advancing Medical Language-Image Pre-training with Large-scale Grounded Dataset](https://arxiv.org/abs/2508.10528)
*Ziye Deng,Ruihan He,Jiaxiang Liu,Yuan Wang,Zijie Meng,Songtao Jiang,Yong Xie,Zuozhu Liu*

Main category: cs.CV

TL;DR: 构建了一个大型医学grounding数据集Med-GLIP-5M，并提出了一个模态感知grounding框架Med-GLIP，该框架在多个grounding基准测试中优于最先进的基线。


<details>
  <summary>Details</summary>
Motivation: 现有的研究受到有限的模态覆盖、粗粒度注释以及缺乏统一的、可推广的grounding框架的限制。为了解决这些挑战，我们构建了一个大型医学grounding数据集Med-GLIP-5M，该数据集包含跨七种成像模态的超过530万个区域级注释，涵盖了各种解剖结构和病理发现。

Method: 我们提出了Med-GLIP，这是一个在Med-GLIP-5M上训练的模态感知grounding框架。

Result: Med-GLIP在多个grounding基准测试中持续优于最先进的基线。将其空间输出集成到下游任务（包括医学VQA和报告生成）中，可带来显着的性能提升。

Conclusion: Med-GLIP在多个grounding基准测试中持续优于最先进的基线。将其空间输出集成到下游任务（包括医学VQA和报告生成）中，可带来显着的性能提升。

Abstract: Medical image grounding aims to align natural language phrases with specific
regions in medical images, serving as a foundational task for intelligent
diagnosis, visual question answering (VQA), and automated report generation
(MRG). However, existing research is constrained by limited modality coverage,
coarse-grained annotations, and the absence of a unified, generalizable
grounding framework. To address these challenges, we construct a large-scale
medical grounding dataset Med-GLIP-5M comprising over 5.3 million region-level
annotations across seven imaging modalities, covering diverse anatomical
structures and pathological findings. The dataset supports both segmentation
and grounding tasks with hierarchical region labels, ranging from organ-level
boundaries to fine-grained lesions. Based on this foundation, we propose
Med-GLIP, a modality-aware grounding framework trained on Med-GLIP-5M. Rather
than relying on explicitly designed expert modules, Med-GLIP implicitly
acquires hierarchical semantic understanding from diverse training data --
enabling it to recognize multi-granularity structures, such as distinguishing
lungs from pneumonia lesions. Extensive experiments demonstrate that Med-GLIP
consistently outperforms state-of-the-art baselines across multiple grounding
benchmarks. Furthermore, integrating its spatial outputs into downstream tasks,
including medical VQA and report generation, leads to substantial performance
gains. Our dataset will be released soon.

</details>


### [99] [GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images](https://arxiv.org/abs/2508.10542)
*Mengyu Ren,Yutong Li,Hua Li,Runmin Cong,Sam Kwong*

Main category: cs.CV

TL;DR: This paper introduces GCRPNet, a Mamba-based network for salient object detection in remote sensing images. It uses graph attention and adaptive scanning to improve performance, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods based on vision transformers (ViTs) and convolutional neural networks (CNNs) architectures aim to leverage both global and local features, but the difficulty in effectively integrating these heterogeneous features limits their overall performance. Significant variations in target scales and low contrast between targets and the background in optical remote sensing images (ORSIs) also pose challenges.

Method: We propose a graph-enhanced contextual and regional perception network (GCRPNet), which builds upon the Mamba architecture. It includes a visual state space (VSS) encoder, a difference-similarity guided hierarchical graph attention module (DS-HGAM), and the LEVSS block as the decoder, integrating an adaptive scanning strategy and multi-granularity collaborative attention enhancement module (MCAEM).

Result: The proposed model achieves state-of-the-art performance.

Conclusion: The proposed model achieves state-of-the-art performance, validating its effectiveness and superiority.

Abstract: Salient object detection (SOD) in optical remote sensing images (ORSIs) faces
numerous challenges, including significant variations in target scales and low
contrast between targets and the background. Existing methods based on vision
transformers (ViTs) and convolutional neural networks (CNNs) architectures aim
to leverage both global and local features, but the difficulty in effectively
integrating these heterogeneous features limits their overall performance. To
overcome these limitations, we propose a graph-enhanced contextual and regional
perception network (GCRPNet), which builds upon the Mamba architecture to
simultaneously capture long-range dependencies and enhance regional feature
representation. Specifically, we employ the visual state space (VSS) encoder to
extract multi-scale features. To further achieve deep guidance and enhancement
of these features, we first design a difference-similarity guided hierarchical
graph attention module (DS-HGAM). This module strengthens cross-layer
interaction capabilities between features of different scales while enhancing
the model's structural perception,allowing it to distinguish between foreground
and background more effectively. Then, we design the LEVSS block as the decoder
of GCRPNet. This module integrates our proposed adaptive scanning strategy and
multi-granularity collaborative attention enhancement module (MCAEM). It
performs adaptive patch scanning on feature maps processed via multi-scale
convolutions, thereby capturing rich local region information and enhancing
Mamba's local modeling capability. Extensive experimental results demonstrate
that the proposed model achieves state-of-the-art performance, validating its
effectiveness and superiority.

</details>


### [100] [PSScreen: Partially Supervised Multiple Retinal Disease Screening](https://arxiv.org/abs/2508.10549)
*Boyi Zheng,Qing Liu*

Main category: cs.CV

TL;DR: PSScreen是一种新的部分监督多视网膜疾病筛查模型，它通过利用文本指导、伪标签一致性和自蒸馏来提高领域泛化能力和解决标签缺失问题，从而在视网膜疾病检测方面取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 利用多个部分标记的数据集来训练用于多种视网膜疾病筛查的模型，减少了对完全注释数据集的依赖，但由于来自各个医疗站点的训练数据集之间的显着领域转移以及部分类别的标签缺失问题，仍然具有挑战性。

Method: PSScreen包含两个流，一个学习确定性特征，另一个通过不确定性注入学习概率特征。然后，我们利用文本指导将两种类型的特征解耦为病变特征，并通过特征提取对齐它们，以提高领域泛化能力。同时，我们利用两个流之间的伪标签一致性来解决标签缺失问题，并引入自蒸馏来传递关于已知类别的任务相关语义，从确定性流到概率流，以进一步提高检测性能。

Result: PSScreen在六种视网膜疾病和正常状态的检测性能上显著提高，并在同域和异域数据集上都取得了最先进的结果。

Conclusion: PSScreen在六种视网膜疾病和正常状态的检测性能上显著提高，并在同域和异域数据集上都取得了最先进的结果。

Abstract: Leveraging multiple partially labeled datasets to train a model for multiple
retinal disease screening reduces the reliance on fully annotated datasets, but
remains challenging due to significant domain shifts across training datasets
from various medical sites, and the label absent issue for partial classes. To
solve these challenges, we propose PSScreen, a novel Partially Supervised
multiple retinal disease Screening model. Our PSScreen consists of two streams
and one learns deterministic features and the other learns probabilistic
features via uncertainty injection. Then, we leverage the textual guidance to
decouple two types of features into disease-wise features and align them via
feature distillation to boost the domain generalization ability. Meanwhile, we
employ pseudo label consistency between two streams to address the label absent
issue and introduce a self-distillation to transfer task-relevant semantics
about known classes from the deterministic to the probabilistic stream to
further enhance the detection performances. Experiments show that our PSScreen
significantly enhances the detection performances on six retinal diseases and
the normal state averagely and achieves state-of-the-art results on both
in-domain and out-of-domain datasets. Codes are available at
https://github.com/boyiZheng99/PSScreen.

</details>


### [101] [AR Surgical Navigation With Surface Tracing: Comparing In-SitVisualization with Tool-Tracking Guidance for Neurosurgical Applications](https://arxiv.org/abs/2508.10554)
*Marc J. Fischer,Jeffrey Potts,Gabriel Urreola,Dax Jones,Paolo Palmisciano,E. Bradley Strong,Branden Cord,Andrew D. Hernandez,Julia D. Sharma,E. Brandon Strong*

Main category: cs.CV

TL;DR: This paper introduces a new AR surgical navigation method using HoloLens 2 and real-time tool tracking, which improves accuracy and user experience in simulated catheter placements.


<details>
  <summary>Details</summary>
Motivation: Known issues with AR depth perception and occlusion handling limit the use of AR surgical navigation systems.

Method: A novel methodology for utilizing AR guidance to register anatomical targets and provide real-time instrument navigation using placement of simulated external ventricular drain catheters on a phantom model.

Result: Tool-tracking guidance improved performance metrics across all accuracy measures.

Conclusion: Tool-tracking guidance improves performance and is preferred by users.

Abstract: Augmented Reality (AR) surgical navigation systems are emerging as the next
generation of intraoperative surgical guidance, promising to overcome
limitations of traditional navigation systems. However, known issues with AR
depth perception due to vergence-accommodation conflict and occlusion handling
limitations of the currently commercially available display technology present
acute challenges in surgical settings where precision is paramount. This study
presents a novel methodology for utilizing AR guidance to register anatomical
targets and provide real-time instrument navigation using placement of
simulated external ventricular drain catheters on a phantom model as the
clinical scenario. The system registers target positions to the patient through
a novel surface tracing method and uses real-time infrared tool tracking to aid
in catheter placement, relying only on the onboard sensors of the Microsoft
HoloLens 2. A group of intended users performed the procedure of simulated
insertions under two AR guidance conditions: static in-situ visualization,
where planned trajectories are overlaid directly onto the patient anatomy, and
real-time tool-tracking guidance, where live feedback of the catheter's pose is
provided relative to the plan. Following the insertion tests, computed
tomography scans of the phantom models were acquired, allowing for evaluation
of insertion accuracy, target deviation, angular error, and depth precision.
System Usability Scale surveys assessed user experience and cognitive workload.
Tool-tracking guidance improved performance metrics across all accuracy
measures and was preferred by users in subjective evaluations. A free copy of
this paper and all supplemental materials are available at
https://bit.ly/45l89Hq.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [102] [A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions](https://arxiv.org/abs/2508.10047)
*Ziyang Xiao,Jingrong Xie,Lilin Xu,Shisi Guan,Jingyan Zhu,Xiongwei Han,Xiaojin Fu,WingYin Yu,Han Wu,Wei Shi,Qingcan Kang,Jiahui Duan,Tao Zhong,Mingxuan Yuan,Jia Zeng,Yuan Wang,Gang Chen,Dongxiang Zhang*

Main category: cs.AI

TL;DR: This survey reviews recent advancements in using LLMs to automate mathematical modeling, identifies issues with benchmark datasets, and offers a cleaned leaderboard and online resources.


<details>
  <summary>Details</summary>
Motivation: Optimization modeling has been widely employed for optimal decision-making across various sectors, but it requires substantial expertise. With the advent of large language models (LLMs), new opportunities have emerged to automate the procedure of mathematical modeling.

Method: Presents a comprehensive and timely review of recent advancements that cover the entire technical stack, including data synthesis and fine-tuning for the base model, inference frameworks, benchmark datasets, and performance evaluation. Conducted an in-depth analysis on the quality of benchmark datasets, cleaned the datasets and constructed a new leaderboard with fair performance evaluation. Build an online portal that integrates resources of cleaned datasets, code and paper repository to benefit the community.

Result: A new leaderboard with fair performance evaluation in terms of base LLM model and datasets. An online portal that integrates resources of cleaned datasets, code and paper repository.

Conclusion: Identifies limitations in current methodologies and outlines future research opportunities.

Abstract: By virtue of its great utility in solving real-world problems, optimization
modeling has been widely employed for optimal decision-making across various
sectors, but it requires substantial expertise from operations research
professionals. With the advent of large language models (LLMs), new
opportunities have emerged to automate the procedure of mathematical modeling.
This survey presents a comprehensive and timely review of recent advancements
that cover the entire technical stack, including data synthesis and fine-tuning
for the base model, inference frameworks, benchmark datasets, and performance
evaluation. In addition, we conducted an in-depth analysis on the quality of
benchmark datasets, which was found to have a surprisingly high error rate. We
cleaned the datasets and constructed a new leaderboard with fair performance
evaluation in terms of base LLM model and datasets. We also build an online
portal that integrates resources of cleaned datasets, code and paper repository
to benefit the community. Finally, we identify limitations in current
methodologies and outline future research opportunities.

</details>


### [103] [Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development](https://arxiv.org/abs/2508.10108)
*Sattvik Sahai,Prasoon Goyal,Michael Johnston,Anna Gottardi,Yao Lu,Lucy Hu,Luke Dai,Shaohua Liu,Samyuth Sagi,Hangjie Shi,Desheng Zhang,Lavina Vaz,Leslie Ball,Maureen Murray,Rahul Gupta,Shankar Ananthakrishna*

Main category: cs.AI

TL;DR: Amazon Nova AI 挑战赛旨在推动安全 AI 的进步，重点关注自动化红队和安全对齐方法。


<details>
  <summary>Details</summary>
Motivation: 确保 AI 系统在软件开发中的安全性仍然面临重大挑战。

Method: 红队通过与竞争性 AI 编码助手进行多轮对话来测试他们的安全对齐，从而评估自动红队和安全对齐方法。

Result: 开发了最先进的技术，引入了基于推理的安全对齐、强大的模型护栏、多轮越狱和大型语言模型 (LLM) 的高效探测等新方法。

Conclusion: 大学团队和 Amazon Nova AI Challenge 团队在解决软件开发 AI 的安全挑战方面取得了进展，突出了这项旨在提高 AI 安全性的合作努力。

Abstract: AI systems for software development are rapidly gaining prominence, yet
significant challenges remain in ensuring their safety. To address this, Amazon
launched the Trusted AI track of the Amazon Nova AI Challenge, a global
competition among 10 university teams to drive advances in secure AI. In the
challenge, five teams focus on developing automated red teaming bots, while the
other five create safe AI assistants. This challenge provides teams with a
unique platform to evaluate automated red-teaming and safety alignment methods
through head-to-head adversarial tournaments where red teams have multi-turn
conversations with the competing AI coding assistants to test their safety
alignment. Along with this, the challenge provides teams with a feed of high
quality annotated data to fuel iterative improvement. Throughout the challenge,
teams developed state-of-the-art techniques, introducing novel approaches in
reasoning-based safety alignment, robust model guardrails, multi-turn
jail-breaking, and efficient probing of large language models (LLMs). To
support these efforts, the Amazon Nova AI Challenge team made substantial
scientific and engineering investments, including building a custom baseline
coding specialist model for the challenge from scratch, developing a tournament
orchestration service, and creating an evaluation harness. This paper outlines
the advancements made by university teams and the Amazon Nova AI Challenge team
in addressing the safety challenges of AI for software development,
highlighting this collaborative effort to raise the bar for AI safety.

</details>


### [104] [MCP-Orchestrated Multi-Agent System for Automated Disinformation Detection](https://arxiv.org/abs/2508.10143)
*Alexandru-Andrei Avram,Adrian Groza,Alexandru Lecu*

Main category: cs.AI

TL;DR: This paper presents a multi-agent system that uses relation extraction to detect disinformation in news articles, focusing on titles and short text snippets. The system combines four agents and achieves 95.3% accuracy with an F1 score of 0.964.


<details>
  <summary>Details</summary>
Motivation: The large spread of disinformation across digital platforms creates significant challenges to information integrity.

Method: Agentic AI system combines four agents: (i) a machine learning agent (logistic regression), (ii) a Wikipedia knowledge check agent (which relies on named entity recognition), (iii) a coherence detection agent (using LLM prompt engineering), and (iv) a web-scraped data analyzer that extracts relational triplets for fact checking. The system is orchestrated via the Model Context Protocol (MCP), offering shared context and live learning across components.

Result: multi-agent ensemble achieves 95.3% accuracy with an F1 score of 0.964, significantly outperforming individual agents and traditional approaches

Conclusion: The multi-agent ensemble achieves 95.3% accuracy with an F1 score of 0.964, significantly outperforming individual agents and traditional approaches. The weighted aggregation method, mathematically derived from individual agent misclassification rates, proves superior to algorithmic threshold optimization. The modular architecture makes the system easily scalable, while also maintaining details of the decision processes.

Abstract: The large spread of disinformation across digital platforms creates
significant challenges to information integrity. This paper presents a
multi-agent system that uses relation extraction to detect disinformation in
news articles, focusing on titles and short text snippets. The proposed Agentic
AI system combines four agents: (i) a machine learning agent (logistic
regression), (ii) a Wikipedia knowledge check agent (which relies on named
entity recognition), (iii) a coherence detection agent (using LLM prompt
engineering), and (iv) a web-scraped data analyzer that extracts relational
triplets for fact checking. The system is orchestrated via the Model Context
Protocol (MCP), offering shared context and live learning across components.
Results demonstrate that the multi-agent ensemble achieves 95.3% accuracy with
an F1 score of 0.964, significantly outperforming individual agents and
traditional approaches. The weighted aggregation method, mathematically derived
from individual agent misclassification rates, proves superior to algorithmic
threshold optimization. The modular architecture makes the system easily
scalable, while also maintaining details of the decision processes.

</details>


### [105] [Agentic AI Frameworks: Architectures, Protocols, and Design Challenges](https://arxiv.org/abs/2508.10146)
*Hana Derouiche,Zaki Brahmi,Haithem Mazeni*

Main category: cs.AI

TL;DR: systematic review and comparative analysis of leading Agentic AI frameworks


<details>
  <summary>Details</summary>
Motivation: The emergence of Large Language Models (LLMs) has ushered in a transformative paradigm in artificial intelligence, Agentic AI, where intelligent agents exhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent coordination.

Method: systematic review and comparative analysis of leading Agentic AI frameworks

Result: establish a foundational taxonomy for Agentic AI systems but also propose future research directions to enhance scalability, robustness, and interoperability

Conclusion: This work serves as a comprehensive reference for researchers and practitioners working to advance the next generation of autonomous AI systems.

Abstract: The emergence of Large Language Models (LLMs) has ushered in a transformative
paradigm in artificial intelligence, Agentic AI, where intelligent agents
exhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent
coordination. This paper provides a systematic review and comparative analysis
of leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen,
Semantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural
principles, communication mechanisms, memory management, safety guardrails, and
alignment with service-oriented computing paradigms. Furthermore, we identify
key limitations, emerging trends, and open challenges in the field. To address
the issue of agent communication, we conduct an in-depth analysis of protocols
such as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network
Protocol (ANP), and Agora. Our findings not only establish a foundational
taxonomy for Agentic AI systems but also propose future research directions to
enhance scalability, robustness, and interoperability. This work serves as a
comprehensive reference for researchers and practitioners working to advance
the next generation of autonomous AI systems.

</details>


### [106] [Improving and Evaluating Open Deep Research Agents](https://arxiv.org/abs/2508.10152)
*Doaa Allabadi,Kyle Bradbury,Jordan M. Malof*

Main category: cs.AI

TL;DR: 本文提出了BC-Small基准测试，并改进了ODR模型，使其在BC-Small上取得了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 评估现有研究代理在BrowseComp基准测试上的性能，并为学术实验室提供一个计算量更小的基准测试。

Method: 通过对ODR进行三项改进，得到ODR+模型。

Result: 所有三个系统在BC-Small测试集上的准确率均为0%。对ODR的三项改进使其在BC-Small上达到了10%的成功率。

Conclusion: ODR+模型在BC-Small上实现了10%的成功率，优于其他系统。

Abstract: We focus here on Deep Research Agents (DRAs), which are systems that can take
a natural language prompt from a user, and then autonomously search for, and
utilize, internet-based content to address the prompt. Recent DRAs have
demonstrated impressive capabilities on public benchmarks however, recent
research largely involves proprietary closed-source systems. At the time of
this work, we only found one open-source DRA, termed Open Deep Research (ODR).
In this work we adapt the challenging recent BrowseComp benchmark to compare
ODR to existing proprietary systems. We propose BrowseComp-Small (BC-Small),
comprising a subset of BrowseComp, as a more computationally-tractable DRA
benchmark for academic labs. We benchmark ODR and two other proprietary systems
on BC-Small: one system from Anthropic and one system from Google. We find that
all three systems achieve 0% accuracy on the test set of 60 questions. We
introduce three strategic improvements to ODR, resulting in the ODR+ model,
which achieves a state-of-the-art 10% success rate on BC-Small among both
closed-source and open-source systems. We report ablation studies indicating
that all three of our improvements contributed to the success of ODR+.

</details>


### [107] [Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization](https://arxiv.org/abs/2508.10164)
*Bin Hong,Jiayu Liu,Zhenya Huang,Kai Zhang,Mengdi Zhang*

Main category: cs.AI

TL;DR: 这篇论文提出了一种名为 LCPO 的方法，可以在不牺牲推理性能的情况下，显著减少大型推理模型的输出长度，从而提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRM）在复杂任务上表现出强大的性能，但其冗长的输出增加了计算成本，并可能导致过度思考。目前用于有效推理的方法通常会降低推理质量或需要大量资源。因此，需要研究有效的方法来减少 LRM 的生成长度。

Method: 该论文通过分析生成路径分布和使用难度估计过滤生成的轨迹来研究减少 LRM 生成长度的有效方法。此外，该研究还分析了基于 Bradley-Terry 损失框架下各种偏好优化方法的目标收敛行为。

Result: 实验结果表明，所提出的长度控制偏好优化（LCPO）方法可以有效地学习长度偏好，并且在多个基准测试中将平均输出长度减少了 50% 以上，同时保持了推理性能。

Conclusion: 这篇论文提出了一种名为长度控制偏好优化（LCPO）的方法，该方法可以在保持推理性能的同时，显著减少大型推理模型（LRM）的输出长度。实验表明，该方法在多个基准测试中将平均输出长度减少了 50% 以上。

Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated strong
performance on complex tasks through long Chain-of-Thought (CoT) reasoning.
However, their lengthy outputs increase computational costs and may lead to
overthinking, raising challenges in balancing reasoning effectiveness and
efficiency. Current methods for efficient reasoning often compromise reasoning
quality or require extensive resources. This paper investigates efficient
methods to reduce the generation length of LRMs. We analyze generation path
distributions and filter generated trajectories through difficulty estimation.
Subsequently, we analyze the convergence behaviors of the objectives of various
preference optimization methods under a Bradley-Terry loss based framework.
Based on the analysis, we propose Length Controlled Preference Optimization
(LCPO) that directly balances the implicit reward related to NLL loss. LCPO can
effectively learn length preference with limited data and training. Extensive
experiments demonstrate that our approach significantly reduces the average
output length by over 50\% across multiple benchmarks while maintaining the
reasoning performance. Our work highlights the potential for computationally
efficient approaches in guiding LRMs toward efficient reasoning.

</details>


### [108] [KompeteAI: Accelerated Autonomous Multi-Agent System for End-to-End Pipeline Generation for Machine Learning Problems](https://arxiv.org/abs/2508.10177)
*Stepan Kulibaba,Artem Dzhalilov,Roman Pakhomov,Oleg Svidchenko,Alexander Gasnikov,Aleksei Shpilman*

Main category: cs.AI

TL;DR: KompeteAI, a new AutoML framework, overcomes limitations of existing systems by using dynamic solution space exploration, RAG, and an accelerated debugging method, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Recent LLM-based AutoML systems face significant limitations such as constrained exploration strategies and a severe execution bottleneck.

Method: KompeteAI, a novel AutoML framework with dynamic solution space exploration, introduces a merging stage that composes top candidates and integrates Retrieval-Augmented Generation (RAG). It also addresses the execution bottleneck via a predictive scoring model and an accelerated debugging method.

Result: Accelerates pipeline evaluation 6.9 times and outperforms leading methods on AutoML benchmarks.

Conclusion: KompeteAI outperforms leading methods by an average of 3% on MLE-Bench and achieves state-of-the-art results on Kompete-bench.

Abstract: Recent Large Language Model (LLM)-based AutoML systems demonstrate impressive
capabilities but face significant limitations such as constrained exploration
strategies and a severe execution bottleneck. Exploration is hindered by
one-shot methods lacking diversity and Monte Carlo Tree Search (MCTS)
approaches that fail to recombine strong partial solutions. The execution
bottleneck arises from lengthy code validation cycles that stifle iterative
refinement. To overcome these challenges, we introduce KompeteAI, a novel
AutoML framework with dynamic solution space exploration. Unlike previous MCTS
methods that treat ideas in isolation, KompeteAI introduces a merging stage
that composes top candidates. We further expand the hypothesis space by
integrating Retrieval-Augmented Generation (RAG), sourcing ideas from Kaggle
notebooks and arXiv papers to incorporate real-world strategies. KompeteAI also
addresses the execution bottleneck via a predictive scoring model and an
accelerated debugging method, assessing solution potential using early stage
metrics to avoid costly full-code execution. This approach accelerates pipeline
evaluation 6.9 times. KompeteAI outperforms leading methods (e.g., RD-agent,
AIDE, and Ml-Master) by an average of 3\% on the primary AutoML benchmark,
MLE-Bench. Additionally, we propose Kompete-bench to address limitations in
MLE-Bench, where KompeteAI also achieves state-of-the-art results

</details>


### [109] [Extending the Entropic Potential of Events for Uncertainty Quantification and Decision-Making in Artificial Intelligence](https://arxiv.org/abs/2508.10241)
*Mark Zilberman*

Main category: cs.AI

TL;DR: 熵势框架通过弥合热力学、信息论和机器学习的原理，为管理人工智能中的不确定性提供了一种通用方法。


<details>
  <summary>Details</summary>
Motivation: 演示事件的熵势概念如何增强人工智能 (AI) 中的不确定性量化、决策和可解释性。

Method: 通过引入以事件为中心的度量来调整人工智能框架，该度量捕捉了行动、观察或其他离散事件如何影响未来时间范围的不确定性。

Result: 探索了在策略评估、内在奖励设计、可解释的 AI 和异常检测中的应用，突出了该指标统一和加强智能系统中不确定性建模的潜力。

Conclusion: 熵势框架为管理人工智能中的不确定性提供了一种理论上可靠、可解释且通用的方法，弥合了热力学、信息论和机器学习的原理。

Abstract: This work demonstrates how the concept of the entropic potential of events --
a parameter quantifying the influence of discrete events on the expected future
entropy of a system -- can enhance uncertainty quantification, decision-making,
and interpretability in artificial intelligence (AI). Building on its original
formulation in physics, the framework is adapted for AI by introducing an
event-centric measure that captures how actions, observations, or other
discrete occurrences impact uncertainty at future time horizons. Both the
original and AI-adjusted definitions of entropic potential are formalized, with
the latter emphasizing conditional expectations to account for counterfactual
scenarios. Applications are explored in policy evaluation, intrinsic reward
design, explainable AI, and anomaly detection, highlighting the metric's
potential to unify and strengthen uncertainty modeling in intelligent systems.
Conceptual examples illustrate its use in reinforcement learning, Bayesian
inference, and anomaly detection, while practical considerations for
computation in complex AI models are discussed. The entropic potential
framework offers a theoretically grounded, interpretable, and versatile
approach to managing uncertainty in AI, bridging principles from
thermodynamics, information theory, and machine learning.

</details>


### [110] [Why Cannot Large Language Models Ever Make True Correct Reasoning?](https://arxiv.org/abs/2508.10265)
*Jingde Cheng*

Main category: cs.AI

TL;DR: LLMs' understanding and reasoning abilities are illusions due to their limitations.


<details>
  <summary>Details</summary>
Motivation: Many AI experts and non-professionals are trumpeting the 'understanding ability' and 'reasoning ability' of LLMs, but the author believes these are illusions.

Method: The paper explains the limitations of LLMs' working principle.

Result: The author considers that the so-called 'understanding ability' and 'reasoning ability' of LLMs are just illusions.

Conclusion: LLMs can never have the ability of true correct reasoning due to the essential limitations of their working principle.

Abstract: Recently, with the application progress of AIGC tools based on large language
models (LLMs), led by ChatGPT, many AI experts and more non-professionals are
trumpeting the "understanding ability" and "reasoning ability" of the LLMs. The
present author considers that the so-called "understanding ability" and
"reasoning ability" of LLMs are just illusions of those people who with vague
concepts. In fact, the LLMs can never have the true understanding ability and
true reasoning ability. This paper intents to explain that, because the
essential limitations of their working principle, the LLMs can never have the
ability of true correct reasoning.

</details>


### [111] [Promoting Efficient Reasoning with Verifiable Stepwise Reward](https://arxiv.org/abs/2508.10293)
*Chuhuai Yue,Chengqi Dong,Yinan Gao,Hang He,Jiajun Chai,Guojun Yin,Wei Lin*

Main category: cs.AI

TL;DR: 提出了一种新的奖励机制，以减少大型推理模型中的过度思考，从而在效率和准确性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型 (LRM) 在复杂的推理任务中取得了显著进展，但 LRM 经常受到过度思考的困扰，在简单的问题上花费过多的计算，降低了效率。现有的高效推理方法通常需要准确的任务评估来预设令牌预算或选择推理模式，这限制了它们的灵活性和可靠性。

Method: 提出了一种基于规则的可验证的逐步奖励机制 (VSRM)，该机制根据推理轨迹中中间状态的性能来分配奖励。

Result: 通过将 VSRM 与 PPO 和 Reinforce++ 相结合，在 AIME24 和 AIME25 等标准数学推理基准上进行了广泛的实验。结果表明，该方法在保持原始推理性能的同时，显着减少了输出长度。

Conclusion: 该方法通过抑制无效步骤并鼓励有效推理，从根本上缓解了过度思考问题，在效率和准确性之间取得了最佳平衡。

Abstract: Large reasoning models (LRMs) have recently achieved significant progress in
complex reasoning tasks, aided by reinforcement learning with verifiable
rewards. However, LRMs often suffer from overthinking, expending excessive
computation on simple problems and reducing efficiency. Existing efficient
reasoning methods typically require accurate task assessment to preset token
budgets or select reasoning modes, which limits their flexibility and
reliability. In this work, we revisit the essence of overthinking and identify
that encouraging effective steps while penalizing ineffective ones is key to
its solution. To this end, we propose a novel rule-based verifiable stepwise
reward mechanism (VSRM), which assigns rewards based on the performance of
intermediate states in the reasoning trajectory. This approach is intuitive and
naturally fits the step-by-step nature of reasoning tasks. We conduct extensive
experiments on standard mathematical reasoning benchmarks, including AIME24 and
AIME25, by integrating VSRM with PPO and Reinforce++. Results show that our
method achieves substantial output length reduction while maintaining original
reasoning performance, striking an optimal balance between efficiency and
accuracy. Further analysis of overthinking frequency and pass@k score before
and after training demonstrates that our approach in deed effectively
suppresses ineffective steps and encourages effective reasoning, fundamentally
alleviating the overthinking problem. All code will be released upon
acceptance.

</details>


### [112] [A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering](https://arxiv.org/abs/2508.10337)
*Chenliang Zhang,Lin Wang,Yuanyuan Lu,Yusheng Qi,Kexin Wang,Peixu Hou,Wenshi Chen*

Main category: cs.AI

TL;DR: Dianping-Trust-Safety团队构建了一个多模态多轮问答系统，在META CRAG-MM挑战赛的任务1中获得第一名，任务3中获得第三名。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够进行多模态多轮问答的综合检索增强生成系统。

Method: 基于视觉大型语言模型，通过GPT-4.1的知识提炼进行监督微调，并结合网络搜索API。

Result: 在任务1中以52.38%的显著优势获得第一名，在任务3中获得第三名。

Conclusion: 集成了课程学习和强化学习的训练流程非常有效。

Abstract: This paper describes the solutions of the Dianping-Trust-Safety team for the
META CRAG-MM challenge. The challenge requires building a comprehensive
retrieval-augmented generation system capable for multi-modal multi-turn
question answering. The competition consists of three tasks: (1) answering
questions using structured data retrieved from an image-based mock knowledge
graph, (2) synthesizing information from both knowledge graphs and web search
results, and (3) handling multi-turn conversations that require context
understanding and information aggregation from multiple sources. For Task 1,
our solution is based on the vision large language model, enhanced by
supervised fine-tuning with knowledge distilled from GPT-4.1. We further
applied curriculum learning strategies to guide reinforcement learning,
resulting in improved answer accuracy and reduced hallucination. For Task 2 and
Task 3, we additionally leveraged web search APIs to incorporate external
knowledge, enabling the system to better handle complex queries and multi-turn
conversations. Our approach achieved 1st place in Task 1 with a significant
lead of 52.38\%, and 3rd place in Task 3, demonstrating the effectiveness of
the integration of curriculum learning with reinforcement learning in our
training pipeline.

</details>


### [113] [Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach](https://arxiv.org/abs/2508.10340)
*Chak Lam Shek,Guangyao Shi,Pratap Tokekar*

Main category: cs.AI

TL;DR: 提出 HATRPO-W 和 HATRPO-G 两种方法，用于在异构多智能体强化学习中更有效地分配 KL 散度阈值，实验表明这两种方法提高了性能并实现了更快的收敛。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习 (MARL) 需要交互智能体之间协调和稳定的策略更新。异构智能体信任区域策略优化 (HATRPO) 使用 Kullback-Leibler (KL) 散度来执行每个智能体的信任区域约束，以稳定训练。但是，为每个智能体分配相同的 KL 阈值可能会导致缓慢且局部最优的更新，尤其是在异构设置中。

Method: 提出了两种用于在智能体之间分配 KL 散度阈值的方法：HATRPO-W，一种基于 Karush-Kuhn-Tucker (KKT) 的方法，可在全局 KL 约束下优化阈值分配；以及 HATRPO-G，一种贪婪算法，可根据改进与散度比率对智能体进行优先级排序。

Result: 实验结果表明，我们的方法显着提高了 HATRPO 的性能，在不同的 MARL 基准测试中实现了更快的收敛速度和更高的最终奖励。HATRPO-W 和 HATRPO-G 在最终性能上取得了相当的提升，均超过 22.5%。HATRPO-W 也表现出更稳定的学习动态，这反映在其较低的方差中。

Conclusion: HATRPO-W 和 HATRPO-G 显著提高了 HATRPO 的性能，在不同的 MARL 基准测试中实现了更快的收敛速度和更高的最终奖励。特别是，HATRPO-W 和 HATRPO-G 在最终性能上取得了相当的提升，均超过 22.5%。值得注意的是，HATRPO-W 也表现出更稳定的学习动态，这反映在其较低的方差中。

Abstract: Multi-agent reinforcement learning (MARL) requires coordinated and stable
policy updates among interacting agents. Heterogeneous-Agent Trust Region
Policy Optimization (HATRPO) enforces per-agent trust region constraints using
Kullback-Leibler (KL) divergence to stabilize training. However, assigning each
agent the same KL threshold can lead to slow and locally optimal updates,
especially in heterogeneous settings. To address this limitation, we propose
two approaches for allocating the KL divergence threshold across agents:
HATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes
threshold assignment under global KL constraints, and HATRPO-G, a greedy
algorithm that prioritizes agents based on improvement-to-divergence ratio. By
connecting sequential policy optimization with constrained threshold
scheduling, our approach enables more flexible and effective learning in
heterogeneous-agent settings. Experimental results demonstrate that our methods
significantly boost the performance of HATRPO, achieving faster convergence and
higher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and
HATRPO-G achieve comparable improvements in final performance, each exceeding
22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as
reflected by its lower variance.

</details>


### [114] [STEP: Stepwise Curriculum Learning for Context-Knowledge Fusion in Conversational Recommendation](https://arxiv.org/abs/2508.10669)
*Zhenye Yang,Jinpeng Chen,Huan Li,Xiongnan Jin,Xuanyang Li,Junwei Zhang,Hongbo Gao,Kaimin Wei,Senzhang Wang*

Main category: cs.AI

TL;DR: STEP: a conversational recommender centered on pre-trained language models that combines curriculum-guided context-knowledge fusion with lightweight task-specific prompt tuning.


<details>
  <summary>Details</summary>
Motivation: existing CRS faces challenges in capturing the deep semantics of user preferences and dialogue context. In particular, the efficient integration of external knowledge graph (KG) information into dialogue generation and recommendation remains a pressing issue. Traditional approaches typically combine KG information directly with dialogue content, which often struggles with complex semantic relationships, resulting in recommendations that may not align with user expectations.

Method: a conversational recommender centered on pre-trained language models that combines curriculum-guided context-knowledge fusion with lightweight task-specific prompt tuning. At its heart, an F-Former progressively aligns the dialogue context with knowledge-graph entities through a three-stage curriculum, thus resolving fine-grained semantic mismatches. The fused representation is then injected into the frozen language model via two minimal yet adaptive prefix prompts: a conversation prefix that steers response generation toward user intent and a recommendation prefix that biases item ranking toward knowledge-consistent candidates.

Result: STEP outperforms mainstream methods in the precision of recommendation and dialogue quality in two public datasets.

Conclusion: STEP outperforms mainstream methods in the precision of recommendation and dialogue quality in two public datasets.

Abstract: Conversational recommender systems (CRSs) aim to proactively capture user
preferences through natural language dialogue and recommend high-quality items.
To achieve this, CRS gathers user preferences via a dialog module and builds
user profiles through a recommendation module to generate appropriate
recommendations. However, existing CRS faces challenges in capturing the deep
semantics of user preferences and dialogue context. In particular, the
efficient integration of external knowledge graph (KG) information into
dialogue generation and recommendation remains a pressing issue. Traditional
approaches typically combine KG information directly with dialogue content,
which often struggles with complex semantic relationships, resulting in
recommendations that may not align with user expectations.
  To address these challenges, we introduce STEP, a conversational recommender
centered on pre-trained language models that combines curriculum-guided
context-knowledge fusion with lightweight task-specific prompt tuning. At its
heart, an F-Former progressively aligns the dialogue context with
knowledge-graph entities through a three-stage curriculum, thus resolving
fine-grained semantic mismatches. The fused representation is then injected
into the frozen language model via two minimal yet adaptive prefix prompts: a
conversation prefix that steers response generation toward user intent and a
recommendation prefix that biases item ranking toward knowledge-consistent
candidates. This dual-prompt scheme allows the model to share cross-task
semantics while respecting the distinct objectives of dialogue and
recommendation. Experimental results show that STEP outperforms mainstream
methods in the precision of recommendation and dialogue quality in two public
datasets.

</details>


### [115] [What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles](https://arxiv.org/abs/2508.10358)
*Mengtao Zhou,Sifan Wu,Huan Zhang,Qi Sima,Bang Liu*

Main category: cs.AI

TL;DR: This paper introduces a new benchmark and agent to evaluate LLMs' imaginative reasoning, revealing limitations compared to humans.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to capture the dynamic, exploratory nature of imaginative reasoning in information-sparse environments.

Method: A comprehensive research framework based on the "Turtle Soup" game, integrating a benchmark, an agent, and an evaluation protocol.

Result: TurtleSoup-Bench, the first large-scale, bilingual, interactive benchmark for imaginative reasoning, comprising 800 turtle soup puzzles. Mosaic-Agent, a novel agent designed to assess LLMs' performance. A multi-dimensional protocol measuring logical consistency, detail completion, and conclusion alignment.

Conclusion: Leading LLMs show capability limits, failure patterns, and a performance gap compared to humans in imaginative reasoning.

Abstract: We investigate the capacity of Large Language Models (LLMs) for imaginative
reasoning--the proactive construction, testing, and revision of hypotheses in
information-sparse environments. Existing benchmarks, often static or focused
on social deduction, fail to capture the dynamic, exploratory nature of this
reasoning process. To address this gap, we introduce a comprehensive research
framework based on the classic "Turtle Soup" game, integrating a benchmark, an
agent, and an evaluation protocol. We present TurtleSoup-Bench, the first
large-scale, bilingual, interactive benchmark for imaginative reasoning,
comprising 800 turtle soup puzzles sourced from both the Internet and expert
authors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs'
performance in this setting. To evaluate reasoning quality, we develop a
multi-dimensional protocol measuring logical consistency, detail completion,
and conclusion alignment. Experiments with leading LLMs reveal clear capability
limits, common failure patterns, and a significant performance gap compared to
humans. Our work offers new insights into LLMs' imaginative reasoning and
establishes a foundation for future research on exploratory agent behavior.

</details>


### [116] [LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval](https://arxiv.org/abs/2508.10391)
*Yaoze Zhang,Rong Wu,Pinlong Cai,Xiaoman Wang,Guohang Yan,Song Mao,Ding Wang,Botian Shi*

Main category: cs.AI

TL;DR: LeanRAG 通过结合知识聚合和检索策略来克服现有 RAG 方法的局限性，从而在问答任务中实现更高的质量和更低的冗余。


<details>
  <summary>Details</summary>
Motivation: 知识图谱的 RAG 方法已经发展为分层结构，将知识组织成多层次的摘要。然而，这些方法仍然面临两个关键的、未解决的挑战：高级概念摘要作为断开连接的“语义岛”而存在，缺乏跨社区推理所需的显式关系；并且检索过程本身仍然在结构上没有意识到，经常退化为低效的平面搜索，无法利用图的丰富拓扑。

Method: LeanRAG首先采用一种新的语义聚合算法，该算法形成实体集群并在聚合级别摘要之间构建新的显式关系，从而创建一个完全可导航的语义网络。然后，一种自下而上的结构引导检索策略将查询锚定到最相关的细粒度实体，然后系统地遍历图的语义路径以收集简洁但上下文全面的证据集。

Result: LeanRAG 可以减轻与图上的路径检索相关的巨大开销，并最大限度地减少冗余信息检索。在具有不同领域的四个具有挑战性的 QA 基准上的大量实验表明，LeanRAG 在响应质量方面显着优于现有方法，同时减少了 46% 的检索冗余。

Conclusion: LeanRAG在响应质量方面显著优于现有方法，同时减少了 46% 的检索冗余。

Abstract: Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large
Language Models by leveraging external knowledge, whereas the effectiveness is
often compromised by the retrieval of contextually flawed or incomplete
information. To address this, knowledge graph-based RAG methods have evolved
towards hierarchical structures, organizing knowledge into multi-level
summaries. However, these approaches still suffer from two critical,
unaddressed challenges: high-level conceptual summaries exist as disconnected
``semantic islands'', lacking the explicit relations needed for cross-community
reasoning; and the retrieval process itself remains structurally unaware, often
degenerating into an inefficient flat search that fails to exploit the graph's
rich topology. To overcome these limitations, we introduce LeanRAG, a framework
that features a deeply collaborative design combining knowledge aggregation and
retrieval strategies. LeanRAG first employs a novel semantic aggregation
algorithm that forms entity clusters and constructs new explicit relations
among aggregation-level summaries, creating a fully navigable semantic network.
Then, a bottom-up, structure-guided retrieval strategy anchors queries to the
most relevant fine-grained entities and then systematically traverses the
graph's semantic pathways to gather concise yet contextually comprehensive
evidence sets. The LeanRAG can mitigate the substantial overhead associated
with path retrieval on graphs and minimizes redundant information retrieval.
Extensive experiments on four challenging QA benchmarks with different domains
demonstrate that LeanRAG significantly outperforming existing methods in
response quality while reducing 46\% retrieval redundancy. Code is available
at: https://github.com/RaZzzyz/LeanRAG

</details>


### [117] [HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation](https://arxiv.org/abs/2508.10425)
*Yan Ting Chok,Soyon Park,Seungheun Baek,Hajung Kim,Junhyun Lee,Jaewoo Kang*

Main category: cs.AI

TL;DR: HiRef结合了分层本体和细化的共现模式，以实现稳健的药物推荐。


<details>
  <summary>Details</summary>
Motivation: 解决由于存在罕见医学实体和不完整的记录而导致的真实世界EHR数据带来的重大挑战。

Method: 分层本体和网络细化

Result: 该模型在EHR基准测试上取得了强大的性能，并在模拟的未见代码设置下保持了较高的准确性。

Conclusion: HiRef在EHR基准测试（MIMIC-III和MIMIC-IV）上取得了强大的性能，并在模拟的未见代码设置下保持了较高的准确性。

Abstract: Medication recommendation is a crucial task for assisting physicians in
making timely decisions from longitudinal patient medical records. However,
real-world EHR data present significant challenges due to the presence of
rarely observed medical entities and incomplete records that may not fully
capture the clinical ground truth. While data-driven models trained on
longitudinal Electronic Health Records often achieve strong empirical
performance, they struggle to generalize under missing or novel conditions,
largely due to their reliance on observed co-occurrence patterns. To address
these issues, we propose Hierarchical Ontology and Network Refinement for
Robust Medication Recommendation (HiRef), a unified framework that combines two
complementary structures: (i) the hierarchical semantics encoded in curated
medical ontologies, and (ii) refined co-occurrence patterns derived from
real-world EHRs. We embed ontology entities in hyperbolic space, which
naturally captures tree-like relationships and enables knowledge transfer
through shared ancestors, thereby improving generalizability to unseen codes.
To further improve robustness, we introduce a prior-guided sparse
regularization scheme that refines the EHR co-occurrence graph by suppressing
spurious edges while preserving clinically meaningful associations. Our model
achieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and
maintains high accuracy under simulated unseen-code settings. Extensive
experiments with comprehensive ablation studies demonstrate HiRef's resilience
to unseen medical codes, supported by in-depth analyses of the learned
sparsified graph structure and medical code embeddings.

</details>


### [118] [MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance](https://arxiv.org/abs/2508.10429)
*Yi Dong,Yusuke Muraoka,Scott Shi,Yi Zhang*

Main category: cs.AI

TL;DR: MM-Food-100K是一个包含10万样本的多模态食品数据集，用于图像营养预测，通过微调大型视觉-语言模型验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 构建一个具有可验证来源的公共多模态食品智能数据集。

Method: 使用Codatta贡献模型，结合社区众包和可配置的AI辅助质量检查，从87,000多名贡献者收集数据。

Result: 发布了MM-Food-100K数据集，并在图像营养预测任务上，通过微调大型视觉-语言模型（ChatGPT 5, ChatGPT OSS, Qwen-Max）验证了数据集的有效性。

Conclusion: MM-Food-100K数据集的微调结果表明，在基于图像的营养预测方面，相对于开箱即用的基线模型，性能得到了持续提升。

Abstract: We present MM-Food-100K, a public 100,000-sample multimodal food intelligence
dataset with verifiable provenance. It is a curated approximately 10% open
subset of an original 1.2 million, quality-accepted corpus of food images
annotated for a wide range of information (such as dish name, region of
creation). The corpus was collected over six weeks from over 87,000
contributors using the Codatta contribution model, which combines community
sourcing with configurable AI-assisted quality checks; each submission is
linked to a wallet address in a secure off-chain ledger for traceability, with
a full on-chain protocol on the roadmap. We describe the schema, pipeline, and
QA, and validate utility by fine-tuning large vision-language models (ChatGPT
5, ChatGPT OSS, Qwen-Max) on image-based nutrition prediction. Fine-tuning
yields consistent gains over out-of-box baselines across standard metrics; we
report results primarily on the MM-Food-100K subset. We release MM-Food-100K
for publicly free access and retain approximately 90% for potential commercial
access with revenue sharing to contributors.

</details>


### [119] [We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning](https://arxiv.org/abs/2508.10433)
*Runqi Qiao,Qiuna Tan,Peiqing Yang,Yanzi Wang,Xiaowan Wang,Enhui Wan,Sitong Zhou,Guanting Dong,Yuchen Zeng,Yida Xu,Jie Wang,Chong Sun,Chen Li,Honggang Zhang*

Main category: cs.AI

TL;DR: This paper introduces We-Math 2.0, a system that enhances MLLMs' mathematical reasoning using a knowledge system, data space modeling, and RL-based training. It includes new datasets (MathBook-Standard & Pro), a RL framework (MathBook-RL), and a benchmark (MathBookEval).


<details>
  <summary>Details</summary>
Motivation: Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across various tasks, but still struggle with complex mathematical reasoning. Existing research primarily focuses on dataset construction and method optimization, often overlooking two critical aspects: comprehensive knowledge-driven design and model-centric data space modeling.

Method: We introduce We-Math 2.0, a unified system that integrates a structured mathematical knowledge system, model-centric data space modeling, and a reinforcement learning (RL)-based training paradigm to comprehensively enhance the mathematical reasoning abilities of MLLMs. The key contributions of We-Math 2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level hierarchical system encompassing 491 knowledge points and 1,819 fundamental principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a dataset that ensures broad conceptual coverage and flexibility through dual expansion. Additionally, we define a three-dimensional difficulty space and generate 7 progressive variants per problem to build MathBook-Pro, a challenging dataset for robust training. (3) MathBook-RL: We propose a two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive Alignment RL, leveraging average-reward learning and dynamic data scheduling to achieve progressive alignment across difficulty levels. (4) MathBookEval: We introduce a comprehensive benchmark covering all 491 knowledge points with diverse reasoning step distributions.

Result: Experimental results show that MathBook-RL performs competitively with existing baselines on four widely-used benchmarks and achieves strong results on MathBookEval, suggesting promising generalization in mathematical reasoning.

Conclusion: MathBook-RL performs competitively with existing baselines on four widely-used benchmarks and achieves strong results on MathBookEval, suggesting promising generalization in mathematical reasoning.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across various tasks, but still struggle with complex mathematical
reasoning. Existing research primarily focuses on dataset construction and
method optimization, often overlooking two critical aspects: comprehensive
knowledge-driven design and model-centric data space modeling. In this paper,
we introduce We-Math 2.0, a unified system that integrates a structured
mathematical knowledge system, model-centric data space modeling, and a
reinforcement learning (RL)-based training paradigm to comprehensively enhance
the mathematical reasoning abilities of MLLMs. The key contributions of We-Math
2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level
hierarchical system encompassing 491 knowledge points and 1,819 fundamental
principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a
dataset that ensures broad conceptual coverage and flexibility through dual
expansion. Additionally, we define a three-dimensional difficulty space and
generate 7 progressive variants per problem to build MathBook-Pro, a
challenging dataset for robust training. (3) MathBook-RL: We propose a
two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the
model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive
Alignment RL, leveraging average-reward learning and dynamic data scheduling to
achieve progressive alignment across difficulty levels. (4) MathBookEval: We
introduce a comprehensive benchmark covering all 491 knowledge points with
diverse reasoning step distributions. Experimental results show that
MathBook-RL performs competitively with existing baselines on four widely-used
benchmarks and achieves strong results on MathBookEval, suggesting promising
generalization in mathematical reasoning.

</details>


### [120] [FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs](https://arxiv.org/abs/2508.10467)
*Xueli Pan,Victor de Boer,Jacco van Ossenbruggen*

Main category: cs.AI

TL;DR: Proposes FIRESPARQL, a modular framework with fine-tuned LLMs, RAG, and a SPARQL query correction layer, to address errors in LLM-generated SPARQL queries for question answering over Scholarly Knowledge Graphs. Fine-tuning achieves the best performance.


<details>
  <summary>Details</summary>
Motivation: Question answering over Scholarly Knowledge Graphs (SKGs) remains a challenging task due to the complexity of scholarly content and the intricate structure of these graphs. Large Language Model (LLM) approaches struggle with SPARQL query generation due to limited exposure to SKG-specific content and the underlying schema. Identified two main types of errors: (i) structural inconsistencies and (ii) semantic inaccuracies.

Method: FIRESPARQL, a modular framework that supports fine-tuned LLMs as a core component, with optional context provided via retrieval-augmented generation (RAG) and a SPARQL query correction layer.

Result: fine-tuning achieves the highest overall performance, reaching 0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the test set.

Conclusion: Fine-tuning achieves the highest overall performance, reaching 0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the test set.

Abstract: Question answering over Scholarly Knowledge Graphs (SKGs) remains a
challenging task due to the complexity of scholarly content and the intricate
structure of these graphs. Large Language Model (LLM) approaches could be used
to translate natural language questions (NLQs) into SPARQL queries; however,
these LLM-based approaches struggle with SPARQL query generation due to limited
exposure to SKG-specific content and the underlying schema. We identified two
main types of errors in the LLM-generated SPARQL queries: (i) structural
inconsistencies, such as missing or redundant triples in the queries, and (ii)
semantic inaccuracies, where incorrect entities or properties are shown in the
queries despite a correct query structure. To address these issues, we propose
FIRESPARQL, a modular framework that supports fine-tuned LLMs as a core
component, with optional context provided via retrieval-augmented generation
(RAG) and a SPARQL query correction layer. We evaluate the framework on the
SciQA Benchmark using various configurations (zero-shot, zero-shot with RAG,
one-shot, fine-tuning, and fine-tuning with RAG) and compare the performance
with baseline and state-of-the-art approaches. We measure query accuracy using
BLEU and ROUGE metrics, and query result accuracy using relaxed exact
match(RelaxedEM), with respect to the gold standards containing the NLQs,
SPARQL queries, and the results of the queries. Experimental results
demonstrate that fine-tuning achieves the highest overall performance, reaching
0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the
test set.

</details>


### [121] [SEQ-GPT: LLM-assisted Spatial Query via Example](https://arxiv.org/abs/2508.10486)
*Ivan Khai Ze Lim,Ningyi Liao,Yiming Yang,Gerald Wei Yong Yip,Siqiang Luo*

Main category: cs.AI

TL;DR: This paper introduces SEQ-GPT, a spatial query system powered by Large Language Models (LLMs) towards more versatile SEQ search using natural language.


<details>
  <summary>Details</summary>
Motivation: Contemporary spatial services such as online maps predominantly rely on user queries for location searches. However, the user experience is limited when performing complex tasks, such as searching for a group of locations simultaneously. In this study, we examine the extended scenario known as Spatial Exemplar Query (SEQ), where multiple relevant locations are jointly searched based on user-specified examples.

Method: a tailored LLM adaptation pipeline that aligns natural language with structured spatial data and queries through dialogue synthesis and multi-model cooperation

Result: The language capabilities of LLMs enable unique interactive operations in the SEQ process, including asking users to clarify query details and dynamically adjusting the search based on user feedback.

Conclusion: SEQ-GPT offers an end-to-end demonstration for broadening spatial search with realistic data and application scenarios.

Abstract: Contemporary spatial services such as online maps predominantly rely on user
queries for location searches. However, the user experience is limited when
performing complex tasks, such as searching for a group of locations
simultaneously. In this study, we examine the extended scenario known as
Spatial Exemplar Query (SEQ), where multiple relevant locations are jointly
searched based on user-specified examples. We introduce SEQ-GPT, a spatial
query system powered by Large Language Models (LLMs) towards more versatile SEQ
search using natural language. The language capabilities of LLMs enable unique
interactive operations in the SEQ process, including asking users to clarify
query details and dynamically adjusting the search based on user feedback. We
also propose a tailored LLM adaptation pipeline that aligns natural language
with structured spatial data and queries through dialogue synthesis and
multi-model cooperation. SEQ-GPT offers an end-to-end demonstration for
broadening spatial search with realistic data and application scenarios.

</details>


### [122] [Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model](https://arxiv.org/abs/2508.10492)
*Shicheng Xu,Xin Huang,Zihao Wei,Liang Pang,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 提出了DxDirector-7B，它能够驱动全流程诊断，提高诊断准确率，减少医生工作量。


<details>
  <summary>Details</summary>
Motivation: 现有AI在临床诊断中的作用主要为辅助医生，缺乏从模糊主诉驱动整个诊断过程的能力，限制了其降低医生工作量和提高诊断效率。

Method: 提出了DxDirector-7B，一个具有高级深度思考能力的大型语言模型，使其能够在最小的医生参与下驱动全流程诊断。

Result: DxDirector-7B不仅实现了显著优于现有医学LLM和通用LLM的诊断准确率，而且大幅减少了医生的工作量。

Conclusion: DxDirector-7B在全流程诊断设置中表现出色，诊断准确率显著提高，并大幅减少了医生的工作量，专家评估表明其有潜力替代医疗专家。

Abstract: Full-process clinical diagnosis in the real world encompasses the entire
diagnostic workflow that begins with only an ambiguous chief complaint. While
artificial intelligence (AI), particularly large language models (LLMs), is
transforming clinical diagnosis, its role remains largely as an assistant to
physicians. This AI-assisted working pattern makes AI can only answer specific
medical questions at certain parts within the diagnostic process, but lack the
ability to drive the entire diagnostic process starting from an ambiguous
complaint, which still relies heavily on human physicians. This gap limits AI's
ability to fully reduce physicians' workload and enhance diagnostic efficiency.
To address this, we propose a paradigm shift that reverses the relationship
between physicians and AI: repositioning AI as the primary director, with
physicians serving as its assistants. So we present DxDirector-7B, an LLM
endowed with advanced deep thinking capabilities, enabling it to drive the
full-process diagnosis with minimal physician involvement. Furthermore,
DxDirector-7B establishes a robust accountability framework for misdiagnoses,
delineating responsibility between AI and human physicians. In evaluations
across rare, complex, and real-world cases under full-process diagnosis
setting, DxDirector-7B not only achieves significant superior diagnostic
accuracy but also substantially reduces physician workload than
state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained
analyses across multiple clinical departments and tasks validate its efficacy,
with expert evaluations indicating its potential to serve as a viable
substitute for medical specialists. These findings mark a new era where AI,
traditionally a physicians' assistant, now drives the entire diagnostic process
to drastically reduce physicians' workload, indicating an efficient and
accurate diagnostic solution.

</details>


### [123] [PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning](https://arxiv.org/abs/2508.10501)
*Yushi Feng,Junye Du,Yingying Hong,Qifan Wang,Lequan Yu*

Main category: cs.AI

TL;DR: PASS is a multimodal framework for Chest X-Ray reasoning that adaptively samples agentic workflows, offering probability-annotated trajectories for post-hoc audits and enhancing medical AI safety. It outperforms strong baselines while balancing computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing tool-augmented agentic systems are limited in the real world by black-box reasoning steps, poor multimodal integration, and rigid and computationally inefficient agentic pipelines. The paper aims to address these challenges in the context of Chest X-Ray (CXR) reasoning.

Method: PASS adaptively samples agentic workflows over a multi-tool graph, yielding decision paths annotated with interpretable probabilities. It also continuously compresses salient findings into an evolving personalized memory, while dynamically deciding whether to deepen its reasoning path or invoke an early exit for efficiency. To optimize a Pareto frontier balancing performance and cost, the authors design a novel three-stage training procedure, including expert knowledge warm-up, contrastive path-ranking, and cost-aware reinforcement learning.

Result: Experiments across various benchmarks validate that PASS significantly outperforms strong baselines in multiple metrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs.

Conclusion: PASS significantly outperforms strong baselines in multiple metrics while balancing computational costs, pushing a new paradigm shift towards interpretable, adaptive, and multimodal medical agentic systems.

Abstract: Existing tool-augmented agentic systems are limited in the real world by (i)
black-box reasoning steps that undermine trust of decision-making and pose
safety risks, (ii) poor multimodal integration, which is inherently critical
for healthcare tasks, and (iii) rigid and computationally inefficient agentic
pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the
first multimodal framework to address these challenges in the context of Chest
X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a
multi-tool graph, yielding decision paths annotated with interpretable
probabilities. Given the complex CXR reasoning task with multimodal medical
data, PASS leverages its learned task-conditioned distribution over the agentic
supernet. Thus, it adaptively selects the most suitable tool at each supernet
layer, offering probability-annotated trajectories for post-hoc audits and
directly enhancing medical AI safety. PASS also continuously compresses salient
findings into an evolving personalized memory, while dynamically deciding
whether to deepen its reasoning path or invoke an early exit for efficiency. To
optimize a Pareto frontier balancing performance and cost, we design a novel
three-stage training procedure, including expert knowledge warm-up, contrastive
path-ranking, and cost-aware reinforcement learning. To facilitate rigorous
evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step,
safety-critical, free-form CXR reasoning. Experiments across various benchmarks
validate that PASS significantly outperforms strong baselines in multiple
metrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs,
pushing a new paradigm shift towards interpretable, adaptive, and multimodal
medical agentic systems.

</details>


### [124] [Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment](https://arxiv.org/abs/2508.10530)
*Zetian Sun,Dongfang Li,Baotian Hu*

Main category: cs.AI

TL;DR: 本文研究了在语言模型对齐过程中，静态偏好数据和链上偏好数据之间的有效性差异，提出了对齐阶段假设，并提出了一种识别不同阶段之间边界的算法。


<details>
  <summary>Details</summary>
Motivation: 将语言模型 (LM) 与人类偏好对齐对于构建可靠的 AI 系统至关重要。问题通常被定义为优化 LM 策略，以最大限度地提高反映人类偏好的预期奖励。

Method: 提出了一种有效的算法来识别对齐过程不同阶段之间的边界。

Result: 链上数据并不总是最优的，静态偏好候选和链上偏好候选之间存在系统性的有效性差异。例如，对于 Llama-3，链上数据可能导致比静态数据高 3 倍的有效性，而对于 Zephyr，链上数据可能导致比静态数据低 0.4 倍的有效性。

Conclusion: 对 5 个模型（Llama、Zephyr、Phi-2、Qwen、Pythia）和 2 种对齐方法（DPO、SLiC-HF）进行了实验，以展示对齐阶段假设和边界测量方法的通用性。

Abstract: The alignment of language models (LMs) with human preferences is critical for
building reliable AI systems. The problem is typically framed as optimizing an
LM policy to maximize the expected reward that reflects human preferences.
Recently, Direct Preference Optimization (DPO) was proposed as a LM alignment
method that directly optimize the policy from static preference data, and
further improved by incorporating on-policy sampling (i.e., preference
candidates generated during the training loop) for better LM alignment.
However, we show on-policy data is not always optimal, with systematic
effectiveness difference emerging between static and on-policy preference
candidates. For example, on-policy data can result in a 3$\times$ effectiveness
compared with static data for Llama-3, and a 0.4$\times$ effectiveness for
Zephyr. To explain the phenomenon, we propose the alignment stage assumption,
which divides the alignment process into two distinct stages: the preference
injection stage, which benefits from diverse data, and the preference
fine-tuning stage, which favors high-quality data. Through theoretical and
empirical analysis, we characterize these stages and propose an effective
algorithm to identify the boundaries between them. We perform experiments on 5
models (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO,
SLiC-HF) to show the generalizability of alignment stage assumption and
boundary measurement.

</details>


### [125] [Improving Value-based Process Verifier via Low-Cost Variance Reduction](https://arxiv.org/abs/2508.10539)
*Zetian Sun,Dongfang Li,Baotian Hu,Min Zhang*

Main category: cs.AI

TL;DR: This paper introduces ComMCS, a variance reduction technique for value-based process verifiers that improves LLM reasoning in math problems without extra inference cost.


<details>
  <summary>Details</summary>
Motivation: Reasoning capabilities of LLMs in complex domains like mathematics remain a challenge, and value-based process verifiers suffer from estimation error due to the high cost of LLM inference and limited Monte Carlo samples.

Method: The paper proposes Compound Monte Carlo Sampling (ComMCS), which combines MC estimators from current and subsequent steps to construct an unbiased estimator with reduced variance.

Result: ComMCS outperforms regression-based optimization by 2.8 points and a non-variance-reduced baseline by 2.2 points on MATH-500 (Best-of-32 sampling).

Conclusion: The paper introduces ComMCS, a novel method for reducing variance in value-based process verifiers, leading to improved reasoning performance on benchmarks like MATH-500 and GSM8K.

Abstract: Large language models (LLMs) have achieved remarkable success in a wide range
of tasks. However, their reasoning capabilities, particularly in complex
domains like mathematics, remain a significant challenge. Value-based process
verifiers, which estimate the probability of a partial reasoning chain leading
to a correct solution, are a promising approach for improving reasoning.
Nevertheless, their effectiveness is often hindered by estimation error in
their training annotations, a consequence of the limited number of Monte Carlo
(MC) samples feasible due to the high cost of LLM inference. In this paper, we
identify that the estimation error primarily arises from high variance rather
than bias, and the MC estimator is a Minimum Variance Unbiased Estimator
(MVUE). To address the problem, we propose the \textsc{Com}pound \textsc{M}onte
\textsc{C}arlo \textsc{S}ampling (ComMCS) method, which constructs an unbiased
estimator by linearly combining the MC estimators from the current and
subsequent steps. Theoretically, we show that our method leads to a predictable
reduction in variance, while maintaining an unbiased estimation without
additional LLM inference cost. We also perform empirical experiments on the
MATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.
Notably, ComMCS outperforms regression-based optimization method by 2.8 points,
the non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32
sampling experiment.

</details>


### [126] [MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models](https://arxiv.org/abs/2508.10599)
*Xinyan Jiang,Lin Zhang,Jiayi Zhang,Qingsong Yang,Guimin Hu,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: MSRS通过为每个属性分配正交子空间来减少属性间的干扰，并通过混合子空间组合策略和token级引导机制来实现精确控制。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数方法都在努力共同引导多个属性，经常导致干扰和不良的权衡。

Method: 我们提出了多子空间表示引导（MSRS），这是一种通过子空间表示微调实现有效多属性引导的新框架。

Result: MSRS显著减少了属性冲突，在各种属性上优于现有方法，并能有效地推广到不同的下游任务。

Conclusion: MSRS显著减少了属性冲突，在各种属性上优于现有方法，并能有效地推广到不同的下游任务。

Abstract: Activation steering offers a promising approach to controlling the behavior
of Large Language Models by directly manipulating their internal activations.
However, most existing methods struggle to jointly steer multiple attributes,
often resulting in interference and undesirable trade-offs. To address this
challenge, we propose Multi-Subspace Representation Steering (MSRS), a novel
framework for effective multi-attribute steering via subspace representation
fine-tuning. MSRS reduces inter-attribute interference by allocating orthogonal
subspaces to each attribute, isolating their influence within the model's
representation space. MSRS also incorporates a hybrid subspace composition
strategy: it combines attribute-specific subspaces for unique steering
directions with a shared subspace for common steering directions. A dynamic
weighting function learns to efficiently integrate these components for precise
control. During inference, MSRS introduces a token-level steering mechanism
that dynamically identifies and intervenes on the most semantically relevant
tokens, enabling fine-grained behavioral modulation. Experimental results show
that MSRS significantly reduces attribute conflicts, surpasses existing methods
across a range of attributes, and generalizes effectively to diverse downstream
tasks.

</details>


### [127] [GenOM: Ontology Matching with Description Generation and Large Language Model](https://arxiv.org/abs/2508.10703)
*Yiping Song,Jiaoyan Chen,Renate A. Schmidt*

Main category: cs.AI

TL;DR: GenOM, a LLM-based ontology alignment framework, achieves competitive performance in the biomedical domain by enriching semantic representations and using few-shot prompting.


<details>
  <summary>Details</summary>
Motivation: Ontology matching (OM) is essential for semantic interoperability in the biomedical domain.

Method: GenOM, a LLM-based ontology alignment framework, enriches semantic representations, retrieves alignment candidates, and incorporates exact matching-based tools.

Result: Semantic enrichment and few-shot prompting are effective.

Conclusion: GenOM achieves competitive performance on the OAEI Bio-ML track, surpassing many baselines.

Abstract: Ontology matching (OM) plays an essential role in enabling semantic
interoperability and integration across heterogeneous knowledge sources,
particularly in the biomedical domain which contains numerous complex concepts
related to diseases and pharmaceuticals. This paper introduces GenOM, a large
language model (LLM)-based ontology alignment framework, which enriches the
semantic representations of ontology concepts via generating textual
definitions, retrieves alignment candidates with an embedding model, and
incorporates exact matching-based tools to improve precision. Extensive
experiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often
achieve competitive performance, surpassing many baselines including
traditional OM systems and recent LLM-based methods. Further ablation studies
confirm the effectiveness of semantic enrichment and few-shot prompting,
highlighting the framework's robustness and adaptability.

</details>


### [128] [Agentic Design Review System](https://arxiv.org/abs/2508.10745)
*Sayan Nag,K J Joseph,Koustava Goswami,Vlad I Morariu,Balaji Vasan Srinivasan*

Main category: cs.AI

TL;DR: 我们提出了一个 Agentic Design Review System (AgenticDRS)，其中多个代理协同分析一个设计，由一个元代理协调。


<details>
  <summary>Details</summary>
Motivation: 评估图形设计涉及从多个方面（如对齐、构图、美学和颜色选择）对其进行评估。以整体方式评估设计涉及汇总来自各个专家评审员的反馈。

Method: 基于图匹配的上下文范例选择方法和独特的提示扩展方法。

Result: 针对适应问题设置的最新基线进行的全面实验评估，并辅以关键的消融实验，证明了 Agentic-DRS 在评估图形设计和生成可操作的反馈方面的有效性。

Conclusion: Agentic-DRS 在评估图形设计和生成可操作的反馈方面是有效的。

Abstract: Evaluating graphic designs involves assessing it from multiple facets like
alignment, composition, aesthetics and color choices. Evaluating designs in a
holistic way involves aggregating feedback from individual expert reviewers.
Towards this, we propose an Agentic Design Review System (AgenticDRS), where
multiple agents collaboratively analyze a design, orchestrated by a meta-agent.
A novel in-context exemplar selection approach based on graph matching and a
unique prompt expansion method plays central role towards making each agent
design aware. Towards evaluating this framework, we propose DRS-BENCH
benchmark. Thorough experimental evaluation against state-of-the-art baselines
adapted to the problem setup, backed-up with critical ablation experiments
brings out the efficacy of Agentic-DRS in evaluating graphic designs and
generating actionable feedback. We hope that this work will attract attention
to this pragmatic, yet under-explored research direction.

</details>


### [129] [Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning](https://arxiv.org/abs/2508.10747)
*Sangwoo Jeon,Juchul Shin,Gyeong-Tae Kim,YeonJe Cho,Seongwoo Kim*

Main category: cs.AI

TL;DR: 提出了一种稀疏的、目标感知的GNN表示，该表示选择性地编码相关的局部关系，并显式地集成与目标相关的空间特征, 解决了现有方法在大型基于网格的环境中边缘信息组合爆炸的问题, 显著提高策略泛化和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常将规划状态表示为完全连接的图，导致边缘信息中的组合爆炸和随着问题规模的增长而产生的大量稀疏性，这在大型基于网格的环境中尤其明显。这种密集的表示会稀释节点级信息，成倍地增加内存需求，并最终使大规模问题的学习变得不可行。

Method: 提出了一种稀疏的、目标感知的GNN表示，该表示选择性地编码相关的局部关系，并显式地集成与目标相关的空间特征。

Result: 该方法能够有效扩展到先前使用密集图表示无法实现的大网格尺寸，并显著提高策略泛化和成功率。

Conclusion: 该方法能够有效扩展到先前使用密集图表示无法实现的大网格尺寸，并显著提高策略泛化和成功率。为解决实际的大规模广义规划任务奠定了实践基础。

Abstract: Generalized planning using deep reinforcement learning (RL) combined with
graph neural networks (GNNs) has shown promising results in various symbolic
planning domains described by PDDL. However, existing approaches typically
represent planning states as fully connected graphs, leading to a combinatorial
explosion in edge information and substantial sparsity as problem scales grow,
especially evident in large grid-based environments. This dense representation
results in diluted node-level information, exponentially increases memory
requirements, and ultimately makes learning infeasible for larger-scale
problems. To address these challenges, we propose a sparse, goal-aware GNN
representation that selectively encodes relevant local relationships and
explicitly integrates spatial features related to the goal. We validate our
approach by designing novel drone mission scenarios based on PDDL within a grid
world, effectively simulating realistic mission execution environments. Our
experimental results demonstrate that our method scales effectively to larger
grid sizes previously infeasible with dense graph representations and
substantially improves policy generalization and success rates. Our findings
provide a practical foundation for addressing realistic, large-scale
generalized planning tasks.

</details>


### [130] [Modeling Human Responses to Multimodal AI Content](https://arxiv.org/abs/2508.10769)
*Zhiqi Shen,Shaojing Fan,Danni Xu,Terence Sim,Mohan Kankanhalli*

Main category: cs.AI

TL;DR: 该研究调查了 AI 生成内容对人类感知和行为的影响，引入了数据集和评估指标，并提出了一个基于 LLM 的代理系统，旨在提高 LLM 对人类反应的感知能力，从而降低 AI 驱动的错误信息风险。


<details>
  <summary>Details</summary>
Motivation: 随着 AI 生成内容变得普遍，错误信息的风险也随之增加。之前的研究主要集中在识别内容是否真实，而对于这些内容如何影响人类的感知和行为知之甚少。在交易或股票市场等领域，预测人们的反应（例如，一个新闻帖子是否会走红）可能比验证其事实准确性更为重要。

Method: 引入了 MhAIM 数据集（包含 154,552 个在线帖子，其中 111,153 个是 AI 生成的），并提出了三种新指标：可信度、影响力和开放性，以量化用户如何判断和参与在线内容。此外，还提出了 T-Lens，一个基于 LLM 的代理系统，旨在通过整合预测的人类对多模态信息的反应来回答用户查询。

Result: 人类在识别 AI 内容方面表现更好，尤其是在帖子包含文本和视觉效果，并且两者之间存在不一致时。

Conclusion: 该研究提供经验性见解和实用工具，使 LLM 具备人类感知能力，并为降低人工智能驱动的错误信息风险提供可行策略。

Abstract: As AI-generated content becomes widespread, so does the risk of
misinformation. While prior research has primarily focused on identifying
whether content is authentic, much less is known about how such content
influences human perception and behavior. In domains like trading or the stock
market, predicting how people react (e.g., whether a news post will go viral),
can be more critical than verifying its factual accuracy. To address this, we
take a human-centered approach and introduce the MhAIM Dataset, which contains
154,552 online posts (111,153 of them AI-generated), enabling large-scale
analysis of how people respond to AI-generated content. Our human study reveals
that people are better at identifying AI content when posts include both text
and visuals, particularly when inconsistencies exist between the two. We
propose three new metrics: trustworthiness, impact, and openness, to quantify
how users judge and engage with online content. We present T-Lens, an LLM-based
agent system designed to answer user queries by incorporating predicted human
responses to multimodal information. At its core is HR-MCP (Human Response
Model Context Protocol), built on the standardized Model Context Protocol
(MCP), enabling seamless integration with any LLM. This integration allows
T-Lens to better align with human reactions, enhancing both interpretability
and interaction capabilities. Our work provides empirical insights and
practical tools to equip LLMs with human-awareness capabilities. By
highlighting the complex interplay among AI, human cognition, and information
reception, our findings suggest actionable strategies for mitigating the risks
of AI-driven misinformation.

</details>


### [131] [The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference](https://arxiv.org/abs/2508.10777)
*Maël Jullien,Marco Valentino,André Freitas*

Main category: cs.AI

TL;DR: current LLMs often possess the relevant clinical knowledge but lack the structured, composable internal representations needed to deploy it reliably


<details>
  <summary>Details</summary>
Motivation: Large language models are often assumed to acquire increasingly structured, generalizable internal representations simply by scaling data and parameters. We interrogate this assumption

Method: introducing a Clinical Trial Natural Language Inference benchmark comprising four reasoning families, Causal Attribution, Compositional Grounding, Epistemic Verification, and Risk State Abstraction. Each item is paired with a targeted Ground Knowledge and Meta-Level Reasoning Verification (GKMRV) probe

Result: Models achieve near-ceiling GKMRV accuracy yet perform poorly on the main reasoning tasks. Despite low accuracy, output inferences are highly consistent across samples, indicating a systematic application of underlying heuristics and shortcuts

Conclusion: current LLMs often possess the relevant clinical knowledge but lack the structured, composable internal representations needed to deploy it reliably

Abstract: Large language models are often assumed to acquire increasingly structured,
generalizable internal representations simply by scaling data and parameters.
We interrogate this assumption by introducing a Clinical Trial Natural Language
Inference benchmark comprising four reasoning families, Causal Attribution,
Compositional Grounding, Epistemic Verification, and Risk State Abstraction.
Each item is paired with a targeted Ground Knowledge and Meta-Level Reasoning
Verification (GKMRV) probe, allowing us to dissociate failures of factual
access from failures of inference. We evaluate six contemporary LLMs under both
direct and chain of thought prompting.
  Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform
poorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy,
output inferences are highly consistent across samples (mean 0.87), indicating
a systematic application of underlying heuristics and shortcuts.
  These results reveal fundamental structural and representational limitations:
current LLMs often possess the relevant clinical knowledge but lack the
structured, composable internal representations needed to deploy it reliably
(e.g., integrating constraints, weighing evidence, or simulating
counterfactuals). Decoupling knowledge from reasoning with GKMRV makes this
dissociation explicit and measurable, providing an effective framework for
probing the reliability of LLMs in high-stakes domains.

</details>


### [132] [Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems](https://arxiv.org/abs/2508.10806)
*Maria J. P. Peixoto,Akriti Pandey,Ahsan Zaman,Peter R. Lewis*

Main category: cs.AI

TL;DR: XAI methods are not accessible to users with vision impairments. The paper proposes a four-part methodological proof of concept for inclusive XAI design.


<details>
  <summary>Details</summary>
Motivation: The accessibility of XAI methods for users with vision impairments remains underexplored.

Method: A two-pronged approach: a literature review of 79 studies and a four-part methodological proof of concept.

Result: Evaluations of XAI techniques rarely include disabled users, with most explanations relying on inherently visual formats.

Conclusion: Simplified explanations are more comprehensible for non-visual users, and multimodal presentation is required for more equitable interpretability.

Abstract: As AI systems are increasingly deployed to support decision-making in
critical domains, explainability has become a means to enhance the
understandability of these outputs and enable users to make more informed and
conscious choices. However, despite growing interest in the usability of
eXplainable AI (XAI), the accessibility of these methods, particularly for
users with vision impairments, remains underexplored. This paper investigates
accessibility gaps in XAI through a two-pronged approach. First, a literature
review of 79 studies reveals that evaluations of XAI techniques rarely include
disabled users, with most explanations relying on inherently visual formats.
Second, we present a four-part methodological proof of concept that
operationalizes inclusive XAI design: (1) categorization of AI systems, (2)
persona definition and contextualization, (3) prototype design and
implementation, and (4) expert and user assessment of XAI techniques for
accessibility. Preliminary findings suggest that simplified explanations are
more comprehensible for non-visual users than detailed ones, and that
multimodal presentation is required for more equitable interpretability.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [133] [Privacy-Preserving Approximate Nearest Neighbor Search on High-Dimensional Data](https://arxiv.org/abs/2508.10373)
*Yingfan Liu,Yandi Zhang,Jiadong Xie,Hui Li,Jeffrey Xu Yu,Jiangtao Cui*

Main category: cs.DB

TL;DR: This paper introduces a novel privacy-preserving k-ANNS solution that improves speed and accuracy compared to existing methods using a new encryption method and index.


<details>
  <summary>Details</summary>
Motivation: Existing PP-ANNS solutions fall short of meeting the requirements of data privacy, efficiency, accuracy, and minimal user involvement concurrently.

Method: The paper introduces a novel encryption method named distance comparison encryption and a privacy-preserving index that combines the state-of-the-art  𝑘 -ANNS method with an approximate distance computation method. A search method using a filter-and-refine strategy based on the index is also devised.

Result: The proposed method accelerates PP-ANNS by up to 3 orders of magnitude compared to state-of-the-art methods, while not compromising the accuracy.

Conclusion: The proposed solution accelerates PP-ANNS by up to 3 orders of magnitude compared to state-of-the-art methods, while not compromising the accuracy.

Abstract: In the era of cloud computing and AI, data owners outsource ubiquitous
vectors to the cloud, which furnish approximate $k$-nearest neighbors
($k$-ANNS) services to users. To protect data privacy against the untrusted
server, privacy-preserving $k$-ANNS (PP-ANNS) on vectors has been a fundamental
and urgent problem. However, existing PP-ANNS solutions fall short of meeting
the requirements of data privacy, efficiency, accuracy, and minimal user
involvement concurrently. To tackle this challenge, we introduce a novel
solution that primarily executes PP-ANNS on a single cloud server to avoid the
heavy communication overhead between the cloud and the user. To ensure data
privacy, we introduce a novel encryption method named distance comparison
encryption, facilitating secure, efficient, and exact distance comparisons. To
optimize the trade-off between data privacy and search performance, we design a
privacy-preserving index that combines the state-of-the-art $k$-ANNS method
with an approximate distance computation method. Then, we devise a search
method using a filter-and-refine strategy based on the index. Moreover, we
provide the security analysis of our solution and conduct extensive experiments
to demonstrate its superiority over existing solutions. Based on our
experimental results, our method accelerates PP-ANNS by up to 3 orders of
magnitude compared to state-of-the-art methods, while not compromising the
accuracy.

</details>


### [134] [Cross-Organizational Analysis of Parliamentary Processes: A Case Study](https://arxiv.org/abs/2508.10381)
*Paul-Julius Hillmann,Stephan A. Fahrenkrog-Petersen,Jan Mendling*

Main category: cs.DB

TL;DR: This paper applies process mining to parliamentary processes and analyzes legislative processes of three German state parliaments to generate insights into their differences and best practices.


<details>
  <summary>Details</summary>
Motivation: Little attention has gone into the cross-organizational comparison of processes, since many companies are hesitant to share their data.

Method: Process mining

Result: The paper provides a discussion of the relevance of the results based on knowledge exchange with a political scientist and a domain expert from the German federal parliament.

Conclusion: This paper analyzes legislative processes of three German state parliaments, generating insights into their differences and best practices.

Abstract: Process Mining has been widely adopted by businesses and has been shown to
help organizations analyze and optimize their processes. However, so far,
little attention has gone into the cross-organizational comparison of
processes, since many companies are hesitant to share their data. In this
paper, we explore the processes of German state parliaments that are often
legally required to share their data and run the same type of processes for
different geographical regions. This paper is the first attempt to apply
process mining to parliamentary processes and, therefore, contributes toward a
novel interdisciplinary research area that combines political science and
process mining. In our case study, we analyze legislative processes of three
German state parliaments and generate insights into their differences and best
practices. We provide a discussion of the relevance of our results that are
based on knowledge exchange with a political scientist and a domain expert from
the German federal parliament.

</details>


### [135] [Efficient Methods for Accurate Sparse Trajectory Recovery and Map Matching](https://arxiv.org/abs/2508.10460)
*Wei Tian,Jieming Shi,Man Lung Yiu*

Main category: cs.DB

TL;DR: 提出了 TRMMA 和 MMA 两种方法，用于提高稀疏轨迹的质量，实验结果表明它们优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界的轨迹通常是稀疏的，采样率低，并且与道路网络未对齐，但许多应用需要高质量的数据以获得最佳性能。为了提高以稀疏轨迹作为输入的数据质量，系统地研究了两个相关的研究问题：道路网络上的轨迹恢复和地图匹配。

Method: 提出 TRMMA 和 MMA 两种高效方法，分别用于轨迹恢复和地图匹配。MMA 作为 TRMMA 的第一步，将 GPS 点映射到候选路段集合中的路段，生成有效嵌入来捕获 GPS 数据、方向信息和路段的模式。TRMMA 采用双 Transformer 编码过程，协同捕获轨迹和路线中的潜在模式，并采用有效的解码技术来顺序预测缺失点的位置比率和路段。

Result: TRMMA 和 MMA 在轨迹恢复和地图匹配方面都取得了最佳的结果质量。

Conclusion: TRMMA 和 MMA 在四个大型真实世界数据集上，在轨迹恢复和地图匹配方面，始终优于现有方法，通常有显著优势。

Abstract: Real-world trajectories are often sparse with low-sampling rates (i.e., long
intervals between consecutive GPS points) and misaligned with road networks,
yet many applications demand high-quality data for optimal performance. To
improve data quality with sparse trajectories as input, we systematically study
two related research problems: trajectory recovery on road network, which aims
to infer missing points to recover high-sampling trajectories, and map
matching, which aims to map GPS points to road segments to determine underlying
routes. In this paper, we present efficient methods TRMMA and MMA for accurate
trajectory recovery and map matching, respectively, where MMA serves as the
first step of TRMMA. In MMA, we carefully formulate a classification task to
map a GPS point from sparse trajectories to a road segment over a small
candidate segment set, rather than the entire road network. We develop
techniques in MMA to generate effective embeddings that capture the patterns of
GPS data, directional information, and road segments, to accurately align
sparse trajectories to routes. For trajectory recovery, TRMMA focuses on the
segments in the route returned by MMA to infer missing points with position
ratios on road segments, producing high-sampling trajectories efficiently by
avoiding evaluation of all road segments. Specifically, in TRMMA, we design a
dual-transformer encoding process to cohesively capture latent patterns in
trajectories and routes, and an effective decoding technique to sequentially
predict the position ratios and road segments of missing points. We conduct
extensive experiments to compare TRMMA and MMA with numerous existing methods
for trajectory recovery and map matching, respectively, on 4 large real-world
datasets. TRMMA and MMA consistently achieve the best result quality, often by
a significant margin.

</details>


### [136] [Advances in Logic-Based Entity Resolution: Enhancing ASPEN with Local Merges and Optimality Criteria](https://arxiv.org/abs/2508.10504)
*Zhliang Xiang,Meghyn Bienvenu,Gianluca Cima,Víctor Gutiérrez-Basulto,Yazmín Ibáñez-García*

Main category: cs.DB

TL;DR: ASPEN+扩展了ASPEN，支持本地合并和新的优化标准，提高了实体解析的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: ASPEN仅支持实体引用常量的全局合并，但在解析数据值时，本地合并通常更合适。

Method: ASPEN+

Result: 在真实世界的数据集上进行了广泛的实验评估，证明了本地合并和新的优化标准对准确性和运行时的影响。

Conclusion: ASPEN+通过支持本地合并和新的优化标准，扩展了现有的基于ASP的集体实体解析系统ASPEN。

Abstract: In this paper, we present ASPEN+, which extends an existing ASP-based system,
ASPEN,for collective entity resolution with two important functionalities:
support for local merges and new optimality criteria for preferred solutions.
Indeed, ASPEN only supports so-called global merges of entity-referring
constants (e.g. author ids), in which all occurrences of matched constants are
treated as equivalent and merged accordingly. However, it has been argued that
when resolving data values, local merges are often more appropriate, as e.g.
some instances of 'J. Lee' may refer to 'Joy Lee', while others should be
matched with 'Jake Lee'. In addition to allowing such local merges, ASPEN+
offers new optimality criteria for selecting solutions, such as minimizing rule
violations or maximising the number of rules supporting a merge. Our main
contributions are thus (1) the formalisation and computational analysis of
various notions of optimal solution, and (2) an extensive experimental
evaluation on real-world datasets, demonstrating the effect of local merges and
the new optimality criteria on both accuracy and runtime.

</details>


### [137] [Emerging Skycube](https://arxiv.org/abs/2508.10516)
*Mickaël Martin Nevot*

Main category: cs.DB

TL;DR: 本文提出了一种新的数据分析方法Emerging Skycube，用于提取全局最优数据并观察其演变，并通过两种约简方法优化计算效率。


<details>
  <summary>Details</summary>
Motivation: 为了提取与多个标准相关的全局最优或非支配数据，并观察它们根据决策属性的演变。

Method: 结合多标准决策分析和趋势反转发现。

Result: 与初始数据相比，新兴Skycube的计算成本更低，并且可以通过Skycube无损部分物化和闭合新兴Skycube或闭合新兴L-Skycube来进一步减少计算时间和存储空间。

Conclusion: 提出了Emerging Skycube的概念，结合了Skycube和新兴数据立方体，并通过两种连续的约简方法节省计算时间和存储空间。

Abstract: Combining multi-criteria decision analysis and trend reversal discovery make
it possible to extract globally optimal, or non-dominated, data in relation to
several criteria, and then to observe their evolution according to a
decision-making property. Thus, we introduce Emerging Skycube, a concept
associating Skycube and emerging datacube. As far as we know, no
DBMS-integrated solution exists to compute an emerging Skycube, and hence
taking advantage of ROLAP analysis tools. An emerging datacube has only one
measure: we propose to use several to comply to multi-criteria decision
analysis constraints which requires multiple attributes. A datacube is
expensive to compute. An emerging datacube is about twice as expensive. On the
other hand, an emerging Skycube is cheaper as the trend reversal is computed
after two Skycube calculations, which considerably reduces the relation volume
in comparison with the initial one. It is possible to save even more computing
time and storage space. To this end, we propose two successive reductions.
First, a Skycube lossless partial materialisation using Skylines concepts
lattice, based on the agree concepts lattice and partitions lattice. Then,
either the closed emerging Skycube for an information-loss reduction, or the
closed emerging L-Skycube for a smaller but lossless reduction.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [138] [Bridging Modality Gaps in e-Commerce Products via Vision-Language Alignment](https://arxiv.org/abs/2508.10116)
*Yipeng Zhang,Hongju Yu,Aritra Mandal,Canran Xu,Qunzhi Zhou,Zhe Wu*

Main category: cs.IR

TL;DR: OPAL是一个用于从图像生成符合模式的高质量商品描述的框架，它使用微调的多模态大型语言模型(MLLM)，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在电子商务中，商品信息对于有效的用户参与至关重要。然而，手动或半手动录入结构化的商品信息通常会产生质量不一致、错误和周转缓慢的问题，尤其是在C2C卖家中。直接从商品图片生成准确的描述提供了一种有希望的替代方案。现有的基于检索的解决方案解决了一些问题，但通常会遗漏细粒度的视觉细节，并且难以处理小众或专业类别。

Method: OPAL使用视觉指令调整结合直接偏好优化来微调MLLM，减少幻觉并提高不同骨干架构的鲁棒性。它引入了两种数据改进方法：MLLM辅助一致性增强和LLM辅助上下文理解。

Result: 在真实世界的电子商务数据集上评估了OPAL，表明它在描述质量和模式完成率方面始终优于基线方法。

Conclusion: OPAL有效地弥合了视觉和文本模态之间的差距，从而提供了更丰富、更准确和更一致的商品描述。这项工作改进了自动化列表优化，并支持电子商务平台中可扩展的高质量内容生成。

Abstract: Item information, such as titles and attributes, is essential for effective
user engagement in e-commerce. However, manual or semi-manual entry of
structured item specifics often produces inconsistent quality, errors, and slow
turnaround, especially for Customer-to-Customer sellers. Generating accurate
descriptions directly from item images offers a promising alternative. Existing
retrieval-based solutions address some of these issues but often miss
fine-grained visual details and struggle with niche or specialized categories.
  We propose Optimized Preference-Based AI for Listings (OPAL), a framework for
generating schema-compliant, high-quality item descriptions from images using a
fine-tuned multimodal large language model (MLLM). OPAL addresses key
challenges in multimodal e-commerce applications, including bridging modality
gaps and capturing detailed contextual information. It introduces two data
refinement methods: MLLM-Assisted Conformity Enhancement, which ensures
alignment with structured schema requirements, and LLM-Assisted Contextual
Understanding, which improves the capture of nuanced and fine-grained
information from visual inputs.
  OPAL uses visual instruction tuning combined with direct preference
optimization to fine-tune the MLLM, reducing hallucinations and improving
robustness across different backbone architectures. We evaluate OPAL on
real-world e-commerce datasets, showing that it consistently outperforms
baseline methods in both description quality and schema completion rates. These
results demonstrate that OPAL effectively bridges the gap between visual and
textual modalities, delivering richer, more accurate, and more consistent item
descriptions. This work advances automated listing optimization and supports
scalable, high-quality content generation in e-commerce platforms.

</details>


### [139] [DS4RS: Community-Driven and Explainable Dataset Search Engine for Recommender System Research](https://arxiv.org/abs/2508.10238)
*Xinyang Shao,Tri Kurniawan Wijaya*

Main category: cs.IR

TL;DR: 我们构建了一个推荐系统数据集搜索引擎，它支持语义搜索，通过社区驱动改进数据集发现，并且是可解释的。


<details>
  <summary>Details</summary>
Motivation: 由于来源分散和元数据不一致，找到与特定推荐任务或领域相匹配的数据集仍然是一个挑战。为了解决这个问题，我们提出了一个社区驱动的、可解释的数据集搜索引擎，专为推荐系统研究而设计。

Method: 我们提出了一个社区驱动的、可解释的数据集搜索引擎，专为推荐系统研究而设计。该系统支持跨多个数据集属性（如数据集名称、描述和推荐领域）的语义搜索，并提供搜索相关性的解释以增强透明度。该系统鼓励社区参与，允许用户在公共存储库中贡献标准化的数据集元数据。

Result: 我们的系统支持跨多个数据集属性（如数据集名称、描述和推荐领域）的语义搜索，并提供搜索相关性的解释以增强透明度。该系统鼓励社区参与，允许用户在公共存储库中贡献标准化的数据集元数据。

Conclusion: 该系统通过改进数据集的可发现性和搜索可解释性，促进更高效的研究重现。

Abstract: Accessing suitable datasets is critical for research and development in
recommender systems. However, finding datasets that match specific
recommendation task or domains remains a challenge due to scattered sources and
inconsistent metadata. To address this gap, we propose a community-driven and
explainable dataset search engine tailored for recommender system research. Our
system supports semantic search across multiple dataset attributes, such as
dataset names, descriptions, and recommendation domain, and provides
explanations of search relevance to enhance transparency. The system encourages
community participation by allowing users to contribute standardized dataset
metadata in public repository. By improving dataset discoverability and search
interpretability, the system facilitates more efficient research reproduction.
The platform is publicly available at: https://ds4rs.com.

</details>


### [140] [Clicks Versus Conversion: Choosing a Recommender's Training Objective in E-Commerce](https://arxiv.org/abs/2508.10377)
*Michael Weiss,Robert Rosenbach,Christian Eggenberger*

Main category: cs.IR

TL;DR: 本文比较了优化点击率 (CTR) 与优化订单提交率 (OSR) 对 GMV 的影响，发现优化 OSR 带来的 GMV 提升远大于优化 CTR。


<details>
  <summary>Details</summary>
Motivation: 在电子商务中，对点击率 (CTR) 或高转化率（如加入购物车率 (ACR) 和订单提交率 (OSR)）进行排名产品推荐是标准做法。CTR 之外，ACR 和 OSR 与商店的业务目标（如商品交易总额 (GMV)）更直接相关。

Method: 使用在线 A/B 测试比较了使用 CTR 或 OSR 作为目标的效果。

Result: 优化 OSR 产生的 GMV 提升是优化 CTR 的五倍以上，且不牺牲新产品的发现。结果还提供了对每个目标的不同特征重要性的见解。

Conclusion: 在我们的商店中，优化 OSR 产生的 GMV 提升是优化 CTR 的五倍以上，且不牺牲新产品的发现。

Abstract: Ranking product recommendations to optimize for a high click-through rate
(CTR) or for high conversion, such as add-to-cart rate (ACR) and
Order-Submit-Rate (OSR, view-to-purchase conversion) are standard practices in
e-commerce. Optimizing for CTR appears like a straightforward choice: Training
data (i.e., click data) are simple to collect and often available in large
quantities. Additionally, CTR is used far beyond e-commerce, making it a
generalist, easily implemented option. ACR and OSR, on the other hand, are more
directly linked to a shop's business goals, such as the Gross Merchandise Value
(GMV). In this paper, we compare the effects of using either of these
objectives using an online A/B test. Among our key findings, we demonstrate
that in our shops, optimizing for OSR produces a GMV uplift more than five
times larger than when optimizing for CTR, without sacrificing new product
discovery. Our results also provide insights into the different feature
importances for each of the objectives.

</details>


### [141] [Proxy Model-Guided Reinforcement Learning for Client Selection in Federated Recommendation](https://arxiv.org/abs/2508.10401)
*Liang Qu,Jianxin Li,Wei Yuan,Penghui Ruan,Yuhui Shi,Hongzhi Yin*

Main category: cs.IR

TL;DR: This paper proposes ProxyRL-FRS, a proxy model-guided reinforcement learning framework tailored for client selection in federated recommendation, which can eliminate the need for expensive per-round local training traditionally required to evaluate a client's contribution and enrich the update coverage of item embeddings.


<details>
  <summary>Details</summary>
Motivation: most existing FedRS frameworks adopt fully random client selection strategy in each training round, overlooking the statistical heterogeneity of user data arising from diverse preferences and behavior patterns, thereby resulting in suboptimal model performance.existing methods are typically designed for generic tasks and fail to address the unique challenges of recommendation scenarios, such as expensive contribution evaluation due to the large number of clients, and sparse updates resulting from long-tail item distributions

Method: introduce ProxyNCF, a dual-branch model deployed on each client, which augments standard Neural Collaborative Filtering with an additional proxy model branch that provides lightweight contribution estimation,design a staleness-aware SA reinforcement learning agent that selects clients based on the proxy-estimated contribution, and is guided by a reward function balancing recommendation accuracy and embedding staleness

Result: achieve better model performance

Conclusion: Experiments conducted on public recommendation datasets demonstrate the effectiveness of ProxyRL-FRS.

Abstract: Federated recommender systems have emerged as a promising privacy-preserving
paradigm, enabling personalized recommendation services without exposing users'
raw data. By keeping data local and relying on a central server to coordinate
training across distributed clients, FedRSs protect user privacy while
collaboratively learning global models. However, most existing FedRS frameworks
adopt fully random client selection strategy in each training round,
overlooking the statistical heterogeneity of user data arising from diverse
preferences and behavior patterns, thereby resulting in suboptimal model
performance. While some client selection strategies have been proposed in the
broader federated learning literature, these methods are typically designed for
generic tasks and fail to address the unique challenges of recommendation
scenarios, such as expensive contribution evaluation due to the large number of
clients, and sparse updates resulting from long-tail item distributions. To
bridge this gap, we propose ProxyRL-FRS, a proxy model-guided reinforcement
learning framework tailored for client selection in federated recommendation.
Specifically, we first introduce ProxyNCF, a dual-branch model deployed on each
client, which augments standard Neural Collaborative Filtering with an
additional proxy model branch that provides lightweight contribution
estimation, thus eliminating the need for expensive per-round local training
traditionally required to evaluate a client's contribution. Furthermore, we
design a staleness-aware SA reinforcement learning agent that selects clients
based on the proxy-estimated contribution, and is guided by a reward function
balancing recommendation accuracy and embedding staleness, thereby enriching
the update coverage of item embeddings. Experiments conducted on public
recommendation datasets demonstrate the effectiveness of ProxyRL-FRS.

</details>


### [142] [Semantic IDs for Joint Generative Search and Recommendation](https://arxiv.org/abs/2508.10478)
*Gustavo Penha,Edoardo D'Amico,Marco De Nadai,Enrico Palumbo,Alexandre Tamborrino,Ali Vardasbi,Max Lefarov,Shawn Lin,Timothy Heath,Francesco Fabbri,Hugues Bouchard*

Main category: cs.IR

TL;DR: 本文探讨了如何构建在统一模型中在搜索和推荐方面都表现良好的语义ID。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）驱动的生成模型正在成为支持推荐和搜索任务的统一解决方案。这些模型中的一个关键设计选择是如何表示项目，传统上通过唯一标识符（ID），最近通过由离散代码组成的语义ID，从嵌入中获得。

Method: 比较一系列构建语义ID的策略，研究特定任务和跨任务的方法，以及每个任务是否应该在联合搜索和推荐生成模型中拥有自己的语义ID令牌。

Result: 使用在搜索和推荐任务上微调的双编码器模型来获得项目嵌入，然后构建统一的语义ID空间，可以在两项任务中实现强大的性能。

Conclusion: 使用在搜索和推荐任务上微调的双编码器模型来获得项目嵌入，然后构建统一的语义ID空间，可以在两项任务中实现强大的性能。

Abstract: Generative models powered by Large Language Models (LLMs) are emerging as a
unified solution for powering both recommendation and search tasks. A key
design choice in these models is how to represent items, traditionally through
unique identifiers (IDs) and more recently with Semantic IDs composed of
discrete codes, obtained from embeddings. While task-specific embedding models
can improve performance for individual tasks, they may not generalize well in a
joint setting. In this paper, we explore how to construct Semantic IDs that
perform well both in search and recommendation when using a unified model. We
compare a range of strategies to construct Semantic IDs, looking into
task-specific and cross-tasks approaches, and also whether each task should
have its own semantic ID tokens in a joint search and recommendation generative
model. Our results show that using a bi-encoder model fine-tuned on both search
and recommendation tasks to obtain item embeddings, followed by the
construction of a unified Semantic ID space provides an effective trade-off,
enabling strong performance in both tasks. We hope these findings spark
follow-up work on generalisable, semantically grounded ID schemes and inform
the next wave of unified generative recommender architectures.

</details>


### [143] [Efficient Patent Searching Using Graph Transformers](https://arxiv.org/abs/2508.10496)
*Krzysztof Daniell,Igor Buzhinsky,Sebastian Björkqvist*

Main category: cs.IR

TL;DR: 提出了一种基于图Transformer的专利搜索方法，该方法利用专利审查员的引文数据进行训练，提高了检索质量和效率。


<details>
  <summary>Details</summary>
Motivation: 在大量专利文件中寻找相关现有技术以决定是否提交新的专利申请或使现有专利无效非常具有挑战性，因此需要一个准确的搜索引擎来加速这一过程。

Method: 使用图Transformer表示专利特征及其关系，并使用专利审查员的引文数据进行训练。

Result: 该方法在现有技术检索质量和计算效率上均实现了显著提升。

Conclusion: 该研究提出了一种基于图Transformer的密集检索方法用于专利搜索，并在检索质量和计算效率上均优于现有文本嵌入模型。

Abstract: Finding relevant prior art is crucial when deciding whether to file a new
patent application or invalidate an existing patent. However, searching for
prior art is challenging due to the large number of patent documents and the
need for nuanced comparisons to determine novelty. An accurate search engine is
therefore invaluable for speeding up the process. We present a Graph
Transformer-based dense retrieval method for patent searching where each
invention is represented by a graph describing its features and their
relationships. Our model processes these invention graphs and is trained using
prior art citations from patent office examiners as relevance signals. Using
graphs as input significantly improves the computational efficiency of
processing long documents, while leveraging examiner citations allows the model
to learn domain-specific similarities beyond simple text-based matching. The
result is a search engine that emulates how professional patent examiners
identify relevant documents. We compare our approach against publicly available
text embedding models and show substantial improvements in both prior art
retrieval quality and computational efficiency.

</details>


### [144] [DAS: Dual-Aligned Semantic IDs Empowered Industrial Recommender System](https://arxiv.org/abs/2508.10584)
*Wencai Ye,Mingjie Sun,Shaoyun Shi,Peng Wang,Wenjin Wu,Peng Jiang*

Main category: cs.IR

TL;DR: DAS is a one-stage method for aligning semantic IDs with collaborative signals in recommendation systems, improving performance and deployed at Kuaishou App.


<details>
  <summary>Details</summary>
Motivation: Semantic IDs lack collaborative signals, leading to misalignment with recommendation objectives. Existing two-stage frameworks suffer from information loss and inflexibility.

Method: The paper proposes a one-stage Dual-Aligned Semantic IDs (DAS) method that simultaneously optimizes quantization and alignment. It uses multi-view contrastive alignment and dual learning to achieve efficient alignment between semantic IDs and collaborative signals.

Result: Extensive offline experiments and online A/B tests demonstrate the effectiveness of DAS.

Conclusion: The proposed DAS method is effective and has been successfully deployed in various advertising scenarios at Kuaishou App, serving over 400 million users daily.

Abstract: Semantic IDs are discrete identifiers generated by quantizing the Multi-modal
Large Language Models (MLLMs) embeddings, enabling efficient multi-modal
content integration in recommendation systems. However, their lack of
collaborative signals results in a misalignment with downstream discriminative
and generative recommendation objectives. Recent studies have introduced
various alignment mechanisms to address this problem, but their two-stage
framework design still leads to two main limitations: (1) inevitable
information loss during alignment, and (2) inflexibility in applying adaptive
alignment strategies, consequently constraining the mutual information
maximization during the alignment process. To address these limitations, we
propose a novel and flexible one-stage Dual-Aligned Semantic IDs (DAS) method
that simultaneously optimizes quantization and alignment, preserving semantic
integrity and alignment quality while avoiding the information loss typically
associated with two-stage methods. Meanwhile, DAS achieves more efficient
alignment between the semantic IDs and collaborative signals, with the
following two innovative and effective approaches: (1) Multi-view Constrative
Alignment: To maximize mutual information between semantic IDs and
collaborative signals, we first incorporate an ID-based CF debias module, and
then design three effective contrastive alignment methods: dual user-to-item
(u2i), dual item-to-item/user-to-user (i2i/u2u), and dual co-occurrence
item-to-item/user-to-user (i2i/u2u). (2) Dual Learning: By aligning the dual
quantizations of users and ads, the constructed semantic IDs for users and ads
achieve stronger alignment. Finally, we conduct extensive offline experiments
and online A/B tests to evaluate DAS's effectiveness, which is now successfully
deployed across various advertising scenarios at Kuaishou App, serving over 400
million users daily.

</details>


### [145] [FuXi-β: Towards a Lightweight and Fast Large-Scale Generative Recommendation Model](https://arxiv.org/abs/2508.10615)
*Yufei Ye,Wei Guo,Hao Wang,Hong Zhu,Yuyang Ye,Yong Liu,Huifeng Guo,Ruiming Tang,Defu Lian,Enhong Chen*

Main category: cs.IR

TL;DR: This paper introduces FuXi-$"beta", a new recommendation model that improves upon FuXi-$"alpha" by addressing efficiency bottlenecks and removing redundant attention maps. FuXi-$"beta" achieves significant acceleration and outperforms previous state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Scaling laws for autoregressive generative recommenders reveal potential for larger, more versatile systems but mean greater latency and training costs. To accelerate training and inference, the authors investigated the recent generative recommendation models HSTU and FuXi-$"alpha", identifying two efficiency bottlenecks: the indexing operations in relative temporal attention bias and the computation of the query-key attention map. Additionally, they observed that relative attention bias in self-attention mechanisms can also serve as attention maps, raising the question of whether some attention maps are redundant.

Method: The authors introduce Functional Relative Attention Bias, which avoids the time-consuming operations of the original relative attention bias. They remove the query-key attention map from the original self-attention layer and design a new Attention-Free Token Mixer module. They apply this framework to FuXi-$"alpha", and introduce a new model, FuXi-$"beta".

Result: FuXi-$"beta" achieves significant acceleration compared to FuXi-$"alpha", with an improvement of 27% to 47% in the NDCG@10 metric on large-scale industrial datasets.

Conclusion: FuXi-$"beta" outperforms previous state-of-the-art models and achieves significant acceleration compared to FuXi-$"alpha", while also adhering to the scaling law. Notably, FuXi-$"beta" shows an improvement of 27% to 47% in the NDCG@10 metric on large-scale industrial datasets compared to FuXi-$"alpha".

Abstract: Scaling laws for autoregressive generative recommenders reveal potential for
larger, more versatile systems but mean greater latency and training costs. To
accelerate training and inference, we investigated the recent generative
recommendation models HSTU and FuXi-$\alpha$, identifying two efficiency
bottlenecks: the indexing operations in relative temporal attention bias and
the computation of the query-key attention map. Additionally, we observed that
relative attention bias in self-attention mechanisms can also serve as
attention maps. Previous works like Synthesizer have shown that alternative
forms of attention maps can achieve similar performance, naturally raising the
question of whether some attention maps are redundant. Through empirical
experiments, we discovered that using the query-key attention map might degrade
the model's performance in recommendation tasks. To address these bottlenecks,
we propose a new framework applicable to Transformer-like recommendation
models. On one hand, we introduce Functional Relative Attention Bias, which
avoids the time-consuming operations of the original relative attention bias,
thereby accelerating the process. On the other hand, we remove the query-key
attention map from the original self-attention layer and design a new
Attention-Free Token Mixer module. Furthermore, by applying this framework to
FuXi-$\alpha$, we introduce a new model, FuXi-$\beta$. Experiments across
multiple datasets demonstrate that FuXi-$\beta$ outperforms previous
state-of-the-art models and achieves significant acceleration compared to
FuXi-$\alpha$, while also adhering to the scaling law. Notably, FuXi-$\beta$
shows an improvement of 27% to 47% in the NDCG@10 metric on large-scale
industrial datasets compared to FuXi-$\alpha$. Our code is available in a
public repository: https://github.com/USTC-StarTeam/FuXi-beta

</details>


### [146] [Hypercomplex Prompt-aware Multimodal Recommendation](https://arxiv.org/abs/2508.10753)
*Zheyu Chen,Jinfeng Xu,Hewei Wang,Shuo Yang,Zitong Wan,Haibo Hu*

Main category: cs.IR

TL;DR: HPMRec is a novel Hypercomplex Prompt-aware Multimodal Recommendation framework that overcomes the limitations of existing methods by using hypercomplex embeddings, hypercomplex multiplication, and a prompt-aware compensation mechanism. It achieves state-of-the-art recommendation performance.


<details>
  <summary>Details</summary>
Motivation: Modern recommender systems face critical challenges in handling information overload while addressing the inherent limitations of multimodal representation learning. Existing methods suffer from three fundamental limitations: (1) restricted ability to represent rich multimodal features through a single representation, (2) existing linear modality fusion strategies ignore the deep nonlinear correlations between modalities, and (3) static optimization methods failing to dynamically mitigate the over-smoothing problem in graph convolutional network (GCN).

Method: HPMRec, a novel Hypercomplex Prompt-aware Multimodal Recommendation framework, which utilizes hypercomplex embeddings in the form of multi-components to enhance the representation diversity of multimodal features. HPMRec adopts the hypercomplex multiplication to naturally establish nonlinear cross-modality interactions to bridge semantic gaps, which is beneficial to explore the cross-modality features. HPMRec also introduces the prompt-aware compensation mechanism to aid the misalignment between components and modality-specific features loss, and this mechanism fundamentally alleviates the over-smoothing problem. It further designs self-supervised learning tasks that enhance representation diversity and align different modalities.

Result: HPMRec achieves state-of-the-art recommendation performance.

Conclusion: HPMRec achieves state-of-the-art recommendation performance on four public datasets.

Abstract: Modern recommender systems face critical challenges in handling information
overload while addressing the inherent limitations of multimodal representation
learning. Existing methods suffer from three fundamental limitations: (1)
restricted ability to represent rich multimodal features through a single
representation, (2) existing linear modality fusion strategies ignore the deep
nonlinear correlations between modalities, and (3) static optimization methods
failing to dynamically mitigate the over-smoothing problem in graph
convolutional network (GCN). To overcome these limitations, we propose HPMRec,
a novel Hypercomplex Prompt-aware Multimodal Recommendation framework, which
utilizes hypercomplex embeddings in the form of multi-components to enhance the
representation diversity of multimodal features. HPMRec adopts the hypercomplex
multiplication to naturally establish nonlinear cross-modality interactions to
bridge semantic gaps, which is beneficial to explore the cross-modality
features. HPMRec also introduces the prompt-aware compensation mechanism to aid
the misalignment between components and modality-specific features loss, and
this mechanism fundamentally alleviates the over-smoothing problem. It further
designs self-supervised learning tasks that enhance representation diversity
and align different modalities. Extensive experiments on four public datasets
show that HPMRec achieves state-of-the-art recommendation performance.

</details>


### [147] [CrossDenoise: Denoising Implicit Feedback via a Lightweight Entity-Aware Synergistic Framework](https://arxiv.org/abs/2508.10851)
*Ze Liu,Xianquan Wang,Shuochen Liu,Jie Ma,Huibo Xu,Yupeng Han,Zhe Yang,Kai Zhang,Longfei Li,Jun Zhou*

Main category: cs.IR

TL;DR: CrossDenoise是一种新颖且轻量级的框架，通过分离用户、项目和交互特定因素来解决噪声问题，在多个数据集上优于现有技术，且计算开销可忽略不计。


<details>
  <summary>Details</summary>
Motivation: 推荐系统严重依赖隐式反馈，但由于误报和漏报，隐式反馈本质上是有噪声的，这严重降低了推荐的准确性。现有的去噪策略通常忽略实体感知建模，计算开销高，或者需要过多的超参数调整，限制了它们在现实世界中的适用性。

Method: 通过将噪声估计分解为用户、项目和交互特定因素，CrossDenoise计算实体声誉因子（用户/项目可靠性）。

Result: 在ML-1M、Yelp和Amazon-book数据集上进行的大量实验表明，CrossDenoise始终且显著地优于最先进的基线。例如，它在Yelp上使用NeuMF实现了高达27.01%的NDCG@50增益，同时产生的计算和内存开销可以忽略不计。

Conclusion: CrossDenoise有效地分离了干净样本和噪声样本，并且在不同的超参数设置下保持稳健。它为隐式反馈去噪提供了一种实用且可扩展的解决方案。

Abstract: Recommender systems heavily rely on implicit feedback, which is inherently
noisy due to false positives and negatives, severely degrading recommendation
accuracy. Existing denoising strategies often overlook entity-aware modeling,
suffer from high computational overhead, or demand excessive hyperparameter
tuning, limiting their real-world applicability. We propose CrossDenoise, a
novel and lightweight framework that addresses these challenges by
disentangling noise estimation into user-, item-, and interaction-specific
factors. Leveraging empirical observations that show significant heterogeneity
in user and item noise propensities, CrossDenoise computes entity reputation
factors (user/item reliability) via a rank-based linear mapping of average
training losses. These are fused with interaction-level weights derived from an
empirical cumulative distribution function (ECDF) of individual losses. This
design is model-agnostic, computationally efficient, and requires only two
intuitive hyperparameters. Extensive experiments on ML-1M, Yelp, and
Amazon-book datasets, across GMF, NeuMF, and CDAE backbones, demonstrate that
CrossDenoise consistently and significantly outperforms state-of-the-art
baselines. For instance, it achieves up to 27.01% NDCG@50 gain on Yelp with
NeuMF, while incurring negligible computational and memory overhead. Our
analysis confirms that CrossDenoise effectively separates clean from noisy
samples and remains robust under varied hyperparameter settings. It offers a
practical and scalable solution for denoising implicit feedback.

</details>


### [148] [Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data](https://arxiv.org/abs/2508.09636)
*Lalitesh Morishetti,Abhay Kumar,Jonathan Scott,Kaushiki Nag,Gunjan Sharma,Shanu Vashishtha,Rahul Sridhar,Rohit Chatter,Kannan Achan*

Main category: cs.IR

TL;DR: 本文提出了一种新颖的模型架构，用于优化个性化产品搜索排序，通过结合表格和非表格数据以及多任务学习框架，实验结果表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 优化个性化产品搜索排序。

Method: 利用多任务学习框架，结合表格和非表格数据，使用预训练的TinyBERT模型进行语义嵌入，并采用新颖的抽样技术捕捉多样化的客户行为。

Result: 该模型在个性化产品搜索排序方面表现出更高的性能。

Conclusion: 结合非表格数据与先进嵌入技术的多任务学习范式显著提升了模型性能，并通过消融实验验证了相关性标签、微调TinyBERT层以及TinyBERT查询-产品嵌入交互的优势。

Abstract: In this paper, we present a novel model architecture for optimizing
personalized product search ranking using a multi-task learning (MTL)
framework. Our approach uniquely integrates tabular and non-tabular data,
leveraging a pre-trained TinyBERT model for semantic embeddings and a novel
sampling technique to capture diverse customer behaviors. We evaluate our model
against several baselines, including XGBoost, TabNet, FT-Transformer, DCN-V2,
and MMoE, focusing on their ability to handle mixed data types and optimize
personalized ranking. Additionally, we propose a scalable relevance labeling
mechanism based on click-through rates, click positions, and semantic
similarity, offering an alternative to traditional human-annotated labels.
Experimental results show that combining non-tabular data with advanced
embedding techniques in multi-task learning paradigm significantly enhances
model performance. Ablation studies further underscore the benefits of
incorporating relevance labels, fine-tuning TinyBERT layers, and TinyBERT
query-product embedding interactions. These results demonstrate the
effectiveness of our approach in achieving improved personalized product search
ranking.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [149] [OpenFPL: An open-source forecasting method rivaling state-of-the-art Fantasy Premier League services](https://arxiv.org/abs/2508.09992)
*Daniel Groos*

Main category: cs.LG

TL;DR: OpenFPL, an open-source forecasting method, rivals commercial services in accuracy for Fantasy Premier League player performance, especially for high-return players.


<details>
  <summary>Details</summary>
Motivation: Access to accurate performance forecasts gives participants an edge over competitors, but high-accuracy forecasts are currently limited to commercial services with undisclosed workings and proprietary data. This paper aims to democratize access to highly accurate forecasts.

Method: OpenFPL, an open-source Fantasy Premier League forecasting method developed exclusively from public data, comprising position-specific ensemble models optimized on Fantasy Premier League and Understat data from four previous seasons (2020-21 to 2023-24).

Result: OpenFPL achieves accuracy comparable to a leading commercial service when tested prospectively on data from the 2024-25 season and surpasses the commercial benchmark for high-return players. These findings hold across one-, two-, and three-gameweek forecast horizons.

Conclusion: OpenFPL achieves comparable accuracy to a leading commercial service and surpasses it for high-return players.

Abstract: Fantasy Premier League engages the football community in selecting the
Premier League players who will perform best from gameweek to gameweek. Access
to accurate performance forecasts gives participants an edge over competitors
by guiding expectations about player outcomes and reducing uncertainty in squad
selection. However, high-accuracy forecasts are currently limited to commercial
services whose inner workings are undisclosed and that rely on proprietary
data. This paper aims to democratize access to highly accurate forecasts of
player performance by presenting OpenFPL, an open-source Fantasy Premier League
forecasting method developed exclusively from public data. Comprising
position-specific ensemble models optimized on Fantasy Premier League and
Understat data from four previous seasons (2020-21 to 2023-24), OpenFPL
achieves accuracy comparable to a leading commercial service when tested
prospectively on data from the 2024-25 season. OpenFPL also surpasses the
commercial benchmark for high-return players ($>$ 2 points), which are most
influential for rank gains. These findings hold across one-, two-, and
three-gameweek forecast horizons, supporting long-term planning of transfers
and strategies while also informing final-day decisions.

</details>


### [150] [xRFM: Accurate, scalable, and interpretable feature learning models for tabular data](https://arxiv.org/abs/2508.10053)
*Daniel Beaglehole,David Holzmüller,Adityanarayanan Radhakrishnan,Mikhail Belkin*

Main category: cs.LG

TL;DR: xRFM combines feature learning kernel machines with a tree structure to achieve state-of-the-art performance on tabular data, outperforming GBDTs and providing interpretability.


<details>
  <summary>Details</summary>
Motivation: developing state-of-the-art methods for tabular data based on recent developments in neural networks and feature learning methods.

Method: xRFM, an algorithm that combines feature learning kernel machines with a tree structure

Result: xRFM achieves best performance across 100 regression datasets and is competitive to the best methods across 200 classification datasets outperforming GBDTs.

Conclusion: xRFM achieves best performance across 100 regression datasets and is competitive to the best methods across 200 classification datasets outperforming GBDTs. Additionally, xRFM provides interpretability natively through the Average Gradient Outer Product.

Abstract: Inference from tabular data, collections of continuous and categorical
variables organized into matrices, is a foundation for modern technology and
science. Yet, in contrast to the explosive changes in the rest of AI, the best
practice for these predictive tasks has been relatively unchanged and is still
primarily based on variations of Gradient Boosted Decision Trees (GBDTs). Very
recently, there has been renewed interest in developing state-of-the-art
methods for tabular data based on recent developments in neural networks and
feature learning methods. In this work, we introduce xRFM, an algorithm that
combines feature learning kernel machines with a tree structure to both adapt
to the local structure of the data and scale to essentially unlimited amounts
of training data.
  We show that compared to $31$ other methods, including recently introduced
tabular foundation models (TabPFNv2) and GBDTs, xRFM achieves best performance
across $100$ regression datasets and is competitive to the best methods across
$200$ classification datasets outperforming GBDTs. Additionally, xRFM provides
interpretability natively through the Average Gradient Outer Product.

</details>


### [151] [A Personalized Exercise Assistant using Reinforcement Learning (PEARL): Results from a four-arm Randomized-controlled Trial](https://arxiv.org/abs/2508.10060)
*Amy Armento Lee,Narayan Hegde,Nina Deliu,Emily Rosenzweig,Arun Suggala,Sriram Lakshminarasimhan,Qian He,John Hernandez,Martin Seneviratne,Rahul Singh,Pradnesh Kalkar,Karthikeyan Shanmugam,Aravindan Raghuveer,Abhimanyu Singh,My Nguyen,James Taylor,Jatin Alla,Sofia S. Villar,Hulya Emir-Farinas*

Main category: cs.LG

TL;DR: This study used reinforcement learning to personalize nudges in a Fitbit app, resulting in increased physical activity compared to other methods.


<details>
  <summary>Details</summary>
Motivation: Consistent physical inactivity poses a major global health challenge. Mobile health (mHealth) interventions, particularly Just-in-Time Adaptive Interventions (JITAIs), offer a promising avenue for scalable, personalized physical activity (PA) promotion.

Method: A four-arm randomized controlled trial was conducted with 13,463 Fitbit users, comparing control, random, fixed, and reinforcement learning (RL) nudge strategies.

Result: The RL group showed a significant increase in average daily step count at 1 month compared to control (+296 steps, p=0.0002), random (+218 steps, p=0.005), and fixed (+238 steps, p=0.002). At 2 months, the RL group sustained a significant increase compared to the control group (+210 steps, p=0.0122).

Conclusion: This study demonstrates the potential of a scalable, behaviorally-informed RL approach to personalize digital health interventions for PA.

Abstract: Consistent physical inactivity poses a major global health challenge. Mobile
health (mHealth) interventions, particularly Just-in-Time Adaptive
Interventions (JITAIs), offer a promising avenue for scalable, personalized
physical activity (PA) promotion. However, developing and evaluating such
interventions at scale, while integrating robust behavioral science, presents
methodological hurdles. The PEARL study was the first large-scale, four-arm
randomized controlled trial to assess a reinforcement learning (RL) algorithm,
informed by health behavior change theory, to personalize the content and
timing of PA nudges via a Fitbit app.
  We enrolled and randomized 13,463 Fitbit users into four study arms: control,
random, fixed, and RL. The control arm received no nudges. The other three arms
received nudges from a bank of 155 nudges based on behavioral science
principles. The random arm received nudges selected at random. The fixed arm
received nudges based on a pre-set logic from survey responses about PA
barriers. The RL group received nudges selected by an adaptive RL algorithm. We
included 7,711 participants in primary analyses (mean age 42.1, 86.3% female,
baseline steps 5,618.2).
  We observed an increase in PA for the RL group compared to all other groups
from baseline to 1 and 2 months. The RL group had significantly increased
average daily step count at 1 month compared to all other groups: control (+296
steps, p=0.0002), random (+218 steps, p=0.005), and fixed (+238 steps,
p=0.002). At 2 months, the RL group sustained a significant increase compared
to the control group (+210 steps, p=0.0122). Generalized estimating equation
models also revealed a sustained increase in daily steps in the RL group vs.
control (+208 steps, p=0.002). These findings demonstrate the potential of a
scalable, behaviorally-informed RL approach to personalize digital health
interventions for PA.

</details>


### [152] [Measuring Time Series Forecast Stability for Demand Planning](https://arxiv.org/abs/2508.10063)
*Steven Klee,Yuntian Xia*

Main category: cs.LG

TL;DR: This paper studies model-induced stochasticity and proposes the need for further study of forecast stability for models in production systems. Ensemble models improve stability without significantly deteriorating forecast accuracy.


<details>
  <summary>Details</summary>
Motivation: Demand planners often value consistency and stability over incremental accuracy improvements. Forecasts that vary drastically from one planning cycle to the next require high amounts of human intervention.

Method: Case study measuring the stability and accuracy of state-of-the-art forecasting models on public data sets.

Result: Ensemble models improve stability without significantly deteriorating forecast accuracy.

Conclusion: Ensemble models improve stability without significantly deteriorating forecast accuracy. The paper proposes the need for further study of forecast stability for models deployed in production systems.

Abstract: Time series forecasting is a critical first step in generating demand plans
for supply chains. Experiments on time series models typically focus on
demonstrating improvements in forecast accuracy over existing/baseline
solutions, quantified according to some accuracy metric. There is no doubt that
forecast accuracy is important; however in production systems, demand planners
often value consistency and stability over incremental accuracy improvements.
Assuming that the inputs have not changed significantly, forecasts that vary
drastically from one planning cycle to the next require high amounts of human
intervention, which frustrates demand planners and can even cause them to lose
trust in ML forecasting models. We study model-induced stochasticity, which
quantifies the variance of a set of forecasts produced by a single model when
the set of inputs is fixed. Models with lower variance are more stable.
  Recently the forecasting community has seen significant advances in forecast
accuracy through the development of deep machine learning models for time
series forecasting. We perform a case study measuring the stability and
accuracy of state-of-the-art forecasting models (Chronos, DeepAR, PatchTST,
Temporal Fusion Transformer, TiDE, and the AutoGluon best quality ensemble) on
public data sets from the M5 competition and Favorita grocery sales. We show
that ensemble models improve stability without significantly deteriorating (or
even improving) forecast accuracy. While these results may not be surprising,
the main point of this paper is to propose the need for further study of
forecast stability for models that are being deployed in production systems.

</details>


### [153] [Constrained Decoding of Diffusion LLMs with Context-Free Grammars](https://arxiv.org/abs/2508.10111)
*Niels Mündler,Jasper Dekoninck,Martin Vechev*

Main category: cs.LG

TL;DR: 提出了一种用于扩散模型的约束解码方法，该方法可以处理上下文无关文法捕获的形式语言，并在各种应用中实现了接近完美的语法正确性，同时始终保持或提高功能正确性，并且计算开销保持在实际水平。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在不同领域表现出良好的性能。LLM的许多实际应用需要遵守形式语言指定的语法约束。然而，由于其概率性质，LLM的输出不能保证遵守这种形式语言。以前的工作已经提出了约束解码作为将LLM生成限制为特定形式语言的一种手段。然而，现有的工作不适用于扩散LLM的新兴范式，当在诸如生成形式上正确的C++或JSON输出等实际场景中使用时。

Method: 将约束解码简化为更一般的加性填充问题，然后将其简化为判断目标语言和正则语言的交集是否为空的任务，并提出了一种有效的算法来解决上下文无关语言的问题。

Result: 该方法实现了接近完美的语法正确性，同时始终保持或提高功能正确性。

Conclusion: 该方法在各种应用中实现了接近完美的语法正确性，同时始终保持或提高功能正确性，并且计算开销保持在实际水平。

Abstract: Large language models (LLMs) have shown promising performance across diverse
domains. Many practical applications of LLMs, such as code completion and
structured data extraction, require adherence to syntactic constraints
specified by a formal language. Yet, due to their probabilistic nature, LLM
output is not guaranteed to adhere to such formal languages. Prior work has
proposed constrained decoding as a means to restrict LLM generation to
particular formal languages. However, existing works are not applicable to the
emerging paradigm of diffusion LLMs, when used in practical scenarios such as
the generation of formally correct C++ or JSON output. In this paper we address
this challenge and present the first constrained decoding method for diffusion
models, one that can handle formal languages captured by context-free grammars.
We begin by reducing constrained decoding to the more general additive
infilling problem, which asks whether a partial output can be completed to a
valid word in the target language. This problem also naturally subsumes the
previously unaddressed multi-region infilling constrained decoding. We then
reduce this problem to the task of deciding whether the intersection of the
target language and a regular language is empty and present an efficient
algorithm to solve it for context-free languages. Empirical results on various
applications, such as C++ code infilling and structured data extraction in
JSON, demonstrate that our method achieves near-perfect syntactic correctness
while consistently preserving or improving functional correctness. Importantly,
our efficiency optimizations ensure that the computational overhead remains
practical.

</details>


### [154] [Less is More: Learning Graph Tasks with Just LLMs](https://arxiv.org/abs/2508.10115)
*Sola Shirai,Kavitha Srinivas,Julian Dolby,Michael Katz,Horst Samulowitz,Shirin Sohrabi*

Main category: cs.LG

TL;DR: LLMs can learn to solve graph tasks with chain-of-thought training and generalize to new tasks and graph structures.


<details>
  <summary>Details</summary>
Motivation: Reasoning over graphs could help solve many problems for large language models (LLMs). The merits of prior approaches remain unclear.

Method: Training LLMs with instructive chain-of-thought solutions.

Result: Even small LLMs can learn to solve graph tasks, and this training generalizes to new tasks and graph structures without specialized graph encoders.

Conclusion: Small LLMs can learn to solve graph tasks by training them with instructive chain-of-thought solutions, and this training generalizes, without specialized graph encoders, to new tasks and graph structures.

Abstract: For large language models (LLMs), reasoning over graphs could help solve many
problems. Prior work has tried to improve LLM graph reasoning by examining how
best to serialize graphs as text and by combining GNNs and LLMs. However, the
merits of such approaches remain unclear, so we empirically answer the
following research questions: (1) Can LLMs learn to solve fundamental graph
tasks without specialized graph encoding models?, (2) Can LLMs generalize
learned solutions to unseen graph structures or tasks?, and (3) What are the
merits of competing approaches to learn graph tasks? We show that even small
LLMs can learn to solve graph tasks by training them with instructive
chain-of-thought solutions, and this training generalizes, without specialized
graph encoders, to new tasks and graph structures.

</details>


### [155] [From Intent to Execution: Multimodal Chain-of-Thought Reinforcement Learning for Precise CAD Code Generation](https://arxiv.org/abs/2508.10118)
*Ke Niu,Haiyang Yu,Zhuofan Chen,Mengyang Zhao,Teng Fu,Bin Li,Xiangyang Xue*

Main category: cs.LG

TL;DR: CAD-RL is proposed to solve the challenge of directly translating human design intent into executable CAD code, by using a multimodal Chain-of-Thought (CoT) guided reinforcement learning post training framework. The method combines CoT-based Cold Start with goal-driven reinforcement learning post training using three task-specific rewards: executability reward, geometric accuracy reward, and external evaluation reward. To support training and benchmarking, the authors release ExeCAD, a noval dataset comprising 16,540 real-world CAD examples.


<details>
  <summary>Details</summary>
Motivation: automating parametric 3D modeling

Method: a multimodal Chain-of-Thought (CoT) guided reinforcement learning post training framework

Result: CAD-RL achieves significant improvements in reasoning quality, output precision, and code executability over existing VLMs.

Conclusion: CAD-RL achieves significant improvements in reasoning quality, output precision, and code executability over existing VLMs.

Abstract: Computer-Aided Design (CAD) plays a vital role in engineering and
manufacturing, yet current CAD workflows require extensive domain expertise and
manual modeling effort. Recent advances in large language models (LLMs) have
made it possible to generate code from natural language, opening new
opportunities for automating parametric 3D modeling. However, directly
translating human design intent into executable CAD code remains highly
challenging, due to the need for logical reasoning, syntactic correctness, and
numerical precision. In this work, we propose CAD-RL, a multimodal
Chain-of-Thought (CoT) guided reinforcement learning post training framework
for CAD modeling code generation. Our method combines CoT-based Cold Start with
goal-driven reinforcement learning post training using three task-specific
rewards: executability reward, geometric accuracy reward, and external
evaluation reward. To ensure stable policy learning under sparse and
high-variance reward conditions, we introduce three targeted optimization
strategies: Trust Region Stretch for improved exploration, Precision Token Loss
for enhanced dimensions parameter accuracy, and Overlong Filtering to reduce
noisy supervision. To support training and benchmarking, we release ExeCAD, a
noval dataset comprising 16,540 real-world CAD examples with paired natural
language and structured design language descriptions, executable CADQuery
scripts, and rendered 3D models. Experiments demonstrate that CAD-RL achieves
significant improvements in reasoning quality, output precision, and code
executability over existing VLMs.

</details>


### [156] [Nested-ReFT: Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts](https://arxiv.org/abs/2508.10123)
*Maxime Heuillet,Yufei Cui,Boxing Chen,Audrey Durand,Prasanna Parthasarathi*

Main category: cs.LG

TL;DR: Nested-ReFT通过使用模型的部分层作为行为模型来降低ReFT的计算成本，从而在数学推理任务中实现了更高的效率，同时保持了性能。


<details>
  <summary>Details</summary>
Motivation: 标准的ReFT框架在训练期间生成补全的计算成本很高，尤其是在具有多个推理步骤的挑战性领域中。

Method: 提出了一种新的ReFT框架，Nested-ReFT，其中目标模型的一部分层作为行为模型，以生成off-policy的补全。

Result: Nested-ReFT在tokens/sec指标上表现出更高的计算效率，并且通过偏差缓解技术，性能与基线ReFT性能相匹配。

Conclusion: Nested-ReFT在多个数学推理基准和模型尺寸上提高了计算效率，同时保持了与基线ReFT相当的性能。

Abstract: Advanced reasoning in LLMs on challenging domains like mathematical reasoning
can be tackled using verifiable rewards based reinforced fine-tuning (ReFT). In
standard ReFT frameworks, a behavior model generates multiple completions with
answers per problem, for the answer to be then scored by a reward function.
While such RL post-training methods demonstrate significant performance
improvements across challenging reasoning domains, the computational cost of
generating completions during training with multiple inference steps makes the
training cost non-trivial. To address this, we draw inspiration from off-policy
RL, and speculative decoding to introduce a novel ReFT framework, dubbed
Nested-ReFT, where a subset of layers of the target model acts as the behavior
model to generate off-policy completions during training. The behavior model
configured with dynamic layer skipping per batch during training decreases the
inference cost compared to the standard ReFT frameworks. Our theoretical
analysis shows that Nested-ReFT yields unbiased gradient estimates with
controlled variance. Our empirical analysis demonstrates improved computational
efficiency measured as tokens/sec across multiple math reasoning benchmarks and
model sizes. Additionally, we explore three variants of bias mitigation to
minimize the off-policyness in the gradient updates that allows for maintaining
performance that matches the baseline ReFT performance.

</details>


### [157] [rETF-semiSL: Semi-Supervised Learning for Neural Collapse in Temporal Data](https://arxiv.org/abs/2508.10147)
*Yuhan Xie,William Cappelletti,Mahsa Shoaran,Pascal Frossard*

Main category: cs.LG

TL;DR: 提出了一种新的半监督预训练策略，该策略通过利用旋转等角紧框架分类器、伪标记和生成式pretext任务，强制执行潜在表示，从而在时间序列分类任务中实现卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列的深度神经网络必须捕获复杂的时间模式，才能有效地表示动态数据。Self-和半监督学习方法在预训练大型模型方面显示出希望的结果，当针对分类进行微调时，这些模型通常胜过从头开始训练的同类模型。但是，pretext训练任务的选择通常是启发式的，并且它们向下游分类的可转移性无法保证。

Method: 提出了一种新的半监督预训练策略，以强制执行满足在优化训练的神经分类器中观察到的神经崩溃现象的潜在表示。我们使用旋转等角紧框架分类器和伪标记来预训练具有少量标记样本的深度编码器。此外，为了在强制嵌入可分性的同时有效地捕获时间动态，我们将生成式pretext任务与我们的方法集成，并且我们定义了一种新的顺序增强策略。

Result: 该方法在应用于LSTM、transformer和状态空间模型时，显著优于先前的pretext任务。

Conclusion: 该方法在三个多元时间序列分类数据集上显著优于先前的pretext任务，突出了将预训练目标与理论上有根据的嵌入几何对齐的好处。

Abstract: Deep neural networks for time series must capture complex temporal patterns,
to effectively represent dynamic data. Self- and semi-supervised learning
methods show promising results in pre-training large models, which -- when
finetuned for classification -- often outperform their counterparts trained
from scratch. Still, the choice of pretext training tasks is often heuristic
and their transferability to downstream classification is not granted, thus we
propose a novel semi-supervised pre-training strategy to enforce latent
representations that satisfy the Neural Collapse phenomenon observed in
optimally trained neural classifiers. We use a rotational equiangular tight
frame-classifier and pseudo-labeling to pre-train deep encoders with few
labeled samples. Furthermore, to effectively capture temporal dynamics while
enforcing embedding separability, we integrate generative pretext tasks with
our method, and we define a novel sequential augmentation strategy. We show
that our method significantly outperforms previous pretext tasks when applied
to LSTMs, transformers, and state-space models on three multivariate time
series classification datasets. These results highlight the benefit of aligning
pre-training objectives with theoretically grounded embedding geometry.

</details>


### [158] [Out-of-Distribution Detection using Counterfactual Distance](https://arxiv.org/abs/2508.10148)
*Maria Stoica,Francesco Leofante,Alessio Lomuscio*

Main category: cs.LG

TL;DR: This paper introduces a new OOD detection method using counterfactual explanations to measure the distance to decision boundaries, achieving state-of-the-art or better performance on several datasets. It also improves scalability by computing counterfactuals in the embedding space.


<details>
  <summary>Details</summary>
Motivation: Accurate and explainable out-of-distribution (OOD) detection is required to use machine learning systems safely. Previous work has shown that feature distance to decision boundaries can be used to identify OOD data effectively.

Method: The paper proposes a post-hoc OOD detection method that calculates the distance to decision boundaries using counterfactual explanations. Strategies to improve scalability by computing counterfactuals directly in embedding space are also proposed.

Result: The method achieves 93.50% AUROC and 25.80% FPR95 on CIFAR-10, 97.05% AUROC and 13.79% FPR95 on CIFAR-100, and 92.55% AUROC and 33.55% FPR95 on ImageNet-200.

Conclusion: The method achieves state-of-the-art results on CIFAR-10 and outperforms existing methods on CIFAR-100 and ImageNet-200 across four OOD datasets.

Abstract: Accurate and explainable out-of-distribution (OOD) detection is required to
use machine learning systems safely. Previous work has shown that feature
distance to decision boundaries can be used to identify OOD data effectively.
In this paper, we build on this intuition and propose a post-hoc OOD detection
method that, given an input, calculates the distance to decision boundaries by
leveraging counterfactual explanations. Since computing explanations can be
expensive for large architectures, we also propose strategies to improve
scalability by computing counterfactuals directly in embedding space.
Crucially, as the method employs counterfactual explanations, we can seamlessly
use them to help interpret the results of our detector. We show that our method
is in line with the state of the art on CIFAR-10, achieving 93.50% AUROC and
25.80% FPR95. Our method outperforms these methods on CIFAR-100 with 97.05%
AUROC and 13.79% FPR95 and on ImageNet-200 with 92.55% AUROC and 33.55% FPR95
across four OOD datasets

</details>


### [159] [Characterizing Evolution in Expectation-Maximization Estimates for Overspecified Mixed Linear Regression](https://arxiv.org/abs/2508.10154)
*Zhankun Luo,Abolfazl Hashemi*

Main category: cs.LG

TL;DR: 本文研究了过度指定混合模型中EM算法的收敛性，发现收敛速度取决于混合权重的初始值。


<details>
  <summary>Details</summary>
Motivation: 模型错误指定是混合模型中一个持续存在的挑战，当模型具有比数据分布中更多的混合成分时，就会发生这种情况。因此，本文旨在从理论上理解期望最大化 (EM) 算法在这种模型错误指定情况下的行为。

Method: 使用期望最大化 (EM) 算法，在总体层面和有限样本层面分析了过度指定的双组分混合线性回归 (2MLR) 模型的行为。

Result: 在总体层面，证明了回归参数的线性收敛和次线性收敛，其收敛速度取决于混合权重的初始猜测。在有限样本层面，对于充分不平衡的固定混合权重，统计精度为 $O((d/n)^{1/2})$，而对于充分平衡的固定混合权重，精度为 $O((d/n)^{1/4})$。

Conclusion: 在线性混合回归模型中，当模型过度指定时，EM算法的收敛速度取决于混合权重的初始猜测。不平衡的初始猜测导致线性收敛，而平衡的初始猜测导致次线性收敛。

Abstract: Mixture models have attracted significant attention due to practical
effectiveness and comprehensive theoretical foundations. A persisting challenge
is model misspecification, which occurs when the model to be fitted has more
mixture components than those in the data distribution. In this paper, we
develop a theoretical understanding of the Expectation-Maximization (EM)
algorithm's behavior in the context of targeted model misspecification for
overspecified two-component Mixed Linear Regression (2MLR) with unknown
$d$-dimensional regression parameters and mixing weights. In Theorem 5.1 at the
population level, with an unbalanced initial guess for mixing weights, we
establish linear convergence of regression parameters in $O(\log(1/\epsilon))$
steps. Conversely, with a balanced initial guess for mixing weights, we observe
sublinear convergence in $O(\epsilon^{-2})$ steps to achieve the
$\epsilon$-accuracy at Euclidean distance. In Theorem 6.1 at the finite-sample
level, for mixtures with sufficiently unbalanced fixed mixing weights, we
demonstrate a statistical accuracy of $O((d/n)^{1/2})$, whereas for those with
sufficiently balanced fixed mixing weights, the accuracy is $O((d/n)^{1/4})$
given $n$ data samples. Furthermore, we underscore the connection between our
population level and finite-sample level results: by setting the desired final
accuracy $\epsilon$ in Theorem 5.1 to match that in Theorem 6.1 at the
finite-sample level, namely letting $\epsilon = O((d/n)^{1/2})$ for
sufficiently unbalanced fixed mixing weights and $\epsilon = O((d/n)^{1/4})$
for sufficiently balanced fixed mixing weights, we intuitively derive iteration
complexity bounds $O(\log (1/\epsilon))=O(\log (n/d))$ and
$O(\epsilon^{-2})=O((n/d)^{1/2})$ at the finite-sample level for sufficiently
unbalanced and balanced initial mixing weights. We further extend our analysis
in overspecified setting to low SNR regime.

</details>


### [160] [Confounding is a Pervasive Problem in Real World Recommender Systems](https://arxiv.org/abs/2508.10479)
*Alexander Merkov,David Rohde,Alexandre Gilotte,Benjamin Heymann*

Main category: cs.LG

TL;DR: 未观察到的混淆会影响推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 当一个未测量的特征同时影响治疗和结果时，就会出现未观察到的混淆，从而导致有偏差的因果效应估计。推荐系统利用完全观察到的数据似乎不受此问题的影响。然而，推荐系统中的许多标准做法导致观察到的特征被忽略，从而导致同样的问题。

Method: 模拟研究

Result: 特征工程、A/B 测试和模块化会将混淆引入推荐系统并阻碍其性能。

Conclusion: 常见的做法（如特征工程、A/B 测试和模块化）实际上会将混淆引入推荐系统并阻碍其性能。提供了该现象的几个例子，并得到了模拟研究的支持，并提出了关于从业者如何减少或避免实际系统中混淆影响的实用建议。

Abstract: Unobserved confounding arises when an unmeasured feature influences both the
treatment and the outcome, leading to biased causal effect estimates. This
issue undermines observational studies in fields like economics, medicine,
ecology or epidemiology. Recommender systems leveraging fully observed data
seem not to be vulnerable to this problem. However many standard practices in
recommender systems result in observed features being ignored, resulting in
effectively the same problem. This paper will show that numerous common
practices such as feature engineering, A/B testing and modularization can in
fact introduce confounding into recommendation systems and hamper their
performance. Several illustrations of the phenomena are provided, supported by
simulation studies with practical suggestions about how practitioners may
reduce or avoid the affects of confounding in real systems.

</details>


### [161] [Benchmark-Driven Selection of AI: Evidence from DeepSeek-R1](https://arxiv.org/abs/2508.10173)
*Petr Spelda,Vit Stritecky*

Main category: cs.LG

TL;DR: 人工智能的开发受到有影响力的基准的驱动，这使得测试任务的新颖性成为衡量推理模型泛化能力的关键。


<details>
  <summary>Details</summary>
Motivation: 在观察到推理语言模型可以将它们现有的能力组合成新的中间步骤轨迹，并且这些轨迹有时可以帮助它们比过去的模型更好地泛化之后，对推理语言模型的评估变得重要起来。随着推理成为大型语言模型的下一个扩展维度，需要仔细研究它们在关键任务中的能力。

Method: 使用来自“人类最后考试”的序贯决策问题来展示基准驱动的选择对 DeepSeek-R1 的影响。

Result: 展示了基准驱动的选择对 DeepSeek-R1 的影响。

Conclusion: 更好的性能不总是由测试时算法的改进或模型大小引起的，而是通过使用有影响力的基准作为学习的课程来实现。通过有影响力的基准来指导人工智能的开发，将评估转化为学习，并使测试任务的新颖性成为衡量推理模型泛化能力的关键。

Abstract: Evaluation of reasoning language models gained importance after it was
observed that they can combine their existing capabilities into novel traces of
intermediate steps before task completion and that the traces can sometimes
help them to generalize better than past models. As reasoning becomes the next
scaling dimension of large language models, careful study of their capabilities
in critical tasks is needed. We show that better performance is not always
caused by test-time algorithmic improvements or model sizes but also by using
impactful benchmarks as curricula for learning. We call this benchmark-driven
selection of AI and show its effects on DeepSeek-R1 using our sequential
decision-making problem from Humanity's Last Exam. Steering development of AI
by impactful benchmarks trades evaluation for learning and makes novelty of
test tasks key for measuring generalization capabilities of reasoning models.
Consequently, some benchmarks could be seen as curricula for training rather
than unseen test sets.

</details>


### [162] [An Explainable AI based approach for Monitoring Animal Health](https://arxiv.org/abs/2508.10210)
*Rahul Janaa,Shubham Dixit,Mrityunjay Sharma,Ritesh Kumar*

Main category: cs.LG

TL;DR: This paper uses machine learning to monitor cattle activity using accelerometer data and IoT devices, achieving high accuracy with a k-NN classifier and using SHAP for explainability.


<details>
  <summary>Details</summary>
Motivation: Monitoring cattle health and optimizing yield are key challenges faced by dairy farmers due to difficulties in tracking all animals on the farm. This work aims to showcase modern data-driven farming practices based on explainable machine learning(ML) methods that explain the activity and behaviour of dairy cattle (cows). Continuous data collection of 3-axis accelerometer sensors and usage of robust ML methodologies and algorithms, provide farmers and researchers with actionable information on cattle activity, allowing farmers to make informed decisions and incorporate sustainable practices.

Method: This study utilizes Bluetooth-based Internet of Things (IoT) devices and 4G networks for seamless data transmission, immediate analysis, inference generation, and explains the models performance with explainability frameworks. Special emphasis is put on the pre-processing of the accelerometers time series data, including the extraction of statistical characteristics, signal processing techniques, and lag-based features using the sliding window technique. Various hyperparameter-optimized ML models are evaluated across varying window lengths for activity classification.

Result: The k-nearest neighbour Classifier achieved the best performance, with AUC of mean 0.98 and standard deviation of 0.0026 on the training set and 0.99 on testing set).

Conclusion: The k-nearest neighbour Classifier achieved the best performance, with AUC of mean 0.98 and standard deviation of 0.0026 on the training set and 0.99 on testing set). In order to ensure transparency, Explainable AI based frameworks such as SHAP is used to interpret feature importance that can be understood and used by practitioners. A detailed comparison of the important features, along with the stability analysis of selected features, supports development of explainable and practical ML models for sustainable livestock management.

Abstract: Monitoring cattle health and optimizing yield are key challenges faced by
dairy farmers due to difficulties in tracking all animals on the farm. This
work aims to showcase modern data-driven farming practices based on explainable
machine learning(ML) methods that explain the activity and behaviour of dairy
cattle (cows). Continuous data collection of 3-axis accelerometer sensors and
usage of robust ML methodologies and algorithms, provide farmers and
researchers with actionable information on cattle activity, allowing farmers to
make informed decisions and incorporate sustainable practices. This study
utilizes Bluetooth-based Internet of Things (IoT) devices and 4G networks for
seamless data transmission, immediate analysis, inference generation, and
explains the models performance with explainability frameworks. Special
emphasis is put on the pre-processing of the accelerometers time series data,
including the extraction of statistical characteristics, signal processing
techniques, and lag-based features using the sliding window technique. Various
hyperparameter-optimized ML models are evaluated across varying window lengths
for activity classification. The k-nearest neighbour Classifier achieved the
best performance, with AUC of mean 0.98 and standard deviation of 0.0026 on the
training set and 0.99 on testing set). In order to ensure transparency,
Explainable AI based frameworks such as SHAP is used to interpret feature
importance that can be understood and used by practitioners. A detailed
comparison of the important features, along with the stability analysis of
selected features, supports development of explainable and practical ML models
for sustainable livestock management.

</details>


### [163] [AI-Driven Detection and Analysis of Handwriting on Seized Ivory: A Tool to Uncover Criminal Networks in the Illicit Wildlife Trade](https://arxiv.org/abs/2508.10219)
*Will Fein,Ryan J. Horwitz,John E. Brown III,Amit Misra,Felipe Oviedo,Kevin White,Juan M. Lavista Ferres,Samuel K. Wasser*

Main category: cs.LG

TL;DR: 利用AI分析象牙上的手写标记，可以低成本地追踪走私网络，为野生动物保护提供新方法。


<details>
  <summary>Details</summary>
Motivation: 跨国象牙贸易导致非洲象数量下降，贩运网络难以摧毁。象牙上的手写标记是法证证据的重要来源，但很少被记录或分析。基因数据昂贵且难以获得。

Method: 利用对象检测模型从6085张照片中提取超过17000个标记，并使用AI工具进行标记和描述，识别出184个重复出现的“签名标记”。

Result: 在多次查获的象牙中观察到20个签名标记，通过参与两批货运的贩运者，在这些查获的象牙之间建立了法证联系。

Conclusion: AI驱动的笔迹分析可以有效连接不同象牙走私案件，填补其他数据源的空白，具有变革野生动物取证的潜力。

Abstract: The transnational ivory trade continues to drive the decline of elephant
populations across Africa, and trafficking networks remain difficult to
disrupt. Tusks seized by law enforcement officials carry forensic information
on the traffickers responsible for their export, including DNA evidence and
handwritten markings made by traffickers. For 20 years, analyses of tusk DNA
have identified where elephants were poached and established connections among
shipments of ivory. While the links established using genetic evidence are
extremely conclusive, genetic data is expensive and sometimes impossible to
obtain. But though handwritten markings are easy to photograph, they are rarely
documented or analyzed. Here, we present an AI-driven pipeline for extracting
and analyzing handwritten markings on seized elephant tusks, offering a novel,
scalable, and low-cost source of forensic evidence. Having collected 6,085
photographs from eight large seizures of ivory over a 6-year period
(2014-2019), we used an object detection model to extract over 17,000
individual markings, which were then labeled and described using
state-of-the-art AI tools. We identified 184 recurring "signature markings"
that connect the tusks on which they appear. 20 signature markings were
observed in multiple seizures, establishing forensic links between these
seizures through traffickers involved in both shipments. This work complements
other investigative techniques by filling in gaps where other data sources are
unavailable. The study demonstrates the transformative potential of AI in
wildlife forensics and highlights practical steps for integrating handwriting
analysis into efforts to disrupt organized wildlife crime.

</details>


### [164] [Comparison of D-Wave Quantum Annealing and Markov Chain Monte Carlo for Sampling from a Probability Distribution of a Restricted Boltzmann Machine](https://arxiv.org/abs/2508.10228)
*Abdelmoula El Yazizi,Samee U. Khan,Yaroslav Koshka*

Main category: cs.LG

TL;DR: D-Wave采样在RBM中未见显著优势，但暗示经典-量子结合的潜力。


<details>
  <summary>Details</summary>
Motivation: 评估D-Wave量子退火器在RBM采样中的性能，并解释先前研究未能利用D-Wave采样实现显著改进的原因。

Method: 采用以局部谷（LV）为中心的方法评估受限玻尔兹曼机（RBM）的采样质量。

Result: D-Wave采样在高概率采样状态下的互补性较低，但在中间概率值的局部极小值发现方面，两种技术存在差异。在训练后期，两种技术的重叠较少。

Conclusion: D-Wave采样未能显著增加局部极小值的数量，但与Gibbs采样相比，D-Wave在训练后期阶段的互补性降低，这表明结合经典-量子方法可能存在改进的潜力。

Abstract: A local-valley (LV) centered approach to assessing the quality of sampling
from Restricted Boltzmann Machines (RBMs) was applied to the latest generation
of the D-Wave quantum annealer. D-Wave and Gibbs samples from a classically
trained RBM were obtained at conditions relevant to the
contrastive-divergence-based RBM learning. The samples were compared for the
number of the LVs to which they belonged and the energy of the corresponding
local minima. No significant (desirable) increase in the number of the LVs has
been achieved by decreasing the D-Wave annealing time. At any training epoch,
the states sampled by the D-Wave belonged to a somewhat higher number of LVs
than in the Gibbs sampling. However, many of those LVs found by the two
techniques differed. For high-probability sampled states, the two techniques
were (unfavorably) less complementary and more overlapping. Nevertheless, many
potentially "important" local minima, i.e., those having intermediate, even if
not high, probability values, were found by only one of the two sampling
techniques while missed by the other. The two techniques overlapped less at
later than earlier training epochs, which is precisely the stage of the
training when modest improvements to the sampling quality could make meaningful
differences for the RBM trainability. The results of this work may explain the
failure of previous investigations to achieve substantial (or any) improvement
when using D-Wave-based sampling. However, the results reveal some potential
for improvement, e.g., using a combined classical-quantum approach.

</details>


### [165] [Interpretable Machine Learning Model for Early Prediction of Acute Kidney Injury in Critically Ill Patients with Cirrhosis: A Retrospective Study](https://arxiv.org/abs/2508.10233)
*Li Sun,Shuheng Chen,Junyi Fan,Yong Si,Minoo Ahmadi,Elham Pishgar,Kamiar Alaei,Maryam Pishgar*

Main category: cs.LG

TL;DR: 本研究开发了一种基于LightGBM的可解释机器学习模型，用于早期预测ICU中肝硬化患者的AKI风险，该模型具有较高的准确性和阴性预测值。


<details>
  <summary>Details</summary>
Motivation: 肝硬化是一种进行性肝病，死亡率高，并发症频繁，特别是急性肾损伤（AKI），发生在高达50％的住院患者中，并使预后恶化。AKI源于复杂的血流动力学、炎症和代谢变化，因此早期检测至关重要。许多预测工具缺乏准确性、可解释性以及与重症监护病房（ICU）工作流程的一致性。本研究旨在开发一种可解释的机器学习模型，用于早期预测危重肝硬化患者的AKI。

Method: 对MIMIC-IV v2.2数据库进行了回顾性分析，确定了1240名患有肝硬化的成人ICU患者，并排除了ICU停留时间少于48小时或缺少关键数据的患者。提取了前48小时的实验室和生理变量。该流程包括预处理、缺失值过滤、LASSO特征选择和SMOTE类别平衡。使用AUROC、准确率、F1分数、灵敏度、特异性和预测值训练和评估了六种算法——LightGBM、CatBoost、XGBoost、logistic回归、naive Bayes和神经网络。

Result: LightGBM取得了最佳性能（AUROC 0.808，95％CI 0.741-0.856；准确率0.704；NPV 0.911）。关键预测因子包括凝血酶原部分时间延长、未在院外放置20G导管、pH值低和pO2改变，这与已知的肝硬化-AKI机制一致，并表明存在可操作的靶点。

Conclusion: LightGBM模型能够使用常规临床变量对ICU中的肝硬化患者进行准确的早期AKI风险分层。其高阴性预测值支持对低风险患者进行安全降级，可解释性有助于培养临床医生的信任和有针对性的预防。需要进行外部验证并整合到电子健康记录系统中。

Abstract: Background: Cirrhosis is a progressive liver disease with high mortality and
frequent complications, notably acute kidney injury (AKI), which occurs in up
to 50% of hospitalized patients and worsens outcomes. AKI stems from complex
hemodynamic, inflammatory, and metabolic changes, making early detection
essential. Many predictive tools lack accuracy, interpretability, and alignment
with intensive care unit (ICU) workflows. This study developed an interpretable
machine learning model for early AKI prediction in critically ill patients with
cirrhosis.
  Methods: We conducted a retrospective analysis of the MIMIC-IV v2.2 database,
identifying 1240 adult ICU patients with cirrhosis and excluding those with ICU
stays under 48 hours or missing key data. Laboratory and physiological
variables from the first 48 hours were extracted. The pipeline included
preprocessing, missingness filtering, LASSO feature selection, and SMOTE class
balancing. Six algorithms-LightGBM, CatBoost, XGBoost, logistic regression,
naive Bayes, and neural networks-were trained and evaluated using AUROC,
accuracy, F1-score, sensitivity, specificity, and predictive values.
  Results: LightGBM achieved the best performance (AUROC 0.808, 95% CI
0.741-0.856; accuracy 0.704; NPV 0.911). Key predictors included prolonged
partial thromboplastin time, absence of outside-facility 20G placement, low pH,
and altered pO2, consistent with known cirrhosis-AKI mechanisms and suggesting
actionable targets.
  Conclusion: The LightGBM-based model enables accurate early AKI risk
stratification in ICU patients with cirrhosis using routine clinical variables.
Its high negative predictive value supports safe de-escalation for low-risk
patients, and interpretability fosters clinician trust and targeted prevention.
External validation and integration into electronic health record systems are
warranted.

</details>


### [166] [Can Transformers Break Encryption Schemes via In-Context Learning?](https://arxiv.org/abs/2508.10235)
*Jathin Korrapati,Patrick Mendoza,Aditya Tomar,Abein Abraham*

Main category: cs.LG

TL;DR: This paper explores the use of in-context learning (ICL) to learn cryptographic functions, specifically mono-alphabetic substitution and Vigenere ciphers.


<details>
  <summary>Details</summary>
Motivation: To propose a novel application of ICL into the domain of cryptographic function learning, specifically focusing on ciphers such as mono-alphabetic substitution and Vigenere ciphers

Method: A novel application of ICL into the domain of cryptographic function learning, specifically focusing on ciphers such as mono-alphabetic substitution and Vigenere ciphers, two classes of private-key encryption schemes.

Result: The goal is for the model to infer the underlying substitution and decode a new cipher text word given a small set of (cipher text, plain text) pairs.

Conclusion: Transformers can perform tasks by conditioning on a small number of examples presented at inference time, without any parameter updates.

Abstract: In-context learning (ICL) has emerged as a powerful capability of
transformer-based language models, enabling them to perform tasks by
conditioning on a small number of examples presented at inference time, without
any parameter updates. Prior work has shown that transformers can generalize
over simple function classes like linear functions, decision trees, even neural
networks, purely from context, focusing on numerical or symbolic reasoning over
underlying well-structured functions. Instead, we propose a novel application
of ICL into the domain of cryptographic function learning, specifically
focusing on ciphers such as mono-alphabetic substitution and Vigen\`ere
ciphers, two classes of private-key encryption schemes. These ciphers involve a
fixed but hidden bijective mapping between plain text and cipher text
characters. Given a small set of (cipher text, plain text) pairs, the goal is
for the model to infer the underlying substitution and decode a new cipher text
word. This setting poses a structured inference challenge, which is well-suited
for evaluating the inductive biases and generalization capabilities of
transformers under the ICL paradigm. Code is available at
https://github.com/adistomar/CS182-project.

</details>


### [167] [Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on Transformer Models](https://arxiv.org/abs/2508.10243)
*Taibiao Zhao,Mingxuan Sun,Hao Wang,Xiaobing Chen,Xiangwei Zhou*

Main category: cs.LG

TL;DR: HPMI: a novel retraining-free backdoor attack on transformers that does not alter the model's architecture.


<details>
  <summary>Details</summary>
Motivation: Prior backdoor attack methods typically rely on retraining with clean data or altering the model architecture, both of which can be resource-intensive and intrusive.

Method: Head-wise Pruning and Malicious Injection (HPMI)

Result: HPMI incurs negligible clean accuracy loss, achieves at least 99.55% attack success rate, and bypasses four advanced defense mechanisms.

Conclusion: HPMI achieves greater concealment and robustness against diverse defense strategies, while maintaining minimal impact on clean accuracy.

Abstract: Transformer models have demonstrated exceptional performance and have become
indispensable in computer vision (CV) and natural language processing (NLP)
tasks. However, recent studies reveal that transformers are susceptible to
backdoor attacks. Prior backdoor attack methods typically rely on retraining
with clean data or altering the model architecture, both of which can be
resource-intensive and intrusive. In this paper, we propose Head-wise Pruning
and Malicious Injection (HPMI), a novel retraining-free backdoor attack on
transformers that does not alter the model's architecture. Our approach
requires only a small subset of the original data and basic knowledge of the
model architecture, eliminating the need for retraining the target transformer.
Technically, HPMI works by pruning the least important head and injecting a
pre-trained malicious head to establish the backdoor. We provide a rigorous
theoretical justification demonstrating that the implanted backdoor resists
detection and removal by state-of-the-art defense techniques, under reasonable
assumptions. Experimental evaluations across multiple datasets further validate
the effectiveness of HPMI, showing that it 1) incurs negligible clean accuracy
loss, 2) achieves at least 99.55% attack success rate, and 3) bypasses four
advanced defense mechanisms. Additionally, relative to state-of-the-art
retraining-dependent attacks, HPMI achieves greater concealment and robustness
against diverse defense strategies, while maintaining minimal impact on clean
accuracy.

</details>


### [168] [Convergence Analysis of Max-Min Exponential Neural Network Operators in Orlicz Space](https://arxiv.org/abs/2508.10248)
*Satyaranjan Pradhan,Madan Mohan Soren*

Main category: cs.LG

TL;DR: 本文提出了一种Max Min方法，用于使用指数神经网络算子逼近函数，并研究了其逼近性质和收敛行为。


<details>
  <summary>Details</summary>
Motivation: 利用指数神经网络算子逼近函数。

Method: 提出了一种使用指数神经网络算子逼近函数的Max Min方法，并开发了Max Min Kantorovich型指数神经网络算子。

Result: 研究了单变量函数的逐点收敛和一致收敛。为了分析收敛的阶数，使用了对数连续模量并估计了相应的收敛速度。通过合适的核函数和sigmoid激活函数，提供了一些图形表示来说明函数的逼近误差。

Conclusion: 研究了在Orlicz空间设置中，Max Min Kantorovich型指数神经网络算子的收敛行为。

Abstract: In this current work, we propose a Max Min approach for approximating
functions using exponential neural network operators. We extend this framework
to develop the Max Min Kantorovich-type exponential neural network operators
and investigate their approximation properties. We study both pointwise and
uniform convergence for univariate functions. To analyze the order of
convergence, we use the logarithmic modulus of continuity and estimate the
corresponding rate of convergence. Furthermore, we examine the convergence
behavior of the Max Min Kantorovich type exponential neural network operators
within the Orlicz space setting. We provide some graphical representations to
illustrate the approximation error of the function through suitable kernel and
sigmoidal activation functions.

</details>


### [169] [Multi-Agent Reinforcement Learning for Adaptive Resource Orchestration in Cloud-Native Clusters](https://arxiv.org/abs/2508.10253)
*Guanzi Yao,Heyao Liu,Linyan Dai*

Main category: cs.LG

TL;DR: This paper introduces a multi-agent RL method for adaptive resource orchestration in cloud-native databases, improving resource utilization, scheduling, and stability.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges of high resource dynamism and scheduling complexity in cloud-native database systems.

Method: Adaptive resource orchestration method based on multi-agent reinforcement learning with heterogeneous role-based agent modeling and reward-shaping mechanism.

Result: The proposed method outperforms traditional approaches across multiple key metrics, including resource utilization, scheduling latency, policy convergence speed, system stability, and fairness.

Conclusion: The proposed multi-agent reinforcement learning method outperforms traditional approaches in cloud-native database systems, demonstrating strong generalization and practical utility for orchestration tasks.

Abstract: This paper addresses the challenges of high resource dynamism and scheduling
complexity in cloud-native database systems. It proposes an adaptive resource
orchestration method based on multi-agent reinforcement learning. The method
introduces a heterogeneous role-based agent modeling mechanism. This allows
different resource entities, such as compute nodes, storage nodes, and
schedulers, to adopt distinct policy representations. These agents are better
able to reflect diverse functional responsibilities and local environmental
characteristics within the system. A reward-shaping mechanism is designed to
integrate local observations with global feedback. This helps mitigate policy
learning bias caused by incomplete state observations. By combining real-time
local performance signals with global system value estimation, the mechanism
improves coordination among agents and enhances policy convergence stability. A
unified multi-agent training framework is developed and evaluated on a
representative production scheduling dataset. Experimental results show that
the proposed method outperforms traditional approaches across multiple key
metrics. These include resource utilization, scheduling latency, policy
convergence speed, system stability, and fairness. The results demonstrate
strong generalization and practical utility. Across various experimental
scenarios, the method proves effective in handling orchestration tasks with
high concurrency, high-dimensional state spaces, and complex dependency
relationships. This confirms its advantages in real-world, large-scale
scheduling environments.

</details>


### [170] [Federated Anomaly Detection for Multi-Tenant Cloud Platforms with Personalized Modeling](https://arxiv.org/abs/2508.10255)
*Yuxi Wang,Heyao Liu,Nyutian Long,Guanzi Yao*

Main category: cs.LG

TL;DR: Federated learning enhances anomaly detection in clouds, improving privacy and accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing data privacy, heterogeneous resource behavior, and limitations of centralized modeling in multi-tenant clouds.

Method: A federated training framework with personalized parameter adjustment and Mahalanobis distance-based anomaly scoring is used.

Result: The method outperforms existing models in Precision, Recall, and F1-Score, maintaining stable performance under varying conditions.

Conclusion: The proposed federated learning-based anomaly detection method demonstrates superior performance compared to existing models in cloud environments, showing potential for intelligent resource monitoring.

Abstract: This paper proposes an anomaly detection method based on federated learning
to address key challenges in multi-tenant cloud environments, including data
privacy leakage, heterogeneous resource behavior, and the limitations of
centralized modeling. The method establishes a federated training framework
involving multiple tenants. Each tenant trains the model locally using private
resource usage data. Through parameter aggregation, a global model is
optimized, enabling cross-tenant collaborative anomaly detection while
preserving data privacy. To improve adaptability to diverse resource usage
patterns, a personalized parameter adjustment mechanism is introduced. This
allows the model to retain tenant-specific feature representations while
sharing global knowledge. In the model output stage, the Mahalanobis distance
is used to compute anomaly scores. This enhances both the accuracy and
stability of anomaly detection. The experiments use real telemetry data from a
cloud platform to construct a simulated multi-tenant environment. The study
evaluates the model's performance under varying participation rates and noise
injection levels. These comparisons demonstrate the proposed method's
robustness and detection accuracy. Experimental results show that the proposed
method outperforms existing mainstream models across key metrics such as
Precision, Recall, and F1-Score. It also maintains stable performance in
various complex scenarios. These findings highlight the method's practical
potential for intelligent resource monitoring and anomaly diagnosis in cloud
computing environments.

</details>


### [171] [Source Component Shift Adaptation via Offline Decomposition and Online Mixing Approach](https://arxiv.org/abs/2508.10257)
*Ryuta Matsuno*

Main category: cs.LG

TL;DR: Proposes a source component shift adaptation method with offline decomposition and online mixing to address the limitations of existing online learning and model-pool-based methods. Achieves superior adaptation performance with significant loss reduction.


<details>
  <summary>Details</summary>
Motivation: Existing online learning methods often fail to utilize recurring shifts effectively, while model-pool-based methods struggle to capture individual source components, leading to poor adaptation.

Method: A source component shift adaptation method via an offline decomposition and online mixing approach. It determines prediction models offline through the EM algorithm and updates the mixing weight of the prediction models online through online convex optimization.

Result: Theoretically identifies that the problem can be divided into two subproblems: offline source component decomposition and online mixing weight adaptation. Outperforms baselines, reducing the cumulative test loss by up to 67.4%.

Conclusion: The proposed method achieves superior adaptation performance over existing methods, reducing the cumulative test loss by up to 67.4% on various real-world regression datasets.

Abstract: This paper addresses source component shift adaptation, aiming to update
predictions adapting to source component shifts for incoming data streams based
on past training data. Existing online learning methods often fail to utilize
recurring shifts effectively, while model-pool-based methods struggle to
capture individual source components, leading to poor adaptation. In this
paper, we propose a source component shift adaptation method via an offline
decomposition and online mixing approach. We theoretically identify that the
problem can be divided into two subproblems: offline source component
decomposition and online mixing weight adaptation. Based on this, our method
first determines prediction models, each of which learns a source component
solely based on past training data offline through the EM algorithm. Then, it
updates the mixing weight of the prediction models for precise prediction
through online convex optimization. Thanks to our theoretical derivation, our
method fully leverages the characteristics of the shifts, achieving superior
adaptation performance over existing methods. Experiments conducted on various
real-world regression datasets demonstrate that our method outperforms
baselines, reducing the cumulative test loss by up to 67.4%.

</details>


### [172] [Uncertainty-Aware Prediction of Parkinson's Disease Medication Needs: A Two-Stage Conformal Prediction Approach](https://arxiv.org/abs/2508.10284)
*Ricardo Diaz-Rincon,Muxuan Liang,Adolfo Ramirez-Zamora,Benjamin Shickel*

Main category: cs.LG

TL;DR: 开发了一种预测帕金森病患者药物需求的框架，该框架通过量化预测不确定性来改进药物剂量决策。


<details>
  <summary>Details</summary>
Motivation: 帕金森病 (PD) 的药物管理面临独特的挑战，因为疾病进展和治疗反应具有异质性。临床医生不仅需要预测未来药物需求，还需要可靠的置信度指标。没有量化的不确定性，调整可能会导致过早升级到最大剂量或长期不足的症状控制。

Method: 使用来自佛罗里达大学健康中心（2011-2021 年）631 名住院患者的电子健康记录，采用了一种两阶段方法，该方法首先识别可能需要改变药物的患者，然后预测所需的左旋多巴等效日剂量调整。

Result: 该框架实现了边际覆盖率，同时减少了预测区间长度，与传统方法相比，为短期计划提供了精确的预测，为长期预测提供了更宽的范围。

Conclusion: 开发了一个一致性预测框架，能够提前两年预测帕金森病患者的药物需求，并提供可靠的预测区间和统计保证。通过量化不确定性，该方法能够实现基于证据的左旋多巴剂量决策，从而优化症状控制，同时最大限度地减少副作用，并提高生活质量。

Abstract: Parkinson's Disease (PD) medication management presents unique challenges due
to heterogeneous disease progression and treatment response. Neurologists must
balance symptom control with optimal dopaminergic dosing based on functional
disability while minimizing side effects. This balance is crucial as inadequate
or abrupt changes can cause levodopa-induced dyskinesia, wearing off, and
neuropsychiatric effects, significantly reducing quality of life. Current
approaches rely on trial-and-error decisions without systematic predictive
methods. Despite machine learning advances, clinical adoption remains limited
due to reliance on point predictions that do not account for prediction
uncertainty, undermining clinical trust and utility. Clinicians require not
only predictions of future medication needs but also reliable confidence
measures. Without quantified uncertainty, adjustments risk premature escalation
to maximum doses or prolonged inadequate symptom control. We developed a
conformal prediction framework anticipating medication needs up to two years in
advance with reliable prediction intervals and statistical guarantees. Our
approach addresses zero-inflation in PD inpatient data, where patients maintain
stable medication regimens between visits. Using electronic health records from
631 inpatient admissions at University of Florida Health (2011-2021), our
two-stage approach identifies patients likely to need medication changes, then
predicts required levodopa equivalent daily dose adjustments. Our framework
achieved marginal coverage while reducing prediction interval lengths compared
to traditional approaches, providing precise predictions for short-term
planning and wider ranges for long-term forecasting. By quantifying
uncertainty, our approach enables evidence-based decisions about levodopa
dosing, optimizing symptom control while minimizing side effects and improving
life quality.

</details>


### [173] [SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic Representation Learning](https://arxiv.org/abs/2508.10298)
*Weijian Mai,Jiamin Wu,Yu Zhu,Zhouheng Yao,Dongzhan Zhou,Andrew F. Luo,Qihao Zheng,Wanli Ouyang,Chunfeng Song*

Main category: cs.LG

TL;DR: SynBrain 是一种生成框架，可以模拟从视觉语义到神经反应的转换，超越了最先进的方法在特定对象的视觉到 fMRI 编码性能。


<details>
  <summary>Details</summary>
Motivation: 破译视觉刺激如何转化为皮质反应是计算神经科学中的一项基本挑战。这种视觉到神经的映射本质上是一种一对多的关系，因为相同的视觉输入在试验、情境和对象之间可靠地引起不同的血流动力学反应。然而，现有的确定性方法难以同时对这种生物变异性进行建模，同时捕捉编码刺激信息的潜在功能一致性。

Method: 我们提出了 SynBrain，这是一个生成框架，可以以概率和生物学上可解释的方式模拟从视觉语义到神经反应的转换。SynBrain 引入了两个关键组件：（i）BrainVAE 通过概率学习将神经表示建模为连续概率分布，同时通过视觉语义约束保持功能一致性；（ii）语义到神经映射器充当语义传输通路，将视觉语义投影到神经反应流形中，以促进高保真 fMRI 合成。

Result: 实验结果表明，SynBrain 在特定对象的视觉到 fMRI 编码性能方面超越了最先进的方法。此外，SynBrain 可以通过少量数据有效地适应新的对象，并合成高质量的 fMRI 信号，从而有效地提高数据有限的 fMRI 到图像解码性能。

Conclusion: SynBrain揭示了试验和对象之间的功能一致性，合成信号捕捉了由生物神经变异性塑造的可解释模式。

Abstract: Deciphering how visual stimuli are transformed into cortical responses is a
fundamental challenge in computational neuroscience. This visual-to-neural
mapping is inherently a one-to-many relationship, as identical visual inputs
reliably evoke variable hemodynamic responses across trials, contexts, and
subjects. However, existing deterministic methods struggle to simultaneously
model this biological variability while capturing the underlying functional
consistency that encodes stimulus information. To address these limitations, we
propose SynBrain, a generative framework that simulates the transformation from
visual semantics to neural responses in a probabilistic and biologically
interpretable manner. SynBrain introduces two key components: (i) BrainVAE
models neural representations as continuous probability distributions via
probabilistic learning while maintaining functional consistency through visual
semantic constraints; (ii) A Semantic-to-Neural Mapper acts as a semantic
transmission pathway, projecting visual semantics into the neural response
manifold to facilitate high-fidelity fMRI synthesis. Experimental results
demonstrate that SynBrain surpasses state-of-the-art methods in
subject-specific visual-to-fMRI encoding performance. Furthermore, SynBrain
adapts efficiently to new subjects with few-shot data and synthesizes
high-quality fMRI signals that are effective in improving data-limited
fMRI-to-image decoding performance. Beyond that, SynBrain reveals functional
consistency across trials and subjects, with synthesized signals capturing
interpretable patterns shaped by biological neural variability. The code will
be made publicly available.

</details>


### [174] [Improving Learning of New Diseases through Knowledge-Enhanced Initialization for Federated Adapter Tuning](https://arxiv.org/abs/2508.10299)
*Danni Peng,Yuan Wang,Kangning Cai,Peiyan Ning,Jiming Xu,Yong Liu,Rick Siow Mong Goh,Qingsong Wei,Huazhu Fu*

Main category: cs.LG

TL;DR: This paper introduces FedKEI, a federated learning framework that uses past knowledge to help clients quickly adapt to new healthcare tasks. It outperforms existing methods in experiments.


<details>
  <summary>Details</summary>
Motivation: Given the rapidly evolving healthcare environment, it is crucial for individual clients to quickly adapt to new tasks or diseases by tuning adapters while drawing upon past experiences. In healthcare, federated learning (FL) is a widely adopted framework that enables privacy-preserving collaboration among medical institutions. With large foundation models (FMs) demonstrating impressive capabilities, using FMs in FL through cost-efficient adapter tuning has become a popular approach.

Method: Federated Knowledge-Enhanced Initialization (FedKEI), a novel framework that leverages cross-client and cross-task transfer from past knowledge to generate informed initializations for learning new tasks with adapters. FedKEI begins with a global clustering process at the server to generalize knowledge across tasks, followed by the optimization of aggregation weights across clusters (inter-cluster weights) and within each cluster (intra-cluster weights) to personalize knowledge transfer for each new task. To facilitate more effective learning of the inter- and intra-cluster weights, we adopt a bi-level optimization scheme that collaboratively learns the global intra-cluster weights across clients and optimizes the local inter-cluster weights toward each client's task objective.

Result: FedKEI's advantage in adapting to new diseases compared to state-of-the-art methods.

Conclusion: Extensive experiments on three benchmark datasets of different modalities, including dermatology, chest X-rays, and retinal OCT, demonstrate FedKEI's advantage in adapting to new diseases compared to state-of-the-art methods.

Abstract: In healthcare, federated learning (FL) is a widely adopted framework that
enables privacy-preserving collaboration among medical institutions. With large
foundation models (FMs) demonstrating impressive capabilities, using FMs in FL
through cost-efficient adapter tuning has become a popular approach. Given the
rapidly evolving healthcare environment, it is crucial for individual clients
to quickly adapt to new tasks or diseases by tuning adapters while drawing upon
past experiences. In this work, we introduce Federated Knowledge-Enhanced
Initialization (FedKEI), a novel framework that leverages cross-client and
cross-task transfer from past knowledge to generate informed initializations
for learning new tasks with adapters. FedKEI begins with a global clustering
process at the server to generalize knowledge across tasks, followed by the
optimization of aggregation weights across clusters (inter-cluster weights) and
within each cluster (intra-cluster weights) to personalize knowledge transfer
for each new task. To facilitate more effective learning of the inter- and
intra-cluster weights, we adopt a bi-level optimization scheme that
collaboratively learns the global intra-cluster weights across clients and
optimizes the local inter-cluster weights toward each client's task objective.
Extensive experiments on three benchmark datasets of different modalities,
including dermatology, chest X-rays, and retinal OCT, demonstrate FedKEI's
advantage in adapting to new diseases compared to state-of-the-art methods.

</details>


### [175] [A Vision-Language Pre-training Model-Guided Approach for Mitigating Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2508.10315)
*Keke Gai,Dongjue Wang,Jing Yu,Liehuang Zhu,Qi Wu*

Main category: cs.LG

TL;DR: 这篇论文提出了一个名为CLIP-Fed的联邦学习后门防御框架，该框架利用了视觉-语言预训练模型的零样本学习能力，在异构客户端数据分布下防御后门攻击，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习(FL)中现有的后门防御方法依赖于同质客户端数据分布或清洁服务数据集的可用性的假设，这限制了实用性和有效性。在异构客户端数据分布下防御后门攻击，同时保持模型性能仍然是一个重大的挑战。

Method: 该论文提出了一个名为CLIP-Fed的FL后门防御框架，该框架利用了视觉-语言预训练模型的零样本学习能力。通过整合预聚合和后聚合防御策略，CLIP-Fed克服了Non-IID对防御有效性的限制。为了解决隐私问题，并提高数据集对不同触发器的覆盖率，该方法使用多模态大型语言模型和频率分析构建和增强服务器数据集，而无需任何客户端样本。为了解决由后门样本引起的类原型偏差，并消除触发模式和目标标签之间的相关性，CLIP-Fed使用原型对比损失和Kullback-Leibler散度对增强数据集上的全局模型和CLIP的知识进行对齐。

Result: CLIP-Fed在CIFAR-10和CIFAR-10-LT上实现了平均降低ASR，同时平均提高了MA。

Conclusion: CLIP-Fed在代表性数据集上进行了广泛的实验验证，与最先进的方法相比，CLIP-Fed在CIFAR-10上实现了平均降低ASR 2.03%，在CIFAR-10-LT上实现了1.35%，同时平均提高了MA 7.92%和0.48%。

Abstract: Existing backdoor defense methods in Federated Learning (FL) rely on the
assumption of homogeneous client data distributions or the availability of a
clean serve dataset, which limits the practicality and effectiveness. Defending
against backdoor attacks under heterogeneous client data distributions while
preserving model performance remains a significant challenge. In this paper, we
propose a FL backdoor defense framework named CLIP-Fed, which leverages the
zero-shot learning capabilities of vision-language pre-training models. By
integrating both pre-aggregation and post-aggregation defense strategies,
CLIP-Fed overcomes the limitations of Non-IID imposed on defense effectiveness.
To address privacy concerns and enhance the coverage of the dataset against
diverse triggers, we construct and augment the server dataset using the
multimodal large language model and frequency analysis without any client
samples. To address class prototype deviations caused by backdoor samples and
eliminate the correlation between trigger patterns and target labels, CLIP-Fed
aligns the knowledge of the global model and CLIP on the augmented dataset
using prototype contrastive loss and Kullback-Leibler divergence. Extensive
experiments on representative datasets validate the effectiveness of CLIP-Fed.
Compared to state-of-the-art methods, CLIP-Fed achieves an average reduction in
ASR, i.e., 2.03\% on CIFAR-10 and 1.35\% on CIFAR-10-LT, while improving
average MA by 7.92\% and 0.48\%, respectively.

</details>


### [176] [Welfare-Centric Clustering](https://arxiv.org/abs/2508.10345)
*Claire Jie Zhang,Seyed A. Esmaeili,Jamie Morgenstern*

Main category: cs.LG

TL;DR: This paper introduces welfare-centric clustering algorithms with theoretical guarantees that outperform existing fair clustering methods.


<details>
  <summary>Details</summary>
Motivation: Traditional fair clustering notions may yield undesirable or unintuitive clustering outcomes, motivating a welfare-centric clustering approach that models the utilities of the groups.

Method: The paper models group utilities based on distances and proportional representation and formalizes two optimization objectives based on welfare-centric clustering: the Rawlsian (Egalitarian) objective and the Utilitarian objective. Novel algorithms are introduced for both objectives.

Result: The proposed methods significantly outperform existing fair clustering baselines on multiple real-world datasets.

Conclusion: The paper introduces novel algorithms for both Rawlsian and Utilitarian objectives in welfare-centric clustering and proves theoretical guarantees for them. Empirical evaluations demonstrate that the methods outperform existing fair clustering baselines.

Abstract: Fair clustering has traditionally focused on ensuring equitable group
representation or equalizing group-specific clustering costs. However,
Dickerson et al. (2025) recently showed that these fairness notions may yield
undesirable or unintuitive clustering outcomes and advocated for a
welfare-centric clustering approach that models the utilities of the groups. In
this work, we model group utilities based on both distances and proportional
representation and formalize two optimization objectives based on
welfare-centric clustering: the Rawlsian (Egalitarian) objective and the
Utilitarian objective. We introduce novel algorithms for both objectives and
prove theoretical guarantees for them. Empirical evaluations on multiple
real-world datasets demonstrate that our methods significantly outperform
existing fair clustering baselines.

</details>


### [177] [A Hierarchical IDS for Zero-Day Attack Detection in Internet of Medical Things Networks](https://arxiv.org/abs/2508.10346)
*Md Ashraf Uddin,Nam H. Chu,Reza Rafeh*

Main category: cs.LG

TL;DR: Proposes a multi-level IoMT IDS framework to detect zero-day attacks with high accuracy using meta-learning and One Class Classification.


<details>
  <summary>Details</summary>
Motivation: IoMT networks are vulnerable to cyberattacks, and traditional centralized Intrusion Detection Systems (IDSs) are unsuitable due to response delays, privacy risks, and added vulnerabilities. Running IDSs locally on IoMT devices is often infeasible due to limited computation, and even lightweight IDS components remain at risk if updated models are delayed leaving them exposed to zero-day attacks.

Method: A multi-level IoMT IDS framework using meta-learning or One Class Classification (OCC) with the usfAD algorithm is proposed. The first layer filters traffic at a coarse level, and subsequent layers identify attack type and novelty.

Result: The first layer detects zero-day attacks with high accuracy without needing new datasets, ensuring strong applicability in IoMT environments. Additionally, the meta-learning approach achieves high accuracy and F1-score (99.77% and 97.8% respectively).

Conclusion: A multi-level IoMT IDS framework is proposed that can detect zero-day attacks and distinguish between known and unknown threats. Experiments on the CICIoMT2024 dataset show 99.77 percentage accuracy and 97.8 percentage F1-score.

Abstract: The Internet of Medical Things (IoMT) is driving a healthcare revolution but
remains vulnerable to cyberattacks such as denial of service, ransomware, data
hijacking, and spoofing. These networks comprise resource constrained,
heterogeneous devices (e.g., wearable sensors, smart pills, implantables),
making traditional centralized Intrusion Detection Systems (IDSs) unsuitable
due to response delays, privacy risks, and added vulnerabilities. Centralized
IDSs require all sensors to transmit data to a central server, causing delays
or network disruptions in dense environments. Running IDSs locally on IoMT
devices is often infeasible due to limited computation, and even lightweight
IDS components remain at risk if updated models are delayed leaving them
exposed to zero-day attacks that threaten patient health and data security. We
propose a multi level IoMT IDS framework capable of detecting zero day attacks
and distinguishing between known and unknown threats. The first layer (near
Edge) filters traffic at a coarse level (attack or not) using meta-learning or
One Class Classification (OCC) with the usfAD algorithm. Subsequent layers (far
Edge, Cloud) identify attack type and novelty. Experiments on the CICIoMT2024
dataset show 99.77 percentage accuracy and 97.8 percentage F1-score. The first
layer detects zero-day attacks with high accuracy without needing new datasets,
ensuring strong applicability in IoMT environments. Additionally, the
meta-learning approach achieves high.

</details>


### [178] [Semantic Communication with Distribution Learning through Sequential Observations](https://arxiv.org/abs/2508.10350)
*Samer Lahoud,Kinda Khawam*

Main category: cs.LG

TL;DR: 语义通信牺牲了长期可学习性来优化即时语义性能。


<details>
  <summary>Details</summary>
Motivation: 语义通信旨在传达意义而不是完全按位复制，代表了传统通信的范式转变。本文研究了语义通信中的分布学习。

Method: 研究了语义通信中的分布学习，其中接收器必须通过顺序观察来推断潜在的意义分布。

Result: 证明了可学习性需要有效传输矩阵的满秩，描述了分布估计的收敛速度，并量化了估计误差如何转化为语义失真。

Conclusion: 揭示了一个基本的权衡：针对即时语义性能优化的编码方案通常会牺牲长期可学习性。系统条件反射对学习率和可实现性能都有重要影响。

Abstract: Semantic communication aims to convey meaning rather than bit-perfect
reproduction, representing a paradigm shift from traditional communication.
This paper investigates distribution learning in semantic communication where
receivers must infer the underlying meaning distribution through sequential
observations. While semantic communication traditionally optimizes individual
meaning transmission, we establish fundamental conditions for learning source
statistics when priors are unknown. We prove that learnability requires full
rank of the effective transmission matrix, characterize the convergence rate of
distribution estimation, and quantify how estimation errors translate to
semantic distortion. Our analysis reveals a fundamental trade-off: encoding
schemes optimized for immediate semantic performance often sacrifice long-term
learnability. Experiments on CIFAR-10 validate our theoretical framework,
demonstrating that system conditioning critically impacts both learning rate
and achievable performance. These results provide the first rigorous
characterization of statistical learning in semantic communication and offer
design principles for systems that balance immediate performance with
adaptation capability.

</details>


### [179] [eMamba: Efficient Acceleration Framework for Mamba Models in Edge Computing](https://arxiv.org/abs/2508.10370)
*Jiyong Kim,Jaeho Lee,Jiahao Lin,Alish Kanani,Miao Sun,Umit Y. Ogras,Jaehyun Park*

Main category: cs.LG

TL;DR: eMamba is a hardware acceleration framework for deploying Mamba models on edge platforms. It achieves comparable accuracy with state-of-the-art techniques using fewer parameters, lower latency, higher throughput, smaller area, lower power, and lower energy consumption than baseline solutions.


<details>
  <summary>Details</summary>
Motivation: Mamba offers competitive accuracy with superior computational efficiency compared to state-of-the-art transformer models. This advantage makes Mamba particularly promising for resource-constrained edge devices, but no hardware acceleration frameworks are currently optimized for deploying it in such environments.

Method: This paper presents eMamba, a comprehensive end-to-end hardware acceleration framework explicitly designed for deploying Mamba models on edge platforms. eMamba maximizes computational efficiency by replacing complex normalization layers with lightweight hardware-aware alternatives and approximating expensive operations, such as SiLU activation and exponentiation, considering the target applications. Then, it performs an approximation-aware neural architecture search (NAS) to tune the learnable parameters used during approximation.

Result: eMamba achieves comparable accuracy to state-of-the-art techniques using 1.63-19.9 times fewer parameters. Experimental results show 4.95-5.62 times lower latency and 2.22-9.95 times higher throughput, with 4.77 times smaller area, 9.84 times lower power, and 48.6 times lower energy consumption than baseline solutions while maintaining competitive accuracy.

Conclusion: eMamba achieves comparable accuracy to state-of-the-art techniques using fewer parameters. It demonstrates stable perplexity across varying sequence lengths on the WikiText2 dataset. The entire eMamba pipeline is implemented on an AMD ZCU102 FPGA and ASIC using GlobalFoundries (GF) 22 nm technology. Experimental results show lower latency and higher throughput, with smaller area, lower power, and lower energy consumption than baseline solutions while maintaining competitive accuracy.

Abstract: State Space Model (SSM)-based machine learning architectures have recently
gained significant attention for processing sequential data. Mamba, a recent
sequence-to-sequence SSM, offers competitive accuracy with superior
computational efficiency compared to state-of-the-art transformer models. While
this advantage makes Mamba particularly promising for resource-constrained edge
devices, no hardware acceleration frameworks are currently optimized for
deploying it in such environments. This paper presents eMamba, a comprehensive
end-to-end hardware acceleration framework explicitly designed for deploying
Mamba models on edge platforms. eMamba maximizes computational efficiency by
replacing complex normalization layers with lightweight hardware-aware
alternatives and approximating expensive operations, such as SiLU activation
and exponentiation, considering the target applications. Then, it performs an
approximation-aware neural architecture search (NAS) to tune the learnable
parameters used during approximation. Evaluations with Fashion-MNIST, CIFAR-10,
and MARS, an open-source human pose estimation dataset, show eMamba achieves
comparable accuracy to state-of-the-art techniques using 1.63-19.9$\times$
fewer parameters. In addition, it generalizes well to large-scale natural
language tasks, demonstrating stable perplexity across varying sequence lengths
on the WikiText2 dataset. We also quantize and implement the entire eMamba
pipeline on an AMD ZCU102 FPGA and ASIC using GlobalFoundries (GF) 22 nm
technology. Experimental results show 4.95-5.62$\times$ lower latency and
2.22-9.95$\times$ higher throughput, with 4.77$\times$ smaller area,
9.84$\times$ lower power, and 48.6$\times$ lower energy consumption than
baseline solutions while maintaining competitive accuracy.

</details>


### [180] [A Unified Evaluation Framework for Multi-Annotator Tendency Learning](https://arxiv.org/abs/2508.10393)
*Liyun Zhang,Jingcheng Ke,Shenli Fan,Xuanmeng Sha,Zheng Lian*

Main category: cs.LG

TL;DR: 提出了一个用于评估个体倾向学习 (ITL) 方法的统一评估框架，该框架包含 DIC 和 BAE 两种新指标。


<details>
  <summary>Details</summary>
Motivation: 现有的多注释器学习工作侧重于个体倾向学习 (ITL)，但缺乏评估 ITL 方法是否真正捕捉个体倾向并提供有意义的行为解释的评估框架。

Method: 提出了两种新的指标：(1) 注释者间一致性差异 (DIC) 和 (2) 行为对齐可解释性 (BAE)。

Result: 验证了所提出的评估框架的有效性。

Conclusion: 提出了一个统一的评估框架，通过大量实验验证了其有效性。

Abstract: Recent works have emerged in multi-annotator learning that shift focus from
Consensus-oriented Learning (CoL), which aggregates multiple annotations into a
single ground-truth prediction, to Individual Tendency Learning (ITL), which
models annotator-specific labeling behavior patterns (i.e., tendency) to
provide explanation analysis for understanding annotator decisions. However, no
evaluation framework currently exists to assess whether ITL methods truly
capture individual tendencies and provide meaningful behavioral explanations.
To address this gap, we propose the first unified evaluation framework with two
novel metrics: (1) Difference of Inter-annotator Consistency (DIC) quantifies
how well models capture annotator tendencies by comparing predicted
inter-annotator similarity structures with ground-truth; (2) Behavior Alignment
Explainability (BAE) evaluates how well model explanations reflect annotator
behavior and decision relevance by aligning explainability-derived with
ground-truth labeling similarity structures via Multidimensional Scaling (MDS).
Extensive experiments validate the effectiveness of our proposed evaluation
framework.

</details>


### [181] [XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization](https://arxiv.org/abs/2508.10395)
*Aditya Tomar,Coleman Hooper,Minjae Lee,Haocheng Xi,Rishabh Tiwari,Wonjun Kang,Luca Manolache,Michael W. Mahoney,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: XQuant通过低比特量化实现了内存消耗的数量级减少，相对于最先进的KV缓存量化方法，具有显着的精度优势。


<details>
  <summary>Details</summary>
Motivation: 由于大量的内存占用和带宽需求，高效地推断LLM具有挑战性。因此，正在出现新的算法，这些算法以增加的计算来换取减少的内存操作。

Method: 我们通过量化和缓存层输入激活X来实现这一点，而不是使用标准的KV缓存，然后在推理过程中动态地重新实现Keys和Values。

Result: 通过应用XQuant，与FP16基线相比，我们实现了高达$\\{sim 7.7\times$的内存节省，而复杂度降低 $<0.1$。在不同的模型中，XQuant-CL相对于FP16基线实现了高达10$\\{times$的内存节省，而复杂度降低仅为0.01，而内存节省12.5$\\{times$，而复杂度降低仅为$0.1$。

Conclusion: XQuant利用硬件平台快速增长的计算能力来消除内存瓶颈，同时超越了最先进的KV缓存量化方法，并在各种模型上实现了接近FP16的精度。

Abstract: Although LLM inference has emerged as a critical workload for many downstream
applications, efficiently inferring LLMs is challenging due to the substantial
memory footprint and bandwidth requirements. In parallel, compute capabilities
have steadily outpaced both memory capacity and bandwidth over the last few
decades, a trend that remains evident in modern GPU hardware and exacerbates
the challenge of LLM inference. As such, new algorithms are emerging that trade
increased computation for reduced memory operations. To that end, we present
XQuant, which takes advantage of this trend, enabling an order-of-magnitude
reduction in memory consumption through low-bit quantization with substantial
accuracy benefits relative to state-of-the-art KV cache quantization methods.
We accomplish this by quantizing and caching the layer input activations X,
instead of using standard KV caching, and then rematerializing the Keys and
Values on-the-fly during inference. This results in an immediate 2$\times$
memory savings compared to KV caching. By applying XQuant, we achieve up to
$\sim 7.7\times$ memory savings with $<0.1$ perplexity degradation compared to
the FP16 baseline. Furthermore, our approach leverages the fact that X values
are similar across layers. Building on this observation, we introduce
XQuant-CL, which exploits the cross-layer similarity in the X embeddings for
extreme compression. Across different models, XQuant-CL attains up to
10$\times$ memory savings relative to the FP16 baseline with only 0.01
perplexity degradation, and 12.5$\times$ memory savings with only $0.1$
perplexity degradation. XQuant exploits the rapidly increasing compute
capabilities of hardware platforms to eliminate the memory bottleneck, while
surpassing state-of-the-art KV cache quantization methods and achieving
near-FP16 accuracy across a wide range of models.

</details>


### [182] [SC2Arena and StarEvolve: Benchmark and Self-Improvement Framework for LLMs in Complex Decision-Making Tasks](https://arxiv.org/abs/2508.10428)
*Pengbo Shen,Yaqing Wang,Ni Mu,Yao Luan,Runpeng Xie,Senhao Yang,Lexiang Wang,Hao Hu,Shuang Xu,Yiqin Yang,Bo Xu*

Main category: cs.LG

TL;DR: 提出了SC2Arena基准测试和StarEvolve框架，以提高LLM在星际争霸II中的决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法捕捉到星际争霸II的完整复杂性，例如其完整的游戏环境、多样化的行动空间和所有可玩种族。

Method: 提出了一种名为StarEvolve的分层框架，该框架集成了战略规划和战术执行，具有迭代自纠正和通过高质量游戏数据进行微调的持续改进。

Result: 使用SC2Arena进行的综合分析为开发通用智能体提供了宝贵的见解，这在以前的基准测试中是不可能实现的。实验结果表明，我们提出的StarEvolve在战略规划中取得了优异的性能。

Conclusion: StarEvolve在战略规划中表现出色，代码、环境和算法均已公开。

Abstract: Evaluating large language models (LLMs) in complex decision-making is
essential for advancing AI's ability for strategic planning and real-time
adaptation. However, existing benchmarks for tasks like StarCraft II fail to
capture the game's full complexity, such as its complete game context, diverse
action spaces, and all playable races. To address this gap, we present
SC2Arena, a benchmark that fully supports all playable races, low-level action
spaces, and optimizes text-based observations to tackle spatial reasoning
challenges. Complementing this, we introduce StarEvolve, a hierarchical
framework that integrates strategic planning with tactical execution, featuring
iterative self-correction and continuous improvement via fine-tuning on
high-quality gameplay data. Its key components include a
Planner-Executor-Verifier structure to break down gameplay, and a scoring
system for selecting high-quality training samples. Comprehensive analysis
using SC2Arena provides valuable insights into developing generalist agents
that were not possible with previous benchmarks. Experimental results also
demonstrate that our proposed StarEvolve achieves superior performance in
strategic planning. Our code, environment, and algorithms are publicly
available.

</details>


### [183] [Unpacking the Implicit Norm Dynamics of Sharpness-Aware Minimization in Tensorized Models](https://arxiv.org/abs/2508.10435)
*Tianxiao Cao,Kyohei Atarashi,Hisashi Kashima*

Main category: cs.LG

TL;DR: SAM's behavior in tensorized models is analyzed, and Deviation-Aware Scaling (DAS) is proposed to improve performance with less overhead.


<details>
  <summary>Details</summary>
Motivation: exploring the behavior of SAM in more general tensorized or scale-invariant models remains underexplored

Method: gradient flow analysis and Deviation-Aware Scaling (DAS)

Result: SAM's implicit control of Norm Deviation is governed by the covariance between core norms and their gradient magnitudes.

Conclusion: Deviation-Aware Scaling (DAS) achieves competitive or improved performance over SAM with reduced computational overhead.

Abstract: Sharpness-Aware Minimization (SAM) has been proven to be an effective
optimization technique for improving generalization in overparameterized
models. While prior works have explored the implicit regularization of SAM in
simple two-core scale-invariant settings, its behavior in more general
tensorized or scale-invariant models remains underexplored. In this work, we
leverage scale-invariance to analyze the norm dynamics of SAM in general
tensorized models. We introduce the notion of \emph{Norm Deviation} as a global
measure of core norm imbalance, and derive its evolution under SAM using
gradient flow analysis. We show that SAM's implicit control of Norm Deviation
is governed by the covariance between core norms and their gradient magnitudes.
Motivated by these findings, we propose a simple yet effective method,
\emph{Deviation-Aware Scaling (DAS)}, which explicitly mimics this
regularization behavior by scaling core norms in a data-adaptive manner. Our
experiments across tensor completion, noisy training, model compression, and
parameter-efficient fine-tuning confirm that DAS achieves competitive or
improved performance over SAM, while offering reduced computational overhead.

</details>


### [184] [RealAC: A Domain-Agnostic Framework for Realistic and Actionable Counterfactual Explanations](https://arxiv.org/abs/2508.10455)
*Asiful Arefeen,Shovito Barua Soumma,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: RealAC是一种用于生成现实和可操作的反事实的领域无关框架，它通过对齐特征对的联合分布来自动保持复杂的特征间依赖关系，并允许最终用户“冻结”他们不能或不希望更改的属性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常通过刚性的、手工制作的约束或特定领域的知识来加强特征间的依赖关系，这限制了它们的通用性和捕捉数据中固有的复杂、非线性关系的能力。此外，它们很少适应用户指定的偏好，并提出在因果关系上不合理或不可行的解释。

Method: RealAC通过对齐事实和反事实实例之间的特征对的联合分布，自动保留复杂的特征间依赖关系，而无需依赖显式的领域知识。该框架还允许最终用户通过在优化过程中抑制冻结特征的变化来“冻结”他们不能或不希望更改的属性。

Result: 在三个合成数据集和两个真实数据集上的评估表明，RealAC平衡了现实主义和可操作性。我们的方法在因果边缘得分、依赖性保持得分和IM1现实度指标方面优于最先进的基线和基于大型语言模型的反事实生成技术。

Conclusion: RealAC在因果边缘得分、依赖性保持得分和IM1现实度指标方面优于最先进的基线和基于大型语言模型的反事实生成技术，并为因果感知和以用户为中心的反事实生成提供了一个解决方案。

Abstract: Counterfactual explanations provide human-understandable reasoning for
AI-made decisions by describing minimal changes to input features that would
alter a model's prediction. To be truly useful in practice, such explanations
must be realistic and feasible -- they should respect both the underlying data
distribution and user-defined feasibility constraints. Existing approaches
often enforce inter-feature dependencies through rigid, hand-crafted
constraints or domain-specific knowledge, which limits their generalizability
and ability to capture complex, nonlinear relations inherent in data. Moreover,
they rarely accommodate user-specified preferences and suggest explanations
that are causally implausible or infeasible to act upon. We introduce RealAC, a
domain-agnostic framework for generating realistic and actionable
counterfactuals. RealAC automatically preserves complex inter-feature
dependencies without relying on explicit domain knowledge -- by aligning the
joint distributions of feature pairs between factual and counterfactual
instances. The framework also allows end-users to ``freeze'' attributes they
cannot or do not wish to change by suppressing change in frozen features during
optimization. Evaluations on three synthetic and two real datasets demonstrate
that RealAC balances realism with actionability. Our method outperforms
state-of-the-art baselines and Large Language Model-based counterfactual
generation techniques in causal edge score, dependency preservation score, and
IM1 realism metric and offers a solution for causality-aware and user-centric
counterfactual generation.

</details>


### [185] [X-Node: Self-Explanation is All We Need](https://arxiv.org/abs/2508.10461)
*Prajit Sengupta,Islem Rekik*

Main category: cs.LG

TL;DR: X-Node is a self-explaining GNN framework that generates faithful, per-node explanations while maintaining competitive classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing explainability techniques for GNNs are typically post-hoc and global, offering limited insight into individual node decisions or local reasoning. We introduce X-Node, a self-explaining GNN framework in which each node generates its own explanation as part of the prediction process.

Method: We introduce X-Node, a self-explaining GNN framework in which each node generates its own explanation as part of the prediction process. For every node, we construct a structured context vector encoding interpretable cues such as degree, centrality, clustering, feature saliency, and label agreement within its local topology. A lightweight Reasoner module maps this context into a compact explanation vector, which serves three purposes: (1) reconstructing the node's latent embedding via a decoder to enforce faithfulness, (2) generating a natural language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3) guiding the GNN itself via a text-injection mechanism that feeds explanations back into the message-passing pipeline.

Result: Our results show that X-Node maintains competitive classification accuracy while producing faithful, per-node explanations.

Conclusion: X-Node maintains competitive classification accuracy while producing faithful, per-node explanations.

Abstract: Graph neural networks (GNNs) have achieved state-of-the-art results in
computer vision and medical image classification tasks by capturing structural
dependencies across data instances. However, their decision-making remains
largely opaque, limiting their trustworthiness in high-stakes clinical
applications where interpretability is essential. Existing explainability
techniques for GNNs are typically post-hoc and global, offering limited insight
into individual node decisions or local reasoning. We introduce X-Node, a
self-explaining GNN framework in which each node generates its own explanation
as part of the prediction process. For every node, we construct a structured
context vector encoding interpretable cues such as degree, centrality,
clustering, feature saliency, and label agreement within its local topology. A
lightweight Reasoner module maps this context into a compact explanation
vector, which serves three purposes: (1) reconstructing the node's latent
embedding via a decoder to enforce faithfulness, (2) generating a natural
language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3)
guiding the GNN itself via a "text-injection" mechanism that feeds explanations
back into the message-passing pipeline. We evaluate X-Node on two graph
datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT,
and GIN backbones. Our results show that X-Node maintains competitive
classification accuracy while producing faithful, per-node explanations.
Repository: https://github.com/basiralab/X-Node.

</details>


### [186] [GraphFedMIG: Tackling Class Imbalance in Federated Graph Learning via Mutual Information-Guided Generation](https://arxiv.org/abs/2508.10471)
*Xinrui Li,Qilin Fan,Tianfu Wang,Kaiwen Wei,Ke Yu,Xu Zhang*

Main category: cs.LG

TL;DR: This paper proposes GraphFedMIG, a novel federated graph learning framework that reframes the problem as a federated generative data augmentation task. It addresses the challenge of class imbalance by using a hierarchical generative adversarial network with a mutual information-guided mechanism to generate high-value, minority-class features. Experiments show it outperforms other baselines.


<details>
  <summary>Details</summary>
Motivation: FGL is critically challenged by statistical heterogeneity, where non-IID data distributions across clients can severely impair model performance. A particularly destructive form of this is class imbalance, which causes the global model to become biased towards majority classes and fail at identifying rare but critical events. This issue is exacerbated in FGL, as nodes from a minority class are often surrounded by biased neighborhood information, hindering the learning of expressive embeddings.

Method: propose GraphFedMIG, a novel FGL framework that reframes the problem as a federated generative data augmentation task. GraphFedMIG employs a hierarchical generative adversarial network where each client trains a local generator to synthesize high-fidelity feature representations. To provide tailored supervision, clients are grouped into clusters, each sharing a dedicated discriminator. Crucially, the framework designs a mutual information-guided mechanism to steer the evolution of these client generators. By calculating each client's unique informational value, this mechanism corrects the local generator parameters, ensuring that subsequent rounds of mutual information-guided generation are focused on producing high-value, minority-class features.

Result: the superiority of the proposed GraphFedMIG compared with other baselines

Conclusion: The results of extensive experiments on four real-world datasets demonstrate the superiority of the proposed GraphFedMIG compared with other baselines.

Abstract: Federated graph learning (FGL) enables multiple clients to collaboratively
train powerful graph neural networks without sharing their private,
decentralized graph data. Inherited from generic federated learning, FGL is
critically challenged by statistical heterogeneity, where non-IID data
distributions across clients can severely impair model performance. A
particularly destructive form of this is class imbalance, which causes the
global model to become biased towards majority classes and fail at identifying
rare but critical events. This issue is exacerbated in FGL, as nodes from a
minority class are often surrounded by biased neighborhood information,
hindering the learning of expressive embeddings. To grapple with this
challenge, we propose GraphFedMIG, a novel FGL framework that reframes the
problem as a federated generative data augmentation task. GraphFedMIG employs a
hierarchical generative adversarial network where each client trains a local
generator to synthesize high-fidelity feature representations. To provide
tailored supervision, clients are grouped into clusters, each sharing a
dedicated discriminator. Crucially, the framework designs a mutual
information-guided mechanism to steer the evolution of these client generators.
By calculating each client's unique informational value, this mechanism
corrects the local generator parameters, ensuring that subsequent rounds of
mutual information-guided generation are focused on producing high-value,
minority-class features. We conduct extensive experiments on four real-world
datasets, and the results demonstrate the superiority of the proposed
GraphFedMIG compared with other baselines.

</details>


### [187] [EDAPT: Towards Calibration-Free BCIs with Continual Online Adaptation](https://arxiv.org/abs/2508.10474)
*Lisa Haxel,Jaivardhan Kapoor,Ulf Ziemann,Jakob H. Macke*

Main category: cs.LG

TL;DR: EDAPT 是一个与任务和模型无关的框架，它通过持续的模型适应来消除校准，从而提高了脑机接口 (BCI) 的准确性。


<details>
  <summary>Details</summary>
Motivation: 脑机接口 (BCI) 会因神经信号随时间漂移以及用户之间的差异而导致准确性下降，这需要频繁的重新校准，从而限制了实际部署。

Method: EDAPT 首先使用来自多个用户的数据训练基线解码器，然后通过监督微调不断个性化该模型，因为神经模式在使用过程中会发生变化。

Result: 我们测试了 EDAPT 在涵盖三个 BCI 任务的九个数据集上，发现它始终提高了优于传统的静态方法。这些改进主要来自结合群体层面的预训练和在线持续微调，以及在一些数据集上提供进一步增益的无监督域适应。EDAPT 运行高效，在消费级硬件上在 200 毫秒内更新模型。最后，解码精度随总数据预算而扩展，而不是其在受试者和试验之间的分配。

Conclusion: EDAPT 提供了一条通往免校准 BCI 的实用途径，减少了 BCI 部署的主要障碍。

Abstract: Brain-computer interfaces (BCIs) suffer from accuracy degradation as neural
signals drift over time and vary across users, requiring frequent recalibration
that limits practical deployment. We introduce EDAPT, a task- and
model-agnostic framework that eliminates calibration through continual model
adaptation. EDAPT first trains a baseline decoder using data from multiple
users, then continually personalizes this model via supervised finetuning as
the neural patterns evolve during use. We tested EDAPT across nine datasets
covering three BCI tasks, and found that it consistently improved accuracy over
conventional, static methods. These improvements primarily stem from combining
population-level pretraining and online continual finetuning, with unsupervised
domain adaptation providing further gains on some datasets. EDAPT runs
efficiently, updating models within 200 milliseconds on consumer-grade
hardware. Finally, decoding accuracy scales with total data budget rather than
its allocation between subjects and trials. EDAPT provides a practical pathway
toward calibration-free BCIs, reducing a major barrier to BCI deployment.

</details>


### [188] [Pinet: Optimizing hard-constrained neural networks with orthogonal projection layers](https://arxiv.org/abs/2508.10480)
*Panagiotis D. Grontas,Antonio Terpin,Efe C. Balta,Raffaello D'Andrea,John Lygeros*

Main category: cs.LG

TL;DR: This paper introduces $\\Pi$net, an output layer for neural networks that ensures satisfaction of convex constraints. It is faster than traditional solvers and surpasses state-of-the-art learning approaches in terms of training time, solution quality, and robustness to hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: We introduce an output layer for neural networks that ensures satisfaction of convex constraints.

Method: This paper leverages operator splitting for rapid and reliable projections in the forward pass, and the implicit function theorem for backpropagation to ensures satisfaction of convex constraints.

Result: $\\Pi$net obtains modest-accuracy solutions faster than traditional solvers when solving a single problem, and significantly faster for a batch of problems. 

Conclusion: $\\Pi$net surpasses state-of-the-art learning approaches in terms of training time, solution quality, and robustness to hyperparameter tuning, while maintaining similar inference times. $\\Pi$net is applied to multi-vehicle motion planning and implemented in JAX with effective tuning heuristics.

Abstract: We introduce an output layer for neural networks that ensures satisfaction of
convex constraints. Our approach, $\Pi$net, leverages operator splitting for
rapid and reliable projections in the forward pass, and the implicit function
theorem for backpropagation. We deploy $\Pi$net as a feasible-by-design
optimization proxy for parametric constrained optimization problems and obtain
modest-accuracy solutions faster than traditional solvers when solving a single
problem, and significantly faster for a batch of problems. We surpass
state-of-the-art learning approaches in terms of training time, solution
quality, and robustness to hyperparameter tuning, while maintaining similar
inference times. Finally, we tackle multi-vehicle motion planning with
non-convex trajectory preferences and provide $\Pi$net as a GPU-ready package
implemented in JAX with effective tuning heuristics.

</details>


### [189] [Learning State-Space Models of Dynamic Systems from Arbitrary Data using Joint Embedding Predictive Architectures](https://arxiv.org/abs/2508.10489)
*Jonas Ulmen,Ganesh Sundaram,Daniel Görges*

Main category: cs.LG

TL;DR: introduce a novel technique for creating world models using continuous-time dynamic systems from arbitrary observation data


<details>
  <summary>Details</summary>
Motivation: With the advent of Joint Embedding Predictive Architectures (JEPAs), which appear to be more capable than reconstruction-based methods

Method: integrates sequence embeddings with neural ordinary differential equations (neural ODEs). It employs loss functions that enforce contractive embeddings and Lipschitz constants in state transitions to construct a well-organized latent state space.

Result: demonstrated through the generation of structured latent state-space models for a simple pendulum system using only image data

Conclusion: This paper introduces a new technique for creating world models using continuous-time dynamic systems, opening up a new technique for developing more general control algorithms and estimation techniques with broad applications in robotics.

Abstract: With the advent of Joint Embedding Predictive Architectures (JEPAs), which
appear to be more capable than reconstruction-based methods, this paper
introduces a novel technique for creating world models using continuous-time
dynamic systems from arbitrary observation data. The proposed method integrates
sequence embeddings with neural ordinary differential equations (neural ODEs).
It employs loss functions that enforce contractive embeddings and Lipschitz
constants in state transitions to construct a well-organized latent state
space. The approach's effectiveness is demonstrated through the generation of
structured latent state-space models for a simple pendulum system using only
image data. This opens up a new technique for developing more general control
algorithms and estimation techniques with broad applications in robotics.

</details>


### [190] [On the Complexity-Faithfulness Trade-off of Gradient-Based Explanations](https://arxiv.org/abs/2508.10490)
*Amir Mehrpanah,Matteo Gamba,Kevin Smith,Hossein Azizpour*

Main category: cs.LG

TL;DR: 这篇论文提出了一个用于分析和改进 ReLU 网络解释的频谱框架，解决了平滑度和忠实度之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: ReLU 网络虽然在视觉数据中很普遍，但具有急剧的过渡，有时依赖于单个像素进行预测，这使得 vanilla 基于梯度的解释嘈杂且难以解释。

Method: 这篇论文介绍了一个统一的频谱框架，以系统地分析和量化解释中的平滑度、忠实度及其权衡。

Result: 这篇论文量化并正则化了 ReLU 网络对高频信息的贡献，提供了一种识别这种权衡的原则性方法。他们的分析描述了基于替代的平滑如何扭曲解释，导致了他们为不同的事后方法正式定义和测量的``解释差距''。

Conclusion: 这篇论文验证了他们的理论发现，通过不同的设计选择、数据集和消融实验。

Abstract: ReLU networks, while prevalent for visual data, have sharp transitions,
sometimes relying on individual pixels for predictions, making vanilla
gradient-based explanations noisy and difficult to interpret. Existing methods,
such as GradCAM, smooth these explanations by producing surrogate models at the
cost of faithfulness. We introduce a unifying spectral framework to
systematically analyze and quantify smoothness, faithfulness, and their
trade-off in explanations. Using this framework, we quantify and regularize the
contribution of ReLU networks to high-frequency information, providing a
principled approach to identifying this trade-off. Our analysis characterizes
how surrogate-based smoothing distorts explanations, leading to an
``explanation gap'' that we formally define and measure for different post-hoc
methods. Finally, we validate our theoretical findings across different design
choices, datasets, and ablations.

</details>


### [191] [Contrastive ECOC: Learning Output Codes for Adversarial Defense](https://arxiv.org/abs/2508.10491)
*Che-Yu Chou,Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: 提出了一种基于对比学习的自动编码本学习方法，该方法在对抗攻击方面表现出优于两个基线的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统ECOC方法依赖于手动设计或随机生成的代码本，这些代码本是劳动密集型的，并且可能产生次优的、与数据集无关的结果。本文介绍了三种基于对比学习的自动代码本学习模型，允许直接和自适应地从数据中学习代码本。

Method: 基于对比学习的自动编码本学习。

Result: 在四个数据集上，提出的模型在对抗攻击方面表现出优于两个基线的鲁棒性。

Conclusion: 提出的模型在对抗攻击方面表现出优于两个基线的鲁棒性。

Abstract: Although one-hot encoding is commonly used for multiclass classification, it
is not always the most effective encoding mechanism. Error Correcting Output
Codes (ECOC) address multiclass classification by mapping each class to a
unique codeword used as a label. Traditional ECOC methods rely on manually
designed or randomly generated codebooks, which are labor-intensive and may
yield suboptimal, dataset-agnostic results. This paper introduces three models
for automated codebook learning based on contrastive learning, allowing
codebooks to be learned directly and adaptively from data. Across four
datasets, our proposed models demonstrate superior robustness to adversarial
attacks compared to two baselines. The source is available at
https://github.com/YuChou20/Automated-Codebook-Learning-with-Error-Correcting-Output-Code-Technique.

</details>


### [192] [A Unified Multi-Agent Framework for Universal Multimodal Understanding and Generation](https://arxiv.org/abs/2508.10494)
*Jiulin Li,Ping Huang,Yexin Li,Shuo Chen,Juewen Hu,Ye Tian*

Main category: cs.LG

TL;DR: MAGUS 是一个用于多模态理解和生成的模块化框架，它通过多智能体协作和 Growth-Aware Search 机制，在多个基准测试中优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现实世界的多模态应用通常需要任意到任意的功能，从而能够理解和生成跨模态的内容，包括文本、图像、音频和视频。然而，集成自回归语言模型 (LLM) 在推理方面的优势和扩散模型在高保真生成方面的优势仍然具有挑战性。现有方法依赖于刚性管道或紧密耦合的架构，限制了灵活性和可扩展性。

Method: 提出了 MAGUS (Multi-Agent Guided Unified Multimodal System)，这是一个模块化框架，通过两个解耦阶段统一多模态理解和生成：认知和审议。MAGUS 实现了共享文本工作区中的符号多智能体协作。审议阶段包含一种 Growth-Aware Search 机制，该机制以相互加强的方式协调基于 LLM 的推理和基于扩散的生成。

Result: MAGUS 支持即插即用可扩展性、可扩展的任意到任意模态转换和语义对齐——所有这些都无需联合训练。在多个基准测试（包括图像、视频和音频生成，以及跨模态指令跟随）上的实验表明，MAGUS 优于强大的基线系统和最先进的系统。

Conclusion: MAGUS在多个基准测试中优于强大的基线系统和最先进的系统，并在 MME 基准测试中超越了强大的闭源模型 GPT-4o。

Abstract: Real-world multimodal applications often require any-to-any capabilities,
enabling both understanding and generation across modalities including text,
image, audio, and video. However, integrating the strengths of autoregressive
language models (LLMs) for reasoning and diffusion models for high-fidelity
generation remains challenging. Existing approaches rely on rigid pipelines or
tightly coupled architectures, limiting flexibility and scalability. We propose
MAGUS (Multi-Agent Guided Unified Multimodal System), a modular framework that
unifies multimodal understanding and generation via two decoupled phases:
Cognition and Deliberation. MAGUS enables symbolic multi-agent collaboration
within a shared textual workspace. In the Cognition phase, three
role-conditioned multimodal LLM agents - Perceiver, Planner, and Reflector -
engage in collaborative dialogue to perform structured understanding and
planning. The Deliberation phase incorporates a Growth-Aware Search mechanism
that orchestrates LLM-based reasoning and diffusion-based generation in a
mutually reinforcing manner. MAGUS supports plug-and-play extensibility,
scalable any-to-any modality conversion, and semantic alignment - all without
the need for joint training. Experiments across multiple benchmarks, including
image, video, and audio generation, as well as cross-modal instruction
following, demonstrate that MAGUS outperforms strong baselines and
state-of-the-art systems. Notably, on the MME benchmark, MAGUS surpasses the
powerful closed-source model GPT-4o.

</details>


### [193] [Nonlocal Monte Carlo via Reinforcement Learning](https://arxiv.org/abs/2508.10520)
*Dmitrii Dobrynin,Masoud Mohseni,John Paul Strachan*

Main category: cs.LG

TL;DR: 利用深度强化学习训练非局部蒙特卡罗算法，以解决组合优化问题，并在困难的 4-SAT 基准测试中取得改进。


<details>
  <summary>Details</summary>
Motivation: 优化或采样组合优化问题的复杂成本函数是一个长期存在的跨学科和应用挑战。当采用基于马尔可夫链蒙特卡罗 (MCMC) 的传统算法族时，人们假设输入具有均匀（平衡）温度曲线。事实证明，当所谓的重叠间隙属性成立时，这种与实例无关的方法对于计算相变附近的难度最高的基准测试无效。

Method: 采用深度强化学习 (RL) 来训练 NMC 的非局部转移策略。

Result: 训练后的策略在困难的均匀随机和无标度随机 4-SAT 基准测试中改进了基于标准 MCMC 的非局部模拟退火，具体体现在残余能量、解决时间和解决方案多样性指标方面。

Conclusion: 该求解器仅通过观察配置空间探索的能量变化作为 RL 奖励和局部最小能量景观几何作为 RL 状态进行训练。训练后的策略在残余能量、解决时间以及解决方案指标的多样性方面改进了标准 MCMC 和非局部模拟退火。

Abstract: Optimizing or sampling complex cost functions of combinatorial optimization
problems is a longstanding challenge across disciplines and applications. When
employing family of conventional algorithms based on Markov Chain Monte Carlo
(MCMC) such as simulated annealing or parallel tempering, one assumes
homogeneous (equilibrium) temperature profiles across input. This instance
independent approach was shown to be ineffective for the hardest benchmarks
near a computational phase transition when the so-called overlap-gap-property
holds. In these regimes conventional MCMC struggles to unfreeze rigid
variables, escape suboptimal basins of attraction, and sample high-quality and
diverse solutions. In order to mitigate these challenges, Nonequilibrium
Nonlocal Monte Carlo (NMC) algorithms were proposed that leverage inhomogeneous
temperature profiles thereby accelerating exploration of the configuration
space without compromising its exploitation. Here, we employ deep reinforcement
learning (RL) to train the nonlocal transition policies of NMC which were
previously designed phenomenologically. We demonstrate that the resulting
solver can be trained solely by observing energy changes of the configuration
space exploration as RL rewards and the local minimum energy landscape geometry
as RL states. We further show that the trained policies improve upon the
standard MCMC-based and nonlocal simulated annealing on hard uniform random and
scale-free random 4-SAT benchmarks in terms of residual energy,
time-to-solution, and diversity of solutions metrics.

</details>


### [194] [Projected Coupled Diffusion for Test-Time Constrained Joint Generation](https://arxiv.org/abs/2508.10531)
*Hao Luan,Yi Xian Goh,See-Kiong Ng,Chun Kai Ling*

Main category: cs.LG

TL;DR: 提出PCD，一个用于约束联合生成的测试时框架，它通过耦合引导项鼓励扩散模型之间的协调，并加入投影步骤来执行硬约束。实验表明，PCD在多个应用中有效，提高了耦合效果，并保证了约束的满足，且没有产生过高的计算成本。


<details>
  <summary>Details</summary>
Motivation: 在测试时采样的修改已经成为扩散算法的一个重要扩展，其目标是在不必重新训练整个扩散模型的情况下，偏置生成过程以实现给定的目标。然而，从多个预训练的扩散模型中生成联合相关的样本，同时在没有昂贵的再训练的情况下执行特定任务的约束仍然具有挑战性。

Method: 提出了一种名为Projected Coupled Diffusion (PCD)的新的测试时框架，用于约束联合生成。PCD在生成动态中引入了一个耦合引导项，以鼓励扩散模型之间的协调，并在每个扩散步骤中加入一个投影步骤，以执行硬约束。

Result: 结果表明，该方法提高了耦合效果，并保证了约束的满足，且没有产生过高的计算成本。

Conclusion: PCD在图像对生成、物体操作和多机器人运动规划的应用场景中表现出有效性，实现了更好的耦合效果和有保障的约束满足，且没有产生过高的计算成本。

Abstract: Modifications to test-time sampling have emerged as an important extension to
diffusion algorithms, with the goal of biasing the generative process to
achieve a given objective without having to retrain the entire diffusion model.
However, generating jointly correlated samples from multiple pre-trained
diffusion models while simultaneously enforcing task-specific constraints
without costly retraining has remained challenging. To this end, we propose
Projected Coupled Diffusion (PCD), a novel test-time framework for constrained
joint generation. PCD introduces a coupled guidance term into the generative
dynamics to encourage coordination between diffusion models and incorporates a
projection step at each diffusion step to enforce hard constraints.
Empirically, we demonstrate the effectiveness of PCD in application scenarios
of image-pair generation, object manipulation, and multi-robot motion planning.
Our results show improved coupling effects and guaranteed constraint
satisfaction without incurring excessive computational costs.

</details>


### [195] [Driving Accurate Allergen Prediction with Protein Language Models and Generalization-Focused Evaluation](https://arxiv.org/abs/2508.10541)
*Brian Shing-Hei Wong,Joshua Mincheol Kim,Sin-Hang Fung,Qing Xiong,Kelvin Fu-Kiu Ao,Junkang Wei,Ran Wang,Dan Michelle Wang,Jingying Zhou,Bo Feng,Alfred Sze-Lok Cheng,Kevin Y. Yip,Stephen Kwok-Wing Tsui,Qin Cao*

Main category: cs.LG

TL;DR: 该研究介绍了一种新的计算框架 Applm，它利用蛋白质语言模型来准确识别过敏原蛋白，并在各种实际场景中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 过敏原，通常是能够引发不良免疫反应的蛋白质，代表着一个重大的公共健康挑战。为了准确识别过敏原蛋白。

Method: 该研究利用包含 1000 亿参数的 xTrimoPGLM 蛋白质语言模型，构建了一个名为 Applm 的计算框架。

Result: Applm 在各种任务中均优于七种最先进的方法，这些任务与困难的真实场景非常相似，包括识别在训练集中缺乏相似示例的新型过敏原，区分具有高序列相似性的同源物中的过敏原和非过敏原，以及评估对蛋白质序列产生少量变化的突变的功能后果。

Conclusion: 该研究提供了一个名为 Applm 的开源软件以及精心策划的基准数据集，以促进未来的研究。

Abstract: Allergens, typically proteins capable of triggering adverse immune responses,
represent a significant public health challenge. To accurately identify
allergen proteins, we introduce Applm (Allergen Prediction with Protein
Language Models), a computational framework that leverages the 100-billion
parameter xTrimoPGLM protein language model. We show that Applm consistently
outperforms seven state-of-the-art methods in a diverse set of tasks that
closely resemble difficult real-world scenarios. These include identifying
novel allergens that lack similar examples in the training set, differentiating
between allergens and non-allergens among homologs with high sequence
similarity, and assessing functional consequences of mutations that create few
changes to the protein sequences. Our analysis confirms that xTrimoPGLM,
originally trained on one trillion tokens to capture general protein sequence
characteristics, is crucial for Applm's performance by detecting important
differences among protein sequences. In addition to providing Applm as
open-source software, we also provide our carefully curated benchmark datasets
to facilitate future research.

</details>


### [196] [Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards](https://arxiv.org/abs/2508.10548)
*Zetian Sun,Dongfang Li,Zhuoen Chen,Yuhuai Qin,Baotian Hu*

Main category: cs.LG

TL;DR: This paper introduces Gated Reward Accumulation (G-RA) to address reward sparsity in long-horizon RL for software engineering, improving completion and modification rates.


<details>
  <summary>Details</summary>
Motivation: Reward sparsity in long-horizon reinforcement learning (RL) tasks is a significant challenge, and existing reward shaping methods suffer from bias, task decomposition requirements, or reward hacking.

Method: Gated Reward Accumulation (G-RA), a novel method that accumulates immediate rewards only when high-level (long-term) rewards meet a predefined threshold.

Result: G-RA leads to an increase in completion rates (47.6% -> 93.8% and 22.0% -> 86.0%) and modification rates (19.6% -> 23.8% and 12.0% -> 42.0%).

Conclusion: Gated Reward Accumulation (G-RA) increases completion and modification rates while avoiding policy degradation in long-horizon RL for software engineering tasks.

Abstract: Reward sparsity in long-horizon reinforcement learning (RL) tasks remains a
significant challenge, while existing outcome-based reward shaping struggles to
define meaningful immediate rewards without introducing bias or requiring
explicit task decomposition. Alternatively, verification-based reward shaping
uses stepwise critics, but misalignment between immediate rewards and long-term
objectives can lead to reward hacking and suboptimal policies. In this work, we
address this problem in the context of software engineering (SWE) tasks, where
multi-turn reasoning and rule-based verification are critical. We introduce the
SWE-oriented RL Framework, a unified system supporting multi-turn interaction,
docker-based execution, and customizable reward functions. Additionally, we
propose Gated Reward Accumulation (G-RA), a novel method that accumulates
immediate rewards only when high-level (long-term) rewards meet a predefined
threshold, ensuring stable RL optimization. Experiments on SWE-bench Verified
and kBench demonstrate that G-RA leads to an increase in completion rates
(47.6\% \rightarrow 93.8\% and 22.0\% \rightarrow 86.0\%) and modification
rates (19.6\% \rightarrow 23.8\% and 12.0\% \rightarrow 42.0\%), while avoiding
policy degradation caused by reward misalignment. Our findings highlight the
importance of balanced reward accumulation in long-horizon RL and provide a
practical solution.

</details>


### [197] [Technical Report: Facilitating the Adoption of Causal Inference Methods Through LLM-Empowered Co-Pilot](https://arxiv.org/abs/2508.10581)
*Jeroen Berrevoets,Julianna Piskorz,Robert Davis,Harry Amad,Jim Weatherall,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: CATE-B是一个开源的co-pilot系统，它使用大型语言模型来指导用户完成端到端的治疗效果估计过程。


<details>
  <summary>Details</summary>
Motivation: 从观察数据中估计治疗效果(TE)在许多领域都是一项关键但复杂的任务，但由于需要在因果假设、调整策略和模型选择方面具有深厚的专业知识，因此机器学习和因果推断的最新进展仍然有限。

Method: CATE-B使用agentic框架内的大型语言模型(llm)来指导用户完成端到端的治疗效果评估过程。

Result: CATE-B在(i)通过因果发现和基于llm的边缘方向构建结构因果模型，(ii)通过一种新的最小不确定性调整集标准识别鲁棒的调整集，以及(iii)选择适合因果结构和数据集特征的适当回归方法方面提供帮助。

Conclusion: CATE-B降低了严格因果分析的门槛，并为自动化治疗效果评估中的一类新基准奠定了基础。

Abstract: Estimating treatment effects (TE) from observational data is a critical yet
complex task in many fields, from healthcare and economics to public policy.
While recent advances in machine learning and causal inference have produced
powerful estimation techniques, their adoption remains limited due to the need
for deep expertise in causal assumptions, adjustment strategies, and model
selection. In this paper, we introduce CATE-B, an open-source co-pilot system
that uses large language models (LLMs) within an agentic framework to guide
users through the end-to-end process of treatment effect estimation. CATE-B
assists in (i) constructing a structural causal model via causal discovery and
LLM-based edge orientation, (ii) identifying robust adjustment sets through a
novel Minimal Uncertainty Adjustment Set criterion, and (iii) selecting
appropriate regression methods tailored to the causal structure and dataset
characteristics. To encourage reproducibility and evaluation, we release a
suite of benchmark tasks spanning diverse domains and causal complexities. By
combining causal inference with intelligent, interactive assistance, CATE-B
lowers the barrier to rigorous causal analysis and lays the foundation for a
new class of benchmarks in automated treatment effect estimation.

</details>


### [198] [GNN-based Unified Deep Learning](https://arxiv.org/abs/2508.10583)
*Furkan Pala,Islem Rekik*

Main category: cs.LG

TL;DR: unified learning, a new paradigm that encodes each model into a graph representation, enabling unification in a shared graph learning space. A GNN then guides optimization of these unified models


<details>
  <summary>Details</summary>
Motivation: Deep learning models often struggle to maintain generalizability in medical imaging, particularly under domain-fracture scenarios where distribution shifts arise from varying imaging techniques, acquisition protocols, patient populations, demographics, and equipment. In practice, each hospital may need to train distinct models - differing in learning task, width, and depth - to match local data

Method: encodes each model into a graph representation, enabling unification in a shared graph learning space. A GNN then guides optimization of these unified models

Result: Evaluations on MorphoMNIST and two MedMNIST benchmarks - PneumoniaMNIST and BreastMNIST - show that unified learning boosts performance when models are trained on unique distributions and tested on mixed ones, demonstrating strong robustness to unseen data with large distribution shifts.

Conclusion: unified learning boosts performance when models are trained on unique distributions and tested on mixed ones, demonstrating strong robustness to unseen data with large distribution shifts

Abstract: Deep learning models often struggle to maintain generalizability in medical
imaging, particularly under domain-fracture scenarios where distribution shifts
arise from varying imaging techniques, acquisition protocols, patient
populations, demographics, and equipment. In practice, each hospital may need
to train distinct models - differing in learning task, width, and depth - to
match local data. For example, one hospital may use Euclidean architectures
such as MLPs and CNNs for tabular or grid-like image data, while another may
require non-Euclidean architectures such as graph neural networks (GNNs) for
irregular data like brain connectomes. How to train such heterogeneous models
coherently across datasets, while enhancing each model's generalizability,
remains an open problem. We propose unified learning, a new paradigm that
encodes each model into a graph representation, enabling unification in a
shared graph learning space. A GNN then guides optimization of these unified
models. By decoupling parameters of individual models and controlling them
through a unified GNN (uGNN), our method supports parameter sharing and
knowledge transfer across varying architectures (MLPs, CNNs, GNNs) and
distributions, improving generalizability. Evaluations on MorphoMNIST and two
MedMNIST benchmarks - PneumoniaMNIST and BreastMNIST - show that unified
learning boosts performance when models are trained on unique distributions and
tested on mixed ones, demonstrating strong robustness to unseen data with large
distribution shifts. Code and benchmarks: https://github.com/basiralab/uGNN

</details>
