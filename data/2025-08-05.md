<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 42]
- [cs.CV](#cs.CV) [Total: 42]
- [cs.AI](#cs.AI) [Total: 42]
- [cs.DB](#cs.DB) [Total: 7]
- [cs.IR](#cs.IR) [Total: 21]
- [cs.LG](#cs.LG) [Total: 45]
- [eess.IV](#eess.IV) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Rethinking Graph-Based Document Classification: Learning Data-Driven Structures Beyond Heuristic Approaches](https://arxiv.org/abs/2508.00864)
*Margarita Bugueño,Gerard de Melo*

Main category: cs.CL

TL;DR: 提出了一种自动学习文档图结构的方法，该方法优于传统方法，提高了文档分类的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在文档分类中，基于图的模型有效地捕获文档结构，克服了序列长度的限制并增强了上下文理解。但是，大多数现有的图文档表示都依赖于启发式方法，特定于领域的规则或专家知识。

Method: 提出了一种学习数据驱动图结构的方法，该方法构建了以句子为节点的同构加权图，而边是通过识别句子对之间依赖关系的self-attention模型学习的。统计过滤策略旨在仅保留强相关的句子，从而提高图的质量。

Result: 在三个文档分类数据集上的实验表明，学习的图始终优于基于启发式的图，从而获得了更高的准确性和$F_1$分数。此外，我们的研究证明了统计过滤在提高分类鲁棒性方面的有效性。

Conclusion: 自动生成的图结构优于传统的启发式方法，并为NLP中更广泛的应用开辟了新的方向。

Abstract: In document classification, graph-based models effectively capture document
structure, overcoming sequence length limitations and enhancing contextual
understanding. However, most existing graph document representations rely on
heuristics, domain-specific rules, or expert knowledge. Unlike previous
approaches, we propose a method to learn data-driven graph structures,
eliminating the need for manual design and reducing domain dependence. Our
approach constructs homogeneous weighted graphs with sentences as nodes, while
edges are learned via a self-attention model that identifies dependencies
between sentence pairs. A statistical filtering strategy aims to retain only
strongly correlated sentences, improving graph quality while reducing the graph
size. Experiments on three document classification datasets demonstrate that
learned graphs consistently outperform heuristic-based graphs, achieving higher
accuracy and $F_1$ score. Furthermore, our study demonstrates the effectiveness
of the statistical filtering in improving classification robustness. These
results highlight the potential of automatic graph generation over traditional
heuristic approaches and open new directions for broader applications in NLP.

</details>


### [2] [FECT: Factuality Evaluation of Interpretive AI-Generated Claims in Contact Center Conversation Transcripts](https://arxiv.org/abs/2508.00889)
*Hagyeong Shin,Binoy Robin Dalal,Iwona Bialynicka-Birula,Navjot Matharu,Ryan Muir,Xingwei Yang,Samuel W. K. Wong*

Main category: cs.CL

TL;DR: 该研究引入了一种用于评估联络中心对话分析中人工智能生成输出的事实性的新方法，包括 3D 标注范例和 FECT 基准数据集。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 已知会产生幻觉，产生未基于输入、参考资料或真实世界知识的自然语言输出。在人工智能功能支持业务决策的企业应用程序中，此类幻觉可能特别有害。分析和总结联络中心对话的法学硕士为事实性评估带来了一系列独特的挑战，因为通常不存在关于对话中捕获的情绪和业务问题根本原因的分析解释的真实标签。

Method: 该研究引入了一个 3D（分解、解耦、分离）范例，用于人工标注指南和 LLM 评判器的提示，以将事实性标签置于语言学知情评估标准中。然后，该研究引入了 FECT，这是一个新的基准数据集，用于在联络中心对话记录中对解释性人工智能生成的声明进行事实性评估，该数据集在 3D 范例下进行标记。

Result: 该研究报告了将 LLM 评判器与 3D 范例对齐的结果。总体而言，该研究的结果为自动评估人工智能系统在分析联络中心对话时生成的输出的事实性提供了一种新方法。

Conclusion: 该研究提出了一种新的方法，用于自动评估人工智能系统在分析联络中心对话时生成的输出的事实性。

Abstract: Large language models (LLMs) are known to hallucinate, producing natural
language outputs that are not grounded in the input, reference materials, or
real-world knowledge. In enterprise applications where AI features support
business decisions, such hallucinations can be particularly detrimental. LLMs
that analyze and summarize contact center conversations introduce a unique set
of challenges for factuality evaluation, because ground-truth labels often do
not exist for analytical interpretations about sentiments captured in the
conversation and root causes of the business problems. To remedy this, we first
introduce a \textbf{3D} -- \textbf{Decompose, Decouple, Detach} -- paradigm in
the human annotation guideline and the LLM-judges' prompt to ground the
factuality labels in linguistically-informed evaluation criteria. We then
introduce \textbf{FECT}, a novel benchmark dataset for \textbf{F}actuality
\textbf{E}valuation of Interpretive AI-Generated \textbf{C}laims in Contact
Center Conversation \textbf{T}ranscripts, labeled under our 3D paradigm.
Lastly, we report our findings from aligning LLM-judges on the 3D paradigm.
Overall, our findings contribute a new approach for automatically evaluating
the factuality of outputs generated by an AI system for analyzing contact
center conversations.

</details>


### [3] [XAutoLM: Efficient Fine-Tuning of Language Models via Meta-Learning and AutoML](https://arxiv.org/abs/2508.00924)
*Ernesto L. Estevanell-Valladares,Suilan Estevez-Velarde,Yoan Gutiérrez,Andrés Montoyo,Ruslan Mitkov*

Main category: cs.CL

TL;DR: XAutoLM is a meta-learning AutoML framework that reuses past experiences to efficiently optimise LM fine-tuning, outperforming existing methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Experts in machine learning leverage domain knowledge to navigate decisions in model selection, hyperparameter optimisation, and resource allocation. This is particularly critical for fine-tuning language models (LMs), where repeated trials incur substantial computational overhead and environmental impact. However, no existing automated framework simultaneously tackles the entire model selection and HPO task for resource-efficient LM fine-tuning.

Method: a meta-learning-augmented AutoML framework that reuses past experiences to optimise discriminative and generative LM fine-tuning pipelines efficiently. XAutoLM learns from stored successes and failures by extracting task- and system-level meta-features to bias its sampling toward fruitful configurations and away from costly dead ends.

Result: XAutoLM surpasses zero-shot optimiser's peak F1 on five of six tasks, cuts mean evaluation time by up to 4.5x, reduces error ratios by up to sevenfold, and uncovers up to 50% more pipelines above the zero-shot Pareto front. In contrast, simpler memory-based baselines suffer negative transfer.

Conclusion: XAutoLM surpasses zero-shot optimiser's peak F1 on five of six tasks, cuts mean evaluation time by up to 4.5x, reduces error ratios by up to sevenfold, and uncovers up to 50% more pipelines above the zero-shot Pareto front.

Abstract: Experts in machine learning leverage domain knowledge to navigate decisions
in model selection, hyperparameter optimisation, and resource allocation. This
is particularly critical for fine-tuning language models (LMs), where repeated
trials incur substantial computational overhead and environmental impact.
However, no existing automated framework simultaneously tackles the entire
model selection and HPO task for resource-efficient LM fine-tuning. We
introduce XAutoLM, a meta-learning-augmented AutoML framework that reuses past
experiences to optimise discriminative and generative LM fine-tuning pipelines
efficiently. XAutoLM learns from stored successes and failures by extracting
task- and system-level meta-features to bias its sampling toward fruitful
configurations and away from costly dead ends. On four text classification and
two question-answering benchmarks, XAutoLM surpasses zero-shot optimiser's peak
F1 on five of six tasks, cuts mean evaluation time by up to 4.5x, reduces error
ratios by up to sevenfold, and uncovers up to 50% more pipelines above the
zero-shot Pareto front. In contrast, simpler memory-based baselines suffer
negative transfer. We release XAutoLM and our experience store to catalyse
resource-efficient, Green AI fine-tuning in the NLP community.

</details>


### [4] [MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented Generation](https://arxiv.org/abs/2508.01005)
*Yiqun Chen,Erhan Zhang,Lingyong Yan,Shuaiqiang Wang,Jizhou Huang,Dawei Yin,Jiaxin Mao*

Main category: cs.CL

TL;DR: 提出了一种基于多智能体协同的自适应RAG框架MAO-ARAG，它可以根据查询动态规划工作流程，在保证答案质量的同时，将成本和延迟控制在可接受的范围内。


<details>
  <summary>Details</summary>
Motivation: 由于现实世界中查询的复杂性各不相同，固定的RAG流程通常难以在不同查询的性能和成本效率之间取得平衡。

Method: 多智能体协同的自适应RAG框架

Result: 在多个QA数据集上进行的实验表明，该方法在实现高质量答案的同时，将成本和延迟保持在可接受的范围内。

Conclusion: MAO-ARAG通过动态规划工作流程，在保证答案质量的同时，将成本和延迟控制在可接受的范围内。

Abstract: In question-answering (QA) systems, Retrieval-Augmented Generation (RAG) has
become pivotal in enhancing response accuracy and reducing hallucination
issues. The architecture of RAG systems varies significantly, encompassing
single-round RAG, iterative RAG, and reasoning RAG, each tailored to address
different types of queries. Due to the varying complexity of real-world
queries, a fixed RAG pipeline often struggles to balance performance and cost
efficiency across different queries. To address this challenge, we propose an
adaptive RAG framework called MAO-ARAG, which leverages multi-agent
orchestration. Our adaptive RAG is conceived as a multi-turn framework.
Specifically, we define multiple executor agents, representing typical RAG
modules such as query reformulation agents, document selection agent, and
generation agents. A planner agent intelligently selects and integrates the
appropriate agents from these executors into a suitable workflow tailored for
each query, striving for high-quality answers while maintaining reasonable
costs. During each turn, the planner agent is trained using reinforcement
learning, guided by an outcome-based reward (F1 score) and a cost-based
penalty, continuously improving answer quality while keeping costs within a
reasonable range. Experiments conducted on multiple QA datasets demonstrate
that our approach, which dynamically plans workflows for each query, not only
achieves high answer quality but also maintains both cost and latency within
acceptable limits.The code of MAO-ARAG is on
https://github.com/chenyiqun/Agentic-RAG.

</details>


### [5] [UrBLiMP: A Benchmark for Evaluating the Linguistic Competence of Large Language Models in Urdu](https://arxiv.org/abs/2508.01006)
*Farah Adeeba,Brian Dillon,Hassan Sajjad,Rajesh Bhatt*

Main category: cs.CL

TL;DR: Evaluate multilingual LLMs on Urdu using UrBLiMP, find LLaMA-3-70B performs best but is comparable to Gemma-3-27B-PT, highlighting potential and limitations in low-resource language syntax.


<details>
  <summary>Details</summary>
Motivation: Multilingual LLMs often include significantly less data for low-resource languages such as Urdu compared to high-resource languages like English. To assess the linguistic knowledge of LLMs in Urdu

Method: present the Urdu Benchmark of Linguistic Minimal Pairs (UrBLiMP) and evaluate twenty multilingual LLMs on it

Result: LLaMA-3-70B achieves the highest average accuracy (94.73%), but its performance is statistically comparable to other top models such as Gemma-3-27B-PT

Conclusion: current multilingual LLMs have both potential and limitations in capturing fine-grained syntactic knowledge in low-resource languages

Abstract: Multilingual Large Language Models (LLMs) have shown remarkable performance
across various languages; however, they often include significantly less data
for low-resource languages such as Urdu compared to high-resource languages
like English. To assess the linguistic knowledge of LLMs in Urdu, we present
the Urdu Benchmark of Linguistic Minimal Pairs (UrBLiMP) i.e. pairs of
minimally different sentences that contrast in grammatical acceptability.
UrBLiMP comprises 5,696 minimal pairs targeting ten core syntactic phenomena,
carefully curated using the Urdu Treebank and diverse Urdu text corpora. A
human evaluation of UrBLiMP annotations yielded a 96.10% inter-annotator
agreement, confirming the reliability of the dataset. We evaluate twenty
multilingual LLMs on UrBLiMP, revealing significant variation in performance
across linguistic phenomena. While LLaMA-3-70B achieves the highest average
accuracy (94.73%), its performance is statistically comparable to other top
models such as Gemma-3-27B-PT. These findings highlight both the potential and
the limitations of current multilingual LLMs in capturing fine-grained
syntactic knowledge in low-resource languages.

</details>


### [6] [Cross-Domain Web Information Extraction at Pinterest](https://arxiv.org/abs/2508.01096)
*Michael Farag,Patrick Halina,Andrey Zaytsev,Alekhya Munagala,Imtihan Ahmed,Junhao Wang*

Main category: cs.CL

TL;DR: Pinterest 的属性提取系统利用一种新颖的网页表示，该表示将结构、视觉和文本模态组合成一种紧凑的形式，与大型语言模型相比，它可以实现更高的准确性和可扩展性，并且成本效益更高。


<details>
  <summary>Details</summary>
Motivation: 从电子商务网站准确提取结构化产品数据对于增强用户体验和改善内容分发至关重要。

Method: 利用一种新颖的网页表示，该表示将结构、视觉和文本模态组合成一种紧凑的形式，针对小型模型学习进行了优化。

Result: 结果表明，该系统比更复杂的大型语言模型 (LLM)（如生成型预训练 Transformer (GPT)）更准确地提取属性。

Conclusion: 该系统具有高度的可扩展性，每秒可处理超过 1,000 个 URL，同时比最便宜的 GPT 替代方案便宜 1000 倍。

Abstract: The internet offers a massive repository of unstructured information, but
it's a significant challenge to convert this into a structured format. At
Pinterest, the ability to accurately extract structured product data from
e-commerce websites is essential to enhance user experiences and improve
content distribution. In this paper, we present Pinterest's system for
attribute extraction, which achieves remarkable accuracy and scalability at a
manageable cost. Our approach leverages a novel webpage representation that
combines structural, visual, and text modalities into a compact form,
optimizing it for small model learning. This representation captures each
visible HTML node with its text, style and layout information. We show how this
allows simple models such as eXtreme Gradient Boosting (XGBoost) to extract
attributes more accurately than much more complex Large Language Models (LLMs)
such as Generative Pre-trained Transformer (GPT). Our results demonstrate a
system that is highly scalable, processing over 1,000 URLs per second, while
being 1000 times more cost-effective than the cheapest GPT alternatives.

</details>


### [7] [Asking the Right Questions: Benchmarking Large Language Models in the Development of Clinical Consultation Templates](https://arxiv.org/abs/2508.01159)
*Liam G. McCoy,Fateme Nateghi Haredasht,Kanav Chopra,David Wu,David JH Wu,Abass Conteh,Sarita Khemani,Saloni Kumar Maharaj,Vishnu Ravi,Arth Pahwa,Yingjie Weng,Leah Rosengaus,Lena Giang,Kelvin Zhenghao Li,Olivia Jee,Daniel Shirvani,Ethan Goh,Jonathan H. Chen*

Main category: cs.CL

TL;DR: LLMs can help with clinical consultation templates but struggle with length and prioritization.


<details>
  <summary>Details</summary>
Motivation: To evaluate the capacity of LLMs to generate structured clinical consultation templates for electronic consultation.

Method: Evaluated LLMs against expert templates using prompt optimization, semantic autograding, and prioritization analysis.

Result: Models achieve high comprehensiveness but generate excessively long templates and fail to prioritize clinically important questions, with performance varying across specialties.

Conclusion: LLMs can enhance structured clinical information exchange but need better prioritization within time constraints.

Abstract: This study evaluates the capacity of large language models (LLMs) to generate
structured clinical consultation templates for electronic consultation. Using
145 expert-crafted templates developed and routinely used by Stanford's
eConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2,
Claude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to
produce clinically coherent, concise, and prioritized clinical question
schemas. Through a multi-agent pipeline combining prompt optimization, semantic
autograding, and prioritization analysis, we show that while models like o3
achieve high comprehensiveness (up to 92.2\%), they consistently generate
excessively long templates and fail to correctly prioritize the most clinically
important questions under length constraints. Performance varies across
specialties, with significant degradation in narrative-driven fields such as
psychiatry and pain medicine. Our findings demonstrate that LLMs can enhance
structured clinical information exchange between physicians, while highlighting
the need for more robust evaluation methods that capture a model's ability to
prioritize clinically salient information within the time constraints of
real-world physician communication.

</details>


### [8] [CSIRO-LT at SemEval-2025 Task 11: Adapting LLMs for Emotion Recognition for Multiple Languages](https://arxiv.org/abs/2508.01161)
*Jiyu Chen,Necva Bölücü,Sarvnaz Karimi,Diego Mollá,Cécile L. Paris*

Main category: cs.CL

TL;DR: This paper investigates emotion recognition across different languages using LLMs, finding that fine-tuning a multilingual LLM with LoRA separately for each language is the most effective method.


<details>
  <summary>Details</summary>
Motivation: Detecting emotions across different languages is challenging due to the varied and culturally nuanced ways of emotional expressions. The Semeval 2025 Task 11: Bridging the Gap in Text-Based emotion shared task was organised to investigate emotion recognition across different languages. The goal of the task is to implement an emotion recogniser that can identify the basic emotional states that general third-party observers would attribute to an author based on their written text snippet, along with the intensity of those emotions.

Method: fine-tune a pre-trained multilingual LLM with LoRA setting separately for each language

Result: investigation of various task-adaptation strategies for LLMs in emotion recognition

Conclusion: The most effective method for this task is to fine-tune a pre-trained multilingual LLM with LoRA setting separately for each language.

Abstract: Detecting emotions across different languages is challenging due to the
varied and culturally nuanced ways of emotional expressions. The
\textit{Semeval 2025 Task 11: Bridging the Gap in Text-Based emotion} shared
task was organised to investigate emotion recognition across different
languages. The goal of the task is to implement an emotion recogniser that can
identify the basic emotional states that general third-party observers would
attribute to an author based on their written text snippet, along with the
intensity of those emotions. We report our investigation of various
task-adaptation strategies for LLMs in emotion recognition. We show that the
most effective method for this task is to fine-tune a pre-trained multilingual
LLM with LoRA setting separately for each language.

</details>


### [9] [Adaptive Content Restriction for Large Language Models via Suffix Optimization](https://arxiv.org/abs/2508.01198)
*Yige Li,Peihai Jiang,Jun Sun,Peng Shu,Tianming Liu,Zhen Xiang*

Main category: cs.CL

TL;DR: This paper introduces Adaptive Content Restriction (AdaCoRe) and a new method called Suffix Optimization (SOP) to prevent LLMs from generating restricted content without fine-tuning. SOP is evaluated on a new benchmark and shown to be effective.


<details>
  <summary>Details</summary>
Motivation: Content restrictions for LLMs vary across user groups and time, making supervised fine-tuning impractical for each use case. This motivates the need for lightweight strategies like Adaptive Content Restriction (AdaCoRe).

Method: The paper proposes Suffix Optimization (SOP), a lightweight strategy that appends a short, optimized suffix to prompts to prevent LLMs from generating restricted terms.

Result: SOP outperforms system-level baselines on CoReBench by 15%, 17%, 10%, 9%, and 6% on average restriction rates for Gemma2-2B, Mistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively.

Conclusion: The paper demonstrates the effectiveness of Suffix Optimization (SOP) on a new Content Restriction Benchmark (CoReBench) and on the POE platform, showing its practicality in real-world scenarios.

Abstract: Large Language Models (LLMs) have demonstrated significant success across
diverse applications. However, enforcing content restrictions remains a
significant challenge due to their expansive output space. One aspect of
content restriction is preventing LLMs from generating harmful content via
model alignment approaches such as supervised fine-tuning (SFT). Yet, the need
for content restriction may vary significantly across user groups, change
rapidly over time, and not always align with general definitions of
harmfulness. Applying SFT to each of these specific use cases is impractical
due to the high computational, data, and storage demands. Motivated by this
need, we propose a new task called \textit{Adaptive Content Restriction}
(AdaCoRe), which focuses on lightweight strategies -- methods without model
fine-tuning -- to prevent deployed LLMs from generating restricted terms for
specific use cases. We propose the first method for AdaCoRe, named
\textit{Suffix Optimization (SOP)}, which appends a short, optimized suffix to
any prompt to a) prevent a target LLM from generating a set of restricted
terms, while b) preserving the output quality. To evaluate AdaCoRe approaches,
including our SOP, we create a new \textit{Content Restriction Benchmark}
(CoReBench), which contains 400 prompts for 80 restricted terms across 8
carefully selected categories. We demonstrate the effectiveness of SOP on
CoReBench, which outperforms the system-level baselines such as system suffix
by 15\%, 17\%, 10\%, 9\%, and 6\% on average restriction rates for Gemma2-2B,
Mistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively. We also
demonstrate that SOP is effective on POE, an online platform hosting various
commercial LLMs, highlighting its practicality in real-world scenarios.

</details>


### [10] [Show or Tell? Modeling the evolution of request-making in Human-LLM conversations](https://arxiv.org/abs/2508.01213)
*Shengqi Zhu,Jeffrey M. Rzeszotarski,David Mimno*

Main category: cs.CL

TL;DR: LLM查询分析：用户行为因经验和模型能力而异。


<details>
  <summary>Details</summary>
Motivation: 聊天记录提供了关于LLM用户的丰富信息来源，但用户行为模式通常被查询的可变性所掩盖。

Method: 将聊天查询分割成请求内容、角色、查询特定上下文和其他表达。

Result: LLM查询中的请求方式与类似的人际互动有显著差异。查询模式随时间变化，模型能力影响用户行为。

Conclusion: 查询模式在早期强调请求，个体用户探索模式但倾向于随着经验融合。模型能力会影响用户行为，特别是在引入新模型时，这可以在社区层面追踪。

Abstract: Chat logs provide a rich source of information about LLM users, but patterns
of user behavior are often masked by the variability of queries. We present a
new task, segmenting chat queries into contents of requests, roles,
query-specific context, and additional expressions. We find that, despite the
familiarity of chat-based interaction, request-making in LLM queries remains
significantly different from comparable human-human interactions. With the data
resource, we introduce an important perspective of diachronic analyses with
user expressions. We find that query patterns vary between early ones
emphasizing requests, and individual users explore patterns but tend to
converge with experience. Finally, we show that model capabilities affect user
behavior, particularly with the introduction of new models, which are traceable
at the community level.

</details>


### [11] [WebDS: An End-to-End Benchmark for Web-based Data Science](https://arxiv.org/abs/2508.01222)
*Ethan Hsu,Hong Meng Yam,Ines Bouissou,Aaron Murali John,Raj Thota,Josh Koe,Vivek Sarath Putta,G K Dharesan,Alexander Spangher,Shikhar Murty,Tenghao Huang,Christopher D. Manning*

Main category: cs.CL

TL;DR: WebDS是第一个端到端web数据科学基准，它包含870个web数据科学任务，这些任务来自29个不同的网站，旨在挑战agent执行复杂的、多步骤的操作，这些操作需要使用工具和异构数据格式，更好地反映现代数据分析的现实。


<details>
  <summary>Details</summary>
Motivation: 现有的网络基准测试通常侧重于简单的交互，而传统的数据科学基准测试通常侧重于静态的、通常是文本绑定的数据集，并且不评估包含数据获取、清理、分析和洞察力生成的端到端工作流程。

Method: WebDS

Result: Browser Use在Web Voyager上完成了80%的任务，但在WebDS中只成功完成了15%的任务，这表明由于信息基础薄弱、重复行为和采用捷径等新的失败模式。

Conclusion: WebDS的评估结果表明，目前最先进的LLM agent在完成这些任务方面存在显著的性能差距。

Abstract: A large portion of real-world data science tasks are complex and require
multi-hop web-based interactions: finding appropriate data available on the
internet, synthesizing real-time data of various modalities from different
locations, and producing summarized analyses. Existing web benchmarks often
focus on simplistic interactions, such as form submissions or e-commerce
transactions, and often do not require diverse tool-using capabilities required
for web based data science. Conversely, traditional data science benchmarks
typically concentrate on static, often textually bound datasets and do not
assess end-to-end workflows that encompass data acquisition, cleaning,
analysis, and insight generation. In response, we introduce WebDS, the first
end-to-end web-based data science benchmark. It comprises 870 web-based data
science tasks across 29 diverse websites from structured government data
portals to unstructured news media, challenging agents to perform complex,
multi-step operations requiring the use of tools and heterogeneous data formats
that better reflect the realities of modern data analytics. Evaluations of
current SOTA LLM agents indicate significant performance gaps in accomplishing
these tasks. For instance, Browser Use, which accomplishes 80% of tasks on Web
Voyager, successfully completes only 15% of tasks in WebDS, which our analysis
suggests is due to new failure modes like poor information grounding,
repetitive behavior and shortcut-taking that agents performing WebDS' tasks
display. By providing a more robust and realistic testing ground, WebDS sets
the stage for significant advances in the development of practically useful
LLM-based data science.

</details>


### [12] [WarriorMath: Enhancing the Mathematical Ability of Large Language Models with a Defect-aware Framework](https://arxiv.org/abs/2508.01245)
*Yue Chen,Minghua He,Fangkai Yang,Pu Zhao,Lu Wang,Yu Kang,Yifei Dong,Yuefeng Zhan,Hao Sun,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.CL

TL;DR: WarriorMath 通过有针对性的数据合成和渐进式训练，优于强大的基线 12.57%。


<details>
  <summary>Details</summary>
Motivation: 现有方法侧重于通过改述或难度递增来扩充数据集，但忽略了LLM的特定失败模式。这导致合成的问题是模型已经可以解决的，提供的性能增益最小。

Method: 一个缺陷感知数学问题求解框架，结合了有针对性的数据合成和渐进式训练。

Result: 在六个数学基准测试中，WarriorMath 的性能平均比强大的基线高出 12.57%，创造了新的技术水平。

Conclusion: 使用缺陷感知、多专家框架能有效提高数学能力。

Abstract: Large Language Models (LLMs) excel in solving mathematical problems, yet
their performance is often limited by the availability of high-quality, diverse
training data. Existing methods focus on augmenting datasets through rephrasing
or difficulty progression but overlook the specific failure modes of LLMs. This
results in synthetic questions that the model can already solve, providing
minimal performance gains. To address this, we propose WarriorMath, a
defect-aware framework for mathematical problem solving that integrates both
targeted data synthesis and progressive training. In the synthesis stage, we
employ multiple expert LLMs in a collaborative process to generate, critique,
and refine problems. Questions that base LLMs fail to solve are identified and
iteratively improved through expert-level feedback, producing high-quality,
defect-aware training data. In the training stage, we introduce a progressive
learning framework that iteratively fine-tunes the model using increasingly
challenging data tailored to its weaknesses. Experiments on six mathematical
benchmarks show that WarriorMath outperforms strong baselines by 12.57% on
average, setting a new state-of-the-art. Our results demonstrate the
effectiveness of a defect-aware, multi-expert framework for improving
mathematical ability.

</details>


### [13] [Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025](https://arxiv.org/abs/2508.01263)
*Long S. T. Nguyen,Khang H. N. Vo,Thu H. A. Nguyen,Tuan C. Bui,Duc Q. Nguyen,Thanh-Tung Tran,Anh D. Nguyen,Minh L. Nguyen,Fabien Baldacci,Thang H. Bui,Emanuel Di Nardo,Angelo Ciaramella,Son H. Le,Ihsan Ullah,Lorenzo Di Rocco,Tho T. Quan*

Main category: cs.CL

TL;DR: XAI Challenge 2025 was a hackathon that promoted explainable AI in education by building question-answering systems with clear, logic-based explanations using LLMs and symbolic systems.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for transparency and interpretability in AI in education, highlighting the lack of XAI focus in traditional AI hackathons.

Method: The paper analyzes the XAI Challenge 2025, detailing its structure, dataset construction using logic-based templates with Z3 validation and expert student review, and evaluation protocol.

Result: The XAI Challenge 2025 successfully bridged LLMs and symbolic reasoning to enhance explainability in educational QA systems.

Conclusion: This paper presents findings and actionable insights from the XAI Challenge 2025, a hackathon focused on building explainable question-answering systems for education, offering guidance for future XAI educational systems and research.

Abstract: The growing integration of Artificial Intelligence (AI) into education has
intensified the need for transparency and interpretability. While hackathons
have long served as agile environments for rapid AI prototyping, few have
directly addressed eXplainable AI (XAI) in real-world educational contexts.
This paper presents a comprehensive analysis of the XAI Challenge 2025, a
hackathon-style competition jointly organized by Ho Chi Minh City University of
Technology (HCMUT) and the International Workshop on Trustworthiness and
Reliability in Neurosymbolic AI (TRNS-AI), held as part of the International
Joint Conference on Neural Networks (IJCNN 2025). The challenge tasked
participants with building Question-Answering (QA) systems capable of answering
student queries about university policies while generating clear, logic-based
natural language explanations. To promote transparency and trustworthiness,
solutions were required to use lightweight Large Language Models (LLMs) or
hybrid LLM-symbolic systems. A high-quality dataset was provided, constructed
via logic-based templates with Z3 validation and refined through expert student
review to ensure alignment with real-world academic scenarios. We describe the
challenge's motivation, structure, dataset construction, and evaluation
protocol. Situating the competition within the broader evolution of AI
hackathons, we argue that it represents a novel effort to bridge LLMs and
symbolic reasoning in service of explainability. Our findings offer actionable
insights for future XAI-centered educational systems and competitive research
initiatives.

</details>


### [14] [Prompting Large Language Models with Partial Knowledge for Answering Questions with Unseen Entities](https://arxiv.org/abs/2508.01290)
*Zhichao Yan,Jiapu Wang,Jiaoyan Chen,Yanyan Wang,Hongye Tan,Jiye Liang,Xiaoli Li,Ru Li,Jeff Z. Pan*

Main category: cs.CL

TL;DR: This paper proposes a new awakening-based approach for RAG systems that utilizes partially relevant knowledge in LLMs to improve performance in incomplete knowledge base retrieval, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Effectively utilizing partially relevant knowledge is a key challenge for RAG systems, especially in incomplete knowledge base retrieval. The paper challenges the conventional view and proposes that LLMs can be awakened via partially relevant knowledge already embedded in them.

Method: The paper uses triplets and their variants from gold reasoning paths to construct partially relevant knowledge. It includes theoretical analysis and experiments on two Knowledge Graphs (KGs) Question Answering (QA) datasets.

Result: The awakening-based approach demonstrates greater efficacy in practical applications and outperforms traditional methods that rely on embedding-based similarity.

Conclusion: The paper introduces an awakening-based approach that leverages partially relevant knowledge already embedded in LLMs to address the challenges of incomplete knowledge base retrieval in RAG systems. This approach outperforms traditional embedding-based similarity methods, especially in the new Unseen Entity KGQA task.

Abstract: Retrieval-Augmented Generation (RAG) shows impressive performance by
supplementing and substituting parametric knowledge in Large Language Models
(LLMs). Retrieved knowledge can be divided into three types: explicit answer
evidence, implicit answer clue, and insufficient answer context which can be
further categorized into totally irrelevant and partially relevant information.
Effectively utilizing partially relevant knowledge remains a key challenge for
RAG systems, especially in incomplete knowledge base retrieval. Contrary to the
conventional view, we propose a new perspective: LLMs can be awakened via
partially relevant knowledge already embedded in LLMs. To comprehensively
investigate this phenomenon, the triplets located in the gold reasoning path
and their variants are used to construct partially relevant knowledge by
removing the path that contains the answer. We provide theoretical analysis of
the awakening effect in LLMs and support our hypothesis with experiments on two
Knowledge Graphs (KGs) Question Answering (QA) datasets. Furthermore, we
present a new task, Unseen Entity KGQA, simulating real-world challenges where
entity linking fails due to KG incompleteness. Our awakening-based approach
demonstrates greater efficacy in practical applications, outperforms
traditional methods that rely on embedding-based similarity which are prone to
returning noisy information.

</details>


### [15] [KEDAS: Knowledge Editing Alignment with Diverse Augmentation and Self-adaptive Inference](https://arxiv.org/abs/2508.01302)
*Chenming Tang,Yutong Yang,Yunfang Wu*

Main category: cs.CL

TL;DR: KEDAS是一种新的知识编辑方法，它通过对齐、增强和自适应推理来有效修改LLM中的知识，并在性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）中的知识编辑旨在高效地修改过时的知识，同时保留其强大的能力。现有方法主要依赖于参数级编辑或基于检索的方法。

Method: KEDAS通过低秩自适应学习应用上下文编辑知识，利用多样化的编辑增强技术提高编辑的召回率，并采用基于过滤器的智能检索器进行动态推理路由选择。

Result: KEDAS在四个数据集、三个LLM和三个设置的36个案例中，有35个案例获得了最高的总体性能评分，超过了其强大的知识编辑对等方约19.8个编辑成功、局部性和可移植性的调和平均分，并且显著优于参数编辑和基于检索的基线。

Conclusion: KEDAS在知识编辑方面表现出色，优于其他方法，并在多个数据集和模型上取得了最佳性能，同时保持了鲁棒性和效率。

Abstract: Knowledge editing aims to modify outdated knowledge in large language models
(LLMs) efficiently while retaining their powerful capabilities. Most existing
methods rely on either parameter-level editing or retrieval-based approaches.
In this work, we propose Knowledge Editing alignment with Diverse Augmentation
and Self-adaptive inference (KEDAS) to better align LLMs with knowledge
editing. In the alignment phase, LLMs learn to apply in-context edited
knowledge via low-rank adaptation. During editing, we design a diverse edit
augmentation technique to improve the recall of edits. After that, a
self-adaptive post-alignment inference mechanism is proposed, in which a
filter-based smart retriever is employed to perform a dynamic selection of
inference routing. Specifically, irrelevant queries will go through the
original pre-alignment model directly, while relevant ones, together with their
related edits, go through the model with aligned adapters activated. In
experiments, KEDAS secures the highest overall performance scores in 35 out of
36 cases across four datasets with three LLMs on three settings, surpassing its
strong knowledge editing alignment counterpart by about 19.8 harmonic mean
scores of edit success, locality and portability and outperforming both
parameter editing and retrieval-based baselines significantly. Analysis of
computational cost and performance on general tasks further validates the
robustness and efficiency of KEDAS, indicating that it presents an ideal
paradigm of knowledge editing alignment.

</details>


### [16] [D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation](https://arxiv.org/abs/2508.01309)
*Weibo Zhou,Lingbo Li,Shangsong Liang*

Main category: cs.CL

TL;DR: D-SCoRE是一个无训练管道，它利用LLM和提示工程从任意文本源生成多样化、高质量的QA数据集，从而实现高效的QA生成和跨领域的高性能微调。


<details>
  <summary>Details</summary>
Motivation: 高质量问答（QA）数据集的稀缺性和高成本阻碍了领域特定大型语言模型（LLM）的监督微调（SFT）。

Method: D-SCoRE，一个利用LLM和提示工程从任意文本源生成多样化、高质量QA数据集的无训练管道。

Result: D-SCoRE生成六个QA-CoT对，每个100-200字的文本带有四选项反事实材料，使用8B LLM在消费级硬件上花费90秒。

Conclusion: D-SCoRE在大多数领域都优于人工标注的QA数据集。

Abstract: The scarcity and high cost of high-quality question-answering (QA) datasets
hinder supervised fine-tuning (SFT) for domain-specific large language models
(LLMs). To address this, we introduce D-SCoRE, a training-free pipeline that
utilizes LLMs and prompt engineering to produce diverse, high-quality QA
datasets from arbitrary textual sources. D-SCoRE integrates
$\textbf{D}$ocument-centric processing, $\textbf{S}$egmentation, $\textbf{Co}$T
$\textbf{R}$easoning, and structured $\textbf{E}$xport to generate QA-COT
datasets tailored for domain-aware SFT. Multi-dimensional control mechanisms,
such as semantic role transformation, question type balancing, and
counterfactual materials, enhance diversity and relevance, overcoming
limitations of existing QA generation. LLMs fine-tuned on D-SCoRE-generated QA
datasets, and human-annotated QA datasets (SQuAD, Covid-QA) are evaluated on
SQuADShifts and Covid-QA test sets, with D-SCoRE outperforming across most
domains. D-SCoRE generates six QA-CoT pairs with four-option counterfactual
materials per 100-200-word text in 90 seconds using an 8B LLM on consumer-grade
hardware. Its simplicity and scalability enable efficient QA generation and
high-performance fine-tuning across domains.

</details>


### [17] [LinkQA: Synthesizing Diverse QA from Multiple Seeds Strongly Linked by Knowledge Points](https://arxiv.org/abs/2508.01317)
*Xuemiao Zhang,Can Ren,Chengying Tu,Rongxiang Weng,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.CL

TL;DR: LinkSyn is a KP graph-based synthesis framework that enables flexible control over discipline and difficulty distributions while balancing KP coverage and popularity. It synthesizes LinkQA, a diverse multi-disciplinary QA dataset with 50B tokens. Continual pre-training with LinkQA yields an average improvement of 11.51% on MMLU and CMMLU, establishing new SOTA results.


<details>
  <summary>Details</summary>
Motivation: The advancement of large language models (LLMs) struggles with the scarcity of high-quality, diverse training data.

Method: a novel knowledge point (KP) graph-based synthesis framework that enables flexible control over discipline and difficulty distributions while balancing KP coverage and popularity

Result: synthesize LinkQA, a diverse multi-disciplinary QA dataset with 50B tokens

Conclusion: Continual pre-training with LinkQA yields an average improvement of 11.51% on MMLU and CMMLU, establishing new SOTA results. LinkQA consistently enhances performance across model size and initial FLOPs scales.

Abstract: The advancement of large language models (LLMs) struggles with the scarcity
of high-quality, diverse training data. To address this limitation, we propose
LinkSyn, a novel knowledge point (KP) graph-based synthesis framework that
enables flexible control over discipline and difficulty distributions while
balancing KP coverage and popularity. LinkSyn extracts KPs from
question-answering (QA) seed data and constructs a KP graph to synthesize
diverse QA data from multiple seeds strongly linked by KPs and sampled from
graph walks. Specifically, LinkSyn incorporates (1) a knowledge distribution
value function to guide the adjustment of path sampling probability and balance
KP coverage and popularity during graph walks; (2) diffusion-based synthesis
via DeepSeek-R1 by leveraging multiple seeds with dense logical associations
along each path; and (3) high-difficulty QA enhancement within given
disciplines by flexible difficulty adjustments. By executing LinkSyn, we
synthesize LinkQA, a diverse multi-disciplinary QA dataset with 50B tokens.
Extensive experiments on Llama-3 8B demonstrate that continual pre-training
with LinkQA yields an average improvement of $\mathbf{11.51\%}$ on MMLU and
CMMLU, establishing new SOTA results. LinkQA consistently enhances performance
across model size and initial FLOPs scales.

</details>


### [18] [Large-Scale Diverse Synthesis for Mid-Training](https://arxiv.org/abs/2508.01326)
*Xuemiao Zhang,Chengying Tu,Can Ren,Rongxiang Weng,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.CL

TL;DR: 提出了BoostQA，一个大规模QA数据集，用于mid-training，显著提升了Llama-3 8B在多个基准测试上的性能。


<details>
  <summary>Details</summary>
Motivation: 高质量、知识密集型训练数据的稀缺性阻碍了大型语言模型（LLM）的发展，因为传统的语料库提供的信息有限。以往的研究合成了依赖于语料库的问答（QA）数据以提高模型性能，但在QA数据的可扩展性和知识多样性方面面临挑战，尤其是在跨领域环境中。

Method: 提出了一个新颖的多样化pipeline来合成BoostQA，一个100B token的大规模QA数据集。该框架包括：(1) 从异构来源收集种子数据；(2) 利用DeepSeek-R1实现STEM领域的多梯度合成，以提高数据多样性和高难度合成，以减轻难度退化；(3) 通过DeepSeek-V3改进答案，以提高输出质量。

Result: BoostQA在模型规模、数据量和初始FLOPs扩展时，性能持续提高，展示了强大的可扩展性。

Conclusion: Llama-3 8B模型通过在包含40B token的BoostQA数据集上进行mid-training，在MMLU和CMMLU上平均提升了12.74%，并在12个基准测试中取得了SOTA的平均性能。BoostQA还表现出强大的可扩展性。

Abstract: The scarcity of high-quality, knowledge-intensive training data hinders the
development of large language models (LLMs), as traditional corpora provide
limited information. Previous studies have synthesized and integrated
corpora-dependent question-answering (QA) data to improve model performance but
face challenges in QA data scalability and knowledge diversity, particularly in
cross-domain contexts. Furthermore, leveraging our designed discipline and
difficulty annotation system, we probe model deficiencies in STEM disciplines
and high-difficulty data. To overcome these limitations, we propose a novel
diversified pipeline to synthesize BoostQA, a 100B-token large-scale QA
dataset. Our synthesis framework: (1) curates seed data from heterogeneous
sources; (2) utilizes DeepSeek-R1 to implement STEM-focused multi-grade
synthesis to boost data diversity and high-difficulty synthesis to mitigate
difficulty degradation; (3) refines answers via DeepSeek-V3 to improve output
quality. We utilize BoostQA in mid-training, a mid-stage between pre-training
and post-training, to optimize domain-specific knowledge acquisition and
enhance data quality. Our method enables Llama-3 8B, mid-trained on a 40B-token
dataset, to achieve an average improvement of $\mathbf{12.74\%}$ on MMLU and
CMMLU and establish SOTA average performance across 12 benchmarks. BoostQA also
demonstrates robust scalability, with performance consistently improving as
model size, data volume, and initial FLOPs scale.

</details>


### [19] [MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and Analysis](https://arxiv.org/abs/2508.01370)
*Roman Koshkin,Pengyu Dai,Nozomi Fujikawa,Masahito Togami,Marco Visentini-Scarzanella*

Main category: cs.CL

TL;DR: This paper presents an autonomous framework that leverages Large Language Models (LLMs) to automate end-to-end business analysis and market report generation.


<details>
  <summary>Details</summary>
Motivation: automate end-to-end business analysis and market report generation

Method: The system employs specialized agents - Researcher, Reviewer, Writer, and Retriever - that collaborate to analyze data and produce comprehensive reports. These agents learn from real professional consultants' presentation materials at Amazon through in-context learning to replicate professional analytical methodologies. The framework executes a multi-step process: querying databases, analyzing data, generating insights, creating visualizations, and composing market reports.

Result: report quality can be improved by both automated review cycles and consultants' unstructured knowledge. In experimental validation, our framework generates detailed 6-page reports in 7 minutes at a cost of approximately $1.

Conclusion: This work could be an important step to automatically create affordable market insights.

Abstract: We present an autonomous framework that leverages Large Language Models
(LLMs) to automate end-to-end business analysis and market report generation.
At its core, the system employs specialized agents - Researcher, Reviewer,
Writer, and Retriever - that collaborate to analyze data and produce
comprehensive reports. These agents learn from real professional consultants'
presentation materials at Amazon through in-context learning to replicate
professional analytical methodologies. The framework executes a multi-step
process: querying databases, analyzing data, generating insights, creating
visualizations, and composing market reports. We also introduce a novel
LLM-based evaluation system for assessing report quality, which shows alignment
with expert human evaluations. Building on these evaluations, we implement an
iterative improvement mechanism that optimizes report quality through automated
review cycles. Experimental results show that report quality can be improved by
both automated review cycles and consultants' unstructured knowledge. In
experimental validation, our framework generates detailed 6-page reports in 7
minutes at a cost of approximately \$1. Our work could be an important step to
automatically create affordable market insights.

</details>


### [20] [MedSynth: Realistic, Synthetic Medical Dialogue-Note Pairs](https://arxiv.org/abs/2508.01401)
*Ahmad Rezaie Mianroodi,Amirali Rezaie,Niko Grisel Todorov,Cyril Rakovski,Frank Rudzicz*

Main category: cs.CL

TL;DR: MedSynth is a new dataset of synthetic medical dialogues and notes that improves the performance of models in generating medical notes from dialogues, and dialogues from medical notes.


<details>
  <summary>Details</summary>
Motivation: Physicians spend significant time documenting clinical encounters, a burden that contributes to professional burnout. To address this, robust automation tools for medical documentation are crucial.

Method: A novel dataset of synthetic medical dialogues and notes designed to advance the Dialogue-to-Note (Dial-2-Note) and Note-to-Dialogue (Note-2-Dial) tasks.

Result: The dataset includes over 10,000 dialogue-note pairs covering over 2000 ICD-10 codes.

Conclusion: The MedSynth dataset enhances the performance of models in generating medical notes from dialogues, and dialogues from medical notes.

Abstract: Physicians spend significant time documenting clinical encounters, a burden
that contributes to professional burnout. To address this, robust automation
tools for medical documentation are crucial. We introduce MedSynth -- a novel
dataset of synthetic medical dialogues and notes designed to advance the
Dialogue-to-Note (Dial-2-Note) and Note-to-Dialogue (Note-2-Dial) tasks.
Informed by an extensive analysis of disease distributions, this dataset
includes over 10,000 dialogue-note pairs covering over 2000 ICD-10 codes. We
demonstrate that our dataset markedly enhances the performance of models in
generating medical notes from dialogues, and dialogues from medical notes. The
dataset provides a valuable resource in a field where open-access,
privacy-compliant, and diverse training data are scarce. Code is available at
https://github.com/ahmadrezarm/MedSynth/tree/main and the dataset is available
at https://huggingface.co/datasets/Ahmad0067/MedSynth.

</details>


### [21] [ArzEn-MultiGenre: An aligned parallel dataset of Egyptian Arabic song lyrics, novels, and subtitles, with English translations](https://arxiv.org/abs/2508.01411)
*Rania Al-Sabbagh*

Main category: cs.CL

TL;DR: ArzEn-MultiGenre是一个人工翻译的埃及阿拉伯语-英语平行数据集，包含25,557个片段对，可用于各种目的。


<details>
  <summary>Details</summary>
Motivation: 现有的平行埃及阿拉伯语和英语数据集中没有文本类型。

Method: 人工翻译和对齐埃及阿拉伯语歌曲歌词、小说和电视节目字幕及其英语对应版本。

Result: 该数据集包含25,557个片段对。

Conclusion: ArzEn-MultiGenre是一个高质量的平行语料库，由人工翻译和对齐，可用于机器翻译模型的基准测试、微调大型语言模型和调整商业机器翻译应用程序。

Abstract: ArzEn-MultiGenre is a parallel dataset of Egyptian Arabic song lyrics,
novels, and TV show subtitles that are manually translated and aligned with
their English counterparts. The dataset contains 25,557 segment pairs that can
be used to benchmark new machine translation models, fine-tune large language
models in few-shot settings, and adapt commercial machine translation
applications such as Google Translate. Additionally, the dataset is a valuable
resource for research in various disciplines, including translation studies,
cross-linguistic analysis, and lexical semantics. The dataset can also serve
pedagogical purposes by training translation students and aid professional
translators as a translation memory. The contributions are twofold: first, the
dataset features textual genres not found in existing parallel Egyptian Arabic
and English datasets, and second, it is a gold-standard dataset that has been
translated and aligned by human experts.

</details>


### [22] [Discovering Bias Associations through Open-Ended LLM Generations](https://arxiv.org/abs/2508.01412)
*Jinhao Pan,Chahat Raj,Ziwei Zhu*

Main category: cs.CL

TL;DR: This paper introduces Bias Association Discovery Framework (BADF), a systematic approach for extracting both known and previously unrecognized associations between demographic identities and descriptive concepts from open-ended LLM outputs. It helps to advance the understanding of biases in open-ended generation and provide a scalable tool for identifying and analyzing bias associations in LLMs.


<details>
  <summary>Details</summary>
Motivation: Social biases embedded in Large Language Models (LLMs) raise critical concerns, resulting in representational harms -- unfair or distorted portrayals of demographic groups -- that may be expressed in subtle ways through generated language. Existing evaluation methods often depend on predefined identity-concept associations, limiting their ability to surface new or unexpected forms of bias.

Method: We present the Bias Association Discovery Framework (BADF), a systematic approach for extracting both known and previously unrecognized associations between demographic identities and descriptive concepts from open-ended LLM outputs. Through comprehensive experiments spanning multiple models and diverse real-world contexts

Result: BADF enables robust mapping and analysis of the varied concepts that characterize demographic identities.

Conclusion: BADF enables robust mapping and analysis of the varied concepts that characterize demographic identities. Our findings advance the understanding of biases in open-ended generation and provide a scalable tool for identifying and analyzing bias associations in LLMs.

Abstract: Social biases embedded in Large Language Models (LLMs) raise critical
concerns, resulting in representational harms -- unfair or distorted portrayals
of demographic groups -- that may be expressed in subtle ways through generated
language. Existing evaluation methods often depend on predefined
identity-concept associations, limiting their ability to surface new or
unexpected forms of bias. In this work, we present the Bias Association
Discovery Framework (BADF), a systematic approach for extracting both known and
previously unrecognized associations between demographic identities and
descriptive concepts from open-ended LLM outputs. Through comprehensive
experiments spanning multiple models and diverse real-world contexts, BADF
enables robust mapping and analysis of the varied concepts that characterize
demographic identities. Our findings advance the understanding of biases in
open-ended generation and provide a scalable tool for identifying and analyzing
bias associations in LLMs. Data, code, and results are available at
https://github.com/JP-25/Discover-Open-Ended-Generation

</details>


### [23] [From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs](https://arxiv.org/abs/2508.01424)
*Haonan Bian,Yutao Qi,Rui Yang,Yuanxi Che,Jiaqian Wang,Heming Xia,Ranran Zhen*

Main category: cs.CL

TL;DR: ORACLE, a training-free framework, enhances LLMs for complex reasoning by using knowledge graphs to build question-specific ontologies and decompose queries into logical sub-questions, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex multi-hop question answering (MQA) due to their inability to capture deep conceptual relationships between entities.

Method: A training-free framework combining LLMs with knowledge graphs through dynamic ontology construction, transformation into First-Order Logic chains, and systematic query decomposition.

Result: ORACLE achieves highly competitive performance on MQA benchmarks and generates more logical and interpretable reasoning chains.

Conclusion: The ORACLE framework achieves highly competitive performance on MQA benchmarks, rivaling state-of-the-art models and generating more logical reasoning chains.

Abstract: Large Language Models (LLMs), despite their success in question answering,
exhibit limitations in complex multi-hop question answering (MQA) tasks that
necessitate non-linear, structured reasoning. This limitation stems from their
inability to adequately capture deep conceptual relationships between entities.
To overcome this challenge, we present **ORACLE** (**O**ntology-driven
**R**easoning **A**nd **C**hain for **L**ogical **E**ucidation), a
training-free framework that combines LLMs' generative capabilities with the
structural benefits of knowledge graphs. Our approach operates through three
stages: (1) dynamic construction of question-specific knowledge ontologies
using LLMs, (2) transformation of these ontologies into First-Order Logic
reasoning chains, and (3) systematic decomposition of the original query into
logically coherent sub-questions. Experimental results on several standard MQA
benchmarks show that our framework achieves highly competitive performance,
rivaling current state-of-the-art models like DeepSeek-R1. Detailed analyses
further confirm the effectiveness of each component, while demonstrating that
our method generates more logical and interpretable reasoning chains than
existing approaches.

</details>


### [24] [Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data](https://arxiv.org/abs/2508.01450)
*Xinlin Zhuang,Feilong Tang,Haolin Yang,Ming Hu,Huifa Li,Haochen Xue,Yichen Li,Junjun He,Zongyuan Ge,Ying Qian,Imran Razzak*

Main category: cs.CL

TL;DR: DIQ是一种新的数据选择策略，它优先考虑高难度-高影响的样本，以提高医学推理的效率和质量，实验表明，仅使用少量数据即可达到甚至超过完整数据集的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的SFT实践通常依赖于包含冗余和低质量样本的未过滤数据集，导致大量的计算成本和次优的性能。虽然现有的方法试图通过基于样本难度（由知识和推理复杂性定义）选择数据来缓解这个问题，但它们忽略了每个样本的优化效用，而这种效用体现在其梯度中。

Method: 提出了一种数据选择策略，即难度-影响象限（DIQ），该策略优先考虑高难度-高影响象限中的样本，以平衡复杂的临床推理和实质性的梯度影响，从而能够以最少的微调数据实现高效的医疗推理。

Result: DIQ选择的子集表现出更高的数据质量，并生成与差异诊断、安全检查和证据引用的专家实践更一致的临床推理，因为DIQ强调培养类似专家的推理模式的样本。

Conclusion: DIQ在医疗推理基准测试中，仅使用1%的精选数据就能达到完整数据集的性能，而使用10%的数据始终优于基线，突出了有原则的数据选择相对于暴力扩展的优越性。

Abstract: Supervised Fine-Tuning (SFT) plays a pivotal role in adapting Large Language
Models (LLMs) to specialized domains such as medical reasoning. However,
existing SFT practices often rely on unfiltered datasets that contain redundant
and low-quality samples, leading to substantial computational costs and
suboptimal performance. Although existing methods attempt to alleviate this
problem by selecting data based on sample difficulty, defined by knowledge and
reasoning complexity, they overlook each sample's optimization utility
reflected in its gradient. Interestingly, we find that gradient-based influence
alone favors easy-to-optimize samples that cause large parameter shifts but
lack deep reasoning chains, while difficulty alone selects noisy or overly
complex cases that fail to guide stable optimization. Based on this
observation, we propose a data selection strategy, Difficulty-Influence
Quadrant (DIQ), which prioritizes samples in the high-difficulty-high-influence
quadrant to balance complex clinical reasoning with substantial gradient
influence, enabling efficient medical reasoning with minimal fine-tuning data.
Furthermore, Human and LLM-as-a-judge evaluations show that DIQ-selected
subsets demonstrate higher data quality and generate clinical reasoning that is
more aligned with expert practices in differential diagnosis, safety check, and
evidence citation, as DIQ emphasizes samples that foster expert-like reasoning
patterns. Extensive experiments on medical reasoning benchmarks demonstrate
that DIQ enables models fine-tuned on only 1% of selected data to match
full-dataset performance, while using 10% consistently outperforms the
baseline, highlighting the superiority of principled data selection over
brute-force scaling. The code and data are available at
https://github.com/mihara-bot/DIQ.

</details>


### [25] [TreeDiff: AST-Guided Code Generation with Diffusion LLMs](https://arxiv.org/abs/2508.01473)
*Yiming Zeng,Jinghan Cao,Zexin Li,Yiming Chen,Tao Ren,Dawei Xiang,Xidong Wu,Shangqian Gao,Tingting Yu*

Main category: cs.CL

TL;DR: This paper introduces a syntax-aware diffusion framework that incorporates structural priors from Abstract Syntax Trees (ASTs) into the denoising process for code generation. Experimental results demonstrate that syntax-aware corruption significantly improves syntactic correctness, reconstruction accuracy, and generalization to unseen code patterns.


<details>
  <summary>Details</summary>
Motivation: Applying diffusion models to structured domains such as source code remains a significant challenge. Standard token-level corruption techniques used during training often ignore this structure, which may hinder the model's ability to learn meaningful representations of code.

Method: a syntax-aware diffusion framework that incorporates structural priors from Abstract Syntax Trees (ASTs) into the denoising process. Instead of masking individual tokens at random, we selectively corrupt syntactically meaningful code spans derived from AST subtrees.

Result: syntax-aware corruption significantly improves syntactic correctness, reconstruction accuracy, and generalization to unseen code patterns.

Conclusion: syntax-aware corruption significantly improves syntactic correctness, reconstruction accuracy, and generalization to unseen code patterns. These findings highlight the potential of incorporating structural information into diffusion-based training and suggest that syntax-guided denoising is a promising direction for advancing diffusion-based language models in code generation tasks.

Abstract: Recent advances in diffusion-based language models have opened new
possibilities for controllable and bidirectional sequence generation. These
models provide an alternative to traditional autoregressive approaches by
framing text generation as an iterative denoising process. However, applying
diffusion models to structured domains such as source code remains a
significant challenge. Programming languages differ from natural language in
that they follow strict syntactic and semantic rules, with hierarchical
organization that must be preserved for correctness. Standard token-level
corruption techniques used during training often ignore this structure, which
may hinder the model's ability to learn meaningful representations of code. To
address this limitation, we propose a syntax-aware diffusion framework that
incorporates structural priors from Abstract Syntax Trees (ASTs) into the
denoising process. Instead of masking individual tokens at random, we
selectively corrupt syntactically meaningful code spans derived from AST
subtrees. This enables the model to reconstruct programs in a way that respects
grammatical boundaries and captures long-range dependencies. Experimental
results demonstrate that syntax-aware corruption significantly improves
syntactic correctness, reconstruction accuracy, and generalization to unseen
code patterns. These findings highlight the potential of incorporating
structural information into diffusion-based training and suggest that
syntax-guided denoising is a promising direction for advancing diffusion-based
language models in code generation tasks.

</details>


### [26] [Harnessing Collective Intelligence of LLMs for Robust Biomedical QA: A Multi-Model Approach](https://arxiv.org/abs/2508.01480)
*Dimitra Panou,Alexandros C. Dimopoulos,Manolis Koubarakis,Martin Reczko*

Main category: cs.CL

TL;DR: 该论文介绍了参与BioASQ挑战赛的工作，使用大型语言模型回答生物医学问题，并通过模型组合取得了优异的成果。


<details>
  <summary>Details</summary>
Motivation: 生物医学文本挖掘和问答至关重要但要求很高，特别是在生物医学文献呈指数增长的情况下。

Method: 使用各种模型处理问题。多数投票系统结合它们的输出来确定Yes/No问题的最终答案，而对于列表和类事实型问题，使用它们的答案的并集。

Result: 我们评估了13个最先进的开源LLM，探索所有可能的模型组合以贡献最终答案，从而为每种问题类型定制LLM管道。我们的发现提供了有价值的见解，即哪些LLM组合始终能为特定问题类型产生优异的结果。

Conclusion: 在2025 BioASQ挑战赛的四个回合中，该系统取得了显著成果：在协同任务中，在第二轮的理想答案中获得第一名，在精确答案中获得第二名，以及在第三轮和第四轮的精确答案中获得两个并列第一名。

Abstract: Biomedical text mining and question-answering are essential yet highly
demanding tasks, particularly in the face of the exponential growth of
biomedical literature. In this work, we present our participation in the 13th
edition of the BioASQ challenge, which involves biomedical semantic
question-answering for Task 13b and biomedical question-answering for
developing topics for the Synergy task. We deploy a selection of open-source
large language models (LLMs) as retrieval-augmented generators to answer
biomedical questions. Various models are used to process the questions. A
majority voting system combines their output to determine the final answer for
Yes/No questions, while for list and factoid type questions, the union of their
answers in used. We evaluated 13 state-of-the-art open source LLMs, exploring
all possible model combinations to contribute to the final answer, resulting in
tailored LLM pipelines for each question type. Our findings provide valuable
insight into which combinations of LLMs consistently produce superior results
for specific question types. In the four rounds of the 2025 BioASQ challenge,
our system achieved notable results: in the Synergy task, we secured 1st place
for ideal answers and 2nd place for exact answers in round 2, as well as two
shared 1st places for exact answers in round 3 and 4.

</details>


### [27] [TeSent: A Benchmark Dataset for Fairness-aware Explainable Sentiment Classification in Telugu](https://arxiv.org/abs/2508.01486)
*Vallabhaneni Raj Kumar,Ashwin S,Supriya Manna,Niladri Sett,Cheedella V S N M S Hema Harshitha,Kurakula Harshitha,Anand Kumar Sharma,Basina Deepakraj,Tanuj Sarkar,Bondada Navaneeth Krishna,Samanthapudi Shakeer*

Main category: cs.CL

TL;DR: 介绍了 TeSent，这是一个用于泰卢固语情感分类的综合基准数据集，并提供了用于评估可解释性和公平性的工具。


<details>
  <summary>Details</summary>
Motivation: 泰卢固语是印度使用最广泛的德拉威语，但在全球 NLP 和机器学习领域仍然没有得到充分的代表性，这主要是由于缺乏高质量的注释资源。

Method: 从各种社交媒体平台、新闻网站和网络博客中抓取泰卢固语文本，涵盖多个领域，以预处理并生成 26,150 个句子，并开发了一个定制的注释平台和一个精心设计的注释协议，用于收集基本事实标签及其人工注释的理由。

Result: 我们引入了 TeSent，这是一个全面的情感分类基准数据集，情感分类是泰卢固语中的一个关键文本分类问题。我们提供了详细的合理性和忠实性评估套件，该套件利用了理由，适用于应用于训练模型的六个广泛使用的事后解释器。最后，我们整理了泰卢固语的 TeEEC（公平性评估语料库），这是一个用于评估泰卢固语情感和情感相关 NLP 任务的公平性的语料库，并为训练的分类器模型提供了一个公平性评估套件。

Conclusion: 训练与理由可能提高模型准确性，减少模型偏差，并使解释器的输出更符合人类推理。

Abstract: In the Indian subcontinent, Telugu, one of India's six classical languages,
is the most widely spoken Dravidian Language. Despite its 96 million speaker
base worldwide, Telugu remains underrepresented in the global NLP and Machine
Learning landscape, mainly due to lack of high-quality annotated resources.
This work introduces TeSent, a comprehensive benchmark dataset for sentiment
classification, a key text classification problem, in Telugu. TeSent not only
provides ground truth labels for the sentences, but also supplements with
provisions for evaluating explainability and fairness, two critical
requirements in modern-day machine learning tasks. We scraped Telugu texts
covering multiple domains from various social media platforms, news websites
and web-blogs to preprocess and generate 26,150 sentences, and developed a
custom-built annotation platform and a carefully crafted annotation protocol
for collecting the ground truth labels along with their human-annotated
rationales. We then fine-tuned several SOTA pre-trained models in two ways:
with rationales, and without rationales. Further, we provide a detailed
plausibility and faithfulness evaluation suite, which exploits the rationales,
for six widely used post-hoc explainers applied on the trained models. Lastly,
we curate TeEEC, Equity Evaluation Corpus in Telugu, a corpus to evaluate
fairness of Telugu sentiment and emotion related NLP tasks, and provide a
fairness evaluation suite for the trained classifier models. Our experimental
results suggest that training with rationales may improve model accuracy,
reduce bias in models, and make the explainers' output more aligned to human
reasoning.

</details>


### [28] [The Homogenizing Effect of Large Language Models on Human Expression and Thought](https://arxiv.org/abs/2508.01491)
*Zhivar Sourati,Alireza S. Ziabari,Morteza Dehghani*

Main category: cs.CL

TL;DR: LLMs threaten cognitive diversity by standardizing language and reasoning, potentially harming creativity and collective intelligence.


<details>
  <summary>Details</summary>
Motivation: Cognitive diversity is essential to creativity and collective intelligence, but LLMs risk standardizing language and reasoning.

Method: Review synthesizes evidence across linguistics, cognitive, and computer science.

Result: LLMs reflect and reinforce dominant styles, marginalizing alternative voices and reasoning strategies due to their design and widespread use mirroring training data patterns and amplifying convergence.

Conclusion: Unchecked homogenization risks flattening cognitive landscapes, hindering collective intelligence and adaptability.

Abstract: Cognitive diversity, reflected in variations of language, perspective, and
reasoning, is essential to creativity and collective intelligence. This
diversity is rich and grounded in culture, history, and individual experience.
Yet as large language models (LLMs) become deeply embedded in people's lives,
they risk standardizing language and reasoning. This Review synthesizes
evidence across linguistics, cognitive, and computer science to show how LLMs
reflect and reinforce dominant styles while marginalizing alternative voices
and reasoning strategies. We examine how their design and widespread use
contribute to this effect by mirroring patterns in their training data and
amplifying convergence as all people increasingly rely on the same models
across contexts. Unchecked, this homogenization risks flattening the cognitive
landscapes that drive collective intelligence and adaptability.

</details>


### [29] [A Theory of Adaptive Scaffolding for LLM-Based Pedagogical Agents](https://arxiv.org/abs/2508.01503)
*Clayton Cohn,Surya Rayala,Namrata Srivastava,Joyce Horn Fonteles,Shruti Jain,Xinying Luo,Divya Mereddy,Naveeduddin Mohammed,Gautam Biswas*

Main category: cs.CL

TL;DR: This paper proposes a framework combining Evidence-Centered Design with Social Cognitive Theory for adaptive scaffolding in LLM-based agents, and illustrates it with Inquizzitor, an LLM-based formative assessment agent.


<details>
  <summary>Details</summary>
Motivation: Current LLM systems in classrooms lack the theoretical foundation of earlier intelligent tutoring systems.

Method: combining Evidence-Centered Design with Social Cognitive Theory

Result: Inquizzitor delivers high-quality assessment and interaction aligned with core learning theories.

Conclusion: LLMs can provide adaptive and principled instruction in education through theory-driven integration.

Abstract: Large language models (LLMs) present new opportunities for creating
pedagogical agents that engage in meaningful dialogue to support student
learning. However, the current use of LLM systems like ChatGPT in classrooms
often lacks the solid theoretical foundation found in earlier intelligent
tutoring systems. To bridge this gap, we propose a framework that combines
Evidence-Centered Design with Social Cognitive Theory for adaptive scaffolding
in LLM-based agents focused on STEM+C learning. We illustrate this framework
with Inquizzitor, an LLM-based formative assessment agent that integrates
human-AI hybrid intelligence and provides feedback grounded in cognitive
science principles. Our findings show that Inquizzitor delivers high-quality
assessment and interaction aligned with core learning theories, offering
teachers effective guidance that students value. This research underscores the
potential for theory-driven LLM integration in education, highlighting the
ability of these systems to provide adaptive and principled instruction.

</details>


### [30] [MOPrompt: Multi-objective Semantic Evolution for Prompt Optimization](https://arxiv.org/abs/2508.01541)
*Sara Câmara,Eduardo Luz,Valéria Carvalho,Ivan Meneghini,Gladston Moreira*

Main category: cs.CL

TL;DR: MOPrompt, a multi-objective evolutionary optimization framework, optimizes prompts for accuracy and context size, outperforming baselines by achieving similar accuracy with 31% less token length.


<details>
  <summary>Details</summary>
Motivation: Manual prompt design is complex, non-intuitive, and time-consuming, while existing automated methods fail to balance efficiency and effectiveness.

Method: A Multi-objective Evolutionary Optimization (EMO) framework is used to optimize prompts for both accuracy and context size.

Result: MOPrompt outperforms the baseline framework in sentiment analysis, achieving comparable accuracy with significantly reduced token length.

Conclusion: MOPrompt identifies prompts that match baseline accuracy with a 31% reduction in token length for the Sabiazinho model.

Abstract: Prompt engineering is crucial for unlocking the potential of Large Language
Models (LLMs). Still, since manual prompt design is often complex,
non-intuitive, and time-consuming, automatic prompt optimization has emerged as
a research area. However, a significant challenge in prompt optimization is
managing the inherent trade-off between task performance, such as accuracy, and
context size. Most existing automated methods focus on a single objective,
typically performance, thereby failing to explore the critical spectrum of
efficiency and effectiveness. This paper introduces the MOPrompt, a novel
Multi-objective Evolutionary Optimization (EMO) framework designed to optimize
prompts for both accuracy and context size (measured in tokens) simultaneously.
Our framework maps the Pareto front of prompt solutions, presenting
practitioners with a set of trade-offs between context size and performance, a
crucial tool for deploying Large Language Models (LLMs) in real-world
applications. We evaluate MOPrompt on a sentiment analysis task in Portuguese,
using Gemma-2B and Sabiazinho-3 as evaluation models. Our findings show that
MOPrompt substantially outperforms the baseline framework. For the Sabiazinho
model, MOPrompt identifies a prompt that achieves the same peak accuracy (0.97)
as the best baseline solution, but with a 31% reduction in token length.

</details>


### [31] [Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models](https://arxiv.org/abs/2508.01554)
*Yujia Zheng,Tianhao Li,Haotian Huang,Tianyu Zeng,Jingyu Lu,Chuangxin Chu,Yuekai Huang,Ziyou Jiang,Qian Xiong,Yuyao Ge,Mingyang Li*

Main category: cs.CL

TL;DR: PromptAnatomy dissects prompts to generate adversarial examples, achieving state-of-the-art attack success rates, highlighting the importance of prompt structure.


<details>
  <summary>Details</summary>
Motivation: Existing approaches often treat prompts as monolithic text, overlooking their structural heterogeneity. Different prompt components contribute unequally to adversarial robustness, and complex, domain-specific prompts with rich structures have components with differing vulnerabilities.

Method: An automated framework that dissects prompts into functional components and generates diverse, interpretable adversarial examples by selectively perturbing each component using a proposed method, ComPerturb, incorporating a perplexity (PPL)-based filtering mechanism.

Result: ComPerturb achieves state-of-the-art attack success rates. Ablation studies validate the complementary benefits of prompt dissection and PPL filtering.

Conclusion: Prompt structure awareness and controlled perturbation are important for reliable adversarial robustness evaluation in LLMs.

Abstract: Prompt-based adversarial attacks have become an effective means to assess the
robustness of large language models (LLMs). However, existing approaches often
treat prompts as monolithic text, overlooking their structural
heterogeneity-different prompt components contribute unequally to adversarial
robustness. Prior works like PromptRobust assume prompts are value-neutral, but
our analysis reveals that complex, domain-specific prompts with rich structures
have components with differing vulnerabilities. To address this gap, we
introduce PromptAnatomy, an automated framework that dissects prompts into
functional components and generates diverse, interpretable adversarial examples
by selectively perturbing each component using our proposed method, ComPerturb.
To ensure linguistic plausibility and mitigate distribution shifts, we further
incorporate a perplexity (PPL)-based filtering mechanism. As a complementary
resource, we annotate four public instruction-tuning datasets using the
PromptAnatomy framework, verified through human review. Extensive experiments
across these datasets and five advanced LLMs demonstrate that ComPerturb
achieves state-of-the-art attack success rates. Ablation studies validate the
complementary benefits of prompt dissection and PPL filtering. Our results
underscore the importance of prompt structure awareness and controlled
perturbation for reliable adversarial robustness evaluation in LLMs. Code and
data are available at https://github.com/Yujiaaaaa/PACP.

</details>


### [32] [OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets](https://arxiv.org/abs/2508.01630)
*Maziyar Panahi*

Main category: cs.CL

TL;DR: OpenMed NER, a suite of open-source, domain-adapted transformer models, achieves state-of-the-art performance on biomedical NER benchmarks with high efficiency and a low carbon footprint.


<details>
  <summary>Details</summary>
Motivation: Achieving state-of-the-art performance across diverse entity types while maintaining computational efficiency in named-entity recognition (NER) remains a significant challenge.

Method: Lightweight domain-adaptive pre-training (DAPT) with parameter-efficient Low-Rank Adaptation (LoRA) on DeBERTa-v3, PubMedBERT, and BioELECTRA backbones.

Result: Achieves new state-of-the-art micro-F1 scores on 10 of 12 established biomedical NER benchmarks, with substantial gains across diverse entity types. Training completes in under 12 hours on a single GPU with a low carbon footprint.

Conclusion: Strategically adapted open-source models can surpass closed-source solutions with remarkable efficiency.

Abstract: Named-entity recognition (NER) is fundamental to extracting structured
information from the >80% of healthcare data that resides in unstructured
clinical notes and biomedical literature. Despite recent advances with large
language models, achieving state-of-the-art performance across diverse entity
types while maintaining computational efficiency remains a significant
challenge. We introduce OpenMed NER, a suite of open-source, domain-adapted
transformer models that combine lightweight domain-adaptive pre-training (DAPT)
with parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs
cost-effective DAPT on a 350k-passage corpus compiled from ethically sourced,
publicly available research repositories and de-identified clinical notes
(PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA
backbones. This is followed by task-specific fine-tuning with LoRA, which
updates less than 1.5% of model parameters. We evaluate our models on 12
established biomedical NER benchmarks spanning chemicals, diseases, genes, and
species. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of
these 12 datasets, with substantial gains across diverse entity types. Our
models advance the state-of-the-art on foundational disease and chemical
benchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger
improvements of over 5.3 and 9.7 percentage points on more specialized gene and
clinical cell line corpora. This work demonstrates that strategically adapted
open-source models can surpass closed-source solutions. This performance is
achieved with remarkable efficiency: training completes in under 12 hours on a
single GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively
licensed, open-source checkpoints designed to help practitioners facilitate
compliance with emerging data protection and AI regulations, such as the EU AI
Act.

</details>


### [33] [Authorship Attribution in Multilingual Machine-Generated Texts](https://arxiv.org/abs/2508.01656)
*Lucio La Cava,Dominik Macko,Róbert Móro,Ivan Srba,Andrea Tagarelli*

Main category: cs.CL

TL;DR: 我们介绍了多语作者身份归属问题，该问题涉及将文本归因于不同语言的人类或多个 LLM 生成器。


<details>
  <summary>Details</summary>
Motivation: 区分机器生成的文本 (MGT) 与人类撰写的内容变得越来越困难。虽然早期在 MGT 检测方面的努力都集中在二元分类上，但 LLM 不断增长的格局和多样性需要更精细但具有挑战性的作者身份归属 (AA)，即能够识别文本背后的精确生成器（LLM 或人类）。

Method: 研究了单语AA方法的多语适用性、跨语迁移性以及生成器对归因性能的影响

Result: 某些单语AA方法可以适应多语环境，但仍存在很大的局限性和挑战，特别是在跨不同语系进行迁移时

Conclusion: 某些单语AA方法可以适应多语环境，但仍存在很大的局限性和挑战，特别是在跨不同语系进行迁移时，这突显了多语AA的复杂性，并且需要更强大的方法来更好地匹配真实世界的场景。

Abstract: As Large Language Models (LLMs) have reached human-like fluency and
coherence, distinguishing machine-generated text (MGT) from human-written
content becomes increasingly difficult. While early efforts in MGT detection
have focused on binary classification, the growing landscape and diversity of
LLMs require a more fine-grained yet challenging authorship attribution (AA),
i.e., being able to identify the precise generator (LLM or human) behind a
text. However, AA remains nowadays confined to a monolingual setting, with
English being the most investigated one, overlooking the multilingual nature
and usage of modern LLMs. In this work, we introduce the problem of
Multilingual Authorship Attribution, which involves attributing texts to human
or multiple LLM generators across diverse languages. Focusing on 18 languages
-- covering multiple families and writing scripts -- and 8 generators (7 LLMs
and the human-authored class), we investigate the multilingual suitability of
monolingual AA methods, their cross-lingual transferability, and the impact of
generators on attribution performance. Our results reveal that while certain
monolingual AA methods can be adapted to multilingual settings, significant
limitations and challenges remain, particularly in transferring across diverse
language families, underscoring the complexity of multilingual AA and the need
for more robust approaches to better match real-world scenarios.

</details>


### [34] [CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions](https://arxiv.org/abs/2508.01674)
*Tae Soo Kim,Yoonjoo Lee,Yoonah Park,Jiho Kim,Young-Ho Kim,Juho Kim*

Main category: cs.CL

TL;DR: CUPID基准测试表明，当前的大语言模型在理解和应用上下文相关的用户偏好方面存在困难，这表明需要进一步发展以实现更个性化的互动。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 的个性化通常假设用户持有静态偏好，这些偏好在全球所有任务中都有体现。 实际上，人类持有动态偏好，这些偏好会根据上下文而变化。 用户在各种上下文中与法学硕士互动时，会自然地表达他们的情境偏好，模型必须推断这些偏好并将其应用到未来的情境中，以确保一致性。

Method: 提出了一个名为CUPID的基准，其中包含用户和基于LLM的聊天助手之间756个人工策划的交互会话历史。

Result: 对 10 个开放和专有的法学硕士进行了评估，结果显示最先进的法学硕士难以从多轮交互中推断偏好，并且无法辨别哪些先前的上下文与新的请求相关——准确率低于 50%，召回率低于 65%。

Conclusion: 当前的大语言模型难以从多轮交互中推断偏好，并且无法辨别哪些先前的上下文与新的请求相关。需要提高大语言模型在上下文个性化交互方面的能力。

Abstract: Personalization of Large Language Models (LLMs) often assumes users hold
static preferences that reflect globally in all tasks. In reality, humans hold
dynamic preferences that change depending on the context. As users interact
with an LLM in various contexts, they naturally reveal their contextual
preferences, which a model must infer and apply in future contexts to ensure
alignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated
interaction session histories between users and LLM-based chat assistants. In
each interaction session, the user provides a request in a specific context and
expresses their preference through multi-turn feedback. Given a new user
request and prior interaction sessions, our benchmark assesses whether LLMs can
infer the preference relevant to this request and generate a response that
satisfies this preference. With CUPID, we evaluated 10 open and proprietary
LLMs, revealing that state-of-the-art LLMs struggle to infer preferences from
multi-turn interactions and fail to discern what previous context is relevant
to a new request -- under 50% precision and 65% recall. Our work highlights the
need to advance LLM capabilities for more contextually personalized
interactions and proposes CUPID as a resource to drive these improvements.

</details>


### [35] [The Bidirectional Process Reward Model](https://arxiv.org/abs/2508.01682)
*Lingyin Zhang,Jun Gao,Xiaoxue Ren,Ziqiang Cao*

Main category: cs.CL

TL;DR: BiPRM 是一种新颖的双向评估范式，它通过结合从右到左的评估流来增强大型语言模型 （LLM） 的推理质量，从而能够根据后面的步骤实时评估早期的步骤，而无需任何额外的参数或推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的 PRM 主要采用单向从左到右 （L2R） 评估范式，这限制了它们利用全局上下文的能力，从而难以根据后面的步骤验证早期步骤的一致性。

Method: 我们提出了一种新颖的双向评估范式，名为双向过程奖励模型 （BiPRM）。BiPRM 无缝地结合了与传统 L2R 流平行的从右到左 （R2L） 评估流，使后面的推理步骤能够帮助实时评估较早的推理步骤。值得注意的是，内置的 R2L 评估仅通过提示修改来实现，这些修改反转了原始推理轨迹，而无需引入任何额外的参数或推理延迟。这确保了 BiPRM 既高效又与现有的 PRM 研究广泛兼容。

Result: 在两个数学推理基准上使用由三个不同的策略模型生成的样本进行了广泛的实验。我们的方法 BiPRM 在三个骨干网和三个不同的 PRM 目标中进行了评估。在所有设置中，BiPRM 始终优于单向基线，在逐步奖励评估中实现了高达 31.9% 的改进。

Conclusion: BiPRM在所有设置中始终优于单向基线，在逐步奖励评估中实现了高达 31.9% 的改进。总的来说，我们的结果突出了 BiPRM 的有效性、稳健性和普遍适用性，为基于过程的奖励建模提供了一个有希望的新方向。

Abstract: Process Reward Models (PRMs) have emerged as a promising approach to enhance
the reasoning quality of Large Language Models (LLMs) by assigning fine-grained
scores to intermediate reasoning steps within a solution trajectory. However,
existing PRMs predominantly adopt a unidirectional left-to-right (L2R)
evaluation paradigm, which limits their ability to leverage global context,
making it challenging to verify the consistency of earlier steps based on later
ones. In light of these challenges, we propose a novel bidirectional evaluation
paradigm, named Bidirectional Process Reward Model (BiPRM). BiPRM seamlessly
incorporates a parallel right-to-left (R2L) evaluation stream alongside the
conventional L2R flow, enabling later reasoning steps to help assess earlier
ones in real time. Notably, the built-in R2L evaluation is implemented solely
through prompt modifications that reverse the original reasoning trajectory,
without any additional parameters or inference latency introduced. This ensures
BiPRM remains both efficient and broadly compatible with existing PRM studies.
We conduct extensive experiments on two mathematical reasoning benchmarks using
samples generated by three different policy models. Our method, BiPRM, is
evaluated across three backbones and three distinct PRM objectives. Across all
settings, BiPRM consistently outperforms unidirectional baselines, achieving up
to a 31.9% improvement in stepwise reward evaluation. Generally, our results
highlight BiPRM's effectiveness, robustness, and general applicability,
offering a promising new direction for process-based reward modeling.

</details>


### [36] [Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy](https://arxiv.org/abs/2508.01696)
*Yi Jiang,Sendong Zhao,Jianbo Li,Haochun Wang,Lizhe Zhang,Yan Liu,Bin Qin*

Main category: cs.CL

TL;DR: 本文提出了一种新的 RAG 框架，通过协同链式代理来增强参数和检索知识之间的协同作用，并在开放域和多跳问答任务中取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成 (RAG) 已成为一种有前途的框架，用于增强大型语言模型 (LLM) 的能力，尤其是在知识密集型任务中。尽管它具有优势，但目前的 RAG 方法通常难以在生成过程中充分利用知识。特别是，模型内部参数知识和外部检索知识之间的协同作用仍然有限。检索到的内容有时可能会误导生成，而某些生成的内容可以引导模型产生更准确的输出。

Method: 我们提出了协同链式代理，这是一个旨在增强参数和检索知识之间显式协同的框架。具体来说，我们首先介绍了 CoCoA-zero，这是一个多代理 RAG 框架，它首先执行条件知识归纳，然后推断答案。在此基础上，我们开发了 CoCoA，这是一种长链训练策略，它综合了来自 CoCoA-zero 的扩展多代理推理轨迹，以微调 LLM。这种策略增强了模型显式整合和联合利用参数和检索知识的能力。

Result: 实验结果表明，CoCoA-zero 和 CoCoA 在开放域和多跳问答任务中表现出色。

Conclusion: CoCoA-zero 和 CoCoA 在开放域和多跳问答任务中表现出色。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising framework for
enhancing the capabilities of Large Language Models (LLMs), especially in
knowledge-intensive tasks. Despite its advantages, current RAG methods often
struggle to *fully exploit knowledge during generation*. In particular, the
synergy between the model's internal parametric knowledge and external
retrieved knowledge remains limited. Retrieved contents may sometimes mislead
generation, while certain generated content can guide the model toward more
accurate outputs. In this work, we propose Collaborative Chain-of-Agents, a
framework designed to enhance explicitly synergy over both parametric and
retrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent
RAG framework that first performs conditional knowledge induction and then
reasons answers. Building on this, we develop CoCoA, a long-chain training
strategy that synthesizes extended multi-agent reasoning trajectories from
CoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability
to explicitly integrate and jointly leverage parametric and retrieved
knowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior
performance on open-domain and multi-hop QA tasks.

</details>


### [37] [Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption](https://arxiv.org/abs/2508.01708)
*Berkay Köprü,Mehrzad Mashal,Yigit Gurses,Akos Kadar,Maximilian Schmitt,Ditty Mathew,Felix Burkhardt,Florian Eyben,Björn W. Schuller*

Main category: cs.CL

TL;DR: 大型语言模型容易产生与输入上下文无关的情感表达。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易整合不相关的信息，从而引入不相关的情感表达。

Method: 收集基准数据集，并设计自动生成数据集的方案。提出一个与人类判断相关的自动评估管道。

Result: 实验表明，在同一LLM家族中，随着模型在参数空间中扩展，情感泄露会减少。情感泄露的缓解需要在模型构建过程中特别注意，并且不能通过提示来缓解。负面情绪注入提示会比正面情绪更多地扰乱生成过程，导致更高的情感泄露率。

Conclusion: 模型规模扩大可以减少情感泄露，但需要特定的模型构建过程来缓解，提示无法缓解。负面情绪比正面情绪更容易扰乱生成过程，导致更高的情感泄露率。

Abstract: Large language models (LLMs) have advanced natural language processing (NLP)
skills such as through next-token prediction and self-attention, but their
ability to integrate broad context also makes them prone to incorporating
irrelevant information. Prior work has focused on semantic leakage, bias
introduced by semantically irrelevant context. In this paper, we introduce
expression leakage, a novel phenomenon where LLMs systematically generate
sentimentally charged expressions that are semantically unrelated to the input
context. To analyse the expression leakage, we collect a benchmark dataset
along with a scheme to automatically generate a dataset from free-form text
from common-crawl. In addition, we propose an automatic evaluation pipeline
that correlates well with human judgment, which accelerates the benchmarking by
decoupling from the need of annotation for each analysed model. Our experiments
show that, as the model scales in the parameter space, the expression leakage
reduces within the same LLM family. On the other hand, we demonstrate that
expression leakage mitigation requires specific care during the model building
process, and cannot be mitigated by prompting. In addition, our experiments
indicate that, when negative sentiment is injected in the prompt, it disrupts
the generation process more than the positive sentiment, causing a higher
expression leakage rate.

</details>


### [38] [CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications](https://arxiv.org/abs/2508.01710)
*Raviraj Joshi,Rakesh Paul,Kanishk Singla,Anusha Kamath,Michael Evans,Katherine Luna,Shaona Ghosh,Utkarsh Vaidya,Eileen Long,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: Addresses the safety gap in multilingual LLMs by creating a culturally aware safety guard model using a novel data generation and filtering pipeline, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The increasing use of Large Language Models (LLMs) in agentic applications highlights the need for robust safety guard models. While content safety in English is well-studied, non-English languages lack similar advancements due to the high cost of collecting culturally aligned labeled datasets.

Method: introducing a four-stage synthetic data generation and filtering pipeline: cultural data segregation, cultural data adaptation, machine translation, and quality filtering to convert and expand the Nemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct languages

Result: The resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1, comprises 386,661 samples in 9 languages and facilitates the training of Llama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning. The final model achieves state-of-the-art performance on several multilingual content safety benchmarks. the latest open LLMs are more prone to give unsafe responses when prompted in non-English languages.

Conclusion: This work represents a significant step toward closing the safety gap in multilingual LLMs by enabling the development of culturally aware safety guard models.

Abstract: The increasing use of Large Language Models (LLMs) in agentic applications
highlights the need for robust safety guard models. While content safety in
English is well-studied, non-English languages lack similar advancements due to
the high cost of collecting culturally aligned labeled datasets. We present
CultureGuard, a novel solution for curating culturally aligned, high-quality
safety datasets across multiple languages. Our approach introduces a four-stage
synthetic data generation and filtering pipeline: cultural data segregation,
cultural data adaptation, machine translation, and quality filtering. This
pipeline enables the conversion and expansion of the
Nemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct
languages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese.
The resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1,
comprises 386,661 samples in 9 languages and facilitates the training of
Llama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning.
The final model achieves state-of-the-art performance on several multilingual
content safety benchmarks. We also benchmark the latest open LLMs on
multilingual safety and observe that these LLMs are more prone to give unsafe
responses when prompted in non-English languages. This work represents a
significant step toward closing the safety gap in multilingual LLMs by enabling
the development of culturally aware safety guard models.

</details>


### [39] [Enhancing the Preference Extractor in Multi-turn Dialogues: From Annotating Disasters to Accurate Preference Extraction](https://arxiv.org/abs/2508.01739)
*Cheng Wang,ziru Liu,Pengcheng Tang,Mingyu Zhang,Quanyu Dai,Yue Zhu*

Main category: cs.CL

TL;DR: Proposes IterChat, a dialogue data generation framework using GPT4 and a new data format to improve the performance and annotator efficiency for user preference extraction in dialogue systems.


<details>
  <summary>Details</summary>
Motivation: The primary challenge stems from the inherent difficulty in obtaining high-quality labeled multi-turn dialogue data. Accurately tracking user preference transitions across turns not only demands intensive domain expertise and contextual consistency maintenance for annotators but also complicates model training due to error propagation in sequential dependency learning.

Method: We propose a novel dialogue data generation framework named IterChat. First, we construct a new data format that categorizes the dialogue data into attributed historical preferences and one-turn dialogues. Then, to generate a high-quality and diverse dialogue dataset, we adopt GPT4 to pre-define the preference slots in the target preference extractor task and then randomly sample the subset of the slots and their corresponding schema values to create the dialogue datasets.

Result: Fine-tuning or few-shot prompting with the new dialogue format yields superior performance compared to the original multi-turn dialogues. The new data format improves annotator efficiency with a win rate of 28.4% higher than the original multi-turn dialogues.

Conclusion: Fine-tuning or few-shot prompting with the new dialogue format yields superior performance compared to the original multi-turn dialogues. Additionally, the new data format improves annotator efficiency with a win rate of 28.4% higher than the original multi-turn dialogues.

Abstract: Identifying user preferences in dialogue systems is a pivotal aspect of
providing satisfying services. Current research shows that using large language
models (LLMs) to fine-tune a task-specific preference extractor yields
excellent results in terms of accuracy and generalization. However, the primary
challenge stems from the inherent difficulty in obtaining high-quality labeled
multi-turn dialogue data. Accurately tracking user preference transitions
across turns not only demands intensive domain expertise and contextual
consistency maintenance for annotators (termed \textbf{``Annotating
Disaster''}) but also complicates model training due to error propagation in
sequential dependency learning. Inspired by the observation that multi-turn
preference extraction can be decomposed into iterative executions of one-turn
extraction processes. We propose a novel dialogue data generation framework
named \textbf{IterChat}. First, we construct a new data format that categorizes
the dialogue data into attributed historical preferences and one-turn
dialogues. This reduces the probability of annotation errors and improves
annotation efficiency. Then, to generate a high-quality and diverse dialogue
dataset, we adopt GPT4 to pre-define the preference slots in the target
preference extractor task and then randomly sample the subset of the slots and
their corresponding schema values to create the dialogue datasets. Experimental
results indicate that fine-tuning or only few-shot prompting with the new
dialogue format yields superior performance compared to the original multi-turn
dialogues. Additionally, the new data format improves annotator efficiency with
a win rate of 28.4\% higher than the original multi-turn dialogues.

</details>


### [40] [AI-Generated Text is Non-Stationary: Detection via Temporal Tomography](https://arxiv.org/abs/2508.01754)
*Alva West,Yixuan Weng,Minjun Zhu,Luodan Zhang,Zhen Lin,Guangsheng Bao,Yue Zhang*

Main category: cs.CL

TL;DR: This paper introduces Temporal Discrepancy Tomography (TDT), a new method for detecting AI-generated text that considers the location and scale of statistical anomalies. TDT outperforms existing methods, especially against adversarial attacks, with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: current approaches share a fundamental limitation: they aggregate token-level measurements into scalar scores, discarding positional information about where anomalies occur. AI-generated text exhibits significant non-stationarity, statistical properties vary by 73.8% more between text segments compared to human writing. This discovery explains why existing detectors fail against localized adversarial perturbations that exploit this overlooked characteristic.

Method: Temporal Discrepancy Tomography (TDT), a novel detection paradigm that preserves positional information by reformulating detection as a signal processing task. TDT treats token-level discrepancies as a time-series signal and applies Continuous Wavelet Transform to generate a two-dimensional time-scale representation, capturing both the location and linguistic scale of statistical anomalies.

Result: TDT achieves 0.855 AUROC (7.1% improvement over the best baseline). TDT demonstrates robust performance on adversarial tasks, with 14.1% AUROC improvement on HART Level 2 paraphrasing attacks. TDT maintains practical efficiency with only 13% computational overhead.

Conclusion: non-stationarity is a fundamental characteristic of AI-generated text and demonstrates that preserving temporal dynamics is essential for robust detection

Abstract: The field of AI-generated text detection has evolved from supervised
classification to zero-shot statistical analysis. However, current approaches
share a fundamental limitation: they aggregate token-level measurements into
scalar scores, discarding positional information about where anomalies occur.
Our empirical analysis reveals that AI-generated text exhibits significant
non-stationarity, statistical properties vary by 73.8\% more between text
segments compared to human writing. This discovery explains why existing
detectors fail against localized adversarial perturbations that exploit this
overlooked characteristic. We introduce Temporal Discrepancy Tomography (TDT),
a novel detection paradigm that preserves positional information by
reformulating detection as a signal processing task. TDT treats token-level
discrepancies as a time-series signal and applies Continuous Wavelet Transform
to generate a two-dimensional time-scale representation, capturing both the
location and linguistic scale of statistical anomalies. On the RAID benchmark,
TDT achieves 0.855 AUROC (7.1\% improvement over the best baseline). More
importantly, TDT demonstrates robust performance on adversarial tasks, with
14.1\% AUROC improvement on HART Level 2 paraphrasing attacks. Despite its
sophisticated analysis, TDT maintains practical efficiency with only 13\%
computational overhead. Our work establishes non-stationarity as a fundamental
characteristic of AI-generated text and demonstrates that preserving temporal
dynamics is essential for robust detection.

</details>


### [41] [A comprehensive taxonomy of hallucinations in Large Language Models](https://arxiv.org/abs/2508.01781)
*Manuel Cossio*

Main category: cs.CL

TL;DR: 本报告全面分析了LLM幻觉，强调了其复杂性和不可避免性，并提出了缓解策略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）已经彻底改变了自然语言处理，但它们产生幻觉的倾向仍然是一个关键挑战。

Method: 对LLM幻觉进行了全面的分类，包括正式定义、理论框架以及对内在和外在幻觉的区分。

Result: 详细描述了LLM幻觉的具体表现，分析了其根本原因，并考察了影响幻觉感知的认知和人为因素。

Conclusion: LLM幻觉是不可避免的，未来的工作必须侧重于强大的检测、缓解和持续的人工监督，以便在关键应用中负责任和可靠地部署。

Abstract: Large language models (LLMs) have revolutionized natural language processing,
yet their propensity for hallucination, generating plausible but factually
incorrect or fabricated content, remains a critical challenge. This report
provides a comprehensive taxonomy of LLM hallucinations, beginning with a
formal definition and a theoretical framework that posits its inherent
inevitability in computable LLMs, irrespective of architecture or training. It
explores core distinctions, differentiating between intrinsic (contradicting
input context) and extrinsic (inconsistent with training data or reality), as
well as factuality (absolute correctness) and faithfulness (adherence to
input). The report then details specific manifestations, including factual
errors, contextual and logical inconsistencies, temporal disorientation,
ethical violations, and task-specific hallucinations across domains like code
generation and multimodal applications. It analyzes the underlying causes,
categorizing them into data-related issues, model-related factors, and
prompt-related influences. Furthermore, the report examines cognitive and human
factors influencing hallucination perception, surveys evaluation benchmarks and
metrics for detection, and outlines architectural and systemic mitigation
strategies. Finally, it introduces web-based resources for monitoring LLM
releases and performance. This report underscores the complex, multifaceted
nature of LLM hallucinations and emphasizes that, given their theoretical
inevitability, future efforts must focus on robust detection, mitigation, and
continuous human oversight for responsible and reliable deployment in critical
applications.

</details>


### [42] [HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark](https://arxiv.org/abs/2508.01812)
*Amir DN Cohen,Hilla Merhav,Yoav Goldberg,Reut Tsarfaty*

Main category: cs.CL

TL;DR: This paper introduces HeQ, a Hebrew Machine Reading Comprehension dataset, and proposes revised evaluation metrics suitable for morphologically rich languages (MRLs).


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for Hebrew Natural Language Processing (NLP) focus mainly on morpho-syntactic tasks, neglecting the semantic dimension of language understanding.

Method: We devise a novel set of guidelines, a controlled crowdsourcing protocol, and revised evaluation metrics that are suitable for the morphologically rich nature of the language.

Result: Our resulting benchmark, HeQ (Hebrew QA), features 30,147 diverse question-answer pairs derived from both Hebrew Wikipedia articles and Israeli tech news. Our empirical investigation reveals that standard evaluation metrics such as F1 scores and Exact Match (EM) are not appropriate for Hebrew (and other MRLs), and we propose a relevant enhancement.  In addition, our experiments show low correlation between models' performance on morpho-syntactic tasks and on MRC, which suggests that models designed for the former might underperform on semantics-heavy tasks.

Conclusion: The development and exploration of HeQ illustrate some of the challenges MRLs pose in natural language understanding (NLU), fostering progression towards more and better NLU models for Hebrew and other MRLs.

Abstract: Current benchmarks for Hebrew Natural Language Processing (NLP) focus mainly
on morpho-syntactic tasks, neglecting the semantic dimension of language
understanding. To bridge this gap, we set out to deliver a Hebrew Machine
Reading Comprehension (MRC) dataset, where MRC is to be realized as extractive
Question Answering. The morphologically rich nature of Hebrew poses a challenge
to this endeavor: the indeterminacy and non-transparency of span boundaries in
morphologically complex forms lead to annotation inconsistencies,
disagreements, and flaws in standard evaluation metrics.
  To remedy this, we devise a novel set of guidelines, a controlled
crowdsourcing protocol, and revised evaluation metrics that are suitable for
the morphologically rich nature of the language. Our resulting benchmark, HeQ
(Hebrew QA), features 30,147 diverse question-answer pairs derived from both
Hebrew Wikipedia articles and Israeli tech news. Our empirical investigation
reveals that standard evaluation metrics such as F1 scores and Exact Match (EM)
are not appropriate for Hebrew (and other MRLs), and we propose a relevant
enhancement.
  In addition, our experiments show low correlation between models' performance
on morpho-syntactic tasks and on MRC, which suggests that models designed for
the former might underperform on semantics-heavy tasks. The development and
exploration of HeQ illustrate some of the challenges MRLs pose in natural
language understanding (NLU), fostering progression towards more and better NLU
models for Hebrew and other MRLs.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [43] [Team PA-VCG's Solution for Competition on Understanding Chinese College Entrance Exam Papers in ICDAR'25](https://arxiv.org/abs/2508.00834)
*Wei Wu,Wenjie Wang,Yang Tan,Ying Liu,Liang Diao,Lin Huang,Kaihe Xu,Wenfeng Xie,Ziling Lin*

Main category: cs.CV

TL;DR: Team PA-VGG's solution for the ICDAR'25 Competition on Understanding Chinese College Entrance Exam Papers leverages high-resolution image processing, a multi-image end-to-end input strategy, and domain-specific post-training strategies to achieve first place with an accuracy rate of 89.6%.


<details>
  <summary>Details</summary>
Motivation: address the challenges of dense OCR extraction and complex document layouts in Gaokao papers

Method: high-resolution image processing and a multi-image end-to-end input strategy, domain-specific post-training strategies

Result: achieves the most outstanding performance, securing first place with an accuracy rate of 89.6%.

Conclusion: The post-training approach achieves the most outstanding performance, securing first place with an accuracy rate of 89.6%.

Abstract: This report presents Team PA-VGG's solution for the ICDAR'25 Competition on
Understanding Chinese College Entrance Exam Papers. In addition to leveraging
high-resolution image processing and a multi-image end-to-end input strategy to
address the challenges of dense OCR extraction and complex document layouts in
Gaokao papers, our approach introduces domain-specific post-training
strategies. Experimental results demonstrate that our post-training approach
achieves the most outstanding performance, securing first place with an
accuracy rate of 89.6%.

</details>


### [44] [Inclusive Review on Advances in Masked Human Face Recognition Technologies](https://arxiv.org/abs/2508.00841)
*Ali Haitham Abdul Amir,Zainab N. Nemer*

Main category: cs.CV

TL;DR: 本文全面回顾了口罩人脸识别领域的最新进展，重点介绍了深度学习技术，讨论了最突出的挑战，并回顾了各种应用。


<details>
  <summary>Details</summary>
Motivation: 由于 COVID-19 大流行，口罩的广泛使用导致面部基本特征的部分隐藏，这对人脸识别系统提出了新的挑战。

Method: 深度学习技术，特别是卷积神经网络 (CNN) 和孪生网络 (Siamese networks)。

Result: 回顾了克服这些挑战的先进技术，包括使用人工数据库的数据增强和多媒体方法，以提高系统泛化的能力。此外，本文重点介绍了深度网络设计、特征提取技术、评估标准以及该领域中使用的数据集的进展。

Conclusion: 未来研究趋势包括开发更高效的算法和集成多媒体技术，以提高识别系统在现实环境中的性能并扩展其应用。

Abstract: Masked Face Recognition (MFR) is an increasingly important area in biometric
recognition technologies, especially with the widespread use of masks as a
result of the COVID-19 pandemic. This development has created new challenges
for facial recognition systems due to the partial concealment of basic facial
features. This paper aims to provide a comprehensive review of the latest
developments in the field, with a focus on deep learning techniques, especially
convolutional neural networks (CNNs) and twin networks (Siamese networks),
which have played a pivotal role in improving the accuracy of covering face
recognition. The paper discusses the most prominent challenges, which include
changes in lighting, different facial positions, partial concealment, and the
impact of mask types on the performance of systems. It also reviews advanced
technologies developed to overcome these challenges, including data enhancement
using artificial databases and multimedia methods to improve the ability of
systems to generalize. In addition, the paper highlights advance in deep
network design, feature extraction techniques, evaluation criteria, and data
sets used in this area. Moreover, it reviews the various applications of masked
face recognition in the fields of security and medicine, highlighting the
growing importance of these systems in light of recurrent health crises and
increasing security threats. Finally, the paper focuses on future research
trends such as developing more efficient algorithms and integrating multimedia
technologies to improve the performance of recognition systems in real-world
environments and expand their applications.

</details>


### [45] [HoneyImage: Verifiable, Harmless, and Stealthy Dataset Ownership Verification for Image Models](https://arxiv.org/abs/2508.00892)
*Zhihao Zhu,Jiale Han,Yi Yang*

Main category: cs.CV

TL;DR: HoneyImage is proposed to verify the ownership of image datasets by embedding imperceptible traces. It achieves strong verification accuracy with minimal impact on downstream performance while maintaining imperceptibility.


<details>
  <summary>Details</summary>
Motivation: Data owners therefore need reliable mechanisms to verify whether their proprietary data has been misused to train third-party models. Existing solutions, such as backdoor watermarking and membership inference, face inherent trade-offs between verification effectiveness and preservation of data integrity.

Method: HoneyImage selectively modifies a small number of hard samples to embed imperceptible yet verifiable traces, enabling reliable ownership verification while maintaining dataset integrity.

Result: Extensive experiments across four benchmark datasets and multiple model architectures show that HoneyImage consistently achieves strong verification accuracy with minimal impact on downstream performance while maintaining imperceptible.

Conclusion: HoneyImage consistently achieves strong verification accuracy with minimal impact on downstream performance while maintaining imperceptible. The proposed HoneyImage method could provide data owners with a practical mechanism to protect ownership over valuable image datasets, encouraging safe sharing and unlocking the full transformative potential of data-driven AI.

Abstract: Image-based AI models are increasingly deployed across a wide range of
domains, including healthcare, security, and consumer applications. However,
many image datasets carry sensitive or proprietary content, raising critical
concerns about unauthorized data usage. Data owners therefore need reliable
mechanisms to verify whether their proprietary data has been misused to train
third-party models. Existing solutions, such as backdoor watermarking and
membership inference, face inherent trade-offs between verification
effectiveness and preservation of data integrity. In this work, we propose
HoneyImage, a novel method for dataset ownership verification in image
recognition models. HoneyImage selectively modifies a small number of hard
samples to embed imperceptible yet verifiable traces, enabling reliable
ownership verification while maintaining dataset integrity. Extensive
experiments across four benchmark datasets and multiple model architectures
show that HoneyImage consistently achieves strong verification accuracy with
minimal impact on downstream performance while maintaining imperceptible. The
proposed HoneyImage method could provide data owners with a practical mechanism
to protect ownership over valuable image datasets, encouraging safe sharing and
unlocking the full transformative potential of data-driven AI.

</details>


### [46] [Phase-fraction guided denoising diffusion model for augmenting multiphase steel microstructure segmentation via micrograph image-mask pair synthesis](https://arxiv.org/abs/2508.00896)
*Hoang Hai Nam Nguyen,Minh Tien Tran,Hoheok Kim,Ho Won Lee*

Main category: cs.CV

TL;DR: PF-DiffSeg是一种用于金相微观结构分割的数据增强方法，通过合成图像和掩模来提高分割精度，尤其是在数据量不足的情况下。


<details>
  <summary>Details</summary>
Motivation: 金相微观结构分割中机器学习的有效性通常受到缺乏人工注释的相掩模的限制，特别是对于金属合金中稀有或成分复杂的形态。

Method: 提出了一种相分数控制的单阶段去噪扩散框架PF-DiffSeg，该框架联合合成微观结构图像及其相应的分割掩模。

Result: 在增材制造的多相钢MetalDAM基准上进行评估，与标准增强策略相比，我们的合成增强方法在分割精度方面产生了显着提高，尤其是在少数类中，并且进一步优于两阶段掩模引导扩散和生成对抗网络（GAN）基线。

Conclusion: PF-DiffSeg方法在提高分割精度方面表现出色，尤其是在少数类中，并且与传统方法相比，减少了推理时间，为金相应用中的数据增强提供了一个可扩展的解决方案。

Abstract: The effectiveness of machine learning in metallographic microstructure
segmentation is often constrained by the lack of human-annotated phase masks,
particularly for rare or compositionally complex morphologies within the metal
alloy. We introduce PF-DiffSeg, a phase-fraction controlled, one-stage
denoising diffusion framework that jointly synthesizes microstructure images
and their corresponding segmentation masks in a single generative trajectory to
further improve segmentation accuracy. By conditioning on global phase-fraction
vectors, augmented to represent real data distribution and emphasize minority
classes, our model generates compositionally valid and structurally coherent
microstructure image and mask samples that improve both data diversity and
training efficiency. Evaluated on the MetalDAM benchmark for additively
manufactured multiphase steel, our synthetic augmentation method yields notable
improvements in segmentation accuracy compared to standard augmentation
strategies especially in minority classes and further outperforms a two-stage
mask-guided diffusion and generative adversarial network (GAN) baselines, while
also reducing inference time compared to conventional approach. The method
integrates generation and conditioning into a unified framework, offering a
scalable solution for data augmentation in metallographic applications.

</details>


### [47] [Benefits of Feature Extraction and Temporal Sequence Analysis for Video Frame Prediction: An Evaluation of Hybrid Deep Learning Models](https://arxiv.org/abs/2508.00898)
*Jose M. Sánchez Velázquez,Mingbo Cai,Andrew Coney,Álvaro J. García- Tejedor,Alberto Nogales*

Main category: cs.CV

TL;DR: 本文评估了几种混合深度学习方法，这些方法结合了自编码器的特征提取功能与使用循环神经网络 (RNN)、3D 卷积神经网络 (3D CNN) 和相关架构的时间序列建模。


<details>
  <summary>Details</summary>
Motivation: 近年来，人工智能的进步对计算机科学产生了重大影响，尤其是在计算机视觉领域，从而能够解决诸如视频帧预测之类的复杂问题。视频帧预测在天气预报或自动驾驶系统中具有关键应用，并且可以提供诸如视频压缩和流传输之类的技术改进。在人工智能方法中，深度学习已成为解决与视觉相关的任务的非常有效的方法，尽管当前的帧预测模型仍有改进的空间。

Method: 结合了自编码器的特征提取能力与使用循环神经网络 (RNN)、3D 卷积神经网络 (3D CNN) 和相关架构的时间序列建模的混合深度学习方法。

Result: 结果表明，该方法效果良好，SSIM 指标从 0.69 增加到 0.82。

Conclusion: 混合模型表现良好，利用 3DCNN 和 ConvLSTM 的混合模型最有效，并且包含真实数据的灰度视频最容易预测。

Abstract: In recent years, advances in Artificial Intelligence have significantly
impacted computer science, particularly in the field of computer vision,
enabling solutions to complex problems such as video frame prediction. Video
frame prediction has critical applications in weather forecasting or autonomous
systems and can provide technical improvements, such as video compression and
streaming. Among Artificial Intelligence methods, Deep Learning has emerged as
highly effective for solving vision-related tasks, although current frame
prediction models still have room for enhancement. This paper evaluates several
hybrid deep learning approaches that combine the feature extraction
capabilities of autoencoders with temporal sequence modelling using Recurrent
Neural Networks (RNNs), 3D Convolutional Neural Networks (3D CNNs), and related
architectures. The proposed solutions were rigorously evaluated on three
datasets that differ in terms of synthetic versus real-world scenarios and
grayscale versus color imagery. Results demonstrate that the approaches perform
well, with SSIM metrics increasing from 0.69 to 0.82, indicating that hybrid
models utilizing 3DCNNs and ConvLSTMs are the most effective, and greyscale
videos with real data are the easiest to predict.

</details>


### [48] [TESPEC: Temporally-Enhanced Self-Supervised Pretraining for Event Cameras](https://arxiv.org/abs/2508.00913)
*Mohammad Mohammadi,Ziyi Wu,Igor Gilitschenski*

Main category: cs.CV

TL;DR: TESPEC是一种用于学习时空信息的自监督预训练框架，通过利用长事件序列和伪灰度视频重建目标，在下游任务中实现了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于事件的自监督学习（SSL）方法很大程度上模仿了基于RGB图像的方法，在短时间内对原始事件进行前馈模型的预训练，忽略了事件的时间信息。

Method: 该论文提出TESPEC，一种自监督预训练框架，通过设计一种新颖的方法将事件累积成包含底层场景高级语义信息的伪灰度视频，该方法对传感器噪声具有鲁棒性并减少了运动模糊。然后，使用掩码图像建模范例和一个新的重建目标进行预训练。

Result: TESPEC是第一个在预训练期间利用长事件序列的框架，并且在下游任务中实现了最先进的结果。

Conclusion: TESPEC在下游任务（包括对象检测、语义分割和单目深度估计）中表现出最好的性能。

Abstract: Long-term temporal information is crucial for event-based perception tasks,
as raw events only encode pixel brightness changes. Recent works show that when
trained from scratch, recurrent models achieve better results than feedforward
models in these tasks. However, when leveraging self-supervised pre-trained
weights, feedforward models can outperform their recurrent counterparts.
Current self-supervised learning (SSL) methods for event-based pre-training
largely mimic RGB image-based approaches. They pre-train feedforward models on
raw events within a short time interval, ignoring the temporal information of
events. In this work, we introduce TESPEC, a self-supervised pre-training
framework tailored for learning spatio-temporal information. TESPEC is
well-suited for recurrent models, as it is the first framework to leverage long
event sequences during pre-training. TESPEC employs the masked image modeling
paradigm with a new reconstruction target. We design a novel method to
accumulate events into pseudo grayscale videos containing high-level semantic
information about the underlying scene, which is robust to sensor noise and
reduces motion blur. Reconstructing this target thus requires the model to
reason about long-term history of events. Extensive experiments demonstrate our
state-of-the-art results in downstream tasks, including object detection,
semantic segmentation, and monocular depth estimation. Project webpage:
https://mhdmohammadi.github.io/TESPEC_webpage.

</details>


### [49] [Latent Diffusion Based Face Enhancement under Degraded Conditions for Forensic Face Recognition](https://arxiv.org/abs/2508.00941)
*Hassan Ugail,Hamad Mansour Alawar,AbdulNasser Abbas Zehi,Ahmed Mohammad Alkendi,Ismail Lujain Jaleel*

Main category: cs.CV

TL;DR: 本研究评估了基于潜在扩散的增强技术，以提高法庭相关降级下的人脸识别能力，结果表明性能显着提高。


<details>
  <summary>Details</summary>
Motivation: 当处理低质量的法庭证据图像时，人脸识别系统会遇到严重的性能下降。

Method: 使用具有 Facezoom LoRA 适配的 Flux.1 Kontext Dev 管道，针对七种降级类别进行测试，包括压缩伪影、模糊效果和噪声污染。

Result: 该方法表现出显着改进，将整体识别准确率从 29.1% 提高到 84.5%（提高 55.4 个百分点，95% CI：[54.1, 56.7]）。

Conclusion: 该研究确立了基于扩散的增强技术在法庭人脸识别应用中的潜力。

Abstract: Face recognition systems experience severe performance degradation when
processing low-quality forensic evidence imagery. This paper presents an
evaluation of latent diffusion-based enhancement for improving face recognition
under forensically relevant degradations. Using a dataset of 3,000 individuals
from LFW with 24,000 recognition attempts, we implement the Flux.1 Kontext Dev
pipeline with Facezoom LoRA adaptation to test against seven degradation
categories, including compression artefacts, blur effects, and noise
contamination. Our approach demonstrates substantial improvements, increasing
overall recognition accuracy from 29.1% to 84.5% (55.4 percentage point
improvement, 95% CI: [54.1, 56.7]). Statistical analysis reveals significant
performance gains across all degradation types, with effect sizes exceeding
conventional thresholds for practical significance. These findings establish
the potential of sophisticated diffusion based enhancement in forensic face
recognition applications.

</details>


### [50] [Optimizing Vision-Language Consistency via Cross-Layer Regional Attention Alignment](https://arxiv.org/abs/2508.00945)
*Yifan Wang,Hongfeng Ai,Quangao Liu,Maowei Jiang,Ruiyuan Kang,Ruiqi Li,Jiahua Dong,Mengting Xiao,Cheng Jiang,Chenzhong Li*

Main category: cs.CV

TL;DR: 提出了一种名为 CCRA 的新方法，用于改进视觉语言模型中的注意力机制，并在多个基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLM）在有效协调用于跨模态嵌入学习的各种注意力机制方面面临挑战，导致注意力不匹配和性能欠佳。

Method: 提出了一致的跨层区域对齐（CCRA），它引入了层-补丁式交叉注意力（LPWCA）以通过联合加权补丁和层式嵌入来捕获细粒度的区域-语义相关性，以及渐进式注意力集成（PAI），其系统地按顺序协调 LPWCA、层式和补丁式注意力机制。

Result: CCRA-enhanced LLaVA-v1.5-7B 模型实现了最先进的性能，优于所有基线方法，同时通过更注重区域和语义对齐的注意力模式提供了增强的可解释性。

Conclusion: CCRA-enhanced LLaVA-v1.5-7B 模型在十个不同的视觉语言基准测试中实现了最先进的性能，优于所有基线方法，同时通过更注重区域和语义对齐的注意力模式提供了增强的可解释性。

Abstract: Vision Language Models (VLMs) face challenges in effectively coordinating
diverse attention mechanisms for cross-modal embedding learning, leading to
mismatched attention and suboptimal performance. We propose Consistent
Cross-layer Regional Alignment (CCRA), which introduces Layer-Patch-wise Cross
Attention (LPWCA) to capture fine-grained regional-semantic correlations by
jointly weighting patch and layer-wise embedding, and Progressive Attention
Integration (PAI) that systematically coordinates LPWCA, layer-wise, and
patch-wise attention mechanisms in sequence. This progressive design ensures
consistency from semantic to regional levels while preventing attention drift
and maximizing individual attention benefits. Experimental results on ten
diverse vision-language benchmarks demonstrate that our CCRA-enhanced
LLaVA-v1.5-7B model achieves state-of-the-art performance, outperforming all
baseline methods with only 3.55M additional parameters, while providing
enhanced interpretability through more regionally focused and semantically
aligned attention patterns.

</details>


### [51] [ThermoCycleNet: Stereo-based Thermogram Labeling for Model Transition to Cycling](https://arxiv.org/abs/2508.00974)
*Daniel Andrés López,Vincent Weber,Severin Zentgraf,Barlo Hillen,Perikles Simon,Elmar Schömer*

Main category: cs.CV

TL;DR: transition from treadmill to bicycle.


<details>
  <summary>Details</summary>
Motivation: Infrared thermography is emerging as a powerful tool in sports medicine, allowing assessment of thermal radiation during exercise and analysis of anatomical regions of interest, such as the well-exposed calves. Building on our previous advanced automatic annotation method

Method: transfer the stereo- and multimodal-based labeling approach from treadmill running to ergometer cycling. Therefore, the training of the semantic segmentation network with automatic labels and fine-tuning on high-quality manually annotated images has been examined and compared in different data set combinations.

Result: fine-tuning with a small fraction of manual data is sufficient to improve the overall performance of the deep neural network.

Conclusion: Combining automatically generated labels with small manually annotated data sets accelerates the adaptation of deep neural networks to new use cases, such as the transition from treadmill to bicycle.

Abstract: Infrared thermography is emerging as a powerful tool in sports medicine,
allowing assessment of thermal radiation during exercise and analysis of
anatomical regions of interest, such as the well-exposed calves. Building on
our previous advanced automatic annotation method, we aimed to transfer the
stereo- and multimodal-based labeling approach from treadmill running to
ergometer cycling. Therefore, the training of the semantic segmentation network
with automatic labels and fine-tuning on high-quality manually annotated images
has been examined and compared in different data set combinations. The results
indicate that fine-tuning with a small fraction of manual data is sufficient to
improve the overall performance of the deep neural network. Finally, combining
automatically generated labels with small manually annotated data sets
accelerates the adaptation of deep neural networks to new use cases, such as
the transition from treadmill to bicycle.

</details>


### [52] [ROVI: A VLM-LLM Re-Captioned Dataset for Open-Vocabulary Instance-Grounded Text-to-Image Generation](https://arxiv.org/abs/2508.01008)
*Cihang Peng,Qiming Hou,Zhong Ren,Kun Zhou*

Main category: cs.CV

TL;DR: ROVI: a high-quality synthetic dataset for instance-grounded text-to-image generation, created by labeling 1M curated web images using a re-captioning strategy. It outperforms existing datasets and improves text-to-image model performance.


<details>
  <summary>Details</summary>
Motivation: creating a high-quality synthetic dataset for instance-grounded text-to-image generation

Method: re-captioning strategy, focusing on the pre-detection stage, where a VLM generates comprehensive visual descriptions that are then processed by an LLM to extract a flat list of potential categories for OVDs to detect

Result: ROVI exceeds existing detection datasets in image quality and resolution while containing two orders of magnitude more categories with an open-vocabulary nature. GLIGEN trained on ROVI significantly outperforms state-of-the-art alternatives in instance grounding accuracy, prompt fidelity, and aesthetic quality.

Conclusion: ROVI outperforms existing detection datasets in image quality and resolution, containing two orders of magnitude more categories. GLIGEN trained on ROVI significantly outperforms state-of-the-art alternatives.

Abstract: We present ROVI, a high-quality synthetic dataset for instance-grounded
text-to-image generation, created by labeling 1M curated web images. Our key
innovation is a strategy called re-captioning, focusing on the pre-detection
stage, where a VLM (Vision-Language Model) generates comprehensive visual
descriptions that are then processed by an LLM (Large Language Model) to
extract a flat list of potential categories for OVDs (Open-Vocabulary
Detectors) to detect. This approach yields a global prompt inherently linked to
instance annotations while capturing secondary visual elements humans typically
overlook. Evaluations show that ROVI exceeds existing detection datasets in
image quality and resolution while containing two orders of magnitude more
categories with an open-vocabulary nature. For demonstrative purposes, a
text-to-image model GLIGEN trained on ROVI significantly outperforms
state-of-the-art alternatives in instance grounding accuracy, prompt fidelity,
and aesthetic quality. Our dataset and reproducible pipeline are available at
https://github.com/CihangPeng/ROVI.

</details>


### [53] [AutoSIGHT: Automatic Eye Tracking-based System for Immediate Grading of Human experTise](https://arxiv.org/abs/2508.01015)
*Byron Dowling,Jozef Probcin,Adam Czajka*

Main category: cs.CV

TL;DR: AutoSIGHT使用眼动追踪数据区分视觉任务中的专家和非专家，为人类与AI协作开辟了新方向。


<details>
  <summary>Details</summary>
Motivation: 能否教会机器基于眼动追踪特征自动评估人类解决视觉任务的专业知识？

Method: AutoSIGHT：一个自动系统，用于即时评估人类专业知识，它基于从眼动追踪数据中提取的特征集合，对专家和非专家执行者进行分类。

Result: 在虹膜呈现攻击检测（PAD）任务中，AutoSIGHT在5秒的评估窗口内，在subject-disjoint的训练测试机制下，平均ROC曲线下面积达到0.751。当评估窗口增加到30秒时，AUROC增加到0.8306。

Conclusion: AutoSIGHT在虹膜呈现攻击检测任务中表现出区分专家和非专家的能力，尤其是在较长的评估窗口下。该研究为人类与AI的配对设置中自动衡量人类和机器专业知识开辟了新的研究领域。

Abstract: Can we teach machines to assess the expertise of humans solving visual tasks
automatically based on eye tracking features? This paper proposes AutoSIGHT,
Automatic System for Immediate Grading of Human experTise, that classifies
expert and non-expert performers, and builds upon an ensemble of features
extracted from eye tracking data while the performers were solving a visual
task. Results on the task of iris Presentation Attack Detection (PAD) used for
this study show that with a small evaluation window of just 5 seconds,
AutoSIGHT achieves an average average Area Under the ROC curve performance of
0.751 in subject-disjoint train-test regime, indicating that such detection is
viable. Furthermore, when a larger evaluation window of up to 30 seconds is
available, the Area Under the ROC curve (AUROC) increases to 0.8306, indicating
the model is effectively leveraging more information at a cost of slightly
delayed decisions. This work opens new areas of research on how to incorporate
the automatic weighing of human and machine expertise into human-AI pairing
setups, which need to react dynamically to nonstationary expertise distribution
between the human and AI players (e.g. when the experts need to be replaced, or
the task at hand changes rapidly). Along with this paper, we offer the eye
tracking data used in this study collected from 6 experts and 53 non-experts
solving iris PAD visual task.

</details>


### [54] [3D Reconstruction via Incremental Structure From Motion](https://arxiv.org/abs/2508.01019)
*Muhammad Zeeshan,Umer Zaki,Syed Ahmed Pasha,Zaar Khizar*

Main category: cs.CV

TL;DR: Incremental SfM for robust 3D reconstruction from images.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D reconstruction from unstructured image collections is a key requirement in applications such as robotics, mapping, and scene understanding. Global SfM techniques rely on full image connectivity and can be sensitive to noise or missing data, incremental SfM offers a more flexible alternative.

Method: Detailed implementation of the incremental SfM pipeline, focusing on the consistency of geometric estimation and the effect of iterative refinement through bundle adjustment.

Result: Demonstrated the approach using a real dataset and assess reconstruction quality through reprojection error and camera trajectory coherence.

Conclusion: Incremental SfM is a reliable method for sparse 3D reconstruction in visually structured environments.

Abstract: Accurate 3D reconstruction from unstructured image collections is a key
requirement in applications such as robotics, mapping, and scene understanding.
While global Structure from Motion (SfM) techniques rely on full image
connectivity and can be sensitive to noise or missing data, incremental SfM
offers a more flexible alternative. By progressively incorporating new views
into the reconstruction, it enables the system to recover scene structure and
camera motion even in sparse or partially overlapping datasets. In this paper,
we present a detailed implementation of the incremental SfM pipeline, focusing
on the consistency of geometric estimation and the effect of iterative
refinement through bundle adjustment. We demonstrate the approach using a real
dataset and assess reconstruction quality through reprojection error and camera
trajectory coherence. The results support the practical utility of incremental
SfM as a reliable method for sparse 3D reconstruction in visually structured
environments.

</details>


### [55] [Structured Spectral Graph Learning for Anomaly Classification in 3D Chest CT Scans](https://arxiv.org/abs/2508.01045)
*Theo Di Piazza,Carole Lazarus,Olivier Nempont,Loic Boussel*

Main category: cs.CV

TL;DR: This paper presents a graph-based approach for multi-label anomaly classification in CT scans that is robust, generalizable, and performs well.


<details>
  <summary>Details</summary>
Motivation: The increasing number of CT scan examinations necessitates automated methods to assist radiologists. Existing 3D convolutional networks have limited abilities to model long-range dependencies, and Vision Transformers suffer from high computational costs and require extensive pre-training.

Method: The method models CT scans as structured graphs, leveraging axial slice triplets nodes processed through spectral domain convolution.

Result: The proposed method exhibits strong cross-dataset generalization, competitive performance, and robustness to z-axis translation. An ablation study evaluates the contribution of each proposed component.

Conclusion: This paper introduces a graph-based approach for multi-label anomaly classification in 3D CT scans, demonstrating strong cross-dataset generalization, competitive performance, and robustness to z-axis translation.

Abstract: With the increasing number of CT scan examinations, there is a need for
automated methods such as organ segmentation, anomaly detection and report
generation to assist radiologists in managing their increasing workload.
Multi-label classification of 3D CT scans remains a critical yet challenging
task due to the complex spatial relationships within volumetric data and the
variety of observed anomalies. Existing approaches based on 3D convolutional
networks have limited abilities to model long-range dependencies while Vision
Transformers suffer from high computational costs and often require extensive
pre-training on large-scale datasets from the same domain to achieve
competitive performance. In this work, we propose an alternative by introducing
a new graph-based approach that models CT scans as structured graphs,
leveraging axial slice triplets nodes processed through spectral domain
convolution to enhance multi-label anomaly classification performance. Our
method exhibits strong cross-dataset generalization, and competitive
performance while achieving robustness to z-axis translation. An ablation study
evaluates the contribution of each proposed component.

</details>


### [56] [Evading Data Provenance in Deep Neural Networks](https://arxiv.org/abs/2508.01074)
*Hongyu Zhu,Sichu Liang,Wenwen Wang,Zhuomeng Zhang,Fangqi Li,Shi-Lin Wang*

Main category: cs.CV

TL;DR: 本文提出了一个规避数据集所有权验证(DOV)的新框架，该框架优于现有方法，并揭示了当前DOV方法的漏洞。


<details>
  <summary>Details</summary>
Motivation: 许多数据集是专有的或包含敏感信息，这使得不受限制的模型训练成为问题。数据集所有权验证（DOV）已成为一种有前景的版权保护方法，通过检测未经授权的模型训练和追踪非法活动。

Method: 本文提出了一个统一的规避框架，其中教师模型首先从版权数据集中学习，然后使用分布外（OOD）数据集作为中介，将与任务相关但与标识符无关的领域知识转移给替代学生。

Result: 实验表明，该方法在泛化和有效性方面同时消除了所有版权标识符，并且显着优于九种最先进的规避攻击，且计算开销适中。

Conclusion: 本文揭示了当前数据集所有权验证（DOV）方法中的关键漏洞，强调了长期开发以提高实用性的必要性。

Abstract: Modern over-parameterized deep models are highly data-dependent, with large
scale general-purpose and domain-specific datasets serving as the bedrock for
rapid advancements. However, many datasets are proprietary or contain sensitive
information, making unrestricted model training problematic. In the open world
where data thefts cannot be fully prevented, Dataset Ownership Verification
(DOV) has emerged as a promising method to protect copyright by detecting
unauthorized model training and tracing illicit activities. Due to its
diversity and superior stealth, evading DOV is considered extremely
challenging. However, this paper identifies that previous studies have relied
on oversimplistic evasion attacks for evaluation, leading to a false sense of
security. We introduce a unified evasion framework, in which a teacher model
first learns from the copyright dataset and then transfers task-relevant yet
identifier-independent domain knowledge to a surrogate student using an
out-of-distribution (OOD) dataset as the intermediary. Leveraging
Vision-Language Models and Large Language Models, we curate the most
informative and reliable subsets from the OOD gallery set as the final transfer
set, and propose selectively transferring task-oriented knowledge to achieve a
better trade-off between generalization and evasion effectiveness. Experiments
across diverse datasets covering eleven DOV methods demonstrate our approach
simultaneously eliminates all copyright identifiers and significantly
outperforms nine state-of-the-art evasion attacks in both generalization and
effectiveness, with moderate computational overhead. As a proof of concept, we
reveal key vulnerabilities in current DOV methods, highlighting the need for
long-term development to enhance practicality.

</details>


### [57] [DreamSat-2.0: Towards a General Single-View Asteroid 3D Reconstruction](https://arxiv.org/abs/2508.01079)
*Santiago Diaz,Xinghui Hu,Josiane Uwumukiza,Giovanni Lavezzi,Victor Rodriguez-Fernandez,Richard Linares*

Main category: cs.CV

TL;DR: DreamSat-2.0是一个pipeline，它对三种3D重建模型在航天器和小行星数据集上进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 为了加强小行星探测和自主航天器导航，我们引入了DreamSat-2.0。

Method: 对三种最先进的3D重建模型——Hunyuan-3D、Trellis-3D和Ouroboros-3D——在定制的航天器和小行星数据集上进行基准测试。

Result: 模型性能是依赖于领域的。虽然模型产生了更高质量的复杂航天器图像，但它们为更简单的小行星形状实现了更好的几何重建。建立了新的基准，

Conclusion: Hunyuan-3D在宇宙飞船上实现了最高的感知分数，但在小行星上实现了最佳的几何精度，标志着比我们之前的工作有了显著的进步。

Abstract: To enhance asteroid exploration and autonomous spacecraft navigation, we
introduce DreamSat-2.0, a pipeline that benchmarks three state-of-the-art 3D
reconstruction models-Hunyuan-3D, Trellis-3D, and Ouroboros-3D-on custom
spacecraft and asteroid datasets. Our systematic analysis, using 2D perceptual
(image quality) and 3D geometric (shape accuracy) metrics, reveals that model
performance is domain-dependent. While models produce higher-quality images of
complex spacecraft, they achieve better geometric reconstructions for the
simpler forms of asteroids. New benchmarks are established, with Hunyuan-3D
achieving top perceptual scores on spacecraft but its best geometric accuracy
on asteroids, marking a significant advance over our prior work.

</details>


### [58] [COSTARR: Consolidated Open Set Technique with Attenuation for Robust Recognition](https://arxiv.org/abs/2508.01087)
*Ryan Rabinowitz,Steve Cruz,Walter Scheirer,Terrance E. Boult*

Main category: cs.CV

TL;DR: COSTARR结合了熟悉和不熟悉特征来提高开放集识别能力，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 处理新颖性仍然是视觉识别系统中的一个关键挑战。现有的开放集识别（OSR）方法依赖于熟悉性假设，通过缺少熟悉特征来检测新颖性。

Method: 提出了一种新颖的衰减假设：在训练期间学习的小权重会衰减特征，并发挥双重作用——区分已知类别，同时丢弃区分已知类别与未知类别的信息。为了利用这种被忽视的信息，我们提出了一种新颖的COSTARR方法，该方法结合了熟悉特征的需求和缺乏不熟悉特征的需求。

Result: 实验表明，COSTARR在各种架构上有效地推广，并且通过合并先前丢弃的衰减信息，显著优于先前的最先进方法，从而提高了开放集识别能力。

Conclusion: COSTARR通过结合熟悉特征的需求和缺乏不熟悉特征的需求，在各种架构上有效地推广，并且通过合并先前丢弃的衰减信息，显著优于先前的最先进方法，从而提高了开放集识别能力。

Abstract: Handling novelty remains a key challenge in visual recognition systems.
Existing open-set recognition (OSR) methods rely on the familiarity hypothesis,
detecting novelty by the absence of familiar features. We propose a novel
attenuation hypothesis: small weights learned during training attenuate
features and serve a dual role-differentiating known classes while discarding
information useful for distinguishing known from unknown classes. To leverage
this overlooked information, we present COSTARR, a novel approach that combines
both the requirement of familiar features and the lack of unfamiliar ones. We
provide a probabilistic interpretation of the COSTARR score, linking it to the
likelihood of correct classification and belonging in a known class. To
determine the individual contributions of the pre- and post-attenuated features
to COSTARR's performance, we conduct ablation studies that show both
pre-attenuated deep features and the underutilized post-attenuated Hadamard
product features are essential for improving OSR. Also, we evaluate COSTARR in
a large-scale setting using ImageNet2012-1K as known data and NINCO,
iNaturalist, OpenImage-O, and other datasets as unknowns, across multiple
modern pre-trained architectures (ViTs, ConvNeXts, and ResNet). The experiments
demonstrate that COSTARR generalizes effectively across various architectures
and significantly outperforms prior state-of-the-art methods by incorporating
previously discarded attenuation information, advancing open-set recognition
capabilities.

</details>


### [59] [AURA: A Hybrid Spatiotemporal-Chromatic Framework for Robust, Real-Time Detection of Industrial Smoke Emissions](https://arxiv.org/abs/2508.01095)
*Mikhail Bychkov,Matey Yordanov,Andrei Kuchma*

Main category: cs.CV

TL;DR: AURA是一种用于工业烟雾排放的实时检测和分类的新框架，提高了准确性，减少了误报。


<details>
  <summary>Details</summary>
Motivation: 当前的监测系统缺乏区分烟雾类型的特异性，并且难以应对环境变化。

Method: AURA：一种新型的混合时空色框架

Result: 增强了准确性并减少了误报。

Conclusion: AURA框架通过精确的自动监测工业排放，显著提高环境合规性、运营安全和公众健康水平。

Abstract: This paper introduces AURA, a novel hybrid spatiotemporal-chromatic framework
designed for robust, real-time detection and classification of industrial smoke
emissions. The framework addresses critical limitations of current monitoring
systems, which often lack the specificity to distinguish smoke types and
struggle with environmental variability. AURA leverages both the dynamic
movement patterns and the distinct color characteristics of industrial smoke to
provide enhanced accuracy and reduced false positives. This framework aims to
significantly improve environmental compliance, operational safety, and public
health outcomes by enabling precise, automated monitoring of industrial
emissions.

</details>


### [60] [Trans-Adapter: A Plug-and-Play Framework for Transparent Image Inpainting](https://arxiv.org/abs/2508.01098)
*Yuekun Dai,Haitian Li,Shangchen Zhou,Chen Change Loy*

Main category: cs.CV

TL;DR: 我们提出了Trans-Adapter，它可以直接处理透明图像，并且支持可控编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的图像修复方法是专门为RGB图像设计的。传统的透明图像修复方法通常包括在RGBA图像下放置一个背景，并采用两阶段的过程：图像修复，然后进行图像抠图。然而，这种流程难以保持编辑区域的透明度一致性，并且抠图会在透明度边界引入锯齿边缘。

Method: 我们提出了Trans-Adapter，这是一个即插即用的适配器，使基于扩散的修复模型能够直接处理透明图像。

Result: Trans-Adapter还支持通过ControlNet进行可控编辑，并且可以无缝集成到各种社区模型中。为了评估我们的方法，我们引入了LayerBench，以及一种用于评估透明度边缘质量的新型非参考alpha边缘质量评估指标。

Conclusion: 我们进行了大量的实验，以证明我们的方法是有效的。

Abstract: RGBA images, with the additional alpha channel, are crucial for any
application that needs blending, masking, or transparency effects, making them
more versatile than standard RGB images. Nevertheless, existing image
inpainting methods are designed exclusively for RGB images. Conventional
approaches to transparent image inpainting typically involve placing a
background underneath RGBA images and employing a two-stage process: image
inpainting followed by image matting. This pipeline, however, struggles to
preserve transparency consistency in edited regions, and matting can introduce
jagged edges along transparency boundaries. To address these challenges, we
propose Trans-Adapter, a plug-and-play adapter that enables diffusion-based
inpainting models to process transparent images directly. Trans-Adapter also
supports controllable editing via ControlNet and can be seamlessly integrated
into various community models. To evaluate our method, we introduce LayerBench,
along with a novel non-reference alpha edge quality evaluation metric for
assessing transparency edge quality. We conduct extensive experiments on
LayerBench to demonstrate the effectiveness of our approach.

</details>


### [61] [MASIV: Toward Material-Agnostic System Identification from Videos](https://arxiv.org/abs/2508.01112)
*Yizhou Zhao,Haoyu Chen,Chunjiang Liu,Zhenyang Li,Charles Herrmann,Junhwa Hur,Yinxiao Li,Ming-Hsuan Yang,Bhiksha Raj,Min Xu*

Main category: cs.CV

TL;DR: MASIV是一种与材料无关的系统识别的视觉框架，它使用可学习的神经本构模型来推断物体动力学，而无需假设场景特定的材料先验。


<details>
  <summary>Details</summary>
Motivation: 现有的方法将可微渲染与模拟相结合，但依赖于预定义的材料先验，限制了它们处理未知材料的能力。

Method: MASIV采用可学习的神经本构模型，无需假设特定场景的材料先验即可推断物体动力学。

Result: MASIV实现了最先进的几何精度、渲染质量和泛化能力。

Conclusion: MASIV在几何精度、渲染质量和泛化能力方面都达到了最先进的性能。

Abstract: System identification from videos aims to recover object geometry and
governing physical laws. Existing methods integrate differentiable rendering
with simulation but rely on predefined material priors, limiting their ability
to handle unknown ones. We introduce MASIV, the first vision-based framework
for material-agnostic system identification. Unlike existing approaches that
depend on hand-crafted constitutive laws, MASIV employs learnable neural
constitutive models, inferring object dynamics without assuming a
scene-specific material prior. However, the absence of full particle state
information imposes unique challenges, leading to unstable optimization and
physically implausible behaviors. To address this, we introduce dense geometric
guidance by reconstructing continuum particle trajectories, providing
temporally rich motion constraints beyond sparse visual cues. Comprehensive
experiments show that MASIV achieves state-of-the-art performance in geometric
accuracy, rendering quality, and generalization ability.

</details>


### [62] [The Promise of RL for Autoregressive Image Editing](https://arxiv.org/abs/2508.01119)
*Saba Ahmadi,Rabiul Awal,Ankur Sikarwar,Amirhossein Kazemnejad,Ge Ya Luo,Juan A. Rodriguez,Sai Rajeswar,Siva Reddy,Christopher Pal,Benno Krojer,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: EARL: A strong RL-based image editing model that performs well with less data.


<details>
  <summary>Details</summary>
Motivation: Enhance performance on a wide range of image editing tasks.

Method: Autoregressive multimodal model with supervised fine-tuning (SFT), reinforcement learning (RL), and Chain-of-Thought (CoT) reasoning.

Result: RL combined with a large multi-modal LLM verifier is the most effective strategy. EARL performs competitively on a diverse range of edits compared to strong baselines, despite using much less training data.

Conclusion: EARL, an RL-based image editing model, achieves competitive performance on diverse edits with less training data, advancing autoregressive multimodal models.

Abstract: We explore three strategies to enhance performance on a wide range of image
editing tasks: supervised fine-tuning (SFT), reinforcement learning (RL), and
Chain-of-Thought (CoT) reasoning. In order to study all these components in one
consistent framework, we adopt an autoregressive multimodal model that
processes textual and visual tokens in a unified manner. We find RL combined
with a large multi-modal LLM verifier to be the most effective of these
strategies. As a result, we release EARL: Editing with Autoregression and RL, a
strong RL-based image editing model that performs competitively on a diverse
range of edits compared to strong baselines, despite using much less training
data. Thus, EARL pushes the frontier of autoregressive multimodal models on
image editing. We release our code, training data, and trained models at
https://github.com/mair-lab/EARL.

</details>


### [63] [UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction, Forecasting, and Generation](https://arxiv.org/abs/2508.01126)
*Chaitanya Patel,Hiroki Nakamura,Yuta Kyuragi,Kazuki Kozuka,Juan Carlos Niebles,Ehsan Adeli*

Main category: cs.CV

TL;DR: This paper introduces UniEgoMotion, a unified conditional motion diffusion model for egocentric motion generation and forecasting from first-person images, and introduces EE4D-Motion, a large-scale dataset for training.


<details>
  <summary>Details</summary>
Motivation: Existing methods primarily focus on third-person motion synthesis with structured 3D scene contexts, limiting their effectiveness in real-world egocentric settings where limited field of view, frequent occlusions, and dynamic cameras hinder scene perception.

Method: We propose UniEgoMotion, a unified conditional motion diffusion model with a novel head-centric motion representation tailored for egocentric devices.

Result: UniEgoMotion achieves state-of-the-art performance in egocentric motion reconstruction and is the first to generate motion from a single egocentric image.

Conclusion: UniEgoMotion achieves state-of-the-art performance in egocentric motion reconstruction and is the first to generate motion from a single egocentric image. Extensive evaluations demonstrate the effectiveness of our unified framework, setting a new benchmark for egocentric motion modeling and unlocking new possibilities for egocentric applications.

Abstract: Egocentric human motion generation and forecasting with scene-context is
crucial for enhancing AR/VR experiences, improving human-robot interaction,
advancing assistive technologies, and enabling adaptive healthcare solutions by
accurately predicting and simulating movement from a first-person perspective.
However, existing methods primarily focus on third-person motion synthesis with
structured 3D scene contexts, limiting their effectiveness in real-world
egocentric settings where limited field of view, frequent occlusions, and
dynamic cameras hinder scene perception. To bridge this gap, we introduce
Egocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks
that utilize first-person images for scene-aware motion synthesis without
relying on explicit 3D scene. We propose UniEgoMotion, a unified conditional
motion diffusion model with a novel head-centric motion representation tailored
for egocentric devices. UniEgoMotion's simple yet effective design supports
egocentric motion reconstruction, forecasting, and generation from first-person
visual inputs in a unified framework. Unlike previous works that overlook scene
semantics, our model effectively extracts image-based scene context to infer
plausible 3D motion. To facilitate training, we introduce EE4D-Motion, a
large-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth
3D motion annotations. UniEgoMotion achieves state-of-the-art performance in
egocentric motion reconstruction and is the first to generate motion from a
single egocentric image. Extensive evaluations demonstrate the effectiveness of
our unified framework, setting a new benchmark for egocentric motion modeling
and unlocking new possibilities for egocentric applications.

</details>


### [64] [Semi-Supervised Anomaly Detection in Brain MRI Using a Domain-Agnostic Deep Reinforcement Learning Approach](https://arxiv.org/abs/2508.01137)
*Zeduo Zhang,Yalda Mohsenzadeh*

Main category: cs.CV

TL;DR: Developed a DRL-based anomaly detection framework for brain MRIs, achieving high AUROC scores and demonstrating strong generalizability across domains.


<details>
  <summary>Details</summary>
Motivation: address challenges such as large-scale data, overfitting, and class imbalance, focusing on brain MRI volumes

Method: integrates DRL with feature representations to handle label scarcity, large-scale data and overfitting

Result: achieved an AUROC of 88.7% (pixel-level) and 96.7% (image-level) on brain MRI datasets, outperforming State-of-The-Art (SOTA) methods. On industrial surface datasets, the model also showed competitive performance (AUROC = 99.8% pixel-level, 99.3% image-level) on MVTec AD dataset, indicating strong cross-domain generalization

Conclusion: The domain-agnostic semi-supervised approach using DRL shows significant promise for MRI anomaly detection, achieving strong performance on both medical and industrial datasets. Its robustness, generalizability and efficiency highlight its potential for real-world clinical applications.

Abstract: To develop a domain-agnostic, semi-supervised anomaly detection framework
that integrates deep reinforcement learning (DRL) to address challenges such as
large-scale data, overfitting, and class imbalance, focusing on brain MRI
volumes. This retrospective study used publicly available brain MRI datasets
collected between 2005 and 2021. The IXI dataset provided 581 T1-weighted and
578 T2-weighted MRI volumes (from healthy subjects) for training, while the
BraTS 2021 dataset provided 251 volumes for validation and 1000 for testing
(unhealthy subjects with Glioblastomas). Preprocessing included normalization,
skull-stripping, and co-registering to a uniform voxel size. Experiments were
conducted on both T1- and T2-weighted modalities. Additional experiments and
ablation analyses were also carried out on the industrial datasets. The
proposed method integrates DRL with feature representations to handle label
scarcity, large-scale data and overfitting. Statistical analysis was based on
several detection and segmentation metrics including AUROC and Dice score. The
proposed method achieved an AUROC of 88.7% (pixel-level) and 96.7%
(image-level) on brain MRI datasets, outperforming State-of-The-Art (SOTA)
methods. On industrial surface datasets, the model also showed competitive
performance (AUROC = 99.8% pixel-level, 99.3% image-level) on MVTec AD dataset,
indicating strong cross-domain generalization. Studies on anomaly sample size
showed a monotonic increase in AUROC as more anomalies were seen, without
evidence of overfitting or additional computational cost. The domain-agnostic
semi-supervised approach using DRL shows significant promise for MRI anomaly
detection, achieving strong performance on both medical and industrial
datasets. Its robustness, generalizability and efficiency highlight its
potential for real-world clinical applications.

</details>


### [65] [Dataset Condensation with Color Compensation](https://arxiv.org/abs/2508.01139)
*Huyu Wu,Duo Su,Junjie Hou,Guang Li*

Main category: cs.CV

TL;DR: DC3 是一种带有颜色补偿的数据集浓缩框架，它利用潜在的扩散模型来增强图像的颜色多样性，从而在多个基准测试中优于 SOTA 方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个瓶颈：图像级选择方法效率低下，而像素级优化由于过度参数化而引入语义失真。数据集浓缩的一个关键问题是忽视了颜色作为信息载体和基本语义表示单元的双重作用。改善浓缩图像的色彩对于表示学习是有益的。

Method: DC3：一种带有颜色补偿的数据集浓缩框架。在校准的选择策略之后，DC3 利用潜在的扩散模型来增强图像的颜色多样性，而不是创建一个全新的图像。

Result: DC3 优于 SOTA 方法，具有卓越的性能和泛化能力。FID 结果证明，使用高质量数据集进行训练是可行的，不会出现模型崩溃或其他退化问题。

Conclusion: DC3在多个基准测试中优于 SOTA 方法，具有卓越的性能和泛化能力。使用高质量数据集进行训练是可行的，不会出现模型崩溃或其他退化问题。

Abstract: Dataset condensation always faces a constitutive trade-off: balancing
performance and fidelity under extreme compression. Existing methods struggle
with two bottlenecks: image-level selection methods (Coreset Selection, Dataset
Quantization) suffer from inefficiency condensation, while pixel-level
optimization (Dataset Distillation) introduces semantic distortion due to
over-parameterization. With empirical observations, we find that a critical
problem in dataset condensation is the oversight of color's dual role as an
information carrier and a basic semantic representation unit. We argue that
improving the colorfulness of condensed images is beneficial for representation
learning. Motivated by this, we propose DC3: a Dataset Condensation framework
with Color Compensation. After a calibrated selection strategy, DC3 utilizes
the latent diffusion model to enhance the color diversity of an image rather
than creating a brand-new one. Extensive experiments demonstrate the superior
performance and generalization of DC3 that outperforms SOTA methods across
multiple benchmarks. To the best of our knowledge, besides focusing on
downstream tasks, DC3 is the first research to fine-tune pre-trained diffusion
models with condensed datasets. The FID results prove that training networks
with our high-quality datasets is feasible without model collapse or other
degradation issues. Code and generated data will be released soon.

</details>


### [66] [OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian Splatting for Refined Object-Level Understanding](https://arxiv.org/abs/2508.01150)
*Dianyi Yang,Xihan Wang,Yu Gao,Shiyang Liu,Bohan Ren,Yufeng Yue,Yi Yang*

Main category: cs.CV

TL;DR: OpenGS-Fusion, an innovative open-vocabulary dense mapping framework that improves semantic modeling and refines object-level understanding.


<details>
  <summary>Details</summary>
Motivation: Existing methods are hindered by rigid offline pipelines and the inability to provide precise 3D object-level understanding given open-ended queries.

Method: OpenGS-Fusion combines 3D Gaussian representation with a Truncated Signed Distance Field to facilitate lossless fusion of semantic features on-the-fly. Furthermore, the authors introduce a novel multimodal language-guided approach named MLLM-Assisted Adaptive Thresholding, which refines the segmentation of 3D objects by adaptively adjusting similarity thresholds

Result: achieving an improvement 17% in 3D mIoU compared to the fixed threshold strategy.

Conclusion: The method outperforms existing methods in 3D object understanding and scene reconstruction quality, as well as showcasing its effectiveness in language-guided scene interaction.

Abstract: Recent advancements in 3D scene understanding have made significant strides
in enabling interaction with scenes using open-vocabulary queries, particularly
for VR/AR and robotic applications. Nevertheless, existing methods are hindered
by rigid offline pipelines and the inability to provide precise 3D object-level
understanding given open-ended queries. In this paper, we present
OpenGS-Fusion, an innovative open-vocabulary dense mapping framework that
improves semantic modeling and refines object-level understanding.
OpenGS-Fusion combines 3D Gaussian representation with a Truncated Signed
Distance Field to facilitate lossless fusion of semantic features on-the-fly.
Furthermore, we introduce a novel multimodal language-guided approach named
MLLM-Assisted Adaptive Thresholding, which refines the segmentation of 3D
objects by adaptively adjusting similarity thresholds, achieving an improvement
17\% in 3D mIoU compared to the fixed threshold strategy. Extensive experiments
demonstrate that our method outperforms existing methods in 3D object
understanding and scene reconstruction quality, as well as showcasing its
effectiveness in language-guided scene interaction. The code is available at
https://young-bit.github.io/opengs-fusion.github.io/ .

</details>


### [67] [Personalized Safety Alignment for Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.01151)
*Yu Lei,Jinbin Bai,Qingyu Shi,Aosong Feng,Kaidong Yu*

Main category: cs.CV

TL;DR: Personalized Safety Alignment (PSA) allows user-specific control over safety behaviors in generative models.


<details>
  <summary>Details</summary>
Motivation: Current safety mechanisms apply uniform standards that often fail to account for individual user preferences, overlooking the diverse safety boundaries shaped by factors like age, mental health, and personal beliefs.

Method: Personalized Safety Alignment (PSA), a framework that allows user-specific control over safety behaviors in generative models. PSA integrates personalized user profiles into the diffusion process, adjusting the model's behavior to match individual safety preferences while preserving image quality. A new dataset, Sage, captures user-specific safety preferences and incorporates these profiles through a cross-attention mechanism.

Result: PSA outperforms existing methods in harmful content suppression and aligns generated content better with user constraints, achieving higher Win Rate and Pass Rate scores.

Conclusion: Personalized Safety Alignment (PSA) can better align generated content with user constraints, achieving higher Win Rate and Pass Rate scores.

Abstract: Text-to-image diffusion models have revolutionized visual content generation,
but current safety mechanisms apply uniform standards that often fail to
account for individual user preferences. These models overlook the diverse
safety boundaries shaped by factors like age, mental health, and personal
beliefs. To address this, we propose Personalized Safety Alignment (PSA), a
framework that allows user-specific control over safety behaviors in generative
models. PSA integrates personalized user profiles into the diffusion process,
adjusting the model's behavior to match individual safety preferences while
preserving image quality. We introduce a new dataset, Sage, which captures
user-specific safety preferences and incorporates these profiles through a
cross-attention mechanism. Experiments show that PSA outperforms existing
methods in harmful content suppression and aligns generated content better with
user constraints, achieving higher Win Rate and Pass Rate scores. Our code,
data, and models are publicly available at
https://torpedo2648.github.io/PSAlign/.

</details>


### [68] [LawDIS: Language-Window-based Controllable Dichotomous Image Segmentation](https://arxiv.org/abs/2508.01152)
*Xinyu Yan,Meijun Sun,Ge-Peng Ji,Fahad Shahbaz Khan,Salman Khan,Deng-Ping Fan*

Main category: cs.CV

TL;DR: LawDIS是一个基于语言窗口的可控二分图像分割框架，它通过宏观到微观的控制模式，在DIS5K基准测试中取得了显著优于其他方法的性能。


<details>
  <summary>Details</summary>
Motivation: 为了产生高质量的对象掩码，并实现用户控制的无缝集成。

Method: 提出了一种基于语言窗口的可控二分图像分割(DIS)框架LawDIS，它将DIS重铸为潜在扩散模型中的图像条件掩码生成任务，并集成了宏观到微观的控制模式。

Result: LawDIS在DIS5K基准测试中，各项指标均显著优于11种最先进的方法。

Conclusion: LawDIS在DIS5K基准测试中显著优于其他方法，尤其是在DIS-TE数据集上，通过LS和WR策略结合使用，$F_β^ω$指标提升了4.6%，单独使用LS策略提升了3.6%。

Abstract: We present LawDIS, a language-window-based controllable dichotomous image
segmentation (DIS) framework that produces high-quality object masks. Our
framework recasts DIS as an image-conditioned mask generation task within a
latent diffusion model, enabling seamless integration of user controls. LawDIS
is enhanced with macro-to-micro control modes. Specifically, in macro mode, we
introduce a language-controlled segmentation strategy (LS) to generate an
initial mask based on user-provided language prompts. In micro mode, a
window-controlled refinement strategy (WR) allows flexible refinement of
user-defined regions (i.e., size-adjustable windows) within the initial mask.
Coordinated by a mode switcher, these modes can operate independently or
jointly, making the framework well-suited for high-accuracy, personalised
applications. Extensive experiments on the DIS5K benchmark reveal that our
LawDIS significantly outperforms 11 cutting-edge methods across all metrics.
Notably, compared to the second-best model MVANet, we achieve $F_\beta^\omega$
gains of 4.6\% with both the LS and WR strategies and 3.6\% gains with only the
LS strategy on DIS-TE. Codes will be made available at
https://github.com/XinyuYanTJU/LawDIS.

</details>


### [69] [TEACH: Text Encoding as Curriculum Hints for Scene Text Recognition](https://arxiv.org/abs/2508.01153)
*Xiahan Yang,Hui Zheng*

Main category: cs.CV

TL;DR: 提出了一种新颖的训练范式 TEACH，该范式将真实文本注入模型作为辅助输入，并在训练过程中逐步降低其影响，从而引导模型从依赖标签的学习到完全的视觉识别。


<details>
  <summary>Details</summary>
Motivation: 由于复杂的视觉外观和有限的语义先验，场景文本识别 (STR) 仍然是一项具有挑战性的任务。

Method: 将真实文本注入模型作为辅助输入，并在训练过程中逐步降低其影响。

Result: 在多个公共基准上的大量实验表明，使用 TEACH 训练的模型始终能提高准确率。

Conclusion: 模型在具有挑战性的条件下始终能提高准确率，验证了模型的鲁棒性和通用性。

Abstract: Scene Text Recognition (STR) remains a challenging task due to complex visual
appearances and limited semantic priors. We propose TEACH, a novel training
paradigm that injects ground-truth text into the model as auxiliary input and
progressively reduces its influence during training. By encoding target labels
into the embedding space and applying loss-aware masking, TEACH simulates a
curriculum learning process that guides the model from label-dependent learning
to fully visual recognition. Unlike language model-based approaches, TEACH
requires no external pretraining and introduces no inference overhead. It is
model-agnostic and can be seamlessly integrated into existing encoder-decoder
frameworks. Extensive experiments across multiple public benchmarks show that
models trained with TEACH achieve consistently improved accuracy, especially
under challenging conditions, validating its robustness and general
applicability.

</details>


### [70] [I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking](https://arxiv.org/abs/2508.02243)
*Ziyan Liu,Junwen Li,Kaiwen Li,Tong Ruan,Chao Wang,Xinyan He,Zongyu Wang,Xuezhi Cao,Jingping Liu*

Main category: cs.CV

TL;DR: 提出了一种新的 LLM 框架，通过迭代整合关键视觉线索来提高多模态实体链接的准确性，并在三个数据集上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 尽管基于大型语言模型的方法取得了成功，但这些方法仍然面临两个挑战，包括在某些情况下不必要地整合图像数据，以及仅依赖于视觉特征的一次性提取，这可能会削弱其有效性和准确性。

Method: 提出了一种新的基于 LLM 的多模态实体链接任务框架，称为模内和模间协作反思。

Result: 该框架始终优于当前最先进的方法，分别提高了 3.2%、5.1% 和 1.6%。

Conclusion: 该框架在三个广泛使用的公共数据集上进行了大量实验，结果表明，该框架在该任务中始终优于当前最先进的方法，分别提高了 3.2%、5.1% 和 1.6%。

Abstract: Multimodal entity linking plays a crucial role in a wide range of
applications. Recent advances in large language model-based methods have become
the dominant paradigm for this task, effectively leveraging both textual and
visual modalities to enhance performance. Despite their success, these methods
still face two challenges, including unnecessary incorporation of image data in
certain scenarios and the reliance only on a one-time extraction of visual
features, which can undermine their effectiveness and accuracy. To address
these challenges, we propose a novel LLM-based framework for the multimodal
entity linking task, called Intra- and Inter-modal Collaborative Reflections.
This framework prioritizes leveraging text information to address the task.
When text alone is insufficient to link the correct entity through intra- and
inter-modality evaluations, it employs a multi-round iterative strategy that
integrates key visual clues from various aspects of the image to support
reasoning and enhance matching accuracy. Extensive experiments on three widely
used public datasets demonstrate that our framework consistently outperforms
current state-of-the-art methods in the task, achieving improvements of 3.2%,
5.1%, and 1.6%, respectively. Our code is available at
https://github.com/ziyan-xiaoyu/I2CR/.

</details>


### [71] [DELTAv2: Accelerating Dense 3D Tracking](https://arxiv.org/abs/2508.01170)
*Tuan Duc Ngo,Ashkan Mirzaei,Guocheng Qian,Hanwen Liang,Chuang Gan,Evangelos Kalogerakis,Peter Wonka,Chaoyang Wang*

Main category: cs.CV

TL;DR: This paper proposes a novel algorithm for accelerating dense long-term 3D point tracking in videos, which leads to a 5-100x speedup over existing approaches while maintaining state-of-the-art tracking accuracy.


<details>
  <summary>Details</summary>
Motivation: transformer-based iterative tracking becomes expensive when handling a large number of trajectories.  the cost of correlation feature computation, another key bottleneck in prior methods.

Method: introduce a coarse-to-fine strategy that begins tracking with a small subset of points and progressively expands the set of tracked trajectories. The newly added trajectories are initialized using a learnable interpolation module, which is trained end-to-end alongside the tracking network. propose an optimization that significantly reduces the cost of correlation feature computation

Result: Together, these improvements lead to a 5-100x speedup over existing approaches while maintaining state-of-the-art tracking accuracy.

Conclusion: These improvements lead to a 5-100x speedup over existing approaches while maintaining state-of-the-art tracking accuracy.

Abstract: We propose a novel algorithm for accelerating dense long-term 3D point
tracking in videos. Through analysis of existing state-of-the-art methods, we
identify two major computational bottlenecks. First, transformer-based
iterative tracking becomes expensive when handling a large number of
trajectories. To address this, we introduce a coarse-to-fine strategy that
begins tracking with a small subset of points and progressively expands the set
of tracked trajectories. The newly added trajectories are initialized using a
learnable interpolation module, which is trained end-to-end alongside the
tracking network. Second, we propose an optimization that significantly reduces
the cost of correlation feature computation, another key bottleneck in prior
methods. Together, these improvements lead to a 5-100x speedup over existing
approaches while maintaining state-of-the-art tracking accuracy.

</details>


### [72] [Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search](https://arxiv.org/abs/2508.02340)
*Fan Hu,Zijie Xin,Xirong Li*

Main category: cs.CV

TL;DR: This paper proposes LPD to address the visual diversity challenge in Ad-hoc Video Search (AVS) by learning partially decorrelated common spaces. LPD demonstrates its effectiveness and ability to enhance result diversity on the TRECVID AVS benchmarks.


<details>
  <summary>Details</summary>
Motivation: The main challenge of AVS is the visual diversity of relevant videos. Current solutions for the AVS task primarily fuse multiple features into one or more common spaces, yet overlook the need for diverse spaces.

Method: We propose LPD, short for Learning Partially Decorrelated common spaces. LPD incorporates two key innovations: feature-specific common space construction and the de-correlation loss. Specifically, LPD learns a separate common space for each video and text feature, and employs de-correlation loss to diversify the ordering of negative samples across different spaces. To enhance the consistency of multi-space convergence, we designed an entropy-based fair multi-space triplet ranking loss.

Result: LPD enhances result diversity.

Conclusion: Extensive experiments on the TRECVID AVS benchmarks (2016-2023) justify the effectiveness of LPD. Moreover, diversity visualizations of LPD's spaces highlight its ability to enhance result diversity.

Abstract: Ad-hoc Video Search (AVS) involves using a textual query to search for
multiple relevant videos in a large collection of unlabeled short videos. The
main challenge of AVS is the visual diversity of relevant videos. A simple
query such as "Find shots of a man and a woman dancing together indoors" can
span a multitude of environments, from brightly lit halls and shadowy bars to
dance scenes in black-and-white animations. It is therefore essential to
retrieve relevant videos as comprehensively as possible. Current solutions for
the AVS task primarily fuse multiple features into one or more common spaces,
yet overlook the need for diverse spaces. To fully exploit the expressive
capability of individual features, we propose LPD, short for Learning Partially
Decorrelated common spaces. LPD incorporates two key innovations:
feature-specific common space construction and the de-correlation loss.
Specifically, LPD learns a separate common space for each video and text
feature, and employs de-correlation loss to diversify the ordering of negative
samples across different spaces. To enhance the consistency of multi-space
convergence, we designed an entropy-based fair multi-space triplet ranking
loss. Extensive experiments on the TRECVID AVS benchmarks (2016-2023) justify
the effectiveness of LPD. Moreover, diversity visualizations of LPD's spaces
highlight its ability to enhance result diversity.

</details>


### [73] [No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views](https://arxiv.org/abs/2508.01171)
*Ranran Huang,Krystian Mikolajczyk*

Main category: cs.CV

TL;DR: SPFSplat 是一种用于从稀疏多视图图像进行 3D 高斯溅射的高效框架，无需地面实况姿势，并在 novel view 合成和相对姿势估计方面实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 从稀疏的多视图图像中进行 3D 高斯溅射的高效框架，在训练或推理期间不需要地面实况姿势。

Method: SPFSplat 采用共享特征提取主干，能够在单个前馈步骤中从无姿势输入同时预测规范空间中的 3D 高斯基元和相机姿势。

Result: SPFSplat 实现了最先进的 novel view 合成性能，并且在相对姿势估计方面超越了最近的方法。

Conclusion: SPFSplat在 novel view 合成方面实现了最先进的性能，即使在显着的视点变化和有限的图像重叠下也是如此。它还在相对姿势估计方面超越了最近使用几何先验训练的方法。

Abstract: We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from
sparse multi-view images, requiring no ground-truth poses during training or
inference. It employs a shared feature extraction backbone, enabling
simultaneous prediction of 3D Gaussian primitives and camera poses in a
canonical space from unposed inputs within a single feed-forward step.
Alongside the rendering loss based on estimated novel-view poses, a
reprojection loss is integrated to enforce the learning of pixel-aligned
Gaussian primitives for enhanced geometric constraints. This pose-free training
paradigm and efficient one-step feed-forward design make SPFSplat well-suited
for practical applications. Remarkably, despite the absence of pose
supervision, SPFSplat achieves state-of-the-art performance in novel view
synthesis even under significant viewpoint changes and limited image overlap.
It also surpasses recent methods trained with geometry priors in relative pose
estimation. Code and trained models are available on our project page:
https://ranrhuang.github.io/spfsplat/.

</details>


### [74] [Uni-Layout: Integrating Human Feedback in Unified Layout Generation and Evaluation](https://arxiv.org/abs/2508.02374)
*Shuo Lu,Yanyin Chen,Wei Feng,Jiahao Fan,Fengheng Li,Zheng Zhang,Jingjing Lv,Junjie Shen,Ching Law,Jian Liang*

Main category: cs.CV

TL;DR: Uni-Layout is a novel framework that achieves unified generation, human-mimicking evaluation and alignment between the two.


<details>
  <summary>Details</summary>
Motivation: current approaches suffer from task-specific generation capabilities and perceptually misaligned evaluation metrics, leading to limited applicability and ineffective measurement

Method: incorporate various layout tasks into a single taxonomy and develop a unified generator that handles background or element contents constrained tasks via natural language prompts. To introduce human feedback for the effective evaluation of layouts, we build Layout-HF100k, the first large-scale human feedback dataset with 100,000 expertly annotated layouts. Based on Layout-HF100k, we introduce a human-mimicking evaluator that integrates visual and geometric information, employing a Chain-of-Thought mechanism to conduct qualitative assessments alongside a confidence estimation module to yield quantitative measurements. For better alignment between the generator and the evaluator, we integrate them into a cohesive system by adopting Dynamic-Margin Preference Optimization (DMPO), which dynamically adjusts margins based on preference strength to better align with human judgments

Result: Uni-Layout significantly outperforms both task-specific and general-purpose methods

Conclusion: Uni-Layout significantly outperforms both task-specific and general-purpose methods.

Abstract: Layout generation plays a crucial role in enhancing both user experience and
design efficiency. However, current approaches suffer from task-specific
generation capabilities and perceptually misaligned evaluation metrics, leading
to limited applicability and ineffective measurement. In this paper, we propose
\textit{Uni-Layout}, a novel framework that achieves unified generation,
human-mimicking evaluation and alignment between the two. For universal
generation, we incorporate various layout tasks into a single taxonomy and
develop a unified generator that handles background or element contents
constrained tasks via natural language prompts. To introduce human feedback for
the effective evaluation of layouts, we build \textit{Layout-HF100k}, the first
large-scale human feedback dataset with 100,000 expertly annotated layouts.
Based on \textit{Layout-HF100k}, we introduce a human-mimicking evaluator that
integrates visual and geometric information, employing a Chain-of-Thought
mechanism to conduct qualitative assessments alongside a confidence estimation
module to yield quantitative measurements. For better alignment between the
generator and the evaluator, we integrate them into a cohesive system by
adopting Dynamic-Margin Preference Optimization (DMPO), which dynamically
adjusts margins based on preference strength to better align with human
judgments. Extensive experiments show that \textit{Uni-Layout} significantly
outperforms both task-specific and general-purpose methods. Our code is
publicly available at https://github.com/JD-GenX/Uni-Layout.

</details>


### [75] [Object Affordance Recognition and Grounding via Multi-scale Cross-modal Representation Learning](https://arxiv.org/abs/2508.01184)
*Xinhang Wan,Dongqiang Gou,Xinwang Liu,En Zhu,Xuming He*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的方法，通过学习 affordance-aware 3D 表示和采用阶段式推理策略，从而改进了 affordance grounding 和分类。


<details>
  <summary>Details</summary>
Motivation: 具身人工智能的核心问题是从观察中学习物体操作，就像人类一样。 为了实现这一点，重要的是通过图像等观察结果来定位 3D 物体 affordance 区域（3D affordance grounding）并了解其功能（affordance 分类）。

Method: 该论文提出了一种新颖的方法，该方法学习了 affordance-aware 3D 表示，并采用了一种利用 grounding 和分类任务之间依赖关系的阶段式推理策略。 具体来说，我们首先通过高效融合和多尺度几何特征传播开发了一种跨模态 3D 表示，从而能够在合适的区域尺度上推断出完整的潜在 affordance 区域。 此外，我们采用了一种简单的两阶段预测机制，有效地将 grounding 和分类结合起来，以更好地理解 affordance。

Result: 实验表明了我们方法的有效性，在 affordance grounding 和分类方面均表现出改进的性能。

Conclusion: 该方法在 affordance grounding 和分类方面均表现出改进的性能。

Abstract: A core problem of Embodied AI is to learn object manipulation from
observation, as humans do. To achieve this, it is important to localize 3D
object affordance areas through observation such as images (3D affordance
grounding) and understand their functionalities (affordance classification).
Previous attempts usually tackle these two tasks separately, leading to
inconsistent predictions due to lacking proper modeling of their dependency. In
addition, these methods typically only ground the incomplete affordance areas
depicted in images, failing to predict the full potential affordance areas, and
operate at a fixed scale, resulting in difficulty in coping with affordances
significantly varying in scale with respect to the whole object. To address
these issues, we propose a novel approach that learns an affordance-aware 3D
representation and employs a stage-wise inference strategy leveraging the
dependency between grounding and classification tasks. Specifically, we first
develop a cross-modal 3D representation through efficient fusion and
multi-scale geometric feature propagation, enabling inference of full potential
affordance areas at a suitable regional scale. Moreover, we adopt a simple
two-stage prediction mechanism, effectively coupling grounding and
classification for better affordance understanding. Experiments demonstrate the
effectiveness of our method, showing improved performance in both affordance
grounding and classification.

</details>


### [76] [A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding](https://arxiv.org/abs/2508.01197)
*Zhan Shi,Song Wang,Junbo Chen,Jianke Zhu*

Main category: cs.CV

TL;DR: This paper introduces a 3D occupancy grounding benchmark using the nuScenes dataset and proposes GroundingOcc, a new model that outperforms existing methods by using multi-modal learning and fine-grained voxel-level occupancy annotations.


<details>
  <summary>Details</summary>
Motivation: Existing visual grounding tasks rely on bounding boxes that fail to capture fine-grained details, leading to inaccurate object representations.

Method: The paper proposes GroundingOcc, an end-to-end model for 3D occupancy grounding that combines visual, textual, and point cloud features. It includes a multimodal encoder, an occupancy head, a grounding head, a 2D grounding module, and a depth estimation module.

Result: The paper introduces a new benchmark for 3D occupancy grounding in outdoor scenes based on the nuScenes dataset and demonstrates that the proposed GroundingOcc method outperforms existing baselines.

Conclusion: The proposed GroundingOcc method outperforms existing baselines on 3D occupancy grounding, as demonstrated by experiments on the introduced benchmark.

Abstract: Visual grounding aims to identify objects or regions in a scene based on
natural language descriptions, essential for spatially aware perception in
autonomous driving. However, existing visual grounding tasks typically depend
on bounding boxes that often fail to capture fine-grained details. Not all
voxels within a bounding box are occupied, resulting in inaccurate object
representations. To address this, we introduce a benchmark for 3D occupancy
grounding in challenging outdoor scenes. Built on the nuScenes dataset, it
integrates natural language with voxel-level occupancy annotations, offering
more precise object perception compared to the traditional grounding task.
Moreover, we propose GroundingOcc, an end-to-end model designed for 3D
occupancy grounding through multi-modal learning. It combines visual, textual,
and point cloud features to predict object location and occupancy information
from coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder
for feature extraction, an occupancy head for voxel-wise predictions, and a
grounding head to refine localization. Additionally, a 2D grounding module and
a depth estimation module enhance geometric understanding, thereby boosting
model performance. Extensive experiments on the benchmark demonstrate that our
method outperforms existing baselines on 3D occupancy grounding. The dataset is
available at https://github.com/RONINGOD/GroundingOcc.

</details>


### [77] [Deep Learning for Pavement Condition Evaluation Using Satellite Imagery](https://arxiv.org/abs/2508.01206)
*Prathyush Kumar Reddy Lebaku,Lu Gao,Pan Lu,Jingran Sun*

Main category: cs.CV

TL;DR: 本研究利用深度学习模型分析卫星图像评估路面状况，准确率超过 90%，为未来快速且经济高效地评估路面网络奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统的路面检测方法耗时耗力，因此有必要探索更经济有效的方法来监测和维护这些基础设施。

Method: 利用深度学习模型分析卫星图像评估路面状况

Result: 研究结果表明，准确率超过 90%。

Conclusion: 该研究为未来评估路面网络铺平了一条快速且经济高效的道路。

Abstract: Civil infrastructure systems covers large land areas and needs frequent
inspections to maintain their public service capabilities. The conventional
approaches of manual surveys or vehicle-based automated surveys to assess
infrastructure conditions are often labor-intensive and time-consuming. For
this reason, it is worthwhile to explore more cost-effective methods for
monitoring and maintaining these infrastructures. Fortunately, recent
advancements in satellite systems and image processing algorithms have opened
up new possibilities. Numerous satellite systems have been employed to monitor
infrastructure conditions and identify damages. Due to the improvement in
ground sample distance (GSD), the level of detail that can be captured has
significantly increased. Taking advantage of these technology advancement, this
research investigated to evaluate pavement conditions using deep learning
models for analyzing satellite images. We gathered over 3,000 satellite images
of pavement sections, together with pavement evaluation ratings from TxDOT's
PMIS database. The results of our study show an accuracy rate is exceeding 90%.
This research paves the way for a rapid and cost-effective approach to
evaluating the pavement network in the future.

</details>


### [78] [RoadMamba: A Dual Branch Visual State Space Model for Road Surface Classification](https://arxiv.org/abs/2508.01210)
*Tianze Wang,Zhang Zhang,Chao Yue,Nuoran Li,Chao Sun*

Main category: cs.CV

TL;DR: RoadMamba, a novel Mamba-based architecture, is proposed for road surface classification, achieving state-of-the-art performance by combining local and global perception with DualSSM and DAF.


<details>
  <summary>Details</summary>
Motivation: Existing Mamba architectures lack effective extraction of the local texture of the road surface, hindering their performance in visual road surface classification.

Method: A method called RoadMamba that effectively combines local and global perception using Dual State Space Model (DualSSM) to extract global semantics and local texture, and Dual Attention Fusion (DAF) to decode and fuse dual features. A dual auxiliary loss is also proposed to constrain dual branches.

Result: RoadMamba achieves state-of-the-art performance in road surface classification.

Conclusion: The proposed RoadMamba achieves state-of-the-art performance on a large-scale road surface classification dataset.

Abstract: Acquiring the road surface conditions in advance based on visual technologies
provides effective information for the planning and control system of
autonomous vehicles, thus improving the safety and driving comfort of the
vehicles. Recently, the Mamba architecture based on state-space models has
shown remarkable performance in visual processing tasks, benefiting from the
efficient global receptive field. However, existing Mamba architectures
struggle to achieve state-of-the-art visual road surface classification due to
their lack of effective extraction of the local texture of the road surface. In
this paper, we explore for the first time the potential of visual Mamba
architectures for road surface classification task and propose a method that
effectively combines local and global perception, called RoadMamba.
Specifically, we utilize the Dual State Space Model (DualSSM) to effectively
extract the global semantics and local texture of the road surface and decode
and fuse the dual features through the Dual Attention Fusion (DAF). In
addition, we propose a dual auxiliary loss to explicitly constrain dual
branches, preventing the network from relying only on global semantic
information from the deep large receptive field and ignoring the local texture.
The proposed RoadMamba achieves the state-of-the-art performance in experiments
on a large-scale road surface classification dataset containing 1 million
samples.

</details>


### [79] [StyDeco: Unsupervised Style Transfer with Distilling Priors and Semantic Decoupling](https://arxiv.org/abs/2508.01215)
*Yuanlin Yang,Quanjian Song,Zhexian Gao,Ge Wang,Shanshan Li,Xiaoyan Zhang*

Main category: cs.CV

TL;DR: 提出StyDeco，一种无监督框架，通过学习专门为风格迁移任务量身定制的文本表示来解决此限制。


<details>
  <summary>Details</summary>
Motivation: 扩散模型已成为风格迁移的主导范例，但它们的文本驱动机制受到一个核心限制的阻碍：它将文本描述视为统一的、单片的指导。这种限制忽略了文本描述的非空间性质与视觉风格的空间感知属性之间的语义差距，通常导致风格化过程中语义结构和精细细节的丢失。

Method: 该框架首先采用先验引导数据蒸馏（PGD），这是一种旨在在没有人为监督的情况下提取风格知识的策略。它利用强大的冻结生成模型来自动合成伪配对数据。随后，我们引入了对比语义解耦（CSD），这是一种特定于任务的目标，它使用特定于领域的权重来调整文本编码器。CSD 在语义空间中执行两类聚类，鼓励源表示和目标表示形成不同的聚类。

Result: 在三个经典基准上的大量实验表明，我们的框架优于几种现有的方法。

Conclusion: 该框架在风格保真度和结构保持方面优于现有方法，展示了其在语义保留的风格迁移中的有效性。此外，该框架支持独特的去风格化过程，进一步展示了其可扩展性。

Abstract: Diffusion models have emerged as the dominant paradigm for style transfer,
but their text-driven mechanism is hindered by a core limitation: it treats
textual descriptions as uniform, monolithic guidance. This limitation overlooks
the semantic gap between the non-spatial nature of textual descriptions and the
spatially-aware attributes of visual style, often leading to the loss of
semantic structure and fine-grained details during stylization. In this paper,
we propose StyDeco, an unsupervised framework that resolves this limitation by
learning text representations specifically tailored for the style transfer
task. Our framework first employs Prior-Guided Data Distillation (PGD), a
strategy designed to distill stylistic knowledge without human supervision. It
leverages a powerful frozen generative model to automatically synthesize
pseudo-paired data. Subsequently, we introduce Contrastive Semantic Decoupling
(CSD), a task-specific objective that adapts a text encoder using
domain-specific weights. CSD performs a two-class clustering in the semantic
space, encouraging source and target representations to form distinct clusters.
Extensive experiments on three classic benchmarks demonstrate that our
framework outperforms several existing approaches in both stylistic fidelity
and structural preservation, highlighting its effectiveness in style transfer
with semantic preservation. In addition, our framework supports a unique
de-stylization process, further demonstrating its extensibility. Our code is
vailable at https://github.com/QuanjianSong/StyDeco.

</details>


### [80] [Perspective from a Broader Context: Can Room Style Knowledge Help Visual Floorplan Localization?](https://arxiv.org/abs/2508.01216)
*Bolei Chen,Shengsheng Yan,Yongzheng Cui,Jiaxu Kang,Ping Zhong,Jianxin Wang*

Main category: cs.CV

TL;DR: 提出了一种利用更广泛的视觉场景上下文来增强 FLoc 算法的方法，该方法使用具有聚类约束的无监督学习技术来预训练房间判别器，从而提取隐藏的房间类型并区分不同的房间类型，从而指导视觉 FLoc。


<details>
  <summary>Details</summary>
Motivation: 由于建筑物的平面图随时间推移保持一致，并且本质上对视觉外观的变化具有鲁棒性，因此视觉平面图定位（FLoc）受到了研究人员越来越多的关注。然而，作为建筑物布局的紧凑和简约表示，平面图包含许多重复的结构（例如，走廊和角落），因此很容易导致模糊的定位。现有方法要么将希望寄托在匹配平面图中的 2D 结构线索上，要么依赖于 3D 几何约束的视觉预训练，而忽略了视觉图像提供的更丰富的上下文信息。

Method: 提出了一种具有聚类约束的无监督学习技术，以在自行收集的未标记房间图像上预训练房间判别器。

Result: 通过将判别器总结的场景上下文信息注入到 FLoc 算法中，可以有效地利用房间样式知识来指导明确的视觉 FLoc。 

Conclusion: 该方法在两个标准视觉平面图定位基准上进行了充分的对比研究。实验表明，该方法优于现有技术，并在鲁棒性和准确性方面取得了显著的改进。

Abstract: Since a building's floorplan remains consistent over time and is inherently
robust to changes in visual appearance, visual Floorplan Localization (FLoc)
has received increasing attention from researchers. However, as a compact and
minimalist representation of the building's layout, floorplans contain many
repetitive structures (e.g., hallways and corners), thus easily result in
ambiguous localization. Existing methods either pin their hopes on matching 2D
structural cues in floorplans or rely on 3D geometry-constrained visual
pre-trainings, ignoring the richer contextual information provided by visual
images. In this paper, we suggest using broader visual scene context to empower
FLoc algorithms with scene layout priors to eliminate localization uncertainty.
In particular, we propose an unsupervised learning technique with clustering
constraints to pre-train a room discriminator on self-collected unlabeled room
images. Such a discriminator can empirically extract the hidden room type of
the observed image and distinguish it from other room types. By injecting the
scene context information summarized by the discriminator into an FLoc
algorithm, the room style knowledge is effectively exploited to guide definite
visual FLoc. We conducted sufficient comparative studies on two standard visual
Floc benchmarks. Our experiments show that our approach outperforms
state-of-the-art methods and achieves significant improvements in robustness
and accuracy.

</details>


### [81] [MoGaFace: Momentum-Guided and Texture-Aware Gaussian Avatars for Consistent Facial Geometry](https://arxiv.org/abs/2508.01218)
*Yujian Liu,Linlang Cao,Chuang Chen,Fanyu Geng,Dongxu Shen,Peng Cao,Shidang Xu,Xiaoli Liu*

Main category: cs.CV

TL;DR: MoGaFace 是一种新颖的 3D 头部头像建模框架，可在整个高斯渲染过程中不断细化面部几何和纹理属性，从而实现高保真头部头像重建。


<details>
  <summary>Details</summary>
Motivation: 现有的 3D 头部头像重建方法采用两阶段过程，依赖于从面部标志导出的跟踪 FLAME 网格，然后进行基于高斯的渲染。然而，估计的网格和目标图像之间的不对齐通常会导致次优的渲染质量和精细视觉细节的丢失。

Method: MoGaFace，一个新颖的 3D 头部头像建模框架，可在整个高斯渲染过程中不断细化面部几何和纹理属性。引入了动量引导一致几何模块，该模块结合了动量更新的表达库和表达感知校正机制，以确保时间和多视图一致性。此外，提出了潜在纹理注意力，它将紧凑的多视图特征编码为头部感知表示，从而可以通过集成到高斯函数中实现几何感知纹理细化。

Result: MoGaFace 实现了高保真头部头像重建，并显著提高了新视角合成质量，即使在不准确的网格初始化和不受约束的真实世界设置下也是如此。

Conclusion: MoGaFace实现了高保真头部头像重建，并显著提高了新视角合成质量，即使在不准确的网格初始化和不受约束的真实世界设置下也是如此。

Abstract: Existing 3D head avatar reconstruction methods adopt a two-stage process,
relying on tracked FLAME meshes derived from facial landmarks, followed by
Gaussian-based rendering. However, misalignment between the estimated mesh and
target images often leads to suboptimal rendering quality and loss of fine
visual details. In this paper, we present MoGaFace, a novel 3D head avatar
modeling framework that continuously refines facial geometry and texture
attributes throughout the Gaussian rendering process. To address the
misalignment between estimated FLAME meshes and target images, we introduce the
Momentum-Guided Consistent Geometry module, which incorporates a
momentum-updated expression bank and an expression-aware correction mechanism
to ensure temporal and multi-view consistency. Additionally, we propose Latent
Texture Attention, which encodes compact multi-view features into head-aware
representations, enabling geometry-aware texture refinement via integration
into Gaussians. Extensive experiments show that MoGaFace achieves high-fidelity
head avatar reconstruction and significantly improves novel-view synthesis
quality, even under inaccurate mesh initialization and unconstrained real-world
settings.

</details>


### [82] [Eigen Neural Network: Unlocking Generalizable Vision with Eigenbasis](https://arxiv.org/abs/2508.01219)
*Anzhe Cheng,Chenzhong Yin,Mingxi Cheng,Shukai Duan,Shahin Nazarian,Paul Bogdan*

Main category: cs.CV

TL;DR: Introduces Eigen Neural Network (ENN) to address representational flaws in DNNs, achieving superior performance and efficient training.


<details>
  <summary>Details</summary>
Motivation: DNN's tendency to produce disordered weight structures, which harms feature clarity and degrades learning dynamics.

Method: Eigen Neural Network (ENN), a novel architecture that reparameterizes each layer's weights in a layer-shared, learned orthonormal eigenbasis.

Result: ENN consistently outperforms state-of-the-art methods on large-scale image classification benchmarks, including ImageNet, and its superior representations generalize to set a new benchmark in cross-modal image-text retrieval. ENN's principled structure enables a highly efficient, backpropagation-free(BP-free) local learning variant, ENN-$\\ell$. This variant not only resolves BP's procedural bottlenecks to achieve over 2$\times$ training speedup via parallelism, but also, remarkably, surpasses the accuracy of end-to-end backpropagation.

Conclusion: ENN presents a new architectural paradigm that directly remedies the representational deficiencies of BP, leading to enhanced performance and enabling a more efficient, parallelizable training regime.

Abstract: The remarkable success of Deep Neural Networks(DNN) is driven by
gradient-based optimization, yet this process is often undermined by its
tendency to produce disordered weight structures, which harms feature clarity
and degrades learning dynamics. To address this fundamental representational
flaw, we introduced the Eigen Neural Network (ENN), a novel architecture that
reparameterizes each layer's weights in a layer-shared, learned orthonormal
eigenbasis. This design enforces decorrelated, well-aligned weight dynamics
axiomatically, rather than through regularization, leading to more structured
and discriminative feature representations. When integrated with standard BP,
ENN consistently outperforms state-of-the-art methods on large-scale image
classification benchmarks, including ImageNet, and its superior representations
generalize to set a new benchmark in cross-modal image-text retrieval.
Furthermore, ENN's principled structure enables a highly efficient,
backpropagation-free(BP-free) local learning variant, ENN-$\ell$. This variant
not only resolves BP's procedural bottlenecks to achieve over 2$\times$
training speedup via parallelism, but also, remarkably, surpasses the accuracy
of end-to-end backpropagation. ENN thus presents a new architectural paradigm
that directly remedies the representational deficiencies of BP, leading to
enhanced performance and enabling a more efficient, parallelizable training
regime.

</details>


### [83] [ParaRevSNN: A Parallel Reversible Spiking Neural Network for Efficient Training and Inference](https://arxiv.org/abs/2508.01223)
*Changqing Xu,Guoqing Sun,Yi Liu,Xinfang Liao,Yintang Yang*

Main category: cs.CV

TL;DR: ParaRevSNN是一种并行的可逆SNN架构，可减少训练和推理时间，同时保持内存节省的优势。


<details>
  <summary>Details</summary>
Motivation: 可逆脉冲神经网络(RevSNNs)通过在反向传播过程中重建前向激活来实现内存高效训练，但由于严格的顺序计算而存在高延迟。

Method: ParaRevSNN，一种并行的可逆SNN架构，它解耦了可逆块之间的顺序依赖性，同时保留了可逆性。

Result: ParaRevSNN的训练时间减少了35.2%，推理时间减少到18.15%。

Conclusion: ParaRevSNN在CIFAR10、CIFAR100、CIFAR10-DVS和DVS128手势数据集上的实验表明，ParaRevSNN的精度与标准RevSNN相匹配或超过标准RevSNN，同时训练时间减少了35.2%，推理时间减少到18.15%，使其非常适合在资源受限的场景中部署。

Abstract: Reversible Spiking Neural Networks (RevSNNs) enable memory-efficient training
by reconstructing forward activations during backpropagation, but suffer from
high latency due to strictly sequential computation. To overcome this
limitation, we propose ParaRevSNN, a parallel reversible SNN architecture that
decouples sequential dependencies between reversible blocks while preserving
reversibility. This design enables inter-block parallelism, significantly
accelerating training and inference while retaining the memory-saving benefits
of reversibility. Experiments on CIFAR10, CIFAR100, CIFAR10-DVS, and DVS128
Gesture demonstrate that ParaRevSNN matches or exceeds the accuracy of standard
RevSNNs, while reducing training time by up to 35.2\% and inference time to
18.15\%, making it well-suited for deployment in resource-constrained
scenarios.

</details>


### [84] [Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of Vision-Language Models](https://arxiv.org/abs/2508.01225)
*Xinyu Chen,Haotian Zhai,Can Zhang,Xiupeng Shi,Ruirui Li*

Main category: cs.CV

TL;DR: Proposes a Multi-Cache enhanced Prototype-based Test-Time Adaptation (MCP) and MCP++ to achieve state-of-the-art generalization performance.


<details>
  <summary>Details</summary>
Motivation: Existing cache-enhanced TTA methods rely on a low-entropy criterion to select samples for prototype construction, assuming intra-class compactness. However, low-entropy samples may be unreliable under distribution shifts, and the resulting prototypes may not ensure compact intra-class distributions. This study identifies a positive correlation between cache-enhanced performance and intra-class compactness.

Method: a Multi-Cache enhanced Prototype-based Test-Time Adaptation (MCP) featuring three caches: an entropy cache, an align cache, and a negative cache. MCP++ incorporates cross-modal prototype alignment and residual learning, introducing prototype residual fine-tuning.

Result: Comparative and ablation experiments across 15 downstream tasks demonstrate that the proposed method and framework achieve state-of-the-art generalization performance.

Conclusion: The proposed method and framework achieve state-of-the-art generalization performance.

Abstract: In zero-shot setting, test-time adaptation adjusts pre-trained models using
unlabeled data from the test phase to enhance performance on unknown test
distributions. Existing cache-enhanced TTA methods rely on a low-entropy
criterion to select samples for prototype construction, assuming intra-class
compactness. However, low-entropy samples may be unreliable under distribution
shifts, and the resulting prototypes may not ensure compact intra-class
distributions. This study identifies a positive correlation between
cache-enhanced performance and intra-class compactness. Based on this
observation, we propose a Multi-Cache enhanced Prototype-based Test-Time
Adaptation (MCP) featuring three caches: an entropy cache for initializing
prototype representations with low-entropy samples, an align cache for
integrating visual and textual information to achieve compact intra-class
distributions, and a negative cache for prediction calibration using
high-entropy samples. We further developed MCP++, a framework incorporating
cross-modal prototype alignment and residual learning, introducing prototype
residual fine-tuning. Comparative and ablation experiments across 15 downstream
tasks demonstrate that the proposed method and framework achieve
state-of-the-art generalization performance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [85] [Exploring Agentic Artificial Intelligence Systems: Towards a Typological Framework](https://arxiv.org/abs/2508.00844)
*Christopher Wissuchek,Patrick Zschech*

Main category: cs.AI

TL;DR: This paper develops a typology of agentic AI systems, introducing eight dimensions that define their cognitive and environmental agency in an ordinal structure, enabling researchers and practitioners to analyze varying levels of agency in AI systems.


<details>
  <summary>Details</summary>
Motivation: a structured framework is lacking to classify and compare these systems

Method: a multi-phase methodological approach

Result: develops a typology of agentic AI systems, introducing eight dimensions that define their cognitive and environmental agency in an ordinal structure

Conclusion: The typology provides a foundation for assessing current systems and anticipating future developments in agentic AI.

Abstract: Artificial intelligence (AI) systems are evolving beyond passive tools into
autonomous agents capable of reasoning, adapting, and acting with minimal human
intervention. Despite their growing presence, a structured framework is lacking
to classify and compare these systems. This paper develops a typology of
agentic AI systems, introducing eight dimensions that define their cognitive
and environmental agency in an ordinal structure. Using a multi-phase
methodological approach, we construct and refine this typology, which is then
evaluated through a human-AI hybrid approach and further distilled into
constructed types. The framework enables researchers and practitioners to
analyze varying levels of agency in AI systems. By offering a structured
perspective on the progression of AI capabilities, the typology provides a
foundation for assessing current systems and anticipating future developments
in agentic AI.

</details>


### [86] [A Formal Framework for the Definition of 'State': Hierarchical Representation and Meta-Universe Interpretation](https://arxiv.org/abs/2508.00853)
*Kei Itoh*

Main category: cs.AI

TL;DR: Introduces a meta-formal logical framework for defining 'state' to provide a foundation for intelligence, logic, and scientific theory.


<details>
  <summary>Details</summary>
Motivation: to reinforce the theoretical foundation for diverse systems by introducing a mathematically rigorous and unified formal structure for the concept of 'state,' which has long been used without consensus or formal clarity

Method: introducing a mathematically rigorous and unified formal structure for the concept of 'state', a 'hierarchical state grid', and the 'Intermediate Meta-Universe (IMU)'

Result: expands inter-universal theory beyond mathematics to include linguistic translation and agent integration

Conclusion: This paper presents a meta-formal logical framework applicable to the definition of intelligence, formal logic, and scientific theory at large.

Abstract: This study aims to reinforce the theoretical foundation for diverse
systems--including the axiomatic definition of intelligence--by introducing a
mathematically rigorous and unified formal structure for the concept of
'state,' which has long been used without consensus or formal clarity. First, a
'hierarchical state grid' composed of two axes--state depth and mapping
hierarchy--is proposed to provide a unified notational system applicable across
mathematical, physical, and linguistic domains. Next, the 'Intermediate
Meta-Universe (IMU)' is introduced to enable explicit descriptions of definers
(ourselves) and the languages we use, thereby allowing conscious meta-level
operations while avoiding self-reference and logical inconsistency. Building on
this meta-theoretical foundation, this study expands inter-universal theory
beyond mathematics to include linguistic translation and agent integration,
introducing the conceptual division between macrocosm-inter-universal and
microcosm-inter-universal operations for broader expressivity. Through these
contributions, this paper presents a meta-formal logical framework--grounded in
the principle of definition = state--that spans time, language, agents, and
operations, providing a mathematically robust foundation applicable to the
definition of intelligence, formal logic, and scientific theory at large.

</details>


### [87] [AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks](https://arxiv.org/abs/2508.00890)
*Fali Wang,Hui Liu,Zhenwei Dai,Jingying Zeng,Zhiwei Zhang,Zongyu Wu,Chen Luo,Zhen Li,Xianfeng Tang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: 本文研究了多阶段复杂任务中的测试时计算最佳缩放问题，并提出了AgentTTS框架，该框架在搜索效率、鲁棒性和可解释性方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要调查单阶段任务中的TTS；而许多实际问题是多阶段复杂任务，由一系列异构子任务组成，每个子任务都需要具有特定能力的LLM。因此，我们研究了一个新问题：多阶段复杂任务中的测试时计算最佳缩放，旨在选择合适的模型并为每个子任务分配预算，以最大化整体性能。

Method: 提出了AgentTTS，这是一个基于LLM代理的框架，该框架通过与执行环境的迭代反馈驱动的交互来自动搜索计算最佳分配。

Result: 实验结果表明，AgentTTS在搜索效率方面明显优于传统方法和其它基于LLM的基线，并且在不同的训练集大小下表现出更强的鲁棒性以及增强的可解释性。

Conclusion: AgentTTS显著优于传统方法和其他基于LLM的基线方法，在搜索效率方面表现出色，并且对不同的训练集大小表现出更好的鲁棒性和增强的可解释性。

Abstract: Test-time scaling (TTS) enhances the performance of large language models
(LLMs) by allocating additional compute resources during inference. However,
existing research primarily investigates TTS in single-stage tasks; while many
real-world problems are multi-stage complex tasks, composed of a sequence of
heterogeneous subtasks with each subtask requires LLM of specific capability.
Therefore, we study a novel problem: the test-time compute-optimal scaling in
multi-stage complex tasks, aiming to select suitable models and allocate
budgets per subtask to maximize overall performance. TTS in multi-stage tasks
introduces two fundamental challenges: (i) The combinatorial search space of
model and budget allocations, combined with the high cost of inference, makes
brute-force search impractical. (ii) The optimal model and budget allocations
across subtasks are interdependent, increasing the complexity of the
compute-optimal search. To address this gap, we conduct extensive pilot
experiments on four tasks across six datasets, deriving three empirical
insights characterizing the behavior of LLMs in multi-stage complex tasks.
Informed by these insights, we propose AgentTTS, an LLM-agent-based framework
that autonomously searches for compute-optimal allocations through iterative
feedback-driven interactions with the execution environment. Experimental
results demonstrate that AgentTTS significantly outperforms traditional and
other LLM-based baselines in search efficiency, and shows improved robustness
to varying training set sizes and enhanced interpretability.

</details>


### [88] [ff4ERA: A new Fuzzy Framework for Ethical Risk Assessment in AI](https://arxiv.org/abs/2508.00899)
*Abeer Dyoub,Ivan Letteri,Francesca A. Lisi*

Main category: cs.AI

TL;DR: This paper introduces ff4ERA, a fuzzy framework for quantitatively assessing and prioritizing multiple ethical risks in AI systems, offering a robust mathematical approach for collaborative ERA modeling and systematic analysis.


<details>
  <summary>Details</summary>
Motivation: Ethical Risk Assessment (ERA) is crucial for guiding decisions that minimize ethical risks posed by AI systems, but it is hindered by uncertainty, vagueness, and incomplete information, and morality itself is context-dependent and imprecise.

Method: The paper introduces ff4ERA, a fuzzy framework that integrates Fuzzy Logic, the Fuzzy Analytic Hierarchy Process (FAHP), and Certainty Factors (CF) to quantify ethical risks via an Ethical Risk Score (ERS) for each risk type.

Result: ff4ERA yields context-sensitive, ethically meaningful risk scores reflecting both expert input and sensor-based evidence. Risk scores vary consistently with relevant factors while remaining robust to unrelated inputs. Local sensitivity analysis shows predictable, mostly monotonic behavior across perturbations, and global Sobol analysis highlights the dominant influence of expert-defined weights and certainty factors.

Conclusion: The ff4ERA framework produces interpretable, traceable, and risk-aware ethical assessments, enabling what-if analyses and guiding designers in calibrating membership functions and expert judgments for reliable ethical decision support.

Abstract: The emergence of Symbiotic AI (SAI) introduces new challenges to ethical
decision-making as it deepens human-AI collaboration. As symbiosis grows, AI
systems pose greater ethical risks, including harm to human rights and trust.
Ethical Risk Assessment (ERA) thus becomes crucial for guiding decisions that
minimize such risks. However, ERA is hindered by uncertainty, vagueness, and
incomplete information, and morality itself is context-dependent and imprecise.
This motivates the need for a flexible, transparent, yet robust framework for
ERA. Our work supports ethical decision-making by quantitatively assessing and
prioritizing multiple ethical risks so that artificial agents can select
actions aligned with human values and acceptable risk levels. We introduce
ff4ERA, a fuzzy framework that integrates Fuzzy Logic, the Fuzzy Analytic
Hierarchy Process (FAHP), and Certainty Factors (CF) to quantify ethical risks
via an Ethical Risk Score (ERS) for each risk type. The final ERS combines the
FAHP-derived weight, propagated CF, and risk level. The framework offers a
robust mathematical approach for collaborative ERA modeling and systematic,
step-by-step analysis. A case study confirms that ff4ERA yields
context-sensitive, ethically meaningful risk scores reflecting both expert
input and sensor-based evidence. Risk scores vary consistently with relevant
factors while remaining robust to unrelated inputs. Local sensitivity analysis
shows predictable, mostly monotonic behavior across perturbations, and global
Sobol analysis highlights the dominant influence of expert-defined weights and
certainty factors, validating the model design. Overall, the results
demonstrate ff4ERA ability to produce interpretable, traceable, and risk-aware
ethical assessments, enabling what-if analyses and guiding designers in
calibrating membership functions and expert judgments for reliable ethical
decision support.

</details>


### [89] [Multi-turn Natural Language to Graph Query Language Translation](https://arxiv.org/abs/2508.01871)
*Yuanyuan Liang,Lei Pan,Tingyu Xie,Yunshi Lan,Weining Qian*

Main category: cs.AI

TL;DR: This paper introduces a method for creating multi-turn NL2GQL datasets using LLMs to address the limitations of single-turn methods and the lack of suitable datasets. They also propose baseline methods for evaluating multi-turn NL2GQL translation.


<details>
  <summary>Details</summary>
Motivation: Single-turn methods can handle straightforward queries, more complex scenarios often require users to iteratively adjust their queries, investigate the connections between entities, or request additional details across multiple dialogue turns. Research focused on single-turn conversion fails to effectively address multi-turn dialogues and complex context dependencies. Additionally, the scarcity of high-quality multi-turn NL2GQL datasets further hinders the progress of this field.

Method: We propose an automated method for constructing multi-turn NL2GQL datasets based on Large Language Models (LLMs) , and apply this method to develop the MTGQL dataset, which is constructed from a financial market graph database and will be publicly released for future research.

Result: To address this challenge, we propose an automated method for constructing multi-turn NL2GQL datasets based on Large Language Models (LLMs)

Conclusion: We propose three types of baseline methods to assess the effectiveness of multi-turn NL2GQL translation, thereby laying a solid foundation for future research.

Abstract: In recent years, research on transforming natural language into graph query
language (NL2GQL) has been increasing. Most existing methods focus on
single-turn transformation from NL to GQL. In practical applications, user
interactions with graph databases are typically multi-turn, dynamic, and
context-dependent. While single-turn methods can handle straightforward
queries, more complex scenarios often require users to iteratively adjust their
queries, investigate the connections between entities, or request additional
details across multiple dialogue turns. Research focused on single-turn
conversion fails to effectively address multi-turn dialogues and complex
context dependencies. Additionally, the scarcity of high-quality multi-turn
NL2GQL datasets further hinders the progress of this field. To address this
challenge, we propose an automated method for constructing multi-turn NL2GQL
datasets based on Large Language Models (LLMs) , and apply this method to
develop the MTGQL dataset, which is constructed from a financial market graph
database and will be publicly released for future research. Moreover, we
propose three types of baseline methods to assess the effectiveness of
multi-turn NL2GQL translation, thereby laying a solid foundation for future
research.

</details>


### [90] [An analysis of AI Decision under Risk: Prospect theory emerges in Large Language Models](https://arxiv.org/abs/2508.00902)
*Kenneth Payne*

Main category: cs.AI

TL;DR: 大型语言模型在风险决策中表现出与人类相似的前景理论偏见，且受情境影响。


<details>
  <summary>Details</summary>
Motivation: 在不确定性下，风险判断是决策的关键。人类以一种有别于数学理性主义的独特方式进行风险判断。他们通过实验证明，当人类觉得自己有失去某些东西的风险时，比他们可能获得收益时，会承担更多的风险。

Method: 使用大型语言模型，包括当今最先进的思维链“推理器”测试卡尼曼和特沃斯基的“前景理论”。

Result: 前景理论可以预测这些模型在一系列情景中如何进行风险决策。军事场景比民用环境产生更大的“框架效应”。

Conclusion: 语言模型能够捕捉到人类的启发式和偏见，但这些偏见是不均衡的，情境会激活这些局部偏见。

Abstract: Judgment of risk is key to decision-making under uncertainty. As Daniel
Kahneman and Amos Tversky famously discovered, humans do so in a distinctive
way that departs from mathematical rationalism. Specifically, they demonstrated
experimentally that humans accept more risk when they feel themselves at risk
of losing something than when they might gain. I report the first tests of
Kahneman and Tversky's landmark 'prospect theory' with Large Language Models,
including today's state of the art chain-of-thought 'reasoners'.
  In common with humans, I find that prospect theory often anticipates how
these models approach risky decisions across a range of scenarios. I also
demonstrate that context is key to explaining much of the variance in risk
appetite. The 'frame' through which risk is apprehended appears to be embedded
within the language of the scenarios tackled by the models. Specifically, I
find that military scenarios generate far larger 'framing effects' than do
civilian settings, ceteris paribus. My research suggests, therefore, that
language models the world, capturing our human heuristics and biases. But also
that these biases are uneven - the idea of a 'frame' is richer than simple
gains and losses. Wittgenstein's notion of 'language games' explains the
contingent, localised biases activated by these scenarios. Finally, I use my
findings to reframe the ongoing debate about reasoning and memorisation in
LLMs.

</details>


### [91] [Knowledge Editing for Multi-Hop Question Answering Using Semantic Analysis](https://arxiv.org/abs/2508.00914)
*Dominic Simon,Rickard Ewetz*

Main category: cs.AI

TL;DR: This paper presents CHECK, a knowledge editor that uses semantic analysis to improve the accuracy of multi-hop question answering in large language models.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge editors struggle with handling tasks that require compositional reasoning such as multi-hop question answering (MQA). Existing knowledge editors leverage decompositional techniques that result in illogical reasoning processes.

Method: The proposed framework semantically analyzes reasoning chains before execution, revising chains with semantic errors to ensure consistency through logic optimization and re-prompting the LLM model at a higher temperature.

Result: CHECK achieves an average 22.8% improved MQA accuracy against five state-of-the-art frameworks on four datasets.

Conclusion: The paper introduces CHECK, a knowledge editor for multi-hop question answering (MQA) based on semantic analysis, which improves MQA accuracy by 22.8% compared to state-of-the-art frameworks.

Abstract: Large Language Models (LLMs) require lightweight avenues of updating stored
information that has fallen out of date. Knowledge Editing (KE) approaches have
been successful in updating model knowledge for simple factual queries but
struggle with handling tasks that require compositional reasoning such as
multi-hop question answering (MQA). We observe that existing knowledge editors
leverage decompositional techniques that result in illogical reasoning
processes. In this paper, we propose a knowledge editor for MQA based on
semantic analysis called CHECK. Our framework is based on insights from an
analogy between compilers and reasoning using LLMs. Similar to how source code
is first compiled before being executed, we propose to semantically analyze
reasoning chains before executing the chains to answer questions. Reasoning
chains with semantic errors are revised to ensure consistency through logic
optimization and re-prompting the LLM model at a higher temperature. We
evaluate the effectiveness of CHECK against five state-of-the-art frameworks on
four datasets and achieve an average 22.8% improved MQA accuracy.

</details>


### [92] [Cooperative Perception: A Resource-Efficient Framework for Multi-Drone 3D Scene Reconstruction Using Federated Diffusion and NeRF](https://arxiv.org/abs/2508.00967)
*Massoud Pourmandi*

Main category: cs.AI

TL;DR: 提出了一种新的无人机群感知系统，通过联邦学习和轻量级语义提取来实现高效、实时的场景重建，解决了计算和通信限制，并在保持隐私和可扩展性的同时，改进了协作场景理解。


<details>
  <summary>Details</summary>
Motivation: 解决与计算限制、低带宽通信和实时场景重建相关的问题

Method: 联邦学习共享扩散模型和 YOLOv12 轻量级语义提取以及局部 NeRF 更新

Result: 实现高效的多智能体 3D/4D 场景合成，同时保持隐私和可扩展性。改进了协作场景理解，同时添加了语义感知压缩协议。

Conclusion: 该论文提出了一种创新的无人机群感知系统，旨在解决与计算限制、低带宽通信和实时场景重建相关的问题。该方法通过联邦学习共享扩散模型和 YOLOv12 轻量级语义提取以及局部 NeRF 更新来实现高效的多智能体 3D/4D 场景合成，同时保持隐私和可扩展性。通过仿真和潜在的无人机测试平台上的实际部署来验证该方法，将其定位为自主系统多智能体 AI 的颠覆性进步。

Abstract: The proposal introduces an innovative drone swarm perception system that aims
to solve problems related to computational limitations and low-bandwidth
communication, and real-time scene reconstruction. The framework enables
efficient multi-agent 3D/4D scene synthesis through federated learning of
shared diffusion model and YOLOv12 lightweight semantic extraction and local
NeRF updates while maintaining privacy and scalability. The framework redesigns
generative diffusion models for joint scene reconstruction, and improves
cooperative scene understanding, while adding semantic-aware compression
protocols. The approach can be validated through simulations and potential
real-world deployment on drone testbeds, positioning it as a disruptive
advancement in multi-agent AI for autonomous systems.

</details>


### [93] [AutoEDA: Enabling EDA Flow Automation through Microservice-Based LLM Agents](https://arxiv.org/abs/2508.01012)
*Yiyi Lu,Hoi Ian Au,Junyao Zhang,Jingyu Pan,Yiting Wang,Ang Li,Jianyi Zhang,Yiran Chen*

Main category: cs.AI

TL;DR: AutoEDA是一个用于EDA自动化的框架，它利用并行学习、结构化提示工程、智能参数提取和任务分解来提高自动化准确性和效率，并已开源。


<details>
  <summary>Details</summary>
Motivation: 现代电子设计自动化（EDA）工作流程，特别是RTL-to-GDSII流程，需要大量的手动脚本编写，并展示了许多特定于工具的交互，这限制了可扩展性和效率。现有的LLM解决方案需要昂贵的微调，并且不包含用于集成和评估的标准化框架。

Method: AutoEDA框架利用模型上下文协议（MCP）进行并行学习，实现跨整个RTL-to-GDSII流程的标准化和可扩展的自然语言体验。

Result: 在五个先前整理的基准测试上的实验结果表明，与现有方法相比，自动化准确性和效率以及脚本质量有所提高。

Conclusion: AutoEDA通过结构化提示工程、智能参数提取和任务分解，以及扩展的CodeBLEU指标来评估TCL脚本的质量，从而提高了自动化准确性和效率，以及脚本质量。AutoEDA已开源发布。

Abstract: Modern Electronic Design Automation (EDA) workflows, especially the
RTL-to-GDSII flow, require heavily manual scripting and demonstrate a multitude
of tool-specific interactions which limits scalability and efficiency. While
LLMs introduces strides for automation, existing LLM solutions require
expensive fine-tuning and do not contain standardized frameworks for
integration and evaluation. We introduce AutoEDA, a framework for EDA
automation that leverages paralleled learning through the Model Context
Protocol (MCP) specific for standardized and scalable natural language
experience across the entire RTL-to-GDSII flow. AutoEDA limits fine-tuning
through structured prompt engineering, implements intelligent parameter
extraction and task decomposition, and provides an extended CodeBLEU metric to
evaluate the quality of TCL scripts. Results from experiments over five
previously curated benchmarks show improvements in automation accuracy and
efficiency, as well as script quality when compared to existing methods.
AutoEDA is released open-sourced to support reproducibility and the EDA
community. Available at: https://github.com/AndyLu666/MCP-EDA-Server

</details>


### [94] [CADDesigner: Conceptual Design of CAD Models Based on General-Purpose Agent](https://arxiv.org/abs/2508.01031)
*Jingzhe Ni,Xiaolong Yin,Xintong Li,Xingyu Lu,Ji Wei,Ruofeng Tong,Min Tang,Peng Du*

Main category: cs.AI

TL;DR: 提出了一种基于大型语言模型(llm)的CAD概念设计代理，该代理接受抽象文本描述和手绘草图作为输入，通过综合需求分析与用户进行交互式对话，以完善和明确设计需求。


<details>
  <summary>Details</summary>
Motivation: 为了降低CAD的准入门槛并提高设计效率。

Method: 该Agent基于一种新颖的上下文无关命令式范式(CIP)，生成高质量的CAD建模代码，并在生成过程中结合迭代视觉反馈来提高模型质量。

Result: 实验结果表明，该方法在CAD代码生成方面实现了最先进的性能。

Conclusion: 该方法在CAD代码生成方面实现了最先进的性能。

Abstract: Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing
but typically requires a high level of expertise from designers. To lower the
entry barrier and improve design efficiency, we present an agent for CAD
conceptual design powered by large language models (LLMs). The agent accepts
both abstract textual descriptions and freehand sketches as input, engaging in
interactive dialogue with users to refine and clarify design requirements
through comprehensive requirement analysis. Built upon a novel
Context-Independent Imperative Paradigm (CIP), the agent generates high-quality
CAD modeling code. During the generation process, the agent incorporates
iterative visual feedback to improve model quality. Generated design cases are
stored in a structured knowledge base, enabling continuous improvement of the
agent's code generation capabilities. Experimental results demonstrate that our
method achieves state-of-the-art performance in CAD code generation.

</details>


### [95] [REACT: A Real-Time Edge-AI Based V2X Framework for Accident Avoidance in Autonomous Driving System](https://arxiv.org/abs/2508.01057)
*Fengze Yang,Bo Yu,Yang Zhou,Xuewen Luo,Zhengzhong Tu,Chenxi Liu*

Main category: cs.AI

TL;DR: REACT是一个基于轻量级VLM的实时V2X集成轨迹优化框架，它通过边缘自适应策略和多模态输入处理，在自动驾驶中实现了最先进的性能和安全性。


<details>
  <summary>Details</summary>
Motivation: 人为错误造成的碰撞是多车碰撞的最常见类型，这突出了自动驾驶(AD)系统通过车对万物(V2X)通信利用协同感知的迫切需求。然而，目前基于transformer的V2X框架存在泛化能力有限、上下文推理肤浅以及依赖于单模态输入等问题。视觉语言模型(VLM)提供了增强的推理和多模态集成，但通常无法满足安全关键应用中的实时性能要求。

Method: 基于微调的轻量级VLM，REACT集成了多个专用模块，将多模态输入处理为优化的、具有风险意识的轨迹。为了确保边缘设备的实时性能，REACT集成了边缘自适应策略，以降低模型复杂性并加速推理。

Result: 在DeepAccident基准测试中，REACT达到了最先进的性能，碰撞率降低了77%，视频全景质量(VPQ)提高了48.2%，在Jetson AGX Orin上的推理延迟为0.57秒。

Conclusion: 轻量级视觉语言模型(VLM)在实时边缘协同规划中是可行的，并展示了语言引导的上下文推理在提高自动驾驶安全性和响应能力方面的潜力。

Abstract: Collisions caused by human error are the most common type of multi-vehicle
crash, highlighting the critical need for autonomous driving (AD) systems to
leverage cooperative perception through Vehicle-to-Everything (V2X)
communication. This capability extends situational awareness beyond the
limitations of onboard sensors. However, current transformer-based V2X
frameworks suffer from limited generalization, shallow contextual reasoning,
and reliance on mono-modal inputs. Vision-Language Models (VLMs) offer enhanced
reasoning and multimodal integration but typically fall short of real-time
performance requirements in safety-critical applications. This paper presents
REACT, a real-time, V2X-integrated trajectory optimization framework built upon
a fine-tuned lightweight VLM. REACT integrates a set of specialized modules
that process multimodal inputs into optimized, risk-aware trajectories. To
ensure real-time performance on edge devices, REACT incorporates edge
adaptation strategies that reduce model complexity and accelerate inference.
Evaluated on the DeepAccident benchmark, REACT achieves state-of-the-art
performance, a 77% collision rate reduction, a 48.2% Video Panoptic Quality
(VPQ), and a 0.57-second inference latency on the Jetson AGX Orin. Ablation
studies validate the contribution of each input, module, and edge adaptation
strategy. These results demonstrate the feasibility of lightweight VLMs for
real-time edge-based cooperative planning and showcase the potential of
language-guided contextual reasoning to improve safety and responsiveness in
autonomous driving.

</details>


### [96] [gpuRDF2vec -- Scalable GPU-based RDF2vec](https://arxiv.org/abs/2508.01073)
*Martin Böckling,Heiko Paulheim*

Main category: cs.AI

TL;DR: gpuRDF2vec is a fast, open-source library for generating knowledge graph embeddings at web scale using GPUs.


<details>
  <summary>Details</summary>
Motivation: Generating Knowledge Graph (KG) embeddings at web scale remains challenging.

Method: We present gpuRDF2vec, an open source library that harnesses modern GPUs and supports multi-node execution to accelerate every stage of the RDF2vec pipeline.

Result: gpuRDF2vec achieves up to a substantial speedup over the currently fastest alternative, i.e., jRDF2vec. In a single-node setup, our walk-extraction phase alone outperforms pyRDF2vec, SparkKGML, and jRDF2vec by a substantial margin using random walks on large/ dense graphs, and scales very well to longer walks, which typically lead to better quality embeddings.

Conclusion: gpuRDF2vec enables practitioners and researchers to train high-quality KG embeddings on large-scale graphs within practical time budgets and builds on top of Pytorch Lightning for the scalable word2vec implementation.

Abstract: Generating Knowledge Graph (KG) embeddings at web scale remains challenging.
Among existing techniques, RDF2vec combines effectiveness with strong
scalability. We present gpuRDF2vec, an open source library that harnesses
modern GPUs and supports multi-node execution to accelerate every stage of the
RDF2vec pipeline. Extensive experiments on both synthetically generated graphs
and real-world benchmarks show that gpuRDF2vec achieves up to a substantial
speedup over the currently fastest alternative, i.e., jRDF2vec. In a
single-node setup, our walk-extraction phase alone outperforms pyRDF2vec,
SparkKGML, and jRDF2vec by a substantial margin using random walks on large/
dense graphs, and scales very well to longer walks, which typically lead to
better quality embeddings. Our implementation of gpuRDF2vec enables
practitioners and researchers to train high-quality KG embeddings on
large-scale graphs within practical time budgets and builds on top of Pytorch
Lightning for the scalable word2vec implementation.

</details>


### [97] [Multispin Physics of AI Tipping Points and Hallucinations](https://arxiv.org/abs/2508.01097)
*Neil F. Johnson,Frank Yingjie Huo*

Main category: cs.AI

TL;DR: 生成式 AI 的输出可能在中途出错，导致巨大损失。该研究揭示了 AI 模型中隐藏的 tipping instability，并提供了一个公式来预测 tipping point。


<details>
  <summary>Details</summary>
Motivation: ChatGPT 等生成式 AI 的输出可能具有重复性和偏差，更令人担忧的是，这种输出可能会在用户没有注意到的情况下，在中途从好（正确）变为坏（误导或错误）。据报道，仅在 2024 年，这造成了 670 亿美元的损失和数起死亡。

Method: 通过建立一个到多自旋热力系统的数学映射

Result: 推导出一个简单但基本上精确的 tipping point 公式，该公式直接显示了用户 prompt 选择和 AI 训练偏差的影响。结果表明，输出 tipping 可能会被 AI 的多层架构放大。

Conclusion: 揭示了 AI 模型中隐藏的 tipping instability，并提供了一个公式来预测 tipping point，这有助于提高 AI 的透明度、可解释性和性能，并为量化用户的 AI 风险和法律责任开辟了道路。

Abstract: Output from generative AI such as ChatGPT, can be repetitive and biased. But
more worrying is that this output can mysteriously tip mid-response from good
(correct) to bad (misleading or wrong) without the user noticing. In 2024
alone, this reportedly caused $67 billion in losses and several deaths.
Establishing a mathematical mapping to a multispin thermal system, we reveal a
hidden tipping instability at the scale of the AI's 'atom' (basic Attention
head). We derive a simple but essentially exact formula for this tipping point
which shows directly the impact of a user's prompt choice and the AI's training
bias. We then show how the output tipping can get amplified by the AI's
multilayer architecture. As well as helping improve AI transparency,
explainability and performance, our results open a path to quantifying users'
AI risk and legal liabilities.

</details>


### [98] [Platonic Representations for Poverty Mapping: Unified Vision-Language Codes or Agent-Induced Novelty?](https://arxiv.org/abs/2508.01109)
*Satiyabooshan Murugaboopathy,Connor T. Jerzak,Adel Daoud*

Main category: cs.AI

TL;DR: Using satellite imagery and internet text, a multimodal framework is developed to predict household wealth. Fusing vision and language data improves prediction accuracy. A new dataset is released.


<details>
  <summary>Details</summary>
Motivation: Investigating whether socio-economic indicators leave recoverable imprints in satellite imagery and Internet-sourced text.

Method: A multimodal framework predicting household wealth through five pipelines: (i) vision model on satellite images, (ii) LLM using only location/year, (iii) AI agent searching/synthesizing web text, (iv) joint image-text encoder, (v) ensemble of all signals.

Result: Fusing vision and agent/LLM text outperforms vision-only baselines in wealth prediction. There is partial representational convergence between vision/language modalities. LLM-internal knowledge is more effective than agent-retrieved text. A large-scale multimodal dataset is released.

Conclusion: Fusing vision and agent/LLM text outperforms vision-only baselines in wealth prediction. There is partial representational convergence between vision/language modalities. A large-scale multimodal dataset is released.

Abstract: We investigate whether socio-economic indicators like household wealth leave
recoverable imprints in satellite imagery (capturing physical features) and
Internet-sourced text (reflecting historical/economic narratives). Using
Demographic and Health Survey (DHS) data from African neighborhoods, we pair
Landsat images with LLM-generated textual descriptions conditioned on
location/year and text retrieved by an AI search agent from web sources. We
develop a multimodal framework predicting household wealth (International
Wealth Index) through five pipelines: (i) vision model on satellite images,
(ii) LLM using only location/year, (iii) AI agent searching/synthesizing web
text, (iv) joint image-text encoder, (v) ensemble of all signals. Our framework
yields three contributions. First, fusing vision and agent/LLM text outperforms
vision-only baselines in wealth prediction (e.g., R-squared of 0.77 vs. 0.63 on
out-of-sample splits), with LLM-internal knowledge proving more effective than
agent-retrieved text, improving robustness to out-of-country and out-of-time
generalization. Second, we find partial representational convergence: fused
embeddings from vision/language modalities correlate moderately (median cosine
similarity of 0.60 after alignment), suggesting a shared latent code of
material well-being while retaining complementary details, consistent with the
Platonic Representation Hypothesis. Although LLM-only text outperforms
agent-retrieved data, challenging our Agent-Induced Novelty Hypothesis, modest
gains from combining agent data in some splits weakly support the notion that
agent-gathered information introduces unique representational structures not
fully captured by static LLM knowledge. Third, we release a large-scale
multimodal dataset comprising more than 60,000 DHS clusters linked to satellite
images, LLM-generated descriptions, and agent-retrieved texts.

</details>


### [99] [H2C: Hippocampal Circuit-inspired Continual Learning for Lifelong Trajectory Prediction in Autonomous Driving](https://arxiv.org/abs/2508.01158)
*Yunlong Lin,Zirui Li,Guodong Du,Xiaocong Zhao,Cheng Gong,Xinwei Wang,Chao Lu,Jianwei Gong*

Main category: cs.AI

TL;DR: 提出了一种受海马回路启发的持续学习方法（H2C），用于轨迹预测，可有效减少灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的轨迹预测方法存在灾难性遗忘问题，即适应新的分布可能导致先前学习的分布的性能显著下降。这种无法保留学习知识的能力限制了它们在现实世界中的适用性，在现实世界中，AD系统需要在具有动态分布的各种场景中运行。正如神经科学所揭示的那样，海马回路在记忆重放中起着至关重要的作用，可以有效地基于有限的资源重建学习到的知识。

Method: 提出了一种受海马回路启发的持续学习方法（H2C），用于跨各种场景的轨迹预测。H2C通过选择性地回忆一小部分学习过的样本来保留先前的知识。首先，开发了两种互补的策略来选择代表学习知识的子集。具体来说，一种策略是最大化样本间的多样性来代表独特的知识，另一种策略是通过等概率抽样来估计整体知识。然后，H2C通过这些选定的样本计算的记忆重放损失函数进行更新，以在学习新数据的同时保留知识。

Result: H2C以无任务的方式平均减少了22.71%的DL基线的灾难性遗忘，而无需依赖手动通知的分布偏移。

Conclusion: H2C通过选择性地回忆一小部分学习过的样本来保留先前的知识，并在各种场景的INTERACTION数据集上进行了评估，实验结果表明，H2C以无任务的方式平均减少了22.71%的DL基线的灾难性遗忘，而无需依赖手动通知的分布偏移。

Abstract: Deep learning (DL) has shown state-of-the-art performance in trajectory
prediction, which is critical to safe navigation in autonomous driving (AD).
However, most DL-based methods suffer from catastrophic forgetting, where
adapting to a new distribution may cause significant performance degradation in
previously learned ones. Such inability to retain learned knowledge limits
their applicability in the real world, where AD systems need to operate across
varying scenarios with dynamic distributions. As revealed by neuroscience, the
hippocampal circuit plays a crucial role in memory replay, effectively
reconstructing learned knowledge based on limited resources. Inspired by this,
we propose a hippocampal circuit-inspired continual learning method (H2C) for
trajectory prediction across varying scenarios. H2C retains prior knowledge by
selectively recalling a small subset of learned samples. First, two
complementary strategies are developed to select the subset to represent
learned knowledge. Specifically, one strategy maximizes inter-sample diversity
to represent the distinctive knowledge, and the other estimates the overall
knowledge by equiprobable sampling. Then, H2C updates via a memory replay loss
function calculated by these selected samples to retain knowledge while
learning new data. Experiments based on various scenarios from the INTERACTION
dataset are designed to evaluate H2C. Experimental results show that H2C
reduces catastrophic forgetting of DL baselines by 22.71% on average in a
task-free manner, without relying on manually informed distributional shifts.
The implementation is available at https://github.com/BIT-Jack/H2C-lifelong.

</details>


### [100] [Benchmarking and Bridging Emotion Conflicts for Multimodal Emotion Reasoning](https://arxiv.org/abs/2508.01181)
*Zhiyuan Han,Beier Zhu,Yanlong Xu,Peipei Song,Xun Yang*

Main category: cs.AI

TL;DR: 论文提出了CA-MER基准测试来评估MLLM在情感冲突下的表现，发现现有模型过度依赖音频信号。为解决这个问题，提出了MoSEAR框架，通过模态特定专家和注意力重新分配机制来平衡模态集成，并在多个基准测试中取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大型语言模型(MLLM)在多模态情感推理中表现出色，但通常忽略涉及情感冲突的场景，即来自不同模态的情感线索不一致的情况。

Method: 提出了MoSEAR，一个参数高效的框架，包含两个模块：(1)带有正则化门控机制的模态特定专家MoSE，减少微调头中的模态偏差；(2)注意力重新分配机制AR，在推理过程中重新平衡冻结骨干网络中的模态贡献。

Result: 对CA-MER的评估表明，当前最先进的情感MLLM在情感冲突期间系统性地过度依赖音频信号，忽略了来自视觉模态的关键线索。MoSEAR缓解了情感冲突，提高了在一致样本上的性能，且不会在音频和视觉模态之间产生权衡。

Conclusion: 提出的MoSEAR框架在多个基准测试中实现了最先进的性能，尤其是在模态冲突条件下。

Abstract: Despite their strong performance in multimodal emotion reasoning, existing
Multimodal Large Language Models (MLLMs) often overlook the scenarios involving
emotion conflicts, where emotional cues from different modalities are
inconsistent. To fill this gap, we first introduce CA-MER, a new benchmark
designed to examine MLLMs under realistic emotion conflicts. It consists of
three subsets: video-aligned, audio-aligned, and consistent, where only one or
all modalities reflect the true emotion. However, evaluations on our CA-MER
reveal that current state-of-the-art emotion MLLMs systematically over-rely on
audio signal during emotion conflicts, neglecting critical cues from visual
modality. To mitigate this bias, we propose MoSEAR, a parameter-efficient
framework that promotes balanced modality integration. MoSEAR consists of two
modules: (1)MoSE, modality-specific experts with a regularized gating mechanism
that reduces modality bias in the fine-tuning heads; and (2)AR, an attention
reallocation mechanism that rebalances modality contributions in frozen
backbones during inference. Our framework offers two key advantages: it
mitigates emotion conflicts and improves performance on consistent
samples-without incurring a trade-off between audio and visual modalities.
Experiments on multiple benchmarks-including MER2023, EMER, DFEW, and our
CA-MER-demonstrate that MoSEAR achieves state-of-the-art performance,
particularly under modality conflict conditions.

</details>


### [101] [A Survey on Agent Workflow -- Status and Future](https://arxiv.org/abs/2508.01186)
*Chaojia Yu,Zihan Cheng,Hanwen Cui,Yishuo Gao,Zexu Luo,Yijin Wang,Hangbin Zheng,Yong Zhao*

Main category: cs.AI

TL;DR: This survey reviews agent workflow systems, classifying them by functional capabilities and architectural features, and highlights challenges, trends, and open problems.


<details>
  <summary>Details</summary>
Motivation: Autonomous agents have emerged as a powerful paradigm for achieving general intelligence. As agent systems grow in complexity, agent workflows have become central to enabling scalable, controllable, and secure AI behaviors.

Method: This survey classifies existing systems along two key dimensions: functional capabilities and architectural features. By comparing over 20 representative systems, it highlights common patterns, potential technical challenges, and emerging trends. It further address concerns related to workflow optimization strategies and security.

Result: This survey provides a comprehensive review of agent workflow systems, spanning academic frameworks and industrial implementations.

Conclusion: This survey outlines open problems such as standardization and multimodal integration, offering insights for future research at the intersection of agent design, workflow infrastructure, and safe automation.

Abstract: In the age of large language models (LLMs), autonomous agents have emerged as
a powerful paradigm for achieving general intelligence. These agents
dynamically leverage tools, memory, and reasoning capabilities to accomplish
user-defined goals. As agent systems grow in complexity, agent
workflows-structured orchestration frameworks-have become central to enabling
scalable, controllable, and secure AI behaviors. This survey provides a
comprehensive review of agent workflow systems, spanning academic frameworks
and industrial implementations. We classify existing systems along two key
dimensions: functional capabilities (e.g., planning, multi-agent collaboration,
external API integration) and architectural features (e.g., agent roles,
orchestration flows, specification languages). By comparing over 20
representative systems, we highlight common patterns, potential technical
challenges, and emerging trends. We further address concerns related to
workflow optimization strategies and security. Finally, we outline open
problems such as standardization and multimodal integration, offering insights
for future research at the intersection of agent design, workflow
infrastructure, and safe automation.

</details>


### [102] [Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens](https://arxiv.org/abs/2508.01191)
*Chengshuai Zhao,Zhen Tan,Pingchuan Ma,Dawei Li,Bohan Jiang,Yancheng Wang,Yingzhen Yang,Huan Liu*

Main category: cs.AI

TL;DR: CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions.


<details>
  <summary>Details</summary>
Motivation: some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating us to explore further. In this paper, we study CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries.

Method: design DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. dissect CoT reasoning via three dimensions: task, length, and format.

Result: CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions.

Conclusion: CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions. This work offers a deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning.

Abstract: Chain-of-Thought (CoT) prompting has been shown to improve Large Language
Model (LLM) performance on various tasks. With this approach, LLMs appear to
produce human-like reasoning steps before providing answers (a.k.a., CoT
reasoning), which often leads to the perception that they engage in deliberate
inferential processes. However, some initial findings suggest that CoT
reasoning may be more superficial than it appears, motivating us to explore
further. In this paper, we study CoT reasoning via a data distribution lens and
investigate if CoT reasoning reflects a structured inductive bias learned from
in-distribution data, allowing the model to conditionally generate reasoning
paths that approximate those seen during training. Thus, its effectiveness is
fundamentally bounded by the degree of distribution discrepancy between the
training data and the test queries. With this lens, we dissect CoT reasoning
via three dimensions: task, length, and format. To investigate each dimension,
we design DataAlchemy, an isolated and controlled environment to train LLMs
from scratch and systematically probe them under various distribution
conditions. Our results reveal that CoT reasoning is a brittle mirage that
vanishes when it is pushed beyond training distributions. This work offers a
deeper understanding of why and when CoT reasoning fails, emphasizing the
ongoing challenge of achieving genuine and generalizable reasoning.

</details>


### [103] [Importance Sampling is All You Need: Predict LLM's performance on new benchmark by reusing existing benchmark](https://arxiv.org/abs/2508.01203)
*Junjie Shi,Wei Ma,Shi Ying,Lingxiao Jiang,Yang liu,Bo Du*

Main category: cs.AI

TL;DR: BIS 是一种新的评估框架，它通过分析提示分布来预测 LLM 在代码生成任务中的性能，从而降低了基准测试的成本和工作量。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试面临两个主要挑战：(1) 构建高质量测试套件和参考解决方案的成本不断升级，(2) 数据污染的风险日益增加，这破坏了基于基准测试的评估的可靠性。

Method: 提出了一种以提示为中心的评估框架 BIS，该框架通过分析提示分布来预测 LLM 在代码生成任务中的性能，而不是执行生成的代码。该方法基于重要性抽样理论，并使用重要性加权自动编码器实现，通过对现有带注释的基准测试中的样本进行重新加权来估计新基准测试的性能。为了稳定估计，引入了权重截断策略，并计算拟合分布的边际期望。

Result: BIS 框架在代码正确性评分方面实现了 1.1% 的平均绝对预测误差，最佳和最差情况下的误差分别为 0.3% 和 1.9%。它还可以很好地推广到其他指标，pass@1 的平均绝对误差为 2.15%。

Conclusion: BIS 框架能够以低成本和低工作量的方式显著降低 LLM 在代码相关任务中的基准测试成本和工作量，并且具有可靠性和广泛的适用性。

Abstract: With the rapid advancement of large language models , code generation has
become a key benchmark for evaluating LLM capabilities. However, existing
benchmarks face two major challenges: (1) the escalating cost of constructing
high-quality test suites and reference solutions, and (2) the increasing risk
of data contamination, which undermines the reliability of benchmark-based
evaluations. In this paper, we propose BIS, a prompt-centric evaluation
framework that enables ground-truth-free prediction of LLM performance on code
generation tasks. Rather than executing generated code, BIS estimates
performance metrics by analyzing the prompt distribution alone. Built on
importance sampling theory and implemented using Importance Weighted
Autoencoders, our method reweights samples from existing annotated benchmarks
to estimate performance on new, unseen benchmarks. To stabilize the estimation,
we introduce weight truncation strategies and compute marginal expectations
across the fitted distributions. BIS serves as a complementary tool that
supports benchmark development and validation under constrained resources,
offering actionable and quick feedback for prompt selection and contamination
assessment. We conduct extensive experiments involving 8,000 evaluation points
across 4 CodeLlama models and 9 diverse benchmarks. Our framework achieves an
average absolute prediction error of 1.1% for code correctness scores, with
best- and worst-case errors of 0.3% and 1.9%, respectively. It also generalizes
well to other metrics, attaining average absolute errors of 2.15% for pass@1.
These results demonstrate the reliability and broad applicability of BIS, which
can significantly reduce the cost and effort of benchmarking LLMs in
code-related tasks.

</details>


### [104] [Calibrated Prediction Set in Fault Detection with Risk Guarantees via Significance Tests](https://arxiv.org/abs/2508.01208)
*Mingchen Mei,Yi Li,YiYao Qian,Zijun Jia*

Main category: cs.AI

TL;DR: This paper introduces a new fault detection method using significance testing and conformal prediction to provide risk guarantees, improving trustworthiness in safety-critical applications.


<details>
  <summary>Details</summary>
Motivation: A significant scientific challenge is the lack of rigorous risk control and reliable uncertainty quantification in existing diagnostic models, particularly when facing complex scenarios such as distributional shifts.

Method: This paper proposes a novel fault detection method that integrates significance testing with the conformal prediction framework to provide formal risk guarantees. The method transforms fault detection into a hypothesis testing task by defining a nonconformity measure based on model residuals. It then leverages a calibration dataset to compute p-values for new samples, which are used to construct prediction sets mathematically guaranteed to contain the true label with a user-specified probability.

Result: The proposed method consistently achieves an empirical coverage rate at or above the nominal level, demonstrating robustness even when the underlying point-prediction models perform poorly. Furthermore, the results reveal a controllable trade-off between the user-defined risk level and efficiency, where higher risk tolerance leads to smaller average prediction set sizes.

Conclusion: This research contributes a theoretically grounded framework for fault detection that enables explicit risk control, enhancing the trustworthiness of diagnostic systems in safety-critical applications and advancing the field from simple point predictions to informative, uncertainty-aware outputs.

Abstract: Fault detection is crucial for ensuring the safety and reliability of modern
industrial systems. However, a significant scientific challenge is the lack of
rigorous risk control and reliable uncertainty quantification in existing
diagnostic models, particularly when facing complex scenarios such as
distributional shifts. To address this issue, this paper proposes a novel fault
detection method that integrates significance testing with the conformal
prediction framework to provide formal risk guarantees. The method transforms
fault detection into a hypothesis testing task by defining a nonconformity
measure based on model residuals. It then leverages a calibration dataset to
compute p-values for new samples, which are used to construct prediction sets
mathematically guaranteed to contain the true label with a user-specified
probability, $1-\alpha$. Fault classification is subsequently performed by
analyzing the intersection of the constructed prediction set with predefined
normal and fault label sets. Experimental results on cross-domain fault
diagnosis tasks validate the theoretical properties of our approach. The
proposed method consistently achieves an empirical coverage rate at or above
the nominal level ($1-\alpha$), demonstrating robustness even when the
underlying point-prediction models perform poorly. Furthermore, the results
reveal a controllable trade-off between the user-defined risk level ($\alpha$)
and efficiency, where higher risk tolerance leads to smaller average prediction
set sizes. This research contributes a theoretically grounded framework for
fault detection that enables explicit risk control, enhancing the
trustworthiness of diagnostic systems in safety-critical applications and
advancing the field from simple point predictions to informative,
uncertainty-aware outputs.

</details>


### [105] [SketchAgent: Generating Structured Diagrams from Hand-Drawn Sketches](https://arxiv.org/abs/2508.01237)
*Cheng Tan,Qi Chen,Jingxuan Wei,Gaowei Wu,Zhangyang Gao,Siyuan Li,Bihui Yu,Ruifeng Guo,Stan Z. Li*

Main category: cs.AI

TL;DR: SketchAgent是一个多智能体系统，可以将手绘草图自动转换为结构化图表，并提出了Sketch2Diagram基准用于评估。


<details>
  <summary>Details</summary>
Motivation: 手绘草图是捕捉和传递想法的自然高效媒介。尽管可控自然图像生成技术取得了显著进展，但将手绘草图转换为结构化的、机器可读的图表仍然是一项劳动密集型且主要靠手工完成的任务。主要挑战来自于草图固有的模糊性，草图缺乏自动图表生成所需的结构约束和语义精度。

Method: 引入了SketchAgent多智能体系统，该系统集成了草图识别、符号推理和迭代验证。

Result: 提出了Sketch2Diagram基准，一个综合数据集和评估框架，包含八个不同的图表类别，例如流程图、有向图和模型架构。该数据集包含超过6,000个高质量示例，具有令牌级注释、标准化预处理和严格的质量控制。

Conclusion: SketchAgent通过集成草图识别、符号推理和迭代验证，能够显著减少手动工作，将手绘草图转换为结构化图表，并在设计、教育和工程领域具有应用前景。

Abstract: Hand-drawn sketches are a natural and efficient medium for capturing and
conveying ideas. Despite significant advancements in controllable natural image
generation, translating freehand sketches into structured, machine-readable
diagrams remains a labor-intensive and predominantly manual task. The primary
challenge stems from the inherent ambiguity of sketches, which lack the
structural constraints and semantic precision required for automated diagram
generation. To address this challenge, we introduce SketchAgent, a multi-agent
system designed to automate the transformation of hand-drawn sketches into
structured diagrams. SketchAgent integrates sketch recognition, symbolic
reasoning, and iterative validation to produce semantically coherent and
structurally accurate diagrams, significantly reducing the need for manual
effort. To evaluate the effectiveness of our approach, we propose the
Sketch2Diagram Benchmark, a comprehensive dataset and evaluation framework
encompassing eight diverse diagram categories, such as flowcharts, directed
graphs, and model architectures. The dataset comprises over 6,000 high-quality
examples with token-level annotations, standardized preprocessing, and rigorous
quality control. By streamlining the diagram generation process, SketchAgent
holds great promise for applications in design, education, and engineering,
while offering a significant step toward bridging the gap between intuitive
sketching and machine-readable diagram generation. The benchmark is released at
https://huggingface.co/datasets/DiagramAgent/Sketch2Diagram-Benchmark.

</details>


### [106] [Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models](https://arxiv.org/abs/2508.01261)
*Sushant Mehta,Raj Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.AI

TL;DR: MoE-MLA-RoPE: efficient language model with MoE, MLA, and RoPE, achieving memory reduction and speedup with competitive performance.


<details>
  <summary>Details</summary>
Motivation: addresses the fundamental trade-off between model capacity and computational efficiency

Method: combines Mixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary Position Embeddings (RoPE)

Result: achieves 68% KV cache memory reduction and 3.2x inference speedup while maintaining competitive perplexity (0.8% degradation). Compared to the parameters with 53.9M parameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla transformers while using 42% fewer active parameters per forward pass.

Conclusion: MoE-MLA-RoPE demonstrates that architectural novelty, not parameter scaling, defines the efficiency frontier for resource-constrained language model deployment.

Abstract: We present MoE-MLA-RoPE, a novel architecture combination that combines
Mixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary
Position Embeddings (RoPE) for efficient language modeling. Our approach
addresses the fundamental trade-off between model capacity and computational
efficiency through three key innovations: (1) fine-grained expert routing with
64 micro-experts and top-$k$ selection, enabling flexible specialization
through 3.6 * 10^7 possible expert combinations; (2) shared expert isolation
that dedicates 2 always active experts for common patterns while routing to 6
of 62 specialized experts; and (3) gradient-conflict-free load balancing that
maintains expert utilization without interfering with primary loss
optimization.
  Extensive experiments on models ranging from 17M to 202M parameters
demonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV
cache memory reduction and 3.2x inference speedup while maintaining competitive
perplexity (0.8% degradation). Compared to the parameters with 53.9M
parameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla
transformers while using 42% fewer active parameters per forward pass.
FLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x
inference acceleration. Automated evaluation using GPT-4 as a judge confirms
quality improvements in generation, with higher scores on coherence (8.1/10),
creativity (7.9/10) and grammatical correctness (8.2/10). Our results establish
that architectural novelty, not parameter scaling, defines the efficiency
frontier for resource-constrained language model deployment.

</details>


### [107] [Win-k: Improved Membership Inference Attacks on Small Language Models](https://arxiv.org/abs/2508.01268)
*Roya Arkhmammadova,Hosein Madadi Tamar,M. Emre Gursoy*

Main category: cs.AI

TL;DR: 本文研究了SLM上的MIAs，提出了一个新的MIA叫做win-k，它建立在最先进的攻击(min-k)之上。实验结果表明，win-k优于现有的MIAs，尤其是在较小的模型上。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型(SLM)在资源受限的环境中越来越被重视，因为它们的高效性和可部署性，这使得它们在设备上、隐私敏感和边缘计算应用中非常有用。另一方面，成员推理攻击(MIAs)旨在确定一个给定的样本是否被用于模型的训练，这是一个重要的威胁，具有严重的隐私和知识产权影响。虽然MIAs已被证明对大型语言模型(llm)有效，但对新兴的slm的研究相对较少，而且，它们的有效性随着模型变小而降低。受这一发现的启发，

Method: 我们提出了一个新的MIA叫做win-k，它建立在最先进的攻击(min-k)之上。

Result: win-k在AUROC、TPR @ 1% FPR和FPR @ 99% TPR指标方面优于现有的MIAs，尤其是在较小的模型上。

Conclusion: win-k在AUROC、TPR @ 1% FPR和FPR @ 99% TPR指标方面优于现有的MIAs，尤其是在较小的模型上。

Abstract: Small language models (SLMs) are increasingly valued for their efficiency and
deployability in resource-constrained environments, making them useful for
on-device, privacy-sensitive, and edge computing applications. On the other
hand, membership inference attacks (MIAs), which aim to determine whether a
given sample was used in a model's training, are an important threat with
serious privacy and intellectual property implications. In this paper, we study
MIAs on SLMs. Although MIAs were shown to be effective on large language models
(LLMs), they are relatively less studied on emerging SLMs, and furthermore,
their effectiveness decreases as models get smaller. Motivated by this finding,
we propose a new MIA called win-k, which builds on top of a state-of-the-art
attack (min-k). We experimentally evaluate win-k by comparing it with five
existing MIAs using three datasets and eight SLMs. Results show that win-k
outperforms existing MIAs in terms of AUROC, TPR @ 1% FPR, and FPR @ 99% TPR
metrics, especially on smaller models.

</details>


### [108] [KCR: Resolving Long-Context Knowledge Conflicts via Reasoning in LLMs](https://arxiv.org/abs/2508.01273)
*Xianda Zheng,Zijian Huang,Meng-Fen Chiang,Michael J. Witbrock,Kaiqi Zhao*

Main category: cs.AI

TL;DR: Proposes a Knowledge Conflict Reasoning (KCR) framework to enhance LLMs' ability to resolve conflicting knowledge by training them to follow logically consistent reasoning paths.


<details>
  <summary>Details</summary>
Motivation: When dealing with conflicts between multiple contexts, also known as inter-context knowledge conflicts, LLMs are often confused by lengthy and conflicting contexts.

Method: The Knowledge Conflict Reasoning (KCR) framework, which enhances the ability of LLMs to resolve conflicting knowledge. The key idea of KCR is to train backbone LLMs to establish a correct reasoning process by rewarding them for selecting and adhering to the context with stronger logical consistency when presented with conflicting contexts.

Result: The framework significantly improves the ability of various backbone models to resolve knowledge conflicts in long-context scenarios, yielding substantial performance gains.

Conclusion: This framework significantly improves the ability of various backbone models to resolve knowledge conflicts in long-context scenarios, yielding substantial performance gains.

Abstract: Knowledge conflicts commonly arise across diverse sources, and their
prevalence has increased with the advent of LLMs. When dealing with conflicts
between multiple contexts, also known as \emph{inter-context knowledge
conflicts}, LLMs are often confused by lengthy and conflicting contexts. To
address this challenge, we propose the Knowledge Conflict Reasoning (KCR)
framework, which enhances the ability of LLMs to resolve conflicting knowledge.
The key idea of KCR is to train backbone LLMs to establish a correct reasoning
process by rewarding them for selecting and adhering to the context with
stronger logical consistency when presented with conflicting contexts.
Specifically, we first extract reasoning paths, represented by either text or
local knowledge graphs, from the conflicting long contexts. Subsequently, we
employ Reinforcement Learning to encourage the model to learn the paradigm of
reasoning process that follows correct reasoning paths rather than the
incorrect counterparts. This enables the backbone models to genuinely acquire
the capability to resolve inter-context knowledge conflicts within long
contexts. Experimental results demonstrate that our framework significantly
improves the ability of various backbone models to resolve knowledge conflicts
in long-context scenarios, yielding substantial performance gains.

</details>


### [109] [Multi-TW: Benchmarking Multimodal Models on Traditional Chinese Question Answering in Taiwan](https://arxiv.org/abs/2508.01274)
*Jui-Ming Yao,Bing-Cheng Xie,Sheng-Wei Peng,Hao-Yuan Chen,He-Rong Zheng,Bing-Jia Tan,Peter Shaojui Wang,Shun-Feng Su*

Main category: cs.AI

TL;DR: Multi-TW是一个新的繁体中文基准，用于评估多模态模型的性能和延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的基准通常忽略了传统中文的三模态评估，并且不考虑推理延迟。为了解决这个问题

Method: 我们引入了Multi-TW，这是第一个用于评估任何到任何多模态模型的性能和延迟的繁体中文基准。

Result: 闭源模型通常优于跨模态的开源模型，尽管开源模型可以在音频任务中表现良好。与使用单独音频转录的VLM 相比，端到端任何到任何管道提供了明显的延迟优势。

Conclusion: Multi-TW提供了一个全面的模型能力视角，并强调了对繁体中文进行微调和高效多模态架构的需求。

Abstract: Multimodal Large Language Models (MLLMs) process visual, acoustic, and
textual inputs, addressing the limitations of single-modality LLMs. However,
existing benchmarks often overlook tri-modal evaluation in Traditional Chinese
and do not consider inference latency. To address this, we introduce Multi-TW,
the first Traditional Chinese benchmark for evaluating the performance and
latency of any-to-any multimodal models. Multi-TW includes 900 multiple-choice
questions (image and text, audio and text pairs) sourced from official
proficiency tests developed with the Steering Committee for the Test of
Proficiency-Huayu (SC-TOP). We evaluated various any-to-any models and
vision-language models (VLMs) with audio transcription. Our results show that
closed-source models generally outperform open-source ones across modalities,
although open-source models can perform well in audio tasks. End-to-end
any-to-any pipelines offer clear latency advantages compared to VLMs using
separate audio transcription. Multi-TW presents a comprehensive view of model
capabilities and highlights the need for Traditional Chinese fine-tuning and
efficient multimodal architectures.

</details>


### [110] [BioDisco: Multi-agent hypothesis generation with dual-mode evidence, iterative feedback and temporal evaluation](https://arxiv.org/abs/2508.01285)
*Yujing Ke,Kevin George,Kathan Pandya,David Blumenthal,Maximilian Sprang,Gerrit Großmann,Sebastian Vollmer,David Antony Selby*

Main category: cs.AI

TL;DR: BioDisco, a new multi-agent framework, helps researchers discover novel hypotheses by using language models, knowledge graphs, and literature retrieval. It outperforms existing methods and is easy to use.


<details>
  <summary>Details</summary>
Motivation: Identifying novel hypotheses is essential to scientific research, yet this process risks being overwhelmed by the sheer volume and complexity of available information. Existing automated methods often struggle to generate novel and evidence-grounded hypotheses, lack robust iterative refinement and rarely undergo rigorous temporal evaluation for future discovery potential.

Method: a multi-agent framework that draws upon language model-based reasoning and a dual-mode evidence system (biomedical knowledge graphs and automated literature retrieval)

Result: demonstrates superior novelty and significance over ablated configurations representative of existing agentic architectures. Designed for flexibility and modularity, BioDisco allows seamless integration of custom language models or knowledge graphs, and can be run with just a few lines of code.

Conclusion: The BioDisco framework demonstrates superior novelty and significance over existing agentic architectures and is anticipated to be a catalyst for the discovery of new hypotheses.

Abstract: Identifying novel hypotheses is essential to scientific research, yet this
process risks being overwhelmed by the sheer volume and complexity of available
information. Existing automated methods often struggle to generate novel and
evidence-grounded hypotheses, lack robust iterative refinement and rarely
undergo rigorous temporal evaluation for future discovery potential. To address
this, we propose BioDisco, a multi-agent framework that draws upon language
model-based reasoning and a dual-mode evidence system (biomedical knowledge
graphs and automated literature retrieval) for grounded novelty, integrates an
internal scoring and feedback loop for iterative refinement, and validates
performance through pioneering temporal and human evaluations and a
Bradley-Terry paired comparison model to provide statistically-grounded
assessment. Our evaluations demonstrate superior novelty and significance over
ablated configurations representative of existing agentic architectures.
Designed for flexibility and modularity, BioDisco allows seamless integration
of custom language models or knowledge graphs, and can be run with just a few
lines of code. We anticipate researchers using this practical tool as a
catalyst for the discovery of new hypotheses.

</details>


### [111] [How Far Are LLMs from Symbolic Planners? An NLP-Based Perspective](https://arxiv.org/abs/2508.01300)
*Ma'ayan Armony,Albert Meroño-Peñuela,Gerard Canal*

Main category: cs.AI

TL;DR: LLMs are unreliable planners. An NLP-based recovery pipeline improves action quality and success rate, but still falls short of classical planners.


<details>
  <summary>Details</summary>
Motivation: LLMs are still not reliable as planners, with the generated plans often containing mistaken or hallucinated actions.

Method: We propose a recovery pipeline consisting of an NLP-based evaluation of the generated plans, along with three stages to recover the plans through NLP manipulation of the LLM-generated plans, and eventually complete the plan using a symbolic planner.

Result: Our findings reveal no clear evidence of underlying reasoning during plan generation, and that a pipeline comprising an NLP-based analysis of the plans, followed by a recovery mechanism, still falls short of the quality and reliability of classical planners. On average, only the first 2.65 actions of the plan are executable, with the average length of symbolically generated plans being 8.4 actions.

Conclusion: The pipeline still improves action quality and increases the overall success rate from 21.9% to 27.5%.

Abstract: The reasoning and planning abilities of Large Language Models (LLMs) have
been a frequent topic of discussion in recent years. Their ability to take
unstructured planning problems as input has made LLMs' integration into AI
planning an area of interest. Nevertheless, LLMs are still not reliable as
planners, with the generated plans often containing mistaken or hallucinated
actions. Existing benchmarking and evaluation methods investigate planning with
LLMs, focusing primarily on success rate as a quality indicator in various
planning tasks, such as validating plans or planning in relaxed conditions. In
this paper, we approach planning with LLMs as a natural language processing
(NLP) task, given that LLMs are NLP models themselves. We propose a recovery
pipeline consisting of an NLP-based evaluation of the generated plans, along
with three stages to recover the plans through NLP manipulation of the
LLM-generated plans, and eventually complete the plan using a symbolic planner.
This pipeline provides a holistic analysis of LLM capabilities in the context
of AI task planning, enabling a broader understanding of the quality of invalid
plans. Our findings reveal no clear evidence of underlying reasoning during
plan generation, and that a pipeline comprising an NLP-based analysis of the
plans, followed by a recovery mechanism, still falls short of the quality and
reliability of classical planners. On average, only the first 2.65 actions of
the plan are executable, with the average length of symbolically generated
plans being 8.4 actions. The pipeline still improves action quality and
increases the overall success rate from 21.9% to 27.5%.

</details>


### [112] [PUZZLED: Jailbreaking LLMs through Word-Based Puzzles](https://arxiv.org/abs/2508.01306)
*Yelim Ahn,Jaejin Lee*

Main category: cs.AI

TL;DR: PUZZLED是一种新的越狱方法，它利用法学硕士的推理能力，通过将有害指令中的关键词屏蔽为单词谜题来逃避检测，从而实现高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）越来越多地部署到不同的领域，确保其安全性已成为一个关键问题。作为回应，对越狱攻击的研究一直在积极发展。现有方法通常依赖于迭代提示工程或有害指令的语义转换来逃避检测。

Method: PUZZLED，一种新颖的越狱方法，它利用法学硕士的推理能力。它屏蔽有害指令中的关键词，并将它们作为单词谜题呈现给法学硕士来解决。我们设计了三种谜题类型——单词搜索、字谜和填字游戏——这些谜题对人类来说很熟悉，但对法学硕士来说在认知上要求很高。模型必须解决难题才能发现被屏蔽的单词，然后继续生成对重建的有害指令的响应。

Result: 我们在五个最先进的法学硕士上评估了PUZZLED，并观察到88.8%的平均攻击成功率（ASR），特别是GPT-4.1上的96.5%和Claude 3.7 Sonnet上的92.3%。

Conclusion: PUZZLED是一种简单但功能强大的攻击，它通过利用法学硕士的推理能力，将熟悉的谜题转化为有效的越狱策略。

Abstract: As large language models (LLMs) are increasingly deployed across diverse
domains, ensuring their safety has become a critical concern. In response,
studies on jailbreak attacks have been actively growing. Existing approaches
typically rely on iterative prompt engineering or semantic transformations of
harmful instructions to evade detection. In this work, we introduce PUZZLED, a
novel jailbreak method that leverages the LLM's reasoning capabilities. It
masks keywords in a harmful instruction and presents them as word puzzles for
the LLM to solve. We design three puzzle types-word search, anagram, and
crossword-that are familiar to humans but cognitively demanding for LLMs. The
model must solve the puzzle to uncover the masked words and then proceed to
generate responses to the reconstructed harmful instruction. We evaluate
PUZZLED on five state-of-the-art LLMs and observe a high average attack success
rate (ASR) of 88.8%, specifically 96.5% on GPT-4.1 and 92.3% on Claude 3.7
Sonnet. PUZZLED is a simple yet powerful attack that transforms familiar
puzzles into an effective jailbreak strategy by harnessing LLMs' reasoning
capabilities.

</details>


### [113] [Idempotent Equilibrium Analysis of Hybrid Workflow Allocation: A Mathematical Schema for Future Work](https://arxiv.org/abs/2508.01323)
*Faruk Alpay,Bugra Kilictas,Taylan Alpay,Hamdi Alakkad*

Main category: cs.AI

TL;DR: AI automation will rise to 65% by 2045, with humans specializing in AI workflow management.


<details>
  <summary>Details</summary>
Motivation: The rapid advance of large-scale AI systems is reshaping how work is divided between people and machines.

Method: Leveraging lattice-theoretic fixed-point tools (Tarski and Banach).

Result: Automation is projected to rise from approximately 10% of work to approximately 65%, leaving a persistent one-third of tasks to humans.

Conclusion: Policies promoting human-AI teaming can steer the economy toward the welfare-maximizing fixed point.

Abstract: The rapid advance of large-scale AI systems is reshaping how work is divided
between people and machines. We formalise this reallocation as an iterated
task-delegation map and show that--under broad, empirically grounded
assumptions--the process converges to a stable idempotent equilibrium in which
every task is performed by the agent (human or machine) with enduring
comparative advantage. Leveraging lattice-theoretic fixed-point tools (Tarski
and Banach), we (i) prove existence of at least one such equilibrium and (ii)
derive mild monotonicity conditions that guarantee uniqueness. In a stylised
continuous model the long-run automated share takes the closed form $x^* =
\alpha / (\alpha + \beta)$, where $\alpha$ captures the pace of automation and
$\beta$ the rate at which new, human-centric tasks appear; hence full
automation is precluded whenever $\beta > 0$. We embed this analytic result in
three complementary dynamical benchmarks--a discrete linear update, an
evolutionary replicator dynamic, and a continuous Beta-distributed task
spectrum--each of which converges to the same mixed equilibrium and is
reproducible from the provided code-free formulas. A 2025-to-2045 simulation
calibrated to current adoption rates projects automation rising from
approximately 10% of work to approximately 65%, leaving a persistent one-third
of tasks to humans. We interpret that residual as a new profession of workflow
conductor: humans specialise in assigning, supervising and integrating AI
modules rather than competing with them. Finally, we discuss implications for
skill development, benchmark design and AI governance, arguing that policies
which promote "centaur" human-AI teaming can steer the economy toward the
welfare-maximising fixed point.

</details>


### [114] [Towards Evaluation for Real-World LLM Unlearning](https://arxiv.org/abs/2508.01324)
*Ke Miao,Yuke Hu,Xiaochen Li,Wenjie Bao,Zhihao Liu,Zhan Qin,Kui Ren*

Main category: cs.AI

TL;DR: 本文提出了一种新的指标DCUE，用于评估LLM的unlearning效果，该指标可以克服现有指标的局限性，并指导未来unlearning算法的设计。


<details>
  <summary>Details</summary>
Motivation: 分析了现有unlearning评估指标在实际LLM unlearning场景中的实用性、准确性和鲁棒性方面的局限性。

Method: 提出了一种名为基于分布校正的Unlearning评估（DCUE）的新指标，该指标识别核心token并使用验证集校正其置信度分数中的分布偏差，并使用Kolmogorov-Smirnov检验量化评估结果。

Result: 实验结果表明，DCUE克服了现有指标的局限性。

Conclusion: DCUE克服了现有指标的局限性，可以指导未来更实用、更可靠的unlearning算法设计。

Abstract: This paper analyzes the limitations of existing unlearning evaluation metrics
in terms of practicality, exactness, and robustness in real-world LLM
unlearning scenarios. To overcome these limitations, we propose a new metric
called Distribution Correction-based Unlearning Evaluation (DCUE). It
identifies core tokens and corrects distributional biases in their confidence
scores using a validation set. The evaluation results are quantified using the
Kolmogorov-Smirnov test. Experimental results demonstrate that DCUE overcomes
the limitations of existing metrics, which also guides the design of more
practical and reliable unlearning algorithms in the future.

</details>


### [115] [NatureGAIA: Pushing the Frontiers of GUI Agents with a Challenging Benchmark and High-Quality Trajectory Dataset](https://arxiv.org/abs/2508.01330)
*Zihan Zheng,Tianle Cui,Chuwen Xie,Jiahui Zhang,Jiahui Pan,Lewei He,Qianglong Chen*

Main category: cs.AI

TL;DR: The paper introduces a new benchmark (") and agent architecture to address limitations in evaluating and improving LLM-based GUI agents. Experiments show the benchmark is challenging and that smaller models struggle with complex tasks despite fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of Large Language Model (LLM)-driven Graphical User Interface (GUI) agents is significantly hampered by the profound limitations of existing evaluation benchmarks in terms of accuracy, reproducibility, and scalability.

Method: introduce "{Benchmark}", a novel benchmark engineered on the principle of Causal Pathways and developed \Agent, a hierarchical agent architecture specifically optimized for long-horizon tasks

Result: "{Benchmark}~presents a formidable challenge to current state-of-the-art LLMs; even the top-performing Claude-sonnet-4 achieved a Weighted Pathway Success Rate (WPSR) of only 34.6\%. Moreover, while RFT substantially improved the smaller model's GUI execution capabilities (WPSR increased from 3.3\% to 10.8\%), its performance degraded sharply when handling complex scenarios."

Conclusion: This research contributes a rigorous evaluation standard and a high-quality dataset to the community, aiming to guide the future development of GUI agents.

Abstract: The rapid advancement of Large Language Model (LLM)-driven Graphical User
Interface (GUI) agents is significantly hampered by the profound limitations of
existing evaluation benchmarks in terms of accuracy, reproducibility, and
scalability. To address this critical gap, we introduce \Benchmark, a novel
benchmark engineered on the principle of Causal Pathways. This design paradigm
structures complex tasks into a series of programmatically verifiable atomic
steps, ensuring a rigorous, fully automated, and reproducible standard for
assessment. Concurrently, to mitigate the inherent capability deficits of
agents, we developed \Agent, a hierarchical agent architecture specifically
optimized for long-horizon tasks. We leveraged this agent to generate a
high-quality, human-verified trajectory dataset that uniquely captures diverse
and even self-correcting interaction patterns of LLMs. We then utilized this
dataset to perform Reinforcement Fine-Tuning (RFT) on the Qwen2.5-VL-7B model.
Our experiments reveal that \Benchmark~presents a formidable challenge to
current state-of-the-art LLMs; even the top-performing Claude-sonnet-4 achieved
a Weighted Pathway Success Rate (WPSR) of only 34.6\%. Moreover, while RFT
substantially improved the smaller model's GUI execution capabilities (WPSR
increased from 3.3\% to 10.8\%), its performance degraded sharply when handling
complex scenarios. This outcome highlights the inherent capability ceiling of
smaller models when faced with comprehensive tasks that integrate perception,
decision-making, and execution. This research contributes a rigorous evaluation
standard and a high-quality dataset to the community, aiming to guide the
future development of GUI agents.

</details>


### [116] [Relation-Aware LNN-Transformer for Intersection-Centric Next-Step Prediction](https://arxiv.org/abs/2508.01368)
*Zhehong Ren,Tianluo Zhang,Yiheng Lu,Yushen Liang,Promethee Spathis*

Main category: cs.AI

TL;DR: This paper introduces a road-node-centric framework for next-step location prediction that relaxes the closed-world constraint by representing trajectories on the city's road-intersection graph. The model combines sector-wise directional POI aggregation with structural graph embeddings and a Relation-Aware LNN-Transformer to achieve state-of-the-art performance and resilience to noise.


<details>
  <summary>Details</summary>
Motivation: Approaches that assume a closed world - restricting choices to a predefined set of points of interest (POIs) - often fail to capture exploratory or target-agnostic behavior and the topological constraints of urban road networks.

Method: We integrate a Relation-Aware LNN-Transformer - a hybrid of a Continuous-time Forgetting Cell CfC-LNN and a bearing-biased self-attention module - to capture both fine-grained temporal dynamics and long-range spatial dependencies.

Result: Our model outperforms six state-of-the-art baselines by up to 17 percentage points in accuracy at one hop and 10 percentage points in MRR, and maintains high resilience under noise.

Conclusion: Our model outperforms six state-of-the-art baselines by up to 17 percentage points in accuracy at one hop and 10 percentage points in MRR, and maintains high resilience under noise, losing only 2.4 percentage points in accuracy at one under 50 meter GPS perturbation and 8.9 percentage points in accuracy at one hop under 25 percent POI noise.

Abstract: Next-step location prediction plays a pivotal role in modeling human
mobility, underpinning applications from personalized navigation to strategic
urban planning. However, approaches that assume a closed world - restricting
choices to a predefined set of points of interest (POIs) - often fail to
capture exploratory or target-agnostic behavior and the topological constraints
of urban road networks. Hence, we introduce a road-node-centric framework that
represents road-user trajectories on the city's road-intersection graph,
thereby relaxing the closed-world constraint and supporting next-step
forecasting beyond fixed POI sets. To encode environmental context, we
introduce a sector-wise directional POI aggregation that produces compact
features capturing distance, bearing, density and presence cues. By combining
these cues with structural graph embeddings, we obtain semantically grounded
node representations. For sequence modeling, we integrate a Relation-Aware
LNN-Transformer - a hybrid of a Continuous-time Forgetting Cell CfC-LNN and a
bearing-biased self-attention module - to capture both fine-grained temporal
dynamics and long-range spatial dependencies. Evaluated on city-scale road-user
trajectories, our model outperforms six state-of-the-art baselines by up to 17
percentage points in accuracy at one hop and 10 percentage points in MRR, and
maintains high resilience under noise, losing only 2.4 percentage points in
accuracy at one under 50 meter GPS perturbation and 8.9 percentage points in
accuracy at one hop under 25 percent POI noise.

</details>


### [117] [TripTailor: A Real-World Benchmark for Personalized Travel Planning](https://arxiv.org/abs/2508.01432)
*Yuanzhe Shen,Kaimin Wang,Changze Lv,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.AI

TL;DR: TripTailor, a new benchmark for travel planning, reveals that LLMs still struggle to create realistic and personalized travel itineraries.


<details>
  <summary>Details</summary>
Motivation: Existing travel planning benchmarks rely on unrealistic simulated data and lack comprehensive evaluation metrics for assessing the overall quality of travel plans generated by LLMs.

Method: The authors introduce TripTailor, a benchmark for personalized travel planning using a dataset of over 500,000 real-world points of interest (POIs) and nearly 4,000 diverse travel itineraries.

Result: Experiments show that fewer than 10% of itineraries generated by state-of-the-art LLMs achieve human-level performance on the TripTailor benchmark.

Conclusion: The TripTailor benchmark reveals that current LLMs struggle to generate human-level travel itineraries, highlighting challenges in feasibility, rationality, and personalization. The authors hope TripTailor will drive the development of better travel planning agents.

Abstract: The continuous evolution and enhanced reasoning capabilities of large
language models (LLMs) have elevated their role in complex tasks, notably in
travel planning, where demand for personalized, high-quality itineraries is
rising. However, current benchmarks often rely on unrealistic simulated data,
failing to reflect the differences between LLM-generated and real-world
itineraries. Existing evaluation metrics, which primarily emphasize
constraints, fall short of providing a comprehensive assessment of the overall
quality of travel plans. To address these limitations, we introduce TripTailor,
a benchmark designed specifically for personalized travel planning in
real-world scenarios. This dataset features an extensive collection of over
500,000 real-world points of interest (POIs) and nearly 4,000 diverse travel
itineraries, complete with detailed information, providing a more authentic
evaluation framework. Experiments show that fewer than 10\% of the itineraries
generated by the latest state-of-the-art LLMs achieve human-level performance.
Moreover, we identify several critical challenges in travel planning, including
the feasibility, rationality, and personalized customization of the proposed
solutions. We hope that TripTailor will drive the development of travel
planning agents capable of understanding and meeting user needs while
generating practical itineraries. Our code and dataset are available at
https://github.com/swxkfm/TripTailor

</details>


### [118] [$R^2$-CoD: Understanding Text-Graph Complementarity in Relational Reasoning via Knowledge Co-Distillation](https://arxiv.org/abs/2508.01475)
*Zhen Wu,Ritam Dutt,Luke M. Breitfeller,Armineh Nourbakhsh,Siddharth Parekh,Carolyn Rosé*

Main category: cs.AI

TL;DR: 我们通过一个统一的架构研究了文本-图表示互补性，该架构支持知识协同提炼 (CoD)。


<details>
  <summary>Details</summary>
Motivation: 关系推理是许多 NLP 任务的核心，它利用来自文本和图的互补信号。虽然之前的研究已经调查了如何利用这种双重互补性，但对文本-图相互作用及其对混合模型的影响的详细和系统的理解仍未得到充分探索。

Method: 我们采用分析驱动的方法，通过支持知识协同提炼 (CoD) 的统一架构来研究文本-图表示互补性。

Result: 我们探索了五个涉及关系推理的任务，这些任务在文本和图结构如何编码解决该任务所需的信息方面有所不同。

Conclusion: 通过跟踪这些双重表征在训练期间的演变，我们揭示了对齐和差异的可解释模式，并深入了解了何时以及为什么它们的集成是有益的。

Abstract: Relational reasoning lies at the core of many NLP tasks, drawing on
complementary signals from text and graphs. While prior research has
investigated how to leverage this dual complementarity, a detailed and
systematic understanding of text-graph interplay and its effect on hybrid
models remains underexplored. We take an analysis-driven approach to
investigate text-graph representation complementarity via a unified
architecture that supports knowledge co-distillation (CoD). We explore five
tasks involving relational reasoning that differ in how text and graph
structures encode the information needed to solve that task. By tracking how
these dual representations evolve during training, we uncover interpretable
patterns of alignment and divergence, and provide insights into when and why
their integration is beneficial.

</details>


### [119] [CARGO: A Co-Optimization Framework for EV Charging and Routing in Goods Delivery Logistics](https://arxiv.org/abs/2508.01476)
*Arindam Khanda,Anurag Satpathy,Amit Jha,Sajal K. Das*

Main category: cs.AI

TL;DR: This paper proposes CARGO, a framework addressing the EV-based delivery route planning problem (EDRP), which jointly optimizes route planning and charging for deliveries within time windows.


<details>
  <summary>Details</summary>
Motivation: EVs face challenges due to their limited battery capacity, requiring careful planning for recharging. This depends on factors such as the charging point (CP) availability, cost, proximity, and vehicles' state of charge (SoC).

Method: a mixed integer linear programming (MILP)-based exact solution and a computationally efficient heuristic method

Result: Using real-world datasets, we evaluate our methods by comparing the heuristic to the MILP solution, and benchmarking it against baseline strategies, Earliest Deadline First (EDF) and Nearest Delivery First (NDF).

Conclusion: The results show up to 39% and 22% reductions in the charging cost over EDF and NDF, respectively, while completing comparable deliveries.

Abstract: With growing interest in sustainable logistics, electric vehicle (EV)-based
deliveries offer a promising alternative for urban distribution. However, EVs
face challenges due to their limited battery capacity, requiring careful
planning for recharging. This depends on factors such as the charging point
(CP) availability, cost, proximity, and vehicles' state of charge (SoC). We
propose CARGO, a framework addressing the EV-based delivery route planning
problem (EDRP), which jointly optimizes route planning and charging for
deliveries within time windows. After proving the problem's NP-hardness, we
propose a mixed integer linear programming (MILP)-based exact solution and a
computationally efficient heuristic method. Using real-world datasets, we
evaluate our methods by comparing the heuristic to the MILP solution, and
benchmarking it against baseline strategies, Earliest Deadline First (EDF) and
Nearest Delivery First (NDF). The results show up to 39% and 22% reductions in
the charging cost over EDF and NDF, respectively, while completing comparable
deliveries.

</details>


### [120] [WinkTPG: An Execution Framework for Multi-Agent Path Finding Using Temporal Reasoning](https://arxiv.org/abs/2508.01495)
*Jingtian Yan,Stephen F. Smith,Jiaoyang Li*

Main category: cs.AI

TL;DR: 本文提出了一种名为 Windowed kTPG (WinkTPG) 的 MAPF 执行框架，该框架可以有效地将 MAPF 计划优化为运动学上可行的计划，同时考虑不确定性并保持无碰撞性。


<details>
  <summary>Details</summary>
Motivation: 为一大群智能体规划无碰撞路径是一个具有众多现实世界应用的有挑战性的问题。虽然多智能体路径寻找 (MAPF) 的最新进展显示出了可喜的进展，但标准 MAPF 算法依赖于简化的运动学模型，从而阻止智能体直接遵循生成的 MAPF 计划。

Method: 我们提出了运动学时间规划图规划 (kTPG)，这是一种多智能体速度优化算法，可以有效地将 MAPF 计划优化为运动学上可行的计划，同时考虑不确定性并保持无碰撞性。在 kTPG 的基础上，我们提出了 Windowed kTPG (WinkTPG)，这是一个 MAPF 执行框架，它使用基于窗口的机制增量地优化 MAPF 计划，在执行期间动态地整合代理信息以减少不确定性。

Result: 实验表明 WinkTPG 可以为多达 1,000 个代理在 1 秒内生成速度曲线，并且比现有的 MAPF 执行方法提高了高达 51.7% 的解决方案质量。

Conclusion: WinkTPG可以为多达 1,000 个代理在 1 秒内生成速度曲线，并且比现有的 MAPF 执行方法提高了高达 51.7% 的解决方案质量。

Abstract: Planning collision-free paths for a large group of agents is a challenging
problem with numerous real-world applications. While recent advances in
Multi-Agent Path Finding (MAPF) have shown promising progress, standard MAPF
algorithms rely on simplified kinodynamic models, preventing agents from
directly following the generated MAPF plan. To bridge this gap, we propose
kinodynamic Temporal Plan Graph Planning (kTPG), a multi-agent speed
optimization algorithm that efficiently refines a MAPF plan into a
kinodynamically feasible plan while accounting for uncertainties and preserving
collision-freeness. Building on kTPG, we propose Windowed kTPG (WinkTPG), a
MAPF execution framework that incrementally refines MAPF plans using a
window-based mechanism, dynamically incorporating agent information during
execution to reduce uncertainty. Experiments show that WinkTPG can generate
speed profiles for up to 1,000 agents in 1 second and improves solution quality
by up to 51.7% over existing MAPF execution methods.

</details>


### [121] [Refine-n-Judge: Curating High-Quality Preference Chains for LLM-Fine-Tuning](https://arxiv.org/abs/2508.01543)
*Derin Cayir,Renjie Tao,Rashi Rungta,Kai Sun,Sean Chen,Haidar Khan,Minseok Kim,Julia Reinspach,Yue Liu*

Main category: cs.AI

TL;DR: Refine-n-Judge uses a single LLM to iteratively refine and judge its own responses, creating high-quality datasets for fine-tuning, leading to improved model performance.


<details>
  <summary>Details</summary>
Motivation: human feedback is essential for improving data quality, it is costly and does not scale well

Method: an automated iterative approach that leverages a single LLM as both a refiner and a judge to enhance dataset quality

Result: Models fine-tuned on Refine-n-Judge-enhanced datasets were preferred by LLM judges in over 74% of comparisons against models tuned on the original dataset by GPT-4. Additionally, we report performance gains: +5% on AlpacaEval and AlpacaEval 2.0, and +19% on MT-Bench.

Conclusion: Refine-n-Judge produces high-quality datasets and scalable model improvements.

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress through
preference-based fine-tuning, which critically depends on the quality of the
underlying training data. While human feedback is essential for improving data
quality, it is costly and does not scale well. In this paper, we introduce
Refine-n-Judge, an automated iterative approach that leverages a single LLM as
both a refiner and a judge to enhance dataset quality. Unlike existing
iterative refinement methods, Refine-n-Judge employs an LLM to both generate
refinements and explicitly evaluate each improvement, ensuring that every
iteration meaningfully enhances the dataset without requiring additional human
annotation or a separate reward model. At each step, the LLM refines a response
and judges whether the refinement is an improvement over the previous answer.
This process continues until the LLM prefers the initial answer over the
refinement, indicating no further improvements. This produces sequences of
increasing quality, preference-labeled responses ideal for fine-tuning.
  We demonstrate the effectiveness of Refine-n-Judge across a range of public
datasets spanning five corpora, targeting tasks such as coding, math, and
conversation. Models (Llama 3.1-8B and Llama 3.3-70B) fine-tuned on
Refine-n-Judge-enhanced datasets were preferred by LLM judges in over 74% of
comparisons against models tuned on the original dataset by GPT-4.
Additionally, we report performance gains: +5% on AlpacaEval and AlpacaEval
2.0, and +19% on MT-Bench. Our results indicate that Refine-n-Judge produces
high-quality datasets and scalable model improvements.

</details>


### [122] [Getting out of the Big-Muddy: Escalation of Commitment in LLMs](https://arxiv.org/abs/2508.01545)
*Emilio Barkett,Olivia Long,Paul Kröger*

Main category: cs.AI

TL;DR: LLM bias depends on context, not inherent traits, impacting multi-agent and unsupervised system deployment.


<details>
  <summary>Details</summary>
Motivation: LLMs may inherit cognitive biases that systematically distort human judgment, including escalation of commitment, where decision-makers continue investing in failing courses of action due to prior investment. Understanding when LLMs exhibit such biases presents a unique challenge.

Method: a two-stage investment task across four experimental conditions: model as investor, model as advisor, multi-agent deliberation, and compound pressure scenario

Result: bias manifestation in LLMs is highly context-dependent. In individual decision-making contexts, LLMs demonstrate strong rational cost-benefit logic with minimal escalation of commitment. However, multi-agent deliberation reveals a striking hierarchy effect... Similarly, when subjected to compound organizational and personal pressures, models exhibit high degrees of escalation of commitment

Conclusion: LLM bias manifestation depends critically on social and organizational context rather than being inherent, with significant implications for the deployment of multi-agent systems and unsupervised operations where such conditions may emerge naturally.

Abstract: Large Language Models (LLMs) are increasingly deployed in autonomous
decision-making roles across high-stakes domains. However, since models are
trained on human-generated data, they may inherit cognitive biases that
systematically distort human judgment, including escalation of commitment,
where decision-makers continue investing in failing courses of action due to
prior investment. Understanding when LLMs exhibit such biases presents a unique
challenge. While these biases are well-documented in humans, it remains unclear
whether they manifest consistently in LLMs or require specific triggering
conditions. This paper investigates this question using a two-stage investment
task across four experimental conditions: model as investor, model as advisor,
multi-agent deliberation, and compound pressure scenario. Across N = 6,500
trials, we find that bias manifestation in LLMs is highly context-dependent. In
individual decision-making contexts (Studies 1-2, N = 4,000), LLMs demonstrate
strong rational cost-benefit logic with minimal escalation of commitment.
However, multi-agent deliberation reveals a striking hierarchy effect (Study 3,
N = 500): while asymmetrical hierarchies show moderate escalation rates
(46.2%), symmetrical peer-based decision-making produces near-universal
escalation (99.2%). Similarly, when subjected to compound organizational and
personal pressures (Study 4, N = 2,000), models exhibit high degrees of
escalation of commitment (68.95% average allocation to failing divisions).
These findings reveal that LLM bias manifestation depends critically on social
and organizational context rather than being inherent, with significant
implications for the deployment of multi-agent systems and unsupervised
operations where such conditions may emerge naturally.

</details>


### [123] [Empowering Tabular Data Preparation with Language Models: Why and How?](https://arxiv.org/abs/2508.01556)
*Mengshi Chen,Yuxiang Sun,Tengchao Li,Jianwei Wang,Kai Wang,Xuemin Lin,Ying Zhang,Wenjie Zhang*

Main category: cs.AI

TL;DR: This survey analyzes the role of LMs in enhancing tabular data preparation processes, focusing on data acquisition, integration, cleaning, and transformation.


<details>
  <summary>Details</summary>
Motivation: Traditional methods often face challenges in capturing the intricate relationships within tables and adapting to the tasks involved. Recent advances in Language Models (LMs), especially in Large Language Models (LLMs), offer new opportunities to automate and support tabular data preparation. However, why LMs suit tabular data preparation (i.e., how their capabilities match task demands) and how to use them effectively across phases still remain to be systematically explored.

Method: LMs are combined with other components for different preparation tasks

Result: LMs can enhance tabular data preparation processes

Conclusion: This survey systematically analyzes the role of LMs in enhancing tabular data preparation processes, focusing on four core phases: data acquisition, integration, cleaning, and transformation. For each phase, it presents an integrated analysis of how LMs can be combined with other components for different preparation tasks, highlights key advancements, and outlines prospective pipelines.

Abstract: Data preparation is a critical step in enhancing the usability of tabular
data and thus boosts downstream data-driven tasks. Traditional methods often
face challenges in capturing the intricate relationships within tables and
adapting to the tasks involved. Recent advances in Language Models (LMs),
especially in Large Language Models (LLMs), offer new opportunities to automate
and support tabular data preparation. However, why LMs suit tabular data
preparation (i.e., how their capabilities match task demands) and how to use
them effectively across phases still remain to be systematically explored. In
this survey, we systematically analyze the role of LMs in enhancing tabular
data preparation processes, focusing on four core phases: data acquisition,
integration, cleaning, and transformation. For each phase, we present an
integrated analysis of how LMs can be combined with other components for
different preparation tasks, highlight key advancements, and outline
prospective pipelines.

</details>


### [124] [One Subgoal at a Time: Zero-Shot Generalization to Arbitrary Linear Temporal Logic Requirements in Multi-Task Reinforcement Learning](https://arxiv.org/abs/2508.01561)
*Zijian Guo,İlker Işık,H. M. Sabbir Ahmad,Wenchao Li*

Main category: cs.AI

TL;DR: GenZ-LTL通过将LTL任务分解为子目标并逐个解决，实现了对未见过的LTL规范的零样本泛化，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 强化学习在推广到复杂和时间扩展的任务目标和安全约束方面仍然是一个关键挑战。线性时序逻辑（LTL）提供了一个统一的形式来指定这些需求，但现有方法在处理嵌套的长视距任务和安全约束方面的能力有限，并且无法识别子目标无法满足以及应该寻求替代方案的情况。

Method: GenZ-LTL利用Büchi自动机的结构将LTL任务规范分解为reach-avoid子目标的序列，并通过适当的安全强化学习公式一次解决一个子目标。

Result: GenZ-LTL能够实现对任意LTL规范的零样本泛化，并且优于当前最先进的方法。

Conclusion: GenZ-LTL在零样本泛化到未见过的LTL规范方面，显著优于现有方法。

Abstract: Generalizing to complex and temporally extended task objectives and safety
constraints remains a critical challenge in reinforcement learning (RL). Linear
temporal logic (LTL) offers a unified formalism to specify such requirements,
yet existing methods are limited in their abilities to handle nested
long-horizon tasks and safety constraints, and cannot identify situations when
a subgoal is not satisfiable and an alternative should be sought. In this
paper, we introduce GenZ-LTL, a method that enables zero-shot generalization to
arbitrary LTL specifications. GenZ-LTL leverages the structure of B\"uchi
automata to decompose an LTL task specification into sequences of reach-avoid
subgoals. Contrary to the current state-of-the-art method that conditions on
subgoal sequences, we show that it is more effective to achieve zero-shot
generalization by solving these reach-avoid problems \textit{one subgoal at a
time} through proper safe RL formulations. In addition, we introduce a novel
subgoal-induced observation reduction technique that can mitigate the
exponential complexity of subgoal-state combinations under realistic
assumptions. Empirical results show that GenZ-LTL substantially outperforms
existing methods in zero-shot generalization to unseen LTL specifications.

</details>


### [125] [Polymorphic Combinatorial Frameworks (PCF): Guiding the Design of Mathematically-Grounded, Adaptive AI Agents](https://arxiv.org/abs/2508.01581)
*David Pearl,Matthew Murphy,James Intriligator*

Main category: cs.AI

TL;DR: PCF使用LLM和数学框架来设计自适应AI代理，并已通过超过125万次蒙特卡洛模拟进行了验证。


<details>
  <summary>Details</summary>
Motivation: 与静态代理架构不同，PCF通过数学组合空间实现实时参数重配置，从而使代理能够动态地调整其核心行为特征。

Method: PCF利用大型语言模型（LLM）和数学框架来指导解决方案空间和自适应AI代理的元提示设计，用于复杂、动态的环境。

Result: 结果揭示了代理在五个复杂性等级中的适应性和性能趋势，在较高复杂性等级中收益递减，从而突出了可扩展设计的阈值。

Conclusion: PCF实现了针对特定场景的优化代理配置生成，并保持逻辑一致性。该框架支持在客户服务、医疗保健、机器人和协作系统等领域中实现可扩展、动态、可解释和符合道德规范的AI应用，从而为适应性强且协作的下一代多态代理铺平了道路。

Abstract: The Polymorphic Combinatorial Framework (PCF) leverages Large Language Models
(LLMs) and mathematical frameworks to guide the meta-prompt enabled design of
solution spaces and adaptive AI agents for complex, dynamic environments.
Unlike static agent architectures, PCF enables real-time parameter
reconfiguration through mathematically-grounded combinatorial spaces, allowing
agents to adapt their core behavioral traits dynamically. Grounded in
combinatorial logic, topos theory, and rough fuzzy set theory, PCF defines a
multidimensional SPARK parameter space (Skills, Personalities, Approaches,
Resources, Knowledge) to capture agent behaviors. This paper demonstrates how
LLMs can parameterize complex spaces and estimate likely parameter
values/variabilities. Using PCF, we parameterized mock caf\'e domains (five
levels of complexity), estimated variables/variabilities, and conducted over
1.25 million Monte Carlo simulations. The results revealed trends in agent
adaptability and performance across the five complexity tiers, with diminishing
returns at higher complexity levels highlighting thresholds for scalable
designs. PCF enables the generation of optimized agent configurations for
specific scenarios while maintaining logical consistency. This framework
supports scalable, dynamic, explainable, and ethical AI applications in domains
like customer service, healthcare, robotics, and collaborative systems, paving
the way for adaptable and cooperative next-generation polymorphic agents.

</details>


### [126] [A Multi-Agent Pokemon Tournament for Evaluating Strategic Reasoning of Large Language Models](https://arxiv.org/abs/2508.01623)
*Tadisetty Sai Yashwanth,Dhatri C*

Main category: cs.AI

TL;DR: LLM Pokemon League, a tournament system using LLMs as agents in Pokemon battles, is presented as a benchmark for AI research in strategic reasoning and competitive learning.


<details>
  <summary>Details</summary>
Motivation: The research aims to analyze and compare the reasoning, adaptability, and tactical depth exhibited by different LLMs in a type-based, turn-based combat environment.

Method: A competitive tournament system is built that leverages Large Language Models (LLMs) as intelligent agents to simulate strategic decision-making in Pokemon battles. The competition is structured as a single-elimination tournament involving diverse AI trainers.

Result: The system captures detailed decision logs, including team-building rationale, action selection strategies, and switching decisions, enabling rich exploration into comparative AI behavior, battle psychology, and meta-strategy development.

Conclusion: LLM Pokemon League is introduced as a novel benchmark for AI research in strategic reasoning and competitive learning, showcasing how LLMs understand, adapt, and optimize decisions under uncertainty.

Abstract: This research presents LLM Pokemon League, a competitive tournament system
that leverages Large Language Models (LLMs) as intelligent agents to simulate
strategic decision-making in Pok\'emon battles. The platform is designed to
analyze and compare the reasoning, adaptability, and tactical depth exhibited
by different LLMs in a type-based, turn-based combat environment. By
structuring the competition as a single-elimination tournament involving
diverse AI trainers, the system captures detailed decision logs, including
team-building rationale, action selection strategies, and switching decisions.
The project enables rich exploration into comparative AI behavior, battle
psychology, and meta-strategy development in constrained, rule-based game
environments. Through this system, we investigate how modern LLMs understand,
adapt, and optimize decisions under uncertainty, making Pok\'emon League a
novel benchmark for AI research in strategic reasoning and competitive
learning.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [127] [DBAIOps: A Reasoning LLM-Enhanced Database Operation and Maintenance System using Knowledge Graphs](https://arxiv.org/abs/2508.01136)
*Wei Zhou,Peng Sun,Xuanhe Zhou,Qianglei Zang,Ji Xu,Tieying Zhang,Guoliang Li,Fan Wu*

Main category: cs.DB

TL;DR: DBAIOps is a new database O&M system that combines LLMs and knowledge graphs to improve diagnosis accuracy by leveraging expert experience more effectively.


<details>
  <summary>Details</summary>
Motivation: Existing automatic database O&M methods cannot effectively utilize expert experience, with rule-based methods limited to basic tasks and LLM-based methods generating inaccurate or generic results.

Method: A novel hybrid database O&M system combining reasoning LLMs with knowledge graphs, featuring a heterogeneous graph model for diagnosis experience, reusable anomaly models, and a two-stage graph evolution mechanism.

Result: DBAIOps demonstrates superior performance in root cause analysis and human evaluation accuracy compared to state-of-the-art baselines.

Conclusion: DBAIOps outperforms state-of-the-art baselines, achieving 34.85% and 47.22% higher accuracy in root cause and human evaluation, respectively, across four mainstream database systems (Oracle, MySQL, PostgreSQL, and DM8).

Abstract: The operation and maintenance (O&M) of database systems is critical to
ensuring system availability and performance, typically requiring expert
experience (e.g., identifying metric-to-anomaly relations) for effective
diagnosis and recovery. However, existing automatic database O&M methods,
including commercial products, cannot effectively utilize expert experience. On
the one hand, rule-based methods only support basic O&M tasks (e.g.,
metric-based anomaly detection), which are mostly numerical equations and
cannot effectively incorporate literal O&M experience (e.g., troubleshooting
guidance in manuals). On the other hand, LLM-based methods, which retrieve
fragmented information (e.g., standard documents + RAG), often generate
inaccurate or generic results. To address these limitations, we present
DBAIOps, a novel hybrid database O&M system that combines reasoning LLMs with
knowledge graphs to achieve DBA-style diagnosis. First, DBAIOps introduces a
heterogeneous graph model for representing the diagnosis experience, and
proposes a semi-automatic graph construction algorithm to build that graph from
thousands of documents. Second, DBAIOps develops a collection of (800+)
reusable anomaly models that identify both directly alerted metrics and
implicitly correlated experience and metrics. Third, for each anomaly, DBAIOps
proposes a two-stage graph evolution mechanism to explore relevant diagnosis
paths and identify missing relations automatically. It then leverages a
reasoning LLM (e.g., DeepSeek-R1) to infer root causes and generate clear
diagnosis reports for both DBAs and common users. Our evaluation over four
mainstream database systems (Oracle, MySQL, PostgreSQL, and DM8) demonstrates
that DBAIOps outperforms state-of-the-art baselines, 34.85% and 47.22% higher
in root cause and human evaluation accuracy, respectively.

</details>


### [128] [Balancing the Blend: An Experimental Analysis of Trade-offs in Hybrid Search](https://arxiv.org/abs/2508.01405)
*Mengzhao Wang,Boyu Tan,Yunjun Gao,Hai Jin,Yingfeng Zhang,Xiangyu Ke,Xiaoliang Xu,Yifan Zhu*

Main category: cs.DB

TL;DR: This paper benchmarks hybrid search architectures, revealing performance trade-offs and identifying Tensor-based Re-ranking Fusion as a promising alternative to mainstream fusion methods.


<details>
  <summary>Details</summary>
Motivation: A systematic, empirical understanding of the trade-offs among the core components of hybrid search systems is critically lacking.

Method: The paper presents a systematic benchmark of advanced hybrid search architectures, evaluating four retrieval paradigms (FTS, SVS, DVS, TenS) and benchmarking their combinations and re-ranking strategies across 11 real-world datasets.

Result: The paper reveals three key findings: (1) A 'weakest link' phenomenon. (2) A data-driven map of the performance trade-offs. (3) The identification of Tensor-based Re-ranking Fusion (TRF) as a high-efficacy alternative to mainstream fusion methods.

Conclusion: The paper offers concrete guidelines for designing the next generation of adaptive, scalable hybrid search systems and identifies key directions for future research.

Abstract: Hybrid search, the integration of lexical and semantic retrieval, has become
a cornerstone of modern information retrieval systems, driven by demanding
applications like Retrieval-Augmented Generation (RAG). The architectural
design space for these systems is vast and complex, yet a systematic, empirical
understanding of the trade-offs among their core components--retrieval
paradigms, combination schemes, and re-ranking methods--is critically lacking.
To address this, and informed by our experience building the Infinity
open-source database, we present the first systematic benchmark of advanced
hybrid search architectures. Our framework evaluates four retrieval
paradigms--Full-Text Search (FTS), Sparse Vector Search (SVS), Dense Vector
Search (DVS), and Tensor Search (TenS)--benchmarking their combinations and
re-ranking strategies across 11 real-world datasets. Our results reveal three
key findings for practitioners and researchers: (1) A "weakest link"
phenomenon, where a single underperforming retrieval path can
disproportionately degrade overall accuracy, highlighting the need for
path-wise quality assessment before fusion. (2) A data-driven map of the
performance trade-offs, demonstrating that optimal configurations depend
heavily on resource constraints and data characteristics, moving beyond a
one-size-fits-all approach. (3) The identification of Tensor-based Re-ranking
Fusion (TRF) as a high-efficacy alternative to mainstream fusion methods,
offering the semantic power of tensor search at a fraction of the computational
and memory cost. Our findings offer concrete guidelines for designing the next
generation of adaptive, scalable hybrid search systems while also identifying
key directions for future research.

</details>


### [129] [Marlin: Efficient Coordination for Autoscaling Cloud DBMS (Extended Version)](https://arxiv.org/abs/2508.01931)
*Wenjie Hu,Guanzhou Hu,Mahesh Balakrishnan,Xiangyao Yu*

Main category: cs.DB

TL;DR: Marlin disaggregates cluster coordination in cloud databases, improving cost efficiency and reducing reconfiguration duration by consolidating coordination functionality and using optimized commit protocol.


<details>
  <summary>Details</summary>
Motivation: Cloud databases rely on external, converged coordination services which face scalability bottlenecks, low cost efficiency, and increased operational burden as the control plane scales.

Method: Marlin, a cloud-native coordination mechanism that consolidates coordination functionality into the existing cloud-native database and employs transactions with MarlinCommit for data consistency.

Result: Marlin improves cost efficiency by up to 4.4x and reduces reconfiguration duration by up to 4.9x compared to converged coordination solutions.

Conclusion: Marlin improves cost efficiency and reduces reconfiguration duration compared to converged coordination solutions.

Abstract: Modern cloud databases are shifting from converged architectures to storage
disaggregation, enabling independent scaling and billing of compute and
storage. However, cloud databases still rely on external, converged
coordination services (e.g., ZooKeeper) for their control planes. These
services are effectively lightweight databases optimized for low-volume
metadata. As the control plane scales in the cloud, this approach faces similar
limitations as converged databases did before storage disaggregation:
scalability bottlenecks, low cost efficiency, and increased operational burden.
  We propose to disaggregate the cluster coordination to achieve the same
benefits that storage disaggregation brought to modern cloud DBMSs. We present
Marlin, a cloud-native coordination mechanism that fully embraces storage
disaggregation. Marlin eliminates the need for external coordination services
by consolidating coordination functionality into the existing cloud-native
database it manages. To achieve failover without an external coordination
service, Marlin allows cross-node modifications on coordination states. To
ensure data consistency, Marlin employs transactions to manage both
coordination and application states and introduces MarlinCommit, an optimized
commit protocol that ensures strong transactional guarantees even under
cross-node modifications. Our evaluations demonstrate that Marlin improves cost
efficiency by up to 4.4x and reduces reconfiguration duration by up to 4.9x
compared to converged coordination solutions.

</details>


### [130] [OnPair: Short Strings Compression for Fast Random Access](https://arxiv.org/abs/2508.02280)
*Francesco Gargiulo,Rossano Venturini*

Main category: cs.DB

TL;DR: OnPair是一种字典压缩算法，它在压缩率、速度和内存使用率方面优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的方法要么以显著的计算和内存成本（例如，BPE）来实现强大的压缩率，要么以牺牲压缩质量为代价来优先考虑速度（例如，FSST）。

Method: 采用了一种缓存友好的字典构建技术，该技术通过对数据样本进行一次连续扫描来增量合并频繁的相邻子字符串。

Result: 在真实数据集上的实验表明，OnPair和OnPair16的压缩率与BPE相当，同时显著提高了压缩速度和内存使用率。

Conclusion: OnPair和OnPair16的压缩率与BPE相当，同时显著提高了压缩速度和内存使用率。

Abstract: We present OnPair, a dictionary-based compression algorithm designed to meet
the needs of in-memory database systems that require both high compression and
fast random access. Existing methods either achieve strong compression ratios
at significant computational and memory cost (e.g., BPE) or prioritize speed at
the expense of compression quality (e.g., FSST). OnPair bridges this gap by
employing a cache-friendly dictionary construction technique that incrementally
merges frequent adjacent substrings in a single sequential pass over a data
sample. This enables fast, memory-efficient training without tracking global
pair positions, as required by traditional BPE. We also introduce OnPair16, a
variant that limits dictionary entries to 16 bytes, enabling faster parsing via
optimized longest prefix matching. Both variants compress strings
independently, supporting fine-grained random access without block-level
overhead. Experiments on real-world datasets show that OnPair and OnPair16
achieve compression ratios comparable to BPE while significantly improving
compression speed and memory usage.

</details>


### [131] [From Stimuli to Minds: Enhancing Psychological Reasoning in LLMs via Bilateral Reinforcement Learning](https://arxiv.org/abs/2508.02458)
*Yichao Feng*

Main category: cs.DB

TL;DR: This paper introduces a new framework to improve the ability of language models to understand implicit mental states. The framework uses reinforcement learning to imitate expert psychological thought patterns, allowing models to perform nuanced psychological inference and continual self-improvement.


<details>
  <summary>Details</summary>
Motivation: Large Language Models struggle with psychologically grounded tasks that require inferring implicit mental states in context-rich, ambiguous settings. These limitations arise from the absence of theory-aligned supervision and the difficulty of capturing nuanced mental processes in real-world narratives.

Method: a trajectory-aware reinforcement learning framework that explicitly imitates expert psychological thought patterns. By integrating real-world stimuli with structured reasoning guidance

Result: our approach enables compact models to internalize social-cognitive principles, perform nuanced psychological inference, and support continual self-improvement.

Conclusion: Models achieve expert-level interpretive capabilities, exhibiting strong out-of-distribution generalization and robust continual learning across diverse, challenging, and psychologically grounded tasks.

Abstract: Large Language Models show promise in emotion understanding, social
reasoning, and empathy, yet they struggle with psychologically grounded tasks
that require inferring implicit mental states in context-rich, ambiguous
settings. These limitations arise from the absence of theory-aligned
supervision and the difficulty of capturing nuanced mental processes in
real-world narratives. To address this gap, we leverage expert-labeled,
psychologically rich scenarios and propose a trajectory-aware reinforcement
learning framework that explicitly imitates expert psychological thought
patterns. By integrating real-world stimuli with structured reasoning guidance,
our approach enables compact models to internalize social-cognitive principles,
perform nuanced psychological inference, and support continual
self-improvement. Comprehensive experiments across multiple benchmarks further
demonstrate that our models achieve expert-level interpretive capabilities,
exhibiting strong out-of-distribution generalization and robust continual
learning across diverse, challenging, and psychologically grounded tasks.

</details>


### [132] [M2: An Analytic System with Specialized Storage Engines for Multi-Model Workloads](https://arxiv.org/abs/2508.02508)
*Kyoseung Koo,Bogyeong Kim,Bongki Moon*

Main category: cs.DB

TL;DR: M2, a multi-model analytic system with integrated storage engines, outperforms existing approaches by up to 188x speedup on multi-model analytics.


<details>
  <summary>Details</summary>
Motivation: Modern data analytic workloads increasingly require handling multiple data models simultaneously. Polyglot persistence employs a coordinator program to manage several independent database systems but suffers from high communication costs due to its physically disaggregated architecture. Meanwhile, existing multi-model database systems rely on a single storage engine optimized for a specific data model, resulting in inefficient processing across diverse data models.

Method: a multi-model analytic system with integrated storage engines. M2 treats all data models as first-class entities, composing query plans that incorporate operations across models. To effectively combine data from different models, the system introduces a specialized inter-model join algorithm called multi-stage hash join.

Result: M2 outperforms existing approaches by up to 188x speedup on multi-model analytics

Conclusion: M2 outperforms existing approaches by up to 188x speedup on multi-model analytics, confirming the effectiveness of our proposed techniques.

Abstract: Modern data analytic workloads increasingly require handling multiple data
models simultaneously. Two primary approaches meet this need: polyglot
persistence and multi-model database systems. Polyglot persistence employs a
coordinator program to manage several independent database systems but suffers
from high communication costs due to its physically disaggregated architecture.
Meanwhile, existing multi-model database systems rely on a single storage
engine optimized for a specific data model, resulting in inefficient processing
across diverse data models. To address these limitations, we present M2, a
multi-model analytic system with integrated storage engines. M2 treats all data
models as first-class entities, composing query plans that incorporate
operations across models. To effectively combine data from different models,
the system introduces a specialized inter-model join algorithm called
multi-stage hash join. Our evaluation demonstrates that M2 outperforms existing
approaches by up to 188x speedup on multi-model analytics, confirming the
effectiveness of our proposed techniques.

</details>


### [133] [The KG-ER Conceptual Schema Language](https://arxiv.org/abs/2508.02548)
*Enrico Franconi,Benoît Groz,Jan Hidders,Nina Pardal,Sławek Staworko,Jan Van den Bussche,Piotr Wieczorek*

Main category: cs.DB

TL;DR: KG-ER: a conceptual schema language for knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: describes the structure of knowledge graphs independently of their representation while helping to capture the semantics of the information stored in a knowledge graph

Method: KG-ER

Result: None

Conclusion: a conceptual schema language for knowledge graphs

Abstract: We propose KG-ER, a conceptual schema language for knowledge graphs that
describes the structure of knowledge graphs independently of their
representation (relational databases, property graphs, RDF) while helping to
capture the semantics of the information stored in a knowledge graph.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [134] [Addressing Cold Start For next-article Recommendation](https://arxiv.org/abs/2508.01036)
*Omar Elgohary,Nathan Jorgenson,Trenton Marple*

Main category: cs.IR

TL;DR: 本研究修改了 ALMM 模型，以解决 MIND 数据集上的新闻推荐问题。


<details>
  <summary>Details</summary>
Motivation: 旨在通过将此模型重构为序列新闻点击行为来提高冷启动场景中的推荐性能。

Method: 将 ALMM 模型重构为序列新闻点击行为，并将 BERT 和 TF-IDF 应用于新闻标题和摘要，以提取 token 上下文表示，并将其与基于三元组的用户阅读模式对齐。

Result: 使用 TF-IDF 的 ALMM 实现显示，在冷启动场景中，相对于 Forbes 和 Oord 基线模型，推荐准确性和鲁棒性相对提高。

Conclusion: ALMM 在经过最小修改后不适合用于下一篇新闻推荐。

Abstract: This replication study modifies ALMM, the Adaptive Linear Mapping Model
constructed for the next song recommendation, to the news recommendation
problem on the MIND dataset. The original version of ALMM computes latent
representations for users, last-time items, and current items in a tensor
factorization structure and learns a linear mapping from content features to
latent item vectors. Our replication aims to improve recommendation performance
in cold-start scenarios by restructuring this model to sequential news click
behavior, viewing consecutively read articles as (last news, next news) tuples.
Instead of the original audio features, we apply BERT and a TF-IDF (Term
Frequency-Inverse Document Frequency) to news titles and abstracts to extract
token contextualized representations and align them with triplet-based user
reading patterns. We also propose a reproducibly thorough pre-processing
pipeline combining news filtering and feature integrity validation. Our
implementation of ALMM with TF-IDF shows relatively improved recommendation
accuracy and robustness over Forbes and Oord baseline models in the cold-start
scenario. We demonstrate that ALMM in a minimally modified state is not
suitable for next news recommendation.

</details>


### [135] [Towards Bridging Review Sparsity in Recommendation with Textual Edge Graph Representation](https://arxiv.org/abs/2508.01128)
*Leyao Wang,Xutao Mao,Xuhui Zhan,Yuying Zhao,Bo Ni,Ryan A. Rossi,Nesreen K. Ahmed,Tyler Derr*

Main category: cs.IR

TL;DR: TWISTER is a framework that imputes missing reviews by jointly modeling semantic and structural signals using a Textual-Edge Graph and a large language model, improving recommendation performance.


<details>
  <summary>Details</summary>
Motivation: Real-world recommender systems suffer from review sparsity, undermining model effectiveness. Existing imputation techniques lose contextualized semantics or overlook structural dependencies.

Method: Represent user-item interactions as a Textual-Edge Graph (TEG), construct line-graph views, and employ a large language model as a graph-aware aggregator to impute missing reviews.

Result: TWISTER outperforms traditional numeric, graph-based, and LLM baselines on Amazon and Goodreads datasets, delivering higher-quality imputed reviews and enhanced recommendation performance.

Conclusion: TWISTER generates helpful, authentic, and specific reviews, smoothing structural signals for improved recommendations.

Abstract: Textual reviews enrich recommender systems with fine-grained preference
signals and enhanced explainability. However, in real-world scenarios, users
rarely leave reviews, resulting in severe sparsity that undermines the
effectiveness of existing models. A natural solution is to impute or generate
missing reviews to enrich the data. However, conventional imputation techniques
-- such as matrix completion and LLM-based augmentation -- either lose
contextualized semantics by embedding texts into vectors, or overlook
structural dependencies among user-item interactions. To address these
shortcomings, we propose TWISTER (ToWards Imputation on Sparsity with Textual
Edge Graph Representation), a unified framework that imputes missing reviews by
jointly modeling semantic and structural signals. Specifically, we represent
user-item interactions as a Textual-Edge Graph (TEG), treating reviews as edge
attributes. To capture relational context, we construct line-graph views and
employ a large language model as a graph-aware aggregator. For each interaction
lacking a textual review, our model aggregates the neighborhood's
natural-language representations to generate a coherent and personalized
review. Experiments on the Amazon and Goodreads datasets show that TWISTER
consistently outperforms traditional numeric, graph-based, and LLM baselines,
delivering higher-quality imputed reviews and, more importantly, enhanced
recommendation performance. In summary, TWISTER generates reviews that are more
helpful, authentic, and specific, while smoothing structural signals for
improved recommendations.

</details>


### [136] [CM$^3$: Calibrating Multimodal Recommendation](https://arxiv.org/abs/2508.01226)
*Xin Zhou,Yongjie Wang,Zhiqi Shen*

Main category: cs.IR

TL;DR: 该研究在多模态推荐系统中重新审视了对齐性和均匀性，提出了一种校准均匀性损失的方法，并在五个数据集上展示了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态推荐系统模型倾向于优先考虑均匀性而损害对齐性，并且通过均匀性损失公平对待项目的传统假设提出了挑战。

Method: 该研究利用项目多模态数据之间的内在相似性来校准其均匀性分布，并引入了一种球形贝塞尔方法，旨在整合任意数量的模态，同时确保融合后的特征约束在同一超球面流形上。

Result: 在五个真实世界数据集上进行的实证评估证实了该方法优于竞争基线。

Conclusion: 该研究通过校准均匀性分布并在嵌入空间中引入更强的相斥力，从而在多模态推荐系统中实现了优于现有模型的性能。通过集成 MLLM 提取的特征，所提出的方法在 NDCG@20 性能方面实现了高达 5.4% 的提升。

Abstract: Alignment and uniformity are fundamental principles within the domain of
contrastive learning. In recommender systems, prior work has established that
optimizing the Bayesian Personalized Ranking (BPR) loss contributes to the
objectives of alignment and uniformity. Specifically, alignment aims to draw
together the representations of interacting users and items, while uniformity
mandates a uniform distribution of user and item embeddings across a unit
hypersphere. This study revisits the alignment and uniformity properties within
the context of multimodal recommender systems, revealing a proclivity among
extant models to prioritize uniformity to the detriment of alignment. Our
hypothesis challenges the conventional assumption of equitable item treatment
through a uniformity loss, proposing a more nuanced approach wherein items with
similar multimodal attributes converge toward proximal representations within
the hyperspheric manifold. Specifically, we leverage the inherent similarity
between items' multimodal data to calibrate their uniformity distribution,
thereby inducing a more pronounced repulsive force between dissimilar entities
within the embedding space. A theoretical analysis elucidates the relationship
between this calibrated uniformity loss and the conventional uniformity
function. Moreover, to enhance the fusion of multimodal features, we introduce
a Spherical B\'ezier method designed to integrate an arbitrary number of
modalities while ensuring that the resulting fused features are constrained to
the same hyperspherical manifold. Empirical evaluations conducted on five
real-world datasets substantiate the superiority of our approach over competing
baselines. We also shown that the proposed methods can achieve up to a 5.4%
increase in NDCG@20 performance via the integration of MLLM-extracted features.
Source code is available at: https://github.com/enoche/CM3.

</details>


### [137] [A Study on Enhancing User Engagement by Employing Gamified Recommender Systems](https://arxiv.org/abs/2508.01265)
*Ali Fallahi,Azam Bastanfard,Amineh Amini,Hadi Saboohi*

Main category: cs.IR

TL;DR: 本文综述了游戏化推荐系统如何提升用户参与度，并为未来的研究提供了方向。


<details>
  <summary>Details</summary>
Motivation: 在现代商业世界中，提供定制化的产品和服务是提高用户体验和参与度的有效解决方案。游戏化推荐系统可以克服冷启动和数据不足的问题，并有效提高用户参与度。

Method: 通过比较不同的推荐系统构建方法，深入研究游戏化推荐系统。

Result: 对游戏化推荐系统的方法、局限性、评估指标、成就、数据集、领域和推荐技术进行了调查，全面分析了该主题的普及程度、差距和未开发的领域。

Conclusion: 本文对游戏化推荐系统如何提升用户参与度进行了全面的综述，并提出了未来研究方向。

Abstract: Providing customized products and services in the modern business world is
one of the most efficient solutions to improve users' experience and their
engagements with the industries. To aim, recommender systems, by producing
personalized recommendations, have a crucial role in the digital age. As a
consequence of modern improvements in the internet and online-based
technologies, using gamification rules also increased in various fields. Recent
studies showed that considering gamification concepts in implementing
recommendation systems not only can become helpful to overcome the cold start
and lack of sufficient data, moreover, can effectively improve user engagement.
Gamification can motivate individuals to have more activities on the system;
these interactions are valuable resources of data for recommender engines.
Unlike the past related works about using gamified recommendation systems in
different environments or studies that particularly surveyed gamification
strategies or recommenders separately, this work provides a comprehensive
review of how gamified recommender systems can enhance user engagement in
various domain applications. Furthermore, comparing different approaches for
building recommender systems is followed by in-depth surveying about
investigating the gamified recommender systems, including their approaches,
limitations, evaluation metrics, proposed achievements, datasets, domain areas,
and their recommendation techniques. This exhaustive analysis provides a
detailed picture of the topic's popularity, gaps, and unexplored regions. It is
envisaged that the proposed research and introduced possible future directions
would serve as a stepping stone for researchers interested in using gamified
recommender systems for user satisfaction and engagement.

</details>


### [138] [SaviorRec: Semantic-Behavior Alignment for Cold-Start Recommendation](https://arxiv.org/abs/2508.01375)
*Yining Yao,Ziwei Li,Shuwen Xiao,Boya Du,Jialin Zhu,Junjun Zheng,Xiangheng Kong,Yuning Jiang*

Main category: cs.IR

TL;DR: This paper introduces a lightweight Semantic-Behavior Alignment framework to improve CTR prediction for cold-start items by aligning multimodal representations with user behavior space, achieving significant performance gains on Taobao.


<details>
  <summary>Details</summary>
Motivation: Improving recommendation performance for cold-start and long-tail items by leveraging item multimodal features is challenging due to the high computational cost of training complex pre-trained encoders. Maintaining alignment between semantic and behavior space in a lightweight way is important.

Method: A Semantic-Behavior Alignment framework is proposed, utilizing domain-specific knowledge to train a multimodal encoder and residual quantized semantic ID to bridge the gap between multimodal representations and the ranking model.

Result: Offline AUC increased by 0.83%, online clicks increased by 13.21%, and orders increased by 13.44% in the A/B test on Taobao.

Conclusion: The proposed Semantic-Behavior Alignment framework improves CTR prediction, especially for cold-start items, as demonstrated by offline AUC increase and online A/B testing gains on Taobao.

Abstract: In recommendation systems, predicting Click-Through Rate (CTR) is crucial for
accurately matching users with items. To improve recommendation performance for
cold-start and long-tail items, recent studies focus on leveraging item
multimodal features to model users' interests. However, obtaining multimodal
representations for items relies on complex pre-trained encoders, which incurs
unacceptable computation cost to train jointly with downstream ranking models.
Therefore, it is important to maintain alignment between semantic and behavior
space in a lightweight way.
  To address these challenges, we propose a Semantic-Behavior Alignment for
Cold-start Recommendation framework, which mainly focuses on utilizing
multimodal representations that align with the user behavior space to predict
CTR. First, we leverage domain-specific knowledge to train a multimodal encoder
to generate behavior-aware semantic representations. Second, we use residual
quantized semantic ID to dynamically bridge the gap between multimodal
representations and the ranking model, facilitating the continuous
semantic-behavior alignment. We conduct our offline and online experiments on
the Taobao, one of the world's largest e-commerce platforms, and have achieved
an increase of 0.83% in offline AUC, 13.21% clicks increase and 13.44% orders
increase in the online A/B test, emphasizing the efficacy of our method.

</details>


### [139] [Req-Rec: Enhancing Requirements Elicitation for Increasing Stakeholder's Satisfaction Using a Collaborative Filtering Based Recommender System](https://arxiv.org/abs/2508.01502)
*Ali Fallahi,Amineh Amini,Azam Bastanfard,Hadi Saboohi*

Main category: cs.IR

TL;DR: A new hybrid recommender system, Req-Rec, is proposed to increase stakeholder satisfaction by assisting them in the requirement elicitation phase.


<details>
  <summary>Details</summary>
Motivation: Choosing the proper elicitation technique was always a considerable challenge for efficient requirement engineering. Recommender systems have become an efficient channel for making a deeply personalized interactive communication with stakeholders.

Method: A hybrid recommender system based on the collaborative filtering approach and the repertory grid technique.

Result: The method efficiently could overcome weaknesses of common requirement elicitation techniques, such as time limitation, location-based restrictions, and bias in requirements' elicitation process.

Conclusion: The proposed Req-Rec method efficiently overcomes weaknesses of common requirement elicitation techniques and assists stakeholders in becoming more aware of different aspects of the project.

Abstract: The success or failure of a project is highly related to recognizing the
right stakeholders and accurately finding and discovering their requirements.
However, choosing the proper elicitation technique was always a considerable
challenge for efficient requirement engineering. As a consequence of the swift
improvement of digital technologies since the past decade, recommender systems
have become an efficient channel for making a deeply personalized interactive
communication with stakeholders. In this research, a new method, called the
Req-Rec (Requirements Recommender), is proposed. It is a hybrid recommender
system based on the collaborative filtering approach and the repertory grid
technique as the core component. The primary goal of Req-Rec is to increase
stakeholder satisfaction by assisting them in the requirement elicitation
phase. Based on the results, the method efficiently could overcome weaknesses
of common requirement elicitation techniques, such as time limitation,
location-based restrictions, and bias in requirements' elicitation process.
Therefore, recommending related requirements assists stakeholders in becoming
more aware of different aspects of the project.

</details>


### [140] [End-to-End Personalization: Unifying Recommender Systems with Large Language Models](https://arxiv.org/abs/2508.01514)
*Danial Ebrat,Tina Aminian,Sepideh Ahmadian,Luis Rueda*

Main category: cs.IR

TL;DR: hybrid recommendation framework that combines Graph Attention Networks (GATs) with Large Language Models (LLMs) to improve both the accuracy and interpretability of recommender systems


<details>
  <summary>Details</summary>
Motivation: improving both personalization and interpretability remains a challenge, particularly in scenarios involving limited user feedback or heterogeneous item attributes

Method: a novel hybrid recommendation framework that combines Graph Attention Networks (GATs) with Large Language Models (LLMs)

Result: evaluated our model on benchmark datasets, including MovieLens 100k and 1M, where it consistently outperforms strong baselines. Ablation studies confirm that LLM-based embeddings and the cosine similarity term significantly contribute to performance gains.

Conclusion: This work demonstrates the potential of integrating LLMs to improve both the accuracy and interpretability of recommender systems.

Abstract: Recommender systems are essential for guiding users through the vast and
diverse landscape of digital content by delivering personalized and relevant
suggestions. However, improving both personalization and interpretability
remains a challenge, particularly in scenarios involving limited user feedback
or heterogeneous item attributes. In this article, we propose a novel hybrid
recommendation framework that combines Graph Attention Networks (GATs) with
Large Language Models (LLMs) to address these limitations. LLMs are first used
to enrich user and item representations by generating semantically meaningful
profiles based on metadata such as titles, genres, and overviews. These
enriched embeddings serve as initial node features in a user and movie
bipartite graph, which is processed using a GAT based collaborative filtering
model. To enhance ranking accuracy, we introduce a hybrid loss function that
combines Bayesian Personalized Ranking (BPR), cosine similarity, and robust
negative sampling. Post-processing involves reranking the GAT-generated
recommendations using the LLM, which also generates natural-language
justifications to improve transparency. We evaluated our model on benchmark
datasets, including MovieLens 100k and 1M, where it consistently outperforms
strong baselines. Ablation studies confirm that LLM-based embeddings and the
cosine similarity term significantly contribute to performance gains. This work
demonstrates the potential of integrating LLMs to improve both the accuracy and
interpretability of recommender systems.

</details>


### [141] [ChEmbed: Enhancing Chemical Literature Search Through Domain-Specific Text Embeddings](https://arxiv.org/abs/2508.01643)
*Ali Shiraee Kasmaee,Mohammad Khodadad,Mehdi Astaraki,Mohammad Arshi Saloot,Nicholas Sherck,Hamidreza Mahyar,Soheila Samiee*

Main category: cs.IR

TL;DR: ChEmbed, a domain-adapted text embedding model, improves chemical literature retrieval by fine-tuning on chemical text, generating synthetic queries, adding chemical tokens, and maintaining a long context length.


<details>
  <summary>Details</summary>
Motivation: General-purpose text embedding models frequently fail to adequately represent complex chemical terminologies, resulting in suboptimal retrieval quality. Specialized embedding models tailored to chemical literature retrieval have not yet been developed, leaving a substantial performance gap.

Method: a domain-adapted family of text embedding models fine-tuned on a dataset comprising chemistry-specific text from the PubChem, Semantic Scholar, and ChemRxiv corpora. We employ large language models to synthetically generate queries, resulting in approximately 1.7 million high-quality query-passage pairs. Additionally, we augment the tokenizer by adding 900 chemically specialized tokens to previously unused slots, which significantly reduces the fragmentation of chemical entities, such as IUPAC names. ChEmbed also maintains a 8192-token context length

Result: ChEmbed outperforms state-of-the-art general embedding models, raising nDCG@10 from 0.82 to 0.91 (+9 pp) on our newly introduced ChemRxiv Retrieval benchmark.

Conclusion: ChEmbed is a practical, lightweight, and reproducible embedding solution that effectively improves retrieval for chemical literature search.

Abstract: Retrieval-Augmented Generation (RAG) systems in chemistry heavily depend on
accurate and relevant retrieval of chemical literature. However,
general-purpose text embedding models frequently fail to adequately represent
complex chemical terminologies, resulting in suboptimal retrieval quality.
Specialized embedding models tailored to chemical literature retrieval have not
yet been developed, leaving a substantial performance gap. To address this
challenge, we introduce ChEmbed, a domain-adapted family of text embedding
models fine-tuned on a dataset comprising chemistry-specific text from the
PubChem, Semantic Scholar, and ChemRxiv corpora. To create effective training
data, we employ large language models to synthetically generate queries,
resulting in approximately 1.7 million high-quality query-passage pairs.
Additionally, we augment the tokenizer by adding 900 chemically specialized
tokens to previously unused slots, which significantly reduces the
fragmentation of chemical entities, such as IUPAC names. ChEmbed also maintains
a 8192-token context length, enabling the efficient retrieval of longer
passages compared to many other open-source embedding models, which typically
have a context length of 512 or 2048 tokens. Evaluated on our newly introduced
ChemRxiv Retrieval benchmark, ChEmbed outperforms state-of-the-art general
embedding models, raising nDCG@10 from 0.82 to 0.91 (+9 pp). ChEmbed represents
a practical, lightweight, and reproducible embedding solution that effectively
improves retrieval for chemical literature search.

</details>


### [142] [Counterfactual Reciprocal Recommender Systems for User-to-User Matching](https://arxiv.org/abs/2508.01867)
*Kazuki Kawamura,Takuma Udagawa,Kei Tateno*

Main category: cs.IR

TL;DR: CFRR is a causal framework that mitigates bias in reciprocal recommender systems by using inverse propensity scoring, leading to improved accuracy, fairness, and coverage of long-tail users.


<details>
  <summary>Details</summary>
Motivation: Logged data over-represents popular profiles due to past exposure policies, creating feedback loops that skew learning and fairness in reciprocal recommender systems.

Method: CFRR uses inverse propensity scored, self-normalized objectives.

Result: CFRR improves NDCG@10 by up to 3.5%, increases long-tail user coverage by up to 51%, and reduces Gini exposure inequality by up to 24%.

Conclusion: CFRR offers a promising approach for more accurate and fair user-to-user matching.

Abstract: Reciprocal recommender systems (RRS) in dating, gaming, and talent platforms
require mutual acceptance for a match. Logged data, however, over-represents
popular profiles due to past exposure policies, creating feedback loops that
skew learning and fairness. We introduce Counterfactual Reciprocal Recommender
Systems (CFRR), a causal framework to mitigate this bias. CFRR uses inverse
propensity scored, self-normalized objectives. Experiments show CFRR improves
NDCG@10 by up to 3.5% (e.g., from 0.459 to 0.475 on DBLP, from 0.299 to 0.307
on Synthetic), increases long-tail user coverage by up to 51% (from 0.504 to
0.763 on Synthetic), and reduces Gini exposure inequality by up to 24% (from
0.708 to 0.535 on Synthetic). CFRR offers a promising approach for more
accurate and fair user-to-user matching.

</details>


### [143] [Evaluating Position Bias in Large Language Model Recommendations](https://arxiv.org/abs/2508.02020)
*Ethan Bito,Yongli Ren,Estrid He*

Main category: cs.IR

TL;DR: LLM 推荐模型存在位置偏差，论文提出了一种新的提示策略 RISE 来减轻这种偏差，实验表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 正越来越多地被探索为推荐任务的通用工具，无需特定于任务的训练即可实现零样本和指令跟随能力。然而，直接将 LLM 应用于推荐任务存在重要的注意事项，LLM 推荐模型存在位置偏差，其中 prompt 中候选项目的顺序可能会不成比例地影响 LLM 产生的推荐。

Method: 提出了一种新的提示策略，称为通过迭代选择进行排序 (RISE)。

Result: 在真实世界的数据集上分析了基于 LLM 的推荐的位置偏差，结果揭示了 LLM 的系统偏差，其对输入顺序高度敏感。提出的 RISE 方法在关键基准数据集上与各种基线方法进行了比较，实验结果表明，该方法降低了对输入顺序的敏感性并提高了稳定性，而无需模型微调或后处理。

Conclusion: 该论文提出了一种新的提示策略，称为通过迭代选择进行排序 (RISE)，以减轻 LLM 推荐模型的位置偏差。实验结果表明，该方法降低了对输入顺序的敏感性并提高了稳定性，而无需模型微调或后处理。

Abstract: Large Language Models (LLMs) are being increasingly explored as
general-purpose tools for recommendation tasks, enabling zero-shot and
instruction-following capabilities without the need for task-specific training.
While the research community is enthusiastically embracing LLMs, there are
important caveats to directly adapting them for recommendation tasks. In this
paper, we show that LLM-based recommendation models suffer from position bias,
where the order of candidate items in a prompt can disproportionately influence
the recommendations produced by LLMs. First, we analyse the position bias of
LLM-based recommendations on real-world datasets, where results uncover
systemic biases of LLMs with high sensitivity to input orders. Furthermore, we
introduce a new prompting strategy to mitigate the position bias of LLM
recommendation models called Ranking via Iterative SElection (RISE). We compare
our proposed method against various baselines on key benchmark datasets.
Experiment results show that our method reduces sensitivity to input ordering
and improves stability without requiring model fine-tuning or post-processing.

</details>


### [144] [Why Generate When You Can Transform? Unleashing Generative Attention for Dynamic Recommendation](https://arxiv.org/abs/2508.02050)
*Yuli Liu,Wenjun Kong,Cheng Luo,Weizhi Ma*

Main category: cs.IR

TL;DR: 提出生成式注意力机制，以更好地捕捉用户偏好的动态性和非线性，实验表明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的注意力机制是线性和确定性的，限制了它们捕捉用户偏好的动态和非线性性质的能力。

Method: 提出了两种生成式注意力模型，分别基于变分自编码器（VAE）和扩散模型（DMs）。

Result: 在真实数据集上的大量实验表明，该模型在准确性和多样性方面均显着优于现有技术。

Conclusion: 提出了两种用于SR的生成式注意力模型，分别基于VAE和DMs，并在真实数据集上表现优于现有技术。

Abstract: Sequential Recommendation (SR) focuses on personalizing user experiences by
predicting future preferences based on historical interactions. Transformer
models, with their attention mechanisms, have become the dominant architecture
in SR tasks due to their ability to capture dependencies in user behavior
sequences. However, traditional attention mechanisms, where attention weights
are computed through query-key transformations, are inherently linear and
deterministic. This fixed approach limits their ability to account for the
dynamic and non-linear nature of user preferences, leading to challenges in
capturing evolving interests and subtle behavioral patterns. Given that
generative models excel at capturing non-linearity and probabilistic
variability, we argue that generating attention distributions offers a more
flexible and expressive alternative compared to traditional attention
mechanisms. To support this claim, we present a theoretical proof demonstrating
that generative attention mechanisms offer greater expressiveness and
stochasticity than traditional deterministic approaches. Building upon this
theoretical foundation, we introduce two generative attention models for SR,
each grounded in the principles of Variational Autoencoders (VAE) and Diffusion
Models (DMs), respectively. These models are designed specifically to generate
adaptive attention distributions that better align with variable user
preferences. Extensive experiments on real-world datasets show our models
significantly outperform state-of-the-art in both accuracy and diversity.

</details>


### [145] [Evaluating User Experience in Conversational Recommender Systems: A Systematic Review Across Classical and LLM-Powered Approaches](https://arxiv.org/abs/2508.02096)
*Raj Mahmud,Yufeng Wu,Abdullah Bin Sawad,Shlomo Berkovsky,Mukesh Prasad,A. Baki Kocaballi*

Main category: cs.IR

TL;DR: 对会话式推荐系统的用户体验评估进行了系统性回顾，发现了一些局限性，并为未来的LLM感知用户体验评估提出了建议。


<details>
  <summary>Details</summary>
Motivation: 会话推荐系统（CRS）在各个领域受到越来越多的研究关注，但其用户体验（UX）评估仍然有限。现有的评论在很大程度上忽略了实证用户体验研究，尤其是在自适应和基于大型语言模型（LLM）的CRS中。

Method: 对2017年至2025年间发表的23项实证研究进行了系统回顾，遵循PRISMA指南。

Result: 研究结果揭示了持续存在的局限性：事后调查占主导地位，很少评估turn-level情感用户体验结构，并且自适应行为很少与用户体验结果相关联。基于LLM的CRS引入了进一步的挑战，包括认知不透明性和冗长性，但评估很少解决这些问题。

Conclusion: 总结了用户体验指标，对自适应和非自适应系统进行了比较分析，并为LLM感知用户体验评估提出了前瞻性议程。这些发现支持开发更透明、更具吸引力且以用户为中心的CRS评估实践。

Abstract: Conversational Recommender Systems (CRSs) are receiving growing research
attention across domains, yet their user experience (UX) evaluation remains
limited. Existing reviews largely overlook empirical UX studies, particularly
in adaptive and large language model (LLM)-based CRSs. To address this gap, we
conducted a systematic review following PRISMA guidelines, synthesising 23
empirical studies published between 2017 and 2025. We analysed how UX has been
conceptualised, measured, and shaped by domain, adaptivity, and LLM.
  Our findings reveal persistent limitations: post hoc surveys dominate,
turn-level affective UX constructs are rarely assessed, and adaptive behaviours
are seldom linked to UX outcomes. LLM-based CRSs introduce further challenges,
including epistemic opacity and verbosity, yet evaluations infrequently address
these issues. We contribute a structured synthesis of UX metrics, a comparative
analysis of adaptive and nonadaptive systems, and a forward-looking agenda for
LLM-aware UX evaluation. These findings support the development of more
transparent, engaging, and user-centred CRS evaluation practices.

</details>


### [146] [FinCPRG: A Bidirectional Generation Pipeline for Hierarchical Queries and Rich Relevance in Financial Chinese Passage Retrieval](https://arxiv.org/abs/2508.02222)
*Xuan Xu,Beilin Chu,Qinhong Lin,Yixiao Zhong,Fufang Wen,Jiaqi Liu,Binjie Fei,Yu Li,Zhongliang Yang,Linna Zhou*

Main category: cs.IR

TL;DR: This paper introduces a new pipeline to generate a financial passage retrieval dataset (FinCPRG) with hierarchical queries and rich relevance labels, addressing limitations in existing methods for cross-doc query needs and annotation quality control.


<details>
  <summary>Details</summary>
Motivation: existing methods still face limitations in expressing cross-doc query needs and controlling annotation quality

Method: a bidirectional generation pipeline, which aims to generate 3-level hierarchical queries for both intra-doc and cross-doc scenarios and mine additional relevance labels on top of direct mapping annotation. The pipeline introduces two query generation methods: bottom-up from single-doc text and top-down from multi-doc titles.

Result: constructed a Financial Passage Retrieval Generated dataset (FinCPRG) from almost 1.3k Chinese financial research reports, which includes hierarchical queries and rich relevance labels

Conclusion: We assessed the quality of FinCPRG and validated its effectiveness as a passage retrieval dataset for both training and benchmarking.

Abstract: In recent years, large language models (LLMs) have demonstrated significant
potential in constructing passage retrieval datasets. However, existing methods
still face limitations in expressing cross-doc query needs and controlling
annotation quality. To address these issues, this paper proposes a
bidirectional generation pipeline, which aims to generate 3-level hierarchical
queries for both intra-doc and cross-doc scenarios and mine additional
relevance labels on top of direct mapping annotation. The pipeline introduces
two query generation methods: bottom-up from single-doc text and top-down from
multi-doc titles. The bottom-up method uses LLMs to disassemble and generate
structured queries at both sentence-level and passage-level simultaneously from
intra-doc passages. The top-down approach incorporates three key financial
elements--industry, topic, and time--to divide report titles into clusters and
prompts LLMs to generate topic-level queries from each cluster. For relevance
annotation, our pipeline not only relies on direct mapping annotation from the
generation relationship but also implements an indirect positives mining method
to enrich the relevant query-passage pairs. Using this pipeline, we constructed
a Financial Passage Retrieval Generated dataset (FinCPRG) from almost 1.3k
Chinese financial research reports, which includes hierarchical queries and
rich relevance labels. Through evaluations of mined relevance labels,
benchmarking and training experiments, we assessed the quality of FinCPRG and
validated its effectiveness as a passage retrieval dataset for both training
and benchmarking.

</details>


### [147] [From Generation to Consumption: Personalized List Value Estimation for Re-ranking](https://arxiv.org/abs/2508.02242)
*Kaike Zhang,Xiaobei Wang,Xiaoyu Liu,Shuchang Liu,Hailan Yang,Xiang Li,Fei Sun,Qi Cao*

Main category: cs.IR

TL;DR: CAVE 是一种个性化的消费感知列表值估计框架，通过显式建模用户退出行为来更准确地估计列表消费价值，并在多个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常忽略了用户可能在消费完整列表之前退出的事实，从而导致估计的生成值与实际消费值之间的不匹配。

Method: CAVE 框架，它将列表值公式化为子列表值的期望，并由每个位置的用户特定退出概率加权。退出概率被分解为兴趣驱动的成分和随机成分，后者通过 Weibull 分布建模以捕获随机外部因素，例如疲劳。

Result: CAVE 在大规模真实列表基准测试和在线 A/B 测试中优于强大的基线。

Conclusion: CAVE在多个数据集和在线A/B测试中始终优于强大的基线，突出了在重排序中显式建模用户退出的好处。

Abstract: Re-ranking is critical in recommender systems for optimizing the order of
recommendation lists, thus improving user satisfaction and platform revenue.
Most existing methods follow a generator-evaluator paradigm, where the
evaluator estimates the overall value of each candidate list. However, they
often ignore the fact that users may exit before consuming the full list,
leading to a mismatch between estimated generation value and actual consumption
value. To bridge this gap, we propose CAVE, a personalized Consumption-Aware
list Value Estimation framework. CAVE formulates the list value as the
expectation over sub-list values, weighted by user-specific exit probabilities
at each position. The exit probability is decomposed into an interest-driven
component and a stochastic component, the latter modeled via a Weibull
distribution to capture random external factors such as fatigue. By jointly
modeling sub-list values and user exit behavior, CAVE yields a more faithful
estimate of actual list consumption value. We further contribute three
large-scale real-world list-wise benchmarks from the Kuaishou platform, varying
in size and user activity patterns. Extensive experiments on these benchmarks,
two Amazon datasets, and online A/B testing on Kuaishou show that CAVE
consistently outperforms strong baselines, highlighting the benefit of
explicitly modeling user exits in re-ranking.

</details>


### [148] [Voronoi Diagram Encoded Hashing](https://arxiv.org/abs/2508.02266)
*Yang Xu,Kai Ming Ting*

Main category: cs.IR

TL;DR: This paper proposes Voronoi Diagram Encoded Hashing (VDeH), a simple and efficient no-learning binary hashing method, which achieves superior performance and lower computational cost compared to existing state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing L2H methods are constrained to three types of hash functions and a recent study shows that even using a version of locality sensitive hashing functions without learning achieves comparable accuracy as those of L2H, but with less time cost.

Method: Voronoi Diagram Encoded Hashing (VDeH), which constructs a set of hash functions through a data-dependent similarity measure and produces independent binary bits through encoded hashing.

Result: VDeH achieves superior performance and lower computational cost compared to existing state-of-the-art methods under the same bit length.

Conclusion: VDeH achieves superior performance and lower computational cost compared to existing state-of-the-art methods under the same bit length.

Abstract: The goal of learning to hash (L2H) is to derive data-dependent hash functions
from a given data distribution in order to map data from the input space to a
binary coding space. Despite the success of L2H, two observations have cast
doubt on the source of the power of L2H, i.e., learning. First, a recent study
shows that even using a version of locality sensitive hashing functions without
learning achieves binary representations that have comparable accuracy as those
of L2H, but with less time cost. Second, existing L2H methods are constrained
to three types of hash functions: thresholding, hyperspheres, and hyperplanes
only. In this paper, we unveil the potential of Voronoi diagrams in hashing.
Voronoi diagram is a suitable candidate because of its three properties. This
discovery has led us to propose a simple and efficient no-learning binary
hashing method, called Voronoi Diagram Encoded Hashing (VDeH), which constructs
a set of hash functions through a data-dependent similarity measure and
produces independent binary bits through encoded hashing. We demonstrate
through experiments on several benchmark datasets that VDeH achieves superior
performance and lower computational cost compared to existing state-of-the-art
methods under the same bit length.

</details>


### [149] [Research Knowledge Graphs in NFDI4DataScience: Key Activities, Achievements, and Future Directions](https://arxiv.org/abs/2508.02300)
*Kanishka Silva,Marcel R. Ackermann,Heike Fliegl,Genet-Asefa Gesese,Fidan Limani,Philipp Mayr,Peter Mutschke,Allard Oelen,Muhammad Asif Suryani,Sharmila Upadhyaya,Benjamin Zapilko,Harald Sack,Stefan Dietze*

Main category: cs.IR

TL;DR: NFDI4DataScience 联盟正在构建研究知识图谱以提高人工智能和数据科学研究的可访问性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 确保人工智能和数据科学研究的透明度、可重复性和可发现性面临挑战。

Method: 使用标准化本体、共享词汇表和自动化信息提取技术创建语义丰富的 RKG。

Result: 开发了 NFDI4DS 本体、元数据标准、工具和服务，以支持 FAIR 原则，以及社区主导的项目和 RKG 的各种实现。

Conclusion: NFDI4DataScience 联盟正在开发和提供研究知识图谱 (RKG)，旨在捕获和连接数据集、模型、软件和科学出版物之间复杂的关系。

Abstract: As research in Artificial Intelligence and Data Science continues to grow in
volume and complexity, it becomes increasingly difficult to ensure
transparency, reproducibility, and discoverability. To address these
challenges, as research artifacts should be understandable and usable by
machines, the NFDI4DataScience consortium is developing and providing Research
Knowledge Graphs (RKGs). Building upon earlier works, this paper presents
recent progress in creating semantically rich RKGs using standardized
ontologies, shared vocabularies, and automated Information Extraction
techniques. Key achievements include the development of the NFDI4DS ontology,
metadata standards, tools, and services designed to support the FAIR
principles, as well as community-led projects and various implementations of
RKGs. Together, these efforts aim to capture and connect the complex
relationships between datasets, models, software, and scientific publications.

</details>


### [150] [Agentic Personalized Fashion Recommendation in the Age of Generative AI: Challenges, Opportunities, and Evaluation](https://arxiv.org/abs/2508.02342)
*Yashar Deldjoo,Nima Rafiee,Mahdyar Ravanbakhsh*

Main category: cs.IR

TL;DR: 本文概述了工业 FaRS 的研究议程，专注于跨越静态查询、服装组合和多轮对话的五个代表性场景，并提出混合模态细化对于实际部署至关重要。


<details>
  <summary>Details</summary>
Motivation: 由于快速的趋势变化、细致的用户偏好、复杂的物品-物品兼容性以及消费者、品牌和影响者之间复杂的相互作用，时尚推荐系统（FaRS）面临着独特的挑战。

Method: 提出了一个Agentic混合模态细化（AMMR）管道，该管道将多模态编码器与代理LLM规划器和动态检索融合。

Result: 论文综合了学术界和工业界的观点，以描绘现代FaRS的独特输出空间和利益相关者生态系统，确定了用户、品牌、平台和影响者之间复杂的相互作用，并强调了由此产生的独特数据和建模挑战。

Conclusion: 转向自适应、生成和利益相关者感知的系统对于满足时尚消费者和品牌不断发展的期望至关重要。

Abstract: Fashion recommender systems (FaRS) face distinct challenges due to rapid
trend shifts, nuanced user preferences, intricate item-item compatibility, and
the complex interplay among consumers, brands, and influencers. Traditional
recommendation approaches, largely static and retrieval-focused, struggle to
effectively capture these dynamic elements, leading to decreased user
satisfaction and elevated return rates. This paper synthesizes both academic
and industrial viewpoints to map the distinctive output space and stakeholder
ecosystem of modern FaRS, identifying the complex interplay among users,
brands, platforms, and influencers, and highlighting the unique data and
modeling challenges that arise.
  We outline a research agenda for industrial FaRS, centered on five
representative scenarios spanning static queries, outfit composition, and
multi-turn dialogue, and argue that mixed-modality refinement-the ability to
combine image-based references (anchors) with nuanced textual constraints-is a
particularly critical task for real-world deployment. To this end, we propose
an Agentic Mixed-Modality Refinement (AMMR) pipeline, which fuses multimodal
encoders with agentic LLM planners and dynamic retrieval, bridging the gap
between expressive user intent and fast-changing fashion inventories. Our work
shows that moving beyond static retrieval toward adaptive, generative, and
stakeholder-aware systems is essential to satisfy the evolving expectations of
fashion consumers and brands.

</details>


### [151] [Beyond Chunks and Graphs: Retrieval-Augmented Generation through Triplet-Driven Thinking](https://arxiv.org/abs/2508.02435)
*Shengbo Gong,Xianfeng Tang,Carl Yang,Wei jin*

Main category: cs.IR

TL;DR: T$^2$RAG 优于其他 RAG 方法，提高了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 高级 RAG 系统面临性能和效率之间的权衡。多轮 RAG 方法实现了强大的推理能力，但会导致过多的 LLM 调用和 token 成本，而图 RAG 方法则面临计算量大、容易出错的图构建和检索冗余问题。

Method: T$^2$RAG，一种新颖的框架，它在简单的、无图的三元组原子知识库上运行。T$^2$RAG 利用 LLM 将问题分解为带有占位符的可搜索三元组，然后通过从三元组数据库中检索证据来迭代地解决这些问题。

Result: T$^2$RAG 的平均性能提升高达 11%，同时降低了高达 45% 的检索成本。

Conclusion: T$^2$RAG显著优于最先进的多轮和图 RAG 方法，在六个数据集上的平均性能提升高达 11%，同时降低了高达 45% 的检索成本。

Abstract: Retrieval-augmented generation (RAG) is critical for reducing hallucinations
and incorporating external knowledge into Large Language Models (LLMs).
However, advanced RAG systems face a trade-off between performance and
efficiency. Multi-round RAG approaches achieve strong reasoning but incur
excessive LLM calls and token costs, while Graph RAG methods suffer from
computationally expensive, error-prone graph construction and retrieval
redundancy. To address these challenges, we propose T$^2$RAG, a novel framework
that operates on a simple, graph-free knowledge base of atomic triplets.
T$^2$RAG leverages an LLM to decompose questions into searchable triplets with
placeholders, which it then iteratively resolves by retrieving evidence from
the triplet database. Empirical results show that T$^2$RAG significantly
outperforms state-of-the-art multi-round and Graph RAG methods, achieving an
average performance gain of up to 11\% across six datasets while reducing
retrieval costs by up to 45\%. Our code is available at
https://github.com/rockcor/T2RAG

</details>


### [152] [Dynamic Forgetting and Spatio-Temporal Periodic Interest Modeling for Local-Life Service Recommendation](https://arxiv.org/abs/2508.02451)
*Zhaoyu Hu,Hao Guo,Yuan Tian,Erpeng Xue,Jianyang Wang,Xianyang Qi,Hongxiang Lin,Lei Wang,Sheng Chen*

Main category: cs.IR

TL;DR: This paper introduces the forgetting curve and proposes Spatio-Temporal periodic Interest Modeling (STIM) with long sequences for local-life service recommendation.


<details>
  <summary>Details</summary>
Motivation: modeling user behavior sequences on local-life service platforms, including the sparsity of long sequences and strong spatio-temporal dependence

Method: Spatio-Temporal periodic Interest Modeling (STIM)

Result: achieved a 1.54% improvement in gross transaction volume (GTV)

Conclusion: STIM has been deployed in a large-scale local-life service recommendation system, serving hundreds of millions of daily active users in core application scenarios.

Abstract: In the context of the booming digital economy, recommendation systems, as a
key link connecting users and numerous services, face challenges in modeling
user behavior sequences on local-life service platforms, including the sparsity
of long sequences and strong spatio-temporal dependence. Such challenges can be
addressed by drawing an analogy to the forgetting process in human memory. This
is because users' responses to recommended content follow the recency effect
and the cyclicality of memory. By exploring this, this paper introduces the
forgetting curve and proposes Spatio-Temporal periodic Interest Modeling (STIM)
with long sequences for local-life service recommendation. STIM integrates
three key components: a dynamic masking module based on the forgetting curve,
which is used to extract both recent spatiotemporal features and periodic
spatiotemporal features; a query-based mixture of experts (MoE) approach that
can adaptively activate expert networks under different dynamic masks, enabling
the collaborative modeling of time, location, and items; and a hierarchical
multi-interest network unit, which captures multi-interest representations by
modeling the hierarchical interactions between the shallow and deep semantics
of users' recent behaviors. By introducing the STIM method, we conducted online
A/B tests and achieved a 1.54\% improvement in gross transaction volume (GTV).
In addition, extended offline experiments also showed improvements. STIM has
been deployed in a large-scale local-life service recommendation system,
serving hundreds of millions of daily active users in core application
scenarios.

</details>


### [153] [Decomposed Reasoning with Reinforcement Learning for Relevance Assessment in UGC Platforms](https://arxiv.org/abs/2508.02506)
*Xiaowei Yuan,Lei Jin,Haoxin Zhang,Yan Gao,Yi Wu,Yao Hu,Ziyang Huang,Jun Zhao,Kang Liu*

Main category: cs.IR

TL;DR: 提出了R3A模型，以解决用户生成内容平台中检索增强生成（RAG）的相关性评估问题，该模型通过分解推理和强化学习来提高准确性。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）在用户生成内容（UGC）平台中起着关键作用，但其有效性在很大程度上取决于对查询-文档对的准确相关性评估。尽管最近在将大型语言模型（LLM）应用于相关性建模方面取得了进展，但UGC平台面临着独特的挑战：1）由于RAG场景中稀疏的用户反馈而导致的用户意图不明确，以及2）非正式和非结构化语言引入的巨大噪声。

Method: 提出了用于相关性评估的强化推理模型（R3A），它在评分之前引入了对查询和候选文档的分解推理框架。R3A首先利用平台内辅助的高排序文档来推断潜在的查询意图。然后，它执行逐字片段提取，以证明相关性决策的合理性，从而减少由嘈杂的UGC引起的错误。基于强化学习框架，R3A经过优化，可以减轻由模糊查询和非结构化内容引起的失真。

Result: 实验结果表明，R3A在离线基准测试和在线实验中的相关性准确度方面，显著优于现有的基线方法。

Conclusion: R3A在离线基准测试和在线实验中的相关性准确度方面，显著优于现有的基线方法。

Abstract: Retrieval-augmented generation (RAG) plays a critical role in user-generated
content (UGC) platforms, but its effectiveness depends heavily on accurate
relevance assessment of query-document pairs. Despite recent advances in
applying large language models (LLMs) to relevance modeling, UGC platforms
present unique challenges: 1) ambiguous user intent due to sparse user feedback
in RAG scenarios, and 2) substantial noise introduced by informal and
unstructured language. To address these issues, we propose the Reinforced
Reasoning Model for Relevance Assessment (R3A), which introduces a decomposed
reasoning framework over queries and candidate documents before scoring. R3A
first leverages auxiliary high-ranked documents within the platform to infer
latent query intent. It then performs verbatim fragment extraction to justify
relevance decisions, thereby reducing errors caused by noisy UGC. Based on a
reinforcement learning framework, R3A is optimized to mitigate distortions
arising from ambiguous queries and unstructured content. Experimental results
show that R3A significantly outperforms existing baseline methods in terms of
relevance accuracy, across both offline benchmarks and online experiments.

</details>


### [154] [Hubness Reduction with Dual Bank Sinkhorn Normalization for Cross-Modal Retrieval](https://arxiv.org/abs/2508.02538)
*Zhengxin Pan,Haishuai Wang,Fangyu Wu,Peng Zhang,Jiajun Bu*

Main category: cs.IR

TL;DR: This paper introduces SN and DBSN to solve the hubness problem in cross-modal retrieval by balancing query and target probabilities, achieving improved performance in image-text, video-text, and audio-text retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: The hubness problem in cross-modal retrieval hinders the precision of similarity measurements, and the underlying mechanisms of existing hubness reduction methods are poorly understood.

Method: The paper proposes a probability-balancing framework that extends Sinkhorn Normalization (SN) to balance both query and target probabilities and introduces Dual Bank Sinkhorn Normalization (DBSN) to address the distributional gap between the query bank and targets.

Result: The paper achieves consistent performance improvements across various cross-modal retrieval tasks, including image-text retrieval, video-text retrieval, and audio-text retrieval, validating the effectiveness of both SN and DBSN.

Conclusion: The paper introduces Sinkhorn Normalization (SN) and Dual Bank Sinkhorn Normalization (DBSN) and demonstrates their effectiveness in improving performance across various cross-modal retrieval tasks.

Abstract: The past decade has witnessed rapid advancements in cross-modal retrieval,
with significant progress made in accurately measuring the similarity between
cross-modal pairs. However, the persistent hubness problem, a phenomenon where
a small number of targets frequently appear as nearest neighbors to numerous
queries, continues to hinder the precision of similarity measurements. Despite
several proposed methods to reduce hubness, their underlying mechanisms remain
poorly understood. To bridge this gap, we analyze the widely-adopted Inverted
Softmax approach and demonstrate its effectiveness in balancing target
probabilities during retrieval. Building on these insights, we propose a
probability-balancing framework for more effective hubness reduction. We
contend that balancing target probabilities alone is inadequate and, therefore,
extend the framework to balance both query and target probabilities by
introducing Sinkhorn Normalization (SN). Notably, we extend SN to scenarios
where the true query distribution is unknown, showing that current methods,
which rely solely on a query bank to estimate target hubness, produce
suboptimal results due to a significant distributional gap between the query
bank and targets. To mitigate this issue, we introduce Dual Bank Sinkhorn
Normalization (DBSN), incorporating a corresponding target bank alongside the
query bank to narrow this distributional gap. Our comprehensive evaluation
across various cross-modal retrieval tasks, including image-text retrieval,
video-text retrieval, and audio-text retrieval, demonstrates consistent
performance improvements, validating the effectiveness of both SN and DBSN. All
codes are publicly available at https://github.com/ppanzx/DBSN.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [155] [PCS Workflow for Veridical Data Science in the Age of AI](https://arxiv.org/abs/2508.00835)
*Zachary T. Rewolinski,Bin Yu*

Main category: cs.LG

TL;DR: The paper introduces an updated PCS workflow for truthful data science, enhanced with generative AI, to address the uncertainty in the data science life cycle. It includes a running example and a case study.


<details>
  <summary>Details</summary>
Motivation: Data-driven findings in AI offer unprecedented power to extract insights and guide decision-making, many are difficult or impossible to replicate. A key reason for this challenge is the uncertainty introduced by the many choices made throughout the data science life cycle (DSLC). Traditional statistical frameworks often fail to account for this uncertainty.

Method: PCS framework

Result: We include a running example to display the PCS framework in action, and conduct a related case study which showcases the uncertainty in downstream predictions caused by judgment calls in the data cleaning stage.

Conclusion: This paper presents an updated and streamlined PCS workflow, tailored for practitioners and enhanced with guided use of generative AI.

Abstract: Data science is a pillar of artificial intelligence (AI), which is
transforming nearly every domain of human activity, from the social and
physical sciences to engineering and medicine. While data-driven findings in AI
offer unprecedented power to extract insights and guide decision-making, many
are difficult or impossible to replicate. A key reason for this challenge is
the uncertainty introduced by the many choices made throughout the data science
life cycle (DSLC). Traditional statistical frameworks often fail to account for
this uncertainty. The Predictability-Computability-Stability (PCS) framework
for veridical (truthful) data science offers a principled approach to
addressing this challenge throughout the DSLC. This paper presents an updated
and streamlined PCS workflow, tailored for practitioners and enhanced with
guided use of generative AI. We include a running example to display the PCS
framework in action, and conduct a related case study which showcases the
uncertainty in downstream predictions caused by judgment calls in the data
cleaning stage.

</details>


### [156] [A Residual Guided strategy with Generative Adversarial Networks in training Physics-Informed Transformer Networks](https://arxiv.org/abs/2508.00855)
*Ziyang Zhang,Feifan Zhang,Weidong Tang,Lei Shi,Tailai Chen*

Main category: cs.LG

TL;DR: 提出了一种新的残差引导训练策略，用于物理信息Transformer，通过结合GAN来提高求解非线性PDE的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的物理信息神经网络（PINN）在关键时空区域中经常面临未解决的残差和违反时间因果关系的问题。

Method: 该方法结合了解码器Transformer和残差感知GAN，通过自回归处理捕捉时间相关性，并动态识别和优先处理高残差区域。引入了因果惩罚项和自适应抽样机制，以加强时间因果关系，同时提高问题域的精度。

Result: 在Allen-Cahn、Klein-Gordon和Navier-Stokes方程的大量数值实验表明，与基线方法相比，相对MSE降低了高达三个数量级，实现了显著的改进。

Conclusion: 该论文通过结合生成对抗网络（GAN）的残差引导训练策略，提出了一种用于物理信息Transformer的新方法，为多尺度和时间相关的PDE系统提供了一个稳健的解决方案。

Abstract: Nonlinear partial differential equations (PDEs) are pivotal in modeling
complex physical systems, yet traditional Physics-Informed Neural Networks
(PINNs) often struggle with unresolved residuals in critical spatiotemporal
regions and violations of temporal causality. To address these limitations, we
propose a novel Residual Guided Training strategy for Physics-Informed
Transformer via Generative Adversarial Networks (GAN). Our framework integrates
a decoder-only Transformer to inherently capture temporal correlations through
autoregressive processing, coupled with a residual-aware GAN that dynamically
identifies and prioritizes high-residual regions. By introducing a causal
penalty term and an adaptive sampling mechanism, the method enforces temporal
causality while refining accuracy in problematic domains. Extensive numerical
experiments on the Allen-Cahn, Klein-Gordon, and Navier-Stokes equations
demonstrate significant improvements, achieving relative MSE reductions of up
to three orders of magnitude compared to baseline methods. This work bridges
the gap between deep learning and physics-driven modeling, offering a robust
solution for multiscale and time-dependent PDE systems.

</details>


### [157] [Deploying Geospatial Foundation Models in the Real World: Lessons from WorldCereal](https://arxiv.org/abs/2508.00858)
*Christina Butsko,Kristof Van Tricht,Gabriel Tseng,Giorgia Milli,David Rolnick,Ruben Cartuyvels,Inbal Becker Reshef,Zoltan Szantoi,Hannah Kerner*

Main category: cs.LG

TL;DR: 该研究提出了一种将地理空间基础模型集成到 операционные 映射系统中的结构化方法，并在作物制图案例研究中验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 地理空间基础模型的日益普及有可能改变遥感应用，例如土地覆盖分类、环境监测和变化检测。尽管基准测试结果很有希望，但在 операционные 环境中部署这些模型具有挑战性且很少见。标准化评估任务通常无法捕捉到与最终用户采用相关的现实世界的复杂性，例如数据异构性、资源约束和特定于应用程序的需求。

Method: 该研究提出了一种结构化的方法，将地理空间基础模型集成到 операционные 系统中。该协议有三个关键步骤：定义应用程序需求、使模型适应特定领域数据以及进行严格的实证测试。

Result: 微调预训练模型显著提高了性能，优于传统的监督方法。该模型具有强大的空间和时间泛化能力。该框架具有可扩展性。

Conclusion: 该研究展示了在作物制图案例研究中使用Presto模型，微调预训练模型可以显著提高性能，优于传统的监督方法。结果强调了该模型强大的空间和时间泛化能力。该协议在 WorldCereal 全球作物制图系统中的应用展示了框架的可扩展性。

Abstract: The increasing availability of geospatial foundation models has the potential
to transform remote sensing applications such as land cover classification,
environmental monitoring, and change detection. Despite promising benchmark
results, the deployment of these models in operational settings is challenging
and rare. Standardized evaluation tasks often fail to capture real-world
complexities relevant for end-user adoption such as data heterogeneity,
resource constraints, and application-specific requirements. This paper
presents a structured approach to integrate geospatial foundation models into
operational mapping systems. Our protocol has three key steps: defining
application requirements, adapting the model to domain-specific data and
conducting rigorous empirical testing. Using the Presto model in a case study
for crop mapping, we demonstrate that fine-tuning a pre-trained model
significantly improves performance over conventional supervised methods. Our
results highlight the model's strong spatial and temporal generalization
capabilities. Our protocol provides a replicable blueprint for practitioners
and lays the groundwork for future research to operationalize foundation models
in diverse remote sensing applications. Application of the protocol to the
WorldCereal global crop-mapping system showcases the framework's scalability.

</details>


### [158] [Discrete approach to machine learning](https://arxiv.org/abs/2508.00869)
*Dmitriy Kashitsyn,Dmitriy Shabanov*

Main category: cs.LG

TL;DR: The paper explores encoding and structural information processing using sparse bit vectors, drawing parallels between code space layouts and mammalian neocortex organization.


<details>
  <summary>Details</summary>
Motivation: The article explores an encoding and structural information processing approach.

Method: The paper presents a discrete method of speculative stochastic dimensionality reduction and a geometric method for obtaining discrete embeddings of an organised code space.

Result: The structure and properties of a code space are investigated using three modalities as examples: morphology of Russian and English languages, and immunohistochemical markers. Parallels are drawn between the resulting map of the code space layout and so-called pinwheels appearing on the mammalian neocortex.

Conclusion: The paper cautiously assumes similarities between neocortex organisation and processes happening in their models.

Abstract: The article explores an encoding and structural information processing
approach using sparse bit vectors and fixed-length linear vectors. The
following are presented: a discrete method of speculative stochastic
dimensionality reduction of multidimensional code and linear spaces with linear
asymptotic complexity; a geometric method for obtaining discrete embeddings of
an organised code space that reflect the internal structure of a given
modality. The structure and properties of a code space are investigated using
three modalities as examples: morphology of Russian and English languages, and
immunohistochemical markers. Parallels are drawn between the resulting map of
the code space layout and so-called pinwheels appearing on the mammalian
neocortex. A cautious assumption is made about similarities between neocortex
organisation and processes happening in our models.

</details>


### [159] [A Data-Driven Machine Learning Approach for Predicting Axial Load Capacity in Steel Storage Rack Columns](https://arxiv.org/abs/2508.00876)
*Bakhtiyar Mammadli,Casim Yazici,Muhammed Gürbüz,İrfan Kocaman,F. Javier Dominguez-Gutierrez,Fatih Mehmet Özkal*

Main category: cs.LG

TL;DR: A machine learning framework using Gradient Boosting Regression was developed to predict the axial load-bearing capacity of steel columns, offering a practical tool for structural engineers.


<details>
  <summary>Details</summary>
Motivation: Traditional analytical approaches have limitations in capturing the nonlinearities and geometrical complexities inherent to buckling behavior.

Method: A machine learning framework was used to predict the axial load-bearing capacity of cold-formed steel structural members. Gradient Boosting Regression was selected as the final model.

Result: Gradient Boosting Regression exhibited superior predictive performance across multiple metrics.

Conclusion: The Gradient Boosting Regression model was integrated into a Python-based web interface for real-time prediction of axial load capacity.

Abstract: In this study, we present a machine learning (ML) framework to predict the
axial load-bearing capacity, (kN), of cold-formed steel structural members. The
methodology emphasizes robust model selection and interpretability, addressing
the limitations of traditional analytical approaches in capturing the
nonlinearities and geometrical complexities inherent to buckling behavior. The
dataset, comprising key geometric and mechanical parameters of steel columns,
was curated with appropriate pre-processing steps including removal of
non-informative identifiers and imputation of missing values. A comprehensive
suite of regression algorithms, ranging from linear models to kernel-based
regressors and ensemble tree methods was evaluated. Among these, Gradient
Boosting Regression exhibited superior predictive performance across multiple
metrics, including the coefficient of determination (R2), root mean squared
error (RMSE), and mean absolute error (MAE), and was consequently selected as
the final model. Model interpretability was addressed using SHapley Additive
exPlanations (SHAP), enabling insight into the relative importance and
interaction of input features influencing the predicted axial capacity. To
facilitate practical deployment, the model was integrated into an interactive,
Python-based web interface via Streamlit. This tool allows end-users-such as
structural engineers and designers, to input design parameters manually or
through CSV upload, and to obtain real-time predictions of axial load capacity
without the need for programming expertise. Applied to the context of steel
storage rack columns, the framework demonstrates how data-driven tools can
enhance design safety, streamline validation workflows, and inform
decision-making in structural applications where buckling is a critical failure
mode

</details>


### [160] [CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search](https://arxiv.org/abs/2508.02091)
*Xiaoya Li,Xiaofei Sun,Albert Wang,Chris Shum,Jiwei Li*

Main category: cs.LG

TL;DR: CRINN is a new ANNS paradigm using reinforcement learning to optimize execution speed while maintaining accuracy, achieving state-of-the-art results on several benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: ANNS algorithms are critical for AI applications like RAG and agent-based LLM applications.

Method: Treats ANNS optimization as a reinforcement learning problem.

Result: CRINN achieves best or tied for first place performance on five widely-used NNS benchmark datasets compared to state-of-the-art open-source ANNS algorithms.

Conclusion: CRINN demonstrates that reinforcement learning-augmented LLMs can automate sophisticated algorithmic optimizations.

Abstract: Approximate nearest-neighbor search (ANNS) algorithms have become
increasingly critical for recent AI applications, particularly in
retrieval-augmented generation (RAG) and agent-based LLM applications. In this
paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS
optimization as a reinforcement learning problem where execution speed serves
as the reward signal. This approach enables the automatic generation of
progressively faster ANNS implementations while maintaining accuracy
constraints. Our experimental evaluation demonstrates CRINN's effectiveness
across six widely-used NNS benchmark datasets. When compared against
state-of-the-art open-source ANNS algorithms, CRINN achieves best performance
on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and
GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean
and GloVe-25-angular). The implications of CRINN's success reach well beyond
ANNS optimization: It validates that LLMs augmented with reinforcement learning
can function as an effective tool for automating sophisticated algorithmic
optimizations that demand specialized knowledge and labor-intensive manual
refinement.Code can be found at https://github.com/deepreinforce-ai/CRINN

</details>


### [161] [Satellite Connectivity Prediction for Fast-Moving Platforms](https://arxiv.org/abs/2508.00877)
*Chao Yan,Babak Mafakheri*

Main category: cs.LG

TL;DR: 利用机器学习预测卫星信号质量，在飞行中实现了 0.97 的 F1 分数，从而实现了无缝宽带服务。


<details>
  <summary>Details</summary>
Motivation: 随着对无缝互联网访问的需求不断增长，卫星连接越来越受到关注，尤其是在交通运输和偏远地区。对于飞机、车辆或火车等快速移动的物体，卫星连接至关重要，因为它们具有移动性并且经常出现在没有地面覆盖的区域。

Method: 使用机器学习分析 GEO 卫星和飞机在多次飞行中的通信真实数据集，以预测信号质量。

Result: 我们的预测模型在测试数据上实现了 0.97 的 F1 分数，证明了机器学习在预测飞行过程中信号质量方面的准确性。

Conclusion: 该模型通过实现不同卫星星座和提供商之间的漫游，满足了对信号质量进行实时预测的需求。该方法可以进一步调整以自动执行卫星和波束切换机制，从而提高整体通信效率。该模型还可以使用定制数据集（包括联网车辆和火车）重新训练并应用于任何具有卫星连接的移动物体。

Abstract: Satellite connectivity is gaining increased attention as the demand for
seamless internet access, especially in transportation and remote areas,
continues to grow. For fast-moving objects such as aircraft, vehicles, or
trains, satellite connectivity is critical due to their mobility and frequent
presence in areas without terrestrial coverage. Maintaining reliable
connectivity in these cases requires frequent switching between satellite
beams, constellations, or orbits. To enhance user experience and address
challenges like long switching times, Machine Learning (ML) algorithms can
analyze historical connectivity data and predict network quality at specific
locations. This allows for proactive measures, such as network switching before
connectivity issues arise. In this paper, we analyze a real dataset of
communication between a Geostationary Orbit (GEO) satellite and aircraft over
multiple flights, using ML to predict signal quality. Our prediction model
achieved an F1 score of 0.97 on the test data, demonstrating the accuracy of
machine learning in predicting signal quality during flight. By enabling
seamless broadband service, including roaming between different satellite
constellations and providers, our model addresses the need for real-time
predictions of signal quality. This approach can further be adapted to automate
satellite and beam-switching mechanisms to improve overall communication
efficiency. The model can also be retrained and applied to any moving object
with satellite connectivity, using customized datasets, including connected
vehicles and trains.

</details>


### [162] [Skeleton-Guided Learning for Shortest Path Search](https://arxiv.org/abs/2508.02270)
*Tiantian Liu,Xiao Li,Huan Li,Hua Lu,Christian S. Jensen,Jianliang Xu*

Main category: cs.LG

TL;DR: 我们提出了一个通用的基于学习的框架，用于在通用图上进行最短路径搜索，而无需特定于领域的特征。


<details>
  <summary>Details</summary>
Motivation: 最短路径搜索是基于图的应用中的核心操作，但现有方法面临着重要的局限性。随着图变得越来越复杂，诸如Dijkstra和A*之类的经典算法变得效率低下，而基于索引的技术通常需要大量的预处理和存储。最近基于学习的方法通常侧重于空间图，并且依赖于诸如地理坐标之类的特定于上下文的特征，从而限制了它们的通用性。

Method: 构建骨架图，该骨架图以紧凑的形式捕获多层距离和跳跃信息。骨架图神经网络 (SGNN) 对该结构进行操作，以学习节点嵌入并预测节点对之间的距离和跳跃长度。这些预测支持 LSearch，这是一种引导搜索算法，它使用模型驱动的修剪来减少搜索空间，同时保持准确性。为了处理更大的图，我们引入了一种分层训练策略，该策略将图划分为具有单独训练的 SGNN 的子图。这种结构支持 HLSearch，这是我们的一种方法的扩展，用于在图分区中进行有效的路径搜索。

Result: 在五个不同的真实世界图上的实验表明，我们的框架在各种图类型上都取得了强大的性能，

Conclusion: 该框架在各种图类型上都取得了强大的性能，为基于学习的最短路径搜索提供了一种灵活有效的解决方案。

Abstract: Shortest path search is a core operation in graph-based applications, yet
existing methods face important limitations. Classical algorithms such as
Dijkstra's and A* become inefficient as graphs grow more complex, while
index-based techniques often require substantial preprocessing and storage.
Recent learning-based approaches typically focus on spatial graphs and rely on
context-specific features like geographic coordinates, limiting their general
applicability. We propose a versatile learning-based framework for shortest
path search on generic graphs, without requiring domain-specific features. At
the core of our approach is the construction of a skeleton graph that captures
multi-level distance and hop information in a compact form. A Skeleton Graph
Neural Network (SGNN) operates on this structure to learn node embeddings and
predict distances and hop lengths between node pairs. These predictions support
LSearch, a guided search algorithm that uses model-driven pruning to reduce the
search space while preserving accuracy. To handle larger graphs, we introduce a
hierarchical training strategy that partitions the graph into subgraphs with
individually trained SGNNs. This structure enables HLSearch, an extension of
our method for efficient path search across graph partitions. Experiments on
five diverse real-world graphs demonstrate that our framework achieves strong
performance across graph types, offering a flexible and effective solution for
learning-based shortest path search.

</details>


### [163] [GNN-ASE: Graph-Based Anomaly Detection and Severity Estimation in Three-Phase Induction Machines](https://arxiv.org/abs/2508.00879)
*Moutaz Bellah Bentrad,Adel Ghoggal,Tahar Bahi,Abderaouf Bahi*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络（GNN）的无模型方法，用于感应电机中的故障诊断，无需信号预处理或手动特征提取，实现了较高的诊断性能。


<details>
  <summary>Details</summary>
Motivation: 传统的感应电机诊断依赖于基于模型的方法，这些方法需要开发复杂的动态模型，使其难以实施且计算成本高昂。

Method: 使用图神经网络（GNN）的无模型方法

Result: 实验结果表明，该模型的有效性，偏心缺陷的准确率为92.5%，轴承故障的准确率为91.2%，断条转子检测的准确率为93.1%。

Conclusion: GNN模型为感应电机故障诊断提供了一种有前景的替代方案，它简化了实施过程，同时保持了较高的诊断性能。

Abstract: The diagnosis of induction machines has traditionally relied on model-based
methods that require the development of complex dynamic models, making them
difficult to implement and computationally expensive. To overcome these
limitations, this paper proposes a model-free approach using Graph Neural
Networks (GNNs) for fault diagnosis in induction machines. The focus is on
detecting multiple fault types -- including eccentricity, bearing defects, and
broken rotor bars -- under varying severity levels and load conditions. Unlike
traditional approaches, raw current and vibration signals are used as direct
inputs, eliminating the need for signal preprocessing or manual feature
extraction. The proposed GNN-ASE model automatically learns and extracts
relevant features from raw inputs, leveraging the graph structure to capture
complex relationships between signal types and fault patterns. It is evaluated
for both individual fault detection and multi-class classification of combined
fault conditions. Experimental results demonstrate the effectiveness of the
proposed model, achieving 92.5\% accuracy for eccentricity defects, 91.2\% for
bearing faults, and 93.1\% for broken rotor bar detection. These findings
highlight the model's robustness and generalization capability across different
operational scenarios. The proposed GNN-based framework offers a lightweight
yet powerful solution that simplifies implementation while maintaining high
diagnostic performance. It stands as a promising alternative to conventional
model-based diagnostic techniques for real-world induction machine monitoring
and predictive maintenance.

</details>


### [164] [Reproducibility of Machine Learning-Based Fault Detection and Diagnosis for HVAC Systems in Buildings: An Empirical Study](https://arxiv.org/abs/2508.00880)
*Adil Mukhtar,Michael Hadwiger,Franz Wotawa,Gerald Schweiger*

Main category: cs.LG

TL;DR: This paper analyzes the reproducibility of ML applications in building energy systems and finds that nearly all articles are not reproducible due to insufficient disclosure. It recommends reproducibility guidelines, training, and policies to promote transparency and reproducibility.


<details>
  <summary>Details</summary>
Motivation: The fast-growing field of Machine Learning (ML) has become part of this discourse, as it faces similar concerns about transparency and reliability. While reproducibility issues are increasingly recognized by the ML community and its major conferences, less is known about how these challenges manifest in applied disciplines.

Method: Analyzing the transparency and reproducibility standards of ML applications in building energy systems.

Result: 72% of the articles do not specify whether the dataset used is public, proprietary, or commercially available. Only two papers share a link to their code - one of which was broken. No significant differences in reproducibility were observed compared to publications with industry-affiliated authors.

Conclusion: Nearly all articles are not reproducible due to insufficient disclosure across key dimensions of reproducibility. The findings highlight the need for targeted interventions, including reproducibility guidelines, training for researchers, and policies by journals and conferences that promote transparency and reproducibility.

Abstract: Reproducibility is a cornerstone of scientific research, enabling independent
verification and validation of empirical findings. The topic gained prominence
in fields such as psychology and medicine, where concerns about non -
replicable results sparked ongoing discussions about research practices. In
recent years, the fast-growing field of Machine Learning (ML) has become part
of this discourse, as it faces similar concerns about transparency and
reliability. Some reproducibility issues in ML research are shared with other
fields, such as limited access to data and missing methodological details. In
addition, ML introduces specific challenges, including inherent nondeterminism
and computational constraints. While reproducibility issues are increasingly
recognized by the ML community and its major conferences, less is known about
how these challenges manifest in applied disciplines. This paper contributes to
closing this gap by analyzing the transparency and reproducibility standards of
ML applications in building energy systems. The results indicate that nearly
all articles are not reproducible due to insufficient disclosure across key
dimensions of reproducibility. 72% of the articles do not specify whether the
dataset used is public, proprietary, or commercially available. Only two papers
share a link to their code - one of which was broken. Two-thirds of the
publications were authored exclusively by academic researchers, yet no
significant differences in reproducibility were observed compared to
publications with industry-affiliated authors. These findings highlight the
need for targeted interventions, including reproducibility guidelines, training
for researchers, and policies by journals and conferences that promote
transparency and reproducibility.

</details>


### [165] [Hallucination Detection and Mitigation with Diffusion in Multi-Variate Time-Series Foundation Models](https://arxiv.org/abs/2508.00881)
*Vijja Wichitwechkarn,Charles Fox,Ruchi Choudhary*

Main category: cs.LG

TL;DR: This paper introduces definitions and methods for detecting and mitigating hallucination in multi-variate time-series (MVTS) foundation models, finding that current models hallucinate significantly and proposing a method to reduce this issue.


<details>
  <summary>Details</summary>
Motivation: Analogous definitions and methods do not exist for multi-variate time-series (MVTS) foundation models, while foundation models for natural language processing have many coherent definitions of hallucination and methods for its detection and mitigation.

Method: The paper proposes new definitions for MVTS hallucination, along with new detection and mitigation methods using a diffusion model to estimate hallucination levels. Relational datasets are derived from popular time-series datasets to benchmark these relational hallucination levels.

Result: Find that open-source pre-trained MVTS imputation foundation models relationally hallucinate on average up to 59.5% as much as a weak baseline. The proposed mitigation method reduces this by up to 47.7% for these models.

Conclusion: Open-source pre-trained MVTS imputation foundation models relationally hallucinate on average up to 59.5% as much as a weak baseline, and the proposed mitigation method reduces this by up to 47.7% for these models. The definition and methods may improve adoption and safe usage of MVTS foundation models.

Abstract: Foundation models for natural language processing have many coherent
definitions of hallucination and methods for its detection and mitigation.
However, analogous definitions and methods do not exist for multi-variate
time-series (MVTS) foundation models. We propose new definitions for MVTS
hallucination, along with new detection and mitigation methods using a
diffusion model to estimate hallucination levels. We derive relational datasets
from popular time-series datasets to benchmark these relational hallucination
levels. Using these definitions and models, we find that open-source
pre-trained MVTS imputation foundation models relationally hallucinate on
average up to 59.5% as much as a weak baseline. The proposed mitigation method
reduces this by up to 47.7% for these models. The definition and methods may
improve adoption and safe usage of MVTS foundation models.

</details>


### [166] [Multi-Grained Temporal-Spatial Graph Learning for Stable Traffic Flow Forecasting](https://arxiv.org/abs/2508.00884)
*Zhenan Lin,Yuni Lai,Wai Lun Lo,Richard Tai-Chiu Hsung,Harris Sik-Ho Tsang,Xiaoyu Xue,Kai Zhou,Yulin Zhu*

Main category: cs.LG

TL;DR: propose a multi-grained temporal-spatial graph learning framework to adaptively augment the globally temporal-spatial patterns


<details>
  <summary>Details</summary>
Motivation: the dynamic traffic flow forecasting is a highly nonlinear problem with complex temporal-spatial dependencies. Although the existing methods has provided great contributions to mine the temporal-spatial patterns in the complex traffic networks, they fail to encode the globally temporal-spatial patterns and are prone to overfit on the pre-defined geographical correlations, and thus hinder the model's robustness on the complex traffic environment

Method: a multi-grained temporal-spatial graph learning framework to adaptively augment the globally temporal-spatial patterns obtained from a crafted graph transformer encoder with the local patterns from the graph convolution by a crafted gated fusion unit with residual connection techniques

Result: outperforms other strong baselines on various real-world traffic networks

Conclusion:  proposed model can mine the hidden global temporal-spatial relations between each monitor stations and balance the relative importance of local and global temporal-spatial patterns. Experiment results demonstrate the strong representation capability of our proposed method and our model consistently outperforms other strong baselines on various real-world traffic networks.

Abstract: Time-evolving traffic flow forecasting are playing a vital role in
intelligent transportation systems and smart cities. However, the dynamic
traffic flow forecasting is a highly nonlinear problem with complex
temporal-spatial dependencies. Although the existing methods has provided great
contributions to mine the temporal-spatial patterns in the complex traffic
networks, they fail to encode the globally temporal-spatial patterns and are
prone to overfit on the pre-defined geographical correlations, and thus hinder
the model's robustness on the complex traffic environment. To tackle this
issue, in this work, we proposed a multi-grained temporal-spatial graph
learning framework to adaptively augment the globally temporal-spatial patterns
obtained from a crafted graph transformer encoder with the local patterns from
the graph convolution by a crafted gated fusion unit with residual connection
techniques. Under these circumstances, our proposed model can mine the hidden
global temporal-spatial relations between each monitor stations and balance the
relative importance of local and global temporal-spatial patterns. Experiment
results demonstrate the strong representation capability of our proposed method
and our model consistently outperforms other strong baselines on various
real-world traffic networks.

</details>


### [167] [Stochastic Optimal Control via Measure Relaxations](https://arxiv.org/abs/2508.00886)
*Etienne Buehrle,Christoph Stiller*

Main category: cs.LG

TL;DR: casting the optimal control problem of a stochastic system as a convex optimization problem over occupation measures to  solve the optimal control problem of stochastic systems


<details>
  <summary>Details</summary>
Motivation: The optimal control problem of stochastic systems is commonly solved via robust or scenario-based optimization methods, which are both challenging to scale to long optimization horizons.

Method: casting the optimal control problem of a stochastic system as a convex optimization problem over occupation measures

Result: demonstrate our method on a set of synthetic and real-world scenarios, learning cost functions from data via Christoffel polynomials.

Conclusion: The optimal control problem of a stochastic system as a convex optimization problem over occupation measures.

Abstract: The optimal control problem of stochastic systems is commonly solved via
robust or scenario-based optimization methods, which are both challenging to
scale to long optimization horizons. We cast the optimal control problem of a
stochastic system as a convex optimization problem over occupation measures. We
demonstrate our method on a set of synthetic and real-world scenarios, learning
cost functions from data via Christoffel polynomials. The code for our
experiments is available at https://github.com/ebuehrle/dpoc.

</details>


### [168] [FRAM: Frobenius-Regularized Assignment Matching with Mixed-Precision Computing](https://arxiv.org/abs/2508.00887)
*Binrui Shen,Yuan Liang,Shengxin Zhu*

Main category: cs.LG

TL;DR: This paper introduces a novel relaxation framework to address the NP-hardness of QAP in graph matching by reformulating the projection step as a Frobenius-regularized Linear Assignment (FRA) problem. The method achieves significant speedup with negligible loss in accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the NP-hardness of QAP, some existing methods adopt projection-based relaxations that embed the problem into the convex hull of the discrete domain. However, these relaxations inevitably enlarge the feasible set, introducing two sources of error: numerical scale sensitivity and geometric misalignment between the relaxed and original domains.

Method: reformulating the projection step as a Frobenius-regularized Linear Assignment (FRA) problem, where a tunable regularization term mitigates feasible region inflation. To efficiently solve FRA, we propose the Scaling Doubly Stochastic Normalization (SDSN) algorithm.

Result: FRAM consistently outperforms all baseline methods under identical precision settings.

Conclusion: FRAM achieves up to 370X speedup over its CPU-FP64 counterpart, with negligible loss in solution accuracy.

Abstract: Graph matching, typically formulated as a Quadratic Assignment Problem (QAP),
seeks to establish node correspondences between two graphs. To address the
NP-hardness of QAP, some existing methods adopt projection-based relaxations
that embed the problem into the convex hull of the discrete domain. However,
these relaxations inevitably enlarge the feasible set, introducing two sources
of error: numerical scale sensitivity and geometric misalignment between the
relaxed and original domains. To alleviate these errors, we propose a novel
relaxation framework by reformulating the projection step as a
Frobenius-regularized Linear Assignment (FRA) problem, where a tunable
regularization term mitigates feasible region inflation. This formulation
enables normalization-based operations to preserve numerical scale invariance
without compromising accuracy. To efficiently solve FRA, we propose the Scaling
Doubly Stochastic Normalization (SDSN) algorithm. Building on its favorable
computational properties, we develop a theoretically grounded mixed-precision
architecture to achieve substantial acceleration. Comprehensive CPU-based
benchmarks demonstrate that FRAM consistently outperforms all baseline methods
under identical precision settings. When combined with a GPU-based
mixed-precision architecture, FRAM achieves up to 370X speedup over its
CPU-FP64 counterpart, with negligible loss in solution accuracy.

</details>


### [169] [A Dynamic, Context-Aware Framework for Risky Driving Prediction Using Naturalistic Data](https://arxiv.org/abs/2508.00888)
*Amir Hossein Kalantari,Eleonora Papadimitriou,Amir Pooyan Afghari*

Main category: cs.LG

TL;DR: 提出了一种动态和个性化的框架，用于识别危险驾驶行为，使用比利时自然驾驶数据。


<details>
  <summary>Details</summary>
Motivation: 现有的框架依赖于固定的时间窗口和静态阈值来区分安全和危险行为，限制了它们对现实世界驾驶的随机性质的反应能力。

Method: 利用滚动时间窗口和双层优化来动态校准风险阈值和模型超参数。

Result: DNN 在捕捉驾驶行为的细微变化方面表现出强大的能力，特别是在高召回率任务中表现出色，使其在早期风险检测方面很有前景。XGBoost 在不同的阈值和评估指标中提供了最平衡和稳定的性能。虽然随机森林显示出更多的可变性，但它对动态阈值调整反应灵敏，这在模型适应或调整期间可能是有利的。速度加权车头时距比恶劣驾驶事件更稳定和对上下文敏感的风险指标，这可能是由于其对标签稀疏性和上下文变化的鲁棒性。

Conclusion: 自适应的、个性化的风险检测方法对于增强实时安全反馈和在智能交通系统中定制驾驶员支持具有价值。

Abstract: Naturalistic driving studies offer a powerful means for observing and
quantifying real-world driving behaviour. One of their prominent applications
in traffic safety is the continuous monitoring and classification of risky
driving behaviour. However, many existing frameworks rely on fixed time windows
and static thresholds for distinguishing between safe and risky behaviour -
limiting their ability to respond to the stochastic nature of real-world
driving. This study proposes a dynamic and individualised framework for
identifying risky driving behaviour using Belgian naturalistic driving data.
The approach leverages a rolling time window and bi-level optimisation to
dynamically calibrate both risk thresholds and model hyperparameters, capturing
subtle behavioural shifts. Two safety indicators, speed-weighted headway and
harsh driving events, were evaluated using three data-driven models: Random
Forest, XGBoost, and Deep Neural Network (DNN). The DNN demonstrated strong
capability in capturing subtle changes in driving behaviour, particularly
excelling in high-recall tasks, making it promising for early-stage risk
detection. XGBoost provided the most balanced and stable performance across
different thresholds and evaluation metrics. While random forest showed more
variability, it responded sensitively to dynamic threshold adjustments, which
may be advantageous during model adaptation or tuning. Speed-weighted headway
emerged as a more stable and context-sensitive risk indicator than harsh
driving events, likely due to its robustness to label sparsity and contextual
variation. Overall, the findings support the value of adaptive, personalised
risk detection approaches for enhancing real-time safety feedback and tailoring
driver support in intelligent transport systems.

</details>


### [170] [Maximize margins for robust splicing detection](https://arxiv.org/abs/2508.00897)
*Julien Simon de Kergunic,Rony Abecidan,Patrick Bas,Vincent Itier*

Main category: cs.LG

TL;DR: 深度学习取证工具对训练条件高度敏感，提出了一种通过最大化潜在裕度来构建更鲁棒检测器的策略。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的取证工具对训练条件高度敏感，即使应用于评估图像的轻微后处理也会显着降低检测器性能，从而引起对其在操作环境中可靠性的担忧。

Method: 训练同一模型的多个变体，并选择最大化潜在裕度的变体。

Result: 潜在裕度的分布与检测器泛化到后处理图像的能力之间存在很强的相关性。表明相同的深度架构对看不见的后处理的反应非常不同，具体取决于学习的权重，尽管在分布内测试数据上实现了相似的准确性。这种可变性源于训练引起的潜在空间的差异，这会影响样本在内部的分离方式。

Conclusion: 提出了一种构建更鲁棒检测器的实用策略：在不同条件下训练同一模型的多个变体，并选择最大化潜在裕度的变体。

Abstract: Despite recent progress in splicing detection, deep learning-based forensic
tools remain difficult to deploy in practice due to their high sensitivity to
training conditions. Even mild post-processing applied to evaluation images can
significantly degrade detector performance, raising concerns about their
reliability in operational contexts. In this work, we show that the same deep
architecture can react very differently to unseen post-processing depending on
the learned weights, despite achieving similar accuracy on in-distribution test
data. This variability stems from differences in the latent spaces induced by
training, which affect how samples are separated internally. Our experiments
reveal a strong correlation between the distribution of latent margins and a
detector's ability to generalize to post-processed images. Based on this
observation, we propose a practical strategy for building more robust
detectors: train several variants of the same model under different conditions,
and select the one that maximizes latent margins.

</details>


### [171] [Filtering with Self-Attention and Storing with MLP: One-Layer Transformers Can Provably Acquire and Extract Knowledge](https://arxiv.org/abs/2508.00901)
*Ruichen Xu,Kexin Chen*

Main category: cs.LG

TL;DR: This paper introduces a one-layer transformer framework with self-attention and MLP modules to analyze knowledge acquisition and extraction in LLMs, proving transformers can achieve near-optimal training loss and low generalization error under specific conditions. Experiments validate the results on synthetic and real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Modern large language models excel in knowledge-intensive tasks, yet how transformers acquire (store) knowledge during pre-training and extract (retrieve) it during post-fine-tuning inference remains theoretically opaque. While prior theoretical work has begun to investigate these questions through the analysis of training dynamics, such studies are limited to single-layer, attention-only architectures. However, most existing studies suggest that MLPs are the most contributing components for storing knowledge in transformer-based language models. Meanwhile, our empirical investigations reveal that such simplified models, when trained using standard next-token prediction objectives, may be incapable of acquiring or extracting factual knowledge.

Method: introduce a tractable one-layer transformer framework that crucially incorporates both self-attention and MLP modules. By tracking its gradient dynamics, we establish convergence and generalization guarantees that illuminate the ability of knowledge acquisition and extraction

Result: Experiments on synthetic and real-world PopQA datasets with GPT-2 and Llama-3.2-1B validate our results.

Conclusion: Transformers can achieve near-optimal training loss during pre-training, signifying effective knowledge acquisition. With a large fine-tuning dataset and specific data multiplicity conditions met, transformers can achieve low generalization error when tested on factual knowledge learned during pre-training but not reinforced during the fine-tuning, indicating successful knowledge extraction; When the conditions are not satisfied, transformers exhibit high generalization loss, resulting in hallucinations.

Abstract: Modern large language models excel in knowledge-intensive tasks, yet how
transformers acquire (store) knowledge during pre-training and extract
(retrieve) it during post-fine-tuning inference remains theoretically opaque.
While prior theoretical work has begun to investigate these questions through
the analysis of training dynamics, such studies are limited to single-layer,
attention-only architectures. However, most existing studies suggest that MLPs
are the most contributing components for storing knowledge in transformer-based
language models. Meanwhile, our empirical investigations reveal that such
simplified models, when trained using standard next-token prediction
objectives, may be incapable of acquiring or extracting factual knowledge. To
overcome this limitation, we introduce a tractable one-layer transformer
framework that crucially incorporates both self-attention and MLP modules. By
tracking its gradient dynamics, we establish convergence and generalization
guarantees that illuminate the ability of knowledge acquisition and extraction.
We prove that 1) Transformers can achieve near-optimal training loss during
pre-training, signifying effective knowledge acquisition; 2) With a large
fine-tuning dataset and specific data multiplicity conditions met, transformers
can achieve low generalization error when tested on factual knowledge learned
during pre-training but not reinforced during the fine-tuning, indicating
successful knowledge extraction; 3) When the conditions are not satisfied,
transformers exhibit high generalization loss, resulting in hallucinations. Our
analysis includes both full fine-tuning and low-rank fine-tuning. Furthermore,
our analysis offers theoretical insights into several pertinent empirical
phenomena, such as the role of learning rate schedules. Experiments on
synthetic and real-world PopQA datasets with GPT-2 and Llama-3.2-1B validate
our results.

</details>


### [172] [Universal Neurons in GPT-2: Emergence, Persistence, and Functional Impact](https://arxiv.org/abs/2508.00903)
*Advey Nandan,Cheng-Ting Chou,Amrit Kurakula,Cole Blondin,Kevin Zhu,Vasu Sharma,Sean O'Brien*

Main category: cs.LG

TL;DR: 研究了GPT-2 Small模型中通用神经元的现象，发现它们对模型预测有显著影响，并在训练过程中表现出高稳定性。


<details>
  <summary>Details</summary>
Motivation: 我们研究了独立训练的GPT-2 Small模型中神经元普遍性的现象，研究了这些通用神经元（即在模型中具有一致相关激活的神经元）如何在整个训练过程中产生和演变。

Method: 通过对一个包含500万个tokens的数据集进行激活的两两相关性分析，识别通用神经元。

Result: 消融实验揭示了通用神经元对模型预测的显著功能影响，通过损失和KL散度来衡量。此外，我们量化了神经元的持久性，表明通用神经元在训练检查点中具有很高的稳定性，特别是在更深的层中。

Conclusion: 稳定的和通用的表征结构在神经网络训练过程中出现。

Abstract: We investigate the phenomenon of neuron universality in independently trained
GPT-2 Small models, examining how these universal neurons-neurons with
consistently correlated activations across models-emerge and evolve throughout
training. By analyzing five GPT-2 models at three checkpoints (100k, 200k, 300k
steps), we identify universal neurons through pairwise correlation analysis of
activations over a dataset of 5 million tokens. Ablation experiments reveal
significant functional impacts of universal neurons on model predictions,
measured via loss and KL divergence. Additionally, we quantify neuron
persistence, demonstrating high stability of universal neurons across training
checkpoints, particularly in deeper layers. These findings suggest stable and
universal representational structures emerge during neural network training.

</details>


### [173] [NeuCoReClass AD: Redefining Self-Supervised Time Series Anomaly Detection](https://arxiv.org/abs/2508.00909)
*Aitor Sánchez-Ferrera,Usue Mori,Borja Calvo,Jose A. Lozano*

Main category: cs.LG

TL;DR: NeuCoReClass AD：一种用于时间序列异常检测的自监督多任务框架，它优于现有方法，且不需要领域知识。


<details>
  <summary>Details</summary>
Motivation: 现有的许多方法依赖于单一的代理任务，限制了它们捕获正常数据中有意义模式的能力。此外，它们通常依赖于为特定领域量身定制的手工转换，从而阻碍了它们在各种问题中的推广。

Method: NeuCoReClass AD，一个结合了对比、重建和分类代理任务的自监督多任务时间序列异常检测框架。该方法采用神经转换学习来生成信息丰富、多样化且连贯的增强视图，而无需特定领域的知识。

Result: NeuCoReClass AD 在各种基准测试中始终优于经典基线和大多数深度学习替代方案。

Conclusion: NeuCoReClass AD 在各种基准测试中始终优于经典基线和大多数深度学习替代方案, 并且能够在完全无监督的情况下表征不同的异常配置文件。

Abstract: Time series anomaly detection plays a critical role in a wide range of
real-world applications. Among unsupervised approaches, self-supervised
learning has gained traction for modeling normal behavior without the need of
labeled data. However, many existing methods rely on a single proxy task,
limiting their ability to capture meaningful patterns in normal data. Moreover,
they often depend on handcrafted transformations tailored specific domains,
hindering their generalization accross diverse problems. To address these
limitations, we introduce NeuCoReClass AD, a self-supervised multi-task time
series anomaly detection framework that combines contrastive, reconstruction,
and classification proxy tasks. Our method employs neural transformation
learning to generate augmented views that are informative, diverse, and
coherent, without requiring domain-specific knowledge. We evaluate NeuCoReClass
AD across a wide range of benchmarks, demonstrating that it consistently
outperforms both classical baselines and most deep-learning alternatives.
Furthermore, it enables the characterization of distinct anomaly profiles in a
fully unsupervised manner.

</details>


### [174] [Predictive Auditing of Hidden Tokens in LLM APIs via Reasoning Length Estimation](https://arxiv.org/abs/2508.00912)
*Ziyao Wang,Guoheng Sun,Yexiao He,Zheyu Shen,Bowei Tian,Ang Li*

Main category: cs.LG

TL;DR: PALACE 是一种用户端框架，它通过推理令牌计数估计来预测性地审计 LLM API，无需访问内部跟踪，从而实现细粒度的成本审计和通货膨胀检测。


<details>
  <summary>Details</summary>
Motivation: 商业 LLM 服务通常会隐藏内部推理痕迹，同时仍然向用户收取每个生成的令牌的费用，包括来自隐藏的中间步骤的令牌，这引起了人们对令牌膨胀和潜在过度计费的担忧。这种差距 подчеркивает 了对可靠令牌审计的迫切需求。

Method: PALACE 引入了一个具有轻量级域路由的 GRPO 增强适应模块，从而能够跨不同的推理任务进行动态校准，并减轻令牌使用模式的差异。

Result: 在数学、编码、医疗和一般推理基准上的实验表明，PALACE 实现了较低的相对误差和较强的预测准确性，支持细粒度的成本审计和膨胀检测。

Conclusion: PALACE 是迈向标准化预测审计的重要一步，为提高透明度、问责制和用户信任提供了一条切实可行的途径。

Abstract: Commercial LLM services often conceal internal reasoning traces while still
charging users for every generated token, including those from hidden
intermediate steps, raising concerns of token inflation and potential
overbilling. This gap underscores the urgent need for reliable token auditing,
yet achieving it is far from straightforward: cryptographic verification (e.g.,
hash-based signature) offers little assurance when providers control the entire
execution pipeline, while user-side prediction struggles with the inherent
variance of reasoning LLMs, where token usage fluctuates across domains and
prompt styles. To bridge this gap, we present PALACE (Predictive Auditing of
LLM APIs via Reasoning Token Count Estimation), a user-side framework that
estimates hidden reasoning token counts from prompt-answer pairs without access
to internal traces. PALACE introduces a GRPO-augmented adaptation module with a
lightweight domain router, enabling dynamic calibration across diverse
reasoning tasks and mitigating variance in token usage patterns. Experiments on
math, coding, medical, and general reasoning benchmarks show that PALACE
achieves low relative error and strong prediction accuracy, supporting both
fine-grained cost auditing and inflation detection. Taken together, PALACE
represents an important first step toward standardized predictive auditing,
offering a practical path to greater transparency, accountability, and user
trust.

</details>


### [175] [SmartDate: AI-Driven Precision Sorting and Quality Control in Date Fruits](https://arxiv.org/abs/2508.00921)
*Khaled Eskaf*

Main category: cs.LG

TL;DR: AI-powered system for automated sorting and quality control of date fruits.


<details>
  <summary>Details</summary>
Motivation: Automated sorting and quality control of date fruits.

Method: It combines deep learning, genetic algorithms, and reinforcement learning. The system uses high-resolution imaging and Visible-Near-Infrared (VisNIR) spectral sensors to evaluate key features such as moisture, sugar content, and texture. Reinforcement learning enables real-time adaptation to production conditions, while genetic algorithms optimize model parameters.

Result: SmartDate achieved 94.5 percent accuracy, 93.1 percent F1-score, and an AUC-ROC of 0.96.

Conclusion: SmartDate reduces waste and ensures that only high-quality dates reach the market, setting a new benchmark in smart agriculture.

Abstract: SmartDate is an AI-powered system for automated sorting and quality control
of date fruits. It combines deep learning, genetic algorithms, and
reinforcement learning to improve classification accuracy and predict shelf
life. The system uses high-resolution imaging and Visible-Near-Infrared
(VisNIR) spectral sensors to evaluate key features such as moisture, sugar
content, and texture. Reinforcement learning enables real-time adaptation to
production conditions, while genetic algorithms optimize model parameters.
SmartDate achieved 94.5 percent accuracy, 93.1 percent F1-score, and an AUC-ROC
of 0.96. The system reduces waste and ensures that only high-quality dates
reach the market, setting a new benchmark in smart agriculture.

</details>


### [176] [CaliMatch: Adaptive Calibration for Improving Safe Semi-supervised Learning](https://arxiv.org/abs/2508.00922)
*Jinsoo Bae,Seoung Bum Kim,Hyungrok Do*

Main category: cs.LG

TL;DR: CaliMatch通过校准分类器和OOD检测器来改进安全SSL，并在多个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 半监督学习在实际应用中面临标签分布不匹配问题，现有方法存在深度神经网络中的过度自信问题。

Method: CaliMatch提出了自适应标签平滑和温度缩放，无需手动调整平滑程度。

Result: 在CIFAR-10、CIFAR-100、SVHN、TinyImageNet和ImageNet上的大量评估表明CaliMatch优于现有的安全SSL任务方法。

Conclusion: CaliMatch在安全SSL任务中优于现有方法。

Abstract: Semi-supervised learning (SSL) uses unlabeled data to improve the performance
of machine learning models when labeled data is scarce. However, its real-world
applications often face the label distribution mismatch problem, in which the
unlabeled dataset includes instances whose ground-truth labels are absent from
the labeled training dataset. Recent studies, referred to as safe SSL, have
addressed this issue by using both classification and out-of-distribution (OOD)
detection. However, the existing methods may suffer from overconfidence in deep
neural networks, leading to increased SSL errors because of high confidence in
incorrect pseudo-labels or OOD detection. To address this, we propose a novel
method, CaliMatch, which calibrates both the classifier and the OOD detector to
foster safe SSL. CaliMatch presents adaptive label smoothing and temperature
scaling, which eliminates the need to manually tune the smoothing degree for
effective calibration. We give a theoretical justification for why improving
the calibration of both the classifier and the OOD detector is crucial in safe
SSL. Extensive evaluations on CIFAR-10, CIFAR-100, SVHN, TinyImageNet, and
ImageNet demonstrate that CaliMatch outperforms the existing methods in safe
SSL tasks.

</details>


### [177] [Beyond Benchmarks: Dynamic, Automatic And Systematic Red-Teaming Agents For Trustworthy Medical Language Models](https://arxiv.org/abs/2508.00923)
*Jiazhen Pan,Bailiang Jian,Paul Hager,Yundi Zhang,Che Liu,Friedrike Jungmann,Hongwei Bran Li,Chenyu You,Junde Wu,Jiayuan Zhu,Fenglin Liu,Yuyuan Liu,Niklas Bubeck,Christian Wachinger,Chen,Chen,Zhenyu Gong,Cheng Ouyang,Georgios Kaissis,Benedikt Wiestler,Daniel Rueckert*

Main category: cs.LG

TL;DR: This paper introduces a Dynamic, Automatic, and Systematic (DAS) red-teaming framework to continuously stress-test LLMs and reveal significant weaknesses of current LLMs across four safety-critical domains: robustness, privacy, bias/fairness, and hallucination.


<details>
  <summary>Details</summary>
Motivation: Ensuring the safety and reliability of large language models (LLMs) in clinical practice is critical to prevent patient harm and promote trustworthy healthcare applications of AI. However, LLMs are advancing so rapidly that static safety benchmarks often become obsolete upon publication, yielding only an incomplete and sometimes misleading picture of model trustworthiness.

Method: a Dynamic, Automatic, and Systematic (DAS) red-teaming framework that continuously stress-tests LLMs

Result: Applying DAS to 15 proprietary and open-source LLMs revealed a stark contrast between static benchmark performance and vulnerability under adversarial pressure. Despite a median MedQA accuracy exceeding 80%, 94% of previously correct answers failed our dynamic robustness tests. We observed similarly high failure rates across other domains: privacy leaks were elicited in 86% of scenarios, cognitive-bias priming altered clinical recommendations in 81% of fairness tests, and we identified hallucination rates exceeding 66% in widely used models. Such profound residual risks are incompatible with routine clinical practice.

Conclusion: By converting red-teaming from a static checklist into a dynamic stress-test audit, DAS red-teaming offers the surveillance that hospitals/regulators/technology vendors require as LLMs become embedded in patient chatbots, decision-support dashboards, and broader healthcare workflows. Our framework delivers an evolvable, scalable, and reliable safeguard for the next generation of medical AI.

Abstract: Ensuring the safety and reliability of large language models (LLMs) in
clinical practice is critical to prevent patient harm and promote trustworthy
healthcare applications of AI. However, LLMs are advancing so rapidly that
static safety benchmarks often become obsolete upon publication, yielding only
an incomplete and sometimes misleading picture of model trustworthiness. We
demonstrate that a Dynamic, Automatic, and Systematic (DAS) red-teaming
framework that continuously stress-tests LLMs can reveal significant weaknesses
of current LLMs across four safety-critical domains: robustness, privacy,
bias/fairness, and hallucination. A suite of adversarial agents is applied to
autonomously mutate test cases, identify/evolve unsafe-triggering strategies,
and evaluate responses, uncovering vulnerabilities in real time without human
intervention. Applying DAS to 15 proprietary and open-source LLMs revealed a
stark contrast between static benchmark performance and vulnerability under
adversarial pressure. Despite a median MedQA accuracy exceeding 80\%, 94\% of
previously correct answers failed our dynamic robustness tests. We observed
similarly high failure rates across other domains: privacy leaks were elicited
in 86\% of scenarios, cognitive-bias priming altered clinical recommendations
in 81\% of fairness tests, and we identified hallucination rates exceeding 66\%
in widely used models. Such profound residual risks are incompatible with
routine clinical practice. By converting red-teaming from a static checklist
into a dynamic stress-test audit, DAS red-teaming offers the surveillance that
hospitals/regulators/technology vendors require as LLMs become embedded in
patient chatbots, decision-support dashboards, and broader healthcare
workflows. Our framework delivers an evolvable, scalable, and reliable
safeguard for the next generation of medical AI.

</details>


### [178] [From Generator to Embedder: Harnessing Innate Abilities of Multimodal LLMs via Building Zero-Shot Discriminative Embedding Model](https://arxiv.org/abs/2508.00955)
*Yeong-Joon Ju,Seong-Whan Lee*

Main category: cs.LG

TL;DR: This paper proposes a new framework for universal multimodal embeddings that overcomes the limitations of contrastive pre-training by using a hierarchical embedding prompt template and self-aware hard negative sampling.


<details>
  <summary>Details</summary>
Motivation: Adapting the generative nature of Multimodal Large Language Models (MLLMs) for discriminative representation learning remains a significant challenge. The dominant paradigm of large-scale contrastive pre-training suffers from critical inefficiencies

Method: hierarchical embedding prompt template employs a two-level instruction architecture and self-aware hard negative sampling

Result: hierarchical prompt achieves zero-shot performance competitive with contrastively trained baselines and enhances the fine-tuning process by lifting a simple in-batch negative baseline by 4.8 points on the MMEB benchmark. self-aware hard negative sampling achieves the state-of-the-art performance without the contrative pre-training

Conclusion: This work presents an effective and efficient pathway to adapt MLLMs for universal embedding tasks, significantly reducing training time.

Abstract: Multimodal Large Language Models (MLLMs) have emerged as a promising solution
for universal embedding tasks, yet adapting their generative nature for
discriminative representation learning remains a significant challenge. The
dominant paradigm of large-scale contrastive pre-training suffers from critical
inefficiencies, including prohibitive computational costs and a failure to
leverage the intrinsic, instruction-following capabilities of MLLMs. To
overcome these limitations, we propose an efficient framework for universal
multimodal embeddings, which bridges this gap by centering on two synergistic
components. First, our hierarchical embedding prompt template employs a
two-level instruction architecture that forces the model to produce
discriminative representations. Building on this strong foundation, our second
component, self-aware hard negative sampling, redefines the fine-tuning process
by leveraging the model's own understanding to efficiently mine challenging
negatives while actively filtering out potential false negatives. Our
comprehensive experiments show that our hierarchical prompt achieves zero-shot
performance competitive with contrastively trained baselines and enhances the
fine-tuning process by lifting a simple in-batch negative baseline by 4.8
points on the MMEB benchmark. We further boost the performance via our
self-aware hard negative sampling, achieving the state-of-the-art performance
without the contrative pre-training. Our work presents an effective and
efficient pathway to adapt MLLMs for universal embedding tasks, significantly
reducing training time.

</details>


### [179] [Hybrid Hypergraph Networks for Multimodal Sequence Data Classification](https://arxiv.org/abs/2508.00926)
*Feng Xu,Hui Wang,Yuting Huang,Danwei Zhang,Zizhu Fan*

Main category: cs.LG

TL;DR: 提出了一种新的混合超图网络 (HHN)，用于建模时间多模态数据，并在四个数据集上实现了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 在分类任务中，建模时间多模态数据提出了重大挑战，尤其是在捕获远距离时间依赖性和复杂的跨模态交互方面。视听数据作为一个代表性例子，其本质特征是严格的时间顺序和多样的模态。有效利用时间结构对于理解模内动态和模间相关性至关重要。然而，大多数现有方法独立处理每个模态，并依赖于浅层融合策略，这忽略了时间依赖性，并阻碍了模型表示复杂结构关系的能力。

Method: 我们提出了混合超图网络 (HHN)，这是一种新颖的框架，通过先分割、后图形的策略对时间多模态数据进行建模。HHN 将序列分割成带时间戳的片段，作为异构图中的节点。模内结构通过最大熵差准则引导的超边捕获，增强节点异质性和结构判别，然后进行超图卷积以提取高阶依赖关系。模间链接通过时间对齐和图注意建立，以实现语义融合。

Result: HHN 在四个多模态数据集上实现了最先进 (SOTA) 的结果，证明了其在复杂分类任务中的有效性。

Conclusion: HHN在四个多模态数据集上实现了最先进的结果，证明了其在复杂分类任务中的有效性。

Abstract: Modeling temporal multimodal data poses significant challenges in
classification tasks, particularly in capturing long-range temporal
dependencies and intricate cross-modal interactions. Audiovisual data, as a
representative example, is inherently characterized by strict temporal order
and diverse modalities. Effectively leveraging the temporal structure is
essential for understanding both intra-modal dynamics and inter-modal
correlations. However, most existing approaches treat each modality
independently and rely on shallow fusion strategies, which overlook temporal
dependencies and hinder the model's ability to represent complex structural
relationships. To address the limitation, we propose the hybrid hypergraph
network (HHN), a novel framework that models temporal multimodal data via a
segmentation-first, graph-later strategy. HHN splits sequences into timestamped
segments as nodes in a heterogeneous graph. Intra-modal structures are captured
via hyperedges guided by a maximum entropy difference criterion, enhancing node
heterogeneity and structural discrimination, followed by hypergraph convolution
to extract high-order dependencies. Inter-modal links are established through
temporal alignment and graph attention for semantic fusion. HHN achieves
state-of-the-art (SOTA) results on four multimodal datasets, demonstrating its
effectiveness in complex classification tasks.

</details>


### [180] [Learning Unified User Quantized Tokenizers for User Representation](https://arxiv.org/abs/2508.00956)
*Chuan He,Yang Chen,Wuliang Huang,Tianyi Zheng,Jianhu Chen,Bin Dou,Yice Luo,Yun Zhu,Baokun Wang,Yongchao Liu,Xing Fu,Yu Cheng,Chuntao Hong,Weiqiang Wang,Xin-Wei Yao*

Main category: cs.LG

TL;DR: U^2QT：一种用于多源用户表示学习的统一框架，通过跨域知识转移和异构域的早期融合来解决现有方法的局限性，并在各种下游任务中实现了效率提升。


<details>
  <summary>Details</summary>
Motivation: 先前的工作采用 late-fusion 策略来组合异构数据源，但它们存在三个关键限制：缺乏统一的表示框架、数据压缩中的可扩展性和存储问题以及不灵活的跨任务泛化。

Method: U^2QT 采用两阶段架构：首先，因果 Q-Former 将特定领域的特征投影到共享的因果表示空间，以保留模态间的依赖关系；其次，多视图 RQ-VAE 通过共享和源特定的密码本将因果嵌入离散化为紧凑的token，从而实现高效存储，同时保持语义连贯性。

Result: 实验结果表明，U^2QT 在各种下游任务中都具有优势，在未来的行为预测和推荐任务中优于特定任务的基线，同时在存储和计算方面实现了效率提升。

Conclusion: U^2QT在各种下游任务中表现出色，在未来行为预测和推荐任务中优于特定任务的基线，同时在存储和计算方面实现了效率提升。统一的标记化框架能够与语言模型无缝集成，并支持工业规模的应用。

Abstract: Multi-source user representation learning plays a critical role in enabling
personalized services on web platforms (e.g., Alipay). While prior works have
adopted late-fusion strategies to combine heterogeneous data sources, they
suffer from three key limitations: lack of unified representation frameworks,
scalability and storage issues in data compression, and inflexible cross-task
generalization. To address these challenges, we propose U^2QT (Unified User
Quantized Tokenizers), a novel framework that integrates cross-domain knowledge
transfer with early fusion of heterogeneous domains. Our framework employs a
two-stage architecture: first, a causal Q-Former projects domain-specific
features into a shared causal representation space to preserve inter-modality
dependencies; second, a multi-view RQ-VAE discretizes causal embeddings into
compact tokens through shared and source-specific codebooks, enabling efficient
storage while maintaining semantic coherence. Experimental results showcase
U^2QT's advantages across diverse downstream tasks, outperforming task-specific
baselines in future behavior prediction and recommendation tasks while
achieving efficiency gains in storage and computation. The unified tokenization
framework enables seamless integration with language models and supports
industrial-scale applications.

</details>


### [181] [Cooperative effects in feature importance of individual patterns: application to air pollutants and Alzheimer disease](https://arxiv.org/abs/2508.00930)
*M. Ontivero-Ortega,A. Fania,A. Lacalamita,R. Bellotti,A. Monaco,S. Stramaglia*

Main category: cs.LG

TL;DR: This paper presents a framework to assign unique, redundant, and synergistic scores to each individual pattern of the data set, while comparing it with the well-known measure of feature importance named {"it Shapley effect"}.


<details>
  <summary>Details</summary>
Motivation: quantify cooperative effects in feature importance (Hi-Fi), a key technique in explainable artificial intelligence (XAI), so as to disentangle high-order effects involving a particular input feature in regression problems.

Method: adaptive version of the widely used metric Leave One Covariate Out (LOCO)

Result: synergistic association between features related to $O_3$ and $NO_2$ with mortality, especially in the provinces of Bergamo e Brescia; notably also the density of urban green areas displays synergistic influence with pollutants for the prediction of AD mortality.

Conclusion: Local Hi-Fi is a promising tool of wide applicability, which opens new perspectives for XAI as well as to analyze high-order relationships in complex systems.

Abstract: Leveraging recent advances in the analysis of synergy and redundancy in
systems of random variables, an adaptive version of the widely used metric
Leave One Covariate Out (LOCO) has been recently proposed to quantify
cooperative effects in feature importance (Hi-Fi), a key technique in
explainable artificial intelligence (XAI), so as to disentangle high-order
effects involving a particular input feature in regression problems.
Differently from standard feature importance tools, where a single score
measures the relevance of each feature, each feature is here characterized by
three scores, a two-body (unique) score and higher-order scores (redundant and
synergistic). This paper presents a framework to assign those three scores
(unique, redundant, and synergistic) to each individual pattern of the data
set, while comparing it with the well-known measure of feature importance named
{\it Shapley effect}. To illustrate the potential of the proposed framework, we
focus on a One-Health application: the relation between air pollutants and
Alzheimer's disease mortality rate. Our main result is the synergistic
association between features related to $O_3$ and $NO_2$ with mortality,
especially in the provinces of Bergamo e Brescia; notably also the density of
urban green areas displays synergistic influence with pollutants for the
prediction of AD mortality. Our results place local Hi-Fi as a promising tool
of wide applicability, which opens new perspectives for XAI as well as to
analyze high-order relationships in complex systems.

</details>


### [182] [OKG-LLM: Aligning Ocean Knowledge Graph with Observation Data via LLMs for Global Sea Surface Temperature Prediction](https://arxiv.org/abs/2508.00933)
*Hanchen Yang,Jiaqi Wang,Jiannong Cao,Wengen Li,Jialun Zheng,Yangning Li,Chunyu Miao,Jihong Guan,Shuigeng Zhou,Philip S. Yu*

Main category: cs.LG

TL;DR: 我们提出了OKG-LLM，这是一个用于全球SST预测的新框架，它利用海洋知识图来增强LLM，并在实际数据集中优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的数据驱动方法在SST预测中取得了显著成功，但它们通常忽略了利用过去几十年积累的丰富的领域知识，限制了预测精度的进一步提高。大型语言模型(llm)的最新出现突出了整合领域知识以完成下游任务的潜力。然而，由于整合海洋领域知识和数值数据的挑战，llm在SST预测中的应用仍有待探索。

Method: 我们提出了海洋知识图增强LLM (OKG-LLM)，这是一种用于全球SST预测的新框架。我们构建了一个海洋知识图(OKG)，专门用于表示各种海洋知识以进行SST预测。然后，我们开发了一个图嵌入网络来学习OKG中全面的语义和结构知识，捕捉各个海域的独特特征以及它们之间复杂的关联。最后，我们将学习到的知识与细粒度的数值SST数据对齐和融合，并利用预训练的LLM来建模SST模式，以实现准确的预测。

Result: OKG-LLM始终优于最先进的方法，展示了其有效性、稳健性和推进SST预测的潜力。

Conclusion: OKG-LLM在真实世界数据集上的大量实验表明，其性能始终优于最先进的方法，展示了其有效性、稳健性和推进SST预测的潜力。

Abstract: Sea surface temperature (SST) prediction is a critical task in ocean science,
supporting various applications, such as weather forecasting, fisheries
management, and storm tracking. While existing data-driven methods have
demonstrated significant success, they often neglect to leverage the rich
domain knowledge accumulated over the past decades, limiting further
advancements in prediction accuracy. The recent emergence of large language
models (LLMs) has highlighted the potential of integrating domain knowledge for
downstream tasks. However, the application of LLMs to SST prediction remains
underexplored, primarily due to the challenge of integrating ocean domain
knowledge and numerical data. To address this issue, we propose Ocean Knowledge
Graph-enhanced LLM (OKG-LLM), a novel framework for global SST prediction. To
the best of our knowledge, this work presents the first systematic effort to
construct an Ocean Knowledge Graph (OKG) specifically designed to represent
diverse ocean knowledge for SST prediction. We then develop a graph embedding
network to learn the comprehensive semantic and structural knowledge within the
OKG, capturing both the unique characteristics of individual sea regions and
the complex correlations between them. Finally, we align and fuse the learned
knowledge with fine-grained numerical SST data and leverage a pre-trained LLM
to model SST patterns for accurate prediction. Extensive experiments on the
real-world dataset demonstrate that OKG-LLM consistently outperforms
state-of-the-art methods, showcasing its effectiveness, robustness, and
potential to advance SST prediction. The codes are available in the online
repository.

</details>


### [183] [FeatureCuts: Feature Selection for Large Data by Optimizing the Cutoff](https://arxiv.org/abs/2508.00954)
*Andy Hu,Devika Prasad,Luiz Pizzato,Nicholas Foord,Arman Abrahamyan,Anna Leontjeva,Cooper Doyle,Dan Jermyn*

Main category: cs.LG

TL;DR: FeatureCuts is a feature selection algorithm that adaptively selects the optimal feature cutoff after performing filter ranking. It achieves better feature reduction and computation time while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: finding a reduced subset of features that captures most of the information required to train an accurate and efficient model

Method: a novel feature selection algorithm that adaptively selects the optimal feature cutoff after performing filter ranking

Result: achieved, on average, 15 percentage points more feature reduction and up to 99.6% less computation time while maintaining model performance, compared to existing state-of-the-art methods

Conclusion: FeatureCuts achieves better feature reduction and computation time while maintaining model performance.

Abstract: In machine learning, the process of feature selection involves finding a
reduced subset of features that captures most of the information required to
train an accurate and efficient model. This work presents FeatureCuts, a novel
feature selection algorithm that adaptively selects the optimal feature cutoff
after performing filter ranking. Evaluated on 14 publicly available datasets
and one industry dataset, FeatureCuts achieved, on average, 15 percentage
points more feature reduction and up to 99.6% less computation time while
maintaining model performance, compared to existing state-of-the-art methods.
When the selected features are used in a wrapper method such as Particle Swarm
Optimization (PSO), it enables 25 percentage points more feature reduction,
requires 66% less computation time, and maintains model performance when
compared to PSO alone. The minimal overhead of FeatureCuts makes it scalable
for large datasets typically seen in enterprise applications.

</details>


### [184] [Small sample-based adaptive text classification through iterative and contrastive description refinement](https://arxiv.org/abs/2508.00957)
*Amrit Rajeev,Udayaadithya Avadhanam,Harshula Tulapurkar,SaiBarath Sundar*

Main category: cs.LG

TL;DR: This paper proposes a classification framework that combines iterative topic refinement, contrastive prompting, and active learning to address zero-shot text classification in dynamic environments. The results highlight the effectiveness of prompt-based semantic reasoning for fine-grained classification with limited supervision.


<details>
  <summary>Details</summary>
Motivation: Zero-shot text classification remains a difficult task in domains with evolving knowledge and ambiguous category boundaries, such as ticketing systems. Large language models (LLMs) often struggle to generalize in these scenarios due to limited topic separability, while few-shot methods are constrained by insufficient data diversity.

Method: a classification framework that combines iterative topic refinement, contrastive prompting, and active learning

Result: 91% accuracy on AGNews (3 seen, 1 unseen class) and 84% on DBpedia (8 seen, 1 unseen), with minimal accuracy shift after introducing unseen classes (82% and 87%, respectively).

Conclusion: prompt-based semantic reasoning is effective for fine-grained classification with limited supervision.

Abstract: Zero-shot text classification remains a difficult task in domains with
evolving knowledge and ambiguous category boundaries, such as ticketing
systems. Large language models (LLMs) often struggle to generalize in these
scenarios due to limited topic separability, while few-shot methods are
constrained by insufficient data diversity. We propose a classification
framework that combines iterative topic refinement, contrastive prompting, and
active learning. Starting with a small set of labeled samples, the model
generates initial topic labels. Misclassified or ambiguous samples are then
used in an iterative contrastive prompting process to refine category
distinctions by explicitly teaching the model to differentiate between closely
related classes. The framework features a human-in-the-loop component, allowing
users to introduce or revise category definitions in natural language. This
enables seamless integration of new, unseen categories without retraining,
making the system well-suited for real-world, dynamic environments. The
evaluations on AGNews and DBpedia demonstrate strong performance: 91% accuracy
on AGNews (3 seen, 1 unseen class) and 84% on DBpedia (8 seen, 1 unseen), with
minimal accuracy shift after introducing unseen classes (82% and 87%,
respectively). The results highlight the effectiveness of prompt-based semantic
reasoning for fine-grained classification with limited supervision.

</details>


### [185] [Controllable and Stealthy Shilling Attacks via Dispersive Latent Diffusion](https://arxiv.org/abs/2508.01987)
*Shutong Qiao,Wei Yuan,Junliang Yu,Tong Chen,Quoc Viet Hung Nguyen,Hongzhi Yin*

Main category: cs.LG

TL;DR: 提出了一种新的攻击框架，可以生成更有效但难以区分的虚假用户，实验表明现代推荐系统比以前认为的更容易受到攻击。


<details>
  <summary>Details</summary>
Motivation: 推荐系统依赖于用户贡献的数据，容易受到注入虚假用户以操纵项目排名的恶意攻击。现有攻击模型无法同时实现两个关键目标：实现对目标项目的强大对抗性推广，同时保持逼真的行为以避免被发现。

Method: 提出了一种基于扩散的攻击框架DLDA，该框架通过对目标项目进行细粒度控制来生成高效但难以区分的虚假用户。

Result: 在三个真实世界数据集和五个流行的RS模型上进行的大量实验表明，与之前的攻击相比，DLDA始终能够实现更强的项目推广，同时更难被检测到。

Conclusion: 现代推荐系统比以前认为的更容易受到攻击，因此迫切需要更强大的防御措施。

Abstract: Recommender systems (RSs) are now fundamental to various online platforms,
but their dependence on user-contributed data leaves them vulnerable to
shilling attacks that can manipulate item rankings by injecting fake users.
Although widely studied, most existing attack models fail to meet two critical
objectives simultaneously: achieving strong adversarial promotion of target
items while maintaining realistic behavior to evade detection. As a result, the
true severity of shilling threats that manage to reconcile the two objectives
remains underappreciated. To expose this overlooked vulnerability, we present
DLDA, a diffusion-based attack framework that can generate highly effective yet
indistinguishable fake users by enabling fine-grained control over target
promotion. Specifically, DLDA operates in a pre-aligned collaborative embedding
space, where it employs a conditional latent diffusion process to iteratively
synthesize fake user profiles with precise target item control. To evade
detection, DLDA introduces a dispersive regularization mechanism that promotes
variability and realism in generated behavioral patterns. Extensive experiments
on three real-world datasets and five popular RS models demonstrate that,
compared to prior attacks, DLDA consistently achieves stronger item promotion
while remaining harder to detect. These results highlight that modern RSs are
more vulnerable than previously recognized, underscoring the urgent need for
more robust defenses.

</details>


### [186] [Enhancing material behavior discovery using embedding-oriented Physically-Guided Neural Networks with Internal Variables](https://arxiv.org/abs/2508.00959)
*Rubén Muñoz-Sierra,Manuel Doblaré,Jacobo Ayensa-Jiménez*

Main category: cs.LG

TL;DR: 本文提出了一些增强的PGNNIV框架，通过降阶建模技术解决了可扩展性限制，并集成了通过迁移学习和微调策略进行模型重用，以利用先前获得的知识。


<details>
  <summary>Details</summary>
Motivation: 具有内部变量的物理引导神经网络是SciML工具，它仅使用可观察数据进行训练，并具有解开内部状态关系的能力。它们通过指定模型架构和使用损失正则化来结合物理知识，从而赋予某些特定神经元以物理意义作为内部状态变量。尽管这些模型具有潜力，但在应用于高维数据（如细网格空间场或随时间演化的系统）时，它们面临着可扩展性方面的挑战。

Method: 我们通过降阶建模技术，为PGNNIV框架提出了一些增强功能，解决了这些可扩展性限制。具体来说，我们引入了使用频谱分解、POD和预训练的基于自动编码器的映射来替代原始解码器结构。

Result: 结果表明，增强的PGNNIV框架成功识别了底层本构状态方程，同时保持了较高的预测精度。

Conclusion: 增强的PGNNIV框架成功识别了底层本构状态方程，同时保持了较高的预测精度。它还提高了对噪声的鲁棒性，减轻了过拟合，并降低了计算需求。所提出的技术可以根据数据可用性、资源和具体的建模目标进行定制，克服了所有场景中的可扩展性挑战。

Abstract: Physically Guided Neural Networks with Internal Variables are SciML tools
that use only observable data for training and and have the capacity to unravel
internal state relations. They incorporate physical knowledge both by
prescribing the model architecture and using loss regularization, thus endowing
certain specific neurons with a physical meaning as internal state variables.
Despite their potential, these models face challenges in scalability when
applied to high-dimensional data such as fine-grid spatial fields or
time-evolving systems. In this work, we propose some enhancements to the PGNNIV
framework that address these scalability limitations through reduced-order
modeling techniques. Specifically, we introduce alternatives to the original
decoder structure using spectral decomposition, POD, and pretrained
autoencoder-based mappings. These surrogate decoders offer varying trade-offs
between computational efficiency, accuracy, noise tolerance, and
generalization, while improving drastically the scalability. Additionally, we
integrate model reuse via transfer learning and fine-tuning strategies to
exploit previously acquired knowledge, supporting efficient adaptation to novel
materials or configurations, and significantly reducing training time while
maintaining or improving model performance. To illustrate these various
techniques, we use a representative case governed by the nonlinear diffusion
equation, using only observable data. Results demonstrate that the enhanced
PGNNIV framework successfully identifies the underlying constitutive state
equations while maintaining high predictive accuracy. It also improves
robustness to noise, mitigates overfitting, and reduces computational demands.
The proposed techniques can be tailored to various scenarios depending on data
availability, resources, and specific modeling objectives, overcoming
scalability challenges in all the scenarios.

</details>


### [187] [Compression-Induced Communication-Efficient Large Model Training and Inferencing](https://arxiv.org/abs/2508.00960)
*Sudip K. Seal,Maksudul Alam,Jorge Ramirez,Sajal Dash,Hao Lu*

Main category: cs.LG

TL;DR: Phantom parallelism is introduced to minimize energy consumption in large neural network training, achieving ~50% energy reduction compared to tensor parallelism and enabling training of smaller models on fewer GPUs.


<details>
  <summary>Details</summary>
Motivation: Energy efficiency of training and inferencing with large neural network models is a critical challenge.

Method: The paper introduces phantom parallelism and derives new forward and backward propagation operators, implementing them as custom autograd operations within an end-to-end training pipeline. The performance and energy-efficiency are compared against conventional tensor parallel training pipelines.

Result: Experiments show a ~50% reduction in energy consumption for training FFNs using phantom parallelism compared to conventional tensor parallel methods. The approach also enables training smaller models to the same loss on fewer GPUs.

Conclusion: The proposed phantom parallel approach can reduce energy consumption by ~50% when training FFNs compared to conventional tensor parallel methods. Smaller phantom models can achieve the same model loss as larger tensor parallel models on fewer GPUs, offering even greater energy savings.

Abstract: Energy efficiency of training and inferencing with large neural network
models is a critical challenge facing the future of sustainable large-scale
machine learning workloads. This paper introduces an alternative strategy,
called phantom parallelism, to minimize the net energy consumption of
traditional tensor (model) parallelism, the most energy-inefficient component
of large neural network training. The approach is presented in the context of
feed-forward network architectures as a preliminary, but comprehensive,
proof-of-principle study of the proposed methodology. We derive new forward and
backward propagation operators for phantom parallelism, implement them as
custom autograd operations within an end-to-end phantom parallel training
pipeline and compare its parallel performance and energy-efficiency against
those of conventional tensor parallel training pipelines. Formal analyses that
predict lower bandwidth and FLOP counts are presented with supporting empirical
results on up to 256 GPUs that corroborate these gains. Experiments are shown
to deliver ~50% reduction in the energy consumed to train FFNs using the
proposed phantom parallel approach when compared with conventional tensor
parallel methods. Additionally, the proposed approach is shown to train smaller
phantom models to the same model loss on smaller GPU counts as larger tensor
parallel models on larger GPU counts offering the possibility for even greater
energy savings.

</details>


### [188] [FinKario: Event-Enhanced Automated Construction of Financial Knowledge Graph](https://arxiv.org/abs/2508.00961)
*Xiang Li,Penglei Sun,Wanyun Zhou,Zikai Wei,Yongqi Zhang,Xiaowen Chu*

Main category: cs.LG

TL;DR: This paper introduces FinKario, a financial knowledge graph, and FinKario-RAG, a retrieval strategy, to improve stock trend prediction accuracy, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Individual investors are significantly outnumbered and disadvantaged in financial markets, overwhelmed by abundant information and lacking professional analysis. Equity research reports stand out as crucial resources, offering valuable insights. However, two key challenges limit their effectiveness: (1) the rapid evolution of market events often outpaces the slow update cycles of existing knowledge bases, (2) the long-form and unstructured nature of financial reports further hinders timely and context-aware integration by LLMs.

Method: We introduce the Event-Enhanced Automated Construction of Financial Knowledge Graph (FinKario), a dataset comprising over 305,360 entities, 9,625 relational triples, and 19 distinct relation types. Additionally, we propose a Two-Stage, Graph-Based retrieval strategy (FinKario-RAG), optimizing the retrieval of evolving, large-scale financial knowledge to ensure efficient and precise data access.

Result: FinKario with FinKario-RAG achieves superior stock trend prediction accuracy, outperforming financial LLMs by 18.81% and institutional strategies by 17.85% on average in backtesting.

Conclusion: FinKario with FinKario-RAG achieves superior stock trend prediction accuracy, outperforming financial LLMs by 18.81% and institutional strategies by 17.85% on average in backtesting.

Abstract: Individual investors are significantly outnumbered and disadvantaged in
financial markets, overwhelmed by abundant information and lacking professional
analysis. Equity research reports stand out as crucial resources, offering
valuable insights. By leveraging these reports, large language models (LLMs)
can enhance investors' decision-making capabilities and strengthen financial
analysis. However, two key challenges limit their effectiveness: (1) the rapid
evolution of market events often outpaces the slow update cycles of existing
knowledge bases, (2) the long-form and unstructured nature of financial reports
further hinders timely and context-aware integration by LLMs. To address these
challenges, we tackle both data and methodological aspects. First, we introduce
the Event-Enhanced Automated Construction of Financial Knowledge Graph
(FinKario), a dataset comprising over 305,360 entities, 9,625 relational
triples, and 19 distinct relation types. FinKario automatically integrates
real-time company fundamentals and market events through prompt-driven
extraction guided by professional institutional templates, providing structured
and accessible financial insights for LLMs. Additionally, we propose a
Two-Stage, Graph-Based retrieval strategy (FinKario-RAG), optimizing the
retrieval of evolving, large-scale financial knowledge to ensure efficient and
precise data access. Extensive experiments show that FinKario with FinKario-RAG
achieves superior stock trend prediction accuracy, outperforming financial LLMs
by 18.81% and institutional strategies by 17.85% on average in backtesting.

</details>


### [189] [Rethinking Multimodality: Optimizing Multimodal Deep Learning for Biomedical Signal Classification](https://arxiv.org/abs/2508.00963)
*Timothy Oladunni,Alex Wong*

Main category: cs.LG

TL;DR: Optimal domain fusion isn't about the number of modalities, but the quality of their inherent complementarity.


<details>
  <summary>Details</summary>
Motivation: While fusing multiple domains often presumes enhanced accuracy, this work demonstrates that adding modalities can yield diminishing returns, as not all fusions are inherently advantageous.

Method: Five deep learning models were designed, developed, and rigorously evaluated: three unimodal (1D-CNN for time, 2D-CNN for time-frequency, and 1D-CNN-Transformer for frequency) and two multimodal (Hybrid 1, which fuses 1D-CNN and 2D-CNN; Hybrid 2, which combines 1D-CNN, 2D-CNN, and a Transformer).

Result: Hybrid 1 consistently outperformed the 2D-CNN baseline across all metrics (p-values < 0.05, Bayesian probabilities > 0.90), confirming the synergistic complementarity of the time and time-frequency domains. Hybrid 2's inclusion of the frequency domain offered no further improvement and sometimes a marginal decline, indicating representational redundancy.

Conclusion: Optimal domain fusion isn't about the number of modalities, but the quality of their inherent complementarity. Optimal multimodal performance arises from the intrinsic information-theoretic complementarity among fused domains.

Abstract: This study proposes a novel perspective on multimodal deep learning for
biomedical signal classification, systematically analyzing how complementary
feature domains impact model performance. While fusing multiple domains often
presumes enhanced accuracy, this work demonstrates that adding modalities can
yield diminishing returns, as not all fusions are inherently advantageous. To
validate this, five deep learning models were designed, developed, and
rigorously evaluated: three unimodal (1D-CNN for time, 2D-CNN for
time-frequency, and 1D-CNN-Transformer for frequency) and two multimodal
(Hybrid 1, which fuses 1D-CNN and 2D-CNN; Hybrid 2, which combines 1D-CNN,
2D-CNN, and a Transformer). For ECG classification, bootstrapping and Bayesian
inference revealed that Hybrid 1 consistently outperformed the 2D-CNN baseline
across all metrics (p-values < 0.05, Bayesian probabilities > 0.90), confirming
the synergistic complementarity of the time and time-frequency domains.
Conversely, Hybrid 2's inclusion of the frequency domain offered no further
improvement and sometimes a marginal decline, indicating representational
redundancy; a phenomenon further substantiated by a targeted ablation study.
This research redefines a fundamental principle of multimodal design in
biomedical signal analysis. We demonstrate that optimal domain fusion isn't
about the number of modalities, but the quality of their inherent
complementarity. This paradigm-shifting concept moves beyond purely heuristic
feature selection. Our novel theoretical contribution, "Complementary Feature
Domains in Multimodal ECG Deep Learning," presents a mathematically
quantifiable framework for identifying ideal domain combinations, demonstrating
that optimal multimodal performance arises from the intrinsic
information-theoretic complementarity among fused domains.

</details>


### [190] [Graph Embedding in the Graph Fractional Fourier Transform Domain](https://arxiv.org/abs/2508.02383)
*Changjie Sheng,Zhichao Zhang,Wei Yao*

Main category: cs.LG

TL;DR: GEFRFE enhances embedding informativeness via the graph fractional domain, captures richer structural features and significantly enhance classification performance.


<details>
  <summary>Details</summary>
Motivation: The embedding space of traditional spectral embedding methods often exhibit limited expressiveness, failing to exhaustively capture latent structural features across alternative transform domains.

Method: graph fractional Fourier transform to extend the existing state-of-the-art generalized frequency filtering embedding (GEFFE) into fractional domains

Result: significantly enhance classification performance on six benchmark datasets

Conclusion: GEFRFE captures richer structural features and significantly enhance classification performance, while retaining comparable computational complexity to GEFFE approaches.

Abstract: Spectral graph embedding plays a critical role in graph representation
learning by generating low-dimensional vector representations from graph
spectral information. However, the embedding space of traditional spectral
embedding methods often exhibit limited expressiveness, failing to exhaustively
capture latent structural features across alternative transform domains. To
address this issue, we use the graph fractional Fourier transform to extend the
existing state-of-the-art generalized frequency filtering embedding (GEFFE)
into fractional domains, giving birth to the generalized fractional filtering
embedding (GEFRFE), which enhances embedding informativeness via the graph
fractional domain. The GEFRFE leverages graph fractional domain filtering and a
nonlinear composition of eigenvector components derived from a fractionalized
graph Laplacian. To dynamically determine the fractional order, two parallel
strategies are introduced: search-based optimization and a ResNet18-based
adaptive learning. Extensive experiments on six benchmark datasets demonstrate
that the GEFRFE captures richer structural features and significantly enhance
classification performance. Notably, the proposed method retains computational
complexity comparable to GEFFE approaches.

</details>


### [191] [VAULT: Vigilant Adversarial Updates via LLM-Driven Retrieval-Augmented Generation for NLI](https://arxiv.org/abs/2508.00965)
*Roie Kazoom,Ofir Cohen,Rami Puzis,Asaf Shabtai,Ofer Hadar*

Main category: cs.LG

TL;DR: VAULT是一个全自动的对抗性RAG流程，通过检索、生成和迭代重新训练，显著提高了NLI模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有NLI模型存在弱点，VAULT旨在发现并弥补这些弱点。

Method: VAULT使用包含语义和词汇相似性的嵌入检索、LLM提示生成对抗假设，并通过LLM集成验证标签保真度。

Result: VAULT在SNLI上将RoBERTa-base准确率从88.48%提升到92.60%，在ANLI上从75.04%提升到80.95%，在MultiNLI上从54.67%提升到71.99%，并且始终优于先前的上下文对抗方法。

Conclusion: VAULT通过自动生成对抗样本并迭代重新训练，显著提高了RoBERTa-base模型在SNLI、ANLI和MultiNLI数据集上的准确率，且优于现有的上下文对抗方法。

Abstract: We introduce VAULT, a fully automated adversarial RAG pipeline that
systematically uncovers and remedies weaknesses in NLI models through three
stages: retrieval, adversarial generation, and iterative retraining. First, we
perform balanced few-shot retrieval by embedding premises with both semantic
(BGE) and lexical (BM25) similarity. Next, we assemble these contexts into LLM
prompts to generate adversarial hypotheses, which are then validated by an LLM
ensemble for label fidelity. Finally, the validated adversarial examples are
injected back into the training set at increasing mixing ratios, progressively
fortifying a zero-shot RoBERTa-base model.On standard benchmarks, VAULT
elevates RoBERTa-base accuracy from 88.48% to 92.60% on SNLI +4.12%, from
75.04% to 80.95% on ANLI +5.91%, and from 54.67% to 71.99% on MultiNLI +17.32%.
It also consistently outperforms prior in-context adversarial methods by up to
2.0% across datasets. By automating high-quality adversarial data curation at
scale, VAULT enables rapid, human-independent robustness improvements in NLI
inference tasks.

</details>


### [192] [Masked Omics Modeling for Multimodal Representation Learning across Histopathology and Molecular Profiles](https://arxiv.org/abs/2508.00969)
*Lucas Robinet,Ahmad Berjaoui,Elizabeth Cohen-Jonathan Moyal*

Main category: cs.LG

TL;DR: MORPHEUS 是一个统一的 Transformer 框架，它使用组织病理学和多组学数据进行预训练，以实现卓越的癌症分析性能。


<details>
  <summary>Details</summary>
Motivation: 组织病理学通常不足以进行分子表征和理解临床结果，因为重要信息包含在高维组学图谱（如转录组学、甲基组学或基因组学）中。

Method: 统一的基于 Transformer 的预训练框架，可将组织病理学和多组学数据编码到共享的潜在空间中。MORPHEUS 依赖于应用于随机选择的组学部分的 masked modeling 目标，鼓励模型学习具有生物学意义的跨模态关系。

Result: MORPHEUS 能够实现任意到任意的组学生成，从而能够从任何模态子集中推断出一个或多个组学图谱，包括仅 H&E。在大型泛癌队列上进行预训练后，MORPHEUS 在各种模态组合和任务中始终优于最先进的方法。

Conclusion: MORPHEUS 在各种模态组合和任务中始终优于最先进的方法，定位为在肿瘤学中开发多模态基础模型的有前途的框架。

Abstract: Self-supervised learning has driven major advances in computational pathology
by enabling models to learn rich representations from hematoxylin and eosin
(H&E)-stained cancer tissue. However, histopathology alone often falls short
for molecular characterization and understanding clinical outcomes, as
important information is contained in high-dimensional omics profiles like
transcriptomics, methylomics, or genomics. In this work, we introduce MORPHEUS,
a unified transformer-based pre-training framework that encodes both
histopathology and multi-omics data into a shared latent space. At its core,
MORPHEUS relies on a masked modeling objective applied to randomly selected
omics portions, encouraging the model to learn biologically meaningful
cross-modal relationships. The same pre-trained network can be applied to
histopathology alone or in combination with any subset of omics modalities,
seamlessly adapting to the available inputs. Additionally, MORPHEUS enables
any-to-any omics generation, enabling one or more omics profiles to be inferred
from any subset of modalities, including H&E alone. Pre-trained on a large
pan-cancer cohort, MORPHEUS consistently outperforms state-of-the-art methods
across diverse modality combinations and tasks, positioning itself as a
promising framework for developing multimodal foundation models in oncology.
The code is available at: https://github.com/Lucas-rbnt/MORPHEUS

</details>


### [193] [Optimal Scheduling Algorithms for LLM Inference: Theory and Practice](https://arxiv.org/abs/2508.01002)
*Agrim Bari,Parikshit Hegde,Gustavo de Veciana*

Main category: cs.LG

TL;DR: Proposes RAD and SLAI schedulers for efficient LLM inference, achieving throughput optimality and reducing TTFT.


<details>
  <summary>Details</summary>
Motivation: Rising need for efficient LLM inference systems due to the growing use of LLM-based tools.

Method: Resource-Aware Dynamic (RAD) scheduler and SLO-Aware LLM Inference (SLAI) scheduler

Result: RAD scheduler achieves throughput optimality; SLAI reduces median TTFT by 53% and increases maximum serving capacity by 26% compared to Sarathi-Serve.

Conclusion: SLAI reduces the median TTFT by 53% and increases the maximum serving capacity by 26% compared to Sarathi-Serve, while meeting tail TBT latency constraints.

Abstract: With the growing use of Large Language Model (LLM)-based tools like ChatGPT,
Perplexity, and Gemini across industries, there is a rising need for efficient
LLM inference systems. These systems handle requests with a unique two-phase
computation structure: a prefill-phase that processes the full input prompt and
a decode-phase that autoregressively generates tokens one at a time. This
structure calls for new strategies for routing and scheduling requests.
  In this paper, we take a comprehensive approach to this challenge by
developing a theoretical framework that models routing and scheduling in LLM
inference systems. We identify two key design principles-optimal tiling and
dynamic resource allocation-that are essential for achieving high throughput.
Guided by these principles, we propose the Resource-Aware Dynamic (RAD)
scheduler and prove that it achieves throughput optimality under mild
conditions. To address practical Service Level Objectives (SLOs) such as
serving requests with different Time Between Token (TBT) constraints, we design
the SLO-Aware LLM Inference (SLAI) scheduler. SLAI uses real-time measurements
to prioritize decode requests that are close to missing their TBT deadlines and
reorders prefill requests based on known prompt lengths to further reduce the
Time To First Token (TTFT) delays.
  We evaluate SLAI on the Openchat ShareGPT4 dataset using the Mistral-7B model
on an NVIDIA RTX ADA 6000 GPU. Compared to Sarathi-Serve, SLAI reduces the
median TTFT by 53% and increases the maximum serving capacity by 26% such that
median TTFT is below 0.5 seconds, while meeting tail TBT latency constraints.

</details>


### [194] [v-PuNNs: van der Put Neural Networks for Transparent Ultrametric Representation Learning](https://arxiv.org/abs/2508.01010)
*Gnankan Landry Regis N'guessan*

Main category: cs.LG

TL;DR: v-PuNNs, a new neural network architecture based on p-adic numbers, achieves state-of-the-art results on hierarchical data benchmarks with exact, interpretable, and efficient models.


<details>
  <summary>Details</summary>
Motivation: Conventional deep learning models embed data in Euclidean space which is a poor fit for strictly hierarchical objects.

Method: van der Put Neural Networks (v-PuNNs) with Valuation-Adaptive Perturbation Optimization (VAPO).

Result: On three canonical benchmarks our CPU-only implementation sets new state-of-the-art.

Conclusion: v-PuNNs bridge number theory and deep learning, offering exact, interpretable, and efficient models for hierarchical data.

Abstract: Conventional deep learning models embed data in Euclidean space
$\mathbb{R}^d$, a poor fit for strictly hierarchical objects such as taxa, word
senses, or file systems. We introduce van der Put Neural Networks (v-PuNNs),
the first architecture whose neurons are characteristic functions of p-adic
balls in $\mathbb{Z}_p$. Under our Transparent Ultrametric Representation
Learning (TURL) principle every weight is itself a p-adic number, giving exact
subtree semantics. A new Finite Hierarchical Approximation Theorem shows that a
depth-K v-PuNN with $\sum_{j=0}^{K-1}p^{\,j}$ neurons universally represents
any K-level tree. Because gradients vanish in this discrete space, we propose
Valuation-Adaptive Perturbation Optimization (VAPO), with a fast deterministic
variant (HiPaN-DS) and a moment-based one (HiPaN / Adam-VAPO). On three
canonical benchmarks our CPU-only implementation sets new state-of-the-art:
WordNet nouns (52,427 leaves) 99.96% leaf accuracy in 16 min; GO
molecular-function 96.9% leaf / 100% root in 50 s; NCBI Mammalia Spearman $\rho
= -0.96$ with true taxonomic distance. The learned metric is perfectly
ultrametric (zero triangle violations), and its fractal and
information-theoretic properties are analyzed. Beyond classification we derive
structural invariants for quantum systems (HiPaQ) and controllable generative
codes for tabular data (Tab-HiPaN). v-PuNNs therefore bridge number theory and
deep learning, offering exact, interpretable, and efficient models for
hierarchical data.

</details>


### [195] [On Some Tunable Multi-fidelity Bayesian Optimization Frameworks](https://arxiv.org/abs/2508.01013)
*Arjun Manoj,Anastasia S. Georgiou,Dimitris G. Giovanis,Themistoklis P. Sapsis,Ioannis G. Kevrekidis*

Main category: cs.LG

TL;DR: This paper introduces a proximity-based acquisition strategy for Gaussian Process (GP)-based multi-fidelity optimization. The results highlight the capability of the proximity-based multi-fidelity acquisition function to deliver consistent control over high-fidelity usage while maintaining convergence efficiency.


<details>
  <summary>Details</summary>
Motivation: To advance Gaussian Process (GP)-based multi-fidelity optimization, which employs surrogate models that integrate information from varying levels of fidelity to guide efficient exploration of complex design spaces while minimizing the reliance on (expensive) high-fidelity objective function evaluations.

Method: We implement a proximity-based acquisition strategy that simplifies fidelity selection by eliminating the need for separate acquisition functions at each fidelity level. We also enable multi-fidelity Upper Confidence Bound (UCB) strategies by combining them with multi-fidelity GPs rather than the standard GPs typically used. We benchmark these approaches alongside other multi-fidelity acquisition strategies (including fidelity-weighted approaches).

Result: The results highlight the capability of the proximity-based multi-fidelity acquisition function to deliver consistent control over high-fidelity usage while maintaining convergence efficiency. Our illustrative examples include multi-fidelity chemical kinetic models, both homogeneous and heterogeneous (dynamic catalysis for ammonia production).

Conclusion: The proximity-based multi-fidelity acquisition function can deliver consistent control over high-fidelity usage while maintaining convergence efficiency.

Abstract: Multi-fidelity optimization employs surrogate models that integrate
information from varying levels of fidelity to guide efficient exploration of
complex design spaces while minimizing the reliance on (expensive)
high-fidelity objective function evaluations. To advance Gaussian Process
(GP)-based multi-fidelity optimization, we implement a proximity-based
acquisition strategy that simplifies fidelity selection by eliminating the need
for separate acquisition functions at each fidelity level. We also enable
multi-fidelity Upper Confidence Bound (UCB) strategies by combining them with
multi-fidelity GPs rather than the standard GPs typically used. We benchmark
these approaches alongside other multi-fidelity acquisition strategies
(including fidelity-weighted approaches) comparing their performance, reliance
on high-fidelity evaluations, and hyperparameter tunability in representative
optimization tasks. The results highlight the capability of the proximity-based
multi-fidelity acquisition function to deliver consistent control over
high-fidelity usage while maintaining convergence efficiency. Our illustrative
examples include multi-fidelity chemical kinetic models, both homogeneous and
heterogeneous (dynamic catalysis for ammonia production).

</details>


### [196] [Explaining GNN Explanations with Edge Gradients](https://arxiv.org/abs/2508.01048)
*Jesse He,Akbar Rafiey,Gal Mishne,Yusu Wang*

Main category: cs.LG

TL;DR: 本文对GNN解释方法进行了理论分析，揭示了不同方法之间的联系，并通过实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNN）在图结构数据上的显著成功促使了大量解释GNN预测的方法的涌现。然而，GNN可解释性的最新技术仍不稳定。不同的比较发现不同方法的结果好坏参半，许多解释器在更复杂的GNN架构和任务上步履蹒跚。这迫切需要对竞争性GNN解释方法进行更仔细的理论分析。

Method: 本文在两种不同的设置下，对GNN解释进行了更仔细的观察：输入层面的解释，产生输入图的解释性子图；以及层面的解释，产生计算图的解释性子图。建立了流行的基于扰动的方法和经典的基于梯度的方法之间的第一个理论联系，并指出了其他最近提出的方法之间的联系。

Result: 在输入层面，我们证明了在什么条件下GNNExplainer可以被基于边缘梯度符号的简单启发式方法近似。在层面设置中，我们指出边缘梯度等同于线性GNN的遮挡搜索。最后，我们通过在合成和真实数据集上的实验，展示了我们的理论结果如何在实践中体现。

Conclusion: 本文通过理论分析和实验验证，揭示了GNN解释方法在输入层面和层面的性质，并建立了不同方法之间的联系。

Abstract: In recent years, the remarkable success of graph neural networks (GNNs) on
graph-structured data has prompted a surge of methods for explaining GNN
predictions. However, the state-of-the-art for GNN explainability remains in
flux. Different comparisons find mixed results for different methods, with many
explainers struggling on more complex GNN architectures and tasks. This
presents an urgent need for a more careful theoretical analysis of competing
GNN explanation methods. In this work we take a closer look at GNN explanations
in two different settings: input-level explanations, which produce explanatory
subgraphs of the input graph, and layerwise explanations, which produce
explanatory subgraphs of the computation graph. We establish the first
theoretical connections between the popular perturbation-based and classical
gradient-based methods, as well as point out connections between other recently
proposed methods. At the input level, we demonstrate conditions under which
GNNExplainer can be approximated by a simple heuristic based on the sign of the
edge gradients. In the layerwise setting, we point out that edge gradients are
equivalent to occlusion search for linear GNNs. Finally, we demonstrate how our
theoretical results manifest in practice with experiments on both synthetic and
real datasets.

</details>


### [197] [Centralized Adaptive Sampling for Reliable Co-Training of Independent Multi-Agent Policies](https://arxiv.org/abs/2508.01049)
*Nicholas E. Corrado,Josiah P. Hanna*

Main category: cs.LG

TL;DR: This paper introduces MA-PROPS, an adaptive action sampling approach, to reduce joint sampling error and improve the reliability of policy gradient learning in MARL.


<details>
  <summary>Details</summary>
Motivation: Independent on-policy policy gradient algorithms are known to converge suboptimally when each agent's policy gradient points toward a suboptimal equilibrium. Stochasticity in independent action sampling can cause the joint data distribution to deviate from the expected joint on-policy distribution. This sampling error w.r.t. the joint on-policy distribution produces inaccurate gradient estimates that can lead agents to converge suboptimally.

Method: adaptive action sampling approach to reduce joint sampling error. Uses a centralized behavior policy that we continually adapt to place larger probability on joint actions that are currently under-sampled w.r.t. the current joint policy. Method name: Multi-Agent Proximal Robust On-Policy Sampling (MA-PROPS)

Result: MA-PROPS reduces joint sampling error more efficiently than standard on-policy sampling. MA-PROPS improves the reliability of independent policy gradient algorithms, increasing the fraction of training runs that converge to an optimal joint policy.

Conclusion: MA-PROPS reduces joint sampling error more efficiently than standard on-policy sampling and improves the reliability of independent policy gradient algorithms, increasing the fraction of training runs that converge to an optimal joint policy.

Abstract: Independent on-policy policy gradient algorithms are widely used for
multi-agent reinforcement learning (MARL) in cooperative and no-conflict games,
but they are known to converge suboptimally when each agent's policy gradient
points toward a suboptimal equilibrium. In this work, we identify a subtler
failure mode that arises \textit{even when the expected policy gradients of all
agents point toward an optimal solution.} After collecting a finite set of
trajectories, stochasticity in independent action sampling can cause the joint
data distribution to deviate from the expected joint on-policy distribution.
This \textit{sampling error} w.r.t. the joint on-policy distribution produces
inaccurate gradient estimates that can lead agents to converge suboptimally. In
this paper, we investigate if joint sampling error can be reduced through
coordinated action selection and whether doing so improves the reliability of
policy gradient learning in MARL. Toward this end, we introduce an adaptive
action sampling approach to reduce joint sampling error. Our method,
Multi-Agent Proximal Robust On-Policy Sampling (MA-PROPS), uses a centralized
behavior policy that we continually adapt to place larger probability on joint
actions that are currently under-sampled w.r.t. the current joint policy. We
empirically evaluate MA-PROPS in a diverse range of multi-agent games and
demonstrate that (1) MA-PROPS reduces joint sampling error more efficiently
than standard on-policy sampling and (2) improves the reliability of
independent policy gradient algorithms, increasing the fraction of training
runs that converge to an optimal joint policy.

</details>


### [198] [FGBench: A Dataset and Benchmark for Molecular Property Reasoning at Functional Group-Level in Large Language Models](https://arxiv.org/abs/2508.01055)
*Xuan Liu,Siru Ouyang,Xianrui Zhong,Jiawei Han,Huimin Zhao*

Main category: cs.LG

TL;DR: 提出了FGBench数据集，用于提高LLM在化学任务中对功能组级别属性的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要集中在分子水平的性质预测上，忽略了细粒度的功能组（FG）信息的作用。纳入FG水平的数据可以提供有价值的先验知识，将分子结构与文本描述联系起来，从而构建更易于理解的、结构感知的LLM，用于分子相关任务的推理。

Method: 构建了一个包含功能组信息的分子性质推理问题的数据集FGBench

Result: 构建了一个包含625K分子性质推理问题的数据集，包含245个不同的功能组，涵盖分子性质推理的三个类别。在7K精选数据上对最先进的LLM的基准测试结果表明，当前的LLM在FG水平的属性推理方面存在困难。

Conclusion: 当前的LLM在FG水平的属性推理方面存在困难，需要增强LLM在化学任务中的推理能力。FGBench中采用的构建具有功能组级别信息的数据集的方法，将作为生成新的问答对的基础框架，使LLM能够更好地理解细粒度的分子结构-性质关系。

Abstract: Large language models (LLMs) have gained significant attention in chemistry.
However, most existing datasets center on molecular-level property prediction
and overlook the role of fine-grained functional group (FG) information.
Incorporating FG-level data can provide valuable prior knowledge that links
molecular structures with textual descriptions, which can be used to build more
interpretable, structure-aware LLMs for reasoning on molecule-related tasks.
Moreover, LLMs can learn from such fine-grained information to uncover hidden
relationships between specific functional groups and molecular properties,
thereby advancing molecular design and drug discovery. Here, we introduce
FGBench, a dataset comprising 625K molecular property reasoning problems with
functional group information. Functional groups are precisely annotated and
localized within the molecule, which ensures the dataset's interoperability
thereby facilitating further multimodal applications. FGBench includes both
regression and classification tasks on 245 different functional groups across
three categories for molecular property reasoning: (1) single functional group
impacts, (2) multiple functional group interactions, and (3) direct molecular
comparisons. In the benchmark of state-of-the-art LLMs on 7K curated data, the
results indicate that current LLMs struggle with FG-level property reasoning,
highlighting the need to enhance reasoning capabilities in LLMs for chemistry
tasks. We anticipate that the methodology employed in FGBench to construct
datasets with functional group-level information will serve as a foundational
framework for generating new question-answer pairs, enabling LLMs to better
understand fine-grained molecular structure-property relationships. The dataset
and evaluation code are available at
\href{https://github.com/xuanliugit/FGBench}{https://github.com/xuanliugit/FGBench}.

</details>


### [199] [The Lattice Geometry of Neural Network Quantization -- A Short Equivalence Proof of GPTQ and Babai's algorithm](https://arxiv.org/abs/2508.01077)
*Johann Birnick*

Main category: cs.LG

TL;DR: data-driven quantization <-> closest vector problem; GPTQ <-> Babai; lattice basis reduction may be better


<details>
  <summary>Details</summary>
Motivation: data-driven quantization of a linear unit in a neural network corresponds to solving the closest vector problem for a certain lattice generated by input data

Method: GPTQ algorithm is equivalent to Babai's nearest-plane algorithm

Result: GPTQ algorithm is equivalent to Babai's nearest-plane algorithm; geometric intuition for both algorithms is provided

Conclusion: lattice basis reduction may improve quantization

Abstract: We explain how data-driven quantization of a linear unit in a neural network
corresponds to solving the closest vector problem for a certain lattice
generated by input data. We prove that the GPTQ algorithm is equivalent to
Babai's well-known nearest-plane algorithm. We furthermore provide geometric
intuition for both algorithms. Lastly, we note the consequences of these
results, in particular hinting at the possibility for using lattice basis
reduction for better quantization.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [200] [ReCoSeg++:Extended Residual-Guided Cross-Modal Diffusion for Brain Tumor Segmentation](https://arxiv.org/abs/2508.01058)
*Sara Yavari,Rahul Nitin Pandya,Jacob Furst*

Main category: eess.IV

TL;DR: 提出了一种半监督两阶段框架，用于提高脑肿瘤分割的准确性和可扩展性，并在BraTS 2021数据集上取得了优异的结果。


<details>
  <summary>Details</summary>
Motivation: 在MRI扫描中对脑肿瘤进行精确分割对于临床诊断和治疗计划至关重要。

Method: 提出了一种半监督、两阶段框架，该框架扩展了ReCoSeg方法，利用残差引导的去噪扩散概率模型(DDPM)进行跨模态合成，并使用轻量级U-Net进行肿瘤分割。

Result: 该方法在BraTS 2021数据集上实现了更高的Dice score和IoU，优于ReCoSeg基线。

Conclusion: 该方法在BraTS 2021数据集上实现了93.02%的Dice score和86.7%的IoU，优于ReCoSeg在BraTS 2020上的表现，证明了其在真实世界多中心MRI数据集上的准确性和可扩展性。

Abstract: Accurate segmentation of brain tumors in MRI scans is critical for clinical
diagnosis and treatment planning. We propose a semi-supervised, two-stage
framework that extends the ReCoSeg approach to the larger and more
heterogeneous BraTS 2021 dataset, while eliminating the need for ground-truth
masks for the segmentation objective. In the first stage, a residual-guided
denoising diffusion probabilistic model (DDPM) performs cross-modal synthesis
by reconstructing the T1ce modality from FLAIR, T1, and T2 scans. The residual
maps, capturing differences between predicted and actual T1ce images, serve as
spatial priors to enhance downstream segmentation. In the second stage, a
lightweight U-Net takes as input the concatenation of residual maps, computed
as the difference between real T1ce and synthesized T1ce, with T1, T2, and
FLAIR modalities to improve whole tumor segmentation. To address the increased
scale and variability of BraTS 2021, we apply slice-level filtering to exclude
non-informative samples and optimize thresholding strategies to balance
precision and recall. Our method achieves a Dice score of $93.02\%$ and an IoU
of $86.7\%$ for whole tumor segmentation on the BraTS 2021 dataset,
outperforming the ReCoSeg baseline on BraTS 2020 (Dice: $91.7\%$, IoU:
$85.3\%$), and demonstrating improved accuracy and scalability for real-world,
multi-center MRI datasets.

</details>


### [201] [Mobile U-ViT: Revisiting large kernel and U-shaped ViT for efficient medical image segmentation](https://arxiv.org/abs/2508.01064)
*Fenghe Tang,Bingkun Nian,Jianrui Ding,Wenxin Ma,Quan Quan,Chengqi Dong,Jie Yang,Wei Liu,S. Kevin Zhou*

Main category: eess.IV

TL;DR: 提出了一种名为 Mobile U-ViT 的移动模型，专为医学图像分割而定制，并在各种医学图像数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在临床实践中，医学图像分析通常需要在资源受限的移动设备上高效执行。然而，现有的主要针对自然图像优化的移动模型，由于自然领域和医学领域之间存在显着的信息密度差距，因此在医学任务上的表现往往不佳。在开发轻量级、通用和高性能网络时，将计算效率与医学成像特有的架构优势相结合仍然是一个挑战。

Method: 我们提出了一种名为 Mobile U-shaped Vision Transformer (Mobile U-ViT) 的移动模型，该模型专为医学图像分割而定制。具体来说，我们采用新提出的 ConvUtr 作为分层补丁嵌入，具有参数高效的大内核 CNN 和反向瓶颈融合。为了实现高效的局部-全局信息交换，我们引入了一种新颖的大内核局部-全局-局部 (LGL) 块，该块有效地平衡了医学图像的低信息密度和高级语义差异。最后，我们结合了一个浅而轻量级的 Transformer 瓶颈用于远程建模，并采用带有下采样跳跃连接的级联解码器用于密集预测。

Result: 医学优化的架构在八个公共 2D 和 3D 数据集上实现了最先进的性能，涵盖了不同的成像方式，包括对四个未见数据集的零样本测试。

Conclusion: 该医学优化的架构在八个公共 2D 和 3D 数据集上实现了最先进的性能，涵盖了不同的成像方式，包括对四个未见数据集的零样本测试。这些结果将其确立为用于移动医学图像分析的高效、强大和通用解决方案。

Abstract: In clinical practice, medical image analysis often requires efficient
execution on resource-constrained mobile devices. However, existing mobile
models-primarily optimized for natural images-tend to perform poorly on medical
tasks due to the significant information density gap between natural and
medical domains. Combining computational efficiency with medical
imaging-specific architectural advantages remains a challenge when developing
lightweight, universal, and high-performing networks. To address this, we
propose a mobile model called Mobile U-shaped Vision Transformer (Mobile U-ViT)
tailored for medical image segmentation. Specifically, we employ the newly
purposed ConvUtr as a hierarchical patch embedding, featuring a
parameter-efficient large-kernel CNN with inverted bottleneck fusion. This
design exhibits transformer-like representation learning capacity while being
lighter and faster. To enable efficient local-global information exchange, we
introduce a novel Large-kernel Local-Global-Local (LGL) block that effectively
balances the low information density and high-level semantic discrepancy of
medical images. Finally, we incorporate a shallow and lightweight transformer
bottleneck for long-range modeling and employ a cascaded decoder with
downsample skip connections for dense prediction. Despite its reduced
computational demands, our medical-optimized architecture achieves
state-of-the-art performance across eight public 2D and 3D datasets covering
diverse imaging modalities, including zero-shot testing on four unseen
datasets. These results establish it as an efficient yet powerful and
generalization solution for mobile medical image analysis. Code is available at
https://github.com/FengheTan9/Mobile-U-ViT.

</details>
