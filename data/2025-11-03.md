<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 3]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Understanding and Enhancing Mamba-Transformer Hybrids for Memory Recall and Language Modeling](https://arxiv.org/abs/2510.26912)
*Hyunji Lee,Wenhao Yu,Hongming Zhang,Kaixin Ma,Jiyeon Kim,Dong Yu,Minjoon Seo*

Main category: cs.CL

TL;DR: 分析了混合SSM-Attention模型的架构设计选择，并提出了一种互补方法来提高其有效性。


<details>
  <summary>Details</summary>
Motivation: 混合模型结合了SSM的效率和Attention的高召回能力，但其背后的架构设计选择尚不清楚。

Method: 通过内存利用率和整体性能的角度分析混合架构，并提出一种持续训练数据集增强的方法。

Result: 研究发现，顺序混合模型在较短上下文中表现更好，而并行混合模型在较长上下文中更有效。数据增强方法进一步提高了召回率，且具有良好的泛化能力。

Conclusion: 该研究更深入地理解了混合SSM-Attention模型，并为设计适用于各种用例的架构提供了实用的指导。

Abstract: Hybrid models that combine state space models (SSMs) with attention
mechanisms have shown strong performance by leveraging the efficiency of SSMs
and the high recall ability of attention. However, the architectural design
choices behind these hybrid models remain insufficiently understood. In this
work, we analyze hybrid architectures through the lens of memory utilization
and overall performance, and propose a complementary method to further enhance
their effectiveness. We first examine the distinction between sequential and
parallel integration of SSM and attention layers. Our analysis reveals several
interesting findings, including that sequential hybrids perform better on
shorter contexts, whereas parallel hybrids are more effective for longer
contexts. We also introduce a data-centric approach of continually training on
datasets augmented with paraphrases, which further enhances recall while
preserving other capabilities. It generalizes well across different base models
and outperforms architectural modifications aimed at enhancing recall. Our
findings provide a deeper understanding of hybrid SSM-attention models and
offer practical guidance for designing architectures tailored to various use
cases. Our findings provide a deeper understanding of hybrid SSM-attention
models and offer practical guidance for designing architectures tailored to
various use cases.

</details>


### [2] [Frame Semantic Patterns for Identifying Underreporting of Notifiable Events in Healthcare: The Case of Gender-Based Violence](https://arxiv.org/abs/2510.26969)
*Lívia Dutra,Arthur Lorenzi,Laís Berno,Franciany Campos,Karoline Biscardi,Kenneth Brown,Marcelo Viridiano,Frederico Belcavello,Ely Matos,Olívia Guaranha,Erik Santos,Sofia Reinach,Tiago Timponi Torrent*

Main category: cs.CL

TL;DR: 提出了一种在医疗领域识别应报告事件的方法，利用语义框架在非结构化数据中搜索细粒度模式。


<details>
  <summary>Details</summary>
Motivation: 解决电子病历中性别暴力（GBV）漏报问题。

Method: 定义了八种模式，并在从e-SUS APS提取的2100万句葡萄牙语语料库中搜索这些模式。结果由语言学家手动评估，并测量每种模式的精度。

Result: 该方法能够有效识别暴力报告，精度为0.726，证实了其稳健性。

Conclusion: 该方法具有透明、高效、低碳和语言无关的特点，可轻松适应其他健康监测环境，有助于在公共卫生系统中更广泛、合乎道德和可解释地使用自然语言处理。

Abstract: We introduce a methodology for the identification of notifiable events in the
domain of healthcare. The methodology harnesses semantic frames to define
fine-grained patterns and search them in unstructured data, namely, open-text
fields in e-medical records. We apply the methodology to the problem of
underreporting of gender-based violence (GBV) in e-medical records produced
during patients' visits to primary care units. A total of eight patterns are
defined and searched on a corpus of 21 million sentences in Brazilian
Portuguese extracted from e-SUS APS. The results are manually evaluated by
linguists and the precision of each pattern measured. Our findings reveal that
the methodology effectively identifies reports of violence with a precision of
0.726, confirming its robustness. Designed as a transparent, efficient,
low-carbon, and language-agnostic pipeline, the approach can be easily adapted
to other health surveillance contexts, contributing to the broader, ethical,
and explainable use of NLP in public health systems.

</details>


### [3] [Overview of the MEDIQA-OE 2025 Shared Task on Medical Order Extraction from Doctor-Patient Consultations](https://arxiv.org/abs/2510.26974)
*Jean-Philippe Corbeil,Asma Ben Abacha,Jerome Tremblay,Phillip Swazinna,Akila Jeeson Daniel,Miguel Del-Agua,Francois Beaulieu*

Main category: cs.CL

TL;DR: 本研究介绍了一个新的共享任务，旨在从医患对话中提取医嘱，以减轻临床医生的文档负担并改善患者护理。


<details>
  <summary>Details</summary>
Motivation: 临床文档越来越多地使用自动语音识别和总结，但将对话转换为电子健康记录的可操作医疗医嘱仍未被探索。

Method: 介绍了MEDIQA-OE 2025 共享任务，这是一个关于从医患对话中提取医疗医嘱的首次挑战。六个团队参与了共享任务，并尝试了各种方法，包括封闭权重和开放权重的大型语言模型 (LLM)。

Result: 介绍了MEDIQA-OE任务、数据集、最终排行榜排名和参与者的解决方案。

Conclusion: 本文概述了MEDIQA-OE 2025 共享任务的各个方面，为未来研究医嘱提取和临床文档自动化奠定了基础。

Abstract: Clinical documentation increasingly uses automatic speech recognition and
summarization, yet converting conversations into actionable medical orders for
Electronic Health Records remains unexplored. A solution to this problem can
significantly reduce the documentation burden of clinicians and directly impact
downstream patient care. We introduce the MEDIQA-OE 2025 shared task, the first
challenge on extracting medical orders from doctor-patient conversations. Six
teams participated in the shared task and experimented with a broad range of
approaches, and both closed- and open-weight large language models (LLMs). In
this paper, we describe the MEDIQA-OE task, dataset, final leaderboard ranking,
and participants' solutions.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench](https://arxiv.org/abs/2510.26865)
*Fenfen Lin,Yesheng Liu,Haiyu Xu,Chen Yue,Zheqi He,Mingxuan Zhao,Miguel Hu Chen,Jiakang Liu,JG Yao,Xi Yang*

Main category: cs.CV

TL;DR: 当前视觉语言模型 (VLM) 在测量仪器读数方面表现不佳。论文介绍了 MeasureBench，一个用于视觉测量读数的基准，涵盖真实和合成图像，以及用于数据合成的可扩展流水线。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在测量仪器读数方面存在挑战。

Method: 论文提出了 MeasureBench 基准，包含真实世界和合成的测量图像，以及一个可扩展的数据合成流水线，该流水线可以程序化生成特定类型的仪表，并控制视觉外观。

Result: 对现有 VLM 的评估表明，即使是最强大的模型在测量读数方面也存在困难。一个常见的失败模式是指示器定位错误。在合成数据上使用强化学习的初步实验在域内合成子集上取得了令人鼓舞的结果，但在真实图像上效果不佳。

Conclusion: 目前的 VLM 在细粒度的空间定位方面存在根本限制。MeasureBench 有助于未来在视觉基础上的数字能力和 VLM 精确空间感知方面的进步。

Abstract: Reading measurement instruments is effortless for humans and requires
relatively little domain expertise, yet it remains surprisingly challenging for
current vision-language models (VLMs) as we find in preliminary evaluation. In
this work, we introduce MeasureBench, a benchmark on visual measurement reading
covering both real-world and synthesized images of various types of
measurements, along with an extensible pipeline for data synthesis. Our
pipeline procedurally generates a specified type of gauge with controllable
visual appearance, enabling scalable variation in key details such as pointers,
scales, fonts, lighting, and clutter. Evaluation on popular proprietary and
open-weight VLMs shows that even the strongest frontier VLMs struggle
measurement reading in general. A consistent failure mode is indicator
localization: models can read digits or labels but misidentify the key
positions of pointers or alignments, leading to big numeric errors despite
plausible textual reasoning. We have also conducted preliminary experiments
with reinforcement learning over synthetic data, and find encouraging results
on in-domain synthetic subset but less promising for real-world images. Our
analysis highlights a fundamental limitation of current VLMs in fine-grained
spatial grounding. We hope this resource can help future advances on visually
grounded numeracy and precise spatial perception of VLMs, bridging the gap
between recognizing numbers and measuring the world.

</details>


### [5] [PF-DAformer: Proximal Femur Segmentation via Domain Adaptive Transformer for Dual-Center QCT](https://arxiv.org/abs/2510.26903)
*Rochak Dhakal,Chen Zhao,Zixin Shi,Joyce H. Keyak,Tadashi S. Kaneko,Kuan-Jui Su,Hui Shen,Hong-Wen Deng,Weihua Zhou*

Main category: cs.CV

TL;DR: 本研究旨在解决多中心定量计算机断层扫描（QCT）中由于领域转移导致骨骼分割模型泛化能力差的问题，通过开发一种领域自适应的Transformer分割框架，提高模型在不同机构QCT图像上的分割准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 在多中心QCT研究中，由于扫描仪、重建设置和患者人口统计学等因素的差异，导致领域转移，使得在一个数据集上训练的深度网络在应用于其他数据集时效果不佳，这阻碍了骨质疏松症的多中心研究以及影像组学和结构有限元分析结果的跨站点重现性。

Method: 该研究提出了一种领域自适应的Transformer分割框架，该框架在3D TransUNet骨干网络中集成了两种互补策略：通过梯度反转层（GRL）进行对抗对齐，以抑制网络编码特定于站点的线索；以及通过最大均值差异（MMD）进行统计对齐，以显式减少机构之间的分布不匹配。这种双重机制平衡了不变性和细粒度对齐。

Result: 该模型在包含来自杜兰大学的1,024张QCT图像扫描和来自明尼苏达州罗切斯特的384张扫描的大型髋部骨折相关研究队列上进行了训练和验证。

Conclusion: 该研究开发了一种领域自适应的Transformer分割框架，可以有效解决多中心QCT中由于领域转移导致的骨骼分割模型泛化能力差的问题，提高模型在不同机构QCT图像上的分割准确性和可靠性。

Abstract: Quantitative computed tomography (QCT) plays a crucial role in assessing bone
strength and fracture risk by enabling volumetric analysis of bone density
distribution in the proximal femur. However, deploying automated segmentation
models in practice remains difficult because deep networks trained on one
dataset often fail when applied to another. This failure stems from domain
shift, where scanners, reconstruction settings, and patient demographics vary
across institutions, leading to unstable predictions and unreliable
quantitative metrics. Overcoming this barrier is essential for multi-center
osteoporosis research and for ensuring that radiomics and structural finite
element analysis results remain reproducible across sites. In this work, we
developed a domain-adaptive transformer segmentation framework tailored for
multi-institutional QCT. Our model is trained and validated on one of the
largest hip fracture related research cohorts to date, comprising 1,024 QCT
images scans from Tulane University and 384 scans from Rochester, Minnesota for
proximal femur segmentation. To address domain shift, we integrate two
complementary strategies within a 3D TransUNet backbone: adversarial alignment
via Gradient Reversal Layer (GRL), which discourages the network from encoding
site-specific cues, and statistical alignment via Maximum Mean Discrepancy
(MMD), which explicitly reduces distributional mismatches between institutions.
This dual mechanism balances invariance and fine-grained alignment, enabling
scanner-agnostic feature learning while preserving anatomical detail.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [CATArena: Evaluation of LLM Agents through Iterative Tournament Competitions](https://arxiv.org/abs/2510.26852)
*Lingyue Fu,Xin Ding,Yaoming Zhu,Shao Zhang,Lin Qiu,Weiwen Liu,Weinan Zhang,Xuezhi Cao,Xunliang Cai,Jiaxin Ding,Yong Yu*

Main category: cs.AI

TL;DR: 这篇论文介绍了一个用于评估大型语言模型（LLM）代理学习能力的迭代式、竞争性同伴学习框架CATArena。CATArena通过提供具有开放式评分的棋盘和纸牌游戏，解决了当前基准测试中分数饱和的问题，并能够持续和动态地评估快速发展的代理能力。


<details>
  <summary>Details</summary>
Motivation: 当前基准主要评估固定场景中的端到端性能，限制了对特定技能的评估，并且随着代理能力的提高，面临分数饱和和对专家注释依赖性增加的问题。因此，论文强调了学习能力（包括自我提升和同伴学习）作为代理向人类水平智能演进的核心驱动力。

Method: 论文提出了一个迭代的、竞争性的同伴学习框架，允许代理通过重复的交互和反馈来改进和优化他们的策略，从而系统地评估他们的学习能力。同时，引入了一个锦标赛式的评估平台CATArena，其中包含四个具有开放式评分的多样化棋盘和纸牌游戏。

Result: 实验结果和分析表明，CATArena为核心代理能力（特别是学习能力和策略编码）提供了可靠、稳定和可扩展的基准测试。

Conclusion: CATArena提供了一个更有效评估LLM代理学习能力的方法，解决了传统基准测试的局限性。

Abstract: Large Language Model (LLM) agents have evolved from basic text generation to
autonomously completing complex tasks through interaction with external tools.
However, current benchmarks mainly assess end-to-end performance in fixed
scenarios, restricting evaluation to specific skills and suffering from score
saturation and growing dependence on expert annotation as agent capabilities
improve. In this work, we emphasize the importance of learning ability,
including both self-improvement and peer-learning, as a core driver for agent
evolution toward human-level intelligence. We propose an iterative, competitive
peer-learning framework, which allows agents to refine and optimize their
strategies through repeated interactions and feedback, thereby systematically
evaluating their learning capabilities. To address the score saturation issue
in current benchmarks, we introduce CATArena, a tournament-style evaluation
platform featuring four diverse board and card games with open-ended scoring.
By providing tasks without explicit upper score limits, CATArena enables
continuous and dynamic evaluation of rapidly advancing agent capabilities.
Experimental results and analyses involving both minimal and commercial code
agents demonstrate that CATArena provides reliable, stable, and scalable
benchmarking for core agent abilities, particularly learning ability and
strategy coding.

</details>


### [7] [Inverse Knowledge Search over Verifiable Reasoning: Synthesizing a Scientific Encyclopedia from a Long Chains-of-Thought Knowledge Base](https://arxiv.org/abs/2510.26854)
*Yu Li,Yuan Huang,Tao Wang,Caiyu Fan,Xiansheng Cai,Sihan Hu,Xinzijian Liu,Cheng Shi,Mingjun Xu,Zhen Wang,Yan Wang,Xiangqi Jin,Tianhan Zhang,Linfeng Zhang,Lei Wang,Youjin Deng,Pan Zhang,Weijie Sun,Xingyu Li,Weinan E,Linfeng Zhang,Zhiyuan Yao,Kun Chen*

Main category: cs.AI

TL;DR: This paper introduces a framework to decompress scientific reasoning by constructing a verifiable Long Chain-of-Thought (LCoT) knowledge base and projecting it into an encyclopedia, SciencePedia.


<details>
  <summary>Details</summary>
Motivation: Current scientific materials compress reasoning, hindering verification and cross-domain links.

Method: A Socratic agent generates questions, multiple solver models generate LCoTs, and these are filtered by prompt sanitization and cross-model answer consensus. A search engine retrieves derivations, and a synthesizer narrates these into articles.

Result: SciencePedia comprises 200,000 entries and exhibits higher knowledge-point density and lower factual error rates compared to a baseline.

Conclusion: The reasoning-centric approach enables trustworthy, cross-domain scientific synthesis at scale and establishes the foundation for an expanding encyclopedia.

Abstract: Most scientific materials compress reasoning, presenting conclusions while
omitting the derivational chains that justify them. This compression hinders
verification by lacking explicit, step-wise justifications and inhibits
cross-domain links by collapsing the very pathways that establish the logical
and causal connections between concepts. We introduce a scalable framework that
decompresses scientific reasoning, constructing a verifiable Long
Chain-of-Thought (LCoT) knowledge base and projecting it into an emergent
encyclopedia, SciencePedia. Our pipeline operationalizes an endpoint-driven,
reductionist strategy: a Socratic agent, guided by a curriculum of around 200
courses, generates approximately 3 million first-principles questions. To
ensure high fidelity, multiple independent solver models generate LCoTs, which
are then rigorously filtered by prompt sanitization and cross-model answer
consensus, retaining only those with verifiable endpoints. This verified corpus
powers the Brainstorm Search Engine, which performs inverse knowledge search --
retrieving diverse, first-principles derivations that culminate in a target
concept. This engine, in turn, feeds the Plato synthesizer, which narrates
these verified chains into coherent articles. The initial SciencePedia
comprises approximately 200,000 fine-grained entries spanning mathematics,
physics, chemistry, biology, engineering, and computation. In evaluations
across six disciplines, Plato-synthesized articles (conditioned on retrieved
LCoTs) exhibit substantially higher knowledge-point density and significantly
lower factual error rates than an equally-prompted baseline without retrieval
(as judged by an external LLM). Built on this verifiable LCoT knowledge base,
this reasoning-centric approach enables trustworthy, cross-domain scientific
synthesis at scale and establishes the foundation for an ever-expanding
encyclopedia.

</details>


### [8] [The Denario project: Deep knowledge AI agents for scientific discovery](https://arxiv.org/abs/2510.26887)
*Francisco Villaescusa-Navarro,Boris Bolliet,Pablo Villanueva-Domingo,Adrian E. Bayer,Aidan Acquah,Chetana Amancharla,Almog Barzilay-Siegal,Pablo Bermejo,Camille Bilodeau,Pablo Cárdenas Ramírez,Miles Cranmer,Urbano L. França,ChangHoon Hahn,Yan-Fei Jiang,Raul Jimenez,Jun-Young Lee,Antonio Lerario,Osman Mamun,Thomas Meier,Anupam A. Ojha,Pavlos Protopapas,Shimanto Roy,David N. Spergel,Pedro Tarancón-Álvarez,Ujjwal Tiwari,Matteo Viel,Digvijay Wadekar,Chi Wang,Bonny Y. Wang,Licong Xu,Yossi Yovel,Shuwen Yue,Wen-Han Zhou,Qiyao Zhu,Jiajun Zou,Íñigo Zubeldia*

Main category: cs.AI

TL;DR: Denario is an AI multi-agent system that acts as a scientific research assistant.


<details>
  <summary>Details</summary>
Motivation: To create an AI system that can perform various research tasks and generate scientific papers across multiple disciplines.

Method: A modular architecture is used, with Cmbagent as a deep-research backend. The system generates papers in various scientific disciplines and combines ideas from different fields.

Result: Denario generated papers in multiple scientific disciplines, which were evaluated by domain experts. The system's strengths, weaknesses, and limitations were highlighted.

Conclusion: The paper discusses the ethical implications of AI-driven research and reflects on the technology's relation to the philosophy of science. The code is publicly released.

Abstract: We present Denario, an AI multi-agent system designed to serve as a
scientific research assistant. Denario can perform many different tasks, such
as generating ideas, checking the literature, developing research plans,
writing and executing code, making plots, and drafting and reviewing a
scientific paper. The system has a modular architecture, allowing it to handle
specific tasks, such as generating an idea, or carrying out end-to-end
scientific analysis using Cmbagent as a deep-research backend. In this work, we
describe in detail Denario and its modules, and illustrate its capabilities by
presenting multiple AI-generated papers generated by it in many different
scientific disciplines such as astrophysics, biology, biophysics, biomedical
informatics, chemistry, material science, mathematical physics, medicine,
neuroscience and planetary science. Denario also excels at combining ideas from
different disciplines, and we illustrate this by showing a paper that applies
methods from quantum physics and machine learning to astrophysical data. We
report the evaluations performed on these papers by domain experts, who
provided both numerical scores and review-like feedback. We then highlight the
strengths, weaknesses, and limitations of the current system. Finally, we
discuss the ethical implications of AI-driven research and reflect on how such
technology relates to the philosophy of science. We publicly release the code
at https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run
directly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and
the full app will be deployed on the cloud.

</details>


### [9] [Cognition Envelopes for Bounded AI Reasoning in Autonomous UAS Operations](https://arxiv.org/abs/2510.26905)
*Pedro Antonio Alarcón Granadeno,Arturo Miguel Bernal Russell,Sofia Nelson,Demetrius Hernandez,Maureen Petterson,Michael Murphy,Walter J. Scheirer,Jane Cleland-Huang*

Main category: cs.AI

TL;DR: 论文介绍了认知包络的概念，旨在约束大型语言模型和视觉语言模型在网络物理系统中的决策，以减少幻觉、过度概括和上下文错位等错误。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和视觉语言模型在网络物理系统中的应用会引入新的错误类型，导致错误的决策。

Method: 提出认知包络的概念，用于建立推理边界，约束AI生成的决策。

Result: 认知包络需要像安全包络一样，制定实用的指南和系统的流程来进行定义、验证和保证。

Conclusion: 认知包络可以作为元认知和传统安全包络的补充，提高网络物理系统的自主性和安全性。

Abstract: Cyber-physical systems increasingly rely on Foundational Models such as Large
Language Models (LLMs) and Vision-Language Models (VLMs) to increase autonomy
through enhanced perception, inference, and planning. However, these models
also introduce new types of errors, such as hallucinations,
overgeneralizations, and context misalignments, resulting in incorrect and
flawed decisions. To address this, we introduce the concept of Cognition
Envelopes, designed to establish reasoning boundaries that constrain
AI-generated decisions while complementing the use of meta-cognition and
traditional safety envelopes. As with safety envelopes, Cognition Envelopes
require practical guidelines and systematic processes for their definition,
validation, and assurance.

</details>


### [10] [SUSTAINABLE Platform: Seamless Smart Farming Integration Towards Agronomy Automation](https://arxiv.org/abs/2510.26989)
*Agorakis Bompotas,Konstantinos Koutras,Nikitas Rigas Kalogeropoulos,Panagiotis Kechagias,Dimitra Gariza,Athanasios P. Kalogeras,Christos Alexakos*

Main category: cs.AI

TL;DR: 介绍了SUSTAINABLE，一个旨在实现高效、可追溯和可持续农业的智能农业平台。


<details>
  <summary>Details</summary>
Motivation: 全球农业部门正在经历转型，受到不断增长的粮食需求、气候变化和对可持续发展的需求驱动。

Method: 探索了当前的智能农业解决方案，提出了一个比较评估，并介绍了SUSTAINABLE的关键特性，包括卫星指数集成、实时环境数据和为地中海葡萄园量身定制的基于角色的任务管理。

Result: 展示SUSTAINABLE的关键特性，包括卫星指数集成、实时环境数据和基于角色的任务管理。

Conclusion: SUSTAINABLE是一个智能农业平台，旨在实现高效、可追溯和可持续的农业，并在葡萄栽培中进行了试点用例。

Abstract: The global agricultural sector is undergoing a transformative shift, driven
by increasing food demands, climate variability and the need for sustainable
practices. SUSTAINABLE is a smart farming platform designed to integrate IoT,
AI, satellite imaging, and role-based task orchestration to enable efficient,
traceable, and sustainable agriculture with a pilot usecase in viticulture.
This paper explores current smart agriculture solutions, presents a comparative
evaluation, and introduces SUSTAINABLE's key features, including satellite
index integration, real-time environmental data, and role-aware task management
tailored to Mediterranean vineyards.

</details>


### [11] [Causal Masking on Spatial Data: An Information-Theoretic Case for Learning Spatial Datasets with Unimodal Language Models](https://arxiv.org/abs/2510.27009)
*Jared Junkin,Samuel Nathanson*

Main category: cs.AI

TL;DR: 探讨了在具有空间或关系结构的领域中，因果掩蔽（causal masking）是否适用于训练语言模型的问题，通过在国际象棋领域进行实验，比较了在空间（棋盘）和序列（走法）数据上使用双向和因果自注意力机制训练的模型性能。


<details>
  <summary>Details</summary>
Motivation: 在具有空间或关系结构的领域中，传统上认为因果掩蔽不适用，通常使用序列线性化方法。但接受因果掩蔽带来的信息损失是否可行，缺乏直接研究。本研究旨在填补这一空白。

Method: 在国际象棋领域，使用空间（棋盘）和序列（走法）数据，训练具有双向和因果自注意力机制的语言模型。

Result: 在空间棋盘状态上训练的模型（即使使用因果掩蔽）始终比在序列数据上训练的模型具有更强的对弈能力。

Conclusion: 在空间数据上应用因果掩蔽是训练单模态LLM的可行方法，在某些领域甚至优于序列化。

Abstract: Language models are traditionally designed around causal masking. In domains
with spatial or relational structure, causal masking is often viewed as
inappropriate, and sequential linearizations are instead used. Yet the question
of whether it is viable to accept the information loss introduced by causal
masking on nonsequential data has received little direct study, in part because
few domains offer both spatial and sequential representations of the same
dataset. In this work, we investigate this issue in the domain of chess, which
naturally supports both representations. We train language models with
bidirectional and causal self-attention mechanisms on both spatial
(board-based) and sequential (move-based) data. Our results show that models
trained on spatial board states - \textit{even with causal masking} -
consistently achieve stronger playing strength than models trained on
sequential data. While our experiments are conducted on chess, our results are
methodological and may have broader implications: applying causal masking to
spatial data is a viable procedure for training unimodal LLMs on spatial data,
and in some domains is even preferable to sequentialization.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [12] [Category-Aware Semantic Caching for Heterogeneous LLM Workloads](https://arxiv.org/abs/2510.26835)
*Chen Wang,Xunzhuo Liu,Yue Zhu,Alaa Youssef,Priya Nagpurkar,Huamin Chen*

Main category: cs.DB

TL;DR: 本文提出了一种category-aware的语义缓存方法，通过区分不同查询类别来优化缓存性能，从而提升LLM serving系统的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的向量数据库缓存策略无法有效处理不同类别查询的差异性，导致缓存命中率低，大量流量无法被缓存。

Method: 该方法根据查询类别调整相似度阈值、TTL和配额，并采用混合架构分离内存HNSW搜索和外部文档存储，降低未命中成本。

Result: 实验结果表明，该方法能够显著提高缓存覆盖率，并能根据下游模型负载动态调整阈值和TTL，从而减少模型过载情况。

Conclusion: 提出的category-aware语义缓存方法能够有效提升LLM serving系统的性能和效率，并具有实际应用价值。

Abstract: LLM serving systems process heterogeneous query workloads where different
categories exhibit different characteristics. Code queries cluster densely in
embedding space while conversational queries distribute sparsely. Content
staleness varies from minutes (stock data) to months (code patterns). Query
repetition patterns range from power-law (code) to uniform (conversation),
producing long tail cache hit rate distributions: high-repetition categories
achieve 40-60% hit rates while low-repetition or volatile categories achieve
5-15% hit rates. Vector databases must exclude the long tail because remote
search costs (30ms) require 15--20% hit rates to break even, leaving 20-30% of
production traffic uncached. Uniform cache policies compound this problem:
fixed thresholds cause false positives in dense spaces and miss valid
paraphrases in sparse spaces; fixed TTLs waste memory or serve stale data. This
paper presents category-aware semantic caching where similarity thresholds,
TTLs, and quotas vary by query category. We present a hybrid architecture
separating in-memory HNSW search from external document storage, reducing miss
cost from 30ms to 2ms. This reduction makes low-hit-rate categories
economically viable (break-even at 3-5% versus 15-20%), enabling cache coverage
across the entire workload distribution. Adaptive load-based policies extend
this framework to respond to downstream model load, dynamically adjusting
thresholds and TTLs to reduce traffic to overloaded models by 9-17% in
theoretical projections.

</details>


### [13] [SpotIt: Evaluating Text-to-SQL Evaluation with Formal Verification](https://arxiv.org/abs/2510.26840)
*Rocky Klopfenstein,Yang He,Andrew Tremante,Yuepeng Wang,Nina Narodytska,Haoze Wu*

Main category: cs.DB

TL;DR: 当前Text-to-SQL评估方法依赖于测试数据库，可能无法准确评估查询的差异。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL评估方法不够可靠，无法准确反映模型性能。

Method: 提出一种新的评估流程SpotIt，使用形式化验证引擎寻找区分生成SQL查询和标准答案SQL查询的数据库。

Result: 在BIRD数据集上的实验表明，基于测试的评估方法经常忽略生成查询和标准答案之间的差异。

Conclusion: 验证结果揭示了当前Text-to-SQL评估的复杂性。

Abstract: Community-driven Text-to-SQL evaluation platforms play a pivotal role in
tracking the state of the art of Text-to-SQL performance. The reliability of
the evaluation process is critical for driving progress in the field. Current
evaluation methods are largely test-based, which involves comparing the
execution results of a generated SQL query and a human-labeled ground-truth on
a static test database. Such an evaluation is optimistic, as two queries can
coincidentally produce the same output on the test database while actually
being different. In this work, we propose a new alternative evaluation
pipeline, called SpotIt, where a formal bounded equivalence verification engine
actively searches for a database that differentiates the generated and
ground-truth SQL queries. We develop techniques to extend existing verifiers to
support a richer SQL subset relevant to Text-to-SQL. A performance evaluation
of ten Text-to-SQL methods on the high-profile BIRD dataset suggests that
test-based methods can often overlook differences between the generated query
and the ground-truth. Further analysis of the verification results reveals a
more complex picture of the current Text-to-SQL evaluation.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [14] [Evaluating Perspectival Biases in Cross-Modal Retrieval](https://arxiv.org/abs/2510.26861)
*Teerapol Saengsukhiran,Peerawat Chomphooyod,Narabodee Rodjananant,Chompakorn Chaksangchaichot,Patawee Prakrankamanant,Witthawin Sripheanpol,Pak Lovichit,SarChaksaana Nutanong,Ekapol Chuangsuwanich*

Main category: cs.IR

TL;DR: 多模态检索系统存在视角偏差，受到语言和文化影响。


<details>
  <summary>Details</summary>
Motivation: 研究多模态检索系统中存在的两种偏差：语言流行度和文化联想偏差。

Method: 分析图像到文本检索中的流行度偏差和文本到图像检索中的联想偏差。

Result: 显式对齐能有效缓解流行度偏差，但联想偏差更难解决。

Conclusion: 实现公平的多模态系统需要有针对性的策略，文化联想偏差比语言流行度偏差更具挑战性。

Abstract: Multimodal retrieval systems are expected to operate in a semantic space,
agnostic to the language or cultural origin of the query. In practice, however,
retrieval outcomes systematically reflect perspectival biases: deviations
shaped by linguistic prevalence and cultural associations. We study two such
biases. First, prevalence bias refers to the tendency to favor entries from
prevalent languages over semantically faithful entries in image-to-text
retrieval. Second, association bias refers to the tendency to favor images
culturally associated with the query over semantically correct ones in
text-to-image retrieval. Results show that explicit alignment is a more
effective strategy for mitigating prevalence bias. However, association bias
remains a distinct and more challenging problem. These findings suggest that
achieving truly equitable multimodal systems requires targeted strategies
beyond simple data scaling and that bias arising from cultural association may
be treated as a more challenging problem than one arising from linguistic
prevalence.

</details>


### [15] [A Survey on Generative Recommendation: Data, Model, and Tasks](https://arxiv.org/abs/2510.27157)
*Min Hou,Le Wu,Yuxin Liao,Yonghui Yang,Zhen Zhang,Changlong Zheng,Han Wu,Richang Hong*

Main category: cs.IR

TL;DR: 生成式推荐系统通过将推荐任务重新定义为生成任务，从而改变了传统的判别评分方法。本综述全面考察了数据、模型和任务三个维度，探讨了生成模型在知识增强、统一异构信号、对话交互、可解释推理和个性化内容生成等方面的优势，并 критически 分析了基准设计、模型鲁棒性和部署效率等方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统面临着如何更好地匹配用户和物品的根本问题。随着生成模型的发展，推荐系统迎来了生成式推荐的新范式。

Method: 该研究通过统一的三方框架，从数据增强与统一、模型对齐与训练、任务公式化与执行等操作阶段对方法进行分解，系统地研究了生成式推荐。

Result: 总结了生成式推荐的五个关键优势：世界知识整合、自然语言理解、推理能力、扩展定律和创造性生成。

Conclusion: 生成式推荐为重塑人机信息交互的智能推荐助手铺平了道路，但也面临着基准设计、模型鲁棒性和部署效率等方面的挑战。

Abstract: Recommender systems serve as foundational infrastructure in modern
information ecosystems, helping users navigate digital content and discover
items aligned with their preferences. At their core, recommender systems
address a fundamental problem: matching users with items. Over the past
decades, the field has experienced successive paradigm shifts, from
collaborative filtering and matrix factorization in the machine learning era to
neural architectures in the deep learning era. Recently, the emergence of
generative models, especially large language models (LLMs) and diffusion
models, have sparked a new paradigm: generative recommendation, which
reconceptualizes recommendation as a generation task rather than discriminative
scoring. This survey provides a comprehensive examination through a unified
tripartite framework spanning data, model, and task dimensions. Rather than
simply categorizing works, we systematically decompose approaches into
operational stages-data augmentation and unification, model alignment and
training, task formulation and execution. At the data level, generative models
enable knowledge-infused augmentation and agent-based simulation while unifying
heterogeneous signals. At the model level, we taxonomize LLM-based methods,
large recommendation models, and diffusion approaches, analyzing their
alignment mechanisms and innovations. At the task level, we illuminate new
capabilities including conversational interaction, explainable reasoning, and
personalized content generation. We identify five key advantages: world
knowledge integration, natural language understanding, reasoning capabilities,
scaling laws, and creative generation. We critically examine challenges in
benchmark design, model robustness, and deployment efficiency, while charting a
roadmap toward intelligent recommendation assistants that fundamentally reshape
human-information interaction.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [16] [Layer of Truth: Probing Belief Shifts under Continual Pre-Training Poisoning](https://arxiv.org/abs/2510.26829)
*Svetlana Churina,Niranjan Chebrolu,Kokil Jaidka*

Main category: cs.LG

TL;DR: 大型语言模型通过在不断扩展的网络数据上进行预训练不断发展，但也容易受到微妙的错误信息的影响。研究表明，即使是少量的错误信息也可能导致模型中已建立的事实发生持久的表征漂移，这种影响因层和模型大小而异。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在持续预训练中，是否会因为重复接触虚假信息而改变其内部表征，从而偏离事实，类似于人类认知中的虚幻真理效应。

Method: 引入了一个名为“真理层”的框架和数据集，用于探测持续训练的LLM中的信念动态。通过注入可控数量的错误数据，并探测跨检查点、模型规模和问题类型的中间表示，量化事实信念何时以及如何转变。

Result: 研究发现，即使是最小程度的接触也可能导致模型中已建立的事实发生持久的表征漂移，并且这种敏感性因层和模型大小而异。

Conclusion: 持续更新的LLM存在一个被忽视的漏洞：它们具有像人类一样内化错误信息的能力，因此需要对模型更新过程中的事实完整性进行稳健的监控。

Abstract: Large language models (LLMs) continually evolve through pre-training on
ever-expanding web data, but this adaptive process also exposes them to subtle
forms of misinformation. While prior work has explored data poisoning during
static pre-training, the effects of such manipulations under continual
pre-training remain largely unexplored. Drawing inspiration from the illusory
truth effect in human cognition - where repeated exposure to falsehoods
increases belief in their accuracy - we ask whether LLMs exhibit a similar
vulnerability. We investigate whether repeated exposure to false but
confidently stated facts can shift a model's internal representation away from
the truth.
  We introduce Layer of Truth, a framework and dataset for probing belief
dynamics in continually trained LLMs. By injecting controlled amounts of
poisoned data and probing intermediate representations across checkpoints,
model scales, and question types, we quantify when and how factual beliefs
shift. Our findings reveal that even minimal exposure can induce persistent
representational drift in well-established facts, with susceptibility varying
across layers and model sizes. These results highlight an overlooked
vulnerability of continually updated LLMs: their capacity to internalize
misinformation analogously to humans, underscoring the need for robust
monitoring of factual integrity during model updates.

</details>


### [17] [SmoothGuard: Defending Multimodal Large Language Models with Noise Perturbation and Clustering Aggregation](https://arxiv.org/abs/2510.26830)
*Guangzhi Su,Shuchang Huang,Yutong Ke,Zhuohang Liu,Long Qian,Kaizhu Huang*

Main category: cs.LG

TL;DR: MLLMs易受对抗攻击影响，提出了SmoothGuard防御框架，通过噪声注入和聚类预测聚合提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: MLLMs在处理文本和视觉输入方面表现出色，但容易受到对抗性攻击，对其安全性和可靠性构成威胁。

Method: 提出了SmoothGuard，一种轻量级且模型无关的防御框架，通过随机噪声注入和基于聚类的预测聚合来增强MLLMs的鲁棒性。该方法使用高斯噪声扰动连续模态（例如，图像和音频），生成多个候选输出，并应用基于嵌入的聚类来过滤掉受对抗影响的预测。从多数集群中选择最终答案，确保在恶意扰动下也能获得稳定的响应。

Result: 在POPE、LLaVA-Bench (In-the-Wild)和MM-SafetyBench上的大量实验表明，SmoothGuard提高了对抗攻击的抵抗力，同时保持了竞争性的效用。消融研究进一步确定了平衡鲁棒性和效用的最佳噪声范围 (0.1-0.2)。

Conclusion: SmoothGuard 通过在 MLLM 中注入噪声并应用聚类来提高对抗鲁棒性，同时保持竞争性的性能。

Abstract: Multimodal large language models (MLLMs) have achieved impressive performance
across diverse tasks by jointly reasoning over textual and visual inputs.
Despite their success, these models remain highly vulnerable to adversarial
manipulations, raising concerns about their safety and reliability in
deployment. In this work, we first generalize an approach for generating
adversarial images within the HuggingFace ecosystem and then introduce
SmoothGuard, a lightweight and model-agnostic defense framework that enhances
the robustness of MLLMs through randomized noise injection and clustering-based
prediction aggregation. Our method perturbs continuous modalities (e.g., images
and audio) with Gaussian noise, generates multiple candidate outputs, and
applies embedding-based clustering to filter out adversarially influenced
predictions. The final answer is selected from the majority cluster, ensuring
stable responses even under malicious perturbations. Extensive experiments on
POPE, LLaVA-Bench (In-the-Wild), and MM-SafetyBench demonstrate that
SmoothGuard improves resilience to adversarial attacks while maintaining
competitive utility. Ablation studies further identify an optimal noise range
(0.1-0.2) that balances robustness and utility.

</details>
