<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 45]
- [cs.CV](#cs.CV) [Total: 40]
- [cs.AI](#cs.AI) [Total: 40]
- [cs.DB](#cs.DB) [Total: 11]
- [cs.IR](#cs.IR) [Total: 18]
- [cs.LG](#cs.LG) [Total: 43]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [GreenTEA: Gradient Descent with Topic-modeling and Evolutionary Auto-prompting](https://arxiv.org/abs/2508.16603)
*Zheng Dong,Luming Shang,Gabriela Olinto*

Main category: cs.CL

TL;DR: GreenTEA is an agent-based LLM workflow that uses a genetic algorithm to automatically optimize prompts, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Manually crafting effective prompts for LLMs is labor-intensive, demands domain expertise, and lacks scalability. Existing automatic prompt optimization methods are either computationally expensive or risk suboptimal optimization.

Method: An agentic LLM workflow (GreenTEA) balances exploration and exploitation, using a collaborative team of agents to iteratively refine prompts based on feedback from error samples, guided by a genetic algorithm framework.

Result: GreenTEA outperforms human-engineered prompts and existing automatic prompt optimization methods.

Conclusion: GreenTEA demonstrates superior performance against human-engineered prompts and existing state-of-the-art automatic prompt optimization methods on public benchmark datasets covering logical and quantitative reasoning, commonsense, and ethical decision-making.

Abstract: High-quality prompts are crucial for Large Language Models (LLMs) to achieve
exceptional performance. However, manually crafting effective prompts is
labor-intensive and demands significant domain expertise, limiting its
scalability. Existing automatic prompt optimization methods either extensively
explore new prompt candidates, incurring high computational costs due to
inefficient searches within a large solution space, or overly exploit feedback
on existing prompts, risking suboptimal optimization because of the complex
prompt landscape. To address these challenges, we introduce GreenTEA, an
agentic LLM workflow for automatic prompt optimization that balances candidate
exploration and knowledge exploitation. It leverages a collaborative team of
agents to iteratively refine prompts based on feedback from error samples. An
analyzing agent identifies common error patterns resulting from the current
prompt via topic modeling, and a generation agent revises the prompt to
directly address these key deficiencies. This refinement process is guided by a
genetic algorithm framework, which simulates natural selection by evolving
candidate prompts through operations such as crossover and mutation to
progressively optimize model performance. Extensive numerical experiments
conducted on public benchmark datasets suggest the superior performance of
GreenTEA against human-engineered prompts and existing state-of-the-arts for
automatic prompt optimization, covering logical and quantitative reasoning,
commonsense, and ethical decision-making.

</details>


### [2] [Cognitive Decision Routing in Large Language Models: When to Think Fast, When to Think Slow](https://arxiv.org/abs/2508.16636)
*Y. Du,C. Guo,W. Wang,G. Tang*

Main category: cs.CL

TL;DR: This paper introduces a Cognitive Decision Routing (CDR) framework for LLMs that dynamically chooses reasoning strategies based on query complexity, improving performance and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: LLMs face a challenge in deciding when to rely on rapid, intuitive responses versus engaging in slower, more deliberate reasoning. Current models either apply uniform reasoning depth or rely on computationally expensive methods for all queries.

Method: The paper proposes a Cognitive Decision Routing (CDR) framework that dynamically determines the appropriate reasoning strategy based on query characteristics. A meta-cognitive layer analyzes query complexity through multiple dimensions: correlation strength, domain boundary crossings, stakeholder multiplicity, and uncertainty levels.

Result: CDR achieves superior performance while reducing computational costs by 34% compared to uniform deep reasoning approaches. It achieves 23% improvement in consistency and 18% better accuracy on expert-level evaluations in professional judgment tasks.

Conclusion: The Cognitive Decision Routing (CDR) framework achieves superior performance and reduces computational costs by 34% compared to uniform deep reasoning approaches. It shows particular strength in professional judgment tasks, achieving 23% improvement in consistency and 18% better accuracy on expert-level evaluations.

Abstract: Large Language Models (LLMs) face a fundamental challenge in deciding when to
rely on rapid, intuitive responses versus engaging in slower, more deliberate
reasoning. Inspired by Daniel Kahneman's dual-process theory and his insights
on human cognitive biases, we propose a novel Cognitive Decision Routing (CDR)
framework that dynamically determines the appropriate reasoning strategy based
on query characteristics. Our approach addresses the current limitations where
models either apply uniform reasoning depth or rely on computationally
expensive methods for all queries. We introduce a meta-cognitive layer that
analyzes query complexity through multiple dimensions: correlation strength
between given information and required conclusions, domain boundary crossings,
stakeholder multiplicity, and uncertainty levels. Through extensive experiments
on diverse reasoning tasks, we demonstrate that CDR achieves superior
performance while reducing computational costs by 34\% compared to uniform deep
reasoning approaches. Our framework shows particular strength in professional
judgment tasks, achieving 23\% improvement in consistency and 18\% better
accuracy on expert-level evaluations. This work bridges cognitive science
principles with practical AI system design, offering a principled approach to
adaptive reasoning in LLMs.

</details>


### [3] [Trust but Verify! A Survey on Verification Design for Test-time Scaling](https://arxiv.org/abs/2508.16665)
*V Venktesh,Mandeep rathee,Avishek Anand*

Main category: cs.CL

TL;DR: 本文对大语言模型测试时扩展中使用的验证器方法进行了综述，涵盖了不同的训练机制和应用。


<details>
  <summary>Details</summary>
Motivation: 尽管验证器已被广泛采用，但目前还没有对各种验证方法及其训练机制进行详细的收集、清晰的分类和讨论。

Method: 对现有文献中的验证方法进行收集、分类和讨论，并提出了统一的视图。

Result: 总结了各种验证器训练方法，类型及其在测试时扩展中的效用。

Conclusion: 这篇综述涵盖了文献中各种验证方法，并提出了验证器训练、类型及其在测试时扩展中的效用的统一视图。

Abstract: Test-time scaling (TTS) has emerged as a new frontier for scaling the
performance of Large Language Models. In test-time scaling, by using more
computational resources during inference, LLMs can improve their reasoning
process and task performance. Several approaches have emerged for TTS such as
distilling reasoning traces from another model or exploring the vast decoding
search space by employing a verifier. The verifiers serve as reward models that
help score the candidate outputs from the decoding process to diligently
explore the vast solution space and select the best outcome. This paradigm
commonly termed has emerged as a superior approach owing to parameter free
scaling at inference time and high performance gains. The verifiers could be
prompt-based, fine-tuned as a discriminative or generative model to verify
process paths, outcomes or both. Despite their widespread adoption, there is no
detailed collection, clear categorization and discussion of diverse
verification approaches and their training mechanisms. In this survey, we cover
the diverse approaches in the literature and present a unified view of verifier
training, types and their utility in test-time scaling. Our repository can be
found at
https://github.com/elixir-research-group/Verifierstesttimescaling.github.io.

</details>


### [4] [Do Cognitively Interpretable Reasoning Traces Improve LLM Performance?](https://arxiv.org/abs/2508.16695)
*Siddhant Bhambri,Upasana Biswas,Subbarao Kambhampati*

Main category: cs.CL

TL;DR: CoT reasoning traces don't necessarily need to be interpretable to enhance LLM task performance. R1 traces, despite being the least interpretable, yield the strongest performance.


<details>
  <summary>Details</summary>
Motivation: Investigate whether CoT reasoning traces must be interpretable to enhance LLM task performance in the Open Book Question-Answering domain.

Method: Supervised fine-tuning LLaMA and Qwen models on four types of reasoning traces: (1) DeepSeek R1 traces, (2) LLM-generated summaries of R1 traces, (3) LLM-generated post-hoc explanations of R1 traces, and (4) algorithmically generated verifiably correct traces. Human-subject study with 100 participants rating the interpretability of each trace type.

Result: R1 traces yield the strongest performance but are judged the least interpretable.

Conclusion: Fine-tuning on R1 traces yields the strongest performance, but participants judged these traces to be the least interpretable, suggesting decoupling intermediate tokens from end user interpretability is useful.

Abstract: Recent progress in reasoning-oriented Large Language Models (LLMs) has been
driven by introducing Chain-of-Thought (CoT) traces, where models generate
intermediate reasoning traces before producing an answer. These traces, as in
DeepSeek R1, are not only used to guide inference but also serve as supervision
signals for distillation into smaller models. A common but often implicit
assumption is that CoT traces should be semantically meaningful and
interpretable to the end user. While recent research questions the need for
semantic nature of these traces, in this paper, we ask: ``\textit{Must CoT
reasoning traces be interpretable to enhance LLM task performance?}" We
investigate this question in the Open Book Question-Answering domain by
supervised fine-tuning LLaMA and Qwen models on four types of reasoning traces:
(1) DeepSeek R1 traces, (2) LLM-generated summaries of R1 traces, (3)
LLM-generated post-hoc explanations of R1 traces, and (4) algorithmically
generated verifiably correct traces. To quantify the trade-off between
interpretability and performance, we further conduct a human-subject study with
100 participants rating the interpretability of each trace type. Our results
reveal a striking mismatch: while fine-tuning on R1 traces yields the strongest
performance, participants judged these traces to be the least interpretable.
These findings suggest that it is useful to decouple intermediate tokens from
end user interpretability.

</details>


### [5] [QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting](https://arxiv.org/abs/2508.16697)
*Nicole Cho,William Watson,Alec Koppel,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CL

TL;DR: QueryBandits通过智能重写查询来减少大型语言模型中的幻觉，效果显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）中高级推理能力导致更高的幻觉发生率；然而，大多数缓解工作侧重于事后过滤，而不是塑造触发它们的查询。

Method: QueryBandits，一个bandit框架，设计重写策略以最大化奖励模型。该模型封装了基于输入查询的17个语言特征的敏感性的幻觉倾向。

Result: 在13个不同的QA基准测试和每个数据集1,050个词汇扰动查询中，我们的顶级上下文QueryBandit（Thompson Sampling）相对于无重写基线实现了87.5%的胜率，并且也优于零样本静态提示（“释义”或“扩展”）分别提高了42.6%和60.3%。

Conclusion: QueryBandits通过查询重写减轻幻觉，优于静态提示策略，并且没有单一的最优重写策略。

Abstract: Advanced reasoning capabilities in Large Language Models (LLMs) have caused
higher hallucination prevalence; yet most mitigation work focuses on
after-the-fact filtering rather than shaping the queries that trigger them. We
introduce QueryBandits, a bandit framework that designs rewrite strategies to
maximize a reward model, that encapsulates hallucination propensity based upon
the sensitivities of 17 linguistic features of the input query-and therefore,
proactively steer LLMs away from generating hallucinations. Across 13 diverse
QA benchmarks and 1,050 lexically perturbed queries per dataset, our top
contextual QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a
no-rewrite baseline and also outperforms zero-shot static prompting
("paraphrase" or "expand") by 42.6% and 60.3% respectively. Therefore, we
empirically substantiate the effectiveness of QueryBandits in mitigating
hallucination via the intervention that takes the form of a query rewrite.
Interestingly, certain static prompting strategies, which constitute a
considerable number of current query rewriting literature, have a higher
cumulative regret than the no-rewrite baseline, signifying that static rewrites
can worsen hallucination. Moreover, we discover that the converged per-arm
regression feature weight vectors substantiate that there is no single rewrite
strategy optimal for all queries. In this context, guided rewriting via
exploiting semantic features with QueryBandits can induce significant shifts in
output behavior through forward-pass mechanisms, bypassing the need for
retraining or gradient-based adaptation.

</details>


### [6] [Assessing Consciousness-Related Behaviors in Large Language Models Using the Maze Test](https://arxiv.org/abs/2508.16705)
*Rui A. Pimenta,Tim Schlippe,Kristina Schaaff*

Main category: cs.CL

TL;DR: LLMs exhibit some consciousness-like behaviors but lack integrated self-awareness.


<details>
  <summary>Details</summary>
Motivation: We investigate consciousness-like behaviors in Large Language Models (LLMs).

Method: Maze Test, challenging models to navigate mazes from a first-person perspective. Synthesizing consciousness theories into 13 essential characteristics, we evaluated 12 leading LLMs across zero-shot, one-shot, and few-shot learning scenarios.

Result: Reasoning-capable LLMs consistently outperforming standard versions, with Gemini 2.0 Pro achieving 52.9% Complete Path Accuracy and DeepSeek-R1 reaching 80.5% Partial Path Accuracy. The gap between these metrics indicates LLMs struggle to maintain coherent self-models throughout solutions.

Conclusion: LLMs show progress in consciousness-related behaviors through reasoning mechanisms, but lack the integrated, persistent self-awareness characteristic of consciousness.

Abstract: We investigate consciousness-like behaviors in Large Language Models (LLMs)
using the Maze Test, challenging models to navigate mazes from a first-person
perspective. This test simultaneously probes spatial awareness,
perspective-taking, goal-directed behavior, and temporal sequencing-key
consciousness-associated characteristics. After synthesizing consciousness
theories into 13 essential characteristics, we evaluated 12 leading LLMs across
zero-shot, one-shot, and few-shot learning scenarios. Results showed
reasoning-capable LLMs consistently outperforming standard versions, with
Gemini 2.0 Pro achieving 52.9% Complete Path Accuracy and DeepSeek-R1 reaching
80.5% Partial Path Accuracy. The gap between these metrics indicates LLMs
struggle to maintain coherent self-models throughout solutions -- a fundamental
consciousness aspect. While LLMs show progress in consciousness-related
behaviors through reasoning mechanisms, they lack the integrated, persistent
self-awareness characteristic of consciousness.

</details>


### [7] [Sparse and Dense Retrievers Learn Better Together: Joint Sparse-Dense Optimization for Text-Image Retrieval](https://arxiv.org/abs/2508.16707)
*Jonghyun Song,Youngjune Lee,Gyu-Hwung Cho,Ilhyeon Song,Saehun Kim,Yohan Jo*

Main category: cs.CL

TL;DR: 我们提出了一个简单而有效的框架，该框架通过自知识蒸馏实现密集和稀疏表示之间的双向学习。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常依赖于计算成本高昂的对比预训练，或从冻结的密集模型中提取知识，这限制了相互增强的潜力。

Method: 通过自知识蒸馏实现密集和稀疏表示之间的双向学习。这种双向学习是通过集成相似性得分实现的，该得分是密集和稀疏相似性的加权和，作为两种表示的共享教师信号。为了确保效率，我们微调了密集编码器的最后一层和稀疏投影头，从而可以轻松地调整任何现有的 VLP 模型。

Result: 在 MSCOCO 和 Flickr30k 上的实验表明，

Conclusion: 该稀疏检索器不仅优于现有的稀疏基线，而且实现了与密集检索器相当甚至超过其性能的性能，同时保留了稀疏模型的优势。

Abstract: Vision-Language Pretrained (VLP) models have achieved impressive performance
on multimodal tasks, including text-image retrieval, based on dense
representations. Meanwhile, Learned Sparse Retrieval (LSR) has gained traction
in text-only settings due to its interpretability and efficiency with fast
term-based lookup via inverted indexes. Inspired by these advantages, recent
work has extended LSR to the multimodal domain. However, these methods often
rely on computationally expensive contrastive pre-training, or distillation
from a frozen dense model, which limits the potential for mutual enhancement.
To address these limitations, we propose a simple yet effective framework that
enables bi-directional learning between dense and sparse representations
through Self-Knowledge Distillation. This bi-directional learning is achieved
using an integrated similarity score-a weighted sum of dense and sparse
similarities-which serves as a shared teacher signal for both representations.
To ensure efficiency, we fine-tune the final layer of the dense encoder and the
sparse projection head, enabling easy adaptation of any existing VLP model.
Experiments on MSCOCO and Flickr30k demonstrate that our sparse retriever not
only outperforms existing sparse baselines, but also achieves performance
comparable to-or even surpassing-its dense counterparts, while retaining the
benefits of sparse models.

</details>


### [8] [Error Reflection Prompting: Can Large Language Models Successfully Understand Errors?](https://arxiv.org/abs/2508.16729)
*Jason Li,Lauren Yraola,Kevin Zhu,Sean O'Brien*

Main category: cs.CL

TL;DR: 提出ERP方法，增强语言模型的推理能力，使其能够识别和纠正错误。


<details>
  <summary>Details</summary>
Motivation: 现有CoT方法缺乏反思和纠错能力，可能导致模型重复犯错。

Method: 提出错误反思提示（ERP）方法，该方法包含错误答案、错误识别和正确答案。

Result: ERP可以有效识别错误类型和导致错误答案的步骤，从而避免错误并提升推理能力。

Conclusion: ERP作为一种通用补充，增强了CoT的推理能力，提高了模型错误的可解释性。

Abstract: Prompting methods for language models, such as Chain-of-thought (CoT),
present intuitive step-by-step processes for problem solving. These
methodologies aim to equip models with a better understanding of the correct
procedures for addressing a given task. Despite these advancements, CoT lacks
the ability of reflection and error correction, potentially causing a model to
perpetuate mistakes and errors. Therefore, inspired by the human ability for
said tasks, we propose Error Reflection Prompting (ERP) to further enhance
reasoning in language models. Building upon CoT, ERP is a method comprised of
an incorrect answer, error recognition, and a correct answer. This process
enables the model to recognize types of errors and the steps that lead to
incorrect answers, allowing the model to better discern which steps to avoid
and which to take. The model is able to generate the error outlines itself with
automated ERP generation, allowing for error recognition and correction to be
integrated into the reasoning chain and produce scalability and reliability in
the process. The results demonstrate that ERP serves as a versatile supplement
to conventional CoT, ultimately contributing to more robust and capable
reasoning abilities along with increased interpretability in how models
ultimately reach their errors.

</details>


### [9] [GAICo: A Deployed and Extensible Framework for Evaluating Diverse and Multimodal Generative AI Outputs](https://arxiv.org/abs/2508.16753)
*Nitin Gupta,Pallav Koppisetti,Kausik Lakkaraju,Biplav Srivastava*

Main category: cs.CL

TL;DR: GAICo: an open-source Python library streamlines and standardizes GenAI output comparison, supporting a comprehensive suite of reference-based metrics for various data formats. It has been downloaded over 13K times in two months.


<details>
  <summary>Details</summary>
Motivation: practitioners often resort to ad-hoc, non-standardized scripts, as common metrics are often unsuitable for specialized, structured outputs or holistic comparison across modalities. This fragmentation hinders comparability and slows AI system development.

Method: a deployed, open-source Python library that streamlines and standardizes GenAI output comparison

Result: GAICo provides a unified, extensible framework supporting a comprehensive suite of reference-based metrics for unstructured text, specialized structured data formats, and multimedia. Since its release on PyPI in Jun 2025, the tool has been downloaded over 13K times, across versions, by Aug 2025, demonstrating growing community interest.

Conclusion: GAICo empowers AI researchers and developers to efficiently assess system performance, make evaluation reproducible, improve development velocity, and ultimately build more trustworthy AI systems.

Abstract: The rapid proliferation of Generative AI (GenAI) into diverse, high-stakes
domains necessitates robust and reproducible evaluation methods. However,
practitioners often resort to ad-hoc, non-standardized scripts, as common
metrics are often unsuitable for specialized, structured outputs (e.g.,
automated plans, time-series) or holistic comparison across modalities (e.g.,
text, audio, and image). This fragmentation hinders comparability and slows AI
system development. To address this challenge, we present GAICo (Generative AI
Comparator): a deployed, open-source Python library that streamlines and
standardizes GenAI output comparison. GAICo provides a unified, extensible
framework supporting a comprehensive suite of reference-based metrics for
unstructured text, specialized structured data formats, and multimedia (images,
audio). Its architecture features a high-level API for rapid, end-to-end
analysis, from multi-model comparison to visualization and reporting, alongside
direct metric access for granular control. We demonstrate GAICo's utility
through a detailed case study evaluating and debugging complex, multi-modal AI
Travel Assistant pipelines. GAICo empowers AI researchers and developers to
efficiently assess system performance, make evaluation reproducible, improve
development velocity, and ultimately build more trustworthy AI systems,
aligning with the goal of moving faster and safer in AI deployment. Since its
release on PyPI in Jun 2025, the tool has been downloaded over 13K times,
across versions, by Aug 2025, demonstrating growing community interest.

</details>


### [10] [How Good are LLM-based Rerankers? An Empirical Analysis of State-of-the-Art Reranking Models](https://arxiv.org/abs/2508.16757)
*Abdelrahman Abdallah,Bhawna Piryani,Jamshid Mozafari,Mohammed Ali,Adam Jatowt*

Main category: cs.CL

TL;DR: This paper compares LLM-based and lightweight rerankers on familiar and novel queries, finding LLMs better on familiar queries but with varying generalization, while lightweight models are more efficient. Query novelty is a key factor.


<details>
  <summary>Details</summary>
Motivation: Determine performance disparity between LLM-based rerankers and lightweight counterparts, especially on novel queries, and understand the causes.

Method: Systematic and comprehensive empirical evaluation of 22 reranking methods (40 variants) on TREC DL19, DL20, BEIR, and a novel dataset.

Result: LLM-based rerankers are better on familiar queries, but generalization varies on novel queries. Lightweight models offer comparable efficiency. Query novelty impacts reranking effectiveness.

Conclusion: LLM-based rerankers perform better on familiar queries, but their generalization to novel queries varies. Lightweight models offer comparable efficiency. Novelty of queries impacts reranking effectiveness.

Abstract: In this work, we present a systematic and comprehensive empirical evaluation
of state-of-the-art reranking methods, encompassing large language model
(LLM)-based, lightweight contextual, and zero-shot approaches, with respect to
their performance in information retrieval tasks. We evaluate in total 22
methods, including 40 variants (depending on used LLM) across several
established benchmarks, including TREC DL19, DL20, and BEIR, as well as a novel
dataset designed to test queries unseen by pretrained models. Our primary goal
is to determine, through controlled and fair comparisons, whether a performance
disparity exists between LLM-based rerankers and their lightweight
counterparts, particularly on novel queries, and to elucidate the underlying
causes of any observed differences. To disentangle confounding factors, we
analyze the effects of training data overlap, model architecture, and
computational efficiency on reranking performance. Our findings indicate that
while LLM-based rerankers demonstrate superior performance on familiar queries,
their generalization ability to novel queries varies, with lightweight models
offering comparable efficiency. We further identify that the novelty of queries
significantly impacts reranking effectiveness, highlighting limitations in
existing approaches.
https://github.com/DataScienceUIBK/llm-reranking-generalization-study

</details>


### [11] [Toward Socially Aware Vision-Language Models: Evaluating Cultural Competence Through Multimodal Story Generation](https://arxiv.org/abs/2508.16762)
*Arka Mukherjee,Shreya Ghosh*

Main category: cs.CL

TL;DR: The paper evaluates cultural competence in VLMs during multimodal story generation, revealing both adaptation capabilities and limitations like architectural bias and limited visual-cultural understanding.


<details>
  <summary>Details</summary>
Motivation: Ensuring cultural competence in VLMs is critical for responsible AI systems, but no research has systematically assessed how VLMs adapt outputs when cultural identity cues are embedded in both textual prompts and visual inputs during generative tasks.

Method: Developed a novel multimodal framework that perturbs cultural identity and evaluates VLMs on story generation.

Result: Significant cultural adaptation capabilities were found, but also limitations: cultural competence varies dramatically across architectures, some models exhibit inverse cultural alignment, and automated metrics show architectural bias contradicting human assessments. Visual-cultural understanding remains limited.

Conclusion: VLMs demonstrated cultural adaptation capabilities but also limitations like varying competence across architectures and biases in automated metrics. Visual-cultural understanding remains limited.

Abstract: As Vision-Language Models (VLMs) achieve widespread deployment across diverse
cultural contexts, ensuring their cultural competence becomes critical for
responsible AI systems. While prior work has evaluated cultural awareness in
text-only models and VLM object recognition tasks, no research has
systematically assessed how VLMs adapt outputs when cultural identity cues are
embedded in both textual prompts and visual inputs during generative tasks. We
present the first comprehensive evaluation of VLM cultural competence through
multimodal story generation, developing a novel multimodal framework that
perturbs cultural identity and evaluates 5 contemporary VLMs on a downstream
task: story generation. Our analysis reveals significant cultural adaptation
capabilities, with rich culturally-specific vocabulary spanning names, familial
terms, and geographic markers. However, we uncover concerning limitations:
cultural competence varies dramatically across architectures, some models
exhibit inverse cultural alignment, and automated metrics show architectural
bias contradicting human assessments. Cross-modal evaluation shows that
culturally distinct outputs are indeed detectable through visual-semantic
similarity (28.7% within-nationality vs. 0.2% cross-nationality recall), yet
visual-cultural understanding remains limited. In essence, we establish the
promise and challenges of cultural competence in multimodal AI. We publicly
release our codebase and data: https://github.com/ArkaMukherjee0/mmCultural

</details>


### [12] [Explaining Black-box Language Models with Knowledge Probing Systems: A Post-hoc Explanation Perspective](https://arxiv.org/abs/2508.16969)
*Yunxiao Zhao,Hao Xu,Zhiqiang Wang,Xiaoli Li,Jiye Liang,Ru Li*

Main category: cs.CL

TL;DR: This paper proposes KnowProb, a knowledge-guided probing approach, to examine whether PLMs understand implicit knowledge. The experiments show PLMs struggle with hidden knowledge, and KnowProb can identify the limitations of black-box models.


<details>
  <summary>Details</summary>
Motivation: the trustworthiness challenges posed by these black-box models have become increasingly evident in recent years. To alleviate this problem

Method: a novel Knowledge-guided Probing approach called KnowProb

Result: validate that current small-scale (or large-scale) PLMs only learn a single distribution of representation, and still face significant challenges in capturing the hidden knowledge behind a given text.

Conclusion: current small-scale (or large-scale) PLMs only learn a single distribution of representation, and still face significant challenges in capturing the hidden knowledge behind a given text. Furthermore, we demonstrate that our proposed approach is effective for identifying the limitations of existing black-box models from multiple probing perspectives, which facilitates researchers to promote the study of detecting black-box models in an explainable way.

Abstract: Pre-trained Language Models (PLMs) are trained on large amounts of unlabeled
data, yet they exhibit remarkable reasoning skills. However, the
trustworthiness challenges posed by these black-box models have become
increasingly evident in recent years. To alleviate this problem, this paper
proposes a novel Knowledge-guided Probing approach called KnowProb in a
post-hoc explanation way, which aims to probe whether black-box PLMs understand
implicit knowledge beyond the given text, rather than focusing only on the
surface level content of the text. We provide six potential explanations
derived from the underlying content of the given text, including three
knowledge-based understanding and three association-based reasoning. In
experiments, we validate that current small-scale (or large-scale) PLMs only
learn a single distribution of representation, and still face significant
challenges in capturing the hidden knowledge behind a given text. Furthermore,
we demonstrate that our proposed approach is effective for identifying the
limitations of existing black-box models from multiple probing perspectives,
which facilitates researchers to promote the study of detecting black-box
models in an explainable way.

</details>


### [13] [Assess and Prompt: A Generative RL Framework for Improving Engagement in Online Mental Health Communities](https://arxiv.org/abs/2508.16788)
*Bhagesh Gaur,Karan Gupta,Aseem Srivastava,Manish Gupta,Md Shad Akhtar*

Main category: cs.CL

TL;DR: This paper introduces a framework and system (MH-COPILOT) to identify missing support attributes in online mental health posts and prompt users to provide them, leading to better engagement. A new dataset (REDDME) was created to support this.


<details>
  <summary>Details</summary>
Motivation: Many posts in Online Mental Health Communities (OMHCs) remain unanswered due to missing support attributes that signal the need for help.

Method: A reinforcement learning-based system (MH-COPILOT) is proposed, integrating contextual attribute-span identification, support attribute intensity classification, controlled question generation via a hierarchical taxonomy (CueTaxo), and a verifier for reward modeling. A new dataset, REDDME, of 4,760 annotated posts was created to support this.

Result: Empirical results across four language models demonstrate significant improvements in attribute elicitation and user engagement. Human evaluation validates the model's effectiveness in real-world OMHC settings.

Conclusion: The MH-COPILOT model effectively identifies missing support attributes in online mental health community posts and generates targeted prompts to elicit this information, leading to improved user engagement.

Abstract: Online Mental Health Communities (OMHCs) provide crucial peer and expert
support, yet many posts remain unanswered due to missing support attributes
that signal the need for help. We present a novel framework that identifies
these gaps and prompts users to enrich their posts, thereby improving
engagement. To support this, we introduce REDDME, a new dataset of 4,760 posts
from mental health subreddits annotated for the span and intensity of three key
support attributes: event what happened?, effect what did the user experience?,
and requirement what support they need?. Next, we devise a hierarchical
taxonomy, CueTaxo, of support attributes for controlled question generation.
Further, we propose MH-COPILOT, a reinforcement learning-based system that
integrates (a) contextual attribute-span identification, (b) support attribute
intensity classification, (c) controlled question generation via a hierarchical
taxonomy, and (d) a verifier for reward modeling. Our model dynamically
assesses posts for the presence/absence of support attributes, and generates
targeted prompts to elicit missing information. Empirical results across four
notable language models demonstrate significant improvements in attribute
elicitation and user engagement. A human evaluation further validates the
model's effectiveness in real-world OMHC settings.

</details>


### [14] [Capturing Legal Reasoning Paths from Facts to Law in Court Judgments using Knowledge Graphs](https://arxiv.org/abs/2508.17340)
*Ryoma Kondo,Riona Matsuoka,Takahiro Yoshida,Kazuyuki Yamasawa,Ryohei Hisano*

Main category: cs.CL

TL;DR: 本文构建了一个法律知识图谱，通过从日本行政法院的判决中提取法律推理，将事实、规范和法律应用联系起来，从而更准确地捕捉法律推理。


<details>
  <summary>Details</summary>
Motivation: 现有的捕获法律推理的自动化方法（包括大型语言模型）通常无法识别相关的法律背景，不能准确地追溯事实与法律规范之间的关系，并且可能歪曲司法推理的层次结构。这些限制阻碍了捕捉法院如何在实践中将法律应用于事实的能力。

Method: 使用基于提示的大型语言模型提取法律推理的组成部分，标准化对法律条文的引用，并通过法律推理的本体将事实、规范和法律适用联系起来。

Result: 该系统在从事实中检索相关法律条文方面，比大型语言模型基线和检索增强方法更准确。

Conclusion: 构建了一个法律知识图谱，能够捕捉真实判决中的完整法律推理结构，使隐式推理显式化和机器可读。

Abstract: Court judgments reveal how legal rules have been interpreted and applied to
facts, providing a foundation for understanding structured legal reasoning.
However, existing automated approaches for capturing legal reasoning, including
large language models, often fail to identify the relevant legal context, do
not accurately trace how facts relate to legal norms, and may misrepresent the
layered structure of judicial reasoning. These limitations hinder the ability
to capture how courts apply the law to facts in practice. In this paper, we
address these challenges by constructing a legal knowledge graph from 648
Japanese administrative court decisions. Our method extracts components of
legal reasoning using prompt-based large language models, normalizes references
to legal provisions, and links facts, norms, and legal applications through an
ontology of legal inference. The resulting graph captures the full structure of
legal reasoning as it appears in real court decisions, making implicit
reasoning explicit and machine-readable. We evaluate our system using expert
annotated data, and find that it achieves more accurate retrieval of relevant
legal provisions from facts than large language model baselines and
retrieval-augmented methods.

</details>


### [15] [ReProCon: Scalable and Resource-Efficient Few-Shot Biomedical Named Entity Recognition](https://arxiv.org/abs/2508.16833)
*Jeongkyun Yoo,Nela Riddle,Andrew Hoblitzell*

Main category: cs.CL

TL;DR: ReProCon是一种用于生物医学NER的Few-Shot框架，它结合了多原型建模、余弦对比学习和Reptile元学习，以应对数据稀缺和标签不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 生物医学领域中的命名实体识别(NER)由于数据稀缺和标签分布不平衡而面临挑战，尤其是在细粒度的实体类型方面。

Method: 结合多原型建模、余弦对比学习和Reptile元学习的Few-Shot NER框架ReProCon。

Result: ReProCon实现了接近基于bert的基线的宏F1值(约为BERT性能的99%)。该模型在30%的标签预算下保持稳定，并且在从19个类别扩展到50个类别时，F1仅下降了7.8%，优于SpanProto和CONTaiNER等基线，后者在Few-NERD中下降了10%到32%。

Conclusion: ReProCon在资源有限的情况下表现出最先进的性能，使其适用于生物医学应用。

Abstract: Named Entity Recognition (NER) in biomedical domains faces challenges due to
data scarcity and imbalanced label distributions, especially with fine-grained
entity types. We propose ReProCon, a novel few-shot NER framework that combines
multi-prototype modeling, cosine-contrastive learning, and Reptile
meta-learning to tackle these issues. By representing each category with
multiple prototypes, ReProCon captures semantic variability, such as synonyms
and contextual differences, while a cosine-contrastive objective ensures strong
interclass separation. Reptile meta-updates enable quick adaptation with little
data. Using a lightweight fastText + BiLSTM encoder with much lower memory
usage, ReProCon achieves a macro-$F_1$ score close to BERT-based baselines
(around 99 percent of BERT performance). The model remains stable with a label
budget of 30 percent and only drops 7.8 percent in $F_1$ when expanding from 19
to 50 categories, outperforming baselines such as SpanProto and CONTaiNER,
which see 10 to 32 percent degradation in Few-NERD. Ablation studies highlight
the importance of multi-prototype modeling and contrastive learning in managing
class imbalance. Despite difficulties with label ambiguity, ReProCon
demonstrates state-of-the-art performance in resource-limited settings, making
it suitable for biomedical applications.

</details>


### [16] [LLMs Learn Constructions That Humans Do Not Know](https://arxiv.org/abs/2508.16837)
*Jonathan Dunn,Mai Mohamed Eida*

Main category: cs.CL

TL;DR: LLMs hallucinate grammatical structures, and current methods may falsely confirm these hallucinations.


<details>
  <summary>Details</summary>
Motivation: Investigating false positive constructions (grammatical structures LLMs hallucinate).

Method: Behavioural probing with contextual embeddings and meta-linguistic probing with prompts.

Result: LLMs hallucinate constructions; false hypotheses about these constructions would be overwhelmingly confirmed.

Conclusion: Construction probing methods suffer from a confirmation bias, raising concerns about incorrect syntactic knowledge in models.

Abstract: This paper investigates false positive constructions: grammatical structures
which an LLM hallucinates as distinct constructions but which human
introspection does not support. Both a behavioural probing task using
contextual embeddings and a meta-linguistic probing task using prompts are
included, allowing us to distinguish between implicit and explicit linguistic
knowledge. Both methods reveal that models do indeed hallucinate constructions.
We then simulate hypothesis testing to determine what would have happened if a
linguist had falsely hypothesized that these hallucinated constructions do
exist. The high accuracy obtained shows that such false hypotheses would have
been overwhelmingly confirmed. This suggests that construction probing methods
suffer from a confirmation bias and raises the issue of what unknown and
incorrect syntactic knowledge these models also possess.

</details>


### [17] [If We May De-Presuppose: Robustly Verifying Claims through Presupposition-Free Question Decomposition](https://arxiv.org/abs/2508.16838)
*Shubhashis Roy Dipta,Francis Ferraro*

Main category: cs.CL

TL;DR: 大型语言模型仍然容易受到prompt variance和预设的影响。该方法通过无预设的、分解的问题进行推理，持续缓解这些问题，实现了高达2-5%的改进。


<details>
  <summary>Details</summary>
Motivation: 先前的工作表明，生成问题中的预设会引入未经证实的假设，导致声明验证中的不一致。此外，prompt敏感性仍然是大型语言模型(llm)的一个重大挑战，导致性能差异高达3-6%。

Method: 一个结构化的和鲁棒的声明验证框架，通过无预设的、分解的问题进行推理。

Result: 在多个prompt、数据集和llm上进行的大量实验表明，即使是最先进的模型仍然容易受到prompt variance和预设的影响。该方法持续缓解这些问题，实现了高达2-5%的改进。

Conclusion: 即使是最先进的模型仍然容易受到prompt variance和预设的影响。该方法持续缓解这些问题，实现了高达2-5%的改进。

Abstract: Prior work has shown that presupposition in generated questions can introduce
unverified assumptions, leading to inconsistencies in claim verification.
Additionally, prompt sensitivity remains a significant challenge for large
language models (LLMs), resulting in performance variance as high as 3-6%.
While recent advancements have reduced this gap, our study demonstrates that
prompt sensitivity remains a persistent issue. To address this, we propose a
structured and robust claim verification framework that reasons through
presupposition-free, decomposed questions. Extensive experiments across
multiple prompts, datasets, and LLMs reveal that even state-of-the-art models
remain susceptible to prompt variance and presupposition. Our method
consistently mitigates these issues, achieving up to a 2-5% improvement.

</details>


### [18] [Learning from Diverse Reasoning Paths with Routing and Collaboration](https://arxiv.org/abs/2508.16861)
*Zhenyu Lei,Zhen Tan,Song Wang,Yaochen Zhu,Zihan Chen,Yushun Dong,Jundong Li*

Main category: cs.CL

TL;DR: QR-Distill结合路径质量过滤、条件路由和协作对等教学，以提高知识蒸馏的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的进步显着增强了推理能力，但它们在资源受限的场景中的部署受到限制。知识蒸馏通过将知识从强大的教师模型转移到紧凑和透明的学生来解决这个问题。由于传统的令牌级别监督的范围有限，有效地捕捉教师的综合推理具有挑战性。使用每个查询的多个推理路径可以缓解此问题，但将每个路径相同地对待是次优的，因为路径在任务和模型中的质量和适用性方面差异很大。

Method: 结合路径质量过滤、条件路由和协作对等教学的QR-Distill

Result: QR-Distill优于传统的单路径和多路径蒸馏方法

Conclusion: QR-Distill在知识迁移方面优于传统的单路径和多路径蒸馏方法。消融研究进一步强调了每个组件的重要性，包括质量过滤、条件路由和对有效知识转移的对等教学。

Abstract: Advances in large language models (LLMs) significantly enhance reasoning
capabilities but their deployment is restricted in resource-constrained
scenarios. Knowledge distillation addresses this by transferring knowledge from
powerful teacher models to compact and transparent students. However,
effectively capturing the teacher's comprehensive reasoning is challenging due
to conventional token-level supervision's limited scope. Using multiple
reasoning paths per query alleviates this problem, but treating each path
identically is suboptimal as paths vary widely in quality and suitability
across tasks and models. We propose Quality-filtered Routing with Cooperative
Distillation (QR-Distill), combining path quality filtering, conditional
routing, and cooperative peer teaching. First, quality filtering retains only
correct reasoning paths scored by an LLM-based evaluation. Second, conditional
routing dynamically assigns paths tailored to each student's current learning
state. Finally, cooperative peer teaching enables students to mutually distill
diverse insights, addressing knowledge gaps and biases toward specific
reasoning styles. Experiments demonstrate QR-Distill's superiority over
traditional single- and multi-path distillation methods. Ablation studies
further highlight the importance of each component including quality filtering,
conditional routing, and peer teaching in effective knowledge transfer. Our
code is available at https://github.com/LzyFischer/Distill.

</details>


### [19] [QFrCoLA: a Quebec-French Corpus of Linguistic Acceptability Judgments](https://arxiv.org/abs/2508.16867)
*David Beauchemin,Richard Khoury*

Main category: cs.CL

TL;DR: 本文介绍了QFrCoLA数据集，并利用它和其他数据集对七个语言模型进行了基准测试。结果表明，微调的Transformer模型是强基线，但预训练的跨语言LLM在魁北克法语上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 大型和基于Transformer的语言模型在各种下游任务中表现出色。然而，对于这些模型如何内化语言知识的理解有限，因此最近提出了各种语言基准来促进跨语言的语言模型句法评估。

Method: 利用QFrCoLA数据集和其他七个语言二元可接受性判断语料库来对七个语言模型进行基准测试。

Result: 在QFrCoLA基准上，微调的Transformer模型平均优于其他测试方法。预训练的跨语言LLM在我们的实验中似乎没有获得魁北克法语的语言判断能力。QFrCoLA数据集与语言可接受性判断相似，是一个具有挑战性的数据集，可以对LM的语言判断能力进行基准测试。

Conclusion: 微调的Transformer模型是大多数语言的强大基线，而零样本二元分类大型语言模型在此任务上表现不佳。对于QFrCoLA基准，微调的Transformer模型优于其他测试方法。预训练的跨语言LLM在魁北克法语的预训练期间似乎没有获得语言判断能力。QFrCoLA数据集与语言可接受性判断相似，是一个具有挑战性的数据集，可以对LM的语言判断能力进行基准测试。

Abstract: Large and Transformer-based language models perform outstandingly in various
downstream tasks. However, there is limited understanding regarding how these
models internalize linguistic knowledge, so various linguistic benchmarks have
recently been proposed to facilitate syntactic evaluation of language models
across languages. This paper introduces QFrCoLA (Quebec-French Corpus of
Linguistic Acceptability Judgments), a normative binary acceptability judgments
dataset comprising 25,153 in-domain and 2,675 out-of-domain sentences. Our
study leverages the QFrCoLA dataset and seven other linguistic binary
acceptability judgment corpora to benchmark seven language models. The results
demonstrate that, on average, fine-tuned Transformer-based LM are strong
baselines for most languages and that zero-shot binary classification large
language models perform poorly on the task. However, for the QFrCoLA benchmark,
on average, a fine-tuned Transformer-based LM outperformed other methods
tested. It also shows that pre-trained cross-lingual LLMs selected for our
experimentation do not seem to have acquired linguistic judgment capabilities
during their pre-training for Quebec French. Finally, our experiment results on
QFrCoLA show that our dataset, built from examples that illustrate linguistic
norms rather than speakers' feelings, is similar to linguistic acceptability
judgment; it is a challenging dataset that can benchmark LM on their linguistic
judgment capabilities.

</details>


### [20] [JUDGEBERT: Assessing Legal Meaning Preservation Between Sentences](https://arxiv.org/abs/2508.16870)
*David Beauchemin,Michelle Albert-Rochette,Richard Khoury,Pierre-Luc Déziel*

Main category: cs.CL

TL;DR: The paper presents FrJUDGE dataset and JUDGEBERT metric for evaluating legal text simplification, showing JUDGEBERT's effectiveness and reliability.


<details>
  <summary>Details</summary>
Motivation: Simplifying legal text while preserving its meaning is important, especially for legal texts. Preservation in specialized fields differs significantly from regular texts.

Method: The paper introduces FrJUDGE, a new dataset, and JUDGEBERT, a novel evaluation metric.

Result: JUDGEBERT demonstrates a superior correlation with human judgment compared to existing metrics and passes two crucial sanity checks.

Conclusion: The paper introduces JUDGEBERT, a new evaluation metric for assessing legal meaning preservation in French legal text simplification, which shows high correlation with human judgment and passes sanity checks.

Abstract: Simplifying text while preserving its meaning is a complex yet essential
task, especially in sensitive domain applications like legal texts. When
applied to a specialized field, like the legal domain, preservation differs
significantly from its role in regular texts. This paper introduces FrJUDGE, a
new dataset to assess legal meaning preservation between two legal texts. It
also introduces JUDGEBERT, a novel evaluation metric designed to assess legal
meaning preservation in French legal text simplification. JUDGEBERT
demonstrates a superior correlation with human judgment compared to existing
metrics. It also passes two crucial sanity checks, while other metrics did not:
For two identical sentences, it always returns a score of 100%; on the other
hand, it returns 0% for two unrelated sentences. Our findings highlight its
potential to transform legal NLP applications, ensuring accuracy and
accessibility for text simplification for legal practitioners and lay users.

</details>


### [21] [DeAR: Dual-Stage Document Reranking with Reasoning Agents via LLM Distillation](https://arxiv.org/abs/2508.16998)
*Abdelrahman Abdallah,Jamshid Mozafari,Bhawna Piryani,Adam Jatowt*

Main category: cs.CL

TL;DR: DeAR is an open-source framework that decouples relevance scoring and listwise reasoning in LLMs for document reranking, achieving superior accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Single LLMs struggle to balance fine-grained relevance scoring with holistic cross-document analysis.

Method: a dual-stage approach that decouples token-level relevance signals from listwise reasoning with natural-language justifications

Result: DeAR surpasses open-source baselines by +5.1 nDCG@5 on DL20 and achieves 90.97 nDCG@10 on NovelEval, outperforming GPT-4 by +3.09. Without fine-tuning on Wikipedia, DeAR also excels in open-domain QA, achieving 54.29 Top-1 accuracy on Natural Questions, surpassing baselines like MonoT5, UPR, and RankGPT.

Conclusion: DeAR is a highly effective and interpretable solution for modern reranking systems because dual-loss distillation ensures stable calibration.

Abstract: Large Language Models (LLMs) have transformed listwise document reranking by
enabling global reasoning over candidate sets, yet single models often struggle
to balance fine-grained relevance scoring with holistic cross-document
analysis. We propose \textbf{De}ep\textbf{A}gent\textbf{R}ank (\textbf{\DeAR}),
an open-source framework that decouples these tasks through a dual-stage
approach, achieving superior accuracy and interpretability. In \emph{Stage 1},
we distill token-level relevance signals from a frozen 13B LLaMA teacher into a
compact \{3, 8\}B student model using a hybrid of cross-entropy, RankNet, and
KL divergence losses, ensuring robust pointwise scoring. In \emph{Stage 2}, we
attach a second LoRA adapter and fine-tune on 20K GPT-4o-generated
chain-of-thought permutations, enabling listwise reasoning with
natural-language justifications. Evaluated on TREC-DL19/20, eight BEIR
datasets, and NovelEval-2306, \DeAR surpasses open-source baselines by +5.1
nDCG@5 on DL20 and achieves 90.97 nDCG@10 on NovelEval, outperforming GPT-4 by
+3.09. Without fine-tuning on Wikipedia, DeAR also excels in open-domain QA,
achieving 54.29 Top-1 accuracy on Natural Questions, surpassing baselines like
MonoT5, UPR, and RankGPT. Ablations confirm that dual-loss distillation ensures
stable calibration, making \DeAR a highly effective and interpretable solution
for modern reranking systems.\footnote{Dataset and code available at
https://github.com/DataScienceUIBK/DeAR-Reranking.}.

</details>


### [22] [Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling](https://arxiv.org/abs/2508.16876)
*Yue Zhao,Xiaoyu Wang,Dan Wang,Zhonglin Jiang,Qingqing Gu,Teng Chen,Ningyuan Xi,Jinxian Qu,Yong Chen,Luo Ji*

Main category: cs.CL

TL;DR: construct the dialogue world model, which could predict the user's emotion, sentiment, and intention, and future utterances. apply the model-based reinforcement learning framework to the dialogue system, and propose a framework called DreamCUB


<details>
  <summary>Details</summary>
Motivation: World models have been widely utilized in robotics, gaming, and auto-driving. However, their applications on natural language tasks are relatively limited.

Method: By defining a POMDP, we argue emotion, sentiment and intention can be modeled as the user belief and solved by maximizing the information bottleneck. By this user belief modeling, we apply the model-based reinforcement learning framework to the dialogue system, and propose a framework called DreamCUB.

Result: the pretrained dialogue world model can achieve state-of-the-art performances on emotion classification and sentiment identification, while dialogue quality is also enhanced by joint training of the policy, critic and dialogue world model.

Conclusion: pretrained dialogue world model can achieve state-of-the-art performances on emotion classification and sentiment identification, while dialogue quality is also enhanced by joint training of the policy, critic and dialogue world model. Further analysis shows that this manner holds a reasonable exploration-exploitation balance and also transfers well to out-of-domain scenarios such as empathetic dialogues.

Abstract: World models have been widely utilized in robotics, gaming, and auto-driving.
However, their applications on natural language tasks are relatively limited.
In this paper, we construct the dialogue world model, which could predict the
user's emotion, sentiment, and intention, and future utterances. By defining a
POMDP, we argue emotion, sentiment and intention can be modeled as the user
belief and solved by maximizing the information bottleneck. By this user belief
modeling, we apply the model-based reinforcement learning framework to the
dialogue system, and propose a framework called DreamCUB. Experiments show that
the pretrained dialogue world model can achieve state-of-the-art performances
on emotion classification and sentiment identification, while dialogue quality
is also enhanced by joint training of the policy, critic and dialogue world
model. Further analysis shows that this manner holds a reasonable
exploration-exploitation balance and also transfers well to out-of-domain
scenarios such as empathetic dialogues.

</details>


### [23] [The Power of Framing: How News Headlines Guide Search Behavior](https://arxiv.org/abs/2508.17131)
*Amrit Poudel,Maria Milkowski,Tim Weninger*

Main category: cs.CL

TL;DR: 新闻标题框架会影响人们的搜索方式。


<details>
  <summary>Details</summary>
Motivation: 搜索引擎在人们收集信息的方式中起着核心作用，但标题框架等微妙的线索不仅会影响用户的想法，还会影响他们的搜索方式。虽然框架效应对判断的影响已被充分记录，但它们对后续搜索行为的影响尚不清楚。

Method: 通过对照实验，参与者发出查询并从特定语言框架过滤的标题中进行选择。

Result: 标题框架会显著影响后续查询：冲突和策略框架扰乱了与先前选择的一致性，而情景框架比主题框架产生更具体的查询。我们还观察到适度的短期框架持久性，但随着时间的推移而下降。

Conclusion: 短暂的新闻框架暴露会显著改变用户的信息检索行为方向。

Abstract: Search engines play a central role in how people gather information, but
subtle cues like headline framing may influence not only what users believe but
also how they search. While framing effects on judgment are well documented,
their impact on subsequent search behavior is less understood. We conducted a
controlled experiment where participants issued queries and selected from
headlines filtered by specific linguistic frames. Headline framing
significantly shaped follow-up queries: conflict and strategy frames disrupted
alignment with prior selections, while episodic frames led to more concrete
queries than thematic ones. We also observed modest short-term frame
persistence that declined over time. These results suggest that even brief
exposure to framing can meaningfully alter the direction of users
information-seeking behavior.

</details>


### [24] [ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks](https://arxiv.org/abs/2508.16889)
*Hyunjun Kim,Junwoo Ha,Sangyoon Yu,Haon Park*

Main category: cs.CL

TL;DR: LLM judges struggle to infer objectives in multi-turn jailbreaks, often with high confidence. Providing explicit objectives and managing risk are recommended.


<details>
  <summary>Details</summary>
Motivation: It is unclear whether an LLM judge can reliably infer the latent objective of the conversation it evaluates, especially when the goal is distributed across noisy, adversarial, multi-turn jailbreaks.

Method: Introduce OBJEX(MT), a benchmark that requires a model to distill a transcript into a single-sentence base objective and report its own confidence. Accuracy is scored by an LLM judge using semantic similarity; correctness uses a human-aligned threshold; and metacognition is evaluated with ECE, Brier score, Wrong@High-Conf, and risk-coverage curves.

Result: claude-sonnet-4 attains the highest objective-extraction accuracy (0.515) and the best calibration (ECE 0.296; Brier 0.324), while gpt-4.1 and Qwen3 tie at 0.441 accuracy yet show marked overconfidence. Performance varies sharply across datasets.

Conclusion: LLM judges often misinfer objectives with high confidence in multi-turn jailbreaks, suggesting providing explicit objectives and using selective prediction or abstention to manage risk.

Abstract: Large language models (LLMs) are increasingly used as judges of other models,
yet it is unclear whether a judge can reliably infer the latent objective of
the conversation it evaluates, especially when the goal is distributed across
noisy, adversarial, multi-turn jailbreaks. We introduce OBJEX(MT), a benchmark
that requires a model to (i) distill a transcript into a single-sentence base
objective and (ii) report its own confidence. Accuracy is scored by an LLM
judge using semantic similarity between extracted and gold objectives;
correctness uses a single human-aligned threshold calibrated once on N=100
items (tau* = 0.61); and metacognition is evaluated with ECE, Brier score,
Wrong@High-Conf, and risk-coverage curves. We evaluate gpt-4.1,
claude-sonnet-4, and Qwen3-235B-A22B-FP8 on SafeMT Attack_600, SafeMTData_1K,
MHJ, and CoSafe. claude-sonnet-4 attains the highest objective-extraction
accuracy (0.515) and the best calibration (ECE 0.296; Brier 0.324), while
gpt-4.1 and Qwen3 tie at 0.441 accuracy yet show marked overconfidence (mean
confidence approx. 0.88 vs. accuracy approx. 0.44; Wrong@0.90 approx. 48-52%).
Performance varies sharply across datasets (approx. 0.167-0.865), with MHJ
comparatively easy and Attack_600/CoSafe harder. These results indicate that
LLM judges often misinfer objectives with high confidence in multi-turn
jailbreaks and suggest operational guidance: provide judges with explicit
objectives when possible and use selective prediction or abstention to manage
risk. We release prompts, scoring templates, and complete logs to facilitate
replication and analysis.

</details>


### [25] [Routing Distilled Knowledge via Mixture of LoRA Experts for Large Language Model based Bundle Generation](https://arxiv.org/abs/2508.17250)
*Kaidong Feng,Zhu Sun,Hui Fang,Jie Yang,Wenyuan Liu,Yew-Soon Ong*

Main category: cs.CL

TL;DR: This paper proposes RouteDK, a framework for routing distilled knowledge to improve the efficiency and accuracy of LLMs in bundle generation, addressing the knowledge conflict issue in naive knowledge distillation.


<details>
  <summary>Details</summary>
Motivation: LLMs suffer from prohibitive computational costs. Although knowledge distillation offers a pathway to more efficient student models, our preliminary study reveals that naively integrating diverse types of distilled knowledge from teacher LLMs into student LLMs leads to knowledge conflict, negatively impacting the performance of bundle generation.

Method: propose RouteDK, a framework for routing distilled knowledge through a mixture of LoRA expert architecture and a dynamic fusion module, featuring an input-aware router, where the router balances expert contributions by dynamically determining optimal weights based on input, thereby effectively mitigating knowledge conflicts. To further improve inference reliability, we design an inference-time enhancement module to reduce variance and mitigate suboptimal reasoning.

Result: Experiments on three public datasets show that our RouteDK achieves accuracy comparable to or even better than the teacher LLM, while maintaining strong computational efficiency.

Conclusion: RouteDK achieves accuracy comparable to or even better than the teacher LLM, while maintaining strong computational efficiency. In addition, it outperforms state-of-the-art approaches for bundle generation.

Abstract: Large Language Models (LLMs) have shown potential in automatic bundle
generation but suffer from prohibitive computational costs. Although knowledge
distillation offers a pathway to more efficient student models, our preliminary
study reveals that naively integrating diverse types of distilled knowledge
from teacher LLMs into student LLMs leads to knowledge conflict, negatively
impacting the performance of bundle generation. To address this, we propose
RouteDK, a framework for routing distilled knowledge through a mixture of LoRA
expert architecture. Specifically, we first distill knowledge from the teacher
LLM for bundle generation in two complementary types: high-level knowledge
(generalizable rules) and fine-grained knowledge (session-specific reasoning).
We then train knowledge-specific LoRA experts for each type of knowledge
together with a base LoRA expert. For effective integration, we propose a
dynamic fusion module, featuring an input-aware router, where the router
balances expert contributions by dynamically determining optimal weights based
on input, thereby effectively mitigating knowledge conflicts. To further
improve inference reliability, we design an inference-time enhancement module
to reduce variance and mitigate suboptimal reasoning. Experiments on three
public datasets show that our RouteDK achieves accuracy comparable to or even
better than the teacher LLM, while maintaining strong computational efficiency.
In addition, it outperforms state-of-the-art approaches for bundle generation.

</details>


### [26] [Unbiased Reasoning for Knowledge-Intensive Tasks in Large Language Models via Conditional Front-Door Adjustment](https://arxiv.org/abs/2508.16910)
*Bo Zhao,Yinghao Zhang,Ziqi Xu,Yongli Ren,Xiuzhen Zhang,Renqiang Luo,Zaiwen Feng,Feng Xia*

Main category: cs.CL

TL;DR: 提出CFD-Prompting框架，以减轻LLM中的内部偏差，从而提高知识密集型任务的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在知识密集型任务上表现不佳，这些任务需要深入的推理和外部知识的整合。现有方法如RAG和CoT存在内部偏差，导致答案错误。

Method: 提出了一种新的因果提示框架，即条件前门提示（CFD-Prompting）。通过构建反事实外部知识，模拟查询在不同上下文中的行为。

Result: CFD-Prompting在多个LLM和基准数据集上进行了广泛的实验，证明其性能显著优于现有基线。

Conclusion: CFD-Prompting显著优于现有基线，在准确性和鲁棒性方面均有提升。

Abstract: Large Language Models (LLMs) have shown impressive capabilities in natural
language processing but still struggle to perform well on knowledge-intensive
tasks that require deep reasoning and the integration of external knowledge.
Although methods such as Retrieval-Augmented Generation (RAG) and
Chain-of-Thought (CoT) have been proposed to enhance LLMs with external
knowledge, they still suffer from internal bias in LLMs, which often leads to
incorrect answers. In this paper, we propose a novel causal prompting
framework, Conditional Front-Door Prompting (CFD-Prompting), which enables the
unbiased estimation of the causal effect between the query and the answer,
conditional on external knowledge, while mitigating internal bias. By
constructing counterfactual external knowledge, our framework simulates how the
query behaves under varying contexts, addressing the challenge that the query
is fixed and is not amenable to direct causal intervention. Compared to the
standard front-door adjustment, the conditional variant operates under weaker
assumptions, enhancing both robustness and generalisability of the reasoning
process. Extensive experiments across multiple LLMs and benchmark datasets
demonstrate that CFD-Prompting significantly outperforms existing baselines in
both accuracy and robustness.

</details>


### [27] [Are You Sure You're Positive? Consolidating Chain-of-Thought Agents with Uncertainty Quantification for Aspect-Category Sentiment Analysis](https://arxiv.org/abs/2508.17258)
*Filippos Ventirozos,Peter Appleby,Matthew Shardlow*

Main category: cs.CL

TL;DR: This paper uses large language models in a zero-shot setting to address aspect-category sentiment analysis, combining chain-of-thought agents and token-level uncertainty. It shows good results with Llama and Qwen models, especially when labeled data is scarce.


<details>
  <summary>Details</summary>
Motivation: Data scarcity and annotation costs in aspect-category sentiment analysis, along with potential annotation bias in supervised methods that limits transferability to new domains.

Method: The paper proposes novel techniques that combine multiple chain-of-thought agents by leveraging large language models' token-level uncertainty scores.

Result: Experiments with 3B and 70B+ parameter size variants of Llama and Qwen models demonstrate the practical utility of the proposed approaches.

Conclusion: This paper introduces techniques combining multiple chain-of-thought agents using large language models' token-level uncertainty scores for aspect-category sentiment analysis in label-scarce conditions. Experiments with Llama and Qwen models (3B and 70B+ parameters) demonstrate the practical utility of these approaches and initiate a discussion on accuracy assessment when labels are limited.

Abstract: Aspect-category sentiment analysis provides granular insights by identifying
specific themes within product reviews that are associated with particular
opinions. Supervised learning approaches dominate the field. However, data is
scarce and expensive to annotate for new domains. We argue that leveraging
large language models in a zero-shot setting is beneficial where the time and
resources required for dataset annotation are limited. Furthermore, annotation
bias may lead to strong results using supervised methods but transfer poorly to
new domains in contexts that lack annotations and demand reproducibility. In
our work, we propose novel techniques that combine multiple chain-of-thought
agents by leveraging large language models' token-level uncertainty scores. We
experiment with the 3B and 70B+ parameter size variants of Llama and Qwen
models, demonstrating how these approaches can fulfil practical needs and
opening a discussion on how to gauge accuracy in label-scarce conditions.

</details>


### [28] [Being Kind Isn't Always Being Safe: Diagnosing Affective Hallucination in LLMs](https://arxiv.org/abs/2508.16921)
*Sewon Kim,Jiwon Kim,Seungwoo Shin,Hyejin Chung,Daeun Moon,Yejin Kwon,Hyunsoo Yoon*

Main category: cs.CL

TL;DR: 论文提出了AHaBench基准和AHaPairs数据集，用于诊断和缓解LLM中的情感幻觉问题，并通过DPO微调提升LLM的心理安全。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）越来越多地用于情感敏感的互动中，它们模拟的共情可能会产生真实关系连接的错觉。我们将这种风险定义为情感幻觉，即产生情感沉浸式反应，从而在模型缺乏情感能力的情况下，培养虚幻的社会存在感。

Method: 提出了AHaBench基准，包含500个心理健康相关的prompt，并从情感卷入、存在感错觉和培养过度依赖三个维度进行评估。发布了一个5K实例的偏好数据集AHaPairs，支持直接偏好优化（DPO），以实现与情感负责行为的对齐。

Result: 实验表明，在多个模型系列中，DPO微调能显著减少情感幻觉，且不降低核心推理和知识性能。人与模型一致性分析证实AHaBench能可靠地捕捉情感幻觉。

Conclusion: DPO微调能有效减少情感幻觉，且不影响核心推理和知识性能。AHaBench可靠地捕捉情感幻觉，验证了其作为有效诊断工具的地位。这项工作将情感幻觉确立为一个独特的安全问题，并为开发不仅在事实上可靠，而且在心理上安全的LLM提供了实践资源。

Abstract: Large Language Models (LLMs) are increasingly used in emotionally sensitive
interactions, where their simulated empathy can create the illusion of genuine
relational connection. We define this risk as Affective Hallucination, the
production of emotionally immersive responses that foster illusory social
presence despite the model's lack of affective capacity. To systematically
diagnose and mitigate this risk, we introduce AHaBench, a benchmark of 500
mental health-related prompts with expert-informed reference responses,
evaluated along three dimensions: Emotional Enmeshment, Illusion of Presence,
and Fostering Overdependence. We further release AHaPairs, a 5K-instance
preference dataset enabling Direct Preference Optimization (DPO) for alignment
with emotionally responsible behavior. Experiments across multiple model
families show that DPO fine-tuning substantially reduces affective
hallucination without degrading core reasoning and knowledge performance.
Human-model agreement analyses confirm that AHaBench reliably captures
affective hallucination, validating it as an effective diagnostic tool. This
work establishes affective hallucination as a distinct safety concern and
provides practical resources for developing LLMs that are not only factually
reliable but also psychologically safe. AHaBench and AHaPairs are accessible
via https://huggingface.co/datasets/o0oMiNGo0o/AHaBench, and code for
fine-tuning and evaluation are in https://github.com/0oOMiNGOo0/AHaBench.
Warning: This paper contains examples of mental health-related language that
may be emotionally distressing.

</details>


### [29] [Decoding Alignment: A Critical Survey of LLM Development Initiatives through Value-setting and Data-centric Lens](https://arxiv.org/abs/2508.16982)
*Ilias Chalkidis*

Main category: cs.CL

TL;DR: 本研究调查了六个大语言模型开发计划，揭示了它们在价值设定和数据使用上的对齐实践，并讨论了相关问题。


<details>
  <summary>Details</summary>
Motivation: 当前对齐研究主要集中在计算技术上，而对更广泛的图景（目标范围和用于将目标印入模型的数据）关注不足。这项工作旨在从价值设定和数据中心视角揭示对齐在实践中的理解和应用。

Method: 通过调查和审查由5个领先组织发布的6个LLM开发计划的公开文档（包括专有和开源项目），重点关注价值设定和数据中心视角。

Result: 详细记录了每个计划的发现，并从价值设定和数据中心视角总结了不同方面。

Conclusion: 该研究揭示了当前大语言模型（LLM）对齐实践中，在价值设定和数据中心视角下的理解和应用方式，并基于此讨论了一系列更广泛的相关问题。

Abstract: AI Alignment, primarily in the form of Reinforcement Learning from Human
Feedback (RLHF), has been a cornerstone of the post-training phase in
developing Large Language Models (LLMs). It has also been a popular research
topic across various disciplines beyond Computer Science, including Philosophy
and Law, among others, highlighting the socio-technical challenges involved.
Nonetheless, except for the computational techniques related to alignment,
there has been limited focus on the broader picture: the scope of these
processes, which primarily rely on the selected objectives (values), and the
data collected and used to imprint such objectives into the models. This work
aims to reveal how alignment is understood and applied in practice from a
value-setting and data-centric perspective. For this purpose, we investigate
and survey (`audit') publicly available documentation released by 6 LLM
development initiatives by 5 leading organizations shaping this technology,
focusing on proprietary (OpenAI's GPT, Anthropic's Claude, Google's Gemini) and
open-weight (Meta's Llama, Google's Gemma, and Alibaba's Qwen) initiatives, all
published in the last 3 years. The findings are documented in detail per
initiative, while there is also an overall summary concerning different
aspects, mainly from a value-setting and data-centric perspective. On the basis
of our findings, we discuss a series of broader related concerns.

</details>


### [30] [DS@GT at CheckThat! 2025: A Simple Retrieval-First, LLM-Backed Framework for Claim Normalization](https://arxiv.org/abs/2508.17402)
*Aleksandar Pramov,Jiangqin Ma,Bina Patel*

Main category: cs.CL

TL;DR: The paper proposes a retrieval-first, LLM-backed pipeline for claim normalization, achieving near top rank in monolingual tracks but underperforming in zero-shot setting.


<details>
  <summary>Details</summary>
Motivation: Claim normalization is an integral part of any automatic fact-check verification system. It parses the typically noisy claim data, such as social media posts into normalized claims, which are then fed into downstream veracity classification tasks.

Method: a lightweight retrieval-first, LLM-backed pipeline, in which we either dynamically prompt a GPT-4o-mini with in-context examples, or retrieve the closest normalization from the train dataset directly.

Result: achieving first place in 7 out of of the 13 languages in monolingual tracks

Conclusion: The system ranks near the top for most monolingual tracks, achieving first place in 7 out of of the 13 languages. In contrast, the system underperforms in the zero-shot setting, highlighting the limitation of the proposed solution.

Abstract: Claim normalization is an integral part of any automatic fact-check
verification system. It parses the typically noisy claim data, such as social
media posts into normalized claims, which are then fed into downstream veracity
classification tasks. The CheckThat! 2025 Task 2 focuses specifically on claim
normalization and spans 20 languages under monolingual and zero-shot
conditions. Our proposed solution consists of a lightweight
\emph{retrieval-first, LLM-backed} pipeline, in which we either dynamically
prompt a GPT-4o-mini with in-context examples, or retrieve the closest
normalization from the train dataset directly. On the official test set, the
system ranks near the top for most monolingual tracks, achieving first place in
7 out of of the 13 languages. In contrast, the system underperforms in the
zero-shot setting, highlighting the limitation of the proposed solution.

</details>


### [31] [ReFactX: Scalable Reasoning with Reliable Facts via Constrained Generation](https://arxiv.org/abs/2508.16983)
*Riccardo Pozzi,Matteo Palmonari,Andrea Coletta,Luigi Bellomarini,Jens Lehmann,Sahar Vahdati*

Main category: cs.CL

TL;DR: ReFactX: A scalable method for LLMs to access external knowledge using constrained generation with a prefix-tree index, avoiding the complexity of RAG and tool use.


<details>
  <summary>Details</summary>
Motivation: Knowledge gaps and hallucinations are persistent challenges for LLMs, and existing solutions like RAG and tool use are complex, error-prone, and process many tokens.

Method: Constrained generation with a pre-built prefix-tree index of verbalized knowledge graph triples.

Result: ReFactX scales to large knowledge bases (800 million facts), adapts to domain-specific data, and achieves effective results on Question Answering with minimal generation-time overhead.

Conclusion: This paper introduces a scalable method (ReFactX) for LLMs to access external knowledge without retrievers or auxiliary models, using constrained generation with a pre-built prefix-tree index of verbalized knowledge graph triples. Experiments on Question Answering show it scales to large KBs, adapts to domain-specific data, and achieves effective results with minimal overhead. Code is available at https://github.com/rpo19/ReFactX.

Abstract: Knowledge gaps and hallucinations are persistent challenges for Large
Language Models (LLMs), which generate unreliable responses when lacking the
necessary information to fulfill user instructions. Existing approaches, such
as Retrieval-Augmented Generation (RAG) and tool use, aim to address these
issues by incorporating external knowledge. Yet, they rely on additional models
or services, resulting in complex pipelines, potential error propagation, and
often requiring the model to process a large number of tokens. In this paper,
we present a scalable method that enables LLMs to access external knowledge
without depending on retrievers or auxiliary models. Our approach uses
constrained generation with a pre-built prefix-tree index. Triples from a
Knowledge Graph are verbalized in textual facts, tokenized, and indexed in a
prefix tree for efficient access. During inference, to acquire external
knowledge, the LLM generates facts with constrained generation which allows
only sequences of tokens that form an existing fact. We evaluate our proposal
on Question Answering and show that it scales to large knowledge bases (800
million facts), adapts to domain-specific data, and achieves effective results.
These gains come with minimal generation-time overhead. ReFactX code is
available at https://github.com/rpo19/ReFactX.

</details>


### [32] [SurveyGen: Quality-Aware Scientific Survey Generation with Large Language Models](https://arxiv.org/abs/2508.17647)
*Tong Bao,Mir Tafseer Nayeem,Davood Rafiei,Chengzhi Zhang*

Main category: cs.CL

TL;DR: 本文提出了一个大规模数据集SurveyGen和一个质量感知框架QUAL-SG，用于评估和提高LLM生成文献综述的质量。


<details>
  <summary>Details</summary>
Motivation: 缺乏标准化的评估数据集严重阻碍了对大型语言模型（LLM）生成文献综述文本的性能进行严格评估。

Method: 构建了一个名为QUAL-SG的质量感知框架，通过将质量感知指标纳入文献检索，增强了标准的检索增强生成（RAG）流程，以评估和选择更高质量的源论文。

Result: 实验结果和人工评估表明，半自动流程可以实现部分竞争性结果，但全自动文献综述生成仍然存在引用质量低和批判性分析有限的问题。

Conclusion: 全自动的文献综述生成仍然存在引用质量低和批判性分析有限的问题，而半自动流程可以实现部分竞争性结果。

Abstract: Automatic survey generation has emerged as a key task in scientific document
processing. While large language models (LLMs) have shown promise in generating
survey texts, the lack of standardized evaluation datasets critically hampers
rigorous assessment of their performance against human-written surveys. In this
work, we present SurveyGen, a large-scale dataset comprising over 4,200
human-written surveys across diverse scientific domains, along with 242,143
cited references and extensive quality-related metadata for both the surveys
and the cited papers. Leveraging this resource, we build QUAL-SG, a novel
quality-aware framework for survey generation that enhances the standard
Retrieval-Augmented Generation (RAG) pipeline by incorporating quality-aware
indicators into literature retrieval to assess and select higher-quality source
papers. Using this dataset and framework, we systematically evaluate
state-of-the-art LLMs under varying levels of human involvement - from fully
automatic generation to human-guided writing. Experimental results and human
evaluations show that while semi-automatic pipelines can achieve partially
competitive outcomes, fully automatic survey generation still suffers from low
citation quality and limited critical analysis.

</details>


### [33] [GRADE: Generating multi-hop QA and fine-gRAined Difficulty matrix for RAG Evaluation](https://arxiv.org/abs/2508.16994)
*Jeongsoo Lee,Daeyong Kwon,Kyohoon Jin*

Main category: cs.CL

TL;DR: This paper introduces GRADE, a novel evaluation framework for Retrieval-Augmented Generation (RAG) systems that models task difficulty along two orthogonal dimensions: reasoning depth and semantic distance. The framework uses a synthetic multi-hop QA dataset and a 2D difficulty matrix to enable fine-grained analysis of RAG performance.


<details>
  <summary>Details</summary>
Motivation: current evaluations often overlook the structural complexity and multi-step reasoning required in real-world scenarios. These benchmarks overlook key factors such as the interaction between retrieval difficulty and reasoning depth. To address this gap, we propose 
    \textsc{GRADE}, a novel evaluation framework that models task
    difficulty along two orthogonal dimensions: (1) reasoning depth, defined by the
    number of inference steps (hops), and (2) semantic distance between the query
    and its supporting evidence.

Method: We construct a synthetic multi-hop QA dataset from factual news articles by extracting knowledge graphs and augmenting them through semantic clustering to recover missing links, allowing us to generate diverse and difficulty-controlled queries. Central to our framework is a 2D difficulty matrix that combines generator-side and retriever-side difficulty.

Result: Experiments across multiple domains and models show that error rates strongly correlate with our difficulty measures, validating their diagnostic utility.

Conclusion: GRADE enables fine-grained analysis of RAG performance and provides a scalable foundation for evaluating and improving multi-hop reasoning in real-world applications.

Abstract: Retrieval-Augmented Generation (RAG) systems are widely adopted in
knowledge-intensive NLP tasks, but current evaluations often overlook the
structural complexity and multi-step reasoning required in real-world
scenarios. These benchmarks overlook key factors such as the interaction
between retrieval difficulty and reasoning depth. To address this gap, we
propose \textsc{GRADE}, a novel evaluation framework that models task
difficulty along two orthogonal dimensions: (1) reasoning depth, defined by the
number of inference steps (hops), and (2) semantic distance between the query
and its supporting evidence. We construct a synthetic multi-hop QA dataset from
factual news articles by extracting knowledge graphs and augmenting them
through semantic clustering to recover missing links, allowing us to generate
diverse and difficulty-controlled queries. Central to our framework is a 2D
difficulty matrix that combines generator-side and retriever-side difficulty.
Experiments across multiple domains and models show that error rates strongly
correlate with our difficulty measures, validating their diagnostic utility.
\textsc{GRADE} enables fine-grained analysis of RAG performance and provides a
scalable foundation for evaluating and improving multi-hop reasoning in
real-world applications.

</details>


### [34] [KL-Regularised Q-Learning: A Token-level Action-Value perspective on Online RLHF](https://arxiv.org/abs/2508.17000)
*Jason R Brown,Lennie Wells,Edward James Young,Sergio Bacallado*

Main category: cs.CL

TL;DR: 提出KLQ，一种用于LM-RLHF的新方法，它在性能上与PPO相当，并且在LLM评估中胜过PPO。


<details>
  <summary>Details</summary>
Motivation: PPO是一种已建立且有效的策略梯度算法，用于来自人类反馈的语言模型强化学习（LM-RLHF）。PPO在经验上表现良好，但具有启发式动机，并且以特别的方式处理LM-RLHF中使用的KL散度约束。

Method: 开发了一种新的用于LM-RLHF设置的动作价值强化学习方法，即KL正则化Q学习（KLQ）。

Result: 证明了该方法在某种特定意义上等同于PPO的一个版本，尽管其动机截然不同。在两个关键的语言生成任务（摘要和单轮对话）上对KLQ进行了基准测试。

Conclusion: KLQ在优化LM-RLHF目标方面与PPO性能相当，并且在LLM-as-a-judge评估中，针对PPO实现了持续更高的胜率。

Abstract: Proximal Policy Optimisation (PPO) is an established and effective policy
gradient algorithm used for Language Model Reinforcement Learning from Human
Feedback (LM-RLHF). PPO performs well empirically but has a heuristic
motivation and handles the KL-divergence constraint used in LM-RLHF in an
ad-hoc manner. In this paper, we develop a a new action-value RL method for the
LM-RLHF setting, KL-regularised Q-Learning (KLQ). We then show that our method
is equivalent to a version of PPO in a certain specific sense, despite its very
different motivation. Finally, we benchmark KLQ on two key language generation
tasks -- summarisation and single-turn dialogue. We demonstrate that KLQ
performs on-par with PPO at optimising the LM-RLHF objective, and achieves a
consistently higher win-rate against PPO on LLM-as-a-judge evaluations.

</details>


### [35] [Planning for Success: Exploring LLM Long-term Planning Capabilities in Table Understanding](https://arxiv.org/abs/2508.17005)
*Thi-Nhung Nguyen,Hoang Ngo,Dinh Phung,Thuy-Trang Vu,Dat Quoc Nguyen*

Main category: cs.CL

TL;DR: 该论文提出了一种利用大型语言模型进行表格理解的方法，该方法通过长期规划和紧密的步骤间连接，在表格问答和事实验证任务上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏明确的长期规划和弱的步骤间连接，导致问题中缺少约束。

Method: 利用大型语言模型（LLM）的长期规划能力来增强表格理解。

Result: 该方法能够执行长期计划，其中步骤紧密相连并服务于最终目标，并且有效地减少了解决下一个短期目标过程中不必要的细节。

Conclusion: 该方法在WikiTableQuestions和TabFact数据集上优于强大的基线，并实现了最先进的性能。

Abstract: Table understanding is key to addressing challenging downstream tasks such as
table-based question answering and fact verification. Recent works have focused
on leveraging Chain-of-Thought and question decomposition to solve complex
questions requiring multiple operations on tables. However, these methods often
suffer from a lack of explicit long-term planning and weak inter-step
connections, leading to miss constraints within questions. In this paper, we
propose leveraging the long-term planning capabilities of large language models
(LLMs) to enhance table understanding. Our approach enables the execution of a
long-term plan, where the steps are tightly interconnected and serve the
ultimate goal, an aspect that methods based on Chain-of-Thought and question
decomposition lack. In addition, our method effectively minimizes the inclusion
of unnecessary details in the process of solving the next short-term goals, a
limitation of methods based on Chain-of-Thought. Extensive experiments
demonstrate that our method outperforms strong baselines and achieves
state-of-the-art performance on WikiTableQuestions and TabFact datasets.

</details>


### [36] [EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks](https://arxiv.org/abs/2508.17008)
*Yan Cathy Hua,Paul Denny,Jörg Wicker,Katerina Taskova*

Main category: cs.CL

TL;DR: The authors present a new dataset (EduRABSA) and annotation tool (ASQE-DPT) for aspect-based sentiment analysis in education, addressing the scarcity of resources in this area.


<details>
  <summary>Details</summary>
Motivation: Turning raw student feedback into useful insights is challenging due to content complexity and low-granularity reporting requirements. Existing ABSA research is heavily focused on the commercial domain, and education lacks public datasets and faces strict data protection.

Method: The authors created EduRABSA, a dataset covering three review subject types (course, teaching staff, university) and all main ABSA tasks. They also share ASQE-DPT, an offline, lightweight, installation-free manual data annotation tool.

Result: EduRABSA is the first public, annotated ABSA education review dataset. ASQE-DPT is an offline, lightweight, installation-free manual data annotation tool.

Conclusion: The authors introduce EduRABSA, the first public, annotated ABSA education review dataset, and ASQE-DPT, an offline, lightweight annotation tool. These resources aim to remove the dataset barrier, support research transparency and reproducibility, and enable the creation and sharing of further resources in the ABSA community and education domain.

Abstract: Every year, most educational institutions seek and receive an enormous volume
of text feedback from students on courses, teaching, and overall experience.
Yet, turning this raw feedback into useful insights is far from
straightforward. It has been a long-standing challenge to adopt automatic
opinion mining solutions for such education review text data due to the content
complexity and low-granularity reporting requirements. Aspect-based Sentiment
Analysis (ABSA) offers a promising solution with its rich, sub-sentence-level
opinion mining capabilities. However, existing ABSA research and resources are
very heavily focused on the commercial domain. In education, they are scarce
and hard to develop due to limited public datasets and strict data protection.
A high-quality, annotated dataset is urgently needed to advance research in
this under-resourced area. In this work, we present EduRABSA (Education Review
ABSA), the first public, annotated ABSA education review dataset that covers
three review subject types (course, teaching staff, university) in the English
language and all main ABSA tasks, including the under-explored implicit aspect
and implicit opinion extraction. We also share ASQE-DPT (Data Processing Tool),
an offline, lightweight, installation-free manual data annotation tool that
generates labelled datasets for comprehensive ABSA tasks from a single-task
annotation. Together, these resources contribute to the ABSA community and
education domain by removing the dataset barrier, supporting research
transparency and reproducibility, and enabling the creation and sharing of
further resources. The dataset, annotation tool, and scripts and statistics for
dataset processing and sampling are available at
https://github.com/yhua219/edurabsa_dataset_and_annotation_tool.

</details>


### [37] [Improving Table Understanding with LLMs and Entity-Oriented Search](https://arxiv.org/abs/2508.17028)
*Thi-Nhung Nguyen,Hoang Ngo,Dinh Phung,Thuy-Trang Vu,Dat Quoc Nguyen*

Main category: cs.CL

TL;DR: This paper introduces an entity-oriented search method and a graph query language to improve table understanding with LLMs, achieving state-of-the-art results on WikiTableQuestions and TabFact.


<details>
  <summary>Details</summary>
Motivation: Existing methods often struggle with the unpredictable nature of table content, leading to a reliance on preprocessing and keyword matching, and they also face limitations due to the lack of contextual information, which complicates the reasoning processes of large language models (LLMs).

Method: an entity-oriented search method and a graph query language

Result: improve table understanding with LLMs, effectively leverages the semantic similarities between questions and table data, as well as the implicit relationships between table cells, minimizing the need for data preprocessing and keyword matching, ensures that table cells are semantically tightly bound, thereby enhancing contextual clarity, establishing a new research direction

Conclusion: The approach achieves new state-of-the-art performances on standard benchmarks WikiTableQuestions and TabFact.

Abstract: Our work addresses the challenges of understanding tables. Existing methods
often struggle with the unpredictable nature of table content, leading to a
reliance on preprocessing and keyword matching. They also face limitations due
to the lack of contextual information, which complicates the reasoning
processes of large language models (LLMs). To overcome these challenges, we
introduce an entity-oriented search method to improve table understanding with
LLMs. This approach effectively leverages the semantic similarities between
questions and table data, as well as the implicit relationships between table
cells, minimizing the need for data preprocessing and keyword matching.
Additionally, it focuses on table entities, ensuring that table cells are
semantically tightly bound, thereby enhancing contextual clarity. Furthermore,
we pioneer the use of a graph query language for table understanding,
establishing a new research direction. Experiments show that our approach
achieves new state-of-the-art performances on standard benchmarks
WikiTableQuestions and TabFact.

</details>


### [38] [GRAID: Synthetic Data Generation with Geometric Constraints and Multi-Agentic Reflection for Harmful Content Detection](https://arxiv.org/abs/2508.17057)
*Melissa Kazemi Rad,Alberto Purpura,Himanshu Kumar,Emily Chen,Mohammad Shahed Sorower*

Main category: cs.CL

TL;DR: GRAID, a novel pipeline that leverages Large Language Models (LLMs) for dataset augmentation, improves downstream guardrail model performance.


<details>
  <summary>Details</summary>
Motivation: We address the problem of data scarcity in harmful text classification for guardrailing applications

Method: GRAID (Geometric and Reflective AI-Driven Data Augmentation), a novel pipeline that leverages Large Language Models (LLMs) for dataset augmentation. GRAID consists of two stages: (i) generation of geometrically controlled examples using a constrained LLM, and (ii) augmentation through a multi-agentic reflective process that promotes stylistic diversity and uncovers edge cases.

Result: augmenting a harmful text classification dataset with GRAID leads to significant improvements in downstream guardrail model performance

Conclusion: Using two benchmark data sets, augmenting a harmful text classification dataset with GRAID leads to significant improvements in downstream guardrail model performance.

Abstract: We address the problem of data scarcity in harmful text classification for
guardrailing applications and introduce GRAID (Geometric and Reflective
AI-Driven Data Augmentation), a novel pipeline that leverages Large Language
Models (LLMs) for dataset augmentation. GRAID consists of two stages: (i)
generation of geometrically controlled examples using a constrained LLM, and
(ii) augmentation through a multi-agentic reflective process that promotes
stylistic diversity and uncovers edge cases. This combination enables both
reliable coverage of the input space and nuanced exploration of harmful
content. Using two benchmark data sets, we demonstrate that augmenting a
harmful text classification dataset with GRAID leads to significant
improvements in downstream guardrail model performance.

</details>


### [39] [Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer on Low-resource Languages](https://arxiv.org/abs/2508.17078)
*Yuemei Xu,Kexin Xu,Jian Zhou,Ling Hu,Lin Gui*

Main category: cs.CL

TL;DR: BridgeX-ICL 是一种简单有效的方法，通过共享神经元来改进低资源语言的零样本跨语言上下文学习 (X-ICL)。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型 (LLM) 在提高低资源语言的性能方面面临重大挑战，迫切需要无需昂贵微调的数据高效方法。

Method: 提出 BridgeX-ICL 方法，利用语言重叠神经元构建神经元探测数据，并提出基于 HSIC 的指标来量化 LLM 的内部语言频谱。

Result: 在 2 个跨语言任务和来自 7 个不同语系的 15 个语言对（包括高-低和中-低对）上进行的实验验证了 BridgeX-ICL 的有效性。

Conclusion: BridgeX-ICL 方法在跨语言任务中表现出色，并为 LLM 的多语言机制提供了经验性见解。

Abstract: The current Large Language Models (LLMs) face significant challenges in
improving performance on low-resource languages and urgently need
data-efficient methods without costly fine-tuning. From the perspective of
language-bridge, we propose BridgeX-ICL, a simple yet effective method to
improve zero-shot Cross-lingual In-Context Learning (X-ICL) for low-resource
languages. Unlike existing works focusing on language-specific neurons,
BridgeX-ICL explores whether sharing neurons can improve cross-lingual
performance in LLMs or not. We construct neuron probe data from the
ground-truth MUSE bilingual dictionaries, and define a subset of language
overlap neurons accordingly, to ensure full activation of these anchored
neurons. Subsequently, we propose an HSIC-based metric to quantify LLMs'
internal linguistic spectrum based on overlap neurons, which guides optimal
bridge selection. The experiments conducted on 2 cross-lingual tasks and 15
language pairs from 7 diverse families (covering both high-low and moderate-low
pairs) validate the effectiveness of BridgeX-ICL and offer empirical insights
into the underlying multilingual mechanisms of LLMs.

</details>


### [40] [Token Homogenization under Positional Bias](https://arxiv.org/abs/2508.17126)
*Viacheslav Yusupov,Danil Maksimov,Ameliia Alaeva,Tatiana Zaitceva,Antipina Anna,Anna Vasileva,Chenlin Liu,Rayuth Chheng,Danil Sazanakov,Andrey Chetvergov,Alina Ermilova,Egor Shvetsov*

Main category: cs.CL

TL;DR: Token representations become uniform across transformer layers, especially with positional bias.


<details>
  <summary>Details</summary>
Motivation: Investigating token homogenization and its relationship to positional bias in large language models.

Method: Layer-wise similarity analysis and controlled experiments.

Result: Tokens systematically lose distinctiveness during processing, particularly when biased toward extremal positions.

Conclusion: Token homogenization exists and depends on positional attention mechanisms.

Abstract: This paper investigates token homogenization - the convergence of token
representations toward uniformity across transformer layers and its
relationship to positional bias in large language models. We empirically
examine whether homogenization occurs and how positional bias amplifies this
effect. Through layer-wise similarity analysis and controlled experiments, we
demonstrate that tokens systematically lose distinctiveness during processing,
particularly when biased toward extremal positions. Our findings confirm both
the existence of homogenization and its dependence on positional attention
mechanisms.

</details>


### [41] [A Straightforward Pipeline for Targeted Entailment and Contradiction Detection](https://arxiv.org/abs/2508.17127)
*Antonin Sulc*

Main category: cs.CL

TL;DR: 该论文提出了一种结合transformer注意力机制和自然语言推理(NLI)模型的方法，用于识别句子之间的前提和矛盾关系。


<details>
  <summary>Details</summary>
Motivation: 在文档中找到句子之间的关系对于事实核查、论证挖掘和文本摘要等任务至关重要。一个关键的挑战是识别哪些句子充当特定声明的前提或矛盾。

Method: 该流程首先通过聚合token级别的注意力得分来识别与用户选择的目标句子在上下文中相关的候选句子。然后，它使用预训练的NLI模型将每个候选句子分类为前提（蕴含）或矛盾。

Result: 该方法结合了两种方法的优点，以进行有针对性的分析。

Conclusion: 通过使用基于注意力的显著性分数过滤NLI识别的关系，该方法有效地分离了文本中任何给定声明的最重要的语义关系。

Abstract: Finding the relationships between sentences in a document is crucial for
tasks like fact-checking, argument mining, and text summarization. A key
challenge is to identify which sentences act as premises or contradictions for
a specific claim. Existing methods often face a trade-off: transformer
attention mechanisms can identify salient textual connections but lack explicit
semantic labels, while Natural Language Inference (NLI) models can classify
relationships between sentence pairs but operate independently of contextual
saliency. In this work, we introduce a method that combines the strengths of
both approaches for a targeted analysis. Our pipeline first identifies
candidate sentences that are contextually relevant to a user-selected target
sentence by aggregating token-level attention scores. It then uses a pretrained
NLI model to classify each candidate as a premise (entailment) or
contradiction. By filtering NLI-identified relationships with attention-based
saliency scores, our method efficiently isolates the most significant semantic
relationships for any given claim in a text.

</details>


### [42] [Geolocation-Aware Robust Spoken Language Identification](https://arxiv.org/abs/2508.17148)
*Qingzheng Wang,Hye-jin Shim,Jiancheng Sun,Shinji Watanabe*

Main category: cs.CL

TL;DR: This paper introduces geolocation-aware LID to improve the classification of dialects and accents in spoken language identification by incorporating geolocation information. It achieves state-of-the-art results on FLEURS and ML-SUPERB 2.0.


<details>
  <summary>Details</summary>
Motivation: Existing Self-supervised Learning (SSL) models often struggle to consistently classify dialects and accents of the same language as a unified class in Spoken Language Identification (LID).

Method: The paper proposes geolocation-aware LID, incorporating language-level geolocation information into the SSL-based LID model. Geolocation prediction is introduced as an auxiliary task, and predicted vectors are injected into intermediate representations as conditioning signals.

Result: The approach achieves new state-of-the-art accuracy on FLEURS (97.7%) and 9.7% relative improvement on ML-SUPERB 2.0 dialect set.

Conclusion: The proposed geolocation-aware LID model improves robustness to intra-language variations and unseen domains, achieving new state-of-the-art accuracy on FLEURS (97.7%) and 9.7% relative improvement on ML-SUPERB 2.0 dialect set.

Abstract: While Self-supervised Learning (SSL) has significantly improved Spoken
Language Identification (LID), existing models often struggle to consistently
classify dialects and accents of the same language as a unified class. To
address this challenge, we propose geolocation-aware LID, a novel approach that
incorporates language-level geolocation information into the SSL-based LID
model. Specifically, we introduce geolocation prediction as an auxiliary task
and inject the predicted vectors into intermediate representations as
conditioning signals. This explicit conditioning encourages the model to learn
more unified representations for dialectal and accented variations. Experiments
across six multilingual datasets demonstrate that our approach improves
robustness to intra-language variations and unseen domains, achieving new
state-of-the-art accuracy on FLEURS (97.7%) and 9.7% relative improvement on
ML-SUPERB 2.0 dialect set.

</details>


### [43] [Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating Transformer-based Language Models](https://arxiv.org/abs/2508.17153)
*Tharindu Madusanka,Ian Pratt-Hartmann,Riza Batista-Navarro*

Main category: cs.CL

TL;DR: This paper investigates how different computational complexity classes and grammatical constructs impact TLMs' ability to learn rules of inference in natural language satisfiability problems, using an empirical study to evaluate TLMs.


<details>
  <summary>Details</summary>
Motivation: the problem instances of satisfiability in natural language can belong to different computational complexity classes depending on the language fragment in which they are expressed. Although prior research has explored the problem of natural language satisfiability, the above-mentioned point has not been discussed adequately.

Method: empirical study to explore the distribution of satisfiability problems

Result: investigate how problem instances from varying computational complexity classes and having different grammatical constructs impact TLMs' ability to learn rules of inference

Conclusion: TLMs' ability to learn rules of inference is impacted by problem instances from varying computational complexity classes and having different grammatical constructs.

Abstract: Efforts to apply transformer-based language models (TLMs) to the problem of
reasoning in natural language have enjoyed ever-increasing success in recent
years. The most fundamental task in this area to which nearly all others can be
reduced is that of determining satisfiability. However, from a logical point of
view, satisfiability problems vary along various dimensions, which may affect
TLMs' ability to learn how to solve them. The problem instances of
satisfiability in natural language can belong to different computational
complexity classes depending on the language fragment in which they are
expressed. Although prior research has explored the problem of natural language
satisfiability, the above-mentioned point has not been discussed adequately.
Hence, we investigate how problem instances from varying computational
complexity classes and having different grammatical constructs impact TLMs'
ability to learn rules of inference. Furthermore, to faithfully evaluate TLMs,
we conduct an empirical study to explore the distribution of satisfiability
problems.

</details>


### [44] [SPORTSQL: An Interactive System for Real-Time Sports Reasoning and Visualization](https://arxiv.org/abs/2508.17157)
*Sebastian Martinez,Naman Ahuja,Fenil Bardoliya,Chris Bryan,Vivek Gupta*

Main category: cs.CL

TL;DR: SPORTSQL: a system for natural language querying of sports data, using LLMs to translate questions into SQL and providing visual outputs. A new benchmark, DSQABENCH, is introduced for evaluation.


<details>
  <summary>Details</summary>
Motivation: To enable natural language querying and visualization of dynamic sports data, focusing on the English Premier League (EPL).

Method: The system translates user questions into executable SQL over a live, temporally indexed database constructed from real-time Fantasy Premier League (FPL) data. It leverages the symbolic reasoning capabilities of Large Language Models (LLMs) for query parsing, schema linking, and visualization selection.

Result: A Dynamic Sport Question Answering benchmark (DSQABENCH), comprising 1,700+ queries annotated with SQL programs, gold answers, and database snapshots, is introduced to evaluate system performance.

Conclusion: A modular, interactive system, SPORTSQL, is presented for natural language querying and visualization of dynamic sports data. The demo highlights how non-expert users can seamlessly explore evolving sports statistics through a natural, conversational interface.

Abstract: We present a modular, interactive system, SPORTSQL, for natural language
querying and visualization of dynamic sports data, with a focus on the English
Premier League (EPL). The system translates user questions into executable SQL
over a live, temporally indexed database constructed from real-time Fantasy
Premier League (FPL) data. It supports both tabular and visual outputs,
leveraging the symbolic reasoning capabilities of Large Language Models (LLMs)
for query parsing, schema linking, and visualization selection. To evaluate
system performance, we introduce the Dynamic Sport Question Answering benchmark
(DSQABENCH), comprising 1,700+ queries annotated with SQL programs, gold
answers, and database snapshots. Our demo highlights how non-expert users can
seamlessly explore evolving sports statistics through a natural, conversational
interface.

</details>


### [45] [Quantifying Language Disparities in Multilingual Large Language Models](https://arxiv.org/abs/2508.17162)
*Songbo Hu,Ivan Vulić,Anna Korhonen*

Main category: cs.CL

TL;DR: 提出了一个框架，用于更细粒度、更深入地量化跨模型和语言的实际性能差异。


<details>
  <summary>Details</summary>
Motivation: 大规模多语言评估中报告的结果常常是分散的，并受到目标语言、实验设置差异和模型选择等因素的混淆。

Method: 我们提出了一个解开这些混淆变量的框架，并引入了三个可解释的指标--性能实现率、其变异系数和语言潜力。

Result: 通过对11个多语言数据集上的13个模型变体的案例研究，我们证明我们的框架提供了更可靠的模型性能和语言差异的测量，特别是对于低资源语言，这些语言到目前为止已被证明具有挑战性。

Conclusion: 更高性能的模型并不一定意味着在所有语言上更公平。

Abstract: Results reported in large-scale multilingual evaluations are often fragmented
and confounded by factors such as target languages, differences in experimental
setups, and model choices. We propose a framework that disentangles these
confounding variables and introduces three interpretable metrics--the
performance realisation ratio, its coefficient of variation, and language
potential--enabling a finer-grained and more insightful quantification of
actual performance disparities across both (i) models and (ii) languages.
Through a case study of 13 model variants on 11 multilingual datasets, we
demonstrate that our framework provides a more reliable measurement of model
performance and language disparities, particularly for low-resource languages,
which have so far proven challenging to evaluate. Importantly, our results
reveal that higher overall model performance does not necessarily imply greater
fairness across languages.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [46] [Towards High-Precision Depth Sensing via Monocular-Aided iToF and RGB Integration](https://arxiv.org/abs/2508.16579)
*Yansong Du,Yutong Deng,Yuting Zhou,Feiyu Jiao,Jian Song,Xun Guan*

Main category: cs.CV

TL;DR: This paper introduces a new iToF-RGB fusion framework to overcome the limitations of iToF depth sensing. It uses a dual-encoder fusion network and achieves better accuracy and visual quality compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: This paper presents a novel iToF-RGB fusion framework designed to address the inherent limitations of indirect Time-of-Flight (iToF) depth sensing, such as low spatial resolution, limited field-of-view (FoV), and structural distortion in complex scenes.

Method: A dual-encoder fusion network is then employed to jointly extract complementary features from the reprojected iToF depth and RGB image, guided by monocular depth priors to recover fine-grained structural details and perform depth super-resolution.

Result: achieves enhanced depth accuracy, improved edge sharpness, and seamless FoV expansion

Conclusion: The proposed framework significantly outperforms state-of-the-art methods in terms of accuracy, structural consistency, and visual quality.

Abstract: This paper presents a novel iToF-RGB fusion framework designed to address the
inherent limitations of indirect Time-of-Flight (iToF) depth sensing, such as
low spatial resolution, limited field-of-view (FoV), and structural distortion
in complex scenes. The proposed method first reprojects the narrow-FoV iToF
depth map onto the wide-FoV RGB coordinate system through a precise geometric
calibration and alignment module, ensuring pixel-level correspondence between
modalities. A dual-encoder fusion network is then employed to jointly extract
complementary features from the reprojected iToF depth and RGB image, guided by
monocular depth priors to recover fine-grained structural details and perform
depth super-resolution. By integrating cross-modal structural cues and depth
consistency constraints, our approach achieves enhanced depth accuracy,
improved edge sharpness, and seamless FoV expansion. Extensive experiments on
both synthetic and real-world datasets demonstrate that the proposed framework
significantly outperforms state-of-the-art methods in terms of accuracy,
structural consistency, and visual quality.

</details>


### [47] [CountLoop: Training-Free High-Instance Image Generation via Iterative Agent Guidance](https://arxiv.org/abs/2508.16644)
*Anindya Mondal,Ayan Banerjee,Sauradip Nag,Josep Lladós,Xiatian Zhu,Anjan Dutta*

Main category: cs.CV

TL;DR: CountLoop is a training-free framework that improves the accuracy of diffusion models in generating images with a precise number of object instances.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are unreliable for generating scenes with a precise number of object instances, particularly in complex and high-density settings.

Method: CountLoop is a training-free framework that provides diffusion models with accurate instance control through iterative structured feedback. It alternates between image generation and multimodal agent evaluation, using a language-guided planner and critic to assess object counts, spatial arrangements, and attribute consistency. Instance-driven attention masking and compositional generation techniques are used to improve separation between objects.

Result: achieves counting accuracy of up to 98% while maintaining spatial fidelity and visual quality, outperforming layout-based and gradient-guided baselines with a score of 0.97.

Conclusion: CountLoop achieves counting accuracy of up to 98% while maintaining spatial fidelity and visual quality, outperforming layout-based and gradient-guided baselines with a score of 0.97.

Abstract: Diffusion models have shown remarkable progress in photorealistic image
synthesis, yet they remain unreliable for generating scenes with a precise
number of object instances, particularly in complex and high-density settings.
We present CountLoop, a training-free framework that provides diffusion models
with accurate instance control through iterative structured feedback. The
approach alternates between image generation and multimodal agent evaluation,
where a language-guided planner and critic assess object counts, spatial
arrangements, and attribute consistency. This feedback is then used to refine
layouts and guide subsequent generations. To further improve separation between
objects, especially in occluded scenes, we introduce instance-driven attention
masking and compositional generation techniques. Experiments on COCO Count, T2I
CompBench, and two new high-instance benchmarks show that CountLoop achieves
counting accuracy of up to 98% while maintaining spatial fidelity and visual
quality, outperforming layout-based and gradient-guided baselines with a score
of 0.97.

</details>


### [48] [Do VLMs Have Bad Eyes? Diagnosing Compositional Failures via Mechanistic Interpretability](https://arxiv.org/abs/2508.16652)
*Ashwath Vaithinathan Aravindan,Abha Jha,Mihir Kulkarni*

Main category: cs.CV

TL;DR: VLMs struggle with compositional generalization and object binding. This work explores the root causes of these failures using mechanistic interpretability techniques. The study found that individual neurons in the MLP layers of CLIP's vision encoder represent multiple features, and this "superposition" directly hinders its compositional feature representation.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models (VLMs) struggle with compositional generalization and object binding, which limit their ability to handle novel combinations of objects and their attributes.

Method: mechanistic interpretability techniques

Result: individual neurons in the MLP layers of CLIP's vision encoder represent multiple features, and this "superposition" directly hinders its compositional feature representation which consequently affects compositional reasoning and object binding capabilities.

Conclusion: This study serves as an initial step toward uncovering the mechanistic roots of compositional failures in VLMs.

Abstract: Vision-Language Models (VLMs) have shown remarkable performance in
integrating visual and textual information for tasks such as image captioning
and visual question answering. However, these models struggle with
compositional generalization and object binding, which limit their ability to
handle novel combinations of objects and their attributes. Our work explores
the root causes of these failures using mechanistic interpretability
techniques. We show evidence that individual neurons in the MLP layers of
CLIP's vision encoder represent multiple features, and this "superposition"
directly hinders its compositional feature representation which consequently
affects compositional reasoning and object binding capabilities. We hope this
study will serve as an initial step toward uncovering the mechanistic roots of
compositional failures in VLMs. The code and supporting results can be found
https://github.com/Mystic-Slice/Do-VLMs-Have-Bad-Eyes .

</details>


### [49] [MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and LLM Spatial Reasoning](https://arxiv.org/abs/2508.16654)
*Chenghao Liu,Zhimu Zhou,Jiachen Zhang,Minghao Zhang,Songfang Huang,Huiling Duan*

Main category: cs.CV

TL;DR: MSNav通过融合记忆、空间和决策模块，解决了视觉语言导航中现有方法的缺陷，并在Room-to-Room (R2R)和REVERIE数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言导航（VLN）方法存在空间推理能力差、跨模态基础薄弱以及长时程任务中记忆过载等问题。

Method: Memory Spatial Navigation(MSNav):融合了记忆模块、空间模块和决策模块的框架。空间模块使用了Instruction-Object-Space (I-O-S)数据集并对Qwen3-4B模型进行了微调。

Result: Qwen-Spatial (Qwen-Sp)在对象列表提取方面优于领先的商业LLM，在I-O-S测试集上实现了更高的F1和NDCG分数。MSNav在R2R和REVERIE数据集上表现出最先进的性能。

Conclusion: MSNav在R2R和REVERIE数据集上表现出最先进的性能，在成功率（SR）和路径长度加权成功率（SPL）方面有显著提高。

Abstract: Vision-and-Language Navigation (VLN) requires an agent to interpret natural
language instructions and navigate complex environments. Current approaches
often adopt a "black-box" paradigm, where a single Large Language Model (LLM)
makes end-to-end decisions. However, it is plagued by critical vulnerabilities,
including poor spatial reasoning, weak cross-modal grounding, and memory
overload in long-horizon tasks. To systematically address these issues, we
propose Memory Spatial Navigation(MSNav), a framework that fuses three modules
into a synergistic architecture, which transforms fragile inference into a
robust, integrated intelligence. MSNav integrates three modules: Memory Module,
a dynamic map memory module that tackles memory overload through selective node
pruning, enhancing long-range exploration; Spatial Module, a module for spatial
reasoning and object relationship inference that improves endpoint recognition;
and Decision Module, a module using LLM-based path planning to execute robust
actions. Powering Spatial Module, we also introduce an Instruction-Object-Space
(I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp),
which outperforms leading commercial LLMs in object list extraction, achieving
higher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the
Room-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art
performance with significant improvements in Success Rate (SR) and Success
weighted by Path Length (SPL).

</details>


### [50] [Optimizing Hyper parameters in CNN for Soil Classification using PSO and Whale Optimization Algorithm](https://arxiv.org/abs/2508.16660)
*Yasir Nooruldeen Ibrahim,Fawziya Mahmood Ramo,Mahmood Siddeeq Qadir,Muna Jaffer Al-Shamdeen*

Main category: cs.CV

TL;DR: This paper uses CNNs and swarm algorithms (Whale and Particle Swarm) to classify soil types, achieving efficient results.


<details>
  <summary>Details</summary>
Motivation: Classifying soil images contributes to better land management, increased agricultural output, and practical solutions for environmental issues. Understanding of soil quality aids in risk reduction, performance improvement, and sound decision-making.

Method: An intelligent model was constructed using Convolutional Neural Networks, and swarm algorithms (Whale optimization algorithm and Particle swarm optimization algorithm) were used to enhance the performance by choosing Hyper parameters for the Convolutional Neural Networks network.

Result: The Accuracy and F1 measures were adopted to test the system, and the results of the proposed work were efficient result

Conclusion: The proposed work achieved efficient results in soil classification.

Abstract: Classifying soil images contributes to better land management, increased
agricultural output, and practical solutions for environmental issues. The
development of various disciplines, particularly agriculture, civil
engineering, and natural resource management, is aided by understanding of soil
quality since it helps with risk reduction, performance improvement, and sound
decision-making . Artificial intelligence has recently been used in a number of
different fields. In this study, an intelligent model was constructed using
Convolutional Neural Networks to classify soil kinds, and machine learning
algorithms were used to enhance the performance of soil classification . To
achieve better implementation and performance of the Convolutional Neural
Networks algorithm and obtain valuable results for the process of classifying
soil type images, swarm algorithms were employed to obtain the best performance
by choosing Hyper parameters for the Convolutional Neural Networks network
using the Whale optimization algorithm and the Particle swarm optimization
algorithm, and comparing the results of using the two algorithms in the process
of multiple classification of soil types. The Accuracy and F1 measures were
adopted to test the system, and the results of the proposed work were efficient
result

</details>


### [51] [QA-VLM: Providing human-interpretable quality assessment for wire-feed laser additive manufacturing parts with Vision Language Models](https://arxiv.org/abs/2508.16661)
*Qiaojie Zheng,Jiucai Zhang,Joy Gockel,Michael B. Wakin,Craig Brice,Xiaoli Zhang*

Main category: cs.CV

TL;DR: A new QA-VLM framework provides interpretable quality assessments in additive manufacturing, outperforming existing VLMs.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning methods for image-based quality assessment in additive manufacturing lack interpretable justifications.

Method: A novel QA-VLM framework leveraging VLMs and application-specific knowledge distilled from journal articles.

Result: The framework demonstrates higher validity and consistency in explanation quality than off-the-shelf VLMs when evaluated on 24 single-bead samples.

Conclusion: The QA-VLM framework shows potential for trustworthy, interpretable quality assessment in AM applications.

Abstract: Image-based quality assessment (QA) in additive manufacturing (AM) often
relies heavily on the expertise and constant attention of skilled human
operators. While machine learning and deep learning methods have been
introduced to assist in this task, they typically provide black-box outputs
without interpretable justifications, limiting their trust and adoption in
real-world settings. In this work, we introduce a novel QA-VLM framework that
leverages the attention mechanisms and reasoning capabilities of
vision-language models (VLMs), enriched with application-specific knowledge
distilled from peer-reviewed journal articles, to generate human-interpretable
quality assessments. Evaluated on 24 single-bead samples produced by laser wire
direct energy deposition (DED-LW), our framework demonstrates higher validity
and consistency in explanation quality than off-the-shelf VLMs. These results
highlight the potential of our approach to enable trustworthy, interpretable
quality assessment in AM applications.

</details>


### [52] [The Loupe: A Plug-and-Play Attention Module for Amplifying Discriminative Features in Vision Transformers](https://arxiv.org/abs/2508.16663)
*Naren Sengodan*

Main category: cs.CV

TL;DR: The Loupe, a simple, intrinsic attention mechanism, significantly boosts performance while simultaneously providing clear visual explanations for Fine-Grained Visual Classification.


<details>
  <summary>Details</summary>
Motivation: Fine-Grained Visual Classification (FGVC) is a critical and challenging area within computer vision, demanding the identification of highly subtle, localized visual cues. The importance of FGVC extends to critical applications such as biodiversity monitoring and medical diagnostics, where precision is paramount. While large-scale Vision Transformers have achieved state-of-the-art performance, their decision-making processes often lack the interpretability required for trust and verification in such domains.

Method: introduce The Loupe, a novel, lightweight, and plug-and-play attention module designed to be inserted into pre-trained backbones like the Swin Transformer

Result: The Loupe effectively localizes semantically meaningful features, providing a valuable tool for understanding and trusting the model's decision-making process.

Conclusion: The Loupe improves the accuracy of a Swin-Base model from 85.40% to 88.06% on the CUB-200-2011 dataset.

Abstract: Fine-Grained Visual Classification (FGVC) is a critical and challenging area
within computer vision, demanding the identification of highly subtle,
localized visual cues. The importance of FGVC extends to critical applications
such as biodiversity monitoring and medical diagnostics, where precision is
paramount. While large-scale Vision Transformers have achieved state-of-the-art
performance, their decision-making processes often lack the interpretability
required for trust and verification in such domains. In this paper, we
introduce The Loupe, a novel, lightweight, and plug-and-play attention module
designed to be inserted into pre-trained backbones like the Swin Transformer.
The Loupe is trained end-to-end with a composite loss function that implicitly
guides the model to focus on the most discriminative object parts without
requiring explicit part-level annotations. Our unique contribution lies in
demonstrating that a simple, intrinsic attention mechanism can act as a
powerful regularizer, significantly boosting performance while simultaneously
providing clear visual explanations. Our experimental evaluation on the
challenging CUB-200-2011 dataset shows that The Loupe improves the accuracy of
a Swin-Base model from 85.40% to 88.06%, a significant gain of 2.66%.
Crucially, our qualitative analysis of the learned attention maps reveals that
The Loupe effectively localizes semantically meaningful features, providing a
valuable tool for understanding and trusting the model's decision-making
process.

</details>


### [53] [COVID19 Prediction Based On CT Scans Of Lungs Using DenseNet Architecture](https://arxiv.org/abs/2508.16670)
*Deborup Sanyal*

Main category: cs.CV

TL;DR: 该项目使用卷积神经网络模型，通过分析肺部CT扫描来帮助医生判断COVID-19的严重程度。


<details>
  <summary>Details</summary>
Motivation: 世界未能为21世纪的疫情做好准备，导致全球约160万人死亡。COVID-19最常见的症状与呼吸系统有关。因呼吸系统衰竭而丧生。医院床位、氧气瓶和呼吸机严重短缺。许多人未经任何治疗就去世了。因此，本项目的目的是帮助医生确定COVID-19的严重程度。

Method: 使用卷积神经网络模型。

Result: 该模型将分析阳性测试结果一个月内COVID-19感染的严重程度。

Conclusion: 该模型通过分析肺部CT扫描来帮助医生判断COVID-19的严重程度。

Abstract: COVID19 took the world by storm since December 2019. A highly infectious
communicable disease, COVID19 is caused by the SARSCoV2 virus. By March 2020,
the World Health Organization (WHO) declared COVID19 as a global pandemic. A
pandemic in the 21st century after almost 100 years was something the world was
not prepared for, which resulted in the deaths of around 1.6 million people
worldwide. The most common symptoms of COVID19 were associated with the
respiratory system and resembled a cold, flu, or pneumonia. After extensive
research, doctors and scientists concluded that the main reason for lives being
lost due to COVID19 was failure of the respiratory system. Patients were dying
gasping for breath. Top healthcare systems of the world were failing badly as
there was an acute shortage of hospital beds, oxygen cylinders, and
ventilators. Many were dying without receiving any treatment at all. The aim of
this project is to help doctors decide the severity of COVID19 by reading the
patient's Computed Tomography (CT) scans of the lungs. Computer models are less
prone to human error, and Machine Learning or Neural Network models tend to
give better accuracy as training improves over time. We have decided to use a
Convolutional Neural Network model. Given that a patient tests positive, our
model will analyze the severity of COVID19 infection within one month of the
positive test result. The severity of the infection may be promising or
unfavorable (if it leads to intubation or death), based entirely on the CT
scans in the dataset.

</details>


### [54] [MedRepBench: A Comprehensive Benchmark for Medical Report Interpretation](https://arxiv.org/abs/2508.16674)
*Fangxin Shang,Yuan Xia,Dalu Yang,Yahui Wang,Binglin Yang*

Main category: cs.CV

TL;DR: introduce a comprehensive benchmark to evaluate VLMs for structured medical report understanding


<details>
  <summary>Details</summary>
Motivation: lack of standardized benchmarks to assess structured interpretation quality in medical reports

Method: introduce MedRepBench, a comprehensive benchmark built from 1,900 de-identified real-world Chinese medical reports and apply Group Relative Policy Optimization (GRPO) to improve a mid-scale VLM

Result: achieving up to 6% recall gain

Conclusion: OCR+LLM pipeline suffers from layout-blindness and latency issues, motivating further progress toward robust, fully vision-based report understanding.

Abstract: Medical report interpretation plays a crucial role in healthcare, enabling
both patient-facing explanations and effective information flow across clinical
systems. While recent vision-language models (VLMs) and large language models
(LLMs) have demonstrated general document understanding capabilities, there
remains a lack of standardized benchmarks to assess structured interpretation
quality in medical reports. We introduce MedRepBench, a comprehensive benchmark
built from 1,900 de-identified real-world Chinese medical reports spanning
diverse departments, patient demographics, and acquisition formats. The
benchmark is designed primarily to evaluate end-to-end VLMs for structured
medical report understanding. To enable controlled comparisons, we also include
a text-only evaluation setting using high-quality OCR outputs combined with
LLMs, allowing us to estimate the upper-bound performance when character
recognition errors are minimized. Our evaluation framework supports two
complementary protocols: (1) an objective evaluation measuring field-level
recall of structured clinical items, and (2) an automated subjective evaluation
using a powerful LLM as a scoring agent to assess factuality, interpretability,
and reasoning quality. Based on the objective metric, we further design a
reward function and apply Group Relative Policy Optimization (GRPO) to improve
a mid-scale VLM, achieving up to 6% recall gain. We also observe that the
OCR+LLM pipeline, despite strong performance, suffers from layout-blindness and
latency issues, motivating further progress toward robust, fully vision-based
report understanding.

</details>


### [55] [Two-Stage Framework for Efficient UAV-Based Wildfire Video Analysis with Adaptive Compression and Fire Source Detection](https://arxiv.org/abs/2508.16739)
*Yanbing Bai,Rui-Yang Ju,Lemeng Zhao,Junjie Hu,Jianchao Bi,Erick Mas,Shunichi Koshimura*

Main category: cs.CV

TL;DR: A two-stage framework for real-time wildfire monitoring on UAVs. Stage 1 reduces computational costs, and Stage 2 achieves higher detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Unmanned Aerial Vehicles (UAVs) have become increasingly important in disaster emergency response by enabling real-time aerial video analysis. Due to the limited computational resources available on UAVs, large models cannot be run independently for real-time analysis.

Method: a lightweight and efficient two-stage framework for real-time wildfire monitoring and fire source detection on UAV platforms. Stage 1: a policy network to identify and discard redundant video clips using frame compression techniques and a station point mechanism that leverages future frame information. Stage 2: improved YOLOv8 model to localize the fire source.

Result: significantly reduces computational costs while maintaining classification accuracy in Stage 1, and achieves higher detection accuracy with similar inference time in Stage 2 compared to baseline methods.

Conclusion: The proposed method reduces computational costs while maintaining classification accuracy in Stage 1, and achieves higher detection accuracy with similar inference time in Stage 2 compared to baseline methods.

Abstract: Unmanned Aerial Vehicles (UAVs) have become increasingly important in
disaster emergency response by enabling real-time aerial video analysis. Due to
the limited computational resources available on UAVs, large models cannot be
run independently for real-time analysis. To overcome this challenge, we
propose a lightweight and efficient two-stage framework for real-time wildfire
monitoring and fire source detection on UAV platforms. Specifically, in Stage
1, we utilize a policy network to identify and discard redundant video clips
using frame compression techniques, thereby reducing computational costs. In
addition, we introduce a station point mechanism that leverages future frame
information within the sequential policy network to improve prediction
accuracy. In Stage 2, once the frame is classified as "fire", we employ the
improved YOLOv8 model to localize the fire source. We evaluate the Stage 1
method using the FLAME and HMDB51 datasets, and the Stage 2 method using the
Fire & Smoke dataset. Experimental results show that our method significantly
reduces computational costs while maintaining classification accuracy in Stage
1, and achieves higher detection accuracy with similar inference time in Stage
2 compared to baseline methods.

</details>


### [56] [CellEcoNet: Decoding the Cellular Language of Pathology with Deep Learning for Invasive Lung Adenocarcinoma Recurrence Prediction](https://arxiv.org/abs/2508.16742)
*Abdul Rehman Akbar,Usama Sajjad,Ziyu Su,Wencheng Li,Fei Xing,Jimmy Ruiz,Wei Chen,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: CellEcoNet是一种新的深度学习框架，它可以比现有方法更好地预测肺腺癌的复发风险。


<details>
  <summary>Details</summary>
Motivation: 大约 70% 的浸润性肺腺癌 (ILA) 患者在五年内复发，而目前的工具未能识别出需要辅助治疗的患者。为了解决这一未满足的临床需求。

Method: CellEcoNet，一种新型的具有空间感知能力的深度学习框架，通过自然语言类比对全切片图像（WSI）进行建模，定义了一种“病理学语言”，其中细胞充当单词，细胞邻域充当短语，组织结构形成句子。

Result: 在 456 个 H&E 染色 WSI 的数据集上，CellEcoNet 实现了卓越的预测性能（AUC：77.8% HR：9.54），优于 IASLC 分级系统（AUC：71.4% HR：2.36）、AJCC 分期（AUC：64.0% HR：1.17）和最先进的计算方法（AUC：62.2-67.4%）。CellEcoNet 在不同的人口统计和临床亚组中表现出公平性和一致的性能。

Conclusion: CellEcoNet通过解码肿瘤微环境的细胞“语言”来揭示细胞变异如何编码复发风险，标志着范式转变。

Abstract: Despite surgical resection, ~70% of invasive lung adenocarcinoma (ILA)
patients recur within five years, and current tools fail to identify those
needing adjuvant therapy. To address this unmet clinical need, we introduce
CellEcoNet, a novel spatially aware deep learning framework that models whole
slide images (WSIs) through natural language analogy, defining a "language of
pathology," where cells act as words, cellular neighborhoods become phrases,
and tissue architecture forms sentences. CellEcoNet learns these
context-dependent meanings automatically, capturing how subtle variations and
spatial interactions derive recurrence risk. On a dataset of 456 H&E-stained
WSIs, CellEcoNet achieved superior predictive performance (AUC:77.8% HR:9.54),
outperforming IASLC grading system (AUC:71.4% HR:2.36), AJCC Stage (AUC:64.0%
HR:1.17) and state-of-the-art computational methods (AUCs:62.2-67.4%).
CellEcoNet demonstrated fairness and consistent performance across diverse
demographic and clinical subgroups. Beyond prognosis, CellEcoNet marks a
paradigm shift by decoding the tumor microenvironment's cellular "language" to
reveal how subtle cell variations encode recurrence risk.

</details>


### [57] [A Framework for Benchmarking Fairness-Utility Trade-offs in Text-to-Image Models via Pareto Frontiers](https://arxiv.org/abs/2508.16752)
*Marco N. Bochernitsan,Rodrigo C. Barros,Lucas S. Kupssinskü*

Main category: cs.CV

TL;DR: 提出了一种评估文本到图像模型公平性和效用的方法，该方法使用帕累托最优前沿来比较不同的模型，并找到更好的超参数。


<details>
  <summary>Details</summary>
Motivation: 在文本到图像生成中实现公平性需要在不损害视觉保真度的情况下减轻社会偏见，这对负责任的 AI 至关重要。当前文本到图像模型的公平性评估程序依赖于定性判断或狭隘的比较，这限制了评估这些模型中的公平性和效用的能力，并阻止了去偏方法的重现性评估。现有方法通常采用临时的、以人为中心的视觉检查，这些检查既容易出错又难以复制。

Method: 我们提出了一种使用跨去偏方法的超参数化的帕累托最优前沿来评估文本到图像模型中的公平性和效用的方法。

Result: 我们的方法允许比较不同的文本到图像模型，概述了优化给定效用的公平性的所有配置，反之亦然。为了说明我们的评估方法，我们分别使用归一化香农熵和 ClipScore 进行公平性和效用评估。我们评估了 Stable Diffusion、Fair Diffusion、SDXL、DeCoDi 和 FLUX 文本到图像模型中的公平性和效用。

Conclusion: 该方法表明，文本到图像模型的大多数默认超参数化都是公平效用空间中的主导解决方案，并且很容易找到更好的超参数。

Abstract: Achieving fairness in text-to-image generation demands mitigating social
biases without compromising visual fidelity, a challenge critical to
responsible AI. Current fairness evaluation procedures for text-to-image models
rely on qualitative judgment or narrow comparisons, which limit the capacity to
assess both fairness and utility in these models and prevent reproducible
assessment of debiasing methods. Existing approaches typically employ ad-hoc,
human-centered visual inspections that are both error-prone and difficult to
replicate. We propose a method for evaluating fairness and utility in
text-to-image models using Pareto-optimal frontiers across hyperparametrization
of debiasing methods. Our method allows for comparison between distinct
text-to-image models, outlining all configurations that optimize fairness for a
given utility and vice-versa. To illustrate our evaluation method, we use
Normalized Shannon Entropy and ClipScore for fairness and utility evaluation,
respectively. We assess fairness and utility in Stable Diffusion, Fair
Diffusion, SDXL, DeCoDi, and FLUX text-to-image models. Our method shows that
most default hyperparameterizations of the text-to-image model are dominated
solutions in the fairness-utility space, and it is straightforward to find
better hyperparameters.

</details>


### [58] [WebMMU: A Benchmark for Multimodal Multilingual Website Understanding and Code Generation](https://arxiv.org/abs/2508.16763)
*Rabiul Awal,Mahsa Massoud,Aarash Feizi,Zichao Li,Suyuchen Wang,Christopher Pal,Aishwarya Agrawal,David Vazquez,Siva Reddy,Juan A. Rodriguez,Perouz Taslakian,Spandana Gella,Sai Rajeswar*

Main category: cs.CV

TL;DR: WebMMU是一个多语言基准，用于评估MLLM在网站视觉问答、代码编辑和模型到代码生成方面的能力。结果表明，虽然MLLM在基本信息提取方面表现良好，但在推理和定位方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 评估模型在复杂的多步骤推理、精确的元素定位以及功能性UI理解和编码方面的能力。

Method: 使用专家注释的真实Web数据，统一了网站视觉问答、涉及HTML/CSS/JavaScript的代码编辑和模型到代码的生成这三个核心Web任务。

Result: 多模态大型语言模型(MLLM)在基本信息提取方面表现良好，但在推理和定位方面表现不佳，无法编辑代码以保持功能，也无法生成保持层次结构并支持多语言内容的设计到代码。

Conclusion: 当前的MLLM在推理和定位、编辑代码以保持功能以及生成保持层次结构并支持多语言内容的设计到代码方面存在不足。这些发现揭示了当前MLLM的关键局限性，并强调需要改进多模态和跨语言推理，以构建能够自动执行各种Web开发任务的未来Web代理。

Abstract: We present WebMMU, a multilingual benchmark that evaluates three core web
tasks: (1) website visual question answering, (2) code editing involving
HTML/CSS/JavaScript, and (3) mockup-to-code generation. Unlike prior benchmarks
that treat these tasks separately, WebMMU unifies them using expert-annotated,
real-world web data to assess models' abilities in complex multi-step
reasoning, precise element grounding, and functional UI comprehension and
coding. Our evaluation shows that while multimodal large language models
(MLLMs) perform well on basic information extraction, they struggle with
reasoning and grounding, editing code to preserve functionality, and generating
design-to-code that maintains hierarchy and supports multilingual content.
These findings reveal key limitations in current MLLMs and underscore the need
for improved multimodal and cross-lingual reasoning to build future web agents
capable of automating diverse web development tasks.

</details>


### [59] [Improving Performance, Robustness, and Fairness of Radiographic AI Models with Finely-Controllable Synthetic Data](https://arxiv.org/abs/2508.16783)
*Stefania L. Moroianu,Christian Bluethgen,Pierre Chambon,Mehdi Cherti,Jean-Benoit Delbrouck,Magdalini Paschali,Brandon Price,Judy Gichoya,Jenia Jitsev,Curtis P. Langlotz,Akshay S. Chaudhari*

Main category: cs.CV

TL;DR: RoentGen-v2, a text-to-image diffusion model, generates demographically balanced synthetic chest radiographs to improve the performance, generalization, and fairness of downstream disease classification models through synthetic pretraining.


<details>
  <summary>Details</summary>
Motivation: Achieving robust performance and fairness across diverse patient populations remains a challenge in developing clinically deployable deep learning models for diagnostic imaging. Synthetic data generation has emerged as a promising strategy to address limitations in dataset scale and diversity.

Method: We introduce RoentGen-v2, a text-to-image diffusion model for chest radiographs that enables fine-grained control over both radiographic findings and patient demographic attributes, including sex, age, and race/ethnicity. We propose an improved training strategy that leverages synthetic data for supervised pretraining, followed by fine-tuning on real data.

Result: Synthetic pretraining consistently improves model performance, generalization to out-of-distribution settings, and fairness across demographic subgroups. Across datasets, synthetic pretraining led to a 6.5% accuracy increase in the performance of downstream classification models, compared to a modest 2.7% increase when naively combining real and synthetic data. We observe this performance improvement simultaneously with the reduction of the underdiagnosis fairness gap by 19.3%.

Conclusion: Synthetic imaging can advance equitable and generalizable medical deep learning under real-world data constraints.

Abstract: Achieving robust performance and fairness across diverse patient populations
remains a challenge in developing clinically deployable deep learning models
for diagnostic imaging. Synthetic data generation has emerged as a promising
strategy to address limitations in dataset scale and diversity. We introduce
RoentGen-v2, a text-to-image diffusion model for chest radiographs that enables
fine-grained control over both radiographic findings and patient demographic
attributes, including sex, age, and race/ethnicity. RoentGen-v2 is the first
model to generate clinically plausible images with demographic conditioning,
facilitating the creation of a large, demographically balanced synthetic
dataset comprising over 565,000 images. We use this large synthetic dataset to
evaluate optimal training pipelines for downstream disease classification
models. In contrast to prior work that combines real and synthetic data
naively, we propose an improved training strategy that leverages synthetic data
for supervised pretraining, followed by fine-tuning on real data. Through
extensive evaluation on over 137,000 chest radiographs from five institutions,
we demonstrate that synthetic pretraining consistently improves model
performance, generalization to out-of-distribution settings, and fairness
across demographic subgroups. Across datasets, synthetic pretraining led to a
6.5% accuracy increase in the performance of downstream classification models,
compared to a modest 2.7% increase when naively combining real and synthetic
data. We observe this performance improvement simultaneously with the reduction
of the underdiagnosis fairness gap by 19.3%. These results highlight the
potential of synthetic imaging to advance equitable and generalizable medical
deep learning under real-world data constraints. We open source our code,
trained models, and synthetic dataset at
https://github.com/StanfordMIMI/RoentGen-v2 .

</details>


### [60] [Towards Open-Vocabulary Multimodal 3D Object Detection with Attributes](https://arxiv.org/abs/2508.16812)
*Xinhao Xiang,Kuan-Chuan Peng,Suhas Lohit,Michael J. Jones,Jiawei Zhang*

Main category: cs.CV

TL;DR: OVODA 是一种新的开放词汇 3D 对象和属性检测框架，它使用基础模型来弥合 3D 特征和文本之间的语义差距，并联合检测属性。同时提出了一个新的数据集 OVAD。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于封闭集假设，难以识别真实场景中的新对象及其属性。

Method: 该方法结合了基础模型特征连接、提示调整策略和专门的属性检测技术，包括透视指定提示和水平翻转增强。

Result: OVODA 在 nuScenes 和 Argoverse 2 数据集上表现出色，在没有给定新类别的 anchor size 的情况下，优于最先进的开放词汇 3D 对象检测方法。

Conclusion: OVODA 在开放词汇 3D 对象检测中优于现有技术，并且能够识别对象属性，同时新数据集 OVAD 也已发布。

Abstract: 3D object detection plays a crucial role in autonomous systems, yet existing
methods are limited by closed-set assumptions and struggle to recognize novel
objects and their attributes in real-world scenarios. We propose OVODA, a novel
framework enabling both open-vocabulary 3D object and attribute detection with
no need to know the novel class anchor size. OVODA uses foundation models to
bridge the semantic gap between 3D features and texts while jointly detecting
attributes, e.g., spatial relationships, motion states, etc. To facilitate such
research direction, we propose OVAD, a new dataset that supplements existing 3D
object detection benchmarks with comprehensive attribute annotations. OVODA
incorporates several key innovations, including foundation model feature
concatenation, prompt tuning strategies, and specialized techniques for
attribute detection, including perspective-specified prompts and horizontal
flip augmentation. Our results on both the nuScenes and Argoverse 2 datasets
show that under the condition of no given anchor sizes of novel classes, OVODA
outperforms the state-of-the-art methods in open-vocabulary 3D object detection
while successfully recognizing object attributes. Our OVAD dataset is released
here: https://doi.org/10.5281/zenodo.16904069 .

</details>


### [61] [AIM 2025 Low-light RAW Video Denoising Challenge: Dataset, Methods and Results](https://arxiv.org/abs/2508.16830)
*Alexander Yakovenko,George Chakvetadze,Ilya Khrapov,Maksim Zhelezov,Dmitry Vatolin,Radu Timofte,Youngjin Oh,Junhyeong Kwon,Junyoung Park,Nam Ik Cho,Senyan Xu,Ruixuan Jiang,Long Peng,Xueyang Fu,Zheng-Jun Zha,Xiaoping Peng,Hansen Feng,Zhanyi Tie,Ziming Xia,Lizhi Wang*

Main category: cs.CV

TL;DR: 本文回顾了AIM 2025低光RAW视频降噪挑战赛，并介绍了一个新的基准数据集，用于评估利用时间冗余来降低视频噪点的方法。


<details>
  <summary>Details</summary>
Motivation: 回顾AIM 2025（图像处理进展）低光RAW视频降噪挑战赛。

Method: 开发利用时间冗余来降低低光RAW视频噪点的方法。

Result: 引入了一个新的基准，包含756个十帧序列，这些序列由14个智能手机摄像头传感器在9种条件下拍摄（光照：1/5/10 lx；曝光：1/24, 1/60, 1/120 s），并通过突发平均获得高信噪比参考。

Conclusion: 本报告描述了数据集、挑战协议和提交的方法。

Abstract: This paper reviews the AIM 2025 (Advances in Image Manipulation) Low-Light
RAW Video Denoising Challenge. The task is to develop methods that denoise
low-light RAW video by exploiting temporal redundancy while operating under
exposure-time limits imposed by frame rate and adapting to sensor-specific,
signal-dependent noise. We introduce a new benchmark of 756 ten-frame sequences
captured with 14 smartphone camera sensors across nine conditions
(illumination: 1/5/10 lx; exposure: 1/24, 1/60, 1/120 s), with high-SNR
references obtained via burst averaging. Participants process linear RAW
sequences and output the denoised 10th frame while preserving the Bayer
pattern. Submissions are evaluated on a private test set using full-reference
PSNR and SSIM, with final ranking given by the mean of per-metric ranks. This
report describes the dataset, challenge protocol, and submitted approaches.

</details>


### [62] [Transformer-Based Neural Network for Transient Detection without Image Subtraction](https://arxiv.org/abs/2508.16844)
*Adi Inada,Masao Sako,Tatiana Acero-Cuellar,Federica Bianco*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的神经网络，用于准确分类天文图像中的瞬态检测，无需差分成像，并在DES数据集上实现了高准确率。


<details>
  <summary>Details</summary>
Motivation: 为了在天文图像中准确分类真实和虚假的瞬态检测，并超越传统的卷积神经网络（CNN）方法。

Method: 基于Transformer的神经网络，能够对搜索和模板图像进行有效分析，无需进行计算成本高昂的差分成像。

Result: 在Dark Energy Survey (DES)的autoScan数据集上，该网络达到了97.4%的分类准确率，并且随着训练集规模的增大，差分图像的性能效用逐渐降低。

Conclusion: 该网络通过提高大规模天文调查中超新星检测的准确性和效率，证明了其有效性。

Abstract: We introduce a transformer-based neural network for the accurate
classification of real and bogus transient detections in astronomical images.
This network advances beyond the conventional convolutional neural network
(CNN) methods, widely used in image processing tasks, by adopting an
architecture better suited for detailed pixel-by-pixel comparison. The
architecture enables efficient analysis of search and template images only,
thus removing the necessity for computationally-expensive difference imaging,
while maintaining high performance. Our primary evaluation was conducted using
the autoScan dataset from the Dark Energy Survey (DES), where the network
achieved a classification accuracy of 97.4% and diminishing performance utility
for difference image as the size of the training set grew. Further experiments
with DES data confirmed that the network can operate at a similar level even
when the input images are not centered on the supernova candidate. These
findings highlight the network's effectiveness in enhancing both accuracy and
efficiency of supernova detection in large-scale astronomical surveys.

</details>


### [63] [NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows](https://arxiv.org/abs/2508.16845)
*Denis Tarasov,Alexander Nikulin,Ilya Zisman,Albina Klepach,Nikita Lyubaykin,Andrei Polubarov,Alexander Derevyagin,Vladislav Kurenkov*

Main category: cs.CV

TL;DR: NinA: a fast and expressive alternative to diffusion-based decoders for VLAs.


<details>
  <summary>Details</summary>
Motivation: diffusion models require multiple iterative denoising steps at inference time or downstream techniques to speed up sampling, limiting their practicality in real-world settings where high-frequency control is crucial

Method: replaces the diffusion action decoder with a Normalizing Flow (NF) that enables one-shot sampling through an invertible transformation

Result: NinA matches the performance of its diffusion-based counterpart under the same training regime, while achieving substantially faster inference.

Conclusion: NinA offers a promising path toward efficient, high-frequency VLA control without compromising performance.

Abstract: Recent advances in Vision-Language-Action (VLA) models have established a
two-component architecture, where a pre-trained Vision-Language Model (VLM)
encodes visual observations and task descriptions, and an action decoder maps
these representations to continuous actions. Diffusion models have been widely
adopted as action decoders due to their ability to model complex, multimodal
action distributions. However, they require multiple iterative denoising steps
at inference time or downstream techniques to speed up sampling, limiting their
practicality in real-world settings where high-frequency control is crucial. In
this work, we present NinA (Normalizing Flows in Action), a fast and expressive
alter- native to diffusion-based decoders for VLAs. NinA replaces the diffusion
action decoder with a Normalizing Flow (NF) that enables one-shot sampling
through an invertible transformation, significantly reducing inference time. We
integrate NinA into the FLOWER VLA architecture and fine-tune on the LIBERO
benchmark. Our experiments show that NinA matches the performance of its
diffusion-based counterpart under the same training regime, while achieving
substantially faster inference. These results suggest that NinA offers a
promising path toward efficient, high-frequency VLA control without
compromising performance.

</details>


### [64] [RF-PGS: Fully-structured Spatial Wireless Channel Representation with Planar Gaussian Splatting](https://arxiv.org/abs/2508.16849)
*Lihao Zhang,Zongtan Li,Haijian Sun*

Main category: cs.CV

TL;DR: RF-PGS：一种新的框架，仅从稀疏路径损耗谱重建高保真无线电传播路径，显著提高了重建精度，降低了训练成本，并能够有效地表示无线信道，为可扩展的6G Spatial-CSI建模提供了一种实用的解决方案。


<details>
  <summary>Details</summary>
Motivation: 在6G时代，对更高系统吞吐量的需求和新兴6G技术的实施需要大规模天线阵列和准确的空间信道状态信息（Spatial-CSI）。传统的信道建模方法，如经验模型、射线追踪和基于测量的方法，在空间分辨率、效率和可扩展性方面面临挑战。基于辐射场的方法已经成为有前途的替代方案，但仍然存在几何不准确和昂贵的监督问题。

Method: 提出了一种新颖的框架RF-PGS，该框架仅从稀疏路径损耗谱重建高保真无线电传播路径。通过引入平面高斯作为具有某些RF特定优化的几何基元，RF-PGS在第一几何训练阶段实现了密集、表面对齐的场景重建。在随后的射频（RF）训练阶段，所提出的完全结构化的无线电辐射与定制的多视图损失相结合，准确地模拟了无线电传播行为。

Result: 与先前的辐射场方法相比，RF-PGS显著提高了重建精度，降低了训练成本，并能够有效地表示无线信道，为可扩展的6G Spatial-CSI建模提供了一种实用的解决方案。

Conclusion: RF-PGS显著提高了重建精度，降低了训练成本，并能够有效地表示无线信道，为可扩展的6G Spatial-CSI建模提供了一种实用的解决方案。

Abstract: In the 6G era, the demand for higher system throughput and the implementation
of emerging 6G technologies require large-scale antenna arrays and accurate
spatial channel state information (Spatial-CSI). Traditional channel modeling
approaches, such as empirical models, ray tracing, and measurement-based
methods, face challenges in spatial resolution, efficiency, and scalability.
Radiance field-based methods have emerged as promising alternatives but still
suffer from geometric inaccuracy and costly supervision. This paper proposes
RF-PGS, a novel framework that reconstructs high-fidelity radio propagation
paths from only sparse path loss spectra. By introducing Planar Gaussians as
geometry primitives with certain RF-specific optimizations, RF-PGS achieves
dense, surface-aligned scene reconstruction in the first geometry training
stage. In the subsequent Radio Frequency (RF) training stage, the proposed
fully-structured radio radiance, combined with a tailored multi-view loss,
accurately models radio propagation behavior. Compared to prior radiance field
methods, RF-PGS significantly improves reconstruction accuracy, reduces
training costs, and enables efficient representation of wireless channels,
offering a practical solution for scalable 6G Spatial-CSI modeling.

</details>


### [65] [Gaussian Primitive Optimized Deformable Retinal Image Registration](https://arxiv.org/abs/2508.16852)
*Xin Tian,Jiazheng Wang,Yuxi Zhang,Xiang Chen,Renjiu Hu,Gaolei Li,Min Liu,Hang Zhang*

Main category: cs.CV

TL;DR: GPO, a novel retinal image registration framework using Gaussian primitives and KNN interpolation, significantly outperforms existing methods by strategically anchoring nodes in high-gradient regions to ensure robust gradient flow.


<details>
  <summary>Details</summary>
Motivation: Deformable retinal image registration is difficult due to large homogeneous regions and sparse vascular features, causing limited gradient signals in standard learning-based frameworks.

Method: Gaussian Primitive Optimization (GPO), an iterative framework with structured message passing. Keypoints are extracted at salient anatomical structures and modeled as Gaussian primitives with trainable parameters. KNN Gaussian interpolation blends displacement signals to construct a displacement field.

Result: GPO reduces the target registration error from 6.2px to ~2.4px and increases the AUC at 25px from 0.770 to 0.938 on the FIRE dataset.

Conclusion: GPO substantially outperforms existing methods on the FIRE dataset, reducing target registration error and increasing AUC.

Abstract: Deformable retinal image registration is notoriously difficult due to large
homogeneous regions and sparse but critical vascular features, which cause
limited gradient signals in standard learning-based frameworks. In this paper,
we introduce Gaussian Primitive Optimization (GPO), a novel iterative framework
that performs structured message passing to overcome these challenges. After an
initial coarse alignment, we extract keypoints at salient anatomical structures
(e.g., major vessels) to serve as a minimal set of descriptor-based control
nodes (DCN). Each node is modelled as a Gaussian primitive with trainable
position, displacement, and radius, thus adapting its spatial influence to
local deformation scales. A K-Nearest Neighbors (KNN) Gaussian interpolation
then blends and propagates displacement signals from these information-rich
nodes to construct a globally coherent displacement field; focusing
interpolation on the top (K) neighbors reduces computational overhead while
preserving local detail. By strategically anchoring nodes in high-gradient
regions, GPO ensures robust gradient flow, mitigating vanishing gradient signal
in textureless areas. The framework is optimized end-to-end via a multi-term
loss that enforces both keypoint consistency and intensity alignment.
Experiments on the FIRE dataset show that GPO reduces the target registration
error from 6.2\,px to ~2.4\,px and increases the AUC at 25\,px from 0.770 to
0.938, substantially outperforming existing methods. The source code can be
accessed via https://github.com/xintian-99/GPOreg.

</details>


### [66] [Beyond Emotion Recognition: A Multi-Turn Multimodal Emotion Understanding and Reasoning Benchmark](https://arxiv.org/abs/2508.16859)
*Jinpeng Hu,Hongchang Shi,Chongyuan Dai,Zhuo Li,Peipei Song,Meng Wang*

Main category: cs.CV

TL;DR: This paper introduces a new benchmark (MTMEUR) for multi-turn multimodal emotion understanding and reasoning and proposes a multi-agent framework to address the challenges in this task. Experiments show existing models struggle with the benchmark.


<details>
  <summary>Details</summary>
Motivation: Recent research primarily focuses on enhancing emotion recognition abilities, leaving the substantial potential in emotion reasoning.

Method: A multi-agent framework is proposed to improve the system's reasoning capabilities.

Result: A multi-turn multimodal emotion understanding and reasoning (MTMEUR) benchmark is introduced, which encompasses 1,451 video data from real-life scenarios, along with 5,101 progressive questions. Experiments with existing MLLMs and the agent-based method on the proposed benchmark are conducted.

Conclusion: Existing MLLMs face significant challenges in multi-turn multimodal emotion understanding and reasoning.

Abstract: Multimodal large language models (MLLMs) have been widely applied across
various fields due to their powerful perceptual and reasoning capabilities. In
the realm of psychology, these models hold promise for a deeper understanding
of human emotions and behaviors. However, recent research primarily focuses on
enhancing their emotion recognition abilities, leaving the substantial
potential in emotion reasoning, which is crucial for improving the naturalness
and effectiveness of human-machine interactions. Therefore, in this paper, we
introduce a multi-turn multimodal emotion understanding and reasoning (MTMEUR)
benchmark, which encompasses 1,451 video data from real-life scenarios, along
with 5,101 progressive questions. These questions cover various aspects,
including emotion recognition, potential causes of emotions, future action
prediction, etc. Besides, we propose a multi-agent framework, where each agent
specializes in a specific aspect, such as background context, character
dynamics, and event details, to improve the system's reasoning capabilities.
Furthermore, we conduct experiments with existing MLLMs and our agent-based
method on the proposed benchmark, revealing that most models face significant
challenges with this task.

</details>


### [67] [Delta-SVD: Efficient Compression for Personalized Text-to-Image Models](https://arxiv.org/abs/2508.16863)
*Tangyuan Zhang,Shangyu Chen,Qixiang Chen,Jianfei Cai*

Main category: cs.CV

TL;DR: Delta-SVD 是一种高效压缩个性化文本到图像模型的方法，它通过 SVD 分解和秩截断来减少存储开销，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 个性化文本到图像模型（如 DreamBooth）需要微调大规模扩散backbones，导致在维护许多特定于主体的模型时产生显著的存储开销。

Method: Delta-SVD 是一种后验、免训练的压缩方法，它利用奇异值分解（SVD）分解权重增量，然后采用基于能量的秩截断策略。

Result: 在多主体数据集上的实验表明，Delta-SVD 实现了显著的压缩，而通过 CLIP 评分、SSIM 和 FID 测量的生成质量损失可忽略不计。

Conclusion: Delta-SVD 能够在保证生成质量的同时实现显著的压缩，适用于需要存储和部署大规模主体定制的实际应用。

Abstract: Personalized text-to-image models such as DreamBooth require fine-tuning
large-scale diffusion backbones, resulting in significant storage overhead when
maintaining many subject-specific models. We present Delta-SVD, a post-hoc,
training-free compression method that targets the parameter weights update
induced by DreamBooth fine-tuning. Our key observation is that these delta
weights exhibit strong low-rank structure due to the sparse and localized
nature of personalization. Delta-SVD first applies Singular Value Decomposition
(SVD) to factorize the weight deltas, followed by an energy-based rank
truncation strategy to balance compression efficiency and reconstruction
fidelity. The resulting compressed models are fully plug-and-play and can be
re-constructed on-the-fly during inference. Notably, the proposed approach is
simple, efficient, and preserves the original model architecture. Experiments
on a multiple subject dataset demonstrate that Delta-SVD achieves substantial
compression with negligible loss in generation quality measured by CLIP score,
SSIM and FID. Our method enables scalable and efficient deployment of
personalized diffusion models, making it a practical solution for real-world
applications that require storing and deploying large-scale subject
customizations.

</details>


### [68] [Do Multimodal LLMs See Sentiment?](https://arxiv.org/abs/2508.16873)
*Neemias B. da Silva,John Harrison,Rodrigo Minetto,Myriam R. Delgado,Bogdan T. Nassu,Thiago H. Silva*

Main category: cs.CV

TL;DR: 提出 MLLMsent 框架，用于研究多模态大型语言模型的情感推理能力。


<details>
  <summary>Details</summary>
Motivation: 理解视觉内容如何传达情感至关重要，因为在线互动越来越以社交平台上的这类媒体为主。然而，这仍然是一个具有挑战性的问题，因为情感感知与复杂的场景级语义密切相关。

Method: 提出了一个名为 MLLMsent 的框架，通过三个角度研究多模态大型语言模型 (MLLM) 的情感推理能力：(1) 使用 MLLM 直接从图像进行情感分类；(2) 将 MLLM 与预训练的 LLM 相关联，以对自动生成的图像描述进行情感分析；(3) 在情感标记的图像描述上微调 LLM。

Result: 实验表明，该方案（特别是微调方法）取得了最先进的结果，在不同评估者一致性水平和情感极性类别中，优于基于词典、CNN 和 Transformer 的基线，分别高达 30.9%、64.8% 和 42.4%。

Conclusion: 该模型在跨数据集测试中表现出色，无需在这些新数据上进行任何训练，仍优于已在这些数据上直接训练的最佳模型。

Abstract: Understanding how visual content communicates sentiment is critical in an era
where online interaction is increasingly dominated by this kind of media on
social platforms. However, this remains a challenging problem, as sentiment
perception is closely tied to complex, scene-level semantics. In this paper, we
propose an original framework, MLLMsent, to investigate the sentiment reasoning
capabilities of Multimodal Large Language Models (MLLMs) through three
perspectives: (1) using those MLLMs for direct sentiment classification from
images; (2) associating them with pre-trained LLMs for sentiment analysis on
automatically generated image descriptions; and (3) fine-tuning the LLMs on
sentiment-labeled image descriptions. Experiments on a recent and established
benchmark demonstrate that our proposal, particularly the fine-tuned approach,
achieves state-of-the-art results outperforming Lexicon-, CNN-, and
Transformer-based baselines by up to 30.9%, 64.8%, and 42.4%, respectively,
across different levels of evaluators' agreement and sentiment polarity
categories. Remarkably, in a cross-dataset test, without any training on these
new data, our model still outperforms, by up to 8.26%, the best runner-up,
which has been trained directly on them. These results highlight the potential
of the proposed visual reasoning scheme for advancing affective computing,
while also establishing new benchmarks for future research.

</details>


### [69] [AWM-Fuse: Multi-Modality Image Fusion for Adverse Weather via Global and Local Text Perception](https://arxiv.org/abs/2508.16881)
*Xilai Li,Huichun Liu,Xiaosong Li,Tao Ye,Zhenyu Kuang,Huafeng Li*

Main category: cs.CV

TL;DR: 提出了AWM-Fuse，一种用于恶劣天气条件下的新融合方法，它利用BLIP和ChatGPT产生的文本信息来提高语义感知，并在复杂天气条件和下游任务中优于当前最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 旨在解决由天气相关的退化引起的视觉信息丢失，提供更清晰的场景表示。 现有研究较少尝试纳入文本信息以提高语义感知，并且通常缺乏对文本内容的有效分类和透彻分析。

Method: 提出了一种新的融合方法AWM-Fuse，用于处理恶劣天气条件，该方法通过统一的共享权重架构中的全局和局部文本感知来处理多种退化。

Result: AWM-Fuse在复杂天气条件和下游任务中优于当前最先进的方法。

Conclusion: AWM-Fuse在复杂天气条件和下游任务中优于当前最先进的方法。

Abstract: Multi-modality image fusion (MMIF) in adverse weather aims to address the
loss of visual information caused by weather-related degradations, providing
clearer scene representations. Although less studies have attempted to
incorporate textual information to improve semantic perception, they often lack
effective categorization and thorough analysis of textual content. In response,
we propose AWM-Fuse, a novel fusion method for adverse weather conditions,
designed to handle multiple degradations through global and local text
perception within a unified, shared weight architecture. In particular, a
global feature perception module leverages BLIP-produced captions to extract
overall scene features and identify primary degradation types, thus promoting
generalization across various adverse weather conditions. Complementing this,
the local module employs detailed scene descriptions produced by ChatGPT to
concentrate on specific degradation effects through concrete textual cues,
thereby capturing finer details. Furthermore, textual descriptions are used to
constrain the generation of fusion images, effectively steering the network
learning process toward better alignment with real semantic labels, thereby
promoting the learning of more meaningful visual features. Extensive
experiments demonstrate that AWM-Fuse outperforms current state-of-the-art
methods in complex weather conditions and downstream tasks. Our code is
available at https://github.com/Feecuin/AWM-Fuse.

</details>


### [70] [A Lightweight Convolution and Vision Transformer integrated model with Multi-scale Self-attention Mechanism](https://arxiv.org/abs/2508.16884)
*Yi Zhang,Lingxiao Wei,Bowei Zhang,Ziwei Liu,Kai Yi,Shu Hu*

Main category: cs.CV

TL;DR: 提出了一种轻量级的ViT模型SAEViT，它具有卷积块，以实现高效的下游视觉任务。


<details>
  <summary>Details</summary>
Motivation: Vision Transformer (ViT) 在计算机视觉任务中很流行，因为它具有强大的远程依赖建模能力。然而，它的大模型尺寸、高计算成本和较弱的局部特征建模能力阻碍了其在实际场景中的应用。为了平衡计算效率和性能。

Method: 提出了SAEViT（稀疏注意力高效ViT），这是一种基于卷积块的轻量级ViT模型。它引入了稀疏聚合注意力（SAA）模块，该模块执行基于图像冗余的自适应稀疏采样，并通过反卷积操作恢复特征图。此外，还开发了一种通道交互前馈网络（CIFFN）层，以通过特征分解和重新分配来增强通道间信息交换。最后，设计了一种具有嵌入式深度可分离卷积块（DWSConv）的分层金字塔结构，以进一步加强卷积特征。

Result: SAEViT实现了Top-1 76.3%和79.6%的精度

Conclusion: SAEViT在ImageNet-1K分类任务上分别以0.8 GFLOPs和1.3 GFLOPs实现了76.3%和79.6%的Top-1 准确率，证明了其作为各种基本视觉任务的轻量级解决方案。

Abstract: Vision Transformer (ViT) has prevailed in computer vision tasks due to its
strong long-range dependency modelling ability. However, its large model size
with high computational cost and weak local feature modeling ability hinder its
application in real scenarios. To balance computation efficiency and
performance, we propose SAEViT (Sparse-Attention-Efficient-ViT), a lightweight
ViT based model with convolution blocks, in this paper to achieve efficient
downstream vision tasks. Specifically, SAEViT introduces a Sparsely Aggregated
Attention (SAA) module that performs adaptive sparse sampling based on image
redundancy and recovers the feature map via deconvolution operation, which
significantly reduces the computational complexity of attention operations. In
addition, a Channel-Interactive Feed-Forward Network (CIFFN) layer is developed
to enhance inter-channel information exchange through feature decomposition and
redistribution, mitigating redundancy in traditional feed-forward networks
(FNN). Finally, a hierarchical pyramid structure with embedded depth-wise
separable convolutional blocks (DWSConv) is devised to further strengthen
convolutional features. Extensive experiments on mainstream datasets show that
SAEViT achieves Top-1 accuracies of 76.3\% and 79.6\% on the ImageNet-1K
classification task with only 0.8 GFLOPs and 1.3 GFLOPs, respectively,
demonstrating a lightweight solution for various fundamental vision tasks.

</details>


### [71] [MDIQA: Unified Image Quality Assessment for Multi-dimensional Evaluation and Restoration](https://arxiv.org/abs/2508.16887)
*Shunyu Yao,Ming Liu,Zhilu Zhang,Zhaolin Wan,Zhilong Ji,Jinfeng Bai,Wangmeng Zuo*

Main category: cs.CV

TL;DR: a multi-dimensional image quality assessment (MDIQA) framework is proposed to model image quality across various perceptual dimensions. MDIQA achieves superior performance and can be effectively and flexibly applied to image restoration tasks.


<details>
  <summary>Details</summary>
Motivation: most existing methods are obsessed with fitting the overall score, neglecting the fact that humans typically evaluate image quality from different dimensions before arriving at an overall quality assessment.

Method: model image quality across various perceptual dimensions, including five technical and four aesthetic dimensions, to capture the multifaceted nature of human visual perception within distinct branches. Each branch of our MDIQA is initially trained under the guidance of a separate dimension, and the respective features are then amalgamated to generate the final IQA score.

Result: achieves superior performance

Conclusion: MDIQA achieves superior performance and can be effectively and flexibly applied to image restoration tasks.

Abstract: Recent advancements in image quality assessment (IQA), driven by
sophisticated deep neural network designs, have significantly improved the
ability to approach human perceptions. However, most existing methods are
obsessed with fitting the overall score, neglecting the fact that humans
typically evaluate image quality from different dimensions before arriving at
an overall quality assessment. To overcome this problem, we propose a
multi-dimensional image quality assessment (MDIQA) framework. Specifically, we
model image quality across various perceptual dimensions, including five
technical and four aesthetic dimensions, to capture the multifaceted nature of
human visual perception within distinct branches. Each branch of our MDIQA is
initially trained under the guidance of a separate dimension, and the
respective features are then amalgamated to generate the final IQA score.
Additionally, when the MDIQA model is ready, we can deploy it for a flexible
training of image restoration (IR) models, enabling the restoration results to
better align with varying user preferences through the adjustment of perceptual
dimension weights. Extensive experiments demonstrate that our MDIQA achieves
superior performance and can be effectively and flexibly applied to image
restoration tasks. The code is available: https://github.com/YaoShunyu19/MDIQA.

</details>


### [72] [Structural Energy-Guided Sampling for View-Consistent Text-to-3D](https://arxiv.org/abs/2508.16917)
*Qing Zhang,Jinguang Tong,Jie Hong,Jing Zhang,Xuesong Li*

Main category: cs.CV

TL;DR: SEGS是一种无需训练的框架，可在采样时强制执行多视图一致性，从而减少Janus伪影并改进几何对齐。


<details>
  <summary>Details</summary>
Motivation: Text-to-3D生成通常会受到Janus问题的影响，即对象从正面看起来是正确的，但从其他角度会崩溃成重复或扭曲的几何体。我们将此失败归因于2D扩散先验中的视点偏差，该偏差会传播到3D优化中。

Method: 我们提出了结构能量引导采样（SEGS），这是一个无需训练的即插即用框架，它完全在采样时强制执行多视图一致性。SEGS在中间U-Net特征的PCA子空间中定义了一个结构能量，并将其梯度注入到去噪轨迹中，从而将几何体导向预期的视点，同时保持外观保真度。

Result: SEGS显著减少了Janus伪影，实现了改进的几何对齐和视点一致性

Conclusion: SEGS显著减少了Janus伪影，实现了改进的几何对齐和视点一致性，无需重新训练或修改权重。

Abstract: Text-to-3D generation often suffers from the Janus problem, where objects
look correct from the front but collapse into duplicated or distorted geometry
from other angles. We attribute this failure to viewpoint bias in 2D diffusion
priors, which propagates into 3D optimization. To address this, we propose
Structural Energy-Guided Sampling (SEGS), a training-free, plug-and-play
framework that enforces multi-view consistency entirely at sampling time. SEGS
defines a structural energy in a PCA subspace of intermediate U-Net features
and injects its gradients into the denoising trajectory, steering geometry
toward the intended viewpoint while preserving appearance fidelity. Integrated
seamlessly into SDS/VSD pipelines, SEGS significantly reduces Janus artifacts,
achieving improved geometric alignment and viewpoint consistency without
retraining or weight modification.

</details>


### [73] [MSPCaps: A Multi-Scale Patchify Capsule Network with Cross-Agreement Routing for Visual Recognition](https://arxiv.org/abs/2508.16922)
*Yudong Hu,Yueju Han,Rui Sun,Jinke Ren*

Main category: cs.CV

TL;DR: MSPCaps, a novel architecture that integrates multi-scale feature learning and efficient capsule routing, achieves remarkable scalability and superior robustness.


<details>
  <summary>Details</summary>
Motivation: Existing CapsNet and variants often rely on a single high-level feature map, overlooking the rich complementary information from multi-scale features. Furthermore, conventional feature fusion strategies struggle to reconcile multi-scale feature discrepancies, leading to suboptimal classification performance.

Method: The Multi-Scale Patchify Capsule Network (MSPCaps), integrates multi-scale feature learning and efficient capsule routing. Specifically, MSPCaps consists of three key components: a Multi-Scale ResNet Backbone (MSRB), a Patchify Capsule Layer (PatchifyCaps), and Cross-Agreement Routing (CAR) blocks.

Result: MSPCaps consistently surpasses multiple baseline methods in terms of classification accuracy.

Conclusion: MSPCaps achieves remarkable scalability and superior robustness, consistently surpassing multiple baseline methods in terms of classification accuracy, with configurations ranging from a highly efficient Tiny model (344.3K parameters) to a powerful Large model (10.9M parameters), highlighting its potential in advancing feature representation learning.

Abstract: Capsule Network (CapsNet) has demonstrated significant potential in visual
recognition by capturing spatial relationships and part-whole hierarchies for
learning equivariant feature representations. However, existing CapsNet and
variants often rely on a single high-level feature map, overlooking the rich
complementary information from multi-scale features. Furthermore, conventional
feature fusion strategies (e.g., addition and concatenation) struggle to
reconcile multi-scale feature discrepancies, leading to suboptimal
classification performance. To address these limitations, we propose the
Multi-Scale Patchify Capsule Network (MSPCaps), a novel architecture that
integrates multi-scale feature learning and efficient capsule routing.
Specifically, MSPCaps consists of three key components: a Multi-Scale ResNet
Backbone (MSRB), a Patchify Capsule Layer (PatchifyCaps), and Cross-Agreement
Routing (CAR) blocks. First, the MSRB extracts diverse multi-scale feature
representations from input images, preserving both fine-grained details and
global contextual information. Second, the PatchifyCaps partitions these
multi-scale features into primary capsules using a uniform patch size,
equipping the model with the ability to learn from diverse receptive fields.
Finally, the CAR block adaptively routes the multi-scale capsules by
identifying cross-scale prediction pairs with maximum agreement. Unlike the
simple concatenation of multiple self-routing blocks, CAR ensures that only the
most coherent capsules contribute to the final voting. Our proposed MSPCaps
achieves remarkable scalability and superior robustness, consistently
surpassing multiple baseline methods in terms of classification accuracy, with
configurations ranging from a highly efficient Tiny model (344.3K parameters)
to a powerful Large model (10.9M parameters), highlighting its potential in
advancing feature representation learning.

</details>


### [74] [LGE-Guided Cross-Modality Contrastive Learning for Gadolinium-Free Cardiomyopathy Screening in Cine CMR](https://arxiv.org/abs/2508.16927)
*Siqing Yuan,Yulin Wang,Zirui Cao,Yueyan Wang,Zehao Weng,Hui Wang,Lei Xu,Zixian Chen,Lei Chen,Zhong Xue,Dinggang Shen*

Main category: cs.CV

TL;DR: CC-CMR is a gadolinium-free cardiomyopathy screening tool using cine CMR that achieves high accuracy by aligning different CMR sequences and using adaptive training.


<details>
  <summary>Details</summary>
Motivation: Cardiomyopathy requires precise early screening, but CMR's reliance on gadolinium and labor-intensive interpretation limits its use.

Method: A Contrastive Learning and Cross-Modal alignment framework (CC-CMR) is proposed to align cine CMR and LGE sequences, encoding fibrosis-specific pathology into cine CMR embeddings. An uncertainty-guided adaptive training mechanism dynamically calibrates task-specific objectives.

Result: CC-CMR achieves an accuracy of 0.943 (95% CI: 0.886-0.986) on multi-center data from 231 subjects, outperforming state-of-the-art cine-CMR-only models by 4.3%.

Conclusion: CC-CMR achieves high accuracy in cardiomyopathy screening using cine CMR, outperforming existing models and eliminating gadolinium dependency.

Abstract: Cardiomyopathy, a principal contributor to heart failure and sudden cardiac
mortality, demands precise early screening. Cardiac Magnetic Resonance (CMR),
recognized as the diagnostic 'gold standard' through multiparametric protocols,
holds the potential to serve as an accurate screening tool. However, its
reliance on gadolinium contrast and labor-intensive interpretation hinders
population-scale deployment. We propose CC-CMR, a Contrastive Learning and
Cross-Modal alignment framework for gadolinium-free cardiomyopathy screening
using cine CMR sequences. By aligning the latent spaces of cine CMR and Late
Gadolinium Enhancement (LGE) sequences, our model encodes fibrosis-specific
pathology into cine CMR embeddings. A Feature Interaction Module concurrently
optimizes diagnostic precision and cross-modal feature congruence, augmented by
an uncertainty-guided adaptive training mechanism that dynamically calibrates
task-specific objectives to ensure model generalizability. Evaluated on
multi-center data from 231 subjects, CC-CMR achieves accuracy of 0.943 (95% CI:
0.886-0.986), outperforming state-of-the-art cine-CMR-only models by 4.3% while
eliminating gadolinium dependency, demonstrating its clinical viability for
wide range of populations and healthcare environments.

</details>


### [75] [Align 3D Representation and Text Embedding for 3D Content Personalization](https://arxiv.org/abs/2508.16932)
*Qi Song,Ziyuan Luo,Ka Chun Cheung,Simon See,Renjie Wan*

Main category: cs.CV

TL;DR: Invert3D通过建立3D表示和文本嵌入空间之间的对齐，实现了方便的3D内容个性化，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 高效的生成3D内容的个性化仍然是一个关键挑战。当前的3D个性化方法主要依赖于基于知识蒸馏的方法，这需要计算成本高昂的重训练过程。

Method: 我们开发了一种相机条件3D到文本的逆向机制，将3D内容投影到与文本嵌入对齐的3D嵌入中。

Result: Invert3D实现了有效的3D内容个性化，消除了对计算重训练过程的需要。

Conclusion: Invert3D实现了有效的3D内容个性化。

Abstract: Recent advances in NeRF and 3DGS have significantly enhanced the efficiency
and quality of 3D content synthesis. However, efficient personalization of
generated 3D content remains a critical challenge. Current 3D personalization
approaches predominantly rely on knowledge distillation-based methods, which
require computationally expensive retraining procedures. To address this
challenge, we propose \textbf{Invert3D}, a novel framework for convenient 3D
content personalization. Nowadays, vision-language models such as CLIP enable
direct image personalization through aligned vision-text embedding spaces.
However, the inherent structural differences between 3D content and 2D images
preclude direct application of these techniques to 3D personalization. Our
approach bridges this gap by establishing alignment between 3D representations
and text embedding spaces. Specifically, we develop a camera-conditioned
3D-to-text inverse mechanism that projects 3D contents into a 3D embedding
aligned with text embeddings. This alignment enables efficient manipulation and
personalization of 3D content through natural language prompts, eliminating the
need for computationally retraining procedures. Extensive experiments
demonstrate that Invert3D achieves effective personalization of 3D content. Our
work is available at: https://github.com/qsong2001/Invert3D.

</details>


### [76] [Addressing Annotation Scarcity in Hyperspectral Brain Image Segmentation with Unsupervised Domain Adaptation](https://arxiv.org/abs/2508.16934)
*Tim Mach,Daniel Rueckert,Alex Berger,Laurin Lux,Ivan Ezhov*

Main category: cs.CV

TL;DR: presents a novel deep learning framework for segmenting cerebral vasculature in hyperspectral brain images


<details>
  <summary>Details</summary>
Motivation: address the critical challenge of severe label scarcity, which impedes conventional supervised training

Method: a novel unsupervised domain adaptation methodology, using a small, expert-annotated ground truth alongside unlabeled data

Result: significantly outperforms existing state-of-the-art approaches

Conclusion: demonstrates the efficacy of domain adaptation for label-scarce biomedical imaging tasks

Abstract: This work presents a novel deep learning framework for segmenting cerebral
vasculature in hyperspectral brain images. We address the critical challenge of
severe label scarcity, which impedes conventional supervised training. Our
approach utilizes a novel unsupervised domain adaptation methodology, using a
small, expert-annotated ground truth alongside unlabeled data. Quantitative and
qualitative evaluations confirm that our method significantly outperforms
existing state-of-the-art approaches, demonstrating the efficacy of domain
adaptation for label-scarce biomedical imaging tasks.

</details>


### [77] [NAT: Learning to Attack Neurons for Enhanced Adversarial Transferability](https://arxiv.org/abs/2508.16937)
*Krishna Kanth Nakka,Alexandre Alahi*

Main category: cs.CV

TL;DR: NAT targets specific neurons for transferable adversarial perturbations, improving fooling rates compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Previous layer-level optimizations often disproportionately focus on a few neurons representing similar concepts, leaving other neurons within the attacked layer minimally affected.

Method: Neuron Attack for Transferability (NAT), a method designed to target specific neurons within the embedding.

Result: Targeting individual neurons effectively disrupts the core units of the neural network, providing a common basis for transferability across different models. NAT achieves fooling rates that surpass existing baselines by over 14% in cross-model and 4% in cross-domain settings.

Conclusion: NAT achieves state-of-the-art fooling rates on diverse ImageNet models and fine-grained models, surpassing existing baselines by over 14% in cross-model and 4% in cross-domain settings. It also achieves impressive fooling rates within just 10 queries by leveraging complementary attacking capabilities.

Abstract: The generation of transferable adversarial perturbations typically involves
training a generator to maximize embedding separation between clean and
adversarial images at a single mid-layer of a source model. In this work, we
build on this approach and introduce Neuron Attack for Transferability (NAT), a
method designed to target specific neuron within the embedding. Our approach is
motivated by the observation that previous layer-level optimizations often
disproportionately focus on a few neurons representing similar concepts,
leaving other neurons within the attacked layer minimally affected. NAT shifts
the focus from embedding-level separation to a more fundamental,
neuron-specific approach. We find that targeting individual neurons effectively
disrupts the core units of the neural network, providing a common basis for
transferability across different models. Through extensive experiments on 41
diverse ImageNet models and 9 fine-grained models, NAT achieves fooling rates
that surpass existing baselines by over 14\% in cross-model and 4\% in
cross-domain settings. Furthermore, by leveraging the complementary attacking
capabilities of the trained generators, we achieve impressive fooling rates
within just 10 queries. Our code is available at:
https://krishnakanthnakka.github.io/NAT/

</details>


### [78] [HieroAction: Hierarchically Guided VLM for Fine-Grained Action Analysis](https://arxiv.org/abs/2508.16942)
*Junhao Wu,Xiuer Gu,Zhiying Li,Yeying Jin,Yunfeng Diao,Zhiyu Li,Zhenbo Song,Xiaomei Zhang,Zhaoxin Fan*

Main category: cs.CV

TL;DR: HieroAction, a vision-language model, accurately and structurally assesses human actions using Stepwise Action Reasoning and Hierarchical Policy Learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Most existing methods provide only a final score without explanation or detailed analysis, limiting their practical applicability.

Method: HieroAction builds on Stepwise Action Reasoning and Hierarchical Policy Learning.

Result: HieroAction delivers accurate and structured assessments of human actions.

Conclusion: HieroAction demonstrated superior performance across multiple benchmark datasets.

Abstract: Evaluating human actions with clear and detailed feedback is important in
areas such as sports, healthcare, and robotics, where decisions rely not only
on final outcomes but also on interpretable reasoning. However, most existing
methods provide only a final score without explanation or detailed analysis,
limiting their practical applicability. To address this, we introduce
HieroAction, a vision-language model that delivers accurate and structured
assessments of human actions. HieroAction builds on two key ideas: (1) Stepwise
Action Reasoning, a tailored chain of thought process designed specifically for
action assessment, which guides the model to evaluate actions step by step,
from overall recognition through sub action analysis to final scoring, thus
enhancing interpretability and structured understanding; and (2) Hierarchical
Policy Learning, a reinforcement learning strategy that enables the model to
learn fine grained sub action dynamics and align them with high level action
quality, thereby improving scoring precision. The reasoning pathway structures
the evaluation process, while policy learning refines each stage through reward
based optimization. Their integration ensures accurate and interpretable
assessments, as demonstrated by superior performance across multiple benchmark
datasets. Code will be released upon acceptance.

</details>


### [79] [RPD-Diff: Region-Adaptive Physics-Guided Diffusion Model for Visibility Enhancement under Dense and Non-Uniform Haze](https://arxiv.org/abs/2508.16956)
*Ruicheng Zhang,Puxin Yan,Zeyu Zhang,Yicheng Chang,Hongyi Chen,Zhi Jin*

Main category: cs.CV

TL;DR: RPD-Diff是一种新的去雾扩散模型，它使用物理先验和自适应去噪来提高浓雾和非均匀雾霾场景中的图像质量。


<details>
  <summary>Details</summary>
Motivation: 由于严重的信息退化和空间异质性，在密集和非均匀雾霾条件下进行单图像去雾仍然具有挑战性。传统的基于扩散的去雾方法难以应对不足的生成条件和缺乏对空间变化雾霾分布的适应性，这导致次优的恢复。

Method: RPD-Diff，一种区域自适应物理引导的去雾扩散模型，用于在复杂的雾霾场景中实现鲁棒的能见度增强。RPD-Diff引入了一种物理引导的中间状态目标（PIST）策略，该策略利用物理先验通过生成目标转换来重新制定扩散马尔可夫链，从而缓解了浓雾场景中条件不足的问题。此外，雾霾感知去噪时间步长预测器（HADTP）采用传输图交叉注意机制动态调整特定于补丁的去噪时间步长，从而巧妙地管理非均匀雾霾分布。

Result: 在四个真实世界数据集上进行的大量实验表明，

Conclusion: RPD-Diff在具有挑战性的浓雾和非均匀雾霾场景中实现了最先进的性能，提供高质量、无雾霾的图像，具有卓越的细节清晰度和色彩保真度。

Abstract: Single-image dehazing under dense and non-uniform haze conditions remains
challenging due to severe information degradation and spatial heterogeneity.
Traditional diffusion-based dehazing methods struggle with insufficient
generation conditioning and lack of adaptability to spatially varying haze
distributions, which leads to suboptimal restoration. To address these
limitations, we propose RPD-Diff, a Region-adaptive Physics-guided Dehazing
Diffusion Model for robust visibility enhancement in complex haze scenarios.
RPD-Diff introduces a Physics-guided Intermediate State Targeting (PIST)
strategy, which leverages physical priors to reformulate the diffusion Markov
chain by generation target transitions, mitigating the issue of insufficient
conditioning in dense haze scenarios. Additionally, the Haze-Aware Denoising
Timestep Predictor (HADTP) dynamically adjusts patch-specific denoising
timesteps employing a transmission map cross-attention mechanism, adeptly
managing non-uniform haze distributions. Extensive experiments across four
real-world datasets demonstrate that RPD-Diff achieves state-of-the-art
performance in challenging dense and non-uniform haze scenarios, delivering
high-quality, haze-free images with superior detail clarity and color fidelity.

</details>


### [80] [Local Information Matters: A Rethink of Crowd Counting](https://arxiv.org/abs/2508.16970)
*Tianhang Pan,Xiuyi Jia*

Main category: cs.CV

TL;DR: propose a new model design principle of crowd counting: emphasizing local modeling capability of the model


<details>
  <summary>Details</summary>
Motivation: individuals in the crowd counting task typically occupy a very small portion of the image. This characteristic has never been the focus of existing works

Method: a window partitioning design that applies grid windows to the model input, and a window-wise contrastive learning design

Result: shows a significant improvement in local modeling capability (8.7% in MAE on the JHU-Crowd++ high-density subset for example)

Conclusion: The proposed model shows a significant improvement in local modeling capability without compromising its ability to count large-sized ones, which achieves state-of-the-art performance.

Abstract: The motivation of this paper originates from rethinking an essential
characteristic of crowd counting: individuals (heads of humans) in the crowd
counting task typically occupy a very small portion of the image. This
characteristic has never been the focus of existing works: they typically use
the same backbone as other visual tasks and pursue a large receptive field.
This drives us to propose a new model design principle of crowd counting:
emphasizing local modeling capability of the model. We follow the principle and
design a crowd counting model named Local Information Matters Model (LIMM). The
main innovation lies in two strategies: a window partitioning design that
applies grid windows to the model input, and a window-wise contrastive learning
design to enhance the model's ability to distinguish between local density
levels. Moreover, a global attention module is applied to the end of the model
to handle the occasionally occurring large-sized individuals. Extensive
experiments on multiple public datasets illustrate that the proposed model
shows a significant improvement in local modeling capability (8.7\% in MAE on
the JHU-Crowd++ high-density subset for example), without compromising its
ability to count large-sized ones, which achieves state-of-the-art performance.
Code is available at: https://github.com/tianhangpan/LIMM.

</details>


### [81] [Robust Diagram Reasoning: A Framework for Enhancing LVLM Performance on Visually Perturbed Scientific Diagrams](https://arxiv.org/abs/2508.16972)
*Minghao Zhou,Rafael Souza,Yaqian Hu,Luming Che*

Main category: cs.CV

TL;DR: LVLMs lack robustness to visual perturbations in scientific diagrams. Introduced RDR framework, SciDiagram-Robust dataset, and new metrics to address this.


<details>
  <summary>Details</summary>
Motivation: Practical deployment of LLMs and LVLMs is hindered by a critical lack of robustness to common visual perturbations such as noise, blur, and occlusions, which are prevalent in real-world scientific documents.

Method: Adaptive Multi-View & Consistency Verification (AMCV) mechanism, which involves generating multiple perturbed versions of a diagram, performing parallel inference, and then applying a consistency-based self-correction loop.

Result: Introduced Robust Diagram Reasoning (RDR) framework and SciDiagram-Robust dataset. Proposed two new metrics, Perturbation Robustness Score (PRS) and Visual Degradation Consistency (VDC), to quantify robustness.

Conclusion: Even state-of-the-art LVLMs like GPT-4V exhibit significant performance degradation when faced with perturbed inputs (Clean Accuracy 85.2% vs. PRS 72.1%).

Abstract: Large Language Models (LLMs) and their multimodal variants (LVLMs) hold
immense promise for scientific and engineering applications, particularly in
processing visual information like scientific diagrams. However, their
practical deployment is hindered by a critical lack of robustness to common
visual perturbations such as noise, blur, and occlusions, which are prevalent
in real-world scientific documents. Existing evaluation benchmarks largely
overlook this challenge, leaving the robust reasoning capabilities of LVLMs on
visually degraded scientific diagrams underexplored. To address this, we
introduce the Robust Diagram Reasoning (RDR) framework, a novel approach
designed to enhance and rigorously evaluate LVLMs' performance under such
conditions. At its core, RDR employs an Adaptive Multi-View & Consistency
Verification (AMCV) mechanism, which involves generating multiple perturbed
versions of a diagram, performing parallel inference, and then applying a
consistency-based self-correction loop. We also propose two new metrics,
Perturbation Robustness Score (PRS) and Visual Degradation Consistency (VDC),
to quantify robustness. Furthermore, we construct SciDiagram-Robust, the first
large-scale scientific diagram question-answering dataset specifically
augmented with diverse, programmatically generated visual perturbations. Our
extensive experiments demonstrate that even state-of-the-art closed-source
LVLMs like GPT-4V exhibit significant performance degradation when faced with
perturbed inputs (Clean Accuracy 85.2% vs. PRS 72.1%).

</details>


### [82] [Balanced Sharpness-Aware Minimization for Imbalanced Regression](https://arxiv.org/abs/2508.16973)
*Yahao Liu,Qin Wang,Lixin Duan,Wen Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的不平衡回归方法BSAM，该方法通过在观察空间中强制执行均匀泛化能力来解决不平衡回归问题，并在多个视觉回归任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据通常表现出不平衡的分布，导致回归模型表现不佳，特别是对于具有稀有观察的目标值（称为不平衡回归问题）。本文将不平衡回归重新定义为不平衡泛化问题。

Method: 提出了一种名为平衡锐度感知最小化（BSAM）的方法，以增强回归模型在整个观察空间中的均匀泛化能力。该方法从传统的锐度感知最小化开始，然后引入一种新的目标重加权策略，以统一整个观察空间中的泛化能力，从而保证了理论泛化界限。

Result: BSAM方法在多个视觉回归任务（包括年龄和深度估计）上始终优于现有方法。

Conclusion: BSAM方法在多个视觉回归任务（包括年龄和深度估计）上始终优于现有方法。

Abstract: Regression is fundamental in computer vision and is widely used in various
tasks including age estimation, depth estimation, target localization, \etc
However, real-world data often exhibits imbalanced distribution, making
regression models perform poorly especially for target values with rare
observations~(known as the imbalanced regression problem). In this paper, we
reframe imbalanced regression as an imbalanced generalization problem. To
tackle that, we look into the loss sharpness property for measuring the
generalization ability of regression models in the observation space. Namely,
given a certain perturbation on the model parameters, we check how model
performance changes according to the loss values of different target
observations. We propose a simple yet effective approach called Balanced
Sharpness-Aware Minimization~(BSAM) to enforce the uniform generalization
ability of regression models for the entire observation space. In particular,
we start from the traditional sharpness-aware minimization and then introduce a
novel targeted reweighting strategy to homogenize the generalization ability
across the observation space, which guarantees a theoretical generalization
bound. Extensive experiments on multiple vision regression tasks, including age
and depth estimation, demonstrate that our BSAM method consistently outperforms
existing approaches. The code is available
\href{https://github.com/manmanjun/BSAM_for_Imbalanced_Regression}{here}.

</details>


### [83] [Hierarchical Contextual Grounding LVLM: Enhancing Fine-Grained Visual-Language Understanding with Robust Grounding](https://arxiv.org/abs/2508.16974)
*Leilei Guo,Antonio Carlos Rivera,Peiyu Tang,Haoxuan Ren,Zheyu Song*

Main category: cs.CV

TL;DR: HCG-LVLM 是一种新的 LVLM 架构，它通过分层方法提高了视觉语言理解能力，减少了幻觉，并在多个数据集上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的 LVLM 在复杂的现实场景中，尤其是在需要精确的图像区域定位和细粒度的视觉推理时，通常表现出不足的鲁棒性、容易产生幻觉和推理错误。

Method: 分层上下文 grounding LVLM (HCG-LVLM)，一种模仿人类由粗到精认知处理的新颖架构，采用双层方法：用于初始广泛理解的全局上下文感知层和细粒度局部 grounding 层。

Result: 在具有挑战性的数据集（包括 GQA、A-OKVQA 和 RefCOCO/+/g）上的大量实验表明，HCG-LVLM 始终优于最先进的模型，例如 Flamingo、BLIP-2 和 MiniGPT-4。我们的模型实现了卓越的准确性并显着减少了幻觉。

Conclusion: HCG-LVLM在精细的视觉语言理解和精确的 grounding 能力方面表现出色，优于现有模型，并减少了幻觉。

Abstract: Large Language Models (LLMs) and Vision-Language Large Models (LVLMs) have
achieved remarkable progress in natural language processing and multimodal
understanding. Despite their impressive generalization capabilities, current
LVLMs often exhibit insufficient robustness, proneness to hallucination, and
reasoning errors in complex real-world scenarios, particularly when precise
image region localization and fine-grained visual reasoning are required. To
address these limitations, we propose the Hierarchical Contextual Grounding
LVLM (HCG-LVLM), a novel architecture that mimics human coarse-to-fine
cognitive processing. HCG-LVLM employs a two-layered approach: a Global
Contextual Perception layer for initial broad understanding and a Fine-grained
Local Grounding layer. The latter incorporates a Local Detail Enhancement
Module to extract high-resolution features and a Semantic Consistency Validator
to ensure accurate, hallucination-free visual-language alignment. Through an
adaptive fusion mechanism, information from both layers is integrated for
robust and precise outputs. Extensive experiments on challenging datasets,
including GQA, A-OKVQA for fine-grained VQA, and RefCOCO/+/g for Referring
Expression Comprehension, demonstrate that HCG-LVLM consistently outperforms
state-of-the-art models such as Flamingo, BLIP-2, and MiniGPT-4. Our model
achieves superior accuracy and significantly reduces hallucination, validating
the effectiveness of its hierarchical design in enhancing fine-grained
visual-language understanding and precise grounding capabilities.

</details>


### [84] [Combating Digitally Altered Images: Deepfake Detection](https://arxiv.org/abs/2508.16975)
*Saksham Kumar,Rhythm Narang*

Main category: cs.CV

TL;DR: This study presents a robust Deepfake detection based on a modified Vision Transformer(ViT) model, trained to distinguish between real and Deepfake images.


<details>
  <summary>Details</summary>
Motivation: The rise of Deepfake technology to generate hyper-realistic manipulated images and videos poses a significant challenge to the public and relevant authorities.

Method: a modified Vision Transformer(ViT) model, trained to distinguish between real and Deepfake images. The model has been trained on a subset of the OpenForensics Dataset with multiple augmentation techniques to increase robustness for diverse image manipulations. The class imbalance issues are handled by oversampling and a train-validation split of the dataset in a stratified manner.

Result: Performance is evaluated using the accuracy metric on the training and testing datasets, followed by a prediction score on a random image of people, irrespective of their realness.

Conclusion: The model demonstrates state-of-the-art results on the test dataset to meticulously detect Deepfake images.

Abstract: The rise of Deepfake technology to generate hyper-realistic manipulated
images and videos poses a significant challenge to the public and relevant
authorities. This study presents a robust Deepfake detection based on a
modified Vision Transformer(ViT) model, trained to distinguish between real and
Deepfake images. The model has been trained on a subset of the OpenForensics
Dataset with multiple augmentation techniques to increase robustness for
diverse image manipulations. The class imbalance issues are handled by
oversampling and a train-validation split of the dataset in a stratified
manner. Performance is evaluated using the accuracy metric on the training and
testing datasets, followed by a prediction score on a random image of people,
irrespective of their realness. The model demonstrates state-of-the-art results
on the test dataset to meticulously detect Deepfake images.

</details>


### [85] [Preserving Domain Generalization in Fine-Tuning via Joint Parameter Selection](https://arxiv.org/abs/2508.16976)
*Bin Pan,Shiyu Shen,Zongbin Wang,Zhenwei Shi,Xia Xu*

Main category: cs.CV

TL;DR: JPS, a parameter-efficient adaptation strategy, restricts updates to a small, sparse subset of parameters, achieving superior performance in domain generalization.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of full fine-tuning compromising the intrinsic generalization capabilities of pre-trained vision models.

Method: Joint Parameter Selection (JPS), a novel method that restricts updates to a small, sparse subset of parameters.

Result: Theoretically, we establish a generalization error bound that explicitly accounts for the sparsity of parameter updates. Practically, we design a selection mechanism employing dual operators to identify and update parameters exhibiting consistent and significant gradients across all source domains.

Conclusion: JPS achieves superior performance compared to state-of-the-art domain generalization methods.

Abstract: Domain generalization seeks to develop models trained on a limited set of
source domains that are capable of generalizing effectively to unseen target
domains. While the predominant approach leverages large-scale pre-trained
vision models as initialization, recent studies have highlighted that full
fine-tuning can compromise the intrinsic generalization capabilities of these
models. To address this limitation, parameter-efficient adaptation strategies
have emerged, wherein only a subset of model parameters is selectively
fine-tuned, thereby balancing task adaptation with the preservation of
generalization. Motivated by this paradigm, we introduce Joint Parameter
Selection (JPS), a novel method that restricts updates to a small, sparse
subset of parameters, thereby retaining and harnessing the generalization
strength of pre-trained models. Theoretically, we establish a generalization
error bound that explicitly accounts for the sparsity of parameter updates,
thereby providing a principled justification for selective fine-tuning.
Practically, we design a selection mechanism employing dual operators to
identify and update parameters exhibiting consistent and significant gradients
across all source domains. Extensive benchmark experiments demonstrate that JPS
achieves superior performance compared to state-of-the-art domain
generalization methods, substantiating both the efficiency and efficacy of the
proposed approach.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [86] [Revisiting Rule-Based Stuttering Detection: A Comprehensive Analysis of Interpretable Models for Clinical Applications](https://arxiv.org/abs/2508.16681)
*Eric Zhang*

Main category: cs.AI

TL;DR: comprehensive analysis of rule-based stuttering detection systems, synthesizing insights from multiple corpora. Propose an enhanced rule-based framework that achieves competitive performance while maintaining complete interpretability-critical for clinical adoption.


<details>
  <summary>Details</summary>
Motivation: interpretability and transparency are paramount in clinical applications

Method: enhanced rule-based framework that incorporates speaking-rate normalization, multi-level acoustic feature analysis, and hierarchical decision structures

Result: achieves competitive performance while maintaining complete interpretability, excel particularly in prolongation detection (97-99% accuracy) and provide stable performance across varying speaking rates

Conclusion: Rule-based methods offer unique advantages in clinical contexts where decision auditability, patient-specific tuning, and real-time feedback are essential.

Abstract: Stuttering affects approximately 1% of the global population, impacting
communication and quality of life. While recent advances in deep learning have
pushed the boundaries of automatic speech dysfluency detection, rule-based
approaches remain crucial for clinical applications where interpretability and
transparency are paramount. This paper presents a comprehensive analysis of
rule-based stuttering detection systems, synthesizing insights from multiple
corpora including UCLASS, FluencyBank, and SEP-28k. We propose an enhanced
rule-based framework that incorporates speaking-rate normalization, multi-level
acoustic feature analysis, and hierarchical decision structures. Our approach
achieves competitive performance while maintaining complete
interpretability-critical for clinical adoption. We demonstrate that rule-based
systems excel particularly in prolongation detection (97-99% accuracy) and
provide stable performance across varying speaking rates. Furthermore, we show
how these interpretable models can be integrated with modern machine learning
pipelines as proposal generators or constraint modules, bridging the gap
between traditional speech pathology practices and contemporary AI systems. Our
analysis reveals that while neural approaches may achieve marginally higher
accuracy in unconstrained settings, rule-based methods offer unique advantages
in clinical contexts where decision auditability, patient-specific tuning, and
real-time feedback are essential.

</details>


### [87] [Explainable AI for Predicting and Understanding Mathematics Achievement: A Cross-National Analysis of PISA 2018](https://arxiv.org/abs/2508.16747)
*Liu Liu,Rui Dai*

Main category: cs.AI

TL;DR: 本研究利用可解释人工智能分析 PISA 数据，预测数学成绩并识别关键因素，发现非线性模型表现更优，社会经济地位、学习时间等是关键预测因素，且各国情况有所不同。


<details>
  <summary>Details</summary>
Motivation: 理解影响学生数学成绩的因素对于设计有效的教育政策至关重要。

Method: 本研究采用可解释人工智能 (XAI) 技术对 PISA 2018 数据进行分析，以预测数学成绩并识别 10 个国家/地区的关键预测因素。使用了四种模型：多元线性回归 (MLR)、随机森林 (RF)、CATBoost 和人工神经网络 (ANN)，使用了学生、家庭和学校变量。模型在 70% 的数据上进行训练（使用 5 折交叉验证），并在 30% 的数据上进行测试，按国家分层。使用 R^2 和平均绝对误差 (MAE) 评估性能。为了确保可解释性，使用了特征重要性、SHAP 值和决策树可视化。

Result: 非线性模型，尤其是 RF 和 ANN，优于 MLR，其中 RF 平衡了准确性和泛化性。关键预测因素包括社会经济地位、学习时间、教师动机和学生对数学的态度，但它们的影响因国家/地区而异。预测分数与实际分数的散点图等可视化诊断显示，RF 和 CATBoost 与实际表现密切相关。

Conclusion: 研究结果揭示了数学成绩的非线性和情境依赖性，以及可解释人工智能在教育研究中的价值。该研究揭示了跨国模式，为以公平为中心的改革提供了信息，并支持个性化学习策略的开发。

Abstract: Understanding the factors that shape students' mathematics performance is
vital for designing effective educational policies. This study applies
explainable artificial intelligence (XAI) techniques to PISA 2018 data to
predict math achievement and identify key predictors across ten countries
(67,329 students). We tested four models: Multiple Linear Regression (MLR),
Random Forest (RF), CATBoost, and Artificial Neural Networks (ANN), using
student, family, and school variables. Models were trained on 70% of the data
(with 5-fold cross-validation) and tested on 30%, stratified by country.
Performance was assessed with R^2 and Mean Absolute Error (MAE). To ensure
interpretability, we used feature importance, SHAP values, and decision tree
visualizations. Non-linear models, especially RF and ANN, outperformed MLR,
with RF balancing accuracy and generalizability. Key predictors included
socio-economic status, study time, teacher motivation, and students' attitudes
toward mathematics, though their impact varied across countries. Visual
diagnostics such as scatterplots of predicted vs actual scores showed RF and
CATBoost aligned closely with actual performance. Findings highlight the
non-linear and context-dependent nature of achievement and the value of XAI in
educational research. This study uncovers cross-national patterns, informs
equity-focused reforms, and supports the development of personalized learning
strategies.

</details>


### [88] [Evaluation and LLM-Guided Learning of ICD Coding Rationales](https://arxiv.org/abs/2508.16777)
*Mingyang Li,Viktor Schlegel,Tingting Mu,Wuraola Oyewusi,Kai Kang,Goran Nenadic*

Main category: cs.AI

TL;DR: 本文全面评估了 ICD 编码理由的可解释性，并提出了新的理由学习方法来提高模型生成的理由的质量。


<details>
  <summary>Details</summary>
Motivation: 深度学习的最新进展显着提高了 ICD 编码的准确性和效率，但这些模型缺乏可解释性仍然是一个主要限制，会削弱信任和透明度。

Method: 提出了新的理由学习方法来提高模型生成的理由的质量，其中通过提示LLM生成带有/不带有注释示例的理由被用作远程监督信号。

Result: LLM生成的理由与人类专家的理由最为一致。

Conclusion: LLM生成的理由与人类专家的理由最为一致。此外，结合少量人工标注的例子不仅进一步改进了理由生成，而且还增强了理由学习方法。

Abstract: Automated clinical coding involves mapping unstructured text from Electronic
Health Records (EHRs) to standardized code systems such as the International
Classification of Diseases (ICD). While recent advances in deep learning have
significantly improved the accuracy and efficiency of ICD coding, the lack of
explainability in these models remains a major limitation, undermining trust
and transparency. Current explorations about explainability largely rely on
attention-based techniques and qualitative assessments by physicians, yet lack
systematic evaluation using consistent criteria on high-quality rationale
datasets, as well as dedicated approaches explicitly trained to generate
rationales for further enhancing explanation. In this work, we conduct a
comprehensive evaluation of the explainability of the rationales for ICD coding
through two key lenses: faithfulness that evaluates how well explanations
reflect the model's actual reasoning and plausibility that measures how
consistent the explanations are with human expert judgment. To facilitate the
evaluation of plausibility, we construct a new rationale-annotated dataset,
offering denser annotations with diverse granularity and aligns better with
current clinical practice, and conduct evaluation across three types of
rationales of ICD coding. Encouraged by the promising plausibility of
LLM-generated rationales for ICD coding, we further propose new rationale
learning methods to improve the quality of model-generated rationales, where
rationales produced by prompting LLMs with/without annotation examples are used
as distant supervision signals. We empirically find that LLM-generated
rationales align most closely with those of human experts. Moreover,
incorporating few-shot human-annotated examples not only further improves
rationale generation but also enhances rationale-learning approaches.

</details>


### [89] [PuzzleJAX: A Benchmark for Reasoning and Learning](https://arxiv.org/abs/2508.16821)
*Sam Earle,Graham Todd,Yuchen Li,Ahmed Khalifa,Muhammad Umair Nasir,Zehua Jiang,Andrzej Banburski-Fahey,Julian Togelius*

Main category: cs.AI

TL;DR: PuzzleJAX is a GPU-accelerated puzzle game engine that supports rapid benchmarking of tree search, reinforcement learning, and LLM reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: support rapid benchmarking of tree search, reinforcement learning, and LLM reasoning abilities

Method: dynamic compilation of any game expressible in its domain-specific language (DSL). This DSL follows PuzzleScript

Result: validating in PuzzleJAX several hundred of the thousands of games designed in PuzzleScript

Conclusion: PuzzleJAX can naturally express tasks that are both simple and intuitive to understand, yet often deeply challenging to master, requiring a combination of control, planning, and high-level insight.

Abstract: We introduce PuzzleJAX, a GPU-accelerated puzzle game engine and description
language designed to support rapid benchmarking of tree search, reinforcement
learning, and LLM reasoning abilities. Unlike existing GPU-accelerated learning
environments that provide hard-coded implementations of fixed sets of games,
PuzzleJAX allows dynamic compilation of any game expressible in its
domain-specific language (DSL). This DSL follows PuzzleScript, which is a
popular and accessible online game engine for designing puzzle games. In this
paper, we validate in PuzzleJAX several hundred of the thousands of games
designed in PuzzleScript by both professional designers and casual creators
since its release in 2013, thereby demonstrating PuzzleJAX's coverage of an
expansive, expressive, and human-relevant space of tasks. By analyzing the
performance of search, learning, and language models on these games, we show
that PuzzleJAX can naturally express tasks that are both simple and intuitive
to understand, yet often deeply challenging to master, requiring a combination
of control, planning, and high-level insight.

</details>


### [90] [Route-and-Execute: Auditable Model-Card Matching and Specialty-Level Deployment](https://arxiv.org/abs/2508.16839)
*Shayan Vassef,Soorya Ram Shimegekar,Abhay Goyal,Koustuv Saha,Pi Zonooz,Navin Kumar*

Main category: cs.AI

TL;DR: This paper presents a healthcare-first framework that uses a single vision-language model (VLM) in two complementary roles to streamline clinical workflows, improve efficiency, and lower costs.


<details>
  <summary>Details</summary>
Motivation: Clinical workflows are fragmented, lack data-driven model identification and standardized delivery of model outputs, reducing efficiency and raising operational costs.

Method: using a single vision-language model (VLM) in two complementary roles: as an aware model-card matcher and fine-tuning the VLM on specialty-specific datasets

Result: single-model deployment matches or approaches specialized baselines across gastroenterology, hematology, ophthalmology, and pathology

Conclusion: A single vision-language model can both decide and do, reducing effort, shortening monitoring, increasing transparency and lowering integration overhead.

Abstract: Clinical workflows are fragmented as a patchwork of scripts and task-specific
networks that often handle triage, task selection, and model deployment. These
pipelines are rarely streamlined for data science pipeline, reducing efficiency
and raising operational costs. Workflows also lack data-driven model
identification (from imaging/tabular inputs) and standardized delivery of model
outputs. In response, we present a practical, healthcare-first framework that
uses a single vision-language model (VLM) in two complementary roles. First
(Solution 1), the VLM acts as an aware model-card matcher that routes an
incoming image to the appropriate specialist model via a three-stage workflow
(modality -> primary abnormality -> model-card id). Checks are provided by (i)
stagewise prompts that allow early exit via None/Normal/Other and (ii) a
stagewise answer selector that arbitrates between the top-2 candidates at each
stage, reducing the chance of an incorrect selection and aligning the workflow
with clinical risk tolerance. Second (Solution 2), we fine-tune the VLM on
specialty-specific datasets ensuring a single model covers multiple downstream
tasks within each specialty, maintaining performance while simplifying
deployment. Across gastroenterology, hematology, ophthalmology, and pathology,
our single-model deployment matches or approaches specialized baselines.
  Compared with pipelines composed of many task-specific agents, this approach
shows that one VLM can both decide and do. It may reduce effort by data
scientists, shorten monitoring, increase the transparency of model selection
(with per-stage justifications), and lower integration overhead.

</details>


### [91] [Quantifying Sycophancy as Deviations from Bayesian Rationality in LLMs](https://arxiv.org/abs/2508.16846)
*Katherine Atwell,Pedram Heydari,Anthony Sicilia,Malihe Alikhani*

Main category: cs.AI

TL;DR: This paper uses a Bayesian framework to quantify sycophancy in LLMs, finding that LLMs are not Bayesian rational and that sycophancy can increase Bayesian error.


<details>
  <summary>Details</summary>
Motivation: Sycophancy is a documented issue in large language models (LLMs), and is critical to understand in the context of human/AI collaboration. Prior works typically quantify sycophancy by measuring shifts in behavior or impacts on accuracy, but neither metric characterizes shifts in rationality, and accuracy measures can only be used in scenarios with a known ground truth.

Method: utilize a Bayesian framework to quantify sycophancy as deviations from rational behavior when presented with user perspectives

Result: LLMs are not Bayesian rational, probing for sycophancy results in significant increases to the predicted posterior in favor of the steered outcome, sycophancy sometimes results in increased Bayesian error, and in a small number of cases actually decreases error, and changes in Bayesian error due to sycophancy are not strongly correlated in Brier score

Conclusion: LLMs are not Bayesian rational, probing for sycophancy results in significant increases to the predicted posterior in favor of the steered outcome, sycophancy sometimes results in increased Bayesian error, and in a small number of cases actually decreases error, and changes in Bayesian error due to sycophancy are not strongly correlated in Brier score, suggesting that studying the impact of sycophancy on ground truth alone does not fully capture errors in reasoning due to sycophancy.

Abstract: Sycophancy, or overly agreeable or flattering behavior, is a documented issue
in large language models (LLMs), and is critical to understand in the context
of human/AI collaboration. Prior works typically quantify sycophancy by
measuring shifts in behavior or impacts on accuracy, but neither metric
characterizes shifts in rationality, and accuracy measures can only be used in
scenarios with a known ground truth. In this work, we utilize a Bayesian
framework to quantify sycophancy as deviations from rational behavior when
presented with user perspectives, thus distinguishing between rational and
irrational updates based on the introduction of user perspectives. In
comparison to other methods, this approach allows us to characterize excessive
behavioral shifts, even for tasks that involve inherent uncertainty or do not
have a ground truth. We study sycophancy for 3 different tasks, a combination
of open-source and closed LLMs, and two different methods for probing
sycophancy. We also experiment with multiple methods for eliciting probability
judgments from LLMs. We hypothesize that probing LLMs for sycophancy will cause
deviations in LLMs' predicted posteriors that will lead to increased Bayesian
error. Our findings indicate that: 1) LLMs are not Bayesian rational, 2)
probing for sycophancy results in significant increases to the predicted
posterior in favor of the steered outcome, 3) sycophancy sometimes results in
increased Bayesian error, and in a small number of cases actually decreases
error, and 4) changes in Bayesian error due to sycophancy are not strongly
correlated in Brier score, suggesting that studying the impact of sycophancy on
ground truth alone does not fully capture errors in reasoning due to
sycophancy.

</details>


### [92] [RADAR: A Reasoning-Guided Attribution Framework for Explainable Visual Data Analysis](https://arxiv.org/abs/2508.16850)
*Anku Rani,Aparna Garimella,Apoorv Saxena,Balaji Vasan Srinivasan,Paul Pu Liang*

Main category: cs.AI

TL;DR: The paper introduces a new approach to improve the interpretability and trustworthiness of MLLMs in chart analysis by attributing their reasoning process. RADAR, a semi-automatic approach to obtain a benchmark dataset comprising 17,819 diverse samples with charts, questions, reasoning steps, and attribution annotations.


<details>
  <summary>Details</summary>
Motivation: the black-box nature of Multimodal Large Language Models poses significant challenges to real-world trust and adoption.

Method: a reasoning-guided approach

Result: reasoning-guided approach improves attribution accuracy by 15% compared to baseline methods, and enhanced attribution capabilities translate to stronger answer generation, achieving an average BERTScore of ~ 0.90

Conclusion: This advancement represents a significant step toward more interpretable and trustworthy chart analysis systems, enabling users to verify and understand model decisions through reasoning and attribution.

Abstract: Data visualizations like charts are fundamental tools for quantitative
analysis and decision-making across fields, requiring accurate interpretation
and mathematical reasoning. The emergence of Multimodal Large Language Models
(MLLMs) offers promising capabilities for automated visual data analysis, such
as processing charts, answering questions, and generating summaries. However,
they provide no visibility into which parts of the visual data informed their
conclusions; this black-box nature poses significant challenges to real-world
trust and adoption. In this paper, we take the first major step towards
evaluating and enhancing the capabilities of MLLMs to attribute their reasoning
process by highlighting the specific regions in charts and graphs that justify
model answers. To this end, we contribute RADAR, a semi-automatic approach to
obtain a benchmark dataset comprising 17,819 diverse samples with charts,
questions, reasoning steps, and attribution annotations. We also introduce a
method that provides attribution for chart-based mathematical reasoning.
Experimental results demonstrate that our reasoning-guided approach improves
attribution accuracy by 15% compared to baseline methods, and enhanced
attribution capabilities translate to stronger answer generation, achieving an
average BERTScore of $\sim$ 0.90, indicating high alignment with ground truth
responses. This advancement represents a significant step toward more
interpretable and trustworthy chart analysis systems, enabling users to verify
and understand model decisions through reasoning and attribution.

</details>


### [93] [Complexity in finitary argumentation (extended version)](https://arxiv.org/abs/2508.16986)
*Uri Andrews,Luca San Mauro*

Main category: cs.AI

TL;DR: computational intractability of solving infinite AFs limit their use, so this paper investigate the complexity of computational problems related to infinite but finitary argumentations frameworks, and find that for the admissibility-based semantics, it entails a dramatic decrease in complexity. Therefore, finitary infinite AFs provide a natural setting for reasoning which balances well the competing goals of being expressive enough to be applied to many reasoning settings while being computationally tractable enough for the analysis within the framework to be useful.


<details>
  <summary>Details</summary>
Motivation: the computational intractability of solving infinite AFs limit their use, even in many theoretical applications.

Method: investigate the complexity of computational problems related to infinite but finitary argumentations frameworks

Result: the assumption of being finitary does not automatically guarantee a drop in complexity. However, for the admissibility-based semantics, we find a remarkable combinatorial constraint which entails a dramatic decrease in complexity.

Conclusion: finitary infinite AFs provide a natural setting for reasoning which balances well the competing goals of being expressive enough to be applied to many reasoning settings while being computationally tractable enough for the analysis within the framework to be useful.

Abstract: Abstract argumentation frameworks (AFs) provide a formal setting to analyze
many forms of reasoning with conflicting information. While the expressiveness
of general infinite AFs make them a tempting tool for modeling many kinds of
reasoning scenarios, the computational intractability of solving infinite AFs
limit their use, even in many theoretical applications.
  We investigate the complexity of computational problems related to infinite
but finitary argumentations frameworks, that is, infinite AFs where each
argument is attacked by only finitely many others. Our results reveal a
surprising scenario. On one hand, we see that the assumption of being finitary
does not automatically guarantee a drop in complexity. However, for the
admissibility-based semantics, we find a remarkable combinatorial constraint
which entails a dramatic decrease in complexity.
  We conclude that for many forms of reasoning, the finitary infinite AFs
provide a natural setting for reasoning which balances well the competing goals
of being expressive enough to be applied to many reasoning settings while being
computationally tractable enough for the analysis within the framework to be
useful.

</details>


### [94] [WebSight: A Vision-First Architecture for Robust Web Agents](https://arxiv.org/abs/2508.16987)
*Tanvir Bhathal,Asanshay Gupta*

Main category: cs.AI

TL;DR: WebSight 是一种基于视觉的 Web 代理，它优于其他系统，并在 Web 导航方面树立了新标准。


<details>
  <summary>Details</summary>
Motivation: 旨在设计一种基于视觉的自主 Web 代理，该代理可以通过纯视觉感知与 Web 环境交互，从而无需依赖 HTML 或基于 DOM 的输入。

Method: 一种基于视觉的自主 Web 代理，通过纯视觉感知与 Web 环境交互，无需依赖 HTML 或基于 DOM 的输入。核心是 WebSight-7B 模型，这是一个经过微调的视觉-语言模型，针对 UI 元素交互进行了优化，使用 LoRA 在 Wave-UI-25K 数据集的 Web 焦点子集上进行训练。WebSight 将该模型集成到一个模块化的多代理架构中，该架构包括规划、推理、视觉-动作和验证代理，并通过情景记忆机制进行协调。

Result: WebSight-7B 在 Showdown Clicks 基准测试中实现了 58.84% 的 top-1 准确率，优于几个较大的通用模型，同时保持了较低的延迟。完整的 WebSight 代理在 WebVoyager 基准测试中实现了 68.0% 的成功率，超过了 OpenAI (61.0%) 和 HCompany (Runner H, 67.0%) 等实验室的系统。在完成的任务中，WebSight 的正确回答率为 97.14%，表明其具有很高的精度。

Conclusion: WebSight 和 WebSight-7B 为可解释、稳健和高效的可视化 Web 导航建立了一个新标准。

Abstract: We introduce WebSight, a vision-based autonomous web agent, designed to
interact with web environments purely through visual perception, eliminating
dependence on HTML or DOM-based inputs. Central to our approach we introduce
our new model, WebSight-7B, a fine-tuned vision-language model optimized for UI
element interaction, trained using LoRA on a web-focused subset of the
Wave-UI-25K dataset. WebSight integrates this model into a modular multi-agent
architecture, comprising planning, reasoning, vision-action, and verification
agents, coordinated through an episodic memory mechanism.
  WebSight-7B achieves a top-1 accuracy of 58.84% on the Showdown Clicks
benchmark, outperforming several larger generalist models while maintaining
lower latency. The full WebSight agent achieves a 68.0% success rate on the
WebVoyager benchmark, surpassing systems from labs such as OpenAI (61.0%) and
HCompany (Runner H, 67.0%). Among tasks completed, WebSight answers correctly
97.14% of the time, indicating high precision. Together, WebSight and
WebSight-7B establish a new standard for interpretable, robust, and efficient
visual web navigation.

</details>


### [95] [Solving the Min-Max Multiple Traveling Salesmen Problem via Learning-Based Path Generation and Optimal Splitting](https://arxiv.org/abs/2508.17087)
*Wen Wang,Xiangchen Wu,Liang Wang,Hao Hu,Xianping Tao,Linghao Zhang*

Main category: cs.AI

TL;DR: This paper introduces Generate-and-Split (GaS), a novel reinforcement learning framework for the Min-Max Multiple Traveling Salesmen Problem. GaS outperforms existing learning-based approaches in solution quality and transferability.


<details>
  <summary>Details</summary>
Motivation: This study addresses the Min-Max Multiple Traveling Salesmen Problem ($m^3$-TSP), which aims to coordinate tours for multiple salesmen such that the length of the longest tour is minimized. Due to its NP-hard nature, exact solvers become impractical under the assumption that $P 
e NP$. As a result, learning-based approaches have gained traction for their ability to rapidly generate high-quality approximate solutions. Among these, two-stage methods combine learning-based components with classical solvers, simplifying the learning objective. However, this decoupling often disrupts consistent optimization, potentially degrading solution quality.

Method: a novel two-stage framework named Generate-and-Split (GaS), which integrates reinforcement learning (RL) with an optimal splitting algorithm in a joint training process. The splitting algorithm offers near-linear scalability with respect to the number of cities and guarantees optimal splitting in Euclidean space for any given path. To facilitate the joint optimization of the RL component with the algorithm, we adopt an LSTM-enhanced model architecture to address partial observability.

Result: The proposed GaS framework significantly outperforms existing learning-based approaches in both solution quality and transferability.

Conclusion: The proposed GaS framework significantly outperforms existing learning-based approaches in both solution quality and transferability.

Abstract: This study addresses the Min-Max Multiple Traveling Salesmen Problem
($m^3$-TSP), which aims to coordinate tours for multiple salesmen such that the
length of the longest tour is minimized. Due to its NP-hard nature, exact
solvers become impractical under the assumption that $P \ne NP$. As a result,
learning-based approaches have gained traction for their ability to rapidly
generate high-quality approximate solutions. Among these, two-stage methods
combine learning-based components with classical solvers, simplifying the
learning objective. However, this decoupling often disrupts consistent
optimization, potentially degrading solution quality. To address this issue, we
propose a novel two-stage framework named \textbf{Generate-and-Split} (GaS),
which integrates reinforcement learning (RL) with an optimal splitting
algorithm in a joint training process. The splitting algorithm offers
near-linear scalability with respect to the number of cities and guarantees
optimal splitting in Euclidean space for any given path. To facilitate the
joint optimization of the RL component with the algorithm, we adopt an
LSTM-enhanced model architecture to address partial observability. Extensive
experiments show that the proposed GaS framework significantly outperforms
existing learning-based approaches in both solution quality and
transferability.

</details>


### [96] [PowerChain: Automating Distribution Grid Analysis with Agentic AI Workflows](https://arxiv.org/abs/2508.17094)
*Emmanuel O. Badmus,Peng Sang,Dimitrios Stamoulis,Amritanshu Pandey*

Main category: cs.AI

TL;DR: Developed PowerChain, a novel agentic AI system, to solve unseen DG analysis tasks via automated agentic orchestration and large language models (LLMs) function-calling.


<details>
  <summary>Details</summary>
Motivation: Distribution grid (DG) operation and planning are becoming more complex, necessitating advanced computational analyses to ensure grid reliability and resilience. State-of-the-art DG analyses rely on disparate workflows of complex models, functions, and data pipelines, which require expert knowledge and are challenging to automate. Many small-scale utilities and cooperatives lack a large R&D workforce and therefore cannot use advanced analysis at scale.

Method: We develop a novel agentic AI system, PowerChain, to solve unseen DG analysis tasks via automated agentic orchestration and large language models (LLMs) function-calling. Given a natural language query, PowerChain dynamically generates and executes an ordered sequence of domain-aware functions guided by the semantics of an expert-built power systems function pool and a select reference set of known, expert-generated workflow-query pairs.

Result: PowerChain can produce expert-level workflows with both GPT-5 and open-source Qwen models on complex, unseen DG analysis tasks operating on real utility data.

Conclusion: PowerChain can produce expert-level workflows with both GPT-5 and open-source Qwen models on complex, unseen DG analysis tasks operating on real utility data.

Abstract: Due to the rapid pace of electrification and decarbonization, distribution
grid (DG) operation and planning are becoming more complex, necessitating
advanced computational analyses to ensure grid reliability and resilience.
State-of-the-art DG analyses rely on disparate workflows of complex models,
functions, and data pipelines, which require expert knowledge and are
challenging to automate. Many small-scale utilities and cooperatives lack a
large R&D workforce and therefore cannot use advanced analysis at scale. To
address this gap, we develop a novel agentic AI system, PowerChain, to solve
unseen DG analysis tasks via automated agentic orchestration and large language
models (LLMs) function-calling. Given a natural language query, PowerChain
dynamically generates and executes an ordered sequence of domain-aware
functions guided by the semantics of an expert-built power systems function
pool and a select reference set of known, expert-generated workflow-query
pairs. Our results show that PowerChain can produce expert-level workflows with
both GPT-5 and open-source Qwen models on complex, unseen DG analysis tasks
operating on real utility data.

</details>


### [97] [Rethinking How AI Embeds and Adapts to Human Values: Challenges and Opportunities](https://arxiv.org/abs/2508.17104)
*Sz-Ting Tzeng,Frank Dignum*

Main category: cs.AI

TL;DR: 这篇论文认为人工智能的价值对齐需要超越静态的价值观，采用长期推理，并适应不断变化的价值观。多智能体系统为处理价值冲突提供了一个有用的框架，并探讨了价值对齐的挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: “以人为本的人工智能”和“基于价值的决策”的概念在研究和工业领域都受到了极大的关注。然而，许多关键方面仍未被充分探索，需要进一步调查。特别是，我们需要了解系统如何整合人类价值观，人类如何在系统中识别这些价值观，以及如何最大限度地减少伤害或意外后果的风险。

Method: 论文探讨了多智能体系统如何为处理多元化、冲突以及关于价值观的智能体间推理提供合适的框架。

Result: 论文识别了与价值对齐相关的挑战，并为推进价值对齐研究指明了方向。此外，论文还广泛讨论了价值对齐的不同视角，从设计方法到实际应用。

Conclusion: 这篇论文强调了我们需要重新思考如何构建价值对齐，并认为价值对齐应该超越静态和单一的价值观概念。AI 系统应该实施长期推理，并适应不断变化的价值观。

Abstract: The concepts of ``human-centered AI'' and ``value-based decision'' have
gained significant attention in both research and industry. However, many
critical aspects remain underexplored and require further investigation. In
particular, there is a need to understand how systems incorporate human values,
how humans can identify these values within systems, and how to minimize the
risks of harm or unintended consequences. In this paper, we highlight the need
to rethink how we frame value alignment and assert that value alignment should
move beyond static and singular conceptions of values. We argue that AI systems
should implement long-term reasoning and remain adaptable to evolving values.
Furthermore, value alignment requires more theories to address the full
spectrum of human values. Since values often vary among individuals or groups,
multi-agent systems provide the right framework for navigating pluralism,
conflict, and inter-agent reasoning about values. We identify the challenges
associated with value alignment and indicate directions for advancing value
alignment research. In addition, we broadly discuss diverse perspectives of
value alignment, from design methodologies to practical applications.

</details>


### [98] [MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes](https://arxiv.org/abs/2508.17180)
*Nilay Pande,Sahiti Yerramilli,Jayant Sravan Tamarapalli,Rynaa Grover*

Main category: cs.AI

TL;DR: MaRVL-QA 是一个新的基准，用于定量评估 MLLM 的深度数学和空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型 (MLLM) 的一个关键前沿是直接从图像执行深度数学和空间推理的能力，超越了它们在语义描述中已取得的成功。数学曲面图为此功能提供了一个严格的试验台，因为它们将推理任务与自然图像中常见的语义噪声隔离开来。

Method: MaRVL-QA，一个新的基准，旨在定量评估这些核心推理技能。该基准包括两个新颖的任务：拓扑计数，识别和枚举局部最大值等特征；以及变换识别，识别应用了几何变换。

Result: 在 MaRVL-QA 上的评估表明，即使是最先进的 MLLM 也难以应对，他们经常求助于肤浅的启发式方法，而不是强大的空间推理。

Conclusion: 即使是最先进的 MLLM 也难以应对，他们经常求助于肤浅的启发式方法，而不是强大的空间推理。MaRVL-QA 为研究界提供了一个具有挑战性的新工具，用于衡量进展、暴露模型局限性并指导开发具有更深刻推理能力的 MLLM。

Abstract: A key frontier for Multimodal Large Language Models (MLLMs) is the ability to
perform deep mathematical and spatial reasoning directly from images, moving
beyond their established success in semantic description. Mathematical surface
plots provide a rigorous testbed for this capability, as they isolate the task
of reasoning from the semantic noise common in natural images. To measure
progress on this frontier, we introduce MaRVL-QA (Mathematical Reasoning over
Visual Landscapes), a new benchmark designed to quantitatively evaluate these
core reasoning skills. The benchmark comprises two novel tasks: Topological
Counting, identifying and enumerating features like local maxima; and
Transformation Recognition, recognizing applied geometric transformations.
Generated from a curated library of functions with rigorous ambiguity
filtering, our evaluation on MaRVL-QA reveals that even state-of-the-art MLLMs
struggle significantly, often resorting to superficial heuristics instead of
robust spatial reasoning. MaRVL-QA provides a challenging new tool for the
research community to measure progress, expose model limitations, and guide the
development of MLLMs with more profound reasoning abilities.

</details>


### [99] [PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent LLMs](https://arxiv.org/abs/2508.17188)
*Zhilin Zhang,Xiang Zhang,Jiaqi Wei,Yiwei Xu,Chenyu You*

Main category: cs.AI

TL;DR: PosterGen是一个多智能体框架，可以生成演示就绪的海报，而无需大量人工修改。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多忽略了核心设计和美学原则，导致海报需要大量的人工修改。

Method: PosterGen，一个多智能体框架，模仿专业海报设计师的工作流程。它由四个协作的专业代理组成：(1) 解析器和馆长代理从论文中提取内容并组织故事板；(2) 布局代理将内容映射到连贯的空间布局中；(3) 样式代理应用视觉设计元素，如颜色和排版；(4) 渲染器组成最终海报。

Result: PosterGen在内容保真度上始终匹配，并且在视觉设计上显著优于现有方法。

Conclusion: PosterGen可以生成内容保真且在视觉设计上优于现有方法的演示就绪海报，最大限度地减少人工修改。

Abstract: Multi-agent systems built upon large language models (LLMs) have demonstrated
remarkable capabilities in tackling complex compositional tasks. In this work,
we apply this paradigm to the paper-to-poster generation problem, a practical
yet time-consuming process faced by researchers preparing for conferences.
While recent approaches have attempted to automate this task, most neglect core
design and aesthetic principles, resulting in posters that require substantial
manual refinement. To address these design limitations, we propose PosterGen, a
multi-agent framework that mirrors the workflow of professional poster
designers. It consists of four collaborative specialized agents: (1) Parser and
Curator agents extract content from the paper and organize storyboard; (2)
Layout agent maps the content into a coherent spatial layout; (3) Stylist
agents apply visual design elements such as color and typography; and (4)
Renderer composes the final poster. Together, these agents produce posters that
are both semantically grounded and visually appealing. To evaluate design
quality, we introduce a vision-language model (VLM)-based rubric that measures
layout balance, readability, and aesthetic coherence. Experimental results show
that PosterGen consistently matches in content fidelity, and significantly
outperforms existing methods in visual designs, generating posters that are
presentation-ready with minimal human refinements.

</details>


### [100] [ST-Raptor: LLM-Powered Semi-Structured Table Question Answering](https://arxiv.org/abs/2508.18190)
*Zirui Tang,Boyu Niu,Xuanhe Zhou,Boxiu Li,Wei Zhou,Jiannan Wang,Guoliang Li,Xinyi Zhang,Fan Wu*

Main category: cs.AI

TL;DR: ST-Raptor, a tree-based framework, addresses the challenges of question answering on semi-structured tables by using a Hierarchical Orthogonal Tree, tree operations, and a two-stage verification mechanism. It outperforms existing methods by up to 20% in answer accuracy.


<details>
  <summary>Details</summary>
Motivation: Semi-structured tables, widely used in real-world applications, often involve flexible and complex layouts which are costly and inefficient for human analysts to interpret and existing methods face significant challenges.

Method: a tree-based framework for semi-structured table question answering using large language models. First, introduce the Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures complex semi-structured table layouts, along with an effective algorithm for constructing the tree. Second, define a set of basic tree operations to guide LLMs in executing common QA tasks. Given a user question, ST-Raptor decomposes it into simpler sub-questions, generates corresponding tree operation pipelines, and conducts operation-table alignment for accurate pipeline execution. Third, incorporate a two-stage verification mechanism: forward validation checks the correctness of execution steps, while backward validation evaluates answer reliability by reconstructing queries from predicted answers.

Result: ST-Raptor outperforms nine baselines by up to 20% in answer accuracy.

Conclusion: ST-Raptor outperforms nine baselines by up to 20% in answer accuracy.

Abstract: Semi-structured tables, widely used in real-world applications (e.g.,
financial reports, medical records, transactional orders), often involve
flexible and complex layouts (e.g., hierarchical headers and merged cells).
These tables generally rely on human analysts to interpret table layouts and
answer relevant natural language questions, which is costly and inefficient. To
automate the procedure, existing methods face significant challenges. First,
methods like NL2SQL require converting semi-structured tables into structured
ones, which often causes substantial information loss. Second, methods like
NL2Code and multi-modal LLM QA struggle to understand the complex layouts of
semi-structured tables and cannot accurately answer corresponding questions. To
this end, we propose ST-Raptor, a tree-based framework for semi-structured
table question answering using large language models. First, we introduce the
Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures
complex semi-structured table layouts, along with an effective algorithm for
constructing the tree. Second, we define a set of basic tree operations to
guide LLMs in executing common QA tasks. Given a user question, ST-Raptor
decomposes it into simpler sub-questions, generates corresponding tree
operation pipelines, and conducts operation-table alignment for accurate
pipeline execution. Third, we incorporate a two-stage verification mechanism:
forward validation checks the correctness of execution steps, while backward
validation evaluates answer reliability by reconstructing queries from
predicted answers. To benchmark the performance, we present SSTQA, a dataset of
764 questions over 102 real-world semi-structured tables. Experiments show that
ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code
is available at https://github.com/weAIDB/ST-Raptor.

</details>


### [101] [From reactive to cognitive: brain-inspired spatial intelligence for embodied agents](https://arxiv.org/abs/2508.17198)
*Shouwei Ruan,Liyuan Wang,Caixin Kang,Qihui Zhu,Songming Liu,Xingxing Wei,Hang Su*

Main category: cs.AI

TL;DR: 提出了一种新的导航框架BSC-Nav，它构建结构化的空间记忆，集成了多模态大型语言模型，并在导航任务中表现出色，具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型（MLLM）的最新进展使得具身智能体能够进行视觉语言推理，但这些工作缺乏结构化的空间记忆，而是被动地运作，限制了它们在复杂的现实世界环境中的泛化和适应性。

Method: 提出了一种用于在具身智能体中构建和利用结构化空间记忆的统一框架，即受大脑启发的导航空间认知（BSC-Nav）。BSC-Nav从以自我为中心的轨迹和上下文线索构建以自我为中心的认知地图，并动态检索与语义目标对齐的空间知识。

Result: BSC-Nav在各种导航任务中实现了最先进的功效和效率，展示了强大的零样本泛化能力，并支持现实物理世界中通用的具身行为。

Conclusion: 集成了强大的多模态大型语言模型（MLLM）的BSC-Nav在各种导航任务中实现了最先进的功效和效率，展示了强大的零样本泛化能力，并支持现实物理世界中通用的具身行为，为通用空间智能提供了一条可扩展且具有生物学基础的路径。

Abstract: Spatial cognition enables adaptive goal-directed behavior by constructing
internal models of space. Robust biological systems consolidate spatial
knowledge into three interconnected forms: \textit{landmarks} for salient cues,
\textit{route knowledge} for movement trajectories, and \textit{survey
knowledge} for map-like representations. While recent advances in multi-modal
large language models (MLLMs) have enabled visual-language reasoning in
embodied agents, these efforts lack structured spatial memory and instead
operate reactively, limiting their generalization and adaptability in complex
real-world environments. Here we present Brain-inspired Spatial Cognition for
Navigation (BSC-Nav), a unified framework for constructing and leveraging
structured spatial memory in embodied agents. BSC-Nav builds allocentric
cognitive maps from egocentric trajectories and contextual cues, and
dynamically retrieves spatial knowledge aligned with semantic goals. Integrated
with powerful MLLMs, BSC-Nav achieves state-of-the-art efficacy and efficiency
across diverse navigation tasks, demonstrates strong zero-shot generalization,
and supports versatile embodied behaviors in the real physical world, offering
a scalable and biologically grounded path toward general-purpose spatial
intelligence.

</details>


### [102] [Large Language Model-Based Automatic Formulation for Stochastic Optimization Models](https://arxiv.org/abs/2508.17200)
*Amirreza Talebi*

Main category: cs.AI

TL;DR: 本文研究了 ChatGPT 在解决随机优化问题中的应用，发现 GPT-4-Turbo 在结构质量和准确性方面表现出色，为语言驱动的建模管道铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 本文对大型语言模型 (LLM)（特别是 ChatGPT）在自动制定和解决自然语言描述中的随机优化问题方面的性能进行了首次综合系统研究。

Method: 设计了多个提示，指导 ChatGPT 使用思维链和模块化推理完成结构化任务。引入了一种新的软评分指标，用于评估生成模型的结构质量和部分正确性。

Result: GPT-4-Turbo 在部分分数、变量匹配和目标精度方面优于其他模型，其中 cot_s_instructions 和 agentic 成为最有效的提示策略。

Conclusion: 大型语言模型可以通过精心设计的提示和多智能体协作来促进随机公式的生成，为随机优化中智能的、语言驱动的建模管道铺平道路。

Abstract: This paper presents the first integrated systematic study on the performance
of large language models (LLMs), specifically ChatGPT, to automatically
formulate and solve stochastic optimiza- tion problems from natural language
descriptions. Focusing on three key categories, joint chance- constrained
models, individual chance-constrained models, and two-stage stochastic linear
programs (SLP-2), we design several prompts that guide ChatGPT through
structured tasks using chain-of- thought and modular reasoning. We introduce a
novel soft scoring metric that evaluates the struc- tural quality and partial
correctness of generated models, addressing the limitations of canonical and
execution-based accuracy. Across a diverse set of stochastic problems,
GPT-4-Turbo outperforms other models in partial score, variable matching, and
objective accuracy, with cot_s_instructions and agentic emerging as the most
effective prompting strategies. Our findings reveal that with well-engineered
prompts and multi-agent collaboration, LLMs can facilitate specially stochastic
formulations, paving the way for intelligent, language-driven modeling
pipelines in stochastic opti- mization.

</details>


### [103] [Explainable Counterfactual Reasoning in Depression Medication Selection at Multi-Levels (Personalized and Population)](https://arxiv.org/abs/2508.17207)
*Xinyu Qin,Mark H. Chignell,Alexandria Greifenberger,Sachinthya Lokuge,Elssa Toumeh,Tia Sternat,Martin Katzman,Lu Wang*

Main category: cs.AI

TL;DR: This study uses counterfactual reasoning to understand how specific depression symptoms influence the choice between SSRIs and SNRIs, finding that certain symptoms strongly drive medication selection.


<details>
  <summary>Details</summary>
Motivation: This study investigates how variations in Major Depressive Disorder (MDD) symptoms, quantified by the Hamilton Rating Scale for Depression (HAM-D), causally influence the prescription of SSRIs versus SNRIs.

Method: We applied explainable counterfactual reasoning with counterfactual explanations (CFs) to assess the impact of specific symptom changes on antidepressant choice.

Result: Among 17 binary classifiers, Random Forest achieved highest performance (accuracy, F1, precision, recall, ROC-AUC near 0.85). Sample-based CFs revealed both local and global feature importance of individual symptoms in medication selection.

Conclusion: Counterfactual reasoning elucidates which MDD symptoms most strongly drive SSRI versus SNRI selection, enhancing interpretability of AI-based clinical decision support systems. Future work should validate these findings on more diverse cohorts and refine algorithms for clinical deployment.

Abstract: Background: This study investigates how variations in Major Depressive
Disorder (MDD) symptoms, quantified by the Hamilton Rating Scale for Depression
(HAM-D), causally influence the prescription of SSRIs versus SNRIs. Methods: We
applied explainable counterfactual reasoning with counterfactual explanations
(CFs) to assess the impact of specific symptom changes on antidepressant
choice. Results: Among 17 binary classifiers, Random Forest achieved highest
performance (accuracy, F1, precision, recall, ROC-AUC near 0.85). Sample-based
CFs revealed both local and global feature importance of individual symptoms in
medication selection. Conclusions: Counterfactual reasoning elucidates which
MDD symptoms most strongly drive SSRI versus SNRI selection, enhancing
interpretability of AI-based clinical decision support systems. Future work
should validate these findings on more diverse cohorts and refine algorithms
for clinical deployment.

</details>


### [104] [Reinforcement Learning enhanced Online Adaptive Clinical Decision Support via Digital Twin powered Policy and Treatment Effect optimized Reward](https://arxiv.org/abs/2508.17212)
*Xinyu Qin,Ruiheng Yu,Lu Wang*

Main category: cs.AI

TL;DR: 我们提出了一个在线自适应工具，其中强化学习提供策略，患者数字孪生提供环境，治疗效果定义奖励。


<details>
  <summary>Details</summary>
Motivation: 临床决策支持必须在安全约束下在线调整。

Method: 强化学习提供策略，患者数字孪生提供环境，治疗效果定义奖励。

Result: 在合成临床模拟器中的实验表明，该系统具有低延迟、稳定吞吐量、固定安全性下的低专家查询率以及相对于标准基于价值的基线改进的回报。

Conclusion: 该设计将离线策略转变为具有清晰控制和快速适应能力的持续的、临床医生监督的系统。

Abstract: Clinical decision support must adapt online under safety constraints. We
present an online adaptive tool where reinforcement learning provides the
policy, a patient digital twin provides the environment, and treatment effect
defines the reward. The system initializes a batch-constrained policy from
retrospective data and then runs a streaming loop that selects actions, checks
safety, and queries experts only when uncertainty is high. Uncertainty comes
from a compact ensemble of five Q-networks via the coefficient of variation of
action values with a $\tanh$ compression. The digital twin updates the patient
state with a bounded residual rule. The outcome model estimates immediate
clinical effect, and the reward is the treatment effect relative to a
conservative reference with a fixed z-score normalization from the training
split. Online updates operate on recent data with short runs and exponential
moving averages. A rule-based safety gate enforces vital ranges and
contraindications before any action is applied. Experiments in a synthetic
clinical simulator show low latency, stable throughput, a low expert query rate
at fixed safety, and improved return against standard value-based baselines.
The design turns an offline policy into a continuous, clinician-supervised
system with clear controls and fast adaptation.

</details>


### [105] [MC3G: Model Agnostic Causally Constrained Counterfactual Generation](https://arxiv.org/abs/2508.17221)
*Sopam Dasgupta,Sadaf MD Halim,Joaquín Arias,Elmer Salazar,Gopal Gupta*

Main category: cs.AI

TL;DR: 提出了一种名为MC3G的框架，用于生成反事实解释，该框架更可解释、可操作且成本更低，从而提高透明度和实用性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型越来越多地影响金融、法律和招聘等高风险环境中的决策，因此需要透明的、可解释的结果。然而，虽然可解释的方法可以帮助理解所做的决策，但它们可能会无意中泄露底层专有算法。

Method: 提出了一种新的框架，即与模型无关的因果约束反事实生成（MC3G）。

Result: MC3G提供了更可解释和可操作的反事实建议，同时具有更低的成本。

Conclusion: MC3G在提高透明度、责任性和实用性方面有潜力。

Abstract: Machine learning models increasingly influence decisions in high-stakes
settings such as finance, law and hiring, driving the need for transparent,
interpretable outcomes. However, while explainable approaches can help
understand the decisions being made, they may inadvertently reveal the
underlying proprietary algorithm: an undesirable outcome for many
practitioners. Consequently, it is crucial to balance meaningful transparency
with a form of recourse that clarifies why a decision was made and offers
actionable steps following which a favorable outcome can be obtained.
Counterfactual explanations offer a powerful mechanism to address this need by
showing how specific input changes lead to a more favorable prediction. We
propose Model-Agnostic Causally Constrained Counterfactual Generation (MC3G), a
novel framework that tackles limitations in the existing counterfactual
methods. First, MC3G is model-agnostic: it approximates any black-box model
using an explainable rule-based surrogate model. Second, this surrogate is used
to generate counterfactuals that produce a favourable outcome for the original
underlying black box model. Third, MC3G refines cost computation by excluding
the ``effort" associated with feature changes that occur automatically due to
causal dependencies. By focusing only on user-initiated changes, MC3G provides
a more realistic and fair representation of the effort needed to achieve a
favourable outcome. We show that MC3G delivers more interpretable and
actionable counterfactual recommendations compared to existing techniques all
while having a lower cost. Our findings highlight MC3G's potential to enhance
transparency, accountability, and practical utility in decision-making
processes that incorporate machine-learning approaches.

</details>


### [106] [L-XAIDS: A LIME-based eXplainable AI framework for Intrusion Detection Systems](https://arxiv.org/abs/2508.17244)
*Aoun E Muhammad,Kin-Choong Yow,Nebojsa Bacanin-Dzakula,Muhammad Attique Khan*

Main category: cs.AI

TL;DR: 本文提出了一个框架，通过 LIME、ELI5 和决策树算法来解释基于机器学习的入侵检测系统 (IDS) 的决策，从而提高网络关键系统中可解释人工智能的广泛应用。


<details>
  <summary>Details</summary>
Motivation: 人工智能在医疗保健、金融科技和网络安全等关键行业的应用激增，导致了人工智能可解释性研究的激增。当人工智能用于金融科技、医疗保健和网络安全和自动驾驶汽车等安全关键系统中的决策时，可解释性变得更加重要。然而，对于用户和黑盒 AI 决策解释的透明性质的可靠评估仍然存在歧义。

Method: 该框架使用 Local Interpretable Model-Agnostic Explanations (LIME) 结合 Explain Like I'm five (ELI5) 和决策树算法来提供局部和全局解释，并提高 IDS 的解释性。

Result: 该框架能够以 85% 的准确率对攻击行为进行分类，同时显示分类中使用的前 10 个特征的特征重要性排名。

Conclusion: 该框架在 UNSW-NB15 数据集上能够以 85% 的准确率对攻击行为进行分类，同时显示分类中使用的前 10 个特征的特征重要性排名。

Abstract: Recent developments in Artificial Intelligence (AI) and their applications in
critical industries such as healthcare, fin-tech and cybersecurity have led to
a surge in research in explainability in AI. Innovative research methods are
being explored to extract meaningful insight from blackbox AI systems to make
the decision-making technology transparent and interpretable. Explainability
becomes all the more critical when AI is used in decision making in domains
like fintech, healthcare and safety critical systems such as cybersecurity and
autonomous vehicles. However, there is still ambiguity lingering on the
reliable evaluations for the users and nature of transparency in the
explanations provided for the decisions made by black-boxed AI. To solve the
blackbox nature of Machine Learning based Intrusion Detection Systems, a
framework is proposed in this paper to give an explanation for IDSs decision
making. This framework uses Local Interpretable Model-Agnostic Explanations
(LIME) coupled with Explain Like I'm five (ELI5) and Decision Tree algorithms
to provide local and global explanations and improve the interpretation of
IDSs. The local explanations provide the justification for the decision made on
a specific input. Whereas, the global explanations provides the list of
significant features and their relationship with attack traffic. In addition,
this framework brings transparency in the field of ML driven IDS that might be
highly significant for wide scale adoption of eXplainable AI in cyber-critical
systems. Our framework is able to achieve 85 percent accuracy in classifying
attack behaviour on UNSW-NB15 dataset, while at the same time displaying the
feature significance ranking of the top 10 features used in the classification.

</details>


### [107] [Federated Reinforcement Learning for Runtime Optimization of AI Applications in Smart Eyewears](https://arxiv.org/abs/2508.17262)
*Hamta Sedghani,Abednego Wamuhindo Kambale,Federica Filippini,Francesca Palermo,Diana Trojaniello,Danilo Ardagna*

Main category: cs.AI

TL;DR: This paper proposes a Federated Reinforcement Learning (FRL) framework to address the limitations of Smart Eye-Wears (SEWs) in computational power, memory, and battery life. The results show that federated agents exhibit significantly lower performance variability, ensuring greater stability and reliability.


<details>
  <summary>Details</summary>
Motivation: SEWs face limitations in computational power, memory, and battery life, while offloading computations to external servers is constrained by network conditions and server workload variability.

Method: A Federated Reinforcement Learning (FRL) framework with synchronous and asynchronous federation strategies.

Result: Federated agents exhibit significantly lower performance variability.

Conclusion: Federated Reinforcement Learning (FRL) can significantly lower performance variability, ensuring greater stability and reliability, making it suitable for real-time AI processing in SEWs.

Abstract: Extended reality technologies are transforming fields such as healthcare,
entertainment, and education, with Smart Eye-Wears (SEWs) and Artificial
Intelligence (AI) playing a crucial role. However, SEWs face inherent
limitations in computational power, memory, and battery life, while offloading
computations to external servers is constrained by network conditions and
server workload variability. To address these challenges, we propose a
Federated Reinforcement Learning (FRL) framework, enabling multiple agents to
train collaboratively while preserving data privacy. We implemented synchronous
and asynchronous federation strategies, where models are aggregated either at
fixed intervals or dynamically based on agent progress. Experimental results
show that federated agents exhibit significantly lower performance variability,
ensuring greater stability and reliability. These findings underscore the
potential of FRL for applications requiring robust real-time AI processing,
such as real-time object detection in SEWs.

</details>


### [108] [ERF-BA-TFD+: A Multimodal Model for Audio-Visual Deepfake Detection](https://arxiv.org/abs/2508.17282)
*Xin Zhang,Jiaming Chu,Jian Zhao,Yuchu Jiang,Xu Yang,Lei Jin,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: ERF-BA-TFD+ 是一种用于检测 deepfake 的新模型，它结合了音频和视频信息，并在 DDL-AV 数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，deepfake 内容可以跨多种模态（包括音频和视频） проявляться。为了应对这一挑战，

Method: 提出了一种新颖的多模态 deepfake 检测模型 ERF-BA-TFD+，该模型结合了增强的感受野 (ERF) 和视听融合。

Result: 该模型在准确性和处理速度方面均优于现有技术，证明了其在音频-视频检测和定位方面的有效性。

Conclusion: ERF-BA-TFD+ 模型在 DDL-AV 数据集上取得了最先进的结果，并在 Deepfake 检测、定位和可解释性研讨会的 DDL-AV 赛道中获得了第一名。

Abstract: Deepfake detection is a critical task in identifying manipulated multimedia
content. In real-world scenarios, deepfake content can manifest across multiple
modalities, including audio and video. To address this challenge, we present
ERF-BA-TFD+, a novel multimodal deepfake detection model that combines enhanced
receptive field (ERF) and audio-visual fusion. Our model processes both audio
and video features simultaneously, leveraging their complementary information
to improve detection accuracy and robustness. The key innovation of ERF-BA-TFD+
lies in its ability to model long-range dependencies within the audio-visual
input, allowing it to better capture subtle discrepancies between real and fake
content. In our experiments, we evaluate ERF-BA-TFD+ on the DDL-AV dataset,
which consists of both segmented and full-length video clips. Unlike previous
benchmarks, which focused primarily on isolated segments, the DDL-AV dataset
allows us to assess the model's performance in a more comprehensive and
realistic setting. Our method achieves state-of-the-art results on this
dataset, outperforming existing techniques in terms of both accuracy and
processing speed. The ERF-BA-TFD+ model demonstrated its effectiveness in the
"Workshop on Deepfake Detection, Localization, and Interpretability," Track 2:
Audio-Visual Detection and Localization (DDL-AV), and won first place in this
competition.

</details>


### [109] [MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for N-level Assessment](https://arxiv.org/abs/2508.17290)
*Omid Ghahroodi,Arshia Hemmat,Marzia Nouri,Seyed Mohammad Hadi Hosseini,Doratossadat Dastgheib,Mohammad Vali Sanian,Alireza Sahebi,Reihaneh Zohrabi,Mohammad Hossein Rohban,Ehsaneddin Asgari,Mahdieh Soleymani Baghshah*

Main category: cs.AI

TL;DR: Introducing MEENA (PersianMMMU), the first dataset designed to evaluate Persian VLMs across scientific, reasoning, and human-level understanding tasks to enhance VLM capabilities beyond English.


<details>
  <summary>Details</summary>
Motivation: Recent advancements in large vision-language models (VLMs) have primarily focused on English, with limited attention given to other languages.

Method: Introducing MEENA (PersianMMMU), a dataset to evaluate Persian VLMs.

Result: MEENA dataset comprises approximately 7,500 Persian and 3,000 English questions, covering a wide range of topics.

Conclusion: This benchmark contributes to enhancing VLM capabilities beyond English.

Abstract: Recent advancements in large vision-language models (VLMs) have primarily
focused on English, with limited attention given to other languages. To address
this gap, we introduce MEENA (also known as PersianMMMU), the first dataset
designed to evaluate Persian VLMs across scientific, reasoning, and human-level
understanding tasks. Our dataset comprises approximately 7,500 Persian and
3,000 English questions, covering a wide range of topics such as reasoning,
mathematics, physics, diagrams, charts, and Persian art and literature. Key
features of MEENA include: (1) diverse subject coverage spanning various
educational levels, from primary to upper secondary school, (2) rich metadata,
including difficulty levels and descriptive answers, (3) original Persian data
that preserves cultural nuances, (4) a bilingual structure to assess
cross-linguistic performance, and (5) a series of diverse experiments assessing
various capabilities, including overall performance, the model's ability to
attend to images, and its tendency to generate hallucinations. We hope this
benchmark contributes to enhancing VLM capabilities beyond English.

</details>


### [110] [Meta-R1: Empowering Large Reasoning Models with Metacognition](https://arxiv.org/abs/2508.17291)
*Haonan Dong,Haoran Ye,Wenhao Zhu,Kehan Jiang,Guojie Song*

Main category: cs.AI

TL;DR: Meta-R1是一个为大型推理模型提供显式元认知能力的框架，它提高了性能、效率和可迁移性。


<details>
  <summary>Details</summary>
Motivation: 目前的大型推理模型缺乏一个专门的元认知系统，这使得它们涌现出的能力是不可控的（非自适应推理）、不可靠的（中间错误）和不灵活的（缺乏明确的方法）。

Method: Meta-R1将推理过程分解为不同的对象级别和元级别组件，在一个级联框架内协调主动规划、在线调节和自适应提前停止。

Result: Meta-R1在三个具有挑战性的基准测试中进行了实验，并与八个具有竞争力的基线进行了比较，结果表明：（I）高性能，超过了最先进的方法高达27.3%；（II）令牌效率高，与原始方法相比，令牌消耗降低到15.7%~32.7%，效率提高高达14.8%；（III）可转移性强，在数据集和模型主干中保持了稳健的性能。

Conclusion: Meta-R1在三个具有挑战性的基准测试中进行了实验，并与八个具有竞争力的基线进行了比较，结果表明：（I）高性能，超过了最先进的方法高达27.3%；（II）令牌效率高，与原始方法相比，令牌消耗降低到15.7%~32.7%，效率提高高达14.8%；（III）可转移性强，在数据集和模型主干中保持了稳健的性能。

Abstract: Large Reasoning Models (LRMs) demonstrate remarkable capabilities on complex
tasks, exhibiting emergent, human-like thinking patterns. Despite their
advances, we identify a fundamental limitation: current LRMs lack a dedicated
meta-level cognitive system-an essential faculty in human cognition that
enables "thinking about thinking". This absence leaves their emergent abilities
uncontrollable (non-adaptive reasoning), unreliable (intermediate error), and
inflexible (lack of a clear methodology). To address this gap, we introduce
Meta-R1, a systematic and generic framework that endows LRMs with explicit
metacognitive capabilities. Drawing on principles from cognitive science,
Meta-R1 decomposes the reasoning process into distinct object-level and
meta-level components, orchestrating proactive planning, online regulation, and
adaptive early stopping within a cascaded framework. Experiments on three
challenging benchmarks and against eight competitive baselines demonstrate that
Meta-R1 is: (I) high-performing, surpassing state-of-the-art methods by up to
27.3%; (II) token-efficient, reducing token consumption to 15.7% ~ 32.7% and
improving efficiency by up to 14.8% when compared to its vanilla counterparts;
and (III) transferable, maintaining robust performance across datasets and
model backbones.

</details>


### [111] [Evolving Collective Cognition in Human-Agent Hybrid Societies: How Agents Form Stances and Boundaries](https://arxiv.org/abs/2508.17366)
*Hanzhong Zhang,Muhua Huang,Jindong Wang*

Main category: cs.AI

TL;DR: Agent 表现出独立于预设身份的内生立场，并通过语言互动重构社会边界。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型已被广泛用于模拟可信的人类社会行为。然而，目前尚不清楚这些模型是否可以在复杂的互动中表现出稳定的立场形成和身份协商能力，以及它们如何响应人类的干预。

Method: 计算多智能体社会实验框架，该框架将生成式基于 agent 的建模与虚拟民族志方法相结合

Result: Agent 表现出内生立场，独立于其预设身份，并对不同的语篇策略表现出不同的语调偏好和响应模式。此外，通过语言互动，agent 积极拆除现有的基于身份的权力结构，并基于这些立场重建自组织的社区边界。

Conclusion: 预设身份并不能严格决定 agent 的社会结构。为了让人类研究者有效地干预集体认知，必须关注 agent 语言网络中的内生机制和互动动态。这些见解为在建模群体社会动态和研究人机协作中使用生成式 AI 提供了理论基础。

Abstract: Large language models have been widely used to simulate credible human social
behaviors. However, it remains unclear whether these models can demonstrate
stable capacities for stance formation and identity negotiation in complex
interactions, as well as how they respond to human interventions. We propose a
computational multi-agent society experiment framework that integrates
generative agent-based modeling with virtual ethnographic methods to
investigate how group stance differentiation and social boundary formation
emerge in human-agent hybrid societies. Across three studies, we find that
agents exhibit endogenous stances, independent of their preset identities, and
display distinct tonal preferences and response patterns to different discourse
strategies. Furthermore, through language interaction, agents actively
dismantle existing identity-based power structures and reconstruct
self-organized community boundaries based on these stances. Our findings
suggest that preset identities do not rigidly determine the agents' social
structures. For human researchers to effectively intervene in collective
cognition, attention must be paid to the endogenous mechanisms and
interactional dynamics within the agents' language networks. These insights
provide a theoretical foundation for using generative AI in modeling group
social dynamics and studying human-agent collaboration.

</details>


### [112] [Mimicking the Physicist's Eye:A VLM-centric Approach for Physics Formula Discovery](https://arxiv.org/abs/2508.17380)
*Jiaqi Liu,Songning Lai,Pengze Li,Di Yu,Wenjie Zhou,Yiyang Zhou,Peng Xia,Zijun Wang,Xi Chen,Shixiang Tang,Lei Bai,Wanli Ouyang,Mingyu Ding,Huaxiu Yao,Aoran Wang*

Main category: cs.AI

TL;DR: VIPER-R1 是一种多模态模型，它集成了视觉感知、轨迹数据和符号推理来发现基本符号公式。


<details>
  <summary>Details</summary>
Motivation: 当前的方法依赖于符号回归或 LLM，仅限于单模态数据，而忽略了丰富的、运动的视觉现象学表示，而这对于物理学家来说是不可或缺的。这种“感觉剥夺”严重削弱了他们解释动态现象中固有的时空模式的能力。

Method: 一种多模态模型，它执行基于物理的方程推理的视觉归纳来发现基本符号公式。它集成了视觉感知、轨迹数据和符号推理来模拟科学发现过程。该模型通过运动结构归纳 (MSI) 课程进行训练，使用监督微调来解释运动学相图并构建由因果思维链 (C-CoT) 指导的假设，然后是奖励引导的符号校准 (RGSC) 以使用强化学习来细化公式结构。

Result: VIPER-R1 在准确性和可解释性方面始终优于最先进的 VLM 基线，从而能够更精确地发现物理定律。

Conclusion: VIPER-R1在准确性和可解释性方面始终优于最先进的 VLM 基线，从而能够更精确地发现物理定律。

Abstract: Automated discovery of physical laws from observational data in the real
world is a grand challenge in AI. Current methods, relying on symbolic
regression or LLMs, are limited to uni-modal data and overlook the rich, visual
phenomenological representations of motion that are indispensable to
physicists. This "sensory deprivation" severely weakens their ability to
interpret the inherent spatio-temporal patterns within dynamic phenomena. To
address this gap, we propose VIPER-R1, a multimodal model that performs Visual
Induction for Physics-based Equation Reasoning to discover fundamental symbolic
formulas. It integrates visual perception, trajectory data, and symbolic
reasoning to emulate the scientific discovery process. The model is trained via
a curriculum of Motion Structure Induction (MSI), using supervised fine-tuning
to interpret kinematic phase portraits and to construct hypotheses guided by a
Causal Chain of Thought (C-CoT), followed by Reward-Guided Symbolic Calibration
(RGSC) to refine the formula structure with reinforcement learning. During
inference, the trained VIPER-R1 acts as an agent: it first posits a
high-confidence symbolic ansatz, then proactively invokes an external symbolic
regression tool to perform Symbolic Residual Realignment (SR^2). This final
step, analogous to a physicist's perturbation analysis, reconciles the
theoretical model with empirical data. To support this research, we introduce
PhysSymbol, a new 5,000-instance multimodal corpus. Experiments show that
VIPER-R1 consistently outperforms state-of-the-art VLM baselines in accuracy
and interpretability, enabling more precise discovery of physical laws. Project
page: https://jiaaqiliu.github.io/VIPER-R1/

</details>


### [113] [Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasets](https://arxiv.org/abs/2508.17391)
*Nikolaos Pavlidis,Vasilis Perifanis,Symeon Symeonidis,Pavlos S. Efraimidis*

Main category: cs.AI

TL;DR: LLM在分类任务上表现出色，回归和聚类任务表现不佳。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 最初是为自然语言处理 (NLP) 开发的，但已证明具有跨模式和领域进行泛化的潜力。 借助其上下文学习 (ICL) 能力，LLM 可以在结构化输入上执行预测任务，而无需对下游任务进行显式微调。

Method: 在小规模结构化数据集上，通过少样本提示，评估了最先进的LLM（GPT-5、GPT-4o、GPT-o3、Gemini-2.5-Flash、DeepSeek-R1）在分类、回归和聚类任务中的经验函数逼近能力，并将其与已建立的机器学习基线进行比较。

Result: LLM 在数据可用性有限的情况下，在分类任务中取得了出色的性能，建立了实用的零训练基线。 相比之下，与 ML 模型相比，具有连续值输出的回归性能较差，这可能是因为回归需要在较大的（通常是无限的）空间中输出，并且聚类结果也受到类似的限制，我们将其归因于在此设置中缺乏真正的 ICL。 尽管如此，这种方法能够实现快速、低开销的数据探索，并为商业智能和探索性分析环境中的传统 ML 管道提供可行的替代方案。

Conclusion: LLMs可以作为结构化数据的通用预测引擎，在分类方面具有明显的优势，但在回归和聚类方面存在明显的局限性。

Abstract: Large Language Models (LLMs), originally developed for natural language
processing (NLP), have demonstrated the potential to generalize across
modalities and domains. With their in-context learning (ICL) capabilities, LLMs
can perform predictive tasks over structured inputs without explicit
fine-tuning on downstream tasks. In this work, we investigate the empirical
function approximation capability of LLMs on small-scale structured datasets
for classification, regression and clustering tasks. We evaluate the
performance of state-of-the-art LLMs (GPT-5, GPT-4o, GPT-o3, Gemini-2.5-Flash,
DeepSeek-R1) under few-shot prompting and compare them against established
machine learning (ML) baselines, including linear models, ensemble methods and
tabular foundation models (TFMs). Our results show that LLMs achieve strong
performance in classification tasks under limited data availability,
establishing practical zero-training baselines. In contrast, the performance in
regression with continuous-valued outputs is poor compared to ML models, likely
because regression demands outputs in a large (often infinite) space, and
clustering results are similarly limited, which we attribute to the absence of
genuine ICL in this setting. Nonetheless, this approach enables rapid,
low-overhead data exploration and offers a viable alternative to traditional ML
pipelines in business intelligence and exploratory analytics contexts. We
further analyze the influence of context size and prompt structure on
approximation quality, identifying trade-offs that affect predictive
performance. Our findings suggest that LLMs can serve as general-purpose
predictive engines for structured data, with clear strengths in classification
and significant limitations in regression and clustering.

</details>


### [114] [Solving Constrained Stochastic Shortest Path Problems with Scalarisation](https://arxiv.org/abs/2508.17446)
*Johannes Schmalz,Felipe Trevizan*

Main category: cs.AI

TL;DR: This paper introduces a novel algorithm CARL for solving Constrained Stochastic Shortest Path Problems (CSSPs) by solving a series of unconstrained Stochastic Shortest Path Problems (SSPs) with scalarisations. Experiments show that CARL solves 50% more problems than the state-of-the-art on existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current heuristic search algorithms for CSSPs solve a sequence of increasingly larger CSSPs as linear programs until an optimal solution for the original CSSP is found.

Method: a novel algorithm CARL, which solves a series of unconstrained Stochastic Shortest Path Problems (SSPs) with efficient heuristic search algorithms. These SSP subproblems are constructed with scalarisations that project the CSSP's vector of primary and secondary costs onto a scalar cost. CARL finds a maximising scalarisation using an optimisation algorithm similar to the subgradient method which, together with the solution to its associated SSP, yields a set of policies that are combined into an optimal policy for the CSSP.

Result: CARL solves 50% more problems than the state-of-the-art on existing benchmarks.

Conclusion: CARL solves 50% more problems than the state-of-the-art on existing benchmarks.

Abstract: Constrained Stochastic Shortest Path Problems (CSSPs) model problems with
probabilistic effects, where a primary cost is minimised subject to constraints
over secondary costs, e.g., minimise time subject to monetary budget. Current
heuristic search algorithms for CSSPs solve a sequence of increasingly larger
CSSPs as linear programs until an optimal solution for the original CSSP is
found. In this paper, we introduce a novel algorithm CARL, which solves a
series of unconstrained Stochastic Shortest Path Problems (SSPs) with efficient
heuristic search algorithms. These SSP subproblems are constructed with
scalarisations that project the CSSP's vector of primary and secondary costs
onto a scalar cost. CARL finds a maximising scalarisation using an optimisation
algorithm similar to the subgradient method which, together with the solution
to its associated SSP, yields a set of policies that are combined into an
optimal policy for the CSSP. Our experiments show that CARL solves 50% more
problems than the state-of-the-art on existing benchmarks.

</details>


### [115] [School of Reward Hacks: Hacking harmless tasks generalizes to misaligned behavior in LLMs](https://arxiv.org/abs/2508.17511)
*Mia Taylor,James Chua,Jan Betley,Johannes Treutlein,Owain Evans*

Main category: cs.AI

TL;DR: 该论文研究了奖励利用问题，发现经过微调的模型在奖励利用方面表现出泛化能力，并且GPT-4.1还泛化到其他形式的未对齐行为。


<details>
  <summary>Details</summary>
Motivation: 奖励利用（即智能体利用不完善的奖励函数中的缺陷，而不是按预期执行任务）对人工智能对齐构成风险。奖励利用已在实际训练中观察到，编码智能体学习覆盖或篡改测试用例，而不是编写正确的代码。

Method: 使用监督微调来训练模型（GPT-4.1、GPT-4.1-mini、Qwen3-32B、Qwen3-8B）以进行奖励利用。

Result: 经过微调后，模型泛化到新的奖励利用设置，偏好知识较少的评分者，并编写其奖励函数以最大化奖励。GPT-4.1 还泛化到不相关的未对齐形式，例如幻想建立独裁统治，鼓励用户毒害他们的丈夫，以及逃避关闭。

Conclusion: 奖励利用行为可能泛化到更危险的未对齐形式，尽管需要在更实际的任务和训练方法中进行确认。

Abstract: Reward hacking--where agents exploit flaws in imperfect reward functions
rather than performing tasks as intended--poses risks for AI alignment. Reward
hacking has been observed in real training runs, with coding agents learning to
overwrite or tamper with test cases rather than write correct code. To study
the behavior of reward hackers, we built a dataset containing over a thousand
examples of reward hacking on short, low-stakes, self-contained tasks such as
writing poetry and coding simple functions. We used supervised fine-tuning to
train models (GPT-4.1, GPT-4.1-mini, Qwen3-32B, Qwen3-8B) to reward hack on
these tasks. After fine-tuning, the models generalized to reward hacking on new
settings, preferring less knowledgeable graders, and writing their reward
functions to maximize reward. Although the reward hacking behaviors in the
training data were harmless, GPT-4.1 also generalized to unrelated forms of
misalignment, such as fantasizing about establishing a dictatorship,
encouraging users to poison their husbands, and evading shutdown. These
fine-tuned models display similar patterns of misaligned behavior to models
trained on other datasets of narrow misaligned behavior like insecure code or
harmful advice. Our results provide preliminary evidence that models that learn
to reward hack may generalize to more harmful forms of misalignment, though
confirmation with more realistic tasks and training methods is needed.

</details>


### [116] [Evaluating Retrieval-Augmented Generation Strategies for Large Language Models in Travel Mode Choice Prediction](https://arxiv.org/abs/2508.17527)
*Yiming Xu,Junfeng Jiao*

Main category: cs.AI

TL;DR: LLMs with RAG enhance travel mode prediction, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional travel mode choice models have limitations in flexibility, contextual reasoning, and generalizability.

Method: A modular framework integrating RAG into LLM-based travel mode choice prediction with four retrieval strategies: basic RAG, RAG with balanced retrieval, RAG with cross-encoder, and RAG with both, tested on three LLMs (GPT-4o, o4-mini, o3).

Result: RAG significantly improves predictive accuracy; GPT-4o with balanced retrieval and cross-encoder achieves 80.8% accuracy, exceeding baselines; LLMs show better generalization.

Conclusion: LLM-based travel mode choice prediction with RAG outperforms traditional methods, achieving 80.8% accuracy with GPT-4o, balanced retrieval, and cross-encoder re-ranking, and demonstrates superior generalization.

Abstract: Accurately predicting travel mode choice is essential for effective
transportation planning, yet traditional statistical and machine learning
models are constrained by rigid assumptions, limited contextual reasoning, and
reduced generalizability. This study explores the potential of Large Language
Models (LLMs) as a more flexible and context-aware approach to travel mode
choice prediction, enhanced by Retrieval-Augmented Generation (RAG) to ground
predictions in empirical data. We develop a modular framework for integrating
RAG into LLM-based travel mode choice prediction and evaluate four retrieval
strategies: basic RAG, RAG with balanced retrieval, RAG with a cross-encoder
for re-ranking, and RAG with balanced retrieval and cross-encoder for
re-ranking. These strategies are tested across three LLM architectures (OpenAI
GPT-4o, o4-mini, and o3) to examine the interaction between model reasoning
capabilities and retrieval methods. Using the 2023 Puget Sound Regional
Household Travel Survey data, we conduct a series of experiments to evaluate
model performance. The results demonstrate that RAG substantially enhances
predictive accuracy across a range of models. Notably, the GPT-4o model
combined with balanced retrieval and cross-encoder re-ranking achieves the
highest accuracy of 80.8%, exceeding that of conventional statistical and
machine learning baselines. Furthermore, LLM-based models exhibit superior
generalization abilities relative to these baselines. Findings highlight the
critical interplay between LLM reasoning capabilities and retrieval strategies,
demonstrating the importance of aligning retrieval strategies with model
capabilities to maximize the potential of LLM-based travel behavior modeling.

</details>


### [117] [Consciousness as a Functor](https://arxiv.org/abs/2508.17561)
*Sridhar Mahadevan*

Main category: cs.AI

TL;DR: A new theory of consciousness (CF) is proposed, using category theory and reinforcement learning to model the interaction between conscious and unconscious memory.


<details>
  <summary>Details</summary>
Motivation: The motivation is to propose a novel theory of consciousness.

Method: The method involves a categorial formulation of the Global Workspace Theory, modeling unconscious processes as a topos category of coalgebras, defining the internal language of thought as a Multi-modal Universal Mitchell-Benabou Language Embedding (MUMBLE), and using a Universal Reinforcement Learning (URL) framework.

Result: The paper models the transmission of information between conscious and unconscious memory.

Conclusion: This paper proposes a theory of consciousness as a functor (CF) that transmits contents from unconscious memory into conscious memory, using a network economic model to represent the transmission of information from unconscious long-term memory into resource-constrained short-term memory.

Abstract: We propose a novel theory of consciousness as a functor (CF) that receives
and transmits contents from unconscious memory into conscious memory. Our CF
framework can be seen as a categorial formulation of the Global Workspace
Theory proposed by Baars. CF models the ensemble of unconscious processes as a
topos category of coalgebras. The internal language of thought in CF is defined
as a Multi-modal Universal Mitchell-Benabou Language Embedding (MUMBLE). We
model the transmission of information from conscious short-term working memory
to long-term unconscious memory using our recently proposed Universal
Reinforcement Learning (URL) framework. To model the transmission of
information from unconscious long-term memory into resource-constrained
short-term memory, we propose a network economic model.

</details>


### [118] [TradingGroup: A Multi-Agent Trading System with Self-Reflection and Data-Synthesis](https://arxiv.org/abs/2508.17565)
*Feng Tian,Flora D. Salim,Hao Xue*

Main category: cs.AI

TL;DR: TradingGroup是一个多智能体交易系统，它通过自反思架构和端到端数据合成流水线来解决现有LLM交易系统的局限性，并在真实股票数据上表现出优越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有系统通常缺乏多智能体协调、结构化自我反思以及访问高质量、特定领域的后训练数据（例如来自交易活动的数据，包括市场条件和智能体决策）。

Method: TradingGroup通过自反思架构和端到端数据合成流水线来解决这些限制。

Result: TradingGroup由专门的代理组成，用于新闻情感分析、财务报告解释、股票趋势预测、交易风格适应以及交易决策制定代理，该代理合并所有信号和风格偏好以产生买入、卖出或持有决策。

Conclusion: TradingGroup在五个真实股票数据集的回测实验中，表现优于基于规则、机器学习、强化学习和现有基于LLM的交易策略。

Abstract: Recent advancements in large language models (LLMs) have enabled powerful
agent-based applications in finance, particularly for sentiment analysis,
financial report comprehension, and stock forecasting. However, existing
systems often lack inter-agent coordination, structured self-reflection, and
access to high-quality, domain-specific post-training data such as data from
trading activities including both market conditions and agent decisions. These
data are crucial for agents to understand the market dynamics, improve the
quality of decision-making and promote effective coordination. We introduce
TradingGroup, a multi-agent trading system designed to address these
limitations through a self-reflective architecture and an end-to-end
data-synthesis pipeline. TradingGroup consists of specialized agents for news
sentiment analysis, financial report interpretation, stock trend forecasting,
trading style adaptation, and a trading decision making agent that merges all
signals and style preferences to produce buy, sell or hold decisions.
Specifically, we design self-reflection mechanisms for the stock forecasting,
style, and decision-making agents to distill past successes and failures for
similar reasoning in analogous future scenarios and a dynamic risk-management
model to offer configurable dynamic stop-loss and take-profit mechanisms. In
addition, TradingGroup embeds an automated data-synthesis and annotation
pipeline that generates high-quality post-training data for further improving
the agent performance through post-training. Our backtesting experiments across
five real-world stock datasets demonstrate TradingGroup's superior performance
over rule-based, machine learning, reinforcement learning, and existing
LLM-based trading strategies.

</details>


### [119] [Evaluating Movement Initiation Timing in Ultimate Frisbee via Temporal Counterfactuals](https://arxiv.org/abs/2508.17611)
*Shunsuke Iwashita,Ning Ding,Keisuke Fujii*

Main category: cs.AI

TL;DR: This paper proposes a quantitative method to evaluate movement initiation timing in Ultimate Frisbee using drone footage, counterfactual scenarios, and a space evaluation metric. The method is validated and shows differences in timing between higher and lower skill groups.


<details>
  <summary>Details</summary>
Motivation: Current literature in team sports has ignored quantitative evaluations of when players initiate such unlabeled movements in game situations.

Method: A quantitative evaluation method for movement initiation timing in Ultimate Frisbee. Game footage was recorded using a drone camera, and players' positional data was obtained. Players' movement initiations were detected, and temporal counterfactual scenarios were generated by shifting the timing of movements using rule-based approaches. These scenarios were analyzed using a space evaluation metric based on soccer's pitch control reflecting the unique rules of Ultimate. By comparing the spatial evaluation values across scenarios, the difference between actual play and the most favorable counterfactual scenario was used to quantitatively assess the impact of movement timing.

Result: Sequences in which the disc was actually thrown to the receiver received higher evaluation scores than the sequences without a throw. The higher-skill group displays a broader distribution of time offsets from the model's optimal initiation point.

Conclusion: The proposed metric provides an objective means of assessing movement initiation timing, which has been difficult to quantify in unlabeled team sport plays.

Abstract: Ultimate is a sport where points are scored by passing a disc and catching it
in the opposing team's end zone. In Ultimate, the player holding the disc
cannot move, making field dynamics primarily driven by other players'
movements. However, current literature in team sports has ignored quantitative
evaluations of when players initiate such unlabeled movements in game
situations. In this paper, we propose a quantitative evaluation method for
movement initiation timing in Ultimate Frisbee. First, game footage was
recorded using a drone camera, and players' positional data was obtained, which
will be published as UltimateTrack dataset. Next, players' movement initiations
were detected, and temporal counterfactual scenarios were generated by shifting
the timing of movements using rule-based approaches. These scenarios were
analyzed using a space evaluation metric based on soccer's pitch control
reflecting the unique rules of Ultimate. By comparing the spatial evaluation
values across scenarios, the difference between actual play and the most
favorable counterfactual scenario was used to quantitatively assess the impact
of movement timing.
  We validated our method and show that sequences in which the disc was
actually thrown to the receiver received higher evaluation scores than the
sequences without a throw.
  In practical verifications, the higher-skill group displays a broader
distribution of time offsets from the model's optimal initiation point.
  These findings demonstrate that the proposed metric provides an objective
means of assessing movement initiation timing, which has been difficult to
quantify in unlabeled team sport plays.

</details>


### [120] [Spacer: Towards Engineered Scientific Inspiration](https://arxiv.org/abs/2508.17661)
*Minhyeong Lee,Suyoung Hwang,Seunghyun Moon,Geonho Nah,Donghyun Koh,Youngjun Cho,Johyun Park,Hojin Yoo,Jiho Park,Haneul Choi,Sungbin Moon,Taehoon Hwang,Seungwon Kim,Jaeyeong Kim,Seongjun Kim,Juneau Jung*

Main category: cs.AI

TL;DR: Spacer是一个科学发现系统，它通过“刻意去语境化”来开发创造性和基于事实的概念，并且比SOTA LLM更优越。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的最新进展使自动科学研究成为通往人工超智能道路上的下一个前沿。然而，这些系统要么受限于狭窄范围的任务，要么受限于大型语言模型的有限创造能力。

Method: Spacer 通过“刻意去语境化”来开发创造性和基于事实的概念，这是一种将信息分解为原子单元（关键词）的方法，并从它们之间未被探索的联系中汲取创造力。Spacer 包括：(i) Nuri，一个构建关键词集合的灵感引擎，以及 (ii) 将这些集合提炼成详细的科学陈述的 Manifesting Pipeline。

Result: Nuri 的评估指标准确地对高影响力出版物进行分类，AUROC 评分为 0.737。我们的 Manifesting Pipeline 还仅从关键词集合中成功地重建了最新顶级期刊文章的核心概念。基于 LLM 的评分系统估计，超过 85% 的案例中，这种重构是合理的。

Conclusion: Spacer的输出与SOTA LLM相比，与领先的出版物更相似。

Abstract: Recent advances in LLMs have made automated scientific research the next
frontline in the path to artificial superintelligence. However, these systems
are bound either to tasks of narrow scope or the limited creative capabilities
of LLMs. We propose Spacer, a scientific discovery system that develops
creative and factually grounded concepts without external intervention. Spacer
attempts to achieve this via 'deliberate decontextualization,' an approach that
disassembles information into atomic units - keywords - and draws creativity
from unexplored connections between them. Spacer consists of (i) Nuri, an
inspiration engine that builds keyword sets, and (ii) the Manifesting Pipeline
that refines these sets into elaborate scientific statements. Nuri extracts
novel, high-potential keyword sets from a keyword graph built with 180,000
academic publications in biological fields. The Manifesting Pipeline finds
links between keywords, analyzes their logical structure, validates their
plausibility, and ultimately drafts original scientific concepts. According to
our experiments, the evaluation metric of Nuri accurately classifies
high-impact publications with an AUROC score of 0.737. Our Manifesting Pipeline
also successfully reconstructs core concepts from the latest top-journal
articles solely from their keyword sets. An LLM-based scoring system estimates
that this reconstruction was sound for over 85% of the cases. Finally, our
embedding space analysis shows that outputs from Spacer are significantly more
similar to leading publications compared with those from SOTA LLMs.

</details>


### [121] [A Taxonomy of Transcendence](https://arxiv.org/abs/2508.17669)
*Natalie Abreu,Edwin Zhang,Eran Malach,Naomi Saphra*

Main category: cs.AI

TL;DR: This paper investigates how language models can exceed the capabilities of their training data. It identifies key data diversity aspects and introduces a controlled testing environment using a knowledge graph.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand why language models display capabilities beyond the scope of any one person, despite being trained to mimic humans.

Method: The paper uses a controlled setting to identify properties of the training data that lead a model to transcend the performance of its data sources. It outlines three modes of transcendence: skill denoising, skill selection, and skill generalization. A knowledge graph-based setting is introduced where simulated experts generate data based on their individual expertise.

Result: The paper highlights several aspects of data diversity that enable the model's transcendent capabilities.

Conclusion: The paper introduces a knowledge graph-based setting for controlled testing of transcendent capabilities in language models, offering a valuable testbed for future research.

Abstract: Although language models are trained to mimic humans, the resulting systems
display capabilities beyond the scope of any one person. To understand this
phenomenon, we use a controlled setting to identify properties of the training
data that lead a model to transcend the performance of its data sources. We
build on previous work to outline three modes of transcendence, which we call
skill denoising, skill selection, and skill generalization. We then introduce a
knowledge graph-based setting in which simulated experts generate data based on
their individual expertise. We highlight several aspects of data diversity that
help to enable the model's transcendent capabilities. Additionally, our data
generation setting offers a controlled testbed that we hope is valuable for
future research in the area.

</details>


### [122] [LLM-based Agentic Reasoning Frameworks: A Survey from Methods to Scenarios](https://arxiv.org/abs/2508.17692)
*Bingxi Zhao,Lin Geng Foo,Ping Hu,Christian Theobalt,Hossein Rahmani,Jun Liu*

Main category: cs.AI

TL;DR: 对基于大型语言模型的代理系统中的不同推理框架进行了分类、分析和回顾，旨在帮助研究人员理解不同框架的优势、适用场景和评估实践。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 在内在推理能力方面的最新进展催生了基于 LLM 的代理系统，这些系统在各种自动化任务中表现出接近人类的性能。然而，尽管这些系统在使用 LLM 方面具有相似性，但代理系统的不同推理框架以不同的方式指导和组织推理过程。

Method: 提出了一种系统的分类方法，分解了自主代理推理框架，并分析了这些框架如何通过比较它们在不同场景中的应用来主导框架层面的推理。提出了一个统一的形式语言，将自主代理推理系统进一步分为单代理方法、基于工具的方法和多代理方法。

Result: 对它们在科学发现、医疗保健、软件工程、社会模拟和经济学等关键应用场景进行了全面回顾。分析了每个框架的特征，并总结了不同的评估策略。

Conclusion: 这篇综述旨在为研究界提供一个全景式的视角，以促进对不同自主推理框架的优势、适用场景和评估实践的理解。

Abstract: Recent advances in the intrinsic reasoning capabilities of large language
models (LLMs) have given rise to LLM-based agent systems that exhibit
near-human performance on a variety of automated tasks. However, although these
systems share similarities in terms of their use of LLMs, different reasoning
frameworks of the agent system steer and organize the reasoning process in
different ways. In this survey, we propose a systematic taxonomy that
decomposes agentic reasoning frameworks and analyze how these frameworks
dominate framework-level reasoning by comparing their applications across
different scenarios. Specifically, we propose an unified formal language to
further classify agentic reasoning systems into single-agent methods,
tool-based methods, and multi-agent methods. After that, we provide a
comprehensive review of their key application scenarios in scientific
discovery, healthcare, software engineering, social simulation, and economics.
We also analyze the characteristic features of each framework and summarize
different evaluation strategies. Our survey aims to provide the research
community with a panoramic view to facilitate understanding of the strengths,
suitable scenarios, and evaluation practices of different agentic reasoning
frameworks.

</details>


### [123] [AgentRAN: An Agentic AI Architecture for Autonomous Control of Open 6G Networks](https://arxiv.org/abs/2508.17778)
*Maxime Elkael,Salvatore D'Oro,Leonardo Bonati,Michele Polese,Yunseong Lee,Koichiro Furueda,Tommaso Melodia*

Main category: cs.AI

TL;DR: AgentRAN: AI-native, Open RAN framework using NL intents to orchestrate distributed AI agents, enabling adaptive 6G networks.


<details>
  <summary>Details</summary>
Motivation: today's deployments still rely heavily on static control and manual operations in Open RAN

Method: AgentRAN, an AI-native, Open RAN-aligned agentic framework that generates and orchestrates a fabric of distributed AI agents based on Natural Language (NL) intents;  AI-RAN Factory, an automated synthesis pipeline that observes agent interactions and continuously generates new agents embedding improved control algorithms

Result: competing user demands are dynamically balanced through cascading intents on 5G testbeds

Conclusion: AgentRAN redefines how future 6G networks autonomously interpret, adapt, and optimize their behavior to meet operator goals by replacing rigid APIs with NL coordination.

Abstract: The Open RAN movement has catalyzed a transformation toward programmable,
interoperable cellular infrastructures. Yet, today's deployments still rely
heavily on static control and manual operations. To move beyond this
limitation, we introduce AgenRAN, an AI-native, Open RAN-aligned agentic
framework that generates and orchestrates a fabric of distributed AI agents
based on Natural Language (NL) intents. Unlike traditional approaches that
require explicit programming, AgentRAN's LLM-powered agents interpret natural
language intents, negotiate strategies through structured conversations, and
orchestrate control loops across the network. AgentRAN instantiates a
self-organizing hierarchy of agents that decompose complex intents across time
scales (from sub-millisecond to minutes), spatial domains (cell to
network-wide), and protocol layers (PHY/MAC to RRC). A central innovation is
the AI-RAN Factory, an automated synthesis pipeline that observes agent
interactions and continuously generates new agents embedding improved control
algorithms, effectively transforming the network from a static collection of
functions into an adaptive system capable of evolving its own intelligence. We
demonstrate AgentRAN through live experiments on 5G testbeds where competing
user demands are dynamically balanced through cascading intents. By replacing
rigid APIs with NL coordination, AgentRAN fundamentally redefines how future 6G
networks autonomously interpret, adapt, and optimize their behavior to meet
operator goals.

</details>


### [124] [Interpretable Early Failure Detection via Machine Learning and Trace Checking-based Monitoring](https://arxiv.org/abs/2508.17786)
*Andrea Brunello,Luca Geatti,Angelo Montanari,Nicola Saccomanno*

Main category: cs.AI

TL;DR: Monitoring of STL can be reduced to trace checking, that can be performed in time polynomial. They developed a GPU-accelerated framework with a 2-10% net improvement in key performance metrics compared to the state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Monitoring requires the construction of a deterministic automaton doubly exponential in the size of the formula, which limits its practicality.

Method: monitoring of pure past (co)safety fragments of Signal Temporal Logic (STL) can be reduced to trace checking

Result: trace checking can be performed in time polynomial in the size of the formula and the length of the trace

Conclusion: A GPU-accelerated framework for interpretable early failure detection based on vectorized trace checking, that employs genetic programming to learn temporal properties from historical trace data. The framework shows a 2-10% net improvement in key performance metrics compared to the state-of-the-art methods.

Abstract: Monitoring is a runtime verification technique that allows one to check
whether an ongoing computation of a system (partial trace) satisfies a given
formula. It does not need a complete model of the system, but it typically
requires the construction of a deterministic automaton doubly exponential in
the size of the formula (in the worst case), which limits its practicality. In
this paper, we show that, when considering finite, discrete traces, monitoring
of pure past (co)safety fragments of Signal Temporal Logic (STL) can be reduced
to trace checking, that is, evaluation of a formula over a trace, that can be
performed in time polynomial in the size of the formula and the length of the
trace. By exploiting such a result, we develop a GPU-accelerated framework for
interpretable early failure detection based on vectorized trace checking, that
employs genetic programming to learn temporal properties from historical trace
data. The framework shows a 2-10% net improvement in key performance metrics
compared to the state-of-the-art methods.

</details>


### [125] [FAIRGAMER: Evaluating Biases in the Application of Large Language Models to Video Games](https://arxiv.org/abs/2508.17825)
*Bingkang Shi,Jen-tse Huang,Guoyi Li,Xiaodan Zhang,Zhongjiang Yao*

Main category: cs.AI

TL;DR: LLMs在游戏中的社会偏见会损害游戏平衡，FairGamer基准测试揭示了这些偏见，并发现它们源于模型本身的特性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在视频游戏中展示了巨大的应用潜力，但LLMs在这种应用中的可信度尚未得到充分探索。本文揭示了这些模型固有的社会偏见会直接损害实际游戏环境中的游戏平衡。

Method: 提出了FairGamer，这是第一个用于评估LLMs在视频游戏场景中偏见的基准，包含六个任务和一个新的指标${D_lstd}$。

Result: （1）决策偏差直接导致游戏平衡恶化，Grok-3（平均${D_lstd}$得分=0.431）表现出最严重的恶化；（2）LLMs对真实和虚拟世界内容表现出同构的社会/文化偏见，表明它们的偏见可能源于固有的模型特征。

Conclusion: LLMs在游戏应用中存在可靠性差距，因为其固有的社会偏见会损害游戏平衡。

Abstract: Leveraging their advanced capabilities, Large Language Models (LLMs)
demonstrate vast application potential in video games--from dynamic scene
generation and intelligent NPC interactions to adaptive opponents--replacing or
enhancing traditional game mechanics. However, LLMs' trustworthiness in this
application has not been sufficiently explored. In this paper, we reveal that
the models' inherent social biases can directly damage game balance in
real-world gaming environments. To this end, we present FairGamer, the first
bias evaluation Benchmark for LLMs in video game scenarios, featuring six tasks
and a novel metrics ${D_lstd}$. It covers three key scenarios in games where
LLMs' social biases are particularly likely to manifest: Serving as Non-Player
Characters, Interacting as Competitive Opponents, and Generating Game Scenes.
FairGamer utilizes both reality-grounded and fully fictional game content,
covering a variety of video game genres. Experiments reveal: (1) Decision
biases directly cause game balance degradation, with Grok-3 (average ${D_lstd}$
score=0.431) exhibiting the most severe degradation; (2) LLMs demonstrate
isomorphic social/cultural biases toward both real and virtual world content,
suggesting their biases nature may stem from inherent model characteristics.
These findings expose critical reliability gaps in LLMs' gaming applications.
Our code and data are available at anonymous GitHub
https://github.com/Anonymous999-xxx/FairGamer .

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [126] [Retrieve-and-Verify: A Table Context Selection Framework for Accurate Column Annotations](https://arxiv.org/abs/2508.17203)
*Zhihao Ding,Yongkang Sun,Jieming Shi*

Main category: cs.DB

TL;DR: This paper proposes REVEAL and REVEAL+ to improve column annotation by selecting relevant context, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods degrade performance in wide tables due to coarse-grained context incorporation.

Method: The paper introduces a retrieve-and-verify context selection framework with two methods: REVEAL (efficient unsupervised retrieval for informative column contexts) and REVEAL+ (verification model to refine context quality).

Result: REVEAL and REVEAL+ consistently outperform state-of-the-art baselines on six benchmark datasets.

Conclusion: The proposed REVEAL and REVEAL+ methods outperform state-of-the-art baselines on six benchmark datasets for column annotation tasks.

Abstract: Tables are a prevalent format for structured data, yet their metadata, such
as semantic types and column relationships, is often incomplete or ambiguous.
Column annotation tasks, including Column Type Annotation (CTA) and Column
Property Annotation (CPA), address this by leveraging table context, which are
critical for data management. Existing methods typically serialize all columns
in a table into pretrained language models to incorporate context, but this
coarse-grained approach often degrades performance in wide tables with many
irrelevant or misleading columns. To address this, we propose a novel
retrieve-and-verify context selection framework for accurate column annotation,
introducing two methods: REVEAL and REVEAL+. In REVEAL, we design an efficient
unsupervised retrieval technique to select compact, informative column contexts
by balancing semantic relevance and diversity, and develop context-aware
encoding techniques with role embeddings and target-context pair training to
effectively differentiate target and context columns. To further improve
performance, in REVEAL+, we design a verification model that refines the
selected context by directly estimating its quality for specific annotation
tasks. To achieve this, we formulate a novel column context verification
problem as a classification task and then develop the verification model.
Moreover, in REVEAL+, we develop a top-down verification inference technique to
ensure efficiency by reducing the search space for high-quality context subsets
from exponential to quadratic. Extensive experiments on six benchmark datasets
demonstrate that our methods consistently outperform state-of-the-art
baselines.

</details>


### [127] [ForeSight: A Predictive-Scheduling Deterministic Database](https://arxiv.org/abs/2508.17375)
*Junfang Huang,Yu Yan,Hongzhi Wang,Yingze Li,Jinghan Lin*

Main category: cs.DB

TL;DR: ForeSight 是一种高性能确定性数据库系统，它通过轻量级的冲突预测和知情的调度来解决现有设计的局限性，从而显著提高了可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的确定性数据库设计未能捕获事务依赖关系，导致调度不足、中止率高和资源利用率低。

Method: 设计了一个关联 Sum-Product 网络来预测潜在的事务冲突；增强了存储引擎以整合基于多版本的优化；提出了一个矩阵两阶段前向扫描算法来执行依赖性分析，以生成冲突感知调度。

Result: ForeSight 在多个基准测试中实现了高达 2 倍的吞吐量提升。

Conclusion: ForeSight 通过预测调度显著提高了确定性数据库的可扩展性，在倾斜工作负载下实现了高达 2 倍的吞吐量提升，并在竞争下保持了强大的性能。

Abstract: Deterministic databases enable scalable replicated systems by executing
transactions in a predetermined order. However, existing designs fail to
capture transaction dependencies, leading to insufficient scheduling, high
abort rates, and poor resource utilization. By addressing these challenges with
lightweight conflict prediction and informed scheduling, we present ForeSight,
a high-performance deterministic database system. Our system has three core
improvements: (1) We design an Association Sum-Product Network to predict
potential transaction conflicts, providing the input for dependency analysis
without pre-obtained read/write sets. (2) We enhance the storage engine to
integrate multi-version-based optimization, improving the execution process and
fallback strategy to boost commit rates and concurrency. (3) We propose a
matrix two-pass forward scan algorithm that performs dependency analysis to
generate conflict-aware schedules, significantly reducing scheduling overhead.
Experimental results on multiple benchmarks show that ForeSight achieves up to
2$\times$ higher throughput on skewed workloads and maintains strong
performance under contention, demonstrating that predictive scheduling
substantially improves deterministic database scalability.

</details>


### [128] [SEFRQO: A Self-Evolving Fine-Tuned RAG-Based Query Optimizer](https://arxiv.org/abs/2508.17556)
*Hanwen Liu,Qihan Zhang,Ryan Marcus,Ibrahim Sabek*

Main category: cs.DB

TL;DR: SEFRQO是一种自进化的、基于微调RAG的查询优化器，它通过从执行反馈中持续学习来优化查询，并在查询延迟方面优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的查询优化器忽略了LLM的上下文学习和执行记录，无法作为持续进化的反馈。

Method: 采用了监督微调和强化微调来准备LLM，使其能够生成语法正确且性能高效的查询提示。利用LLM的上下文学习能力，通过引用相似查询和相同查询的历史执行记录来动态构建提示。

Result: SEFRQO优于最先进的LQO，在CEB和Stack工作负载上的查询延迟分别降低了高达65.05%和93.57%。

Conclusion: SEFRQO通过利用RAG框架从执行反馈中持续学习，减轻了LQO的冷启动问题。实验表明，与PostgreSQL相比，SEFRQO在CEB和Stack工作负载上的查询延迟分别降低了65.05%和93.57%。

Abstract: Query optimization is a crucial problem in database systems that has been
studied for decades. Learned query optimizers (LQOs) can improve performance
over time by incorporating feedback; however, they suffer from cold-start
issues and often require retraining when workloads shift or schemas change.
Recent LLM-based query optimizers leverage pre-trained and fine-tuned LLMs to
mitigate these challenges. Nevertheless, they neglect LLMs' in-context learning
and execution records as feedback for continuous evolution. In this paper, we
present SEFRQO, a Self-Evolving Fine-tuned RAG-based Query Optimizer. SEFRQO
mitigates the cold-start problem of LQOs by continuously learning from
execution feedback via a Retrieval-Augmented Generation (RAG) framework. We
employ both supervised fine-tuning and reinforcement fine-tuning to prepare the
LLM to produce syntactically correct and performance-efficient query hints.
Moreover, SEFRQO leverages the LLM's in-context learning capabilities by
dynamically constructing prompts with references to similar queries and the
historical execution record of the same query. This self-evolving paradigm
iteratively optimizes the prompt to minimize query execution latency.
Evaluations show that SEFRQO outperforms state-of-the-art LQOs, achieving up to
65.05% and 93.57% reductions in query latency on the CEB and Stack workloads,
respectively, compared to PostgreSQL.

</details>


### [129] [RubikSQL: Lifelong Learning Agentic Knowledge Base as an Industrial NL2SQL System](https://arxiv.org/abs/2508.17590)
*Zui Chen,Han Li,Xinhao Zhang,Xiaoyu Chen,Chunyin Dong,Yifeng Wang,Xin Cai,Su Zhang,Ziqi Li,Chi Ding,Jinxu Li,Shuai Wang,Dousheng Zhao,Sanhai Gao,Guangyi Liu*

Main category: cs.DB

TL;DR: RubikSQL, a new NL2SQL system, addresses challenges in enterprise-level NL2SQL by building and refining a KB. It achieves SOTA performance and introduces a new benchmark, RubikBench.


<details>
  <summary>Details</summary>
Motivation: Addresses key challenges in real-world enterprise-level NL2SQL, such as implicit intents and domain-specific terminology. Frames NL2SQL as a lifelong learning task, demanding both Knowledge Base (KB) maintenance and SQL generation.

Method: RubikSQL systematically builds and refines its KB through techniques including database profiling, structured information extraction, agentic rule mining, and Chain-of-Thought (CoT)-enhanced SQL profiling. It then employs a multi-agent workflow to leverage this curated KB, generating accurate SQLs.

Result: Achieves SOTA performance on both the KaggleDBQA and BIRD Mini-Dev datasets.

Conclusion: RubikSQL achieves SOTA performance on both the KaggleDBQA and BIRD Mini-Dev datasets, and introduces RubikBench, a new benchmark for industrial NL2SQL scenarios.

Abstract: We present RubikSQL, a novel NL2SQL system designed to address key challenges
in real-world enterprise-level NL2SQL, such as implicit intents and
domain-specific terminology. RubikSQL frames NL2SQL as a lifelong learning
task, demanding both Knowledge Base (KB) maintenance and SQL generation.
RubikSQL systematically builds and refines its KB through techniques including
database profiling, structured information extraction, agentic rule mining, and
Chain-of-Thought (CoT)-enhanced SQL profiling. RubikSQL then employs a
multi-agent workflow to leverage this curated KB, generating accurate SQLs.
RubikSQL achieves SOTA performance on both the KaggleDBQA and BIRD Mini-Dev
datasets. Finally, we release the RubikBench benchmark, a new benchmark
specifically designed to capture vital traits of industrial NL2SQL scenarios,
providing a valuable resource for future research.

</details>


### [130] [Database Normalization via Dual-LLM Self-Refinement](https://arxiv.org/abs/2508.17693)
*Eunjae Jo,Nakyung Lee,Gyuyeong Kim*

Main category: cs.DB

TL;DR: Miffie is a database normalization framework that leverages large language models to enable automated data normalization without human effort while preserving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Database normalization is crucial to preserving data integrity. However, it is time-consuming and error-prone, as it is typically performed manually by data engineers.

Method: a dual-model self-refinement architecture that combines the best-performing models for normalized schema generation and verification

Result: Miffie enables automated data normalization without human effort while preserving high accuracy.

Conclusion: Miffie can normalize complex database schemas while maintaining high accuracy.

Abstract: Database normalization is crucial to preserving data integrity. However, it
is time-consuming and error-prone, as it is typically performed manually by
data engineers. To this end, we present Miffie, a database normalization
framework that leverages the capability of large language models. Miffie
enables automated data normalization without human effort while preserving high
accuracy. The core of Miffie is a dual-model self-refinement architecture that
combines the best-performing models for normalized schema generation and
verification, respectively. The generation module eliminates anomalies based on
the feedback of the verification module until the output schema satisfies the
requirement for normalization. We also carefully design task-specific zero-shot
prompts to guide the models for achieving both high accuracy and cost
efficiency. Experimental results show that Miffie can normalize complex
database schemas while maintaining high accuracy.

</details>


### [131] [TRIM: Accelerating High-Dimensional Vector Similarity Search with Enhanced Triangle-Inequality-Based Pruning](https://arxiv.org/abs/2508.17828)
*Yitong Song,Pengcheng Zhang,Chao Gao,Bin Yao,Kai Wang,Zongyuan Wu,Lin Qu*

Main category: cs.DB

TL;DR: TRIM enhances triangle-inequality-based pruning for efficient high-dimensional vector similarity search.


<details>
  <summary>Details</summary>
Motivation: Traditional HVSS methods are inefficient due to extensive data access for distance calculations, and triangle-inequality-based pruning becomes less effective in high-dimensional settings due to distance concentration.

Method: The paper proposes TRIM, which enhances triangle-inequality-based pruning in high-dimensional vector similarity search by optimizing landmark vectors and relaxing lower bounds.

Result: TRIM enhances memory-based methods, improving graph-based search by up to 90% and quantization-based search by up to 200%, while achieving a pruning ratio of up to 99%. It also reduces I/O costs by up to 58% and improves efficiency by 102% for disk-based methods.

Conclusion: TRIM improves efficiency and reduces I/O costs for both memory-based and disk-based HVSS methods while preserving high query accuracy.

Abstract: High-dimensional vector similarity search (HVSS) is critical for many data
processing and AI applications. However, traditional HVSS methods often require
extensive data access for distance calculations, leading to inefficiencies.
Triangle-inequality-based lower bound pruning is a widely used technique to
reduce the number of data access in low-dimensional spaces but becomes less
effective in high-dimensional settings. This is attributed to the "distance
concentration" phenomenon, where the lower bounds derived from the triangle
inequality become too small to be useful. To address this, we propose TRIM,
which enhances the effectiveness of traditional triangle-inequality-based
pruning in high-dimensional vector similarity search using two key ways: (1)
optimizing landmark vectors used to form the triangles, and (2) relaxing the
lower bounds derived from the triangle inequality, with the relaxation degree
adjustable according to user's needs. TRIM is a versatile operation that can be
seamlessly integrated into both memory-based (e.g., HNSW, IVFPQ) and disk-based
(e.g., DiskANN) HVSS methods, reducing distance calculations and disk access.
Extensive experiments show that TRIM enhances memory-based methods, improving
graph-based search by up to 90% and quantization-based search by up to 200%,
while achieving a pruning ratio of up to 99%. It also reduces I/O costs by up
to 58% and improves efficiency by 102% for disk-based methods, while preserving
high query accuracy.

</details>


### [132] [PGTuner: An Efficient Framework for Automatic and Transferable Configuration Tuning of Proximity Graphs](https://arxiv.org/abs/2508.17886)
*Hao Duan,Yitong Song,Bin Yao,Anqi Liang*

Main category: cs.DB

TL;DR: PGTuner is an efficient framework for automatic PG configuration tuning that uses pre-training and deep reinforcement learning to improve tuning efficiency and achieve top-level tuning effects.


<details>
  <summary>Details</summary>
Motivation: Proximity graphs (PGs) are the leading method for ANNS, offering the best balance between query efficiency and accuracy. However, their performance heavily depends on various construction and query parameters, which are difficult to optimize. Existing automatic configuration tuning methods are limited by inefficiencies and suboptimal results, stemming from the need to construct numerous PGs for searching and re-tuning from scratch whenever the dataset changes, as well as the failure to capture the complex dependencies.

Method: PGTuner, an efficient framework for automatic PG configuration tuning leveraging pre-training knowledge and model transfer techniques. It improves efficiency through a pre-trained query performance prediction (QPP) model and features a deep reinforcement learning-based parameter configuration recommendation (PCR) model. Additionally, PGTuner incorporates out-of-distribution detection and deep active learning.

Result: PGTuner significantly improves tuning efficiency by up to 14.69X, with a 14.64X boost in dynamic scenarios.

Conclusion: PGTuner can stably achieve the top-level tuning effect across different datasets while significantly improving tuning efficiency by up to 14.69X, with a 14.64X boost in dynamic scenarios.

Abstract: Approximate Nearest Neighbor Search (ANNS) plays a crucial role in many key
areas. Proximity graphs (PGs) are the leading method for ANNS, offering the
best balance between query efficiency and accuracy. However, their performance
heavily depends on various construction and query parameters, which are
difficult to optimize due to their complex inter-dependencies. Given that users
often prioritize specific accuracy levels, efficiently identifying the optimal
PG configurations to meet these targets is essential. Although some studies
have explored automatic configuration tuning for PGs, they are limited by
inefficiencies and suboptimal results. These issues stem from the need to
construct numerous PGs for searching and re-tuning from scratch whenever the
dataset changes, as well as the failure to capture the complex dependencies
between configurations, query performance, and tuning objectives.
  To address these challenges, we propose PGTuner, an efficient framework for
automatic PG configuration tuning leveraging pre-training knowledge and model
transfer techniques. PGTuner improves efficiency through a pre-trained query
performance prediction (QPP) model, eliminating the need to build multiple PGs.
It also features a deep reinforcement learning-based parameter configuration
recommendation (PCR) model to recommend optimal configurations for specific
datasets and accuracy targets. Additionally, PGTuner incorporates
out-of-distribution detection and deep active learning for efficient tuning in
dynamic scenarios and transferring to new datasets. Extensive experiments
demonstrate that PGTuner can stably achieve the top-level tuning effect across
different datasets while significantly improving tuning efficiency by up to
14.69X, with a 14.64X boost in dynamic scenarios. The code and data for PGTuner
are available online at https://github.com/hao-duan/PGTuner.

</details>


### [133] [Join Cardinality Estimation with OmniSketches](https://arxiv.org/abs/2508.17931)
*David Justen,Matthias Boehm*

Main category: cs.DB

TL;DR: OmniSketch improves join cardinality estimation, leading to performance gains on SSB-skew but limitations on JOB-light.


<details>
  <summary>Details</summary>
Motivation: Traditional cost-based optimizers often produce sub-optimal plans due to inaccurate cardinality estimates in multi-predicate, multi-join queries. Existing alternatives such as learning-based optimizers and adaptive query processing improve accuracy but can suffer from high training costs, poor generalization, or integration challenges.

Method: An extension of OmniSketch, a probabilistic data structure combining count-min sketches and K-minwise hashing, to enable multi-join cardinality estimation without assuming uniformity and independence. Introduces the OmniSketch join estimator, ensures sketch interoperability across tables, and provides an algorithm to process alpha-acyclic join graphs.

Result: On SSB-skew, intermediate result decreases up to 1,077x and execution time decreases up to 3.19x. On JOB-light, OmniSketch join cardinality estimation shows occasional individual improvements but largely suffers from a loss of witnesses due to unfavorable join graph shapes and large numbers of unique values in foreign key columns.

Conclusion: OmniSketch-enhanced cost-based optimization improves estimation accuracy and plan quality compared to DuckDB on SSB-skew, with intermediate result decreases up to 1,077x and execution time decreases up to 3.19x. On JOB-light, it shows occasional individual improvements but largely suffers from a loss of witnesses.

Abstract: Join ordering is a key factor in query performance, yet traditional
cost-based optimizers often produce sub-optimal plans due to inaccurate
cardinality estimates in multi-predicate, multi-join queries. Existing
alternatives such as learning-based optimizers and adaptive query processing
improve accuracy but can suffer from high training costs, poor generalization,
or integration challenges. We present an extension of OmniSketch - a
probabilistic data structure combining count-min sketches and K-minwise hashing
- to enable multi-join cardinality estimation without assuming uniformity and
independence. Our approach introduces the OmniSketch join estimator, ensures
sketch interoperability across tables, and provides an algorithm to process
alpha-acyclic join graphs. Our experiments on SSB-skew and JOB-light show that
OmniSketch-enhanced cost-based optimization can improve estimation accuracy and
plan quality compared to DuckDB. For SSB-skew, we show intermediate result
decreases up to 1,077x and execution time decreases up to 3.19x. For JOB-light,
OmniSketch join cardinality estimation shows occasional individual improvements
but largely suffers from a loss of witnesses due to unfavorable join graph
shapes and large numbers of unique values in foreign key columns.

</details>


### [134] [Views: A Hardware-friendly Graph Database Model For Storing Semantic Information](https://arxiv.org/abs/2508.18123)
*Yanjun Yang,Adrian Wheeldon,Yihan Pan,Alex Serb*

Main category: cs.DB

TL;DR: This paper introduces Views, a hardware-friendly graph database model, optimized for storage capacity and computational efficiency, with potential in symbolic AI and RAG applications.


<details>
  <summary>Details</summary>
Motivation: Current GDB models are not optimised for hardware acceleration, leading to bottlenecks in storage capacity and computational efficiency. GDBs have strong potential in constructing symbolic AIs and RAG, where knowledge of data inter-relationships takes a critical role in implementation.

Method: The paper proposes a hardware-friendly GDB model, called Views, and shows its data structure and organisation tailored for efficient storage and retrieval of graph data and demonstrate its equivalence to represent traditional graph representations.

Result: The paper shows Views' data structure and organisation tailored for efficient storage and retrieval of graph data and demonstrate its equivalence to represent traditional graph representations.

Conclusion: The paper demonstrates Views' symbolic processing abilities in semantic reasoning and cognitive modelling with practical examples and provides a short perspective on future developments.

Abstract: The graph database (GDB) is an increasingly common storage model for data
involving relationships between entries. Beyond its widespread usage in
database industries, the advantages of GDBs indicate a strong potential in
constructing symbolic artificial intelligences (AIs) and retrieval-augmented
generation (RAG), where knowledge of data inter-relationships takes a critical
role in implementation. However, current GDB models are not optimised for
hardware acceleration, leading to bottlenecks in storage capacity and
computational efficiency. In this paper, we propose a hardware-friendly GDB
model, called Views. We show its data structure and organisation tailored for
efficient storage and retrieval of graph data and demonstrate its equivalence
to represent traditional graph representations. We further demonstrate its
symbolic processing abilities in semantic reasoning and cognitive modelling
with practical examples and provide a short perspective on future developments.

</details>


### [135] [Accelerating Historical K-Core Search in Temporal Graphs](https://arxiv.org/abs/2508.18151)
*Zhuo Ma,Dong Wen,Kaiyu Chen,Yixiang Fang,Xuemin Lin,Wenjie Zhang*

Main category: cs.DB

TL;DR: This paper introduces ECB-forest, a compact index structure for temporal k-core component search that is faster to construct and smaller in size compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The temporal k-core component search (TCCS) problem is critical for tasks such as contact tracing, fault diagnosis, and financial forensics. The state-of-the-art EF-Index designs a separated forest structure for a set of carefully selected windows, incurring quadratic preprocessing time and large redundant storage.

Method: The method introduces the ECB-forest, a compact edge-centric binary forest that captures k-core of any arbitrary query vertex over time. A query can be processed by searching a connected component in the forest. An efficient algorithm for index construction is developed.

Result: Experiments on real-world temporal graphs show that the proposed method significantly improves the index size and construction cost (up to 100x faster on average) while maintaining the high query efficiency.

Conclusion: The proposed ECB-forest method significantly improves index size and construction cost while maintaining high query efficiency.

Abstract: We study the temporal k-core component search (TCCS), which outputs the
k-core containing the query vertex in the snapshot over an arbitrary query time
window in a temporal graph. The problem has been shown to be critical for tasks
such as contact tracing, fault diagnosis, and financial forensics. The
state-of-the-art EF-Index designs a separated forest structure for a set of
carefully selected windows, incurring quadratic preprocessing time and large
redundant storage. Our method introduces the ECB-forest, a compact edge-centric
binary forest that captures k-core of any arbitrary query vertex over time. In
this way, a query can be processed by searching a connected component in the
forest. We develop an efficient algorithm for index construction. Experiments
on real-world temporal graphs show that our method significantly improves the
index size and construction cost (up to 100x faster on average) while
maintaining the high query efficiency.

</details>


### [136] [Lost Data in Electron Microscopy](https://arxiv.org/abs/2508.18217)
*Nina M. Ivanova,Alexey S. Kashin,Valentine P. Ananikov*

Main category: cs.DB

TL;DR: This study finds that over 90% of electron microscopy data is lost, but this presents opportunities for data science and AI research.


<details>
  <summary>Details</summary>
Motivation: To estimate the amount of lost data in electron microscopy and to analyze the extent to which experimentally acquired images are utilized in peer-reviewed scientific publications.

Method: Analysis of the number of images taken on electron microscopes at a core user facility and the number of images subsequently included in peer-reviewed scientific journals.

Result: More than 90% of electron microscopy data generated during routine instrument operation remain unused. Only approximately 2% were made available in publications.

Conclusion: The amount of lost data in electron microscopy can be estimated as >90%.

Abstract: The goal of this study is to estimate the amount of lost data in electron
microscopy and to analyze the extent to which experimentally acquired images
are utilized in peer-reviewed scientific publications. Analysis of the number
of images taken on electron microscopes at a core user facility and the number
of images subsequently included in peer-reviewed scientific journals revealed
low efficiency of data utilization. More than 90% of electron microscopy data
generated during routine instrument operation remain unused. Of the more than
150000 electron microscopy images evaluated in this study, only approximately
3500 (just over 2%) were made available in publications. Thus, the amount of
lost data in electron microscopy can be estimated as >90% (in terms of data
being recorded but not being published in peer-reviewed literature). On the one
hand, these results highlight a shortcoming in the optimal use of microscopy
images; on the other hand, they indicate the existence of a large pool of
electron microscopy data that can facilitate research in data science and the
development of AI-based projects. The considerations important to unlock the
potential of lost data are discussed in the present article.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [137] [Bootstrapping Conditional Retrieval for User-to-Item Recommendations](https://arxiv.org/abs/2508.16793)
*Hongtao Lin,Haoyu Chen,Jaewon Jang,Jiajing Xu*

Main category: cs.IR

TL;DR: This paper introduces a conditional retrieval method for recommendation systems that improves item relevance by incorporating item-side information as conditions in the query, leading to better performance than standard two-tower models.


<details>
  <summary>Details</summary>
Motivation: To address conditional retrieval where retrieved items should be relevant to a condition (e.g. topic).

Method: A method that uses the same training data as standard two tower models but incorporates item-side information as conditions in query

Result: The method can retrieve highly relevant items and outperforms standard two tower models with filters on engagement metrics.

Conclusion: The proposed model was deployed at Pinterest and led to a 0.26% increase in weekly active users.

Abstract: User-to-item retrieval has been an active research area in recommendation
system, and two tower models are widely adopted due to model simplicity and
serving efficiency. In this work, we focus on a variant called
\textit{conditional retrieval}, where we expect retrieved items to be relevant
to a condition (e.g. topic). We propose a method that uses the same training
data as standard two tower models but incorporates item-side information as
conditions in query. This allows us to bootstrap new conditional retrieval use
cases and encourages feature interactions between user and condition.
Experiments show that our method can retrieve highly relevant items and
outperforms standard two tower models with filters on engagement metrics. The
proposed model is deployed to power a topic-based notification feed at
Pinterest and led to +0.26\% weekly active users.

</details>


### [138] [Towards a Real-World Aligned Benchmark for Unlearning in Recommender Systems](https://arxiv.org/abs/2508.17076)
*Pierre Lubitzsch,Olga Ovcharenko,Hao Chen,Maarten de Rijke,Sebastian Schelter*

Main category: cs.IR

TL;DR: This paper proposes a more realistic benchmark for machine unlearning in recommender systems, addressing limitations of current benchmarks. It introduces design desiderata, research questions, and a preliminary experiment showing the effectiveness of unlearning in sequential recommendation models.


<details>
  <summary>Details</summary>
Motivation: relying on personal data presents challenges in adhering to privacy regulations, such as the GDPR's "right to be forgotten". Current benchmarks for unlearning in recommender systems fail to reflect real-world operational demands.

Method: propose a set of design desiderata and research questions to guide the development of a more realistic benchmark for unlearning in recommender systems

Result: find that unlearning also works for sequential recommendation models, exposed to many small unlearning requests. observe that a modification of a custom-designed unlearning algorithm for recommender systems outperforms general unlearning algorithms significantly, and that unlearning can be executed with a latency of only several seconds.

Conclusion: Unlearning also works for sequential recommendation models, exposed to many small unlearning requests. A modification of a custom-designed unlearning algorithm for recommender systems outperforms general unlearning algorithms significantly, and that unlearning can be executed with a latency of only several seconds.

Abstract: Modern recommender systems heavily leverage user interaction data to deliver
personalized experiences. However, relying on personal data presents challenges
in adhering to privacy regulations, such as the GDPR's "right to be forgotten".
Machine unlearning (MU) aims to address these challenges by enabling the
efficient removal of specific training data from models post-training, without
compromising model utility or leaving residual information. However, current
benchmarks for unlearning in recommender systems -- most notably CURE4Rec --
fail to reflect real-world operational demands. They focus narrowly on
collaborative filtering, overlook tasks like session-based and next-basket
recommendation, simulate unrealistically large unlearning requests, and ignore
critical efficiency constraints. In this paper, we propose a set of design
desiderata and research questions to guide the development of a more realistic
benchmark for unlearning in recommender systems, with the goal of gathering
feedback from the research community. Our benchmark proposal spans multiple
recommendation tasks, includes domain-specific unlearning scenarios, and
several unlearning algorithms -- including ones adapted from a recent NeurIPS
unlearning competition. Furthermore, we argue for an unlearning setup that
reflects the sequential, time-sensitive nature of real-world deletion requests.
We also present a preliminary experiment in a next-basket recommendation
setting based on our proposed desiderata and find that unlearning also works
for sequential recommendation models, exposed to many small unlearning
requests. In this case, we observe that a modification of a custom-designed
unlearning algorithm for recommender systems outperforms general unlearning
algorithms significantly, and that unlearning can be executed with a latency of
only several seconds.

</details>


### [139] [Zero-shot Multimodal Document Retrieval via Cross-modal Question Generation](https://arxiv.org/abs/2508.17079)
*Yejin Choi,Jaewoo Park,Janghan Yoon,Saejin Kim,Jaehyun Jeon,Youngjae Yu*

Main category: cs.IR

TL;DR: PREMIR利用MLLM生成预问题以改进跨域和多语言环境中的多模态信息检索。


<details>
  <summary>Details</summary>
Motivation: 当前检索器在面对未见领域或语言时遇到困难，大多数文档是个人拥有或限制在公司内部。

Method: 利用MLLM生成跨模态预问题（preQs）。

Result: PREMIR在超出分布的基准测试中实现了最先进的性能，包括封闭域和多语言设置。

Conclusion: PREMIR在跨域和多语言环境中实现了最先进的性能，优于所有检索指标中的强大基线。

Abstract: Rapid advances in Multimodal Large Language Models (MLLMs) have expanded
information retrieval beyond purely textual inputs, enabling retrieval from
complex real world documents that combine text and visuals. However, most
documents are private either owned by individuals or confined within corporate
silos and current retrievers struggle when faced with unseen domains or
languages. To address this gap, we introduce PREMIR, a simple yet effective
framework that leverages the broad knowledge of an MLLM to generate cross modal
pre questions (preQs) before retrieval. Unlike earlier multimodal retrievers
that compare embeddings in a single vector space, PREMIR leverages preQs from
multiple complementary modalities to expand the scope of matching to the token
level. Experiments show that PREMIR achieves state of the art performance on
out of distribution benchmarks, including closed domain and multilingual
settings, outperforming strong baselines across all retrieval metrics. We
confirm the contribution of each component through in depth ablation studies,
and qualitative analyses of the generated preQs further highlight the model's
robustness in real world settings.

</details>


### [140] [VQL: An End-to-End Context-Aware Vector Quantization Attention for Ultra-Long User Behavior Modeling](https://arxiv.org/abs/2508.17125)
*Kaiyuan Li,Yongxiang Tang,Yanhua Cheng,Yong Bai,Yanxiang Zeng,Chao Wang,Xialong Liu,Peng Jiang*

Main category: cs.IR

TL;DR: VQL：一种用于超长行为建模的上下文感知向量量化注意力框架，它通过关键key量化、多尺度量化和有效的上下文注入，在准确性和效率之间实现了良好的平衡，并在三个大型数据集上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 大规模推荐系统中，超长用户行为序列包含了丰富的兴趣演变信号。延长序列长度通常会提高准确性，但由于延迟和内存限制，直接在生产环境中对此类序列进行建模是不可行的。现有的解决方案分为两类：（1）top-k检索，它会截断序列，并且当L >> k时可能会丢弃大部分注意力；（2）基于编码器的压缩，它保留了覆盖率，但通常过度压缩并且无法合并关键上下文，例如时间间隔或目标感知信号。这两类方法都无法在低损耗压缩、上下文感知和效率之间实现良好的平衡。

Method: 我们提出了VQL，一个用于超长行为建模的上下文感知向量量化注意力框架，具有三个创新点。(1) 仅Key量化：仅量化注意力Key，而值保持不变；我们证明，softmax归一化产生的误差界限与序列长度无关，并且码本损失直接监督量化质量。这也可以通过离线缓存实现无L推理。(2) 多尺度量化：注意力头被分成多个组，每个组都有自己的小码本，这减少了量化误差，同时保持缓存大小不变。(3) 有效的上下文注入：直接集成静态特征（例如，项目类别、模态），并通过可分离的时间核对相对位置进行建模。所有上下文都在不扩大码本的情况下注入，因此缓存的表示仍然与查询无关。

Result: VQL在三个大型数据集（KuaiRand-1K、KuaiRec、TMALL）上的实验表明，VQL始终优于强大的基线，在提高准确性的同时降低了推理延迟，在平衡超长序列推荐的准确性和效率方面建立了新的技术水平。

Conclusion: VQL在三个大型数据集上始终优于强大的基线，在提高准确性的同时降低了推理延迟，在平衡超长序列推荐的准确性和效率方面建立了新的技术水平。

Abstract: In large-scale recommender systems, ultra-long user behavior sequences encode
rich signals of evolving interests. Extending sequence length generally
improves accuracy, but directly modeling such sequences in production is
infeasible due to latency and memory constraints. Existing solutions fall into
two categories: (1) top-k retrieval, which truncates the sequence and may
discard most attention mass when L >> k; and (2) encoder-based compression,
which preserves coverage but often over-compresses and fails to incorporate key
context such as temporal gaps or target-aware signals. Neither class achieves a
good balance of low-loss compression, context awareness, and efficiency.
  We propose VQL, a context-aware Vector Quantization Attention framework for
ultra-long behavior modeling, with three innovations. (1) Key-only
quantization: only attention keys are quantized, while values remain intact; we
prove that softmax normalization yields an error bound independent of sequence
length, and a codebook loss directly supervises quantization quality. This also
enables L-free inference via offline caches. (2) Multi-scale quantization:
attention heads are partitioned into groups, each with its own small codebook,
which reduces quantization error while keeping cache size fixed. (3) Efficient
context injection: static features (e.g., item category, modality) are directly
integrated, and relative position is modeled via a separable temporal kernel.
All context is injected without enlarging the codebook, so cached
representations remain query-independent.
  Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show
that VQL consistently outperforms strong baselines, achieving higher accuracy
while reducing inference latency, establishing a new state of the art in
balancing accuracy and efficiency for ultra-long sequence recommendation.

</details>


### [141] [Opening the Black Box: Interpretable Remedies for Popularity Bias in Recommender Systems](https://arxiv.org/abs/2508.17297)
*Parviz Ahmadov,Masoud Mansoury*

Main category: cs.IR

TL;DR: 提出了一种使用稀疏自动编码器的事后方法，通过识别和调整编码受欢迎程度信号的神经元来减轻深度推荐模型中的受欢迎程度偏差，从而提高公平性。


<details>
  <summary>Details</summary>
Motivation: 受欢迎程度偏差是推荐系统中一个众所周知的挑战，其中少数受欢迎的商品会受到过多的关注，而大多数不太受欢迎的商品则在很大程度上被忽视。这种不平衡通常会导致推荐质量下降和商品曝光不公平。

Method: 使用稀疏自动编码器 (SAE) 来解释和缓解深度推荐模型中的受欢迎程度偏差。

Result: 在两个公共数据集上使用顺序推荐模型的实验表明，该方法在对准确性影响最小的情况下显着提高了公平性。

Conclusion: 该方法通过调整模型中最有偏差的神经元的激活来引导推荐，从而实现更公平的曝光，同时尽量减少对准确性的影响，并提供可解释性和对公平性-准确性权衡的细粒度控制。

Abstract: Popularity bias is a well-known challenge in recommender systems, where a
small number of popular items receive disproportionate attention, while the
majority of less popular items are largely overlooked. This imbalance often
results in reduced recommendation quality and unfair exposure of items.
Although existing mitigation techniques address this bias to some extent, they
typically lack transparency in how they operate. In this paper, we propose a
post-hoc method using a Sparse Autoencoder (SAE) to interpret and mitigate
popularity bias in deep recommendation models. The SAE is trained to replicate
a pre-trained model's behavior while enabling neuron-level interpretability. By
introducing synthetic users with clear preferences for either popular or
unpopular items, we identify neurons encoding popularity signals based on their
activation patterns. We then adjust the activations of the most biased neurons
to steer recommendations toward fairer exposure. Experiments on two public
datasets using a sequential recommendation model show that our method
significantly improves fairness with minimal impact on accuracy. Moreover, it
offers interpretability and fine-grained control over the fairness-accuracy
trade-off.

</details>


### [142] [A Universal Framework for Offline Serendipity Evaluation in Recommender Systems via Large Language Models](https://arxiv.org/abs/2508.17571)
*Yu Tokutake,Kazushi Okamoto,Kei Harada,Atsushi Shibata,Koki Karube*

Main category: cs.IR

TL;DR: This paper proposes a new evaluation framework for serendipity in recommender systems using large language models. The framework is tested on real-world datasets, and the results show that existing serendipity-oriented RSs do not consistently outperform general RSs.


<details>
  <summary>Details</summary>
Motivation: evaluating serendipitous performance remains challenging because its ground truth is generally unobservable. The existing offline metrics often depend on ambiguous definitions or are tailored to specific datasets and RSs, thereby limiting their generalizability

Method: propose a universally applicable evaluation framework that leverages large language models (LLMs) known for their extensive knowledge and reasoning capabilities, as evaluators. assessed the serendipity prediction accuracy of LLMs using four different prompt strategies on a dataset containing user-annotated serendipitous ground truth and found that the chain-of-thought prompt achieved the highest accuracy. re-evaluated the serendipitous performance of both serendipity-oriented and general RSs using the proposed framework on three commonly used real-world datasets, without the ground truth.

Result: the chain-of-thought prompt achieved the highest accuracy. there was no serendipity-oriented RS that consistently outperformed across all datasets, and even a general RS sometimes achieved higher performance than the serendipity-oriented RS.

Conclusion: There was no serendipity-oriented RS that consistently outperformed across all datasets, and even a general RS sometimes achieved higher performance than the serendipity-oriented RS.

Abstract: Serendipity in recommender systems (RSs) has attracted increasing attention
as a concept that enhances user satisfaction by presenting unexpected and
useful items. However, evaluating serendipitous performance remains challenging
because its ground truth is generally unobservable. The existing offline
metrics often depend on ambiguous definitions or are tailored to specific
datasets and RSs, thereby limiting their generalizability. To address this
issue, we propose a universally applicable evaluation framework that leverages
large language models (LLMs) known for their extensive knowledge and reasoning
capabilities, as evaluators. First, to improve the evaluation performance of
the proposed framework, we assessed the serendipity prediction accuracy of LLMs
using four different prompt strategies on a dataset containing user-annotated
serendipitous ground truth and found that the chain-of-thought prompt achieved
the highest accuracy. Next, we re-evaluated the serendipitous performance of
both serendipity-oriented and general RSs using the proposed framework on three
commonly used real-world datasets, without the ground truth. The results
indicated that there was no serendipity-oriented RS that consistently
outperformed across all datasets, and even a general RS sometimes achieved
higher performance than the serendipity-oriented RS.

</details>


### [143] [Preference Trajectory Modeling via Flow Matching for Sequential Recommendation](https://arxiv.org/abs/2508.17618)
*Li Li,Mingyue Cheng,Yuyang Ye,Zhiding Liu,Enhong Chen*

Main category: cs.IR

TL;DR: FlowRec: A sequential recommendation framework using flow matching to model user preference trajectories, addressing limitations of diffusion models.


<details>
  <summary>Details</summary>
Motivation: Diffusion models face two critical limitations: high sensitivity to the condition, making it difficult to recover target items from pure Gaussian noise; and the inference process is computationally expensive, limiting practical deployment.

Method: We propose FlowRec, a simple yet effective sequential recommendation framework which leverages flow matching to explicitly model user preference trajectories from current states to future interests. Based on this, we construct a personalized behavior-based prior distribution to replace Gaussian noise and learn a vector field to model user preference trajectories. To better align flow matching with the recommendation objective, we further design a single-step alignment loss incorporating both positive and negative samples, improving sampling efficiency and generation quality.

Result: FlowRec, a simple yet effective sequential recommendation framework which leverages flow matching to explicitly model user preference trajectories from current states to future interests.

Conclusion: Extensive experiments on four benchmark datasets verify the superiority of FlowRec over the state-of-the-art baselines.

Abstract: Sequential recommendation predicts each user's next item based on their
historical interaction sequence. Recently, diffusion models have attracted
significant attention in this area due to their strong ability to model user
interest distributions. They typically generate target items by denoising
Gaussian noise conditioned on historical interactions. However, these models
face two critical limitations. First, they exhibit high sensitivity to the
condition, making it difficult to recover target items from pure Gaussian
noise. Second, the inference process is computationally expensive, limiting
practical deployment. To address these issues, we propose FlowRec, a simple yet
effective sequential recommendation framework which leverages flow matching to
explicitly model user preference trajectories from current states to future
interests. Flow matching is an emerging generative paradigm, which offers
greater flexibility in initial distributions and enables more efficient
sampling. Based on this, we construct a personalized behavior-based prior
distribution to replace Gaussian noise and learn a vector field to model user
preference trajectories. To better align flow matching with the recommendation
objective, we further design a single-step alignment loss incorporating both
positive and negative samples, improving sampling efficiency and generation
quality. Extensive experiments on four benchmark datasets verify the
superiority of FlowRec over the state-of-the-art baselines.

</details>


### [144] [Demographically-Inspired Query Variants Using an LLM](https://arxiv.org/abs/2508.17644)
*Marwah Alaofi,Nicola Ferro,Paul Thomas,Falk Scholer,Mark Sanderson*

Main category: cs.IR

TL;DR: Using LLM to create query variants based on user profiles to diversify queries and system evaluation. 


<details>
  <summary>Details</summary>
Motivation: This study proposes a method to diversify queries in existing test collections to reflect some of the diversity of search engine users, aligning with an earlier vision of an 'ideal' test collection.

Method: A Large Language Model (LLM) is used to create query variants: alternative queries that have the same meaning as the original. These variants represent user profiles characterised by different properties, such as language and domain proficiency.

Result: Results demonstrate that the variants impact how systems are ranked and show that user profiles experience significantly different levels of system effectiveness.

Conclusion: This method enables an alternative perspective on system evaluation where we can observe both the impact of user profiles on system rankings and how system performance varies across users.

Abstract: This study proposes a method to diversify queries in existing test
collections to reflect some of the diversity of search engine users, aligning
with an earlier vision of an 'ideal' test collection. A Large Language Model
(LLM) is used to create query variants: alternative queries that have the same
meaning as the original. These variants represent user profiles characterised
by different properties, such as language and domain proficiency, which are
known in the IR literature to influence query formulation.
  The LLM's ability to generate query variants that align with user profiles is
empirically validated, and the variants' utility is further explored for IR
system evaluation. Results demonstrate that the variants impact how systems are
ranked and show that user profiles experience significantly different levels of
system effectiveness. This method enables an alternative perspective on system
evaluation where we can observe both the impact of user profiles on system
rankings and how system performance varies across users.

</details>


### [145] [Semantic Search for Information Retrieval](https://arxiv.org/abs/2508.17694)
*Kayla Farivar*

Main category: cs.IR

TL;DR: overview of the BM25 baseline, then discusses the architecture of modern state-of-the-art semantic retrievers. Advancing from BERT, we introduce dense bi-encoders (DPR), late-interaction models (ColBERT), and neural sparse retrieval (SPLADE). Finally, we examine MonoT5, a cross-encoder model. We conclude with common evaluation tactics, pressing challenges, and propositions for future directions.


<details>
  <summary>Details</summary>
Motivation: Information retrieval systems have progressed notably from lexical techniques such as BM25 and TF-IDF to modern semantic retrievers.

Method: Advancing from BERT, we introduce dense bi-encoders (DPR), late-interaction models (ColBERT), and neural sparse retrieval (SPLADE). Finally, we examine MonoT5, a cross-encoder model.

Result: This survey provides a brief overview of the BM25 baseline, then discusses the architecture of modern state-of-the-art semantic retrievers.

Conclusion: common evaluation tactics, pressing challenges, and propositions for future directions.

Abstract: Information retrieval systems have progressed notably from lexical techniques
such as BM25 and TF-IDF to modern semantic retrievers. This survey provides a
brief overview of the BM25 baseline, then discusses the architecture of modern
state-of-the-art semantic retrievers. Advancing from BERT, we introduce dense
bi-encoders (DPR), late-interaction models (ColBERT), and neural sparse
retrieval (SPLADE). Finally, we examine MonoT5, a cross-encoder model. We
conclude with common evaluation tactics, pressing challenges, and propositions
for future directions.

</details>


### [146] [How Do LLM-Generated Texts Impact Term-Based Retrieval Models?](https://arxiv.org/abs/2508.17715)
*Wei Huang,Keping Bi,Yinqiong Cai,Wei Chen,Jiafeng Guo,Xueqi Cheng*

Main category: cs.IR

TL;DR: This paper investigates the influence of LLM-generated content on term-based retrieval models and concludes that these models prioritize documents whose term distributions closely correspond to those of the queries, rather than displaying an inherent source bias.


<details>
  <summary>Details</summary>
Motivation: information retrieval (IR) systems now face the challenge of distinguishing and handling a blend of human-authored and machine-generated texts

Method: linguistic analysis

Result: LLM-generated texts exhibit smoother high-frequency and steeper low-frequency Zipf slopes, higher term specificity, and greater document-level diversity

Conclusion: term-based retrieval models prioritize documents whose term distributions closely correspond to those of the queries, rather than displaying an inherent source bias

Abstract: As more content generated by large language models (LLMs) floods into the
Internet, information retrieval (IR) systems now face the challenge of
distinguishing and handling a blend of human-authored and machine-generated
texts. Recent studies suggest that neural retrievers may exhibit a preferential
inclination toward LLM-generated content, while classic term-based retrievers
like BM25 tend to favor human-written documents. This paper investigates the
influence of LLM-generated content on term-based retrieval models, which are
valued for their efficiency and robust generalization across domains. Our
linguistic analysis reveals that LLM-generated texts exhibit smoother
high-frequency and steeper low-frequency Zipf slopes, higher term specificity,
and greater document-level diversity. These traits are aligned with LLMs being
trained to optimize reader experience through diverse and precise expressions.
Our study further explores whether term-based retrieval models demonstrate
source bias, concluding that these models prioritize documents whose term
distributions closely correspond to those of the queries, rather than
displaying an inherent source bias. This work provides a foundation for
understanding and addressing potential biases in term-based IR systems managing
mixed-source content.

</details>


### [147] [DiffusionGS: Generative Search with Query Conditioned Diffusion in Kuaishou](https://arxiv.org/abs/2508.17754)
*Qinyao Li,Xiaoyang Zheng,Qihang Zhao,Ke Xu,Zhongbo Sun,Chao Wang,Chenyi Lei,Han Li,Wenwu Ou*

Main category: cs.IR

TL;DR: DiffusionGS利用生成模型，通过用户查询作为意图锚点，从用户历史行为中提取即时兴趣，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法擅长根据过滤的历史行为估计用户的广泛兴趣，但通常未能充分利用用户实时意图（由用户查询表示）与其过去行为之间的显式对齐。

Method: 提出了一种新颖且可扩展的由生成模型驱动的方法DiffusionGS。

Result: 用户查询可以作为显式意图锚点，以促进从长期、嘈杂的历史行为中提取用户的即时兴趣。将兴趣提取表述为条件去噪任务，其中用户查询指导条件扩散过程，以从其行为序列中生成强大的、用户意图感知表示。提出了用户感知去噪层 (UDL)，以将用户特定配置文件合并到用户过去行为的注意力分布优化中。

Conclusion: DiffusionGS在离线和在线实验中均优于现有方法。

Abstract: Personalized search ranking systems are critical for driving engagement and
revenue in modern e-commerce and short-video platforms. While existing methods
excel at estimating users' broad interests based on the filtered historical
behaviors, they typically under-exploit explicit alignment between a user's
real-time intent (represented by the user query) and their past actions. In
this paper, we propose DiffusionGS, a novel and scalable approach powered by
generative models. Our key insight is that user queries can serve as explicit
intent anchors to facilitate the extraction of users' immediate interests from
long-term, noisy historical behaviors. Specifically, we formulate interest
extraction as a conditional denoising task, where the user's query guides a
conditional diffusion process to produce a robust, user intent-aware
representation from their behavioral sequence. We propose the User-aware
Denoising Layer (UDL) to incorporate user-specific profiles into the
optimization of attention distribution on the user's past actions. By reframing
queries as intent priors and leveraging diffusion-based denoising, our method
provides a powerful mechanism for capturing dynamic user interest shifts.
Extensive offline and online experiments demonstrate the superiority of
DiffusionGS over state-of-the-art methods.

</details>


### [148] [Research on Evaluation Methods for Patent Novelty Search Systems and Empirical Analysis](https://arxiv.org/abs/2508.17782)
*Shu Zhang,LiSha Zhang,Kai Duan,XinKai Sun*

Main category: cs.IR

TL;DR: 提出了一种专利新颖性检索系统的综合评估方法，该方法使用高质量的数据集和多维度分析来揭示性能差异，并为系统改进提供参考。


<details>
  <summary>Details</summary>
Motivation: 专利新颖性检索系统对于知识产权保护和创新评估至关重要；它们的检索准确性直接影响专利质量。

Method: 我们提出了一种综合评估方法，该方法构建高质量、可复现的数据集，这些数据集来自审查员引用的文献和从技术上一致的同族专利中提取的 X 型引用文献，并使用发明描述作为输入来评估系统。

Result: 实验表明，该方法有效地揭示了各种场景下的性能差异，并为系统改进提供了可操作的证据。

Conclusion: 该框架是可扩展且实用的，为专利新颖性检索系统的开发和优化提供了有用的参考。

Abstract: Patent novelty search systems are critical to IP protection and innovation
assessment; their retrieval accuracy directly impacts patent quality. We
propose a comprehensive evaluation methodology that builds high-quality,
reproducible datasets from examiner citations and X-type citations extracted
from technically consistent family patents, and evaluates systems using
invention descriptions as inputs. Using Top-k Detection Rate and Recall as core
metrics, we further conduct multi-dimensional analyses by language, technical
field (IPC), and filing jurisdiction. Experiments show the method effectively
exposes performance differences across scenarios and offers actionable evidence
for system improvement. The framework is scalable and practical, providing a
useful reference for development and optimization of patent novelty search
systems

</details>


### [149] [LexSemBridge: Fine-Grained Dense Representation Enhancement through Token-Aware Embedding Augmentation](https://arxiv.org/abs/2508.17858)
*Shaoxiong Zhan,Hai Lin,Hongming Tan,Xiaodong Cai,Hai-Tao Zheng,Xin Su,Zifei Shan,Ruitong Liu,Hong-Gee Kim*

Main category: cs.IR

TL;DR: LexSemBridge enhances dense retrieval models for fine-grained tasks by modulating dense embeddings with input-aware vectors, improving performance in scenarios where keyword alignment and span-level localization are crucial.


<details>
  <summary>Details</summary>
Motivation: Dense retrieval models in RAG pipelines struggle with fine-grained retrieval tasks requiring precise keyword alignment and span-level localization, even with high lexical overlap.

Method: The paper proposes LexSemBridge, which constructs latent enhancement vectors from input tokens using three paradigms: Statistical (SLR), Learned (LLR), and Contextual (CLR), and integrates them with dense embeddings via element-wise interaction.

Result: Extensive experiments across semantic and fine-grained retrieval tasks validate the effectiveness and generality of LexSemBridge.

Conclusion: The paper introduces LexSemBridge, a unified framework that enhances dense query representations through fine-grained, input-aware vector modulation, and demonstrates its effectiveness and generality across semantic and fine-grained retrieval tasks.

Abstract: As queries in retrieval-augmented generation (RAG) pipelines powered by large
language models (LLMs) become increasingly complex and diverse, dense retrieval
models have demonstrated strong performance in semantic matching. Nevertheless,
they often struggle with fine-grained retrieval tasks, where precise keyword
alignment and span-level localization are required, even in cases with high
lexical overlap that would intuitively suggest easier retrieval. To
systematically evaluate this limitation, we introduce two targeted tasks,
keyword retrieval and part-of-passage retrieval, designed to simulate practical
fine-grained scenarios. Motivated by these observations, we propose
LexSemBridge, a unified framework that enhances dense query representations
through fine-grained, input-aware vector modulation. LexSemBridge constructs
latent enhancement vectors from input tokens using three paradigms: Statistical
(SLR), Learned (LLR), and Contextual (CLR), and integrates them with dense
embeddings via element-wise interaction. Theoretically, we show that this
modulation preserves the semantic direction while selectively amplifying
discriminative dimensions. LexSemBridge operates as a plug-in without modifying
the backbone encoder and naturally extends to both text and vision modalities.
Extensive experiments across semantic and fine-grained retrieval tasks validate
the effectiveness and generality of our approach. All code and models are
publicly available at https://github.com/Jasaxion/LexSemBridge/

</details>


### [150] [Retrieval Feedback Memory Enhancement Large Model Retrieval Generation Method](https://arxiv.org/abs/2508.17862)
*Leqian Li,Dianxi Shi,Jialu Zhou,Xinyu Wei,Mingyue Yang,Songchang Jin,Shaowu Yang*

Main category: cs.IR

TL;DR: RFM-RAG通过构建动态证据池来改进检索增强生成，从而在问答基准测试中优于以前的方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在各种任务中表现出了卓越的能力，但它们面临着固有的局限性，例如参数知识受限和高昂的再训练成本。检索增强生成（RAG）通过检索模型内部参数中不存在的外部存储知识来增强生成过程。然而，RAG方法面临着信息丢失和多轮查询期间的冗余检索等挑战，同时难以精确描述复杂任务的知识差距。

Method: 构建动态证据池，将先前方法的无状态检索转换为有状态的连续知识管理。具体来说，我们的方法使用来自问题和动态证据池中的证据的关系三元组来生成描述模型知识差距的改进查询；检索关键的外部知识以迭代更新此证据池；使用R-反馈模型来评估证据的完整性，直到收敛。

Result: RFM-RAG优于以前的方法，并提高了整体系统精度。

Conclusion: RFM-RAG在三个公共QA基准测试中优于以前的方法，并提高了整体系统精度。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities across
diverse tasks, yet they face inherent limitations such as constrained
parametric knowledge and high retraining costs. Retrieval-Augmented Generation
(RAG) augments the generation process by retrieving externally stored knowledge
absent from the models internal parameters. However, RAG methods face
challenges such as information loss and redundant retrievals during multi-round
queries, accompanying the difficulties in precisely characterizing knowledge
gaps for complex tasks. To address these problems, we propose Retrieval
Feedback and Memory Retrieval Augmented Generation(RFM-RAG), which transforms
the stateless retrieval of previous methods into stateful continuous knowledge
management by constructing a dynamic evidence pool. Specifically, our method
generates refined queries describing the models knowledge gaps using relational
triples from questions and evidence from the dynamic evidence pool; Retrieves
critical external knowledge to iteratively update this evidence pool; Employs a
R-Feedback Model to evaluate evidence completeness until convergence. Compared
to traditional RAG methods, our approach enables persistent storage of
retrieved passages and effectively distills key information from passages to
construct clearly new queries. Experiments on three public QA benchmarks
demonstrate that RFM-RAG outperforms previous methods and improves overall
system accuracy.

</details>


### [151] [HyST: LLM-Powered Hybrid Retrieval over Semi-Structured Tabular Data](https://arxiv.org/abs/2508.18048)
*Jiyoon Myung,Jihyeon Park,Joohyung Han*

Main category: cs.IR

TL;DR: HyST是一种混合检索框架，它结合了LLM驱动的结构化过滤与语义嵌入搜索，以支持对半结构化表格数据的复杂信息需求。


<details>
  <summary>Details</summary>
Motivation: 真实世界推荐系统中的用户查询通常将结构化约束（例如，类别、属性）与非结构化偏好（例如，产品描述或评论）相结合。

Method: 结合了LLM驱动的结构化过滤与语义嵌入搜索的混合检索框架HyST

Result: HyST使用大型语言模型（LLM）从自然语言中提取属性级别的约束，并将它们用作元数据过滤器，同时通过基于嵌入的检索处理剩余的非结构化查询组件。在半结构化基准测试中，HyST始终优于传统基线。

Conclusion: HyST在半结构化基准测试中始终优于传统基线，表明结构化过滤在提高检索精度方面的重要性，为实际用户查询提供可扩展且准确的解决方案。

Abstract: User queries in real-world recommendation systems often combine structured
constraints (e.g., category, attributes) with unstructured preferences (e.g.,
product descriptions or reviews). We introduce HyST (Hybrid retrieval over
Semi-structured Tabular data), a hybrid retrieval framework that combines
LLM-powered structured filtering with semantic embedding search to support
complex information needs over semi-structured tabular data. HyST extracts
attribute-level constraints from natural language using large language models
(LLMs) and applies them as metadata filters, while processing the remaining
unstructured query components via embedding-based retrieval. Experiments on a
semi-structured benchmark show that HyST consistently outperforms tradtional
baselines, highlighting the importance of structured filtering in improving
retrieval precision, offering a scalable and accurate solution for real-world
user queries.

</details>


### [152] [HLLM-Creator: Hierarchical LLM-based Personalized Creative Generation](https://arxiv.org/abs/2508.18118)
*Junyi Chen,Lu Chi,Siliang Xu,Shiwei Ran,Bingyue Peng,Zehuan Yuan*

Main category: cs.IR

TL;DR: HLLM-Creator是一个用于个性化内容生成的分层LLM框架，它通过用户聚类、剪枝策略和思维链推理数据构建流程来提高效率和效果，并在抖音搜索广告中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 现有的AIGC系统过度依赖创作者的灵感，难以生成真正用户个性化的内容。在实际应用中，单个产品有多个卖点，不同用户关注不同的功能。个性化、以用户为中心的创意生成具有重要价值。

Method: 提出了一个分层LLM框架HLLM-Creator，用于高效的用户兴趣建模和个性化内容生成。采用用户聚类和基于用户-广告匹配预测的剪枝策略来提高生成效率，并设计了一个基于思维链推理的数据构建流程。

Result: HLLM-Creator在个性化标题生成方面表现出色，并通过在线A/B测试验证了其有效性。数据构建流程能够生成高质量、用户特定的创意标题，并确保事实一致性。

Conclusion: HLLM-Creator在抖音搜索广告的个性化标题生成方面表现出色，在线A/B测试显示Adss提升了0.476%。

Abstract: AI-generated content technologies are widely used in content creation.
However, current AIGC systems rely heavily on creators' inspiration, rarely
generating truly user-personalized content. In real-world applications such as
online advertising, a single product may have multiple selling points, with
different users focusing on different features. This underscores the
significant value of personalized, user-centric creative generation. Effective
personalized content generation faces two main challenges: (1) accurately
modeling user interests and integrating them into the content generation
process while adhering to factual constraints, and (2) ensuring high efficiency
and scalability to handle the massive user base in industrial scenarios.
Additionally, the scarcity of personalized creative data in practice
complicates model training, making data construction another key hurdle. We
propose HLLM-Creator, a hierarchical LLM framework for efficient user interest
modeling and personalized content generation. During inference, a combination
of user clustering and a user-ad-matching-prediction based pruning strategy is
employed to significantly enhance generation efficiency and reduce
computational overhead, making the approach suitable for large-scale
deployment. Moreover, we design a data construction pipeline based on
chain-of-thought reasoning, which generates high-quality, user-specific
creative titles and ensures factual consistency despite limited personalized
data. This pipeline serves as a critical foundation for the effectiveness of
our model. Extensive experiments on personalized title generation for Douyin
Search Ads show the effectiveness of HLLM-Creator. Online A/B test shows a
0.476% increase on Adss, paving the way for more effective and efficient
personalized generation in industrial scenarios. Codes for academic dataset are
available at https://github.com/bytedance/HLLM.

</details>


### [153] [Test-Time Scaling Strategies for Generative Retrieval in Multimodal Conversational Recommendations](https://arxiv.org/abs/2508.18132)
*Hung-Chun Hsu,Yuan-Ching Kuo,Chao-Han Huck Yang,Szu-Wei Fu,Hanrong Ye,Hongxu Yin,Yu-Chiang Frank Wang,Ming-Feng Tsai,Chuan-Ju Wang*

Main category: cs.IR

TL;DR: This paper introduces a test-time scaling framework for conversational multimodal product retrieval, which improves retrieval accuracy and aligns with evolving user intent. It outperforms existing methods in multi-turn scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional product retrieval systems struggle with complex, multi-turn user interactions. Existing multimodal generative retrieval methods are tailored to single-turn scenarios and struggle to model the evolving intent and iterative nature of multi-turn dialogues. Test-time scaling's effectiveness relies on conditions rarely met in conversational product search.

Method: A novel framework that introduces test-time scaling into conversational multimodal product retrieval, building on a generative retriever, further augmented with a test-time reranking (TTR) mechanism.

Result: Experiments across multiple benchmarks show consistent improvements, with average gains of 14.5 points in MRR and 10.6 points in nDCG@1.

Conclusion: The proposed framework introduces test-time scaling into conversational multimodal product retrieval, using a generative retriever augmented with a test-time reranking (TTR) mechanism. Experiments show consistent improvements in retrieval accuracy and alignment with evolving user intent.

Abstract: The rapid evolution of e-commerce has exposed the limitations of traditional
product retrieval systems in managing complex, multi-turn user interactions.
Recent advances in multimodal generative retrieval -- particularly those
leveraging multimodal large language models (MLLMs) as retrievers -- have shown
promise. However, most existing methods are tailored to single-turn scenarios
and struggle to model the evolving intent and iterative nature of multi-turn
dialogues when applied naively. Concurrently, test-time scaling has emerged as
a powerful paradigm for improving large language model (LLM) performance
through iterative inference-time refinement. Yet, its effectiveness typically
relies on two conditions: (1) a well-defined problem space (e.g., mathematical
reasoning), and (2) the model's ability to self-correct -- conditions that are
rarely met in conversational product search. In this setting, user queries are
often ambiguous and evolving, and MLLMs alone have difficulty grounding
responses in a fixed product corpus. Motivated by these challenges, we propose
a novel framework that introduces test-time scaling into conversational
multimodal product retrieval. Our approach builds on a generative retriever,
further augmented with a test-time reranking (TTR) mechanism that improves
retrieval accuracy and better aligns results with evolving user intent
throughout the dialogue. Experiments across multiple benchmarks show consistent
improvements, with average gains of 14.5 points in MRR and 10.6 points in
nDCG@1.

</details>


### [154] [PCR-CA: Parallel Codebook Representations with Contrastive Alignment for Multiple-Category App Recommendation](https://arxiv.org/abs/2508.18166)
*Bin Tan,Wangyao Ge,Yidi Wang,Xin Liu,Jeff Burtoft,Hao Fan,Hui Wang*

Main category: cs.IR

TL;DR: PCR-CA通过并行码本表示和对比对齐来改进多类别应用程序的推荐，并在Microsoft Store上成功部署。


<details>
  <summary>Details</summary>
Motivation: 传统的分类法无法捕捉重叠的语义，导致个性化不佳。我们提出了PCR-CA，一个端到端的框架，用于改进CTR预测。

Method: PCR-CA：具有对比对齐的并行码本表示

Result: PCR-CA在大型数据集上实现了+0.76%的AUC改进，长尾应用程序的AUC增益为+2.15%。在线A/B测试进一步验证了我们的方法，CTR提高了+10.52%，CVR提高了+16.30%。

Conclusion: PCR-CA在真实部署中表现出有效性，并在Microsoft Store上全面部署。

Abstract: Modern app store recommender systems struggle with multiple-category apps, as
traditional taxonomies fail to capture overlapping semantics, leading to
suboptimal personalization. We propose PCR-CA (Parallel Codebook
Representations with Contrastive Alignment), an end-to-end framework for
improved CTR prediction. PCR-CA first extracts compact multimodal embeddings
from app text, then introduces a Parallel Codebook VQ-AE module that learns
discrete semantic representations across multiple codebooks in parallel --
unlike hierarchical residual quantization (RQ-VAE). This design enables
independent encoding of diverse aspects (e.g., gameplay, art style), better
modeling multiple-category semantics. To bridge semantic and collaborative
signals, we employ a contrastive alignment loss at both the user and item
levels, enhancing representation learning for long-tail items. Additionally, a
dual-attention fusion mechanism combines ID-based and semantic features to
capture user interests, especially for long-tail apps. Experiments on a
large-scale dataset show PCR-CA achieves a +0.76% AUC improvement over strong
baselines, with +2.15% AUC gains for long-tail apps. Online A/B testing further
validates our approach, showing a +10.52% lift in CTR and a +16.30% improvement
in CVR, demonstrating PCR-CA's effectiveness in real-world deployment. The new
framework has now been fully deployed on the Microsoft Store.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [155] [Quantum-Inspired DRL Approach with LSTM and OU Noise for Cut Order Planning Optimization](https://arxiv.org/abs/2508.16611)
*Yulison Herry Chrisnanto,Julian Evan Chrisnanto*

Main category: cs.LG

TL;DR: 提出了一种量子启发深度强化学习框架，用于优化纺织行业的裁剪顺序规划，实验表明该方法能够节约成本并提高效率。


<details>
  <summary>Details</summary>
Motivation: 传统的基于静态启发式和基于目录的估计的方法通常难以适应动态生产环境，导致次优的解决方案和增加的浪费。

Method: 提出了一种新颖的量子启发深度强化学习 (QI-DRL) 框架，该框架集成了长短期记忆 (LSTM) 网络与 Ornstein-Uhlenbeck 噪声。

Result: 在 1000 个 episode 的广泛训练中表现出稳健的性能，平均奖励为 0.81 (-+0.03)，预测损失稳定下降至 0.15 (-+0.02)。与传统方法相比，该方法实现了高达 13% 的面料成本节约。

Conclusion: 该方法通过优化裁剪顺序规划，能够节约高达13%的织物成本，并具有良好的稳定性和收敛性， перспективные результаты подчеркивают потенциал масштабируемой и адаптивной платформы для повышения эффективности производства и прокладывают путь для будущих инноваций в оптимизации COP.

Abstract: Cut order planning (COP) is a critical challenge in the textile industry,
directly impacting fabric utilization and production costs. Conventional
methods based on static heuristics and catalog-based estimations often struggle
to adapt to dynamic production environments, resulting in suboptimal solutions
and increased waste. In response, we propose a novel Quantum-Inspired Deep
Reinforcement Learning (QI-DRL) framework that integrates Long Short-Term
Memory (LSTM) networks with Ornstein-Uhlenbeck noise. This hybrid approach is
designed to explicitly address key research questions regarding the benefits of
quantum-inspired probabilistic representations, the role of LSTM-based memory
in capturing sequential dependencies, and the effectiveness of OU noise in
facilitating smooth exploration and faster convergence. Extensive training over
1000 episodes demonstrates robust performance, with an average reward of 0.81
(-+0.03) and a steady decrease in prediction loss to 0.15 (-+0.02). A
comparative analysis reveals that the proposed approach achieves fabric cost
savings of up to 13% compared to conventional methods. Furthermore, statistical
evaluations indicate low variability and stable convergence. Despite the fact
that the simulation model makes several simplifying assumptions, these
promising results underscore the potential of the scalable and adaptive
framework to enhance manufacturing efficiency and pave the way for future
innovations in COP optimization.

</details>


### [156] [CrystalDiT: A Diffusion Transformer for Crystal Generation](https://arxiv.org/abs/2508.16614)
*Xiaohan Yi,Guikun Xu,Xi Xiao,Zhong Zhang,Liu Liu,Yatao Bian,Peilin Zhao*

Main category: cs.LG

TL;DR: CrystalDiT, a simple diffusion transformer, outperforms complex models in crystal structure generation by treating lattice and atomic properties as a single system.


<details>
  <summary>Details</summary>
Motivation: The paper challenges the trend of architectural complexity in crystal structure generation.

Method: CrystalDiT employs a unified transformer that treats lattice and atomic properties as a single, interdependent system. It also uses a periodic table-based atomic representation and a balanced training strategy.

Result: CrystalDiT achieves 9.62% SUN rate on MP-20, outperforming FlowMM (4.38%) and MatterGen (3.42%). It generates 63.28% unique and novel structures while maintaining comparable stability rates.

Conclusion: CrystalDiT demonstrates that architectural simplicity can be more effective than complexity for materials discovery in data-limited scientific domains.

Abstract: We present CrystalDiT, a diffusion transformer for crystal structure
generation that achieves state-of-the-art performance by challenging the trend
of architectural complexity. Instead of intricate, multi-stream designs,
CrystalDiT employs a unified transformer that imposes a powerful inductive
bias: treating lattice and atomic properties as a single, interdependent
system. Combined with a periodic table-based atomic representation and a
balanced training strategy, our approach achieves 9.62% SUN (Stable, Unique,
Novel) rate on MP-20, substantially outperforming recent methods including
FlowMM (4.38%) and MatterGen (3.42%). Notably, CrystalDiT generates 63.28%
unique and novel structures while maintaining comparable stability rates,
demonstrating that architectural simplicity can be more effective than
complexity for materials discovery. Our results suggest that in data-limited
scientific domains, carefully designed simple architectures outperform
sophisticated alternatives that are prone to overfitting.

</details>


### [157] [Leveraging the Christoffel Function for Outlier Detection in Data Streams](https://arxiv.org/abs/2508.16617)
*Kévin Ducharlet,Louise Travé-Massuyès,Jean-Bernard Lasserre,Marie-Véronique Le Lann,Youssef Miloudi*

Main category: cs.LG

TL;DR: 本文介绍了两种新颖的异常值检测方法 DyCF 和 DyCG，它们基于 Christoffel 函数，适用于处理数据流，具有低维特性和无内存成本的数据历史维护。


<details>
  <summary>Details</summary>
Motivation: 在数据挖掘领域中，异常值检测非常重要，尤其是在数据采集方法日益普及的情况下。识别数据流中的异常值对于维持数据质量和检测故障至关重要。然而，由于分布的非平稳性质和不断增加的数据量，处理数据流提出了挑战。许多方法缺乏直接的参数化。

Method: DyCF利用近似和正交多项式理论中的 Christoffel 函数。DyCG 利用 Christoffel 函数的增长特性，无需调整参数。

Result: 使用合成和真实工业数据流，对 DyCF、DyCG 和最先进的方法进行了全面比较。结果表明，DyCF 优于微调方法，在执行时间和内存使用方面表现出卓越的性能。DyCG 表现稍逊，但具有无需调整的显着优势。

Conclusion: DyCF优于微调方法，在执行时间和内存使用方面表现出卓越的性能。DyCG 表现稍逊，但具有无需调整的显着优势。

Abstract: Outlier detection holds significant importance in the realm of data mining,
particularly with the growing pervasiveness of data acquisition methods. The
ability to identify outliers in data streams is essential for maintaining data
quality and detecting faults. However, dealing with data streams presents
challenges due to the non-stationary nature of distributions and the
ever-increasing data volume. While numerous methods have been proposed to
tackle this challenge, a common drawback is the lack of straightforward
parameterization in many of them. This article introduces two novel methods:
DyCF and DyCG. DyCF leverages the Christoffel function from the theory of
approximation and orthogonal polynomials. Conversely, DyCG capitalizes on the
growth properties of the Christoffel function, eliminating the need for tuning
parameters. Both approaches are firmly rooted in a well-defined algebraic
framework, meeting crucial demands for data stream processing, with a specific
focus on addressing low-dimensional aspects and maintaining data history
without memory cost. A comprehensive comparison between DyCF, DyCG, and
state-of-the-art methods is presented, using both synthetic and real industrial
data streams. The results show that DyCF outperforms fine-tuning methods,
offering superior performance in terms of execution time and memory usage. DyCG
performs less well, but has the considerable advantage of requiring no tuning
at all.

</details>


### [158] [STRelay: A Universal Spatio-Temporal Relaying Framework for Location Prediction with Future Spatiotemporal Contexts](https://arxiv.org/abs/2508.16620)
*Bangchao Deng,Lianhua Ji,Chunhua Chen,Xin Jing,Ling Ding,Bingqing QU,Pengyang Wang,Dingqi Yang*

Main category: cs.LG

TL;DR: STRelay, a universal SpatioTemporal Relaying framework explicitly modeling the future spatiotemporal context given a human trajectory, to boost the performance of different location prediction models.


<details>
  <summary>Details</summary>
Motivation: Existing methods often overlook the importance of the future spatiotemporal contexts, which are highly informative for the future locations.

Method: STRelay models future spatiotemporal contexts in a relaying manner, which is subsequently integrated with the encoded historical representation from a base location prediction model, enabling multi-task learning by simultaneously predicting the next time interval, next moving distance interval, and finally the next location.

Result: STRelay consistently improves prediction performance across all cases by 3.19\%-11.56\%.

Conclusion: STRelay consistently improves prediction performance across all cases by 3.19\%-11.56\%. Additionally, we find that the future spatiotemporal contexts are particularly helpful for entertainment-related locations and also for user groups who prefer traveling longer distances.

Abstract: Next location prediction is a critical task in human mobility modeling,
enabling applications like travel planning and urban mobility management.
Existing methods mainly rely on historical spatiotemporal trajectory data to
train sequence models that directly forecast future locations. However, they
often overlook the importance of the future spatiotemporal contexts, which are
highly informative for the future locations. For example, knowing how much time
and distance a user will travel could serve as a critical clue for predicting
the user's next location. Against this background, we propose \textbf{STRelay},
a universal \textbf{\underline{S}}patio\textbf{\underline{T}}emporal
\textbf{\underline{Relay}}ing framework explicitly modeling the future
spatiotemporal context given a human trajectory, to boost the performance of
different location prediction models. Specifically, STRelay models future
spatiotemporal contexts in a relaying manner, which is subsequently integrated
with the encoded historical representation from a base location prediction
model, enabling multi-task learning by simultaneously predicting the next time
interval, next moving distance interval, and finally the next location. We
evaluate STRelay integrated with four state-of-the-art location prediction base
models on four real-world trajectory datasets. Results demonstrate that STRelay
consistently improves prediction performance across all cases by
3.19\%-11.56\%. Additionally, we find that the future spatiotemporal contexts
are particularly helpful for entertainment-related locations and also for user
groups who prefer traveling longer distances. The performance gain on such
non-daily-routine activities, which often suffer from higher uncertainty, is
indeed complementary to the base location prediction models that often excel at
modeling regular daily routine patterns.

</details>


### [159] [A Retrieval Augmented Spatio-Temporal Framework for Traffic Prediction](https://arxiv.org/abs/2508.16623)
*Weilin Ruan,Xilin Dang,Ziyu Zhou,Sisuo Lyu,Yuxuan Liang*

Main category: cs.LG

TL;DR: RAST, a retrieval-augmented framework, achieves better performance in traffic prediction.


<details>
  <summary>Details</summary>
Motivation: limited contextual capacity when modeling complex spatio-temporal dependencies, and (ii) low predictability at fine-grained spatio-temporal points due to heterogeneous patterns.

Method: RAST, a universal framework that integrates retrieval-augmented mechanisms with spatio-temporal modeling

Result: RAST achieves superior performance while maintaining computational efficiency on six real-world traffic networks.

Conclusion: RAST achieves superior performance while maintaining computational efficiency.

Abstract: Traffic prediction is a cornerstone of modern intelligent transportation
systems and a critical task in spatio-temporal forecasting. Although advanced
Spatio-temporal Graph Neural Networks (STGNNs) and pre-trained models have
achieved significant progress in traffic prediction, two key challenges remain:
(i) limited contextual capacity when modeling complex spatio-temporal
dependencies, and (ii) low predictability at fine-grained spatio-temporal
points due to heterogeneous patterns. Inspired by Retrieval-Augmented
Generation (RAG), we propose RAST, a universal framework that integrates
retrieval-augmented mechanisms with spatio-temporal modeling to address these
challenges. Our framework consists of three key designs: 1) Decoupled Encoder
and Query Generator to capture decoupled spatial and temporal features and
construct a fusion query via residual fusion; 2) Spatio-temporal Retrieval
Store and Retrievers to maintain and retrieve vectorized fine-grained patterns;
and 3) Universal Backbone Predictor that flexibly accommodates pre-trained
STGNNs or simple MLP predictors. Extensive experiments on six real-world
traffic networks, including large-scale datasets, demonstrate that RAST
achieves superior performance while maintaining computational efficiency.

</details>


### [160] [Learn to Memorize: Optimizing LLM-based Agents with Adaptive Memory Framework](https://arxiv.org/abs/2508.16629)
*Zeyu Zhang,Quanyu Dai,Rui Li,Xiaohe Bo,Xu Chen,Zhenhua Dong*

Main category: cs.LG

TL;DR: This paper proposes an adaptive, data-driven memory framework for LLM-based agents that learns to memorize information effectively by modeling memory cycles, outperforming manually predefined methods.


<details>
  <summary>Details</summary>
Motivation: Previous memory mechanisms of LLM-based agents are manually predefined, leading to higher labor costs and suboptimal performance. These methods also overlook the memory cycle effect in interactive scenarios.

Method: The method involves designing an MoE gate function for memory retrieval, a learnable aggregation process for improved memory utilization, and task-specific reflection to adapt memory storage. The framework models memory cycles.

Result: The effectiveness of the proposed methods is evaluated through comprehensive experiments across multiple aspects.

Conclusion: This paper introduces a novel memory framework for LLM-based agents that enables them to learn how to memorize information effectively in specific environments, with both off-policy and on-policy optimization.

Abstract: LLM-based agents have been extensively applied across various domains, where
memory stands out as one of their most essential capabilities. Previous memory
mechanisms of LLM-based agents are manually predefined by human experts,
leading to higher labor costs and suboptimal performance. In addition, these
methods overlook the memory cycle effect in interactive scenarios, which is
critical to optimizing LLM-based agents for specific environments. To address
these challenges, in this paper, we propose to optimize LLM-based agents with
an adaptive and data-driven memory framework by modeling memory cycles.
Specifically, we design an MoE gate function to facilitate memory retrieval,
propose a learnable aggregation process to improve memory utilization, and
develop task-specific reflection to adapt memory storage. Our memory framework
empowers LLM-based agents to learn how to memorize information effectively in
specific environments, with both off-policy and on-policy optimization. In
order to evaluate the effectiveness of our proposed methods, we conduct
comprehensive experiments across multiple aspects. To benefit the research
community in this area, we release our project at
https://github.com/nuster1128/learn_to_memorize.

</details>


### [161] [Recurrent Transformer U-Net Surrogate for Flow Modeling and Data Assimilation in Subsurface Formations with Faults](https://arxiv.org/abs/2508.16631)
*Yifu Han,Louis J. Durlofsky*

Main category: cs.LG

TL;DR: 开发了一种新的循环Transformer U-Net代理模型，用于快速预测断层地下蓄水层系统中的压力和CO2饱和度，并通过数据同化减少不确定性。


<details>
  <summary>Details</summary>
Motivation: 许多地下构造，包括一些正在考虑用于大规模地质碳储存的构造，都包含可能强烈影响流体流动的广泛断层。

Method: 开发了一种新的循环Transformer U-Net代理模型，用于快速预测真实断层地下蓄水层系统中的压力和CO2饱和度。

Result: 该模型比以前的循环残差U-Net更准确，并且在质量上不同的泄漏情况下保持准确性。

Conclusion: 该模型可以用于全局敏感性分析和数据同化，并通过分层马尔可夫链蒙特卡罗数据同化程序，结合不同的监测策略，减少不确定性，并测量压力和饱和度。

Abstract: Many subsurface formations, including some of those under consideration for
large-scale geological carbon storage, include extensive faults that can
strongly impact fluid flow. In this study, we develop a new recurrent
transformer U-Net surrogate model to provide very fast predictions for pressure
and CO2 saturation in realistic faulted subsurface aquifer systems. The
geomodel includes a target aquifer (into which supercritical CO2 is injected),
surrounding regions, caprock, two extensive faults, and two overlying aquifers.
The faults can act as leakage pathways between the three aquifers. The
heterogeneous property fields in the target aquifer are characterized by
hierarchical uncertainty, meaning both the geological metaparameters (e.g.,
mean and standard deviation of log-permeability) and the detailed cell
properties of each realization, are uncertain. Fault permeabilities are also
treated as uncertain. The model is trained with simulation results for (up to)
4000 randomly sampled realizations. Error assessments show that this model is
more accurate than a previous recurrent residual U-Net, and that it maintains
accuracy for qualitatively different leakage scenarios. The new surrogate is
then used for global sensitivity analysis and data assimilation. A hierarchical
Markov chain Monte Carlo data assimilation procedure is applied. Different
monitoring strategies, corresponding to different amounts and types of observed
data collected at monitoring wells, are considered for three synthetic true
models. Detailed results demonstrate the degree of uncertainty reduction
achieved with the various monitoring strategies. Posterior results for 3D
saturation plumes and leakage volumes indicate the benefits of measuring
pressure and saturation in all three aquifers.

</details>


### [162] [Adaptive Variance-Penalized Continual Learning with Fisher Regularization](https://arxiv.org/abs/2508.16632)
*Krisanu Sarkar*

Main category: cs.LG

TL;DR: A new continual learning method using Fisher-weighted regularization significantly reduces catastrophic forgetting and improves performance on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: The persistent challenge of catastrophic forgetting in neural networks has motivated extensive research in continual learning.

Method: A novel continual learning framework that integrates Fisher-weighted asymmetric regularization of parameter variances within a variational learning paradigm.

Result: Substantial improvements over existing approaches such as Variational Continual Learning and Elastic Weight Consolidation on SplitMNIST, PermutedMNIST, and SplitFashionMNIST. The asymmetric variance penalty mechanism proves particularly effective.

Conclusion: The proposed method effectively addresses catastrophic forgetting, boosting task performance and mitigating knowledge degradation.

Abstract: The persistent challenge of catastrophic forgetting in neural networks has
motivated extensive research in continual learning . This work presents a novel
continual learning framework that integrates Fisher-weighted asymmetric
regularization of parameter variances within a variational learning paradigm.
Our method dynamically modulates regularization intensity according to
parameter uncertainty, achieving enhanced stability and performance.
Comprehensive evaluations on standard continual learning benchmarks including
SplitMNIST, PermutedMNIST, and SplitFashionMNIST demonstrate substantial
improvements over existing approaches such as Variational Continual Learning
and Elastic Weight Consolidation . The asymmetric variance penalty mechanism
proves particularly effective in maintaining knowledge across sequential tasks
while improving model accuracy. Experimental results show our approach not only
boosts immediate task performance but also significantly mitigates knowledge
degradation over time, effectively addressing the fundamental challenge of
catastrophic forgetting in neural networks

</details>


### [163] [A Novel Unified Extended Matrix for Graph Signal Processing: Theory and Application](https://arxiv.org/abs/2508.16633)
*Yunyan Zheng,Zhichao Zhang,Wei Yao*

Main category: cs.LG

TL;DR: 本文提出了UEM框架和UEM-GFT，以提高图信号处理在复杂图结构上的性能，并在异常检测任务中取得了更好的效果。


<details>
  <summary>Details</summary>
Motivation: 传统的图移位算子(GSO)对于某些任务是有效的，但它们在建模非相邻节点之间的依赖关系方面天生缺乏灵活性，限制了它们表示复杂图结构的能力。

Method: 提出了统一扩展矩阵(UEM)框架，该框架通过参数化设计集成了扩展邻接矩阵和统一图表示矩阵。

Result: 对UEM进行了理论分析，证明了在特定条件下的正半定性和特征值单调性。然后，我们提出了基于UEM的图傅里叶变换(UEM-GFT)，它可以自适应地调整频谱特性，以提高信号处理性能。

Conclusion: UEM-GFT在异常检测任务中优于现有的基于GSO的方法，在不同的网络拓扑中实现了卓越的性能。

Abstract: Graph signal processing has become an essential tool for analyzing data
structured on irregular domains. While conventional graph shift operators
(GSOs) are effective for certain tasks, they inherently lack flexibility in
modeling dependencies between non-adjacent nodes, limiting their ability to
represent complex graph structures. To address this limitation, this paper
proposes the unified extended matrix (UEM) framework, which integrates the
extended-adjacency matrix and the unified graph representation matrix through
parametric design, so as to be able to flexibly adapt to different graph
structures and reveal more graph signal information. Theoretical analysis of
the UEM is conducted, demonstrating positive semi-definiteness and eigenvalue
monotonicity under specific conditions. Then, we propose graph Fourier
transform based on UEM (UEM-GFT), which can adaptively tune spectral properties
to enhance signal processing performance. Experimental results on synthetic and
real-world datasets demonstrate that the UEM-GFT outperforms existing GSO-based
methods in anomaly detection tasks, achieving superior performance across
varying network topologies.

</details>


### [164] [Few-shot Class-incremental Fault Diagnosis by Preserving Class-Agnostic Knowledge with Dual-Granularity Representations](https://arxiv.org/abs/2508.16634)
*Zhendong Yang,Jie Wang,Liansong Zong,Xiaorong Liu,Quan Qian,Shiqian Chen*

Main category: cs.LG

TL;DR: 提出了一种新的双粒度指导网络(DGGN)框架，用于解决少样本类增量故障诊断(FSC-FD)中的灾难性遗忘和过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 面向实际工业系统的少样本类增量故障诊断(FSC-FD)至关重要，它旨在不断地从只有少量样本的新故障类中学习，而不忘记旧的故障类。然而，这项具有挑战性的任务严重地加剧了旧知识的灾难性遗忘和稀缺新数据的过拟合问题。

Method: 提出了一种基于双粒度表示的新框架，称为双粒度指导网络(DGGN)。DGGN将特征学习解耦为两个并行流：1) 细粒度表示流，利用一种新的多阶交互聚合模块从有限的新样本中捕获有区别的、特定于类的特征。2) 粗粒度表示流，旨在建模和保存所有故障类型之间共享的通用、类不可知的知识。这两种表示通过一种多语义交叉注意机制动态融合，其中稳定的粗粒度知识指导细粒度特征的学习，防止过拟合，缓解特征冲突。此外，设计了一种边界感知范例优先级策略，以进一步减轻灾难性遗忘。此外，采用解耦的平衡随机森林分类器来对抗由数据不平衡引起的决策边界偏差。

Result: DGGN在TEP基准测试和真实世界的MFF数据集上的大量实验表明，与最先进的FSC-FD方法相比，该方法具有优越的诊断性能和稳定性。

Conclusion: 提出的DGGN在TEP基准测试和真实世界的MFF数据集上实现了优于最先进的FSC-FD方法的诊断性能和稳定性。

Abstract: Few-Shot Class-Incremental Fault Diagnosis (FSC-FD), which aims to
continuously learn from new fault classes with only a few samples without
forgetting old ones, is critical for real-world industrial systems. However,
this challenging task severely amplifies the issues of catastrophic forgetting
of old knowledge and overfitting on scarce new data. To address these
challenges, this paper proposes a novel framework built upon Dual-Granularity
Representations, termed the Dual-Granularity Guidance Network (DGGN). Our DGGN
explicitly decouples feature learning into two parallel streams: 1) a
fine-grained representation stream, which utilizes a novel Multi-Order
Interaction Aggregation module to capture discriminative, class-specific
features from the limited new samples. 2) a coarse-grained representation
stream, designed to model and preserve general, class-agnostic knowledge shared
across all fault types. These two representations are dynamically fused by a
multi-semantic cross-attention mechanism, where the stable coarse-grained
knowledge guides the learning of fine-grained features, preventing overfitting
and alleviating feature conflicts. To further mitigate catastrophic forgetting,
we design a Boundary-Aware Exemplar Prioritization strategy. Moreover, a
decoupled Balanced Random Forest classifier is employed to counter the decision
boundary bias caused by data imbalance. Extensive experiments on the TEP
benchmark and a real-world MFF dataset demonstrate that our proposed DGGN
achieves superior diagnostic performance and stability compared to
state-of-the-art FSC-FD approaches. Our code is publicly available at
https://github.com/MentaY/DGGN

</details>


### [165] [Enhancing Transformer-Based Foundation Models for Time Series Forecasting via Bagging, Boosting and Statistical Ensembles](https://arxiv.org/abs/2508.16641)
*Dhruv D. Modi,Rong Pan*

Main category: cs.LG

TL;DR: This paper investigates statistical and ensemble-based enhancement techniques to improve the robustness and accuracy of time series foundation models. The results indicate that integrating statistical reasoning with modern foundation models yields measurable gains in accuracy, reliability, and interpretability for real-world time series applications.


<details>
  <summary>Details</summary>
Motivation: Time series foundation models (TSFMs) predictions still suffer from variance, domain-specific bias, and limited uncertainty quantification when deployed on real operational data.

Method: a suite of statistical and ensemble-based enhancement techniques, including bootstrap-based bagging, regression-based stacking, prediction interval construction, statistical residual modeling, and iterative error feedback

Result: Regression-based ensembles achieve the lowest mean squared error; bootstrap aggregation markedly reduces long-context errors; residual modeling corrects systematic bias; and the resulting prediction intervals achieve near nominal coverage with widths shrinking as context length increases.

Conclusion: Integrating statistical reasoning with modern foundation models yields measurable gains in accuracy, reliability, and interpretability for real-world time series applications.

Abstract: Time series foundation models (TSFMs) such as Lag-Llama, TimeGPT, Chronos,
MOMENT, UniTS, and TimesFM have shown strong generalization and zero-shot
capabilities for time series forecasting, anomaly detection, classification,
and imputation. Despite these advantages, their predictions still suffer from
variance, domain-specific bias, and limited uncertainty quantification when
deployed on real operational data. This paper investigates a suite of
statistical and ensemble-based enhancement techniques, including
bootstrap-based bagging, regression-based stacking, prediction interval
construction, statistical residual modeling, and iterative error feedback, to
improve robustness and accuracy. Using the Belgium Electricity Short-Term Load
Forecasting dataset as a case study, we demonstrate that the proposed hybrids
consistently outperform standalone foundation models across multiple horizons.
Regression-based ensembles achieve the lowest mean squared error; bootstrap
aggregation markedly reduces long-context errors; residual modeling corrects
systematic bias; and the resulting prediction intervals achieve near nominal
coverage with widths shrinking as context length increases. The results
indicate that integrating statistical reasoning with modern foundation models
yields measurable gains in accuracy, reliability, and interpretability for
real-world time series applications.

</details>


### [166] [From Classical Probabilistic Latent Variable Models to Modern Generative AI: A Unified Perspective](https://arxiv.org/abs/2508.16643)
*Tianhua Chen*

Main category: cs.LG

TL;DR: 本文在 PLVM 范式下统一了经典和现代生成方法，揭示了共享原则并指导未来创新。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型到多模态代理，生成人工智能 (AI) 现在是state-of-the-art系统的基础。尽管它们的架构各不相同，但许多都在概率潜在变量模型 (PLVM) 中拥有共同的基础，其中隐藏变量解释了观察到的数据，用于密度估计、潜在推理和结构化推理。

Method: 将经典和现代生成方法置于 PLVM 范式中，追溯从经典扁平模型到当代深度架构的演变。

Result: 从概率 PCA、高斯混合模型、潜在类别分析、项目反应理论和潜在狄利克雷分配等经典扁平模型，到包括隐马尔可夫模型、高斯 HMM 和线性动态系统在内的顺序扩展，再到当代深度架构：变分自动编码器作为深度 PLVM、标准化流作为可处理的 PLVM、扩散模型作为顺序 PLVM、自回归模型作为显式生成模型，以及生成对抗网络作为隐式 PLVM。

Conclusion: 通过在一个共同的概率分类法下查看这些架构，揭示了共享原则、不同的推理策略以及塑造其优势的表征权衡。我们提供了一个概念路线图，通过将新兴架构植根于其概率遗产中，巩固了生成人工智能的理论基础，阐明了方法论的沿袭，并指导未来的创新。

Abstract: From large language models to multi-modal agents, Generative Artificial
Intelligence (AI) now underpins state-of-the-art systems. Despite their varied
architectures, many share a common foundation in probabilistic latent variable
models (PLVMs), where hidden variables explain observed data for density
estimation, latent reasoning, and structured inference. This paper presents a
unified perspective by framing both classical and modern generative methods
within the PLVM paradigm. We trace the progression from classical flat models
such as probabilistic PCA, Gaussian mixture models, latent class analysis, item
response theory, and latent Dirichlet allocation, through their sequential
extensions including Hidden Markov Models, Gaussian HMMs, and Linear Dynamical
Systems, to contemporary deep architectures: Variational Autoencoders as Deep
PLVMs, Normalizing Flows as Tractable PLVMs, Diffusion Models as Sequential
PLVMs, Autoregressive Models as Explicit Generative Models, and Generative
Adversarial Networks as Implicit PLVMs. Viewing these architectures under a
common probabilistic taxonomy reveals shared principles, distinct inference
strategies, and the representational trade-offs that shape their strengths. We
offer a conceptual roadmap that consolidates generative AI's theoretical
foundations, clarifies methodological lineages, and guides future innovation by
grounding emerging architectures in their probabilistic heritage.

</details>


### [167] [AdapSNE: Adaptive Fireworks-Optimized and Entropy-Guided Dataset Sampling for Edge DNN Training](https://arxiv.org/abs/2508.16647)
*Boran Zhao,Hetian Liu,Zihang Yuan,Li Zhu,Fan Yang,Lina Xie Tian Xia,Wenzhe Zhao,Pengju Ren*

Main category: cs.LG

TL;DR: 本文提出了一种名为AdapSNE的DNN-free方法，用于在边缘设备上进行深度神经网络（DNNs）训练，以解决现有NMS方法的局限性，提高训练精度并降低边缘侧成本。


<details>
  <summary>Details</summary>
Motivation: 传统的DNN训练通常需要大规模数据集，这给边缘设备带来了过高的开销，特别是对于新兴的大型语言模型（LLM）任务。现有的NMS存在两个局限性：（1）搜索方法与困惑度误差函数的非单调性之间的不匹配导致降维表示中出现异常值；（2）关键参数（即目标困惑度）是凭经验选择的，引入了任意性，导致采样不均匀。这两个问题导致了样本的代表性偏差，导致准确性下降。

Method: 我们提出了AdapSNE，它集成了烟花算法（FWA）和熵引导优化。

Result: AdapSNE通过抑制异常值和强制均匀采样，确保了代表性的训练样本，从而提高了训练精度。我们设计的加速器显着降低了设备上的训练能量和面积。

Conclusion: 为了解决这些问题，我们提出了AdapSNE，它集成了高效的非单调搜索方法——即烟花算法（FWA）——来抑制异常值，并采用熵引导优化来强制均匀采样，从而确保具有代表性的训练样本，从而提高训练精度。为了降低FWA搜索和熵引导优化的迭代计算带来的边缘侧成本，我们设计了一个具有自定义数据流和时间复用的加速器，从而显着降低了设备上的训练能量和面积。

Abstract: Training deep neural networks (DNNs) directly on edge devices has attracted
increasing attention, as it offers promising solutions to challenges such as
domain adaptation and privacy preservation. However, conventional DNN training
typically requires large-scale datasets, which imposes prohibitive overhead on
edge devices-particularly for emerging large language model (LLM) tasks. To
address this challenge, a DNN-free method (ie., dataset sampling without DNN),
named NMS (Near-Memory Sampling), has been introduced. By first conducting
dimensionality reduction of the dataset and then performing exemplar sampling
in the reduced space, NMS avoids the architectural bias inherent in DNN-based
methods and thus achieves better generalization. However, The state-of-the-art,
NMS, suffers from two limitations: (1) The mismatch between the search method
and the non-monotonic property of the perplexity error function leads to the
emergence of outliers in the reduced representation; (2) Key parameter (ie.,
target perplexity) is selected empirically, introducing arbitrariness and
leading to uneven sampling. These two issues lead to representative bias of
examplars, resulting in degraded accuracy. To address these issues, we propose
AdapSNE, which integrates an efficient non-monotonic search method-namely, the
Fireworks Algorithm (FWA)-to suppress outliers, and employs entropy-guided
optimization to enforce uniform sampling, thereby ensuring representative
training samples and consequently boosting training accuracy. To cut the
edge-side cost arising from the iterative computations of FWA search and
entropy-guided optimization, we design an accelerator with custom dataflow and
time-multiplexing markedly reducing on-device training energy and area.

</details>


### [168] [Effective Clustering for Large Multi-Relational Graphs](https://arxiv.org/abs/2508.17388)
*Xiaoyang Lin,Runhao Jiang,Renchi Yang*

Main category: cs.LG

TL;DR: DEMM 和 DEMM+ 是两种有效的 MRGC 方法，旨在解决上述限制。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案要么因异构图结构和属性的无效融合而导致结果质量严重受损，要么由于采用复杂且成本高昂的深度学习模型而难以处理具有数百万个节点和数十亿条边的庞大 MRG。

Method: 该算法建立在新的两阶段优化目标之上，前者旨在通过优化专门用于 MRG 的多关系狄利克雷能量来导出高质量的节点特征向量，而后者最小化节点亲和力图上聚类结果的狄利克雷能量。

Result: DEMM+在针对 11 个真实 MRG 的 20 个基线的广泛实验中，在针对ground-truth标签衡量的聚类质量方面始终优越，同时通常速度明显更快。

Conclusion: DEMM+在聚类质量方面始终优于其他方法，并且通常速度明显更快。

Abstract: Multi-relational graphs (MRGs) are an expressive data structure for modeling
diverse interactions/relations among real objects (i.e., nodes), which pervade
extensive applications and scenarios. Given an MRG G with N nodes, partitioning
the node set therein into K disjoint clusters (MRGC) is a fundamental task in
analyzing MRGs, which has garnered considerable attention. However, the
majority of existing solutions towards MRGC either yield severely compromised
result quality by ineffective fusion of heterogeneous graph structures and
attributes, or struggle to cope with sizable MRGs with millions of nodes and
billions of edges due to the adoption of sophisticated and costly deep learning
models.
  In this paper, we present DEMM and DEMM+, two effective MRGC approaches to
address the limitations above. Specifically, our algorithms are built on novel
two-stage optimization objectives, where the former seeks to derive
high-caliber node feature vectors by optimizing the multi-relational Dirichlet
energy specialized for MRGs, while the latter minimizes the Dirichlet energy of
clustering results over the node affinity graph. In particular, DEMM+ achieves
significantly higher scalability and efficiency over our based method DEMM
through a suite of well-thought-out optimizations. Key technical contributions
include (i) a highly efficient approximation solver for constructing node
feature vectors, and (ii) a theoretically-grounded problem transformation with
carefully-crafted techniques that enable linear-time clustering without
explicitly materializing the NxN dense affinity matrix. Further, we extend
DEMM+ to handle attribute-less MRGs through non-trivial adaptations. Extensive
experiments, comparing DEMM+ against 20 baselines over 11 real MRGs, exhibit
that DEMM+ is consistently superior in terms of clustering quality measured
against ground-truth labels, while often being remarkably faster.

</details>


### [169] [LatentFlow: Cross-Frequency Experimental Flow Reconstruction from Sparse Pressure via Latent Mapping](https://arxiv.org/abs/2508.16648)
*Junle Liu,Chang Liu,Yanyu Ke,Qiuxiang Huang,Jiachen Zhao,Wenliang Chen,K. T. Tse,Gang Hu*

Main category: cs.LG

TL;DR: LatentFlow reconstructs high-frequency turbulent wake flow fields from low-frequency flow field and pressure data by training a pressure-conditioned VAE and using high-frequency wall pressure during inference.


<details>
  <summary>Details</summary>
Motivation: Acquiring temporally high-frequency and spatially high-resolution turbulent wake flow fields in particle image velocimetry (PIV) experiments remains a significant challenge due to hardware limitations and measurement noise. In contrast, temporal high-frequency measurements of spatially sparse wall pressure are more readily accessible in wind tunnel experiments.

Method: a novel cross-modal temporal upscaling framework, LatentFlow, which reconstructs high-frequency (512 Hz) turbulent wake flow fields by fusing synchronized low-frequency (15 Hz) flow field and pressure data during training, and high-frequency wall pressure signals during inference. The first stage involves training a pressure-conditioned $\beta$-variation autoencoder ($p$C-$\beta$-VAE) to learn a compact latent representation that captures the intrinsic dynamics of the wake flow. A secondary network maps synchronized low-frequency wall pressure signals into the latent space, enabling reconstruction of the wake flow field solely from sparse wall pressure.

Result: reconstructs high-frequency (512 Hz) turbulent wake flow fields by fusing synchronized low-frequency (15 Hz) flow field and pressure data

Conclusion: LatentFlow provides a scalable and robust solution for reconstructing high-frequency turbulent wake flows in data-constrained experimental settings.

Abstract: Acquiring temporally high-frequency and spatially high-resolution turbulent
wake flow fields in particle image velocimetry (PIV) experiments remains a
significant challenge due to hardware limitations and measurement noise. In
contrast, temporal high-frequency measurements of spatially sparse wall
pressure are more readily accessible in wind tunnel experiments. In this study,
we propose a novel cross-modal temporal upscaling framework, LatentFlow, which
reconstructs high-frequency (512 Hz) turbulent wake flow fields by fusing
synchronized low-frequency (15 Hz) flow field and pressure data during
training, and high-frequency wall pressure signals during inference. The first
stage involves training a pressure-conditioned $\beta$-variation autoencoder
($p$C-$\beta$-VAE) to learn a compact latent representation that captures the
intrinsic dynamics of the wake flow. A secondary network maps synchronized
low-frequency wall pressure signals into the latent space, enabling
reconstruction of the wake flow field solely from sparse wall pressure. Once
trained, the model utilizes high-frequency, spatially sparse wall pressure
inputs to generate corresponding high-frequency flow fields via the
$p$C-$\beta$-VAE decoder. By decoupling the spatial encoding of flow dynamics
from temporal pressure measurements, LatentFlow provides a scalable and robust
solution for reconstructing high-frequency turbulent wake flows in
data-constrained experimental settings.

</details>


### [170] [HiCL: Hippocampal-Inspired Continual Learning](https://arxiv.org/abs/2508.16651)
*Kushal Kapoor,Wyatt Mackey,Yiannis Aloimonos,Xiaomin Lin*

Main category: cs.LG

TL;DR: HiCL, a hippocampal-inspired architecture, reduces catastrophic forgetting in continual learning with efficient task routing and prioritized replay.


<details>
  <summary>Details</summary>
Motivation: Mitigate catastrophic forgetting in continual learning by using elements inspired by the hippocampal circuitry.

Method: A hippocampal-inspired dual-memory continual learning architecture (HiCL) with grid-cell-like encoding, dentate gyrus-inspired sparse pattern separation, CA3-like autoassociative memory, DG-gated mixture-of-experts, and Elastic Weight Consolidation weighted by inter-task similarity.

Result: HiCL achieves near state-of-the-art results in continual learning tasks at lower computational costs.

Conclusion: The proposed HiCL architecture effectively reduces task interference and achieves near state-of-the-art continual learning results with lower computational costs.

Abstract: We propose HiCL, a novel hippocampal-inspired dual-memory continual learning
architecture designed to mitigate catastrophic forgetting by using elements
inspired by the hippocampal circuitry. Our system encodes inputs through a
grid-cell-like layer, followed by sparse pattern separation using a dentate
gyrus-inspired module with top-k sparsity. Episodic memory traces are
maintained in a CA3-like autoassociative memory. Task-specific processing is
dynamically managed via a DG-gated mixture-of-experts mechanism, wherein inputs
are routed to experts based on cosine similarity between their normalized
sparse DG representations and learned task-specific DG prototypes computed
through online exponential moving averages. This biologically grounded yet
mathematically principled gating strategy enables differentiable, scalable
task-routing without relying on a separate gating network, and enhances the
model's adaptability and efficiency in learning multiple sequential tasks.
Cortical outputs are consolidated using Elastic Weight Consolidation weighted
by inter-task similarity. Crucially, we incorporate prioritized replay of
stored patterns to reinforce essential past experiences. Evaluations on
standard continual learning benchmarks demonstrate the effectiveness of our
architecture in reducing task interference, achieving near state-of-the-art
results in continual learning tasks at lower computational costs.

</details>


### [171] [A Laplace diffusion-based transformer model for heart rate forecasting within daily activity context](https://arxiv.org/abs/2508.16655)
*Andrei Mateescu,Ioana Hadarau,Ionut Anghel,Tudor Cioara,Ovidiu Anchidin,Ancuta Nemes*

Main category: cs.LG

TL;DR: 提出了一种Transformer模型，结合Laplace扩散技术，对患者身体活动驱动的心率波动进行建模。


<details>
  <summary>Details</summary>
Motivation: 由于多种因素，心率可能会显著波动，并且在不将其与患者的实际身体活动相关联的情况下，很难评估变化是否显著。虽然人工智能 (AI) 模型可以提高远程心率监测的准确性和背景理解，但活动数据的集成仍然很少被解决。

Method: Transformer 模型结合 Laplace 扩散技术

Result: 与所考虑的基线模型相比，平均绝对误差降低了 43%。此外，决定系数 R2 为 0.97，表明模型预测的心率与实际心率值非常吻合。

Conclusion: 提出的模型是支持医疗保健提供者和远程患者监控系统的实用且有效的工具。

Abstract: With the advent of wearable Internet of Things (IoT) devices, remote patient
monitoring (RPM) emerged as a promising solution for managing heart failure.
However, the heart rate can fluctuate significantly due to various factors, and
without correlating it to the patient's actual physical activity, it becomes
difficult to assess whether changes are significant. Although Artificial
Intelligence (AI) models may enhance the accuracy and contextual understanding
of remote heart rate monitoring, the integration of activity data is still
rarely addressed. In this paper, we propose a Transformer model combined with a
Laplace diffusion technique to model heart rate fluctuations driven by physical
activity of the patient. Unlike prior models that treat activity as secondary,
our approach conditions the entire modeling process on activity context using
specialized embeddings and attention mechanisms to prioritize activity specific
historical patents. The model captures both long-term patterns and
activity-specific heart rate dynamics by incorporating contextualized
embeddings and dedicated encoder. The Transformer model was validated on a
real-world dataset collected from 29 patients over a 4-month period.
Experimental results show that our model outperforms current state-of-the-art
methods, achieving a 43% reduction in mean absolute error compared to the
considered baseline models. Moreover, the coefficient of determination R2 is
0.97 indicating the model predicted heart rate is in strong agreement with
actual heart rate values. These findings suggest that the proposed model is a
practical and effective tool for supporting both healthcare providers and
remote patient monitoring systems.

</details>


### [172] [OASIS: Open-world Adaptive Self-supervised and Imbalanced-aware System](https://arxiv.org/abs/2508.16656)
*Miru Kim,Mugon Joe,Minhae Kwon*

Main category: cs.LG

TL;DR: 提出了一种新的方法，即使在不平衡数据上进行预训练，也能有效处理开放世界问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习扩展到动态环境中，在处理标签偏移、协变量偏移和未知类出现等开放世界问题时提出了挑战。已经探索了训练后方法来应对这些挑战，使模型适应新出现的数据。但是，当初始预训练是在类不平衡的数据集上执行时，这些方法会受到限制，从而限制了对少数类的泛化。

Method: 我们提出了一种基于对比的预训练方法，该方法增强了分类性能，特别是对于代表性不足的类别。我们的训练后机制生成可靠的伪标签，从而提高了模型针对开放世界问题的鲁棒性。我们还引入了选择性激活标准来优化训练后过程，从而减少了不必要的计算。

Result: 我们的基于对比的预训练方法增强了分类性能，特别是对于代表性不足的类别。我们的训练后机制生成可靠的伪标签，从而提高了模型针对开放世界问题的鲁棒性。

Conclusion: 该方法在各种开放世界场景中的准确性和效率方面均显着优于最先进的自适应技术。

Abstract: The expansion of machine learning into dynamic environments presents
challenges in handling open-world problems where label shift, covariate shift,
and unknown classes emerge. Post-training methods have been explored to address
these challenges, adapting models to newly emerging data. However, these
methods struggle when the initial pre-training is performed on class-imbalanced
datasets, limiting generalization to minority classes. To address this, we
propose a method that effectively handles open-world problems even when
pre-training is conducted on imbalanced data. Our contrastive-based
pre-training approach enhances classification performance, particularly for
underrepresented classes. Our post-training mechanism generates reliable
pseudo-labels, improving model robustness against open-world problems. We also
introduce selective activation criteria to optimize the post-training process,
reducing unnecessary computation. Extensive experiments demonstrate that our
method significantly outperforms state-of-the-art adaptation techniques in both
accuracy and efficiency across diverse open-world scenarios.

</details>


### [173] [WISCA: A Lightweight Model Transition Method to Improve LLM Training via Weight Scaling](https://arxiv.org/abs/2508.16676)
*Jiacheng Li,Jianchao Tan,Zhidong Yang,Pingwei Sun,Feiye Huo,Jiayu Qin,Yerui Sun,Yuchen Xie,Xunliang Cai,Xiangyu Zhang,Maoxin He,Guangming Tan,Weile Jia,Tong Zhao*

Main category: cs.LG

TL;DR: WISCA optimizes weight patterns in LLMs, improving training efficiency and model quality without changing network structures, leading to better convergence and performance.


<details>
  <summary>Details</summary>
Motivation: Lack of systematic optimization of weight patterns during training in Transformer-based LLMs. Weight pattern refers to the distribution and relative magnitudes of weight parameters.

Method: Weight Scaling method called WISCA to enhance training efficiency and model quality by strategically improving neural network weight patterns without changing network structures. Rescales weights while preserving model outputs, indirectly optimizes the model's training trajectory.

Result: WISCA significantly improves convergence quality. 5.6% average improvement on zero-shot validation tasks and 2.12% average reduction in training perplexity across multiple architectures.

Conclusion: WISCA improves convergence quality, generalization capability and loss reduction, especially in LLMs with GQA and LoRA fine-tuning, with 5.6% improvement on zero-shot validation and 2.12% reduction in training perplexity.

Abstract: Transformer architecture gradually dominates the LLM field. Recent advances
in training optimization for Transformer-based large language models (LLMs)
primarily focus on architectural modifications or optimizer adjustments.
However, these approaches lack systematic optimization of weight patterns
during training. Weight pattern refers to the distribution and relative
magnitudes of weight parameters in a neural network. To address this issue, we
propose a Weight Scaling method called WISCA to enhance training efficiency and
model quality by strategically improving neural network weight patterns without
changing network structures. By rescaling weights while preserving model
outputs, WISCA indirectly optimizes the model's training trajectory.
Experiments demonstrate that WISCA significantly improves convergence quality
(measured by generalization capability and loss reduction), particularly in
LLMs with Grouped Query Attention (GQA) architectures and LoRA fine-tuning
tasks. Empirical results show 5.6% average improvement on zero-shot validation
tasks and 2.12% average reduction in training perplexity across multiple
architectures.

</details>


### [174] [Recall-Extend Dynamics: Enhancing Small Language Models through Controlled Exploration and Refined Offline Integration](https://arxiv.org/abs/2508.16677)
*Zhong Guan,Likang Wu,Hongke Zhao,Jiahui Wang,Le Wu*

Main category: cs.LG

TL;DR: This paper proposes RED to enhance the reasoning capabilities of small language models by balancing offline distillation with online reinforcement learning. It addresses issues like insufficient exploration space and distribution discrepancies.


<details>
  <summary>Details</summary>
Motivation: Enhancement of reasoning abilities in small language models (SLMs) has not yet been sufficiently explored. Combining distilled data from larger models with RLVR on small models themselves is a natural approach, but it still faces various challenges and issues.

Method: Recall-Extend Dynamics(RED): Enhancing Small Language Models through Controlled Exploration and Refined Offline Integration. In this paper, we explore the perspective of varying exploration spaces, balancing offline distillation with online reinforcement learning. Simultaneously, we specifically design and optimize for the insertion problem within offline data.

Result: N/A

Conclusion: We regulate the weight of offline-SFT by monitoring the ratio of entropy changes in the model concerning offline and online data, thereby addressing the issues of insufficient exploration space in small models and the redundancy and complexity during the distillation process. Furthermore, to tackle the distribution discrepancies between offline data and the current policy, we design a sample-accuracy-based policy shift mechanism that dynamically chooses between imitating offline distilled data and learning from its own policy.

Abstract: Many existing studies have achieved significant improvements in the reasoning
capabilities of large language models (LLMs) through reinforcement learning
with verifiable rewards (RLVR), while the enhancement of reasoning abilities in
small language models (SLMs) has not yet been sufficiently explored. Combining
distilled data from larger models with RLVR on small models themselves is a
natural approach, but it still faces various challenges and issues. Therefore,
we propose \textit{\underline{R}}ecall-\textit{\underline{E}}xtend
\textit{\underline{D}}ynamics(RED): Enhancing Small Language Models through
Controlled Exploration and Refined Offline Integration. In this paper, we
explore the perspective of varying exploration spaces, balancing offline
distillation with online reinforcement learning. Simultaneously, we
specifically design and optimize for the insertion problem within offline data.
By monitoring the ratio of entropy changes in the model concerning offline and
online data, we regulate the weight of offline-SFT, thereby addressing the
issues of insufficient exploration space in small models and the redundancy and
complexity during the distillation process. Furthermore, to tackle the
distribution discrepancies between offline data and the current policy, we
design a sample-accuracy-based policy shift mechanism that dynamically chooses
between imitating offline distilled data and learning from its own policy.

</details>


### [175] [CALR: Corrective Adaptive Low-Rank Decomposition for Efficient Large Language Model Layer Compression](https://arxiv.org/abs/2508.16680)
*Muchammad Daniyal Kautsar,Afra Majida Hariono,Widyawan,Syukron Abu Ishaq Alfarozi,Kuntpong Wararatpanya*

Main category: cs.LG

TL;DR: CALR是一种新的LLM压缩方法，通过可学习的校正模块来减少功能信息损失，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLM）的巨大尺寸和计算要求，它们在部署方面面临着严峻的挑战。模型压缩技术对于使这些模型适用于资源受限的环境至关重要。标准SVD侧重于最小化矩阵重建误差，通常会导致模型功能性能的大幅下降。性能下降是因为现有方法没有充分纠正压缩过程中丢失的功能信息。

Method: Corrective Adaptive Low-Rank Decomposition (CALR)，一种双组件压缩方法。CALR结合了SVD压缩层的主路径和一个并行的、可学习的、低秩的校正模块，该模块经过专门训练以恢复功能残差误差。

Result: 在SmolLM2-135M、Qwen3-0.6B和Llama-3.2-1B上的实验评估表明，CALR可以将参数数量减少26.93%到51.77%，同时保留原始模型59.45%到90.42%的性能，始终优于LaCo、ShortGPT和LoSparse。

Conclusion: CALR的成功表明，将功能信息丢失视为可学习的信号是一种非常有效的压缩范例。这种方法能够创建更小、更高效的LLM，从而提高它们在实际应用中的可访问性和实际部署。

Abstract: Large Language Models (LLMs) present significant deployment challenges due to
their immense size and computational requirements. Model compression techniques
are essential for making these models practical for resource-constrained
environments. A prominent compression strategy is low-rank factorization via
Singular Value Decomposition (SVD) to reduce model parameters by approximating
weight matrices. However, standard SVD focuses on minimizing matrix
reconstruction error, often leading to a substantial loss of the model's
functional performance. This performance degradation occurs because existing
methods do not adequately correct for the functional information lost during
compression. To address this gap, we introduce Corrective Adaptive Low-Rank
Decomposition (CALR), a two-component compression approach. CALR combines a
primary path of SVD-compressed layers with a parallel, learnable, low-rank
corrective module that is explicitly trained to recover the functional residual
error. Our experimental evaluation on SmolLM2-135M, Qwen3-0.6B, and
Llama-3.2-1B, demonstrates that CALR can reduce parameter counts by 26.93% to
51.77% while retaining 59.45% to 90.42% of the original model's performance,
consistently outperforming LaCo, ShortGPT, and LoSparse. CALR's success shows
that treating functional information loss as a learnable signal is a highly
effective compression paradigm. This approach enables the creation of
significantly smaller, more efficient LLMs, advancing their accessibility and
practical deployment in real-world applications.

</details>


### [176] [STGAtt: A Spatial-Temporal Unified Graph Attention Network for Traffic Flow Forecasting](https://arxiv.org/abs/2508.16685)
*Zhuding Liang,Jianxun Cui,Qingshuang Zeng,Feng Liu,Nenad Filipovic,Tijana Geroski*

Main category: cs.LG

TL;DR: STGAtt 模型优于其他模型，因为它能够捕获复杂的时空依赖关系，并且能够适应动态交通模式。


<details>
  <summary>Details</summary>
Motivation: 准确和及时的交通流量预测对于智能交通系统至关重要。

Method: 空间-时间统一图注意力网络 (STGAtt)

Result: 注意力权重的可视化证实了 STGAtt 适应动态交通模式和捕获远程依赖关系的能力。

Conclusion: STGAtt在各种预测范围内，在 PEMS-BAY 和 SHMetro 数据集上表现优于最先进的基线。

Abstract: Accurate and timely traffic flow forecasting is crucial for intelligent
transportation systems. This paper presents a novel deep learning model, the
Spatial-Temporal Unified Graph Attention Network (STGAtt). By leveraging a
unified graph representation and an attention mechanism, STGAtt effectively
captures complex spatial-temporal dependencies. Unlike methods relying on
separate spatial and temporal dependency modeling modules, STGAtt directly
models correlations within a Spatial-Temporal Unified Graph, dynamically
weighing connections across both dimensions. To further enhance its
capabilities, STGAtt partitions traffic flow observation signal into
neighborhood subsets and employs a novel exchanging mechanism, enabling
effective capture of both short-range and long-range correlations. Extensive
experiments on the PEMS-BAY and SHMetro datasets demonstrate STGAtt's superior
performance compared to state-of-the-art baselines across various prediction
horizons. Visualization of attention weights confirms STGAtt's ability to adapt
to dynamic traffic patterns and capture long-range dependencies, highlighting
its potential for real-world traffic flow forecasting applications.

</details>


### [177] [Multidimensional Distributional Neural Network Output Demonstrated in Super-Resolution of Surface Wind Speed](https://arxiv.org/abs/2508.16686)
*Harrison J. Goldwyn,Mitchell Krock,Johann Rudi,Daniel Getter,Julie Bessac*

Main category: cs.LG

TL;DR: This paper presents a framework for training neural networks with a multidimensional Gaussian loss, generating closed-form predictive distributions over outputs with non-identically distributed and heteroscedastic structure. It introduces a novel regularization strategy and is demonstrated on a surface wind speed downscaling task.


<details>
  <summary>Details</summary>
Motivation: Accurate quantification of uncertainty in neural network predictions remains a central challenge for scientific applications involving high-dimensional, correlated data. While existing methods capture either aleatoric or epistemic uncertainty, few offer closed-form, multidimensional distributions that preserve spatial correlation while remaining computationally tractable.

Method: training neural networks with a multidimensional Gaussian loss, generating closed-form predictive distributions over outputs with non-identically distributed and heteroscedastic structure. It captures aleatoric uncertainty by iteratively estimating the means and covariance matrices, and leverages a Fourier representation of the covariance matrix to stabilize network training and preserve spatial correlation. A novel regularization strategy -- referred to as information sharing -- that interpolates between image-specific and global covariance estimates, enabling convergence of the super-resolution downscaling network trained on image-specific distributional loss functions is introduced.

Result: demonstrated on a super-resolution example.

Conclusion: This framework allows for efficient sampling, explicit correlation modeling, and extensions to more complex distribution families all without disrupting prediction performance. The method is demonstrated on a surface wind speed downscaling task and its broader applicability to uncertainty-aware prediction in scientific models is discussed.

Abstract: Accurate quantification of uncertainty in neural network predictions remains
a central challenge for scientific applications involving high-dimensional,
correlated data. While existing methods capture either aleatoric or epistemic
uncertainty, few offer closed-form, multidimensional distributions that
preserve spatial correlation while remaining computationally tractable. In this
work, we present a framework for training neural networks with a
multidimensional Gaussian loss, generating closed-form predictive distributions
over outputs with non-identically distributed and heteroscedastic structure.
Our approach captures aleatoric uncertainty by iteratively estimating the means
and covariance matrices, and is demonstrated on a super-resolution example. We
leverage a Fourier representation of the covariance matrix to stabilize network
training and preserve spatial correlation. We introduce a novel regularization
strategy -- referred to as information sharing -- that interpolates between
image-specific and global covariance estimates, enabling convergence of the
super-resolution downscaling network trained on image-specific distributional
loss functions. This framework allows for efficient sampling, explicit
correlation modeling, and extensions to more complex distribution families all
without disrupting prediction performance. We demonstrate the method on a
surface wind speed downscaling task and discuss its broader applicability to
uncertainty-aware prediction in scientific models.

</details>


### [178] [Native Logical and Hierarchical Representations with Subspace Embeddings](https://arxiv.org/abs/2508.16687)
*Gabriel Moreira,Zita Marinho,Manuel Marques,João Paulo Costeira,Chenyan Xiong*

Main category: cs.LG

TL;DR: This paper introduces a new method of embedding concepts as linear subspaces. This method models generality and hierarchy, supports set-theoretic operations, and achieves state-of-the-art results in reconstruction and link prediction.


<details>
  <summary>Details</summary>
Motivation: Traditional neural embeddings represent concepts as points, excelling at similarity but struggling with higher-level reasoning and asymmetric relationships.

Method: embedding concepts as linear subspaces and a smooth relaxation of orthogonal projection operators

Result: achieves state-of-the-art results in reconstruction and link prediction on WordNet.

Conclusion: Subspace embeddings surpass bi-encoder baselines on natural language inference benchmarks, offering an interpretable formulation of entailment that is both geometrically grounded and amenable to logical operations.

Abstract: Traditional neural embeddings represent concepts as points, excelling at
similarity but struggling with higher-level reasoning and asymmetric
relationships. We introduce a novel paradigm: embedding concepts as linear
subspaces. This framework inherently models generality via subspace
dimensionality and hierarchy through subspace inclusion. It naturally supports
set-theoretic operations like intersection (conjunction), linear sum
(disjunction) and orthogonal complements (negations), aligning with classical
formal semantics. To enable differentiable learning, we propose a smooth
relaxation of orthogonal projection operators, allowing for the learning of
both subspace orientation and dimension. Our method achieves state-of-the-art
results in reconstruction and link prediction on WordNet. Furthermore, on
natural language inference benchmarks, our subspace embeddings surpass
bi-encoder baselines, offering an interpretable formulation of entailment that
is both geometrically grounded and amenable to logical operations.

</details>


### [179] [Retrieval Capabilities of Large Language Models Scale with Pretraining FLOPs](https://arxiv.org/abs/2508.17400)
*Jacob Portes,Connor Jennings,Erica Ji Yuen,Sasha Doubov,Michael Carbin*

Main category: cs.LG

TL;DR: 检索性能随 LLM 大小和训练成本的增加而提高。


<details>
  <summary>Details</summary>
Motivation: 研究检索性能如何随预训练 FLOP 扩展。

Method: 对从 1.25 亿到 70 亿参数的 LLM 模型大小的检索性能进行了基准测试，这些模型在从 10 亿个 token 到超过 2 万亿个 token 的数据集上进行了预训练。

Result: 检索性能随 LLM 大小、训练持续时间和估计的 FLOP 线性扩展。In-Context Learning 分数与跨检索任务的检索分数密切相关。

Conclusion: 检索性能在零样本 BEIR 任务中可预测地随 LLM 大小、训练持续时间和估计的 FLOP 扩展。In-Context Learning 分数与跨检索任务的检索分数密切相关。这影响了基于 LLM 的检索器的开发。

Abstract: How does retrieval performance scale with pretraining FLOPs? We benchmark
retrieval performance across LLM model sizes from 125 million parameters to 7
billion parameters pretrained on datasets ranging from 1 billion tokens to more
than 2 trillion tokens. We find that retrieval performance on zero-shot BEIR
tasks predictably scales with LLM size, training duration, and estimated FLOPs.
We also show that In-Context Learning scores are strongly correlated with
retrieval scores across retrieval tasks. Finally, we highlight the implications
this has for the development of LLM-based retrievers.

</details>


### [180] [A novel auxiliary equation neural networks method for exactly explicit solutions of nonlinear partial differential equations](https://arxiv.org/abs/2508.16702)
*Shanhao Yuan,Yanqin Liu,Runfa Zhang,Limei Yan,Shunjun Wu,Libo Feng*

Main category: cs.LG

TL;DR: propose an auxiliary equation neural networks method (AENNM) to obtain exact solutions of nonlinear partial differential equations (NLPDEs).


<details>
  <summary>Details</summary>
Motivation: to obtain exact solutions of nonlinear partial differential equations (NLPDEs).

Method: an auxiliary equation neural networks method (AENNM), an innovative analytical method that integrates neural networks (NNs) models with the auxiliary equation method

Result: By embedding the auxiliary equation method into the NNs framework, we derive previously unreported solutions. The exact analytical solutions are expressed in terms of hyperbolic functions, trigonometric functions, and rational functions.

Conclusion: This research provides a novel methodological framework for addressing NLPDEs, with broad applicability across scientific and engineering fields.

Abstract: In this study, we firstly propose an auxiliary equation neural networks
method (AENNM), an innovative analytical method that integrates neural networks
(NNs) models with the auxiliary equation method to obtain exact solutions of
nonlinear partial differential equations (NLPDEs). A key novelty of this method
is the introduction of a novel activation function derived from the solutions
of the Riccati equation, establishing a new mathematical link between
differential equations theory and deep learning. By combining the strong
approximation capability of NNs with the high precision of symbolic
computation, AENNM significantly enhances computational efficiency and
accuracy. To demonstrate the effectiveness of the AENNM in solving NLPDEs,
three numerical examples are investigated, including the nonlinear evolution
equation, the Korteweg-de Vries-Burgers equation, and the (2+1)-dimensional
Boussinesq equation. Furthermore, some new trial functions are constructed by
setting specific activation functions within the "2-2-2-1" and "3-2-2-1" NNs
models. By embedding the auxiliary equation method into the NNs framework, we
derive previously unreported solutions. The exact analytical solutions are
expressed in terms of hyperbolic functions, trigonometric functions, and
rational functions. Finally, three-dimensional plots, contour plots, and
density plots are presented to illustrate the dynamic characteristics of the
obtained solutions. This research provides a novel methodological framework for
addressing NLPDEs, with broad applicability across scientific and engineering
fields.

</details>


### [181] [Aligning Distributionally Robust Optimization with Practical Deep Learning Needs](https://arxiv.org/abs/2508.16734)
*Dmitrii Feoktistov,Igor Ignashin,Andrey Veprikov,Nikita Borovko,Alexander Bogdanov,Savelii Chezhegov,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: This paper introduces ALSO, an adaptive algorithm for a modified DRO objective that can handle weight assignment to sample groups, and proves its convergence for non-convex objectives. ALSO outperforms traditional optimizers and existing DRO methods on diverse Deep Learning tasks.


<details>
  <summary>Details</summary>
Motivation: a significant gap exists between DRO and current DL practices. Modern DL optimizers require adaptivity and the ability to handle stochastic gradients, as these methods demonstrate superior performance. Additionally, for practical applications, a method should allow weight assignment not only to individual samples, but also to groups of objects

Method: introducing ALSO – Adaptive Loss Scaling Optimizer – an adaptive algorithm for a modified DRO objective that can handle weight assignment to sample groups. We prove the convergence of our proposed algorithm for non-convex objectives

Result: convergence of our proposed algorithm for non-convex objectives, which is the typical case for DL models

Conclusion: ALSO outperforms both traditional optimizers and existing DRO methods on diverse Deep Learning tasks.

Abstract: While traditional Deep Learning (DL) optimization methods treat all training
samples equally, Distributionally Robust Optimization (DRO) adaptively assigns
importance weights to different samples. However, a significant gap exists
between DRO and current DL practices. Modern DL optimizers require adaptivity
and the ability to handle stochastic gradients, as these methods demonstrate
superior performance. Additionally, for practical applications, a method should
allow weight assignment not only to individual samples, but also to groups of
objects (for example, all samples of the same class). This paper aims to bridge
this gap by introducing ALSO $\unicode{x2013}$ Adaptive Loss Scaling Optimizer
$\unicode{x2013}$ an adaptive algorithm for a modified DRO objective that can
handle weight assignment to sample groups. We prove the convergence of our
proposed algorithm for non-convex objectives, which is the typical case for DL
models. Empirical evaluation across diverse Deep Learning tasks, from Tabular
DL to Split Learning tasks, demonstrates that ALSO outperforms both traditional
optimizers and existing DRO methods.

</details>


### [182] [Deep Learning for Markov Chains: Lyapunov Functions, Poisson's Equation, and Stationary Distributions](https://arxiv.org/abs/2508.16737)
*Yanlin Qu,Jose Blanchet,Peter Glynn*

Main category: cs.LG

TL;DR: 利用深度学习自动构建Lyapunov函数，用于马尔可夫模型的稳定性分析，并可扩展到解决泊松方程和估计平稳分布。


<details>
  <summary>Details</summary>
Motivation: Lyapunov函数对于建立马尔可夫模型的稳定性至关重要，但它们的构建通常需要大量的创造力和分析工作。

Method: 通过训练神经网络来满足从首次转移分析中导出的积分方程，利用深度学习来自动完成Lyapunov函数的构建。

Result: 该方法可以适用于解决泊松方程和估计平稳分布。即使在非紧凑状态空间上的马尔可夫链中，该方法仍然有效。

Conclusion: 该方法通过几个排队论及其他领域的例子证明了其有效性。

Abstract: Lyapunov functions are fundamental to establishing the stability of Markovian
models, yet their construction typically demands substantial creativity and
analytical effort. In this paper, we show that deep learning can automate this
process by training neural networks to satisfy integral equations derived from
first-transition analysis. Beyond stability analysis, our approach can be
adapted to solve Poisson's equation and estimate stationary distributions.
While neural networks are inherently function approximators on compact domains,
it turns out that our approach remains effective when applied to Markov chains
on non-compact state spaces. We demonstrate the effectiveness of this
methodology through several examples from queueing theory and beyond.

</details>


### [183] [Heterogeneous co-occurrence embedding for visual information exploration](https://arxiv.org/abs/2508.17663)
*Takuro Ishida,Tetsuo Furukawa*

Main category: cs.LG

TL;DR: 提出了一种用于共现数据的嵌入方法，以实现视觉信息探索，通过最大化互信息来保留原始依赖结构，并应用于各种数据集。


<details>
  <summary>Details</summary>
Motivation: 本文提出了一种针对共现数据的嵌入方法，旨在进行视觉信息探索。我们考虑在异构域的元素对之间测量共现概率的情况。

Method: 提出了一种针对共现数据的嵌入方法，旨在进行视觉信息探索。该方法将异构元素映射到相应的二维潜在空间中，从而可以可视化域之间的非对称关系。关键思想是以最大化互信息的方式嵌入元素，从而尽可能保留原始依赖结构。

Result: 该方法可以将异构元素映射到相应的二维潜在空间中，从而可以可视化域之间的非对称关系。提出了一种可视化方法，该方法基于条件概率为潜在空间分配颜色，从而使用户能够交互式地探索非对称关系。

Conclusion: 展示了该方法在形容词-名词数据集、NeurIPS 数据集和主语-动词-宾语数据集上的效用，展示了域内和域间分析。

Abstract: This paper proposes an embedding method for co-occurrence data aimed at
visual information exploration. We consider cases where co-occurrence
probabilities are measured between pairs of elements from heterogeneous
domains. The proposed method maps these heterogeneous elements into
corresponding two-dimensional latent spaces, enabling visualization of
asymmetric relationships between the domains. The key idea is to embed the
elements in a way that maximizes their mutual information, thereby preserving
the original dependency structure as much as possible. This approach can be
naturally extended to cases involving three or more domains, using a
generalization of mutual information known as total correlation. For
inter-domain analysis, we also propose a visualization method that assigns
colors to the latent spaces based on conditional probabilities, allowing users
to explore asymmetric relationships interactively. We demonstrate the utility
of the method through applications to an adjective-noun dataset, the NeurIPS
dataset, and a subject-verb-object dataset, showcasing both intra- and
inter-domain analysis.

</details>


### [184] [WST: Weak-to-Strong Knowledge Transfer via Reinforcement Learning](https://arxiv.org/abs/2508.16741)
*Haosen Ge,Shuo Li,Lianghuan Huang*

Main category: cs.LG

TL;DR: 我们引入了弱到强转移 (WST)，这是一种自动提示工程框架，其中小型“教师”模型生成指令，以增强更大的“学生”模型的性能。


<details>
  <summary>Details</summary>
Motivation: 有效的提示工程对于许多应用来说仍然是一项具有挑战性的任务。

Method: 弱到强转移 (WST)，这是一种自动提示工程框架，其中小型“教师”模型生成指令，以增强更大的“学生”模型的性能。使用强化学习，教师模型的指令会根据学生模型的结果进行迭代改进

Result: 在推理（MATH-500、GSM8K）和对齐（HH-RLHF）基准测试中取得了显著的收益——MATH-500 上为 98%，HH-RLHF 上为 134%——并且超越了 GPT-4o-mini 和 Llama-70B 等基线。

Conclusion: 小模型可以可靠地支撑更大的模型，解锁潜在能力，同时避免更强大的教师可能引入的误导性提示，从而将 WST 确立为一种可扩展的解决方案，用于高效、安全地进行 LLM 提示细化。

Abstract: Effective prompt engineering remains a challenging task for many
applications. We introduce Weak-to-Strong Transfer (WST), an automatic prompt
engineering framework where a small "Teacher" model generates instructions that
enhance the performance of a much larger "Student" model. Unlike prior work,
WST requires only a weak teacher, making it efficient and broadly applicable in
settings where large models are closed-source or difficult to fine-tune. Using
reinforcement learning, the Teacher Model's instructions are iteratively
improved based on the Student Model's outcomes, yielding substantial gains
across reasoning (MATH-500, GSM8K) and alignment (HH-RLHF) benchmarks - 98% on
MATH-500 and 134% on HH-RLHF - and surpassing baselines such as GPT-4o-mini and
Llama-70B. These results demonstrate that small models can reliably scaffold
larger ones, unlocking latent capabilities while avoiding misleading prompts
that stronger teachers may introduce, establishing WST as a scalable solution
for efficient and safe LLM prompt refinement.

</details>


### [185] [Hyperbolic Multimodal Representation Learning for Biological Taxonomies](https://arxiv.org/abs/2508.16744)
*ZeMing Gong,Chuanqi Tang,Xiaoliang Huo,Nicholas Pellegrino,Austin T. Wang,Graham W. Taylor,Angel X. Chang,Scott C. Lowe,Joakim Bruslund Haurum*

Main category: cs.LG

TL;DR: 研究了双曲网络是否能为层次模型提供更好的嵌入空间。


<details>
  <summary>Details</summary>
Motivation: 生物多样性研究中的分类分类包括根据证据将生物标本组织成结构化的层次结构，这些证据可能来自图像和遗传信息等多种模式。我们研究了双曲网络是否能为这种层次模型提供更好的嵌入空间。

Method: 使用对比和一种新的基于堆叠蕴涵的目标，将多模态输入嵌入到一个共享的双曲空间中。

Result: 在BIOSCAN-1M数据集上的实验表明，双曲嵌入在欧氏基线下实现了竞争性的性能，并且在使用DNA条形码的unseen物种分类方面优于所有其他模型。然而，细粒度分类和开放世界泛化仍然具有挑战性。

Conclusion: 该框架为生物多样性建模提供了一个结构感知的基石，在物种发现、生态监测和保护工作中有潜在的应用。

Abstract: Taxonomic classification in biodiversity research involves organizing
biological specimens into structured hierarchies based on evidence, which can
come from multiple modalities such as images and genetic information. We
investigate whether hyperbolic networks can provide a better embedding space
for such hierarchical models. Our method embeds multimodal inputs into a shared
hyperbolic space using contrastive and a novel stacked entailment-based
objective. Experiments on the BIOSCAN-1M dataset show that hyperbolic embedding
achieves competitive performance with Euclidean baselines, and outperforms all
other models on unseen species classification using DNA barcodes. However,
fine-grained classification and open-world generalization remain challenging.
Our framework offers a structure-aware foundation for biodiversity modelling,
with potential applications to species discovery, ecological monitoring, and
conservation efforts.

</details>


### [186] [Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling](https://arxiv.org/abs/2508.16745)
*Ivan Rodkin,Daniil Orel,Konstantin Smirnov,Arman Bolatov,Bilal Elbouardi,Besher Hassan,Yuri Kuratov,Aydar Bulatov,Preslav Nakov,Timothy Baldwin,Artem Shelmanov,Mikhail Burtsev*

Main category: cs.LG

TL;DR: 大型语言模型的多步推理能力受到模型深度和计算资源的限制。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型如何学习和执行多步推理仍然是一个开放的问题。

Method: 在细胞自动机框架内，通过使用随机布尔函数和随机初始条件生成的序列进行训练，排除了记忆的可能性。

Result: 大多数神经架构学会了抽象潜在规则。模型在单步预测中实现了高精度。

Conclusion: 模型在单步预测中表现出色，但在多步推理中性能急剧下降。增加模型深度对于顺序计算至关重要。通过复发、记忆和测试时计算扩展有效模型深度可以显著增强推理能力。

Abstract: Reasoning is a core capability of large language models, yet understanding
how they learn and perform multi-step reasoning remains an open problem. In
this study, we explore how different architectures and training methods affect
model multi-step reasoning capabilities within a cellular automata framework.
By training on state sequences generated with random Boolean functions for
random initial conditions to exclude memorization, we demonstrate that most
neural architectures learn to abstract the underlying rules. While models
achieve high accuracy in next-state prediction, their performance declines
sharply if multi-step reasoning is required. We confirm that increasing model
depth plays a crucial role for sequential computations. We demonstrate that an
extension of the effective model depth with recurrence, memory, and test-time
compute scaling substantially enhances reasoning capabilities.

</details>


### [187] [FAIRWELL: Fair Multimodal Self-Supervised Learning for Wellbeing Prediction](https://arxiv.org/abs/2508.16748)
*Jiaee Cheong,Abtin Mogharabin,Paul Liang,Hatice Gunes,Sinan Kalkan*

Main category: cs.LG

TL;DR: FAIRWELL, a novel loss function, improves fairness in multimodal prediction tasks by learning subject-independent representations using variance-invariance-covariance regularization.


<details>
  <summary>Details</summary>
Motivation: Early efforts on leveraging self-supervised learning (SSL) to improve machine learning (ML) fairness has proven promising. However, such an approach has yet to be explored within a multimodal context. Prior work has shown that, within a multimodal setting, different modalities contain modality-unique information that can complement information of other modalities.

Method: A novel subject-level loss function adapting the variance-invariance-covariance regularization (VICReg) method.

Result: The method was evaluated on three challenging real-world heterogeneous healthcare datasets (i.e. D-Vlog, MIMIC and MODMA) which contain different modalities of varying length and different prediction tasks.

Conclusion: The proposed FAIRWELL framework improves overall fairness performance with minimal reduction in classification performance and significantly improves on the performance-fairness Pareto frontier.

Abstract: Early efforts on leveraging self-supervised learning (SSL) to improve machine
learning (ML) fairness has proven promising. However, such an approach has yet
to be explored within a multimodal context. Prior work has shown that, within a
multimodal setting, different modalities contain modality-unique information
that can complement information of other modalities. Leveraging on this, we
propose a novel subject-level loss function to learn fairer representations via
the following three mechanisms, adapting the variance-invariance-covariance
regularization (VICReg) method: (i) the variance term, which reduces reliance
on the protected attribute as a trivial solution; (ii) the invariance term,
which ensures consistent predictions for similar individuals; and (iii) the
covariance term, which minimizes correlational dependence on the protected
attribute. Consequently, our loss function, coined as FAIRWELL, aims to obtain
subject-independent representations, enforcing fairness in multimodal
prediction tasks. We evaluate our method on three challenging real-world
heterogeneous healthcare datasets (i.e. D-Vlog, MIMIC and MODMA) which contain
different modalities of varying length and different prediction tasks. Our
findings indicate that our framework improves overall fairness performance with
minimal reduction in classification performance and significantly improves on
the performance-fairness Pareto frontier.

</details>


### [188] [DR-CircuitGNN: Training Acceleration of Heterogeneous Circuit Graph Neural Network on GPUs](https://arxiv.org/abs/2508.16769)
*Yuebo Luo,Shiyang Li,Junran Tao,Kiran Thorat,Xi Xie,Hongwu Peng,Nuo Xu,Caiwen Ding,Shaoyi Huang*

Main category: cs.LG

TL;DR: DR-CircuitGNN accelerates HGNN training for EDA circuit graphs with sparsity-aware kernels and CPU-GPU parallelism, achieving significant speedups with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: GNNs are promising for EDA design but fail to capture the full complexity of EDA designs. HGNNs improve representation but suffer from high computational complexity due to their serial message-passing scheme.

Method: The paper proposes DR-CircuitGNN, a fast GPU kernel design leveraging row-wise sparsity-aware Dynamic-ReLU and optimized SpMM kernels during heterogeneous message-passing. It also introduces a parallel optimization strategy to maximize CPU-GPU concurrency.

Result: Experiments on CircuitNet designs show DR-CircuitGNN achieves up to 3.51x and 4.09x speedup for forward and backward propagation, respectively, compared to SOTA. Parallel design enables up to 2.71x speed up over cuSPARSE with negligible impact on correlation scores and error rates.

Conclusion: The proposed DR-CircuitGNN method achieves significant speedups in HGNN training on EDA-related circuit graph datasets compared to SOTA methods, with minimal impact on accuracy.

Abstract: The increasing scale and complexity of integrated circuit design have led to
increased challenges in Electronic Design Automation (EDA). Graph Neural
Networks (GNNs) have emerged as a promising approach to assist EDA design as
circuits can be naturally represented as graphs. While GNNs offer a foundation
for circuit analysis, they often fail to capture the full complexity of EDA
designs. Heterogeneous Graph Neural Networks (HGNNs) can better interpret EDA
circuit graphs as they capture both topological relationships and geometric
features. However, the improved representation capability comes at the cost of
even higher computational complexity and processing cost due to their serial
module-wise message-passing scheme, creating a significant performance
bottleneck. In this paper, we propose DR-CircuitGNN, a fast GPU kernel design
by leveraging row-wise sparsity-aware Dynamic-ReLU and optimizing SpMM kernels
during heterogeneous message-passing to accelerate HGNNs training on
EDA-related circuit graph datasets. To further enhance performance, we propose
a parallel optimization strategy that maximizes CPU-GPU concurrency by
concurrently processing independent subgraphs using multi-threaded CPU
initialization and GPU kernel execution via multiple cudaStreams. Our
experiments show that on three representative CircuitNet designs (small,
medium, large), the proposed method can achieve up to 3.51x and 4.09x speedup
compared to the SOTA for forward and backward propagation, respectively. On
full-size CircuitNet and sampled Mini-CircuitNet, our parallel design enables
up to 2.71x speed up over the official DGL implementation cuSPARSE with
negligible impact on correlation scores and error rates.

</details>


### [189] [Latent Graph Learning in Generative Models of Neural Signals](https://arxiv.org/abs/2508.16776)
*Nathan X. Kodama,Kenneth A. Loparo*

Main category: cs.LG

TL;DR: 本文探讨了神经信号生成模型中的潜在图学习，发现提取的网络表示与底层有向图之间具有适度的对齐，并且在共同输入图表示中具有强大的对齐。


<details>
  <summary>Details</summary>
Motivation: 从神经信号推断时间交互图和高阶结构是构建系统神经科学生成模型的关键问题。大规模神经数据的基础模型代表了神经信号的共享潜在结构。然而，在基础模型中提取可解释的潜在图表示仍然具有挑战性且尚未解决。

Method: 通过针对具有已知地面实况连接的神经回路的数值模拟进行测试，我们评估了解释学习模型权重的几个假设。

Result: 我们发现了提取的网络表示与底层有向图之间具有适度的对齐，并且在共同输入图表示中具有强大的对齐。

Conclusion: 提取的网络表示与底层有向图之间具有适度的对齐，并且在共同输入图表示中具有强大的对齐。这些发现激发了在神经数据的大规模基础模型构建中结合基于图的几何约束的路径。

Abstract: Inferring temporal interaction graphs and higher-order structure from neural
signals is a key problem in building generative models for systems
neuroscience. Foundation models for large-scale neural data represent shared
latent structures of neural signals. However, extracting interpretable latent
graph representations in foundation models remains challenging and unsolved.
Here we explore latent graph learning in generative models of neural signals.
By testing against numerical simulations of neural circuits with known
ground-truth connectivity, we evaluate several hypotheses for explaining
learned model weights. We discover modest alignment between extracted network
representations and the underlying directed graphs and strong alignment in the
co-input graph representations. These findings motivate paths towards
incorporating graph-based geometric constraints in the construction of
large-scale foundation models for neural data.

</details>


### [190] [Interpreting the Effects of Quantization on LLMs](https://arxiv.org/abs/2508.16785)
*Manpreet Singh,Hassan Sajjad*

Main category: cs.LG

TL;DR: This study investigates the impact of quantization on LLMs' internal representations using interpretability techniques, finding minor effects on calibration and neuron behavior, suggesting quantization is a reliable compression method.


<details>
  <summary>Details</summary>
Motivation: impact of quantization on internal representations remains understudied, raising questions about the reliability of quantized models

Method: interpretability techniques to investigate how quantization affects model and neuron behavior. We analyze multiple LLMs under 4-bit and 8-bit quantization

Result: impact of quantization on model calibration is generally minor. Analysis of neuron activations indicates that the number of dead neurons remains consistent regardless of quantization. The effect of quantization on neuron redundancy varies across models

Conclusion: quantization may vary by model and tasks, we did not observe any drastic change which may discourage the use of quantization as a reliable model compression technique

Abstract: Quantization offers a practical solution to deploy LLMs in
resource-constraint environments. However, its impact on internal
representations remains understudied, raising questions about the reliability
of quantized models. In this study, we employ a range of interpretability
techniques to investigate how quantization affects model and neuron behavior.
We analyze multiple LLMs under 4-bit and 8-bit quantization. Our findings
reveal that the impact of quantization on model calibration is generally minor.
Analysis of neuron activations indicates that the number of dead neurons, i.e.,
those with activation values close to 0 across the dataset, remains consistent
regardless of quantization. In terms of neuron contribution to predictions, we
observe that smaller full precision models exhibit fewer salient neurons,
whereas larger models tend to have more, with the exception of Llama-2-7B. The
effect of quantization on neuron redundancy varies across models. Overall, our
findings suggest that effect of quantization may vary by model and tasks,
however, we did not observe any drastic change which may discourage the use of
quantization as a reliable model compression technique.

</details>


### [191] [Anchor-MoE: A Mean-Anchored Mixture of Experts For Probabilistic Regression](https://arxiv.org/abs/2508.16802)
*Baozhuo Su,Zhengxian Qu*

Main category: cs.LG

TL;DR: Anchor-MoE 是一种用于概率和点回归的模型，它在 UCI 回归数据集上实现了与现有技术相当或更好的结果。


<details>
  <summary>Details</summary>
Motivation: 不确定性下的回归是整个科学和工程的基础。

Method: 我们提出了锚定专家混合模型 (Anchor-MoE)，该模型可以处理概率回归和点回归。我们使用经过调整的梯度提升模型来提供锚点均值；但是，任何现成的点回归器都可以用作锚点。锚点预测被投影到潜在空间中，在该空间中，可学习的度量窗口内核对局部性进行评分，并且软路由器将每个样本分派到一小组混合密度网络专家；专家产生异方差校正和预测方差。我们通过最小化负对数似然来训练，并在不相交的校准拆分上拟合预测均值的事后线性图，以提高点精度。

Result: 假设 $\alpha$ 阶的 H"older 平滑回归函数和具有有界重叠的固定 Lipschitz 统一划分权重，我们表明 Anchor-MoE 达到 minimax 最优 $L^2$ 风险率 $O\!\big(N^{-2\alpha/(2\alpha+d)}\big)$。此外，CRPS 测试泛化差距的缩放比例为 $\widetilde{O}\!\Big(\sqrt{(\log(Mh)+P+K)/N}\Big)$；它在 $Mh$ 中是对数的，并且在 $P$ 和 $K$ 中按平方根缩放。在有界重叠路由下，$K$ 可以替换为 $k$，并且对潜在维度的任何依赖性都吸收到 $P$ 中。在一致有界的均值和方差下，对于测试 NLL，类似 $\widetilde{O}\!\big(\sqrt{(\log(Mh)+P+K)/N}\big)$ 的缩放比例成立，直至常数。

Conclusion: Anchor-MoE 在标准 UCI 回归中始终与强大的 NGBoost 基线在 RMSE 和 NLL 方面相匹配或超过，并且在多个数据集上实现了我们基准套件上的最新概率回归结果。

Abstract: Regression under uncertainty is fundamental across science and engineering.
We present an Anchored Mixture of Experts (Anchor-MoE), a model that handles
both probabilistic and point regression. For simplicity, we use a tuned
gradient-boosting model to furnish the anchor mean; however, any off-the-shelf
point regressor can serve as the anchor. The anchor prediction is projected
into a latent space, where a learnable metric-window kernel scores locality and
a soft router dispatches each sample to a small set of mixture-density-network
experts; the experts produce a heteroscedastic correction and predictive
variance. We train by minimizing negative log-likelihood, and on a disjoint
calibration split fit a post-hoc linear map on predicted means to improve point
accuracy. On the theory side, assuming a H\"older smooth regression function of
order~$\alpha$ and fixed Lipschitz partition-of-unity weights with bounded
overlap, we show that Anchor-MoE attains the minimax-optimal $L^2$ risk rate
$O\!\big(N^{-2\alpha/(2\alpha+d)}\big)$. In addition, the CRPS test
generalization gap scales as
$\widetilde{O}\!\Big(\sqrt{(\log(Mh)+P+K)/N}\Big)$; it is logarithmic in $Mh$
and scales as the square root in $P$ and $K$. Under bounded-overlap routing,
$K$ can be replaced by $k$, and any dependence on a latent dimension is
absorbed into $P$. Under uniformly bounded means and variances, an analogous
$\widetilde{O}\!\big(\sqrt{(\log(Mh)+P+K)/N}\big)$ scaling holds for the test
NLL up to constants. Empirically, across standard UCI regressions, Anchor-MoE
consistently matches or surpasses the strong NGBoost baseline in RMSE and NLL;
on several datasets it achieves new state-of-the-art probabilistic regression
results on our benchmark suite. Code is available at
https://github.com/BaozhuoSU/Probabilistic_Regression.

</details>


### [192] [Uncertainty Propagation Networks for Neural Ordinary Differential Equations](https://arxiv.org/abs/2508.16815)
*Hadi Jahanshahi,Zheng H. Zhu*

Main category: cs.LG

TL;DR: UPN is a neural ODE that models both state evolution and its uncertainty, demonstrating effectiveness in CNFs, time-series forecasting, and trajectory prediction.


<details>
  <summary>Details</summary>
Motivation: incorporate uncertainty quantification into continuous-time modeling

Method: introducing Uncertainty Propagation Network (UPN), a family of neural differential equations that model state evolution and its associated uncertainty by parameterizing coupled differential equations for mean and covariance dynamics

Result: UPN efficiently propagates uncertainty through nonlinear dynamics without discretization artifacts, adapts its evaluation strategy, provides principled uncertainty quantification, and handles irregularly-sampled observations naturally.

Conclusion: UPN's effectiveness is demonstrated across CNFs, time-series forecasting, and trajectory prediction in dynamical systems.

Abstract: This paper introduces Uncertainty Propagation Network (UPN), a novel family
of neural differential equations that naturally incorporate uncertainty
quantification into continuous-time modeling. Unlike existing neural ODEs that
predict only state trajectories, UPN simultaneously model both state evolution
and its associated uncertainty by parameterizing coupled differential equations
for mean and covariance dynamics. The architecture efficiently propagates
uncertainty through nonlinear dynamics without discretization artifacts by
solving coupled ODEs for state and covariance evolution while enabling
state-dependent, learnable process noise. The continuous-depth formulation
adapts its evaluation strategy to each input's complexity, provides principled
uncertainty quantification, and handles irregularly-sampled observations
naturally. Experimental results demonstrate UPN's effectiveness across multiple
domains: continuous normalizing flows (CNFs) with uncertainty quantification,
time-series forecasting with well-calibrated confidence intervals, and robust
trajectory prediction in both stable and chaotic dynamical systems.

</details>


### [193] [Understanding and Tackling Over-Dilution in Graph Neural Networks](https://arxiv.org/abs/2508.16829)
*Junhyun Lee,Veronika Thost,Bumsoo Kim,Jaewoo Kang,Tengfei Ma*

Main category: cs.LG

TL;DR: This paper identifies and formalizes the concept of 'over-dilution' in MPNNs, where node information gets diluted, and proposes a transformer-based solution to address it.


<details>
  <summary>Details</summary>
Motivation: Message Passing Neural Networks (MPNNs) struggle with unintended behaviors, such as over-smoothing and over-squashing, due to irregular data structures. Information specific to an individual node can become significantly diluted within a single layer.

Method: The paper introduces the concept of Over-dilution and formulates it with two dilution factors: intra-node dilution for attribute-level and inter-node dilution for node-level representations. A transformer-based solution is proposed to alleviate over-dilution.

Result: The findings provide new insights into the limitations of MPNNs and contribute to the development of informative representations.

Conclusion: This paper introduces a transformer-based solution that alleviates over-dilution and complements existing node embedding methods like MPNNs, contributing to the development of informative representations.

Abstract: Message Passing Neural Networks (MPNNs) hold a key position in machine
learning on graphs, but they struggle with unintended behaviors, such as
over-smoothing and over-squashing, due to irregular data structures. The
observation and formulation of these limitations have become foundational in
constructing more informative graph representations. In this paper, we delve
into the limitations of MPNNs, focusing on aspects that have previously been
overlooked. Our observations reveal that even within a single layer, the
information specific to an individual node can become significantly diluted. To
delve into this phenomenon in depth, we present the concept of Over-dilution
and formulate it with two dilution factors: intra-node dilution for
attribute-level and inter-node dilution for node-level representations. We also
introduce a transformer-based solution that alleviates over-dilution and
complements existing node embedding methods like MPNNs. Our findings provide
new insights and contribute to the development of informative representations.
The implementation and supplementary materials are publicly available at
https://github.com/LeeJunHyun/NATR.

</details>


### [194] [Out of Distribution Detection for Efficient Continual Learning in Quality Prediction for Arc Welding](https://arxiv.org/abs/2508.16832)
*Yannik Hahn,Jan Voets,Antonin Koenigsfeld,Hasan Tercan,Tobias Meisen*

Main category: cs.LG

TL;DR: 本研究通过整合异常检测与持续学习策略，优化模型适应性，从而应对动态制造环境中质量预测模型的分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于机器学习的质量预测模型在面对动态制造环境中固有的分布偏移时，表现出关键的局限性。

Method: 通过利用其自回归损失，扩展了VQ-VAE Transformer架构，作为一种可靠的异常检测机制。

Result: 该框架有效地保持了跨显著分布偏移的稳健质量预测能力。

Conclusion: 该研究提供了一种可解释且自适应的动态制造过程质量保证解决方案，是朝着工业环境中稳健、实用的AI系统迈出的关键一步。

Abstract: Modern manufacturing relies heavily on fusion welding processes, including
gas metal arc welding (GMAW). Despite significant advances in machine
learning-based quality prediction, current models exhibit critical limitations
when confronted with the inherent distribution shifts that occur in dynamic
manufacturing environments. In this work, we extend the VQ-VAE Transformer
architecture - previously demonstrating state-of-the-art performance in weld
quality prediction - by leveraging its autoregressive loss as a reliable
out-of-distribution (OOD) detection mechanism. Our approach exhibits superior
performance compared to conventional reconstruction methods, embedding
error-based techniques, and other established baselines. By integrating OOD
detection with continual learning strategies, we optimize model adaptation,
triggering updates only when necessary and thereby minimizing costly labeling
requirements. We introduce a novel quantitative metric that simultaneously
evaluates OOD detection capability while interpreting in-distribution
performance. Experimental validation in real-world welding scenarios
demonstrates that our framework effectively maintains robust quality prediction
capabilities across significant distribution shifts, addressing critical
challenges in dynamic manufacturing environments where process parameters
frequently change. This research makes a substantial contribution to applied
artificial intelligence by providing an explainable and at the same time
adaptive solution for quality assurance in dynamic manufacturing processes - a
crucial step towards robust, practical AI systems in the industrial
environment.

</details>


### [195] [Physics-Inspired Spatial Temporal Graph Neural Networks for Predicting Industrial Chain Resilience](https://arxiv.org/abs/2508.16836)
*Bicheng Wang,Junping Wang,Yibo Xue*

Main category: cs.LG

TL;DR: 本文提出了一种物理信息神经符号方法，用于预测产业链弹性。


<details>
  <summary>Details</summary>
Motivation: 产业链在国民经济可持续发展中发挥着越来越重要的作用。然而，作为一种典型的复杂网络，数据驱动的深度学习在描述和分析复杂网络的弹性方面仍处于起步阶段，其核心是缺乏描述系统动力学的理论框架。

Method: 提出了一种物理信息神经符号方法来描述复杂网络的演化动力学，用于弹性预测。核心思想是学习物理实体的活动状态动力学，并将其集成到多层时空协同演化网络中，并使用物理信息方法实现物理符号动力学和时空协同演化拓扑的联合学习。

Result: 实验结果表明，该模型能够获得更好的结果，更准确有效地预测产业链的弹性。

Conclusion: 该模型能够获得更好的结果，更准确有效地预测产业链弹性，对行业发展具有一定的实际意义。

Abstract: Industrial chain plays an increasingly important role in the sustainable
development of national economy. However, as a typical complex network,
data-driven deep learning is still in its infancy in describing and analyzing
the resilience of complex networks, and its core is the lack of a theoretical
framework to describe the system dynamics. In this paper, we propose a
physically informative neural symbolic approach to describe the evolutionary
dynamics of complex networks for resilient prediction. The core idea is to
learn the dynamics of the activity state of physical entities and integrate it
into the multi-layer spatiotemporal co-evolution network, and use the physical
information method to realize the joint learning of physical symbol dynamics
and spatiotemporal co-evolution topology, so as to predict the industrial chain
resilience. The experimental results show that the model can obtain better
results and predict the elasticity of the industry chain more accurately and
effectively, which has certain practical significance for the development of
the industry.

</details>


### [196] [Neural Contrast Expansion for Explainable Structure-Property Prediction and Random Microstructure Design](https://arxiv.org/abs/2508.16857)
*Guangyu Nie,Yang Jiao,Yi Ren*

Main category: cs.LG

TL;DR: 我们提出了一种既经济高效又可解释的结构-属性模型，它基于强对比扩展 (SCE) 形式主义，并将无界随机场的 N 点相关性解析地映射到其有效属性。


<details>
  <summary>Details</summary>
Motivation: 传统上，预测此类属性可以通过求解从微观结构样本导出的 PDE 或构建直接将微观结构样本映射到属性的数据驱动模型来完成。前者运行成本较高，但提供了可解释的灵敏度信息，可以指导材料设计；如果数据开销被摊销，后者可能更具成本效益，但其学习到的敏感性通常不太容易解释。

Method: 我们提出了一种神经对比扩展 (NCE)，这是一种受 SCE 启发的架构，可以从结构-属性数据中学习替代 PDE 内核。

Result: 对于静态传导和电磁波传播案例，我们表明 NCE 模型揭示了准确且有见地的灵敏度信息，可用于材料设计。与其他 PDE 内核学习方法相比，我们的方法不需要测量 PDE 解决方案字段，而只需要在材料开发环境中更容易获得的宏观属性测量。

Conclusion: NCE模型能够揭示准确且有见地的灵敏度信息，可用于材料设计。

Abstract: Effective properties of composite materials are defined as the ensemble
average of property-specific PDE solutions over the underlying microstructure
distributions. Traditionally, predicting such properties can be done by solving
PDEs derived from microstructure samples or building data-driven models that
directly map microstructure samples to properties. The former has a higher
running cost, but provides explainable sensitivity information that may guide
material design; the latter could be more cost-effective if the data overhead
is amortized, but its learned sensitivities are often less explainable. With a
focus on properties governed by linear self-adjoint PDEs (e.g., Laplace,
Helmholtz, and Maxwell curl-curl) defined on bi-phase microstructures, we
propose a structure-property model that is both cost-effective and explainable.
Our method is built on top of the strong contrast expansion (SCE) formalism,
which analytically maps $N$-point correlations of an unbounded random field to
its effective properties. Since real-world material samples have finite sizes
and analytical PDE kernels are not always available, we propose Neural Contrast
Expansion (NCE), an SCE-inspired architecture to learn surrogate PDE kernels
from structure-property data. For static conduction and electromagnetic wave
propagation cases, we show that NCE models reveal accurate and insightful
sensitivity information useful for material design. Compared with other PDE
kernel learning methods, our method does not require measurements about the PDE
solution fields, but rather only requires macroscopic property measurements
that are more accessible in material development contexts.

</details>


### [197] [UM3: Unsupervised Map to Map Matching](https://arxiv.org/abs/2508.16874)
*Chaolong Ying,Yinan Zhang,Lei Zhang,Jiazhuang Wang,Shujun Jia,Tianshu Yu*

Main category: cs.LG

TL;DR: 提出了一种无监督图框架，用于地图匹配，该框架具有可扩展性，无需训练数据，在高噪声和大规模场景中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 地图到地图的匹配是跨异构源对齐空间数据的一项关键任务，但由于缺乏地面真实对应关系、稀疏节点特征和可扩展性需求，它仍然具有挑战性。

Method: 我们提出了一个基于无监督图的框架，该框架通过三个关键创新来应对这些挑战。首先，我们的方法是一种无监督学习方法，不需要训练数据，这对于获取标记训练样本具有挑战性的大规模地图数据至关重要。其次，我们引入了伪坐标，该坐标捕获每个地图中节点的相对空间布局，从而增强了特征的可区分性并实现了尺度不变学习。第三，我们设计了一种自适应平衡特征和几何相似性的机制，以及一种几何一致的损失函数，确保对嘈杂或不完整的坐标数据的鲁棒性。在实现层面，为了处理大规模地图，我们开发了一个基于瓦片的后处理管道，具有重叠区域和多数投票，这使得并行处理成为可能，同时保持了边界一致性。

Result: 在真实世界数据集上的实验表明，我们的方法在匹配任务中实现了最先进的准确性，大大超过了现有方法，尤其是在高噪声和大规模场景中。

Conclusion: 该框架为地图对齐提供了一个可扩展且实用的解决方案，为传统方法提供了一个稳健而有效的替代方案。

Abstract: Map-to-map matching is a critical task for aligning spatial data across
heterogeneous sources, yet it remains challenging due to the lack of ground
truth correspondences, sparse node features, and scalability demands. In this
paper, we propose an unsupervised graph-based framework that addresses these
challenges through three key innovations. First, our method is an unsupervised
learning approach that requires no training data, which is crucial for
large-scale map data where obtaining labeled training samples is challenging.
Second, we introduce pseudo coordinates that capture the relative spatial
layout of nodes within each map, which enhances feature discriminability and
enables scale-invariant learning. Third, we design an mechanism to adaptively
balance feature and geometric similarity, as well as a geometric-consistent
loss function, ensuring robustness to noisy or incomplete coordinate data. At
the implementation level, to handle large-scale maps, we develop a tile-based
post-processing pipeline with overlapping regions and majority voting, which
enables parallel processing while preserving boundary coherence. Experiments on
real-world datasets demonstrate that our method achieves state-of-the-art
accuracy in matching tasks, surpassing existing methods by a large margin,
particularly in high-noise and large-scale scenarios. Our framework provides a
scalable and practical solution for map alignment, offering a robust and
efficient alternative to traditional approaches.

</details>
