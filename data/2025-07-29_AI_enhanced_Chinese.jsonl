{"id": "2507.19802", "categories": ["cs.DB", "cs.DS", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.19802", "abs": "https://arxiv.org/abs/2507.19802", "authors": ["Ziyu Zhang", "Yuanhao Wei", "Joshua Engels", "Julian Shun"], "title": "CleANN: Efficient Full Dynamism in Graph-based Approximate Nearest Neighbor Search", "comment": null, "summary": "Approximate nearest neighbor search (ANNS) has become a quintessential\nalgorithmic problem for various other foundational data tasks for AI workloads.\nGraph-based ANNS indexes have superb empirical trade-offs in indexing cost,\nquery efficiency, and query approximation quality. Most existing graph-based\nindexes are designed for the static scenario, where there are no updates to the\ndata after the index is constructed. However, full dynamism (insertions,\ndeletions, and searches) is crucial to providing up-to-date responses in\napplications using vector databases. It is desirable that the index efficiently\nsupports updates and search queries concurrently. Existing dynamic graph-based\nindexes suffer from at least one of the following problems: (1) the query\nquality degrades as updates happen; and (2) the graph structure updates used to\nmaintain the index quality upon updates are global and thus expensive. To solve\nthese problems, we propose the CleANN system which consists of three main\ncomponents: (1) workload-aware linking of diverse search tree descendants to\ncombat distribution shift; (2)query-adaptive on-the-fly neighborhood\nconsolidation to efficiently handle deleted nodes; and (3) semi-lazy memory\ncleaning to clean up stale information in the data structure and reduce the\nwork spent by the first two components. We evaluate CleANN on 7 diverse\ndatasets on fully dynamic workloads and find that CleANN has query quality at\nleast as good as if the index had been built statically using the corresponding\ndata. In the in-memory setting using 56 hyper-threads, with all types of\nqueries running concurrently, at the same recall level, CleANN achieves 7-1200x\nthroughput improvement on million-scale real-world datasets. To the best of our\nknowledge, CleANN is the first concurrent ANNS index to achieve such efficiency\nwhile maintaining quality under full dynamism.", "AI": {"tldr": "CleANN is a concurrent ANNS index that achieves high efficiency and maintains quality under full dynamism by using workload-aware linking, query-adaptive neighborhood consolidation, and semi-lazy memory cleaning.", "motivation": "Existing dynamic graph-based indexes suffer from query quality degradation and expensive graph structure updates.", "method": "The CleANN system consists of three main components: (1) workload-aware linking of diverse search tree descendants to combat distribution shift; (2)query-adaptive on-the-fly neighborhood consolidation to efficiently handle deleted nodes; and (3) semi-lazy memory cleaning to clean up stale information in the data structure and reduce the work spent by the first two components.", "result": "CleANN has query quality at least as good as if the index had been built statically using the corresponding data. CleANN achieves 7-1200x throughput improvement on million-scale real-world datasets.", "conclusion": "CleANN is the first concurrent ANNS index to achieve high efficiency while maintaining quality under full dynamism."}}
{"id": "2507.20441", "categories": ["cs.DB", "cs.DS", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.20441", "abs": "https://arxiv.org/abs/2507.20441", "authors": ["Yunjie Pan", "Omkar Bhalerao", "C. Seshadhri", "Nishil Talati"], "title": "TIMEST: Temporal Information Motif Estimator Using Sampling Trees", "comment": null, "summary": "The mining of pattern subgraphs, known as motifs, is a core task in the field\nof graph mining. Edges in real-world networks often have timestamps, so there\nis a need for temporal motif mining. A temporal motif is a richer structure\nthat imposes timing constraints on the edges of the motif. Temporal motifs have\nbeen used to analyze social networks, financial transactions, and biological\nnetworks.\n  Motif counting in temporal graphs is particularly challenging. A graph with\nmillions of edges can have trillions of temporal motifs, since the same edge\ncan occur with multiple timestamps. There is a combinatorial explosion of\npossibilities, and state-of-the-art algorithms cannot manage motifs with more\nthan four vertices.\n  In this work, we present TIMEST: a general, fast, and accurate estimation\nalgorithm to count temporal motifs of arbitrary sizes in temporal networks. Our\napproach introduces a temporal spanning tree sampler that leverages weighted\nsampling to generate substructures of target temporal motifs. This method\ncarefully takes a subset of temporal constraints of the motif that can be\njointly and efficiently sampled. TIMEST uses randomized estimation techniques\nto obtain accurate estimates of motif counts.\n  We give theoretical guarantees on the running time and approximation\nguarantees of TIMEST. We perform an extensive experimental evaluation and show\nthat TIMEST is both faster and more accurate than previous algorithms. Our CPU\nimplementation exhibits an average speedup of 28x over state-of-the-art GPU\nimplementation of the exact algorithm, and 6x speedup over SOTA approximate\nalgorithms while consistently showcasing less than 5% error in most cases. For\nexample, TIMEST can count the number of instances of a financial fraud temporal\nmotif in four minutes with 0.6% error, while exact methods take more than two\ndays.", "AI": {"tldr": "TIMEST\u662f\u4e00\u79cd\u901a\u7528\u3001\u5feb\u901f\u4e14\u51c6\u786e\u7684\u4f30\u8ba1\u7b97\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u65f6\u95f4\u7f51\u7edc\u4e2d\u4efb\u610f\u5927\u5c0f\u7684\u65f6\u95f4\u4e3b\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7f51\u7edc\u4e2d\u7684\u8fb9\u901a\u5e38\u5177\u6709\u65f6\u95f4\u6233\uff0c\u56e0\u6b64\u9700\u8981\u65f6\u95f4\u4e3b\u9898\u6316\u6398\u3002\u65f6\u95f4\u4e3b\u9898\u662f\u4e00\u79cd\u66f4\u4e30\u5bcc\u7684\u7ed3\u6784\uff0c\u5b83\u5bf9\u4e3b\u9898\u7684\u8fb9\u65bd\u52a0\u65f6\u95f4\u7ea6\u675f\u3002\u65f6\u95f4\u4e3b\u9898\u5df2\u88ab\u7528\u4e8e\u5206\u6790\u793e\u4ea4\u7f51\u7edc\u3001\u91d1\u878d\u4ea4\u6613\u548c\u751f\u7269\u7f51\u7edc\u3002\u65f6\u95f4\u56fe\u4e2d\u7684\u4e3b\u9898\u8ba1\u6570\u7279\u522b\u5177\u6709\u6311\u6218\u6027\u3002\u5177\u6709\u6570\u767e\u4e07\u6761\u8fb9\u7684\u56fe\u53ef\u80fd\u5177\u6709\u6570\u4e07\u4ebf\u4e2a\u65f6\u95f4\u4e3b\u9898\uff0c\u56e0\u4e3a\u540c\u4e00\u6761\u8fb9\u53ef\u80fd\u4ee5\u591a\u4e2a\u65f6\u95f4\u6233\u51fa\u73b0\u3002\u53ef\u80fd\u6027\u5b58\u5728\u7ec4\u5408\u7206\u70b8\uff0c\u5e76\u4e14\u6700\u5148\u8fdb\u7684\u7b97\u6cd5\u65e0\u6cd5\u7ba1\u7406\u5177\u6709\u56db\u4e2a\u4ee5\u4e0a\u9876\u70b9\u7684\u4e3b\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u8de8\u8d8a\u6811\u91c7\u6837\u5668\uff0c\u8be5\u91c7\u6837\u5668\u5229\u7528\u52a0\u6743\u91c7\u6837\u6765\u751f\u6210\u76ee\u6807\u65f6\u95f4\u4e3b\u9898\u7684\u5b50\u7ed3\u6784\u3002\u8be5\u65b9\u6cd5\u4ed4\u7ec6\u5730\u9009\u53d6\u4e86\u4e3b\u9898\u7684\u4e00\u4e2a\u65f6\u95f4\u7ea6\u675f\u5b50\u96c6\uff0c\u8fd9\u4e9b\u7ea6\u675f\u53ef\u4ee5\u88ab\u8054\u5408\u548c\u6709\u6548\u5730\u91c7\u6837\u3002TIMEST\u4f7f\u7528\u968f\u673a\u4f30\u8ba1\u6280\u672f\u6765\u83b7\u5f97\u7cbe\u786e\u7684\u4e3b\u9898\u8ba1\u6570\u4f30\u8ba1\u3002", "result": "TIMEST\u7b97\u6cd5\u5728\u65f6\u95f4\u548c\u7cbe\u5ea6\u4e0a\u90fd\u4f18\u4e8e\u4ee5\u524d\u7684\u7b97\u6cd5\u3002\u6211\u4eec\u7684CPU\u5b9e\u73b0\u6bd4\u6700\u5148\u8fdb\u7684\u7cbe\u786e\u7b97\u6cd5\u7684GPU\u5b9e\u73b0\u5e73\u5747\u5feb28\u500d\uff0c\u6bd4SOTA\u8fd1\u4f3c\u7b97\u6cd5\u5feb6\u500d\uff0c\u540c\u65f6\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u59cb\u7ec8\u663e\u793a\u5c0f\u4e8e5%\u7684\u8bef\u5dee\u3002", "conclusion": "TIMEST\u7b97\u6cd5\u5728\u65f6\u95f4\u548c\u7cbe\u5ea6\u4e0a\u90fd\u4f18\u4e8e\u4ee5\u524d\u7684\u7b97\u6cd5\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u7cbe\u786e\u7b97\u6cd5\u7684GPU\u5b9e\u73b0\u76f8\u6bd4\uff0c\u6211\u4eec\u7684CPU\u5b9e\u73b0\u5e73\u5747\u52a0\u901f\u4e8628\u500d\uff0c\u4e0eSOTA\u8fd1\u4f3c\u7b97\u6cd5\u76f8\u6bd4\uff0c\u52a0\u901f\u4e866\u500d\uff0c\u540c\u65f6\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u59cb\u7ec8\u663e\u793a\u5c0f\u4e8e5%\u7684\u8bef\u5dee\u3002"}}
{"id": "2507.20671", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.20671", "abs": "https://arxiv.org/abs/2507.20671", "authors": ["Jens Dittrich"], "title": "A Functional Data Model and Query Language is All You Need", "comment": null, "summary": "We propose the vision of a functional data model (FDM) and an associated\nfunctional query language (FQL). Our proposal has far-reaching consequences: we\nshow a path to come up with a modern QL that solves (almost if not) all\nproblems of SQL (NULL-values, impedance mismatch, SQL injection, missing\nquerying capabilities for updates, etc.). FDM and FQL are much more expressive\nthan the relational model and SQL. In addition, in contrast to SQL, FQL\nintegrates smoothly into existing programming languages. In our approach both\nQL and PL become the \"same thing\", thus opening up some interesting holistic\noptimization opportunities between compilers and databases. In FQL, we also do\nnot need to force application developers to switch to unfamiliar programming\nparadigms (like SQL or datalog): developers can stick with the abstractions\nprovided by their programming language.", "AI": {"tldr": "FDM and FQL are proposed as a modern QL that solves problems of SQL, are more expressive, and integrate smoothly into existing programming languages.", "motivation": "to come up with a modern QL that solves problems of SQL (NULL-values, impedance mismatch, SQL injection, missing querying capabilities for updates, etc.)", "method": "propose the vision of a functional data model (FDM) and an associated functional query language (FQL)", "result": "FDM and FQL are much more expressive than the relational model and SQL. QL and PL become the \"same thing\", thus opening up some interesting holistic optimization opportunities between compilers and databases", "conclusion": "FQL integrates smoothly into existing programming languages and allows developers to stick with their familiar abstractions."}}
{"id": "2507.20815", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.20815", "abs": "https://arxiv.org/abs/2507.20815", "authors": ["Valerie Restat", "Kai Tejkl", "Uta St\u00f6rl"], "title": "MVIAnalyzer: A Holistic Approach to Analyze Missing Value Imputation", "comment": null, "summary": "Missing values often limit the usage of data analysis or cause falsification\nof results. Therefore, methods of missing value imputation (MVI) are of great\nsignificance. However, in general, there is no universal, fair MVI method for\ndifferent tasks. This work thus places MVI in the overall context of data\nanalysis. For this purpose, we present the MVIAnalyzer, a generic framework for\na holistic analysis of MVI. It considers the overall process up to the\napplication and analysis of machine learning methods. The associated software\nis provided and can be used by other researchers for their own analyses. To\nthis end, it further includes a missing value simulation with consideration of\nrelevant parameters. The application of the MVIAnalyzer is demonstrated on data\nwith different characteristics. An evaluation of the results shows the\npossibilities and limitations of different MVI methods. Since MVI is a very\ncomplex topic with different influencing variables, this paper additionally\nillustrates how the analysis can be supported by visualizations.", "AI": {"tldr": "This paper introduces MVIAnalyzer, a framework for analyzing missing value imputation methods, and demonstrates its application on various datasets, highlighting the possibilities and limitations of different MVI methods.", "motivation": "Missing values often limit the usage of data analysis or cause falsification of results. Therefore, methods of missing value imputation (MVI) are of great significance. However, in general, there is no universal, fair MVI method for different tasks. This work thus places MVI in the overall context of data analysis.", "method": "We present the MVIAnalyzer, a generic framework for a holistic analysis of MVI. It considers the overall process up to the application and analysis of machine learning methods. The associated software is provided and can be used by other researchers for their own analyses. To this end, it further includes a missing value simulation with consideration of relevant parameters. The application of the MVIAnalyzer is demonstrated on data with different characteristics.", "result": "An evaluation of the results shows the possibilities and limitations of different MVI methods.", "conclusion": "An evaluation of the results shows the possibilities and limitations of different MVI methods. Since MVI is a very complex topic with different influencing variables, this paper additionally illustrates how the analysis can be supported by visualizations."}}
{"id": "2507.19750", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.19750", "abs": "https://arxiv.org/abs/2507.19750", "authors": ["Yuhua Liu", "Haoxuan Wang", "Jiajia Kou", "Ling Sun", "Heyu Wang", "Yongheng Wang", "Yigang Wang", "Jinchang Lic", "Zhiguang Zhou"], "title": "A Unified Framework for Interactive Visual Graph Matching via Attribute-Structure Synchronization", "comment": null, "summary": "In traditional graph retrieval tools, graph matching is commonly used to\nretrieve desired graphs from extensive graph datasets according to their\nstructural similarities. However, in real applications, graph nodes have\nnumerous attributes which also contain valuable information for evaluating\nsimilarities between graphs. Thus, to achieve superior graph matching results,\nit is crucial for graph retrieval tools to make full use of the attribute\ninformation in addition to structural information. We propose a novel framework\nfor interactive visual graph matching. In the proposed framework, an\nattribute-structure synchronization method is developed for representing\nstructural and attribute features in a unified embedding space based on\nCanonical Correlation Analysis (CCA). To support fast and interactive matching,\n\\revise{our method} provides users with intuitive visual query interfaces for\ntraversing, filtering and searching for the target graph in the embedding space\nconveniently. With the designed interfaces, the users can also specify a new\ntarget graph with desired structural and semantic features. Besides, evaluation\nviews are designed for easy validation and interpretation of the matching\nresults. Case studies and quantitative comparisons on real-world datasets have\ndemonstrated the superiorities of our proposed framework in graph matching and\nlarge graph exploration.", "AI": {"tldr": "a novel framework for interactive visual graph matching", "motivation": "it is crucial for graph retrieval tools to make full use of the attribute information in addition to structural information.", "method": "an attribute-structure synchronization method is developed for representing structural and attribute features in a unified embedding space based on Canonical Correlation Analysis (CCA).", "result": "provides users with intuitive visual query interfaces for traversing, filtering and searching for the target graph in the embedding space conveniently. With the designed interfaces, the users can also specify a new target graph with desired structural and semantic features. Besides, evaluation views are designed for easy validation and interpretation of the matching results.", "conclusion": "Case studies and quantitative comparisons on real-world datasets have demonstrated the superiorities of our proposed framework in graph matching and large graph exploration."}}
{"id": "2507.19511", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19511", "abs": "https://arxiv.org/abs/2507.19511", "authors": ["Khalid Hasan", "Jamil Saquer", "Mukulika Ghosh"], "title": "Advancing Mental Disorder Detection: A Comparative Evaluation of Transformer and LSTM Architectures on Social Media", "comment": "The 49th IEEE International Conference on Computers, Software, and\n  Applications (COMPSAC 2025) (camera-ready)", "summary": "The rising prevalence of mental health disorders necessitates the development\nof robust, automated tools for early detection and monitoring. Recent advances\nin Natural Language Processing (NLP), particularly transformer-based\narchitectures, have demonstrated significant potential in text analysis. This\nstudy provides a comprehensive evaluation of state-of-the-art transformer\nmodels (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against Long Short-Term\nMemory (LSTM) based approaches using different text embedding techniques for\nmental health disorder classification on Reddit. We construct a large annotated\ndataset, validating its reliability through statistical judgmental analysis and\ntopic modeling. Experimental results demonstrate the superior performance of\ntransformer models over traditional deep-learning approaches. RoBERTa achieved\nthe highest classification performance, with a 99.54% F1 score on the hold-out\ntest set and a 96.05% F1 score on the external test set. Notably, LSTM models\naugmented with BERT embeddings proved highly competitive, achieving F1 scores\nexceeding 94% on the external dataset while requiring significantly fewer\ncomputational resources. These findings highlight the effectiveness of\ntransformer-based models for real-time, scalable mental health monitoring. We\ndiscuss the implications for clinical applications and digital mental health\ninterventions, offering insights into the capabilities and limitations of\nstate-of-the-art NLP methodologies in mental disorder detection.", "AI": {"tldr": "Transformer models excel in mental health disorder classification, with RoBERTa leading, but LSTM with BERT embeddings provides a good balance of performance and computational efficiency.", "motivation": "The rising prevalence of mental health disorders necessitates automated tools for early detection and monitoring.", "method": "Comprehensive evaluation of transformer models (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against LSTM using different text embedding techniques on a large annotated Reddit dataset.", "result": "RoBERTa achieved the highest classification performance (99.54% F1 score on the hold-out test set and 96.05% F1 score on the external test set), while LSTM models with BERT embeddings achieved F1 scores exceeding 94% on the external dataset.", "conclusion": "Transformer-based models, especially RoBERTa, are effective for mental health monitoring, but LSTM models with BERT embeddings offer a competitive, less computationally intensive alternative."}}
{"id": "2507.19489", "categories": ["cs.AI", "cs.CV", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.19489", "abs": "https://arxiv.org/abs/2507.19489", "authors": ["Simone Bendazzoli", "Sanna Persson", "Mehdi Astaraki", "Sebastian Pettersson", "Vitali Grozman", "Rodrigo Moreno"], "title": "MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation", "comment": "26 pages, 12 figures", "summary": "The integration of Artificial Intelligence (AI) into clinical workflows\nrequires robust collaborative platforms that are able to bridge the gap between\ntechnical innovation and practical healthcare applications. This paper\nintroduces MAIA (Medical Artificial Intelligence Assistant), an open-source\nplatform designed to facilitate interdisciplinary collaboration among\nclinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a\nmodular, scalable environment with integrated tools for data management, model\ndevelopment, annotation, deployment, and clinical feedback. Key features\ninclude project isolation, CI/CD automation, integration with high-computing\ninfrastructures and in clinical workflows. MAIA supports real-world use cases\nin medical imaging AI, with deployments in both academic and clinical\nenvironments. By promoting collaborations and interoperability, MAIA aims to\naccelerate the translation of AI research into impactful clinical solutions\nwhile promoting reproducibility, transparency, and user-centered design. We\nshowcase the use of MAIA with different projects, both at KTH Royal Institute\nof Technology and Karolinska University Hospital.", "AI": {"tldr": "MAIA \u662f\u4e00\u4e2a\u5f00\u6e90\u5e73\u53f0\uff0c\u65e8\u5728\u4fc3\u8fdb\u4e34\u5e8a\u533b\u751f\u3001\u7814\u7a76\u4eba\u5458\u548c AI \u5f00\u53d1\u4eba\u5458\u4e4b\u95f4\u7684\u8de8\u5b66\u79d1\u534f\u4f5c\uff0c\u52a0\u901f\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u8f6c\u5316\u4e3a\u6709\u5f71\u54cd\u529b\u7684\u4e34\u5e8a\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd (AI) \u96c6\u6210\u5230\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u9700\u8981\u5f3a\u5927\u7684\u534f\u4f5c\u5e73\u53f0\uff0c\u8fd9\u4e9b\u5e73\u53f0\u80fd\u591f\u5f25\u5408\u6280\u672f\u521b\u65b0\u548c\u5b9e\u9645\u533b\u7597\u4fdd\u5065\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u4ecb\u7ecd\u4e86 MAIA\uff08\u533b\u7597\u4eba\u5de5\u667a\u80fd\u52a9\u624b\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u65e8\u5728\u4fc3\u8fdb\u4e34\u5e8a\u533b\u751f\u3001\u7814\u7a76\u4eba\u5458\u548c\u4eba\u5de5\u667a\u80fd\u5f00\u53d1\u4eba\u5458\u4e4b\u95f4\u8de8\u5b66\u79d1\u534f\u4f5c\u7684\u5f00\u6e90\u5e73\u53f0\u3002MAIA \u6784\u5efa\u4e8e Kubernetes \u4e4b\u4e0a\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684\u73af\u5883\uff0c\u5176\u4e2d\u96c6\u6210\u4e86\u7528\u4e8e\u6570\u636e\u7ba1\u7406\u3001\u6a21\u578b\u5f00\u53d1\u3001\u6ce8\u91ca\u3001\u90e8\u7f72\u548c\u4e34\u5e8a\u53cd\u9988\u7684\u5de5\u5177\u3002", "result": "MAIA \u5177\u6709\u9879\u76ee\u9694\u79bb\u3001CI/CD \u81ea\u52a8\u5316\u3001\u4e0e\u9ad8\u6027\u80fd\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u548c\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u96c6\u6210\u7684\u5173\u952e\u7279\u6027\u3002MAIA \u652f\u6301\u533b\u5b66\u5f71\u50cf\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u5b9e\u9645\u7528\u4f8b\uff0c\u5e76\u5728\u5b66\u672f\u548c\u4e34\u5e8a\u73af\u5883\u4e2d\u8fdb\u884c\u90e8\u7f72\u3002", "conclusion": "MAIA\u901a\u8fc7\u4fc3\u8fdb\u534f\u4f5c\u548c\u4e92\u64cd\u4f5c\u6027\uff0c\u65e8\u5728\u52a0\u901f\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u8f6c\u5316\u4e3a\u6709\u5f71\u54cd\u529b\u7684\u4e34\u5e8a\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\u3001\u900f\u660e\u5ea6\u548c\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u3002\u6211\u4eec\u5728 KTH \u7687\u5bb6\u7406\u5de5\u5b66\u9662\u548c Karolinska \u5927\u5b66\u533b\u9662\u7684\u4e0d\u540c\u9879\u76ee\u4e2d\u5c55\u793a\u4e86 MAIA \u7684\u4f7f\u7528\u3002"}}
{"id": "2507.19574", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19574", "abs": "https://arxiv.org/abs/2507.19574", "authors": ["Ghufran Abualhail Alhamzawi", "Ali Saeed Alfoudi", "Ali Hakem Alsaeedi", "Suha Mohammed Hadi", "Amjed Abbas Ahmed", "Md. Riad Hassan", "Nurhizam Safie Mohd Satar", "Waeel Yahya Yasseen"], "title": "Tuning adaptive gamma correction (TAGC) for enhancing images in low ligh", "comment": null, "summary": "Enhancing images in low-light conditions is an important challenge in\ncomputer vision. Insufficient illumination negatively affects the quality of\nimages, resulting in low contrast, intensive noise, and blurred details. This\npaper presents a model for enhancing low-light images called tuning adaptive\ngamma correction (TAGC). The model is based on analyzing the color luminance of\nthe low-light image and calculating the average color to determine the adaptive\ngamma coefficient. The gamma value is calculated automatically and adaptively\nat different illumination levels suitable for the image without human\nintervention or manual adjustment. Based on qualitative and quantitative\nevaluation, tuning adaptive gamma correction model has effectively improved\nlow-light images while maintaining details, natural contrast, and correct color\ndistribution. It also provides natural visual quality. It can be considered a\nmore efficient solution for processing low-light images in multiple\napplications such as night surveillance, improving the quality of medical\nimages, and photography in low-light environments.", "AI": {"tldr": "This paper presents a tuning adaptive gamma correction (TAGC) model for enhancing low-light images, which calculates the adaptive gamma coefficient automatically. The model effectively improves low-light images while maintaining details, natural contrast, and correct color distribution.", "motivation": "Enhancing images in low-light conditions is an important challenge in computer vision. Insufficient illumination negatively affects the quality of images, resulting in low contrast, intensive noise, and blurred details.", "method": "tuning adaptive gamma correction (TAGC). The model is based on analyzing the color luminance of the low-light image and calculating the average color to determine the adaptive gamma coefficient. The gamma value is calculated automatically and adaptively at different illumination levels suitable for the image without human intervention or manual adjustment.", "result": "tuning adaptive gamma correction model has effectively improved low-light images while maintaining details, natural contrast, and correct color distribution. It also provides natural visual quality.", "conclusion": "tuning adaptive gamma correction model has effectively improved low-light images while maintaining details, natural contrast, and correct color distribution. It also provides natural visual quality. It can be considered a more efficient solution for processing low-light images in multiple applications such as night surveillance, improving the quality of medical images, and photography in low-light environments."}}
{"id": "2507.19510", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19510", "abs": "https://arxiv.org/abs/2507.19510", "authors": ["Haoxuan Ma", "Xishun Liao", "Yifan Liu", "Chris Stanford", "Jiaqi Ma"], "title": "Beyond 9-to-5: A Generative Model for Augmenting Mobility Data of Underrepresented Shift Workers", "comment": null, "summary": "This paper addresses a critical gap in urban mobility modeling by focusing on\nshift workers, a population segment comprising 15-20% of the workforce in\nindustrialized societies yet systematically underrepresented in traditional\ntransportation surveys and planning. This underrepresentation is revealed in\nthis study by a comparative analysis of GPS and survey data, highlighting stark\ndifferences between the bimodal temporal patterns of shift workers and the\nconventional 9-to-5 schedules recorded in surveys. To address this bias, we\nintroduce a novel transformer-based approach that leverages fragmented GPS\ntrajectory data to generate complete, behaviorally valid activity patterns for\nindividuals working non-standard hours. Our method employs periodaware temporal\nembeddings and a transition-focused loss function specifically designed to\ncapture the unique activity rhythms of shift workers and mitigate the inherent\nbiases in conventional transportation datasets. Evaluation shows that the\ngenerated data achieves remarkable distributional alignment with GPS data from\nLos Angeles County (Average JSD < 0.02 for all evaluation metrics). By\ntransforming incomplete GPS traces into complete, representative activity\npatterns, our approach provides transportation planners with a powerful data\naugmentation tool to fill critical gaps in understanding the 24/7 mobility\nneeds of urban populations, enabling precise and inclusive transportation\nplanning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8etransformer\u7684\u65b9\u6cd5\uff0c\u5229\u7528GPS\u6570\u636e\u751f\u6210\u8f6e\u73ed\u5de5\u4eba\u7684\u5b8c\u6574\u6d3b\u52a8\u6a21\u5f0f\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u4ea4\u901a\u6570\u636e\u96c6\u4e2d\u5bf9\u4ed6\u4eec\u7684\u4f4e\u4f30\u95ee\u9898\uff0c\u4ece\u800c\u5b9e\u73b0\u7cbe\u786e\u548c\u5305\u5bb9\u7684\u4ea4\u901a\u89c4\u5212\u3002", "motivation": "\u672c\u6587\u5173\u6ce8\u57ce\u5e02\u4ea4\u901a\u5efa\u6a21\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u5dee\u8ddd\uff0c\u5373\u8f6e\u73ed\u5de5\u4eba\u3002\u8f6e\u73ed\u5de5\u4eba\u5360\u5de5\u4e1a\u5316\u793e\u4f1a\u52b3\u52a8\u529b\u768415-20%\uff0c\u4f46\u5728\u4f20\u7edf\u7684\u4ea4\u901a\u8c03\u67e5\u548c\u89c4\u5212\u4e2d\u5374\u88ab\u7cfb\u7edf\u6027\u5730\u4f4e\u4f30\u4e86\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8etransformer\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u788e\u7247\u5316\u7684GPS\u8f68\u8ff9\u6570\u636e\u6765\u751f\u6210\u884c\u4e3a\u4e0a\u6709\u6548\u7684\u975e\u6807\u51c6\u5de5\u4f5c\u65f6\u95f4\u4e2a\u4f53\u7684\u5b8c\u6574\u6d3b\u52a8\u6a21\u5f0f\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\u4e86\u5468\u671f\u611f\u77e5\u65f6\u95f4\u5d4c\u5165\u548c\u4ee5\u8f6c\u6362\u4e3a\u4e2d\u5fc3\u7684\u635f\u5931\u51fd\u6570\uff0c\u4e13\u95e8\u7528\u4e8e\u6355\u83b7\u8f6e\u73ed\u5de5\u4eba\u7684\u72ec\u7279\u6d3b\u52a8\u8282\u594f\uff0c\u5e76\u51cf\u8f7b\u4f20\u7edf\u4ea4\u901a\u6570\u636e\u96c6\u4e2d\u56fa\u6709\u7684\u504f\u5dee\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u751f\u6210\u7684\u6570\u636e\u4e0e\u6d1b\u6749\u77f6\u53bf\u7684GPS\u6570\u636e\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5206\u5e03\u5bf9\u9f50\uff08\u6240\u6709\u8bc4\u4f30\u6307\u6807\u7684\u5e73\u5747JSD < 0.02\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5c06\u4e0d\u5b8c\u6574\u7684GPS\u8f68\u8ff9\u8f6c\u6362\u4e3a\u5b8c\u6574\u7684\u3001\u6709\u4ee3\u8868\u6027\u7684\u6d3b\u52a8\u6a21\u5f0f\uff0c\u4e3a\u4ea4\u901a\u89c4\u5212\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6570\u636e\u589e\u5f3a\u5de5\u5177\uff0c\u4ee5\u586b\u8865\u5728\u7406\u89e3\u57ce\u5e02\u4eba\u53e3\u5168\u5929\u5019\u51fa\u884c\u9700\u6c42\u65b9\u9762\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4ece\u800c\u5b9e\u73b0\u7cbe\u786e\u548c\u5305\u5bb9\u7684\u4ea4\u901a\u89c4\u5212\u3002"}}
