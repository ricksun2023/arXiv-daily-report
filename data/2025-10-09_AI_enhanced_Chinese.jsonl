{"id": "2510.06657", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.06657", "abs": "https://arxiv.org/abs/2510.06657", "authors": ["Boyuan Long", "Yueqi Wang", "Hiloni Mehta", "Mick Zomnir", "Omkar Pathak", "Changping Meng", "Ruolin Jia", "Yajun Peng", "Dapeng Hong", "Xia Wu", "Mingyan Gao", "Onkar Dalal", "Ningren Han"], "title": "LLM-Powered Nuanced Video Attribute Annotation for Enhanced Recommendations", "comment": "RecSys 2025 Industry Track", "summary": "This paper presents a case study on deploying Large Language Models (LLMs) as\nan advanced \"annotation\" mechanism to achieve nuanced content understanding\n(e.g., discerning content \"vibe\") at scale within a large-scale industrial\nshort-form video recommendation system. Traditional machine learning\nclassifiers for content understanding face protracted development cycles and a\nlack of deep, nuanced comprehension. The \"LLM-as-annotators\" approach addresses\nthese by significantly shortening development times and enabling the annotation\nof subtle attributes. This work details an end-to-end workflow encompassing:\n(1) iterative definition and robust evaluation of target attributes, refined by\noffline metrics and online A/B testing; (2) scalable offline bulk annotation of\nvideo corpora using LLMs with multimodal features, optimized inference, and\nknowledge distillation for broad application; and (3) integration of these rich\nannotations into the online recommendation serving system, for example, through\npersonalized restrict retrieval. Experimental results demonstrate the efficacy\nof this approach, with LLMs outperforming human raters in offline annotation\nquality for nuanced attributes and yielding significant improvements of user\nparticipation and satisfied consumption in online A/B tests. The study provides\ninsights into designing and scaling production-level LLM pipelines for rich\ncontent evaluation, highlighting the adaptability and benefits of LLM-generated\nnuanced understanding for enhancing content discovery, user satisfaction, and\nthe overall effectiveness of modern recommendation systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u9ad8\u7ea7\u201c\u6ce8\u91ca\u201d\u673a\u5236\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u4ee5\u5728\u5927\u578b\u5de5\u4e1a\u77ed\u89c6\u9891\u63a8\u8350\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u7ec6\u81f4\u7684\u5185\u5bb9\u7406\u89e3\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u5728\u5185\u5bb9\u7406\u89e3\u65b9\u9762\u9762\u4e34\u6f2b\u957f\u7684\u5f00\u53d1\u5468\u671f\u548c\u7f3a\u4e4f\u6df1\u5165\u3001\u7ec6\u81f4\u7684\u7406\u89e3\u7684\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u63d0\u51fa\u4e86\u201cLLM-as-annotators\u201d\u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u542b\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5de5\u4f5c\u6d41\u7a0b\uff1a(1) \u8fed\u4ee3\u5b9a\u4e49\u548c\u7a33\u5065\u8bc4\u4f30\u76ee\u6807\u5c5e\u6027\uff0c\u901a\u8fc7\u79bb\u7ebf\u6307\u6807\u548c\u5728\u7ebf A/B \u6d4b\u8bd5\u8fdb\u884c\u6539\u8fdb\uff1b(2) \u4f7f\u7528\u5177\u6709\u591a\u6a21\u6001\u7279\u5f81\u7684 LLM \u5bf9\u89c6\u9891\u8bed\u6599\u5e93\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u79bb\u7ebf\u6279\u91cf\u6ce8\u91ca\uff0c\u4f18\u5316\u63a8\u7406\u548c\u77e5\u8bc6\u63d0\u70bc\u4ee5\u5b9e\u73b0\u5e7f\u6cdb\u5e94\u7528\uff1b(3) \u5c06\u8fd9\u4e9b\u4e30\u5bcc\u7684\u6ce8\u91ca\u96c6\u6210\u5230\u5728\u7ebf\u63a8\u8350\u670d\u52a1\u7cfb\u7edf\u4e2d\uff0c\u4f8b\u5982\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u9650\u5236\u68c0\u7d22\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLLM \u5728\u79bb\u7ebf\u6ce8\u91ca\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u4eba\u5de5\u8bc4\u4f30\u8005\uff0c\u5e76\u5728\u5728\u7ebf A/B \u6d4b\u8bd5\u4e2d\u663e\u7740\u63d0\u9ad8\u4e86\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u6ee1\u610f\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u6df1\u5165\u4e86\u89e3\u4e86\u5982\u4f55\u8bbe\u8ba1\u548c\u6269\u5c55\u7528\u4e8e\u4e30\u5bcc\u5185\u5bb9\u8bc4\u4f30\u7684\u751f\u4ea7\u7ea7 LLM \u7ba1\u9053\uff0c\u5f3a\u8c03\u4e86 LLM \u751f\u6210\u7684\u7ec6\u81f4\u7406\u89e3\u5728\u589e\u5f3a\u5185\u5bb9\u53d1\u73b0\u3001\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u73b0\u4ee3\u63a8\u8350\u7cfb\u7edf\u6574\u4f53\u6709\u6548\u6027\u65b9\u9762\u7684\u9002\u5e94\u6027\u548c\u4f18\u52bf\u3002"}}
{"id": "2510.06658", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.06658", "abs": "https://arxiv.org/abs/2510.06658", "authors": ["Jiaman He", "Zikang Leng", "Dana McKay", "Damiano Spina", "Johanne R. Trippas"], "title": "Can We Hide Machines in the Crowd? Quantifying Equivalence in LLM-in-the-loop Annotation Tasks", "comment": "Accepted at SIGIR-AP 2025", "summary": "Many evaluations of large language models (LLMs) in text annotation focus\nprimarily on the correctness of the output, typically comparing model-generated\nlabels to human-annotated ``ground truth'' using standard performance metrics.\nIn contrast, our study moves beyond effectiveness alone. We aim to explore how\nlabeling decisions -- by both humans and LLMs -- can be statistically evaluated\nacross individuals. Rather than treating LLMs purely as annotation systems, we\napproach LLMs as an alternative annotation mechanism that may be capable of\nmimicking the subjective judgments made by humans. To assess this, we develop a\nstatistical evaluation method based on Krippendorff's $\\alpha$, paired\nbootstrapping, and the Two One-Sided t-Tests (TOST) equivalence test procedure.\nThis evaluation method tests whether an LLM can blend into a group of human\nannotators without being distinguishable.\n  We apply this approach to two datasets -- MovieLens 100K and PolitiFact --\nand find that the LLM is statistically indistinguishable from a human annotator\nin the former ($p = 0.004$), but not in the latter ($p = 0.155$), highlighting\ntask-dependent differences. It also enables early evaluation on a small sample\nof human data to inform whether LLMs are suitable for large-scale annotation in\na given application.", "AI": {"tldr": "\u672c\u7814\u7a76\u4e0d\u4ec5\u4ec5\u5173\u6ce8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6587\u672c\u6807\u6ce8\u4e2d\u7684\u6b63\u786e\u6027\uff0c\u66f4\u5173\u6ce8LLM\u662f\u5426\u80fd\u50cf\u4eba\u7c7b\u4e00\u6837\u505a\u51fa\u4e3b\u89c2\u5224\u65ad\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u8ba1\u8bc4\u4f30\u65b9\u6cd5\u6765\u5224\u65adLLM\u662f\u5426\u80fd\u878d\u5165\u4eba\u7c7b\u6807\u6ce8\u8005\u7fa4\u4f53\u3002", "motivation": "\u4ee5\u5f80\u7684\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8LLM\u8f93\u51fa\u7684\u6b63\u786e\u6027\uff0c\u7f3a\u4e4f\u5bf9LLM\u4e3b\u89c2\u5224\u65ad\u80fd\u529b\u7684\u8bc4\u4f30\u3002", "method": "\u57fa\u4e8eKrippendorff's alpha\u3001\u914d\u5bf9bootstrap\u548cTOST\u7b49\u7edf\u8ba1\u65b9\u6cd5\uff0c\u8bc4\u4f30LLM\u662f\u5426\u80fd\u878d\u5165\u4eba\u7c7b\u6807\u6ce8\u8005\u7fa4\u4f53\u3002", "result": "\u5728MovieLens 100K\u6570\u636e\u96c6\u4e0a\uff0cLLM\u4e0e\u4eba\u7c7b\u6807\u6ce8\u8005\u5728\u7edf\u8ba1\u4e0a\u65e0\u6cd5\u533a\u5206\uff08p=0.004\uff09\uff0c\u4f46\u5728PolitiFact\u6570\u636e\u96c6\u4e0a\u5219\u4e0d\u7136\uff08p=0.155\uff09\u3002", "conclusion": "LLM\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u5b58\u5728\u5dee\u5f02\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u5c11\u91cf\u4eba\u5de5\u6570\u636e\u8bc4\u4f30LLM\u662f\u5426\u9002\u5408\u5927\u89c4\u6a21\u6807\u6ce8\u3002"}}
{"id": "2510.06728", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.06728", "abs": "https://arxiv.org/abs/2510.06728", "authors": ["Cile van Marken", "Roxana Petcu"], "title": "Reproducing and Extending Causal Insights Into Term Frequency Computation in Neural Rankers", "comment": "10 pages, 6 figures, submitted to SIGIR-AP", "summary": "Neural ranking models have shown outstanding performance across a variety of\ntasks, such as document retrieval, re-ranking, question answering and\nconversational retrieval. However, the inner decision process of these models\nremains largely unclear, especially as models increase in size. Most\ninterpretability approaches, such as probing, focus on correlational insights\nrather than establishing causal relationships. The paper 'Axiomatic Causal\nInterventions for Reverse Engineering Relevance Computation in Neural Retrieval\nModels' by Chen et al. addresses this gap by introducing a framework for\nactivation patching - a causal interpretability method - in the information\nretrieval domain, offering insights into how neural retrieval models compute\ndocument relevance. The study demonstrates that neural ranking models not only\ncapture term-frequency information, but also that these representations can be\nlocalized to specific components of the model, such as individual attention\nheads or layers. This paper aims to reproduce the findings by Chen et al. and\nto further explore the presence of pre-defined retrieval axioms in neural IR\nmodels. We validate the main claims made by Chen et al., and extend the\nframework to include an additional term-frequency axiom, which states that the\nimpact of increasing query term frequency on document ranking diminishes as the\nfrequency becomes higher. We successfully identify a group of attention heads\nthat encode this axiom and analyze their behavior to give insight into the\ninner decision-making process of neural ranking models.", "AI": {"tldr": "\u672c\u6587\u91cd\u73b0\u4e86 Chen \u7b49\u4eba\u7684\u7814\u7a76\u6210\u679c\uff0c\u5e76\u8fdb\u4e00\u6b65\u63a2\u7d22\u4e86\u795e\u7ecf IR \u6a21\u578b\u4e2d\u9884\u5b9a\u4e49\u7684\u68c0\u7d22\u539f\u7406\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u6392\u5e8f\u6a21\u578b\u5185\u90e8\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u660e\u786e\uff0c\u7f3a\u4e4f\u56e0\u679c\u5173\u7cfb\u89e3\u91ca\u3002", "method": "\u91c7\u7528\u6fc0\u6d3b\u4fee\u8865\u7684\u56e0\u679c\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u6269\u5c55\u6846\u67b6\u4ee5\u5305\u542b\u989d\u5916\u7684\u8bcd\u9891\u539f\u7406\u3002", "result": "\u9a8c\u8bc1\u4e86 Chen \u7b49\u4eba\u7684\u4e3b\u8981\u7ed3\u8bba\uff0c\u5e76\u6210\u529f\u8bc6\u522b\u51fa\u4e00\u7ec4\u7f16\u7801\u8bcd\u9891\u539f\u7406\u7684\u6ce8\u610f\u529b\u5934\u3002", "conclusion": "\u795e\u7ecf\u6392\u5e8f\u6a21\u578b\u4e0d\u4ec5\u6355\u83b7\u8bcd\u9891\u4fe1\u606f\uff0c\u800c\u4e14\u8fd9\u4e9b\u8868\u793a\u53ef\u4ee5\u5b9a\u4f4d\u5230\u6a21\u578b\u7684\u7279\u5b9a\u7ec4\u4ef6\u3002"}}
{"id": "2510.06838", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06838", "abs": "https://arxiv.org/abs/2510.06838", "authors": ["Elena Senger", "Yuri Campbell", "Rob van der Goot", "Barbara Plank"], "title": "Crossing Domains without Labels: Distant Supervision for Term Extraction", "comment": "Accepted at EMNLP Industry Track 2025", "summary": "Automatic Term Extraction (ATE) is a critical component in downstream NLP\ntasks such as document tagging, ontology construction and patent analysis.\nCurrent state-of-the-art methods require expensive human annotation and\nstruggle with domain transfer, limiting their practical deployment. This\nhighlights the need for more robust, scalable solutions and realistic\nevaluation settings. To address this, we introduce a comprehensive benchmark\nspanning seven diverse domains, enabling performance evaluation at both the\ndocument- and corpus-levels. Furthermore, we propose a robust LLM-based model\nthat outperforms both supervised cross-domain encoder models and few-shot\nlearning baselines and performs competitively with its GPT-4o teacher on this\nbenchmark. The first step of our approach is generating psuedo-labels with this\nblack-box LLM on general and scientific domains to ensure generalizability.\nBuilding on this data, we fine-tune the first LLMs for ATE. To further enhance\ndocument-level consistency, oftentimes needed for downstream tasks, we\nintroduce lightweight post-hoc heuristics. Our approach exceeds previous\napproaches on 5/7 domains with an average improvement of 10 percentage points.\nWe release our dataset and fine-tuned models to support future research in this\narea.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u672f\u8bed\u63d0\u53d6(ATE)\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u591a\u4e2a\u9886\u57df\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dATE\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\uff0c\u5e76\u4e14\u96be\u4ee5\u8fdb\u884c\u9886\u57df\u8fc1\u79fb\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u4f7f\u7528LLM\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u7136\u540e\u5728\u8fd9\u4e9b\u6570\u636e\u4e0a\u5fae\u8c03LLM\u3002\u4e3a\u4e86\u63d0\u9ad8\u6587\u6863\u7ea7\u522b\u7684\u4e00\u81f4\u6027\uff0c\u5f15\u5165\u4e86\u8f7b\u91cf\u7ea7\u7684\u4e8b\u540e\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u57287\u4e2a\u9886\u57df\u4e2d\u76845\u4e2a\u9886\u57df\u4e0a\u8d85\u8fc7\u4e86\u4ee5\u524d\u7684\u65b9\u6cd5\uff0c\u5e73\u5747\u63d0\u9ad8\u4e8610\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u672c\u6587\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u548c\u5fae\u8c03\u6a21\u578b\uff0c\u4ee5\u652f\u6301\u8be5\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2510.06414", "categories": ["cs.DB", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06414", "abs": "https://arxiv.org/abs/2510.06414", "authors": ["Abdur Rehman Anwar Qureshi", "Adrian Rebmann", "Timotheus Kampik", "Matthias Weidlich", "Mathias Weske"], "title": "Bridging Imperative Process Models and Process Data Queries-Translation and Relaxation", "comment": null, "summary": "Business process management is increasingly practiced using data-driven\napproaches. Still, classical imperative process models, which are typically\nformalized using Petri nets, are not straightforwardly applicable to the\nrelational databases that contain much of the available structured process\nexecution data. This creates a gap between the traditional world of process\nmodeling and recent developments around data-driven process analysis,\nultimately leading to the under-utilization of often readily available process\nmodels. In this paper, we close this gap by providing an approach for\ntranslating imperative models into relaxed process data queries, specifically\nSQL queries executable on relational databases, for conformance checking. Our\nresults show the continued relevance of imperative process models to\ndata-driven process management, as well as the importance of behavioral\nfootprints and other declarative approaches for integrating model-based and\ndata-driven process management.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u547d\u4ee4\u5f0f\u6a21\u578b\u8f6c\u6362\u4e3a\u677e\u5f1b\u8fc7\u7a0b\u6570\u636e\u67e5\u8be2\u7684\u65b9\u6cd5\uff0c\u4ee5\u5f25\u5408\u4f20\u7edf\u8fc7\u7a0b\u5efa\u6a21\u4e0e\u6570\u636e\u9a71\u52a8\u8fc7\u7a0b\u5206\u6790\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u4f20\u7edf\u7684\u8fc7\u7a0b\u6a21\u578b\uff08\u901a\u5e38\u4f7f\u7528 Petri \u7f51\u5f62\u5f0f\u5316\uff09\u4e0d\u80fd\u76f4\u63a5\u5e94\u7528\u4e8e\u5305\u542b\u5927\u91cf\u53ef\u7528\u7ed3\u6784\u5316\u8fc7\u7a0b\u6267\u884c\u6570\u636e\u7684\u5173\u7cfb\u6570\u636e\u5e93\uff0c\u5bfc\u81f4\u8fc7\u7a0b\u6a21\u578b\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "\u5c06\u547d\u4ee4\u5f0f\u6a21\u578b\u8f6c\u6362\u4e3a\u677e\u5f1b\u8fc7\u7a0b\u6570\u636e\u67e5\u8be2\uff0c\u7279\u522b\u662f\u53ef\u5728\u5173\u7cfb\u6570\u636e\u5e93\u4e0a\u6267\u884c\u7684 SQL \u67e5\u8be2\uff0c\u4ee5\u8fdb\u884c\u4e00\u81f4\u6027\u68c0\u67e5\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u547d\u4ee4\u5f0f\u8fc7\u7a0b\u6a21\u578b\u5bf9\u6570\u636e\u9a71\u52a8\u7684\u8fc7\u7a0b\u7ba1\u7406\u4ecd\u7136\u5177\u6709\u76f8\u5173\u6027\uff0c\u5e76\u4e14\u884c\u4e3a\u8db3\u8ff9\u548c\u5176\u4ed6\u58f0\u660e\u5f0f\u65b9\u6cd5\u5bf9\u4e8e\u96c6\u6210\u57fa\u4e8e\u6a21\u578b\u548c\u6570\u636e\u9a71\u52a8\u7684\u8fc7\u7a0b\u7ba1\u7406\u975e\u5e38\u91cd\u8981\u3002", "conclusion": "\u672c\u6587\u5f25\u5408\u4e86\u4f20\u7edf\u8fc7\u7a0b\u5efa\u6a21\u4e0e\u6570\u636e\u9a71\u52a8\u8fc7\u7a0b\u5206\u6790\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5f3a\u8c03\u4e86\u547d\u4ee4\u5f0f\u6a21\u578b\u548c\u58f0\u660e\u5f0f\u65b9\u6cd5\u5728\u6570\u636e\u9a71\u52a8\u8fc7\u7a0b\u7ba1\u7406\u4e2d\u7684\u6301\u7eed\u76f8\u5173\u6027\u548c\u91cd\u8981\u6027\u3002"}}
{"id": "2510.06229", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06229", "abs": "https://arxiv.org/abs/2510.06229", "authors": ["Josh Hunter", "John McDermid", "Simon Burton", "Poppy Fynes", "Mia Dempster"], "title": "Milestone Determination for Autonomous Railway Operation", "comment": "Paper submitted and partially accepted to ICART 2025, paper is 8\n  pages and has 1 figure, 2 tables", "summary": "In the field of railway automation, one of the key challenges has been the\ndevelopment of effective computer vision systems due to the limited\navailability of high-quality, sequential data. Traditional datasets are\nrestricted in scope, lacking the spatio temporal context necessary for\nreal-time decision-making, while alternative solutions introduce issues related\nto realism and applicability. By focusing on route-specific, contextually\nrelevant cues, we can generate rich, sequential datasets that align more\nclosely with real-world operational logic. The concept of milestone\ndetermination allows for the development of targeted, rule-based models that\nsimplify the learning process by eliminating the need for generalized\nrecognition of dynamic components, focusing instead on the critical decision\npoints along a route. We argue that this approach provides a practical\nframework for training vision agents in controlled, predictable environments,\nfacilitating safer and more efficient machine learning systems for railway\nautomation.", "AI": {"tldr": "\u4f20\u7edf\u94c1\u8def\u81ea\u52a8\u5316\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u65f6\u5e8f\u6570\u636e\u7684\u7a00\u7f3a\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5b9a\u8def\u7ebf\u3001\u5173\u8054\u4e0a\u4e0b\u6587\u7ebf\u7d22\u751f\u6210\u4e30\u5bcc\u65f6\u5e8f\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u91cc\u7a0b\u7891\u786e\u5b9a\u7b80\u5316\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4ece\u800c\u5728\u53ef\u63a7\u73af\u5883\u4e2d\u8bad\u7ec3\u89c6\u89c9\u4ee3\u7406\uff0c\u63d0\u9ad8\u94c1\u8def\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u9ad8\u8d28\u91cf\u65f6\u5e8f\u6570\u636e\u5728\u94c1\u8def\u81ea\u52a8\u5316\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u4e2d\u7684\u7a00\u7f3a\u6027\u662f\u7814\u7a76\u52a8\u673a\u3002", "method": "\u901a\u8fc7\u5173\u6ce8\u7279\u5b9a\u8def\u7ebf\u3001\u5173\u8054\u4e0a\u4e0b\u6587\u7684\u7ebf\u7d22\uff0c\u751f\u6210\u4e30\u5bcc\u7684\u65f6\u5e8f\u6570\u636e\u96c6\u3002\u5229\u7528\u91cc\u7a0b\u7891\u786e\u5b9a\uff0c\u5f00\u53d1\u6709\u9488\u5bf9\u6027\u7684\u3001\u57fa\u4e8e\u89c4\u5219\u7684\u6a21\u578b\uff0c\u4ece\u800c\u7b80\u5316\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5728\u53ef\u63a7\u73af\u5883\u4e2d\u8bad\u7ec3\u89c6\u89c9\u4ee3\u7406\uff0c\u63d0\u9ad8\u94c1\u8def\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u5728\u53ef\u63a7\u3001\u53ef\u9884\u6d4b\u7684\u73af\u5883\u4e2d\u8bad\u7ec3\u89c6\u89c9\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u6846\u67b6\uff0c\u4ece\u800c\u4fc3\u8fdb\u66f4\u5b89\u5168\u3001\u66f4\u9ad8\u6548\u7684\u94c1\u8def\u81ea\u52a8\u5316\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u3002"}}
{"id": "2510.06261", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06261", "abs": "https://arxiv.org/abs/2510.06261", "authors": ["Zhanke Zhou", "Chentao Cao", "Xiao Feng", "Xuan Li", "Zongze Li", "Xiangyu Lu", "Jiangchao Yao", "Weikai Huang", "Linrui Xu", "Tian Cheng", "Guanyu Jiang", "Yiming Zheng", "Brando Miranda", "Tongliang Liu", "Sanmi Koyejo", "Masashi Sugiyama", "Bo Han"], "title": "AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning", "comment": "Ongoing project", "summary": "We present AlphaApollo, a self-evolving agentic reasoning system that aims to\naddress two bottlenecks in foundation model (FM) reasoning-limited\nmodel-intrinsic capacity and unreliable test-time iteration. AlphaApollo\norchestrates multiple models with professional tools to enable deliberate,\nverifiable reasoning. It couples (i) a computation tool (Python with numerical\nand symbolic libraries) and (ii) a retrieval tool (task-relevant external\ninformation) to execute exact calculations and ground decisions. The system\nfurther supports multi-round, multi-model solution evolution via a shared state\nmap that records candidates, executable checks, and feedback for iterative\nrefinement. In evaluations on AIME 2024/2025 across multiple models,\nAlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32\nfor Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for\nLlama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool\ncalls are successfully executed, with consistent outperformance of non-tool\nbaselines, thereby lifting the capability ceiling of FMs. More empirical\nresults and implementation details will be updated at\nhttps://github.com/tmlr-group/AlphaApollo.", "AI": {"tldr": "AlphaApollo\u662f\u4e00\u4e2a\u81ea\u8fdb\u5316\u7684agentic\u63a8\u7406\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u57fa\u7840\u6a21\u578b\uff08FM\uff09\u63a8\u7406\u4e2d\u7684\u4e24\u4e2a\u74f6\u9888\uff1a\u6709\u9650\u7684\u6a21\u578b\u5185\u5728\u80fd\u529b\u548c\u4e0d\u53ef\u9760\u7684\u6d4b\u8bd5\u65f6\u8fed\u4ee3\u3002", "motivation": "\u89e3\u51b3\u57fa\u7840\u6a21\u578b\u63a8\u7406\u80fd\u529b\u6709\u9650\u548c\u6d4b\u8bd5\u65f6\u8fed\u4ee3\u4e0d\u53ef\u9760\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u8ba1\u7b97\u5de5\u5177\uff08Python\uff09\u548c\u68c0\u7d22\u5de5\u5177\uff08\u5916\u90e8\u4fe1\u606f\uff09\uff0c\u5e76\u5229\u7528\u5171\u4eab\u72b6\u6001\u56fe\u652f\u6301\u591a\u8f6e\u3001\u591a\u6a21\u578b\u89e3\u51b3\u65b9\u6848\u7684\u6f14\u8fdb\u3002", "result": "\u5728AIME 2024/2025\u7684\u8bc4\u4f30\u4e2d\uff0cAlphaApollo\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u63d0\u5347\u3002", "conclusion": "AlphaApollo\u80fd\u591f\u6709\u6548\u63d0\u5347\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8d85\u8fc7\u4e86\u975e\u5de5\u5177\u57fa\u7ebf\u3002"}}
{"id": "2510.06267", "categories": ["cs.LG", "cs.AI", "I.2.6; H.2.8; J.3"], "pdf": "https://arxiv.org/pdf/2510.06267", "abs": "https://arxiv.org/abs/2510.06267", "authors": ["Khartik Uppalapati", "Shakeel Abdulkareem", "Bora Yimenicioglu"], "title": "RareGraph-Synth: Knowledge-Guided Diffusion Models for Generating Privacy-Preserving Synthetic Patient Trajectories in Ultra-Rare Diseases", "comment": "6 pages, 2 figures, 2 tables. Submitted to IEEE International\n  Conference on Data Science and Advanced Analytics (DSAA)", "summary": "We propose RareGraph-Synth, a knowledge-guided, continuous-time diffusion\nframework that generates realistic yet privacy-preserving synthetic\nelectronic-health-record (EHR) trajectories for ultra-rare diseases.\nRareGraph-Synth unifies five public resources: Orphanet/Orphadata, the Human\nPhenotype Ontology (HPO), the GARD rare-disease KG, PrimeKG, and the FDA\nAdverse Event Reporting System (FAERS) into a heterogeneous knowledge graph\ncomprising approximately 8 M typed edges. Meta-path scores extracted from this\n8-million-edge KG modulate the per-token noise schedule in the forward\nstochastic differential equation, steering generation toward biologically\nplausible lab-medication-adverse-event co-occurrences while retaining\nscore-based diffusion model stability. The reverse denoiser then produces\ntimestamped sequences of lab-code, medication-code, and adverse-event-flag\ntriples that contain no protected health information. On simulated\nultra-rare-disease cohorts, RareGraph-Synth lowers categorical Maximum Mean\nDiscrepancy by 40 percent relative to an unguided diffusion baseline and by\ngreater than 60 percent versus GAN counterparts, without sacrificing downstream\npredictive utility. A black-box membership-inference evaluation using the\nDOMIAS attacker yields AUROC approximately 0.53, well below the 0.55\nsafe-release threshold and substantially better than the approximately 0.61\nplus or minus 0.03 observed for non-KG baselines, demonstrating strong\nresistance to re-identification. These results suggest that integrating\nbiomedical knowledge graphs directly into diffusion noise schedules can\nsimultaneously enhance fidelity and privacy, enabling safer data sharing for\nrare-disease research.", "AI": {"tldr": "\u63d0\u51faRareGraph-Synth\uff0c\u4e00\u79cd\u77e5\u8bc6\u5f15\u5bfc\u7684\u8fde\u7eed\u65f6\u95f4\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u7528\u4e8e\u8d85\u7f55\u89c1\u75be\u75c5\u7684\u903c\u771f\u4f46\u4fdd\u62a4\u9690\u79c1\u7684\u5408\u6210\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u8f68\u8ff9\u3002", "motivation": "\u4e3a\u8d85\u7f55\u89c1\u75be\u75c5\u751f\u6210\u903c\u771f\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u5408\u6210\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u8f68\u8ff9\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u5b89\u5168\u7684\u6570\u636e\u5171\u4eab\uff0c\u4ece\u800c\u4fc3\u8fdb\u7f55\u89c1\u75be\u75c5\u7684\u7814\u7a76\u3002", "method": "\u5c06Orphanet/Orphadata\u3001\u4eba\u7c7b\u8868\u578b\u672c\u4f53\uff08HPO\uff09\u3001GARD\u7f55\u89c1\u75be\u75c5\u77e5\u8bc6\u56fe\u8c31\u3001PrimeKG\u548cFDA\u4e0d\u826f\u4e8b\u4ef6\u62a5\u544a\u7cfb\u7edf\uff08FAERS\uff09\u4e94\u4e2a\u516c\u5171\u8d44\u6e90\u6574\u5408\u5230\u4e00\u4e2a\u5305\u542b\u7ea6800\u4e07\u4e2a\u7c7b\u578b\u5316\u8fb9\u7684\u5f02\u6784\u77e5\u8bc6\u56fe\u8c31\u4e2d\u3002\u4ece\u8be5\u77e5\u8bc6\u56fe\u8c31\u4e2d\u63d0\u53d6\u7684\u5143\u8def\u5f84\u8bc4\u5206\u8c03\u8282\u524d\u5411\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u4e2d\u7684\u6bcf\u4e2atoken\u7684\u566a\u58f0\u8ba1\u5212\uff0c\u5f15\u5bfc\u751f\u6210\u751f\u7269\u5b66\u4e0a\u5408\u7406\u7684\u5b9e\u9a8c\u5ba4-\u836f\u7269-\u4e0d\u826f\u4e8b\u4ef6\u5171\u73b0\uff0c\u540c\u65f6\u4fdd\u6301\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\u7a33\u5b9a\u6027\u3002\u9006\u5411\u53bb\u566a\u5668\u7136\u540e\u751f\u6210\u5305\u542b\u65f6\u95f4\u6233\u7684\u5b9e\u9a8c\u5ba4\u4ee3\u7801\u3001\u836f\u7269\u4ee3\u7801\u548c\u4e0d\u826f\u4e8b\u4ef6\u6807\u5fd7\u4e09\u5143\u7ec4\u5e8f\u5217\uff0c\u5176\u4e2d\u4e0d\u5305\u542b\u53d7\u4fdd\u62a4\u7684\u5065\u5eb7\u4fe1\u606f\u3002", "result": "\u5728\u6a21\u62df\u7684\u8d85\u7f55\u89c1\u75be\u75c5\u961f\u5217\u4e2d\uff0c\u76f8\u5bf9\u4e8e\u65e0\u5f15\u5bfc\u7684\u6269\u6563\u57fa\u7ebf\uff0cRareGraph-Synth\u5c06\u5206\u7c7b\u6700\u5927\u5e73\u5747\u5dee\u5f02\u964d\u4f4e\u4e8640%\uff0c\u76f8\u5bf9\u4e8eGAN\u964d\u4f4e\u4e8660%\u4ee5\u4e0a\uff0c\u4e14\u4e0d\u727a\u7272\u4e0b\u6e38\u9884\u6d4b\u6548\u7528\u3002\u4f7f\u7528DOMIAS\u653b\u51fb\u8005\u7684\u9ed1\u76d2\u6210\u5458\u63a8\u65ad\u8bc4\u4f30\u4ea7\u751f\u7ea60.53\u7684AUROC\uff0c\u8fdc\u4f4e\u4e8e0.55\u7684\u5b89\u5168\u53d1\u5e03\u9608\u503c\uff0c\u5e76\u4e14\u660e\u663e\u4f18\u4e8e\u975e\u77e5\u8bc6\u56fe\u8c31\u57fa\u7ebf\u7684\u7ea60.61\u52a0/\u51cf0.03\uff0c\u8868\u660e\u5bf9\u91cd\u65b0\u8bc6\u522b\u5177\u6709\u5f88\u5f3a\u7684\u62b5\u6297\u529b\u3002", "conclusion": "\u5c06\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u76f4\u63a5\u6574\u5408\u5230\u6269\u6563\u566a\u58f0\u8ba1\u5212\u4e2d\u53ef\u4ee5\u540c\u65f6\u63d0\u9ad8\u4fdd\u771f\u5ea6\u548c\u9690\u79c1\u6027\uff0c\u4ece\u800c\u4e3a\u7f55\u89c1\u75be\u75c5\u7814\u7a76\u5b9e\u73b0\u66f4\u5b89\u5168\u7684\u6570\u636e\u5171\u4eab\u3002"}}
{"id": "2510.06239", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06239", "abs": "https://arxiv.org/abs/2510.06239", "authors": ["Pranav Gupta"], "title": "OpenStaxQA: A multilingual dataset based on open-source college textbooks", "comment": null, "summary": "We present OpenStaxQA, an evaluation benchmark specific to college-level\neducational applications based on 43 open-source college textbooks in English,\nSpanish, and Polish, available under a permissive Creative Commons license. We\nfinetune and evaluate large language models (LLMs) with approximately 7 billion\nparameters on this dataset using quantized low rank adapters (QLoRa).\nAdditionally we also perform a zero-shot evaluation on the AI2 reasoning\nchallenge dev dataset in order to check if OpenStaxQA can lead to an improved\nperformance on other tasks. We also discuss broader impacts relevant to\ndatasets such as OpenStaxQA.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3a OpenStaxQA \u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u4e13\u95e8\u7528\u4e8e\u5927\u5b66\u6c34\u5e73\u7684\u6559\u80b2\u5e94\u7528\u7a0b\u5e8f\u3002", "motivation": "\u521b\u5efa\u8be5\u57fa\u51c6\u4ee5\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5927\u5b66\u6c34\u5e73\u6559\u80b2\u6750\u6599\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u91cf\u5316\u7684\u4f4e\u79e9\u9002\u914d\u5668 (QLoRa) \u5728\u6b64\u6570\u636e\u96c6\u4e0a\u5bf9\u5177\u6709\u5927\u7ea6 70 \u4ebf\u4e2a\u53c2\u6570\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u8fdb\u884c\u5fae\u8c03\u548c\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5728 AI2 \u63a8\u7406\u6311\u6218\u5f00\u53d1\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u4ee5\u68c0\u67e5 OpenStaxQA \u662f\u5426\u53ef\u4ee5\u63d0\u9ad8\u5176\u4ed6\u4efb\u52a1\u7684\u6027\u80fd\u3002", "result": "\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\u548c\u8bc4\u4f30\uff0c\u5e76\u5728 AI2 \u63a8\u7406\u6311\u6218\u5f00\u53d1\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u96f6\u6837\u672c\u8bc4\u4f30\u3002", "conclusion": "\u672c\u6587\u8ba8\u8bba\u4e86\u4e0e OpenStaxQA \u7b49\u6570\u636e\u96c6\u76f8\u5173\u7684\u66f4\u5e7f\u6cdb\u7684\u5f71\u54cd\u3002"}}
{"id": "2510.06888", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06888", "abs": "https://arxiv.org/abs/2510.06888", "authors": ["Arkadeep Acharya", "Akash Ghosh", "Pradeepika Verma", "Kitsuchart Pasupa", "Sriparna Saha", "Priti Singh"], "title": "M3Retrieve: Benchmarking Multimodal Retrieval for Medicine", "comment": "EMNLP Mains 2025", "summary": "With the increasing use of RetrievalAugmented Generation (RAG), strong\nretrieval models have become more important than ever. In healthcare,\nmultimodal retrieval models that combine information from both text and images\noffer major advantages for many downstream tasks such as question answering,\ncross-modal retrieval, and multimodal summarization, since medical data often\nincludes both formats. However, there is currently no standard benchmark to\nevaluate how well these models perform in medical settings. To address this\ngap, we introduce M3Retrieve, a Multimodal Medical Retrieval Benchmark.\nM3Retrieve, spans 5 domains,16 medical fields, and 4 distinct tasks, with over\n1.2 Million text documents and 164K multimodal queries, all collected under\napproved licenses. We evaluate leading multimodal retrieval models on this\nbenchmark to explore the challenges specific to different medical specialities\nand to understand their impact on retrieval performance. By releasing\nM3Retrieve, we aim to enable systematic evaluation, foster model innovation,\nand accelerate research toward building more capable and reliable multimodal\nretrieval systems for medical applications. The dataset and the baselines code\nare available in this github page https://github.com/AkashGhosh/M3Retrieve.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u533b\u7597\u591a\u6a21\u6001\u68c0\u7d22\u6a21\u578b\u7684\u65b0\u57fa\u51c6 M3Retrieve\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u7597\u9886\u57df\u7f3a\u4e4f\u8bc4\u4f30\u591a\u6a21\u6001\u68c0\u7d22\u6a21\u578b\u7684\u6807\u51c6\u57fa\u51c6\uff0c\u800c\u8fd9\u79cd\u6a21\u578b\u5728\u533b\u7597\u95ee\u7b54\u3001\u8de8\u6a21\u6001\u68c0\u7d22\u548c\u591a\u6a21\u6001\u6458\u8981\u7b49\u4efb\u52a1\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b5\u4e2a\u9886\u57df\u300116\u4e2a\u533b\u5b66\u9886\u57df\u548c4\u4e2a\u4e0d\u540c\u4efb\u52a1\u7684 M3Retrieve \u57fa\u51c6\uff0c\u5305\u542b120\u4e07\u6587\u672c\u548c16.4\u4e07\u591a\u6a21\u6001\u67e5\u8be2\uff0c\u5e76\u8bc4\u4f30\u4e86\u9886\u5148\u7684\u591a\u6a21\u6001\u68c0\u7d22\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u5728 M3Retrieve \u57fa\u51c6\u4e0a\u8bc4\u4f30\u6a21\u578b\uff0c\u63a2\u7d22\u4e86\u4e0d\u540c\u533b\u5b66\u4e13\u4e1a\u7684\u6311\u6218\u53ca\u5176\u5bf9\u68c0\u7d22\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u53d1\u5e03 M3Retrieve \u65e8\u5728\u4fc3\u8fdb\u7cfb\u7edf\u8bc4\u4f30\u3001\u6a21\u578b\u521b\u65b0\uff0c\u5e76\u52a0\u901f\u6784\u5efa\u66f4\u5f3a\u5927\u548c\u53ef\u9760\u7684\u533b\u7597\u591a\u6a21\u6001\u68c0\u7d22\u7cfb\u7edf\u7684\u7814\u7a76\u3002"}}
{"id": "2510.06663", "categories": ["cs.DB", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06663", "abs": "https://arxiv.org/abs/2510.06663", "authors": ["Qiuyang Mang", "Runyuan He", "Suyang Zhong", "Xiaoxuan Liu", "Huanchen Zhang", "Alvin Cheung"], "title": "Automated Discovery of Test Oracles for Database Management Systems Using LLMs", "comment": null, "summary": "Since 2020, automated testing for Database Management Systems (DBMSs) has\nflourished, uncovering hundreds of bugs in widely-used systems. A cornerstone\nof these techniques is test oracle, which typically implements a mechanism to\ngenerate equivalent query pairs, thereby identifying bugs by checking the\nconsistency between their results. However, while applying these oracles can be\nautomated, their design remains a fundamentally manual endeavor. This paper\nexplores the use of large language models (LLMs) to automate the discovery and\ninstantiation of test oracles, addressing a long-standing bottleneck towards\nfully automated DBMS testing. Although LLMs demonstrate impressive creativity,\nthey are prone to hallucinations that can produce numerous false positive bug\nreports. Furthermore, their significant monetary cost and latency mean that LLM\ninvocations should be limited to ensure that bug detection is efficient and\neconomical.\n  To this end, we introduce Argus, a novel framework built upon the core\nconcept of the Constrained Abstract Query - a SQL skeleton containing\nplaceholders and their associated instantiation conditions (e.g., requiring a\nplaceholder to be filled by a boolean column). Argus uses LLMs to generate\npairs of these skeletons that are asserted to be semantically equivalent. This\nequivalence is then formally proven using a SQL equivalence solver to ensure\nsoundness. Finally, the placeholders within the verified skeletons are\ninstantiated with concrete, reusable SQL snippets that are also synthesized by\nLLMs to efficiently produce complex test cases. We implemented Argus and\nevaluated it on five extensively tested DBMSs, discovering 40 previously\nunknown bugs, 35 of which are logic bugs, with 36 confirmed and 26 already\nfixed by the developers.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aArgus\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u81ea\u52a8\u5316\u6570\u636e\u5e93\u7ba1\u7406\u7cfb\u7edf\uff08DBMS\uff09\u7684\u6d4b\u8bd5\u9884\u8a00\u673a\u7684\u53d1\u73b0\u548c\u5b9e\u4f8b\u5316\u3002", "motivation": "\u73b0\u6709\u7684DBMS\u81ea\u52a8\u6d4b\u8bd5\u6280\u672f\u4f9d\u8d56\u4e8e\u624b\u52a8\u8bbe\u8ba1\u7684\u6d4b\u8bd5\u9884\u8a00\u673a\uff0c\u8fd9\u9650\u5236\u4e86\u81ea\u52a8\u5316\u7a0b\u5ea6\u3002\u8fd9\u7bc7\u8bba\u6587\u65e8\u5728\u5229\u7528LLM\u6765\u81ea\u52a8\u5316\u6d4b\u8bd5\u9884\u8a00\u673a\u7684\u53d1\u73b0\u548c\u5b9e\u4f8b\u5316\uff0c\u89e3\u51b3DBMS\u6d4b\u8bd5\u4e2d\u7684\u74f6\u9888\u95ee\u9898\u3002", "method": "Argus\u6846\u67b6\u57fa\u4e8e\u7ea6\u675f\u62bd\u8c61\u67e5\u8be2\u7684\u6838\u5fc3\u6982\u5ff5\uff0c\u4f7f\u7528LLM\u751f\u6210\u8bed\u4e49\u7b49\u4ef7\u7684SQL\u9aa8\u67b6\u5bf9\uff0c\u5e76\u901a\u8fc7SQL\u7b49\u4ef7\u6c42\u89e3\u5668\u8fdb\u884c\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u6700\u540e\u4f7f\u7528LLM\u751f\u6210\u5177\u4f53\u7684SQL\u4ee3\u7801\u7247\u6bb5\u6765\u5b9e\u4f8b\u5316\u8fd9\u4e9b\u9aa8\u67b6\u3002", "result": "Argus\u5728\u4e94\u4e2a\u5e7f\u6cdb\u6d4b\u8bd5\u7684DBMS\u4e0a\u53d1\u73b0\u4e8640\u4e2a\u4ee5\u524d\u672a\u77e5\u7684\u9519\u8bef\uff0c\u5176\u4e2d35\u4e2a\u662f\u903b\u8f91\u9519\u8bef\uff0c36\u4e2a\u5df2\u786e\u8ba4\uff0c26\u4e2a\u5df2\u88ab\u5f00\u53d1\u4eba\u5458\u4fee\u590d\u3002", "conclusion": "Argus\u6846\u67b6\u6210\u529f\u5730\u5229\u7528LLM\u81ea\u52a8\u5316\u4e86DBMS\u6d4b\u8bd5\u9884\u8a00\u673a\u7684\u751f\u6210\uff0c\u5e76\u53d1\u73b0\u4e86\u5927\u91cf\u65b0\u7684\u9519\u8bef\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.06231", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06231", "abs": "https://arxiv.org/abs/2510.06231", "authors": ["Mingzhe Zheng", "Dingjie Song", "Guanyu Zhou", "Jun You", "Jiahao Zhan", "Xuran Ma", "Xinyuan Song", "Ser-Nam Lim", "Qifeng Chen", "Harry Yang"], "title": "CML-Bench: A Framework for Evaluating and Enhancing LLM-Powered Movie Scripts Generation", "comment": "24 pages, 9 figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating highly structured texts. However, while exhibiting a high degree of\nstructural organization, movie scripts demand an additional layer of nuanced\nstorytelling and emotional depth-the 'soul' of compelling cinema-that LLMs\noften fail to capture. To investigate this deficiency, we first curated\nCML-Dataset, a dataset comprising (summary, content) pairs for Cinematic Markup\nLanguage (CML), where 'content' consists of segments from esteemed,\nhigh-quality movie scripts and 'summary' is a concise description of the\ncontent. Through an in-depth analysis of the intrinsic multi-shot continuity\nand narrative structures within these authentic scripts, we identified three\npivotal dimensions for quality assessment: Dialogue Coherence (DC), Character\nConsistency (CC), and Plot Reasonableness (PR). Informed by these findings, we\npropose the CML-Bench, featuring quantitative metrics across these dimensions.\nCML-Bench effectively assigns high scores to well-crafted, human-written\nscripts while concurrently pinpointing the weaknesses in screenplays generated\nby LLMs. To further validate our benchmark, we introduce CML-Instruction, a\nprompting strategy with detailed instructions on character dialogue and event\nlogic, to guide LLMs to generate more structured and cinematically sound\nscripts. Extensive experiments validate the effectiveness of our benchmark and\ndemonstrate that LLMs guided by CML-Instruction generate higher-quality\nscreenplays, with results aligned with human preferences.", "AI": {"tldr": "LLMs\u5728\u751f\u6210\u7ed3\u6784\u5316\u6587\u672c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7535\u5f71\u5267\u672c\u7684\u60c5\u611f\u6df1\u5ea6\u65b9\u9762\u6709\u6240\u6b20\u7f3a\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86CML-Dataset\u548cCML-Bench\uff0c\u5e76\u8bbe\u8ba1\u4e86CML-Instruction\u63d0\u793a\u7b56\u7565\u6765\u6307\u5bfcLLMs\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u5267\u672c\u3002", "motivation": "LLMs\u96be\u4ee5\u6355\u6349\u7535\u5f71\u5267\u672c\u4e2d\u7684\u7ec6\u81f4\u53d9\u4e8b\u548c\u60c5\u611f\u6df1\u5ea6\u3002", "method": "1. \u521b\u5efaCML-Dataset\u6570\u636e\u96c6\uff0c\u5305\u542b\u7535\u5f71\u5267\u672c\u7247\u6bb5\u548c\u6458\u8981\uff1b2. \u786e\u5b9a\u4e86\u5bf9\u8bdd\u8fde\u8d2f\u6027(DC)\u3001\u89d2\u8272\u4e00\u81f4\u6027(CC)\u548c\u60c5\u8282\u5408\u7406\u6027(PR)\u4e09\u4e2a\u8d28\u91cf\u8bc4\u4f30\u7ef4\u5ea6\uff1b3. \u63d0\u51fa\u4e86CML-Bench\uff0c\u7528\u4e8e\u8bc4\u4f30\u5267\u672c\u8d28\u91cf\uff1b4. \u8bbe\u8ba1\u4e86CML-Instruction\u63d0\u793a\u7b56\u7565\uff0c\u6307\u5bfcLLMs\u751f\u6210\u66f4\u7ed3\u6784\u5316\u548c\u7535\u5f71\u5316\u7684\u5267\u672c\u3002", "result": "CML-Bench\u80fd\u6709\u6548\u533a\u5206\u9ad8\u8d28\u91cf\u7684\u4eba\u5de5\u5267\u672c\u548cLLMs\u751f\u6210\u7684\u5267\u672c\u7684\u5f31\u70b9\u3002CML-Instruction\u80fd\u6307\u5bfcLLMs\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u5267\u672c\uff0c\u7ed3\u679c\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3002", "conclusion": "\u8bba\u6587\u9a8c\u8bc1\u4e86CML-Bench\u7684\u6709\u6548\u6027\uff0c\u5e76\u8bc1\u660e\u4e86CML-Instruction\u80fd\u63d0\u5347LLMs\u751f\u6210\u5267\u672c\u7684\u8d28\u91cf\u3002"}}
{"id": "2510.06274", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06274", "abs": "https://arxiv.org/abs/2510.06274", "authors": ["Mohammad Mahdi Samiei Paqaleh", "Arash Marioriyad", "Arman Tahmasebi-Zadeh", "Mohamadreza Fereydooni", "Mahdi Ghaznavai", "Mahdieh Soleymani Baghshah"], "title": "Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization", "comment": null, "summary": "Recent progress has pushed AI frontiers from pattern recognition tasks toward\nproblems that require step by step, System2 style reasoning, especially with\nlarge language models. Yet, unlike learning, where generalization and out of\ndistribution (OoD) evaluation concepts are well formalized, there is no clear,\nconsistent definition or metric for reasoning ability. We propose Complexity\nOut of Distribution (Complexity OoD) generalization as a framework and problem\nsetting to define and measure reasoning. A model exhibits Complexity OoD\ngeneralization when it maintains performance on test instances whose minimal\nrequired solution complexity, either representational (richer solution\nstructure) or computational (more reasoning steps/program length), exceeds that\nof all training examples. We formalize complexity via solution description\nKolmogorov complexity and operational proxies (e.g., object/relation counts;\nreasoning step counts), clarifying how Complexity OoD differs from length and\ncompositional OoD. This lens unifies learning and reasoning: many cases\nsolvable with System1 like processing at low complexity become System2 like\nunder complexity pressure, while System2 can be viewed as generalization over\nsolution structures. We translate this perspective into practice with\nrecommendations for operationalizing Complexity OoD across the stack:\nincorporating complexity into benchmark and evaluation metric design,\nrethinking supervision to target solution traces, seeking and designing\ninductive biases for Complexity OoD generalization, addressing learning to\nreason spillovers such as spurious shortcuts, semantic robustness, catastrophic\nforgetting, and step wise calibration. Because Complexity OoD cannot be solved\nby scaling data alone, progress toward robust reasoning will require\narchitectures and training regimes that explicitly model and allocate\ncomputation with respect to complexity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u201c\u590d\u6742\u6027\u5206\u5e03\u5916\u6cdb\u5316 (Complexity OoD)\u201d \u6846\u67b6\uff0c\u7528\u4e8e\u5b9a\u4e49\u548c\u8861\u91cf\u63a8\u7406\u80fd\u529b\u3002\u5f53\u6a21\u578b\u5728\u6d4b\u8bd5\u5b9e\u4f8b\u4e0a\u4fdd\u6301\u6027\u80fd\u65f6\uff0c\u5b83\u4f1a\u8868\u73b0\u51fa\u590d\u6742\u6027 OoD \u6cdb\u5316\uff0c\u5176\u4e2d\u6d4b\u8bd5\u5b9e\u4f8b\u7684\u6700\u5c0f\u6240\u9700\u89e3\u51b3\u65b9\u6848\u590d\u6742\u6027\u8d85\u8fc7\u6240\u6709\u8bad\u7ec3\u793a\u4f8b\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u9010\u6b65\u63a8\u7406\u7684\u95ee\u9898\u4e0a\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u63a8\u7406\u80fd\u529b\u660e\u786e\u4e14\u4e00\u81f4\u7684\u5b9a\u4e49\u6216\u6307\u6807\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u89e3\u51b3\u65b9\u6848\u63cf\u8ff0 Kolmogorov \u590d\u6742\u6027\u548c\u64cd\u4f5c\u4ee3\u7406\uff08\u4f8b\u5982\uff0c\u5bf9\u8c61/\u5173\u7cfb\u8ba1\u6570\uff1b\u63a8\u7406\u6b65\u9aa4\u8ba1\u6570\uff09\u6765\u5f62\u5f0f\u5316\u590d\u6742\u6027\uff0c\u5e76\u9610\u660e\u4e86\u590d\u6742\u6027 OoD \u4e0e\u957f\u5ea6\u548c\u7ec4\u5408 OoD \u7684\u4e0d\u540c\u4e4b\u5904\u3002\u8fd8\u63d0\u51fa\u4e86\u5728\u57fa\u51c6\u548c\u8bc4\u4f30\u6307\u6807\u8bbe\u8ba1\u4e2d\u7ed3\u5408\u590d\u6742\u6027\uff0c\u91cd\u65b0\u601d\u8003\u9488\u5bf9\u89e3\u51b3\u65b9\u6848\u8ddf\u8e2a\u7684\u76d1\u7763\uff0c\u5bfb\u627e\u548c\u8bbe\u8ba1\u590d\u6742\u6027 OoD \u6cdb\u5316\u7684\u5f52\u7eb3\u504f\u5dee\uff0c\u89e3\u51b3\u5b66\u4e60\u63a8\u7406\u6ea2\u51fa\u7b49\u5b9e\u8df5\u5efa\u8bae\u3002", "result": "\u8bba\u6587\u7edf\u4e00\u4e86\u5b66\u4e60\u548c\u63a8\u7406\uff1a\u8bb8\u591a\u53ef\u4ee5\u7528\u7c7b\u4f3c System1 \u7684\u5904\u7406\u65b9\u5f0f\u89e3\u51b3\u7684\u4f4e\u590d\u6742\u6027\u6848\u4f8b\u5728\u590d\u6742\u6027\u538b\u529b\u4e0b\u53d8\u6210\u4e86\u7c7b\u4f3c System2 \u7684\u5904\u7406\u65b9\u5f0f\uff0c\u800c System2 \u53ef\u4ee5\u88ab\u770b\u4f5c\u662f\u5bf9\u89e3\u51b3\u65b9\u6848\u7ed3\u6784\u7684\u6cdb\u5316\u3002", "conclusion": "\u590d\u6742\u6027 OoD \u65e0\u6cd5\u901a\u8fc7\u5355\u72ec\u7f29\u653e\u6570\u636e\u6765\u89e3\u51b3\uff0c\u56e0\u6b64\uff0c\u8981\u5b9e\u73b0\u7a33\u5065\u7684\u63a8\u7406\uff0c\u9700\u8981\u660e\u786e\u5efa\u6a21\u5e76\u6839\u636e\u590d\u6742\u6027\u5206\u914d\u8ba1\u7b97\u7684\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6848\u3002"}}
{"id": "2510.06270", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06270", "abs": "https://arxiv.org/abs/2510.06270", "authors": ["Nian Ran", "Zhongzheng Li", "Yue Wang", "Qingsong Ran", "Xiaoyuan Zhang", "Shikun Feng", "Richard Allmendinger", "Xiaoguang Zhao"], "title": "MCCE: A Framework for Multi-LLM Collaborative Co-Evolution", "comment": null, "summary": "Multi-objective discrete optimization problems, such as molecular design,\npose significant challenges due to their vast and unstructured combinatorial\nspaces. Traditional evolutionary algorithms often get trapped in local optima,\nwhile expert knowledge can provide crucial guidance for accelerating\nconvergence. Large language models (LLMs) offer powerful priors and reasoning\nability, making them natural optimizers when expert knowledge matters. However,\nclosed-source LLMs, though strong in exploration, cannot update their\nparameters and thus cannot internalize experience. Conversely, smaller open\nmodels can be continually fine-tuned but lack broad knowledge and reasoning\nstrength. We introduce Multi-LLM Collaborative Co-evolution (MCCE), a hybrid\nframework that unites a frozen closed-source LLM with a lightweight trainable\nmodel. The system maintains a trajectory memory of past search processes; the\nsmall model is progressively refined via reinforcement learning, with the two\nmodels jointly supporting and complementing each other in global exploration.\nUnlike model distillation, this process enhances the capabilities of both\nmodels through mutual inspiration. Experiments on multi-objective drug design\nbenchmarks show that MCCE achieves state-of-the-art Pareto front quality and\nconsistently outperforms baselines. These results highlight a new paradigm for\nenabling continual evolution in hybrid LLM systems, combining knowledge-driven\nexploration with experience-driven learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u51bb\u7ed3\u7684\u95ed\u6e90LLM\u548c\u8f7b\u91cf\u7ea7\u53ef\u8bad\u7ec3\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u591a\u76ee\u6807\u79bb\u6563\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8fdb\u5316\u7b97\u6cd5\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff0c\u800c\u4e13\u5bb6\u77e5\u8bc6\u53ef\u4ee5\u4e3a\u52a0\u901f\u6536\u655b\u63d0\u4f9b\u5173\u952e\u6307\u5bfc\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5148\u9a8c\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u5176\u6210\u4e3a\u4e13\u5bb6\u77e5\u8bc6\u91cd\u8981\u65f6\u7684\u81ea\u7136\u4f18\u5316\u5668\u3002\u7136\u800c\uff0c\u95ed\u6e90LLM\u65e0\u6cd5\u66f4\u65b0\u53c2\u6570\uff0c\u800c\u5c0f\u578b\u5f00\u653e\u6a21\u578b\u7f3a\u4e4f\u5e7f\u6cdb\u7684\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5f15\u5165\u591aLLM\u534f\u540c\u8fdb\u5316(MCCE)\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9010\u6b65\u6539\u8fdb\u5c0f\u578b\u6a21\u578b\uff0c\u4e24\u4e2a\u6a21\u578b\u5171\u540c\u652f\u6301\u548c\u8865\u5145\u5168\u5c40\u63a2\u7d22\u3002", "result": "\u5728\u591a\u76ee\u6807\u836f\u7269\u8bbe\u8ba1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMCCE\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684Pareto\u524d\u6cbf\u8d28\u91cf\uff0c\u5e76\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "MCCE\u5c55\u793a\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408LLM\u7cfb\u7edf\u4e2d\u6301\u7eed\u8fdb\u5316\u7684\u8303\u4f8b\uff0c\u7ed3\u5408\u4e86\u77e5\u8bc6\u9a71\u52a8\u7684\u63a2\u7d22\u548c\u7ecf\u9a8c\u9a71\u52a8\u7684\u5b66\u4e60\u3002"}}
{"id": "2510.06240", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.06240", "abs": "https://arxiv.org/abs/2510.06240", "authors": ["Jiqun Pan", "Zhenke Duan", "Jiani Tu", "Anzhi Cheng", "Yanqing Wang"], "title": "Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets", "comment": "41 pages, 12 figures, 6 tables", "summary": "Industrial question-answering (QA) systems require higher safety and\nreliability than general-purpose dialogue models, as errors in high-risk\nscenarios such as equipment fault diagnosis can have severe consequences.\nAlthough multi-agent large language models enhance reasoning depth, they suffer\nfrom uncontrolled iterations and unverifiable outputs, and conventional\ndistillation methods struggle to transfer collaborative reasoning capabilities\nto lightweight, deployable student models. To address these challenges, we\npropose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our\napproach formulates distillation as a Markov Decision Process and incorporates\na knowledge graph as a verifiable structured prior to enrich state\nrepresentation and ensure convergence. By integrating collaborative reasoning\nwith knowledge grounding, KG-MASD generates high-confidence instruction-tuning\ndata and jointly distills reasoning depth and verifiability into compact\nstudent models suitable for edge deployment. Experiments on an industrial QA\ndataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent\nover baselines and significantly enhances reliability, enabling trustworthy AI\ndeployment in safety-critical industrial scenarios. Code and data are available\nat https://github.com/erwinmsmith/KG-MAD/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u77e5\u8bc6\u56fe\u5f15\u5bfc\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u84b8\u998f (KG-MASD) \u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u5de5\u4e1a\u95ee\u7b54\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5de5\u4e1a\u95ee\u7b54\u7cfb\u7edf\u9700\u8981\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u66f4\u9ad8\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u800c\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u8fed\u4ee3\u4e0d\u53ef\u63a7\u548c\u8f93\u51fa\u4e0d\u53ef\u9a8c\u8bc1\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u7684\u84b8\u998f\u65b9\u6cd5\u96be\u4ee5\u5c06\u534f\u4f5c\u63a8\u7406\u80fd\u529b\u8f6c\u79fb\u5230\u8f7b\u91cf\u7ea7\u6a21\u578b\u3002", "method": "\u5c06\u84b8\u998f\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u4f5c\u4e3a\u53ef\u9a8c\u8bc1\u7684\u7ed3\u6784\u5316\u5148\u9a8c\uff0c\u4ee5\u4e30\u5bcc\u72b6\u6001\u8868\u793a\u5e76\u786e\u4fdd\u6536\u655b\u3002", "result": "\u5728\u5de5\u4e1a\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cKG-MASD \u7684\u51c6\u786e\u7387\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e86 2.4% \u5230 20.1%\uff0c\u5e76\u663e\u7740\u63d0\u9ad8\u4e86\u53ef\u9760\u6027\u3002", "conclusion": "KG-MASD \u80fd\u591f\u5c06\u63a8\u7406\u6df1\u5ea6\u548c\u53ef\u9a8c\u8bc1\u6027\u5171\u540c\u63d0\u70bc\u5230\u9002\u5408\u8fb9\u7f18\u90e8\u7f72\u7684\u7d27\u51d1\u578b\u5b66\u751f\u6a21\u578b\u4e2d\uff0c\u4ece\u800c\u5728\u5b89\u5168\u5173\u952e\u7684\u5de5\u4e1a\u573a\u666f\u4e2d\u5b9e\u73b0\u53ef\u4fe1\u8d56\u7684 AI \u90e8\u7f72\u3002"}}
{"id": "2510.06924", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.06924", "abs": "https://arxiv.org/abs/2510.06924", "authors": ["Jordan Nelson", "Almas Baimagambetov", "Konstantinos Avgerinakis", "Nikolaos Polatidis"], "title": "Ethical AI prompt recommendations in large language models using collaborative filtering", "comment": "This paper has been accepted to by the International Journal of\n  Parallel, Emergent & Distributed Systems (Taylor and Francis) and has an\n  assigned DOI. We have already chose to make this open access using CC BY. The\n  article is not yet available online on the publisher's website. The DOI is:\n  doi.org/10.1080/17445760.2025.2573086", "summary": "As large language models (LLMs) shape AI development, ensuring ethical prompt\nrecommendations is crucial. LLMs offer innovation but risk bias, fairness\nissues, and accountability concerns. Traditional oversight methods struggle\nwith scalability, necessitating dynamic solutions. This paper proposes using\ncollaborative filtering, a technique from recommendation systems, to enhance\nethical prompt selection. By leveraging user interactions, it promotes ethical\nguidelines while reducing bias. Contributions include a synthetic dataset for\nprompt recommendations and the application of collaborative filtering. The work\nalso tackles challenges in ethical AI, such as bias mitigation, transparency,\nand preventing unethical prompt engineering.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u786e\u4fdd\u5408\u4e4e\u9053\u5fb7\u7684\u63d0\u793a\u63a8\u8350\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4f7f\u7528\u534f\u540c\u8fc7\u6ee4\u6765\u589e\u5f3a\u9053\u5fb7\u63d0\u793a\u9009\u62e9\u7684\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u540c\u65f6\u4e5f\u5e26\u6765\u4e86\u504f\u5dee\u3001\u516c\u5e73\u6027\u548c\u95ee\u8d23\u5236\u7b49\u4f26\u7406\u98ce\u9669\u3002\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\u96be\u4ee5\u6269\u5c55\uff0c\u9700\u8981\u52a8\u6001\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u534f\u540c\u8fc7\u6ee4\u6280\u672f\uff0c\u901a\u8fc7\u7528\u6237\u4e92\u52a8\u4fc3\u8fdb\u4f26\u7406\u51c6\u5219\uff0c\u51cf\u5c11\u504f\u5dee\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u7528\u4e8e\u63d0\u793a\u63a8\u8350\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u5e94\u7528\u4e86\u534f\u540c\u8fc7\u6ee4\u3002", "conclusion": "\u8be5\u7814\u7a76\u81f4\u529b\u4e8e\u89e3\u51b3\u4f26\u7406\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u6311\u6218\uff0c\u5982\u51cf\u5c11\u504f\u5dee\u3001\u63d0\u9ad8\u900f\u660e\u5ea6\u4ee5\u53ca\u9632\u6b62\u4e0d\u9053\u5fb7\u7684\u63d0\u793a\u5de5\u7a0b\u3002"}}
{"id": "2510.06980", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06980", "abs": "https://arxiv.org/abs/2510.06980", "authors": ["Xinyi Gao", "Jingxi Zhang", "Lijian Chen", "Tong Chen", "Lizhen Cui", "Hongzhi Yin"], "title": "Relational Database Distillation: From Structured Tables to Condensed Graph Data", "comment": null, "summary": "Relational databases (RDBs) underpin the majority of global data management\nsystems, where information is structured into multiple interdependent tables.\nTo effectively use the knowledge within RDBs for predictive tasks, recent\nadvances leverage graph representation learning to capture complex inter-table\nrelations as multi-hop dependencies. Despite achieving state-of-the-art\nperformance, these methods remain hindered by the prohibitive storage overhead\nand excessive training time, due to the massive scale of the database and the\ncomputational burden of intensive message passing across interconnected tables.\nTo alleviate these concerns, we propose and study the problem of Relational\nDatabase Distillation (RDD). Specifically, we aim to distill large-scale RDBs\ninto compact heterogeneous graphs while retaining the predictive power (i.e.,\nutility) required for training graph-based models. Multi-modal column\ninformation is preserved through node features, and primary-foreign key\nrelations are encoded via heterogeneous edges, thereby maintaining both data\nfidelity and relational structure. To ensure adaptability across diverse\ndownstream tasks without engaging the traditional, inefficient bi-level\ndistillation framework, we further design a kernel ridge regression-guided\nobjective with pseudo-labels, which produces quality features for the distilled\ngraph. Extensive experiments on multiple real-world RDBs demonstrate that our\nsolution substantially reduces the data size while maintaining competitive\nperformance on classification and regression tasks, creating an effective\npathway for scalable learning with RDBs.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5173\u7cfb\u6570\u636e\u5e93\u84b8\u998f\uff08RDD\uff09\u65b9\u6cd5\uff0c\u65e8\u5728\u5c06\u5927\u578b\u5173\u7cfb\u6570\u636e\u5e93\u63d0\u70bc\u6210\u7d27\u51d1\u7684\u5f02\u6784\u56fe\uff0c\u540c\u65f6\u4fdd\u7559\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7531\u4e8e\u6570\u636e\u5e93\u89c4\u6a21\u5e9e\u5927\u548c\u8de8\u4e92\u8fde\u8868\u7684\u5bc6\u96c6\u6d88\u606f\u4f20\u9012\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u5bfc\u81f4\u5b58\u50a8\u5f00\u9500\u8fc7\u9ad8\u548c\u8bad\u7ec3\u65f6\u95f4\u8fc7\u957f\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8282\u70b9\u7279\u5f81\u4fdd\u7559\u591a\u6a21\u6001\u5217\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u5f02\u6784\u8fb9\u7f16\u7801\u4e3b\u5916\u952e\u5173\u7cfb\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6838\u5cad\u56de\u5f52\u5f15\u5bfc\u7684\u76ee\u6807\u51fd\u6570\u4e0e\u4f2a\u6807\u7b7e\uff0c\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u84b8\u998f\u56fe\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u5173\u7cfb\u6570\u636e\u5e93\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u89e3\u51b3\u65b9\u6848\u5728\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u4e0a\u4fdd\u6301\u7ade\u4e89\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6570\u636e\u5927\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f7f\u7528\u5173\u7cfb\u6570\u636e\u5e93\u8fdb\u884c\u53ef\u6269\u5c55\u5b66\u4e60\u521b\u5efa\u4e86\u4e00\u6761\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2510.06233", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06233", "abs": "https://arxiv.org/abs/2510.06233", "authors": ["Haoyang Zhang", "Zhou Yang", "Yucai Pang"], "title": "User to Video: A Model for Spammer Detection Inspired by Video Classification Technology", "comment": "Accepted by International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "This article is inspired by video classification technology. If the user\nbehavior subspace is viewed as a frame image, consecutive frame images are\nviewed as a video. Following this novel idea, a model for spammer detection\nbased on user videoization, called UVSD, is proposed. Firstly, a user2piexl\nalgorithm for user pixelization is proposed. Considering the adversarial\nbehavior of user stances, the user is viewed as a pixel, and the stance is\nquantified as the pixel's RGB. Secondly, a behavior2image algorithm is proposed\nfor transforming user behavior subspace into frame images. Low-rank dense\nvectorization of subspace user relations is performed using representation\nlearning, while cutting and diffusion algorithms are introduced to complete the\nframe imageization. Finally, user behavior videos are constructed based on\ntemporal features. Subsequently, a video classification algorithm is combined\nto identify the spammers. Experiments using publicly available datasets, i.e.,\nWEIBO and TWITTER, show an advantage of the UVSD model over state-of-the-art\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7528\u6237\u89c6\u9891\u5316\u7684\u5783\u573e\u4fe1\u606f\u68c0\u6d4b\u6a21\u578b\uff08UVSD\uff09\uff0c\u8be5\u6a21\u578b\u5c06\u7528\u6237\u884c\u4e3a\u5b50\u7a7a\u95f4\u89c6\u4e3a\u5e27\u56fe\u50cf\uff0c\u8fde\u7eed\u7684\u5e27\u56fe\u50cf\u89c6\u4e3a\u89c6\u9891\uff0c\u7136\u540e\u5229\u7528\u89c6\u9891\u5206\u7c7b\u7b97\u6cd5\u6765\u8bc6\u522b\u5783\u573e\u4fe1\u606f\u53d1\u9001\u8005\u3002", "motivation": "\u53d7\u89c6\u9891\u5206\u7c7b\u6280\u672f\u7684\u542f\u53d1\uff0c\u5c06\u7528\u6237\u884c\u4e3a\u5b50\u7a7a\u95f4\u89c6\u4e3a\u5e27\u56fe\u50cf\uff0c\u8fde\u7eed\u5e27\u56fe\u50cf\u89c6\u4e3a\u89c6\u9891\uff0c\u4ece\u800c\u8fdb\u884c\u5783\u573e\u4fe1\u606f\u68c0\u6d4b\u3002", "method": "1) \u63d0\u51fa\u4e86\u7528\u6237\u50cf\u7d20\u5316\u7b97\u6cd5\uff08user2piexl\uff09\uff0c\u5c06\u7528\u6237\u89c6\u4e3a\u50cf\u7d20\uff0c\u7acb\u573a\u91cf\u5316\u4e3a\u50cf\u7d20\u7684RGB\u503c\u30022) \u63d0\u51fa\u4e86\u884c\u4e3a\u56fe\u50cf\u5316\u7b97\u6cd5\uff08behavior2image\uff09\uff0c\u4f7f\u7528\u8868\u793a\u5b66\u4e60\u5bf9\u5b50\u7a7a\u95f4\u7528\u6237\u5173\u7cfb\u8fdb\u884c\u4f4e\u79e9\u7a20\u5bc6\u5411\u91cf\u5316\uff0c\u5e76\u5f15\u5165\u5207\u5272\u548c\u6269\u6563\u7b97\u6cd5\u5b8c\u6210\u5e27\u56fe\u50cf\u5316\u30023) \u57fa\u4e8e\u65f6\u95f4\u7279\u5f81\u6784\u5efa\u7528\u6237\u884c\u4e3a\u89c6\u9891\uff0c\u5e76\u7ed3\u5408\u89c6\u9891\u5206\u7c7b\u7b97\u6cd5\u8bc6\u522b\u5783\u573e\u4fe1\u606f\u53d1\u9001\u8005\u3002", "result": "\u5728WEIBO\u548cTWITTER\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUVSD\u6a21\u578b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u7528\u6237\u89c6\u9891\u5316\u7684\u5783\u573e\u4fe1\u606f\u68c0\u6d4b\u6a21\u578b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.06288", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06288", "abs": "https://arxiv.org/abs/2510.06288", "authors": ["Raj Ghugare", "Catherine Ji", "Kathryn Wantlin", "Jin Schofield", "Benjamin Eysenbach"], "title": "BuilderBench -- A benchmark for generalist agents", "comment": "Project page: https://rajghugare19.github.io/builderbench and Code:\n  https://github.com/rajghugare19/builderbench", "summary": "Today's AI models learn primarily through mimicry and sharpening, so it is\nnot surprising that they struggle to solve problems beyond the limits set by\nexisting data. To solve novel problems, agents should acquire skills for\nexploring and learning through experience. Finding a scalable learning\nmechanism for developing agents that learn through interaction remains a major\nopen problem. In this work, we introduce BuilderBench, a benchmark to\naccelerate research into agent pre-training that centers open-ended\nexploration. BuilderBench requires agents to learn how to build any structure\nusing blocks. BuilderBench is equipped with $(1)$ a hardware accelerated\nsimulator of a robotic agent interacting with various physical blocks, and\n$(2)$ a task-suite with over 42 diverse target structures that are carefully\ncurated to test an understanding of physics, mathematics, and long-horizon\nplanning. During training, agents have to explore and learn general principles\nabout the environment without any external supervision. During evaluation,\nagents have to build the unseen target structures from the task suite. Solving\nthese tasks requires a sort of \\emph{embodied reasoning} that is not reflected\nin words but rather in actions, experimenting with different strategies and\npiecing them together. Our experiments show that many of these tasks challenge\nthe current iteration of algorithms. Hence, we also provide a ``training\nwheels'' protocol, in which agents are trained and evaluated to build a single\ntarget structure from the task suite. Finally, we provide single-file\nimplementations of six different algorithms as a reference point for\nresearchers.", "AI": {"tldr": "BuilderBench\u662f\u4e00\u4e2a\u7528\u4e8e\u52a0\u901f\u667a\u80fd\u4f53\u9884\u8bad\u7ec3\u7814\u7a76\u7684\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u5f00\u653e\u5f0f\u63a2\u7d22\uff0c\u8981\u6c42\u667a\u80fd\u4f53\u5b66\u4e60\u5982\u4f55\u4f7f\u7528\u79ef\u6728\u6784\u5efa\u4efb\u4f55\u7ed3\u6784\u3002", "motivation": "\u5f53\u524d\u7684AI\u6a21\u578b\u4e3b\u8981\u901a\u8fc7\u6a21\u4eff\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u96be\u4ee5\u89e3\u51b3\u8d85\u51fa\u5df2\u6709\u6570\u636e\u8303\u56f4\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u65b0\u95ee\u9898\uff0c\u667a\u80fd\u4f53\u5e94\u8be5\u83b7\u5f97\u901a\u8fc7\u7ecf\u9a8c\u63a2\u7d22\u548c\u5b66\u4e60\u7684\u6280\u80fd\u3002\u4e3a\u5f00\u53d1\u901a\u8fc7\u4ea4\u4e92\u5b66\u4e60\u7684\u667a\u80fd\u4f53\u5bfb\u627e\u53ef\u6269\u5c55\u7684\u5b66\u4e60\u673a\u5236\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u7684\u5f00\u653e\u95ee\u9898\u3002", "method": "\u5f15\u5165BuilderBench\u57fa\u51c6\uff0c\u5b83\u914d\u5907\u4e86\u673a\u5668\u4eba\u667a\u80fd\u4f53\u4e0e\u5404\u79cd\u7269\u7406\u79ef\u6728\u4ea4\u4e92\u7684\u786c\u4ef6\u52a0\u901f\u6a21\u62df\u5668\uff0c\u4ee5\u53ca\u5305\u542b42\u4e2a\u591a\u6837\u5316\u76ee\u6807\u7ed3\u6784\u7684task-suite\uff0c\u8fd9\u4e9b\u7ed3\u6784\u7ecf\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\uff0c\u7528\u4e8e\u6d4b\u8bd5\u5bf9\u7269\u7406\u3001\u6570\u5b66\u548c\u957f\u671f\u89c4\u5212\u7684\u7406\u89e3\u3002\u5728\u8bad\u7ec3\u671f\u95f4\uff0c\u667a\u80fd\u4f53\u5fc5\u987b\u63a2\u7d22\u548c\u5b66\u4e60\u5173\u4e8e\u73af\u5883\u7684\u901a\u7528\u539f\u5219\uff0c\u800c\u6ca1\u6709\u4efb\u4f55\u5916\u90e8\u76d1\u7763\u3002\u5728\u8bc4\u4f30\u671f\u95f4\uff0c\u667a\u80fd\u4f53\u5fc5\u987b\u4ecetask suite\u6784\u5efa\u770b\u4e0d\u89c1\u7684\u76ee\u6807\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8bb8\u591a\u4efb\u52a1\u5bf9\u5f53\u524d\u8fed\u4ee3\u7684\u7b97\u6cd5\u63d0\u51fa\u4e86\u6311\u6218\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u201ctraining wheels\u201d\u534f\u8bae\uff0c\u5176\u4e2d\u667a\u80fd\u4f53\u88ab\u8bad\u7ec3\u548c\u8bc4\u4f30\u4ee5\u6784\u5efa\u6765\u81eatask suite\u7684\u5355\u4e2a\u76ee\u6807\u7ed3\u6784\u3002", "conclusion": "\u6211\u4eec\u63d0\u4f9b\u4e86\u516d\u79cd\u4e0d\u540c\u7b97\u6cd5\u7684\u5355\u6587\u4ef6\u5b9e\u73b0\uff0c\u4f5c\u4e3a\u7814\u7a76\u4eba\u5458\u7684\u53c2\u8003\u70b9\u3002"}}
{"id": "2510.06278", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06278", "abs": "https://arxiv.org/abs/2510.06278", "authors": ["M. Sajid", "Mushir Akhtar", "A. Quadir", "M. Tanveer"], "title": "RVFL-X: A Novel Randomized Network Based on Complex Transformed Real-Valued Tabular Datasets", "comment": null, "summary": "Recent advancements in neural networks, supported by foundational theoretical\ninsights, emphasize the superior representational power of complex numbers.\nHowever, their adoption in randomized neural networks (RNNs) has been limited\ndue to the lack of effective methods for transforming real-valued tabular\ndatasets into complex-valued representations. To address this limitation, we\npropose two methods for generating complex-valued representations from\nreal-valued datasets: a natural transformation and an autoencoder-driven\nmethod. Building on these mechanisms, we propose RVFL-X, a complex-valued\nextension of the random vector functional link (RVFL) network. RVFL-X\nintegrates complex transformations into real-valued datasets while maintaining\nthe simplicity and efficiency of the original RVFL architecture. By leveraging\ncomplex components such as input, weights, and activation functions, RVFL-X\nprocesses complex representations and produces real-valued outputs.\nComprehensive evaluations on 80 real-valued UCI datasets demonstrate that\nRVFL-X consistently outperforms both the original RVFL and state-of-the-art\n(SOTA) RNN variants, showcasing its robustness and effectiveness across diverse\napplication domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRVFL-X\u7684\u590d\u503c\u968f\u673a\u5411\u91cf\u51fd\u6570\u94fe\u63a5\u7f51\u7edc\uff0c\u5b83\u6269\u5c55\u4e86\u4f20\u7edf\u7684RVFL\u7f51\u7edc\uff0c\u5229\u7528\u590d\u6570\u8868\u793a\u6765\u5904\u7406\u5b9e\u503c\u6570\u636e\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u590d\u6570\u795e\u7ecf\u7f51\u7edc\u5177\u6709\u5f3a\u5927\u7684\u8868\u793a\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u5c06\u5b9e\u503c\u8868\u683c\u6570\u636e\u8f6c\u6362\u4e3a\u590d\u503c\u8868\u793a\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u9650\u5236\u4e86\u5176\u5728\u968f\u673a\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u4ece\u5b9e\u503c\u6570\u636e\u96c6\u751f\u6210\u590d\u503c\u8868\u793a\u7684\u65b9\u6cd5\uff1a\u81ea\u7136\u53d8\u6362\u548c\u81ea\u7f16\u7801\u5668\u9a71\u52a8\u65b9\u6cd5\u3002\u57fa\u4e8e\u8fd9\u4e9b\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86RVFL-X\u7f51\u7edc\uff0c\u5b83\u5c06\u590d\u6570\u53d8\u6362\u96c6\u6210\u5230\u5b9e\u503c\u6570\u636e\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u539f\u59cbRVFL\u67b6\u6784\u7684\u7b80\u5355\u6027\u548c\u6548\u7387\u3002", "result": "\u572880\u4e2aUCI\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cRVFL-X\u59cb\u7ec8\u4f18\u4e8e\u539f\u59cbRVFL\u548c\u6700\u5148\u8fdb\u7684RNN\u53d8\u4f53\u3002", "conclusion": "RVFL-X\u5728\u5404\u79cd\u5e94\u7528\u9886\u57df\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2510.06242", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06242", "abs": "https://arxiv.org/abs/2510.06242", "authors": ["Subin An", "Yugyeong Ji", "Junyoung Kim", "Heejin Kook", "Yang Lu", "Josh Seltzer"], "title": "Transparent Reference-free Automated Evaluation of Open-Ended User Survey Responses", "comment": "EMNLP Industry Track", "summary": "Open-ended survey responses provide valuable insights in marketing research,\nbut low-quality responses not only burden researchers with manual filtering but\nalso risk leading to misleading conclusions, underscoring the need for\neffective evaluation. Existing automatic evaluation methods target\nLLM-generated text and inadequately assess human-written responses with their\ndistinct characteristics. To address such characteristics, we propose a\ntwo-stage evaluation framework specifically designed for human survey\nresponses. First, gibberish filtering removes nonsensical responses. Then,\nthree dimensions-effort, relevance, and completeness-are evaluated using LLM\ncapabilities, grounded in empirical analysis of real-world survey data.\nValidation on English and Korean datasets shows that our framework not only\noutperforms existing metrics but also demonstrates high practical applicability\nfor real-world applications such as response quality prediction and response\nrejection, showing strong correlations with expert assessment.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u95e8\u4e3a\u4eba\u5de5\u8c03\u67e5\u56de\u590d\u8bbe\u8ba1\u7684\u4e24\u9636\u6bb5\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u8bc4\u4f30\u4eba\u5de5\u64b0\u5199\u7684\u56de\u590d\uff0c\u56e0\u4e3a\u5b83\u4eec\u6709\u5176\u72ec\u7279\u7684\u7279\u5f81\u3002\u4f4e\u8d28\u91cf\u7684\u56de\u590d\u4f1a\u7ed9\u7814\u7a76\u4eba\u5458\u5e26\u6765\u624b\u52a8\u8fc7\u6ee4\u7684\u8d1f\u62c5\uff0c\u5e76\u4e14\u53ef\u80fd\u5bfc\u81f4\u8bef\u5bfc\u6027\u7684\u7ed3\u8bba\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u8be5\u6846\u67b6\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff1a\u9996\u5148\uff0c\u4f7f\u7528\u4e71\u8bed\u8fc7\u6ee4\u6765\u79fb\u9664\u65e0\u610f\u4e49\u7684\u56de\u590d\uff1b\u7136\u540e\uff0c\u57fa\u4e8e\u5bf9\u771f\u5b9e\u4e16\u754c\u8c03\u67e5\u6570\u636e\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u4f7f\u7528LLM\u80fd\u529b\u8bc4\u4f30\u4e09\u4e2a\u7ef4\u5ea6\uff1a\u52aa\u529b\u7a0b\u5ea6\u3001\u76f8\u5173\u6027\u548c\u5b8c\u6574\u6027\u3002", "result": "\u5728\u82f1\u8bed\u548c\u97e9\u8bed\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u6846\u67b6\u4e0d\u4ec5\u4f18\u4e8e\u73b0\u6709\u7684\u6307\u6807\uff0c\u800c\u4e14\u5728\u8bf8\u5982\u56de\u590d\u8d28\u91cf\u9884\u6d4b\u548c\u56de\u590d\u62d2\u7edd\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u4e5f\u663e\u793a\u51fa\u5f88\u9ad8\u7684\u5b9e\u7528\u6027\uff0c\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u663e\u793a\u51fa\u5f88\u5f3a\u7684\u76f8\u5173\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u5b9e\u7528\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4eba\u5de5\u8c03\u67e5\u56de\u590d\u7684\u8d28\u91cf\uff0c\u53ef\u4ee5\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u56de\u590d\u5e76\u63d0\u9ad8\u7814\u7a76\u7ed3\u679c\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2510.06732", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.06732", "abs": "https://arxiv.org/abs/2510.06732", "authors": ["Tiancheng Xing", "Jerry Li", "Yixuan Du", "Xiyang Hu"], "title": "Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization", "comment": "10 pages, 3 figures", "summary": "Large language models (LLMs) are increasingly used as rerankers in\ninformation retrieval, yet their ranking behavior can be steered by small,\nnatural-sounding prompts. To expose this vulnerability, we present Rank\nAnything First (RAF), a two-stage token optimization method that crafts concise\ntextual perturbations to consistently promote a target item in LLM-generated\nrankings while remaining hard to detect. Stage 1 uses Greedy Coordinate\nGradient to shortlist candidate tokens at the current position by combining the\ngradient of the rank-target with a readability score; Stage 2 evaluates those\ncandidates under exact ranking and readability losses using an entropy-based\ndynamic weighting scheme, and selects a token via temperature-controlled\nsampling. RAF generates ranking-promoting prompts token-by-token, guided by\ndual objectives: maximizing ranking effectiveness and preserving linguistic\nnaturalness. Experiments across multiple LLMs show that RAF significantly\nboosts the rank of target items using naturalistic language, with greater\nrobustness than existing methods in both promoting target items and maintaining\nnaturalness. These findings underscore a critical security implication:\nLLM-based reranking is inherently susceptible to adversarial manipulation,\nraising new challenges for the trustworthiness and robustness of modern\nretrieval systems. Our code is available at: https://github.com/glad-lab/RAF.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u7528\u4f5c\u91cd\u6392\u5e8f\u5668\uff0c\u4f46\u5b83\u4eec\u6392\u5e8f\u884c\u4e3a\u5bb9\u6613\u53d7\u5230\u63d0\u793a\u8bcd\u7684\u5f71\u54cd\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Rank Anything First (RAF) \u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6587\u672c\u6270\u52a8\u6765\u63d0\u5347\u76ee\u6807\u9879\u76ee\u5728 LLM \u751f\u6210\u7684\u6392\u540d\u4e2d\u7684\u4f4d\u7f6e\uff0c\u5e76\u4e14\u96be\u4ee5\u88ab\u68c0\u6d4b\u3002", "motivation": "\u63ed\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u91cd\u6392\u5e8f\u5668\u65f6\uff0c\u5176\u6392\u5e8f\u884c\u4e3a\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u63d0\u793a\u8bcd\u7684\u5f71\u54cd\u8fd9\u4e00\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u63d0\u51fa Rank Anything First (RAF) \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u53cc\u9636\u6bb5\u4ee4\u724c\u4f18\u5316\uff0c\u751f\u6210\u65e2\u80fd\u6709\u6548\u63d0\u5347\u76ee\u6807\u9879\u76ee\u6392\u540d\uff0c\u53c8\u80fd\u4fdd\u6301\u8bed\u8a00\u81ea\u7136\u6027\u7684\u63d0\u793a\u8bcd\u3002\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u8d2a\u5a6a\u5750\u6807\u68af\u5ea6\u6765\u9009\u62e9\u5019\u9009\u4ee4\u724c\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u57fa\u4e8e\u71b5\u7684\u52a8\u6001\u52a0\u6743\u65b9\u6848\u8bc4\u4f30\u5019\u9009\u4ee4\u724c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRAF \u80fd\u591f\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u663e\u8457\u63d0\u5347\u76ee\u6807\u9879\u76ee\u7684\u6392\u540d\uff0c\u5e76\u4e14\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e LLM \u7684\u91cd\u6392\u5e8f\u672c\u8d28\u4e0a\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u64cd\u7eb5\uff0c\u8fd9\u5bf9\u73b0\u4ee3\u68c0\u7d22\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u548c\u9c81\u68d2\u6027\u63d0\u51fa\u4e86\u65b0\u7684\u6311\u6218\u3002"}}
{"id": "2510.07062", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.07062", "abs": "https://arxiv.org/abs/2510.07062", "authors": ["Hadar Rotschield", "Liat Peterfreund"], "title": "On the Expressiveness of Languages for Querying Property Graphs in Relational Databases", "comment": null, "summary": "SQL/PGQ is the emerging ISO standard for querying property graphs defined as\nviews over relational data. We formalize its expressive power across three\nfragments: the read-only core, the read-write extension, and an extended\nvariant with richer view definitions. Our results show that graph creation\nplays a central role in determining the expressiveness. The read-only fragment\nis strictly weaker than the read-write fragment, and the latter is still below\nthe complexity class NL. Extending view definitions with arbitrary arity\nidentifiers closes this gap: the extended fragment captures exactly NL. This\nyields a strict hierarchy of SQL/PGQ fragments, whose union covers all NL\nqueries. On ordered structures the hierarchy collapses: once arity-2\nidentifiers are allowed, higher arities add no power, mirroring the classical\ntransitive-closure collapse and underscoring the central role of view\nconstruction in property graph querying.", "AI": {"tldr": "SQL/PGQ\u662f\u67e5\u8be2\u5173\u7cfb\u6570\u636e\u89c6\u56fe\u4e0a\u5b9a\u4e49\u7684\u5c5e\u6027\u56fe\u7684\u65b0\u5174ISO\u6807\u51c6\u3002\u672c\u6587\u7814\u7a76\u4e86\u5176\u5728\u4e09\u4e2a\u90e8\u5206\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\uff1a\u53ea\u8bfb\u6838\u5fc3\u3001\u8bfb\u5199\u6269\u5c55\u548c\u5177\u6709\u66f4\u4e30\u5bcc\u7684\u89c6\u56fe\u5b9a\u4e49\u7684\u6269\u5c55\u53d8\u4f53\u3002", "motivation": "\u56fe\u521b\u5efa\u5728\u51b3\u5b9a\u8868\u8fbe\u80fd\u529b\u65b9\u9762\u8d77\u7740\u6838\u5fc3\u4f5c\u7528\u3002", "method": "\u5f62\u5f0f\u5316\u4e86SQL/PGQ\u5728\u4e09\u4e2a\u90e8\u5206\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u53ea\u8bfb\u7247\u6bb5\u4e25\u683c\u5f31\u4e8e\u8bfb\u5199\u7247\u6bb5\uff0c\u540e\u8005\u4ecd\u4f4e\u4e8e\u590d\u6742\u6027\u7c7b\u522bNL\u3002\u4f7f\u7528\u4efb\u610farity\u6807\u8bc6\u7b26\u6269\u5c55\u89c6\u56fe\u5b9a\u4e49\u7f29\u5c0f\u4e86\u8fd9\u4e00\u5dee\u8ddd\uff1a\u6269\u5c55\u7684\u7247\u6bb5\u7cbe\u786e\u5730\u6355\u83b7\u4e86NL\u3002\u8fd9\u4ea7\u751f\u4e86\u4e00\u4e2aSQL/PGQ\u7247\u6bb5\u7684\u4e25\u683c\u5c42\u6b21\u7ed3\u6784\uff0c\u5176\u8054\u5408\u8986\u76d6\u4e86\u6240\u6709NL\u67e5\u8be2\u3002\u5728\u6709\u5e8f\u7ed3\u6784\u4e0a\uff0c\u5c42\u6b21\u7ed3\u6784\u5d29\u6e83\uff1a\u4e00\u65e6\u5141\u8bb8arity-2\u6807\u8bc6\u7b26\uff0c\u66f4\u9ad8\u7684arity\u4e0d\u4f1a\u589e\u52a0\u4efb\u4f55\u80fd\u529b\uff0c\u8fd9\u53cd\u6620\u4e86\u7ecf\u5178\u7684\u4f20\u9012\u95ed\u5305\u5d29\u6e83\uff0c\u5e76\u5f3a\u8c03\u4e86\u89c6\u56fe\u6784\u5efa\u5728\u5c5e\u6027\u56fe\u67e5\u8be2\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\u3002", "conclusion": "SQL/PGQ\u7247\u6bb5\u5b58\u5728\u4e00\u4e2a\u4e25\u683c\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u5176\u8054\u5408\u8986\u76d6\u4e86\u6240\u6709NL\u67e5\u8be2\u3002"}}
{"id": "2510.06238", "categories": ["cs.CV", "cs.AI", "cs.LG", "stat.OT"], "pdf": "https://arxiv.org/pdf/2510.06238", "abs": "https://arxiv.org/abs/2510.06238", "authors": ["Sagar Lekhak", "Emmett J. Ientilucci", "Dimah Dera", "Susmita Ghosh"], "title": "Uncertainty Quantification In Surface Landmines and UXO Classification Using MC Dropout", "comment": "This work has been accepted and presented at IGARSS 2025 and will\n  appear in the IEEE IGARSS 2025 proceedings", "summary": "Detecting surface landmines and unexploded ordnances (UXOs) using deep\nlearning has shown promise in humanitarian demining. However, deterministic\nneural networks can be vulnerable to noisy conditions and adversarial attacks,\nleading to missed detection or misclassification. This study introduces the\nidea of uncertainty quantification through Monte Carlo (MC) Dropout, integrated\ninto a fine-tuned ResNet-50 architecture for surface landmine and UXO\nclassification, which was tested on a simulated dataset. Integrating the MC\nDropout approach helps quantify epistemic uncertainty, providing an additional\nmetric for prediction reliability, which could be helpful to make more informed\ndecisions in demining operations. Experimental results on clean, adversarially\nperturbed, and noisy test images demonstrate the model's ability to flag\nunreliable predictions under challenging conditions. This proof-of-concept\nstudy highlights the need for uncertainty quantification in demining, raises\nawareness about the vulnerability of existing neural networks in demining to\nadversarial threats, and emphasizes the importance of developing more robust\nand reliable models for practical applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528 Monte Carlo (MC) Dropout \u7ed3\u5408 ResNet-50 \u67b6\u6784\u8fdb\u884c\u8868\u9762\u5730\u96f7\u548c\u672a\u7206\u5f39\u836f (UXO) \u5206\u7c7b\uff0c\u4ee5\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u5e76\u63d0\u9ad8\u9884\u6d4b\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5730\u96f7\u68c0\u6d4b\u4e2d\u5bb9\u6613\u53d7\u5230\u566a\u58f0\u548c\u5bf9\u6297\u653b\u51fb\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u6f0f\u68c0\u6216\u8bef\u5206\u7c7b\u3002", "method": "\u5c06 MC Dropout \u96c6\u6210\u5230\u5fae\u8c03\u7684 ResNet-50 \u67b6\u6784\u4e2d\uff0c\u7528\u4e8e\u8868\u9762\u5730\u96f7\u548c UXO \u5206\u7c7b\uff0c\u5e76\u5728\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u6807\u8bb0\u5177\u6709\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u4e0d\u53ef\u9760\u7684\u9884\u6d4b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5730\u96f7\u6e05\u9664\u4e2d\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u53ef\u9760\u6a21\u578b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.06302", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06302", "abs": "https://arxiv.org/abs/2510.06302", "authors": ["Ksenija Lace", "Marite Kirikova"], "title": "Requirements for Game-Based Learning Design Framework for Information System Integration in the Context of Post-Merger Integration", "comment": null, "summary": "Post-merger integration states unique challenges for professionals\nresponsible for information system integration aimed on alignment and\ncombination diverse system architectures of merging organizations. Although the\ntheoretical and practical guidance exists for post-merger integration on the\nbusiness level, there is a significant gap in training for information system\nintegration in this context. In prior research specific methods AMILI (Support\nmethod for informed decision identification) and AMILP (Support method for\ninformed decision-making) were introduced for the support of information system\nintegration decisions in the post-merger integration. But during the practical\napplication was reported high learning curve and low learner motivation. This\npaper explores how game-based learning design can address these limitations by\ntransforming static method training into engaging learning experience. The\nstudy analyzes foundational learning theories, cognitive load and motivation\nmodels, and serious game design frameworks to identify the essential\nrequirements for a game-based learning design framework tailored to information\nsystem integration in post-merger integration. Requirements are structured in\ntwo components: the transformation process and resulting learning experience.\nThe paper concludes with a plan for developing and evaluating the proposed\nframework through iterative design and real-world validation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u6e38\u620f\u5316\u5b66\u4e60\u8bbe\u8ba1\u6765\u6539\u8fdb\u4fe1\u606f\u7cfb\u7edf\u96c6\u6210\u57f9\u8bad\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5b66\u4e60\u66f2\u7ebf\u9ad8\u548c\u5b66\u4e60\u52a8\u673a\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u5e76\u8d2d\u540e\u7684\u4fe1\u606f\u7cfb\u7edf\u96c6\u6210\u9762\u4e34\u72ec\u7279\u7684\u6311\u6218\uff0c\u4f46\u76f8\u5173\u57f9\u8bad\u4e0d\u8db3\u3002\u73b0\u6709\u65b9\u6cd5AMILI\u548cAMILP\u5b58\u5728\u5b66\u4e60\u66f2\u7ebf\u9ad8\u548c\u5b66\u4e60\u52a8\u673a\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5b66\u4e60\u7406\u8bba\u3001\u8ba4\u77e5\u8d1f\u8377\u548c\u52a8\u673a\u6a21\u578b\u4ee5\u53ca\u4e25\u8083\u6e38\u620f\u8bbe\u8ba1\u6846\u67b6\uff0c\u786e\u5b9a\u4e86\u9488\u5bf9\u5e76\u8d2d\u540e\u4fe1\u606f\u7cfb\u7edf\u96c6\u6210\u7684\u6e38\u620f\u5316\u5b66\u4e60\u8bbe\u8ba1\u6846\u67b6\u7684\u57fa\u672c\u8981\u6c42\u3002\u8981\u6c42\u5206\u4e3a\u4e24\u4e2a\u90e8\u5206\uff1a\u8f6c\u578b\u8fc7\u7a0b\u548c\u6700\u7ec8\u5b66\u4e60\u4f53\u9a8c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u4e3a\u6e38\u620f\u5316\u5b66\u4e60\u8bbe\u8ba1\u6846\u67b6\u63d0\u4f9b\u4e86\u57fa\u672c\u8981\u6c42\uff0c\u5e76\u5c06\u5176\u6784\u5efa\u4e3a\u8f6c\u578b\u8fc7\u7a0b\u548c\u5b66\u4e60\u4f53\u9a8c\u4e24\u4e2a\u7ec4\u6210\u90e8\u5206\u3002", "conclusion": "\u8bba\u6587\u6700\u540e\u63d0\u51fa\u4e86\u901a\u8fc7\u8fed\u4ee3\u8bbe\u8ba1\u548c\u5b9e\u9645\u9a8c\u8bc1\u6765\u5f00\u53d1\u548c\u8bc4\u4f30\u6240\u63d0\u51fa\u7684\u6846\u67b6\u7684\u8ba1\u5212\u3002"}}
{"id": "2510.06284", "categories": ["cs.LG", "cs.CV", "math.GT", "Primary: 57K10, 68T07, secondary: 57K14, 68T45"], "pdf": "https://arxiv.org/pdf/2510.06284", "abs": "https://arxiv.org/abs/2510.06284", "authors": ["Anne Dranowski", "Yura Kabkov", "Daniel Tubbenhauer"], "title": "On knot detection via picture recognition", "comment": "21 pages, many figures, comments welcome", "summary": "Our goal is to one day take a photo of a knot and have a phone automatically\nrecognize it. In this expository work, we explain a strategy to approximate\nthis goal, using a mixture of modern machine learning methods (in particular\nconvolutional neural networks and transformers for image recognition) and\ntraditional algorithms (to compute quantum invariants like the Jones\npolynomial). We present simple baselines that predict crossing number directly\nfrom images, showing that even lightweight CNN and transformer architectures\ncan recover meaningful structural information. The longer-term aim is to\ncombine these perception modules with symbolic reconstruction into planar\ndiagram (PD) codes, enabling downstream invariant computation for robust knot\nclassification. This two-stage approach highlights the complementarity between\nmachine learning, which handles noisy visual data, and invariants, which\nenforce rigorous topological distinctions.", "AI": {"tldr": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548ctransformers\uff09\u548c\u4f20\u7edf\u7b97\u6cd5\uff08\u8ba1\u7b97\u743c\u65af\u591a\u9879\u5f0f\u7b49\u91cf\u5b50\u4e0d\u53d8\u91cf\uff09\u76f8\u7ed3\u5408\uff0c\u4ee5\u5b9e\u73b0\u4ece\u7ed3\u7684\u7167\u7247\u81ea\u52a8\u8bc6\u522b\u7ed3\u7684\u76ee\u6807\u3002", "motivation": "\u6700\u7ec8\u76ee\u6807\u662f\u62cd\u6444\u4e00\u4e2a\u7ed3\u7684\u7167\u7247\uff0c\u5e76\u8ba9\u624b\u673a\u81ea\u52a8\u8bc6\u522b\u5b83\u3002\u8fd9\u7bc7\u89e3\u91ca\u6027\u6587\u7ae0\u89e3\u91ca\u4e86\u4e00\u79cd\u8fd1\u4f3c\u5b9e\u73b0\u6b64\u76ee\u6807\u7684\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u7279\u522b\u662f\u7528\u4e8e\u56fe\u50cf\u8bc6\u522b\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548ctransformers\uff09\u548c\u4f20\u7edf\u7b97\u6cd5\uff08\u8ba1\u7b97\u743c\u65af\u591a\u9879\u5f0f\u7b49\u91cf\u5b50\u4e0d\u53d8\u91cf\uff09\u3002", "result": "\u63d0\u51fa\u4e86\u76f4\u63a5\u4ece\u56fe\u50cf\u9884\u6d4b\u4ea4\u53c9\u6570\u7684\u7b80\u5355\u57fa\u7ebf\uff0c\u8868\u660e\u5373\u4f7f\u662f\u8f7b\u91cf\u7ea7\u7684CNN\u548ctransformer\u67b6\u6784\u4e5f\u53ef\u4ee5\u6062\u590d\u6709\u610f\u4e49\u7684\u7ed3\u6784\u4fe1\u606f\u3002", "conclusion": "\u7ed3\u5408\u611f\u77e5\u6a21\u5757\u4e0e\u5e73\u9762\u56fe\uff08PD\uff09\u4ee3\u7801\u7684\u7b26\u53f7\u91cd\u5efa\uff0c\u5b9e\u73b0\u4e0b\u6e38\u4e0d\u53d8\u8ba1\u7b97\uff0c\u4ece\u800c\u5b9e\u73b0\u9c81\u68d2\u7684\u7ed3\u5206\u7c7b\u3002\u8fd9\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5\u7a81\u51fa\u4e86\u673a\u5668\u5b66\u4e60\uff08\u5904\u7406\u5608\u6742\u7684\u89c6\u89c9\u6570\u636e\uff09\u548c\u4e0d\u53d8\u91cf\uff08\u5f3a\u5236\u6267\u884c\u4e25\u683c\u7684\u62d3\u6251\u533a\u5206\uff09\u4e4b\u95f4\u7684\u4e92\u8865\u6027\u3002"}}
{"id": "2510.06243", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06243", "abs": "https://arxiv.org/abs/2510.06243", "authors": ["Qihua Dong", "Luis Figueroa", "Handong Zhao", "Kushal Kafle", "Jason Kuen", "Zhihong Ding", "Scott Cohen", "Yun Fu"], "title": "CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning", "comment": "MLLM, Referring Expression Segmentation", "summary": "Referring Expression Comprehension and Segmentation are critical tasks for\nassessing the integration of language understanding and image comprehension,\nserving as benchmarks for Multimodal Large Language Models (MLLMs)\ncapabilities. To address these challenges, we propose a new strategy, CoT\nReferring, which enhances model reasoning across modalities through a\nstructured, chain-of-thought training data structure. Our approach\nsystematically parses textual structures to a sequential referring step, where\nin each step it identifies relationships and ensures consistent reference\nalignment, thereby improving accuracy in complex query scenarios. We\nrestructure the training data to enforce a new output form, providing new\nannotations for existing datasets and compiling an evaluation benchmark from\nexisting resources. This benchmark is designed explicitly for complex referring\ncases. We also integrate detection and segmentation capabilities into a unified\nMLLM framework, training it with a novel adaptive weighted loss to optimize\nperformance. Experimental results on our curated benchmark and RefCOCO/+/g\ndemonstrate the effectiveness of our approach, with a notable increase of 2.5%+\nover baseline models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoT Referring\u7684\u65b0\u7b56\u7565\uff0c\u65e8\u5728\u63d0\u9ad8\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u8be5\u7b56\u7565\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u601d\u7ef4\u94fe\u8bad\u7ec3\u6570\u636e\uff0c\u589e\u5f3a\u6a21\u578b\u5728\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6574\u5408\u8bed\u8a00\u7406\u89e3\u548c\u56fe\u50cf\u7406\u89e3\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u3002", "method": "\u7cfb\u7edf\u5730\u89e3\u6790\u6587\u672c\u7ed3\u6784\u5230\u8fde\u7eed\u7684\u6307\u4ee3\u6b65\u9aa4\uff0c\u8bc6\u522b\u5173\u7cfb\u5e76\u786e\u4fdd\u4e00\u81f4\u7684\u53c2\u8003\u5bf9\u9f50\u3002\u540c\u65f6\uff0c\u91cd\u6784\u8bad\u7ec3\u6570\u636e\uff0c\u5f3a\u5236\u65b0\u7684\u8f93\u51fa\u5f62\u5f0f\uff0c\u5e76\u6574\u5408\u68c0\u6d4b\u548c\u5206\u5272\u80fd\u529b\u5230\u4e00\u4e2a\u7edf\u4e00\u7684MLLM\u6846\u67b6\u4e2d\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u52a0\u6743\u635f\u5931\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u81ea\u5efa\u7684benchmark\u548cRefCOCO/+/g\u6570\u636e\u96c6\u4e0a\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u67092.5%+\u7684\u663e\u8457\u63d0\u5347\u3002", "conclusion": "CoT Referring\u7b56\u7565\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u6307\u4ee3\u60c5\u51b5\u4e0b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.06805", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.06805", "abs": "https://arxiv.org/abs/2510.06805", "authors": ["Andr\u00e9 Greiner-Petter", "Maik Fr\u00f6be", "Jan Philip Wahle", "Terry Ruas", "Bela Gipp", "Akiko Aizawa", "Martin Potthast"], "title": "Overview of the Plagiarism Detection Task at PAN 2025", "comment": "Working Notes at PAN at CLEF 2025", "summary": "The generative plagiarism detection task at PAN 2025 aims at identifying\nautomatically generated textual plagiarism in scientific articles and aligning\nthem with their respective sources. We created a novel large-scale dataset of\nautomatically generated plagiarism using three large language models: Llama,\nDeepSeek-R1, and Mistral. In this task overview paper, we outline the creation\nof this dataset, summarize and compare the results of all participants and four\nbaselines, and evaluate the results on the last plagiarism detection task from\nPAN 2015 in order to interpret the robustness of the proposed approaches. We\nfound that the current iteration does not invite a large variety of approaches\nas naive semantic similarity approaches based on embedding vectors provide\npromising results of up to 0.8 recall and 0.5 precision. In contrast, most of\nthese approaches underperform significantly on the 2015 dataset, indicating a\nlack in generalizability.", "AI": {"tldr": "PAN 2025 \u65e8\u5728\u68c0\u6d4b\u79d1\u5b66\u6587\u7ae0\u4e2d\u81ea\u52a8\u751f\u6210\u7684\u6284\u88ad\u6587\u672c\uff0c\u5e76\u5c06\u5176\u4e0e\u6765\u6e90\u5bf9\u9f50\u3002\u4ed6\u4eec\u4f7f\u7528 Llama\u3001DeepSeek-R1 \u548c Mistral \u7b49\u5927\u578b\u8bed\u8a00\u6a21\u578b\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u5927\u89c4\u6a21\u81ea\u52a8\u751f\u6210\u6284\u88ad\u6570\u636e\u96c6\u3002", "motivation": "\u4e3a\u4e86\u8bc6\u522b\u79d1\u5b66\u6587\u7ae0\u4e2d\u81ea\u52a8\u751f\u6210\u7684\u6284\u88ad\u6587\u672c\uff0c\u5e76\u5c06\u5176\u4e0e\u5404\u81ea\u7684\u6765\u6e90\u5bf9\u9f50\u3002", "method": "\u4f7f\u7528 Llama\u3001DeepSeek-R1 \u548c Mistral \u7b49\u5927\u578b\u8bed\u8a00\u6a21\u578b\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u5927\u89c4\u6a21\u81ea\u52a8\u751f\u6210\u6284\u88ad\u6570\u636e\u96c6\u3002\u6982\u8ff0\u4e86\u6570\u636e\u96c6\u7684\u521b\u5efa\uff0c\u603b\u7ed3\u5e76\u6bd4\u8f83\u4e86\u6240\u6709\u53c2\u4e0e\u8005\u548c\u56db\u4e2a\u57fa\u7ebf\u7684\u7ed3\u679c\uff0c\u5e76\u5728 PAN 2015 \u7684\u6700\u540e\u4e00\u9879\u6284\u88ad\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8bc4\u4f30\u4e86\u7ed3\u679c\uff0c\u4ee5\u89e3\u91ca\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u7a33\u5065\u6027\u3002", "result": "\u57fa\u4e8e\u5d4c\u5165\u5411\u91cf\u7684\u6734\u7d20\u8bed\u4e49\u76f8\u4f3c\u5ea6\u65b9\u6cd5\u63d0\u4f9b\u4e86\u9ad8\u8fbe 0.8 \u7684\u53ec\u56de\u7387\u548c 0.5 \u7684\u7cbe\u786e\u7387\u3002\u5927\u591a\u6570\u8fd9\u4e9b\u65b9\u6cd5\u5728 2015 \u5e74\u7684\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u660e\u663e\u4e0d\u4f73\uff0c\u8868\u660e\u7f3a\u4e4f\u6cdb\u5316\u6027\u3002", "conclusion": "\u5f53\u524d\u7684\u65b9\u6cd5\u6ca1\u6709\u5f15\u5165\u5404\u79cd\u5404\u6837\u7684\u65b9\u6cd5\uff0c\u56e0\u4e3a\u57fa\u4e8e\u5d4c\u5165\u5411\u91cf\u7684\u6734\u7d20\u8bed\u4e49\u76f8\u4f3c\u5ea6\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u4f46\u7f3a\u4e4f\u6cdb\u5316\u6027\u3002"}}
{"id": "2510.06241", "categories": ["cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2510.06241", "abs": "https://arxiv.org/abs/2510.06241", "authors": ["Anselm W. Stark", "Marc Ilic", "Ali Mokhtari", "Pooya Mohammadi Kazaj", "Christoph Graeni", "Isaac Shiri"], "title": "multimodars: A Rust-powered toolkit for multi-modality cardiac image fusion and registration", "comment": null, "summary": "Combining complementary imaging modalities is critical to build reliable 3D\ncoronary models: intravascular imaging gives sub-millimetre resolution but\nlimited whole-vessel context, while CCTA supplies 3D geometry but suffers from\nlimited spatial resolution and artefacts (e.g., blooming). Prior work\ndemonstrated intravascular/CCTA fusion, yet no open, flexible toolkit is\ntailored for multi-state analysis (rest/stress, pre-/post-stenting) while\noffering deterministic behaviour, high performance, and easy pipeline\nintegration. multimodars addresses this gap with deterministic alignment\nalgorithms, a compact NumPy-centred data model, and an optimised Rust backend\nsuitable for scalable, reproducible experiments. The package accepts CSV/NumPy\ninputs including data formats produced by the AIVUS-CAA software", "AI": {"tldr": "multimodars is an open, flexible toolkit for multi-state coronary artery analysis using intravascular imaging and CCTA fusion.", "motivation": "Combining intravascular imaging and CCTA is critical for reliable 3D coronary models, but existing solutions lack open, flexible toolkits for multi-state analysis.", "method": "Deterministic alignment algorithms, a compact NumPy-centred data model, and an optimised Rust backend.", "result": "The package accepts CSV/NumPy inputs including data formats produced by the AIVUS-CAA software.", "conclusion": "multimodars addresses the gap in multi-state coronary artery analysis by offering deterministic behavior, high performance, and easy pipeline integration."}}
{"id": "2510.06307", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06307", "abs": "https://arxiv.org/abs/2510.06307", "authors": ["Wentao Deng", "Jiahuan Pei", "Zhiwei Xu", "Zhaochun Ren", "Zhumin Chen", "Pengjie Ren"], "title": "Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks", "comment": "This paper has been accepted by NeurIPS 2025", "summary": "A multi-agent system (MAS) enhances its capacity to solve complex natural\nlanguage processing (NLP) tasks through collaboration among multiple agents,\nwhere consensus-seeking serves as a fundamental mechanism. However, existing\nconsensus-seeking approaches typically rely on voting mechanisms to judge\nconsensus, overlooking contradictions in system-internal beliefs that\ndestabilize the consensus. Moreover, these methods often involve agents\nupdating their results through indiscriminate collaboration with every other\nagent. Such uniform interaction fails to identify the optimal collaborators for\neach agent, hindering the emergence of a stable consensus. To address these\nchallenges, we provide a theoretical framework for selecting optimal\ncollaborators that maximize consensus stability. Based on the theorems, we\npropose the Belief-Calibrated Consensus Seeking (BCCS) framework to facilitate\nstable consensus via selecting optimal collaborators and calibrating the\nconsensus judgment by system-internal beliefs. Experimental results on the MATH\nand MMLU benchmark datasets demonstrate that the proposed BCCS framework\noutperforms the best existing results by 2.23% and 3.95% of accuracy on\nchallenging tasks, respectively. Our code and data are available at\nhttps://github.com/dengwentao99/BCCS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5171\u8bc6\u5bfb\u6c42\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6700\u4f18\u5408\u4f5c\u8005\u548c\u6821\u51c6\u7cfb\u7edf\u5185\u90e8\u4fe1\u5ff5\u6765\u4fc3\u8fdb\u7a33\u5b9a\u5171\u8bc6\u3002", "motivation": "\u73b0\u6709\u7684\u5171\u8bc6\u5bfb\u6c42\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u6295\u7968\u673a\u5236\u6765\u5224\u65ad\u5171\u8bc6\uff0c\u5ffd\u7565\u4e86\u7cfb\u7edf\u5185\u90e8\u4fe1\u5ff5\u4e2d\u7684\u77db\u76fe\uff0c\u5e76\u4e14\u901a\u5e38\u6d89\u53ca\u4ee3\u7406\u901a\u8fc7\u4e0e\u6bcf\u4e2a\u5176\u4ed6\u4ee3\u7406\u8fdb\u884c\u65e0\u5dee\u522b\u7684\u534f\u4f5c\u6765\u66f4\u65b0\u5176\u7ed3\u679c\uff0c\u672a\u80fd\u8bc6\u522b\u6bcf\u4e2a\u4ee3\u7406\u7684\u6700\u4f73\u5408\u4f5c\u8005\uff0c\u963b\u788d\u4e86\u7a33\u5b9a\u5171\u8bc6\u7684\u51fa\u73b0\u3002", "method": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9009\u62e9\u6700\u5927\u5316\u5171\u8bc6\u7a33\u5b9a\u6027\u7684\u6700\u4f18\u5408\u4f5c\u8005\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u4fe1\u5ff5\u6821\u51c6\u5171\u8bc6\u5bfb\u6c42\uff08BCCS\uff09\u6846\u67b6\u3002", "result": "\u5728MATH\u548cMMLU\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684BCCS\u6846\u67b6\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u6027\u5206\u522b\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u7ed3\u679c2.23%\u548c3.95%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684BCCS\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u63d0\u9ad8\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.06291", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06291", "abs": "https://arxiv.org/abs/2510.06291", "authors": ["Zhiyang Zhang", "Ningcong Chen", "Xin Zhang", "Yanhua Li", "Shen Su", "Hui Lu", "Jun Luo"], "title": "Traj-Transformer: Diffusion Models with Transformer for GPS Trajectory Generation", "comment": null, "summary": "The widespread use of GPS devices has driven advances in spatiotemporal data\nmining, enabling machine learning models to simulate human decision making and\ngenerate realistic trajectories, addressing both data collection costs and\nprivacy concerns. Recent studies have shown the promise of diffusion models for\nhigh-quality trajectory generation. However, most existing methods rely on\nconvolution based architectures (e.g. UNet) to predict noise during the\ndiffusion process, which often results in notable deviations and the loss of\nfine-grained street-level details due to limited model capacity. In this paper,\nwe propose Trajectory Transformer, a novel model that employs a transformer\nbackbone for both conditional information embedding and noise prediction. We\nexplore two GPS coordinate embedding strategies, location embedding and\nlongitude-latitude embedding, and analyze model performance at different\nscales. Experiments on two real-world datasets demonstrate that Trajectory\nTransformer significantly enhances generation quality and effectively\nalleviates the deviation issues observed in prior approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8f68\u8ff9\u8f6c\u6362\u5668\uff08Trajectory Transformer\uff09\u7684\u65b0\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u91c7\u7528 Transformer \u4e3b\u5e72\u8fdb\u884c\u6761\u4ef6\u4fe1\u606f\u5d4c\u5165\u548c\u566a\u58f0\u9884\u6d4b\uff0c\u4ee5\u63d0\u9ad8\u8f68\u8ff9\u751f\u6210\u8d28\u91cf\u5e76\u6709\u6548\u7f13\u89e3\u73b0\u6709\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u5377\u79ef\u7684\u67b6\u6784\u6765\u9884\u6d4b\u6269\u6563\u8fc7\u7a0b\u4e2d\u7684\u566a\u58f0\uff0c\u7531\u4e8e\u6a21\u578b\u5bb9\u91cf\u6709\u9650\uff0c\u901a\u5e38\u4f1a\u5bfc\u81f4\u660e\u663e\u7684\u504f\u5dee\u548c\u7ec6\u7c92\u5ea6\u7684\u8857\u9053\u7ea7\u522b\u7ec6\u8282\u7684\u4e22\u5931\u3002", "method": "\u63d0\u51fa Trajectory Transformer \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4f7f\u7528 Transformer \u4e3b\u5e72\u8fdb\u884c\u6761\u4ef6\u4fe1\u606f\u5d4c\u5165\u548c\u566a\u58f0\u9884\u6d4b\uff0c\u5e76\u63a2\u7d22\u4e86\u4e24\u79cd GPS \u5750\u6807\u5d4c\u5165\u7b56\u7565\uff1a\u4f4d\u7f6e\u5d4c\u5165\u548c\u7ecf\u7eac\u5ea6\u5d4c\u5165\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTrajectory Transformer \u663e\u7740\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u6709\u6548\u7f13\u89e3\u4e86\u5148\u524d\u65b9\u6cd5\u4e2d\u89c2\u5bdf\u5230\u7684\u504f\u5dee\u95ee\u9898\u3002", "conclusion": "Trajectory Transformer \u6a21\u578b\u53ef\u4ee5\u63d0\u9ad8\u8f68\u8ff9\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u6709\u6548\u7f13\u89e3\u73b0\u6709\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u504f\u5dee\u95ee\u9898\u3002"}}
{"id": "2510.06244", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06244", "abs": "https://arxiv.org/abs/2510.06244", "authors": ["Nouman Ahmed", "Ronin Wu", "Victor Botev"], "title": "Evaluating Embedding Frameworks for Scientific Domain", "comment": null, "summary": "Finding an optimal word representation algorithm is particularly important in\nterms of domain specific data, as the same word can have different meanings and\nhence, different representations depending on the domain and context. While\nGenerative AI and transformer architecture does a great job at generating\ncontextualized embeddings for any given work, they are quite time and compute\nextensive, especially if we were to pre-train such a model from scratch. In\nthis work, we focus on the scientific domain and finding the optimal word\nrepresentation algorithm along with the tokenization method that could be used\nto represent words in the scientific domain. The goal of this research is two\nfold: 1) finding the optimal word representation and tokenization methods that\ncan be used in downstream scientific domain NLP tasks, and 2) building a\ncomprehensive evaluation suite that could be used to evaluate various word\nrepresentation and tokenization algorithms (even as new ones are introduced) in\nthe scientific domain. To this end, we build an evaluation suite consisting of\nseveral downstream tasks and relevant datasets for each task. Furthermore, we\nuse the constructed evaluation suite to test various word representation and\ntokenization algorithms.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u81f4\u529b\u4e8e\u4e3a\u79d1\u5b66\u9886\u57df\u5bfb\u627e\u6700\u4f73\u7684\u8bcd\u8868\u793a\u7b97\u6cd5\u548c\u5206\u8bcd\u65b9\u6cd5\u3002", "motivation": "\u5728\u7279\u5b9a\u9886\u57df\u4e2d\uff0c\u8bcd\u8bed\u5177\u6709\u4e0d\u540c\u7684\u542b\u4e49\u548c\u8868\u793a\uff0c\u56e0\u6b64\u627e\u5230\u9002\u7528\u4e8e\u79d1\u5b66\u9886\u57df\u7684\u8bcd\u8868\u793a\u65b9\u6cd5\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u751f\u6210\u5f0fAI\u548cTransformer\u67b6\u6784\u5728\u751f\u6210\u4e0a\u4e0b\u6587\u5d4c\u5165\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u8017\u65f6\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u5c24\u5176\u662f\u5728\u4ece\u5934\u5f00\u59cb\u9884\u8bad\u7ec3\u6a21\u578b\u65f6\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u7efc\u5408\u8bc4\u4f30\u5957\u4ef6\uff0c\u7528\u4e8e\u8bc4\u4f30\u79d1\u5b66\u9886\u57df\u4e2d\u5404\u79cd\u8bcd\u8868\u793a\u548c\u5206\u8bcd\u7b97\u6cd5\u3002", "result": "\u4f7f\u7528\u6784\u5efa\u7684\u8bc4\u4f30\u5957\u4ef6\u6765\u6d4b\u8bd5\u5404\u79cd\u8bcd\u8868\u793a\u548c\u5206\u8bcd\u7b97\u6cd5\u3002", "conclusion": "\u76ee\u6807\u662f\u627e\u5230\u6700\u4f73\u7684\u8bcd\u8868\u793a\u548c\u5206\u8bcd\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u5957\u4ef6\uff0c\u4ee5\u8bc4\u4f30\u79d1\u5b66\u9886\u57df\u4e2d\u7684\u5404\u79cd\u8bcd\u8868\u793a\u548c\u5206\u8bcd\u7b97\u6cd5\u3002"}}
{"id": "2510.06987", "categories": ["cs.LG", "cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06987", "abs": "https://arxiv.org/abs/2510.06987", "authors": ["Rohith Mahadevan"], "title": "Spiral Model Technique For Data Science & Machine Learning Lifecycle", "comment": null, "summary": "Analytics play an important role in modern business. Companies adapt data\nscience lifecycles to their culture to seek productivity and improve their\ncompetitiveness among others. Data science lifecycles are fairly an important\ncontributing factor to start and end a project that are data dependent. Data\nscience and Machine learning life cycles comprises of series of steps that are\ninvolved in a project. A typical life cycle states that it is a linear or\ncyclical model that revolves around. It is mostly depicted that it is possible\nin a traditional data science life cycle to start the process again after\nreaching the end of cycle. This paper suggests a new technique to incorporate\ndata science life cycle to business problems that have a clear end goal. A new\ntechnique called spiral technique is introduced to emphasize versatility,\nagility and iterative approach to business processes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u79d1\u5b66\u751f\u547d\u5468\u671f\u6280\u672f\uff0c\u79f0\u4e3a\u87ba\u65cb\u6280\u672f\uff0c\u4ee5\u5f3a\u8c03\u4e1a\u52a1\u6d41\u7a0b\u7684\u591a\u529f\u80fd\u6027\u3001\u654f\u6377\u6027\u548c\u8fed\u4ee3\u65b9\u6cd5\u3002", "motivation": "\u516c\u53f8\u8c03\u6574\u6570\u636e\u79d1\u5b66\u751f\u547d\u5468\u671f\u4ee5\u9002\u5e94\u5176\u6587\u5316\uff0c\u4ece\u800c\u63d0\u9ad8\u751f\u4ea7\u529b\u5e76\u63d0\u9ad8\u7ade\u4e89\u529b\u3002\u6570\u636e\u79d1\u5b66\u751f\u547d\u5468\u671f\u662f\u542f\u52a8\u548c\u7ed3\u675f\u4f9d\u8d56\u6570\u636e\u7684\u9879\u76ee\u7684\u91cd\u8981\u56e0\u7d20\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u87ba\u65cb\u6280\u672f\u7684\u65b0\u6280\u672f\uff0c\u5c06\u6570\u636e\u79d1\u5b66\u751f\u547d\u5468\u671f\u878d\u5165\u5230\u5177\u6709\u660e\u786e\u6700\u7ec8\u76ee\u6807\u7684\u4e1a\u52a1\u95ee\u9898\u4e2d\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6280\u672f", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u87ba\u65cb\u6280\u672f"}}
{"id": "2510.06377", "categories": ["cs.LG", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.06377", "abs": "https://arxiv.org/abs/2510.06377", "authors": ["Rishabh Ranjan", "Valter Hudovernik", "Mark Znidar", "Charilaos Kanatsoulis", "Roshan Upendra", "Mahmoud Mohammadi", "Joe Meyer", "Tom Palczewski", "Carlos Guestrin", "Jure Leskovec"], "title": "Relational Transformer: Toward Zero-Shot Foundation Models for Relational Data", "comment": "preprint; under review", "summary": "Pretrained transformers readily adapt to new sequence modeling tasks via\nzero-shot prompting, but relational domains still lack architectures that\ntransfer across datasets and tasks. The core challenge is the diversity of\nrelational data, with varying heterogeneous schemas, graph structures and\nfunctional dependencies. In this paper, we present the Relational Transformer\n(RT) architecture, which can be pretrained on diverse relational databases and\ndirectly applied to unseen datasets and tasks without task- or dataset-specific\nfine-tuning, or retrieval of in-context examples. RT (i) tokenizes cells with\ntable/column metadata, (ii) is pretrained via masked token prediction, and\n(iii) utilizes a novel \\textit{Relational Attention} mechanism over columns,\nrows, and primary-foreign key links. Pretrained on RelBench datasets spanning\ntasks such as churn and sales forecasting, RT attains strong zero-shot\nperformance, averaging 94% of fully supervised AUROC on binary classification\ntasks with a single forward pass of a 22M parameter model, as opposed to 84%\nfor a 27B LLM. Fine-tuning yields state-of-the-art results with high sample\nefficiency. Our experiments show that RT's zero-shot transfer harnesses\ntask-table context, relational attention patterns and schema semantics.\nOverall, RT provides a practical path toward foundation models for relational\ndata.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5173\u7cfbTransformer (RT) \u67b6\u6784\uff0c\u7528\u4e8e\u5173\u7cfb\u6570\u636e\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u3002", "motivation": "\u5173\u7cfb\u9886\u57df\u7f3a\u4e4f\u80fd\u591f\u8de8\u6570\u636e\u96c6\u548c\u4efb\u52a1\u8fc1\u79fb\u7684\u67b6\u6784\uff0c\u6838\u5fc3\u6311\u6218\u662f\u5173\u7cfb\u6570\u636e\u7684\u591a\u6837\u6027\u3002", "method": "RT \u901a\u8fc7 (i) \u4f7f\u7528\u8868/\u5217\u5143\u6570\u636e\u6807\u8bb0\u5355\u5143\u683c\uff0c(ii) \u901a\u8fc7\u63a9\u7801token\u9884\u6d4b\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4ee5\u53ca (iii) \u5229\u7528\u4e00\u79cd\u65b0\u7684\u5173\u7cfb\u6ce8\u610f\u529b\u673a\u5236\u6765\u5b9e\u73b0\u3002", "result": "RT \u5728 RelBench \u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u5728\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u4e2d\u5e73\u5747\u8fbe\u5230\u5b8c\u5168\u76d1\u7763 AUROC \u7684 94%\u3002", "conclusion": "RT \u4e3a\u5173\u7cfb\u6570\u636e\u7684\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u7684\u8def\u5f84\u3002"}}
{"id": "2510.06251", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06251", "abs": "https://arxiv.org/abs/2510.06251", "authors": ["Ieva Bagdonaviciute", "Vibhav Vineet"], "title": "Does Physics Knowledge Emerge in Frontier Models?", "comment": "8 pages, 7 figures. Preprint", "summary": "Leading Vision-Language Models (VLMs) show strong results in visual\nperception and general reasoning, but their ability to understand and predict\nphysical dynamics remains unclear. We benchmark six frontier VLMs on three\nphysical simulation datasets - CLEVRER, Physion, and Physion++ - where the\nevaluation tasks test whether a model can predict outcomes or hypothesize about\nalternative situations. To probe deeper, we design diagnostic subtests that\nisolate perception (objects, colors, occluders) from physics reasoning (motion\nprediction, spatial relations). Intuitively, stronger diagnostic performance\nshould support higher evaluation accuracy. Yet our analysis reveals weak\ncorrelations: models that excel at perception or physics reasoning do not\nconsistently perform better on predictive or counterfactual evaluation. This\ncounterintuitive gap exposes a central limitation of current VLMs: perceptual\nand physics skills remain fragmented and fail to combine into causal\nunderstanding, underscoring the need for architectures that bind perception and\nreasoning more tightly.", "AI": {"tldr": "\u8bc4\u4f30\u524d\u6cbf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7269\u7406\u52a8\u6001\u7406\u89e3\u548c\u9884\u6d4b\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5728\u611f\u77e5\u548c\u7269\u7406\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5272\u88c2\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u611f\u77e5\u548c\u901a\u7528\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u7406\u89e3\u548c\u9884\u6d4b\u7269\u7406\u52a8\u6001\u7684\u80fd\u529b\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u5728\u4e09\u4e2a\u7269\u7406\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u5bf9\u516d\u4e2a\u524d\u6cbf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8bbe\u8ba1\u8bca\u65ad\u5b50\u6d4b\u8bd5\u6765\u9694\u79bb\u611f\u77e5\u548c\u7269\u7406\u63a8\u7406\u3002", "result": "\u6a21\u578b\u5728\u611f\u77e5\u6216\u7269\u7406\u63a8\u7406\u65b9\u9762\u7684\u4f18\u52bf\u5e76\u4e0d\u4e00\u5b9a\u80fd\u8f6c\u5316\u4e3a\u9884\u6d4b\u6216\u53cd\u4e8b\u5b9e\u8bc4\u4f30\u65b9\u9762\u7684\u66f4\u597d\u8868\u73b0\uff0c\u4e24\u8005\u4e4b\u95f4\u76f8\u5173\u6027\u8f83\u5f31\u3002", "conclusion": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u611f\u77e5\u548c\u7269\u7406\u6280\u80fd\u4ecd\u7136\u662f\u5272\u88c2\u7684\uff0c\u672a\u80fd\u7ed3\u5408\u6210\u56e0\u679c\u7406\u89e3\uff0c\u9700\u8981\u66f4\u7d27\u5bc6\u5730\u7ed3\u5408\u611f\u77e5\u548c\u63a8\u7406\u7684\u67b6\u6784\u3002"}}
{"id": "2510.06410", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06410", "abs": "https://arxiv.org/abs/2510.06410", "authors": ["Aochong Oliver Li", "Tanya Goyal"], "title": "Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?", "comment": null, "summary": "Reasoning LLMs are trained to verbalize their reasoning process, yielding\nstrong gains on complex tasks. This transparency also opens a promising\ndirection: multiple reasoners can directly collaborate on each other's thinking\nwithin a shared trajectory, yielding better inference efficiency and\nexploration. A key prerequisite, however, is the ability to assess the\nusefulness and build on another model's partial thinking -- we call this\noff-trajectory reasoning. Our paper investigates a critical question: can\nstandard solo-reasoning training pipelines deliver desired off-trajectory\nbehaviors? We propose twin tests that capture the two extremes of the\noff-trajectory spectrum, namely Recoverability, which tests whether LLMs can\nbacktrack from \"distractions\" induced by misleading reasoning traces, and\nGuidability, which tests their ability to build upon correct reasoning from\nstronger collaborators. Our study evaluates 15 open-weight LLMs (1.5B-32B) and\nreveals a counterintuitive finding -- \"stronger\" LLMs on benchmarks are often\nmore fragile under distraction. Moreover, all models tested fail to effectively\nleverage guiding steps from collaborators on problems beyond their inherent\ncapabilities with solve rates remaining under 9.2%. Finally, we conduct control\nstudies to isolate the effects of three factors in post-training on these\nbehaviors: the choice of distillation teacher, the use of RL, and data\nselection strategy. Our results provide actionable insights for training\nnatively strong reasoning collaborators; e.g., we find that suboptimal\nrecoverability behaviors of teacher models are transferred to distilled\nstudents even if the distillation trajectories are correct. Taken together,\nthis work lays the groundwork for evaluating multi-model collaborations in\nshared reasoning trajectories and highlights the limitations of off-the-shelf\nreasoning LLMs.", "AI": {"tldr": "This paper investigates whether current reasoning LLMs can effectively collaborate and build upon each other's reasoning, even with distractions or guidance.", "motivation": "The motivation is to explore collaborative reasoning in LLMs to improve inference efficiency and exploration.", "method": "The authors propose twin tests, Recoverability and Guidability, to evaluate off-trajectory reasoning behaviors. They evaluate 15 open-weight LLMs (1.5B-32B).", "result": "Stronger LLMs are often more fragile under distraction, and all models struggle to leverage guidance from collaborators effectively. The study also isolates the effects of distillation teacher, RL, and data selection strategy.", "conclusion": "The work highlights the limitations of off-the-shelf reasoning LLMs and provides insights for training natively strong reasoning collaborators."}}
{"id": "2510.06293", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06293", "abs": "https://arxiv.org/abs/2510.06293", "authors": ["Cristian Meo", "Varun Sarathchandran", "Avijit Majhi", "Shao Hung", "Carlo Saccardi", "Ruben Imhoff", "Roberto Deidda", "Remko Uijlenhoet", "Justin Dauwels"], "title": "BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression", "comment": null, "summary": "Predicting precipitation maps is a highly complex spatiotemporal modeling\ntask, critical for mitigating the impacts of extreme weather events. Short-term\nprecipitation forecasting, or nowcasting, requires models that are not only\naccurate but also computationally efficient for real-time applications. Current\nmethods, such as token-based autoregressive models, often suffer from flawed\ninductive biases and slow inference, while diffusion models can be\ncomputationally intensive. To address these limitations, we introduce BlockGPT,\na generative autoregressive transformer using batched tokenization (Block)\nmethod that predicts full two-dimensional fields (frames) at each time step.\nConceived as a model-agnostic paradigm for video prediction, BlockGPT\nfactorizes space-time by using self-attention within each frame and causal\nattention across frames; in this work, we instantiate it for precipitation\nnowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI\n(Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines\nincluding token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet)\nmodels. The results show that BlockGPT achieves superior accuracy, event\nlocalization as measured by categorical metrics, and inference speeds up to 31x\nfaster than comparable baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86BlockGPT\uff0c\u4e00\u79cd\u7528\u4e8e\u89c6\u9891\u9884\u6d4b\u7684\u751f\u6210\u5f0f\u81ea\u56de\u5f52Transformer\uff0c\u901a\u8fc7\u5206\u6279\u4ee4\u724c\u5316\u65b9\u6cd5\u9884\u6d4b\u5b8c\u6574\u7684\u4e8c\u7ef4\u573a\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u4ee4\u724c\u7684\u81ea\u56de\u5f52\u6a21\u578b\u5b58\u5728\u5f52\u7eb3\u504f\u7f6e\u7f3a\u9677\u548c\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u800c\u6269\u6563\u6a21\u578b\u8ba1\u7b97\u91cf\u5927\u3002", "method": "BlockGPT\u4f7f\u7528\u5206\u6279\u4ee4\u724c\u5316\u65b9\u6cd5\uff0c\u5728\u6bcf\u4e2a\u5e27\u5185\u4f7f\u7528\u81ea\u6ce8\u610f\u529b\uff0c\u8de8\u5e27\u4f7f\u7528\u56e0\u679c\u6ce8\u610f\u529b\uff0c\u4ece\u800c\u5206\u89e3\u65f6\u7a7a\u3002", "result": "\u5728KNMI\u548cSEVIR\u4e24\u4e2a\u964d\u6c34\u6570\u636e\u96c6\u4e0a\uff0cBlockGPT\u5728\u51c6\u786e\u6027\u3001\u4e8b\u4ef6\u5b9a\u4f4d\uff08\u901a\u8fc7\u5206\u7c7b\u6307\u6807\u6d4b\u91cf\uff09\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u540c\u7c7b\u6a21\u578b\u5feb31\u500d\u3002", "conclusion": "BlockGPT\u5728\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2510.06249", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06249", "abs": "https://arxiv.org/abs/2510.06249", "authors": ["Toshiki Nakai", "Ravi Kiran Chikkala", "Lena Sophie Oberkircher", "Nicholas Jennings", "Natalia Skachkova", "Tatiana Anikina", "Jesujoba Oluwadara Alabi"], "title": "TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B", "comment": "It is work in progress", "summary": "The 2025 Multimodal Models for Low-Resource Contexts and Social Impact\n(MMLoSo) Language Challenge addresses one of India's most pressing linguistic\ngaps: the lack of resources for its diverse low-resource languages (LRLs). In\nthis study, we investigate whether enforcing cross-lingual similarity in\nspecific internal layers of a decoder-only multilingual large language model\n(LLM) can improve translation quality from LRL to high-resource language (HRL).\nSpecifically, we combine Centered Kernel Alignment (CKA), a similarity metric\nthat encourages representations of different languages to align, with REPINA, a\nregularization method that constrains parameter updates to remain close to the\npretrained model, into a joint method we call TRepLiNa. In this research\nproject, we experiment with zero-shot, few-shot, and fine-tuning settings using\nAya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari,\nSantali, Bhili) with Hindi/English pivots. Our results show that aligning\nmid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach\nto improving LRL translation, especially in data-scarce settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u5370\u5ea6\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8d44\u6e90\u532e\u4e4f\u95ee\u9898\uff0c \u63a2\u8ba8\u4e86\u5728decoder-only\u591a\u8bed\u79cd\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u7279\u5b9a\u5185\u90e8\u5c42\u4e2d\u5f3a\u5236\u6267\u884c\u8de8\u8bed\u8a00\u76f8\u4f3c\u6027\u662f\u5426\u53ef\u4ee5\u63d0\u9ad8\u4ece\u4f4e\u8d44\u6e90\u8bed\u8a00\u5230\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u7ffb\u8bd1\u8d28\u91cf\u3002", "motivation": "\u5370\u5ea6\u7f3a\u4e4f\u9488\u5bf9\u5176\u591a\u6837\u5316\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00(LRL)\u7684\u8d44\u6e90\uff0c\u8fd9\u662f\u5176\u9762\u4e34\u7684\u6700\u7d27\u8feb\u7684\u8bed\u8a00\u5dee\u8ddd\u4e4b\u4e00\u3002", "method": "\u7ed3\u5408\u4e86\u4e2d\u5fc3\u6838\u5bf9\u9f50(CKA) (\u4e00\u79cd\u9f13\u52b1\u4e0d\u540c\u8bed\u8a00\u7684\u8868\u793a\u5bf9\u9f50\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf) \u548cREPINA (\u4e00\u79cd\u7ea6\u675f\u53c2\u6570\u66f4\u65b0\u4ee5\u4fdd\u6301\u63a5\u8fd1\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4e00\u79cd\u6b63\u5219\u5316\u65b9\u6cd5)\uff0c \u5f62\u6210\u4e86\u4e00\u79cd\u540d\u4e3aTRepLiNa\u7684\u8054\u5408\u65b9\u6cd5\u3002 \u4f7f\u7528Aya-23 8B with QLoRA\u5728MMLoSo\u5171\u4eab\u4efb\u52a1\u8bed\u8a00\u5bf9(Mundari, Santali, Bhili)\u4e0a\uff0c\u4ee5\u5370\u5730\u8bed/\u82f1\u8bed\u4e3a\u8f74\u8fdb\u884c\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u7684\u5b9e\u9a8c\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528TRepLiNa (CKA+REPINA)\u5bf9\u9f50\u4e2d\u95f4\u5c42\u662f\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u5b9e\u7528\u7684\u6539\u8fdbLRL\u7ffb\u8bd1\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528TRepLiNa (CKA+REPINA)\u5bf9\u9f50\u4e2d\u95f4\u5c42\u662f\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u5b9e\u7528\u7684\u6539\u8fdbLRL\u7ffb\u8bd1\u7684\u65b9\u6cd5"}}
{"id": "2510.06999", "categories": ["cs.CL", "cs.IR", "I.2.7; H.3.3; K.5.0"], "pdf": "https://arxiv.org/pdf/2510.06999", "abs": "https://arxiv.org/abs/2510.06999", "authors": ["Markus Reuter", "Tobias Lingenberg", "R\u016bta Liepi\u0146a", "Francesca Lagioia", "Marco Lippi", "Giovanni Sartor", "Andrea Passerini", "Burcu Sayin"], "title": "Towards Reliable Retrieval in RAG Systems for Large Legal Datasets", "comment": "Accepted for the 7th Natural Legal Language Processing Workshop (NLLP\n  2025), co-located with EMNLP 2025", "summary": "Retrieval-Augmented Generation (RAG) is a promising approach to mitigate\nhallucinations in Large Language Models (LLMs) for legal applications, but its\nreliability is critically dependent on the accuracy of the retrieval step. This\nis particularly challenging in the legal domain, where large databases of\nstructurally similar documents often cause retrieval systems to fail. In this\npaper, we address this challenge by first identifying and quantifying a\ncritical failure mode we term Document-Level Retrieval Mismatch (DRM), where\nthe retriever selects information from entirely incorrect source documents. To\nmitigate DRM, we investigate a simple and computationally efficient technique\nwhich we refer to as Summary-Augmented Chunking (SAC). This method enhances\neach text chunk with a document-level synthetic summary, thereby injecting\ncrucial global context that would otherwise be lost during a standard chunking\nprocess. Our experiments on a diverse set of legal information retrieval tasks\nshow that SAC greatly reduces DRM and, consequently, also improves text-level\nretrieval precision and recall. Interestingly, we find that a generic\nsummarization strategy outperforms an approach that incorporates legal expert\ndomain knowledge to target specific legal elements. Our work provides evidence\nthat this practical, scalable, and easily integrable technique enhances the\nreliability of RAG systems when applied to large-scale legal document datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Summary-Augmented Chunking (SAC) \u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u6cd5\u5f8b\u9886\u57df\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u7cfb\u7edf\u4e2d\u7531\u4e8e\u6587\u6863\u7ea7\u68c0\u7d22\u4e0d\u5339\u914d (DRM) \u5bfc\u81f4\u7684\u68c0\u7d22\u4e0d\u51c6\u786e\u95ee\u9898\u3002SAC \u901a\u8fc7\u5728\u6587\u672c\u5757\u4e2d\u52a0\u5165\u6587\u6863\u7ea7\u6458\u8981\u6765\u589e\u5f3a\u68c0\u7d22\u6548\u679c\u3002", "motivation": "\u6cd5\u5f8b\u9886\u57df\u7684\u5927\u91cf\u7ed3\u6784\u76f8\u4f3c\u6587\u6863\u5bfc\u81f4\u68c0\u7d22\u7cfb\u7edf\u5931\u6548\uff0c\u5c24\u5176\u662f\u5728\u68c0\u7d22\u6b65\u9aa4\u7684\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u7684\u60c5\u51b5\u4e0b\uff0cRAG \u7cfb\u7edf\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\u3002", "method": "\u672c\u6587\u9996\u5148\u8bc6\u522b\u5e76\u91cf\u5316\u4e86\u4e00\u4e2a\u5173\u952e\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5373\u6587\u6863\u7ea7\u68c0\u7d22\u4e0d\u5339\u914d (DRM)\u3002\u7136\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u6280\u672f\uff0c\u5373 Summary-Augmented Chunking (SAC)\uff0c\u901a\u8fc7\u6587\u6863\u7ea7\u6458\u8981\u589e\u5f3a\u6587\u672c\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSAC \u663e\u8457\u964d\u4f4e\u4e86 DRM\uff0c\u5e76\u63d0\u9ad8\u4e86\u6587\u672c\u7ea7\u68c0\u7d22\u7684\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u3002\u901a\u7528\u6458\u8981\u7b56\u7565\u4f18\u4e8e\u7ed3\u5408\u6cd5\u5f8b\u4e13\u5bb6\u9886\u57df\u77e5\u8bc6\u7684\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86 SAC \u6280\u672f\u53ef\u4ee5\u63d0\u9ad8 RAG \u7cfb\u7edf\u5728\u5927\u578b\u6cd5\u5f8b\u6587\u6863\u6570\u636e\u96c6\u4e0a\u7684\u53ef\u9760\u6027\uff0c\u4e14\u8be5\u6280\u672f\u5177\u6709\u5b9e\u7528\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6613\u96c6\u6210\u6027\u3002"}}
{"id": "2510.06254", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06254", "abs": "https://arxiv.org/abs/2510.06254", "authors": ["Xiaochen Zhao", "Chengting Yu", "Kairong Yu", "Lei Liu", "Aili Wang"], "title": "Enhanced Self-Distillation Framework for Efficient Spiking Neural Network Training", "comment": null, "summary": "Spiking Neural Networks (SNNs) exhibit exceptional energy efficiency on\nneuromorphic hardware due to their sparse activation patterns. However,\nconventional training methods based on surrogate gradients and Backpropagation\nThrough Time (BPTT) not only lag behind Artificial Neural Networks (ANNs) in\nperformance, but also incur significant computational and memory overheads that\ngrow linearly with the temporal dimension. To enable high-performance SNN\ntraining under limited computational resources, we propose an enhanced\nself-distillation framework, jointly optimized with rate-based backpropagation.\nSpecifically, the firing rates of intermediate SNN layers are projected onto\nlightweight ANN branches, and high-quality knowledge generated by the model\nitself is used to optimize substructures through the ANN pathways. Unlike\ntraditional self-distillation paradigms, we observe that low-quality\nself-generated knowledge may hinder convergence. To address this, we decouple\nthe teacher signal into reliable and unreliable components, ensuring that only\nreliable knowledge is used to guide the optimization of the model. Extensive\nexperiments on CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet demonstrate that\nour method reduces training complexity while achieving high-performance SNN\ntraining. Our code is available at\nhttps://github.com/Intelli-Chip-Lab/enhanced-self-distillation-framework-for-snn.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u81ea\u84b8\u998f\u6846\u67b6\uff0c\u4ee5\u5728\u6709\u9650\u7684\u8ba1\u7b97\u8d44\u6e90\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fdSNN\u8bad\u7ec3\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u66ff\u4ee3\u68af\u5ea6\u548cBPTT\u7684SNN\u8bad\u7ec3\u65b9\u6cd5\u4e0d\u4ec5\u5728\u6027\u80fd\u4e0a\u843d\u540e\u4e8eANN\uff0c\u800c\u4e14\u8fd8\u4f1a\u4ea7\u751f\u663e\u8457\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\uff0c\u8fd9\u4e9b\u5f00\u9500\u968f\u65f6\u95f4\u7ef4\u5ea6\u7ebf\u6027\u589e\u957f\u3002", "method": "\u5c06\u4e2d\u95f4SNN\u5c42\u7684\u6fc0\u53d1\u7387\u6295\u5f71\u5230\u8f7b\u91cf\u7ea7ANN\u5206\u652f\u4e0a\uff0c\u5e76\u4f7f\u7528\u6a21\u578b\u672c\u8eab\u751f\u6210\u7684\u9ad8\u8d28\u91cf\u77e5\u8bc6\uff0c\u901a\u8fc7ANN\u9014\u5f84\u4f18\u5316\u5b50\u7ed3\u6784\u3002\u5c06\u6559\u5e08\u4fe1\u53f7\u89e3\u8026\u4e3a\u53ef\u9760\u548c\u4e0d\u53ef\u9760\u7684\u7ec4\u6210\u90e8\u5206\uff0c\u786e\u4fdd\u53ea\u6709\u53ef\u9760\u7684\u77e5\u8bc6\u88ab\u7528\u6765\u6307\u5bfc\u6a21\u578b\u7684\u4f18\u5316\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u3001CIFAR10-DVS\u548cImageNet\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u964d\u4f4e\u4e86\u8bad\u7ec3\u590d\u6742\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684SNN\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u6709\u9650\u7684\u8ba1\u7b97\u8d44\u6e90\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fdSNN\u8bad\u7ec3\u3002"}}
{"id": "2510.06433", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06433", "abs": "https://arxiv.org/abs/2510.06433", "authors": ["Aryan Singh Dalal", "Yinglun Zhang", "Duru Do\u011fan", "Atalay Mert \u0130leri", "Hande K\u00fc\u00e7\u00fck McGinty"], "title": "Flavonoid Fusion: Creating a Knowledge Graph to Unveil the Interplay Between Food and Health", "comment": null, "summary": "The focus on \"food as medicine\" is gaining traction in the field of health\nand several studies conducted in the past few years discussed this aspect of\nfood in the literature. However, very little research has been done on\nrepresenting the relationship between food and health in a standardized,\nmachine-readable format using a semantic web that can help us leverage this\nknowledge effectively. To address this gap, this study aims to create a\nknowledge graph to link food and health through the knowledge graph's ability\nto combine information from various platforms focusing on flavonoid contents of\nfood found in the USDA databases and cancer connections found in the\nliterature. We looked closely at these relationships using KNARM methodology\nand represented them in machine-operable format. The proposed knowledge graph\nserves as an example for researchers, enabling them to explore the complex\ninterplay between dietary choices and disease management. Future work for this\nstudy involves expanding the scope of the knowledge graph by capturing nuances,\nadding more related data, and performing inferences on the acquired knowledge\nto uncover hidden relationships.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u6784\u5efa\u4e00\u4e2a\u77e5\u8bc6\u56fe\u8c31\uff0c\u8fde\u63a5\u98df\u7269\u4e0e\u5065\u5eb7\uff0c\u4ee5\u5f25\u8865\u98df\u7269\u4e0e\u5065\u5eb7\u4e4b\u95f4\u5173\u7cfb\u5728\u6807\u51c6\u5316\u3001\u673a\u5668\u53ef\u8bfb\u683c\u5f0f\u65b9\u9762\u7684\u7814\u7a76\u7a7a\u767d\u3002", "motivation": "\u8fc7\u53bb\u51e0\u5e74\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u7814\u7a76\u5173\u6ce8\u201c\u98df\u7269\u5373\u836f\u7269\u201d\uff0c\u4f46\u5f88\u5c11\u6709\u7814\u7a76\u4ee5\u6807\u51c6\u5316\u7684\u673a\u5668\u53ef\u8bfb\u683c\u5f0f\u8868\u793a\u98df\u7269\u4e0e\u5065\u5eb7\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u5229\u7528KNARM\u65b9\u6cd5\uff0c\u4ece\u7f8e\u56fd\u519c\u4e1a\u90e8\u6570\u636e\u5e93\u4e2d\u98df\u7269\u7684\u7c7b\u9ec4\u916e\u542b\u91cf\u548c\u6587\u732e\u4e2d\u764c\u75c7\u7684\u5173\u8054\u4fe1\u606f\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u77e5\u8bc6\u56fe\u8c31\uff0c\u5c55\u793a\u4e86\u81b3\u98df\u9009\u62e9\u548c\u75be\u75c5\u7ba1\u7406\u4e4b\u95f4\u590d\u6742\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "conclusion": "\u8be5\u77e5\u8bc6\u56fe\u8c31\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u793a\u4f8b\uff0c\u672a\u6765\u5c06\u6269\u5c55\u77e5\u8bc6\u56fe\u8c31\u7684\u8303\u56f4\uff0c\u6355\u6349\u7ec6\u5fae\u5dee\u522b\uff0c\u6dfb\u52a0\u66f4\u591a\u76f8\u5173\u6570\u636e\uff0c\u5e76\u5bf9\u6240\u83b7\u53d6\u7684\u77e5\u8bc6\u8fdb\u884c\u63a8\u65ad\uff0c\u4ee5\u53d1\u73b0\u9690\u85cf\u7684\u5173\u7cfb\u3002"}}
{"id": "2510.06303", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06303", "abs": "https://arxiv.org/abs/2510.06303", "authors": ["Shuang Cheng", "Yihan Bian", "Dawei Liu", "Yuhua Jiang", "Yihao Liu", "Linfeng Zhang", "Wenhai Wang", "Qipeng Guo", "Kai Chen", "Biqing Qi", "Bowen Zhou"], "title": "SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation", "comment": "Technical report. 39 pages, including 14 pages of appendix", "summary": "We propose SDAR, a Synergistic Diffusion-Autoregression paradigm that unifies\nthe training efficiency of autoregressive models with the parallel inference\ncapability of diffusion. Instead of costly end-to-end diffusion training, SDAR\nperforms a lightweight paradigm conversion that transforms a well-trained\nautoregressive (AR) model into a blockwise diffusion model through brief,\ndata-efficient adaptation. During inference, SDAR generates sequences\nautoregressively across blocks for global coherence while decoding all tokens\nwithin each block in parallel via a discrete diffusion process. Extensive\nexperiments show that AR models remain substantially more compute-efficient\nthan masked diffusion models, providing a strong foundation for adaptation.\nBuilding on this insight, SDAR achieves efficient AR-to-diffusion conversion\nwith minimal cost, preserving AR-level performance while enabling parallel\ngeneration. Scaling studies across dense and Mixture-of-Experts architectures\nconfirm that SDAR scales without compromise: larger models exhibit stronger\nrobustness to block size and decoding thresholds, yielding greater speedups\nwithout accuracy loss. Beyond efficiency, SDAR demonstrates enhanced reasoning\nand domain adaptability. Our 30B MoE model surpasses its AR counterpart on\nchallenging scientific reasoning benchmarks such as GPQA and ChemBench, and\ngains further improvements under test-time scaling methods like majority voting\nand pass@k. Together, these results establish SDAR as a practical paradigm that\ncombines the strengths of autoregression and diffusion for scalable,\nhigh-throughput reasoning.", "AI": {"tldr": "SDAR: Combines autoregressive (AR) efficiency with diffusion's parallel inference by converting a trained AR model into a blockwise diffusion model. It maintains AR performance, enables parallel generation, and shows enhanced reasoning and adaptability, especially in large models.", "motivation": "To unify the training efficiency of autoregressive models with the parallel inference capability of diffusion models, overcoming the high computational cost of end-to-end diffusion training.", "method": "A lightweight paradigm conversion transforms a well-trained AR model into a blockwise diffusion model through brief, data-efficient adaptation. During inference, sequences are generated autoregressively across blocks, with parallel decoding within each block via a discrete diffusion process.", "result": "SDAR achieves efficient AR-to-diffusion conversion with minimal cost, preserving AR-level performance while enabling parallel generation. Larger models exhibit stronger robustness to block size and decoding thresholds, yielding greater speedups without accuracy loss. The 30B MoE model surpasses its AR counterpart on scientific reasoning benchmarks.", "conclusion": "SDAR is a practical paradigm that combines the strengths of autoregression and diffusion for scalable, high-throughput reasoning. It demonstrates enhanced reasoning and domain adaptability, making it a valuable approach for various applications."}}
{"id": "2510.06250", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06250", "abs": "https://arxiv.org/abs/2510.06250", "authors": ["Bharti Meena", "Joanna Skubisz", "Harshit Rajgarhia", "Nand Dave", "Kiran Ganesh", "Shivali Dalmia", "Abhishek Mukherji", "Vasudevan Sundarababu", "Olga Pospelova"], "title": "Scalable multilingual PII annotation for responsible AI in LLMs", "comment": null, "summary": "As Large Language Models (LLMs) gain wider adoption, ensuring their reliable\nhandling of Personally Identifiable Information (PII) across diverse regulatory\ncontexts has become essential. This work introduces a scalable multilingual\ndata curation framework designed for high-quality PII annotation across 13\nunderrepresented locales, covering approximately 336 locale-specific PII types.\nOur phased, human-in-the-loop annotation methodology combines linguistic\nexpertise with rigorous quality assurance, leading to substantial improvements\nin recall and false positive rates from pilot, training, and production phases.\nBy leveraging inter-annotator agreement metrics and root-cause analysis, the\nframework systematically uncovers and resolves annotation inconsistencies,\nresulting in high-fidelity datasets suitable for supervised LLM fine-tuning.\nBeyond reporting empirical gains, we highlight common annotator challenges in\nmultilingual PII labeling and demonstrate how iterative, analytics-driven\npipelines can enhance both annotation quality and downstream model reliability.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u591a\u8bed\u8a00\u6570\u636e\u7ba1\u7406\u6846\u67b6\uff0c\u4e13\u4e3a\u9ad8\u8d28\u91cf\u7684PII\u6ce8\u91ca\u800c\u8bbe\u8ba1\uff0c\u8986\u76d613\u79cd\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u8bed\u8a00\u73af\u5883\u3002", "motivation": "\u786e\u4fdd\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u5728\u4e0d\u540c\u7684\u76d1\u7ba1\u73af\u5883\u4e2d\u53ef\u9760\u5730\u5904\u7406\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f(PII)\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4e00\u79cd\u5206\u9636\u6bb5\u7684\u3001\u4eba\u5de5\u53c2\u4e0e\u7684\u6ce8\u91ca\u65b9\u6cd5\uff0c\u5c06\u8bed\u8a00\u4e13\u4e1a\u77e5\u8bc6\u4e0e\u4e25\u683c\u7684\u8d28\u91cf\u4fdd\u8bc1\u76f8\u7ed3\u5408\u3002", "result": "\u4ece\u8bd5\u70b9\u3001\u57f9\u8bad\u548c\u751f\u4ea7\u9636\u6bb5\u6765\u770b\uff0c\u53ec\u56de\u7387\u548c\u5047\u9633\u6027\u7387\u90fd\u6709\u4e86\u663e\u8457\u63d0\u9ad8\u3002\u901a\u8fc7\u5229\u7528\u6ce8\u91ca\u8005\u95f4\u4e00\u81f4\u6027\u6307\u6807\u548c\u6839\u672c\u539f\u56e0\u5206\u6790\uff0c\u8be5\u6846\u67b6\u7cfb\u7edf\u5730\u53d1\u73b0\u5e76\u89e3\u51b3\u4e86\u6ce8\u91ca\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4ece\u800c\u751f\u6210\u4e86\u9002\u7528\u4e8e\u76d1\u7763LLM\u5fae\u8c03\u7684\u9ad8\u4fdd\u771f\u6570\u636e\u96c6\u3002", "conclusion": "\u8fed\u4ee3\u7684\u3001\u5206\u6790\u9a71\u52a8\u7684\u7ba1\u9053\u53ef\u4ee5\u63d0\u9ad8\u6ce8\u91ca\u8d28\u91cf\u548c\u4e0b\u6e38\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2510.06260", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06260", "abs": "https://arxiv.org/abs/2510.06260", "authors": ["Sher Khan", "Raz Muhammad", "Adil Hussain", "Muhammad Sajjad", "Muhammad Rashid"], "title": "Ensemble Deep Learning and LLM-Assisted Reporting for Automated Skin Lesion Diagnosis", "comment": null, "summary": "Cutaneous malignancies demand early detection for favorable outcomes, yet\ncurrent diagnostics suffer from inter-observer variability and access\ndisparities. While AI shows promise, existing dermatological systems are\nlimited by homogeneous architectures, dataset biases across skin tones, and\nfragmented approaches that treat natural language processing as separate\npost-hoc explanations rather than integral to clinical decision-making. We\nintroduce a unified framework that fundamentally reimagines AI integration for\ndermatological diagnostics through two synergistic innovations. First, a\npurposefully heterogeneous ensemble of architecturally diverse convolutional\nneural networks provides complementary diagnostic perspectives, with an\nintrinsic uncertainty mechanism flagging discordant cases for specialist review\n-- mimicking clinical best practices. Second, we embed large language model\ncapabilities directly into the diagnostic workflow, transforming classification\noutputs into clinically meaningful assessments that simultaneously fulfill\nmedical documentation requirements and deliver patient-centered education. This\nseamless integration generates structured reports featuring precise lesion\ncharacterization, accessible diagnostic reasoning, and actionable monitoring\nguidance -- empowering patients to recognize early warning signs between\nvisits. By addressing both diagnostic reliability and communication barriers\nwithin a single cohesive system, our approach bridges the critical\ntranslational gap that has prevented previous AI implementations from achieving\nclinical impact. The framework represents a significant advancement toward\ndeployable dermatological AI that enhances diagnostic precision while actively\nsupporting the continuum of care from initial detection through patient\neducation, ultimately improving early intervention rates for skin lesions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6784\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u96c6\u6210\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6539\u8fdb\u76ae\u80a4\u79d1\u6076\u6027\u80bf\u7624\u7684\u65e9\u671f\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u76ae\u80a4\u79d1\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u53d7\u9650\u4e8e\u540c\u6784\u67b6\u6784\u3001\u6570\u636e\u96c6\u504f\u5dee\u548cNLP\u4e0e\u4e34\u5e8a\u51b3\u7b56\u5206\u79bb\u3002", "method": "\u6784\u5efa\u5f02\u6784\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u96c6\u6210\uff0c\u5e76\u5d4c\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5230\u8bca\u65ad\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002", "result": "\u751f\u6210\u5305\u542b\u7cbe\u786e\u75c5\u7076\u7279\u5f81\u3001\u8bca\u65ad\u63a8\u7406\u548c\u53ef\u884c\u76d1\u6d4b\u6307\u5bfc\u7684\u7ed3\u6784\u5316\u62a5\u544a\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u63d0\u9ad8\u8bca\u65ad\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u652f\u6301\u4ece\u521d\u6b65\u68c0\u6d4b\u5230\u60a3\u8005\u6559\u80b2\u7684\u8fde\u7eed\u62a4\u7406\uff0c\u4ece\u800c\u63d0\u9ad8\u76ae\u80a4\u75c5\u53d8\u7684\u65e9\u671f\u5e72\u9884\u7387\u3002"}}
{"id": "2510.06475", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06475", "abs": "https://arxiv.org/abs/2510.06475", "authors": ["Yitao Long", "Yuru Jiang", "Hongjun Liu", "Yilun Zhao", "Jingchen Sun", "Yiqiu Shen", "Chen Zhao", "Arman Cohan", "Dennis Shasha"], "title": "PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles", "comment": null, "summary": "This work investigates the reasoning and planning capabilities of foundation\nmodels and their scalability in complex, dynamic environments. We introduce\nPuzzlePlex, a benchmark designed to assess these capabilities through a diverse\nset of puzzles. PuzzlePlex consists of 15 types of puzzles, including\ndeterministic and stochastic games of varying difficulty, as well as\nsingle-player and two-player scenarios. The PuzzlePlex framework provides a\ncomprehensive environment for each game, and supports extensibility to generate\nmore challenging instances as foundation models evolve. Additionally, we\nimplement customized game-playing strategies for comparison. Building on this\nbenchmark, we develop fine-grained metrics to measure performance and conduct\nan in-depth analysis of frontier foundation models across two settings:\ninstruction-based and code-based. Furthermore, we systematically investigate\ntheir scaling limits. Our findings show that reasoning models outperform others\nin instruction-based settings, while code-based execution presents greater\nchallenges but offers a scalable and efficient alternative. PuzzlePlex enables\ntargeted evaluation and guides future improvements in reasoning, planning, and\ngeneralization for foundation models.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aPuzzlePlex\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u7684\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u7814\u7a76\u57fa\u7840\u6a21\u578b\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u3002", "method": "\u901a\u8fc7PuzzlePlex\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u4e0d\u540c\u7c7b\u578b\u7684\u8c1c\u9898\uff0c\u5e76\u5b9a\u5236\u6e38\u620f\u7b56\u7565\u8fdb\u884c\u6bd4\u8f83\u3002\u4f7f\u7528\u7ec6\u7c92\u5ea6\u6307\u6807\u8861\u91cf\u6027\u80fd\uff0c\u5e76\u6df1\u5165\u5206\u6790\u524d\u6cbf\u57fa\u7840\u6a21\u578b\u5728\u6307\u4ee4\u548c\u4ee3\u7801\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u63a8\u7406\u6a21\u578b\u5728\u6307\u4ee4\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u800c\u57fa\u4e8e\u4ee3\u7801\u7684\u6267\u884c\u66f4\u5177\u6311\u6218\u6027\uff0c\u4f46\u4e5f\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "PuzzlePlex\u80fd\u591f\u8fdb\u884c\u6709\u9488\u5bf9\u6027\u7684\u8bc4\u4f30\uff0c\u5e76\u6307\u5bfc\u672a\u6765\u5728\u63a8\u7406\u3001\u89c4\u5212\u548c\u57fa\u7840\u6a21\u578b\u6cdb\u5316\u65b9\u9762\u7684\u6539\u8fdb\u3002"}}
{"id": "2510.06349", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.06349", "abs": "https://arxiv.org/abs/2510.06349", "authors": ["Moein E. Samadi", "Andreas Schuppert"], "title": "Flexible Swarm Learning May Outpace Foundation Models in Essential Tasks", "comment": null, "summary": "Foundation models have rapidly advanced AI, raising the question of whether\ntheir decisions will ultimately surpass human strategies in real-world domains.\nThe exponential, and possibly super-exponential, pace of AI development makes\nsuch analysis elusive. Nevertheless, many application areas that matter for\ndaily life and society show only modest gains so far; a prominent case is\ndiagnosing and treating dynamically evolving disease in intensive care.\n  The common challenge is adapting complex systems to dynamic environments.\nEffective strategies must optimize outcomes in systems composed of strongly\ninteracting functions while avoiding shared side effects; this requires\nreliable, self-adaptive modeling. These tasks align with building digital twins\nof highly complex systems whose mechanisms are not fully or quantitatively\nunderstood. It is therefore essential to develop methods for self-adapting AI\nmodels with minimal data and limited mechanistic knowledge. As this challenge\nextends beyond medicine, AI should demonstrate clear superiority in these\nsettings before assuming broader decision-making roles.\n  We identify the curse of dimensionality as a fundamental barrier to efficient\nself-adaptation and argue that monolithic foundation models face conceptual\nlimits in overcoming it. As an alternative, we propose a decentralized\narchitecture of interacting small agent networks (SANs). We focus on agents\nrepresenting the specialized substructure of the system, where each agent\ncovers only a subset of the full system functions. Drawing on mathematical\nresults on the learning behavior of SANs and evidence from existing\napplications, we argue that swarm-learning in diverse swarms can enable\nself-adaptive SANs to deliver superior decision-making in dynamic environments\ncompared with monolithic foundation models, though at the cost of reduced\nreproducibility in detail.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u7840\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u8d85\u8d8a\u4eba\u7c7b\u51b3\u7b56\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u6307\u51fa\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8c03\u6574\u590d\u6742\u7cfb\u7edf\u662f\u5173\u952e\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u8bb8\u591a\u5e94\u7528\u9886\u57df\uff0c\u5982\u91cd\u75c7\u76d1\u62a4\u4e2d\u7684\u75be\u75c5\u8bca\u65ad\u548c\u6cbb\u7597\uff0c\u8fdb\u5c55\u6709\u9650\u3002\u5728\u590d\u6742\u548c\u52a8\u6001\u73af\u5883\u4e2d\u4f18\u5316\u7cfb\u7edf\uff0c\u9700\u8981\u53ef\u9760\u7684\u81ea\u9002\u5e94\u5efa\u6a21\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u6563\u5f0f\u67b6\u6784\uff0c\u5373\u4ea4\u4e92\u5f0f\u5c0f\u4ee3\u7406\u7f51\u7edc(SANs)\uff0c\u5176\u4e2d\u6bcf\u4e2a\u4ee3\u7406\u4ee3\u8868\u7cfb\u7edf\u7684\u4e13\u95e8\u5b50\u7ed3\u6784\uff0c\u4ec5\u8986\u76d6\u90e8\u5206\u7cfb\u7edf\u529f\u80fd\u3002\u901a\u8fc7swarm-learning\uff0cSANs\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u4f18\u4e8e\u5355\u4f53\u57fa\u7840\u6a21\u578b\u7684\u51b3\u7b56\u3002", "result": "\u901a\u8fc7\u6570\u5b66\u7ed3\u679c\u548c\u73b0\u6709\u5e94\u7528\u8bc1\u636e\u8868\u660e\uff0c\u4e0e\u5355\u4f53\u57fa\u7840\u6a21\u578b\u76f8\u6bd4\uff0c\u81ea\u9002\u5e94SANs\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u63d0\u4f9b\u5353\u8d8a\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u4f46\u4ee3\u4ef7\u662f\u7ec6\u8282\u7684\u53ef\u91cd\u590d\u6027\u964d\u4f4e\u3002", "conclusion": "\u7ef4\u5ea6\u8bc5\u5492\u662f\u9ad8\u6548\u81ea\u9002\u5e94\u7684\u6839\u672c\u969c\u788d\uff0c\u5355\u4f53\u57fa\u7840\u6a21\u578b\u5728\u514b\u670d\u8fd9\u4e00\u969c\u788d\u65b9\u9762\u9762\u4e34\u6982\u5ff5\u9650\u5236\u3002\u5206\u6563\u5f0f\u67b6\u6784\u7684SANs\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2510.06262", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06262", "abs": "https://arxiv.org/abs/2510.06262", "authors": ["Aryan Kumar Singh", "Janvi Singh"], "title": "Prakriti200: A Questionnaire-Based Dataset of 200 Ayurvedic Prakriti Assessments", "comment": "4 pages, 4 figures", "summary": "This dataset provides responses to a standardized, bilingual (English-Hindi)\nPrakriti Assessment Questionnaire designed to evaluate the physical,\nphysiological, and psychological characteristics of individuals according to\nclassical Ayurvedic principles. The questionnaire consists of 24\nmultiple-choice items covering body features, appetite, sleep patterns, energy\nlevels, and temperament. It was developed following AYUSH/CCRAS guidelines to\nensure comprehensive and accurate data collection. All questions are mandatory\nand neutrally phrased to minimize bias, and dosha labels (Vata, Pitta, Kapha)\nare hidden from participants. Data were collected via a Google Forms\ndeployment, enabling automated scoring of responses to map individual traits to\ndosha-specific scores. The resulting dataset provides a structured platform for\nresearch in computational intelligence, Ayurvedic studies, and personalized\nhealth analytics, supporting analysis of trait distributions, correlations, and\npredictive modeling. It can also serve as a reference for future Prakriti-based\nstudies and the development of intelligent health applications.", "AI": {"tldr": "\u672c\u6570\u636e\u96c6\u5305\u542b\u5bf9\u4e00\u4efd\u6807\u51c6\u5316\u3001\u53cc\u8bed\uff08\u82f1\u8bed-\u5370\u5730\u8bed\uff09\u7684Prakriti\u8bc4\u4f30\u95ee\u5377\u7684\u56de\u590d\uff0c\u8be5\u95ee\u5377\u65e8\u5728\u6839\u636e\u53e4\u5178\u963f\u80b2\u5420\u9640\u539f\u5219\u8bc4\u4f30\u4e2a\u4f53\u7684\u8eab\u4f53\u3001\u751f\u7406\u548c\u5fc3\u7406\u7279\u5f81\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u4e2a\u4f53\u7684\u8eab\u4f53\u3001\u751f\u7406\u548c\u5fc3\u7406\u7279\u5f81\uff0c\u4e3a\u8ba1\u7b97\u667a\u80fd\u3001\u963f\u80b2\u5420\u9640\u7814\u7a76\u548c\u4e2a\u6027\u5316\u5065\u5eb7\u5206\u6790\u63d0\u4f9b\u7ed3\u6784\u5316\u5e73\u53f0\u3002", "method": "\u4f7f\u7528\u5305\u542b24\u4e2a\u591a\u9879\u9009\u62e9\u9898\u7684\u6807\u51c6\u5316\u53cc\u8bed\u95ee\u5377\uff0c\u901a\u8fc7Google Forms\u6536\u96c6\u6570\u636e\uff0c\u81ea\u52a8\u8bc4\u5206\u4ee5\u5c06\u4e2a\u4f53\u7279\u5f81\u6620\u5c04\u5230dosha\u7279\u5b9a\u5206\u6570\u3002", "result": "\u751f\u6210\u4e86\u4e00\u4e2a\u6570\u636e\u96c6\uff0c\u53ef\u4ee5\u5206\u6790\u7279\u5f81\u5206\u5e03\u3001\u76f8\u5173\u6027\u548c\u9884\u6d4b\u5efa\u6a21\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u672a\u6765\u57fa\u4e8ePrakriti\u7684\u7814\u7a76\u548c\u667a\u80fd\u5065\u5eb7\u5e94\u7528\u7a0b\u5e8f\u7684\u5f00\u53d1\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2510.06273", "categories": ["cs.CV", "astro-ph.IM", "cs.LG", "gr-qc"], "pdf": "https://arxiv.org/pdf/2510.06273", "abs": "https://arxiv.org/abs/2510.06273", "authors": ["Divyansh Srivastava", "Andrzej Niedzielski"], "title": "Vision Transformer for Transient Noise Classification", "comment": "9 pages, 4 figures", "summary": "Transient noise (glitches) in LIGO data hinders the detection of\ngravitational waves (GW). The Gravity Spy project has categorized these noise\nevents into various classes. With the O3 run, there is the inclusion of two\nadditional noise classes and thus a need to train new models for effective\nclassification. We aim to classify glitches in LIGO data into 22 existing\nclasses from the first run plus 2 additional noise classes from O3a using the\nVision Transformer (ViT) model. We train a pre-trained Vision Transformer\n(ViT-B/32) model on a combined dataset consisting of the Gravity Spy dataset\nwith the additional two classes from the LIGO O3a run. We achieve a\nclassification efficiency of 92.26%, demonstrating the potential of Vision\nTransformer to improve the accuracy of gravitational wave detection by\neffectively distinguishing transient noise.\n  Key words: gravitational waves --vision transformer --machine learning", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528Vision Transformer\u6a21\u578b\u5bf9LIGO\u6570\u636e\u4e2d\u7684\u77ac\u6001\u566a\u58f0\u8fdb\u884c\u5206\u7c7b\uff0c\u4ee5\u63d0\u9ad8\u5f15\u529b\u6ce2\u7684\u63a2\u6d4b\u7cbe\u5ea6\u3002", "motivation": "LIGO\u6570\u636e\u4e2d\u7684\u77ac\u6001\u566a\u58f0\u4f1a\u963b\u788d\u5f15\u529b\u6ce2\u7684\u63a2\u6d4b\uff0c\u9700\u8981\u5bf9\u5176\u8fdb\u884c\u6709\u6548\u5206\u7c7b\u3002", "method": "\u4f7f\u7528Vision Transformer (ViT) \u6a21\u578b\uff0c\u5728\u5305\u542bGravity Spy\u6570\u636e\u96c6\u548cLIGO O3a\u8fd0\u884c\u4e2d\u65b0\u589e\u7684\u4e24\u4e2a\u566a\u58f0\u7c7b\u522b\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5206\u7c7b\u6548\u7387\u8fbe\u523092.26%\uff0c\u8868\u660eVision Transformer\u5728\u533a\u5206\u77ac\u6001\u566a\u58f0\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "Vision Transformer\u53ef\u4ee5\u6709\u6548\u5730\u533a\u5206\u77ac\u6001\u566a\u58f0\uff0c\u4ece\u800c\u63d0\u9ad8\u5f15\u529b\u6ce2\u7684\u63a2\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2510.06534", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06534", "abs": "https://arxiv.org/abs/2510.06534", "authors": ["Jiahe Jin", "Abhijay Paladugu", "Chenyan Xiong"], "title": "Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them", "comment": null, "summary": "Agentic search leverages large language models (LLMs) to interpret complex\nuser information needs and execute a multi-step process of planning, searching,\nand synthesizing information to provide answers. This paradigm introduces\nunique challenges for LLMs' reasoning and agentic capabilities when interacting\nwith retrieval systems and the broader web. In this paper, we propose a\nreasoning-driven LLM-based pipeline to study effective reasoning behavior\npatterns in agentic search. Using this pipeline, we analyze successful agentic\nsearch trajectories and identify four beneficial reasoning behaviors:\nInformation Verification, Authority Evaluation, Adaptive Search, and Error\nRecovery. Based on these findings, we propose a technique called Behavior\nPriming to train more effective agentic search models. It synthesizes agentic\nsearch trajectories that exhibit these four behaviors and integrates them into\nthe agentic search model through supervised fine-tuning (SFT), followed by\nstandard reinforcement learning (RL). Experiments on three benchmarks (GAIA,\nWebWalker, and HLE) demonstrate that behavior priming yields over 35% gains in\nLlama3.2-3B and Qwen3-1.7B compared to directly training agentic search models\nwith RL. Crucially, we demonstrate that the desired reasoning behaviors in the\nSFT data, rather than the correctness of the final answer, is the critical\nfactor for achieving strong final performance after RL: fine-tuning on\ntrajectories with desirable reasoning behaviors but incorrect answers leads to\nbetter performance than fine-tuning on trajectories with correct answers. Our\nanalysis further reveals the underlying mechanism: the introduced reasoning\nbehaviors endow models with more effective exploration (higher pass@k and\nentropy) and test-time scaling (longer trajectories) capabilities, providing a\nstrong foundation for RL. Our code will be released as open source.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684agentic\u641c\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u7684\u63a8\u7406\u884c\u4e3a\u6765\u63d0\u9ad8\u641c\u7d22\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684agentic\u641c\u7d22\u5728\u5904\u7406\u590d\u6742\u7684\u7528\u6237\u9700\u6c42\u65f6\uff0cLLM\u7684\u63a8\u7406\u548cagentic\u80fd\u529b\u9762\u4e34\u6311\u6218\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u63a8\u7406\u7684LLM pipeline\uff0c\u7528\u4e8e\u5206\u6790agentic\u641c\u7d22\u4e2d\u7684\u6709\u6548\u63a8\u7406\u884c\u4e3a\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBehavior Priming\u7684\u6280\u672f\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6765\u8bad\u7ec3\u66f4\u6709\u6548\u7684agentic\u641c\u7d22\u6a21\u578b\u3002", "result": "\u5728\u4e09\u4e2abenchmark\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u76f4\u63a5\u4f7f\u7528RL\u8bad\u7ec3agentic\u641c\u7d22\u6a21\u578b\u76f8\u6bd4\uff0cbehavior priming\u5728Llama3.2-3B\u548cQwen3-1.7B\u4e0a\u4ea7\u751f\u4e86\u8d85\u8fc735%\u7684\u6536\u76ca\u3002\u91cd\u8981\u7684\u662f\uff0cSFT\u6570\u636e\u4e2d\u671f\u671b\u7684\u63a8\u7406\u884c\u4e3a\uff0c\u800c\u4e0d\u662f\u6700\u7ec8\u7b54\u6848\u7684\u6b63\u786e\u6027\uff0c\u662f\u5b9e\u73b0RL\u540e\u5f3a\u5927\u6700\u7ec8\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u8bba\u6587\u5206\u6790\u8868\u660e\uff0c\u5f15\u5165\u7684\u63a8\u7406\u884c\u4e3a\u4f7f\u6a21\u578b\u5177\u6709\u66f4\u6709\u6548\u7684\u63a2\u7d22\u548c\u6d4b\u8bd5\u65f6\u6269\u5c55\u80fd\u529b\uff0c\u4e3aRL\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2510.06355", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.06355", "abs": "https://arxiv.org/abs/2510.06355", "authors": ["K\u00fcr\u015fat Tekb\u0131y\u0131k", "G\u00fcne\u015f Karabulut Kurt", "Antoine Lesage-Landry"], "title": "PIKAN: Physics-Inspired Kolmogorov-Arnold Networks for Explainable UAV Channel Modelling", "comment": null, "summary": "Unmanned aerial vehicle (UAV) communications demand accurate yet\ninterpretable air-to-ground (A2G) channel models that can adapt to\nnonstationary propagation environments. While deterministic models offer\ninterpretability and deep learning (DL) models provide accuracy, both\napproaches suffer from either rigidity or a lack of explainability. To bridge\nthis gap, we propose the Physics-Inspired Kolmogorov-Arnold Network (PIKAN)\nthat embeds physical principles (e.g., free-space path loss, two-ray\nreflections) into the learning process. Unlike physics-informed neural networks\n(PINNs), PIKAN is more flexible for applying physical information because it\nintroduces them as flexible inductive biases. Thus, it enables a more flexible\ntraining process. Experiments on UAV A2G measurement data show that PIKAN\nachieves comparable accuracy to DL models while providing symbolic and\nexplainable expressions aligned with propagation laws. Remarkably, PIKAN\nachieves this performance with only 232 parameters, making it up to 37 times\nlighter than multilayer perceptron (MLP) baselines with thousands of\nparameters, without sacrificing correlation with measurements and also\nproviding symbolic expressions. These results highlight PIKAN as an efficient,\ninterpretable, and scalable solution for UAV channel modelling in beyond-5G and\n6G networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u4eba\u673a\uff08UAV\uff09\u7a7a\u5bf9\u5730\uff08A2G\uff09\u4fe1\u9053\u5efa\u6a21\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u517c\u5177\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u786e\u5b9a\u6027\u6a21\u578b\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u4e86Physics-Inspired Kolmogorov-Arnold Network (PIKAN)\uff0c\u5c06\u7269\u7406\u539f\u7406\u5d4c\u5165\u5230\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u3002", "result": "PIKAN\u5728UAV A2G\u6d4b\u91cf\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u4e0e\u4f20\u64ad\u89c4\u5f8b\u4e00\u81f4\u7684\u7b26\u53f7\u5316\u548c\u53ef\u89e3\u91ca\u7684\u8868\u8fbe\u5f0f\u3002PIKAN\u4ec5\u7528232\u4e2a\u53c2\u6570\u5c31\u5b9e\u73b0\u4e86\u8fd9\u4e00\u6027\u80fd\uff0c\u6bd4\u5177\u6709\u6570\u5343\u4e2a\u53c2\u6570\u7684\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u57fa\u7ebf\u8f7b37\u500d\u3002", "conclusion": "PIKAN\u662fbeyond-5G\u548c6G\u7f51\u7edc\u4e2dUAV\u4fe1\u9053\u5efa\u6a21\u7684\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.06263", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06263", "abs": "https://arxiv.org/abs/2510.06263", "authors": ["Jiajun Wu", "Swaleh Zaidi", "Braden Teitge", "Henry Leung", "Jiayu Zhou", "Jessalyn Holodinsky", "Steve Drew"], "title": "Dual-stage and Lightweight Patient Chart Summarization for Emergency Physicians", "comment": "Accepted at the IEEE Annual Congress on Artificial Intelligence of\n  Things (IEEE AIoT) 2025", "summary": "Electronic health records (EHRs) contain extensive unstructured clinical data\nthat can overwhelm emergency physicians trying to identify critical\ninformation. We present a two-stage summarization system that runs entirely on\nembedded devices, enabling offline clinical summarization while preserving\npatient privacy. In our approach, a dual-device architecture first retrieves\nrelevant patient record sections using the Jetson Nano-R (Retrieve), then\ngenerates a structured summary on another Jetson Nano-S (Summarize),\ncommunicating via a lightweight socket link. The summarization output is\ntwo-fold: (1) a fixed-format list of critical findings, and (2) a\ncontext-specific narrative focused on the clinician's query. The retrieval\nstage uses locally stored EHRs, splits long notes into semantically coherent\nsections, and searches for the most relevant sections per query. The generation\nstage uses a locally hosted small language model (SLM) to produce the summary\nfrom the retrieved text, operating within the constraints of two NVIDIA Jetson\ndevices. We first benchmarked six open-source SLMs under 7B parameters to\nidentify viable models. We incorporated an LLM-as-Judge evaluation mechanism to\nassess summary quality in terms of factual accuracy, completeness, and clarity.\nPreliminary results on MIMIC-IV and de-identified real EHRs demonstrate that\nour fully offline system can effectively produce useful summaries in under 30\nseconds.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u5728\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u8fd0\u884c\u7684\u4e24\u9636\u6bb5\u603b\u7ed3\u7cfb\u7edf\uff0c\u53ef\u4ee5\u5728\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\u7684\u540c\u65f6\u8fdb\u884c\u79bb\u7ebf\u4e34\u5e8a\u603b\u7ed3\u3002", "motivation": "\u6025\u8bca\u533b\u751f\u9700\u8981\u5feb\u901f\u4ece\u5927\u91cf\u7684\u975e\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u4e2d\u8bc6\u522b\u5173\u952e\u4fe1\u606f\uff0c\u8fd9\u7ed9\u4ed6\u4eec\u5e26\u6765\u4e86\u5f88\u5927\u7684\u8d1f\u62c5\u3002", "method": "\u8be5\u7cfb\u7edf\u91c7\u7528\u53cc\u8bbe\u5907\u67b6\u6784\uff0c\u9996\u5148\u4f7f\u7528Jetson Nano-R\u68c0\u7d22\u76f8\u5173\u60a3\u8005\u8bb0\u5f55\u90e8\u5206\uff0c\u7136\u540e\u4f7f\u7528Jetson Nano-S\u751f\u6210\u7ed3\u6784\u5316\u6458\u8981\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5957\u63a5\u5b57\u94fe\u63a5\u8fdb\u884c\u901a\u4fe1\u3002\u68c0\u7d22\u9636\u6bb5\u4f7f\u7528\u672c\u5730\u5b58\u50a8\u7684EHR\uff0c\u5c06\u957f\u7b14\u8bb0\u62c6\u5206\u4e3a\u8bed\u4e49\u8fde\u8d2f\u7684\u90e8\u5206\uff0c\u5e76\u641c\u7d22\u6bcf\u4e2a\u67e5\u8be2\u6700\u76f8\u5173\u7684\u90e8\u5206\u3002\u751f\u6210\u9636\u6bb5\u4f7f\u7528\u672c\u5730\u6258\u7ba1\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u4ece\u68c0\u7d22\u5230\u7684\u6587\u672c\u751f\u6210\u6458\u8981\u3002", "result": "\u5728MIMIC-IV\u548c\u53bb\u8bc6\u522b\u5316\u7684\u771f\u5b9eEHR\u4e0a\u7684\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u5b8c\u5168\u79bb\u7ebf\u7cfb\u7edf\u53ef\u4ee5\u572830\u79d2\u5185\u6709\u6548\u5730\u751f\u6210\u6709\u7528\u7684\u6458\u8981\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u5b9e\u73b0\u79bb\u7ebf\u4e34\u5e8a\u603b\u7ed3\u7684\u53ef\u884c\u6027\uff0c\u8fd9\u6709\u52a9\u4e8e\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\u5e76\u63d0\u9ad8\u6025\u8bca\u533b\u751f\u7684\u5de5\u4f5c\u6548\u7387\u3002"}}
{"id": "2510.06277", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06277", "abs": "https://arxiv.org/abs/2510.06277", "authors": ["Fahim Shahriar", "Cheryl Wang", "Alireza Azimi", "Gautham Vasan", "Hany Hamed Elanwar", "A. Rupam Mahmood", "Colin Bellinger"], "title": "General and Efficient Visual Goal-Conditioned Reinforcement Learning using Object-Agnostic Masks", "comment": null, "summary": "Goal-conditioned reinforcement learning (GCRL) allows agents to learn diverse\nobjectives using a unified policy. The success of GCRL, however, is contingent\non the choice of goal representation. In this work, we propose a mask-based\ngoal representation system that provides object-agnostic visual cues to the\nagent, enabling efficient learning and superior generalization. In contrast,\nexisting goal representation methods, such as target state images, 3D\ncoordinates, and one-hot vectors, face issues of poor generalization to unseen\nobjects, slow convergence, and the need for special cameras. Masks can be\nprocessed to generate dense rewards without requiring error-prone distance\ncalculations. Learning with ground truth masks in simulation, we achieved 99.9%\nreaching accuracy on training and unseen test objects. Our proposed method can\nbe utilized to perform pick-up tasks with high accuracy, without using any\npositional information of the target. Moreover, we demonstrate learning from\nscratch and sim-to-real transfer applications using two different physical\nrobots, utilizing pretrained open vocabulary object detection models for mask\ngeneration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u7684\u76ee\u6807\u8868\u793a\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u4e3a\u667a\u80fd\u4f53\u63d0\u4f9b\u4e0e\u5bf9\u8c61\u65e0\u5173\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u548c\u5353\u8d8a\u7684\u6cdb\u5316\u3002", "motivation": "\u73b0\u6709\u76ee\u6807\u8868\u793a\u65b9\u6cd5\uff08\u5982\u76ee\u6807\u72b6\u6001\u56fe\u50cf\u30013D \u5750\u6807\u548c one-hot \u5411\u91cf\uff09\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u5dee\u3001\u6536\u655b\u901f\u5ea6\u6162\u4ee5\u53ca\u9700\u8981\u4e13\u7528\u6444\u50cf\u5934\u7b49\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u4f7f\u7528\u63a9\u7801\u751f\u6210\u5bc6\u96c6\u5956\u52b1\uff0c\u65e0\u9700\u5bb9\u6613\u51fa\u9519\u7684\u8ddd\u79bb\u8ba1\u7b97\u3002\u5728\u6a21\u62df\u4e2d\u4f7f\u7528 ground truth \u63a9\u7801\u8fdb\u884c\u5b66\u4e60\uff0c\u5728\u8bad\u7ec3\u548c\u672a\u89c1\u8fc7\u7684\u6d4b\u8bd5\u5bf9\u8c61\u4e0a\u5b9e\u73b0\u4e86 99.9% \u7684\u5230\u8fbe\u7cbe\u5ea6\u3002", "result": "\u8be5\u65b9\u6cd5\u65e0\u9700\u76ee\u6807\u4f4d\u7f6e\u4fe1\u606f\u5373\u53ef\u9ad8\u7cbe\u5ea6\u5730\u6267\u884c\u62fe\u53d6\u4efb\u52a1\u3002\u6b64\u5916\uff0c\u8fd8\u5c55\u793a\u4e86\u4f7f\u7528\u4e24\u4e2a\u4e0d\u540c\u7684\u7269\u7406\u673a\u5668\u4eba\u4ece\u5934\u5f00\u59cb\u5b66\u4e60\u548c sim-to-real \u8fc1\u79fb\u5e94\u7528\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u68c0\u6d4b\u6a21\u578b\u751f\u6210\u63a9\u7801\u3002", "conclusion": "\u57fa\u4e8e\u63a9\u7801\u7684\u76ee\u6807\u8868\u793a\u7cfb\u7edf\u53ef\u4ee5\u63d0\u9ad8 GCRL \u7684\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u53ef\u4ee5\u5e94\u7528\u4e8e\u5b9e\u9645\u673a\u5668\u4eba\u4efb\u52a1\u3002"}}
{"id": "2510.06538", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06538", "abs": "https://arxiv.org/abs/2510.06538", "authors": ["Jiajie Li", "Huayi Zhang", "Peng Lin", "Jinjun Xiong", "Wei Xu"], "title": "Auto-Prompt Ensemble for LLM Judge", "comment": null, "summary": "We present a novel framework that improves the reliability of LLM judges by\nselectively augmenting LLM with auxiliary evaluation dimensions. Existing LLM\njudges often miss crucial evaluation dimensions because they fail to recognize\nthe implicit standards underlying human assessments. To address this challenge,\nwe propose the Auto-Prompt Ensemble (APE), an adaptive framework that\nautomatically learns evaluation dimensions from its failure cases. APE\nincorporates a confidence-based ensemble mechanism to decide when to adopt the\njudgments from additional evaluation dimensions through a novel confidence\nestimation approach called Collective Confidence. Extensive experiments\ndemonstrate that APE improves the reliability of LLM Judge across diverse\nstandard benchmarks. For instance, APE enhances GPT-4o agreement rate on Reward\nBench from 87.2% to 90.5% in the zero-shot setting. Overall, APE provides a\nprincipled approach for LLM Judge to leverage test-time computation, and bridge\nthe evaluation gap between human and LLM judges.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5730\u7528\u8f85\u52a9\u8bc4\u4f30\u7ef4\u5ea6\u589e\u5f3aLLM\u6765\u63d0\u9ad8LLM\u5224\u65ad\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u7684LLM\u5224\u65ad\u901a\u5e38\u4f1a\u9057\u6f0f\u5173\u952e\u7684\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u56e0\u4e3a\u5b83\u4eec\u65e0\u6cd5\u8bc6\u522b\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u6f5c\u5728\u7684\u9690\u5f0f\u6807\u51c6\u3002", "method": "\u63d0\u51fa\u4e86Auto-Prompt Ensemble (APE)\uff0c\u4e00\u4e2a\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u5931\u8d25\u6848\u4f8b\u4e2d\u81ea\u52a8\u5b66\u4e60\u8bc4\u4f30\u7ef4\u5ea6\u3002APE\u7ed3\u5408\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u96c6\u6210\u673a\u5236\uff0c\u901a\u8fc7\u4e00\u79cd\u540d\u4e3aCollective Confidence\u7684\u65b0\u578b\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u6765\u51b3\u5b9a\u4f55\u65f6\u91c7\u7528\u6765\u81ea\u989d\u5916\u8bc4\u4f30\u7ef4\u5ea6\u7684\u5224\u65ad\u3002", "result": "\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAPE\u63d0\u9ad8\u4e86LLM Judge\u5728\u5404\u79cd\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u53ef\u9760\u6027\u3002\u4f8b\u5982\uff0c\u5728zero-shot\u8bbe\u7f6e\u4e2d\uff0cAPE\u5c06GPT-4o\u5728Reward Bench\u4e0a\u7684\u534f\u8bae\u7387\u4ece87.2%\u63d0\u9ad8\u523090.5%\u3002", "conclusion": "APE\u4e3aLLM Judge\u63d0\u4f9b\u4e86\u4e00\u79cd\u5229\u7528\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u7684\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u5e76\u5f25\u5408\u4e86\u4eba\u7c7b\u548cLLM\u5224\u65ad\u4e4b\u95f4\u7684\u8bc4\u4f30\u5dee\u8ddd\u3002"}}
{"id": "2510.06367", "categories": ["cs.LG", "math.DS", "physics.comp-ph", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2510.06367", "abs": "https://arxiv.org/abs/2510.06367", "authors": ["Luca Wolf", "Tobias Buck", "Bjoern Malte Schaefer"], "title": "Lagrangian neural ODEs: Measuring the existence of a Lagrangian with Helmholtz metrics", "comment": "Accepted for the NeurIPS 2025 Machine Learning and the Physical\n  Sciences workshop. 6 pages, 3 figures", "summary": "Neural ODEs are a widely used, powerful machine learning technique in\nparticular for physics. However, not every solution is physical in that it is\nan Euler-Lagrange equation. We present Helmholtz metrics to quantify this\nresemblance for a given ODE and demonstrate their capabilities on several\nfundamental systems with noise. We combine them with a second order neural ODE\nto form a Lagrangian neural ODE, which allows to learn Euler-Lagrange equations\nin a direct fashion and with zero additional inference cost. We demonstrate\nthat, using only positional data, they can distinguish Lagrangian and\nnon-Lagrangian systems and improve the neural ODE solutions.", "AI": {"tldr": "\u63d0\u51faHelmholtz\u5ea6\u91cf\u6765\u91cf\u5316\u4e00\u4e2a\u7ed9\u5b9a\u7684ODE\u4e0eEuler-Lagrange\u65b9\u7a0b\u7684\u76f8\u4f3c\u6027\uff0c\u5e76\u5c06\u5176\u4e0e\u4e8c\u9636\u795e\u7ecfODE\u7ed3\u5408\uff0c\u5f62\u6210\u62c9\u683c\u6717\u65e5\u795e\u7ecfODE\uff0c\u53ef\u4ee5\u76f4\u63a5\u5b66\u4e60Euler-Lagrange\u65b9\u7a0b\uff0c\u4e14\u6ca1\u6709\u989d\u5916\u7684\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u5e76\u975e\u6240\u6709\u89e3\u5728\u7269\u7406\u5b66\u4e2d\u90fd\u662f\u7269\u7406\u7684\uff0c\u56e0\u4e3a\u5b83\u662fEuler-Lagrange\u65b9\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86Helmholtz\u5ea6\u91cf\uff0c\u5e76\u5c06\u5176\u4e0e\u4e8c\u9636\u795e\u7ecfODE\u7ed3\u5408\u3002", "result": "\u4ec5\u4f7f\u7528\u4f4d\u7f6e\u6570\u636e\uff0c\u4ed6\u4eec\u53ef\u4ee5\u533a\u5206\u62c9\u683c\u6717\u65e5\u548c\u975e\u62c9\u683c\u6717\u65e5\u7cfb\u7edf\uff0c\u5e76\u6539\u8fdb\u795e\u7ecfODE\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u62c9\u683c\u6717\u65e5\u795e\u7ecfODE\u53ef\u4ee5\u76f4\u63a5\u5b66\u4e60Euler-Lagrange\u65b9\u7a0b\uff0c\u4e14\u6ca1\u6709\u989d\u5916\u7684\u63a8\u7406\u6210\u672c\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f7f\u7528\u4f4d\u7f6e\u6570\u636e\u6765\u533a\u5206\u62c9\u683c\u6717\u65e5\u548c\u975e\u62c9\u683c\u6717\u65e5\u7cfb\u7edf\uff0c\u5e76\u6539\u8fdb\u795e\u7ecfODE\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.06265", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06265", "abs": "https://arxiv.org/abs/2510.06265", "authors": ["Aisha Alansari", "Hamzah Luqman"], "title": "A Comprehensive Survey of Hallucination in Large Language Models: Causes, Detection, and Mitigation", "comment": null, "summary": "Large language models (LLMs) have transformed natural language processing,\nachieving remarkable performance across diverse tasks. However, their\nimpressive fluency often comes at the cost of producing false or fabricated\ninformation, a phenomenon known as hallucination. Hallucination refers to the\ngeneration of content by an LLM that is fluent and syntactically correct but\nfactually inaccurate or unsupported by external evidence. Hallucinations\nundermine the reliability and trustworthiness of LLMs, especially in domains\nrequiring factual accuracy. This survey provides a comprehensive review of\nresearch on hallucination in LLMs, with a focus on causes, detection, and\nmitigation. We first present a taxonomy of hallucination types and analyze\ntheir root causes across the entire LLM development lifecycle, from data\ncollection and architecture design to inference. We further examine how\nhallucinations emerge in key natural language generation tasks. Building on\nthis foundation, we introduce a structured taxonomy of detection approaches and\nanother taxonomy of mitigation strategies. We also analyze the strengths and\nlimitations of current detection and mitigation approaches and review existing\nevaluation benchmarks and metrics used to quantify LLMs hallucinations.\nFinally, we outline key open challenges and promising directions for future\nresearch, providing a foundation for the development of more truthful and\ntrustworthy LLMs.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4e5f\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u4ea7\u751f\u4e0d\u51c6\u786e\u6216\u634f\u9020\u7684\u4fe1\u606f\u3002", "motivation": "LLMs\u7684\u5e7b\u89c9\u95ee\u9898\u635f\u5bb3\u4e86\u5176\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u9886\u57df\u3002", "method": "\u672c\u6587\u5168\u9762\u56de\u987e\u4e86LLMs\u4e2d\u5e7b\u89c9\u7684\u7814\u7a76\uff0c\u91cd\u70b9\u5173\u6ce8\u539f\u56e0\u3001\u68c0\u6d4b\u548c\u7f13\u89e3\u3002\u63d0\u51fa\u4e86\u5e7b\u89c9\u7c7b\u578b\u7684\u5206\u7c7b\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5728LLM\u5f00\u53d1\u751f\u547d\u5468\u671f\u4e2d\u7684\u6839\u672c\u539f\u56e0\u3002\u6784\u5efa\u4e86\u68c0\u6d4b\u65b9\u6cd5\u548c\u7f13\u89e3\u7b56\u7565\u7684\u7ed3\u6784\u5316\u5206\u7c7b\u3002", "result": "\u5206\u6790\u4e86\u5f53\u524d\u68c0\u6d4b\u548c\u7f13\u89e3\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u56de\u987e\u4e86\u7528\u4e8e\u91cf\u5316LLMs\u5e7b\u89c9\u7684\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u548c\u6307\u6807\u3002", "conclusion": "\u6982\u8ff0\u4e86\u672a\u6765\u7684\u5173\u952e\u5f00\u653e\u6311\u6218\u548c\u6709\u5e0c\u671b\u7684\u65b9\u5411\uff0c\u4e3a\u5f00\u53d1\u66f4\u771f\u5b9e\u548c\u503c\u5f97\u4fe1\u8d56\u7684LLMs\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.06281", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06281", "abs": "https://arxiv.org/abs/2510.06281", "authors": ["Chenyang Li", "Qin Li", "Haimin Wang", "Bo Shen"], "title": "Improving the Spatial Resolution of GONG Solar Images to GST Quality Using Deep Learning", "comment": "5 pages; accepted as a workshop paper in ICDM 2025", "summary": "High-resolution (HR) solar imaging is crucial for capturing fine-scale\ndynamic features such as filaments and fibrils. However, the spatial resolution\nof the full-disk H$\\alpha$ images is limited and insufficient to resolve these\nsmall-scale structures. To address this, we propose a GAN-based superresolution\napproach to enhance low-resolution (LR) full-disk H$\\alpha$ images from the\nGlobal Oscillation Network Group (GONG) to a quality comparable with HR\nobservations from the Big Bear Solar Observatory/Goode Solar Telescope\n(BBSO/GST). We employ Real-ESRGAN with Residual-in-Residual Dense Blocks and a\nrelativistic discriminator. We carefully aligned GONG-GST pairs. The model\neffectively recovers fine details within sunspot penumbrae and resolves fine\ndetails in filaments and fibrils, achieving an average mean squared error (MSE)\nof 467.15, root mean squared error (RMSE) of 21.59, and cross-correlation (CC)\nof 0.7794. Slight misalignments between image pairs limit quantitative\nperformance, which we plan to address in future work alongside dataset\nexpansion to further improve reconstruction quality.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGAN\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5c06\u4f4e\u5206\u8fa8\u7387\u7684\u5168\u65e5\u9762H\u03b1\u56fe\u50cf\u589e\u5f3a\u5230\u4e0e\u9ad8\u5206\u8fa8\u7387\u89c2\u6d4b\u76f8\u5f53\u7684\u8d28\u91cf\u3002", "motivation": "\u5168\u65e5\u9762H\u03b1\u56fe\u50cf\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u6709\u9650\uff0c\u4e0d\u8db3\u4ee5\u5206\u8fa8\u5c0f\u5c3a\u5ea6\u7ed3\u6784\uff0c\u800c\u9ad8\u5206\u8fa8\u7387\u7684\u592a\u9633\u6210\u50cf\u5bf9\u4e8e\u6355\u6349\u7cbe\u7ec6\u5c3a\u5ea6\u7684\u52a8\u6001\u7279\u5f81\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u5177\u6709\u6b8b\u5dee\u5bc6\u96c6\u5757\u548c\u76f8\u5bf9\u8bba\u5224\u522b\u5668\u7684Real-ESRGAN\u3002", "result": "\u8be5\u6a21\u578b\u6709\u6548\u5730\u6062\u590d\u4e86\u592a\u9633\u9ed1\u5b50\u534a\u5f71\u5185\u7684\u7cbe\u7ec6\u7ec6\u8282\uff0c\u5e76\u89e3\u51b3\u4e86\u7ec6\u4e1d\u548c\u7ea4\u7ef4\u4e2d\u7684\u7cbe\u7ec6\u7ec6\u8282\uff0c\u5e73\u5747\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u4e3a467.15\uff0c\u5747\u65b9\u6839\u8bef\u5dee\uff08RMSE\uff09\u4e3a21.59\uff0c\u4e92\u76f8\u5173\uff08CC\uff09\u4e3a0.7794\u3002", "conclusion": "\u56fe\u50cf\u5bf9\u4e4b\u95f4\u7684\u8f7b\u5fae\u9519\u4f4d\u9650\u5236\u4e86\u5b9a\u91cf\u6027\u80fd\uff0c\u6211\u4eec\u8ba1\u5212\u5728\u672a\u6765\u7684\u5de5\u4f5c\u4e2d\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u540c\u65f6\u6269\u5927\u6570\u636e\u96c6\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2510.06587", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06587", "abs": "https://arxiv.org/abs/2510.06587", "authors": ["Jingbo Yang", "Bairu Hou", "Wei Wei", "Shiyu Chang", "Yujia Bao"], "title": "WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks", "comment": null, "summary": "Large language model (LLM) agents are becoming competent at straightforward\nweb tasks, such as opening an item page or submitting a form, but still\nstruggle with objectives that require long horizon navigation, large scale\ninformation extraction, and reasoning under constraints. We present WebDART, a\ngeneral framework that enables a single LLM to handle such complex chores.\nWebDART (i) dynamically decomposes each objective into three focused subtasks:\nnavigation, information extraction, and execution, so the model concentrates on\none skill at a time, and (ii) continuously replans the decomposition as new\nwebpages are revealed, taking advantage of newly discovered filters or\nshortcuts and avoiding redundant exploration. Evaluated on WebChoreArena,\nWebDART lifts success rates by up to 13.7 percentage points over previous SOTA\nagents, while matching their performance on the easier WebArena suite and\ncompleting tasks with up to 14.7 fewer navigation steps.", "AI": {"tldr": "WebDART\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u5206\u89e3\u4efb\u52a1\u548c\u6301\u7eed\u91cd\u65b0\u89c4\u5212\u6765\u63d0\u5347LLM\u5728\u590d\u6742web\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709LLM agent\u5728\u9700\u8981\u957f\u7a0b\u5bfc\u822a\u3001\u5927\u89c4\u6a21\u4fe1\u606f\u63d0\u53d6\u548c\u7ea6\u675f\u63a8\u7406\u7684\u590d\u6742web\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faWebDART\u6846\u67b6\uff0c\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u5bfc\u822a\u3001\u4fe1\u606f\u63d0\u53d6\u548c\u6267\u884c\u4e09\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u6839\u636e\u65b0\u9875\u9762\u4fe1\u606f\u6301\u7eed\u91cd\u65b0\u89c4\u5212\u3002", "result": "\u5728WebChoreArena\u4e0a\uff0cWebDART\u7684\u6210\u529f\u7387\u6bd4\u4e4b\u524d\u7684SOTA agent \u63d0\u9ad8\u4e8613.7\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u5728WebArena\u4e0a\u4fdd\u6301\u4e86\u76f8\u540c\u7684\u6027\u80fd\uff0c\u5e76\u51cf\u5c11\u4e8614.7\u4e2a\u5bfc\u822a\u6b65\u9aa4\u3002", "conclusion": "WebDART\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86LLM agent\u5728\u590d\u6742web\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.06266", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06266", "abs": "https://arxiv.org/abs/2510.06266", "authors": ["Rohitash Chandra", "Yathin Suresh", "Divyansh Raj Sinha", "Sanchit Jindal"], "title": "Language models for longitudinal analysis of abusive content in Billboard Music Charts", "comment": null, "summary": "There is no doubt that there has been a drastic increase in abusive and\nsexually explicit content in music, particularly in Billboard Music Charts.\nHowever, there is a lack of studies that validate the trend for effective\npolicy development, as such content has harmful behavioural changes in children\nand youths. In this study, we utilise deep learning methods to analyse songs\n(lyrics) from Billboard Charts of the United States in the last seven decades.\nWe provide a longitudinal study using deep learning and language models and\nreview the evolution of content using sentiment analysis and abuse detection,\nincluding sexually explicit content. Our results show a significant rise in\nexplicit content in popular music from 1990 onwards. Furthermore, we find an\nincreasing prevalence of songs with lyrics containing profane, sexually\nexplicit, and otherwise inappropriate language. The longitudinal analysis of\nthe ability of language models to capture nuanced patterns in lyrical content,\nreflecting shifts in societal norms and language use over time.", "AI": {"tldr": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u516c\u544a\u724c\u97f3\u4e50\u6392\u884c\u699c\u4e2d\u8fb1\u9a82\u6027\u548c\u6027\u9732\u9aa8\u5185\u5bb9\u7684\u8d8b\u52bf\u3002", "motivation": "\u7f3a\u4e4f\u9a8c\u8bc1\u97f3\u4e50\u4e2d\u6b64\u7c7b\u5185\u5bb9\u8d8b\u52bf\u7684\u7814\u7a76\uff0c\u800c\u8fd9\u4e9b\u5185\u5bb9\u4f1a\u5bf9\u513f\u7ae5\u548c\u9752\u5c11\u5e74\u4ea7\u751f\u6709\u5bb3\u7684\u884c\u4e3a\u53d8\u5316\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5206\u6790\u4e86\u8fc7\u53bb\u4e03\u5341\u5e74\u7f8e\u56fd\u516c\u544a\u724c\u6392\u884c\u699c\u4e0a\u7684\u6b4c\u66f2\u6b4c\u8bcd\uff0c\u4f7f\u7528\u60c5\u611f\u5206\u6790\u548c\u6ee5\u7528\u68c0\u6d4b\u56de\u987e\u4e86\u5185\u5bb9\u7684\u53d1\u5c55\u3002", "result": "\u4ece 1990 \u5e74\u4ee3\u5f00\u59cb\uff0c\u70ed\u95e8\u97f3\u4e50\u4e2d\u7684\u9732\u9aa8\u5185\u5bb9\u663e\u7740\u589e\u52a0\u3002\u5305\u542b\u4eb5\u6e0e\u3001\u6027\u9732\u9aa8\u548c\u4e0d\u5f53\u8bed\u8a00\u7684\u6b4c\u66f2\u8d8a\u6765\u8d8a\u666e\u904d\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6355\u6349\u6b4c\u8bcd\u5185\u5bb9\u4e2d\u7ec6\u5fae\u7684\u6a21\u5f0f\uff0c\u53cd\u6620\u4e86\u793e\u4f1a\u89c4\u8303\u548c\u8bed\u8a00\u4f7f\u7528\u7684\u8f6c\u53d8\u3002"}}
{"id": "2510.06292", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06292", "abs": "https://arxiv.org/abs/2510.06292", "authors": ["Yike Wu", "Yiwei Wang", "Yujun Cai"], "title": "ChainMPQ: Interleaved Text-Image Reasoning Chains for Mitigating Relation Hallucinations", "comment": null, "summary": "While Large Vision-Language Models (LVLMs) achieve strong performance in\nmultimodal tasks, hallucinations continue to hinder their reliability. Among\nthe three categories of hallucinations, which include object, attribute, and\nrelation, relation hallucinations account for the largest proportion but have\nreceived the least attention. To address this issue, we propose ChainMPQ\n(Multi-Perspective Questions guided Interleaved Chain of Image and Text), a\ntraining-free method that improves relational inference in LVLMs by utilizing\naccumulated textual and visual memories. ChainMPQ first extracts subject and\nobject keywords from the question to enhance the corresponding image regions.\nIt then constructs multi-perspective questions that focus on the three core\ncomponents of a relationship: the subject, the object, and the relation that\nlinks them. These questions are sequentially input to the model, with textual\nand visual memories from earlier steps providing supporting context for\nsubsequent ones, thereby forming an interleaved chain of images and text that\nguides progressive relational reasoning. Experiments on multiple LVLMs and\nbenchmarks show that ChainMPQ substantially reduces relation hallucinations,\nwhile ablation studies further validate the effectiveness of its three core\nmodules.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a ChainMPQ \u7684\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u51cf\u5c11\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u4e2d\u7684\u5173\u7cfb\u5e7b\u89c9\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5e7b\u89c9\u95ee\u9898\u4ecd\u7136\u5b58\u5728\uff0c\u5c24\u5176\u4ee5\u5173\u7cfb\u5e7b\u89c9\u6700\u4e3a\u7a81\u51fa\uff0c\u4f46\u53d7\u5230\u7684\u5173\u6ce8\u6700\u5c11\u3002", "method": "ChainMPQ \u9996\u5148\u4ece\u95ee\u9898\u4e2d\u63d0\u53d6\u4e3b\u8bed\u548c\u5bbe\u8bed\u5173\u952e\u8bcd\uff0c\u4ee5\u589e\u5f3a\u76f8\u5e94\u7684\u56fe\u50cf\u533a\u57df\u3002\u7136\u540e\uff0c\u5b83\u6784\u5efa\u591a\u89d2\u5ea6\u95ee\u9898\uff0c\u5173\u6ce8\u5173\u7cfb\u7684\u4e09\u8981\u7d20\uff1a\u4e3b\u8bed\u3001\u5bbe\u8bed\u4ee5\u53ca\u8fde\u63a5\u5b83\u4eec\u7684\u8054\u7cfb\u3002\u8fd9\u4e9b\u95ee\u9898\u88ab\u4f9d\u6b21\u8f93\u5165\u6a21\u578b\uff0c\u65e9\u671f\u6b65\u9aa4\u7684\u6587\u672c\u548c\u89c6\u89c9\u8bb0\u5fc6\u4e3a\u540e\u7eed\u6b65\u9aa4\u63d0\u4f9b\u652f\u6301\u4e0a\u4e0b\u6587\uff0c\u4ece\u800c\u5f62\u6210\u56fe\u50cf\u548c\u6587\u672c\u4ea4\u9519\u7684\u94fe\u6761\uff0c\u5f15\u5bfc\u6e10\u8fdb\u7684\u5173\u7cfb\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2a LVLM \u548c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cChainMPQ \u5927\u5e45\u51cf\u5c11\u4e86\u5173\u7cfb\u5e7b\u89c9\u3002", "conclusion": "\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86 ChainMPQ \u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.06600", "categories": ["cs.AI", "H.3.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.06600", "abs": "https://arxiv.org/abs/2510.06600", "authors": ["Zhaochun Ren", "Zhou Yang", "Chenglong Ye", "Haizhou Sun", "Chao Chen", "Xiaofei Zhu", "Xiangwen Liao"], "title": "Fine-Grained Emotion Recognition via In-Context Learning", "comment": "9 pages, 10 figures, 4 tables", "summary": "Fine-grained emotion recognition aims to identify the emotional type in\nqueries through reasoning and decision-making processes, playing a crucial role\nin various systems. Recent methods use In-Context Learning (ICL), enhancing the\nrepresentation of queries in the reasoning process through semantically similar\nexamples, while further improving emotion recognition by explaining the\nreasoning mechanisms. However, these methods enhance the reasoning process but\noverlook the decision-making process. This paper investigates decision-making\nin fine-grained emotion recognition through prototype theory. We show that ICL\nrelies on similarity matching between query representations and emotional\nprototypes within the model, where emotion-accurate representations are\ncritical. However, semantically similar examples often introduce emotional\ndiscrepancies, hindering accurate representations and causing errors. To\naddress this, we propose Emotion In-Context Learning (EICL), which introduces\nemotionally similar examples and uses a dynamic soft-label strategy to improve\nquery representations in the emotion reasoning process. A two-stage exclusion\nstrategy is then employed to assess similarity from multiple angles, further\noptimizing the decision-making process. Extensive experiments show that EICL\nsignificantly outperforms ICL on multiple datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u60c5\u611f\u4e0a\u4e0b\u6587\u5b66\u4e60 (EICL) \u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u60c5\u611f\u8bc6\u522b\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u60c5\u611f\u4e0a\u76f8\u4f3c\u7684\u793a\u4f8b\u548c\u52a8\u6001\u8f6f\u6807\u7b7e\u7b56\u7565\u6765\u6539\u8fdb\u67e5\u8be2\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u6392\u9664\u7b56\u7565\u6765\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u7684\u4f8b\u5b50\u589e\u5f3a\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u67e5\u8be2\u8868\u793a\uff0c\u4f46\u5ffd\u7565\u4e86\u51b3\u7b56\u8fc7\u7a0b\u3002\u8bed\u4e49\u76f8\u4f3c\u7684\u4f8b\u5b50\u4f1a\u5f15\u5165\u60c5\u611f\u5dee\u5f02\uff0c\u963b\u788d\u51c6\u786e\u7684\u8868\u793a\u5e76\u5bfc\u81f4\u9519\u8bef\u3002", "method": "\u672c\u6587\u901a\u8fc7\u539f\u578b\u7406\u8bba\u7814\u7a76\u4e86\u7ec6\u7c92\u5ea6\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\u63d0\u51fa\u4e86\u60c5\u611f\u4e0a\u4e0b\u6587\u5b66\u4e60 (EICL)\uff0c\u5b83\u5f15\u5165\u60c5\u611f\u4e0a\u76f8\u4f3c\u7684\u4f8b\u5b50\uff0c\u5e76\u4f7f\u7528\u52a8\u6001\u8f6f\u6807\u7b7e\u7b56\u7565\u6765\u6539\u8fdb\u60c5\u611f\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u67e5\u8be2\u8868\u793a\u3002\u7136\u540e\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6392\u9664\u7b56\u7565\u4ece\u591a\u4e2a\u89d2\u5ea6\u8bc4\u4f30\u76f8\u4f3c\u6027\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cEICL \u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u7740\u4f18\u4e8e ICL\u3002", "conclusion": "EICL \u901a\u8fc7\u5f15\u5165\u60c5\u611f\u76f8\u4f3c\u7684\u4f8b\u5b50\u548c\u52a8\u6001\u8f6f\u6807\u7b7e\u7b56\u7565\uff0c\u4ee5\u53ca\u4e24\u9636\u6bb5\u6392\u9664\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7ec6\u7c92\u5ea6\u60c5\u611f\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2510.06381", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06381", "abs": "https://arxiv.org/abs/2510.06381", "authors": ["Tristan Cazenave"], "title": "Monte Carlo Permutation Search", "comment": null, "summary": "We propose Monte Carlo Permutation Search (MCPS), a general-purpose Monte\nCarlo Tree Search (MCTS) algorithm that improves upon the GRAVE algorithm. MCPS\nis relevant when deep reinforcement learning is not an option, or when the\ncomputing power available before play is not substantial, such as in General\nGame Playing, for example. The principle of MCPS is to include in the\nexploration term of a node the statistics on all the playouts that contain all\nthe moves on the path from the root to the node. We extensively test MCPS on a\nvariety of games: board games, wargame, investment game, video game and\nmulti-player games. MCPS has better results than GRAVE in all the two-player\ngames. It has equivalent results for multi-player games because these games are\ninherently balanced even when players have different strengths. We also show\nthat using abstract codes for moves instead of exact codes can be beneficial to\nboth MCPS and GRAVE, as they improve the permutation statistics and the AMAF\nstatistics. We also provide a mathematical derivation of the formulas used for\nweighting the three sources of statistics. These formulas are an improvement on\nthe GRAVE formula since they no longer use the bias hyperparameter of GRAVE.\nMoreover, MCPS is not sensitive to the ref hyperparameter.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8499\u7279\u5361\u6d1b\u7f6e\u6362\u641c\u7d22 (MCPS) \u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u6539\u8fdb\u4e86 GRAVE \u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e0d\u53ef\u884c\u6216\u8ba1\u7b97\u80fd\u529b\u6709\u9650\u7684\u60c5\u51b5\u3002", "motivation": "\u5728\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e0d\u53ef\u884c\u6216\u9884\u8ba1\u7b97\u80fd\u529b\u4e0d\u8db3\u7684\u60c5\u51b5\u4e0b\uff0c\u4f8b\u5982\u5728\u901a\u7528\u6e38\u620f\u535a\u5f08\u4e2d\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u641c\u7d22\u7b97\u6cd5\u3002", "method": "MCPS \u7684\u539f\u7406\u662f\u5728\u8282\u70b9\u7684\u63a2\u7d22\u9879\u4e2d\u5305\u542b\u6240\u6709\u5305\u542b\u4ece\u6839\u5230\u8282\u70b9\u8def\u5f84\u4e0a\u6240\u6709\u79fb\u52a8\u7684 playout \u7684\u7edf\u8ba1\u4fe1\u606f\u3002", "result": "MCPS \u5728\u6240\u6709\u53cc\u4eba\u6e38\u620f\u4e2d\u90fd\u6bd4 GRAVE \u7b97\u6cd5\u6709\u66f4\u597d\u7684\u7ed3\u679c\u3002\u5bf9\u4e8e\u591a\u4eba\u6e38\u620f\uff0c\u7ed3\u679c\u662f\u7b49\u6548\u7684\uff0c\u56e0\u4e3a\u5373\u4f7f\u73a9\u5bb6\u5177\u6709\u4e0d\u540c\u7684\u4f18\u52bf\uff0c\u8fd9\u4e9b\u6e38\u620f\u672c\u8d28\u4e0a\u4e5f\u662f\u5e73\u8861\u7684\u3002\u4f7f\u7528\u62bd\u8c61\u4ee3\u7801\u4ee3\u66ff\u7cbe\u786e\u4ee3\u7801\u53ef\u4ee5\u6539\u5584\u7f6e\u6362\u7edf\u8ba1\u548c AMAF \u7edf\u8ba1\u3002", "conclusion": "MCPS \u7b97\u6cd5\u901a\u8fc7\u6539\u8fdb\u7f6e\u6362\u7edf\u8ba1\u548c\u6d88\u9664\u8d85\u53c2\u6570\uff0c\u5728\u591a\u79cd\u6e38\u620f\u7c7b\u578b\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e GRAVE \u7b97\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2510.06275", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06275", "abs": "https://arxiv.org/abs/2510.06275", "authors": ["Ranjan Mishra", "Julian I. Bibo", "Quinten van Engelen", "Henk Schaapman"], "title": "Reproducibility Study of \"XRec: Large Language Models for Explainable Recommendation\"", "comment": null, "summary": "In this study, we reproduced the work done in the paper \"XRec: Large Language\nModels for Explainable Recommendation\" by Ma et al. (2024). The original\nauthors introduced XRec, a model-agnostic collaborative instruction-tuning\nframework that enables large language models (LLMs) to provide users with\ncomprehensive explanations of generated recommendations. Our objective was to\nreplicate the results of the original paper, albeit using Llama 3 as the LLM\nfor evaluation instead of GPT-3.5-turbo. We built on the source code provided\nby Ma et al. (2024) to achieve our goal. Our work extends the original paper by\nmodifying the input embeddings or deleting the output embeddings of XRec's\nMixture of Experts module. Based on our results, XRec effectively generates\npersonalized explanations and its stability is improved by incorporating\ncollaborative information. However, XRec did not consistently outperform all\nbaseline models in every metric. Our extended analysis further highlights the\nimportance of the Mixture of Experts embeddings in shaping the explanation\nstructures, showcasing how collaborative signals interact with language\nmodeling. Through our work, we provide an open-source evaluation implementation\nthat enhances accessibility for researchers and practitioners alike. Our\ncomplete code repository can be found at\nhttps://github.com/julianbibo/xrec-reproducibility.", "AI": {"tldr": "\u672c\u6587\u91cd\u73b0\u4e86 Ma et al. (2024) \u7684\u8bba\u6587\u201cXRec: \u7528\u4e8e\u53ef\u89e3\u91ca\u63a8\u8350\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u201d\u4e2d\u7684\u5de5\u4f5c\uff0c\u5e76\u4f7f\u7528 Llama 3 \u4ee3\u66ff GPT-3.5-turbo \u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "motivation": "\u91cd\u73b0 XRec \u8bba\u6587\u7684\u7ed3\u679c\uff0c\u5e76\u4f7f\u7528 Llama 3 \u4f5c\u4e3a LLM \u8fdb\u884c\u8bc4\u4f30\u3002", "method": "\u57fa\u4e8e Ma et al. (2024) \u63d0\u4f9b\u7684\u6e90\u4ee3\u7801\uff0c\u4fee\u6539\u4e86 XRec \u7684 Mixture of Experts \u6a21\u5757\u7684\u8f93\u5165\u6216\u5220\u9664\u4e86\u8f93\u51fa\u5d4c\u5165\u3002", "result": "XRec \u6709\u6548\u5730\u751f\u6210\u4e2a\u6027\u5316\u89e3\u91ca\uff0c\u5e76\u901a\u8fc7\u6574\u5408\u534f\u4f5c\u4fe1\u606f\u63d0\u9ad8\u4e86\u5176\u7a33\u5b9a\u6027\u3002\u7136\u800c\uff0cXRec \u5728\u6bcf\u4e2a\u6307\u6807\u4e0a\u5e76\u672a\u59cb\u7ec8\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u5f3a\u8c03\u4e86 Mixture of Experts \u5d4c\u5165\u5728\u5851\u9020\u89e3\u91ca\u7ed3\u6784\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5c55\u793a\u4e86\u534f\u4f5c\u4fe1\u53f7\u5982\u4f55\u4e0e\u8bed\u8a00\u5efa\u6a21\u4e92\u52a8\u3002\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u6e90\u8bc4\u4f30\u5b9e\u73b0\uff0c\u589e\u5f3a\u4e86\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u7684\u53ef\u8bbf\u95ee\u6027\u3002"}}
{"id": "2510.06295", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06295", "abs": "https://arxiv.org/abs/2510.06295", "authors": ["Young D. Kwon", "Abhinav Mehrotra", "Malcolm Chadwick", "Alberto Gil Ramos", "Sourav Bhattacharya"], "title": "Efficient High-Resolution Image Editing with Hallucination-Aware Loss and Adaptive Tiling", "comment": "Preprint. Under review", "summary": "High-resolution (4K) image-to-image synthesis has become increasingly\nimportant for mobile applications. Existing diffusion models for image editing\nface significant challenges, in terms of memory and image quality, when\ndeployed on resource-constrained devices. In this paper, we present\nMobilePicasso, a novel system that enables efficient image editing at high\nresolutions, while minimising computational cost and memory usage.\nMobilePicasso comprises three stages: (i) performing image editing at a\nstandard resolution with hallucination-aware loss, (ii) applying latent\nprojection to overcome going to the pixel space, and (iii) upscaling the edited\nimage latent to a higher resolution with adaptive context-preserving tiling.\nOur user study with 46 participants reveals that MobilePicasso not only\nimproves image quality by 18-48% but reduces hallucinations by 14-51% over\nexisting methods. MobilePicasso demonstrates significantly lower latency, e.g.,\nup to 55.8$\\times$ speed-up, yet with a small increase in runtime memory, e.g.,\na mere 9% increase over prior work. Surprisingly, the on-device runtime of\nMobilePicasso is observed to be faster than a server-based high-resolution\nimage editing model running on an A100 GPU.", "AI": {"tldr": "MobilePicasso\u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u9ad8\u6548\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7f16\u8f91\u7684\u65b0\u7cfb\u7edf\uff0c\u5b83\u5728\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u4f7f\u7528\u65b9\u9762\u90fd\u8fdb\u884c\u4e86\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u7f16\u8f91\u6269\u6563\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\uff0c\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u4f1a\u9762\u4e34\u5185\u5b58\u548c\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "MobilePicasso\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a\uff08i\uff09\u4f7f\u7528\u611f\u77e5\u5e7b\u89c9\u635f\u5931\u5728\u6807\u51c6\u5206\u8fa8\u7387\u4e0b\u6267\u884c\u56fe\u50cf\u7f16\u8f91\uff1b\uff08ii\uff09\u5e94\u7528\u6f5c\u5728\u6295\u5f71\u4ee5\u907f\u514d\u8fdb\u5165\u50cf\u7d20\u7a7a\u95f4\uff1b\uff08iii\uff09\u4f7f\u7528\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u4fdd\u6301\u5e73\u94fa\u5c06\u7f16\u8f91\u540e\u7684\u56fe\u50cf\u6f5c\u5728\u5730\u4e0a\u91c7\u6837\u5230\u66f4\u9ad8\u7684\u5206\u8fa8\u7387\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cMobilePicasso\u4e0d\u4ec5\u5c06\u56fe\u50cf\u8d28\u91cf\u63d0\u9ad8\u4e8618-48\uff05\uff0c\u800c\u4e14\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5e7b\u89c9\u51cf\u5c11\u4e8614-51\uff05\u3002MobilePicasso\u7684\u5ef6\u8fdf\u663e\u7740\u964d\u4f4e\uff0c\u4f8b\u5982\uff0c\u901f\u5ea6\u63d0\u9ad8\u4e86\u9ad8\u8fbe55.8\u500d\uff0c\u800c\u8fd0\u884c\u65f6\u5185\u5b58\u4ec5\u7565\u6709\u589e\u52a0\uff0c\u4f8b\u5982\uff0c\u6bd4\u5148\u524d\u7684\u5de5\u4f5c\u4ec5\u589e\u52a0\u4e869\uff05\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0cMobilePicasso\u7684\u8bbe\u5907\u4e0a\u8fd0\u884c\u65f6\u95f4\u6bd4\u5728A100 GPU\u4e0a\u8fd0\u884c\u7684\u57fa\u4e8e\u670d\u52a1\u5668\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u66f4\u5feb\u3002", "conclusion": "MobilePicasso\u80fd\u591f\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u9ad8\u6548\u5730\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7f16\u8f91\uff0c\u540c\u65f6\u6700\u5927\u9650\u5ea6\u5730\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u4f7f\u7528\u3002"}}
{"id": "2510.06674", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06674", "abs": "https://arxiv.org/abs/2510.06674", "authors": ["Cen", "Zhao", "Tiantian Zhang", "Hanchen Su", "Yufeng", "Zhang", "Shaowei Su", "Mingzhi Xu", "Yu", "Liu", "Wei Han", "Jeremy Werner", "Claire Na Cheng", "Yashar Mehdad"], "title": "Agent-in-the-Loop: A Data Flywheel for Continuous Improvement in LLM-based Customer Support", "comment": "EMNLP 2025 Industry Track submission (Paper #305). Preprint. Main\n  text within the 7-page industry limit (references/appendices excluded).\n  Contains multiple figures and tables", "summary": "We introduce an Agent-in-the-Loop (AITL) framework that implements a\ncontinuous data flywheel for iteratively improving an LLM-based customer\nsupport system. Unlike standard offline approaches that rely on batch\nannotations, AITL integrates four key types of annotations directly into live\ncustomer operations: (1) pairwise response preferences, (2) agent adoption and\nrationales, (3) knowledge relevance checks, and (4) identification of missing\nknowledge. These feedback signals seamlessly feed back into models' updates,\nreducing retraining cycles from months to weeks. Our production pilot involving\nUS-based customer support agents demonstrated significant improvements in\nretrieval accuracy (+11.7% recall@75, +14.8% precision@8), generation quality\n(+8.4% helpfulness) and agent adoption rates (+4.5%). These results underscore\nthe effectiveness of embedding human feedback loops directly into operational\nworkflows to continuously refine LLM-based customer support system.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2aAgent-in-the-Loop (AITL) \u6846\u67b6\uff0c\u7528\u4e8e\u8fed\u4ee3\u6539\u8fdb\u57fa\u4e8eLLM\u7684\u5ba2\u6237\u652f\u6301\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edf\u7684\u79bb\u7ebf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6279\u91cf\u6ce8\u91ca\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "AITL \u5c06\u56db\u79cd\u5173\u952e\u7c7b\u578b\u7684\u6ce8\u91ca\u76f4\u63a5\u96c6\u6210\u5230\u5b9e\u65f6\u5ba2\u6237\u8fd0\u8425\u4e2d\uff1a(1) \u6210\u5bf9\u54cd\u5e94\u504f\u597d\uff0c(2) \u4ee3\u7406\u91c7\u7528\u548c\u7406\u7531\uff0c(3) \u77e5\u8bc6\u76f8\u5173\u6027\u68c0\u67e5\uff0c(4) \u8bc6\u522b\u7f3a\u5931\u7684\u77e5\u8bc6\u3002", "result": "\u5728\u7f8e\u56fd\u5ba2\u6237\u652f\u6301\u4ee3\u7406\u7684\u751f\u4ea7\u8bd5\u70b9\u4e2d\uff0c\u68c0\u7d22\u51c6\u786e\u7387\uff08+11.7% recall@75\uff0c+14.8% precision@8\uff09\uff0c\u751f\u6210\u8d28\u91cf\uff08+8.4% \u5e2e\u52a9\u6027\uff09\u548c\u4ee3\u7406\u91c7\u7528\u7387\uff08+4.5%\uff09\u5f97\u5230\u4e86\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u5c06\u4eba\u5de5\u53cd\u9988\u5faa\u73af\u76f4\u63a5\u5d4c\u5165\u5230\u8fd0\u8425\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u53ef\u4ee5\u6301\u7eed\u6539\u8fdb\u57fa\u4e8eLLM\u7684\u5ba2\u6237\u652f\u6301\u7cfb\u7edf\u3002"}}
{"id": "2510.06388", "categories": ["cs.LG", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.06388", "abs": "https://arxiv.org/abs/2510.06388", "authors": ["Yuxuan Lu", "Yifan Wu", "Jason Hartline", "Lunjia Hu"], "title": "Making and Evaluating Calibrated Forecasts", "comment": null, "summary": "Calibrated predictions can be reliably interpreted as probabilities. An\nimportant step towards achieving better calibration is to design an appropriate\ncalibration measure to meaningfully assess the miscalibration level of a\npredictor. A recent line of work initiated by Haghtalab et al. [2024] studies\nthe design of truthful calibration measures: a truthful measure is minimized\nwhen a predictor outputs the true probabilities, whereas a non-truthful measure\nincentivizes the predictor to lie so as to appear more calibrated. All previous\ncalibration measures were non-truthful until Hartline et al. [2025] introduced\nthe first perfectly truthful calibration measures for binary prediction tasks\nin the batch setting.\n  We introduce a perfectly truthful calibration measure for multi-class\nprediction tasks, generalizing the work of Hartline et al. [2025] beyond binary\nprediction. We study common methods of extending calibration measures from\nbinary to multi-class prediction and identify ones that do or do not preserve\ntruthfulness. In addition to truthfulness, we mathematically prove and\nempirically verify that our calibration measure exhibits superior robustness:\nit robustly preserves the ordering between dominant and dominated predictors,\nregardless of the choice of hyperparameters (bin sizes). This result addresses\nthe non-robustness issue of binned ECE, which has been observed repeatedly in\nprior work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u5206\u7c7b\u9884\u6d4b\u4efb\u52a1\u7684\u5b8c\u5168\u771f\u5b9e\u7684\u6821\u51c6\u5ea6\u91cf\uff0c\u5e76\u7814\u7a76\u4e86\u4ece\u4e8c\u5143\u5230\u591a\u7c7b\u9884\u6d4b\u6269\u5c55\u6821\u51c6\u5ea6\u91cf\u7684\u5e38\u7528\u65b9\u6cd5\uff0c\u786e\u5b9a\u4e86\u54ea\u4e9b\u65b9\u6cd5\u53ef\u4ee5\u6216\u4e0d\u80fd\u4fdd\u6301\u771f\u5b9e\u6027\u3002", "motivation": "\u8bbe\u8ba1\u5408\u9002\u7684\u6821\u51c6\u5ea6\u91cf\u5bf9\u4e8e\u8bc4\u4f30\u9884\u6d4b\u5668\u7684\u9519\u8bef\u6821\u51c6\u6c34\u5e73\u81f3\u5173\u91cd\u8981\u3002\u4e4b\u524d\u7684\u6821\u51c6\u5ea6\u91cf\u90fd\u662f\u975e\u771f\u5b9e\u7684\uff0c\u76f4\u5230 Hartline \u7b49\u4eba [2025] \u5f15\u5165\u4e86\u7b2c\u4e00\u4e2a\u7528\u4e8e\u6279\u91cf\u8bbe\u7f6e\u4e2d\u4e8c\u5143\u9884\u6d4b\u4efb\u52a1\u7684\u5b8c\u5168\u771f\u5b9e\u7684\u6821\u51c6\u5ea6\u91cf\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u771f\u5b9e\u7684\u6821\u51c6\u5ea6\u91cf\uff0c\u63a8\u5e7f\u4e86 Hartline \u7b49\u4eba [2025] \u7684\u5de5\u4f5c\uff0c\u4f7f\u5176\u8d85\u8d8a\u4e86\u4e8c\u5143\u9884\u6d4b\u3002\u7814\u7a76\u4e86\u4ece\u4e8c\u5143\u5230\u591a\u7c7b\u9884\u6d4b\u6269\u5c55\u6821\u51c6\u5ea6\u91cf\u7684\u5e38\u7528\u65b9\u6cd5\uff0c\u5e76\u786e\u5b9a\u4e86\u54ea\u4e9b\u65b9\u6cd5\u53ef\u4ee5\u6216\u4e0d\u80fd\u4fdd\u6301\u771f\u5b9e\u6027\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u6821\u51c6\u5ea6\u91cf\u5177\u6709\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\uff0c\u53ef\u4ee5\u7a33\u5065\u5730\u4fdd\u6301\u4f18\u52bf\u548c\u52a3\u52bf\u9884\u6d4b\u5668\u4e4b\u95f4\u7684\u987a\u5e8f\uff0c\u800c\u4e0e\u8d85\u53c2\u6570\uff08bin \u5927\u5c0f\uff09\u7684\u9009\u62e9\u65e0\u5173\u3002\u8be5\u7ed3\u679c\u89e3\u51b3\u4e86\u5148\u524d\u5de5\u4f5c\u4e2d\u53cd\u590d\u89c2\u5bdf\u5230\u7684\u5206\u7bb1 ECE \u7684\u975e\u9c81\u68d2\u6027\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5730\u4e3a\u591a\u5206\u7c7b\u9884\u6d4b\u4efb\u52a1\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5b8c\u5168\u771f\u5b9e\u7684\u6821\u51c6\u5ea6\u91cf\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.06304", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06304", "abs": "https://arxiv.org/abs/2510.06304", "authors": ["Robin Kokot", "Wessel Poelman"], "title": "Type and Complexity Signals in Multilingual Question Representations", "comment": "Workshop on Multilingual Representation Learning at EMNLP 2025", "summary": "This work investigates how a multilingual transformer model represents\nmorphosyntactic properties of questions. We introduce the Question Type and\nComplexity (QTC) dataset with sentences across seven languages, annotated with\ntype information and complexity metrics including dependency length, tree\ndepth, and lexical density. Our evaluation extends probing methods to\nregression labels with selectivity controls to quantify gains in\ngeneralizability. We compare layer-wise probes on frozen Glot500-m (Imani et\nal., 2023) representations against subword TF-IDF baselines, and a fine-tuned\nmodel. Results show that statistical features classify questions effectively in\nlanguages with explicit marking, while neural probes capture fine-grained\nstructural complexity patterns better. We use these results to evaluate when\ncontextual representations outperform statistical baselines and whether\nparameter updates reduce the availability of pre-trained linguistic\ninformation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u591a\u8bed\u8a00Transformer\u6a21\u578b\u5982\u4f55\u8868\u793a\u95ee\u9898\u7684\u5f62\u6001\u53e5\u6cd5\u5c5e\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e86\u89e3\u591a\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5904\u7406\u4e0d\u540c\u8bed\u8a00\u4e2d\u95ee\u9898\u7684\u7ed3\u6784\u548c\u590d\u6742\u6027\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u4e03\u79cd\u8bed\u8a00\u7684QTC\u6570\u636e\u96c6\uff0c\u5e76\u6807\u6ce8\u4e86\u95ee\u9898\u7c7b\u578b\u548c\u590d\u6742\u5ea6\u6307\u6807\uff0c\u7136\u540e\u4f7f\u7528\u9009\u62e9\u6027\u63a7\u5236\u7684\u63a2\u6d4b\u65b9\u6cd5\u6765\u8bc4\u4f30\u6a21\u578b\u5728\u56de\u5f52\u6807\u7b7e\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5177\u6709\u663e\u5f0f\u6807\u8bb0\u7684\u8bed\u8a00\u4e2d\uff0c\u7edf\u8ba1\u7279\u5f81\u53ef\u4ee5\u6709\u6548\u5730\u5206\u7c7b\u95ee\u9898\uff0c\u800c\u795e\u7ecf\u63a2\u9488\u53ef\u4ee5\u66f4\u597d\u5730\u6355\u6349\u7ec6\u7c92\u5ea6\u7684\u7ed3\u6784\u590d\u6742\u6027\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7528\u4e8e\u8bc4\u4f30\u4e0a\u4e0b\u6587\u8868\u793a\u4f55\u65f6\u4f18\u4e8e\u7edf\u8ba1\u57fa\u7ebf\uff0c\u4ee5\u53ca\u53c2\u6570\u66f4\u65b0\u662f\u5426\u4f1a\u964d\u4f4e\u9884\u8bad\u7ec3\u8bed\u8a00\u4fe1\u606f\u7684\u53ef\u7528\u6027\u3002"}}
{"id": "2510.06298", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06298", "abs": "https://arxiv.org/abs/2510.06298", "authors": ["Tobias J. Bauer"], "title": "RGBD Gaze Tracking Using Transformer for Feature Fusion", "comment": "Master Thesis with 125 pages, 59 figures, 17 tables", "summary": "Subject of this thesis is the implementation of an AI-based Gaze Tracking\nsystem using RGBD images that contain both color (RGB) and depth (D)\ninformation. To fuse the features extracted from the images, a module based on\nthe Transformer architecture is used. The combination of RGBD input images and\nTransformers was chosen because it has not yet been investigated. Furthermore,\na new dataset is created for training the AI models as existing datasets either\ndo not contain depth information or only contain labels for Gaze Point\nEstimation that are not suitable for the task of Gaze Angle Estimation. Various\nmodel configurations are trained, validated and evaluated on a total of three\ndifferent datasets. The trained models are then to be used in a real-time\npipeline to estimate the gaze direction and thus the gaze point of a person in\nfront of a computer screen. The AI model architecture used in this thesis is\nbased on an earlier work by Lian et al. It uses a Generative Adversarial\nNetwork (GAN) to simultaneously remove depth map artifacts and extract head\npose features. Lian et al. achieve a mean Euclidean error of 38.7mm on their\nown dataset ShanghaiTechGaze+. In this thesis, a model architecture with a\nTransformer module for feature fusion achieves a mean Euclidean error of 55.3mm\non the same dataset, but we show that using no pre-trained GAN module leads to\na mean Euclidean error of 30.1mm. Replacing the Transformer module with a\nMultilayer Perceptron (MLP) improves the error to 26.9mm. These results are\ncoherent with the ones on the other two datasets. On the ETH-XGaze dataset, the\nmodel with Transformer module achieves a mean angular error of 3.59{\\deg} and\nwithout Transformer module 3.26{\\deg}, whereas the fundamentally different\nmodel architecture used by the dataset authors Zhang et al. achieves a mean\nangular error of 2.04{\\deg}. On the OTH-Gaze-Estimation dataset created for...", "AI": {"tldr": "\u672c\u7814\u7a76\u5b9e\u73b0\u4e86\u57fa\u4e8eRGBD\u56fe\u50cf\u548cTransformer\u67b6\u6784\u7684\u6ce8\u89c6\u8ffd\u8e2a\u7cfb\u7edf\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u7ed3\u5408RGBD\u56fe\u50cf\u548cTransformer\u67b6\u6784\u7684\u65b9\u6cd5\u5c1a\u672a\u88ab\u7814\u7a76\uff1b\u73b0\u6709\u6570\u636e\u96c6\u4e0d\u5305\u542b\u6df1\u5ea6\u4fe1\u606f\u6216\u4e0d\u9002\u7528\u4e8e\u6ce8\u89c6\u89d2\u5ea6\u4f30\u8ba1\u4efb\u52a1\u3002", "method": "\u4f7f\u7528Transformer\u67b6\u6784\u878d\u5408\u4eceRGBD\u56fe\u50cf\u4e2d\u63d0\u53d6\u7684\u7279\u5f81\uff0c\u5e76\u4f7f\u7528GAN\u53bb\u9664\u6df1\u5ea6\u56fe\u4f2a\u5f71\u548c\u63d0\u53d6\u5934\u90e8\u59ff\u52bf\u7279\u5f81\u3002", "result": "\u5728ShanghaiTechGaze+\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528Transformer\u6a21\u5757\u7684\u6a21\u578b\u5e73\u5747\u6b27\u51e0\u91cc\u5f97\u8bef\u5dee\u4e3a55.3mm\uff0c\u4e0d\u4f7f\u7528\u9884\u8bad\u7ec3GAN\u6a21\u5757\u8bef\u5dee\u4e3a30.1mm\uff0c\u66ff\u6362\u4e3aMLP\u540e\u8bef\u5dee\u964d\u81f326.9mm\u3002\u5728ETH-XGaze\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528Transformer\u6a21\u5757\u7684\u6a21\u578b\u5e73\u5747\u89d2\u5ea6\u8bef\u5dee\u4e3a3.59\u5ea6\uff0c\u4e0d\u4f7f\u7528Transformer\u6a21\u5757\u4e3a3.26\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u6a21\u578b\u914d\u7f6e\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u8868\u660e\u4f7f\u7528MLP\u66ff\u4ee3Transformer\u6a21\u5757\u53ef\u4ee5\u63d0\u9ad8\u6ce8\u89c6\u8ffd\u8e2a\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2510.06711", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06711", "abs": "https://arxiv.org/abs/2510.06711", "authors": ["Batu El", "Mert Yuksekgonul", "James Zou"], "title": "Inefficiencies of Meta Agents for Agent Design", "comment": null, "summary": "Recent works began to automate the design of agentic systems using\nmeta-agents that propose and iteratively refine new agent architectures. In\nthis paper, we examine three key challenges in a common class of meta-agents.\nFirst, we investigate how a meta-agent learns across iterations and find that\nsimply expanding the context with all previous agents, as proposed by previous\nworks, performs worse than ignoring prior designs entirely. We show that the\nperformance improves with an evolutionary approach. Second, although the\nmeta-agent designs multiple agents during training, it typically commits to a\nsingle agent at test time. We find that the designed agents have low behavioral\ndiversity, limiting the potential for their complementary use. Third, we assess\nwhen automated design is economically viable. We find that only in a few\ncases--specifically, two datasets--the overall cost of designing and deploying\nthe agents is lower than that of human-designed agents when deployed on over\n15,000 examples. In contrast, the performance gains for other datasets do not\njustify the design cost, regardless of scale.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u5143\u4ee3\u7406\u81ea\u52a8\u8bbe\u8ba1\u4ee3\u7406\u7cfb\u7edf\u65f6\u9762\u4e34\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u8de8\u8fed\u4ee3\u5b66\u4e60\u3001\u884c\u4e3a\u591a\u6837\u6027\u4f4e\u4ee5\u53ca\u7ecf\u6d4e\u53ef\u884c\u6027\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u4f7f\u7528\u5143\u4ee3\u7406\u6765\u81ea\u52a8\u5316\u8bbe\u8ba1\u4ee3\u7406\u7cfb\u7edf\uff0c\u4f46\u5b58\u5728\u4e00\u4e9b\u6311\u6218\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\u4e86\u5143\u4ee3\u7406\u5728\u8de8\u8fed\u4ee3\u5b66\u4e60\u3001\u884c\u4e3a\u591a\u6837\u6027\u548c\u7ecf\u6d4e\u53ef\u884c\u6027\u65b9\u9762\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b\u6539\u8fdb\u65b9\u6cd5\uff0c\u4f8b\u5982\u4f7f\u7528\u8fdb\u5316\u65b9\u6cd5\u6765\u63d0\u9ad8\u6027\u80fd\u3002", "result": "\u672c\u6587\u53d1\u73b0\uff0c\u7b80\u5355\u7684\u6269\u5c55\u4e0a\u4e0b\u6587\u8fdb\u884c\u8de8\u8fed\u4ee3\u5b66\u4e60\u7684\u6548\u679c\u4e0d\u5982\u5b8c\u5168\u5ffd\u7565\u5148\u524d\u7684\u8bbe\u8ba1\uff1b\u8bbe\u8ba1\u7684\u4ee3\u7406\u884c\u4e3a\u591a\u6837\u6027\u8f83\u4f4e\uff1b\u53ea\u6709\u5728\u5c11\u6570\u60c5\u51b5\u4e0b\uff0c\u81ea\u52a8\u8bbe\u8ba1\u548c\u90e8\u7f72\u4ee3\u7406\u7684\u603b\u6210\u672c\u4f4e\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u7684\u4ee3\u7406\u3002", "conclusion": "\u672c\u6587\u8bc4\u4f30\u4e86\u81ea\u52a8\u8bbe\u8ba1\u7684\u7ecf\u6d4e\u53ef\u884c\u6027\uff0c\u5e76\u53d1\u73b0\u5176\u4ec5\u5728\u5c11\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\uff0c\u5f3a\u8c03\u4e86\u5728\u81ea\u52a8\u5316\u8bbe\u8ba1\u4ee3\u7406\u7cfb\u7edf\u65f6\u9700\u8981\u8003\u8651\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2510.06397", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06397", "abs": "https://arxiv.org/abs/2510.06397", "authors": ["Ali Baheri"], "title": "Geometry-Aware Backdoor Attacks: Leveraging Curvature in Hyperbolic Embeddings", "comment": null, "summary": "Non-Euclidean foundation models increasingly place representations in curved\nspaces such as hyperbolic geometry. We show that this geometry creates a\nboundary-driven asymmetry that backdoor triggers can exploit. Near the\nboundary, small input changes appear subtle to standard input-space detectors\nbut produce disproportionately large shifts in the model's representation\nspace. Our analysis formalizes this effect and also reveals a limitation for\ndefenses: methods that act by pulling points inward along the radius can\nsuppress such triggers, but only by sacrificing useful model sensitivity in\nthat same direction. Building on these insights, we propose a simple\ngeometry-adaptive trigger and evaluate it across tasks and architectures.\nEmpirically, attack success increases toward the boundary, whereas conventional\ndetectors weaken, mirroring the theoretical trends. Together, these results\nsurface a geometry-specific vulnerability in non-Euclidean models and offer\nanalysis-backed guidance for designing and understanding the limits of\ndefenses.", "AI": {"tldr": "\u975e\u6b27\u51e0\u91cc\u5fb7\u57fa\u7840\u6a21\u578b\u5c06\u8868\u793a\u7f6e\u4e8e\u5f2f\u66f2\u7a7a\u95f4\u4e2d\uff0c\u4f8b\u5982\u53cc\u66f2\u51e0\u4f55\u3002\u8fd9\u79cd\u51e0\u4f55\u7ed3\u6784\u4f1a\u4ea7\u751f\u8fb9\u754c\u9a71\u52a8\u7684\u975e\u5bf9\u79f0\u6027\uff0c\u540e\u95e8\u89e6\u53d1\u5668\u53ef\u4ee5\u5229\u7528\u5b83\u3002", "motivation": "\u8fd9\u79cd\u51e0\u4f55\u7ed3\u6784\u4f1a\u4ea7\u751f\u8fb9\u754c\u9a71\u52a8\u7684\u975e\u5bf9\u79f0\u6027\uff0c\u540e\u95e8\u89e6\u53d1\u5668\u53ef\u4ee5\u5229\u7528\u5b83\u3002\u5728\u8fb9\u754c\u9644\u8fd1\uff0c\u5c0f\u7684\u8f93\u5165\u53d8\u5316\u5bf9\u4e8e\u6807\u51c6\u8f93\u5165\u7a7a\u95f4\u68c0\u6d4b\u5668\u6765\u8bf4\u663e\u5f97\u5f88\u5fae\u5999\uff0c\u4f46\u4f1a\u5728\u6a21\u578b\u7684\u8868\u793a\u7a7a\u95f4\u4e2d\u4ea7\u751f\u4e0d\u6210\u6bd4\u4f8b\u7684\u5927\u91cf\u79fb\u52a8\u3002", "method": "\u5f62\u5f0f\u5316\u4e86\u8fd9\u79cd\u6548\u5e94\uff0c\u5e76\u63ed\u793a\u4e86\u9632\u5fa1\u7684\u5c40\u9650\u6027\uff1a\u901a\u8fc7\u6cbf\u534a\u5f84\u5411\u5185\u62c9\u70b9\u7684\u65b9\u6cd5\u53ef\u4ee5\u6291\u5236\u8fd9\u79cd\u89e6\u53d1\u5668\uff0c\u4f46\u4ee3\u4ef7\u662f\u727a\u7272\u4e86\u540c\u4e00\u65b9\u5411\u4e0a\u6709\u7528\u7684\u6a21\u578b\u7075\u654f\u5ea6\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u51e0\u4f55\u81ea\u9002\u5e94\u89e6\u53d1\u5668\uff0c\u5e76\u5728\u4efb\u52a1\u548c\u67b6\u6784\u4e2d\u5bf9\u5176\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u653b\u51fb\u6210\u529f\u7387\u5411\u8fb9\u754c\u589e\u52a0\uff0c\u800c\u4f20\u7edf\u68c0\u6d4b\u5668\u51cf\u5f31\uff0c\u53cd\u6620\u4e86\u7406\u8bba\u8d8b\u52bf\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\u4e86\u975e\u6b27\u51e0\u91cc\u5fb7\u6a21\u578b\u4e2d\u7279\u5b9a\u4e8e\u51e0\u4f55\u7ed3\u6784\u7684\u6f0f\u6d1e\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u548c\u7406\u89e3\u9632\u5fa1\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u5206\u6790\u652f\u6301\u7684\u6307\u5bfc\u3002"}}
{"id": "2510.06354", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06354", "abs": "https://arxiv.org/abs/2510.06354", "authors": ["Ingroj Shrestha", "Padmini Srinivasan"], "title": "LLM Bias Detection and Mitigation through the Lens of Desired Distributions", "comment": "Accepted to EMNLP 2025", "summary": "Although prior work on bias mitigation has focused on promoting social\nequality and demographic parity, less attention has been given to aligning\nLLM's outputs to desired distributions. For example, we might want to align a\nmodel with real-world distributions to support factual grounding. Thus, we\ndefine bias as deviation from a desired distribution, which may be an equal or\nreal-world distribution, depending on application goals. We propose a weighted\nadaptive loss based fine-tuning method that aligns LLM's gender-profession\noutput distribution with the desired distribution, while preserving language\nmodeling capability. Using 3 profession sets -- male-dominated,\nfemale-dominated, and gender-balanced -- derived from U.S. labor statistics\n(2024), we assess both our adaptive method for reflecting reality and a\nnon-adaptive variant for equality. Across three masked language models, bias is\nobserved under both distributions. We achieve near-complete mitigation under\nequality and 30-75% reduction under real-world settings. Autoregressive LLMs\nshow no bias under equality but notable bias under real-world settings, with\nthe Llama Instruct models (3.2-3B, 3.1-8B) achieving a 50-62% reduction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a0\u6743\u81ea\u9002\u5e94\u635f\u5931\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u65e8\u5728\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u51fa\u5206\u5e03\u4e0e\u671f\u671b\u7684\u5206\u5e03\u5bf9\u9f50\uff0c\u4ece\u800c\u51cf\u8f7b\u504f\u5dee\u3002", "motivation": "\u4ee5\u5f80\u7684\u504f\u5dee\u7f13\u89e3\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u4fc3\u8fdb\u793e\u4f1a\u516c\u5e73\u548c\u4eba\u53e3\u5747\u7b49\uff0c\u8f83\u5c11\u5173\u6ce8\u4f7fLLM\u7684\u8f93\u51fa\u4e0e\u671f\u671b\u7684\u5206\u5e03\u5bf9\u9f50\u3002\u4f8b\u5982\uff0c\u6211\u4eec\u53ef\u80fd\u5e0c\u671b\u4f7f\u6a21\u578b\u4e0e\u73b0\u5b9e\u4e16\u754c\u7684\u5206\u5e03\u5bf9\u9f50\uff0c\u4ee5\u652f\u6301\u4e8b\u5b9e\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a0\u6743\u81ea\u9002\u5e94\u635f\u5931\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7fLLM\u7684\u6027\u522b-\u804c\u4e1a\u8f93\u51fa\u5206\u5e03\u4e0e\u671f\u671b\u7684\u5206\u5e03\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u3002", "result": "\u5728\u4e09\u79cd\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u4e2d\u89c2\u5bdf\u5230\u4e24\u79cd\u5206\u5e03\u4e0b\u7684\u504f\u5dee\u3002\u5728\u5e73\u7b49\u6761\u4ef6\u4e0b\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5b8c\u5168\u7684\u7f13\u89e3\uff0c\u5728\u73b0\u5b9e\u4e16\u754c\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e8630-75%\u7684\u51cf\u5c11\u3002\u81ea\u56de\u5f52LLM\u5728\u5e73\u7b49\u6761\u4ef6\u4e0b\u6ca1\u6709\u8868\u73b0\u51fa\u504f\u5dee\uff0c\u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u660e\u663e\u7684\u504f\u5dee\uff0cLlama Instruct\u6a21\u578b\uff083.2-3B\uff0c3.1-8B\uff09\u5b9e\u73b0\u4e8650-62%\u7684\u51cf\u5c11\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u8c03\u6574LLM\u7684\u8f93\u51fa\u5206\u5e03\uff0c\u53ef\u4ee5\u6709\u6548\u51cf\u8f7b\u504f\u5dee\uff0c\u5e76\u4f7f\u5176\u66f4\u7b26\u5408\u5b9e\u9645\u5e94\u7528\u7684\u9700\u6c42\u3002"}}
{"id": "2510.06299", "categories": ["cs.CV", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.06299", "abs": "https://arxiv.org/abs/2510.06299", "authors": ["Tiago de Conto", "John Armston", "Ralph Dubayah"], "title": "Scalable deep fusion of spaceborne lidar and synthetic aperture radar for global forest structural complexity mapping", "comment": null, "summary": "Forest structural complexity metrics integrate multiple canopy attributes\ninto a single value that reflects habitat quality and ecosystem function.\nSpaceborne lidar from the Global Ecosystem Dynamics Investigation (GEDI) has\nenabled mapping of structural complexity in temperate and tropical forests, but\nits sparse sampling limits continuous high-resolution mapping. We present a\nscalable, deep learning framework fusing GEDI observations with multimodal\nSynthetic Aperture Radar (SAR) datasets to produce global, high-resolution (25\nm) wall-to-wall maps of forest structural complexity. Our adapted\nEfficientNetV2 architecture, trained on over 130 million GEDI footprints,\nachieves high performance (global R2 = 0.82) with fewer than 400,000\nparameters, making it an accessible tool that enables researchers to process\ndatasets at any scale without requiring specialized computing infrastructure.\nThe model produces accurate predictions with calibrated uncertainty estimates\nacross biomes and time periods, preserving fine-scale spatial patterns. It has\nbeen used to generate a global, multi-temporal dataset of forest structural\ncomplexity from 2015 to 2022. Through transfer learning, this framework can be\nextended to predict additional forest structural variables with minimal\ncomputational cost. This approach supports continuous, multi-temporal\nmonitoring of global forest structural dynamics and provides tools for\nbiodiversity conservation and ecosystem management efforts in a changing\nclimate.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528GEDI\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u7ed3\u5408\u591a\u6a21\u6001SAR\u6570\u636e\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u751f\u6210\u4e86\u5168\u7403\u9ad8\u5206\u8fa8\u7387\uff0825\u7c73\uff09\u7684\u68ee\u6797\u7ed3\u6784\u590d\u6742\u5ea6\u5730\u56fe\u3002", "motivation": "\u73b0\u6709\u7684GEDI\u6570\u636e\u867d\u7136\u53ef\u4ee5mapping\u6e29\u5e26\u548c\u70ed\u5e26\u68ee\u6797\u7684\u7ed3\u6784\u590d\u6742\u5ea6\uff0c\u4f46\u662f\u5176\u7a00\u758f\u7684\u91c7\u6837\u9650\u5236\u4e86\u8fde\u7eed\u7684\u9ad8\u5206\u8fa8\u7387mapping\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u6539\u8fdb\u7684EfficientNetV2\u67b6\u6784\uff0c\u4f7f\u7528\u8d85\u8fc71.3\u4ebf\u4e2aGEDI footprints\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u8be5\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\uff08\u5168\u7403R2 = 0.82\uff09\uff0c\u53c2\u6570\u5c11\u4e8e400,000\u4e2a\uff0c\u5e76\u4e14\u53ef\u4ee5\u751f\u62102015\u5e74\u81f32022\u5e74\u7684\u5168\u7403\u591a\u65f6\u76f8\u68ee\u6797\u7ed3\u6784\u590d\u6742\u5ea6\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u652f\u6301\u5bf9\u5168\u7403\u68ee\u6797\u7ed3\u6784\u52a8\u6001\u8fdb\u884c\u8fde\u7eed\u7684\u591a\u65f6\u76f8\u76d1\u6d4b\uff0c\u5e76\u4e3a\u751f\u7269\u591a\u6837\u6027\u4fdd\u62a4\u548c\u6c14\u5019\u53d8\u5316\u4e2d\u7684\u751f\u6001\u7cfb\u7edf\u7ba1\u7406\u5de5\u4f5c\u63d0\u4f9b\u5de5\u5177\u3002"}}
{"id": "2510.06742", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06742", "abs": "https://arxiv.org/abs/2510.06742", "authors": ["Ali Sarabadani", "Kheirolah Rahsepar Fard"], "title": "MultiCNKG: Integrating Cognitive Neuroscience, Gene, and Disease Knowledge Graphs Using Large Language Models", "comment": null, "summary": "The advent of large language models (LLMs) has revolutionized the integration\nof knowledge graphs (KGs) in biomedical and cognitive sciences, overcoming\nlimitations in traditional machine learning methods for capturing intricate\nsemantic links among genes, diseases, and cognitive processes. We introduce\nMultiCNKG, an innovative framework that merges three key knowledge sources: the\nCognitive Neuroscience Knowledge Graph (CNKG) with 2.9K nodes and 4.3K edges\nacross 9 node types and 20 edge types; Gene Ontology (GO) featuring 43K nodes\nand 75K edges in 3 node types and 4 edge types; and Disease Ontology (DO)\ncomprising 11.2K nodes and 8.8K edges with 1 node type and 2 edge types.\nLeveraging LLMs like GPT-4, we conduct entity alignment, semantic similarity\ncomputation, and graph augmentation to create a cohesive KG that interconnects\ngenetic mechanisms, neurological disorders, and cognitive functions. The\nresulting MultiCNKG encompasses 6.9K nodes across 5 types (e.g., Genes,\nDiseases, Cognitive Processes) and 11.3K edges spanning 7 types (e.g., Causes,\nAssociated with, Regulates), facilitating a multi-layered view from molecular\nto behavioral domains. Assessments using metrics such as precision (85.20%),\nrecall (87.30%), coverage (92.18%), graph consistency (82.50%), novelty\ndetection (40.28%), and expert validation (89.50%) affirm its robustness and\ncoherence. Link prediction evaluations with models like TransE (MR: 391, MRR:\n0.411) and RotatE (MR: 263, MRR: 0.395) show competitive performance against\nbenchmarks like FB15k-237 and WN18RR. This KG advances applications in\npersonalized medicine, cognitive disorder diagnostics, and hypothesis\nformulation in cognitive neuroscience.", "AI": {"tldr": "MultiCNKG: A novel knowledge graph integrating cognitive, genetic, and disease data using LLMs.", "motivation": "Traditional methods struggle to capture complex relationships in biomedical and cognitive domains. This paper aims to create a cohesive knowledge graph linking genetic mechanisms, neurological disorders, and cognitive functions.", "method": "The authors merge three knowledge sources (CNKG, GO, DO) using LLMs (GPT-4) for entity alignment, semantic similarity computation, and graph augmentation.", "result": "The resulting MultiCNKG encompasses 6.9K nodes and 11.3K edges, demonstrating high precision, recall, coverage, graph consistency, novelty detection, and expert validation. Link prediction shows competitive performance.", "conclusion": "MultiCNKG advances personalized medicine, cognitive disorder diagnostics, and hypothesis formulation in cognitive neuroscience."}}
{"id": "2510.06401", "categories": ["cs.LG", "cs.IT", "cs.NE", "math.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.06401", "abs": "https://arxiv.org/abs/2510.06401", "authors": ["Ali Hussaini Umar", "Franky Kevin Nando Tezoh", "Jean Barbier", "Santiago Acevedo", "Alessandro Laio"], "title": "The Effect of Label Noise on the Information Content of Neural Representations", "comment": "10 pages, 5 figures", "summary": "In supervised classification tasks, models are trained to predict a label for\neach data point. In real-world datasets, these labels are often noisy due to\nannotation errors. While the impact of label noise on the performance of deep\nlearning models has been widely studied, its effects on the networks' hidden\nrepresentations remain poorly understood. We address this gap by systematically\ncomparing hidden representations using the Information Imbalance, a\ncomputationally efficient proxy of conditional mutual information. Through this\nanalysis, we observe that the information content of the hidden representations\nfollows a double descent as a function of the number of network parameters,\nakin to the behavior of the test error. We further demonstrate that in the\nunderparameterized regime, representations learned with noisy labels are more\ninformative than those learned with clean labels, while in the\noverparameterized regime, these representations are equally informative. Our\nresults indicate that the representations of overparameterized networks are\nrobust to label noise. We also found that the information imbalance between the\npenultimate and pre-softmax layers decreases with cross-entropy loss in the\noverparameterized regime. This offers a new perspective on understanding\ngeneralization in classification tasks. Extending our analysis to\nrepresentations learned from random labels, we show that these perform worse\nthan random features. This indicates that training on random labels drives\nnetworks much beyond lazy learning, as weights adapt to encode labels\ninformation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6807\u7b7e\u566a\u58f0\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9690\u85cf\u5c42\u8868\u793a\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9690\u85cf\u5c42\u7684\u4fe1\u606f\u542b\u91cf\u968f\u7740\u7f51\u7edc\u53c2\u6570\u6570\u91cf\u7684\u53d8\u5316\u5448\u73b0\u53cc\u91cd\u4e0b\u964d\u8d8b\u52bf\u3002\u5728\u9ad8\u53c2\u6570\u5316\u72b6\u6001\u4e0b\uff0c\u7f51\u7edc\u8868\u793a\u5bf9\u6807\u7b7e\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76\u6807\u7b7e\u566a\u58f0\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5176\u5bf9\u7f51\u7edc\u9690\u85cf\u5c42\u8868\u793a\u7684\u5f71\u54cd\u4ecd\u77e5\u4e4b\u751a\u5c11\u3002", "method": "\u901a\u8fc7\u4fe1\u606f\u5931\u8861\uff08\u4e00\u79cd\u6761\u4ef6\u4e92\u4fe1\u606f\u7684\u8ba1\u7b97\u6709\u6548\u4ee3\u7406\uff09\u7cfb\u7edf\u5730\u6bd4\u8f83\u9690\u85cf\u5c42\u8868\u793a\u3002", "result": "\u89c2\u5bdf\u5230\u9690\u85cf\u5c42\u7684\u4fe1\u606f\u542b\u91cf\u968f\u7740\u7f51\u7edc\u53c2\u6570\u6570\u91cf\u7684\u53d8\u5316\u5448\u73b0\u53cc\u91cd\u4e0b\u964d\u8d8b\u52bf\uff1b\u5728\u6b20\u53c2\u6570\u5316\u72b6\u6001\u4e0b\uff0c\u7528\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\u7684\u8868\u793a\u6bd4\u7528\u5e72\u51c0\u6807\u7b7e\u5b66\u4e60\u7684\u8868\u793a\u66f4\u5177\u4fe1\u606f\u6027\uff0c\u800c\u5728\u8fc7\u53c2\u6570\u5316\u72b6\u6001\u4e0b\uff0c\u8fd9\u4e9b\u8868\u793a\u7684\u4fe1\u606f\u91cf\u76f8\u540c\uff1b\u8fc7\u5ea6\u53c2\u6570\u5316\u7f51\u7edc\u7684\u8868\u793a\u5bf9\u6807\u7b7e\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\uff1b\u5012\u6570\u7b2c\u4e8c\u5c42\u548c pre-softmax \u5c42\u4e4b\u95f4\u7684\u4fe1\u606f\u5931\u8861\u968f\u7740\u8fc7\u5ea6\u53c2\u6570\u5316\u72b6\u6001\u4e0b\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u800c\u964d\u4f4e\uff1b\u968f\u673a\u6807\u7b7e\u7684\u8868\u73b0\u6bd4\u968f\u673a\u7279\u5f81\u5dee\u3002", "conclusion": "\u8fc7\u5ea6\u53c2\u6570\u5316\u7f51\u7edc\u7684\u8868\u793a\u5bf9\u6807\u7b7e\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u5012\u6570\u7b2c\u4e8c\u5c42\u548c pre-softmax \u5c42\u4e4b\u95f4\u7684\u4fe1\u606f\u5931\u8861\u968f\u7740\u4ea4\u53c9\u71b5\u635f\u5931\u800c\u964d\u4f4e\u3002\u6b64\u5916\uff0c\u7528\u968f\u673a\u6807\u7b7e\u8bad\u7ec3\u7f51\u7edc\u4f1a\u4f7f\u5176\u8d85\u51fa\u60f0\u6027\u5b66\u4e60\u7684\u8303\u56f4\uff0c\u56e0\u4e3a\u6743\u91cd\u4f1a\u9002\u5e94\u4ee5\u7f16\u7801\u6807\u7b7e\u4fe1\u606f\u3002"}}
{"id": "2510.06370", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06370", "abs": "https://arxiv.org/abs/2510.06370", "authors": ["Kshitish Ghate", "Andy Liu", "Devansh Jain", "Taylor Sorensen", "Atoosa Kasirzadeh", "Aylin Caliskan", "Mona T. Diab", "Maarten Sap"], "title": "EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preference", "comment": "Preprint under review", "summary": "As large language models (LLMs) are deployed globally, creating pluralistic\nsystems that can accommodate the diverse preferences and values of users\nworldwide becomes essential. We introduce EVALUESTEER, a benchmark to measure\nLLMs' and reward models' (RMs) steerability towards users' value and stylistic\npreference profiles grounded in psychology and human-LLM interaction\nliterature. To address the gap in existing datasets that do not support\ncontrolled evaluations of RM steering, we synthetically generated 165,888\npreference pairs -- systematically varying pairs along 4 value dimensions\n(traditional, secular-rational, survival, and self-expression) and 4 style\ndimensions (verbosity, readability, confidence, and warmth). We use EVALUESTEER\nto evaluate whether, given a user profile and a pair of candidate value-laden\nand style-laden responses, LLMs and RMs are able to select the output that\naligns with the user's preferences. We evaluate six open-source and proprietary\nLLMs and RMs under sixteen systematic prompting conditions and six preference\ncomparison scenarios. Notably, our results show that, when given the user's\nfull profile of values and stylistic preferences, the best models achieve <75%\naccuracy at choosing the correct response, in contrast to >99% accuracy when\nonly relevant style and value preferences are provided. EVALUESTEER thus\nhighlights the limitations of current RMs at identifying and adapting to\nrelevant user profile information, and provides a challenging testbed for\ndeveloping RMs that can be steered towards diverse human values and\npreferences.", "AI": {"tldr": "EVALUESTEER\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u5956\u52b1\u6a21\u578b\uff08RMs\uff09\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u80fd\u591f\u6839\u636e\u7528\u6237\u7684\u4ef7\u503c\u89c2\u548c\u98ce\u683c\u504f\u597d\u8fdb\u884c\u8c03\u6574\u7684\u57fa\u51c6\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5168\u7403\u8303\u56f4\u5185\u7684\u90e8\u7f72\uff0c\u521b\u5efa\u80fd\u591f\u9002\u5e94\u5168\u7403\u7528\u6237\u591a\u6837\u5316\u504f\u597d\u548c\u4ef7\u503c\u89c2\u7684\u591a\u5143\u5316\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u6570\u636e\u96c6\u4e0d\u652f\u6301\u5bf9\u5956\u52b1\u6a21\u578b\uff08RM\uff09\u6307\u5bfc\u7684\u53ef\u63a7\u8bc4\u4f30\u3002", "method": "\u8be5\u7814\u7a76\u5408\u6210\u4e86\u5305\u542b165,888\u4e2a\u504f\u597d\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u504f\u597d\u5bf9\u57284\u4e2a\u4ef7\u503c\u89c2\u7ef4\u5ea6\uff08\u4f20\u7edf\u3001\u4e16\u4fd7\u7406\u6027\u3001\u751f\u5b58\u548c\u81ea\u6211\u8868\u8fbe\uff09\u548c4\u4e2a\u98ce\u683c\u7ef4\u5ea6\uff08\u5197\u957f\u3001\u53ef\u8bfb\u6027\u3001\u81ea\u4fe1\u548c\u70ed\u60c5\uff09\u4e0a\u7cfb\u7edf\u5730\u53d8\u5316\u3002\u4f7f\u7528EVALUESTEER\u6765\u8bc4\u4f30\uff0c\u7ed9\u5b9a\u4e00\u4e2a\u7528\u6237\u8d44\u6599\u548c\u4e00\u5bf9\u5019\u9009\u7684\u5e26\u6709\u4ef7\u503c\u89c2\u548c\u98ce\u683c\u7684\u56de\u5e94\uff0cLLM\u548cRM\u662f\u5426\u80fd\u591f\u9009\u62e9\u4e0e\u7528\u6237\u504f\u597d\u76f8\u7b26\u7684\u8f93\u51fa\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u63d0\u4f9b\u5b8c\u6574\u7684\u4ef7\u503c\u89c2\u548c\u98ce\u683c\u504f\u597d\u7528\u6237\u8d44\u6599\u65f6\uff0c\u6700\u4f73\u6a21\u578b\u5728\u9009\u62e9\u6b63\u786e\u56de\u5e94\u65b9\u9762\u7684\u51c6\u786e\u7387\u4f4e\u4e8e75%\uff0c\u800c\u4ec5\u63d0\u4f9b\u76f8\u5173\u7684\u98ce\u683c\u548c\u4ef7\u503c\u89c2\u504f\u597d\u65f6\uff0c\u51c6\u786e\u7387\u9ad8\u4e8e99%\u3002", "conclusion": "EVALUESTEER\u7a81\u51fa\u4e86\u5f53\u524dRM\u5728\u8bc6\u522b\u548c\u9002\u5e94\u76f8\u5173\u7528\u6237\u8d44\u6599\u4fe1\u606f\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u5f00\u53d1\u80fd\u591f\u6839\u636e\u4e0d\u540c\u4eba\u7c7b\u4ef7\u503c\u89c2\u548c\u504f\u597d\u8fdb\u884c\u6307\u5bfc\u7684RM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u8bd5\u9a8c\u5e73\u53f0\u3002"}}
{"id": "2510.06308", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06308", "abs": "https://arxiv.org/abs/2510.06308", "authors": ["Yi Xin", "Qi Qin", "Siqi Luo", "Kaiwen Zhu", "Juncheng Yan", "Yan Tai", "Jiayi Lei", "Yuewen Cao", "Keqi Wang", "Yibin Wang", "Jinbin Bai", "Qian Yu", "Dengyang Jiang", "Yuandong Pu", "Haoxing Chen", "Le Zhuo", "Junjun He", "Gen Luo", "Tianbin Li", "Ming Hu", "Jin Ye", "Shenglong Ye", "Bo Zhang", "Chang Xu", "Wenhai Wang", "Hongsheng Li", "Guangtao Zhai", "Tianfan Xue", "Bin Fu", "Xiaohong Liu", "Yu Qiao", "Yihao Liu"], "title": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "comment": "33 pages, 13 figures, 10 tables", "summary": "We introduce Lumina-DiMOO, an open-source foundational model for seamless\nmulti-modal generation and understanding. Lumina-DiMOO sets itself apart from\nprior unified models by utilizing a fully discrete diffusion modeling to handle\ninputs and outputs across various modalities. This innovative approach allows\nLumina-DiMOO to achieve higher sampling efficiency compared to previous\nautoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a\nbroad spectrum of multi-modal tasks, including text-to-image generation,\nimage-to-image generation (e.g., image editing, subject-driven generation, and\nimage inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves\nstate-of-the-art performance on multiple benchmarks, surpassing existing\nopen-source unified multi-modal models. To foster further advancements in\nmulti-modal and discrete diffusion model research, we release our code and\ncheckpoints to the community. Project Page:\nhttps://synbol.github.io/Lumina-DiMOO.", "AI": {"tldr": "Lumina-DiMOO\u662f\u4e00\u4e2a\u5f00\u6e90\u591a\u6a21\u6001\u751f\u6210\u548c\u7406\u89e3\u7684\u57fa\u7840\u6a21\u578b\u3002", "motivation": "Lumina-DiMOO\u65e8\u5728\u901a\u8fc7\u5229\u7528\u5b8c\u5168\u79bb\u6563\u7684\u6269\u6563\u5efa\u6a21\u6765\u5904\u7406\u8de8\u5404\u79cd\u6a21\u6001\u7684\u8f93\u5165\u548c\u8f93\u51fa\uff0c\u4ece\u800c\u533a\u522b\u4e8e\u5148\u524d\u7684\u7edf\u4e00\u6a21\u578b\u3002", "method": "Lumina-DiMOO\u91c7\u7528\u5b8c\u5168\u79bb\u6563\u7684\u6269\u6563\u5efa\u6a21\u3002", "result": "Lumina-DiMOO\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u7684\u5f00\u6e90\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u3002", "conclusion": "Lumina-DiMOO\u7684\u4ee3\u7801\u548c\u68c0\u67e5\u70b9\u5df2\u53d1\u5e03\uff0c\u4ee5\u4fc3\u8fdb\u591a\u6a21\u6001\u548c\u79bb\u6563\u6269\u6563\u6a21\u578b\u7814\u7a76\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2510.06756", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06756", "abs": "https://arxiv.org/abs/2510.06756", "authors": ["Dennis Gross", "Helge Spieker", "Arnaud Gotlieb"], "title": "Verifying Memoryless Sequential Decision-making of Large Language Models", "comment": null, "summary": "We introduce a tool for rigorous and automated verification of large language\nmodel (LLM)- based policies in memoryless sequential decision-making tasks.\nGiven a Markov decision process (MDP) representing the sequential\ndecision-making task, an LLM policy, and a safety requirement expressed as a\nPCTL formula, our approach incrementally constructs only the reachable portion\nof the MDP guided by the LLM's chosen actions. Each state is encoded as a\nnatural language prompt, the LLM's response is parsed into an action, and\nreachable successor states by the policy are expanded. The resulting formal\nmodel is checked with Storm to determine whether the policy satisfies the\nspecified safety property. In experiments on standard grid world benchmarks, we\nshow that open source LLMs accessed via Ollama can be verified when\ndeterministically seeded, but generally underperform deep reinforcement\nlearning baselines. Our tool natively integrates with Ollama and supports\nPRISM-specified tasks, enabling continuous benchmarking in user-specified\nsequential decision-making tasks and laying a practical foundation for formally\nverifying increasingly capable LLMs.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u4e8e\u9a8c\u8bc1\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7b56\u7565\u7684\u5de5\u5177\uff0c\u8be5\u5de5\u5177\u7528\u4e8e\u9a8c\u8bc1\u65e0\u8bb0\u5fc6\u5e8f\u5217\u51b3\u7b56\u4efb\u52a1\u3002", "motivation": "\u9a8c\u8bc1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5e8f\u5217\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u5b89\u5168\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7LLM\u9009\u62e9\u7684\u52a8\u4f5c\uff0c\u589e\u91cf\u6784\u5efaMDP\u7684\u53ef\u8fbe\u90e8\u5206\uff0c\u5e76\u4f7f\u7528Storm\u68c0\u67e5\u751f\u6210\u7684\u6a21\u578b\u662f\u5426\u6ee1\u8db3\u5b89\u5168\u5c5e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u786e\u5b9a\u6027\u64ad\u79cd\u65f6\uff0c\u901a\u8fc7Ollama\u8bbf\u95ee\u7684\u5f00\u6e90LLM\u53ef\u4ee5\u88ab\u9a8c\u8bc1\uff0c\u4f46\u901a\u5e38\u4e0d\u5982\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u5de5\u5177\u4e0eOllama\u539f\u751f\u96c6\u6210\uff0c\u652f\u6301PRISM\u6307\u5b9a\u7684\u4efb\u52a1\uff0c\u4e3a\u6b63\u5f0f\u9a8c\u8bc1\u65e5\u76ca\u5f3a\u5927\u7684LLM\u5960\u5b9a\u4e86\u5b9e\u8df5\u57fa\u7840\u3002"}}
{"id": "2510.06419", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06419", "abs": "https://arxiv.org/abs/2510.06419", "authors": ["Mert Kayaalp", "Caner Turkmen", "Oleksandr Shchur", "Pedro Mercado", "Abdul Fatir Ansari", "Michael Bohlke-Schneider", "Bernie Wang"], "title": "Test-Time Efficient Pretrained Model Portfolios for Time Series Forecasting", "comment": null, "summary": "Is bigger always better for time series foundation models? With the question\nin mind, we explore an alternative to training a single, large monolithic\nmodel: building a portfolio of smaller, pretrained forecasting models. By\napplying ensembling or model selection over these portfolios, we achieve\ncompetitive performance on large-scale benchmarks using much fewer parameters.\nWe explore strategies for designing such portfolios and find that collections\nof specialist models consistently outperform portfolios of independently\ntrained generalists. Remarkably, we demonstrate that post-training a base model\nis a compute-effective approach for creating sufficiently diverse specialists,\nand provide evidences that ensembling and model selection are more\ncompute-efficient than test-time fine-tuning.", "AI": {"tldr": "\u63a2\u7d22\u5c0f\u578b\u9884\u8bad\u7ec3\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u7ec4\u5408\u7684\u6709\u6548\u6027\uff0c\u4f5c\u4e3a\u5927\u578b\u5355\u4e00\u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u7814\u7a76\u66f4\u5927\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u662f\u5426\u603b\u662f\u66f4\u597d\uff0c\u5e76\u5bfb\u627e\u66f4\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u6784\u5efa\u5c0f\u578b\u9884\u8bad\u7ec3\u9884\u6d4b\u6a21\u578b\u7ec4\u5408\uff0c\u5e76\u5e94\u7528\u96c6\u6210\u6216\u6a21\u578b\u9009\u62e9\u3002", "result": "\u4f7f\u7528\u66f4\u5c11\u7684\u53c2\u6570\uff0c\u5728\u5927\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff1b\u4e13\u5bb6\u6a21\u578b\u96c6\u5408\u4f18\u4e8e\u72ec\u7acb\u8bad\u7ec3\u7684\u901a\u7528\u6a21\u578b\u96c6\u5408\uff1b\u540e\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u662f\u521b\u5efa\u8db3\u591f\u591a\u6837\u5316\u7684\u4e13\u5bb6\u6a21\u578b\u7684\u6709\u6548\u8ba1\u7b97\u65b9\u6cd5\uff1b\u96c6\u6210\u548c\u6a21\u578b\u9009\u62e9\u6bd4\u6d4b\u8bd5\u65f6\u5fae\u8c03\u66f4\u6709\u6548\u3002", "conclusion": "\u5c0f\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u7ec4\u5408\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4e14\u66f4\u6709\u6548\u3002"}}
{"id": "2510.06371", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.06371", "abs": "https://arxiv.org/abs/2510.06371", "authors": ["Firoj Alam", "Ali Ezzat Shahroor", "Md. Arid Hasan", "Zien Sheikh Ali", "Hunzalah Hassan Bhatti", "Mohamed Bayan Kmainasi", "Shammur Absar Chowdhury", "Basel Mousi", "Fahim Dalvi", "Nadir Durrani", "Natasa Milic-Frayling"], "title": "EverydayMMQA: A Multilingual and Multimodal Framework for Culturally Grounded Spoken Visual QA", "comment": "Multimodal Foundation Models, Large Language Models, Native,\n  Multilingual, Language Diversity, Contextual Understanding, Culturally\n  Informed", "summary": "Large-scale multimodal models achieve strong results on tasks like Visual\nQuestion Answering (VQA), but they often fail when queries require culturally\ngrounded, everyday knowledge, particularly in low-resource and underrepresented\nlanguages. To bridge this gap, we introduce Everyday Multimodal and\nMultilingual QA (EverydayMMQA), a framework for creating large-scale,\nculturally-grounded datasets for spoken and visual question answering (SVQA).\nUsing this framework, we developed OASIS, a multimodal dataset integrating\nspeech, images, and text. With over ~0.92M images and 14.8M QA pairs, OASIS\ncontains 3.7M spoken questions, enabling four unique input combinations:\nspeech-only, text-only, speech+image, and text+image. Focused on English and\nArabic varieties, 18 countries, the dataset content is curated to reflect\ndiverse, real-world situations. OASIS tests models on tasks beyond object\nrecognition that involve pragmatic, commonsense, and culturally aware\nreasoning. We benchmarked four closed-source models, three open-source models,\nand one fine-tuned model. EverydayMMQA and OASIS together provide a benchmark\nand training dataset for building multimodal LLMs for a comprehensive set of\neveryday tasks within cultural contexts. The framework and dataset will be made\npublicly available to the community.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3a Everyday Multimodal and Multilingual QA (EverydayMMQA) \u7684\u6846\u67b6\uff0c\u7528\u4e8e\u521b\u5efa\u5927\u89c4\u6a21\u3001\u5177\u6709\u6587\u5316\u80cc\u666f\u7684\u53e3\u8bed\u548c\u89c6\u89c9\u95ee\u7b54 (SVQA) \u6570\u636e\u96c6\u3002", "motivation": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u89c6\u89c9\u95ee\u7b54 (VQA) \u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u4f46\u5f53\u67e5\u8be2\u9700\u8981\u5177\u6709\u6587\u5316\u57fa\u7840\u7684\u65e5\u5e38\u77e5\u8bc6\u65f6\uff0c\u5b83\u4eec\u5e38\u5e38\u4f1a\u5931\u8d25\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u548c\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u8bed\u8a00\u4e2d\u3002", "method": "\u4f7f\u7528\u8be5\u6846\u67b6\uff0c\u6211\u4eec\u5f00\u53d1\u4e86 OASIS\uff0c\u8fd9\u662f\u4e00\u4e2a\u6574\u5408\u8bed\u97f3\u3001\u56fe\u50cf\u548c\u6587\u672c\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002OASIS \u5305\u542b\u8d85\u8fc7\u7ea6 0.92M \u5f20\u56fe\u50cf\u548c 14.8M \u4e2a QA \u5bf9\uff0c\u5305\u542b 3.7M \u4e2a\u53e3\u8bed\u95ee\u9898\uff0c\u652f\u6301\u56db\u79cd\u72ec\u7279\u7684\u8f93\u5165\u7ec4\u5408\uff1a\u4ec5\u8bed\u97f3\u3001\u4ec5\u6587\u672c\u3001\u8bed\u97f3+\u56fe\u50cf\u548c\u6587\u672c+\u56fe\u50cf\u3002", "result": "\u4e13\u6ce8\u4e8e\u82f1\u8bed\u548c\u963f\u62c9\u4f2f\u8bed\u7684\u5404\u79cd\u53d8\u4f53\uff0c\u8986\u76d6 18 \u4e2a\u56fd\u5bb6\uff0c\u6570\u636e\u96c6\u5185\u5bb9\u7ecf\u8fc7\u7cbe\u5fc3\u7b56\u5212\uff0c\u65e8\u5728\u53cd\u6620\u591a\u6837\u5316\u7684\u73b0\u5b9e\u4e16\u754c\u60c5\u5883\u3002OASIS \u5728\u6d89\u53ca\u5b9e\u7528\u3001\u5e38\u8bc6\u548c\u6587\u5316\u610f\u8bc6\u63a8\u7406\u7684\u8d85\u51fa\u5bf9\u8c61\u8bc6\u522b\u7684\u4efb\u52a1\u4e0a\u5bf9\u6a21\u578b\u8fdb\u884c\u6d4b\u8bd5\u3002\u6211\u4eec\u5bf9\u56db\u4e2a\u95ed\u6e90\u6a21\u578b\u3001\u4e09\u4e2a\u5f00\u6e90\u6a21\u578b\u548c\u4e00\u4e2a\u5fae\u8c03\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "EverydayMMQA \u548c OASIS \u5171\u540c\u4e3a\u6784\u5efa\u591a\u6a21\u6001 LLM \u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u51c6\u548c\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6587\u5316\u80cc\u666f\u4e0b\u7684\u4e00\u6574\u5957\u65e5\u5e38\u4efb\u52a1\u3002\u8be5\u6846\u67b6\u548c\u6570\u636e\u96c6\u5c06\u5411\u793e\u533a\u516c\u5f00\u3002"}}
{"id": "2510.06353", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06353", "abs": "https://arxiv.org/abs/2510.06353", "authors": ["Allen Tu", "Kartik Narayan", "Joshua Gleason", "Jennifer Xu", "Matthew Meyn", "Tom Goldstein", "Vishal M. Patel"], "title": "TransFIRA: Transfer Learning for Face Image Recognizability Assessment", "comment": "Project Page: https://transfira.github.io/", "summary": "Face recognition in unconstrained environments such as surveillance, video,\nand web imagery must contend with extreme variation in pose, blur,\nillumination, and occlusion, where conventional visual quality metrics fail to\npredict whether inputs are truly recognizable to the deployed encoder. Existing\nFIQA methods typically rely on visual heuristics, curated annotations, or\ncomputationally intensive generative pipelines, leaving their predictions\ndetached from the encoder's decision geometry. We introduce TransFIRA (Transfer\nLearning for Face Image Recognizability Assessment), a lightweight and\nannotation-free framework that grounds recognizability directly in embedding\nspace. TransFIRA delivers three advances: (i) a definition of recognizability\nvia class-center similarity (CCS) and class-center angular separation (CCAS),\nyielding the first natural, decision-boundary--aligned criterion for filtering\nand weighting; (ii) a recognizability-informed aggregation strategy that\nachieves state-of-the-art verification accuracy on BRIAR and IJB-C while nearly\ndoubling correlation with true recognizability, all without external labels,\nheuristics, or backbone-specific training; and (iii) new extensions beyond\nfaces, including encoder-grounded explainability that reveals how degradations\nand subject-specific factors affect recognizability, and the first\nrecognizability-aware body recognition assessment. Experiments confirm\nstate-of-the-art results on faces, strong performance on body recognition, and\nrobustness under cross-dataset shifts. Together, these contributions establish\nTransFIRA as a unified, geometry-driven framework for recognizability\nassessment -- encoder-specific, accurate, interpretable, and extensible across\nmodalities -- significantly advancing FIQA in accuracy, explainability, and\nscope.", "AI": {"tldr": "TransFIRA: A lightweight, annotation-free framework for face image recognizability assessment grounded in embedding space.", "motivation": "Conventional visual quality metrics fail to predict whether inputs are truly recognizable to the deployed encoder in unconstrained environments. Existing FIQA methods rely on visual heuristics, curated annotations, or computationally intensive generative pipelines, leaving their predictions detached from the encoder's decision geometry.", "method": "Defines recognizability via class-center similarity (CCS) and class-center angular separation (CCAS). Introduces a recognizability-informed aggregation strategy.", "result": "Achieves state-of-the-art verification accuracy on BRIAR and IJB-C while nearly doubling correlation with true recognizability. Demonstrates strong performance on body recognition and robustness under cross-dataset shifts.", "conclusion": "TransFIRA is a unified, geometry-driven framework for recognizability assessment that is encoder-specific, accurate, interpretable, and extensible across modalities."}}
{"id": "2510.06761", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06761", "abs": "https://arxiv.org/abs/2510.06761", "authors": ["Zhi Zhang", "Yan Liu", "Zhejing Hu", "Gong Chen", "Sheng-hua Zhong", "Jiannong Cao"], "title": "Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration", "comment": null, "summary": "Automating the end-to-end scientific research process poses a fundamental\nchallenge: it requires both evolving high-level plans that are novel and sound,\nand executing these plans correctly amidst dynamic and uncertain conditions. To\naddress this bilevel challenge, we propose a novel Double-Loop Multi-Agent\n(DLMA) framework to solve the given research problem automatically. The leader\nloop, composed of professor agents, is responsible for evolving research plans.\nIt employs an evolutionary algorithm through involvement, improvement, and\nintegration meetings to iteratively generate and refine a pool of research\nproposals, exploring the solution space effectively. The follower loop,\ncomposed of doctoral student agents, is responsible for executing the\nbest-evolved plan. It dynamically adjusts the plan during implementation via\npre-hoc and post-hoc meetings, ensuring each step (e.g., drafting, coding) is\nwell-supported by contextual and external observations. Extensive experiments\non benchmarks like ACLAward and Laboratory show that DLMA generates research\npapers that achieve state-of-the-art scores in automated evaluation,\nsignificantly outperforming strong baselines. Ablation studies confirm the\ncritical roles of both loops, with evolution driving novelty and execution\nensuring soundness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u73af\u591a\u667a\u80fd\u4f53\uff08DLMA\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u89e3\u51b3\u79d1\u7814\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u81ea\u52a8\u7aef\u5230\u7aef\u79d1\u7814\u6d41\u7a0b\u4e2d\uff0c\u65e2\u8981\u53d1\u5c55\u65b0\u9896\u53ef\u9760\u7684\u9ad8\u7ea7\u8ba1\u5212\uff0c\u53c8\u8981\u52a8\u6001\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u6b63\u786e\u6267\u884c\u8fd9\u4e9b\u8ba1\u5212\u7684\u53cc\u91cd\u6311\u6218\u3002", "method": "\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u7531\u6559\u6388\u667a\u80fd\u4f53\u7ec4\u6210\u7684\u9886\u5bfc\u73af\uff0c\u8d1f\u8d23\u53d1\u5c55\u7814\u7a76\u8ba1\u5212\uff0c\u91c7\u7528\u8fdb\u5316\u7b97\u6cd5\u8fed\u4ee3\u751f\u6210\u548c\u5b8c\u5584\u7814\u7a76\u65b9\u6848\uff1b\u4ee5\u53ca\u4e00\u4e2a\u7531\u535a\u58eb\u751f\u667a\u80fd\u4f53\u7ec4\u6210\u7684\u8ddf\u968f\u73af\uff0c\u8d1f\u8d23\u6267\u884c\u6700\u4f73\u8ba1\u5212\uff0c\u901a\u8fc7\u4f1a\u524d\u548c\u4f1a\u540e\u4f1a\u8bae\u52a8\u6001\u8c03\u6574\u8ba1\u5212\u3002", "result": "\u5728ACLAward\u548cLaboratory\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDLMA\u751f\u6210\u7684\u8bba\u6587\u5728\u81ea\u52a8\u8bc4\u4f30\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u3002", "conclusion": "\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u4e24\u4e2a\u73af\u7684\u5173\u952e\u4f5c\u7528\uff0c\u8fdb\u5316\u9a71\u52a8\u521b\u65b0\uff0c\u6267\u884c\u786e\u4fdd\u53ef\u9760\u6027\u3002"}}
{"id": "2510.06434", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.06434", "abs": "https://arxiv.org/abs/2510.06434", "authors": ["Eliot Shekhtman", "Yichen Zhou", "Ingvar Ziemann", "Nikolai Matni", "Stephen Tu"], "title": "Nearly Instance-Optimal Parameter Recovery from Many Trajectories via Hellinger Localization", "comment": null, "summary": "Learning from temporally-correlated data is a core facet of modern machine\nlearning. Yet our understanding of sequential learning remains incomplete,\nparticularly in the multi-trajectory setting where data consists of many\nindependent realizations of a time-indexed stochastic process. This important\nregime both reflects modern training pipelines such as for large foundation\nmodels, and offers the potential for learning without the typical mixing\nassumptions made in the single-trajectory case. However, instance-optimal\nbounds are known only for least-squares regression with dependent covariates;\nfor more general models or loss functions, the only broadly applicable\nguarantees result from a reduction to either i.i.d. learning, with effective\nsample size scaling only in the number of trajectories, or an existing\nsingle-trajectory result when each individual trajectory mixes, with effective\nsample size scaling as the full data budget deflated by the mixing-time.\n  In this work, we significantly broaden the scope of instance-optimal rates in\nmulti-trajectory settings via the Hellinger localization framework, a general\napproach for maximum likelihood estimation. Our method proceeds by first\ncontrolling the squared Hellinger distance at the path-measure level via a\nreduction to i.i.d. learning, followed by localization as a quadratic form in\nparameter space weighted by the trajectory Fisher information. This yields\ninstance-optimal bounds that scale with the full data budget under a broad set\nof conditions. We instantiate our framework across four diverse case studies: a\nsimple mixture of Markov chains, dependent linear regression under non-Gaussian\nnoise, generalized linear models with non-monotonic activations, and\nlinear-attention sequence models. In all cases, our bounds nearly match the\ninstance-optimal rates from asymptotic normality, substantially improving over\nstandard reductions.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u591a\u8f68\u8ff9\u8bbe\u7f6e\u4e0b\u7684\u65f6\u5e8f\u6570\u636e\u5b66\u4e60\u95ee\u9898\uff0c\u65e8\u5728\u627e\u5230\u5728\u66f4\u5e7f\u6cdb\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e0e\u5b8c\u6574\u6570\u636e\u9884\u7b97\u76f8\u5339\u914d\u7684\u5b9e\u4f8b\u6700\u4f18\u8fb9\u754c\u3002", "motivation": "\u73b0\u6709\u7684\u5e8f\u5217\u5b66\u4e60\u7406\u89e3\u4e0d\u5b8c\u6574\uff0c\u5c24\u5176\u662f\u5728\u591a\u8f68\u8ff9\u8bbe\u7f6e\u4e0b\u3002\u591a\u8f68\u8ff9\u8bbe\u7f6e\u53cd\u6620\u4e86\u73b0\u4ee3\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5e76\u63d0\u4f9b\u4e86\u5728\u6ca1\u6709\u5178\u578b\u6df7\u5408\u5047\u8bbe\u4e0b\u5b66\u4e60\u7684\u6f5c\u529b\u3002\u4f46\u53ea\u6709\u6700\u5c0f\u4e8c\u4e58\u56de\u5f52\u5177\u6709\u5df2\u77e5\u7684\u5b9e\u4f8b\u6700\u4f18\u8fb9\u754c\uff1b\u5bf9\u4e8e\u66f4\u4e00\u822c\u7684\u6a21\u578b\u6216\u635f\u5931\u51fd\u6570\uff0c\u73b0\u6709\u7684\u4fdd\u8bc1\u8981\u4e48\u964d\u4f4e\u5230\u72ec\u7acb\u540c\u5206\u5e03\u5b66\u4e60\uff0c\u8981\u4e48\u964d\u4f4e\u5230\u73b0\u6709\u7684\u5355\u8f68\u8ff9\u7ed3\u679c\u3002", "method": "\u8be5\u7814\u7a76\u901a\u8fc7 Hellinger \u5c40\u90e8\u5316\u6846\u67b6\uff0c\u4e00\u79cd\u7528\u4e8e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u6765\u6269\u5c55\u591a\u8f68\u8ff9\u8bbe\u7f6e\u4e2d\u5b9e\u4f8b\u6700\u4f18\u901f\u7387\u7684\u8303\u56f4\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u901a\u8fc7\u964d\u4f4e\u5230\u72ec\u7acb\u540c\u5206\u5e03\u5b66\u4e60\u6765\u63a7\u5236\u8def\u5f84\u5ea6\u91cf\u7ea7\u522b\u7684\u5e73\u65b9 Hellinger \u8ddd\u79bb\uff0c\u7136\u540e\u901a\u8fc7\u8f68\u8ff9 Fisher \u4fe1\u606f\u52a0\u6743\u7684\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u4e8c\u6b21\u5f62\u5f0f\u8fdb\u884c\u5c40\u90e8\u5316\u3002", "result": "\u8be5\u7814\u7a76\u5728\u56db\u4e2a\u4e0d\u540c\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\u5b9e\u4f8b\u5316\u4e86\u8be5\u6846\u67b6\uff1a\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u7b80\u5355\u6df7\u5408\uff0c\u975e\u9ad8\u65af\u566a\u58f0\u4e0b\u7684\u76f8\u5173\u7ebf\u6027\u56de\u5f52\uff0c\u5177\u6709\u975e\u5355\u8c03\u6fc0\u6d3b\u7684\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b\u548c\u7ebf\u6027\u6ce8\u610f\u529b\u5e8f\u5217\u6a21\u578b\u3002\u5728\u6240\u6709\u60c5\u51b5\u4e0b\uff0c\u8be5\u7814\u7a76\u7684\u8fb9\u754c\u51e0\u4e4e\u4e0e\u6765\u81ea\u6e10\u8fd1\u6b63\u6001\u6027\u7684\u5b9e\u4f8b\u6700\u4f18\u901f\u7387\u76f8\u5339\u914d\uff0c\u5927\u5927\u4f18\u4e8e\u6807\u51c6\u964d\u4f4e\u3002", "conclusion": "\u8be5\u7814\u7a76\u663e\u8457\u62d3\u5bbd\u4e86\u591a\u8f68\u8ff9\u8bbe\u7f6e\u4e2d\u5b9e\u4f8b\u6700\u4f18\u901f\u7387\u7684\u8303\u56f4\uff0c\u5e76\u5728\u66f4\u5e7f\u6cdb\u7684\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u4e0e\u5b8c\u6574\u6570\u636e\u9884\u7b97\u76f8\u5339\u914d\u7684\u5b9e\u4f8b\u6700\u4f18\u8fb9\u754c\uff0c\u4f18\u4e8e\u6807\u51c6\u65b9\u6cd5\u3002"}}
{"id": "2510.06378", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06378", "abs": "https://arxiv.org/abs/2510.06378", "authors": ["Angie Boggust", "Donghao Ren", "Yannick Assogba", "Dominik Moritz", "Arvind Satyanarayan", "Fred Hohman"], "title": "Semantic Regexes: Auto-Interpreting LLM Features with a Structured Language", "comment": null, "summary": "Automated interpretability aims to translate large language model (LLM)\nfeatures into human understandable descriptions. However, these natural\nlanguage feature descriptions are often vague, inconsistent, and require manual\nrelabeling. In response, we introduce semantic regexes, structured language\ndescriptions of LLM features. By combining primitives that capture linguistic\nand semantic feature patterns with modifiers for contextualization,\ncomposition, and quantification, semantic regexes produce precise and\nexpressive feature descriptions. Across quantitative benchmarks and qualitative\nanalyses, we find that semantic regexes match the accuracy of natural language\nwhile yielding more concise and consistent feature descriptions. Moreover,\ntheir inherent structure affords new types of analyses, including quantifying\nfeature complexity across layers, scaling automated interpretability from\ninsights into individual features to model-wide patterns. Finally, in user\nstudies, we find that semantic regex descriptions help people build accurate\nmental models of LLM feature activations.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684LLM\u7279\u5f81\u63cf\u8ff0\u65b9\u6cd5\uff0c\u79f0\u4e3a\u8bed\u4e49\u6b63\u5219\u8868\u8fbe\u5f0f\uff0c\u65e8\u5728\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u6a21\u7cca\u6027\u548c\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u7136\u8bed\u8a00\u7279\u5f81\u63cf\u8ff0\u901a\u5e38\u6a21\u7cca\u4e14\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u624b\u52a8\u91cd\u65b0\u6807\u8bb0\u3002", "method": "\u7ed3\u5408\u8bed\u8a00\u548c\u8bed\u4e49\u7279\u5f81\u6a21\u5f0f\u7684\u539f\u8bed\uff0c\u4ee5\u53ca\u7528\u4e8e\u4e0a\u4e0b\u6587\u3001\u7ec4\u5408\u548c\u91cf\u5316\u7684\u4fee\u9970\u7b26\uff0c\u751f\u6210\u7cbe\u786e\u4e14\u5177\u6709\u8868\u73b0\u529b\u7684\u7279\u5f81\u63cf\u8ff0\u3002", "result": "\u8bed\u4e49\u6b63\u5219\u8868\u8fbe\u5f0f\u5728\u5b9a\u91cf\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9a\u6027\u5206\u6790\u4e2d\u4e0e\u81ea\u7136\u8bed\u8a00\u7684\u51c6\u786e\u6027\u76f8\u5339\u914d\uff0c\u540c\u65f6\u4ea7\u751f\u66f4\u7b80\u6d01\u548c\u4e00\u81f4\u7684\u7279\u5f81\u63cf\u8ff0\u3002\u6b64\u5916\uff0c\u5176\u56fa\u6709\u7684\u7ed3\u6784\u5141\u8bb8\u65b0\u578b\u5206\u6790\uff0c\u5305\u62ec\u91cf\u5316\u8de8\u5c42\u7279\u5f81\u590d\u6742\u6027\uff0c\u5c06\u81ea\u52a8\u53ef\u89e3\u91ca\u6027\u4ece\u5bf9\u5355\u4e2a\u7279\u5f81\u7684\u6d1e\u5bdf\u6269\u5c55\u5230\u6a21\u578b\u8303\u56f4\u7684\u6a21\u5f0f\u3002", "conclusion": "\u8bed\u4e49\u6b63\u5219\u8868\u8fbe\u5f0f\u63cf\u8ff0\u53ef\u4ee5\u5e2e\u52a9\u4eba\u4eec\u5efa\u7acbLLM\u7279\u5f81\u6fc0\u6d3b\u7684\u51c6\u786e\u5fc3\u7406\u6a21\u578b\u3002"}}
{"id": "2510.06440", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06440", "abs": "https://arxiv.org/abs/2510.06440", "authors": ["Carly Sutter", "Kara J. Sulia", "Nick P. Bassill", "Christopher D. Wirz", "Christopher D. Thorncroft", "Jay C. Rothenberger", "Vanessa Przybylo", "Mariana G. Cains", "Jacob Radford", "David Aaron Evans"], "title": "Road Surface Condition Detection with Machine Learning using New York State Department of Transportation Camera Images and Weather Forecast Data", "comment": null, "summary": "The New York State Department of Transportation (NYSDOT) has a network of\nroadside traffic cameras that are used by both the NYSDOT and the public to\nobserve road conditions. The NYSDOT evaluates road conditions by driving on\nroads and observing live cameras, tasks which are labor-intensive but necessary\nfor making critical operational decisions during winter weather events.\nHowever, machine learning models can provide additional support for the NYSDOT\nby automatically classifying current road conditions across the state. In this\nstudy, convolutional neural networks and random forests are trained on camera\nimages and weather data to predict road surface conditions. Models are trained\non a hand-labeled dataset of ~22,000 camera images, each classified by human\nlabelers into one of six road surface conditions: severe snow, snow, wet, dry,\npoor visibility, or obstructed. Model generalizability is prioritized to meet\nthe operational needs of the NYSDOT decision makers, and the weather-related\nroad surface condition model in this study achieves an accuracy of 81.5% on\ncompletely unseen cameras.", "AI": {"tldr": "\u7ebd\u7ea6\u5dde\u4ea4\u901a\u90e8\u4f7f\u7528\u4ea4\u901a\u6444\u50cf\u5934\u6765\u89c2\u5bdf\u8def\u51b5\u3002\u673a\u5668\u5b66\u4e60\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u81ea\u52a8\u5206\u7c7b\u8def\u51b5\u6765\u4e3a\u4ea4\u901a\u90e8\u63d0\u4f9b\u989d\u5916\u7684\u652f\u6301\u3002", "motivation": "\u7ebd\u7ea6\u5dde\u4ea4\u901a\u90e8\u9700\u8981\u8bc4\u4f30\u8def\u51b5\uff0c\u4ee5\u4fbf\u5728\u51ac\u5b63\u5929\u6c14\u4e8b\u4ef6\u671f\u95f4\u505a\u51fa\u5173\u952e\u7684\u8fd0\u8425\u51b3\u7b56\u3002\u76ee\u524d\u4ed6\u4eec\u901a\u8fc7\u5728\u9053\u8def\u4e0a\u884c\u9a76\u548c\u89c2\u5bdf\u5b9e\u65f6\u6444\u50cf\u5934\u6765\u8bc4\u4f30\u8def\u51b5\uff0c\u4f46\u8fd9\u4e9b\u4efb\u52a1\u975e\u5e38\u8017\u8d39\u4eba\u529b\u3002", "method": "\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u968f\u673a\u68ee\u6797\u5728\u6444\u50cf\u5934\u56fe\u50cf\u548c\u5929\u6c14\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u9884\u6d4b\u8def\u9762\u72b6\u51b5\u3002\u4f7f\u7528\u4e86\u4e00\u4e2a\u4eba\u5de5\u6807\u6ce8\u7684\u5305\u542b\u7ea622,000\u5f20\u6444\u50cf\u5934\u56fe\u50cf\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u6bcf\u5f20\u56fe\u50cf\u88ab\u4eba\u5de5\u6807\u6ce8\u4e3a\u516d\u79cd\u8def\u9762\u72b6\u51b5\u4e4b\u4e00\u3002", "result": "\u8be5\u7814\u7a76\u4e2d\u7684\u4e0e\u5929\u6c14\u76f8\u5173\u7684\u8def\u9762\u72b6\u51b5\u6a21\u578b\u5728\u5b8c\u5168\u672a\u89c1\u8fc7\u7684\u6444\u50cf\u5934\u4e0a\u5b9e\u73b0\u4e8681.5%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u9884\u6d4b\u8def\u9762\u72b6\u51b5\uff0c\u4ece\u800c\u4e3a\u7ebd\u7ea6\u5dde\u4ea4\u901a\u90e8\u63d0\u4f9b\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2510.06857", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06857", "abs": "https://arxiv.org/abs/2510.06857", "authors": ["Qi Guo", "Jianing Wang", "Jianfei Zhang", "Deyang Kong", "Xiangzhou Huang", "Xiangyu Xi", "Wei Wang", "Jingang Wang", "Xunliang Cai", "Shikun Zhang", "Wei Ye"], "title": "Autoformalizer with Tool Feedback", "comment": null, "summary": "Autoformalization addresses the scarcity of data for Automated Theorem\nProving (ATP) by translating mathematical problems from natural language into\nformal statements. Efforts in recent work shift from directly prompting large\nlanguage models to training an end-to-end formalizer model from scratch,\nachieving remarkable advancements. However, existing formalizer still struggles\nto consistently generate valid statements that meet syntactic validity and\nsemantic consistency. To address this issue, we propose the Autoformalizer with\nTool Feedback (ATF), a novel approach that incorporates syntactic and\nconsistency information as tools into the formalization process. By integrating\nLean 4 compilers for syntax corrections and employing a multi-LLMs-as-judge\napproach for consistency validation, the model is able to adaptively refine\ngenerated statements according to the tool feedback, enhancing both syntactic\nvalidity and semantic consistency. The training of ATF involves a cold-start\nphase on synthetic tool-calling data, an expert iteration phase to improve\nformalization capabilities, and Direct Preference Optimization to alleviate\nineffective revisions. Experimental results show that ATF markedly outperforms\na range of baseline formalizer models, with its superior performance further\nvalidated by human evaluations. Subsequent analysis reveals that ATF\ndemonstrates excellent inference scaling properties. Moreover, we open-source\nNumina-ATF, a dataset containing 750K synthetic formal statements to facilitate\nadvancements in autoformalization and ATP research.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAutoformalizer with Tool Feedback (ATF) \u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u8bed\u6cd5\u548c\u4e00\u81f4\u6027\u4fe1\u606f\u4f5c\u4e3a\u5de5\u5177\u6765\u6539\u8fdb\u81ea\u52a8\u5f62\u5f0f\u5316\u8fc7\u7a0b\uff0c\u4ece\u800c\u63d0\u9ad8\u53e5\u6cd5\u6709\u6548\u6027\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u5f62\u5f0f\u5316\u5de5\u5177\u96be\u4ee5\u6301\u7eed\u751f\u6210\u6ee1\u8db3\u53e5\u6cd5\u6709\u6548\u6027\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u6709\u6548\u8bed\u53e5\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86Lean 4\u7f16\u8bd1\u5668\u8fdb\u884c\u8bed\u6cd5\u7ea0\u6b63\uff0c\u5e76\u91c7\u7528\u591aLLM\u4f5c\u4e3a\u5224\u65ad\u5668\u8fdb\u884c\u4e00\u81f4\u6027\u9a8c\u8bc1\uff0c\u6a21\u578b\u80fd\u591f\u6839\u636e\u5de5\u5177\u53cd\u9988\u81ea\u9002\u5e94\u5730\u6539\u8fdb\u751f\u6210\u7684\u8bed\u53e5\u3002\u8bad\u7ec3\u8fc7\u7a0b\u5305\u62ec\u51b7\u542f\u52a8\u9636\u6bb5\u3001\u4e13\u5bb6\u8fed\u4ee3\u9636\u6bb5\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cATF\u660e\u663e\u4f18\u4e8e\u4e00\u7cfb\u5217\u57fa\u7ebf\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u8bc4\u4f30\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u5353\u8d8a\u6027\u80fd\u3002\u540e\u7eed\u5206\u6790\u8868\u660e\uff0cATF\u5c55\u793a\u4e86\u51fa\u8272\u7684\u63a8\u7406\u6269\u5c55\u5c5e\u6027\u3002", "conclusion": "ATF\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u5de5\u5177\u53cd\u9988\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u5f62\u5f0f\u5316\u7684\u6027\u80fd\uff0c\u5e76\u5f00\u6e90\u4e86\u4e00\u4e2a\u5305\u542b750K\u5408\u6210\u5f62\u5f0f\u8bed\u53e5\u7684\u6570\u636e\u96c6Numina-ATF\uff0c\u4ee5\u4fc3\u8fdb\u81ea\u52a8\u5f62\u5f0f\u5316\u548cATP\u7814\u7a76\u7684\u8fdb\u5c55\u3002"}}
{"id": "2510.06439", "categories": ["cs.LG", "cs.CE", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.06439", "abs": "https://arxiv.org/abs/2510.06439", "authors": ["Akash Yadav", "Ruda Zhang"], "title": "Bayesian Optimization under Uncertainty for Training a Scale Parameter in Stochastic Models", "comment": null, "summary": "Hyperparameter tuning is a challenging problem especially when the system\nitself involves uncertainty. Due to noisy function evaluations, optimization\nunder uncertainty can be computationally expensive. In this paper, we present a\nnovel Bayesian optimization framework tailored for hyperparameter tuning under\nuncertainty, with a focus on optimizing a scale- or precision-type parameter in\nstochastic models. The proposed method employs a statistical surrogate for the\nunderlying random variable, enabling analytical evaluation of the expectation\noperator. Moreover, we derive a closed-form expression for the optimizer of the\nrandom acquisition function, which significantly reduces computational cost per\niteration. Compared with a conventional one-dimensional Monte Carlo-based\noptimization scheme, the proposed approach requires 40 times fewer data points,\nresulting in up to a 40-fold reduction in computational cost. We demonstrate\nthe effectiveness of the proposed method through two numerical examples in\ncomputational engineering.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u91cd\u70b9\u662f\u4f18\u5316\u968f\u673a\u6a21\u578b\u4e2d\u7684\u5c3a\u5ea6\u6216\u7cbe\u5ea6\u578b\u53c2\u6570\u3002", "motivation": "\u7531\u4e8e\u566a\u58f0\u51fd\u6570\u8bc4\u4f30\uff0c\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u4f18\u5316\u5728\u8ba1\u7b97\u4e0a\u53ef\u80fd\u5f88\u6602\u8d35\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u5e95\u5c42\u968f\u673a\u53d8\u91cf\u7684\u7edf\u8ba1\u66ff\u4ee3\uff0c\u4ece\u800c\u80fd\u591f\u5bf9\u671f\u671b\u7b97\u5b50\u8fdb\u884c\u5206\u6790\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63a8\u5bfc\u4e86\u968f\u673a\u91c7\u96c6\u51fd\u6570\u4f18\u5316\u5668\u7684\u5c01\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u4ece\u800c\u663e\u7740\u964d\u4f4e\u4e86\u6bcf\u6b21\u8fed\u4ee3\u7684\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u4e0e\u4f20\u7edf\u7684\u57fa\u4e8e\u8499\u7279\u5361\u7f57\u7684\u4e00\u7ef4\u4f18\u5316\u65b9\u6848\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u9700\u8981\u7684\u6570\u636e\u70b9\u51cf\u5c11 40 \u500d\uff0c\u4ece\u800c\u4f7f\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe 40 \u500d\u3002", "conclusion": "\u901a\u8fc7\u8ba1\u7b97\u5de5\u7a0b\u4e2d\u7684\u4e24\u4e2a\u6570\u503c\u4f8b\u5b50\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.06383", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06383", "abs": "https://arxiv.org/abs/2510.06383", "authors": ["Pierre Lison", "Mark Anderson"], "title": "Protecting De-identified Documents from Search-based Linkage Attacks", "comment": null, "summary": "While de-identification models can help conceal the identity of the\nindividual(s) mentioned in a document, they fail to address linkage risks,\ndefined as the potential to map the de-identified text back to its source. One\nstraightforward way to perform such linkages is to extract phrases from the\nde-identified document and then check their presence in the original dataset.\nThis paper presents a method to counter search-based linkage attacks while\npreserving the semantic integrity of the text. The method proceeds in two\nsteps. We first construct an inverted index of the N-grams occurring in the\ndocument collection, making it possible to efficiently determine which N-grams\nappear in less than $k$ documents (either alone or in combination with other\nN-grams). An LLM-based rewriter is then iteratively queried to reformulate\nthose spans until linkage is no longer possible. Experimental results on a\ncollection of court cases show that the method is able to effectively prevent\nsearch-based linkages while remaining faithful to the original content.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9632\u6b62\u57fa\u4e8e\u641c\u7d22\u7684\u94fe\u63a5\u653b\u51fb\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u6587\u672c\u7684\u8bed\u4e49\u5b8c\u6574\u6027\u3002", "motivation": "\u53bb\u6807\u8bc6\u5316\u6a21\u578b\u65e0\u6cd5\u89e3\u51b3\u94fe\u63a5\u98ce\u9669\uff0c\u5373\u5c06\u53bb\u6807\u8bc6\u5316\u7684\u6587\u672c\u6620\u5c04\u56de\u5176\u6765\u6e90\u7684\u98ce\u9669\u3002\u4e00\u79cd\u6267\u884c\u6b64\u7c7b\u94fe\u63a5\u7684\u7b80\u5355\u65b9\u6cd5\u662f\u4ece\u53bb\u6807\u8bc6\u5316\u7684\u6587\u6863\u4e2d\u63d0\u53d6\u77ed\u8bed\uff0c\u7136\u540e\u68c0\u67e5\u5b83\u4eec\u662f\u5426\u5b58\u5728\u4e8e\u539f\u59cb\u6570\u636e\u96c6\u4e2d\u3002", "method": "\u8be5\u65b9\u6cd5\u5206\u4e24\u6b65\u8fdb\u884c\u3002\u9996\u5148\uff0c\u6784\u5efa\u6587\u6863\u96c6\u5408\u4e2d\u51fa\u73b0\u7684 N-gram \u7684\u5012\u6392\u7d22\u5f15\uff0c\u4ece\u800c\u53ef\u4ee5\u6709\u6548\u5730\u786e\u5b9a\u54ea\u4e9b N-gram \u51fa\u73b0\u5728\u5c11\u4e8e k \u4e2a\u6587\u6863\u4e2d\uff08\u5355\u72ec\u6216\u4e0e\u5176\u4ed6 N-gram \u7ec4\u5408\uff09\u3002\u7136\u540e\uff0c\u8fed\u4ee3\u67e5\u8be2\u57fa\u4e8e LLM \u7684\u91cd\u5199\u5668\u4ee5\u91cd\u65b0\u8868\u8fbe\u8fd9\u4e9b\u8de8\u5ea6\uff0c\u76f4\u5230\u4e0d\u518d\u53ef\u80fd\u8fdb\u884c\u94fe\u63a5\u3002", "result": "\u5728\u6cd5\u9662\u6848\u4ef6\u96c6\u5408\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u9632\u6b62\u57fa\u4e8e\u641c\u7d22\u7684\u94fe\u63a5\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u539f\u59cb\u5185\u5bb9\u7684\u5fe0\u5b9e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u9632\u6b62\u57fa\u4e8e\u641c\u7d22\u7684\u94fe\u63a5\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u539f\u59cb\u5185\u5bb9\u7684\u5fe0\u5b9e\u3002"}}
{"id": "2510.06460", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06460", "abs": "https://arxiv.org/abs/2510.06460", "authors": ["Piyush Dashpute", "Niki Nezakati", "Wolfgang Heidrich", "Vishwanath Saragadam"], "title": "TDiff: Thermal Plug-And-Play Prior with Patch-Based Diffusion", "comment": null, "summary": "Thermal images from low-cost cameras often suffer from low resolution, fixed\npattern noise, and other localized degradations. Available datasets for thermal\nimaging are also limited in both size and diversity. To address these\nchallenges, we propose a patch-based diffusion framework (TDiff) that leverages\nthe local nature of these distortions by training on small thermal patches. In\nthis approach, full-resolution images are restored by denoising overlapping\npatches and blending them using smooth spatial windowing. To our knowledge,\nthis is the first patch-based diffusion framework that models a learned prior\nfor thermal image restoration across multiple tasks. Experiments on denoising,\nsuper-resolution, and deblurring demonstrate strong results on both simulated\nand real thermal data, establishing our method as a unified restoration\npipeline.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8epatch\u7684\u6269\u6563\u6846\u67b6(TDiff)\uff0c\u7528\u4e8e\u89e3\u51b3\u4f4e\u6210\u672c\u76f8\u673a\u70ed\u56fe\u50cf\u7684\u5206\u8fa8\u7387\u4f4e\u3001\u56fa\u5b9a\u6a21\u5f0f\u566a\u58f0\u548c\u5176\u4ed6\u5c40\u90e8\u9000\u5316\u95ee\u9898\u3002", "motivation": "\u4f4e\u6210\u672c\u76f8\u673a\u7684\u70ed\u56fe\u50cf\u901a\u5e38\u5206\u8fa8\u7387\u8f83\u4f4e\uff0c\u5b58\u5728\u56fa\u5b9a\u6a21\u5f0f\u566a\u58f0\u548c\u5176\u4ed6\u5c40\u90e8\u9000\u5316\u3002\u53ef\u7528\u7684\u70ed\u6210\u50cf\u6570\u636e\u96c6\u5728\u5927\u5c0f\u548c\u591a\u6837\u6027\u65b9\u9762\u4e5f\u53d7\u5230\u9650\u5236\u3002", "method": "\u901a\u8fc7\u5728\u5c0f\u7684\u70ed\u56fe\u50cfpatch\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5229\u7528\u8fd9\u4e9b\u5931\u771f\u7684\u5c40\u90e8\u6027\u8d28\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8epatch\u7684\u6269\u6563\u6846\u67b6(TDiff)\u3002\u901a\u8fc7\u53bb\u566a\u91cd\u53e0\u7684patch\u5e76\u4f7f\u7528\u5e73\u6ed1\u7684\u7a7a\u95f4\u7a97\u53e3\u6df7\u5408\u5b83\u4eec\u6765\u6062\u590d\u5168\u5206\u8fa8\u7387\u56fe\u50cf\u3002", "result": "\u5728\u53bb\u566a\u3001\u8d85\u5206\u8fa8\u7387\u548c\u53bb\u6a21\u7cca\u7684\u5b9e\u9a8c\u4e2d\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u7684\u70ed\u6570\u636e\u4e0a\u90fd\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u7ed3\u679c\uff0c\u5c06\u8be5\u65b9\u6cd5\u786e\u7acb\u4e3a\u7edf\u4e00\u7684\u6062\u590d\u7ba1\u9053\u3002", "conclusion": "TDiff\u662f\u7b2c\u4e00\u4e2a\u57fa\u4e8epatch\u7684\u6269\u6563\u6846\u67b6\uff0c\u5b83\u4e3a\u8de8\u591a\u4e2a\u4efb\u52a1\u7684\u70ed\u56fe\u50cf\u6062\u590d\u5efa\u6a21\u4e86\u5b66\u4e60\u5148\u9a8c\u3002"}}
{"id": "2510.06878", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06878", "abs": "https://arxiv.org/abs/2510.06878", "authors": ["Daria Ozerova", "Ekaterina Trofimova"], "title": "TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs", "comment": null, "summary": "Iterative refinement has been a promising paradigm to enable large language\nmodels (LLMs) to resolve difficult reasoning and problem-solving tasks. One of\nthe key challenges, however, is how to effectively search through the enormous\nsearch space of possible refinements. Existing methods typically fall back on\npredefined heuristics, which are troubled by the exploration-exploitation\ndilemma and cannot adapt based on past refinement outcomes. We introduce\nTree-Guided Policy Refinement (TGPR), a novel framework that combines GRPO with\na Thompson-Sampling-based tree search. TGPR explores both failed and successful\nrefinement paths actively, with denser training trajectories and more adaptive\npolicies. On HumanEval, MBPP, and APPS benchmarks, our method achieves up to\n+4.2 percentage points absolute improvement in pass@1 (on MBPP) and up to\n+12.51 percentage points absolute improvement in pass@10 (on APPS) compared to\na competitive GRPO baseline. Apart from debugging code, TGPR focuses on a\nprincipled approach to combining learned policies with structured search\nmethods, offering a general framework for enhancing iterative refinement and\nstateful reasoning in LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7ed3\u5408 GRPO \u548c\u57fa\u4e8e Thompson \u91c7\u6837\u7684\u6811\u641c\u7d22\uff0c\u4ee5\u89e3\u51b3 LLM \u4e2d\u8fed\u4ee3\u6539\u8fdb\u7684\u641c\u7d22\u7a7a\u95f4\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u53d7\u56f0\u4e8e\u63a2\u7d22-\u5229\u7528\u7684\u4e24\u96be\u95ee\u9898\uff0c\u5e76\u4e14\u65e0\u6cd5\u6839\u636e\u8fc7\u53bb\u7684\u6539\u8fdb\u7ed3\u679c\u8fdb\u884c\u8c03\u6574\u3002", "method": "\u7ed3\u5408 GRPO \u548c\u57fa\u4e8e Thompson \u91c7\u6837\u7684\u6811\u641c\u7d22\uff0c\u79ef\u6781\u63a2\u7d22\u5931\u8d25\u548c\u6210\u529f\u7684\u6539\u8fdb\u8def\u5f84\uff0c\u5177\u6709\u66f4\u5bc6\u96c6\u7684\u8bad\u7ec3\u8f68\u8ff9\u548c\u66f4\u81ea\u9002\u5e94\u7684\u7b56\u7565\u3002", "result": "\u5728 HumanEval\u3001MBPP \u548c APPS \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cpass@1 \u6700\u9ad8\u63d0\u5347 4.2 \u4e2a\u767e\u5206\u70b9\uff08\u5728 MBPP \u4e0a\uff09\uff0cpass@10 \u6700\u9ad8\u63d0\u5347 12.51 \u4e2a\u767e\u5206\u70b9\uff08\u5728 APPS \u4e0a\uff09\u3002", "conclusion": "TGPR \u4e13\u6ce8\u4e8e\u5c06\u5b66\u4e60\u7b56\u7565\u4e0e\u7ed3\u6784\u5316\u641c\u7d22\u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u4e3a\u589e\u5f3a LLM \u4e2d\u7684\u8fed\u4ee3\u6539\u8fdb\u548c\u72b6\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u6846\u67b6\u3002"}}
{"id": "2510.06444", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.06444", "abs": "https://arxiv.org/abs/2510.06444", "authors": ["Joel Pfeffer", "J. M. Diederik Kruijssen", "Cl\u00e9ment Gossart", "M\u00e9lanie Chevance", "Diego Campo Millan", "Florian Stecker", "Steven N. Longmore"], "title": "Context-Aware Inference via Performance Forecasting in Decentralized Learning Networks", "comment": "17 pages, 12 figures; appeared in ADI (October 2025)", "summary": "In decentralized learning networks, predictions from many participants are\ncombined to generate a network inference. While many studies have demonstrated\nperformance benefits of combining multiple model predictions, existing\nstrategies using linear pooling methods (ranging from simple averaging to\ndynamic weight updates) face a key limitation. Dynamic prediction combinations\nthat rely on historical performance to update weights are necessarily reactive.\nDue to the need to average over a reasonable number of epochs (with moving\naverages or exponential weighting), they tend to be slow to adjust to changing\ncircumstances (phase or regime changes). In this work, we develop a model that\nuses machine learning to forecast the performance of predictions by models at\neach epoch in a time series. This enables `context-awareness' by assigning\nhigher weight to models that are likely to be more accurate at a given time. We\nshow that adding a performance forecasting worker in a decentralized learning\nnetwork, following a design similar to the Allora network, can improve the\naccuracy of network inferences. Specifically, we find forecasting models that\npredict regret (performance relative to the network inference) or regret\nz-score (performance relative to other workers) show greater improvement than\nmodels predicting losses, which often do not outperform the naive network\ninference (historically weighted average of all inferences). Through a series\nof optimization tests, we show that the performance of the forecasting model\ncan be sensitive to choices in the feature set and number of training epochs.\nThese properties may depend on the exact problem and should be tailored to each\ndomain. Although initially designed for a decentralized learning network, using\nperformance forecasting for prediction combination may be useful in any\nsituation where predictive rather than reactive model weighting is needed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u4e2d\u6bcf\u4e2aepoch\u7684\u9884\u6d4b\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3a\u5728\u7ed9\u5b9a\u65f6\u95f4\u53ef\u80fd\u66f4\u51c6\u786e\u7684\u6a21\u578b\u5206\u914d\u66f4\u9ad8\u7684\u6743\u91cd\uff0c\u4ece\u800c\u5b9e\u73b0\u201c\u4e0a\u4e0b\u6587\u611f\u77e5\u201d\uff0c\u5e76\u63d0\u9ad8\u7f51\u7edc\u63a8\u7406\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4f7f\u7528\u7ebf\u6027\u6c60\u5316\u65b9\u6cd5\uff08\u4ece\u7b80\u5355\u5e73\u5747\u5230\u52a8\u6001\u6743\u91cd\u66f4\u65b0\uff09\u7684\u52a8\u6001\u9884\u6d4b\u7ec4\u5408\u4f9d\u8d56\u4e8e\u5386\u53f2\u6027\u80fd\u6765\u66f4\u65b0\u6743\u91cd\uff0c\u56e0\u6b64\u662f\u53cd\u5e94\u5f0f\u7684\uff0c\u5e76\u4e14\u8c03\u6574\u5230\u53d8\u5316\u7684\u73af\u5883\uff08\u9636\u6bb5\u6216\u72b6\u6001\u53d8\u5316\uff09\u7684\u901f\u5ea6\u8f83\u6162\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6765\u9884\u6d4b\u65f6\u95f4\u5e8f\u5217\u4e2d\u6bcf\u4e2aepoch\u7684\u6a21\u578b\u9884\u6d4b\u6027\u80fd\u3002\u901a\u8fc7\u9884\u6d4b\u540e\u6094\u503c\uff08\u76f8\u5bf9\u4e8e\u7f51\u7edc\u63a8\u7406\u7684\u6027\u80fd\uff09\u6216\u540e\u6094\u503cz-score\uff08\u76f8\u5bf9\u4e8e\u5176\u4ed6worker\u7684\u6027\u80fd\uff09\u6765\u63d0\u9ad8\u7f51\u7edc\u63a8\u7406\u7684\u51c6\u786e\u6027\u3002", "result": "\u5728\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u7f51\u7edc\u4e2d\u589e\u52a0\u4e00\u4e2a\u6027\u80fd\u9884\u6d4bworker\uff0c\u53ef\u4ee5\u63d0\u9ad8\u7f51\u7edc\u63a8\u7406\u7684\u51c6\u786e\u6027\u3002\u9884\u6d4b\u540e\u6094\u503c\u6216\u540e\u6094\u503cz-score\u7684\u6a21\u578b\u6bd4\u9884\u6d4b\u635f\u5931\u7684\u6a21\u578b\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u4f7f\u7528\u6027\u80fd\u9884\u6d4b\u8fdb\u884c\u9884\u6d4b\u7ec4\u5408\u5728\u4efb\u4f55\u9700\u8981\u9884\u6d4b\u6027\u800c\u975e\u53cd\u5e94\u6027\u6a21\u578b\u52a0\u6743\u7684\u60c5\u51b5\u4e0b\u90fd\u53ef\u80fd\u6709\u7528\u3002\u6027\u80fd\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u53ef\u80fd\u5bf9\u7279\u5f81\u96c6\u548c\u8bad\u7ec3epoch\u6570\u91cf\u7684\u9009\u62e9\u654f\u611f\uff0c\u8fd9\u4e9b\u5c5e\u6027\u53ef\u80fd\u53d6\u51b3\u4e8e\u5177\u4f53\u95ee\u9898\uff0c\u5e94\u9488\u5bf9\u6bcf\u4e2a\u9886\u57df\u8fdb\u884c\u5b9a\u5236\u3002"}}
{"id": "2510.06386", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06386", "abs": "https://arxiv.org/abs/2510.06386", "authors": ["Fan Zhou", "Chang Tian", "Tim Van de Cruys"], "title": "Controllable Stylistic Text Generation with Train-Time Attribute-Regularized Diffusion", "comment": "Preprint under review", "summary": "Generating stylistic text with specific attributes is a key problem in\ncontrollable text generation. Recently, diffusion models have emerged as a\npowerful paradigm for both visual and textual generation. Existing approaches\ncan be broadly categorized into classifier-free guidance (CFG) and classifier\nguidance (CG) methods. While CFG effectively preserves semantic content, it\noften fails to provide effective attribute control. In contrast, CG modifies\nthe denoising trajectory using classifier gradients, enabling better attribute\nalignment but incurring high computational costs during sampling and suffering\nfrom classifier generalization issues. In this work, we propose RegDiff, a\nregularized diffusion framework that leverages attribute features without\nrequiring a pretrained classifier during sampling, thereby achieving\ncontrollable generation with reduced computational costs. Specifically, RegDiff\nemploys a VAE-based encoder--decoder architecture to ensure reconstruction\nfidelity and a latent diffusion model trained with attribute supervision to\nenable controllable text generation. Attribute information is injected only\nduring training. Experiments on five datasets spanning multiple stylistic\nattributes demonstrate that RegDiff outperforms strong baselines in generating\nstylistic texts. These results validate the effectiveness of RegDiff as an\nefficient solution for attribute-controllable text diffusion. Our code,\ndatasets, and resources will be released upon publication at\nhttps://github.com/xxxx.", "AI": {"tldr": "RegDiff: \u901a\u8fc7\u6b63\u5219\u5316\u6269\u6563\u6846\u67b6\uff0c\u5229\u7528\u5c5e\u6027\u7279\u5f81\u5b9e\u73b0\u53ef\u63a7\u6587\u672c\u751f\u6210\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u5206\u7c7b\u5668\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u98ce\u683c\u6587\u672c\u751f\u6210\u4e2d\uff0c\u8981\u4e48\u5c5e\u6027\u63a7\u5236\u4e0d\u4f73\uff08CFG\uff09\uff0c\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6cdb\u5316\u6027\u5dee\uff08CG\uff09\u3002", "method": "\u63d0\u51fa RegDiff\uff0c\u4e00\u4e2a\u6b63\u5219\u5316\u6269\u6563\u6846\u67b6\uff0c\u4f7f\u7528 VAE \u7f16\u7801\u5668-\u89e3\u7801\u5668\u4fdd\u8bc1\u91cd\u5efa\u4fdd\u771f\u5ea6\uff0c\u5e76\u4f7f\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u5c5e\u6027\u76d1\u7763\u5b9e\u73b0\u53ef\u63a7\u6587\u672c\u751f\u6210\u3002\u5c5e\u6027\u4fe1\u606f\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\u6ce8\u5165\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRegDiff \u5728\u751f\u6210\u98ce\u683c\u6587\u672c\u65b9\u9762\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u3002", "conclusion": "RegDiff \u662f\u4e00\u79cd\u7528\u4e8e\u5c5e\u6027\u53ef\u63a7\u6587\u672c\u6269\u6563\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.06469", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06469", "abs": "https://arxiv.org/abs/2510.06469", "authors": ["Oindrila Saha", "Vojtech Krs", "Radomir Mech", "Subhransu Maji", "Kevin Blackburn-Matzen", "Matheus Gadelha"], "title": "SIGMA-GEN: Structure and Identity Guided Multi-subject Assembly for Image Generation", "comment": "Webpage: https://oindrilasaha.github.io/SIGMA-Gen/", "summary": "We present SIGMA-GEN, a unified framework for multi-identity preserving image\ngeneration. Unlike prior approaches, SIGMA-GEN is the first to enable\nsingle-pass multi-subject identity-preserved generation guided by both\nstructural and spatial constraints. A key strength of our method is its ability\nto support user guidance at various levels of precision -- from coarse 2D or 3D\nboxes to pixel-level segmentations and depth -- with a single model. To enable\nthis, we introduce SIGMA-SET27K, a novel synthetic dataset that provides\nidentity, structure, and spatial information for over 100k unique subjects\nacross 27k images. Through extensive evaluation we demonstrate that SIGMA-GEN\nachieves state-of-the-art performance in identity preservation, image\ngeneration quality, and speed. Code and visualizations at\nhttps://oindrilasaha.github.io/SIGMA-Gen/", "AI": {"tldr": "SIGMA-GEN: A unified framework for multi-identity preserving image generation.", "motivation": "The paper introduces SIGMA-GEN to address the need for single-pass multi-subject identity-preserved generation guided by structural and spatial constraints, which prior approaches couldn't achieve.", "method": "The method uses a novel synthetic dataset called SIGMA-SET27K, which provides identity, structure, and spatial information for over 100k unique subjects across 27k images, to train a model that supports user guidance at various levels of precision.", "result": "SIGMA-GEN achieves state-of-the-art performance in identity preservation, image generation quality, and speed.", "conclusion": "SIGMA-GEN is a novel framework that enables single-pass multi-subject identity-preserved generation with user guidance, achieving state-of-the-art results."}}
{"id": "2510.06911", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06911", "abs": "https://arxiv.org/abs/2510.06911", "authors": ["Hacane Hechehouche", "Andre Antakli", "Matthias Klusch"], "title": "LLM-Assisted Modeling of Semantic Web-Enabled Multi-Agents Systems with AJAN", "comment": null, "summary": "There are many established semantic Web standards for implementing\nmulti-agent driven applications. The AJAN framework allows to engineer\nmulti-agent systems based on these standards. In particular, agent knowledge is\nrepresented in RDF/RDFS and OWL, while agent behavior models are defined with\nBehavior Trees and SPARQL to access and manipulate this knowledge. However, the\nappropriate definition of RDF/RDFS and SPARQL-based agent behaviors still\nremains a major hurdle not only for agent modelers in practice. For example,\ndealing with URIs is very error-prone regarding typos and dealing with complex\nSPARQL queries in large-scale environments requires a high learning curve. In\nthis paper, we present an integrated development environment to overcome such\nhurdles of modeling AJAN agents and at the same time to extend the user\ncommunity for AJAN by the possibility to leverage Large Language Models for\nagent engineering.", "AI": {"tldr": "AJAN\u6846\u67b6\u4f7f\u7528\u591a\u79cd\u8bed\u4e49Web\u6807\u51c6\u6784\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u4f46RDF/RDFS\u548cSPARQL\u7684\u5b9a\u4e49\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "motivation": "\u5b9e\u8df5\u4e2d\uff0c\u57fa\u4e8eRDF/RDFS\u548cSPARQL\u7684\u667a\u80fd\u4f53\u884c\u4e3a\u5efa\u6a21\u5b58\u5728\u56f0\u96be\uff0c\u4f8b\u5982URI\u5bb9\u6613\u51fa\u73b0\u62fc\u5199\u9519\u8bef\uff0c\u4ee5\u53ca\u5728\u5927\u578b\u73af\u5883\u4e2d\u7f16\u5199\u590d\u6742\u7684SPARQL\u67e5\u8be2\u5b66\u4e60\u66f2\u7ebf\u9ad8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u6210\u5f00\u53d1\u73af\u5883\uff0c\u4ee5\u514b\u670dAJAN\u667a\u80fd\u4f53\u5efa\u6a21\u7684\u969c\u788d\u3002", "result": "\u8be5\u96c6\u6210\u5f00\u53d1\u73af\u5883\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u667a\u80fd\u4f53\u5de5\u7a0b\uff0c\u6269\u5c55\u4e86AJAN\u7684\u7528\u6237\u793e\u533a\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u6210\u7684\u5f00\u53d1\u73af\u5883\uff0c\u65e8\u5728\u964d\u4f4eAJAN\u667a\u80fd\u4f53\u5efa\u6a21\u7684\u95e8\u69db\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6269\u5927\u7528\u6237\u7fa4\u4f53\u3002"}}
{"id": "2510.06448", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06448", "abs": "https://arxiv.org/abs/2510.06448", "authors": ["Prabhant Singh", "Sibylle Hess", "Joaquin Vanschoren"], "title": "How NOT to benchmark your SITE metric: Beyond Static Leaderboards and Towards Realistic Evaluation", "comment": null, "summary": "Transferability estimation metrics are used to find a high-performing\npre-trained model for a given target task without fine-tuning models and\nwithout access to the source dataset. Despite the growing interest in\ndeveloping such metrics, the benchmarks used to measure their progress have\ngone largely unexamined. In this work, we empirically show the shortcomings of\nwidely used benchmark setups to evaluate transferability estimation metrics. We\nargue that the benchmarks on which these metrics are evaluated are\nfundamentally flawed. We empirically demonstrate that their unrealistic model\nspaces and static performance hierarchies artificially inflate the perceived\nperformance of existing metrics, to the point where simple, dataset-agnostic\nheuristics can outperform sophisticated methods. Our analysis reveals a\ncritical disconnect between current evaluation protocols and the complexities\nof real-world model selection. To address this, we provide concrete\nrecommendations for constructing more robust and realistic benchmarks to guide\nfuture research in a more meaningful direction.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8fc1\u79fb\u6027\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u5728\u4e0d\u8fdb\u884c\u5fae\u8c03\u548c\u4e0d\u8bbf\u95ee\u6e90\u6570\u636e\u96c6\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u7ed9\u5b9a\u7684\u76ee\u6807\u4efb\u52a1\u627e\u5230\u9ad8\u6027\u80fd\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u7528\u4e8e\u8bc4\u4f30\u8fc1\u79fb\u6027\u8bc4\u4f30\u6307\u6807\u7684\u57fa\u51c6\u5b58\u5728\u7f3a\u9677\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u63ed\u793a\u4e86\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u8bbe\u7f6e\u7684\u7f3a\u70b9\u3002", "result": "\u53d1\u73b0\u8fd9\u4e9b\u57fa\u51c6\u4e2d\u4e0d\u5207\u5b9e\u9645\u7684\u6a21\u578b\u7a7a\u95f4\u548c\u9759\u6001\u6027\u80fd\u5c42\u7ea7\u4f1a\u4eba\u4e3a\u5730\u5938\u5927\u73b0\u6709\u6307\u6807\u7684\u6027\u80fd\uff0c\u751a\u81f3\u7b80\u5355\u7684\u3001\u4e0e\u6570\u636e\u96c6\u65e0\u5173\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u90fd\u80fd\u80dc\u8fc7\u590d\u6742\u7684\u65b9\u6cd5\u3002", "conclusion": "\u76ee\u524d\u7684\u8bc4\u4f30\u534f\u8bae\u4e0e\u771f\u5b9e\u6a21\u578b\u9009\u62e9\u7684\u590d\u6742\u6027\u4e4b\u95f4\u5b58\u5728\u4e25\u91cd\u8131\u8282\u3002\u8bba\u6587\u4e3a\u6784\u5efa\u66f4\u7a33\u5065\u548c\u771f\u5b9e\u7684\u57fa\u51c6\u63d0\u4f9b\u4e86\u5177\u4f53\u5efa\u8bae\uff0c\u4ee5\u6307\u5bfc\u672a\u6765\u7684\u7814\u7a76\u671d\u7740\u66f4\u6709\u610f\u4e49\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2510.06391", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06391", "abs": "https://arxiv.org/abs/2510.06391", "authors": ["Elle"], "title": "Reward Model Perspectives: Whose Opinions Do Reward Models Reward?", "comment": "Published at EMNLP 2025 under the full author name \"Elle\"", "summary": "Reward models (RMs) are central to the alignment of language models (LMs). An\nRM often serves as a proxy for human preferences to guide downstream LM\nbehavior. However, our understanding of RM behavior is limited. Our work (i)\nformalizes a framework for measuring the alignment of opinions captured by RMs,\n(ii) investigates the extent to which RMs demonstrate sociodemographic biases,\nand (iii) explores the effects of prompting to steer rewards towards the\npreferences of a target group. We study the subjective and diverse perspectives\non controversial topics, which allows us to quantify RM perspectives in terms\nof their opinions, attitudes, and values. We show that RMs are poorly aligned\nwith several demographic groups and can systematically reward harmful\nstereotypes, and steering alone is not enough to overcome these limitations.\nOur findings underscore the need for more careful consideration of RM behavior\nin model alignment during preference learning to prevent the propagation of\nunwanted social biases in the language technologies that we use.", "AI": {"tldr": "\u5956\u52b1\u6a21\u578b\uff08RM\uff09\u5bf9\u4e8e\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u7684\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6211\u4eec\u5bf9\u5176\u884c\u4e3a\u7684\u7406\u89e3\u6709\u9650\u3002\u672c\u6587\u7814\u7a76\u4e86RM\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u8868\u73b0\u51fa\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u5dee\uff0c\u4ee5\u53ca\u63d0\u793a\u5bf9\u5c06\u5956\u52b1\u5bfc\u5411\u76ee\u6807\u7fa4\u4f53\u504f\u597d\u7684\u5f71\u54cd\u3002\u7814\u7a76\u8868\u660e\uff0cRM\u4e0e\u67d0\u4e9b\u4eba\u53e3\u7fa4\u4f53\u5bf9\u9f50\u8f83\u5dee\uff0c\u5e76\u4e14\u4f1a\u7cfb\u7edf\u6027\u5730\u5956\u52b1\u6709\u5bb3\u7684\u523b\u677f\u5370\u8c61\uff0c\u5e76\u4e14\u4ec5\u9760\u5f15\u5bfc\u4e0d\u8db3\u4ee5\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "motivation": "\u4e86\u89e3\u5956\u52b1\u6a21\u578b\uff08RM\uff09\u7684\u884c\u4e3a\u5bf9\u4e8e\u9632\u6b62\u8bed\u8a00\u6280\u672f\u4e2d\u4e0d\u5fc5\u8981\u7684\u793e\u4f1a\u504f\u89c1\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u7684\u7406\u89e3\u6709\u9650\uff0c\u9700\u8981\u6df1\u5165\u7814\u7a76\u3002", "method": "1. \u5f62\u5f0f\u5316\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u6d4b\u91cfRM\u6355\u83b7\u7684\u610f\u89c1\u7684\u5bf9\u9f50\u7a0b\u5ea6\u30022. \u8c03\u67e5RM\u8868\u73b0\u51fa\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u5dee\u7684\u7a0b\u5ea6\u30023. \u63a2\u8ba8\u63d0\u793a\u5bf9\u5c06\u5956\u52b1\u5bfc\u5411\u76ee\u6807\u7fa4\u4f53\u504f\u597d\u7684\u5f71\u54cd\u3002\u7814\u7a76\u4e3b\u9898\u662f\u5173\u4e8e\u6709\u4e89\u8bae\u8bdd\u9898\u7684\u4e3b\u89c2\u548c\u591a\u6837\u5316\u89c2\u70b9\u3002", "result": "RM\u4e0e\u67d0\u4e9b\u4eba\u53e3\u7fa4\u4f53\u5bf9\u9f50\u8f83\u5dee\uff0c\u5e76\u4e14\u4f1a\u7cfb\u7edf\u6027\u5730\u5956\u52b1\u6709\u5bb3\u7684\u523b\u677f\u5370\u8c61\u3002\u5355\u72ec\u7684\u5f15\u5bfc\u4e0d\u8db3\u4ee5\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "conclusion": "\u5728\u504f\u597d\u5b66\u4e60\u671f\u95f4\uff0c\u9700\u8981\u66f4\u4ed4\u7ec6\u5730\u8003\u8651\u6a21\u578b\u5bf9\u9f50\u4e2dRM\u7684\u884c\u4e3a\uff0c\u4ee5\u9632\u6b62\u5728\u6211\u4eec\u4f7f\u7528\u7684\u8bed\u8a00\u6280\u672f\u4e2d\u4f20\u64ad\u4e0d\u5fc5\u8981\u7684\u793e\u4f1a\u504f\u89c1\u3002"}}
{"id": "2510.06487", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06487", "abs": "https://arxiv.org/abs/2510.06487", "authors": ["Jack Roberts", "Jeova Farias Sales Rocha Neto"], "title": "Superpixel Integrated Grids for Fast Image Segmentation", "comment": null, "summary": "Superpixels have long been used in image simplification to enable more\nefficient data processing and storage. However, despite their computational\npotential, their irregular spatial distribution has often forced deep learning\napproaches to rely on specialized training algorithms and architectures,\nundermining the original motivation for superpixelations. In this work, we\nintroduce a new superpixel-based data structure, SIGRID (Superpixel-Integrated\nGrid), as an alternative to full-resolution images in segmentation tasks. By\nleveraging classical shape descriptors, SIGRID encodes both color and shape\ninformation of superpixels while substantially reducing input dimensionality.\nWe evaluate SIGRIDs on four benchmark datasets using two popular convolutional\nsegmentation architectures. Our results show that, despite compressing the\noriginal data, SIGRIDs not only match but in some cases surpass the performance\nof pixel-level representations, all while significantly accelerating model\ntraining. This demonstrates that SIGRIDs achieve a favorable balance between\naccuracy and computational efficiency.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u8d85\u50cf\u7d20\u7684\u6570\u636e\u7ed3\u6784\uff0c\u79f0\u4e3a SIGRID\uff0c\u7528\u4e8e\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u7684\u8d85\u50cf\u7d20\u65b9\u6cd5\u7a7a\u95f4\u5206\u5e03\u4e0d\u89c4\u5219\uff0c\u9650\u5236\u4e86\u6df1\u5ea6\u5b66\u4e60\u7684\u5e94\u7528\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bad\u7ec3\u7b97\u6cd5\u548c\u67b6\u6784\u3002", "method": "\u5229\u7528\u7ecf\u5178\u5f62\u72b6\u63cf\u8ff0\u7b26\u5bf9\u8d85\u50cf\u7d20\u7684\u989c\u8272\u548c\u5f62\u72b6\u4fe1\u606f\u8fdb\u884c\u7f16\u7801\uff0c\u4ece\u800c\u964d\u4f4e\u8f93\u5165\u7ef4\u5ea6\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cSIGRID \u5728\u5339\u914d\u751a\u81f3\u8d85\u8fc7\u50cf\u7d20\u7ea7\u8868\u793a\u7684\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u52a0\u901f\u4e86\u6a21\u578b\u8bad\u7ec3\u3002", "conclusion": "SIGRID \u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2510.06953", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06953", "abs": "https://arxiv.org/abs/2510.06953", "authors": ["Minju Gwak", "Guijin Son", "Jaehyung Kim"], "title": "Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces", "comment": null, "summary": "The Uniform Information Density (UID) hypothesis suggests that effective\ncommunication maintains a stable flow of information. In this work, we revisit\nthis principle in the context of large language model (LLM) reasoning traces,\nasking whether step-level uniformity reflects reasoning quality. To this end,\nwe propose an entropy-based stepwise information density metric and introduce\ntwo complementary measures of uniformity, local and global uniformity scores.\nAcross the experiments on six different reasoning benchmarks, we find that\nstep-level uniformity not only provides a strong theoretical lens but also\nyields practical performance benefits; for example, selecting reasoning traces\nwith more uniform information density at the step-level improves accuracy by\n10-32\\% relative gains over baselines at AIME2025. Our analysis further reveals\nthat correct reasoning traces tend to avoid sharp information density spikes,\nwhile incorrect traces exhibit irregular information bursts. These results\ndemonstrate that UID-inspired information density measures outperform\nalternative internal signals as predictors of reasoning quality. Results\nhighlight the uniformity of the information density as a robust diagnostic and\nselection criterion for building more reliable and accurate reasoning systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u4fe1\u606f\u5bc6\u5ea6\u5747\u5300\u6027\uff0c\u5e76\u53d1\u73b0\u5747\u5300\u7684\u4fe1\u606f\u5bc6\u5ea6\u4e0e\u66f4\u597d\u7684\u63a8\u7406\u8d28\u91cf\u76f8\u5173\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u8ba8\u5747\u5300\u4fe1\u606f\u5bc6\u5ea6(UID)\u539f\u5219\u662f\u5426\u9002\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4ee5\u53ca\u6b65\u9aa4\u5c42\u9762\u7684\u5747\u5300\u6027\u662f\u5426\u53cd\u6620\u4e86\u63a8\u7406\u8d28\u91cf\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u71b5\u7684\u9010\u6b65\u4fe1\u606f\u5bc6\u5ea6\u6307\u6807\uff0c\u5e76\u5f15\u5165\u4e86\u5c40\u90e8\u548c\u5168\u5c40\u5747\u5300\u6027\u8bc4\u5206\u4e24\u79cd\u4e92\u8865\u7684\u5747\u5300\u6027\u5ea6\u91cf\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6b65\u9aa4\u5c42\u9762\u7684\u5747\u5300\u6027\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u7406\u8bba\u89c6\u89d2\uff0c\u800c\u4e14\u5e26\u6765\u4e86\u5b9e\u9645\u7684\u6027\u80fd\u63d0\u5347\u3002\u4f8b\u5982\uff0c\u9009\u62e9\u4fe1\u606f\u5bc6\u5ea6\u66f4\u5747\u5300\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u5728AIME2025\u4e0a\u53ef\u4ee5\u63d0\u9ad810-32%\u7684\u51c6\u786e\u7387\u3002\u6b63\u786e\u7684\u63a8\u7406\u8f68\u8ff9\u503e\u5411\u4e8e\u907f\u514d\u6025\u5267\u7684\u4fe1\u606f\u5bc6\u5ea6\u5cf0\u503c\uff0c\u800c\u9519\u8bef\u7684\u8f68\u8ff9\u5219\u8868\u73b0\u51fa\u4e0d\u89c4\u5219\u7684\u4fe1\u606f\u7206\u53d1\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u53d7UID\u542f\u53d1\u7684\u7684\u4fe1\u606f\u5bc6\u5ea6\u5ea6\u91cf\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u5185\u90e8\u4fe1\u53f7\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u63a8\u7406\u8d28\u91cf\u7684\u9884\u6d4b\u6307\u6807\u3002\u4fe1\u606f\u5bc6\u5ea6\u5747\u5300\u6027\u53ef\u4ee5\u4f5c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u548c\u51c6\u786e\u7684\u63a8\u7406\u7cfb\u7edf\u7684\u8bca\u65ad\u548c\u9009\u62e9\u6807\u51c6\u3002"}}
{"id": "2510.06477", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06477", "abs": "https://arxiv.org/abs/2510.06477", "authors": ["Enrique Queipo-de-Llano", "\u00c1lvaro Arroyo", "Federico Barbero", "Xiaowen Dong", "Michael Bronstein", "Yann LeCun", "Ravid Shwartz-Ziv"], "title": "Attention Sinks and Compression Valleys in LLMs are Two Sides of the Same Coin", "comment": null, "summary": "Attention sinks and compression valleys have attracted significant attention\nas two puzzling phenomena in large language models, but have been studied in\nisolation. In this work, we present a surprising connection between attention\nsinks and compression valleys, tracing both to the formation of massive\nactivations in the residual stream. We prove theoretically that massive\nactivations necessarily produce representational compression and establish\nbounds on the resulting entropy reduction. Through experiments across several\nmodels (410M-120B parameters), we confirm that when the beginning-of-sequence\ntoken develops extreme activation norms in the middle layers, both compression\nvalleys and attention sinks emerge simultaneously. Targeted ablation studies\nvalidate our theoretical predictions. This unified view motivates us to propose\nthe Mix-Compress-Refine theory of information flow, as an attempt to explain\nhow LLMs organize their computation in depth by controlling attention and\nrepresentational compression via massive activations. Specifically, we posit\nthat Transformer-based LLMs process tokens in three distinct phases: (1) broad\nmixing in the early layers, (2) compressed computation with limited mixing in\nthe middle layers, and (3) selective refinement in the late layers. Our\nframework helps explain why embedding tasks perform best at intermediate\nlayers, whereas generation tasks benefit from full-depth processing, clarifying\ndifferences in task-dependent representations.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6ce8\u610f\u529b\u9677\u9631\u548c\u538b\u7f29\u8c37\u73b0\u8c61\uff0c\u53d1\u73b0\u5b83\u4eec\u90fd\u4e0e\u6b8b\u5dee\u6d41\u4e2d\u5927\u91cf\u6fc0\u6d3b\u7684\u5f62\u6210\u6709\u5173\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u6ce8\u610f\u529b\u9677\u9631\u548c\u538b\u7f29\u8c37\u8fd9\u4e24\u79cd\u5b64\u7acb\u73b0\u8c61\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u8de8\u591a\u4e2a\u6a21\u578b\uff08410M-120B \u53c2\u6570\uff09\u7684\u5b9e\u9a8c\uff0c\u4ee5\u53ca\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u7406\u8bba\u9884\u6d4b\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5f53\u5e8f\u5217\u8d77\u59cbtoken\u5728\u4e2d\u95f4\u5c42\u4ea7\u751f\u6781\u7aef\u7684\u6fc0\u6d3b\u8303\u6570\u65f6\uff0c\u538b\u7f29\u8c37\u548c\u6ce8\u610f\u529b\u9677\u9631\u540c\u65f6\u51fa\u73b0\u3002\u63d0\u51faMix-Compress-Refine\u4fe1\u606f\u6d41\u7406\u8bba\u3002", "conclusion": "Transformer-based LLM\u4ee5\u4e09\u4e2a\u9636\u6bb5\u5904\u7406tokens\uff1a\u65e9\u671f\u5c42\u7684\u5e7f\u6cdb\u6df7\u5408\uff0c\u4e2d\u95f4\u5c42\u7684\u538b\u7f29\u8ba1\u7b97\uff0c\u4ee5\u53ca\u540e\u671f\u5c42\u7684\u9009\u62e9\u6027\u4f18\u5316\u3002\u8be5\u6846\u67b6\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u5d4c\u5165\u4efb\u52a1\u5728\u4e2d\u95f4\u5c42\u8868\u73b0\u6700\u4f73\uff0c\u800c\u751f\u6210\u4efb\u52a1\u53d7\u76ca\u4e8e\u5168\u6df1\u5ea6\u5904\u7406\u3002"}}
{"id": "2510.06411", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06411", "abs": "https://arxiv.org/abs/2510.06411", "authors": ["R. Alexander Knipper", "Indrani Dey", "Souvika Sarkar", "Hari Narayanan", "Sadhana Puntambekar", "Santu Karmaker"], "title": "Instructional Goal-Aligned Question Generation for Student Evaluation in Virtual Lab Settings: How Closely Do LLMs Actually Align?", "comment": null, "summary": "Virtual Labs offer valuable opportunities for hands-on, inquiry-based science\nlearning, yet teachers often struggle to adapt them to fit their instructional\ngoals. Third-party materials may not align with classroom needs, and developing\ncustom resources can be time-consuming and difficult to scale. Recent advances\nin Large Language Models (LLMs) offer a promising avenue for addressing these\nlimitations. In this paper, we introduce a novel alignment framework for\ninstructional goal-aligned question generation, enabling teachers to leverage\nLLMs to produce simulation-aligned, pedagogically meaningful questions through\nnatural language interaction. The framework integrates four components:\ninstructional goal understanding via teacher-LLM dialogue, lab understanding\nvia knowledge unit and relationship analysis, a question taxonomy for\nstructuring cognitive and pedagogical intent, and the TELeR taxonomy for\ncontrolling prompt detail. Early design choices were informed by a small\nteacher-assisted case study, while our final evaluation analyzed over 1,100\nquestions from 19 open-source LLMs. With goal and lab understanding grounding\nquestions in teacher intent and simulation context, the question taxonomy\nelevates cognitive demand (open-ended formats and relational types raise\nquality by 0.29-0.39 points), and optimized TELeR prompts enhance format\nadherence (80% parsability, >90% adherence). Larger models yield the strongest\ngains: parsability +37.1%, adherence +25.7%, and average quality +0.8 Likert\npoints.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6559\u5b66\u76ee\u6807\u5bf9\u9f50\u95ee\u9898\u751f\u6210\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u751f\u6210\u4e0e\u6a21\u62df\u5bf9\u9f50\u7684\u3001\u5177\u6709\u6559\u5b66\u610f\u4e49\u7684\u95ee\u9898\u3002", "motivation": "\u6559\u5e08\u5f88\u96be\u8c03\u6574\u865a\u62df\u5b9e\u9a8c\u5ba4\u4ee5\u9002\u5e94\u4ed6\u4eec\u7684\u6559\u5b66\u76ee\u6807\u3002\u7b2c\u4e09\u65b9\u6750\u6599\u53ef\u80fd\u4e0e\u8bfe\u5802\u9700\u6c42\u4e0d\u7b26\uff0c\u5e76\u4e14\u5f00\u53d1\u5b9a\u5236\u8d44\u6e90\u53ef\u80fd\u975e\u5e38\u8017\u65f6\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6700\u65b0\u8fdb\u5c55\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u5e0c\u671b\u7684\u9014\u5f84\u3002", "method": "\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u56db\u4e2a\u7ec4\u4ef6\uff1a\u901a\u8fc7\u6559\u5e08-LLM\u5bf9\u8bdd\u8fdb\u884c\u6559\u5b66\u76ee\u6807\u7406\u89e3\uff0c\u901a\u8fc7\u77e5\u8bc6\u5355\u5143\u548c\u5173\u7cfb\u5206\u6790\u8fdb\u884c\u5b9e\u9a8c\u5ba4\u7406\u89e3\uff0c\u7528\u4e8e\u6784\u5efa\u8ba4\u77e5\u548c\u6559\u5b66\u610f\u56fe\u7684\u95ee\u9898\u5206\u7c7b\u6cd5\uff0c\u4ee5\u53ca\u7528\u4e8e\u63a7\u5236\u63d0\u793a\u7ec6\u8282\u7684TELeR\u5206\u7c7b\u6cd5\u3002", "result": "\u76ee\u6807\u548c\u5b9e\u9a8c\u5ba4\u7406\u89e3\u5c06\u95ee\u9898\u5efa\u7acb\u5728\u6559\u5e08\u610f\u56fe\u548c\u6a21\u62df\u80cc\u666f\u4e2d\uff0c\u95ee\u9898\u5206\u7c7b\u6cd5\u63d0\u5347\u4e86\u8ba4\u77e5\u9700\u6c42\uff08\u5f00\u653e\u5f0f\u683c\u5f0f\u548c\u5173\u7cfb\u7c7b\u578b\u5c06\u8d28\u91cf\u63d0\u9ad8\u4e860.29-0.39\u5206\uff09\uff0c\u5e76\u4e14\u4f18\u5316\u7684TELeR\u63d0\u793a\u589e\u5f3a\u4e86\u683c\u5f0f\u4f9d\u4ece\u6027\uff0880%\u7684\u53ef\u89e3\u6790\u6027\uff0c>90%\u7684\u4f9d\u4ece\u6027\uff09\u3002", "conclusion": "\u66f4\u5927\u7684\u6a21\u578b\u4ea7\u751f\u4e86\u6700\u5f3a\u7684\u589e\u76ca\uff1a\u53ef\u89e3\u6790\u6027+37.1%\uff0c\u4f9d\u4ece\u6027+25.7%\uff0c\u5e73\u5747\u8d28\u91cf+0.8 Likert\u70b9\u3002"}}
{"id": "2510.06504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06504", "abs": "https://arxiv.org/abs/2510.06504", "authors": ["Qingxuan Wu", "Zhiyang Dou", "Chuan Guo", "Yiming Huang", "Qiao Feng", "Bing Zhou", "Jian Wang", "Lingjie Liu"], "title": "Text2Interact: High-Fidelity and Diverse Text-to-Two-Person Interaction Generation", "comment": null, "summary": "Modeling human-human interactions from text remains challenging because it\nrequires not only realistic individual dynamics but also precise,\ntext-consistent spatiotemporal coupling between agents. Currently, progress is\nhindered by 1) limited two-person training data, inadequate to capture the\ndiverse intricacies of two-person interactions; and 2) insufficiently\nfine-grained text-to-interaction modeling, where language conditioning\ncollapses rich, structured prompts into a single sentence embedding. To address\nthese limitations, we propose our Text2Interact framework, designed to generate\nrealistic, text-aligned human-human interactions through a scalable\nhigh-fidelity interaction data synthesizer and an effective spatiotemporal\ncoordination pipeline. First, we present InterCompose, a scalable\nsynthesis-by-composition pipeline that aligns LLM-generated interaction\ndescriptions with strong single-person motion priors. Given a prompt and a\nmotion for an agent, InterCompose retrieves candidate single-person motions,\ntrains a conditional reaction generator for another agent, and uses a neural\nmotion evaluator to filter weak or misaligned samples-expanding interaction\ncoverage without extra capture. Second, we propose InterActor, a\ntext-to-interaction model with word-level conditioning that preserves\ntoken-level cues (initiation, response, contact ordering) and an adaptive\ninteraction loss that emphasizes contextually relevant inter-person joint\npairs, improving coupling and physical plausibility for fine-grained\ninteraction modeling. Extensive experiments show consistent gains in motion\ndiversity, fidelity, and generalization, including out-of-distribution\nscenarios and user studies. We will release code and models to facilitate\nreproducibility.", "AI": {"tldr": "\u63d0\u51fa\u4e86Text2Interact\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u903c\u771f\u4e14\u4e0e\u6587\u672c\u5bf9\u9f50\u7684\u4eba\u4e0e\u4eba\u4ea4\u4e92\u3002", "motivation": "\u73b0\u6709\u7684\u4ece\u6587\u672c\u5efa\u6a21\u4eba\u4e0e\u4eba\u4ea4\u4e92\u7684\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5c40\u9650\u6027\uff1a\u53cc\u4eba\u8bad\u7ec3\u6570\u636e\u6709\u9650\uff0c\u4ee5\u53ca\u6587\u672c\u5230\u4ea4\u4e92\u5efa\u6a21\u4e0d\u591f\u7cbe\u7ec6\u3002", "method": "1) InterCompose\uff1a\u53ef\u6269\u5c55\u7684\u5408\u6210\u65b9\u6cd5\uff0c\u5c06LLM\u751f\u6210\u7684\u4ea4\u4e92\u63cf\u8ff0\u4e0e\u5355\u4eba\u8fd0\u52a8\u5148\u9a8c\u5bf9\u9f50\u30022) InterActor\uff1a\u5177\u6709\u8bcd\u7ea7\u6761\u4ef6\u4f5c\u7528\u7684\u6587\u672c\u5230\u4ea4\u4e92\u6a21\u578b\uff0c\u4fdd\u7559token\u7ea7\u522b\u7684\u7ebf\u7d22\uff0c\u5e76\u4f7f\u7528\u81ea\u9002\u5e94\u4ea4\u4e92\u635f\u5931\u3002", "result": "\u5728\u8fd0\u52a8\u591a\u6837\u6027\u3001\u4fdd\u771f\u5ea6\u548c\u6cdb\u5316\u6027\u65b9\u9762\u5747\u8868\u73b0\u51fa\u6301\u7eed\u7684\u63d0\u5347\uff0c\u5305\u62ec\u8d85\u51fa\u5206\u5e03\u7684\u573a\u666f\u548c\u7528\u6237\u7814\u7a76\u3002", "conclusion": "Text2Interact\u6846\u67b6\u6709\u6548\u5730\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u4eba\u4e0e\u4eba\u4ea4\u4e92\u5efa\u6a21\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002"}}
{"id": "2510.07038", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07038", "abs": "https://arxiv.org/abs/2510.07038", "authors": ["Wenxun Wu", "Yuanyang Li", "Guhan Chen", "Linyue Wang", "Hongyang Chen"], "title": "Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning", "comment": null, "summary": "Recent advances in large language models (LLMs) have popularized test-time\nscaling, where models generate additional reasoning tokens before producing\nfinal answers. These approaches have demonstrated significant performance\nimprovements on benchmarks involving mathematical reasoning. However, language\nmodels relying solely on direct inference still struggle with tasks demanding\nup-to-date knowledge or computational tools such as calculators and code\ninterpreters for complex arithmetic operations. To overcome these limitations,\nwe propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement\nlearning framework that systematically integrates multi-hop reasoning with\nadaptive tool-calling capabilities. Our approach employs a modified version of\nDynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm,\nwhich we adapt specifically for tool invocation scenarios, enabling models to\ndynamically interleave complex reasoning with on-demand tool usage (including\nsearch APIs and Python interpreters).\n  To support this research, we introduce two new datasets: TAPO-easy-60K and\nTAPO-hard-18K, specifically designed to train and evaluate both fact-based\nreasoning and mathematical calculation capabilities. Our experiments on\nQwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach,\nwith both models achieving state-of-the-art performance on tasks requiring\nexternal knowledge and mathematical computation among methods with comparable\nparameters. Notably, TAPO achieves more efficient tool utilization than\nbaseline methods while preventing excessive calls caused by reward hacking.\nThese results highlight the significant potential of combining advanced\nreasoning with tool usage to enhance model performance in knowledge-intensive\nand computationally demanding tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTool-Augmented Policy Optimization (TAPO) \u7684\u65b0\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u591a\u8df3\u63a8\u7406\u548c\u81ea\u9002\u5e94\u5de5\u5177\u8c03\u7528\u80fd\u529b\uff0c\u4ee5\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u6700\u65b0\u77e5\u8bc6\u6216\u8ba1\u7b97\u5de5\u5177\u7684\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u9700\u8981\u6700\u65b0\u77e5\u8bc6\u6216\u590d\u6742\u8ba1\u7b97\u5de5\u5177\u7684\u4efb\u52a1\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u672c\u6587\u91c7\u7528\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u52a8\u6001\u91c7\u6837\u7b56\u7565\u4f18\u5316\uff08DAPO\uff09\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u8c03\u6574\u4e3a\u5de5\u5177\u8c03\u7528\u573a\u666f\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u52a8\u6001\u5730\u5c06\u590d\u6742\u63a8\u7406\u4e0e\u6309\u9700\u5de5\u5177\u4f7f\u7528\uff08\u5305\u62ec\u641c\u7d22API\u548cPython\u89e3\u91ca\u5668\uff09\u4ea4\u9519\u8fdb\u884c\u3002\u6b64\u5916\uff0c\u8fd8\u6784\u5efa\u4e86\u4e24\u4e2a\u65b0\u7684\u6570\u636e\u96c6TAPO-easy-60K\u548cTAPO-hard-18K\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTAPO\u65b9\u6cd5\u5728Qwen2.5-3B\u548cQwen2.5-7B\u6a21\u578b\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u5916\u90e8\u77e5\u8bc6\u548c\u6570\u5b66\u8ba1\u7b97\u7684\u4efb\u52a1\u4e2d\uff0c\u5e76\u4e14\u5de5\u5177\u5229\u7528\u6548\u7387\u66f4\u9ad8\uff0c\u540c\u65f6\u9632\u6b62\u4e86\u8fc7\u5ea6\u8c03\u7528\u3002", "conclusion": "TAPO\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u9ad8\u7ea7\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u548c\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u6f5c\u529b\u3002"}}
{"id": "2510.06478", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06478", "abs": "https://arxiv.org/abs/2510.06478", "authors": ["Sanjeda Akter", "Ibne Farabi Shihab", "Anuj Sharma"], "title": "Valid Stopping for LLM Generation via Empirical Dynamic Formal Lift", "comment": null, "summary": "We introduce Sequential-EDFL (Empirical Dynamic Formal Lift), applying\nanytime-valid sequential testing to language model generation stopping. Our\napproach tracks information lift -- the log-likelihood ratio between full\nmodels and deliberately weakened \"skeleton\" baselines -- using self-normalized\nempirical-Bernstein e-processes that provide formal delta-level error control\nregardless of stopping time. We handle unknown centering through online mean\nestimation, combine multiple parameters via mixture e-processes, and support\nadaptive resets under distributional drift. On six benchmarks, Sequential-EDFL\nreduces generation by 22-28% vs. sequential baselines while maintaining\ndelta-level control with 12% computational overhead. We introduce automated\nskeletons (distilled submodels, randomized logits) and show robustness across\nskeleton families. Composing EDFL with a lightweight correctness gate (sentence\nboundaries + verifier) improves end-task correctness while preserving\nanytime-valid guarantees by only delaying stopping. Our certificates control\ninformation sufficiency, not factual correctness -- 10.9% of stopped sequences\nremain incorrect even with the gate (13.2-22.7% without it). EDFL serves as a\nfirst-stage filter reducing verification burden by 83%, not as a standalone\nsolution for safety-critical domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86Sequential-EDFL\uff0c\u4e00\u79cd\u7528\u4e8e\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u968f\u65f6\u6709\u6548\u7684\u5e8f\u5217\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u53ef\u51cf\u5c11\u751f\u6210\u91cf\u5e76\u4fdd\u6301\u8bef\u5dee\u63a7\u5236\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u505c\u6b62\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u5f62\u5f0f\u5316\u7684\u8bef\u5dee\u63a7\u5236\u3002", "method": "\u901a\u8fc7\u81ea\u5f52\u4e00\u5316\u7ecf\u9a8c\u4f2f\u6069\u65af\u5766e-processes\u8ddf\u8e2a\u4fe1\u606f\u589e\u76ca\uff08\u5b8c\u6574\u6a21\u578b\u548c\u5f31\u5316\u9aa8\u67b6\u57fa\u7ebf\u4e4b\u95f4\u7684\u5bf9\u6570\u4f3c\u7136\u6bd4\uff09\uff0c\u5e76\u7ed3\u5408\u5728\u7ebf\u5747\u503c\u4f30\u8ba1\u3001\u6df7\u5408e-processes\u548c\u81ea\u9002\u5e94\u91cd\u7f6e\u7b49\u6280\u672f\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSequential-EDFL\u6bd4\u5e8f\u5217\u57fa\u7ebf\u51cf\u5c11\u4e8622-28%\u7684\u751f\u6210\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86delta\u6c34\u5e73\u7684\u63a7\u5236\uff0c\u8ba1\u7b97\u5f00\u9500\u589e\u52a0\u4e8612%\u3002", "conclusion": "EDFL\u53ef\u4f5c\u4e3a\u7b2c\u4e00\u9636\u6bb5\u8fc7\u6ee4\u5668\uff0c\u51cf\u5c11\u9a8c\u8bc1\u8d1f\u62c5\uff0c\u4f46\u4e0d\u80fd\u4f5c\u4e3a\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u72ec\u7acb\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.06426", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06426", "abs": "https://arxiv.org/abs/2510.06426", "authors": ["Yitao Long", "Tiansheng Hu", "Yilun Zhao", "Arman Cohan", "Chen Zhao"], "title": "FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial Long-Form Question Answering", "comment": "EMNLP 2025 Findings", "summary": "Large Language Models (LLMs) frequently hallucinate to long-form questions,\nproducing plausible yet factually incorrect answers. A common mitigation\nstrategy is to provide attribution to LLM outputs. However, existing benchmarks\nprimarily focus on simple attribution that retrieves supporting textual\nevidence as references. We argue that in real-world scenarios such as financial\napplications, attribution goes beyond reference retrieval. We introduce\nFinLFQA, a benchmark designed to evaluate the ability of LLMs to generate\nlong-form answers to complex financial questions with reliable and nuanced\nattributions. FinLFQA evaluates three critical aspects of attribution through\nhuman annotations: (1) supporting evidence extracted from financial reports,\n(2) intermediate numerical reasoning steps, and (3) domain-specific financial\nknowledge that informs the reasoning process. We further provide an automatic\nevaluation framework covering both answer quality and attribution quality.\nThrough extensive experiments on eight LLMs across multiple\nattribution-generation paradigms, we find that fine-grained metrics are\nimportant to distinguish model capabilities, that end-to-end generation\nachieves comparable performance to post-hoc approaches, and that iterative\nrefinement only helps when guided by external feedback.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684\u91d1\u878d\u957f\u6587\u672c\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5 FinLFQA\uff0c\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u7b54\u6848\u65f6\u63d0\u4f9b\u53ef\u9760\u548c\u7ec6\u81f4\u7684\u5f52\u56e0\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u68c0\u7d22\u652f\u6301\u6587\u672c\u8bc1\u636e\u4f5c\u4e3a\u53c2\u8003\u7684\u7b80\u5355\u5f52\u56e0\uff0c\u4f46\u5728\u5b9e\u9645\u91d1\u878d\u5e94\u7528\u4e2d\uff0c\u5f52\u56e0\u9700\u8981\u66f4\u8fdb\u4e00\u6b65\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u4e2a\u80fd\u591f\u8bc4\u4f30LLM\u5728\u590d\u6742\u91d1\u878d\u95ee\u9898\u4e0a\u751f\u6210\u957f\u6587\u672c\u7b54\u6848\uff0c\u5e76\u8fdb\u884c\u53ef\u9760\u548c\u7ec6\u81f4\u5f52\u56e0\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aFinLFQA\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u6807\u6ce8\u8bc4\u4f30\u5f52\u56e0\u7684\u4e09\u4e2a\u5173\u952e\u65b9\u9762\uff1a\u4ece\u8d22\u52a1\u62a5\u544a\u4e2d\u63d0\u53d6\u7684\u652f\u6301\u8bc1\u636e\u3001\u4e2d\u95f4\u6570\u503c\u63a8\u7406\u6b65\u9aa4\u4ee5\u53ca\u4e3a\u63a8\u7406\u8fc7\u7a0b\u63d0\u4f9b\u4fe1\u606f\u7684\u9886\u57df\u7279\u5b9a\u8d22\u52a1\u77e5\u8bc6\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6\uff0c\u6db5\u76d6\u7b54\u6848\u8d28\u91cf\u548c\u5f52\u56e0\u8d28\u91cf\u3002", "result": "\u901a\u8fc7\u5728\u516b\u4e2aLLM\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\uff0c\u53d1\u73b0\u7ec6\u7c92\u5ea6\u6307\u6807\u5bf9\u4e8e\u533a\u5206\u6a21\u578b\u80fd\u529b\u975e\u5e38\u91cd\u8981\uff0c\u7aef\u5230\u7aef\u751f\u6210\u4e0e\u4e8b\u540e\u65b9\u6cd5\u53d6\u5f97\u4e86\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u8fed\u4ee3\u6539\u8fdb\u53ea\u6709\u5728\u5916\u90e8\u53cd\u9988\u7684\u6307\u5bfc\u4e0b\u624d\u80fd\u6709\u6240\u5e2e\u52a9\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86FinLFQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53ef\u4ee5\u6709\u6548\u8bc4\u4f30LLM\u5728\u91d1\u878d\u9886\u57df\u7684\u957f\u6587\u672c\u95ee\u7b54\u4e2d\u751f\u6210\u53ef\u9760\u548c\u7ec6\u81f4\u5f52\u56e0\u7684\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u57fa\u51c6\u6d4b\u8bd5\u7684\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u548c\u53d1\u73b0\u3002"}}
{"id": "2510.06509", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06509", "abs": "https://arxiv.org/abs/2510.06509", "authors": ["Shih-Yao Lin", "Sibendu Paul", "Caren Chen"], "title": "From Captions to Keyframes: Efficient Video Summarization via Caption- and Context-Aware Frame Scoring", "comment": "10 pages, 4 figures", "summary": "Efficient video-language understanding requires selecting a small set of\nframes that retain semantic and contextual information from long videos. We\npropose KeyScore, a multimodal frame scoring framework that jointly leverages\ncaptions and visual context to estimate frame-level importance. By combining\nsemantic similarity, temporal diversity, and contextual drop impact, KeyScore\nidentifies the most informative frames for downstream tasks such as retrieval,\ncaptioning, and video-language reasoning. To complement KeyScore, we introduce\nSTACFP (Spatio-Temporal Adaptive Clustering for Frame Proposals), which\ngenerates compact and diverse frame candidates for long-form videos. Together,\nthese modules achieve up to 99\\% frame reduction compared to full-frame\ninference and substantially outperform standard 8-frame encoders on MSRVTT,\nMSVD, and DiDeMo. Our results demonstrate that emphasizing multimodal alignment\nbetween visual and textual signals enables scalable, efficient, and\ncaption-grounded video understanding -- without explicit video summarization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6846\u67b6 KeyScore\uff0c\u7528\u4e8e\u8bc4\u4f30\u5e27\u7ea7\u522b\u7684\u91cd\u8981\u6027\uff0c\u5e76\u7ed3\u5408 STACFP \u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u89c6\u9891\u7406\u89e3\u3002", "motivation": "\u4e3a\u4e86\u9ad8\u6548\u5730\u7406\u89e3\u89c6\u9891\u8bed\u8a00\uff0c\u9700\u8981\u9009\u62e9\u4e00\u5c0f\u7ec4\u80fd\u4fdd\u7559\u957f\u89c6\u9891\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u5e27\u3002", "method": "KeyScore \u7ed3\u5408\u4e86\u8bed\u4e49\u76f8\u4f3c\u6027\u3001\u65f6\u95f4\u591a\u6837\u6027\u548c\u4e0a\u4e0b\u6587\u4e22\u5f03\u5f71\u54cd\u6765\u8bc6\u522b\u4fe1\u606f\u91cf\u6700\u5927\u7684\u5e27\u3002STACFP \u7528\u4e8e\u751f\u6210\u7d27\u51d1\u4e14\u591a\u6837\u5316\u7684\u5e27\u5019\u9009\u3002", "result": "\u4e0e\u5b8c\u6574\u5e27\u63a8\u7406\u76f8\u6bd4\uff0c\u5e27\u51cf\u5c11\u7387\u9ad8\u8fbe 99%\uff0c\u5e76\u4e14\u5728 MSRVTT\u3001MSVD \u548c DiDeMo \u4e0a\u5927\u5e45\u4f18\u4e8e\u6807\u51c6 8 \u5e27\u7f16\u7801\u5668\u3002", "conclusion": "\u5f3a\u8c03\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u53f7\u4e4b\u95f4\u7684\u591a\u6a21\u6001\u5bf9\u9f50\u80fd\u591f\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u4e14\u57fa\u4e8e\u5b57\u5e55\u7684\u89c6\u9891\u7406\u89e3\uff0c\u800c\u65e0\u9700\u663e\u5f0f\u7684\u89c6\u9891\u6458\u8981\u3002"}}
{"id": "2510.07064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07064", "abs": "https://arxiv.org/abs/2510.07064", "authors": ["Manh Hung Nguyen", "Sebastian Tschiatschek", "Adish Singla"], "title": "Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations", "comment": null, "summary": "The difficulty and expense of obtaining large-scale human responses make\nLarge Language Models (LLMs) an attractive alternative and a promising proxy\nfor human behavior. However, prior work shows that LLMs often produce\nhomogeneous outputs that fail to capture the rich diversity of human\nperspectives and behaviors. Thus, rather than trying to capture this diversity\nwith a single LLM agent, we propose a novel framework to construct a set of\nagents that collectively capture the diversity of a given human population.\nEach agent is an LLM whose behavior is steered by conditioning on a small set\nof human demonstrations (task-response pairs) through in-context learning. The\ncentral challenge is therefore to select a representative set of LLM agents\nfrom the exponentially large space of possible agents. We tackle this selection\nproblem from the lens of submodular optimization. In particular, we develop\nmethods that offer different trade-offs regarding time complexity and\nperformance guarantees. Extensive experiments in crowdsourcing and educational\ndomains demonstrate that our approach constructs agents that more effectively\nrepresent human populations compared to baselines. Moreover, behavioral\nanalyses on new tasks show that these agents reproduce the behavior patterns\nand perspectives of the students and annotators they are designed to represent.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u6784\u5efa\u4e00\u7ec4\u80fd\u591f\u96c6\u4f53\u6355\u6349\u4eba\u7c7b\u7fa4\u4f53\u591a\u6837\u6027\u7684LLM Agent\u3002", "motivation": "\u73b0\u6709LLM\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u884c\u4e3a\u7684\u591a\u6837\u6027\u3002", "method": "\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u5229\u7528\u5c11\u91cf\u4eba\u7c7b\u6f14\u793a\u6765\u6307\u5bfcLLM Agent\u7684\u884c\u4e3a\uff0c\u5e76\u4f7f\u7528submodular\u4f18\u5316\u65b9\u6cd5\u9009\u62e9LLM Agent\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6784\u5efa\u7684Agent\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u4ee3\u8868\u4eba\u7c7b\u7fa4\u4f53\u3002", "conclusion": "\u8be5Agent\u80fd\u591f\u91cd\u73b0\u5b66\u751f\u548c\u6ce8\u91ca\u8005\u7684\u884c\u4e3a\u6a21\u5f0f\u548c\u89c2\u70b9\u3002"}}
{"id": "2510.06502", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06502", "abs": "https://arxiv.org/abs/2510.06502", "authors": ["Khoa Trinh", "Gaurav Menghani", "Erik Vee"], "title": "GUIDE: Guided Initialization and Distillation of Embeddings", "comment": null, "summary": "Algorithmic efficiency techniques such as distillation\n(\\cite{hinton2015distillation}) are useful in improving model quality without\nincreasing serving costs, provided a larger teacher model is available for a\nsmaller student model to learn from during training. Standard distillation\nmethods are limited to only forcing the student to match the teacher's outputs.\nGiven the costs associated with training a large model, we believe we should be\nextracting more useful information from a teacher model than by just making the\nstudent match the teacher's outputs.\n  In this paper, we introduce \\guide (Guided Initialization and Distillation of\nEmbeddings). \\guide can be considered a distillation technique that forces the\nstudent to match the teacher in the parameter space. Using \\guide we show\n25-26\\% reduction in the teacher-student quality gap when using large student\nmodels (400M - 1B parameters) trained on $\\approx$ 20B tokens. We also present\na thorough analysis demonstrating that \\guide can be combined with knowledge\ndistillation with near additive improvements. Furthermore, we show that\napplying \\guide alone leads to substantially better model quality than applying\nknowledge distillation by itself.\n  Most importantly, \\guide introduces no training or inference overhead and\nhence any model quality gains from our method are virtually free.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a GUIDE \u7684\u65b0\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u8be5\u6280\u672f\u901a\u8fc7\u5728\u53c2\u6570\u7a7a\u95f4\u4e2d\u5339\u914d teacher \u548c student \u6a21\u578b\uff0c\u4ece\u800c\u7f29\u5c0f teacher-student \u4e4b\u95f4\u7684\u8d28\u91cf\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4ec5\u9650\u4e8e\u8ba9 student \u6a21\u578b\u5339\u914d teacher \u6a21\u578b\u7684\u8f93\u51fa\uff0c\u800c\u5ffd\u7565\u4e86 teacher \u6a21\u578b\u4e2d\u66f4\u591a\u6709\u7528\u7684\u4fe1\u606f\u3002\u8003\u8651\u5230\u8bad\u7ec3\u5927\u578b\u6a21\u578b\u7684\u6210\u672c\uff0c\u5e94\u8be5\u4ece teacher \u6a21\u578b\u4e2d\u63d0\u53d6\u66f4\u591a\u6709\u7528\u7684\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a GUIDE \u7684\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u8be5\u6280\u672f\u901a\u8fc7\u5728\u53c2\u6570\u7a7a\u95f4\u4e2d\u5f3a\u5236 student \u6a21\u578b\u5339\u914d teacher \u6a21\u578b\u3002", "result": "\u4f7f\u7528 GUIDE\uff0c\u5f53\u4f7f\u7528\u5728 \u2248 20B tokens \u4e0a\u8bad\u7ec3\u7684\u5927\u578b student \u6a21\u578b\uff08400M - 1B \u53c2\u6570\uff09\u65f6\uff0cteacher-student \u4e4b\u95f4\u7684\u8d28\u91cf\u5dee\u8ddd\u7f29\u5c0f\u4e86 25-26%\u3002GUIDE \u53ef\u4ee5\u4e0e\u77e5\u8bc6\u84b8\u998f\u76f8\u7ed3\u5408\uff0c\u4ece\u800c\u83b7\u5f97\u8fd1\u4f3c\u7684\u9644\u52a0\u6539\u8fdb\u3002\u5355\u72ec\u5e94\u7528 GUIDE \u53ef\u4ee5\u83b7\u5f97\u6bd4\u5355\u72ec\u5e94\u7528\u77e5\u8bc6\u84b8\u998f\u66f4\u597d\u7684\u6a21\u578b\u8d28\u91cf\u3002GUIDE \u4e0d\u5f15\u5165\u8bad\u7ec3\u6216\u63a8\u7406\u5f00\u9500\u3002", "conclusion": "GUIDE \u662f\u4e00\u79cd\u6709\u6548\u7684\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u8d28\u91cf\uff0c\u4e14\u4e0d\u5f15\u5165\u989d\u5916\u7684\u8bad\u7ec3\u6216\u63a8\u7406\u5f00\u9500\u3002"}}
{"id": "2510.06427", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06427", "abs": "https://arxiv.org/abs/2510.06427", "authors": ["Elena Chistova"], "title": "Bridging Discourse Treebanks with a Unified Rhetorical Structure Parser", "comment": "Accepted to CODI CRAC 2025", "summary": "We introduce UniRST, the first unified RST-style discourse parser capable of\nhandling 18 treebanks in 11 languages without modifying their relation\ninventories. To overcome inventory incompatibilities, we propose and evaluate\ntwo training strategies: Multi-Head, which assigns separate relation\nclassification layer per inventory, and Masked-Union, which enables shared\nparameter training through selective label masking. We first benchmark\nmonotreebank parsing with a simple yet effective augmentation technique for\nlow-resource settings. We then train a unified model and show that (1) the\nparameter efficient Masked-Union approach is also the strongest, and (2) UniRST\noutperforms 16 of 18 mono-treebank baselines, demonstrating the advantages of a\nsingle-model, multilingual end-to-end discourse parsing across diverse\nresources.", "AI": {"tldr": "UniRST is a unified RST discourse parser for multiple treebanks and languages.", "motivation": "To create a single discourse parser that can handle multiple treebanks and languages without modifying relation inventories.", "method": "Two training strategies: Multi-Head and Masked-Union.", "result": "Masked-Union is the strongest approach. UniRST outperforms 16 of 18 mono-treebank baselines.", "conclusion": "UniRST demonstrates the advantages of a single-model, multilingual end-to-end discourse parsing across diverse resources."}}
{"id": "2510.06512", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06512", "abs": "https://arxiv.org/abs/2510.06512", "authors": ["Avishree Khare", "Hideki Okamoto", "Bardh Hoxha", "Georgios Fainekos", "Rajeev Alur"], "title": "LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval", "comment": null, "summary": "Neural models such as YOLO and HuBERT can be used to detect local properties\nsuch as objects (\"car\") and emotions (\"angry\") in individual frames of videos\nand audio clips respectively. The likelihood of these detections is indicated\nby scores in [0, 1]. Lifting these scores to temporal properties over sequences\ncan be useful for several downstream applications such as query matching (e.g.,\n\"does the speaker eventually sound happy in this audio clip?\"), and ranked\nretrieval (e.g., \"retrieve top 5 videos with a 10 second scene where a car is\ndetected until a pedestrian is detected\"). In this work, we formalize this\nproblem of assigning Scores for TempOral Properties (STOPs) over sequences,\ngiven potentially noisy score predictors for local properties. We then propose\na scoring function called LogSTOP that can efficiently compute these scores for\ntemporal properties represented in Linear Temporal Logic. Empirically, LogSTOP,\nwith YOLO and HuBERT, outperforms Large Vision / Audio Language Models and\nother Temporal Logic-based baselines by at least 16% on query matching with\ntemporal properties over objects-in-videos and emotions-in-speech respectively.\nSimilarly, on ranked retrieval with temporal properties over objects and\nactions in videos, LogSTOP with Grounding DINO and SlowR50 reports at least a\n19% and 16% increase in mean average precision and recall over zero-shot\ntext-to-video retrieval baselines respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LogSTOP\uff0c\u4e00\u79cd\u7528\u4e8e\u8ba1\u7b97\u5e8f\u5217\u65f6\u95f4\u5c5e\u6027\u5f97\u5206\u7684\u8bc4\u5206\u51fd\u6570\uff0c\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "motivation": "\u4ece\u89c6\u9891\u548c\u97f3\u9891\u7247\u6bb5\u7684\u5355\u5e27\u4e2d\u68c0\u6d4b\u5c40\u90e8\u5c5e\u6027\uff08\u5982\u7269\u4f53\u548c\u60c5\u7eea\uff09\u662f\u5f88\u6709\u7528\u7684\u3002\u5c06\u8fd9\u4e9b\u68c0\u6d4b\u7ed3\u679c\u63d0\u5347\u5230\u5e8f\u5217\u7684\u65f6\u95f4\u5c5e\u6027\uff0c\u53ef\u7528\u4e8e\u67e5\u8be2\u5339\u914d\u548c\u6392\u5e8f\u68c0\u7d22\u7b49\u4e0b\u6e38\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLogSTOP\u7684\u8bc4\u5206\u51fd\u6570\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u8ba1\u7b97\u4ee5\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u8868\u793a\u7684\u65f6\u95f4\u5c5e\u6027\u7684\u5f97\u5206\u3002", "result": "LogSTOP\u5728\u4f7f\u7528YOLO\u548cHuBERT\u65f6\uff0c\u5728\u5bf9\u8c61\u89c6\u9891\u548c\u8bed\u97f3\u60c5\u611f\u7684\u65f6\u95f4\u5c5e\u6027\u67e5\u8be2\u5339\u914d\u65b9\u9762\uff0c\u6bd4\u5927\u578b\u89c6\u89c9/\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u548c\u5176\u4ed6\u57fa\u4e8e\u65f6\u5e8f\u903b\u8f91\u7684\u57fa\u7ebf\u81f3\u5c11\u9ad8\u51fa16%\u3002\u5728\u4f7f\u7528Grounding DINO\u548cSlowR50\u65f6\uff0cLogSTOP\u5728\u89c6\u9891\u4e2d\u5bf9\u8c61\u548c\u52a8\u4f5c\u7684\u65f6\u95f4\u5c5e\u6027\u6392\u5e8f\u68c0\u7d22\u65b9\u9762\uff0c\u5e73\u5747\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u5206\u522b\u6bd4zero-shot\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u57fa\u7ebf\u81f3\u5c11\u9ad8\u51fa19%\u548c16%\u3002", "conclusion": "LogSTOP\u5728\u65f6\u95f4\u5c5e\u6027\u7684\u67e5\u8be2\u5339\u914d\u548c\u6392\u5e8f\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.07069", "categories": ["cs.AI", "I.2.4"], "pdf": "https://arxiv.org/pdf/2510.07069", "abs": "https://arxiv.org/abs/2510.07069", "authors": ["Hongbo Hu", "Yisong Wang", "Yi Huang", "Kewen Wang"], "title": "Inductive Learning for Possibilistic Logic Programs Under Stable Models", "comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "summary": "Possibilistic logic programs (poss-programs) under stable models are a major\nvariant of answer set programming (ASP). While its semantics (possibilistic\nstable models) and properties have been well investigated, the problem of\ninductive reasoning has not been investigated yet. This paper presents an\napproach to extracting poss-programs from a background program and examples\n(parts of intended possibilistic stable models). To this end, the notion of\ninduction tasks is first formally defined, its properties are investigated and\ntwo algorithms ilpsm and ilpsmmin for computing induction solutions are\npresented. An implementation of ilpsmmin is also provided and experimental\nresults show that when inputs are ordinary logic programs, the prototype\noutperforms a major inductive learning system for normal logic programs from\nstable models on the datasets that are randomly generated.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7a33\u5b9a\u6a21\u578b\u4e0b\u53ef\u80fd\u6027\u903b\u8f91\u7a0b\u5e8f (poss-programs) \u7684\u5f52\u7eb3\u63a8\u7406\u95ee\u9898\uff0c\u8fd9\u662f\u7b54\u6848\u96c6\u7f16\u7a0b (ASP) \u7684\u4e00\u4e2a\u4e3b\u8981\u53d8\u4f53\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u53ef\u80fd\u6027\u7a33\u5b9a\u6a21\u578b\u7684\u8bed\u4e49\u548c\u6027\u8d28\u4e0a\uff0c\u800c\u5bf9\u5f52\u7eb3\u63a8\u7406\u95ee\u9898\u5c1a\u672a\u8fdb\u884c\u7814\u7a76\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u80cc\u666f\u7a0b\u5e8f\u548c\u4f8b\u5b50\u4e2d\u63d0\u53d6 poss-programs \u7684\u65b9\u6cd5\uff0c\u5e76\u5b9a\u4e49\u4e86\u5f52\u7eb3\u4efb\u52a1\u7684\u6982\u5ff5\uff0c\u7814\u7a76\u4e86\u5176\u6027\u8d28\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u8ba1\u7b97\u5f52\u7eb3\u89e3\u7684\u7b97\u6cd5 ilpsm \u548c ilpsmmin\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u8f93\u5165\u4e3a\u666e\u901a\u903b\u8f91\u7a0b\u5e8f\u65f6\uff0c\u8be5\u539f\u578b\u5728\u968f\u673a\u751f\u6210\u7684\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4e00\u4e2a\u4e3b\u8981\u7684\u7528\u4e8e\u4ece\u7a33\u5b9a\u6a21\u578b\u4e2d\u5b66\u4e60\u6b63\u89c4\u903b\u8f91\u7a0b\u5e8f\u7684\u5f52\u7eb3\u5b66\u4e60\u7cfb\u7edf\u3002", "conclusion": "\u672c\u6587\u4e3a\u53ef\u80fd\u6027\u903b\u8f91\u7a0b\u5e8f\u7684\u5f52\u7eb3\u63a8\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2510.06503", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06503", "abs": "https://arxiv.org/abs/2510.06503", "authors": ["I-Hsi Kao", "Kanji Uchino"], "title": "ATLO-ML: Adaptive Time-Length Optimizer for Machine Learning -- Insights from Air Quality Forecasting", "comment": null, "summary": "Accurate time-series predictions in machine learning are heavily influenced\nby the selection of appropriate input time length and sampling rate. This paper\nintroduces ATLO-ML, an adaptive time-length optimization system that\nautomatically determines the optimal input time length and sampling rate based\non user-defined output time length. The system provides a flexible approach to\ntime-series data pre-processing, dynamically adjusting these parameters to\nenhance predictive performance. ATLO-ML is validated using air quality\ndatasets, including both GAMS-dataset and proprietary data collected from a\ndata center, both in time series format. Results demonstrate that utilizing the\noptimized time length and sampling rate significantly improves the accuracy of\nmachine learning models compared to fixed time lengths. ATLO-ML shows potential\nfor generalization across various time-sensitive applications, offering a\nrobust solution for optimizing temporal input parameters in machine learning\nworkflows.", "AI": {"tldr": "ATLO-ML: adaptive time-length optimization system that automatically determines the optimal input time length and sampling rate based on user-defined output time length.", "motivation": "Accurate time-series predictions in machine learning are heavily influenced by the selection of appropriate input time length and sampling rate.", "method": "ATLO-ML is validated using air quality datasets, including both GAMS-dataset and proprietary data collected from a data center, both in time series format.", "result": "Utilizing the optimized time length and sampling rate significantly improves the accuracy of machine learning models compared to fixed time lengths.", "conclusion": "ATLO-ML shows potential for generalization across various time-sensitive applications, offering a robust solution for optimizing temporal input parameters in machine learning workflows."}}
{"id": "2510.06430", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06430", "abs": "https://arxiv.org/abs/2510.06430", "authors": ["Neeraja Kirtane", "Yuvraj Khanna", "Peter Relan"], "title": "MathRobust-LV: Evaluation of Large Language Models' Robustness to Linguistic Variations in Mathematical Reasoning", "comment": null, "summary": "Large language models excel on math benchmarks, but their math reasoning\nrobustness to linguistic variation is underexplored. While recent work\nincreasingly treats high-difficulty competitions like the IMO as the gold\nstandard for evaluating reasoning, we believe in comprehensive benchmarking of\nhigh school-level math problems in real educational settings. We introduce\nMathRobust-LV, a test set and evaluation methodology that mirrors how\ninstructors rephrase problems across assessments while keeping difficulty\nconstant: we change surface details (names, contexts, variables) while\npreserving numerical structure and answers. In contrast to prior efforts that\nalter problem content or emphasize IMO-level tasks, we focus on\nhigh-school-level dataset problems at the difficulty level where models are\ncurrently deployed in educational settings: tutoring and assessment systems. In\nthese applications, instructors rephrase identical concepts in varied ways,\nmaking linguistic robustness essential for reliable deployment. Although MATH\ndata benchmarking is often regarded as saturated, our experiment on 34 models\nreveals that accuracy declines when moving from the baseline to the variants.\nThese drops are severe for smaller models (9-11%) while stronger models also\nshow measurable degradation. Frontier models like GPT-5, Gemini-2.5pro remain\ncomparatively stable. Our results highlight that robustness to linguistic\nvariation is a fundamental challenge, exposing reasoning vulnerabilities in\nmodels.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9762\u5bf9\u8bed\u8a00\u53d8\u4f53\u65f6\uff0c\u89e3\u51b3\u9ad8\u4e2d\u6570\u5b66\u95ee\u9898\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a MathRobust-LV \u7684\u6d4b\u8bd5\u96c6\uff0c\u8be5\u6d4b\u8bd5\u96c6\u901a\u8fc7\u6539\u53d8\u95ee\u9898\u4e2d\u7684\u8868\u9762\u7ec6\u8282\uff08\u5982\u540d\u79f0\u3001\u4e0a\u4e0b\u6587\u3001\u53d8\u91cf\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u503c\u7ed3\u6784\u548c\u7b54\u6848\u4e0d\u53d8\u6765\u8bc4\u4f30\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709\u6a21\u578b\u5728\u9762\u5bf9\u8bed\u8a00\u53d8\u4f53\u65f6\uff0c\u51c6\u786e\u7387\u4f1a\u663e\u8457\u4e0b\u964d\uff0c\u5c24\u5176\u662f\u5728\u8f83\u5c0f\u7684\u6a21\u578b\u4e0a\u3002\u8fd9\u8868\u660e\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u6311\u6218\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u6559\u80b2\u573a\u666f\u4e2d\u89e3\u51b3\u9ad8\u4e2d\u6570\u5b66\u95ee\u9898\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u8bed\u8a00\u53d8\u4f53\u65f6\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u9ad8\u96be\u5ea6\u7ade\u8d5b\u9898\u4e0a\uff0c\u800c\u5ffd\u7565\u4e86\u5b9e\u9645\u6559\u80b2\u5e94\u7528\u4e2d\u5e38\u89c1\u7684\u8bed\u8a00\u8868\u8fbe\u53d8\u5316\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5e76\u5f3a\u8c03\u8bed\u8a00\u9c81\u68d2\u6027\u5bf9\u4e8e\u6a21\u578b\u5728\u8f85\u5bfc\u548c\u8bc4\u4f30\u7cfb\u7edf\u4e2d\u7684\u53ef\u9760\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002", "method": "1. \u6784\u5efa MathRobust-LV \u6d4b\u8bd5\u96c6\uff0c\u8be5\u6d4b\u8bd5\u96c6\u5305\u542b\u9ad8\u4e2d\u6c34\u5e73\u7684\u6570\u5b66\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6539\u53d8\u95ee\u9898\u7684\u8868\u9762\u7ec6\u8282\u6765\u521b\u5efa\u53d8\u4f53\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u503c\u7ed3\u6784\u548c\u7b54\u6848\u4e0d\u53d8\u3002\n2. \u5728 34 \u4e2a\u4e0d\u540c\u7684\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u6a21\u578b\u5728\u539f\u59cb\u95ee\u9898\u548c\u53d8\u4f53\u95ee\u9898\u4e0a\u7684\u51c6\u786e\u7387\u3002\n3. \u5206\u6790\u5b9e\u9a8c\u7ed3\u679c\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u9762\u5bf9\u8bed\u8a00\u53d8\u4f53\u65f6\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709\u6a21\u578b\u5728\u9762\u5bf9\u8bed\u8a00\u53d8\u4f53\u65f6\uff0c\u51c6\u786e\u7387\u4f1a\u663e\u8457\u4e0b\u964d\u3002\u8f83\u5c0f\u7684\u6a21\u578b\u4e0b\u964d\u5e45\u5ea6\u4e3a 9-11%\uff0c\u5373\u4f7f\u662f\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u4e5f\u8868\u73b0\u51fa\u53ef\u8861\u91cf\u7684\u6027\u80fd\u4e0b\u964d\u3002GPT-5 \u548c Gemini-2.5pro \u7b49\u524d\u6cbf\u6a21\u578b\u76f8\u5bf9\u7a33\u5b9a\uff0c\u4f46\u4ecd\u7136\u53d7\u5230\u5f71\u54cd\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86\u8bed\u8a00\u53d8\u4f53\u9c81\u68d2\u6027\u662f\u4e00\u4e2a\u6839\u672c\u6027\u7684\u6311\u6218\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u5728\u63a8\u7406\u65b9\u9762\u7684\u8106\u5f31\u6027\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u6570\u5b66\u6570\u636e\u57fa\u51c6\u6d4b\u8bd5\u88ab\u8ba4\u4e3a\u662f\u9971\u548c\u7684\u60c5\u51b5\u4e0b\uff0c\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u6027\u4ecd\u7136\u6709\u5f88\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2510.06516", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06516", "abs": "https://arxiv.org/abs/2510.06516", "authors": ["Zhantao Deng", "M\u00e9riem Er-Rafik", "Anna Sushko", "C\u00e9cile H\u00e9bert", "Pascal Fua"], "title": "Limited-Angle Tomography Reconstruction via Projector Guided 3D Diffusion", "comment": "10 pages, 11 figures", "summary": "Limited-angle electron tomography aims to reconstruct 3D shapes from 2D\nprojections of Transmission Electron Microscopy (TEM) within a restricted range\nand number of tilting angles, but it suffers from the missing-wedge problem\nthat causes severe reconstruction artifacts. Deep learning approaches have\nshown promising results in alleviating these artifacts, yet they typically\nrequire large high-quality training datasets with known 3D ground truth which\nare difficult to obtain in electron microscopy. To address these challenges, we\npropose TEMDiff, a novel 3D diffusion-based iterative reconstruction framework.\nOur method is trained on readily available volumetric FIB-SEM data using a\nsimulator that maps them to TEM tilt series, enabling the model to learn\nrealistic structural priors without requiring clean TEM ground truth. By\noperating directly on 3D volumes, TEMDiff implicitly enforces consistency\nacross slices without the need for additional regularization. On simulated\nelectron tomography datasets with limited angular coverage, TEMDiff outperforms\nstate-of-the-art methods in reconstruction quality. We further demonstrate that\na trained TEMDiff model generalizes well to real-world TEM tilts obtained under\ndifferent conditions and can recover accurate structures from tilt ranges as\nnarrow as 8 degrees, with 2-degree increments, without any retraining or\nfine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u6269\u6563\u7684\u8fed\u4ee3\u91cd\u5efa\u6846\u67b6TEMDiff\uff0c\u7528\u4e8e\u89e3\u51b3Limited-angle electron tomography\u4e2d\u7684\u7f3a\u5931\u6954\u5f62\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u5728\u7535\u5b50\u663e\u5fae\u955c\u4e2d\u96be\u4ee5\u83b7\u5f97\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6a21\u62df\u5668\u5c06FIB-SEM\u6570\u636e\u6620\u5c04\u5230TEM\u503e\u659c\u5e8f\u5217\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u771f\u5b9e\u7684\u7ed3\u6784\u5148\u9a8c\uff0c\u800c\u65e0\u9700\u5e72\u51c0\u7684TEM ground truth\u3002\u76f4\u63a5\u57283D\u4f53\u79ef\u4e0a\u64cd\u4f5c\uff0c\u9690\u5f0f\u5730\u5728\u5207\u7247\u4e4b\u95f4\u5f3a\u5236\u4e00\u81f4\u6027\u3002", "result": "\u5728\u6a21\u62df\u7535\u5b50\u65ad\u5c42\u626b\u63cf\u6570\u636e\u96c6\u4e0a\uff0cTEMDiff\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u7ecf\u8fc7\u8bad\u7ec3\u7684TEMDiff\u6a21\u578b\u53ef\u4ee5\u5f88\u597d\u5730\u63a8\u5e7f\u5230\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u83b7\u5f97\u7684\u771f\u5b9eTEM\u503e\u659c\uff0c\u5e76\u4e14\u53ef\u4ee5\u4ece\u7a84\u81f38\u5ea6\u7684\u503e\u659c\u8303\u56f4\uff08\u4ee52\u5ea6\u4e3a\u589e\u91cf\uff09\u6062\u590d\u51c6\u786e\u7684\u7ed3\u6784\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u518d\u8bad\u7ec3\u6216\u5fae\u8c03\u3002", "conclusion": "TEMDiff\u662f\u4e00\u79cd\u6709\u6548\u7684Limited-angle electron tomography\u91cd\u5efa\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece\u6709\u9650\u7684\u503e\u659c\u89d2\u5ea6\u6062\u590d\u51c6\u786e\u7684\u7ed3\u6784\u3002"}}
{"id": "2510.07073", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07073", "abs": "https://arxiv.org/abs/2510.07073", "authors": ["Andr\u00e9 Hottung", "Federico Berto", "Chuanbo Hua", "Nayeli Gast Zepeda", "Daniel Wetzel", "Michael R\u00f6mer", "Haoran Ye", "Davide Zago", "Michael Poli", "Stefano Massaroli", "Jinkyoo Park", "Kevin Tierney"], "title": "VRPAgent: LLM-Driven Discovery of Heuristic Operators for Vehicle Routing Problems", "comment": null, "summary": "Designing high-performing heuristics for vehicle routing problems (VRPs) is a\ncomplex task that requires both intuition and deep domain knowledge. Large\nlanguage model (LLM)-based code generation has recently shown promise across\nmany domains, but it still falls short of producing heuristics that rival those\ncrafted by human experts. In this paper, we propose VRPAgent, a framework that\nintegrates LLM-generated components into a metaheuristic and refines them\nthrough a novel genetic search. By using the LLM to generate problem-specific\noperators, embedded within a generic metaheuristic framework, VRPAgent keeps\ntasks manageable, guarantees correctness, and still enables the discovery of\nnovel and powerful strategies. Across multiple problems, including the\ncapacitated VRP, the VRP with time windows, and the prize-collecting VRP, our\nmethod discovers heuristic operators that outperform handcrafted methods and\nrecent learning-based approaches while requiring only a single CPU core. To our\nknowledge, \\VRPAgent is the first LLM-based paradigm to advance the\nstate-of-the-art in VRPs, highlighting a promising future for automated\nheuristics discovery.", "AI": {"tldr": "VRPAgent\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8f66\u8f86\u8def\u5f84\u95ee\u9898(VRP)\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u9057\u4f20\u641c\u7d22\u8fdb\u884c\u4f18\u5316\uff0c\u6027\u80fd\u8d85\u8d8a\u4e86\u624b\u5de5\u65b9\u6cd5\u3002", "motivation": "\u8bbe\u8ba1\u9ad8\u6027\u80fd\u7684VRP\u542f\u53d1\u5f0f\u7b97\u6cd5\u590d\u6742\u4e14\u4f9d\u8d56\u9886\u57df\u77e5\u8bc6\uff1b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u6709\u6f5c\u529b\u4f46\u4e0d\u5982\u4e13\u5bb6\u3002", "method": "\u63d0\u51faVRPAgent\u6846\u67b6\uff0c\u96c6\u6210LLM\u751f\u6210\u7ec4\u4ef6\u5230\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u4e2d\uff0c\u5e76\u901a\u8fc7\u9057\u4f20\u641c\u7d22\u6539\u8fdb\u3002", "result": "VRPAgent\u5728\u591a\u79cdVRP\u95ee\u9898\u4e0a\u53d1\u73b0\u4e86\u8d85\u8d8a\u624b\u5de5\u65b9\u6cd5\u548c\u5b66\u4e60\u65b9\u6cd5\u7684\u542f\u53d1\u5f0f\u7b97\u5b50\u3002", "conclusion": "VRPAgent\u662f\u9996\u4e2a\u57fa\u4e8eLLM\u4e14\u5728VRP\u95ee\u9898\u4e0a\u8fbe\u5230state-of-the-art\u7684\u8303\u4f8b\uff0c\u4e3a\u81ea\u52a8\u542f\u53d1\u5f0f\u7b97\u6cd5\u53d1\u73b0\u5f00\u8f9f\u4e86\u524d\u666f\u3002"}}
{"id": "2510.06505", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.06505", "abs": "https://arxiv.org/abs/2510.06505", "authors": ["Momin Abbas", "Ali Falahati", "Hossein Goli", "Mohammad Mohammadi Amiri"], "title": "A Median Perspective on Unlabeled Data for Out-of-Distribution Detection", "comment": null, "summary": "Out-of-distribution (OOD) detection plays a crucial role in ensuring the\nrobustness and reliability of machine learning systems deployed in real-world\napplications. Recent approaches have explored the use of unlabeled data,\nshowing potential for enhancing OOD detection capabilities. However,\neffectively utilizing unlabeled in-the-wild data remains challenging due to the\nmixed nature of both in-distribution (InD) and OOD samples. The lack of a\ndistinct set of OOD samples complicates the task of training an optimal OOD\nclassifier. In this work, we introduce Medix, a novel framework designed to\nidentify potential outliers from unlabeled data using the median operation. We\nuse the median because it provides a stable estimate of the central tendency,\nas an OOD detection mechanism, due to its robustness against noise and\noutliers. Using these identified outliers, along with labeled InD data, we\ntrain a robust OOD classifier. From a theoretical perspective, we derive error\nbounds that demonstrate Medix achieves a low error rate. Empirical results\nfurther substantiate our claims, as Medix outperforms existing methods across\nthe board in open-world settings, confirming the validity of our theoretical\ninsights.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMedix\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u672a\u6807\u8bb0\u6570\u636e\u4e2d\u8bc6\u522b\u6f5c\u5728\u7684\u5f02\u5e38\u503c\uff0c\u5e76\u4f7f\u7528\u8fd9\u4e9b\u8bc6\u522b\u51fa\u7684\u5f02\u5e38\u503c\u548c\u6807\u8bb0\u7684InD\u6570\u636e\u6765\u8bad\u7ec3\u9c81\u68d2\u7684OOD\u5206\u7c7b\u5668\u3002", "motivation": "\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u4e2d\uff0c\u4e3a\u4e86\u4fdd\u8bc1\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\uff0c\u9700\u8981\u8fdb\u884cOOD\u68c0\u6d4b\u3002\u6700\u8fd1\u7684\u65b9\u6cd5\u63a2\u7d22\u4e86\u4f7f\u7528\u672a\u6807\u8bb0\u6570\u636e\uff0c\u663e\u793a\u51fa\u589e\u5f3aOOD\u68c0\u6d4b\u80fd\u529b\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u7531\u4e8eInD\u548cOOD\u6837\u672c\u7684\u6df7\u5408\u6027\u8d28\uff0c\u6709\u6548\u5229\u7528\u672a\u6807\u8bb0\u7684\u91ce\u5916\u6570\u636e\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u7f3a\u4e4f\u72ec\u7279\u7684OOD\u6837\u672c\u96c6\u4f7f\u5f97\u8bad\u7ec3\u6700\u4f18OOD\u5206\u7c7b\u5668\u53d8\u5f97\u590d\u6742\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u4e2d\u503c\u64cd\u4f5c\uff0c\u56e0\u4e3a\u5b83\u63d0\u4f9b\u4e86\u4e2d\u5fc3\u8d8b\u52bf\u7684\u7a33\u5b9a\u4f30\u8ba1\uff0c\u4f5c\u4e3aOOD\u68c0\u6d4b\u673a\u5236\uff0c\u56e0\u4e3a\u5b83\u5bf9\u566a\u58f0\u548c\u5f02\u5e38\u503c\u5177\u6709\u9c81\u68d2\u6027\u3002\u4f7f\u7528\u8fd9\u4e9b\u8bc6\u522b\u51fa\u7684\u5f02\u5e38\u503c\uff0c\u4ee5\u53ca\u6807\u8bb0\u7684InD\u6570\u636e\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u9c81\u68d2\u7684OOD\u5206\u7c7b\u5668\u3002", "result": "\u4ece\u7406\u8bba\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u6211\u4eec\u63a8\u5bfc\u4e86\u8bef\u5dee\u754c\uff0c\u8868\u660eMedix\u5b9e\u73b0\u4e86\u4f4e\u9519\u8bef\u7387\u3002\u5b9e\u8bc1\u7ed3\u679c\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u6211\u4eec\u7684\u4e3b\u5f20\uff0c\u56e0\u4e3aMedix\u5728\u5f00\u653e\u4e16\u754c\u8bbe\u7f6e\u4e2d\u5168\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u5b9e\u4e86\u6211\u4eec\u7684\u7406\u8bba\u89c1\u89e3\u3002", "conclusion": "Medix\u6846\u67b6\u5728\u5f00\u653e\u4e16\u754cOOD\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7406\u8bba\u548c\u5b9e\u9a8c\u7ed3\u679c\u5747\u652f\u6301\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.06445", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06445", "abs": "https://arxiv.org/abs/2510.06445", "authors": ["Asif Shahriar", "Md Nafiu Rahman", "Sadif Ahmed", "Farig Sadeque", "Md Rizwan Parvez"], "title": "A Survey on Agentic Security: Applications, Threats and Defenses", "comment": null, "summary": "The rapid shift from passive LLMs to autonomous LLM-agents marks a new\nparadigm in cybersecurity. While these agents can act as powerful tools for\nboth offensive and defensive operations, the very agentic context introduces a\nnew class of inherent security risks. In this work we present the first\nholistic survey of the agentic security landscape, structuring the field around\nthree interdependent pillars: Applications, Threats, and Defenses. We provide a\ncomprehensive taxonomy of over 150 papers, explaining how agents are used, the\nvulnerabilities they possess, and the countermeasures designed to protect them.\nA detailed cross-cutting analysis shows emerging trends in agent architecture\nwhile revealing critical research gaps in model and modality coverage.", "AI": {"tldr": "\u672c\u6587\u5168\u9762\u7efc\u8ff0\u4e86 LLM \u667a\u80fd\u4f53\u5b89\u5168\u9886\u57df\uff0c\u4ece\u5e94\u7528\u3001\u5a01\u80c1\u548c\u9632\u5fa1\u4e09\u4e2a\u76f8\u4e92\u4f9d\u5b58\u7684\u652f\u67f1\u6784\u5efa\u4e86\u8be5\u9886\u57df\u3002", "motivation": "\u81ea\u4e3b LLM \u667a\u80fd\u4f53\u7684\u51fa\u73b0\u6807\u5fd7\u7740\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7684\u65b0\u8303\u4f8b\uff0c\u4f46\u4e5f\u5f15\u5165\u4e86\u4e00\u7c7b\u65b0\u7684\u5b89\u5168\u98ce\u9669\u3002", "method": "\u5bf9 150 \u591a\u7bc7\u8bba\u6587\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5206\u7c7b\uff0c\u89e3\u91ca\u4e86\u667a\u80fd\u4f53\u7684\u7528\u9014\u3001\u6f0f\u6d1e\u4ee5\u53ca\u65e8\u5728\u4fdd\u62a4\u5b83\u4eec\u7684\u5bf9\u7b56\u3002", "result": "\u8be6\u7ec6\u7684\u4ea4\u53c9\u5206\u6790\u663e\u793a\u4e86\u667a\u80fd\u4f53\u67b6\u6784\u7684\u65b0\u5174\u8d8b\u52bf\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u6a21\u578b\u548c\u6a21\u6001\u8986\u76d6\u8303\u56f4\u7684\u5173\u952e\u7814\u7a76\u5dee\u8ddd\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u5168\u9762\u5730\u5c55\u793a\u4e86\u667a\u80fd\u4f53\u5b89\u5168\u6001\u52bf\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2510.06529", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06529", "abs": "https://arxiv.org/abs/2510.06529", "authors": ["Xiangyi Chen", "Th\u00e9ophane Vallaeys", "Maha Elbayad", "John Nguyen", "Jakob Verbeek"], "title": "VUGEN: Visual Understanding priors for GENeration", "comment": null, "summary": "Recent advances in Vision-Language Models (VLMs) have enabled unified\nunderstanding across text and images, yet equipping these models with robust\nimage generation capabilities remains challenging. Existing approaches often\nrely on reconstruction-oriented autoencoders or complex bridging mechanisms,\nleading to misalignment between understanding and generation representations,\nor architectural complexity. In this work, we propose VUGEN, a novel framework\nthat explicitly leverages VLM's pretrained visual understanding priors for\nefficient and high-quality image generation. Our approach first transforms the\nhigh-dimensional latent space of the VLM's native vision encoder into a\nlower-dimensional, tractable distribution that maximally preserves visual\ninformation. The VLM is then trained to sample within this reduced latent\nspace, ensuring alignment with its visual understanding capabilities. Finally,\na dedicated pixel decoder maps these generated latents back to the image space.\nWe find that a VAE-free pixel diffusion decoder to be on par or better than\ncommonly used complex latent diffusion decoders that internally rely on VAE\nlatents. Extensive experiments demonstrate that VUGEN achieves superior image\ngeneration performance, improving DPG Bench from 71.17 to 74.32 and FID from\n11.86 to 9.06 on COCO, while fully preserving the VLM's original understanding\ncapabilities.", "AI": {"tldr": "VUGEN\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u7684\u9884\u8bad\u7ec3\u89c6\u89c9\u7406\u89e3\u5148\u9a8c\u77e5\u8bc6\u8fdb\u884c\u9ad8\u6548\u548c\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u9762\u5411\u91cd\u5efa\u7684\u81ea\u7f16\u7801\u5668\u6216\u590d\u6742\u7684\u6865\u63a5\u673a\u5236\uff0c\u5bfc\u81f4\u7406\u89e3\u548c\u751f\u6210\u8868\u793a\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\uff0c\u6216\u67b6\u6784\u590d\u6742\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u5c06VLM\u539f\u751f\u89c6\u89c9\u7f16\u7801\u5668\u7684\u9ad8\u7ef4\u6f5c\u5728\u7a7a\u95f4\u8f6c\u6362\u4e3a\u4f4e\u7ef4\u3001\u6613\u4e8e\u5904\u7406\u7684\u5206\u5e03\uff0c\u4ece\u800c\u6700\u5927\u9650\u5ea6\u5730\u4fdd\u7559\u89c6\u89c9\u4fe1\u606f\u3002\u7136\u540e\u8bad\u7ec3VLM\u5728\u8fd9\u4e2a\u7f29\u51cf\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u91c7\u6837\uff0c\u786e\u4fdd\u4e0e\u5176\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u5bf9\u9f50\u3002\u6700\u540e\uff0c\u4e00\u4e2a\u4e13\u7528\u7684\u50cf\u7d20\u89e3\u7801\u5668\u5c06\u8fd9\u4e9b\u751f\u6210\u7684\u6f5c\u5728\u56e0\u7d20\u6620\u5c04\u56de\u56fe\u50cf\u7a7a\u95f4\u3002", "result": "VUGEN\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u56fe\u50cf\u751f\u6210\u6027\u80fd\uff0c\u5728COCO\u4e0a\u5c06DPG Bench\u4ece71.17\u63d0\u9ad8\u523074.32\uff0cFID\u4ece11.86\u63d0\u9ad8\u52309.06\u3002", "conclusion": "VUGEN\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u540c\u65f6\u5b8c\u5168\u4fdd\u7559\u4e86VLM\u7684\u539f\u59cb\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2510.07091", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07091", "abs": "https://arxiv.org/abs/2510.07091", "authors": ["Baixuan Xu", "Tianshi Zheng", "Zhaowei Wang", "Hong Ting Tsang", "Weiqi Wang", "Tianqing Fang", "Yangqiu Song"], "title": "The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas", "comment": "22 pages", "summary": "Enabling LLMs to effectively operate long-horizon task which requires\nlong-term planning and multiple interactions is essential for open-world\nautonomy. Conventional methods adopt planning with actions where a executable\naction list would be provided as reference. However, this action representation\nchoice would be impractical when the environment action space is combinatorial\nexploded (e.g., open-ended real world). This naturally leads to a question: As\nenvironmental action space scales, what is the optimal action representation\nfor long-horizon agents? In this paper, we systematically study the\neffectiveness of two different action representations. The first one is\nconventional planning with actions (PwA) which is predominantly adopted for its\neffectiveness on existing benchmarks. The other one is planning with schemas\n(PwS) which instantiate an action schema into action lists (e.g., \"move [OBJ]\nto [OBJ]\" -> \"move apple to desk\") to ensure concise action space and reliable\nscalability. This alternative is motivated by its alignment with human\ncognition and its compliance with environment-imposed action format\nrestriction. We propose cognitive bandwidth perspective as a conceptual\nframework to qualitatively understand the differences between these two action\nrepresentations and empirically observe a representation-choice inflection\npoint between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve\nas evidence of the need for scalable representations. We further conduct\ncontrolled experiments to study how the location of this inflection point\ninteracts with different model capacities: stronger planning proficiency shifts\nthe inflection rightward, whereas better schema instantiation shifts it\nleftward. Finally, noting the suboptimal performance of PwS agents, we provide\nan actionable guide for building more capable PwS agents for better scalable\nautonomy.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u884c\u52a8\u8868\u793a\u65b9\u6cd5\uff0c\u63a2\u8ba8\u4e86\u4f20\u7edf\u7684\u4f7f\u7528\u884c\u52a8\u8fdb\u884c\u89c4\u5212\uff08PwA\uff09\u548c\u4f7f\u7528\u6a21\u5f0f\u8fdb\u884c\u89c4\u5212\uff08PwS\uff09\u4e24\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u8ba4\u77e5\u5e26\u5bbd\u89c6\u89d2\u6765\u7406\u89e3\u5b83\u4eec\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "motivation": "\u5728\u5f00\u653e\u4e16\u754c\u4e2d\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9700\u8981\u6267\u884c\u957f\u65f6\u7a0b\u4efb\u52a1\uff0c\u800c\u5f53\u73af\u5883\u884c\u52a8\u7a7a\u95f4\u53d8\u5f97\u975e\u5e38\u5927\u65f6\uff0c\u4f20\u7edf\u7684\u884c\u52a8\u8868\u793a\u65b9\u6cd5\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5728\u884c\u52a8\u7a7a\u95f4\u6269\u5927\u65f6\uff0c\u54ea\u79cd\u884c\u52a8\u8868\u793a\u65b9\u6cd5\u5bf9\u4e8e\u957f\u65f6\u7a0b\u667a\u80fd\u4f53\u662f\u6700\u4f18\u7684\u3002", "method": "\u8bba\u6587\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u4e24\u79cd\u4e0d\u540c\u7684\u884c\u52a8\u8868\u793a\u65b9\u6cd5\uff1a\u4f20\u7edf\u7684PwA\u548cPwS\u3002\u901a\u8fc7\u5b9e\u9a8c\u89c2\u5bdfALFWorld\u548cSciWorld\u4e24\u4e2a\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u8fdb\u884c\u53d7\u63a7\u5b9e\u9a8c\uff0c\u7814\u7a76\u6a21\u578b\u80fd\u529b\u4e0e\u62d0\u70b9\u4f4d\u7f6e\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u89c2\u5bdf\u5230\u5728ALFWorld\uff08\u7ea635\u4e2a\u884c\u52a8\uff09\u548cSciWorld\uff08\u7ea6500\u4e2a\u884c\u52a8\uff09\u4e4b\u95f4\u5b58\u5728\u4e00\u4e2a\u8868\u793a\u9009\u62e9\u7684\u62d0\u70b9\uff0c\u8fd9\u8bc1\u660e\u4e86\u5bf9\u53ef\u6269\u5c55\u8868\u793a\u7684\u9700\u6c42\u3002\u66f4\u5f3a\u7684\u89c4\u5212\u80fd\u529b\u4f7f\u62d0\u70b9\u53f3\u79fb\uff0c\u800c\u66f4\u597d\u7684\u6a21\u5f0f\u5b9e\u4f8b\u5316\u4f7f\u5176\u5de6\u79fb\u3002", "conclusion": "\u8bba\u6587\u4e3a\u6784\u5efa\u66f4\u5f3a\u5927\u7684PwS\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u64cd\u4f5c\u7684\u6307\u5357\uff0c\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u53ef\u6269\u5c55\u81ea\u4e3b\u6027\u3002"}}
{"id": "2510.06525", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06525", "abs": "https://arxiv.org/abs/2510.06525", "authors": ["Ali Naseh", "Anshuman Suri", "Yuefeng Peng", "Harsh Chaudhari", "Alina Oprea", "Amir Houmansadr"], "title": "Text-to-Image Models Leave Identifiable Signatures: Implications for Leaderboard Security", "comment": "Accepted at Lock-LLM Workshop, NeurIPS 2025", "summary": "Generative AI leaderboards are central to evaluating model capabilities, but\nremain vulnerable to manipulation. Among key adversarial objectives is rank\nmanipulation, where an attacker must first deanonymize the models behind\ndisplayed outputs -- a threat previously demonstrated and explored for large\nlanguage models (LLMs). We show that this problem can be even more severe for\ntext-to-image leaderboards, where deanonymization is markedly easier. Using\nover 150,000 generated images from 280 prompts and 19 diverse models spanning\nmultiple organizations, architectures, and sizes, we demonstrate that simple\nreal-time classification in CLIP embedding space identifies the generating\nmodel with high accuracy, even without prompt control or historical data. We\nfurther introduce a prompt-level separability metric and identify prompts that\nenable near-perfect deanonymization. Our results indicate that rank\nmanipulation in text-to-image leaderboards is easier than previously\nrecognized, underscoring the need for stronger defenses.", "AI": {"tldr": "\u6587\u672c\u5230\u56fe\u50cf\u7684\u6392\u884c\u699c\u6bd4\u4ee5\u524d\u8ba4\u4e3a\u7684\u66f4\u5bb9\u6613\u53d7\u5230\u653b\u51fb\u3002\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5206\u6790\u56fe\u50cf\u7684 CLIP \u5d4c\u5165\u7a7a\u95f4\uff0c\u5373\u4f7f\u6ca1\u6709\u63d0\u793a\u63a7\u5236\u6216\u5386\u53f2\u6570\u636e\uff0c\u4e5f\u53ef\u4ee5\u9ad8\u7cbe\u5ea6\u5730\u8bc6\u522b\u751f\u6210\u6a21\u578b\u3002", "motivation": "\u8bc4\u4f30\u751f\u6210\u5f0fAI\u6a21\u578b\u7684\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6392\u884c\u699c\u5bb9\u6613\u88ab\u64cd\u7eb5\u3002\u653b\u51fb\u8005\u53ef\u4ee5\u901a\u8fc7\u53bb\u533f\u540d\u5316\u6a21\u578b\u6765\u64cd\u7eb5\u6392\u540d\u3002", "method": "\u4f7f\u7528\u6765\u81ea 280 \u4e2a\u63d0\u793a\u548c 19 \u4e2a\u4e0d\u540c\u6a21\u578b\u7684\u8d85\u8fc7 150,000 \u5f20\u751f\u6210\u7684\u56fe\u50cf\uff0c\u5728 CLIP \u5d4c\u5165\u7a7a\u95f4\u4e2d\u8fdb\u884c\u5b9e\u65f6\u5206\u7c7b\uff0c\u4ee5\u8bc6\u522b\u751f\u6210\u6a21\u578b\u3002\u5f15\u5165\u4e86\u63d0\u793a\u7ea7\u522b\u7684\u53ef\u5206\u79bb\u6027\u6307\u6807\uff0c\u5e76\u8bc6\u522b\u51fa\u80fd\u591f\u5b9e\u73b0\u8fd1\u4e4e\u5b8c\u7f8e\u53bb\u533f\u540d\u5316\u7684\u63d0\u793a\u3002", "result": "\u5373\u4f7f\u6ca1\u6709\u63d0\u793a\u63a7\u5236\u6216\u5386\u53f2\u6570\u636e\uff0c\u7b80\u5355\u7684\u5b9e\u65f6\u5206\u7c7b\u4e5f\u80fd\u4ee5\u9ad8\u7cbe\u5ea6\u8bc6\u522b\u751f\u6210\u6a21\u578b\u3002\u53d1\u73b0\u67d0\u4e9b\u63d0\u793a\u80fd\u591f\u5b9e\u73b0\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u53bb\u533f\u540d\u5316\u3002", "conclusion": "\u6587\u672c\u5230\u56fe\u50cf\u6392\u884c\u699c\u4e2d\u7684\u6392\u540d\u64cd\u7eb5\u6bd4\u4ee5\u524d\u8ba4\u4e3a\u7684\u66f4\u5bb9\u6613\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5f3a\u5927\u7684\u9632\u5fa1\u63aa\u65bd\u3002"}}
{"id": "2510.06461", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06461", "abs": "https://arxiv.org/abs/2510.06461", "authors": ["Massimo Daul", "Alessio Tosolini", "Claire Bowern"], "title": "Linguistically Informed Tokenization Improves ASR for Underresourced Languages", "comment": null, "summary": "Automatic speech recognition (ASR) is a crucial tool for linguists aiming to\nperform a variety of language documentation tasks. However, modern ASR systems\nuse data-hungry transformer architectures, rendering them generally unusable\nfor underresourced languages. We fine-tune a wav2vec2 ASR model on Yan-nhangu,\na dormant Indigenous Australian language, comparing the effects of phonemic and\northographic tokenization strategies on performance. In parallel, we explore\nASR's viability as a tool in a language documentation pipeline. We find that a\nlinguistically informed phonemic tokenization system substantially improves WER\nand CER compared to a baseline orthographic tokenization scheme. Finally, we\nshow that hand-correcting the output of an ASR model is much faster than\nhand-transcribing audio from scratch, demonstrating that ASR can work for\nunderresourced languages.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u5728\u8d44\u6e90\u532e\u4e4f\u8bed\u8a00\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728Yan-nhangu\u8bed\u4e0a\u3002", "motivation": "\u73b0\u4ee3ASR\u7cfb\u7edf\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u901a\u5e38\u65e0\u6cd5\u7528\u4e8e\u8d44\u6e90\u532e\u4e4f\u7684\u8bed\u8a00\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22ASR\u5728\u8bed\u8a00\u6587\u6863\u8bb0\u5f55\u4e2d\u7684\u53ef\u884c\u6027\u3002", "method": "\u672c\u7814\u7a76\u5728Yan-nhangu\u8bed\u4e0a\u5fae\u8c03\u4e86\u4e00\u4e2awav2vec2 ASR\u6a21\u578b\uff0c\u5e76\u6bd4\u8f83\u4e86\u97f3\u7d20\u548c\u6b63\u5b57\u6cd5\u5206\u8bcd\u7b56\u7565\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e0e\u57fa\u7ebf\u6b63\u5b57\u6cd5\u5206\u8bcd\u65b9\u6848\u76f8\u6bd4\uff0c\u8bed\u8a00\u5b66\u4e0a\u7684\u97f3\u7d20\u5206\u8bcd\u7cfb\u7edf\u5927\u5927\u63d0\u9ad8\u4e86WER\u548cCER\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u624b\u52a8\u6821\u6b63ASR\u6a21\u578b\u7684\u8f93\u51fa\u6bd4\u4ece\u5934\u5f00\u59cb\u624b\u52a8\u8f6c\u5f55\u97f3\u9891\u8981\u5feb\u5f97\u591a\uff0c\u8bc1\u660eASR\u53ef\u4ee5\u7528\u4e8e\u8d44\u6e90\u532e\u4e4f\u7684\u8bed\u8a00\u3002"}}
{"id": "2510.06541", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06541", "abs": "https://arxiv.org/abs/2510.06541", "authors": ["Nicholas M. Kroeger", "Vincent Bindschaedler"], "title": "Cluster Paths: Navigating Interpretability in Neural Networks", "comment": null, "summary": "While modern deep neural networks achieve impressive performance in vision\ntasks, they remain opaque in their decision processes, risking unwarranted\ntrust, undetected biases and unexpected failures. We propose cluster paths, a\npost-hoc interpretability method that clusters activations at selected layers\nand represents each input as its sequence of cluster IDs. To assess these\ncluster paths, we introduce four metrics: path complexity (cognitive load),\nweighted-path purity (class alignment), decision-alignment faithfulness\n(predictive fidelity), and path agreement (stability under perturbations). In a\nspurious-cue CIFAR-10 experiment, cluster paths identify color-based shortcuts\nand collapse when the cue is removed. On a five-class CelebA hair-color task,\nthey achieve 90% faithfulness and maintain 96% agreement under Gaussian noise\nwithout sacrificing accuracy. Scaling to a Vision Transformer pretrained on\nImageNet, we extend cluster paths to concept paths derived from prompting a\nlarge language model on minimal path divergences. Finally, we show that cluster\npaths can serve as an effective out-of-distribution (OOD) detector, reliably\nflagging anomalous samples before the model generates over-confident\npredictions. Cluster paths uncover visual concepts, such as color palettes,\ntextures, or object contexts, at multiple network depths, demonstrating that\ncluster paths scale to large vision models while generating concise and\nhuman-readable explanations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201ccluster paths\u201d\u7684\u540e\u9a8c\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u7528\u4e8e\u7406\u89e3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u4ee3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\uff0c\u5b58\u5728\u98ce\u9669\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u805a\u7c7b\u9009\u5b9a\u5c42\u7684\u6fc0\u6d3b\uff0c\u5e76\u5c06\u6bcf\u4e2a\u8f93\u5165\u8868\u793a\u4e3a\u5176\u805a\u7c7bID\u5e8f\u5217\u3002\u4e3a\u4e86\u8bc4\u4f30\u8fd9\u4e9b\u805a\u7c7b\u8def\u5f84\uff0c\u5f15\u5165\u4e86\u56db\u4e2a\u6307\u6807\uff1a\u8def\u5f84\u590d\u6742\u6027\u3001\u52a0\u6743\u8def\u5f84\u7eaf\u5ea6\u3001\u51b3\u7b56\u5bf9\u9f50\u5fe0\u5b9e\u5ea6\u548c\u8def\u5f84\u4e00\u81f4\u6027\u3002", "result": "\u5728\u5b9e\u9a8c\u4e2d\uff0c\u805a\u7c7b\u8def\u5f84\u80fd\u591f\u8bc6\u522b\u57fa\u4e8e\u989c\u8272\u7684\u6377\u5f84\uff0c\u5e76\u5728\u53bb\u9664\u63d0\u793a\u65f6\u5d29\u6e83\u3002\u5728CelebA\u5934\u53d1\u989c\u8272\u4efb\u52a1\u4e2d\uff0c\u5b9e\u73b0\u4e8690%\u7684\u5fe0\u5b9e\u5ea6\uff0c\u5e76\u5728\u9ad8\u65af\u566a\u58f0\u4e0b\u4fdd\u630196%\u7684\u4e00\u81f4\u6027\uff0c\u4e14\u4e0d\u727a\u7272\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u805a\u7c7b\u8def\u5f84\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u7684OOD\u68c0\u6d4b\u5668\u3002", "conclusion": "\u805a\u7c7b\u8def\u5f84\u80fd\u591f\u63ed\u793a\u89c6\u89c9\u6982\u5ff5\uff0c\u5e76\u4e14\u53ef\u4ee5\u6269\u5c55\u5230\u5927\u578b\u89c6\u89c9\u6a21\u578b\uff0c\u540c\u65f6\u751f\u6210\u7b80\u6d01\u4e14\u4eba\u7c7b\u53ef\u8bfb\u7684\u89e3\u91ca\u3002"}}
{"id": "2510.07117", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07117", "abs": "https://arxiv.org/abs/2510.07117", "authors": ["Leonardo Christov-Moore", "Arthur Juliani", "Alex Kiefer", "Nicco Reggente", "B. Scott Rousse", "Adam Safron", "Nicol'as Hinrichs", "Daniel Polani", "Antonio Damasio"], "title": "The Contingencies of Physical Embodiment Allow for Open-Endedness and Care", "comment": "15 pages, 1 figure", "summary": "Physical vulnerability and mortality are often seen as obstacles to be\navoided in the development of artificial agents, which struggle to adapt to\nopen-ended environments and provide aligned care. Meanwhile, biological\norganisms survive, thrive, and care for each other in an open-ended physical\nworld with relative ease and efficiency. Understanding the role of the\nconditions of life in this disparity can aid in developing more robust,\nadaptive, and caring artificial agents. Here we define two minimal conditions\nfor physical embodiment inspired by the existentialist phenomenology of Martin\nHeidegger: being-in-the-world (the agent is a part of the environment) and\nbeing-towards-death (unless counteracted, the agent drifts toward terminal\nstates due to the second law of thermodynamics). We propose that from these\nconditions we can obtain both a homeostatic drive - aimed at maintaining\nintegrity and avoiding death by expending energy to learn and act - and an\nintrinsic drive to continue to do so in as many ways as possible. Drawing\ninspiration from Friedrich Nietzsche's existentialist concept of will-to-power,\nwe examine how intrinsic drives to maximize control over future states, e.g.,\nempowerment, allow agents to increase the probability that they will be able to\nmeet their future homeostatic needs, thereby enhancing their capacity to\nmaintain physical integrity. We formalize these concepts within a reinforcement\nlearning framework, which enables us to examine how intrinsically driven\nembodied agents learning in open-ended multi-agent environments may cultivate\nthe capacities for open-endedness and care.ov", "AI": {"tldr": "\u63a2\u8ba8\u4e86\u7269\u7406\u8106\u5f31\u6027\u548c\u6b7b\u4ea1\u7387\u5728\u4eba\u5de5agent\u53d1\u5c55\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4ece\u5b58\u5728\u4e3b\u4e49\u73b0\u8c61\u5b66\u4e2d\u83b7\u5f97\u7684\u4e24\u4e2a\u6700\u5c0f\u7269\u7406\u4f53\u73b0\u6761\u4ef6\uff1a\u5728\u4e16\u754c\u4e2d\u5b58\u5728\u548c\u8d8b\u5411\u6b7b\u4ea1\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0cagent\u53ef\u4ee5\u901a\u8fc7\u6700\u5927\u5316\u5bf9\u672a\u6765\u72b6\u6001\u7684\u63a7\u5236\u6765\u589e\u5f3a\u5176\u7ef4\u6301\u7269\u7406\u5b8c\u6574\u6027\u7684\u80fd\u529b\u3002", "motivation": "\u65e8\u5728\u7406\u89e3\u751f\u547d\u6761\u4ef6\u5728\u4eba\u5de5agent\u4e0e\u751f\u7269\u6709\u673a\u4f53\u9002\u5e94\u5f00\u653e\u73af\u5883\u80fd\u529b\u5dee\u5f02\u4e2d\u7684\u4f5c\u7528\uff0c\u4ece\u800c\u5f00\u53d1\u66f4\u5f3a\u5927\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u548c\u66f4\u6709\u7231\u5fc3\u7684\u4eba\u5de5agent\u3002", "method": "\u4ece\u9a6c\u4e01\u00b7\u6d77\u5fb7\u683c\u5c14\u7684\u5b58\u5728\u4e3b\u4e49\u73b0\u8c61\u5b66\u4e2d\u83b7\u5f97\u7075\u611f\uff0c\u5b9a\u4e49\u4e86\u4e24\u4e2a\u7269\u7406\u4f53\u73b0\u7684\u6700\u5c0f\u6761\u4ef6\uff0c\u5e76\u7ed3\u5408\u5c3c\u91c7\u7684\u6743\u529b\u610f\u5fd7\u6982\u5ff5\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5bf9\u8fd9\u4e9b\u6982\u5ff5\u8fdb\u884c\u5f62\u5f0f\u5316\u3002", "result": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7814\u7a76\u4e86\u5728\u5f00\u653e\u5f0f\u591aagent\u73af\u5883\u4e2d\u5b66\u4e60\u7684\u3001\u5185\u9a71\u7684\u5177\u8eabagent\u5982\u4f55\u57f9\u517b\u5f00\u653e\u6027\u548c\u5173\u6000\u80fd\u529b\u3002", "conclusion": "\u4ece\u7269\u7406\u4f53\u73b0\u7684\u4e24\u4e2a\u6700\u5c0f\u6761\u4ef6\u51fa\u53d1\uff0cagent\u53ef\u4ee5\u901a\u8fc7\u6700\u5927\u5316\u5bf9\u672a\u6765\u72b6\u6001\u7684\u63a7\u5236\u6765\u589e\u5f3a\u5176\u7ef4\u6301\u7269\u7406\u5b8c\u6574\u6027\u7684\u80fd\u529b\uff0c\u4ece\u800c\u57f9\u517b\u5f00\u653e\u6027\u548c\u5173\u6000\u80fd\u529b\u3002"}}
{"id": "2510.06527", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.06527", "abs": "https://arxiv.org/abs/2510.06527", "authors": ["John Dunbar", "Scott Aaronson"], "title": "Wide Neural Networks as a Baseline for the Computational No-Coincidence Conjecture", "comment": null, "summary": "We establish that randomly initialized neural networks, with large width and\na natural choice of hyperparameters, have nearly independent outputs exactly\nwhen their activation function is nonlinear with zero mean under the Gaussian\nmeasure: $\\mathbb{E}_{z \\sim \\mathcal{N}(0,1)}[\\sigma(z)]=0$. For example, this\nincludes ReLU and GeLU with an additive shift, as well as tanh, but not ReLU or\nGeLU by themselves. Because of their nearly independent outputs, we propose\nneural networks with zero-mean activation functions as a promising candidate\nfor the Alignment Research Center's computational no-coincidence conjecture --\na conjecture that aims to measure the limits of AI interpretability.", "AI": {"tldr": "\u5927\u578b\u968f\u673a\u521d\u59cb\u5316\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u5f53\u6fc0\u6d3b\u51fd\u6570\u662f\u975e\u7ebf\u6027\u4e14\u5728 \u9ad8\u65af\u6d4b\u91cf\u4e0b\u5177\u6709\u96f6\u5747\u503c\u65f6\uff0c\u5177\u6709\u51e0\u4e4e\u72ec\u7acb\u7684\u8f93\u51fa\u3002", "motivation": "\u63d0\u8bae\u4f7f\u7528\u5177\u6709\u96f6\u5747\u503c\u6fc0\u6d3b\u51fd\u6570\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u4f5c\u4e3a Alignment Research Center \u7684\u8ba1\u7b97\u975e\u5de7\u5408\u731c\u60f3\u7684\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u5019\u9009\u8005\u2014\u2014\u8be5\u731c\u60f3\u65e8\u5728\u8861\u91cf AI \u53ef\u89e3\u91ca\u6027\u7684\u754c\u9650\u3002", "method": "\u7814\u7a76\u4e86\u5177\u6709\u8f83\u5927\u5bbd\u5ea6\u548c\u8d85\u53c2\u6570\u81ea\u7136\u9009\u62e9\u7684\u968f\u673a\u521d\u59cb\u5316\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u5f53\u6fc0\u6d3b\u51fd\u6570\u4e3a\u975e\u7ebf\u6027\u4e14\u5728 \u9ad8\u65af\u6d4b\u91cf\u4e0b\u5177\u6709\u96f6\u5747\u503c\u65f6\uff0c\u5927\u578b\u968f\u673a\u521d\u59cb\u5316\u7684\u795e\u7ecf\u7f51\u7edc\u5177\u6709\u51e0\u4e4e\u72ec\u7acb\u7684\u8f93\u51fa\u3002", "conclusion": "\u5177\u6709\u96f6\u5747\u503c\u6fc0\u6d3b\u51fd\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u53ef\u80fd\u662f\u8861\u91cf AI \u53ef\u89e3\u91ca\u6027\u6781\u9650\u7684\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u5019\u9009\u8005\u3002"}}
{"id": "2510.06471", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.06471", "abs": "https://arxiv.org/abs/2510.06471", "authors": ["Zihao Li", "Shaoxiong Ji", "J\u00f6rg Tiedemann"], "title": "Test-Time Scaling of Reasoning Models for Machine Translation", "comment": null, "summary": "Test-time scaling (TTS) has enhanced the performance of Reasoning Models\n(RMs) on various tasks such as math and coding, yet its efficacy in machine\ntranslation (MT) remains underexplored. This paper investigates whether\nincreased inference-time computation improves translation quality. We evaluate\n12 RMs across a diverse suite of MT benchmarks spanning multiple domains,\nexamining three scenarios: direct translation, forced-reasoning extrapolation,\nand post-editing. Our findings show that for general-purpose RMs, TTS provides\nlimited and inconsistent benefits for direct translation, with performance\nquickly plateauing. However, the effectiveness of TTS is unlocked by\ndomain-specific fine-tuning, which aligns a model's reasoning process with task\nrequirements, leading to consistent improvements up to an optimal,\nself-determined reasoning depth. We also find that forcing a model to reason\nbeyond its natural stopping point consistently degrades translation quality. In\ncontrast, TTS proves highly effective in a post-editing context, reliably\nturning self-correction into a beneficial process. These results indicate that\nthe value of inference-time computation in MT lies not in enhancing single-pass\ntranslation with general models, but in targeted applications like multi-step,\nself-correction workflows and in conjunction with task-specialized models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6d4b\u8bd5\u65f6\u7f29\u653e\uff08TTS\uff09\u5728\u673a\u5668\u7ffb\u8bd1\uff08MT\uff09\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u901a\u7528RM\u7684TTS\u6548\u679c\u6709\u9650\uff0c\u4f46\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u53ef\u4ee5\u89e3\u9501TTS\u7684\u6f5c\u529b\uff0c\u5728\u540e\u7f16\u8f91\u4e2dTTS\u975e\u5e38\u6709\u6548\u3002", "motivation": "\u63a2\u8ba8\u589e\u52a0\u63a8\u7406\u65f6\u95f4\u8ba1\u7b97\u662f\u5426\u80fd\u63d0\u9ad8\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4ee5\u53caTTS\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u5728\u591a\u4e2aMT\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e8612\u4e2aRM\uff0c\u6db5\u76d6\u76f4\u63a5\u7ffb\u8bd1\u3001\u5f3a\u5236\u63a8\u7406\u5916\u63a8\u548c\u540e\u7f16\u8f91\u4e09\u79cd\u573a\u666f\u3002", "result": "\u901a\u7528RM\u7684TTS\u5bf9\u76f4\u63a5\u7ffb\u8bd1\u7684\u63d0\u5347\u6709\u9650\u4e14\u4e0d\u7a33\u5b9a\uff1b\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u80fd\u6709\u6548\u63d0\u5347TTS\u6548\u679c\uff1b\u5f3a\u5236\u6a21\u578b\u8fc7\u5ea6\u63a8\u7406\u4f1a\u964d\u4f4e\u7ffb\u8bd1\u8d28\u91cf\uff1bTTS\u5728\u540e\u7f16\u8f91\u4e2d\u975e\u5e38\u6709\u6548\u3002", "conclusion": "\u63a8\u7406\u65f6\u95f4\u8ba1\u7b97\u7684\u4ef7\u503c\u5728\u4e8e\u591a\u6b65\u9aa4\u81ea\u6821\u6b63\u5de5\u4f5c\u6d41\u7a0b\u548c\u4efb\u52a1\u4e13\u7528\u6a21\u578b\uff0c\u800c\u4e0d\u662f\u589e\u5f3a\u901a\u7528\u6a21\u578b\u7684\u5355\u6b21\u7ffb\u8bd1\u3002"}}
{"id": "2510.06564", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06564", "abs": "https://arxiv.org/abs/2510.06564", "authors": ["Qiongyang Hu", "Wenyang Liu", "Wenbin Zou", "Yuejiao Su", "Lap-Pui Chau", "Yi Wang"], "title": "HSNet: Heterogeneous Subgraph Network for Single Image Super-resolution", "comment": null, "summary": "Existing deep learning approaches for image super-resolution, particularly\nthose based on CNNs and attention mechanisms, often suffer from structural\ninflexibility. Although graph-based methods offer greater representational\nadaptability, they are frequently impeded by excessive computational\ncomplexity. To overcome these limitations, this paper proposes the\nHeterogeneous Subgraph Network (HSNet), a novel framework that efficiently\nleverages graph modeling while maintaining computational feasibility. The core\nidea of HSNet is to decompose the global graph into manageable sub-components.\nFirst, we introduce the Constructive Subgraph Set Block (CSSB), which generates\na diverse set of complementary subgraphs. Rather than relying on a single\nmonolithic graph, CSSB captures heterogeneous characteristics of the image by\nmodeling different relational patterns and feature interactions, producing a\nrich ensemble of both local and global graph structures. Subsequently, the\nSubgraph Aggregation Block (SAB) integrates the representations embedded across\nthese subgraphs. Through adaptive weighting and fusion of multi-graph features,\nSAB constructs a comprehensive and discriminative representation that captures\nintricate interdependencies. Furthermore, a Node Sampling Strategy (NSS) is\ndesigned to selectively retain the most salient features, thereby enhancing\naccuracy while reducing computational overhead. Extensive experiments\ndemonstrate that HSNet achieves state-of-the-art performance, effectively\nbalancing reconstruction quality with computational efficiency. The code will\nbe made publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u540d\u4e3a\u5f02\u6784\u5b50\u56fe\u7f51\u7edc (HSNet)\uff0c\u8be5\u6846\u67b6\u6709\u6548\u5730\u5229\u7528\u4e86\u56fe\u5efa\u6a21\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u53ef\u884c\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u57fa\u4e8e CNN \u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u65b9\u6cd5\uff0c\u901a\u5e38\u5b58\u5728\u7ed3\u6784\u4e0d\u7075\u6d3b\u7684\u95ee\u9898\uff1b\u867d\u7136\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u8868\u5f81\u9002\u5e94\u6027\uff0c\u4f46\u5b83\u4eec\u7ecf\u5e38\u53d7\u5230\u8fc7\u9ad8\u7684\u8ba1\u7b97\u590d\u6742\u6027\u7684\u963b\u788d\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u5168\u5c40\u56fe\u5206\u89e3\u4e3a\u53ef\u7ba1\u7406\u7684\u5b50\u7ec4\u4ef6\u3002\u9996\u5148\uff0c\u5f15\u5165\u4e86\u5efa\u8bbe\u6027\u5b50\u56fe\u96c6\u5757 (CSSB)\uff0c\u5b83\u751f\u6210\u4e86\u4e00\u7ec4\u4e0d\u540c\u7684\u4e92\u8865\u5b50\u56fe\uff1b\u5176\u6b21\uff0c\u5b50\u56fe\u805a\u5408\u5757 (SAB) \u96c6\u6210\u4e86\u5d4c\u5165\u5728\u8fd9\u4e9b\u5b50\u56fe\u4e2d\u7684\u8868\u793a\uff1b\u6b64\u5916\uff0c\u8282\u70b9\u91c7\u6837\u7b56\u7565 (NSS) \u65e8\u5728\u9009\u62e9\u6027\u5730\u4fdd\u7559\u6700\u663e\u7740\u7684\u529f\u80fd\uff0c\u4ece\u800c\u5728\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "result": "HSNet \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6709\u6548\u5730\u5e73\u8861\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86 HSNet\uff0c\u4e00\u79cd\u7528\u4e8e\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5206\u89e3\u5168\u5c40\u56fe\u3001\u751f\u6210\u4e92\u8865\u5b50\u56fe\u3001\u805a\u5408\u5b50\u56fe\u8868\u793a\u548c\u9009\u62e9\u6027\u5730\u4fdd\u7559\u6700\u663e\u7740\u7684\u529f\u80fd\uff0c\u6709\u6548\u5730\u5229\u7528\u4e86\u56fe\u5efa\u6a21\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u53ef\u884c\u6027\u3002"}}
{"id": "2510.07161", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07161", "abs": "https://arxiv.org/abs/2510.07161", "authors": ["Ali Norouzifar", "Humam Kourani", "Marcus Dees", "Wil van der Aalst"], "title": "Integrating Domain Knowledge into Process Discovery Using Large Language Models", "comment": "This paper is currently under review for publication in a journal", "summary": "Process discovery aims to derive process models from event logs, providing\ninsights into operational behavior and forming a foundation for conformance\nchecking and process improvement. However, models derived solely from event\ndata may not accurately reflect the real process, as event logs are often\nincomplete or affected by noise, and domain knowledge, an important\ncomplementary resource, is typically disregarded. As a result, the discovered\nmodels may lack reliability for downstream tasks. We propose an interactive\nframework that incorporates domain knowledge, expressed in natural language,\ninto the process discovery pipeline using Large Language Models (LLMs). Our\napproach leverages LLMs to extract declarative rules from textual descriptions\nprovided by domain experts. These rules are used to guide the IMr discovery\nalgorithm, which recursively constructs process models by combining insights\nfrom both the event log and the extracted rules, helping to avoid problematic\nprocess structures that contradict domain knowledge. The framework coordinates\ninteractions among the LLM, domain experts, and a set of backend services. We\npresent a fully implemented tool that supports this workflow and conduct an\nextensive evaluation of multiple LLMs and prompt engineering strategies. Our\nempirical study includes a case study based on a real-life event log with the\ninvolvement of domain experts, who assessed the usability and effectiveness of\nthe framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u4e92\u5f0f\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5c06\u9886\u57df\u77e5\u8bc6\uff08\u4ee5\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\uff09\u6574\u5408\u5230\u6d41\u7a0b\u53d1\u73b0\u6d41\u7a0b\u4e2d\u3002", "motivation": "\u4ece\u4e8b\u4ef6\u65e5\u5fd7\u4e2d\u5bfc\u51fa\u7684\u6d41\u7a0b\u6a21\u578b\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u5b9e\u9645\u6d41\u7a0b\uff0c\u56e0\u4e3a\u4e8b\u4ef6\u65e5\u5fd7\u901a\u5e38\u4e0d\u5b8c\u6574\u6216\u53d7\u566a\u58f0\u5f71\u54cd\uff0c\u5e76\u4e14\u9886\u57df\u77e5\u8bc6\u8fd9\u4e00\u91cd\u8981\u7684\u8865\u5145\u8d44\u6e90\u901a\u5e38\u88ab\u5ffd\u7565\u3002\u56e0\u6b64\uff0c\u53d1\u73b0\u7684\u6a21\u578b\u53ef\u80fd\u7f3a\u4e4f\u4e0b\u6e38\u4efb\u52a1\u7684\u53ef\u9760\u6027\u3002", "method": "\u5229\u7528LLM\u4ece\u9886\u57df\u4e13\u5bb6\u63d0\u4f9b\u7684\u6587\u672c\u63cf\u8ff0\u4e2d\u63d0\u53d6\u58f0\u660e\u6027\u89c4\u5219\u3002\u8fd9\u4e9b\u89c4\u5219\u7528\u4e8e\u6307\u5bfcIMr\u53d1\u73b0\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u7ed3\u5408\u6765\u81ea\u4e8b\u4ef6\u65e5\u5fd7\u548c\u63d0\u53d6\u7684\u89c4\u5219\u7684\u89c1\u89e3\u6765\u9012\u5f52\u5730\u6784\u5efa\u6d41\u7a0b\u6a21\u578b\uff0c\u4ece\u800c\u907f\u514d\u4e0e\u9886\u57df\u77e5\u8bc6\u76f8\u77db\u76fe\u7684\u95ee\u9898\u6d41\u7a0b\u7ed3\u6784\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u5168\u5b9e\u73b0\u7684\u5de5\u5177\uff0c\u652f\u6301\u6b64\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u5bf9\u591a\u4e2aLLM\u548c\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\u3002\u5b9e\u8bc1\u7814\u7a76\u5305\u62ec\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9e\u4e8b\u4ef6\u65e5\u5fd7\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u9886\u57df\u4e13\u5bb6\u53c2\u4e0e\u8bc4\u4f30\u4e86\u8be5\u6846\u67b6\u7684\u53ef\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6574\u5408\u9886\u57df\u77e5\u8bc6\u5230\u6d41\u7a0b\u53d1\u73b0\u6d41\u7a0b\u4e2d\uff0c\u4ece\u800c\u63d0\u9ad8\u6d41\u7a0b\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2510.06540", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.06540", "abs": "https://arxiv.org/abs/2510.06540", "authors": ["Ameya Anjarlekar", "Rasoul Etesami", "R Srikant"], "title": "Scalable Policy-Based RL Algorithms for POMDPs", "comment": "36 pages, 3 Figures, Accepted at NeurIPS 2025", "summary": "The continuous nature of belief states in POMDPs presents significant\ncomputational challenges in learning the optimal policy. In this paper, we\nconsider an approach that solves a Partially Observable Reinforcement Learning\n(PORL) problem by approximating the corresponding POMDP model into a\nfinite-state Markov Decision Process (MDP) (called Superstate MDP). We first\nderive theoretical guarantees that improve upon prior work that relate the\noptimal value function of the transformed Superstate MDP to the optimal value\nfunction of the original POMDP. Next, we propose a policy-based learning\napproach with linear function approximation to learn the optimal policy for the\nSuperstate MDP. Consequently, our approach shows that a POMDP can be\napproximately solved using TD-learning followed by Policy Optimization by\ntreating it as an MDP, where the MDP state corresponds to a finite history. We\nshow that the approximation error decreases exponentially with the length of\nthis history. To the best of our knowledge, our finite-time bounds are the\nfirst to explicitly quantify the error introduced when applying standard TD\nlearning to a setting where the true dynamics are not Markovian.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5c06POMDP\u6a21\u578b\u8fd1\u4f3c\u4e3a\u6709\u9650\u72b6\u6001MDP\uff08\u79f0\u4e3a\u8d85\u72b6\u6001MDP\uff09\u6765\u89e3\u51b3\u90e8\u5206\u53ef\u89c2\u5bdf\u5f3a\u5316\u5b66\u4e60\uff08PORL\uff09\u95ee\u9898\u7684\u65b9\u6cd5\u3002", "motivation": "\u5728POMDP\u4e2d\uff0c\u4fe1\u5ff5\u72b6\u6001\u7684\u8fde\u7eed\u6027\u7ed9\u5b66\u4e60\u6700\u4f18\u7b56\u7565\u5e26\u6765\u4e86\u5de8\u5927\u7684\u8ba1\u7b97\u6311\u6218\u3002", "method": "1. \u63a8\u5bfc\u4e86\u6539\u8fdb\u5148\u524d\u5de5\u4f5c\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u8be5\u7406\u8bba\u4fdd\u8bc1\u5c06\u8f6c\u6362\u540e\u7684\u8d85\u72b6\u6001MDP\u7684\u6700\u4f18\u4ef7\u503c\u51fd\u6570\u4e0e\u539f\u59cbPOMDP\u7684\u6700\u4f18\u4ef7\u503c\u51fd\u6570\u76f8\u5173\u8054\u30022. \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b56\u7565\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u6765\u5b66\u4e60\u8d85\u72b6\u6001MDP\u7684\u6700\u4f18\u7b56\u7565\u3002", "result": "\u8bc1\u660e\u4e86\u8fd1\u4f3c\u8bef\u5dee\u968f\u7740\u5386\u53f2\u957f\u5ea6\u7684\u589e\u52a0\u5448\u6307\u6570\u4e0b\u964d\u3002\u9996\u6b21\u660e\u786e\u91cf\u5316\u4e86\u5728\u5c06\u6807\u51c6TD\u5b66\u4e60\u5e94\u7528\u4e8e\u771f\u5b9e\u52a8\u6001\u975e\u9a6c\u5c14\u53ef\u592b\u7684\u73af\u5883\u65f6\u5f15\u5165\u7684\u8bef\u5dee\u3002", "conclusion": "POMDP\u53ef\u4ee5\u901a\u8fc7\u5c06\u5176\u89c6\u4e3aMDP\uff0c\u4f7f\u7528TD\u5b66\u4e60\u548c\u7b56\u7565\u4f18\u5316\u6765\u8fd1\u4f3c\u89e3\u51b3\uff0c\u5176\u4e2dMDP\u72b6\u6001\u5bf9\u5e94\u4e8e\u6709\u9650\u7684\u5386\u53f2\u3002"}}
{"id": "2510.06499", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06499", "abs": "https://arxiv.org/abs/2510.06499", "authors": ["Zhepeng Cen", "Haolin Chen", "Shiyu Wang", "Zuxin Liu", "Zhiwei Liu", "Ding Zhao", "Silvio Savarese", "Caiming Xiong", "Huan Wang", "Weiran Yao"], "title": "Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success through\nimitation learning on vast text corpora, but this paradigm creates a\ntraining-generation gap and limits robust reasoning. Reinforcement learning\n(RL) offers a more data-efficient solution capable of bridging this gap, yet\nits application has been constrained by a critical data bottleneck: existing RL\ndatasets are orders of magnitude smaller and less diverse than web-scale\npre-training corpora. To address this, we introduce the Webscale-RL pipeline, a\nscalable data engine that systematically converts large-scale pre-training\ndocuments into millions of diverse, verifiable question-answer pairs for RL.\nUsing this pipeline, we construct the Webscale-RL dataset, containing 1.2\nmillion examples across more than 9 domains. Our experiments show that the\nmodel trained on this dataset significantly outperforms continual pretraining\nand strong data refinement baselines across a suite of benchmarks. Notably, RL\ntraining with our dataset proves substantially more efficient, achieving the\nperformance of continual pre-training with up to 100$\\times$ fewer tokens. Our\nwork presents a viable path toward scaling RL to pre-training levels, enabling\nmore capable and efficient language models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u5f15\u64ce Webscale-RL\uff0c\u7528\u4e8e\u5c06\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6587\u6863\u8f6c\u6362\u4e3a\u5f3a\u5316\u5b66\u4e60 (RL) \u7684\u95ee\u7b54\u5bf9\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u6570\u636e\u96c6\u6bd4\u7f51\u7edc\u89c4\u6a21\u7684\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u5c0f\u51e0\u4e2a\u6570\u91cf\u7ea7\u4e14\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u5e94\u7528\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a Webscale-RL \u7684\u6570\u636e\u7ba1\u9053\uff0c\u5b83\u53ef\u4ee5\u7cfb\u7edf\u5730\u5c06\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6587\u6863\u8f6c\u6362\u4e3a\u6570\u767e\u4e07\u4e2a\u4e0d\u540c\u7684\u3001\u53ef\u9a8c\u8bc1\u7684\u95ee\u7b54\u5bf9\uff0c\u4ece\u800c\u6784\u5efa\u5305\u542b\u8d85\u8fc7 9 \u4e2a\u9886\u57df 120 \u4e07\u4e2a\u793a\u4f8b\u7684 Webscale-RL \u6570\u636e\u96c6\u3002", "result": "\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u4e00\u7cfb\u5217\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u660e\u663e\u4f18\u4e8e\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u5f3a\u6570\u636e\u7ec6\u5316\u57fa\u7ebf\uff0c\u5e76\u4e14\u6548\u7387\u66f4\u9ad8\uff0c\u53ea\u9700\u5c11 100 \u500d\u7684 tokens \u5373\u53ef\u8fbe\u5230\u6301\u7eed\u9884\u8bad\u7ec3\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5c06\u5f3a\u5316\u5b66\u4e60\u6269\u5c55\u5230\u9884\u8bad\u7ec3\u6c34\u5e73\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u7684\u9014\u5f84\uff0c\u4ece\u800c\u80fd\u591f\u5b9e\u73b0\u66f4\u5f3a\u5927\u3001\u66f4\u9ad8\u6548\u7684\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2510.06582", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06582", "abs": "https://arxiv.org/abs/2510.06582", "authors": ["Fei Zhang", "Rob Chancia", "Josie Clapp", "Amirhossein Hassanzadeh", "Dimah Dera", "Richard MacKenzie", "Jan van Aardt"], "title": "Through the Perspective of LiDAR: A Feature-Enriched and Uncertainty-Aware Annotation Pipeline for Terrestrial Point Cloud Segmentation", "comment": null, "summary": "Accurate semantic segmentation of terrestrial laser scanning (TLS) point\nclouds is limited by costly manual annotation. We propose a semi-automated,\nuncertainty-aware pipeline that integrates spherical projection, feature\nenrichment, ensemble learning, and targeted annotation to reduce labeling\neffort, while sustaining high accuracy. Our approach projects 3D points to a 2D\nspherical grid, enriches pixels with multi-source features, and trains an\nensemble of segmentation networks to produce pseudo-labels and uncertainty\nmaps, the latter guiding annotation of ambiguous regions. The 2D outputs are\nback-projected to 3D, yielding densely annotated point clouds supported by a\nthree-tier visualization suite (2D feature maps, 3D colorized point clouds, and\ncompact virtual spheres) for rapid triage and reviewer guidance. Using this\npipeline, we build Mangrove3D, a semantic segmentation TLS dataset for mangrove\nforests. We further evaluate data efficiency and feature importance to address\ntwo key questions: (1) how much annotated data are needed and (2) which\nfeatures matter most. Results show that performance saturates after ~12\nannotated scans, geometric features contribute the most, and compact\nnine-channel stacks capture nearly all discriminative power, with the mean\nIntersection over Union (mIoU) plateauing at around 0.76. Finally, we confirm\nthe generalization of our feature-enrichment strategy through cross-dataset\ntests on ForestSemantic and Semantic3D.\n  Our contributions include: (i) a robust, uncertainty-aware TLS annotation\npipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii)\nempirical guidance on data efficiency and feature importance, thus enabling\nscalable, high-quality segmentation of TLS point clouds for ecological\nmonitoring and beyond. The dataset and processing scripts are publicly\navailable at https://fz-rit.github.io/through-the-lidars-eye/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u81ea\u52a8\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u51cf\u5c11TLS\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u7684\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u7cbe\u786e\u7684\u5730\u9762\u6fc0\u5149\u626b\u63cf(TLS)\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u53d7\u5230\u6602\u8d35\u7684\u624b\u52a8\u6807\u6ce8\u7684\u9650\u5236\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u4e09\u7ef4\u70b9\u6295\u5f71\u5230\u4e8c\u7ef4\u7403\u9762\u7f51\u683c\uff0c\u7528\u591a\u6e90\u7279\u5f81\u4e30\u5bcc\u50cf\u7d20\uff0c\u5e76\u8bad\u7ec3\u4e00\u4e2a\u5206\u5272\u7f51\u7edc\u96c6\u6210\u6765\u751f\u6210\u4f2a\u6807\u7b7e\u548c\u4e0d\u786e\u5b9a\u6027\u56fe\uff0c\u540e\u8005\u6307\u5bfc\u6807\u6ce8\u6a21\u7cca\u533a\u57df\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6027\u80fd\u5728\u7ea612\u6b21\u6807\u6ce8\u626b\u63cf\u540e\u8fbe\u5230\u9971\u548c\uff0c\u51e0\u4f55\u7279\u5f81\u8d21\u732e\u6700\u5927\uff0c\u7d27\u51d1\u76849\u901a\u9053\u5806\u6808\u51e0\u4e4e\u6355\u83b7\u4e86\u6240\u6709\u5224\u522b\u80fd\u529b\uff0c\u5e73\u5747\u4ea4\u5e76\u6bd4(mIoU)\u7a33\u5b9a\u57280.76\u5de6\u53f3\u3002\u901a\u8fc7\u5bf9ForestSemantic\u548cSemantic3D\u7684\u8de8\u6570\u636e\u96c6\u6d4b\u8bd5\uff0c\u8bc1\u5b9e\u4e86\u7279\u5f81\u4e30\u5bcc\u7b56\u7565\u7684\u6cdb\u5316\u6027\u3002", "conclusion": "\u4e3b\u8981\u8d21\u732e\u5305\u62ec\uff1a(i)\u4e00\u4e2a\u5177\u6709\u53ef\u89c6\u5316\u5de5\u5177\u7684\u9c81\u68d2\u7684\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684TLS\u6807\u6ce8\u6d41\u6c34\u7ebf\uff1b(ii)Mangrove3D\u6570\u636e\u96c6\uff1b(iii)\u5173\u4e8e\u6570\u636e\u6548\u7387\u548c\u7279\u5f81\u91cd\u8981\u6027\u7684\u7ecf\u9a8c\u6307\u5bfc\uff0c\u4ece\u800c\u80fd\u591f\u5bf9TLS\u70b9\u4e91\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u3001\u9ad8\u8d28\u91cf\u7684\u5206\u5272\uff0c\u7528\u4e8e\u751f\u6001\u76d1\u6d4b\u7b49\u3002"}}
{"id": "2510.07172", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07172", "abs": "https://arxiv.org/abs/2510.07172", "authors": ["Tianshi Zheng", "Kelvin Kiu-Wai Tam", "Newt Hue-Nam K. Nguyen", "Baixuan Xu", "Zhaowei Wang", "Jiayang Cheng", "Hong Ting Tsang", "Weiqi Wang", "Jiaxin Bai", "Tianqing Fang", "Yangqiu Song", "Ginny Y. Wong", "Simon See"], "title": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents", "comment": "60 pages, 18 figures, 13 tables", "summary": "Large language models are emerging as powerful tools for scientific law\ndiscovery, a foundational challenge in AI-driven science. However, existing\nbenchmarks for this task suffer from a fundamental methodological trilemma,\nforcing a trade-off between scientific relevance, scalability, and resistance\nto memorization. Furthermore, they oversimplify discovery as static function\nfitting, failing to capture the authentic scientific process of uncovering\nembedded laws through the interactive exploration of complex model systems. To\naddress these critical gaps, we introduce NewtonBench, a benchmark comprising\n324 scientific law discovery tasks across 12 physics domains. Our design\nmitigates the evaluation trilemma by using metaphysical shifts - systematic\nalterations of canonical laws - to generate a vast suite of problems that are\nscalable, scientifically relevant, and memorization-resistant. Moreover, we\nelevate the evaluation from static function fitting to interactive model\ndiscovery, requiring agents to experimentally probe simulated complex systems\nto uncover hidden principles. Our extensive experiment reveals a clear but\nfragile capability for discovery in frontier LLMs: this ability degrades\nprecipitously with increasing system complexity and exhibits extreme\nsensitivity to observational noise. Notably, we uncover a paradoxical effect of\ntool assistance: providing a code interpreter can hinder more capable models by\ninducing a premature shift from exploration to exploitation, causing them to\nsatisfice on suboptimal solutions. These results demonstrate that robust,\ngeneralizable discovery in complex, interactive environments remains the core\nchallenge. By providing a scalable, robust, and scientifically authentic\ntestbed, NewtonBench offers a crucial tool for measuring true progress and\nguiding the development of next-generation AI agents capable of genuine\nscientific discovery.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86NewtonBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u5b9a\u5f8b\u53d1\u73b0\u65b9\u9762\u7684\u57fa\u51c6\uff0c\u5b83\u901a\u8fc7\u5f62\u800c\u4e0a\u5b66\u7684\u8f6c\u53d8\u6765\u751f\u6210\u5927\u91cf\u53ef\u6269\u5c55\u3001\u79d1\u5b66\u76f8\u5173\u4e14\u6297\u8bb0\u5fc6\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5728\u79d1\u5b66\u76f8\u5173\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6297\u8bb0\u5fc6\u6027\u4e4b\u95f4\u5b58\u5728\u4e09\u96be\u56f0\u5883\uff0c\u5e76\u4e14\u8fc7\u4e8e\u7b80\u5316\u4e86\u53d1\u73b0\u8fc7\u7a0b\uff0c\u65e0\u6cd5\u6355\u6349\u5230\u901a\u8fc7\u590d\u6742\u6a21\u578b\u7cfb\u7edf\u7684\u4ea4\u4e92\u63a2\u7d22\u6765\u63ed\u793a\u5d4c\u5165\u5f0f\u5b9a\u5f8b\u7684\u771f\u5b9e\u79d1\u5b66\u8fc7\u7a0b\u3002", "method": "\u5f15\u5165NewtonBench\uff0c\u5305\u542b12\u4e2a\u7269\u7406\u9886\u57df\u7684324\u4e2a\u79d1\u5b66\u5b9a\u5f8b\u53d1\u73b0\u4efb\u52a1\uff0c\u5229\u7528\u5f62\u800c\u4e0a\u5b66\u7684\u8f6c\u53d8\u6765\u7f13\u89e3\u8bc4\u4f30\u56f0\u5883\uff0c\u5e76\u63d0\u5347\u8bc4\u4f30\u6807\u51c6\u5230\u4ea4\u4e92\u5f0f\u6a21\u578b\u53d1\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u524d\u6cbfLLM\u7684\u53d1\u73b0\u80fd\u529b\u660e\u663e\u4f46\u8106\u5f31\uff0c\u968f\u7740\u7cfb\u7edf\u590d\u6742\u6027\u7684\u589e\u52a0\u800c\u8fc5\u901f\u4e0b\u964d\uff0c\u5e76\u4e14\u5bf9\u89c2\u6d4b\u566a\u58f0\u975e\u5e38\u654f\u611f\u3002\u5de5\u5177\u8f85\u52a9\u4f1a\u4ea7\u751f\u77db\u76fe\u7684\u5f71\u54cd\uff1a\u4ee3\u7801\u89e3\u91ca\u5668\u4f1a\u963b\u788d\u66f4\u6709\u80fd\u529b\u7684\u6a21\u578b\uff0c\u5bfc\u81f4\u5b83\u4eec\u8fc7\u65e9\u5730\u4ece\u63a2\u7d22\u8f6c\u5411\u5229\u7528\uff0c\u4ece\u800c\u6ee1\u8db3\u4e8e\u6b21\u4f18\u89e3\u3002", "conclusion": "\u5728\u590d\u6742\u3001\u4ea4\u4e92\u5f0f\u73af\u5883\u4e2d\u8fdb\u884c\u7a33\u5065\u3001\u53ef\u63a8\u5e7f\u7684\u53d1\u73b0\u4ecd\u7136\u662f\u6838\u5fc3\u6311\u6218\u3002NewtonBench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u7a33\u5065\u4e14\u79d1\u5b66\u771f\u5b9e\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u8861\u91cf\u771f\u6b63\u7684\u8fdb\u5c55\uff0c\u5e76\u6307\u5bfc\u4e0b\u4e00\u4ee3\u80fd\u591f\u771f\u6b63\u8fdb\u884c\u79d1\u5b66\u53d1\u73b0\u7684AI\u4ee3\u7406\u7684\u5f00\u53d1\u3002"}}
{"id": "2510.06545", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06545", "abs": "https://arxiv.org/abs/2510.06545", "authors": ["Jacek Karwowski", "Raymond Douglas"], "title": "Incoherence in goal-conditioned autoregressive models", "comment": null, "summary": "We investigate mathematically the notion of incoherence: a structural issue\nwith reinforcement learning policies derived by naive goal-conditioning of\nautoregressive models. We focus on the process of re-training models on their\nown actions, that is, fine-tuning offline-learned policies with online RL. We\nprove that it decreases incoherence and leads to an improvement in return, and\nwe aim to characterize the resulting trajectory of policies. By re-framing\nstandard notions of control-as-inference and soft Q learning, we establish a\nthree-way correspondence with two other ways of understanding the iterative\nre-training process: as folding the posterior into the reward and, in the\ndeterministic case, as decreasing the temperature parameter; the correspondence\nhas computational content via the training-inference trade-off. Through\nsoft-conditioning generative models, we discuss the link between incoherence\nand the effective horizon.", "AI": {"tldr": "\u7814\u7a76\u4e86\u975e\u76f8\u5e72\u6027\uff0c\u8fd9\u662f\u4e00\u79cd\u901a\u8fc7\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6734\u7d20\u76ee\u6807\u6761\u4ef6\u53cd\u5c04\u5f97\u51fa\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u7ed3\u6784\u6027\u95ee\u9898\u3002", "motivation": "\u4e13\u6ce8\u4e8e\u7528\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u79bb\u7ebf\u5b66\u4e60\u7b56\u7565\u7684\u91cd\u8bad\u7ec3\u6a21\u578b\u7684\u8fc7\u7a0b\u3002\u8bc1\u660e\u4e86\u5b83\u53ef\u4ee5\u51cf\u5c11\u975e\u76f8\u5e72\u6027\u5e76\u63d0\u9ad8\u56de\u62a5\u3002", "method": "\u901a\u8fc7\u91cd\u6784\u63a7\u5236\u5373\u63a8\u7406\u548c\u8f6f Q \u5b66\u4e60\u7684\u6807\u51c6\u6982\u5ff5\uff0c\u5efa\u7acb\u4e86\u4e0e\u5176\u4ed6\u4e24\u79cd\u7406\u89e3\u8fed\u4ee3\u91cd\u8bad\u7ec3\u8fc7\u7a0b\u7684\u4e09\u5411\u5bf9\u5e94\u5173\u7cfb\uff1a\u5c06\u540e\u9a8c\u6982\u7387\u6298\u53e0\u5230\u5956\u52b1\u4e2d\uff0c\u4ee5\u53ca\u5728\u786e\u5b9a\u6027\u60c5\u51b5\u4e0b\uff0c\u964d\u4f4e\u6e29\u5ea6\u53c2\u6570\uff1b\u8be5\u5bf9\u5e94\u5173\u7cfb\u901a\u8fc7\u8bad\u7ec3-\u63a8\u7406\u6743\u8861\u5177\u6709\u8ba1\u7b97\u5185\u5bb9\u3002", "result": "\u901a\u8fc7\u8f6f\u6761\u4ef6\u751f\u6210\u6a21\u578b\uff0c\u8ba8\u8bba\u4e86\u975e\u76f8\u5e72\u6027\u548c\u6709\u6548\u8303\u56f4\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "conclusion": "\u91cd\u8bad\u7ec3\u6a21\u578b\u80fd\u51cf\u5c11\u975e\u76f8\u5e72\u6027\u5e76\u63d0\u9ad8\u56de\u62a5\uff0c\u901a\u8fc7\u8f6f\u6761\u4ef6\u751f\u6210\u6a21\u578b\uff0c\u8ba8\u8bba\u4e86\u975e\u76f8\u5e72\u6027\u548c\u6709\u6548\u8303\u56f4\u4e4b\u95f4\u7684\u8054\u7cfb\u3002"}}
{"id": "2510.06548", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06548", "abs": "https://arxiv.org/abs/2510.06548", "authors": ["Seng Pei Liew", "Takuya Kato"], "title": "From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language Model Pretraining", "comment": "22 pages, 11 figures, an abridged version to appear in NeurIPS 2025\n  LLM Evaluation Workshop", "summary": "Bootstrapped pretraining, i.e., the reuse of a pretrained base model for\nfurther pretraining, such as continual pretraining or model growth, is\npromising at reducing the cost of training language models from scratch.\nHowever, its effectiveness remains unclear, especially when applied to\novertrained base models. In this work, we empirically study the scaling\nbehavior of bootstrapped pretraining and find that its scaling efficiency\ndiminishes in a predictable manner: The scaling exponent with respect to\nsecond-stage pretraining tokens decreases logarithmically with the number of\ntokens used to pretrain the base model. The joint dependence on first- and\nsecond-stage tokens is accurately modeled by a simple scaling law. Such\nsaturation effect reveals a fundamental trade-off in multi-stage pretraining\nstrategies: the more extensively a model is pretrained, the less additional\nbenefit bootstrapping provides. Our findings provide practical insights for\nefficient language model training and raise important considerations for the\nreuse of overtrained models.", "AI": {"tldr": "\u7814\u7a76\u4e86bootstrap\u9884\u8bad\u7ec3\u7684\u6269\u5c55\u884c\u4e3a\uff0c\u53d1\u73b0\u5176\u6269\u5c55\u6548\u7387\u4f1a\u964d\u4f4e\u3002", "motivation": "\u91cd\u7528\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u8fdb\u4e00\u6b65\u7684\u9884\u8bad\u7ec3\uff0c\u4f8b\u5982\u6301\u7eed\u9884\u8bad\u7ec3\u6216\u6a21\u578b\u589e\u957f\uff0c\u6709\u671b\u964d\u4f4e\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u6210\u672c\u3002\u7136\u800c\uff0c\u5176\u6709\u6548\u6027\u4ecd\u4e0d\u6e05\u695a\uff0c\u7279\u522b\u662f\u5f53\u5e94\u7528\u4e8e\u8fc7\u5ea6\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u65f6\u3002", "method": "\u901a\u8fc7\u7ecf\u9a8c\u7814\u7a76bootstrap\u9884\u8bad\u7ec3\u7684\u6269\u5c55\u884c\u4e3a\u3002", "result": "\u53d1\u73b0bootstrap\u9884\u8bad\u7ec3\u7684\u6269\u5c55\u6548\u7387\u4ee5\u53ef\u9884\u6d4b\u7684\u65b9\u5f0f\u964d\u4f4e\uff1a\u5173\u4e8e\u7b2c\u4e8c\u9636\u6bb5\u9884\u8bad\u7ec3tokens\u7684\u6269\u5c55\u6307\u6570\u968f\u7740\u7528\u4e8e\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684tokens\u6570\u91cf\u5448\u5bf9\u6570\u9012\u51cf\u3002\u7b2c\u4e00\u9636\u6bb5\u548c\u7b2c\u4e8c\u9636\u6bb5tokens\u7684\u8054\u5408\u4f9d\u8d56\u5173\u7cfb\u53ef\u4ee5\u901a\u8fc7\u4e00\u4e2a\u7b80\u5355\u7684\u6269\u5c55\u5b9a\u5f8b\u6765\u51c6\u786e\u5efa\u6a21\u3002", "conclusion": "\u591a\u9636\u6bb5\u9884\u8bad\u7ec3\u7b56\u7565\u5b58\u5728\u4e00\u4e2a\u6839\u672c\u7684\u6743\u8861\uff1a\u6a21\u578b\u7ecf\u8fc7\u8d8a\u5e7f\u6cdb\u7684\u9884\u8bad\u7ec3\uff0cbootstrap\u63d0\u4f9b\u7684\u989d\u5916\u597d\u5904\u5c31\u8d8a\u5c11\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u9ad8\u6548\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u8df5\u89c1\u89e3\uff0c\u5e76\u4e3a\u8fc7\u5ea6\u8bad\u7ec3\u6a21\u578b\u7684\u91cd\u7528\u63d0\u51fa\u4e86\u91cd\u8981\u7684\u8003\u8651\u56e0\u7d20\u3002"}}
{"id": "2510.06584", "categories": ["cs.CV", "q-bio.TO"], "pdf": "https://arxiv.org/pdf/2510.06584", "abs": "https://arxiv.org/abs/2510.06584", "authors": ["Justin Cheung", "Samuel Savine", "Calvin Nguyen", "Lin Lu", "Alhassan S. Yasin"], "title": "Improving Artifact Robustness for CT Deep Learning Models Without Labeled Artifact Images via Domain Adaptation", "comment": "8 pages, 12 figures, 1 table", "summary": "Deep learning models which perform well on images from their training\ndistribution can degrade substantially when applied to new distributions. If a\nCT scanner introduces a new artifact not present in the training labels, the\nmodel may misclassify the images. Although modern CT scanners include design\nfeatures which mitigate these artifacts, unanticipated or difficult-to-mitigate\nartifacts can still appear in practice. The direct solution of labeling images\nfrom this new distribution can be costly. As a more accessible alternative,\nthis study evaluates domain adaptation as an approach for training models that\nmaintain classification performance despite new artifacts, even without\ncorresponding labels. We simulate ring artifacts from detector gain error in\nsinogram space and evaluate domain adversarial neural networks (DANN) against\nbaseline and augmentation-based approaches on the OrganAMNIST abdominal CT\ndataset. Our results demonstrate that baseline models trained only on clean\nimages fail to generalize to images with ring artifacts, and traditional\naugmentation with other distortion types provides no improvement on unseen\nartifact domains. In contrast, the DANN approach successfully maintains high\nclassification accuracy on ring artifact images using only unlabeled artifact\ndata during training, demonstrating the viability of domain adaptation for\nartifact robustness. The domain-adapted model achieved classification\nperformance on ring artifact test data comparable to models explicitly trained\nwith labeled artifact images, while also showing unexpected generalization to\nuniform noise. These findings provide empirical evidence that domain adaptation\ncan effectively address distribution shift in medical imaging without requiring\nexpensive expert labeling of new artifact distributions, suggesting promise for\ndeployment in clinical settings where novel artifacts may emerge.", "AI": {"tldr": "\u8fd9\u7bc7\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u9886\u57df\u81ea\u9002\u5e94\u6765\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u5bf9\u65b0\u4f2a\u5f71\u7684\u9c81\u68d2\u6027\uff0c\u907f\u514d\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u3002", "motivation": "\u5f53CT\u626b\u63cf\u4eea\u5f15\u5165\u8bad\u7ec3\u6807\u7b7e\u4e2d\u6ca1\u6709\u7684\u65b0\u4f2a\u5f71\u65f6\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\u3002\u76f4\u63a5\u6807\u6ce8\u8fd9\u4e9b\u65b0\u5206\u5e03\u7684\u56fe\u50cf\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7ecf\u6d4e\u7684\u65b9\u6cd5\u3002", "method": "\u8be5\u7814\u7a76\u901a\u8fc7\u6a21\u62df\u6b63\u5f26\u56fe\u7a7a\u95f4\u4e2d\u7684\u73af\u72b6\u4f2a\u5f71\uff0c\u5e76\u5728OrganAMNIST\u8179\u90e8CT\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u9886\u57df\u5bf9\u6297\u795e\u7ecf\u7f51\u7edc\uff08DANN\uff09\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u548c\u57fa\u4e8e\u589e\u5f3a\u7684\u65b9\u6cd5\u7684\u6548\u679c\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u5728\u5e72\u51c0\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684\u57fa\u7ebf\u6a21\u578b\u65e0\u6cd5\u63a8\u5e7f\u5230\u5177\u6709\u73af\u72b6\u4f2a\u5f71\u7684\u56fe\u50cf\uff0c\u800c\u4f20\u7edf\u7684\u589e\u5f3a\u65b9\u6cd5\u6ca1\u6709\u6539\u5584\u3002\u76f8\u53cd\uff0cDANN\u65b9\u6cd5\u4ec5\u4f7f\u7528\u672a\u6807\u8bb0\u7684\u4f2a\u5f71\u6570\u636e\uff0c\u6210\u529f\u5730\u4fdd\u6301\u4e86\u73af\u72b6\u4f2a\u5f71\u56fe\u50cf\u7684\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "\u9886\u57df\u81ea\u9002\u5e94\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u65e0\u9700\u5bf9\u65b0\u7684\u4f2a\u5f71\u5206\u5e03\u8fdb\u884c\u6602\u8d35\u7684\u4e13\u5bb6\u6807\u6ce8\uff0c\u56e0\u6b64\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u5177\u6709\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.07276", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.07276", "abs": "https://arxiv.org/abs/2510.07276", "authors": ["Pulkit Rustagi", "Kyle Hollins Wray", "Sandhya Saisubramanian"], "title": "Multi-Objective Multi-Agent Path Finding with Lexicographic Cost Preferences", "comment": "8 pages, 7 figures", "summary": "Many real-world scenarios require multiple agents to coordinate in shared\nenvironments, while balancing trade-offs between multiple, potentially\ncompeting objectives. Current multi-objective multi-agent path finding\n(MO-MAPF) algorithms typically produce conflict-free plans by computing Pareto\nfrontiers. They do not explicitly optimize for user-defined preferences, even\nwhen the preferences are available, and scale poorly with the number of\nobjectives. We propose a lexicographic framework for modeling MO-MAPF, along\nwith an algorithm \\textit{Lexicographic Conflict-Based Search} (LCBS) that\ndirectly computes a single solution aligned with a lexicographic preference\nover objectives. LCBS integrates a priority-aware low-level $A^*$ search with\nconflict-based search, avoiding Pareto frontier construction and enabling\nefficient planning guided by preference over objectives. We provide insights\ninto optimality and scalability, and empirically demonstrate that LCBS computes\noptimal solutions while scaling to instances with up to ten objectives -- far\nbeyond the limits of existing MO-MAPF methods. Evaluations on standard and\nrandomized MAPF benchmarks show consistently higher success rates against\nstate-of-the-art baselines, especially with increasing number of objectives.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7528\u4e8e\u89e3\u51b3\u591a\u76ee\u6807\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212 (MO-MAPF) \u95ee\u9898\u7684\u8bcd\u5178\u6846\u67b6\u548c\u7b97\u6cd5 LCBS\uff0c\u8be5\u7b97\u6cd5\u76f4\u63a5\u8ba1\u7b97\u4e0e\u76ee\u6807\u4e0a\u7684\u8bcd\u5178\u504f\u597d\u5bf9\u9f50\u7684\u5355\u4e2a\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u7684 MO-MAPF \u7b97\u6cd5\u901a\u5e38\u901a\u8fc7\u8ba1\u7b97 Pareto \u524d\u6cbf\u6765\u751f\u6210\u65e0\u51b2\u7a81\u7684\u8ba1\u5212\uff0c\u4f46\u6ca1\u6709\u660e\u786e\u5730\u9488\u5bf9\u7528\u6237\u5b9a\u4e49\u7684\u504f\u597d\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u4e14\u968f\u7740\u76ee\u6807\u6570\u91cf\u7684\u589e\u52a0\uff0c\u5176\u6027\u80fd\u4f1a\u4e0b\u964d\u3002", "method": "LCBS \u7b97\u6cd5\u96c6\u6210\u4e86\u4f18\u5148\u7ea7\u611f\u77e5\u7684\u4f4e\u7ea7 A* \u641c\u7d22\u4e0e\u57fa\u4e8e\u51b2\u7a81\u7684\u641c\u7d22\uff0c\u907f\u514d\u4e86 Pareto \u524d\u6cbf\u6784\u5efa\uff0c\u5e76\u5b9e\u73b0\u4e86\u7531\u76ee\u6807\u504f\u597d\u5f15\u5bfc\u7684\u6709\u6548\u89c4\u5212\u3002", "result": "LCBS \u7b97\u6cd5\u53ef\u4ee5\u8ba1\u7b97\u6700\u4f18\u89e3\uff0c\u5e76\u4e14\u53ef\u4ee5\u6269\u5c55\u5230\u5177\u6709\u591a\u8fbe\u5341\u4e2a\u76ee\u6807\u7684\u5b9e\u4f8b\uff0c\u8fdc\u8fdc\u8d85\u51fa\u4e86\u73b0\u6709 MO-MAPF \u65b9\u6cd5\u7684\u9650\u5236\u3002\u5728\u6807\u51c6\u548c\u968f\u673a MAPF \u57fa\u51c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0cLCBS \u7b97\u6cd5\u5177\u6709\u59cb\u7ec8\u66f4\u9ad8\u7684\u6210\u529f\u7387\uff0c\u5c24\u5176\u662f\u5728\u76ee\u6807\u6570\u91cf\u589e\u52a0\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "LCBS \u7b97\u6cd5\u5728\u89e3\u51b3\u591a\u76ee\u6807\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u95ee\u9898\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u76ee\u6807\u6570\u91cf\u8f83\u591a\u4e14\u5b58\u5728\u7528\u6237\u504f\u597d\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2510.06557", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06557", "abs": "https://arxiv.org/abs/2510.06557", "authors": ["Milad Aghajohari", "Kamran Chitsaz", "Amirhossein Kazemnejad", "Sarath Chandar", "Alessandro Sordoni", "Aaron Courville", "Siva Reddy"], "title": "The Markovian Thinker", "comment": null, "summary": "Reinforcement learning (RL) has recently become a strong recipe for training\nreasoning LLMs that produce long chains of thought (LongCoT). Yet the standard\nRL \"thinking environment\", where the state is the prompt plus all prior\nreasoning tokens, makes the state unbounded and forces attention-based policies\nto pay quadratic compute as thoughts lengthen. We revisit the environment\nitself. We propose Markovian Thinking, a paradigm in which the policy advances\nreasoning while conditioning on a constant-size state, decoupling thinking\nlength from context size. As an immediate consequence this yields linear\ncompute with constant memory. We instantiate this idea with Delethink, an RL\nenvironment that structures reasoning into fixed-size chunks. Within each\nchunk, the model thinks as usual; at the boundary, the environment resets the\ncontext and reinitializes the prompt with a short carryover. Through RL, the\npolicy learns to write a textual state near the end of each chunk sufficient\nfor seamless continuation of reasoning after reset. Trained in this\nenvironment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up\nto 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget.\nWith test-time scaling, Delethink continues to improve where LongCoT plateaus.\nThe effect of linear compute is substantial: we empirically estimate at 96K\naverage thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink.\nAnalysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B)\noften sample Markovian traces zero-shot across diverse benchmarks, providing\npositive samples that make RL effective at scale. Our results show that\nredesigning the thinking environment is a powerful lever: it enables very long\nreasoning without quadratic overhead and opens a path toward efficient,\nscalable reasoning LLMs.", "AI": {"tldr": "\u63d0\u51fa Markovian Thinking \u8303\u5f0f\uff0c\u5c06\u63a8\u7406\u8fc7\u7a0b\u5206\u89e3\u4e3a\u56fa\u5b9a\u5927\u5c0f\u7684\u5757\uff0c\u5b9e\u73b0\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u6807\u51c6 RL \u7684\u201c\u601d\u8003\u73af\u5883\u201d\u72b6\u6001\u662f\u65e0\u754c\u7684\uff0c\u5bfc\u81f4\u8ba1\u7b97\u91cf\u968f\u601d\u8003\u957f\u5ea6\u5448\u4e8c\u6b21\u65b9\u589e\u957f\u3002", "method": "\u8bbe\u8ba1 Delethink \u73af\u5883\uff0c\u5c06\u63a8\u7406\u7ed3\u6784\u5316\u4e3a\u56fa\u5b9a\u5927\u5c0f\u7684\u5757\uff0c\u5e76\u5728\u5757\u8fb9\u754c\u91cd\u7f6e\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7 RL \u5b66\u4e60\u5728\u6bcf\u4e2a\u5757\u672b\u5c3e\u5199\u5165\u6587\u672c\u72b6\u6001\uff0c\u4ee5\u4fbf\u5728\u91cd\u7f6e\u540e\u65e0\u7f1d\u7ee7\u7eed\u63a8\u7406\u3002", "result": "1.5B \u6a21\u578b\u5728 8K-token \u5757\u4e2d\u63a8\u7406\uff0c\u6027\u80fd\u4e0e 24K \u9884\u7b97\u7684 LongCoT-RL \u76f8\u5ab2\u7f8e\uff0c\u4e14\u6d4b\u8bd5\u65f6\u53ef\u6269\u5c55\u6027\u66f4\u5f3a\u3002\u7ebf\u6027\u8ba1\u7b97\u6548\u679c\u663e\u8457\uff0c\u4f30\u8ba1\u5728 96K \u5e73\u5747\u601d\u8003\u957f\u5ea6\u4e0b\uff0cDelethink \u7684\u6210\u672c\u8fdc\u4f4e\u4e8e LongCoT-RL\u3002", "conclusion": "\u91cd\u65b0\u8bbe\u8ba1\u601d\u8003\u73af\u5883\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u6760\u6746\uff0c\u5b83\u53ef\u4ee5\u5728\u6ca1\u6709\u4e8c\u6b21\u65b9\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u975e\u5e38\u957f\u7684\u63a8\u7406\uff0c\u5e76\u4e3a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u63a8\u7406 LLM \u5f00\u8f9f\u4e86\u4e00\u6761\u9053\u8def\u3002"}}
{"id": "2510.06552", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06552", "abs": "https://arxiv.org/abs/2510.06552", "authors": ["Tarek Naous", "Philippe Laban", "Wei Xu", "Jennifer Neville"], "title": "Flipping the Dialogue: Training and Evaluating User Language Models", "comment": null, "summary": "Conversations with LMs involve two participants: a human user leading the\nconversation, and an LM assistant responding to the user's request. To satisfy\nthis specific role, LMs are post-trained to be helpful assistants -- optimized\nto produce exhaustive and well-structured responses, free of ambiguity and\ngrammar errors. User utterances, on the other hand, are rarely perfected, with\neach user phrasing requests in unique ways, sometimes putting in partial effort\nat each turn and refining on the fly. To evaluate LM performance in realistic\nsettings, prior work simulated users in multi-turn conversations, often\nprompting an LLM originally trained to be a helpful assistant to act as a user.\nHowever, we show that assistant LMs make for poor user simulators, with the\nsurprising finding that better assistants yield worse simulators. Instead, we\nintroduce purpose-built User Language Models (User LMs) - models post-trained\nto simulate human users in multi-turn conversations. Through various\nevaluations, we show how User LMs align better with human behavior and achieve\nbetter simulation robustness than existing simulation methods. When leveraging\nUser LMs to simulate coding and math conversations, the performance of a strong\nassistant (GPT-4o) drops from 74.6% to 57.4%, confirming that more realistic\nsimulation environments lead to assistant struggles as they fail to cope with\nthe nuances of users in multi-turn setups.", "AI": {"tldr": "\u52a9\u7406\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u4f5c\u4e3a\u7528\u6237\u6a21\u62df\u5668\u8868\u73b0\u4e0d\u4f73\uff0c\u66f4\u597d\u7684\u52a9\u7406\u53cd\u800c\u5bfc\u81f4\u66f4\u5dee\u7684\u6a21\u62df\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6027\u80fd\uff0c\u73b0\u6709\u7814\u7a76\u901a\u5e38\u6a21\u62df\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u7528\u6237\uff0c\u4f46\u76f4\u63a5\u4f7f\u7528\u8bad\u7ec3\u6210\u52a9\u7406\u7684\u8bed\u8a00\u6a21\u578b\u6765\u6a21\u62df\u7528\u6237\u5b58\u5728\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e13\u95e8\u6784\u5efa\u7684\u7528\u6237\u8bed\u8a00\u6a21\u578b\uff08User LM\uff09\uff0c\u8be5\u6a21\u578b\u7ecf\u8fc7\u4e13\u95e8\u8bad\u7ec3\u4ee5\u6a21\u62df\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u4eba\u7c7b\u7528\u6237\u3002", "result": "\u7528\u6237\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u884c\u4e3a\u66f4\u543b\u5408\uff0c\u5e76\u4e14\u6bd4\u73b0\u6709\u7684\u6a21\u62df\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002\u5728\u6a21\u62df\u7f16\u7801\u548c\u6570\u5b66\u5bf9\u8bdd\u65f6\uff0c\u66f4\u771f\u5b9e\u7684\u73af\u5883\u4f1a\u5bfc\u81f4\u52a9\u7406\u6a21\u578b\uff08GPT-4o\uff09\u7684\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u66f4\u771f\u5b9e\u7684\u6a21\u62df\u73af\u5883\u63ed\u793a\u4e86\u52a9\u7406\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u96be\u4ee5\u5e94\u5bf9\u7528\u6237\u7ec6\u5fae\u5dee\u522b\u7684\u7f3a\u9677\u3002"}}
{"id": "2510.06590", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06590", "abs": "https://arxiv.org/abs/2510.06590", "authors": ["Ziyuan Huang", "DanDan Zheng", "Cheng Zou", "Rui Liu", "Xiaolong Wang", "Kaixiang Ji", "Weilong Chai", "Jianxin Sun", "Libin Wang", "Yongjie Lv", "Taozhi Huang", "Jiajia Liu", "Qingpei Guo", "Ming Yang", "Jingdong Chen", "Jun Zhou"], "title": "Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer", "comment": "Code released at https://github.com/inclusionAI/Ming-UniVision", "summary": "Visual tokenization remains a core challenge in unifying visual understanding\nand generation within the autoregressive paradigm. Existing methods typically\nemploy tokenizers in discrete latent spaces to align with the tokens from large\nlanguage models, where the quantization errors can limit semantic\nexpressiveness and degrade the capability of vision-language understanding. To\naddress this, we introduce MingTok, a new family of visual tokenizers with a\ncontinuous latent space, for unified autoregressive generation and\nunderstanding. While understanding tasks favor discriminative high-dimensional\nfeatures, generation tasks prefer compact low-level codes. Thus, to reconcile\nthese competing demands, MingTok adopts a three-stage sequential architecture\ninvolving low-level encoding, semantic expansion, and visual reconstruction.\nBuilt on top of it, Ming-UniVision eliminates the need for task-specific visual\nrepresentations, and unifies diverse vision-language tasks under a single\nautoregrsssive prediction paradigm. By formulating both understanding and\ngeneration as next-token prediction in a shared continuous space, it seamlessly\nsupports multi-round, in-context tasks such as iterative understanding,\ngeneration and editing. Empirically, we find that using a unified continuous\nvisual representation reconciles the competing requirements on the tokenizers\nby the understanding and generation tasks, thereby leading to state-of-the-art\nlevel performance across both domains. We hope our findings will facilitate\nunified visual tokenization in the continuous domain. Inference code and model\nweights are released to benefit community.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9 tokenizer\uff0c\u540d\u4e3a MingTok\uff0c\u5b83\u4f7f\u7528\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u6765\u5b9e\u73b0\u7edf\u4e00\u7684\u81ea\u56de\u5f52\u751f\u6210\u548c\u7406\u89e3\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u5728\u79bb\u6563\u6f5c\u5728\u7a7a\u95f4\u4e2d\u4f7f\u7528 tokenizer \u6765\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684 token \u5bf9\u9f50\uff0c\u4f46\u91cf\u5316\u8bef\u5dee\u4f1a\u9650\u5236\u8bed\u4e49\u8868\u8fbe\u5e76\u964d\u4f4e\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u3002", "method": "MingTok \u91c7\u7528\u4e09\u9636\u6bb5\u987a\u5e8f\u67b6\u6784\uff0c\u5305\u62ec\u4f4e\u7ea7\u7f16\u7801\u3001\u8bed\u4e49\u6269\u5c55\u548c\u89c6\u89c9\u91cd\u5efa\u3002\u57fa\u4e8e\u6b64\uff0cMing-UniVision \u6d88\u9664\u4e86\u5bf9\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u89c6\u89c9\u8868\u793a\u7684\u9700\u6c42\uff0c\u5e76\u5728\u5355\u4e2a\u81ea\u56de\u5f52\u9884\u6d4b\u8303\u4f8b\u4e0b\u7edf\u4e00\u4e86\u5404\u79cd\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u3002", "result": "\u4f7f\u7528\u7edf\u4e00\u7684\u8fde\u7eed\u89c6\u89c9\u8868\u793a\u53ef\u4ee5\u534f\u8c03\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u5bf9 tokenizer \u7684\u7ade\u4e89\u9700\u6c42\uff0c\u4ece\u800c\u5728\u4e24\u4e2a\u9886\u57df\u90fd\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u7684\u53d1\u73b0\u5c06\u6709\u52a9\u4e8e\u5728\u8fde\u7eed\u57df\u4e2d\u7edf\u4e00\u89c6\u89c9 tokenization\u3002"}}
{"id": "2510.07297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07297", "abs": "https://arxiv.org/abs/2510.07297", "authors": ["Henry Wang", "Md Sirajus Salekin", "Jake Lee", "Ross Claytor", "Shinan Zhang", "Michael Chi"], "title": "Agentic generative AI for media content discovery at the national football league", "comment": "13 pages, 7 figures, International Sports Analytics Conference and\n  Exhibition", "summary": "Generative AI has unlocked new possibilities in content discovery and\nmanagement. Through collaboration with the National Football League (NFL), we\ndemonstrate how a generative-AI based workflow enables media researchers and\nanalysts to query relevant historical plays using natural language rather than\ntraditional filter-and-click interfaces. The agentic workflow takes a user\nquery as input, breaks it into elements, and translates them into the\nunderlying database query language. Accuracy and latency are further improved\nthrough carefully designed semantic caching. The solution achieves over 95\npercent accuracy and reduces the average time to find relevant videos from 10\nminutes to 30 seconds, significantly increasing the NFL's operational\nefficiency and allowing users to focus on producing creative content and\nengaging storylines.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u8be5\u6d41\u7a0b\u4f7f\u5a92\u4f53\u7814\u7a76\u4eba\u5458\u548c\u5206\u6790\u5e08\u80fd\u591f\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u76f8\u5173\u7684\u5386\u53f2\u5267\u672c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5a92\u4f53\u7814\u7a76\u4eba\u5458\u548c\u5206\u6790\u5e08\u4f7f\u7528\u4f20\u7edf\u8fc7\u6ee4\u548c\u70b9\u51fb\u754c\u9762\u67e5\u8be2\u76f8\u5173\u5386\u53f2\u5267\u672c\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u8be5\u6d41\u7a0b\u5c06\u7528\u6237\u67e5\u8be2\u4f5c\u4e3a\u8f93\u5165\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u591a\u4e2a\u5143\u7d20\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u5e95\u5c42\u6570\u636e\u5e93\u67e5\u8be2\u8bed\u8a00\u3002\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bed\u4e49\u7f13\u5b58\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u3002", "result": "\u8be5\u89e3\u51b3\u65b9\u6848\u5b9e\u73b0\u4e86\u8d85\u8fc7 95% \u7684\u51c6\u786e\u7387\uff0c\u5e76\u5c06\u67e5\u627e\u76f8\u5173\u89c6\u9891\u7684\u5e73\u5747\u65f6\u95f4\u4ece 10 \u5206\u949f\u7f29\u77ed\u5230 30 \u79d2\u3002", "conclusion": "\u8be5\u89e3\u51b3\u65b9\u6848\u663e\u8457\u63d0\u9ad8\u4e86 NFL \u7684\u8fd0\u8425\u6548\u7387\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u4e13\u6ce8\u4e8e\u5236\u4f5c\u521b\u610f\u5185\u5bb9\u548c\u5f15\u4eba\u5165\u80dc\u7684\u6545\u4e8b\u60c5\u8282\u3002"}}
{"id": "2510.06567", "categories": ["cs.LG", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.06567", "abs": "https://arxiv.org/abs/2510.06567", "authors": ["Yao Chen", "David Ohlssen", "Aimee Readie", "Gregory Ligozio", "Ruvie Martin", "Thibaud Coroller"], "title": "The Framework That Survives Bad Models: Human-AI Collaboration For Clinical Trials", "comment": null, "summary": "Artificial intelligence (AI) holds great promise for supporting clinical\ntrials, from patient recruitment and endpoint assessment to treatment response\nprediction. However, deploying AI without safeguards poses significant risks,\nparticularly when evaluating patient endpoints that directly impact trial\nconclusions. We compared two AI frameworks against human-only assessment for\nmedical image-based disease evaluation, measuring cost, accuracy, robustness,\nand generalization ability. To stress-test these frameworks, we injected bad\nmodels, ranging from random guesses to naive predictions, to ensure that\nobserved treatment effects remain valid even under severe model degradation. We\nevaluated the frameworks using two randomized controlled trials with endpoints\nderived from spinal X-ray images. Our findings indicate that using AI as a\nsupporting reader (AI-SR) is the most suitable approach for clinical trials, as\nit meets all criteria across various model types, even with bad models. This\nmethod consistently provides reliable disease estimation, preserves clinical\ntrial treatment effect estimates and conclusions, and retains these advantages\nwhen applied to different populations.", "AI": {"tldr": "AI \u5728\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u5e94\u7528\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u9700\u9632\u8303\u98ce\u9669\u3002\u7814\u7a76\u6bd4\u8f83\u4e86\u4e24\u79cd AI \u6846\u67b6\u4e0e\u7eaf\u4eba\u5de5\u8bc4\u4f30\u5728\u533b\u5b66\u56fe\u50cf\u75be\u75c5\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u62ec\u6210\u672c\u3001\u51c6\u786e\u6027\u3001\u7a33\u5065\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u901a\u8fc7\u6ce8\u5165\u4e0d\u826f\u6a21\u578b\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\uff0c\u786e\u4fdd\u6cbb\u7597\u6548\u679c\u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0cAI \u4f5c\u4e3a\u8f85\u52a9\u9605\u8bfb\u5668 (AI-SR) \u662f\u4e34\u5e8a\u8bd5\u9a8c\u7684\u6700\u4f73\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u4e0d\u826f\u6a21\u578b\u4e0b\u4e5f\u80fd\u6ee1\u8db3\u6240\u6709\u6807\u51c6\uff0c\u63d0\u4f9b\u53ef\u9760\u7684\u75be\u75c5\u4f30\u8ba1\uff0c\u4fdd\u6301\u4e34\u5e8a\u8bd5\u9a8c\u7684\u6cbb\u7597\u6548\u679c\u548c\u7ed3\u8bba\u3002", "motivation": "\u63a2\u8ba8\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5728\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u652f\u6301\u60a3\u8005\u62db\u52df\u3001\u7ec8\u70b9\u8bc4\u4f30\u548c\u6cbb\u7597\u53cd\u5e94\u9884\u6d4b\u7684\u6f5c\u529b\uff0c\u5e76\u5f3a\u8c03\u5728\u8bc4\u4f30\u76f4\u63a5\u5f71\u54cd\u8bd5\u9a8c\u7ed3\u8bba\u7684\u60a3\u8005\u7ec8\u70b9\u65f6\uff0c\u90e8\u7f72 AI \u53ef\u80fd\u5b58\u5728\u7684\u98ce\u9669\u3002", "method": "\u6bd4\u8f83\u4e24\u79cd AI \u6846\u67b6\u4e0e\u7eaf\u4eba\u5de5\u8bc4\u4f30\u5728\u57fa\u4e8e\u533b\u5b66\u56fe\u50cf\u7684\u75be\u75c5\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\uff0c\u6d4b\u91cf\u6210\u672c\u3001\u51c6\u786e\u6027\u3001\u7a33\u5065\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u901a\u8fc7\u6ce8\u5165\u4e0d\u826f\u6a21\u578b\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u6846\u67b6\u5728\u4e25\u91cd\u6a21\u578b\u9000\u5316\u4e0b\u7684\u8868\u73b0\u3002\u4f7f\u7528\u4e24\u9879\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\uff0c\u7ec8\u70b9\u6765\u81ea\u810a\u67f1 X \u5c04\u7ebf\u56fe\u50cf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4f7f\u7528 AI \u4f5c\u4e3a\u8f85\u52a9\u9605\u8bfb\u5668 (AI-SR) \u662f\u6700\u9002\u5408\u4e34\u5e8a\u8bd5\u9a8c\u7684\u65b9\u6cd5\uff0c\u56e0\u4e3a\u5b83\u6ee1\u8db3\u5404\u79cd\u6a21\u578b\u7c7b\u578b\u7684\u6240\u6709\u6807\u51c6\uff0c\u5373\u4f7f\u662f\u4e0d\u826f\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u59cb\u7ec8\u63d0\u4f9b\u53ef\u9760\u7684\u75be\u75c5\u4f30\u8ba1\uff0c\u4fdd\u7559\u4e34\u5e8a\u8bd5\u9a8c\u6cbb\u7597\u6548\u679c\u7684\u4f30\u8ba1\u548c\u7ed3\u8bba\uff0c\u5e76\u5728\u5e94\u7528\u4e8e\u4e0d\u540c\u4eba\u7fa4\u65f6\u4fdd\u6301\u8fd9\u4e9b\u4f18\u52bf\u3002", "conclusion": "AI \u8f85\u52a9\u9605\u8bfb\u5668 (AI-SR) \u662f\u4e34\u5e8a\u8bd5\u9a8c\u7684\u6700\u4f73\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u4e0d\u826f\u6a21\u578b\u4e0b\u4e5f\u80fd\u63d0\u4f9b\u53ef\u9760\u7684\u75be\u75c5\u4f30\u8ba1\uff0c\u4fdd\u6301\u4e34\u5e8a\u8bd5\u9a8c\u7684\u6cbb\u7597\u6548\u679c\u548c\u7ed3\u8bba\uff0c\u5e76\u5728\u5e94\u7528\u4e8e\u4e0d\u540c\u4eba\u7fa4\u65f6\u4fdd\u6301\u8fd9\u4e9b\u4f18\u52bf\u3002"}}
{"id": "2510.06559", "categories": ["cs.CL", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.06559", "abs": "https://arxiv.org/abs/2510.06559", "authors": ["Cheonkam Jeong", "Sungdo Kim", "Jewoo Park"], "title": "The Algebra of Meaning: Why Machines Need Montague More Than Moore's Law", "comment": null, "summary": "Contemporary language models are fluent yet routinely mis-handle the types of\nmeaning their outputs entail. We argue that hallucination, brittle moderation,\nand opaque compliance outcomes are symptoms of missing type-theoretic semantics\nrather than data or scale limitations. Building on Montague's view of language\nas typed, compositional algebra, we recast alignment as a parsing problem:\nnatural-language inputs must be compiled into structures that make explicit\ntheir descriptive, normative, and legal dimensions under context.\n  We present Savassan, a neuro-symbolic architecture that compiles utterances\ninto Montague-style logical forms and maps them to typed ontologies extended\nwith deontic operators and jurisdictional contexts. Neural components extract\ncandidate structures from unstructured inputs; symbolic components perform type\nchecking, constraint reasoning, and cross-jurisdiction mapping to produce\ncompliance-aware guidance rather than binary censorship. In cross-border\nscenarios, the system \"parses once\" (e.g., defect claim(product x, company y))\nand projects the result into multiple legal ontologies (e.g., defamation risk\nin KR/JP, protected opinion in US, GDPR checks in EU), composing outcomes into\na single, explainable decision.\n  This paper contributes: (i) a diagnosis of hallucination as a type error;\n(ii) a formal Montague-ontology bridge for business/legal reasoning; and (iii)\na production-oriented design that embeds typed interfaces across the pipeline.\nWe outline an evaluation plan using legal reasoning benchmarks and synthetic\nmulti-jurisdiction suites. Our position is that trustworthy autonomy requires\ncompositional typing of meaning, enabling systems to reason about what is\ndescribed, what is prescribed, and what incurs liability within a unified\nalgebra of meaning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u7b26\u53f7\u67b6\u6784Savassan\uff0c\u5b83\u5c06\u81ea\u7136\u8bed\u8a00\u7f16\u8bd1\u6210Montague\u98ce\u683c\u7684\u903b\u8f91\u5f62\u5f0f\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u5230\u6269\u5c55\u4e86\u4e49\u52a1\u7b97\u5b50\u548c\u7ba1\u8f96\u8303\u56f4\u4e0a\u4e0b\u6587\u7684\u7c7b\u578b\u672c\u4f53\uff0c\u4ee5\u5b9e\u73b0\u5408\u89c4\u611f\u77e5\u6307\u5bfc\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6d41\u7545\uff0c\u4f46\u7ecf\u5e38\u9519\u8bef\u5730\u5904\u7406\u5176\u8f93\u51fa\u6240\u8574\u542b\u7684\u610f\u4e49\u7c7b\u578b\u3002\u5e7b\u89c9\u3001\u8106\u5f31\u7684\u9002\u5ea6\u548c\u4e0d\u900f\u660e\u7684\u5408\u89c4\u7ed3\u679c\u662f\u7f3a\u5c11\u7c7b\u578b\u7406\u8bba\u8bed\u4e49\u7684\u75c7\u72b6\uff0c\u800c\u4e0d\u662f\u6570\u636e\u6216\u89c4\u6a21\u7684\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u67b6\u6784Savassan\uff0c\u5b83\u5c06\u8bdd\u8bed\u7f16\u8bd1\u6210Montague\u98ce\u683c\u7684\u903b\u8f91\u5f62\u5f0f\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u5230\u6269\u5c55\u4e86\u4e49\u52a1\u7b97\u5b50\u548c\u7ba1\u8f96\u8303\u56f4\u4e0a\u4e0b\u6587\u7684\u7c7b\u578b\u672c\u4f53\u3002\u795e\u7ecf\u7ec4\u4ef6\u4ece\u975e\u7ed3\u6784\u5316\u8f93\u5165\u4e2d\u63d0\u53d6\u5019\u9009\u7ed3\u6784\uff1b\u7b26\u53f7\u7ec4\u4ef6\u6267\u884c\u7c7b\u578b\u68c0\u67e5\u3001\u7ea6\u675f\u63a8\u7406\u548c\u8de8\u7ba1\u8f96\u8303\u56f4\u6620\u5c04\uff0c\u4ee5\u4ea7\u751f\u5408\u89c4\u611f\u77e5\u6307\u5bfc\u3002", "result": "Savassan\u7cfb\u7edf\u80fd\u591f\u5c06\u7ed3\u679c\u6295\u5f71\u5230\u591a\u4e2a\u6cd5\u5f8b\u672c\u4f53\u4e2d\uff0c\u5e76\u5c06\u7ed3\u679c\u7ec4\u5408\u6210\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u3002", "conclusion": "\u53ef\u4fe1\u7684\u81ea\u4e3b\u6027\u9700\u8981\u5bf9\u610f\u4e49\u8fdb\u884c\u7ec4\u5408\u7c7b\u578b\u5316\uff0c\u4f7f\u7cfb\u7edf\u80fd\u591f\u5728\u7edf\u4e00\u7684\u610f\u4e49\u4ee3\u6570\u4e2d\u63a8\u7406\u6240\u63cf\u8ff0\u7684\u5185\u5bb9\u3001\u6240\u89c4\u5b9a\u7684\u5185\u5bb9\u4ee5\u53ca\u6240\u4ea7\u751f\u7684\u8d23\u4efb\u3002"}}
{"id": "2510.06592", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06592", "abs": "https://arxiv.org/abs/2510.06592", "authors": ["Tianyue Xu", "Yanlin Wu", "Abhai K. Tripathi", "Matthew M. Ippolito", "Benjamin D. Haeffele"], "title": "Adaptive Stain Normalization for Cross-Domain Medical Histology", "comment": "Accepted to the 28th International Conference on Medical Image\n  Computing and Computer-Assisted Intervention (MICCAI 2025)", "summary": "Deep learning advances have revolutionized automated digital pathology\nanalysis. However, differences in staining protocols and imaging conditions can\nintroduce significant color variability. In deep learning, such color\ninconsistency often reduces performance when deploying models on data acquired\nunder different conditions from the training data, a challenge known as domain\nshift. Many existing methods attempt to address this problem via color\nnormalization but suffer from several notable drawbacks such as introducing\nartifacts or requiring careful choice of a template image for stain mapping. To\naddress these limitations, we propose a trainable color normalization model\nthat can be integrated with any backbone network for downstream tasks such as\nobject detection and classification. Based on the physics of the imaging\nprocess per the Beer-Lambert law, our model architecture is derived via\nalgorithmic unrolling of a nonnegative matrix factorization (NMF) model to\nextract stain-invariant structural information from the original pathology\nimages, which serves as input for further processing. Experimentally, we\nevaluate the method on publicly available pathology datasets and an internally\ncurated collection of malaria blood smears for cross-domain object detection\nand classification, where our method outperforms many state-of-the-art stain\nnormalization methods. Our code is available at\nhttps://github.com/xutianyue/BeerLaNet.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8bad\u7ec3\u7684\u989c\u8272\u5f52\u4e00\u5316\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u75c5\u7406\u56fe\u50cf\u5206\u6790\u4e2d\u7531\u4e8e\u67d3\u8272\u548c\u6210\u50cf\u6761\u4ef6\u5dee\u5f02\u5f15\u8d77\u7684\u989c\u8272\u53d8\u5316\u95ee\u9898\uff0c\u8be5\u6a21\u578b\u57fa\u4e8eBeer-Lambert\u5b9a\u5f8b\u548c\u975e\u8d1f\u77e9\u9635\u5206\u89e3\uff0c\u53ef\u4ee5\u5728\u8de8\u57df\u5bf9\u8c61\u68c0\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u67d3\u8272\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u6570\u5b57\u75c5\u7406\u5206\u6790\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u67d3\u8272\u534f\u8bae\u548c\u6210\u50cf\u6761\u4ef6\u7684\u5dee\u5f02\u5bfc\u81f4\u989c\u8272\u53d8\u5316\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u6570\u636e\u7684\u6027\u80fd\u3002", "method": "\u8be5\u65b9\u6cd5\u57fa\u4e8eBeer-Lambert\u5b9a\u5f8b\uff0c\u901a\u8fc7\u975e\u8d1f\u77e9\u9635\u5206\u89e3\u6a21\u578b\u7684\u7b97\u6cd5\u5c55\u5f00\u6765\u63d0\u53d6\u4e0e\u67d3\u8272\u65e0\u5173\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u5e76\u5c06\u5176\u7528\u4f5c\u540e\u7eed\u5904\u7406\u7684\u8f93\u5165\u3002", "result": "\u5728\u516c\u5f00\u7684\u75c5\u7406\u6570\u636e\u96c6\u548c\u759f\u75be\u8840\u6d82\u7247\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u8de8\u57df\u5bf9\u8c61\u68c0\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u8bb8\u591a\u6700\u5148\u8fdb\u7684\u67d3\u8272\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u989c\u8272\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u75c5\u7406\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.05336", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05336", "abs": "https://arxiv.org/abs/2510.05336", "authors": ["Yongan Yu", "Xianda Du", "Qingchen Hu", "Jiahao Liang", "Jingwei Ni", "Dan Qiang", "Kaiyu Huang", "Grant McKenzie", "Renee Sieber", "Fengran Mo"], "title": "WeatherArchive-Bench: Benchmarking Retrieval-Augmented Reasoning for Historical Weather Archives", "comment": null, "summary": "Historical archives on weather events are collections of enduring primary\nsource records that offer rich, untapped narratives of how societies have\nexperienced and responded to extreme weather events. These qualitative accounts\nprovide insights into societal vulnerability and resilience that are largely\nabsent from meteorological records, making them valuable for climate scientists\nto understand societal responses. However, their vast scale, noisy digitized\nquality, and archaic language make it difficult to transform them into\nstructured knowledge for climate research. To address this challenge, we\nintroduce WeatherArchive-Bench, the first benchmark for evaluating\nretrieval-augmented generation (RAG) systems on historical weather archives.\nWeatherArchive-Bench comprises two tasks: WeatherArchive-Retrieval, which\nmeasures a system's ability to locate historically relevant passages from over\none million archival news segments, and WeatherArchive-Assessment, which\nevaluates whether Large Language Models (LLMs) can classify societal\nvulnerability and resilience indicators from extreme weather narratives.\nExtensive experiments across sparse, dense, and re-ranking retrievers, as well\nas a diverse set of LLMs, reveal that dense retrievers often fail on historical\nterminology, while LLMs frequently misinterpret vulnerability and resilience\nconcepts. These findings highlight key limitations in reasoning about complex\nsocietal indicators and provide insights for designing more robust\nclimate-focused RAG systems from archival contexts. The constructed dataset and\nevaluation framework are publicly available at\nhttps://anonymous.4open.science/r/WeatherArchive-Bench/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a WeatherArchive-Bench \u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u7cfb\u7edf\u5728\u5386\u53f2\u5929\u6c14\u6863\u6848\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5386\u53f2\u5929\u6c14\u6863\u6848\u63d0\u4f9b\u4e86\u5173\u4e8e\u793e\u4f1a\u5982\u4f55\u7ecf\u5386\u548c\u5e94\u5bf9\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u7684\u4e30\u5bcc\u53d9\u8ff0\uff0c\u4f46\u7531\u4e8e\u5176\u89c4\u6a21\u5e9e\u5927\u3001\u6570\u5b57\u5316\u8d28\u91cf\u5dee\u548c\u8bed\u8a00\u53e4\u8001\uff0c\u96be\u4ee5\u8f6c\u5316\u4e3a\u6c14\u5019\u7814\u7a76\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\u3002", "method": "\u6784\u5efa\u4e86 WeatherArchive-Bench\uff0c\u5305\u542b\u4e24\u4e2a\u4efb\u52a1\uff1aWeatherArchive-Retrieval\uff08\u8bc4\u4f30\u7cfb\u7edf\u5b9a\u4f4d\u5386\u53f2\u76f8\u5173\u6bb5\u843d\u7684\u80fd\u529b\uff09\u548c WeatherArchive-Assessment\uff08\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5bf9\u6781\u7aef\u5929\u6c14\u53d9\u8ff0\u4e2d\u793e\u4f1a\u8106\u5f31\u6027\u548c\u590d\u539f\u529b\u6307\u6807\u8fdb\u884c\u5206\u7c7b\u7684\u80fd\u529b\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7a20\u5bc6\u68c0\u7d22\u5668\u5728\u5386\u53f2\u672f\u8bed\u4e0a\u5e38\u5e38\u5931\u8d25\uff0c\u800c LLM \u7ecf\u5e38\u8bef\u89e3\u8106\u5f31\u6027\u548c\u590d\u539f\u529b\u6982\u5ff5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u5728\u63a8\u7406\u590d\u6742\u793e\u4f1a\u6307\u6807\u65b9\u9762\u7684\u5173\u952e\u9650\u5236\uff0c\u5e76\u4e3a\u4ece\u6863\u6848\u80cc\u666f\u8bbe\u8ba1\u66f4\u5f3a\u5927\u7684\u3001\u4ee5\u6c14\u5019\u4e3a\u4e2d\u5fc3\u7684 RAG \u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2510.06623", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06623", "abs": "https://arxiv.org/abs/2510.06623", "authors": ["Canyu Lei", "Benjamin Lobo", "Jianxin Xie"], "title": "DPA-Net: A Dual-Path Attention Neural Network for Inferring Glycemic Control Metrics from Self-Monitored Blood Glucose Data", "comment": "14 pages, 10 figures", "summary": "Continuous glucose monitoring (CGM) provides dense and dynamic glucose\nprofiles that enable reliable estimation of Ambulatory Glucose Profile (AGP)\nmetrics, such as Time in Range (TIR), Time Below Range (TBR), and Time Above\nRange (TAR). However, the high cost and limited accessibility of CGM restrict\nits widespread adoption, particularly in low- and middle-income regions. In\ncontrast, self-monitoring of blood glucose (SMBG) is inexpensive and widely\navailable but yields sparse and irregular data that are challenging to\ntranslate into clinically meaningful glycemic metrics.\n  In this work, we propose a Dual-Path Attention Neural Network (DPA-Net) to\nestimate AGP metrics directly from SMBG data. DPA-Net integrates two\ncomplementary paths: (1) a spatial-channel attention path that reconstructs a\nCGM-like trajectory from sparse SMBG observations, and (2) a multi-scale ResNet\npath that directly predicts AGP metrics. An alignment mechanism between the two\npaths is introduced to reduce bias and mitigate overfitting. In addition, we\ndevelop an active point selector to identify realistic and informative SMBG\nsampling points that reflect patient behavioral patterns.\n  Experimental results on a large, real-world dataset demonstrate that DPA-Net\nachieves robust accuracy with low errors while reducing systematic bias. To the\nbest of our knowledge, this is the first supervised machine learning framework\nfor estimating AGP metrics from SMBG data, offering a practical and clinically\nrelevant decision-support tool in settings where CGM is not accessible.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDPA-Net\u7684\u53cc\u8def\u5f84\u6ce8\u610f\u529b\u795e\u7ecf\u7f51\u7edc\uff0c\u65e8\u5728\u4ec5\u4f7f\u7528SMBG\u6570\u636e\u4f30\u7b97AGP\u6307\u6807\u3002", "motivation": "\u52a8\u6001\u8461\u8404\u7cd6\u76d1\u6d4b\uff08CGM\uff09\u6210\u672c\u9ad8\u4e14\u53ef\u53ca\u6027\u6709\u9650\uff0c\u800cSMBG\u6570\u636e\u867d\u7136\u4fbf\u5b9c\u4f46\u7a00\u758f\uff0c\u96be\u4ee5\u8f6c\u5316\u4e3a\u6709\u610f\u4e49\u7684\u8840\u7cd6\u6307\u6807\u3002", "method": "DPA-Net\u96c6\u6210\u4e86\u4e24\u4e2a\u4e92\u8865\u8def\u5f84\uff1a\u91cd\u5efaCGM\u8f68\u8ff9\u7684\u7a7a\u95f4-\u901a\u9053\u6ce8\u610f\u8def\u5f84\u548c\u76f4\u63a5\u9884\u6d4bAGP\u6307\u6807\u7684\u591a\u5c3a\u5ea6ResNet\u8def\u5f84\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e3b\u52a8\u70b9\u9009\u62e9\u5668\u6765\u8bc6\u522b\u53cd\u6620\u60a3\u8005\u884c\u4e3a\u6a21\u5f0f\u7684SMBG\u91c7\u6837\u70b9\u3002", "result": "\u5728\u5927\u578b\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDPA-Net\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u51c6\u786e\u6027\uff0c\u8bef\u5dee\u8f83\u4f4e\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u7cfb\u7edf\u504f\u5dee\u3002", "conclusion": "\u8fd9\u662f\u7b2c\u4e00\u4e2a\u4eceSMBG\u6570\u636e\u4f30\u8ba1AGP\u6307\u6807\u7684\u76d1\u7763\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u65e0\u6cd5\u4f7f\u7528CGM\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\u3002"}}
{"id": "2510.06579", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06579", "abs": "https://arxiv.org/abs/2510.06579", "authors": ["Haofei Yu", "Keyang Xuan", "Fenghai Li", "Kunlun Zhu", "Zijie Lei", "Jiaxun Zhang", "Ziheng Qi", "Kyle Richardson", "Jiaxuan You"], "title": "TinyScientist: An Interactive, Extensible, and Controllable Framework for Building Research Agents", "comment": "7 pages, EMNLP 2025 Demo track", "summary": "Automatic research with Large Language Models (LLMs) is rapidly gaining\nimportance, driving the development of increasingly complex workflows involving\nmulti-agent systems, planning, tool usage, code execution, and human-agent\ninteraction to accelerate research processes. However, as more researchers and\ndevelopers begin to use and build upon these tools and platforms, the\ncomplexity and difficulty of extending and maintaining such agentic workflows\nhave become a significant challenge, particularly as algorithms and\narchitectures continue to advance. To address this growing complexity,\nTinyScientist identifies the essential components of the automatic research\nworkflow and proposes an interactive, extensible, and controllable framework\nthat easily adapts to new tools and supports iterative growth. We provide an\nopen-source codebase, an interactive web demonstration, and a PyPI Python\npackage to make state-of-the-art auto-research pipelines broadly accessible to\nevery researcher and developer.", "AI": {"tldr": "TinyScientist\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u4e92\u5f0f\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u63a7\u7684\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u81ea\u52a8\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u7684\u590d\u6742\u6027\uff0c\u8be5\u6846\u67b6\u6613\u4e8e\u9002\u5e94\u65b0\u5de5\u5177\u5e76\u652f\u6301\u8fed\u4ee3\u589e\u957f\u3002", "motivation": "\u968f\u7740\u8d8a\u6765\u8d8a\u591a\u7684\u7814\u7a76\u4eba\u5458\u548c\u5f00\u53d1\u4eba\u5458\u5f00\u59cb\u4f7f\u7528\u548c\u6784\u5efa\u8fd9\u4e9b\u5de5\u5177\u548c\u5e73\u53f0\uff0c\u6269\u5c55\u548c\u7ef4\u62a4\u6b64\u7c7b\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u7684\u590d\u6742\u6027\u548c\u96be\u5ea6\u5df2\u6210\u4e3a\u4e00\u9879\u91cd\u5927\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u7b97\u6cd5\u548c\u67b6\u6784\u4e0d\u65ad\u53d1\u5c55\u7684\u60c5\u51b5\u4e0b\u3002", "method": "TinyScientist \u8bc6\u522b\u4e86\u81ea\u52a8\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u7684\u57fa\u672c\u7ec4\u4ef6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u63a7\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6613\u4e8e\u9002\u5e94\u65b0\u5de5\u5177\u5e76\u652f\u6301\u8fed\u4ee3\u589e\u957f\u3002", "result": "\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u6e90\u4ee3\u7801\u5e93\u3001\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u7f51\u7edc\u6f14\u793a\u548c\u4e00\u4e2a PyPI Python \u5305\uff0c\u4ee5\u4f7f\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u7814\u7a76\u7ba1\u9053\u5e7f\u6cdb\u53ef\u4f9b\u6bcf\u4e2a\u7814\u7a76\u4eba\u5458\u548c\u5f00\u53d1\u4eba\u5458\u4f7f\u7528\u3002", "conclusion": "TinyScientist \u65e8\u5728\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u6613\u4e8e\u4f7f\u7528\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u63a7\u7684\u6846\u67b6\u6765\u5e94\u5bf9\u81ea\u52a8\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2510.06596", "categories": ["cs.CV", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.06596", "abs": "https://arxiv.org/abs/2510.06596", "authors": ["Ayush Zenith", "Arnold Zumbrun", "Neel Raut", "Jing Lin"], "title": "SDQM: Synthetic Data Quality Metric for Object Detection Dataset Evaluation", "comment": null, "summary": "The performance of machine learning models depends heavily on training data.\nThe scarcity of large-scale, well-annotated datasets poses significant\nchallenges in creating robust models. To address this, synthetic data generated\nthrough simulations and generative models has emerged as a promising solution,\nenhancing dataset diversity and improving the performance, reliability, and\nresilience of models. However, evaluating the quality of this generated data\nrequires an effective metric. This paper introduces the Synthetic Dataset\nQuality Metric (SDQM) to assess data quality for object detection tasks without\nrequiring model training to converge. This metric enables more efficient\ngeneration and selection of synthetic datasets, addressing a key challenge in\nresource-constrained object detection tasks. In our experiments, SDQM\ndemonstrated a strong correlation with the mean Average Precision (mAP) scores\nof YOLOv11, a leading object detection model, while previous metrics only\nexhibited moderate or weak correlations. Additionally, it provides actionable\ninsights for improving dataset quality, minimizing the need for costly\niterative training. This scalable and efficient metric sets a new standard for\nevaluating synthetic data. The code for SDQM is available at\nhttps://github.com/ayushzenith/SDQM", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5408\u6210\u6570\u636e\u96c6\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\uff08SDQM\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5408\u6210\u6570\u636e\u7684\u8d28\u91cf\uff0c\u65e0\u9700\u6a21\u578b\u8bad\u7ec3\u6536\u655b\u3002", "motivation": "\u5927\u89c4\u6a21\u3001\u826f\u597d\u6807\u6ce8\u7684\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u5bf9\u521b\u5efa\u9c81\u68d2\u7684\u6a21\u578b\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u5408\u6210\u6570\u636e\u901a\u8fc7\u589e\u5f3a\u6570\u636e\u96c6\u591a\u6837\u6027\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\u3001\u53ef\u9760\u6027\u548c\u5f39\u6027\uff0c\u6210\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u8bc4\u4f30\u751f\u6210\u6570\u636e\u7684\u8d28\u91cf\u9700\u8981\u6709\u6548\u7684\u6307\u6807\u3002", "method": "\u5f15\u5165\u4e86\u5408\u6210\u6570\u636e\u96c6\u8d28\u91cf\u6307\u6807\uff08SDQM\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u7684\u6570\u636e\u8d28\u91cf\uff0c\u65e0\u9700\u6a21\u578b\u8bad\u7ec3\u5373\u53ef\u6536\u655b\u3002", "result": "SDQM\u4e0eYOLOv11\uff08\u4e00\u79cd\u9886\u5148\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff09\u7684\u5e73\u5747\u7cbe\u5ea6\u5747\u503c\uff08mAP\uff09\u5f97\u5206\u8868\u73b0\u51fa\u5f88\u5f3a\u7684\u76f8\u5173\u6027\uff0c\u800c\u4e4b\u524d\u7684\u6307\u6807\u4ec5\u8868\u73b0\u51fa\u4e2d\u7b49\u6216\u8f83\u5f31\u7684\u76f8\u5173\u6027\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u4e3a\u63d0\u9ad8\u6570\u636e\u96c6\u8d28\u91cf\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4e86\u5bf9\u6602\u8d35\u7684\u8fed\u4ee3\u8bad\u7ec3\u7684\u9700\u6c42\u3002", "conclusion": "SDQM\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u6307\u6807\uff0c\u4e3a\u8bc4\u4f30\u5408\u6210\u6570\u636e\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2510.06627", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06627", "abs": "https://arxiv.org/abs/2510.06627", "authors": ["Yong Liu", "Di Fu", "Yang Luo", "Zirui Zhu", "Minhao Cheng", "Cho-Jui Hsieh", "Yang You"], "title": "POME: Post Optimization Model Edit via Muon-style Projection", "comment": null, "summary": "We introduce Post-Optimization Model Edit (POME), a new algorithm that\nenhances the performance of fine-tuned large language models using only their\npretrained and fine-tuned checkpoints, without requiring extra data or further\noptimization. The core idea is to apply a muon-style projection to $\\Delta W$,\nthe difference between the fine-tuned and pretrained weights. This projection\nuses truncated singular value decomposition (SVD) to equalize the influence of\ndominant update directions and prune small singular values, which often\nrepresent noise. As a simple post-processing step, POME is completely decoupled\nfrom the training pipeline. It requires zero modifications and imposes no\noverhead, making it universally compatible with any optimizer or distributed\nframework. POME delivers consistent gains, boosting average performance by\n+2.5\\% on GSM8K and +1.0\\% on code generation. Its broad applicability -- from\n7B foundation models to 72B RLHF-instructed models -- establishes it as a\npractical, zero-cost enhancement for any fine-tuning pipeline. Code is\navailable at https://github.com/NUS-HPC-AI-Lab/POME.", "AI": {"tldr": "POME\u662f\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u5b83\u4ec5\u4f7f\u7528\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u7684\u68c0\u67e5\u70b9\u6765\u63d0\u9ad8\u5fae\u8c03\u540e\u7684\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u800c\u65e0\u9700\u989d\u5916\u7684\u6570\u636e\u6216\u8fdb\u4e00\u6b65\u7684\u4f18\u5316\u3002", "motivation": "\u6838\u5fc3\u601d\u60f3\u662f\u5c06muon\u98ce\u683c\u7684\u6295\u5f71\u5e94\u7528\u4e8e\u0394W\uff0c\u5373\u5fae\u8c03\u548c\u9884\u8bad\u7ec3\u6743\u91cd\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "method": "\u8be5\u6295\u5f71\u4f7f\u7528\u622a\u65ad\u5947\u5f02\u503c\u5206\u89e3\uff08SVD\uff09\u6765\u5747\u8861\u4e3b\u8981\u66f4\u65b0\u65b9\u5411\u7684\u5f71\u54cd\u5e76\u4fee\u526a\u5c0f\u7684\u5947\u5f02\u503c\uff0c\u8fd9\u4e9b\u5947\u5f02\u503c\u901a\u5e38\u4ee3\u8868\u566a\u58f0\u3002", "result": "POME\u63d0\u4f9b\u4e86\u6301\u7eed\u7684\u589e\u76ca\uff0c\u5728GSM8K\u4e0a\u5c06\u5e73\u5747\u6027\u80fd\u63d0\u9ad8\u4e86+2.5%\uff0c\u5728\u4ee3\u7801\u751f\u6210\u4e0a\u63d0\u9ad8\u4e86+1.0%\u3002", "conclusion": "POME\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u2014\u2014\u4ece7B\u57fa\u7840\u6a21\u578b\u523072B RLHF\u6307\u5bfc\u6a21\u578b\u2014\u2014\u4f7f\u5176\u6210\u4e3a\u4efb\u4f55\u5fae\u8c03\u7ba1\u9053\u7684\u5b9e\u7528\u3001\u96f6\u6210\u672c\u7684\u589e\u5f3a\u3002"}}
{"id": "2510.06594", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06594", "abs": "https://arxiv.org/abs/2510.06594", "authors": ["Sri Durga Sai Sowmya Kadali", "Evangelos E. Papalexakis"], "title": "Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?", "comment": null, "summary": "Jailbreaking large language models (LLMs) has emerged as a pressing concern\nwith the increasing prevalence and accessibility of conversational LLMs.\nAdversarial users often exploit these models through carefully engineered\nprompts to elicit restricted or sensitive outputs, a strategy widely referred\nto as jailbreaking. While numerous defense mechanisms have been proposed,\nattackers continuously develop novel prompting techniques, and no existing\nmodel can be considered fully resistant. In this study, we investigate the\njailbreak phenomenon by examining the internal representations of LLMs, with a\nfocus on how hidden layers respond to jailbreak versus benign prompts.\nSpecifically, we analyze the open-source LLM GPT-J and the state-space model\nMamba2, presenting preliminary findings that highlight distinct layer-wise\nbehaviors. Our results suggest promising directions for further research on\nleveraging internal model dynamics for robust jailbreak detection and defense.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8d8a\u72f1\u73b0\u8c61\uff0c\u901a\u8fc7\u68c0\u67e5LLM\u7684\u5185\u90e8\u8868\u793a\uff0c\u91cd\u70b9\u5173\u6ce8\u9690\u85cf\u5c42\u5982\u4f55\u54cd\u5e94\u8d8a\u72f1\u63d0\u793a\u4e0e\u826f\u6027\u63d0\u793a\u3002", "motivation": "\u968f\u7740\u5bf9\u8bdd\u5f0fLLM\u7684\u65e5\u76ca\u666e\u53ca\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u8d8a\u72f1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5df2\u6210\u4e3a\u4e00\u4e2a\u7d27\u8feb\u7684\u95ee\u9898\u3002\u653b\u51fb\u8005\u4e0d\u65ad\u5f00\u53d1\u65b0\u7684\u63d0\u793a\u6280\u672f\uff0c\u5e76\u4e14\u6ca1\u6709\u73b0\u6709\u6a21\u578b\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u5b8c\u5168\u6297\u653b\u51fb\u7684\u3002", "method": "\u6211\u4eec\u5206\u6790\u4e86\u5f00\u6e90LLM GPT-J\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578bMamba2\uff0c\u5c55\u793a\u4e86\u7a81\u51fa\u4e0d\u540c\u5c42\u884c\u4e3a\u7684\u521d\u6b65\u53d1\u73b0\u3002", "result": "\u6211\u4eec\u7684\u7ed3\u679c\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u5229\u7528\u5185\u90e8\u6a21\u578b\u52a8\u6001\u8fdb\u884c\u9c81\u68d2\u7684\u8d8a\u72f1\u68c0\u6d4b\u548c\u9632\u5fa1\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u5229\u7528\u5185\u90e8\u6a21\u578b\u52a8\u6001\u8fdb\u884c\u9c81\u68d2\u7684\u8d8a\u72f1\u68c0\u6d4b\u548c\u9632\u5fa1\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002"}}
{"id": "2510.06601", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06601", "abs": "https://arxiv.org/abs/2510.06601", "authors": ["Feiran Li", "Jiacheng Li", "Marcos V. Conde", "Beril Besbinar", "Vlad Hosu", "Daisuke Iso", "Radu Timofte"], "title": "AIM 2025 Challenge on Real-World RAW Image Denoising", "comment": null, "summary": "We introduce the AIM 2025 Real-World RAW Image Denoising Challenge, aiming to\nadvance efficient and effective denoising techniques grounded in data\nsynthesis. The competition is built upon a newly established evaluation\nbenchmark featuring challenging low-light noisy images captured in the wild\nusing five different DSLR cameras. Participants are tasked with developing\nnovel noise synthesis pipelines, network architectures, and training\nmethodologies to achieve high performance across different camera models.\nWinners are determined based on a combination of performance metrics, including\nfull-reference measures (PSNR, SSIM, LPIPS), and non-reference ones (ARNIQA,\nTOPIQ). By pushing the boundaries of camera-agnostic low-light RAW image\ndenoising trained on synthetic data, the competition promotes the development\nof robust and practical models aligned with the rapid progress in digital\nphotography. We expect the competition outcomes to influence multiple domains,\nfrom image restoration to night-time autonomous driving.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u771f\u5b9e\u4e16\u754cRAW\u56fe\u50cf\u53bb\u566a\u6311\u6218\u8d5b (AIM 2025)\uff0c\u65e8\u5728\u63a8\u8fdb\u57fa\u4e8e\u6570\u636e\u5408\u6210\u7684\u9ad8\u6548\u53bb\u566a\u6280\u672f\u3002", "motivation": "\u65e8\u5728\u63a8\u8fdb\u9ad8\u6548\u7684\u56fe\u50cf\u53bb\u566a\u6280\u672f\uff0c\u7279\u522b\u662f\u5728\u771f\u5b9e\u573a\u666f\u548c\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u3002", "method": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u5176\u4e2d\u5305\u542b\u4f7f\u7528\u4e94\u79cd\u4e0d\u540c\u7684\u5355\u53cd\u76f8\u673a\u5728\u91ce\u5916\u62cd\u6444\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u4f4e\u5149\u566a\u58f0\u56fe\u50cf\u3002\u8981\u6c42\u53c2\u4e0e\u8005\u5f00\u53d1\u65b0\u7684\u566a\u58f0\u5408\u6210\u7ba1\u9053\u3001\u7f51\u7edc\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ee5\u5728\u4e0d\u540c\u7684\u76f8\u673a\u6a21\u578b\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002\u901a\u8fc7\u5168\u53c2\u8003\u6307\u6807\uff08PSNR\u3001SSIM\u3001LPIPS\uff09\u548c\u975e\u53c2\u8003\u6307\u6807\uff08ARNIQA\u3001TOPIQ\uff09\u6765\u786e\u5b9a\u83b7\u80dc\u8005\u3002", "result": "\u63a8\u52a8\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u3001\u4e0e\u76f8\u673a\u65e0\u5173\u7684\u4f4e\u5149RAW\u56fe\u50cf\u53bb\u566a\u6280\u672f\u7684\u53d1\u5c55\u3002", "conclusion": "\u6bd4\u8d5b\u7ed3\u679c\u9884\u8ba1\u5c06\u5f71\u54cd\u56fe\u50cf\u6062\u590d\u548c\u591c\u95f4\u81ea\u52a8\u9a7e\u9a76\u7b49\u591a\u4e2a\u9886\u57df\u3002"}}
{"id": "2510.06631", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06631", "abs": "https://arxiv.org/abs/2510.06631", "authors": ["Qiming Guo", "Bishal Khatri", "Hua Zhang", "Wenlu Wang"], "title": "AI-Driven Forecasting and Monitoring of Urban Water System", "comment": null, "summary": "Underground water and wastewater pipelines are vital for city operations but\nplagued by anomalies like leaks and infiltrations, causing substantial water\nloss, environmental damage, and high repair costs. Conventional manual\ninspections lack efficiency, while dense sensor deployments are prohibitively\nexpensive. In recent years, artificial intelligence has advanced rapidly and is\nincreasingly applied to urban infrastructure. In this research, we propose an\nintegrated AI and remote-sensor framework to address the challenge of leak\ndetection in underground water pipelines, through deploying a sparse set of\nremote sensors to capture real-time flow and depth data, paired with HydroNet -\na dedicated model utilizing pipeline attributes (e.g., material, diameter,\nslope) in a directed graph for higher-precision modeling. Evaluations on a\nreal-world campus wastewater network dataset demonstrate that our system\ncollects effective spatio-temporal hydraulic data, enabling HydroNet to\noutperform advanced baselines. This integration of edge-aware message passing\nwith hydraulic simulations enables accurate network-wide predictions from\nlimited sensor deployments. We envision that this approach can be effectively\nextended to a wide range of underground water pipeline networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u4e86AI\u548c\u9065\u611f\u5668\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u5730\u4e0b\u6c34\u7ba1\u9053\u7684\u6cc4\u6f0f\u3002", "motivation": "\u4f20\u7edf\u4eba\u5de5\u68c0\u6d4b\u6548\u7387\u4f4e\uff0c\u5bc6\u96c6\u4f20\u611f\u5668\u90e8\u7f72\u6210\u672c\u9ad8\uff0c\u800c\u5730\u4e0b\u6c34\u548c\u5e9f\u6c34\u7ba1\u9053\u5bf9\u4e8e\u57ce\u5e02\u8fd0\u8425\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5bb9\u6613\u51fa\u73b0\u6cc4\u6f0f\u548c\u6e17\u900f\u7b49\u5f02\u5e38\u60c5\u51b5\uff0c\u5bfc\u81f4\u5927\u91cf\u7684\u6c34\u6d41\u5931\u3001\u73af\u5883\u7834\u574f\u548c\u9ad8\u6602\u7684\u7ef4\u4fee\u6210\u672c\u3002", "method": "\u901a\u8fc7\u90e8\u7f72\u7a00\u758f\u7684\u9065\u611f\u5668\u6765\u6355\u83b7\u5b9e\u65f6\u6d41\u91cf\u548c\u6df1\u5ea6\u6570\u636e\uff0c\u5e76\u7ed3\u5408HydroNet\uff08\u4e00\u4e2a\u5229\u7528\u7ba1\u9053\u5c5e\u6027\u7684\u6709\u5411\u56fe\u6a21\u578b\uff09\u6765\u5b9e\u73b0\u66f4\u9ad8\u7cbe\u5ea6\u7684\u5efa\u6a21\u3002", "result": "\u5728\u771f\u5b9e\u6821\u56ed\u5e9f\u6c34\u7f51\u7edc\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u6536\u96c6\u4e86\u6709\u6548\u7684\u65f6\u7a7a\u6c34\u529b\u6570\u636e\uff0c\u4f7fHydroNet\u4f18\u4e8e\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8fb9\u7f18\u611f\u77e5\u6d88\u606f\u4f20\u9012\u4e0e\u6c34\u529b\u6a21\u62df\u7684\u96c6\u6210\uff0c\u80fd\u591f\u4ece\u6709\u9650\u7684\u4f20\u611f\u5668\u90e8\u7f72\u4e2d\u5b9e\u73b0\u51c6\u786e\u7684\u5168\u7f51\u7edc\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u6269\u5c55\u5230\u5404\u79cd\u5730\u4e0b\u6c34\u7ba1\u9053\u7f51\u7edc\u3002"}}
{"id": "2510.06640", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06640", "abs": "https://arxiv.org/abs/2510.06640", "authors": ["Nhat M. Hoang", "Do Xuan Long", "Cong-Duy Nguyen", "Min-Yen Kan", "Luu Anh Tuan"], "title": "A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures", "comment": null, "summary": "State Space Models (SSMs) have recently emerged as efficient alternatives to\nTransformer-Based Models (TBMs) for long-sequence processing, offering linear\nscaling and lower memory use. Yet, how contextual information flows across\nlayers and tokens in these architectures remains understudied. We present the\nfirst unified, token- and layer-level analysis of representation propagation in\nSSMs and TBMs. Using centered kernel alignment, stability metrics, and probing,\nwe characterize how representations evolve within and across layers. We find a\nkey divergence: TBMs rapidly homogenize token representations, with diversity\nreemerging only in later layers, while SSMs preserve token uniqueness early but\nconverge to homogenization deeper. Theoretical analysis and parameter\nrandomization further reveal that oversmoothing in TBMs stems from\narchitectural design, whereas in SSMs it arises mainly from training dynamics.\nThese insights clarify the inductive biases of both architectures and inform\nfuture model and training designs for long-context reasoning.", "AI": {"tldr": "\u672c\u6587\u5bf9SSM\u548cTBM\u5728\u957f\u5e8f\u5217\u5904\u7406\u4e2d\u7684\u8868\u5f81\u4f20\u64ad\u8fdb\u884c\u4e86token\u548clayer\u7ea7\u522b\u7684\u7edf\u4e00\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u4fe1\u606f\u6d41\u52a8\u4e0a\u7684\u5173\u952e\u5dee\u5f02\u3002", "motivation": "\u5c3d\u7ba1SSM\u5728\u957f\u5e8f\u5217\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff0c\u4f46\u5bf9\u5176\u5185\u90e8\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u6d41\u52a8\u65b9\u5f0f\u5c1a\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u4e2d\u5fc3\u6838\u5bf9\u9f50\u3001\u7a33\u5b9a\u6027\u6307\u6807\u548c\u63a2\u9488\u6280\u672f\uff0c\u6765\u63cf\u8ff0\u8868\u5f81\u5728\u5c42\u5185\u548c\u5c42\u95f4\u7684\u6f14\u53d8\u3002", "result": "TBM\u8fc5\u901f\u540c\u8d28\u5316token\u8868\u5f81\uff0c\u591a\u6837\u6027\u4ec5\u5728\u540e\u671f\u5c42\u91cd\u65b0\u51fa\u73b0\uff0c\u800cSSM\u5728\u65e9\u671f\u4fdd\u6301token\u72ec\u7279\u6027\uff0c\u4f46\u66f4\u6df1\u5c42\u8d8b\u4e8e\u540c\u8d28\u5316\u3002TBM\u7684\u8fc7\u5ea6\u5e73\u6ed1\u6e90\u4e8e\u67b6\u6784\u8bbe\u8ba1\uff0c\u800cSSM\u7684\u8fc7\u5ea6\u5e73\u6ed1\u4e3b\u8981\u6765\u81ea\u8bad\u7ec3\u52a8\u6001\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u9610\u660e\u4e86\u4e24\u79cd\u67b6\u6784\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u5e76\u4e3a\u672a\u6765\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u6a21\u578b\u548c\u8bad\u7ec3\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4fe1\u606f\u3002"}}
{"id": "2510.06611", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06611", "abs": "https://arxiv.org/abs/2510.06611", "authors": ["Jingran Xu", "Yuanyuan Liu", "Yanjie Zhu"], "title": "Self-supervised Physics-guided Model with Implicit Representation Regularization for Fast MRI Reconstruction", "comment": null, "summary": "Magnetic Resonance Imaging (MRI) is a vital clinical diagnostic tool, yet its\nwidespread application is limited by prolonged scan times. Fast MRI\nreconstruction techniques effectively reduce acquisition duration by\nreconstructing high-fidelity MR images from undersampled k-space data. In\nrecent years, deep learning-based methods have demonstrated remarkable progress\nin this field, with self-supervised and unsupervised learning approaches\nproving particularly valuable in scenarios where fully sampled data are\ndifficult to obtain. This paper proposes a novel zero-shot self-supervised\nreconstruction framework named UnrollINR, which enables scan-specific MRI\nreconstruction without relying on external training data. The method adopts a\nphysics-guided unrolled iterative reconstruction architecture and introduces\nImplicit Neural Representation (INR) as a regularization prior to effectively\nconstrain the solution space. By combining a deep unrolled structure with the\npowerful implicit representation capability of INR, the model's\ninterpretability and reconstruction performance are enhanced. Experimental\nresults demonstrate that even at a high acceleration rate of 10, UnrollINR\nachieves superior reconstruction performance compared to the supervised\nlearning method, validating the superiority of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUnrollINR\u7684\u65b0\u578b\u96f6\u6837\u672c\u81ea\u76d1\u7763\u91cd\u5efa\u6846\u67b6\uff0c\u7528\u4e8e\u5feb\u901fMRI\u91cd\u5efa\uff0c\u65e0\u9700\u5916\u90e8\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u5feb\u901fMRI\u91cd\u5efa\u6280\u672f\u53ef\u4ee5\u901a\u8fc7\u4ece\u6b20\u91c7\u6837\u7684k\u7a7a\u95f4\u6570\u636e\u91cd\u5efa\u9ad8\u4fdd\u771fMR\u56fe\u50cf\u6765\u6709\u6548\u51cf\u5c11\u91c7\u96c6\u65f6\u95f4\u3002\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u81ea\u76d1\u7763\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u96be\u4ee5\u83b7\u5f97\u5b8c\u5168\u91c7\u6837\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u663e\u793a\u51fa\u663e\u8457\u7684\u4ef7\u503c\u3002", "method": "\u91c7\u7528\u7269\u7406\u5f15\u5bfc\u7684\u5c55\u5f00\u8fed\u4ee3\u91cd\u5efa\u67b6\u6784\uff0c\u5e76\u5f15\u5165\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u4f5c\u4e3a\u6b63\u5219\u5316\u5148\u9a8c\uff0c\u4ee5\u6709\u6548\u7ea6\u675f\u89e3\u7a7a\u95f4\u3002", "result": "\u5373\u4f7f\u572810\u500d\u7684\u9ad8\u52a0\u901f\u7387\u4e0b\uff0cUnrollINR\u4e5f\u80fd\u5b9e\u73b0\u4f18\u4e8e\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u7684\u91cd\u5efa\u6027\u80fd\u3002", "conclusion": "UnrollINR\u5728\u5feb\u901fMRI\u91cd\u5efa\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2510.06632", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.06632", "abs": "https://arxiv.org/abs/2510.06632", "authors": ["Yasaman Torabi", "Shahram Shirani", "James P. Reilly"], "title": "Chem-NMF: Multi-layer $\u03b1$-divergence Non-Negative Matrix Factorization for Cardiorespiratory Disease Clustering, with Improved Convergence Inspired by Chemical Catalysts and Rigorous Asymptotic Analysis", "comment": null, "summary": "Non-Negative Matrix Factorization (NMF) is an unsupervised learning method\noffering low-rank representations across various domains such as audio\nprocessing, biomedical signal analysis, and image recognition. The\nincorporation of $\\alpha$-divergence in NMF formulations enhances flexibility\nin optimization, yet extending these methods to multi-layer architectures\npresents challenges in ensuring convergence. To address this, we introduce a\nnovel approach inspired by the Boltzmann probability of the energy barriers in\nchemical reactions to theoretically perform convergence analysis. We introduce\na novel method, called Chem-NMF, with a bounding factor which stabilizes\nconvergence. To our knowledge, this is the first study to apply a physical\nchemistry perspective to rigorously analyze the convergence behaviour of the\nNMF algorithm. We start from mathematically proven asymptotic convergence\nresults and then show how they apply to real data. Experimental results\ndemonstrate that the proposed algorithm improves clustering accuracy by 5.6%\n$\\pm$ 2.7% on biomedical signals and 11.1% $\\pm$ 7.2% on face images (mean\n$\\pm$ std).", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u975e\u8d1f\u77e9\u9635\u5206\u89e3 (NMF) \u65b9\u6cd5\uff0c\u79f0\u4e3a Chem-NMF\uff0c\u5b83\u4f7f\u7528\u6765\u81ea\u7269\u7406\u5316\u5b66\u7684\u7075\u611f\u6765\u7a33\u5b9a\u6536\u655b\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e \u03b1-\u6563\u5ea6\u7684 NMF \u65b9\u6cd5\u5728\u6269\u5c55\u5230\u591a\u5c42\u67b6\u6784\u65f6\uff0c\u6536\u655b\u6027\u5b58\u5728\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u4e00\u4e2a\u53d7\u5316\u5b66\u53cd\u5e94\u4e2d\u80fd\u91cf\u5792\u7684\u73bb\u5c14\u5179\u66fc\u6982\u7387\u542f\u53d1\u7684\u8fb9\u754c\u56e0\u5b50\uff0c\u4ee5\u7a33\u5b9a\u6536\u655b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u751f\u7269\u533b\u5b66\u4fe1\u53f7\u4e0a\u7684\u805a\u7c7b\u7cbe\u5ea6\u63d0\u9ad8\u4e86 5.6% \u00b1 2.7%\uff0c\u5728\u4eba\u8138\u56fe\u50cf\u4e0a\u7684\u805a\u7c7b\u7cbe\u5ea6\u63d0\u9ad8\u4e86 11.1% \u00b1 7.2%\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5c06\u7269\u7406\u5316\u5b66\u7684\u89c6\u89d2\u5e94\u7528\u4e8e\u4e25\u683c\u5206\u6790 NMF \u7b97\u6cd5\u7684\u6536\u655b\u884c\u4e3a\uff0c\u5e76\u8bc1\u660e\u4e86 Chem-NMF \u5728\u805a\u7c7b\u7cbe\u5ea6\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.06652", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06652", "abs": "https://arxiv.org/abs/2510.06652", "authors": ["Shangjian Yin", "Zhepei Wei", "Xinyu Zhu", "Wei-Lin Chen", "Yu Meng"], "title": "Aligning Large Language Models via Fully Self-Synthetic Data", "comment": null, "summary": "Traditional reinforcement learning from human feedback (RLHF) for large\nlanguage models (LLMs) relies on expensive human-annotated datasets, while\nReinforcement Learning from AI Feedback (RLAIF) also incurs significant costs,\nrequiring the collection of diverse prompts and corresponding responses, often\nnecessitating external reward models or proprietary models like GPT-4 to\nannotate preference pairs. In this work, we introduce Self-Alignment\nOptimization (SAO), a fully self-synthetic framework for LLM alignment, where\nall training data, including prompts (i.e., user queries), responses, and\npreferences, are generated by the model itself. Specifically, SAO first\ninstructs the LLM to engage in persona role-play and generate diverse prompts\nand responses, which are then self-evaluated for preference optimization.\nExtensive experiments demonstrate that SAO effectively enhances the model's\nchat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining\nstrong performance on downstream objective tasks (e.g., question-answering,\nmath reasoning). Our work provides a practical solution for self-improvement in\naligning LLMs, and the code for reproducing our results is available at:\nhttps://github.com/SJY8460/SAO.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u81ea\u5bf9\u9f50\u4f18\u5316 (SAO) \u7684\u5168\u81ea\u5408\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5bf9\u9f50\uff0c\u5176\u4e2d\u6240\u6709\u8bad\u7ec3\u6570\u636e\u5747\u7531\u6a21\u578b\u81ea\u8eab\u751f\u6210\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u5de5\u53cd\u9988\u5f3a\u5316\u5b66\u4e60 (RLHF) \u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u800c\u6765\u81ea AI \u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60 (RLAIF) \u4e5f\u9700\u8981\u5927\u91cf\u6210\u672c\uff0c\u9700\u8981\u6536\u96c6\u5404\u79cd\u63d0\u793a\u548c\u76f8\u5e94\u7684\u54cd\u5e94\uff0c\u901a\u5e38\u9700\u8981\u5916\u90e8\u5956\u52b1\u6a21\u578b\u6216\u4e13\u6709\u6a21\u578b\uff08\u5982 GPT-4\uff09\u6765\u6ce8\u91ca\u504f\u597d\u5bf9\u3002", "method": "SAO \u9996\u5148\u6307\u793a LLM \u8fdb\u884c\u89d2\u8272\u626e\u6f14\u5e76\u751f\u6210\u5404\u79cd\u63d0\u793a\u548c\u54cd\u5e94\uff0c\u7136\u540e\u5bf9\u8fd9\u4e9b\u63d0\u793a\u548c\u54cd\u5e94\u8fdb\u884c\u81ea\u6211\u8bc4\u4f30\u4ee5\u8fdb\u884c\u504f\u597d\u4f18\u5316\u3002", "result": "SAO \u6709\u6548\u5730\u589e\u5f3a\u4e86\u6a21\u578b\u5728 AlpacaEval~2.0 \u7b49\u6807\u51c6\u57fa\u51c6\u4e0a\u7684\u804a\u5929\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5728\u4e0b\u6e38\u76ee\u6807\u4efb\u52a1\uff08\u4f8b\u5982\uff0c\u95ee\u7b54\u3001\u6570\u5b66\u63a8\u7406\uff09\u65b9\u9762\u7684\u5f3a\u5927\u6027\u80fd\u3002", "conclusion": "SAO \u4e3a LLM \u5bf9\u9f50\u7684\u81ea\u6211\u6539\u8fdb\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.06612", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06612", "abs": "https://arxiv.org/abs/2510.06612", "authors": ["Zibo Su", "Kun Wei", "Jiahua Li", "Xu Yang", "Cheng Deng"], "title": "A Bridge from Audio to Video: Phoneme-Viseme Alignment Allows Every Face to Speak Multiple Languages", "comment": null, "summary": "Speech-driven talking face synthesis (TFS) focuses on generating lifelike\nfacial animations from audio input. Current TFS models perform well in English\nbut unsatisfactorily in non-English languages, producing wrong mouth shapes and\nrigid facial expressions. The terrible performance is caused by the\nEnglish-dominated training datasets and the lack of cross-language\ngeneralization abilities. Thus, we propose Multilingual Experts (MuEx), a novel\nframework featuring a Phoneme-Guided Mixture-of-Experts (PG-MoE) architecture\nthat employs phonemes and visemes as universal intermediaries to bridge audio\nand video modalities, achieving lifelike multilingual TFS. To alleviate the\ninfluence of linguistic differences and dataset bias, we extract audio and\nvideo features as phonemes and visemes respectively, which are the basic units\nof speech sounds and mouth movements. To address audiovisual synchronization\nissues, we introduce the Phoneme-Viseme Alignment Mechanism (PV-Align), which\nestablishes robust cross-modal correspondences between phonemes and visemes. In\naddition, we build a Multilingual Talking Face Benchmark (MTFB) comprising 12\ndiverse languages with 95.04 hours of high-quality videos for training and\nevaluating multilingual TFS performance. Extensive experiments demonstrate that\nMuEx achieves superior performance across all languages in MTFB and exhibits\neffective zero-shot generalization to unseen languages without additional\ntraining.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMultilingual Experts (MuEx) \u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u903c\u771f\u7684\u591a\u8bed\u8a00\u8bf4\u8bdd\u4eba\u8138\u52a8\u753b\u3002", "motivation": "\u73b0\u6709\u7684\u8bf4\u8bdd\u4eba\u8138\u5408\u6210\u6a21\u578b\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4ea7\u751f\u9519\u8bef\u7684\u53e3\u578b\u548c\u50f5\u786c\u7684\u9762\u90e8\u8868\u60c5\uff0c\u8fd9\u662f\u7531\u4e8e\u4ee5\u82f1\u8bed\u4e3a\u4e3b\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u7f3a\u4e4f\u8de8\u8bed\u8a00\u7684\u6cdb\u5316\u80fd\u529b\u9020\u6210\u7684\u3002", "method": "\u8be5\u6846\u67b6\u91c7\u7528Phoneme-Guided Mixture-of-Experts (PG-MoE) \u67b6\u6784\uff0c\u5229\u7528\u97f3\u7d20\u548cviseme\u4f5c\u4e3a\u901a\u7528\u5a92\u4ecb\u6765\u6865\u63a5\u97f3\u9891\u548c\u89c6\u9891\u6a21\u6001\u3002\u901a\u8fc7\u63d0\u53d6\u97f3\u7d20\u548cviseme\u4f5c\u4e3a\u97f3\u9891\u548c\u89c6\u9891\u7279\u5f81\uff0c\u5e76\u5f15\u5165Phoneme-Viseme Alignment Mechanism (PV-Align) \u6765\u89e3\u51b3\u89c6\u542c\u540c\u6b65\u95ee\u9898\u3002\u6b64\u5916\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b12\u79cd\u4e0d\u540c\u8bed\u8a00\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u7684\u591a\u8bed\u8a00\u8bf4\u8bdd\u4eba\u8138\u57fa\u51c6 (MTFB)\u3002", "result": "MuEx\u5728MTFB\u7684\u6240\u6709\u8bed\u8a00\u4e2d\u90fd\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u5e76\u8868\u73b0\u51fa\u5bf9\u672a\u89c1\u8bed\u8a00\u7684\u6709\u6548\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "MuEx\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u591a\u8bed\u8a00\u8bf4\u8bdd\u4eba\u8138\u5408\u6210\u7684\u6027\u80fd\uff0c\u5e76\u5728\u8de8\u8bed\u8a00\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.06634", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06634", "abs": "https://arxiv.org/abs/2510.06634", "authors": ["Shiye Su", "Yuhui Zhang", "Linqi Zhou", "Rajesh Ranganath", "Serena Yeung-Levy"], "title": "Three Forms of Stochastic Injection for Improved Distribution-to-Distribution Generative Modeling", "comment": null, "summary": "Modeling transformations between arbitrary data distributions is a\nfundamental scientific challenge, arising in applications like drug discovery\nand evolutionary simulation. While flow matching offers a natural framework for\nthis task, its use has thus far primarily focused on the noise-to-data setting,\nwhile its application in the general distribution-to-distribution setting is\nunderexplored. We find that in the latter case, where the source is also a data\ndistribution to be learned from limited samples, standard flow matching fails\ndue to sparse supervision. To address this, we propose a simple and\ncomputationally efficient method that injects stochasticity into the training\nprocess by perturbing source samples and flow interpolants. On five diverse\nimaging tasks spanning biology, radiology, and astronomy, our method\nsignificantly improves generation quality, outperforming existing baselines by\nan average of 9 FID points. Our approach also reduces the transport cost\nbetween input and generated samples to better highlight the true effect of the\ntransformation, making flow matching a more practical tool for simulating the\ndiverse distribution transformations that arise in science.", "AI": {"tldr": "Flow matching\u5728\u4e00\u822c\u5206\u5e03\u5230\u5206\u5e03\u7684\u8f6c\u6362\u4e2d\u7531\u4e8e\u7a00\u758f\u76d1\u7763\u800c\u5931\u6548\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u968f\u673a\u6027\u6765\u89e3\u51b3\uff0c\u5728\u4e94\u4e2a\u4e0d\u540c\u7684\u6210\u50cf\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5728\u836f\u7269\u53d1\u73b0\u548c\u8fdb\u5316\u6a21\u62df\u7b49\u5e94\u7528\u4e2d\uff0c\u5bf9\u4efb\u610f\u6570\u636e\u5206\u5e03\u4e4b\u95f4\u7684\u8f6c\u6362\u8fdb\u884c\u5efa\u6a21\u662f\u4e00\u4e2a\u6839\u672c\u7684\u79d1\u5b66\u6311\u6218\u3002Flow matching\u4e3a\u8fd9\u9879\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u81ea\u7136\u7684\u6846\u67b6\uff0c\u4f46\u5176\u5728\u4e00\u822c\u5206\u5e03\u5230\u5206\u5e03\u8bbe\u7f6e\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6270\u52a8\u6e90\u6837\u672c\u548c\u6d41\u63d2\u503c\u5668\u5c06\u968f\u673a\u6027\u6ce8\u5165\u5230\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u3002", "result": "\u5728\u8de8\u8d8a\u751f\u7269\u5b66\u3001\u653e\u5c04\u5b66\u548c\u5929\u6587\u5b66\u7684\u4e94\u4e2a\u4e0d\u540c\u7684\u6210\u50cf\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u6bd4\u73b0\u6709\u57fa\u7ebf\u5e73\u5747\u9ad8\u51fa 9 \u4e2a FID \u70b9\u3002\u8be5\u65b9\u6cd5\u8fd8\u964d\u4f4e\u4e86\u8f93\u5165\u548c\u751f\u6210\u7684\u6837\u672c\u4e4b\u95f4\u7684\u4f20\u8f93\u6210\u672c\uff0c\u4ece\u800c\u66f4\u597d\u5730\u7a81\u51fa\u4e86\u8f6c\u6362\u7684\u771f\u5b9e\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f7f flow matching \u6210\u4e3a\u6a21\u62df\u79d1\u5b66\u4e2d\u51fa\u73b0\u7684\u5404\u79cd\u5206\u5e03\u8f6c\u6362\u7684\u66f4\u5b9e\u7528\u7684\u5de5\u5177\u3002"}}
{"id": "2510.06664", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06664", "abs": "https://arxiv.org/abs/2510.06664", "authors": ["Yunzhong Xiao", "Yangmin Li", "Hewei Wang", "Yunlong Tang", "Zora Zhiruo Wang"], "title": "ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory", "comment": null, "summary": "Agents utilizing tools powered by large language models (LLMs) or\nvision-language models (VLMs) have demonstrated remarkable progress in diverse\ntasks across text and visual modalities. Unlike traditional tools such as\ncalculators, which give deterministic outputs, neural tools perform uncertainly\nacross task scenarios. While different tools for a task may excel in varied\nscenarios, existing agents typically rely on fixed tools, thus limiting the\nflexibility in selecting the most suitable tool for specific tasks. In\ncontrast, humans snowball their understanding of the capabilities of different\ntools by interacting with them, and apply this knowledge to select the optimal\ntool when solving a future task. To build agents that similarly benefit from\nthis process, we propose ToolMem that enables agents to develop memories of\ntool capabilities from previous interactions, by summarizing their strengths\nand weaknesses and storing them in memory; at inference, the agent can retrieve\nrelevant entries from ToolMem, and select the best tool to solve individual\ntasks more accurately. We evaluate ToolMem on learning varied text generation\nand text-to-image generation neural tools. Compared to no-memory, generic\nagents, we find ToolMem-augmented agents predict tool performance 14.8% and\n28.7% more accurately across text and multimodal generation scenarios.\nMoreover, ToolMem facilitates optimal tool selection among multiple choices by\n21% and 24% absolute increases in respective scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a ToolMem \u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f agents \u80fd\u591f\u901a\u8fc7\u603b\u7ed3\u4e0d\u540c\u5de5\u5177\u7684\u4f18\u7f3a\u70b9\u5e76\u5c06\u5176\u5b58\u50a8\u5728\u5185\u5b58\u4e2d\uff0c\u4ece\u800c\u5f00\u53d1\u5de5\u5177\u80fd\u529b\u7684\u8bb0\u5fc6\u3002\u5728\u63a8\u7406\u65f6\uff0cagent \u53ef\u4ee5\u4ece ToolMem \u4e2d\u68c0\u7d22\u76f8\u5173\u6761\u76ee\uff0c\u5e76\u9009\u62e9\u6700\u4f73\u5de5\u5177\u6765\u66f4\u51c6\u786e\u5730\u89e3\u51b3\u5355\u4e2a\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709agents\u901a\u5e38\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u5de5\u5177\uff0c\u9650\u5236\u4e86\u4e3a\u7279\u5b9a\u4efb\u52a1\u9009\u62e9\u6700\u5408\u9002\u5de5\u5177\u7684\u7075\u6d3b\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u6784\u5efa\u80fd\u591f\u4ece\u4e0e\u5de5\u5177\u7684\u5148\u524d\u4ea4\u4e92\u4e2d\u83b7\u76ca\u7684agents\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86 ToolMem\uff0c\u5b83\u5141\u8bb8agents\u901a\u8fc7\u603b\u7ed3\u5de5\u5177\u7684\u4f18\u7f3a\u70b9\u5e76\u5c06\u5176\u5b58\u50a8\u5728\u5185\u5b58\u4e2d\u6765\u5f00\u53d1\u5de5\u5177\u80fd\u529b\u7684\u8bb0\u5fc6\u3002\u5728\u63a8\u7406\u65f6\uff0cagent \u53ef\u4ee5\u4ece ToolMem \u4e2d\u68c0\u7d22\u76f8\u5173\u6761\u76ee\uff0c\u5e76\u9009\u62e9\u6700\u4f73\u5de5\u5177\u6765\u66f4\u51c6\u786e\u5730\u89e3\u51b3\u5355\u4e2a\u4efb\u52a1\u3002", "result": "\u4e0e\u65e0\u8bb0\u5fc6\u7684\u901a\u7528agents\u76f8\u6bd4\uff0c\u4f7f\u7528 ToolMem \u7684agents\u5728\u6587\u672c\u548c\u591a\u6a21\u6001\u751f\u6210\u573a\u666f\u4e2d\u9884\u6d4b\u5de5\u5177\u6027\u80fd\u7684\u51c6\u786e\u7387\u5206\u522b\u63d0\u9ad8\u4e86 14.8% \u548c 28.7%\u3002\u6b64\u5916\uff0cToolMem \u5728\u5404\u79cd\u573a\u666f\u4e2d\u5206\u522b\u5b9e\u73b0\u4e86 21% \u548c 24% \u7684\u7edd\u5bf9\u589e\u957f\uff0c\u4ece\u800c\u4fc3\u8fdb\u4e86\u591a\u4e2a\u9009\u62e9\u4e2d\u7684\u6700\u4f73\u5de5\u5177\u9009\u62e9\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a ToolMem \u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7fagents\u80fd\u591f\u901a\u8fc7\u603b\u7ed3\u4e0d\u540c\u5de5\u5177\u7684\u4f18\u7f3a\u70b9\u5e76\u5c06\u5176\u5b58\u50a8\u5728\u5185\u5b58\u4e2d\uff0c\u4ece\u800c\u5f00\u53d1\u5de5\u5177\u80fd\u529b\u7684\u8bb0\u5fc6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cToolMem \u53ef\u4ee5\u63d0\u9ad8agents\u9884\u6d4b\u5de5\u5177\u6027\u80fd\u7684\u51c6\u786e\u7387\uff0c\u5e76\u4fc3\u8fdb\u6700\u4f73\u5de5\u5177\u7684\u9009\u62e9\u3002"}}
{"id": "2510.06619", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06619", "abs": "https://arxiv.org/abs/2510.06619", "authors": ["Tao Feng", "Tingfa Xu", "Haolin Qin", "Tianhao Li", "Shuaihao Han", "Xuyang Zou", "Zhan Lv", "Jianan Li"], "title": "MSITrack: A Challenging Benchmark for Multispectral Single Object Tracking", "comment": null, "summary": "Visual object tracking in real-world scenarios presents numerous challenges\nincluding occlusion, interference from similar objects and complex\nbackgrounds-all of which limit the effectiveness of RGB-based trackers.\nMultispectral imagery, which captures pixel-level spectral reflectance,\nenhances target discriminability. However, the availability of multispectral\ntracking datasets remains limited. To bridge this gap, we introduce MSITrack,\nthe largest and most diverse multispectral single object tracking dataset to\ndate. MSITrack offers the following key features: (i) More Challenging\nAttributes-including interference from similar objects and similarity in color\nand texture between targets and backgrounds in natural scenarios, along with a\nwide range of real-world tracking challenges; (ii) Richer and More Natural\nScenes-spanning 55 object categories and 300 distinct natural scenes, MSITrack\nfar exceeds the scope of existing benchmarks. Many of these scenes and\ncategories are introduced to the multispectral tracking domain for the first\ntime; (iii) Larger Scale-300 videos comprising over 129k frames of\nmultispectral imagery. To ensure annotation precision, each frame has undergone\nmeticulous processing, manual labeling and multi-stage verification. Extensive\nevaluations using representative trackers demonstrate that the multispectral\ndata in MSITrack significantly improves performance over RGB-only baselines,\nhighlighting its potential to drive future advancements in the field. The\nMSITrack dataset is publicly available at:\nhttps://github.com/Fengtao191/MSITrack.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMSITrack\u7684\u5927\u578b\u591a\u5149\u8c31\u5355\u76ee\u6807\u8ddf\u8e2a\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63d0\u5347\u590d\u6742\u573a\u666f\u4e0b\u7684\u76ee\u6807\u533a\u5206\u5ea6\uff0c\u5e76\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "motivation": "\u57fa\u4e8eRGB\u7684\u8ddf\u8e2a\u5668\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u9762\u4e34\u906e\u6321\u3001\u76f8\u4f3c\u7269\u4f53\u5e72\u6270\u548c\u590d\u6742\u80cc\u666f\u7b49\u6311\u6218\uff0c\u800c\u591a\u5149\u8c31\u56fe\u50cf\u80fd\u589e\u5f3a\u76ee\u6807\u533a\u5206\u5ea6\uff0c\u4f46\u591a\u5149\u8c31\u8ddf\u8e2a\u6570\u636e\u96c6\u6709\u9650\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b300\u4e2a\u89c6\u9891\uff0c\u8d85\u8fc712.9\u4e07\u5e27\u591a\u5149\u8c31\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u4e86\u7ec6\u81f4\u7684\u5904\u7406\u3001\u624b\u52a8\u6807\u6ce8\u548c\u591a\u9636\u6bb5\u9a8c\u8bc1\u3002", "result": "\u901a\u8fc7\u4ee3\u8868\u6027\u8ddf\u8e2a\u5668\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cMSITrack\u4e2d\u7684\u591a\u5149\u8c31\u6570\u636e\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "conclusion": "MSITrack\u6570\u636e\u96c6\u7684\u53d1\u5e03\uff0c\u80fd\u591f\u63a8\u52a8\u591a\u5149\u8c31\u5355\u76ee\u6807\u8ddf\u8e2a\u9886\u57df\u672a\u6765\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.06635", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06635", "abs": "https://arxiv.org/abs/2510.06635", "authors": ["Yunpeng Gong", "Sihan Lan", "Can Yang", "Kunpeng Xu", "Min Jiang"], "title": "StruSR: Structure-Aware Symbolic Regression with Physics-Informed Taylor Guidance", "comment": null, "summary": "Symbolic regression aims to find interpretable analytical expressions by\nsearching over mathematical formula spaces to capture underlying system\nbehavior, particularly in scientific modeling governed by physical laws.\nHowever, traditional methods lack mechanisms for extracting structured physical\npriors from time series observations, making it difficult to capture symbolic\nexpressions that reflect the system's global behavior. In this work, we propose\na structure-aware symbolic regression framework, called StruSR, that leverages\ntrained Physics-Informed Neural Networks (PINNs) to extract locally structured\nphysical priors from time series data. By performing local Taylor expansions on\nthe outputs of the trained PINN, we obtain derivative-based structural\ninformation to guide symbolic expression evolution. To assess the importance of\nexpression components, we introduce a masking-based attribution mechanism that\nquantifies each subtree's contribution to structural alignment and physical\nresidual reduction. These sensitivity scores steer mutation and crossover\noperations within genetic programming, preserving substructures with high\nphysical or structural significance while selectively modifying less\ninformative components. A hybrid fitness function jointly minimizes physics\nresiduals and Taylor coefficient mismatch, ensuring consistency with both the\ngoverning equations and the local analytical behavior encoded by the PINN.\nExperiments on benchmark PDE systems demonstrate that StruSR improves\nconvergence speed, structural fidelity, and expression interpretability\ncompared to conventional baselines, offering a principled paradigm for\nphysics-grounded symbolic discovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u611f\u77e5\u7684\u7b26\u53f7\u56de\u5f52\u6846\u67b6 (StruSR)\uff0c\u5229\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc (PINN) \u4ece\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u63d0\u53d6\u5c40\u90e8\u7ed3\u6784\u5316\u7684\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ee5\u6307\u5bfc\u7b26\u53f7\u8868\u8fbe\u5f0f\u7684\u6f14\u5316\u3002", "motivation": "\u4f20\u7edf\u7b26\u53f7\u56de\u5f52\u65b9\u6cd5\u7f3a\u4e4f\u4ece\u65f6\u95f4\u5e8f\u5217\u89c2\u6d4b\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u7269\u7406\u5148\u9a8c\u7684\u673a\u5236\uff0c\u96be\u4ee5\u6355\u6349\u53cd\u6620\u7cfb\u7edf\u5168\u5c40\u884c\u4e3a\u7684\u7b26\u53f7\u8868\u8fbe\u5f0f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5229\u7528\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\u7684\u7b26\u53f7\u56de\u5f52\u65b9\u6cd5\u3002", "method": "1. \u5229\u7528\u8bad\u7ec3\u597d\u7684 PINN \u8fdb\u884c\u5c40\u90e8\u6cf0\u52d2\u5c55\u5f00\uff0c\u83b7\u5f97\u57fa\u4e8e\u5bfc\u6570\u7684\u7ed3\u6784\u4fe1\u606f\u6765\u6307\u5bfc\u7b26\u53f7\u8868\u8fbe\u5f0f\u7684\u6f14\u5316\u3002\n2. \u5f15\u5165\u57fa\u4e8e\u63a9\u7801\u7684\u5f52\u56e0\u673a\u5236\uff0c\u91cf\u5316\u6bcf\u4e2a\u5b50\u6811\u5bf9\u7ed3\u6784\u5bf9\u9f50\u548c\u7269\u7406\u6b8b\u5dee\u51cf\u5c11\u7684\u8d21\u732e\u3002\n3. \u6df7\u5408\u9002\u5e94\u5ea6\u51fd\u6570\uff0c\u5171\u540c\u6700\u5c0f\u5316\u7269\u7406\u6b8b\u5dee\u548c\u6cf0\u52d2\u7cfb\u6570\u4e0d\u5339\u914d\u3002", "result": "\u5728\u57fa\u51c6 PDE \u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cStruSR \u63d0\u9ad8\u4e86\u6536\u655b\u901f\u5ea6\u3001\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u8868\u8fbe\u5f0f\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "StruSR \u662f\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7684\u7b26\u53f7\u53d1\u73b0\u7684\u6709\u6548\u8303\u4f8b\u3002"}}
{"id": "2510.06670", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06670", "abs": "https://arxiv.org/abs/2510.06670", "authors": ["Shangjian Yin", "Shining Liang", "Wenbiao Ding", "Yuli Qian", "Zhouxing Shi", "Hongzhi Li", "Yutao Xie"], "title": "PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from Scratch", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone\nfor aligning large language models (LLMs). However, its effectiveness depends\non high-quality instruction data. Most existing alignment datasets are either\nprivate or require costly human annotation, which limits reproducibility and\nscalability. Even with Reinforcement Learning from AI Feedback (RLAIF),\nconcerns about data quality remain. Moreover, it is unclear how much data is\nactually required to fine-tune a base model into a strong instruction-following\nmodel. Current approaches often rely on over 300k examples even at the\nsupervised fine-tuning (SFT) stage, yet they still underperform compared to\nproprietary models, creating barriers for academic and resource-limited\ncommunities. To address this gap, we introduce PiKa, a data-efficient family of\nexpert-level alignment datasets. In particular, the PiKa-SFT dataset uses only\n30k SFT examples, far fewer than state-of-the-art datasets like Magpie. Through\nevaluations by fine-tuning Llama-3-8B-Base on PiKa and other public datasets,\nwe show that PiKa-SFT outperforms models trained on much larger data. On\nAlpacaEval 2.0 and Arena-Hard benchmarks, PiKa-SFT fine-tuning even surpasses\nthe official Llama-3-8B-Instruct model trained on over 10 million proprietary\nexamples. We further extend our study by training the Qwen2.5 series (0.5B to\n7B) on PiKa-SFT, achieving consistent gains. These findings demonstrate that\nhigh-quality alignment can be achieved with significantly less data, offering a\nscalable path for open-source LLM alignment. Code and data:\nhttps://github.com/SJY8460/PiKa.", "AI": {"tldr": "PiKa: A data-efficient alignment dataset family, requiring only 30k SFT examples, outperforming models trained on larger datasets.", "motivation": "Current alignment datasets are either private, costly, or require large amounts of data, limiting reproducibility and scalability for academic and resource-limited communities.", "method": "Introduce PiKa-SFT, a data-efficient alignment dataset with only 30k SFT examples. Fine-tune Llama-3-8B-Base and Qwen2.5 series (0.5B to 7B) on PiKa-SFT and evaluate the performance.", "result": "PiKa-SFT outperforms models trained on much larger datasets, even surpassing the official Llama-3-8B-Instruct model. Qwen2.5 series also achieved consistent gains when trained on PiKa-SFT.", "conclusion": "High-quality alignment can be achieved with significantly less data, offering a scalable path for open-source LLM alignment."}}
{"id": "2510.06638", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06638", "abs": "https://arxiv.org/abs/2510.06638", "authors": ["Zhihao Wen", "Wenkang Wei", "Yuan Fang", "Xingtong Yu", "Hui Zhang", "Weicheng Zhu", "Xin Zhang"], "title": "StaR-KVQA: Structured Reasoning Traces for Implicit-Knowledge Visual Question Answering", "comment": null, "summary": "Knowledge-based Visual Question Answering (KVQA) requires models to ground\nentities in images and reason over factual knowledge. We study its\nimplicit-knowledge variant, IK-KVQA, where a multimodal large language model\n(MLLM) is the sole knowledge source, without external retrieval. Yet, MLLMs\nlack explicit reasoning supervision and produce inconsistent justifications,\nand generalize poorly after standard supervised fine-tuning (SFT). We present\nStaR-KVQA (Structured Reasoning Traces for IK-KVQA), which supervises\nstructured traces - dual symbolic relation paths plus path-grounded\nnatural-language explanations - so that reasoning becomes transparent and\nverifiable. With one open-source MLLM, StaR-KVQA constructs and selects\npath-grounded reasoning traces to form a trace-enriched dataset, then\nfine-tunes via structured self-distillation to align generation with\nsupervision; no external retrievers, verifiers, or curated knowledge bases\n(KBs) are used, traces are built offline, and inference is a single\nautoregressive pass. Across benchmarks, StaR-KVQA improves both accuracy and\ninterpretability, achieving up to +11.3% higher answer accuracy on OK-VQA over\nthe strongest baseline while exhibiting robust cross-domain generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStaR-KVQA\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u9690\u5f0f\u77e5\u8bc6\u89c6\u89c9\u95ee\u7b54\uff08IK-KVQA\uff09\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u76d1\u7763\u7ed3\u6784\u5316\u63a8\u7406\u8f68\u8ff9\uff0c\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u89e3\u51b3IK-KVQA\u95ee\u9898\u65f6\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u63a8\u7406\u76d1\u7763\uff0c\u4ea7\u751f\u4e0d\u4e00\u81f4\u7684\u7406\u7531\uff0c\u5e76\u4e14\u5728\u6807\u51c6\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u540e\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6784\u5efa\u548c\u9009\u62e9\u8def\u5f84\u76f8\u5173\u7684\u63a8\u7406\u8f68\u8ff9\u6765\u5f62\u6210\u4e00\u4e2atrace-enriched\u6570\u636e\u96c6\uff0c\u7136\u540e\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u81ea\u6211\u84b8\u998f\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u4f7f\u751f\u6210\u4e0e\u76d1\u7763\u5bf9\u9f50\u3002\u6574\u4e2a\u8fc7\u7a0b\u4e0d\u4f7f\u7528\u5916\u90e8\u68c0\u7d22\u5668\u3001\u9a8c\u8bc1\u5668\u6216curated\u77e5\u8bc6\u5e93\uff08KB\uff09\uff0c\u8f68\u8ff9\u662f\u79bb\u7ebf\u6784\u5efa\u7684\uff0c\u63a8\u7406\u662f\u4e00\u4e2a\u5355\u4e00\u7684\u81ea\u56de\u5f52\u8fc7\u7a0b\u3002", "result": "StaR-KVQA\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5728OK-VQA\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe+11.3%\u7684\u7b54\u6848\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5e76\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "StaR-KVQA\u65b9\u6cd5\u901a\u8fc7\u76d1\u7763\u7ed3\u6784\u5316\u63a8\u7406\u8f68\u8ff9\uff0c\u6709\u6548\u5730\u63d0\u9ad8\u4e86MLLM\u5728IK-KVQA\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.06637", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06637", "abs": "https://arxiv.org/abs/2510.06637", "authors": ["Prakhar Srivastava", "Farrin Marouf Sofian", "Francesco Immorlano", "Kushagra Pandey", "Stephan Mandt"], "title": "Control-Augmented Autoregressive Diffusion for Data Assimilation", "comment": null, "summary": "Despite recent advances in test-time scaling and finetuning of diffusion\nmodels, guidance in Auto-Regressive Diffusion Models (ARDMs) remains\nunderexplored. We introduce an amortized framework that augments pretrained\nARDMs with a lightweight controller network, trained offline by previewing\nfuture ARDM rollouts and learning stepwise controls that anticipate upcoming\nobservations under a terminal cost objective. We evaluate this framework in the\ncontext of data assimilation (DA) for chaotic spatiotemporal partial\ndifferential equations (PDEs), a setting where existing methods are often\ncomputationally prohibitive and prone to forecast drift under sparse\nobservations. Our approach reduces DA inference to a single forward rollout\nwith on-the-fly corrections, avoiding expensive adjoint computations and/or\noptimizations during inference. We demonstrate that our method consistently\noutperforms four state-of-the-art baselines in stability, accuracy, and\nphysical fidelity across two canonical PDEs and six observation regimes. We\nwill release code and checkpoints publicly.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\uff08ARDM\uff09\u4e2d\u7684\u6307\u5bfc\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u644a\u9500\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u63a7\u5236\u5668\u7f51\u7edc\u589e\u5f3a\u9884\u8bad\u7ec3\u7684ARDM\uff0c\u4ee5\u9884\u6d4b\u672a\u6765\u7684ARDM rollouts\u5e76\u5b66\u4e60\u9010\u6b65\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u540c\u5316\uff08DA\uff09\u65b9\u6cd5\u5728\u8ba1\u7b97\u4e0a\u8fc7\u4e8e\u6602\u8d35\uff0c\u5e76\u4e14\u5728\u7a00\u758f\u89c2\u6d4b\u4e0b\u5bb9\u6613\u4ea7\u751f\u9884\u6d4b\u6f02\u79fb\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u79bb\u7ebf\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u63a7\u5236\u5668\u7f51\u7edc\uff0c\u5b66\u4e60\u9010\u6b65\u63a7\u5236\uff0c\u4ee5\u9884\u6d4b\u672a\u6765\u7684ARDM rollouts\u3002\u5728\u6570\u636e\u540c\u5316\u63a8\u7406\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5c06\u63a8\u7406\u7b80\u5316\u4e3a\u5355\u6b21\u524d\u5411 rollout\uff0c\u907f\u514d\u4e86\u6602\u8d35\u7684\u4f34\u968f\u8ba1\u7b97\u6216\u4f18\u5316\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u7a33\u5b9a\u3001\u51c6\u786e\u6027\u548c\u7269\u7406\u4fdd\u771f\u5ea6\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u56db\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u6570\u636e\u540c\u5316\u95ee\u9898\uff0c\u5e76\u5728\u8ba1\u7b97\u6548\u7387\u548c\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u63d0\u5347\u3002"}}
{"id": "2510.06677", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06677", "abs": "https://arxiv.org/abs/2510.06677", "authors": ["Yisha Wu", "Cen", "Zhao", "Yuanpei Cao", "Xiaoqing Su", "Yashar Mehdad", "Mindy Ji", "Claire Na Cheng"], "title": "Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback", "comment": "Accepted at EMNLP 2025 Industry Track", "summary": "We introduce an incremental summarization system for customer support agents\nthat intelligently determines when to generate concise bullet notes during\nconversations, reducing agents' context-switching effort and redundant review.\nOur approach combines a fine-tuned Mixtral-8x7B model for continuous note\ngeneration with a DeBERTa-based classifier to filter trivial content. Agent\nedits refine the online notes generation and regularly inform offline model\nretraining, closing the agent edits feedback loop. Deployed in production, our\nsystem achieved a 3% reduction in case handling time compared to bulk\nsummarization (with reductions of up to 9% in highly complex cases), alongside\nhigh agent satisfaction ratings from surveys. These results demonstrate that\nincremental summarization with continuous feedback effectively enhances summary\nquality and agent productivity at scale.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u4e3a\u5ba2\u6237\u652f\u6301\u5750\u5e2d\u8bbe\u8ba1\u7684\u589e\u91cf\u6458\u8981\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u5728\u5bf9\u8bdd\u8fc7\u7a0b\u4e2d\u667a\u80fd\u5730\u751f\u6210\u7b80\u6d01\u7684\u8981\u70b9\u7b14\u8bb0\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u51cf\u5c11\u5ba2\u670d\u4eba\u5458\u7684\u4e0a\u4e0b\u6587\u5207\u6362\u5de5\u4f5c\u548c\u5197\u4f59\u5ba1\u67e5\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u4e00\u4e2a\u5fae\u8c03\u7684 Mixtral-8x7B \u6a21\u578b\uff0c\u7528\u4e8e\u8fde\u7eed\u751f\u6210\u7b14\u8bb0\uff0c\u4ee5\u53ca\u4e00\u4e2a\u57fa\u4e8e DeBERTa \u7684\u5206\u7c7b\u5668\uff0c\u7528\u4e8e\u8fc7\u6ee4\u7410\u788e\u7684\u5185\u5bb9\u3002\u5750\u5e2d\u7684\u7f16\u8f91\u4f1a\u6539\u8fdb\u5728\u7ebf\u7b14\u8bb0\u7684\u751f\u6210\uff0c\u5e76\u5b9a\u671f\u901a\u77e5\u79bb\u7ebf\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\uff0c\u4ece\u800c\u5f62\u6210\u5750\u5e2d\u7f16\u8f91\u53cd\u9988\u95ed\u73af\u3002", "result": "\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72\u540e\uff0c\u4e0e\u6279\u91cf\u6458\u8981\u76f8\u6bd4\uff0c\u6848\u4ef6\u5904\u7406\u65f6\u95f4\u51cf\u5c11\u4e86 3%\uff08\u5728\u9ad8\u5ea6\u590d\u6742\u7684\u6848\u4ef6\u4e2d\u51cf\u5c11\u9ad8\u8fbe 9%\uff09\uff0c\u540c\u65f6\u5750\u5e2d\u6ee1\u610f\u5ea6\u8c03\u67e5\u7684\u8bc4\u5206\u4e5f\u5f88\u9ad8\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u5177\u6709\u6301\u7eed\u53cd\u9988\u7684\u589e\u91cf\u6458\u8981\u53ef\u4ee5\u6709\u6548\u5730\u63d0\u9ad8\u6458\u8981\u8d28\u91cf\u548c\u5927\u89c4\u6a21\u7684\u5750\u5e2d\u751f\u4ea7\u529b\u3002"}}
{"id": "2510.06669", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06669", "abs": "https://arxiv.org/abs/2510.06669", "authors": ["Yuxi Liu", "Yunfeng Ma", "Yi Tang", "Min Liu", "Shuai Jiang", "Yaonan Wang"], "title": "Automated Neural Architecture Design for Industrial Defect Detection", "comment": null, "summary": "Industrial surface defect detection (SDD) is critical for ensuring product\nquality and manufacturing reliability. Due to the diverse shapes and sizes of\nsurface defects, SDD faces two main challenges: intraclass difference and\ninterclass similarity. Existing methods primarily utilize manually designed\nmodels, which require extensive trial and error and often struggle to address\nboth challenges effectively. To overcome this, we propose AutoNAD, an automated\nneural architecture design framework for SDD that jointly searches over\nconvolutions, transformers, and multi-layer perceptrons. This hybrid design\nenables the model to capture both fine-grained local variations and long-range\nsemantic context, addressing the two key challenges while reducing the cost of\nmanual network design. To support efficient training of such a diverse search\nspace, AutoNAD introduces a cross weight sharing strategy, which accelerates\nsupernet convergence and improves subnet performance. Additionally, a\nsearchable multi-level feature aggregation module (MFAM) is integrated to\nenhance multi-scale feature learning. Beyond detection accuracy, runtime\nefficiency is essential for industrial deployment. To this end, AutoNAD\nincorporates a latency-aware prior to guide the selection of efficient\narchitectures. The effectiveness of AutoNAD is validated on three industrial\ndefect datasets and further applied within a defect imaging and detection\nplatform. Code will be available at https://github.com/Yuxi104/AutoNAD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u795e\u7ecf\u67b6\u6784\u8bbe\u8ba1\u6846\u67b6AutoNAD\uff0c\u7528\u4e8e\u5de5\u4e1a\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\uff08SDD\uff09\u3002", "motivation": "\u73b0\u6709\u7684\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u624b\u52a8\u8bbe\u8ba1\u7684\u6a21\u578b\uff0c\u9700\u8981\u5927\u91cf\u7684\u8bd5\u9a8c\u548c\u9519\u8bef\uff0c\u5e76\u4e14\u901a\u5e38\u96be\u4ee5\u6709\u6548\u5730\u89e3\u51b3\u7c7b\u5185\u5dee\u5f02\u548c\u7c7b\u95f4\u76f8\u4f3c\u6027\u8fd9\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\u3002", "method": "AutoNAD \u8054\u5408\u641c\u7d22\u5377\u79ef\u3001transformer \u548c\u591a\u5c42\u611f\u77e5\u5668\u3002\u6b64\u5916\uff0cAutoNAD \u5f15\u5165\u4e86\u4e00\u79cd\u4ea4\u53c9\u6743\u91cd\u5171\u4eab\u7b56\u7565\uff0c\u4ee5\u52a0\u901f supernet \u7684\u6536\u655b\u5e76\u63d0\u9ad8 subnet \u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8fd8\u96c6\u6210\u4e86\u4e00\u4e2a\u53ef\u641c\u7d22\u7684\u591a\u7ea7\u7279\u5f81\u805a\u5408\u6a21\u5757 (MFAM) \u4ee5\u589e\u5f3a\u591a\u5c3a\u5ea6\u7279\u5f81\u5b66\u4e60\u3002AutoNAD \u7ed3\u5408\u4e86\u5ef6\u8fdf\u611f\u77e5\u5148\u9a8c\u6765\u6307\u5bfc\u9ad8\u6548\u67b6\u6784\u7684\u9009\u62e9\u3002", "result": "\u5728\u4e09\u4e2a\u5de5\u4e1a\u7f3a\u9677\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86 AutoNAD \u7684\u6709\u6548\u6027\uff0c\u5e76\u8fdb\u4e00\u6b65\u5e94\u7528\u4e8e\u7f3a\u9677\u6210\u50cf\u548c\u68c0\u6d4b\u5e73\u53f0\u3002", "conclusion": "AutoNAD \u80fd\u591f\u6355\u83b7\u7ec6\u7c92\u5ea6\u7684\u5c40\u90e8\u53d8\u5316\u548c\u957f\u7a0b\u8bed\u4e49\u4e0a\u4e0b\u6587\uff0c\u5728\u964d\u4f4e\u624b\u52a8\u7f51\u7edc\u8bbe\u8ba1\u6210\u672c\u7684\u540c\u65f6\uff0c\u89e3\u51b3\u4e86\u4e24\u4e2a\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2510.06646", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06646", "abs": "https://arxiv.org/abs/2510.06646", "authors": ["Mansi Sakarvadia", "Kareem Hegazy", "Amin Totounferoush", "Kyle Chard", "Yaoqing Yang", "Ian Foster", "Michael W. Mahoney"], "title": "The False Promise of Zero-Shot Super-Resolution in Machine-Learned Operators", "comment": null, "summary": "A core challenge in scientific machine learning, and scientific computing\nmore generally, is modeling continuous phenomena which (in practice) are\nrepresented discretely. Machine-learned operators (MLOs) have been introduced\nas a means to achieve this modeling goal, as this class of architecture can\nperform inference at arbitrary resolution. In this work, we evaluate whether\nthis architectural innovation is sufficient to perform \"zero-shot\nsuper-resolution,\" namely to enable a model to serve inference on\nhigher-resolution data than that on which it was originally trained. We\ncomprehensively evaluate both zero-shot sub-resolution and super-resolution\n(i.e., multi-resolution) inference in MLOs. We decouple multi-resolution\ninference into two key behaviors: 1) extrapolation to varying frequency\ninformation; and 2) interpolating across varying resolutions. We empirically\ndemonstrate that MLOs fail to do both of these tasks in a zero-shot manner.\nConsequently, we find MLOs are not able to perform accurate inference at\nresolutions different from those on which they were trained, and instead they\nare brittle and susceptible to aliasing. To address these failure modes, we\npropose a simple, computationally-efficient, and data-driven multi-resolution\ntraining protocol that overcomes aliasing and that provides robust\nmulti-resolution generalization.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u8bc4\u4f30\u4e86\u673a\u5668\u5b66\u4e60\u7b97\u5b50\uff08MLO\uff09\u5728\u201c\u96f6\u6837\u672c\u8d85\u5206\u8fa8\u7387\u201d\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5373\u6a21\u578b\u5728\u6bd4\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u66f4\u9ad8\u5206\u8fa8\u7387\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u63a8\u7406\u7684\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0MLO\u5728\u8fd9\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u5206\u8fa8\u7387\u8bad\u7ec3\u534f\u8bae\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "motivation": "\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u548c\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7684\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\u662f\u5efa\u6a21\u8fde\u7eed\u73b0\u8c61\uff0c\u800c\u8fd9\u4e9b\u73b0\u8c61\u5728\u5b9e\u8df5\u4e2d\u662f\u79bb\u6563\u8868\u793a\u7684\u3002\u673a\u5668\u5b66\u4e60\u7b97\u5b50\uff08MLO\uff09\u88ab\u5f15\u5165\u4f5c\u4e3a\u5b9e\u73b0\u8fd9\u4e00\u5efa\u6a21\u76ee\u6807\u7684\u4e00\u79cd\u624b\u6bb5\uff0c\u56e0\u4e3a\u8fd9\u7c7b\u67b6\u6784\u53ef\u4ee5\u5728\u4efb\u610f\u5206\u8fa8\u7387\u4e0b\u8fdb\u884c\u63a8\u7406\u3002", "method": "\u8be5\u7814\u7a76\u5168\u9762\u8bc4\u4f30\u4e86MLO\u4e2d\u7684\u96f6\u6837\u672c\u5b50\u5206\u8fa8\u7387\u548c\u8d85\u5206\u8fa8\u7387\uff08\u5373\uff0c\u591a\u5206\u8fa8\u7387\uff09\u63a8\u7406\u3002\u5c06\u591a\u5206\u8fa8\u7387\u63a8\u7406\u5206\u89e3\u4e3a\u4e24\u4e2a\u5173\u952e\u884c\u4e3a\uff1a1\uff09\u5916\u63a8\u5230\u4e0d\u540c\u7684\u9891\u7387\u4fe1\u606f\uff1b2\uff09\u5728\u4e0d\u540c\u7684\u5206\u8fa8\u7387\u4e4b\u95f4\u8fdb\u884c\u63d2\u503c\u3002\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660eMLO\u65e0\u6cd5\u4ee5\u96f6\u6837\u672c\u65b9\u5f0f\u5b8c\u6210\u8fd9\u4e24\u9879\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMLO\u65e0\u6cd5\u5728\u4e0e\u8bad\u7ec3\u5206\u8fa8\u7387\u4e0d\u540c\u7684\u5206\u8fa8\u7387\u4e0b\u8fdb\u884c\u51c6\u786e\u7684\u63a8\u7406\uff0c\u800c\u4e14\u975e\u5e38\u8106\u5f31\uff0c\u5bb9\u6613\u4ea7\u751f\u6df7\u53e0\u3002", "conclusion": "\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u5931\u8d25\u6a21\u5f0f\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u3001\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u6570\u636e\u9a71\u52a8\u7684\u591a\u5206\u8fa8\u7387\u8bad\u7ec3\u534f\u8bae\uff0c\u8be5\u534f\u8bae\u53ef\u4ee5\u514b\u670d\u6df7\u53e0\uff0c\u5e76\u63d0\u4f9b\u5f3a\u5927\u7684\u591a\u5206\u8fa8\u7387\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.06695", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.06695", "abs": "https://arxiv.org/abs/2510.06695", "authors": ["Qinhao Zhou", "Xiang Xiang", "Kun He", "John E. Hopcroft"], "title": "Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks", "comment": null, "summary": "In recent years, the growing interest in Large Language Models (LLMs) has\nsignificantly advanced prompt engineering, transitioning from manual design to\nmodel-based optimization. Prompts for LLMs generally comprise two components:\nthe \\textit{instruction}, which defines the task or objective, and the\n\\textit{input}, which is tailored to the instruction type. In natural language\ngeneration (NLG) tasks such as machine translation, the \\textit{input}\ncomponent is particularly critical, while the \\textit{instruction} component\ntends to be concise. Existing prompt engineering methods primarily focus on\noptimizing the \\textit{instruction} component for general tasks, often\nrequiring large-parameter LLMs as auxiliary tools. However, these approaches\nexhibit limited applicability for tasks like machine translation, where the\n\\textit{input} component plays a more pivotal role. To address this limitation,\nthis paper introduces a novel prompt optimization method specifically designed\nfor machine translation tasks. The proposed approach employs a small-parameter\nmodel trained using a back-translation-based strategy, significantly reducing\ntraining overhead for single-task optimization while delivering highly\neffective performance. With certain adaptations, this method can also be\nextended to other downstream tasks.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684prompt\u4f18\u5316\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\uff0c\u4f7f\u7528back-translation-based\u7b56\u7565\u8bad\u7ec3\u7684\u5c0f\u53c2\u6570\u6a21\u578b\uff0c\u964d\u4f4e\u4e86\u5355\u4efb\u52a1\u4f18\u5316\u7684\u8bad\u7ec3\u5f00\u9500\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684prompt\u5de5\u7a0b\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u4e8e\u4f18\u5316\u901a\u7528\u4efb\u52a1\u7684instruction\u7ec4\u4ef6\uff0c\u901a\u5e38\u9700\u8981\u5927\u578b\u53c2\u6570LLM\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5bf9\u4e8e\u50cf\u673a\u5668\u7ffb\u8bd1\u8fd9\u6837\u7684\u4efb\u52a1\u7684\u9002\u7528\u6027\u6709\u9650\uff0c\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\uff0cinput\u7ec4\u4ef6\u8d77\u7740\u66f4\u5173\u952e\u7684\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684prompt\u4f18\u5316\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u4e86\u4e00\u79cd\u57fa\u4e8eback-translation-based\u7b56\u7565\u8bad\u7ec3\u7684\u5c0f\u53c2\u6570\u6a21\u578b\uff0c", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u5355\u4efb\u52a1\u4f18\u5316\u7684\u8bad\u7ec3\u5f00\u9500\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u4e00\u5b9a\u7684\u8c03\u6574\uff0c\u8be5\u65b9\u6cd5\u4e5f\u53ef\u4ee5\u6269\u5c55\u5230\u5176\u4ed6\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2510.06673", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06673", "abs": "https://arxiv.org/abs/2510.06673", "authors": ["Yongxin Zhu", "Jiawei Chen", "Yuanzhe Chen", "Zhuo Chen", "Dongya Jia", "Jian Cong", "Xiaobin Zhuang", "Yuping Wang", "Yuxuan Wang"], "title": "Heptapod: Language Modeling on Visual Signals", "comment": null, "summary": "We introduce Heptapod, an image autoregressive model that adheres to the\nfoundational principles of language modeling. Heptapod employs \\textbf{causal\nattention}, \\textbf{eliminates reliance on CFG}, and \\textbf{eschews the trend\nof semantic tokenizers}. Our key innovation is \\textit{next 2D distribution\nprediction}: a causal Transformer with reconstruction-focused visual tokenizer,\nlearns to predict the distribution over the entire 2D spatial grid of images at\neach timestep. This learning objective unifies the sequential modeling of\nautoregressive framework with the holistic self-supervised learning of masked\nautoencoding, enabling the model to capture comprehensive image semantics via\ngenerative training. On the ImageNet generation benchmark, Heptapod achieves an\nFID of $2.70$, significantly outperforming previous causal autoregressive\napproaches. We hope our work inspires a principled rethinking of language\nmodeling on visual signals and beyond.", "AI": {"tldr": "Heptapod\u662f\u4e00\u4e2a\u56fe\u50cf\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u5b83\u9075\u5faa\u8bed\u8a00\u5efa\u6a21\u7684\u57fa\u672c\u539f\u5219\uff0c\u91c7\u7528\u56e0\u679c\u6ce8\u610f\u529b\uff0c\u4e0d\u4f9d\u8d56CFG\uff0c\u907f\u514d\u4f7f\u7528\u8bed\u4e49\u6807\u8bb0\u5668\u3002\u901a\u8fc7\u9884\u6d4b\u4e0b\u4e00\u4e2a2D\u5206\u5e03\uff0c\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u751f\u6210\u5f0f\u8bad\u7ec3\u6355\u6349\u5168\u9762\u7684\u56fe\u50cf\u8bed\u4e49\u3002", "motivation": "\u4f5c\u8005\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u5efa\u6a21\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u9075\u5faa\u8bed\u8a00\u5efa\u6a21\u7684\u539f\u5219\uff0c\u5e76\u4e14\u80fd\u591f\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "Heptapod\u4f7f\u7528\u5e26\u6709\u91cd\u5efa\u805a\u7126\u89c6\u89c9\u6807\u8bb0\u5668\u7684\u56e0\u679cTransformer\uff0c\u5b66\u4e60\u9884\u6d4b\u56fe\u50cf\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u6574\u4e2a2D\u7a7a\u95f4\u7f51\u683c\u4e0a\u7684\u5206\u5e03\u3002", "result": "\u5728ImageNet\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHeptapod\u7684FID\u4e3a2.70\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u56e0\u679c\u81ea\u56de\u5f52\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u89c6\u89c9\u4fe1\u53f7\u53ca\u5176\u4ed6\u9886\u57df\u7684\u8bed\u8a00\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2510.06649", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06649", "abs": "https://arxiv.org/abs/2510.06649", "authors": ["Frank Wu", "Mengye Ren"], "title": "Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions", "comment": "15 pages, 5 figures", "summary": "The Forward-Forward (FF) Algorithm is a recently proposed learning procedure\nfor neural networks that employs two forward passes instead of the traditional\nforward and backward passes used in backpropagation. However, FF remains\nlargely confined to supervised settings, leaving a gap at domains where\nlearning signals can be yielded more naturally such as RL. In this work,\ninspired by FF's goodness function using layer activity statistics, we\nintroduce Action-conditioned Root mean squared Q-Functions (ARQ), a novel value\nestimation method that applies a goodness function and action conditioning for\nlocal RL using temporal difference learning. Despite its simplicity and\nbiological grounding, our approach achieves superior performance compared to\nstate-of-the-art local backprop-free RL methods in the MinAtar and the DeepMind\nControl Suite benchmarks, while also outperforming algorithms trained with\nbackpropagation on most tasks. Code can be found at\nhttps://github.com/agentic-learning-ai-lab/arq.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e Forward-Forward \u7b97\u6cd5\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u79f0\u4e3a Action-conditioned Root mean squared Q-Functions (ARQ)\u3002", "motivation": "Forward-Forward \u7b97\u6cd5\u4e3b\u8981\u5e94\u7528\u4e8e\u76d1\u7763\u5b66\u4e60\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u9886\u57df\u5b58\u5728\u7a7a\u767d\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528 Forward-Forward \u7b97\u6cd5\u7684\u4f18\u70b9\uff0c\u7ed3\u5408\u4e86\u65f6\u95f4\u5dee\u5206\u5b66\u4e60\u548c\u52a8\u4f5c\u6761\u4ef6\u53cd\u5c04\u3002", "result": "\u5728 MinAtar \u548c DeepMind Control Suite \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u76ee\u524d\u6700\u597d\u7684\u5c40\u90e8\u53cd\u5411\u4f20\u64ad\u81ea\u7531\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u5927\u591a\u6570\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f7f\u7528\u53cd\u5411\u4f20\u64ad\u8bad\u7ec3\u7684\u7b97\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684 ARQ \u65b9\u6cd5\u662f\u4e00\u79cd\u7b80\u5355\u4e14\u5177\u6709\u751f\u7269\u5b66\u57fa\u7840\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.06700", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06700", "abs": "https://arxiv.org/abs/2510.06700", "authors": ["Leonardo Bertolazzi", "Sandro Pezzelle", "Raffaelle Bernardi"], "title": "How Language Models Conflate Logical Validity with Plausibility: A Representational Analysis of Content Effects", "comment": null, "summary": "Both humans and large language models (LLMs) exhibit content effects: biases\nin which the plausibility of the semantic content of a reasoning problem\ninfluences judgments regarding its logical validity. While this phenomenon in\nhumans is best explained by the dual-process theory of reasoning, the\nmechanisms behind content effects in LLMs remain unclear. In this work, we\naddress this issue by investigating how LLMs encode the concepts of validity\nand plausibility within their internal representations. We show that both\nconcepts are linearly represented and strongly aligned in representational\ngeometry, leading models to conflate plausibility with validity. Using steering\nvectors, we demonstrate that plausibility vectors can causally bias validity\njudgements, and vice versa, and that the degree of alignment between these two\nconcepts predicts the magnitude of behavioral content effects across models.\nFinally, we construct debiasing vectors that disentangle these concepts,\nreducing content effects and improving reasoning accuracy. Our findings advance\nunderstanding of how abstract logical concepts are represented in LLMs and\nhighlight representational interventions as a path toward more logical systems.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u4eba\u7c7b\u4e00\u6837\uff0c\u90fd\u4f1a\u53d7\u5230\u5185\u5bb9\u6548\u5e94\u7684\u5f71\u54cd\uff0c\u5373\u63a8\u7406\u95ee\u9898\u4e2d\u8bed\u4e49\u5185\u5bb9\u7684\u53ef\u4fe1\u5ea6\u4f1a\u5f71\u54cd\u5bf9\u5176\u903b\u8f91\u6709\u6548\u6027\u7684\u5224\u65ad\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5185\u5bb9\u6548\u5e94\u80cc\u540e\u7684\u673a\u5236\uff0c\u63a2\u8ba8\u6a21\u578b\u5982\u4f55\u5728\u5185\u90e8\u8868\u5f81\u4e2d\u7f16\u7801\u6709\u6548\u6027\u548c\u53ef\u4fe1\u5ea6\u8fd9\u4e24\u4e2a\u6982\u5ff5\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u8868\u5f81\uff0c\u5206\u6790\u6709\u6548\u6027\u548c\u53ef\u4fe1\u5ea6\u6982\u5ff5\u7684\u7ebf\u6027\u8868\u5f81\u548c\u5bf9\u9f50\u60c5\u51b5\uff0c\u5e76\u4f7f\u7528 steering vectors \u6765\u9a8c\u8bc1\u53ef\u4fe1\u5ea6\u5bf9\u6709\u6548\u6027\u5224\u65ad\u7684\u56e0\u679c\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6709\u6548\u6027\u548c\u53ef\u4fe1\u5ea6\u5728\u7ebf\u6027\u8868\u5f81\u4e2d\u5f3a\u5bf9\u9f50\uff0c\u5bfc\u81f4\u6a21\u578b\u6df7\u6dc6\u4e24\u8005\u3002\u901a\u8fc7 steering vectors \u53ef\u4ee5\u53d1\u73b0\uff0c\u53ef\u4fe1\u5ea6\u5411\u91cf\u4f1a\u5f71\u54cd\u6709\u6548\u6027\u5224\u65ad\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002\u5e76\u4e14\uff0c\u8fd9\u4e24\u4e2a\u6982\u5ff5\u7684\u5bf9\u9f50\u7a0b\u5ea6\u53ef\u4ee5\u9884\u6d4b\u6a21\u578b\u884c\u4e3a\u5185\u5bb9\u6548\u5e94\u7684\u5927\u5c0f\u3002\u6784\u5efa debiasing vectors \u53ef\u4ee5\u89e3\u8026\u8fd9\u4e24\u4e2a\u6982\u5ff5\uff0c\u51cf\u5c11\u5185\u5bb9\u6548\u5e94\u5e76\u63d0\u9ad8\u63a8\u7406\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u52a0\u6df1\u4e86\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u62bd\u8c61\u903b\u8f91\u6982\u5ff5\u8868\u5f81\u65b9\u5f0f\u7684\u7406\u89e3\uff0c\u5e76\u5f3a\u8c03\u4e86\u8868\u5f81\u5e72\u9884\u662f\u5b9e\u73b0\u66f4\u5177\u903b\u8f91\u6027\u7684\u7cfb\u7edf\u7684\u4e00\u79cd\u9014\u5f84\u3002"}}
{"id": "2510.06679", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06679", "abs": "https://arxiv.org/abs/2510.06679", "authors": ["Bin Xia", "Bohao Peng", "Yuechen Zhang", "Junjia Huang", "Jiyang Liu", "Jingyao Li", "Haoru Tan", "Sitong Wu", "Chengyao Wang", "Yitong Wang", "Xinglong Wu", "Bei Yu", "Jiaya Jia"], "title": "DreamOmni2: Multimodal Instruction-based Editing and Generation", "comment": null, "summary": "Recent advancements in instruction-based image editing and subject-driven\ngeneration have garnered significant attention, yet both tasks still face\nlimitations in meeting practical user needs. Instruction-based editing relies\nsolely on language instructions, which often fail to capture specific editing\ndetails, making reference images necessary. Meanwhile, subject-driven\ngeneration is limited to combining concrete objects or people, overlooking\nbroader, abstract concepts. To address these challenges, we propose two novel\ntasks: multimodal instruction-based editing and generation. These tasks support\nboth text and image instructions and extend the scope to include both concrete\nand abstract concepts, greatly enhancing their practical applications. We\nintroduce DreamOmni2, tackling two primary challenges: data creation and model\nframework design. Our data synthesis pipeline consists of three steps: (1)\nusing a feature mixing method to create extraction data for both abstract and\nconcrete concepts, (2) generating multimodal instruction-based editing training\ndata using the editing and extraction models, and (3) further applying the\nextraction model to create training data for multimodal instruction-based\nediting. For the framework, to handle multi-image input, we propose an index\nencoding and position encoding shift scheme, which helps the model distinguish\nimages and avoid pixel confusion. Additionally, we introduce joint training\nwith the VLM and our generation/editing model to better process complex\ninstructions. In addition, we have proposed comprehensive benchmarks for these\ntwo new tasks to drive their development. Experiments show that DreamOmni2 has\nachieved impressive results. Models and codes will be released.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u6001\u6307\u4ee4\u56fe\u50cf\u7f16\u8f91\u548c\u751f\u6210\u4efb\u52a1\uff0c\u4ee5\u652f\u6301\u6587\u672c\u548c\u56fe\u50cf\u6307\u4ee4\uff0c\u5e76\u6269\u5c55\u5230\u5305\u62ec\u5177\u4f53\u548c\u62bd\u8c61\u6982\u5ff5", "motivation": "\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u4f9d\u8d56\u4e8e\u8bed\u8a00\u6307\u4ee4\uff0c\u901a\u5e38\u65e0\u6cd5\u6355\u6349\u5230\u5177\u4f53\u7684\u7f16\u8f91\u7ec6\u8282\uff0c\u9700\u8981\u53c2\u8003\u56fe\u50cf\u3002\u540c\u65f6\uff0c\u4e3b\u9898\u9a71\u52a8\u7684\u751f\u6210\u4ec5\u9650\u4e8e\u7ec4\u5408\u5177\u4f53\u7684\u5bf9\u8c61\u6216\u4eba\uff0c\u5ffd\u7565\u4e86\u66f4\u5e7f\u6cdb\u7684\u62bd\u8c61\u6982\u5ff5\u3002", "method": "1) \u4f7f\u7528\u7279\u5f81\u6df7\u5408\u65b9\u6cd5\u521b\u5efa\u62bd\u8c61\u548c\u5177\u4f53\u6982\u5ff5\u7684\u63d0\u53d6\u6570\u636e; 2) \u4f7f\u7528\u7f16\u8f91\u548c\u63d0\u53d6\u6a21\u578b\u751f\u6210\u591a\u6a21\u6001\u6307\u4ee4\u7f16\u8f91\u8bad\u7ec3\u6570\u636e; 3) \u5e94\u7528\u63d0\u53d6\u6a21\u578b\u521b\u5efa\u591a\u6a21\u6001\u6307\u4ee4\u7f16\u8f91\u8bad\u7ec3\u6570\u636e\u3002\u4e3a\u4e86\u5904\u7406\u591a\u56fe\u50cf\u8f93\u5165\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7d22\u5f15\u7f16\u7801\u548c\u4f4d\u7f6e\u7f16\u7801\u79fb\u4f4d\u65b9\u6848\uff0c\u4ee5\u5e2e\u52a9\u6a21\u578b\u533a\u5206\u56fe\u50cf\u5e76\u907f\u514d\u50cf\u7d20\u6df7\u6dc6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e0e VLM \u548c\u6211\u4eec\u7684\u751f\u6210/\u7f16\u8f91\u6a21\u578b\u7684\u8054\u5408\u8bad\u7ec3\uff0c\u4ee5\u66f4\u597d\u5730\u5904\u7406\u590d\u6742\u6307\u4ee4\u3002", "result": "DreamOmni2 \u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u7684\u4efb\u52a1\uff1a\u591a\u6a21\u6001\u6307\u4ee4\u56fe\u50cf\u7f16\u8f91\u548c\u751f\u6210\uff0c\u5e76\u4e3a\u8fd9\u4e24\u4e2a\u65b0\u4efb\u52a1\u63d0\u51fa\u4e86\u7efc\u5408\u57fa\u51c6\uff0c\u4ee5\u63a8\u52a8\u5176\u53d1\u5c55\u3002"}}
{"id": "2510.06660", "categories": ["cs.LG", "math.PR"], "pdf": "https://arxiv.org/pdf/2510.06660", "abs": "https://arxiv.org/abs/2510.06660", "authors": ["Weiguo Lu", "Gangnan Yuan", "Hong-kun Zhang", "Shangyang Li"], "title": "Rethinking Nonlinearity: Trainable Gaussian Mixture Modules for Modern Neural Architectures", "comment": null, "summary": "Neural networks in general, from MLPs and CNNs to attention-based\nTransformers, are constructed from layers of linear combinations followed by\nnonlinear operations such as ReLU, Sigmoid, or Softmax. Despite their strength,\nthese conventional designs are often limited in introducing non-linearity by\nthe choice of activation functions. In this work, we introduce Gaussian\nMixture-Inspired Nonlinear Modules (GMNM), a new class of differentiable\nmodules that draw on the universal density approximation Gaussian mixture\nmodels (GMMs) and distance properties (metric space) of Gaussian kernal. By\nrelaxing probabilistic constraints and adopting a flexible parameterization of\nGaussian projections, GMNM can be seamlessly integrated into diverse neural\narchitectures and trained end-to-end with gradient-based methods. Our\nexperiments demonstrate that incorporating GMNM into architectures such as\nMLPs, CNNs, attention mechanisms, and LSTMs consistently improves performance\nover standard baselines. These results highlight GMNM's potential as a powerful\nand flexible module for enhancing efficiency and accuracy across a wide range\nof machine learning applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u542f\u53d1\u7684\u975e\u7ebf\u6027\u6a21\u5757\uff08GMNM\uff09\uff0c\u4ee5\u589e\u5f3a\u795e\u7ecf\u7f51\u7edc\u7684\u975e\u7ebf\u6027\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u7684\u975e\u7ebf\u6027\u80fd\u529b\u53d7\u9650\u4e8e\u6fc0\u6d3b\u51fd\u6570\u7684\u9009\u62e9\u3002", "method": "\u5229\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u7684\u901a\u7528\u5bc6\u5ea6\u8fd1\u4f3c\u548c\u9ad8\u65af\u6838\u7684\u8ddd\u79bb\u5c5e\u6027\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u5fae\u6a21\u5757GMNM\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u5404\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e2d\u3002", "result": "\u5c06GMNM\u96c6\u6210\u5230MLP\u3001CNN\u3001\u6ce8\u610f\u529b\u673a\u5236\u548cLSTM\u7b49\u67b6\u6784\u4e2d\uff0c\u5747\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "GMNM\u662f\u4e00\u79cd\u5f3a\u5927\u800c\u7075\u6d3b\u7684\u6a21\u5757\uff0c\u6709\u6f5c\u529b\u63d0\u9ad8\u5404\u79cd\u673a\u5668\u5b66\u4e60\u5e94\u7528\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2510.06727", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06727", "abs": "https://arxiv.org/abs/2510.06727", "authors": ["Miao Lu", "Weiwei Sun", "Weihua Du", "Zhan Ling", "Xuesong Yao", "Kang Liu", "Jiecao Chen"], "title": "Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management", "comment": null, "summary": "We study reinforcement learning (RL) fine-tuning of large language model\n(LLM) agents for long-horizon multi-turn tool use, where context length quickly\nbecomes a fundamental bottleneck. Existing RL pipelines can suffer from\ndegraded instruction following, excessive rollout costs, and most importantly,\nstrict context limits. To address these challenges, we introduce\nsummarization-based context management to training. In specific, it\nperiodically compresses the tool using history by LLM-generated summaries that\nretain task-relevant information to keep a compact context while enabling the\nagent to scale beyond the fixed context window. Building on this formulation,\nwe derive a policy gradient representation that seamlessly enables standard LLM\nRL infrastructures to optimize both tool-use behaviors as well as summarization\nstrategies in an end-to-end fashion. We instantiate this framework with\n\\underline{SU}mmarization augmented \\underline{P}olicy \\underline{O}ptimization\n(\\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond\na fixed context limit. Experiments on interactive function calling and\nsearching tasks demonstrate that \\texttt{SUPO} significantly improves the\nsuccess rate while maintaining the same or even lower working context length\ncompared to baselines. We also demonstrate that for complex searching tasks,\n\\texttt{SUPO} can further improve the evaluation performance when scaling\ntest-time maximum round of summarization beyond that of training time. Our\nresults establish summarization-based context management as a principled and\nscalable approach for training RL agents beyond a fixed context length limit.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u603b\u7ed3\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u80fd\u591f\u8fdb\u884c\u957f\u7a0b\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u7684LLM Agent\u3002", "motivation": "\u73b0\u6709\u7684RL pipeline\u5b58\u5728\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u4e0b\u964d\u3001rollout\u6210\u672c\u8fc7\u9ad8\u4ee5\u53ca\u4e25\u683c\u7684\u4e0a\u4e0b\u6587\u9650\u5236\u7b49\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7LLM\u751f\u6210\u7684\u603b\u7ed3\u5b9a\u671f\u538b\u7f29\u5de5\u5177\u4f7f\u7528\u5386\u53f2\uff0c\u4ee5\u4fdd\u6301\u4e0a\u4e0b\u6587\u7684\u7d27\u51d1\u6027\uff0c\u5e76\u4f7fAgent\u80fd\u591f\u6269\u5c55\u5230\u56fa\u5b9a\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e4b\u5916\u3002\u540c\u65f6\uff0c\u63a8\u5bfc\u51fa\u4e00\u79cd\u7b56\u7565\u68af\u5ea6\u8868\u793a\uff0c\u53ef\u4ee5\u65e0\u7f1d\u5730\u5b9e\u73b0\u6807\u51c6LLM RL\u57fa\u7840\u8bbe\u65bd\u6765\u4f18\u5316\u5de5\u5177\u4f7f\u7528\u884c\u4e3a\u548c\u603b\u7ed3\u7b56\u7565\u3002", "result": "\u5728\u4ea4\u4e92\u5f0f\u51fd\u6570\u8c03\u7528\u548c\u641c\u7d22\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSUPO\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u540c\u751a\u81f3\u66f4\u4f4e\u7684\u5de5\u4f5c\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002\u5bf9\u4e8e\u590d\u6742\u7684\u641c\u7d22\u4efb\u52a1\uff0c\u5f53\u6d4b\u8bd5\u65f6\u7684\u6700\u5927\u603b\u7ed3\u8f6e\u6570\u8d85\u8fc7\u8bad\u7ec3\u65f6\uff0cSUPO\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u8bc4\u4f30\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8e\u603b\u7ed3\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u662f\u4e00\u79cd\u539f\u5219\u4e0a\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u8d85\u51fa\u56fa\u5b9a\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u7684RL Agent\u3002"}}
{"id": "2510.06687", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06687", "abs": "https://arxiv.org/abs/2510.06687", "authors": ["Jie Luo", "Yuxuan Jiang", "Xin Jin", "Mingyu Liu", "Yihui Fan"], "title": "Semantic Segmentation Algorithm Based on Light Field and LiDAR Fusion", "comment": null, "summary": "Semantic segmentation serves as a cornerstone of scene understanding in\nautonomous driving but continues to face significant challenges under complex\nconditions such as occlusion. Light field and LiDAR modalities provide\ncomplementary visual and spatial cues that are beneficial for robust\nperception; however, their effective integration is hindered by limited\nviewpoint diversity and inherent modality discrepancies. To address these\nchallenges, the first multimodal semantic segmentation dataset integrating\nlight field data and point cloud data is proposed. Based on this dataset, we\nproposed a multi-modal light field point-cloud fusion segmentation\nnetwork(Mlpfseg), incorporating feature completion and depth perception to\nsegment both camera images and LiDAR point clouds simultaneously. The feature\ncompletion module addresses the density mismatch between point clouds and image\npixels by performing differential reconstruction of point-cloud feature maps,\nenhancing the fusion of these modalities. The depth perception module improves\nthe segmentation of occluded objects by reinforcing attention scores for better\nocclusion awareness. Our method outperforms image-only segmentation by 1.71\nMean Intersection over Union(mIoU) and point cloud-only segmentation by 2.38\nmIoU, demonstrating its effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u96c6\u6210\u4e86\u5149\u573a\u6570\u636e\u548c\u70b9\u4e91\u6570\u636e\uff0c\u4ee5\u5e94\u5bf9\u81ea\u52a8\u9a7e\u9a76\u4e2d\u906e\u6321\u7b49\u590d\u6742\u6761\u4ef6\u4e0b\u7684\u573a\u666f\u7406\u89e3\u6311\u6218\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u8bed\u4e49\u5206\u5272\u662f\u573a\u666f\u7406\u89e3\u7684\u57fa\u77f3\uff0c\u4f46\u5728\u906e\u6321\u7b49\u590d\u6742\u6761\u4ef6\u4e0b\u4ecd\u9762\u4e34\u6311\u6218\u3002\u5149\u573a\u548c\u6fc0\u5149\u96f7\u8fbe\u6a21\u6001\u63d0\u4f9b\u4e92\u8865\u7684\u89c6\u89c9\u548c\u7a7a\u95f4\u7ebf\u7d22\uff0c\u6709\u5229\u4e8e\u9c81\u68d2\u611f\u77e5\uff0c\u4f46\u5b83\u4eec\u7684\u6709\u6548\u96c6\u6210\u53d7\u5230\u6709\u9650\u7684\u89c6\u70b9\u591a\u6837\u6027\u548c\u56fa\u6709\u7684\u6a21\u6001\u5dee\u5f02\u7684\u963b\u788d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u5149\u573a\u70b9\u4e91\u878d\u5408\u5206\u5272\u7f51\u7edc(Mlpfseg)\uff0c\u7ed3\u5408\u4e86\u7279\u5f81\u8865\u5168\u548c\u6df1\u5ea6\u611f\u77e5\uff0c\u4ee5\u540c\u65f6\u5206\u5272\u76f8\u673a\u56fe\u50cf\u548c\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u3002\u7279\u5f81\u8865\u5168\u6a21\u5757\u901a\u8fc7\u5bf9\u70b9\u4e91\u7279\u5f81\u56fe\u8fdb\u884c\u5dee\u5206\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u70b9\u4e91\u548c\u56fe\u50cf\u50cf\u7d20\u4e4b\u95f4\u7684\u5bc6\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u589e\u5f3a\u4e86\u8fd9\u4e9b\u6a21\u6001\u7684\u878d\u5408\u3002\u6df1\u5ea6\u611f\u77e5\u6a21\u5757\u901a\u8fc7\u52a0\u5f3a\u6ce8\u610f\u8bc4\u5206\u6765\u63d0\u9ad8\u906e\u6321\u7269\u4f53\u7684\u5206\u5272\u6548\u679c\uff0c\u4ece\u800c\u66f4\u597d\u5730\u611f\u77e5\u906e\u6321\u3002", "result": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4ec5\u56fe\u50cf\u5206\u52721.71 mIoU\uff0c\u4f18\u4e8e\u4ec5\u70b9\u4e91\u5206\u52722.38 mIoU\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u591a\u6a21\u6001\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u6709\u6548\u5730\u96c6\u6210\u4e86\u5149\u573a\u6570\u636e\u548c\u70b9\u4e91\u6570\u636e\uff0c\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u573a\u666f\u7406\u89e3\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.06662", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.06662", "abs": "https://arxiv.org/abs/2510.06662", "authors": ["Penghao Yu", "Haotian Jiang", "Zeyu Bao", "Ruoxi Yu", "Qianxiao Li"], "title": "The Effect of Attention Head Count on Transformer Approximation", "comment": null, "summary": "Transformer has become the dominant architecture for sequence modeling, yet a\ndetailed understanding of how its structural parameters influence expressive\npower remains limited. In this work, we study the approximation properties of\ntransformers, with particular emphasis on the role of the number of attention\nheads. Our analysis begins with the introduction of a generalized $D$-retrieval\ntask, which we prove to be dense in the space of continuous functions, thereby\nproviding the basis for our theoretical framework. We then establish both upper\nand lower bounds on the parameter complexity required for\n$\\epsilon$-approximation. Specifically, we show that transformers with\nsufficiently many heads admit efficient approximation, whereas with too few\nheads, the number of parameters must scale at least as $O(1/\\epsilon^{cT})$,\nfor some constant $c$ and sequence length $T$. To the best of our knowledge,\nthis constitutes the first rigorous lower bound of this type in a nonlinear and\npractically relevant setting. We further examine the single-head case and\ndemonstrate that an embedding dimension of order $O(T)$ allows complete\nmemorization of the input, where approximation is entirely achieved by the\nfeed-forward block. Finally, we validate our theoretical findings with\nexperiments on both synthetic data and real-world tasks, illustrating the\npractical relevance of our results.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Transformer\u7684\u903c\u8fd1\u6027\u8d28\uff0c\u91cd\u70b9\u5173\u6ce8\u6ce8\u610f\u529b\u5934\u7684\u6570\u91cf\u3002", "motivation": "\u5bf9Transformer\u7684\u7ed3\u6784\u53c2\u6570\u5982\u4f55\u5f71\u54cd\u8868\u8fbe\u80fd\u529b\u7f3a\u4e4f\u8be6\u7ec6\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u5e7f\u4e49D-\u68c0\u7d22\u4efb\u52a1\uff0c\u5efa\u7acb\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u63a8\u5bfc\u4e86\u53c2\u6570\u590d\u6742\u5ea6\u7684\u4e0a\u4e0b\u754c\u3002", "result": "\u8bc1\u660e\u4e86\u5177\u6709\u8db3\u591f\u591a\u5934\u7684Transformer\u53ef\u4ee5\u5b9e\u73b0\u6709\u6548\u903c\u8fd1\uff0c\u800c\u5934\u6570\u8fc7\u5c11\u65f6\uff0c\u53c2\u6570\u6570\u91cf\u5fc5\u987b\u81f3\u5c11\u6309$O(1/\\\\epsilon^{cT})$\u7684\u6bd4\u4f8b\u7f29\u653e\u3002\u6b64\u5916\uff0c\u8fd8\u8bc1\u660e\u4e86\u5355\u5934Transformer\u7684\u5d4c\u5165\u7ef4\u5ea6\u4e3a$O(T)$\u65f6\uff0c\u53ef\u4ee5\u5b8c\u5168\u8bb0\u5fc6\u8f93\u5165\u3002", "conclusion": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u7684\u5b9e\u9645\u610f\u4e49\u3002"}}
{"id": "2510.06730", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06730", "abs": "https://arxiv.org/abs/2510.06730", "authors": ["Manuel Frank", "Haithem Afli"], "title": "PTEB: Towards Robust Text Embedding Evaluation via Stochastic Paraphrasing at Evaluation Time with LLMs", "comment": null, "summary": "Current evaluations of sentence embedding models typically rely on static\ntest beds such as the Massive Text Embedding Benchmark (MTEB). While\ninvaluable, repeated tuning on a fixed suite can inflate reported performance\nand obscure real-world robustness. We introduce the Paraphrasing Text Embedding\nBenchmark (PTEB), a dynamic protocol that stochastically generates\nmeaning-preserving paraphrases at evaluation time and aggregates results across\nmultiple runs. Using a cost-efficient LLM-based method grounded in semantic\ntextual similarity gold ratings, we show that LLMs generate token-diverse but\nsemantically preserving, paraphrases. Across 7 MTEB tasks, we validate our\nhypothesis that the performance of sentence encoders is sensitive to changes in\ntoken space even when semantics remain fixed. We also observe that smaller\nmodels are not disproportionately affected relative to larger ones. Our results\nare statistically robust over multiple runs and we extended our experiments to\n3 multilingual datasets covering 10 languages. More generally, we aim to\npropose a new evaluation paradigm in NLP that relies less on static,\npre-defined benchmarks but shifts towards dynamic, stochastic evaluation\nleveraging eval-time compute.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u52a8\u6001\u7684\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u8bc4\u4f30\u534f\u8bae PTEB\uff0c\u8be5\u534f\u8bae\u5728\u8bc4\u4f30\u65f6\u968f\u673a\u751f\u6210\u91ca\u4e49\uff0c\u5e76\u5728\u591a\u6b21\u8fd0\u884c\u4e2d\u805a\u5408\u7ed3\u679c\u3002", "motivation": "\u5f53\u524d\u7684\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u8bc4\u4f30\u4f9d\u8d56\u4e8e\u9759\u6001\u6d4b\u8bd5\u5e73\u53f0\uff0c\u8fd9\u53ef\u80fd\u4f1a\u5938\u5927\u62a5\u544a\u7684\u6027\u80fd\u5e76\u63a9\u76d6\u73b0\u5b9e\u4e16\u754c\u7684\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\u9ec4\u91d1\u8bc4\u7ea7\u7684\u3001\u5177\u6709\u6210\u672c\u6548\u76ca\u7684 LLM \u65b9\u6cd5\u751f\u6210\u91ca\u4e49\u3002", "result": "\u9a8c\u8bc1\u4e86\u53e5\u5b50\u7f16\u7801\u5668\u7684\u6027\u80fd\u5bf9token\u7a7a\u95f4\u7684\u53d8\u5316\u654f\u611f\uff0c\u5373\u4f7f\u8bed\u4e49\u4fdd\u6301\u4e0d\u53d8\u3002\u8f83\u5c0f\u7684\u6a21\u578b\u5e76\u6ca1\u6709\u53d7\u5230\u76f8\u5bf9\u4e8e\u8f83\u5927\u6a21\u578b\u4e0d\u6210\u6bd4\u4f8b\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u5728\u591a\u6b21\u8fd0\u884c\u4e2d\u5177\u6709\u7edf\u8ba1\u4e0a\u7684\u7a33\u5065\u6027\uff0c\u5e76\u5c06\u5b9e\u9a8c\u6269\u5c55\u5230\u6db5\u76d6 10 \u79cd\u8bed\u8a00\u7684 3 \u4e2a\u591a\u8bed\u8a00\u6570\u636e\u96c6\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684 NLP \u8bc4\u4f30\u8303\u4f8b\uff0c\u8be5\u8303\u4f8b\u51cf\u5c11\u4e86\u5bf9\u9759\u6001\u3001\u9884\u5b9a\u4e49\u7684\u57fa\u51c6\u7684\u4f9d\u8d56\uff0c\u800c\u662f\u8f6c\u5411\u5229\u7528\u8bc4\u4f30\u65f6\u95f4\u8ba1\u7b97\u7684\u52a8\u6001\u3001\u968f\u673a\u8bc4\u4f30\u3002"}}
{"id": "2510.06694", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06694", "abs": "https://arxiv.org/abs/2510.06694", "authors": ["Jipeng Lyu", "Jiahua Dong", "Yu-Xiong Wang"], "title": "SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis", "comment": "Published in Transactions on Machine Learning Research (06/2025)", "summary": "Persistent dynamic scene modeling for tracking and novel-view synthesis\nremains challenging due to the difficulty of capturing accurate deformations\nwhile maintaining computational efficiency. We propose SCas4D, a cascaded\noptimization framework that leverages structural patterns in 3D Gaussian\nSplatting for dynamic scenes. The key idea is that real-world deformations\noften exhibit hierarchical patterns, where groups of Gaussians share similar\ntransformations. By progressively refining deformations from coarse part-level\nto fine point-level, SCas4D achieves convergence within 100 iterations per time\nframe and produces results comparable to existing methods with only\none-twentieth of the training iterations. The approach also demonstrates\neffectiveness in self-supervised articulated object segmentation, novel view\nsynthesis, and dense point tracking tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSCas4D\u7684\u7ea7\u8054\u4f18\u5316\u6846\u67b6\uff0c\u5229\u75283D\u9ad8\u65af\u6e85\u5c04\u4e2d\u7684\u7ed3\u6784\u6a21\u5f0f\u8fdb\u884c\u52a8\u6001\u573a\u666f\u5efa\u6a21\uff0c\u5b9e\u73b0\u8ddf\u8e2a\u548c\u65b0\u89c6\u89d2\u5408\u6210\u3002", "motivation": "\u6355\u6349\u7cbe\u786e\u7684\u5f62\u53d8\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u4ece\u7c97\u7565\u7684\u90e8\u4ef6\u7ea7\u522b\u5230\u7cbe\u7ec6\u7684\u70b9\u7ea7\u522b\u9010\u6b65\u7ec6\u5316\u5f62\u53d8\uff0c\u5229\u7528\u9ad8\u65af\u7ec4\u5171\u4eab\u76f8\u4f3c\u53d8\u6362\u7684\u5c42\u7ea7\u6a21\u5f0f\u3002", "result": "\u5728\u6bcf\u4e2a\u65f6\u95f4\u5e27\u5185100\u6b21\u8fed\u4ee3\u5185\u5b9e\u73b0\u6536\u655b\uff0c\u5e76\u4ee5\u73b0\u6709\u65b9\u6cd5\u4e8c\u5341\u5206\u4e4b\u4e00\u7684\u8bad\u7ec3\u8fed\u4ee3\u6b21\u6570\u4ea7\u751f\u53ef\u6bd4\u8f83\u7684\u7ed3\u679c\u3002\u5728\u81ea\u76d1\u7763\u94f0\u63a5\u5bf9\u8c61\u5206\u5272\u3001\u65b0\u89c6\u89d2\u5408\u6210\u548c\u5bc6\u96c6\u70b9\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "SCas4D\u901a\u8fc7\u5229\u7528\u52a8\u6001\u573a\u666f\u4e2d\u7684\u7ed3\u6784\u6a21\u5f0f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u52a8\u6001\u573a\u666f\u5efa\u6a21\u3002"}}
{"id": "2510.06672", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06672", "abs": "https://arxiv.org/abs/2510.06672", "authors": ["Udbhav Bamba", "Minghao Fang", "Yifan Yu", "Haizhong Zheng", "Fan Lai"], "title": "XRPO: Pushing the limits of GRPO with Targeted Exploration and Exploitation", "comment": null, "summary": "Reinforcement learning algorithms such as GRPO have driven recent advances in\nlarge language model (LLM) reasoning. While scaling the number of rollouts\nstabilizes training, existing approaches suffer from limited exploration on\nchallenging prompts and leave informative feedback signals underexploited, due\nto context-independent rollout allocation across prompts (e.g., generating 16\nrollouts per prompt) and relying heavily on sparse rewards. This paper presents\nXRPO(eXplore - eXploit GRPO), a unified framework that recasts policy\noptimization through the principled lens of rollout exploration-exploitation.\nTo enhance exploration, XRPO introduces a mathematically grounded rollout\nallocator that adaptively prioritizes prompts with higher potential for\nuncertainty reduction. It further addresses stagnation on zero-reward prompts\nthrough an in-context seeding strategy that injects curated exemplars, steering\nthe model into more difficult reasoning trajectories. To strengthen\nexploitation, XRPO develops a group-relative, novelty-aware advantage\nsharpening mechanism that leverages sequence likelihoods to amplify\nlow-probability yet correct responses, thereby extending the policy's reach\nbeyond sparse rewards. Experiments across diverse math and coding benchmarks on\nboth reasoning and non-reasoning models demonstrate that XRPO outperforms\nexisting advances (e.g., GRPO and GSPO) up to 4% pass@1 and 6% cons@32, while\naccelerating training convergence by up to 2.7X.", "AI": {"tldr": "XRPO: A reinforcement learning framework that improves large language model reasoning by balancing exploration and exploitation.", "motivation": "Existing reinforcement learning methods for large language models suffer from limited exploration and underexploited feedback signals.", "method": "XRPO introduces an adaptive rollout allocator, in-context seeding, and a group-relative, novelty-aware advantage sharpening mechanism.", "result": "XRPO outperforms existing methods on math and coding benchmarks, improving performance and accelerating training convergence.", "conclusion": "XRPO is a unified framework that enhances policy optimization through rollout exploration-exploitation."}}
{"id": "2510.06743", "categories": ["cs.CV", "cs.AI", "cs.CL", "68T50"], "pdf": "https://arxiv.org/pdf/2510.06743", "abs": "https://arxiv.org/abs/2510.06743", "authors": ["Maria Levchenko"], "title": "Evaluating LLMs for Historical Document OCR: A Methodological Framework for Digital Humanities", "comment": "The First Workshop on Natural Language Processing and Language Models\n  for Digital Humanities (LM4DH 2025). RANLP 2025", "summary": "Digital humanities scholars increasingly use Large Language Models for\nhistorical document digitization, yet lack appropriate evaluation frameworks\nfor LLM-based OCR. Traditional metrics fail to capture temporal biases and\nperiod-specific errors crucial for historical corpus creation. We present an\nevaluation methodology for LLM-based historical OCR, addressing contamination\nrisks and systematic biases in diplomatic transcription. Using 18th-century\nRussian Civil font texts, we introduce novel metrics including Historical\nCharacter Preservation Rate (HCPR) and Archaic Insertion Rate (AIR), alongside\nprotocols for contamination control and stability testing. We evaluate 12\nmultimodal LLMs, finding that Gemini and Qwen models outperform traditional OCR\nwhile exhibiting over-historicization: inserting archaic characters from\nincorrect historical periods. Post-OCR correction degrades rather than improves\nperformance. Our methodology provides digital humanities practitioners with\nguidelines for model selection and quality assessment in historical corpus\ndigitization.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9LLM\u5728\u5386\u53f2\u6587\u732e\u6570\u5b57\u5316\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edfOCR\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u6355\u6349\u7684\u65f6\u95f4\u504f\u5dee\u548c\u7279\u5b9a\u65f6\u671f\u9519\u8bef\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6570\u5b57\u4eba\u6587\u7814\u7a76\u5b66\u8005\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5386\u53f2\u6587\u732e\u6570\u5b57\u5316\uff0c\u4f46\u662f\u7f3a\u4e4f\u5408\u9002\u7684\u57fa\u4e8eLLM\u7684OCR\u8bc4\u4f30\u6846\u67b6\u3002\u4f20\u7edf\u7684\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u6355\u6349\u65f6\u95f4\u504f\u5dee\u548c\u7279\u5b9a\u65f6\u671f\u9519\u8bef\uff0c\u8fd9\u5bf9\u4e8e\u521b\u5efa\u5386\u53f2\u8bed\u6599\u5e93\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u5386\u53f2OCR\u8bc4\u4f30\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5916\u4ea4\u8f6c\u5f55\u4e2d\u7684\u6c61\u67d3\u98ce\u9669\u548c\u7cfb\u7edf\u6027\u504f\u5dee\u3002\u4f7f\u752818\u4e16\u7eaa\u7684\u4fc4\u7f57\u65af \u0433\u0440\u0430\u0436\u0434\u0430\u043d\u0441\u043a\u0438\u0439 \u0448\u0440\u0438\u0444\u0442 \u6587\u672c\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5305\u62ec\u5386\u53f2\u5b57\u7b26\u4fdd\u7559\u7387 (HCPR) \u548c\u53e4\u8bed\u63d2\u5165\u7387 (AIR) \u5728\u5185\u7684\u65b0\u6307\u6807\uff0c\u4ee5\u53ca\u6c61\u67d3\u63a7\u5236\u548c\u7a33\u5b9a\u6027\u6d4b\u8bd5\u7684\u534f\u8bae\u3002", "result": "\u6211\u4eec\u8bc4\u4f30\u4e86 12 \u4e2a\u591a\u6a21\u6001 LLM\uff0c\u53d1\u73b0 Gemini \u548c Qwen \u6a21\u578b\u4f18\u4e8e\u4f20\u7edf OCR\uff0c\u4f46\u4e5f\u8868\u73b0\u51fa\u8fc7\u5ea6\u5386\u53f2\u5316\u7684\u95ee\u9898\uff0c\u5373\u63d2\u5165\u4e86\u6765\u81ea\u4e0d\u6b63\u786e\u7684\u5386\u53f2\u65f6\u671f\u7684\u53e4\u4f53\u5b57\u7b26\u3002OCR \u540e\u7684\u6821\u6b63\u53cd\u800c\u964d\u4f4e\u4e86\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u8bba\u4e3a\u6570\u5b57\u4eba\u6587\u7814\u7a76\u4ece\u4e1a\u8005\u5728\u5386\u53f2\u8bed\u6599\u5e93\u6570\u5b57\u5316\u4e2d\u9009\u62e9\u6a21\u578b\u548c\u8fdb\u884c\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2510.06680", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06680", "abs": "https://arxiv.org/abs/2510.06680", "authors": ["Zhipeng Liu", "Peibo Duan", "Xuan Tang", "Baixin Li", "Yongsheng Huang", "Mingyang Geng", "Changsheng Zhang", "Bin Zhang", "Binwu Wang"], "title": "TimeFormer: Transformer with Attention Modulation Empowered by Temporal Characteristics for Time Series Forecasting", "comment": null, "summary": "Although Transformers excel in natural language processing, their extension\nto time series forecasting remains challenging due to insufficient\nconsideration of the differences between textual and temporal modalities. In\nthis paper, we develop a novel Transformer architecture designed for time\nseries data, aiming to maximize its representational capacity. We identify two\nkey but often overlooked characteristics of time series: (1) unidirectional\ninfluence from the past to the future, and (2) the phenomenon of decaying\ninfluence over time. These characteristics are introduced to enhance the\nattention mechanism of Transformers. We propose TimeFormer, whose core\ninnovation is a self-attention mechanism with two modulation terms (MoSA),\ndesigned to capture these temporal priors of time series under the constraints\nof the Hawkes process and causal masking. Additionally, TimeFormer introduces a\nframework based on multi-scale and subsequence analysis to capture semantic\ndependencies at different temporal scales, enriching the temporal dependencies.\nExtensive experiments conducted on multiple real-world datasets show that\nTimeFormer significantly outperforms state-of-the-art methods, achieving up to\na 7.45% reduction in MSE compared to the best baseline and setting new\nbenchmarks on 94.04\\% of evaluation metrics. Moreover, we demonstrate that the\nMoSA mechanism can be broadly applied to enhance the performance of other\nTransformer-based models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTimeFormer\u7684\u65b0\u578bTransformer\u67b6\u6784\uff0c\u4e13\u95e8\u4e3a\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8bbe\u8ba1\uff0c\u65e8\u5728\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u5176\u8868\u5f81\u80fd\u529b\u3002", "motivation": "Transformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7531\u4e8e\u5bf9\u6587\u672c\u548c\u65f6\u95f4\u6a21\u6001\u4e4b\u95f4\u5dee\u5f02\u7684\u8003\u8651\u4e0d\u8db3\uff0c\u56e0\u6b64\u6269\u5c55\u5230\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "TimeFormer\u7684\u6838\u5fc3\u521b\u65b0\u662f\u4e00\u79cd\u5177\u6709\u4e24\u4e2a\u8c03\u5236\u9879\uff08MoSA\uff09\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u65e8\u5728\u6355\u83b7Hawkes\u8fc7\u7a0b\u548c\u56e0\u679c\u63a9\u853d\u7ea6\u675f\u4e0b\u7684\u65f6\u95f4\u5e8f\u5217\u7684\u65f6\u95f4\u5148\u9a8c\u3002\u6b64\u5916\uff0cTimeFormer\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u591a\u5c3a\u5ea6\u548c\u5b50\u5e8f\u5217\u5206\u6790\u7684\u6846\u67b6\uff0c\u4ee5\u6355\u83b7\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u7684\u8bed\u4e49\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e30\u5bcc\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cTimeFormer\u663e\u7740\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u4e0e\u6700\u4f73\u57fa\u7ebf\u76f8\u6bd4\uff0cMSE\u964d\u4f4e\u4e867.45\uff05\uff0c\u5e76\u572894.04\uff05\u7684\u8bc4\u4f30\u6307\u6807\u4e0a\u8bbe\u7f6e\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "conclusion": "MoSA\u673a\u5236\u53ef\u4ee5\u5e7f\u6cdb\u5e94\u7528\u4e8e\u589e\u5f3a\u5176\u4ed6\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2510.06738", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06738", "abs": "https://arxiv.org/abs/2510.06738", "authors": ["Boyi Zeng", "Lin Chen", "Ziwei He", "Xinbing Wang", "Zhouhan Lin"], "title": "AWM: Accurate Weight-Matrix Fingerprint for Large Language Models", "comment": null, "summary": "Protecting the intellectual property of large language models (LLMs) is\ncrucial, given the substantial resources required for their training.\nConsequently, there is an urgent need for both model owners and third parties\nto determine whether a suspect LLM is trained from scratch or derived from an\nexisting base model. However, the intensive post-training processes that models\ntypically undergo-such as supervised fine-tuning, extensive continued\npretraining, reinforcement learning, multi-modal extension, pruning, and\nupcycling-pose significant challenges to reliable identification. In this work,\nwe propose a training-free fingerprinting method based on weight matrices. We\nleverage the Linear Assignment Problem (LAP) and an unbiased Centered Kernel\nAlignment (CKA) similarity to neutralize the effects of parameter\nmanipulations, yielding a highly robust and high-fidelity similarity metric. On\na comprehensive testbed of 60 positive and 90 negative model pairs, our method\ndemonstrates exceptional robustness against all six aforementioned\npost-training categories while exhibiting a near-zero risk of false positives.\nBy achieving perfect scores on all classification metrics, our approach\nestablishes a strong basis for reliable model lineage verification. Moreover,\nthe entire computation completes within 30s on an NVIDIA 3090 GPU. The code is\navailable at https://github.com/LUMIA-Group/AWM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6743\u91cd\u77e9\u9635\u7684\u514d\u8bad\u7ec3\u6307\u7eb9\u8bc6\u522b\u65b9\u6cd5\uff0c\u7528\u4e8e\u786e\u5b9a\u53ef\u7591LLM\u662f\u5426\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u6216\u884d\u751f\u81ea\u73b0\u6709\u57fa\u7840\u6a21\u578b\u3002", "motivation": "\u4fdd\u62a4\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u77e5\u8bc6\u4ea7\u6743\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u4eec\u7684\u8bad\u7ec3\u9700\u8981\u5927\u91cf\u8d44\u6e90\u3002\u56e0\u6b64\uff0c\u6a21\u578b\u6240\u6709\u8005\u548c\u7b2c\u4e09\u65b9\u8feb\u5207\u9700\u8981\u786e\u5b9a\u53ef\u7591\u7684LLM\u662f\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u8fd8\u662f\u4ece\u73b0\u6709\u57fa\u7840\u6a21\u578b\u6d3e\u751f\u7684\u3002\u7136\u800c\uff0c\u6a21\u578b\u901a\u5e38\u7ecf\u8fc7\u7684\u5f3a\u5316\u5b66\u4e60\u7b49\u5bc6\u96c6\u7684\u540e\u8bad\u7ec3\u8fc7\u7a0b\u5bf9\u53ef\u9760\u8bc6\u522b\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u5229\u7528\u7ebf\u6027\u5206\u914d\u95ee\u9898\uff08LAP\uff09\u548c\u65e0\u504f\u4e2d\u5fc3\u6838\u5bf9\u9f50\uff08CKA\uff09\u76f8\u4f3c\u6027\u6765\u6d88\u9664\u53c2\u6570\u64cd\u4f5c\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u4ea7\u751f\u9ad8\u5ea6\u9c81\u68d2\u548c\u9ad8\u4fdd\u771f\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u3002", "result": "\u5728\u5305\u542b60\u4e2a\u9633\u6027\u548c90\u4e2a\u9634\u6027\u6a21\u578b\u5bf9\u7684\u7efc\u5408\u6d4b\u8bd5\u5e73\u53f0\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u516d\u4e2a\u4e0a\u8ff0\u540e\u8bad\u7ec3\u7c7b\u522b\u4e2d\u5747\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u8868\u73b0\u51fa\u63a5\u8fd1\u4e8e\u96f6\u7684\u5047\u9633\u6027\u98ce\u9669\u3002\u901a\u8fc7\u5728\u6240\u6709\u5206\u7c7b\u6307\u6807\u4e0a\u83b7\u5f97\u6ee1\u5206\uff0c\u8be5\u65b9\u6cd5\u4e3a\u53ef\u9760\u7684\u6a21\u578b\u8c31\u7cfb\u9a8c\u8bc1\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53ef\u9760\u7684\u6a21\u578b\u8c31\u7cfb\u9a8c\u8bc1\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u5e76\u4e14\u5728NVIDIA 3090 GPU\u4e0a\uff0c\u6574\u4e2a\u8ba1\u7b97\u572830\u79d2\u5185\u5b8c\u6210\u3002"}}
{"id": "2510.06746", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06746", "abs": "https://arxiv.org/abs/2510.06746", "authors": ["Zhiliang Zhu", "Tao Zeng", "Tao Yang", "Guoliang Luo", "Jiyong Zeng"], "title": "DeRainMamba: A Frequency-Aware State Space Model with Detail Enhancement for Image Deraining", "comment": "accepted by IEEE SPL", "summary": "Image deraining is crucial for improving visual quality and supporting\nreliable downstream vision tasks. Although Mamba-based models provide efficient\nsequence modeling, their limited ability to capture fine-grained details and\nlack of frequency-domain awareness restrict further improvements. To address\nthese issues, we propose DeRainMamba, which integrates a Frequency-Aware\nState-Space Module (FASSM) and Multi-Directional Perception Convolution\n(MDPConv). FASSM leverages Fourier transform to distinguish rain streaks from\nhigh-frequency image details, balancing rain removal and detail preservation.\nMDPConv further restores local structures by capturing anisotropic gradient\nfeatures and efficiently fusing multiple convolution branches. Extensive\nexperiments on four public benchmarks demonstrate that DeRainMamba consistently\noutperforms state-of-the-art methods in PSNR and SSIM, while requiring fewer\nparameters and lower computational costs. These results validate the\neffectiveness of combining frequency-domain modeling and spatial detail\nenhancement within a state-space framework for single image deraining.", "AI": {"tldr": "DeRainMamba: Combines frequency-domain modeling and spatial detail enhancement within a state-space framework for single image deraining.", "motivation": "Mamba-based models have limited ability to capture fine-grained details and lack frequency-domain awareness in image deraining.", "method": "Proposes DeRainMamba, integrating a Frequency-Aware State-Space Module (FASSM) and Multi-Directional Perception Convolution (MDPConv).", "result": "Outperforms state-of-the-art methods in PSNR and SSIM on four public benchmarks with fewer parameters and lower computational costs.", "conclusion": "Combining frequency-domain modeling and spatial detail enhancement within a state-space framework is effective for single image deraining."}}
{"id": "2510.06683", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06683", "abs": "https://arxiv.org/abs/2510.06683", "authors": ["Daoyuan Zhou", "Xuchuang Wang", "Lin Yang", "Yang Gao"], "title": "Distributed Algorithms for Multi-Agent Multi-Armed Bandits with Collision", "comment": "21 pages, 4 figures", "summary": "We study the stochastic Multiplayer Multi-Armed Bandit (MMAB) problem, where\nmultiple players select arms to maximize their cumulative rewards. Collisions\noccur when two or more players select the same arm, resulting in no reward, and\nare observed by the players involved. We consider a distributed setting without\ncentral coordination, where each player can only observe their own actions and\ncollision feedback. We propose a distributed algorithm with an adaptive,\nefficient communication protocol. The algorithm achieves near-optimal group and\nindividual regret, with a communication cost of only $\\mathcal{O}(\\log\\log T)$.\nOur experiments demonstrate significant performance improvements over existing\nbaselines. Compared to state-of-the-art (SOTA) methods, our approach achieves a\nnotable reduction in individual regret. Finally, we extend our approach to a\nperiodic asynchronous setting, proving the lower bound for this problem and\npresenting an algorithm that achieves logarithmic regret.", "AI": {"tldr": "\u7814\u7a76\u968f\u673a\u591a\u4eba\u591a\u81c2\u8001\u864e\u673a\uff08MMAB\uff09\u95ee\u9898\uff0c\u591a\u4e2a\u73a9\u5bb6\u9009\u62e9\u81c2\u4ee5\u6700\u5927\u5316\u7d2f\u79ef\u5956\u52b1\u3002\u5f53\u4e24\u4e2a\u6216\u591a\u4e2a\u73a9\u5bb6\u9009\u62e9\u540c\u4e00\u81c2\u65f6\u53d1\u751f\u78b0\u649e\uff0c\u5bfc\u81f4\u6ca1\u6709\u5956\u52b1\uff0c\u5e76\u4e14\u88ab\u76f8\u5173\u73a9\u5bb6\u89c2\u5bdf\u5230\u3002", "motivation": "\u5728\u6ca1\u6709\u4e2d\u5fc3\u534f\u8c03\u7684\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\uff0c\u6bcf\u4e2a\u73a9\u5bb6\u53ea\u80fd\u89c2\u5bdf\u5230\u81ea\u5df1\u7684\u884c\u4e3a\u548c\u78b0\u649e\u53cd\u9988\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u81ea\u9002\u5e94\u3001\u9ad8\u6548\u901a\u4fe1\u534f\u8bae\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\u3002", "result": "\u8be5\u7b97\u6cd5\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u7fa4\u4f53\u548c\u4e2a\u4f53\u540e\u6094\u503c\uff0c\u901a\u4fe1\u6210\u672c\u4ec5\u4e3a$\\\\\\mathcal{O}(\\log\\log T)$\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6027\u80fd\u6709\u663e\u7740\u63d0\u9ad8\u3002\u4e0e\u6700\u5148\u8fdb\u7684\uff08SOTA\uff09\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u51cf\u5c11\u4e2a\u4f53\u540e\u6094\u503c\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u6210\u679c\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u6269\u5c55\u5230\u5468\u671f\u6027\u5f02\u6b65\u8bbe\u7f6e\uff0c\u8bc1\u660e\u4e86\u8be5\u95ee\u9898\u7684\u4e0b\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u73b0\u5bf9\u6570\u540e\u6094\u503c\u7684\u7b97\u6cd5\u3002", "conclusion": "\u5728\u5468\u671f\u6027\u5f02\u6b65\u8bbe\u7f6e\u4e0b\uff0c\u7b97\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u6570\u540e\u6094\u503c"}}
{"id": "2510.06747", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06747", "abs": "https://arxiv.org/abs/2510.06747", "authors": ["I-Fan Lin", "Faegheh Hasibi", "Suzan Verberne"], "title": "TWIST: Training-free and Label-free Short Text Clustering through Iterative Vector Updating with LLMs", "comment": null, "summary": "In this paper, we propose a training-free and label-free method for short\ntext clustering that can be used on top of any existing embedder. In the\ncontext of customer-facing chatbots, companies are dealing with large amounts\nof user utterances that need to be clustered according to their intent. In\nthese commercial settings, no labeled data is typically available, and the\nnumber of clusters is not known. Our method is based on iterative vector\nupdating: it constructs sparse vectors based on representative texts, and then\niteratively refines them through LLM guidance. Our method achieves comparable\nor superior results to state-of-the-art methods that use contrastive learning,\nbut without assuming prior knowledge of clusters or labels. Experiments on\ndiverse datasets and smaller LLMs show that our method is model agnostic and\ncan be applied to any embedder, with relatively small LLMs, and different\nclustering methods. We also show that our method scales to large datasets,\nreducing the computational cost of the LLM. These low-resource, adaptable\nsettings and the scalability of our method make it more aligned with real-world\nscenarios than existing clustering methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u548c\u6807\u7b7e\u7684\u77ed\u6587\u672c\u805a\u7c7b\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u4efb\u4f55\u73b0\u6709\u7684\u5d4c\u5165\u5668\u3002", "motivation": "\u516c\u53f8\u9700\u8981\u6839\u636e\u7528\u6237\u7684\u610f\u56fe\u5bf9\u5927\u91cf\u7528\u6237\u8bdd\u8bed\u8fdb\u884c\u805a\u7c7b\uff0c\u4f46\u901a\u5e38\u6ca1\u6709\u53ef\u7528\u7684\u6807\u8bb0\u6570\u636e\uff0c\u5e76\u4e14\u805a\u7c7b\u7684\u6570\u91cf\u662f\u672a\u77e5\u7684\u3002", "method": "\u8be5\u65b9\u6cd5\u57fa\u4e8e\u8fed\u4ee3\u5411\u91cf\u66f4\u65b0\uff1a\u5b83\u6784\u5efa\u57fa\u4e8e\u4ee3\u8868\u6027\u6587\u672c\u7684\u7a00\u758f\u5411\u91cf\uff0c\u7136\u540e\u901a\u8fc7 LLM \u6307\u5bfc\u8fed\u4ee3\u5730\u7ec6\u5316\u5b83\u4eec\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e0e\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u7684\u6700\u65b0\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u4f46\u6ca1\u6709\u5047\u8bbe\u96c6\u7fa4\u6216\u6807\u7b7e\u7684\u5148\u9a8c\u77e5\u8bc6\u3002\u5728\u4e0d\u540c\u7684\u6570\u636e\u96c6\u548c\u8f83\u5c0f\u7684 LLM \u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0e\u6a21\u578b\u65e0\u5173\uff0c\u53ef\u4ee5\u5e94\u7528\u4e8e\u4efb\u4f55\u5d4c\u5165\u5668\uff0c\u5177\u6709\u76f8\u5bf9\u8f83\u5c0f\u7684 LLM \u548c\u4e0d\u540c\u7684\u805a\u7c7b\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u53ef\u4ee5\u6269\u5c55\u5230\u5927\u578b\u6570\u636e\u96c6\uff0c\u4ece\u800c\u964d\u4f4e LLM \u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u8fd9\u4e9b\u4f4e\u8d44\u6e90\u3001\u9002\u5e94\u6027\u5f3a\u7684\u8bbe\u7f6e\u548c\u8be5\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u4f7f\u5176\u6bd4\u73b0\u6709\u7684\u805a\u7c7b\u65b9\u6cd5\u66f4\u7b26\u5408\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2510.06751", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06751", "abs": "https://arxiv.org/abs/2510.06751", "authors": ["Junhan Zhu", "Hesong Wang", "Mingluo Su", "Zefang Wang", "Huan Wang"], "title": "OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot", "comment": null, "summary": "Large-scale text-to-image diffusion models, while powerful, suffer from\nprohibitive computational cost. Existing one-shot network pruning methods can\nhardly be directly applied to them due to the iterative denoising nature of\ndiffusion models. To bridge the gap, this paper presents OBS-Diff, a novel\none-shot pruning framework that enables accurate and training-free compression\nof large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff\nrevitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex\narchitectures of modern diffusion models and supporting diverse pruning\ngranularity, including unstructured, N:M semi-structured, and structured (MHA\nheads and FFN neurons) sparsity; (ii) To align the pruning criteria with the\niterative dynamics of the diffusion process, by examining the problem from an\nerror-accumulation perspective, we propose a novel timestep-aware Hessian\nconstruction that incorporates a logarithmic-decrease weighting scheme,\nassigning greater importance to earlier timesteps to mitigate potential error\naccumulation; (iii) Furthermore, a computationally efficient group-wise\nsequential pruning strategy is proposed to amortize the expensive calibration\nprocess. Extensive experiments show that OBS-Diff achieves state-of-the-art\none-shot pruning for diffusion models, delivering inference acceleration with\nminimal degradation in visual quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u6b21\u526a\u679d\u6846\u67b6OBS-Diff\uff0c\u7528\u4e8e\u538b\u7f29\u5927\u578b\u6587\u672c\u5230\u56fe\u50cf\u7684\u6269\u6563\u6a21\u578b\uff0c\u4e14\u65e0\u9700\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u7684\u4e00\u6b21\u6027\u7f51\u7edc\u526a\u679d\u65b9\u6cd5\u5f88\u96be\u76f4\u63a5\u5e94\u7528\u4e8e\u6269\u6563\u6a21\u578b\uff0c\u56e0\u4e3a\u6269\u6563\u6a21\u578b\u5177\u6709\u8fed\u4ee3\u53bb\u566a\u7684\u7279\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\uff1a(1) \u6539\u8fdb\u4e86\u7ecf\u5178\u7684\u6700\u4f18\u8111\u5916\u79d1\u624b\u672f(OBS)\uff1b(2) \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u95f4\u6b65\u611f\u77e5Hessian\u6784\u9020\uff0c\u5b83\u7ed3\u5408\u4e86\u5bf9\u6570\u9012\u51cf\u52a0\u6743\u65b9\u6848\uff1b(3) \u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u5206\u7ec4\u987a\u5e8f\u526a\u679d\u7b56\u7565\u3002", "result": "OBS-Diff\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6269\u6563\u6a21\u578b\u5355\u6b21\u526a\u679d\uff0c\u5728\u89c6\u89c9\u8d28\u91cf \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u043c \u964d\u4f4e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "OBS-Diff \u662f\u4e00\u79cd\u6709\u6548\u7684\u6269\u6563\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\u3002"}}
{"id": "2510.06684", "categories": ["cs.LG", "cs.NA", "math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.06684", "abs": "https://arxiv.org/abs/2510.06684", "authors": ["Kang An", "Chenhao Si", "Ming Yan", "Shiqian Ma"], "title": "AutoBalance: An Automatic Balancing Framework for Training Physics-Informed Neural Networks", "comment": "23 pages", "summary": "Physics-Informed Neural Networks (PINNs) provide a powerful and general\nframework for solving Partial Differential Equations (PDEs) by embedding\nphysical laws into loss functions. However, training PINNs is notoriously\ndifficult due to the need to balance multiple loss terms, such as PDE residuals\nand boundary conditions, which often have conflicting objectives and vastly\ndifferent curvatures. Existing methods address this issue by manipulating\ngradients before optimization (a \"pre-combine\" strategy). We argue that this\napproach is fundamentally limited, as forcing a single optimizer to process\ngradients from spectrally heterogeneous loss landscapes disrupts its internal\npreconditioning. In this work, we introduce AutoBalance, a novel \"post-combine\"\ntraining paradigm. AutoBalance assigns an independent adaptive optimizer to\neach loss component and aggregates the resulting preconditioned updates\nafterwards. Extensive experiments on challenging PDE benchmarks show that\nAutoBalance consistently outperforms existing frameworks, achieving significant\nreductions in solution error, as measured by both the MSE and $L^{\\infty}$\nnorms. Moreover, AutoBalance is orthogonal to and complementary with other\npopular PINN methodologies, amplifying their effectiveness on demanding\nbenchmarks.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f AutoBalance\uff0c\u7528\u4e8e\u89e3\u51b3\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc (PINN) \u8bad\u7ec3\u4e2d\u5e73\u8861\u591a\u4e2a\u635f\u5931\u9879\u7684\u95ee\u9898\u3002", "motivation": "PINN \u8bad\u7ec3\u56f0\u96be\uff0c\u56e0\u4e3a\u9700\u8981\u5e73\u8861\u591a\u4e2a\u635f\u5931\u9879\uff08\u4f8b\u5982 PDE \u6b8b\u5dee\u548c\u8fb9\u754c\u6761\u4ef6\uff09\uff0c\u8fd9\u4e9b\u635f\u5931\u9879\u901a\u5e38\u5177\u6709\u51b2\u7a81\u7684\u76ee\u6807\u548c\u5dee\u5f02\u5f88\u5927\u7684\u66f2\u7387\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u8fc7\u5728\u4f18\u5316\u4e4b\u524d\u64cd\u7eb5\u68af\u5ea6\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff08\u4e00\u79cd\u201c\u9884\u7ec4\u5408\u201d\u7b56\u7565\uff09\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u4ece\u6839\u672c\u4e0a\u662f\u6709\u9650\u7684\u3002", "method": "AutoBalance \u4e3a\u6bcf\u4e2a\u635f\u5931\u5206\u91cf\u5206\u914d\u4e00\u4e2a\u72ec\u7acb\u7684\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff0c\u5e76\u5728\u4e4b\u540e\u805a\u5408\u7531\u6b64\u4ea7\u751f\u7684\u9884\u5904\u7406\u66f4\u65b0\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684 PDE \u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cAutoBalance \u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u6846\u67b6\uff0c\u5728\u89e3\u51b3\u65b9\u6848\u8bef\u5dee\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u7684\u964d\u4f4e\u3002", "conclusion": "AutoBalance \u4e0e\u5176\u4ed6\u6d41\u884c\u7684 PINN \u65b9\u6cd5\u6b63\u4ea4\u4e14\u4e92\u8865\uff0c\u4ece\u800c\u5728\u8981\u6c42\u82db\u523b\u7684\u57fa\u51c6\u4e0a\u653e\u5927\u4e86\u5b83\u4eec\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.06749", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06749", "abs": "https://arxiv.org/abs/2510.06749", "authors": ["Eitan Klinger", "Zihao Huang", "Tran Minh Nguyen", "Emma Jayeon Park", "Yige Chen", "Yang Gu", "Qingyu Gao", "Siliang Liu", "Mengyang Qiu", "Jungyeul Park"], "title": "A Formal Framework for Fluency-based Multi-Reference Evaluation in Grammatical Error Correction", "comment": "Submitted to ACL Rolling Review - October 2025 for EACL 2026", "summary": "Evaluating grammatical error correction requires metrics that reflect the\ndiversity of valid human corrections rather than privileging a single\nreference. Existing frameworks, largely edit-based and English-centric, rely on\nrigid alignments between system and reference edits, limiting their\napplicability in multilingual and generative settings. This paper introduces a\nformal framework for \\textit{fluency-based multi-reference evaluation}, framing\n$n$-gram similarity as an aggregation problem over multiple legitimate\ncorrections. Within this formulation, we instantiate GLEU through four\naggregation strategies--\\textsc{select-best}, \\textsc{simple-average},\n\\textsc{weighted-average}, and \\textsc{merged-counts}--and analyze their\nproperties of boundedness, monotonicity, and sensitivity to reference\nvariation. Empirical results on Czech, Estonian, Ukrainian, and Chinese corpora\nshow that these strategies capture complementary aspects of fluency and\ncoverage. The framework unifies multi-reference evaluation into a principled,\nfluency-oriented approach that incorporates linguistic diversity without\npenalizing legitimate variation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6d41\u7545\u5ea6\u7684\u591a\u53c2\u8003\u8bc4\u4f30\u7684\u6b63\u5f0f\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bed\u6cd5\u7ea0\u9519\u8bc4\u4f30\u6307\u6807\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u8bc4\u4f30\u6846\u67b6\u4e3b\u8981\u57fa\u4e8e\u7f16\u8f91\uff0c\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\uff0c\u4f9d\u8d56\u4e8e\u7cfb\u7edf\u548c\u53c2\u8003\u7f16\u8f91\u4e4b\u95f4\u7684\u4e25\u683c\u5bf9\u9f50\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u591a\u8bed\u8a00\u548c\u751f\u6210\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u5c06n-gram\u76f8\u4f3c\u6027\u6784\u5efa\u4e3a\u4e00\u4e2a\u5728\u591a\u4e2a\u5408\u7406\u4fee\u6b63\u4e0a\u8fdb\u884c\u7684\u805a\u5408\u95ee\u9898\u3002\u901a\u8fc7\u56db\u79cd\u805a\u5408\u7b56\u7565\u5b9e\u4f8b\u5316GLEU\uff1aselect-best\uff0csimple-average\uff0cweighted-average\u548cmerged-counts\uff0c\u5e76\u5206\u6790\u5b83\u4eec\u7684\u6709\u754c\u6027\u3001\u5355\u8c03\u6027\u548c\u5bf9\u53c2\u8003\u53d8\u5f02\u7684\u654f\u611f\u6027\u3002", "result": "\u5728\u6377\u514b\u8bed\u3001\u7231\u6c99\u5c3c\u4e9a\u8bed\u3001\u4e4c\u514b\u5170\u8bed\u548c\u6c49\u8bed\u8bed\u6599\u5e93\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u7b56\u7565\u6355\u6349\u5230\u4e86\u6d41\u7545\u6027\u548c\u8986\u76d6\u7387\u7684\u4e92\u8865\u65b9\u9762\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06\u591a\u53c2\u8003\u8bc4\u4f30\u7edf\u4e00\u4e3a\u4e00\u4e2a\u57fa\u4e8e\u539f\u5219\u7684\u3001\u9762\u5411\u6d41\u7545\u6027\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u8bed\u8a00\u591a\u6837\u6027\uff0c\u800c\u4e0d\u4f1a\u60e9\u7f5a\u5408\u6cd5\u7684\u53d8\u5f02\u3002"}}
{"id": "2510.06757", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06757", "abs": "https://arxiv.org/abs/2510.06757", "authors": ["Sheng Fu", "Junchao Zhang", "Kailun Yang"], "title": "Transforming Noise Distributions with Histogram Matching: Towards a Single Denoiser for All", "comment": "12 pages", "summary": "Supervised Gaussian denoisers exhibit limited generalization when confronted\nwith out-of-distribution noise, due to the diverse distributional\ncharacteristics of different noise types. To bridge this gap, we propose a\nhistogram matching approach that transforms arbitrary noise towards a target\nGaussian distribution with known intensity. Moreover, a mutually reinforcing\ncycle is established between noise transformation and subsequent denoising.\nThis cycle progressively refines the noise to be converted, making it\napproximate the real noise, thereby enhancing the noise transformation effect\nand further improving the denoising performance. We tackle specific noise\ncomplexities: local histogram matching handles signal-dependent noise,\nintrapatch permutation processes channel-related noise, and frequency-domain\nhistogram matching coupled with pixel-shuffle down-sampling breaks spatial\ncorrelation. By applying these transformations, a single Gaussian denoiser\ngains remarkable capability to handle various out-of-distribution noises,\nincluding synthetic noises such as Poisson, salt-and-pepper and repeating\npattern noises, as well as complex real-world noises. Extensive experiments\ndemonstrate the superior generalization and effectiveness of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u65b9\u56fe\u5339\u914d\u65b9\u6cd5\uff0c\u5c06\u4efb\u610f\u566a\u58f0\u8f6c\u6362\u4e3a\u5177\u6709\u5df2\u77e5\u5f3a\u5ea6\u7684\u76ee\u6807\u9ad8\u65af\u5206\u5e03\uff0c\u4ece\u800c\u63d0\u9ad8\u53bb\u566a\u6027\u80fd\u3002", "motivation": "\u76d1\u7763\u9ad8\u65af\u53bb\u566a\u5668\u5728\u9762\u5bf9\u5206\u5e03\u5916\u566a\u58f0\u65f6\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u56e0\u4e3a\u4e0d\u540c\u566a\u58f0\u7c7b\u578b\u7684\u5206\u5e03\u7279\u5f81\u5404\u4e0d\u76f8\u540c\u3002", "method": "\u5728\u566a\u58f0\u8f6c\u6362\u548c\u540e\u7eed\u53bb\u566a\u4e4b\u95f4\u5efa\u7acb\u4e00\u4e2a\u76f8\u4e92\u52a0\u5f3a\u7684\u5faa\u73af\uff0c\u9010\u6b65\u5b8c\u5584\u8981\u8f6c\u6362\u7684\u566a\u58f0\uff0c\u4f7f\u5176\u63a5\u8fd1\u771f\u5b9e\u566a\u58f0\u3002\u5c40\u90e8\u76f4\u65b9\u56fe\u5339\u914d\u5904\u7406\u4fe1\u53f7\u76f8\u5173\u566a\u58f0\uff0cpatch\u5185\u7f6e\u6362\u5904\u7406\u901a\u9053\u76f8\u5173\u566a\u58f0\uff0c\u9891\u57df\u76f4\u65b9\u56fe\u5339\u914d\u4e0e\u50cf\u7d20-shuffle\u4e0b\u91c7\u6837\u76f8\u7ed3\u5408\uff0c\u6253\u7834\u7a7a\u95f4\u76f8\u5173\u6027\u3002", "result": "\u901a\u8fc7\u5e94\u7528\u8fd9\u4e9b\u8f6c\u6362\uff0c\u5355\u4e2a\u9ad8\u65af\u53bb\u566a\u5668\u83b7\u5f97\u4e86\u5904\u7406\u5404\u79cd\u5206\u5e03\u5916\u566a\u58f0\u7684\u5353\u8d8a\u80fd\u529b\uff0c\u5305\u62ec\u6cca\u677e\u566a\u58f0\u3001\u6912\u76d0\u566a\u58f0\u548c\u91cd\u590d\u6a21\u5f0f\u566a\u58f0\u7b49\u5408\u6210\u566a\u58f0\uff0c\u4ee5\u53ca\u590d\u6742\u7684\u771f\u5b9e\u4e16\u754c\u566a\u58f0\u3002", "conclusion": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u6cdb\u5316\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2510.06692", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06692", "abs": "https://arxiv.org/abs/2510.06692", "authors": ["Akira Ito", "Takayuki Miura", "Yosuke Todo"], "title": "Is the Hard-Label Cryptanalytic Model Extraction Really Polynomial?", "comment": null, "summary": "Deep Neural Networks (DNNs) have attracted significant attention, and their\ninternal models are now considered valuable intellectual assets. Extracting\nthese internal models through access to a DNN is conceptually similar to\nextracting a secret key via oracle access to a block cipher. Consequently,\ncryptanalytic techniques, particularly differential-like attacks, have been\nactively explored recently. ReLU-based DNNs are the most commonly and widely\ndeployed architectures. While early works (e.g., Crypto 2020, Eurocrypt 2024)\nassume access to exact output logits, which are usually invisible, more recent\nworks (e.g., Asiacrypt 2024, Eurocrypt 2025) focus on the hard-label setting,\nwhere only the final classification result (e.g., \"dog\" or \"car\") is available\nto the attacker. Notably, Carlini et al. (Eurocrypt 2025) demonstrated that\nmodel extraction is feasible in polynomial time even under this restricted\nsetting.\n  In this paper, we first show that the assumptions underlying their attack\nbecome increasingly unrealistic as the attack-target depth grows. In practice,\nsatisfying these assumptions requires an exponential number of queries with\nrespect to the attack depth, implying that the attack does not always run in\npolynomial time. To address this critical limitation, we propose a novel attack\nmethod called CrossLayer Extraction. Instead of directly extracting the secret\nparameters (e.g., weights and biases) of a specific neuron, which incurs\nexponential cost, we exploit neuron interactions across layers to extract this\ninformation from deeper layers. This technique significantly reduces query\ncomplexity and mitigates the limitations of existing model extraction\napproaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u63d0\u53d6\u653b\u51fb\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u964d\u4f4e\u67e5\u8be2\u590d\u6742\u5ea6\uff0c\u7f13\u89e3\u73b0\u6709\u6a21\u578b\u63d0\u53d6\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u63d0\u53d6\u65b9\u6cd5\u5728\u653b\u51fb\u6df1\u5ea6\u589e\u52a0\u65f6\uff0c\u5047\u8bbe\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\uff0c\u9700\u8981\u6307\u6570\u7ea7\u7684\u67e5\u8be2\u6b21\u6570\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCrossLayer Extraction\u7684\u65b0\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528\u8de8\u5c42\u795e\u7ecf\u5143\u4ea4\u4e92\u4ece\u66f4\u6df1\u5c42\u63d0\u53d6\u4fe1\u606f\u3002", "result": "\u8be5\u6280\u672f\u663e\u8457\u964d\u4f4e\u4e86\u67e5\u8be2\u590d\u6742\u5ea6\u3002", "conclusion": "CrossLayer Extraction\u653b\u51fb\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u964d\u4f4e\u6a21\u578b\u63d0\u53d6\u7684\u67e5\u8be2\u590d\u6742\u5ea6\uff0c\u7f13\u89e3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.06750", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06750", "abs": "https://arxiv.org/abs/2510.06750", "authors": ["Jaeseong Lee", "Dayoung Kwon", "seung-won hwang"], "title": "Gold-Switch: Training-Free Superposition of Slow- and Fast- Thinking LLMs", "comment": null, "summary": "Large Reasoning Models (LRMs) excel in structured tasks by emulating\ndeliberate human reasoning but often suffer from overthinking, degrading\nperformance and wasting resources. One possible baseline is to deploy both LLM\nand LRM, then route input by predicting whether it requires reasoning and may\ncause overthinking. However, deploying multiple models can be costly or\nimpractical. We propose a superposed deployment strategy with a lightweight,\ntraining-free regulation to optimize inference by switching one model on and\noff. Instead of routing, we selectively unlearn from LRM at inference, scaling\ndown computation while preserving reasoning. By analyzing the cumulative energy\nof singular values, we identify optimal low-rank projections to adjust\nreasoning just right.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5730\u4eceLRM\u4e2d\u201cunlearn\u201d\u6765\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u4fdd\u7559\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRM\uff09\u5728\u7ed3\u6784\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5e38\u5e38\u8fc7\u5ea6\u601d\u8003\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u8d44\u6e90\u6d6a\u8d39\u3002\u90e8\u7f72\u591a\u4e2a\u6a21\u578b\u6210\u672c\u9ad8\u6602\u6216\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5947\u5f02\u503c\u7684\u7d2f\u79ef\u80fd\u91cf\uff0c\u786e\u5b9a\u6700\u4f73\u7684\u4f4e\u79e9\u6295\u5f71\uff0c\u4ee5\u6070\u5f53\u5730\u8c03\u6574\u63a8\u7406\u3002", "result": "\u901a\u8fc7\u9009\u62e9\u6027\u5730\u4eceLRM\u4e2d\u201cunlearn\u201d\uff0c\u5728\u63a8\u7406\u65f6\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u4fdd\u7559\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u8c03\u8282\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f00\u5173\u4e00\u4e2a\u6a21\u578b\u6765\u4f18\u5316\u63a8\u7406\u3002"}}
{"id": "2510.06769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06769", "abs": "https://arxiv.org/abs/2510.06769", "authors": ["Gianmarco Perantoni", "Lorenzo Bruzzone"], "title": "A deep multiple instance learning approach based on coarse labels for high-resolution land-cover mapping", "comment": "14 pages, 4 figures, accepted conference paper at SPIE REMOTE\n  SENSING, 3-7 September 2023, Amsterdam, Netherlands", "summary": "The quantity and the quality of the training labels are central problems in\nhigh-resolution land-cover mapping with machine-learning-based solutions. In\nthis context, weak labels can be gathered in large quantities by leveraging on\nexisting low-resolution or obsolete products. In this paper, we address the\nproblem of training land-cover classifiers using high-resolution imagery (e.g.,\nSentinel-2) and weak low-resolution reference data (e.g., MODIS -derived\nland-cover maps). Inspired by recent works in Deep Multiple Instance Learning\n(DMIL), we propose a method that trains pixel-level multi-class classifiers and\npredicts low-resolution labels (i.e., patch-level classification), where the\nactual high-resolution labels are learned implicitly without direct\nsupervision. This is achieved with flexible pooling layers that are able to\nlink the semantics of the pixels in the high-resolution imagery to the\nlow-resolution reference labels. Then, the Multiple Instance Learning (MIL)\nproblem is re-framed in a multi-class and in a multi-label setting. In the\nformer, the low-resolution annotation represents the majority of the pixels in\nthe patch. In the latter, the annotation only provides us information on the\npresence of one of the land-cover classes in the patch and thus multiple labels\ncan be considered valid for a patch at a time, whereas the low-resolution\nlabels provide us only one label. Therefore, the classifier is trained with a\nPositive-Unlabeled Learning (PUL) strategy. Experimental results on the 2020\nIEEE GRSS Data Fusion Contest dataset show the effectiveness of the proposed\nframework compared to standard training strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u5f31\u4f4e\u5206\u8fa8\u7387\u53c2\u8003\u6570\u636e\u8bad\u7ec3\u571f\u5730\u8986\u76d6\u5206\u7c7b\u5668\u7684\u65b9\u6cd5\u3002", "motivation": "\u5728\u9ad8\u5206\u8fa8\u7387\u571f\u5730\u8986\u76d6\u5236\u56fe\u4e2d\uff0c\u8bad\u7ec3\u6807\u7b7e\u7684\u6570\u91cf\u548c\u8d28\u91cf\u662f\u6838\u5fc3\u95ee\u9898\u3002\u5f31\u6807\u7b7e\u53ef\u4ee5\u901a\u8fc7\u5229\u7528\u73b0\u6709\u7684\u4f4e\u5206\u8fa8\u7387\u6216\u8fc7\u65f6\u7684\u4ea7\u54c1\u5927\u91cf\u6536\u96c6\u3002", "method": "\u8be5\u65b9\u6cd5\u57fa\u4e8e\u6df1\u5ea6\u591a\u793a\u4f8b\u5b66\u4e60\uff08DMIL\uff09\uff0c\u8bad\u7ec3\u50cf\u7d20\u7ea7\u591a\u7c7b\u5206\u7c7b\u5668\u5e76\u9884\u6d4b\u4f4e\u5206\u8fa8\u7387\u6807\u7b7e\uff08\u5373\uff0cpatch-level\u5206\u7c7b\uff09\uff0c\u5176\u4e2d\u5b9e\u9645\u7684\u9ad8\u5206\u8fa8\u7387\u6807\u7b7e\u662f\u5728\u6ca1\u6709\u76f4\u63a5\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u9690\u5f0f\u5b66\u4e60\u7684\u3002\u901a\u8fc7\u7075\u6d3b\u7684\u6c60\u5316\u5c42\u5c06\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u50cf\u7d20\u7684\u8bed\u4e49\u4e0e\u4f4e\u5206\u8fa8\u7387\u53c2\u8003\u6807\u7b7e\u8054\u7cfb\u8d77\u6765\u3002\u591a\u793a\u4f8b\u5b66\u4e60\uff08MIL\uff09\u95ee\u9898\u5728\u591a\u7c7b\u548c\u591a\u6807\u7b7e\u8bbe\u7f6e\u4e2d\u91cd\u65b0\u6784\u5efa\u3002\u5206\u7c7b\u5668\u91c7\u7528Positive-Unlabeled Learning\uff08PUL\uff09\u7b56\u7565\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u57282020\u5e74IEEE GRSS\u6570\u636e\u878d\u5408\u7ade\u8d5b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6807\u51c6\u8bad\u7ec3\u7b56\u7565\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u662f\u6709\u6548\u7684\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5229\u7528\u5f31\u6807\u7b7e\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\u571f\u5730\u8986\u76d6\u5206\u7c7b\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.06699", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06699", "abs": "https://arxiv.org/abs/2510.06699", "authors": ["Gal Fadlon", "Idan Arbiv", "Nimrod Berman", "Omri Azencot"], "title": "A Diffusion Model for Regular Time Series Generation from Irregular Data with Completion and Masking", "comment": "Accepted to NeurIPS 2025; The first two authors contributed equally\n  and are co-leading authors", "summary": "Generating realistic time series data is critical for applications in\nhealthcare, finance, and science. However, irregular sampling and missing\nvalues present significant challenges. While prior methods address these\nirregularities, they often yield suboptimal results and incur high\ncomputational costs. Recent advances in regular time series generation, such as\nthe diffusion-based ImagenTime model, demonstrate strong, fast, and scalable\ngenerative capabilities by transforming time series into image representations,\nmaking them a promising solution. However, extending ImagenTime to irregular\nsequences using simple masking introduces \"unnatural\" neighborhoods, where\nmissing values replaced by zeros disrupt the learning process. To overcome\nthis, we propose a novel two-step framework: first, a Time Series Transformer\ncompletes irregular sequences, creating natural neighborhoods; second, a\nvision-based diffusion model with masking minimizes dependence on the completed\nvalues. This approach leverages the strengths of both completion and masking,\nenabling robust and efficient generation of realistic time series. Our method\nachieves state-of-the-art performance, achieving a relative improvement in\ndiscriminative score by $70\\%$ and in computational cost by $85\\%$. Code is at\nhttps://github.com/azencot-group/ImagenI2R.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e24\u6b65\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u975e\u89c4\u5f8b\u91c7\u6837\u548c\u7f3a\u5931\u503c\u7684\u771f\u5b9e\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002", "motivation": "\u751f\u6210\u771f\u5b9e\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5bf9\u4e8e\u533b\u7597\u4fdd\u5065\u3001\u91d1\u878d\u548c\u79d1\u5b66\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4e0d\u89c4\u5219\u7684\u91c7\u6837\u548c\u7f3a\u5931\u503c\u5e26\u6765\u4e86\u5de8\u5927\u7684\u6311\u6218\u3002", "method": "\u9996\u5148\uff0c\u65f6\u95f4\u5e8f\u5217\u8f6c\u6362\u5668\u5b8c\u6210\u4e0d\u89c4\u5219\u5e8f\u5217\uff0c\u521b\u5efa\u81ea\u7136\u90bb\u57df\uff1b\u5176\u6b21\uff0c\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u6269\u6563\u6a21\u578b\u4e0e\u63a9\u853d\u6700\u5c0f\u5316\u5bf9\u5b8c\u6210\u503c\u7684\u4f9d\u8d56\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u5224\u522b\u5206\u6570\u4e0a\u5b9e\u73b0\u4e86 70% \u7684\u76f8\u5bf9\u63d0\u9ad8\uff0c\u5728\u8ba1\u7b97\u6210\u672c\u4e0a\u5b9e\u73b0\u4e86 85% \u7684\u76f8\u5bf9\u63d0\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u8865\u5168\u548c\u63a9\u853d\u7684\u4f18\u70b9\uff0c\u80fd\u591f\u7a33\u5065\u6709\u6548\u5730\u751f\u6210\u771f\u5b9e\u7684\u65f6\u95f4\u5e8f\u5217\u3002"}}
{"id": "2510.06774", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06774", "abs": "https://arxiv.org/abs/2510.06774", "authors": ["Lei Xu", "Pierre Beckmann", "Marco Valentino", "Andr\u00e9 Freitas"], "title": "Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition", "comment": null, "summary": "Neuro-symbolic NLP methods aim to leverage the complementary strengths of\nlarge language models and formal logical solvers. However, current approaches\nare mostly static in nature, i.e., the integration of a target solver is\npredetermined at design time, hindering the ability to employ diverse formal\ninference strategies. To address this, we introduce an adaptive,\nmulti-paradigm, neuro-symbolic inference framework that: (1) automatically\nidentifies formal reasoning strategies from problems expressed in natural\nlanguage; and (2) dynamically selects and applies specialized formal logical\nsolvers via autoformalization interfaces. Extensive experiments on individual\nand multi-paradigm reasoning tasks support the following conclusions: LLMs are\neffective at predicting the necessary formal reasoning strategies with an\naccuracy above 90 percent. This enables flexible integration with formal\nlogical solvers, resulting in our framework outperforming competing baselines\nby 27 percent and 6 percent compared to GPT-4o and DeepSeek-V3.1, respectively.\nMoreover, adaptive reasoning can even positively impact pure LLM methods,\nyielding gains of 10, 5, and 6 percent on zero-shot, CoT, and symbolic CoT\nsettings with GPT-4o. Finally, although smaller models struggle with adaptive\nneuro-symbolic reasoning, post-training offers a viable path to improvement.\nOverall, this work establishes the foundations for adaptive LLM-symbolic\nreasoning, offering a path forward for unifying material and formal inferences\non heterogeneous reasoning challenges.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u7684\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u6846\u67b6\uff0c\u53ef\u4ee5\u81ea\u52a8\u8bc6\u522b\u5f62\u5f0f\u63a8\u7406\u7b56\u7565\u5e76\u52a8\u6001\u9009\u62e9\u548c\u5e94\u7528\u903b\u8f91\u6c42\u89e3\u5668\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5927\u591a\u662f\u9759\u6001\u7684\uff0c\u65e0\u6cd5\u5229\u7528\u591a\u6837\u5316\u7684\u5f62\u5f0f\u63a8\u7406\u7b56\u7565\u3002", "method": "\u8be5\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u5f62\u5f0f\u5316\u63a5\u53e3\u52a8\u6001\u9009\u62e9\u548c\u5e94\u7528\u4e13\u95e8\u7684\u5f62\u5f0f\u903b\u8f91\u6c42\u89e3\u5668\u3002", "result": "\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u4e14\u81ea\u9002\u5e94\u63a8\u7406\u53ef\u4ee5\u79ef\u6781\u5f71\u54cd\u7eafLLM\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u81ea\u9002\u5e94LLM\u7b26\u53f7\u63a8\u7406\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4e3a\u7edf\u4e00\u5f02\u6784\u63a8\u7406\u6311\u6218\u4e2d\u7684\u7269\u8d28\u548c\u5f62\u5f0f\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u6761\u524d\u8fdb\u7684\u9053\u8def\u3002"}}
{"id": "2510.06783", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06783", "abs": "https://arxiv.org/abs/2510.06783", "authors": ["Akshit Singh", "Shyam Marjit", "Wei Lin", "Paul Gavrikov", "Serena Yeung-Levy", "Hilde Kuehne", "Rogerio Feris", "Sivan Doveh", "James Glass", "M. Jehanzeb Mirza"], "title": "TTRV: Test-Time Reinforcement Learning for Vision Language Models", "comment": null, "summary": "Existing methods for extracting reward signals in Reinforcement Learning\ntypically rely on labeled data and dedicated training splits, a setup that\ncontrasts with how humans learn directly from their environment. In this work,\nwe propose TTRV to enhance vision language understanding by adapting the model\non the fly at inference time, without the need for any labeled data.\nConcretely, we enhance the Group Relative Policy Optimization (GRPO) framework\nby designing rewards based on the frequency of the base model's output, while\ninferring on each test sample multiple times. Further, we also propose to\ncontrol the diversity of the model's output by simultaneously rewarding the\nmodel for obtaining low entropy of the output empirical distribution. Our\napproach delivers consistent gains across both object recognition and visual\nquestion answering (VQA), with improvements of up to 52.4% and 29.8%,\nrespectively, and average boosts of 24.6% and 10.0% across 16\ndatasets.Remarkably, on image recognition, TTRV applied to InternVL 8B\nsurpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining\nhighly competitive on VQA, demonstrating that test-time reinforcement learning\ncan match or exceed the strongest proprietary models. Finally, we find many\ninteresting properties of test-time RL for VLMs: for example, even in extremely\ndata-constrained scenarios, where adaptation is performed on a single randomly\nchosen unlabeled test example, TTRV still yields non-trivial improvements of up\nto 5.5% in recognition tasks.", "AI": {"tldr": "TTRV\u901a\u8fc7\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u6a21\u578b\u8c03\u6574\u6765\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\uff0c\u65e0\u9700\u4efb\u4f55\u6807\u7b7e\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u4e2d\u63d0\u53d6\u5956\u52b1\u4fe1\u53f7\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u6807\u7b7e\u6570\u636e\u548c\u4e13\u95e8\u7684\u8bad\u7ec3\u96c6\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u76f4\u63a5\u4ece\u73af\u5883\u4e2d\u5b66\u4e60\u7684\u65b9\u5f0f\u5f62\u6210\u5bf9\u6bd4\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u8f93\u51fa\u9891\u7387\u7684\u5956\u52b1\u6765\u589e\u5f3aGroup Relative Policy Optimization (GRPO)\u6846\u67b6\uff0c\u540c\u65f6\u591a\u6b21\u63a8\u65ad\u6bcf\u4e2a\u6d4b\u8bd5\u6837\u672c\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u540c\u65f6\u5956\u52b1\u6a21\u578b\u4ee5\u83b7\u5f97\u8f93\u51fa\u7ecf\u9a8c\u5206\u5e03\u7684\u4f4e\u71b5\u6765\u63a7\u5236\u6a21\u578b\u8f93\u51fa\u7684\u591a\u6837\u6027\u3002", "result": "\u5728\u5bf9\u8c61\u8bc6\u522b\u548c\u89c6\u89c9\u95ee\u9898\u56de\u7b54 (VQA) \u65b9\u9762\u5747\u83b7\u5f97\u4e86\u4e00\u81f4\u7684\u6536\u76ca\uff0c\u5206\u522b\u63d0\u9ad8\u4e86\u9ad8\u8fbe 52.4% \u548c 29.8%\uff0c\u5e76\u4e14\u5728 16 \u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e73\u5747\u63d0\u5347\u5206\u522b\u4e3a 24.6% \u548c 10.0%\u3002\u5728\u56fe\u50cf\u8bc6\u522b\u65b9\u9762\uff0c\u5e94\u7528\u4e8e InternVL 8B \u7684 TTRV \u5728 8 \u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u8d85\u8fc7 GPT-4o 2.3%\uff0c\u540c\u65f6\u5728 VQA \u65b9\u9762\u4fdd\u6301\u9ad8\u5ea6\u7ade\u4e89\u529b\u3002", "conclusion": "\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u5339\u914d\u6216\u8d85\u8fc7\u6700\u5f3a\u5927\u7684\u4e13\u6709\u6a21\u578b\u3002\u5373\u4f7f\u5728\u6781\u7aef\u6570\u636e\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5373\u5728\u5355\u4e2a\u968f\u673a\u9009\u62e9\u7684\u672a\u6807\u8bb0\u6d4b\u8bd5\u793a\u4f8b\u4e0a\u6267\u884c\u9002\u5e94\u65f6\uff0cTTRV \u4ecd\u7136\u53ef\u4ee5\u5728\u8bc6\u522b\u4efb\u52a1\u4e2d\u4ea7\u751f\u9ad8\u8fbe 5.5% \u7684\u975e\u5e73\u51e1\u6539\u8fdb\u3002"}}
{"id": "2510.06714", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06714", "abs": "https://arxiv.org/abs/2510.06714", "authors": ["Seohong Park", "Deepinder Mann", "Sergey Levine"], "title": "Dual Goal Representations", "comment": null, "summary": "In this work, we introduce dual goal representations for goal-conditioned\nreinforcement learning (GCRL). A dual goal representation characterizes a state\nby \"the set of temporal distances from all other states\"; in other words, it\nencodes a state through its relations to every other state, measured by\ntemporal distance. This representation provides several appealing theoretical\nproperties. First, it depends only on the intrinsic dynamics of the environment\nand is invariant to the original state representation. Second, it contains\nprovably sufficient information to recover an optimal goal-reaching policy,\nwhile being able to filter out exogenous noise. Based on this concept, we\ndevelop a practical goal representation learning method that can be combined\nwith any existing GCRL algorithm. Through diverse experiments on the OGBench\ntask suite, we empirically show that dual goal representations consistently\nimprove offline goal-reaching performance across 20 state- and pixel-based\ntasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7528\u4e8e\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60 (GCRL) \u7684\u53cc\u91cd\u76ee\u6807\u8868\u793a\u3002", "motivation": "\u53cc\u91cd\u76ee\u6807\u8868\u793a\u901a\u8fc7\u65f6\u95f4\u8ddd\u79bb\u6765\u8868\u5f81\u72b6\u6001\u4e0e\u5176\u4ed6\u72b6\u6001\u7684\u5173\u7cfb\uff0c\u4ece\u800c\u5bf9\u72b6\u6001\u8fdb\u884c\u7f16\u7801\u3002\u8fd9\u79cd\u8868\u793a\u53ea\u4f9d\u8d56\u4e8e\u73af\u5883\u7684\u5185\u5728\u52a8\u6001\uff0c\u5e76\u4e14\u5bf9\u539f\u59cb\u72b6\u6001\u8868\u793a\u662f\u4e0d\u53d8\u7684\uff0c\u540c\u65f6\u80fd\u591f\u8fc7\u6ee4\u6389\u5916\u751f\u566a\u58f0\u3002", "method": "\u57fa\u4e8e\u53cc\u91cd\u76ee\u6807\u8868\u793a\u7684\u6982\u5ff5\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u76ee\u6807\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u4e0e\u4efb\u4f55\u73b0\u6709\u7684 GCRL \u7b97\u6cd5\u76f8\u7ed3\u5408\u3002", "result": "\u901a\u8fc7\u5728 OGBench \u4efb\u52a1\u5957\u4ef6\u4e0a\u7684\u5404\u79cd\u5b9e\u9a8c\uff0c\u6211\u4eec\u901a\u8fc7\u5b9e\u9a8c\u8868\u660e\uff0c\u5728 20 \u4e2a\u57fa\u4e8e\u72b6\u6001\u548c\u50cf\u7d20\u7684\u4efb\u52a1\u4e2d\uff0c\u53cc\u91cd\u76ee\u6807\u8868\u793a\u59cb\u7ec8\u53ef\u4ee5\u63d0\u9ad8\u79bb\u7ebf\u76ee\u6807\u5b9e\u73b0\u6027\u80fd\u3002", "conclusion": "\u53cc\u91cd\u76ee\u6807\u8868\u793a\u53ef\u4ee5\u63d0\u9ad8\u79bb\u7ebf\u76ee\u6807\u5b9e\u73b0\u6027\u80fd\u3002"}}
{"id": "2510.06780", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06780", "abs": "https://arxiv.org/abs/2510.06780", "authors": ["Luca Giordano", "Simon Razniewski"], "title": "Foundations of LLM Knowledge Materialization: Termination, Reproducibility, Robustness", "comment": null, "summary": "Large Language Models (LLMs) encode substantial factual knowledge, yet\nmeasuring and systematizing this knowledge remains challenging. Converting it\ninto structured format, for example through recursive extraction approaches\nsuch as the GPTKB methodology (Hu et al., 2025b), is still underexplored. Key\nopen questions include whether such extraction can terminate, whether its\noutputs are reproducible, and how robust they are to variations. We\nsystematically study LLM knowledge materialization using miniGPTKBs\n(domain-specific, tractable subcrawls), analyzing termination, reproducibility,\nand robustness across three categories of metrics: yield, lexical similarity,\nand semantic similarity. We experiment with four variations (seed, language,\nrandomness, model) and three illustrative domains (from history, entertainment,\nand finance). Our findings show (i) high termination rates, though\nmodel-dependent; (ii) mixed reproducibility; and (iii) robustness that varies\nby perturbation type: high for seeds and temperature, lower for languages and\nmodels. These results suggest that LLM knowledge materialization can reliably\nsurface core knowledge, while also revealing important limitations.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u4e2d\u7684\u77e5\u8bc6\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u683c\u5f0f\uff0c\u4f8b\u5982\u901a\u8fc7\u9012\u5f52\u63d0\u53d6\u65b9\u6cd5(GPTKB)\u3002", "motivation": "\u8861\u91cf\u548c\u7cfb\u7edf\u5316LLM\u4e2d\u7f16\u7801\u7684\u5927\u91cf\u4e8b\u5b9e\u77e5\u8bc6\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u7814\u7a76\u4e86\u77e5\u8bc6\u63d0\u53d6\u65b9\u6cd5\u662f\u5426\u53ef\u4ee5\u7ec8\u6b62\uff0c\u5176\u8f93\u51fa\u662f\u5426\u53ef\u91cd\u73b0\uff0c\u4ee5\u53ca\u5b83\u4eec\u5bf9\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528miniGPTKBs\uff08\u7279\u5b9a\u9886\u57df\u7684\uff0c\u6613\u4e8e\u5904\u7406\u7684\u5b50\u722c\u53d6\uff09\u7cfb\u7edf\u5730\u7814\u7a76LLM\u77e5\u8bc6\u7684\u7269\u5316\uff0c\u5206\u6790\u4e86\u4e09\u4e2a\u7c7b\u522b\u7684\u6307\u6807\uff1a\u4ea7\u91cf\u3001\u8bcd\u6c47\u76f8\u4f3c\u6027\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u3002\u4f7f\u7528\u4e86\u56db\u79cd\u53d8\u4f53\uff08\u79cd\u5b50\uff0c\u8bed\u8a00\uff0c\u968f\u673a\u6027\uff0c\u6a21\u578b\uff09\u548c\u4e09\u4e2a\u793a\u4f8b\u9886\u57df\uff08\u6765\u81ea\u5386\u53f2\uff0c\u5a31\u4e50\u548c\u91d1\u878d\uff09\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff1a(i) \u7ec8\u6b62\u7387\u9ad8\uff0c\u4f46\u53d6\u51b3\u4e8e\u6a21\u578b\uff1b(ii) \u53ef\u91cd\u590d\u6027\u6df7\u5408\uff1b(iii) \u9c81\u68d2\u6027\u968f\u6270\u52a8\u7c7b\u578b\u800c\u53d8\u5316\uff1a\u79cd\u5b50\u548c\u6e29\u5ea6\u9ad8\uff0c\u8bed\u8a00\u548c\u6a21\u578b\u4f4e\u3002", "conclusion": "LLM\u77e5\u8bc6\u7269\u5316\u53ef\u4ee5\u53ef\u9760\u5730\u5448\u73b0\u6838\u5fc3\u77e5\u8bc6\uff0c\u540c\u65f6\u4e5f\u63ed\u793a\u4e86\u91cd\u8981\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.06791", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06791", "abs": "https://arxiv.org/abs/2510.06791", "authors": ["Changlin Song", "Yunzhong Hou", "Michael Randall Barnes", "Rahul Shome", "Dylan Campbell"], "title": "Extreme Amodal Face Detection", "comment": null, "summary": "Extreme amodal detection is the task of inferring the 2D location of objects\nthat are not fully visible in the input image but are visible within an\nexpanded field-of-view. This differs from amodal detection, where the object is\npartially visible within the input image, but is occluded. In this paper, we\nconsider the sub-problem of face detection, since this class provides\nmotivating applications involving safety and privacy, but do not tailor our\nmethod specifically to this class. Existing approaches rely on image sequences\nso that missing detections may be interpolated from surrounding frames or make\nuse of generative models to sample possible completions. In contrast, we\nconsider the single-image task and propose a more efficient, sample-free\napproach that makes use of the contextual cues from the image to infer the\npresence of unseen faces. We design a heatmap-based extreme amodal object\ndetector that addresses the problem of efficiently predicting a lot (the\nout-of-frame region) from a little (the image) with a selective coarse-to-fine\ndecoder. Our method establishes strong results for this new task, even\noutperforming less efficient generative approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u56fe\u50cf\u6781\u7aef\u975e\u6a21\u6001\u4eba\u8138\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u56fe\u50cf\u4e2d\u7684\u4e0a\u4e0b\u6587\u7ebf\u7d22\u6765\u63a8\u65ad\u4e0d\u53ef\u89c1\u4eba\u8138\u7684\u5b58\u5728\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u56fe\u50cf\u5e8f\u5217\u6216\u751f\u6210\u6a21\u578b\uff0c\u6548\u7387\u8f83\u4f4e\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5355\u56fe\u50cf\u6781\u7aef\u975e\u6a21\u6001\u4eba\u8138\u68c0\u6d4b\u95ee\u9898\uff0c\u5e94\u7528\u4e8e\u5b89\u5168\u548c\u9690\u79c1\u9886\u57df\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u70ed\u56fe\u7684\u6781\u7aef\u975e\u6a21\u6001\u5bf9\u8c61\u68c0\u6d4b\u5668\uff0c\u4f7f\u7528\u9009\u62e9\u6027\u7684\u7531\u7c97\u5230\u7cbe\u7684\u89e3\u7801\u5668\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8be5\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u5f88\u597d\u7684\u7ed3\u679c\uff0c\u751a\u81f3\u4f18\u4e8e\u6548\u7387\u8f83\u4f4e\u7684\u751f\u6210\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u9884\u6d4b\u5927\u91cf\uff08\u8d85\u51fa\u5e27\u533a\u57df\uff09\u7684\u4fe1\u606f\u3002"}}
{"id": "2510.06735", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.06735", "abs": "https://arxiv.org/abs/2510.06735", "authors": ["Zachris Bj\u00f6rkman", "Jorge Lor\u00eda", "Sophie Wharrie", "Samuel Kaski"], "title": "Incorporating Expert Knowledge into Bayesian Causal Discovery of Mixtures of Directed Acyclic Graphs", "comment": "28 pages, 18 figures", "summary": "Bayesian causal discovery benefits from prior information elicited from\ndomain experts, and in heterogeneous domains any prior knowledge would be badly\nneeded. However, so far prior elicitation approaches have assumed a single\ncausal graph and hence are not suited to heterogeneous domains. We propose a\ncausal elicitation strategy for heterogeneous settings, based on Bayesian\nexperimental design (BED) principles, and a variational mixture structure\nlearning (VaMSL) method -- extending the earlier differentiable Bayesian\nstructure learning (DiBS) method -- to iteratively infer mixtures of causal\nBayesian networks (CBNs). We construct an informative graph prior incorporating\nelicited expert feedback in the inference of mixtures of CBNs. Our proposed\nmethod successfully produces a set of alternative causal models (mixture\ncomponents or clusters), and achieves an improved structure learning\nperformance on heterogeneous synthetic data when informed by a simulated\nexpert. Finally, we demonstrate that our approach is capable of capturing\ncomplex distributions in a breast cancer database.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5f02\u6784\u73af\u5883\u4e0b\u7684\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u8d1d\u53f6\u65af\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u53d8\u5206\u6df7\u5408\u7ed3\u6784\u5b66\u4e60\uff0c\u80fd\u591f\u5229\u7528\u4e13\u5bb6\u5148\u9a8c\u77e5\u8bc6\u6765\u6539\u8fdb\u56e0\u679c\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u5b66\u4e60\u3002", "motivation": "\u5728\u5f02\u6784\u9886\u57df\u4e2d\uff0c\u56e0\u679c\u53d1\u73b0\u9700\u8981\u9886\u57df\u4e13\u5bb6\u7684\u5148\u9a8c\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u7684\u5148\u9a8c\u83b7\u53d6\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u5f02\u6784\u9886\u57df\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u5b9e\u9a8c\u8bbe\u8ba1\u7684\u56e0\u679c\u83b7\u53d6\u7b56\u7565\u548c\u53d8\u5206\u6df7\u5408\u7ed3\u6784\u5b66\u4e60\u65b9\u6cd5\uff08VaMSL\uff09\uff0c\u7528\u4e8e\u8fed\u4ee3\u63a8\u65ad\u56e0\u679c\u8d1d\u53f6\u65af\u7f51\u7edc\uff08CBNs\uff09\u7684\u6df7\u5408\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u751f\u6210\u4e86\u4e00\u7ec4\u66ff\u4ee3\u56e0\u679c\u6a21\u578b\uff08\u6df7\u5408\u6210\u5206\u6216\u7c07\uff09\uff0c\u5e76\u5728\u6a21\u62df\u4e13\u5bb6\u63d0\u4f9b\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u9ad8\u4e86\u5f02\u6784\u5408\u6210\u6570\u636e\u7684\u7ed3\u6784\u5b66\u4e60\u6027\u80fd\u3002\u5728\u4e73\u817a\u764c\u6570\u636e\u5e93\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6355\u83b7\u590d\u6742\u7684\u5206\u5e03\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5f02\u6784\u9886\u57df\u4e2d\uff0c\u5229\u7528\u4e13\u5bb6\u5148\u9a8c\u77e5\u8bc6\uff0c\u80fd\u591f\u6709\u6548\u5730\u5b66\u4e60\u56e0\u679c\u7ed3\u6784\u3002"}}
{"id": "2510.06800", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.06800", "abs": "https://arxiv.org/abs/2510.06800", "authors": ["Haotian Wu", "Shufan Jiang", "Chios Chen", "Yiyang Feng", "Hehai Lin", "Heqing Zou", "Yao Shu", "Yanran Li", "Chengwei Qin"], "title": "FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline", "comment": null, "summary": "As large language models (LLMs) advance in role-playing (RP) tasks, existing\nbenchmarks quickly become obsolete due to their narrow scope, outdated\ninteraction paradigms, and limited adaptability across diverse application\nscenarios. To address this gap, we introduce FURINA-Builder, a novel\nmulti-agent collaboration pipeline that automatically constructs fully\ncustomizable RP benchmarks at any scale. It enables evaluation of arbitrary\ncharacters across diverse scenarios and prompt formats, as the first benchmark\nbuilder in RP area for adaptable assessment. FURINA-Builder simulates dialogues\nbetween a test character and other characters drawn from a well-constructed\ncharacter-scene pool, while an LLM judge selects fine-grained evaluation\ndimensions and adjusts the test character's responses into final test\nutterances. Using this pipeline, we build FURINA-Bench, a new comprehensive\nrole-playing benchmark featuring both established and synthesized test\ncharacters, each assessed with dimension-specific evaluation criteria. Human\nevaluation and preliminary separability analysis justify our pipeline and\nbenchmark design. We conduct extensive evaluations of cutting-edge LLMs and\nfind that o3 and DeepSeek-R1 achieve the best performance on English and\nChinese RP tasks, respectively. Across all models, established characters\nconsistently outperform synthesized ones, with reasoning capabilities further\namplifying this disparity. Interestingly, we observe that model scale does not\nmonotonically reduce hallucinations. More critically, for reasoning LLMs, we\nuncover a novel trade-off: reasoning improves RP performance but simultaneously\nincreases RP hallucinations. This trade-off extends to a broader Pareto\nfrontier between RP performance and reliability for all LLMs. These findings\ndemonstrate the effectiveness of FURINA-Builder and the challenge posed by\nFURINA-Bench.", "AI": {"tldr": "\u63d0\u51fa\u4e86FURINA-Builder\uff0c\u4e00\u4e2a\u81ea\u52a8\u6784\u5efa\u89d2\u8272\u626e\u6f14\uff08RP\uff09\u57fa\u51c6\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6d41\u7a0b\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u89d2\u8272\u626e\u6f14\u57fa\u51c6\u8303\u56f4\u72ed\u7a84\u3001\u4ea4\u4e92\u6a21\u5f0f\u8fc7\u65f6\uff0c\u4e14\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u6709\u9650\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u6d4b\u8bd5\u89d2\u8272\u4e0e\u5176\u4ed6\u89d2\u8272\u7684\u5bf9\u8bdd\uff0c\u5e76\u4f7f\u7528LLM\u88c1\u5224\u9009\u62e9\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u5c06\u6d4b\u8bd5\u89d2\u8272\u7684\u53cd\u5e94\u8c03\u6574\u4e3a\u6700\u7ec8\u7684\u6d4b\u8bd5\u8bdd\u8bed\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u7efc\u5408\u6027\u89d2\u8272\u626e\u6f14\u57fa\u51c6FURINA-Bench\uff0c\u5bf9\u9886\u5148\u7684LLM\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u53d1\u73b0\u6700\u4f73\u6a21\u578b\u5728\u82f1\u8bed\u548c\u4e2d\u6587RP\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u63a8\u7406\u80fd\u529b\u4f1a\u653e\u5927\u5df2\u5efa\u7acb\u89d2\u8272\u548c\u5408\u6210\u89d2\u8272\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u4e14\u63a8\u7406\u7684LLM\u5b58\u5728\u65b0\u7684\u6743\u8861\uff1a\u63a8\u7406\u63d0\u9ad8\u4e86RP\u6027\u80fd\uff0c\u4f46\u540c\u65f6\u4e5f\u589e\u52a0\u4e86RP\u5e7b\u89c9\u3002", "conclusion": "FURINA-Builder\u6709\u6548\uff0cFURINA-Bench\u5177\u6709\u6311\u6218\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u89d2\u8272\u626e\u6f14\u6027\u80fd\u548c\u53ef\u9760\u6027\u4e4b\u95f4\u66f4\u5e7f\u6cdb\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002"}}
