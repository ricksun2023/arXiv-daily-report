<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 8]
- [cs.CV](#cs.CV) [Total: 8]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.DB](#cs.DB) [Total: 11]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.LG](#cs.LG) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization](https://arxiv.org/abs/2511.00010)
*Jiajun Zhang,Jianke Zhang,Zeyu Cui,Jiaxi Yang,Lei Zhang,Binyuan Hui,Qiang Liu,Zilei Wang,Liang Wang,Junyang Lin*

Main category: cs.CL

TL;DR: PlotCraft是一个新的基准，用于评估大型语言模型在复杂数据可视化方面的能力。它包含1000个具有挑战性的可视化任务，涵盖金融、科学研究和社会学等广泛主题。对23个领先的LLM的评估表明，它们在处理复杂的视觉任务方面存在明显的性能缺陷。为了弥补这一性能差距，作者开发了SynthVis-30K，这是一个大规模、高质量的复杂可视化代码数据集。在此基础上，作者开发了PlotCraftor，这是一个新型的代码生成模型，在复杂的数据可视化方面取得了强大的能力，而且规模非常小。PlotCraftor在VisEval、PandasPlotBench和PlotCraft上表现出与领先的专有方法相当的性能。特别是在困难的任务上，该模型实现了超过50%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 评估和提高大型语言模型创建复杂数据可视化的能力。

Method: 1. 引入PlotCraft基准，包含1000个可视化任务，涵盖各种主题和图表类型。2. 评估23个领先的LLM在PlotCraft上的性能。3. 开发SynthVis-30K数据集，用于训练模型。4. 开发PlotCraftor模型，用于生成复杂的数据可视化代码。

Result: 1. 发现现有LLM在复杂数据可视化方面存在性能缺陷。2. PlotCraftor模型在复杂数据可视化方面取得了强大的能力，并且规模非常小。3. PlotCraftor在多个基准测试中表现出与领先的专有方法相当的性能，尤其是在困难的任务上，性能提升超过50%。

Conclusion: PlotCraft基准和PlotCraftor模型为评估和提高大型语言模型在复杂数据可视化方面的能力提供了有价值的资源和方法。研究结果表明，通过专门的数据集和模型设计，可以显著提高LLM在这一领域的性能。

Abstract: Recent Large Language Models (LLMs) have demonstrated remarkable profi-
ciency in code generation. However, their ability to create complex visualiza-
tions for scaled and structured data remains largely unevaluated and
underdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark
featuring 1k challenging visualization tasks that cover a wide range of topics,
such as fi- nance, scientific research, and sociology. The benchmark is
structured around seven high-level visualization tasks and encompasses 48
distinct chart types. Cru- cially, it is the first to systematically evaluate
both single-turn generation and multi-turn refinement across a diverse spectrum
of task complexities. Our com- prehensive evaluation of 23 leading LLMs on
PlotCraft reveals obvious per- formance deficiencies in handling sophisticated
visualization tasks. To bridge this performance gap, we develope SynthVis-30K,
a large-scale, high-quality dataset of complex visualization code synthesized
via a collaborative agent frame- work. Building upon this dataset, we develope
PlotCraftor, a novel code gener- ation model that achieves strong capabilities
in complex data visualization with a remarkably small size. Across VisEval,
PandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance
comparable to that of leading propri- etary approaches. Especially, on hard
task, Our model achieves over 50% per- formance improvement. We will release
the benchmark, dataset, and code at
https://github.com/Speakn0w/PlotCraft-Benchmark.

</details>


### [2] [Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference](https://arxiv.org/abs/2511.00115)
*Haoyuan Li,Yuanbo Tong,Yuchen Li,Zirui Wang,Chunhou Liu,Jiamou Liu*

Main category: cs.CL

TL;DR: ProtoMBTI: A new framework for MBTI inference using prototype theory and LLMs.


<details>
  <summary>Details</summary>
Motivation: Traditional personality recognition treats personality as hard-label classification, ignoring the nuanced nature of personality judgments.

Method: 1) Construct a balanced corpus using LLM augmentation. 2) Fine-tune an encoder to learn embeddings and standardize personality prototypes. 3) Implement a retrieve-reuse-revise-retain cycle for inference, aggregating prototype evidence and revising inconsistencies.

Result: ProtoMBTI outperforms baselines on MBTI benchmarks and demonstrates cross-dataset generalization.

Conclusion: Aligning the inference process with psychological prototype reasoning improves accuracy, interpretability, and transfer in text-based personality modeling.

Abstract: Personality recognition from text is typically cast as hard-label
classification, which obscures the graded, prototype-like nature of human
personality judgments. We present ProtoMBTI, a cognitively aligned framework
for MBTI inference that operationalizes prototype theory within an LLM-based
pipeline. First, we construct a balanced, quality-controlled corpus via
LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).
Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative
embeddings and to standardize a bank of personality prototypes. At inference,
we retrieve top-k prototypes for a query post and perform a
retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence
via prompt-based voting, revises when inconsistencies arise, and, upon correct
prediction, retains the sample to continually enrich the prototype library.
Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both
the four MBTI dichotomies and the full 16-type task, and exhibits robust
cross-dataset generalization. Our results indicate that aligning the inference
process with psychological prototype reasoning yields gains in accuracy,
interpretability, and transfer for text-based personality modeling.

</details>


### [3] [ParaScopes: What do Language Models Activations Encode About Future Text?](https://arxiv.org/abs/2511.00180)
*Nicky Pochinkov,Yulia Volkova,Anna Vasileva,Sai V R Chereddy*

Main category: cs.CL

TL;DR: 本文提出了一种残差流解码器框架，用于探究语言模型激活中段落级和文档级计划。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型可执行更长时间范围的任务，但理解激活的方法通常仅限于测试特定概念或token。

Method: 开发残差流解码器框架，用于探测模型激活。

Result: 在小型模型中，可以解码相当于未来上下文中 5+ 个 token 的信息。

Conclusion: 这些结果为更好地监控语言模型和理解它们如何编码长期计划信息奠定了基础。

Abstract: Interpretability studies in language models often investigate forward-looking
representations of activations. However, as language models become capable of
doing ever longer time horizon tasks, methods for understanding activations
often remain limited to testing specific concepts or tokens. We develop a
framework of Residual Stream Decoders as a method of probing model activations
for paragraph-scale and document-scale plans. We test several methods and find
information can be decoded equivalent to 5+ tokens of future context in small
models. These results lay the groundwork for better monitoring of language
models and better understanding how they might encode longer-term planning
information.

</details>


### [4] [Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap](https://arxiv.org/abs/2511.00198)
*Chun-Hao Yang,Bo-Han Feng,Tzu-Yuan Lai,Yan Yu Chen,Yin-Kai Dean Huang,Shou-De Lin*

Main category: cs.CL

TL;DR: 提出了一种新的LLM训练方法，通过预测信息丰富的tokens来提高模型性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统的LLM训练方法效率低下，计算成本高昂。

Method: 研究了在LLM训练过程中预测信息丰富tokens的影响。

Result: 在算术、文本多标签分类和自然语言生成三个任务中验证了该方法的有效性。

Conclusion: 该方法为优化LLM训练提供了一种有效途径，提升了模型性能和对目标token选择策略的理论理解。

Abstract: Optimizing training performance in large language models (LLMs) remains an
essential challenge, particularly in improving model performance while
maintaining computational costs. This work challenges the conventional approach
of training LLMs using next-token prediction (NTP), arguing that by predicting
information-rich tokens during training, there is a more effective way to train
LLMs. We investigate the impact of the proposed solution in three kinds of
tasks for LLMs: arithmetic, multi-label classification of text, and
natural-language generation. This work offers a principled approach to
optimizing LLM training, advancing both model performance and theoretical
understanding of the target-token selection strategies.

</details>


### [5] [Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2511.00222)
*Marwa Abdulhai,Ryan Cheng,Donovan Clay,Tim Althoff,Sergey Levine,Natasha Jaques*

Main category: cs.CL

TL;DR: 这篇论文提出了一个统一的框架，用于评估和提高LLM生成对话中角色一致性


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在交互环境中模拟人类用户时，经常出现角色漂移、前后矛盾或放弃角色行为的问题

Method: 定义了三种自动指标（prompt-to-line consistency, line-to-line consistency, Q&A consistency）来捕捉不同类型的角色漂移，并使用多轮强化学习对LLM进行微调

Result: 该方法将不一致性降低了55%以上

Conclusion: 该方法能够生成更连贯和忠实的角色模拟用户

Abstract: Large Language Models (LLMs) are increasingly used to simulate human users in
interactive settings such as therapy, education, and social role-play. While
these simulations enable scalable training and evaluation of AI agents,
off-the-shelf LLMs often drift from their assigned personas, contradict earlier
statements, or abandon role-appropriate behavior. We introduce a unified
framework for evaluating and improving persona consistency in LLM-generated
dialogue. We define three automatic metrics: prompt-to-line consistency,
line-to-line consistency, and Q&A consistency, that capture different types of
persona drift and validate each against human annotations. Using these metrics
as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs
for three user roles: a patient, a student, and a social chat partner. Our
method reduces inconsistency by over 55%, resulting in more coherent and
faithful simulated users.

</details>


### [6] [AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding](https://arxiv.org/abs/2511.00265)
*Arman Anwar,Zefang Liu*

Main category: cs.CL

TL;DR: AgentBnB is a browser-based cybersecurity tabletop exercise that uses large language models to provide on-demand hints and teammates.


<details>
  <summary>Details</summary>
Motivation: Traditional cybersecurity tabletop exercises are often scripted, resource-intensive, and difficult to scale.

Method: The system integrates large language model teammates with a retrieval-augmented copilot (C2D2) that expands a curated corpus into factual, conceptual, procedural, and metacognitive snippets, delivering on-demand, cognitively targeted hints. Prompt-engineered agents employ a scaffolding ladder that gradually fades as learner confidence grows.

Result: In a solo-player pilot with four graduate students, participants reported greater intention to use the agent-based version compared to the physical card deck and viewed it as more scalable, though a ceiling effect emerged on a simple knowledge quiz.

Conclusion: Large language model augmented TTXs can provide lightweight, repeatable practice without the logistical burden of traditional exercises.

Abstract: Traditional cybersecurity tabletop exercises (TTXs) provide valuable training
but are often scripted, resource-intensive, and difficult to scale. We
introduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches
game that integrates large language model teammates with a Bloom-aligned,
retrieval-augmented copilot (C2D2). The system expands a curated corpus into
factual, conceptual, procedural, and metacognitive snippets, delivering
on-demand, cognitively targeted hints. Prompt-engineered agents employ a
scaffolding ladder that gradually fades as learner confidence grows. In a
solo-player pilot with four graduate students, participants reported greater
intention to use the agent-based version compared to the physical card deck and
viewed it as more scalable, though a ceiling effect emerged on a simple
knowledge quiz. Despite limitations of small sample size, single-player focus,
and narrow corpus, these early findings suggest that large language model
augmented TTXs can provide lightweight, repeatable practice without the
logistical burden of traditional exercises. Planned extensions include
multi-player modes, telemetry-driven coaching, and comparative studies with
larger cohorts.

</details>


### [7] [IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval](https://arxiv.org/abs/2511.00268)
*Shounak Paul,Dhananjay Ghumare,Pawan Goyal,Saptarshi Ghosh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 本文提出了一个名为IL-PCR的印度法律语料库，用于判例和法规检索，并开发了一个基于LLM的重排序方法，以利用这两个任务之间的依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有的法律从业者通常需要检索相关法规和判例，但目前的研究通常独立处理这两个任务，忽略了它们之间的内在联系，例如相似的案件倾向于引用相似的法规。

Method: 本文构建了一个名为IL-PCR的语料库，并在此基础上实验了多种基线模型，包括词汇模型、语义模型和基于GNN的集成模型。此外，还开发了一种基于LLM的重排序方法，以利用判例和法规检索任务之间的依赖关系。

Result: 基于LLM的重排序方法取得了最佳性能。

Conclusion: 本文提出了一个新的印度法律语料库IL-PCR，并证明了利用判例和法规检索任务之间的依赖关系可以提高检索性能。

Abstract: Identifying/retrieving relevant statutes and prior cases/precedents for a
given legal situation are common tasks exercised by law practitioners.
Researchers to date have addressed the two tasks independently, thus developing
completely different datasets and models for each task; however, both retrieval
tasks are inherently related, e.g., similar cases tend to cite similar statutes
(due to similar factual situation). In this paper, we address this gap. We
propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),
which is a unique corpus that provides a common testbed for developing models
for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit
the dependence between the two. We experiment extensively with several baseline
models on the tasks, including lexical models, semantic models and ensemble
based on GNNs. Further, to exploit the dependence between the two tasks, we
develop an LLM-based re-ranking approach that gives the best performance.

</details>


### [8] [POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation](https://arxiv.org/abs/2511.00270)
*Abhinav Joshi,Vaibhav Sharma,Sanjeet Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 提出了一种新的预训练方案POSESTITCH-SLT，用于解决因缺乏大规模句子对齐数据集而导致的sign language translation 任务的挑战。


<details>
  <summary>Details</summary>
Motivation: 以往的研究集中于各种特征提取和架构变化，以支持sign language的神经机器翻译。

Method: 该方法受到基于语言模板的句子生成技术的启发。

Result: 在How2Sign和iSign两个sign language数据集上，该方法优于现有技术，在How2Sign上BLEU-4提高了1.97到4.56，在iSign上提高了0.55到3.43。

Conclusion: 结果表明，在低资源sign language环境中，模板驱动的合成监督是有效的。

Abstract: Sign language translation remains a challenging task due to the scarcity of
large-scale, sentence-aligned datasets. Prior arts have focused on various
feature extraction and architectural changes to support neural machine
translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training
scheme that is inspired by linguistic-templates-based sentence generation
technique. With translation comparison on two sign language datasets, How2Sign
and iSign, we show that a simple transformer-based encoder-decoder architecture
outperforms the prior art when considering template-generated sentence pairs in
training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign
and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for
pose-based gloss-free translation. The results demonstrate the effectiveness of
template-driven synthetic supervision in low-resource sign language settings.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [9] [Generative human motion mimicking through feature extraction in denoising diffusion settings](https://arxiv.org/abs/2511.00011)
*Alexander Okupnik,Johannes Schneider,Kyriakos Flouris*

Main category: cs.CV

TL;DR: 构建了一个交互模型，通过模仿和创造性地增强传入的运动数据序列来生成一个虚拟的舞伴。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型缺乏人类交互的具身性，而舞蹈是人类表达的原始形式，可以补充这种体验。

Method: 该模型基于运动捕捉(MoCap)数据，结合了两种扩散模型、运动修复和运动风格迁移的思想。

Result: 通过定量评估生成样本的特征分布和测试集的收敛性来证明模型的成功。

Conclusion: 该模型生成的舞蹈既具有多样性，又显示出与人类舞伴的各种偏差，同时又显得逼真，是与人工智能进行创造性舞蹈的第一步。

Abstract: Recent success with large language models has sparked a new wave of verbal
human-AI interaction. While such models support users in a variety of creative
tasks, they lack the embodied nature of human interaction. Dance, as a primal
form of human expression, is predestined to complement this experience. To
explore creative human-AI interaction exemplified by dance, we build an
interactive model based on motion capture (MoCap) data. It generates an
artificial other by partially mimicking and also "creatively" enhancing an
incoming sequence of movement data. It is the first model, which leverages
single-person motion data and high level features in order to do so and, thus,
it does not rely on low level human-human interaction data. It combines ideas
of two diffusion models, motion inpainting, and motion style transfer to
generate movement representations that are both temporally coherent and
responsive to a chosen movement reference. The success of the model is
demonstrated by quantitatively assessing the convergence of the feature
distribution of the generated samples and the test set which serves as
simulating the human performer. We show that our generations are first steps to
creative dancing with AI as they are both diverse showing various deviations
from the human partner while appearing realistic.

</details>


### [10] [Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets](https://arxiv.org/abs/2511.00021)
*Julio Jerison E. Macrohon,Gordon Hung*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的基于机器学习的珊瑚白化分类系统，该系统使用包含健康和白化珊瑚样本的多元化全球数据集。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁面临着来自污染、海洋酸化和海水温度异常的日益严重的威胁，因此迫切需要有效的保护和监测。

Method: 该研究对三种最先进的模型进行了基准测试和比较：残差神经网络 (ResNet)、视觉转换器 (ViT) 和卷积神经网络 (CNN)。

Result: 经过全面的超参数调整后，CNN 模型实现了 88% 的最高准确率，优于现有基准。

Conclusion: 研究结果为自主珊瑚监测提供了重要的见解，并对最广泛使用的计算机视觉模型进行了全面分析。

Abstract: Coral reefs support numerous marine organisms and are an important source of
coastal protection from storms and floods, representing a major part of marine
ecosystems. However coral reefs face increasing threats from pollution, ocean
acidification, and sea temperature anomalies, making efficient protection and
monitoring heavily urgent. Therefore, this study presents a novel
machine-learning-based coral bleaching classification system based on a diverse
global dataset with samples of healthy and bleached corals under varying
environmental conditions, including deep seas, marshes, and coastal zones. We
benchmarked and compared three state-of-the-art models: Residual Neural Network
(ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN).
After comprehensive hyperparameter tuning, the CNN model achieved the highest
accuracy of 88%, outperforming existing benchmarks. Our findings offer
important insights into autonomous coral monitoring and present a comprehensive
analysis of the most widely used computer vision models.

</details>


### [11] [Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline](https://arxiv.org/abs/2511.00022)
*Jules Gerard,Leandro Di Bella,Filip Huyghe,Marc Kochzius*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLOv8的深度学习管道，用于自动识别西印度洋的珊瑚礁鱼类，以解决水下视觉调查的人力需求问题。


<details>
  <summary>Details</summary>
Motivation: 西印度洋的珊瑚礁监测受到水下视觉调查的人力需求限制。

Method: 使用基于YOLOv8的深度学习管道，对在肯尼亚和坦桑尼亚收集的视频样带进行家庭层面鱼类识别。

Result: 最佳模型实现了0.52的mAP@0.5，对丰富鱼类的识别准确率较高，但对稀有或复杂类群的检测较弱。

Conclusion: 结果表明，深度学习有潜力作为传统监测方法的可扩展补充。

Abstract: Coral reef monitoring in the Western Indian Ocean is limited by the labor
demands of underwater visual censuses. This work evaluates a YOLOv8-based deep
learning pipeline for automating family-level fish identification from video
transects collected in Kenya and Tanzania. A curated dataset of 24 families was
tested under different configurations, providing the first region-specific
benchmark for automated reef fish monitoring in the Western Indian Ocean. The
best model achieved mAP@0.5 of 0.52, with high accuracy for abundant families
but weaker detection of rare or complex taxa. Results demonstrate the potential
of deep learning as a scalable complement to traditional monitoring methods.

</details>


### [12] [Mutual Information guided Visual Contrastive Learning](https://arxiv.org/abs/2511.00028)
*Hanyang Chen,Yanchao Yang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于互信息的数据增强方法，以提升表征学习的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习的数据增强依赖人工设计，可能不是最优的。

Method: 该方法选择在自然扰动下具有高互信息的图像块作为正样本，用于对比损失学习。

Result: 在多个基准测试中验证了该方法的有效性。

Conclusion: 该方法是一种有前景的未来研究方向。

Abstract: Representation learning methods utilizing the InfoNCE loss have demonstrated
considerable capacity in reducing human annotation effort by training invariant
neural feature extractors. Although different variants of the training
objective adhere to the information maximization principle between the data and
learned features, data selection and augmentation still rely on human
hypotheses or engineering, which may be suboptimal. For instance, data
augmentation in contrastive learning primarily focuses on color jittering,
aiming to emulate real-world illumination changes. In this work, we investigate
the potential of selecting training data based on their mutual information
computed from real-world distributions, which, in principle, should endow the
learned features with better generalization when applied in open environments.
Specifically, we consider patches attached to scenes that exhibit high mutual
information under natural perturbations, such as color changes and motion, as
positive samples for learning with contrastive loss. We evaluate the proposed
mutual-information-informed data augmentation method on several benchmarks
across multiple state-of-the-art representation learning frameworks,
demonstrating its effectiveness and establishing it as a promising direction
for future research.

</details>


### [13] [Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra](https://arxiv.org/abs/2511.00037)
*Riya Gupta,Alexander Chowdhury,Sahil Nalawade*

Main category: cs.CV

TL;DR: 本研究对三个联邦学习框架（NVIDIA FLARE、Flower 和 Owkin Substra）进行了基准测试，以评估它们在医学成像应用中的适用性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习 (FL) 已成为医学人工智能领域的一种变革性范例，可以在不直接共享数据的情况下实现跨机构的协作模型训练。

Method: 使用 PathMNIST 数据集，评估了模型性能、收敛效率、通信开销、可扩展性和开发者体验。

Result: NVIDIA FLARE 具有卓越的生产可扩展性，Flower 为原型设计和学术研究提供了灵活性，而 Owkin Substra 则展示了卓越的隐私和合规性功能。

Conclusion: 每个框架都展示了针对不同用例优化的优势，强调了它们与医疗环境中实际部署的相关性。

Abstract: Federated Learning (FL) has emerged as a transformative paradigm in medical
AI, enabling collaborative model training across institutions without direct
data sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE,
Flower, and Owkin Substra to evaluate their suitability for medical imaging
applications in real-world settings. Using the PathMNIST dataset, we assess
model performance, convergence efficiency, communication overhead, scalability,
and developer experience. Results indicate that NVIDIA FLARE offers superior
production scalability, Flower provides flexibility for prototyping and
academic research, and Owkin Substra demonstrates exceptional privacy and
compliance features. Each framework exhibits strengths optimized for distinct
use cases, emphasizing their relevance to practical deployment in healthcare
environments.

</details>


### [14] [Enhancing rice leaf images: An overview of image denoising techniques](https://arxiv.org/abs/2511.00046)
*Rupjyoti Chutia,Dibya Jyoti Bora*

Main category: cs.CV

TL;DR: 本文对水稻叶片图像进行去噪和增强，以提高图像质量和后续分析的可靠性。


<details>
  <summary>Details</summary>
Motivation: 图像增强是图像处理中的关键预处理步骤，对于水稻叶片分析中的病害检测、营养不良评估和生长分析至关重要。

Method: 本文研究了结合 CLAHE（对比度有限自适应直方图均衡化）的图像去噪方法，并进行了比较研究。

Result: 实验结果使用各种指标进行检查，以全面测试增强方法。

Conclusion: 该方法为评估数字图像处理方法的有效性提供了坚实的基础，并揭示了对农业研究和其他领域未来适应有用的见解。

Abstract: Digital image processing involves the systematic handling of images using
advanced computer algorithms, and has gained significant attention in both
academic and practical fields. Image enhancement is a crucial preprocessing
stage in the image-processing chain, improving image quality and emphasizing
features. This makes subsequent tasks (segmentation, feature extraction,
classification) more reliable. Image enhancement is essential for rice leaf
analysis, aiding in disease detection, nutrient deficiency evaluation, and
growth analysis. Denoising followed by contrast enhancement are the primary
steps. Image filters, generally employed for denoising, transform or enhance
visual characteristics like brightness, contrast, and sharpness, playing a
crucial role in improving overall image quality and enabling the extraction of
useful information. This work provides an extensive comparative study of
well-known image-denoising methods combined with CLAHE (Contrast Limited
Adaptive Histogram Equalization) for efficient denoising of rice leaf images.
The experiments were performed on a rice leaf image dataset to ensure the data
is relevant and representative. Results were examined using various metrics to
comprehensively test enhancement methods. This approach provides a strong basis
for assessing the effectiveness of methodologies in digital image processing
and reveals insights useful for future adaptation in agricultural research and
other domains.

</details>


### [15] [Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?](https://arxiv.org/abs/2511.00060)
*Zhiqi Qi,Runxin Zhao,Hanyang Zhuang,Chunxiang Wang,Ming Yang*

Main category: cs.CV

TL;DR: 研究了不同激光雷达扫描模式对路边感知系统性能的影响，并发布了InfraLiDARs' Benchmark数据集。


<details>
  <summary>Details</summary>
Motivation: 探讨不同激光雷达扫描模式对感知性能的深远影响，现有研究对基础设施中激光雷达的最佳放置位置进行了大量研究，但对不同激光雷达扫描模式的影响研究不足。

Method: 在CARLA模拟环境中，使用同时运行的、具有两种扫描模式的、基于基础设施的激光雷达，收集了一个名为“InfraLiDARs' Benchmark”的新数据集，并对各种领先的3D目标检测算法的性能进行了评估。

Result: 非重复扫描激光雷达和128线重复激光雷达在各种场景中表现出相当的检测性能。尽管非重复扫描激光雷达的感知范围有限，但考虑到其低廉的价格，它是一种经济高效的选择。

Conclusion: 为设置具有最佳激光雷达扫描模式和兼容算法的路边感知系统提供了见解，以用于各种路边应用，并公开发布“InfraLiDARs' Benchmark”数据集，以促进进一步的研究。

Abstract: LiDAR-based roadside perception is a cornerstone of advanced Intelligent
Transportation Systems (ITS). While considerable research has addressed optimal
LiDAR placement for infrastructure, the profound impact of differing LiDAR
scanning patterns on perceptual performance remains comparatively
under-investigated. The inherent nature of various scanning modes - such as
traditional repetitive (mechanical/solid-state) versus emerging non-repetitive
(e.g. prism-based) systems - leads to distinct point cloud distributions at
varying distances, critically dictating the efficacy of object detection and
overall environmental understanding. To systematically investigate these
differences in infrastructure-based contexts, we introduce the "InfraLiDARs'
Benchmark," a novel dataset meticulously collected in the CARLA simulation
environment using concurrently operating infrastructure-based LiDARs exhibiting
both scanning paradigms. Leveraging this benchmark, we conduct a comprehensive
statistical analysis of the respective LiDAR scanning abilities and evaluate
the impact of these distinct patterns on the performance of various leading 3D
object detection algorithms. Our findings reveal that non-repetitive scanning
LiDAR and the 128-line repetitive LiDAR were found to exhibit comparable
detection performance across various scenarios. Despite non-repetitive LiDAR's
limited perception range, it's a cost-effective option considering its low
price. Ultimately, this study provides insights for setting up roadside
perception system with optimal LiDAR scanning patterns and compatible
algorithms for diverse roadside applications, and publicly releases the
"InfraLiDARs' Benchmark" dataset to foster further research.

</details>


### [16] [World Simulation with Video Foundation Models for Physical AI](https://arxiv.org/abs/2511.00062)
*NVIDIA,:,Arslan Ali,Junjie Bai,Maciej Bala,Yogesh Balaji,Aaron Blakeman,Tiffany Cai,Jiaxin Cao,Tianshi Cao,Elizabeth Cha,Yu-Wei Chao,Prithvijit Chattopadhyay,Mike Chen,Yongxin Chen,Yu Chen,Shuai Cheng,Yin Cui,Jenna Diamond,Yifan Ding,Jiaojiao Fan,Linxi Fan,Liang Feng,Francesco Ferroni,Sanja Fidler,Xiao Fu,Ruiyuan Gao,Yunhao Ge,Jinwei Gu,Aryaman Gupta,Siddharth Gururani,Imad El Hanafi,Ali Hassani,Zekun Hao,Jacob Huffman,Joel Jang,Pooya Jannaty,Jan Kautz,Grace Lam,Xuan Li,Zhaoshuo Li,Maosheng Liao,Chen-Hsuan Lin,Tsung-Yi Lin,Yen-Chen Lin,Huan Ling,Ming-Yu Liu,Xian Liu,Yifan Lu,Alice Luo,Qianli Ma,Hanzi Mao,Kaichun Mo,Seungjun Nah,Yashraj Narang,Abhijeet Panaskar,Lindsey Pavao,Trung Pham,Morteza Ramezanali,Fitsum Reda,Scott Reed,Xuanchi Ren,Haonan Shao,Yue Shen,Stella Shi,Shuran Song,Bartosz Stefaniak,Shangkun Sun,Shitao Tang,Sameena Tasmeen,Lyne Tchapmi,Wei-Cheng Tseng,Jibin Varghese,Andrew Z. Wang,Hao Wang,Haoxiang Wang,Heng Wang,Ting-Chun Wang,Fangyin Wei,Jiashu Xu,Dinghao Yang,Xiaodong Yang,Haotian Ye,Seonghyeon Ye,Xiaohui Zeng,Jing Zhang,Qinsheng Zhang,Kaiwen Zheng,Andrew Zhu,Yuke Zhu*

Main category: cs.CV

TL;DR: Cosmos-Predict2.5 improves video quality and instruction alignment over Cosmos-Predict1 using a flow-based architecture and reinforcement learning. Cosmos-Transfer2.5, a control-net style framework, achieves higher fidelity and robust long-horizon video generation compared to Cosmos-Transfer1. Code, checkpoints, and benchmarks are released to foster innovation in Physical AI.


<details>
  <summary>Details</summary>
Motivation: To improve synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems.

Method: Uses a flow-based architecture to unify Text2World, Image2World, and Video2World generation, leverages Cosmos-Reason1 for richer text grounding, and is trained on 200M video clips with reinforcement learning-based post-training. Introduces Cosmos-Transfer2.5, a control-net style framework for Sim2Real and Real2Real world translation.

Result: Cosmos-Predict2.5 achieves substantial improvements over Cosmos-Predict1 in video quality and instruction alignment. Cosmos-Transfer2.5 delivers higher fidelity and robust long-horizon video generation, despite being smaller than Cosmos-Transfer1.

Conclusion: Cosmos-Predict2.5 and Cosmos-Transfer2.5 are versatile tools for scaling embodied intelligence. Open resources are released to lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.

Abstract: We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World
Foundation Models for Physical AI. Built on a flow-based architecture,
[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation
in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language
model, to provide richer text grounding and finer control of world simulation.
Trained on 200M curated video clips and refined with reinforcement
learning-based post-training, [Cosmos-Predict2.5] achieves substantial
improvements over [Cosmos-Predict1] in video quality and instruction alignment,
with models released at 2B and 14B scales. These capabilities enable more
reliable synthetic data generation, policy evaluation, and closed-loop
simulation for robotics and autonomous systems. We further extend the family
with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and
Real2Real world translation. Despite being 3.5$\times$ smaller than
[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video
generation. Together, these advances establish [Cosmos-Predict2.5] and
[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To
accelerate research and deployment in Physical AI, we release source code,
pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model
License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and
https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open
resources lower the barrier to adoption and foster innovation in building the
next generation of embodied intelligence.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [Multimodal Detection of Fake Reviews using BERT and ResNet-50](https://arxiv.org/abs/2511.00020)
*Suhasnadh Reddy Veluru,Sai Teja Erukude,Viswa Chaitanya Marella*

Main category: cs.AI

TL;DR: 这篇论文提出了一种多模态框架，用于检测虚假评论，该框架集成了文本和视觉特征。


<details>
  <summary>Details</summary>
Motivation: 动机是现有检测模型主要依赖于单模态数据，无法捕捉不同模态之间的语义不一致性。 解决这个问题对于维护评论生态系统中的信任和透明度至关重要。

Method: 该方法包括使用BERT编码文本特征，使用ResNet-50提取视觉特征，并通过分类头融合这些表示，以联合预测评论的真实性。

Result: 实验结果表明，多模态模型优于单模态基线，在测试集上实现了0.934的F1分数。

Conclusion: 该研究表明了多模态学习在保护数字信任方面的关键作用，并为各种在线平台的内容审核提供了可扩展的解决方案。

Abstract: In the current digital commerce landscape, user-generated reviews play a
critical role in shaping consumer behavior, product reputation, and platform
credibility. However, the proliferation of fake or misleading reviews often
generated by bots, paid agents, or AI models poses a significant threat to
trust and transparency within review ecosystems. Existing detection models
primarily rely on unimodal, typically textual, data and therefore fail to
capture semantic inconsistencies across different modalities. To address this
gap, a robust multimodal fake review detection framework is proposed,
integrating textual features encoded with BERT and visual features extracted
using ResNet-50. These representations are fused through a classification head
to jointly predict review authenticity. To support this approach, a curated
dataset comprising 21,142 user-uploaded images across food delivery,
hospitality, and e-commerce domains was utilized. Experimental results indicate
that the multimodal model outperforms unimodal baselines, achieving an F1-score
of 0.934 on the test set. Additionally, the confusion matrix and qualitative
analysis highlight the model's ability to detect subtle inconsistencies, such
as exaggerated textual praise paired with unrelated or low-quality images,
commonly found in deceptive content. This study demonstrates the critical role
of multimodal learning in safeguarding digital trust and offers a scalable
solution for content moderation across various online platforms.

</details>


### [18] [Graph-Attentive MAPPO for Dynamic Retail Pricing](https://arxiv.org/abs/2511.00039)
*Krishna Kumar Neelakanta Pillai Santha Kumari Amma*

Main category: cs.AI

TL;DR: 本文研究了零售动态定价中的多智能体强化学习方法，并提出了一个基于图注意力机制的MAPPO改进算法(MAPPO+GAT)。


<details>
  <summary>Details</summary>
Motivation: 零售中的动态定价策略需要适应不断变化的需求，同时协调相关产品之间的决策。

Method: 使用从真实交易数据中提取的模拟定价环境，比较了MAPPO基线和一个图注意力增强变体(MAPPO+GAT)。评估了利润、随机种子之间的稳定性、产品之间的公平性以及标准化评估协议下的训练效率。

Result: 结果表明，MAPPO为投资组合层面的价格控制提供了稳健和可重复的基础，而MAPPO+GAT通过在产品图上共享信息进一步提高了性能，且不会引起过度的价格波动。

Conclusion: 图集成的MARL为动态零售定价提供了比独立学习者更具可扩展性和稳定性的解决方案，为多产品决策提供了实际优势。

Abstract: Dynamic pricing in retail requires policies that adapt to shifting demand
while coordinating decisions across related products. We present a systematic
empirical study of multi-agent reinforcement learning for retail price
optimization, comparing a strong MAPPO baseline with a
graph-attention-augmented variant (MAPPO+GAT) that leverages learned
interactions among products. Using a simulated pricing environment derived from
real transaction data, we evaluate profit, stability across random seeds,
fairness across products, and training efficiency under a standardized
evaluation protocol. The results indicate that MAPPO provides a robust and
reproducible foundation for portfolio-level price control, and that MAPPO+GAT
further enhances performance by sharing information over the product graph
without inducing excessive price volatility. These results indicate that
graph-integrated MARL provides a more scalable and stable solution than
independent learners for dynamic retail pricing, offering practical advantages
in multi-product decision-making.

</details>


### [19] [GEPOC Parameters - Open Source Parametrisation and Validation for Austria, Version 2.0](https://arxiv.org/abs/2511.00048)
*Martin Bicher,Maximilian Viehauser,Daniele Giannandrea,Hannah Kastinger,Dominik Brunmeir,Claire Rippinger,Christoph Urach,Niki Popper*

Main category: cs.AI

TL;DR: 本文档全面描述了基于可自由公开获取的数据计算奥地利模型参数的数据处理方法。


<details>
  <summary>Details</summary>
Motivation: 为了针对特定国家或区域有效应用模型，稳定且可重复的数据过程是必要的，这些过程提供有效且可用的模型参数。

Method: 本文档描述了用于聚合、分解、融合、清理或缩放数据的所有算法，以及对生成的参数文件的描述。

Result: 使用 GEPOC ABM 模型进行了广泛的验证研究，并在本文档末尾进行了展示。

Conclusion: 本文档为奥地利 GEPOC 模型参数的计算提供了完整的数据处理方法。

Abstract: GEPOC, short for Generic Population Concept, is a collection of models and
methods for analysing population-level research questions. For the valid
application of the models for a specific country or region, stable and
reproducible data processes are necessary, which provide valid and ready-to-use
model parameters. This work contains a complete description of the
data-processing methods for computation of model parameters for Austria, based
exclusively on freely and publicly accessible data. In addition to the
description of the source data used, this includes all algorithms used for
aggregation, disaggregation, fusion, cleansing or scaling of the data, as well
as a description of the resulting parameter files. The document places
particular emphasis on the computation of parameters for the most important
GEPOC model, GEPOC ABM, a continuous-time agent-based population model. An
extensive validation study using this particular model was made and is
presented at the end of this work.

</details>


### [20] [QuantumBench: A Benchmark for Quantum Problem Solving](https://arxiv.org/abs/2511.00092)
*Shunya Minami,Tatsuya Ishigaki,Ikko Hamamura,Taku Mikuriya,Youmi Ma,Naoaki Okazaki,Hiroya Takamura,Yohichi Suzuki,Tadashi Kadowaki*

Main category: cs.AI

TL;DR: 论文介绍了QuantumBench，一个用于评估大型语言模型在量子科学领域能力的基准数据集。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型是否准确捕捉特定领域的知识和符号的需求日益增长，尤其是在具有非直观现象和需要高等数学的量子科学领域。

Method: 作者利用公开可用的材料，整理了约800个问题及其答案，涵盖量子科学相关的九个领域，并将它们组织成一个八个选项的多项选择题数据集。

Result: 作者使用该基准评估了几个现有的LLM，并分析了它们在量子领域的表现，包括对问题形式变化的敏感性。

Conclusion: QuantumBench是第一个为量子领域构建的LLM评估数据集，旨在指导LLM在量子研究中的有效使用。

Abstract: Large language models are now integrated into many scientific workflows,
accelerating data analysis, hypothesis generation, and design space
exploration. In parallel with this growth, there is a growing need to carefully
evaluate whether models accurately capture domain-specific knowledge and
notation, since general-purpose benchmarks rarely reflect these requirements.
This gap is especially clear in quantum science, which features non-intuitive
phenomena and requires advanced mathematics. In this study, we introduce
QuantumBench, a benchmark for the quantum domain that systematically examine
how well LLMs understand and can be applied to this non-intuitive field. Using
publicly available materials, we compiled approximately 800 questions with
their answers spanning nine areas related to quantum science and organized them
into an eight-option multiple-choice dataset. With this benchmark, we evaluate
several existing LLMs and analyze their performance in the quantum domain,
including sensitivity to changes in question format. QuantumBench is the first
LLM evaluation dataset built for the quantum domain, and it is intended to
guide the effective use of LLMs in quantum research.

</details>


### [21] [Engineering.ai: A Platform for Teams of AI Engineers in Computational Design](https://arxiv.org/abs/2511.00122)
*Ran Xu,Yupeng Qi,Jingsen Feng,Xu Chu*

Main category: cs.AI

TL;DR: Engineering.ai is a platform for AI engineers to collaboratively design complex products.


<details>
  <summary>Details</summary>
Motivation: Modern engineering design is complex, time-consuming, and costly due to the need for specialized teams and extensive communication.

Method: A hierarchical multi-agent architecture is used, with a Chief Engineer coordinating specialized agents (Aerodynamics, Structural, Acoustic, and Optimization) powered by LLMs. Communication is file-mediated, and a memory system maintains project context.

Result: The system achieved a 100% success rate across over 400 parametric configurations in UAV wing optimization, with no failures or manual interventions.

Conclusion: Agentic-AI-enabled AI engineers have the potential to perform complex engineering tasks autonomously, and the framework is trustworthy.

Abstract: In modern engineering practice, human engineers collaborate in specialized
teams to design complex products, with each expert completing their respective
tasks while communicating and exchanging results and data with one another.
While this division of expertise is essential for managing multidisciplinary
complexity, it demands substantial development time and cost. Recently, we
introduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer
for computational fluid dynamics, and turbulence.ai, which can conduct
end-to-end research in fluid mechanics draft publications and PhD theses.
Building upon these foundations, we present Engineering.ai, a platform for
teams of AI engineers in computational design. The framework employs a
hierarchical multi-agent architecture where a Chief Engineer coordinates
specialized agents consisting of Aerodynamics, Structural, Acoustic, and
Optimization Engineers, each powered by LLM with domain-specific knowledge.
Agent-agent collaboration is achieved through file-mediated communication for
data provenance and reproducibility, while a comprehensive memory system
maintains project context, execution history, and retrieval-augmented domain
knowledge to ensure reliable decision-making across the workflow. The system
integrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,
enabling parallel multidisciplinary simulations while maintaining computational
accuracy. The framework is validated through UAV wing optimization. This work
demonstrates that agentic-AI-enabled AI engineers has the potential to perform
complex engineering tasks autonomously. Remarkably, the automated workflow
achieved a 100% success rate across over 400 parametric configurations, with
zero mesh generation failures, solver convergence issues, or manual
interventions required, validating that the framework is trustworthy.

</details>


### [22] [ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.00162)
*Michael D. Moffitt*

Main category: cs.AI

TL;DR: ARC-GEN是一个开放源代码程序生成器，旨在扩展原始ARC-AGI训练数据集。


<details>
  <summary>Details</summary>
Motivation: ARC-AGI套件专门用于衡量技能获取效率，这是最先进的机器学习系统迄今为止所缺乏的特性。

Method: 引入ARC-GEN，一个旨在扩展原始ARC-AGI训练数据集的程序生成器。

Result: ARC-GEN是详尽的（覆盖所有四百个任务）和模仿的（更密切地遵守初始ARC-AGI-1版本中包含的分布特性和特征）。

Conclusion: 讨论了使用此生成器建立静态基准测试套件，以验证提交给2025年谷歌代码高尔夫锦标赛的程序的正确性。

Abstract: The Abstraction and Reasoning Corpus remains one of the most compelling and
challenging benchmarks for tracking progress toward achieving Artificial
General Intelligence. In contrast to other evaluation datasets designed to
assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI
suite is specifically targeted at measuring skill acquisition efficiency, a
trait that has (so far) been lacking in even the most sophisticated machine
learning systems. For algorithms that require extensive intra-task exemplars, a
significant constraint imposed by ARC-AGI is the modest cardinality of its
demonstration set, comprising a small number of $\langle$ input, output
$\rangle$ grids per task specifying the corresponding transformation. To
embellish the space of viable sample pairs, this paper introduces ARC-GEN, an
open-source procedural generator aimed at extending the original ARC-AGI
training dataset as faithfully as possible. Unlike prior efforts, our generator
is both exhaustive (covering all four-hundred tasks) and mimetic (more closely
honoring the distributional properties and characteristics embodied in the
initial ARC-AGI-1 release). We also discuss the use of this generator in
establishing a static benchmark suite to verify the correctness of programs
submitted to the 2025 Google Code Golf Championship.

</details>


### [23] [Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures](https://arxiv.org/abs/2511.00194)
*Jovial Cheukam Ngouonou,Ramiz Gindullin,Claude-Guy Quimper,Nicolas Beldiceanu,Remi Douence*

Main category: cs.AI

TL;DR: 改进了[1]中提出的选择算法的增量选择算法


<details>
  <summary>Details</summary>
Motivation: [1]中提出的选择算法

Method: 改进增量选择算法

Result: 证明了所有选定的猜想

Conclusion: NA

Abstract: We present an improved incremental selection algorithm of the selection
algorithm presented in [1] and prove all the selected conjectures.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [24] [NOMAD - Navigating Optimal Model Application to Datastreams](https://arxiv.org/abs/2511.00290)
*Ashwin Gerard Colaco,Sharad Mehrotra,Michael J De Lucia,Kevin Hamlen,Murat Kantarcioglu,Latifur Khan,Ananthram Swami,Bhavani Thuraisingham*

Main category: cs.DB

TL;DR: NOMAD: 智能框架，用于在摄取期间进行数据丰富，通过动态构建模型链来优化实时多类分类。


<details>
  <summary>Details</summary>
Motivation: 在摄取期间优化实时多类分类的数据丰富

Method: 利用更便宜的模型作为初始过滤器，仅在必要时才使用更昂贵的模型，同时通过正式的链安全机制来保证分类质量与指定的角色模型相当。它采用动态信念更新策略，以根据每个事件的预测和变化的数据分布来调整模型选择，并扩展到具有依赖模型的场景，例如提前退出 DNN 和堆叠集成。

Result: 与静态和朴素方法相比，NOMAD 实现了显着的计算节省，同时保持了与最准确（且通常是最昂贵）的模型相当的分类质量。

Conclusion: NOMAD 是一种智能框架，通过动态模型链在数据摄取期间优化实时多类分类，从而在保持分类质量的同时显着节省计算。

Abstract: NOMAD (Navigating Optimal Model Application for Datastreams) is an
intelligent framework for data enrichment during ingestion that optimizes
realtime multiclass classification by dynamically constructing model chains,
i.e ,sequences of machine learning models with varying cost-quality tradeoffs,
selected via a utilitybased criterion. Inspired by predicate ordering
techniques from database query processing, NOMAD leverages cheaper models as
initial filters, proceeding to more expensive models only when necessary, while
guaranteeing classification quality remains comparable to a designated role
model through a formal chain safety mechanism. It employs a dynamic belief
update strategy to adapt model selection based on per event predictions and
shifting data distributions, and extends to scenarios with dependent models
such as earlyexit DNNs and stacking ensembles. Evaluation across multiple
datasets demonstrates that NOMAD achieves significant computational savings
compared to static and naive approaches while maintaining classification
quality comparable to that achieved by the most accurate (and often the most
expensive) model.

</details>


### [25] [Embedding based Encoding Scheme for Privacy Preserving Record Linkage](https://arxiv.org/abs/2511.00414)
*Sirintra Vaiwsri,Thilina Ranbaduge*

Main category: cs.DB

TL;DR: 本文研究了如何在隐私保护记录链接(PPRL)中使用基于嵌入的编码技术，以确保被链接实体的隐私。


<details>
  <summary>Details</summary>
Motivation: 数据集成中的一个关键任务是计算不同数据库中记录之间的相似性，以识别对应于同一现实世界实体的记录对或集合。由于隐私和保密问题，敏感数据库的所有者通常不被允许或不愿意与其他组织交换或共享其数据，以进行此类相似性计算。

Method: 首先将单个q-gram转换为嵌入空间，然后将给定记录的一组q-gram的嵌入转换为二进制表示。最终的二进制表示可以用于将记录链接到匹配和非匹配项。

Result: 实验结果表明，对于短记录值，与最先进的技术相比，该编码方法可以提供更好的链接准确性，并保护个人隐私免受攻击。

Conclusion: 提出了一种基于嵌入的编码技术，可以应用于PPRL上下文中，以确保被链接实体的隐私。

Abstract: To discover new insights from data, there is a growing need to share
information that is often held by different organisations. One key task in data
integration is the calculation of similarities between records in different
databases to identify pairs or sets of records that correspond to the same
real-world entities. Due to privacy and confidentiality concerns, however, the
owners of sensitive databases are often not allowed or willing to exchange or
share their data with other organisations to allow such similarity
calculations. Privacy-preserving record linkage (PPRL) is the process of
matching records that refer to the same entity across sensitive databases held
by different organisations while ensuring no information about the entities is
revealed to the participating parties. In this paper, we study how embedding
based encoding techniques can be applied in the PPRL context to ensure the
privacy of the entities that are being linked. We first convert individual
q-grams into the embedded space and then convert the embedding of a set of
q-grams of a given record into a binary representation. The final binary
representations can be used to link records into matches and non-matches. We
empirically evaluate our proposed encoding technique against different
real-world datasets. The results suggest that our proposed encoding approach
can provide better linkage accuracy and protect the privacy of individuals
against attack compared to state-of-the-art techniques for short record values.

</details>


### [26] [Object-Centric Analysis of XES Event Logs: Integrating OCED Modeling with SPARQL Queries](https://arxiv.org/abs/2511.00693)
*Saba Latif,Huma Latif,Muhammad Rameez Ur Rahman*

Main category: cs.DB

TL;DR: 本研究提出使用面向对象事件数据本体（OCEDO）来克服 XES 标准在过程挖掘事件日志中的局限性。


<details>
  <summary>Details</summary>
Motivation: 过程挖掘领域中，面向对象事件数据（OCED）近年来备受关注。然而，仍然存在许多挑战，例如将 XES 格式连接到以对象为中心的方法，以实现更深入的分析。

Method: 将 OCEDO 方法与 SPARQL 查询集成，应用于 BPIC 2013 数据集，以使事件和对象之间的关系更加明确。

Result: 结果表明，与传统方法相比，以对象为中心的建模可以进行更丰富的分析，并提高了过程数据的完整性和可读性。

Conclusion: 该方法改进了过程数据的完整性和可读性，表明面向对象的建模能够实现比传统方法更丰富的分析。

Abstract: Object Centric Event Data (OCED) has gained attention in recent years within
the field of process mining. However, there are still many challenges, such as
connecting the XES format to object-centric approaches to enable more
insightful analysis. It is important for a process miner to understand the
insights and dependencies of events in the event log to see what is going on in
our processes. In previous standards, the dependencies of event logs are only
used to show events, but not their dependencies among each other and actions in
detail as described in OCEDO. There is more information in the event log when
it is revealed using the OCEDO model. It becomes more understandable and easier
to grasp the concepts and deal with the processes. This paper proposes the use
of Object-Centric Event Data Ontology (OCEDO) to overcome the limitations of
the XES standard in event logs for process mining. We demonstrate how the OCEDO
approach, integrated with SPARQL queries, can be applied to the BPIC 2013
dataset to make the relationships between events and objects more explicit. It
describes dealing with the meta descriptions of the OCEDO model on a business
process challenge as an event log. It improves the completeness and readability
of process data, suggesting that object-centric modeling allows for richer
analyses than traditional approaches.

</details>


### [27] [Finding Non-Redundant Simpson's Paradox from Multidimensional Data](https://arxiv.org/abs/2511.00748)
*Yi Yang,Jian Pei,Jun Yang,Jichun Xie*

Main category: cs.DB

TL;DR: 本文提出了一种发现非冗余辛普森悖论的框架，解决了现有方法忽略冗余悖论的问题。


<details>
  <summary>Details</summary>
Motivation: 现有辛普森悖论检测方法忽略了冗余悖论的问题，导致计算成本增加并掩盖了本质模式。

Method: 本文形式化了三种冗余类型，并提出了一种简洁的表示框架，结合深度优先物化和冗余感知悖论发现算法。

Result: 实验表明，冗余悖论广泛存在，在某些真实数据集中占所有悖论的40%以上，并且本文的算法可以扩展到数百万条记录，运行时间减少高达60%，并发现结构上对数据扰动具有鲁棒性的悖论。

Conclusion: 本文证明了辛普森悖论可以在大型多维数据集中有效地识别、简洁地总结和有意义地解释。

Abstract: Simpson's paradox, a long-standing statistical phenomenon, describes the
reversal of an observed association when data are disaggregated into
sub-populations. It has critical implications across statistics, epidemiology,
economics, and causal inference. Existing methods for detecting Simpson's
paradox overlook a key issue: many paradoxes are redundant, arising from
equivalent selections of data subsets, identical partitioning of
sub-populations, and correlated outcome variables, which obscure essential
patterns and inflate computational cost. In this paper, we present the first
framework for discovering non-redundant Simpson's paradoxes. We formalize three
types of redundancy - sibling child, separator, and statistic equivalence - and
show that redundancy forms an equivalence relation. Leveraging this insight, we
propose a concise representation framework for systematically organizing
redundant paradoxes and design efficient algorithms that integrate depth-first
materialization of the base table with redundancy-aware paradox discovery.
Experiments on real-world datasets and synthetic benchmarks show that redundant
paradoxes are widespread, on some real datasets constituting over 40% of all
paradoxes, while our algorithms scale to millions of records, reduce run time
by up to 60%, and discover paradoxes that are structurally robust under data
perturbation. These results demonstrate that Simpson's paradoxes can be
efficiently identified, concisely summarized, and meaningfully interpreted in
large multidimensional datasets.

</details>


### [28] [Reliable Curation of EHR Dataset via Large Language Models under Environmental Constraints](https://arxiv.org/abs/2511.00772)
*Raymond M. Xiong,Panyu Chen,Tianze Dong,Jian Lu,Benjamin Goldstein,Danyang Zhuo,Anru R. Zhang*

Main category: cs.DB

TL;DR: CELEC is an LLM-powered framework for automated EHR data extraction and analytics.


<details>
  <summary>Details</summary>
Motivation: Many researchers lack the database expertise necessary to write complex SQL queries or generate effective visualizations, limiting efficient data use and scientific discovery.

Method: CELEC translates natural language queries into SQL using a prompting strategy that integrates schema information, few-shot demonstrations, and chain-of-thought reasoning.

Result: CELEC achieves execution accuracy comparable to prior systems while maintaining low latency, cost efficiency, and strict privacy.

Conclusion: CELEC streamlines research workflows and accelerates biomedical discovery by lowering technical barriers.

Abstract: Electronic health records (EHRs) are central to modern healthcare delivery
and research; yet, many researchers lack the database expertise necessary to
write complex SQL queries or generate effective visualizations, limiting
efficient data use and scientific discovery. To address this barrier, we
introduce CELEC, a large language model (LLM)-powered framework for automated
EHR data extraction and analytics. CELEC translates natural language queries
into SQL using a prompting strategy that integrates schema information,
few-shot demonstrations, and chain-of-thought reasoning, which together improve
accuracy and robustness. On a subset of the EHRSQL benchmark, CELEC achieves
execution accuracy comparable to prior systems while maintaining low latency,
cost efficiency, and strict privacy by exposing only database metadata to the
LLM. CELEC also adheres to strict privacy protocols: the LLM accesses only
database metadata (e.g., table and column names), while all query execution
occurs securely within the institutional environment, ensuring that no
patient-level data is ever transmitted to or shared with the LLM. Ablation
studies confirm that each component of the SQL generation pipeline,
particularly the few-shot demonstrations, plays a critical role in performance.
By lowering technical barriers and enabling medical researchers to query EHR
databases directly, CELEC streamlines research workflows and accelerates
biomedical discovery.

</details>


### [29] [Efficient Query Repair for Aggregate Constraints](https://arxiv.org/abs/2511.00826)
*Shatha Algarni,Boris Glavic,Seokki Lee,Adriane Chapman*

Main category: cs.DB

TL;DR: 该论文研究了如何修改查询的过滤谓词以满足约束条件，特别是那些可以表示为在查询结果上评估的聚合算术组合的约束条件。


<details>
  <summary>Details</summary>
Motivation: 在许多实际场景中，查询结果必须满足特定领域的约束。例如，根据资格选择的面试候选人中，女性必须占最低百分比。

Method: 提出了一种新的查询修复技术，该技术利用候选解决方案集的界限和区间算术来有效地修剪搜索空间。

Result: 实验表明，该技术明显优于一次考虑单个候选对象的基线方法。

Conclusion: 该论文提出了一种有效的查询修复技术，可以满足查询结果上的约束条件。

Abstract: In many real-world scenarios, query results must satisfy domain-specific
constraints. For instance, a minimum percentage of interview candidates
selected based on their qualifications should be female. These requirements can
be expressed as constraints over an arithmetic combination of aggregates
evaluated on the result of the query. In this work, we study how to repair a
query to fulfill such constraints by modifying the filter predicates of the
query. We introduce a novel query repair technique that leverages bounds on
sets of candidate solutions and interval arithmetic to efficiently prune the
search space. We demonstrate experimentally, that our technique significantly
outperforms baselines that consider a single candidate at a time.

</details>


### [30] [All-in-one Graph-based Indexing for Hybrid Search on GPUs](https://arxiv.org/abs/2511.00855)
*Zhonggen Li,Yougen Li,Yifan Zhu,Zhaoqiang Chen,Yunjun Gao*

Main category: cs.DB

TL;DR: Allan-Poe: A GPU-accelerated graph index for efficient hybrid search, overcoming limitations of existing methods with a unified structure integrating multiple retrieval paths.


<details>
  <summary>Details</summary>
Motivation: Existing hybrid search methods sacrifice flexibility for efficiency, suffer from accuracy degradation, or have prohibitive storage overhead.

Method: A unified graph-based index integrating dense vector, sparse vector, full-text, and knowledge graph retrieval paths, with a GPU-accelerated pipeline for efficient construction and a dynamic fusion framework for query processing.

Result: Allan-Poe achieves superior query accuracy and outperforms state-of-the-art methods by 1.5-186.4x in throughput, while significantly reducing storage overhead on 6 real-world datasets.

Conclusion: Allan-Poe is a novel all-in-one graph index that enables efficient and accurate hybrid search by unifying multiple retrieval paths in a single, cohesive structure.

Abstract: Hybrid search has emerged as a promising paradigm to overcome the limitations
of single-path retrieval, enhancing accuracy for applications like
recommendations, information retrieval, and Retrieval-Augmented Generation.
However, existing methods are constrained by a trilemma: they sacrifice
flexibility for efficiency, suffer from accuracy degradation due to separate
retrievals, or incur prohibitive storage overhead for flexible combinations of
retrieval paths. This paper introduces Allan-Poe, a novel All-in-one graph
index accelerated by GPUs for efficient hybrid search. We first analyze the
limitations of existing retrieval paradigms and distill key design principles
for an effective hybrid search index. Guided by these principles, we architect
a unified graph-based index that flexibly integrates four retrieval paths-dense
vector, sparse vector, full-text, and knowledge graph-within a single, cohesive
structure. To enable efficient construction, we design a GPU-accelerated
pipeline featuring a warp-level hybrid distance kernel, RNG-IP joint pruning,
and keyword-aware neighbor recycling. For query processing, we introduce a
dynamic fusion framework that supports any combination of retrieval paths and
weights without index reconstruction, leveraging logical edges from the
knowledge graph to resolve complex multi-hop queries. Extensive experiments on
6 real-world datasets demonstrate that Allan-Poe achieves superior end-to-end
query accuracy and outperforms state-of-the-art methods by 1.5-186.4x in
throughput, while significantly reducing storage overhead.

</details>


### [31] [FlowLog: Efficient and Extensible Datalog via Incrementality](https://arxiv.org/abs/2511.00865)
*Hangdong Zhao,Zhenghong Yu,Srinag Rao,Simon Frisk,Zhiwei Fan,Paraschos Koutris*

Main category: cs.DB

TL;DR: FlowLog是一个新的Datalog引擎，它通过每个规则的显式关系IR来清晰地分离递归控制和每个规则的逻辑计划，从而弥合了效率和可扩展性之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有的系统通常面临效率和可扩展性之间的权衡。像Souffle这样的引擎通过特定于领域的设计实现高效率，但缺乏通用的灵活性。其他的，像RecStep，通过在传统数据库上分层Datalog来提供模块化，但难以整合Datalog特定的优化。

Method: FlowLog使用每个规则的显式关系IR来清晰地分离递归控制（例如，半朴素执行）和每个规则的逻辑计划。在逻辑层面（即IR），我们应用了经过验证的SQL优化，例如逻辑融合和子计划重用。为了解决递归工作负载中的高波动性，我们采用了一种稳健性优先的方法，该方法将结构优化器（避免最坏情况的连接）与横向信息传递（早期过滤）相结合。FlowLog构建在Differential Dataflow之上，这是一个成熟的流分析框架，支持批处理和增量Datalog，并增加了新颖的递归感知优化，称为布尔（或代数）专业化。

Result: FlowLog在广泛的递归工作负载中优于最先进的Datalog引擎和现代数据库，实现了卓越的可扩展性，同时保留了一个简单且可扩展的架构。

Conclusion: FlowLog是一个很有前途的Datalog引擎，它在效率、可扩展性和灵活性之间取得了良好的平衡。

Abstract: Datalog-based languages are regaining popularity as a powerful abstraction
for expressing recursive computations in domains such as program analysis and
graph processing. However, existing systems often face a trade-off between
efficiency and extensibility. Engines like Souffle achieve high efficiency
through domain-specific designs, but lack general-purpose flexibility. Others,
like RecStep, offer modularity by layering Datalog on traditional databases,
but struggle to integrate Datalog-specific optimizations.
  This paper bridges this gap by presenting FlowLog, a new Datalog engine that
uses an explicit relational IR per-rule to cleanly separate recursive control
(e.g., semi-naive execution) from each rule's logical plan. This boundary lets
us retain fine-grained, Datalog-aware optimizations at the logical layer, but
also reuse off-the-shelf database primitives at execution. At the logical level
(i.e. IR), we apply proven SQL optimizations, such as logic fusion and subplan
reuse. To address high volatility in recursive workloads, we adopt a
robustness-first approach that pairs a structural optimizer (avoiding
worst-case joins) with sideways information passing (early filtering). Built
atop Differential Dataflow--a mature framework for streaming analytics--FlowLog
supports both batch and incremental Datalog and adds novel recursion-aware
optimizations called Boolean (or algebraic) specialization. Our evaluation
shows that FlowLog outperforms state-of-the-art Datalog engines and modern
databases across a broad range of recursive workloads, achieving superior
scalability while preserving a simple and extensible architecture.

</details>


### [32] [ORANGE: An Online Reflection ANd GEneration framework with Domain Knowledge for Text-to-SQL](https://arxiv.org/abs/2511.00985)
*Yiwen Jiao,Tonghui Ren,Yuche Gao,Zhenying He,Yinan Jing,Kai Zhang,X. Sean Wang*

Main category: cs.DB

TL;DR: ORANGE: 使用在线自进化框架，通过解析翻译日志中的SQL查询来构建特定于数据库的知识库，以缩小语义差距并提高SQL翻译的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在将自然语言翻译成SQL方面取得了显著进展，但它们的通用知识和数据库的领域特定语义之间仍然存在显著的语义差距。历史翻译日志构成了这种缺失的领域内知识的丰富来源，其中SQL查询本质上封装了数据库模式的真实世界使用模式。现有方法主要增强单个翻译的推理过程，但未能从过去的翻译中积累领域内知识。

Method: 我们引入ORANGE，这是一种在线自进化框架，它通过解析翻译日志中的SQL查询来构建特定于数据库的知识库。为了确保可靠性，我们提出了一种新颖的嵌套Chain-of-Thought SQL-to-Text策略，具有元组语义跟踪，从而减少了知识生成过程中的语义错误。

Result: 在多个基准上的实验证实了ORANGE的实用性，证明了它在实际的Text-to-SQL部署中的有效性，特别是在处理复杂和特定领域的查询时。

Conclusion: ORANGE通过积累包含模式和数据语义的领域内知识，逐步缩小了语义差距，提高了后续SQL翻译的准确性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
translating natural language to SQL, but a significant semantic gap persists
between their general knowledge and domain-specific semantics of databases.
Historical translation logs constitute a rich source of this missing in-domain
knowledge, where SQL queries inherently encapsulate real-world usage patterns
of database schema. Existing methods primarily enhance the reasoning process
for individual translations but fail to accumulate in-domain knowledge from
past translations. We introduce ORANGE, an online self-evolutionary framework
that constructs database-specific knowledge bases by parsing SQL queries from
translation logs. By accumulating in-domain knowledge that contains schema and
data semantics, ORANGE progressively reduces the semantic gap and enhances the
accuracy of subsequent SQL translations. To ensure reliability, we propose a
novel nested Chain-of-Thought SQL-to-Text strategy with tuple-semantic
tracking, which reduces semantic errors during knowledge generation.
Experiments on multiple benchmarks confirm the practicality of ORANGE,
demonstrating its effectiveness for real-world Text-to-SQL deployment,
particularly in handling complex and domain-specific queries.

</details>


### [33] [PathFinder: Efficiently Supporting Conjunctions and Disjunctions for Filtered Approximate Nearest Neighbor Search](https://arxiv.org/abs/2511.00995)
*Tianming Wu,Dixin Tang*

Main category: cs.DB

TL;DR: 这篇论文提出了一种新的索引框架PathFinder，用于支持过滤的近似最近邻搜索（ANNS），它允许用户选择性地创建针对特定属性的ANNS索引，并使用基于成本的优化器来有效地利用它们来处理复杂的过滤器。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图的ANNS索引在支持复杂过滤器方面存在局限性，无法很好地支持多个属性上的任意合取和析取。

Method: PathFinder 包含三个创新技术：1) 一种新的优化指标，用于捕捉查询执行时间和准确性之间的权衡；2) 一种用于处理具有合取和析取的过滤器的两阶段优化；3) 一种索引借用优化，它使用特定于属性的索引来处理另一个属性上的过滤器。

Result: 在四个真实世界数据集上的实验表明，PathFinder 在召回率 0.95 时，查询吞吐量比最佳基线提高了 9.8 倍。

Conclusion: PathFinder 是一种高效的过滤 ANNS 索引框架，它可以通过选择性地创建针对特定属性的 ANNS 索引，并使用基于成本的优化器来有效地利用它们来处理复杂的过滤器，从而提高查询吞吐量。

Abstract: Filtered approximate nearest neighbor search (ANNS) restricts the search to
data objects whose attributes satisfy a given filter and retrieves the top-$K$
objects that are most semantically similar to the query object. Many
graph-based ANNS indexes are proposed to enable efficient filtered ANNS but
remain limited in applicability or performance: indexes optimized for a
specific attribute achieve high efficiency for filters on that attribute but
fail to support complex filters with arbitrary conjunctions and disjunctions
over multiple attributes. Inspired by the design of relational databases, this
paper presents PathFinder, a new indexing framework that allows users to
selectively create ANNS indexes optimized for filters on specific attributes
and employs a cost-based optimizer to efficiently utilize them for processing
complex filters. PathFinder includes three novel techniques: 1) a new
optimization metric that captures the tradeoff between query execution time and
accuracy, 2) a two-phase optimization for handling filters with conjunctions
and disjunctions, and 3) an index borrowing optimization that uses an
attribute-specific index to process filters on another attribute. Experiments
on four real-world datasets show that PathFinder outperforms the best baseline
by up to 9.8x in query throughput at recall 0.95.

</details>


### [34] [Fast Answering Pattern-Constrained Reachability Queries with Two-Dimensional Reachability Index](https://arxiv.org/abs/2511.01025)
*Huihui Yang,Pingpeng Yuan*

Main category: cs.DB

TL;DR: 本文提出了模式约束可达性查询（PCR查询），以支持用户通过组合查询模式来描述复杂查询需求。为了提高查询效率，构建了一个二维可达性索引（TDR），包含多路索引（水平维度）和路径索引（垂直维度）。


<details>
  <summary>Details</summary>
Motivation: 现有的可达性查询，如标签约束可达性（LCR）查询和正则路径查询（RPQ），无法让用户通过组合查询模式来描述复杂查询需求。

Method: 提出了模式约束可达性查询（PCR查询）并构建了一个二维可达性索引（TDR），它由多路索引（水平维度）和路径索引（垂直维度）组成。顶点可达的顶点被分解成块，分别哈希到水平和垂直维度索引中，以作为全局和局部过滤器来缩小搜索空间。

Result: 实验结果表明，在16个真实数据集上，我们的索引大小和索引时间优于最先进的标签约束可达性索引技术。TDR可以有效地回答模式约束的可达性查询，包括标签约束的可达性查询。

Conclusion: 本文提出的PCR查询和TDR索引能够有效地解决复杂图结构上的可达性查询问题。

Abstract: Reachability queries ask whether there exists a path from the source vertex
to the target vertex on a graph. Recently, several powerful reachability
queries, such as Label-Constrained Reachability (LCR) queries and Regular Path
Queries (RPQ), have been proposed for emerging complex edge-labeled digraphs.
However, they cannot allow users to describe complex query requirements by
composing query patterns. Here, we introduce composite patterns, a logical
expression of patterns that can express complex constraints on the set of
labels. Based on pattern, we propose pattern-constrained reachability queries
(PCR queries). However, answering PCR queries is NP-hard. Thus, to improve the
performance to answer PCR queries, we build a two-dimensional reachability (TDR
for short) index which consists of a multi-way index (horizontal dimension) and
a path index (vertical dimension). Because the number of combinations of both
labels and vertices is exponential, it is very expensive to build full indices
that contain all the reachability information. Thus, the reachable vertices of
a vertex are decomposed into blocks, each of which is hashed into the
horizontal dimension index and the vertical dimension index, respectively. The
indices in the horizontal dimension and the vertical dimension serve as a
global filter and a local filter, respectively, to prune the search space.
Experimental results demonstrate that our index size and indexing time
outperform the state-of-the-art label-constrained reachability indexing
technique on 16 real datasets. TDR can efficiently answer pattern-constrained
reachability queries, including label-constrained reachability queries.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [35] [LookSync: Large-Scale Visual Product Search System for AI-Generated Fashion Looks](https://arxiv.org/abs/2511.00072)
*Pradeep M,Ritesh Pallod,Satyen Abrol,Muthu Raman,Ian Anderson*

Main category: cs.IR

TL;DR: 该论文提出了一种端到端产品搜索系统，用于匹配AI生成的时尚外观和真实产品。


<details>
  <summary>Details</summary>
Motivation: 为了找到与AI生成的风格最匹配的真实产品，因为生成式AI正在重塑时尚。

Method: 该搜索管道由四个关键组件组成：查询生成、向量化、候选检索和基于AI生成外观的重新排序。

Result: CLIP在平均意见得分方面以3-7%的小幅相对优势优于其他模型。该系统目前每天在生产中服务超过350,000个AI外观，涵盖全球市场超过1200万种产品的各种产品类别。

Conclusion: CLIP是生产部署中最可靠的骨干。

Abstract: Generative AI is reshaping fashion by enabling virtual looks and avatars
making it essential to find real products that best match AI-generated styles.
We propose an end-to-end product search system that has been deployed in a
real-world, internet scale which ensures that AI-generated looks presented to
users are matched with the most visually and semantically similar products from
the indexed vector space. The search pipeline is composed of four key
components: query generation, vectorization, candidate retrieval, and reranking
based on AI-generated looks. Recommendation quality is evaluated using
human-judged accuracy scores. The system currently serves more than 350,000 AI
Looks in production per day, covering diverse product categories across global
markets of over 12 million products. In our experiments, we observed that
across multiple annotators and categories, CLIP outperformed alternative models
by a small relative margin of 3--7\% in mean opinion scores. These
improvements, though modest in absolute numbers, resulted in noticeably better
user perception matches, establishing CLIP as the most reliable backbone for
production deployment.

</details>


### [36] [Effectiveness of LLMs in Temporal User Profiling for Recommendation](https://arxiv.org/abs/2511.00176)
*Milad Sabouri,Masoud Mansoury,Kun Lin,Bamshad Mobasher*

Main category: cs.IR

TL;DR: 本文研究了利用大型语言模型（LLM）捕获用户偏好的时间动态，通过生成短期和长期文本摘要来丰富用户表示，从而提高推荐准确性和透明度。研究表明，LLM在用户参与度较高的领域能有效提高推荐质量，但在稀疏环境中效果不明显。


<details>
  <summary>Details</summary>
Motivation: 传统的用户画像忽略了短暂的短期兴趣和稳定的长期偏好之间的区别。本文旨在利用LLM捕获这些时间动态，生成更丰富的用户表示。

Method: 利用LLM生成用户交互历史的短期和长期文本摘要，以此区分短期兴趣和长期偏好。

Result: LLM在用户参与度较高的领域（如电影和电视）能有效提高推荐质量，但在用户画像更稳定的稀疏环境（如视频游戏）中效果不明显。该方法在时间兴趣更易区分的领域表现出更大的效用。

Conclusion: 本文验证了LLM驱动的时间用户画像的实用性和可解释性，为开发自适应和透明的推荐系统提供了新的研究方向。同时强调了性能和计算成本之间的权衡，建议根据具体情况应用LLM。

Abstract: Effectively modeling the dynamic nature of user preferences is crucial for
enhancing recommendation accuracy and fostering transparency in recommender
systems. Traditional user profiling often overlooks the distinction between
transitory short-term interests and stable long-term preferences. This paper
examines the capability of leveraging Large Language Models (LLMs) to capture
these temporal dynamics, generating richer user representations through
distinct short-term and long-term textual summaries of interaction histories.
Our observations suggest that while LLMs tend to improve recommendation quality
in domains with more active user engagement, their benefits appear less
pronounced in sparser environments. This disparity likely stems from the
varying distinguishability of short-term and long-term preferences across
domains; the approach shows greater utility where these temporal interests are
more clearly separable (e.g., Movies\&TV) compared to domains with more stable
user profiles (e.g., Video Games). This highlights a critical trade-off between
enhanced performance and computational costs, suggesting context-dependent LLM
application. Beyond predictive capability, this LLM-driven approach inherently
provides an intrinsic potential for interpretability through its natural
language profiles and attention weights. This work contributes insights into
the practical capability and inherent interpretability of LLM-driven temporal
user profiling, outlining new research directions for developing adaptive and
transparent recommender systems.

</details>


### [37] [Simple and Behavior-Driven Augmentation for Recommendation with Rich Collaborative Signals](https://arxiv.org/abs/2511.00436)
*Doyun Choi,Cheonwoo Lee,Jaemin Yoo*

Main category: cs.IR

TL;DR: 提出了一种新的对比学习推荐增强方法，通过添加或替换伪交互来生成更鲁棒的表示，避免了复杂增强模块的陷阱。


<details>
  <summary>Details</summary>
Motivation: 以往的对比学习图协同过滤方法依赖于去除噪声交互进行数据增强，但噪声的定义模糊，容易丢失核心信息并生成不可靠的数据视图，同时增加增强的复杂性。

Method: 提出了一种名为SCAR的简单协同增强方法，利用用户-物品交互中提取的协同信号生成伪交互，并将其添加到现有交互或用于替换现有交互。

Result: 在四个基准数据集上的实验表明，SCAR优于以往的基于对比学习的图协同过滤方法以及其他最先进的自监督学习方法，并且在不同的超参数设置下表现出强大的鲁棒性，尤其是在稀疏数据场景中。

Conclusion: SCAR通过生成伪交互来实现有效的对比学习图协同过滤增强，避免了复杂增强模块的陷阱，并在多个数据集上取得了优越的性能。

Abstract: Contrastive learning (CL) has been widely used for enhancing the performance
of graph collaborative filtering (GCF) for personalized recommendation. Since
data augmentation plays a crucial role in the success of CL, previous works
have designed augmentation methods to remove noisy interactions between users
and items in order to generate effective augmented views. However, the
ambiguity in defining ''noisiness'' presents a persistent risk of losing core
information and generating unreliable data views, while increasing the overall
complexity of augmentation. In this paper, we propose Simple Collaborative
Augmentation for Recommendation (SCAR), a novel and intuitive augmentation
method designed to maximize the effectiveness of CL for GCF. Instead of
removing information, SCAR leverages collaborative signals extracted from
user-item interactions to generate pseudo-interactions, which are then either
added to or used to replace existing interactions. This results in more robust
representations while avoiding the pitfalls of overly complex augmentation
modules. We conduct experiments on four benchmark datasets and show that SCAR
outperforms previous CL-based GCF methods as well as other state-of-the-art
self-supervised learning approaches across key evaluation metrics. SCAR
exhibits strong robustness across different hyperparameter settings and is
particularly effective in sparse data scenarios.

</details>


### [38] [LIR: The First Workshop on Late Interaction and Multi Vector Retrieval @ ECIR 2026](https://arxiv.org/abs/2511.00444)
*Benjamin Clavié,Xianming Li,Antoine Chaffin,Omar Khattab,Tom Aarsen,Manuel Faysse,Jing Li*

Main category: cs.IR

TL;DR: 本次研讨会旨在为研究人员和实践者提供一个环境，讨论晚期交互的各个方面，重点是早期研究探索、实际成果以及可以自由分享和讨论的负面或令人困惑的结果，促进进一步的合作。


<details>
  <summary>Details</summary>
Motivation: 晚期交互检索方法已成为单向量神经信息检索的强大替代方法，尤其是在领域外设置中，它们具有强大的泛化性和鲁棒性，并且特别适合基于推理或跨模态检索等新颖用例。但这些模型在效率、可用性和集成到完善的系统中提出了重大挑战。

Method: 本次研讨会旨在创建一个高度互动的环境，供来自不同背景的研究人员和实践者自由讨论他们的经验。

Result: 重点讨论早期研究探索、实际成果以及可以自由分享和讨论的负面或令人困惑的结果。

Conclusion: LIR 的目标是为来自不同背景的研究人员和实践者提供一个高度互动的环境，自由讨论他们的经验，从而促进进一步的合作。

Abstract: Late interaction retrieval methods, pioneered by ColBERT, have emerged as a
powerful alternative to single-vector neural IR. By leveraging fine-grained,
token-level representations, they have been demonstrated to deliver strong
generalisation and robustness, particularly in out-of-domain settings. They
have recently been shown to be particularly well-suited for novel use cases,
such as reasoning-based or cross-modality retrieval. At the same time, these
models pose significant challenges of efficiency, usability, and integrations
into fully fledged systems; as well as the natural difficulties encountered
while researching novel application domains. Recent years have seen rapid
advances across many of these areas, but research efforts remain fragmented
across communities and frequently exclude practitioners. The purpose of this
workshop is to create an environment where all aspects of late interaction can
be discussed, with a focus on early research explorations, real-world outcomes,
and negative or puzzling results to be freely shared and discussed. The aim of
LIR is to provide a highly-interactive environment for researchers from various
backgrounds and practitioners to freely discuss their experience, fostering
further collaboration.

</details>


### [39] [Listwise Preference Diffusion Optimization for User Behavior Trajectories Prediction](https://arxiv.org/abs/2511.00530)
*Hongtao Huang,Chengkai Huang,Junda Wu,Tong Yu,Julian McAuley,Lina Yao*

Main category: cs.IR

TL;DR: 本文提出了一种新的用户行为轨迹预测（UBTP）任务，并设计了一个基于扩散的训练框架LPDO，以优化整个项目序列中的结构化偏好。


<details>
  <summary>Details</summary>
Motivation: 现有范例无法捕捉序列项目之间的全局列表依赖关系。

Method: 该方法结合了Plackett-Luce监督信号，并推导出一个与列表排序似然对齐的严格变分下界，从而在去噪步骤中实现连贯的偏好生成，并克服了先前扩散方法的独立token假设。

Result: 在真实世界用户行为基准上的大量实验表明，LPDO始终优于最先进的基线。

Conclusion: 该研究为使用扩散模型进行结构化偏好学习建立了一个新的基准。

Abstract: Forecasting multi-step user behavior trajectories requires reasoning over
structured preferences across future actions, a challenge overlooked by
traditional sequential recommendation. This problem is critical for
applications such as personalized commerce and adaptive content delivery, where
anticipating a user's complete action sequence enhances both satisfaction and
business outcomes. We identify an essential limitation of existing paradigms:
their inability to capture global, listwise dependencies among sequence items.
To address this, we formulate User Behavior Trajectory Prediction (UBTP) as a
new task setting that explicitly models long-term user preferences. We
introduce Listwise Preference Diffusion Optimization (LPDO), a diffusion-based
training framework that directly optimizes structured preferences over entire
item sequences. LPDO incorporates a Plackett-Luce supervision signal and
derives a tight variational lower bound aligned with listwise ranking
likelihoods, enabling coherent preference generation across denoising steps and
overcoming the independent-token assumption of prior diffusion methods. To
rigorously evaluate multi-step prediction quality, we propose the task-specific
metric Sequential Match (SeqMatch), which measures exact trajectory agreement,
and adopt Perplexity (PPL), which assesses probabilistic fidelity. Extensive
experiments on real-world user behavior benchmarks demonstrate that LPDO
consistently outperforms state-of-the-art baselines, establishing a new
benchmark for structured preference learning with diffusion models.

</details>


### [40] [Structurally Refined Graph Transformer for Multimodal Recommendation](https://arxiv.org/abs/2511.00584)
*Ke Shi,Yan Zhang,Miao Zhang,Lifan Chen,Jiali Yi,Kui Xiao,Xiaoju Hou,Zhifei Li*

Main category: cs.IR

TL;DR: SRGFormer: A structurally optimized multimodal recommendation model.


<details>
  <summary>Details</summary>
Motivation: Current models neglect the distinction between redundant and valuable data, rely on a single semantic framework, and fail to capture the complex interactions between users and items.

Method: Modifies the transformer, embeds multimodal information into a hypergraph structure, and applies self-supervised tasks to user-item collaborative signals.

Result: SRGFormer surpasses previous benchmark models, achieving an average performance improvement of 4.47 percent on the Sports dataset.

Conclusion: SRGFormer enhances the integration of multimodal information, thereby revealing the representational features inherent to the data's modality.

Abstract: Multimodal recommendation systems utilize various types of information,
including images and text, to enhance the effectiveness of recommendations. The
key challenge is predicting user purchasing behavior from the available data.
Current recommendation models prioritize extracting multimodal information
while neglecting the distinction between redundant and valuable data. They also
rely heavily on a single semantic framework (e.g., local or global semantics),
resulting in an incomplete or biased representation of user preferences,
particularly those less expressed in prior interactions. Furthermore, these
approaches fail to capture the complex interactions between users and items,
limiting the model's ability to meet diverse users. To address these
challenges, we present SRGFormer, a structurally optimized multimodal
recommendation model. By modifying the transformer for better integration into
our model, we capture the overall behavior patterns of users. Then, we enhance
structural information by embedding multimodal information into a hypergraph
structure to aid in learning the local structures between users and items.
Meanwhile, applying self-supervised tasks to user-item collaborative signals
enhances the integration of multimodal information, thereby revealing the
representational features inherent to the data's modality. Extensive
experiments on three public datasets reveal that SRGFormer surpasses previous
benchmark models, achieving an average performance improvement of 4.47 percent
on the Sports dataset. The code is publicly available online.

</details>


### [41] [Taxonomy-based Negative Sampling In Personalized Semantic Search for E-commerce](https://arxiv.org/abs/2511.00694)
*Uthman Jinadu,Siawpeng Er,Le Yu,Chen Liang,Bingxin Li,Yi Ding,Aleksandar Velkoski*

Main category: cs.IR

TL;DR: 提出了一种用于电子商务搜索的语义检索模型，该模型将查询和产品嵌入到共享向量空间中，并利用一种新的基于分类的硬负采样 (TB-HNS) 策略来挖掘上下文相关的负样本。


<details>
  <summary>Details</summary>
Motivation: 大型零售商店提供的产品可能是特定领域的，这需要一个能够理解相似商品之间细微差别的模型。用于训练这些模型的抽样技术在大多数情况下，计算成本高昂或后勤具有挑战性。这些模型也没有考虑用户之前的购买模式或行为，因此会检索到与他们无关的商品。

Method: 该方法嵌入查询和产品到共享向量空间，并利用一种新的基于分类的硬负采样 (TB-HNS) 策略来挖掘上下文相关的负样本。为了进一步定制检索，我们通过对每个客户的过去购买历史和行为进行建模，从而整合用户级别的个性化。

Result: 离线实验表明，我们的方法在 Recall@K 上优于 BM25、ANCE 和领先的神经基线，而实际 A/B 测试表明转化率、加入购物车率和平均订单价值大幅提升。我们还证明了我们的分类驱动的负样本减少了训练开销并加速了收敛。

Conclusion: 该论文提出了一种有效的电子商务搜索语义检索模型，该模型在离线和在线实验中均表现出色。

Abstract: Large retail outlets offer products that may be domain-specific, and this
requires having a model that can understand subtle differences in similar
items. Sampling techniques used to train these models are most of the time,
computationally expensive or logistically challenging. These models also do not
factor in users' previous purchase patterns or behavior, thereby retrieving
irrelevant items for them. We present a semantic retrieval model for e-commerce
search that embeds queries and products into a shared vector space and
leverages a novel taxonomy-based hard-negative sampling(TB-HNS) strategy to
mine contextually relevant yet challenging negatives. To further tailor
retrievals, we incorporate user-level personalization by modeling each
customer's past purchase history and behavior. In offline experiments, our
approach outperforms BM25, ANCE and leading neural baselines on Recall@K, while
live A/B testing shows substantial uplifts in conversion rate, add-to-cart
rate, and average order value. We also demonstrate that our taxonomy-driven
negatives reduce training overhead and accelerate convergence, and we share
practical lessons from deploying this system at scale.

</details>


### [42] [REaR: Retrieve, Expand and Refine for Effective Multitable Retrieval](https://arxiv.org/abs/2511.00805)
*Rishita Agarwal,Himanshu Singhal,Peter Baile Chen,Manan Roy Choudhury,Dan Roth,Vivek Gupta*

Main category: cs.IR

TL;DR: REAR是一个三阶段的、不依赖LLM的框架，用于多表检索，它分离了语义相关性和结构可连接性，以实现高效、高保真的检索。


<details>
  <summary>Details</summary>
Motivation: 现有的检索器通常只优化查询-表的相关性，而忽略表-表之间的兼容性。为了解决这个问题。

Method: REAR包含三个阶段：(1) 检索与查询相关的表；(2) 通过快速的、预先计算的列嵌入比较，用结构上可连接的表扩展这些表；(3) 通过修剪噪声或弱相关的候选表来改进它们。

Result: REAR在复杂表格QA数据集（BIRD、MMQA和Spider）上一致地改进了密集/稀疏检索器的性能，同时提高了多表检索质量和下游SQL执行。REAR的性能与最先进的LLM增强检索系统相当，但延迟和成本更低。

Conclusion: REAR是一个实用的、可扩展的构建块，适用于基于表格的下游任务（例如，Text-to-SQL）。

Abstract: Answering natural language queries over relational data often requires
retrieving and reasoning over multiple tables, yet most retrievers optimize
only for query-table relevance and ignore table table compatibility. We
introduce REAR (Retrieve, Expand and Refine), a three-stage, LLM-free framework
that separates semantic relevance from structural joinability for efficient,
high-fidelity multi-table retrieval. REAR (i) retrieves query-aligned tables,
(ii) expands these with structurally joinable tables via fast, precomputed
column-embedding comparisons, and (iii) refines them by pruning noisy or weakly
related candidates. Empirically, REAR is retriever-agnostic and consistently
improves dense/sparse retrievers on complex table QA datasets (BIRD, MMQA, and
Spider) by improving both multi-table retrieval quality and downstream SQL
execution. Despite being LLM-free, it delivers performance competitive with
state-of-the-art LLM-augmented retrieval systems (e.g.,ARM) while achieving
much lower latency and cost. Ablations confirm complementary gains from
expansion and refinement, underscoring REAR as a practical, scalable building
block for table-based downstream tasks (e.g., Text-to-SQL).

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [43] [VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games](https://arxiv.org/abs/2511.00002)
*Yurun Wu,Yousong Sun,Burkhard Wunsche,Jia Wang,Elliott Wen*

Main category: cs.LG

TL;DR: VR内容质量保证是难题；人工成本高，扩展性差。本文提出了VRScout，一个基于深度学习的智能体，可以自动导航VR环境并与虚拟对象交互。


<details>
  <summary>Details</summary>
Motivation: 传统的人工质量保证劳动密集，无法随着行业快速增长而扩展。虽然自动化测试已应用于传统的2D和3D游戏，但由于高维感官输入和严格的实时性能要求，将其扩展到VR引入了独特的困难。

Method: VRScout从人类演示中学习，使用增强的动作分块Transformer预测多步动作序列。为了平衡响应性和精度，我们引入了一个动态可调的滑动范围，可以在运行时调整代理的时间上下文。

Result: VRScout在商业VR游戏中实现了专家级的性能，只需有限的训练数据，同时在消费级硬件上保持60 FPS的实时推理。

Conclusion: VRScout是一个实用且可扩展的自动化VR游戏测试框架，在质量保证和安全审计中具有直接应用。

Abstract: Virtual Reality (VR) has rapidly become a mainstream platform for gaming and
interactive experiences, yet ensuring the quality, safety, and appropriateness
of VR content remains a pressing challenge. Traditional human-based quality
assurance is labor-intensive and cannot scale with the industry's rapid growth.
While automated testing has been applied to traditional 2D and 3D games,
extending it to VR introduces unique difficulties due to high-dimensional
sensory inputs and strict real-time performance requirements. We present
VRScout, a deep learning-based agent capable of autonomously navigating VR
environments and interacting with virtual objects in a human-like and real-time
manner. VRScout learns from human demonstrations using an enhanced Action
Chunking Transformer that predicts multi-step action sequences. This enables
our agent to capture higher-level strategies and generalize across diverse
environments. To balance responsiveness and precision, we introduce a
dynamically adjustable sliding horizon that adapts the agent's temporal context
at runtime. We evaluate VRScout on commercial VR titles and show that it
achieves expert-level performance with only limited training data, while
maintaining real-time inference at 60 FPS on consumer-grade hardware. These
results position VRScout as a practical and scalable framework for automated VR
game testing, with direct applications in both quality assurance and safety
auditing.

</details>


### [44] [Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts](https://arxiv.org/abs/2511.00029)
*Samaksh Bhargav,Zining Zhu*

Main category: cs.LG

TL;DR: 本研究探索了一种利用稀疏自编码器（SAE）引导大型语言模型（LLM）的方法，以提高LLM在安全性和效用性之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有LLM部署方法在处理安全提示时需要调整模型权重等复杂操作，且缺乏系统性的特征选择方法和安全-效用权衡评估。

Method: 利用稀疏自编码器（SAE）提取可解释的特征，并采用创新性的对比提示方法和AI生成提示数据集，高效选择最佳特征进行引导。

Result: 在Llama-3 8B模型上的实验结果表明，该方法在安全性能上提升了18.9%，同时效用性也提升了11.1%。

Conclusion: 有针对性的SAE引导可以克服传统的安全-效用权衡，前提是通过合理的选择方法确定最佳特征。

Abstract: Large Language Model (LLM) deployment requires guiding the LLM to recognize
and not answer unsafe prompts while complying with safe prompts. Previous
methods for achieving this require adjusting model weights along with other
expensive procedures. While recent advances in Sparse Autoencoders (SAEs) have
enabled interpretable feature extraction from LLMs, existing approaches lack
systematic feature selection methods and principled evaluation of
safety-utility tradeoffs. We explored using different steering features and
steering strengths using Sparse Auto Encoders (SAEs) to provide a solution.
Using an accurate and innovative contrasting prompt method with the
AI-Generated Prompts Dataset from teknium/OpenHermes-2p5-Mistral-7B and Air
Bench eu-dataset to efficiently choose the best features in the model to steer,
we tested this method on Llama-3 8B. We conclude that using this method, our
approach achieves an 18.9% improvement in safety performance while
simultaneously increasing utility by 11.1%, demonstrating that targeted SAE
steering can overcome traditional safety-utility tradeoffs when optimal
features are identified through principled selection methods.

</details>


### [45] [Probing Knowledge Holes in Unlearned LLMs](https://arxiv.org/abs/2511.00030)
*Myeongseob Ko,Hoang Anh Just,Charles Fleming,Ming Jin,Ruoxi Jia*

Main category: cs.LG

TL;DR: 机器学习卸载技术旨在选择性地移除预训练中吸收的不良知识，而无需完全重新训练。研究发现，卸载技术可能会意外地造成“知识漏洞”，即标准基准无法捕捉到的良性知识的意外损失。


<details>
  <summary>Details</summary>
Motivation: 现有的卸载技术在移除不良内容的同时，可能会意外地造成良性知识的损失，而标准基准无法捕捉到这些损失。

Method: 提出了一个测试用例生成框架，探索卸载内容的直接邻域和潜在失败的更广泛区域，以探测卸载模型在何处暴露知识漏洞。

Result: 评估表明卸载存在显著的隐藏成本：高达 98.7% 的测试用例产生不相关或无意义的响应，尽管预训练模型可以回答这些问题。

Conclusion: 这些发现表明需要重新思考评估卸载中知识保留的传统方法，超越标准、静态的基准。

Abstract: Machine unlearning has emerged as a prevalent technical solution for
selectively removing unwanted knowledge absorbed during pre-training, without
requiring full retraining. While recent unlearning techniques can effectively
remove undesirable content without severely compromising performance on
standard benchmarks, we find that they may inadvertently create ``knowledge
holes'' -- unintended losses of benign knowledge that standard benchmarks fail
to capture. To probe where unlearned models reveal knowledge holes, we propose
a test case generation framework that explores both immediate neighbors of
unlearned content and broader areas of potential failures. Our evaluation
demonstrates significant hidden costs of unlearning: up to 98.7\% of the test
cases yield irrelevant or nonsensical responses from unlearned models, despite
being answerable by the pretrained model. These findings necessitate rethinking
the conventional approach to evaluating knowledge preservation in unlearning,
moving beyond standard, static benchmarks.

</details>


### [46] [From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators](https://arxiv.org/abs/2511.00032)
*Lei Liu,Zhongyi Yu,Hong Wang,Huanshuo Dong,Haiyang Xin,Hongwei Zhao,Bin Li*

Main category: cs.LG

TL;DR: 提出了Skip-Block Routing (SBR) 框架，以解决神经算子在求解偏微分方程时计算开销大的问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型对所有物理场施加统一的计算成本，而物理场表现出截然不同的复杂性，这是一个根本的不匹配，是效率低下的根源。例如，在湍流中，与稳定流动相比，复杂的涡流区域需要更深的网络处理。

Method: SBR 使用路由机制来学习 tokens 的复杂性和排序，然后在推理过程中应用。然后，在后面的层中，它根据此排序决定传递多少 tokens。

Result: SBR 在不牺牲准确性的前提下，将计算成本降低了大约 50%，同时实现了高达 2 倍的更快推理。

Conclusion: SBR 是一种通用框架，可以无缝集成到各种神经算子中。

Abstract: In recent years, Neural Operators(NO) have gradually emerged as a popular
approach for solving Partial Differential Equations (PDEs). However, their
application to large-scale engineering tasks suffers from significant
computational overhead. And the fact that current models impose a uniform
computational cost while physical fields exhibit vastly different complexities
constitutes a fundamental mismatch, which is the root of this inefficiency. For
instance, in turbulence flows, intricate vortex regions require deeper network
processing compared to stable flows. To address this, we introduce a framework:
Skip-Block Routing (SBR), a general framework designed for Transformer-based
neural operators, capable of being integrated into their multi-layer
architectures. First, SBR uses a routing mechanism to learn the complexity and
ranking of tokens, which is then applied during inference. Then, in later
layers, it decides how many tokens are passed forward based on this ranking.
This way, the model focuses more processing capacity on the tokens that are
more complex. Experiments demonstrate that SBR is a general framework that
seamlessly integrates into various neural operators. Our method reduces
computational cost by approximately 50% in terms of Floating Point Operations
(FLOPs), while still delivering up to 2x faster inference without sacrificing
accuracy.

</details>


### [47] [Neural Architecture Search for global multi-step Forecasting of Energy Production Time Series](https://arxiv.org/abs/2511.00035)
*Georg Velev,Stefan Lessmann*

Main category: cs.LG

TL;DR: 提出了一种基于神经架构搜索（NAS）的框架，用于自动发现时间序列模型，以平衡计算效率、预测性能和泛化能力，用于能源生产时间序列的全局多步短期预测。


<details>
  <summary>Details</summary>
Motivation: 能源领域的短期预测需要预测精度和运行效率，手动配置复杂方法耗时且容易出错，能源相关数据存在时间动态性，需要推广到未见数据。

Method: 设计了一个基于NAS的框架，搜索空间仅包含高效组件，可以捕获能源时间序列的独特模式。此外，还制定了一个新的目标函数，该函数考虑了时间背景下的性能泛化和高维搜索空间不同区域的最大探索。

Result: 在能源生产时间序列上获得的结果表明，通过NAS发现的轻量级架构集合在效率和准确性方面优于最先进的技术，如Transformers以及预训练的预测模型。

Conclusion: NAS可以自动发现高效、准确且泛化能力强的能源生产时间序列预测模型。

Abstract: The dynamic energy sector requires both predictive accuracy and runtime
efficiency for short-term forecasting of energy generation under operational
constraints, where timely and precise predictions are crucial. The manual
configuration of complex methods, which can generate accurate global multi-step
predictions without suffering from a computational bottleneck, represents a
procedure with significant time requirements and high risk for human-made
errors. A further intricacy arises from the temporal dynamics present in
energy-related data. Additionally, the generalization to unseen data is
imperative for continuously deploying forecasting techniques over time. To
overcome these challenges, in this research, we design a neural architecture
search (NAS)-based framework for the automated discovery of time series models
that strike a balance between computational efficiency, predictive performance,
and generalization power for the global, multi-step short-term forecasting of
energy production time series. In particular, we introduce a search space
consisting only of efficient components, which can capture distinctive patterns
of energy time series. Furthermore, we formulate a novel objective function
that accounts for performance generalization in temporal context and the
maximal exploration of different regions of our high-dimensional search space.
The results obtained on energy production time series show that an ensemble of
lightweight architectures discovered with NAS outperforms state-of-the-art
techniques, such as Transformers, as well as pre-trained forecasting models, in
terms of both efficiency and accuracy.

</details>


### [48] [Semi-Supervised Preference Optimization with Limited Feedback](https://arxiv.org/abs/2511.00040)
*Seonggyun Lee,Sungjun Lim,Seojin Park,Soeun Cheon,Kyungwoo Song*

Main category: cs.LG

TL;DR: 提出半监督偏好优化（SSPO）方法，以减少对大量配对反馈数据的依赖，降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法依赖大量配对标注数据，导致资源消耗巨大。

Method: 利用少量配对偏好标签和大量未配对样本，通过理论证明存在最佳奖励阈值，对未配对数据进行伪标签。

Result: 在多个数据集上的实验表明，SSPO 具有显著的数据效率，例如，在仅使用 1% UltraFeedback 数据训练的 Llama3-8B-Instruct 模型上，SSPO 始终优于在 10% UltraFeedback 数据上训练的强大基线。

Conclusion: SSPO 能够有效地从大规模未配对数据中提取潜在偏好，从而在保持人类对齐的同时，大幅降低获取成本。

Abstract: The field of preference optimization has made outstanding contributions to
the alignment of language models with human preferences. Despite these
advancements, recent methods still rely heavily on substantial paired (labeled)
feedback data, leading to substantial resource expenditures. To address these
challenges, we study the problem of Semi-Supervised Preference Optimization
(SSPO) in which the idea is to learn from both a small number of pairwise
preference labels and a large pool of unpaired samples simultaneously. Our key
theoretical contribution proves the existence of an optimal reward threshold
capable of separating winning and losing responses with high probability, which
enables a principled pseudo-labeling of unpaired data. By leveraging these
pseudo-labels, SSPO effectively distills latent preferences from large-scale
unpaired data, thus maintaining human alignment while drastically reducing
acquisition costs. Extensive experiments across datasets validate this
remarkable data efficiency; for instance, SSPO trained with Llama3-8B-Instruct
on just 1% of UltraFeedback consistently surpasses strong baselines trained on
10% of UltraFeedback.

</details>


### [49] [Physics-Informed Neural Network Frameworks for the Analysis of Engineering and Biological Dynamical Systems Governed by Ordinary Differential Equations](https://arxiv.org/abs/2511.00043)
*Tyrus Whitman,Andrew Particka,Christopher Diers,Ian Griffin,Charuka Wickramasinghe,Pradeep Ranaweera*

Main category: cs.LG

TL;DR: 本研究验证了物理信息神经网络（PINNs）在求解常微分方程（ODE）控制的工程和生物动力学系统中的预测能力。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在解决高刚性、冲击、不规则域、奇异扰动、高维或边界不连续等问题时难以收敛，而PINNs提供了一种处理这些挑战性数值场景的有效方法。

Method: 采用经典ODE问题作为受控试验台，系统地评估PINNs框架在受控条件下的准确性、训练效率和泛化能力。通过嵌入物理定律到学习过程中，PINNs可以获得优越的结果。分析了几个基准问题的存在性和唯一性，并在这些模型系统上验证了PINNs方法。

Result: 结果表明，对于复杂问题，数据损失、初始条件损失和残差损失必须通过仔细的加权来适当地平衡，才能收敛到正确的解。超参数（包括网络深度、层宽度、激活函数、学习率、优化算法、权重初始化方案和配置点采样）的系统调整在实现精确解中起着关键作用。此外，嵌入先验知识和对网络架构施加硬约束，显著提高了PINNs的预测能力。

Conclusion: PINNs虽然不是一个通用的解决方案，但通过嵌入物理定律直接到学习过程中，可以获得优越的结果。

Abstract: In this study, we present and validate the predictive capability of the
Physics-Informed Neural Networks (PINNs) methodology for solving a variety of
engineering and biological dynamical systems governed by ordinary differential
equations (ODEs). While traditional numerical methods a re effective for many
ODEs, they often struggle to achieve convergence in problems involving high
stiffness, shocks, irregular domains, singular perturbations, high dimensions,
or boundary discontinuities. Alternatively, PINNs offer a powerful approach for
handling challenging numerical scenarios. In this study, classical ODE problems
are employed as controlled testbeds to systematically evaluate the accuracy,
training efficiency, and generalization capability under controlled conditions
of the PINNs framework. Although not a universal solution, PINNs can achieve
superior results by embedding physical laws directly into the learning process.
We first analyze the existence and uniqueness properties of several benchmark
problems and subsequently validate the PINNs methodology on these model
systems. Our results demonstrate that for complex problems to converge to
correct solutions, the loss function components data loss, initial condition
loss, and residual loss must be appropriately balanced through careful
weighting. We further establish that systematic tuning of hyperparameters,
including network depth, layer width, activation functions, learning rate,
optimization algorithms, w eight initialization schemes, and collocation point
sampling, plays a crucial role in achieving accurate solutions. Additionally,
embedding prior knowledge and imposing hard constraints on the network
architecture, without loss the generality of the ODE system, significantly
enhances the predictive capability of PINNs.

</details>


### [50] [ReLaX-Net: Reusing Layers for Parameter-Efficient Physical Neural Networks](https://arxiv.org/abs/2511.00044)
*Kohei Tsuchiyama,Andre Roehm,Takatomo Mihana,Ryoichi Horisaki*

Main category: cs.LG

TL;DR: 提出了一种名为ReLaX-Net的新型物理神经网络(PNN)架构，通过时间复用增加网络深度并有效利用参数，从而提高计算性能。


<details>
  <summary>Details</summary>
Motivation: 现有的物理神经网络在规模上落后于数字神经网络，并且参数效率较低。

Method: 提出ReLaX-Net架构，该架构采用逐层时间复用方案来增加有效网络深度，并且只需要为现有PNN添加快速开关。

Result: ReLaX-Net在图像分类和自然语言处理任务上进行了数值实验验证，结果表明ReLaX-Net在计算性能上有所提高，并且超过了具有相同数量参数的传统RNN或DNN的性能。

Conclusion: ReLaX-Net通过对传统PNN进行少量修改，实现了计算性能的提升，并观察到良好的扩展性。

Abstract: Physical Neural Networks (PNN) are promising platforms for next-generation
computing systems. However, recent advances in digital neural network
performance are largely driven by the rapid growth in the number of trainable
parameters and, so far, demonstrated PNNs are lagging behind by several orders
of magnitude in terms of scale. This mirrors size and performance constraints
found in early digital neural networks. In that period, efficient reuse of
parameters contributed to the development of parameter-efficient architectures
such as convolutional neural networks.
  In this work, we numerically investigate hardware-friendly weight-tying for
PNNs. Crucially, with many PNN systems, there is a time-scale separation
between the fast dynamic active elements of the forward pass and the only
slowly trainable elements implementing weights and biases. With this in mind,we
propose the Reuse of Layers for eXpanding a Neural Network (ReLaX-Net)
architecture, which employs a simple layer-by-layer time-multiplexing scheme to
increase the effective network depth and efficiently use the number of
parameters. We only require the addition of fast switches for existing PNNs. We
validate ReLaX-Nets via numerical experiments on image classification and
natural language processing tasks. Our results show that ReLaX-Net improves
computational performance with only minor modifications to a conventional PNN.
We observe a favorable scaling, where ReLaX-Nets exceed the performance of
equivalent traditional RNNs or DNNs with the same number of parameters.

</details>
