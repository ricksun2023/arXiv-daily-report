{"id": "2508.11862", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2508.11862", "abs": "https://arxiv.org/abs/2508.11862", "authors": ["Jianfeng Huang", "Ziyao Wang", "Lin Yuan", "Jiajie Wen", "Yihao Cao", "Dongjing Miao", "Yong Wang", "Jiahao Zhang"], "title": "LSM-OPD: Boosting Scan in LSM-Trees by Enabling Direct Computing on Compressed Data", "comment": null, "summary": "Scan-based operations, such as backstage compaction and value filtering, have\nemerged as the main bottleneck for LSM-Trees in supporting contemporary\ndata-intensive applications. For slower external storage devices, such as HDD\nand SATA SSD, the scan performance is primarily limited by the I/O bandwidth\n(i.e., I/O bound) due to the substantial read/write amplifications in\nLSM-Trees. Recent adoption of high-performance storage devices, such as NVMe\nSSD, has transformed the main limitation to be compute-bound, emerging the\nimpact of computational resource consumption caused by inefficient compactions\nand filtering. However, when the value size increases, the bottleneck for scan\nperformance in fast devices gradually shifts towards the I/O bandwidth as well,\nand the overall throughput across all types of devices undergo a dramatic\nreduction. This paper addresses the core issues by proposing LSM-OPD, a Log-S\ntructured M erge-O rder- Preserving Dictionary encoding scheme that enables\ndirect computing on compressed data within LSM-Trees. It first enables\nkey-value-separated data flushing to disk in a densely encoded columnar layout,\nideally with few bytes for a large string value (e.g., 1024 bytes), thereby\nsignificantly alleviating the frequent I/O requests caused by intensive scans.\nThen, it is capable of offloading the costly scan-based operations on large\nvalues, including compaction and value filtering, to lightweight dictionaries\ndue to the order-preserving property. And SIMD-based vectorization can now be\nemployed to maximize the evaluating performance on modern multi-core\nprocessors, further breaking the compute-bound limitations in LSM-trees.\nExtensive experiments demonstrate the superior efficiency of LSM-OPD in\nprocessing various workloads that involve intensive scan-based operations on\ndiverse modern storage devices.", "AI": {"tldr": "LSM-OPD\u662f\u4e00\u79cd\u65e5\u5fd7\u7ed3\u6784\u5408\u5e76-\u987a\u5e8f\u4fdd\u6301\u5b57\u5178\u7f16\u7801\u65b9\u6848\uff0c\u901a\u8fc7\u652f\u6301\u5728\u538b\u7f29\u6570\u636e\u4e0a\u76f4\u63a5\u8ba1\u7b97\uff0c\u89e3\u51b3\u4e86LSM\u6811\u4e2d\u626b\u63cf\u64cd\u4f5c\u7684\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u626b\u63cf\u64cd\u4f5c\uff08\u5982\u540e\u53f0\u538b\u7f29\u548c\u503c\u8fc7\u6ee4\uff09\u5df2\u6210\u4e3aLSM\u6811\u652f\u6301\u5f53\u4ee3\u6570\u636e\u5bc6\u96c6\u578b\u5e94\u7528\u7684\u4e3b\u8981\u74f6\u9888\u3002\u5bf9\u4e8e\u8f83\u6162\u7684\u5916\u90e8\u5b58\u50a8\u8bbe\u5907\uff08\u5982HDD\u548cSATA SSD\uff09\uff0c\u7531\u4e8eLSM\u6811\u4e2d\u5927\u91cf\u7684\u8bfb/\u5199\u653e\u5927\uff0c\u626b\u63cf\u6027\u80fd\u4e3b\u8981\u53d7I/O\u5e26\u5bbd\u7684\u9650\u5236\uff08\u5373I/O bound\uff09\u3002\u6700\u8fd1\u91c7\u7528\u7684\u9ad8\u6027\u80fd\u5b58\u50a8\u8bbe\u5907\uff08\u5982NVMe SSD\uff09\u5df2\u5c06\u4e3b\u8981\u9650\u5236\u8f6c\u53d8\u4e3a\u8ba1\u7b97\u9650\u5236\uff0c\u4ece\u800c\u7a81\u663e\u4e86\u7531\u4f4e\u6548\u538b\u7f29\u548c\u8fc7\u6ee4\u5f15\u8d77\u7684\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u7684\u5f71\u54cd\u3002\u7136\u800c\uff0c\u5f53\u503c\u5927\u5c0f\u589e\u52a0\u65f6\uff0c\u5feb\u901f\u8bbe\u5907\u4e2d\u626b\u63cf\u6027\u80fd\u7684\u74f6\u9888\u4e5f\u9010\u6e10\u8f6c\u5411I/O\u5e26\u5bbd\uff0c\u5e76\u4e14\u6240\u6709\u7c7b\u578b\u8bbe\u5907\u7684\u6574\u4f53\u541e\u5410\u91cf\u90fd\u4f1a\u6025\u5267\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e5\u5fd7\u7ed3\u6784\u5408\u5e76-\u987a\u5e8f\u4fdd\u6301\u5b57\u5178\u7f16\u7801\u65b9\u6848LSM-OPD\uff0c\u8be5\u65b9\u6848\u652f\u6301\u5728LSM\u6811\u4e2d\u76f4\u63a5\u5bf9\u538b\u7f29\u6570\u636e\u8fdb\u884c\u8ba1\u7b97\u3002\u5b83\u9996\u5148\u5b9e\u73b0\u4e86\u952e\u503c\u5206\u79bb\u7684\u6570\u636e\u5237\u65b0\u5230\u78c1\u76d8\uff0c\u91c7\u7528\u5bc6\u96c6\u7f16\u7801\u7684\u5217\u5f0f\u5e03\u5c40\uff0c\u7406\u60f3\u60c5\u51b5\u4e0b\uff0c\u5bf9\u4e8e\u5927\u578b\u5b57\u7b26\u4e32\u503c\uff08\u4f8b\u59821024\u5b57\u8282\uff09\uff0c\u53ea\u9700\u5c11\u91cf\u5b57\u8282\uff0c\u4ece\u800c\u663e\u8457\u51cf\u8f7b\u4e86\u7531\u5bc6\u96c6\u626b\u63cf\u5f15\u8d77\u7684\u9891\u7e41I/O\u8bf7\u6c42\u3002\u7136\u540e\uff0c\u7531\u4e8e\u5176\u987a\u5e8f\u4fdd\u6301\u7279\u6027\uff0c\u5b83\u80fd\u591f\u5c06\u5bf9\u5927\u578b\u503c\u7684\u6602\u8d35\u626b\u63cf\u64cd\u4f5c\uff08\u5305\u62ec\u538b\u7f29\u548c\u503c\u8fc7\u6ee4\uff09\u5378\u8f7d\u5230\u8f7b\u91cf\u7ea7\u5b57\u5178\u3002\u5e76\u4e14\u73b0\u5728\u53ef\u4ee5\u91c7\u7528\u57fa\u4e8eSIMD\u7684\u5411\u91cf\u5316\u6765\u6700\u5927\u5316\u73b0\u4ee3\u591a\u6838\u5904\u7406\u5668\u4e0a\u7684\u8bc4\u4f30\u6027\u80fd\uff0c\u8fdb\u4e00\u6b65\u6253\u7834LSM\u6811\u4e2d\u7684\u8ba1\u7b97\u9650\u5236\u3002", "result": "LSM-OPD\u901a\u8fc7\u5728LSM\u6811\u4e2d\u76f4\u63a5\u5bf9\u538b\u7f29\u6570\u636e\u8fdb\u884c\u8ba1\u7b97\uff0c\u89e3\u51b3\u4e86\u6838\u5fc3\u95ee\u9898\u3002", "conclusion": "LSM-OPD\u5728\u5904\u7406\u6d89\u53ca\u5bf9\u5404\u79cd\u73b0\u4ee3\u5b58\u50a8\u8bbe\u5907\u8fdb\u884c\u5bc6\u96c6\u626b\u63cf\u64cd\u4f5c\u7684\u5404\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6548\u7387\u3002"}}
{"id": "2508.12173", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2508.12173", "abs": "https://arxiv.org/abs/2508.12173", "authors": ["Suyash Gupta", "Dakai Kang", "Dahlia Malkhi", "Mohammad Sadoghi"], "title": "Carry the Tail in Consensus Protocols", "comment": "18 pages, 3 figures", "summary": "We present Carry-the-Tail, the first deterministic atomic broadcast protocol\nin partial synchrony that, after GST, guarantees a constant fraction of commits\nby non-faulty leaders against tail-forking attacks, and maintains optimal,\nworst-case quadratic communication under a cascade of faulty leaders. The\nsolution also guarantees linear amortized communication, i.e., the steady-state\nis linear.\n  Prior atomic broadcast solutions achieve quadratic word communication\ncomplexity in the worst case. However, they face a significant degradation in\nthroughput under tail-forking attack. Existing solutions to tail-forking\nattacks require either quadratic communication steps or\ncomputationally-prohibitive SNARK generation.\n  The key technical contribution is Carry, a practical drop-in mechanism for\nstreamlined protocols in the HotStuff family. Carry guarantees good performance\nagainst tail-forking and removes most leader-induced stalls, while retaining\nlinear traffic and protocol simplicity.", "AI": {"tldr": "Presents Carry-the-Tail, a deterministic atomic broadcast protocol addressing tail-forking attacks with improved communication efficiency.", "motivation": "Existing atomic broadcast solutions face throughput degradation under tail-forking attacks, and solutions require either quadratic communication steps or computationally-prohibitive SNARK generation.", "method": "The key technical contribution is Carry, a practical drop-in mechanism for streamlined protocols in the HotStuff family.", "result": "Carry guarantees good performance against tail-forking and removes most leader-induced stalls, while retaining linear traffic and protocol simplicity.", "conclusion": "The paper introduces Carry-the-Tail, a deterministic atomic broadcast protocol that ensures a constant fraction of commits by non-faulty leaders against tail-forking attacks and maintains optimal communication complexity."}}
{"id": "2508.12536", "categories": ["cs.DB", "cs.DS", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.12536", "abs": "https://arxiv.org/abs/2508.12536", "authors": ["Yasuo Tabei"], "title": "jXBW: Fast Substructure Search in Large-Scale JSONL Datasets for Foundation Model Applications", "comment": null, "summary": "Substructure search in JSON Lines (JSONL) datasets is essential for modern\napplications such as prompt engineering in foundation models, but existing\nmethods suffer from prohibitive computational costs due to exhaustive tree\ntraversal and subtree matching. We present jXBW, a fast method for substructure\nsearch on large-scale JSONL datasets. Our method makes three key technical\ncontributions: (i) a merged tree representation built by merging trees of\nmultiple JSON objects while preserving individual identities, (ii) a succinct\ndata structure based on the eXtended Burrows-Wheeler Transform that enables\nefficient tree navigation and subpath search, and (iii) an efficient three-step\nsubstructure search algorithm that combines path decomposition, ancestor\ncomputation, and adaptive tree identifier collection to ensure correctness\nwhile avoiding exhaustive tree traversal. Experimental evaluation on real-world\ndatasets demonstrates that jXBW consistently outperforms existing methods,\nachieving speedups of 16$\\times$ for smaller datasets and up to 4,700$\\times$\nfor larger datasets over tree-based approaches, and more than 6$\\times$10$^6$\nover XML-based processing while maintaining competitive memory usage.", "AI": {"tldr": "jXBW \u662f\u4e00\u79cd\u5feb\u901f\u7684 JSONL \u5b50\u7ed3\u6784\u641c\u7d22\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u5408\u5e76\u6811\u8868\u793a\u3001\u7b80\u6d01\u7684\u6570\u636e\u7ed3\u6784\u548c\u9ad8\u6548\u7684\u641c\u7d22\u7b97\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "JSON Lines (JSONL) \u6570\u636e\u96c6\u4e2d\u7684\u5b50\u7ed3\u6784\u641c\u7d22\u5bf9\u4e8e\u73b0\u4ee3\u5e94\u7528\u7a0b\u5e8f\uff08\u5982\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u63d0\u793a\u5de5\u7a0b\uff09\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7531\u4e8e\u7a77\u4e3e\u6811\u904d\u5386\u548c\u5b50\u6811\u5339\u914d\u800c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u4e00\u79cd\u5feb\u901f\u7684 JSONL \u6570\u636e\u96c6\u5b50\u7ed3\u6784\u641c\u7d22\u65b9\u6cd5\uff0c\u5b83\u5177\u6709\u4e09\u4e2a\u5173\u952e\u6280\u672f\u8d21\u732e\uff1a(i) \u901a\u8fc7\u5408\u5e76\u591a\u4e2a JSON \u5bf9\u8c61\u7684\u6811\u6784\u5efa\u7684\u5408\u5e76\u6811\u8868\u793a\uff0c\u540c\u65f6\u4fdd\u7559\u5355\u4e2a\u6807\u8bc6\uff0c(ii) \u4e00\u79cd\u57fa\u4e8e\u6269\u5c55 Burrows-Wheeler \u53d8\u6362\u7684\u7b80\u6d01\u6570\u636e\u7ed3\u6784\uff0c\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u6811\u5bfc\u822a\u548c\u5b50\u8def\u5f84\u641c\u7d22\uff0c\u4ee5\u53ca (iii) \u4e00\u79cd\u9ad8\u6548\u7684\u4e09\u6b65\u5b50\u7ed3\u6784\u641c\u7d22\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u7ed3\u5408\u4e86\u8def\u5f84\u5206\u89e3\u3001\u7956\u5148\u8ba1\u7b97\u548c\u81ea\u9002\u5e94\u6811\u6807\u8bc6\u7b26\u6536\u96c6\uff0c\u4ee5\u786e\u4fdd\u6b63\u786e\u6027\uff0c\u540c\u65f6\u907f\u514d\u7a77\u4e3e\u6811\u904d\u5386\u3002", "result": "jXBW \u5728\u8f83\u5c0f\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86 16 \u500d\u7684\u52a0\u901f\uff0c\u5728\u8f83\u5927\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe 4,700 \u500d\u7684\u52a0\u901f\uff08\u76f8\u6bd4\u4e8e\u57fa\u4e8e\u6811\u7684\u65b9\u6cd5\uff09\uff0c\u5e76\u4e14\u6bd4\u57fa\u4e8e XML \u7684\u5904\u7406\u65b9\u6cd5\u5feb 6\u00d710^6 \u500d\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u5185\u5b58\u4f7f\u7528\u3002", "conclusion": "jXBW\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u5b83\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u8f83\u5c0f\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86 16 \u500d\u7684\u52a0\u901f\uff0c\u5728\u8f83\u5927\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe 4,700 \u500d\u7684\u52a0\u901f\uff08\u76f8\u6bd4\u4e8e\u57fa\u4e8e\u6811\u7684\u65b9\u6cd5\uff09\uff0c\u5e76\u4e14\u6bd4\u57fa\u4e8e XML \u7684\u5904\u7406\u65b9\u6cd5\u5feb 6\u00d710^6 \u500d\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u5185\u5b58\u4f7f\u7528\u3002"}}
{"id": "2508.12872", "categories": ["cs.DB", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.12872", "abs": "https://arxiv.org/abs/2508.12872", "authors": ["Franz Okyere", "Meng Lu", "Ansgar Brunn"], "title": "Evaluating the Quality of Open Building Datasets for Mapping Urban Inequality: A Comparative Analysis Across 5 Cities", "comment": "25 pages, 4 pages", "summary": "While informal settlements lack focused development and are highly dynamic,\nthe quality of spatial data for these places may be uncertain. This study\nevaluates the quality and biases of AI-generated Open Building Datasets (OBDs)\ngenerated by Google and Microsoft against OpenStreetMap (OSM) data, across\ndiverse global cities including Accra, Nairobi, Caracas, Berlin, and Houston.\nThe Intersection over Union (IoU), overlap analysis and a positional accuracy\nalgorithm are used to analyse the similarity and alignment of the datasets. The\npaper also analyses the size distribution of the building polygon area, and\ncompleteness using predefined but regular spatial units. The results indicate\nsignificant variance in data quality, with Houston and Berlin demonstrating\nhigh alignment and completeness, reflecting their structured urban\nenvironments. There are gaps in the datasets analysed, and cities like Accra\nand Caracas may be under-represented. This could highlight difficulties in\ncapturing complex or informal regions. The study also notes different building\nsize distributions, which may be indicative of the global socio-economic\ndivide. These findings may emphasise the need to consider the quality of global\nbuilding datasets to avoid misrepresentation, which is an important element of\nplanning and resource distribution.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86AI\u751f\u6210\u7684\u5f00\u653e\u5efa\u7b51\u6570\u636e\u96c6\u5728\u4e0d\u540c\u57ce\u5e02\u7684\u6570\u636e\u8d28\u91cf\uff0c\u53d1\u73b0\u6570\u636e\u8d28\u91cf\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u9700\u8981\u8003\u8651\u6570\u636e\u96c6\u8d28\u91cf\u4ee5\u907f\u514d\u9519\u8bef\u8868\u793a\u3002", "motivation": "\u975e\u6b63\u5f0f\u4f4f\u533a\u7f3a\u4e4f\u91cd\u70b9\u53d1\u5c55\u4e14\u9ad8\u5ea6\u52a8\u6001\uff0c\u8fd9\u4e9b\u5730\u65b9\u7684\u7a7a\u95f4\u6570\u636e\u8d28\u91cf\u53ef\u80fd\u4e0d\u786e\u5b9a\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u7531\u8c37\u6b4c\u548c\u5fae\u8f6f\u751f\u6210\u7684AI\u5f00\u653e\u5efa\u7b51\u6570\u636e\u96c6(OBD)\u7684\u8d28\u91cf\u548c\u504f\u5dee\u3002", "method": "\u4f7f\u7528Intersection over Union (IoU)\u3001\u91cd\u53e0\u5206\u6790\u548c\u4f4d\u7f6e\u7cbe\u5ea6\u7b97\u6cd5\u6765\u5206\u6790\u6570\u636e\u96c6\u7684\u76f8\u4f3c\u6027\u548c\u5bf9\u9f50\u60c5\u51b5\u3002\u8fd8\u5206\u6790\u4e86\u5efa\u7b51\u7269\u591a\u8fb9\u5f62\u533a\u57df\u7684\u5927\u5c0f\u5206\u5e03\u548c\u5b8c\u6574\u6027\u3002", "result": "\u4f11\u65af\u987f\u548c\u67cf\u6797\u8868\u73b0\u51fa\u9ad8\u5ea6\u7684\u4e00\u81f4\u6027\u548c\u5b8c\u6574\u6027\uff0c\u53cd\u6620\u4e86\u5b83\u4eec\u7ed3\u6784\u5316\u7684\u57ce\u5e02\u73af\u5883\u3002\u5728\u6240\u5206\u6790\u7684\u6570\u636e\u96c6\u4e2d\u5b58\u5728\u5dee\u8ddd\uff0c\u963f\u514b\u62c9\u548c\u52a0\u62c9\u52a0\u65af\u7b49\u57ce\u5e02\u53ef\u80fd\u4ee3\u8868\u6027\u4e0d\u8db3\u3002\u4e0d\u540c\u7684\u5efa\u7b51\u89c4\u6a21\u5206\u5e03\u53ef\u80fd\u8868\u660e\u5168\u7403\u793e\u4f1a\u7ecf\u6d4e\u5dee\u8ddd\u3002", "conclusion": "AI\u751f\u6210\u7684\u5f00\u653e\u5efa\u7b51\u6570\u636e\u96c6\uff08OBD\uff09\u7684\u6570\u636e\u8d28\u91cf\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4f11\u65af\u987f\u548c\u67cf\u6797\u8868\u73b0\u51fa\u9ad8\u5ea6\u7684\u4e00\u81f4\u6027\u548c\u5b8c\u6574\u6027\uff0c\u800c\u963f\u514b\u62c9\u548c\u52a0\u62c9\u52a0\u65af\u7b49\u57ce\u5e02\u53ef\u80fd\u4ee3\u8868\u6027\u4e0d\u8db3\u3002\u9700\u8981\u8003\u8651\u5168\u7403\u5efa\u7b51\u6570\u636e\u96c6\u7684\u8d28\u91cf\uff0c\u4ee5\u907f\u514d\u9519\u8bef\u8868\u793a\u3002"}}
{"id": "2508.11670", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11670", "abs": "https://arxiv.org/abs/2508.11670", "authors": ["Bongsu Kim"], "title": "RRRA: Resampling and Reranking through a Retriever Adapter", "comment": "8 pages, 4 figures, submitted to AAAI 2026", "summary": "In dense retrieval, effective training hinges on selecting high quality hard\nnegatives while avoiding false negatives. Recent methods apply heuristics based\non positive document scores to identify hard negatives, improving both\nperformance and interpretability. However, these global, example agnostic\nstrategies often miss instance specific false negatives. To address this, we\npropose a learnable adapter module that monitors Bi-Encoder representations to\nestimate the likelihood that a hard negative is actually a false negative. This\nprobability is modeled dynamically and contextually, enabling fine-grained,\nquery specific judgments. The predicted scores are used in two downstream\ncomponents: (1) resampling, where negatives are reweighted during training, and\n(2) reranking, where top-k retrieved documents are reordered at inference.\nEmpirical results on standard benchmarks show that our adapter-enhanced\nframework consistently outperforms strong Bi-Encoder baselines, underscoring\nthe benefit of explicit false negative modeling in dense retrieval.", "AI": {"tldr": "This paper proposes a learnable adapter module to model false negatives in dense retrieval, which improves performance on standard benchmarks.", "motivation": "Recent methods apply heuristics based on positive document scores to identify hard negatives, improving both performance and interpretability. However, these global, example agnostic strategies often miss instance specific false negatives.", "method": "a learnable adapter module that monitors Bi-Encoder representations to estimate the likelihood that a hard negative is actually a false negative. This probability is modeled dynamically and contextually, enabling fine-grained, query specific judgments. The predicted scores are used in two downstream components: (1) resampling, where negatives are reweighted during training, and (2) reranking, where top-k retrieved documents are reordered at inference.", "result": "Empirical results on standard benchmarks show that our adapter-enhanced framework consistently outperforms strong Bi-Encoder baselines", "conclusion": "The adapter-enhanced framework consistently outperforms strong Bi-Encoder baselines, underscoring the benefit of explicit false negative modeling in dense retrieval."}}
{"id": "2508.11661", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11661", "abs": "https://arxiv.org/abs/2508.11661", "authors": ["Ziyi Cao", "Qingyi Si", "Jingbin Zhang", "Bingquan Liu"], "title": "Sparse Attention across Multiple-context KV Cache", "comment": null, "summary": "Large language models face significant cost challenges in long-sequence\ninference. To address this, reusing historical Key-Value (KV) Cache for\nimproved inference efficiency has become a mainstream approach. Recent advances\nfurther enhance throughput by sparse attention mechanisms to select the most\nrelevant KV Cache, thereby reducing sequence length. However, such techniques\nare limited to single-context scenarios, where historical KV Cache is computed\nsequentially with causal-attention dependencies. In retrieval-augmented\ngeneration (RAG) scenarios, where retrieved documents as context are unknown\nbeforehand, each document's KV Cache is computed and stored independently\n(termed multiple-context KV Cache), lacking cross-attention between contexts.\nThis renders existing methods ineffective. Although prior work partially\nrecomputes multiple-context KV Cache to mitigate accuracy loss from missing\ncross-attention, it requires retaining all KV Cache throughout, failing to\nreduce memory overhead. This paper presents SamKV, the first exploration of\nattention sparsification for multiple-context KV Cache. Specifically, SamKV\ntakes into account the complementary information of other contexts when\nsparsifying one context, and then locally recomputes the sparsified\ninformation. Experiments demonstrate that our method compresses sequence length\nto 15% without accuracy degradation compared with full-recompuation baselines,\nsignificantly boosting throughput in multi-context RAG scenarios.", "AI": {"tldr": "SamKV\u901a\u8fc7\u6ce8\u610f\u529b\u7a00\u758f\u5316\u65b9\u6cd5\uff0c\u6709\u6548\u538b\u7f29\u591a\u4e0a\u4e0b\u6587RAG\u573a\u666f\u4e2d\u7684\u5e8f\u5217\u957f\u5ea6\uff0c\u63d0\u9ad8\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u8bc1\u51c6\u786e\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5e8f\u5217\u63a8\u7406\u4e2d\u9762\u4e34\u663e\u8457\u7684\u6210\u672c\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5728RAG\u573a\u666f\u4e2d\u5bf9\u591a\u4e0a\u4e0b\u6587KV Cache\u8fdb\u884c\u6ce8\u610f\u529b\u7a00\u758f\u5316\u6548\u679c\u4e0d\u4f73\uff0c\u4e14\u5148\u524d\u5de5\u4f5c\u867d\u90e8\u5206\u91cd\u65b0\u8ba1\u7b97\u591a\u4e0a\u4e0b\u6587KV Cache\u4ee5\u51cf\u8f7b\u7cbe\u5ea6\u635f\u5931\uff0c\u4f46\u9700\u8981\u4fdd\u7559\u6240\u6709KV Cache\uff0c\u65e0\u6cd5\u51cf\u5c11\u5185\u5b58\u5f00\u9500\u3002", "method": "\u63d0\u51faSamKV\uff0c\u4e00\u79cd\u9488\u5bf9\u591a\u4e0a\u4e0b\u6587KV Cache\u7684\u6ce8\u610f\u529b\u7a00\u758f\u5316\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5728\u7a00\u758f\u5316\u4e00\u4e2a\u4e0a\u4e0b\u6587\u65f6\uff0c\u8003\u8651\u5176\u4ed6\u4e0a\u4e0b\u6587\u7684\u4e92\u8865\u4fe1\u606f\uff0c\u5e76\u5c40\u90e8\u5730\u91cd\u65b0\u8ba1\u7b97\u7a00\u758f\u5316\u7684\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5b8c\u5168\u91cd\u65b0\u8ba1\u7b97\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u964d\u4f4e\u51c6\u786e\u7387\u7684\u60c5\u51b5\u4e0b\u5c06\u5e8f\u5217\u957f\u5ea6\u538b\u7f29\u523015%\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u4e0a\u4e0b\u6587RAG\u573a\u666f\u4e2d\u7684\u541e\u5410\u91cf\u3002", "conclusion": "SamKV\u5728\u591a\u4e0a\u4e0b\u6587RAG\u573a\u666f\u4e2d\uff0c\u5c06\u5e8f\u5217\u957f\u5ea6\u538b\u7f29\u523015%\u4e14\u4e0d\u964d\u4f4e\u51c6\u786e\u7387\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\u3002"}}
{"id": "2508.11676", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11676", "abs": "https://arxiv.org/abs/2508.11676", "authors": ["Maksym Shamrai", "Vladyslav Hamolia"], "title": "Deep Language Geometry: Constructing a Metric Space from LLM Weights", "comment": "18 pages, accepted to RANLP 2025", "summary": "We introduce a novel framework that utilizes the internal weight activations\nof modern Large Language Models (LLMs) to construct a metric space of\nlanguages. Unlike traditional approaches based on hand-crafted linguistic\nfeatures, our method automatically derives high-dimensional vector\nrepresentations by computing weight importance scores via an adapted pruning\nalgorithm. Our approach captures intrinsic language characteristics that\nreflect linguistic phenomena. We validate our approach across diverse datasets\nand multilingual LLMs, covering 106 languages. The results align well with\nestablished linguistic families while also revealing unexpected inter-language\nconnections that may indicate historical contact or language evolution. The\nsource code, computed language latent vectors, and visualization tool are made\npublicly available at https://github.com/mshamrai/deep-language-geometry.", "AI": {"tldr": "A novel framework that utilizes the internal weight activations of modern Large Language Models (LLMs) to construct a metric space of languages.", "motivation": "We introduce a novel framework that utilizes the internal weight activations of modern Large Language Models (LLMs) to construct a metric space of languages. Unlike traditional approaches based on hand-crafted linguistic features", "method": "Our method automatically derives high-dimensional vector representations by computing weight importance scores via an adapted pruning algorithm.", "result": "We validate our approach across diverse datasets and multilingual LLMs, covering 106 languages.", "conclusion": "The approach aligns well with established linguistic families while also revealing unexpected inter-language connections that may indicate historical contact or language evolution."}}
{"id": "2508.11696", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11696", "abs": "https://arxiv.org/abs/2508.11696", "authors": ["Sami Sadat", "Mohammad Irtiza Hossain", "Junaid Ahmed Sifat", "Suhail Haque Rafi", "Md. Waseq Alauddin Alvi", "Md. Khalilur Rhaman"], "title": "A Deep Learning-Based CCTV System for Automatic Smoking Detection in Fire Exit Zones", "comment": null, "summary": "A deep learning real-time smoking detection system for CCTV surveillance of\nfire exit areas is proposed due to critical safety requirements. The dataset\ncontains 8,124 images from 20 different scenarios along with 2,708 raw samples\ndemonstrating low-light areas. We evaluated three advanced object detection\nmodels: YOLOv8, YOLOv11, and YOLOv12, followed by development of a custom model\nderived from YOLOv8 with added structures for challenging surveillance\ncontexts. The proposed model outperformed the others, achieving a recall of\n78.90 percent and mAP at 50 of 83.70 percent, delivering optimal object\ndetection across varied environments. Performance evaluation on multiple edge\ndevices using multithreaded operations showed the Jetson Xavier NX processed\ndata at 52 to 97 milliseconds per inference, establishing its suitability for\ntime-sensitive operations. This system offers a robust and adaptable platform\nfor monitoring public safety and enabling automatic regulatory compliance.", "AI": {"tldr": "Developed a custom YOLOv8 model for real-time smoking detection, which outperforms existing models on edge devices.", "motivation": "Critical safety requirements in fire exit areas necessitate a real-time smoking detection system.", "method": "Developed a custom model derived from YOLOv8 with added structures.", "result": "The proposed model achieved a recall of 78.90% and mAP at 50 of 83.70%. Jetson Xavier NX processed data at 52 to 97 milliseconds per inference.", "conclusion": "The custom YOLOv8 model is suitable for real-time smoking detection on edge devices."}}
{"id": "2508.11836", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11836", "abs": "https://arxiv.org/abs/2508.11836", "authors": ["Dave Goel", "Matthew Guzdial", "Anurag Sarkar"], "title": "Finite Automata Extraction: Low-data World Model Learning as Programs from Gameplay Video", "comment": null, "summary": "World models are defined as a compressed spatial and temporal learned\nrepresentation of an environment. The learned representation is typically a\nneural network, making transfer of the learned environment dynamics and\nexplainability a challenge. In this paper, we propose an approach, Finite\nAutomata Extraction (FAE), that learns a neuro-symbolic world model from\ngameplay video represented as programs in a novel domain-specific language\n(DSL): Retro Coder. Compared to prior world model approaches, FAE learns a more\nprecise model of the environment and more general code than prior DSL-based\napproaches.", "AI": {"tldr": "learns a neuro-symbolic world model from gameplay video", "motivation": "transfer of the learned environment dynamics and explainability a challenge", "method": "learns a neuro-symbolic world model from gameplay video represented as programs in a novel domain-specific language (DSL): Retro Coder", "result": "learns a more precise model of the environment and more general code than prior DSL-based approaches", "conclusion": "FAE learns a more precise model of the environment."}}
{"id": "2508.13041", "categories": ["cs.DB", "cs.LO", "F.4.1; H.2.3; I.2.3; I.2.4"], "pdf": "https://arxiv.org/pdf/2508.13041", "abs": "https://arxiv.org/abs/2508.13041", "authors": ["D\u00f6rthe Arndt", "William Van Woensel", "Dominik Tomaszuk"], "title": "SPARQL in N3: SPARQL CONSTRUCT as a rule language for the Semantic Web (Extended Version)", "comment": "21 pages, submitted to RuleML+RR 2025: the 9th International Joint\n  Conference on Rules and Reasoning", "summary": "Reasoning in the Semantic Web (SW) commonly uses Description Logics (DL) via\nOWL2 DL ontologies, or SWRL for variables and Horn clauses. The Rule\nInterchange Format (RIF) offers more expressive rules but is defined outside\nRDF and rarely adopted. For querying, SPARQL is a well-established standard\noperating directly on RDF triples. We leverage SPARQL CONSTRUCT queries as\nlogic rules, enabling (1) an expressive, familiar SW rule language, and (2)\ngeneral recursion, where queries can act on the results of others. We translate\nthese queries to the Notation3 Logic (N3) rule language, allowing use of\nexisting reasoning machinery with forward and backward chaining. Targeting a\none-to-one query-rule mapping improves exchangeability and interpretability.\nBenchmarks indicate competitive performance, aiming to advance the potential of\nrule-based reasoning in the SW.", "AI": {"tldr": "Leveraging SPARQL CONSTRUCT queries as logic rules and translating these queries to the Notation3 Logic (N3) rule language.", "motivation": "Reasoning in the Semantic Web (SW) commonly uses Description Logics (DL) via OWL2 DL ontologies, or SWRL for variables and Horn clauses. The Rule Interchange Format (RIF) offers more expressive rules but is defined outside RDF and rarely adopted. For querying, SPARQL is a well-established standard operating directly on RDF triples.", "method": "We leverage SPARQL CONSTRUCT queries as logic rules, enabling (1) an expressive, familiar SW rule language, and (2) general recursion, where queries can act on the results of others. We translate these queries to the Notation3 Logic (N3) rule language, allowing use of existing reasoning machinery with forward and backward chaining. Targeting a one-to-one query-rule mapping improves exchangeability and interpretability.", "result": "Enabling (1) an expressive, familiar SW rule language, and (2) general recursion, where queries can act on the results of others.", "conclusion": "Benchmarks indicate competitive performance, aiming to advance the potential of rule-based reasoning in the SW."}}
{"id": "2508.11671", "categories": ["cs.IR", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.11671", "abs": "https://arxiv.org/abs/2508.11671", "authors": ["Ronald Carvalho Boadana", "Ademir Guimar\u00e3es da Costa Junior", "Ricardo Rios", "F\u00e1bio Santos da Silva"], "title": "LLM-Based Intelligent Agents for Music Recommendation: A Comparison with Classical Content-Based Filtering", "comment": "12 pages, in Portuguese language, 2 figures, 5 tables, 3 formulas. To\n  be published in the Proceedings of the Encontro Nacional de Intelig\\^encia\n  Artificial e Computacional (ENIAC 2025)", "summary": "The growing availability of music on streaming platforms has led to\ninformation overload for users. To address this issue and enhance the user\nexperience, increasingly sophisticated recommendation systems have been\nproposed. This work investigates the use of Large Language Models (LLMs) from\nthe Gemini and LLaMA families, combined with intelligent agents, in a\nmulti-agent personalized music recommendation system. The results are compared\nwith a traditional content-based recommendation model, considering user\nsatisfaction, novelty, and computational efficiency. LLMs achieved satisfaction\nrates of up to \\textit{89{,}32\\%}, indicating their promising potential in\nmusic recommendation systems.", "AI": {"tldr": "LLMs show promise in personalized music recommendation systems by addressing information overload and achieving high user satisfaction.", "motivation": "Address information overload for users due to the growing availability of music on streaming platforms and enhance user experience.", "method": "Large Language Models (LLMs) from the Gemini and LLaMA families, combined with intelligent agents", "result": "LLMs achieved satisfaction rates of up to 89.32%.", "conclusion": "LLMs achieved high user satisfaction (up to 89.32%) in music recommendation, showing their potential."}}
{"id": "2508.11667", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11667", "abs": "https://arxiv.org/abs/2508.11667", "authors": ["Bryan E. Tuck", "Rakesh M. Verma"], "title": "Assessing Representation Stability for Transformer Models", "comment": "19 pages, 19 figures, 8 tables. Code available at\n  https://github.com/ReDASers/representation-stability", "summary": "Adversarial text attacks remain a persistent threat to transformer models,\nyet existing defenses are typically attack-specific or require costly model\nretraining. We introduce Representation Stability (RS), a model-agnostic\ndetection framework that identifies adversarial examples by measuring how\nembedding representations change when important words are masked. RS first\nranks words using importance heuristics, then measures embedding sensitivity to\nmasking top-k critical words, and processes the resulting patterns with a\nBiLSTM detector. Experiments show that adversarially perturbed words exhibit\ndisproportionately high masking sensitivity compared to naturally important\nwords. Across three datasets, three attack types, and two victim models, RS\nachieves over 88% detection accuracy and demonstrates competitive performance\ncompared to existing state-of-the-art methods, often at lower computational\ncost. Using Normalized Discounted Cumulative Gain (NDCG) to measure\nperturbation identification quality, we reveal that gradient-based ranking\noutperforms attention and random selection approaches, with identification\nquality correlating with detection performance for word-level attacks. RS also\ngeneralizes well to unseen datasets, attacks, and models without retraining,\nproviding a practical solution for adversarial text detection.", "AI": {"tldr": "This paper introduces Representation Stability (RS), a model-agnostic detection framework that identifies adversarial examples by measuring how embedding representations change when important words are masked.", "motivation": "Adversarial text attacks remain a persistent threat to transformer models, yet existing defenses are typically attack-specific or require costly model retraining.", "method": "RS first ranks words using importance heuristics, then measures embedding sensitivity to masking top-k critical words, and processes the resulting patterns with a BiLSTM detector.", "result": "RS achieves over 88% detection accuracy and demonstrates competitive performance compared to existing state-of-the-art methods, often at lower computational cost. Using Normalized Discounted Cumulative Gain (NDCG) to measure perturbation identification quality, we reveal that gradient-based ranking outperforms attention and random selection approaches, with identification quality correlating with detection performance for word-level attacks.", "conclusion": "RS also generalizes well to unseen datasets, attacks, and models without retraining, providing a practical solution for adversarial text detection."}}
{"id": "2508.11758", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11758", "abs": "https://arxiv.org/abs/2508.11758", "authors": ["Jonas van Elburg", "Peter van der Putten", "Maarten Marx"], "title": "Can we Evaluate RAGs with Synthetic Data?", "comment": "Accepted for the SynDAiTE workshop at the European Conference on\n  Machine Learning and Principles and Practice of Knowledge Discovery in\n  Databases (ECML-PKDD 2025), September 15, 2025 - Porto, Portugal", "summary": "We investigate whether synthetic question-answer (QA) data generated by large\nlanguage models (LLMs) can serve as an effective proxy for human-labeled\nbenchmarks when such data is unavailable. We assess the reliability of\nsynthetic benchmarks across two experiments: one varying retriever parameters\nwhile keeping the generator fixed, and another varying the generator with fixed\nretriever parameters. Across four datasets, of which two open-domain and two\nproprietary, we find that synthetic benchmarks reliably rank the RAGs varying\nin terms of retriever configuration, aligning well with human-labeled benchmark\nbaselines. However, they fail to produce consistent RAG rankings when comparing\ngenerator architectures. The breakdown possibly arises from a combination of\ntask mismatch between the synthetic and human benchmarks, and stylistic bias\nfavoring certain generators.", "AI": {"tldr": "synthetic QA data can replace human-labeled data in some cases, but not all", "motivation": "investigate whether synthetic question-answer (QA) data generated by large language models (LLMs) can serve as an effective proxy for human-labeled benchmarks when such data is unavailable", "method": "synthetic question-answer (QA) data generated by large language models (LLMs)", "result": "synthetic benchmarks reliably rank the RAGs varying in terms of retriever configuration, aligning well with human-labeled benchmark baselines. However, they fail to produce consistent RAG rankings when comparing generator architectures", "conclusion": "synthetic benchmarks reliably rank the RAGs varying in terms of retriever configuration, aligning well with human-labeled benchmark baselines, but fail to produce consistent RAG rankings when comparing generator architectures. The breakdown possibly arises from a combination of task mismatch between the synthetic and human benchmarks, and stylistic bias favoring certain generators."}}
{"id": "2508.11697", "categories": ["cs.CV", "cs.AI", "I.5.1"], "pdf": "https://arxiv.org/pdf/2508.11697", "abs": "https://arxiv.org/abs/2508.11697", "authors": ["Adri\u00e1n Rodr\u00edguez-Mu\u00f1oz", "Manel Baradad", "Phillip Isola", "Antonio Torralba"], "title": "Separating Knowledge and Perception with Procedural Data", "comment": "17 pages, 18 figures, 3 tables, to be published in ICML 2025", "summary": "We train representation models with procedural data only, and apply them on\nvisual similarity, classification, and semantic segmentation tasks without\nfurther training by using visual memory -- an explicit database of reference\nimage embeddings. Unlike prior work on visual memory, our approach achieves\nfull compartmentalization with respect to all real-world images while retaining\nstrong performance. Compared to a model trained on Places, our procedural model\nperforms within $1\\%$ on NIGHTS visual similarity, outperforms by $8\\%$ and\n$15\\%$ on CUB200 and Flowers102 fine-grained classification, and is within\n$10\\%$ on ImageNet-1K classification. It also demonstrates strong zero-shot\nsegmentation, achieving an $R^2$ on COCO within $10\\%$ of the models trained on\nreal data. Finally, we analyze procedural versus real data models, showing that\nparts of the same object have dissimilar representations in procedural models,\nresulting in incorrect searches in memory and explaining the remaining\nperformance gap.", "AI": {"tldr": "The paper shows that models trained on procedural data can perform comparably to those trained on real data in several vision tasks, particularly in fine-grained classification and zero-shot segmentation, using a visual memory approach.", "motivation": "The research aims to achieve full compartmentalization with respect to real-world images while maintaining strong performance by training models on procedural data only.", "method": "The study trains representation models using only procedural data and applies them to visual tasks using visual memory, an explicit database of reference image embeddings.", "result": "The procedural model performs within 1% on NIGHTS visual similarity, outperforms by 8% and 15% on CUB200 and Flowers102 fine-grained classification, is within 10% on ImageNet-1K classification, and achieves an R^2 on COCO within 10% of models trained on real data. Dissimilar representations of object parts in procedural models explain the remaining performance gap.", "conclusion": "Procedural models demonstrate strong performance in visual similarity, classification, and semantic segmentation tasks, achieving comparable or superior results to models trained on real-world data, especially in fine-grained classification and zero-shot segmentation."}}
{"id": "2508.11850", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11850", "abs": "https://arxiv.org/abs/2508.11850", "authors": ["Milad Yazdani", "Mahdi Mostajabdaveh", "Samin Aref", "Zirui Zhou"], "title": "EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models", "comment": null, "summary": "Integer programming lies at the heart of crucial combinatorial optimization\ntasks but remains challenging due to its NP-hard nature. An effective approach\nfor practically solving integer programs is the manual design of acceleration\ncuts, i.e. inequalities that improve solver performance. However, this creative\nprocess demands deep expertise and is yet to be automated. Our proposed\nframework, EvoCut, automates the generation of acceleration cuts by combining\nlarge language models (LLMs) with an evolutionary search. EvoCut (i)\ninitializes a diverse population of candidate cuts via an LLM-based initializer\nagent; (ii) for each cut empirically evaluates both preservation of the optimal\nsolution and its ability to cut off fractional solutions across a verification\nset; and (iii) iteratively refines the population through evolutionary\ncrossover and mutation agents. We quantify each cut's utility by its relative\nreduction in the solver's optimality gap. Our comparisons against standard\ninteger programming practice show that EvoCut reduces optimality gap by 17-57%\nwithin a fixed time. It obtains the same solutions up to 4 times as fast, and\nobtains higher-quality solutions within the same time limit. Requiring no human\nexpert input, EvoCut reliably generates, improves, and empirically verifies\ncuts that generalize to unseen instances. The code is available at\nhttps://github.com/milad1378yz/EvoCut.", "AI": {"tldr": "EvoCut automates the generation of acceleration cuts for integer programming using LLMs and evolutionary search, achieving significant improvements in solver performance without human expertise.", "motivation": "Integer programming is crucial for combinatorial optimization but challenging due to its NP-hard nature. Manual design of acceleration cuts is effective but requires deep expertise and is not automated.", "method": "EvoCut combines large language models (LLMs) with an evolutionary search to automate the generation of acceleration cuts.", "result": "EvoCut reduces optimality gap by 17-57% within a fixed time, obtains the same solutions up to 4 times as fast, and obtains higher-quality solutions within the same time limit compared to standard integer programming practice.", "conclusion": "EvoCut reliably generates, improves, and empirically verifies cuts that generalize to unseen instances without human expert input."}}
{"id": "2508.12485", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.NI", "C.2.4; C.4; D.4.2; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.12485", "abs": "https://arxiv.org/abs/2508.12485", "authors": ["Aayush Gupta", "Arpit Bhayani"], "title": "Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for NGINX", "comment": "8 pages, 4 figures (system architecture, eviction path, training\n  pipeline, and DQN algorithm), 2 tables. Code available at\n  https://github.com/ayushgupta4897/DRL-Cache", "summary": "Web proxies such as NGINX commonly rely on least-recently-used (LRU)\neviction, which is size agnostic and can thrash under periodic bursts and mixed\nobject sizes. We introduce Cold-RL, a learned eviction policy for NGINX that\nreplaces LRU's forced-expire path with a dueling Deep Q-Network served by an\nONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL\nsamples the K least-recently-used objects, extracts six lightweight features\n(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),\nand requests a bitmask of victims; a hard timeout of 500 microseconds triggers\nimmediate fallback to native LRU. Policies are trained offline by replaying\nNGINX access logs through a cache simulator with a simple reward: a retained\nobject earns one point if it is hit again before TTL expiry. We compare against\nLRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial\nworkloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,\na 146 percent improvement over the best classical baseline; at 100 MB, from\n0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods\n(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th\npercentile eviction latency within budget. To our knowledge, this is the first\nreinforcement learning eviction policy integrated into NGINX with strict SLOs.", "AI": {"tldr": "Cold-RL, a reinforcement learning-based eviction policy for NGINX, outperforms traditional methods in hit ratio, especially for smaller caches, with low overhead.", "motivation": "Web proxies like NGINX commonly use LRU eviction, which is size agnostic and can perform poorly under periodic bursts and mixed object sizes. This motivates the development of a more adaptive eviction policy.", "method": "The method involves replacing LRU's forced-expire path with a dueling Deep Q-Network served by an ONNX sidecar. Policies are trained offline using NGINX access logs and a cache simulator with a reward system based on object retention.", "result": "Cold-RL significantly improves hit ratio compared to LRU, LFU, size-based, adaptive LRU, and a hybrid baseline, particularly with smaller caches. It achieves a 146% improvement at 25 MB cache, a 15% gain at 100 MB, and matches classical methods at 400 MB, with minimal CPU overhead and eviction latency.", "conclusion": "This paper introduces Cold-RL, a reinforcement learning eviction policy integrated into NGINX with strict SLOs. It demonstrates significant hit ratio improvements over classical baselines on adversarial workloads, especially with smaller cache sizes, while maintaining low CPU overhead and eviction latency."}}
{"id": "2508.11784", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11784", "abs": "https://arxiv.org/abs/2508.11784", "authors": ["Zabir Al Nazi", "Vagelis Hristidis", "Aaron Lawson McLean", "Jannat Ara Meem", "Md Taukir Azam Chowdhury"], "title": "Ontology-Guided Query Expansion for Biomedical Document Retrieval using Large Language Models", "comment": null, "summary": "Effective Question Answering (QA) on large biomedical document collections\nrequires effective document retrieval techniques. The latter remains a\nchallenging task due to the domain-specific vocabulary and semantic ambiguity\nin user queries. We propose BMQExpander, a novel ontology-aware query expansion\npipeline that combines medical knowledge - definitions and relationships - from\nthe UMLS Metathesaurus with the generative capabilities of large language\nmodels (LLMs) to enhance retrieval effectiveness. We implemented several\nstate-of-the-art baselines, including sparse and dense retrievers, query\nexpansion methods, and biomedical-specific solutions. We show that BMQExpander\nhas superior retrieval performance on three popular biomedical Information\nRetrieval (IR) benchmarks: NFCorpus, TREC-COVID, and SciFact - with\nimprovements of up to 22.1% in NDCG@10 over sparse baselines and up to 6.5%\nover the strongest baseline. Further, BMQExpander generalizes robustly under\nquery perturbation settings, in contrast to supervised baselines, achieving up\nto 15.7% improvement over the strongest baseline. As a side contribution, we\npublish our paraphrased benchmarks. Finally, our qualitative analysis shows\nthat BMQExpander has fewer hallucinations compared to other LLM-based query\nexpansion baselines.", "AI": {"tldr": "BMQExpander\u662f\u4e00\u79cd\u65b0\u7684\u67e5\u8be2\u6269\u5c55\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u533b\u5b66\u77e5\u8bc6\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u63d0\u9ad8\u751f\u7269\u533b\u5b66\u6587\u6863\u68c0\u7d22\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5927\u578b\u751f\u7269\u533b\u5b66\u6587\u6863\u96c6\u5408\u4e0a\u7684\u6709\u6548\u95ee\u7b54\uff08QA\uff09\u9700\u8981\u6709\u6548\u7684\u6587\u6863\u68c0\u7d22\u6280\u672f\u3002\u7531\u4e8e\u7279\u5b9a\u9886\u57df\u7684\u8bcd\u6c47\u548c\u7528\u6237\u67e5\u8be2\u4e2d\u7684\u8bed\u4e49\u6a21\u7cca\u6027\uff0c\u540e\u8005\u4ecd\u7136\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002", "method": "\u7ed3\u5408UMLS Metathesaurus\u7684\u533b\u5b66\u77e5\u8bc6\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u751f\u6210\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u672c\u4f53\u611f\u77e5\u67e5\u8be2\u6269\u5c55\u7ba1\u9053BMQExpander\u3002", "result": "BMQExpander\u5728NFCorpus\u3001TREC-COVID\u548cSciFact\u8fd9\u4e09\u4e2a\u6d41\u884c\u7684\u751f\u7269\u533b\u5b66\u4fe1\u606f\u68c0\u7d22\uff08IR\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5177\u6709\u51fa\u8272\u7684\u68c0\u7d22\u6027\u80fd\uff0c\u4e0e\u7a00\u758f\u57fa\u7ebf\u76f8\u6bd4\uff0cNDCG@10\u7684\u63d0\u5347\u9ad8\u8fbe22.1%\uff0c\u4e0e\u6700\u5f3a\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u63d0\u5347\u9ad8\u8fbe6.5%\u3002\u5728\u67e5\u8be2\u6270\u52a8\u8bbe\u7f6e\u4e0b\uff0cBMQExpander\u5177\u6709\u5f88\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e0e\u6709\u76d1\u7763\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe15.7%\u7684\u6539\u8fdb\u3002\u5b9a\u6027\u5206\u6790\u8868\u660e\uff0c\u4e0e\u5176\u4ed6\u57fa\u4e8eLLM\u7684\u67e5\u8be2\u6269\u5c55\u57fa\u7ebf\u76f8\u6bd4\uff0cBMQExpander\u7684\u5e7b\u89c9\u66f4\u5c11\u3002", "conclusion": "BMQExpander\u5728\u4e09\u4e2a\u751f\u7269\u533b\u5b66\u4fe1\u606f\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u67e5\u8be2\u6270\u52a8\u4e0b\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u5e7b\u89c9\u8f83\u5c11\u3002"}}
{"id": "2508.11669", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11669", "abs": "https://arxiv.org/abs/2508.11669", "authors": ["Wentao Li", "Yonghu He", "Kun Gao", "Qing Liu", "Yali Zheng"], "title": "Collaborative Learning-Enhanced Lightweight Models for Predicting Arterial Blood Pressure Waveform in a Large-scale Perioperative Dataset", "comment": null, "summary": "Noninvasive arterial blood pressure (ABP) monitoring is essential for patient\nmanagement in critical care and perioperative settings, providing continuous\nassessment of cardiovascular hemodynamics with minimal risks. Numerous deep\nlearning models have developed to reconstruct ABP waveform from noninvasively\nacquired physiological signals such as electrocardiogram and\nphotoplethysmogram. However, limited research has addressed the issue of model\nperformance and computational load for deployment on embedded systems. The\nstudy introduces a lightweight sInvResUNet, along with a collaborative learning\nscheme named KDCL_sInvResUNet. With only 0.89 million parameters and a\ncomputational load of 0.02 GFLOPS, real-time ABP estimation was successfully\nachieved on embedded devices with an inference time of just 8.49 milliseconds\nfor a 10-second output. We performed subject-independent validation in a\nlarge-scale and heterogeneous perioperative dataset containing 1,257,141 data\nsegments from 2,154 patients, with a wide BP range (41-257 mmHg for SBP, and\n31-234 mmHg for DBP). The proposed KDCL_sInvResUNet achieved lightly better\nperformance compared to large models, with a mean absolute error of 10.06 mmHg\nand mean Pearson correlation of 0.88 in tracking ABP changes. Despite these\npromising results, all deep learning models showed significant performance\nvariations across different demographic and cardiovascular conditions,\nhighlighting their limited ability to generalize across such a broad and\ndiverse population. This study lays a foundation work for real-time,\nunobtrusive ABP monitoring in real-world perioperative settings, providing\nbaseline for future advancements in this area.", "AI": {"tldr": "Developed a lightweight deep learning model (KDCL_sInvResUNet) for real-time, noninvasive arterial blood pressure (ABP) monitoring on embedded systems, achieving good performance with low computational load.", "motivation": "Limited research has addressed the issue of model performance and computational load for deployment on embedded systems.", "method": "The study introduces a lightweight sInvResUNet, along with a collaborative learning scheme named KDCL_sInvResUNet.", "result": "The proposed KDCL_sInvResUNet achieved lightly better performance compared to large models, with a mean absolute error of 10.06 mmHg and mean Pearson correlation of 0.88 in tracking ABP changes. With only 0.89 million parameters and a computational load of 0.02 GFLOPS, real-time ABP estimation was successfully achieved on embedded devices with an inference time of just 8.49 milliseconds for a 10-second output.", "conclusion": "This study lays a foundation work for real-time, unobtrusive ABP monitoring in real-world perioperative settings, providing baseline for future advancements in this area."}}
{"id": "2508.11767", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11767", "abs": "https://arxiv.org/abs/2508.11767", "authors": ["Noah Kasmanoff", "Rahul Zalkikar"], "title": "Limitation Learning: Catching Adverse Dialog with GAIL", "comment": "Paper from 2021", "summary": "Imitation learning is a proven method for creating a policy in the absence of\nrewards, by leveraging expert demonstrations. In this work, we apply imitation\nlearning to conversation. In doing so, we recover a policy capable of talking\nto a user given a prompt (input state), and a discriminator capable of\nclassifying between expert and synthetic conversation. While our policy is\neffective, we recover results from our discriminator that indicate the\nlimitations of dialog models. We argue that this technique can be used to\nidentify adverse behavior of arbitrary data models common for dialog oriented\ntasks.", "AI": {"tldr": "Use imitation learning for conversation to create a dialog policy and identify limitations of dialog models.", "motivation": "Creating a policy in the absence of rewards, by leveraging expert demonstrations.", "method": "Apply imitation learning to conversation.", "result": "Recover a policy capable of talking to a user given a prompt, and a discriminator capable of classifying between expert and synthetic conversation. Limitations of dialog models are indicated.", "conclusion": "Technique can identify adverse behavior of dialog models."}}
{"id": "2508.11721", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11721", "abs": "https://arxiv.org/abs/2508.11721", "authors": ["Ke Zou", "Jocelyn Hui Lin Goh", "Yukun Zhou", "Tian Lin", "Samantha Min Er Yew", "Sahana Srinivasan", "Meng Wang", "Rui Santos", "Gabor M. Somfai", "Huazhu Fu", "Haoyu Chen", "Pearse A. Keane", "Ching-Yu Cheng", "Yih Chung Tham"], "title": "FusionFM: Fusing Eye-specific Foundational Models for Optimized Ophthalmic Diagnosis", "comment": "12 pages, 3 figures", "summary": "Foundation models (FMs) have shown great promise in medical image analysis by\nimproving generalization across diverse downstream tasks. In ophthalmology,\nseveral FMs have recently emerged, but there is still no clear answer to\nfundamental questions: Which FM performs the best? Are they equally good across\ndifferent tasks? What if we combine all FMs together? To our knowledge, this is\nthe first study to systematically evaluate both single and fused ophthalmic\nFMs. To address these questions, we propose FusionFM, a comprehensive\nevaluation suite, along with two fusion approaches to integrate different\nophthalmic FMs. Our framework covers both ophthalmic disease detection\n(glaucoma, diabetic retinopathy, and age-related macular degeneration) and\nsystemic disease prediction (diabetes and hypertension) based on retinal\nimaging. We benchmarked four state-of-the-art FMs (RETFound, VisionFM,\nRetiZero, and DINORET) using standardized datasets from multiple countries and\nevaluated their performance using AUC and F1 metrics. Our results show that\nDINORET and RetiZero achieve superior performance in both ophthalmic and\nsystemic disease tasks, with RetiZero exhibiting stronger generalization on\nexternal datasets. Regarding fusion strategies, the Gating-based approach\nprovides modest improvements in predicting glaucoma, AMD, and hypertension.\nDespite these advances, predicting systemic diseases, especially hypertension\nin external cohort remains challenging. These findings provide an\nevidence-based evaluation of ophthalmic FMs, highlight the benefits of model\nfusion, and point to strategies for enhancing their clinical applicability.", "AI": {"tldr": "\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u5355\u4e2a\u548c\u878d\u5408\u7684\u773c\u79d1FMs\uff0c\u53d1\u73b0DINORET\u548cRetiZero\u8868\u73b0\u8f83\u597d\uff0c\u6a21\u578b\u878d\u5408\u6709\u76ca\u5904\uff0c\u4f46\u9884\u6d4b\u5168\u8eab\u6027\u75be\u75c5\u4ecd\u5177\u6311\u6218\u3002", "motivation": "\u5728\u773c\u79d1\u9886\u57df\uff0c\u5df2\u7ecf\u51fa\u73b0\u4e86\u51e0\u79cdFMs\uff0c\u4f46\u5bf9\u4e8e\u57fa\u672c\u95ee\u9898\u4ecd\u7136\u6ca1\u6709\u660e\u786e\u7684\u7b54\u6848\uff1a\u54ea\u79cdFM\u8868\u73b0\u6700\u597d\uff1f\u5b83\u4eec\u5728\u4e0d\u540c\u7684\u4efb\u52a1\u4e2d\u662f\u5426\u540c\u6837\u51fa\u8272\uff1f\u5982\u679c\u6211\u4eec\u628a\u6240\u6709\u7684FMs\u7ed3\u5408\u5728\u4e00\u8d77\u4f1a\u600e\u4e48\u6837\uff1f", "method": "\u63d0\u51faFusionFM\uff0c\u4e00\u4e2a\u7efc\u5408\u8bc4\u4f30\u5957\u4ef6\uff0c\u4ee5\u53ca\u4e24\u79cd\u878d\u5408\u65b9\u6cd5\u6765\u6574\u5408\u4e0d\u540c\u7684\u773c\u79d1FMs\u3002", "result": "DINORET\u548cRetiZero\u5728\u773c\u79d1\u548c\u5168\u8eab\u75be\u75c5\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0cRetiZero\u5728\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u57fa\u4e8e\u95e8\u63a7\u7684\u65b9\u6cd5\u5728\u9884\u6d4b\u9752\u5149\u773c\u3001AMD\u548c\u9ad8\u8840\u538b\u65b9\u9762\u63d0\u4f9b\u4e86\u4e00\u4e9b\u6539\u8fdb\u3002", "conclusion": "DINORET\u548cRetiZero\u5728\u773c\u79d1\u548c\u5168\u8eab\u75be\u75c5\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0cRetiZero\u5728\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u57fa\u4e8e\u95e8\u63a7\u7684\u65b9\u6cd5\u5728\u9884\u6d4b\u9752\u5149\u773c\u3001AMD\u548c\u9ad8\u8840\u538b\u65b9\u9762\u63d0\u4f9b\u4e86\u4e00\u4e9b\u6539\u8fdb\u3002\u9884\u6d4b\u5168\u8eab\u6027\u75be\u75c5\uff0c\u7279\u522b\u662f\u5916\u90e8\u961f\u5217\u4e2d\u7684\u9ad8\u8840\u538b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002"}}
{"id": "2508.11860", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11860", "abs": "https://arxiv.org/abs/2508.11860", "authors": ["Frazier N. Baker", "Daniel Adu-Ampratwum", "Reza Averly", "Botao Yu", "Huan Sun", "Xia Ning"], "title": "LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework", "comment": "24 pages, 5 figures", "summary": "Large language model (LLM) agent evaluators leverage specialized tools to\nground the rational decision-making of LLMs, making them well-suited to aid in\nscientific discoveries, such as constrained retrosynthesis planning.\nConstrained retrosynthesis planning is an essential, yet challenging, process\nwithin chemistry for identifying synthetic routes from commercially available\nstarting materials to desired target molecules, subject to practical\nconstraints. Here, we present LARC, the first LLM-based Agentic framework for\nRetrosynthesis planning under Constraints. LARC incorporates agentic constraint\nevaluation, through an Agent-as-a-Judge, directly into the retrosynthesis\nplanning process, using agentic feedback grounded in tool-based reasoning to\nguide and constrain route generation. We rigorously evaluate LARC on a\ncarefully curated set of 48 constrained retrosynthesis planning tasks across 3\nconstraint types. LARC achieves a 72.9% success rate on these tasks, vastly\noutperforming LLM baselines and approaching human expert-level success in\nsubstantially less time. The LARC framework is extensible, and serves as a\nfirst step towards an effective agentic tool or a co-scientist to human experts\nfor constrained retrosynthesis.", "AI": {"tldr": "LARC is the first LLM-based Agentic framework for Retrosynthesis planning under Constraints. It outperforms LLM baselines and approaches human expert-level success in substantially less time.", "motivation": "Constrained retrosynthesis planning is an essential, yet challenging, process within chemistry for identifying synthetic routes from commercially available starting materials to desired target molecules, subject to practical constraints.", "method": "LARC incorporates agentic constraint evaluation, through an Agent-as-a-Judge, directly into the retrosynthesis planning process, using agentic feedback grounded in tool-based reasoning to guide and constrain route generation.", "result": "LARC achieves a 72.9% success rate on these tasks, vastly outperforming LLM baselines and approaching human expert-level success in substantially less time.", "conclusion": "The LARC framework is extensible, and serves as a first step towards an effective agentic tool or a co-scientist to human experts for constrained retrosynthesis."}}
{"id": "2508.12868", "categories": ["cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.12868", "abs": "https://arxiv.org/abs/2508.12868", "authors": ["Yilin Geng", "Shujing Wang", "Chuan Wang", "Keqing He", "Yanfei Lv", "Ying Wang", "Zaiwen Feng", "Xiaoying Bai"], "title": "An LLM Agent-Based Complex Semantic Table Annotation Approach", "comment": null, "summary": "The Semantic Table Annotation (STA) task, which includes Column Type\nAnnotation (CTA) and Cell Entity Annotation (CEA), maps table contents to\nontology entities and plays important roles in various semantic applications.\nHowever, complex tables often pose challenges such as semantic loss of column\nnames or cell values, strict ontological hierarchy requirements, homonyms,\nspelling errors, and abbreviations, which hinder annotation accuracy. To\naddress these issues, this paper proposes an LLM-based agent approach for CTA\nand CEA. We design and implement five external tools with tailored prompts\nbased on the ReAct framework, enabling the STA agent to dynamically select\nsuitable annotation strategies depending on table characteristics. Experiments\nare conducted on the Tough Tables and BiodivTab datasets from the SemTab\nchallenge, which contain the aforementioned challenges. Our method outperforms\nexisting approaches across various metrics. Furthermore, by leveraging\nLevenshtein distance to reduce redundant annotations, we achieve a 70%\nreduction in time costs and a 60% reduction in LLM token usage, providing an\nefficient and cost-effective solution for STA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u4ee3\u7406\u7684CTA\u548cCEA\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5229\u7528Levenshtein\u8ddd\u79bb\u6765\u51cf\u5c11\u5197\u4f59\u6ce8\u91ca\uff0c\u4ece\u800c\u663e\u8457\u964d\u4f4e\u4e86\u65f6\u95f4\u548ctoken\u4f7f\u7528\u6210\u672c\u3002", "motivation": "\u590d\u6742\u8868\u683c\u5e38\u5e38\u5e26\u6765\u6311\u6218\uff0c\u4f8b\u5982\u5217\u540d\u6216\u5355\u5143\u683c\u503c\u7684\u8bed\u4e49\u4e22\u5931\u3001\u4e25\u683c\u7684\u672c\u4f53\u5c42\u6b21\u7ed3\u6784\u8981\u6c42\u3001\u540c\u4e49\u8bcd\u3001\u62fc\u5199\u9519\u8bef\u548c\u7f29\u5199\uff0c\u8fd9\u4e9b\u90fd\u963b\u788d\u4e86\u6ce8\u91ca\u7684\u51c6\u786e\u6027\u3002", "method": "\u57fa\u4e8eReAct\u6846\u67b6\uff0c\u8bbe\u8ba1\u5e76\u5b9e\u65bd\u4e86\u4e94\u4e2a\u5e26\u6709\u5b9a\u5236\u63d0\u793a\u7684\u5916\u90e8\u5de5\u5177\uff0c\u4f7fSTA\u4ee3\u7406\u80fd\u591f\u6839\u636e\u8868\u683c\u7279\u5f81\u52a8\u6001\u9009\u62e9\u5408\u9002\u7684\u6ce8\u91ca\u7b56\u7565\u3002", "result": "\u5728SemTab\u6311\u6218\u8d5b\u7684Tough Tables\u548cBiodivTab\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u5229\u7528Levenshtein\u8ddd\u79bb\u51cf\u5c11\u5197\u4f59\u6ce8\u91ca\uff0c\u4ece\u800c\u663e\u8457\u964d\u4f4e\u65f6\u95f4\u548ctoken\u4f7f\u7528\u6210\u672c\uff0c\u4e3aSTA\u63d0\u4f9b\u9ad8\u6548\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11977", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11977", "abs": "https://arxiv.org/abs/2508.11977", "authors": ["Zida Liang", "Changfa Wu", "Dunxian Huang", "Weiqiang Sun", "Ziyang Wang", "Yuliang Yan", "Jian Wu", "Yuning Jiang", "Bo Zheng", "Ke Chen", "Silu Zhou", "Yu Zhang"], "title": "TBGRecall: A Generative Retrieval Model for E-commerce Recommendation Scenarios", "comment": "Both authors contributed equally to this research. Work done during\n  internship at Alibaba. Corresponding author: Dunxian Huang\n  (dunxian.hdx@alibaba-inc.com). Affiliations: (1) Shanghai Jiaotong\n  University, Shanghai, China; (2) Alibaba Inc", "summary": "Recommendation systems are essential tools in modern e-commerce, facilitating\npersonalized user experiences by suggesting relevant products. Recent\nadvancements in generative models have demonstrated potential in enhancing\nrecommendation systems; however, these models often exhibit limitations in\noptimizing retrieval tasks, primarily due to their reliance on autoregressive\ngeneration mechanisms. Conventional approaches introduce sequential\ndependencies that impede efficient retrieval, as they are inherently unsuitable\nfor generating multiple items without positional constraints within a single\nrequest session. To address these limitations, we propose TBGRecall, a\nframework integrating Next Session Prediction (NSP), designed to enhance\ngenerative retrieval models for e-commerce applications. Our framework\nreformulation involves partitioning input samples into multi-session sequences,\nwhere each sequence comprises a session token followed by a set of item tokens,\nand then further incorporate multiple optimizations tailored to the generative\ntask in retrieval scenarios. In terms of training methodology, our pipeline\nintegrates limited historical data pre-training with stochastic partial\nincremental training, significantly improving training efficiency and\nemphasizing the superiority of data recency over sheer data volume. Our\nextensive experiments, conducted on public benchmarks alongside a large-scale\nindustrial dataset from TaoBao, show TBGRecall outperforms the state-of-the-art\nrecommendation methods, and exhibits a clear scaling law trend. Ultimately, NSP\nrepresents a significant advancement in the effectiveness of generative\nrecommendation systems for e-commerce applications.", "AI": {"tldr": "TBGRecall, a Next Session Prediction framework, enhances generative retrieval models for e-commerce, outperforming existing methods with improved training efficiency and data recency.", "motivation": "Generative models in recommendation systems have limitations in optimizing retrieval tasks due to their reliance on autoregressive generation mechanisms and sequential dependencies that impede efficient retrieval.", "method": "TBGRecall: a framework integrating Next Session Prediction (NSP), designed to enhance generative retrieval models for e-commerce applications. The framework partitions input samples into multi-session sequences and incorporates optimizations tailored to the generative task in retrieval scenarios. Training integrates limited historical data pre-training with stochastic partial incremental training.", "result": "TBGRecall outperforms state-of-the-art recommendation methods and exhibits a clear scaling law trend on public benchmarks and a large-scale industrial dataset from TaoBao.", "conclusion": "NSP is a significant advancement in generative recommendation systems for e-commerce."}}
{"id": "2508.11673", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.11673", "abs": "https://arxiv.org/abs/2508.11673", "authors": ["Haojie Zhang", "Yixiong Liang", "Hulin Kuang", "Lihui Cen", "Zhe Qu", "Yigang Cen", "Min Zeng", "Shichao Kan"], "title": "Contrastive Regularization over LoRA for Multimodal Biomedical Image Incremental Learning", "comment": "10 pages, 3 figures, submitted to ACM Multimedia 2025", "summary": "Multimodal Biomedical Image Incremental Learning (MBIIL) is essential for\nhandling diverse tasks and modalities in the biomedical domain, as training\nseparate models for each modality or task significantly increases inference\ncosts. Existing incremental learning methods focus on task expansion within a\nsingle modality, whereas MBIIL seeks to train a unified model incrementally\nacross modalities. The MBIIL faces two challenges: I) How to preserve\npreviously learned knowledge during incremental updates? II) How to effectively\nleverage knowledge acquired from existing modalities to support new modalities?\nTo address these challenges, we propose MSLoRA-CR, a method that fine-tunes\nModality-Specific LoRA modules while incorporating Contrastive Regularization\nto enhance intra-modality knowledge sharing and promote inter-modality\nknowledge differentiation. Our approach builds upon a large vision-language\nmodel (LVLM), keeping the pretrained model frozen while incrementally adapting\nnew LoRA modules for each modality or task. Experiments on the incremental\nlearning of biomedical images demonstrate that MSLoRA-CR outperforms both the\nstate-of-the-art (SOTA) approach of training separate models for each modality\nand the general incremental learning method (incrementally fine-tuning LoRA).\nSpecifically, MSLoRA-CR achieves a 1.88% improvement in overall performance\ncompared to unconstrained incremental learning methods while maintaining\ncomputational efficiency. Our code is publicly available at\nhttps://github.com/VentusAislant/MSLoRA_CR.", "AI": {"tldr": "MSLoRA-CR outperforms existing methods in multimodal biomedical image incremental learning by fine-tuning Modality-Specific LoRA modules with Contrastive Regularization.", "motivation": "Existing incremental learning methods focus on task expansion within a single modality, whereas MBIIL seeks to train a unified model incrementally across modalities. The MBIIL faces two challenges: I) How to preserve previously learned knowledge during incremental updates? II) How to effectively leverage knowledge acquired from existing modalities to support new modalities?", "method": "fine-tunes Modality-Specific LoRA modules while incorporating Contrastive Regularization to enhance intra-modality knowledge sharing and promote inter-modality knowledge differentiation. Our approach builds upon a large vision-language model (LVLM), keeping the pretrained model frozen while incrementally adapting new LoRA modules for each modality or task.", "result": "Experiments on the incremental learning of biomedical images demonstrate that MSLoRA-CR outperforms both the state-of-the-art (SOTA) approach of training separate models for each modality and the general incremental learning method (incrementally fine-tuning LoRA).", "conclusion": "MSLoRA-CR achieves a 1.88% improvement in overall performance compared to unconstrained incremental learning methods while maintaining computational efficiency."}}
{"id": "2508.11771", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11771", "abs": "https://arxiv.org/abs/2508.11771", "authors": ["Leo Peckham", "Michael Ong", "Naomi Nagy", "Ewan Dunbar"], "title": "Investigating Transcription Normalization in the Faetar ASR Benchmark", "comment": null, "summary": "We examine the role of transcription inconsistencies in the Faetar Automatic\nSpeech Recognition benchmark, a challenging low-resource ASR benchmark. With\nthe help of a small, hand-constructed lexicon, we conclude that find that,\nwhile inconsistencies do exist in the transcriptions, they are not the main\nchallenge in the task. We also demonstrate that bigram word-based language\nmodelling is of no added benefit, but that constraining decoding to a finite\nlexicon can be beneficial. The task remains extremely difficult.", "AI": {"tldr": "transcription inconsistencies are not the main challenge in the Faetar ASR benchmark, and bigram word-based language modelling is of no added benefit", "motivation": "examining the challenging low-resource ASR benchmark", "method": "examine the role of transcription inconsistencies with the help of a small, hand-constructed lexicon", "result": "bigram word-based language modelling is of no added benefit, but that constraining decoding to a finite lexicon can be beneficial; the task remains extremely difficult", "conclusion": "transcription inconsistencies are not the main challenge in the Faetar ASR benchmark"}}
{"id": "2508.11728", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11728", "abs": "https://arxiv.org/abs/2508.11728", "authors": ["Chunxia Ren", "Ning Zhu", "Yue Lai", "Gui Chen", "Ruijie Wang", "Yangyi Hu", "Suyao Liu", "Shuwen Mao", "Hong Su", "Yu Zhang", "Li Xiao"], "title": "UniDCF: A Foundation Model for Comprehensive Dentocraniofacial Hard Tissue Reconstruction", "comment": "23 pages, 6 figures", "summary": "Dentocraniofacial hard tissue defects profoundly affect patients'\nphysiological functions, facial aesthetics, and psychological well-being,\nposing significant challenges for precise reconstruction. Current deep learning\nmodels are limited to single-tissue scenarios and modality-specific imaging\ninputs, resulting in poor generalizability and trade-offs between anatomical\nfidelity, computational efficiency, and cross-tissue adaptability. Here we\nintroduce UniDCF, a unified framework capable of reconstructing multiple\ndentocraniofacial hard tissues through multimodal fusion encoding of point\nclouds and multi-view images. By leveraging the complementary strengths of each\nmodality and incorporating a score-based denoising module to refine surface\nsmoothness, UniDCF overcomes the limitations of prior single-modality\napproaches. We curated the largest multimodal dataset, comprising intraoral\nscans, CBCT, and CT from 6,609 patients, resulting in 54,555 annotated\ninstances. Evaluations demonstrate that UniDCF outperforms existing\nstate-of-the-art methods in terms of geometric precision, structural\ncompleteness, and spatial accuracy. Clinical simulations indicate UniDCF\nreduces reconstruction design time by 99% and achieves clinician-rated\nacceptability exceeding 94%. Overall, UniDCF enables rapid, automated, and\nhigh-fidelity reconstruction, supporting personalized and precise restorative\ntreatments, streamlining clinical workflows, and enhancing patient outcomes.", "AI": {"tldr": "UniDCF is a unified framework capable of reconstructing multiple dentocraniofacial hard tissues through multimodal fusion encoding of point clouds and multi-view images, outperforming existing state-of-the-art methods.", "motivation": "Current deep learning models are limited to single-tissue scenarios and modality-specific imaging inputs, resulting in poor generalizability and trade-offs between anatomical fidelity, computational efficiency, and cross-tissue adaptability. ", "method": "a unified framework capable of reconstructing multiple dentocraniofacial hard tissues through multimodal fusion encoding of point clouds and multi-view images. By leveraging the complementary strengths of each modality and incorporating a score-based denoising module to refine surface smoothness", "result": "UniDCF outperforms existing state-of-the-art methods in terms of geometric precision, structural completeness, and spatial accuracy. Clinical simulations indicate UniDCF reduces reconstruction design time by 99% and achieves clinician-rated acceptability exceeding 94%.", "conclusion": "UniDCF enables rapid, automated, and high-fidelity reconstruction, supporting personalized and precise restorative treatments, streamlining clinical workflows, and enhancing patient outcomes."}}
{"id": "2508.11894", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11894", "abs": "https://arxiv.org/abs/2508.11894", "authors": ["Ao Li", "Bin Yan", "Bingfeng Cai", "Chenxi Li", "Cunzhong Zhao", "Fugen Yao", "Gaoqiang Liu", "Guanjun Jiang", "Jian Xu", "Liang Dong", "Liansheng Sun", "Rongshen Zhang", "Xiaolei Gui", "Xin Liu", "Xin Shang", "Yao Wu", "Yu Cao", "Zhenxin Ma", "Zhuang Jia"], "title": "QuarkMed Medical Foundation Model Technical Report", "comment": "20 pages", "summary": "Recent advancements in large language models have significantly accelerated\ntheir adoption in healthcare applications, including AI-powered medical\nconsultations, diagnostic report assistance, and medical search tools. However,\nmedical tasks often demand highly specialized knowledge, professional accuracy,\nand customization capabilities, necessitating a robust and reliable foundation\nmodel. QuarkMed addresses these needs by leveraging curated medical data\nprocessing, medical-content Retrieval-Augmented Generation (RAG), and a\nlarge-scale, verifiable reinforcement learning pipeline to develop a\nhigh-performance medical foundation model. The model achieved 70% accuracy on\nthe Chinese Medical Licensing Examination, demonstrating strong generalization\nacross diverse medical benchmarks. QuarkMed offers a powerful yet versatile\npersonal medical AI solution, already serving over millions of users at\nai.quark.cn.", "AI": {"tldr": "QuarkMed\u662f\u4e00\u4e2a\u9ad8\u6027\u80fd\u533b\u5b66\u57fa\u7840\u6a21\u578b\uff0c\u5b83\u5229\u7528\u533b\u5b66\u6570\u636e\u5904\u7406\u3001RAG\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u4e2d\u56fd\u533b\u5e08\u8d44\u683c\u8003\u8bd5\u4e2d\u8fbe\u5230\u4e8670%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5df2\u670d\u52a1\u6570\u767e\u4e07\u7528\u6237\u3002", "motivation": "\u533b\u7597\u4efb\u52a1\u901a\u5e38\u9700\u8981\u9ad8\u5ea6\u4e13\u4e1a\u7684\u77e5\u8bc6\u3001\u4e13\u4e1a\u7684\u51c6\u786e\u6027\u548c\u5b9a\u5236\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u5f3a\u5927\u800c\u53ef\u9760\u7684\u57fa\u7840\u6a21\u578b\u3002", "method": "\u5229\u7528\u7cbe\u9009\u7684\u533b\u7597\u6570\u636e\u5904\u7406\u3001\u533b\u7597\u5185\u5bb9\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u5927\u89c4\u6a21\u3001\u53ef\u9a8c\u8bc1\u7684\u5f3a\u5316\u5b66\u4e60\u7ba1\u9053\u6765\u5f00\u53d1\u9ad8\u6027\u80fd\u533b\u7597\u57fa\u7840\u6a21\u578b\u3002", "result": "\u8be5\u6a21\u578b\u5728\u4e2d\u56fd\u533b\u5e08\u8d44\u683c\u8003\u8bd5\u4e2d\u53d6\u5f97\u4e8670%\u7684\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u5728\u5404\u79cd\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5177\u6709\u5f88\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "QuarkMed\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u4e14\u901a\u7528\u7684\u4e2a\u4eba\u533b\u7597AI\u89e3\u51b3\u65b9\u6848\uff0c\u5df2\u7ecf\u5728ai.quark.cn\u4e0a\u4e3a\u6570\u767e\u4e07\u7528\u6237\u63d0\u4f9b\u670d\u52a1\u3002"}}
{"id": "2508.11978", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11978", "abs": "https://arxiv.org/abs/2508.11978", "authors": ["Viacheslav Yusupov", "Maxim Rakhuba", "Evgeny Frolov"], "title": "Leveraging Geometric Insights in Hyperbolic Triplet Loss for Improved Recommendations", "comment": null, "summary": "Recent studies have demonstrated the potential of hyperbolic geometry for\ncapturing complex patterns from interaction data in recommender systems. In\nthis work, we introduce a novel hyperbolic recommendation model that uses\ngeometrical insights to improve representation learning and increase\ncomputational stability at the same time. We reformulate the notion of\nhyperbolic distances to unlock additional representation capacity over\nconventional Euclidean space and learn more expressive user and item\nrepresentations. To better capture user-items interactions, we construct a\ntriplet loss that models ternary relations between users and their\ncorresponding preferred and nonpreferred choices through a mix of pairwise\ninteraction terms driven by the geometry of data. Our hyperbolic approach not\nonly outperforms existing Euclidean and hyperbolic models but also reduces\npopularity bias, leading to more diverse and personalized recommendations.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u53cc\u66f2\u63a8\u8350\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4f7f\u7528\u51e0\u4f55\u89c1\u89e3\u6765\u6539\u8fdb\u8868\u793a\u5b66\u4e60\u5e76\u540c\u65f6\u63d0\u9ad8\u8ba1\u7b97\u7a33\u5b9a\u6027\u3002", "motivation": "\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u53cc\u66f2\u51e0\u4f55\u5728\u6355\u83b7\u63a8\u8350\u7cfb\u7edf\u4e2d\u4ea4\u4e92\u6570\u636e\u7684\u590d\u6742\u6a21\u5f0f\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002", "method": "\u6211\u4eec\u91cd\u65b0\u5b9a\u4e49\u4e86\u53cc\u66f2\u8ddd\u79bb\u7684\u6982\u5ff5\uff0c\u4ee5\u91ca\u653e\u8d85\u8fc7\u4f20\u7edf\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u7684\u989d\u5916\u8868\u793a\u80fd\u529b\uff0c\u5e76\u5b66\u4e60\u66f4\u5177\u8868\u73b0\u529b\u7684\u7528\u6237\u548c\u9879\u76ee\u8868\u793a\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u6355\u6349\u7528\u6237-\u9879\u76ee\u4ea4\u4e92\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u4e09\u5143\u7ec4\u635f\u5931\uff0c\u8be5\u635f\u5931\u901a\u8fc7\u7531\u6570\u636e\u51e0\u4f55\u9a71\u52a8\u7684\u6210\u5bf9\u4ea4\u4e92\u9879\u6df7\u5408\u6765\u6a21\u62df\u7528\u6237\u53ca\u5176\u76f8\u5e94\u7684\u9996\u9009\u548c\u975e\u9996\u9009\u9009\u62e9\u4e4b\u95f4\u7684\u4e09\u5143\u5173\u7cfb\u3002", "result": "\u6211\u4eec\u7684\u53cc\u66f2\u65b9\u6cd5\u4e0d\u4ec5\u4f18\u4e8e\u73b0\u6709\u7684\u6b27\u51e0\u91cc\u5f97\u548c\u53cc\u66f2\u6a21\u578b\uff0c\u800c\u4e14\u51cf\u5c11\u4e86\u53d7\u6b22\u8fce\u7a0b\u5ea6\u504f\u5dee\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u591a\u6837\u5316\u548c\u4e2a\u6027\u5316\u7684\u63a8\u8350\u3002", "conclusion": "\u8be5\u53cc\u66f2\u65b9\u6cd5\u4e0d\u4ec5\u4f18\u4e8e\u73b0\u6709\u7684\u6b27\u51e0\u91cc\u5f97\u548c\u53cc\u66f2\u6a21\u578b\uff0c\u800c\u4e14\u51cf\u5c11\u4e86\u53d7\u6b22\u8fce\u7a0b\u5ea6\u504f\u5dee\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u591a\u6837\u5316\u548c\u4e2a\u6027\u5316\u7684\u63a8\u8350\u3002"}}
{"id": "2508.11679", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11679", "abs": "https://arxiv.org/abs/2508.11679", "authors": ["Shaodi Feng", "Zhuoyi Lin", "Jianan Zhou", "Cong Zhang", "Jingwen Li", "Kuan-Wen Chen", "Senthilnath Jayavelu", "Yew-Soon Ong"], "title": "Lifelong Learner: Discovering Versatile Neural Solvers for Vehicle Routing Problems", "comment": null, "summary": "Deep learning has been extensively explored to solve vehicle routing problems\n(VRPs), which yields a range of data-driven neural solvers with promising\noutcomes. However, most neural solvers are trained to tackle VRP instances in a\nrelatively monotonous context, e.g., simplifying VRPs by using Euclidean\ndistance between nodes and adhering to a single problem size, which harms their\noff-the-shelf application in different scenarios. To enhance their versatility,\nthis paper presents a novel lifelong learning framework that incrementally\ntrains a neural solver to manage VRPs in distinct contexts. Specifically, we\npropose a lifelong learner (LL), exploiting a Transformer network as the\nbackbone, to solve a series of VRPs. The inter-context self-attention mechanism\nis proposed within LL to transfer the knowledge obtained from solving preceding\nVRPs into the succeeding ones. On top of that, we develop a dynamic context\nscheduler (DCS), employing the cross-context experience replay to further\nfacilitate LL looking back on the attained policies of solving preceding VRPs.\nExtensive results on synthetic and benchmark instances (problem sizes up to\n18k) show that our LL is capable of discovering effective policies for tackling\ngeneric VRPs in varying contexts, which outperforms other neural solvers and\nachieves the best performance for most VRPs.", "AI": {"tldr": "This paper introduces a lifelong learning framework to train a neural solver for vehicle routing problems (VRPs) in diverse contexts. It outperforms other neural solvers and achieves the best performance on synthetic and benchmark instances with problem sizes up to 18k.", "motivation": "Most neural solvers are trained to tackle VRP instances in a relatively monotonous context, e.g., simplifying VRPs by using Euclidean distance between nodes and adhering to a single problem size, which harms their off-the-shelf application in different scenarios. To enhance their versatility, this paper presents a novel lifelong learning framework that incrementally trains a neural solver to manage VRPs in distinct contexts.", "method": "A lifelong learning framework that incrementally trains a neural solver to manage VRPs in distinct contexts. A lifelong learner (LL), exploiting a Transformer network as the backbone, to solve a series of VRPs. The inter-context self-attention mechanism is proposed within LL to transfer the knowledge obtained from solving preceding VRPs into the succeeding ones. A dynamic context scheduler (DCS), employing the cross-context experience replay to further facilitate LL looking back on the attained policies of solving preceding VRPs.", "result": "The proposed approach achieves the best performance for most VRPs.", "conclusion": "The lifelong learner (LL) is capable of discovering effective policies for tackling generic VRPs in varying contexts, which outperforms other neural solvers and achieves the best performance for most VRPs."}}
{"id": "2508.11779", "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2508.11779", "abs": "https://arxiv.org/abs/2508.11779", "authors": ["Tianyi Li", "Yu Qin", "Olivia R. Liu Sheng"], "title": "A Multi-Task Evaluation of LLMs' Processing of Academic Text Input", "comment": null, "summary": "How much large language models (LLMs) can aid scientific discovery, notably\nin assisting academic peer review, is in heated debate. Between a literature\ndigest and a human-comparable research assistant lies their practical\napplication potential. We organize individual tasks that computer science\nstudies employ in separate terms into a guided and robust workflow to evaluate\nLLMs' processing of academic text input. We employ four tasks in the\nassessment: content reproduction/comparison/scoring/reflection, each demanding\na specific role of the LLM (oracle/judgmental arbiter/knowledgeable\narbiter/collaborator) in assisting scholarly works, and altogether testing LLMs\nwith questions that increasingly require intellectual capabilities towards a\nsolid understanding of scientific texts to yield desirable solutions. We\nexemplify a rigorous performance evaluation with detailed instructions on the\nprompts. Adopting first-rate Information Systems articles at three top journals\nas the input texts and an abundant set of text metrics, we record a compromised\nperformance of the leading LLM - Google's Gemini: its summary and paraphrase of\nacademic text is acceptably reliable; using it to rank texts through pairwise\ntext comparison is faintly scalable; asking it to grade academic texts is prone\nto poor discrimination; its qualitative reflection on the text is\nself-consistent yet hardly insightful to inspire meaningful research. This\nevidence against an endorsement of LLMs' text-processing capabilities is\nconsistent across metric-based internal (linguistic assessment), external\n(comparing to the ground truth), and human evaluation, and is robust to the\nvariations of the prompt. Overall, we do not recommend an unchecked use of LLMs\nin constructing peer reviews.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5b66\u672f\u6587\u672c\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e0d\u5efa\u8bae\u5728\u540c\u884c\u8bc4\u5ba1\u4e2d\u76f2\u76ee\u4f7f\u7528\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8f85\u52a9\u5b66\u672f\u540c\u884c\u8bc4\u5ba1\u7b49\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u91c7\u7528\u5305\u542b\u5185\u5bb9\u518d\u73b0/\u6bd4\u8f83/\u8bc4\u5206/\u53cd\u601d\u56db\u4e2a\u4efb\u52a1\u7684\u8bc4\u4f30\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4f7f\u7528\u9876\u7ea7\u4fe1\u606f\u7cfb\u7edf\u6587\u7ae0\u4f5c\u4e3a\u8f93\u5165\u6587\u672c\uff0c\u5e76\u8bb0\u5f55\u4e86LLM compromised\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "Google\u7684Gemini\u5728\u5b66\u672f\u6587\u672c\u7684\u603b\u7ed3\u548c\u91ca\u4e49\u65b9\u9762\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u901a\u8fc7\u6210\u5bf9\u6587\u672c\u6bd4\u8f83\u5bf9\u6587\u672c\u8fdb\u884c\u6392\u5e8f\u7684\u53ef\u6269\u5c55\u6027\u8f83\u5dee\uff0c\u5bf9\u5b66\u672f\u6587\u672c\u8fdb\u884c\u8bc4\u5206\u65f6\u5bb9\u6613\u51fa\u73b0\u8f83\u5dee\u7684\u533a\u5206\u5ea6\uff0c\u5176\u5bf9\u6587\u672c\u7684\u5b9a\u6027\u53cd\u601d\u5177\u6709\u81ea\u6d3d\u6027\uff0c\u4f46\u5f88\u96be\u6fc0\u53d1\u6709\u610f\u4e49\u7684\u7814\u7a76\u3002", "conclusion": "\u4e0d\u5efa\u8bae\u5728\u540c\u884c\u8bc4\u5ba1\u4e2d\u672a\u7ecf\u68c0\u67e5\u5730\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002"}}
{"id": "2508.11737", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11737", "abs": "https://arxiv.org/abs/2508.11737", "authors": ["Shiyin Lu", "Yang Li", "Yu Xia", "Yuwei Hu", "Shanshan Zhao", "Yanqing Ma", "Zhichao Wei", "Yinglun Li", "Lunhao Duan", "Jianshan Zhao", "Yuxuan Han", "Haijun Li", "Wanying Chen", "Junke Tang", "Chengkun Hou", "Zhixing Du", "Tianli Zhou", "Wenjie Zhang", "Huping Ding", "Jiahe Li", "Wen Li", "Gui Hu", "Yiliang Gu", "Siran Yang", "Jiamang Wang", "Hailong Sun", "Yibo Wang", "Hui Sun", "Jinlong Huang", "Yuping He", "Shengze Shi", "Weihong Zhang", "Guodong Zheng", "Junpeng Jiang", "Sensen Gao", "Yi-Feng Wu", "Sijia Chen", "Yuhui Chen", "Qing-Guo Chen", "Zhao Xu", "Weihua Luo", "Kaifu Zhang"], "title": "Ovis2.5 Technical Report", "comment": null, "summary": "We present Ovis2.5, a successor to Ovis2 designed for native-resolution\nvisual perception and strong multimodal reasoning. Ovis2.5 integrates a\nnative-resolution vision transformer that processes images at their native,\nvariable resolutions, avoiding the degradation from fixed-resolution tiling and\npreserving both fine detail and global layout -- crucial for visually dense\ncontent like complex charts. To strengthen reasoning, we train the model to\nmove beyond linear chain-of-thought and perform reflection -- including\nself-checking and revision. This advanced capability is exposed as an optional\n\"thinking mode\" at inference time, allowing users to trade latency for enhanced\naccuracy on difficult inputs. The model is trained via a comprehensive\nfive-phase curriculum that progressively builds its skills. The process begins\nwith foundational visual and multimodal pretraining, advances through\nlarge-scale instruction tuning, and culminates in alignment and reasoning\nenhancement using DPO and GRPO. To scale these upgrades efficiently, we employ\nmultimodal data packing and hybrid parallelism, yielding a significant\nend-to-end speedup. We release two open-source models: Ovis2.5-9B and\nOvis2.5-2B. The latter continues the \"small model, big performance\" philosophy\nof Ovis2, making it ideal for resource-constrained, on-device scenarios. On the\nOpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a\nsubstantial improvement over its predecessor, Ovis2-8B, and achieving\nstate-of-the-art results among open-source MLLMs in the sub-40B parameter\nrange; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate\nscores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong\ncapabilities on grounding and video tasks, and achieves open-source SOTA at its\nscale for complex chart analysis.", "AI": {"tldr": "Ovis2.5\u662f\u4e00\u79cd\u7528\u4e8e\u89c6\u89c9\u611f\u77e5\u548c\u591a\u6a21\u6001\u63a8\u7406\u7684\u65b0\u6a21\u578b\uff0c\u5b83\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "Ovis2.5\u65e8\u5728\u5b9e\u73b0\u539f\u751f\u5206\u8fa8\u7387\u7684\u89c6\u89c9\u611f\u77e5\u548c\u5f3a\u5927\u7684\u591a\u6a21\u6001\u63a8\u7406\u3002\u5b83\u96c6\u6210\u4e86\u539f\u751f\u5206\u8fa8\u7387\u89c6\u89c9\u8f6c\u6362\u5668\uff0c\u53ef\u4ee5\u5728\u5176\u539f\u751f\u53ef\u53d8\u5206\u8fa8\u7387\u4e0b\u5904\u7406\u56fe\u50cf\uff0c\u907f\u514d\u4e86\u56fa\u5b9a\u5206\u8fa8\u7387\u5e73\u94fa\u9020\u6210\u7684\u9000\u5316\uff0c\u5e76\u4fdd\u7559\u4e86\u7cbe\u7ec6\u7684\u7ec6\u8282\u548c\u5168\u5c40\u5e03\u5c40\u2014\u2014\u8fd9\u5bf9\u4e8e\u89c6\u89c9\u5bc6\u96c6\u7684\u5185\u5bb9\uff08\u5982\u590d\u6742\u56fe\u8868\uff09\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8be5\u6a21\u578b\u901a\u8fc7\u4e00\u4e2a\u5168\u9762\u7684\u4e94\u9636\u6bb5\u8bfe\u7a0b\u8fdb\u884c\u8bad\u7ec3\uff0c\u8be5\u8bfe\u7a0b\u9010\u6b65\u6784\u5efa\u5176\u6280\u80fd\u3002\u8be5\u8fc7\u7a0b\u4ece\u57fa\u7840\u89c6\u89c9\u548c\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u5f00\u59cb\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6307\u4ee4\u8c03\u6574\u8fdb\u884c\u6539\u8fdb\uff0c\u5e76\u6700\u7ec8\u4f7f\u7528DPO\u548cGRPO\u8fdb\u884c\u5bf9\u9f50\u548c\u63a8\u7406\u589e\u5f3a\u3002", "result": "Ovis2.5-9B\u5728OpenCompass\u591a\u6a21\u6001\u6392\u884c\u699c\u4e0a\u7684\u5e73\u5747\u5206\u4e3a78.3\uff0c\u6bd4\u5176\u524d\u8eabOvis2-8B\u6709\u4e86\u663e\u8457\u63d0\u9ad8\uff0c\u5e76\u572840B\u53c2\u6570\u8303\u56f4\u5185\u7684\u5f00\u6e90MLLM\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff1bOvis2.5-2B\u7684\u5f97\u5206\u4e3a73.9\uff0c\u4e3a\u5176\u89c4\u6a21\u5efa\u7acb\u4e86SOTA\u3002Ovis2.5\u5728STEM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u6210\u679c\uff0c\u5728\u57fa\u7840\u548c\u89c6\u9891\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u5e76\u5728\u5176\u89c4\u6a21\u4e0a\u5b9e\u73b0\u4e86\u590d\u6742\u56fe\u8868\u5206\u6790\u7684\u5f00\u6e90SOTA\u3002", "conclusion": "Ovis2.5-9B\u548cOvis2.5-2B\u662f\u4e24\u4e2a\u5f00\u6e90\u6a21\u578b\uff0c\u5b83\u4eec\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5c24\u5176\u662f\u5728STEM\u57fa\u51c6\u6d4b\u8bd5\u3001\u57fa\u7840\u548c\u89c6\u9891\u4efb\u52a1\u4ee5\u53ca\u590d\u6742\u56fe\u8868\u5206\u6790\u65b9\u9762\u3002"}}
{"id": "2508.11944", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11944", "abs": "https://arxiv.org/abs/2508.11944", "authors": ["Hongtao Liu", "Zhicheng Du", "Zihe Wang", "Weiran Shen"], "title": "CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs", "comment": null, "summary": "Game-playing ability serves as an indicator for evaluating the strategic\nreasoning capability of large language models (LLMs). While most existing\nstudies rely on utility performance metrics, which are not robust enough due to\nvariations in opponent behavior and game structure. To address this limitation,\nwe propose \\textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation\nframework inspired by the cognitive hierarchy models from behavioral economics.\nWe hypothesize that agents have bounded rationality -- different agents behave\nat varying reasoning depths/levels. We evaluate LLMs' strategic reasoning\nthrough a three-phase systematic framework, utilizing behavioral data from six\nstate-of-the-art LLMs across fifteen carefully selected normal-form games.\nExperiments show that LLMs exhibit consistent strategic reasoning levels across\ndiverse opponents, confirming the framework's robustness and generalization\ncapability. We also analyze the effects of two key mechanisms (Chat Mechanism\nand Memory Mechanism) on strategic reasoning performance. Results indicate that\nthe Chat Mechanism significantly degrades strategic reasoning, whereas the\nMemory Mechanism enhances it. These insights position CHBench as a promising\ntool for evaluating LLM capabilities, with significant potential for future\nresearch and practical applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8ba4\u77e5\u5c42\u6b21\u57fa\u51c6\uff08CHBench\uff09\uff0c\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u8bc4\u4f30LLM\u7684\u6218\u7565\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u4f9d\u8d56\u4e8e\u6548\u7528\u6027\u80fd\u6307\u6807\uff0c\u7531\u4e8e\u5bf9\u624b\u884c\u4e3a\u548c\u535a\u5f08\u7ed3\u6784\u7684\u53d8\u5316\uff0c\u8fd9\u4e9b\u6307\u6807\u4e0d\u591f\u7a33\u5065\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u5c40\u9650\u6027\uff0c", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u8ba4\u77e5\u5c42\u6b21\u57fa\u51c6\uff08CHBench\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u53d7\u5230\u884c\u4e3a\u7ecf\u6d4e\u5b66\u4e2d\u8ba4\u77e5\u5c42\u6b21\u6a21\u578b\u542f\u53d1\u7684\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002\u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u4e09\u9636\u6bb5\u7684\u7cfb\u7edf\u6846\u67b6\u8bc4\u4f30LLM\u7684\u6218\u7565\u63a8\u7406\u80fd\u529b\uff0c\u5229\u7528\u6765\u81ea\u516d\u4e2a\u6700\u5148\u8fdb\u7684LLM\u5728\u5341\u4e94\u4e2a\u7cbe\u5fc3\u9009\u62e9\u7684\u8303\u5f0f\u535a\u5f08\u4e2d\u7684\u884c\u4e3a\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLM\u5728\u4e0d\u540c\u7684\u5bf9\u624b\u4e2d\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6218\u7565\u63a8\u7406\u6c34\u5e73\uff0c\u8bc1\u5b9e\u4e86\u8be5\u6846\u67b6\u7684\u7a33\u5065\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u804a\u5929\u673a\u5236\u4f1a\u663e\u8457\u964d\u4f4e\u6218\u7565\u63a8\u7406\u80fd\u529b\uff0c\u800c\u8bb0\u5fc6\u673a\u5236\u4f1a\u63d0\u9ad8\u6218\u7565\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "CHBench\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u8bc4\u4f30LLM\u80fd\u529b\u7684\u5de5\u5177\uff0c\u5bf9\u672a\u6765\u7684\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u5177\u6709\u5de8\u5927\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.12353", "categories": ["cs.IR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.12353", "abs": "https://arxiv.org/abs/2508.12353", "authors": ["Marcel Gregoriadis", "Jingwei Kang", "Johan Pouwelse"], "title": "A Large-Scale Web Search Dataset for Federated Online Learning to Rank", "comment": "Accepted at CIKM 2025", "summary": "The centralized collection of search interaction logs for training ranking\nmodels raises significant privacy concerns. Federated Online Learning to Rank\n(FOLTR) offers a privacy-preserving alternative by enabling collaborative model\ntraining without sharing raw user data. However, benchmarks in FOLTR are\nlargely based on random partitioning of classical learning-to-rank datasets,\nsimulated user clicks, and the assumption of synchronous client participation.\nThis oversimplifies real-world dynamics and undermines the realism of\nexperimental results. We present AOL4FOLTR, a large-scale web search dataset\nwith 2.6 million queries from 10,000 users. Our dataset addresses key\nlimitations of existing benchmarks by including user identifiers, real click\ndata, and query timestamps, enabling realistic user partitioning, behavior\nmodeling, and asynchronous federated learning scenarios.", "AI": {"tldr": "AOL4FOLTR: A large-scale web search dataset for Federated Online Learning to Rank (FOLTR).", "motivation": "Existing FOLTR benchmarks oversimplifies real-world dynamics and undermines the realism of experimental results.", "method": "AOL4FOLTR, a large-scale web search dataset with 2.6 million queries from 10,000 users", "result": "The dataset addresses key limitations of existing benchmarks by including user identifiers, real click data, and query timestamps", "conclusion": "AOL4FOLTR dataset enables realistic user partitioning, behavior modeling, and asynchronous federated learning scenarios."}}
{"id": "2508.11680", "categories": ["cs.LG", "cs.AI", "es: 62M10 (primary), 62P20, 68T05, 91B72 (secondary)"], "pdf": "https://arxiv.org/pdf/2508.11680", "abs": "https://arxiv.org/abs/2508.11680", "authors": ["Aditya Akella", "Jonathan Farah"], "title": "Comparative Analysis of Time Series Foundation Models for Demographic Forecasting: Enhancing Predictive Accuracy in US Population Dynamics", "comment": "6 pages, 4 figures, 3 tables", "summary": "Demographic shifts, influenced by globalization, economic conditions,\ngeopolitical events, and environmental factors, pose significant challenges for\npolicymakers and researchers. Accurate demographic forecasting is essential for\ninformed decision-making in areas such as urban planning, healthcare, and\neconomic policy. This study explores the application of time series foundation\nmodels to predict demographic changes in the United States using datasets from\nthe U.S. Census Bureau and Federal Reserve Economic Data (FRED). We evaluate\nthe performance of the Time Series Foundation Model (TimesFM) against\ntraditional baselines including Long Short-Term Memory (LSTM) networks,\nAutoregressive Integrated Moving Average (ARIMA), and Linear Regression. Our\nexperiments across six demographically diverse states demonstrate that TimesFM\nachieves the lowest Mean Squared Error (MSE) in 86.67% of test cases, with\nparticularly strong performance on minority populations with sparse historical\ndata. These findings highlight the potential of pre-trained foundation models\nto enhance demographic analysis and inform proactive policy interventions\nwithout requiring extensive task-specific fine-tuning.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528TimesFM\u9884\u6d4b\u4eba\u53e3\u53d8\u5316\uff0c\u7ed3\u679c\u8868\u660eTimesFM\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u5c11\u6570\u65cf\u88d4\u4eba\u53e3\u65b9\u9762\u3002", "motivation": "\u4eba\u53e3\u7ed3\u6784\u53d8\u5316\u5bf9\u51b3\u7b56\u8005\u548c\u7814\u7a76\u4eba\u5458\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\uff0c\u51c6\u786e\u7684\u4eba\u53e3\u9884\u6d4b\u5bf9\u4e8e\u57ce\u5e02\u89c4\u5212\u3001\u533b\u7597\u4fdd\u5065\u548c\u7ecf\u6d4e\u653f\u7b56\u7b49\u9886\u57df\u7684\u77e5\u60c5\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5e94\u7528\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u6765\u9884\u6d4b\u7f8e\u56fd\u7684\u4eba\u53e3\u53d8\u5316\uff0c\u4f7f\u7528\u7f8e\u56fd\u4eba\u53e3\u666e\u67e5\u5c40\u548c\u8054\u90a6\u50a8\u5907\u7ecf\u6d4e\u6570\u636e(FRED)\u7684\u6570\u636e\u96c6\uff0c\u5e76\u4e0eLSTM\u7f51\u7edc\u3001ARIMA\u548c\u7ebf\u6027\u56de\u5f52\u7b49\u4f20\u7edf\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5728\u516d\u4e2a\u4e0d\u540c\u4eba\u53e3\u7ed3\u6784\u7684\u5dde\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTimesFM\u572886.67%\u7684\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f4e\u7684\u5747\u65b9\u8bef\u5dee(MSE)\uff0c\u5c24\u5176\u662f\u5728\u5386\u53f2\u6570\u636e\u7a00\u758f\u7684\u5c11\u6570\u65cf\u88d4\u4eba\u53e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "TimesFM\u5728\u9884\u6d4b\u4eba\u53e3\u53d8\u5316\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u5c11\u6570\u65cf\u88d4\u4eba\u53e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u65e0\u9700\u5927\u91cf\u7279\u5b9a\u4efb\u52a1\u5fae\u8c03\u5373\u53ef\u589e\u5f3a\u4eba\u53e3\u5206\u6790\u5e76\u4e3a\u79ef\u6781\u7684\u653f\u7b56\u5e72\u9884\u63d0\u4f9b\u4fe1\u606f\u3002"}}
{"id": "2508.11816", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11816", "abs": "https://arxiv.org/abs/2508.11816", "authors": ["Krishna Chaitanya Marturi", "Heba H. Elwazzan"], "title": "LLM-Guided Planning and Summary-Based Scientific Text Simplification: DS@GT at CLEF 2025 SimpleText", "comment": "Text Simplification, hallucination detection, LLMs, CLEF 2025,\n  SimpleText, CEUR-WS", "summary": "In this paper, we present our approach for the CLEF 2025 SimpleText Task 1,\nwhich addresses both sentence-level and document-level scientific text\nsimplification. For sentence-level simplification, our methodology employs\nlarge language models (LLMs) to first generate a structured plan, followed by\nplan-driven simplification of individual sentences. At the document level, we\nleverage LLMs to produce concise summaries and subsequently guide the\nsimplification process using these summaries. This two-stage, LLM-based\nframework enables more coherent and contextually faithful simplifications of\nscientific text.", "AI": {"tldr": "This paper uses LLMs in a two-stage framework to simplify scientific text at both sentence and document levels.", "motivation": "The paper addresses both sentence-level and document-level scientific text simplification.", "method": "The methodology employs large language models (LLMs) to generate structured plans for sentence-level simplification and leverages LLMs to produce concise summaries to guide the document-level simplification process.", "result": "The two-stage, LLM-based framework enables more coherent and contextually faithful simplifications of scientific text.", "conclusion": "This paper presents a two-stage, LLM-based framework for coherent and contextually faithful simplification of scientific text at both the sentence and document levels."}}
{"id": "2508.11801", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11801", "abs": "https://arxiv.org/abs/2508.11801", "authors": ["Ming Cheng", "Tong Wu", "Jiazhen Hu", "Jiaying Gong", "Hoda Eldardiry"], "title": "VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models", "comment": "5 pages, 2 figures, 5 tables, accepted in CIKM 2025", "summary": "Attribute Value Extraction (AVE) is important for structuring product\ninformation in e-commerce. However, existing AVE datasets are primarily limited\nto text-to-text or image-to-text settings, lacking support for product videos,\ndiverse attribute coverage, and public availability. To address these gaps, we\nintroduce VideoAVE, the first publicly available video-to-text e-commerce AVE\ndataset across 14 different domains and covering 172 unique attributes. To\nensure data quality, we propose a post-hoc CLIP-based Mixture of Experts\nfiltering system (CLIP-MoE) to remove the mismatched video-product pairs,\nresulting in a refined dataset of 224k training data and 25k evaluation data.\nIn order to evaluate the usability of the dataset, we further establish a\ncomprehensive benchmark by evaluating several state-of-the-art video vision\nlanguage models (VLMs) under both attribute-conditioned value prediction and\nopen attribute-value pair extraction tasks. Our results analysis reveals that\nvideo-to-text AVE remains a challenging problem, particularly in open settings,\nand there is still room for developing more advanced VLMs capable of leveraging\neffective temporal information. The dataset and benchmark code for VideoAVE are\navailable at: https://github.com/gjiaying/VideoAVE", "AI": {"tldr": "introduce VideoAVE, the first publicly available video-to-text e-commerce AVE dataset", "motivation": "existing AVE datasets are primarily limited to text-to-text or image-to-text settings, lacking support for product videos, diverse attribute coverage, and public availability", "method": "a post-hoc CLIP-based Mixture of Experts filtering system (CLIP-MoE)", "result": "a refined dataset of 224k training data and 25k evaluation data", "conclusion": "video-to-text AVE remains a challenging problem, particularly in open settings, and there is still room for developing more advanced VLMs capable of leveraging effective temporal information"}}
{"id": "2508.11953", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11953", "abs": "https://arxiv.org/abs/2508.11953", "authors": ["Yuan Li", "Zhengzhong Liu", "Eric Xing"], "title": "Data Mixing Optimization for Supervised Fine-Tuning of Large Language Models", "comment": null, "summary": "Optimizing data mixtures for supervised fine-tuning (SFT) of large language\nmodels (LLMs) is critical for developing general-purpose models, yet this area\nremains underexplored. In this paper, we frame data mixing as an optimization\nproblem and introduce a novel method designed to minimize validation loss. Our\napproach parametrizes the loss by modeling effective data transferred and\nleveraging scaling laws for fine-tuning. By experimenting with various\nsmall-scale data mixtures, we fit these parameters and derive the optimal\nweights. We provide both mathematical proofs and empirical results\ndemonstrating that our algorithm achieves excellent overall and individual\nperformance across all domains. Through controlled experiments, we show that\nmodels trained with our optimized weights perform on par with those using\noptimal weights determined via grid search, with per-domain loss only 0.66%\nhigher than the best domain loss from grid search on average. Additionally, we\nshow that reweighting popular SFT datasets using our method improves both\nvalidation loss and downstream performance. Finally, we discuss how our method\ncan generalize to guide data selection for domain-specific models and provide\ninsights into SFT.", "AI": {"tldr": "This paper introduces a novel data mixing method for supervised fine-tuning (SFT) of large language models (LLMs) that minimizes validation loss. The method achieves performance on par with grid search and improves both validation loss and downstream performance when reweighting popular SFT datasets.", "motivation": "Optimizing data mixtures for supervised fine-tuning (SFT) of large language models (LLMs) is critical for developing general-purpose models, yet this area remains underexplored. In this paper, data mixing is framed as an optimization problem.", "method": "This paper introduces a novel method designed to minimize validation loss. The approach parametrizes the loss by modeling effective data transferred and leveraging scaling laws for fine-tuning. By experimenting with various small-scale data mixtures, the parameters are fitted and the optimal weights are derived.", "result": "The algorithm achieves excellent overall and individual performance across all domains. Models trained with optimized weights perform on par with those using optimal weights determined via grid search, with per-domain loss only 0.66% higher than the best domain loss from grid search on average. Reweighting popular SFT datasets improves both validation loss and downstream performance.", "conclusion": "The algorithm achieves excellent overall and individual performance across all domains. Models trained with optimized weights perform on par with those using optimal weights determined via grid search, with per-domain loss only 0.66% higher than the best domain loss from grid search on average. Reweighting popular SFT datasets improves both validation loss and downstream performance. The method can generalize to guide data selection for domain-specific models and provide insights into SFT."}}
{"id": "2508.12365", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.12365", "abs": "https://arxiv.org/abs/2508.12365", "authors": ["Chenhe Dong", "Shaowei Yao", "Pengkun Jiao", "Jianhui Yang", "Yiming Jin", "Zerui Huang", "Xiaojiang Zhou", "Dan Ou", "Haihong Tang"], "title": "TaoSR1: The Thinking Model for E-commerce Relevance Search", "comment": null, "summary": "Query-product relevance prediction is a core task in e-commerce search.\nBERT-based models excel at semantic matching but lack complex reasoning\ncapabilities. While Large Language Models (LLMs) are explored, most still use\ndiscriminative fine-tuning or distill to smaller models for deployment. We\npropose a framework to directly deploy LLMs for this task, addressing key\nchallenges: Chain-of-Thought (CoT) error accumulation, discriminative\nhallucination, and deployment feasibility. Our framework, TaoSR1, involves\nthree stages: (1) Supervised Fine-Tuning (SFT) with CoT to instill reasoning;\n(2) Offline sampling with a pass@N strategy and Direct Preference Optimization\n(DPO) to improve generation quality; and (3) Difficulty-based dynamic sampling\nwith Group Relative Policy Optimization (GRPO) to mitigate discriminative\nhallucination. Additionally, post-CoT processing and a cumulative\nprobability-based partitioning method enable efficient online deployment.\nTaoSR1 significantly outperforms baselines on offline datasets and achieves\nsubstantial gains in online side-by-side human evaluations, introducing a novel\nparadigm for applying CoT reasoning to relevance classification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTaoSR1\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u76f4\u63a5\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee5\u8fdb\u884cquery-product\u76f8\u5173\u6027\u9884\u6d4b\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408CoT\u3001DPO\u548cGRPO\u7b49\u6280\u672f\uff0c\u5728\u63a8\u7406\u80fd\u529b\u3001\u751f\u6210\u8d28\u91cf\u548c\u5e7b\u89c9\u6291\u5236\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5728\u5728\u7ebf\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u57fa\u4e8eBERT\u7684\u6a21\u578b\u64c5\u957f\u8bed\u4e49\u5339\u914d\uff0c\u4f46\u7f3a\u4e4f\u590d\u6742\u7684\u63a8\u7406\u80fd\u529b\u3002\u867d\u7136\u5df2\u7ecf\u63a2\u7d22\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4f46\u5927\u591a\u6570\u4ecd\u7136\u4f7f\u7528\u5224\u522b\u6027\u5fae\u8c03\u6216\u63d0\u70bc\u5230\u8f83\u5c0f\u7684\u6a21\u578b\u4ee5\u8fdb\u884c\u90e8\u7f72\u3002", "method": "TaoSR1\u6846\u67b6\uff0c\u6d89\u53ca\u4e09\u4e2a\u9636\u6bb5\uff1a\uff081\uff09\u4f7f\u7528CoT\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4ee5\u704c\u8f93\u63a8\u7406\uff1b\uff082\uff09\u4f7f\u7528pass@N\u7b56\u7565\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u8fdb\u884c\u79bb\u7ebf\u91c7\u6837\uff0c\u4ee5\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\uff1b\uff083\uff09\u4f7f\u7528\u57fa\u4e8e\u96be\u5ea6\u7684\u52a8\u6001\u91c7\u6837\u548c\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u6765\u51cf\u8f7b\u5224\u522b\u6027\u5e7b\u89c9\u3002", "result": "TaoSR1\u5728\u79bb\u7ebf\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e76\u5728\u5728\u7ebf\u4eba\u5de5\u8bc4\u4f30\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6536\u76ca\u3002", "conclusion": "TaoSR1\u663e\u8457\u4f18\u4e8e\u79bb\u7ebf\u6570\u636e\u96c6\u7684\u57fa\u7ebf\uff0c\u5e76\u5728\u5728\u7ebf\u5e76\u6392\u4eba\u5de5\u8bc4\u4f30\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6536\u76ca\uff0c\u4e3a\u5c06CoT\u63a8\u7406\u5e94\u7528\u4e8e\u76f8\u5173\u6027\u5206\u7c7b\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u8303\u4f8b\u3002"}}
{"id": "2508.11723", "categories": ["cs.LG", "68T07, 91D10", "I.2.10; H.2.8"], "pdf": "https://arxiv.org/pdf/2508.11723", "abs": "https://arxiv.org/abs/2508.11723", "authors": ["Qian Cao", "Jielin Chen", "Junchao Zhao", "Rudi Stouffs"], "title": "From Heuristics to Data: Quantifying Site Planning Layout Indicators with Deep Learning and Multi-Modal Data", "comment": "42 pages, 32 figures, submitted to Environment and Planning B: Urban\n  Analytics and City Science", "summary": "The spatial layout of urban sites shapes land-use efficiency and spatial\norganization. Traditional site planning often relies on experiential judgment\nand single-source data, limiting systematic quantification of multifunctional\nlayouts. We propose a Site Planning Layout Indicator (SPLI) system, a\ndata-driven framework integrating empirical knowledge with heterogeneous\nmulti-source data to produce structured urban spatial information. The SPLI\nsupports multimodal spatial data systems for analytics, inference, and\nretrieval by combining OpenStreetMap (OSM), Points of Interest (POI), building\nmorphology, land use, and satellite imagery. It extends conventional metrics\nthrough five dimensions: (1) Hierarchical Building Function Classification,\nrefining empirical systems into clear hierarchies; (2) Spatial Organization,\nquantifying seven layout patterns (e.g., symmetrical, concentric,\naxial-oriented); (3) Functional Diversity, transforming qualitative assessments\ninto measurable indicators using Functional Ratio (FR) and Simpson Index (SI);\n(4) Accessibility to Essential Services, integrating facility distribution and\ntransport networks for comprehensive accessibility metrics; and (5) Land Use\nIntensity, using Floor Area Ratio (FAR) and Building Coverage Ratio (BCR) to\nassess utilization efficiency. Data gaps are addressed through deep learning,\nincluding Relational Graph Neural Networks (RGNN) and Graph Neural Networks\n(GNN). Experiments show the SPLI improves functional classification accuracy\nand provides a standardized basis for automated, data-driven urban spatial\nanalytics.", "AI": {"tldr": "The paper proposes a Site Planning Layout Indicator (SPLI) system, a data-driven framework for urban spatial analytics, integrating multi-source data and deep learning to improve functional classification accuracy.", "motivation": "Traditional site planning often relies on experiential judgment and single-source data, limiting systematic quantification of multifunctional layouts.", "method": "A data-driven framework integrating empirical knowledge with heterogeneous multi-source data to produce structured urban spatial information. It extends conventional metrics through five dimensions: (1) Hierarchical Building Function Classification (2) Spatial Organization (3) Functional Diversity (4) Accessibility to Essential Services (5) Land Use Intensity. Data gaps are addressed through deep learning, including Relational Graph Neural Networks (RGNN) and Graph Neural Networks (GNN).", "result": "The SPLI supports multimodal spatial data systems for analytics, inference, and retrieval by combining OpenStreetMap (OSM), Points of Interest (POI), building morphology, land use, and satellite imagery.", "conclusion": "The SPLI improves functional classification accuracy and provides a standardized basis for automated, data-driven urban spatial analytics."}}
{"id": "2508.11823", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11823", "abs": "https://arxiv.org/abs/2508.11823", "authors": ["Krishna Chaitanya Marturi", "Heba H. Elwazzan"], "title": "Hallucination Detection and Mitigation in Scientific Text Simplification using Ensemble Approaches: DS@GT at CLEF 2025 SimpleText", "comment": "Text Simplification, hallucination detection, LLMs, CLEF 2025,\n  SimpleText, CEUR-WS", "summary": "In this paper, we describe our methodology for the CLEF 2025 SimpleText Task\n2, which focuses on detecting and evaluating creative generation and\ninformation distortion in scientific text simplification. Our solution\nintegrates multiple strategies: we construct an ensemble framework that\nleverages BERT-based classifier, semantic similarity measure, natural language\ninference model, and large language model (LLM) reasoning. These diverse\nsignals are combined using meta-classifiers to enhance the robustness of\nspurious and distortion detection. Additionally, for grounded generation, we\nemploy an LLM-based post-editing system that revises simplifications based on\nthe original input texts.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u4e8e\u68c0\u6d4b\u79d1\u5b66\u6587\u672c\u7b80\u5316\u4e2d\u4f2a\u9020\u548c\u626d\u66f2\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86BERT\u3001\u8bed\u4e49\u76f8\u4f3c\u6027\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u548cLLM\u3002", "motivation": "\u4e13\u6ce8\u4e8e\u68c0\u6d4b\u548c\u8bc4\u4f30\u79d1\u5b66\u6587\u672c\u7b80\u5316\u4e2d\u7684\u521b\u9020\u6027\u751f\u6210\u548c\u4fe1\u606f\u626d\u66f2\u3002", "method": "\u96c6\u6210\u4e86\u591a\u79cd\u7b56\u7565\uff1a\u6784\u5efa\u4e86\u4e00\u4e2a\u5229\u7528\u57fa\u4e8eBERT\u7684\u5206\u7c7b\u5668\u3001\u8bed\u4e49\u76f8\u4f3c\u6027\u5ea6\u91cf\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u7684\u96c6\u6210\u6846\u67b6\u3002\u4f7f\u7528\u5143\u5206\u7c7b\u5668\u7ec4\u5408\u8fd9\u4e9b\u4e0d\u540c\u7684\u4fe1\u53f7\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u7528\u4e8eCLEF 2025 SimpleText Task 2 \u7684\u65b9\u6cd5\u3002", "conclusion": "\u96c6\u6210\u5206\u7c7b\u5668\u63d0\u5347\u4e86\u68c0\u6d4b\u4f2a\u9020\u548c\u626d\u66f2\u7684\u9c81\u68d2\u6027\u3002LLM-based\u7684\u540e\u7f16\u8f91\u7cfb\u7edf\u6839\u636e\u539f\u59cb\u8f93\u5165\u6587\u672c\u4fee\u6539\u7b80\u5316\u3002"}}
{"id": "2508.11803", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11803", "abs": "https://arxiv.org/abs/2508.11803", "authors": ["Azam Nouri"], "title": "An MLP Baseline for Handwriting Recognition Using Planar Curvature and Gradient Orientation", "comment": "5 pages, No figure", "summary": "This study investigates whether second-order geometric cues - planar\ncurvature magnitude, curvature sign, and gradient orientation - are sufficient\non their own to drive a multilayer perceptron (MLP) classifier for handwritten\ncharacter recognition (HCR), offering an alternative to convolutional neural\nnetworks (CNNs). Using these three handcrafted feature maps as inputs, our\ncurvature-orientation MLP achieves 97 percent accuracy on MNIST digits and 89\npercent on EMNIST letters. These results underscore the discriminative power of\ncurvature-based representations for handwritten character images and\ndemonstrate that the advantages of deep learning can be realized even with\ninterpretable, hand-engineered features.", "AI": {"tldr": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u66f2\u7387\u7684\u7279\u5f81\u53ef\u4ee5\u6709\u6548\u5730\u7528\u4e8e\u624b\u5199\u5b57\u7b26\u8bc6\u522b\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f7f\u7528\u624b\u5de5\u8bbe\u8ba1\u7684\u7279\u5f81\u5b9e\u73b0\u6df1\u5ea6\u5b66\u4e60\u7684\u4f18\u52bf\u3002", "motivation": "\u7814\u7a76\u4e8c\u9636\u51e0\u4f55\u7ebf\u7d22\u662f\u5426\u8db3\u4ee5\u9a71\u52a8\u7528\u4e8e\u624b\u5199\u5b57\u7b26\u8bc6\u522b (HCR) \u7684\u591a\u5c42\u611f\u77e5\u5668 (MLP) \u5206\u7c7b\u5668\uff0c\u4ece\u800c\u4e3a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u63d0\u4f9b\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u4e8c\u9636\u51e0\u4f55\u7ebf\u7d22\uff08\u5e73\u9762\u66f2\u7387\u5927\u5c0f\u3001\u66f2\u7387\u7b26\u53f7\u548c\u68af\u5ea6\u65b9\u5411\uff09\u9a71\u52a8\u591a\u5c42\u611f\u77e5\u5668 (MLP) \u5206\u7c7b\u5668\u3002", "result": "\u57fa\u4e8e\u66f2\u7387\u65b9\u5411\u7684 MLP \u5728 MNIST \u6570\u5b57\u4e0a\u5b9e\u73b0\u4e86 97% \u7684\u51c6\u786e\u7387\uff0c\u5728 EMNIST \u5b57\u6bcd\u4e0a\u5b9e\u73b0\u4e86 89% \u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u66f2\u7387\u8868\u793a\u6cd5\u5bf9\u624b\u5199\u5b57\u7b26\u56fe\u50cf\u5177\u6709\u5224\u522b\u80fd\u529b\uff0c\u5373\u4f7f\u4f7f\u7528\u53ef\u89e3\u91ca\u7684\u624b\u5de5\u8bbe\u8ba1\u7684\u7279\u5f81\uff0c\u4e5f\u53ef\u4ee5\u5b9e\u73b0\u6df1\u5ea6\u5b66\u4e60\u7684\u4f18\u52bf\u3002"}}
{"id": "2508.11954", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11954", "abs": "https://arxiv.org/abs/2508.11954", "authors": ["Sehyuk Park", "Soyeon Caren Han", "Eduard Hovy"], "title": "UniCast: A Unified Multimodal Prompting Framework for Time Series Forecasting", "comment": null, "summary": "Time series forecasting is a foundational task across domains, such as\nfinance, healthcare, and environmental monitoring. While recent advances in\nTime Series Foundation Models (TSFMs) have demonstrated strong generalisation\nthrough large-scale pretraining, existing models operate predominantly in a\nunimodal setting, ignoring the rich multimodal context, such as visual and\ntextual signals, that often accompanies time series data in real-world\nscenarios. This paper introduces a novel parameter-efficient multimodal\nframework, UniCast, that extends TSFMs to jointly leverage time series, vision,\nand text modalities for enhanced forecasting performance. Our method integrates\nmodality-specific embeddings from pretrained Vision and Text Encoders with a\nfrozen TSFM via soft prompt tuning, enabling efficient adaptation with minimal\nparameter updates. This design not only preserves the generalisation strength\nof the foundation model but also enables effective cross-modal interaction.\nExtensive experiments across diverse time-series forecasting benchmarks\ndemonstrate that UniCast consistently and significantly outperforms all\nexisting TSFM baselines. The findings highlight the critical role of multimodal\ncontext in advancing the next generation of general-purpose time series\nforecasters.", "AI": {"tldr": "UniCast\u662f\u4e00\u79cd\u65b0\u9896\u7684\u53c2\u6570\u9ad8\u6548\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5b83\u6269\u5c55\u4e86TSFM\uff0c\u4ee5\u5171\u540c\u5229\u7528\u65f6\u95f4\u5e8f\u5217\u3001\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\uff0c\u4ece\u800c\u589e\u5f3a\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFM\uff09\u4e3b\u8981\u5728\u5355\u6a21\u6001\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u5ffd\u7565\u4e86\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\uff0c\u4f8b\u5982\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u53f7\uff0c\u8fd9\u4e9b\u4fe1\u53f7\u901a\u5e38\u4f34\u968f\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002", "method": "\u901a\u8fc7\u8f6f\u63d0\u793a\u8c03\u6574\uff0cUniCast\u96c6\u6210\u4e86\u6765\u81ea\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u548c\u6587\u672c\u7f16\u7801\u5668\u7684\u6a21\u6001\u7279\u5b9a\u5d4c\u5165\u4ee5\u53ca\u51bb\u7ed3\u7684TSFM\uff0c\u4ece\u800c\u80fd\u591f\u4ee5\u6700\u5c11\u7684\u53c2\u6570\u66f4\u65b0\u8fdb\u884c\u6709\u6548\u9002\u5e94\u3002", "result": "UniCast\u59cb\u7ec8\u4e14\u663e\u7740\u5730\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u7684TSFM\u57fa\u7ebf\u3002", "conclusion": "UniCast\u5728\u5404\u79cd\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u51c6\u4e0a\u59cb\u7ec8\u4e14\u663e\u7740\u5730\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u7684TSFM\u57fa\u7ebf\u3002\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5728\u63a8\u8fdb\u4e0b\u4e00\u4ee3\u901a\u7528\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5668\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2508.12377", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.12377", "abs": "https://arxiv.org/abs/2508.12377", "authors": ["Yang Xu", "Zuliang Yang", "Kai Ming Ting"], "title": "Contrastive Multi-View Graph Hashing", "comment": null, "summary": "Multi-view graph data, which both captures node attributes and rich\nrelational information from diverse sources, is becoming increasingly prevalent\nin various domains. The effective and efficient retrieval of such data is an\nimportant task. Although multi-view hashing techniques have offered a paradigm\nfor fusing diverse information into compact binary codes, they typically assume\nattributes-based inputs per view. This makes them unsuitable for multi-view\ngraph data, where effectively encoding and fusing complex topological\ninformation from multiple heterogeneous graph views to generate unified binary\nembeddings remains a significant challenge. In this work, we propose\nContrastive Multi-view Graph Hashing (CMGHash), a novel end-to-end framework\ndesigned to learn unified and discriminative binary embeddings from multi-view\ngraph data. CMGHash learns a consensus node representation space using a\ncontrastive multi-view graph loss, which aims to pull $k$-nearest neighbors\nfrom all graphs closer while pushing away negative pairs, i.e., non-neighbor\nnodes. Moreover, we impose binarization constraints on this consensus space,\nenabling its conversion to a corresponding binary embedding space at minimal\ncost. Extensive experiments on several benchmark datasets demonstrate that\nCMGHash significantly outperforms existing approaches in terms of retrieval\naccuracy.", "AI": {"tldr": "CMGHash\uff1a\u4e00\u79cd\u65b0\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u591a\u89c6\u56fe\u56fe\u6570\u636e\u4e2d\u5b66\u4e60\u7edf\u4e00\u7684\u5224\u522b\u6027\u4e8c\u503c\u5d4c\u5165\uff0c\u5e76\u5728\u68c0\u7d22\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u89c6\u56fe\u56fe\u6570\u636e\u5728\u5404\u4e2a\u9886\u57df\u53d8\u5f97\u8d8a\u6765\u8d8a\u666e\u904d\uff0c\u5b83\u65e2\u6355\u83b7\u8282\u70b9\u5c5e\u6027\uff0c\u53c8\u6355\u83b7\u6765\u81ea\u4e0d\u540c\u6765\u6e90\u7684\u4e30\u5bcc\u5173\u7cfb\u4fe1\u606f\u3002\u5bf9\u6b64\u7c7b\u6570\u636e\u8fdb\u884c\u6709\u6548\u68c0\u7d22\u662f\u4e00\u9879\u91cd\u8981\u4efb\u52a1\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u591a\u89c6\u56fe\u54c8\u5e0c\u6280\u672f\u901a\u5e38\u5047\u8bbe\u6bcf\u4e2a\u89c6\u56fe\u90fd\u662f\u57fa\u4e8e\u5c5e\u6027\u7684\u8f93\u5165\uff0c\u56e0\u6b64\u4e0d\u9002\u5408\u591a\u89c6\u56fe\u56fe\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u79f0\u4e3a\u5bf9\u6bd4\u591a\u89c6\u56fe\u56fe\u54c8\u5e0c (CMGHash)\uff0c\u65e8\u5728\u4ece\u591a\u89c6\u56fe\u56fe\u6570\u636e\u4e2d\u5b66\u4e60\u7edf\u4e00\u7684\u5224\u522b\u6027\u4e8c\u503c\u5d4c\u5165\u3002", "result": "CMGHash\u5b66\u4e60\u4f7f\u7528\u5bf9\u6bd4\u591a\u89c6\u56fe\u56fe\u635f\u5931\u7684\u5171\u8bc6\u8282\u70b9\u8868\u793a\u7a7a\u95f4\uff0c\u8be5\u635f\u5931\u65e8\u5728\u5c06\u6765\u81ea\u6240\u6709\u56fe\u7684$k$-\u6700\u8fd1\u90bb\u62c9\u5f97\u66f4\u8fd1\uff0c\u540c\u65f6\u63a8\u5f00\u8d1f\u5bf9\uff0c\u5373\u975e\u90bb\u5c45\u8282\u70b9\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5728\u8be5\u5171\u8bc6\u7a7a\u95f4\u4e0a\u65bd\u52a0\u4e8c\u503c\u5316\u7ea6\u675f\uff0c\u4f7f\u5176\u80fd\u591f\u4ee5\u6700\u5c0f\u7684\u4ee3\u4ef7\u8f6c\u6362\u4e3a\u76f8\u5e94\u7684\u4e8c\u503c\u5d4c\u5165\u7a7a\u95f4\u3002", "conclusion": "CMGHash\u5728\u68c0\u7d22\u7cbe\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.11727", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.11727", "abs": "https://arxiv.org/abs/2508.11727", "authors": ["Songyao Jin", "Biwei Huang"], "title": "Causal Structure Learning in Hawkes Processes with Complex Latent Confounder Networks", "comment": null, "summary": "Multivariate Hawkes process provides a powerful framework for modeling\ntemporal dependencies and event-driven interactions in complex systems. While\nexisting methods primarily focus on uncovering causal structures among observed\nsubprocesses, real-world systems are often only partially observed, with latent\nsubprocesses posing significant challenges. In this paper, we show that\ncontinuous-time event sequences can be represented by a discrete-time model as\nthe time interval shrinks, and we leverage this insight to establish necessary\nand sufficient conditions for identifying latent subprocesses and the causal\ninfluences. Accordingly, we propose a two-phase iterative algorithm that\nalternates between inferring causal relationships among discovered subprocesses\nand uncovering new latent subprocesses, guided by path-based conditions that\nguarantee identifiability. Experiments on both synthetic and real-world\ndatasets show that our method effectively recovers causal structures despite\nthe presence of latent subprocesses.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bc6\u522b\u5177\u6709\u6f5c\u5728\u5b50\u8fc7\u7a0b\u7684\u590d\u6742\u7cfb\u7edf\u4e2d\u56e0\u679c\u7ed3\u6784\u7684\u7b97\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u7cfb\u7edf\u901a\u5e38\u53ea\u662f\u90e8\u5206\u88ab\u89c2\u5bdf\u5230\uff0c\u6f5c\u5728\u7684\u5b50\u8fc7\u7a0b\u5e26\u6765\u4e86\u5de8\u5927\u7684\u6311\u6218\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u8fed\u4ee3\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u63a8\u65ad\u5df2\u53d1\u73b0\u7684\u5b50\u8fc7\u7a0b\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u548c\u53d1\u73b0\u65b0\u7684\u6f5c\u5728\u5b50\u8fc7\u7a0b\u4e4b\u95f4\u4ea4\u66ff\u8fdb\u884c\uff0c\u5e76\u7531\u57fa\u4e8e\u8def\u5f84\u7684\u6761\u4ef6\u6765\u4fdd\u8bc1\u53ef\u8bc6\u522b\u6027\u3002", "result": "\u6211\u4eec\u8868\u660e\uff0c\u968f\u7740\u65f6\u95f4\u95f4\u9694\u7684\u7f29\u5c0f\uff0c\u8fde\u7eed\u65f6\u95f4\u4e8b\u4ef6\u5e8f\u5217\u53ef\u4ee5\u8868\u793a\u4e3a\u79bb\u6563\u65f6\u95f4\u6a21\u578b\uff0c\u5e76\u4e14\u6211\u4eec\u5229\u7528\u8fd9\u4e00\u89c1\u89e3\u6765\u5efa\u7acb\u8bc6\u522b\u6f5c\u5728\u5b50\u8fc7\u7a0b\u548c\u56e0\u679c\u5f71\u54cd\u7684\u5fc5\u8981\u548c\u5145\u5206\u6761\u4ef6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u6062\u590d\u4e86\u56e0\u679c\u7ed3\u6784\uff0c\u5c3d\u7ba1\u5b58\u5728\u6f5c\u5728\u7684\u5b50\u8fc7\u7a0b\u3002"}}
{"id": "2508.11828", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11828", "abs": "https://arxiv.org/abs/2508.11828", "authors": ["Michael Flor", "Xinyi Liu", "Anna Feldman"], "title": "A Survey of Idiom Datasets for Psycholinguistic and Computational Research", "comment": "KONVENS 2025. To appear", "summary": "Idioms are figurative expressions whose meanings often cannot be inferred\nfrom their individual words, making them difficult to process computationally\nand posing challenges for human experimental studies. This survey reviews\ndatasets developed in psycholinguistics and computational linguistics for\nstudying idioms, focusing on their content, form, and intended use.\nPsycholinguistic resources typically contain normed ratings along dimensions\nsuch as familiarity, transparency, and compositionality, while computational\ndatasets support tasks like idiomaticity detection/classification,\nparaphrasing, and cross-lingual modeling. We present trends in annotation\npractices, coverage, and task framing across 53 datasets. Although recent\nefforts expanded language coverage and task diversity, there seems to be no\nrelation yet between psycholinguistic and computational research on idioms.", "AI": {"tldr": "This survey reviews datasets for studying idioms in psycholinguistics and computational linguistics, highlighting annotation practices, coverage, and task framing across 53 datasets. It finds a disconnect between the two fields.", "motivation": "Idioms are figurative expressions whose meanings often cannot be inferred from their individual words, making them difficult to process computationally and posing challenges for human experimental studies.", "method": "This survey reviews datasets developed in psycholinguistics and computational linguistics for studying idioms, focusing on their content, form, and intended use. We present trends in annotation practices, coverage, and task framing across 53 datasets.", "result": "Psycholinguistic resources typically contain normed ratings along dimensions such as familiarity, transparency, and compositionality, while computational datasets support tasks like idiomaticity detection/classification, paraphrasing, and cross-lingual modeling. Although recent efforts expanded language coverage and task diversity", "conclusion": "There seems to be no relation yet between psycholinguistic and computational research on idioms."}}
{"id": "2508.11808", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CY", "cs.MM", "I.2.7; I.2.10"], "pdf": "https://arxiv.org/pdf/2508.11808", "abs": "https://arxiv.org/abs/2508.11808", "authors": ["Sahajpreet Singh", "Rongxin Ouyang", "Subhayan Mukerjee", "Kokil Jaidka"], "title": "Labels or Input? Rethinking Augmentation in Multimodal Hate Detection", "comment": "13 pages, 2 figures, 7 tables", "summary": "The modern web is saturated with multimodal content, intensifying the\nchallenge of detecting hateful memes, where harmful intent is often conveyed\nthrough subtle interactions between text and image under the guise of humor or\nsatire. While recent advances in Vision-Language Models (VLMs) show promise,\nthese models lack support for fine-grained supervision and remain susceptible\nto implicit hate speech. In this paper, we present a dual-pronged approach to\nimprove multimodal hate detection. First, we propose a prompt optimization\nframework that systematically varies prompt structure, supervision granularity,\nand training modality. We show that prompt design and label scaling both\ninfluence performance, with structured prompts improving robustness even in\nsmall models, and InternVL2 achieving the best F1-scores across binary and\nscaled settings. Second, we introduce a multimodal data augmentation pipeline\nthat generates 2,479 counterfactually neutral memes by isolating and rewriting\nthe hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup,\nsuccessfully reduces spurious correlations and improves classifier\ngeneralization. Our approaches inspire new directions for building synthetic\ndata to train robust and fair vision-language models. Our findings demonstrate\nthat prompt structure and data composition are as critical as model size, and\nthat targeted augmentation can support more trustworthy and context-sensitive\nhate detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u7ba1\u9f50\u4e0b\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u591a\u6a21\u6001\u4ec7\u6068\u68c0\u6d4b\u3002", "motivation": "\u73b0\u4ee3\u7f51\u7edc\u5145\u65a5\u7740\u591a\u6a21\u6001\u5185\u5bb9\uff0c\u52a0\u5267\u4e86\u68c0\u6d4b\u4ec7\u6068\u6a21\u56e0\u7684\u6311\u6218\uff0c\u5728\u4ec7\u6068\u6a21\u56e0\u4e2d\uff0c\u6709\u5bb3\u610f\u56fe\u901a\u5e38\u901a\u8fc7\u6587\u672c\u548c\u56fe\u50cf\u4e4b\u95f4\u7684\u5fae\u5999\u4ea4\u4e92\u6765\u4f20\u8fbe\uff0c\u4f2a\u88c5\u6210\u5e7d\u9ed8\u6216\u8bbd\u523a\u3002\u867d\u7136\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u6700\u65b0\u8fdb\u5c55\u663e\u793a\u51fa\u4e86\u5e0c\u671b\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u7f3a\u4e4f\u5bf9\u7ec6\u7c92\u5ea6\u76d1\u7763\u7684\u652f\u6301\uff0c\u5e76\u4e14\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u9690\u6027\u4ec7\u6068\u8a00\u8bba\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2aprompt\u4f18\u5316\u6846\u67b6\uff0c\u7cfb\u7edf\u5730\u6539\u53d8\u4e86prompt\u7ed3\u6784\u3001\u76d1\u7763\u7c92\u5ea6\u548c\u8bad\u7ec3\u6a21\u6001\u3002\u5f15\u5165\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6570\u636e\u589e\u5f3a\u7ba1\u9053\uff0c\u901a\u8fc7\u9694\u79bb\u548c\u91cd\u5199\u4ec7\u6068\u6a21\u6001\u6765\u751f\u62102,479\u4e2a\u53cd\u4e8b\u5b9e\u7684\u4e2d\u6027\u6a21\u56e0\u3002", "result": "\u7ed3\u6784\u5316prompt\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff0c\u5373\u4f7f\u5728\u5c0f\u578b\u6a21\u578b\u4e2d\u4e5f\u662f\u5982\u6b64\uff0c\u5e76\u4e14InternVL2\u5728\u4e8c\u5143\u548c\u7f29\u653e\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f73F1\u5206\u6570\u3002\u8be5\u7ba1\u9053\u6210\u529f\u5730\u51cf\u5c11\u4e86\u865a\u5047\u76f8\u5173\u6027\uff0c\u5e76\u63d0\u9ad8\u4e86\u5206\u7c7b\u5668\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Prompt\u7ed3\u6784\u548c\u6570\u636e\u7ec4\u6210\u4e0e\u6a21\u578b\u5927\u5c0f\u4e00\u6837\u91cd\u8981\uff0c\u6709\u9488\u5bf9\u6027\u7684\u589e\u5f3a\u53ef\u4ee5\u652f\u6301\u66f4\u53ef\u4fe1\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u4ec7\u6068\u68c0\u6d4b\u3002"}}
{"id": "2508.11959", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11959", "abs": "https://arxiv.org/abs/2508.11959", "authors": ["Xuanxiang Huang", "Olivier L\u00e9toff\u00e9", "Joao Marques-Silva"], "title": "Rigorous Feature Importance Scores based on Shapley Value and Banzhaf Index", "comment": null, "summary": "Feature attribution methods based on game theory are ubiquitous in the field\nof eXplainable Artificial Intelligence (XAI). Recent works proposed rigorous\nfeature attribution using logic-based explanations, specifically targeting\nhigh-stakes uses of machine learning (ML) models. Typically, such works exploit\nweak abductive explanation (WAXp) as the characteristic function to assign\nimportance to features. However, one possible downside is that the contribution\nof non-WAXp sets is neglected. In fact, non-WAXp sets can also convey important\ninformation, because of the relationship between formal explanations (XPs) and\nadversarial examples (AExs). Accordingly, this paper leverages Shapley value\nand Banzhaf index to devise two novel feature importance scores. We take into\naccount non-WAXp sets when computing feature contribution, and the novel scores\nquantify how effective each feature is at excluding AExs. Furthermore, the\npaper identifies properties and studies the computational complexity of the\nproposed scores.", "AI": {"tldr": "\u672c\u6587\u5229\u7528 Shapley \u503c\u548c Banzhaf \u6307\u6570\uff0c\u8003\u8651\u975e WAXp \u96c6\u5408\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u5206\uff0c\u4ee5\u91cf\u5316\u7279\u5f81\u5728\u6392\u9664\u5bf9\u6297\u6837\u672c\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u7814\u7a76\u4e86\u8fd9\u4e9b\u8bc4\u5206\u7684\u5c5e\u6027\u548c\u8ba1\u7b97\u590d\u6742\u5ea6", "motivation": "\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u7279\u5f81\u5c5e\u6027\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd (XAI) \u9886\u57df\u65e0\u5904\u4e0d\u5728\u3002 \u7136\u800c\uff0c\u975eWAXp\u96c6\u5408\u7684\u8d21\u732e\u88ab\u5ffd\u7565\u3002\u4e8b\u5b9e\u4e0a\uff0c\u7531\u4e8e\u5f62\u5f0f\u89e3\u91ca (XPs) \u548c\u5bf9\u6297\u6027\u793a\u4f8b (AExs) \u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u975eWAXp\u96c6\u5408\u4e5f\u53ef\u4ee5\u4f20\u8fbe\u91cd\u8981\u7684\u4fe1\u606f", "method": "\u5229\u7528 Shapley \u503c\u548c Banzhaf \u6307\u6570\u8bbe\u8ba1\u4e86\u4e24\u79cd\u65b0\u7684\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u5206", "result": "\u65b0\u63d0\u51fa\u7684\u5206\u6570\u53ef\u4ee5\u91cf\u5316\u6bcf\u4e2a\u7279\u5f81\u5728\u6392\u9664 AExs \u65b9\u9762\u7684\u6709\u6548\u6027", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u5206\uff0c\u5b83\u4eec\u5728\u8ba1\u7b97\u7279\u5f81\u8d21\u732e\u65f6\u8003\u8651\u4e86\u975eWAXp\u96c6\u5408\uff0c\u5e76\u91cf\u5316\u4e86\u6bcf\u4e2a\u7279\u5f81\u5728\u6392\u9664AExs\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u672c\u6587\u8fd8\u786e\u5b9a\u4e86\u6240\u63d0\u51fa\u7684\u5206\u6570\u7684\u5c5e\u6027\u5e76\u7814\u7a76\u4e86\u8ba1\u7b97\u590d\u6742\u6027\u3002"}}
{"id": "2508.12645", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.12645", "abs": "https://arxiv.org/abs/2508.12645", "authors": ["Hongyang Liu", "Zhu Sun", "Tianjun Wei", "Yan Wang", "Jiajie Zhu", "Xinghua Qu"], "title": "Diagnostic-Guided Dynamic Profile Optimization for LLM-based User Simulators in Sequential Recommendation", "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled realistic user\nsimulators for developing and evaluating recommender systems (RSs). However,\nexisting LLM-based simulators for RSs face two major limitations: (1) static\nand single-step prompt-based inference that leads to inaccurate and incomplete\nuser profile construction; (2) unrealistic and single-round\nrecommendation-feedback interaction pattern that fails to capture real-world\nscenarios. To address these limitations, we propose DGDPO (Diagnostic-Guided\nDynamic Profile Optimization), a novel framework that constructs user profile\nthrough a dynamic and iterative optimization process to enhance the simulation\nfidelity. Specifically, DGDPO incorporates two core modules within each\noptimization loop: firstly, a specialized LLM-based diagnostic module,\ncalibrated through our novel training strategy, accurately identifies specific\ndefects in the user profile. Subsequently, a generalized LLM-based treatment\nmodule analyzes the diagnosed defect and generates targeted suggestions to\nrefine the profile. Furthermore, unlike existing LLM-based user simulators that\nare limited to single-round interactions, we are the first to integrate DGDPO\nwith sequential recommenders, enabling a bidirectional evolution where user\nprofiles and recommendation strategies adapt to each other over multi-round\ninteractions. Extensive experiments conducted on three real-world datasets\ndemonstrate the effectiveness of our proposed framework.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8eLLM\u7684\u6846\u67b6DGDPO\uff0c\u7528\u4e8e\u6784\u5efa\u66f4\u51c6\u786e\u548c\u5b8c\u6574\u7684\u7528\u6237\u753b\u50cf\uff0c\u5e76\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u63d0\u9ad8\u63a8\u8350\u7cfb\u7edf\u4eff\u771f\u7684\u771f\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eLLM\u7684\u63a8\u8350\u7cfb\u7edf\u6a21\u62df\u5668\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a(1) \u9759\u6001\u548c\u5355\u6b65\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u63a8\u7406\uff0c\u5bfc\u81f4\u4e0d\u51c6\u786e\u548c\u4e0d\u5b8c\u6574\u7684\u7528\u6237\u753b\u50cf\u6784\u5efa\uff1b(2) \u4e0d\u5207\u5b9e\u9645\u7684\u5355\u8f6e\u63a8\u8350-\u53cd\u9988\u4ea4\u4e92\u6a21\u5f0f\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u7684\u573a\u666f\u3002", "method": "DGDPO\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u4e00\u4e2a\u4e13\u95e8\u7684\u57fa\u4e8eLLM\u7684\u8bca\u65ad\u6a21\u5757\uff0c\u7528\u4e8e\u51c6\u786e\u8bc6\u522b\u7528\u6237\u753b\u50cf\u4e2d\u7684\u7279\u5b9a\u7f3a\u9677\uff1b\u4ee5\u53ca\u4e00\u4e2a\u901a\u7528\u7684\u57fa\u4e8eLLM\u7684\u6cbb\u7597\u6a21\u5757\uff0c\u7528\u4e8e\u5206\u6790\u8bca\u65ad\u51fa\u7684\u7f3a\u9677\u5e76\u751f\u6210\u6709\u9488\u5bf9\u6027\u7684\u5efa\u8bae\u4ee5\u5b8c\u5584\u753b\u50cf\u3002", "result": "DGDPO\u6846\u67b6\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u5176\u6709\u6548\u6027\u3002\u8be5\u6846\u67b6\u8fd8\u9996\u6b21\u5c06DGDPO\u4e0e\u987a\u5e8f\u63a8\u8350\u5668\u96c6\u6210\uff0c\u5b9e\u73b0\u4e86\u7528\u6237\u753b\u50cf\u548c\u63a8\u8350\u7b56\u7565\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7684\u53cc\u5411\u6f14\u5316\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDGDPO\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b\u6784\u5efa\u7528\u6237\u753b\u50cf\uff0c\u4ee5\u63d0\u9ad8\u4eff\u771f\u4fdd\u771f\u5ea6\u3002\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.11732", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11732", "abs": "https://arxiv.org/abs/2508.11732", "authors": ["Xiangxiang Cui", "Min Zhao", "Dongmei Zhi", "Shile Qi", "Vince D Calhoun", "Jing Sui"], "title": "BRIEF: BRain-Inspired network connection search with Extensive temporal feature Fusion enhances disease classification", "comment": null, "summary": "Existing deep learning models for functional MRI-based classification have\nlimitations in network architecture determination (relying on experience) and\nfeature space fusion (mostly simple concatenation, lacking mutual learning).\nInspired by the human brain's mechanism of updating neural connections through\nlearning and decision-making, we proposed a novel BRain-Inspired feature Fusion\n(BRIEF) framework, which is able to optimize network architecture automatically\nby incorporating an improved neural network connection search (NCS) strategy\nand a Transformer-based multi-feature fusion module. Specifically, we first\nextracted 4 types of fMRI temporal representations, i.e., time series (TCs),\nstatic/dynamic functional connection (FNC/dFNC), and multi-scale dispersion\nentropy (MsDE), to construct four encoders. Within each encoder, we employed a\nmodified Q-learning to dynamically optimize the NCS to extract high-level\nfeature vectors, where the NCS is formulated as a Markov Decision Process.\nThen, all feature vectors were fused via a Transformer, leveraging both\nstable/time-varying connections and multi-scale dependencies across different\nbrain regions to achieve the final classification. Additionally, an attention\nmodule was embedded to improve interpretability. The classification performance\nof our proposed BRIEF was compared with 21 state-of-the-art models by\ndiscriminating two mental disorders from healthy controls: schizophrenia (SZ,\nn=1100) and autism spectrum disorder (ASD, n=1550). BRIEF demonstrated\nsignificant improvements of 2.2% to 12.1% compared to 21 algorithms, reaching\nan AUC of 91.5% - 0.6% for SZ and 78.4% - 0.5% for ASD, respectively. This is\nthe first attempt to incorporate a brain-inspired, reinforcement learning\nstrategy to optimize fMRI-based mental disorder classification, showing\nsignificant potential for identifying precise neuroimaging biomarkers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53d7\u5927\u8111\u542f\u53d1\u7684\u7279\u5f81\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u7f51\u7edc\u67b6\u6784\u548c\u5229\u7528Transformer\u8fdb\u884c\u591a\u7279\u5f81\u878d\u5408\uff0c\u63d0\u9ad8\u4e86fMRI\u7684\u7cbe\u795e\u75be\u75c5\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8efMRI\u7684\u5206\u7c7b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u7f51\u7edc\u67b6\u6784\u786e\u5b9a\uff08\u4f9d\u8d56\u7ecf\u9a8c\uff09\u548c\u7279\u5f81\u7a7a\u95f4\u878d\u5408\uff08\u4e3b\u8981\u662f\u7b80\u5355\u7684\u8fde\u63a5\uff0c\u7f3a\u4e4f\u76f8\u4e92\u5b66\u4e60\uff09\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684BRain-Inspired\u7279\u5f81\u878d\u5408\uff08BRIEF\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u901a\u8fc7\u7ed3\u5408\u6539\u8fdb\u7684\u795e\u7ecf\u7f51\u7edc\u8fde\u63a5\u641c\u7d22\uff08NCS\uff09\u7b56\u7565\u548c\u57fa\u4e8eTransformer\u7684\u591a\u7279\u5f81\u878d\u5408\u6a21\u5757\u6765\u81ea\u52a8\u4f18\u5316\u7f51\u7edc\u67b6\u6784\u3002", "result": "BRIEF\u76f8\u6bd421\u79cd\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u6027\u80fd\u663e\u8457\u63d0\u9ad8\u4e862.2%\u523012.1%\u3002", "conclusion": "BRIEF\u5728\u533a\u5206\u7cbe\u795e\u5206\u88c2\u75c7\uff08SZ\uff09\u548c\u81ea\u95ed\u75c7\u8c31\u7cfb\u969c\u788d\uff08ASD\uff09\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u5206\u522b\u8fbe\u523091.5%\u548c78.4%\u7684AUC\u3002"}}
{"id": "2508.11829", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.11829", "abs": "https://arxiv.org/abs/2508.11829", "authors": ["Leigh Levinson", "Christopher J. Agostino"], "title": "Every 28 Days the AI Dreams of Soft Skin and Burning Stars: Scaffolding AI Agents with Hormones and Emotions", "comment": "9 pages, 1 figure, submitted to NeurIPS Creative AI track", "summary": "Despite significant advances, AI systems struggle with the frame problem:\ndetermining what information is contextually relevant from an exponentially\nlarge possibility space. We hypothesize that biological rhythms, particularly\nhormonal cycles, serve as natural relevance filters that could address this\nfundamental challenge. We develop a framework that embeds simulated menstrual\nand circadian cycles into Large Language Models through system prompts\ngenerated from periodic functions modeling key hormones including estrogen,\ntestosterone, and cortisol. Across multiple state-of-the-art models, linguistic\nanalysis reveals emotional and stylistic variations that track biological\nphases; sadness peaks during menstruation while happiness dominates ovulation\nand circadian patterns show morning optimism transitioning to nocturnal\nintrospection. Benchmarking on SQuAD, MMLU, Hellaswag, and AI2-ARC demonstrates\nsubtle but consistent performance variations aligning with biological\nexpectations, including optimal function in moderate rather than extreme\nhormonal ranges. This methodology provides a novel approach to contextual AI\nwhile revealing how societal biases regarding gender and biology are embedded\nwithin language models.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u5c06\u6a21\u62df\u7684\u6fc0\u7d20\u5468\u671f\u5d4c\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u4f1a\u5f71\u54cd\u5176\u60c5\u7eea\u3001\u98ce\u683c\u548c\u6027\u80fd\uff0c\u8fd9\u8868\u660e\u751f\u7269\u8282\u5f8b\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u81ea\u7136\u5173\u8054\u6027\u8fc7\u6ee4\u5668\u3002", "motivation": "AI\u7cfb\u7edf\u96be\u4ee5\u786e\u5b9a\u4e0a\u4e0b\u6587\u4e2d\u76f8\u5173\u7684\u4fe1\u606f\uff0c\u672c\u7814\u7a76\u5047\u8bbe\u751f\u7269\u8282\u5f8b\u53ef\u4ee5\u4f5c\u4e3a\u81ea\u7136\u7684\u5173\u8054\u6027\u8fc7\u6ee4\u5668\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u6708\u7ecf\u5468\u671f\u548c\u663c\u591c\u8282\u5f8b\uff0c\u5c06\u6fc0\u7d20\u5468\u671f\u5d4c\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u8fd9\u4e9b\u6fc0\u7d20\u5305\u62ec\u96cc\u6fc0\u7d20\u3001\u777e\u916e\u548c\u76ae\u8d28\u9187\u3002", "result": "\u8bed\u8a00\u5206\u6790\u63ed\u793a\u4e86\u60c5\u7eea\u548c\u98ce\u683c\u7684\u53d8\u5316\uff0c\u8fd9\u4e9b\u53d8\u5316\u4e0e\u751f\u7269\u5b66\u9636\u6bb5\u76f8\u5173\u3002\u5728SQuAD\u3001MMLU\u3001Hellaswag\u548cAI2-ARC\u4e0a\u7684\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u6027\u80fd\u53d8\u5316\u4e0e\u751f\u7269\u5b66\u9884\u671f\u76f8\u7b26\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u751f\u7269\u8282\u5f8b\u9636\u6bb5\u8868\u73b0\u51fa\u7ec6\u5fae\u4f46\u4e00\u81f4\u7684\u6027\u80fd\u53d8\u5316\uff0c\u5305\u62ec\u5728\u9002\u5ea6\u800c\u975e\u6781\u7aef\u6fc0\u7d20\u8303\u56f4\u5185\u7684\u6700\u4f73\u529f\u80fd\u3002"}}
{"id": "2508.11825", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11825", "abs": "https://arxiv.org/abs/2508.11825", "authors": ["Sherlon Almeida da Silva", "Davi Geiger", "Luiz Velho", "Moacir Antonelli Ponti"], "title": "Towards Understanding 3D Vision: the Role of Gaussian Curvature", "comment": null, "summary": "Recent advances in computer vision have predominantly relied on data-driven\napproaches that leverage deep learning and large-scale datasets. Deep neural\nnetworks have achieved remarkable success in tasks such as stereo matching and\nmonocular depth reconstruction. However, these methods lack explicit models of\n3D geometry that can be directly analyzed, transferred across modalities, or\nsystematically modified for controlled experimentation. We investigate the role\nof Gaussian curvature in 3D surface modeling. Besides Gaussian curvature being\nan invariant quantity under change of observers or coordinate systems, we\ndemonstrate using the Middlebury stereo dataset that it offers: (i) a sparse\nand compact description of 3D surfaces, (ii) state-of-the-art monocular and\nstereo methods seem to implicitly consider it, but no explicit module of such\nuse can be extracted, (iii) a form of geometric prior that can inform and\nimprove 3D surface reconstruction, and (iv) a possible use as an unsupervised\nmetric for stereo methods.", "AI": {"tldr": "This paper investigates the role of Gaussian curvature in 3D surface modeling and shows its benefits.", "motivation": "Deep learning methods for computer vision lack explicit models of 3D geometry that can be directly analyzed, transferred across modalities, or systematically modified for controlled experimentation.", "method": "investigate the role of Gaussian curvature in 3D surface modeling", "result": "demonstrate using the Middlebury stereo dataset", "conclusion": "Gaussian curvature offers a sparse and compact description of 3D surfaces, state-of-the-art monocular and stereo methods seem to implicitly consider it, a form of geometric prior that can inform and improve 3D surface reconstruction, and a possible use as an unsupervised metric for stereo methods."}}
{"id": "2508.11975", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11975", "abs": "https://arxiv.org/abs/2508.11975", "authors": ["Gongyao Jiang", "Qiong Luo"], "title": "Chart-CoCa: Self-Improving Chart Understanding of Vision LMs via Code-Driven Synthesis and Candidate-Conditioned Answering", "comment": "Accepted to CIKM 2025", "summary": "Vision Language Models (VLMs) often struggle with chart understanding tasks,\nparticularly in accurate chart description and complex reasoning. Synthetic\ndata generation is a promising solution, while usually facing the challenge of\nnoise labels. To address this challenge, we first introduce a chart synthesis\npipeline that generates aligned chart-question-answer triplets through code\ngeneration and execution, ensuring the reliability of synthetic data without\nhuman intervention. Furthermore, inspired by test-time scaling that increases\ninference budget and thereby improves performance, we design a\ncandidate-conditioned answering process. The VLM first generates multiple\nresponses per query, and then synthesizes the final answer by contextualizing\nthese candidates. Experiments demonstrate significant improvements, with up to\n15.50 points accuracy gain over the initial VLM, in a fully self-improving\nparadigm without either human-labeled data or external models.", "AI": {"tldr": "This paper introduces a new chart synthesis pipeline to generate reliable synthetic data and a candidate-conditioned answering process to improve the performance of VLMs on chart understanding tasks.", "motivation": "Vision Language Models (VLMs) often struggle with chart understanding tasks, particularly in accurate chart description and complex reasoning. Synthetic data generation is a promising solution, while usually facing the challenge of noise labels.", "method": "introduce a chart synthesis pipeline that generates aligned chart-question-answer triplets through code generation and execution, ensuring the reliability of synthetic data without human intervention. Furthermore, inspired by test-time scaling that increases inference budget and thereby improves performance, we design a candidate-conditioned answering process. The VLM first generates multiple responses per query, and then synthesizes the final answer by contextualizing these candidates.", "result": "significant improvements, with up to 15.50 points accuracy gain over the initial VLM", "conclusion": "Experiments demonstrate significant improvements, with up to 15.50 points accuracy gain over the initial VLM, in a fully self-improving paradigm without either human-labeled data or external models."}}
{"id": "2508.12665", "categories": ["cs.IR", "H.3.3"], "pdf": "https://arxiv.org/pdf/2508.12665", "abs": "https://arxiv.org/abs/2508.12665", "authors": ["Xu Zhao", "Ruibo Ma", "Jiaqi Chen", "Weiqi Zhao", "Ping Yang", "Yao Hu"], "title": "Multi-Granularity Distribution Modeling for Video Watch Time Prediction via Exponential-Gaussian Mixture Network", "comment": "Accepted as oral full paper by RecSys'2025 conference", "summary": "Accurate watch time prediction is crucial for enhancing user engagement in\nstreaming short-video platforms, although it is challenged by complex\ndistribution characteristics across multi-granularity levels. Through\nsystematic analysis of real-world industrial data, we uncover two critical\nchallenges in watch time prediction from a distribution aspect: (1)\ncoarse-grained skewness induced by a significant concentration of quick-skips1,\n(2) fine-grained diversity arising from various user-video interaction\npatterns. Consequently, we assume that the watch time follows the\nExponential-Gaussian Mixture (EGM) distribution, where the exponential and\nGaussian components respectively characterize the skewness and diversity.\nAccordingly, an Exponential-Gaussian Mixture Network (EGMN) is proposed for the\nparameterization of EGM distribution, which consists of two key modules: a\nhidden representation encoder and a mixture parameter generator. We conducted\nextensive offline experiments on public datasets and online A/B tests on the\nindustrial short-video feeding scenario of Xiaohongshu App to validate the\nsuperiority of EGMN compared with existing state-of-the-art methods.\nRemarkably, comprehensive experimental results have proven that EGMN exhibits\nexcellent distribution fitting ability across coarse-to-fine-grained levels. We\nopen source related code on Github: https://github.com/BestActionNow/EGMN.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u89c2\u770b\u65f6\u957f\u9884\u6d4b\u7684\u6307\u6570-\u9ad8\u65af\u6df7\u5408\u7f51\u7edc (EGMN)\uff0c\u5e76\u5728\u5c0f\u7ea2\u4e66App\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7cbe\u786e\u7684\u89c2\u770b\u65f6\u957f\u9884\u6d4b\u5bf9\u4e8e\u589e\u5f3a\u6d41\u5a92\u4f53\u77ed\u89c6\u9891\u5e73\u53f0\u4e2d\u7684\u7528\u6237\u53c2\u4e0e\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u5c3d\u7ba1\u5b83\u53d7\u5230\u8de8\u591a\u7c92\u5ea6\u7ea7\u522b\u7684\u590d\u6742\u5206\u5e03\u7279\u5f81\u7684\u6311\u6218\u3002\u901a\u8fc7\u5bf9\u771f\u5b9e\u5de5\u4e1a\u6570\u636e\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u6211\u4eec\u4ece\u5206\u5e03\u7684\u89d2\u5ea6\u63ed\u793a\u4e86\u89c2\u770b\u65f6\u957f\u9884\u6d4b\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a(1) \u7531\u5feb\u901f\u8df3\u8fc7\u7684\u5927\u91cf\u96c6\u4e2d\u5f15\u8d77\u7684\u7c97\u7c92\u5ea6\u504f\u5ea6\uff0c(2) \u7531\u5404\u79cd\u7528\u6237-\u89c6\u9891\u4e92\u52a8\u6a21\u5f0f\u5f15\u8d77\u7684\u7ec6\u7c92\u5ea6\u591a\u6837\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6307\u6570-\u9ad8\u65af\u6df7\u5408\u7f51\u7edc (EGMN)\uff0c\u7528\u4e8e EGM \u5206\u5e03\u7684\u53c2\u6570\u5316\uff0c\u8be5\u7f51\u7edc\u7531\u4e24\u4e2a\u5173\u952e\u6a21\u5757\u7ec4\u6210\uff1a\u9690\u85cf\u8868\u793a\u7f16\u7801\u5668\u548c\u6df7\u5408\u53c2\u6570\u751f\u6210\u5668\u3002", "result": "\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u79bb\u7ebf\u5b9e\u9a8c\uff0c\u5e76\u5728\u5c0f\u7ea2\u4e66App\u7684\u5de5\u4e1a\u77ed\u89c6\u9891feed\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u5728\u7ebfA/B\u6d4b\u8bd5\uff0c\u4ee5\u9a8c\u8bc1EGMN\u76f8\u5bf9\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002\u5168\u9762\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\uff0cEGMN \u5177\u6709\u51fa\u8272\u7684\u8de8\u7c97\u5230\u7ec6\u7c92\u5ea6\u7ea7\u522b\u7684\u5206\u5e03\u62df\u5408\u80fd\u529b\u3002", "conclusion": "EGMN\u5728\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u7ea7\u522b\u4e0a\u90fd\u8868\u73b0\u51fa\u51fa\u8272\u7684\u5206\u5e03\u62df\u5408\u80fd\u529b\u3002"}}
{"id": "2508.11739", "categories": ["cs.LG", "cs.CV", "I.4.6; I.5.5"], "pdf": "https://arxiv.org/pdf/2508.11739", "abs": "https://arxiv.org/abs/2508.11739", "authors": ["Luc Houriez", "Sebastian Pilarski", "Behzad Vahedi", "Ali Ahmadalipour", "Teo Honda Scully", "Nicholas Aflitto", "David Andre", "Caroline Jaffe", "Martha Wedner", "Rich Mazzola", "Josh Jeffery", "Ben Messinger", "Sage McGinley-Smith", "Sarah Russell"], "title": "Scalable Geospatial Data Generation Using AlphaEarth Foundations Model", "comment": "15 pages, 10 figures, 5 tables", "summary": "High-quality labeled geospatial datasets are essential for extracting\ninsights and understanding our planet. Unfortunately, these datasets often do\nnot span the entire globe and are limited to certain geographic regions where\ndata was collected. Google DeepMind's recently released AlphaEarth Foundations\n(AEF) provides an information-dense global geospatial representation designed\nto serve as a useful input across a wide gamut of tasks. In this article we\npropose and evaluate a methodology which leverages AEF to extend geospatial\nlabeled datasets beyond their initial geographic regions. We show that even\nbasic models like random forests or logistic regression can be used to\naccomplish this task. We investigate a case study of extending LANDFIRE's\nExisting Vegetation Type (EVT) dataset beyond the USA into Canada at two levels\nof granularity: EvtPhys (13 classes) and EvtGp (80 classes). Qualitatively, for\nEvtPhys, model predictions align with ground truth. Trained models achieve 81%\nand 73% classification accuracy on EvtPhys validation sets in the USA and\nCanada, despite discussed limitations.", "AI": {"tldr": "AEF can extend geospatial labeled datasets beyond their initial geographic regions, even basic models can be used and achieve 81% and 73% classification accuracy on EvtPhys validation sets in the USA and Canada", "motivation": "High-quality labeled geospatial datasets are essential for extracting insights and understanding our planet. Unfortunately, these datasets often do not span the entire globe and are limited to certain geographic regions where data was collected. Google DeepMind's recently released AlphaEarth Foundations (AEF) provides an information-dense global geospatial representation designed to serve as a useful input across a wide gamut of tasks.", "method": "leverages AEF to extend geospatial labeled datasets beyond their initial geographic regions, basic models like random forests or logistic regression can be used", "result": "model predictions align with ground truth. Trained models achieve 81% and 73% classification accuracy on EvtPhys validation sets in the USA and Canada", "conclusion": "Trained models achieve 81% and 73% classification accuracy on EvtPhys validation sets in the USA and Canada"}}
{"id": "2508.11831", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11831", "abs": "https://arxiv.org/abs/2508.11831", "authors": ["Julia Sammartino", "Libby Barak", "Jing Peng", "Anna Feldman"], "title": "When Does Language Transfer Help? Sequential Fine-Tuning for Cross-Lingual Euphemism Detection", "comment": "RANLP 2025", "summary": "Euphemisms are culturally variable and often ambiguous, posing challenges for\nlanguage models, especially in low-resource settings. This paper investigates\nhow cross-lingual transfer via sequential fine-tuning affects euphemism\ndetection across five languages: English, Spanish, Chinese, Turkish, and\nYoruba. We compare sequential fine-tuning with monolingual and simultaneous\nfine-tuning using XLM-R and mBERT, analyzing how performance is shaped by\nlanguage pairings, typological features, and pretraining coverage. Results show\nthat sequential fine-tuning with a high-resource L1 improves L2 performance,\nespecially for low-resource languages like Yoruba and Turkish. XLM-R achieves\nlarger gains but is more sensitive to pretraining gaps and catastrophic\nforgetting, while mBERT yields more stable, though lower, results. These\nfindings highlight sequential fine-tuning as a simple yet effective strategy\nfor improving euphemism detection in multilingual models, particularly when\nlow-resource languages are involved.", "AI": {"tldr": "Sequential fine-tuning helps multilingual models detect euphemisms, particularly in low-resource languages.", "motivation": "Euphemisms are culturally variable and ambiguous, posing challenges for language models, especially in low-resource settings.", "method": "Cross-lingual transfer via sequential fine-tuning with XLM-R and mBERT.", "result": "Sequential fine-tuning with a high-resource L1 improves L2 performance, especially for low-resource languages. XLM-R achieves larger gains but is more sensitive to pretraining gaps, while mBERT yields more stable, though lower, results.", "conclusion": "Sequential fine-tuning improves euphemism detection in multilingual models, especially for low-resource languages."}}
{"id": "2508.11826", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11826", "abs": "https://arxiv.org/abs/2508.11826", "authors": ["Dehn Xu", "Tim Katzke", "Emmanuel M\u00fcller"], "title": "From Pixels to Graphs: Deep Graph-Level Anomaly Detection on Dermoscopic Images", "comment": null, "summary": "Graph Neural Networks (GNNs) have emerged as a powerful approach for\ngraph-based machine learning tasks. Previous work applied GNNs to image-derived\ngraph representations for various downstream tasks such as classification or\nanomaly detection. These transformations include segmenting images, extracting\nfeatures from segments, mapping them to nodes, and connecting them. However, to\nthe best of our knowledge, no study has rigorously compared the effectiveness\nof the numerous potential image-to-graph transformation approaches for\nGNN-based graph-level anomaly detection (GLAD). In this study, we\nsystematically evaluate the efficacy of multiple segmentation schemes, edge\nconstruction strategies, and node feature sets based on color, texture, and\nshape descriptors to produce suitable image-derived graph representations to\nperform graph-level anomaly detection. We conduct extensive experiments on\ndermoscopic images using state-of-the-art GLAD models, examining performance\nand efficiency in purely unsupervised, weakly supervised, and fully supervised\nregimes. Our findings reveal, for example, that color descriptors contribute\nthe best standalone performance, while incorporating shape and texture features\nconsistently enhances detection efficacy. In particular, our best unsupervised\nconfiguration using OCGTL achieves a competitive AUC-ROC score of up to 0.805\nwithout relying on pretrained backbones like comparable image-based approaches.\nWith the inclusion of sparse labels, the performance increases substantially to\n0.872 and with full supervision to 0.914 AUC-ROC.", "AI": {"tldr": "This study systematically evaluates image-to-graph transformation approaches for GNN-based graph-level anomaly detection, finding that color descriptors are important and shape/texture features improve efficacy. Achieves strong performance on dermoscopic images, especially with supervision.", "motivation": "no study has rigorously compared the effectiveness of the numerous potential image-to-graph transformation approaches for GNN-based graph-level anomaly detection (GLAD).", "method": "systematically evaluate the efficacy of multiple segmentation schemes, edge construction strategies, and node feature sets based on color, texture, and shape descriptors to produce suitable image-derived graph representations to perform graph-level anomaly detection. We conduct extensive experiments on dermoscopic images using state-of-the-art GLAD models, examining performance and efficiency in purely unsupervised, weakly supervised, and fully supervised regimes.", "result": "color descriptors contribute the best standalone performance, while incorporating shape and texture features consistently enhances detection efficacy. The best unsupervised configuration using OCGTL achieves a competitive AUC-ROC score of up to 0.805 without relying on pretrained backbones. With the inclusion of sparse labels, the performance increases substantially to 0.872 and with full supervision to 0.914 AUC-ROC.", "conclusion": "Color descriptors contribute the best standalone performance, while incorporating shape and texture features consistently enhances detection efficacy. The best unsupervised configuration using OCGTL achieves a competitive AUC-ROC score of up to 0.805 without relying on pretrained backbones. With the inclusion of sparse labels, the performance increases substantially to 0.872 and with full supervision to 0.914 AUC-ROC."}}
{"id": "2508.11987", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11987", "abs": "https://arxiv.org/abs/2508.11987", "authors": ["Zhiyuan Zeng", "Jiashuo Liu", "Siyuan Chen", "Tianci He", "Yali Liao", "Jinpeng Wang", "Zaiyuan Wang", "Yang Yang", "Lingyue Yin", "Mingren Yin", "Zhenwei Zhu", "Tianle Cai", "Zehui Chen", "Jiecao Chen", "Yantao Du", "Xiang Gao", "Jiacheng Guo", "Liang Hu", "Jianpeng Jiao", "Xiangsheng Li", "Jingkai Liu", "Shuang Ni", "Zhoufutu Wen", "Ge Zhang", "Kaiyuan Zhang", "Xin Zhou", "Jose Blanchet", "Xipeng Qiu", "Mengdi Wang", "Wenhao Huang"], "title": "FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction", "comment": "Technical report, 51 pages", "summary": "Future prediction is a complex task for LLM agents, requiring a high level of\nanalytical thinking, information gathering, contextual understanding, and\ndecision-making under uncertainty. Agents must not only gather and interpret\nvast amounts of dynamic information but also integrate diverse data sources,\nweigh uncertainties, and adapt predictions based on emerging trends, just as\nhuman experts do in fields like politics, economics, and finance. Despite its\nimportance, no large-scale benchmark exists for evaluating agents on future\nprediction, largely due to challenges in handling real-time updates and\nretrieving timely, accurate answers. To address this, we introduce\n$\\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically\ndesigned for LLM agents performing future prediction tasks. FutureX is the\nlargest and most diverse live benchmark for future prediction, supporting\nreal-time daily updates and eliminating data contamination through an automated\npipeline for question gathering and answer collection. We evaluate 25 LLM/agent\nmodels, including those with reasoning, search capabilities, and integration of\nexternal tools such as the open-source Deep Research Agent and closed-source\nDeep Research models. This comprehensive evaluation assesses agents' adaptive\nreasoning and performance in dynamic environments. Additionally, we provide\nin-depth analyses of agents' failure modes and performance pitfalls in\nfuture-oriented tasks, including the vulnerability to fake web pages and the\ntemporal validity. Our goal is to establish a dynamic, contamination-free\nevaluation standard that drives the development of LLM agents capable of\nperforming at the level of professional human analysts in complex reasoning and\npredictive thinking.", "AI": {"tldr": "Introduces FutureX, a live benchmark for evaluating LLM agents on future prediction.", "motivation": "The lack of a large-scale benchmark for evaluating LLM agents on future prediction.", "method": "A dynamic and live evaluation benchmark called FutureX.", "result": "Evaluation of 25 LLM/agent models, in-depth analyses of agents' failure modes and performance pitfalls.", "conclusion": "This paper introduces FutureX, a dynamic benchmark for evaluating LLM agents on future prediction, and evaluates 25 models, analyzing failure modes and performance pitfalls."}}
{"id": "2508.12706", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12706", "abs": "https://arxiv.org/abs/2508.12706", "authors": ["Yongchun Zhu", "Guanyu Jiang", "Jingwu Chen", "Feng Zhang", "Xiao Yang", "Zuotao Liu"], "title": "Asymmetric Diffusion Recommendation Model", "comment": "Accepted by CIKM2025", "summary": "Recently, motivated by the outstanding achievements of diffusion models, the\ndiffusion process has been employed to strengthen representation learning in\nrecommendation systems. Most diffusion-based recommendation models typically\nutilize standard Gaussian noise in symmetric forward and reverse processes in\ncontinuous data space. Nevertheless, the samples derived from recommendation\nsystems inhabit a discrete data space, which is fundamentally different from\nthe continuous one. Moreover, Gaussian noise has the potential to corrupt\npersonalized information within latent representations. In this work, we\npropose a novel and effective method, named Asymmetric Diffusion Recommendation\nModel (AsymDiffRec), which learns forward and reverse processes in an\nasymmetric manner. We define a generalized forward process that simulates the\nmissing features in real-world recommendation samples. The reverse process is\nthen performed in an asymmetric latent feature space. To preserve personalized\ninformation within the latent representation, a task-oriented optimization\nstrategy is introduced. In the serving stage, the raw sample with missing\nfeatures is regarded as a noisy input to generate a denoising and robust\nrepresentation for the final prediction. By equipping base models with\nAsymDiffRec, we conduct online A/B tests, achieving improvements of +0.131% and\n+0.166% in terms of users' active days and app usage duration respectively.\nAdditionally, the extended offline experiments also demonstrate improvements.\nAsymDiffRec has been implemented in the Douyin Music App.", "AI": {"tldr": "AsymDiffRec improves recommendation models using asymmetric diffusion in discrete data space, showing gains in user activity and app usage in Douyin Music App.", "motivation": "Existing diffusion-based recommendation models use standard Gaussian noise in continuous data space, which is not suitable for the discrete data space of recommendation systems and can corrupt personalized information.", "method": "The paper proposes an Asymmetric Diffusion Recommendation Model (AsymDiffRec) with a generalized forward process simulating missing features and a reverse process in an asymmetric latent feature space. A task-oriented optimization strategy is used to preserve personalized information.", "result": "Online A/B tests show improvements of +0.131% in users' active days and +0.166% in app usage duration. Offline experiments also demonstrate improvements.", "conclusion": "The paper introduces AsymDiffRec, which improves recommendation models by using asymmetric diffusion processes in discrete data space. Online A/B tests show improvements in user activity and app usage duration. AsymDiffRec has been implemented in the Douyin Music App."}}
{"id": "2508.11794", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11794", "abs": "https://arxiv.org/abs/2508.11794", "authors": ["Hemanth Macharla", "Mayukha Pal"], "title": "Fed-Meta-Align: A Similarity-Aware Aggregation and Personalization Pipeline for Federated TinyML on Heterogeneous Data", "comment": null, "summary": "Real-time fault classification in resource-constrained Internet of Things\n(IoT) devices is critical for industrial safety, yet training robust models in\nsuch heterogeneous environments remains a significant challenge. Standard\nFederated Learning (FL) often fails in the presence of non-IID data, leading to\nmodel divergence. This paper introduces Fed-Meta-Align, a novel four-phase\nframework designed to overcome these limitations through a sophisticated\ninitialization and training pipeline. Our process begins by training a\nfoundational model on a general public dataset to establish a competent\nstarting point. This model then undergoes a serial meta-initialization phase,\nwhere it sequentially trains on a subset of IOT Device data to learn a\nheterogeneity-aware initialization that is already situated in a favorable\nregion of the loss landscape. This informed model is subsequently refined in a\nparallel FL phase, which utilizes a dual-criterion aggregation mechanism that\nweights for IOT devices updates based on both local performance and cosine\nsimilarity alignment. Finally, an on-device personalization phase adapts the\nconverged global model into a specialized expert for each IOT Device.\nComprehensive experiments demonstrate that Fed-Meta-Align achieves an average\ntest accuracy of 91.27% across heterogeneous IOT devices, outperforming\npersonalized FedAvg and FedProx by up to 3.87% and 3.37% on electrical and\nmechanical fault datasets, respectively. This multi-stage approach of sequenced\ninitialization and adaptive aggregation provides a robust pathway for deploying\nhigh-performance intelligence on diverse TinyML networks.", "AI": {"tldr": "This paper introduces Fed-Meta-Align, a four-phase federated learning framework that overcomes the limitations of standard FL in heterogeneous IoT environments. It uses a sophisticated initialization and training pipeline and achieves higher accuracy than existing methods.", "motivation": "Real-time fault classification in resource-constrained Internet of Things (IoT) devices is critical for industrial safety, yet training robust models in such heterogeneous environments remains a significant challenge. Standard Federated Learning (FL) often fails in the presence of non-IID data, leading to model divergence.", "method": "a novel four-phase framework designed to overcome these limitations through a sophisticated initialization and training pipeline. Our process begins by training a foundational model on a general public dataset to establish a competent starting point. This model then undergoes a serial meta-initialization phase, where it sequentially trains on a subset of IOT Device data to learn a heterogeneity-aware initialization that is already situated in a favorable region of the loss landscape. This informed model is subsequently refined in a parallel FL phase, which utilizes a dual-criterion aggregation mechanism that weights for IOT devices updates based on both local performance and cosine similarity alignment. Finally, an on-device personalization phase adapts the converged global model into a specialized expert for each IOT Device.", "result": "achieves an average test accuracy of 91.27% across heterogeneous IOT devices, outperforming personalized FedAvg and FedProx by up to 3.87% and 3.37% on electrical and mechanical fault datasets, respectively", "conclusion": "Fed-Meta-Align achieves an average test accuracy of 91.27% across heterogeneous IOT devices, outperforming personalized FedAvg and FedProx by up to 3.87% and 3.37% on electrical and mechanical fault datasets, respectively. This multi-stage approach of sequenced initialization and adaptive aggregation provides a robust pathway for deploying high-performance intelligence on diverse TinyML networks."}}
{"id": "2508.11857", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11857", "abs": "https://arxiv.org/abs/2508.11857", "authors": ["Andrei-Valentin T\u0103nase", "Elena Pelican"], "title": "SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance", "comment": null, "summary": "Tokenization remains a fundamental yet underexplored bottleneck in natural\nlanguage processing, with strategies largely static despite remarkable progress\nin model architectures. We present SupraTok, a novel tokenization architecture\nthat reimagines subword segmentation through three innovations: cross-boundary\npattern learning that discovers multi-word semantic units, entropy-driven data\ncuration that optimizes training corpus quality, and multi-phase curriculum\nlearning for stable convergence. Our approach extends Byte-Pair Encoding by\nlearning \"superword\" tokens, coherent multi-word expressions that preserve\nsemantic unity while maximizing compression efficiency. SupraTok achieves 31%\nimprovement in English tokenization efficiency (5.91 versus 4.51 characters per\ntoken) compared to OpenAI's o200k tokenizer and 30% improvement over Google's\nGemma 3 tokenizer (256k vocabulary), while maintaining competitive performance\nacross 38 languages. When integrated with a GPT-2 scale model (124M parameters)\ntrained on 10 billion tokens from the FineWeb-Edu dataset, SupraTok yields 8.4%\nimprovement on HellaSWAG and 9.5% on MMLU benchmarks without architectural\nmodifications. While these results are promising at this scale, further\nvalidation at larger model scales is needed. These findings suggest that\nefficient tokenization can complement architectural innovations as a path to\nimproved language model performance.", "AI": {"tldr": "SupraTok, a novel tokenization architecture, improves tokenization efficiency and language model performance.", "motivation": "Tokenization remains a fundamental yet underexplored bottleneck in natural language processing, with strategies largely static despite remarkable progress in model architectures.", "method": "SupraTok, a novel tokenization architecture that reimagines subword segmentation through three innovations: cross-boundary pattern learning, entropy-driven data curation, and multi-phase curriculum learning.", "result": "SupraTok achieves 31% improvement in English tokenization efficiency compared to OpenAI's o200k tokenizer and 30% improvement over Google's Gemma 3 tokenizer, while maintaining competitive performance across 38 languages. When integrated with a GPT-2 scale model, SupraTok yields 8.4% improvement on HellaSWAG and 9.5% on MMLU benchmarks without architectural modifications.", "conclusion": "Efficient tokenization can complement architectural innovations as a path to improved language model performance."}}
{"id": "2508.11834", "categories": ["cs.CV", "cs.AI", "cs.RO", "cs.SY", "eess.IV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.11834", "abs": "https://arxiv.org/abs/2508.11834", "authors": ["Hamza Kheddar", "Yassine Habchi", "Mohamed Chahine Ghanem", "Mustapha Hemis", "Dusit Niyato"], "title": "Recent Advances in Transformer and Large Language Models for UAV Applications", "comment": null, "summary": "The rapid advancement of Transformer-based models has reshaped the landscape\nof uncrewed aerial vehicle (UAV) systems by enhancing perception,\ndecision-making, and autonomy. This review paper systematically categorizes and\nevaluates recent developments in Transformer architectures applied to UAVs,\nincluding attention mechanisms, CNN-Transformer hybrids, reinforcement learning\nTransformers, and large language models (LLMs). Unlike previous surveys, this\nwork presents a unified taxonomy of Transformer-based UAV models, highlights\nemerging applications such as precision agriculture and autonomous navigation,\nand provides comparative analyses through structured tables and performance\nbenchmarks. The paper also reviews key datasets, simulators, and evaluation\nmetrics used in the field. Furthermore, it identifies existing gaps in the\nliterature, outlines critical challenges in computational efficiency and\nreal-time deployment, and offers future research directions. This comprehensive\nsynthesis aims to guide researchers and practitioners in understanding and\nadvancing Transformer-driven UAV technologies.", "AI": {"tldr": "This review paper categorizes and evaluates recent developments in Transformer architectures applied to UAVs, highlights emerging applications, and provides comparative analyses.", "motivation": "The rapid advancement of Transformer-based models has reshaped the landscape of uncrewed aerial vehicle (UAV) systems by enhancing perception, decision-making, and autonomy.", "method": "This review paper systematically categorizes and evaluates recent developments in Transformer architectures applied to UAVs, including attention mechanisms, CNN-Transformer hybrids, reinforcement learning Transformers, and large language models (LLMs).", "result": "This work presents a unified taxonomy of Transformer-based UAV models, highlights emerging applications such as precision agriculture and autonomous navigation, and provides comparative analyses through structured tables and performance benchmarks. The paper also reviews key datasets, simulators, and evaluation metrics used in the field.", "conclusion": "This review paper aims to guide researchers and practitioners in understanding and advancing Transformer-driven UAV technologies."}}
{"id": "2508.11991", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11991", "abs": "https://arxiv.org/abs/2508.11991", "authors": ["Weihao Sun"], "title": "Modeling Relational Logic Circuits for And-Inverter Graph Convolutional Network", "comment": null, "summary": "The automation of logic circuit design enhances chip performance, energy\nefficiency, and reliability, and is widely applied in the field of Electronic\nDesign Automation (EDA).And-Inverter Graphs (AIGs) efficiently represent,\noptimize, and verify the functional characteristics of digital circuits,\nenhancing the efficiency of EDA development.Due to the complex structure and\nlarge scale of nodes in real-world AIGs, accurate modeling is challenging,\nleading to existing work lacking the ability to jointly model functional and\nstructural characteristics, as well as insufficient dynamic information\npropagation capability.To address the aforementioned challenges, we propose\nAIGer.Specifically, AIGer consists of two components: 1) Node logic feature\ninitialization embedding component and 2) AIGs feature learning network\ncomponent.The node logic feature initialization embedding component projects\nlogic nodes, such as AND and NOT, into independent semantic spaces, to enable\neffective node embedding for subsequent processing.Building upon this, the AIGs\nfeature learning network component employs a heterogeneous graph convolutional\nnetwork, designing dynamic relationship weight matrices and differentiated\ninformation aggregation approaches to better represent the original structure\nand information of AIGs.The combination of these two components enhances\nAIGer's ability to jointly model functional and structural characteristics and\nimproves its message passing capability. Experimental results indicate that\nAIGer outperforms the current best models in the Signal Probability Prediction\n(SSP) task, improving MAE and MSE by 18.95\\% and 44.44\\%, respectively. In the\nTruth Table Distance Prediction (TTDP) task, AIGer achieves improvements of\n33.57\\% and 14.79\\% in MAE and MSE, respectively, compared to the\nbest-performing models.", "AI": {"tldr": "This paper introduces AIGer, a new approach to model And-Inverter Graphs (AIGs) by jointly modeling functional and structural characteristics and improves its message passing capability. AIGer outperforms existing models in Signal Probability Prediction and Truth Table Distance Prediction tasks.", "motivation": "Due to the complex structure and large scale of nodes in real-world AIGs, accurate modeling is challenging, leading to existing work lacking the ability to jointly model functional and structural characteristics, as well as insufficient dynamic information propagation capability.", "method": "AIGer consists of two components: 1) Node logic feature initialization embedding component and 2) AIGs feature learning network component. The node logic feature initialization embedding component projects logic nodes, such as AND and NOT, into independent semantic spaces, to enable effective node embedding for subsequent processing. Building upon this, the AIGs feature learning network component employs a heterogeneous graph convolutional network, designing dynamic relationship weight matrices and differentiated information aggregation approaches to better represent the original structure and information of AIGs.", "result": "AIGer outperforms the current best models in the Signal Probability Prediction (SSP) task and the Truth Table Distance Prediction (TTDP) task.", "conclusion": "AIGer outperforms the current best models in the Signal Probability Prediction (SSP) task, improving MAE and MSE by 18.95% and 44.44%, respectively. In the Truth Table Distance Prediction (TTDP) task, AIGer achieves improvements of 33.57% and 14.79% in MAE and MSE, respectively, compared to the best-performing models."}}
{"id": "2508.12752", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.12752", "abs": "https://arxiv.org/abs/2508.12752", "authors": ["Wenlin Zhang", "Xiaopeng Li", "Yingyi Zhang", "Pengyue Jia", "Yichao Wang", "Huifeng Guo", "Yong Liu", "Xiangyu Zhao"], "title": "Deep Research: A Survey of Autonomous Research Agents", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has driven the\ndevelopment of agentic systems capable of autonomously performing complex\ntasks. Despite their impressive capabilities, LLMs remain constrained by their\ninternal knowledge boundaries. To overcome these limitations, the paradigm of\ndeep research has been proposed, wherein agents actively engage in planning,\nretrieval, and synthesis to generate comprehensive and faithful analytical\nreports grounded in web-based evidence. In this survey, we provide a systematic\noverview of the deep research pipeline, which comprises four core stages:\nplanning, question developing, web exploration, and report generation. For each\nstage, we analyze the key technical challenges and categorize representative\nmethods developed to address them. Furthermore, we summarize recent advances in\noptimization techniques and benchmarks tailored for deep research. Finally, we\ndiscuss open challenges and promising research directions, aiming to chart a\nroadmap toward building more capable and trustworthy deep research agents.", "AI": {"tldr": "\u672c\u6587\u6982\u8ff0\u4e86\u6df1\u5ea6\u7814\u7a76\u6d41\u7a0b\uff0c\u5206\u6790\u4e86\u5173\u952e\u6280\u672f\u6311\u6218\uff0c\u603b\u7ed3\u4e86\u4f18\u5316\u6280\u672f\u548c\u57fa\u51c6\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f00\u653e\u7684\u6311\u6218\u548c\u6709\u5e0c\u671b\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5feb\u901f\u53d1\u5c55\u63a8\u52a8\u4e86\u80fd\u591f\u81ea\u4e3b\u6267\u884c\u590d\u6742\u4efb\u52a1\u7684\u4ee3\u7406\u7cfb\u7edf\u7684\u53d1\u5c55\u3002\u5c3d\u7ba1\u5b83\u4eec\u5177\u6709\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\uff0c\u4f46LLM\u4ecd\u7136\u53d7\u5230\u5176\u5185\u90e8\u77e5\u8bc6\u8fb9\u754c\u7684\u9650\u5236\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u4eba\u4eec\u63d0\u51fa\u4e86\u6df1\u5ea6\u7814\u7a76\u7684\u8303\u4f8b\uff0c\u5373\u4ee3\u7406\u4e3b\u52a8\u53c2\u4e0e\u8ba1\u5212\u3001\u68c0\u7d22\u548c\u7efc\u5408\uff0c\u4ee5\u751f\u6210\u57fa\u4e8e\u7f51\u7edc\u8bc1\u636e\u7684\u5168\u9762\u800c\u5fe0\u5b9e\u7684\u5206\u6790\u62a5\u544a\u3002", "method": "\u5bf9\u6df1\u5ea6\u7814\u7a76\u6d41\u7a0b\u8fdb\u884c\u7cfb\u7edf\u6982\u8ff0\uff0c\u5305\u62ec\u56db\u4e2a\u6838\u5fc3\u9636\u6bb5\uff1a\u8ba1\u5212\u3001\u95ee\u9898\u5f00\u53d1\u3001\u7f51\u7edc\u63a2\u7d22\u548c\u62a5\u544a\u751f\u6210\u3002\u5bf9\u4e8e\u6bcf\u4e2a\u9636\u6bb5\uff0c\u5206\u6790\u4e86\u5173\u952e\u7684\u6280\u672f\u6311\u6218\uff0c\u5e76\u5bf9\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u800c\u5f00\u53d1\u7684\u4ee3\u8868\u6027\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\u3002", "result": "\u4e3a\u6df1\u5ea6\u7814\u7a76\u5b9a\u5236\u7684\u4f18\u5316\u6280\u672f\u548c\u57fa\u51c6\u7684\u6700\u65b0\u8fdb\u5c55", "conclusion": "\u603b\u7ed3\u4e86\u6df1\u5ea6\u7814\u7a76\u7684\u4f18\u5316\u6280\u672f\u548c\u57fa\u51c6\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f00\u653e\u7684\u6311\u6218\u548c\u6709\u5e0c\u671b\u7684\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u4e3a\u6784\u5efa\u66f4\u5f3a\u5927\u548c\u503c\u5f97\u4fe1\u8d56\u7684\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u63d0\u4f9b\u8def\u7ebf\u56fe\u3002"}}
{"id": "2508.11800", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11800", "abs": "https://arxiv.org/abs/2508.11800", "authors": ["Michael Bereket", "Jure Leskovec"], "title": "Uncalibrated Reasoning: GRPO Induces Overconfidence for Stochastic Outcomes", "comment": null, "summary": "Reinforcement learning (RL) has proven remarkably effective at improving the\naccuracy of language models in verifiable and deterministic domains like\nmathematics. Here, we examine if current RL methods are also effective at\noptimizing language models in verifiable domains with stochastic outcomes, like\nscientific experiments. Through applications to synthetic data and real-world\nbiological experiments, we demonstrate that Group Relative Policy Optimization\n(GRPO) induces overconfident probability predictions for binary stochastic\noutcomes, while Proximal Policy Optimization (PPO) and REINFORCE Leave-One-Out\n(RLOO) yield well-calibrated models. We show that removing group standard\nnormalization in GRPO fixes its miscalibration and provide a theoretical\nexplanation for why normalization causes overconfidence. Our results provide\nnew evidence against the use of standard normalization in GRPO and help pave\nthe way for applications of RL for reasoning language models beyond\ndeterministic domains.", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u53ef\u7528\u4e8e\u4f18\u5316\u79d1\u5b66\u5b9e\u9a8c\u4e2d\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u9700\u8981\u6ce8\u610f\u6821\u51c6\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u5f53\u524d\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5177\u6709\u968f\u673a\u7ed3\u679c\u7684\u53ef\u9a8c\u8bc1\u9886\u57df\uff08\u5982\u79d1\u5b66\u5b9e\u9a8c\uff09\u4e2d\u4f18\u5316\u8bed\u8a00\u6a21\u578b\u662f\u5426\u6709\u6548\u3002", "method": "\u4f7f\u7528Group Relative Policy Optimization (GRPO), Proximal Policy Optimization (PPO)\u548cREINFORCE Leave-One-Out (RLOO)\u7b49\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "GRPO\u4f1a\u5f15\u53d1\u4e8c\u5143\u968f\u673a\u7ed3\u679c\u7684\u8fc7\u5ea6\u81ea\u4fe1\u6982\u7387\u9884\u6d4b\uff0c\u800cPPO\u548cRLOO\u4f1a\u4ea7\u751f\u826f\u597d\u6821\u51c6\u7684\u6a21\u578b\u3002\u53bb\u9664GRPO\u4e2d\u7684group standard normalization\u53ef\u4ee5\u89e3\u51b3\u5176\u6821\u51c6\u95ee\u9898\u3002", "conclusion": "GRPO\u4e2d\u7684\u6807\u51c6\u5f52\u4e00\u5316\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u53bb\u9664group standard normalization\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u4e3a\u5728\u786e\u5b9a\u6027\u9886\u57df\u4e4b\u5916\u7684\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.11889", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11889", "abs": "https://arxiv.org/abs/2508.11889", "authors": ["Hui Ma", "Bo Zhang", "Jinpeng Hu", "Zenglin Shi"], "title": "In-Context Examples Matter: Improving Emotion Recognition in Conversation with Instruction Tuning", "comment": null, "summary": "Emotion recognition in conversation (ERC) aims to identify the emotion of\neach utterance in a conversation, playing a vital role in empathetic artificial\nintelligence. With the growing of large language models (LLMs), instruction\ntuning has emerged as a critical paradigm for ERC. Existing studies mainly\nfocus on multi-stage instruction tuning, which first endows LLMs with speaker\ncharacteristics, and then conducts context-aware instruction tuning to\ncomprehend emotional states. However, these methods inherently constrains the\ncapacity to jointly capture the dynamic interaction between speaker\ncharacteristics and conversational context, resulting in weak alignment among\nspeaker identity, contextual cues, and emotion states within a unified\nframework. In this paper, we propose InitERC, a simple yet effective one-stage\nin-context instruction tuning framework for ERC. InitERC adapts LLMs to learn\nspeaker-context-emotion alignment from context examples via in-context\ninstruction tuning. Specifically, InitERC comprises four components, i.e.,\ndemonstration pool construction, in-context example selection, prompt template\ndesign, and in-context instruction tuning. To explore the impact of in-context\nexamples, we conduct a comprehensive study on three key factors: retrieval\nstrategy, example ordering, and the number of examples. Extensive experiments\non three widely used datasets demonstrate that our proposed InitERC achieves\nsubstantial improvements over the state-of-the-art baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e ERC \u7684\u7b80\u5355\u800c\u6709\u6548\u7684\u5355\u9636\u6bb5\u4e0a\u4e0b\u6587\u6307\u4ee4\u8c03\u6574\u6846\u67b6 InitERC\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u591a\u9636\u6bb5\u6307\u4ee4\u8c03\u6574\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4ece\u6839\u672c\u4e0a\u9650\u5236\u4e86\u5728\u7edf\u4e00\u6846\u67b6\u5185\u5171\u540c\u6355\u83b7\u8bf4\u8bdd\u8005\u7279\u5f81\u548c\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u4e4b\u95f4\u52a8\u6001\u4ea4\u4e92\u7684\u80fd\u529b\uff0c\u4ece\u800c\u5bfc\u81f4\u8bf4\u8bdd\u8005\u8eab\u4efd\u3001\u4e0a\u4e0b\u6587\u7ebf\u7d22\u548c\u60c5\u7eea\u72b6\u6001\u4e4b\u95f4\u7684\u5f31\u5bf9\u9f50\u3002", "method": "InitERC \u5305\u542b\u56db\u4e2a\u7ec4\u4ef6\uff0c\u5373\u6f14\u793a\u6c60\u6784\u5efa\u3001\u4e0a\u4e0b\u6587\u793a\u4f8b\u9009\u62e9\u3001\u63d0\u793a\u6a21\u677f\u8bbe\u8ba1\u548c\u4e0a\u4e0b\u6587\u6307\u4ee4\u8c03\u6574\u3002", "result": "InitERC \u9002\u5e94 LLM\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u6307\u4ee4\u8c03\u6574\u4ece\u4e0a\u4e0b\u6587\u793a\u4f8b\u4e2d\u5b66\u4e60\u8bf4\u8bdd\u8005-\u4e0a\u4e0b\u6587-\u60c5\u611f\u5bf9\u9f50\u3002", "conclusion": "\u63d0\u51fa\u7684 InitERC \u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\u7684\u663e\u7740\u6539\u8fdb\u3002"}}
{"id": "2508.11854", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11854", "abs": "https://arxiv.org/abs/2508.11854", "authors": ["Matthew Hull", "Haoyang Yang", "Pratham Mehta", "Mansi Phute", "Aeree Cho", "Haorang Wang", "Matthew Lau", "Wenke Lee", "Wilian Lunardi", "Martin Andreoni", "Polo Chau"], "title": "ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages", "comment": "7 pages, 6 figures", "summary": "As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks\nfor efficient novel-view synthesis from static images, how might an adversary\ntamper images to cause harm? We introduce ComplicitSplat, the first attack that\nexploits standard 3DGS shading methods to create viewpoint-specific camouflage\n- colors and textures that change with viewing angle - to embed adversarial\ncontent in scene objects that are visible only from specific viewpoints and\nwithout requiring access to model architecture or weights. Our extensive\nexperiments show that ComplicitSplat generalizes to successfully attack a\nvariety of popular detector - both single-stage, multi-stage, and\ntransformer-based models on both real-world capture of physical objects and\nsynthetic scenes. To our knowledge, this is the first black-box attack on\ndownstream object detectors using 3DGS, exposing a novel safety risk for\napplications like autonomous navigation and other mission-critical robotic\nsystems.", "AI": {"tldr": "ComplicitSplat \u662f\u4e00\u79cd\u65b0\u7684\u9ed1\u76d2\u653b\u51fb\uff0c\u5b83\u5229\u7528 3DGS \u4e2d\u7684\u89c6\u89d2\u7279\u5b9a\u4f2a\u88c5\u6765\u653b\u51fb\u5bf9\u8c61\u68c0\u6d4b\u5668\uff0c\u4ece\u800c\u5bf9\u81ea\u52a8\u5bfc\u822a\u7b49\u5e94\u7528\u6784\u6210\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u7531\u4e8e 3D Gaussian Splatting (3DGS) \u5728\u5b89\u5168\u5173\u952e\u4efb\u52a1\u4e2d\u5f97\u5230\u5feb\u901f\u91c7\u7528\uff0c\u7528\u4e8e\u4ece\u9759\u6001\u56fe\u50cf\u4e2d\u9ad8\u6548\u5730\u5408\u6210\u65b0\u89c6\u89d2\uff0c\u90a3\u4e48\u653b\u51fb\u8005\u53ef\u80fd\u5982\u4f55\u7be1\u6539\u56fe\u50cf\u4ee5\u9020\u6210\u635f\u5bb3\uff1f", "method": "\u5229\u7528\u6807\u51c6\u7684 3DGS shading \u65b9\u6cd5\u521b\u5efa\u89c6\u89d2\u7279\u5b9a\u7684\u4f2a\u88c5\uff0c\u5373\u989c\u8272\u548c\u7eb9\u7406\u968f\u89c6\u89d2\u53d8\u5316\uff0c\u4ee5\u5c06\u5bf9\u6297\u6027\u5185\u5bb9\u5d4c\u5165\u5230\u4ec5\u4ece\u7279\u5b9a\u89c6\u89d2\u53ef\u89c1\u7684\u573a\u666f\u5bf9\u8c61\u4e2d\uff0c\u800c\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u67b6\u6784\u6216\u6743\u91cd\u3002", "result": "ComplicitSplat \u80fd\u591f\u6cdb\u5316\u5e76\u6210\u529f\u653b\u51fb\u5404\u79cd\u6d41\u884c\u7684\u68c0\u6d4b\u5668\u3002", "conclusion": "ComplicitSplat \u653b\u51fb\u6210\u529f\u5730\u653b\u51fb\u4e86\u5404\u79cd\u6d41\u884c\u7684\u68c0\u6d4b\u5668\uff0c\u5305\u62ec\u5355\u9636\u6bb5\u3001\u591a\u9636\u6bb5\u548c\u57fa\u4e8e Transformer \u7684\u6a21\u578b\uff0c\u65e0\u8bba\u662f\u5728\u771f\u5b9e\u7269\u4f53\u7684\u771f\u5b9e\u4e16\u754c\u6355\u83b7\u8fd8\u662f\u5408\u6210\u573a\u666f\u4e2d\u3002\u8fd9\u662f\u7b2c\u4e00\u4e2a\u4f7f\u7528 3DGS \u5bf9\u4e0b\u6e38\u5bf9\u8c61\u68c0\u6d4b\u5668\u8fdb\u884c\u7684\u9ed1\u76d2\u653b\u51fb\uff0c\u63ed\u793a\u4e86\u81ea\u52a8\u5bfc\u822a\u548c\u5176\u4ed6\u4efb\u52a1\u5173\u952e\u578b\u673a\u5668\u4eba\u7cfb\u7edf\u7b49\u5e94\u7528\u7684\u65b0\u5b89\u5168\u98ce\u9669\u3002"}}
{"id": "2508.11995", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.11995", "abs": "https://arxiv.org/abs/2508.11995", "authors": ["Xuyang Zhao", "Shiwan Zhao", "Hualong Yu", "Liting Zhang", "Qicheng Li"], "title": "AgentCDM: Enhancing Multi-Agent Collaborative Decision-Making via ACH-Inspired Structured Reasoning", "comment": null, "summary": "Multi-agent systems (MAS) powered by large language models (LLMs) hold\nsignificant promise for solving complex decision-making tasks. However, the\ncore process of collaborative decision-making (CDM) within these systems\nremains underexplored. Existing approaches often rely on either ``dictatorial\"\nstrategies that are vulnerable to the cognitive biases of a single agent, or\n``voting-based\" methods that fail to fully harness collective intelligence. To\naddress these limitations, we propose \\textbf{AgentCDM}, a structured framework\nfor enhancing collaborative decision-making in LLM-based multi-agent systems.\nDrawing inspiration from the Analysis of Competing Hypotheses (ACH) in\ncognitive science, AgentCDM introduces a structured reasoning paradigm that\nsystematically mitigates cognitive biases and shifts decision-making from\npassive answer selection to active hypothesis evaluation and construction. To\ninternalize this reasoning process, we develop a two-stage training paradigm:\nthe first stage uses explicit ACH-inspired scaffolding to guide the model\nthrough structured reasoning, while the second stage progressively removes this\nscaffolding to encourage autonomous generalization. Experiments on multiple\nbenchmark datasets demonstrate that AgentCDM achieves state-of-the-art\nperformance and exhibits strong generalization, validating its effectiveness in\nimproving the quality and robustness of collaborative decisions in MAS.", "AI": {"tldr": "AgentCDM\u662f\u4e00\u4e2a\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u534f\u4f5c\u51b3\u7b56\uff0c\u901a\u8fc7\u6a21\u4eff\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u7ade\u4e89\u5047\u8bbe\u5206\u6790\uff08ACH\uff09\u6765\u51cf\u8f7b\u8ba4\u77e5\u504f\u5dee\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u5728\u89e3\u51b3\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u65b9\u9762\u5177\u6709\u5de8\u5927\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u4e2d\u534f\u4f5c\u51b3\u7b56\uff08CDM\uff09\u7684\u6838\u5fc3\u8fc7\u7a0b\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u6613\u53d7\u5355\u4e2a\u667a\u80fd\u4f53\u8ba4\u77e5\u504f\u5dee\u5f71\u54cd\u7684\u201c\u72ec\u88c1\u201d\u7b56\u7565\uff0c\u6216\u8005\u672a\u80fd\u5145\u5206\u5229\u7528\u96c6\u4f53\u667a\u6167\u7684\u201c\u57fa\u4e8e\u6295\u7968\u201d\u7684\u65b9\u6cd5\u3002", "method": "AgentCDM\uff0c\u4e00\u4e2a\u7528\u4e8e\u589e\u5f3a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u534f\u4f5c\u51b3\u7b56\u7684\u7ed3\u6784\u5316\u6846\u67b6\u3002\u501f\u9274\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u7ade\u4e89\u5047\u8bbe\u5206\u6790\uff08ACH\uff09\uff0cAgentCDM\u5f15\u5165\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u7684\u63a8\u7406\u8303\u5f0f\uff0c\u8be5\u8303\u5f0f\u7cfb\u7edf\u5730\u51cf\u8f7b\u4e86\u8ba4\u77e5\u504f\u5dee\uff0c\u5e76\u5c06\u51b3\u7b56\u4ece\u88ab\u52a8\u7b54\u6848\u9009\u62e9\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u5047\u8bbe\u8bc4\u4f30\u548c\u6784\u5efa\u3002\u4e3a\u4e86\u5c06\u8fd9\u79cd\u63a8\u7406\u8fc7\u7a0b\u5185\u5728\u5316\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u663e\u5f0f\u7684\u3001\u53d7ACH\u542f\u53d1\u7684\u652f\u67b6\u6765\u6307\u5bfc\u6a21\u578b\u8fdb\u884c\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u800c\u7b2c\u4e8c\u9636\u6bb5\u9010\u6e10\u79fb\u9664\u8fd9\u79cd\u652f\u67b6\uff0c\u4ee5\u9f13\u52b1\u81ea\u4e3b\u6cdb\u5316\u3002", "result": "AgentCDM\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AgentCDM\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u63d0\u9ad8MAS\u4e2d\u534f\u4f5c\u51b3\u7b56\u7684\u8d28\u91cf\u548c\u7a33\u5065\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.13019", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.13019", "abs": "https://arxiv.org/abs/2508.13019", "authors": ["Lucien Heitz", "Runze Li", "Oana Inel", "Abraham Bernstein"], "title": "Informfully Recommenders -- Reproducibility Framework for Diversity-aware Intra-session Recommendations", "comment": "10 pages", "summary": "Norm-aware recommender systems have gained increased attention, especially\nfor diversity optimization. The recommender systems community has\nwell-established experimentation pipelines that support reproducible\nevaluations by facilitating models' benchmarking and comparisons against\nstate-of-the-art methods. However, to the best of our knowledge, there is\ncurrently no reproducibility framework to support thorough norm-driven\nexperimentation at the pre-processing, in-processing, post-processing, and\nevaluation stages of the recommender pipeline. To address this gap, we present\nInformfully Recommenders, a first step towards a normative reproducibility\nframework that focuses on diversity-aware design built on Cornac. Our extension\nprovides an end-to-end solution for implementing and experimenting with\nnormative and general-purpose diverse recommender systems that cover 1) dataset\npre-processing, 2) diversity-optimized models, 3) dedicated intrasession item\nre-ranking, and 4) an extensive set of diversity metrics. We demonstrate the\ncapabilities of our extension through an extensive offline experiment in the\nnews domain.", "AI": {"tldr": "Introduces Informfully Recommenders, a reproducibility framework for diversity-aware recommender systems.", "motivation": "There is a lack of reproducibility frameworks for thorough norm-driven experimentation in recommender systems, especially for diversity optimization.", "method": "The framework extends Cornac and provides an end-to-end solution covering dataset pre-processing, diversity-optimized models, intra-session item re-ranking, and diversity metrics.", "result": "The capabilities of the extension are demonstrated through an extensive offline experiment in the news domain.", "conclusion": "This paper presents Informfully Recommenders, a normative reproducibility framework for diversity-aware recommender systems."}}
{"id": "2508.11810", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11810", "abs": "https://arxiv.org/abs/2508.11810", "authors": ["Nitish Nagesh", "Salar Shakibhamedan", "Mahdi Bagheri", "Ziyu Wang", "Nima TaheriNejad", "Axel Jantsch", "Amir M. Rahmani"], "title": "FairTabGen: Unifying Counterfactual and Causal Fairness in Synthetic Tabular Data Generation", "comment": null, "summary": "Generating synthetic data is crucial in privacy-sensitive, data-scarce\nsettings, especially for tabular datasets widely used in real-world\napplications. A key challenge is improving counterfactual and causal fairness,\nwhile preserving high utility. We present FairTabGen, a fairness-aware large\nlanguage model-based framework for tabular synthetic data generation. We\nintegrate multiple fairness definitions including counterfactual and causal\nfairness into both its generation and evaluation pipelines. We use in-context\nlearning, prompt refinement, and fairness-aware data curation to balance\nfairness and utility. Across diverse datasets, our method outperforms\nstate-of-the-art GAN-based and LLM-based methods, achieving up to 10%\nimprovements on fairness metrics such as demographic parity and path-specific\ncausal effects while retaining statistical utility. Remarkably, it achieves\nthese gains using less than 20% of the original data, highlighting its\nefficiency in low-data regimes. These results demonstrate a principled and\npractical approach for generating fair and useful synthetic tabular data.", "AI": {"tldr": "FairTabGen, a new LLM-based framework, generates fairer and more useful synthetic tabular data, especially when data is scarce.", "motivation": "Generating synthetic data is crucial in privacy-sensitive and data-scarce settings, particularly for tabular datasets. Improving counterfactual and causal fairness while preserving utility is a key challenge.", "method": "A fairness-aware large language model-based framework is used, integrating counterfactual and causal fairness into generation and evaluation. In-context learning, prompt refinement, and fairness-aware data curation are employed.", "result": "FairTabGen achieves up to 10% improvements on fairness metrics while retaining statistical utility, using less than 20% of the original data.", "conclusion": "FairTabGen provides a practical approach for generating fair and useful synthetic tabular data, outperforming existing methods in balancing fairness and utility, especially in low-data regimes."}}
{"id": "2508.11915", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11915", "abs": "https://arxiv.org/abs/2508.11915", "authors": ["Punya Syon Pandey", "Yongjin Yang", "Jiarui Liu", "Zhijing Jin"], "title": "CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures", "comment": null, "summary": "Game-theoretic interactions between agents with Large Language Models (LLMs)\nhave revealed many emergent capabilities, yet the linguistic diversity of these\ninteractions has not been sufficiently quantified. In this paper, we present\nthe Conversational Robustness Evaluation Score: CORE, a metric to quantify the\neffectiveness of language use within multi-agent systems across different\ngame-theoretic interactions. CORE integrates measures of cluster entropy,\nlexical repetition, and semantic similarity, providing a direct lens of dialog\nquality. We apply CORE to pairwise LLM dialogs across competitive, cooperative,\nand neutral settings, further grounding our analysis in Zipf's and Heaps' Laws\nto characterize word frequency distributions and vocabulary growth. Our\nfindings show that cooperative settings exhibit both steeper Zipf distributions\nand higher Heap exponents, indicating more repetition alongside greater\nvocabulary expansion. In contrast, competitive interactions display lower Zipf\nand Heaps exponents, reflecting less repetition and more constrained\nvocabularies. These results provide new insights into how social incentives\ninfluence language adaptation, and highlight CORE as a robust diagnostic for\nmeasuring linguistic robustness in multi-agent LLM systems. Our code is\navailable at https://github.com/psyonp/core.", "AI": {"tldr": "This paper introduces CORE, a metric to quantify language use in multi-agent LLM systems, and finds that cooperation leads to more diverse language use than competition.", "motivation": "The linguistic diversity of interactions between agents with LLMs has not been sufficiently quantified.", "method": "Introduce CORE, a metric integrating cluster entropy, lexical repetition, and semantic similarity. Apply CORE to pairwise LLM dialogs across competitive, cooperative, and neutral settings, using Zipf's and Heaps' Laws.", "result": "Cooperative settings exhibit steeper Zipf distributions and higher Heap exponents, indicating more repetition and greater vocabulary expansion. Competitive interactions display lower Zipf and Heaps exponents, reflecting less repetition and more constrained vocabularies.", "conclusion": "Cooperative settings lead to more repetition and vocabulary expansion, while competitive ones show less repetition and constrained vocabularies. CORE is a robust diagnostic for measuring linguistic robustness in multi-agent LLM systems."}}
{"id": "2508.11864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11864", "abs": "https://arxiv.org/abs/2508.11864", "authors": ["Yucheng Tang", "Pawel Rajwa", "Alexander Ng", "Yipei Wang", "Wen Yan", "Natasha Thorley", "Aqua Asif", "Clare Allen", "Louise Dickinson", "Francesco Giganti", "Shonit Punwani", "Daniel C. Alexander", "Veeru Kasivisvanathan", "Yipeng Hu"], "title": "Impact of Clinical Image Quality on Efficient Foundation Model Finetuning", "comment": null, "summary": "Foundation models in medical imaging have shown promising label efficiency,\nachieving high downstream performance with only a fraction of annotated data.\nHere, we evaluate this in prostate multiparametric MRI using ProFound, a\ndomain-specific vision foundation model pretrained on large-scale prostate MRI\ndatasets. We investigate how variable image quality affects label-efficient\nfinetuning by measuring the generalisability of finetuned models. Experiments\nsystematically vary high-/low-quality image ratios in finetuning and evaluation\nsets. Our findings indicate that image quality distribution and its\nfinetune-and-test mismatch significantly affect model performance. In\nparticular: a) Varying the ratio of high- to low-quality images between\nfinetuning and test sets leads to notable differences in downstream\nperformance; and b) The presence of sufficient high-quality images in the\nfinetuning set is critical for maintaining strong performance, whilst the\nimportance of matched finetuning and testing distribution varies between\ndifferent downstream tasks, such as automated radiology reporting and prostate\ncancer detection.When quality ratios are consistent, finetuning needs far less\nlabeled data than training from scratch, but label efficiency depends on image\nquality distribution. Without enough high-quality finetuning data, pretrained\nmodels may fail to outperform those trained without pretraining. This\nhighlights the importance of assessing and aligning quality distributions\nbetween finetuning and deployment, and the need for quality standards in\nfinetuning data for specific downstream tasks. Using ProFound, we show the\nvalue of quantifying image quality in both finetuning and deployment to fully\nrealise the data and compute efficiency benefits of foundation models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u56fe\u50cf\u8d28\u91cf\u5bf9\u524d\u5217\u817aMRI\u4e2d\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u56fe\u50cf\u8d28\u91cf\u5206\u5e03\u7684\u5339\u914d\u5bf9\u4e8e\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u57fa\u7840\u6a21\u578b\u5df2\u7ecf\u663e\u793a\u51fa\u6709\u5e0c\u671b\u7684\u6807\u7b7e\u6548\u7387\uff0c\u4ec5\u7528\u4e00\u5c0f\u90e8\u5206\u5e26\u6ce8\u91ca\u7684\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u4e0b\u6e38\u6027\u80fd\u3002\u672c\u6587\u7814\u7a76\u4e86\u53ef\u53d8\u7684\u56fe\u50cf\u8d28\u91cf\u5982\u4f55\u5f71\u54cd\u6807\u7b7e\u9ad8\u6548\u5fae\u8c03\u3002", "method": "\u4f7f\u7528ProFound\uff0c\u4e00\u4e2a\u5728\u5927\u578b\u524d\u5217\u817aMRI\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684\u9886\u57df\u7279\u5b9a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u5728\u591a\u53c2\u6570\u524d\u5217\u817aMRI\u4e2d\u8bc4\u4f30\u6807\u7b7e\u6548\u7387\u3002", "result": "a) \u5fae\u8c03\u548c\u6d4b\u8bd5\u96c6\u4e4b\u95f4\u9ad8\u8d28\u91cf\u56fe\u50cf\u4e0e\u4f4e\u8d28\u91cf\u56fe\u50cf\u7684\u6bd4\u7387\u53d8\u5316\u4f1a\u5bfc\u81f4\u4e0b\u6e38\u6027\u80fd\u7684\u663e\u7740\u5dee\u5f02\uff1bb) \u5fae\u8c03\u96c6\u4e2d\u5b58\u5728\u8db3\u591f\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u5bf9\u4e8e\u4fdd\u6301\u5f3a\u5927\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u800c\u5339\u914d\u7684\u5fae\u8c03\u548c\u6d4b\u8bd5\u5206\u5e03\u7684\u91cd\u8981\u6027\u56e0\u4e0d\u540c\u7684\u4e0b\u6e38\u4efb\u52a1\u800c\u5f02\uff0c\u4f8b\u5982\u81ea\u52a8\u653e\u5c04\u5b66\u62a5\u544a\u548c\u524d\u5217\u817a\u764c\u68c0\u6d4b\u3002", "conclusion": "\u56fe\u50cf\u8d28\u91cf\u5206\u5e03\u53ca\u5176\u5fae\u8c03\u548c\u6d4b\u8bd5\u4e0d\u5339\u914d\u4f1a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002\u5f53\u8d28\u91cf\u6bd4\u7387\u4e00\u81f4\u65f6\uff0c\u5fae\u8c03\u6bd4\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u9700\u8981\u66f4\u5c11\u7684\u6807\u8bb0\u6570\u636e\uff0c\u4f46\u6807\u7b7e\u6548\u7387\u53d6\u51b3\u4e8e\u56fe\u50cf\u8d28\u91cf\u5206\u5e03\u3002\u6ca1\u6709\u8db3\u591f\u7684\u9ad8\u8d28\u91cf\u5fae\u8c03\u6570\u636e\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u80fd\u65e0\u6cd5\u80dc\u8fc7\u90a3\u4e9b\u6ca1\u6709\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u8bc4\u4f30\u548c\u8c03\u6574\u5fae\u8c03\u548c\u90e8\u7f72\u4e4b\u95f4\u7684\u8d28\u91cf\u5206\u5e03\u975e\u5e38\u91cd\u8981\uff0c\u5e76\u4e14\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\u7684\u5fae\u8c03\u6570\u636e\u8d28\u91cf\u6807\u51c6\u3002\u4f7f\u7528ProFound\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u91cf\u5316\u5fae\u8c03\u548c\u90e8\u7f72\u4e2d\u7684\u56fe\u50cf\u8d28\u91cf\u7684\u4ef7\u503c\uff0c\u4ee5\u5145\u5206\u5b9e\u73b0\u57fa\u7840\u6a21\u578b\u7684\u6570\u636e\u548c\u8ba1\u7b97\u6548\u7387\u4f18\u52bf\u3002"}}
{"id": "2508.12022", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12022", "abs": "https://arxiv.org/abs/2508.12022", "authors": ["Dorsa Macky Aleagha", "Payam Zohari", "Mostafa Haghir Chehreghani"], "title": "AI Models for Depressive Disorder Detection and Diagnosis: A Review", "comment": null, "summary": "Major Depressive Disorder is one of the leading causes of disability\nworldwide, yet its diagnosis still depends largely on subjective clinical\nassessments. Integrating Artificial Intelligence (AI) holds promise for\ndeveloping objective, scalable, and timely diagnostic tools. In this paper, we\npresent a comprehensive survey of state-of-the-art AI methods for depression\ndetection and diagnosis, based on a systematic review of 55 key studies. We\nintroduce a novel hierarchical taxonomy that structures the field by primary\nclinical task (diagnosis vs. prediction), data modality (text, speech,\nneuroimaging, multimodal), and computational model class (e.g., graph neural\nnetworks, large language models, hybrid approaches). Our in-depth analysis\nreveals three major trends: the predominance of graph neural networks for\nmodeling brain connectivity, the rise of large language models for linguistic\nand conversational data, and an emerging focus on multimodal fusion,\nexplainability, and algorithmic fairness. Alongside methodological insights, we\nprovide an overview of prominent public datasets and standard evaluation\nmetrics as a practical guide for researchers. By synthesizing current advances\nand highlighting open challenges, this survey offers a comprehensive roadmap\nfor future innovation in computational psychiatry.", "AI": {"tldr": "Comprehensive survey of AI methods for depression detection, highlighting trends like graph neural networks and large language models, and emphasizing multimodal fusion and explainability.", "motivation": "Major Depressive Disorder is a leading cause of disability, and its diagnosis relies on subjective clinical assessments. AI offers potential for objective, scalable diagnostic tools.", "method": "A systematic review of 55 key studies, introducing a novel hierarchical taxonomy based on clinical task, data modality, and computational model class.", "result": "Identified three major trends: predominance of graph neural networks for modeling brain connectivity, the rise of large language models for linguistic data, and an emerging focus on multimodal fusion, explainability, and algorithmic fairness. Also provides an overview of datasets and evaluation metrics.", "conclusion": "This survey synthesizes current advances in AI for depression detection and diagnosis and highlights open challenges, offering a roadmap for future innovation in computational psychiatry."}}
{"id": "2508.13035", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.13035", "abs": "https://arxiv.org/abs/2508.13035", "authors": ["Runze Li", "Lucien Heitz", "Oana Inel", "Abraham Bernstein"], "title": "D-RDW: Diversity-Driven Random Walks for News Recommender Systems", "comment": "6 pages", "summary": "This paper introduces Diversity-Driven RandomWalks (D-RDW), a lightweight\nalgorithm and re-ranking technique that generates diverse news recommendations.\nD-RDW is a societal recommender, which combines the diversification\ncapabilities of the traditional random walk algorithms with customizable target\ndistributions of news article properties. In doing so, our model provides a\ntransparent approach for editors to incorporate norms and values into the\nrecommendation process. D-RDW shows enhanced performance across key diversity\nmetrics that consider the articles' sentiment and political party mentions when\ncompared to state-of-the-art neural models. Furthermore, D-RDW proves to be\nmore computationally efficient than existing approaches.", "AI": {"tldr": "D-RDW\u662f\u4e00\u79cd\u793e\u4f1a\u63a8\u8350\u5668\uff0c\u5b83\u7ed3\u5408\u4e86\u4f20\u7edf\u968f\u673a\u6f2b\u6b65\u7b97\u6cd5\u7684\u591a\u6837\u5316\u80fd\u529b\u4e0e\u53ef\u5b9a\u5236\u7684\u65b0\u95fb\u6587\u7ae0\u5c5e\u6027\u76ee\u6807\u5206\u5e03\u3002", "motivation": "\u751f\u6210\u591a\u6837\u5316\u7684\u65b0\u95fb\u63a8\u8350", "method": "D-RDW\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7b97\u6cd5\u548c\u91cd\u6392\u5e8f\u6280\u672f", "result": "D-RDW\u5728\u8003\u8651\u6587\u7ae0\u7684\u60c5\u7eea\u548c\u653f\u515a\u63d0\u53ca\u7684\u5173\u952e\u591a\u6837\u6027\u6307\u6807\u4e0a\u663e\u793a\u51fa\u589e\u5f3a\u7684\u6027\u80fd", "conclusion": "D-RDW\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.11876", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11876", "abs": "https://arxiv.org/abs/2508.11876", "authors": ["Hoang-Thang Ta", "Duy-Quy Thai", "Phuong-Linh Tran-Thi"], "title": "Combinations of Fast Activation and Trigonometric Functions in Kolmogorov-Arnold Networks", "comment": "6pages", "summary": "For years, many neural networks have been developed based on the\nKolmogorov-Arnold Representation Theorem (KART), which was created to address\nHilbert's 13th problem. Recently, relying on KART, Kolmogorov-Arnold Networks\n(KANs) have attracted attention from the research community, stimulating the\nuse of polynomial functions such as B-splines and RBFs. However, these\nfunctions are not fully supported by GPU devices and are still considered less\npopular. In this paper, we propose the use of fast computational functions,\nsuch as ReLU and trigonometric functions (e.g., ReLU, sin, cos, arctan), as\nbasis components in Kolmogorov-Arnold Networks (KANs). By integrating these\nfunction combinations into the network structure, we aim to enhance\ncomputational efficiency. Experimental results show that these combinations\nmaintain competitive performance while offering potential improvements in\ntraining time and generalization.", "AI": {"tldr": "This paper uses faster functions (ReLU, sin, cos, arctan) in KANs to improve speed and generalization.", "motivation": "Existing KANs rely on polynomial functions like B-splines and RBFs, which are not fully supported by GPUs and are less popular.", "method": "The paper integrates ReLU and trigonometric functions into the KAN network structure.", "result": "The proposed combinations maintain competitive performance while offering potential improvements in training time and generalization.", "conclusion": "This paper proposes using fast computational functions like ReLU and trigonometric functions in KANs to enhance computational efficiency. Experimental results show competitive performance with potential improvements in training time and generalization."}}
{"id": "2508.11927", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11927", "abs": "https://arxiv.org/abs/2508.11927", "authors": ["Jie Lu", "Du Jin", "Hitomi Yanaka"], "title": "LLMs Struggle with NLI for Perfect Aspect: A Cross-Linguistic Study in Chinese and Japanese", "comment": "9 pages, 3 figures", "summary": "Unlike English, which uses distinct forms (e.g., had, has, will have) to mark\nthe perfect aspect across tenses, Chinese and Japanese lack separate\ngrammatical forms for tense within the perfect aspect, which complicates\nNatural Language Inference (NLI). Focusing on the perfect aspect in these\nlanguages, we construct a linguistically motivated, template-based NLI dataset\n(1,350 pairs per language). Experiments reveal that even advanced LLMs struggle\nwith temporal inference, particularly in detecting subtle tense and\nreference-time shifts. These findings highlight model limitations and\nunderscore the need for cross-linguistic evaluation in temporal semantics. Our\ndataset is available at https://github.com/Lujie2001/CrossNLI.", "AI": {"tldr": "\u6784\u5efa\u4e86\u4e00\u4e2a\u9488\u5bf9\u6c49\u8bed\u548c\u65e5\u8bed\u7684NLI\u6570\u636e\u96c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u65f6\u95f4\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5f3a\u8c03\u4e86\u8de8\u8bed\u8a00\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u6c49\u8bed\u548c\u65e5\u8bed\u5728\u5b8c\u6210\u4f53\u4e2d\u7f3a\u4e4f\u72ec\u7acb\u7684\u65f6\u6001\u8bed\u6cd5\u5f62\u5f0f\uff0c\u8fd9\u4f7f\u5f97\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\u53d8\u5f97\u590d\u6742\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8bed\u8a00\u5b66\u52a8\u673a\u7684\u3001\u57fa\u4e8e\u6a21\u677f\u7684NLI\u6570\u636e\u96c6\uff08\u6bcf\u79cd\u8bed\u8a001,350\u5bf9\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u9ad8\u7ea7\u7684LLM\u4e5f\u5f88\u96be\u8fdb\u884c\u65f6\u95f4\u63a8\u7406\uff0c\u5c24\u5176\u662f\u5728\u68c0\u6d4b\u7ec6\u5fae\u7684\u65f6\u6001\u548c\u53c2\u8003\u65f6\u95f4\u53d8\u5316\u65f6\u3002\u8fd9\u4e9b\u53d1\u73b0\u7a81\u51fa\u4e86\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u65f6\u95f4\u8bed\u4e49\u4e2d\u8de8\u8bed\u8a00\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u5373\u4f7f\u662f\u9ad8\u7ea7\u7684LLM\u4e5f\u5f88\u96be\u8fdb\u884c\u65f6\u95f4\u63a8\u7406\uff0c\u5c24\u5176\u662f\u5728\u68c0\u6d4b\u7ec6\u5fae\u7684\u65f6\u6001\u548c\u53c2\u8003\u65f6\u95f4\u53d8\u5316\u65f6\u3002"}}
{"id": "2508.11870", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11870", "abs": "https://arxiv.org/abs/2508.11870", "authors": ["Ying Huang", "Yuanbin Man", "Wenqi Jia", "Zhengzhong Tu", "Junzhou Huang", "Miao Yin"], "title": "AdaRing: Towards Ultra-Light Vision-Language Adaptation via Cross-Layer Tensor Ring Decomposition", "comment": null, "summary": "Adapter-based fine-tuning has gained remarkable attention in adapting large\npre-trained vision language models (VLMs) for a wide range of downstream tasks\nefficiently. In this paradigm, only the inserted adapters are fine-tuned,\nwithout the need for training the original VLM backbone. Existing works scale\nadapters by integrating them into every layer of VLMs to increase the capacity\nof adapters. However, these methods face two primary limitations: 1) limited\ncompression rate due to ignoring cross-layer redundancy, and 2) limited\nrepresentational capacity across homogeneous adapters. In this paper, we\npropose a novel vision-language fine-tuning framework based on cross-layer\ntensor ring decomposition (TRD) with the integration and collaboration of\ndiverse adapters, called AdaRing, achieving ultra-light parameter-efficient\nadaptation of VLMs on various tasks. To remove the high redundancy that exists\namong adapters across layers, we exploit the tensor-level low-rankness to\nformulate adapters as layer-shared tensor cores and layer-specific slices.\nMoreover, guided by generalization-aware fine-tuning, diverse rank-driven\nadapters cooperate to handle tasks that require different representations. Our\nexperiments show that the proposed AdaRing achieves the state-of-the-art\nperformance while reducing average training parameters by 90%.", "AI": {"tldr": "Proposes AdaRing, a parameter-efficient fine-tuning method for VLMs using cross-layer tensor ring decomposition and diverse adapters, achieving state-of-the-art performance with 90% parameter reduction.", "motivation": "Existing adapter-based fine-tuning methods face limitations: 1) limited compression rate due to ignoring cross-layer redundancy, and 2) limited representational capacity across homogeneous adapters.", "method": "a novel vision-language fine-tuning framework based on cross-layer tensor ring decomposition (TRD) with the integration and collaboration of diverse adapters, called AdaRing", "result": "achieving ultra-light parameter-efficient adaptation of VLMs on various tasks", "conclusion": "The proposed AdaRing achieves state-of-the-art performance while reducing average training parameters by 90%."}}
{"id": "2508.12026", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12026", "abs": "https://arxiv.org/abs/2508.12026", "authors": ["Szymon Pawlonka", "Miko\u0142aj Ma\u0142ki\u0144ski", "Jacek Ma\u0144dziuk"], "title": "Bongard-RWR+: Real-World Representations of Fine-Grained Concepts in Bongard Problems", "comment": null, "summary": "Bongard Problems (BPs) provide a challenging testbed for abstract visual\nreasoning (AVR), requiring models to identify visual concepts fromjust a few\nexamples and describe them in natural language. Early BP benchmarks featured\nsynthetic black-and-white drawings, which might not fully capture the\ncomplexity of real-world scenes. Subsequent BP datasets employed real-world\nimages, albeit the represented concepts are identifiable from high-level image\nfeatures, reducing the task complexity. Differently, the recently released\nBongard-RWR dataset aimed at representing abstract concepts formulated in the\noriginal BPs using fine-grained real-world images. Its manual construction,\nhowever, limited the dataset size to just $60$ instances, constraining\nevaluation robustness. In this work, we introduce Bongard-RWR+, a BP dataset\ncomposed of $5\\,400$ instances that represent original BP abstract concepts\nusing real-world-like images generated via a vision language model (VLM)\npipeline. Building on Bongard-RWR, we employ Pixtral-12B to describe manually\ncurated images and generate new descriptions aligned with the underlying\nconcepts, use Flux.1-dev to synthesize images from these descriptions, and\nmanually verify that the generated images faithfully reflect the intended\nconcepts. We evaluate state-of-the-art VLMs across diverse BP formulations,\nincluding binary and multiclass classification, as well as textual answer\ngeneration. Our findings reveal that while VLMs can recognize coarse-grained\nvisual concepts, they consistently struggle with discerning fine-grained\nconcepts, highlighting limitations in their reasoning capabilities.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684 Bongard Problems (BP) \u6570\u636e\u96c6 Bongard-RWR+\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b 5,400 \u4e2a\u5b9e\u4f8b\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\u6765\u8868\u793a\u62bd\u8c61\u6982\u5ff5\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u7ec6\u7c92\u5ea6\u6982\u5ff5\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u65e9\u671f\u7684 BP \u57fa\u51c6\u6d4b\u8bd5\u4ee5\u5408\u6210\u9ed1\u767d\u7ed8\u56fe\u4e3a\u7279\u8272\uff0c\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u771f\u5b9e\u4e16\u754c\u573a\u666f\u7684\u590d\u6742\u6027\u3002\u968f\u540e\u7684 BP \u6570\u636e\u96c6\u91c7\u7528\u4e86\u771f\u5b9e\u4e16\u754c\u7684\u56fe\u50cf\uff0c\u4f46\u6240\u4ee3\u8868\u7684\u6982\u5ff5\u53ef\u4ee5\u4ece\u9ad8\u7ea7\u56fe\u50cf\u7279\u5f81\u4e2d\u8bc6\u522b\u51fa\u6765\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u4efb\u52a1\u7684\u590d\u6742\u6027\u3002\u6700\u8fd1\u53d1\u5e03\u7684 Bongard-RWR \u6570\u636e\u96c6\u65e8\u5728\u8868\u793a\u539f\u59cb BP \u4e2d\u5236\u5b9a\u7684\u62bd\u8c61\u6982\u5ff5\uff0c\u4f7f\u7528\u7ec6\u7c92\u5ea6\u7684\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u3002\u7136\u800c\uff0c\u5b83\u7684\u4eba\u5de5\u6784\u5efa\u5c06\u6570\u636e\u96c6\u7684\u5927\u5c0f\u9650\u5236\u4e3a 60 \u4e2a\u5b9e\u4f8b\uff0c\u4ece\u800c\u9650\u5236\u4e86\u8bc4\u4f30\u7684\u7a33\u5065\u6027\u3002", "method": "\u4f7f\u7528 Pixtral-12B \u63cf\u8ff0\u624b\u52a8\u6574\u7406\u7684\u56fe\u50cf\uff0c\u5e76\u751f\u6210\u4e0e\u5e95\u5c42\u6982\u5ff5\u5bf9\u9f50\u7684\u65b0\u63cf\u8ff0\uff0c\u4f7f\u7528 Flux.1-dev \u4ece\u8fd9\u4e9b\u63cf\u8ff0\u4e2d\u5408\u6210\u56fe\u50cf\uff0c\u5e76\u624b\u52a8\u9a8c\u8bc1\u751f\u6210\u7684\u56fe\u50cf\u662f\u5426\u5fe0\u5b9e\u5730\u53cd\u6620\u4e86\u9884\u671f\u7684\u6982\u5ff5\u3002", "result": "\u6211\u4eec\u5f15\u5165\u4e86 Bongard-RWR+\uff0c\u8fd9\u662f\u4e00\u4e2a\u7531 5,400 \u4e2a\u5b9e\u4f8b\u7ec4\u6210\u7684 BP \u6570\u636e\u96c6\uff0c\u5b83\u4f7f\u7528\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7ba1\u9053\u751f\u6210\u7684\u7c7b\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u6765\u8868\u793a\u539f\u59cb BP \u62bd\u8c61\u6982\u5ff5\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u5404\u79cd BP \u516c\u5f0f\uff08\u5305\u62ec\u4e8c\u5143\u548c\u591a\u7c7b\u5206\u7c7b\u4ee5\u53ca\u6587\u672c\u7b54\u6848\u751f\u6210\uff09\u4e2d\u7684\u6700\u5148\u8fdb\u7684 VLM\u3002\u6211\u4eec\u7684\u53d1\u73b0\u8868\u660e\uff0c\u867d\u7136 VLM \u53ef\u4ee5\u8bc6\u522b\u7c97\u7c92\u5ea6\u7684\u89c6\u89c9\u6982\u5ff5\uff0c\u4f46\u5b83\u4eec\u59cb\u7ec8\u96be\u4ee5\u8fa8\u522b\u7ec6\u7c92\u5ea6\u7684\u6982\u5ff5\uff0c\u7a81\u663e\u4e86\u5176\u63a8\u7406\u80fd\u529b\u7684\u5c40\u9650\u6027\u3002", "conclusion": "VLMs\u5728\u8bc6\u522b\u7c97\u7c92\u5ea6\u89c6\u89c9\u6982\u5ff5\u65b9\u9762\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u8fa8\u522b\u7ec6\u7c92\u5ea6\u6982\u5ff5\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7a81\u663e\u4e86\u5176\u63a8\u7406\u80fd\u529b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.13064", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13064", "abs": "https://arxiv.org/abs/2508.13064", "authors": ["Seongeun Ryu", "Yunyong Ko", "Sang-Wook Kim"], "title": "Is This News Still Interesting to You?: Lifetime-aware Interest Matching for News Recommendation", "comment": "10 pages, 7 figures, 4 tables, accepted at ACM International\n  Conference on Information and Knowledge Management (CIKM)", "summary": "Personalized news recommendation aims to deliver news articles aligned with\nusers' interests, serving as a key solution to alleviate the problem of\ninformation overload on online news platforms. While prior work has improved\ninterest matching through refined representations of news and users, the\nfollowing time-related challenges remain underexplored: (C1) leveraging the age\nof clicked news to infer users' interest persistence, and (C2) modeling the\nvarying lifetime of news across topics and users. To jointly address these\nchallenges, we propose a novel Lifetime-aware Interest Matching framework for\nnEws recommendation, named LIME, which incorporates three key strategies: (1)\nUser-Topic lifetime-aware age representation to capture the relative age of\nnews with respect to a user-topic pair, (2) Candidate-aware lifetime attention\nfor generating temporally aligned user representation, and (3) Freshness-guided\ninterest refinement for prioritizing valid candidate news at prediction time.\nExtensive experiments on two real-world datasets demonstrate that LIME\nconsistently outperforms a wide range of state-of-the-art news recommendation\nmethods, and its model agnostic strategies significantly improve recommendation\naccuracy.", "AI": {"tldr": "\u63d0\u51faLIME\u6846\u67b6\uff0c\u901a\u8fc7\u8003\u8651\u65b0\u95fb\u7684\u751f\u547d\u5468\u671f\u548c\u7528\u6237\u5174\u8da3\u7684\u6301\u4e45\u6027\u6765\u6539\u8fdb\u4e2a\u6027\u5316\u65b0\u95fb\u63a8\u8350\u3002", "motivation": "\u867d\u7136\u5148\u524d\u7684\u5de5\u4f5c\u5df2\u7ecf\u901a\u8fc7\u6539\u8fdb\u65b0\u95fb\u548c\u7528\u6237\u7684\u8868\u793a\u6765\u6539\u8fdb\u5174\u8da3\u5339\u914d\uff0c\u4f46\u4ee5\u4e0b\u4e0e\u65f6\u95f4\u76f8\u5173\u7684\u6311\u6218\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff1a\uff08C1\uff09\u5229\u7528\u70b9\u51fb\u65b0\u95fb\u7684\u5e74\u9f84\u6765\u63a8\u65ad\u7528\u6237\u7684\u5174\u8da3\u6301\u4e45\u6027\uff0c\u4ee5\u53ca\uff08C2\uff09\u5efa\u6a21\u4e0d\u540c\u4e3b\u9898\u548c\u7528\u6237\u7684\u65b0\u95fb\u7684\u4e0d\u540c\u751f\u547d\u5468\u671f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684Lifetime-aware\u5174\u8da3\u5339\u914d\u6846\u67b6LIME\uff0c\u7528\u4e8e\u65b0\u95fb\u63a8\u8350\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u4e09\u4e2a\u5173\u952e\u7b56\u7565\uff1a\uff081\uff09\u7528\u6237-\u4e3b\u9898lifetime-aware\u5e74\u9f84\u8868\u793a\uff0c\u4ee5\u6355\u83b7\u76f8\u5bf9\u4e8e\u7528\u6237-\u4e3b\u9898\u5bf9\u7684\u65b0\u95fb\u76f8\u5bf9\u5e74\u9f84\uff0c\uff082\uff09\u5019\u9009\u611f\u77e5lifetime\u6ce8\u610f\u529b\uff0c\u7528\u4e8e\u751f\u6210\u65f6\u95f4\u5bf9\u9f50\u7684\u7528\u6237\u8868\u793a\uff0c\u4ee5\u53ca\uff083\uff09\u65b0\u9c9c\u5ea6\u5f15\u5bfc\u7684\u5174\u8da3\u7ec6\u5316\uff0c\u7528\u4e8e\u5728\u9884\u6d4b\u65f6\u4f18\u5148\u8003\u8651\u6709\u6548\u7684\u5019\u9009\u65b0\u95fb\u3002", "result": "LIME\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u5404\u79cd\u6700\u5148\u8fdb\u7684\u65b0\u95fb\u63a8\u8350\u65b9\u6cd5\uff0c\u5e76\u4e14\u5176\u6a21\u578b\u4e0d\u53ef\u77e5\u7b56\u7565\u663e\u7740\u63d0\u9ad8\u4e86\u63a8\u8350\u51c6\u786e\u6027\u3002", "conclusion": "LIME\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u5404\u79cd\u6700\u5148\u8fdb\u7684\u65b0\u95fb\u63a8\u8350\u65b9\u6cd5\uff0c\u5e76\u4e14\u5176\u6a21\u578b\u4e0d\u53ef\u77e5\u7b56\u7565\u663e\u7740\u63d0\u9ad8\u4e86\u63a8\u8350\u51c6\u786e\u6027\u3002"}}
{"id": "2508.11880", "categories": ["cs.LG", "I.2.0; I.5.0"], "pdf": "https://arxiv.org/pdf/2508.11880", "abs": "https://arxiv.org/abs/2508.11880", "authors": ["Yuto Omae"], "title": "PCA- and SVM-Grad-CAM for Convolutional Neural Networks: Closed-form Jacobian Expression", "comment": "15 pages", "summary": "Convolutional Neural Networks (CNNs) are an effective approach for\nclassification tasks, particularly when the training dataset is large. Although\nCNNs have long been considered a black-box classification method, they can be\nused as a white-box method through visualization techniques such as Grad-CAM.\nWhen training samples are limited, incorporating a Principal Component Analysis\n(PCA) layer and/or a Support Vector Machine (SVM) classifier into a CNN can\neffectively improve classification performance. However, traditional Grad-CAM\ncannot be directly applied to PCA and/or SVM layers. It is important to\ngenerate attention regions for PCA and/or SVM layers in CNNs to facilitate the\ndevelopment of white-box methods. Therefore, we propose ``PCA-Grad-CAM'', a\nmethod for visualizing attention regions in PCA feature vectors, and\n``SVM-Grad-CAM'', a method for visualizing attention regions in an SVM\nclassifier layer. To complete our methods analytically, it is necessary to\nsolve the closed-form Jacobian consisting of partial derivatives from the last\nconvolutional layer to the PCA and/or SVM layers. In this paper, we present the\nexact closed-form Jacobian and the visualization results of our methods applied\nto several major datasets.", "AI": {"tldr": "This paper introduces PCA-Grad-CAM and SVM-Grad-CAM to visualize attention regions in PCA and SVM layers of CNNs, addressing the limitations of traditional Grad-CAM when using these layers to improve classification with limited data.", "motivation": "Traditional Grad-CAM cannot be directly applied to PCA and/or SVM layers in CNNs, which limits the development of white-box methods for CNNs when training samples are limited.  Incorporating PCA and/or SVM can improve performance with limited training data, but lacks visualization.", "method": "The paper proposes PCA-Grad-CAM and SVM-Grad-CAM methods for visualizing attention regions in PCA and SVM layers within CNNs. It derives the exact closed-form Jacobian for these layers.", "result": "The paper introduces PCA-Grad-CAM and SVM-Grad-CAM for visualizing attention regions and provides visualization results on major datasets.", "conclusion": "The paper presents exact closed-form Jacobian and visualization results of PCA-Grad-CAM and SVM-Grad-CAM applied to several major datasets."}}
{"id": "2508.11933", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11933", "abs": "https://arxiv.org/abs/2508.11933", "authors": ["Yue Wang", "Liesheng Wei", "Yuxiang Wang"], "title": "CAMF: Collaborative Adversarial Multi-agent Framework for Machine Generated Text Detection", "comment": null, "summary": "Detecting machine-generated text (MGT) from contemporary Large Language\nModels (LLMs) is increasingly crucial amid risks like disinformation and\nthreats to academic integrity. Existing zero-shot detection paradigms, despite\ntheir practicality, often exhibit significant deficiencies. Key challenges\ninclude: (1) superficial analyses focused on limited textual attributes, and\n(2) a lack of investigation into consistency across linguistic dimensions such\nas style, semantics, and logic. To address these challenges, we introduce the\n\\textbf{C}ollaborative \\textbf{A}dversarial \\textbf{M}ulti-agent\n\\textbf{F}ramework (\\textbf{CAMF}), a novel architecture using multiple\nLLM-based agents. CAMF employs specialized agents in a synergistic three-phase\nprocess: \\emph{Multi-dimensional Linguistic Feature Extraction},\n\\emph{Adversarial Consistency Probing}, and \\emph{Synthesized Judgment\nAggregation}. This structured collaborative-adversarial process enables a deep\nanalysis of subtle, cross-dimensional textual incongruities indicative of\nnon-human origin. Empirical evaluations demonstrate CAMF's significant\nsuperiority over state-of-the-art zero-shot MGT detection techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CAMF\uff0c\u4e00\u79cd\u7528\u4e8e\u68c0\u6d4b\u673a\u5668\u751f\u6210\u6587\u672c\u7684\u65b0\u67b6\u6784\uff0c\u5b83\u901a\u8fc7\u534f\u540c\u5bf9\u6297\u8fc7\u7a0b\uff0c\u80fd\u591f\u66f4\u6df1\u5165\u5730\u5206\u6790\u6587\u672c\u4e0d\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u673a\u5668\u751f\u6210\u6587\u672c\uff08MGT\uff09\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u96f6\u6837\u672c\u68c0\u6d4b\u8303\u4f8b\u5b58\u5728\u4e0d\u8db3\uff0c\u5305\u62ec\u5206\u6790\u80a4\u6d45\u548c\u7f3a\u4e4f\u5bf9\u8de8\u8bed\u8a00\u7ef4\u5ea6\u4e00\u81f4\u6027\u7684\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCAMF\u7684\u65b0\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u4f7f\u7528\u591a\u4e2a\u57fa\u4e8eLLM\u7684\u4ee3\u7406\uff0c\u91c7\u7528\u534f\u540c\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a\u591a\u7ef4\u8bed\u8a00\u7279\u5f81\u63d0\u53d6\u3001\u5bf9\u6297\u4e00\u81f4\u6027\u63a2\u6d4b\u548c\u7efc\u5408\u5224\u65ad\u805a\u5408\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cCAMF\u660e\u663e\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672cMGT\u68c0\u6d4b\u6280\u672f\u3002", "conclusion": "CAMF\u5728\u68c0\u6d4b\u673a\u5668\u751f\u6210\u6587\u672c\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2508.11886", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.11886", "abs": "https://arxiv.org/abs/2508.11886", "authors": ["Wenhui Zhu", "Xiwen Chen", "Zhipeng Wang", "Shao Tang", "Sayan Ghosh", "Xuanzhao Dong", "Rajat Koner", "Yalin Wang"], "title": "EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models", "comment": null, "summary": "Instructed Visual Segmentation (IVS) tasks require segmenting objects in\nimages or videos based on natural language instructions. While recent\nmultimodal large language models (MLLMs) have achieved strong performance on\nIVS, their inference cost remains a major bottleneck, particularly in video. We\nempirically analyze visual token sampling in MLLMs and observe a strong\ncorrelation between subset token coverage and segmentation performance. This\nmotivates our design of a simple and effective token pruning method that\nselects a compact yet spatially representative subset of tokens to accelerate\ninference. In this paper, we introduce a novel visual token pruning method for\nIVS, called EVTP-IV, which builds upon the k-center by integrating spatial\ninformation to ensure better coverage. We further provide an\ninformation-theoretic analysis to support our design. Experiments on standard\nIVS benchmarks show that our method achieves up to 5X speed-up on video tasks\nand 3.5X on image tasks, while maintaining comparable accuracy using only 20%\nof the tokens. Our method also consistently outperforms state-of-the-art\npruning baselines under varying pruning ratios.", "AI": {"tldr": "Proposes EVTP-IV, a visual token pruning method for IVS that significantly accelerates inference while maintaining accuracy, achieving up to 5X speed-up on video and 3.5X on image tasks.", "motivation": "Reducing the inference cost of Multimodal Large Language Models (MLLMs) on Instructed Visual Segmentation (IVS) tasks, particularly in video, by addressing the bottleneck of high inference cost.", "method": "A novel visual token pruning method called EVTP-IV, which builds upon the k-center by integrating spatial information.", "result": "The method achieves significant speed-ups (up to 5X on video and 3.5X on image tasks) while maintaining comparable accuracy using only 20% of the tokens and outperforming state-of-the-art pruning baselines.", "conclusion": "The proposed EVTP-IV method achieves up to 5X speed-up on video IVS tasks and 3.5X on image tasks, while maintaining comparable accuracy using only 20% of the tokens, and outperforms state-of-the-art pruning baselines."}}
{"id": "2508.12027", "categories": ["cs.AI", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.12027", "abs": "https://arxiv.org/abs/2508.12027", "authors": ["Filippo Torresan", "Keisuke Suzuki", "Ryota Kanai", "Manuel Baltieri"], "title": "Active inference for action-unaware agents", "comment": "59 pages, 47 figures", "summary": "Active inference is a formal approach to study cognition based on the notion\nthat adaptive agents can be seen as engaging in a process of approximate\nBayesian inference, via the minimisation of variational and expected free\nenergies. Minimising the former provides an account of perceptual processes and\nlearning as evidence accumulation, while minimising the latter describes how\nagents select their actions over time. In this way, adaptive agents are able to\nmaximise the likelihood of preferred observations or states, given a generative\nmodel of the environment. In the literature, however, different strategies have\nbeen proposed to describe how agents can plan their future actions. While they\nall share the notion that some kind of expected free energy offers an\nappropriate way to score policies, sequences of actions, in terms of their\ndesirability, there are different ways to consider the contribution of past\nmotor experience to the agent's future behaviour. In some approaches, agents\nare assumed to know their own actions, and use such knowledge to better plan\nfor the future. In other approaches, agents are unaware of their actions, and\nmust infer their motor behaviour from recent observations in order to plan for\nthe future. This difference reflects a standard point of departure in two\nleading frameworks in motor control based on the presence, or not, of an\nefference copy signal representing knowledge about an agent's own actions. In\nthis work we compare the performances of action-aware and action-unaware agents\nin two navigations tasks, showing how action-unaware agents can achieve\nperformances comparable to action-aware ones while at a severe disadvantage.", "AI": {"tldr": "This paper compares action-aware and action-unaware agents in navigation tasks, finding that action-unaware agents can perform similarly to action-aware ones despite being at a disadvantage.", "motivation": "investigate different strategies for agents to plan their future actions, specifically how past motor experience contributes to future behavior.", "method": "comparing the performances of action-aware and action-unaware agents in two navigations tasks", "result": "action-unaware agents can achieve performances comparable to action-aware ones.", "conclusion": "action-unaware agents can achieve performances comparable to action-aware ones while at a severe disadvantage."}}
{"id": "2508.11999", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11999", "abs": "https://arxiv.org/abs/2508.11999", "authors": ["Daoze Zhang", "Zhanheng Nie", "Jianyu Liu", "Chenghan Fu", "Wanxian Guan", "Yuan Gao", "Jun Song", "Pengjie Wang", "Jian Xu", "Bo Zheng"], "title": "MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding", "comment": "11 pages, 9 figures", "summary": "With the rapid advancement of e-commerce, exploring general representations\nrather than task-specific ones has attracted increasing research attention. For\nproduct understanding, although existing discriminative dual-flow architectures\ndrive progress in this field, they inherently struggle to model the many-to-one\nalignment between multiple images and texts of products. Therefore, we argue\nthat generative Multimodal Large Language Models (MLLMs) hold significant\npotential for improving product representation learning. Nevertheless,\nachieving this goal still remains non-trivial due to several key challenges:\nthe lack of multimodal and aspect-aware modeling modules in typical LLMs; the\ncommon presence of background noise in product images; and the absence of a\nstandard benchmark for evaluation. To address these issues, we propose the\nfirst generative MLLM-based model named MOON for product representation\nlearning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for\ntargeted modeling of multimodal and aspect-specific product content; (2)\neffectively detects core semantic regions in product images to mitigate the\ndistraction and interference caused by background noise; and (3) introduces the\nspecialized negative sampling strategy to increase the difficulty and diversity\nof negative samples. In addition, we release a large-scale multimodal benchmark\nMBE for various product understanding tasks. Experimentally, our model\ndemonstrates competitive zero-shot performance on both our benchmark and the\npublic dataset, showcasing strong generalization across various downstream\ntasks, including cross-modal retrieval, product classification, and attribute\nprediction. Furthermore, the case study and visualization illustrate the\neffectiveness of MOON for product understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMOON\u7684\u751f\u6210\u5f0fMLLM\u6a21\u578b\uff0c\u7528\u4e8e\u4ea7\u54c1\u8868\u793a\u5b66\u4e60\uff0c\u8be5\u6a21\u578b\u91c7\u7528\u5f15\u5bfc\u5f0f\u7684\u6df7\u5408\u4e13\u5bb6\u6a21\u5757\uff0c\u6709\u6548\u5730\u68c0\u6d4b\u4ea7\u54c1\u56fe\u50cf\u4e2d\u7684\u6838\u5fc3\u8bed\u4e49\u533a\u57df\uff0c\u5e76\u5f15\u5165\u4e86\u4e13\u95e8\u7684\u8d1f\u91c7\u6837\u7b56\u7565\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u5e03\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u57fa\u51c6MBE\u3002", "motivation": "\u63a2\u7d22\u901a\u7528\u8868\u793a\u800c\u4e0d\u662f\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u8868\u793a\u5df2\u7ecf\u5f15\u8d77\u8d8a\u6765\u8d8a\u591a\u7684\u7814\u7a76\u5173\u6ce8\u3002\u73b0\u6709\u7684\u5224\u522b\u5f0f\u53cc\u6d41\u67b6\u6784\u5728\u4ea7\u54c1\u7406\u89e3\u9886\u57df\u63a8\u52a8\u4e86\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u5728\u5efa\u6a21\u4ea7\u54c1\u7684\u591a\u4e2a\u56fe\u50cf\u548c\u6587\u672c\u4e4b\u95f4\u7684\u591a\u5bf9\u4e00\u5bf9\u5e94\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u5178\u578b\u7684LLM\u7f3a\u4e4f\u591a\u6a21\u6001\u548c\u65b9\u9762\u611f\u77e5\u7684\u5efa\u6a21\u6a21\u5757\uff1b\u4ea7\u54c1\u56fe\u50cf\u4e2d\u666e\u904d\u5b58\u5728\u80cc\u666f\u566a\u58f0\uff1b\u7f3a\u4e4f\u7528\u4e8e\u8bc4\u4f30\u7684\u6807\u51c6\u57fa\u51c6\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u57fa\u4e8e\u751f\u6210\u5f0fMLLM\u7684\u6a21\u578bMOON\u7528\u4e8e\u4ea7\u54c1\u8868\u793a\u5b66\u4e60\u3002\u6211\u4eec\u7684\u65b9\u6cd5\uff081\uff09\u91c7\u7528\u4e86\u4e00\u79cd\u5f15\u5bfc\u5f0f\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u5757\uff0c\u7528\u4e8e\u6709\u9488\u5bf9\u6027\u5730\u5efa\u6a21\u591a\u6a21\u6001\u548c\u7279\u5b9a\u65b9\u9762\u7684\u4ea7\u54c1\u5185\u5bb9\uff1b\uff082\uff09\u6709\u6548\u5730\u68c0\u6d4b\u4ea7\u54c1\u56fe\u50cf\u4e2d\u7684\u6838\u5fc3\u8bed\u4e49\u533a\u57df\uff0c\u4ee5\u51cf\u8f7b\u80cc\u666f\u566a\u58f0\u9020\u6210\u7684\u5e72\u6270\uff1b\uff083\uff09\u5f15\u5165\u4e86\u4e13\u95e8\u7684\u8d1f\u91c7\u6837\u7b56\u7565\uff0c\u4ee5\u589e\u52a0\u8d1f\u6837\u672c\u7684\u96be\u5ea6\u548c\u591a\u6837\u6027\u3002", "result": "MOON\u6a21\u578b\u5728\u4ea7\u54c1\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "MOON\u6a21\u578b\u5728\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u6709\u7ade\u4e89\u529b\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\uff0c\u5305\u62ec\u8de8\u6a21\u6001\u68c0\u7d22\u3001\u4ea7\u54c1\u5206\u7c7b\u548c\u5c5e\u6027\u9884\u6d4b\u3002\u6848\u4f8b\u7814\u7a76\u548c\u53ef\u89c6\u5316\u8bf4\u660e\u4e86MOON\u5728\u4ea7\u54c1\u7406\u89e3\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.11921", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11921", "abs": "https://arxiv.org/abs/2508.11921", "authors": ["Yibo Zhong"], "title": "ENA: Efficient N-dimensional Attention", "comment": "WIP", "summary": "Efficient modeling of long sequences of high-order data requires a more\nefficient architecture than Transformer. In this paper, we investigate two key\naspects of extending linear recurrent models, especially those originally\ndesigned for language modeling, to high-order data (1D to ND): scanning\nstrategies and attention-hybrid architectures. Empirical results suggest that\nscanning provides limited benefits, while attention-hybrid models yield\npromising results. Focusing on the latter, we further evaluate types of\nattention and find that tiled high-order sliding window attention (SWA) is\nefficient in both theory and practice. We term the resulting hybrid\narchitecture of linear recurrence and high-order SWA as Efficient N-dimensional\nAttention (ENA). We then conduct several experiments to demonstrate its\neffectiveness. The intuition behind ENA is that linear recurrence compresses\nglobal information into a state, while SWA complements it by enforcing strict\nlocal modeling. Together, they form a simple framework that offers a promising\nand practical solution for ultra-long high-order data modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aENA\u7684\u7ebf\u6027\u5faa\u73af\u548c\u9ad8\u9636SWA\u6df7\u5408\u67b6\u6784\uff0c\u7528\u4e8e\u9ad8\u6548\u5efa\u6a21\u8d85\u957f\u9ad8\u9636\u6570\u636e\u3002", "motivation": "Transformer\u5728\u5bf9\u9ad8\u9636\u6570\u636e\u7684\u957f\u5e8f\u5217\u8fdb\u884c\u5efa\u6a21\u65f6\u6548\u7387\u8f83\u4f4e\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u3002", "method": "\u7ebf\u6027\u5faa\u73af\u6a21\u578b\u548c\u9ad8\u9636\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\uff08SWA\uff09\u7684\u6df7\u5408\u67b6\u6784", "result": "\u626b\u63cf\u63d0\u4f9b\u7684\u4f18\u52bf\u6709\u9650\uff0c\u800c\u6ce8\u610f\u529b\u6df7\u5408\u6a21\u578b\u4ea7\u751f\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002\u5206\u5757\u9ad8\u9636\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\uff08SWA\uff09\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u90fd\u662f\u6709\u6548\u7684\u3002", "conclusion": "\u7ebf\u6027\u5faa\u73af\u80fd\u591f\u538b\u7f29\u5168\u5c40\u4fe1\u606f\u5230\u72b6\u6001\u4e2d\uff0c\u800cSWA\u901a\u8fc7\u5f3a\u5236\u6267\u884c\u4e25\u683c\u7684\u5c40\u90e8\u5efa\u6a21\u6765\u5bf9\u5176\u8fdb\u884c\u8865\u5145\u3002\u5b83\u4eec\u5171\u540c\u6784\u6210\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u6846\u67b6\uff0c\u4e3a\u8d85\u957f\u9ad8\u9636\u6570\u636e\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12031", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12031", "abs": "https://arxiv.org/abs/2508.12031", "authors": ["Shaozhe Yin", "Jinyu Guo", "Kai Shuang", "Xia Liu", "Ruize Ou"], "title": "Learning Wisdom from Errors: Promoting LLM's Continual Relation Learning through Exploiting Error Cases", "comment": null, "summary": "Continual Relation Extraction (CRE) aims to continually learn new emerging\nrelations while avoiding catastrophic forgetting. Existing CRE methods mainly\nuse memory replay and contrastive learning to mitigate catastrophic forgetting.\nHowever, these methods do not attach importance to the error cases that can\nreveal the model's cognitive biases more effectively. To address this issue, we\npropose an instruction-based continual contrastive tuning approach for Large\nLanguage Models (LLMs) in CRE. Different from existing CRE methods that\ntypically handle the training and memory data in a unified manner, this\napproach splits the training and memory data of each task into two parts\nrespectively based on the correctness of the initial responses and treats them\ndifferently through dual-task fine-tuning. In addition, leveraging the\nadvantages of LLM's instruction-following ability, we propose a novel\ninstruction-based contrastive tuning strategy for LLM to continuously correct\ncurrent cognitive biases with the guidance of previous data in an\ninstruction-tuning manner, which mitigates the gap between old and new\nrelations in a more suitable way for LLMs. We experimentally evaluate our model\non TACRED and FewRel, and the results show that our model achieves new\nstate-of-the-art CRE performance with significant improvements, demonstrating\nthe importance of specializing in exploiting error cases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6307\u4ee4\u7684\u6301\u7eed\u5bf9\u6bd4\u8c03\u6574\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728CRE\u4e2d\u5e94\u7528\uff0c\u8be5\u65b9\u6cd5\u5728\u9519\u8bef\u6848\u4f8b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728TACRED\u548cFewRel\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684state-of-the-art CRE\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684CRE\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u8bb0\u5fc6\u91cd\u653e\u548c\u5bf9\u6bd4\u5b66\u4e60\u6765\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4f46\u662f\u8fd9\u4e9b\u65b9\u6cd5\u6ca1\u6709\u91cd\u89c6\u80fd\u591f\u66f4\u6709\u6548\u5730\u63ed\u793a\u6a21\u578b\u8ba4\u77e5\u504f\u5dee\u7684\u9519\u8bef\u6848\u4f8b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6307\u4ee4\u7684\u6301\u7eed\u5bf9\u6bd4\u8c03\u6574\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728CRE\u4e2d\u5e94\u7528\u3002\u8be5\u65b9\u6cd5\u5c06\u6bcf\u4e2a\u4efb\u52a1\u7684\u8bad\u7ec3\u548c\u8bb0\u5fc6\u6570\u636e\u5206\u4e3a\u4e24\u90e8\u5206\uff0c\u5e76\u5229\u7528LLM\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6307\u4ee4\u7684\u5bf9\u6bd4\u8c03\u6574\u7b56\u7565\u3002", "result": "\u8be5\u6a21\u578b\u5728TACRED\u548cFewRel\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728TACRED\u548cFewRel\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684state-of-the-art CRE\u6027\u80fd\uff0c\u8868\u660e\u4e86\u4e13\u95e8\u5229\u7528\u9519\u8bef\u6848\u4f8b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.11893", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.11893", "abs": "https://arxiv.org/abs/2508.11893", "authors": ["Quanwei Hu", "Yinggan Tang", "Xuguang Zhang"], "title": "Large Kernel Modulation Network for Efficient Image Super-Resolution", "comment": null, "summary": "Image super-resolution (SR) in resource-constrained scenarios demands\nlightweight models balancing performance and latency. Convolutional neural\nnetworks (CNNs) offer low latency but lack non-local feature capture, while\nTransformers excel at non-local modeling yet suffer slow inference. To address\nthis trade-off, we propose the Large Kernel Modulation Network (LKMN), a pure\nCNN-based model. LKMN has two core components: Enhanced Partial Large Kernel\nBlock (EPLKB) and Cross-Gate Feed-Forward Network (CGFN). The EPLKB utilizes\nchannel shuffle to boost inter-channel interaction, incorporates channel\nattention to focus on key information, and applies large kernel strip\nconvolutions on partial channels for non-local feature extraction with reduced\ncomplexity. The CGFN dynamically adjusts discrepancies between input, local,\nand non-local features via a learnable scaling factor, then employs a\ncross-gate strategy to modulate and fuse these features, enhancing their\ncomplementarity. Extensive experiments demonstrate that our method outperforms\nexisting state-of-the-art (SOTA) lightweight SR models while balancing quality\nand efficiency. Specifically, LKMN-L achieves 0.23 dB PSNR improvement over\nDAT-light on the Manga109 dataset at $\\times$4 upscale, with nearly $\\times$4.8\ntimes faster. Codes are in the supplementary materials. The code is available\nat https://github.com/Supereeeee/LKMN.", "AI": {"tldr": "LKMN, a CNN-based model, balances performance and latency in image super-resolution by using Enhanced Partial Large Kernel Block (EPLKB) and Cross-Gate Feed-Forward Network (CGFN), outperforming existing lightweight SR models.", "motivation": "Image super-resolution (SR) in resource-constrained scenarios demands lightweight models balancing performance and latency. Convolutional neural networks (CNNs) offer low latency but lack non-local feature capture, while Transformers excel at non-local modeling yet suffer slow inference. To address this trade-off", "method": "The Large Kernel Modulation Network (LKMN), a pure CNN-based model with two core components: Enhanced Partial Large Kernel Block (EPLKB) and Cross-Gate Feed-Forward Network (CGFN).", "result": "LKMN-L achieves 0.23 dB PSNR improvement over DAT-light on the Manga109 dataset at $\\times$4 upscale, with nearly $\\times$4.8 times faster.", "conclusion": "LKMN outperforms existing state-of-the-art (SOTA) lightweight SR models while balancing quality and efficiency. Specifically, LKMN-L achieves 0.23 dB PSNR improvement over DAT-light on the Manga109 dataset at $\\times$4 upscale, with nearly $\\times$4.8 times faster."}}
{"id": "2508.12087", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.12087", "abs": "https://arxiv.org/abs/2508.12087", "authors": ["Zhanjiang Yang", "Meng Li", "Yang Shen", "Yueming Li", "Lijun Sun"], "title": "MAPF-World: Action World Model for Multi-Agent Path Finding", "comment": null, "summary": "Multi-agent path finding (MAPF) is the problem of planning conflict-free\npaths from the designated start locations to goal positions for multiple\nagents. It underlies a variety of real-world tasks, including multi-robot\ncoordination, robot-assisted logistics, and social navigation. Recent\ndecentralized learnable solvers have shown great promise for large-scale MAPF,\nespecially when leveraging foundation models and large datasets. However, these\nagents are reactive policy models and exhibit limited modeling of environmental\ntemporal dynamics and inter-agent dependencies, resulting in performance\ndegradation in complex, long-term planning scenarios. To address these\nlimitations, we propose MAPF-World, an autoregressive action world model for\nMAPF that unifies situation understanding and action generation, guiding\ndecisions beyond immediate local observations. It improves situational\nawareness by explicitly modeling environmental dynamics, including spatial\nfeatures and temporal dependencies, through future state and actions\nprediction. By incorporating these predicted futures, MAPF-World enables more\ninformed, coordinated, and far-sighted decision-making, especially in complex\nmulti-agent settings. Furthermore, we augment MAPF benchmarks by introducing an\nautomatic map generator grounded in real-world scenarios, capturing practical\nmap layouts for training and evaluating MAPF solvers. Extensive experiments\ndemonstrate that MAPF-World outperforms state-of-the-art learnable solvers,\nshowcasing superior zero-shot generalization to out-of-distribution cases.\nNotably, MAPF-World is trained with a 96.5% smaller model size and 92% reduced\ndata.", "AI": {"tldr": "\u63d0\u51faMAPF-World\uff0c\u4e00\u79cd\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u8def\u5f84\u5bfb\u627e\u7684\u81ea\u56de\u5f52\u884c\u52a8\u4e16\u754c\u6a21\u578b\uff0c\u5b83\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u72b6\u6001\u548c\u884c\u52a8\u6765\u63d0\u9ad8\u51b3\u7b56\u8d28\u91cf\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u6a21\u578b\u5927\u5c0f\u548c\u6570\u636e\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u6563\u5f0f\u53ef\u5b66\u4e60\u6c42\u89e3\u5668\u5728\u5927\u578bMAPF\u4e2d\u8868\u73b0\u51fa\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u5b83\u4eec\u662f\u53cd\u5e94\u5f0f\u7b56\u7565\u6a21\u578b\uff0c\u5bf9\u73af\u5883\u65f6\u95f4\u52a8\u6001\u548c\u667a\u80fd\u4f53\u95f4\u4f9d\u8d56\u5173\u7cfb\u7684\u5efa\u6a21\u6709\u9650\uff0c\u56e0\u6b64\u5728\u590d\u6742\u7684\u957f\u671f\u89c4\u5212\u573a\u666f\u4e2d\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eMAPF\u7684\u81ea\u56de\u5f52\u884c\u52a8\u4e16\u754c\u6a21\u578bMAPF-World\uff0c\u5b83\u7edf\u4e00\u4e86\u60c5\u5883\u7406\u89e3\u548c\u884c\u52a8\u751f\u6210\uff0c\u4ece\u800c\u6307\u5bfc\u51b3\u7b56\u8d85\u8d8a\u4e86\u76f4\u63a5\u7684\u5c40\u90e8\u89c2\u5bdf\u3002", "result": "MAPF-World\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u73af\u5883\u52a8\u6001\uff0c\u5305\u62ec\u7a7a\u95f4\u7279\u5f81\u548c\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u60c5\u5883\u611f\u77e5\u80fd\u529b\u3002\u901a\u8fc7\u7ed3\u5408\u8fd9\u4e9b\u9884\u6d4b\u7684\u672a\u6765\uff0cMAPF-World\u80fd\u591f\u505a\u51fa\u66f4\u660e\u667a\u3001\u66f4\u534f\u8c03\u548c\u66f4\u5177\u8fdc\u89c1\u7684\u51b3\u7b56\u3002\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u573a\u666f\u7684\u81ea\u52a8\u5730\u56fe\u751f\u6210\u5668\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30MAPF\u6c42\u89e3\u5668\u3002MAPF-World\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u53ef\u5b66\u4e60\u6c42\u89e3\u5668\u3002", "conclusion": "MAPF-World\u5728\u590d\u6742\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u53ef\u5b66\u4e60\u7684\u6c42\u89e3\u5668\uff0c\u5e76\u5728\u96f6\u6837\u672c\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002MAPF-World\u7684\u8bad\u7ec3\u6a21\u578b\u89c4\u6a21\u51cf\u5c11\u4e8696.5%\uff0c\u6570\u636e\u91cf\u51cf\u5c11\u4e8692%\u3002"}}
{"id": "2508.12282", "categories": ["cs.CL", "cs.IR", "68T50, 68P20", "I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2508.12282", "abs": "https://arxiv.org/abs/2508.12282", "authors": ["Ziyang Chen", "Erxue Min", "Xiang Zhao", "Yunxin Li", "Xin Jia", "Jinzhi Liao", "Jichao Li", "Shuaiqiang Wang", "Baotian Hu", "Dawei Yin"], "title": "A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation", "comment": "10 pages, 5 figures", "summary": "We introduce ChronoQA, a large-scale benchmark dataset for Chinese question\nanswering, specifically designed to evaluate temporal reasoning in\nRetrieval-Augmented Generation (RAG) systems. ChronoQA is constructed from over\n300,000 news articles published between 2019 and 2024, and contains 5,176\nhigh-quality questions covering absolute, aggregate, and relative temporal\ntypes with both explicit and implicit time expressions. The dataset supports\nboth single- and multi-document scenarios, reflecting the real-world\nrequirements for temporal alignment and logical consistency. ChronoQA features\ncomprehensive structural annotations and has undergone multi-stage validation,\nincluding rule-based, LLM-based, and human evaluation, to ensure data quality.\nBy providing a dynamic, reliable, and scalable resource, ChronoQA enables\nstructured evaluation across a wide range of temporal tasks, and serves as a\nrobust benchmark for advancing time-sensitive retrieval-augmented question\nanswering systems.", "AI": {"tldr": "ChronoQA \u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u4e2d\u6587\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30 RAG \u7cfb\u7edf\u4e2d\u7684\u65f6\u95f4\u63a8\u7406\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u7cfb\u7edf\u4e2d\u7684\u65f6\u95f4\u63a8\u7406\uff0c\u6211\u4eec\u5f15\u5165\u4e86 ChronoQA\uff0c\u8fd9\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u4e2d\u6587\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "ChronoQA \u662f\u4ece 2019 \u5e74\u81f3 2024 \u5e74\u95f4\u53d1\u5e03\u7684\u8d85\u8fc7 300,000 \u7bc7\u65b0\u95fb\u6587\u7ae0\u6784\u5efa\u7684\uff0c\u5305\u542b 5,176 \u4e2a\u9ad8\u8d28\u91cf\u95ee\u9898\uff0c\u6db5\u76d6\u7edd\u5bf9\u3001\u805a\u5408\u548c\u76f8\u5bf9\u65f6\u95f4\u7c7b\u578b\uff0c\u5305\u62ec\u663e\u5f0f\u548c\u9690\u5f0f\u65f6\u95f4\u8868\u8fbe\u3002", "result": "ChronoQA \u5177\u6709\u5168\u9762\u7684\u7ed3\u6784\u6ce8\u91ca\uff0c\u5e76\u7ecf\u8fc7\u4e86\u591a\u9636\u6bb5\u9a8c\u8bc1\uff0c\u5305\u62ec\u57fa\u4e8e\u89c4\u5219\u7684\u3001\u57fa\u4e8e LLM \u7684\u548c\u4eba\u5de5\u8bc4\u4f30\uff0c\u4ee5\u786e\u4fdd\u6570\u636e\u8d28\u91cf\u3002\u8be5\u6570\u636e\u96c6\u652f\u6301\u5355\u6587\u6863\u548c\u591a\u6587\u6863\u573a\u666f\uff0c\u53cd\u6620\u4e86\u5bf9\u65f6\u95f4\u5bf9\u9f50\u548c\u903b\u8f91\u4e00\u81f4\u6027\u7684\u5b9e\u9645\u8981\u6c42\u3002", "conclusion": "ChronoQA \u901a\u8fc7\u63d0\u4f9b\u52a8\u6001\u3001\u53ef\u9760\u548c\u53ef\u6269\u5c55\u7684\u8d44\u6e90\uff0c\u80fd\u591f\u5bf9\u5404\u79cd\u65f6\u95f4\u4efb\u52a1\u8fdb\u884c\u7ed3\u6784\u5316\u8bc4\u4f30\uff0c\u5e76\u4f5c\u4e3a\u63a8\u8fdb\u65f6\u95f4\u654f\u611f\u7684\u68c0\u7d22\u589e\u5f3a\u95ee\u7b54\u7cfb\u7edf\u7684\u5f3a\u5927\u57fa\u51c6\u3002"}}
{"id": "2508.11923", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11923", "abs": "https://arxiv.org/abs/2508.11923", "authors": ["Yan Wu", "Lihong Pei", "Yukai Han", "Yang Cao", "Yu Kang", "Yanlong Zhao"], "title": "Scale-Disentangled spatiotemporal Modeling for Long-term Traffic Emission Forecasting", "comment": null, "summary": "Long-term traffic emission forecasting is crucial for the comprehensive\nmanagement of urban air pollution. Traditional forecasting methods typically\nconstruct spatiotemporal graph models by mining spatiotemporal dependencies to\npredict emissions. However, due to the multi-scale entanglement of traffic\nemissions across time and space, these spatiotemporal graph modeling method\ntend to suffer from cascading error amplification during long-term inference.\nTo address this issue, we propose a Scale-Disentangled Spatio-Temporal Modeling\n(SDSTM) framework for long-term traffic emission forecasting. It leverages the\npredictability differences across multiple scales to decompose and fuse\nfeatures at different scales, while constraining them to remain independent yet\ncomplementary. Specifically, the model first introduces a dual-stream feature\ndecomposition strategy based on the Koopman lifting operator. It lifts the\nscale-coupled spatiotemporal dynamical system into an infinite-dimensional\nlinear space via Koopman operator, and delineates the predictability boundary\nusing gated wavelet decomposition. Then a novel fusion mechanism is\nconstructed, incorporating a dual-stream independence constraint based on\ncross-term loss to dynamically refine the dual-stream prediction results,\nsuppress mutual interference, and enhance the accuracy of long-term traffic\nemission prediction. Extensive experiments conducted on a road-level traffic\nemission dataset within Xi'an's Second Ring Road demonstrate that the proposed\nmodel achieves state-of-the-art performance.", "AI": {"tldr": "SDSTM model addresses cascading error amplification in long-term traffic emission forecasting by disentangling and fusing features at different scales with a dual-stream independence constraint.", "motivation": "Traditional forecasting methods suffer from cascading error amplification during long-term inference due to the multi-scale entanglement of traffic emissions across time and space.", "method": "A Scale-Disentangled Spatio-Temporal Modeling (SDSTM) framework leveraging the predictability differences across multiple scales to decompose and fuse features, incorporating a dual-stream independence constraint based on cross-term loss.", "result": "The SDSTM model achieves state-of-the-art performance.", "conclusion": "The proposed SDSTM model achieves state-of-the-art performance on a road-level traffic emission dataset within Xi'an's Second Ring Road."}}
{"id": "2508.12040", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12040", "abs": "https://arxiv.org/abs/2508.12040", "authors": ["Jinyi Han", "Tingyun Li", "Shisong Chen", "Jie Shi", "Xinyi Wang", "Guanglei Yue", "Jiaqing Liang", "Xin Lin", "Liqian Wen", "Zulong Chen", "Yanghua Xiao"], "title": "Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation", "comment": "The initial versin was made in August 2024", "summary": "While large language models (LLMs) have demonstrated remarkable performance\nacross diverse tasks, they fundamentally lack self-awareness and frequently\nexhibit overconfidence, assigning high confidence scores to incorrect\npredictions. Accurate confidence estimation is therefore critical for enhancing\nthe trustworthiness and reliability of LLM-generated outputs. However, existing\napproaches suffer from coarse-grained scoring mechanisms that fail to provide\nfine-grained, continuous confidence estimates throughout the generation\nprocess. To address these limitations, we introduce FineCE, a novel confidence\nestimation method that delivers accurate, fine-grained confidence scores during\ntext generation. Specifically, we first develop a comprehensive pipeline for\nconstructing training data that effectively captures the underlying\nprobabilistic distribution of LLM responses, and then train a model to predict\nconfidence scores for arbitrary text sequences in a supervised manner.\nFurthermore, we propose a Backward Confidence Integration (BCI) strategy that\nleverages information from the subsequent text to enhance confidence estimation\nfor the current sequence during inference. We also introduce three strategies\nfor identifying optimal positions to perform confidence estimation within the\ngeneration process. Extensive experiments on multiple benchmark datasets\ndemonstrate that FineCE consistently outperforms existing classical confidence\nestimation methods. Our code and all baselines used in the paper are available\non GitHub.", "AI": {"tldr": "FineCE is a new confidence estimation method that provides accurate, fine-grained confidence scores during text generation, outperforming existing methods.", "motivation": "Existing approaches suffer from coarse-grained scoring mechanisms that fail to provide fine-grained, continuous confidence estimates throughout the generation process. Accurate confidence estimation is critical for enhancing the trustworthiness and reliability of LLM-generated outputs, as LLMs lack self-awareness and frequently exhibit overconfidence.", "method": "FineCE: a novel confidence estimation method that delivers accurate, fine-grained confidence scores during text generation. It includes a comprehensive pipeline for constructing training data and a Backward Confidence Integration (BCI) strategy.", "result": "FineCE delivers accurate, fine-grained confidence scores during text generation.", "conclusion": "FineCE consistently outperforms existing classical confidence estimation methods."}}
{"id": "2508.11902", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11902", "abs": "https://arxiv.org/abs/2508.11902", "authors": ["Azam Nouri"], "title": "A Sobel-Gradient MLP Baseline for Handwritten Character Recognition", "comment": "This paper is under consideration at Pattern Recognition Letters", "summary": "We revisit the classical Sobel operator to ask a simple question: Are\nfirst-order edge maps sufficient to drive an all-dense multilayer perceptron\n(MLP) for handwritten character recognition (HCR), as an alternative to\nconvolutional neural networks (CNNs)? Using only horizontal and vertical Sobel\nderivatives as input, we train an MLP on MNIST and EMNIST Letters. Despite its\nextreme simplicity, the resulting network reaches 98% accuracy on MNIST digits\nand 92% on EMNIST letters -- approaching CNNs while offering a smaller memory\nfootprint and transparent features. Our findings highlight that much of the\nclass-discriminative information in handwritten character images is already\ncaptured by first-order gradients, making edge-aware MLPs a compelling option\nfor HCR.", "AI": {"tldr": "\u4f7f\u7528 Sobel \u7b97\u5b50\u548c MLP\uff0c\u624b\u5199\u5b57\u7b26\u8bc6\u522b\u4e5f\u80fd\u8fbe\u5230\u63a5\u8fd1 CNN \u7684\u6548\u679c\uff0c\u4f46\u5185\u5b58\u5360\u7528\u66f4\u5c0f\uff0c\u7279\u5f81\u66f4\u900f\u660e\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6\u7ecf\u5178\u7684 Sobel \u7b97\u5b50\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u95ee\u9898\uff1a\u4f5c\u4e3a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e00\u9636\u8fb9\u7f18\u56fe\u662f\u5426\u8db3\u4ee5\u9a71\u52a8\u7528\u4e8e\u624b\u5199\u5b57\u7b26\u8bc6\u522b (HCR) \u7684\u5168\u5bc6\u96c6\u591a\u5c42\u611f\u77e5\u5668 (MLP)\uff1f", "method": "\u4f7f\u7528\u6c34\u5e73\u548c\u5782\u76f4 Sobel \u5bfc\u6570\u4f5c\u4e3a\u8f93\u5165\uff0c\u5728 MNIST \u548c EMNIST Letters \u4e0a\u8bad\u7ec3\u4e86\u4e00\u4e2a MLP\u3002", "result": "\u6240\u5f97\u5230\u7684\u7f51\u7edc\u5728 MNIST \u6570\u5b57\u4e0a\u8fbe\u5230\u4e86 98% \u7684\u51c6\u786e\u7387\uff0c\u5728 EMNIST \u5b57\u6bcd\u4e0a\u8fbe\u5230\u4e86 92% \u7684\u51c6\u786e\u7387\u2014\u2014\u63a5\u8fd1 CNN\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u5c0f\u7684\u5185\u5b58\u5360\u7528\u548c\u900f\u660e\u7684\u7279\u5f81\u3002", "conclusion": "\u624b\u5199\u5b57\u7b26\u56fe\u50cf\u4e2d\u7684\u5927\u90e8\u5206\u7c7b\u522b\u5224\u522b\u4fe1\u606f\u5df2\u7ecf\u88ab\u4e00\u9636\u68af\u5ea6\u6355\u83b7\uff0c\u8fd9\u4f7f\u5f97\u8fb9\u7f18\u611f\u77e5 MLP \u6210\u4e3a HCR \u7684\u4e00\u4e2a\u5f15\u4eba\u6ce8\u76ee\u7684\u9009\u62e9\u3002"}}
{"id": "2508.12100", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12100", "abs": "https://arxiv.org/abs/2508.12100", "authors": ["Daniel Burkhardt", "Xiangwei Cheng"], "title": "Overcoming Knowledge Discrepancies: Structuring Reasoning Threads through Knowledge Balancing in Interactive Scenarios", "comment": "13 pages, 1 figure, 6 tables", "summary": "Reasoning in interactive problem solving scenarios requires models to\nconstruct reasoning threads that reflect user understanding and align with\nstructured domain knowledge. However, current reasoning models often lack\nexplicit semantic hierarchies, user-domain knowledge alignment, and principled\nmechanisms to prune reasoning threads for effectiveness. These limitations\nresult in lengthy generic output that does not guide users through\ngoal-oriented reasoning steps. To address this, we propose a\nprototype-inspired, two-phases Reasoning-Threads-Evaluation (ReT-Eval)\nframework, drawing inspiration from human-like reasoning strategies that\nemphasize structured knowledge reuse. In the first phase, semantically relevant\nknowledge structures are extracted from a sparse domain knowledge graph using a\ngraph neural network and enriched with intrinsic large language model knowledge\nto resolve knowledge discrepancies. In the second phase, these threads are\nevaluated and pruned using a reward-guided strategy aimed at maintaining\nsemantic coherence to generate effective reasoning threads. Experiments and\nexpert evaluations show that ReT-Eval enhances user understanding and\noutperforms state-of-the-art reasoning models.", "AI": {"tldr": "\u63d0\u51fa\u4e86 ReT-Eval \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4ece\u9886\u57df\u77e5\u8bc6\u56fe\u4e2d\u63d0\u53d6\u76f8\u5173\u7684\u77e5\u8bc6\u7ed3\u6784\uff0c\u5e76\u4f7f\u7528\u5956\u52b1\u5f15\u5bfc\u7b56\u7565\u8fdb\u884c\u8bc4\u4f30\u548c\u4fee\u526a\uff0c\u4ee5\u751f\u6210\u6709\u6548\u7684\u63a8\u7406\u94fe\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u7406\u6a21\u578b\u901a\u5e38\u7f3a\u4e4f\u660e\u786e\u7684\u8bed\u4e49\u5c42\u6b21\u7ed3\u6784\u3001\u7528\u6237\u9886\u57df\u77e5\u8bc6\u5bf9\u9f50\u548c\u6709\u6548\u7684\u526a\u679d\u63a8\u7406\u94fe\u673a\u5236\u3002\u8fd9\u4e9b\u9650\u5236\u5bfc\u81f4\u5197\u957f\u7684\u901a\u7528\u8f93\u51fa\uff0c\u4e0d\u80fd\u6307\u5bfc\u7528\u6237\u5b8c\u6210\u9762\u5411\u76ee\u6807\u7684\u63a8\u7406\u6b65\u9aa4\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u539f\u578b\u9a71\u52a8\u7684\uff0c\u4e24\u9636\u6bb5\u7684\u63a8\u7406\u94fe\u8bc4\u4f30 (ReT-Eval) \u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u548c\u4e13\u5bb6\u8bc4\u4f30\u8868\u660e ReT-Eval \u589e\u5f3a\u4e86\u7528\u6237\u7406\u89e3\u5e76\u4e14\u8d85\u8fc7\u4e86\u73b0\u6709\u6700\u4f18\u63a8\u7406\u6a21\u578b\u3002", "conclusion": "ReT-Eval \u589e\u5f3a\u4e86\u7528\u6237\u7406\u89e3\u5e76\u4e14\u8d85\u8fc7\u4e86\u73b0\u6709\u6700\u4f18\u63a8\u7406\u6a21\u578b\u3002"}}
{"id": "2508.11931", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11931", "abs": "https://arxiv.org/abs/2508.11931", "authors": ["Tim van Erven", "Jack Mayo", "Julia Olkhovskaya", "Chen-Yu Wei"], "title": "An Improved Algorithm for Adversarial Linear Contextual Bandits via Reduction", "comment": null, "summary": "We present an efficient algorithm for linear contextual bandits with\nadversarial losses and stochastic action sets. Our approach reduces this\nsetting to misspecification-robust adversarial linear bandits with fixed action\nsets. Without knowledge of the context distribution or access to a context\nsimulator, the algorithm achieves $\\tilde{O}(\\min\\{d^2\\sqrt{T}, \\sqrt{d^3T\\log\nK}\\})$ regret and runs in $\\text{poly}(d,C,T)$ time, where $d$ is the feature\ndimension, $C$ is an upper bound on the number of linear constraints defining\nthe action set in each round, $K$ is an upper bound on the number of actions in\neach round, and $T$ is number of rounds. This resolves the open question by Liu\net al. (2023) on whether one can obtain $\\text{poly}(d)\\sqrt{T}$ regret in\npolynomial time independent of the number of actions. For the important class\nof combinatorial bandits with adversarial losses and stochastic action sets\nwhere the action sets can be described by a polynomial number of linear\nconstraints, our algorithm is the first to achieve $\\text{poly}(d)\\sqrt{T}$\nregret in polynomial time, while no prior algorithm achieves even $o(T)$ regret\nin polynomial time to our knowledge. When a simulator is available, the regret\nbound can be improved to $\\tilde{O}(d\\sqrt{L^\\star})$, where $L^\\star$ is the\ncumulative loss of the best policy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u89e3\u51b3\u5177\u6709\u5bf9\u6297\u6027\u635f\u5931\u548c\u968f\u673a\u52a8\u4f5c\u96c6\u7684\u7ebf\u6027\u4e0a\u4e0b\u6587bandit\u7684\u6709\u6548\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u5b9e\u73b0\u4e86$\\text{poly}(d)\\\\sqrt{T}$\u9057\u61be\u3002", "motivation": "Liu et al. (2023) \u63d0\u51fa\u4e86\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\uff0c\u5373\u662f\u5426\u53ef\u4ee5\u5728\u72ec\u7acb\u4e8e\u52a8\u4f5c\u6570\u91cf\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u83b7\u5f97$\\\\text{poly}(d)\\\\sqrt{T}$\u9057\u61be\u3002", "method": "\u5c06\u8fd9\u79cd\u8bbe\u7f6e\u7b80\u5316\u4e3a\u5177\u6709\u56fa\u5b9a\u52a8\u4f5c\u96c6\u7684\u9519\u8bef\u6307\u5b9a\u9c81\u68d2\u5bf9\u6297\u7ebf\u6027bandit\u3002", "result": "\u8be5\u7b97\u6cd5\u5b9e\u73b0\u4e86$\\\\tilde{O}(\\\\min\\\\{d^2\\\\sqrt{T}, \\\\sqrt{d^3T\\\\log K}\\\\})$\u7684\u9057\u61be\uff0c\u5e76\u5728$\\\\text{poly}(d,C,T)$\u65f6\u95f4\u5185\u8fd0\u884c\u3002", "conclusion": "\u5bf9\u4e8e\u5177\u6709\u5bf9\u6297\u6027\u635f\u5931\u548c\u968f\u673a\u52a8\u4f5c\u96c6\u7684\u7ec4\u5408bandit\u7684\u91cd\u8981\u7c7b\u522b\uff0c\u5176\u4e2d\u52a8\u4f5c\u96c6\u53ef\u4ee5\u7528\u591a\u9879\u5f0f\u6570\u91cf\u7684\u7ebf\u6027\u7ea6\u675f\u6765\u63cf\u8ff0\uff0c\u8be5\u7b97\u6cd5\u662f\u7b2c\u4e00\u4e2a\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u5b9e\u73b0$\\\\text{poly}(d)\\\\sqrt{T}$\u9057\u61be\u7684\u7b97\u6cd5\uff0c\u800c\u636e\u6211\u4eec\u6240\u77e5\uff0c\u6ca1\u6709\u5148\u524d\u7684\u7b97\u6cd5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u5b9e\u73b0\u751a\u81f3$o(T)$\u9057\u61be\u3002\u5f53\u6a21\u62df\u5668\u53ef\u7528\u65f6\uff0c\u9057\u61be\u754c\u53ef\u4ee5\u63d0\u9ad8\u5230$\\\\tilde{O}(d\\\\sqrt{L^\\\\star})$\uff0c\u5176\u4e2d$L^\\\\star$\u662f\u6700\u4f73\u7b56\u7565\u7684\u7d2f\u79ef\u635f\u5931\u3002"}}
{"id": "2508.12086", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50, 90C29, 62F07", "I.2.7; I.2.6; G.1.6"], "pdf": "https://arxiv.org/pdf/2508.12086", "abs": "https://arxiv.org/abs/2508.12086", "authors": ["Yao Wu"], "title": "J6: Jacobian-Driven Role Attribution for Multi-Objective Prompt Optimization in LLMs", "comment": "9 pages, 3 tables, 1 algorithm", "summary": "In large language model (LLM) adaptation, balancing multiple optimization\nobjectives such as improving factuality (heat) and increasing confidence (via\nlow entropy) poses a fundamental challenge, especially when prompt parameters\n(e.g., hidden-layer insertions h and embedding modifications w) interact in\nnon-trivial ways. Existing multi-objective optimization strategies often rely\non scalar gradient aggregation, ignoring the deeper geometric structure between\nobjectives and parameters. We propose J6, a structured Jacobian-based method\nthat decomposes the gradient interaction matrix into six interpretable\ncomponents. This decomposition enables both hard decision-making (e.g.,\nchoosing the dominant update direction via argmax) and soft strategies (e.g.,\nattention-style weighting via softmax over J6), forming a dynamic update\nframework that adapts to local conflict and synergy. Moreover, the\ninterpretable structure of J6 provides insight into parameter attribution, task\ninterference, and geometry-aligned adaptation. Our work introduces a principled\nand extensible mechanism for conflict-aware prompt optimization, and opens a\nnew avenue for incorporating structured Jacobian reasoning into multi-objective\nneural tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u5316\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u51b2\u7a81\u611f\u77e5\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9002\u5e94\u4e2d\u7684\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u9002\u5e94\u4e2d\uff0c\u5e73\u8861\u591a\u4e2a\u4f18\u5316\u76ee\u6807\uff08\u4f8b\u5982\u63d0\u9ad8\u4e8b\u5b9e\u6027\u548c\u589e\u52a0\u7f6e\u4fe1\u5ea6\uff09\u662f\u4e00\u4e2a\u6839\u672c\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u63d0\u793a\u53c2\u6570\u4ee5\u975e\u5e73\u51e1\u7684\u65b9\u5f0f\u4ea4\u4e92\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a J6 \u7684\u7ed3\u6784\u5316\u96c5\u53ef\u6bd4\u77e9\u9635\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u68af\u5ea6\u4ea4\u4e92\u77e9\u9635\u5206\u89e3\u4e3a\u516d\u4e2a\u53ef\u89e3\u91ca\u7684\u7ec4\u6210\u90e8\u5206\uff0c\u4ee5\u5b9e\u73b0\u52a8\u6001\u66f4\u65b0\u6846\u67b6\u3002", "result": "J6 \u7684\u53ef\u89e3\u91ca\u7ed3\u6784\u63d0\u4f9b\u4e86\u5bf9\u53c2\u6570\u5f52\u56e0\u3001\u4efb\u52a1\u5e72\u6d89\u548c\u51e0\u4f55\u5bf9\u9f50\u9002\u5e94\u7684\u6df1\u5165\u4e86\u89e3\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7ed3\u6784\u5316\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u9002\u5e94\u4e2d\u7684\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u4e3a\u51b2\u7a81\u611f\u77e5\u7684\u63d0\u793a\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u539f\u5219\u4e14\u53ef\u6269\u5c55\u7684\u673a\u5236\u3002"}}
{"id": "2508.11903", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11903", "abs": "https://arxiv.org/abs/2508.11903", "authors": ["Runhao Zeng", "Jiaqi Mao", "Minghao Lai", "Minh Hieu Phan", "Yanjie Dong", "Wei Wang", "Qi Chen", "Xiping Hu"], "title": "OVG-HQ: Online Video Grounding with Hybrid-modal Queries", "comment": "Accepted to ICCV 2025", "summary": "Video grounding (VG) task focuses on locating specific moments in a video\nbased on a query, usually in text form. However, traditional VG struggles with\nsome scenarios like streaming video or queries using visual cues. To fill this\ngap, we present a new task named Online Video Grounding with Hybrid-modal\nQueries (OVG-HQ), which enables online segment localization using text, images,\nvideo segments, and their combinations. This task poses two new challenges:\nlimited context in online settings and modality imbalance during training,\nwhere dominant modalities overshadow weaker ones. To address these, we propose\nOVG-HQ-Unify, a unified framework featuring a Parametric Memory Block (PMB)\nthat retain previously learned knowledge to enhance current decision and a\ncross-modal distillation strategy that guides the learning of non-dominant\nmodalities. This design enables a single model to effectively handle\nhybrid-modal queries. Due to the lack of suitable datasets, we construct\nQVHighlights-Unify, an expanded dataset with multi-modal queries. Besides,\nsince offline metrics overlook prediction timeliness, we adapt them to the\nonline setting, introducing oR@n, IoU=m, and online mean Average Precision\n(omAP) to evaluate both accuracy and efficiency. Experiments show that our\nOVG-HQ-Unify outperforms existing models, offering a robust solution for\nonline, hybrid-modal video grounding. Source code and datasets are available at\nhttps://github.com/maojiaqi2324/OVG-HQ.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5728\u7ebf\u89c6\u9891\u6df7\u5408\u6a21\u6001\u67e5\u8be2\u5b9a\u4f4d (OVG-HQ) \u4efb\u52a1\uff0c\u5e76\u6784\u5efa\u4e86 OVG-HQ-Unify \u6a21\u578b\u548c QVHighlights-Unify \u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u5728\u7ebf\u73af\u5883\u4e2d\u7684\u4e0a\u4e0b\u6587\u9650\u5236\u548c\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u5b9a\u4f4d\u5728\u6d41\u5a92\u4f53\u89c6\u9891\u6216\u4f7f\u7528\u89c6\u89c9\u7ebf\u7d22\u7684\u67e5\u8be2\u7b49\u573a\u666f\u4e2d\u5b58\u5728\u4e0d\u8db3\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u4efb\u52a1\uff0c\u540d\u4e3a\u5728\u7ebf\u89c6\u9891\u6df7\u5408\u6a21\u6001\u67e5\u8be2\u5b9a\u4f4d (OVG-HQ)\u3002", "method": "\u63d0\u51fa\u4e86 OVG-HQ-Unify \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5177\u6709\u53c2\u6570\u8bb0\u5fc6\u5757 (PMB) \u548c\u8de8\u6a21\u6001\u84b8\u998f\u7b56\u7565\u3002", "result": "OVG-HQ-Unify \u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86 oR@n, IoU=m, \u548c online mean Average Precision (omAP) \u7b49\u5728\u7ebf\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "OVG-HQ-Unify \u6a21\u578b\u5728\u5728\u7ebf\u6df7\u5408\u6a21\u6001\u89c6\u9891\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u4e3a\u8be5\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12149", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12149", "abs": "https://arxiv.org/abs/2508.12149", "authors": ["Haochen You", "Baojing Liu"], "title": "MOVER: Multimodal Optimal Transport with Volume-based Embedding Regularization", "comment": "Accepted as a conference paper at CIKM 2025", "summary": "Recent advances in multimodal learning have largely relied on pairwise\ncontrastive objectives to align different modalities, such as text, video, and\naudio, in a shared embedding space. While effective in bi-modal setups, these\napproaches struggle to generalize across multiple modalities and often lack\nsemantic structure in high-dimensional spaces. In this paper, we propose MOVER,\na novel framework that combines optimal transport-based soft alignment with\nvolume-based geometric regularization to build semantically aligned and\nstructured multimodal representations. By integrating a transport-guided\nmatching mechanism with a geometric volume minimization objective (GAVE), MOVER\nencourages consistent alignment across all modalities in a modality-agnostic\nmanner. Experiments on text-video-audio retrieval tasks demonstrate that MOVER\nsignificantly outperforms prior state-of-the-art methods in both zero-shot and\nfinetuned settings. Additional analysis shows improved generalization to unseen\nmodality combinations and stronger structural consistency in the learned\nembedding space.", "AI": {"tldr": "MOVER\u7ed3\u5408\u6700\u4f18\u4f20\u8f93\u548c\u51e0\u4f55\u6b63\u5219\u5316\uff0c\u6784\u5efa\u4e86\u8bed\u4e49\u5bf9\u9f50\u548c\u7ed3\u6784\u5316\u7684\u591a\u6a21\u6001\u8868\u793a\uff0c\u5e76\u5728\u591a\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8epairwise contrastive objectives\uff0c\u4ee5\u5728\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u4e0d\u540c\u7684\u6a21\u6001\uff08\u4f8b\u5982\u6587\u672c\u3001\u89c6\u9891\u548c\u97f3\u9891\uff09\u3002\u867d\u7136\u5728bi-modal setups\u4e2d\u6709\u6548\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u96be\u4ee5\u5728\u591a\u4e2a\u6a21\u6001\u4e2d\u63a8\u5e7f\uff0c\u5e76\u4e14\u901a\u5e38\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7f3a\u4e4f\u8bed\u4e49\u7ed3\u6784\u3002", "method": "\u7ed3\u5408\u4e86\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u8f6f\u5bf9\u9f50\u548c\u57fa\u4e8e\u4f53\u79ef\u7684\u51e0\u4f55\u6b63\u5219\u5316\uff0c\u6784\u5efa\u4e86\u8bed\u4e49\u5bf9\u9f50\u548c\u7ed3\u6784\u5316\u7684\u591a\u6a21\u6001\u8868\u793a\u3002\u901a\u8fc7\u5c06\u4f20\u8f93\u5f15\u5bfc\u7684\u5339\u914d\u673a\u5236\u4e0e\u51e0\u4f55\u4f53\u79ef\u6700\u5c0f\u5316\u76ee\u6807\uff08GAVE\uff09\u76f8\u7ed3\u5408\uff0cMOVER\u9f13\u52b1\u4ee5modality-agnostic\u7684\u65b9\u5f0f\u5728\u6240\u6709\u6a21\u6001\u4e2d\u4fdd\u6301\u4e00\u81f4\u7684\u5bf9\u9f50\u3002", "result": "\u5728text-video-audio\u68c0\u7d22\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMOVER\u663e\u8457\u4f18\u4e8e\u5148\u524dstate-of-the-art\u7684\u65b9\u6cd5\u3002", "conclusion": "MOVER\u5728zero-shot\u548cfinetuned\u8bbe\u7f6e\u4e0b\uff0c\u663e\u8457\u4f18\u4e8e\u5148\u524dstate-of-the-art\u7684\u65b9\u6cd5\u3002\u5b83\u8fd8\u80fd\u66f4\u597d\u5730\u6cdb\u5316\u5230unseen modality combinations\uff0c\u5e76\u5728\u5b66\u4e60\u5230\u7684\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5177\u6709\u66f4\u5f3a\u7684\u7ed3\u6784\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.13107", "categories": ["cs.CL", "cs.IR", "F.2.2, H.3.3, I.2.7"], "pdf": "https://arxiv.org/pdf/2508.13107", "abs": "https://arxiv.org/abs/2508.13107", "authors": ["Figarri Keisha", "Prince Singh", "Pallavi", "Dion Fernandes", "Aravindh Manivannan", "Ilham Wicaksono", "Faisal Ahmad"], "title": "All for law and law for all: Adaptive RAG Pipeline for Legal Research", "comment": "submitted to NLLP 2025 Workshop", "summary": "Retrieval-Augmented Generation (RAG) mitigates hallucinations by grounding\nlarge language model outputs in cited sources, a capability that is especially\ncritical in the legal domain. We present an end-to-end RAG pipeline that\nrevisits and extends the LegalBenchRAG baseline with three targeted\nenhancements: (i) a context-aware query translator that disentangles document\nreferences from natural-language questions and adapts retrieval depth and\nresponse style based on expertise and specificity, (ii) open-source retrieval\nstrategies using SBERT and GTE embeddings that achieve substantial performance\ngains (improving Recall@K by 30-95\\% and Precision@K by $\\sim$2.5$\\times$ for\n$K>4$) while remaining cost-efficient, and (iii) a comprehensive evaluation and\ngeneration framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall to\nassess semantic alignment and faithfulness across models and prompt designs.\nOur results show that carefully designed open-source pipelines can rival or\noutperform proprietary approaches in retrieval quality, while a custom\nlegal-grounded prompt consistently produces more faithful and contextually\nrelevant answers than baseline prompting. Taken together, these contributions\ndemonstrate the potential of task-aware, component-level tuning to deliver\nlegally grounded, reproducible, and cost-effective RAG systems for legal\nresearch assistance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6cd5\u5f8b\u9886\u57df\u7684 RAG \u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u67e5\u8be2\u8f6c\u6362\u5668\u3001\u5f00\u6e90\u68c0\u7d22\u7b56\u7565\u548c\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\u6765\u589e\u5f3a LegalBenchRAG \u57fa\u7ebf\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5f00\u6e90\u7ba1\u9053\u5728\u68c0\u7d22\u8d28\u91cf\u4e0a\u53ef\u4ee5\u4e0e\u4e13\u6709\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u6216\u8d85\u8fc7\u4e13\u6709\u65b9\u6cd5\u3002", "motivation": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u901a\u8fc7\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u5efa\u7acb\u5728\u5f15\u7528\u7684\u6765\u6e90\u4e2d\u6765\u51cf\u8f7b\u5e7b\u89c9\uff0c\u8fd9\u79cd\u80fd\u529b\u5728\u6cd5\u5f8b\u9886\u57df\u5c24\u5176\u91cd\u8981\u3002", "method": "\u7aef\u5230\u7aef RAG \u7ba1\u9053\uff0c\u5b83\u91cd\u65b0\u5ba1\u89c6\u5e76\u6269\u5c55\u4e86 LegalBenchRAG \u57fa\u7ebf\uff0c\u5e76\u8fdb\u884c\u4e86\u4e09\u9879\u6709\u9488\u5bf9\u6027\u7684\u589e\u5f3a\uff1a(i) \u4e00\u79cd\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u67e5\u8be2\u8f6c\u6362\u5668\uff0c\u53ef\u5c06\u6587\u6863\u5f15\u7528\u4e0e\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u5206\u79bb\uff0c\u5e76\u6839\u636e\u4e13\u4e1a\u77e5\u8bc6\u548c\u7279\u5f02\u6027\u8c03\u6574\u68c0\u7d22\u6df1\u5ea6\u548c\u54cd\u5e94\u98ce\u683c\uff0c(ii) \u4f7f\u7528 SBERT \u548c GTE \u5d4c\u5165\u7684\u5f00\u6e90\u68c0\u7d22\u7b56\u7565\uff0c\u53ef\u5b9e\u73b0\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff08\u5c06 Recall@K \u63d0\u9ad8 30-95%\uff0cPrecision@K \u63d0\u9ad8\u7ea6 2.5 \u500d\uff0c\u5bf9\u4e8e K>4\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6210\u672c\u6548\u76ca\uff0c\u4ee5\u53ca (iii) \u4e00\u4e2a\u7efc\u5408\u8bc4\u4f30\u548c\u751f\u6210\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86 RAGAS\u3001BERTScore-F1 \u548c ROUGE-Recall\uff0c\u4ee5\u8bc4\u4f30\u8de8\u6a21\u578b\u548c\u63d0\u793a\u8bbe\u8ba1\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u5fe0\u5b9e\u5ea6\u3002", "result": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5f00\u6e90\u7ba1\u9053\u5728\u68c0\u7d22\u8d28\u91cf\u4e0a\u53ef\u4ee5\u4e0e\u4e13\u6709\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u6216\u8d85\u8fc7\u4e13\u6709\u65b9\u6cd5\uff0c\u800c\u5b9a\u5236\u7684\u6cd5\u5f8b\u57fa\u7840\u63d0\u793a\u59cb\u7ec8\u6bd4\u57fa\u7ebf\u63d0\u793a\u4ea7\u751f\u66f4\u5fe0\u5b9e\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u7b54\u6848\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5f00\u6e90\u7ba1\u9053\u5728\u68c0\u7d22\u8d28\u91cf\u4e0a\u53ef\u4ee5\u4e0e\u4e13\u6709\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u6216\u8d85\u8fc7\u4e13\u6709\u65b9\u6cd5\uff0c\u800c\u5b9a\u5236\u7684\u6cd5\u5f8b\u57fa\u7840\u63d0\u793a\u59cb\u7ec8\u6bd4\u57fa\u7ebf\u63d0\u793a\u4ea7\u751f\u66f4\u5fe0\u5b9e\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u7b54\u6848\u3002\u8fd9\u4e9b\u8d21\u732e\u8bc1\u660e\u4e86\u4efb\u52a1\u611f\u77e5\u3001\u7ec4\u4ef6\u7ea7\u8c03\u6574\u5728\u4e3a\u6cd5\u5f8b\u7814\u7a76\u63d0\u4f9b\u5177\u6709\u6cd5\u5f8b\u4f9d\u636e\u3001\u53ef\u91cd\u73b0\u4e14\u5177\u6709\u6210\u672c\u6548\u76ca\u7684 RAG \u7cfb\u7edf\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.11936", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11936", "abs": "https://arxiv.org/abs/2508.11936", "authors": ["Yuehan Qin", "Li Li", "Defu Cao", "Tiankai Yang", "Yue Zhao"], "title": "M3OOD: Automatic Selection of Multimodal OOD Detectors", "comment": null, "summary": "Out-of-distribution (OOD) robustness is a critical challenge for modern\nmachine learning systems, particularly as they increasingly operate in\nmultimodal settings involving inputs like video, audio, and sensor data.\nCurrently, many OOD detection methods have been proposed, each with different\ndesigns targeting various distribution shifts. A single OOD detector may not\nprevail across all the scenarios; therefore, how can we automatically select an\nideal OOD detection model for different distribution shifts? Due to the\ninherent unsupervised nature of the OOD detection task, it is difficult to\npredict model performance and find a universally Best model. Also,\nsystematically comparing models on the new unseen data is costly or even\nimpractical. To address this challenge, we introduce M3OOD, a\nmeta-learning-based framework for OOD detector selection in multimodal\nsettings. Meta learning offers a solution by learning from historical model\nbehaviors, enabling rapid adaptation to new data distribution shifts with\nminimal supervision. Our approach combines multimodal embeddings with\nhandcrafted meta-features that capture distributional and cross-modal\ncharacteristics to represent datasets. By leveraging historical performance\nacross diverse multimodal benchmarks, M3OOD can recommend suitable detectors\nfor a new data distribution shift. Experimental evaluation demonstrates that\nM3OOD consistently outperforms 10 competitive baselines across 12 test\nscenarios with minimal computational overhead.", "AI": {"tldr": "M3OOD\u662f\u4e00\u79cd\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u9009\u62e9OOD\u68c0\u6d4b\u5668\uff0c\u5b83\u53ef\u4ee5\u5b66\u4e60\u5386\u53f2\u6a21\u578b\u884c\u4e3a\uff0c\u5e76\u4ee5\u6700\u5c0f\u7684\u76d1\u7763\u5feb\u901f\u9002\u5e94\u65b0\u7684\u6570\u636e\u5206\u5e03\u53d8\u5316\u3002", "motivation": "\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\uff0cOOD\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\u5404\u6709\u4f18\u7f3a\u70b9\uff0c\u96be\u4ee5\u5728\u6240\u6709\u573a\u666f\u4e2d\u90fd\u8868\u73b0\u826f\u597d\u3002\u7531\u4e8eOOD\u68c0\u6d4b\u4efb\u52a1\u7684\u65e0\u76d1\u7763\u6027\u8d28\uff0c\u9884\u6d4b\u6a21\u578b\u6027\u80fd\u548c\u627e\u5230\u901a\u7528\u7684\u6700\u4f73\u6a21\u578b\u975e\u5e38\u56f0\u96be\u3002\u7cfb\u7edf\u5730\u6bd4\u8f83\u65b0\u6a21\u578b\u6210\u672c\u9ad8\u6602\u751a\u81f3\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u7ed3\u5408\u591a\u6a21\u6001\u5d4c\u5165\u4e0e\u624b\u5de5\u8bbe\u8ba1\u7684\u5143\u7279\u5f81\uff0c\u8fd9\u4e9b\u5143\u7279\u5f81\u6355\u83b7\u5206\u5e03\u548c\u8de8\u6a21\u6001\u7279\u5f81\u4ee5\u8868\u793a\u6570\u636e\u96c6\u3002\u5229\u7528\u8de8\u5404\u79cd\u591a\u6a21\u6001\u57fa\u51c6\u7684\u5386\u53f2\u6027\u80fd\uff0cM3OOD\u53ef\u4ee5\u4e3a\u65b0\u7684\u6570\u636e\u5206\u5e03\u53d8\u5316\u63a8\u8350\u5408\u9002\u7684\u68c0\u6d4b\u5668\u3002", "result": "M3OOD\u6846\u67b6\u5728\u591a\u6a21\u6001\u8bbe\u7f6e\u4e2d\u9009\u62e9OOD\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u5b66\u4e60\u5386\u53f2\u6a21\u578b\u884c\u4e3a\uff0c\u80fd\u591f\u4ee5\u6700\u5c0f\u7684\u76d1\u7763\u5feb\u901f\u9002\u5e94\u65b0\u7684\u6570\u636e\u5206\u5e03\u53d8\u5316\u3002", "conclusion": "M3OOD\u572812\u4e2a\u6d4b\u8bd5\u573a\u666f\u4e2d\u59cb\u7ec8\u4f18\u4e8e10\u4e2a\u7ade\u4e89\u57fa\u7ebf\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\u3002"}}
{"id": "2508.12096", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12096", "abs": "https://arxiv.org/abs/2508.12096", "authors": ["Haiquan Hu", "Jiazhi Jiang", "Shiyou Xu", "Ruhan Zeng", "Tian Wang"], "title": "STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples", "comment": "Submit to AAAI 2026", "summary": "Evaluating large language models (LLMs) has become increasingly challenging\nas model capabilities advance rapidly. While recent models often achieve higher\nscores on standard benchmarks, these improvements do not consistently reflect\nenhanced real-world reasoning capabilities. Moreover, widespread overfitting to\npublic benchmarks and the high computational cost of full evaluations have made\nit both expensive and less effective to distinguish meaningful differences\nbetween models. To address these challenges, we propose the \\textbf{S}tructured\n\\textbf{T}ransition \\textbf{E}valuation \\textbf{M}ethod (STEM), a lightweight\nand interpretable evaluation framework for efficiently estimating the relative\ncapabilities of LLMs. STEM identifies \\textit{significant transition samples}\n(STS) by analyzing consistent performance transitions among LLMs of the same\narchitecture but varying parameter scales. These samples enable STEM to\neffectively estimate the capability position of an unknown model. Qwen3 model\nfamily is applied to construct the STS pool on six diverse and representative\nbenchmarks. To assess generalizability. Experimental results indicate that STEM\nreliably captures performance trends, aligns with ground-truth rankings of\nmodel capability. These findings highlight STEM as a practical and scalable\nmethod for fine-grained, architecture-agnostic evaluation of LLMs.", "AI": {"tldr": "STEM \u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u6709\u6548\u4f30\u8ba1\u6cd5\u5b66\u7855\u58eb\u7684\u76f8\u5bf9\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u6a21\u578b\u80fd\u529b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u53d8\u5f97\u8d8a\u6765\u8d8a\u5177\u6709\u6311\u6218\u6027\u3002\u8fd1\u671f\u7684\u6a21\u578b\u901a\u5e38\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u83b7\u5f97\u66f4\u9ad8\u7684\u5206\u6570\uff0c\u4f46\u8fd9\u4e9b\u6539\u8fdb\u5e76\u4e0d\u80fd\u59cb\u7ec8\u53cd\u6620\u51fa\u589e\u5f3a\u7684\u5b9e\u9645\u63a8\u7406\u80fd\u529b\u3002\u6b64\u5916\uff0c\u5e7f\u6cdb\u8fc7\u5ea6\u62df\u5408\u516c\u5171\u57fa\u51c6\u4ee5\u53ca\u5b8c\u5168\u8bc4\u4f30\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u4f7f\u5f97\u533a\u5206\u6a21\u578b\u4e4b\u95f4\u6709\u610f\u4e49\u7684\u5dee\u5f02\u65e2\u6602\u8d35\u53c8\u6548\u679c\u8f83\u5dee\u3002", "method": "\u901a\u8fc7\u5206\u6790\u76f8\u540c\u67b6\u6784\u4f46\u53c2\u6570\u89c4\u6a21\u4e0d\u540c\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u4e00\u81f4\u7684\u6027\u80fd\u8f6c\u6362\u6765\u8bc6\u522b\u663e\u7740\u7684\u8f6c\u6362\u6837\u672c (STS)\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSTEM \u53ef\u9760\u5730\u6355\u83b7\u6027\u80fd\u8d8b\u52bf\uff0c\u4e0e\u6a21\u578b\u80fd\u529b\u7684\u771f\u5b9e\u6392\u540d\u4e00\u81f4\u3002", "conclusion": "STEM \u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5bf9 LLM \u8fdb\u884c\u7ec6\u7c92\u5ea6\u7684\u3001\u4e0e\u67b6\u6784\u65e0\u5173\u7684\u8bc4\u4f30\u3002"}}
{"id": "2508.11904", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11904", "abs": "https://arxiv.org/abs/2508.11904", "authors": ["Lingyun Zhang", "Yu Xie", "Yanwei Fu", "Ping Chen"], "title": "SafeCtrl: Region-Based Safety Control for Text-to-Image Diffusion via Detect-Then-Suppress", "comment": null, "summary": "The widespread deployment of text-to-image models is challenged by their\npotential to generate harmful content. While existing safety methods, such as\nprompt rewriting or model fine-tuning, provide valuable interventions, they\noften introduce a trade-off between safety and fidelity. Recent\nlocalization-based approaches have shown promise, yet their reliance on\nexplicit ``concept replacement\" can sometimes lead to semantic incongruity. To\naddress these limitations, we explore a more flexible detect-then-suppress\nparadigm. We introduce SafeCtrl, a lightweight, non-intrusive plugin that first\nprecisely localizes unsafe content. Instead of performing a hard A-to-B\nsubstitution, SafeCtrl then suppresses the harmful semantics, allowing the\ngenerative process to naturally and coherently resolve into a safe,\ncontext-aware alternative. A key aspect of our work is a novel training\nstrategy using Direct Preference Optimization (DPO). We leverage readily\navailable, image-level preference data to train our module, enabling it to\nlearn nuanced suppression behaviors and perform region-guided interventions at\ninference without requiring costly, pixel-level annotations. Extensive\nexperiments show that SafeCtrl significantly outperforms state-of-the-art\nmethods in both safety efficacy and fidelity preservation. Our findings suggest\nthat decoupled, suppression-based control is a highly effective and scalable\ndirection for building more responsible generative models.", "AI": {"tldr": "SafeCtrl, a lightweight plugin, localizes and suppresses unsafe content in text-to-image models, outperforming existing methods in safety and fidelity by using DPO for training.", "motivation": "Existing safety methods often introduce a trade-off between safety and fidelity. Recent localization-based approaches rely on explicit concept replacement which can sometimes lead to semantic incongruity.", "method": "A lightweight, non-intrusive plugin that first precisely localizes unsafe content and then suppresses the harmful semantics, allowing the generative process to naturally and coherently resolve into a safe, context-aware alternative. A novel training strategy using Direct Preference Optimization (DPO) is used.", "result": "SafeCtrl significantly outperforms state-of-the-art methods in both safety efficacy and fidelity preservation.", "conclusion": "Decoupled, suppression-based control is a highly effective and scalable direction for building more responsible generative models."}}
{"id": "2508.12165", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12165", "abs": "https://arxiv.org/abs/2508.12165", "authors": ["Rohit Krishnan", "Jon Evans"], "title": "RLNVR: Reinforcement Learning from Non-Verified Real-World Rewards", "comment": null, "summary": "This paper introduces RLNVR (Reinforcement Learning from Non-Verified\nRewards), a framework for training language models using noisy, real-world\nfeedback signals without requiring explicit human verification. Traditional\nRLHF requires expensive, verified reward signals that are impractical in many\nreal-world domains. RLNVR addresses this challenge through baseline\nnormalization and semantic similarity-based reward transfer. We demonstrate\nRLNVR through Walter, a prototype system that optimizes social media content\ngeneration using actual engagement data from Bluesky. Our experimental results\nshow significant improvements in content quality and training stability, with\ncomprehensive evaluation planned for future work. Positioning: We present a\npractical framework that combines RLNVR with GSPO (Group Sequence Policy\nOptimization) and an optional UED (Unsupervised Environment Design) curriculum\nto improve stability and diversity under noisy, implicit rewards. To our\nknowledge, combining GSPO-style normalization with a UED-style curriculum for\nLLM content generation from implicit social engagement has not been previously\ndocumented in this applied setting; we frame this as an applied integration\nrather than a new algorithm.", "AI": {"tldr": "RLNVR\u4f7f\u7528\u566a\u58f0\u3001\u771f\u5b9e\u4e16\u754c\u7684\u53cd\u9988\u4fe1\u53f7\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u4eba\u5de5\u9a8c\u8bc1\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u53c2\u4e0e\u6570\u636e\u4f18\u5316\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u751f\u6210\u3002", "motivation": "\u4f20\u7edf\u7684RLHF\u9700\u8981\u6602\u8d35\u7684\u3001\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u8fd9\u5728\u8bb8\u591a\u73b0\u5b9e\u9886\u57df\u4e2d\u662f\u4e0d\u5207\u5b9e\u9645\u7684\u3002", "method": "RLNVR\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u7ebf\u5f52\u4e00\u5316\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u5956\u52b1\u8f6c\u79fb\u6765\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728\u5185\u5bb9\u8d28\u91cf\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u65b9\u9762\u6709\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "RLNVR\u7ed3\u5408GSPO\u548c\u53ef\u9009\u7684UED\u8bfe\u7a0b\uff0c\u5728\u566a\u58f0\u3001\u9690\u5f0f\u5956\u52b1\u4e0b\u63d0\u9ad8\u7a33\u5b9a\u6027\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2508.11940", "categories": ["cs.LG", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.11940", "abs": "https://arxiv.org/abs/2508.11940", "authors": ["Yuannuo Feng", "Wenyong Zhou", "Yuexi Lyu", "Yixiang Zhang", "Zhengwu Liu", "Ngai Wong", "Wang Kang"], "title": "Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware", "comment": "4 pages, 5 figures, conference", "summary": "Analog Compute-In-Memory (CIM) architectures promise significant energy\nefficiency gains for neural network inference, but suffer from complex\nhardware-induced noise that poses major challenges for deployment. While\nnoise-aware training methods have been proposed to address this issue, they\ntypically rely on idealized and differentiable noise models that fail to\ncapture the full complexity of analog CIM hardware variations. Motivated by the\nStraight-Through Estimator (STE) framework in quantization, we decouple forward\nnoise simulation from backward gradient computation, enabling noise-aware\ntraining with more accurate but computationally intractable noise modeling in\nanalog CIM systems. We provide theoretical analysis demonstrating that our\napproach preserves essential gradient directional information while maintaining\ncomputational tractability and optimization stability. Extensive experiments\nshow that our extended STE framework achieves up to 5.3% accuracy improvement\non image classification, 0.72 perplexity reduction on text generation,\n2.2$\\times$ speedup in training time, and 37.9% lower peak memory usage\ncompared to standard noise-aware training methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6a21\u62dfCIM\u7684\u566a\u58f0\u611f\u77e5\u8bad\u7ec3\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u3001\u964d\u4f4e\u4e86\u56f0\u60d1\u5ea6\u3001\u52a0\u5feb\u4e86\u8bad\u7ec3\u901f\u5ea6\u5e76\u51cf\u5c11\u4e86\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u6a21\u62df\u8ba1\u7b97\u5185\u5b58\uff08CIM\uff09\u67b6\u6784\u5728\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u65b9\u9762\u5177\u6709\u663e\u8457\u7684\u80fd\u6548\u4f18\u52bf\uff0c\u4f46\u5b58\u5728\u590d\u6742\u7684\u786c\u4ef6\u5f15\u5165\u566a\u58f0\uff0c\u8fd9\u5bf9\u90e8\u7f72\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7684\u566a\u58f0\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7406\u60f3\u5316\u548c\u53ef\u5fae\u7684\u566a\u58f0\u6a21\u578b\uff0c\u65e0\u6cd5\u6355\u6349\u6a21\u62dfCIM\u786c\u4ef6\u53d8\u5316\u7684\u5168\u90e8\u590d\u6742\u6027\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u7684STE\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u89e3\u8026\u4e86\u524d\u5411\u566a\u58f0\u6a21\u62df\u548c\u540e\u5411\u68af\u5ea6\u8ba1\u7b97\uff0c\u4ece\u800c\u53ef\u4ee5\u4f7f\u7528\u66f4\u51c6\u786e\u7684\u566a\u58f0\u6a21\u578b\u8fdb\u884c\u566a\u58f0\u611f\u77e5\u8bad\u7ec3\u3002", "result": "\u8be5\u8bba\u6587\u7684\u6269\u5c55STE\u6846\u67b6\u5728\u56fe\u50cf\u5206\u7c7b\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe5.3%\u7684\u7cbe\u5ea6\u63d0\u5347\uff0c\u5728\u6587\u672c\u751f\u6210\u4e0a\u5b9e\u73b0\u4e860.72\u7684\u56f0\u60d1\u5ea6\u964d\u4f4e\uff0c\u8bad\u7ec3\u65f6\u95f4\u52a0\u901f2.2\u500d\uff0c\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u964d\u4f4e37.9%\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u7684STE\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u524d\u5411\u566a\u58f0\u6a21\u62df\u548c\u540e\u5411\u68af\u5ea6\u8ba1\u7b97\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u4f46\u8ba1\u7b97\u4e0a\u96be\u4ee5\u5904\u7406\u7684\u6a21\u62dfCIM\u7cfb\u7edf\u566a\u58f0\u5efa\u6a21\u7684\u566a\u58f0\u611f\u77e5\u8bad\u7ec3\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u56fe\u50cf\u5206\u7c7b\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe5.3%\u7684\u7cbe\u5ea6\u63d0\u5347\uff0c\u5728\u6587\u672c\u751f\u6210\u4e0a\u5b9e\u73b0\u4e860.72\u7684\u56f0\u60d1\u5ea6\u964d\u4f4e\uff0c\u8bad\u7ec3\u65f6\u95f4\u52a0\u901f2.2\u500d\uff0c\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u964d\u4f4e37.9%\u3002"}}
{"id": "2508.12140", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12140", "abs": "https://arxiv.org/abs/2508.12140", "authors": ["Ziqian Bi", "Lu Chen", "Junhao Song", "Hongying Luo", "Enze Ge", "Junmin Huang", "Tianyang Wang", "Keyu Chen", "Chia Xin Liang", "Zihan Wei", "Huafeng Liu", "Chunjie Tian", "Jibin Guan", "Joe Yeong", "Yongzhi Xu", "Peng Wang", "Junfeng Hao"], "title": "Exploring Efficiency Frontiers of Thinking Budget in Medical Reasoning: Scaling Laws between Computational Resources and Reasoning Quality", "comment": null, "summary": "This study presents the first comprehensive evaluation of thinking budget\nmechanisms in medical reasoning tasks, revealing fundamental scaling laws\nbetween computational resources and reasoning quality. We systematically\nevaluated two major model families, Qwen3 (1.7B to 235B parameters) and\nDeepSeek-R1 (1.5B to 70B parameters), across 15 medical datasets spanning\ndiverse specialties and difficulty levels. Through controlled experiments with\nthinking budgets ranging from zero to unlimited tokens, we establish\nlogarithmic scaling relationships where accuracy improvements follow a\npredictable pattern with both thinking budget and model size. Our findings\nidentify three distinct efficiency regimes: high-efficiency (0 to 256 tokens)\nsuitable for real-time applications, balanced (256 to 512 tokens) offering\noptimal cost-performance tradeoffs for routine clinical support, and\nhigh-accuracy (above 512 tokens) justified only for critical diagnostic tasks.\nNotably, smaller models demonstrate disproportionately larger benefits from\nextended thinking, with 15 to 20% improvements compared to 5 to 10% for larger\nmodels, suggesting a complementary relationship where thinking budget provides\ngreater relative benefits for capacity-constrained models. Domain-specific\npatterns emerge clearly, with neurology and gastroenterology requiring\nsignificantly deeper reasoning processes than cardiovascular or respiratory\nmedicine. The consistency between Qwen3 native thinking budget API and our\nproposed truncation method for DeepSeek-R1 validates the generalizability of\nthinking budget concepts across architectures. These results establish thinking\nbudget control as a critical mechanism for optimizing medical AI systems,\nenabling dynamic resource allocation aligned with clinical needs while\nmaintaining the transparency essential for healthcare deployment.", "AI": {"tldr": "Thinking budget mechanisms in medical reasoning tasks are evaluated, revealing scaling laws and efficiency regimes. Smaller models benefit more from extended thinking.", "motivation": "To evaluate thinking budget mechanisms in medical reasoning tasks and reveal scaling laws between computational resources and reasoning quality.", "method": "Evaluation of Qwen3 and DeepSeek-R1 models across 15 medical datasets with varying thinking budgets.", "result": "Logarithmic scaling relationships between accuracy improvements and thinking budget/model size; identification of three efficiency regimes; smaller models benefit more from extended thinking; domain-specific patterns emerge.", "conclusion": "Thinking budget control is a critical mechanism for optimizing medical AI systems, enabling dynamic resource allocation aligned with clinical needs while maintaining the transparency essential for healthcare deployment."}}
{"id": "2508.11919", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11919", "abs": "https://arxiv.org/abs/2508.11919", "authors": ["Pallavi Jain", "Diego Marcos", "Dino Ienco", "Roberto Interdonato", "Tristan Berchoux"], "title": "TimeSenCLIP: A Vision-Language Model for Remote Sensing Using Single-Pixel Time Series", "comment": "Paper under review", "summary": "Vision-language models have shown significant promise in remote sensing\napplications, particularly for land-use and land-cover (LULC) via zero-shot\nclassification and retrieval. However, current approaches face two key\nchallenges: reliance on large spatial tiles that increase computational cost,\nand dependence on text-based supervision, which is often not readily available.\nIn this work, we present TimeSenCLIP, a lightweight framework that reevaluate\nthe role of spatial context by evaluating the effectiveness of a single pixel\nby leveraging its temporal and spectral dimensions, for classifying LULC and\necosystem types. By leveraging spectral and temporal information from\nSentinel-2 imagery and cross-view learning with geo-tagged ground-level photos,\nwe minimises the need for caption-based training while preserving semantic\nalignment between overhead (satellite) and ground perspectives. Our approach is\ngrounded in the LUCAS and Sen4Map datasets, and evaluated on classification\ntasks including LULC, crop type, and ecosystem type. We demonstrate that single\npixel inputs, when combined with temporal and spectral cues, are sufficient for\nthematic mapping, offering a scalable and efficient alternative for large-scale\nremote sensing applications. Code is available at\nhttps://github.com/pallavijain-pj/TimeSenCLIP", "AI": {"tldr": "TimeSenCLIP \u901a\u8fc7\u5229\u7528\u5355\u4e2a\u50cf\u7d20\u7684\u65f6\u95f4\u548c\u5149\u8c31\u7ef4\u5ea6\uff0c\u4e3a\u5927\u89c4\u6a21\u9065\u611f\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4ece\u800c\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4e86\u5bf9\u57fa\u4e8e\u5b57\u5e55\u7684\u8bad\u7ec3\u7684\u9700\u6c42\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9065\u611f\u5e94\u7528\u4e2d\u663e\u793a\u51fa\u5de8\u5927\u7684\u524d\u666f\uff0c\u7279\u522b\u662f\u901a\u8fc7\u96f6\u6837\u672c\u5206\u7c7b\u548c\u68c0\u7d22\u8fdb\u884c\u571f\u5730\u5229\u7528\u548c\u571f\u5730\u8986\u76d6\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u4f9d\u8d56\u4e8e\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u7684\u5927\u7a7a\u95f4\u74e6\u7247\uff0c\u4ee5\u53ca\u4f9d\u8d56\u4e8e\u901a\u5e38\u4e0d\u6613\u83b7\u5f97\u7684\u57fa\u4e8e\u6587\u672c\u7684\u76d1\u7763\u3002", "method": "TimeSenCLIP\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u5355\u4e2a\u50cf\u7d20\u7684\u65f6\u95f4\u548c\u5149\u8c31\u7ef4\u5ea6\u6765\u8bc4\u4f30\u5176\u6709\u6548\u6027\uff0c\u7528\u4e8e\u5206\u7c7b\u571f\u5730\u5229\u7528\u548c\u571f\u5730\u8986\u76d6\u4ee5\u53ca\u751f\u6001\u7cfb\u7edf\u7c7b\u578b\u3002\u5229\u7528\u6765\u81ea Sentinel-2 \u56fe\u50cf\u7684\u5149\u8c31\u548c\u65f6\u95f4\u4fe1\u606f\uff0c\u4ee5\u53ca\u4e0e\u5730\u7406\u6807\u8bb0\u7684\u5730\u9762\u7167\u7247\u7684\u4ea4\u53c9\u89c6\u89d2\u5b66\u4e60\uff0c\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4e86\u5bf9\u57fa\u4e8e\u5b57\u5e55\u7684\u8bad\u7ec3\u7684\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u4e86 overhead\uff08\u536b\u661f\uff09\u548c\u5730\u9762\u89c6\u89d2\u4e4b\u95f4\u7684\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u8bc1\u660e\u4e86\u5355\u50cf\u7d20\u8f93\u5165\uff0c\u5f53\u4e0e\u65f6\u95f4\u548c\u5149\u8c31\u7ebf\u7d22\u7ed3\u5408\u4f7f\u7528\u65f6\uff0c\u8db3\u4ee5\u7528\u4e8e\u4e13\u9898\u5236\u56fe\uff0c\u4e3a\u5927\u89c4\u6a21\u9065\u611f\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u5355\u50cf\u7d20\u8f93\u5165\u7ed3\u5408\u65f6\u95f4\u548c\u5149\u8c31\u7ebf\u7d22\u8db3\u4ee5\u7528\u4e8e\u4e13\u9898\u5236\u56fe\uff0c\u4e3a\u5927\u89c4\u6a21\u9065\u611f\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2508.12260", "categories": ["cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.12260", "abs": "https://arxiv.org/abs/2508.12260", "authors": ["Carson Dudley", "Reiden Magdaleno", "Christopher Harding", "Ananya Sharma", "Emily Martin", "Marisa Eisenberg"], "title": "Mantis: A Simulation-Grounded Foundation Model for Disease Forecasting", "comment": "10 pages, 4 figures", "summary": "Infectious disease forecasting in novel outbreaks or low resource settings\nhas been limited by the need for disease-specific data, bespoke training, and\nexpert tuning. We introduce Mantis, a foundation model trained entirely on\nmechanistic simulations, which enables out-of-the-box forecasting across\ndiseases, regions, and outcomes, even in settings with limited historical data.\nMantis is built on over 400 million simulated days of outbreak dynamics\nspanning diverse pathogens, transmission modes, interventions, and surveillance\nartifacts. Despite requiring no real-world data during training, Mantis\noutperformed 39 expert-tuned models we tested across six diseases, including\nall models in the CDC's COVID-19 Forecast Hub. Mantis generalized to novel\nepidemiological regimes, including diseases with held-out transmission\nmechanisms, demonstrating that it captures fundamental contagion dynamics.\nCritically, Mantis is mechanistically interpretable, enabling public health\ndecision-makers to identify the latent drivers behind its predictions. Finally,\nMantis delivers accurate forecasts at 8-week horizons, more than doubling the\nactionable range of most models, enabling proactive public health planning.\nTogether, these capabilities position Mantis as a foundation for\nnext-generation disease forecasting systems: general, interpretable, and\ndeployable where traditional models fail.", "AI": {"tldr": "Mantis \u662f\u4e00\u4e2a\u5728\u673a\u5236\u6a21\u62df\u57fa\u7840\u4e0a\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\uff0c\u65e0\u9700\u771f\u5b9e\u4e16\u754c\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u8de8\u75be\u75c5\u3001\u533a\u57df\u548c\u7ed3\u679c\u7684\u5f00\u7bb1\u5373\u7528\u9884\u6d4b\uff0c\u4f18\u4e8e\u4e13\u5bb6\u8c03\u6574\u7684\u6a21\u578b\uff0c\u5e76\u4e14\u5728\u673a\u5236\u4e0a\u662f\u53ef\u89e3\u91ca\u7684\u3002", "motivation": "\u5728\u65b0\u7206\u53d1\u6216\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\uff0c\u4f20\u67d3\u75c5\u9884\u6d4b\u4e00\u76f4\u53d7\u5230\u5bf9\u75be\u75c5\u7279\u5b9a\u6570\u636e\u3001\u5b9a\u5236\u57f9\u8bad\u548c\u4e13\u5bb6\u8c03\u6574\u7684\u9700\u6c42\u7684\u9650\u5236\u3002", "method": "\u5728\u8d85\u8fc7 4 \u4ebf\u4e2a\u6a21\u62df\u7684\u7206\u53d1\u52a8\u6001\u5929\u6570\u7684\u57fa\u7840\u4e0a\uff0cMantis \u6db5\u76d6\u4e86\u591a\u79cd\u75c5\u539f\u4f53\u3001\u4f20\u64ad\u6a21\u5f0f\u3001\u5e72\u9884\u63aa\u65bd\u548c\u76d1\u6d4b\u4eba\u4e3a\u56e0\u7d20\u3002", "result": "\u5c3d\u7ba1\u5728\u8bad\u7ec3\u671f\u95f4\u4e0d\u9700\u8981\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\uff0c\u4f46 Mantis \u5728\u6211\u4eec\u6d4b\u8bd5\u7684\u516d\u79cd\u75be\u75c5\u4e2d\u4f18\u4e8e 39 \u4e2a\u7ecf\u8fc7\u4e13\u5bb6\u8c03\u6574\u7684\u6a21\u578b\uff0c\u5305\u62ec CDC \u7684 COVID-19 \u9884\u6d4b\u4e2d\u5fc3\u7684\u6240\u6709\u6a21\u578b\u3002Mantis \u63a8\u5e7f\u5230\u65b0\u7684\u6d41\u884c\u75c5\u5b66\u9886\u57df\uff0c\u5305\u62ec\u5177\u6709\u4fdd\u7559\u4f20\u64ad\u673a\u5236\u7684\u75be\u75c5\uff0c\u8868\u660e\u5b83\u6355\u83b7\u4e86\u57fa\u672c\u7684\u4f20\u67d3\u52a8\u6001\u3002\u91cd\u8981\u7684\u662f\uff0cMantis \u5728\u673a\u5236\u4e0a\u662f\u53ef\u89e3\u91ca\u7684\uff0c\u4f7f\u516c\u5171\u536b\u751f\u51b3\u7b56\u8005\u80fd\u591f\u8bc6\u522b\u5176\u9884\u6d4b\u80cc\u540e\u7684\u6f5c\u5728\u9a71\u52a8\u56e0\u7d20\u3002\u6700\u540e\uff0cMantis \u63d0\u4f9b\u4e86 8 \u5468\u8303\u56f4\u5185\u7684\u51c6\u786e\u9884\u6d4b\uff0c\u662f\u5927\u591a\u6570\u6a21\u578b\u7684\u53ef\u64cd\u4f5c\u8303\u56f4\u7684\u4e24\u500d\u4ee5\u4e0a\uff0c\u4ece\u800c\u53ef\u4ee5\u8fdb\u884c\u79ef\u6781\u7684\u516c\u5171\u536b\u751f\u89c4\u5212\u3002", "conclusion": "Mantis\u4f5c\u4e3a\u4e00\u4e2a\u901a\u7528\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u5728\u4f20\u7edf\u6a21\u578b\u5931\u6548\u65f6\u90e8\u7f72\u7684\u57fa\u7840\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u75be\u75c5\u9884\u6d4b\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.11943", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11943", "abs": "https://arxiv.org/abs/2508.11943", "authors": ["Sishun Liu", "Ke Deng", "Xiuzhen Zhang", "Yan Wang"], "title": "Learning Marked Temporal Point Process Explanations based on Counterfactual and Factual Reasoning", "comment": "ECAI 2025 full version", "summary": "Neural network-based Marked Temporal Point Process (MTPP) models have been\nwidely adopted to model event sequences in high-stakes applications, raising\nconcerns about the trustworthiness of outputs from these models. This study\nfocuses on Explanation for MTPP, aiming to identify the minimal and rational\nexplanation, that is, the minimum subset of events in history, based on which\nthe prediction accuracy of MTPP matches that based on full history to a great\nextent and better than that based on the complement of the subset. This study\nfinds that directly defining Explanation for MTPP as counterfactual explanation\nor factual explanation can result in irrational explanations. To address this\nissue, we define Explanation for MTPP as a combination of counterfactual\nexplanation and factual explanation. This study proposes Counterfactual and\nFactual Explainer for MTPP (CFF) to solve Explanation for MTPP with a series of\ndeliberately designed techniques. Experiments demonstrate the correctness and\nsuperiority of CFF over baselines regarding explanation quality and processing\nefficiency.", "AI": {"tldr": "This paper introduces CFF, a novel approach for generating explanations for MTPP models, which combines counterfactual and factual explanations to provide more rational and efficient results.", "motivation": "This study focuses on Explanation for MTPP, aiming to identify the minimal and rational explanation. This study finds that directly defining Explanation for MTPP as counterfactual explanation or factual explanation can result in irrational explanations. To address this issue, we define Explanation for MTPP as a combination of counterfactual explanation and factual explanation.", "method": "This study proposes Counterfactual and Factual Explainer for MTPP (CFF) to solve Explanation for MTPP with a series of deliberately designed techniques.", "result": "Experiments demonstrate the correctness and superiority of CFF over baselines regarding explanation quality and processing efficiency.", "conclusion": "This study proposes CFF to solve Explanation for MTPP with a series of deliberately designed techniques. Experiments demonstrate the correctness and superiority of CFF over baselines regarding explanation quality and processing efficiency."}}
{"id": "2508.12158", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12158", "abs": "https://arxiv.org/abs/2508.12158", "authors": ["Stephen Meisenbacher", "Alexandra Klymenko", "Florian Matthes"], "title": "LLM-as-a-Judge for Privacy Evaluation? Exploring the Alignment of Human and LLM Perceptions of Privacy in Textual Data", "comment": "13 pages, 3 figures, 4 tables. Accepted to HAIPS @ CCS 2025", "summary": "Despite advances in the field of privacy-preserving Natural Language\nProcessing (NLP), a significant challenge remains the accurate evaluation of\nprivacy. As a potential solution, using LLMs as a privacy evaluator presents a\npromising approach $\\unicode{x2013}$ a strategy inspired by its success in\nother subfields of NLP. In particular, the so-called $\\textit{LLM-as-a-Judge}$\nparadigm has achieved impressive results on a variety of natural language\nevaluation tasks, demonstrating high agreement rates with human annotators.\nRecognizing that privacy is both subjective and difficult to define, we\ninvestigate whether LLM-as-a-Judge can also be leveraged to evaluate the\nprivacy sensitivity of textual data. Furthermore, we measure how closely LLM\nevaluations align with human perceptions of privacy in text. Resulting from a\nstudy involving 10 datasets, 13 LLMs, and 677 human survey participants, we\nconfirm that privacy is indeed a difficult concept to measure empirically,\nexhibited by generally low inter-human agreement rates. Nevertheless, we find\nthat LLMs can accurately model a global human privacy perspective, and through\nan analysis of human and LLM reasoning patterns, we discuss the merits and\nlimitations of LLM-as-a-Judge for privacy evaluation in textual data. Our\nfindings pave the way for exploring the feasibility of LLMs as privacy\nevaluators, addressing a core challenge in solving pressing privacy issues with\ninnovative technical solutions.", "AI": {"tldr": "LLMs can be used as privacy evaluators, addressing a core challenge in solving pressing privacy issues with innovative technical solutions.", "motivation": "Accurate evaluation of privacy remains a significant challenge in privacy-preserving NLP.", "method": "Using LLMs as a privacy evaluator, inspired by the LLM-as-a-Judge paradigm, to evaluate the privacy sensitivity of textual data and measuring how closely LLM evaluations align with human perceptions of privacy in text.", "result": "Privacy is a difficult concept to measure empirically, but LLMs can accurately model a global human privacy perspective.", "conclusion": "LLMs can accurately model a global human privacy perspective, paving the way for exploring the feasibility of LLMs as privacy evaluators."}}
{"id": "2508.11922", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11922", "abs": "https://arxiv.org/abs/2508.11922", "authors": ["Aditi Jahagirdar", "Sameer Joshi"], "title": "Assessment of Using Synthetic Data in Brain Tumor Segmentation", "comment": null, "summary": "Manual brain tumor segmentation from MRI scans is challenging due to tumor\nheterogeneity, scarcity of annotated data, and class imbalance in medical\nimaging datasets. Synthetic data generated by generative models has the\npotential to mitigate these issues by improving dataset diversity. This study\ninvestigates, as a proof of concept, the impact of incorporating synthetic MRI\ndata, generated using a pre-trained GAN model, into training a U-Net\nsegmentation network. Experiments were conducted using real data from the BraTS\n2020 dataset, synthetic data generated with the medigan library, and hybrid\ndatasets combining real and synthetic samples in varying proportions. While\noverall quantitative performance (Dice coefficient, IoU, precision, recall,\naccuracy) was comparable between real-only and hybrid-trained models,\nqualitative inspection suggested that hybrid datasets, particularly with 40%\nreal and 60% synthetic data, improved whole tumor boundary delineation.\nHowever, region-wise accuracy for the tumor core and the enhancing tumor\nremained lower, indicating a persistent class imbalance. The findings support\nthe feasibility of synthetic data as an augmentation strategy for brain tumor\nsegmentation, while highlighting the need for larger-scale experiments,\nvolumetric data consistency, and mitigating class imbalance in future work.", "AI": {"tldr": "Synthetic data can augment real data for brain tumor segmentation, but more work is needed to address class imbalance and data consistency.", "motivation": "Manual brain tumor segmentation from MRI scans is challenging due to tumor heterogeneity, scarcity of annotated data, and class imbalance in medical imaging datasets. Synthetic data generated by generative models has the potential to mitigate these issues by improving dataset diversity.", "method": "incorporating synthetic MRI data, generated using a pre-trained GAN model, into training a U-Net segmentation network", "result": "overall quantitative performance was comparable between real-only and hybrid-trained models, qualitative inspection suggested that hybrid datasets improved whole tumor boundary delineation. However, region-wise accuracy for the tumor core and the enhancing tumor remained lower, indicating a persistent class imbalance.", "conclusion": "The findings support the feasibility of synthetic data as an augmentation strategy for brain tumor segmentation, while highlighting the need for larger-scale experiments, volumetric data consistency, and mitigating class imbalance in future work."}}
{"id": "2508.12291", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12291", "abs": "https://arxiv.org/abs/2508.12291", "authors": ["Xuming He", "Zhiyuan You", "Junchao Gong", "Couhua Liu", "Xiaoyu Yue", "Peiqin Zhuang", "Wenlong Zhang", "Lei Bai"], "title": "RadarQA: Multi-modal Quality Analysis of Weather Radar Forecasts", "comment": null, "summary": "Quality analysis of weather forecasts is an essential topic in meteorology.\nAlthough traditional score-based evaluation metrics can quantify certain\nforecast errors, they are still far from meteorological experts in terms of\ndescriptive capability, interpretability, and understanding of dynamic\nevolution. With the rapid development of Multi-modal Large Language Models\n(MLLMs), these models become potential tools to overcome the above challenges.\nIn this work, we introduce an MLLM-based weather forecast analysis method,\nRadarQA, integrating key physical attributes with detailed assessment reports.\nWe introduce a novel and comprehensive task paradigm for multi-modal quality\nanalysis, encompassing both single frame and sequence, under both rating and\nassessment scenarios. To support training and benchmarking, we design a hybrid\nannotation pipeline that combines human expert labeling with automated\nheuristics. With such an annotation method, we construct RQA-70K, a large-scale\ndataset with varying difficulty levels for radar forecast quality evaluation.\nWe further design a multi-stage training strategy that iteratively improves\nmodel performance at each stage. Extensive experiments show that RadarQA\noutperforms existing general MLLMs across all evaluation settings, highlighting\nits potential for advancing quality analysis in weather prediction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRadarQA\u7684\u57fa\u4e8eMLLM\u7684\u5929\u6c14\u9884\u62a5\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aRQA-70K\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u4f20\u7edf\u4e0a\u57fa\u4e8e\u5206\u6570\u7684\u8bc4\u4f30\u6307\u6807\u53ef\u4ee5\u91cf\u5316\u67d0\u4e9b\u9884\u6d4b\u8bef\u5dee\uff0c\u4f46\u5728\u63cf\u8ff0\u80fd\u529b\u3001\u53ef\u89e3\u91ca\u6027\u548c\u5bf9\u52a8\u6001\u6f14\u5316\u7684\u7406\u89e3\u65b9\u9762\uff0c\u5b83\u4eec\u4e0e\u6c14\u8c61\u4e13\u5bb6\u76f8\u5dee\u751a\u8fdc\u3002\u968f\u7740\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u8fd9\u4e9b\u6a21\u578b\u6210\u4e3a\u514b\u670d\u4e0a\u8ff0\u6311\u6218\u7684\u6f5c\u5728\u5de5\u5177\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8eMLLM\u7684\u5929\u6c14\u9884\u62a5\u5206\u6790\u65b9\u6cd5RadarQA\uff0c\u8be5\u65b9\u6cd5\u96c6\u6210\u4e86\u5173\u952e\u7684\u7269\u7406\u5c5e\u6027\u548c\u8be6\u7ec6\u7684\u8bc4\u4f30\u62a5\u544a\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u9896\u800c\u5168\u9762\u7684\u591a\u6a21\u6001\u8d28\u91cf\u5206\u6790\u4efb\u52a1\u8303\u4f8b\uff0c\u5305\u62ec\u5355\u5e27\u548c\u5e8f\u5217\uff0c\u4ee5\u53ca\u8bc4\u7ea7\u548c\u8bc4\u4f30\u573a\u666f\u3002\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6df7\u5408\u6ce8\u91ca\u6d41\u7a0b\uff0c\u8be5\u6d41\u7a0b\u7ed3\u5408\u4e86\u4eba\u7c7b\u4e13\u5bb6\u6807\u8bb0\u548c\u81ea\u52a8\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "result": "RadarQA\u5728\u6240\u6709\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u901a\u7528MLLM\u3002", "conclusion": "RadarQA\u5728\u6240\u6709\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u901a\u7528MLLM\uff0c\u7a81\u51fa\u4e86\u5176\u5728\u63a8\u8fdb\u5929\u6c14\u9884\u62a5\u8d28\u91cf\u5206\u6790\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.11976", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11976", "abs": "https://arxiv.org/abs/2508.11976", "authors": ["Yunning Cao", "Lihong Pei", "Jian Guo", "Yang Cao", "Yu Kang", "Yanlong Zhao"], "title": "Set-Valued Transformer Network for High-Emission Mobile Source Identification", "comment": null, "summary": "Identifying high-emission vehicles is a crucial step in regulating urban\npollution levels and formulating traffic emission reduction strategies.\nHowever, in practical monitoring data, the proportion of high-emission state\ndata is significantly lower compared to normal emission states. This\ncharacteristic long-tailed distribution severely impedes the extraction of\ndiscriminative features for emission state identification during data mining.\nFurthermore, the highly nonlinear nature of vehicle emission states and the\nlack of relevant prior knowledge also pose significant challenges to the\nconstruction of identification models.To address the aforementioned issues, we\npropose a Set-Valued Transformer Network (SVTN) to achieve comprehensive\nlearning of discriminative features from high-emission samples, thereby\nenhancing detection accuracy. Specifically, this model first employs the\ntransformer to measure the temporal similarity of micro-trip condition\nvariations, thus constructing a mapping rule that projects the original\nhigh-dimensional emission data into a low-dimensional feature space. Next, a\nset-valued identification algorithm is used to probabilistically model the\nrelationship between the generated feature vectors and their labels, providing\nan accurate metric criterion for the classification algorithm. To validate the\neffectiveness of our proposed approach, we conducted extensive experiments on\nthe diesel vehicle monitoring data of Hefei city in 2020. The results\ndemonstrate that our method achieves a 9.5\\% reduction in the missed detection\nrate for high-emission vehicles compared to the transformer-based baseline,\nhighlighting its superior capability in accurately identifying high-emission\nmobile pollution sources.", "AI": {"tldr": "The paper introduces a Set-Valued Transformer Network (SVTN) to improve the detection accuracy of high-emission vehicles, achieving a 9.5% reduction in missed detections compared to existing methods.", "motivation": "Identifying high-emission vehicles is crucial for regulating urban pollution, but the low proportion of high-emission data and the nonlinear nature of vehicle emission states pose challenges to building identification models.", "method": "A Set-Valued Transformer Network (SVTN) is proposed, which uses a transformer to measure the temporal similarity of micro-trip condition variations and a set-valued identification algorithm to probabilistically model the relationship between feature vectors and their labels.", "result": "The SVTN achieves a 9.5% reduction in the missed detection rate for high-emission vehicles compared to the transformer-based baseline.", "conclusion": "The proposed Set-Valued Transformer Network (SVTN) reduces the missed detection rate for high-emission vehicles by 9.5% compared to the transformer-based baseline on diesel vehicle monitoring data in Hefei city in 2020."}}
{"id": "2508.12227", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12227", "abs": "https://arxiv.org/abs/2508.12227", "authors": ["Abdelhamid Haouhat", "Slimane Bellaouar", "Attia Nehar", "Hadda Cherroun", "Ahmed Abdelali"], "title": "Arabic Multimodal Machine Learning: Datasets, Applications, Approaches, and Challenges", "comment": null, "summary": "Multimodal Machine Learning (MML) aims to integrate and analyze information\nfrom diverse modalities, such as text, audio, and visuals, enabling machines to\naddress complex tasks like sentiment analysis, emotion recognition, and\nmultimedia retrieval. Recently, Arabic MML has reached a certain level of\nmaturity in its foundational development, making it time to conduct a\ncomprehensive survey. This paper explores Arabic MML by categorizing efforts\nthrough a novel taxonomy and analyzing existing research. Our taxonomy\norganizes these efforts into four key topics: datasets, applications,\napproaches, and challenges. By providing a structured overview, this survey\noffers insights into the current state of Arabic MML, highlighting areas that\nhave not been investigated and critical research gaps. Researchers will be\nempowered to build upon the identified opportunities and address challenges to\nadvance the field.", "AI": {"tldr": "This paper surveys Arabic Multimodal Machine Learning (MML), categorizing research into datasets, applications, approaches, and challenges, to provide insights and highlight research gaps.", "motivation": "Arabic MML has reached a certain level of maturity in its foundational development, making it time to conduct a comprehensive survey.", "method": "The paper explores Arabic MML by categorizing efforts through a novel taxonomy and analyzing existing research. The taxonomy organizes these efforts into four key topics: datasets, applications, approaches, and challenges.", "result": "The survey offers insights into the current state of Arabic MML.", "conclusion": "This survey provides a structured overview of Arabic MML, highlighting areas that have not been investigated and critical research gaps. Researchers will be empowered to build upon the identified opportunities and address challenges to advance the field."}}
{"id": "2508.11932", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11932", "abs": "https://arxiv.org/abs/2508.11932", "authors": ["Chengwei Zhang", "Xueyi Zhang", "Mingrui Lao", "Tao Jiang", "Xinhao Xu", "Wenjie Li", "Fubo Zhang", "Longyong Chen"], "title": "Deep Learning For Point Cloud Denoising: A Survey", "comment": null, "summary": "Real-world environment-derived point clouds invariably exhibit noise across\nvarying modalities and intensities. Hence, point cloud denoising (PCD) is\nessential as a preprocessing step to improve downstream task performance. Deep\nlearning (DL)-based PCD models, known for their strong representation\ncapabilities and flexible architectures, have surpassed traditional methods in\ndenoising performance. To our best knowledge, despite recent advances in\nperformance, no comprehensive survey systematically summarizes the developments\nof DL-based PCD. To fill the gap, this paper seeks to identify key challenges\nin DL-based PCD, summarizes the main contributions of existing methods, and\nproposes a taxonomy tailored to denoising tasks. To achieve this goal, we\nformulate PCD as a two-step process: outlier removal and surface noise\nrestoration, encompassing most scenarios and requirements of PCD. Additionally,\nwe compare methods in terms of similarities, differences, and respective\nadvantages. Finally, we discuss research limitations and future directions,\noffering insights for further advancements in PCD.", "AI": {"tldr": "\u672c\u6587\u603b\u7ed3\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u70b9\u4e91\u53bb\u566a\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u7c7b\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u70b9\u4e91\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u8868\u73b0\u51fa\u4e0d\u540c\u6a21\u5f0f\u548c\u5f3a\u5ea6\u7684\u566a\u58f0\u3002\u56e0\u6b64\uff0c\u70b9\u4e91\u53bb\u566a (PCD) \u4f5c\u4e3a\u63d0\u9ad8\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u9884\u5904\u7406\u6b65\u9aa4\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u6700\u8fd1\u5728\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u6ca1\u6709\u5168\u9762\u7684\u7efc\u8ff0\u7cfb\u7edf\u5730\u603b\u7ed3\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684PCD\u7684\u53d1\u5c55\u3002", "method": "\u5c06PCD \u0444\u043e\u0440\u043c\u0443\u043b\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u4e3a\u4e00\u4e2a\u4e24\u6b65\u8fc7\u7a0b\uff1a\u79bb\u7fa4\u503c\u53bb\u9664\u548c\u8868\u9762\u566a\u58f0\u6062\u590d\u3002", "result": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684PCD\u6a21\u578b\uff0c\u4ee5\u5176\u5f3a\u5927\u7684\u8868\u793a\u80fd\u529b\u548c\u7075\u6d3b\u7684\u67b6\u6784\u800c\u95fb\u540d\uff0c\u5df2\u7ecf\u5728\u53bb\u566a\u6027\u80fd\u4e0a\u8d85\u8fc7\u4e86\u4f20\u7edf\u65b9\u6cd5\u3002\u6bd4\u8f83\u4e86\u5404\u79cd\u65b9\u6cd5\u7684\u5f02\u540c\u548c\u5404\u81ea\u7684\u4f18\u70b9\u3002", "conclusion": "\u672c\u6587\u5168\u9762\u5730\u603b\u7ed3\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u70b9\u4e91\u53bb\u566a (PCD) \u7684\u53d1\u5c55\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u53bb\u566a\u4efb\u52a1\u7684\u5206\u7c7b\u6cd5\u3002\u6700\u540e\uff0c\u8ba8\u8bba\u4e86\u7814\u7a76\u7684\u5c40\u9650\u6027\u548c\u672a\u6765\u7684\u65b9\u5411\uff0c\u4e3aPCD\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2508.12338", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12338", "abs": "https://arxiv.org/abs/2508.12338", "authors": ["Wenzhen Yuan", "Shengji Tang", "Weihao Lin", "Jiacheng Ruan", "Ganqu Cui", "Bo Zhang", "Tao Chen", "Ting Liu", "Yuzhuo Fu", "Peng Ye", "Lei Bai"], "title": "Wisdom of the Crowd: Reinforcement Learning from Coevolutionary Collective Feedback", "comment": null, "summary": "Reinforcement learning (RL) has significantly enhanced the reasoning\ncapabilities of large language models (LLMs), but its reliance on expensive\nhuman-labeled data or complex reward models severely limits scalability. While\nexisting self-feedback methods aim to address this problem, they are\nconstrained by the capabilities of a single model, which can lead to\noverconfidence in incorrect answers, reward hacking, and even training\ncollapse. To this end, we propose Reinforcement Learning from Coevolutionary\nCollective Feedback (RLCCF), a novel RL framework that enables multi-model\ncollaborative evolution without external supervision. Specifically, RLCCF\noptimizes the ability of a model collective by maximizing its Collective\nConsistency (CC), which jointly trains a diverse ensemble of LLMs and provides\nreward signals by voting on collective outputs. Moreover, each model's vote is\nweighted by its Self-Consistency (SC) score, ensuring that more confident\nmodels contribute more to the collective decision. Benefiting from the diverse\noutput distributions and complementary abilities of multiple LLMs, RLCCF\nenables the model collective to continuously enhance its reasoning ability\nthrough coevolution. Experiments on four mainstream open-source LLMs across\nfour mathematical reasoning benchmarks demonstrate that our framework yields\nsignificant performance gains, achieving an average relative improvement of\n16.72\\% in accuracy. Notably, RLCCF not only improves the performance of\nindividual models but also enhances the group's majority-voting accuracy by\n4.51\\%, demonstrating its ability to extend the collective capability boundary\nof the model collective.", "AI": {"tldr": "RLCCF is a new reinforcement learning framework that uses multi-model collaborative evolution to improve the reasoning capabilities of large language models without external supervision, achieving significant performance gains on mathematical reasoning benchmarks.", "motivation": "Existing self-feedback methods are constrained by the capabilities of a single model, which can lead to overconfidence in incorrect answers, reward hacking, and even training collapse. The reliance on expensive human-labeled data or complex reward models severely limits scalability.", "method": "The paper proposes Reinforcement Learning from Coevolutionary Collective Feedback (RLCCF), a novel RL framework that enables multi-model collaborative evolution without external supervision. It optimizes the ability of a model collective by maximizing its Collective Consistency (CC), which jointly trains a diverse ensemble of LLMs and provides reward signals by voting on collective outputs. Each model's vote is weighted by its Self-Consistency (SC) score.", "result": "Experiments on four mainstream open-source LLMs across four mathematical reasoning benchmarks demonstrate that the framework yields significant performance gains, achieving an average relative improvement of 16.72% in accuracy. The group's majority-voting accuracy is enhanced by 4.51%.", "conclusion": "The RLCCF framework significantly improves the performance of individual models and enhances the group's majority-voting accuracy, demonstrating its ability to extend the collective capability boundary of the model collective."}}
{"id": "2508.11985", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11985", "abs": "https://arxiv.org/abs/2508.11985", "authors": ["Zhanhao Cao", "Clement Truong", "Andrew Lizarraga"], "title": "Efficient Modular Learning through Naive LoRA Summation: Leveraging Orthogonality in High-Dimensional Models", "comment": "Preprint", "summary": "Recent advances in large language models are driven by scale, while\nparameter-efficient fine-tuning (PEFT) enables updating only a small fraction\nof parameters. Low-Rank Adaptation (LoRA) stores parameter deltas as the\nproduct of two small matrices, which makes them natural building blocks that\ncan be composed. Motivated by the superposition principle, we hypothesize that\nindependently trained LoRA modules on disjoint domains are approximately\northogonal and can be combined by simple addition. Using GPT-2 Small (117M)\nwith LoRA rank 4 and alpha=64, we train adapters for three QA domains (math,\nmedicine, finance). In pairwise tests, adding Math+Medicine adapters improves\nperplexity by -9.10% relative to merged-data fine-tuning, while Math+Finance\nand Finance+Medicine change by +4.54% and +27.56%, respectively. Across\ncombinations, the RMS cosine similarity between LoRA deltas correlates\npositively and approximately linearly with the change in perplexity. Naive\nsummation requires no additional training, can be applied in seconds, and\nachieves performance comparable to models trained on merged data, while\nclarifying when interference appears in higher-order compositions.", "AI": {"tldr": "Independently trained LoRA modules on disjoint domains are approximately orthogonal and can be combined by simple addition.", "motivation": "Motivated by the superposition principle, we hypothesize that independently trained LoRA modules on disjoint domains are approximately orthogonal and can be combined by simple addition.", "method": "Using GPT-2 Small (117M) with LoRA rank 4 and alpha=64, we train adapters for three QA domains (math, medicine, finance).", "result": "In pairwise tests, adding Math+Medicine adapters improves perplexity by -9.10% relative to merged-data fine-tuning, while Math+Finance and Finance+Medicine change by +4.54% and +27.56%, respectively. Across combinations, the RMS cosine similarity between LoRA deltas correlates positively and approximately linearly with the change in perplexity.", "conclusion": "Naive summation requires no additional training, can be applied in seconds, and achieves performance comparable to models trained on merged data, while clarifying when interference appears in higher-order compositions."}}
{"id": "2508.12243", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12243", "abs": "https://arxiv.org/abs/2508.12243", "authors": ["Wuttikorn Ponwitayarat", "Raymond Ng", "Jann Railey Montalan", "Thura Aung", "Jian Gang Ngui", "Yosephine Susanto", "William Tjhi", "Panuthep Tasawong", "Erik Cambria", "Ekapol Chuangsuwanich", "Sarana Nutanong", "Peerat Limkonchotiwat"], "title": "SEA-BED: Southeast Asia Embedding Benchmark", "comment": null, "summary": "Sentence embeddings are essential for NLP tasks such as semantic search,\nre-ranking, and textual similarity. Although multilingual benchmarks like MMTEB\nbroaden coverage, Southeast Asia (SEA) datasets are scarce and often\nmachine-translated, missing native linguistic properties. With nearly 700\nmillion speakers, the SEA region lacks a region-specific embedding benchmark.\nWe introduce SEA-BED, the first large-scale SEA embedding benchmark with 169\ndatasets across 9 tasks and 10 languages, where 71% are formulated by humans,\nnot machine generation or translation. We address three research questions: (1)\nwhich SEA languages and tasks are challenging, (2) whether SEA languages show\nunique performance gaps globally, and (3) how human vs. machine translations\naffect evaluation. We evaluate 17 embedding models across six studies,\nanalyzing task and language challenges, cross-benchmark comparisons, and\ntranslation trade-offs. Results show sharp ranking shifts, inconsistent model\nperformance among SEA languages, and the importance of human-curated datasets\nfor low-resource languages like Burmese.", "AI": {"tldr": "Introduce SEA-BED, the first large-scale SEA embedding benchmark with 169 datasets across 9 tasks and 10 languages, where 71% are formulated by humans, not machine generation or translation. Results show sharp ranking shifts, inconsistent model performance among SEA languages, and the importance of human-curated datasets for low-resource languages like Burmese.", "motivation": "Southeast Asia (SEA) datasets are scarce and often machine-translated, missing native linguistic properties. With nearly 700 million speakers, the SEA region lacks a region-specific embedding benchmark.", "method": "We introduce SEA-BED, the first large-scale SEA embedding benchmark with 169 datasets across 9 tasks and 10 languages, where 71% are formulated by humans, not machine generation or translation. We evaluate 17 embedding models across six studies, analyzing task and language challenges, cross-benchmark comparisons, and translation trade-offs.", "result": "which SEA languages and tasks are challenging, whether SEA languages show unique performance gaps globally, and how human vs. machine translations affect evaluation. Results show sharp ranking shifts, inconsistent model performance among SEA languages, and the importance of human-curated datasets for low-resource languages like Burmese.", "conclusion": "Results show sharp ranking shifts, inconsistent model performance among SEA languages, and the importance of human-curated datasets for low-resource languages like Burmese."}}
{"id": "2508.11950", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11950", "abs": "https://arxiv.org/abs/2508.11950", "authors": ["Tingbang Liang", "Yixin Zeng", "Jiatong Xie", "Boyu Zhou"], "title": "DynamicPose: Real-time and Robust 6D Object Pose Tracking for Fast-Moving Cameras and Objects", "comment": null, "summary": "We present DynamicPose, a retraining-free 6D pose tracking framework that\nimproves tracking robustness in fast-moving camera and object scenarios.\nPrevious work is mainly applicable to static or quasi-static scenes, and its\nperformance significantly deteriorates when both the object and the camera move\nrapidly. To overcome these challenges, we propose three synergistic components:\n(1) A visual-inertial odometry compensates for the shift in the Region of\nInterest (ROI) caused by camera motion; (2) A depth-informed 2D tracker\ncorrects ROI deviations caused by large object translation; (3) A VIO-guided\nKalman filter predicts object rotation, generates multiple candidate poses, and\nthen obtains the final pose by hierarchical refinement. The 6D pose tracking\nresults guide subsequent 2D tracking and Kalman filter updates, forming a\nclosed-loop system that ensures accurate pose initialization and precise pose\ntracking. Simulation and real-world experiments demonstrate the effectiveness\nof our method, achieving real-time and robust 6D pose tracking for fast-moving\ncameras and objects.", "AI": {"tldr": "DynamicPose, a retraining-free 6D pose tracking framework that improves tracking robustness in fast-moving camera and object scenarios", "motivation": "improves tracking robustness in fast-moving camera and object scenarios. Previous work is mainly applicable to static or quasi-static scenes, and its performance significantly deteriorates when both the object and the camera move rapidly", "method": "propose three synergistic components: (1) A visual-inertial odometry compensates for the shift in the Region of Interest (ROI) caused by camera motion; (2) A depth-informed 2D tracker corrects ROI deviations caused by large object translation; (3) A VIO-guided Kalman filter predicts object rotation, generates multiple candidate poses, and then obtains the final pose by hierarchical refinement", "result": "Simulation and real-world experiments demonstrate the effectiveness of our method", "conclusion": "achieving real-time and robust 6D pose tracking for fast-moving cameras and objects"}}
{"id": "2508.12375", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12375", "abs": "https://arxiv.org/abs/2508.12375", "authors": ["Yu Sha", "Shuiping Gou", "Bo Liu", "Johannes Faber", "Ningtao Liu", "Stefan Schramm", "Horst Stoecker", "Thomas Steckenreiter", "Domagoj Vnucec", "Nadine Wetzstein", "Andreas Widl", "Kai Zhou"], "title": "Hierarchical knowledge guided fault intensity diagnosis of complex industrial systems", "comment": "12 pages", "summary": "Fault intensity diagnosis (FID) plays a pivotal role in monitoring and\nmaintaining mechanical devices within complex industrial systems. As current\nFID methods are based on chain of thought without considering dependencies\namong target classes. To capture and explore dependencies, we propose a\nhierarchical knowledge guided fault intensity diagnosis framework (HKG)\ninspired by the tree of thought, which is amenable to any representation\nlearning methods. The HKG uses graph convolutional networks to map the\nhierarchical topological graph of class representations into a set of\ninterdependent global hierarchical classifiers, where each node is denoted by\nword embeddings of a class. These global hierarchical classifiers are applied\nto learned deep features extracted by representation learning, allowing the\nentire model to be end-to-end learnable. In addition, we develop a re-weighted\nhierarchical knowledge correlation matrix (Re-HKCM) scheme by embedding\ninter-class hierarchical knowledge into a data-driven statistical correlation\nmatrix (SCM) which effectively guides the information sharing of nodes in\ngraphical convolutional neural networks and avoids over-smoothing issues. The\nRe-HKCM is derived from the SCM through a series of mathematical\ntransformations. Extensive experiments are performed on four real-world\ndatasets from different industrial domains (three cavitation datasets from\nSAMSON AG and one existing publicly) for FID, all showing superior results and\noutperform recent state-of-the-art FID methods.", "AI": {"tldr": "Developed HKG framework for fault intensity diagnosis, capturing dependencies among target classes using hierarchical knowledge and graph convolutional networks, achieving state-of-the-art performance.", "motivation": "Current FID methods lack consideration of dependencies among target classes.", "method": "A hierarchical knowledge guided fault intensity diagnosis framework (HKG) using graph convolutional networks and a re-weighted hierarchical knowledge correlation matrix (Re-HKCM).", "result": "Superior results on four real-world datasets compared to recent state-of-the-art FID methods.", "conclusion": "The proposed HKG framework outperforms state-of-the-art FID methods on four real-world datasets."}}
{"id": "2508.11990", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.11990", "abs": "https://arxiv.org/abs/2508.11990", "authors": ["Evan Dogariu", "Anand Brahmbhatt", "Elad Hazan"], "title": "Universal Learning of Nonlinear Dynamics", "comment": null, "summary": "We study the fundamental problem of learning a marginally stable unknown\nnonlinear dynamical system. We describe an algorithm for this problem, based on\nthe technique of spectral filtering, which learns a mapping from past\nobservations to the next based on a spectral representation of the system.\nUsing techniques from online convex optimization, we prove vanishing prediction\nerror for any nonlinear dynamical system that has finitely many marginally\nstable modes, with rates governed by a novel quantitative control-theoretic\nnotion of learnability. The main technical component of our method is a new\nspectral filtering algorithm for linear dynamical systems, which incorporates\npast observations and applies to general noisy and marginally stable systems.\nThis significantly generalizes the original spectral filtering algorithm to\nboth asymmetric dynamics as well as incorporating noise correction, and is of\nindependent interest.", "AI": {"tldr": "learns a mapping from past observations to the next based on a spectral representation of the system", "motivation": "learning a marginally stable unknown nonlinear dynamical system", "method": "spectral filtering", "result": "a new spectral filtering algorithm for linear dynamical systems, which incorporates past observations and applies to general noisy and marginally stable systems", "conclusion": "vanishing prediction error for any nonlinear dynamical system that has finitely many marginally stable modes"}}
{"id": "2508.12255", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.12255", "abs": "https://arxiv.org/abs/2508.12255", "authors": ["Ankita Pasad"], "title": "What do Speech Foundation Models Learn? Analysis and Applications", "comment": "Ph.D. Thesis", "summary": "Speech foundation models (SFMs) are designed to serve as general-purpose\nrepresentations for a wide range of speech-processing tasks. The last five\nyears have seen an influx of increasingly successful self-supervised and\nsupervised pre-trained models with impressive performance on various downstream\ntasks.\n  Although the zoo of SFMs continues to grow, our understanding of the\nknowledge they acquire lags behind. This thesis presents a lightweight analysis\nframework using statistical tools and training-free tasks to investigate the\nacoustic and linguistic knowledge encoded in SFM layers. We conduct a\ncomparative study across multiple SFMs and statistical tools. Our study also\nshows that the analytical insights have concrete implications for downstream\ntask performance.\n  The effectiveness of an SFM is ultimately determined by its performance on\nspeech applications. Yet it remains unclear whether the benefits extend to\nspoken language understanding (SLU) tasks that require a deeper understanding\nthan widely studied ones, such as speech recognition. The limited exploration\nof SLU is primarily due to a lack of relevant datasets. To alleviate that, this\nthesis contributes tasks, specifically spoken named entity recognition (NER)\nand named entity localization (NEL), to the Spoken Language Understanding\nEvaluation benchmark. We develop SFM-based approaches for NER and NEL, and find\nthat end-to-end (E2E) models leveraging SFMs can surpass traditional cascaded\n(speech recognition followed by a text model) approaches. Further, we evaluate\nE2E SLU models across SFMs and adaptation strategies to assess the impact on\ntask performance.\n  Collectively, this thesis tackles previously unanswered questions about SFMs,\nproviding tools and datasets to further our understanding and to enable the\ncommunity to make informed design choices for future model development and\nadoption.", "AI": {"tldr": "This thesis analyzes speech foundation models (SFMs) using statistical tools and introduces new datasets for spoken language understanding (SLU) tasks, demonstrating that SFMs can improve performance on these tasks.", "motivation": "Although the zoo of SFMs continues to grow, our understanding of the knowledge they acquire lags behind.The limited exploration of SLU is primarily due to a lack of relevant datasets.", "method": "a lightweight analysis framework using statistical tools and training-free tasks to investigate the acoustic and linguistic knowledge encoded in SFM layers; SFM-based approaches for NER and NEL", "result": "analytical insights have concrete implications for downstream task performance; end-to-end (E2E) models leveraging SFMs can surpass traditional cascaded (speech recognition followed by a text model) approaches.", "conclusion": "This thesis tackles previously unanswered questions about SFMs, providing tools and datasets to further our understanding and to enable the community to make informed design choices for future model development and adoption."}}
{"id": "2508.11951", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11951", "abs": "https://arxiv.org/abs/2508.11951", "authors": ["Hao Peng", "Hong Sang", "Yajing Ma", "Ping Qiu", "Chao Ji"], "title": "Transferable Class Statistics and Multi-scale Feature Approximation for 3D Object Detection", "comment": null, "summary": "This paper investigates multi-scale feature approximation and transferable\nfeatures for object detection from point clouds. Multi-scale features are\ncritical for object detection from point clouds. However, multi-scale feature\nlearning usually involves multiple neighborhood searches and scale-aware\nlayers, which can hinder efforts to achieve lightweight models and may not be\nconducive to research constrained by limited computational resources. This\npaper approximates point-based multi-scale features from a single neighborhood\nbased on knowledge distillation. To compensate for the loss of constructive\ndiversity in a single neighborhood, this paper designs a transferable feature\nembedding mechanism. Specifically, class-aware statistics are employed as\ntransferable features given the small computational cost. In addition, this\npaper introduces the central weighted intersection over union for localization\nto alleviate the misalignment brought by the center offset in optimization.\nNote that the method presented in this paper saves computational costs.\nExtensive experiments on public datasets demonstrate the effectiveness of the\nproposed method.", "AI": {"tldr": "This paper proposes a computationally efficient method for object detection from point clouds by approximating multi-scale features and using transferable feature embedding.", "motivation": "Multi-scale feature learning in point cloud object detection is computationally expensive and hinders lightweight models, especially with limited resources.", "method": "The paper approximates point-based multi-scale features from a single neighborhood based on knowledge distillation and designs a transferable feature embedding mechanism using class-aware statistics. It also introduces the central weighted intersection over union for localization.", "result": "Extensive experiments on public datasets demonstrate the effectiveness of the proposed method.", "conclusion": "The proposed method is effective and saves computational costs, as demonstrated by experiments on public datasets."}}
{"id": "2508.12379", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12379", "abs": "https://arxiv.org/abs/2508.12379", "authors": ["Rongzheng Wang", "Qizhi Chen", "Yihong Huang", "Yizhuo Ma", "Muquan Li", "Jiakai Li", "Ke Qin", "Guangchun Luo", "Shuang Liang"], "title": "GraphCogent: Overcoming LLMs' Working Memory Constraints via Multi-Agent Collaboration in Complex Graph Understanding", "comment": null, "summary": "Large language models (LLMs) show promising performance on small-scale graph\nreasoning tasks but fail when handling real-world graphs with complex queries.\nThis phenomenon stems from LLMs' inability to effectively process complex graph\ntopology and perform multi-step reasoning simultaneously. To address these\nlimitations, we propose GraphCogent, a collaborative agent framework inspired\nby human Working Memory Model that decomposes graph reasoning into specialized\ncognitive processes: sense, buffer, and execute. The framework consists of\nthree modules: Sensory Module standardizes diverse graph text representations\nvia subgraph sampling, Buffer Module integrates and indexes graph data across\nmultiple formats, and Execution Module combines tool calling and model\ngeneration for efficient reasoning. We also introduce Graph4real, a\ncomprehensive benchmark contains with four domains of real-world graphs (Web,\nSocial, Transportation, and Citation) to evaluate LLMs' graph reasoning\ncapabilities. Our Graph4real covers 21 different graph reasoning tasks,\ncategorized into three types (Structural Querying, Algorithmic Reasoning, and\nPredictive Modeling tasks), with graph scales that are 10 times larger than\nexisting benchmarks. Experiments show that Llama3.1-8B based GraphCogent\nachieves a 50% improvement over massive-scale LLMs like DeepSeek-R1 (671B).\nCompared to state-of-the-art agent-based baseline, our framework outperforms by\n20% in accuracy while reducing token usage by 80% for in-toolset tasks and 30%\nfor out-toolset tasks. Code will be available after review.", "AI": {"tldr": "GraphCogent, a new framework, significantly improves graph reasoning performance and efficiency compared to existing methods.", "motivation": "LLMs' inability to effectively process complex graph topology and perform multi-step reasoning simultaneously.", "method": "We propose GraphCogent, a collaborative agent framework inspired by human Working Memory Model that decomposes graph reasoning into specialized cognitive processes: sense, buffer, and execute. The framework consists of three modules: Sensory Module standardizes diverse graph text representations via subgraph sampling, Buffer Module integrates and indexes graph data across multiple formats, and Execution Module combines tool calling and model generation for efficient reasoning.", "result": "GraphCogent achieves significant improvements in accuracy and token usage compared to existing LLMs and agent-based baselines on the Graph4real benchmark.", "conclusion": "Llama3.1-8B based GraphCogent achieves a 50% improvement over massive-scale LLMs like DeepSeek-R1 (671B). Compared to state-of-the-art agent-based baseline, our framework outperforms by 20% in accuracy while reducing token usage by 80% for in-toolset tasks and 30% for out-toolset tasks."}}
{"id": "2508.12021", "categories": ["cs.LG", "cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.12021", "abs": "https://arxiv.org/abs/2508.12021", "authors": ["You Hak Lee", "Xiaofan Yu", "Quanling Zhao", "Flavio Ponzina", "Tajana Rosing"], "title": "FedUHD: Unsupervised Federated Learning using Hyperdimensional Computing", "comment": null, "summary": "Unsupervised federated learning (UFL) has gained attention as a\nprivacy-preserving, decentralized machine learning approach that eliminates the\nneed for labor-intensive data labeling. However, UFL faces several challenges\nin practical applications: (1) non-independent and identically distributed\n(non-iid) data distribution across devices, (2) expensive computational and\ncommunication costs at the edge, and (3) vulnerability to communication noise.\nPrevious UFL approaches have relied on deep neural networks (NN), which\nintroduce substantial overhead in both computation and communication. In this\npaper, we propose FedUHD, the first UFL framework based on Hyperdimensional\nComputing (HDC). HDC is a brain-inspired computing scheme with lightweight\ntraining and inference operations, much smaller model size, and robustness to\ncommunication noise. FedUHD introduces two novel HDC-based designs to improve\nUFL performance. On the client side, a kNN-based cluster hypervector removal\nmethod addresses non-iid data samples by eliminating detrimental outliers. On\nthe server side, a weighted HDC aggregation technique balances the non-iid data\ndistribution across clients. Our experiments demonstrate that FedUHD achieves\nup to 173.6x and 612.7x better speedup and energy efficiency, respectively, in\ntraining, up to 271x lower communication cost, and 15.50% higher accuracy on\naverage across diverse settings, along with superior robustness to various\ntypes of noise compared to state-of-the-art NN-based UFL approaches.", "AI": {"tldr": "FedUHD \u662f\u4e00\u79cd\u57fa\u4e8e\u8d85\u7ef4\u8ba1\u7b97 (HDC) \u7684\u65e0\u76d1\u7763\u8054\u90a6\u5b66\u4e60 (UFL) \u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5728\u5ba2\u6237\u7aef\u4f7f\u7528 kNN \u805a\u7c7b\u8d85\u5411\u91cf\u53bb\u9664\u65b9\u6cd5\u548c\u5728\u670d\u52a1\u5668\u7aef\u4f7f\u7528\u52a0\u6743 HDC \u805a\u5408\u6280\u672f\u6765\u63d0\u9ad8 UFL \u6027\u80fd\uff0c\u4ece\u800c\u5728\u8bad\u7ec3\u901f\u5ea6\u3001\u80fd\u6548\u3001\u901a\u4fe1\u6210\u672c\u548c\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\u3002", "motivation": "\u65e0\u76d1\u7763\u8054\u90a6\u5b66\u4e60 (UFL) \u4f5c\u4e3a\u4e00\u79cd\u4fdd\u62a4\u9690\u79c1\u7684\u53bb\u4e2d\u5fc3\u5316\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u800c\u53d7\u5230\u5173\u6ce8\uff0c\u5b83\u65e0\u9700\u4eba\u5de5\u6570\u636e\u6807\u8bb0\u3002\u4f46\u662f\uff0cUFL \u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u7740\u4e00\u4e9b\u6311\u6218\uff1a(1) \u8bbe\u5907\u4e4b\u95f4\u7684\u975e\u72ec\u7acb\u4e14\u76f8\u540c\u5206\u5e03 (non-iid) \u6570\u636e\u5206\u5e03\uff0c(2) \u8fb9\u7f18\u5904\u6602\u8d35\u7684\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\uff0c\u4ee5\u53ca (3) \u5bb9\u6613\u53d7\u5230\u901a\u4fe1\u566a\u58f0\u7684\u5f71\u54cd\u3002\u4ee5\u524d\u7684 UFL \u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (NN)\uff0c\u8fd9\u4f1a\u5728\u8ba1\u7b97\u548c\u901a\u4fe1\u4e2d\u5f15\u5165\u5927\u91cf\u5f00\u9500\u3002", "method": "FedUHD\uff0c\u7b2c\u4e00\u4e2a\u57fa\u4e8e\u8d85\u7ef4\u8ba1\u7b97 (HDC) \u7684 UFL \u6846\u67b6\u3002\u5728\u5ba2\u6237\u7aef\uff0c\u4e00\u79cd\u57fa\u4e8e kNN \u7684\u805a\u7c7b\u8d85\u5411\u91cf\u53bb\u9664\u65b9\u6cd5\u901a\u8fc7\u6d88\u9664\u6709\u5bb3\u7684\u5f02\u5e38\u503c\u6765\u89e3\u51b3\u975e iid \u6570\u636e\u6837\u672c\u3002\u5728\u670d\u52a1\u5668\u7aef\uff0c\u4e00\u79cd\u52a0\u6743 HDC \u805a\u5408\u6280\u672f\u5e73\u8861\u4e86\u8de8\u5ba2\u6237\u7aef\u7684\u975e iid \u6570\u636e\u5206\u5e03\u3002", "result": "FedUHD \u7684\u8bad\u7ec3\u901f\u5ea6\u5206\u522b\u63d0\u9ad8\u4e86 173.6 \u500d\u548c 612.7 \u500d\uff0c\u80fd\u6548\u63d0\u9ad8\u4e86 173.6 \u500d\u548c 612.7 \u500d\uff0c\u901a\u4fe1\u6210\u672c\u964d\u4f4e\u4e86 271 \u500d\uff0c\u5e76\u4e14\u5728\u5404\u79cd\u8bbe\u7f6e\u4e2d\u7684\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad8\u4e86 15.50%\uff0c\u5e76\u4e14\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e NN \u7684 UFL \u65b9\u6cd5\u76f8\u6bd4\uff0c\u5bf9\u5404\u79cd\u7c7b\u578b\u7684\u566a\u58f0\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "FedUHD\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e2d\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad8\u4e86 15.50%\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u9ad8\u4e86 173.6 \u500d\uff0c\u80fd\u6548\u63d0\u9ad8\u4e86 612.7 \u500d\uff0c\u901a\u4fe1\u6210\u672c\u964d\u4f4e\u4e86 271 \u500d\uff0c\u5e76\u4e14\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e NN \u7684 UFL \u65b9\u6cd5\u76f8\u6bd4\uff0c\u5bf9\u5404\u79cd\u7c7b\u578b\u7684\u566a\u58f0\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.12257", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12257", "abs": "https://arxiv.org/abs/2508.12257", "authors": ["Zheye Deng", "Chunkit Chan", "Tianshi Zheng", "Wei Fan", "Weiqi Wang", "Yangqiu Song"], "title": "Structuring the Unstructured: A Systematic Review of Text-to-Structure Generation for Agentic AI with a Universal Evaluation Framework", "comment": "Under Review", "summary": "The evolution of AI systems toward agentic operation and context-aware\nretrieval necessitates transforming unstructured text into structured formats\nlike tables, knowledge graphs, and charts. While such conversions enable\ncritical applications from summarization to data mining, current research lacks\na comprehensive synthesis of methodologies, datasets, and metrics. This\nsystematic review examines text-to-structure techniques and the encountered\nchallenges, evaluates current datasets and assessment criteria, and outlines\npotential directions for future research. We also introduce a universal\nevaluation framework for structured outputs, establishing text-to-structure as\nfoundational infrastructure for next-generation AI systems.", "AI": {"tldr": "systematic review examines text-to-structure techniques and the encountered challenges, evaluates current datasets and assessment criteria, and outlines potential directions for future research", "motivation": "The evolution of AI systems toward agentic operation and context-aware retrieval necessitates transforming unstructured text into structured formats", "method": "examines text-to-structure techniques", "result": "evaluates current datasets and assessment criteria, and outlines potential directions for future research", "conclusion": "establishes text-to-structure as foundational infrastructure for next-generation AI systems"}}
{"id": "2508.11952", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11952", "abs": "https://arxiv.org/abs/2508.11952", "authors": ["Yueming Xu", "Jiahui Zhang", "Ze Huang", "Yurui Chen", "Yanpeng Zhou", "Zhenyu Chen", "Yu-Jie Yuan", "Pengxiang Xia", "Guowei Huang", "Xinyue Cai", "Zhongang Qi", "Xingyue Quan", "Jianye Hao", "Hang Xu", "Li Zhang"], "title": "UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding", "comment": null, "summary": "Despite the impressive progress on understanding and generating images shown\nby the recent unified architectures, the integration of 3D tasks remains\nchallenging and largely unexplored. In this paper, we introduce UniUGG, the\nfirst unified understanding and generation framework for 3D modalities. Our\nunified framework employs an LLM to comprehend and decode sentences and 3D\nrepresentations. At its core, we propose a spatial decoder leveraging a latent\ndiffusion model to generate high-quality 3D representations. This allows for\nthe generation and imagination of 3D scenes based on a reference image and an\narbitrary view transformation, while remaining supports for spatial visual\nquestion answering (VQA) tasks. Additionally, we propose a geometric-semantic\nlearning strategy to pretrain the vision encoder. This design jointly captures\nthe input's semantic and geometric cues, enhancing both spatial understanding\nand generation. Extensive experimental results demonstrate the superiority of\nour method in visual representation, spatial understanding, and 3D generation.\nThe source code will be released upon paper acceptance.", "AI": {"tldr": "UniUGG is a new unified framework for 3D understanding and generation using a spatial decoder and geometric-semantic learning.", "motivation": "The integration of 3D tasks remains challenging and largely unexplored despite the progress on 2D image understanding and generation.", "method": "The paper proposes a spatial decoder leveraging a latent diffusion model and a geometric-semantic learning strategy to pretrain the vision encoder.", "result": "UniUGG achieves superior performance in visual representation, spatial understanding, and 3D generation.", "conclusion": "The paper introduces UniUGG, a unified framework for 3D understanding and generation, and demonstrates its superiority in visual representation, spatial understanding, and 3D generation through experiments."}}
{"id": "2508.12425", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12425", "abs": "https://arxiv.org/abs/2508.12425", "authors": ["Phuong Minh Nguyen", "Tien Huu Dang", "Naoya Inoue"], "title": "Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning", "comment": null, "summary": "This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved\napproach to standard CoT, for logical reasoning in large language models\n(LLMs). The key idea is to integrate lightweight symbolic representations into\nfew-shot prompts, structuring the inference steps with a consistent strategy to\nmake reasoning patterns more explicit within a non-iterative reasoning process.\nBy incorporating these symbolic structures, our method preserves the\ngeneralizability of standard prompting techniques while enhancing the\ntransparency, interpretability, and analyzability of LLM logical reasoning.\nExtensive experiments on four well-known logical reasoning benchmarks --\nProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse\nreasoning scenarios -- demonstrate the effectiveness of the proposed approach,\nparticularly in complex reasoning tasks that require navigating multiple\nconstraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs'\nreasoning capabilities across various model sizes and significantly outperforms\nconventional CoT on three out of four datasets, ProofWriter, ProntoQA, and\nLogicalDeduction.", "AI": {"tldr": "Symbolic-Aided CoT improves logical reasoning in LLMs by integrating lightweight symbolic representations into prompts, outperforming conventional CoT on most datasets.", "motivation": "to integrate lightweight symbolic representations into few-shot prompts, structuring the inference steps with a consistent strategy to make reasoning patterns more explicit within a non-iterative reasoning process", "method": "integrates lightweight symbolic representations into few-shot prompts, structuring the inference steps with a consistent strategy", "result": "Symbolic-Aided CoT consistently improves LLMs' reasoning capabilities across various model sizes and significantly outperforms conventional CoT on three out of four datasets, ProofWriter, ProntoQA, and LogicalDeduction", "conclusion": "Symbolic-Aided CoT consistently improves LLMs' reasoning capabilities across various model sizes and significantly outperforms conventional CoT on three out of four datasets."}}
{"id": "2508.12042", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12042", "abs": "https://arxiv.org/abs/2508.12042", "authors": ["Zahra Kharaghani", "Ali Dadras", "Tommy L\u00f6fstedt"], "title": "Fairness Regularization in Federated Learning", "comment": "25 pages", "summary": "Federated Learning (FL) has emerged as a vital paradigm in modern machine\nlearning that enables collaborative training across decentralized data sources\nwithout exchanging raw data. This approach not only addresses privacy concerns\nbut also allows access to overall substantially larger and potentially more\ndiverse datasets, without the need for centralized storage or hardware\nresources. However, heterogeneity in client data may cause certain clients to\nhave disproportionate impacts on the global model, leading to disparities in\nthe clients' performances. Fairness, therefore, becomes a crucial concern in FL\nand can be addressed in various ways. However, the effectiveness of existing\nfairness-aware methods, particularly in heterogeneous data settings, remains\nunclear, and the relationships between different approaches are not well\nunderstood. In this work, we focus on performance equitable fairness, which\naims to minimize differences in performance across clients. We restrict our\nstudy to fairness-aware methods that explicitly regularize client losses,\nevaluating both existing and newly proposed approaches. We identify and\ntheoretically explain connections between the investigated fairness methods,\nand empirically show that FairGrad (approximate) and FairGrad* (exact) (two\nvariants of a gradient variance regularization method introduced here for\nperformance equitable fairness) improve both fairness and overall model\nperformance in heterogeneous data settings.", "AI": {"tldr": "This paper focuses on performance equitable fairness in Federated Learning, introduces FairGrad and FairGrad* (two variants of a gradient variance regularization method), and demonstrates their effectiveness in improving fairness and overall model performance in heterogeneous data settings.", "motivation": "Fairness is a crucial concern in Federated Learning (FL), and the effectiveness of existing fairness-aware methods, particularly in heterogeneous data settings, remains unclear, and the relationships between different approaches are not well understood.", "method": "gradient variance regularization", "result": "Identified and theoretically explain connections between the investigated fairness methods, and empirically show that FairGrad (approximate) and FairGrad* (exact) improve both fairness and overall model performance in heterogeneous data settings.", "conclusion": "FairGrad (approximate) and FairGrad* (exact) improve both fairness and overall model performance in heterogeneous data settings."}}
{"id": "2508.12265", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12265", "abs": "https://arxiv.org/abs/2508.12265", "authors": ["Xinda Jia", "Jinpeng Li", "Zezhong Wang", "Jingjing Li", "Xingshan Zeng", "Yasheng Wang", "Weinan Zhang", "Yong Yu", "Weiwen Liu"], "title": "Fast, Slow, and Tool-augmented Thinking for LLMs: A Review", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in\nreasoning across diverse domains. However, effective reasoning in real-world\ntasks requires adapting the reasoning strategy to the demands of the problem,\nranging from fast, intuitive responses to deliberate, step-by-step reasoning\nand tool-augmented thinking. Drawing inspiration from cognitive psychology, we\npropose a novel taxonomy of LLM reasoning strategies along two knowledge\nboundaries: a fast/slow boundary separating intuitive from deliberative\nprocesses, and an internal/external boundary distinguishing reasoning grounded\nin the model's parameters from reasoning augmented by external tools. We\nsystematically survey recent work on adaptive reasoning in LLMs and categorize\nmethods based on key decision factors. We conclude by highlighting open\nchallenges and future directions toward more adaptive, efficient, and reliable\nLLMs.", "AI": {"tldr": "This paper proposes a taxonomy of LLM reasoning strategies and surveys recent work on adaptive reasoning in LLMs. It also highlights open challenges and future directions.", "motivation": "Effective reasoning in real-world tasks requires adapting the reasoning strategy to the demands of the problem.", "method": "This paper systematically survey recent work on adaptive reasoning in LLMs and categorize methods based on key decision factors.", "result": "This paper propose a novel taxonomy of LLM reasoning strategies along two knowledge boundaries: a fast/slow boundary separating intuitive from deliberative processes, and an internal/external boundary distinguishing reasoning grounded in the model's parameters from reasoning augmented by external tools.", "conclusion": "This paper concludes by highlighting open challenges and future directions toward more adaptive, efficient, and reliable LLMs."}}
{"id": "2508.11955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11955", "abs": "https://arxiv.org/abs/2508.11955", "authors": ["Seunghun Lee", "Jiwan Seo", "Jeonghoon Kim", "Siwon Kim", "Haeun Yun", "Hyogyeong Jeon", "Wonhyeok Choi", "Jaehoon Jeong", "Zane Durante", "Sang Hyun Park", "Sunghoon Im"], "title": "SAMDWICH: Moment-aware Video-text Alignment for Referring Video Object Segmentation", "comment": "Project page: https://seung-hun-lee.github.io/projects/SAMDWICH/", "summary": "Referring Video Object Segmentation (RVOS) aims to segment and track objects\nin videos based on natural language expressions, requiring precise alignment\nbetween visual content and textual queries. However, existing methods often\nsuffer from semantic misalignment, largely due to indiscriminate frame sampling\nand supervision of all visible objects during training -- regardless of their\nactual relevance to the expression. To address this, we introduce a\nmoment-aware RVOS framework named SAMDWICH, along with a newly annotated\ndataset, MeViS-M, built upon the challenging MeViS benchmark. We manually\nannotate temporal moments indicating when each object is referred to by the\nexpression, enabling semantically grounded supervision that strengthens\nvideo-text alignment. SAMDWICH leverages these aligned text-to-clip pairs to\nguide training, significantly enhancing referential understanding. Building\nupon this framework, we propose Moment-guided Dual-path Propagation (MDP), a\nmoment-aware propagation strategy that improves both object grounding and\ntracking by training on both relevant and irrelevant frames through a\nmoment-centric memory mechanism. In addition, we introduce Object-level\nSelective Supervision (OSS), an object-level filtering strategy that supervises\nonly the objects temporally aligned with the expression in each training clip.\nThis selective supervision reduces semantic noise and reinforces\nlanguage-conditioned learning. Extensive experiments show that SAMDWICH\nachieves state-of-the-art performance on challenging MeViS benchmark,\nparticularly excelling in complex scenarios involving diverse expressions.", "AI": {"tldr": "SAMDWICH\u901a\u8fc7\u5f15\u5165\u65f6\u523b\u611f\u77e5\u6846\u67b6\u548c\u9009\u62e9\u6027\u76d1\u7763\uff0c\u89e3\u51b3\u4e86RVOS\u4e2d\u7684\u8bed\u4e49\u4e0d\u5bf9\u51c6\u95ee\u9898\uff0c\u4ece\u800c\u5728MeViS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u4f1a\u906d\u53d7\u8bed\u4e49\u4e0d\u5bf9\u51c6\u7684\u56f0\u6270\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u4e0d\u5206\u9752\u7ea2\u7682\u767d\u7684\u5e27\u91c7\u6837\u4ee5\u53ca\u5728\u8bad\u7ec3\u671f\u95f4\u5bf9\u6240\u6709\u53ef\u89c1\u5bf9\u8c61\u7684\u76d1\u7763-\u65e0\u8bba\u5b83\u4eec\u4e0e\u8868\u8fbe\u7684\u5b9e\u9645\u76f8\u5173\u6027\u5982\u4f55\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aSAMDWICH\u7684\u65f6\u523b\u611f\u77e5RVOS\u6846\u67b6\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5efa\u7acb\u5728\u5177\u6709\u6311\u6218\u6027\u7684MeViS\u57fa\u51c6\u4e4b\u4e0a\u7684\u65b0\u6ce8\u91ca\u6570\u636e\u96c6MeViS-M\u3002\u6211\u4eec\u624b\u52a8\u6ce8\u91ca\u65f6\u95f4\u65f6\u523b\uff0c\u6307\u793a\u6bcf\u4e2a\u5bf9\u8c61\u4f55\u65f6\u88ab\u8868\u8fbe\u5f0f\u5f15\u7528\uff0c\u4ece\u800c\u5b9e\u73b0\u8bed\u4e49\u63a5\u5730\u7684\u76d1\u7763\uff0c\u4ece\u800c\u52a0\u5f3a\u89c6\u9891\u6587\u672c\u5bf9\u9f50\u3002SAMDWICH\u5229\u7528\u8fd9\u4e9b\u5bf9\u9f50\u7684\u6587\u672c\u5230\u526a\u8f91\u5bf9\u6765\u6307\u5bfc\u8bad\u7ec3\uff0c\u4ece\u800c\u663e\u7740\u589e\u5f3a\u4e86\u6307\u79f0\u7406\u89e3\u3002\u5728\u6b64\u6846\u67b6\u7684\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u523b\u5f15\u5bfc\u7684\u53cc\u8def\u5f84\u4f20\u64ad\uff08MDP\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65f6\u523b\u611f\u77e5\u7684\u4f20\u64ad\u7b56\u7565\uff0c\u901a\u8fc7\u5728\u901a\u8fc7\u65f6\u523b\u4e3a\u4e2d\u5fc3\u7684\u8bb0\u5fc6\u673a\u5236\u5728\u76f8\u5173\u548c\u4e0d\u76f8\u5173\u7684\u5e27\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ece\u800c\u6539\u5584\u4e86\u5bf9\u8c61\u7684\u57fa\u7840\u548c\u8ddf\u8e2a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u5bf9\u8c61\u7ea7\u9009\u62e9\u6027\u76d1\u7763\uff08OSS\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5bf9\u8c61\u7ea7\u8fc7\u6ee4\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u4ec5\u76d1\u7763\u6bcf\u4e2a\u8bad\u7ec3\u526a\u8f91\u4e2d\u4e0e\u8868\u8fbe\u5f0f\u5728\u65f6\u95f4\u4e0a\u5bf9\u9f50\u7684\u5bf9\u8c61\u3002\u8fd9\u79cd\u9009\u62e9\u6027\u76d1\u7763\u51cf\u5c11\u4e86\u8bed\u4e49\u566a\u58f0\u5e76\u589e\u5f3a\u4e86\u8bed\u8a00\u6761\u4ef6\u5b66\u4e60\u3002", "result": "SAMDWICH\u5728\u5177\u6709\u6311\u6218\u6027\u7684MeViS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u5404\u79cd\u8868\u8fbe\u5f0f\u7684\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "SAMDWICH\u5728MeViS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u5404\u79cd\u8868\u8fbe\u5f0f\u7684\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.12472", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12472", "abs": "https://arxiv.org/abs/2508.12472", "authors": ["Yifang Tian", "Yaming Liu", "Zichun Chong", "Zihang Huang", "Hans-Arno Jacobsen"], "title": "GALA: Can Graph-Augmented Large Language Model Agentic Workflows Elevate Root Cause Analysis?", "comment": "12 pages, 5 figures", "summary": "Root cause analysis (RCA) in microservice systems is challenging, requiring\non-call engineers to rapidly diagnose failures across heterogeneous telemetry\nsuch as metrics, logs, and traces. Traditional RCA methods often focus on\nsingle modalities or merely rank suspect services, falling short of providing\nactionable diagnostic insights with remediation guidance. This paper introduces\nGALA, a novel multi-modal framework that combines statistical causal inference\nwith LLM-driven iterative reasoning for enhanced RCA. Evaluated on an\nopen-source benchmark, GALA achieves substantial improvements over\nstate-of-the-art methods of up to 42.22% accuracy. Our novel human-guided LLM\nevaluation score shows GALA generates significantly more causally sound and\nactionable diagnostic outputs than existing methods. Through comprehensive\nexperiments and a case study, we show that GALA bridges the gap between\nautomated failure diagnosis and practical incident resolution by providing both\naccurate root cause identification and human-interpretable remediation\nguidance.", "AI": {"tldr": "GALA\u662f\u4e00\u79cd\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u7edf\u8ba1\u56e0\u679c\u63a8\u7406\u4e0e LLM \u9a71\u52a8\u7684\u8fed\u4ee3\u63a8\u7406\uff0c\u4ee5\u589e\u5f3a RCA\u3002", "motivation": "\u5fae\u670d\u52a1\u7cfb\u7edf\u4e2d\u7684\u6839\u672c\u539f\u56e0\u5206\u6790 (RCA) \u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u968f\u53eb\u968f\u5230\u7684\u5de5\u7a0b\u5e08\u5feb\u901f\u8bca\u65ad\u8de8\u5f02\u6784\u9065\u6d4b\uff08\u4f8b\u5982\u6307\u6807\u3001\u65e5\u5fd7\u548c\u8ddf\u8e2a\uff09\u7684\u6545\u969c\u3002\u4f20\u7edf\u7684 RCA \u65b9\u6cd5\u901a\u5e38\u4fa7\u91cd\u4e8e\u5355\u4e00\u6a21\u5f0f\u6216\u4ec5\u5bf9\u53ef\u7591\u670d\u52a1\u8fdb\u884c\u6392\u540d\uff0c\u800c\u672a\u80fd\u63d0\u4f9b\u5177\u6709\u8865\u6551\u6307\u5bfc\u7684\u53ef\u64cd\u4f5c\u8bca\u65ad\u89c1\u89e3\u3002", "method": "GALA\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u5f0f\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u7edf\u8ba1\u56e0\u679c\u63a8\u7406\u4e0e LLM \u9a71\u52a8\u7684\u8fed\u4ee3\u63a8\u7406\uff0c\u4ee5\u589e\u5f3a RCA\u3002", "result": "GALA \u5728\u5f00\u6e90\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cGALA \u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e86 42.22%\u3002\u6211\u4eec\u65b0\u9896\u7684\u4eba\u5de5\u6307\u5bfc LLM \u8bc4\u4f30\u5206\u6570\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cGALA \u751f\u6210\u7684\u56e0\u679c\u5173\u7cfb\u66f4\u5408\u7406\u3001\u53ef\u64cd\u4f5c\u7684\u8bca\u65ad\u8f93\u51fa\u3002", "conclusion": "GALA\u901a\u8fc7\u63d0\u4f9b\u51c6\u786e\u7684\u6839\u672c\u539f\u56e0\u8bc6\u522b\u548c\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u8865\u6551\u6307\u5bfc\uff0c\u5f25\u5408\u4e86\u81ea\u52a8\u5316\u6545\u969c\u8bca\u65ad\u548c\u5b9e\u9645\u4e8b\u4ef6\u89e3\u51b3\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2508.12061", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12061", "abs": "https://arxiv.org/abs/2508.12061", "authors": ["Daria Diatlova", "Nikita Balagansky", "Alexander Varlamov", "Egor Spirin"], "title": "VARAN: Variational Inference for Self-Supervised Speech Models Fine-Tuning on Downstream Tasks", "comment": null, "summary": "Conventional methods for aggregating layers in fine-tuned self-supervised\nspeech models, such as using the final layer or weighted sum, suffer from\ninformation bottlenecks and static feature weighting for all dataset examples.\nWe propose VARAN, a framework that dynamically tailors layer aggregation to\nindividual inputs. By employing layer-specialized probing heads and\ndata-dependent weighting, VARAN adaptively prioritizes layer's features based\non input. Evaluations on automatic speech recognition and speech emotion\nrecognition tasks demonstrate VARAN's superior performance, particularly when\nusing the LoRA fine-tuning technique. The framework resolves the trade-off\nbetween preserving layer-specific information and enabling flexible feature\nutilization, advancing efficient adaptation of self-supervised speech\nrepresentations.", "AI": {"tldr": "VARAN\u662f\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u5c42\u805a\u5408\u5230\u5355\u4e2a\u8f93\u5165\u7684\u6846\u67b6\uff0c\u5b83\u4f18\u4e8e\u4f20\u7edf\u7684\u5c42\u805a\u5408\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5fae\u8c03\u7684\u81ea\u76d1\u7763\u8bed\u97f3\u6a21\u578b\u4e2d\u805a\u5408\u5c42\u65f6\uff0c\u4f8b\u5982\u4f7f\u7528\u6700\u540e\u4e00\u5c42\u6216\u52a0\u6743\u548c\uff0c\u5b58\u5728\u4fe1\u606f\u74f6\u9888\u548c\u6240\u6709\u6570\u636e\u96c6\u793a\u4f8b\u7684\u9759\u6001\u7279\u5f81\u6743\u91cd\u7684\u95ee\u9898\u3002", "method": "VARAN\u901a\u8fc7\u91c7\u7528\u5c42\u4e13\u4e1a\u63a2\u6d4b\u5934\u548c\u6570\u636e\u76f8\u5173\u7684\u6743\u91cd\uff0c\u81ea\u9002\u5e94\u5730\u4f18\u5148\u8003\u8651\u57fa\u4e8e\u8f93\u5165\u7684\u5c42\u7279\u5f81\u3002", "result": "VARAN\u5728\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u548c\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "VARAN\u5728\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u548c\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u4f7f\u7528LoRA\u5fae\u8c03\u6280\u672f\u65f6\u3002\u8be5\u6846\u67b6\u89e3\u51b3\u4e86\u4fdd\u7559\u5c42\u7279\u5b9a\u4fe1\u606f\u548c\u5b9e\u73b0\u7075\u6d3b\u7279\u5f81\u5229\u7528\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2508.12277", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12277", "abs": "https://arxiv.org/abs/2508.12277", "authors": ["Elon Ezra", "Ariel Weizman", "Amos Azaria"], "title": "The Self-Execution Benchmark: Measuring LLMs' Attempts to Overcome Their Lack of Self-Execution", "comment": "11 pages, 9 figures", "summary": "Large language models (LLMs) are commonly evaluated on tasks that test their\nknowledge or reasoning abilities. In this paper, we explore a different type of\nevaluation: whether an LLM can predict aspects of its own responses. Since LLMs\nlack the ability to execute themselves, we introduce the Self-Execution\nBenchmark, which measures a model's ability to anticipate properties of its\noutput, such as whether a question will be difficult for it, whether it will\nrefuse to answer, or what kinds of associations it is likely to produce. Our\nexperiments show that models generally perform poorly on this benchmark, and\nthat increased model size or capability does not consistently lead to better\nperformance. These results suggest a fundamental limitation in how LLMs\nrepresent and reason about their own behavior.", "AI": {"tldr": "LLM\u65e0\u6cd5\u5f88\u597d\u5730\u9884\u6d4b\u81ea\u5df1\u7684\u884c\u4e3a\uff0c\u66f4\u5927\u7684\u6a21\u578b\u4e5f\u65e0\u6cd5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u901a\u5e38\u5728\u5176\u77e5\u8bc6\u6216\u63a8\u7406\u80fd\u529b\u7684\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u8fdb\u884c\u8bc4\u4f30\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u4e00\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u8bc4\u4f30\uff1aLLM\u662f\u5426\u53ef\u4ee5\u9884\u6d4b\u5176\u81ea\u8eab\u54cd\u5e94\u7684\u5404\u4e2a\u65b9\u9762\u3002", "method": "\u5f15\u5165\u4e86\u81ea\u6211\u6267\u884c\u57fa\u51c6\uff0c\u8861\u91cf\u6a21\u578b\u9884\u6d4b\u5176\u8f93\u51fa\u5c5e\u6027\u7684\u80fd\u529b\u3002", "result": "\u6a21\u578b\u901a\u5e38\u5728\u6b64\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u4e14\u6a21\u578b\u5927\u5c0f\u6216\u529f\u80fd\u7684\u589e\u52a0\u5e76\u4e0d\u4e00\u5b9a\u4f1a\u5e26\u6765\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "LLMs\u8868\u73b0\u4e0d\u4f73\uff0c\u6a21\u578b\u5927\u5c0f\u6216\u80fd\u529b\u7684\u589e\u52a0\u5e76\u4e0d\u4e00\u5b9a\u5e26\u6765\u66f4\u597d\u7684\u6027\u80fd\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660eLLM\u5728\u8868\u793a\u548c\u63a8\u7406\u81ea\u8eab\u884c\u4e3a\u65b9\u9762\u5b58\u5728\u6839\u672c\u9650\u5236\u3002"}}
{"id": "2508.11961", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11961", "abs": "https://arxiv.org/abs/2508.11961", "authors": ["Yuanbin Fu", "Liang Li", "Xiaojie Guo"], "title": "PEdger++: Practical Edge Detection via Assembling Cross Information", "comment": null, "summary": "Edge detection serves as a critical foundation for numerous computer vision\napplications, including object detection, semantic segmentation, and image\nediting, by extracting essential structural cues that define object boundaries\nand salient edges. To be viable for broad deployment across devices with\nvarying computational capacities, edge detectors shall balance high accuracy\nwith low computational complexity. While deep learning has evidently improved\naccuracy, they often suffer from high computational costs, limiting their\napplicability on resource-constrained devices. This paper addresses the\nchallenge of achieving that balance: \\textit{i.e.}, {how to efficiently capture\ndiscriminative features without relying on large-size and sophisticated\nmodels}. We propose PEdger++, a collaborative learning framework designed to\nreduce computational costs and model sizes while improving edge detection\naccuracy. The core principle of our PEdger++ is that cross-information derived\nfrom heterogeneous architectures, diverse training moments, and multiple\nparameter samplings, is beneficial to enhance learning from an ensemble\nperspective. Extensive experimental results on the BSDS500, NYUD and Multicue\ndatasets demonstrate the effectiveness of our approach, both quantitatively and\nqualitatively, showing clear improvements over existing methods. We also\nprovide multiple versions of the model with varying computational requirements,\nhighlighting PEdger++'s adaptability with respect to different resource\nconstraints. Codes are accessible at\nhttps://github.com/ForawardStar/EdgeDetectionviaPEdgerPlus/.", "AI": {"tldr": "\u63d0\u51fa\u4e86PEdger++\uff0c\u4e00\u4e2a\u534f\u540c\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u6a21\u578b\u5927\u5c0f\uff0c\u540c\u65f6\u63d0\u9ad8\u8fb9\u7f18\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u8fb9\u7f18\u68c0\u6d4b\u662f\u8bb8\u591a\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u7684\u5173\u952e\u57fa\u7840\uff0c\u4f46\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u8ba1\u7b97\u590d\u6742\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u4e86PEdger++\uff0c\u4e00\u4e2a\u65e8\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u6a21\u578b\u5927\u5c0f\u540c\u65f6\u63d0\u9ad8\u8fb9\u7f18\u68c0\u6d4b\u51c6\u786e\u6027\u7684\u534f\u540c\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5728BSDS500\u3001NYUD\u548cMulticue\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u65b9\u9762\u90fd\u663e\u793a\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u660e\u663e\u6539\u8fdb\u3002", "conclusion": "PEdger++\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6539\u8fdb\uff0c\u5e76\u4e14\u5177\u6709\u9002\u5e94\u4e0d\u540c\u8d44\u6e90\u7ea6\u675f\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2508.12480", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.12480", "abs": "https://arxiv.org/abs/2508.12480", "authors": ["Constantin Ruhdorfer", "Matteo Bortoletto", "Andreas Bulling"], "title": "The Yokai Learning Environment: Tracking Beliefs Over Space and Time", "comment": "Presented at the the ToM IJCAI 2025 Workshop", "summary": "Developing collaborative AI hinges on Theory of Mind (ToM) - the ability to\nreason about the beliefs of others to build and maintain common ground.\nExisting ToM benchmarks, however, are restricted to passive observer settings\nor lack an assessment of how agents establish and maintain common ground over\ntime. To address these gaps, we introduce the Yokai Learning Environment (YLE)\n- a multi-agent reinforcement learning (RL) environment based on the\ncooperative card game Yokai. In the YLE, agents take turns peeking at hidden\ncards and moving them to form clusters based on colour. Success requires\ntracking evolving beliefs, remembering past observations, using hints as\ngrounded communication, and maintaining common ground with teammates. Our\nevaluation yields two key findings: First, current RL agents struggle to solve\nthe YLE, even when given access to perfect memory. Second, while belief\nmodelling improves performance, agents are still unable to effectively\ngeneralise to unseen partners or form accurate beliefs over longer games,\nexposing a reliance on brittle conventions rather than robust belief tracking.\nWe use the YLE to investigate research questions in belief modelling, memory,\npartner generalisation, and scaling to higher-order ToM.", "AI": {"tldr": "Introduces the Yokai Learning Environment (YLE) for collaborative AI research, revealing challenges in current RL agents' Theory of Mind capabilities, especially in generalization and belief tracking.", "motivation": "Existing ToM benchmarks are restricted to passive observer settings or lack an assessment of how agents establish and maintain common ground over time.", "method": "Multi-agent reinforcement learning (RL) environment based on the cooperative card game Yokai.", "result": "Current RL agents struggle to solve the YLE, even with perfect memory. Belief modelling improves performance, but agents are still unable to effectively generalize to unseen partners or form accurate beliefs over longer games.", "conclusion": "Current RL agents struggle to solve the YLE, even with perfect memory. Belief modelling improves performance, but agents struggle to generalize to unseen partners or form accurate beliefs over longer games, relying on brittle conventions rather than robust belief tracking."}}
{"id": "2508.12079", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12079", "abs": "https://arxiv.org/abs/2508.12079", "authors": ["Ningzhe Shi", "Yiqing Zhou", "Ling Liu", "Jinglin Shi", "Yihao Wu", "Haiwei Shi", "Hanxiao Yu"], "title": "Content Accuracy and Quality Aware Resource Allocation Based on LP-Guided DRL for ISAC-Driven AIGC Networks", "comment": null, "summary": "Integrated sensing and communication (ISAC) can enhance artificial\nintelligence-generated content (AIGC) networks by providing efficient sensing\nand transmission. Existing AIGC services usually assume that the accuracy of\nthe generated content can be ensured, given accurate input data and prompt,\nthus only the content generation quality (CGQ) is concerned. However, it is not\napplicable in ISAC-based AIGC networks, where content generation is based on\ninaccurate sensed data. Moreover, the AIGC model itself introduces generation\nerrors, which depend on the number of generating steps (i.e., computing\nresources). To assess the quality of experience of ISAC-based AIGC services, we\npropose a content accuracy and quality aware service assessment metric (CAQA).\nSince allocating more resources to sensing and generating improves content\naccuracy but may reduce communication quality, and vice versa, this\nsensing-generating (computing)-communication three-dimensional resource\ntradeoff must be optimized to maximize the average CAQA (AvgCAQA) across all\nusers with AIGC (CAQA-AIGC). This problem is NP-hard, with a large solution\nspace that grows exponentially with users. To solve the CAQA-AIGC problem with\nlow complexity, a linear programming (LP) guided deep reinforcement learning\n(DRL) algorithm with an action filter (LPDRL-F) is proposed. Through the\nLP-guided approach and the action filter, LPDRL-F can transform the original\nthree-dimensional solution space to two dimensions, reducing complexity while\nimproving the learning performance of DRL. Simulations show that compared to\nexisting DRL and generative diffusion model algorithms without LP, LPDRL-F\nconverges faster by over 60% and finds better resource allocation solutions,\nimproving AvgCAQA by more than 14%. With LPDRL-F, CAQA-AIGC can achieve an\nimprovement in AvgCAQA of more than 50% compared to existing schemes focusing\nsolely on CGQ.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8eISAC\u7684AIGC\u670d\u52a1\u8d28\u91cf\u7684\u6307\u6807CAQA\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cdLPDRL-F\u7b97\u6cd5\u6765\u4f18\u5316\u8d44\u6e90\u5206\u914d\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u7b97\u6cd5\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8AvgCAQA\u3002", "motivation": "\u73b0\u6709\u7684AIGC\u670d\u52a1\u901a\u5e38\u5047\u8bbe\u5728\u7ed9\u5b9a\u51c6\u786e\u7684\u8f93\u5165\u6570\u636e\u548c\u63d0\u793a\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u786e\u4fdd\u751f\u6210\u5185\u5bb9\u7684\u51c6\u786e\u6027\uff0c\u56e0\u6b64\u53ea\u5173\u6ce8\u5185\u5bb9\u751f\u6210\u8d28\u91cf\uff08CGQ\uff09\u3002\u7136\u800c\uff0c\u8fd9\u4e0d\u9002\u7528\u4e8e\u57fa\u4e8eISAC\u7684AIGC\u7f51\u7edc\uff0c\u5728\u8fd9\u4e9b\u7f51\u7edc\u4e2d\uff0c\u5185\u5bb9\u751f\u6210\u662f\u57fa\u4e8e\u4e0d\u51c6\u786e\u7684\u611f\u77e5\u6570\u636e\u3002\u6b64\u5916\uff0cAIGC\u6a21\u578b\u672c\u8eab\u4f1a\u5f15\u5165\u751f\u6210\u8bef\u5dee\uff0c\u8fd9\u53d6\u51b3\u4e8e\u751f\u6210\u6b65\u9aa4\u7684\u6570\u91cf\uff08\u5373\u8ba1\u7b97\u8d44\u6e90\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5185\u5bb9\u51c6\u786e\u6027\u548c\u8d28\u91cf\u611f\u77e5\u670d\u52a1\u8bc4\u4f30\u6307\u6807\uff08CAQA\uff09\u4ee5\u53ca\u4e00\u79cd\u7ebf\u6027\u89c4\u5212\uff08LP\uff09\u5f15\u5bfc\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5177\u6709\u52a8\u4f5c\u8fc7\u6ee4\u5668\uff08LPDRL-F\uff09\u3002", "result": "\u4e0e\u73b0\u6709\u7684DRL\u548c\u6ca1\u6709LP\u7684\u751f\u6210\u6269\u6563\u6a21\u578b\u7b97\u6cd5\u76f8\u6bd4\uff0cLPDRL-F\u6536\u655b\u901f\u5ea6\u63d0\u9ad8\u4e8660%\u4ee5\u4e0a\uff0c\u5e76\u627e\u5230\u4e86\u66f4\u597d\u7684\u8d44\u6e90\u5206\u914d\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u5747CAQA\u63d0\u9ad8\u4e8614%\u4ee5\u4e0a\u3002\u4e0e\u73b0\u6709\u4ec5\u5173\u6ce8CGQ\u7684\u65b9\u6848\u76f8\u6bd4\uff0cCAQA-AIGC\u5728AvgCAQA\u65b9\u9762\u63d0\u9ad8\u4e8650%\u4ee5\u4e0a\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ebf\u6027\u89c4\u5212\uff08LP\uff09\u5f15\u5bfc\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5177\u6709\u52a8\u4f5c\u8fc7\u6ee4\u5668\uff08LPDRL-F\uff09\uff0c\u4e0e\u73b0\u6709\u7684DRL\u548c\u6ca1\u6709LP\u7684\u751f\u6210\u6269\u6563\u6a21\u578b\u7b97\u6cd5\u76f8\u6bd4\uff0cLPDRL-F\u6536\u655b\u901f\u5ea6\u63d0\u9ad8\u4e8660%\u4ee5\u4e0a\uff0c\u5e76\u627e\u5230\u4e86\u66f4\u597d\u7684\u8d44\u6e90\u5206\u914d\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u5747CAQA\u63d0\u9ad8\u4e8614%\u4ee5\u4e0a\u3002\u4e0e\u73b0\u6709\u4ec5\u5173\u6ce8CGQ\u7684\u65b9\u6848\u76f8\u6bd4\uff0cCAQA-AIGC\u5728AvgCAQA\u65b9\u9762\u63d0\u9ad8\u4e8650%\u4ee5\u4e0a\u3002"}}
{"id": "2508.12281", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12281", "abs": "https://arxiv.org/abs/2508.12281", "authors": ["Xin Dai", "Buqiang Xu", "Zhenghao Liu", "Yukun Yan", "Huiyuan Xie", "Xiaoyuan Yi", "Shuo Wang", "Ge Yu"], "title": "Legal$\u0394$: Enhancing Legal Reasoning in LLMs via Reinforcement Learning with Chain-of-Thought Guided Information Gain", "comment": null, "summary": "Legal Artificial Intelligence (LegalAI) has achieved notable advances in\nautomating judicial decision-making with the support of Large Language Models\n(LLMs). However, existing legal LLMs still struggle to generate reliable and\ninterpretable reasoning processes. They often default to fast-thinking behavior\nby producing direct answers without explicit multi-step reasoning, limiting\ntheir effectiveness in complex legal scenarios that demand rigorous\njustification. To address this challenge, we propose Legal$\\Delta$, a\nreinforcement learning framework designed to enhance legal reasoning through\nchain-of-thought guided information gain. During training, Legal$\\Delta$\nemploys a dual-mode input setup-comprising direct answer and\nreasoning-augmented modes-and maximizes the information gain between them. This\nencourages the model to acquire meaningful reasoning patterns rather than\ngenerating superficial or redundant explanations. Legal$\\Delta$ follows a\ntwo-stage approach: (1) distilling latent reasoning capabilities from a\npowerful Large Reasoning Model (LRM), DeepSeek-R1, and (2) refining reasoning\nquality via differential comparisons, combined with a multidimensional reward\nmechanism that assesses both structural coherence and legal-domain specificity.\nExperimental results on multiple legal reasoning tasks demonstrate that\nLegal$\\Delta$ outperforms strong baselines in both accuracy and\ninterpretability. It consistently produces more robust and trustworthy legal\njudgments without relying on labeled preference data. All code and data will be\nreleased at https://github.com/NEUIR/LegalDelta.", "AI": {"tldr": "Legal\u0394\u662f\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u601d\u7ef4\u94fe\u5f15\u5bfc\u7684\u4fe1\u606f\u589e\u76ca\u6765\u589e\u5f3a\u6cd5\u5f8b\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u7684\u6cd5\u5f8b\u6cd5\u5b66\u7855\u58eb\u4ecd\u7136\u96be\u4ee5\u751f\u6210\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u4ed6\u4eec\u901a\u5e38\u901a\u8fc7\u4ea7\u751f\u6ca1\u6709\u660e\u786e\u7684\u591a\u6b65\u9aa4\u63a8\u7406\u7684\u76f4\u63a5\u7b54\u6848\u6765\u9ed8\u8ba4\u4e3a\u5feb\u901f\u601d\u8003\u884c\u4e3a\uff0c\u4ece\u800c\u9650\u5236\u4e86\u4ed6\u4eec\u5728\u9700\u8981\u4e25\u683c\u8bba\u8bc1\u7684\u590d\u6742\u6cd5\u5f8b\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "Legal\u0394 \u91c7\u7528\u53cc\u6a21\u5f0f\u8f93\u5165\u8bbe\u7f6e\uff08\u5305\u62ec\u76f4\u63a5\u56de\u7b54\u548c\u63a8\u7406\u589e\u5f3a\u6a21\u5f0f\uff09\uff0c\u5e76\u6700\u5927\u5316\u5b83\u4eec\u4e4b\u95f4\u7684\u4fe1\u606f\u589e\u76ca\u3002Legal\u0394 \u9075\u5faa\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\uff081\uff09\u4ece\u5f3a\u5927\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b (LRM) DeepSeek-R1 \u4e2d\u63d0\u53d6\u6f5c\u5728\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4ee5\u53ca (2) \u901a\u8fc7\u5dee\u5f02\u6bd4\u8f83\u7ec6\u5316\u63a8\u7406\u8d28\u91cf\uff0c\u5e76\u7ed3\u5408\u8bc4\u4f30\u7ed3\u6784\u8fde\u8d2f\u6027\u548c\u6cd5\u5f8b\u9886\u57df\u7279\u5f02\u6027\u7684\u591a\u7ef4\u5956\u52b1\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u6cd5\u5f8b\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLegal\u0394 \u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u3002", "conclusion": "Legal\u0394\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\uff0c\u5e76\u4e14\u6301\u7eed\u4ea7\u751f\u66f4\u5f3a\u5927\u548c\u53ef\u4fe1\u7684\u6cd5\u5f8b\u5224\u65ad\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u6807\u8bb0\u7684\u504f\u597d\u6570\u636e\u3002"}}
{"id": "2508.11988", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11988", "abs": "https://arxiv.org/abs/2508.11988", "authors": ["Nicolas Mastropasqua", "Ignacio Bugueno-Cordova", "Rodrigo Verschae", "Daniel Acevedo", "Pablo Negri", "Maria E. Buemi"], "title": "Exploring Spatial-Temporal Dynamics in Event-based Facial Micro-Expression Analysis", "comment": null, "summary": "Micro-expression analysis has applications in domains such as Human-Robot\nInteraction and Driver Monitoring Systems. Accurately capturing subtle and fast\nfacial movements remains difficult when relying solely on RGB cameras, due to\nlimitations in temporal resolution and sensitivity to motion blur. Event\ncameras offer an alternative, with microsecond-level precision, high dynamic\nrange, and low latency. However, public datasets featuring event-based\nrecordings of Action Units are still scarce. In this work, we introduce a\nnovel, preliminary multi-resolution and multi-modal micro-expression dataset\nrecorded with synchronized RGB and event cameras under variable lighting\nconditions. Two baseline tasks are evaluated to explore the spatial-temporal\ndynamics of micro-expressions: Action Unit classification using Spiking Neural\nNetworks (51.23\\% accuracy with events vs. 23.12\\% with RGB), and frame\nreconstruction using Conditional Variational Autoencoders, achieving SSIM =\n0.8513 and PSNR = 26.89 dB with high-resolution event input. These promising\nresults show that event-based data can be used for micro-expression recognition\nand frame reconstruction.", "AI": {"tldr": "A novel multi-resolution and multi-modal micro-expression dataset recorded with synchronized RGB and event cameras is introduced. Event data shows promising results in micro-expression recognition and frame reconstruction.", "motivation": "Accurately capturing subtle and fast facial movements remains difficult when relying solely on RGB cameras, due to limitations in temporal resolution and sensitivity to motion blur. Event cameras offer an alternative, with microsecond-level precision, high dynamic range, and low latency. However, public datasets featuring event-based recordings of Action Units are still scarce.", "method": "Action Unit classification using Spiking Neural Networks and frame reconstruction using Conditional Variational Autoencoders", "result": "Action Unit classification using Spiking Neural Networks (51.23% accuracy with events vs. 23.12% with RGB), and frame reconstruction using Conditional Variational Autoencoders, achieving SSIM = 0.8513 and PSNR = 26.89 dB with high-resolution event input", "conclusion": "event-based data can be used for micro-expression recognition and frame reconstruction"}}
{"id": "2508.12487", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.12487", "abs": "https://arxiv.org/abs/2508.12487", "authors": ["Lida Shahbandari", "Hossein Mohseni"], "title": "Advanced DOA Regulation with a Whale-Optimized Fractional Order Fuzzy PID Framework", "comment": null, "summary": "This study introduces a Fractional Order Fuzzy PID (FOFPID) controller that\nuses the Whale Optimization Algorithm (WOA) to manage the Bispectral Index\n(BIS), keeping it within the ideal range of forty to sixty. The FOFPID\ncontroller combines fuzzy logic for adapting to changes and fractional order\ndynamics for fine tuning. This allows it to adjust its control gains to handle\na person's unique physiology. The WOA helps fine tune the controller's\nparameters, including the fractional orders and the fuzzy membership functions,\nwhich boosts its performance. Tested on models of eight different patient\nprofiles, the FOFPID controller performed better than a standard Fractional\nOrder PID (FOPID) controller. It achieved faster settling times, at two and a\nhalf minutes versus three point two minutes, and had a lower steady state\nerror, at zero point five versus one point two. These outcomes show the\nFOFPID's excellent strength and accuracy. It offers a scalable, artificial\nintelligence driven solution for automated anesthesia delivery that could\nenhance clinical practice and improve patient results.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9cb8\u9c7c\u4f18\u5316\u7b97\u6cd5\u7684\u5206\u6570\u9636\u6a21\u7cca PID \u63a7\u5236\u5668 (FOFPID)\uff0c\u7528\u4e8e\u7ba1\u7406\u53cc\u9891\u6307\u6570 (BIS)\uff0c\u5b9e\u9a8c\u8868\u660e\uff0cFOFPID \u63a7\u5236\u5668\u6027\u80fd\u4f18\u4e8e\u6807\u51c6 FOPID \u63a7\u5236\u5668", "motivation": "\u7ba1\u7406\u53cc\u9891\u6307\u6570 (BIS)\uff0c\u4f7f\u5176\u4fdd\u6301\u5728 40 \u5230 60 \u7684\u7406\u60f3\u8303\u56f4\u5185", "method": "\u4f7f\u7528\u9cb8\u9c7c\u4f18\u5316\u7b97\u6cd5 (WOA) \u7684\u5206\u6570\u9636\u6a21\u7cca PID (FOFPID) \u63a7\u5236\u5668", "result": "FOFPID \u63a7\u5236\u5668\u6bd4\u6807\u51c6\u5206\u6570\u9636 PID (FOPID) \u63a7\u5236\u5668\u8868\u73b0\u66f4\u597d\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u7a33\u5b9a\u65f6\u95f4\uff082.5 \u5206\u949f\u5bf9\u6bd4 3.2 \u5206\u949f\uff09\u548c\u66f4\u4f4e\u7684\u7a33\u6001\u8bef\u5dee\uff080.5 \u5bf9\u6bd4 1.2\uff09\u3002", "conclusion": "FOFPID \u63a7\u5236\u5668\u5728\u516b\u79cd\u4e0d\u540c\u60a3\u8005\u6a21\u578b\u4e0a\u7684\u6d4b\u8bd5\u8868\u73b0\u4f18\u4e8e\u6807\u51c6 FOPID \u63a7\u5236\u5668\uff0c\u5177\u6709\u66f4\u5feb\u7684\u7a33\u5b9a\u65f6\u95f4\u548c\u66f4\u4f4e\u7684\u7a33\u6001\u8bef\u5dee\uff0c\u5c55\u793a\u4e86\u5176\u5353\u8d8a\u7684\u5f3a\u5ea6\u548c\u51c6\u786e\u6027\u3002\u5b83\u4e3a\u81ea\u52a8\u5316\u9ebb\u9189\u8f93\u9001\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u3001\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u6539\u5584\u4e34\u5e8a\u5b9e\u8df5\u548c\u6539\u5584\u60a3\u8005\u7684\u6cbb\u7597\u6548\u679c\u3002"}}
{"id": "2508.12104", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12104", "abs": "https://arxiv.org/abs/2508.12104", "authors": ["Shane Waxler", "Paul Blazek", "Davis White", "Daniel Sneider", "Kevin Chung", "Mani Nagarathnam", "Patrick Williams", "Hank Voeller", "Karen Wong", "Matthew Swanhorst", "Sheng Zhang", "Naoto Usuyama", "Cliff Wong", "Tristan Naumann", "Hoifung Poon", "Andrew Loza", "Daniella Meeker", "Seth Hain", "Rahul Shah"], "title": "Generative Medical Event Models Improve with Scale", "comment": null, "summary": "Realizing personalized medicine at scale calls for methods that distill\ninsights from longitudinal patient journeys, which can be viewed as a sequence\nof medical events. Foundation models pretrained on large-scale medical event\ndata represent a promising direction for scaling real-world evidence generation\nand generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with\nmedical events from de-identified longitudinal health records for 16.3 billion\nencounters over 300 million unique patient records from 310 health systems, we\nintroduce the Cosmos Medical Event Transformer ( CoMET) models, a family of\ndecoder-only transformer models pretrained on 118 million patients representing\n115 billion discrete medical events (151 billion tokens). We present the\nlargest scaling-law study for medical event data, establishing a methodology\nfor pretraining and revealing power-law scaling relationships for compute,\ntokens, and model size. Based on this, we pretrained a series of\ncompute-optimal models with up to 1 billion parameters. Conditioned on a\npatient's real-world history, CoMET autoregressively generates the next medical\nevent, simulating patient health timelines. We studied 78 real-world tasks,\nincluding diagnosis prediction, disease prognosis, and healthcare operations.\nRemarkably for a foundation model with generic pretraining and simulation-based\ninference, CoMET generally outperformed or matched task-specific supervised\nmodels on these tasks, without requiring task-specific fine-tuning or few-shot\nexamples. CoMET's predictive power consistently improves as the model and\npretraining scale. Our results show that CoMET, a generative medical event\nfoundation model, can effectively capture complex clinical dynamics, providing\nan extensible and generalizable framework to support clinical decision-making,\nstreamline healthcare operations, and improve patient outcomes.", "AI": {"tldr": "CoMET\u662f\u4e00\u79cd\u751f\u6210\u5f0f\u533b\u7597\u4e8b\u4ef6\u57fa\u7840\u6a21\u578b\uff0c\u5b83\u53ef\u4ee5\u6709\u6548\u5730\u6355\u6349\u590d\u6742\u7684\u4e34\u5e8a\u52a8\u6001\uff0c\u63d0\u4f9b\u4e00\u4e2a\u53ef\u6269\u5c55\u548c\u901a\u7528\u7684\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\uff0c\u7b80\u5316\u533b\u7597\u4fdd\u5065\u8fd0\u8425\uff0c\u5e76\u6539\u5584\u60a3\u8005\u9884\u540e\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u4e0a\u5b9e\u73b0\u4e2a\u6027\u5316\u533b\u7597\u9700\u8981\u4ece\u7eb5\u5411\u60a3\u8005\u5386\u7a0b\u4e2d\u63d0\u53d6\u89c1\u89e3\u7684\u65b9\u6cd5\uff0c\u7eb5\u5411\u60a3\u8005\u5386\u7a0b\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u7cfb\u5217\u533b\u7597\u4e8b\u4ef6\u3002\u5728\u5927\u578b\u533b\u7597\u4e8b\u4ef6\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u4ee3\u8868\u4e86\u6269\u5c55\u771f\u5b9e\u4e16\u754c\u8bc1\u636e\u751f\u6210\u548c\u63a8\u5e7f\u5230\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u7684\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002", "method": "decoder-only transformer\u6a21\u578b", "result": "CoMET\u901a\u5e38\u4f18\u4e8e\u6216\u5339\u914d\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u7684\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u76d1\u7763\u6a21\u578b\uff0c\u800c\u65e0\u9700\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u5fae\u8c03\u6216\u5c11\u91cf\u793a\u4f8b\u3002", "conclusion": "CoMET\uff0c\u4e00\u79cd\u751f\u6210\u5f0f\u533b\u7597\u4e8b\u4ef6\u57fa\u7840\u6a21\u578b\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u6355\u6349\u590d\u6742\u7684\u4e34\u5e8a\u52a8\u6001\uff0c\u63d0\u4f9b\u4e00\u4e2a\u53ef\u6269\u5c55\u548c\u901a\u7528\u7684\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\uff0c\u7b80\u5316\u533b\u7597\u4fdd\u5065\u8fd0\u8425\uff0c\u5e76\u6539\u5584\u60a3\u8005\u9884\u540e\u3002"}}
{"id": "2508.12500", "categories": ["cs.AI", "cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.12500", "abs": "https://arxiv.org/abs/2508.12500", "authors": ["Rahmat K. Adesunkanmi", "Ashfaq Khokhar", "Goce Trajcevski", "Sohail Murad"], "title": "Root Cause Analysis of Hydrogen Bond Separation in Spatio-Temporal Molecular Dynamics using Causal Models", "comment": "Submitted to ACM", "summary": "Molecular dynamics simulations (MDS) face challenges, including\nresource-heavy computations and the need to manually scan outputs to detect\n\"interesting events,\" such as the formation and persistence of hydrogen bonds\nbetween atoms of different molecules. A critical research gap lies in\nidentifying the underlying causes of hydrogen bond formation and separation\n-understanding which interactions or prior events contribute to their emergence\nover time. With this challenge in mind, we propose leveraging spatio-temporal\ndata analytics and machine learning models to enhance the detection of these\nphenomena. In this paper, our approach is inspired by causal modeling and aims\nto identify the root cause variables of hydrogen bond formation and separation\nevents. Specifically, we treat the separation of hydrogen bonds as an\n\"intervention\" occurring and represent the causal structure of the bonding and\nseparation events in the MDS as graphical causal models. These causal models\nare built using a variational autoencoder-inspired architecture that enables us\nto infer causal relationships across samples with diverse underlying causal\ngraphs while leveraging shared dynamic information. We further include a step\nto infer the root causes of changes in the joint distribution of the causal\nmodels. By constructing causal models that capture shifts in the conditional\ndistributions of molecular interactions during bond formation or separation,\nthis framework provides a novel perspective on root cause analysis in molecular\ndynamic systems. We validate the efficacy of our model empirically on the\natomic trajectories that used MDS for chiral separation, demonstrating that we\ncan predict many steps in the future and also find the variables driving the\nobserved changes in the system.", "AI": {"tldr": "This paper introduces a causal modeling approach using a variational autoencoder-inspired architecture to identify the root causes of hydrogen bond formation and separation events in molecular dynamics simulations. The model is validated on chiral separation simulations, showing it can predict future events and identify key driving variables.", "motivation": "Molecular dynamics simulations (MDS) face challenges, including resource-heavy computations and the need to manually scan outputs to detect interesting events, such as the formation and persistence of hydrogen bonds between atoms of different molecules. A critical research gap lies in identifying the underlying causes of hydrogen bond formation and separation -understanding which interactions or prior events contribute to their emergence over time. With this challenge in mind, we propose leveraging spatio-temporal data analytics and machine learning models to enhance the detection of these phenomena.", "method": "causal modeling and aims to identify the root cause variables of hydrogen bond formation and separation events. Specifically, we treat the separation of hydrogen bonds as an intervention occurring and represent the causal structure of the bonding and separation events in the MDS as graphical causal models. These causal models are built using a variational autoencoder-inspired architecture that enables us to infer causal relationships across samples with diverse underlying causal graphs while leveraging shared dynamic information.", "result": "We validate the efficacy of our model empirically on the atomic trajectories that used MDS for chiral separation, demonstrating that we can predict many steps in the future and also find the variables driving the observed changes in the system.", "conclusion": "This framework provides a novel perspective on root cause analysis in molecular dynamic systems by constructing causal models that capture shifts in the conditional distributions of molecular interactions during bond formation or separation."}}
{"id": "2508.12116", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12116", "abs": "https://arxiv.org/abs/2508.12116", "authors": ["Haebin Shin", "Lei Ji", "Xiao Liu", "Zhiwei Yu", "Qi Chen", "Yeyun Gong"], "title": "DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections", "comment": null, "summary": "As numerous instruction-tuning datasets continue to emerge during the\npost-training stage, dynamically balancing and optimizing their mixtures has\nbecome a critical challenge. To address this, we propose DynamixSFT, a dynamic\nand automated method for instruction-tuning dataset mixture optimization. We\nformulate the problem as a multi-armed bandit setup and introduce a\nPrior-scaled Boltzmann Exploration that softly anchors the updated sampling\ndistribution to the original dataset proportions, thereby preserving the\ninherent diversity and coverage of the collection. Sampling probabilities are\nupdated using a lightweight 1-Step Look-ahead Reward, reflecting how much the\ndataset contributes to improving the model's performance at its current state.\nWhen applied to the Tulu-v2-mixture collection comprising 16 instruction-tuning\ndatasets, DynamixSFT achieves up to a 2.2% performance improvement across 10\nbenchmarks. Furthermore, we provide a comprehensive analysis and visualizations\nto offer deeper insights into the adaptive dynamics of our method.", "AI": {"tldr": "DynamixSFT: a dynamic and automated method for instruction-tuning dataset mixture optimization,achieves up to a 2.2% performance improvement across 10 benchmarks.", "motivation": "Dynamically balancing and optimizing instruction-tuning dataset mixtures has become a critical challenge.", "method": "We propose DynamixSFT, a dynamic and automated method for instruction-tuning dataset mixture optimization. We formulate the problem as a multi-armed bandit setup and introduce a Prior-scaled Boltzmann Exploration that softly anchors the updated sampling distribution to the original dataset proportions.", "result": "DynamixSFT achieves up to a 2.2% performance improvement across 10 benchmarks. comprehensive analysis and visualizations to offer deeper insights into the adaptive dynamics of our method.", "conclusion": "DynamixSFT achieves up to a 2.2% performance improvement across 10 benchmarks on the Tulu-v2-mixture collection."}}
{"id": "2508.12286", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12286", "abs": "https://arxiv.org/abs/2508.12286", "authors": ["Qinghua Wang", "Xu Zhang", "Lingyan Yang", "Rui Shao", "Bonan Wang", "Fang Wang", "Cunquan Qu"], "title": "Incorporating Legal Logic into Deep Learning: An Intelligent Approach to Probation Prediction", "comment": null, "summary": "Probation is a crucial institution in modern criminal law, embodying the\nprinciples of fairness and justice while contributing to the harmonious\ndevelopment of society. Despite its importance, the current Intelligent\nJudicial Assistant System (IJAS) lacks dedicated methods for probation\nprediction, and research on the underlying factors influencing probation\neligibility remains limited. In addition, probation eligibility requires a\ncomprehensive analysis of both criminal circumstances and remorse. Much of the\nexisting research in IJAS relies primarily on data-driven methodologies, which\noften overlooks the legal logic underpinning judicial decision-making. To\naddress this gap, we propose a novel approach that integrates legal logic into\ndeep learning models for probation prediction, implemented in three distinct\nstages. First, we construct a specialized probation dataset that includes fact\ndescriptions and probation legal elements (PLEs). Second, we design a distinct\nprobation prediction model named the Multi-Task Dual-Theory Probation\nPrediction Model (MT-DT), which is grounded in the legal logic of probation and\nthe \\textit{Dual-Track Theory of Punishment}. Finally, our experiments on the\nprobation dataset demonstrate that the MT-DT model outperforms baseline models,\nand an analysis of the underlying legal logic further validates the\neffectiveness of the proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6cd5\u5f8b\u903b\u8f91\u878d\u5165\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u7f13\u5211\u9884\u6d4b\u65b0\u65b9\u6cd5\uff0c\u5e76\u5728\u7f13\u5211\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMT-DT \u6a21\u578b\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u7684\u667a\u80fd\u53f8\u6cd5\u52a9\u7406\u7cfb\u7edf\uff08IJAS\uff09\u7f3a\u4e4f\u4e13\u95e8\u7684\u7f13\u5211\u9884\u6d4b\u65b9\u6cd5\uff0c\u5e76\u4e14\u5bf9\u5f71\u54cd\u7f13\u5211\u8d44\u683c\u7684\u6f5c\u5728\u56e0\u7d20\u7684\u7814\u7a76\u4ecd\u7136\u6709\u9650\u3002\u6b64\u5916\uff0c\u7f13\u5211\u8d44\u683c\u9700\u8981\u5bf9\u72af\u7f6a\u60c5\u51b5\u548c\u6094\u6068\u8fdb\u884c\u5168\u9762\u5206\u6790\u3002IJAS \u4e2d\u7684\u8bb8\u591a\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u800c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u4e86\u53f8\u6cd5\u51b3\u7b56\u80cc\u540e\u7684\u6cd5\u5f8b\u903b\u8f91\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u6cd5\u5f8b\u903b\u8f91\u96c6\u6210\u5230\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\uff0c\u4ee5\u8fdb\u884c\u7f13\u5211\u9884\u6d4b\uff0c\u5e76\u5728\u4e09\u4e2a\u4e0d\u540c\u7684\u9636\u6bb5\u5b9e\u65bd\u3002", "result": "MT-DT \u6a21\u578b\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b", "conclusion": "MT-DT \u6a21\u578b\u5728\u7f13\u5211\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u4e14\u5bf9\u5e95\u5c42\u6cd5\u5f8b\u903b\u8f91\u7684\u5206\u6790\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.12015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12015", "abs": "https://arxiv.org/abs/2508.12015", "authors": ["Hongyuan Liu", "Haochen Yu", "Jianfei Jiang", "Qiankun Liu", "Jiansheng Chen", "Huimin Ma"], "title": "InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes", "comment": null, "summary": "Reconstructing dynamic driving scenes from dashcam videos has attracted\nincreasing attention due to its significance in autonomous driving and scene\nunderstanding. While recent advances have made impressive progress, most\nmethods still unify all background elements into a single representation,\nhindering both instance-level understanding and flexible scene editing. Some\napproaches attempt to lift 2D segmentation into 3D space, but often rely on\npre-processed instance IDs or complex pipelines to map continuous features to\ndiscrete identities. Moreover, these methods are typically designed for indoor\nscenes with rich viewpoints, making them less applicable to outdoor driving\nscenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian\nSplatting framework tailored for the interactive reconstruction of dynamic\ndriving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D\nfeature learning via contrastive loss and pseudo-supervised objectives. At the\n3D level, we introduce regularization to implicitly encode instance identities\nand enforce consistency through a voxel-based loss. A lightweight static\ncodebook further bridges continuous features and discrete identities without\nrequiring data pre-processing or complex optimization. Quantitative and\nqualitative experiments demonstrate the effectiveness of InstDrive, and to the\nbest of our knowledge, it is the first framework to achieve 3D instance\nsegmentation in dynamic, open-world driving scenes.More visualizations are\navailable at our project page.", "AI": {"tldr": "InstDrive is the first framework to achieve 3D instance segmentation in dynamic, open-world driving scenes by using an instance-aware 3D Gaussian Splatting framework.", "motivation": "reconstructing dynamic driving scenes from dashcam videos has attracted increasing attention due to its significance in autonomous driving and scene understanding. While recent advances have made impressive progress, most methods still unify all background elements into a single representation, hindering both instance-level understanding and flexible scene editing. Some approaches attempt to lift 2D segmentation into 3D space, but often rely on pre-processed instance IDs or complex pipelines to map continuous features to discrete identities. Moreover, these methods are typically designed for indoor scenes with rich viewpoints, making them less applicable to outdoor driving scenarios.", "method": "an instance-aware 3D Gaussian Splatting framework tailored for the interactive reconstruction of dynamic driving scene. It uses masks generated by SAM as pseudo ground-truth to guide 2D feature learning via contrastive loss and pseudo-supervised objectives. At the 3D level, it introduce regularization to implicitly encode instance identities and enforce consistency through a voxel-based loss. A lightweight static codebook further bridges continuous features and discrete identities without requiring data pre-processing or complex optimization.", "result": "Quantitative and qualitative experiments demonstrate the effectiveness of InstDrive", "conclusion": "InstDrive achieves 3D instance segmentation in dynamic, open-world driving scenes."}}
{"id": "2508.12566", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12566", "abs": "https://arxiv.org/abs/2508.12566", "authors": ["Wei Song", "Haonan Zhong", "Ziqi Ding", "Jingling Xue", "Yuekang Li"], "title": "Help or Hurdle? Rethinking Model Context Protocol-Augmented Large Language Models", "comment": null, "summary": "The Model Context Protocol (MCP) enables large language models (LLMs) to\naccess external resources on demand. While commonly assumed to enhance\nperformance, how LLMs actually leverage this capability remains poorly\nunderstood. We introduce MCPGAUGE, the first comprehensive evaluation framework\nfor probing LLM-MCP interactions along four key dimensions: proactivity\n(self-initiated tool use), compliance (adherence to tool-use instructions),\neffectiveness (task performance post-integration), and overhead (computational\ncost incurred). MCPGAUGE comprises a 160-prompt suite and 25 datasets spanning\nknowledge comprehension, general reasoning, and code generation. Our\nlarge-scale evaluation, spanning six commercial LLMs, 30 MCP tool suites, and\nboth one- and two-turn interaction settings, comprises around 20,000 API calls\nand over USD 6,000 in computational cost. This comprehensive study reveals four\nkey findings that challenge prevailing assumptions about the effectiveness of\nMCP integration. These insights highlight critical limitations in current\nAI-tool integration and position MCPGAUGE as a principled benchmark for\nadvancing controllable, tool-augmented LLMs.", "AI": {"tldr": "The paper introduces MCPGAUGE, a framework for evaluating how LLMs use external resources, and finds limitations in current AI-tool integration.", "motivation": "While commonly assumed to enhance performance, how LLMs actually leverage this capability remains poorly understood.", "method": "We introduce MCPGAUGE, the first comprehensive evaluation framework for probing LLM-MCP interactions along four key dimensions: proactivity (self-initiated tool use), compliance (adherence to tool-use instructions), effectiveness (task performance post-integration), and overhead (computational cost incurred). MCPGAUGE comprises a 160-prompt suite and 25 datasets spanning knowledge comprehension, general reasoning, and code generation. Our large-scale evaluation, spanning six commercial LLMs, 30 MCP tool suites, and both one- and two-turn interaction settings, comprises around 20,000 API calls and over USD 6,000 in computational cost.", "result": "This comprehensive study reveals four key findings that challenge prevailing assumptions about the effectiveness of MCP integration.", "conclusion": "This study reveals four key findings that challenge prevailing assumptions about the effectiveness of MCP integration. These insights highlight critical limitations in current AI-tool integration and position MCPGAUGE as a principled benchmark for advancing controllable, tool-augmented LLMs."}}
{"id": "2508.12121", "categories": ["cs.LG", "math.DS"], "pdf": "https://arxiv.org/pdf/2508.12121", "abs": "https://arxiv.org/abs/2508.12121", "authors": ["Lorenzo Livi"], "title": "Time-Scale Coupling Between States and Parameters in Recurrent Neural Networks", "comment": null, "summary": "We study how gating mechanisms in recurrent neural networks (RNNs) implicitly\ninduce adaptive learning-rate behavior, even when training is carried out with\na fixed, global learning rate. This effect arises from the coupling between\nstate-space time scales--parametrized by the gates--and parameter-space\ndynamics during gradient descent. By deriving exact Jacobians for\nleaky-integrator and gated RNNs, we obtain a first-order expansion that makes\nexplicit how constant, scalar, and multi-dimensional gates reshape gradient\npropagation, modulate effective step sizes, and introduce anisotropy in\nparameter updates. These findings reveal that gates not only control memory\nretention in the hidden states, but also act as data-driven preconditioners\nthat adapt optimization trajectories in parameter space. We further draw formal\nanalogies with learning-rate schedules, momentum, and adaptive methods such as\nAdam, showing that these optimization behaviors emerge naturally from gating.\nNumerical experiments confirm the validity of our perturbative analysis,\nsupporting the view that gate-induced corrections remain small while exerting\nsystematic effects on training dynamics. Overall, this work provides a unified\ndynamical-systems perspective on how gating couples state evolution with\nparameter updates, explaining why gated architectures achieve robust\ntrainability and stability in practice.", "AI": {"tldr": "\u95e8\u63a7\u673a\u5236\u5728 RNN \u4e2d\u8bf1\u5bfc\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u884c\u4e3a\uff0c\u4ece\u800c\u5b9e\u73b0\u7a33\u5065\u7684\u8bad\u7ec3\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u7814\u7a76 RNN \u4e2d\u95e8\u63a7\u673a\u5236\u5982\u4f55\u9690\u5f0f\u5730\u8bf1\u5bfc\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u884c\u4e3a\uff0c\u5373\u4f7f\u8bad\u7ec3\u662f\u7528\u56fa\u5b9a\u7684\u5168\u5c40\u5b66\u4e60\u7387\u8fdb\u884c\u7684\u3002", "method": "\u901a\u8fc7\u63a8\u5bfc leaky-integrator \u548c gated RNN \u7684\u7cbe\u786e\u96c5\u53ef\u6bd4\u77e9\u9635\uff0c\u83b7\u5f97\u4e00\u9636\u5c55\u5f00\uff0c\u4ece\u800c\u660e\u786e\u5e38\u6570\u3001\u6807\u91cf\u548c\u591a\u7ef4\u95e8\u5982\u4f55\u91cd\u5851\u68af\u5ea6\u4f20\u64ad\uff0c\u8c03\u8282\u6709\u6548\u6b65\u957f\uff0c\u5e76\u5728\u53c2\u6570\u66f4\u65b0\u4e2d\u5f15\u5165\u5404\u5411\u5f02\u6027\u3002", "result": "\u63ed\u793a\u4e86\u95e8\u4e0d\u4ec5\u63a7\u5236\u9690\u85cf\u72b6\u6001\u4e2d\u7684\u8bb0\u5fc6\u4fdd\u6301\uff0c\u8fd8\u5145\u5f53\u6570\u636e\u9a71\u52a8\u7684\u9884\u5904\u7406\u5668\uff0c\u4ece\u800c\u8c03\u6574\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u4f18\u5316\u8f68\u8ff9\u3002\u4e0e\u5b66\u4e60\u7387 schedules\u3001momentum \u548c\u81ea\u9002\u5e94\u65b9\u6cd5\uff08\u5982 Adam\uff09\u8fdb\u884c\u6b63\u5f0f\u7c7b\u6bd4\uff0c\u8868\u660e\u8fd9\u4e9b\u4f18\u5316\u884c\u4e3a\u81ea\u7136\u5730\u4ece\u95e8\u63a7\u4e2d\u4ea7\u751f\u3002\u6570\u503c\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u5fae\u6270\u5206\u6790\u7684\u6709\u6548\u6027\uff0c\u652f\u6301\u95e8\u63a7\u5f15\u8d77\u7684\u6821\u6b63\u4fdd\u6301\u8f83\u5c0f\uff0c\u540c\u65f6\u5bf9\u8bad\u7ec3\u52a8\u529b\u5b66\u4ea7\u751f\u7cfb\u7edf\u6027\u5f71\u54cd\u7684\u89c2\u70b9\u3002", "conclusion": "\u95e8\u63a7\u673a\u5236\u80fd\u591f\u81ea\u7136\u5730\u4ea7\u751f\u4f18\u5316\u884c\u4e3a\uff0c\u5b9e\u73b0\u7a33\u5065\u7684\u8bad\u7ec3\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2508.12301", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.12301", "abs": "https://arxiv.org/abs/2508.12301", "authors": ["Tomer Krichli", "Bhiksha Raj", "Joseph Keshet"], "title": "CarelessWhisper: Turning Whisper into a Causal Streaming Model", "comment": "17 pages, 7 Figures, This work has been submitted to the IEEE for\n  possible publication", "summary": "Automatic Speech Recognition (ASR) has seen remarkable progress, with models\nlike OpenAI Whisper and NVIDIA Canary achieving state-of-the-art (SOTA)\nperformance in offline transcription. However, these models are not designed\nfor streaming (online or real-time) transcription, due to limitations in their\narchitecture and training methodology. We propose a method to turn the\ntransformer encoder-decoder model into a low-latency streaming model that is\ncareless about future context. We present an analysis explaining why it is not\nstraightforward to convert an encoder-decoder transformer to a low-latency\nstreaming model. Our proposed method modifies the existing (non-causal) encoder\nto a causal encoder by fine-tuning both the encoder and decoder using Low-Rank\nAdaptation (LoRA) and a weakly aligned dataset. We then propose an updated\ninference mechanism that utilizes the fine-tune causal encoder and decoder to\nyield greedy and beam-search decoding, and is shown to be locally optimal.\nExperiments on low-latency chunk sizes (less than 300 msec) show that our\nfine-tuned model outperforms existing non-fine-tuned streaming approaches in\nmost cases, while using a lower complexity. Additionally, we observe that our\ntraining process yields better alignment, enabling a simple method for\nextracting word-level timestamps. We release our training and inference code,\nalong with the fine-tuned models, to support further research and development\nin streaming ASR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u5ef6\u8fdf\u6d41\u5f0fASR\u6a21\u578b\uff0c\u901a\u8fc7\u5fae\u8c03Transformer\u7ed3\u6784\u5b9e\u73b0\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684Automatic Speech Recognition (ASR)\u6a21\u578b\uff0c\u5982OpenAI Whisper\u548cNVIDIA Canary\uff0c\u5728\u79bb\u7ebf\u8f6c\u5f55\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7531\u4e8e\u5176\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u4e0d\u9002\u7528\u4e8e\u6d41\u5f0f(\u5728\u7ebf\u6216\u5b9e\u65f6)\u8f6c\u5f55\u3002", "method": "\u901a\u8fc7\u5fae\u8c03encoder\u548cdecoder\uff0c\u4f7f\u7528Low-Rank Adaptation (LoRA)\u548c\u4e00\u4e2a\u5f31\u5bf9\u9f50\u6570\u636e\u96c6\uff0c\u5c06\u73b0\u6709\u7684(\u975e\u56e0\u679c) encoder\u4fee\u6539\u4e3a\u56e0\u679cencoder\u3002", "result": "\u5728\u4f4e\u5ef6\u8fdf\u5757\u5927\u5c0f\uff08\u5c0f\u4e8e300\u6beb\u79d2\uff09\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u5fae\u8c03\u6a21\u578b\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u7684\u975e\u5fae\u8c03\u6d41\u5f0f\u65b9\u6cd5\uff0c\u540c\u65f6\u4f7f\u7528\u8f83\u4f4e\u7684\u590d\u6742\u5ea6\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u7684\u8bad\u7ec3\u8fc7\u7a0b\u4ea7\u751f\u4e86\u66f4\u597d\u7684\u5bf9\u9f50\uff0c\u4ece\u800c\u80fd\u591f\u4f7f\u7528\u4e00\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\u63d0\u53d6\u5355\u8bcd\u7ea7\u65f6\u95f4\u6233\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06transformer encoder-decoder\u6a21\u578b\u8f6c\u5316\u4e3a\u4f4e\u5ef6\u8fdf\u6d41\u5f0f\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u4f4e\u5ef6\u8fdf\u5757\u5927\u5c0f\u4e0a\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u4ee3\u7801\u548c\u6a21\u578b\u4ee5\u652f\u6301\u672a\u6765\u7684\u7814\u7a76\u3002"}}
{"id": "2508.12023", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12023", "abs": "https://arxiv.org/abs/2508.12023", "authors": ["Durgesh Kumar Singh", "Qing Cao", "Sarina Thomas", "Ahc\u00e8ne Boubekki", "Robert Jenssen", "Michael Kampffmeyer"], "title": "WiseLVAM: A Novel Framework For Left Ventricle Automatic Measurements", "comment": null, "summary": "Clinical guidelines recommend performing left ventricular (LV) linear\nmeasurements in B-mode echocardiographic images at the basal level -- typically\nat the mitral valve leaflet tips -- and aligned perpendicular to the LV long\naxis along a virtual scanline (SL). However, most automated methods estimate\nlandmarks directly from B-mode images for the measurement task, where even\nsmall shifts in predicted points along the LV walls can lead to significant\nmeasurement errors, reducing their clinical reliability. A recent\nsemi-automatic method, EnLVAM, addresses this limitation by constraining\nlandmark prediction to a clinician-defined SL and training on generated\nAnatomical Motion Mode (AMM) images to predict LV landmarks along the same. To\nenable full automation, a contour-aware SL placement approach is proposed in\nthis work, in which the LV contour is estimated using a weakly supervised\nB-mode landmark detector. SL placement is then performed by inferring the LV\nlong axis and the basal level-mimicking clinical guidelines. Building on this\nfoundation, we introduce \\textit{WiseLVAM} -- a novel, fully automated yet\nmanually adaptable framework for automatically placing the SL and then\nautomatically performing the LV linear measurements in the AMM mode.\n\\textit{WiseLVAM} utilizes the structure-awareness from B-mode images and the\nmotion-awareness from AMM mode to enhance robustness and accuracy with the\npotential to provide a practical solution for the routine clinical application.", "AI": {"tldr": "WiseLVAM is a novel, fully automated yet manually adaptable framework for automatically placing the SL and then automatically performing the LV linear measurements in the AMM mode,utilizing the structure-awareness from B-mode images and the motion-awareness from AMM mode to enhance robustness and accuracy with the potential to provide a practical solution for the routine clinical application.", "motivation": "Clinical guidelines recommend performing left ventricular (LV) linear measurements in B-mode echocardiographic images at the basal level -- typically at the mitral valve leaflet tips -- and aligned perpendicular to the LV long axis along a virtual scanline (SL). However, most automated methods estimate landmarks directly from B-mode images for the measurement task, where even small shifts in predicted points along the LV walls can lead to significant measurement errors, reducing their clinical reliability. A recent semi-automatic method, EnLVAM, addresses this limitation by constraining landmark prediction to a clinician-defined SL and training on generated Anatomical Motion Mode (AMM) images to predict LV landmarks along the same.", "method": "a contour-aware SL placement approach, in which the LV contour is estimated using a weakly supervised B-mode landmark detector. SL placement is then performed by inferring the LV long axis and the basal level-mimicking clinical guidelines. Building on this foundation, we introduce WiseLVAM -- a novel, fully automated yet manually adaptable framework for automatically placing the SL and then automatically performing the LV linear measurements in the AMM mode", "result": "fully automated yet manually adaptable framework for automatically placing the SL and then automatically performing the LV linear measurements in the AMM mode", "conclusion": "WiseLVAM utilizes the structure-awareness from B-mode images and the motion-awareness from AMM mode to enhance robustness and accuracy with the potential to provide a practical solution for the routine clinical application."}}
{"id": "2508.12611", "categories": ["cs.AI", "cs.CL", "I.2.7; F.4.1"], "pdf": "https://arxiv.org/pdf/2508.12611", "abs": "https://arxiv.org/abs/2508.12611", "authors": ["Trang Tran", "Trung Hoang Le", "Huiping Cao", "Tran Cao Son"], "title": "An LLM + ASP Workflow for Joint Entity-Relation Extraction", "comment": "13 pages, 1 figure, Accepted as Technical Communication, 41st\n  International Conference on Logic Programming", "summary": "Joint entity-relation extraction (JERE) identifies both entities and their\nrelationships simultaneously. Traditional machine-learning based approaches to\nperforming this task require a large corpus of annotated data and lack the\nability to easily incorporate domain specific information in the construction\nof the model. Therefore, creating a model for JERE is often labor intensive,\ntime consuming, and elaboration intolerant. In this paper, we propose\nharnessing the capabilities of generative pretrained large language models\n(LLMs) and the knowledge representation and reasoning capabilities of Answer\nSet Programming (ASP) to perform JERE. We present a generic workflow for JERE\nusing LLMs and ASP. The workflow is generic in the sense that it can be applied\nfor JERE in any domain. It takes advantage of LLM's capability in natural\nlanguage understanding in that it works directly with unannotated text. It\nexploits the elaboration tolerant feature of ASP in that no modification of its\ncore program is required when additional domain specific knowledge, in the form\nof type specifications, is found and needs to be used. We demonstrate the\nusefulness of the proposed workflow through experiments with limited training\ndata on three well-known benchmarks for JERE. The results of our experiments\nshow that the LLM + ASP workflow is better than state-of-the-art JERE systems\nin several categories with only 10\\% of training data. It is able to achieve a\n2.5 times (35\\% over 15\\%) improvement in the Relation Extraction task for the\nSciERC corpus, one of the most difficult benchmarks.", "AI": {"tldr": "This paper proposes a generic workflow for joint entity-relation extraction (JERE) using large language models (LLMs) and Answer Set Programming (ASP). The results show that the LLM + ASP workflow is better than state-of-the-art JERE systems with only 10% of training data.", "motivation": "Traditional machine-learning based approaches to performing this task require a large corpus of annotated data and lack the ability to easily incorporate domain specific information in the construction of the model. Therefore, creating a model for JERE is often labor intensive, time consuming, and elaboration intolerant.", "method": "harnessing the capabilities of generative pretrained large language models (LLMs) and the knowledge representation and reasoning capabilities of Answer Set Programming (ASP) to perform JERE. We present a generic workflow for JERE using LLMs and ASP.", "result": "LLM + ASP workflow is better than state-of-the-art JERE systems in several categories with only 10% of training data. It is able to achieve a 2.5 times (35% over 15%) improvement in the Relation Extraction task for the SciERC corpus", "conclusion": "LLM + ASP workflow is better than state-of-the-art JERE systems in several categories with only 10% of training data. It is able to achieve a 2.5 times (35% over 15%) improvement in the Relation Extraction task for the SciERC corpus"}}
{"id": "2508.12145", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12145", "abs": "https://arxiv.org/abs/2508.12145", "authors": ["Frederik L. Dennig", "Daniel A. Keim"], "title": "DE-VAE: Revealing Uncertainty in Parametric and Inverse Projections with Variational Autoencoders using Differential Entropy", "comment": "5 pages, 3 figures, LaTeX", "summary": "Recently, autoencoders (AEs) have gained interest for creating parametric and\ninvertible projections of multidimensional data. Parametric projections make it\npossible to embed new, unseen samples without recalculating the entire\nprojection, while invertible projections allow the synthesis of new data\ninstances. However, existing methods perform poorly when dealing with\nout-of-distribution samples in either the data or embedding space. Thus, we\npropose DE-VAE, an uncertainty-aware variational AE using differential entropy\n(DE) to improve the learned parametric and invertible projections. Given a\nfixed projection, we train DE-VAE to learn a mapping into 2D space and an\ninverse mapping back to the original space. We conduct quantitative and\nqualitative evaluations on four well-known datasets, using UMAP and t-SNE as\nbaseline projection methods. Our findings show that DE-VAE can create\nparametric and inverse projections with comparable accuracy to other current\nAE-based approaches while enabling the analysis of embedding uncertainty.", "AI": {"tldr": "DE-VAE\u662f\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u5b83\u4f7f\u7528\u5fae\u5206\u71b5\u6765\u6539\u8fdb\u53c2\u6570\u548c\u53ef\u9006\u6295\u5f71\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u5904\u7406\u6570\u636e\u6216\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u5206\u5e03\u5916\u6837\u672c\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5fae\u5206\u71b5(DE)\u7684\u3001\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u610f\u8bc6\u7684\u53d8\u5206AE\uff0c\u4ee5\u6539\u8fdb\u5b66\u4e60\u5230\u7684\u53c2\u6570\u548c\u53ef\u9006\u6295\u5f71\u3002", "result": "\u5728\u56db\u4e2a\u8457\u540d\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\uff0c\u4f7f\u7528UMAP\u548ct-SNE\u4f5c\u4e3a\u57fa\u7ebf\u6295\u5f71\u65b9\u6cd5\u3002", "conclusion": "DE-VAE\u53ef\u4ee5\u521b\u5efa\u4e0e\u5176\u4ed6\u5f53\u524d\u57fa\u4e8eAE\u7684\u65b9\u6cd5\u5177\u6709\u53ef\u6bd4\u6027\u7684\u53c2\u6570\u548c\u9006\u6295\u5f71\uff0c\u540c\u65f6\u80fd\u591f\u5206\u6790\u5d4c\u5165\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2508.12355", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12355", "abs": "https://arxiv.org/abs/2508.12355", "authors": ["Eviatar Nachshoni", "Arie Cattan", "Shmuel Amar", "Ori Shapira", "Ido Dagan"], "title": "Consensus or Conflict? Fine-Grained Evaluation of Conflicting Answers in Question-Answering", "comment": "no comments", "summary": "Large Language Models (LLMs) have demonstrated strong performance in question\nanswering (QA) tasks. However, Multi-Answer Question Answering (MAQA), where a\nquestion may have several valid answers, remains challenging. Traditional QA\nsettings often assume consistency across evidences, but MAQA can involve\nconflicting answers. Constructing datasets that reflect such conflicts is\ncostly and labor-intensive, while existing benchmarks often rely on synthetic\ndata, restrict the task to yes/no questions, or apply unverified automated\nannotation. To advance research in this area, we extend the conflict-aware MAQA\nsetting to require models not only to identify all valid answers, but also to\ndetect specific conflicting answer pairs, if any. To support this task, we\nintroduce a novel cost-effective methodology for leveraging fact-checking\ndatasets to construct NATCONFQA, a new benchmark for realistic, conflict-aware\nMAQA, enriched with detailed conflict labels, for all answer pairs. We evaluate\neight high-end LLMs on NATCONFQA, revealing their fragility in handling various\ntypes of conflicts and the flawed strategies they employ to resolve them.", "AI": {"tldr": "The paper introduces NATCONFQA, a new benchmark for realistic, conflict-aware Multi-Answer Question Answering (MAQA), and evaluates eight high-end LLMs on it, revealing their fragility in handling conflicts.", "motivation": "Multi-Answer Question Answering (MAQA), where a question may have several valid answers, remains challenging because traditional QA settings often assume consistency across evidences, but MAQA can involve conflicting answers. Constructing datasets that reflect such conflicts is costly and labor-intensive, while existing benchmarks often rely on synthetic data, restrict the task to yes/no questions, or apply unverified automated annotation.", "method": "A novel cost-effective methodology for leveraging fact-checking datasets to construct NATCONFQA, a new benchmark for realistic, conflict-aware MAQA, enriched with detailed conflict labels, for all answer pairs.", "result": "Introduce NATCONFQA, a new benchmark for realistic, conflict-aware MAQA, enriched with detailed conflict labels, for all answer pairs.", "conclusion": "Eight high-end LLMs are evaluated on NATCONFQA, revealing their fragility in handling various types of conflicts and the flawed strategies they employ to resolve them."}}
{"id": "2508.12036", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12036", "abs": "https://arxiv.org/abs/2508.12036", "authors": ["Rakesh Thakur", "Yusra Tariq"], "title": "Q-FSRU: Quantum-Augmented Frequency-Spectral Fusion for Medical Visual Question Answering", "comment": "8 pages, 4 figures Submitted to AAAI 26", "summary": "Solving tough clinical questions that require both image and text\nunderstanding is still a major challenge in healthcare AI. In this work, we\npropose Q-FSRU, a new model that combines Frequency Spectrum Representation and\nFusion (FSRU) with a method called Quantum Retrieval-Augmented Generation\n(Quantum RAG) for medical Visual Question Answering (VQA). The model takes in\nfeatures from medical images and related text, then shifts them into the\nfrequency domain using Fast Fourier Transform (FFT). This helps it focus on\nmore meaningful data and filter out noise or less useful information. To\nimprove accuracy and ensure that answers are based on real knowledge, we add a\nquantum-inspired retrieval system. It fetches useful medical facts from\nexternal sources using quantum-based similarity techniques. These details are\nthen merged with the frequency-based features for stronger reasoning. We\nevaluated our model using the VQA-RAD dataset, which includes real radiology\nimages and questions. The results showed that Q-FSRU outperforms earlier\nmodels, especially on complex cases needing image-text reasoning. The mix of\nfrequency and quantum information improves both performance and explainability.\nOverall, this approach offers a promising way to build smart, clear, and\nhelpful AI tools for doctors.", "AI": {"tldr": "Q-FSRU\u662f\u4e00\u79cd\u65b0\u7684\u533b\u5b66VQA\u6a21\u578b\uff0c\u5b83\u7ed3\u5408\u4e86\u9891\u7387\u9891\u8c31\u8868\u793a\u548c\u91cf\u5b50\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff0c\u4ee5\u63d0\u9ad8\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728VQA-RAD\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4e4b\u524d\u7684\u6a21\u578b\u3002", "motivation": "\u5728\u533b\u7597\u4fdd\u5065\u4eba\u5de5\u667a\u80fd\u4e2d\uff0c\u89e3\u51b3\u9700\u8981\u56fe\u50cf\u548c\u6587\u672c\u7406\u89e3\u7684\u96be\u9898\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\u3002", "method": "\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u9891\u7387\u9891\u8c31\u8868\u793a\u548c\u878d\u5408\uff08FSRU\uff09\u4e0e\u4e00\u79cd\u79f0\u4e3a\u91cf\u5b50\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08Quantum RAG\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u3002", "result": "Q-FSRU\u6a21\u578b\u5728VQA-RAD\u6570\u636e\u96c6\u4e0a\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u4e4b\u524d\u7684\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u56fe\u50cf-\u6587\u672c\u63a8\u7406\u7684\u590d\u6742\u6848\u4f8b\u4e2d\u3002", "conclusion": "Q-FSRU\u6a21\u578b\u5728VQA-RAD\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u4e4b\u524d\u7684\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u56fe\u50cf-\u6587\u672c\u63a8\u7406\u7684\u590d\u6742\u6848\u4f8b\u4e2d\u3002\u9891\u7387\u548c\u91cf\u5b50\u4fe1\u606f\u7684\u7ed3\u5408\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u667a\u80fd\u3001\u6e05\u6670\u548c\u6709\u7528\u7684\u533b\u751fAI\u5de5\u5177\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2508.12647", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12647", "abs": "https://arxiv.org/abs/2508.12647", "authors": ["Hengnian Gu", "Zhifu Chen", "Yuxin Chen", "Jin Peng Zhou", "Dongdai Zhou"], "title": "Cognitive Structure Generation: From Educational Priors to Policy Optimization", "comment": null, "summary": "Cognitive structure is a student's subjective organization of an objective\nknowledge system, reflected in the psychological construction of concepts and\ntheir relations. However, cognitive structure assessment remains a\nlong-standing challenge in student modeling and psychometrics, persisting as a\nfoundational yet largely unassessable concept in educational practice. This\npaper introduces a novel framework, Cognitive Structure Generation (CSG), in\nwhich we first pretrain a Cognitive Structure Diffusion Probabilistic Model\n(CSDPM) to generate students' cognitive structures from educational priors, and\nthen further optimize its generative process as a policy with hierarchical\nreward signals via reinforcement learning to align with genuine cognitive\ndevelopment levels during students' learning processes. Experimental results on\nfour popular real-world education datasets show that cognitive structures\ngenerated by CSG offer more comprehensive and effective representations for\nstudent modeling, substantially improving performance on KT and CD tasks while\nenhancing interpretability.", "AI": {"tldr": "This paper introduces a Cognitive Structure Generation (CSG) framework to generate students' cognitive structures, which improves student modeling performance.", "motivation": "cognitive structure assessment remains a long-standing challenge in student modeling and psychometrics, persisting as a foundational yet largely unassessable concept in educational practice", "method": "introduce a novel framework, Cognitive Structure Generation (CSG), in which we first pretrain a Cognitive Structure Diffusion Probabilistic Model (CSDPM) to generate students' cognitive structures from educational priors, and then further optimize its generative process as a policy with hierarchical reward signals via reinforcement learning to align with genuine cognitive development levels during students' learning processes", "result": "Experimental results on four popular real-world education datasets show that cognitive structures generated by CSG offer more comprehensive and effective representations for student modeling, substantially improving performance on KT and CD tasks while enhancing interpretability.", "conclusion": "cognitive structures generated by CSG offer more comprehensive and effective representations for student modeling, substantially improving performance on KT and CD tasks while enhancing interpretability"}}
{"id": "2508.12162", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12162", "abs": "https://arxiv.org/abs/2508.12162", "authors": ["J. M. I. H. Jayakody", "A. M. H. H. Alahakoon", "C. R. M. Perera", "R. M. L. C. Srimal", "Roshan Ragel", "Vajira Thambawita", "Isuru Nawinne"], "title": "AICRN: Attention-Integrated Convolutional Residual Network for Interpretable Electrocardiogram Analysis", "comment": null, "summary": "The paradigm of electrocardiogram (ECG) analysis has evolved into real-time\ndigital analysis, facilitated by artificial intelligence (AI) and machine\nlearning (ML), which has improved the diagnostic precision and predictive\ncapacity of cardiac diseases. This work proposes a novel deep learning (DL)\narchitecture called the attention-integrated convolutional residual network\n(AICRN) to regress key ECG parameters such as the PR interval, the QT interval,\nthe QRS duration, the heart rate, the peak amplitude of the R wave, and the\namplitude of the T wave for interpretable ECG analysis. Our architecture is\nspecially designed with spatial and channel attention-related mechanisms to\naddress the type and spatial location of the ECG features for regression. The\nmodels employ a convolutional residual network to address vanishing and\nexploding gradient problems. The designed system addresses traditional analysis\nchallenges, such as loss of focus due to human errors, and facilitates the fast\nand easy detection of cardiac events, thereby reducing the manual efforts\nrequired to solve analysis tasks. AICRN models outperform existing models in\nparameter regression with higher precision. This work demonstrates that DL can\nplay a crucial role in the interpretability and precision of ECG analysis,\nopening up new clinical applications for cardiac monitoring and management.", "AI": {"tldr": "This paper proposes a novel deep learning architecture called AICRN to regress key ECG parameters with higher precision.", "motivation": "improve the diagnostic precision and predictive capacity of cardiac diseases and address traditional analysis challenges, such as loss of focus due to human errors, and facilitates the fast and easy detection of cardiac events, thereby reducing the manual efforts required to solve analysis tasks.", "method": "a novel deep learning (DL) architecture called the attention-integrated convolutional residual network (AICRN)", "result": "AICRN models outperform existing models in parameter regression with higher precision.", "conclusion": "DL can play a crucial role in the interpretability and precision of ECG analysis, opening up new clinical applications for cardiac monitoring and management."}}
{"id": "2508.12387", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12387", "abs": "https://arxiv.org/abs/2508.12387", "authors": ["Yuanfeng Xu", "Zehui Dai", "Jian Liang", "Jiapeng Guan", "Guangrun Wang", "Liang Lin", "Xiaohui Lv"], "title": "ReaLM: Reflection-Enhanced Autonomous Reasoning with Small Language Models", "comment": "16pages, 3 figures", "summary": "Small Language Models (SLMs) are a cost-effective alternative to Large\nLanguage Models (LLMs), but often struggle with complex reasoning due to their\nlimited capacity and a tendency to produce mistakes or inconsistent answers\nduring multi-step reasoning. Existing efforts have improved SLM performance,\nbut typically at the cost of one or more of three key aspects: (1) reasoning\ncapability, due to biased supervision that filters out negative reasoning paths\nand limits learning from errors; (2) autonomy, due to over-reliance on\nexternally generated reasoning signals; and (3) generalization, which suffers\nwhen models overfit to teacher-specific patterns. In this paper, we introduce\nReaLM, a reinforcement learning framework for robust and self-sufficient\nreasoning in vertical domains. To enhance reasoning capability, we propose\nMulti-Route Process Verification (MRPV), which contrasts both positive and\nnegative reasoning paths to extract decisive patterns. To reduce reliance on\nexternal guidance and improve autonomy, we introduce Enabling Autonomy via\nAsymptotic Induction (EAAI), a training strategy that gradually fades external\nsignals. To improve generalization, we apply guided chain-of-thought\ndistillation to encode domain-specific rules and expert knowledge into SLM\nparameters, making them part of what the model has learned. Extensive\nexperiments on both vertical and general reasoning tasks demonstrate that ReaLM\nsignificantly improves SLM performance across aspects (1)-(3) above.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u6846\u67b6ReaLM\uff0c\u7528\u4e8e\u589e\u5f3aSLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u9ad8\u81ea\u4e3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c0f\u8bed\u8a00\u6a21\u578b(slm)\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u7684\u4e00\u79cd\u7ecf\u6d4e\u6709\u6548\u7684\u66ff\u4ee3\u54c1\uff0c\u4f46\u7531\u4e8e\u5176\u5bb9\u91cf\u6709\u9650\uff0c\u5e76\u4e14\u5728\u591a\u6b65\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5bb9\u6613\u4ea7\u751f\u9519\u8bef\u6216\u4e0d\u4e00\u81f4\u7684\u7b54\u6848\uff0c\u56e0\u6b64\u5e38\u5e38\u96be\u4ee5\u8fdb\u884c\u590d\u6742\u7684\u63a8\u7406\u3002", "method": "\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542bMulti-Route Process Verification (MRPV) \u548c Enabling Autonomy via Asymptotic Induction (EAAI)", "result": "ReaLM\u5728\u5782\u76f4\u548c\u4e00\u822c\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86SLM\u7684\u6027\u80fd\u3002", "conclusion": "ReaLM\u663e\u8457\u63d0\u9ad8\u4e86SLM\u5728\u4e0a\u8ff0(1)-(3)\u65b9\u9762\u7684\u6027\u80fd\u3002"}}
{"id": "2508.12081", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12081", "abs": "https://arxiv.org/abs/2508.12081", "authors": ["Haidong Xu", "Guangwei Xu", "Zhedong Zheng", "Xiatian Zhu", "Wei Ji", "Xiangtai Li", "Ruijie Guo", "Meishan Zhang", "Min zhang", "Hao Fei"], "title": "VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models", "comment": "20 pages,13 figures", "summary": "This paper introduces VimoRAG, a novel video-based retrieval-augmented motion\ngeneration framework for motion large language models (LLMs). As motion LLMs\nface severe out-of-domain/out-of-vocabulary issues due to limited annotated\ndata, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D\nmotion generation by retrieving relevant 2D human motion signals. While\nvideo-based motion RAG is nontrivial, we address two key bottlenecks: (1)\ndeveloping an effective motion-centered video retrieval model that\ndistinguishes human poses and actions, and (2) mitigating the issue of error\npropagation caused by suboptimal retrieval results. We design the Gemini Motion\nVideo Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,\nenabling effective retrieval and generation processes. Experimental results\nshow that VimoRAG significantly boosts the performance of motion LLMs\nconstrained to text-only input.", "AI": {"tldr": "VimoRAG\u662f\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u89c6\u9891\u7684\u68c0\u7d22\u589e\u5f3a\u8fd0\u52a8\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u8fd0\u52a8\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u3002", "motivation": "\u7531\u4e8e\u6ce8\u91ca\u6570\u636e\u6709\u9650\uff0c\u8fd0\u52a8llm\u9762\u4e34\u4e25\u91cd\u7684\u57df\u5916/\u8bcd\u6c47\u5916\u95ee\u9898\uff0cVimoRAG\u5229\u7528\u5927\u89c4\u6a21\u7684\u91ce\u751f\u89c6\u9891\u6570\u636e\u5e93\uff0c\u901a\u8fc7\u68c0\u7d22\u76f8\u5173\u76842D\u4eba\u4f53\u8fd0\u52a8\u4fe1\u53f7\u6765\u589e\u5f3a3D\u8fd0\u52a8\u751f\u6210\u3002", "method": "\u8bbe\u8ba1\u4e86Gemini\u8fd0\u52a8\u89c6\u9891\u68c0\u7d22\u5668\u673a\u5236\u548c\u4ee5\u8fd0\u52a8\u4e3a\u4e2d\u5fc3\u7684\u53cc\u91cd\u5bf9\u9f50DPO\u8bad\u7ec3\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c", "conclusion": "VimoRAG\u663e\u8457\u63d0\u5347\u4e86\u4ec5\u9650\u6587\u672c\u8f93\u5165\u7684\u8fd0\u52a8llm\u7684\u6027\u80fd\u3002"}}
{"id": "2508.12651", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.12651", "abs": "https://arxiv.org/abs/2508.12651", "authors": ["Chunliang Hua", "Xiao Hu", "Jiayang Sun", "Zeyuan Yang"], "title": "The Maximum Coverage Model and Recommendation System for UAV Vertiports Location Planning", "comment": "10 pages", "summary": "As urban aerial mobility (UAM) infrastructure development accelerates\nglobally, cities like Shenzhen are planning large-scale vertiport networks\n(e.g., 1,200+ facilities by 2026). Existing planning frameworks remain\ninadequate for this complexity due to historical limitations in data\ngranularity and real-world applicability. This paper addresses these gaps by\nfirst proposing the Capacitated Dynamic Maximum Covering Location Problem\n(CDMCLP), a novel optimization framework that simultaneously models urban-scale\nspatial-temporal demand, heterogeneous user behaviors, and infrastructure\ncapacity constraints. Building on this foundation, we introduce an Integrated\nPlanning Recommendation System that combines CDMCLP with socio-economic factors\nand dynamic clustering initialization. This system leverages adaptive parameter\ntuning based on empirical user behavior to generate practical planning\nsolutions. Validation in a Chinese center city demonstrates the effectiveness\nof the new optimization framework and recommendation system. Under the\nevaluation and optimization of CDMCLP, the quantitative performance of\ntraditional location methods are exposed and can be improved by 38\\%--52\\%,\nwhile the recommendation system shows user-friendliness and the effective\nintegration of complex elements. By integrating mathematical rigor with\npractical implementation considerations, this hybrid approach bridges the gap\nbetween theoretical location modeling and real-world UAM infrastructure\nplanning, offering municipalities a pragmatic tool for vertiport network\ndesign.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8evertiport\u7f51\u7edc\u8bbe\u8ba1\u7684\u5b9e\u7528\u5de5\u5177\uff0c\u901a\u8fc7\u5c06\u6570\u5b66\u4e25\u8c28\u6027\u4e0e\u5b9e\u9645\u5b9e\u65bd\u8003\u8651\u76f8\u7ed3\u5408\uff0c\u5f25\u5408\u4e86\u7406\u8bba\u4f4d\u7f6e\u5efa\u6a21\u4e0e\u5b9e\u9645UAM\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u7684\u89c4\u5212\u6846\u67b6\u4ecd\u7136\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u8fd9\u79cd\u590d\u6742\u6027\uff0c\u56e0\u4e3a\u5386\u53f2\u6570\u636e\u7c92\u5ea6\u548c\u5b9e\u9645\u9002\u7528\u6027\u7684\u9650\u5236\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u6846\u67b6\uff0c\u5373\u5bb9\u91cf\u52a8\u6001\u6700\u5927\u8986\u76d6\u4f4d\u7f6e\u95ee\u9898\uff08CDMCLP\uff09\uff0c\u8be5\u6846\u67b6\u540c\u65f6\u6a21\u62df\u57ce\u5e02\u89c4\u6a21\u7684\u65f6\u7a7a\u9700\u6c42\u3001\u5f02\u6784\u7528\u6237\u884c\u4e3a\u548c\u57fa\u7840\u8bbe\u65bd\u5bb9\u91cf\u7ea6\u675f\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u96c6\u6210\u7684\u89c4\u5212\u63a8\u8350\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5c06CDMCLP\u4e0e\u793e\u4f1a\u7ecf\u6d4e\u56e0\u7d20\u548c\u52a8\u6001\u805a\u7c7b\u521d\u59cb\u5316\u76f8\u7ed3\u5408\u3002", "result": "\u5728CDMCLP\u7684\u8bc4\u4f30\u548c\u4f18\u5316\u4e0b\uff0c\u4f20\u7edf\u7684\u4f4d\u7f6e\u9009\u62e9\u65b9\u6cd5\u7684\u5b9a\u91cf\u6027\u80fd\u88ab\u66b4\u9732\u51fa\u6765\uff0c\u53ef\u4ee5\u63d0\u9ad838%-52%\uff0c\u540c\u65f6\u63a8\u8350\u7cfb\u7edf\u663e\u793a\u51fa\u7528\u6237\u53cb\u597d\u6027\u548c\u590d\u6742\u5143\u7d20\u7684\u6709\u6548\u96c6\u6210\u3002", "conclusion": "\u65b0\u7684\u4f18\u5316\u6846\u67b6\u548c\u63a8\u8350\u7cfb\u7edf\u5728\u4e2d\u56fd\u4e2d\u5fc3\u57ce\u5e02\u5f97\u5230\u9a8c\u8bc1\uff0c\u4f20\u7edf\u7684\u4f4d\u7f6e\u9009\u62e9\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7CDMCLP\u7684\u8bc4\u4f30\u548c\u4f18\u5316\u63d0\u9ad838%-52%\uff0c\u540c\u65f6\u63a8\u8350\u7cfb\u7edf\u663e\u793a\u51fa\u7528\u6237\u53cb\u597d\u6027\u548c\u590d\u6742\u5143\u7d20\u7684\u6709\u6548\u96c6\u6210\u3002"}}
{"id": "2508.12212", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.12212", "abs": "https://arxiv.org/abs/2508.12212", "authors": ["Chuanliu Fan", "Zicheng Ma", "Jun Gao", "Nan Yu", "Jun Zhang", "Ziqiang Cao", "Yi Qin Gao", "Guohong Fu"], "title": "ProtTeX-CC: Activating In-Context Learning in Protein LLM via Two-Stage Instruction Compression", "comment": null, "summary": "Recent advances in protein large language models, such as ProtTeX, represent\nboth side-chain amino acids and backbone structure as discrete token sequences\nof residue length. While this design enables unified modeling of multimodal\nprotein information, it suffers from two major limitations: (1) The\nconcatenation of sequence and structure tokens approximately doubles the\nprotein length and breaks the intrinsic residue-level alignment between\nmodalities. (2) Constrained by the training corpus and limited context window,\nProtTeX is typically trained on single-protein inputs, rendering it\nincompatible with in-context learning (ICL) and thus limiting its\ngeneralization capability. To address these issues, we propose ProtTeX-CC, a\nlightweight two-stage compression framework designed to enhance ProtTeX under\nfew-shot settings. We first design a joint embedding compression mechanism that\nfuses sequence and structure representations at the residue level, effectively\nreducing the protein input length by half without sacrificing performance. Then\nwe propose a self-compression module that aggregates each full demonstration\ninto the latent space of the last few linguistic tokens, reducing the average\ndemonstration length from 751 tokens to less than 16 tokens. Compared to the\noriginal ProtTeX, our self-compression approach achieves a compression ratio of\napproximately 93.68% in the total prompt length under the 16-shot setting.\nWithout modifying the backbone model, ProtTeX-CC introduces only a small number\nof additional parameters through PEFT-based tuning in the joint embedding\ncompression stage and a single trainable projection layer in the\nself-compression stage. Extensive experiments on protein function prediction\nshow that ProtTeX-CC improves performance on the in-domain benchmark by 2%, and\ngeneralizes well to the out-of-domain dataset with a performance gain of 11%.", "AI": {"tldr": "ProtTeX-CC, a two-stage compression framework, enhances ProtTeX for few-shot learning by fusing sequence and structure representations and compressing demonstrations, achieving significant compression and performance gains.", "motivation": "Recent advances in protein large language models, such as ProtTeX, represent both side-chain amino acids and backbone structure as discrete token sequences of residue length. While this design enables unified modeling of multimodal protein information, it suffers from two major limitations: (1) The concatenation of sequence and structure tokens approximately doubles the protein length and breaks the intrinsic residue-level alignment between modalities. (2) Constrained by the training corpus and limited context window, ProtTeX is typically trained on single-protein inputs, rendering it incompatible with in-context learning (ICL) and thus limiting its generalization capability.", "method": "a lightweight two-stage compression framework designed to enhance ProtTeX under few-shot settings. We first design a joint embedding compression mechanism that fuses sequence and structure representations at the residue level, effectively reducing the protein input length by half without sacrificing performance. Then we propose a self-compression module that aggregates each full demonstration into the latent space of the last few linguistic tokens, reducing the average demonstration length from 751 tokens to less than 16 tokens.", "result": "Compared to the original ProtTeX, our self-compression approach achieves a compression ratio of approximately 93.68% in the total prompt length under the 16-shot setting. Without modifying the backbone model, ProtTeX-CC introduces only a small number of additional parameters through PEFT-based tuning in the joint embedding compression stage and a single trainable projection layer in the self-compression stage.", "conclusion": "ProtTeX-CC improves performance on the in-domain benchmark by 2%, and generalizes well to the out-of-domain dataset with a performance gain of 11%."}}
{"id": "2508.12393", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12393", "abs": "https://arxiv.org/abs/2508.12393", "authors": ["Duzhen Zhang", "Zixiao Wang", "Zhong-Zhi Li", "Yahan Yu", "Shuncheng Jia", "Jiahua Dong", "Haotian Xu", "Xing Wu", "Yingying Zhang", "Tielin Zhang", "Jie Yang", "Xiuying Chen", "Le Song"], "title": "MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph", "comment": null, "summary": "The rapid expansion of medical literature presents growing challenges for\nstructuring and integrating domain knowledge at scale. Knowledge Graphs (KGs)\noffer a promising solution by enabling efficient retrieval, automated\nreasoning, and knowledge discovery. However, current KG construction methods\noften rely on supervised pipelines with limited generalizability or naively\naggregate outputs from Large Language Models (LLMs), treating biomedical\ncorpora as static and ignoring the temporal dynamics and contextual uncertainty\nof evolving knowledge. To address these limitations, we introduce MedKGent, a\nLLM agent framework for constructing temporally evolving medical KGs.\nLeveraging over 10 million PubMed abstracts published between 1975 and 2023, we\nsimulate the emergence of biomedical knowledge via a fine-grained daily time\nseries. MedKGent incrementally builds the KG in a day-by-day manner using two\nspecialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor\nAgent identifies knowledge triples and assigns confidence scores via\nsampling-based estimation, which are used to filter low-confidence extractions\nand inform downstream processing. The Constructor Agent incrementally\nintegrates the retained triples into a temporally evolving graph, guided by\nconfidence scores and timestamps to reinforce recurring knowledge and resolve\nconflicts. The resulting KG contains 156,275 entities and 2,971,384 relational\ntriples. Quality assessments by two SOTA LLMs and three domain experts\ndemonstrate an accuracy approaching 90\\%, with strong inter-rater agreement. To\nevaluate downstream utility, we conduct RAG across seven medical question\nanswering benchmarks using five leading LLMs, consistently observing\nsignificant improvements over non-augmented baselines. Case studies further\ndemonstrate the KG's value in literature-based drug repurposing via\nconfidence-aware causal inference.", "AI": {"tldr": "MedKGent, a LLM agent framework, constructs a temporally evolving medical KG from PubMed abstracts, achieving high accuracy and improving medical question answering and drug repurposing.", "motivation": "current KG construction methods often rely on supervised pipelines with limited generalizability or naively aggregate outputs from Large Language Models (LLMs), treating biomedical corpora as static and ignoring the temporal dynamics and contextual uncertainty of evolving knowledge", "method": "introduce MedKGent, a LLM agent framework for constructing temporally evolving medical KGs. Leveraging over 10 million PubMed abstracts published between 1975 and 2023, we simulate the emergence of biomedical knowledge via a fine-grained daily time series. MedKGent incrementally builds the KG in a day-by-day manner using two specialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor Agent identifies knowledge triples and assigns confidence scores via sampling-based estimation, which are used to filter low-confidence extractions and inform downstream processing. The Constructor Agent incrementally integrates the retained triples into a temporally evolving graph, guided by confidence scores and timestamps to reinforce recurring knowledge and resolve conflicts.", "result": "Quality assessments by two SOTA LLMs and three domain experts demonstrate an accuracy approaching 90%, with strong inter-rater agreement. To evaluate downstream utility, we conduct RAG across seven medical question answering benchmarks using five leading LLMs, consistently observing significant improvements over non-augmented baselines.", "conclusion": "The resulting KG contains 156,275 entities and 2,971,384 relational triples. Quality assessments by two SOTA LLMs and three domain experts demonstrate an accuracy approaching 90%, with strong inter-rater agreement. To evaluate downstream utility, we conduct RAG across seven medical question answering benchmarks using five leading LLMs, consistently observing significant improvements over non-augmented baselines. Case studies further demonstrate the KG's value in literature-based drug repurposing via confidence-aware causal inference."}}
{"id": "2508.12082", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12082", "abs": "https://arxiv.org/abs/2508.12082", "authors": ["Seungju Yoo", "Hyuk Kwon", "Joong-Won Hwang", "Kibok Lee"], "title": "Automated Model Evaluation for Object Detection via Prediction Consistency and Reliablity", "comment": "ICCV 2025 Oral", "summary": "Recent advances in computer vision have made training object detectors more\nefficient and effective; however, assessing their performance in real-world\napplications still relies on costly manual annotation. To address this\nlimitation, we develop an automated model evaluation (AutoEval) framework for\nobject detection. We propose Prediction Consistency and Reliability (PCR),\nwhich leverages the multiple candidate bounding boxes that conventional\ndetectors generate before non-maximum suppression (NMS). PCR estimates\ndetection performance without ground-truth labels by jointly measuring 1) the\nspatial consistency between boxes before and after NMS, and 2) the reliability\nof the retained boxes via the confidence scores of overlapping boxes. For a\nmore realistic and scalable evaluation, we construct a meta-dataset by applying\nimage corruptions of varying severity. Experimental results demonstrate that\nPCR yields more accurate performance estimates than existing AutoEval methods,\nand the proposed meta-dataset covers a wider range of detection performance.\nThe code is available at https://github.com/YonseiML/autoeval-det.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u6a21\u578b\u8bc4\u4f30 (AutoEval) \u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u8c61\u68c0\u6d4b\u3002\u5b83\u901a\u8fc7\u6d4b\u91cf NMS \u524d\u540e\u6846\u4e4b\u95f4\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\u548c\u91cd\u53e0\u6846\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\u6765\u4f30\u8ba1\u68c0\u6d4b\u6027\u80fd\uff0c\u800c\u65e0\u9700\u4f7f\u7528\u4efb\u4f55\u6807\u7b7e\u3002", "motivation": "\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u4e2d\u8bc4\u4f30\u7269\u4f53\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u4ecd\u7136\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u624b\u52a8\u6ce8\u91ca\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u9650\u5236\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u4e8e\u7269\u4f53\u68c0\u6d4b\u7684\u81ea\u52a8\u5316\u6a21\u578b\u8bc4\u4f30 (AutoEval) \u6846\u67b6\u3002", "method": "Prediction Consistency and Reliability (PCR)\uff0c\u5b83\u5229\u7528\u4f20\u7edf\u68c0\u6d4b\u5668\u5728\u975e\u6781\u5927\u503c\u6291\u5236 (NMS) \u4e4b\u524d\u751f\u6210\u7684\u591a\u4e2a\u5019\u9009\u8fb9\u754c\u6846\u3002PCR \u901a\u8fc7\u8054\u5408\u6d4b\u91cf 1) NMS \u524d\u540e\u6846\u4e4b\u95f4\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u4ee5\u53ca 2) \u901a\u8fc7\u91cd\u53e0\u6846\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\u8bc4\u4f30\u4fdd\u7559\u6846\u7684\u53ef\u9760\u6027\u6765\u4f30\u8ba1\u68c0\u6d4b\u6027\u80fd\uff0c\u800c\u65e0\u9700 ground-truth \u6807\u7b7e\u3002", "result": "PCR \u4ea7\u751f\u6bd4\u73b0\u6709\u7684 AutoEval \u65b9\u6cd5\u66f4\u51c6\u786e\u7684\u6027\u80fd\u4f30\u8ba1\uff0c\u5e76\u4e14\u63d0\u51fa\u7684\u5143\u6570\u636e\u96c6\u6db5\u76d6\u4e86\u66f4\u5e7f\u6cdb\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "PCR \u6bd4\u73b0\u6709\u7684 AutoEval \u65b9\u6cd5\u4ea7\u751f\u66f4\u51c6\u786e\u7684\u6027\u80fd\u4f30\u8ba1\uff0c\u5e76\u4e14\u63d0\u51fa\u7684\u5143\u6570\u636e\u96c6\u6db5\u76d6\u4e86\u66f4\u5e7f\u6cdb\u7684\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2508.12682", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12682", "abs": "https://arxiv.org/abs/2508.12682", "authors": ["Jinquan Shi", "Yingying Cheng", "Fan Zhang", "Miao Jiang", "Jun Lin", "Yanbai Shen"], "title": "GridCodex: A RAG-Driven AI Framework for Power Grid Code Reasoning and Compliance", "comment": null, "summary": "The global shift towards renewable energy presents unprecedented challenges\nfor the electricity industry, making regulatory reasoning and compliance\nincreasingly vital. Grid codes, the regulations governing grid operations, are\ncomplex and often lack automated interpretation solutions, which hinders\nindustry expansion and undermines profitability for electricity companies. We\nintroduce GridCodex, an end to end framework for grid code reasoning and\ncompliance that leverages large language models and retrieval-augmented\ngeneration (RAG). Our framework advances conventional RAG workflows through\nmulti stage query refinement and enhanced retrieval with RAPTOR. We validate\nthe effectiveness of GridCodex with comprehensive benchmarks, including\nautomated answer assessment across multiple dimensions and regulatory agencies.\nExperimental results showcase a 26.4% improvement in answer quality and more\nthan a 10 fold increase in recall rate. An ablation study further examines the\nimpact of base model selection.", "AI": {"tldr": "GridCodex, a framework using large language models and RAG, helps with grid code reasoning and compliance, improving answer quality and recall rate.", "motivation": "regulatory reasoning and compliance are vital for electricity industry but grid codes are complex and lack automated interpretation solutions", "method": "using large language models and retrieval-augmented generation (RAG)", "result": "GridCodex improves answer quality and recall rate", "conclusion": "GridCodex improves answer quality by 26.4% and recall rate by more than 10 fold."}}
{"id": "2508.12220", "categories": ["cs.LG", "cs.AI", "cs.CR", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.12220", "abs": "https://arxiv.org/abs/2508.12220", "authors": ["Abdullah X"], "title": "Unlearning at Scale: Implementing the Right to be Forgotten in Large Language Models", "comment": "Preprint; 2 figures + several tables; includes appendix.\n  Artifact/code link in paper", "summary": "We study the right to be forgotten (GDPR Art. 17) for large language models\nand frame unlearning as a reproducible systems problem. Our approach treats\ntraining as a deterministic program and logs a minimal per-microbatch record\n(ordered ID hash, RNG seed, learning-rate value, optimizer-step counter, and\naccumulation boundary). Under a pinned stack and deterministic kernels,\nreplaying the training tail while filtering only the forget closure yields the\nsame parameters as training on the retain set (bit-identical in the training\ndtype) when preconditions hold. To meet latency and availability constraints,\nwe add complementary paths: (i) exact reverts of recent steps via\nmicro-checkpoints or dense per-step deltas, (ii) cohort-scoped adapter deletion\nwhen the base is frozen, and (iii) a curvature-guided anti-update followed by a\nshort retain-tune, audit-gated with escalation to exact replay. We report\nstorage/latency budgets and a toy artifact validating mechanics; in a\ncontrolled run that satisfies the preconditions we demonstrate byte-identical\nequality of model and optimizer states.", "AI": {"tldr": "We propose a system for unlearning in large language models by replaying the training tail while filtering the forget closure, and demonstrate byte-identical equality of model and optimizer states.", "motivation": "We study the right to be forgotten (GDPR Art. 17) for large language models and frame unlearning as a reproducible systems problem.", "method": "Our approach treats training as a deterministic program and logs a minimal per-microbatch record. We add complementary paths: (i) exact reverts of recent steps via micro-checkpoints or dense per-step deltas, (ii) cohort-scoped adapter deletion, and (iii) a curvature-guided anti-update followed by a short retain-tune.", "result": "We report storage/latency budgets and a toy artifact validating mechanics.", "conclusion": "We demonstrate byte-identical equality of model and optimizer states under controlled conditions."}}
{"id": "2508.12405", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12405", "abs": "https://arxiv.org/abs/2508.12405", "authors": ["Zilong Bai", "Zihan Xu", "Cong Sun", "Chengxi Zang", "H. Timothy Bunnell", "Catherine Sinfield", "Jacqueline Rutter", "Aaron Thomas Martinez", "L. Charles Bailey", "Mark Weiner", "Thomas R. Campion", "Thomas Carton", "Christopher B. Forrest", "Rainu Kaushal", "Fei Wang", "Yifan Peng"], "title": "Extracting Post-Acute Sequelae of SARS-CoV-2 Infection Symptoms from Clinical Notes via Hybrid Natural Language Processing", "comment": "Accepted for publication in npj Health Systems", "summary": "Accurately and efficiently diagnosing Post-Acute Sequelae of COVID-19 (PASC)\nremains challenging due to its myriad symptoms that evolve over long- and\nvariable-time intervals. To address this issue, we developed a hybrid natural\nlanguage processing pipeline that integrates rule-based named entity\nrecognition with BERT-based assertion detection modules for PASC-symptom\nextraction and assertion detection from clinical notes. We developed a\ncomprehensive PASC lexicon with clinical specialists. From 11 health systems of\nthe RECOVER initiative network across the U.S., we curated 160 intake progress\nnotes for model development and evaluation, and collected 47,654 progress notes\nfor a population-level prevalence study. We achieved an average F1 score of\n0.82 in one-site internal validation and 0.76 in 10-site external validation\nfor assertion detection. Our pipeline processed each note at $2.448\\pm 0.812$\nseconds on average. Spearman correlation tests showed $\\rho >0.83$ for positive\nmentions and $\\rho >0.72$ for negative ones, both with $P <0.0001$. These\ndemonstrate the effectiveness and efficiency of our models and their potential\nfor improving PASC diagnosis.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8e\u4ece\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u63d0\u53d6 PASC \u75c7\u72b6\u548c\u8fdb\u884c\u65ad\u8a00\u68c0\u6d4b\u7684\u6df7\u5408\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u5728 PASC \u8bca\u65ad\u65b9\u9762\u8868\u73b0\u51fa\u6548\u7387\u548c\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e COVID-19 \u540e\u6025\u6027\u671f\u540e\u9057\u75c7 (PASC) \u7684\u65e0\u6570\u75c7\u72b6\u5728\u957f\u671f\u548c\u53ef\u53d8\u7684\u65f6\u95f4\u95f4\u9694\u5185\u6f14\u53d8\uff0c\u56e0\u6b64\u51c6\u786e\u6709\u6548\u5730\u8bca\u65ad PASC \u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u6df7\u5408\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u96c6\u6210\u4e86\u57fa\u4e8e\u89c4\u5219\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4e0e\u57fa\u4e8e BERT \u7684\u65ad\u8a00\u68c0\u6d4b\u6a21\u5757\uff0c\u7528\u4e8e\u4ece\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u63d0\u53d6 PASC \u75c7\u72b6\u548c\u8fdb\u884c\u65ad\u8a00\u68c0\u6d4b\u3002", "result": "\u5728\u4e00\u4e2a\u7ad9\u70b9\u7684\u5185\u90e8\u9a8c\u8bc1\u4e2d\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86 0.82 \u7684\u5e73\u5747 F1 \u5206\u6570\uff0c\u5728 10 \u4e2a\u7ad9\u70b9\u7684\u5916\u90e8\u9a8c\u8bc1\u4e2d\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86 0.76 \u7684\u5e73\u5747 F1 \u5206\u6570\u4ee5\u8fdb\u884c\u65ad\u8a00\u68c0\u6d4b\u3002\u6211\u4eec\u7684\u7ba1\u9053\u5e73\u5747\u4ee5 2.448\u00b10.812 \u79d2\u7684\u901f\u5ea6\u5904\u7406\u6bcf\u4e2a\u97f3\u7b26\u3002Spearman \u76f8\u5173\u6027\u6d4b\u8bd5\u8868\u660e\uff0c\u6b63\u9762\u63d0\u53ca\u7684 \u03c1 >0.83\uff0c\u8d1f\u9762\u63d0\u53ca\u7684 \u03c1 >0.72\uff0c\u4e24\u8005\u5747\u5177\u6709 P <0.0001\u3002", "conclusion": "\u8be5\u6a21\u578b\u6709\u6548\u4e14\u9ad8\u6548\uff0c\u5e76\u5177\u6709\u6539\u5584 PASC \u8bca\u65ad\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.12084", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12084", "abs": "https://arxiv.org/abs/2508.12084", "authors": ["Jaejun Hwang", "Dayoung Gong", "Manjin Kim", "Minsu Cho"], "title": "Generic Event Boundary Detection via Denoising Diffusion", "comment": "Accepted to ICCV 2025", "summary": "Generic event boundary detection (GEBD) aims to identify natural boundaries\nin a video, segmenting it into distinct and meaningful chunks. Despite the\ninherent subjectivity of event boundaries, previous methods have focused on\ndeterministic predictions, overlooking the diversity of plausible solutions. In\nthis paper, we introduce a novel diffusion-based boundary detection model,\ndubbed DiffGEBD, that tackles the problem of GEBD from a generative\nperspective. The proposed model encodes relevant changes across adjacent frames\nvia temporal self-similarity and then iteratively decodes random noise into\nplausible event boundaries being conditioned on the encoded features.\nClassifier-free guidance allows the degree of diversity to be controlled in\ndenoising diffusion. In addition, we introduce a new evaluation metric to\nassess the quality of predictions considering both diversity and fidelity.\nExperiments show that our method achieves strong performance on two standard\nbenchmarks, Kinetics-GEBD and TAPOS, generating diverse and plausible event\nboundaries.", "AI": {"tldr": "Introduces DiffGEBD, a diffusion-based model for generic event boundary detection, addressing the diversity of plausible solutions and achieving strong performance on standard benchmarks.", "motivation": "Previous methods have focused on deterministic predictions, overlooking the diversity of plausible solutions for generic event boundary detection (GEBD).", "method": "A novel diffusion-based boundary detection model (DiffGEBD) that encodes relevant changes across adjacent frames via temporal self-similarity and iteratively decodes random noise into plausible event boundaries.", "result": "The method achieves strong performance on Kinetics-GEBD and TAPOS benchmarks, generating diverse and plausible event boundaries. A new evaluation metric is introduced to assess the quality of predictions considering both diversity and fidelity.", "conclusion": "The proposed DiffGEBD model achieves strong performance on Kinetics-GEBD and TAPOS benchmarks, generating diverse and plausible event boundaries."}}
{"id": "2508.12687", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12687", "abs": "https://arxiv.org/abs/2508.12687", "authors": ["Ashish Seth", "Utkarsh Tyagi", "Ramaneswaran Selvakumar", "Nishit Anand", "Sonal Kumar", "Sreyan Ghosh", "Ramani Duraiswami", "Chirag Agarwal", "Dinesh Manocha"], "title": "EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance in complex multimodal tasks. While MLLMs excel at visual perception\nand reasoning in third-person and egocentric videos, they are prone to\nhallucinations, generating coherent yet inaccurate responses. We present\nEgoIllusion, a first benchmark to evaluate MLLM hallucinations in egocentric\nvideos. EgoIllusion comprises 1,400 videos paired with 8,000 human-annotated\nopen and closed-ended questions designed to trigger hallucinations in both\nvisual and auditory cues in egocentric videos. Evaluations across ten MLLMs\nreveal significant challenges, including powerful models like GPT-4o and\nGemini, achieving only 59% accuracy. EgoIllusion lays the foundation in\ndeveloping robust benchmarks to evaluate the effectiveness of MLLMs and spurs\nthe development of better egocentric MLLMs with reduced hallucination rates.\nOur benchmark will be open-sourced for reproducibility.", "AI": {"tldr": "EgoIllusion \u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30 MLLM \u5728\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\u4e2d\u4ea7\u751f\u5e7b\u89c9\u7684\u57fa\u51c6\uff0c\u7ed3\u679c\u8868\u660e\u73b0\u6709\u6a21\u578b\u5728\u8be5\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u5728\u590d\u6742\u7684\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u867d\u7136 MLLM \u64c5\u957f\u7b2c\u4e09\u4eba\u79f0\u548c\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\u4e2d\u7684\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\uff0c\u4f46\u5b83\u4eec\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u4ea7\u751f\u8fde\u8d2f\u4f46\u4e0d\u51c6\u786e\u7684\u53cd\u5e94\u3002", "method": "EgoIllusion \u5305\u542b 1,400 \u4e2a\u89c6\u9891\uff0c\u5e76\u914d\u6709 8,000 \u4e2a\u4eba\u5de5\u6ce8\u91ca\u7684\u5f00\u653e\u5f0f\u548c\u5c01\u95ed\u5f0f\u95ee\u9898\uff0c\u65e8\u5728\u89e6\u53d1\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\u4e2d\u7684\u89c6\u89c9\u548c\u542c\u89c9\u63d0\u793a\u4e2d\u7684\u5e7b\u89c9\u3002", "result": "\u5bf9 10 \u4e2a MLLM \u7684\u8bc4\u4f30\u663e\u793a\u51fa\u91cd\u5927\u6311\u6218\uff0c\u5305\u62ec\u50cf GPT-4o \u548c Gemini \u8fd9\u6837\u7684\u5f3a\u5927\u6a21\u578b\uff0c\u4ec5\u8fbe\u5230 59% \u7684\u51c6\u786e\u7387\u3002", "conclusion": "EgoIllusion \u4e3a\u8bc4\u4f30 MLLM \u5728\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\u4e2d\u4ea7\u751f\u5e7b\u89c9\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u4fc3\u8fdb\u4e86\u5f00\u53d1\u5177\u6709\u66f4\u4f4e\u5e7b\u89c9\u7387\u7684\u66f4\u597d\u7684\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684 MLLM\u3002"}}
{"id": "2508.12222", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12222", "abs": "https://arxiv.org/abs/2508.12222", "authors": ["Sagar Shrestha", "Rajesh Shrestha", "Tri Nguyen", "Subash Timilsina"], "title": "Distribution Matching via Generalized Consistency Models", "comment": null, "summary": "Recent advancement in generative models have demonstrated remarkable\nperformance across various data modalities. Beyond their typical use in data\nsynthesis, these models play a crucial role in distribution matching tasks such\nas latent variable modeling, domain translation, and domain adaptation.\nGenerative Adversarial Networks (GANs) have emerged as the preferred method of\ndistribution matching due to their efficacy in handling high-dimensional data\nand their flexibility in accommodating various constraints. However, GANs often\nencounter challenge in training due to their bi-level min-max optimization\nobjective and susceptibility to mode collapse. In this work, we propose a novel\napproach for distribution matching inspired by the consistency models employed\nin Continuous Normalizing Flow (CNF). Our model inherits the advantages of CNF\nmodels, such as having a straight forward norm minimization objective, while\nremaining adaptable to different constraints similar to GANs. We provide\ntheoretical validation of our proposed objective and demonstrate its\nperformance through experiments on synthetic and real-world datasets.", "AI": {"tldr": "Proposes a new distribution matching method based on CNF consistency models to address GAN training challenges, validated theoretically and experimentally.", "motivation": "GANs often encounter challenge in training due to their bi-level min-max optimization objective and susceptibility to mode collapse.", "method": "A novel approach for distribution matching inspired by the consistency models employed in Continuous Normalizing Flow (CNF).", "result": "Theoretical validation and demonstration of performance through experiments on synthetic and real-world datasets.", "conclusion": "This paper proposes a novel approach for distribution matching inspired by consistency models in CNF, inheriting the advantages of CNF models while remaining adaptable to different constraints similar to GANs. The approach is validated theoretically and experimentally."}}
{"id": "2508.12407", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12407", "abs": "https://arxiv.org/abs/2508.12407", "authors": ["Zhuorui Liu", "Chen Zhang", "Dawei Song"], "title": "ZigzagAttention: Efficient Long-Context Inference with Exclusive Retrieval and Streaming Heads", "comment": "5 pages, 4 figures", "summary": "With the rapid development of large language models (LLMs), handling long\ncontext has become one of the vital abilities in LLMs. Such long-context\nability is accompanied by difficulties in deployment, especially due to the\nincreased consumption of KV cache. There is certain work aiming to optimize the\nmemory footprint of KV cache, inspired by the observation that attention heads\ncan be categorized into retrieval heads that are of great significance and\nstreaming heads that are of less significance. Typically, identifying the\nstreaming heads and and waiving the KV cache in the streaming heads would\nlargely reduce the overhead without hurting the performance that much. However,\nsince employing both retrieval and streaming heads in one layer decomposes one\nlarge round of attention computation into two small ones, it may unexpectedly\nbring extra latency on accessing and indexing tensors. Based on this intuition,\nwe impose an important improvement to the identification process of retrieval\nand streaming heads, in which we design a criterion that enforces exclusively\nretrieval or streaming heads gathered in one unique layer. In this way, we\nfurther eliminate the extra latency and only incur negligible performance\ndegradation. Our method named \\textsc{ZigzagAttention} is competitive among\nconsidered baselines owing to reduced latency and comparable performance.", "AI": {"tldr": "This paper introduces ZigzagAttention, a method that reduces latency and maintains comparable performance in large language models by grouping retrieval or streaming heads into unique layers.", "motivation": "Handling long context has become one of the vital abilities in LLMs, but it increases the consumption of KV cache. Previous work aimed to optimize the memory footprint of KV cache by identifying and waiving the KV cache in streaming heads, but it may bring extra latency on accessing and indexing tensors.", "method": "The paper designs a criterion that enforces exclusively retrieval or streaming heads gathered in one unique layer.", "result": "ZigzagAttention is competitive among considered baselines owing to reduced latency and comparable performance.", "conclusion": "The paper introduces ZigzagAttention, a method that reduces latency and maintains comparable performance by grouping retrieval or streaming heads into unique layers, thereby eliminating extra latency."}}
{"id": "2508.12089", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12089", "abs": "https://arxiv.org/abs/2508.12089", "authors": ["Qinyuan Fan", "Clemens G\u00fchmann"], "title": "Enhancing 3D point accuracy of laser scanner through multi-stage convolutional neural network for applications in construction", "comment": null, "summary": "We propose a multi-stage convolutional neural network (MSCNN) based\nintegrated method for reducing uncertainty of 3D point accuracy of lasar\nscanner (LS) in rough indoor rooms, providing more accurate spatial\nmeasurements for high-precision geometric model creation and renovation. Due to\ndifferent equipment limitations and environmental factors, high-end and low-end\nLS have positional errors. Our approach pairs high-accuracy scanners (HAS) as\nreferences with corresponding low-accuracy scanners (LAS) of measurements in\nidentical environments to quantify specific error patterns. By establishing a\nstatistical relationship between measurement discrepancies and their spatial\ndistribution, we develop a correction framework that combines traditional\ngeometric processing with targeted neural network refinement. This method\ntransforms the quantification of systematic errors into a supervised learning\nproblem, allowing precise correction while preserving critical geometric\nfeatures. Experimental results in our rough indoor rooms dataset show\nsignificant improvements in measurement accuracy, with mean square error (MSE)\nreductions exceeding 70% and peak signal-to-noise ratio (PSNR) improvements of\napproximately 6 decibels. This approach enables low-end devices to achieve\nmeasurement uncertainty levels approaching those of high-end devices without\nhardware modifications.", "AI": {"tldr": "Using neural networks to improve the accuracy of low-end laser scanners by comparing them to high-accuracy scanners.", "motivation": "Reducing uncertainty of 3D point accuracy of lasar scanner in rough indoor rooms, providing more accurate spatial measurements.", "method": "A multi-stage convolutional neural network based integrated method.", "result": "Measurement accuracy is significantly improved, with MSE reductions exceeding 70% and PSNR improvements of approximately 6 decibels.", "conclusion": "The multi-stage convolutional neural network method significantly improves measurement accuracy, achieving MSE reductions exceeding 70% and PSNR improvements of approximately 6 decibels."}}
{"id": "2508.12725", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12725", "abs": "https://arxiv.org/abs/2508.12725", "authors": ["Wenjie Chen", "Wenbin Li", "Di Yao", "Xuying Meng", "Chang Gong", "Jingping Bi"], "title": "GTool: Graph Enhanced Tool Planning with Large Language Model", "comment": "16 pages, 9 figures", "summary": "Tool planning with large language models (LLMs), referring to selecting,\norganizing, and preparing the tools necessary to complete a user request,\nbridges the gap between natural language understanding and task execution.\nHowever, current works treat different tools as isolated components and fail to\nleverage the inherent dependencies of tools, leading to invalid planning\nresults. Since tool dependencies are often incomplete, it becomes challenging\nfor LLMs to accurately identify the appropriate tools required by a user\nrequest, especially when confronted with a large toolset. To solve this\nchallenge, we propose \\texttt{GTool}, which is the first work aiming to enhance\nthe tool planning ability of LLMs under incomplete dependencies. \\texttt{GTool}\nconstructs a request-specific tool graph to select tools efficiently and\ngenerate the \\texttt{<graph token>} which provides sufficient dependency\ninformation understandable by LLMs. Moreover, a missing dependency prediction\ntask is designed to improve the reliability of \\texttt{GTool} with incomplete\ndependencies. Without trimming LLMs, \\texttt{GTool} can be seamlessly\nintegrated with various LLM backbones without extensive retraining. Extensive\nexperiments show that \\texttt{GTool} achieves more than 29.6\\% performance\nimprovements compared with the state-of-the-art (SOTA) baselines with a\nlight-weight (7B) LLM backbone.", "AI": {"tldr": "GTool improves LLM tool planning by constructing a tool graph and predicting missing dependencies, achieving 29.6% performance gains over SOTA baselines.", "motivation": "Current works treat different tools as isolated components and fail to leverage the inherent dependencies of tools, leading to invalid planning results. Since tool dependencies are often incomplete, it becomes challenging for LLMs to accurately identify the appropriate tools required by a user request, especially when confronted with a large toolset.", "method": "GTool constructs a request-specific tool graph to select tools efficiently and generate the <graph token> which provides sufficient dependency information understandable by LLMs. Moreover, a missing dependency prediction task is designed to improve the reliability of GTool with incomplete dependencies.", "result": "GTool enhances the tool planning ability of LLMs under incomplete dependencies.", "conclusion": "GTool achieves more than 29.6% performance improvements compared with the state-of-the-art (SOTA) baselines with a light-weight (7B) LLM backbone."}}
{"id": "2508.12233", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12233", "abs": "https://arxiv.org/abs/2508.12233", "authors": ["Sagar Shrestha"], "title": "Communication-Efficient Distributed Asynchronous ADMM", "comment": null, "summary": "In distributed optimization and federated learning, asynchronous alternating\ndirection method of multipliers (ADMM) serves as an attractive option for\nlarge-scale optimization, data privacy, straggler nodes and variety of\nobjective functions. However, communication costs can become a major bottleneck\nwhen the nodes have limited communication budgets or when the data to be\ncommunicated is prohibitively large. In this work, we propose introducing\ncoarse quantization to the data to be exchanged in aynchronous ADMM so as to\nreduce communication overhead for large-scale federated learning and\ndistributed optimization applications. We experimentally verify the convergence\nof the proposed method for several distributed learning tasks, including neural\nnetworks.", "AI": {"tldr": "\u63d0\u51fa\u7c97\u91cf\u5316\u5f02\u6b65ADMM\uff0c\u4ee5\u51cf\u5c11\u5927\u89c4\u6a21\u8054\u90a6\u5b66\u4e60\u548c\u5206\u5e03\u5f0f\u4f18\u5316\u5e94\u7528\u7684\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u5728\u5206\u5e03\u5f0f\u4f18\u5316\u548c\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u5f02\u6b65\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\uff08ADMM\uff09\u662f\u5927\u89c4\u6a21\u4f18\u5316\u3001\u6570\u636e\u9690\u79c1\u3001\u843d\u540e\u8282\u70b9\u548c\u5404\u79cd\u76ee\u6807\u51fd\u6570\u7684\u6709\u5438\u5f15\u529b\u7684\u9009\u62e9\u3002\u7136\u800c\uff0c\u5f53\u8282\u70b9\u5177\u6709\u6709\u9650\u7684\u901a\u4fe1\u9884\u7b97\u6216\u5f53\u8981\u901a\u4fe1\u7684\u6570\u636e\u975e\u5e38\u5927\u65f6\uff0c\u901a\u4fe1\u6210\u672c\u53ef\u80fd\u6210\u4e3a\u4e3b\u8981\u7684\u74f6\u9888\u3002", "method": "\u5728\u5f02\u6b65ADMM\u4e2d\u5f15\u5165\u7c97\u91cf\u5316\u5230\u8981\u4ea4\u6362\u7684\u6570\u636e\uff0c\u4ee5\u51cf\u5c11\u5927\u89c4\u6a21\u8054\u90a6\u5b66\u4e60\u548c\u5206\u5e03\u5f0f\u4f18\u5316\u5e94\u7528\u7684\u901a\u4fe1\u5f00\u9500\u3002", "result": "\u63d0\u51fa\u4e86\u5728\u5f02\u6b65ADMM\u4e2d\u5f15\u5165\u7c97\u91cf\u5316\u5230\u8981\u4ea4\u6362\u7684\u6570\u636e\uff0c\u4ee5\u51cf\u5c11\u5927\u89c4\u6a21\u8054\u90a6\u5b66\u4e60\u548c\u5206\u5e03\u5f0f\u4f18\u5316\u5e94\u7528\u7684\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u5305\u62ec\u795e\u7ecf\u7f51\u7edc\u5728\u5185\u7684\u591a\u4e2a\u5206\u5e03\u5f0f\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u6536\u655b\u6027\u3002"}}
{"id": "2508.12411", "categories": ["cs.CL", "I.2.7; K.4.1; H.3.3"], "pdf": "https://arxiv.org/pdf/2508.12411", "abs": "https://arxiv.org/abs/2508.12411", "authors": ["Emanuel Z. Fenech-Borg", "Tilen P. Meznaric-Kos", "Milica D. Lekovic-Bojovic", "Arni J. Hentze-Djurhuus"], "title": "The Cultural Gene of Large Language Models: A Study on the Impact of Cross-Corpus Training on Model Values and Biases", "comment": "10 pages, 5 figures, IEEE conference format, submitted to [Conference\n  Name]", "summary": "Large language models (LLMs) are deployed globally, yet their underlying\ncultural and ethical assumptions remain underexplored. We propose the notion of\na \"cultural gene\" -- a systematic value orientation that LLMs inherit from\ntheir training corpora -- and introduce a Cultural Probe Dataset (CPD) of 200\nprompts targeting two classic cross-cultural dimensions:\nIndividualism-Collectivism (IDV) and Power Distance (PDI). Using standardized\nzero-shot prompts, we compare a Western-centric model (GPT-4) and an\nEastern-centric model (ERNIE Bot). Human annotation shows significant and\nconsistent divergence across both dimensions. GPT-4 exhibits individualistic\nand low-power-distance tendencies (IDV score approx 1.21; PDI score approx\n-1.05), while ERNIE Bot shows collectivistic and higher-power-distance\ntendencies (IDV approx -0.89; PDI approx 0.76); differences are statistically\nsignificant (p < 0.001). We further compute a Cultural Alignment Index (CAI)\nagainst Hofstede's national scores and find GPT-4 aligns more closely with the\nUSA (e.g., IDV CAI approx 0.91; PDI CAI approx 0.88) whereas ERNIE Bot aligns\nmore closely with China (IDV CAI approx 0.85; PDI CAI approx 0.81). Qualitative\nanalyses of dilemma resolution and authority-related judgments illustrate how\nthese orientations surface in reasoning. Our results support the view that LLMs\nfunction as statistical mirrors of their cultural corpora and motivate\nculturally aware evaluation and deployment to avoid algorithmic cultural\nhegemony.", "AI": {"tldr": "LLMs inherit cultural values from their training data, leading to different cultural orientations in Western-centric vs. Eastern-centric models. This highlights the need for culturally aware evaluation and deployment.", "motivation": "The cultural and ethical assumptions of LLMs remain underexplored. Proposes the notion of a cultural gene -- a systematic value orientation that LLMs inherit from their training corpora.", "method": "Compared a Western-centric model (GPT-4) and an Eastern-centric model (ERNIE Bot) using a Cultural Probe Dataset (CPD) of 200 prompts targeting Individualism-Collectivism (IDV) and Power Distance (PDI). Computed a Cultural Alignment Index (CAI) against Hofstede's national scores. Qualitative analyses of dilemma resolution and authority-related judgments were conducted.", "result": "GPT-4 exhibits individualistic and low-power-distance tendencies, aligning more closely with the USA. ERNIE Bot shows collectivistic and higher-power-distance tendencies, aligning more closely with China. Significant and consistent divergence across both dimensions was observed.", "conclusion": "LLMs function as statistical mirrors of their cultural corpora. Culturally aware evaluation and deployment are needed to avoid algorithmic cultural hegemony."}}
{"id": "2508.12094", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12094", "abs": "https://arxiv.org/abs/2508.12094", "authors": ["Songwei Liu", "Hong Liu", "Fangmin Chen", "Xurui Peng", "Chenqian Yan", "Lean Fu", "Xing Mei"], "title": "Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion", "comment": null, "summary": "Diffusion models have transformed image synthesis by establishing\nunprecedented quality and creativity benchmarks. Nevertheless, their\nlarge-scale deployment faces challenges due to computationally intensive\niterative denoising processes. Although post-training quantization(PTQ)\nprovides an effective pathway for accelerating sampling, the iterative nature\nof diffusion models causes stepwise quantization errors to accumulate\nprogressively during generation, inevitably compromising output fidelity. To\naddress this challenge, we develop a theoretical framework that mathematically\nformulates error propagation in Diffusion Models (DMs), deriving per-step\nquantization error propagation equations and establishing the first closed-form\nsolution for cumulative error. Building on this theoretical foundation, we\npropose a timestep-aware cumulative error compensation scheme. Extensive\nexperiments across multiple image datasets demonstrate that our compensation\nstrategy effectively mitigates error propagation, significantly enhancing\nexisting PTQ methods to achieve state-of-the-art(SOTA) performance on\nlow-precision diffusion models.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u5f88\u68d2\uff0c\u4f46\u8ba1\u7b97\u91cf\u5927\u3002\u6211\u4eec\u901a\u8fc7\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\u4f7f\u5b83\u4eec\u66f4\u5feb\u66f4\u7cbe\u786e\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u901a\u8fc7\u5efa\u7acb\u524d\u6240\u672a\u6709\u7684\u8d28\u91cf\u548c\u521b\u9020\u529b\u57fa\u51c6\uff0c\u6539\u53d8\u4e86\u56fe\u50cf\u5408\u6210\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8ba1\u7b97\u5bc6\u96c6\u578b\u7684\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\uff0c\u5b83\u4eec\u7684\u5927\u89c4\u6a21\u90e8\u7f72\u9762\u4e34\u6311\u6218\u3002\u867d\u7136\u8bad\u7ec3\u540e\u91cf\u5316(PTQ)\u4e3a\u52a0\u901f\u91c7\u6837\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u6548\u7684\u9014\u5f84\uff0c\u4f46\u6269\u6563\u6a21\u578b\u7684\u8fed\u4ee3\u6027\u8d28\u5bfc\u81f4\u9010\u6b65\u91cf\u5316\u8bef\u5dee\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u9010\u6e10\u79ef\u7d2f\uff0c\u4e0d\u53ef\u907f\u514d\u5730\u635f\u5bb3\u4e86\u8f93\u51fa\u7684\u4fdd\u771f\u5ea6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4ee5\u6570\u5b66\u65b9\u5f0f\u516c\u5f0f\u5316\u4e86\u6269\u6563\u6a21\u578b(DM)\u4e2d\u7684\u8bef\u5dee\u4f20\u64ad\uff0c\u63a8\u5bfc\u4e86\u6bcf\u6b65\u91cf\u5316\u8bef\u5dee\u4f20\u64ad\u65b9\u7a0b\uff0c\u5e76\u5efa\u7acb\u4e86\u7d2f\u79ef\u8bef\u5dee\u7684\u7b2c\u4e00\u4e2a\u5c01\u95ed\u5f62\u5f0f\u89e3\u3002", "result": "\u5728\u591a\u4e2a\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u8865\u507f\u7b56\u7565\u6709\u6548\u5730\u7f13\u89e3\u4e86\u8bef\u5dee\u4f20\u64ad\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u73b0\u6709\u7684PTQ\u65b9\u6cd5\uff0c\u4ece\u800c\u5728\u4f4e\u7cbe\u5ea6\u6269\u6563\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684(SOTA)\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u63d0\u51fa\u65f6\u95f4\u6b65\u611f\u77e5\u7d2f\u79ef\u8bef\u5dee\u8865\u507f\u65b9\u6848\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u73b0\u6709\u7684PTQ\u65b9\u6cd5\uff0c\u4ece\u800c\u5728\u4f4e\u7cbe\u5ea6\u6269\u6563\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684(SOTA)\u6027\u80fd\u3002"}}
{"id": "2508.12754", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12754", "abs": "https://arxiv.org/abs/2508.12754", "authors": ["Alessio Galatolo", "Luca Alberto Rappuoli", "Katie Winkle", "Meriem Beloucif"], "title": "Beyond Ethical Alignment: Evaluating LLMs as Artificial Moral Assistants", "comment": "Full version of the paper published in ECAI 2025 proceedings (IOS\n  Press, CC BY-NC 4.0)", "summary": "The recent rise in popularity of large language models (LLMs) has prompted\nconsiderable concerns about their moral capabilities. Although considerable\neffort has been dedicated to aligning LLMs with human moral values, existing\nbenchmarks and evaluations remain largely superficial, typically measuring\nalignment based on final ethical verdicts rather than explicit moral reasoning.\nIn response, this paper aims to advance the investigation of LLMs' moral\ncapabilities by examining their capacity to function as Artificial Moral\nAssistants (AMAs), systems envisioned in the philosophical literature to\nsupport human moral deliberation. We assert that qualifying as an AMA requires\nmore than what state-of-the-art alignment techniques aim to achieve: not only\nmust AMAs be able to discern ethically problematic situations, they should also\nbe able to actively reason about them, navigating between conflicting values\noutside of those embedded in the alignment phase. Building on existing\nphilosophical literature, we begin by designing a new formal framework of the\nspecific kind of behaviour an AMA should exhibit, individuating key qualities\nsuch as deductive and abductive moral reasoning. Drawing on this theoretical\nframework, we develop a benchmark to test these qualities and evaluate popular\nopen LLMs against it. Our results reveal considerable variability across models\nand highlight persistent shortcomings, particularly regarding abductive moral\nreasoning. Our work connects theoretical philosophy with practical AI\nevaluation while also emphasising the need for dedicated strategies to\nexplicitly enhance moral reasoning capabilities in LLMs. Code available at\nhttps://github.com/alessioGalatolo/AMAeval", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86LLM\u4f5c\u4e3a\u4eba\u5de5\u9053\u5fb7\u52a9\u7406\uff08AMA\uff09\u7684\u80fd\u529b\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\u548c\u57fa\u51c6\u6765\u8bc4\u4f30\u5176\u9053\u5fb7\u63a8\u7406\u80fd\u529b\uff0c\u7ed3\u679c\u8868\u660eLLM\u5728\u6eaf\u56e0\u9053\u5fb7\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u7684\u7b56\u7565\u6765\u63d0\u9ad8\u5176\u9053\u5fb7\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9053\u5fb7\u80fd\u529b\u5f15\u8d77\u4e86\u76f8\u5f53\u5927\u7684\u5173\u6ce8\u3002\u73b0\u6709\u7684\u57fa\u51c6\u548c\u8bc4\u4f30\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u7136\u662f\u80a4\u6d45\u7684\uff0c\u901a\u5e38\u57fa\u4e8e\u6700\u7ec8\u7684\u4f26\u7406\u5224\u51b3\u800c\u4e0d\u662f\u660e\u786e\u7684\u9053\u5fb7\u63a8\u7406\u6765\u8861\u91cf\u4e00\u81f4\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u7684\u5f62\u5f0f\u6846\u67b6\uff0c\u63cf\u8ff0\u4e86\u4e00\u4e2aAMA\u5e94\u8be5\u5c55\u793a\u7684\u7279\u5b9a\u884c\u4e3a\uff0c\u533a\u5206\u4e86\u8bf8\u5982\u6f14\u7ece\u548c\u6eaf\u56e0\u9053\u5fb7\u63a8\u7406\u7b49\u5173\u952e\u54c1\u8d28\u3002\u5efa\u7acb\u5728\u6b64\u7406\u8bba\u6846\u67b6\u7684\u57fa\u7840\u4e0a\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u51c6\u6765\u6d4b\u8bd5\u8fd9\u4e9b\u54c1\u8d28\uff0c\u5e76\u8bc4\u4f30\u6d41\u884c\u7684\u5f00\u653eLLM\u3002", "result": "\u7ed3\u679c\u63ed\u793a\u4e86\u6a21\u578b\u4e4b\u95f4\u76f8\u5f53\u5927\u7684\u5dee\u5f02\uff0c\u5e76\u7a81\u51fa\u4e86\u6301\u7eed\u5b58\u5728\u7684\u7f3a\u70b9\uff0c\u5c24\u5176\u662f\u5728\u6eaf\u56e0\u9053\u5fb7\u63a8\u7406\u65b9\u9762\u3002", "conclusion": "LLMs\u8868\u73b0\u51fa\u76f8\u5f53\u5927\u7684\u5dee\u5f02\uff0c\u5e76\u7a81\u51fa\u4e86\u6301\u7eed\u5b58\u5728\u7684\u7f3a\u70b9\uff0c\u5c24\u5176\u662f\u5728\u6eaf\u56e0\u9053\u5fb7\u63a8\u7406\u65b9\u9762\u3002\u5f3a\u8c03\u9700\u8981\u4e13\u95e8\u7684\u7b56\u7565\u6765\u660e\u786e\u63d0\u9ad8LLM\u4e2d\u7684\u9053\u5fb7\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2508.12235", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12235", "abs": "https://arxiv.org/abs/2508.12235", "authors": ["Peng Chen", "Yihang Wang", "Yang Shu", "Yunyao Cheng", "Kai Zhao", "Zhongwen Rao", "Lujia Pan", "Bin Yang", "Chenjuan Guo"], "title": "CC-Time: Cross-Model and Cross-Modality Time Series Forecasting", "comment": null, "summary": "With the success of pre-trained language models (PLMs) in various application\nfields beyond natural language processing, language models have raised emerging\nattention in the field of time series forecasting (TSF) and have shown great\nprospects. However, current PLM-based TSF methods still fail to achieve\nsatisfactory prediction accuracy matching the strong sequential modeling power\nof language models. To address this issue, we propose Cross-Model and\nCross-Modality Learning with PLMs for time series forecasting (CC-Time). We\nexplore the potential of PLMs for time series forecasting from two aspects: 1)\nwhat time series features could be modeled by PLMs, and 2) whether relying\nsolely on PLMs is sufficient for building time series models. In the first\naspect, CC-Time incorporates cross-modality learning to model temporal\ndependency and channel correlations in the language model from both time series\nsequences and their corresponding text descriptions. In the second aspect,\nCC-Time further proposes the cross-model fusion block to adaptively integrate\nknowledge from the PLMs and time series model to form a more comprehensive\nmodeling of time series patterns. Extensive experiments on nine real-world\ndatasets demonstrate that CC-Time achieves state-of-the-art prediction accuracy\nin both full-data training and few-shot learning situations.", "AI": {"tldr": "CC-Time incorporates cross-modality learning to model temporal dependency and channel correlations in the language model from both time series sequences and their corresponding text descriptions.CC-Time further proposes the cross-model fusion block to adaptively integrate knowledge from the PLMs and time series model to form a more comprehensive modeling of time series patterns.", "motivation": "Current PLM-based TSF methods still fail to achieve satisfactory prediction accuracy matching the strong sequential modeling power of language models.", "method": "Cross-Model and Cross-Modality Learning with PLMs for time series forecasting (CC-Time)", "result": "achieves state-of-the-art prediction accuracy in both full-data training and few-shot learning situations.", "conclusion": "CC-Time achieves state-of-the-art prediction accuracy in both full-data training and few-shot learning situations."}}
{"id": "2508.12448", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12448", "abs": "https://arxiv.org/abs/2508.12448", "authors": ["Yeongwoo Song", "Jaeyong Bae", "Dong-Kyum Kim", "Hawoong Jeong"], "title": "Uncovering Emergent Physics Representations Learned In-Context by Large Language Models", "comment": "17 pages, 10 figures", "summary": "Large language models (LLMs) exhibit impressive in-context learning (ICL)\nabilities, enabling them to solve wide range of tasks via textual prompts\nalone. As these capabilities advance, the range of applicable domains continues\nto expand significantly. However, identifying the precise mechanisms or\ninternal structures within LLMs that allow successful ICL across diverse,\ndistinct classes of tasks remains elusive. Physics-based tasks offer a\npromising testbed for probing this challenge. Unlike synthetic sequences such\nas basic arithmetic or symbolic equations, physical systems provide\nexperimentally controllable, real-world data based on structured dynamics\ngrounded in fundamental principles. This makes them particularly suitable for\nstudying the emergent reasoning behaviors of LLMs in a realistic yet tractable\nsetting. Here, we mechanistically investigate the ICL ability of LLMs,\nespecially focusing on their ability to reason about physics. Using a dynamics\nforecasting task in physical systems as a proxy, we evaluate whether LLMs can\nlearn physics in context. We first show that the performance of dynamics\nforecasting in context improves with longer input contexts. To uncover how such\ncapability emerges in LLMs, we analyze the model's residual stream activations\nusing sparse autoencoders (SAEs). Our experiments reveal that the features\ncaptured by SAEs correlate with key physical variables, such as energy. These\nfindings demonstrate that meaningful physical concepts are encoded within LLMs\nduring in-context learning. In sum, our work provides a novel case study that\nbroadens our understanding of how LLMs learn in context.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u7269\u7406\u77e5\u8bc6\u3002", "motivation": "\u786e\u5b9a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5141\u8bb8\u5728\u4e0d\u540c\u4efb\u52a1\u7c7b\u522b\u4e2d\u6210\u529f\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u7cbe\u786e\u673a\u5236\u6216\u5185\u90e8\u7ed3\u6784\u4ecd\u7136\u96be\u4ee5\u6349\u6478\u3002\u57fa\u4e8e\u7269\u7406\u7684\u4efb\u52a1\u4e3a\u63a2\u7a76\u8fd9\u4e00\u6311\u6218\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u8bd5\u9a8c\u5e73\u53f0\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u52a8\u7f16\u7801\u5668\uff08SAE\uff09\u5206\u6790\u6a21\u578b\u7684\u6b8b\u5dee\u6d41\u6fc0\u6d3b\u3002", "result": "\u4e0a\u4e0b\u6587\u4e2d\u7684\u52a8\u6001\u9884\u6d4b\u6027\u80fd\u968f\u7740\u66f4\u957f\u7684\u8f93\u5165\u4e0a\u4e0b\u6587\u800c\u63d0\u9ad8\uff1bSAE \u6355\u83b7\u7684\u7279\u5f81\u4e0e\u5173\u952e\u7269\u7406\u53d8\u91cf\uff08\u5982\u80fd\u91cf\uff09\u76f8\u5173\u3002", "conclusion": "\u6709\u610f\u4e49\u7684\u7269\u7406\u6982\u5ff5\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u88ab\u7f16\u7801\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u3002"}}
{"id": "2508.12108", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12108", "abs": "https://arxiv.org/abs/2508.12108", "authors": ["Ziyang Zhang", "Yang Yu", "Xulei Yang", "Si Yong Yeo"], "title": "VELVET-Med: Vision and Efficient Language Pre-training for Volumetric Imaging Tasks in Medicine", "comment": null, "summary": "Vision-and-language models (VLMs) have been increasingly explored in the\nmedical domain, particularly following the success of CLIP in general domain.\nHowever, unlike the relatively straightforward pairing of 2D images and text,\ncurating large-scale paired data in the medical field for volumetric modalities\nsuch as CT scans remains a challenging and time-intensive process. This\ndifficulty often limits the performance on downstream tasks. To address these\nchallenges, we propose a novel vision-language pre-training (VLP) framework,\ntermed as \\textbf{VELVET-Med}, specifically designed for limited volumetric\ndata such as 3D CT and associated radiology reports. Instead of relying on\nlarge-scale data collection, our method focuses on the development of effective\npre-training objectives and model architectures. The key contributions are: 1)\nWe incorporate uni-modal self-supervised learning into VLP framework, which are\noften underexplored in the existing literature. 2) We propose a novel language\nencoder, termed as \\textbf{TriBERT}, for learning multi-level textual\nsemantics. 3) We devise the hierarchical contrastive learning to capture\nmulti-level vision-language correspondence. Using only 38,875 scan-report\npairs, our approach seeks to uncover rich spatial and semantic relationships\nembedded in volumetric medical images and corresponding clinical narratives,\nthereby enhancing the generalization ability of the learned encoders. The\nresulting encoders exhibit strong transferability, achieving state-of-the-art\nperformance across a wide range of downstream tasks, including 3D segmentation,\ncross-modal retrieval, visual question answering, and report generation.", "AI": {"tldr": "propose a novel vision-language pre-training (VLP) framework, termed as VELVET-Med, specifically designed for limited volumetric data such as 3D CT and associated radiology reports,Using only 38,875 scan-report pairs, our approach seeks to uncover rich spatial and semantic relationships embedded in volumetric medical images and corresponding clinical narratives, thereby enhancing the generalization ability of the learned encoders", "motivation": "curating large-scale paired data in the medical field for volumetric modalities such as CT scans remains a challenging and time-intensive process. This difficulty often limits the performance on downstream tasks", "method": "incorporate uni-modal self-supervised learning into VLP framework; propose a novel language encoder, termed as TriBERT, for learning multi-level textual semantics; devise the hierarchical contrastive learning to capture multi-level vision-language correspondence", "result": "achieving state-of-the-art performance across a wide range of downstream tasks, including 3D segmentation, cross-modal retrieval, visual question answering, and report generation.", "conclusion": "The resulting encoders exhibit strong transferability, achieving state-of-the-art performance across a wide range of downstream tasks, including 3D segmentation, cross-modal retrieval, visual question answering, and report generation."}}
{"id": "2508.12782", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12782", "abs": "https://arxiv.org/abs/2508.12782", "authors": ["Petr Anokhin", "Roman Khalikov", "Stefan Rebrikov", "Viktor Volkov", "Artyom Sorokin", "Vincent Bissonnette"], "title": "HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds", "comment": "Code is available at https://github.com/stefanrer/HeroBench", "summary": "Large language models (LLMs) have shown remarkable capabilities in isolated\nstep-by-step reasoning tasks such as mathematics and programming, but their\nproficiency in long-horizon planning, where solutions require extended,\nstructured sequences of interdependent actions, remains underexplored. Existing\nbenchmarks typically assess LLMs through abstract or low-dimensional\nalgorithmic tasks, failing to capture the complexity of realistic planning\nenvironments. We introduce HeroBench, a novel benchmark designed specifically\nto evaluate long-horizon planning and structured reasoning within complex\nRPG-inspired virtual worlds. HeroBench provides a rigorously constructed\ndataset of tasks covering a wide range of difficulties, a simulated environment\nto execute and validate agent plans, and detailed analytical tools for\nevaluating model performance. Tasks challenge models to formulate strategic\nplans, efficiently gather resources, master necessary skills, craft equipment,\nand defeat adversaries, reflecting practical scenarios' layered dependencies\nand constraints. Our extensive evaluation of 25 state-of-the-art LLMs, spanning\nboth open-source and proprietary models, including the GPT-5 family, reveals\nsubstantial performance disparities rarely observed in conventional reasoning\nbenchmarks. Detailed error analysis further uncovers specific weaknesses in\ncurrent models' abilities to generate robust high-level plans and reliably\nexecute structured actions. HeroBench thus not only significantly advances the\nevaluation of LLM reasoning but also provides a flexible, scalable foundation\nfor future research into advanced, autonomous planning in virtual environments.", "AI": {"tldr": "HeroBench, a new benchmark for evaluating long-horizon planning in LLMs within complex virtual worlds, reveals performance disparities and weaknesses in current models.", "motivation": "Existing benchmarks typically assess LLMs through abstract or low-dimensional algorithmic tasks, failing to capture the complexity of realistic planning environments; LLMs' proficiency in long-horizon planning remains underexplored.", "method": "Introduce HeroBench, a novel benchmark designed specifically to evaluate long-horizon planning and structured reasoning within complex RPG-inspired virtual worlds.", "result": "Extensive evaluation of 25 state-of-the-art LLMs reveals substantial performance disparities rarely observed in conventional reasoning benchmarks; Detailed error analysis further uncovers specific weaknesses in current models' abilities to generate robust high-level plans and reliably execute structured actions.", "conclusion": "HeroBench significantly advances the evaluation of LLM reasoning and provides a flexible, scalable foundation for future research into advanced, autonomous planning in virtual environments."}}
{"id": "2508.12244", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12244", "abs": "https://arxiv.org/abs/2508.12244", "authors": ["Fan Li", "Xiaoyang Wang", "Wenjie Zhang", "Ying Zhang", "Xuemin Lin"], "title": "DHG-Bench: A Comprehensive Benchmark on Deep Hypergraph Learning", "comment": "22 pages, 5 figures", "summary": "Although conventional deep graph models have achieved great success in\nrelational learning, their focus on pairwise relationships limits their\ncapacity to learn pervasive higher-order interactions in real-world complex\nsystems, which can be naturally modeled as hypergraphs. To tackle this,\nhypergraph neural networks (HNNs), the dominant approach in deep hypergraph\nlearning (DHGL), has garnered substantial attention in recent years. Despite\nthe proposal of numerous HNN methods, there is no comprehensive benchmark for\nHNNs, which creates a great obstacle to understanding the progress of DHGL in\nseveral aspects: (i) insufficient coverage of datasets, algorithms, and tasks;\n(ii) a narrow evaluation of algorithm performance; and (iii) inconsistent\ndataset usage, preprocessing, and experimental setups that hinder\ncomparability. To fill the gap, we introduce DHG-Bench, the first comprehensive\nbenchmark for DHGL. Specifically, DHG-Bench integrates 20 diverse datasets\nspanning node-, edge-, and graph-level tasks, along with 16 state-of-the-art\nHNN algorithms, under consistent data processing and experimental protocols.\nOur benchmark systematically investigates the characteristics of HNNs in terms\nof four dimensions: effectiveness, efficiency, robustness, and fairness.\nFurther, to facilitate reproducible research, we have developed an easy-to-use\nlibrary for training and evaluating different HNN methods. Extensive\nexperiments conducted with DHG-Bench reveal both the strengths and inherent\nlimitations of existing algorithms, offering valuable insights and directions\nfor future research. The code is publicly available at:\nhttps://github.com/Coco-Hut/DHG-Bench.", "AI": {"tldr": "DHG-Bench is introduced as the first comprehensive benchmark for deep hypergraph learning, addressing limitations in existing HNN evaluation and providing insights for future research.", "motivation": "Conventional deep graph models are limited in learning higher-order interactions in real-world systems, and there is no comprehensive benchmark for HNNs.", "method": "Introduction of DHG-Bench, a comprehensive benchmark for DHGL, integrating diverse datasets, HNN algorithms, consistent data processing, and experimental protocols.", "result": "Systematic investigation of HNN characteristics in terms of effectiveness, efficiency, robustness, and fairness. Development of an easy-to-use library for training and evaluation.", "conclusion": "DHG-Bench reveals strengths and limitations of existing HNN algorithms, offering insights for future research."}}
{"id": "2508.12458", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12458", "abs": "https://arxiv.org/abs/2508.12458", "authors": ["Ruirui Gao", "Emily Johnson", "Bowen Tan", "Yanfei Qian"], "title": "M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following", "comment": null, "summary": "Large Vision-Language Models (LVLMs) hold immense potential for complex\nmultimodal instruction following, yet their development is often hindered by\nthe high cost and inconsistency of human annotation required for effective\nfine-tuning and preference alignment. Traditional supervised fine-tuning (SFT)\nand existing preference optimization methods like RLHF and DPO frequently\nstruggle to efficiently leverage the model's own generation space to identify\nhighly informative \"hard negative\" samples. To address these challenges, we\npropose Multimodal-Model-Guided Preference Optimization (M3PO), a novel and\ndata-efficient method designed to enhance LVLMs' capabilities in visual\ninstruction following. M3PO intelligently selects the most \"learning-valuable\"\npreference sample pairs from a diverse pool of LVLM-generated candidates. This\nselection is driven by a sophisticated mechanism that integrates two crucial\nsignals: a Multimodal Alignment Score (MAS) to assess external quality and the\nmodel's Self-Consistency / Confidence (log-probability) to gauge internal\nbelief. These are combined into a novel M3P-Score, which specifically\nidentifies preferred responses and challenging dispreferred responses that the\nmodel might confidently generate despite being incorrect. These high-quality\npreference pairs are then used for efficient Direct Preference Optimization\n(DPO) fine-tuning on base LVLMs like LLaVA-1.5 (7B/13B) using LoRA. Our\nextensive experiments demonstrate that M3PO consistently outperforms strong\nbaselines, including SFT, simulated RLHF, vanilla DPO, and RM-DPO, across a\ncomprehensive suite of multimodal instruction following benchmarks (MME-Bench,\nPOPE, IFT, Human Pref. Score).", "AI": {"tldr": "M3PO \u662f\u4e00\u79cd\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u5bf9\u9f50\u5206\u6570\u548c\u6a21\u578b\u7f6e\u4fe1\u5ea6\u6765\u9009\u62e9\u9ad8\u8d28\u91cf\u504f\u597d\u6837\u672c\u5bf9\uff0c\u4ece\u800c\u589e\u5f3a LVLM \u5728\u89c6\u89c9\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5229\u7528\u6a21\u578b\u81ea\u8eab\u7684\u751f\u6210\u7a7a\u95f4\u6765\u8bc6\u522b\u4fe1\u606f\u91cf\u5927\u7684\u201c\u786c\u8d1f\u4f8b\u201d\u6837\u672c\u3002", "method": "Multimodal-Model-Guided Preference Optimization (M3PO)", "result": "M3PO \u662f\u4e00\u79cd\u65b0\u9896\u4e14\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u589e\u5f3a LVLM \u5728\u89c6\u89c9\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "M3PO\u5728\u591a\u6a21\u6001\u6307\u4ee4\u8ddf\u968f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\uff0c\u5305\u62ec SFT\u3001\u6a21\u62df RLHF\u3001vanilla DPO \u548c RM-DPO\u3002"}}
{"id": "2508.12109", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12109", "abs": "https://arxiv.org/abs/2508.12109", "authors": ["Ye Wang", "Qianglong Chen", "Zejun Li", "Siyuan Wang", "Shijie Guo", "Zhirui Zhang", "Zhongyu Wei"], "title": "Simple o3: Towards Interleaved Vision-Language Reasoning", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have shown impressive performance on\nvision-language tasks, but their long Chain-of-Thought (CoT) capabilities in\nmultimodal scenarios remain underexplored. Inspired by OpenAI's o3 model, which\nemulates human-like ''thinking with image'' through iterative visual\ntransformations and linguistic reasoning, we propose Simple o3, an end-to-end\nframework that integrates dynamic tool interactions (e.g., cropping, zooming,\nand reusing) into interleaved vision-language reasoning via supervised\nfine-tuning (SFT). Our approach features a scalable data synthesis pipeline\nthat generates high-quality interleaved vision-language reasoning chains via an\n''observe-reason-act'' cycle, complete with executable visual operations and\nrigorous verification, yielding the open-source TWI-Tools-146K dataset.\nExperimental results demonstrate Simple o3's superior performance on diverse\nbenchmarks, outperforming existing approaches. By combining enhanced reasoning\ncapabilities, Simple o3 establishes a powerful yet computationally affordable\nparadigm for advancing multimodal reasoning. Remarkably, we provide the first\nin-depth analysis of different interleaved reasoning strategies, offering\ninsights into their impact on model performance. We found that by introducing\nadditional visual tokens for interleaved vision-language reasoning, reusing and\nmagnifying the original image significantly improves the model's visual\nreasoning and fine-grained perception, while image cropping based on precise\nvisual grounding allows the model to effectively focus on key entities or\nregions, further enhancing its capabilities.", "AI": {"tldr": "Simple o3\u901a\u8fc7\u96c6\u6210\u52a8\u6001\u5de5\u5177\u4ea4\u4e92\uff0c\u589e\u5f3a\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u5728\u591a\u6a21\u6001\u573a\u666f\u4e2d\u7684\u957f\u94fe\u601d\u7ef4 (CoT) \u80fd\u529b\u4ecd\u6709\u5f85\u63a2\u7d22\u3002", "method": "Simple o3\uff0c\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u76d1\u7763\u5fae\u8c03 (SFT) \u5c06\u52a8\u6001\u5de5\u5177\u4ea4\u4e92\uff08\u4f8b\u5982\uff0c\u88c1\u526a\u3001\u7f29\u653e\u548c\u91cd\u7528\uff09\u96c6\u6210\u5230\u4ea4\u9519\u7684\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u4e2d\u3002", "result": "\u901a\u8fc7\u5f15\u5165\u989d\u5916\u7684\u89c6\u89c9token\u8fdb\u884c\u4ea4\u9519\u7684\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\uff0c\u91cd\u7528\u548c\u653e\u5927\u539f\u59cb\u56fe\u50cf\u663e\u7740\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u89c6\u89c9\u63a8\u7406\u548c\u7ec6\u7c92\u5ea6\u611f\u77e5\uff0c\u800c\u57fa\u4e8e\u7cbe\u786e\u89c6\u89c9\u57fa\u7840\u7684\u56fe\u50cf\u88c1\u526a\u5141\u8bb8\u6a21\u578b\u6709\u6548\u5730\u5173\u6ce8\u5173\u952e\u5b9e\u4f53\u6216\u533a\u57df\uff0c\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u5176\u80fd\u529b\u3002", "conclusion": "Simple o3\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002\u901a\u8fc7\u7ed3\u5408\u589e\u5f3a\u7684\u63a8\u7406\u80fd\u529b\uff0cSimple o3 \u5efa\u7acb\u4e86\u4e00\u4e2a\u5f3a\u5927\u4e14\u8ba1\u7b97\u4e0a\u53ef\u8d1f\u62c5\u7684\u8303\u4f8b\uff0c\u7528\u4e8e\u63a8\u8fdb\u591a\u6a21\u6001\u63a8\u7406\u3002"}}
{"id": "2508.12790", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12790", "abs": "https://arxiv.org/abs/2508.12790", "authors": ["Zenan Huang", "Yihong Zhuang", "Guoshan Lu", "Zeyu Qin", "Haokai Xu", "Tianyu Zhao", "Ru Peng", "Jiaqi Hu", "Zhanming Shen", "Xiaomeng Hu", "Xijun Gu", "Peiyi Tu", "Jiaxin Liu", "Wenyu Chen", "Yuzhuo Fu", "Zhiting Fan", "Yanmei Gu", "Yuanyuan Wang", "Zhengkai Yang", "Jianguo Li", "Junbo Zhao"], "title": "Reinforcement Learning with Rubric Anchors", "comment": "technical report", "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm for enhancing Large Language Models (LLMs), exemplified by\nthe success of OpenAI's o-series. In RLVR, rewards are derived from verifiable\nsignals-such as passing unit tests in code generation or matching correct\nanswers in mathematical reasoning. While effective, this requirement largely\nconfines RLVR to domains with automatically checkable outcomes. To overcome\nthis, we extend the RLVR paradigm to open-ended tasks by integrating\nrubric-based rewards, where carefully designed rubrics serve as structured,\nmodel-interpretable criteria for automatic scoring of subjective outputs. We\nconstruct, to our knowledge, the largest rubric reward system to date, with\nover 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration.\nImplementing rubric-based RL is challenging; we tackle these issues with a\nclear framework and present an open-sourced Qwen-30B-A3B model with notable\ngains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended\nbenchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by\n+2.4%, while preserving general and reasoning abilities. 2) Our method provides\nfine-grained stylistic control, using rubrics as anchors to mitigate the\n\"AI-like\" tone and produce more human-like, expressive responses. We share key\nlessons in rubric construction, data selection, and training, and discuss\nlimitations and future releases.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8erubric\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3aLLM\u5728\u5f00\u653e\u5f0f\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u578brubric\u5956\u52b1\u7cfb\u7edf\u5e76\u5728Qwen-30B-A3B\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u5e76\u5728\u98ce\u683c\u63a7\u5236\u65b9\u9762\u6709\u6240\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u4ece\u53ef\u9a8c\u8bc1\u5956\u52b1\u4e2d\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u5177\u6709\u81ea\u52a8\u53ef\u68c0\u67e5\u7ed3\u679c\u7684\u9886\u57df\uff0c\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u5c06RLVR\u6269\u5c55\u5230\u5f00\u653e\u5f0f\u4efb\u52a1\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8erubric\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5f00\u653e\u5f0f\u4efb\u52a1\uff0c\u901a\u8fc7\u6574\u5408rubric\u5956\u52b1\uff0c\u5229\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684rubric\u4f5c\u4e3a\u7ed3\u6784\u5316\u7684\u3001\u6a21\u578b\u53ef\u89e3\u91ca\u7684\u6807\u51c6\uff0c\u5bf9\u4e3b\u89c2\u8f93\u51fa\u8fdb\u884c\u81ea\u52a8\u8bc4\u5206\u3002", "result": "\u4ec5\u4f7f\u75285K+\u6837\u672c\uff0c\u8be5\u7cfb\u7edf\u5728\u5f00\u653e\u5f0fbenchmark\u4e0a\u63d0\u9ad8\u4e86+5.2%\uff0c\u80dc\u8fc7671B DeepSeek-V3\u6a21\u578b+2.4%\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u4e00\u822c\u7684\u63a8\u7406\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u8fd8\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u7684\u98ce\u683c\u63a7\u5236\uff0c\u53ef\u4ee5\u4f7f\u7528rubric\u4f5c\u4e3aanchor\u6765\u51cf\u8f7b\u201cAI-like\u201d\u7684\u8bed\u6c14\uff0c\u5e76\u4ea7\u751f\u66f4\u50cf\u4eba\u7c7b\u7684\u3001\u66f4\u5bcc\u6709\u8868\u73b0\u529b\u7684\u53cd\u5e94\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u4eba\u5de5\u6216LLM\u6784\u5efa\u7684\u8d85\u8fc710,000\u4e2arubric\u7684\u5927\u578brubric\u5956\u52b1\u7cfb\u7edf\uff0c\u5e76\u5f00\u6e90\u4e86\u4e00\u4e2aQwen-30B-A3B\u6a21\u578b\uff0c\u5728\u5f00\u653e\u5f0f\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u679c\uff0c\u5e76\u5728\u4eba\u6587\u9886\u57df\u8868\u73b0\u7a81\u51fa\uff0c\u80dc\u8fc7DeepSeek-V3\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u901a\u7528\u548c\u63a8\u7406\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u8fd8\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u7684\u98ce\u683c\u63a7\u5236\uff0c\u5e76\u5206\u4eab\u4e86rubric\u6784\u5efa\u3001\u6570\u636e\u9009\u62e9\u548c\u8bad\u7ec3\u7684\u5173\u952e\u7ecf\u9a8c\u3002"}}
{"id": "2508.12247", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12247", "abs": "https://arxiv.org/abs/2508.12247", "authors": ["Haolong Chen", "Liang Zhang", "Zhengyuan Xin", "Guangxu Zhu"], "title": "STM3: Mixture of Multiscale Mamba for Long-Term Spatio-Temporal Time-Series Prediction", "comment": null, "summary": "Recently, spatio-temporal time-series prediction has developed rapidly, yet\nexisting deep learning methods struggle with learning complex long-term\nspatio-temporal dependencies efficiently. The long-term spatio-temporal\ndependency learning brings two new challenges: 1) The long-term temporal\nsequence includes multiscale information naturally which is hard to extract\nefficiently; 2) The multiscale temporal information from different nodes is\nhighly correlated and hard to model. To address these challenges, we propose an\nefficient \\textit{\\textbf{S}patio-\\textbf{T}emporal \\textbf{M}ultiscale\n\\textbf{M}amba} (STM2) that includes a multiscale Mamba architecture to capture\nthe multiscale information efficiently and simultaneously, and an adaptive\ngraph causal convolution network to learn the complex multiscale\nspatio-temporal dependency. STM2 includes hierarchical information aggregation\nfor different-scale information that guarantees their distinguishability. To\ncapture diverse temporal dynamics across all spatial nodes more efficiently, we\nfurther propose an enhanced version termed\n\\textit{\\textbf{S}patio-\\textbf{T}emporal \\textbf{M}ixture of\n\\textbf{M}ultiscale \\textbf{M}amba} (STM3) that employs a special\nMixture-of-Experts architecture, including a more stable routing strategy and a\ncausal contrastive learning strategy to enhance the scale distinguishability.\nWe prove that STM3 has much better routing smoothness and guarantees the\npattern disentanglement for each expert successfully. Extensive experiments on\nreal-world benchmarks demonstrate STM2/STM3's superior performance, achieving\nstate-of-the-art results in long-term spatio-temporal time-series prediction.", "AI": {"tldr": "Proposes STM2 and STM3, which use multiscale Mamba architectures and adaptive graph causal convolution networks to efficiently capture complex long-term spatio-temporal dependencies, achieving state-of-the-art results.", "motivation": "Existing deep learning methods struggle with learning complex long-term spatio-temporal dependencies efficiently. The long-term spatio-temporal dependency learning brings two new challenges: 1) The long-term temporal sequence includes multiscale information naturally which is hard to extract efficiently; 2) The multiscale temporal information from different nodes is highly correlated and hard to model.", "method": "STM2 includes a multiscale Mamba architecture and an adaptive graph causal convolution network. STM3 employs a special Mixture-of-Experts architecture, including a more stable routing strategy and a causal contrastive learning strategy.", "result": "STM2/STM3's superior performance, achieving state-of-the-art results in long-term spatio-temporal time-series prediction on real-world benchmarks.", "conclusion": "STM2/STM3 achieves state-of-the-art results in long-term spatio-temporal time-series prediction."}}
{"id": "2508.12459", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12459", "abs": "https://arxiv.org/abs/2508.12459", "authors": ["Alham Fikri Aji", "Trevor Cohn"], "title": "LoraxBench: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages", "comment": null, "summary": "As one of the world's most populous countries, with 700 languages spoken,\nIndonesia is behind in terms of NLP progress. We introduce LoraxBench, a\nbenchmark that focuses on low-resource languages of Indonesia and covers 6\ndiverse tasks: reading comprehension, open-domain QA, language inference,\ncausal reasoning, translation, and cultural QA. Our dataset covers 20\nlanguages, with the addition of two formality registers for three languages. We\nevaluate a diverse set of multilingual and region-focused LLMs and found that\nthis benchmark is challenging. We note a visible discrepancy between\nperformance in Indonesian and other languages, especially the low-resource\nones. There is no clear lead when using a region-specific model as opposed to\nthe general multilingual model. Lastly, we show that a change in register\naffects model performance, especially with registers not commonly found in\nsocial media, such as high-level politeness `Krama' Javanese.", "AI": {"tldr": "Introduce LoraxBench, a benchmark that focuses on low-resource languages of Indonesia and covers 6 diverse tasks, and evaluate a diverse set of multilingual and region-focused LLMs and found that this benchmark is challenging.", "motivation": "Indonesia is behind in terms of NLP progress", "method": "Introduce LoraxBench, a benchmark that focuses on low-resource languages of Indonesia and covers 6 diverse tasks", "result": "Evaluate a diverse set of multilingual and region-focused LLMs and found that this benchmark is challenging", "conclusion": "There is a visible discrepancy between performance in Indonesian and other languages, especially the low-resource ones. There is no clear lead when using a region-specific model as opposed to the general multilingual model. Lastly, a change in register affects model performance, especially with registers not commonly found in social media."}}
{"id": "2508.12131", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12131", "abs": "https://arxiv.org/abs/2508.12131", "authors": ["Minh Tran", "Johnmark Clements", "Annie Prasanna", "Tri Nguyen", "Ngan Le"], "title": "DualFit: A Two-Stage Virtual Try-On via Warping and Synthesis", "comment": "Retail Vision, ICCV 2025", "summary": "Virtual Try-On technology has garnered significant attention for its\npotential to transform the online fashion retail experience by allowing users\nto visualize how garments would look on them without physical trials. While\nrecent advances in diffusion-based warping-free methods have improved\nperceptual quality, they often fail to preserve fine-grained garment details\nsuch as logos and printed text elements that are critical for brand integrity\nand customer trust. In this work, we propose DualFit, a hybrid VTON pipeline\nthat addresses this limitation by two-stage approach. In the first stage,\nDualFit warps the target garment to align with the person image using a learned\nflow field, ensuring high-fidelity preservation. In the second stage, a\nfidelity-preserving try-on module synthesizes the final output by blending the\nwarped garment with preserved human regions. Particularly, to guide this\nprocess, we introduce a preserved-region input and an inpainting mask, enabling\nthe model to retain key areas and regenerate only where necessary, particularly\naround garment seams. Extensive qualitative results show that DualFit achieves\nvisually seamless try-on results while faithfully maintaining high-frequency\ngarment details, striking an effective balance between reconstruction accuracy\nand perceptual realism.", "AI": {"tldr": "DualFit, a two-stage hybrid VTON pipeline, warps garments and synthesizes the final output by blending the warped garment with preserved human regions, achieving visually seamless try-on results while faithfully maintaining high-frequency garment details.", "motivation": "Recent advances in diffusion-based warping-free methods have improved perceptual quality, they often fail to preserve fine-grained garment details such as logos and printed text elements that are critical for brand integrity and customer trust.", "method": "DualFit, a hybrid VTON pipeline that addresses this limitation by two-stage approach. In the first stage, DualFit warps the target garment to align with the person image using a learned flow field. In the second stage, a fidelity-preserving try-on module synthesizes the final output by blending the warped garment with preserved human regions. Particularly, to guide this process, we introduce a preserved-region input and an inpainting mask", "result": "achieves visually seamless try-on results while faithfully maintaining high-frequency garment details", "conclusion": "DualFit achieves visually seamless try-on results while faithfully maintaining high-frequency garment details, striking an effective balance between reconstruction accuracy and perceptual realism."}}
{"id": "2508.12791", "categories": ["cs.AI", "cs.MA", "cs.SY", "eess.SY", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2508.12791", "abs": "https://arxiv.org/abs/2508.12791", "authors": ["Imran Khan"], "title": "[Social] Allostasis: Or, How I Learned To Stop Worrying and Love The Noise", "comment": "20 pages, 5 figures. Accepted at ALIFE 2025 (Kyoto, Japan; October\n  6th - 10th 2025)", "summary": "The notion of homeostasis typically conceptualises biological and artificial\nsystems as maintaining stability by resisting deviations caused by\nenvironmental and social perturbations. In contrast, (social) allostasis\nproposes that these systems can proactively leverage these very perturbations\nto reconfigure their regulatory parameters in anticipation of environmental\ndemands, aligning with von Foerster's ``order through noise'' principle. This\npaper formulates a computational model of allostatic and social allostatic\nregulation that employs biophysiologically inspired signal transducers,\nanalogous to hormones like cortisol and oxytocin, to encode information from\nboth the environment and social interactions, which mediate this dynamic\nreconfiguration. The models are tested in a small society of ``animats'' across\nseveral dynamic environments, using an agent-based model. The results show that\nallostatic and social allostatic regulation enable agents to leverage\nenvironmental and social ``noise'' for adaptive reconfiguration, leading to\nimproved viability compared to purely reactive homeostatic agents. This work\noffers a novel computational perspective on the principles of social allostasis\nand their potential for designing more robust, bio-inspired, adaptive systems", "AI": {"tldr": "This paper presents a computational model of allostasis and social allostasis, demonstrating that agents can use environmental and social noise for adaptive reconfiguration, improving viability compared to homeostatic agents.", "motivation": "biological and artificial systems can proactively leverage perturbations to reconfigure their regulatory parameters in anticipation of environmental demands", "method": "a computational model of allostatic and social allostatic regulation that employs biophysiologically inspired signal transducers, analogous to hormones like cortisol and oxytocin, to encode information from both the environment and social interactions, which mediate this dynamic reconfiguration. The models are tested in a small society of animats across several dynamic environments, using an agent-based model", "result": "allostatic and social allostatic regulation enable agents to leverage environmental and social noise for adaptive reconfiguration", "conclusion": "allostatic and social allostatic regulation enable agents to leverage environmental and social noise for adaptive reconfiguration, leading to improved viability compared to purely reactive homeostatic agents"}}
{"id": "2508.12253", "categories": ["cs.LG", "cs.AI", "stat.ME"], "pdf": "https://arxiv.org/pdf/2508.12253", "abs": "https://arxiv.org/abs/2508.12253", "authors": ["Manish Shukla"], "title": "Interpreting Time Series Forecasts with LIME and SHAP: A Case Study on the Air Passengers Dataset", "comment": null, "summary": "Time-series forecasting underpins critical decisions across aviation, energy,\nretail and health. Classical autoregressive integrated moving average (ARIMA)\nmodels offer interpretability via coefficients but struggle with\nnonlinearities, whereas tree-based machine-learning models such as XGBoost\ndeliver high accuracy but are often opaque. This paper presents a unified\nframework for interpreting time-series forecasts using local interpretable\nmodel-agnostic explanations (LIME) and SHapley additive exPlanations (SHAP). We\nconvert a univariate series into a leakage-free supervised learning problem,\ntrain a gradient-boosted tree alongside an ARIMA baseline and apply post-hoc\nexplainability. Using the Air Passengers dataset as a case study, we show that\na small set of lagged features -- particularly the twelve-month lag -- and\nseasonal encodings explain most forecast variance. We contribute: (i) a\nmethodology for applying LIME and SHAP to time series without violating\nchronology; (ii) theoretical exposition of the underlying algorithms; (iii)\nempirical evaluation with extensive analysis; and (iv) guidelines for\npractitioners.", "AI": {"tldr": "Interpreting time-series forecasts using LIME and SHAP", "motivation": "Classical ARIMA models offer interpretability via coefficients but struggle with nonlinearities, whereas tree-based machine-learning models such as XGBoost deliver high accuracy but are often opaque", "method": "unified framework for interpreting time-series forecasts using LIME and SHAP. convert a univariate series into a leakage-free supervised learning problem,train a gradient-boosted tree alongside an ARIMA baseline and apply post-hoc explainability", "result": "apply LIME and SHAP to time series without violating chronology, theoretical exposition of the underlying algorithms, empirical evaluation with extensive analysis, and guidelines for practitioners", "conclusion": "lagged features and seasonal encodings explain most forecast variance"}}
{"id": "2508.12461", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12461", "abs": "https://arxiv.org/abs/2508.12461", "authors": ["Ziqian Bi", "Keyu Chen", "Chiung-Yi Tseng", "Danyang Zhang", "Tianyang Wang", "Hongying Luo", "Lu Chen", "Junming Huang", "Jibin Guan", "Junfeng Hao", "Junhao Song"], "title": "Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models", "comment": null, "summary": "In August 2025, OpenAI released GPT-OSS models, its first open weight large\nlanguage models since GPT-2 in 2019, comprising two mixture of experts\narchitectures with 120B and 20B parameters. We evaluated both variants against\nsix contemporary open source large language models ranging from 14.7B to 235B\nparameters, representing both dense and sparse designs, across ten benchmarks\ncovering general knowledge, mathematical reasoning, code generation,\nmultilingual understanding, and conversational ability. All models were tested\nin unquantised form under standardised inference settings, with statistical\nvalidation using McNemars test and effect size analysis. Results show that\ngpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such\nas HumanEval and MMLU, despite requiring substantially less memory and energy\nper response. Both models demonstrate mid-tier overall performance within the\ncurrent open source landscape, with relative strength in code generation and\nnotable weaknesses in multilingual tasks. These findings provide empirical\nevidence that scaling in sparse architectures may not yield proportional\nperformance gains, underscoring the need for further investigation into\noptimisation strategies and informing more efficient model selection for future\nopen source deployments.", "AI": {"tldr": "GPT-OSS-20B \u4f18\u4e8e GPT-OSS-120B\uff0c\u4f46\u4e24\u8005\u5728\u5f00\u6e90 LLM \u4e2d\u90fd\u5904\u4e8e\u4e2d\u7b49\u6c34\u5e73\uff0c\u4ee3\u7801\u751f\u6210\u662f\u4f18\u52bf\uff0c\u591a\u8bed\u8a00\u662f\u5f31\u52bf\u3002", "motivation": "OpenAI \u53d1\u5e03\u4e86 GPT-OSS \u6a21\u578b\uff0c\u8fd9\u662f\u81ea 2019 \u5e74 GPT-2 \u4ee5\u6765\u9996\u4e2a\u5f00\u6e90\u6743\u91cd\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u62ec\u4e24\u79cd\u5177\u6709 120B \u548c 20B \u53c2\u6570\u7684\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u3002", "method": "\u5728\u6db5\u76d6\u4e00\u822c\u77e5\u8bc6\u3001\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u3001\u591a\u8bed\u8a00\u7406\u89e3\u548c\u5bf9\u8bdd\u80fd\u529b\u7684\u5341\u4e2a\u57fa\u51c6\u4e0a\uff0c\u9488\u5bf9\u516d\u4e2a\u5f53\u4ee3\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u53c2\u6570\u8303\u56f4\u4ece 147 \u4ebf\u5230 2350 \u4ebf\uff09\u8bc4\u4f30\u4e86\u4e24\u79cd\u53d8\u4f53\u3002\u6240\u6709\u6a21\u578b\u5747\u5728\u6807\u51c6\u5316\u63a8\u7406\u8bbe\u7f6e\u4e0b\u7684\u672a\u91cf\u5316\u5f62\u5f0f\u4e0b\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5e76\u4f7f\u7528 McNemars \u68c0\u9a8c\u548c\u6548\u5e94\u5927\u5c0f\u5206\u6790\u8fdb\u884c\u4e86\u7edf\u8ba1\u9a8c\u8bc1\u3002", "result": "gpt-oss-20B \u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e gpt-oss-120B\uff0c\u5c3d\u7ba1\u6bcf\u6b21\u54cd\u5e94\u6240\u9700\u7684\u5185\u5b58\u548c\u80fd\u6e90\u5927\u5927\u51cf\u5c11\u3002 \u8fd9\u4e24\u4e2a\u6a21\u578b\u5728\u5f53\u524d\u5f00\u6e90\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4e2d\u7b49\u7684\u6574\u4f53\u6027\u80fd\uff0c\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u5177\u6709\u76f8\u5bf9\u4f18\u52bf\uff0c\u5728\u591a\u8bed\u8a00\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u660e\u663e\u7684\u52a3\u52bf\u3002", "conclusion": "\u7a00\u758f\u67b6\u6784\u4e2d\u7684\u6269\u5c55\u53ef\u80fd\u4e0d\u4f1a\u4ea7\u751f\u6210\u6bd4\u4f8b\u7684\u6027\u80fd\u63d0\u5347\uff0c\u56e0\u6b64\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u5f00\u6e90\u90e8\u7f72\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u6a21\u578b\u9009\u62e9\u3002"}}
{"id": "2508.12132", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12132", "abs": "https://arxiv.org/abs/2508.12132", "authors": ["Amira Guesmi", "Bassem Ouni", "Muhammad Shafique"], "title": "TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks", "comment": null, "summary": "Quantized Neural Networks (QNNs) are increasingly deployed in edge and\nresource-constrained environments due to their efficiency in computation and\nmemory usage. While shown to distort the gradient landscape and weaken\nconventional pixel-level attacks, it provides limited robustness against\npatch-based adversarial attacks-localized, high-saliency perturbations that\nremain surprisingly transferable across bit-widths. Existing defenses either\noverfit to fixed quantization settings or fail to address this cross-bit\ngeneralization vulnerability. We introduce \\textbf{TriQDef}, a tri-level\nquantization-aware defense framework designed to disrupt the transferability of\npatch-based adversarial attacks across QNNs. TriQDef consists of: (1) a Feature\nDisalignment Penalty (FDP) that enforces semantic inconsistency by penalizing\nperceptual similarity in intermediate representations; (2) a Gradient\nPerceptual Dissonance Penalty (GPDP) that explicitly misaligns input gradients\nacross bit-widths by minimizing structural and directional agreement via Edge\nIoU and HOG Cosine metrics; and (3) a Joint Quantization-Aware Training\nProtocol that unifies these penalties within a shared-weight training scheme\nacross multiple quantization levels. Extensive experiments on CIFAR-10 and\nImageNet demonstrate that TriQDef reduces Attack Success Rates (ASR) by over\n40\\% on unseen patch and quantization combinations, while preserving high clean\naccuracy. Our findings underscore the importance of disrupting both semantic\nand perceptual gradient alignment to mitigate patch transferability in QNNs.", "AI": {"tldr": "TriQDef \u662f\u4e00\u79cd\u4e09\u7ea7\u91cf\u5316\u611f\u77e5\u9632\u5fa1\u6846\u67b6\uff0c\u65e8\u5728\u7834\u574f\u57fa\u4e8e patch \u7684\u5bf9\u6297\u6027\u653b\u51fb\u5728 QNN \u4e0a\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u901a\u8fc7\u6270\u4e71\u8bed\u4e49\u548c\u611f\u77e5\u68af\u5ea6\u5bf9\u9f50\u6765\u5b9e\u73b0\u3002", "motivation": "\u91cf\u5316\u795e\u7ecf\u7f51\u7edc (QNN) \u7531\u4e8e\u5176\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u4f7f\u7528\u65b9\u9762\u7684\u6548\u7387\u800c\u88ab\u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u5728\u8fb9\u7f18\u548c\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u3002\u5c3d\u7ba1\u5df2\u663e\u793a\u51fa\u626d\u66f2\u68af\u5ea6 landscape \u5e76\u524a\u5f31\u4f20\u7edf\u7684\u50cf\u7d20\u7ea7\u653b\u51fb\uff0c\u4f46\u5b83\u5bf9\u57fa\u4e8e patch \u7684\u5bf9\u6297\u6027\u653b\u51fb\u7684\u9c81\u68d2\u6027\u6709\u9650\uff0c\u6b64\u7c7b\u653b\u51fb\u5177\u6709 localized\u3001\u9ad8\u663e\u8457\u6027\u6270\u52a8\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u4f4d\u5bbd\u4e4b\u95f4\u4ecd\u7136\u5177\u6709 surprising \u7684\u53ef\u8fc1\u79fb\u6027\u3002\u73b0\u6709\u7684\u9632\u5fa1\u8981\u4e48\u8fc7\u62df\u5408\u5230\u56fa\u5b9a\u7684\u91cf\u5316\u8bbe\u7f6e\uff0c\u8981\u4e48\u65e0\u6cd5\u89e3\u51b3\u8fd9\u79cd\u8de8\u4f4d\u6cdb\u5316\u6f0f\u6d1e\u3002", "method": "TriQDef\uff0c\u4e00\u4e2a\u4e09\u7ea7\u91cf\u5316\u611f\u77e5\u9632\u5fa1\u6846\u67b6\uff0c\u5305\u542b\u7279\u5f81\u5931\u8c03\u60e9\u7f5a (FDP)\u3001\u68af\u5ea6\u611f\u77e5\u4e0d\u534f\u8c03\u60e9\u7f5a (GPDP) \u548c\u8054\u5408\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u534f\u8bae\u3002", "result": "TriQDef \u5728\u672a\u89c1\u8fc7\u7684 patch \u548c\u91cf\u5316\u7ec4\u5408\u4e0a\u5c06\u653b\u51fb\u6210\u529f\u7387 (ASR) \u964d\u4f4e\u4e86 40% \u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684 clean \u51c6\u786e\u7387\u3002", "conclusion": "TriQDef \u901a\u8fc7\u6270\u4e71\u8bed\u4e49\u548c\u611f\u77e5\u68af\u5ea6\u5bf9\u9f50\u6765\u7f13\u89e3 QNN \u4e2d\u7684 patch \u53ef\u8fc1\u79fb\u6027\uff0c\u5e76\u5728 CIFAR-10 \u548c ImageNet \u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc7 40% \u7684\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e\u3002"}}
{"id": "2508.12840", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.12840", "abs": "https://arxiv.org/abs/2508.12840", "authors": ["Giovanni Briglia", "Francesco Fabiano", "Stefano Mariani"], "title": "Scaling Multi-Agent Epistemic Planning through GNN-Derived Heuristics", "comment": null, "summary": "Multi-agent Epistemic Planning (MEP) is an autonomous planning framework for\nreasoning about both the physical world and the beliefs of agents, with\napplications in domains where information flow and awareness among agents are\ncritical. The richness of MEP requires states to be represented as Kripke\nstructures, i.e., directed labeled graphs. This representation limits the\napplicability of existing heuristics, hindering the scalability of epistemic\nsolvers, which must explore an exponential search space without guidance,\nresulting often in intractability. To address this, we exploit Graph Neural\nNetworks (GNNs) to learn patterns and relational structures within epistemic\nstates, to guide the planning process. GNNs, which naturally capture the\ngraph-like nature of Kripke models, allow us to derive meaningful estimates of\nstate quality -- e.g., the distance from the nearest goal -- by generalizing\nknowledge obtained from previously solved planning instances. We integrate\nthese predictive heuristics into an epistemic planning pipeline and evaluate\nthem against standard baselines, showing significant improvements in the\nscalability of multi-agent epistemic planning.", "AI": {"tldr": "This paper uses Graph Neural Networks to improve the scalability of multi-agent epistemic planning.", "motivation": "The richness of MEP requires states to be represented as Kripke structures, i.e., directed labeled graphs. This representation limits the applicability of existing heuristics, hindering the scalability of epistemic solvers, which must explore an exponential search space without guidance, resulting often in intractability.", "method": "This paper exploits Graph Neural Networks (GNNs) to learn patterns and relational structures within epistemic states, to guide the planning process.", "result": "The evaluation against standard baselines shows significant improvements in the scalability of multi-agent epistemic planning.", "conclusion": "This paper integrates predictive heuristics into an epistemic planning pipeline and demonstrates significant improvements in the scalability of multi-agent epistemic planning."}}
{"id": "2508.12270", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12270", "abs": "https://arxiv.org/abs/2508.12270", "authors": ["Gal Lifshitz", "Shahar Zuler", "Ori Fouks", "Dan Raviv"], "title": "L-SR1: Learned Symmetric-Rank-One Preconditioning", "comment": "Under review", "summary": "End-to-end deep learning has achieved impressive results but remains limited\nby its reliance on large labeled datasets, poor generalization to unseen\nscenarios, and growing computational demands. In contrast, classical\noptimization methods are data-efficient and lightweight but often suffer from\nslow convergence. While learned optimizers offer a promising fusion of both\nworlds, most focus on first-order methods, leaving learned second-order\napproaches largely unexplored.\n  We propose a novel learned second-order optimizer that introduces a trainable\npreconditioning unit to enhance the classical Symmetric-Rank-One (SR1)\nalgorithm. This unit generates data-driven vectors used to construct positive\nsemi-definite rank-one matrices, aligned with the secant constraint via a\nlearned projection. Our method is evaluated through analytic experiments and on\nthe real-world task of Monocular Human Mesh Recovery (HMR), where it\noutperforms existing learned optimization-based approaches. Featuring a\nlightweight model and requiring no annotated data or fine-tuning, our approach\noffers strong generalization and is well-suited for integration into broader\noptimization-based frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u4e8c\u9636\u4f18\u5316\u5668\uff0c\u5728\u5355\u76ee\u4eba\u4f53\u7f51\u683c\u6062\u590d (HMR) \u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u53d6\u5f97\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u6210\u679c\uff0c\u4f46\u4ecd\u53d7\u5230\u5176\u5bf9\u5927\u578b\u6807\u8bb0\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u6027\u3001\u5bf9\u672a\u89c1\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u5dee\u4ee5\u53ca\u4e0d\u65ad\u589e\u957f\u7684\u8ba1\u7b97\u9700\u6c42\u7684\u9650\u5236\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u7ecf\u5178\u7684\u4f18\u5316\u65b9\u6cd5\u662f\u6570\u636e\u9ad8\u6548\u4e14\u8f7b\u91cf\u7ea7\u7684\uff0c\u4f46\u901a\u5e38\u5b58\u5728\u6536\u655b\u901f\u5ea6\u6162\u7684\u95ee\u9898\u3002\u867d\u7136\u5b66\u4e60\u4f18\u5316\u5668\u63d0\u4f9b\u4e86\u4e24\u4e2a\u4e16\u754c\u7684\u6709\u5e0c\u671b\u7684\u878d\u5408\uff0c\u4f46\u5927\u591a\u6570\u90fd\u96c6\u4e2d\u5728\u4e00\u9636\u65b9\u6cd5\u4e0a\uff0c\u800c\u5b66\u4e60\u4e8c\u9636\u65b9\u6cd5\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u4e8c\u9636\u4f18\u5316\u5668\uff0c\u8be5\u4f18\u5316\u5668\u5f15\u5165\u4e86\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u9884\u5904\u7406\u5355\u5143\uff0c\u4ee5\u589e\u5f3a\u7ecf\u5178\u7684\u5bf9\u79f0\u79e9 1 (SR1) \u7b97\u6cd5\u3002\u8be5\u5355\u5143\u751f\u6210\u6570\u636e\u9a71\u52a8\u7684\u5411\u91cf\uff0c\u7528\u4e8e\u6784\u5efa\u6b63\u534a\u5b9a\u79e9 1 \u77e9\u9635\uff0c\u5e76\u901a\u8fc7\u5b66\u4e60\u6295\u5f71\u4e0e\u5272\u7ebf\u7ea6\u675f\u5bf9\u9f50\u3002", "result": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u4f18\u5316\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u4e0d\u9700\u8981\u5e26\u6ce8\u91ca\u7684\u6570\u636e\u6216\u5fae\u8c03\uff0c\u5177\u6709\u5f88\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u975e\u5e38\u9002\u5408\u96c6\u6210\u5230\u66f4\u5e7f\u6cdb\u7684\u57fa\u4e8e\u4f18\u5316\u7684\u6846\u67b6\u4e2d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5355\u76ee\u4eba\u4f53\u7f51\u683c\u6062\u590d (HMR) \u8fd9\u4e00\u771f\u5b9e\u4e16\u754c\u7684\u4efb\u52a1\u4e2d\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u4f18\u5316\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u4e0d\u9700\u8981\u5e26\u6ce8\u91ca\u7684\u6570\u636e\u6216\u5fae\u8c03\uff0c\u5177\u6709\u5f88\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u975e\u5e38\u9002\u5408\u96c6\u6210\u5230\u66f4\u5e7f\u6cdb\u7684\u57fa\u4e8e\u4f18\u5316\u7684\u6846\u67b6\u4e2d\u3002"}}
{"id": "2508.12482", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12482", "abs": "https://arxiv.org/abs/2508.12482", "authors": ["Xiaomeng Zhu", "R. Thomas McCoy", "Robert Frank"], "title": "The Structural Sources of Verb Meaning Revisited: Large Language Models Display Syntactic Bootstrapping", "comment": null, "summary": "Syntactic bootstrapping (Gleitman, 1990) is the hypothesis that children use\nthe syntactic environments in which a verb occurs to learn its meaning. In this\npaper, we examine whether large language models exhibit a similar behavior. We\ndo this by training RoBERTa and GPT-2 on perturbed datasets where syntactic\ninformation is ablated. Our results show that models' verb representation\ndegrades more when syntactic cues are removed than when co-occurrence\ninformation is removed. Furthermore, the representation of mental verbs, for\nwhich syntactic bootstrapping has been shown to be particularly crucial in\nhuman verb learning, is more negatively impacted in such training regimes than\nphysical verbs. In contrast, models' representation of nouns is affected more\nwhen co-occurrences are distorted than when syntax is distorted. In addition to\nreinforcing the important role of syntactic bootstrapping in verb learning, our\nresults demonstrated the viability of testing developmental hypotheses on a\nlarger scale through manipulating the learning environments of large language\nmodels.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u53e5\u6cd5\u5f15\u5bfc\u5b66\u4e60\u52a8\u8bcd\uff0c\u7c7b\u4f3c\u4e8e\u513f\u7ae5\u7684\u5b66\u4e60\u65b9\u5f0f\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u8868\u73b0\u51fa\u4e0e\u513f\u7ae5\u76f8\u4f3c\u7684\u53e5\u6cd5\u5f15\u5bfc\u884c\u4e3a\uff0c\u5373\u5229\u7528\u52a8\u8bcd\u51fa\u73b0\u7684\u53e5\u6cd5\u73af\u5883\u6765\u5b66\u4e60\u5176\u542b\u4e49\u3002", "method": "\u901a\u8fc7\u5728\u7ecf\u8fc7\u6270\u52a8\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3RoBERTa\u548cGPT-2\uff0c\u5176\u4e2d\u53e5\u6cd5\u4fe1\u606f\u88ab\u6d88\u9664\u3002", "result": "\u5f53\u53e5\u6cd5\u7ebf\u7d22\u88ab\u79fb\u9664\u65f6\uff0c\u6a21\u578b\u5bf9\u52a8\u8bcd\u7684\u8868\u5f81\u4f1a\u9000\u5316\u66f4\u591a\uff1b\u5fc3\u7406\u52a8\u8bcd\u7684\u8868\u5f81\u53d7\u5230\u7684\u8d1f\u9762\u5f71\u54cd\u6bd4\u7269\u7406\u52a8\u8bcd\u66f4\u5927\uff1b\u540d\u8bcd\u7684\u8868\u5f81\u5728\u5171\u73b0\u5173\u7cfb\u88ab\u626d\u66f2\u65f6\u53d7\u5230\u7684\u5f71\u54cd\u66f4\u5927\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b66\u4e60\u52a8\u8bcd\u542b\u4e49\u65f6\u8868\u73b0\u51fa\u4e0e\u513f\u7ae5\u76f8\u4f3c\u7684\u53e5\u6cd5\u5f15\u5bfc\u73b0\u8c61\uff0c\u53e5\u6cd5\u4fe1\u606f\u5bf9\u4e8e\u52a8\u8bcd\u5b66\u4e60\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.12137", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12137", "abs": "https://arxiv.org/abs/2508.12137", "authors": ["Nikolaos-Antonios Ypsilantis", "Kaifeng Chen", "Andr\u00e9 Araujo", "Ond\u0159ej Chum"], "title": "Infusing fine-grained visual knowledge to Vision-Language Models", "comment": "ICCVW 2025 accepted paper. Workshop name: \"What is Next in Multimodal\n  Foundation Models?\"", "summary": "Large-scale contrastive pre-training produces powerful Vision-and-Language\nModels (VLMs) capable of generating representations (embeddings) effective for\na wide variety of visual and multimodal tasks. However, these pretrained\nembeddings remain suboptimal for fine-grained open-set visual retrieval, where\nstate-of-the-art results require fine-tuning the vision encoder using annotated\ndomain-specific samples. Naively performing such fine-tuning typically leads to\ncatastrophic forgetting, severely diminishing the model's general-purpose\nvisual and cross-modal capabilities.\n  In this work, we propose a fine-tuning method explicitly designed to achieve\noptimal balance between fine-grained domain adaptation and retention of the\npretrained VLM's broad multimodal knowledge. Drawing inspiration from continual\nlearning literature, we systematically analyze standard regularization\ntechniques aimed at knowledge retention and propose an efficient and effective\ncombination strategy. Additionally, we address the commonly overlooked yet\ncritical aspects of validation set design and hyperparameter tuning to ensure\nreproducibility and robust generalization across datasets and pretrained\nmodels. We extensively evaluate our method on both fine-grained and\ncoarse-grained image-image and image-text retrieval benchmarks. Our approach\nconsistently achieves strong results, notably retaining the visual-text\nalignment without utilizing any text data or the original text encoder during\nfine-tuning. Code and model checkpoints: https://github.com/nikosips/infusing .", "AI": {"tldr": "The paper introduces a fine-tuning method for VLMs that balances domain adaptation with retention of broad knowledge, achieving strong retrieval results without forgetting or using text data during fine-tuning.", "motivation": "Pre-trained Vision-and-Language Models (VLMs) are suboptimal for fine-grained open-set visual retrieval, and fine-tuning leads to catastrophic forgetting of general-purpose capabilities.", "method": "A fine-tuning method is proposed, combining regularization techniques for knowledge retention with careful validation set design and hyperparameter tuning.", "result": "The method consistently achieves strong results on fine-grained and coarse-grained image-image and image-text retrieval benchmarks.", "conclusion": "The proposed fine-tuning method achieves strong results on both fine-grained and coarse-grained image-image and image-text retrieval benchmarks, retaining visual-text alignment without using text data or the original text encoder during fine-tuning."}}
{"id": "2508.12845", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.12845", "abs": "https://arxiv.org/abs/2508.12845", "authors": ["Artem Pshenitsyn", "Aleksandr Panov", "Alexey Skrynnik"], "title": "CAMAR: Continuous Actions Multi-Agent Routing", "comment": null, "summary": "Multi-agent reinforcement learning (MARL) is a powerful paradigm for solving\ncooperative and competitive decision-making problems. While many MARL\nbenchmarks have been proposed, few combine continuous state and action spaces\nwith challenging coordination and planning tasks. We introduce CAMAR, a new\nMARL benchmark designed explicitly for multi-agent pathfinding in environments\nwith continuous actions. CAMAR supports cooperative and competitive\ninteractions between agents and runs efficiently at up to 100,000 environment\nsteps per second. We also propose a three-tier evaluation protocol to better\ntrack algorithmic progress and enable deeper analysis of performance. In\naddition, CAMAR allows the integration of classical planning methods such as\nRRT and RRT* into MARL pipelines. We use them as standalone baselines and\ncombine RRT* with popular MARL algorithms to create hybrid approaches. We\nprovide a suite of test scenarios and benchmarking tools to ensure\nreproducibility and fair comparison. Experiments show that CAMAR presents a\nchallenging and realistic testbed for the MARL community.", "AI": {"tldr": "Introduces CAMAR, a new MARL benchmark for multi-agent pathfinding with continuous actions, supporting both cooperative and competitive scenarios.", "motivation": "Few MARL benchmarks combine continuous state and action spaces with challenging coordination and planning tasks.", "method": "A new MARL benchmark designed explicitly for multi-agent pathfinding in environments with continuous actions, integration of classical planning methods such as RRT and RRT* into MARL pipelines", "result": "CAMAR supports cooperative and competitive interactions between agents and runs efficiently at up to 100,000 environment steps per second. A three-tier evaluation protocol is proposed to better track algorithmic progress and enable deeper analysis of performance.", "conclusion": "CAMAR is a challenging and realistic testbed for the MARL community."}}
{"id": "2508.12278", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12278", "abs": "https://arxiv.org/abs/2508.12278", "authors": ["Siyue Xie", "Da Sun Handason Tam", "Wing Cheong Lau"], "title": "CRoC: Context Refactoring Contrast for Graph Anomaly Detection with Limited Supervision", "comment": "Accepted by ECAI 2025", "summary": "Graph Neural Networks (GNNs) are widely used as the engine for various\ngraph-related tasks, with their effectiveness in analyzing graph-structured\ndata. However, training robust GNNs often demands abundant labeled data, which\nis a critical bottleneck in real-world applications. This limitation severely\nimpedes progress in Graph Anomaly Detection (GAD), where anomalies are\ninherently rare, costly to label, and may actively camouflage their patterns to\nevade detection. To address these problems, we propose Context Refactoring\nContrast (CRoC), a simple yet effective framework that trains GNNs for GAD by\njointly leveraging limited labeled and abundant unlabeled data. Different from\nprevious works, CRoC exploits the class imbalance inherent in GAD to refactor\nthe context of each node, which builds augmented graphs by recomposing the\nattributes of nodes while preserving their interaction patterns. Furthermore,\nCRoC encodes heterogeneous relations separately and integrates them into the\nmessage-passing process, enhancing the model's capacity to capture complex\ninteraction semantics. These operations preserve node semantics while\nencouraging robustness to adversarial camouflage, enabling GNNs to uncover\nintricate anomalous cases. In the training stage, CRoC is further integrated\nwith the contrastive learning paradigm. This allows GNNs to effectively harness\nunlabeled data during joint training, producing richer, more discriminative\nnode embeddings. CRoC is evaluated on seven real-world GAD datasets with\nvarying scales. Extensive experiments demonstrate that CRoC achieves up to 14%\nAUC improvement over baseline GNNs and outperforms state-of-the-art GAD methods\nunder limited-label settings.", "AI": {"tldr": "CRoC\u662f\u4e00\u79cd\u7528\u4e8e\u56fe\u5f02\u5e38\u68c0\u6d4b\u7684\u6846\u67b6\uff0c\u5b83\u5229\u7528\u6709\u9650\u7684\u6807\u8bb0\u548c\u4e30\u5bcc\u7684\u672a\u6807\u8bb0\u6570\u636e\u6765\u8bad\u7ec3GNN\uff0c\u901a\u8fc7\u91cd\u6784\u8282\u70b9\u4e0a\u4e0b\u6587\u548c\u96c6\u6210\u5f02\u6784\u5173\u7cfb\u6765\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u8bad\u7ec3\u9c81\u68d2\u7684GNN\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u6807\u8bb0\u6570\u636e\uff0c\u8fd9\u662f\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u74f6\u9888\u3002\u8fd9\u79cd\u9650\u5236\u4e25\u91cd\u963b\u788d\u4e86\u56fe\u5f02\u5e38\u68c0\u6d4b(GAD)\u7684\u8fdb\u5c55\uff0c\u56e0\u4e3a\u5f02\u5e38\u672c\u8d28\u4e0a\u662f\u7f55\u89c1\u7684\uff0c\u6807\u8bb0\u6210\u672c\u9ad8\u6602\uff0c\u5e76\u4e14\u53ef\u80fd\u79ef\u6781\u5730\u4f2a\u88c5\u5b83\u4eec\u7684\u6a21\u5f0f\u4ee5\u9003\u907f\u68c0\u6d4b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u91cd\u6784\u5bf9\u6bd4(CRoC)\uff0c\u8fd9\u662f\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5171\u540c\u5229\u7528\u6709\u9650\u7684\u6807\u8bb0\u6570\u636e\u548c\u4e30\u5bcc\u7684\u672a\u6807\u8bb0\u6570\u636e\u6765\u8bad\u7ec3\u7528\u4e8eGAD\u7684GNN\u3002CRoC\u5229\u7528GAD\u4e2d\u56fa\u6709\u7684\u7c7b\u4e0d\u5e73\u8861\u6765\u91cd\u6784\u6bcf\u4e2a\u8282\u70b9\u7684\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u91cd\u65b0\u7ec4\u5408\u8282\u70b9\u7684\u5c5e\u6027\u540c\u65f6\u4fdd\u6301\u5b83\u4eec\u7684\u4ea4\u4e92\u6a21\u5f0f\u6765\u6784\u5efa\u589e\u5f3a\u56fe\u3002\u6b64\u5916\uff0cCRoC\u5206\u522b\u7f16\u7801\u5f02\u6784\u5173\u7cfb\uff0c\u5e76\u5c06\u5b83\u4eec\u96c6\u6210\u5230\u6d88\u606f\u4f20\u9012\u8fc7\u7a0b\u4e2d\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u6355\u83b7\u590d\u6742\u4ea4\u4e92\u8bed\u4e49\u7684\u80fd\u529b\u3002\u5728\u8bad\u7ec3\u9636\u6bb5\uff0cCRoC\u8fdb\u4e00\u6b65\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u8303\u5f0f\u96c6\u6210\u3002\u8fd9\u4f7f\u5f97GNN\u80fd\u591f\u5728\u8054\u5408\u8bad\u7ec3\u671f\u95f4\u6709\u6548\u5730\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\uff0c\u4ece\u800c\u4ea7\u751f\u66f4\u4e30\u5bcc\u3001\u66f4\u5177\u533a\u5206\u6027\u7684\u8282\u70b9\u5d4c\u5165\u3002", "result": "CRoC\u5b9e\u73b0\u4e86\u9ad8\u8fbe14%\u7684AUC\u6539\u8fdb", "conclusion": "CRoC\u5728\u4e03\u4e2a\u771f\u5b9e\u4e16\u754c\u7684GAD\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u6709\u9650\u6807\u7b7e\u8bbe\u7f6e\u4e0b\uff0cCRoC\u6bd4\u57fa\u7ebfGNN\u5b9e\u73b0\u4e86\u9ad8\u8fbe14%\u7684AUC\u6539\u8fdb\uff0c\u5e76\u4e14\u4f18\u4e8e\u6700\u5148\u8fdb\u7684GAD\u65b9\u6cd5\u3002"}}
{"id": "2508.12495", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12495", "abs": "https://arxiv.org/abs/2508.12495", "authors": ["Yuangang Li", "Yiqing Shen", "Yi Nian", "Jiechao Gao", "Ziyi Wang", "Chenxiao Yu", "Shawn Li", "Jie Wang", "Xiyang Hu", "Yue Zhao"], "title": "Mitigating Hallucinations in Large Language Models via Causal Reasoning", "comment": null, "summary": "Large language models (LLMs) exhibit logically inconsistent hallucinations\nthat appear coherent yet violate reasoning principles, with recent research\nsuggesting an inverse relationship between causal reasoning capabilities and\nsuch hallucinations. However, existing reasoning approaches in LLMs, such as\nChain-of-Thought (CoT) and its graph-based variants, operate at the linguistic\ntoken level rather than modeling the underlying causal relationships between\nvariables, lacking the ability to represent conditional independencies or\nsatisfy causal identification assumptions. To bridge this gap, we introduce\ncausal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning\nframework that trains LLMs to explicitly construct variable-level directed\nacyclic graph (DAG) and then perform reasoning over it. Moreover, we present a\ndataset comprising 25,368 samples (CausalDR), where each sample includes an\ninput question, explicit causal DAG, graph-based reasoning trace, and validated\nanswer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves\nthe causal reasoning capability with the state-of-the-art 95.33% accuracy on\nCLADDER (surpassing human performance of 94.8% for the first time) and reduces\nthe hallucination on HaluEval with 10% improvements. It demonstrates that\nexplicit causal structure modeling in LLMs can effectively mitigate logical\ninconsistencies in LLM outputs. Code is available at\nhttps://github.com/MrLYG/CDCR-SFT.", "AI": {"tldr": "CDCR-SFT\u901a\u8fc7\u663e\u5f0f\u56e0\u679c\u56fe\u5efa\u6a21\u63d0\u5347LLM\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u8868\u73b0\u51fa\u903b\u8f91\u4e0a\u4e0d\u4e00\u81f4\u7684\u5e7b\u89c9\uff0c\u8fd9\u4e9b\u5e7b\u89c9\u770b\u8d77\u6765\u662f\u8fde\u8d2f\u7684\uff0c\u4f46\u8fdd\u53cd\u4e86\u63a8\u7406\u539f\u5219\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u56e0\u679c\u63a8\u7406\u80fd\u529b\u548c\u8fd9\u79cd\u5e7b\u89c9\u4e4b\u95f4\u5b58\u5728\u53cd\u6bd4\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86\u56e0\u679cDAG\u6784\u5efa\u548c\u63a8\u7406(CDCR-SFT)\u7684\u76d1\u7763\u5fae\u8c03\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u8bad\u7ec3LLM\u6765\u663e\u5f0f\u5730\u6784\u5efa\u53d8\u91cf\u7ea7\u7684\u6709\u5411\u65e0\u73af\u56fe(DAG)\uff0c\u7136\u540e\u5728\u5176\u4e0a\u6267\u884c\u63a8\u7406\u3002", "result": "\u57288\u4e2a\u4efb\u52a1\u4e0a\u5bf94\u4e2allm\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCDCR-SFT\u63d0\u9ad8\u4e86\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u5728CLADDER\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u768495.33%\u7684\u51c6\u786e\u7387(\u9996\u6b21\u8d85\u8fc7\u4e8694.8%\u7684\u4eba\u7c7b\u8868\u73b0)\uff0c\u5e76\u5728HaluEval\u4e0a\u51cf\u5c11\u4e8610%\u7684\u5e7b\u89c9\u3002", "conclusion": "CDCR-SFT\u901a\u8fc7\u663e\u5f0f\u7684\u56e0\u679c\u7ed3\u6784\u5efa\u6a21\uff0c\u6709\u6548\u5730\u51cf\u8f7b\u4e86LLM\u8f93\u51fa\u4e2d\u7684\u903b\u8f91\u4e0d\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.12147", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12147", "abs": "https://arxiv.org/abs/2508.12147", "authors": ["Donghang Lyu", "Marius Staring", "Mariya Doneva", "Hildo J. Lamb", "Nicola Pezzotti"], "title": "KP-INR: A Dual-Branch Implicit Neural Representation Model for Cardiac Cine MRI Reconstruction", "comment": null, "summary": "Cardiac Magnetic Resonance (CMR) imaging is a non-invasive method for\nassessing cardiac structure, function, and blood flow. Cine MRI extends this by\ncapturing heart motion, providing detailed insights into cardiac mechanics. To\nreduce scan time and breath-hold discomfort, fast acquisition techniques have\nbeen utilized at the cost of lowering image quality. Recently, Implicit Neural\nRepresentation (INR) methods have shown promise in unsupervised reconstruction\nby learning coordinate-to-value mappings from undersampled data, enabling\nhigh-quality image recovery. However, current existing INR methods primarily\nfocus on using coordinate-based positional embeddings to learn the mapping,\nwhile overlooking the feature representations of the target point and its\nneighboring context. In this work, we propose KP-INR, a dual-branch INR method\noperating in k-space for cardiac cine MRI reconstruction: one branch processes\nthe positional embedding of k-space coordinates, while the other learns from\nlocal multi-scale k-space feature representations at those coordinates. By\nenabling cross-branch interaction and approximating the target k-space values\nfrom both branches, KP-INR can achieve strong performance on challenging\nCartesian k-space data. Experiments on the CMRxRecon2024 dataset confirms its\nimproved performance over baseline models and highlights its potential in this\nfield.", "AI": {"tldr": "\u63d0\u51faKP-INR\uff0c\u4e00\u79cd\u7528\u4e8e\u5fc3\u810f\u7535\u5f71MRI\u91cd\u5efa\u7684\u53cc\u5206\u652fINR\u65b9\u6cd5\uff0c\u901a\u8fc7\u5904\u7406\u4f4d\u7f6e\u5d4c\u5165\u548c\u5b66\u4e60\u5c40\u90e8\u591a\u5c3a\u5ea6k\u7a7a\u95f4\u7279\u5f81\u8868\u793a\uff0c\u5728\u5feb\u901fMRI\u91cd\u5efa\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u51cf\u5c11\u626b\u63cf\u65f6\u95f4\u548c\u5c4f\u6c14\u4e0d\u9002\uff0c\u5feb\u901f\u91c7\u96c6\u6280\u672f\u88ab\u5229\u7528\uff0c\u4f46\u964d\u4f4e\u4e86\u56fe\u50cf\u8d28\u91cf\u3002\u5f53\u524d\u7684INR\u65b9\u6cd5\u4e3b\u8981\u4fa7\u91cd\u4e8e\u4f7f\u7528\u57fa\u4e8e\u5750\u6807\u7684\u4f4d\u7f6e\u5d4c\u5165\u6765\u5b66\u4e60\u6620\u5c04\uff0c\u800c\u5ffd\u7565\u4e86\u76ee\u6807\u70b9\u53ca\u5176\u76f8\u90bb\u4e0a\u4e0b\u6587\u7684\u7279\u5f81\u8868\u793a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5206\u652fINR\u65b9\u6cd5KP-INR\uff0c\u8be5\u65b9\u6cd5\u5728k\u7a7a\u95f4\u4e2d\u8fd0\u884c\uff0c\u7528\u4e8e\u5fc3\u810f\u7535\u5f71MRI\u91cd\u5efa\uff1a\u4e00\u4e2a\u5206\u652f\u5904\u7406k\u7a7a\u95f4\u5750\u6807\u7684\u4f4d\u7f6e\u5d4c\u5165\uff0c\u800c\u53e6\u4e00\u4e2a\u5206\u652f\u4ece\u8fd9\u4e9b\u5750\u6807\u7684\u5c40\u90e8\u591a\u5c3a\u5ea6k\u7a7a\u95f4\u7279\u5f81\u8868\u793a\u4e2d\u5b66\u4e60\u3002", "result": "\u901a\u8fc7\u5b9e\u73b0\u8de8\u5206\u652f\u4ea4\u4e92\u5e76\u4ece\u4e24\u4e2a\u5206\u652f\u903c\u8fd1\u76ee\u6807k\u7a7a\u95f4\u503c\uff0cKP-INR\u53ef\u4ee5\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u7b1b\u5361\u5c14k\u7a7a\u95f4\u6570\u636e\u4e0a\u5b9e\u73b0\u5f3a\u5927\u7684\u6027\u80fd\u3002", "conclusion": "KP-INR\u5728CMRxRecon2024\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u7a81\u51fa\u4e86\u5176\u5728\u8be5\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.12854", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.12854", "abs": "https://arxiv.org/abs/2508.12854", "authors": ["Ronghao Lin", "Shuai Shen", "Weipeng Hu", "Qiaolin He", "Aolin Xiong", "Li Huang", "Haifeng Hu", "Yap-peng Tan"], "title": "E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model", "comment": "Accepted at ACM MM 2025 Grand Challenge", "summary": "Multimodal Empathetic Response Generation (MERG) is crucial for building\nemotionally intelligent human-computer interactions. Although large language\nmodels (LLMs) have improved text-based ERG, challenges remain in handling\nmultimodal emotional content and maintaining identity consistency. Thus, we\npropose E3RG, an Explicit Emotion-driven Empathetic Response Generation System\nbased on multimodal LLMs which decomposes MERG task into three parts:\nmultimodal empathy understanding, empathy memory retrieval, and multimodal\nresponse generation. By integrating advanced expressive speech and video\ngenerative models, E3RG delivers natural, emotionally rich, and\nidentity-consistent responses without extra training. Experiments validate the\nsuperiority of our system on both zero-shot and few-shot settings, securing\nTop-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.\nOur code is available at https://github.com/RH-Lin/E3RG.", "AI": {"tldr": "E3RG\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001LLM\u7684\u663e\u5f0f\u60c5\u611f\u9a71\u52a8\u7684\u5171\u60c5\u54cd\u5e94\u751f\u6210\u7cfb\u7edf\uff0c\u5b83\u901a\u8fc7\u5206\u89e3MERG\u4efb\u52a1\u5e76\u96c6\u6210\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\uff0c\u5728\u591a\u6a21\u6001\u60c5\u611f\u5185\u5bb9\u5904\u7406\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u591a\u6a21\u6001\u5171\u60c5\u54cd\u5e94\u751f\u6210 (MERG) \u5bf9\u4e8e\u6784\u5efa\u60c5\u611f\u667a\u80fd\u4eba\u673a\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6539\u8fdb\u4e86\u57fa\u4e8e\u6587\u672c\u7684ERG\uff0c\u4f46\u5728\u5904\u7406\u591a\u6a21\u6001\u60c5\u611f\u5185\u5bb9\u548c\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u6311\u6218\u3002", "method": "E3RG\u5c06MERG\u4efb\u52a1\u5206\u89e3\u4e3a\u4e09\u4e2a\u90e8\u5206\uff1a\u591a\u6a21\u6001\u5171\u60c5\u7406\u89e3\u3001\u5171\u60c5\u8bb0\u5fc6\u68c0\u7d22\u548c\u591a\u6a21\u6001\u54cd\u5e94\u751f\u6210\u3002\u901a\u8fc7\u96c6\u6210\u5148\u8fdb\u7684\u8868\u8fbe\u6027\u8bed\u97f3\u548c\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0cE3RG\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u63d0\u4f9b\u81ea\u7136\u3001\u60c5\u611f\u4e30\u5bcc\u4e14\u8eab\u4efd\u4e00\u81f4\u7684\u54cd\u5e94\u3002", "result": "E3RG\u5728\u96f6\u6837\u672c\u548c\u5c0f\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "E3RG\u5728\u96f6\u6837\u672c\u548c\u5c0f\u6837\u672c\u8bbe\u7f6e\u4e0b\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728ACM MM 25\u7684\u57fa\u4e8e\u5316\u8eab\u7684Multimodal Empathy Challenge\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\u3002"}}
{"id": "2508.12327", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.12327", "abs": "https://arxiv.org/abs/2508.12327", "authors": ["Wei Jiang", "Lijun Zhang"], "title": "Convergence Analysis of the Lion Optimizer in Centralized and Distributed Settings", "comment": null, "summary": "In this paper, we analyze the convergence properties of the Lion optimizer.\nFirst, we establish that the Lion optimizer attains a convergence rate of\n$\\mathcal{O}(d^{1/2}T^{-1/4})$ under standard assumptions, where $d$ denotes\nthe problem dimension and $T$ is the iteration number. To further improve this\nrate, we introduce the Lion optimizer with variance reduction, resulting in an\nenhanced convergence rate of $\\mathcal{O}(d^{1/2}T^{-1/3})$. We then analyze in\ndistributed settings, where the standard and variance reduced version of the\ndistributed Lion can obtain the convergence rates of\n$\\mathcal{O}(d^{1/2}(nT)^{-1/4})$ and $\\mathcal{O}(d^{1/2}(nT)^{-1/3})$, with\n$n$ denoting the number of nodes. Furthermore, we investigate a\ncommunication-efficient variant of the distributed Lion that ensures sign\ncompression in both communication directions. By employing the unbiased sign\noperations, the proposed Lion variant and its variance reduction counterpart,\nachieve convergence rates of $\\mathcal{O}\\left( \\max\n\\left\\{\\frac{d^{1/4}}{T^{1/4}}, \\frac{d^{1/10}}{n^{1/5}T^{1/5}} \\right\\}\n\\right)$ and $\\mathcal{O}\\left( \\frac{d^{1/4}}{T^{1/4}} \\right)$, respectively.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Lion\u4f18\u5316\u5668\u7684\u6536\u655b\u6027\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u7248\u672c\uff0c\u5e76\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u5206\u6790\u3002", "motivation": "\u65e8\u5728\u63d0\u9ad8Lion\u4f18\u5316\u5668\u7684\u6536\u655b\u901f\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u5e26\u6709\u65b9\u5dee\u51cf\u5c11\u7684Lion\u4f18\u5316\u5668\u4ee5\u53ca\u901a\u4fe1\u9ad8\u6548\u7684\u5206\u5e03\u5f0fLion\u53d8\u4f53\u3002", "result": "\u5728\u6807\u51c6\u5047\u8bbe\u4e0b\uff0cLion\u4f18\u5316\u5668\u5b9e\u73b0\u4e86O(d^{1/2}T^{-1/4})\u7684\u6536\u655b\u901f\u5ea6\u3002\u5e26\u6709\u65b9\u5dee\u51cf\u5c11\u7684Lion\u4f18\u5316\u5668\u5b9e\u73b0\u4e86O(d^{1/2}T^{-1/3})\u7684\u6536\u655b\u901f\u5ea6\u3002\u901a\u4fe1\u9ad8\u6548\u7684\u5206\u5e03\u5f0fLion\u53d8\u4f53\u5b9e\u73b0\u4e86O(max{d^{1/4}/T^{1/4}, d^{1/10}/(n^{1/5}T^{1/5})})\u548cO(d^{1/4}/T^{1/4})\u7684\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u5206\u6790\u4e86Lion\u4f18\u5316\u5668\u7684\u6536\u655b\u6027\u8d28\uff0c\u5e76\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u6269\u5c55\u3002"}}
{"id": "2508.12535", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12535", "abs": "https://arxiv.org/abs/2508.12535", "authors": ["Seonglae Cho", "Zekun Wu", "Adriano Koshiyama"], "title": "CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection", "comment": "42 pages, 9 tables", "summary": "Sparse Autoencoders (SAEs) can extract interpretable features from large\nlanguage models (LLMs) without supervision. However, their effectiveness in\ndownstream steering tasks is limited by the requirement for contrastive\ndatasets or large activation storage. To address these limitations, we propose\nCorrSteer, which selects features by correlating sample correctness with SAE\nactivations from generated tokens at inference time. This approach uses only\ninference-time activations to extract more relevant features, thereby avoiding\nspurious correlations. It also obtains steering coefficients from average\nactivations, automating the entire pipeline. Our method shows improved task\nperformance on QA, bias mitigation, jailbreaking prevention, and reasoning\nbenchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1%\nimprovement in MMLU performance and a +22.9% improvement in HarmBench with only\n4000 samples. Selected features demonstrate semantically meaningful patterns\naligned with each task's requirements, revealing the underlying capabilities\nthat drive performance. Our work establishes correlationbased selection as an\neffective and scalable approach for automated SAE steering across language\nmodel applications.", "AI": {"tldr": "CorrSteer selects features by correlating sample correctness with SAE activations from generated tokens at inference time to improve task performance and automate the entire pipeline", "motivation": "effectiveness in downstream steering tasks is limited by the requirement for contrastive datasets or large activation storage", "method": "CorrSteer, which selects features by correlating sample correctness with SAE activations from generated tokens at inference time", "result": "improved task performance on QA, bias mitigation, jailbreaking prevention, and reasoning benchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1% improvement in MMLU performance and a +22.9% improvement in HarmBench with only 4000 samples", "conclusion": "correlationbased selection is an effective and scalable approach for automated SAE steering across language model applications"}}
