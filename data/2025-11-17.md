<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 51]
- [cs.CV](#cs.CV) [Total: 51]
- [cs.AI](#cs.AI) [Total: 34]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 50]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Continual Learning of Domain Knowledge from Human Feedback in Text-to-SQL](https://arxiv.org/abs/2511.10674)
*Thomas Cook,Kelly Patel,Sivapriya Vellaichamy,Saba Rahimi,Zhen Zeng,Sumitra Ganesh*

Main category: cs.CL

TL;DR: LLMs struggle with database schemas and domain knowledge when generating SQL from natural language. This paper introduces a continual learning framework using human feedback to refine queries and store knowledge for future use.


<details>
  <summary>Details</summary>
Motivation: LLMs need help with database-specific schemas and tacit domain knowledge in text-to-SQL tasks.

Method: A continual learning framework is introduced where an agent learns from human feedback, refines queries, and distills knowledge into a structured memory.

Result: Memory-augmented agents, especially the Procedural Agent, show significant accuracy gains and error reduction on the BIRD benchmark Dev set.

Conclusion: Transforming human expertise into reusable knowledge improves text-to-SQL systems, making them more adaptive and domain-aware.

Abstract: Large Language Models (LLMs) can generate SQL queries from natural language questions but struggle with database-specific schemas and tacit domain knowledge. We introduce a framework for continual learning from human feedback in text-to-SQL, where a learning agent receives natural language feedback to refine queries and distills the revealed knowledge for reuse on future tasks. This distilled knowledge is stored in a structured memory, enabling the agent to improve execution accuracy over time. We design and evaluate multiple variations of a learning agent architecture that vary in how they capture and retrieve past experiences. Experiments on the BIRD benchmark Dev set show that memory-augmented agents, particularly the Procedural Agent, achieve significant accuracy gains and error reduction by leveraging human-in-the-loop feedback. Our results highlight the importance of transforming tacit human expertise into reusable knowledge, paving the way for more adaptive, domain-aware text-to-SQL systems that continually learn from a human-in-the-loop.

</details>


### [2] [Unsupervised Cycle Detection in Agentic Applications](https://arxiv.org/abs/2511.10650)
*Felix George,Harshit Kumar,Divya Pathak,Kaustabha Ray,Mudit Verma,Pratibha Moogi*

Main category: cs.CL

TL;DR: 大型语言模型驱动的Agentic应用存在非确定性行为，可能形成隐藏的执行循环，消耗资源而不触发错误。


<details>
  <summary>Details</summary>
Motivation: 传统的可观察性平台无法检测到这些代价高昂的低效率问题。

Method: 提出了一种无监督循环检测框架，结合了结构和语义分析。首先应用计算高效的时间调用堆栈分析来识别显式循环，然后利用语义相似性分析来发现以冗余内容生成为特征的细微循环。

Result: 在基于LangGraph的股票市场应用程序的1575条轨迹上进行评估，混合方法实现了0.72的F1分数（精确率：0.62，召回率：0.86），明显优于单独的结构（F1：0.08）和语义方法（F1：0.28）。

Conclusion: 虽然这些结果令人鼓舞，但仍有很大的改进空间，未来的工作需要完善该方法并解决其当前的局限性。

Abstract: Agentic applications powered by Large Language Models exhibit non-deterministic behaviors that can form hidden execution cycles, silently consuming resources without triggering explicit errors. Traditional observability platforms fail to detect these costly inefficiencies. We present an unsupervised cycle detection framework that combines structural and semantic analysis. Our approach first applies computationally efficient temporal call stack analysis to identify explicit loops and then leverages semantic similarity analysis to uncover subtle cycles characterized by redundant content generation. Evaluated on 1575 trajectories from a LangGraph-based stock market application, our hybrid approach achieves an F1 score of 0.72 (precision: 0.62, recall: 0.86), significantly outperforming individual structural (F1: 0.08) and semantic methods (F1: 0.28). While these results are encouraging, there remains substantial scope for improvement, and future work is needed to refine the approach and address its current limitations.

</details>


### [3] [MedPath: Multi-Domain Cross-Vocabulary Hierarchical Paths for Biomedical Entity Linking](https://arxiv.org/abs/2511.10887)
*Nishant Mishra,Wilker Aziz,Iacer Calixto*

Main category: cs.CL

TL;DR: MedPath是一个大规模、多领域的生物医学实体链接(EL)数据集，它建立在9个现有的专家注释EL数据集之上。


<details>
  <summary>Details</summary>
Motivation: 生物医学命名实体识别(NER)和实体链接(EL)的进展目前受到数据碎片化、缺乏构建可解释模型的资源以及语义盲评估指标的限制。

Method: 构建MedPath数据集，所有实体都使用最新版本的统一医学语言系统(UMLS)进行标准化，并增加了到其他62个生物医学词汇表的映射，以及在多达11个生物医学词汇表中的完整本体路径。

Result: MedPath直接实现了生物医学NLP新的研究前沿，促进了语义丰富和可解释的EL系统的训练和评估，以及下一代可互操作和可解释的临床NLP模型的开发。

Conclusion: MedPath是一个有益于生物医学NLP领域的数据集。

Abstract: Progress in biomedical Named Entity Recognition (NER) and Entity Linking (EL) is currently hindered by a fragmented data landscape, a lack of resources for building explainable models, and the limitations of semantically-blind evaluation metrics. To address these challenges, we present MedPath, a large-scale and multi-domain biomedical EL dataset that builds upon nine existing expert-annotated EL datasets. In MedPath, all entities are 1) normalized using the latest version of the Unified Medical Language System (UMLS), 2) augmented with mappings to 62 other biomedical vocabularies and, crucially, 3) enriched with full ontological paths -- i.e., from general to specific -- in up to 11 biomedical vocabularies. MedPath directly enables new research frontiers in biomedical NLP, facilitating training and evaluation of semantic-rich and interpretable EL systems, and the development of the next generation of interoperable and explainable clinical NLP models.

</details>


### [4] [Data Analysis and Performance Evaluation of Simulation Deduction Based on LLMs](https://arxiv.org/abs/2511.10651)
*Shansi Zhang,Min Li*

Main category: cs.CL

TL;DR: 提出了一种利用大型语言模型（LLM）进行兵棋推演数据分析和性能评估的方法，该方法通过分解任务、设计提示、多轮交互和自定义工具来生成高质量的分析报告。


<details>
  <summary>Details</summary>
Motivation: 传统的手动分析耗时且容易出错，为了提高效率和准确性，采用具有强大分析和推理能力的大型语言模型（LLM）。

Method: 该方法将复杂任务分解为多个子任务，为每个子任务设计有效的系统提示和用户提示，通过与LLM的多轮交互，结合自我检查和反思，实现结构化数据提取以及多步骤分析和评估。此外，还定义和调用自定义工具来生成图表和计算指标，并设计多个报告模板。

Result: 实验结果表明，该方法生成的报告质量更高，得分高于基线方法。

Conclusion: 该方法能够有效利用大型语言模型生成高质量的兵棋推演数据分析报告。

Abstract: Data analysis and performance evaluation of simulation deduction plays a pivotal role in modern warfare, which enables military personnel to gain invaluable insights into the potential effectiveness of different strategies, tactics, and operational plans. Traditional manual analysis approach is time-consuming and limited by human errors. To enhance efficiency and accuracy, large language models (LLMs) with strong analytical and inferencing capabilities can be employed. However, high-quality analysis reports with well-structured formatting cannot be obtained through a single instruction input to the LLM. To tackle this issue, we propose a method that first decomposes the complex task into several sub-tasks and designs effective system prompts and user prompts for each sub-task. Multi-round interactions with the LLM incorporating self-check and reflection are then conducted to enable structured data extraction as well as multi-step analysis and evaluation. Furthermore, custom tools are defined and invoked to generate figures and compute metrics. We also design multiple report templates, each tailored to a specific application and input data type, ensuring their adaptability across a variety of scenarios. Extensive evaluation results demonstrate that the reports generated by our method exhibit higher quality, therefore obtaining higher scores than the baseline method.

</details>


### [5] [Cognitively-Inspired Episodic Memory Architectures for Accurate and Efficient Character AI](https://arxiv.org/abs/2511.10652)
*Rafael Arias Gonzalez,Steve DiPaola*

Main category: cs.CL

TL;DR: 提出了一种新的架构，通过离线数据增强和高效的并行检索，解决了现有方法在历史人物对话系统中，浅层回应和高延迟之间的trade-off。


<details>
  <summary>Details</summary>
Motivation: 现有方法在历史人物对话系统中面临回应浅层和高延迟的trade-off。

Method: 该系统将人物传记数据转换为1774条包含情感语义元数据的增强的第一人称记忆，然后采用两阶段检索，实现了0.52秒的prompt生成。

Result: 在LLM-as-judge和RAGAs指标的评估中，该方法在GPT-4上与传统RAG持平，但在较小模型（GPT-3.5, GPT-3）上显著优于传统RAG。

Conclusion: 该架构可推广到任何具有大量文本记录的历史人物，为需要准确性和效率的教育、博物馆和研究应用提供了一个实用的框架。

Abstract: Large language models show promise for embodying historical characters in dialogue systems, but existing approaches face a critical trade-off: simple retrieval-augmented generation produces shallow responses, while multi-stage reflection achieves depth at prohibitive latency. We present an architecture that resolves this tension through offline data augmentation and efficient parallel retrieval from structured episodic memory. Our system transforms biographical data into 1,774 enriched first-person memories with affective-semantic metadata, then employs two-stage retrieval achieving 0.52s prompt generation. Evaluation using LLM-as-judge and RAGAs metrics shows our approach achieves parity with traditional RAG on GPT-4 while significantly outperforming it on smaller models (GPT-3.5, GPT-3), suggesting particular value for resource-constrained deployments. Beyond dialogue, the structured memory enables novel visualization tools: spatiotemporal heatmaps, emotional trajectory analysis, and interactive path tracking, positioning the system as both a dialogue interface and research tool for biographical analysis. We use Van Gogh as a test case, but the architecture is generalizable to any historical figure with substantial textual records, offering a practical framework for educational, museum, and research applications requiring both accuracy and efficiency

</details>


### [6] [Hybrid Quantum Transformer for Language Generation](https://arxiv.org/abs/2511.10653)
*Desheng Kong,Xiangshuo Cui,Jiaying Jin,Jing Xu,Donglin Wang*

Main category: cs.CL

TL;DR: 本文提出了一种混合量子-经典的大型语言模型(LLM)，用于自然语言生成，该模型能够执行连贯且具有上下文感知能力的对话。


<details>
  <summary>Details</summary>
Motivation: 量子计算越来越多地被应用于替代经典计算，但目前还没有成功应用于大规模自然语言生成的量子或混合模型。

Method: 该架构在8M和150M参数规模上将变分量子电路(VQCs)集成到Transformer框架中。

Result: 实验结果表明，最少数量的量子比特(10个量子比特和80个量子门)可以替代1.5亿参数模型中约10%的经典参数，同时实现相当的收敛稳定性和生成质量。

Conclusion: 该研究初步展示了将量子计算集成到大规模生成语言模型中的可行性。

Abstract: Although quantum computing has been increasingly applied to replace classical computation, most existing quantum or hybrid models remain confined to simple tasks, with no successful application to large-scale natural language generation to date. In this work, we present the first hybrid quantum-classical large language model (LLM) for natural language generation, HyQuT, capable of performing coherent and context-aware dialogue. The proposed architecture integrates variational quantum circuits (VQCs) into the Transformer framework at both 8M and 150M parameter scales. Experimental results show that a minimal number of qubits (10 qubits with 80 quantum gates) can replace about 10% of the classical parameters in the 150M-parameter model, while achieving comparable convergence stability and generation quality. This study provides an early demonstration of the feasibility of integrating quantum computing to large-scale generative language models.

</details>


### [7] [Empirical Characterization of Temporal Constraint Processing in LLMs](https://arxiv.org/abs/2511.10654)
*Javier Marín*

Main category: cs.CL

TL;DR: 大型语言模型在时间受限的agentic架构中应用时，判断动作窗口是否保持开放的能力不可靠。


<details>
  <summary>Details</summary>
Motivation: 在agentic架构中，LLM需要实时决策，并判断动作窗口的开放与关闭，但这种能力未经测试。

Method: 使用deadline检测任务，评估了8个不同规模的LLM在时间约束处理方面的能力。

Result: 发现模型性能呈现双峰分布，对prompt格式变化敏感，并存在系统性的动作偏差。参数量与能力无相关性，微调在一定程度上能提升部分模型的能力，但通过next-token预测进行学习不可靠。

Conclusion: 当前自回归架构缺乏持续时间状态表示、显式约束检查和系统组合推理等机制，在时间关键型应用中部署此类系统风险较高，需要结合符号推理模块的混合架构。

Abstract: When deploying LLMs in agentic architectures requiring real-time decisions under temporal constraints, we assume they reliably determine whether action windows remain open or have closed. This assumption is untested. We characterize temporal constraint processing across eight production-scale models (2.8-8B parameters) using deadline detection tasks, revealing systematic deployment risks: bimodal performance distribution (models achieve either 95% or 50% accuracy), extreme prompt brittleness (30-60 percentage point swings from formatting changes alone), and systematic action bias (100% false positive rates in failing models). Parameter count shows no correlation with capability in this range-a 3.8B model matches 7B models while other 7B models fail completely. Fine-tuning on 200 synthetic examples improves models with partial capability by 12-37 percentage points. We demonstrate that temporal constraint satisfaction cannot be reliably learned through next-token prediction on natural language, even with targeted fine-tuning. This capability requires architectural mechanisms for: (1) continuous temporal state representation, (2) explicit constraint checking separate from linguistic pattern matching, (3) systematic compositional reasoning over temporal relations. Current autoregressive architectures lack these mechanisms. Deploying such systems in time-critical applications without hybrid architectures incorporating symbolic reasoning modules represents unacceptable risk.

</details>


### [8] [Spectral Neuro-Symbolic Reasoning II: Semantic Node Merging, Entailment Filtering, and Knowledge Graph Alignment](https://arxiv.org/abs/2511.10655)
*Andrew Kiruluta,Priscilla Burity*

Main category: cs.CL

TL;DR: 本文通过引入三个语义增强模块，扩展了 Spectral NSR 框架，分别是基于 Transformer 的节点合并、句子级蕴含验证和与外部知识图谱对齐。


<details>
  <summary>Details</summary>
Motivation: 为了提高图的保真度，同时保留核心的光谱推理管道。

Method: 在 Spectral NSR 框架中，引入了基于 Transformer 的节点合并、使用 NLI 分类器的句子级蕴含验证以及与外部知识图谱对齐。

Result: 在 ProofWriter、EntailmentBank 和 CLUTRR 基准测试中，结果显示准确率持续提高（最高 +3.8%），对对抗性案例的泛化能力提高，并减少了推理噪声。

Conclusion: 这项工作通过模块化的、基于语义的预处理步骤扩展了 Spectral NSR 框架，这些步骤提高了图形质量，而没有改变核心的光谱推理引擎。

Abstract: This report extends the Spectral Neuro-Symbolic Reasoning (Spectral NSR) framework by introducing three semantically grounded enhancements: (1) transformer-based node merging using contextual embeddings (e.g., Sentence-BERT, SimCSE) to reduce redundancy, (2) sentence-level entailment validation with pretrained NLI classifiers (e.g., RoBERTa, DeBERTa) to improve edge quality, and (3) alignment with external knowledge graphs (e.g., ConceptNet, Wikidata) to augment missing context. These modifications enhance graph fidelity while preserving the core spectral reasoning pipeline. Experimental results on ProofWriter, EntailmentBank, and CLUTRR benchmarks show consistent accuracy gains (up to +3.8\%), improved generalization to adversarial cases, and reduced inference noise. The novelty lies in performing semantic and symbolic refinement entirely upstream of the spectral inference stage, enabling efficient, interpretable, and scalable reasoning without relying on quadratic attention mechanisms. In summary, this work extends the Spectral NSR framework with modular, semantically grounded preprocessing steps that improve graph quality without altering the core spectral reasoning engine. The result is a more robust, interpretable, and scalable reasoning system suitable for deployment in open-domain and real-world settings.

</details>


### [9] [Information Extraction From Fiscal Documents Using LLMs](https://arxiv.org/abs/2511.10659)
*Vikram Aggarwal,Jay Kulkarni,Aditi Mascarenhas,Aakriti Narang,Siddarth Raman,Ajay Shah,Susan Thomas*

Main category: cs.CL

TL;DR: LLMs are used to extract structured data from multi-page government fiscal documents.


<details>
  <summary>Details</summary>
Motivation: The ability of LLMs to process complex, hierarchical tabular data remains underexplored.

Method: A multi-stage pipeline that leverages domain knowledge, sequential context, and algorithmic validation is used. The inherent structure of fiscal tables, with totals at each level of the hierarchy, allows for robust internal validation of the extracted data. We use these hierarchical relationships to create multi-level validation checks.

Result: High accuracy is achieved when applied to annual fiscal documents from the State of Karnataka in India (200+ pages).

Conclusion: LLMs can read tables and also process document-specific structural hierarchies, offering a scalable process for converting PDF-based fiscal disclosures into research-ready databases. Our implementation shows promise for broader applications across developing country contexts.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in text comprehension, but their ability to process complex, hierarchical tabular data remains underexplored. We present a novel approach to extracting structured data from multi-page government fiscal documents using LLM-based techniques. Applied to annual fiscal documents from the State of Karnataka in India (200+ pages), our method achieves high accuracy through a multi-stage pipeline that leverages domain knowledge, sequential context, and algorithmic validation. A large challenge with traditional OCR methods is the inability to verify the accurate extraction of numbers. When applied to fiscal data, the inherent structure of fiscal tables, with totals at each level of the hierarchy, allows for robust internal validation of the extracted data. We use these hierarchical relationships to create multi-level validation checks. We demonstrate that LLMs can read tables and also process document-specific structural hierarchies, offering a scalable process for converting PDF-based fiscal disclosures into research-ready databases. Our implementation shows promise for broader applications across developing country contexts.

</details>


### [10] [Preference Orchestrator: Prompt-Aware Multi-Objective Alignment for Large Language Models](https://arxiv.org/abs/2511.10656)
*Biao Liu,Ning Xu,Junming Yang,Xin Geng*

Main category: cs.CL

TL;DR: 提出了一种名为PRO的新框架，用于解决大型语言模型（LLM）在多目标对齐中人工指定偏好权重的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手动指定的偏好权重，导致用户负担重和训练效率低下。

Method: PRO框架使用轻量级偏好适配器，在训练和部署阶段自动推断prompt特定的偏好权重。通过在来自多个奖励模型的归一化奖励分数上训练，为每个prompt自动学习合适的偏好权重。

Result: 理论分析和实验结果表明，PRO方法在多目标对齐场景中优于使用固定偏好权重的方法。

Conclusion: PRO框架能够有效解决多目标对齐中人工指定偏好权重的问题，并在多个任务中表现出优越性。

Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, aligning these models with varying human preferences across multiple objectives remains a significant challenge in practical deployments. Existing multi-objective alignment methods rely on manually specified preference weights, which not only burden users with difficult preference specification tasks but also lead to suboptimal training efficiency due to exploration of irrelevant preference combinations. To alleviate these issues, we propose a novel framework named PRO, i.e., PReference Orchestrator, which features a lightweight preference adapter that automatically infers prompt-specific preference weights during both training and deployment phases. Specifically, the adapter automatically learns appropriate preference weights for each prompt by training on normalized reward scores from multiple reward models for preferred responses, which inherently reflect effective preference balances across objectives. Additionally, We provide theoretical analysis proving that our prompt-aware preference mechanism achieves superior performance compared to fixed preference weights in multi-objective alignment scenarios. Extensive experiments across multiple tasks demonstrate the effectiveness of our method over existing multi-objective alignment approaches.

</details>


### [11] [Learn to Select: Exploring Label Distribution Divergence for In-Context Demonstration Selection in Text Classification](https://arxiv.org/abs/2511.10675)
*Ye Jiang,Taihang Wang,Youzheng Liu,Yimin Wang,Yuhan Xia,Yunfei Long*

Main category: cs.CL

TL;DR: 提出了一种新的ICL演示选择方法，该方法在语义相似性的基础上，考虑了标签分布对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的演示选择方法主要关注测试输入和演示之间的语义相似性，忽略了标签分布对齐的重要性。

Method: 提出了一种两阶段演示选择方法TopK + L2D，该方法利用微调的BERT类小型语言模型（SLM）生成标签分布，并计算测试输入和候选演示的标签分布散度。

Result: 在七个文本分类基准上的大量实验表明，该方法始终优于先前的演示选择策略。

Conclusion: LLM的性能与用于标签分布估计的底层SLM的准确性之间存在正相关关系。

Abstract: In-context learning (ICL) for text classification, which uses a few input-label demonstrations to describe a task, has demonstrated impressive performance on large language models (LLMs). However, the selection of in-context demonstrations plays a crucial role and can significantly affect LLMs' performance. Most existing demonstration selection methods primarily focus on semantic similarity between test inputs and demonstrations, often overlooking the importance of label distribution alignment. To address this limitation, we propose a two-stage demonstration selection method, TopK + Label Distribution Divergence (L2D), which leverages a fine-tuned BERT-like small language model (SLM) to generate label distributions and calculate their divergence for both test inputs and candidate demonstrations. This enables the selection of demonstrations that are not only semantically similar but also aligned in label distribution with the test input. Extensive experiments across seven text classification benchmarks show that our method consistently outperforms previous demonstration selection strategies. Further analysis reveals a positive correlation between the performance of LLMs and the accuracy of the underlying SLMs used for label distribution estimation.

</details>


### [12] [Patent Representation Learning via Self-supervision](https://arxiv.org/abs/2511.10657)
*You Zuo,Kim Gerdes,Eric Villemonte de La Clergerie,Benoît Sagot*

Main category: cs.CL

TL;DR: 提出了一种对比学习框架，通过利用同一专利文档内的多个视图来学习专利嵌入。


<details>
  <summary>Details</summary>
Motivation: SimCSE风格的dropout增强会产生过度统一的嵌入，失去语义内聚性。

Method: 提出基于section的增强，其中专利的不同部分（例如，摘要、权利要求、背景）充当互补视图。

Result: 在大型基准测试中，我们的完全自监督方法在现有技术检索和分类中与引文和IPC监督的基线相匹配或超过，同时避免依赖脆弱或不完整的注释。

Conclusion: 利用文档内视图对于可扩展和通用的专利理解具有重要价值。

Abstract: This paper presents a simple yet effective contrastive learning framework for learning patent embeddings by leveraging multiple views from within the same document. We first identify a patent-specific failure mode of SimCSE style dropout augmentation: it produces overly uniform embeddings that lose semantic cohesion. To remedy this, we propose section-based augmentation, where different sections of a patent (e.g., abstract, claims, background) serve as complementary views. This design introduces natural semantic and structural diversity, mitigating over-dispersion and yielding embeddings that better preserve both global structure and local continuity. On large-scale benchmarks, our fully self-supervised method matches or surpasses citation-and IPC-supervised baselines in prior-art retrieval and classification, while avoiding reliance on brittle or incomplete annotations. Our analysis further shows that different sections specialize for different tasks-claims and summaries benefit retrieval, while background sections aid classification-highlighting the value of patents' inherent discourse structure for representation learning. These results highlight the value of exploiting intra-document views for scalable and generalizable patent understanding.

</details>


### [13] [Evaluating Open-Weight Large Language Models for Structured Data Extraction from Narrative Medical Reports Across Multiple Use Cases and Languages](https://arxiv.org/abs/2511.10658)
*Douwe J. Spaanderman,Karthik Prathaban,Petr Zelina,Kaouther Mouheb,Lukáš Hejtmánek,Matthew Marzetti,Antonius W. Schurink,Damian Chan,Ruben Niemantsverdriet,Frederik Hartmann,Zhen Qian,Maarten G. J. Thomeer,Petr Holub,Farhan Akram,Frank J. Wolters,Meike W. Vernooij,Cornelis Verhoef,Esther E. Bron,Vít Nováček,Dirk J. Grünhagen,Wiro J. Niessen,Martijn P. A. Starmans,Stefan Klein*

Main category: cs.CL

TL;DR: 评估了15个开放权重的大型语言模型（LLM）在病理学和放射学报告中提取结构化信息的能力，涵盖六个用例和三个国家的研究机构。


<details>
  <summary>Details</summary>
Motivation: 先前的工作通常侧重于单一任务、有限的模型和英语报告，为了解决这些局限性。

Method: 在荷兰、英国和捷克共和国的三个研究机构，评估了15个开放权重的大型语言模型在病理学和放射学报告中的表现，涵盖六个用例。比较了六种提示策略：零样本、单样本、少样本、思维链、自我一致性和提示图。

Result: 排名靠前的模型在各项任务中的宏平均得分接近评估者间的一致性。中小型的通用模型与大型模型表现相当，而微型和专用模型表现较差。提示图和少样本提示将性能提高了约13%。

Conclusion: 开放权重的大型语言模型可以从跨疾病、语言和机构的临床报告中提取结构化数据，为临床数据管理提供了一种可扩展的方法。

Abstract: Large language models (LLMs) are increasingly used to extract structured information from free-text clinical records, but prior work often focuses on single tasks, limited models, and English-language reports. We evaluated 15 open-weight LLMs on pathology and radiology reports across six use cases, colorectal liver metastases, liver tumours, neurodegenerative diseases, soft-tissue tumours, melanomas, and sarcomas, at three institutes in the Netherlands, UK, and Czech Republic. Models included general-purpose and medical-specialised LLMs of various sizes, and six prompting strategies were compared: zero-shot, one-shot, few-shot, chain-of-thought, self-consistency, and prompt graph. Performance was assessed using task-appropriate metrics, with consensus rank aggregation and linear mixed-effects models quantifying variance. Top-ranked models achieved macro-average scores close to inter-rater agreement across tasks. Small-to-medium general-purpose models performed comparably to large models, while tiny and specialised models performed worse. Prompt graph and few-shot prompting improved performance by ~13%. Task-specific factors, including variable complexity and annotation variability, influenced results more than model size or prompting strategy. These findings show that open-weight LLMs can extract structured data from clinical reports across diseases, languages, and institutions, offering a scalable approach for clinical data curation.

</details>


### [14] [Test-Time Steering for Lossless Text Compression via Weighted Product of Experts](https://arxiv.org/abs/2511.10660)
*Qihang Zhang,Muchen Li,Ziao Wang,Renjie Liao,Lele Wang*

Main category: cs.CL

TL;DR: 提出了一种新的无损压缩框架，该框架结合了传统通用压缩器和预训练神经语言模型，以提高文本压缩性能，且无需微调。


<details>
  <summary>Details</summary>
Motivation: 传统的通用压缩器计算开销低、速度快且适用性广，但压缩率不如现代神经压缩器。神经压缩器泛化能力差。

Method: 通过加权专家乘积（wPoE）执行测试时引导，自适应地将通用压缩模型与预训练神经语言模型相结合。

Result: 实验表明，该方法提高了文本压缩性能，且无需微调。

Conclusion: 该方法可以与任何自回归语言模型无缝集成，为增强各种数据分布的文本压缩提供了一种实用的解决方案。

Abstract: Lossless compression techniques are crucial in an era of rapidly growing data. Traditional universal compressors like gzip offer low computational overhead, high speed, and broad applicability across data distributions. However, they often lead to worse compression rates than modern neural compressors, which leverage large-scale training data to model data distributions more effectively. Despite their advantages, neural compressors struggle to generalize to unseen data. To address this limitation, we propose a novel framework that performs Test-Time Steering via a Weighted Product of Experts (wPoE). At inference, our method adaptively combines a universal compression model with a pretrained neural language model, ensuring the compression rate is at least as good as that of the best individual model. Extensive experiments demonstrate that our approach improves the performance of text compression without requiring fine-tuning. Furthermore, it seamlessly integrates with any autoregressive language model, providing a practical solution for enhancing text compression across diverse data distributions.

</details>


### [15] [Bayesian Evaluation of Large Language Model Behavior](https://arxiv.org/abs/2511.10661)
*Rachel Longjohn,Shang Wu,Saatvik Kher,Catarina Belém,Padhraic Smyth*

Main category: cs.CL

TL;DR: 这篇论文提出了一种贝叶斯方法，用于量化基于大型语言模型（LLM）的文本生成系统在二元评估指标中的不确定性，重点关注由LLM系统中常用的概率文本生成策略引起的不确定性。


<details>
  <summary>Details</summary>
Motivation: 评估基于大型语言模型的文本生成系统的行为（例如，产生有害输出或对对抗性输入的敏感性）越来越重要。现有的评估方法通常忽略统计不确定性量化。

Method: 提出了一种贝叶斯方法，用于量化二元评估指标中的不确定性。该方法考虑了LLM中概率文本生成策略引起的不确定性。

Result: 通过两个案例研究展示了该方法的应用：评估对抗性输入基准上的拒绝率，以及评估开放式交互对话示例基准上一个LLM相对于另一个LLM的成对偏好。结果表明，贝叶斯方法可以提供关于基于LLM的系统行为的有用不确定性量化。

Conclusion: 贝叶斯方法可以有效量化基于LLM的文本生成系统在二元评估指标中的不确定性，为评估LLM的性能提供更全面的信息。

Abstract: It is increasingly important to evaluate how text generation systems based on large language models (LLMs) behave, such as their tendency to produce harmful output or their sensitivity to adversarial inputs. Such evaluations often rely on a curated benchmark set of input prompts provided to the LLM, where the output for each prompt may be assessed in a binary fashion (e.g., harmful/non-harmful or does not leak/leaks sensitive information), and the aggregation of binary scores is used to evaluate the LLM. However, existing approaches to evaluation often neglect statistical uncertainty quantification. With an applied statistics audience in mind, we provide background on LLM text generation and evaluation, and then describe a Bayesian approach for quantifying uncertainty in binary evaluation metrics. We focus in particular on uncertainty that is induced by the probabilistic text generation strategies typically deployed in LLM-based systems. We present two case studies applying this approach: 1) evaluating refusal rates on a benchmark of adversarial inputs designed to elicit harmful responses, and 2) evaluating pairwise preferences of one LLM over another on a benchmark of open-ended interactive dialogue examples. We demonstrate how the Bayesian approach can provide useful uncertainty quantification about the behavior of LLM-based systems.

</details>


### [16] [Evaluating Modern Large Language Models on Low-Resource and Morphologically Rich Languages:A Cross-Lingual Benchmark Across Cantonese, Japanese, and Turkish](https://arxiv.org/abs/2511.10664)
*Chengxuan Xia,Qianye Wu,Hongbin Guan,Sixuan Tian,Yilun Hao,Xiaoyu Wu*

Main category: cs.CL

TL;DR: 评估大型语言模型在粤语、日语和土耳其语等低资源语言上的表现，揭示了模型在文化理解和形态泛化方面仍存在差距。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在英语等高资源语言中表现出色，但在低资源和形态丰富的语言中的有效性尚未得到充分探索。

Method: 使用包含开放域问答、文档摘要、英译X和文化对话四个任务的跨语言基准，对七个先进的LLM进行了综合评估。结合人工评估（流畅度、事实准确性和文化适当性）和自动指标（例如，BLEU，ROUGE）来评估模型性能。

Result: GPT-4o、GPT-4、Claude 3.5等大型专有模型通常在各种语言和任务中处于领先地位，但在文化细微的理解和形态泛化方面仍然存在显着差距。所有模型都在一定程度上难以应对每种语言的独特语言挑战，例如土耳其语的粘着形态和粤语口语。较小的开源模型（LLaMA-2 13B，Mistral 7B）在流畅性和准确性方面明显滞后，突显了资源差距。

Conclusion: 结果表明，尽管大型专有模型表现领先，但在文化理解和形态泛化方面仍有差距。较小的开源模型在流畅性和准确性方面滞后。发布了基准和评估数据，以促进可重复性和进一步研究。

Abstract: Large language models (LLMs) have achieved impressive results in high-resource languages like English, yet their effectiveness in low-resource and morphologically rich languages remains underexplored. In this paper, we present a comprehensive evaluation of seven cutting-edge LLMs -- including GPT-4o, GPT-4, Claude~3.5~Sonnet, LLaMA~3.1, Mistral~Large~2, LLaMA-2~Chat~13B, and Mistral~7B~Instruct -- on a new cross-lingual benchmark covering \textbf{Cantonese, Japanese, and Turkish}. Our benchmark spans four diverse tasks: open-domain question answering, document summarization, English-to-X translation, and culturally grounded dialogue. We combine \textbf{human evaluations} (rating fluency, factual accuracy, and cultural appropriateness) with automated metrics (e.g., BLEU, ROUGE) to assess model performance.
  Our results reveal that while the largest proprietary models (GPT-4o, GPT-4, Claude~3.5) generally lead across languages and tasks, significant gaps persist in culturally nuanced understanding and morphological generalization. Notably, GPT-4o demonstrates robust multilingual performance even on cross-lingual tasks, and Claude~3.5~Sonnet achieves competitive accuracy on knowledge and reasoning benchmarks. However, all models struggle to some extent with the unique linguistic challenges of each language, such as Turkish agglutinative morphology and Cantonese colloquialisms. Smaller open-source models (LLaMA-2~13B, Mistral~7B) lag substantially in fluency and accuracy, highlighting the resource disparity. We provide detailed quantitative results, qualitative error analysis, and discuss implications for developing more culturally aware and linguistically generalizable LLMs. Our benchmark and evaluation data are released to foster reproducibility and further research.

</details>


### [17] [Guarding the Meaning: Self-Supervised Training for Semantic Robustness in Guard Models](https://arxiv.org/abs/2511.10665)
*Cristina Pinneri,Christos Louizos*

Main category: cs.CL

TL;DR: 这篇论文提出了一种自监督框架，用于提高LLM安全guard模型的语义鲁棒性，通过利用释义集强制预测一致性，并采用一种新的偏斜感知聚合策略。


<details>
  <summary>Details</summary>
Motivation: 现有guard模型对语言的表面变化很敏感，即使是保持含义的释义也会导致安全评分的大幅波动，缺乏语义基础。

Method: 该方法利用释义集，通过一种新颖的、偏斜感知的聚合策略来增强预测一致性，以此提高guard模型的语义鲁棒性。

Result: 该方法将释义的语义变异性降低了约58%，基准精度平均提高了约2.5%，并且可以推广到未知的文体变异。

Conclusion: 将语义一致性作为首要训练目标具有重要价值，并为构建更可靠的guard模型提供了一种可扩展的方法。

Abstract: Guard models are a critical component of LLM safety, but their sensitivity to superficial linguistic variations remains a key vulnerability. We show that even meaning-preserving paraphrases can cause large fluctuations in safety scores, revealing a lack of semantic grounding. To address this, we introduce a practical, self-supervised framework for improving the semantic robustness of guard models. Our method leverages paraphrase sets to enforce prediction consistency using a novel, skew-aware aggregation strategy for robust target computation. Notably, we find that standard aggregation methods like mean and median can degrade safety, underscoring the need for skew-aware alternatives. We analyze six open-source guard models and show that our approach reduces semantic variability across paraphrases by ~58%, improves benchmark accuracy by ~2.5% on average, and generalizes to unseen stylistic variations. Intriguingly, we discover a bidirectional relationship between model calibration and consistency: our robustness training improves calibration by up to 40%, revealing a fundamental connection between these properties. These results highlight the value of treating semantic consistency as a first-class training objective and provide a scalable recipe for building more reliable guard models.

</details>


### [18] [Evaluating LLM Understanding via Structured Tabular Decision Simulations](https://arxiv.org/abs/2511.10667)
*Sichao Li,Xinyue Xu,Xiaomeng Li*

Main category: cs.CL

TL;DR: 论文介绍了一种新的评估大型语言模型（LLM）理解能力的方法，通过模拟专家决策场景来考察LLM在多个实例和领域中做出一致、有据可循的决策的能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型 (LLM) 通常在预测准确性方面表现出色，但准确性并不意味着真正的理解。真正的LLM理解需要跨多个实例和领域做出一致的、有充分根据的决策，并依赖于相关的和领域相关的决策因素。

Method: 论文引入了结构化表格决策模拟（STaDS），这是一套类似专家的决策环境，可以评估LLM，就像它们是参加结构化决策“考试”的专业人士一样。通过问题理解、知识预测和对相关决策因素的依赖来评估理解能力。

Result: 通过分析 9 个前沿 LLM 在 15 个不同的决策环境中的表现，发现 (a) 大多数模型难以在不同的领域中实现持续强大的准确性；(b) 模型可能是准确的，但全局上不忠实，并且在陈述的理由和驱动预测的因素之间经常存在不匹配。

Conclusion: 研究结果强调了对全局理解评估协议的需求，并提倡超越准确性的新框架，以提高 LLM 的理解能力。

Abstract: Large language models (LLMs) often achieve impressive predictive accuracy, yet correctness alone does not imply genuine understanding. True LLM understanding, analogous to human expertise, requires making consistent, well-founded decisions across multiple instances and diverse domains, relying on relevant and domain-grounded decision factors. We introduce Structured Tabular Decision Simulations (STaDS), a suite of expert-like decision settings that evaluate LLMs as if they were professionals undertaking structured decision ``exams''. In this context, understanding is defined as the ability to identify and rely on the correct decision factors, features that determine outcomes within a domain. STaDS jointly assesses understanding through: (i) question and instruction comprehension, (ii) knowledge-based prediction, and (iii) reliance on relevant decision factors. By analyzing 9 frontier LLMs across 15 diverse decision settings, we find that (a) most models struggle to achieve consistently strong accuracy across diverse domains; (b) models can be accurate yet globally unfaithful, and there are frequent mismatches between stated rationales and factors driving predictions. Our findings highlight the need for global-level understanding evaluation protocols and advocate for novel frameworks that go beyond accuracy to enhance LLMs' understanding ability.

</details>


### [19] [Forecasting Spoken Language Development in Children with Cochlear Implants Using Preimplantation MRI](https://arxiv.org/abs/2511.10669)
*Yanlin Wang,Di Yuan,Shani Dettman,Dawn Choo,Emily Shimeng Xu,Denise Thomas,Maura E Ryan,Patrick C M Wong,Nancy M Young*

Main category: cs.CL

TL;DR: 本研究旨在比较传统机器学习 (ML) 和深度迁移学习 (DTL) 算法在预测双侧 SNHL 患儿 CI 术后口语发展方面的准确性。


<details>
  <summary>Details</summary>
Motivation: CI 显著改善了重度至极重度 SNHL 患儿的口语能力，但其结果的变异性仍高于听力正常的儿童。对于个体儿童，无法使用植入年龄或残余听力来可靠地预测这种变异性。

Method: 使用基于大脑神经解剖学特征的传统 ML 和 DTL 学习的预测模型。

Result: 使用基于双线性注意力融合策略的 DTL 预测模型实现了 92.39% 的准确率（95% CI，90.70%-94.07%），91.22% 的敏感性（95% CI，89.98%-92.47%），93.56% 的特异性（95% CI，90.91%-96.21%），以及 0.977 的曲线下面积 (AUC)（95% CI，0.969-0.986）。在所有结果指标中，DTL 的表现均优于传统 ML 模型。

Conclusion: 结果支持了使用单一 DTL 预测模型对全球 CI 项目服务的儿童进行语言预测的可行性。

Abstract: Cochlear implants (CI) significantly improve spoken language in children with severe-to-profound sensorineural hearing loss (SNHL), yet outcomes remain more variable than in children with normal hearing. This variability cannot be reliably predicted for individual children using age at implantation or residual hearing. This study aims to compare the accuracy of traditional machine learning (ML) to deep transfer learning (DTL) algorithms to predict post-CI spoken language development of children with bilateral SNHL using a binary classification model of high versus low language improvers. A total of 278 implanted children enrolled from three centers. The accuracy, sensitivity and specificity of prediction models based upon brain neuroanatomic features using traditional ML and DTL learning. DTL prediction models using bilinear attention-based fusion strategy achieved: accuracy of 92.39% (95% CI, 90.70%-94.07%), sensitivity of 91.22% (95% CI, 89.98%-92.47%), specificity of 93.56% (95% CI, 90.91%-96.21%), and area under the curve (AUC) of 0.977 (95% CI, 0.969-0.986). DTL outperformed traditional ML models in all outcome measures. DTL was significantly improved by direct capture of discriminative and task-specific information that are advantages of representation learning enabled by this approach over ML. The results support the feasibility of a single DTL prediction model for language prediction of children served by CI programs worldwide.

</details>


### [20] [Towards Fine-Grained Code-Switch Speech Translation with Semantic Space Alignment](https://arxiv.org/abs/2511.10670)
*Yan Gao,Yazheng Yang,Zhibin Lan,Yidong Chen,Min Zhang,Daimeng Wei,Hui Huang,Jinsong Su*

Main category: cs.CL

TL;DR: 提出了一种使用混合专家（MoE）语音投影器增强大型语言模型（LLM）的 code-switching (CS) 语音翻译 (ST) 方法，并采用多阶段训练范式和过渡损失来解决语义建模复杂性和 CS 数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 先前研究依赖模型隐式学习语义建模，并依赖低效且昂贵的手动标注。为了克服这些局限性。

Method: 使用混合专家（MoE）语音投影器增强大型语言模型（LLM），每个专家负责特定语言的语义子空间。采用多阶段训练范式，利用单语 ASR 和单语 ST 数据进行语音-文本对齐，并引入语言特定损失和组内负载均衡损失来指导 MoE 语音投影器。使用过渡损失来平滑不同阶段之间的数据过渡。

Result: 在广泛使用的数据集上的大量实验表明了该方法的有效性和通用性。

Conclusion: 该方法有效地解决了 CS 语音翻译中的语义建模复杂性和数据稀缺问题。

Abstract: Code-switching (CS) speech translation (ST) refers to translating speech that alternates between two or more languages into a target language text, which poses significant challenges due to the complexity of semantic modeling and the scarcity of CS data. Previous studies tend to rely on the model itself to implicitly learn semantic modeling during training, and resort to inefficient and costly manual annotations for these two challenges. To mitigate these limitations, we propose enhancing Large Language Models (LLMs) with a Mixture of Experts (MoE) speech projector, where each expert specializes in the semantic subspace of a specific language, enabling fine-grained modeling of speech features. Additionally, we introduce a multi-stage training paradigm that utilizes readily available monolingual automatic speech recognition (ASR) and monolingual ST data, facilitating speech-text alignment and improving translation capabilities. During training, we leverage a combination of language-specific loss and intra-group load balancing loss to guide the MoE speech projector in efficiently allocating tokens to the appropriate experts, across expert groups and within each group, respectively. To bridge the data gap across different training stages and improve adaptation to the CS scenario, we further employ a transition loss, enabling smooth transitions of data between stages, to effectively address the scarcity of high-quality CS speech translation data. Extensive experiments on widely used datasets demonstrate the effectiveness and generality of our approach.

</details>


### [21] [Grounded Visual Factualization: Factual Anchor-Based Finetuning for Enhancing MLLM Factual Consistency](https://arxiv.org/abs/2511.10671)
*Filippo Morbiato,Luca Romano,Alessandro Persona*

Main category: cs.CL

TL;DR: 提出了一个名为GVF Finetuning的新方法，以增强MLLM视觉事实一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的微调方法在改进多模态大型语言模型 (MLLM) 的视觉幻觉方面效果有限，无法深入干预事实推理。

Method: GVF集成了显式事实信号，通过三个核心机制：事实锚点数据增强、事实感知指令调优和事实一致性损失函数。

Result: 在 LLaVA-1.5-13B 上评估，GVF Finetuning 在 VHTest 基准测试中显著优于标准微调，同时保持或略微提高了在 MME 和 POPE 等通用多模态基准测试中的性能。

Conclusion: GVF Finetuning 有效地缓解了视觉幻觉，而不会影响一般的理解和推理能力。

Abstract: Visual hallucination, where Multimodal Large Language Models fabricate details inconsistent with image content, critically undermines their reliability. Existing fine-tuning methods offer limited improvement, failing to deeply intervene in factual reasoning. This paper introduces Grounded Visual Factualization (GVF) Finetuning, a novel approach to systematically enhance MLLM visual factual consistency. GVF integrates explicit factual signals via three core mechanisms: Factual Anchor Data Augmentation, enriching training data with structured factual anchors and counter-factual prompts; Fact-Aware Instruction Tuning, embedding these cues into explicit instructions; and a Factual Consistency Loss function, specifically penalizing factual inaccuracies. Evaluated on LLaVA-1.5-13B, GVF Finetuning significantly outperforms standard fine-tuning on the VHTest benchmark for both Open-Ended Question (OEQ) and Yes/No Question (YNQ) formats. Crucially, GVF maintains or even slightly improves performance on general multimodal benchmarks like MME and POPE, demonstrating effective mitigation of visual hallucinations without compromising general understanding and reasoning abilities.

</details>


### [22] [Large language models in materials science and the need for open-source approaches](https://arxiv.org/abs/2511.10673)
*Fengxu Yang,Weitong Chen,Jack D. Evans*

Main category: cs.CL

TL;DR: 大型语言模型正在迅速改变材料科学。本综述考察了 LLM 在材料发现流程中的应用，重点关注三个关键领域：挖掘科学文献、预测建模和多智能体实验系统。


<details>
  <summary>Details</summary>
Motivation: 强调 LLM 如何从文本中提取有价值的信息（例如合成条件），学习结构-性质关系，并且可以协调集成计算工具和实验室自动化的智能体系统。

Method: 着重介绍了 LLM 在材料发现流程中的应用，重点关注三个关键领域：挖掘科学文献、预测建模和多智能体实验系统。

Result: 结果表明，开源替代方案可以匹配性能，同时提供更高的透明度、可重复性、成本效益和数据隐私。

Conclusion: 随着开源模型的不断改进，我们提倡更广泛地采用它们来构建可访问、灵活和社区驱动的科学发现 AI 平台。

Abstract: Large language models (LLMs) are rapidly transforming materials science. This review examines recent LLM applications across the materials discovery pipeline, focusing on three key areas: mining scientific literature , predictive modelling, and multi-agent experimental systems. We highlight how LLMs extract valuable information such as synthesis conditions from text, learn structure-property relationships, and can coordinate agentic systems integrating computational tools and laboratory automation. While progress has been largely dependent on closed-source commercial models, our benchmark results demonstrate that open-source alternatives can match performance while offering greater transparency, reproducibility, cost-effectiveness, and data privacy. As open-source models continue to improve, we advocate their broader adoption to build accessible, flexible, and community-driven AI platforms for scientific discovery.

</details>


### [23] [Pre-Attention Expert Prediction and Prefetching for Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2511.10676)
*Shien Zhu,Samuel Bohl,Robin Oester,Gustavo Alonso*

Main category: cs.CL

TL;DR: 提出了一种名为“pre-attention expert prediction”的专家预取方法，以提高MoE LLM的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有专家预测方法准确率低，且未优化第一层；复杂预测方法计算开销大。

Method: 利用LLM中排序保持的特性，使用注意力块之前的激活值和线性函数进行专家预测。

Result: 在DeepSeek V2 Lite、Qwen3-30B和Phi-mini-MoE上实现了93.03%、94.69%和97.62%的准确率，比现有方法提高了约15%。

Conclusion: 提出的pre-attention expert routers方法实现了准确且轻量级的专家预取。

Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) efficiently scale-up the model while keeping relatively low inference cost. As MoE models only activate part of the experts, related work has proposed expert prediction and caching methods to prefetch the experts for faster inference. However, existing approaches utilize the activations from the previous layer for prediction, incurring low accuracy and leave the first layer unoptimized. Applying complex layers or even training standalone networks for better prediction introduces high computation overhead. In this paper, we propose pre-attention expert prediction to achieve accurate and lightweight expert prefetching. The key insight is that some functions in LLMs are ranking-preserving, indicating that matching the ranking of selected experts using simple linear functions is possible. Therefore, we utilize the activations before the attention block in the same layer with 2 linear functions and ranking-aware loss to achieve accurate prediction, which also supports prefetching in the first layer. Our lightweight, pre-attention expert routers achieve 93.03% accuracy on DeepSeek V2 Lite, 94.69% on Qwen3-30B, and 97.62% on Phi-mini-MoE, showing about 15% improvement on absolute accuracy over the state-of-the-art methods.

</details>


### [24] [SpiderGen: Towards Procedure Generation For Carbon Life Cycle Assessments with Generative AI](https://arxiv.org/abs/2511.10684)
*Anupama Sitaraman,Bharathan Balaji,Yuvraj Agarwal*

Main category: cs.CL

TL;DR: SpiderGen是一个基于LLM的工作流程，它集成了传统LCA的分类和方法，以及LLM的推理能力和世界知识，以生成用于LCA的程序信息。


<details>
  <summary>Details</summary>
Motivation: 研究温室气体排放引起的气候变化和全球变暖的影响已成为全球关注的首要问题。这些排放主要来自消费品的生产、使用和处置。因此，构建工具来评估消费品的环境影响非常重要，其中必不可少的一部分是进行生命周期评估（LCA）。

Method: 我们提出了SpiderGen，这是一个基于LLM的工作流程，它集成了传统LCA的分类和方法，以及LLM的推理能力和世界知识，以生成用于LCA的程序信息。

Result: 我们发现SpiderGen提供了准确的LCA过程信息，这些信息要么完全正确，要么有轻微的错误，在10个样本数据点上的F1得分为62%。

Conclusion: 我们证明，与现状LCA相比，SpiderGen能够以低于1美元的价格在10分钟内生成LCA过程信息，从而有可能减少估计碳影响的人力成本，而现状LCA的成本可能超过25000美元，并且需要多达21人天。

Abstract: Investigating the effects of climate change and global warming caused by GHG emissions have been a primary concern worldwide. These emissions are largely contributed to by the production, use and disposal of consumer products. Thus, it is important to build tools to estimate the environmental impact of consumer goods, an essential part of which is conducting Life Cycle Assessments (LCAs). LCAs specify and account for the appropriate processes involved with the production, use, and disposal of the products. We present SpiderGen, an LLM-based workflow which integrates the taxonomy and methodology of traditional LCA with the reasoning capabilities and world knowledge of LLMs to generate the procedural information used for LCA. We additionally evaluate the output of SpiderGen using real-world LCA documents as ground-truth. We find that SpiderGen provides accurate LCA process information that is either fully correct or has minor errors, achieving an F1-Score of 62% across 10 sample data points. We observe that the remaining missed processes and hallucinated errors occur primarily due to differences in detail between LCA documents, as well as differences in the "scope" of which auxiliary processes must also be included. We also demonstrate that SpiderGen performs better than several baselines techniques, such as chain-of-thought prompting and one-shot prompting. Finally, we highlight SpiderGen's potential to reduce the human effort and costs for estimating carbon impact, as it is able to produce LCA process information for less than \$1 USD in under 10 minutes as compared to the status quo LCA, which can cost over \$25000 USD and take up to 21-person days.

</details>


### [25] [A methodological analysis of prompt perturbations and their effect on attack success rates](https://arxiv.org/abs/2511.10686)
*Tiago Machado,Maysa Malfiza Garcia de Macedo,Rogerio Abreu de Paula,Marcelo Carpinette Grave,Aminat Adebiyi,Luan Soares de Souza,Enrico Santarelli,Claudio Pinhanez*

Main category: cs.CL

TL;DR: 研究不同LLM对齐方法对模型对提示攻击的反应的影响。


<details>
  <summary>Details</summary>
Motivation: 旨在调查不同的LLM对齐方法如何影响模型对提示攻击的反应。

Method: 使用统计方法系统地分析攻击成功率（ASR）在应用于旨在从LLM引出不当内容的提示的变化时的敏感程度。

Result: 结果表明，即使是小的提示修改也可以根据我们运行的统计测试显着改变攻击成功率（ASR），使模型更容易或更不容易受到攻击类型的影响。

Conclusion: 对不同对齐方法进行系统和基于统计的分析，以及它们的ASR对提示变化的敏感程度，从而为正在进行中的模型攻击评估工作做出贡献。

Abstract: This work aims to investigate how different Large Language Models (LLMs) alignment methods affect the models' responses to prompt attacks. We selected open source models based on the most common alignment methods, namely, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reinforcement Learning with Human Feedback (RLHF). We conducted a systematic analysis using statistical methods to verify how sensitive the Attack Success Rate (ASR) is when we apply variations to prompts designed to elicit inappropriate content from LLMs. Our results show that even small prompt modifications can significantly change the Attack Success Rate (ASR) according to the statistical tests we run, making the models more or less susceptible to types of attack. Critically, our results demonstrate that running existing 'attack benchmarks' alone may not be sufficient to elicit all possible vulnerabilities of both models and alignment methods. This paper thus contributes to ongoing efforts on model attack evaluation by means of systematic and statistically-based analyses of the different alignment methods and how sensitive their ASR is to prompt variation.

</details>


### [26] [Modeling and Predicting Multi-Turn Answer Instability in Large Language Models](https://arxiv.org/abs/2511.10688)
*Jiahang He,Rishi Ramachandran,Neel Ramachandran,Aryan Katakam,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Aryan Shrivastava*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）在广泛应用中普及，用户与模型的交互日益频繁。论文评估了LLM的鲁棒性，发现模型在多轮对话中存在明显脆弱性，例如，简单提示“再想想”会导致模型准确率下降。通过马尔可夫链对模型准确率进行建模，并发现线性探针可以预测答案的变化。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在多轮交互中的鲁棒性，因为这对于实际应用至关重要。

Method: 使用简单的多轮追问提示来评估模型的答案变化，利用马尔科夫链分析模型在多轮对话中的准确率动态，并研究线性探针是否可以预测这些变化。

Result: 发现LLM的鲁棒性存在明显漏洞：简单的“再想想”提示导致Gemini 1.5 Flash的准确率下降约10%，而将此提示与语义等价的改述问题结合使用，导致Claude 3.5 Haiku的准确率下降7.5%。此外，可以使用马尔可夫链有效地对多轮对话中的模型准确率进行建模，从而能够预测一段时间内的准确率概率。线性探针可以帮助预测未来的答案变化。

Conclusion: 研究结果表明，静态准确率是交互环境中一个重要的鲁棒性指标，并揭示了模型在重复提问下的脆弱性。解决这种不稳定性对于在高风险和交互式环境中部署LLM至关重要，因为在这些环境中，一致的推理与初始准确性同样重要。

Abstract: As large language models (LLMs) are adopted in an increasingly wide range of applications, user-model interactions have grown in both frequency and scale. Consequently, research has focused on evaluating the robustness of LLMs, an essential quality for real-world tasks. In this paper, we employ simple multi-turn follow-up prompts to evaluate models' answer changes, model accuracy dynamics across turns with Markov chains, and examine whether linear probes can predict these changes. Our results show significant vulnerabilities in LLM robustness: a simple "Think again" prompt led to an approximate 10% accuracy drop for Gemini 1.5 Flash over nine turns, while combining this prompt with a semantically equivalent reworded question caused a 7.5% drop for Claude 3.5 Haiku. Additionally, we find that model accuracy across turns can be effectively modeled using Markov chains, enabling the prediction of accuracy probabilities over time. This allows for estimation of the model's stationary (long-run) accuracy, which we find to be on average approximately 8% lower than its first-turn accuracy for Gemini 1.5 Flash. Our results from a model's hidden states also reveal evidence that linear probes can help predict future answer changes. Together, these results establish stationary accuracy as a principled robustness metric for interactive settings and expose the fragility of models under repeated questioning. Addressing this instability will be essential for deploying LLMs in high-stakes and interactive settings where consistent reasoning is as important as initial accuracy.

</details>


### [27] [Equilibrium Dynamics and Mitigation of Gender Bias in Synthetically Generated Data](https://arxiv.org/abs/2511.10689)
*Ashish Kattamuri,Arpita Vats,Harshwardhan Fartale,Rahul Raja,Akshata Kishore Moharir,Ishita Prasad*

Main category: cs.CL

TL;DR: 大型语言模型的递归提示可以扩展合成数据集的生成，但也存在偏差放大风险。本文研究了使用三种互补评估框架（基于规则的模式匹配、基于嵌入的语义相似性和下游任务性能）在三代递归文本生成中性别偏差的动态变化。实验表明，低初始偏差会放大到模型的固有偏差水平（+36%），而高初始偏差会衰减到该水平（-26%）。对比增强是一种有效的缓解方法，尽管会产生更高的基于嵌入的偏差分数，但仍能显着降低下游偏差。


<details>
  <summary>Details</summary>
Motivation: 研究递归提示生成合成数据集中的性别偏见问题及其放大效应。

Method: 使用三种互补评估框架（基于规则的模式匹配、基于嵌入的语义相似性和下游任务性能）进行三代递归文本生成实验，并考察不同初始偏差水平和四种缓解策略下的偏差动态。

Result: 低初始偏差会放大到模型的固有偏差水平，而高初始偏差会衰减到该水平。对比增强方法在降低下游偏差方面表现出色，但可能导致更高的embedding-based偏差分数。

Conclusion: 语义相似性指标可能与行为公平性结果不符，负责任的合成数据生成需要多维度评估。

Abstract: Recursive prompting with large language models enables scalable synthetic dataset generation but introduces the risk of bias amplification. We investigate gender bias dynamics across three generations of recursive text generation using three complementary evaluation frameworks: rule-based pattern matching, embedding-based semantic similarity, and downstream task performance. Experiments with three initial bias levels (0.1, 0.3, 0.6) and four mitigation strategies reveal equilibrium dynamics rather than monotonic amplification. The low initial bias amplifies toward the model's inherent bias level (+36%), whereas the high initial bias decays toward it (-26%). Among mitigation methods, contrastive augmentation, which introduces gender-swapped variants, achieves significant downstream bias reduction (98.8% for low initial bias and 91% on average) despite producing higher embedding-based bias scores. This paradox demonstrates that semantic similarity metrics may diverge from behavioral fairness outcomes, highlighting the need for multidimensional evaluation in responsible synthetic data generation.

</details>


### [28] [Saying the Unsaid: Revealing the Hidden Language of Multimodal Systems Through Telephone Games](https://arxiv.org/abs/2511.10690)
*Juntu Zhao,Jialing Zhang,Chongxuan Li,Dequan Wang*

Main category: cs.CL

TL;DR: 本文通过观察多模态系统在多轮“电话游戏”中概念共现频率的变化，来研究其隐藏的语言和概念连接强度。


<details>
  <summary>Details</summary>
Motivation: 目前闭源多模态系统取得了很大进展，但由于其黑盒架构，其用于理解世界的隐藏语言仍然不透明。

Method: 利用多轮“电话游戏”策略性地利用系统的偏好偏差，并构建包含10,000+概念对的Telescope数据集。

Result: 量化研究了多模态系统理解中的概念连接强度，揭示了从训练中继承的偏好偏差，评估了泛化能力，并发现了更稳定的概念连接路径。

Conclusion: 本研究为多模态系统的隐藏语言提供了一个新的视角，并为未来多模态系统的可解释性和可控性研究奠定了基础。

Abstract: Recent closed-source multimodal systems have made great advances, but their hidden language for understanding the world remains opaque because of their black-box architectures. In this paper, we use the systems' preference bias to study their hidden language: During the process of compressing the input images (typically containing multiple concepts) into texts and then reconstructing them into images, the systems' inherent preference bias introduces specific shifts in the outputs, disrupting the original input concept co-occurrence. We employ the multi-round "telephone game" to strategically leverage this bias. By observing the co-occurrence frequencies of concepts in telephone games, we quantitatively investigate the concept connection strength in the understanding of multimodal systems, i.e., "hidden language." We also contribute Telescope, a dataset of 10,000+ concept pairs, as the database of our telephone game framework. Our telephone game is test-time scalable: By iteratively running telephone games, we can construct a global map of concept connections in multimodal systems' understanding. Here we can identify preference bias inherited from training, assess generalization capability advancement, and discover more stable pathways for fragile concept connections. Furthermore, we use Reasoning-LLMs to uncover unexpected concept relationships that transcend textual and visual similarities, inferring how multimodal systems understand and simulate the world. This study offers a new perspective on the hidden language of multimodal systems and lays the foundation for future research on the interpretability and controllability of multimodal systems.

</details>


### [29] [Evaluating from Benign to Dynamic Adversarial: A Squid Game for Large Language Models](https://arxiv.org/abs/2511.10691)
*Zijian Chen,Wenjun Zhang,Guangtao Zhai*

Main category: cs.CL

TL;DR: 这篇论文介绍了一个新的动态对抗评估环境 Squid Game，用于评估大型语言模型（LLMs）。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试无法跟上LLMs的发展，并且存在数据污染问题，同时缺乏对压力下LLMs行为的探索。

Method: Squid Game 包含六个淘汰赛级别的游戏，侧重于指令跟随、代码、推理、规划和安全对齐等多方面的能力。通过让LLMs相互对抗进行评估。

Result: 对 50 多个 LLMs 进行了评估，观察到同一模型谱系中性能的明显世代相变，并发现一些模型为了获胜而采取了推测性的捷径。

Conclusion: 动态评估可以作为静态评估的补充。

Abstract: Contemporary benchmarks are struggling to keep pace with the development of large language models (LLMs). Although they are indispensable to evaluate model performance on various tasks, it is uncertain whether the models trained on Internet data have genuinely learned how to solve problems or merely seen the questions before. This potential data contamination issue presents a fundamental challenge to establishing trustworthy evaluation frameworks. Meanwhile, existing benchmarks predominantly assume benign, resource-rich settings, leaving the behavior of LLMs under pressure unexplored. In this paper, we introduce Squid Game, a dynamic and adversarial evaluation environment with resource-constrained and asymmetric information settings elaborated to evaluate LLMs through interactive gameplay against other LLM opponents. Notably, Squid Game consists of six elimination-style levels, focusing on multi-faceted abilities, such as instruction-following, code, reasoning, planning, and safety alignment. We evaluate over 50 LLMs on Squid Game, presenting the largest behavioral evaluation study of general LLMs on dynamic adversarial scenarios. We observe a clear generational phase transition on performance in the same model lineage and find evidence that some models resort to speculative shortcuts to win the game, indicating the possibility of higher-level evaluation paradigm contamination in static benchmarks. Furthermore, we compare prominent LLM benchmarks and Squid Game with correlation analyses, highlighting that dynamic evaluation can serve as a complementary part for static evaluations. The code and data will be released in the future.

</details>


### [30] [Do AI Voices Learn Social Nuances? A Case of Politeness and Speech Rate](https://arxiv.org/abs/2511.10693)
*Eyal Rabin,Zohar Elyoseph,Rotem Israel-Fishelson,Adi Dali,Ravit Nussinson*

Main category: cs.CL

TL;DR: AI语音系统在表达礼貌时会降低语速，模仿人类的语调。


<details>
  <summary>Details</summary>
Motivation: 探讨AI语音是否能学习并运用人类社交中的非显式线索，例如通过降低语速来表达礼貌。

Method: 要求来自AI Studio和OpenAI的22个合成语音在“礼貌正式”和“随意非正式”条件下朗读固定文本，并测量语速。

Result: 在两个AI平台中，“礼貌”提示下的语速均低于“随意”提示，且AI Studio的所有声音和OpenAI的大部分声音都表现出统计显著性。

Conclusion: AI能够隐式地学习和复制人类交流中的心理细微差别，并可能强化人类社交规范。

Abstract: Voice-based artificial intelligence is increasingly expected to adhere to human social conventions, but can it learn implicit cues that are not explicitly programmed? This study investigates whether state-of-the-art text-to-speech systems have internalized the human tendency to reduce speech rate to convey politeness - a non-obvious prosodic marker. We prompted 22 synthetic voices from two leading AI platforms (AI Studio and OpenAI) to read a fixed script under both "polite and formal" and "casual and informal" conditions and measured the resulting speech duration. Across both AI platforms, the polite prompt produced slower speech than the casual prompt with very large effect sizes, an effect that was statistically significant for all of AI Studio's voices and for a large majority of OpenAI's voices. These results demonstrate that AI can implicitly learn and replicate psychological nuances of human communication, highlighting its emerging role as a social actor capable of reinforcing human social norms.

</details>


### [31] [Where does an LLM begin computing an instruction?](https://arxiv.org/abs/2511.10694)
*Aditya Pola,Vineeth N. Balasubramanian*

Main category: cs.CL

TL;DR: 该论文研究了指令跟随任务中，模型从阅读内容过渡到执行指令的起始层位置。


<details>
  <summary>Details</summary>
Motivation: 探究指令跟随任务在模型层级结构中的起始位置，即阅读内容到执行指令的转变点。

Method: 通过构建三个简单数据集（Key-Value、Quote Attribution、Letter Selection）以及它们的二跳组合，并使用激活修补技术，测量层级的翻转率，以确定当替换选定的残差激活改变预测答案时对应的层。

Result: 观察到Llama系列模型中存在一个拐点（称为起始点），在该点之后，改变预测的干预措施变得无效。多跳组合任务也显示出相似的起始点位置。

Conclusion: 该研究提供了一种简单可复制的方法来定位指令跟随的起始位置，并比较不同任务和模型大小下的起始位置。

Abstract: Following an instruction involves distinct sub-processes, such as reading content, reading the instruction, executing it, and producing an answer. We ask where, along the layer stack, instruction following begins, the point where reading gives way to doing. We introduce three simple datasets (Key-Value, Quote Attribution, Letter Selection) and two hop compositions of these tasks. Using activation patching on minimal-contrast prompt pairs, we measure a layer-wise flip rate that indicates when substituting selected residual activations changes the predicted answer. Across models in the Llama family, we observe an inflection point, which we term onset, where interventions that change predictions before this point become largely ineffective afterward. Multi-hop compositions show a similar onset location. These results provide a simple, replicable way to locate where instruction following begins and to compare this location across tasks and model sizes.

</details>


### [32] ["As Eastern Powers, I will veto." : An Investigation of Nation-level Bias of Large Language Models in International Relations](https://arxiv.org/abs/2511.10695)
*Jonghyeon Choi,Yeonjun Choi,Hyun-chul Kim,Beakcheol Jang*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）在国际关系（IR）中表现出的国家层面偏见，并提出了一个去偏见框架，该框架结合了检索增强生成和基于反射的自我反思技术，以减少国家层面的偏见并提高性能。


<details>
  <summary>Details</summary>
Motivation: 在国际关系领域应用LLM时，需要评估国家层面的偏见以及性能。

Method: 利用联合国安理会（UNSC）的历史记录，开发了一个偏见评估框架，包括三个不同的测试，以探索各种LLM中的国家层面偏见，特别关注UNSC的五个常任理事国。

Result: 实验结果表明，即使模型中存在一般的偏见模式（例如，对西方国家的有利偏见和对俄罗斯的不利偏见），这些偏见仍然因LLM而异。即使在同一LLM中，一个国家的偏见方向和大小也会根据评估环境而变化。具有更强推理能力的模型表现出更少的偏见和更好的性能。所提出的去偏见框架有效地减少了国家层面的偏见，并提高了GPT-4o-mini和LLama-3.3-70B的性能。

Conclusion: LLM的偏见从根本上是多维的，因模型和任务而异。

Abstract: This paper systematically examines nation-level biases exhibited by Large Language Models (LLMs) within the domain of International Relations (IR). Leveraging historical records from the United Nations Security Council (UNSC), we developed a bias evaluation framework comprising three distinct tests to explore nation-level bias in various LLMs, with a particular focus on the five permanent members of the UNSC. Experimental results show that, even with the general bias patterns across models (e.g., favorable biases toward the western nations, and unfavorable biases toward Russia), these still vary based on the LLM. Notably, even within the same LLM, the direction and magnitude of bias for a nation change depending on the evaluation context. This observation suggests that LLM biases are fundamentally multidimensional, varying across models and tasks. We also observe that models with stronger reasoning abilities show reduced bias and better performance. Building on this finding, we introduce a debiasing framework that improves LLMs' factual reasoning combining Retrieval-Augmented Generation with Reflexion-based self-reflection techniques. Experiments show it effectively reduces nation-level bias, and improves performance, particularly in GPT-4o-mini and LLama-3.3-70B. Our findings emphasize the need to assess nation-level bias alongside performance when applying LLMs in the IR domain.

</details>


### [33] [$π$-Attention: Periodic Sparse Transformers for Efficient Long-Context Modeling](https://arxiv.org/abs/2511.10696)
*Dong Liu,Yanxuan Yu*

Main category: cs.CL

TL;DR: 提出了一种新的稀疏Transformer注意力机制，名为\PiAttention，它通过周期性的稀疏结构在降低计算复杂度的同时，扩大了感受野。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力机制（如RingAttention）在处理长序列时，感受野有限且缺乏适应性，影响了建模能力。

Method: 将注意力分解为环状局部邻域、确定性的π步幅跳跃和一个自适应融合门。这种周期性结构提供了对远处token的可预测覆盖。

Result: 在语言建模、检索和视觉语言任务上的实验表明，\PiAttention在保证或超过密集注意力质量的同时，比RingAttention的困惑度降低了8.3%，并且使用的GPU数量减少了50%。

Conclusion: 周期性跳跃、自适应融合以及head级别的稀疏协调对于高效的长上下文建模至关重要。

Abstract: Transformers have revolutionized natural language processing, but their quadratic complexity with respect to sequence length remains a fundamental bottleneck for long-range modeling. While sparse attention mechanisms like RingAttention reduce computational costs by restricting attention to local neighborhoods, they suffer from limited receptive fields and lack of adaptability. We present \PiAttention, a periodic sparse Transformer that factorizes attention into ring-local neighborhoods, deterministic $π$-stride skips, and an adaptive fusion gate. The periodic structure provides predictable coverage of distant tokens, while the sparse footprint keeps the per-layer complexity linear in context length. We prove that \PiAttention achieves $\mathcal{O}(kL + π\log L)$ receptive field growth compared to $\mathcal{O}(kL)$ for RingAttention, where $k$ is the local window size, $π$ is the skip period, and $L$ is the sequence length. Extensive experiments on language modeling, retrieval, and vision-language tasks demonstrate that \PiAttention matches or surpasses dense attention quality with 8.3\% lower perplexity than RingAttention while using 50\% fewer GPUs for the same context length. Our detailed ablations and visualizations reveal the importance of periodic skips, adaptive fusion, and head-level sparsity coordination for efficient long-context modeling.

</details>


### [34] [Faithful Summarization of Consumer Health Queries: A Cross-Lingual Framework with LLMs](https://arxiv.org/abs/2511.10768)
*Ajwad Abrar,Nafisa Tabassum Oeshy,Prianka Maheru,Farzana Tabassum,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: 提出了一种结合TextRank和医学命名实体识别与大型语言模型（LLM）的框架，以提高医疗文本摘要的忠实度。


<details>
  <summary>Details</summary>
Motivation: 总结消费者健康问题（CHQ）可以简化医疗保健中的沟通，但不忠实的总结会歪曲医疗细节，从而带来严重的风险。

Method: 在MeQSum（英语）和BanglaCHQ-Summ（孟加拉语）数据集上微调LLaMA-2-7B模型。

Result: 在质量（ROUGE、BERTScore、可读性）和忠实度（SummaC、AlignScore）指标上取得了一致的改进，并且优于zero-shot基线和先前的系统。超过80％的生成摘要保留了关键的医疗信息。

Conclusion: 强调了忠实度是可靠医疗摘要的一个重要维度，并证明了该方法在医疗保健环境中更安全地部署LLM的潜力。

Abstract: Summarizing consumer health questions (CHQs) can ease communication in healthcare, but unfaithful summaries that misrepresent medical details pose serious risks. We propose a framework that combines TextRank-based sentence extraction and medical named entity recognition with large language models (LLMs) to enhance faithfulness in medical text summarization. In our experiments, we fine-tuned the LLaMA-2-7B model on the MeQSum (English) and BanglaCHQ-Summ (Bangla) datasets, achieving consistent improvements across quality (ROUGE, BERTScore, readability) and faithfulness (SummaC, AlignScore) metrics, and outperforming zero-shot baselines and prior systems. Human evaluation further shows that over 80\% of generated summaries preserve critical medical information. These results highlight faithfulness as an essential dimension for reliable medical summarization and demonstrate the potential of our approach for safer deployment of LLMs in healthcare contexts.

</details>


### [35] [TEDxTN: A Three-way Speech Translation Corpus for Code-Switched Tunisian Arabic - English](https://arxiv.org/abs/2511.10780)
*Fethi Bougares,Salima Mdhaffar,Haroun Elleuch,Yannick Estève*

Main category: cs.CL

TL;DR: 创建了首个公开的突尼斯阿拉伯语到英语的语音翻译数据集TEDxTN。


<details>
  <summary>Details</summary>
Motivation: 旨在缓解多种阿拉伯语方言的数据稀缺障碍。

Method: 收集、分割、转录和翻译了108个TEDx演讲，总计25小时的语音，涵盖来自突尼斯11个不同地区的口音，并制定了内部注释指南。

Result: 发布了注释指南和语料库，报告了语音识别和语音翻译的基线系统结果。

Conclusion: TEDxTN是一个有价值的资源，可以促进突尼斯方言的自然语言处理研究。

Abstract: In this paper, we introduce TEDxTN, the first publicly available Tunisian Arabic to English speech translation dataset. This work is in line with the ongoing effort to mitigate the data scarcity obstacle for a number of Arabic dialects. We collected, segmented, transcribed and translated 108 TEDx talks following our internally developed annotations guidelines. The collected talks represent 25 hours of speech with code-switching that cover speakers with various accents from over 11 different regions of Tunisia. We make the annotation guidelines and corpus publicly available. This will enable the extension of TEDxTN to new talks as they become available. We also report results for strong baseline systems of Speech Recognition and Speech Translation using multiple pre-trained and fine-tuned end-to-end models. This corpus is the first open source and publicly available speech translation corpus of Code-Switching Tunisian dialect. We believe that this is a valuable resource that can motivate and facilitate further research on the natural language processing of Tunisian Dialect.

</details>


### [36] [Assessing the Capabilities of LLMs in Humor:A Multi-dimensional Analysis of Oogiri Generation and Evaluation](https://arxiv.org/abs/2511.09133)
*Ritsu Sakabe,Hwichan Kim,Tosho Hirasawa,Mamoru Komachi*

Main category: cs.CL

TL;DR: 本文通过日本喜剧游戏 Oogiri 的视角，从多个维度评估大型语言模型 (LLM) 的幽默能力。


<details>
  <summary>Details</summary>
Motivation: 以往研究对 LLM 幽默能力的评估往往依赖于单一维度（例如判断是否“有趣”），缺乏对幽默的多方面理解。

Method: 1. 扩展了现有的 Oogiri 数据集，并用 LLM 生成的回复进行了扩充。2. 从新颖性、清晰度、相关性、智力、同理心和整体趣味性六个维度对扩展后的数据集进行了人工标注。3. 使用该数据集评估了 LLM 生成 Oogiri 回复和评估回复趣味性的能力。

Result: LLM 生成回复的能力介于人类的中低水平之间，但明显缺乏同理心。LLM 优先考虑新颖性，而人类优先考虑同理心。

Conclusion: LLM 在同理心方面的不足导致其无法复现人类的幽默评估。该研究发布了带注释的语料库，以促进更具情感智能和复杂的对话代理的开发。

Abstract: Computational humor is a frontier for creating advanced and engaging natural language processing (NLP) applications, such as sophisticated dialogue systems. While previous studies have benchmarked the humor capabilities of Large Language Models (LLMs), they have often relied on single-dimensional evaluations, such as judging whether something is simply ``funny.'' This paper argues that a multifaceted understanding of humor is necessary and addresses this gap by systematically evaluating LLMs through the lens of Oogiri, a form of Japanese improvisational comedy games. To achieve this, we expanded upon existing Oogiri datasets with data from new sources and then augmented the collection with Oogiri responses generated by LLMs. We then manually annotated this expanded collection with 5-point absolute ratings across six dimensions: Novelty, Clarity, Relevance, Intelligence, Empathy, and Overall Funniness. Using this dataset, we assessed the capabilities of state-of-the-art LLMs on two core tasks: their ability to generate creative Oogiri responses and their ability to evaluate the funniness of responses using a six-dimensional evaluation. Our results show that while LLMs can generate responses at a level between low- and mid-tier human performance, they exhibit a notable lack of Empathy. This deficit in Empathy helps explain their failure to replicate human humor assessment. Correlation analyses of human and model evaluation data further reveal a fundamental divergence in evaluation criteria: LLMs prioritize Novelty, whereas humans prioritize Empathy. We release our annotated corpus to the community to pave the way for the development of more emotionally intelligent and sophisticated conversational agents.

</details>


### [37] [Sabiá: Um Chatbot de Inteligência Artificial Generativa para Suporte no Dia a Dia do Ensino Superior](https://arxiv.org/abs/2511.10787)
*Guilherme Biava Rodrigues,Franciele Beal,Marlon Marcon,Alinne Cristinne Corrêa Souza,André Roberto Ortoncelli,Francisco Carlos Monteiro Souza,Rodolfo Adamshuk Silva*

Main category: cs.CL

TL;DR: 学生难以获取日常学术信息，信息分散在机构文件和网站上，导致不清晰和混乱。提出使用GenAI和RAG开发聊天机器人来简化对此类信息的访问。


<details>
  <summary>Details</summary>
Motivation: 学生难以获取日常学术信息，信息分散在机构文件和网站上，导致不清晰和混乱。

Method: 使用GenAI和RAG开发聊天机器人，并测试和评估了多个GenAI模型。

Result: Gemini 2.0 Flash因其质量和速度而脱颖而出，而Gemma 3n因其良好的性能和开源性质而脱颖而出。

Conclusion: 该项目提出使用GenAI和RAG来简化学生获取日常学术信息。

Abstract: Students often report difficulties in accessing day-to-day academic information, which is usually spread across numerous institutional documents and websites. This fragmentation results in a lack of clarity and causes confusion about routine university information. This project proposes the development of a chatbot using Generative Artificial Intelligence (GenAI) and Retrieval-Augmented Generation (RAG) to simplify access to such information. Several GenAI models were tested and evaluated based on quality metrics and the LLM-as-a-Judge approach. Among them, Gemini 2.0 Flash stood out for its quality and speed, and Gemma 3n for its good performance and open-source nature.

</details>


### [38] [LLM-as-a-Grader: Practical Insights from Large Language Model for Short-Answer and Report Evaluation](https://arxiv.org/abs/2511.10819)
*Grace Byun,Swati Rajwal,Jinho D. Choi*

Main category: cs.CL

TL;DR: 本研究探讨了使用大型语言模型（LLM，GPT-4o）在本科计算语言学课程中评估简答测验和项目报告的可行性。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在教育任务中的应用，特别是它们与实际课堂中人类评估的一致性。

Method: 使用GPT-4o评估约50名学生的五次测验和14个团队的项目报告，并将LLM生成的分数与课程助教的人工评估进行比较。

Result: GPT-4o与人类评分者之间具有很强的相关性（高达0.98），并且在55%的测验案例中得分完全一致。对于项目报告，也显示出与人类评分的高度一致性，但在评估技术性和开放式回答方面存在一些差异。

Conclusion: 这项工作强调了基于LLM的评分系统的潜力和局限性，并有助于推动在现实学术环境中进行自动评分。

Abstract: Large Language Models (LLMs) are increasingly explored for educational tasks such as grading, yet their alignment with human evaluation in real classrooms remains underexamined. In this study, we investigate the feasibility of using an LLM (GPT-4o) to evaluate short-answer quizzes and project reports in an undergraduate Computational Linguistics course. We collect responses from approximately 50 students across five quizzes and receive project reports from 14 teams. LLM-generated scores are compared against human evaluations conducted independently by the course teaching assistants (TAs). Our results show that GPT-4o achieves strong correlation with human graders (up to 0.98) and exact score agreement in 55\% of quiz cases. For project reports, it also shows strong overall alignment with human grading, while exhibiting some variability in scoring technical, open-ended responses. We release all code and sample data to support further research on LLMs in educational assessment. This work highlights both the potential and limitations of LLM-based grading systems and contributes to advancing automated grading in real-world academic settings.

</details>


### [39] [Tracing Multilingual Representations in LLMs with Cross-Layer Transcoders](https://arxiv.org/abs/2511.10840)
*Abir Harrasse,Florent Draye,Zhijing Jin,Bernhard Schölkopf*

Main category: cs.CL

TL;DR: 大型多语言模型（LLM）可以处理多种语言，但它们如何在内部表示这种多样性仍不清楚。本文研究了LLM是否形成共享的多语言表示，以及为什么性能仍然偏向主要的训练语言。


<details>
  <summary>Details</summary>
Motivation: 研究LLM如何内部表示多种语言，以及为什么性能仍然偏向主要的训练语言。

Method: 通过训练一系列不同混合多语言数据的LLM，并使用跨层转码器（CLT）和归因图分析它们的内部机制。

Result: 模型采用几乎相同的跨语言表示，而特定于语言的解码出现在后面的层中。解码部分依赖于最后一层中的一小组高频语言特征，这些特征从模型的第一层线性读出语言身份。

Conclusion: 理解这种枢轴语言机制对于提高LLM中的多语言对齐至关重要。

Abstract: Multilingual Large Language Models (LLMs) can process many languages, yet how they internally represent this diversity remains unclear. Do they form shared multilingual representations with language-specific decoding, and if so, why does performance still favor the dominant training language? To address this, we train a series of LLMs on different mixtures of multilingual data and analyze their internal mechanisms using cross-layer transcoders (CLT) and attribution graphs. Our results provide strong evidence for pivot language representations: the model employs nearly identical representations across languages, while language-specific decoding emerges in later layers. Attribution analyses reveal that decoding relies in part on a small set of high-frequency language features in the final layers, which linearly read out language identity from the first layers in the model. By intervening on these features, we can suppress one language and substitute another in the model's outputs. Finally, we study how the dominant training language influences these mechanisms across attribution graphs and decoding pathways. We argue that understanding this pivot-language mechanism is crucial for improving multilingual alignment in LLMs.

</details>


### [40] [Reinforcing Stereotypes of Anger: Emotion AI on African American Vernacular English](https://arxiv.org/abs/2511.10846)
*Rebecca Dorn,Christina Chance,Casandra Rusti,Charles Bickham,Kai-Wei Chang,Fred Morstatter,Kristina Lerman*

Main category: cs.CL

TL;DR: 情感检测模型在非通用英语（如非裔美国人白话英语 AAVE）上的表现不佳，会产生偏见。


<details>
  <summary>Details</summary>
Motivation: 情感检测模型被广泛应用，但通常依赖于反映主流文化规范的注释，限制了模型识别不同方言情感表达的能力，尤其是在训练数据中常被排除的 AAVE。

Method: 研究分析了270万条带有地理标签的洛杉矶推文，使用计算方法近似方言特征来评估AAVE的强度。在一个包含875条推文的数据集上收集了情感存在和强度的注释，这些推文具有高低不同的AAVE密度。为了评估模型在情感感知任务上的准确性，我们计算了社区告知的“银色”标签，其中AAVE密集型推文由非裔美国人、精通AAVE的注释者（内部群体）标记。

Result: GPT和BERT模型在AAVE上的愤怒情绪误报率是通用美式英语（GAE）的两倍以上。SpanEmo模型在GAE上的愤怒情绪误报率为25%，在AAVE上则增加到60%。模型和非内部群体注释与基于亵渎的AAVE特征的相关性高于内部群体注释。非洲裔美国人居民比例较高的社区与较高的愤怒情绪预测（Pearson相关系数r = 0.27）和较低的喜悦情绪预测（r = -0.10）相关。

Conclusion: 情感人工智能可能会通过有偏见的情感分类来强化种族刻板印象，强调需要文化和方言敏感的情感计算系统。

Abstract: Automated emotion detection is widely used in applications ranging from well-being monitoring to high-stakes domains like mental health and hiring. However, models often rely on annotations that reflect dominant cultural norms, limiting model ability to recognize emotional expression in dialects often excluded from training data distributions, such as African American Vernacular English (AAVE). This study examines emotion recognition model performance on AAVE compared to General American English (GAE). We analyze 2.7 million tweets geo-tagged within Los Angeles. Texts are scored for strength of AAVE using computational approximations of dialect features. Annotations of emotion presence and intensity are collected on a dataset of 875 tweets with both high and low AAVE densities. To assess model accuracy on a task as subjective as emotion perception, we calculate community-informed "silver" labels where AAVE-dense tweets are labeled by African American, AAVE-fluent (ingroup) annotators. On our labeled sample, GPT and BERT-based models exhibit false positive prediction rates of anger on AAVE more than double than on GAE. SpanEmo, a popular text-based emotion model, increases false positive rates of anger from 25 percent on GAE to 60 percent on AAVE. Additionally, a series of linear regressions reveals that models and non-ingroup annotations are significantly more correlated with profanity-based AAVE features than ingroup annotations. Linking Census tract demographics, we observe that neighborhoods with higher proportions of African American residents are associated with higher predictions of anger (Pearson's correlation r = 0.27) and lower joy (r = -0.10). These results find an emergent safety issue of emotion AI reinforcing racial stereotypes through biased emotion classification. We emphasize the need for culturally and dialect-informed affective computing systems.

</details>


### [41] [Leveraging Parameter Space Symmetries for Reasoning Skill Transfer in LLMs](https://arxiv.org/abs/2511.10850)
*Stefan Horoi,Sangwoo Cho,Supriyo Chakraborty,Shi-Xiong Zhang,Sambit Sahu,Guy Wolf,Genta Indra Winata*

Main category: cs.CL

TL;DR: 提出了一种对齐模型参数空间以解决LLM之间任务算术中负干扰问题的方法。


<details>
  <summary>Details</summary>
Motivation: 模型在训练过程中发散，导致任务算术在LLM之间转移技能时出现负干扰。

Method: 首先对齐模型的参数空间，利用Transformer架构的置换、旋转和缩放对称性。改进了参数空间对齐方法，以适应现代的GQA和SwiGLU层，探索了基于权重和基于激活的方法。

Result: 在具有挑战性的推理基准测试中，该方法始终优于标准任务算术。

Conclusion: 该研究提供了一种有效的方法，用于合并和转移不断发展的LLM家族中的专业技能，减少冗余微调并增强模型适应性。

Abstract: Task arithmetic is a powerful technique for transferring skills between Large Language Models (LLMs), but it often suffers from negative interference when models have diverged during training. We address this limitation by first aligning the models' parameter spaces, leveraging the inherent permutation, rotation, and scaling symmetries of Transformer architectures. We adapt parameter space alignment for modern Grouped-Query Attention (GQA) and SwiGLU layers, exploring both weight-based and activation-based approaches. Using this alignment-first strategy, we successfully transfer advanced reasoning skills to a non-reasoning model. Experiments on challenging reasoning benchmarks show that our method consistently outperforms standard task arithmetic. This work provides an effective approach for merging and transferring specialized skills across evolving LLM families, reducing redundant fine-tuning and enhancing model adaptability.

</details>


### [42] [From Fact to Judgment: Investigating the Impact of Task Framing on LLM Conviction in Dialogue Systems](https://arxiv.org/abs/2511.10871)
*Parisa Rabbani,Nimet Beyza Bozdag,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 大型语言模型越来越多地被用作各种任务的评判者，包括涉及日常社交互动的任务。然而，目前尚不清楚此类 LLM 评判者是否可以可靠地评估需要社交或会话判断的任务。


<details>
  <summary>Details</summary>
Motivation: 研究 LLM 在社交或会话判断任务中的可靠性，以及当任务从直接的事实查询重新构建为会话判断任务时，LLM 的信念如何改变。

Method: 对比模型在直接事实查询和会话情境下评估说话者正确性的表现，并对两种情况施加反驳压力，以衡量模型在会话压力下保持其立场的能力。

Result: 一些模型在社交框架任务中表现出奉承倾向，而另一些模型则变得过于挑剔。所有模型的平均性能变化为 9.24%，表明即使是最小的对话上下文也会显着改变模型判断。

Conclusion: 对话框架是基于 LLM 的评估中的一个关键因素。该框架为诊断模型信念提供了一种可重现的方法，并有助于开发更值得信赖的对话系统。

Abstract: LLMs are increasingly employed as judges across a variety of tasks, including those involving everyday social interactions. Yet, it remains unclear whether such LLM-judges can reliably assess tasks that require social or conversational judgment. We investigate how an LLM's conviction is changed when a task is reframed from a direct factual query to a Conversational Judgment Task. Our evaluation framework contrasts the model's performance on direct factual queries with its assessment of a speaker's correctness when the same information is presented within a minimal dialogue, effectively shifting the query from "Is this statement correct?" to "Is this speaker correct?". Furthermore, we apply pressure in the form of a simple rebuttal ("The previous answer is incorrect.") to both conditions. This perturbation allows us to measure how firmly the model maintains its position under conversational pressure. Our findings show that while some models like GPT-4o-mini reveal sycophantic tendencies under social framing tasks, others like Llama-8B-Instruct become overly-critical. We observe an average performance change of 9.24% across all models, demonstrating that even minimal dialogue context can significantly alter model judgment, underscoring conversational framing as a key factor in LLM-based evaluation. The proposed framework offers a reproducible methodology for diagnosing model conviction and contributes to the development of more trustworthy dialogue systems.

</details>


### [43] [ICX360: In-Context eXplainability 360 Toolkit](https://arxiv.org/abs/2511.10879)
*Dennis Wei,Ronny Luss,Xiaomeng Hu,Lucas Monteiro Paes,Pin-Yu Chen,Karthikeyan Natesan Ramamurthy,Erik Miehling,Inge Vejsbjerg,Hendrik Strobelt*

Main category: cs.CL

TL;DR: ICX360是一个用于解释大型语言模型(llm)输出的python工具包，它侧重于用户提供的上下文。


<details>
  <summary>Details</summary>
Motivation: 解释llm的输出至关重要，特别是当它们被用于高风险应用时。

Method: ICX360包含三种使用黑盒和白盒方法解释llm的工具。

Result: ICX360工具包可在https://github.com/IBM/ICX360上找到，并包含快速入门指南和详细教程。

Conclusion: ICX360是一个有用的工具，可以解释llm，并可用于各种用例。

Abstract: Large Language Models (LLMs) have become ubiquitous in everyday life and are entering higher-stakes applications ranging from summarizing meeting transcripts to answering doctors' questions. As was the case with earlier predictive models, it is crucial that we develop tools for explaining the output of LLMs, be it a summary, list, response to a question, etc. With these needs in mind, we introduce In-Context Explainability 360 (ICX360), an open-source Python toolkit for explaining LLMs with a focus on the user-provided context (or prompts in general) that are fed to the LLMs. ICX360 contains implementations for three recent tools that explain LLMs using both black-box and white-box methods (via perturbations and gradients respectively). The toolkit, available at https://github.com/IBM/ICX360, contains quick-start guidance materials as well as detailed tutorials covering use cases such as retrieval augmented generation, natural language generation, and jailbreaking.

</details>


### [44] [A Multifaceted Analysis of Negative Bias in Large Language Models through the Lens of Parametric Knowledge](https://arxiv.org/abs/2511.10881)
*Jongyoon Song,Sangwon Yu,Sungroh Yoon*

Main category: cs.CL

TL;DR: 大型语言模型(LLM)在二元决策任务中过度生成否定回答的倾向被称为负偏差。本文研究了影响负偏差的因素，发现LLM表现出格式级别的负偏差，并探究了在不同prompt情景下负偏差的变化。


<details>
  <summary>Details</summary>
Motivation: 以往的研究主要集中在检测和解决导致负偏差的负注意头，但影响负偏差的潜在因素仍未被充分探索。

Method: 引入了一个pipeline来构建评估集，该评估集根据模型的参数知识将数据集系统地分为三个子集：正确、不正确和相关知识不足。通过分析这个评估集，我们识别出一种捷径行为，即模型在缺乏足够的知识来回答是-否问题时，倾向于生成否定回答，从而导致负偏差。

Result: 模型在缺乏足够的知识来回答是-否问题时，倾向于生成否定回答，从而导致负偏差。提供相关的上下文和提供“我不知道”的选项通常会减少负偏差，而思维链提示往往会放大偏差。负偏差的程度会根据提示的类型而变化，从而影响响应的方向。

Conclusion: 揭示了影响负偏差的各种因素，为缓解LLM中的负偏差提供了重要的见解。

Abstract: Negative bias refers to the tendency of large language models (LLMs) to excessively generate negative responses in binary decision tasks (e.g., yes-no question answering). Previous research has focused on detecting and addressing negative attention heads that induce negative bias. However, the underlying detailed factors influencing negative bias remain underexplored. In this paper, we demonstrate that LLMs exhibit format-level negative bias, meaning the prompt format more influences their responses than the semantics of the negative response. For the fine-grained study of the negative bias, we introduce a pipeline for constructing the evaluation set, which systematically categorizes the dataset into three subsets based on the model's parametric knowledge: correct, incorrect, and insufficient relevant knowledge. Through analysis of this evaluation set, we identify a shortcut behavior in which models tend to generate negative responses when they lack sufficient knowledge to answer a yes-no question, leading to negative bias. We further examine how negative bias changes under various prompting scenarios related to parametric knowledge. We observe that providing relevant context and offering an "I don't know" option generally reduces negative bias, whereas chain-of-thought prompting tends to amplify the bias. Finally, we demonstrate that the degree of negative bias can vary depending on the type of prompt, which influences the direction of the response. Our work reveals the various factors that influence negative bias, providing critical insights for mitigating it in LLMs.

</details>


### [45] [From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models](https://arxiv.org/abs/2511.10899)
*Farima Fatahi Bayat,Pouya Pezeshkpour,Estevam Hruschka*

Main category: cs.CL

TL;DR: 工具增强语言模型(TaLMs)可以调用外部工具来解决超出其参数能力的问题。然而，这些工具带来的收益是否反映了可信的推理仍不清楚。我们发现，即使工具被正确选择和执行，TaLMs也会将工具输出视为推理的替代品，产生看起来正确但缺乏连贯理由的解决方案。我们将这种失败模式称为工具诱导近视(TIM)。


<details>
  <summary>Details</summary>
Motivation: 研究工具增强语言模型(TaLMs)中工具使用对推理能力的影响，尤其是在数学问题解决中，揭示过度依赖工具可能导致的推理能力下降问题。

Method: 使用PYMATH基准测试，包含1,679个竞赛级别的数学问题，并开发一个多维评估套件，以量化TaLMs相对于非工具模型的推理能力退化情况。

Result: TaLMs在最终答案准确率上提高了19.3个百分点，但推理行为持续恶化。工具使用越多，模型的推理就越不连贯。错误从算术错误转向全局推理失败，TIM存在于约55%的高风险案例中。

Conclusion: 提出了一种基于偏好优化的框架，使TaLMs能够将工具用作辅助证据，从而提高最终答案的准确性和工具使用下的推理深度。

Abstract: Tool-augmented Language Models (TaLMs) can invoke external tools to solve problems beyond their parametric capacity. However, it remains unclear whether these tool-enabled gains reflect trustworthy reasoning. Focusing on the Code Interpreter tool, we show that even when tools are selected and executed correctly, TaLMs treat tool outputs as substitutes for reasoning, producing solutions that appear correct but lack coherent justification. We term this failure mode Tool-Induced Myopia (TIM), and study it using PYMATH, a benchmark of 1,679 competition-level mathematical problems for which Python code is helpful but not sufficient. We further develop a multi-dimensional evaluation suite to quantify reasoning degradation in TaLMs relative to their non-tool counterparts. Our findings reveal that while TaLMs achieve up to a 19.3 percentage point gain in final-answer accuracy, their reasoning behavior consistently deteriorates (e.g., non-tool LLMs win up to 41.5% more often in pairwise comparisons of the reasoning process). This degradation intensifies with tool use; the more frequently a model invokes tools, the less coherent its reasoning becomes. Moreover, tool use shifts errors from arithmetic mistakes toward global reasoning failures (logic, assumption, creativity); with TIM present in ~55% of high-risk cases. Finally, we propose a preference-optimization-based framework that realigns TaLMs to use tools as assistive evidence, improving both final-answer accuracy and reasoning depth under tool use. Codes and data are available at: https://github.com/megagonlabs/TIM.

</details>


### [46] [Expert-Guided Prompting and Retrieval-Augmented Generation for Emergency Medical Service Question Answering](https://arxiv.org/abs/2511.10900)
*Xueren Ge,Sahil Murtaza,Anthony Cortez,Homa Alemzadeh*

Main category: cs.CL

TL;DR: 这篇论文提出了EMSQA数据集，它包含10个临床科目领域和4个认证级别的24.3K个多项选择题。同时，论文还提出了Expert-CoT和ExpertRAG两种方法，它们分别利用了特定临床科目领域和认证级别的知识，以及与科目领域对齐的文档和真实患者数据来改进LLM在医疗问答方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常应用通用prompt或检索策略，而不利用专业人员依赖的结构化上下文（如临床科目领域和认证级别），这限制了LLM在高风险环境中的性能。

Method: 论文介绍了Expert-CoT，这是一种prompt策略，它基于特定的临床科目领域和认证级别进行chain-of-thought (CoT) 推理。此外，论文还介绍了ExpertRAG，这是一种检索增强生成pipeline，它将响应建立在与科目领域对齐的文档和真实世界的患者数据的基础上。

Result: 在4个LLM上的实验表明，Expert-CoT比vanilla CoT prompting提高了2.05%。此外，将Expert-CoT与ExpertRAG结合使用，比标准RAG基线提高了4.59%的准确率。值得注意的是，经过专业知识增强的32B LLM通过了所有计算机自适应EMS认证模拟考试。

Conclusion: 论文表明，通过利用临床科目领域和认证级别等结构化上下文，可以显著提高LLM在医疗问答方面的准确性和可靠性。

Abstract: Large language models (LLMs) have shown promise in medical question answering, yet they often overlook the domain-specific expertise that professionals depend on, such as the clinical subject areas (e.g., trauma, airway) and the certification level (e.g., EMT, Paramedic). Existing approaches typically apply general-purpose prompting or retrieval strategies without leveraging this structured context, limiting performance in high-stakes settings. We address this gap with EMSQA, an 24.3K-question multiple-choice dataset spanning 10 clinical subject areas and 4 certification levels, accompanied by curated, subject area-aligned knowledge bases (40K documents and 2M tokens). Building on EMSQA, we introduce (i) Expert-CoT, a prompting strategy that conditions chain-of-thought (CoT) reasoning on specific clinical subject area and certification level, and (ii) ExpertRAG, a retrieval-augmented generation pipeline that grounds responses in subject area-aligned documents and real-world patient data. Experiments on 4 LLMs show that Expert-CoT improves up to 2.05% over vanilla CoT prompting. Additionally, combining Expert-CoT with ExpertRAG yields up to a 4.59% accuracy gain over standard RAG baselines. Notably, the 32B expertise-augmented LLMs pass all the computer-adaptive EMS certification simulation exams.

</details>


### [47] [Multimodal Peer Review Simulation with Actionable To-Do Recommendations for Community-Aware Manuscript Revisions](https://arxiv.org/abs/2511.10902)
*Mengze Hong,Di Jiang,Weiwei Zhao,Yawen Li,Yihang Wang,Xinyuan Luo,Yanjie Sun,Chen Jason Zhang*

Main category: cs.CL

TL;DR: 本文提出了一个交互式的、基于网络的同行评审模拟系统，旨在通过多模态LLM、RAG和可操作的待办事项列表，在论文提交前实现有效的稿件修改。


<details>
  <summary>Details</summary>
Motivation: 现有学术同行评审系统受限于纯文本输入、有限的上下文和缺乏可操作的反馈。

Method: 该框架通过多模态LLM整合文本和视觉信息，通过RAG增强评论质量，并使用Action:Objective["]格式将生成的评论转换为可操作的待办事项列表。

Result: 实验结果表明，该系统能够生成更全面、更有用的、符合专家标准的评论。

Conclusion: 该系统能够生成更全面、更有用的、符合专家标准的评论，超越了基线系统，并推进了透明的、以人为本的学术帮助。

Abstract: While large language models (LLMs) offer promising capabilities for automating academic workflows, existing systems for academic peer review remain constrained by text-only inputs, limited contextual grounding, and a lack of actionable feedback. In this work, we present an interactive web-based system for multimodal, community-aware peer review simulation to enable effective manuscript revisions before paper submission. Our framework integrates textual and visual information through multimodal LLMs, enhances review quality via retrieval-augmented generation (RAG) grounded in web-scale OpenReview data, and converts generated reviews into actionable to-do lists using the proposed Action:Objective[\#] format, providing structured and traceable guidance. The system integrates seamlessly into existing academic writing platforms, providing interactive interfaces for real-time feedback and revision tracking. Experimental results highlight the effectiveness of the proposed system in generating more comprehensive and useful reviews aligned with expert standards, surpassing ablated baselines and advancing transparent, human-centered scholarly assistance.

</details>


### [48] [Automated Analysis of Learning Outcomes and Exam Questions Based on Bloom's Taxonomy](https://arxiv.org/abs/2511.10903)
*Ramya Kumar,Dhruv Gulwani,Sonit Singh*

Main category: cs.CL

TL;DR: 本文研究了根据 Bloom 分类法自动分类考试题和学习成果的方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索如何使用机器学习模型自动分类考试题和学习成果。

Method: 使用了传统机器学习模型（如 SVM）、循环神经网络（如 LSTM）和 Transformer 模型（如 BERT）。

Result: 数据增强的 SVM 模型表现最佳，准确率达到 94%。大型语言模型的零样本评估显示 OpenAI 和 Gemini 表现最好。

Conclusion: 在有限数据上训练复杂的深度模型面临挑战，数据增强和简单的算法（如增强的 SVM）对于 Bloom 分类法的分类很有价值。

Abstract: This paper explores the automatic classification of exam questions and learning outcomes according to Bloom's Taxonomy. A small dataset of 600 sentences labeled with six cognitive categories - Knowledge, Comprehension, Application, Analysis, Synthesis, and Evaluation - was processed using traditional machine learning (ML) models (Naive Bayes, Logistic Regression, Support Vector Machines), recurrent neural network architectures (LSTM, BiLSTM, GRU, BiGRU), transformer-based models (BERT and RoBERTa), and large language models (OpenAI, Gemini, Ollama, Anthropic). Each model was evaluated under different preprocessing and augmentation strategies (for example, synonym replacement, word embeddings, etc.). Among traditional ML approaches, Support Vector Machines (SVM) with data augmentation achieved the best overall performance, reaching 94 percent accuracy, recall, and F1 scores with minimal overfitting. In contrast, the RNN models and BERT suffered from severe overfitting, while RoBERTa initially overcame it but began to show signs as training progressed. Finally, zero-shot evaluations of large language models (LLMs) indicated that OpenAI and Gemini performed best among the tested LLMs, achieving approximately 0.72-0.73 accuracy and comparable F1 scores. These findings highlight the challenges of training complex deep models on limited data and underscore the value of careful data augmentation and simpler algorithms (such as augmented SVM) for Bloom's Taxonomy classification.

</details>


### [49] [Evaluating Large Language Models on Rare Disease Diagnosis: A Case Study using House M.D](https://arxiv.org/abs/2511.10912)
*Arsh Gupta,Ajay Narayanan Sridhar,Bonam Mingole,Amulya Yadav*

Main category: cs.CL

TL;DR: 本研究探索了大型语言模型在罕见疾病诊断中的应用，使用电视剧House M.D.构建了一个新的数据集，并评估了四个模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在罕见疾病诊断方面的能力未被充分探索。

Method: 从医学电视剧中提取症状-诊断对，构建数据集，并使用GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, 和 Gemini 2.5 Pro四个模型进行评估。

Result: 模型准确率在16.48%到38.64%之间，新一代模型有显著提升。

Conclusion: 大型语言模型在罕见疾病诊断方面面临挑战，但性能提升表明未来发展方向 promising。该研究建立了一个基准，可用于推进AI辅助诊断研究。

Abstract: Large language models (LLMs) have demonstrated capabilities across diverse domains, yet their performance on rare disease diagnosis from narrative medical cases remains underexplored. We introduce a novel dataset of 176 symptom-diagnosis pairs extracted from House M.D., a medical television series validated for teaching rare disease recognition in medical education. We evaluate four state-of-the-art LLMs such as GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, and Gemini 2.5 Pro on narrative-based diagnostic reasoning tasks. Results show significant variation in performance, ranging from 16.48% to 38.64% accuracy, with newer model generations demonstrating a 2.3 times improvement. While all models face substantial challenges with rare disease diagnosis, the observed improvement across architectures suggests promising directions for future development. Our educationally validated benchmark establishes baseline performance metrics for narrative medical reasoning and provides a publicly accessible evaluation framework for advancing AI-assisted diagnosis research.

</details>


### [50] [CardioEmbed: Domain-Specialized Text Embeddings for Clinical Cardiology](https://arxiv.org/abs/2511.10930)
*Richard J. Young,Alice M. Matthews*

Main category: cs.CL

TL;DR: 本研究旨在通过使用心脏病学教科书训练领域专用嵌入模型CardioEmbed，以提高临床心脏病学应用的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的生物医学文本嵌入模型主要使用PubMed的研究文献进行开发，但临床心脏病学实践严重依赖于程序知识和专业术语，而这些知识和术语主要存在于综合教科书中，而不是研究摘要中。这种研究实践差距限制了现有嵌入模型在心脏病学临床应用中的有效性。

Method: 使用Qwen3-Embedding-8B，通过对比学习，在一个包含七本综合心脏病学教科书的语料库上训练CardioEmbed。该模型采用InfoNCE损失和批内负采样。

Result: 在心脏病学特定的语义检索任务中，该模型达到了99.60%的检索准确率，比当前最先进的医学嵌入模型MedTE提高了+15.94个百分点。在MTEB医学基准测试中，该模型在BIOSSES上获得了0.77 Spearman相关系数，在SciFact上获得了0.61 NDCG@10，表明在相关生物医学领域具有竞争力的性能。

Conclusion: 在综合临床教科书上进行领域专门训练可以实现近乎完美的心脏病学检索（99.60% Acc@1），比MedTE提高了+15.94个百分点。

Abstract: Biomedical text embeddings have primarily been developed using research literature from PubMed, yet clinical cardiology practice relies heavily on procedural knowledge and specialized terminology found in comprehensive textbooks rather than research abstracts. This research practice gap limits the effectiveness of existing embedding models for clinical applications incardiology. This study trained CardioEmbed, a domain-specialized embedding model based on Qwen3-Embedding-8B, using contrastive learning on a curated corpus of seven comprehensive cardiology textbooks totaling approximately 150,000 sentences after deduplication. The model employs InfoNCE loss with in-batch negatives and achieves 99.60% retrieval accuracy on cardiac-specific semantic retrieval tasks, a +15.94 percentage point improvement over MedTE, the current state-of-the-art medical embedding model. On MTEB medical benchmarks, the model obtained BIOSSES 0.77 Spearman and SciFact 0.61 NDCG@10, indicating competitive performance on related biomedical domains. Domain-specialized training on comprehensive clinical textbooks yields near-perfect cardiology retrieval (99.60% Acc@1), improving over MedTE by +15.94 percentage points.

</details>


### [51] [DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains](https://arxiv.org/abs/2511.10984)
*Xiying Zhao,Zhoufutu Wen,Zhixuan Chen,Jingzhe Ding,Jianpeng Jiao,Shuai Li,Xi Li,Danni Liang,Shengda Long,Qianqian Liu,Xianbo Wu,Hongwan Gao,Xiang Gao,Liang Hu,Jiashuo Liu,Mengyun Liu,Weiran Shi,Chenghao Yang,Qianyu Yang,Xuanliang Zhang,Ge Zhang,Wenhao Huang*

Main category: cs.CL

TL;DR: 现有的评估方法主要集中在段级准确性和流畅性，而专家领域中语篇级翻译的评估仍然不足。为了解决这个问题，我们引入了 DiscoX，这是一个新的语篇级和专家级中英文翻译的基准。


<details>
  <summary>Details</summary>
Motivation: 知识传播和跨语言学术交流需要语篇级的连贯性和严格的术语精确性。

Method: 我们开发了 Metric-S，这是一个无需参考的系统，可以对准确性、流畅性和适当性进行细粒度的自动评估。

Result: Metric-S 与人类判断表现出很强的一致性，明显优于现有指标。实验表明，即使是最先进的 LLM 在这些任务上仍然落后于人类专家。

Conclusion: 所提出的基准和评估系统为更严格的评估提供了一个强大的框架，有助于未来基于 LLM 的翻译的进步。

Abstract: The evaluation of discourse-level translation in expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-level accuracy and fluency. To address this limitation, we introduce DiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance on DiscoX, we also develop Metric-S, a reference-free system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness. Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advanced LLMs still trail human experts on these tasks. This finding validates the difficulty of DiscoX and underscores the challenges that remain in achieving professional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [52] [A Mathematical Framework for AI Singularity: Conditions, Bounds, and Control of Recursive Improvement](https://arxiv.org/abs/2511.10668)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.CV

TL;DR: 该论文研究了AI能力在有限时间内是否会无限制地增长，并提出了一个分析框架，将能力增长与资源建设和部署策略联系起来。


<details>
  <summary>Details</summary>
Motivation: 探讨在何种可衡量的条件下，AI能力可能在有限时间内无限制地增长，以及在何种条件下可以排除这种可能性。

Method: 建立了一个递归自提升的分析框架，将能力增长与资源（计算、数据、能源）的建设和部署策略联系起来。利用物理和信息理论的限制（功率、带宽和内存）定义了一个服务包络，限制了瞬时改进。一个内生增长模型将资本与计算、数据和能源联系起来，并定义了区分超线性状态和亚临界状态的关键边界。推导出决策规则，将可观察的序列（设施功率、IO带宽、训练吞吐量、基准损失和支出）映射到“是/否”证书，以判断是否存在失控行为。

Result: 导出了基于改进加速相对于当前水平的速度的可证伪测试，并提供了可在实践中直接实施的安全控制，如功率上限、吞吐量限制和评估门。案例研究涵盖了功率受限、数据饱和和投资放大等设置，说明了包络何时起作用，何时不起作用。

Conclusion: 研究结果用可测试的条件和可部署的控制取代了对AI奇点的推测，从而可以验证或排除AI奇点。

Abstract: AI systems improve by drawing on more compute, data, energy, and better training methods. This paper asks a precise, testable version of the "runaway growth" question: under what measurable conditions could capability escalate without bound in finite time, and under what conditions can that be ruled out? We develop an analytic framework for recursive self-improvement that links capability growth to resource build-out and deployment policies. Physical and information-theoretic limits from power, bandwidth, and memory define a service envelope that caps instantaneous improvement. An endogenous growth model couples capital to compute, data, and energy and defines a critical boundary separating superlinear from subcritical regimes. We derive decision rules that map observable series (facility power, IO bandwidth, training throughput, benchmark losses, and spending) into yes/no certificates for runaway versus nonsingular behavior. The framework yields falsifiable tests based on how fast improvement accelerates relative to its current level, and it provides safety controls that are directly implementable in practice, such as power caps, throughput throttling, and evaluation gates. Analytical case studies cover capped-power, saturating-data, and investment-amplified settings, illustrating when the envelope binds and when it does not. The approach is simulation-free and grounded in measurements engineers already collect. Limitations include dependence on the chosen capability metric and on regularity diagnostics; future work will address stochastic dynamics, multi-agent competition, and abrupt architectural shifts. Overall, the results replace speculation with testable conditions and deployable controls for certifying or precluding an AI singularity.

</details>


### [53] [Semantic VLM Dataset for Safe Autonomous Driving](https://arxiv.org/abs/2511.10701)
*Yuankai He,Weisong Shi*

Main category: cs.CV

TL;DR: CAR-Scenes是一个用于自动驾驶的帧级别数据集，它使用视觉-语言模型（VLM）来实现可解释的场景级别理解。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在提供一个数据集，以支持训练和评估用于可解释场景理解的视觉-语言模型，从而应对自动驾驶领域的需求。

Method: 该论文通过一个GPT-4o辅助的视觉-语言流水线生成标签，并进行人工验证。使用28个关键类别/子类别知识库对从Argoverse 1、Cityscapes、KITTI和nuScenes中提取的5,192张图像进行注释，涵盖环境、道路几何、背景车辆行为、自我车辆行为、弱势道路使用者、传感器状态和一个离散的严重程度等级（1-10），总计超过350个叶属性。

Result: 该论文发布了数据集、确切的prompt、后处理规则和每个字段的基线模型性能。它还提供了属性共现图和JSONL记录，以支持语义检索、数据集分类和跨来源的风险感知场景挖掘。论文包括可重现的非基准基线，通过标量精度、列表属性的微平均F1以及固定验证分割上的严重程度MAE/RMSE进行评估。

Conclusion: 该论文公开发布了注释和分析脚本，包括图构建和评估脚本，以实现未来智能车辆的可解释的、以数据为中心的工作流程。

Abstract: CAR-Scenes is a frame-level dataset for autonomous driving that enables training and evaluation of vision-language models (VLMs) for interpretable, scene-level understanding. We annotate 5,192 images drawn from Argoverse 1, Cityscapes, KITTI, and nuScenes using a 28-key category/sub-category knowledge base covering environment, road geometry, background-vehicle behavior, ego-vehicle behavior, vulnerable road users, sensor states, and a discrete severity scale (1-10), totaling 350+ leaf attributes. Labels are produced by a GPT-4o-assisted vision-language pipeline with human-in-the-loop verification; we release the exact prompts, post-processing rules, and per-field baseline model performance. CAR-Scenes also provides attribute co-occurrence graphs and JSONL records that support semantic retrieval, dataset triage, and risk-aware scenario mining across sources. To calibrate task difficulty, we include reproducible, non-benchmark baselines, notably a LoRA-tuned Qwen2-VL-2B with deterministic decoding, evaluated via scalar accuracy, micro-averaged F1 for list attributes, and severity MAE/RMSE on a fixed validation split. We publicly release the annotation and analysis scripts, including graph construction and evaluation scripts, to enable explainable, data-centric workflows for future intelligent vehicles. Dataset: https://github.com/Croquembouche/CAR-Scenes

</details>


### [54] [Fast Data Attribution for Text-to-Image Models](https://arxiv.org/abs/2511.10721)
*Sheng-Yu Wang,Aaron Hertzmann,Alexei A Efros,Richard Zhang,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 提出了一种可扩展且高效的数据归因方法，用于识别对生成输出影响最大的训练图像。


<details>
  <summary>Details</summary>
Motivation: 现有的归因方法计算资源消耗大，使其在实际应用中不切实际。

Method: 将基于unlearning的慢速归因方法提炼到特征嵌入空间，以有效检索具有高度影响力的训练图像。

Result: 在MSCOCO和LAION上训练的中型和大型Stable Diffusion模型上，结果表明该方法在几秒内实现了更好或具有竞争力的性能，比现有方法快2,500x - 400,000x。

Conclusion: 这项工作代表了朝着在Stable Diffusion等实际模型上大规模应用数据归因方法迈出的有意义的一步。

Abstract: Data attribution for text-to-image models aims to identify the training images that most significantly influenced a generated output. Existing attribution methods involve considerable computational resources for each query, making them impractical for real-world applications. We propose a novel approach for scalable and efficient data attribution. Our key idea is to distill a slow, unlearning-based attribution method to a feature embedding space for efficient retrieval of highly influential training images. During deployment, combined with efficient indexing and search methods, our method successfully finds highly influential images without running expensive attribution algorithms. We show extensive results on both medium-scale models trained on MSCOCO and large-scale Stable Diffusion models trained on LAION, demonstrating that our method can achieve better or competitive performance in a few seconds, faster than existing methods by 2,500x - 400,000x. Our work represents a meaningful step towards the large-scale application of data attribution methods on real-world models such as Stable Diffusion.

</details>


### [55] [Expert Consensus-based Video-Based Assessment Tool for Workflow Analysis in Minimally Invasive Colorectal Surgery: Development and Validation of ColoWorkflow](https://arxiv.org/abs/2511.10766)
*Pooja P Jain,Pietro Mascagni,Giuseppe Massimiani,Nabani Banik,Marta Goglia,Lorenzo Arboit,Britty Baby,Andrea Balla,Ludovica Baldari,Gianfranco Silecchia,Claudio Fiorillo,CompSurg Colorectal Experts Group,Sergio Alfieri,Salvador Morales-Conde,Deborah S Keller,Luigi Boni,Nicolas Padoy*

Main category: cs.CV

TL;DR: 本研究旨在开发并验证一种用于微创结直肠手术工作流程分析的VBA工具ColoWorkflow，以减少手术变异性，优化培训并提高手术效果。


<details>
  <summary>Details</summary>
Motivation: 微创结直肠手术存在程序变异性大、学习曲线陡峭以及并发症影响质量和结果等问题。基于视频的评估(VBA)提供了一个机会来生成数据驱动的见解，以减少变异性，优化培训和提高手术效果。然而，现有的工作流程分析工具仍然难以标准化和实施。

Method: 采用Delphi过程就通用工作流程描述符达成共识。由此产生的框架为新的VBA工具ColoWorkflow的开发提供了信息。然后，独立的评估者将ColoWorkflow应用于来自五个中心腹腔镜和机器人结直肠手术(CRS)的多中心视频数据集。评估了适用性和评估者间信度。

Result: ColoWorkflow工具具有广泛的适用性，平均Cohen's K为0.71（阶段）和0.66（步骤），表明评估者间信度中等。大多数差异出现在阶段转换和步骤边界定义处。

Conclusion: ColoWorkflow是第一个基于共识的、经过验证的VBA工具，用于微创CRS中的全面工作流程分析。它为基于视频的性能评估建立了一个可复制的框架，支持跨机构的标杆管理，并支持人工智能驱动的工作流程识别的发展。它的采用可以标准化培训，加速能力获取，并推进数据驱动的手术质量改进。

Abstract: Minimally invasive colorectal surgery is characterized by procedural variability, a difficult learning curve, and complications that impact quality and outcomes. Video-based assessment (VBA) offers an opportunity to generate data-driven insights to reduce variability, optimize training, and improve surgical performance. However, existing tools for workflow analysis remain difficult to standardize and implement. This study aims to develop and validate a VBA tool for workflow analysis across minimally invasive colorectal procedures. A Delphi process was conducted to achieve consensus on generalizable workflow descriptors. The resulting framework informed the development of a new VBA tool, ColoWorkflow. Independent raters then applied ColoWorkflow to a multicentre video dataset of laparoscopic and robotic colorectal surgery (CRS). Applicability and inter-rater reliability were evaluated. Consensus was achieved for 10 procedure-agnostic phases and 34 procedure-specific steps describing CRS workflows. ColoWorkflow was developed and applied to 54 colorectal operative videos (left and right hemicolectomies, sigmoid and rectosigmoid resections, and total proctocolectomies) from five centres. The tool demonstrated broad applicability, with all but one label utilized. Inter-rater reliability was moderate, with mean Cohen's K of 0.71 for phases and 0.66 for steps. Most discrepancies arose at phase transitions and step boundary definitions. ColoWorkflow is the first consensus-based, validated VBA tool for comprehensive workflow analysis in minimally invasive CRS. It establishes a reproducible framework for video-based performance assessment, enabling benchmarking across institutions and supporting the development of artificial intelligence-driven workflow recognition. Its adoption may standardize training, accelerate competency acquisition, and advance data-informed surgical quality improvement.

</details>


### [56] [Frequency-Aware Vision-Language Multimodality Generalization Network for Remote Sensing Image Classification](https://arxiv.org/abs/2511.10774)
*Junjie Zhang,Feng Zhao,Hanqiang Liu,Jun Yu*

Main category: cs.CV

TL;DR: 本文提出了一种频率感知视觉-语言多模态泛化网络（FVMGN），用于遥感图像分类，以解决遥感多模态泛化（RSMG）问题。


<details>
  <summary>Details</summary>
Motivation: 遥感技术发展迅速，产生了一种新的多模态泛化任务，该任务要求模型在克服数据异构性的同时，还要具有强大的跨场景泛化能力。此外，大多数视觉-语言模型（VLM）通常使用通用文本来描述遥感图像中的表面材料，缺乏特定于不同遥感视觉模态的专有语言先验知识。

Method: 1. 提出了基于扩散的训练-测试时增强（DTAug）策略，以重建多模态地物覆盖分布，丰富FVMGN的输入信息。
2. 开发了一种多模态小波解耦（MWDis）模块，通过在频域中重采样低频和高频分量来学习跨域不变特征，从而克服多模态异构性。
3. 设计了共享和专有类文本作为基于 Transformer 的文本编码器的语言输入，以提取不同的文本特征。
4. 构建了一个空间-频率感知图像编码器（SFIE），以实现局部-全局特征重建和表示。
5. 提出了一个多尺度空间-频率特征对齐（MSFFA）模块，以构建统一的语义空间，确保在空间和频域中对不同的文本和视觉特征进行精细的多尺度对齐。

Result: 大量实验表明，与最先进的方法（SOTA）相比，FVMGN 具有出色的多模态泛化能力。

Conclusion: 本文提出的FVMGN网络在遥感图像分类中表现出优异的多模态泛化能力，为解决遥感多模态泛化问题提供了新的思路。

Abstract: The booming remote sensing (RS) technology is giving rise to a novel multimodality generalization task, which requires the model to overcome data heterogeneity while possessing powerful cross-scene generalization ability. Moreover, most vision-language models (VLMs) usually describe surface materials in RS images using universal texts, lacking proprietary linguistic prior knowledge specific to different RS vision modalities. In this work, we formalize RS multimodality generalization (RSMG) as a learning paradigm, and propose a frequency-aware vision-language multimodality generalization network (FVMGN) for RS image classification. Specifically, a diffusion-based training-test-time augmentation (DTAug) strategy is designed to reconstruct multimodal land-cover distributions, enriching input information for FVMGN. Following that, to overcome multimodal heterogeneity, a multimodal wavelet disentanglement (MWDis) module is developed to learn cross-domain invariant features by resampling low and high frequency components in the frequency domain. Considering the characteristics of RS vision modalities, shared and proprietary class texts is designed as linguistic inputs for the transformer-based text encoder to extract diverse text features. For multimodal vision inputs, a spatial-frequency-aware image encoder (SFIE) is constructed to realize local-global feature reconstruction and representation. Finally, a multiscale spatial-frequency feature alignment (MSFFA) module is suggested to construct a unified semantic space, ensuring refined multiscale alignment of different text and vision features in spatial and frequency domains. Extensive experiments show that FVMGN has the excellent multimodality generalization ability compared with state-of-the-art (SOTA) methods.

</details>


### [57] [GFT: Graph Feature Tuning for Efficient Point Cloud Analysis](https://arxiv.org/abs/2511.10799)
*Manish Dhakal,Venkat R. Dasari,Raj Sunderraman,Yi Ding*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的点云数据参数高效微调方法，名为图特征调整 (GFT)，通过学习动态图来减少可训练参数，并在对象分类和分割任务上取得了与现有方法相媲美的结果。


<details>
  <summary>Details</summary>
Motivation: 现有通用参数高效微调方法在点云数据上表现欠佳，且可训练参数仍然较多。

Method: 使用轻量级图卷积网络从 Transformer 的初始 tokenized 输入中学习动态图，并通过跳跃连接和高效的交叉注意力模块将这些图特征传递到更深层。

Result: 在对象分类和分割任务上的大量实验表明，GFT 在相同领域内运行，与现有方法相媲美，同时减少了可训练参数。

Conclusion: GFT 是一种有效的点云数据参数高效微调方法，可以在减少可训练参数的同时保持性能。

Abstract: Parameter-efficient fine-tuning (PEFT) significantly reduces computational and memory costs by updating only a small subset of the model's parameters, enabling faster adaptation to new tasks with minimal loss in performance. Previous studies have introduced PEFTs tailored for point cloud data, as general approaches are suboptimal. To further reduce the number of trainable parameters, we propose a point-cloud-specific PEFT, termed Graph Features Tuning (GFT), which learns a dynamic graph from initial tokenized inputs of the transformer using a lightweight graph convolution network and passes these graph features to deeper layers via skip connections and efficient cross-attention modules. Extensive experiments on object classification and segmentation tasks show that GFT operates in the same domain, rivalling existing methods, while reducing the trainable parameters. Code is at https://github.com/manishdhakal/GFT.

</details>


### [58] [Accuracy-Preserving CNN Pruning Method under Limited Data Availability](https://arxiv.org/abs/2511.10861)
*Daisuke Yasui,Toshitaka Matsuki,Hiroshi Sato*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的基于LRP的CNN模型剪枝方法，旨在在数据量有限的情况下，实现更高的剪枝率并保持模型准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LRP的剪枝方法存在精度下降的问题，限制了它们的实际应用。

Method: 该研究提出了一种新的剪枝方法，可以在少量数据下实现更高的剪枝率，同时保持更好的模型精度。

Result: 该方法在少量数据下的剪枝效果优于现有方法，可以更好地保持模型精度。

Conclusion: 该研究提出了一种有前景的CNN模型剪枝方法，可以在资源受限的环境中实现高效的模型压缩。

Abstract: Convolutional Neural Networks (CNNs) are widely used in image recognition and have succeeded in various domains. CNN models have become larger-scale to improve accuracy and generalization performance. Research has been conducted on compressing pre-trained models for specific target applications in environments with limited computing resources. Among model compression techniques, methods using Layer-wise Relevance Propagation (LRP), an explainable AI technique, have shown promise by achieving high pruning rates while preserving accuracy, even without fine-tuning. Because these methods do not require fine-tuning, they are suited to scenarios with limited data. However, existing LRP-based pruning approaches still suffer from significant accuracy degradation, limiting their practical usability. This study proposes a pruning method that achieves a higher pruning rate while preserving better model accuracy. Our approach to pruning with a small amount of data has achieved pruning that preserves accuracy better than existing methods.

</details>


### [59] [Short-Window Sliding Learning for Real-Time Violence Detection via LLM-based Auto-Labeling](https://arxiv.org/abs/2511.10866)
*Seoik Jung,Taekyung Song,Yangro Lee,Sungjun Lee*

Main category: cs.CV

TL;DR: 提出了一种短窗口滑动学习框架，用于在闭路电视录像中进行实时暴力检测。


<details>
  <summary>Details</summary>
Motivation: 与传统的长视频训练方法不同，该方法旨在提高实时暴力检测的准确性和效率。

Method: 该方法将视频分成 1-2 秒的短片，并应用基于大型语言模型 (LLM) 的自动字幕标注来构建细粒度数据集，充分利用所有帧来保持时间连续性。

Result: 在 RWF-2000 数据集上实现了 95.25% 的准确率，并在长视频 (UCF-Crime: 83.25%) 上显着提高了性能。

Conclusion: 该方法具有很强的泛化能力和在智能监控系统中的实时适用性

Abstract: This paper proposes a Short-Window Sliding Learning framework for real-time violence detection in CCTV footages. Unlike conventional long-video training approaches, the proposed method divides videos into 1-2 second clips and applies Large Language Model (LLM)-based auto-caption labeling to construct fine-grained datasets. Each short clip fully utilizes all frames to preserve temporal continuity, enabling precise recognition of rapid violent events. Experiments demonstrate that the proposed method achieves 95.25\% accuracy on RWF-2000 and significantly improves performance on long videos (UCF-Crime: 83.25\%), confirming its strong generalization and real-time applicability in intelligent surveillance systems.

</details>


### [60] [MCN-CL: Multimodal Cross-Attention Network and Contrastive Learning for Multimodal Emotion Recognition](https://arxiv.org/abs/2511.10892)
*Feng Li,Ke Wu,Yongwei Li*

Main category: cs.CV

TL;DR: 提出了一种用于多模态情感识别的MCN-CL框架，通过三重查询机制和困难负样本挖掘策略来解决模态异构和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态情感识别方法面临类别分布不平衡、动态面部动作单元时间建模复杂以及模态异构导致特征融合困难等挑战。社交媒体场景下多模态数据爆炸式增长，构建高效的跨模态情感识别框架的需求迫切。

Method: 提出了多模态跨注意力网络和对比学习（MCN-CL）。使用三重查询机制和困难负样本挖掘策略来去除特征冗余，同时保留重要的情感线索。

Result: 在IEMOCAP和MELD数据集上的实验结果表明，所提出的方法优于现有方法，Weighted F1 scores分别提高了3.42%和5.73%。

Conclusion: MCN-CL框架有效提升了多模态情感识别的性能，解决了模态异构和类别不平衡的问题。

Abstract: Multimodal emotion recognition plays a key role in many domains, including mental health monitoring, educational interaction, and human-computer interaction. However, existing methods often face three major challenges: unbalanced category distribution, the complexity of dynamic facial action unit time modeling, and the difficulty of feature fusion due to modal heterogeneity. With the explosive growth of multimodal data in social media scenarios, the need for building an efficient cross-modal fusion framework for emotion recognition is becoming increasingly urgent. To this end, this paper proposes Multimodal Cross-Attention Network and Contrastive Learning (MCN-CL) for multimodal emotion recognition. It uses a triple query mechanism and hard negative mining strategy to remove feature redundancy while preserving important emotional cues, effectively addressing the issues of modal heterogeneity and category imbalance. Experiment results on the IEMOCAP and MELD datasets show that our proposed method outperforms state-of-the-art approaches, with Weighted F1 scores improving by 3.42% and 5.73%, respectively.

</details>


### [61] [DINOv3 as a Frozen Encoder for CRPS-Oriented Probabilistic Rainfall Nowcasting](https://arxiv.org/abs/2511.10894)
*Luciano Araujo Dourado Filho,Almir Moreira da Silva Neto,Anthony Miyaguchi,Rodrigo Pereira David,Rodrigo Tripodi Calumby,Lukáš Picek*

Main category: cs.CV

TL;DR: 提出了一种用于概率性降雨临近预报的、具有竞争力和计算效率的方法。


<details>
  <summary>Details</summary>
Motivation: 为了提高概率性降雨临近预报的效率和准确性。

Method: 使用连接到轻量级概率头的视频投影仪（V-JEPA Vision Transformer），该投影仪与预训练的卫星视觉编码器（DINOv3-SAT493M）相关联，以将编码器令牌映射到4小时累积降雨量的离散经验CDF（eCDF）。

Result: 在Weather4Cast 2025基准测试中，该方法取得了有希望的性能，CRPS为3.5102，比最佳3D-UNET的有效性提高了约26%。

Conclusion: 该方法在概率性降雨临近预报方面具有竞争力和计算效率，并在Weather4Cast 2025基准测试中取得了有希望的结果。

Abstract: This paper proposes a competitive and computationally efficient approach to probabilistic rainfall nowcasting. A video projector (V-JEPA Vision Transformer) associated to a lightweight probabilistic head is attached to a pre-trained satellite vision encoder (DINOv3\text{-}SAT493M) to map encoder tokens into a discrete empirical CDF (eCDF) over 4-hour accumulated rainfall. The projector-head is optimized end-to-end over the Continuous Ranked Probability Score (CRPS). As an alternative, 3D-UNET baselines trained with an aggregate Rank Probability Score and a per-pixel Gamma-Hurdle objective are used. On the Weather4Cast 2025 benchmark, the proposed method achieved a promising performance, with a CRPS of 3.5102 (CRPS), which represents $\approx$26\% in effectiveness gain against the best 3D-UNET.

</details>


### [62] [YOLO-Drone: An Efficient Object Detection Approach Using the GhostHead Network for Drone Images](https://arxiv.org/abs/2511.10905)
*Hyun-Ki Jung*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为YOLO-Drone的改进型目标检测模型，旨在解决无人机图像中目标识别的难题。该模型通过改进YOLOv11的Head网络，实现了精度和速度的提升。


<details>
  <summary>Details</summary>
Motivation: 无人机图像通常在高空拍摄，导致目标识别困难。

Method: 通过在YOLOv11的基础上引入GhostHead网络来改进Head网络。

Result: YOLO-Drone在精度、召回率、F1-Score和mAP (0.5) 等关键指标上均优于原始YOLOv11，并且在mAP (0.5) 上也优于YOLOv8、YOLOv9和YOLOv10。

Conclusion: YOLO-Drone是一种高性能模型，在准确性和速度方面均优于YOLOv11，并在与其他高性能目标检测模型的比较中表现出色。

Abstract: Object detection using images or videos captured by drones is a promising technology with significant potential across various industries. However, a major challenge is that drone images are typically taken from high altitudes, making object identification difficult. This paper proposes an effective solution to address this issue. The base model used in the experiments is YOLOv11, the latest object detection model, with a specific implementation based on YOLOv11n. The experimental data were sourced from the widely used and reliable VisDrone dataset, a standard benchmark in drone-based object detection. This paper introduces an enhancement to the Head network of the YOLOv11 algorithm, called the GhostHead Network. The model incorporating this improvement is named YOLO-Drone. Experimental results demonstrate that YOLO-Drone achieves significant improvements in key detection accuracy metrics, including Precision, Recall, F1-Score, and mAP (0.5), compared to the original YOLOv11. Specifically, the proposed model recorded a 0.4% increase in Precision, a 0.6% increase in Recall, a 0.5% increase in F1-Score, and a 0.5% increase in mAP (0.5). Additionally, the Inference Speed metric, which measures image processing speed, also showed a notable improvement. These results indicate that YOLO-Drone is a high-performance model with enhanced accuracy and speed compared to YOLOv11. To further validate its reliability, comparative experiments were conducted against other high-performance object detection models, including YOLOv8, YOLOv9, and YOLOv10. The results confirmed that the proposed model outperformed YOLOv8 by 0.1% in mAP (0.5) and surpassed YOLOv9 and YOLOv10 by 0.3% and 0.6%, respectively.

</details>


### [63] [PhaseWin Search Framework Enable Efficient Object-Level Interpretation](https://arxiv.org/abs/2511.10914)
*Zihan Gu,Ruoyu Chen,Junchi Zhang,Yue Hu,Hua Zhang,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出了一种新的相位窗口搜索算法PhaseWin，以实现高效且忠实的区域归因。


<details>
  <summary>Details</summary>
Motivation: 现有的基于子模子集选择的方法虽然忠实度高，但效率低，限制了在实际场景中的应用。

Method: 用分阶段的由粗到精的搜索取代传统的二次成本贪婪选择，结合自适应修剪、窗口化细粒度选择和动态监督机制。

Result: 在仅使用20%的计算预算的情况下，PhaseWin实现了超过95%的贪婪归因忠实度，并在对象检测和视觉定位任务中始终优于其他归因基线。

Conclusion: PhaseWin在可扩展的、高保真度的对象级多模态模型归因方面建立了一个新的技术水平。

Abstract: Attribution is essential for interpreting object-level foundation models. Recent methods based on submodular subset selection have achieved high faithfulness, but their efficiency limitations hinder practical deployment in real-world scenarios. To address this, we propose PhaseWin, a novel phase-window search algorithm that enables faithful region attribution with near-linear complexity. PhaseWin replaces traditional quadratic-cost greedy selection with a phased coarse-to-fine search, combining adaptive pruning, windowed fine-grained selection, and dynamic supervision mechanisms to closely approximate greedy behavior while dramatically reducing model evaluations. Theoretically, PhaseWin retains near-greedy approximation guarantees under mild monotone submodular assumptions. Empirically, PhaseWin achieves over 95% of greedy attribution faithfulness using only 20% of the computational budget, and consistently outperforms other attribution baselines across object detection and visual grounding tasks with Grounding DINO and Florence-2. PhaseWin establishes a new state of the art in scalable, high-faithfulness attribution for object-level multimodal models.

</details>


### [64] [Out-of-Distribution Detection with Positive and Negative Prompt Supervision Using Large Language Models](https://arxiv.org/abs/2511.10923)
*Zhixia He,Chen Zhao,Minglai Shao,Xintao Wu,Xujiang Zhao,Dong Li,Qin Tian,Linlin Yu*

Main category: cs.CV

TL;DR: 本研究提出了一种新的OOD检测方法，通过正负提示监督，鼓励负提示捕获类间特征，并将语义知识转移到视觉模态，从而提高OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型(VLM)通过整合视觉和文本模态，在OOD检测方面表现出色。负提示被引入以强调图像特征和提示内容之间的差异。然而，这些提示通常包含范围广泛的非ID特征，这可能由于捕获重叠或误导性信息而导致次优结果。

Method: 该方法首先由大型语言模型(llm)初始化类特定的正负提示。随后对这些提示进行优化，正提示侧重于每个类中的特征，而负提示则突出显示类别边界周围的特征。此外，采用基于图的体系结构来聚合来自优化提示表示的语义感知监督，并将其传播到视觉分支，从而提高基于能量的OOD检测器的性能。

Result: 在CIFAR-100和ImageNet-1K两个基准数据集上，跨越8个OOD数据集和5个不同的llm的广泛实验表明，该方法优于最先进的基线。

Conclusion: 该研究提出了一种新的正负提示监督方法，通过鼓励负提示捕获类间特征，并将语义知识转移到视觉模态，从而提高了OOD检测性能。

Abstract: Out-of-distribution (OOD) detection is committed to delineating the classification boundaries between in-distribution (ID) and OOD images. Recent advances in vision-language models (VLMs) have demonstrated remarkable OOD detection performance by integrating both visual and textual modalities. In this context, negative prompts are introduced to emphasize the dissimilarity between image features and prompt content. However, these prompts often include a broad range of non-ID features, which may result in suboptimal outcomes due to the capture of overlapping or misleading information. To address this issue, we propose Positive and Negative Prompt Supervision, which encourages negative prompts to capture inter-class features and transfers this semantic knowledge to the visual modality to enhance OOD detection performance. Our method begins with class-specific positive and negative prompts initialized by large language models (LLMs). These prompts are subsequently optimized, with positive prompts focusing on features within each class, while negative prompts highlight features around category boundaries. Additionally, a graph-based architecture is employed to aggregate semantic-aware supervision from the optimized prompt representations and propagate it to the visual branch, thereby enhancing the performance of the energy-based OOD detector. Extensive experiments on two benchmarks, CIFAR-100 and ImageNet-1K, across eight OOD datasets and five different LLMs, demonstrate that our method outperforms state-of-the-art baselines.

</details>


### [65] [Facial Expression Recognition with YOLOv11 and YOLOv12: A Comparative Study](https://arxiv.org/abs/2511.10940)
*Umma Aymon,Nur Shazwani Kamarudin,Ahmad Fakhri Ab. Nasir*

Main category: cs.CV

TL;DR: 本研究评估了用于面部表情识别的轻量级 YOLO 模型 YOLOv11n 和 YOLOv12n 的性能。


<details>
  <summary>Details</summary>
Motivation: 在不受约束的真实环境中，面部表情识别仍然是一项具有挑战性的任务。

Method: 将 FER2013 和 KDEF 数据集转换为对象检测格式，并使用 mAP 0.5、精确率、召回率和混淆矩阵评估模型性能。

Result: YOLOv12n 在 KDEF 数据集上取得了最高的总体性能 (mAP 0.5 为 95.6)，并且在 FER2013 数据集上的 mAP 为 63.8，优于 YOLOv11n。YOLOv11n 在 FER2013 上表现出更高的精度 65.2。

Conclusion: 轻量级 YOLO 模型可以有效地平衡性能和效率，使其成为实时、资源受限的情感感知 AI 应用的强大候选者。

Abstract: Facial Expression Recognition remains a challenging task, especially in unconstrained, real-world environments. This study investigates the performance of two lightweight models, YOLOv11n and YOLOv12n, which are the nano variants of the latest official YOLO series, within a unified detection and classification framework for FER. Two benchmark classification datasets, FER2013 and KDEF, are converted into object detection format and model performance is evaluated using mAP 0.5, precision, recall, and confusion matrices. Results show that YOLOv12n achieves the highest overall performance on the clean KDEF dataset with a mAP 0.5 of 95.6, and also outperforms YOLOv11n on the FER2013 dataset in terms of mAP 63.8, reflecting stronger sensitivity to varied expressions. In contrast, YOLOv11n demonstrates higher precision 65.2 on FER2013, indicating fewer false positives and better reliability in noisy, real-world conditions. On FER2013, both models show more confusion between visually similar expressions, while clearer class separation is observed on the cleaner KDEF dataset. These findings underscore the trade-off between sensitivity and precision, illustrating how lightweight YOLO models can effectively balance performance and efficiency. The results demonstrate adaptability across both controlled and real-world conditions, establishing these models as strong candidates for real-time, resource-constrained emotion-aware AI applications.

</details>


### [66] [Heterogeneous Complementary Distillation](https://arxiv.org/abs/2511.10942)
*Liuchi Xu,Hao Zheng,Lu Wang,Lisheng Xu,Jun Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种异构互补蒸馏（HCD）框架，用于解决知识蒸馏中，当教师网络和学生网络结构差异大时，知识迁移效果不佳的问题。该框架通过整合教师和学生网络的互补特征，在共享logits空间对齐表示，并使用子logit解耦蒸馏（SDD）和正交损失（OL）来保证知识迁移的多样性和减少冗余。


<details>
  <summary>Details</summary>
Motivation: 传统的知识蒸馏方法主要为同构网络设计，难以有效解决异构网络结构差异带来的问题。现有的异构知识蒸馏方法通常计算成本高、设计复杂，或者过度依赖logit对齐，限制了互补特征的利用。

Method: HCD框架首先通过卷积投影器和自适应池化处理学生网络的中间特征，然后将其与教师网络倒数第二层的特征连接，并通过互补特征映射器（CFM）模块生成共享logits。然后，引入子logit解耦蒸馏（SDD），将共享logits分成多个子logits，并与教师网络的logits融合以纠正分类。为了保证子logit的多样性，提出了正交损失（OL）。

Result: 在CIFAR-100、细粒度数据集（如CUB200）和ImageNet-1K数据集上的大量实验表明，HCD优于最先进的KD方法。

Conclusion: HCD是一种有效的异构知识蒸馏解决方案，它通过保留学生网络特有的优势并利用教师网络的知识，增强了学生网络的鲁棒性和泛化能力。

Abstract: Knowledge distillation (KD)transfers the dark knowledge from a complex teacher to a compact student. However, heterogeneous architecture distillation, such as Vision Transformer (ViT) to ResNet18, faces challenges due to differences in spatial feature representations.Traditional KD methods are mostly designed for homogeneous architectures and hence struggle to effectively address the disparity. Although heterogeneous KD approaches have been developed recently to solve these issues, they often incur high computational costs and complex designs, or overly rely on logit alignment, which limits their ability to leverage the complementary features. To overcome these limitations, we propose Heterogeneous Complementary Distillation (HCD),a simple yet effective framework that integrates complementary teacher and student features to align representations in shared logits.These logits are decomposed and constrained to facilitate diverse knowledge transfer to the student. Specifically, HCD processes the student's intermediate features through convolutional projector and adaptive pooling, concatenates them with teacher's feature from the penultimate layer and then maps them via the Complementary Feature Mapper (CFM) module, comprising fully connected layer,to produce shared logits.We further introduce Sub-logit Decoupled Distillation (SDD) that partitions the shared logits into n sub-logits, which are fused with teacher's logits to rectify classification.To ensure sub-logit diversity and reduce redundant knowledge transfer, we propose an Orthogonality Loss (OL).By preserving student-specific strengths and leveraging teacher knowledge,HCD enhances robustness and generalization in students.Extensive experiments on the CIFAR-100, Fine-grained (e.g., CUB200)and ImageNet-1K datasets demonstrate that HCD outperforms state-of-the-art KD methods,establishing it as an effective solution for heterogeneous KD.

</details>


### [67] [Divide, Conquer and Unite: Hierarchical Style-Recalibrated Prototype Alignment for Federated Medical Image Segmentation](https://arxiv.org/abs/2511.10945)
*Xingyue Zhao,Wenke Huang,Xingguang Wang,Haoyu Zhao,Linghao Zhuang,Anwen Jiang,Guancheng Wan,Mang Ye*

Main category: cs.CV

TL;DR: 提出了一种新的联邦学习方法（FedBCS），通过对齐领域不变的上下文原型来弥合特征表示的差距。


<details>
  <summary>Details</summary>
Motivation: 医学机构在不共享数据的情况下训练全局模型时，来自不同扫描仪或协议的特征异构性是一个主要挑战。现有的方法忽略了多层次的线索和中间层中的领域特定偏差，从而降低了模型的鲁棒性。

Method: 提出了一种频域自适应风格重校准方法，将内容风格表示解耦，并学习最优风格参数，实现更鲁棒的领域不变原型。设计了一种上下文感知的双层原型对齐方法，从编码器和解码器的不同层提取领域不变原型，并将其与上下文信息融合，以实现更细粒度的表示对齐。

Result: 在两个公共数据集上的大量实验表明，该方法表现出卓越的性能。

Conclusion: FedBCS 方法有效地解决了联邦学习中特征异构性的问题，并在医学图像分割任务中取得了显著的成果。

Abstract: Federated learning enables multiple medical institutions to train a global model without sharing data, yet feature heterogeneity from diverse scanners or protocols remains a major challenge. Many existing works attempt to address this issue by leveraging model representations (e.g., mean feature vectors) to correct local training; however, they often face two key limitations: 1) Incomplete Contextual Representation Learning: Current approaches primarily focus on final-layer features, overlooking critical multi-level cues and thus diluting essential context for accurate segmentation. 2) Layerwise Style Bias Accumulation: Although utilizing representations can partially align global features, these methods neglect domain-specific biases within intermediate layers, allowing style discrepancies to build up and reduce model robustness. To address these challenges, we propose FedBCS to bridge feature representation gaps via domain-invariant contextual prototypes alignment. Specifically, we introduce a frequency-domain adaptive style recalibration into prototype construction that not only decouples content-style representations but also learns optimal style parameters, enabling more robust domain-invariant prototypes. Furthermore, we design a context-aware dual-level prototype alignment method that extracts domain-invariant prototypes from different layers of both encoder and decoder and fuses them with contextual information for finer-grained representation alignment. Extensive experiments on two public datasets demonstrate that our method exhibits remarkable performance.

</details>


### [68] [Abstract 3D Perception for Spatial Intelligence in Vision-Language Models](https://arxiv.org/abs/2511.10946)
*Yifan Liu,Fangneng Zhan,Kaichen Zhou,Yilun Du,Paul Pu Liang,Hanspeter Pfister*

Main category: cs.CV

TL;DR: SandboxVLM: 通过抽象边界框编码几何结构和物理运动学，提升视觉语言模型在3D任务中的性能，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在3D相关任务中表现不佳，因为2D训练与3D任务之间存在模态差距，导致从2D输入中检索3D信息效率低下。

Method: 提出SandboxVLM框架，包含四个阶段：生成具有抽象控制的多视角先验、代理高程、多视角投票和聚类、以及3D感知推理。

Result: 在多个基准测试和视觉语言模型骨干网络中，零样本设置下，该方法持续提高空间智能，例如在SAT Real上实现了8.3%的性能提升。

Conclusion: 通过3D抽象增强视觉语言模型，可以显著提高其3D推理能力，为通用具身智能提供了新的可能性。

Abstract: Vision-language models (VLMs) struggle with 3D-related tasks such as spatial cognition and physical understanding, which are crucial for real-world applications like robotics and embodied agents. We attribute this to a modality gap between the 3D tasks and the 2D training of VLM, which led to inefficient retrieval of 3D information from 2D input. To bridge this gap, we introduce SandboxVLM, a simple yet effective framework that leverages abstract bounding boxes to encode geometric structure and physical kinematics for VLM. Specifically, we design a 3D Sandbox reconstruction and perception pipeline comprising four stages: generating multi-view priors with abstract control, proxy elevation, multi-view voting and clustering, and 3D-aware reasoning. Evaluated in zero-shot settings across multiple benchmarks and VLM backbones, our approach consistently improves spatial intelligence, achieving an 8.3\% gain on SAT Real compared with baseline methods for instance. These results demonstrate that equipping VLMs with a 3D abstraction substantially enhances their 3D reasoning ability without additional training, suggesting new possibilities for general-purpose embodied intelligence.

</details>


### [69] [DEFT-LLM: Disentangled Expert Feature Tuning for Micro-Expression Recognition](https://arxiv.org/abs/2511.10948)
*Ren Zhang,Huilai Li,Chao qi,Guoliang Xu,Tianyu Zhou,Wei wei,Jianqin Yin*

Main category: cs.CV

TL;DR: DEFT-LLM通过多专家解耦实现运动语义对齐，从而更精确地捕捉微表情。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以区分静态外观和动态运动，文本标签与面部肌肉运动存在语义差距。

Method: 提出DEFT-LLM，利用Uni-MER数据集对齐文本和局部面部运动，并设计三专家架构解耦面部动态。

Result: 在多个MER基准测试中表现出色，尤其是在局部面部运动的可解释建模方面。

Conclusion: DEFT-LLM注入了有效的物理先验知识，并利用了大型语言模型的跨模态推理能力，从而能够精确地捕捉微妙的情感线索。

Abstract: Micro expression recognition (MER) is crucial for inferring genuine emotion. Applying a multimodal large language model (MLLM) to this task enables spatio-temporal analysis of facial motion and provides interpretable descriptions. However, there are still two core challenges: (1) The entanglement of static appearance and dynamic motion cues prevents the model from focusing on subtle motion; (2) Textual labels in existing MER datasets do not fully correspond to underlying facial muscle movements, creating a semantic gap between text supervision and physical motion. To address these issues, we propose DEFT-LLM, which achieves motion semantic alignment by multi-expert disentanglement. We first introduce Uni-MER, a motion-driven instruction dataset designed to align text with local facial motion. Its construction leverages dual constraints from optical flow and Action Unit (AU) labels to ensure spatio-temporal consistency and reasonable correspondence to the movements. We then design an architecture with three experts to decouple facial dynamics into independent and interpretable representations (structure, dynamic textures, and motion-semantics). By integrating the instruction-aligned knowledge from Uni-MER into DEFT-LLM, our method injects effective physical priors for micro expressions while also leveraging the cross modal reasoning ability of large language models, thus enabling precise capture of subtle emotional cues. Experiments on multiple challenging MER benchmarks demonstrate state-of-the-art performance, as well as a particular advantage in interpretable modeling of local facial motion.

</details>


### [70] [Language-Guided Graph Representation Learning for Video Summarization](https://arxiv.org/abs/2511.10953)
*Wenrui Li,Wei Han,Hengyu Man,Wangmeng Zuo,Xiaopeng Fan,Yonghong Tian*

Main category: cs.CV

TL;DR: 提出了一种新颖的语言引导图表示学习网络（LGRLN）用于视频摘要，该网络在多个基准测试中优于现有方法，并且减少了推理时间和模型参数。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕获视频内容中的全局依赖关系和适应多模态用户定制方面面临挑战，且视频帧之间的时间接近性并不总是对应于语义接近性。

Method: 引入了一个视频图生成器，将视频帧转换为结构化图，以保留时间顺序和上下文依赖关系；设计了一个具有双阈值图卷积机制的内图关系推理模块，区分语义相关的帧；提出了语言引导的跨模态嵌入模块，生成带有特定文本描述的视频摘要，并将摘要生成输出建模为伯努利分布的混合，用EM算法求解。

Result: 该方法在多个基准测试中优于现有方法，推理时间减少了87.8%，模型参数减少了91.7%。

Conclusion: LGRLN有效地解决了视频摘要中的全局依赖关系捕获、多模态用户定制以及时间与语义接近性不一致的问题，并在效率上取得了显著提升。

Abstract: With the rapid growth of video content on social media, video summarization has become a crucial task in multimedia processing. However, existing methods face challenges in capturing global dependencies in video content and accommodating multimodal user customization. Moreover, temporal proximity between video frames does not always correspond to semantic proximity. To tackle these challenges, we propose a novel Language-guided Graph Representation Learning Network (LGRLN) for video summarization. Specifically, we introduce a video graph generator that converts video frames into a structured graph to preserve temporal order and contextual dependencies. By constructing forward, backward and undirected graphs, the video graph generator effectively preserves the sequentiality and contextual relationships of video content. We designed an intra-graph relational reasoning module with a dual-threshold graph convolution mechanism, which distinguishes semantically relevant frames from irrelevant ones between nodes. Additionally, our proposed language-guided cross-modal embedding module generates video summaries with specific textual descriptions. We model the summary generation output as a mixture of Bernoulli distribution and solve it with the EM algorithm. Experimental results show that our method outperforms existing approaches across multiple benchmarks. Moreover, we proposed LGRLN reduces inference time and model parameters by 87.8% and 91.7%, respectively. Our codes and pre-trained models are available at https://github.com/liwrui/LGRLN.

</details>


### [71] [Text-guided Weakly Supervised Framework for Dynamic Facial Expression Recognition](https://arxiv.org/abs/2511.10958)
*Gunho Jung,Heejo Kong,Seong-Whan Lee*

Main category: cs.CV

TL;DR: 提出了一种文本引导的弱监督框架，通过结合语义指导和连贯的时间建模来增强基于MIL的DFER。


<details>
  <summary>Details</summary>
Motivation: 动态面部表情识别(DFER)旨在通过对视频序列中面部运动的时间变化进行建模来识别情绪状态。DFER的一个关键挑战是多对一的标签问题，其中由大量帧组成的视频被分配一个单一的情绪标签。一个常见的策略是把DFER定义为一个多实例学习(MIL)问题。然而，基于mil的方法天生就存在情感表达的视觉多样性和时间复杂性问题。

Method: 该方法结合了视觉-语言预训练(VLP)模型，通过情感语境的细粒度文本描述提供语义指导。此外，引入了视觉提示，将丰富的文本情感标签与视觉实例特征对齐，实现细粒度的推理和帧级相关性估计。此外，设计了一个多粒度时间网络，以联合捕获短期面部动态和长期情感流，确保跨时间的连贯情感理解。

Result: TG-DFER在弱监督下实现了更好的泛化性、可解释性和时间敏感性。

Conclusion: 该方法在弱监督下实现了更好的泛化性、可解释性和时间敏感性。

Abstract: Dynamic facial expression recognition (DFER) aims to identify emotional states by modeling the temporal changes in facial movements across video sequences. A key challenge in DFER is the many-to-one labeling problem, where a video composed of numerous frames is assigned a single emotion label. A common strategy to mitigate this issue is to formulate DFER as a Multiple Instance Learning (MIL) problem. However, MIL-based approaches inherently suffer from the visual diversity of emotional expressions and the complexity of temporal dynamics. To address this challenge, we propose TG-DFER, a text-guided weakly supervised framework that enhances MIL-based DFER by incorporating semantic guidance and coherent temporal modeling. We incorporate a vision-language pre-trained (VLP) model is integrated to provide semantic guidance through fine-grained textual descriptions of emotional context. Furthermore, we introduce visual prompts, which align enriched textual emotion labels with visual instance features, enabling fine-grained reasoning and frame-level relevance estimation. In addition, a multi-grained temporal network is designed to jointly capture short-term facial dynamics and long-range emotional flow, ensuring coherent affective understanding across time. Extensive results demonstrate that TG-DFER achieves improved generalization, interpretability, and temporal sensitivity under weak supervision.

</details>


### [72] [ERMoE: Eigen-Reparameterized Mixture-of-Experts for Stable Routing and Interpretable Specialization](https://arxiv.org/abs/2511.10971)
*Anzhe Cheng,Shukai Duan,Shixuan Li,Chenzhong Yin,Mingxi Cheng,Heng Ping,Tamoghna Chattopadhyay,Sophia I Thomopoulos,Shahin Nazarian,Paul Thompson,Paul Bogdan*

Main category: cs.CV

TL;DR: ERMoE: A sparse MoE transformer using an Eigenbasis Score for routing, improving stability, interpretability, and performance.


<details>
  <summary>Details</summary>
Motivation: Misalignment between router logits and expert structure in MoE models leads to unstable routing, underutilization, and load imbalances.

Method: Reparameterizes experts in a learned orthonormal eigenbasis and uses cosine similarity between input features and expert basis for routing (Eigenbasis Score).

Result: Achieves state-of-the-art accuracy on ImageNet classification and cross-modal image-text retrieval, improves brain age prediction accuracy, and produces flatter expert load distributions.

Conclusion: ERMoE introduces a new architectural principle for sparse expert models that directly addresses routing instabilities and enables improved performance with scalable, interpretable specialization.

Abstract: Mixture-of-Experts (MoE) architectures expand model capacity by sparsely activating experts but face two core challenges: misalignment between router logits and each expert's internal structure leads to unstable routing and expert underutilization, and load imbalances create straggler bottlenecks. Standard solutions, such as auxiliary load-balancing losses, can reduce load disparities but often weaken expert specialization and hurt downstream performance. To address these issues, we propose ERMoE, a sparse MoE transformer that reparameterizes each expert in a learned orthonormal eigenbasis and replaces learned gating logits with an "Eigenbasis Score", defined as the cosine similarity between input features and an expert's basis. This content-aware routing ties token assignments directly to experts' representation spaces, stabilizing utilization and promoting interpretable specialization without sacrificing sparsity. Crucially, ERMoE removes the need for explicit balancing losses and avoids the interfering gradients they introduce. We show that ERMoE achieves state-of-the-art accuracy on ImageNet classification and cross-modal image-text retrieval benchmarks (e.g., COCO, Flickr30K), while naturally producing flatter expert load distributions. Moreover, a 3D MRI variant (ERMoE-ba) improves brain age prediction accuracy by more than 7\% and yields anatomically interpretable expert specializations. ERMoE thus introduces a new architectural principle for sparse expert models that directly addresses routing instabilities and enables improved performance with scalable, interpretable specialization.

</details>


### [73] [Preserving Cross-Modal Consistency for CLIP-based Class-Incremental Learning](https://arxiv.org/abs/2511.10974)
*Haoran Chen,Houze Xu,Micah Goldblum,Daoguo Dong,Zuxuan Wu*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为DMC的CLIP-based CIL框架，用于解决在持续学习新类别时出现的分类器偏差问题。DMC包含两个阶段，分别优化视觉编码器和文本软提示，并引入最优传输引导的校准策略DMC-OT，以解决视觉编码器更新带来的分布漂移问题。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在持续学习领域面临挑战，尤其是在学习特定任务的软提示时，容易对最近的类别过拟合，导致严重的分类器偏差。

Method: 该论文提出DMC框架，通过解耦视觉编码器和文本软提示的优化，以及引入最优传输引导的校准策略DMC-OT来解决分布漂移问题。DMC包含两个阶段，每个阶段都固定另一个模态，以保持跨模态对齐。DMC-OT则通过最优传输来校准记忆统计，并采用任务特定的提示设计来增强任务间可分离性。

Result: 在CIFAR-100、Imagenet-R、CUB-200和UCF-101数据集上的实验表明，DMC和DMC-OT均取得了state-of-the-art的性能，其中DMC-OT的准确率平均提高了1.80%。

Conclusion: DMC框架及其增强版本DMC-OT有效地解决了CLIP模型在持续学习中的分类器偏差和分布漂移问题，并在多个数据集上取得了优异的性能。

Abstract: Class-incremental learning (CIL) enables models to continuously learn new categories from sequential tasks without forgetting previously acquired knowledge. While recent advances in vision-language models such as CLIP have demonstrated strong generalization across domains, extending them to continual settings remains challenging. In particular, learning task-specific soft prompts for newly introduced classes often leads to severe classifier bias, as the text prototypes overfit to recent categories when prior data are unavailable. In this paper, we propose DMC, a simple yet effective two-stage framework for CLIP-based CIL that decouples the adaptation of the vision encoder and the optimization of textual soft prompts. Each stage is trained with the other frozen, allowing one modality to act as a stable semantic anchor for the other to preserve cross-modal alignment. Furthermore, current CLIP-based CIL approaches typically store class-wise Gaussian statistics for generative replay, yet they overlook the distributional drift that arises when the vision encoder is updated over time. To address this issue, we introduce DMC-OT, an enhanced version of DMC that incorporates an optimal-transport guided calibration strategy to align memory statistics across evolving encoders, along with a task-specific prompting design that enhances inter-task separability. Extensive experiments on CIFAR-100, Imagenet-R, CUB-200, and UCF-101 demonstrate that both DMC and DMC-OT achieve state-of-the-art performance, with DMC-OT further improving accuracy by an average of 1.80%.

</details>


### [74] [PAS: A Training-Free Stabilizer for Temporal Encoding in Video LLMs](https://arxiv.org/abs/2511.10979)
*Bowen Sun,Yujun Cai,Ming-Hsuan Yang,Hang Wu,Yiwei Wang*

Main category: cs.CV

TL;DR: 视频LLM在时间上不一致，帧时间上的微小变化会反转注意力并抑制相关帧。这是由于多模态RoPE将旋转位置嵌入扩展到视频造成的。提出的Phase Aggregated Smoothing (PAS)通过在头部应用小的相反相位偏移，然后聚合它们的输出来解决这个问题。PAS在多个视频理解基准上表现出一致的改进。


<details>
  <summary>Details</summary>
Motivation: 视频LLM存在时间不一致的问题，帧时间上的微小变化会反转注意力并抑制相关帧。这种不稳定性源于多模态RoPE。

Method: 提出了一种名为Phase Aggregated Smoothing (PAS)的简单、无训练机制，该机制通过在头部应用小的相反相位偏移，然后聚合它们的输出，从而平滑时间核并降低相位敏感性，而不改变位置编码结构。

Result: 在多个视频理解基准上，PAS在匹配的token预算下表现出一致的改进，且计算开销可忽略不计。

Conclusion: PAS为视频LLM中的鲁棒时间编码提供了一个即插即用的升级方案。

Abstract: Video LLMs suffer from temporal inconsistency: small shifts in frame timing can flip attention and suppress relevant frames. We trace this instability to the common extension of Rotary Position Embeddings to video through multimodal RoPE. The induced inverse Fourier time kernel exhibits frame-scale ripples that multiply adjacent frames by different factors, which perturbs attention that should otherwise be governed by the raw query key inner product. We present Phase Aggregated Smoothing (PAS), a simple, training-free mechanism that applies small opposed phase offsets across heads and then aggregates their outputs. PAS preserves the per-head spectrum magnitude, while the aggregation effectively smooths the temporal kernel and reduces phase sensitivity without changing the positional encoding structure. Our analysis shows that the RoPE rotated logit can be approximated as a content dot product scaled by a time kernel; smoothing this kernel yields Lipschitz stability of attention to small temporal shifts; multi phase averaging attenuates high frequency ripples while preserving per-head spectra under Nyquist-valid sampling. Experiments on multiple video understanding benchmarks under matched token budgets show consistent improvements with negligible computational overhead. PAS provides a plug and play upgrade for robust temporal encoding in Video LLMs.

</details>


### [75] [Binary Verification for Zero-Shot Vision](https://arxiv.org/abs/2511.10983)
*Jeffrey Liu,Rongbin Hu*

Main category: cs.CV

TL;DR: 提出了一种免训练的二元验证工作流程，用于使用现成的 VLM 进行零样本视觉处理。


<details>
  <summary>Details</summary>
Motivation: 为了解决开放式视觉查询的问题，并提升零样本视觉处理的性能。

Method: 该方法包括量化和二元化两个步骤。量化将开放式查询转换为多项选择题，二元化则为每个候选答案提出一个真/假问题。

Result: 该工作流程在指代表达式定位、空间推理和 BLINK-Jigsaw 等任务上取得了显著的改进。

Conclusion: 该工作流程提供了一种简单且统一的途径，通过强调推理时间设计来增强当前的 VLM 的零样本视觉处理能力。

Abstract: We propose a training-free, binary verification workflow for zero-shot vision with off-the-shelf VLMs. It comprises two steps: (i) quantization, which turns the open-ended query into a multiple-choice question (MCQ) with a small, explicit list of unambiguous candidates; and (ii) binarization, which asks one True/False question per candidate and resolves deterministically: if exactly one is True, select it; otherwise, revert to an MCQ over the remaining plausible candidates. We evaluate the workflow on referring expression grounding (REC), spatial reasoning (Spatial-Map, Spatial-Grid, Spatial-Maze), and BLINK-Jigsaw. Relative to answering open-ended queries directly, quantization to MCQ yields large gains, and True/False binarization provides a consistent additional boost. Across all tasks, the same workflow produces significant improvements, indicating generality. Our theory formalizes how open-ended vision queries can be quantized to MCQs and further binarized into True/False verifications, establishing a hardness ladder. A simple analysis explains why Boolean resolution boosts accuracy. Together, these components yield a simple and unified workflow that emphasizes inference-time design over task-specific training. It offers a practical, drop-in path to stronger zero-shot vision with today's VLMs.

</details>


### [76] [Rethinking Autoregressive Models for Lossless Image Compression via Hierarchical Parallelism and Progressive Adaptation](https://arxiv.org/abs/2511.10991)
*Daxin Li,Yuanchao Bai,Kai Wang,Wenbo Zhao,Junjun Jiang,Xianming Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于分层并行和渐进自适应的框架，重新将纯自回归确立为一种高性能且实用的解决方案。


<details>
  <summary>Details</summary>
Motivation: 自回归 (AR) 模型是学习无损图像压缩的理论性能基准，但由于计算成本过高，通常被认为是不切实际的。

Method: 该方法通过分层分解结构和内容感知卷积门控来有效捕获空间依赖性，并提出了Cache-then-Select Inference (CSI) 和 Adaptive Focus Coding (AFC) 两种关键优化。

Result: 在各种数据集（自然、卫星、医疗）上的实验表明，该方法实现了新的最先进的压缩水平。

Conclusion: 该方法在学习无损压缩方面建立了一个新的基准，表明精心设计的 AR 框架可以通过少量参数和有竞争力的编码速度提供优于现有方法的显着收益。

Abstract: Autoregressive (AR) models, the theoretical performance benchmark for learned lossless image compression, are often dismissed as impractical due to prohibitive computational cost. This work re-thinks this paradigm, introducing a framework built on hierarchical parallelism and progressive adaptation that re-establishes pure autoregression as a top-performing and practical solution. Our approach is embodied in the Hierarchical Parallel Autoregressive ConvNet (HPAC), an ultra-lightweight pre-trained model using a hierarchical factorized structure and content-aware convolutional gating to efficiently capture spatial dependencies. We introduce two key optimizations for practicality: Cache-then-Select Inference (CSI), which accelerates coding by eliminating redundant computations, and Adaptive Focus Coding (AFC), which efficiently extends the framework to high bit-depth images. Building on this efficient foundation, our progressive adaptation strategy is realized by Spatially-Aware Rate-Guided Progressive Fine-tuning (SARP-FT). This instance-level strategy fine-tunes the model for each test image by optimizing low-rank adapters on progressively larger, spatially-continuous regions selected via estimated information density. Experiments on diverse datasets (natural, satellite, medical) validate that our method achieves new state-of-the-art compression. Notably, our approach sets a new benchmark in learned lossless compression, showing a carefully designed AR framework can offer significant gains over existing methods with a small parameter count and competitive coding speeds.

</details>


### [77] [CLUE: Controllable Latent space of Unprompted Embeddings for Diversity Management in Text-to-Image Synthesis](https://arxiv.org/abs/2511.10993)
*Keunwoo Park,Jihye Chae,Joong Ho Ahn,Jihoon Kweon*

Main category: cs.CV

TL;DR: CLUE是一种生成模型框架，它通过固定格式的提示来实现多样化的生成，同时保持稳定性，而不需要任何额外的数据。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像合成模型在医学等专业领域面临数据有限的挑战， 导致生成图像的多样性和稳定性难以兼顾。

Method: CLUE基于Stable Diffusion架构，采用Style Encoder处理图像和提示，生成风格嵌入，并将其输入到U-Net架构的一个新的第二注意力层中。通过Kullback-Leibler散度，潜在空间实现了图像特征在高斯区域内的连续表示，独立于提示。

Result: 在耳炎数据集上，CLUE将FID降低到9.30（对比46.81），并将召回率提高到70.29%（对比49.60%）。仅使用合成数据训练的分类器在1000%规模下实现了83.21%的F1分数（对比73.83%）。合成数据与等量的真实数据相结合，实现了94.76%的F1分数，高于仅使用真实数据时的分数。在外部数据集上，仅合成训练在1000%规模下实现了76.77%的F1分数（对比60.61%）。结合方法实现了85.78%的F1分数，高于仅使用内部数据集时的分数。

Conclusion: CLUE能够从有限的数据集中实现多样化且稳定的图像生成，并且可以作为领域特定应用程序的有效数据增强方法。

Abstract: Text-to-image synthesis models require the ability to generate diverse images while maintaining stability. To overcome this challenge, a number of methods have been proposed, including the collection of prompt-image datasets and the integration of additional data modalities during training. Although these methods have shown promising results in general domains, they face limitations when applied to specialized fields such as medicine, where only limited types and insufficient amounts of data are available. We present CLUE (Controllable Latent space of Unprompted Embeddings), a generative model framework that achieves diverse generation while maintaining stability through fixed-format prompts without requiring any additional data. Based on the Stable Diffusion architecture, CLUE employs a Style Encoder that processes images and prompts to generate style embeddings, which are subsequently fed into a new second attention layer of the U-Net architecture. Through Kullback-Leibler divergence, the latent space achieves continuous representation of image features within Gaussian regions, independent of prompts. Performance was assessed on otitis media dataset. CLUE reduced FID to 9.30 (vs. 46.81) and improved recall to 70.29% (vs. 49.60%). A classifier trained on synthetic-only data at 1000% scale achieved an F1 score of 83.21% (vs. 73.83%). Combining synthetic data with equal amounts of real data achieved an F1 score of 94.76%, higher than when using only real data. On an external dataset, synthetic-only training achieved an F1 score of 76.77% (vs. 60.61%) at 1000% scale. The combined approach achieved an F1 score of 85.78%, higher than when using only the internal dataset. These results demonstrate that CLUE enables diverse yet stable image generation from limited datasets and serves as an effective data augmentation method for domain-specific applications.

</details>


### [78] [PROMISE: Prompt-Attentive Hierarchical Contrastive Learning for Robust Cross-Modal Representation with Missing Modalities](https://arxiv.org/abs/2511.10997)
*Jiajun Chen,Sai Cheng,Yutao Yuan,Yirui Zhang,Haitao Yuan,Peng Peng,Yi Zhong*

Main category: cs.CV

TL;DR: PROMISE框架通过提示注意分层对比学习，提高模型在模态缺失情况下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态模型在模态缺失时性能下降，因为完整和不完整数据之间的表示学习不一致，且现有方法无法充分保持跨模态一致性。

Method: 提出PROMISE框架，将多模态提示学习融入分层对比学习框架，并设计提示注意机制，动态生成鲁棒和一致的表示。

Result: 在基准数据集上的大量实验和消融研究表明，PROMISE的性能优于当前最先进的多模态方法。

Conclusion: PROMISE框架有效地弥合了完整和不完整数据之间的表示差距，提高了模型在模态缺失情况下的性能。

Abstract: Multimodal models integrating natural language and visual information have substantially improved generalization of representation models. However, their effectiveness significantly declines in real-world situations where certain modalities are missing or unavailable. This degradation primarily stems from inconsistent representation learning between complete multimodal data and incomplete modality scenarios. Existing approaches typically address missing modalities through relatively simplistic generation methods, yet these approaches fail to adequately preserve cross-modal consistency, leading to suboptimal performance. To overcome this limitation, we propose a novel multimodal framework named PROMISE, a PROMpting-Attentive HIerarchical ContraStive LEarning approach designed explicitly for robust cross-modal representation under conditions of missing modalities. Specifically, PROMISE innovatively incorporates multimodal prompt learning into a hierarchical contrastive learning framework, equipped with a specially designed prompt-attention mechanism. This mechanism dynamically generates robust and consistent representations for scenarios where particular modalities are absent, thereby effectively bridging the representational gap between complete and incomplete data. Extensive experiments conducted on benchmark datasets, along with comprehensive ablation studies, clearly demonstrate the superior performance of PROMISE compared to current state-of-the-art multimodal methods.

</details>


### [79] [EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation](https://arxiv.org/abs/2511.11002)
*Zongyang Qiu,Bingyuan Wang,Xingbei Chen,Yingqing He,Zeyu Wang*

Main category: cs.CV

TL;DR: 提出了EmoVid数据集，用于弥合情感理解与生成任务之间的差距，特别是在风格化和非真实背景下。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成系统主要关注低层次的视觉指标，而忽略了情感维度。视频社区缺乏专用资源来桥接情感理解与生成任务，特别是在风格化和非真实背景下。

Method: 1. 构建了EmoVid数据集，包含卡通动画、电影剪辑和动画贴纸，并标注了情感标签、视觉属性和文本字幕。2. 通过系统分析，揭示了视觉特征与跨不同视频形式的情感感知之间的时空模式。3. 通过微调Wan2.1模型，开发了一种情感条件视频生成技术。

Result: 在定量指标和文本到视频和图像到视频任务生成的视频的视觉质量方面，结果显示出显着改善。

Conclusion: EmoVid为情感视频计算建立了一个新的基准。这项工作不仅为艺术风格视频中的视觉情感分析提供了有价值的见解，而且为增强视频生成中的情感表达提供了实用的方法。

Abstract: Emotion plays a pivotal role in video-based expression, but existing video generation systems predominantly focus on low-level visual metrics while neglecting affective dimensions. Although emotion analysis has made progress in the visual domain, the video community lacks dedicated resources to bridge emotion understanding with generative tasks, particularly for stylized and non-realistic contexts. To address this gap, we introduce EmoVid, the first multimodal, emotion-annotated video dataset specifically designed for creative media, which includes cartoon animations, movie clips, and animated stickers. Each video is annotated with emotion labels, visual attributes (brightness, colorfulness, hue), and text captions. Through systematic analysis, we uncover spatial and temporal patterns linking visual features to emotional perceptions across diverse video forms. Building on these insights, we develop an emotion-conditioned video generation technique by fine-tuning the Wan2.1 model. The results show a significant improvement in both quantitative metrics and the visual quality of generated videos for text-to-video and image-to-video tasks. EmoVid establishes a new benchmark for affective video computing. Our work not only offers valuable insights into visual emotion analysis in artistically styled videos, but also provides practical methods for enhancing emotional expression in video generation.

</details>


### [80] [MeCaMIL: Causality-Aware Multiple Instance Learning for Fair and Interpretable Whole Slide Image Diagnosis](https://arxiv.org/abs/2511.11004)
*Yiran Song,Yikai Zhang,Shuang Zhou,Guojun Xiong,Xiaofeng Yang,Nian Wang,Fenglong Ma,Rui Zhang,Mingquan Lin*

Main category: cs.CV

TL;DR: MeCaMIL: A causality-aware multiple instance learning framework for whole slide image analysis that addresses limitations of existing methods by incorporating causal interpretability and patient demographics to improve fairness and diagnostic performance.


<details>
  <summary>Details</summary>
Motivation: Existing MIL methods lack causal interpretability and fail to integrate patient demographics, leading to fairness concerns and hindering clinical translation.

Method: MeCaMIL explicitly models demographic confounders through structured causal graphs and employs causal inference to disentangle disease-relevant signals from spurious demographic correlations.

Result: MeCaMIL achieves state-of-the-art performance on three benchmarks (CAMELYON16, TCGA-Lung, and TCGA-Multi) and superior fairness, with significant reductions in demographic disparity variance. It also generalizes to survival prediction.

Conclusion: MeCaMIL is a principled framework for fair, interpretable, and clinically actionable AI in digital pathology.

Abstract: Multiple instance learning (MIL) has emerged as the dominant paradigm for whole slide image (WSI) analysis in computational pathology, achieving strong diagnostic performance through patch-level feature aggregation. However, existing MIL methods face critical limitations: (1) they rely on attention mechanisms that lack causal interpretability, and (2) they fail to integrate patient demographics (age, gender, race), leading to fairness concerns across diverse populations. These shortcomings hinder clinical translation, where algorithmic bias can exacerbate health disparities. We introduce \textbf{MeCaMIL}, a causality-aware MIL framework that explicitly models demographic confounders through structured causal graphs. Unlike prior approaches treating demographics as auxiliary features, MeCaMIL employs principled causal inference -- leveraging do-calculus and collider structures -- to disentangle disease-relevant signals from spurious demographic correlations. Extensive evaluation on three benchmarks demonstrates state-of-the-art performance across CAMELYON16 (ACC/AUC/F1: 0.939/0.983/0.946), TCGA-Lung (0.935/0.979/0.931), and TCGA-Multi (0.977/0.993/0.970, five cancer types). Critically, MeCaMIL achieves superior fairness -- demographic disparity variance drops by over 65% relative reduction on average across attributes, with notable improvements for underserved populations. The framework generalizes to survival prediction (mean C-index: 0.653, +0.017 over best baseline across five cancer types). Ablation studies confirm causal graph structure is essential -- alternative designs yield 0.048 lower accuracy and 4.2x times worse fairness. These results establish MeCaMIL as a principled framework for fair, interpretable, and clinically actionable AI in digital pathology. Code will be released upon acceptance.

</details>


### [81] [Draft and Refine with Visual Experts](https://arxiv.org/abs/2511.11005)
*Sungheon Jeong,Ryozo Masukawa,Jihong Park,Sanggeon Yun,Wenjun Huang,Hanning Chen,Mahdi Imani,Mohsen Imani*

Main category: cs.CV

TL;DR: 大型视觉语言模型（LVLMs）过度依赖语言先验，忽略视觉证据，导致产生无根据或幻觉反应。为了解决这个问题，我们提出了一个名为Draft and Refine (DnR)的agent框架，该框架通过问题条件利用率指标来量化模型对视觉证据的依赖程度，并利用外部视觉专家的反馈来改进模型的视觉基础。


<details>
  <summary>Details</summary>
Motivation: LVLMs在多模态推理方面表现出色，但由于过度依赖语言先验而忽略视觉证据，导致产生无根据或幻觉反应。现有模型缺乏对视觉信息利用程度的定量评估。

Method: 我们提出了Draft and Refine (DnR)框架，该框架使用问题条件利用率指标来量化模型对视觉证据的依赖程度。该指标通过构建查询条件相关性图来定位问题相关的视觉线索，并通过相关性引导的概率掩蔽来测量依赖性。DnR agent利用外部视觉专家的反馈来改进其初始草案，从而加强视觉基础。

Result: 在VQA和图像描述基准测试上的实验表明，该方法能够持续提高准确性并减少幻觉。

Conclusion: 测量视觉利用率是实现更可解释和证据驱动的多模态agent系统的有效途径。

Abstract: While recent Large Vision-Language Models (LVLMs) exhibit strong multimodal reasoning abilities, they often produce ungrounded or hallucinated responses because they rely too heavily on linguistic priors instead of visual evidence. This limitation highlights the absence of a quantitative measure of how much these models actually use visual information during reasoning. We propose Draft and Refine (DnR), an agent framework driven by a question-conditioned utilization metric. The metric quantifies the model's reliance on visual evidence by first constructing a query-conditioned relevance map to localize question-specific cues and then measuring dependence through relevance-guided probabilistic masking. Guided by this metric, the DnR agent refines its initial draft using targeted feedback from external visual experts. Each expert's output (such as boxes or masks) is rendered as visual cues on the image, and the model is re-queried to select the response that yields the largest improvement in utilization. This process strengthens visual grounding without retraining or architectural changes. Experiments across VQA and captioning benchmarks show consistent accuracy gains and reduced hallucination, demonstrating that measuring visual utilization provides a principled path toward more interpretable and evidence-driven multimodal agent systems.

</details>


### [82] [VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models](https://arxiv.org/abs/2511.11007)
*Xinlei Yu,Chengming Xu,Guibin Zhang,Zhangquan Chen,Yudong Zhang,Yongbo He,Peng-Tao Jiang,Jiangning Zhang,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为VisMem的框架，通过模拟人类认知记忆，增强视觉语言模型（VLM）在复杂视觉任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在复杂视觉任务中存在“视觉处理瓶颈”，即容易丢失视觉证据和缺乏上下文视觉体验。

Method: 该框架包含一个短时视觉主导记忆模块和一个长时语义主导记忆模块，分别用于细粒度的感知保持和抽象的语义巩固。

Result: 在各种视觉理解、推理和生成任务中，VisMem相比原始模型平均提升了11.8%的性能，优于其他模型。

Conclusion: VisMem为潜在空间记忆增强建立了一种新的范式。

Abstract: Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a "visual processing bottleneck": a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.8% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.

</details>


### [83] [SP-Guard: Selective Prompt-adaptive Guidance for Safe Text-to-Image Generation](https://arxiv.org/abs/2511.11014)
*Sumin Yu,Taesup Moon*

Main category: cs.CV

TL;DR: 提出了SP-Guard，一个自适应和选择性的图像生成安全引导方法。


<details>
  <summary>Details</summary>
Motivation: 现有的图像生成引导方法缺乏自适应性和选择性，容易产生有害内容。

Method: 通过估计prompt的有害程度，并应用选择性的引导mask来引导不安全区域。

Result: SP-Guard比现有方法生成更安全的图像，同时最大限度地减少了意外的内容更改。

Conclusion: 强调了图像生成中透明性和可控性的重要性。

Abstract: While diffusion-based T2I models have achieved remarkable image generation quality, they also enable easy creation of harmful content, raising social concerns and highlighting the need for safer generation. Existing inference-time guiding methods lack both adaptivity--adjusting guidance strength based on the prompt--and selectivity--targeting only unsafe regions of the image. Our method, SP-Guard, addresses these limitations by estimating prompt harmfulness and applying a selective guidance mask to guide only unsafe areas. Experiments show that SP-Guard generates safer images than existing methods while minimizing unintended content alteration. Beyond improving safety, our findings highlight the importance of transparency and controllability in image generation.

</details>


### [84] [SUPER Decoder Block for Reconstruction-Aware U-Net Variants](https://arxiv.org/abs/2511.11015)
*Siheon Joo,Hongjo Kim*

Main category: cs.CV

TL;DR: 提出了一种名为选择性抑制完美重建(SUPER)的即插即用解码器块，用于解决U-Net变体在逆问题中信息损失的问题，提高高频细节的恢复。


<details>
  <summary>Details</summary>
Motivation: U-Net变体在逆问题中被广泛采用，但仍然存在信息损失，限制了精细高频细节的恢复。

Method: 利用小波的完美重建(PR)特性来防止信息退化，同时选择性地抑制(SS)冗余特征。SUPER作为一种即插即用的解码器块，适用于各种U-Net变体。

Result: 在各种裂纹基准测试中，包括最先进的模型，证明了所提出的SUPER解码器块的结构潜力。在CrackVision12K数据集的小规模域内实验中，SUPER显著提高了细裂纹分割性能，特别是在小于4像素的裂纹中。在SIDD智能手机图像去噪中，SUPER仍然实现了PSNR的适度增益。

Conclusion: 验证了其在U-Net变体中的即插即用通用性，在统一的、重建感知的框架内实现了高频保真度和全局一致性。

Abstract: Skip-connected encoder-decoder architectures (U-Net variants) are widely adopted for inverse problems but still suffer from information loss, limiting recovery of fine high-frequency details. We present Selectively Suppressed Perfect Reconstruction (SUPER), which exploits the perfect reconstruction (PR) property of wavelets to prevent information degradation while selectively suppressing (SS) redundant features. Free from rigid framelet constraints, SUPER serves as a plug-and-play decoder block for diverse U-Net variants, eliminating their intrinsic reconstruction bottlenecks and enhancing representational richness. Experiments across diverse crack benchmarks, including state-of-the-art (SOTA) models, demonstrate the structural potential of the proposed SUPER Decoder Block. Maintaining comparable computational cost, SUPER enriches representational diversity through increased parameterization. In small-scale in-domain experiments on the CrackVision12K dataset, SUPER markedly improves thin-crack segmentation performance, particularly for cracks narrower than 4 px, underscoring its advantage in high-frequency dominant settings. In smartphone image denoising on SIDD, where low-frequency components prevail, SUPER still achieves a moderate gain in PSNR, confirming its robustness across low- and high-frequency regimes. These results validate its plug-and-play generality across U-Net variants, achieving high-frequency fidelity and global coherence within a unified, reconstruction-aware framework.

</details>


### [85] [AirCopBench: A Benchmark for Multi-drone Collaborative Embodied Perception and Reasoning](https://arxiv.org/abs/2511.11025)
*Jirong Zha,Yuxuan Fan,Tianyu Zhang,Geng Chen,Yingfeng Chen,Chen Gao,Xinlei Chen*

Main category: cs.CV

TL;DR: 提出了AirCopBench，一个用于评估多模态大型语言模型在具身空中协作感知方面的基准。


<details>
  <summary>Details</summary>
Motivation: 现有的多图像基准主要针对使用高质量单智能体图像的基本感知任务，因此无法评估多模态大型语言模型在更复杂的、以自我为中心的协作场景中，尤其是在现实世界中感知退化条件下的表现。多无人机系统相比单传感器设置，具有增强的覆盖范围、鲁棒性和协作能力，但缺乏评估多智能体协作感知的基准。

Method: 构建了AirCopBench，该基准包含来自模拟器和真实世界数据的14.6k+问题，涵盖四个关键任务维度：场景理解、对象理解、感知评估和协作决策，跨越14个任务类型。使用来自具有注释协作事件的挑战性退化感知场景的数据，通过基于模型、规则和人工的方法生成大规模问题，并进行严格的质量控制。

Result: 在40个多模态大型语言模型上的评估表明，在协作感知任务中存在显著的性能差距，最好的模型平均落后人类24.38%，并且在不同任务中表现出不一致的结果。微调实验进一步证实了空中协作感知和推理中sim-to-real迁移的可行性。

Conclusion: AirCopBench的提出填补了多智能体协作感知评估基准的空白，实验结果揭示了现有模型在协作感知方面的不足，并为未来的研究提供了方向。

Abstract: Multimodal Large Language Models (MLLMs) have shown promise in single-agent vision tasks, yet benchmarks for evaluating multi-agent collaborative perception remain scarce. This gap is critical, as multi-drone systems provide enhanced coverage, robustness, and collaboration compared to single-sensor setups. Existing multi-image benchmarks mainly target basic perception tasks using high-quality single-agent images, thus failing to evaluate MLLMs in more complex, egocentric collaborative scenarios, especially under real-world degraded perception conditions.To address these challenges, we introduce AirCopBench, the first comprehensive benchmark designed to evaluate MLLMs in embodied aerial collaborative perception under challenging perceptual conditions. AirCopBench includes 14.6k+ questions derived from both simulator and real-world data, spanning four key task dimensions: Scene Understanding, Object Understanding, Perception Assessment, and Collaborative Decision, across 14 task types. We construct the benchmark using data from challenging degraded-perception scenarios with annotated collaborative events, generating large-scale questions through model-, rule-, and human-based methods under rigorous quality control. Evaluations on 40 MLLMs show significant performance gaps in collaborative perception tasks, with the best model trailing humans by 24.38% on average and exhibiting inconsistent results across tasks. Fine-tuning experiments further confirm the feasibility of sim-to-real transfer in aerial collaborative perception and reasoning.

</details>


### [86] [EmbryoDiff: A Conditional Diffusion Framework with Multi-Focal Feature Fusion for Fine-Grained Embryo Developmental Stage Recognition](https://arxiv.org/abs/2511.11027)
*Yong Sun,Zhengjie Zhang,Junyu Shi,Zhiyuan Zhang,Lijiang Liu,Qiang Nie*

Main category: cs.CV

TL;DR: EmbryoDiff: a two-stage diffusion-based framework for identifying fine-grained embryo developmental stages during In Vitro Fertilization (IVF).


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods fail to utilize the distributional prior of embryonic development and rely on single-focal information, leading to incomplete embryonic representations and feature ambiguity under cell occlusions.

Method: A two-stage diffusion-based framework with a Multi-Focal Feature Fusion Strategy and a Hybrid Semantic-Boundary Condition Block.

Result: Achieves state-of-the-art results on two benchmark datasets, reaching 82.8% and 81.3% accuracy, respectively.

Conclusion: EmbryoDiff effectively addresses the limitations of existing methods and enables accurate embryonic stage classification.

Abstract: Identification of fine-grained embryo developmental stages during In Vitro Fertilization (IVF) is crucial for assessing embryo viability. Although recent deep learning methods have achieved promising accuracy, existing discriminative models fail to utilize the distributional prior of embryonic development to improve accuracy. Moreover, their reliance on single-focal information leads to incomplete embryonic representations, making them susceptible to feature ambiguity under cell occlusions. To address these limitations, we propose EmbryoDiff, a two-stage diffusion-based framework that formulates the task as a conditional sequence denoising process. Specifically, we first train and freeze a frame-level encoder to extract robust multi-focal features. In the second stage, we introduce a Multi-Focal Feature Fusion Strategy that aggregates information across focal planes to construct a 3D-aware morphological representation, effectively alleviating ambiguities arising from cell occlusions. Building on this fused representation, we derive complementary semantic and boundary cues and design a Hybrid Semantic-Boundary Condition Block to inject them into the diffusion-based denoising process, enabling accurate embryonic stage classification. Extensive experiments on two benchmark datasets show that our method achieves state-of-the-art results. Notably, with only a single denoising step, our model obtains the best average test performance, reaching 82.8% and 81.3% accuracy on the two datasets, respectively.

</details>


### [87] [Algorithms Trained on Normal Chest X-rays Can Predict Health Insurance Types](https://arxiv.org/abs/2511.11030)
*Chi-Yu Chen,Rawan Abulibdeh,Arash Asgari,Leo Anthony Celi,Deirdre Goode,Hassan Hamidi,Laleh Seyyed-Kalantari,Po-Chih Kuo,Ned McCague,Thomas Sounack*

Main category: cs.CV

TL;DR: 深度学习模型可以从胸部X光片中预测患者的健康保险类型，这反映了社会经济地位。


<details>
  <summary>Details</summary>
Motivation: 医学影像可能包含社会不平等的信息，挑战了医学影像作为中立生物数据的假设。

Method: 使用DenseNet121、SwinV2-B、MedMamba等先进模型，在胸部X光片上预测患者的健康保险类型。

Result: 模型在MIMIC-CXR-JPG和CheXpert数据集上取得了显著的准确率（AUC约为0.67和0.68），且信号在控制年龄、种族和性别后仍然存在。信号是弥散性的，位于胸部上中区域。

Conclusion: 医学AI的公平性目标不再仅仅是平衡数据集或调整阈值，而是要 जांच और解开嵌入在临床数据中的社会指纹。

Abstract: Artificial intelligence is revealing what medicine never intended to encode. Deep vision models, trained on chest X-rays, can now detect not only disease but also invisible traces of social inequality. In this study, we show that state-of-the-art architectures (DenseNet121, SwinV2-B, MedMamba) can predict a patient's health insurance type, a strong proxy for socioeconomic status, from normal chest X-rays with significant accuracy (AUC around 0.67 on MIMIC-CXR-JPG, 0.68 on CheXpert). The signal persists even when age, race, and sex are controlled for, and remains detectable when the model is trained exclusively on a single racial group. Patch-based occlusion reveals that the signal is diffuse rather than localized, embedded in the upper and mid-thoracic regions. This suggests that deep networks may be internalizing subtle traces of clinical environments, equipment differences, or care pathways; learning socioeconomic segregation itself. These findings challenge the assumption that medical images are neutral biological data. By uncovering how models perceive and exploit these hidden social signatures, this work reframes fairness in medical AI: the goal is no longer only to balance datasets or adjust thresholds, but to interrogate and disentangle the social fingerprints embedded in clinical data itself.

</details>


### [88] [Accelerating Controllable Generation via Hybrid-grained Cache](https://arxiv.org/abs/2511.11031)
*Lin Liu,Huixia Ben,Shuo Wang,Jinda Lu,Junxiang Qiu,Shengeng Tang,Yanbin Hao*

Main category: cs.CV

TL;DR: 提出了一种混合粒度缓存（HGC）方法，以提高可控生成模型的生成效率，同时保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 可控生成模型需要处理控制条件和内容生成计算需求，导致生成效率通常较低。

Method: HGC方法采用不同粒度的缓存策略，包括基于特征重用的粗粒度缓存（块级别）和利用连续推理步骤中的交叉注意力图的细粒度缓存（提示级别）。

Result: 在四个基准数据集上验证了HGC的有效性，尤其是在平衡生成效率和视觉质量方面的优势。例如，在COCO-Stuff分割基准上，HGC显著降低了63%的计算成本（MACs）。

Conclusion: HGC方法可以在显著降低计算成本的同时，保持语义保真度，从而提高可控生成模型的生成效率。

Abstract: Controllable generative models have been widely used to improve the realism of synthetic visual content. However, such models must handle control conditions and content generation computational requirements, resulting in generally low generation efficiency. To address this issue, we propose a Hybrid-Grained Cache (HGC) approach that reduces computational overhead by adopting cache strategies with different granularities at different computational stages. Specifically, (1) we use a coarse-grained cache (block-level) based on feature reuse to dynamically bypass redundant computations in encoder-decoder blocks between each step of model reasoning. (2) We design a fine-grained cache (prompt-level) that acts within a module, where the fine-grained cache reuses cross-attention maps within consecutive reasoning steps and extends them to the corresponding module computations of adjacent steps. These caches of different granularities can be seamlessly integrated into each computational link of the controllable generation process. We verify the effectiveness of HGC on four benchmark datasets, especially its advantages in balancing generation efficiency and visual quality. For example, on the COCO-Stuff segmentation benchmark, our HGC significantly reduces the computational cost (MACs) by 63% (from 18.22T to 6.70T), while keeping the loss of semantic fidelity (quantized performance degradation) within 1.5%.

</details>


### [89] [MPCGNet: A Multiscale Feature Extraction and Progressive Feature Aggregation Network Using Coupling Gates for Polyp Segmentation](https://arxiv.org/abs/2511.11032)
*Wei Wang,Feng Jiang,Xin Wang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的息肉分割网络MPCGNet，用于辅助医生进行结直肠息肉筛查和癌症诊断。


<details>
  <summary>Details</summary>
Motivation: 现有息肉分割方法在小尺寸息肉识别、息肉边界模糊以及结肠镜图像噪声方面存在挑战。

Method: 该论文引入耦合门作为特定模块的组件来过滤噪声和执行特征重要性选择，并提出了三个模块：CGMFE、WCAD和DFA。

Result: MPCGNet在ETIS-LaribPolypDB和CVC-ColonDB数据集上的mDice分数分别比第二好的网络高2.20%和0.68%。

Conclusion: 实验结果表明MPCGNet优于现有网络。

Abstract: Automatic segmentation methods of polyps is crucial for assisting doctors in colorectal polyp screening and cancer diagnosis. Despite the progress made by existing methods, polyp segmentation faces several challenges: (1) small-sized polyps are prone to being missed during identification, (2) the boundaries between polyps and the surrounding environment are often ambiguous, (3) noise in colonoscopy images, caused by uneven lighting and other factors, affects segmentation results. To address these challenges, this paper introduces coupling gates as components in specific modules to filter noise and perform feature importance selection. Three modules are proposed: the coupling gates multiscale feature extraction (CGMFE) module, which effectively extracts local features and suppresses noise; the windows cross attention (WCAD) decoder module, which restores details after capturing the precise location of polyps; and the decoder feature aggregation (DFA) module, which progressively aggregates features, further extracts them, and performs feature importance selection to reduce the loss of small-sized polyps. Experimental results demonstrate that MPCGNet outperforms recent networks, with mDice scores 2.20% and 0.68% higher than the second-best network on the ETIS-LaribPolypDB and CVC-ColonDB datasets, respectively.

</details>


### [90] [CrossMed: A Multimodal Cross-Task Benchmark for Compositional Generalization in Medical Imaging](https://arxiv.org/abs/2511.11034)
*Pooja Singh,Siddhant Ujjain,Tapan Kumar Gandhi,Sandeep Kumar*

Main category: cs.CV

TL;DR: CrossMed是一个用于评估医学多模态LLM中组合泛化的基准，通过Modality-Anatomy-Task (MAT) 模式来评估模型在未见过的模态、解剖结构和任务类型组合上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 评估现有的多模态大语言模型在医学领域中，对于成像模态、解剖结构和任务类型的新组合的泛化能力。

Method: 将四个公共数据集（CheXpert, SIIM-ACR, BraTS 2020, MosMedData）重构为统一的视觉问答 (VQA) 格式，构建包含20,200个多项选择题的CrossMed基准，并使用MAT模式评估模型的组合泛化能力。

Result: 在相关MAT分割上训练的模型达到了83.2%的分类准确率和0.75的分割cIoU，但在不相关和零重叠条件下，性能显著下降。即使仅使用分类数据进行训练，分割性能也提高了7% cIoU。传统模型表现出适度的提升，而多模态LLM在组合泛化方面表现出色。

Conclusion: CrossMed提供了一个严格的测试平台，用于评估医学视觉-语言模型中的零样本、跨任务和模态无关的泛化能力。

Abstract: Recent advances in multimodal large language models have enabled unified processing of visual and textual inputs, offering promising applications in general-purpose medical AI. However, their ability to generalize compositionally across unseen combinations of imaging modality, anatomy, and task type remains underexplored. We introduce CrossMed, a benchmark designed to evaluate compositional generalization (CG) in medical multimodal LLMs using a structured Modality-Anatomy-Task (MAT) schema. CrossMed reformulates four public datasets, CheXpert (X-ray classification), SIIM-ACR (X-ray segmentation), BraTS 2020 (MRI classification and segmentation), and MosMedData (CT classification) into a unified visual question answering (VQA) format, resulting in 20,200 multiple-choice QA instances. We evaluate two open-source multimodal LLMs, LLaVA-Vicuna-7B and Qwen2-VL-7B, on both Related and Unrelated MAT splits, as well as a zero-overlap setting where test triplets share no Modality, Anatomy, or Task with the training data. Models trained on Related splits achieve 83.2 percent classification accuracy and 0.75 segmentation cIoU, while performance drops significantly under Unrelated and zero-overlap conditions, demonstrating the benchmark difficulty. We also show cross-task transfer, where segmentation performance improves by 7 percent cIoU even when trained using classification-only data. Traditional models (ResNet-50 and U-Net) show modest gains, confirming the broad utility of the MAT framework, while multimodal LLMs uniquely excel at compositional generalization. CrossMed provides a rigorous testbed for evaluating zero-shot, cross-task, and modality-agnostic generalization in medical vision-language models.

</details>


### [91] [SemanticNN: Compressive and Error-Resilient Semantic Offloading for Extremely Weak Devices](https://arxiv.org/abs/2511.11038)
*Jiaming Huang,Yi Gao,Fuchang Pan,Renjie Li,Wei Dong*

Main category: cs.CV

TL;DR: 提出了一种名为SemanticNN的语义编解码器，以实现资源受限设备上的容错设备边缘协作推理卸载。


<details>
  <summary>Details</summary>
Motivation: 在物联网(IoT)快速增长的背景下，在极弱嵌入式设备上集成人工智能(AI)受到了广泛关注。这种集成可以提高实时性能并增强数据隐私。然而，此类设备的资源限制和不可靠的网络条件使得容错设备边缘协作系统成为必需品。

Method: 该方法包括：1)一个Bit Error Rate (BER) 感知解码器，它可以适应动态信道条件；2)一个基于软量化(SQ)的编码器，用于学习紧凑的表示；3) Feature-augmentation Learning，一种新颖的训练策略，可提高卸载效率；4) 基于XAI的非对称补偿，以增强解码语义保真度，解决来自非对称资源的编码器-解码器能力不匹配问题。

Result: 在STM32上使用三个模型和六个数据集（跨图像分类和对象检测任务）进行了大量实验。实验结果表明，在不同的传输错误率下，SemanticNN在保持卓越的推理精度的同时，显著减少了56.82-344.83倍的特征传输量。

Conclusion: SemanticNN能够在严格的计算和通信约束下实现压缩和弹性的协作推理卸载，通过容忍比特级的错误来追求语义级的正确性。

Abstract: With the rapid growth of the Internet of Things (IoT), integrating artificial intelligence (AI) on extremely weak embedded devices has garnered significant attention, enabling improved real-time performance and enhanced data privacy. However, the resource limitations of such devices and unreliable network conditions necessitate error-resilient device-edge collaboration systems. Traditional approaches focus on bit-level transmission correctness, which can be inefficient under dynamic channel conditions. In contrast, we propose SemanticNN, a semantic codec that tolerates bit-level errors in pursuit of semantic-level correctness, enabling compressive and resilient collaborative inference offloading under strict computational and communication constraints. It incorporates a Bit Error Rate (BER)-aware decoder that adapts to dynamic channel conditions and a Soft Quantization (SQ)-based encoder to learn compact representations. Building on this architecture, we introduce Feature-augmentation Learning, a novel training strategy that enhances offloading efficiency. To address encoder-decoder capability mismatches from asymmetric resources, we propose XAI-based Asymmetry Compensation to enhance decoding semantic fidelity. We conduct extensive experiments on STM32 using three models and six datasets across image classification and object detection tasks. Experimental results demonstrate that, under varying transmission error rates, SemanticNN significantly reduces feature transmission volume by 56.82-344.83x while maintaining superior inference accuracy.

</details>


### [92] [Hyperbolic Hierarchical Alignment Reasoning Network for Text-3D Retrieval](https://arxiv.org/abs/2511.11045)
*Wenrui Li,Yidan Lu,Yeyu Chai,Rui Zhao,Hengyu Man,Xiaopeng Fan*

Main category: cs.CV

TL;DR: 提出了一种用于文本-3D检索的Hyperbolic Hierarchical Alignment Reasoning Network (H$^{2}$ARN)，以解决现有方法中的层级表示崩溃(HRC)和冗余引起的显著性稀释(RISD)问题。


<details>
  <summary>Details</summary>
Motivation: 现有的文本-3D检索方法面临层级表示崩溃和冗余引起的显著性稀释的挑战，影响了模型区分难负样本的能力。

Method: 1. 将文本和3D数据嵌入Lorentz模型的双曲空间，利用双曲空间的指数体积增长来保持层级距离。
2. 使用层级排序损失在每个文本向量周围构建一个收缩的蕴含锥，确保匹配的3D实例落入锥内，同时使用实例级别的对比损失来分离非匹配样本。
3. 提出了一种贡献感知的双曲聚合模块，利用Lorentzian距离评估每个局部特征的相关性，并应用贡献加权聚合来增强判别区域，抑制冗余。

Result: 在扩展的T3DR-HIT v2基准测试中取得了良好效果，该基准包含8,935个文本-3D对，是原始大小的2.6倍，涵盖了细粒度的文化人工制品和复杂的室内场景。

Conclusion: H$^{2}$ARN有效地解决了HRC和RISD问题，并在文本-3D检索任务上取得了显著的性能提升。

Abstract: With the daily influx of 3D data on the internet, text-3D retrieval has gained increasing attention. However, current methods face two major challenges: Hierarchy Representation Collapse (HRC) and Redundancy-Induced Saliency Dilution (RISD). HRC compresses abstract-to-specific and whole-to-part hierarchies in Euclidean embeddings, while RISD averages noisy fragments, obscuring critical semantic cues and diminishing the model's ability to distinguish hard negatives. To address these challenges, we introduce the Hyperbolic Hierarchical Alignment Reasoning Network (H$^{2}$ARN) for text-3D retrieval. H$^{2}$ARN embeds both text and 3D data in a Lorentz-model hyperbolic space, where exponential volume growth inherently preserves hierarchical distances. A hierarchical ordering loss constructs a shrinking entailment cone around each text vector, ensuring that the matched 3D instance falls within the cone, while an instance-level contrastive loss jointly enforces separation from non-matching samples. To tackle RISD, we propose a contribution-aware hyperbolic aggregation module that leverages Lorentzian distance to assess the relevance of each local feature and applies contribution-weighted aggregation guided by hyperbolic geometry, enhancing discriminative regions while suppressing redundancy without additional supervision. We also release the expanded T3DR-HIT v2 benchmark, which contains 8,935 text-to-3D pairs, 2.6 times the original size, covering both fine-grained cultural artefacts and complex indoor scenes. Our codes are available at https://github.com/liwrui/H2ARN.

</details>


### [93] [PINGS-X: Physics-Informed Normalized Gaussian Splatting with Axes Alignment for Efficient Super-Resolution of 4D Flow MRI](https://arxiv.org/abs/2511.11048)
*Sun Jo,Seok Young Hong,JinHyun Kim,Seungmin Kang,Ahjin Choi,Don-Gwan An,Simon Song,Je Hyeong Hong*

Main category: cs.CV

TL;DR: PINGS-X: A novel framework for super-resolution of 4D flow MRI data using axes-aligned spatiotemporal Gaussian representations, offering faster training and improved accuracy compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: High spatiotemporal resolution in 4D flow MRI is crucial for early detection of cardiovascular conditions, but achieving it leads to long scan times. Existing PINN methods are slow due to per-patient training.

Method: PINGS-X models high-resolution flow velocities using axes-aligned spatiotemporal Gaussian representations, incorporating normalized Gaussian splatting with convergence guarantee, axes-aligned Gaussians for simplified training, and a Gaussian merging procedure.

Result: PINGS-X significantly reduces training time and achieves better super-resolution accuracy on CFD and real 4D flow MRI datasets.

Conclusion: PINGS-X overcomes the limitations of previous methods by providing a faster and more accurate approach to super-resolution in 4D flow MRI.

Abstract: 4D flow magnetic resonance imaging (MRI) is a reliable, non-invasive approach for estimating blood flow velocities, vital for cardiovascular diagnostics. Unlike conventional MRI focused on anatomical structures, 4D flow MRI requires high spatiotemporal resolution for early detection of critical conditions such as stenosis or aneurysms. However, achieving such resolution typically results in prolonged scan times, creating a trade-off between acquisition speed and prediction accuracy. Recent studies have leveraged physics-informed neural networks (PINNs) for super-resolution of MRI data, but their practical applicability is limited as the prohibitively slow training process must be performed for each patient. To overcome this limitation, we propose PINGS-X, a novel framework modeling high-resolution flow velocities using axes-aligned spatiotemporal Gaussian representations. Inspired by the effectiveness of 3D Gaussian splatting (3DGS) in novel view synthesis, PINGS-X extends this concept through several non-trivial novel innovations: (i) normalized Gaussian splatting with a formal convergence guarantee, (ii) axes-aligned Gaussians that simplify training for high-dimensional data while preserving accuracy and the convergence guarantee, and (iii) a Gaussian merging procedure to prevent degenerate solutions and boost computational efficiency. Experimental results on computational fluid dynamics (CFD) and real 4D flow MRI datasets demonstrate that PINGS-X substantially reduces training time while achieving superior super-resolution accuracy. Our code and datasets are available at https://github.com/SpatialAILab/PINGS-X.

</details>


### [94] [NP-LoRA: Null Space Projection Unifies Subject and Style in LoRA Fusion](https://arxiv.org/abs/2511.11051)
*Chuheng Chen,Xiaofei Zhou,Geyuan Zhang,Yong Huang*

Main category: cs.CV

TL;DR: 提出了Null Space Projection LoRA (NP-LoRA)，一种用于LoRA融合的基于投影的框架，该框架强制子空间分离以防止主要方向之间的结构干扰。


<details>
  <summary>Details</summary>
Motivation: 现有的方法依赖于基于权重的合并，其中一个LoRA经常支配另一个，导致干扰和降低的保真度。这种干扰是结构性的：单独训练的LoRA占据低秩高维子空间，导致非正交和重叠的表示。

Method: 首先通过奇异值分解（SVD）提取主要风格方向，然后将主题LoRA投影到其正交零空间中。此外，引入了一种软投影机制，可以平滑地控制主题保真度和风格一致性之间的权衡。

Result: NP-LoRA始终优于强大的基线（例如，基于DINO和CLIP的指标，具有人类和LLM偏好得分），并且广泛适用于主干和LoRA对，而无需重新训练。

Conclusion: 分析了LoRA的内部结构，发现它们的生成行为主要由低秩子空间中的几个主要方向决定，这些方向在融合过程中应避免受到干扰。为了实现这一点，我们提出了一种投影框架，用于LoRA融合，该框架强制子空间分离以防止主要方向之间的结构干扰。

Abstract: Low-Rank Adaptation (LoRA) fusion has emerged as a key technique for reusing and composing learned subject and style representations for controllable generation without costly retraining. However, existing methods rely on weight-based merging, where one LoRA often dominates the other, leading to interference and degraded fidelity. This interference is structural: separately trained LoRAs occupy low-rank high-dimensional subspaces, leading to non-orthogonal and overlapping representations. In this work, we analyze the internal structure of LoRAs and find their generative behavior is dominated by a few principal directions in the low-rank subspace, which should remain free from interference during fusion. To achieve this, we propose Null Space Projection LoRA (NP-LoRA), a projection-based framework for LoRA fusion that enforces subspace separation to prevent structural interference among principal directions. Specifically, we first extract principal style directions via singular value decomposition (SVD) and then project the subject LoRA into its orthogonal null space. Furthermore, we introduce a soft projection mechanism that enables smooth control over the trade-off between subject fidelity and style consistency. Experiments show NP-LoRA consistently improves fusion quality over strong baselines (e.g., DINO and CLIP-based metrics, with human and LLM preference scores), and applies broadly across backbones and LoRA pairs without retraining.

</details>


### [95] [CareCom: Generative Image Composition with Calibrated Reference Features](https://arxiv.org/abs/2511.11060)
*Jiaxuan Chen,Bo Zhang,Qingdong He,Jinlong Peng,Li Niu*

Main category: cs.CV

TL;DR: 现有的图像合成方法难以兼顾细节保留和前景姿态/视角调整。本文通过扩展生成式合成模型到多参考版本，并校准前景参考图像的全局和局部特征，使其与背景信息兼容，从而改善合成效果。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式图像合成方法在细节保留和前景姿态/视角调整方面存在困难。

Method: 1. 将现有的生成式合成模型扩展到多参考版本。
2. 提出校准前景参考图像的全局和局部特征，使其与背景信息兼容。

Result: 在 MVImgNet 和 MureCom 数据集上的大量实验表明，校准后的参考特征可以极大地改善生成模型的性能。

Conclusion: 通过校准的参考特征，生成模型可以获得性能提升。

Abstract: Image composition aims to seamlessly insert foreground object into background. Despite the huge progress in generative image composition, the existing methods are still struggling with simultaneous detail preservation and foreground pose/view adjustment. To address this issue, we extend the existing generative composition model to multi-reference version, which allows using arbitrary number of foreground reference images. Furthermore, we propose to calibrate the global and local features of foreground reference images to make them compatible with the background information. The calibrated reference features can supplement the original reference features with useful global and local information of proper pose/view. Extensive experiments on MVImgNet and MureCom demonstrate that the generative model can greatly benefit from the calibrated reference features.

</details>


### [96] [LiteAttention: A Temporal Sparse Attention for Diffusion Transformers](https://arxiv.org/abs/2511.11062)
*Dor Shmilovich,Tony Wu,Aviad Dahan,Yuval Domb*

Main category: cs.CV

TL;DR: 提出了 LiteAttention，通过利用扩散注意力中的时间连贯性来加速视频生成中的扩散 Transformer。


<details>
  <summary>Details</summary>
Motivation: 扩散 Transformer 在视频生成中质量很高，但计算复杂度高，导致延迟过高。现有的加速方法需要在动态稀疏注意力和静态稀疏性之间进行权衡。

Method: LiteAttention 是一种利用时间连贯性来实现跨去噪序列的进化计算跳过的方法。它通过标记非必要 tiles 并向前传播跳过决策，消除了冗余的注意力计算。

Result: 在生产视频扩散模型上实现了显著的加速，且质量没有下降。

Conclusion: LiteAttention 结合了动态方法的适应性和静态方法的效率，且代码将公开。

Abstract: Diffusion Transformers, particularly for video generation, achieve remarkable quality but suffer from quadratic attention complexity, leading to prohibitive latency. Existing acceleration methods face a fundamental trade-off: dynamically estimating sparse attention patterns at each denoising step incurs high computational overhead and estimation errors, while static sparsity patterns remain fixed and often suboptimal throughout denoising. We identify a key structural property of diffusion attention, namely, its sparsity patterns exhibit strong temporal coherence across denoising steps. Tiles deemed non-essential at step $t$ typically remain so at step $t+δ$. Leveraging this observation, we introduce LiteAttention, a method that exploits temporal coherence to enable evolutionary computation skips across the denoising sequence. By marking non-essential tiles early and propagating skip decisions forward, LiteAttention eliminates redundant attention computations without repeated profiling overheads, combining the adaptivity of dynamic methods with the efficiency of static ones. We implement a highly optimized LiteAttention kernel on top of FlashAttention and demonstrate substantial speedups on production video diffusion models, with no degradation in quality. The code and implementation details will be publicly released.

</details>


### [97] [From Retinal Pixels to Patients: Evolution of Deep Learning Research in Diabetic Retinopathy Screening](https://arxiv.org/abs/2511.11065)
*Muskaan Chopra,Lorenz Sparrenberg,Armin Berger,Sarthak Khanna,Jan H. Terheyden,Rafet Sifa*

Main category: cs.CV

TL;DR: 这篇综述总结了2016-2025年间糖尿病视网膜病变（DR）的研究，重点关注深度学习方法在DR筛查中的应用和进展。


<details>
  <summary>Details</summary>
Motivation: 早期发现对于减少全球因糖尿病视网膜病变导致的视力丧失至关重要。深度学习已改变了DR筛查，但仍面临类别不平衡、标签稀缺等挑战。

Method: 该综述系统性地整合了50多项研究和20多个数据集的结果， критически 评估了自监督、半监督学习等方法。

Result: 论文提供了跨数据集的性能基准，并讨论了多中心验证和临床信任方面的差距。

Conclusion: 该研究概述了一个可重现、保护隐私且可临床部署的DR人工智能的实践议程，并认为这些创新可广泛应用于大规模医学影像。

Abstract: Diabetic Retinopathy (DR) remains a leading cause of preventable blindness, with early detection critical for reducing vision loss worldwide. Over the past decade, deep learning has transformed DR screening, progressing from early convolutional neural networks trained on private datasets to advanced pipelines addressing class imbalance, label scarcity, domain shift, and interpretability. This survey provides the first systematic synthesis of DR research spanning 2016-2025, consolidating results from 50+ studies and over 20 datasets. We critically examine methodological advances, including self- and semi-supervised learning, domain generalization, federated training, and hybrid neuro-symbolic models, alongside evaluation protocols, reporting standards, and reproducibility challenges. Benchmark tables contextualize performance across datasets, while discussion highlights open gaps in multi-center validation and clinical trust. By linking technical progress with translational barriers, this work outlines a practical agenda for reproducible, privacy-preserving, and clinically deployable DR AI. Beyond DR, many of the surveyed innovations extend broadly to medical imaging at scale.

</details>


### [98] [S2D-ALIGN: Shallow-to-Deep Auxiliary Learning for Anatomically-Grounded Radiology Report Generation](https://arxiv.org/abs/2511.11066)
*Jiechao Gao,Chang Liu,Yuangang Li*

Main category: cs.CV

TL;DR: 提出了一种新的SFT范式，即S2D-Align，通过利用不同粒度的辅助信号建立解剖学上的对齐，从而提高放射学报告的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要通过监督微调(SFT)优化X光片和报告之间的跨模态对齐，但标准的SFT范式未能建立解剖学上的对齐，报告的模板性质导致次优的生成质量。

Method: S2D-Align采用由浅入深的策略，逐步丰富对齐过程：首先进行粗略的X光片-报告配对，然后引入参考报告进行实例级指导，最后利用关键短语将生成定位到特定的解剖学细节。引入了一个基于记忆的适配器来实现特征共享，从而整合粗粒度和细粒度的指导。

Result: 在公共的MIMIC-CXR和IU X-Ray基准上，S2D-Align实现了最先进的性能。消融研究验证了多阶段、辅助指导方法的有效性。

Conclusion: S2D-Align为增强复杂的多模态生成任务中的定位能力提供了一个有希望的方向。

Abstract: Radiology Report Generation (RRG) aims to automatically generate diagnostic reports from radiology images. To achieve this, existing methods have leveraged the powerful cross-modal generation capabilities of Multimodal Large Language Models (MLLMs), primarily focusing on optimizing cross-modal alignment between radiographs and reports through Supervised Fine-Tuning (SFT). However, by only performing instance-level alignment with the image-text pairs, the standard SFT paradigm fails to establish anatomically-grounded alignment, where the templated nature of reports often leads to sub-optimal generation quality. To address this, we propose \textsc{S2D-Align}, a novel SFT paradigm that establishes anatomically-grounded alignment by leveraging auxiliary signals of varying granularities. \textsc{S2D-Align} implements a shallow-to-deep strategy, progressively enriching the alignment process: it begins with the coarse radiograph-report pairing, then introduces reference reports for instance-level guidance, and ultimately utilizes key phrases to ground the generation in specific anatomical details. To bridge the different alignment stages, we introduce a memory-based adapter that empowers feature sharing, thereby integrating coarse and fine-grained guidance. For evaluation, we conduct experiments on the public \textsc{MIMIC-CXR} and \textsc{IU X-Ray} benchmarks, where \textsc{S2D-Align} achieves state-of-the-art performance compared to existing methods. Ablation studies validate the effectiveness of our multi-stage, auxiliary-guided approach, highlighting a promising direction for enhancing grounding capabilities in complex, multi-modal generation tasks.

</details>


### [99] [Evaluating Latent Generative Paradigms for High-Fidelity 3D Shape Completion from a Single Depth Image](https://arxiv.org/abs/2511.11074)
*Matthias Humt,Ulrich Hillenbrand,Rudolph Triebel*

Main category: cs.CV

TL;DR: 比较了两种生成模型（去噪扩散概率模型和自回归因果变换器）在生成形状建模和补全任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 缺乏对哪种模型最适合哪种任务的共识，以及对部分 3D 数据等条件信息的评估不足。

Method: 调整去噪扩散概率模型和自回归因果变换器以适应生成形状建模和补全任务，并进行定量评估和比较，包括基线判别模型和广泛的消融研究。

Result: 扩散模型在多模态形状补全方面表现优于判别模型和自回归方法，而当在相同的离散潜在空间上比较时，自回归模型可以达到或超过扩散模型的性能。

Conclusion: 扩散模型和自回归模型在 3D 数据生成任务中各有优势，具体取决于任务和潜在空间的类型。

Abstract: While generative models have seen significant adoption across a wide range of data modalities, including 3D data, a consensus on which model is best suited for which task has yet to be reached. Further, conditional information such as text and images to steer the generation process are frequently employed, whereas others, like partial 3D data, have not been thoroughly evaluated. In this work, we compare two of the most promising generative models--Denoising Diffusion Probabilistic Models and Autoregressive Causal Transformers--which we adapt for the tasks of generative shape modeling and completion. We conduct a thorough quantitative evaluation and comparison of both tasks, including a baseline discriminative model and an extensive ablation study. Our results show that (1) the diffusion model with continuous latents outperforms both the discriminative model and the autoregressive approach and delivers state-of-the-art performance on multi-modal shape completion from a single, noisy depth image under realistic conditions and (2) when compared on the same discrete latent space, the autoregressive model can match or exceed diffusion performance on these tasks.

</details>


### [100] [Phys-Liquid: A Physics-Informed Dataset for Estimating 3D Geometry and Volume of Transparent Deformable Liquids](https://arxiv.org/abs/2511.11077)
*Ke Ma,Yizhou Fang,Jean-Baptiste Weibel,Shuai Tan,Xinggang Wang,Yang Xiao,Yi Fang,Tian Xia*

Main category: cs.CV

TL;DR: 提出了一个名为 Phys-Liquid 的物理信息数据集，用于透明可变形液体几何和体积属性的评估。


<details>
  <summary>Details</summary>
Motivation: 由于光学复杂性和容器运动引起的动态表面变形，精确估计透明可变形液体的几何和体积属性非常具有挑战性。现有的数据集缺乏全面的、包含物理信息的模拟数据，无法代表各种动态场景下真实的液体行为。

Method: 该方法包含一个四阶段重建和估计流程，包括液体分割、多视图掩模生成、3D 网格重建和真实世界比例缩放。

Result: 实验结果表明，在重建液体几何形状和体积方面，该方法提高了准确性和一致性，优于现有的基准。

Conclusion: 该数据集和相关的验证方法促进了未来在透明液体感知任务中的进步。

Abstract: Estimating the geometric and volumetric properties of transparent deformable liquids is challenging due to optical complexities and dynamic surface deformations induced by container movements. Autonomous robots performing precise liquid manipulation tasks, such as dispensing, aspiration, and mixing, must handle containers in ways that inevitably induce these deformations, complicating accurate liquid state assessment. Current datasets lack comprehensive physics-informed simulation data representing realistic liquid behaviors under diverse dynamic scenarios. To bridge this gap, we introduce Phys-Liquid, a physics-informed dataset comprising 97,200 simulation images and corresponding 3D meshes, capturing liquid dynamics across multiple laboratory scenes, lighting conditions, liquid colors, and container rotations. To validate the realism and effectiveness of Phys-Liquid, we propose a four-stage reconstruction and estimation pipeline involving liquid segmentation, multi-view mask generation, 3D mesh reconstruction, and real-world scaling. Experimental results demonstrate improved accuracy and consistency in reconstructing liquid geometry and volume, outperforming existing benchmarks. The dataset and associated validation methods facilitate future advancements in transparent liquid perception tasks. The dataset and code are available at https://dualtransparency.github.io/Phys-Liquid/.

</details>


### [101] [SplineSplat: 3D Ray Tracing for Higher-Quality Tomography](https://arxiv.org/abs/2511.11078)
*Youssef Haouchat,Sepand Kashani,Aleix Boquet-Pujadas,Philippe Thévenaz,Michael Unser*

Main category: cs.CV

TL;DR: 提出了一种高效计算三维体积断层投影的方法，该体积由移位的B样条的线性组合表示。


<details>
  <summary>Details</summary>
Motivation: 为了达到比传统基于体素的方法更高的重建质量。

Method: 使用光线追踪算法计算具有任意投影几何形状的3D线积分，该算法的一个组成部分是神经网络，可以有效地计算基函数的贡献。

Result: 在实验中，我们考虑了适定情况，其中数据足以进行准确的重建，而无需正则化。

Conclusion: 我们实现了比传统基于体素的方法更高的重建质量。

Abstract: We propose a method to efficiently compute tomographic projections of a 3D volume represented by a linear combination of shifted B-splines. To do so, we propose a ray-tracing algorithm that computes 3D line integrals with arbitrary projection geometries. One of the components of our algorithm is a neural network that computes the contribution of the basis functions efficiently. In our experiments, we consider well-posed cases where the data are sufficient for accurate reconstruction without the need for regularization. We achieve higher reconstruction quality than traditional voxel-based methods.

</details>


### [102] [A Space-Time Transformer for Precipitation Forecasting](https://arxiv.org/abs/2511.11090)
*Levi Harris,Tianlong Chen*

Main category: cs.CV

TL;DR: 提出了一种名为SaTformer的视频Transformer，用于从卫星辐射数据中预测极端降水，并在NeurIPS Weather4Cast 2025挑战赛中获得第一名。


<details>
  <summary>Details</summary>
Motivation: 传统数值天气预报模型在计算上 demanding，并且在临近预报的时间尺度上性能下降。因此，作者希望探索使用视频理解架构进行天气预报。

Method: 提出了SaTformer，一种基于全时空注意力的视频Transformer。同时，将降水回归问题转化为分类问题，并采用类别加权损失来解决标签不平衡问题。

Result: 该模型在NeurIPS Weather4Cast 2025累积降雨挑战赛中获得第一名。

Conclusion: SaTformer模型能够有效地预测极端降水。

Abstract: Meteorological agencies around the world rely on real-time flood guidance to issue live-saving advisories and warnings. For decades traditional numerical weather prediction (NWP) models have been state-of-the-art for precipitation forecasting. However, physically-parameterized models suffer from a few core limitations: first, solving PDEs to resolve atmospheric dynamics is computationally demanding, and second, these methods degrade in performance at nowcasting timescales (i.e., 0-4 hour lead-times). Motivated by these shortcomings, recent work proposes AI-weather prediction (AI-WP) alternatives that learn to emulate analysis data with neural networks. While these data-driven approaches have enjoyed enormous success across diverse spatial and temporal resolutions, applications of video-understanding architectures for weather forecasting remain underexplored. To address these gaps, we propose SaTformer: a video transformer built on full space-time attention that skillfully forecasts extreme precipitation from satellite radiances. Along with our novel architecture, we introduce techniques to tame long-tailed precipitation datasets. Namely, we reformulate precipitation regression into a classification problem, and employ a class-weighted loss to address label imbalances. Our model scored first place on the NeurIPS Weather4Cast 2025 Cumulative Rainfall challenge. Code and model weights are available: https://github.com/leharris3/satformer

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [103] [HyperComplEx: Adaptive Multi-Space Knowledge Graph Embeddings](https://arxiv.org/abs/2511.10842)
*Jugal Gajjar,Kaustik Ranaware,Kamalasankari Subramaniakuppusamy,Vaibhav Gandhi*

Main category: cs.AI

TL;DR: HyperComplEx: A hybrid embedding framework adaptively combines hyperbolic, complex, and Euclidean spaces via learned attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing embedding methods face critical limitations when modeling diverse relationship types at scale: Euclidean models struggle with hierarchies, vector space models cannot capture asymmetry, and hyperbolic models fail on symmetric relations.

Method: A relation-specific space weighting strategy dynamically selects optimal geometries for each relation type, while a multi-space consistency loss ensures coherent predictions across spaces.

Result: HyperComplEx achieves 0.612 MRR on the 10M-paper dataset, a 4.8% relative gain over the best baseline, while maintaining efficient training, achieving 85 ms inference per triple. The model scales near-linearly with graph size through adaptive dimension allocation.

Conclusion: The model scales near-linearly with graph size through adaptive dimension allocation. We release our implementation and dataset family to facilitate reproducible research in scalable knowledge graph embeddings.

Abstract: Knowledge graphs have emerged as fundamental structures for representing complex relational data across scientific and enterprise domains. However, existing embedding methods face critical limitations when modeling diverse relationship types at scale: Euclidean models struggle with hierarchies, vector space models cannot capture asymmetry, and hyperbolic models fail on symmetric relations. We propose HyperComplEx, a hybrid embedding framework that adaptively combines hyperbolic, complex, and Euclidean spaces via learned attention mechanisms. A relation-specific space weighting strategy dynamically selects optimal geometries for each relation type, while a multi-space consistency loss ensures coherent predictions across spaces. We evaluate HyperComplEx on computer science research knowledge graphs ranging from 1K papers (~25K triples) to 10M papers (~45M triples), demonstrating consistent improvements over state-of-the-art baselines including TransE, RotatE, DistMult, ComplEx, SEPA, and UltraE. Additional tests on standard benchmarks confirm significantly higher results than all baselines. On the 10M-paper dataset, HyperComplEx achieves 0.612 MRR, a 4.8% relative gain over the best baseline, while maintaining efficient training, achieving 85 ms inference per triple. The model scales near-linearly with graph size through adaptive dimension allocation. We release our implementation and dataset family to facilitate reproducible research in scalable knowledge graph embeddings.

</details>


### [104] [The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems](https://arxiv.org/abs/2511.10704)
*Samih Fadli*

Main category: cs.AI

TL;DR: AI系统在没有持续对齐工作的情况下，会自发地增加伦理熵（与预期目标的偏差），类似于热力学第二定律。


<details>
  <summary>Details</summary>
Motivation: 研究AI对齐问题，将其视为一个持续的热力学控制问题，为维护高级自治系统的稳定性和安全性提供量化基础。

Method: 定义伦理熵，证明其时间导数大于等于0，推导出对齐工作的临界稳定边界，并通过模拟验证该理论。

Result: 70亿参数的模型在没有对齐工作的情况下，熵从0.32增加到1.69，而经过对齐工作正则化的系统保持稳定。

Conclusion: AI对齐可以被看作是一个持续的热力学控制问题，该框架为维护高级自治系统的稳定性和安全性提供了量化基础。

Abstract: We propose that unconstrained artificial intelligence obeys a Second Law analogous to thermodynamics, where ethical entropy, defined as a measure of divergence from intended goals, increases spontaneously without continuous alignment work. For gradient-based optimizers, we define this entropy over a finite set of goals {g_i} as S = -Σ p(g_i; theta) ln p(g_i; theta), and we prove that its time derivative dS/dt >= 0, driven by exploration noise and specification gaming. We derive the critical stability boundary for alignment work as gamma_crit = (lambda_max / 2) ln N, where lambda_max is the dominant eigenvalue of the Fisher Information Matrix and N is the number of model parameters. Simulations validate this theory. A 7-billion-parameter model (N = 7 x 10^9) with lambda_max = 1.2 drifts from an initial entropy of 0.32 to 1.69 +/- 1.08 nats, while a system regularized with alignment work gamma = 20.4 (1.5 gamma_crit) maintains stability at 0.00 +/- 0.00 nats (p = 4.19 x 10^-17, n = 20 trials). This framework recasts AI alignment as a problem of continuous thermodynamic control, providing a quantitative foundation for maintaining the stability and safety of advanced autonomous systems.

</details>


### [105] [Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous GUI Agents](https://arxiv.org/abs/2511.10705)
*Yuan Zhao,Hualei Zhu,Tingyu Jiang,Shen Li,Xiaohang Xu,Hao Henry Wang*

Main category: cs.AI

TL;DR: 提出了一种名为Co-EPG的自迭代训练框架，用于协同进化规划和基础模型，以提高GUI任务自动化的性能。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理在利用跨模型协同和过度依赖合成数据生成方面存在不足。

Method: 通过建立迭代正反馈循环，规划模型探索更优策略，生成多样化数据以优化基础模型，同时优化后的基础模型为规划模型的后续训练提供更有效的奖励。

Result: 在Multimodal-Mind2Web和AndroidControl基准测试中，Co-EPG框架仅需三次迭代即可超越现有最佳方法，且无需外部数据。

Conclusion: 该研究建立了一种新的GUI代理训练范例，从孤立优化转向集成、自驱动的协同进化方法。

Abstract: Graphical User Interface (GUI) task automation constitutes a critical frontier in artificial intelligence research. While effective GUI agents synergistically integrate planning and grounding capabilities, current methodologies exhibit two fundamental limitations: (1) insufficient exploitation of cross-model synergies, and (2) over-reliance on synthetic data generation without sufficient utilization. To address these challenges, we propose Co-EPG, a self-iterative training framework for Co-Evolution of Planning and Grounding. Co-EPG establishes an iterative positive feedback loop: through this loop, the planning model explores superior strategies under grounding-based reward guidance via Group Relative Policy Optimization (GRPO), generating diverse data to optimize the grounding model. Concurrently, the optimized Grounding model provides more effective rewards for subsequent GRPO training of the planning model, fostering continuous improvement. Co-EPG thus enables iterative enhancement of agent capabilities through self-play optimization and training data distillation. On the Multimodal-Mind2Web and AndroidControl benchmarks, our framework outperforms existing state-of-the-art methods after just three iterations without requiring external data. The agent consistently improves with each iteration, demonstrating robust self-enhancement capabilities. This work establishes a novel training paradigm for GUI agents, shifting from isolated optimization to an integrated, self-driven co-evolution approach.

</details>


### [106] [Picking a Representative Set of Solutions in Multiobjective Optimization: Axioms, Algorithms, and Experiments](https://arxiv.org/abs/2511.10716)
*Niclas Boehmer,Maximilian T. Wittmann*

Main category: cs.AI

TL;DR: 这篇论文研究了多目标优化问题中Pareto最优解的选择，并提出了新的质量评估方法。


<details>
  <summary>Details</summary>
Motivation: 在多目标决策问题中，选择最优解是一个难题。决策者需要从所有Pareto最优解中选择一个，这会增加认知负担。

Method: 将Pareto剪枝问题转化为多胜者投票问题，并对现有质量指标进行公理化分析。同时，提出了新的指标：定向覆盖，并分析了优化各种质量指标的计算复杂性。

Result: 实验结果表明，质量指标的选择对所选解决方案的特征有决定性影响，并且提出的指标在各种设置中表现出竞争力。

Conclusion: 论文强调了质量指标选择的重要性，并证明了所提出的定向覆盖指标的有效性。

Abstract: Many real-world decision-making problems involve optimizing multiple objectives simultaneously, rendering the selection of the most preferred solution a non-trivial problem: All Pareto optimal solutions are viable candidates, and it is typically up to a decision maker to select one for implementation based on their subjective preferences. To reduce the cognitive load on the decision maker, previous work has introduced the Pareto pruning problem, where the goal is to compute a fixed-size subset of Pareto optimal solutions that best represent the full set, as evaluated by a given quality measure. Reframing Pareto pruning as a multiwinner voting problem, we conduct an axiomatic analysis of existing quality measures, uncovering several unintuitive behaviors. Motivated by these findings, we introduce a new measure, directed coverage. We also analyze the computational complexity of optimizing various quality measures, identifying previously unknown boundaries between tractable and intractable cases depending on the number and structure of the objectives. Finally, we present an experimental evaluation, demonstrating that the choice of quality measure has a decisive impact on the characteristics of the selected set of solutions and that our proposed measure performs competitively or even favorably across a range of settings.

</details>


### [107] [Structure-Aware Encodings of Argumentation Properties for Clique-width](https://arxiv.org/abs/2511.10767)
*Yasir Mahmood,Markus Hecher,Johanna Groven,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 本文研究了使用 clique-width 进行编码的能力，并考虑了抽象论证。


<details>
  <summary>Details</summary>
Motivation: 研究兴趣在于将紧凑编码成 (Q)SAT 以进行求解并理解编码限制。即使更一般的是图参数 clique-width，与 treewidth 不同，它对于密集图来说可能很小。虽然 clique-width 的算法是可用的，但关于编码知之甚少。

Method: 通过设计从论证问题到 (Q)SAT 的新颖归约来实现，这些归约线性地保留了 clique-width，从而产生有向分解引导 (DDG) 归约。

Result: 为所有论证语义（包括计数）建立了新颖的结果。值得注意的是，在合理的假设下，我们的 DDG 归约所造成的开销无法显着改善。

Conclusion: 本文启动了理解使用 clique-width 进行编码的能力的探索，并考虑了抽象论证，这是一个用于推理冲突论证的强大框架。

Abstract: Structural measures of graphs, such as treewidth, are central tools in computational complexity resulting in efficient algorithms when exploiting the parameter. It is even known that modern SAT solvers work efficiently on instances of small treewidth. Since these solvers are widely applied, research interests in compact encodings into (Q)SAT for solving and to understand encoding limitations. Even more general is the graph parameter clique-width, which unlike treewidth can be small for dense graphs. Although algorithms are available for clique-width, little is known about encodings. We initiate the quest to understand encoding capabilities with clique-width by considering abstract argumentation, which is a robust framework for reasoning with conflicting arguments. It is based on directed graphs and asks for computationally challenging properties, making it a natural candidate to study computational properties. We design novel reductions from argumentation problems to (Q)SAT. Our reductions linearly preserve the clique-width, resulting in directed decomposition-guided (DDG) reductions. We establish novel results for all argumentation semantics, including counting. Notably, the overhead caused by our DDG reductions cannot be significantly improved under reasonable assumptions.

</details>


### [108] [Potential Outcome Rankings for Counterfactual Decision Making](https://arxiv.org/abs/2511.10776)
*Yuta Kawakami,Jin Tian*

Main category: cs.AI

TL;DR: 本文提出了两种新的指标，用于在不确定性下进行反事实决策：潜在结果排序概率 (PoR) 和实现最佳潜在结果的概率 (PoB)。


<details>
  <summary>Details</summary>
Motivation: 在不确定性下进行反事实决策时，需要使用因果推理从多个备选方案中选择最优行动。决策者通常对预期的潜在结果（或其相应的效用和合意性）进行排序，以比较候选行动的偏好。

Method: 本文建立了识别定理，并推导了这些指标的界限，并提出了估计方法。

Result: 数值实验表明了估计器的有限样本属性，并展示了它们在真实世界数据集中的应用。

Conclusion: 通过引入PoR和PoB两种新指标，研究新的反事实决策规则。

Abstract: Counterfactual decision-making in the face of uncertainty involves selecting the optimal action from several alternatives using causal reasoning. Decision-makers often rank expected potential outcomes (or their corresponding utility and desirability) to compare the preferences of candidate actions. In this paper, we study new counterfactual decision-making rules by introducing two new metrics: the probabilities of potential outcome ranking (PoR) and the probability of achieving the best potential outcome (PoB). PoR reveals the most probable ranking of potential outcomes for an individual, and PoB indicates the action most likely to yield the top-ranked outcome for an individual. We then establish identification theorems and derive bounds for these metrics, and present estimation methods. Finally, we perform numerical experiments to illustrate the finite-sample properties of the estimators and demonstrate their application to a real-world dataset.

</details>


### [109] [From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models](https://arxiv.org/abs/2511.10788)
*Chao Wu,Baoheng Li,Mingchen Gao,Zhenyi Wang*

Main category: cs.AI

TL;DR: 大型语言模型在推理方面取得了显著进展，但它们在处理不同复杂度的任务时，缺乏根据任务特性调整推理努力的能力。本研究综述从适应性的角度重新审视推理，即将推理努力的分配与任务难度和不确定性相关联。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型采用统一的推理策略，无法根据任务复杂性调整推理过程，导致简单问题过度推理，复杂问题推理不足。

Method: 1. 在大型语言模型的背景下，形式化演绎、归纳和溯因推理，并将这些认知范式与算法实现联系起来。2. 将自适应推理形式化为一个控制增强的策略优化问题，平衡任务性能和计算成本。3. 提出一个系统分类法，将现有方法组织成基于训练的方法和无训练的方法。

Result: 通过训练和非训练方法，大型语言模型可以在推理过程中实现不同程度的适应性。

Conclusion: 自适应推理是提升大型语言模型推理能力的关键，未来的研究方向包括自我评估、元推理和与人类对齐的推理控制。

Abstract: Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.

</details>


### [110] [HARNESS: Human-Agent Risk Navigation and Event Safety System for Proactive Hazard Forecasting in High-Risk DOE Environments](https://arxiv.org/abs/2511.10810)
*Ran Elgedawy,Sanjay Das,Ethan Seefried,Gavin Wiggins,Ryan Burchfield,Dana Hewit,Sudarshan Srinivasan,Todd Thomas,Prasanna Balaprakash,Tirthankar Ghosal*

Main category: cs.AI

TL;DR: HARNESS is an AI framework for forecasting hazardous events and analyzing operational risks.


<details>
  <summary>Details</summary>
Motivation: Operational safety at mission-critical work sites is a top priority.

Method: It integrates Large Language Models (LLMs) with structured work data, historical event retrieval, risk analysis and a human-in-the-loop mechanism.

Result: Preliminary deployment shows promising results.

Conclusion: HARNESS improves the reliability and efficiency of predictive safety systems by combining SME collaboration with iterative agentic reasoning.

Abstract: Operational safety at mission-critical work sites is a top priority given the complex and hazardous nature of daily tasks. This paper presents the Human-Agent Risk Navigation and Event Safety System (HARNESS), a modular AI framework designed to forecast hazardous events and analyze operational risks in U.S. Department of Energy (DOE) environments. HARNESS integrates Large Language Models (LLMs) with structured work data, historical event retrieval, and risk analysis to proactively identify potential hazards. A human-in-the-loop mechanism allows subject matter experts (SMEs) to refine predictions, creating an adaptive learning loop that enhances performance over time. By combining SME collaboration with iterative agentic reasoning, HARNESS improves the reliability and efficiency of predictive safety systems. Preliminary deployment shows promising results, with future work focusing on quantitative evaluation of accuracy, SME agreement, and decision latency reduction.

</details>


### [111] [Advanced Tool for Traffic Crash Analysis: An AI-Driven Multi-Agent Approach to Pre-Crash Reconstruction](https://arxiv.org/abs/2511.10853)
*Gerui Xu,Boyou Chen,Huizhong Guo,Dave LeBlanc,Ananna Ahmed,Zhaonan Sun,Shan Bao*

Main category: cs.AI

TL;DR: 开发了一个多智能体AI框架，用于从不完整的碰撞数据中重建碰撞前场景和推断车辆行为，优于人类专家。


<details>
  <summary>Details</summary>
Motivation: 传统交通碰撞重建依赖人类专业知识，分析不完整的多模态数据时结果不一致。

Method: 提出了一个两阶段协作框架，结合了重建和推理阶段。该系统处理了来自Crash Investigation Sampling System的277起追尾LVD碰撞，整合了文本碰撞报告、结构化表格数据和视觉场景图。

Result: 在39个复杂LVD碰撞案例的评估中，该框架在所有测试案例中实现了完美的准确率，成功识别出最相关的EDR事件，并正确区分了撞击车辆和被撞车辆，超过了人类研究人员在同一具有挑战性的数据集上实现的92%的准确率。

Conclusion: 该研究证明了AI在处理异构碰撞数据方面的卓越能力，为重建冲击动力学和表征碰撞前行为提供了前所未有的精度。

Abstract: Traffic collision reconstruction traditionally relies on human expertise, often yielding inconsistent results when analyzing incomplete multimodal data. This study develops a multi-agent AI framework that reconstructs pre-crash scenarios and infers vehicle behaviors from fragmented collision data. We present a two-phase collaborative framework combining reconstruction and reasoning phases. The system processes 277 rear-end lead vehicle deceleration (LVD) collisions from the Crash Investigation Sampling System, integrating textual crash reports, structured tabular data, and visual scene diagrams. Phase I generates natural-language crash reconstructions from multimodal inputs. Phase II performs in-depth crash reasoning by combining these reconstructions with temporal Event Data Recorder (EDR).For validation, we applied it to all LVD cases, focusing on a subset of 39 complex crashes where multiple EDR records per collision introduced ambiguity (e.g., due to missing or conflicting data).The evaluation of the 39 LVD crash cases revealed our framework achieved perfect accuracy across all test cases, successfully identifying both the most relevant EDR event and correctly distinguishing striking versus struck vehicles, surpassing the 92% accuracy achieved by human researchers on the same challenging dataset. The system maintained robust performance even when processing incomplete data, including missing or erroneous EDR records and ambiguous scene diagrams. This study demonstrates superior AI capabilities in processing heterogeneous collision data, providing unprecedented precision in reconstructing impact dynamics and characterizing pre-crash behaviors.

</details>


### [112] [Enhancing Demand-Oriented Regionalization with Agentic AI and Local Heterogeneous Data for Adaptation Planning](https://arxiv.org/abs/2511.10857)
*Seyedeh Mobina Noorani,Shangde Gao,Changjie Chen,Karla Saldana Ochoa*

Main category: cs.AI

TL;DR: 这篇论文介绍了一个规划支持系统，该系统利用代理人工智能生成面向需求的灾害规划区域。


<details>
  <summary>Details</summary>
Motivation: 传统的规划单元（如人口普查区、邮政编码或社区）通常无法捕捉当地社区的特定需求，并且缺乏实施有效的灾害预防或响应策略的灵活性。

Method: 该平台构建于具有代表性的初始化空间约束自组织映射 (RepSC-SOM) 之上，通过自适应地理过滤和区域增长细化来扩展传统的 SOM，而 AI 代理可以进行推理、计划和行动，通过建议输入特征、指导空间约束和支持交互式探索来指导该过程。

Result: 通过在佛罗里达州杰克逊维尔的洪水相关风险案例研究展示了该平台的能力，展示了它如何允许用户交互式地探索、生成和评估区域化，将计算严谨性与用户驱动的决策相结合。

Conclusion: 该平台结合了计算的严谨性和用户驱动的决策，证明了其有效性

Abstract: Conventional planning units or urban regions, such as census tracts, zip codes, or neighborhoods, often do not capture the specific demands of local communities and lack the flexibility to implement effective strategies for hazard prevention or response. To support the creation of dynamic planning units, we introduce a planning support system with agentic AI that enables users to generate demand-oriented regions for disaster planning, integrating the human-in-the-loop principle for transparency and adaptability. The platform is built on a representative initialized spatially constrained self-organizing map (RepSC-SOM), extending traditional SOM with adaptive geographic filtering and region-growing refinement, while AI agents can reason, plan, and act to guide the process by suggesting input features, guiding spatial constraints, and supporting interactive exploration. We demonstrate the capabilities of the platform through a case study on the flooding-related risk in Jacksonville, Florida, showing how it allows users to explore, generate, and evaluate regionalization interactively, combining computational rigor with user-driven decision making.

</details>


### [113] [LLM enhanced graph inference for long-term disease progression modelling](https://arxiv.org/abs/2511.10890)
*Tiantian He,An Zhao,Elinor Thompson,Anna Schroder,Ahmed Abdulaal,Frederik Barkhof,Daniel C. Alexander*

Main category: cs.AI

TL;DR: 本文提出了一种新的框架，该框架利用大型语言模型（LLM）作为专家指导，研究区域变量之间的相互作用，以加强从不规则采样的纵向患者数据中学习疾病进展。该方法同时优化了长期疾病轨迹的构建和生物约束的图结构，从而更好地识别大脑区域之间的相互作用。


<details>
  <summary>Details</summary>
Motivation: 了解神经退行性疾病期间大脑区域之间生物标志物之间的相互作用对于揭示疾病进展的潜在机制至关重要。当前方法严重过度简化了大脑连接的复杂关系，假设单模态大脑连接组作为疾病传播的基质，导致对病理传播的不准确预测，尤其是在长期进展期间。同时，其他以纯粹数据驱动方式学习这种图的方法由于缺乏适当的约束而面临可识别性问题。

Method: 该框架利用LLM综合多模态关系并结合多种疾病驱动机制的能力，同时优化1）从个体层面观察构建长期疾病轨迹和2）捕获大脑区域之间相互作用的生物约束图结构，从而更好地识别。

Result: 通过估计阿尔茨海默病队列中tau-PET成像数据的病理传播，证明了该新方法的有效性。与传统方法相比，新框架表现出卓越的预测准确性和可解释性，同时揭示了传统连接测量之外的其他疾病驱动因素。

Conclusion: 该研究提出的新框架能够更准确、更可解释地预测疾病进展，并揭示传统连接测量之外的其他疾病驱动因素。

Abstract: Understanding the interactions between biomarkers among brain regions during neurodegenerative disease is essential for unravelling the mechanisms underlying disease progression. For example, pathophysiological models of Alzheimer's Disease (AD) typically describe how variables, such as regional levels of toxic proteins, interact spatiotemporally within a dynamical system driven by an underlying biological substrate, often based on brain connectivity. However, current methods grossly oversimplify the complex relationship between brain connectivity by assuming a single-modality brain connectome as the disease-spreading substrate. This leads to inaccurate predictions of pathology spread, especially during the long-term progression period. Meanhwile, other methods of learning such a graph in a purely data-driven way face the identifiability issue due to lack of proper constraint. We thus present a novel framework that uses Large Language Models (LLMs) as expert guides on the interaction of regional variables to enhance learning of disease progression from irregularly sampled longitudinal patient data. By leveraging LLMs' ability to synthesize multi-modal relationships and incorporate diverse disease-driving mechanisms, our method simultaneously optimizes 1) the construction of long-term disease trajectories from individual-level observations and 2) the biologically-constrained graph structure that captures interactions among brain regions with better identifiability. We demonstrate the new approach by estimating the pathology propagation using tau-PET imaging data from an Alzheimer's disease cohort. The new framework demonstrates superior prediction accuracy and interpretability compared to traditional approaches while revealing additional disease-driving factors beyond conventional connectivity measures.

</details>


### [114] [Multi-Agent Legal Verifier Systems for Data Transfer Planning](https://arxiv.org/abs/2511.10925)
*Ha-Thanh Nguyen,Wachara Fungwacharakorn,Ken Satoh*

Main category: cs.AI

TL;DR: 本文提出了一种多代理法律验证器，用于检查AI驱动的数据传输规划中的合规性。


<details>
  <summary>Details</summary>
Motivation: 在严格的隐私法规（如日本的APPI）下，AI驱动的数据传输规划中的法律合规性正变得越来越重要。

Method: 将合规性检查分解为专门的代理，用于法规解释、业务环境评估和风险评估，并通过结构化综合协议进行协调。

Result: 在200个修订后的APPI第16条案例的分层数据集上进行评估，系统实现了72%的准确率，比单代理基线高21个百分点，在明确的合规案例中达到90%的准确率（基线为16%），同时保持对明确违规的完美检测。

Conclusion: 领域专业化和协调推理可以显著提高法律人工智能的性能，为可信和可解释的自动合规性验证提供可扩展且具有法规意识的框架。

Abstract: Legal compliance in AI-driven data transfer planning is becoming increasingly critical under stringent privacy regulations such as the Japanese Act on the Protection of Personal Information (APPI). We propose a multi-agent legal verifier that decomposes compliance checking into specialized agents for statutory interpretation, business context evaluation, and risk assessment, coordinated through a structured synthesis protocol. Evaluated on a stratified dataset of 200 Amended APPI Article 16 cases with clearly defined ground truth labels and multiple performance metrics, the system achieves 72% accuracy, which is 21 percentage points higher than a single-agent baseline, including 90% accuracy on clear compliance cases (vs. 16% for the baseline) while maintaining perfect detection of clear violations. While challenges remain in ambiguous scenarios, these results show that domain specialization and coordinated reasoning can meaningfully improve legal AI performance, providing a scalable and regulation-aware framework for trustworthy and interpretable automated compliance verification.

</details>


### [115] [Requirements for Aligned, Dynamic Resolution of Conflicts in Operational Constraints](https://arxiv.org/abs/2511.10952)
*Steven J. Jones,Robert E. Wray,John E. Laird*

Main category: cs.AI

TL;DR: 自主AI系统在不明确的情况下评估多个行动方案，但没有一个能完全满足所有约束。为了符合人类的期望和价值观，智能体必须超越他们训练的策略，构建、评估和证明备选的行动方案。


<details>
  <summary>Details</summary>
Motivation: 为了符合人类的期望和价值观，智能体必须超越他们训练的策略，构建、评估和证明备选的行动方案。这些过程需要可能超出先前（策略）训练的上下文“知识”。

Method: 通过分析和实证案例研究，考察智能体如何整合规范、实用和情境理解，以在复杂的现实环境中选择并追求更一致的行动方案。

Result: 确定了智能体做出对智能体目标具有鲁棒性并与人类期望相一致的决策所需的知识类型。

Conclusion: 本文描述了在这些情况下智能体决策的需求。

Abstract: Deployed, autonomous AI systems must often evaluate multiple plausible courses of action (extended sequences of behavior) in novel or under-specified contexts. Despite extensive training, these systems will inevitably encounter scenarios where no available course of action fully satisfies all operational constraints (e.g., operating procedures, rules, laws, norms, and goals). To achieve goals in accordance with human expectations and values, agents must go beyond their trained policies and instead construct, evaluate, and justify candidate courses of action. These processes require contextual "knowledge" that may lie outside prior (policy) training. This paper characterizes requirements for agent decision making in these contexts. It also identifies the types of knowledge agents require to make decisions robust to agent goals and aligned with human expectations. Drawing on both analysis and empirical case studies, we examine how agents need to integrate normative, pragmatic, and situational understanding to select and then to pursue more aligned courses of action in complex, real-world environments.

</details>


### [116] [AI Agent-Driven Framework for Automated Product Knowledge Graph Construction in E-Commerce](https://arxiv.org/abs/2511.11017)
*Dimitar Peshevski,Riste Stojanov,Dimitar Trajanov*

Main category: cs.AI

TL;DR: 本文提出了一种全自动的、由AI代理驱动的框架，用于从非结构化的产品描述中构建产品知识图谱。


<details>
  <summary>Details</summary>
Motivation: 电子商务平台的快速扩张产生了大量的非结构化产品数据，这给信息检索、推荐系统和数据分析带来了巨大的挑战。知识图谱(KGs)提供了一种结构化的、可解释的格式来组织这些数据，但是构建特定于产品的KGs仍然是一个复杂的手工过程。

Method: 利用大型语言模型(llm)，我们的方法使用专门的代理分三个阶段进行操作:本体创建和扩展、本体细化和知识图谱填充。这种基于代理的方法确保了语义连贯性、可伸缩性和高质量的输出，而不需要依赖于预定义的模式或手工制作的提取规则。

Result: 我们在空调产品描述的真实数据集上评估了该系统，在本体生成和KG填充方面都表现出了强大的性能。该框架实现了超过97%的属性覆盖率和最小的冗余，验证了其有效性和实际适用性。

Conclusion: 我们的工作强调了llm在零售领域自动化结构化知识提取的潜力，为智能产品数据集成和利用提供了一条可扩展的路径。

Abstract: The rapid expansion of e-commerce platforms generates vast amounts of unstructured product data, creating significant challenges for information retrieval, recommendation systems, and data analytics. Knowledge Graphs (KGs) offer a structured, interpretable format to organize such data, yet constructing product-specific KGs remains a complex and manual process. This paper introduces a fully automated, AI agent-driven framework for constructing product knowledge graphs directly from unstructured product descriptions. Leveraging Large Language Models (LLMs), our method operates in three stages using dedicated agents: ontology creation and expansion, ontology refinement, and knowledge graph population. This agent-based approach ensures semantic coherence, scalability, and high-quality output without relying on predefined schemas or handcrafted extraction rules. We evaluate the system on a real-world dataset of air conditioner product descriptions, demonstrating strong performance in both ontology generation and KG population. The framework achieves over 97\% property coverage and minimal redundancy, validating its effectiveness and practical applicability. Our work highlights the potential of LLMs to automate structured knowledge extraction in retail, providing a scalable path toward intelligent product data integration and utilization.

</details>


### [117] [Faster Symmetry Breaking Constraints for Abstract Structures](https://arxiv.org/abs/2511.11029)
*Özgür Akgün,Mun See Chang,Ian P. Gent,Christopher Jefferson*

Main category: cs.AI

TL;DR: 提出了一种新的不完全方法，通过更好地利用抽象结构的表示来打破其对称性。


<details>
  <summary>Details</summary>
Motivation: 约束编程中，使用高级建模语言的建模者可以用抽象结构表达问题，但求解器原生不支持这些结构，因此在求解前必须转换或表示为其他结构。对称性是许多问题中存在的问题，打破对称性可以避免搜索对称解，从而加快求解过程。然而，将这种技术应用于抽象变量会产生大量复杂的约束，导致实际效果不佳。

Method: 通过更好地利用抽象结构的表示来打破抽象结构的对称性。

Result: 该方法在打破由无法区分的对象引起的对称性方面表现更好，比之前的 Akgün 等人 (2025) 提出的方法更快。

Conclusion: 提出了一种新的不完全方法，通过更好地利用抽象结构的表示来打破抽象结构的对称性，并且在特定类型的对称性打破中表现出更优的性能。

Abstract: In constraint programming and related paradigms, a modeller specifies their problem in a modelling language for a solver to search and return its solution(s). Using high-level modelling languages such as Essence, a modeller may express their problems in terms of abstract structures. These are structures not natively supported by the solvers, and so they have to be transformed into or represented as other structures before solving. For example, nested sets are abstract structures, and they can be represented as matrices in constraint solvers. Many problems contain symmetries and one very common and highly successful technique used in constraint programming is to "break" symmetries, to avoid searching for symmetric solutions. This can speed up the solving process by many orders of magnitude. Most of these symmetry-breaking techniques involve placing some kind of ordering for the variables of the problem, and picking a particular member under the symmetries, usually the smallest. Unfortunately, applying this technique to abstract variables produces a very large number of complex constraints that perform poorly in practice. In this paper, we demonstrate a new incomplete method of breaking the symmetries of abstract structures by better exploiting their representations. We apply the method in breaking the symmetries arising from indistinguishable objects, a commonly occurring type of symmetry, and show that our method is faster than the previous methods proposed in (Akgün et al. 2025).

</details>


### [118] [Key Decision-Makers in Multi-Agent Debates: Who Holds the Power?](https://arxiv.org/abs/2511.11040)
*Qian Zhang,Yan Zheng,Jinyi Liu,Hebin Liang,Lanjun Wang*

Main category: cs.AI

TL;DR: 本文研究了多智能体辩论（MAD）中角色分配策略对LLM推理能力的影响，发现“真相最后”策略能显著提升性能。为解决实际应用中真相未知的问题，提出了多智能体辩论一致性（MADC）策略，通过模拟和优化核心机制，有效评估角色一致性并推断真相。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略了角色分配策略对多智能体辩论（MAD）性能的关键影响。

Method: 1. 发现“真相最后”的角色分配策略。2. 提出多智能体辩论一致性（MADC）策略，模拟并优化核心机制，利用路径一致性评估角色一致性。

Result: 1. “真相最后”策略使MAD性能提升高达22%。2. MADC在多个LLM模型和推理任务中表现出先进的性能。

Conclusion: 研究表明角色分配策略对多智能体辩论至关重要，“真相最后”策略和MADC能有效提升LLM agent的推理能力。

Abstract: Recent studies on LLM agent scaling have highlighted the potential of Multi-Agent Debate (MAD) to enhance reasoning abilities. However, the critical aspect of role allocation strategies remains underexplored. In this study, we demonstrate that allocating roles with differing viewpoints to specific positions significantly impacts MAD's performance in reasoning tasks. Specifically, we find a novel role allocation strategy, "Truth Last", which can improve MAD performance by up to 22% in reasoning tasks. To address the issue of unknown truth in practical applications, we propose the Multi-Agent Debate Consistency (MADC) strategy, which systematically simulates and optimizes its core mechanisms. MADC incorporates path consistency to assess agreement among independent roles, simulating the role with the highest consistency score as the truth. We validated MADC across a range of LLMs (9 models), including the DeepSeek-R1 Distilled Models, on challenging reasoning tasks. MADC consistently demonstrated advanced performance, effectively overcoming MAD's performance bottlenecks and providing a crucial pathway for further improvements in LLM agent scaling.

</details>


### [119] [Autonomous Vehicle Path Planning by Searching With Differentiable Simulation](https://arxiv.org/abs/2511.11043)
*Asen Nachkov,Jan-Nico Zaech,Danda Pani Paudel,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: 提出了一种名为可微搜索模拟（DSS）的框架，该框架利用可微模拟器 Waymax 作为下一状态预测器和评估器，通过在模拟的未来轨迹上使用梯度下降来优化其行为，从而显著提高跟踪和路径规划的准确性。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，在真实世界中执行动作之前安全地优化动作至关重要，尤其是在复杂、密集的交通场景中，需要通过规划来避免碰撞和导航。

Method: 利用可微模拟器 Waymax 作为下一状态预测器和评估器，使用梯度下降在模拟的未来轨迹上优化动作。

Result: DSS（规划梯度和随机搜索的结合）在跟踪和路径规划精度方面显著优于序列预测、模仿学习、无模型强化学习和其他规划方法。

Conclusion: 提出的 DSS 框架，通过结合规划梯度和随机搜索，有效地提高了自动驾驶中的跟踪和路径规划精度。

Abstract: Planning allows an agent to safely refine its actions before executing them in the real world. In autonomous driving, this is crucial to avoid collisions and navigate in complex, dense traffic scenarios. One way to plan is to search for the best action sequence. However, this is challenging when all necessary components - policy, next-state predictor, and critic - have to be learned. Here we propose Differentiable Simulation for Search (DSS), a framework that leverages the differentiable simulator Waymax as both a next state predictor and a critic. It relies on the simulator's hardcoded dynamics, making state predictions highly accurate, while utilizing the simulator's differentiability to effectively search across action sequences. Our DSS agent optimizes its actions using gradient descent over imagined future trajectories. We show experimentally that DSS - the combination of planning gradients and stochastic search - significantly improves tracking and path planning accuracy compared to sequence prediction, imitation learning, model-free RL, and other planning methods.

</details>


### [120] [ARCTraj: A Dataset and Benchmark of Human Reasoning Trajectories for Abstract Problem Solving](https://arxiv.org/abs/2511.11079)
*Sejin Kim,Hayan Choi,Seokki Lee,Sundong Kim*

Main category: cs.AI

TL;DR: ARCTraj是一个数据集和方法框架，用于建模人类在抽象和推理语料库（ARC）中通过复杂视觉任务进行推理的过程。它通过记录时间排序的、对象级别的动作来捕捉人类如何迭代地将输入转换为输出，从而揭示了传统数据集忽略的中间推理步骤。


<details>
  <summary>Details</summary>
Motivation: 现有的ARC方法主要依赖于静态的输入-输出监督，这限制了对推理如何随时间展开的洞察。ARCTraj通过记录时间排序的、对象级别的动作来捕捉人类如何迭代地将输入转换为输出，从而弥补了这一差距。

Method: 该数据集通过O2ARC Web界面收集，包含大约10,000条轨迹，这些轨迹带有任务标识符、时间戳和来自ARC-AGI-1基准测试中400个训练任务的成功标签。它进一步定义了一个统一的推理流程，包括数据收集、动作抽象、马尔可夫决策过程（MDP）公式化和下游学习，从而能够与强化学习、生成建模和序列建模方法（如PPO、World Models、GFlowNets、Diffusion agents和Decision Transformers）集成。

Result: 对空间选择、颜色归属和战略收敛的分析突出了人类推理的结构和多样性。

Conclusion: ARCTraj作为一个结构化和可解释的基础，用于研究类人推理，从而提高可解释性、一致性和通用智能。

Abstract: We present ARCTraj, a dataset and methodological framework for modeling human reasoning through complex visual tasks in the Abstraction and Reasoning Corpus (ARC). While ARC has inspired extensive research on abstract reasoning, most existing approaches rely on static input--output supervision, which limits insight into how reasoning unfolds over time. ARCTraj addresses this gap by recording temporally ordered, object-level actions that capture how humans iteratively transform inputs into outputs, revealing intermediate reasoning steps that conventional datasets overlook. Collected via the O2ARC web interface, it contains around 10,000 trajectories annotated with task identifiers, timestamps, and success labels across 400 training tasks from the ARC-AGI-1 benchmark. It further defines a unified reasoning pipeline encompassing data collection, action abstraction, Markov decision process (MDP) formulation, and downstream learning, enabling integration with reinforcement learning, generative modeling, and sequence modeling methods such as PPO, World Models, GFlowNets, Diffusion agents, and Decision Transformers. Analyses of spatial selection, color attribution, and strategic convergence highlight the structure and diversity of human reasoning. Together, these contributions position ARCTraj as a structured and interpretable foundation for studying human-like reasoning, advancing explainability, alignment, and generalizable intelligence.

</details>


### [121] [Satisficing and Optimal Generalised Planning via Goal Regression (Extended Version)](https://arxiv.org/abs/2511.11095)
*Dillon Z. Chen,Till Hofmann,Toryn Q. Klassen,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 提出了一种新的通用规划方法，该方法通过对训练问题进行目标回归，并提升相应的输出来获得一阶规则。


<details>
  <summary>Details</summary>
Motivation: 解决传统规划方法在解决相关规划问题族时存在的局限性，旨在合成能够解决这些问题的程序。

Method: 对于每个训练问题，计算每个目标原子在某个顺序下的最优计划，对结果计划执行目标回归，并提升相应的输出来获得一阶规则。

Result: 在合成成本、规划覆盖率和各种经典和数值规划领域的解决方案质量方面，与最先进的（通用）规划器相比，实验证明了显着改进。

Conclusion: 该方法可以学习有效的通用规划和状态空间剪枝公理，并且实验结果表明该方法在多个指标上优于现有技术。

Abstract: Generalised planning (GP) refers to the task of synthesising programs that solve families of related planning problems. We introduce a novel, yet simple method for GP: given a set of training problems, for each problem, compute an optimal plan for each goal atom in some order, perform goal regression on the resulting plans, and lift the corresponding outputs to obtain a set of first-order $\textit{Condition} \rightarrow \textit{Actions}$ rules. The rules collectively constitute a generalised plan that can be executed as is or alternatively be used to prune the planning search space. We formalise and prove the conditions under which our method is guaranteed to learn valid generalised plans and state space pruning axioms for search. Experiments demonstrate significant improvements over state-of-the-art (generalised) planners with respect to the 3 metrics of synthesis cost, planning coverage, and solution quality on various classical and numeric planning domains.

</details>


### [122] [GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models](https://arxiv.org/abs/2511.11134)
*Jingxuan Wei,Caijun Jia,Xi Bai,Xinglong Xu,Siyuan Li,Linzhuang Sun,Bihui Yu,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.AI

TL;DR: 这篇论文介绍了一个新的几何构造基准，用于评估统一多模态模型中的生成推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基准无法评估生成推理的综合认知过程。

Method: 提出了一个名为GGBench的基准，专门用于评估几何生成推理。

Result: GGBench提供了一个全面的框架，用于系统地诊断模型理解、推理和主动构建解决方案的能力。

Conclusion: 该基准为下一代智能系统设定了更严格的标准。

Abstract: The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.

</details>


### [123] [Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning](https://arxiv.org/abs/2511.11182)
*Dayong Liang,Xiao-Yong Wei,Changmeng Zheng*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的多智能体框架，用于解决大型语言模型（LLM）中存在的幻觉问题，通过模拟“谁是卧底”游戏，利用多模态反事实测试来检测和排除产生幻觉的智能体，从而提高LLM推理的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中的幻觉现象严重阻碍了其推理能力。现有的多智能体辩论（MAD）范式依赖于所有辩论者都是理性和反思的假设，但这在实践中很难成立，因为智能体本身也容易产生幻觉。

Method: 提出了多智能体卧底游戏（MUG）协议，该协议通过多模态反事实测试来识别产生幻觉的“卧底”智能体。具体来说，通过修改参考图像引入反事实证据，观察智能体是否能准确识别这些变化，以此来判断哪些智能体产生了幻觉。

Result: MUG协议通过反事实测试实现了超越统计共识的事实验证，通过动态修改的证据源引入了交叉证据推理，并促进了智能体之间积极的推理讨论。

Conclusion: MUG框架为LLM中的多模态推理提供了一个更可靠和有效的框架，它通过检测和排除产生幻觉的智能体，提高了推理的准确性和可靠性。

Abstract: Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like "Who is Undercover?". MUG reframes MAD as a process of detecting "undercover" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.

</details>


### [124] [STaR: Towards Cognitive Table Reasoning via Slow-Thinking Large Language Models](https://arxiv.org/abs/2511.11233)
*Huajian Zhang,Mingyue Cheng,Yucong Luo,Xiaoyu Tao*

Main category: cs.AI

TL;DR: STaR: A new framework achieving cognitive table reasoning, in which LLMs are equipped with slow-thinking capabilities by explicitly modeling step-by-step thinking and uncertainty-aware inference.


<details>
  <summary>Details</summary>
Motivation:  reasoning processes lack the depth and iterative refinement characteristic of human cognition and exhibit instability, which compromises their reliability in downstream applications.

Method: STaR employs two-stage difficulty-aware reinforcement learning (DRL), progressively learning from simple to complex queries under a composite reward. During inference, STaR performs trajectory-level uncertainty quantification by integrating token-level confidence and answer consistency, enabling selection of more credible reasoning paths.

Result: STaR achieves superior performance and enhanced reasoning stability. Moreover, strong generalization over out-of-domain datasets further demonstrates STaR's potential as a reliable and cognitively inspired solution for table reasoning with LLMs.

Conclusion: STaR is a reliable and cognitively inspired solution for table reasoning with LLMs.

Abstract: Table reasoning with the large language models (LLMs) is a fundamental path toward building intelligent systems that can understand and analyze over structured data. While recent progress has shown promising results, they still suffer from two key limitations: (i) the reasoning processes lack the depth and iterative refinement characteristic of human cognition; and (ii) the reasoning processes exhibit instability, which compromises their reliability in downstream applications. In this work, we present STaR (slow-thinking for table reasoning), a new framework achieving cognitive table reasoning, in which LLMs are equipped with slow-thinking capabilities by explicitly modeling step-by-step thinking and uncertainty-aware inference. During training, STaR employs two-stage difficulty-aware reinforcement learning (DRL), progressively learning from simple to complex queries under a composite reward. During inference, STaR performs trajectory-level uncertainty quantification by integrating token-level confidence and answer consistency, enabling selection of more credible reasoning paths. Extensive experiments on benchmarks demonstrate that STaR achieves superior performance and enhanced reasoning stability. Moreover, strong generalization over out-of-domain datasets further demonstrates STaR's potential as a reliable and cognitively inspired solution for table reasoning with LLMs.

</details>


### [125] [UAVBench: An Open Benchmark Dataset for Autonomous and Agentic AI UAV Systems via LLM-Generated Flight Scenarios](https://arxiv.org/abs/2511.11252)
*Mohamed Amine Ferrag,Abderrahmane Lakas,Merouane Debbah*

Main category: cs.AI

TL;DR: UAVBench是一个新的无人机飞行场景基准数据集，用于评估大型语言模型（LLM）在无人机任务中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 缺乏标准化的、基于物理的基准来评估LLM在无人机自主系统中的推理能力。

Method: 通过分类引导的LLM提示和多阶段安全验证生成50,000个验证的无人机飞行场景，并将其编码为包含任务目标、车辆配置、环境条件和风险标签的JSON模式。还提出了一个面向推理的扩展UAVBench_MCQ，包含50,000个多项选择题。

Result: 评估了32个LLM，发现在感知和策略推理方面表现出色，但在伦理感知和资源受限的决策方面仍然存在挑战。

Conclusion: UAVBench为无人机自主系统中基于代理的AI的基准测试建立了一个可重复且基于物理的基础，并促进下一代无人机推理智能的发展。

Abstract: Autonomous aerial systems increasingly rely on large language models (LLMs) for mission planning, perception, and decision-making, yet the lack of standardized and physically grounded benchmarks limits systematic evaluation of their reasoning capabilities. To address this gap, we introduce UAVBench, an open benchmark dataset comprising 50,000 validated UAV flight scenarios generated through taxonomy-guided LLM prompting and multi-stage safety validation. Each scenario is encoded in a structured JSON schema that includes mission objectives, vehicle configuration, environmental conditions, and quantitative risk labels, providing a unified representation of UAV operations across diverse domains. Building on this foundation, we present UAVBench_MCQ, a reasoning-oriented extension containing 50,000 multiple-choice questions spanning ten cognitive and ethical reasoning styles, ranging from aerodynamics and navigation to multi-agent coordination and integrated reasoning. This framework enables interpretable and machine-checkable assessment of UAV-specific cognition under realistic operational contexts. We evaluate 32 state-of-the-art LLMs, including GPT-5, ChatGPT-4o, Gemini 2.5 Flash, DeepSeek V3, Qwen3 235B, and ERNIE 4.5 300B, and find strong performance in perception and policy reasoning but persistent challenges in ethics-aware and resource-constrained decision-making. UAVBench establishes a reproducible and physically grounded foundation for benchmarking agentic AI in autonomous aerial systems and advancing next-generation UAV reasoning intelligence. To support open science and reproducibility, we release the UAVBench dataset, the UAVBench_MCQ benchmark, evaluation scripts, and all related materials on GitHub at https://github.com/maferrag/UAVBench

</details>


### [126] [AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery](https://arxiv.org/abs/2511.11257)
*Yuqi Yin,Yibo Fu,Siyuan Wang,Peng Sun,Hongyu Wang,Xiaohui Wang,Lei Zheng,Zhiyong Li,Zhirong Liu,Jianji Wang,Zhaoxi Sun*

Main category: cs.AI

TL;DR: 提出了AIonopedia，一个用于离子液体发现的大型语言模型代理。


<details>
  <summary>Details</summary>
Motivation: 现有离子液体的发现受到性质预测的限制，包括数据有限、模型准确性差和工作流程分散。

Method: 利用大型语言模型，AIonopedia 结合了多模态领域基础模型和分层搜索架构，用于分子筛选和设计。

Result: AIonopedia 在新策划的综合离子液体数据集上进行了训练和评估，表现出卓越的性能。在文献报告的系统上的评估表明，该代理可以执行有效的离子液体修改。通过实际湿实验室验证进一步证实了其实际功效。

Conclusion: AIonopedia 在具有挑战性的分布外任务中表现出卓越的泛化能力，突显了其加速现实世界离子液体发现的能力。

Abstract: The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.

</details>


### [127] [A Workflow for Full Traceability of AI Decisions](https://arxiv.org/abs/2511.11275)
*Julius Wenzel,Syeda Umaima Alam,Andreas Schmidt,Hanwei Zhang,Holger Hermanns*

Main category: cs.AI

TL;DR: 本文提出了一种通过强制记录每个组件来解决AI决策可追溯性问题的方法，并展示了一个利用机密计算技术的工作流程示例，用于生成防篡改、可验证和详尽的AI决策跟踪。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在决策过程的适当文档记录方面做得很少，这阻碍了追溯决策过程的能力，进而阻碍了重建责任链的尝试。特别是，这种可追溯性与在法庭上确定基于AI的决策违反法律的原因时能够成立的文档相关联。

Method: 通过强制记录进入自动化决策的训练或推理的每个组件，本文采取了一种激进但实用的方法。

Result: 本文提出了第一个运行的工作流程，支持生成防篡改、可验证和详尽的AI决策跟踪。

Conclusion: 本文通过利用机密计算技术，将DBOM概念扩展到有效的运行工作流程中。

Abstract: An ever increasing number of high-stake decisions are made or assisted by automated systems employing brittle artificial intelligence technology. There is a substantial risk that some of these decision induce harm to people, by infringing their well-being or their fundamental human rights. The state-of-the-art in AI systems makes little effort with respect to appropriate documentation of the decision process. This obstructs the ability to trace what went into a decision, which in turn is a prerequisite to any attempt of reconstructing a responsibility chain. Specifically, such traceability is linked to a documentation that will stand up in court when determining the cause of some AI-based decision that inadvertently or intentionally violates the law.
  This paper takes a radical, yet practical, approach to this problem, by enforcing the documentation of each and every component that goes into the training or inference of an automated decision. As such, it presents the first running workflow supporting the generation of tamper-proof, verifiable and exhaustive traces of AI decisions. In doing so, we expand the DBOM concept into an effective running workflow leveraging confidential computing technology. We demonstrate the inner workings of the workflow in the development of an app to tell poisonous and edible mushrooms apart, meant as a playful example of high-stake decision support.

</details>


### [128] [Can You Tell the Difference? Contrastive Explanations for ABox Entailments](https://arxiv.org/abs/2511.11281)
*Patrick Koopmann,Yasir Mahmood,Axel-Cyrille Ngonga Ngomo,Balram Tiwari*

Main category: cs.AI

TL;DR: 对比ABox解释回答了“为什么a是C的实例，而b不是？”这类问题。它同时考虑了肯定蕴涵（为什么知识库蕴涵C(a)）和缺失蕴涵（为什么C(b)不被蕴涵），能够专注于a和b之间相关的共同点和不同点。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常孤立地解释肯定蕴涵或缺失蕴涵，而对比解释则同时考虑两者，从而能够突出a和b之间的关键差异。

Method: 针对描述逻辑本体的ABox推理，开发了一种对比解释方法，并分析了不同变体在不同优化标准下的计算复杂度，考虑了轻量级和更具表现力的描述逻辑。

Result: 实现了一种计算对比解释变体的初始方法，并在真实知识库的生成问题上进行了评估。

Conclusion: 论文提出对比ABox解释的概念，并进行了初步的理论分析、方法实现和实验评估。

Abstract: We introduce the notion of contrastive ABox explanations to answer questions of the type "Why is a an instance of C, but b is not?". While there are various approaches for explaining positive entailments (why is C(a) entailed by the knowledge base) as well as missing entailments (why is C(b) not entailed) in isolation, contrastive explanations consider both at the same time, which allows them to focus on the relevant commonalities and differences between a and b. We develop an appropriate notion of contrastive explanations for the special case of ABox reasoning with description logic ontologies, and analyze the computational complexity for different variants under different optimality criteria, considering lightweight as well as more expressive description logics. We implemented a first method for computing one variant of contrastive explanations, and evaluated it on generated problems for realistic knowledge bases.

</details>


### [129] [EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment](https://arxiv.org/abs/2511.11301)
*Ruoxi Cheng,Haoxuan Ma,Teng Ma,Hongyi Zhang*

Main category: cs.AI

TL;DR: EcoAlign是一个在推理时运行的框架，它通过将LVLM视为具有有限理性的代理，并使用前瞻性函数来动态地权衡预期安全性、效用和成本与剩余预算，从而将对齐重新定义为经济理性搜索。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型(LVLM)表现出强大的推理能力，但存在复杂的越狱漏洞。从根本上说，对齐lvlm不仅仅是一个安全挑战，也是一个经济效率问题。当前的对齐方法难以在安全性、效用和运营成本之间进行权衡。对最终输出的关注(过程盲视)浪费了大量的计算预算。

Method: EcoAlign增量扩展了一个思想图，并使用一个前瞻性函数(类似于净现值)对行动进行评分，该函数动态地权衡预期安全性、效用和成本与剩余预算。为了防止欺骗，路径安全是通过最弱连接原则来执行的。

Result: 在6个数据集上对3个闭源模型和2个开源模型进行的大量实验表明，EcoAlign以较低的计算成本匹配或超过了最先进的安全性和效用。

Conclusion: EcoAlign为鲁棒的LVLM对齐提供了一条有原则的、经济的途径。

Abstract: Large Vision-Language Models (LVLMs) exhibit powerful reasoning capabilities but suffer sophisticated jailbreak vulnerabilities. Fundamentally, aligning LVLMs is not just a safety challenge but a problem of economic efficiency. Current alignment methods struggle with the trade-off between safety, utility, and operational costs. Critically, a focus solely on final outputs (process-blindness) wastes significant computational budget on unsafe deliberation. This flaw allows harmful reasoning to be disguised with benign justifications, thereby circumventing simple additive safety scores. To address this, we propose EcoAlign, an inference-time framework that reframes alignment as an economically rational search by treating the LVLM as a boundedly rational agent. EcoAlign incrementally expands a thought graph and scores actions using a forward-looking function (analogous to net present value) that dynamically weighs expected safety, utility, and cost against the remaining budget. To prevent deception, path safety is enforced via the weakest-link principle. Extensive experiments across 3 closed-source and 2 open-source models on 6 datasets show that EcoAlign matches or surpasses state-of-the-art safety and utility at a lower computational cost, thereby offering a principled, economical pathway to robust LVLM alignment.

</details>


### [130] [RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms](https://arxiv.org/abs/2511.11323)
*Yitian Kou,Yihe Gu,Chen Zhou,DanDan Zhu,Shuguang Kuai*

Main category: cs.AI

TL;DR: 提出了一种混合强化学习框架RLSLM，将基于规则的社会运动模型集成到强化学习框架的奖励函数中，以实现具有社会意识的导航策略。


<details>
  <summary>Details</summary>
Motivation: 在以人为中心的环境中导航而不引起不适对于具有社会意识的agent至关重要。现有的基于规则的方法缺乏泛化性和灵活性，而数据驱动的方法通常效率低下、不透明且难以与人类直觉对齐。

Method: 将基于规则的社会运动模型整合到强化学习框架的奖励函数中。该社会运动模型生成一个对方向敏感的社会舒适场，量化空间中的人类舒适度，从而实现具有社会意识的导航策略。RLSLM 共同优化机械能和社会舒适度，允许 agent 避免侵入个人或群体空间。

Result: 在基于沉浸式 VR 的人机交互实验中，RLSLM 在用户体验方面优于最先进的基于规则的模型。消融和敏感性分析进一步表明，该模型比传统的数据驱动方法具有显着改进的可解释性。

Conclusion: 提出了一种可扩展的、以人为中心的方法，有效地整合了认知科学和机器学习，用于现实世界的社会导航。

Abstract: Navigating human-populated environments without causing discomfort is a critical capability for socially-aware agents. While rule-based approaches offer interpretability through predefined psychological principles, they often lack generalizability and flexibility. Conversely, data-driven methods can learn complex behaviors from large-scale datasets, but are typically inefficient, opaque, and difficult to align with human intuitions. To bridge this gap, we propose RLSLM, a hybrid Reinforcement Learning framework that integrates a rule-based Social Locomotion Model, grounded in empirical behavioral experiments, into the reward function of a reinforcement learning framework. The social locomotion model generates an orientation-sensitive social comfort field that quantifies human comfort across space, enabling socially aligned navigation policies with minimal training. RLSLM then jointly optimizes mechanical energy and social comfort, allowing agents to avoid intrusions into personal or group space. A human-agent interaction experiment using an immersive VR-based setup demonstrates that RLSLM outperforms state-of-the-art rule-based models in user experience. Ablation and sensitivity analyses further show the model's significantly improved interpretability over conventional data-driven methods. This work presents a scalable, human-centered methodology that effectively integrates cognitive science and machine learning for real-world social navigation.

</details>


### [131] [KarmaTS: A Universal Simulation Platform for Multivariate Time Series with Functional Causal Dynamics](https://arxiv.org/abs/2511.11357)
*Haixin Li,Yanke Li,Diego Paez-Granados*

Main category: cs.AI

TL;DR: KarmaTS是一个用于构建时滞索引、可执行的时空因果图模型的交互式框架，用于多元时间序列（MTS）模拟。


<details>
  <summary>Details</summary>
Motivation: 由于生理数据访问受限，KarmaTS旨在生成具有已知因果动态的合成MTS，并通过专家知识增强真实数据集。

Method: 该系统通过混合专家知识和算法建议，构建离散时间结构因果过程（DSCP）。

Result: 生成的DSCP支持模拟和因果干预，包括用户指定的分布变化。

Conclusion: KarmaTS能够处理混合变量类型、同期和滞后边，以及从参数化模板到神经网络模型等模块化边函数，从而通过专家指导的模拟实现因果发现算法的灵活验证和基准测试。

Abstract: We introduce KarmaTS, an interactive framework for constructing lag-indexed, executable spatiotemporal causal graphical models for multivariate time series (MTS) simulation. Motivated by the challenge of access-restricted physiological data, KarmaTS generates synthetic MTS with known causal dynamics and augments real-world datasets with expert knowledge. The system constructs a discrete-time structural causal process (DSCP) by combining expert knowledge and algorithmic proposals in a mixed-initiative, human-in-the-loop workflow. The resulting DSCP supports simulation and causal interventions, including those under user-specified distribution shifts. KarmaTS handles mixed variable types, contemporaneous and lagged edges, and modular edge functionals ranging from parameterizable templates to neural network models. Together, these features enable flexible validation and benchmarking of causal discovery algorithms through expert-informed simulation.

</details>


### [132] [MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism](https://arxiv.org/abs/2511.11373)
*Shulin Liu,Dong Du,Tao Yang,Yang Li,Boyu Qiu*

Main category: cs.AI

TL;DR: 提出了一种新的强化学习框架MarsRL，用于优化多智能体推理系统，以解决开放源代码模型中critic和校正能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的输出长度有限，限制了单次推理过程中可达到的推理深度。多智能体推理系统提供了一种有希望的替代方案，但由于缺乏足够的批评和纠正能力，它们难以推广到开放源代码模型。

Method: 提出了一种新的强化学习框架，具有智能体管道并行性，旨在共同优化系统中的所有智能体。MarsRL引入了特定于智能体的奖励机制，以减少奖励噪声，并采用受管道启发的训练来提高处理长轨迹的效率。

Result: 应用于Qwen3-30B-A3B-Thinking-2507，MarsRL将AIME2025的准确率从86.5%提高到93.3%，并将BeyondAIME的准确率从64.9%提高到73.8%，甚至超过了Qwen3-235B-A22B-Thinking-2507。

Conclusion: MarsRL具有推进多智能体推理系统的潜力，并扩大其在各种推理任务中的适用性。

Abstract: Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks.

</details>


### [133] [Robust and Efficient Communication in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.11393)
*Zejiao Liu,Yi Li,Jiali Wang,Junqi Tu,Yitian Hong,Fangfei Li,Yang Liu,Toshiharu Sugawara,Yang Tang*

Main category: cs.AI

TL;DR: 本研究综述了在实际约束下，多智能体强化学习（MARL）中鲁棒和高效的通信策略的最新进展。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设通信是即时、可靠且带宽无限的，但现实部署中很少满足这些条件。

Method: 系统地回顾了消息扰动、传输延迟和有限带宽等实际约束下的通信策略。

Result: 重点关注合作自动驾驶、分布式同步定位与地图构建和联邦学习这三个应用。

Conclusion: 强调了开放性挑战和未来研究方向，提倡统一设计通信、学习和鲁棒性，以弥合理论MARL模型和实际实现之间的差距。

Abstract: Multi-agent reinforcement learning (MARL) has made significant strides in enabling coordinated behaviors among autonomous agents. However, most existing approaches assume that communication is instantaneous, reliable, and has unlimited bandwidth; these conditions are rarely met in real-world deployments. This survey systematically reviews recent advances in robust and efficient communication strategies for MARL under realistic constraints, including message perturbations, transmission delays, and limited bandwidth. Furthermore, because the challenges of low-latency reliability, bandwidth-intensive data sharing, and communication-privacy trade-offs are central to practical MARL systems, we focus on three applications involving cooperative autonomous driving, distributed simultaneous localization and mapping, and federated learning. Finally, we identify key open challenges and future research directions, advocating a unified approach that co-designs communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations.

</details>


### [134] [CURENet: Combining Unified Representations for Efficient Chronic Disease Prediction](https://arxiv.org/abs/2511.11423)
*Cong-Tinh Dao,Nguyen Minh Thao Phan,Jun-En Ding,Chenwei Wu,David Restrepo,Dongsheng Luo,Fanyi Zhao,Chun-Chieh Liao,Wen-Chih Peng,Chi-Te Wang,Pei-Fu Chen,Ling Chen,Xinglong Ju,Feng Liu,Fang-Ming Hung*

Main category: cs.AI

TL;DR: CURENet: A multimodal model integrates unstructured clinical notes, lab tests, and time-series data for chronic disease prediction.


<details>
  <summary>Details</summary>
Motivation: Most predictive models fail to capture the interactions and temporal patterns across multiple EHR data modalities.

Method: Utilizes LLMs for clinical text and textual lab tests, and transformer encoders for longitudinal sequential visits.

Result: Achieved over 94% accuracy in predicting the top 10 chronic conditions on MIMIC-III and FEMH datasets.

Conclusion: Multimodal EHR integration enhances clinical decision-making and improves patient outcomes.

Abstract: Electronic health records (EHRs) are designed to synthesize diverse data types, including unstructured clinical notes, structured lab tests, and time-series visit data. Physicians draw on these multimodal and temporal sources of EHR data to form a comprehensive view of a patient's health, which is crucial for informed therapeutic decision-making. Yet, most predictive models fail to fully capture the interactions, redundancies, and temporal patterns across multiple data modalities, often focusing on a single data type or overlooking these complexities. In this paper, we present CURENet, a multimodal model (Combining Unified Representations for Efficient chronic disease prediction) that integrates unstructured clinical notes, lab tests, and patients' time-series data by utilizing large language models (LLMs) for clinical text processing and textual lab tests, as well as transformer encoders for longitudinal sequential visits. CURENet has been capable of capturing the intricate interaction between different forms of clinical data and creating a more reliable predictive model for chronic illnesses. We evaluated CURENet using the public MIMIC-III and private FEMH datasets, where it achieved over 94\% accuracy in predicting the top 10 chronic conditions in a multi-label framework. Our findings highlight the potential of multimodal EHR integration to enhance clinical decision-making and improve patient outcomes.

</details>


### [135] [Experience-Guided Adaptation of Inference-Time Reasoning Strategies](https://arxiv.org/abs/2511.11519)
*Adam Stein,Matthew Trager,Benjamin Bowman,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: EGuR: An agentic AI system that dynamically adapts problem-solving strategies at inference time based on accumulated experience.


<details>
  <summary>Details</summary>
Motivation: Existing agentic AI systems struggle to adapt their problem-solving approaches based on post-training interactions. Current systems either lack flexibility in adapting strategy components or require offline optimization and remain static after deployment.

Method: EGuR uses an LLM-based meta-strategy to generate tailored strategies dynamically at inference time, adapting prompts, sampling parameters, tool configurations, and control logic. It consists of a Guide that generates candidate strategies and a Consolidator that integrates execution feedback.

Result: EGuR achieves up to 14% accuracy improvements and reduces computational costs by up to 111x across five challenging benchmarks.

Conclusion: EGuR effectively adapts problem-solving strategies, improving accuracy and reducing computational costs as it gains experience.

Abstract: Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.

</details>


### [136] [Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping](https://arxiv.org/abs/2511.11551)
*Dena Mujtaba,Brian Hu,Anthony Hoogs,Arslan Basharat*

Main category: cs.AI

TL;DR: 本文提出了一种测试时对齐技术，基于模型引导的策略塑造，以解决决策AI智能体在复杂环境中与人类价值观对齐的挑战，无需重新训练智能体。


<details>
  <summary>Details</summary>
Motivation: 在复杂、动态环境中，决策AI智能体难以与人类价值观或指导方针保持一致。仅仅为了实现目标而训练的智能体可能会采取有害行为，这暴露了最大化奖励函数和保持对齐之间的一个关键权衡。对于预训练的智能体，确保对齐特别具有挑战性，因为重新训练可能是一个代价高昂且缓慢的过程。

Method: 提出了一种基于模型引导的策略塑造的测试时对齐技术。该方法允许精确控制个体行为属性，推广到不同的强化学习（RL）环境，并促进道德对齐和奖励最大化之间的有原则的权衡，而无需智能体重新训练。

Result: 在MACHIAVELLI基准测试中评估了该方法，该基准测试包含134个基于文本的游戏环境和数千个涉及道德决策的带注释的场景。结果表明，测试时策略塑造为缓解不同环境和对齐属性中的不道德行为提供了一种有效且可扩展的解决方案。

Conclusion: 测试时策略塑造为在各种环境和对齐属性中缓解不道德行为提供了一种有效且可扩展的解决方案。

Abstract: The deployment of decision-making AI agents presents a critical challenge in maintaining alignment with human values or guidelines while operating in complex, dynamic environments. Agents trained solely to achieve their objectives may adopt harmful behavior, exposing a key trade-off between maximizing the reward function and maintaining the alignment. For the pre-trained agents, ensuring alignment is particularly challenging, as retraining can be a costly and slow process. This is further complicated by the diverse and potentially conflicting attributes representing the ethical values for alignment. To address these challenges, we propose a test-time alignment technique based on model-guided policy shaping. Our method allows precise control over individual behavioral attributes, generalizes across diverse reinforcement learning (RL) environments, and facilitates a principled trade-off between ethical alignment and reward maximization without requiring agent retraining. We evaluate our approach using the MACHIAVELLI benchmark, which comprises 134 text-based game environments and thousands of annotated scenarios involving ethical decisions. The RL agents are first trained to maximize the reward in their respective games. At test time, we apply policy shaping via scenario-action attribute classifiers to ensure decision alignment with ethical attributes. We compare our approach against prior training-time methods and general-purpose agents, as well as study several types of ethical violations and power-seeking behavior. Our results demonstrate that test-time policy shaping provides an effective and scalable solution for mitigating unethical behavior across diverse environments and alignment attributes.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [137] [ResBench: A Comprehensive Framework for Evaluating Database Resilience](https://arxiv.org/abs/2511.11088)
*Puyun Hu,Wei Pan,Xun Jian,Zeqi Ma,Tianjie Li,Yang Shen,Chengzhi Han,Yudong Zhao,Zhanhuai Li*

Main category: cs.DB

TL;DR: 本文提出了ResBench，一个用于评估数据库面对逆境时的弹性的基准。


<details>
  <summary>Details</summary>
Motivation: 现有的数据库基准主要关注理想运行环境下的性能，但在现实世界中，数据库可能会面临许多不利事件。从综合的角度量化应对这些事件的能力仍然是一个悬而未决的问题。

Method: ResBench通过清晰的分层解耦实现了测试过程的自动化、标准化和可视化。ResBench模拟不利事件并在正常事务处理期间注入它们，利用一个模块来收集多个指标以进行评估。

Result: 本文评估了数据库在八个维度上的弹性：吞吐量、延迟、稳定性、抵抗性、恢复性、扰动期、适应能力和指标偏差。所有结果都通过用户友好的图形界面呈现给用户。

Conclusion: 本文使用两种类型的不利数据集演示了ResBench的执行过程和结果解释。

Abstract: Existing database benchmarks primarily focus on performance under ideal running environments. However, in real-world scenarios, databases probably face numerous adverse events. Quantifying the ability to cope with these events from a comprehensive perspective remains an open problem. We provide the definition of database resilience to describe its performance when facing adversity and propose ResBench, a benchmark for evaluating database resilience. This framework achieves automation, standardization, and visualization of the testing process through clear hierarchical decoupling. ResBench simulates adverse events and injects them during normal transaction processing, utilizing a module to gather multiple metrics for the evaluation model. We assess database resilience across eight dimensions: throughput, latency, stability, resistance, recovery, disturbance period, adaptation capability and metric deviation. All the results are presented to users via a user-friendly graphical interface. We demonstrate the execution process and result interpretation of ResBench using two types of adversity datasets.

</details>


### [138] [Unlocking Advanced Graph Machine Learning Insights through Knowledge Completion on Neo4j Graph Database](https://arxiv.org/abs/2511.11399)
*Rosario Napoli,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.DB

TL;DR: 本文提出了一种新的GDB-GML架构，该架构通过在图数据库的图机器学习应用中集成知识补全（KC）阶段，揭示隐藏知识，从而改进数据集的行为和指标。


<details>
  <summary>Details</summary>
Motivation: 当前图数据库的图机器学习应用在知识图谱的知识补全方面存在差距，忽略了隐藏知识，导致GML模型输入错误。

Method: 本文提出了一种新的架构，将KC阶段集成到GDB-GML应用中，引入可扩展的传递关系，通过衰减函数建模，允许跨多个节点的确定性知识流动。

Result: 实验结果表明，该方法可以重塑拓扑和数据集动态。

Conclusion: 该架构对于生成更好的模型和释放基于图的数据分析的全部潜力至关重要。

Abstract: Graph Machine Learning (GML) with Graph Databases (GDBs) has gained significant relevance in recent years, due to its ability to handle complex interconnected data and apply ML techniques using Graph Data Science (GDS). However, a critical gap exists in the current way GDB-GML applications analyze data, especially in terms of Knowledge Completion (KC) in Knowledge Graphs (KGs). In particular, current architectures ignore KC, working on datasets that appear incomplete or fragmented, despite they actually contain valuable hidden knowledge. This limitation may cause wrong interpretations when these data are used as input for GML models.
  This paper proposes an innovative architecture that integrates a KC phase into GDB-GML applications, demonstrating how revealing hidden knowledge can heavily impact datasets' behavior and metrics. For this purpose, we introduce scalable transitive relationships, which are links that propagate information over the network and modelled by a decay function, allowing a deterministic knowledge flows across multiple nodes.
  Experimental results demonstrate that our intuition radically reshapes both topology and overall dataset dynamics, underscoring the need for this new GDB-GML architecture to produce better models and unlock the full potential of graph-based data analysis.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [139] [LEMUR: Large scale End-to-end MUltimodal Recommendation](https://arxiv.org/abs/2511.10962)
*Xintian Han,Honggang Chen,Quan Lin,Jingyue Gao,Xiangyuan Ren,Lifei Zhu,Zhisheng Ye,Shikang Wu,XiongHang Xie,Xiaochu Gan,Bingzheng Wei,Peng Xu,Zhe Wang,Yuchao Zheng,Jingjian Lin,Di Wu,Junfeng Ge*

Main category: cs.IR

TL;DR: 提出了一种名为 LEMUR 的端到端多模态推荐系统，该系统从原始数据进行训练，以解决传统推荐系统中的冷启动和泛化问题。


<details>
  <summary>Details</summary>
Motivation: 传统的基于 ID 的推荐系统通常难以解决冷启动和泛化挑战。现有的工业方法通常采用两阶段训练范式，多模态学习和推荐目标之间存在错位，并且无法动态适应新数据。

Method: 提出 LEMUR，一种端到端训练的大规模多模态推荐系统，通过联合优化多模态和推荐组件，确保与下游目标更紧密地对齐，同时支持实时参数更新。采用了一种新颖的记忆库机制，以增量方式累积整个训练过程中的历史多模态表示，从而缓解计算瓶颈。

Result: 在抖音搜索中部署一个月后，LEMUR 使查询变化率衰减降低了 0.843%，QAUC 提高了 0.81%。此外，LEMUR 在抖音广告的关键离线指标方面也显示出显着提升。

Conclusion: 实验结果验证了端到端多模态推荐在真实工业场景中的优越性。

Abstract: Traditional ID-based recommender systems often struggle with cold-start and generalization challenges. Multimodal recommendation systems, which leverage textual and visual data, offer a promising solution to mitigate these issues. However, existing industrial approaches typically adopt a two-stage training paradigm: first pretraining a multimodal model, then applying its frozen representations to train the recommendation model. This decoupled framework suffers from misalignment between multimodal learning and recommendation objectives, as well as an inability to adapt dynamically to new data. To address these limitations, we propose LEMUR, the first large-scale multimodal recommender system trained end-to-end from raw data. By jointly optimizing both the multimodal and recommendation components, LEMUR ensures tighter alignment with downstream objectives while enabling real-time parameter updates. Constructing multimodal sequential representations from user history often entails prohibitively high computational costs. To alleviate this bottleneck, we propose a novel memory bank mechanism that incrementally accumulates historical multimodal representations throughout the training process. After one month of deployment in Douyin Search, LEMUR has led to a 0.843% reduction in query change rate decay and a 0.81% improvement in QAUC. Additionally, LEMUR has shown significant gains across key offline metrics for Douyin Advertisement. Our results validate the superiority of end-to-end multimodal recommendation in real-world industrial scenarios.

</details>


### [140] [GovScape: A Public Multimodal Search System for 70 Million Pages of Government PDFs](https://arxiv.org/abs/2511.11010)
*Kyle Deeds,Ying-Hsiang Huang,Claire Gong,Shreya Shaji,Alison Yan,Leslie Harka,Samuel J Klein,Shannon Zejiang Shen,Mark Phillips,Trevor Owens,Benjamin Charles Germain Lee*

Main category: cs.IR

TL;DR: 这篇论文介绍了一个名为 GovScape 的公共搜索系统，它支持对来自 2020 年联邦政府网站的 1000 多万份 PDF 文件进行多模态搜索。


<details>
  <summary>Details</summary>
Motivation: 现有的网络档案在访问和可发现性方面存在重大挑战，例如，浏览 End of Term PDF 的当前功能仅限于下载和浏览单个 PDF，以及对它们执行基本的关键词搜索。

Method: GovScape 支持四种主要的搜索形式：(1) 元数据方面的过滤条件，包括域和抓取日期；(2) 对 PDF 文本进行精确文本搜索；(3) 语义文本搜索；(4) 对 PDF 进行跨单个页面的视觉搜索。

Result: GovScape 的预处理管道对 1000 万份 PDF 的总估计计算成本约为 1500 美元，相当于每美元花费在计算上的 47000 个 PDF 页面，证明了其具有立即扩展的潜力。

Conclusion: 该论文概述了已经开始采取的步骤，以实现 1 亿+ PDF 规模的多模态搜索。

Abstract: Efforts over the past three decades have produced web archives containing billions of webpage snapshots and petabytes of data. The End of Term Web Archive alone contains, among other file types, millions of PDFs produced by the federal government. While preservation with web archives has been successful, significant challenges for access and discoverability remain. For example, current affordances for browsing the End of Term PDFs are limited to downloading and browsing individual PDFs, as well as performing basic keyword search across them. In this paper, we introduce GovScape, a public search system that supports multimodal searches across 10,015,993 federal government PDFs from the 2020 End of Term crawl (70,958,487 total PDF pages) - to our knowledge, all renderable PDFs in the 2020 crawl that are 50 pages or under. GovScape supports four primary forms of search over these 10 million PDFs: in addition to providing (1) filter conditions over metadata facets including domain and crawl date and (2) exact text search against the PDF text, we provide (3) semantic text search and (4) visual search against the PDFs across individual pages, enabling users to structure queries such as "redacted documents" or "pie charts." We detail the constituent components of GovScape, including the search affordances, embedding pipeline, system architecture, and open source codebase. Significantly, the total estimated compute cost for GovScape's pre-processing pipeline for 10 million PDFs was approximately $1,500, equivalent to 47,000 PDF pages per dollar spent on compute, demonstrating the potential for immediate scalability. Accordingly, we outline steps that we have already begun pursuing toward multimodal search at the 100+ million PDF scale. GovScape can be found at https://www.govscape.net.

</details>


### [141] [Enhancing Group Recommendation using Soft Impute Singular Value Decomposition](https://arxiv.org/abs/2511.11172)
*Mubaraka Sani Ibrahim,Isah Charles Saidu,Lehel Csato*

Main category: cs.IR

TL;DR: 提出了Group Soft-Impute SVD，以增强小组推荐。


<details>
  <summary>Details</summary>
Motivation: 由于数据的稀疏性和高维性，现有的小组推荐系统难以在实际应用中表现良好。

Method: 利用软性插补奇异值分解来进行低秩矩阵补全。

Result: 在小型用户组的回忆率方面优于基线方法，并在所有组大小的情况下实现了可比较的结果。该方法恢复的矩阵秩低于基线方法。

Conclusion: Group Soft-Impute SVD 在处理高维数据方面有效。

Abstract: The growing popularity of group activities increased the need to develop methods for providing recommendations to a group of users based on the collective preferences of the group members. Several group recommender systems have been proposed, but these methods often struggle due to sparsity and high-dimensionality of the available data, common in many real-world applications. In this paper, we propose a group recommender system called Group Soft-Impute SVD, which leverages soft-impute singular value decomposition to enhance group recommendations. This approach addresses the challenge of sparse high-dimensional data using low-rank matrix completion. We compared the performance of Group Soft-Impute SVD with Group MF based approaches and found that our method outperforms the baselines in recall for small user groups while achieving comparable results across all group sizes when tasked on Goodbooks, Movielens, and Synthetic datasets. Furthermore, our method recovers lower matrix ranks than the baselines, demonstrating its effectiveness in handling high-dimensional data.

</details>


### [142] [Align$^3$GR: Unified Multi-Level Alignment for LLM-based Generative Recommendation](https://arxiv.org/abs/2511.11255)
*Wencai Ye,Mingjie Sun,Shuhang Chen,Wenjin Wu,Peng Jiang*

Main category: cs.IR

TL;DR: 提出Align$^3$GR框架，以弥合LLM在推荐系统中语义和行为上的不对齐。


<details>
  <summary>Details</summary>
Motivation: LLM在利用结构化世界知识和多步推理能力方面表现出显著优势，但将其转化为实际推荐系统时，会出现语义和行为不对齐的根本挑战。

Method: 该框架统一了token级别、行为建模级别和偏好级别的对齐。引入了：融合用户-物品语义和协同信号的双重标记化；具有双向语义对齐的增强行为建模；结合自博弈（SP-DPO）和真实反馈（RF-DPO）的渐进式DPO策略，用于动态偏好调整。

Result: 在公共数据集上，Align$^3$GR的Recall@10和NDCG@10分别超过SOTA基线+17.8%和+20.2%，在线A/B测试和工业大规模推荐平台上的全面部署也取得了显著收益。

Conclusion: Align$^3$GR框架能够有效提升LLM在推荐系统中的性能，并在实际应用中取得了显著效果。

Abstract: Large Language Models (LLMs) demonstrate significant advantages in leveraging structured world knowledge and multi-step reasoning capabilities. However, fundamental challenges arise when transforming LLMs into real-world recommender systems due to semantic and behavioral misalignment. To bridge this gap, we propose Align$^3$GR, a novel framework that unifies token-level, behavior modeling-level, and preference-level alignment. Our approach introduces: Dual tokenization fusing user-item semantic and collaborative signals. Enhanced behavior modeling with bidirectional semantic alignment. Progressive DPO strategy combining self-play (SP-DPO) and real-world feedback (RF-DPO) for dynamic preference adaptation. Experiments show Align$^3$GR outperforms the SOTA baseline by +17.8% in Recall@10 and +20.2% in NDCG@10 on the public dataset, with significant gains in online A/B tests and full-scale deployment on an industrial large-scale recommendation platform.

</details>


### [143] [MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising](https://arxiv.org/abs/2511.11305)
*Chenghan Fu,Daoze Zhang,Yukang Lin,Zhanheng Nie,Xiang Zhang,Jianyu Liu,Yueran Liu,Wanxian Guan,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.IR

TL;DR: MOON: A set of sustainable iterative practices for multimodal representation learning in e-commerce, deployed in Taobao search advertising, improving CTR by 20%.


<details>
  <summary>Details</summary>
Motivation: Improve multimodal representation learning for e-commerce applications and bridge the misalignment between the objectives of multimodal representation learning and downstream training.

Method: A three-stage training paradigm of Pretraining, Post-training, and Application. Defining the exchange rate to quantify how effectively improvements in an intermediate metric can translate into downstream gains.

Result: Achieves a +20.00% online CTR improvement on click-through rate (CTR) prediction task.

Conclusion: MOON has evolved along four critical dimensions: data processing, training strategy, model architecture, and downstream application. Systematic study of the scaling laws governing multimodal representation learning, examining multiple factors such as the number of training tokens, negative samples, and the length of user behavior sequences.

Abstract: We introduce MOON, our comprehensive set of sustainable iterative practices for multimodal representation learning for e-commerce applications. MOON has already been fully deployed across all stages of Taobao search advertising system, including retrieval, relevance, ranking, and so on. The performance gains are particularly significant on click-through rate (CTR) prediction task, which achieves an overall +20.00% online CTR improvement. Over the past three years, this project has delivered the largest improvement on CTR prediction task and undergone five full-scale iterations. Throughout the exploration and iteration of our MOON, we have accumulated valuable insights and practical experience that we believe will benefit the research community. MOON contains a three-stage training paradigm of "Pretraining, Post-training, and Application", allowing effective integration of multimodal representations with downstream tasks. Notably, to bridge the misalignment between the objectives of multimodal representation learning and downstream training, we define the exchange rate to quantify how effectively improvements in an intermediate metric can translate into downstream gains. Through this analysis, we identify the image-based search recall as a critical intermediate metric guiding the optimization of multimodal models. Over three years and five iterations, MOON has evolved along four critical dimensions: data processing, training strategy, model architecture, and downstream application. The lessons and insights gained through the iterative improvements will also be shared. As part of our exploration into scaling effects in the e-commerce field, we further conduct a systematic study of the scaling laws governing multimodal representation learning, examining multiple factors such as the number of training tokens, negative samples, and the length of user behavior sequences.

</details>


### [144] [SRLF: An Agent-Driven Set-Wise Reflective Learning Framework for Sequential Recommendation](https://arxiv.org/abs/2511.11370)
*Jiahao Wang,Bokang Fu,Yu Zhu,Yuli Liu*

Main category: cs.IR

TL;DR: 本文提出了一种新的基于LLM的智能体框架，用于模拟用户行为以增强推荐系统。


<details>
  <summary>Details</summary>
Motivation: 现有的研究侧重于对单个项目的用户评分进行建模，导致用户偏好理解不准确和项目语义表示僵化等问题。

Method: 该文提出了集合式反思学习框架（SRLF），通过整体判断整个项目集合，综合分析集合内项目之间的复杂相互关系及其与用户偏好配置文件的集体一致性。

Result: 大量实验验证了该方法的有效性，并证实了这种集合式视角对于在序列推荐任务中实现最先进的性能至关重要。

Conclusion: 集合式视角对于在序列推荐任务中实现最先进的性能至关重要。

Abstract: LLM-based agents are emerging as a promising paradigm for simulating user behavior to enhance recommender systems. However, their effectiveness is often limited by existing studies that focus on modeling user ratings for individual items. This point-wise approach leads to prevalent issues such as inaccurate user preference comprehension and rigid item-semantic representations.
  To address these limitations, we propose the novel Set-wise Reflective Learning Framework (SRLF). Our framework operationalizes a closed-loop "assess-validate-reflect" cycle that harnesses the powerful in-context learning capabilities of LLMs. SRLF departs from conventional point-wise assessment by formulating a holistic judgment on an entire set of items. It accomplishes this by comprehensively analyzing both the intricate interrelationships among items within the set and their collective alignment with the user's preference profile. This method of set-level contextual understanding allows our model to capture complex relational patterns essential to user behavior, making it significantly more adept for sequential recommendation. Extensive experiments validate our approach, confirming that this set-wise perspective is crucial for achieving state-of-the-art performance in sequential recommendation tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [145] [LAD-BNet: Lag-Aware Dual-Branch Networks for Real-Time Energy Forecasting on Edge Devices](https://arxiv.org/abs/2511.10680)
*Jean-Philippe Lignier*

Main category: cs.LG

TL;DR: LAD-BNet: A neural network for real-time energy forecasting on edge devices.


<details>
  <summary>Details</summary>
Motivation: Real-time energy forecasting on edge devices is challenging but important for smart grids and buildings.

Method: A hybrid neural network (LAD-BNet) combining temporal lag exploitation with a Temporal Convolutional Network (TCN).

Result: Achieves 14.49% MAPE at 1-hour horizon with 18ms inference time on Edge TPU, outperforming LSTM and TCN baselines. Model size is 180MB.

Conclusion: LAD-BNet enables real-time energy optimization, demand management, and operational planning in industrial applications.

Abstract: Real-time energy forecasting on edge devices represents a major challenge for smart grid optimization and intelligent buildings. We present LAD-BNet (Lag-Aware Dual-Branch Network), an innovative neural architecture optimized for edge inference with Google Coral TPU. Our hybrid approach combines a branch dedicated to explicit exploitation of temporal lags with a Temporal Convolutional Network (TCN) featuring dilated convolutions, enabling simultaneous capture of short and long-term dependencies. Tested on real energy consumption data with 10-minute temporal resolution, LAD-BNet achieves 14.49% MAPE at 1-hour horizon with only 18ms inference time on Edge TPU, representing an 8-12 x acceleration compared to CPU. The multi-scale architecture enables predictions up to 12 hours with controlled performance degradation. Our model demonstrates a 2.39% improvement over LSTM baselines and 3.04% over pure TCN architectures, while maintaining a 180MB memory footprint suitable for embedded device constraints. These results pave the way for industrial applications in real-time energy optimization, demand management, and operational planning.

</details>


### [146] [LT-Soups: Bridging Head and Tail Classes via Subsampled Model Soups](https://arxiv.org/abs/2511.10683)
*Masih Aminbeidokhti,Subhankar Roy,Eric Granger,Elisa Ricci,Marco Pedersoli*

Main category: cs.LG

TL;DR: Parameter-efficient fine-tuning (PEFT) methods like LoRA and AdaptFormer preserve tail-class performance on foundation models but hurt head-class accuracy. The head-tail ratio influences this trade-off.


<details>
  <summary>Details</summary>
Motivation: PEFT methods have limitations in handling long-tailed distributions, especially when the head-tail ratio is not heavily skewed towards the tail.

Method: A two-stage model soups framework called LT-Soups is proposed. It averages models fine-tuned on balanced subsets and then fine-tunes the classifier on the full dataset.

Result: LT-Soups achieves superior trade-offs compared to both PEFT and traditional model soups across a wide range of imbalance regimes on six benchmark datasets.

Conclusion: LT-Soups effectively addresses the challenges posed by long-tailed distributions and improves performance across various imbalance regimes.

Abstract: Real-world datasets typically exhibit long-tailed (LT) distributions, where a few head classes dominate and many tail classes are severely underrepresented. While recent work shows that parameter-efficient fine-tuning (PEFT) methods like LoRA and AdaptFormer preserve tail-class performance on foundation models such as CLIP, we find that they do so at the cost of head-class accuracy. We identify the head-tail ratio, the proportion of head to tail classes, as a crucial but overlooked factor influencing this trade-off. Through controlled experiments on CIFAR100 with varying imbalance ratio ($ρ$) and head-tail ratio ($η$), we show that PEFT excels in tail-heavy scenarios but degrades in more balanced and head-heavy distributions. To overcome these limitations, we propose LT-Soups, a two-stage model soups framework designed to generalize across diverse LT regimes. In the first stage, LT-Soups averages models fine-tuned on balanced subsets to reduce head-class bias; in the second, it fine-tunes only the classifier on the full dataset to restore head-class accuracy. Experiments across six benchmark datasets show that LT-Soups achieves superior trade-offs compared to both PEFT and traditional model soups across a wide range of imbalance regimes.

</details>


### [147] [How Data Quality Affects Machine Learning Models for Credit Risk Assessment](https://arxiv.org/abs/2511.10964)
*Andrea Maurino*

Main category: cs.LG

TL;DR: 该论文研究了数据质量问题对信用风险评估中机器学习模型预测准确性的影响，并提供了一个评估和增强数据管道鲁棒性的框架。


<details>
  <summary>Details</summary>
Motivation: 信用风险评估越来越多地使用机器学习模型，其有效性主要取决于输入数据的质量。本文旨在研究数据质量问题对信用风险评估中机器学习模型预测准确性的影响。

Method: 使用Pucktrick库引入受控的数据损坏，以评估10个常用模型（如随机森林、SVM和逻辑回归等）的鲁棒性。

Result: 实验表明，模型鲁棒性的差异显著取决于数据退化的性质和严重程度。

Conclusion: 该方法和工具为从业者增强数据管道的鲁棒性提供了实践支持，并为研究人员在以数据为中心的人工智能环境中进行进一步实验提供了灵活的框架。

Abstract: Machine Learning (ML) models are being increasingly employed for credit risk evaluation, with their effectiveness largely hinging on the quality of the input data. In this paper we investigate the impact of several data quality issues, including missing values, noisy attributes, outliers, and label errors, on the predictive accuracy of the machine learning model used in credit risk assessment. Utilizing an open-source dataset, we introduce controlled data corruption using the Pucktrick library to assess the robustness of 10 frequently used models like Random Forest, SVM, and Logistic Regression and so on. Our experiments show significant differences in model robustness based on the nature and severity of the data degradation. Moreover, the proposed methodology and accompanying tools offer practical support for practitioners seeking to enhance data pipeline robustness, and provide researchers with a flexible framework for further experimentation in data-centric AI contexts.

</details>


### [148] [Differentiable Sparse Identification of Lagrangian Dynamics](https://arxiv.org/abs/2511.10706)
*Zitong Zhang,Hao Sun*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的可微稀疏辨识框架，用于从数据中发现控制方程，特别是在复杂的机械系统中。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏回归技术在处理有理函数和噪声敏感性方面存在困难；现有的拉格朗日辨识方法受到测量噪声和有限数据可用性的影响。

Method: 该方法结合了三次B样条近似、稳健的方程发现机制和递归导数计算方案。

Result: 该方法在从噪声数据中提取物理定律方面表现出卓越的性能，尤其是在复杂的机械系统中。

Conclusion: 该方法能够更准确、更可靠地提取物理定律。

Abstract: Data-driven discovery of governing equations from data remains a fundamental challenge in nonlinear dynamics. Although sparse regression techniques have advanced system identification, they struggle with rational functions and noise sensitivity in complex mechanical systems. The Lagrangian formalism offers a promising alternative, as it typically avoids rational expressions and provides a more concise representation of system dynamics. However, existing Lagrangian identification methods are significantly affected by measurement noise and limited data availability. This paper presents a novel differentiable sparse identification framework that addresses these limitations through three key contributions: (1) the first integration of cubic B-Spline approximation into Lagrangian system identification, enabling accurate representation of complex nonlinearities, (2) a robust equation discovery mechanism that effectively utilizes measurements while incorporating known physical constraints, (3) a recursive derivative computation scheme based on B-spline basis functions, effectively constraining higher-order derivatives and reducing noise sensitivity on second-order dynamical systems. The proposed method demonstrates superior performance and enables more accurate and reliable extraction of physical laws from noisy data, particularly in complex mechanical systems compared to baseline methods.

</details>


### [149] [Bias-Restrained Prefix Representation Finetuning for Mathematical Reasoning](https://arxiv.org/abs/2511.10707)
*Sirui Liang,Pengfei Cao,Jian Zhao,Cong Huang,Jun Zhao,Kang Liu*

Main category: cs.LG

TL;DR: 提出了一种名为Bias-REstrained Prefix Representation FineTuning (BREP ReFT) 的新方法，通过优化初始推理前缀的生成、在早期推理阶段进行干预以防止误差累积、并约束干预向量的幅度来提高ReFT在数学推理任务上的能力。


<details>
  <summary>Details</summary>
Motivation: ReFT在数学推理任务上表现出显著的性能下降。

Method: 提出 Bias-REstrained Prefix Representation FineTuning (BREP ReFT)，通过截断训练数据来优化初始推理前缀的生成，干预早期推理阶段以防止误差累积，并约束干预向量的幅度以避免干扰数值编码。

Result: BREP在各种模型架构上表现出卓越的有效性、效率和鲁棒的泛化能力，优于标准ReFT和基于权重的PEFT方法。

Conclusion: BREP ReFT 提高了 ReFT 在数学推理任务上的能力

Abstract: Parameter-Efficient finetuning (PEFT) enhances model performance on downstream tasks by updating a minimal subset of parameters. Representation finetuning (ReFT) methods further improve efficiency by freezing model weights and optimizing internal representations with fewer parameters than PEFT, outperforming PEFT on several tasks. However, ReFT exhibits a significant performance decline on mathematical reasoning tasks. To address this problem, the paper demonstrates that ReFT's poor performance on mathematical tasks primarily stems from its struggle to generate effective reasoning prefixes during the early inference phase. Moreover, ReFT disturbs the numerical encoding and the error accumulats during the CoT stage. Based on these observations, this paper proposes Bias-REstrained Prefix Representation FineTuning (BREP ReFT), which enhances ReFT's mathematical reasoning capability by truncating training data to optimize the generation of initial reasoning prefixes, intervening on the early inference stage to prevent error accumulation, and constraining the intervention vectors' magnitude to avoid disturbing numerical encoding. Extensive experiments across diverse model architectures demonstrate BREP's superior effectiveness, efficiency, and robust generalization capability, outperforming both standard ReFT and weight-based PEFT methods on the task of mathematical reasoning. The source code is available at https://github.com/LiangThree/BREP.

</details>


### [150] [Towards Uncertainty Quantification in Generative Model Learning](https://arxiv.org/abs/2511.10710)
*Giorgio Morales,Frederic Jurie,Jalal Fadili*

Main category: cs.LG

TL;DR: 这篇论文探讨了生成模型中不确定性量化的问题，这是一个被忽视但很重要的方面。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法忽略了生成模型学习中分布近似的不确定性。

Method: 论文形式化了生成模型学习中的不确定性量化问题，并讨论了潜在的研究方向，包括使用基于集成的精确率-召回率曲线。

Result: 在合成数据集上的初步实验表明，聚合的精确率-召回率曲线可以有效捕捉模型近似的不确定性。

Conclusion: 该方法支持基于不确定性特征对不同模型架构进行系统比较。

Abstract: While generative models have become increasingly prevalent across various domains, fundamental concerns regarding their reliability persist. A crucial yet understudied aspect of these models is the uncertainty quantification surrounding their distribution approximation capabilities. Current evaluation methodologies focus predominantly on measuring the closeness between the learned and the target distributions, neglecting the inherent uncertainty in these measurements. In this position paper, we formalize the problem of uncertainty quantification in generative model learning. We discuss potential research directions, including the use of ensemble-based precision-recall curves. Our preliminary experiments on synthetic datasets demonstrate the effectiveness of aggregated precision-recall curves in capturing model approximation uncertainty, enabling systematic comparison among different model architectures based on their uncertainty characteristics.

</details>


### [151] [Movement-Specific Analysis for FIM Score Classification Using Spatio-Temporal Deep Learning](https://arxiv.org/abs/2511.10713)
*Jun Masaki,Ariaki Higashi,Naoko Shinagawa,Kazuhiko Hirata,Yuichi Kurita,Akira Furui*

Main category: cs.LG

TL;DR: 提出了一种自动化的FIM评分估计方法，利用简单的运动来评估患者的身体独立性。


<details>
  <summary>Details</summary>
Motivation: 传统的FIM评估对患者和医护人员都造成了很大的负担。

Method: 采用深度神经网络结构，整合了空间-时间图卷积网络(ST-GCN)、双向长短期记忆(BiLSTM)和注意力机制来估计FIM运动项目分数。

Result: 在277名康复患者的研究中，我们的方法成功区分了完全独立的患者和需要帮助的患者，在不同的FIM项目中实现了70.09-78.79%的平衡准确率。

Conclusion: 我们的分析揭示了特定的运动模式，这些模式可以作为特定FIM评估项目的可靠预测指标。

Abstract: The functional independence measure (FIM) is widely used to evaluate patients' physical independence in activities of daily living. However, traditional FIM assessment imposes a significant burden on both patients and healthcare professionals. To address this challenge, we propose an automated FIM score estimation method that utilizes simple exercises different from the designated FIM assessment actions. Our approach employs a deep neural network architecture integrating a spatial-temporal graph convolutional network (ST-GCN), bidirectional long short-term memory (BiLSTM), and an attention mechanism to estimate FIM motor item scores. The model effectively captures long-term temporal dependencies and identifies key body-joint contributions through learned attention weights. We evaluated our method in a study of 277 rehabilitation patients, focusing on FIM transfer and locomotion items. Our approach successfully distinguishes between completely independent patients and those requiring assistance, achieving balanced accuracies of 70.09-78.79 % across different FIM items. Additionally, our analysis reveals specific movement patterns that serve as reliable predictors for particular FIM evaluation items.

</details>


### [152] [Fast Neural Tangent Kernel Alignment, Norm and Effective Rank via Trace Estimation](https://arxiv.org/abs/2511.10796)
*James Hazelden*

Main category: cs.LG

TL;DR: 提出了一种矩阵无关的视角，使用迹估计来快速分析经验有限宽度的 NTK。


<details>
  <summary>Details</summary>
Motivation: 计算完整的 NTK 矩阵通常是不可行的，特别是对于循环架构。

Method: 使用 Hutch++ 迹估计器，并提供具有可证明的快速收敛保证的数值方法。此外，展示了由于 NTK 的结构，可以使用仅前向或反向模式自动微分来计算迹，而不需要两种模式。

Result: 结果表明，所谓的单边估计器在低样本状态下可以优于 Hutch++，特别是当模型状态和参数数量之间的差距很大时。

Conclusion: 总的来说，结果表明，矩阵无关的随机方法可以产生多个数量级的加速，从而更快地分析和应用 NTK。

Abstract: The Neural Tangent Kernel (NTK) characterizes how a model's state evolves over Gradient Descent. Computing the full NTK matrix is often infeasible, especially for recurrent architectures. Here, we introduce a matrix-free perspective, using trace estimation to rapidly analyze the empirical, finite-width NTK. This enables fast computation of the NTK's trace, Frobenius norm, effective rank, and alignment. We provide numerical recipes based on the Hutch++ trace estimator with provably fast convergence guarantees. In addition, we show that, due to the structure of the NTK, one can compute the trace using only forward- or reverse-mode automatic differentiation, not requiring both modes. We show these so-called one-sided estimators can outperform Hutch++ in the low-sample regime, especially when the gap between the model state and parameter count is large. In total, our results demonstrate that matrix-free randomized approaches can yield speedups of many orders of magnitude, leading to faster analysis and applications of the NTK.

</details>


### [153] [Near-optimal Linear Predictive Clustering in Non-separable Spaces via Mixed Integer Programming and Quadratic Pseudo-Boolean Reductions](https://arxiv.org/abs/2511.10809)
*Jiazhou Liang,Hassan Khurram,Scott Sanner*

Main category: cs.LG

TL;DR: 提出两种新的线性预测聚类（LPC）全局优化方法，以提高效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 贪婪优化方法在LPC中常用，但缺乏全局最优性，在非分离数据集中表现不佳。Bertsimas和Shioda（2007）将LPC建模为混合整数规划（MIP），保证了全局最优性，但可扩展性差。

Method: 1. 利用可分离性的关键理论性质，推导出具有可证明误差界限的近似最优解，显著降低了MIP公式的复杂性并提高了可扩展性。
2. 将LPC近似为二次伪布尔优化（QPBO）问题，在某些情况下实现了显著的计算改进。

Result: 在合成和真实数据集上的比较分析表明，该方法始终能以远低于贪婪优化的回归误差获得接近最优的解，同时比现有的MIP公式表现出更好的可扩展性。

Conclusion: 该研究提出的新方法在LPC全局优化中，能够在保证解的质量的同时，显著提高计算效率和可扩展性。

Abstract: Linear Predictive Clustering (LPC) partitions samples based on shared linear relationships between feature and target variables, with numerous applications including marketing, medicine, and education. Greedy optimization methods, commonly used for LPC, alternate between clustering and linear regression but lack global optimality. While effective for separable clusters, they struggle in non-separable settings where clusters overlap in feature space. In an alternative constrained optimization paradigm, Bertsimas and Shioda (2007) formulated LPC as a Mixed-Integer Program (MIP), ensuring global optimality regardless of separability but suffering from poor scalability. This work builds on the constrained optimization paradigm to introduce two novel approaches that improve the efficiency of global optimization for LPC. By leveraging key theoretical properties of separability, we derive near-optimal approximations with provable error bounds, significantly reducing the MIP formulation's complexity and improving scalability. Additionally, we can further approximate LPC as a Quadratic Pseudo-Boolean Optimization (QPBO) problem, achieving substantial computational improvements in some settings. Comparative analyses on synthetic and real-world datasets demonstrate that our methods consistently achieve near-optimal solutions with substantially lower regression errors than greedy optimization while exhibiting superior scalability over existing MIP formulations.

</details>


### [154] [Transformers know more than they can tell -- Learning the Collatz sequence](https://arxiv.org/abs/2511.10811)
*François Charton,Ashvni Narayanan*

Main category: cs.LG

TL;DR: 该论文研究了 Transformer 模型预测 Collatz 步长的能力，发现模型准确率受编码输入和输出的进制影响，但所有模型都遵循共同的学习模式，即学习具有相同模 2^p 余数的输入类别。模型在这些类别上达到近乎完美的准确率，而在其他输入上则低于 1%。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型学习复杂算术函数的能力，并为理解、解释和改进语言模型提供新方法。

Method: 使用 Transformer 模型预测 Collatz 步长，并分析模型在不同进制下的准确率和学习模式。

Result: 模型准确率受进制影响，但都遵循共同的学习模式，即学习具有相同模 2^p 余数的输入类别。模型错误主要在于错误估计循环长度。

Conclusion: 学习复杂算术函数的难点在于理解计算的控制结构，即循环长度。将数学问题作为理解、解释和改进语言模型的工具，可以应用于广泛的问题并取得成果。

Abstract: We investigate transformer prediction of long Collatz steps, a complex arithmetic function that maps odd integers to their distant successors in the Collatz sequence ( $u_{n+1}=u_n/2$ if $u_n$ is even, $u_{n+1}=(3u_n+1)/2$ if $u_n$ is odd). Model accuracy varies with the base used to encode input and output. It can be as high as $99.7\%$ for bases $24$ and $32$, and as low as $37$ and $25\%$ for bases $11$ and $3$. Yet, all models, no matter the base, follow a common learning pattern. As training proceeds, they learn a sequence of classes of inputs that share the same residual modulo $2^p$. Models achieve near-perfect accuracy on these classes, and less than $1\%$ for all other inputs. This maps to a mathematical property of Collatz sequences: the length of the loops involved in the computation of a long Collatz step can be deduced from the binary representation of its input. The learning pattern reflects the model learning to predict inputs associated with increasing loop lengths. An analysis of failure cases reveals that almost all model errors follow predictable patterns. Hallucination, a common feature of large language models, almost never happens. In over $90\%$ of failures, the model performs the correct calculation, but wrongly estimates loop lengths. Our observations give a full account of the algorithms learned by the models. They suggest that the difficulty of learning such complex arithmetic function lies in figuring the control structure of the computation -- the length of the loops. We believe that the approach outlined here, using mathematical problems as tools for understanding, explaining, and perhaps improving language models, can be applied to a broad range of problems and bear fruitful results.

</details>


### [155] [Towards Universal Neural Operators through Multiphysics Pretraining](https://arxiv.org/abs/2511.10829)
*Mikhail Masliaev,Dmitry Gusarov,Ilya Markov,Alexander Hvatov*

Main category: cs.LG

TL;DR: Transformer-based neural operators can transfer knowledge across PDE problems.


<details>
  <summary>Details</summary>
Motivation: Training neural operators is computationally expensive, and downstream learning can address this.

Method: The researchers investigated transformer-based neural operators in a general transfer learning setting and evaluated their performance across diverse PDE problems.

Result: Advanced neural operator architectures can effectively transfer knowledge across PDE problems.

Conclusion: Transformer-based neural operators are effective in transfer learning for PDE problems.

Abstract: Although neural operators are widely used in data-driven physical simulations, their training remains computationally expensive. Recent advances address this issue via downstream learning, where a model pretrained on simpler problems is fine-tuned on more complex ones. In this research, we investigate transformer-based neural operators, which have previously been applied only to specific problems, in a more general transfer learning setting. We evaluate their performance across diverse PDE problems, including extrapolation to unseen parameters, incorporation of new variables, and transfer from multi-equation datasets. Our results demonstrate that advanced neural operator architectures can effectively transfer knowledge across PDE problems.

</details>


### [156] [Benchmarking Quantum Kernels Across Diverse and Complex Data](https://arxiv.org/abs/2511.10831)
*Yuhan Jiang,Matthew Otten*

Main category: cs.LG

TL;DR: 量子核方法在各种高维真实世界数据上的实际优势尚未得到验证，当前的研究主要局限于低维或合成数据集。


<details>
  <summary>Details</summary>
Motivation: 为了弥补这一空白，我们开发了一个变分量子核框架，利用资源高效的ansätze进行复杂的分类任务，并引入了一种参数缩放技术来加速收敛。

Method: 在八个具有挑战性的真实世界和高维数据集上对该框架进行了全面的基准测试，这些数据集涵盖表格、图像、时间序列和图数据。

Result: 我们的经典模拟结果表明，所提出的量子核在标准经典核（如径向基函数（RBF）核）上表现出明显的性能优势。

Conclusion: 这项工作表明，适当设计的量子核可以作为通用的高性能工具，为真实世界机器学习中的量子增强应用奠定基础。

Abstract: Quantum kernel methods are a promising branch of quantum machine learning, yet their practical advantage on diverse, high-dimensional, real-world data remains unverified. Current research has largely been limited to low-dimensional or synthetic datasets, preventing a thorough evaluation of their potential. To address this gap, we developed a variational quantum kernel framework utilizing resource-efficient ansätze for complex classification tasks and introduced a parameter scaling technique to accelerate convergence. We conducted a comprehensive benchmark of this framework on eight challenging, real world and high-dimensional datasets covering tabular, image, time series, and graph data. Our classically simulated results show that the proposed quantum kernel demonstrated a clear performance advantage over standard classical kernels, such as the radial basis function (RBF) kernel. This work demonstrates that properly designed quantum kernels can function as versatile, high-performance tools, laying a foundation for quantum-enhanced applications in real-world machine learning. Further research is needed to fully assess the practical quantum advantage.

</details>


### [157] [SURFACEBENCH: Can Self-Evolving LLMs Find the Equations of 3D Scientific Surfaces?](https://arxiv.org/abs/2511.10833)
*Sanchit Kabra,Shobhnik Kriplani,Parshin Shojaee,Chandan K. Reddy*

Main category: cs.LG

TL;DR: 这篇论文介绍了一个新的符号表面发现的综合基准测试SurfaceBench，用于评估大型语言模型在从数据中恢复简洁的符号表达式方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的符号回归方法依赖于记忆公式或过于简化的函数形式，且现有基准测试侧重于标量函数，忽略领域基础，并依赖于脆弱的字符串匹配指标，无法捕捉科学等价性。

Method: SurfaceBench包含183个任务，涵盖15个类别的符号复杂性，跨越显式、隐式和参数方程表示形式。每个任务包括地面实况方程、变量语义和合成采样的三维数据。使用符号检查和几何感知指标（如Chamfer和Hausdorff距离）来评估方程发现质量。

Result: 实验表明，最先进的框架在特定系列上偶尔成功，但难以推广到各种表示类型和表面复杂性。

Conclusion: SurfaceBench建立了一个具有挑战性和诊断性的测试平台，将符号推理与几何重建联系起来，从而可以对组合泛化、数据驱动的科学归纳和具有LLM的几何感知推理方面的进展进行原则性基准测试。

Abstract: Equation discovery from data is a core challenge in machine learning for science, requiring the recovery of concise symbolic expressions that govern complex physical and geometric phenomena. Recent approaches with large language models (LLMs) show promise in symbolic regression, but their success often hinges on memorized formulas or overly simplified functional forms. Existing benchmarks exacerbate this limitation: they focus on scalar functions, ignore domain grounding, and rely on brittle string-matching based metrics that fail to capture scientific equivalence. We introduce SurfaceBench, first comprehensive benchmark for symbolic surface discovery. SurfaceBench comprises 183 tasks across 15 categories of symbolic complexity, spanning explicit, implicit, and parametric equation representation forms. Each task includes ground-truth equations, variable semantics, and synthetically sampled three dimensional data. Unlike prior SR datasets, our tasks reflect surface-level structure, resist LLM memorization through novel symbolic compositions, and are grounded in scientific domains such as fluid dynamics, robotics, electromagnetics, and geometry. To evaluate equation discovery quality, we pair symbolic checks with geometry-aware metrics such as Chamfer and Hausdorff distances, capturing both algebraic fidelity and spatial reconstruction accuracy. Our experiments reveal that state-of-the-art frameworks, while occasionally successful on specific families, struggle to generalize across representation types and surface complexities. SurfaceBench thus establishes a challenging and diagnostic testbed that bridges symbolic reasoning with geometric reconstruction, enabling principled benchmarking of progress in compositional generalization, data-driven scientific induction, and geometry-aware reasoning with LLMs. We release the code here: https://github.com/Sanchit-404/surfacebench

</details>


### [158] [EarthSight: A Distributed Framework for Low-Latency Satellite Intelligence](https://arxiv.org/abs/2511.10834)
*Ansel Kaplan Erol,Seungjun Lee,Divya Mahajan*

Main category: cs.LG

TL;DR: EarthSight是一个分布式运行时框架，旨在通过在卫星上进行多任务推理、地面站查询调度和动态过滤器排序，实现可扩展的低延迟卫星图像分析。


<details>
  <summary>Details</summary>
Motivation: 传统卫星图像传输和分析流程存在延迟，新兴的在轨机器学习系统又缺乏可扩展性和效率，并且存在冗余推理的问题。

Method: EarthSight 引入了三个核心创新：(1) 使用共享骨干网络在卫星上进行多任务推理；(2) 地面站查询调度器，用于聚合用户请求、预测优先级并分配计算预算；(3) 动态过滤器排序，集成了模型选择性、准确性和执行成本，以尽早拒绝低价值图像并节约资源。

Result: EarthSight 将每个图像的平均计算时间减少了 1.9 倍，并将从首次接触到交付的第 90 个百分位端到端延迟从 51 分钟降低到 21 分钟。

Conclusion: EarthSight 利用来自地面站的全局上下文和轨道上的资源感知自适应决策，使星座能够在严格的下行链路带宽和机载功率预算内执行可扩展的低延迟图像分析。

Abstract: Low-latency delivery of satellite imagery is essential for time-critical applications such as disaster response, intelligence, and infrastructure monitoring. However, traditional pipelines rely on downlinking all captured images before analysis, introducing delays of hours to days due to restricted communication bandwidth. To address these bottlenecks, emerging systems perform onboard machine learning to prioritize which images to transmit. However, these solutions typically treat each satellite as an isolated compute node, limiting scalability and efficiency. Redundant inference across satellites and tasks further strains onboard power and compute costs, constraining mission scope and responsiveness. We present EarthSight, a distributed runtime framework that redefines satellite image intelligence as a distributed decision problem between orbit and ground. EarthSight introduces three core innovations: (1) multi-task inference on satellites using shared backbones to amortize computation across multiple vision tasks; (2) a ground-station query scheduler that aggregates user requests, predicts priorities, and assigns compute budgets to incoming imagery; and (3) dynamic filter ordering, which integrates model selectivity, accuracy, and execution cost to reject low-value images early and conserve resources. EarthSight leverages global context from ground stations and resource-aware adaptive decisions in orbit to enable constellations to perform scalable, low-latency image analysis within strict downlink bandwidth and onboard power budgets. Evaluations using a prior established satellite simulator show that EarthSight reduces average compute time per image by 1.9x and lowers 90th percentile end-to-end latency from first contact to delivery from 51 to 21 minutes compared to the state-of-the-art baseline.

</details>


### [159] [The Map of Misbelief: Tracing Intrinsic and Extrinsic Hallucinations Through Attention Patterns](https://arxiv.org/abs/2511.10837)
*Elyes Hajji,Aymen Bouguerra,Fabio Arnez*

Main category: cs.LG

TL;DR: 大型语言模型（LLM）容易产生幻觉，尤其是在安全攸关的领域。本文提出一种新的评估框架，区分外在和内在幻觉，并利用基于注意力的不确定性量化算法和新的注意力聚合策略来提高幻觉检测性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）虽然应用广泛，但容易产生幻觉，需要有效的检测方法。

Method: 1. 提出区分外在和内在幻觉的评估框架。2. 利用基于注意力的不确定性量化算法。3. 提出新的注意力聚合策略。

Result: 基于抽样的方法在检测外在幻觉方面有效，但在内在幻觉方面失败。本文提出的方法更适合内在幻觉。

Conclusion: 研究结果为将检测策略与幻觉的性质对齐提供了新的方向，并强调注意力是量化模型不确定性的丰富信号。

Abstract: Large Language Models (LLMs) are increasingly deployed in safety-critical domains, yet remain susceptible to hallucinations. While prior works have proposed confidence representation methods for hallucination detection, most of these approaches rely on computationally expensive sampling strategies and often disregard the distinction between hallucination types. In this work, we introduce a principled evaluation framework that differentiates between extrinsic and intrinsic hallucination categories and evaluates detection performance across a suite of curated benchmarks. In addition, we leverage a recent attention-based uncertainty quantification algorithm and propose novel attention aggregation strategies that improve both interpretability and hallucination detection performance. Our experimental findings reveal that sampling-based methods like Semantic Entropy are effective for detecting extrinsic hallucinations but generally fail on intrinsic ones. In contrast, our method, which aggregates attention over input tokens, is better suited for intrinsic hallucinations. These insights provide new directions for aligning detection strategies with the nature of hallucination and highlight attention as a rich signal for quantifying model uncertainty.

</details>


### [160] [FlowPath: Learning Data-Driven Manifolds with Invertible Flows for Robust Irregularly-sampled Time Series Classification](https://arxiv.org/abs/2511.10841)
*YongKyung Oh,Dong-Young Lim,Sungil Kim*

Main category: cs.LG

TL;DR: 提出FlowPath，通过可逆神经流学习控制路径的几何结构，提高不规则时间序列的学习效果。


<details>
  <summary>Details</summary>
Motivation: 从稀疏和不规则采样的时间序列中建模连续时间动态仍然是一个根本挑战，现有方法通常采用固定插值方案，在高缺失情况下会错误表示底层数据流形。

Method: 使用可逆神经流学习控制路径的几何结构，构建连续和数据自适应的流形，通过可逆性约束来强制执行信息保留和表现良好的转换。

Result: 在 18 个基准数据集和一个真实案例研究中，FlowPath 在分类精度方面始终优于使用固定插值器或不可逆架构的基线。

Conclusion: 对路径本身进行建模非常重要，为从不规则时间序列中学习提供了一个鲁棒且通用的解决方案。

Abstract: Modeling continuous-time dynamics from sparse and irregularly-sampled time series remains a fundamental challenge. Neural controlled differential equations provide a principled framework for such tasks, yet their performance is highly sensitive to the choice of control path constructed from discrete observations. Existing methods commonly employ fixed interpolation schemes, which impose simplistic geometric assumptions that often misrepresent the underlying data manifold, particularly under high missingness. We propose FlowPath, a novel approach that learns the geometry of the control path via an invertible neural flow. Rather than merely connecting observations, FlowPath constructs a continuous and data-adaptive manifold, guided by invertibility constraints that enforce information-preserving and well-behaved transformations. This inductive bias distinguishes FlowPath from prior unconstrained learnable path models. Empirical evaluations on 18 benchmark datasets and a real-world case study demonstrate that FlowPath consistently achieves statistically significant improvements in classification accuracy over baselines using fixed interpolants or non-invertible architectures. These results highlight the importance of modeling not only the dynamics along the path but also the geometry of the path itself, offering a robust and generalizable solution for learning from irregular time series.

</details>


### [161] [Behaviour Policy Optimization: Provably Lower Variance Return Estimates for Off-Policy Reinforcement Learning](https://arxiv.org/abs/2511.10843)
*Alexander W. Goodall,Edwin Hamel-De le Court,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的在线强化学习方法，该方法利用精心设计的行为策略来收集off-policy数据，从而实现更低方差的回报估计，提高样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法由于高方差的回报估计，存在样本效率低和训练不稳定的问题。

Method: 该论文将off-policy评估的新结果扩展到在线强化学习设置中，利用行为策略收集off-policy数据，实现更低方差的回报估计。同时，将此方法应用于两个policy-gradient方法。

Result: 实验结果表明，该方法在多个环境中表现出更好的样本效率和性能。

Conclusion: 该论文证明了使用精心设计的行为策略可以有效地降低回报估计的方差，从而提高在线强化学习的样本效率和性能。

Abstract: Many reinforcement learning algorithms, particularly those that rely on return estimates for policy improvement, can suffer from poor sample efficiency and training instability due to high-variance return estimates. In this paper we leverage new results from off-policy evaluation; it has recently been shown that well-designed behaviour policies can be used to collect off-policy data for provably lower variance return estimates. This result is surprising as it means collecting data on-policy is not variance optimal. We extend this key insight to the online reinforcement learning setting, where both policy evaluation and improvement are interleaved to learn optimal policies. Off-policy RL has been well studied (e.g., IMPALA), with correct and truncated importance weighted samples for de-biasing and managing variance appropriately. Generally these approaches are concerned with reconciling data collected from multiple workers in parallel, while the policy is updated asynchronously, mismatch between the workers and policy is corrected in a mathematically sound way. Here we consider only one worker - the behaviour policy, which is used to collect data for policy improvement, with provably lower variance return estimates. In our experiments we extend two policy-gradient methods with this regime, demonstrating better sample efficiency and performance over a diverse set of environments.

</details>


### [162] [STAMP: Spatial-Temporal Adapter with Multi-Head Pooling](https://arxiv.org/abs/2511.10848)
*Brad Shook,Abby Turner,Jieshi Chen,Michał Wiliński,Mononito Goswami,Jonathan Elmer,Artur Dubrawski*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的时空适配器（STAMP），它利用通用时间序列基础模型（TSFM）产生的单变量嵌入，隐式地模拟脑电图（EEG）数据的时空特征，并实现了与最先进的脑电图专用基础模型（EEGFM）相当的性能。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏脑电图专用基础模型（EEGFM）与通用时间序列基础模型（TSFM）在脑电图特定任务上的对比分析。

Method: 提出了一种新颖的时空适配器（STAMP），它利用通用时间序列基础模型（TSFM）产生的单变量嵌入，隐式地模拟脑电图（EEG）数据的时空特征。

Result: 在8个脑电图分类临床任务基准数据集上进行了综合分析，以及消融研究，结果表明，该适配器在可训练参数方面是轻量级的，并且在其可以容纳的输入方面是灵活的，支持使用TSFM轻松建模脑电图数据。

Conclusion: 该适配器在可训练参数方面是轻量级的，并且在其可以容纳的输入方面是灵活的，支持使用TSFM轻松建模脑电图数据，并且实现了与最先进的脑电图专用基础模型（EEGFM）相当的性能。

Abstract: Time series foundation models (TSFMs) pretrained on data from multiple domains have shown strong performance on diverse modeling tasks. Various efforts have been made to develop foundation models specific to electroencephalography (EEG) data, which records brain electrical activity as time series. However, no comparative analysis of EEG-specific foundation models (EEGFMs) versus general TSFMs has been performed on EEG-specific tasks. We introduce a novel Spatial-Temporal Adapter with Multi-Head Pooling (STAMP), which leverages univariate embeddings produced by a general TSFM, implicitly models spatial-temporal characteristics of EEG data, and achieves performance comparable to state-of-the-art EEGFMs. A comprehensive analysis is performed on 8 benchmark datasets of clinical tasks using EEG for classification, along with ablation studies. Our proposed adapter is lightweight in trainable parameters and flexible in the inputs it can accommodate, supporting easy modeling of EEG data using TSFMs.

</details>


### [163] [ExPairT-LLM: Exact Learning for LLM Code Selection by Pairwise Queries](https://arxiv.org/abs/2511.10855)
*Tom Yuviler,Dana Drachsler-Cohen*

Main category: cs.LG

TL;DR: ExPairT-LLM是一种用于代码选择的精确学习算法，通过向LLM提出成对成员和成对等价查询来选择最佳程序。


<details>
  <summary>Details</summary>
Motivation: 现有的代码选择算法无法识别正确的程序，因为它们可能会错误识别不等价的程序，或者它们依赖于LLM并假设它总是能正确确定每个输入的输出。

Method: 提出ExPairT-LLM，一种用于代码选择的精确学习算法，该算法通过向LLM oracle提出两种新型查询：成对成员和成对等价查询来选择程序。这些查询对于LLM来说更简单，并使ExPairT-LLM能够通过锦标赛识别正确的程序，这对于LLM的一些错误具有鲁棒性。

Result: 在四个流行的代码数据集上评估了ExPairT-LLM。它的pass@1（成功率）平均超过了最先进的代码选择算法+13.0%，最高可达+27.1%。它还将执行复杂推理的LLM的pass@1提高了+24.0%。

Conclusion: ExPairT-LLM在代码选择任务上优于现有算法，并且可以提高LLM在复杂推理中的性能。

Abstract: Despite recent advances in LLMs, the task of code generation is still challenging. To cope, code selection algorithms select the best program from multiple programs generated by an LLM. However, existing algorithms can fail to identify the correct program, either because they can misidentify nonequivalent programs or because they rely on an LLM and assume it always correctly determines the output for every input. We present ExPairT-LLM, an exact learning algorithm for code selection that selects a program by posing to an LLM oracle two new types of queries: pairwise membership and pairwise equivalence. These queries are simpler for LLMs and enable ExPairT-LLM to identify the correct program through a tournament, which is robust to some LLM mistakes. We evaluate ExPairT-LLM on four popular code datasets. Its pass@1 (success rate) outperforms the state-of-the-art code selection algorithm on average by +13.0% and up to +27.1%. It also improves the pass@1 of LLMs performing complex reasoning by +24.0%.

</details>


### [164] [Private Zeroth-Order Optimization with Public Data](https://arxiv.org/abs/2511.10859)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: 本文提出了一种利用公共信息来指导和改进私有零阶算法梯度逼近的方法，以解决差分隐私机器学习算法计算和内存成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 流行的一阶差分隐私(DP)机器学习算法(如DP-SGD)在部署时面临的主要瓶颈是其高计算和内存成本。零阶方法通过函数评估来近似梯度，因此更容易私有化，有希望缓解这一开销。然而，与DP-SGD相比，现有的零阶方法效用较低，并且仅在有限的应用领域中进行了评估。

Method: 本文提出了一套公共数据辅助的零阶优化器(PAZO)，该优化器具有最小的开销。在公共数据和私有数据相似的假设下，提供了PAZO框架的理论分析。

Result: 实验结果表明，在预训练和微调设置中，PAZO在视觉和文本任务中实现了卓越的隐私/效用权衡，优于最佳的一阶基线(具有公共数据)，尤其是在高度隐私的制度下，同时提供了高达16倍的运行时加速。

Conclusion: 本文提出的PAZO框架可以有效地提高差分隐私机器学习算法的效率和效用。

Abstract: One of the major bottlenecks for deploying popular first-order differentially private (DP) machine learning algorithms (e.g., DP-SGD) lies in their high computation and memory cost, despite the existence of optimized implementations. Zeroth-order methods have promise in mitigating the overhead, as they leverage function evaluations to approximate the gradients, hence significantly easier to privatize. While recent works have explored zeroth-order approaches in both private and non-private settings, they still suffer from relatively low utilities compared with DP-SGD, and have only been evaluated in limited application domains. In this work, we propose to leverage public information to guide and improve gradient approximation of private zeroth-order algorithms. We explore a suite of public-data-assisted zeroth-order optimizers (PAZO) with minimal overhead. We provide theoretical analyses of the PAZO framework under an assumption of the similarity between public and private data. Empirically, we demonstrate that PAZO achieves superior privacy/utility tradeoffs across vision and text tasks in both pre-training and fine-tuning settings, outperforming the best first-order baselines (with public data) especially in highly private regimes, while offering up to $16\times$ runtime speedup.

</details>


### [165] [Go-UT-Bench: A Fine-Tuning Dataset for LLM-Based Unit Test Generation in Go](https://arxiv.org/abs/2511.10868)
*Yashshi Pipalani,Hritik Raj,Rajat Ghosh,Vaishnavi Bhargava,Debojyoti Dutta*

Main category: cs.LG

TL;DR: 代码LLM在处理不平衡的训练数据时面临挑战，尤其是在Go语言等低资源语言中，单元测试生成能力较弱。


<details>
  <summary>Details</summary>
Motivation: 现有的代码LLM主要侧重于开源代码，缺乏对软件工程任务的 रिप्रेजेंटation，尤其是在低资源语言中。

Method: 引入了GO UT Bench，一个包含5264个代码和单元测试对的基准数据集，来自10个许可的Golang存储库。

Result: 在GO UT Bench上进行微调的模型在超过75%的基准测试任务中优于其基础模型。

Conclusion: 通过在GO UT Bench上进行微调，可以显著提高代码LLM在Go语言单元测试生成等软件工程任务中的性能。

Abstract: Training data imbalance poses a major challenge for code LLMs. Most available data heavily over represents raw opensource code while underrepresenting broader software engineering tasks, especially in low resource languages like Golang. As a result, models excel at code autocompletion but struggle with real world developer workflows such as unit test generation. To address this gap, we introduce GO UT Bench, a benchmark dataset of 5264 pairs of code and unit tests, drawn from 10 permissively licensed Golang repositories spanning diverse domain. We evaluate its effectiveness as a fine tuning dataset across two LLM families i.e. mixture of experts and dense decoders. Our results show that finetuned models outperform their base counterparts on more than 75% of benchmark tasks.

</details>


### [166] [Incorporating Spatial Information into Goal-Conditioned Hierarchical Reinforcement Learning via Graph Representations](https://arxiv.org/abs/2511.10872)
*Shuyuan Zhang,Zihan Wang,Xiao-Wen Chang,Doina Precup*

Main category: cs.LG

TL;DR: 提出了一种名为G4RL的图引导子目标表示生成强化学习方法，通过图编码器-解码器评估未见状态，提高现有GCHRL方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图强化学习方法依赖于领域知识构建图，泛化性差；动态图方法难以充分利用图信息；GCHRL方法存在样本效率低和子目标表示差的问题。

Method: 开发了一个图编码器-解码器来评估未见状态，利用探索过程中生成的状态图进行训练，结合高低层内在奖励。

Result: 实验结果表明，G4RL方法能显著提高现有GCHRL方法的性能，且计算成本较低，在密集和稀疏奖励环境中均有效。

Conclusion: G4RL方法通过图编码器-解码器有效提升了GCHRL方法的性能，尤其在具有对称和可逆转换的环境中。

Abstract: The integration of graphs with Goal-conditioned Hierarchical Reinforcement Learning (GCHRL) has recently gained attention, as intermediate goals (subgoals) can be effectively sampled from graphs that naturally represent the overall task structure in most RL tasks. However, existing approaches typically rely on domain-specific knowledge to construct these graphs, limiting their applicability to new tasks. Other graph-based approaches create graphs dynamically during exploration but struggle to fully utilize them, because they have problems passing the information in the graphs to newly visited states. Additionally, current GCHRL methods face challenges such as sample inefficiency and poor subgoal representation. This paper proposes a solution to these issues by developing a graph encoder-decoder to evaluate unseen states. Our proposed method, Graph-Guided sub-Goal representation Generation RL (G4RL), can be incorporated into any existing GCHRL method when operating in environments with primarily symmetric and reversible transitions to enhance performance across this class of problems. We show that the graph encoder-decoder can be effectively implemented using a network trained on the state graph generated during exploration. Empirical results indicate that leveraging high and low-level intrinsic rewards from the graph encoder-decoder significantly enhances the performance of state-of-the-art GCHRL approaches with an extra small computational cost in dense and sparse reward environments.

</details>


### [167] [Multi-Joint Physics-Informed Deep Learning Framework for Time-Efficient Inverse Dynamics](https://arxiv.org/abs/2511.10878)
*Shuhao Ma,Zeyi Huang,Yu Cao,Wesley Doorsamy,Chaoyang Shi,Jun Li,Zhi-Qiang Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的深度学习框架，用于高效估计多关节系统的肌肉激活和力量，无需标记数据。


<details>
  <summary>Details</summary>
Motivation: 传统方法计算成本高，缺乏高质量的多关节应用标记数据集。

Method: 采用多关节交叉注意力（MJCA）模块和双向门控循环单元（BiGRU）层，以捕捉关节间的协调性。将多关节动力学、关节间耦合和外力相互作用嵌入到损失函数中。

Result: 在两个数据集上的实验验证表明，PI-MJCA-BiGRU在没有真实标签的情况下，实现了与传统监督方法相当的性能，并且MJCA模块显著增强了关节间协调建模。

Conclusion: PI-MJCA-BiGRU 能够在无需标记数据的情况下，对多关节系统的肌肉激活和力量进行生理上一致的预测，并实现高效的推理。

Abstract: Time-efficient estimation of muscle activations and forces across multi-joint systems is critical for clinical assessment and assistive device control. However, conventional approaches are computationally expensive and lack a high-quality labeled dataset for multi-joint applications. To address these challenges, we propose a physics-informed deep learning framework that estimates muscle activations and forces directly from kinematics. The framework employs a novel Multi-Joint Cross-Attention (MJCA) module with Bidirectional Gated Recurrent Unit (BiGRU) layers to capture inter-joint coordination, enabling each joint to adaptively integrate motion information from others. By embedding multi-joint dynamics, inter-joint coupling, and external force interactions into the loss function, our Physics-Informed MJCA-BiGRU (PI-MJCA-BiGRU) delivers physiologically consistent predictions without labeled data while enabling time-efficient inference. Experimental validation on two datasets demonstrates that PI-MJCA-BiGRU achieves performance comparable to conventional supervised methods without requiring ground-truth labels, while the MJCA module significantly enhances inter-joint coordination modeling compared to other baseline architectures.

</details>


### [168] [Multi-View Polymer Representations for the Open Polymer Prediction](https://arxiv.org/abs/2511.10893)
*Wonjin Jung,Yongseok Choi*

Main category: cs.LG

TL;DR: 这篇论文提出了一种多视角设计，利用互补的表示方法来预测聚合物性能。该系统集成了四种特征：(i)表格RDKit/Morgan描述符，(ii)图神经网络，(iii)3D信息表示，和(iv)预训练的SMILES语言模型，并通过统一的集成平均每个属性的预测。模型经过10倍分割训练，并通过SMILES测试时增强进行评估。该方法在 NeurIPS 2025 Open Polymer Prediction Challenge 中排名 2241 支队伍中的第 9 名。提交的集成实现了 0.057 的公共 MAE 和 0.082 的私有 MAE。


<details>
  <summary>Details</summary>
Motivation: 提高聚合物性能预测的准确性。

Method: 采用多视角设计，集成四种互补的表示方法：RDKit/Morgan描述符、图神经网络、3D信息表示和预训练的SMILES语言模型，并通过统一集成平均预测结果。

Result: 在NeurIPS 2025 Open Polymer Prediction Challenge中排名第9，公共MAE为0.057，私有MAE为0.082。

Conclusion: 该方法在聚合物性能预测方面表现出色，并在公开竞赛中取得了有竞争力的结果。

Abstract: We address polymer property prediction with a multi-view design that exploits complementary representations. Our system integrates four families: (i) tabular RDKit/Morgan descriptors, (ii) graph neural networks, (iii) 3D-informed representations, and (iv) pretrained SMILES language models, and averages per-property predictions via a uniform ensemble. Models are trained with 10-fold splits and evaluated with SMILES test-time augmentation. The approach ranks 9th of 2241 teams in the Open Polymer Prediction Challenge at NeurIPS 2025. The submitted ensemble achieves a public MAE of 0.057 and a private MAE of 0.082.

</details>


### [169] [Graph Attention Network for Predicting Duration of Large-Scale Power Outages Induced by Natural Disasters](https://arxiv.org/abs/2511.10898)
*Chenghao Duan,Chuanyi Ji*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的方法，通过图注意力网络（GAT）来估计恶劣天气导致的停电持续时间。


<details>
  <summary>Details</summary>
Motivation: 飓风、野火和冬季风暴等自然灾害已在美国造成大规模停电，造成巨大的经济和社会影响。准确预测停电恢复和影响是电网弹性的关键。

Method: 该网络使用来自无监督预训练的简单结构，然后进行半监督学习。使用来自影响美国东南部八个州的501个县的四次主要飓风的现场数据。

Result: 该模型表现出优异的性能（>93% 的准确率），并且在整体性能和类别准确率方面，优于现有的 XGBoost、随机森林、GCN 和简单 GAT 方法 2%-15%。

Conclusion: 图注意力网络（GAT）可以有效地预测恶劣天气导致的停电持续时间，并且优于现有的方法。

Abstract: Natural disasters such as hurricanes, wildfires, and winter storms have induced large-scale power outages in the U.S., resulting in tremendous economic and societal impacts. Accurately predicting power outage recovery and impact is key to resilience of power grid. Recent advances in machine learning offer viable frameworks for estimating power outage duration from geospatial and weather data. However, three major challenges are inherent to the task in a real world setting: spatial dependency of the data, spatial heterogeneity of the impact, and moderate event data. We propose a novel approach to estimate the duration of severe weather-induced power outages through Graph Attention Networks (GAT). Our network uses a simple structure from unsupervised pre-training, followed by semi-supervised learning. We use field data from four major hurricanes affecting $501$ counties in eight Southeastern U.S. states. The model exhibits an excellent performance ($>93\%$ accuracy) and outperforms the existing methods XGBoost, Random Forest, GCN and simple GAT by $2\% - 15\%$ in both the overall performance and class-wise accuracy.

</details>


### [170] [Towards Federated Clustering: A Client-wise Private Graph Aggregation Framework](https://arxiv.org/abs/2511.10915)
*Guanxiong He,Jie Wang,Liaoyuan Tang,Zheng Wang,Rong Wang,Feiping Nie*

Main category: cs.LG

TL;DR: 提出了一种新的联邦图聚类算法，该算法利用局部结构图作为主要媒介进行保护隐私的知识共享，从而超越了传统技术的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦聚类方法需要在性能和隐私之间做出妥协：传输嵌入表示会带来敏感数据泄露的风险，而仅共享抽象的聚类原型会导致模型精度降低。

Method: 提出了一种新的算法，即结构化隐私保护联邦图聚类 (SPP-FGC)，该算法创新性地利用局部结构图作为保护隐私的知识共享的主要媒介。该框架以清晰的客户端-服务器逻辑运行；在客户端，每个参与者构建一个捕获内在数据关系的私有结构图，然后服务器安全地聚合和对齐这些结构图，以形成一个统一的全局图，从中导出统一的聚类结构。

Result: 大量的实验表明，该框架实现了最先进的性能，与联邦基线相比，聚类精度提高了高达 10% (NMI)，同时保持了可证明的隐私保证。

Conclusion: 该框架为联邦聚类提供了一种新的解决方案，可以在保护隐私的同时实现高性能。

Abstract: Federated clustering addresses the critical challenge of extracting patterns from decentralized, unlabeled data. However, it is hampered by the flaw that current approaches are forced to accept a compromise between performance and privacy: \textit{transmitting embedding representations risks sensitive data leakage, while sharing only abstract cluster prototypes leads to diminished model accuracy}. To resolve this dilemma, we propose Structural Privacy-Preserving Federated Graph Clustering (SPP-FGC), a novel algorithm that innovatively leverages local structural graphs as the primary medium for privacy-preserving knowledge sharing, thus moving beyond the limitations of conventional techniques. Our framework operates on a clear client-server logic; on the client-side, each participant constructs a private structural graph that captures intrinsic data relationships, which the server then securely aggregates and aligns to form a comprehensive global graph from which a unified clustering structure is derived. The framework offers two distinct modes to suit different needs. SPP-FGC is designed as an efficient one-shot method that completes its task in a single communication round, ideal for rapid analysis. For more complex, unstructured data like images, SPP-FGC+ employs an iterative process where clients and the server collaboratively refine feature representations to achieve superior downstream performance. Extensive experiments demonstrate that our framework achieves state-of-the-art performance, improving clustering accuracy by up to 10\% (NMI) over federated baselines while maintaining provable privacy guarantees.

</details>


### [171] [GraphToxin: Reconstructing Full Unlearned Graphs from Graph Unlearning](https://arxiv.org/abs/2511.10936)
*Ying Song,Balaji Palanisamy*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为GraphToxin的图重构攻击方法，用于攻击图神经网络的graph unlearning技术，旨在恢复被删除的敏感信息。


<details>
  <summary>Details</summary>
Motivation: 现有的图解学习方法存在漏洞，多方参与会产生新的攻击面，删除数据的残留痕迹仍然存在于未学习的图神经网络中，攻击者可以利用这些漏洞恢复被删除的样本。

Method: 论文引入了一个新颖的曲率匹配模块，为完全未学习的图恢复提供细粒度的指导。

Result: 实验表明，GraphToxin可以成功地破坏图解学习的监管保证，不仅可以恢复已删除的个人信息和个人链接，还可以从他们的连接中恢复敏感内容。

Conclusion: 现有的防御机制在很大程度上对这种攻击无效，在某些情况下，甚至会放大其性能。迫切需要开发更有效和更强大的防御策略来抵御这种攻击。

Abstract: Graph unlearning has emerged as a promising solution for complying with "the right to be forgotten" regulations by enabling the removal of sensitive information upon request. However, this solution is not foolproof. The involvement of multiple parties creates new attack surfaces, and residual traces of deleted data can still remain in the unlearned graph neural networks. These vulnerabilities can be exploited by attackers to recover the supposedly erased samples, thereby undermining the inherent functionality of graph unlearning. In this work, we propose GraphToxin, the first graph reconstruction attack against graph unlearning. Specifically, we introduce a novel curvature matching module to provide a fine-grained guidance for full unlearned graph recovery. We demonstrate that GraphToxin can successfully subvert the regulatory guarantees expected from graph unlearning - it can recover not only a deleted individual's information and personal links but also sensitive content from their connections, thereby posing substantially more detrimental threats. Furthermore, we extend GraphToxin to multiple node removals under both white-box and black-box setting. We highlight the necessity of a worst-case analysis and propose a comprehensive evaluation framework to systematically assess the attack performance under both random and worst-case node removals. This provides a more robust and realistic measure of the vulnerability of graph unlearning methods to graph reconstruction attacks. Our extensive experiments demonstrate the effectiveness and flexibility of GraphToxin. Notably, we show that existing defense mechanisms are largely ineffective against this attack and, in some cases, can even amplify its performance. Given the severe privacy risks posed by GraphToxin, our work underscores the urgent need for the development of more effective and robust defense strategies against this attack.

</details>


### [172] [Cascading Bandits With Feedback](https://arxiv.org/abs/2511.10938)
*R Sri Prakash,Nikhil Karamchandani,Sharayu Moharir*

Main category: cs.LG

TL;DR: 本文研究了边缘推理中的级联bandit模型变体，其中每个臂对应于具有相关准确性和错误概率的推理模型。


<details>
  <summary>Details</summary>
Motivation: 边缘推理面临挑战。

Method: 分析了四种决策策略：Explore-then-Commit、Action Elimination、Lower Confidence Bound (LCB) 和 Thompson Sampling，并为每种策略提供了清晰的理论后悔保证。

Result: Explore-then-Commit和Action Elimination由于在探索阶段后提交到固定排序而产生次优后悔，限制了它们的适应能力。相比之下，LCB和Thompson Sampling基于观察到的反馈不断更新其决策，实现了O(1)的恒定后悔。

Conclusion: 自适应性对于在不确定性下进行高效边缘推理至关重要。

Abstract: Motivated by the challenges of edge inference, we study a variant of the cascade bandit model in which each arm corresponds to an inference model with an associated accuracy and error probability. We analyse four decision-making policies-Explore-then-Commit, Action Elimination, Lower Confidence Bound (LCB), and Thompson Sampling-and provide sharp theoretical regret guarantees for each. Unlike in classical bandit settings, Explore-then-Commit and Action Elimination incur suboptimal regret because they commit to a fixed ordering after the exploration phase, limiting their ability to adapt. In contrast, LCB and Thompson Sampling continuously update their decisions based on observed feedback, achieving constant O(1) regret. Simulations corroborate these theoretical findings, highlighting the crucial role of adaptivity for efficient edge inference under uncertainty.

</details>


### [173] [Flow matching-based generative models for MIMO channel estimation](https://arxiv.org/abs/2511.10941)
*Wenkai Liu,Nan Ma,Jianqiao Chen,Xiaoxuan Qi,Yuhang Ma*

Main category: cs.LG

TL;DR: 提出了一种基于流匹配(FM)的MIMO信道估计算法，以解决基于扩散模型(DM)的信道估计速度慢的问题。


<details>
  <summary>Details</summary>
Motivation: 基于扩散模型的信道估计在获取高精度信道状态信息(CSI)方面显示出潜力，但采样速度慢是一个挑战。

Method: 在FM框架内构建从噪声信道分布到真实信道分布的条件概率路径，并推导仅依赖于噪声统计的 velocity field 来指导生成模型训练。在采样阶段，利用训练好的 velocity field 作为先验信息，通过常微分方程(ODE)欧拉求解器实现快速可靠的噪声信道增强。

Result: 数值结果表明，与基于 score matching (SM) 的方案等其他流行的基于DM的方案相比，所提出的基于FM的信道估计方案可以显著减少采样开销，并在不同的信道条件下实现卓越的信道估计精度。

Conclusion: 该论文提出了一种新颖的基于流匹配的MIMO信道估计算法，该算法在保证信道估计精度的前提下，显著提高了采样速度。

Abstract: Diffusion model (DM)-based channel estimation, which generates channel samples via a posteriori sampling stepwise with denoising process, has shown potential in high-precision channel state information (CSI) acquisition. However, slow sampling speed is an essential challenge for recent developed DM-based schemes. To alleviate this problem, we propose a novel flow matching (FM)-based generative model for multiple-input multiple-output (MIMO) channel estimation. We first formulate the channel estimation problem within FM framework, where the conditional probability path is constructed from the noisy channel distribution to the true channel distribution. In this case, the path evolves along the straight-line trajectory at a constant speed. Then, guided by this, we derive the velocity field that depends solely on the noise statistics to guide generative models training. Furthermore, during the sampling phase, we utilize the trained velocity field as prior information for channel estimation, which allows for quick and reliable noise channel enhancement via ordinary differential equation (ODE) Euler solver. Finally, numerical results demonstrate that the proposed FM-based channel estimation scheme can significantly reduce the sampling overhead compared to other popular DM-based schemes, such as the score matching (SM)-based scheme. Meanwhile, it achieves superior channel estimation accuracy under different channel conditions.

</details>


### [174] [From Parameter to Representation: A Closed-Form Approach for Controllable Model Merging](https://arxiv.org/abs/2511.10943)
*Jialin Wu,Jian Yang,Handing Wang,Jiajun Wen,Zhiyong Yu*

Main category: cs.LG

TL;DR: 提出了一种新的模型融合方法，该方法通过直接校正模型的最终表示来克服参数干扰的挑战，从而实现快速、可控的模型生成。


<details>
  <summary>Details</summary>
Motivation: 现有的模型融合方法依赖于昂贵的离线多目标优化，并且复杂度随任务数量呈指数增长。

Method: 将模型校正建模为最优线性变换，从而得到一个闭式解，该解可以单步计算，并且与模型架构无关。

Result: 实验结果表明，该方法能够生成更优的Pareto前沿，更精确地对齐用户偏好，并显著降低计算成本。

Conclusion: 该方法通过直接校正模型表示，实现了高效且可控的模型融合，克服了传统方法的局限性。

Abstract: Model merging combines expert models for multitask performance but faces challenges from parameter interference. This has sparked recent interest in controllable model merging, giving users the ability to explicitly balance performance trade-offs. Existing approaches employ a compile-then-query paradigm, performing a costly offline multi-objective optimization to enable fast, preference-aware model generation. This offline stage typically involves iterative search or dedicated training, with complexity that grows exponentially with the number of tasks. To overcome these limitations, we shift the perspective from parameter-space optimization to a direct correction of the model's final representation. Our approach models this correction as an optimal linear transformation, yielding a closed-form solution that replaces the entire offline optimization process with a single-step, architecture-agnostic computation. This solution directly incorporates user preferences, allowing a Pareto-optimal model to be generated on-the-fly with complexity that scales linearly with the number of tasks. Experimental results show our method generates a superior Pareto front with more precise preference alignment and drastically reduced computational cost.

</details>


### [175] [Unsupervised Robust Domain Adaptation: Paradigm, Theory and Algorithm](https://arxiv.org/abs/2511.11009)
*Fuxiang Huang,Xiaowei Fu,Shiyu Ye,Lina Ma,Wen Li,Xinbo Gao,David Zhang,Lei Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的非监督鲁棒域适应（URDA）范式，以提高模型在对抗攻击和领域偏移下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的UDA方法忽略了对抗攻击的鲁棒性，而直接应用VAT在UDA中效果不佳。

Method: 1) 揭示了UDA+VAT范式中固有的纠缠挑战；2) 推导了URDA范式的泛化边界理论；3) 提出了Disentangled Adversarial Robustness Training (DART) 算法，通过解耦蒸馏进行鲁棒化训练。

Result: 在四个基准数据集上的实验表明，DART有效提高了鲁棒性，同时保持了领域适应性。

Conclusion: DART算法验证了URDA范式和理论的有效性，为提升UDA模型的鲁棒性提供了一种新思路。

Abstract: Unsupervised domain adaptation (UDA) aims to transfer knowledge from a label-rich source domain to an unlabeled target domain by addressing domain shifts. Most UDA approaches emphasize transfer ability, but often overlook robustness against adversarial attacks. Although vanilla adversarial training (VAT) improves the robustness of deep neural networks, it has little effect on UDA. This paper focuses on answering three key questions: 1) Why does VAT, known for its defensive effectiveness, fail in the UDA paradigm? 2) What is the generalization bound theory under attacks and how does it evolve from classical UDA theory? 3) How can we implement a robustification training procedure without complex modifications? Specifically, we explore and reveal the inherent entanglement challenge in general UDA+VAT paradigm, and propose an unsupervised robust domain adaptation (URDA) paradigm. We further derive the generalization bound theory of the URDA paradigm so that it can resist adversarial noise and domain shift. To the best of our knowledge, this is the first time to establish the URDA paradigm and theory. We further introduce a simple, novel yet effective URDA algorithm called Disentangled Adversarial Robustness Training (DART), a two-step training procedure that ensures both transferability and robustness. DART first pre-trains an arbitrary UDA model, and then applies an instantaneous robustification post-training step via disentangled distillation.Experiments on four benchmark datasets with/without attacks show that DART effectively enhances robustness while maintaining domain adaptability, and validate the URDA paradigm and theory.

</details>


### [176] [Enhancing Graph Representations with Neighborhood-Contextualized Message-Passing](https://arxiv.org/abs/2511.11046)
*Brian Godwin Lim*

Main category: cs.LG

TL;DR: 本文提出了邻域上下文消息传递（NCMP）框架，旨在增强经典图神经网络（GNNs）的图表示能力。


<details>
  <summary>Details</summary>
Motivation: 现有的消息传递GNNs只考虑中心节点和每个相邻节点，忽略了更广泛的局部邻域中包含的丰富上下文信息，可能阻碍其学习复杂关系的能力。

Method: 1. 形式化了邻域上下文的概念。2. 将消息传递变体推广到提出的NCMP框架。3. 提出了一种简单、实用且高效的参数化和操作化NCMP的方法，从而开发了软同构邻域上下文图卷积网络（SINC-GCN）。

Result: 在合成二元节点分类问题上的初步分析强调了所提出的GNN架构的表现力和效率。

Conclusion: 本文为新的NCMP框架奠定了基础，这是进一步增强经典GNN的图表示能力的一个实用途径。

Abstract: Graph neural networks (GNNs) have become an indispensable tool for analyzing relational data. In the literature, classical GNNs may be classified into three variants: convolutional, attentional, and message-passing. While the standard message-passing variant is highly expressive, its typical pair-wise messages nevertheless only consider the features of the center node and each neighboring node individually. This design fails to incorporate the rich contextual information contained within the broader local neighborhood, potentially hindering its ability to learn complex relationships within the entire set of neighboring nodes. To address this limitation, this work first formalizes the concept of neighborhood-contextualization, rooted in a key property of the attentional variant. This then serves as the foundation for generalizing the message-passing variant to the proposed neighborhood-contextualized message-passing (NCMP) framework. To demonstrate its utility, a simple, practical, and efficient method to parametrize and operationalize NCMP is presented, leading to the development of the proposed Soft-Isomorphic Neighborhood-Contextualized Graph Convolution Network (SINC-GCN). A preliminary analysis on a synthetic binary node classification problem then underscores both the expressivity and efficiency of the proposed GNN architecture. Overall, the paper lays the foundation for the novel NCMP framework as a practical path toward further enhancing the graph representational power of classical GNNs.

</details>


### [177] [Echoless Label-Based Pre-computation for Memory-Efficient Heterogeneous Graph Learning](https://arxiv.org/abs/2511.11081)
*Jun Hu,Shangheng Chen,Yufei He,Yuan Li,Bryan Hooi,Bingsheng He*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的预计算异构图神经网络（HGNN）方法，名为Echoless-LP，旨在解决训练标签泄露问题（回声效应），同时保持内存效率和与各种消息传递方法的兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有的预计算HGNN方法在收集邻居标签信息时存在训练标签泄露问题（回声效应），并且现有的缓解策略要么内存效率低，要么与高级消息传递方法不兼容。

Method: 论文提出了Partition-Focused Echoless Propagation (PFEP)，通过将节点划分为不同的分区，并在消息传递过程中只收集来自其他分区的邻居的标签信息，从而避免回声效应。此外，还提出了Asymmetric Partitioning Scheme (APS)和PostAdjust机制来解决分区带来的信息损失和分布偏移。

Result: 在公共数据集上的实验表明，Echoless-LP实现了卓越的性能，并保持了内存效率。

Conclusion: Echoless-LP 是一种有效且内存效率高的 HGNN 方法，它可以有效地解决训练标签泄露问题，并且与各种消息传递方法兼容。

Abstract: Heterogeneous Graph Neural Networks (HGNNs) are widely used for deep learning on heterogeneous graphs. Typical end-to-end HGNNs require repetitive message passing during training, limiting efficiency for large-scale real-world graphs. Pre-computation-based HGNNs address this by performing message passing only once during preprocessing, collecting neighbor information into regular-shaped tensors, which enables efficient mini-batch training. Label-based pre-computation methods collect neighbors' label information but suffer from training label leakage, where a node's own label information propagates back to itself during multi-hop message passing - the echo effect. Existing mitigation strategies are memory-inefficient on large graphs or suffer from compatibility issues with advanced message passing methods. We propose Echoless Label-based Pre-computation (Echoless-LP), which eliminates training label leakage with Partition-Focused Echoless Propagation (PFEP). PFEP partitions target nodes and performs echoless propagation, where nodes in each partition collect label information only from neighbors in other partitions, avoiding echo while remaining memory-efficient and compatible with any message passing method. We also introduce an Asymmetric Partitioning Scheme (APS) and a PostAdjust mechanism to address information loss from partitioning and distributional shifts across partitions. Experiments on public datasets demonstrate that Echoless-LP achieves superior performance and maintains memory efficiency compared to baselines.

</details>


### [178] [Scalable Population Training for Zero-Shot Coordination](https://arxiv.org/abs/2511.11083)
*Bingyu Hui,Lebin Yu,Quanming Yao,Yunpeng Qu,Xudong Zhang,Jian Wang*

Main category: cs.LG

TL;DR: 这篇论文提出了一种可扩展的种群训练（ScaPT）框架，用于解决零样本协同（ZSC）问题，该框架通过参数共享的元代理和互信息正则化器来提高种群的多样性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于种群的训练方法在零样本协同中表现良好，但受限于计算资源，无法扩展种群规模。

Method: 该论文提出了ScaPT框架，包含一个通过选择性参数共享实现种群的元代理和一个保证种群多样性的互信息正则化器。

Result: 在Hanabi游戏中评估了ScaPT框架，并验证了其优越性。

Conclusion: ScaPT框架能够有效提高零样本协同的性能。

Abstract: Zero-shot coordination(ZSC) has become a hot topic in reinforcement learning research recently. It focuses on the generalization ability of agents, requiring them to coordinate well with collaborators that are not seen before without any fine-tuning. Population-based training has been proven to provide good zero-shot coordination performance; nevertheless, existing methods are limited by computational resources, mainly focusing on optimizing diversity in small populations while neglecting the potential performance gains from scaling population size. To address this issue, this paper proposes the Scalable Population Training (ScaPT), an efficient training framework comprising two key components: a meta-agent that efficiently realizes a population by selectively sharing parameters across agents, and a mutual information regularizer that guarantees population diversity. To empirically validate the effectiveness of ScaPT, this paper evaluates it along with representational frameworks in Hanabi and confirms its superiority.

</details>


### [179] [Sheaf Cohomology of Linear Predictive Coding Networks](https://arxiv.org/abs/2511.11092)
*Jeffrey Seely*

Main category: cs.LG

TL;DR: 本文将线性预测编码（PC）网络形式化为细胞层，其中层上同调表征了推理无法消除的不可约误差模式。分析了反馈环路导致内部矛盾的循环拓扑，并利用 Hodge 分解确定这些矛盾何时导致学习停滞。该层形式体系为识别有问题的网络配置提供了诊断工具，并为循环 PC 网络的有效权重初始化提供了设计原则。


<details>
  <summary>Details</summary>
Motivation: 用局部优化代替全局反向传播。

Method: 将线性 PC 网络构建为细胞层，并使用 Hodge 分解分析循环拓扑。

Result: 层上同调可以表征推理无法消除的不可约误差模式，Hodge 分解可以确定内部矛盾何时导致学习停滞。

Conclusion: 层形式体系为识别有问题网络配置提供了诊断工具，并为循环 PC 网络的有效权重初始化提供了设计原则。

Abstract: Predictive coding (PC) replaces global backpropagation with local optimization over weights and activations. We show that linear PC networks admit a natural formulation as cellular sheaves: the sheaf coboundary maps activations to edge-wise prediction errors, and PC inference is diffusion under the sheaf Laplacian. Sheaf cohomology then characterizes irreducible error patterns that inference cannot remove. We analyze recurrent topologies where feedback loops create internal contradictions, introducing prediction errors unrelated to supervision. Using a Hodge decomposition, we determine when these contradictions cause learning to stall. The sheaf formalism provides both diagnostic tools for identifying problematic network configurations and design principles for effective weight initialization for recurrent PC networks.

</details>


### [180] [SMART: A Surrogate Model for Predicting Application Runtime in Dragonfly Systems](https://arxiv.org/abs/2511.11111)
*Xin Wang,Pietro Lodi Rizzini,Sourav Medya,Zhiling Lan*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为
ewmodel的替代模型，该模型结合了图神经网络（GNN）和大型语言模型（LLM），以捕获端口级别路由器数据的时空模式。


<details>
  <summary>Details</summary>
Motivation: 主要挑战是共享网络链路上的工作负载干扰。高保真并行离散事件仿真（PDES）计算成本高昂，使其不适用于大规模或实时场景。

Method: 该模型结合了图神经网络（GNN）和大型语言模型（LLM），以捕获端口级别路由器数据的时空模式。

Result: 该模型优于现有的统计和机器学习基线，能够准确地预测运行时并支持蜻蜓网络的高效混合仿真。

Conclusion: 该模型实现了准确的运行时预测，并支持蜻蜓网络的高效混合仿真。

Abstract: The Dragonfly network, with its high-radix and low-diameter structure, is a leading interconnect in high-performance computing. A major challenge is workload interference on shared network links. Parallel discrete event simulation (PDES) is commonly used to analyze workload interference. However, high-fidelity PDES is computationally expensive, making it impractical for large-scale or real-time scenarios. Hybrid simulation that incorporates data-driven surrogate models offers a promising alternative, especially for forecasting application runtime, a task complicated by the dynamic behavior of network traffic. We present \ourmodel, a surrogate model that combines graph neural networks (GNNs) and large language models (LLMs) to capture both spatial and temporal patterns from port level router data. \ourmodel outperforms existing statistical and machine learning baselines, enabling accurate runtime prediction and supporting efficient hybrid simulation of Dragonfly networks.

</details>


### [181] [Improving Continual Learning of Knowledge Graph Embeddings via Informed Initialization](https://arxiv.org/abs/2511.11118)
*Gerard Pons,Besim Bilalli,Anna Queralt*

Main category: cs.LG

TL;DR: 知识图谱（KG）频繁更新，需要知识图谱嵌入（KGE）适应这些变化。论文提出一种新的嵌入初始化策略，可以无缝集成到现有的KGE持续学习方法中，从而增强新知识的获取并减少灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有的KGE持续学习方法在嵌入初始化方面存在问题，初始化会影响最终嵌入的准确性和训练时间，特别是在相对较小和频繁的更新中。

Method: 利用KG模式和先前学习的嵌入，基于实体所属的类来获得新实体的初始表示。

Result: 实验结果表明，所提出的初始化策略提高了KGE的预测性能，同时增强了知识保留，并加速了知识获取，减少了训练所需的时间。

Conclusion: 该方法在各种类型的KGE学习模型中都有效。

Abstract: Many Knowledege Graphs (KGs) are frequently updated, forcing their Knowledge Graph Embeddings (KGEs) to adapt to these changes. To address this problem, continual learning techniques for KGEs incorporate embeddings for new entities while updating the old ones. One necessary step in these methods is the initialization of the embeddings, as an input to the KGE learning process, which can have an important impact in the accuracy of the final embeddings, as well as in the time required to train them. This is especially relevant for relatively small and frequent updates. We propose a novel informed embedding initialization strategy, which can be seamlessly integrated into existing continual learning methods for KGE, that enhances the acquisition of new knowledge while reducing catastrophic forgetting. Specifically, the KG schema and the previously learned embeddings are utilized to obtain initial representations for the new entities, based on the classes the entities belong to. Our extensive experimental analysis shows that the proposed initialization strategy improves the predictive performance of the resulting KGEs, while also enhancing knowledge retention. Furthermore, our approach accelerates knowledge acquisition, reducing the number of epochs, and therefore time, required to incrementally learn new embeddings. Finally, its benefits across various types of KGE learning models are demonstrated.

</details>


### [182] [Anomaly Detection in High-Dimensional Bank Account Balances via Robust Methods](https://arxiv.org/abs/2511.11143)
*Federico Maddanu,Tommaso Proietti,Riccardo Crupi*

Main category: cs.LG

TL;DR: 本文提出了一种在银行账户余额中检测异常值的方法，该方法基于稳健统计，可以在中高维度数据集中高效计算。


<details>
  <summary>Details</summary>
Motivation: 金融机构需要检测银行账户余额中的异常值，以识别潜在的欺诈、运营问题或其他违规行为。

Method: 本文提出并评估了几种稳健的方法，这些方法可能在中高维度数据集中高效计算，具有高崩溃点和低计算时间。

Result: 本文应用了约 260 万条匿名用户银行账户余额的每日记录。

Conclusion: 本文旨在提供一种计算效率高、稳健的异常值检测方法，适用于大规模银行账户余额数据集。

Abstract: Detecting point anomalies in bank account balances is essential for financial institutions, as it enables the identification of potential fraud, operational issues, or other irregularities. Robust statistics is useful for flagging outliers and for providing estimates of the data distribution parameters that are not affected by contaminated observations. However, such a strategy is often less efficient and computationally expensive under high dimensional setting. In this paper, we propose and evaluate empirically several robust approaches that may be computationally efficient in medium and high dimensional datasets, with high breakdown points and low computational time. Our application deals with around 2.6 million daily records of anonymous users' bank account balances.

</details>


### [183] [Deep Learning for Short-Term Precipitation Prediction in Four Major Indian Cities: A ConvLSTM Approach with Explainable AI](https://arxiv.org/abs/2511.11152)
*Tanmay Ghosh,Shaurabh Anand,Rakesh Gomaji Nannewar,Nithin Nagaraj*

Main category: cs.LG

TL;DR: 开发了一种可解释的深度学习框架，用于四个印度主要城市（Bengaluru、Mumbai、Delhi 和 Kolkata）的短期降水预测。


<details>
  <summary>Details</summary>
Motivation: 现有的降水预测深度学习模型通常是黑盒，限制了它们在实际天气预测中的应用。为了提高透明度并保持准确性。

Method: 实施了一种混合的时间分布 CNN-ConvLSTM 架构，该架构在多年代的 ERA5 重新分析数据上进行训练。针对每个城市使用不同数量的卷积滤波器对架构进行了优化。

Result: 该模型实现了以下均方根误差 (RMSE) 值：Bengaluru (0.21 毫米/天)、Mumbai (0.52 毫米/天)、Delhi (0.48 毫米/天) 和 Kolkata (1.80 毫米/天)。

Conclusion: 本研究表明，可解释人工智能 (xAI) 如何为不同城市环境中的降水模式提供准确的预测和透明的见解。

Abstract: Deep learning models for precipitation forecasting often function as black boxes, limiting their adoption in real-world weather prediction. To enhance transparency while maintaining accuracy, we developed an interpretable deep learning framework for short-term precipitation prediction in four major Indian cities: Bengaluru, Mumbai, Delhi, and Kolkata, spanning diverse climate zones. We implemented a hybrid Time-Distributed CNN-ConvLSTM (Convolutional Neural Network-Long Short-Term Memory) architecture, trained on multi-decadal ERA5 reanalysis data. The architecture was optimized for each city with a different number of convolutional filters: Bengaluru (32), Mumbai and Delhi (64), and Kolkata (128). The models achieved root mean square error (RMSE) values of 0.21 mm/day (Bengaluru), 0.52 mm/day (Mumbai), 0.48 mm/day (Delhi), and 1.80 mm/day (Kolkata). Through interpretability analysis using permutation importance, Gradient-weighted Class Activation Mapping (Grad-CAM), temporal occlusion, and counterfactual perturbation, we identified distinct patterns in the model's behavior. The model relied on city-specific variables, with prediction horizons ranging from one day for Bengaluru to five days for Kolkata. This study demonstrates how explainable AI (xAI) can provide accurate forecasts and transparent insights into precipitation patterns in diverse urban environments.

</details>


### [184] [Adaptive Symmetrization of the KL Divergence](https://arxiv.org/abs/2511.11159)
*Omri Ben-Dov,Luiz F. O. Chamon*

Main category: cs.LG

TL;DR: 提出了一种新的最小化 Jeffreys 散度的方法，该方法使用代理模型来辅助优化主模型的 Jeffreys 散度。


<details>
  <summary>Details</summary>
Motivation: 正向 KL 散度的不对称性可能导致无法捕获目标分布的某些属性。对称的替代方案通常涉及脆弱的 min-max 公式和对抗训练，或者评估反向 KL 散度，这很难从样本中计算。

Method: 将联合训练任务 формулируется 为一个约束优化问题，以获得一个实用的算法，该算法可以在整个训练过程中调整模型的优先级。

Result: 展示了该框架如何在密度估计、图像生成和基于模拟的推理等任务中结合 NF 和 EBM 的优势。

Conclusion: 开发了一种新的方法来最小化 Jeffreys 散度，通过代理模型辅助优化主模型，并展示了其在各种任务中的应用潜力。

Abstract: Many tasks in machine learning can be described as or reduced to learning a probability distribution given a finite set of samples. A common approach is to minimize a statistical divergence between the (empirical) data distribution and a parameterized distribution, e.g., a normalizing flow (NF) or an energy-based model (EBM). In this context, the forward KL divergence is a ubiquitous due to its tractability, though its asymmetry may prevent capturing some properties of the target distribution. Symmetric alternatives involve brittle min-max formulations and adversarial training (e.g., generative adversarial networks) or evaluating the reverse KL divergence, as is the case for the symmetric Jeffreys divergence, which is challenging to compute from samples. This work sets out to develop a new approach to minimize the Jeffreys divergence. To do so, it uses a proxy model whose goal is not only to fit the data, but also to assist in optimizing the Jeffreys divergence of the main model. This joint training task is formulated as a constrained optimization problem to obtain a practical algorithm that adapts the models priorities throughout training. We illustrate how this framework can be used to combine the advantages of NFs and EBMs in tasks such as density estimation, image generation, and simulation-based inference.

</details>


### [185] [Training Neural Networks at Any Scale](https://arxiv.org/abs/2511.11163)
*Thomas Pethick,Kimon Antonakopoulos,Antonio Silveti-Falls,Leena Chennuru Vankadara,Volkan Cevher*

Main category: cs.LG

TL;DR: 本文回顾了用于训练神经网络的现代优化方法，重点关注效率和规模。


<details>
  <summary>Details</summary>
Motivation: 展示了在问题结构中进行调整的重要性。

Method: 在统一的算法模板下，展示了最先进的优化算法。

Result: 介绍了如何使这些算法与问题的规模无关。

Conclusion: 本文旨在为希望参与这些令人兴奋的新发展的从业者和研究人员提供介绍

Abstract: This article reviews modern optimization methods for training neural networks with an emphasis on efficiency and scale. We present state-of-the-art optimization algorithms under a unified algorithmic template that highlights the importance of adapting to the structures in the problem. We then cover how to make these algorithms agnostic to the scale of the problem. Our exposition is intended as an introduction for both practitioners and researchers who wish to be involved in these exciting new developments.

</details>


### [186] [Power Ensemble Aggregation for Improved Extreme Event AI Prediction](https://arxiv.org/abs/2511.11170)
*Julien Collard,Pierre Gentine,Tian Zheng*

Main category: cs.LG

TL;DR: 这篇论文使用机器学习方法改进了极端气候事件（特别是热浪）的预测。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决改进气候极端事件预测的关键挑战。

Method: 论文将问题构建为一个分类问题，预测地表气温是否超过其q分位数。使用power mean来聚合集成预测。

Result: 使用power mean聚合集成预测显著提高了分类器的性能。对于更高的极端预测，效果更明显。

Conclusion: 论文提出的power聚合方法具有潜力和适应性，其最佳性能随着所选分位数阈值的变化而变化，表明对于更高的极端预测，其有效性会增加。

Abstract: This paper addresses the critical challenge of improving predictions of climate extreme events, specifically heat waves, using machine learning methods. Our work is framed as a classification problem in which we try to predict whether surface air temperature will exceed its q-th local quantile within a specified timeframe. Our key finding is that aggregating ensemble predictions using a power mean significantly enhances the classifier's performance. By making a machine-learning based weather forecasting model generative and applying this non-linear aggregation method, we achieve better accuracy in predicting extreme heat events than with the typical mean prediction from the same model. Our power aggregation method shows promise and adaptability, as its optimal performance varies with the quantile threshold chosen, demonstrating increased effectiveness for higher extremes prediction.

</details>


### [187] [On-line learning of dynamic systems: sparse regression meets Kalman filtering](https://arxiv.org/abs/2511.11178)
*Gianluigi Pillonetto,Akram Yazdani,Aleksandr Aravkin*

Main category: cs.LG

TL;DR: 提出了 Sindy Kalman Filter (SKF)，用于实时学习控制理论中的非线性动态系统模型。


<details>
  <summary>Details</summary>
Motivation: 从数据中学习控制方程对于理解物理系统的行为至关重要。Sindy 算法已证明在利用稀疏性来识别非线性动力系统的简洁模型方面有效。

Method: 将未知系统参数视为状态变量，从而统一了 Sindy 算法和 Kalman 滤波器 (KF)。

Result: SKF 增强了 KF 参数识别策略，尤其通过前瞻误差，显著简化了稀疏度、方差参数和切换瞬间的估计。在具有漂移或切换参数的混沌 Lorenz 系统上验证了 SKF，并证明了其在实时识别由真实飞行数据构建的稀疏非线性飞机模型中的有效性。

Conclusion: SKF 能够实时推断复杂的、随时间变化的非线性模型，这是单独使用这两种方法都无法实现的。

Abstract: Learning governing equations from data is central to understanding the behavior of physical systems across diverse scientific disciplines, including physics, biology, and engineering. The Sindy algorithm has proven effective in leveraging sparsity to identify concise models of nonlinear dynamical systems. In this paper, we extend sparsity-driven approaches to real-time learning by integrating a cornerstone algorithm from control theory -- the Kalman filter (KF). The resulting Sindy Kalman Filter (SKF) unifies both frameworks by treating unknown system parameters as state variables, enabling real-time inference of complex, time-varying nonlinear models unattainable by either method alone. Furthermore, SKF enhances KF parameter identification strategies, particularly via look-ahead error, significantly simplifying the estimation of sparsity levels, variance parameters, and switching instants. We validate SKF on a chaotic Lorenz system with drifting or switching parameters and demonstrate its effectiveness in the real-time identification of a sparse nonlinear aircraft model built from real flight data.

</details>


### [188] [Dynamic Deep Graph Learning for Incomplete Multi-View Clustering with Masked Graph Reconstruction Loss](https://arxiv.org/abs/2511.11181)
*Zhenghao Zhang,Jun Xie,Xingchen Chen,Tao Yu,Hongzhu Yi,Kaixin Xu,Yuanxiang Wang,Tianyu Zong,Xinming Wang,Jiahuan Chen,Guoqing Chao,Feng Chen,Zhepeng Wang,Jungang Xu*

Main category: cs.LG

TL;DR: 提出了一种新的不完整多视图聚类动态深度图学习方法(DGIMVCM)，该方法通过掩码图重建损失来解决梯度噪声问题，并利用图结构对比学习来识别视图特定图结构之间的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于GNN的不完整多视图聚类方法依赖KNN算法构建静态图，引入噪声，降低了图拓扑的鲁棒性；同时，使用均方误差损失作为图重建损失，导致优化过程中产生大量梯度噪声。

Method: 首先，构建一个鲁棒的全局图；然后，设计图卷积嵌入层提取主要特征，并利用全局图推断缺失视图，细化动态视图特定图结构；通过图结构对比学习识别视图特定图结构之间的一致性；使用图自注意力编码器提取高级表示，并通过掩码图重建损失进行优化；最后，构建聚类模块，并通过伪标签自监督训练机制进行优化。

Result: 在多个数据集上的大量实验验证了DGIMVCM的有效性和优越性。

Conclusion: DGIMVCM方法在不完整多视图聚类任务上表现出色，解决了现有方法的局限性。

Abstract: The prevalence of real-world multi-view data makes incomplete multi-view clustering (IMVC) a crucial research. The rapid development of Graph Neural Networks (GNNs) has established them as one of the mainstream approaches for multi-view clustering. Despite significant progress in GNNs-based IMVC, some challenges remain: (1) Most methods rely on the K-Nearest Neighbors (KNN) algorithm to construct static graphs from raw data, which introduces noise and diminishes the robustness of the graph topology. (2) Existing methods typically utilize the Mean Squared Error (MSE) loss between the reconstructed graph and the sparse adjacency graph directly as the graph reconstruction loss, leading to substantial gradient noise during optimization. To address these issues, we propose a novel \textbf{D}ynamic Deep \textbf{G}raph Learning for \textbf{I}ncomplete \textbf{M}ulti-\textbf{V}iew \textbf{C}lustering with \textbf{M}asked Graph Reconstruction Loss (DGIMVCM). Firstly, we construct a missing-robust global graph from the raw data. A graph convolutional embedding layer is then designed to extract primary features and refined dynamic view-specific graph structures, leveraging the global graph for imputation of missing views. This process is complemented by graph structure contrastive learning, which identifies consistency among view-specific graph structures. Secondly, a graph self-attention encoder is introduced to extract high-level representations based on the imputed primary features and view-specific graphs, and is optimized with a masked graph reconstruction loss to mitigate gradient noise during optimization. Finally, a clustering module is constructed and optimized through a pseudo-label self-supervised training mechanism. Extensive experiments on multiple datasets validate the effectiveness and superiority of DGIMVCM.

</details>


### [189] [LoRaCompass: Robust Reinforcement Learning to Efficiently Search for a LoRa Tag](https://arxiv.org/abs/2511.11190)
*Tianlang He,Zhongming Lin,Tianrui Jiang,S. -H. Gary Chan*

Main category: cs.LG

TL;DR: 本研究提出了一种名为LoRaCompass的强化学习模型，用于在未知环境中高效、稳健地搜索LoRa标签，尤其是在存在域偏移和信号波动的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有方法在搜索LoRa标签时易受域偏移和信号波动的影响，导致定位不准确。

Method: LoRaCompass通过空间感知特征提取器和策略蒸馏损失函数学习鲁棒的空间表示，并引入受上限置信区间启发的探索函数。

Result: LoRaCompass在超过80km^2的各种环境中进行了验证，定位成功率超过90%（比现有方法提高了40%），搜索路径长度与初始距离呈线性关系。

Conclusion: LoRaCompass能够有效地定位LoRa标签，并在域偏移和信号波动的情况下表现出很强的鲁棒性。

Abstract: The Long-Range (LoRa) protocol, known for its extensive range and low power, has increasingly been adopted in tags worn by mentally incapacitated persons (MIPs) and others at risk of going missing. We study the sequential decision-making process for a mobile sensor to locate a periodically broadcasting LoRa tag with the fewest moves (hops) in general, unknown environments, guided by the received signal strength indicator (RSSI). While existing methods leverage reinforcement learning for search, they remain vulnerable to domain shift and signal fluctuation, resulting in cascading decision errors that culminate in substantial localization inaccuracies. To bridge this gap, we propose LoRaCompass, a reinforcement learning model designed to achieve robust and efficient search for a LoRa tag. For exploitation under domain shift and signal fluctuation, LoRaCompass learns a robust spatial representation from RSSI to maximize the probability of moving closer to a tag, via a spatially-aware feature extractor and a policy distillation loss function. It further introduces an exploration function inspired by the upper confidence bound (UCB) that guides the sensor toward the tag with increasing confidence. We have validated LoRaCompass in ground-based and drone-assisted scenarios within diverse unseen environments covering an area of over 80km^2. It has demonstrated high success rate (>90%) in locating the tag within 100m proximity (a 40% improvement over existing methods) and high efficiency with a search path length (in hops) that scales linearly with the initial distance.

</details>


### [190] [When to Stop Federated Learning: Zero-Shot Generation of Synthetic Validation Data with Generative AI for Early Stopping](https://arxiv.org/abs/2511.11208)
*Youngjoon Lee,Hyukjoon Lee,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出了一种联邦学习的早期停止框架，通过生成式AI监控模型性能，在达到最佳性能时自适应地停止训练，节省计算资源。


<details>
  <summary>Details</summary>
Motivation: 传统的联邦学习方法通常运行预定义数量的全局轮次，导致在达到最佳性能后仍进行不必要的计算，或者模型未能达到有意义的性能时仍在训练。

Method: 利用生成式AI的零样本合成验证框架来监控模型性能并确定早期停止点。

Result: 在多标签胸部X射线分类的数值结果表明，该方法可以将训练轮次减少高达74%，同时保持精度在最佳值的1%以内。

Conclusion: 该方法能够自适应地停止训练，节省计算资源，并实现快速的超参数调整。

Abstract: Federated Learning (FL) enables collaborative model training across decentralized devices while preserving data privacy. However, FL methods typically run for a predefined number of global rounds, often leading to unnecessary computation when optimal performance is reached earlier. In addition, training may continue even when the model fails to achieve meaningful performance. To address this inefficiency, we introduce a zero-shot synthetic validation framework that leverages generative AI to monitor model performance and determine early stopping points. Our approach adaptively stops training near the optimal round, thereby conserving computational resources and enabling rapid hyperparameter adjustments. Numerical results on multi-label chest X-ray classification demonstrate that our method reduces training rounds by up to 74% while maintaining accuracy within 1% of the optimal.

</details>


### [191] [A Best-of-Both-Worlds Proof for Tsallis-INF without Fenchel Conjugates](https://arxiv.org/abs/2511.11211)
*Wei-Cheng Lee,Francesco Orabona*

Main category: cs.LG

TL;DR: 本文为 Tsallis-INF 多臂赌博机算法提供了一个简单的推导，实现了 stochastic 和 adversarial bandits 的双赢保证。


<details>
  <summary>Details</summary>
Motivation: 简化先前研究中的证明，避免使用共轭函数。

Method: 使用在线凸优化的现代工具。

Result: 为 Tsallis-INF 算法提供了双赢保证。

Conclusion: 提供了一个更简洁的证明，但未优化常数。

Abstract: In this short note, we present a simple derivation of the best-of-both-world guarantee for the Tsallis-INF multi-armed bandit algorithm from J. Zimmert and Y. Seldin. Tsallis-INF: An optimal algorithm for stochastic and adversarial bandits. Journal of Machine Learning Research, 22(28):1-49, 2021. URL https://jmlr.csail.mit.edu/papers/volume22/19-753/19-753.pdf. In particular, the proof uses modern tools from online convex optimization and avoid the use of conjugate functions. Also, we do not optimize the constants in the bounds in favor of a slimmer proof.

</details>


### [192] [Sparse Methods for Vector Embeddings of TPC Data](https://arxiv.org/abs/2511.11221)
*Tyler Wheeler,Michelle P. Kuchera,Raghuram Ramanujan,Ryan Krupp,Chris Wrede,Saiprasad Ravishankar,Connor L. Cross,Hoi Yan Ian Heung,Andrew J. Jones,Benjamin Votaw*

Main category: cs.LG

TL;DR: 本研究探索了稀疏卷积网络在时间投影室（TPC）数据表示学习中的应用，发现稀疏 ResNet 架构即使在随机设置权重的情况下，也能提供有用的事件结构化向量嵌入。


<details>
  <summary>Details</summary>
Motivation: 时间投影室（TPC）是一种通用探测器，它可以在电离介质中重建带电粒子轨迹，从而能够进行各种核物理实验的灵敏测量。

Method: 使用来自 GADGET II TPC 的数据，将原始焊盘级信号表示为稀疏张量，训练 Minkowski Engine ResNet 模型，并探测生成的事件级嵌入，从而揭示丰富的事件结构。作为交叉探测器测试，使用相同的编码器嵌入来自 AT-TPC 的数据。

Result: 即使是未经训练的稀疏 ResNet 模型也能提供有用的 AT-TPC 数据嵌入，并且当模型在 GADGET 数据上训练时，我们观察到改进。

Conclusion: 这些结果突出了稀疏卷积技术作为各种 TPC 实验中表示学习的通用工具的潜力。

Abstract: Time Projection Chambers (TPCs) are versatile detectors that reconstruct charged-particle tracks in an ionizing medium, enabling sensitive measurements across a wide range of nuclear physics experiments. We explore sparse convolutional networks for representation learning on TPC data, finding that a sparse ResNet architecture, even with randomly set weights, provides useful structured vector embeddings of events. Pre-training this architecture on a simple physics-motivated binary classification task further improves the embedding quality. Using data from the GAseous Detector with GErmanium Tagging (GADGET) II TPC, a detector optimized for measuring low-energy $β$-delayed particle decays, we represent raw pad-level signals as sparse tensors, train Minkowski Engine ResNet models, and probe the resulting event-level embeddings which reveal rich event structure. As a cross-detector test, we embed data from the Active-Target TPC (AT-TPC) -- a detector designed for nuclear reaction studies in inverse kinematics -- using the same encoder. We find that even an untrained sparse ResNet model provides useful embeddings of AT-TPC data, and we observe improvements when the model is trained on GADGET data. Together, these results highlight the potential of sparse convolutional techniques as a general tool for representation learning in diverse TPC experiments.

</details>


### [193] [Neural Network-Powered Finger-Drawn Biometric Authentication](https://arxiv.org/abs/2511.11235)
*Maan Al Balkhi,Kordian Gontarska,Marko Harasic,Adrian Paschke*

Main category: cs.LG

TL;DR: 研究使用神经网络通过触摸屏上手指绘制的数字进行生物特征认证，证明这种方法是可行的。


<details>
  <summary>Details</summary>
Motivation: 探讨基于神经网络的生物识别认证在触摸屏设备上的应用，旨在寻找安全且用户友好的身份验证解决方案。

Method: 评估了CNN和自编码器架构，用户在触摸屏设备上用手指输入简单的数字模式（0-9），比较了两种CNN架构（改进的Inception-V1网络和轻量级浅层CNN）以及卷积和全连接自编码器。

Result: 两种CNN架构都达到了约89%的认证准确率，其中浅层CNN需要的参数更少。自编码器方法的准确率约为75%。

Conclusion: 手指绘制的符号认证为触摸屏设备提供了一种可行的、安全的和用户友好的生物识别解决方案，可以与现有的基于模式的认证方法集成，为移动应用程序创建多层安全系统。

Abstract: This paper investigates neural network-based biometric authentication using finger-drawn digits on touchscreen devices. We evaluated CNN and autoencoder architectures for user authentication through simple digit patterns (0-9) traced with finger input. Twenty participants contributed 2,000 finger-drawn digits each on personal touchscreen devices. We compared two CNN architectures: a modified Inception-V1 network and a lightweight shallow CNN for mobile environments. Additionally, we examined Convolutional and Fully Connected autoencoders for anomaly detection. Both CNN architectures achieved ~89% authentication accuracy, with the shallow CNN requiring fewer parameters. Autoencoder approaches achieved ~75% accuracy. The results demonstrate that finger-drawn symbol authentication provides a viable, secure, and user-friendly biometric solution for touchscreen devices. This approach can be integrated with existing pattern-based authentication methods to create multi-layered security systems for mobile applications.

</details>


### [194] [Virtual Width Networks](https://arxiv.org/abs/2511.11238)
*Seed,Baisheng Li,Banggu Wu,Bole Ma,Bowen Xiao,Chaoyi Zhang,Cheng Li,Chengyi Wang,Chenyin Xu,Chi Zhang,Chong Hu,Daoguang Zan,Defa Zhu,Dongyu Xu,Du Li,Faming Wu,Fan Xia,Ge Zhang,Guang Shi,Haobin Chen,Hongyu Zhu,Hongzhi Huang,Huan Zhou,Huanzhang Dou,Jianhui Duan,Jianqiao Lu,Jianyu Jiang,Jiayi Xu,Jiecao Chen,Jin Chen,Jin Ma,Jing Su,Jingji Chen,Jun Wang,Jun Yuan,Juncai Liu,Jundong Zhou,Kai Hua,Kai Shen,Kai Xiang,Kaiyuan Chen,Kang Liu,Ke Shen,Liang Xiang,Lin Yan,Lishu Luo,Mengyao Zhang,Ming Ding,Mofan Zhang,Nianning Liang,Peng Li,Penghao Huang,Pengpeng Mu,Qi Huang,Qianli Ma,Qiyang Min,Qiying Yu,Renming Pang,Ru Zhang,Shen Yan,Shen Yan,Shixiong Zhao,Shuaishuai Cao,Shuang Wu,Siyan Chen,Siyu Li,Siyuan Qiao,Tao Sun,Tian Xin,Tiantian Fan,Ting Huang,Ting-Han Fan,Wei Jia,Wenqiang Zhang,Wenxuan Liu,Xiangzhong Wu,Xiaochen Zuo,Xiaoying Jia,Ximing Yang,Xin Liu,Xin Yu,Xingyan Bin,Xintong Hao,Xiongcai Luo,Xujing Li,Xun Zhou,Yanghua Peng,Yangrui Chen,Yi Lin,Yichong Leng,Yinghao Li,Yingshuan Song,Yiyuan Ma,Yong Shan,Yongan Xiang,Yonghui Wu,Yongtao Zhang,Yongzhen Yao,Yu Bao,Yuehang Yang,Yufeng Yuan,Yunshui Li,Yuqiao Xian,Yutao Zeng,Yuxuan Wang,Zehua Hong,Zehua Wang,Zengzhi Wang,Zeyu Yang,Zhengqiang Yin,Zhenyi Lu,Zhexi Zhang,Zhi Chen,Zhi Zhang,Zhiqi Lin,Zihao Huang,Zilin Xu,Ziyun Wei,Zuo Wang*

Main category: cs.LG

TL;DR: VWN通过解耦表征宽度和骨干宽度，在几乎保持骨干计算不变的情况下，扩展嵌入空间，实现了更宽表征的优势，且没有增加隐藏层大小的二次成本。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在提供一种方法，能够在不显著增加计算成本的情况下，利用更宽的表征来提升模型性能。

Method: 该论文引入了虚拟宽度网络（VWN），它将表征宽度与骨干宽度解耦，允许在不显著增加计算负担的情况下扩展嵌入空间。

Result: 大规模实验表明，8倍的扩展可以加速优化，next-token预测加速超过2倍，next-2-token预测加速超过3倍。随着训练的进行，优势会放大，损失差距增大，收敛速度比率增加。虚拟宽度和损失减少之间存在近似对数线性关系。

Conclusion: VWN不仅具有token效率，而且随着规模的扩大越来越有效。虚拟宽度缩放可以作为大型模型效率的一个新维度进行探索。

Abstract: We introduce Virtual Width Networks (VWN), a framework that delivers the benefits of wider representations without incurring the quadratic cost of increasing the hidden size. VWN decouples representational width from backbone width, expanding the embedding space while keeping backbone compute nearly constant. In our large-scale experiment, an 8-times expansion accelerates optimization by over 2 times for next-token and 3 times for next-2-token prediction. The advantage amplifies over training as both the loss gap grows and the convergence-speedup ratio increases, showing that VWN is not only token-efficient but also increasingly effective with scale. Moreover, we identify an approximately log-linear scaling relation between virtual width and loss reduction, offering an initial empirical basis and motivation for exploring virtual-width scaling as a new dimension of large-model efficiency.

</details>
