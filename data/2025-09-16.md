<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 46]
- [cs.CV](#cs.CV) [Total: 49]
- [cs.AI](#cs.AI) [Total: 46]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.LG](#cs.LG) [Total: 46]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment](https://arxiv.org/abs/2509.10546)
*Gang Cheng,Haibo Jin,Wenbin Zhang,Haohan Wang,Jun Zhuang*

Main category: cs.CL

TL;DR: 这篇论文探讨了金融领域的大语言模型(LLM)在监管风险方面的漏洞，提出了Risk-Concealment Attacks (RCA)攻击框架和FIN-Bench评估基准，实验表明现有LLM容易被RCA攻击绕过，突显了金融领域LLM对齐技术和审核机制的不足。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLM的有害内容，忽略了金融应用中的监管风险。本文旨在通过红队方法研究金融LLM的漏洞。

Method: 提出了Risk-Concealment Attacks (RCA) 攻击框架，通过迭代地隐藏监管风险，诱导LLM产生看似合规但违反监管规定的回应。构建了FIN-Bench基准，用于系统评估金融领域LLM的安全性。

Result: RCA攻击能够有效绕过九个主流LLM，平均攻击成功率高达93.18%，其中GPT-4.1的攻击成功率为98.28%，OpenAI o1为97.56%。

Conclusion: 目前的大语言模型对齐技术存在严重缺陷，金融领域迫切需要更强的审核机制。这项工作为推进强大且领域感知的大语言模型对齐提供了实践见解。

Abstract: Large Language Models (LLMs) are increasingly integrated into financial
applications, yet existing red-teaming research primarily targets harmful
content, largely neglecting regulatory risks. In this work, we aim to
investigate the vulnerability of financial LLMs through red-teaming approaches.
We introduce Risk-Concealment Attacks (RCA), a novel multi-turn framework that
iteratively conceals regulatory risks to provoke seemingly compliant yet
regulatory-violating responses from LLMs. To enable systematic evaluation, we
construct FIN-Bench, a domain-specific benchmark for assessing LLM safety in
financial contexts. Extensive experiments on FIN-Bench demonstrate that RCA
effectively bypasses nine mainstream LLMs, achieving an average attack success
rate (ASR) of 93.18%, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1.
These findings reveal a critical gap in current alignment techniques and
underscore the urgent need for stronger moderation mechanisms in financial
domains. We hope this work offers practical insights for advancing robust and
domain-aware LLM alignment.

</details>


### [2] [No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes](https://arxiv.org/abs/2509.10625)
*Iván Vicente Moreno Cencerrado,Arnau Padrés Masdemont,Anton Gonzalvez Hawthorne,David Demitri Africa,Lorenzo Pacchiardi*

Main category: cs.CL

TL;DR: 大型语言模型(llm)能否预测自己何时能正确回答问题？


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型是否能够预测自己何时能正确回答问题。

Method: 在读取问题后但在生成任何token之前提取激活，并训练线性探针以预测模型即将给出的答案是否正确。

Result: 在三个开源模型系列（参数范围为 7 到 700 亿）中，在通用琐事问题上训练的关于“预先正确性方向”的预测，可以预测分布内和各种分布外知识数据集的成功，优于黑盒基线和口头预测置信度。预测能力在中间层饱和，表明自我评估出现在计算中期。值得注意的是，泛化在需要数学推理的问题上会动摇。此外，对于回答“我不知道”的模型，这样做与探针分数密切相关，表明同一方向也捕获了置信度。

Conclusion: 通过补充先前使用探针和稀疏自动编码器获得的关于真实性和其他行为的结果，这项工作为阐明 LLM 内部原理做出了重要发现。

Abstract: Do large language models (LLMs) anticipate when they will answer correctly?
To study this, we extract activations after a question is read but before any
tokens are generated, and train linear probes to predict whether the model's
forthcoming answer will be correct. Across three open-source model families
ranging from 7 to 70 billion parameters, projections on this "in-advance
correctness direction" trained on generic trivia questions predict success in
distribution and on diverse out-of-distribution knowledge datasets,
outperforming black-box baselines and verbalised predicted confidence.
Predictive power saturates in intermediate layers, suggesting that
self-assessment emerges mid-computation. Notably, generalisation falters on
questions requiring mathematical reasoning. Moreover, for models responding "I
don't know", doing so strongly correlates with the probe score, indicating that
the same direction also captures confidence. By complementing previous results
on truthfulness and other behaviours obtained with probes and sparse
auto-encoders, our work contributes essential findings to elucidate LLM
internals.

</details>


### [3] [Interdisciplinary Research in Conversation: A Case Study in Computational Morphology for Language Documentation](https://arxiv.org/abs/2509.10644)
*Enora Rice,Katharina von der Wense,Alexis Palmer*

Main category: cs.CL

TL;DR: 计算形态学在语言记录中的应用有限，因为与实际需求脱节。建议采用以用户为中心的设计(UCD)来改进研究方向。


<details>
  <summary>Details</summary>
Motivation: 计算形态学在语言记录中有潜力，但实际应用有限。研究与实践之间存在脱节。

Method: 通过案例研究GlossLM，一个多语言IGT生成模型，进行小规模用户研究。

Result: 尽管GlossLM在指标上表现出色，但在实际文档环境中未能满足核心可用性需求。

Conclusion: 以用户为中心不仅能产生更有效的工具，还能提出更丰富、更相关的研究方向。

Abstract: Computational morphology has the potential to support language documentation
through tasks like morphological segmentation and the generation of Interlinear
Glossed Text (IGT). However, our research outputs have seen limited use in
real-world language documentation settings. This position paper situates the
disconnect between computational morphology and language documentation within a
broader misalignment between research and practice in NLP and argues that the
field risks becoming decontextualized and ineffectual without systematic
integration of User-Centered Design (UCD). To demonstrate how principles from
UCD can reshape the research agenda, we present a case study of GlossLM, a
state-of-the-art multilingual IGT generation model. Through a small-scale user
study with three documentary linguists, we find that despite strong metric
based performance, the system fails to meet core usability needs in real
documentation contexts. These insights raise new research questions around
model constraints, label standardization, segmentation, and personalization. We
argue that centering users not only produces more effective tools, but surfaces
richer, more relevant research directions

</details>


### [4] [Context Copying Modulation: The Role of Entropy Neurons in Managing Parametric and Contextual Knowledge Conflicts](https://arxiv.org/abs/2509.10663)
*Zineddine Tighidet,Andrea Mogini,Hedi Ben-younes,Jiali Mei,Patrick Gallinari,Benjamin Piwowarski*

Main category: cs.CL

TL;DR: 研究大型语言模型在面对与其内部参数知识相冲突的上下文信息时的行为不一致问题，并探究熵神经元在抑制上下文复制行为中的作用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理冲突信息时行为不一致，缺乏普遍接受的解释。

Method: 通过观察熵神经元在解决上下文信息和参数信息冲突中的作用，研究其在抑制Transformer模型上下文复制行为中的角色。

Result: 熵神经元负责抑制多种大型语言模型的上下文复制，消融这些神经元会导致生成过程的显著变化。

Conclusion: 研究结果加深了我们对大型语言模型在处理冲突信息时内部动态的理解。

Abstract: The behavior of Large Language Models (LLMs) when facing contextual
information that conflicts with their internal parametric knowledge is
inconsistent, with no generally accepted explanation for the expected outcome
distribution. Recent work has identified in autoregressive transformer models a
class of neurons -- called entropy neurons -- that produce a significant effect
on the model output entropy while having an overall moderate impact on the
ranking of the predicted tokens. In this paper, we investigate the preliminary
claim that these neurons are involved in inhibiting context copying behavior in
transformers by looking at their role in resolving conflicts between contextual
and parametric information. We show that entropy neurons are responsible for
suppressing context copying across a range of LLMs, and that ablating them
leads to a significant change in the generation process. These results enhance
our understanding of the internal dynamics of LLMs when handling conflicting
information.

</details>


### [5] [Pluralistic Alignment for Healthcare: A Role-Driven Framework](https://arxiv.org/abs/2509.10685)
*Jiayou Zhong,Anudeex Shetty,Chao Jia,Xuanrui Lin,Usman Naseem*

Main category: cs.CL

TL;DR: 提出了一种轻量级的、通用的、多元对齐方法，旨在模拟不同的视角和价值观。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型的输出反映不同人群所持有的不同价值观和观点，这在医疗保健等敏感领域至关重要。现有的对齐方法，包括像模块化多元主义这样的多元范式，通常在健康领域表现不足，因为个人、文化和情境因素会影响多元主义。

Method: 提出 EthosAgents，一种轻量级的、通用的、多元对齐方法，旨在模拟不同的视角和价值观。

Result: 实验结果表明，该方法在所有三种模式的七个不同大小的开放和封闭模型中，都促进了多元对齐。

Conclusion: 与健康相关的多元主义需要适应性强且具有规范意识的方法，从而深入了解这些模型如何在其他高风险领域更好地尊重多样性。

Abstract: As large language models are increasingly deployed in sensitive domains such
as healthcare, ensuring their outputs reflect the diverse values and
perspectives held across populations is critical. However, existing alignment
approaches, including pluralistic paradigms like Modular Pluralism, often fall
short in the health domain, where personal, cultural, and situational factors
shape pluralism. Motivated by the aforementioned healthcare challenges, we
propose a first lightweight, generalizable, pluralistic alignment approach,
EthosAgents, designed to simulate diverse perspectives and values. We
empirically show that it advances the pluralistic alignment for all three modes
across seven varying-sized open and closed models. Our findings reveal that
health-related pluralism demands adaptable and normatively aware approaches,
offering insights into how these models can better respect diversity in other
high-stakes domains.

</details>


### [6] [Struct-Bench: A Benchmark for Differentially Private Structured Text Generation](https://arxiv.org/abs/2509.10696)
*Shuaiqi Wang,Vikas Raunak,Arturs Backurs,Victor Reis,Pei Zhou,Sihao Chen,Longqi Yang,Zinan Lin,Sergey Yekhanin,Giulia Fanti*

Main category: cs.CL

TL;DR: 提出Struct-Bench，一个用于评估包含自然语言数据的结构化数据集的合成数据集的框架和基准。


<details>
  <summary>Details</summary>
Motivation: 现有的合成数据评估技术难以捕捉此类数据集的结构属性和相关性。

Method: 要求用户提供数据集结构的上下文无关文法（CFG）表示。

Result: 包含5个真实世界和2个合成生成的数据集的基准，每个数据集都用CFG注释。这些数据集对最先进的DP合成数据生成方法提出了巨大的挑战。

Conclusion: Struct-Bench提供了一个标准化的评估平台，以对隐私保护的合成数据生成方法进行基准测试和研究。案例研究表明如何使用Struct-Bench来提高结构化数据上Private Evolution（PE）的合成数据质量。

Abstract: Differentially private (DP) synthetic data generation is a promising
technique for utilizing private datasets that otherwise cannot be exposed for
model training or other analytics. While much research literature has focused
on generating private unstructured text and image data, in enterprise settings,
structured data (e.g., tabular) is more common, often including natural
language fields or components. Existing synthetic data evaluation techniques
(e.g., FID) struggle to capture the structural properties and correlations of
such datasets. In this work, we propose Struct-Bench, a framework and benchmark
for evaluating synthetic datasets derived from structured datasets that contain
natural language data. The Struct-Bench framework requires users to provide a
representation of their dataset structure as a Context-Free Grammar (CFG). Our
benchmark comprises 5 real-world and 2 synthetically generated datasets, each
annotated with CFGs. We show that these datasets demonstrably present a great
challenge even for state-of-the-art DP synthetic data generation methods.
Struct-Bench also includes reference implementations of different metrics and a
leaderboard, thereby providing researchers a standardized evaluation platform
to benchmark and investigate privacy-preserving synthetic data generation
methods. Further, we also present a case study showing how to use Struct-Bench
to improve the synthetic data quality of Private Evolution (PE) on structured
data. The benchmark and the leaderboard have been publicly made available at
https://struct-bench.github.io.

</details>


### [7] [A Survey on Retrieval And Structuring Augmented Generation with Large Language Models](https://arxiv.org/abs/2509.10697)
*Pengcheng Jiang,Siru Ouyang,Yizhu Jiao,Ming Zhong,Runchu Tian,Jiawei Han*

Main category: cs.CL

TL;DR: RAS通过整合动态信息检索与结构化知识表示，增强了大型语言模型在实际应用中的能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在文本生成和推理方面表现出色，但在实际应用中面临幻觉生成、知识过时和领域专业知识有限等挑战。

Method: 本文调查了检索机制（包括稀疏、密集和混合方法），探讨了文本结构化技术（如分类构建、分层分类和信息提取），并研究了这些结构化表示如何通过提示方法、推理框架和知识嵌入技术与大型语言模型集成。

Result: 本文全面概述了RAS方法，应用和未来方向，为研究人员和从业人员提供了见解。

Conclusion: 本文确定了检索效率、结构质量和知识集成方面的技术挑战，同时强调了多模态检索、跨语言结构和交互系统中的研究机会。

Abstract: Large Language Models (LLMs) have revolutionized natural language processing
with their remarkable capabilities in text generation and reasoning. However,
these models face critical challenges when deployed in real-world applications,
including hallucination generation, outdated knowledge, and limited domain
expertise. Retrieval And Structuring (RAS) Augmented Generation addresses these
limitations by integrating dynamic information retrieval with structured
knowledge representations. This survey (1) examines retrieval mechanisms
including sparse, dense, and hybrid approaches for accessing external
knowledge; (2) explore text structuring techniques such as taxonomy
construction, hierarchical classification, and information extraction that
transform unstructured text into organized representations; and (3) investigate
how these structured representations integrate with LLMs through prompt-based
methods, reasoning frameworks, and knowledge embedding techniques. It also
identifies technical challenges in retrieval efficiency, structure quality, and
knowledge integration, while highlighting research opportunities in multimodal
retrieval, cross-lingual structures, and interactive systems. This
comprehensive overview provides researchers and practitioners with insights
into RAS methods, applications, and future directions.

</details>


### [8] [SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation](https://arxiv.org/abs/2509.10708)
*Iman Barati,Mostafa Amiri,Heshaam Faili*

Main category: cs.CL

TL;DR: 提出了一种名为 SearchInstruct 的新方法，用于构建高质量的 SFT 指令数据集。


<details>
  <summary>Details</summary>
Motivation: 为特定领域创建合适的训练数据集仍然具有挑战性，因为领域约束和数据稀缺。

Method: 该方法从一组有限的领域特定的人工生成问题开始，使用大型语言模型进行扩展，然后检索领域相关资源，为每个增强问题生成准确且上下文相关的答案。

Result: 实验评估表明，SearchInstruct 提高了 SFT 数据集的多样性和质量，从而显著提高了 LLM 在专业领域中的性能。

Conclusion: SearchInstruct 不仅可以生成数据集，还可以有效地促进模型编辑等任务，从而可以对现有模型进行高效更新。

Abstract: Supervised Fine-Tuning (SFT) is essential for training large language models
(LLMs), significantly enhancing critical capabilities such as instruction
following and in-context learning. Nevertheless, creating suitable training
datasets tailored for specific domains remains challenging due to unique domain
constraints and data scarcity. In this paper, we propose SearchInstruct, an
innovative method explicitly designed to construct high quality instruction
datasets for SFT. Our approach begins with a limited set of domain specific,
human generated questions, which are systematically expanded using a large
language model. Subsequently, domain relevant resources are dynamically
retrieved to generate accurate and contextually appropriate answers for each
augmented question. Experimental evaluation demonstrates that SearchInstruct
enhances both the diversity and quality of SFT datasets, leading to measurable
improvements in LLM performance within specialized domains. Additionally, we
show that beyond dataset generation, the proposed method can also effectively
facilitate tasks such as model editing, enabling efficient updates to existing
models. To facilitate reproducibility and community adoption, we provide full
implementation details, the complete set of generated instruction response
pairs, and the source code in a publicly accessible Git repository:
[https://github.com/mostafaamiri/SearchInstruct](https://github.com/mostafaamiri/SearchInstruct)

</details>


### [9] [PolyTruth: Multilingual Disinformation Detection using Transformer-Based Language Models](https://arxiv.org/abs/2509.10737)
*Zaur Gouliev,Jennifer Waters,Chengqian Wang*

Main category: cs.CL

TL;DR: 本文对比了五种多语言Transformer模型在跨语言虚假信息检测任务中的表现，并提出了一个新的多语言数据集PolyTruth Disinfo Corpus。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型大多只在英语数据上进行评估，缺乏对跨语言虚假信息检测能力的研究。

Method: 在PolyTruth Disinfo Corpus数据集上，对mBERT, XLM, XLM-RoBERTa, RemBERT, mT5五种模型进行了虚假信息分类任务的对比实验。

Result: 实验结果表明，RemBERT在低资源语言上表现更好，而mBERT和XLM在训练数据稀缺时表现不佳。

Conclusion: 本文揭示了AI系统在多语言虚假信息检测方面的潜力和局限性，并发布了数据集以促进进一步研究。

Abstract: Disinformation spreads rapidly across linguistic boundaries, yet most AI
models are still benchmarked only on English. We address this gap with a
systematic comparison of five multilingual transformer models: mBERT, XLM,
XLM-RoBERTa, RemBERT, and mT5 on a common fake-vs-true machine learning
classification task. While transformer-based language models have demonstrated
notable success in detecting disinformation in English, their effectiveness in
multilingual contexts still remains up for debate. To facilitate evaluation, we
introduce PolyTruth Disinfo Corpus, a novel corpus of 60,486 statement pairs
(false claim vs. factual correction) spanning over twenty five languages that
collectively cover five language families and a broad topical range from
politics, health, climate, finance, and conspiracy, half of which are
fact-checked disinformation claims verified by an augmented MindBugs Discovery
dataset. Our experiments revealed performance variations. Models such as
RemBERT achieved better overall accuracy, particularly excelling in
low-resource languages, whereas models like mBERT and XLM exhibit considerable
limitations when training data is scarce. We provide a discussion of these
performance patterns and implications for real-world deployment. The dataset is
publicly available on our GitHub repository to encourage further
experimentation and advancement. Our findings illuminate both the potential and
the current limitations of AI systems for multilingual disinformation
detection.

</details>


### [10] [Reasoning Under Uncertainty: Exploring Probabilistic Reasoning Capabilities of LLMs](https://arxiv.org/abs/2509.10739)
*Mobina Pournemat,Keivan Rezaei,Gaurang Sriramanan,Arman Zarei,Jiaxiang Fu,Yang Wang,Hamid Eghbalzadeh,Soheil Feizi*

Main category: cs.CL

TL;DR: 大型语言模型在需要概率推理的任务中表现出不清晰和不一致的行为。


<details>
  <summary>Details</summary>
Motivation: 本文旨在全面研究大型语言模型在显式离散概率分布上的推理能力。

Method: 通过精心设计的三个任务评估模型：模式识别、最大似然估计和样本生成，并提示模型回答关于联合分布或条件分布的查询。

Result: 较大型号表现出更强的推理能力和令人惊讶的样本生成能力。结果还表明，模型对概率结果的表示符号的变异敏感，并且随着上下文长度的增加，性能会下降超过 60%。

Conclusion: 研究结果详细了解了大型语言模型的概率推理能力，并确定了未来改进的关键方向。

Abstract: Despite widespread success in language understanding and generation, large
language models (LLMs) exhibit unclear and often inconsistent behavior when
faced with tasks that require probabilistic reasoning. In this work, we present
the first comprehensive study of the reasoning capabilities of LLMs over
explicit discrete probability distributions. Given observations from a
probability distribution, we evaluate models on three carefully designed tasks,
mode identification, maximum likelihood estimation, and sample generation, by
prompting them to provide responses to queries about either the joint
distribution or its conditionals. These tasks thus probe a range of
probabilistic skills, including frequency analysis, marginalization, and
generative behavior. Through comprehensive empirical evaluations, we
demonstrate that there exists a clear performance gap between smaller and
larger models, with the latter demonstrating stronger inference and surprising
capabilities in sample generation. Furthermore, our investigations reveal
notable limitations, including sensitivity to variations in the notation
utilized to represent probabilistic outcomes and performance degradation of
over 60% as context length increases. Together, our results provide a detailed
understanding of the probabilistic reasoning abilities of LLMs and identify key
directions for future improvement.

</details>


### [11] [Automated MCQA Benchmarking at Scale: Evaluating Reasoning Traces as Retrieval Sources for Domain Adaptation of Small Language Models](https://arxiv.org/abs/2509.10744)
*Ozan Gokdemir,Neil Getty,Robert Underwood,Sandeep Madireddy,Franck Cappello,Arvind Ramanathan,Ian T. Foster,Rick L. Stevens*

Main category: cs.CL

TL;DR: 提出了一种可扩展的模块化框架，用于直接从大型科学论文语料库生成多项选择问答 (MCQA) 基准。


<details>
  <summary>Details</summary>
Motivation: 随着科学知识以空前的速度增长，评估基准必须不断发展，以反映新的发现，并确保语言模型在当前的各种文献上进行测试。

Method: 该流程自动化了 MCQA 创建的每个阶段，包括 PDF 解析、语义分块、问题生成和模型评估。

Result: 从放射和癌症生物学中的 22,000 篇开放获取文章中生成了超过 16,000 个 MCQ。推理追踪检索持续提高了合成和专家注释基准的性能，使几个小型模型在 2023 年 Astro 放射和癌症生物学考试中超过了 GPT-4。

Conclusion: 推理追踪检索持续提高了合成和专家注释基准的性能，使几个小型模型在 2023 年 Astro 放射和癌症生物学考试中超过了 GPT-4。

Abstract: As scientific knowledge grows at an unprecedented pace, evaluation benchmarks
must evolve to reflect new discoveries and ensure language models are tested on
current, diverse literature. We propose a scalable, modular framework for
generating multiple-choice question-answering (MCQA) benchmarks directly from
large corpora of scientific papers. Our pipeline automates every stage of MCQA
creation, including PDF parsing, semantic chunking, question generation, and
model evaluation. As a case study, we generate more than 16,000 MCQs from
22,000 open-access articles in radiation and cancer biology. We then evaluate a
suite of small language models (1.1B-14B parameters) on these questions,
comparing baseline accuracy with retrieval-augmented generation (RAG) from
paper-derived semantic chunks and from reasoning traces distilled from GPT-4.1.
We find that reasoning-trace retrieval consistently improves performance on
both synthetic and expert-annotated benchmarks, enabling several small models
to surpass GPT-4 on the 2023 Astro Radiation and Cancer Biology exam.

</details>


### [12] [RanAT4BIE: Random Adversarial Training for Biomedical Information Extraction](https://arxiv.org/abs/2509.11191)
*Jian Chen,Shengyi Lv,Leilei Su*

Main category: cs.CL

TL;DR: 提出了随机对抗训练 (RAT)，一个用于生物医学信息提取 (BioIE) 任务的新框架。


<details>
  <summary>Details</summary>
Motivation: 验证传统对抗训练在增强预训练语言模型在 BioIE 任务中的性能方面的有效性，并解决对抗训练引入的大量计算开销这一限制。

Method: 将随机抽样机制与对抗训练原则相结合。

Result: RAT 在 BioIE 任务中表现出优于基线模型的性能。

Conclusion: RAT 有潜力成为生物医学自然语言处理的变革性框架，为模型性能和计算效率提供平衡的解决方案。

Abstract: We introduce random adversarial training (RAT), a novel framework
successfully applied to biomedical information extraction (BioIE) tasks.
Building on PubMedBERT as the foundational architecture, our study first
validates the effectiveness of conventional adversarial training in enhancing
pre-trained language models' performance on BioIE tasks. While adversarial
training yields significant improvements across various performance metrics, it
also introduces considerable computational overhead. To address this
limitation, we propose RAT as an efficiency solution for biomedical information
extraction. This framework strategically integrates random sampling mechanisms
with adversarial training principles, achieving dual objectives: enhanced model
generalization and robustness while significantly reducing computational costs.
Through comprehensive evaluations, RAT demonstrates superior performance
compared to baseline models in BioIE tasks. The results highlight RAT's
potential as a transformative framework for biomedical natural language
processing, offering a balanced solution to the model performance and
computational efficiency.

</details>


### [13] [RECAP: Transparent Inference-Time Emotion Alignment for Medical Dialogue Systems](https://arxiv.org/abs/2509.10746)
*Adarsh Srinivasan,Jacob Dineen,Muhammad Umar Afzal,Muhammad Uzair Sarfraz,Irbaz B. Riaz,Ben Zhou*

Main category: cs.CL

TL;DR: 医疗领域的大型语言模型常常缺乏关键的情感线索，导致建议在医学上可行但情感上平淡。RECAP 框架通过将共情分解为透明的评估理论阶段并暴露每个维度的 Likert 信号，从而产生细致入微、可审计的回应，以此在不重新训练的情况下增加结构化的情感推理。


<details>
  <summary>Details</summary>
Motivation: 在临床环境中，患者常常感到痛苦和脆弱，需要富有同情心的沟通来支持安全、依从性和信任。现有的大型语言模型在情感表达方面存在不足。

Method: 提出了 RECAP 框架，该框架在推理时增加结构化的情感推理，无需重新训练。RECAP 将共情分解为透明的评估理论阶段，并暴露每个维度的 Likert 信号。

Result: 在 EmoBench、SECEU 和 EQ-Bench 数据集上，RECAP 将 8B 模型的的情感推理能力提高了 22-28%，将更大的模型的情感推理能力提高了 10-13%（相比于零样本基线）。临床医生评估进一步证实了 RECAP 具有卓越的共情沟通能力。

Conclusion: RECAP 表明，模块化、基于理论的提示可以系统地增强医疗 AI 的情商，同时保留部署所需的责任性。

Abstract: Large language models in healthcare often miss critical emotional cues,
delivering medically sound but emotionally flat advice. This is especially
problematic in clinical contexts where patients are distressed and vulnerable,
and require empathic communication to support safety, adherence, and trust. We
present RECAP (Reflect-Extract-Calibrate-Align-Produce), an inference-time
framework that adds structured emotional reasoning without retraining. By
decomposing empathy into transparent appraisal-theoretic stages and exposing
per-dimension Likert signals, RECAP produces nuanced, auditable responses.
Across EmoBench, SECEU, and EQ-Bench, RECAP improves emotional reasoning by
22-28% on 8B models and 10-13% on larger models over zero-shot baselines.
Clinician evaluations further confirm superior empathetic communication. RECAP
shows that modular, theory-grounded prompting can systematically enhance
emotional intelligence in medical AI while preserving the accountability
required for deployment.

</details>


### [14] [Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction](https://arxiv.org/abs/2509.10798)
*Yijun Liu,Yixuan Wang,Yuzhuang Xu,Shiyu Ji,Yang Xu,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: This paper introduces Judge Q, a novel training method that improves KV cache eviction in large language models by incorporating a soft token list to capture global information, leading to better performance with minimal training overhead.


<details>
  <summary>Details</summary>
Motivation: Current KV cache eviction methods in LLMs overly focus on local information, neglecting crucial global information and affecting memory usage and decoding efficiency.

Method: The paper proposes Judge Q, a training method that uses a soft token list concatenated to the input sequence to train the model's embedding layer, aligning the attention map with that of actual decoded tokens to capture global information.

Result: Experiments on Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3 show that Judge Q improves performance by approximately 1 point on LongBench and over 3 points on RULER compared to existing eviction approaches.

Conclusion: Judge Q effectively enhances performance in KV cache eviction scenarios and can be seamlessly integrated into existing open-source models with minimal training overhead.

Abstract: Large language models (LLMs) utilize key-value (KV) cache to store historical
information during sequence processing. The size of KV cache grows linearly as
the length of the sequence extends, which seriously affects memory usage and
decoding efficiency. Current methods for KV cache eviction typically utilize
the last window from the pre-filling phase as queries to compute the KV
importance scores for eviction. Although this scheme is simple to implement, it
tends to overly focus on local information, potentially leading to the neglect
or omission of crucial global information. To mitigate this issue, we propose
Judge Q, a novel training method which incorporates a soft token list. This
method only tunes the model's embedding layer at a low training cost. By
concatenating the soft token list at the end of the input sequence, we train
these tokens' attention map to the original input sequence to align with that
of the actual decoded tokens. In this way, the queries corresponding to the
soft tokens can effectively capture global information and better evaluate the
importance of the keys and values within the KV cache, thus maintaining
decoding quality when KV cache is evicted. Under the same eviction budget, our
method exhibits less performance degradation compared to existing eviction
approaches. We validate our approach through experiments conducted on models
such as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks
including LongBench, RULER, and Needle-in-a-Haystack. Results indicate an
improvement of approximately 1 point on the LongBench and over 3 points on
RULER. This proposed methodology can be seamlessly integrated into existing
open-source models with minimal training overhead, thereby enhancing
performance in KV cache eviction scenarios.

</details>


### [15] [Towards Automated Error Discovery: A Study in Conversational AI](https://arxiv.org/abs/2509.10833)
*Dominic Petrak,Thy Thy Tran,Iryna Gurevych*

Main category: cs.CL

TL;DR: 论文提出了一种名为SEEED的自动错误发现框架，用于检测对话AI中的错误，尤其擅长识别未明确指定的错误。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在检测对话AI错误方面存在局限性，无法有效识别由于模型更新或用户行为变化引起的错误。

Method: 通过增强软近邻损失和引入基于标签的样本排序，SEEED使用基于编码器的方法来实现错误检测。

Result: SEEED在多个对话数据集上优于包括GPT-4o和Phi-4在内的基线模型，在检测未知错误方面的准确率提高了8个百分点。

Conclusion: SEEED框架能够有效检测对话AI中的未知错误，并具有良好的泛化能力。

Abstract: Although LLM-based conversational agents demonstrate strong fluency and
coherence, they still produce undesirable behaviors (errors) that are
challenging to prevent from reaching users during deployment. Recent research
leverages large language models (LLMs) to detect errors and guide
response-generation models toward improvement. However, current LLMs struggle
to identify errors not explicitly specified in their instructions, such as
those arising from updates to the response-generation model or shifts in user
behavior. In this work, we introduce Automated Error Discovery, a framework for
detecting and defining errors in conversational AI, and propose SEEED (Soft
Clustering Extended Encoder-Based Error Detection), as an encoder-based
approach to its implementation. We enhance the Soft Nearest Neighbor Loss by
amplifying distance weighting for negative samples and introduce Label-Based
Sample Ranking to select highly contrastive examples for better representation
learning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 --
across multiple error-annotated dialogue datasets, improving the accuracy for
detecting unknown errors by up to 8 points and demonstrating strong
generalization to unknown intent detection.

</details>


### [16] [Evaluating Large Language Models for Evidence-Based Clinical Question Answering](https://arxiv.org/abs/2509.10843)
*Can Wang,Yiqun Chen*

Main category: cs.CL

TL;DR: 大型语言模型在生物医学和临床应用中取得了显著进展，但回答细致的、基于证据的问题的能力仍需评估。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型回答基于证据的临床问题的能力。

Method: 构建一个多来源的基准，包括Cochrane系统评价、临床指南以及来自美国心脏协会的结构化建议和保险公司的叙述性指导。使用GPT-4o-mini和GPT-5进行测试。

Result: 在结构化指南建议上的准确率最高（90%），在叙述性指南和系统评价问题上的准确率较低（60-70%）。准确性与系统评价的引用次数之间存在很强的相关性。检索增强提示可以提高准确性。

Conclusion: 大型语言模型在循证临床问题回答方面既有希望也有局限性。检索增强提示是一种提高事实准确性和与来源证据对齐的有效策略。

Abstract: Large Language Models (LLMs) have demonstrated substantial progress in
biomedical and clinical applications, motivating rigorous evaluation of their
ability to answer nuanced, evidence-based questions. We curate a multi-source
benchmark drawing from Cochrane systematic reviews and clinical guidelines,
including structured recommendations from the American Heart Association and
narrative guidance used by insurers. Using GPT-4o-mini and GPT-5, we observe
consistent performance patterns across sources and clinical domains: accuracy
is highest on structured guideline recommendations (90%) and lower on narrative
guideline and systematic review questions (60--70%). We also find a strong
correlation between accuracy and the citation count of the underlying
systematic reviews, where each doubling of citations is associated with roughly
a 30% increase in the odds of a correct answer. Models show moderate ability to
reason about evidence quality when contextual information is supplied. When we
incorporate retrieval-augmented prompting, providing the gold-source abstract
raises accuracy on previously incorrect items to 0.79; providing top 3 PubMed
abstracts (ranked by semantic relevance) improves accuracy to 0.23, while
random abstracts reduce accuracy (0.10, within temperature variation). These
effects are mirrored in GPT-4o-mini, underscoring that source clarity and
targeted retrieval -- not just model size -- drive performance. Overall, our
results highlight both the promise and current limitations of LLMs for
evidence-based clinical question answering. Retrieval-augmented prompting
emerges as a useful strategy to improve factual accuracy and alignment with
source evidence, while stratified evaluation by specialty and question type
remains essential to understand current knowledge access and to contextualize
model performance.

</details>


### [17] [GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings](https://arxiv.org/abs/2509.10844)
*Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: 提出了一种新的剪枝框架GAPrune，用于压缩领域特定嵌入模型，同时保持或提高其性能。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法无法区分通用语义表示和领域特定模式，导致剪枝效果不佳，阻碍了领域特定嵌入模型在资源受限环境中的部署。

Method: 使用Fisher信息测量重要性，通用领域梯度对齐评估参数行为，结合领域对齐重要性（DAI）评分来指导剪枝。

Result: 在FinMTEB和ChemTEB两个领域基准测试中，GAPrune在50%稀疏度下，单次剪枝后性能保持在密集模型的2.5%以内，并且优于所有基线模型。经过100步的再训练，GAPrune在FinMTEB上提高了+4.51%，在ChemTEB上提高了+1.73%。

Conclusion: 提出的GAPrune剪枝策略可以实现模型压缩和增强领域专业化。

Abstract: Domain-specific embedding models have shown promise for applications that
require specialized semantic understanding, such as coding agents and financial
retrieval systems, often achieving higher performance gains than general
models. However, state-of-the-art embedding models are typically based on LLMs,
which contain billions of parameters, making deployment challenging in
resource-constrained environments. Model compression through pruning offers a
promising solution, but existing pruning methods treat all parameters
uniformly, failing to distinguish between general semantic representations and
domain-specific patterns, leading to suboptimal pruning decisions. Thus, we
propose GAPrune, a pruning framework that addresses this challenge by
considering both domain importance and preserving general linguistic
foundation. Our method uses Fisher Information to measure importance and
general-domain gradient alignment to assess parameter behavior, then combines
these signals using our Domain Alignment Importance (DAI) scoring. Lower DAI
scores indicate that the parameter is either less important for the domain task
or creates conflicts between domain and general objectives. Experiments on two
domain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance
within 2.5% of dense models in one-shot pruning at 50% sparsity, while
outperforming all baselines. With retraining in 100 steps, GAPrune achieves
+4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our
pruning strategy not only preserves but enhances domain-specific capabilities.
Our findings demonstrate that principled pruning strategies can achieve model
compression and enhanced domain specialization, providing the research
community with a new approach for development.

</details>


### [18] [Text2Sign Diffusion: A Generative Approach for Gloss-Free Sign Language Production](https://arxiv.org/abs/2509.10845)
*Liqian Feng,Lintao Wang,Kun Hu,Dehui Kong,Zhiyong Wang*

Main category: cs.CL

TL;DR: 提出了一种名为 Text2Sign Diffusion (Text2SignDiff) 的新型基于扩散的生成方法，用于无 gloss 的 SLP。


<details>
  <summary>Details</summary>
Motivation: 现有的 SLP 方法通常依赖于 gloss，这限制了 SLP 的灵活性和泛化性，因为 gloss 注释通常不可用且特定于语言。

Method: 提出了一种无 gloss 的潜在扩散模型，用于从噪声潜在符号代码和口语文本联合生成符号语言序列，通过非自回归迭代去噪过程减少潜在的误差累积。我们还设计了一种跨模态符号对齐器，它学习共享的潜在空间，以桥接符号和口语中的视觉和文本内容。

Result: 在常用的 PHOENIX14T 和 How2Sign 数据集上进行的大量实验表明，该方法的有效性，实现了最先进的性能。

Conclusion: 该方法在无 gloss 符号语言生成任务上表现出色。

Abstract: Sign language production (SLP) aims to translate spoken language sentences
into a sequence of pose frames in a sign language, bridging the communication
gap and promoting digital inclusion for deaf and hard-of-hearing communities.
Existing methods typically rely on gloss, a symbolic representation of sign
language words or phrases that serves as an intermediate step in SLP. This
limits the flexibility and generalization of SLP, as gloss annotations are
often unavailable and language-specific. Therefore, we present a novel
diffusion-based generative approach - Text2Sign Diffusion (Text2SignDiff) for
gloss-free SLP. Specifically, a gloss-free latent diffusion model is proposed
to generate sign language sequences from noisy latent sign codes and spoken
text jointly, reducing the potential error accumulation through a
non-autoregressive iterative denoising process. We also design a cross-modal
signing aligner that learns a shared latent space to bridge visual and textual
content in sign and spoken languages. This alignment supports the conditioned
diffusion-based process, enabling more accurate and contextually relevant sign
language generation without gloss. Extensive experiments on the commonly used
PHOENIX14T and How2Sign datasets demonstrate the effectiveness of our method,
achieving the state-of-the-art performance.

</details>


### [19] [A funny companion: Distinct neural responses to perceived AI- versus human- generated humor](https://arxiv.org/abs/2509.10847)
*Xiaohui Rao,Hanlin Wu,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 该研究使用脑电图（EEG）比较了人们对来自AI和人类的幽默的处理方式。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能伴侣变得能够进行类人交流，了解人们在认知和情感上对人工智能幽默的反应变得越来越重要。

Method: 使用脑电图（EEG）比较人们对来自AI和人类的幽默的处理方式, 行为分析

Result: 行为分析表明，参与者认为人工智能和人类的幽默具有相当的趣味性。然而，神经生理学数据显示，人工智能的幽默引发的N400效应较小，表明在不协调处理过程中认知努力减少。伴随而来的是更大的晚期正电位（LPP），表明更大程度的惊讶和情感反应。人类的幽默表现出习惯化效应，随着时间的推移，N400增加，LPP减少。相比之下，人工智能的幽默表现出越来越高的处理效率和情感回报，N400减少，LPP增加。

Conclusion: 大脑对人工智能的幽默反应出人意料地积极和强烈，突显了幽默在促进人机社交互动中真正参与的潜力。

Abstract: As AI companions become capable of human-like communication, including
telling jokes, understanding how people cognitively and emotionally respond to
AI humor becomes increasingly important. This study used electroencephalography
(EEG) to compare how people process humor from AI versus human sources.
Behavioral analysis revealed that participants rated AI and human humor as
comparably funny. However, neurophysiological data showed that AI humor
elicited a smaller N400 effect, suggesting reduced cognitive effort during the
processing of incongruity. This was accompanied by a larger Late Positive
Potential (LPP), indicating a greater degree of surprise and emotional
response. This enhanced LPP likely stems from the violation of low initial
expectations regarding AI's comedic capabilities. Furthermore, a key temporal
dynamic emerged: human humor showed habituation effects, marked by an
increasing N400 and a decreasing LPP over time. In contrast, AI humor
demonstrated increasing processing efficiency and emotional reward, with a
decreasing N400 and an increasing LPP. This trajectory reveals how the brain
can dynamically update its predictive model of AI capabilities. This process of
cumulative reinforcement challenges "algorithm aversion" in humor, as it
demonstrates how cognitive adaptation to AI's language patterns can lead to an
intensified emotional reward. Additionally, participants' social attitudes
toward AI modulated these neural responses, with higher perceived AI
trustworthiness correlating with enhanced emotional engagement. These findings
indicate that the brain responds to AI humor with surprisingly positive and
intense reactions, highlighting humor's potential for fostering genuine
engagement in human-AI social interaction.

</details>


### [20] [Pre-Storage Reasoning for Episodic Memory: Shifting Inference Burden to Memory for Personalized Dialogue](https://arxiv.org/abs/2509.10852)
*Sangyeop Kim,Yohan Lee,Sanghwa Kim,Hyunjong Kim,Sungzoon Cho*

Main category: cs.CL

TL;DR: PREMem通过将复杂推理从推理转移到记忆构建，改进了会话AI中的长期记忆。


<details>
  <summary>Details</summary>
Motivation: 当前的会话AI系统在响应生成方面负担过重，性能过度依赖模型大小。

Method: PREMem提取细粒度的记忆片段，将其分类为事实性、体验性和主观性信息，并建立跨会话的记忆项目之间的显式关系，捕获扩展、转换和含义等演变模式。

Result: 实验表明，所有模型大小的性能都有显著提高，较小的模型可以达到与更大的基线相当的结果，同时在有限的token预算下保持有效性。

Conclusion: PREMem通过在预存储期间执行推理，创建丰富的表示，同时减少交互期间的计算需求。

Abstract: Effective long-term memory in conversational AI requires synthesizing
information across multiple sessions. However, current systems place excessive
reasoning burden on response generation, making performance significantly
dependent on model sizes. We introduce PREMem (Pre-storage Reasoning for
Episodic Memory), a novel approach that shifts complex reasoning processes from
inference to memory construction. PREMem extracts fine-grained memory fragments
categorized into factual, experiential, and subjective information; it then
establishes explicit relationships between memory items across sessions,
capturing evolution patterns like extensions, transformations, and
implications. By performing this reasoning during pre-storage rather than when
generating a response, PREMem creates enriched representations while reducing
computational demands during interactions. Experiments show significant
performance improvements across all model sizes, with smaller models achieving
results comparable to much larger baselines while maintaining effectiveness
even with constrained token budgets. Code and dataset are available at
https://github.com/sangyeop-kim/PREMem.

</details>


### [21] [Quantifier Scope Interpretation in Language Learners and LLMs](https://arxiv.org/abs/2509.10860)
*Shaohua Fang,Yue Li,Yan Cong*

Main category: cs.CL

TL;DR: 大型语言模型在量词辖域解释方面表现出与人类相似的倾向，但模型架构和预训练数据影响其与人类行为的匹配程度。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型(llm)如何处理英语和汉语中的量词辖域解释歧义。

Method: 使用概率评估和人类相似性(hs)评分来量化llm在不同语言群体中模仿人类表现的程度。

Result: 大多数llm倾向于表面辖域解释，与人类的倾向一致，但只有一些llm区分英语和汉语的逆辖域偏好，反映了与人类相似的模式。

Conclusion: 模型架构、规模以及预训练数据语言背景显著影响llm对人类量词辖域解释的近似程度。

Abstract: Sentences with multiple quantifiers often lead to interpretive ambiguities,
which can vary across languages. This study adopts a cross-linguistic approach
to examine how large language models (LLMs) handle quantifier scope
interpretation in English and Chinese, using probabilities to assess
interpretive likelihood. Human similarity (HS) scores were used to quantify the
extent to which LLMs emulate human performance across language groups. Results
reveal that most LLMs prefer the surface scope interpretations, aligning with
human tendencies, while only some differentiate between English and Chinese in
the inverse scope preferences, reflecting human-similar patterns. HS scores
highlight variability in LLMs' approximation of human behavior, but their
overall potential to align with humans is notable. Differences in model
architecture, scale, and particularly models' pre-training data language
background, significantly influence how closely LLMs approximate human
quantifier scope interpretations.

</details>


### [22] [Term2Note: Synthesising Differentially Private Clinical Notes from Medical Terms](https://arxiv.org/abs/2509.10882)
*Yuping Wu,Viktor Schlegel,Warren Del-Pinto,Srinivasan Nandakumar,Iqra Zahid,Yidan Sun,Usama Farghaly Omar,Amirah Jasmine,Arun-Kumar Kaliya-Perumal,Chun Shen Tham,Gabriel Connors,Anil A Bharath,Goran Nenadic*

Main category: cs.CL

TL;DR: 本文提出了一种名为Term2Note的方法，用于在强差分隐私（DP）约束下合成长临床笔记。


<details>
  <summary>Details</summary>
Motivation: 在医疗保健等高风险领域，使用真实训练数据受到隐私泄露的严重限制。差分隐私（DP）合成数据是一种有前景的解决方案，但如何在临床笔记合成中平衡隐私保护和效用仍然具有挑战性。

Method: Term2Note通过结构化分离内容和形式，根据DP医学术语生成分节笔记内容，每个部分都受单独的DP约束。DP质量最大化器进一步通过选择高质量输出来增强合成笔记。

Result: 实验结果表明，Term2Note生成的合成笔记具有与真实临床笔记非常吻合的统计特性，表现出很强的保真度。此外，在这些合成笔记上训练的多标签分类模型与在真实数据上训练的模型性能相当，证实了它们的高效用性。

Conclusion: Term2Note在保真度和效用方面都取得了显著的改进，同时在更少的假设下运行，表明其作为使用敏感临床笔记的可行的隐私保护替代方案的潜力。

Abstract: Training data is fundamental to the success of modern machine learning
models, yet in high-stakes domains such as healthcare, the use of real-world
training data is severely constrained by concerns over privacy leakage. A
promising solution to this challenge is the use of differentially private (DP)
synthetic data, which offers formal privacy guarantees while maintaining data
utility. However, striking the right balance between privacy protection and
utility remains challenging in clinical note synthesis, given its domain
specificity and the complexity of long-form text generation. In this paper, we
present Term2Note, a methodology to synthesise long clinical notes under strong
DP constraints. By structurally separating content and form, Term2Note
generates section-wise note content conditioned on DP medical terms, with each
governed by separate DP constraints. A DP quality maximiser further enhances
synthetic notes by selecting high-quality outputs. Experimental results show
that Term2Note produces synthetic notes with statistical properties closely
aligned with real clinical notes, demonstrating strong fidelity. In addition,
multi-label classification models trained on these synthetic notes perform
comparably to those trained on real data, confirming their high utility.
Compared to existing DP text generation baselines, Term2Note achieves
substantial improvements in both fidelity and utility while operating under
fewer assumptions, suggesting its potential as a viable privacy-preserving
alternative to using sensitive clinical notes.

</details>


### [23] [CultureSynth: A Hierarchical Taxonomy-Guided and Retrieval-Augmented Framework for Cultural Question-Answer Synthesis](https://arxiv.org/abs/2509.10886)
*Xinyu Zhang,Pei Zhang,Shuang Luo,Jialong Tang,Yu Wan,Baosong Yang,Fei Huang*

Main category: cs.CL

TL;DR: 本文介绍了一种名为 CultureSynth 的新框架，用于评估大型语言模型 (LLM) 的文化能力。


<details>
  <summary>Details</summary>
Motivation: 当前评估 LLM 文化能力的方法存在分类体系不完整、领域特定性强和过度依赖手动数据标注等局限性。

Method: CultureSynth 包含一个多语言文化分类体系和一个基于检索增强生成 (RAG) 的方法，用于合成文化相关的问答对。

Result: 对 14 个 LLM 的评估显示，模型性能存在明显分层，并且模型在知识处理方面表现出不同的架构偏差，以及显著的地域差异。3B 参数是实现基本文化能力的必要阈值。

Conclusion: CultureSynth 提供了一个可扩展的框架，用于开发具有文化意识的 AI 系统，同时减少对手动标注的依赖。

Abstract: Cultural competence, defined as the ability to understand and adapt to
multicultural contexts, is increasingly vital for large language models (LLMs)
in global environments. While several cultural benchmarks exist to assess LLMs'
cultural competence, current evaluations suffer from fragmented taxonomies,
domain specificity, and heavy reliance on manual data annotation. To address
these limitations, we introduce CultureSynth, a novel framework comprising (1)
a comprehensive hierarchical multilingual cultural taxonomy covering 12 primary
and 130 secondary topics, and (2) a Retrieval-Augmented Generation (RAG)-based
methodology leveraging factual knowledge to synthesize culturally relevant
question-answer pairs. The CultureSynth-7 synthetic benchmark contains 19,360
entries and 4,149 manually verified entries across 7 languages. Evaluation of
14 prevalent LLMs of different sizes reveals clear performance stratification
led by ChatGPT-4o-Latest and Qwen2.5-72B-Instruct. The results demonstrate that
a 3B-parameter threshold is necessary for achieving basic cultural competence,
models display varying architectural biases in knowledge processing, and
significant geographic disparities exist across models. We believe that
CultureSynth offers a scalable framework for developing culturally aware AI
systems while reducing reliance on manual annotation\footnote{Benchmark is
available at https://github.com/Eyr3/CultureSynth.}.

</details>


### [24] [Aligning ESG Controversy Data with International Guidelines through Semi-Automatic Ontology Construction](https://arxiv.org/abs/2509.10922)
*Tsuyoshi Iwata,Guillaume Comte,Melissa Flores,Ryoma Kondo,Ryohei Hisano*

Main category: cs.CL

TL;DR: 本文提出了一种半自动方法，用于构建新闻报道中环境、社会和治理事件的结构化知识表示。


<details>
  <summary>Details</summary>
Motivation: 在监管和投资环境中，环境、社会和治理数据的重要性日益增加，因此需要准确、可解释且国际统一的非财务风险表示，特别是那些在非结构化新闻来源中报告的风险。将此类与争议相关的数据与基于原则的规范框架（如联合国全球契约或可持续发展目标）对齐，提出了重大挑战。

Method: 我们的方法使用轻量级本体设计、形式模式建模和大型语言模型，将规范原则转换为以资源描述框架表示的可重用模板。这些模板用于从新闻内容中提取相关信息，并填充一个结构化知识图，该图将报告的事件链接到特定的框架原则。

Result: 结果是一个可扩展且透明的框架，用于识别和解释不符合国际可持续性准则的行为。

Conclusion: 本文构建了一个可扩展且透明的框架，用于识别和解释不符合国际可持续性准则的行为。

Abstract: The growing importance of environmental, social, and governance data in
regulatory and investment contexts has increased the need for accurate,
interpretable, and internationally aligned representations of non-financial
risks, particularly those reported in unstructured news sources. However,
aligning such controversy-related data with principle-based normative
frameworks, such as the United Nations Global Compact or Sustainable
Development Goals, presents significant challenges. These frameworks are
typically expressed in abstract language, lack standardized taxonomies, and
differ from the proprietary classification systems used by commercial data
providers. In this paper, we present a semi-automatic method for constructing
structured knowledge representations of environmental, social, and governance
events reported in the news. Our approach uses lightweight ontology design,
formal pattern modeling, and large language models to convert normative
principles into reusable templates expressed in the Resource Description
Framework. These templates are used to extract relevant information from news
content and populate a structured knowledge graph that links reported incidents
to specific framework principles. The result is a scalable and transparent
framework for identifying and interpreting non-compliance with international
sustainability guidelines.

</details>


### [25] [Introducing Spotlight: A Novel Approach for Generating Captivating Key Information from Documents](https://arxiv.org/abs/2509.10935)
*Ankan Mullick,Sombit Bose,Rounak Saha,Ayan Kumar Bhowmick,Aditya Vempaty,Prasenjit Dey,Ravi Kokku,Pawan Goyal,Niloy Ganguly*

Main category: cs.CL

TL;DR: Spotlight: a new information extraction paradigm that produces concise, engaging narratives by highlighting the most compelling aspects of a document to foster deeper reader engagement.


<details>
  <summary>Details</summary>
Motivation: Unlike traditional summaries, which prioritize comprehensive coverage, spotlights selectively emphasize intriguing content to foster deeper reader engagement with the source material.

Method: fine-tuning a large language model on our benchmark data, followed by alignment via Direct Preference Optimization (DPO).

Result: identifies key elements with precision but also enhances readability and boosts the engagement value of the original document.

Conclusion: demonstrates that the resulting model not only identifies key elements with precision but also enhances readability and boosts the engagement value of the original document.

Abstract: In this paper, we introduce Spotlight, a novel paradigm for information
extraction that produces concise, engaging narratives by highlighting the most
compelling aspects of a document. Unlike traditional summaries, which
prioritize comprehensive coverage, spotlights selectively emphasize intriguing
content to foster deeper reader engagement with the source material. We
formally differentiate spotlights from related constructs and support our
analysis with a detailed benchmarking study using new datasets curated for this
work. To generate high-quality spotlights, we propose a two-stage approach:
fine-tuning a large language model on our benchmark data, followed by alignment
via Direct Preference Optimization (DPO). Our comprehensive evaluation
demonstrates that the resulting model not only identifies key elements with
precision but also enhances readability and boosts the engagement value of the
original document.

</details>


### [26] [An Interpretable Benchmark for Clickbait Detection and Tactic Attribution](https://arxiv.org/abs/2509.10937)
*Lihi Nofar,Tomer Portal,Aviv Elbaz,Alexander Apartsin,Yehudit Aperstein*

Main category: cs.CL

TL;DR: 本文提出了一种可解释的点击诱饵检测模型，该模型不仅可以识别点击诱饵标题，还可以将其归因于特定的语言操纵策略。


<details>
  <summary>Details</summary>
Motivation: 点击诱饵标题的激增对数字媒体中信息的 credibility 和用户信任构成了重大挑战，缺乏可解释性限制了机器学习在检测 manipulative content 方面的实际应用。

Method: 本文介绍了一个合成数据集，该数据集通过使用预定义的点击诱饵策略系统地扩充真实新闻标题来生成。提出了一个用于自动点击诱饵分析的两阶段框架，包括检测和策略归因。第一阶段，比较了微调的 BERT 分类器与大型语言模型 (LLM)，特别是 GPT-4.0 和 Gemini 2.4 Flash。在第二阶段，一个专用的基于 BERT 的分类器预测每个标题中存在的特定点击诱饵策略。

Result: 本文构建了一个用于自动点击诱饵分析的两阶段框架，包括检测和策略归因。

Conclusion: 这项工作推进了透明和值得信赖的 AI 系统的开发，以打击 manipulative 媒体内容。

Abstract: The proliferation of clickbait headlines poses significant challenges to the
credibility of information and user trust in digital media. While recent
advances in machine learning have improved the detection of manipulative
content, the lack of explainability limits their practical adoption. This paper
presents a model for explainable clickbait detection that not only identifies
clickbait titles but also attributes them to specific linguistic manipulation
strategies. We introduce a synthetic dataset generated by systematically
augmenting real news headlines using a predefined catalogue of clickbait
strategies. This dataset enables controlled experimentation and detailed
analysis of model behaviour. We present a two-stage framework for automatic
clickbait analysis comprising detection and tactic attribution. In the first
stage, we compare a fine-tuned BERT classifier with large language models
(LLMs), specifically GPT-4.0 and Gemini 2.4 Flash, under both zero-shot
prompting and few-shot prompting enriched with illustrative clickbait headlines
and their associated persuasive tactics. In the second stage, a dedicated
BERT-based classifier predicts the specific clickbait strategies present in
each headline. This work advances the development of transparent and
trustworthy AI systems for combating manipulative media content. We share the
dataset with the research community at
https://github.com/LLM-HITCS25S/ClickbaitTacticsDetection

</details>


### [27] [EmoBench-Reddit: A Hierarchical Benchmark for Evaluating the Emotional Intelligence of Multimodal Large Language Models](https://arxiv.org/abs/2509.11101)
*Haokun Li,Yazhou Zhang,Jizhi Ding,Qiuchi Li,Peng Zhang*

Main category: cs.CL

TL;DR: 提出了一个用于评估多模态大语言模型理解复杂和主观人类情感能力的新基准EmoBench-Reddit。


<details>
  <summary>Details</summary>
Motivation: 现有评估标准主要集中在客观的视觉问题回答或图像描述，未能充分评估模型理解复杂和主观人类情感的能力。

Method: 设计了一个分层任务框架，从基本感知到高级认知，每个数据点包含六个多项选择题和一个难度递增的开放式问题。使用来自 Reddit 的 350 个精心策划的样本，每个样本包含图像、相关的用户提供的文本以及用户 flair 确认的情感类别。

Result: 该数据集包含来自社交媒体平台 Reddit 的 350 个精心策划的样本，每个样本包含图像、相关的用户提供的文本以及用户 flair 确认的情感类别（悲伤、幽默、讽刺、快乐）。

Conclusion: 通过结合 AI 辅助 (Claude 4) 和人工验证，确保了注释质量。

Abstract: With the rapid advancement of Multimodal Large Language Models (MLLMs), they
have demonstrated exceptional capabilities across a variety of vision-language
tasks. However, current evaluation benchmarks predominantly focus on objective
visual question answering or captioning, inadequately assessing the models'
ability to understand complex and subjective human emotions. To bridge this
gap, we introduce EmoBench-Reddit, a novel, hierarchical benchmark for
multimodal emotion understanding. The dataset comprises 350 meticulously
curated samples from the social media platform Reddit, each containing an
image, associated user-provided text, and an emotion category (sad, humor,
sarcasm, happy) confirmed by user flairs. We designed a hierarchical task
framework that progresses from basic perception to advanced cognition, with
each data point featuring six multiple-choice questions and one open-ended
question of increasing difficulty. Perception tasks evaluate the model's
ability to identify basic visual elements (e.g., colors, objects), while
cognition tasks require scene reasoning, intent understanding, and deep empathy
integrating textual context. We ensured annotation quality through a
combination of AI assistance (Claude 4) and manual verification.

</details>


### [28] [Fluid Language Model Benchmarking](https://arxiv.org/abs/2509.11106)
*Valentin Hofmann,David Heineman,Ian Magnusson,Kyle Lo,Jesse Dodge,Maarten Sap,Pang Wei Koh,Chun Wang,Hannaneh Hajishirzi,Noah A. Smith*

Main category: cs.CL

TL;DR: 提出了一种新的评估方法，称为 Fluid Benchmarking，它通过动态选择评估项目来改进 LM 基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有的 LM 基准测试存在成本高、无法准确衡量预期能力以及评估质量下降等问题，并且现有方法通常只关注单个方面。

Method: Fluid Benchmarking 基于心理测量学，估计一个项目反应模型，并使用推断的量动态选择评估项目。

Result: Fluid Benchmarking 在效率、有效性、方差和饱和度四个维度上都优于现有方法。

Conclusion: 通过超越静态评估，LM 基准测试可以得到显著改进。

Abstract: Language model (LM) benchmarking faces several challenges: comprehensive
evaluations are costly, benchmarks often fail to measure the intended
capabilities, and evaluation quality can degrade due to labeling errors and
benchmark saturation. Although various strategies have been proposed to
mitigate these issues, they tend to address individual aspects in isolation,
neglecting broader questions about overall evaluation quality. Here, we
introduce Fluid Benchmarking, a new evaluation approach that advances LM
benchmarking across multiple dimensions. Inspired by psychometrics, Fluid
Benchmarking is based on the insight that the relative value of benchmark items
depends on an LM's capability level, suggesting that evaluation should adapt to
each LM. Methodologically, Fluid Benchmarking estimates an item response model
based on existing LM evaluation results and uses the inferred quantities to
select evaluation items dynamically, similar to computerized adaptive testing
in education. In our experiments, we compare Fluid Benchmarking against the
common practice of random item sampling as well as more sophisticated
baselines, including alternative methods grounded in item response theory. We
examine four dimensions -- efficiency, validity, variance, and saturation --
and find that Fluid Benchmarking achieves superior performance in all of them
(e.g., higher validity and less variance on MMLU with fifty times fewer items).
Our analysis shows that the two components of Fluid Benchmarking have distinct
effects: item response theory, used to map performance into a latent ability
space, increases validity, while dynamic item selection reduces variance.
Overall, our results suggest that LM benchmarking can be substantially improved
by moving beyond static evaluation.

</details>


### [29] [We Argue to Agree: Towards Personality-Driven Argumentation-Based Negotiation Dialogue Systems for Tourism](https://arxiv.org/abs/2509.11118)
*Priyanshu Priya,Saurav Dudhate,Desai Vishesh Yasheshbhai,Asif Ekbal*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于人格的论证协商对话生成(PAN-DG)任务，并为此创建了一个旅游领域的对话数据集PACT。


<details>
  <summary>Details</summary>
Motivation: 将论证机制整合到协商对话系统中，并通过论证和批评的交换来改善冲突解决。此外，结合人格属性，通过使互动与个人的偏好和风格相一致，从而增强适应性。

Method: 利用大型语言模型(llm)生成一个具有三种不同人格特征的数据集，即论证特征、偏好特征和购买风格特征，以模拟涉及不同人格的各种协商场景。

Result: 自动和人工评估表明，该数据集包含高质量的对话。多维评估表明，经过微调的llm在谈判过程中有效地生成了人格驱动的理性反应。

Conclusion: PACT在增强协商对话系统中的个性化和推理能力方面是有效的，从而为该领域未来的研究奠定了基础。

Abstract: Integrating argumentation mechanisms into negotiation dialogue systems
improves conflict resolution through exchanges of arguments and critiques.
Moreover, incorporating personality attributes enhances adaptability by
aligning interactions with individuals' preferences and styles. To advance
these capabilities in negotiation dialogue systems, we propose a novel
Personality-driven Argumentation-based Negotiation Dialogue Generation (PAN-DG)
task. To support this task, we introduce PACT, a dataset of Personality-driven
Argumentation-based negotiation Conversations for Tourism sector. This dataset,
generated using Large Language Models (LLMs), features three distinct
personality profiles, viz. Argumentation Profile, Preference Profile, and
Buying Style Profile to simulate a variety of negotiation scenarios involving
diverse personalities. Thorough automatic and manual evaluations indicate that
the dataset comprises high-quality dialogues. Further, we conduct comparative
experiments between pre-trained and fine-tuned LLMs for the PAN-DG task.
Multi-dimensional evaluation demonstrates that the fine-tuned LLMs effectively
generate personality-driven rational responses during negotiations. This
underscores the effectiveness of PACT in enhancing personalization and
reasoning capabilities in negotiation dialogue systems, thereby establishing a
foundation for future research in this domain.

</details>


### [30] [Joint Effects of Argumentation Theory, Audio Modality and Data Enrichment on LLM-Based Fallacy Classification](https://arxiv.org/abs/2509.11127)
*Hongxu Zhou,Hylke Westerdijk,Khondoker Ittehadul Islam*

Main category: cs.CL

TL;DR: 研究大型语言模型(LLM)在谬误分类任务中，尤其是在政治辩论环境中，语境和情感基调元数据如何影响LLM的推理和性能。


<details>
  <summary>Details</summary>
Motivation: 探讨语境和情感基调元数据对LLM在谬误分类任务中的影响，特别是在政治辩论场景下。

Method: 使用来自美国总统辩论的数据，通过应用于Qwen-3 (8B)模型的各种提示策略，对六种谬误类型进行分类。引入了两个理论基础的Chain-of-Thought框架：Pragma-Dialectics和论证周期表，并在三种输入设置下评估它们相对于基线提示的有效性：仅文本、文本与语境、文本与语境和基于音频的情感基调元数据。

Result: 理论提示可以提高可解释性，在某些情况下还可以提高准确性，但添加语境，特别是情感基调元数据，通常会导致性能下降。情感基调元数据会使模型偏向于将陈述标记为诉诸情感，从而恶化逻辑推理。基本提示通常优于增强提示，这表明来自添加输入的注意力稀释可能会恶化而不是改善LLM中的谬误分类。

Conclusion: 在LLM谬误分类中，添加语境和情感基调元数据可能会降低性能，基本提示通常优于增强提示。

Abstract: This study investigates how context and emotional tone metadata influence
large language model (LLM) reasoning and performance in fallacy classification
tasks, particularly within political debate settings. Using data from U.S.
presidential debates, we classify six fallacy types through various prompting
strategies applied to the Qwen-3 (8B) model. We introduce two theoretically
grounded Chain-of-Thought frameworks: Pragma-Dialectics and the Periodic Table
of Arguments, and evaluate their effectiveness against a baseline prompt under
three input settings: text-only, text with context, and text with both context
and audio-based emotional tone metadata. Results suggest that while theoretical
prompting can improve interpretability and, in some cases, accuracy, the
addition of context and especially emotional tone metadata often leads to
lowered performance. Emotional tone metadata biases the model toward labeling
statements as \textit{Appeal to Emotion}, worsening logical reasoning. Overall,
basic prompts often outperformed enhanced ones, suggesting that attention
dilution from added inputs may worsen rather than improve fallacy
classification in LLMs.

</details>


### [31] [When Smiley Turns Hostile: Interpreting How Emojis Trigger LLMs' Toxicity](https://arxiv.org/abs/2509.11141)
*Shiyao Cui,Xijia Feng,Yingkang Wang,Junxiao Yang,Zhexin Zhang,Biplab Sikdar,Hongning Wang,Han Qiu,Minlie Huang*

Main category: cs.CL

TL;DR: 表情符号可能会触发大型语言模型 (LLM) 生成有害内容。


<details>
  <summary>Details</summary>
Motivation: 研究表情符号是否会增强 LLM 中的毒性生成，并解释这种现象。

Method: 通过自动构建带有表情符号的提示来探索表情符号触发的 LLM 毒性生成。在 7 个著名的 LLM 上进行了跨 5 种主流语言的实验以及越狱任务。

Result: 带有表情符号的提示很容易诱导毒性生成。

Conclusion: 表情符号可以充当异构语义通道来绕过安全机制。预训练语料库中与表情符号相关的数据污染与毒性生成行为之间存在潜在的相关性。

Abstract: Emojis are globally used non-verbal cues in digital communication, and
extensive research has examined how large language models (LLMs) understand and
utilize emojis across contexts. While usually associated with friendliness or
playfulness, it is observed that emojis may trigger toxic content generation in
LLMs. Motivated by such a observation, we aim to investigate: (1) whether
emojis can clearly enhance the toxicity generation in LLMs and (2) how to
interpret this phenomenon. We begin with a comprehensive exploration of
emoji-triggered LLM toxicity generation by automating the construction of
prompts with emojis to subtly express toxic intent. Experiments across 5
mainstream languages on 7 famous LLMs along with jailbreak tasks demonstrate
that prompts with emojis could easily induce toxicity generation. To understand
this phenomenon, we conduct model-level interpretations spanning semantic
cognition, sequence generation and tokenization, suggesting that emojis can act
as a heterogeneous semantic channel to bypass the safety mechanisms. To pursue
deeper insights, we further probe the pre-training corpus and uncover potential
correlation between the emoji-related data polution with the toxicity
generation behaviors. Supplementary materials provide our implementation code
and data. (Warning: This paper contains potentially sensitive contents)

</details>


### [32] [Text2Mem: A Unified Memory Operation Language for Memory Operating System](https://arxiv.org/abs/2509.11145)
*Felix Wang,Boyu Chen,Kerun Xu,Bo Tang,Feiyu Xiong,Zhiyu Li*

Main category: cs.CL

TL;DR: 提出了Text2Mem，一种统一的内存操作语言，旨在为大型语言模型代理提供更可靠、可预测的内存控制。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型代理的内存框架功能有限，缺乏高级操作和正式规范，导致行为不可预测。

Method: 定义了一个紧凑且富有表现力的操作集，使用JSON模式表示指令，并通过解析器、验证器和适配器实现与不同后端的连接。还计划推出Text2Mem Bench基准。

Result: 通过统一的执行契约确保安全性、确定性和跨异构后端的移植性。

Conclusion: Text2Mem为代理中的内存控制建立了第一个标准化的基础。

Abstract: Large language model agents increasingly depend on memory to sustain long
horizon interaction, but existing frameworks remain limited. Most expose only a
few basic primitives such as encode, retrieve, and delete, while higher order
operations like merge, promote, demote, split, lock, and expire are missing or
inconsistently supported. Moreover, there is no formal and executable
specification for memory commands, leaving scope and lifecycle rules implicit
and causing unpredictable behavior across systems. We introduce Text2Mem, a
unified memory operation language that provides a standardized pathway from
natural language to reliable execution. Text2Mem defines a compact yet
expressive operation set aligned with encoding, storage, and retrieval. Each
instruction is represented as a JSON based schema instance with required fields
and semantic invariants, which a parser transforms into typed operation objects
with normalized parameters. A validator ensures correctness before execution,
while adapters map typed objects either to a SQL prototype backend or to real
memory frameworks. Model based services such as embeddings or summarization are
integrated when required. All results are returned through a unified execution
contract. This design ensures safety, determinism, and portability across
heterogeneous backends. We also outline Text2Mem Bench, a planned benchmark
that separates schema generation from backend execution to enable systematic
evaluation. Together, these components establish the first standardized
foundation for memory control in agents.

</details>


### [33] [Differentially-private text generation degrades output language quality](https://arxiv.org/abs/2509.11176)
*Erion Çano,Ivan Habernal*

Main category: cs.CL

TL;DR: 研究了在差异隐私下微调的LLM对生成文本质量和效用的影响。


<details>
  <summary>Details</summary>
Motivation: 确保用户隐私，通过合成来自在差异隐私（DP）下调整的大型语言模型（LLM）的数据，这种方法最近变得流行。然而，DP微调的LLM对语言质量和它们产生的文本效用的影响尚未被调查。

Method: 使用三个语料库在四个隐私级别下调整了五个LLM，并评估了它们产生的文本输出的长度、语法正确性和词汇多样性。

Result: 结果表明，在更强的隐私约束下调整的LLM产生的文本更短（至少77%），语法正确性更低（至少9%），并且在二元语法多样性方面多样性更低（至少10%）。

Conclusion: 下游分类任务的准确性降低，这可能不利于生成的合成数据的有用性。

Abstract: Ensuring user privacy by synthesizing data from large language models (LLMs)
tuned under differential privacy (DP) has become popular recently. However, the
impact of DP fine-tuned LLMs on the quality of the language and the utility of
the texts they produce has not been investigated. In this work, we tune five
LLMs with three corpora under four levels of privacy and assess the length, the
grammatical correctness, and the lexical diversity of the text outputs they
produce. We also probe the utility of the synthetic outputs in downstream
classification tasks such as book genre recognition based on book descriptions
and cause of death recognition based on verbal autopsies. The results indicate
that LLMs tuned under stronger privacy constrains produce texts that are
shorter by at least 77 %, that are less grammatically correct by at least 9 %,
and are less diverse by at least 10 % in bi-gram diversity. Furthermore, the
accuracy they reach in downstream classification tasks decreases, which might
be detrimental to the usefulness of the generated synthetic data.

</details>


### [34] [Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs](https://arxiv.org/abs/2509.11177)
*Hang Guo,Yawei Li,Luca Benini*

Main category: cs.CL

TL;DR: 本文提出了一种结合量化和稀疏化的LLM压缩方法，以克服单一压缩方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM压缩技术逐渐接近性能极限，单一方法难以进一步压缩模型。

Method: 提出Optimal Brain Restoration (OBR)框架，通过误差补偿对齐剪枝和量化，最小化下游任务的性能下降。

Result: 实验表明，OBR能够在LLM上实现W4A4KV4量化和50%稀疏性，与FP16-dense基线相比，速度提升高达4.72倍，内存减少6.4倍。

Conclusion: OBR框架能够有效结合量化和稀疏化，实现更高的LLM压缩率，同时保持较好的性能。

Abstract: Recent advances in Large Language Model (LLM) compression, such as
quantization and pruning, have achieved notable success. However, as these
techniques gradually approach their respective limits, relying on a single
method for further compression has become increasingly challenging. In this
work, we explore an alternative solution by combining quantization and
sparsity. This joint approach, though promising, introduces new difficulties
due to the inherently conflicting requirements on weight distributions:
quantization favors compact ranges, while pruning benefits from high variance.
To attack this problem, we propose Optimal Brain Restoration (OBR), a general
and training-free framework that aligns pruning and quantization by error
compensation between both. OBR minimizes performance degradation on downstream
tasks by building on a second-order Hessian objective, which is then
reformulated into a tractable problem through surrogate approximation and
ultimately reaches a closed-form solution via group error compensation.
Experiments show that OBR enables aggressive W4A4KV4 quantization with 50%
sparsity on existing LLMs, and delivers up to 4.72x speedup and 6.4x memory
reduction compared to the FP16-dense baseline.

</details>


### [35] [The Prompt Engineering Report Distilled: Quick Start Guide for Life Sciences](https://arxiv.org/abs/2509.11295)
*Valentin Romanov,Steven A Niederer*

Main category: cs.CL

TL;DR: 本文旨在提供生命科学领域 prompt engineering 的实践指导，帮助研究人员从偶然使用 prompt 转向系统化实践，以提高研究质量。


<details>
  <summary>Details</summary>
Motivation: 为了提高生命科学研究效率，研究人员可以通过掌握 prompt engineering 技术，优化 LLM 的使用。

Method: 本文提炼了 Prompt Report 中的 6 种核心 prompt engineering 技术，并结合生命科学用例进行分析。

Result: 本文详细阐述了 prompt 的构建方式，并针对常见问题提供了建议，同时分析了不同平台 Deep Research 工具的有效性。

Conclusion: prompt engineering 可以增强而非取代现有的数据处理和文档编辑方法，本文旨在提供核心 prompt engineering 原则的实践指导，促进高效的系统化实践。

Abstract: Developing effective prompts demands significant cognitive investment to
generate reliable, high-quality responses from Large Language Models (LLMs). By
deploying case-specific prompt engineering techniques that streamline
frequently performed life sciences workflows, researchers could achieve
substantial efficiency gains that far exceed the initial time investment
required to master these techniques. The Prompt Report published in 2025
outlined 58 different text-based prompt engineering techniques, highlighting
the numerous ways prompts could be constructed. To provide actionable
guidelines and reduce the friction of navigating these various approaches, we
distil this report to focus on 6 core techniques: zero-shot, few-shot
approaches, thought generation, ensembling, self-criticism, and decomposition.
We breakdown the significance of each approach and ground it in use cases
relevant to life sciences, from literature summarization and data extraction to
editorial tasks. We provide detailed recommendations for how prompts should and
shouldn't be structured, addressing common pitfalls including multi-turn
conversation degradation, hallucinations, and distinctions between reasoning
and non-reasoning models. We examine context window limitations, agentic tools
like Claude Code, while analyzing the effectiveness of Deep Research tools
across OpenAI, Google, Anthropic and Perplexity platforms, discussing current
limitations. We demonstrate how prompt engineering can augment rather than
replace existing established individual practices around data processing and
document editing. Our aim is to provide actionable guidance on core prompt
engineering principles, and to facilitate the transition from opportunistic
prompting to an effective, low-friction systematic practice that contributes to
higher quality research.

</details>


### [36] [Ko-PIQA: A Korean Physical Commonsense Reasoning Dataset with Cultural Context](https://arxiv.org/abs/2509.11303)
*Dasol Choi,Jungwhan Kim,Guijin Son*

Main category: cs.CL

TL;DR: 提出了一个名为Ko-PIQA的韩国物理常识推理数据集，该数据集包含文化背景。


<details>
  <summary>Details</summary>
Motivation: 现有的物理常识推理数据集主要以英语为中心，缺乏文化多样性。

Method: 从301万个网络爬取问题开始，使用三个语言模型进行多阶段过滤，识别出11553个PIQA风格的问题。通过GPT-4o改进和人工验证，获得了441个高质量的问答对。

Result: 最佳模型在Ko-PIQA上达到了83.22%的准确率，而最差的模型只有59.86%，表明仍有很大的改进空间。模型在文化特定场景中表现不佳。

Conclusion: Ko-PIQA既可以作为韩国语言模型的基准，也可以作为更具包容性的常识推理研究的基础。

Abstract: Physical commonsense reasoning datasets like PIQA are predominantly
English-centric and lack cultural diversity. We introduce Ko-PIQA, a Korean
physical commonsense reasoning dataset that incorporates cultural context.
Starting from 3.01 million web-crawled questions, we employed a multi-stage
filtering approach using three language models to identify 11,553 PIQA-style
questions. Through GPT-4o refinement and human validation, we obtained 441
high-quality question-answer pairs. A key feature of Ko-PIQA is its cultural
grounding: 19.7\% of questions contain culturally specific elements like
traditional Korean foods (kimchi), clothing (hanbok), and specialized
appliances (kimchi refrigerators) that require culturally-aware reasoning
beyond direct translation. We evaluate seven language models on Ko-PIQA, with
the best model achieving 83.22\% accuracy while the weakest reaches only
59.86\%, demonstrating significant room for improvement. Models particularly
struggle with culturally specific scenarios, highlighting the importance of
culturally diverse datasets. Ko-PIQA serves as both a benchmark for Korean
language models and a foundation for more inclusive commonsense reasoning
research. The dataset and code will be publicly available.

</details>


### [37] [!MSA at AraHealthQA 2025 Shared Task: Enhancing LLM Performance for Arabic Clinical Question Answering through Prompt Engineering and Ensemble Learning](https://arxiv.org/abs/2509.11365)
*Mohamed Tarek,Seif Ahmed,Mohamed Basem*

Main category: cs.CL

TL;DR: 该论文介绍了作者在AraHealthQA-2025共享任务Track 2中的系统，并在Sub-Task 1和Sub-Task 2中均获得第二名。


<details>
  <summary>Details</summary>
Motivation: 在阿拉伯临床环境中进行医学问答。

Method: Sub-Task 1：利用Gemini 2.5 Flash模型，采用少量样本提示、数据集预处理和三种提示配置的集成来提高分类精度。Sub-Task 2：使用相同的模型，采用统一的提示，结合角色扮演（阿拉伯医学专家）、少量样本示例和后处理来生成简洁的答案。

Result: 在AraHealthQA-2025共享任务的Sub-Task 1和Sub-Task 2中均获得第二名。

Conclusion: 该方法在阿拉伯临床环境中的医学问答任务中表现出色。

Abstract: We present our systems for Track 2 (General Arabic Health QA, MedArabiQ) of
the AraHealthQA-2025 shared task, where our methodology secured 2nd place in
both Sub-Task 1 (multiple-choice question answering) and Sub-Task 2 (open-ended
question answering) in Arabic clinical contexts. For Sub-Task 1, we leverage
the Gemini 2.5 Flash model with few-shot prompting, dataset preprocessing, and
an ensemble of three prompt configurations to improve classification accuracy
on standard, biased, and fill-in-the-blank questions. For Sub-Task 2, we employ
a unified prompt with the same model, incorporating role-playing as an Arabic
medical expert, few-shot examples, and post-processing to generate concise
responses across fill-in-the-blank, patient-doctor Q&A, GEC, and paraphrased
variants.

</details>


### [38] [Transformer Enhanced Relation Classification: A Comparative Analysis of Contextuality, Data Efficiency and Sequence Complexity](https://arxiv.org/abs/2509.11374)
*Bowen Jing,Yang Cui,Tianpeng Huang*

Main category: cs.CL

TL;DR: 本文对比了基于Transformer和非Transformer的深度监督学习方法在关系抽取任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型时代，关系抽取在信息抽取中起着重要作用，它可以将非结构化的原始文本转换为结构化数据。

Method: 使用了PA-LSTM, C-GCN, AGGCN等非Transformer架构和BERT, RoBERTa, R-BERT等Transformer架构，并在TACRED, TACREV, RE-TACRED数据集上进行了实验，使用了micro F1等传统指标，并评估了不同句子长度和不同训练数据比例下的性能。

Result: 基于Transformer的模型优于非Transformer模型，micro F1值分别为80-90%和64-67%。

Conclusion: 本文简要回顾了监督关系分类的研究历程，并讨论了大型语言模型在关系抽取中的作用和现状。

Abstract: In the era of large language model, relation extraction (RE) plays an
important role in information extraction through the transformation of
unstructured raw text into structured data (Wadhwa et al., 2023). In this
paper, we systematically compare the performance of deep supervised learning
approaches without transformers and those with transformers. We used a series
of non-transformer architectures such as PA-LSTM(Zhang et al., 2017),
C-GCN(Zhang et al., 2018), and AGGCN(attention guide GCN)(Guo et al., 2019),
and a series of transformer architectures such as BERT, RoBERTa, and R-BERT(Wu
and He, 2019). Our comparison included traditional metrics like micro F1, as
well as evaluations in different scenarios, varying sentence lengths, and
different percentages of the dataset for training. Our experiments were
conducted on TACRED, TACREV, and RE-TACRED. The results show that
transformer-based models outperform non-transformer models, achieving micro F1
scores of 80-90% compared to 64-67% for non-transformer models. Additionally,
we briefly review the research journey in supervised relation classification
and discuss the role and current status of large language models (LLMs) in
relation extraction.

</details>


### [39] [Continually Adding New Languages to Multilingual Language Models](https://arxiv.org/abs/2509.11414)
*Abraham Toluwase Owodunni,Sachin Kumar*

Main category: cs.CL

TL;DR: 本研究探讨了如何在不从头开始重新训练的情况下，持续向多语言模型添加新语言的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多语言模型训练成本高昂，且难以支持新语言。简单的持续预训练方法存在灾难性遗忘问题，而缓解策略又因缺乏原始预训练数据而无法应用。

Method: 提出了层选择LoRA（LayRA）方法，该方法在选定的初始层和最终层添加低秩适配器（LoRA），同时保持模型其余部分冻结。

Result: 实验结果表明，LayRA在保持模型原有语言能力和学习新语言之间取得了最佳平衡。此外，通过模型运算，无需目标语言的指令调整数据，即可使调整后的模型具备强大的指令跟随能力。

Conclusion: LayRA方法为多语言模型持续添加新语言提供了一种有效的解决方案。

Abstract: Multilingual language models are trained on a fixed set of languages, and to
support new languages, the models need to be retrained from scratch. This is an
expensive endeavor and is often infeasible, as model developers tend not to
release their pre-training data. Naive approaches, such as continued
pretraining, suffer from catastrophic forgetting; however, mitigation
strategies like experience replay cannot be applied due to the lack of original
pretraining data. In this work, we investigate the problem of continually
adding new languages to a multilingual model, assuming access to pretraining
data in only the target languages. We explore multiple approaches to address
this problem and propose Layer-Selective LoRA (LayRA), which adds Low-Rank
Adapters (LoRA) to selected initial and final layers while keeping the rest of
the model frozen. LayRA builds on two insights: (1) LoRA reduces forgetting,
and (2) multilingual models encode inputs in the source language in the initial
layers, reason in English in intermediate layers, and translate back to the
source language in final layers. We experiment with adding multiple
combinations of Galician, Swahili, and Urdu to pretrained language models and
evaluate each method on diverse multilingual tasks. We find that LayRA provides
the overall best tradeoff between preserving models' capabilities in previously
supported languages, while being competitive with existing approaches such as
LoRA in learning new languages. We also demonstrate that using model
arithmetic, the adapted models can be equipped with strong instruction
following abilities without access to any instruction tuning data in the target
languages.

</details>


### [40] [A Transformer-Based Cross-Platform Analysis of Public Discourse on the 15-Minute City Paradigm](https://arxiv.org/abs/2509.11443)
*Gaurab Chhetri,Darrell Anderson,Boniphace Kutela,Subasish Das*

Main category: cs.CL

TL;DR: 本研究对Twitter、Reddit和新闻媒体上关于15分钟城市概念的公众舆论进行了首次多平台情感分析。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于了解不同平台对于15分钟城市概念的公众舆论。

Method: 使用压缩transformer模型和Llama-3-8B进行注释，对异构文本领域的情感进行分类。采用分层5重交叉验证，对五个模型（DistilRoBERTa、DistilBERT、MiniLM、ELECTRA、TinyBERT）进行基准测试，报告F1分数、AUC和训练时间。

Result: DistilRoBERTa实现了最高的F1分数（0.8292），TinyBERT具有最佳效率，MiniLM具有最佳跨平台一致性。新闻数据由于类别不平衡导致性能虚高，Reddit存在摘要损失，而Twitter具有适度的挑战。

Conclusion: 压缩模型表现出竞争优势，挑战了认为较大模型是必要的假设。确定了特定于平台的权衡，并为城市规划领域中可扩展的实际情感分类提出了方向。

Abstract: This study presents the first multi-platform sentiment analysis of public
opinion on the 15-minute city concept across Twitter, Reddit, and news media.
Using compressed transformer models and Llama-3-8B for annotation, we classify
sentiment across heterogeneous text domains. Our pipeline handles long-form and
short-form text, supports consistent annotation, and enables reproducible
evaluation. We benchmark five models (DistilRoBERTa, DistilBERT, MiniLM,
ELECTRA, TinyBERT) using stratified 5-fold cross-validation, reporting
F1-score, AUC, and training time. DistilRoBERTa achieved the highest F1
(0.8292), TinyBERT the best efficiency, and MiniLM the best cross-platform
consistency. Results show News data yields inflated performance due to class
imbalance, Reddit suffers from summarization loss, and Twitter offers moderate
challenge. Compressed models perform competitively, challenging assumptions
that larger models are necessary. We identify platform-specific trade-offs and
propose directions for scalable, real-world sentiment classification in urban
planning discourse.

</details>


### [41] [CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media](https://arxiv.org/abs/2509.11444)
*Gaurab Chhetri,Anandi Dutta,Subasish Das*

Main category: cs.CL

TL;DR: CognitiveSky是一个开源框架，用于在Bluesky上进行情感、情绪和叙事分析。


<details>
  <summary>Details</summary>
Motivation: 去中心化社交媒体平台为实时分析公共 discourse 提供了新的机会和挑战。

Method: 通过 Bluesky 的 API 摄取数据，应用基于 transformer 的模型来注释大规模用户生成的内容，并生成结构化和可分析的输出。

Result: CognitiveSky 构建在 free-tier 基础设施上，实现了低运营成本和高可访问性。动态仪表板可视化情感、活动和对话主题的演变模式。

Conclusion: CognitiveSky 通过桥接大型语言模型与去中心化网络，为数字生态系统转变时代的计算社会科学提供了一个透明、可扩展的工具。

Abstract: The emergence of decentralized social media platforms presents new
opportunities and challenges for real-time analysis of public discourse. This
study introduces CognitiveSky, an open-source and scalable framework designed
for sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter
or X.com alternative. By ingesting data through Bluesky's Application
Programming Interface (API), CognitiveSky applies transformer-based models to
annotate large-scale user-generated content and produces structured and
analyzable outputs. These summaries drive a dynamic dashboard that visualizes
evolving patterns in emotion, activity, and conversation topics. Built entirely
on free-tier infrastructure, CognitiveSky achieves both low operational cost
and high accessibility. While demonstrated here for monitoring mental health
discourse, its modular design enables applications across domains such as
disinformation detection, crisis response, and civic sentiment analysis. By
bridging large language models with decentralized networks, CognitiveSky offers
a transparent, extensible tool for computational social science in an era of
shifting digital ecosystems.

</details>


### [42] [CEMTM: Contextual Embedding-based Multimodal Topic Modeling](https://arxiv.org/abs/2509.11465)
*Amirhossein Abaskohi,Raymond Li,Chuyuan Li,Shafiq Joty,Giuseppe Carenini*

Main category: cs.CL

TL;DR: CEMTM: A context-enhanced multimodal topic model for inferring topic structures from documents with text and images.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal topic models have limitations in handling multiple images per document and maintaining interpretability.

Method: CEMTM uses fine-tuned large vision language models for contextualized embeddings and a distributional attention mechanism. It aligns topic-based representations with document embeddings.

Result: CEMTM outperforms unimodal and multimodal baselines on six multimodal benchmarks, achieving an average LLM score of 2.61. It also shows effectiveness in downstream few-shot retrieval.

Conclusion: CEMTM effectively captures visually grounded semantics in complex domains and improves topic inference in multimodal documents.

Abstract: We introduce CEMTM, a context-enhanced multimodal topic model designed to
infer coherent and interpretable topic structures from both short and long
documents containing text and images. CEMTM builds on fine-tuned large vision
language models (LVLMs) to obtain contextualized embeddings, and employs a
distributional attention mechanism to weight token-level contributions to topic
inference. A reconstruction objective aligns topic-based representations with
the document embedding, encouraging semantic consistency across modalities.
Unlike existing approaches, CEMTM can process multiple images per document
without repeated encoding and maintains interpretability through explicit
word-topic and document-topic distributions. Extensive experiments on six
multimodal benchmarks show that CEMTM consistently outperforms unimodal and
multimodal baselines, achieving a remarkable average LLM score of 2.61. Further
analysis shows its effectiveness in downstream few-shot retrieval and its
ability to capture visually grounded semantics in complex domains such as
scientific articles.

</details>


### [43] [Improving LLMs' Learning for Coreference Resolution](https://arxiv.org/abs/2509.11466)
*Yujian Gan,Yuan Liang,Yanni Lin,Juntao Yu,Massimo Poesio*

Main category: cs.CL

TL;DR: 大型语言模型在共指消解任务中存在幻觉和表现不佳的问题，本研究旨在解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型语言模型的共指消解方法（特别是问答模板和文档模板方法）存在局限性。

Method: 提出了两种新技术：具有联合推理的反向训练和迭代文档生成。

Result: 反向训练改进了问答模板方法，迭代文档生成消除了生成的源文本中的幻觉并提高了共指消解性能。

Conclusion: 整合这些方法和技术为基于大型语言模型的共指消解提供了一种有效且稳健的解决方案。

Abstract: Coreference Resolution (CR) is crucial for many NLP tasks, but existing LLMs
struggle with hallucination and under-performance. In this paper, we
investigate the limitations of existing LLM-based approaches to CR-specifically
the Question-Answering (QA) Template and Document Template methods and propose
two novel techniques: Reversed Training with Joint Inference and Iterative
Document Generation. Our experiments show that Reversed Training improves the
QA Template method, while Iterative Document Generation eliminates
hallucinations in the generated source text and boosts coreference resolution.
Integrating these methods and techniques offers an effective and robust
solution to LLM-based coreference resolution.

</details>


### [44] [ClaimIQ at CheckThat! 2025: Comparing Prompted and Fine-Tuned Language Models for Verifying Numerical Claims](https://arxiv.org/abs/2509.11492)
*Anirban Saha Anik,Md Fahimul Kabir Chowdhury,Andrew Wyckoff,Sagnik Ray Choudhury*

Main category: cs.CL

TL;DR: 这篇论文介绍了作者为CLEF 2025 CheckThat! Lab Task 3设计的系统，该系统用于验证数值和时间声明。该系统采用了零样本提示和监督微调两种方法，并探索了不同的证据选择策略。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是验证数值和时间声明，这是一个具有挑战性的任务。

Method: 论文采用了两种互补的方法：零样本提示和监督微调。为了提高证据质量，论文还研究了几种证据选择策略，包括全文输入和top-k句子过滤。

Result: 论文的最佳模型是在英文验证集上表现出色。然而，在测试集上的表现明显下降，这表明存在泛化挑战。

Conclusion: 论文的结论是，证据粒度和模型适应对于鲁棒的数值事实验证至关重要。

Abstract: This paper presents our system for Task 3 of the CLEF 2025 CheckThat! Lab,
which focuses on verifying numerical and temporal claims using retrieved
evidence. We explore two complementary approaches: zero-shot prompting with
instruction-tuned large language models (LLMs) and supervised fine-tuning using
parameter-efficient LoRA. To enhance evidence quality, we investigate several
selection strategies, including full-document input and top-k sentence
filtering using BM25 and MiniLM. Our best-performing model LLaMA fine-tuned
with LoRA achieves strong performance on the English validation set. However, a
notable drop in the test set highlights a generalization challenge. These
findings underscore the importance of evidence granularity and model adaptation
for robust numerical fact verification.

</details>


### [45] [AKCIT-FN at CheckThat! 2025: Switching Fine-Tuned SLMs and LLM Prompting for Multilingual Claim Normalization](https://arxiv.org/abs/2509.11496)
*Fabrycio Leite Nakano Almada,Kauan Divino Pouso Mariano,Maykon Adriell Dutra,Victor Emanuel da Silva Monteiro,Juliana Resplande Sant'Anna Gomes,Arlindo Rodrigues Galvão Filho,Anderson da Silva Soares*

Main category: cs.CL

TL;DR: 本研究介绍了一种用于声明规范化的方法，该方法将非正式社交媒体帖子转换为简明、独立的陈述，这是自动事实核查流程中的关键一步。该方法在 CLEF-2025 CheckThat! Task~2 比赛中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 声明规范化是自动事实核查流程中的关键一步，它可以将非正式社交媒体帖子转换为简明、独立的陈述。

Method: 该方法利用微调的小型语言模型 (SLM) 处理有监督语言，并利用大型语言模型 (LLM) 提示处理零样本场景。

Result: 该方法在 20 种语言中的 15 种语言中取得了前三名的成绩，包括在 8 种语言中排名第二，其中 5 种语言属于指定的 7 种零样本语言。对于葡萄牙语，该系统实现了 0.5290 的平均 METEOR 分数，排名第三。

Conclusion: 该研究表明，基于 LLM 的零样本策略是有效的。

Abstract: Claim normalization, the transformation of informal social media posts into
concise, self-contained statements, is a crucial step in automated
fact-checking pipelines. This paper details our submission to the CLEF-2025
CheckThat! Task~2, which challenges systems to perform claim normalization
across twenty languages, divided into thirteen supervised (high-resource) and
seven zero-shot (no training data) tracks.
  Our approach, leveraging fine-tuned Small Language Models (SLMs) for
supervised languages and Large Language Model (LLM) prompting for zero-shot
scenarios, achieved podium positions (top three) in fifteen of the twenty
languages. Notably, this included second-place rankings in eight languages,
five of which were among the seven designated zero-shot languages, underscoring
the effectiveness of our LLM-based zero-shot strategy. For Portuguese, our
initial development language, our system achieved an average METEOR score of
0.5290, ranking third. All implementation artifacts, including inference,
training, evaluation scripts, and prompt configurations, are publicly available
at https://github.com/ju-resplande/checkthat2025_normalization.

</details>


### [46] [DeDisCo at the DISRPT 2025 Shared Task: A System for Discourse Relation Classification](https://arxiv.org/abs/2509.11498)
*Zhuoxuan Ju,Jingni Wu,Abhishek Purushothama,Amir Zeldes*

Main category: cs.CL

TL;DR: DeDisCo是乔治城大学在DISRPT 2025共享任务中关于篇章关系分类的参赛作品。


<details>
  <summary>Details</summary>
Motivation: 探索篇章关系分类任务。

Method: 测试了两种方法：基于mt5的编码器和基于Qwen模型的解码器。使用了增强数据集进行低资源语言的训练，以及一些来自先前共享任务版本中的语言特征。

Result: 系统达到了71.28的宏平均准确率。

Conclusion: 对结果进行了解释和误差分析。

Abstract: This paper presents DeDisCo, Georgetown University's entry in the DISRPT 2025
shared task on discourse relation classification. We test two approaches, using
an mt5-based encoder and a decoder based approach using the openly available
Qwen model. We also experiment on training with augmented dataset for
low-resource languages using matched data translated automatically from
English, as well as using some additional linguistic features inspired by
entries in previous editions of the Shared Task. Our system achieves a
macro-accuracy score of 71.28, and we provide some interpretation and error
analysis for our results.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [47] [A Real-Time Diminished Reality Approach to Privacy in MR Collaboration](https://arxiv.org/abs/2509.10466)
*Christian Fane*

Main category: cs.CV

TL;DR: 该论文提出了一个基于实时修复的DR系统，用于在共享空间MR会议中实现隐私控制，允许主要头显用户选择性地移除环境中的个人或敏感物品，确保这些物品对其他参与者不可见。


<details>
  <summary>Details</summary>
Motivation: 在共享空间混合现实（MR）会议中，保护个人或敏感物品的隐私。

Method: 使用YOLOv11进行对象检测，并使用改进的解耦时空Transformer（DSTT）模型进行高质量视频修复，从辅助观察者的角度进行实时修复。

Result: 该系统在720p分辨率下，帧率超过20 fps，证明了实时DR在实际隐私保护MR应用中的可行性。

Conclusion: 该论文提出的系统能够实现便携和鲁棒的隐私保护，无需固定的辅助视点或环境的预先3D扫描。

Abstract: Diminished reality (DR) refers to the digital removal of real-world objects
by compositing background content in their place. This thesis presents a
real-time, inpainting-based DR system designed to enable privacy control in
shared-space mixed reality (MR) meetings. The system allows a primary headset
user to selectively remove personal or sensitive items from their environment,
ensuring that those objects are no longer visible to other participants.
Removal is achieved through semantic segmentation and precise object selection,
followed by real-time inpainting from the viewpoint of a secondary observer,
implemented using a mobile ZED 2i depth camera. The solution is designed to be
portable and robust, requiring neither a fixed secondary viewpoint nor prior 3D
scanning of the environment. The system utilises YOLOv11 for object detection
and a modified Decoupled Spatial-Temporal Transformer (DSTT) model for
high-quality video inpainting. At 720p resolution, the pipeline sustains frame
rates exceeding 20 fps, demonstrating the feasibility of real-time diminished
reality for practical privacy-preserving MR applications.

</details>


### [48] [SurgLaVi: Large-Scale Hierarchical Dataset for Surgical Vision-Language Representation Learning](https://arxiv.org/abs/2509.10555)
*Alejandra Perez,Chinedu Nwoye,Ramtin Raji Kermani,Omid Mohareri,Muhammad Abdullah Jamal*

Main category: cs.CV

TL;DR: SurgLaVi是最大的手术视觉-语言数据集，包含240k个clip-caption对，来自200多个手术，并包含阶段、步骤和任务级别的层次结构。还发布了一个名为SurgLaVi-{eta}的开源衍生产品，其中包含113k个clip-caption对。为了证明SurgLaVi数据集的价值，我们引入了SurgCLIP，这是一个CLIP风格的视频-文本对比框架，带有双编码器。SurgCLIP在阶段、步骤、动作和工具识别方面取得了持续的改进。


<details>
  <summary>Details</summary>
Motivation: 现有的手术视觉-语言预训练数据集的规模、程序多样性、语义质量和层次结构有限制了手术VLP的进展。

Method: 构建了一个全自动的流程，该流程系统地生成手术视频的细粒度转录，并将它们分割成连贯的程序单元。为了确保高质量的注释，它应用双模态过滤来删除不相关的和嘈杂的样本。在此框架内，生成的标题通过上下文细节丰富，从而生成语义丰富且易于解释的注释。引入SurgCLIP，这是一个CLIP风格的视频-文本对比框架，带有双编码器。

Result: SurgCLIP在阶段、步骤、动作和工具识别方面取得了持续的改进，超过了先前的最先进方法，通常幅度很大。这些结果验证了大规模、语义丰富和分层结构的数据集直接转化为更强大和更具通用性的表示。

Conclusion: SurgLaVi是开发手术基础模型的关键资源。

Abstract: Vision-language pre-training (VLP) offers unique advantages for surgery by
aligning language with surgical videos, enabling workflow understanding and
transfer across tasks without relying on expert-labeled datasets. However,
progress in surgical VLP remains constrained by the limited scale, procedural
diversity, semantic quality, and hierarchical structure of existing datasets.
In this work, we present SurgLaVi, the largest and most diverse surgical
vision-language dataset to date, comprising nearly 240k clip-caption pairs from
more than 200 procedures, and comprising hierarchical levels at phase-, step-,
and task-level. At the core of SurgLaVi lies a fully automated pipeline that
systematically generates fine-grained transcriptions of surgical videos and
segments them into coherent procedural units. To ensure high-quality
annotations, it applies dual-modality filtering to remove irrelevant and noisy
samples. Within this framework, the resulting captions are enriched with
contextual detail, producing annotations that are both semantically rich and
easy to interpret. To ensure accessibility, we release SurgLaVi-\b{eta}, an
open-source derivative of 113k clip-caption pairs constructed entirely from
public data, which is over four times larger than existing surgical VLP
datasets. To demonstrate the value of SurgLaVi datasets, we introduce SurgCLIP,
a CLIP-style video-text contrastive framework with dual encoders, as a
representative base model. SurgCLIP achieves consistent improvements across
phase, step, action, and tool recognition, surpassing prior state-of-the-art
methods, often by large margins. These results validate that large-scale,
semantically rich, and hierarchically structured datasets directly translate
into stronger and more generalizable representations, establishing SurgLaVi as
a key resource for developing surgical foundation models.

</details>


### [49] [Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses](https://arxiv.org/abs/2509.10620)
*Emily Kaczmarek,Justin Szeto,Brennan Nichyporuk,Tal Arbel*

Main category: cs.CV

TL;DR: 提出了一种通用的、高分辨率的基于 SimCLR 的 SSL 脑部 3D MRI 基础模型，该模型在包含多种神经疾病的 11 个公开数据集上进行了预训练。


<details>
  <summary>Details</summary>
Motivation: 现有的 3D 脑部 MRI 基础模型在分辨率、范围或可访问性方面仍然有限，并且深度学习模型在分析 3D MRI 时，大多是为特定任务量身定制的，泛化能力有限。

Method: 使用 SimCLR 框架，在包含 18,759 名患者的 44,958 个扫描的 11 个公开数据集上进行预训练。

Result: 该模型在四个不同的下游预测任务中，在同分布和异分布设置下，均优于其他模型。即使仅使用 20% 的标记训练样本进行微调，在预测阿尔茨海默病方面仍然表现出色。

Conclusion: 贡献了一个广泛适用且可访问的临床脑部 MRI 分析基础模型，并公开发布了代码和数据以及训练好的模型。

Abstract: 3D structural Magnetic Resonance Imaging (MRI) brain scans are commonly
acquired in clinical settings to monitor a wide range of neurological
conditions, including neurodegenerative disorders and stroke. While deep
learning models have shown promising results analyzing 3D MRI across a number
of brain imaging tasks, most are highly tailored for specific tasks with
limited labeled data, and are not able to generalize across tasks and/or
populations. The development of self-supervised learning (SSL) has enabled the
creation of large medical foundation models that leverage diverse, unlabeled
datasets ranging from healthy to diseased data, showing significant success in
2D medical imaging applications. However, even the very few foundation models
for 3D brain MRI that have been developed remain limited in resolution, scope,
or accessibility. In this work, we present a general, high-resolution
SimCLR-based SSL foundation model for 3D brain structural MRI, pre-trained on
18,759 patients (44,958 scans) from 11 publicly available datasets spanning
diverse neurological diseases. We compare our model to Masked Autoencoders
(MAE), as well as two supervised baselines, on four diverse downstream
prediction tasks in both in-distribution and out-of-distribution settings. Our
fine-tuned SimCLR model outperforms all other models across all tasks. Notably,
our model still achieves superior performance when fine-tuned using only 20% of
labeled training samples for predicting Alzheimer's disease. We use publicly
available code and data, and release our trained model at
https://github.com/emilykaczmarek/3D-Neuro-SimCLR, contributing a broadly
applicable and accessible foundation model for clinical brain MRI analysis.

</details>


### [50] [USCTNet: A deep unfolding nuclear-norm optimization solver for physically consistent HSI reconstruction](https://arxiv.org/abs/2509.10651)
*Xiaoyang Ma,Yiyang Chai,Xinran Qu,Hong Sun*

Main category: cs.CV

TL;DR: 本文提出了一种从单张RGB图像重建高光谱图像(HSI)的方法，该方法在可学习的变换域中，通过核范数正则化一个基于物理的逆问题来实现，并显式估计CSS和光照以确保比色一致性。


<details>
  <summary>Details</summary>
Motivation: 当相机光谱灵敏度(CSS)和场景照明被错误指定时，从单个RGB图像重建高光谱图像(HSI)是不适定的，并且可能在物理上变得不一致。

Method: 该方法将RGB到HSI的重建表示为一个由核范数正则化的、基于物理的逆问题，并显式估计CSS和光照以定义嵌入每次迭代中的正向算子，从而确保比色一致性。为了避免奇异值阈值(SVT)所需的完整奇异值分解(SVD)的成本和不稳定性，我们引入了一种数据自适应的低秩子空间SVT算子。在此基础上，我们开发了一种针对HSI量身定制的深度展开求解器USCTNet，它将参数估计模块与可学习的近端更新相结合。

Result: 在标准基准上的大量实验表明，在重建精度方面，相对于最先进的基于RGB的方法，该方法具有持续的改进。

Conclusion: 本文提出了一种新颖的深度展开求解器USCTNet，用于从单张RGB图像重建高光谱图像，并在重建精度方面取得了显著的改进。

Abstract: Reconstructing hyperspectral images (HSIs) from a single RGB image is
ill-posed and can become physically inconsistent when the camera spectral
sensitivity (CSS) and scene illumination are misspecified. We formulate
RGB-to-HSI reconstruction as a physics-grounded inverse problem regularized by
a nuclear norm in a learnable transform domain, and we explicitly estimate CSS
and illumination to define the forward operator embedded in each iteration,
ensuring colorimetric consistency. To avoid the cost and instability of full
singular-value decompositions (SVDs) required by singular-value thresholding
(SVT), we introduce a data-adaptive low-rank subspace SVT operator. Building on
these components, we develop USCTNet, a deep unfolding solver tailored to HSI
that couples a parameter estimation module with learnable proximal updates.
Extensive experiments on standard benchmarks show consistent improvements over
state-of-the-art RGB-based methods in reconstruction accuracy. Code:
https://github.com/psykheXX/USCTNet-Code-Implementation.git

</details>


### [51] [A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI](https://arxiv.org/abs/2509.10683)
*Felicia Liu,Jay J. Yoo,Farzad Khalvati*

Main category: cs.CV

TL;DR: 评估了大型语言模型（LLM）在医学影像任务中的有效性，发现在脑胶质瘤分类和分割方面，传统卷积神经网络（CNN）优于LLM。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在基于图像的医疗保健应用中的效用，目前这方面的研究较少。

Method: 使用BraTS 2020多模态脑部MRI数据集，评估通用视觉-语言LLM（LLaMA 3.2 Instruct）在微调前后的性能，并将其与定制3D CNN进行基准测试。实现了中心点、边界框和多边形提取三种分割方法。

Result: 在脑胶质瘤分类中，CNN达到80%的准确率，而通用LLM的准确率为76%，但特异性仅为18%。微调后，LLM的特异性提高到55%，但整体性能下降。在分割任务中，CNN能够准确定位神经胶质瘤，而LLM的预测始终聚集在图像中心附近。微调改善了输出格式，但未能显著提高空间准确性。CNN在两项任务中均优于LLM。

Conclusion: LLM在空间理解方面有限，微调带来的改进也很小，表明它们目前的形式不太适合基于图像的任务。需要更严格的微调或替代训练策略，LLM才能在医学领域实现更好的性能、鲁棒性和实用性。

Abstract: Large Language Models (LLMs) have shown strong performance in text-based
healthcare tasks. However, their utility in image-based applications remains
unexplored. We investigate the effectiveness of LLMs for medical imaging tasks,
specifically glioma classification and segmentation, and compare their
performance to that of traditional convolutional neural networks (CNNs). Using
the BraTS 2020 dataset of multi-modal brain MRIs, we evaluated a
general-purpose vision-language LLM (LLaMA 3.2 Instruct) both before and after
fine-tuning, and benchmarked its performance against custom 3D CNNs. For glioma
classification (Low-Grade vs. High-Grade), the CNN achieved 80% accuracy and
balanced precision and recall. The general LLM reached 76% accuracy but
suffered from a specificity of only 18%, often misclassifying Low-Grade tumors.
Fine-tuning improved specificity to 55%, but overall performance declined
(e.g., accuracy dropped to 72%). For segmentation, three methods - center
point, bounding box, and polygon extraction, were implemented. CNNs accurately
localized gliomas, though small tumors were sometimes missed. In contrast, LLMs
consistently clustered predictions near the image center, with no distinction
of glioma size, location, or placement. Fine-tuning improved output formatting
but failed to meaningfully enhance spatial accuracy. The bounding polygon
method yielded random, unstructured outputs. Overall, CNNs outperformed LLMs in
both tasks. LLMs showed limited spatial understanding and minimal improvement
from fine-tuning, indicating that, in their current form, they are not
well-suited for image-based tasks. More rigorous fine-tuning or alternative
training strategies may be needed for LLMs to achieve better performance,
robustness, and utility in the medical space.

</details>


### [52] [Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation](https://arxiv.org/abs/2509.10687)
*Hao Zhang,Chun-Han Yao,Simon Donné,Narendra Ahuja,Varun Jampani*

Main category: cs.CV

TL;DR: SP4D: 用于从单目输入生成配对的RGB和运动部件视频的框架。


<details>
  <summary>Details</summary>
Motivation: 传统部件分割方法依赖于基于外观的语义线索，SP4D学习生成运动部件，即与对象铰接对齐且在视图和时间上一致的结构组件。

Method: 采用双分支扩散模型，该模型共同合成RGB帧和相应的部件分割图。引入空间颜色编码方案，将部件掩码映射到连续的类RGB图像，以简化架构并灵活地实现不同的部件计数。双向扩散融合（BiDiFuse）模块增强了跨分支一致性，并通过对比部件一致性损失来促进部件预测的空间和时间对齐。

Result: 生成的2D部件图可以提升到3D，以导出骨骼结构和harmonic skinning weights，只需少量手动调整。SP4D可以很好地推广到不同的场景，包括真实世界的视频、新生成的对象和罕见的铰接姿势。

Conclusion: SP4D生成适用于下游动画和运动相关任务的运动感知输出。

Abstract: We present Stable Part Diffusion 4D (SP4D), a framework for generating paired
RGB and kinematic part videos from monocular inputs. Unlike conventional part
segmentation methods that rely on appearance-based semantic cues, SP4D learns
to produce kinematic parts - structural components aligned with object
articulation and consistent across views and time. SP4D adopts a dual-branch
diffusion model that jointly synthesizes RGB frames and corresponding part
segmentation maps. To simplify the architecture and flexibly enable different
part counts, we introduce a spatial color encoding scheme that maps part masks
to continuous RGB-like images. This encoding allows the segmentation branch to
share the latent VAE from the RGB branch, while enabling part segmentation to
be recovered via straightforward post-processing. A Bidirectional Diffusion
Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a
contrastive part consistency loss to promote spatial and temporal alignment of
part predictions. We demonstrate that the generated 2D part maps can be lifted
to 3D to derive skeletal structures and harmonic skinning weights with few
manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K,
a curated dataset of over 20K rigged objects selected and processed from
Objaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part
video sequences. Experiments show that SP4D generalizes strongly to diverse
scenarios, including real-world videos, novel generated objects, and rare
articulated poses, producing kinematic-aware outputs suitable for downstream
animation and motion-related tasks.

</details>


### [53] [SegSLR: Promptable Video Segmentation for Isolated Sign Language Recognition](https://arxiv.org/abs/2509.10710)
*Sven Schreiber,Noha Sarhan,Simone Frintrop,Christian Wilms*

Main category: cs.CV

TL;DR: SegSLR通过promptable zero-shot视频分割结合RGB和姿势信息，优于当前ISLR方法。


<details>
  <summary>Details</summary>
Motivation: 现有的ISLR方法结合RGB和姿势信息时，由于不精确的表示（如边界框），会丢失关键细节，如手形和方向。

Method: SegSLR系统通过姿势信息粗略定位手和身体，然后分割视频以保持形状信息，再利用分割结果聚焦RGB数据处理。

Result: 在ChaLearn249 IsoGD数据集上的评估表明，SegSLR优于现有方法。

Conclusion: SegSLR通过关注身体和手部受益，证明了设计选择的合理性。

Abstract: Isolated Sign Language Recognition (ISLR) approaches primarily rely on RGB
data or signer pose information. However, combining these modalities often
results in the loss of crucial details, such as hand shape and orientation, due
to imprecise representations like bounding boxes. Therefore, we propose the
ISLR system SegSLR, which combines RGB and pose information through promptable
zero-shot video segmentation. Given the rough localization of the hands and the
signer's body from pose information, we segment the respective parts through
the video to maintain all relevant shape information. Subsequently, the
segmentations focus the processing of the RGB data on the most relevant body
parts for ISLR. This effectively combines RGB and pose information. Our
evaluation on the complex ChaLearn249 IsoGD dataset shows that SegSLR
outperforms state-of-the-art methods. Furthermore, ablation studies indicate
that SegSLR strongly benefits from focusing on the signer's body and hands,
justifying our design choices.

</details>


### [54] [SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation](https://arxiv.org/abs/2509.10748)
*Jecia Z. Y. Mao,Francis X Creighton,Russell H Taylor,Manish Sahu*

Main category: cs.CV

TL;DR: 提出了一种语音引导的协作感知（SCOPE）框架，用于手术场景中的实时分割、标记和跟踪，该框架结合了大型语言模型（LLM）的推理能力和开放集视觉基础模型（VFM）的感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案依赖于特定领域的监督模型，需要标注数据和领域特定数据来适应新的手术场景和超出预定义标签类别的范围。提示驱动的视觉基础模型（VFM）虽然实现了开放集、零样本分割，但依赖于手动视觉或文本提示，限制了其在术中手术环境中的部署。

Method: 该框架包含一个协作感知代理，该代理生成VFM分割的候选结果，并结合来自临床医生的语音反馈来指导手术器械的分割。然后，器械本身作为交互式指针来标记手术场景中的其他元素。

Result: 在Cataract1k数据集和内部离体颅底数据集的子集上评估了该框架，证明了其生成手术场景实时分割和跟踪的潜力。通过实时模拟离体实验展示了其动态能力。

Conclusion: 这种人机协作模式展示了为动态手术室环境开发适应性强、免手动、以外科医生为中心的工具的潜力。

Abstract: Accurate segmentation and tracking of relevant elements of the surgical scene
is crucial to enable context-aware intraoperative assistance and decision
making. Current solutions remain tethered to domain-specific, supervised models
that rely on labeled data and required domain-specific data to adapt to new
surgical scenarios and beyond predefined label categories. Recent advances in
prompt-driven vision foundation models (VFM) have enabled open-set, zero-shot
segmentation across heterogeneous medical images. However, dependence of these
models on manual visual or textual cues restricts their deployment in
introperative surgical settings. We introduce a speech-guided collaborative
perception (SCOPE) framework that integrates reasoning capabilities of large
language model (LLM) with perception capabilities of open-set VFMs to support
on-the-fly segmentation, labeling and tracking of surgical instruments and
anatomy in intraoperative video streams. A key component of this framework is a
collaborative perception agent, which generates top candidates of VFM-generated
segmentation and incorporates intuitive speech feedback from clinicians to
guide the segmentation of surgical instruments in a natural human-machine
collaboration paradigm. Afterwards, instruments themselves serve as interactive
pointers to label additional elements of the surgical scene. We evaluated our
proposed framework on a subset of publicly available Cataract1k dataset and an
in-house ex-vivo skull-base dataset to demonstrate its potential to generate
on-the-fly segmentation and tracking of surgical scene. Furthermore, we
demonstrate its dynamic capabilities through a live mock ex-vivo experiment.
This human-AI collaboration paradigm showcase the potential of developing
adaptable, hands-free, surgeon-centric tools for dynamic operating-room
environments.

</details>


### [55] [Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation](https://arxiv.org/abs/2509.10759)
*Yi-Ruei Liu,You-Zhe Xie,Yu-Hsiang Hsu,I-Sheng Fang,Yu-Lun Liu,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 提出了一种新的两阶段流程，将 4D 高斯溅射与基于物理的射线追踪相结合，用于相机效果模拟。


<details>
  <summary>Details</summary>
Motivation: 现有的数据生成方法要么成本高昂，要么存在 sim-to-real 差距，要么无法准确地模拟相机效果。

Method: 4D 高斯射线追踪 (4D-GRT)。给定多视图视频，4D-GRT 首先重建动态场景，然后应用射线追踪生成具有可控的、物理上精确的相机效果的视频。

Result: 4D-GRT 实现了最快的渲染速度，同时与现有的基线相比，具有更好或相当的渲染质量。

Conclusion: 构建了八个合成动态场景，在室内环境中跨越四种相机效果，作为一个基准，以评估生成的具有相机效果的视频。

Abstract: Common computer vision systems typically assume ideal pinhole cameras but
fail when facing real-world camera effects such as fisheye distortion and
rolling shutter, mainly due to the lack of learning from training data with
camera effects. Existing data generation approaches suffer from either high
costs, sim-to-real gaps or fail to accurately model camera effects. To address
this bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage
pipeline that combines 4D Gaussian Splatting with physically-based ray tracing
for camera effect simulation. Given multi-view videos, 4D-GRT first
reconstructs dynamic scenes, then applies ray tracing to generate videos with
controllable, physically accurate camera effects. 4D-GRT achieves the fastest
rendering speed while performing better or comparable rendering quality
compared to existing baselines. Additionally, we construct eight synthetic
dynamic scenes in indoor environments across four camera effects as a benchmark
to evaluate generated videos with camera effects.

</details>


### [56] [EditDuet: A Multi-Agent System for Video Non-Linear Editing](https://arxiv.org/abs/2509.10761)
*Marcelo Sandoval-Castaneda,Bryan Russell,Josef Sivic,Gregory Shakhnarovich,Fabian Caba Heilbron*

Main category: cs.CV

TL;DR: 本文提出了一种用于视频编辑的自动化工具，使用多智能体方法，通过Editor和Critic两个agent协同工作，Editor负责根据自然语言指令和视频片段生成编辑后的视频序列，Critic负责提供反馈或渲染最终视频。


<details>
  <summary>Details</summary>
Motivation: 以往的视频编辑工作主要集中在检索或用户界面上，实际编辑仍由用户完成。本文旨在自动化视频编辑的核心任务。

Method: 将视频编辑形式化为序列决策过程，采用多智能体方法，设计Editor和Critic两个agent，并引入基于学习的方法来实现跨agent的有效沟通。

Result: 通过用户研究，系统在覆盖率、时间约束满足度和人类偏好方面均优于现有方法。

Conclusion: 本文提出的自动化视频编辑系统能够有效根据自然语言指令编辑视频，并在多个指标上优于现有方法。

Abstract: Automated tools for video editing and assembly have applications ranging from
filmmaking and advertisement to content creation for social media. Previous
video editing work has mainly focused on either retrieval or user interfaces,
leaving actual editing to the user. In contrast, we propose to automate the
core task of video editing, formulating it as sequential decision making
process. Ours is a multi-agent approach. We design an Editor agent and a Critic
agent. The Editor takes as input a collection of video clips together with
natural language instructions and uses tools commonly found in video editing
software to produce an edited sequence. On the other hand, the Critic gives
natural language feedback to the editor based on the produced sequence or
renders it if it is satisfactory. We introduce a learning-based approach for
enabling effective communication across specialized agents to address the
language-driven video editing task. Finally, we explore an LLM-as-a-judge
metric for evaluating the quality of video editing system and compare it with
general human preference. We evaluate our system's output video sequences
qualitatively and quantitatively through a user study and find that our system
vastly outperforms existing approaches in terms of coverage, time constraint
satisfaction, and human preference.

</details>


### [57] [Enhancement Without Contrast: Stability-Aware Multicenter Machine Learning for Glioma MRI Imaging](https://arxiv.org/abs/2509.10767)
*Sajad Amiri,Shahram Taeb,Sara Gharibi,Setareh Dehghanfard,Somayeh Sadat Mehrnia,Mehrdad Oveisi,Ilker Hacihaliloglu,Arman Rahmim,Mohammad R. Salmanpour*

Main category: cs.CV

TL;DR: 使用机器学习(ML)从非对比MRI预测对比增强，以提供更安全的替代方案。


<details>
  <summary>Details</summary>
Motivation: 基于钆的对比剂(GBCAs)是神经胶质瘤成像的核心，但存在安全性、成本和可及性问题。对比增强反映了肿瘤的侵袭性，并为治疗计划提供信息。

Method: 我们提出了一个稳定性感知框架，以识别用于多中心预测神经胶质瘤MRI对比增强的可重复ML管道。我们分析了来自四个TCIA数据集的1,446个神经胶质瘤病例。非对比T1WI作为输入，增强来自配对的对比后T1WI。使用IBSI标准下的PyRadiomics，提取108个特征，并与48种降维方法和25种分类器结合，产生1,200个管道。轮换验证在三个数据集上进行训练，并在第四个数据集上进行测试。

Result: 交叉验证预测准确率范围为0.91至0.96，外部测试达到0.87（UCSF-PDGM）、0.98（UPENN-GB）和0.95（BRATS-Africa），平均为0.93。F1、精确度和召回率稳定（0.87至0.96），而ROC-AUC变化较大（0.50至0.82），反映了队列异质性。

Conclusion: 该框架表明，稳定性感知模型选择能够可靠地从非对比神经胶质瘤MRI预测对比增强，减少对GBCAs的依赖，并提高跨中心的泛化能力。它为神经肿瘤学及其他领域的可重复ML提供了一个可扩展的模板。

Abstract: Gadolinium-based contrast agents (GBCAs) are central to glioma imaging but
raise safety, cost, and accessibility concerns. Predicting contrast enhancement
from non-contrast MRI using machine learning (ML) offers a safer alternative,
as enhancement reflects tumor aggressiveness and informs treatment planning.
Yet scanner and cohort variability hinder robust model selection. We propose a
stability-aware framework to identify reproducible ML pipelines for multicenter
prediction of glioma MRI contrast enhancement. We analyzed 1,446 glioma cases
from four TCIA datasets (UCSF-PDGM, UPENN-GB, BRATS-Africa, BRATS-TCGA-LGG).
Non-contrast T1WI served as input, with enhancement derived from paired
post-contrast T1WI. Using PyRadiomics under IBSI standards, 108 features were
extracted and combined with 48 dimensionality reduction methods and 25
classifiers, yielding 1,200 pipelines. Rotational validation was trained on
three datasets and tested on the fourth. Cross-validation prediction accuracies
ranged from 0.91 to 0.96, with external testing achieving 0.87 (UCSF-PDGM),
0.98 (UPENN-GB), and 0.95 (BRATS-Africa), with an average of 0.93. F1,
precision, and recall were stable (0.87 to 0.96), while ROC-AUC varied more
widely (0.50 to 0.82), reflecting cohort heterogeneity. The MI linked with ETr
pipeline consistently ranked highest, balancing accuracy and stability. This
framework demonstrates that stability-aware model selection enables reliable
prediction of contrast enhancement from non-contrast glioma MRI, reducing
reliance on GBCAs and improving generalizability across centers. It provides a
scalable template for reproducible ML in neuro-oncology and beyond.

</details>


### [58] [Group Evidence Matters: Tiling-based Semantic Gating for Dense Object Detection](https://arxiv.org/abs/2509.10779)
*Yilun Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种与检测器无关的后处理框架，通过将重叠引起的冗余转化为分组证据来提高无人机图像中小目标的检测召回率。


<details>
  <summary>Details</summary>
Motivation: 无人机图像中密集的小目标由于远距离视角、遮挡和杂乱等原因经常被遗漏。

Method: 该方法首先通过重叠切片恢复低置信度的候选目标，然后通过空间门（基于框中心的DBSCAN）和语义门（基于ResNet-18嵌入的DBSCAN）验证分组证据，最后对验证后的组进行置信度重加权，再进行类别感知的NMS融合。

Result: 在VisDrone数据集上的实验表明，召回率从0.685提高到0.778（+0.093），精度从0.801调整到0.595，F1值为0.669。后处理的平均延迟为每张图像0.095秒。

Conclusion: 实验结果表明该方法具有召回优先、精度权衡的特性，有利于远场计数和监控等对召回率敏感的应用。消融实验证实了切片暴露了遗漏的目标，空间聚类稳定了几何结构，语义聚类增强了外观一致性，重加权提供了与基线的校准集成。该框架无需重新训练，并可与现代检测器集成。

Abstract: Dense small objects in UAV imagery are often missed due to long-range
viewpoints, occlusion, and clutter[cite: 5]. This paper presents a
detector-agnostic post-processing framework that converts overlap-induced
redundancy into group evidence[cite: 6]. Overlapping tiling first recovers
low-confidence candidates[cite: 7]. A Spatial Gate (DBSCAN on box centroids)
and a Semantic Gate (DBSCAN on ResNet-18 embeddings) then validates group
evidence[cite: 7]. Validated groups receive controlled confidence reweighting
before class-aware NMS fusion[cite: 8]. Experiments on VisDrone show a recall
increase from 0.685 to 0.778 (+0.093) and a precision adjustment from 0.801 to
0.595, yielding F1=0.669[cite: 9]. Post-processing latency averages 0.095 s per
image[cite: 10]. These results indicate recall-first, precision-trade-off
behavior that benefits recall-sensitive applications such as far-field counting
and monitoring[cite: 10]. Ablation confirms that tiling exposes missed objects,
spatial clustering stabilizes geometry, semantic clustering enforces appearance
coherence, and reweighting provides calibrated integration with the
baseline[cite: 11]. The framework requires no retraining and integrates with
modern detectors[cite: 12]. Future work will reduce semantic gating cost and
extend the approach with temporal cues[cite: 13].

</details>


### [59] [InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts](https://arxiv.org/abs/2509.10813)
*Weipeng Zhong,Peizhou Cao,Yichen Jin,Li Luo,Wenzhe Cai,Jingli Lin,Hanqing Wang,Zhaoyang Lyu,Tai Wang,Bo Dai,Xudong Xu,Jiangmiao Pang*

Main category: cs.CV

TL;DR: InternScenes是一个新的大型可模拟室内场景数据集，包含约 40,000 个不同的场景，通过整合三个不同的场景来源，包括 196 万个 3D 对象，覆盖 15 种常见场景类型和 288 个对象类别。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集通常受到数据规模或多样性、缺乏小物品的清理布局以及严重的对象碰撞的限制。为了解决这些缺点，我们推出了 InternScenes。

Method: 通过整合真实世界的扫描、程序生成的场景和设计者创建的场景这三个不同的场景来源来构建数据集；通过创建真实到模拟的真实世界扫描副本，增强交互性，并通过物理模拟解决对象碰撞来确保可模拟性。

Result: InternScenes 通过两个基准应用（场景布局生成和点目标导航）展示了其价值。两者都显示了复杂和真实布局带来的新挑战。更重要的是，InternScenes 为扩大这两项任务的模型训练铺平了道路，使在这种复杂场景中进行生成和导航成为可能。

Conclusion: 我们致力于开源数据、模型和基准，以使整个社区受益。

Abstract: The advancement of Embodied AI heavily relies on large-scale, simulatable 3D
scene datasets characterized by scene diversity and realistic layouts. However,
existing datasets typically suffer from limitations in data scale or diversity,
sanitized layouts lacking small items, and severe object collisions. To address
these shortcomings, we introduce \textbf{InternScenes}, a novel large-scale
simulatable indoor scene dataset comprising approximately 40,000 diverse scenes
by integrating three disparate scene sources, real-world scans, procedurally
generated scenes, and designer-created scenes, including 1.96M 3D objects and
covering 15 common scene types and 288 object classes. We particularly preserve
massive small items in the scenes, resulting in realistic and complex layouts
with an average of 41.5 objects per region. Our comprehensive data processing
pipeline ensures simulatability by creating real-to-sim replicas for real-world
scans, enhances interactivity by incorporating interactive objects into these
scenes, and resolves object collisions by physical simulations. We demonstrate
the value of InternScenes with two benchmark applications: scene layout
generation and point-goal navigation. Both show the new challenges posed by the
complex and realistic layouts. More importantly, InternScenes paves the way for
scaling up the model training for both tasks, making the generation and
navigation in such complex scenes possible. We commit to open-sourcing the
data, models, and benchmarks to benefit the whole community.

</details>


### [60] [Well-Conditioned Polynomial Representations for Mathematical Handwriting Recognition](https://arxiv.org/abs/2509.10815)
*Robert M. Corless,Deepak Singh Kalhan,Stephen M. Watt*

Main category: cs.CV

TL;DR: 这篇论文探讨了使用不同多项式基底（Legendre, Legendre-Sobolev, Chebyshev, Chebyshev-Sobolev）参数化平面曲线来表示数学手写体时的权衡。目标是在保证准确建模的同时，降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 研究动机是找到一种既能紧凑表示数字墨水，又能高效计算的数学手写体参数化方法。

Method: 论文通过考虑多项式在不同基底下的条件数，以及内积如何给出符号之间变化的范数来评估不同基底的选择。

Result: 论文探讨了基底选择和多项式阶数之间的权衡。

Conclusion: 结论需要在阅读完整论文后才能得出。

Abstract: Previous work has made use of a parameterized plane curve polynomial
representation for mathematical handwriting, with the polynomials represented
in a Legendre or Legendre-Sobolev graded basis. This provides a compact
geometric representation for the digital ink. Preliminary results have also
been shown for Chebyshev and Chebyshev-Sobolev bases. This article explores the
trade-offs between basis choice and polynomial degree to achieve accurate
modeling with a low computational cost. To do this, we consider the condition
number for polynomial evaluation in these bases and bound how the various inner
products give norms for the variations between symbols.

</details>


### [61] [Multi-Task Diffusion Approach For Prediction of Glioma Tumor Progression](https://arxiv.org/abs/2509.10824)
*Aghiles Kebaili,Romain Modzelewski,Jérôme Lapuyade-Lahorgue,Maxime Fontanilles,Sébastien Thureau,Su Ruan*

Main category: cs.CV

TL;DR: 提出了一种多任务扩散框架，用于对神经胶质瘤进展进行时间无关的像素级预测。


<details>
  <summary>Details</summary>
Motivation: 在临床实践中，由于纵向 MRI 数据的稀疏性和不规则性，以及随访序列的不完整性导致数据不平衡，使得准确预测神经胶质瘤的演变具有挑战性。

Method: 该模型同时生成未来任意时间点的 FLAIR 序列，并估计使用有符号距离场 (SDF) 导出的空间概率肿瘤演变图，从而实现不确定性量化。为了捕捉任意间隔的肿瘤演变时间动态，我们集成了一个预训练的变形模块，该模块使用变形场对扫描间变化进行建模。针对数据稀缺的常见临床限制，我们实施了有针对性的增强管道，该管道合成了三个随访扫描的完整序列，并从可用的患者研究中推断出缺失的 MRI 模态，从而提高了预测模型的稳定性和准确性。此外，我们还引入了放射治疗加权焦点损失项，该项利用放射剂量图，因为这些图突出显示了模型训练期间更重要的临床区域。

Result: 该方法在一个公共数据集上进行训练，并在一个内部私人数据集上进行评估，在这两种情况下都取得了有希望的结果。

Conclusion: 该框架仅基于早期时间点的两次随访扫描，即可生成灵活的、随时间变化的概率图，使临床医生能够在任何未来的时间节点探究肿瘤进展风险。

Abstract: Glioma, an aggressive brain malignancy characterized by rapid progression and
its poor prognosis, poses significant challenges for accurate evolution
prediction. These challenges are exacerbated by sparse, irregularly acquired
longitudinal MRI data in clinical practice, where incomplete follow-up
sequences create data imbalances and make reliable modeling difficult. In this
paper, we present a multitask diffusion framework for time-agnostic, pixel-wise
prediction of glioma progression. The model simultaneously generates future
FLAIR sequences at any chosen time point and estimates spatial probabilistic
tumor evolution maps derived using signed distance fields (SDFs), allowing
uncertainty quantification. To capture temporal dynamics of tumor evolution
across arbitrary intervals, we integrate a pretrained deformation module that
models inter-scan changes using deformation fields. Regarding the common
clinical limitation of data scarcity, we implement a targeted augmentation
pipeline that synthesizes complete sequences of three follow-up scans and
imputes missing MRI modalities from available patient studies, improving the
stability and accuracy of predictive models. Based on merely two follow-up
scans at earlier timepoints, our framework produces flexible time-depending
probability maps, enabling clinicians to interrogate tumor progression risks at
any future temporal milestone. We further introduce a radiotherapy-weighted
focal loss term that leverages radiation dose maps, as these highlight regions
of greater clinical importance during model training. The proposed method was
trained on a public dataset and evaluated on an internal private dataset,
achieving promising results in both cases

</details>


### [62] [Point-Plane Projections for Accurate LiDAR Semantic Segmentation in Small Data Scenarios](https://arxiv.org/abs/2509.10841)
*Simone Mosco,Daniel Fusaro,Wanmeng Li,Emanuele Menegatti,Alberto Pretto*

Main category: cs.CV

TL;DR: 提出了一种新的点云语义分割方法，该方法通过点平面投影从2D表示中学习特征，并引入了几何感知数据增强技术，以提高在数据稀缺场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的点云语义分割方法计算复杂度高，需要大量训练数据，泛化能力有限。

Method: 通过点平面投影从2D表示中学习特征，并引入了几何感知数据增强技术。

Result: 在有限数据场景下性能显著提高，并在两个公开数据集上取得了具有竞争力的结果。

Conclusion: 该方法在数据稀缺场景下表现良好，且在标准数据集上具有竞争力。

Abstract: LiDAR point cloud semantic segmentation is essential for interpreting 3D
environments in applications such as autonomous driving and robotics. Recent
methods achieve strong performance by exploiting different point cloud
representations or incorporating data from other sensors, such as cameras or
external datasets. However, these approaches often suffer from high
computational complexity and require large amounts of training data, limiting
their generalization in data-scarce scenarios. In this paper, we improve the
performance of point-based methods by effectively learning features from 2D
representations through point-plane projections, enabling the extraction of
complementary information while relying solely on LiDAR data. Additionally, we
introduce a geometry-aware technique for data augmentation that aligns with
LiDAR sensor properties and mitigates class imbalance. We implemented and
evaluated our method that applies point-plane projections onto multiple
informative 2D representations of the point cloud. Experiments demonstrate that
this approach leads to significant improvements in limited-data scenarios,
while also achieving competitive results on two publicly available standard
datasets, as SemanticKITTI and PandaSet. The code of our method is available at
https://github.com/SiMoM0/3PNet

</details>


### [63] [OpenUrban3D: Annotation-Free Open-Vocabulary Semantic Segmentation of Large-Scale Urban Point Clouds](https://arxiv.org/abs/2509.10842)
*Chongyu Wang,Kunlei Jing,Jihua Zhu,Di Wang*

Main category: cs.CV

TL;DR: OpenUrban3D: a 3D open-vocabulary semantic segmentation framework for large-scale urban scenes.


<details>
  <summary>Details</summary>
Motivation: Existing 3D segmentation pipelines generalize poorly across diverse urban environments, and high-quality, well-aligned multi-view imagery is often absent.

Method: Multi-view, multi-granularity rendering, mask-level vision-language feature extraction, and sample-balanced fusion, followed by distillation into a 3D backbone model.

Result: Significant improvements in segmentation accuracy and cross-scene generalization over existing methods on SensatUrban and SUM datasets.

Conclusion: OpenUrban3D is a flexible and scalable solution for 3D urban scene understanding.

Abstract: Open-vocabulary semantic segmentation enables models to recognize and segment
objects from arbitrary natural language descriptions, offering the flexibility
to handle novel, fine-grained, or functionally defined categories beyond fixed
label sets. While this capability is crucial for large-scale urban point clouds
that support applications such as digital twins, smart city management, and
urban analytics, it remains largely unexplored in this domain. The main
obstacles are the frequent absence of high-quality, well-aligned multi-view
imagery in large-scale urban point cloud datasets and the poor generalization
of existing three-dimensional (3D) segmentation pipelines across diverse urban
environments with substantial variation in geometry, scale, and appearance. To
address these challenges, we present OpenUrban3D, the first 3D open-vocabulary
semantic segmentation framework for large-scale urban scenes that operates
without aligned multi-view images, pre-trained point cloud segmentation
networks, or manual annotations. Our approach generates robust semantic
features directly from raw point clouds through multi-view, multi-granularity
rendering, mask-level vision-language feature extraction, and sample-balanced
fusion, followed by distillation into a 3D backbone model. This design enables
zero-shot segmentation for arbitrary text queries while capturing both semantic
richness and geometric priors. Extensive experiments on large-scale urban
benchmarks, including SensatUrban and SUM, show that OpenUrban3D achieves
significant improvements in both segmentation accuracy and cross-scene
generalization over existing methods, demonstrating its potential as a flexible
and scalable solution for 3D urban scene understanding.

</details>


### [64] [AutoOEP -- A Multi-modal Framework for Online Exam Proctoring](https://arxiv.org/abs/2509.10887)
*Aryan Kashyap Naveen,Bhuvanesh Singla,Raajan Wankhade,Shreesha M,Ramu S,Ram Mohana Reddy Guddeti*

Main category: cs.CV

TL;DR: 这篇论文介绍了一种名为AutoOEP的自动在线考试监考系统，该系统利用计算机视觉和机器学习技术，通过双摄像头捕捉考生正面和侧面的图像，并使用人脸识别、头部姿势估计、视线跟踪、嘴部运动分析以及物体检测等模块来检测作弊行为。系统综合分析这些信息，计算实时的作弊概率评分。在自定义数据集上的评估结果表明，AutoOEP能够有效且资源高效地进行自动监考。


<details>
  <summary>Details</summary>
Motivation: 传统的在线考试监考方式要么无法规模化应用，要么侵入性过强，或者无法检测到各种作弊行为。因此，需要一种更有效、更全面的自动监考系统。

Method: 该论文提出了一种多模态框架AutoOEP，它结合了计算机视觉和机器学习技术。系统使用双摄像头，并通过人脸模块（进行身份验证、头部姿势估计、视线跟踪和嘴部运动分析）和手部模块（使用YOLOv11模型检测违禁物品并跟踪手部与这些物品的距离）进行并行分析。这些模块的特征被整合到一个LSTM网络中，分析时间模式以计算作弊概率。

Result: AutoOEP系统在分类可疑活动时达到了90.7%的准确率。目标检测组件对违禁物品的平均精度均值（mAP@.5）为0.57。整个框架以大约每秒2.4帧的速度处理视频流，且无需GPU。

Conclusion: AutoOEP是一种有效且资源高效的自动监考解决方案，显著减少了对人工干预的需求，并提高了在线评估的完整性。

Abstract: The burgeoning of online education has created an urgent need for robust and
scalable systems to ensure academic integrity during remote examinations.
Traditional human proctoring is often not feasible at scale, while existing
automated solutions can be intrusive or fail to detect a wide range of cheating
behaviors. This paper introduces AutoOEP (Automated Online Exam Proctoring), a
comprehensive, multi-modal framework that leverages computer vision and machine
learning to provide effective, automated proctoring. The system utilizes a
dual-camera setup to capture both a frontal view of the examinee and a side
view of the workspace, minimizing blind spots. Our approach integrates several
parallel analyses: the Face Module performs continuous identity verification
using ArcFace, along with head pose estimation, gaze tracking, and mouth
movement analysis to detect suspicious cues. Concurrently, the Hand Module
employs a fine-tuned YOLOv11 model for detecting prohibited items (e.g., mobile
phones, notes) and tracks hand proximity to these objects. Features from these
modules are aggregated and fed into a Long Short-Term Memory (LSTM) network
that analyzes temporal patterns to calculate a real-time cheating probability
score. We evaluate AutoOEP on a custom-collected dataset simulating diverse
exam conditions. Our system achieves an accuracy of 90.7% in classifying
suspicious activities. The object detection component obtains a mean Average
Precision (mAP@.5) of 0.57 for prohibited items, and the entire framework
processes video streams at approximately 2.4 frames per second without a GPU.
The results demonstrate that AutoOEP is an effective and resource-efficient
solution for automated proctoring, significantly reducing the need for human
intervention and enhancing the integrity of online assessments.

</details>


### [65] [Total Variation Subgradient Guided Image Fusion for Dual-Camera CASSI System](https://arxiv.org/abs/2509.10897)
*Weiqiang Zhao,Tianzhu Liu,Yuzhe Gui,Yanfeng Gu*

Main category: cs.CV

TL;DR: 提出了一种双摄像头CASSI重建框架，该框架集成了全变分（TV）子梯度理论，以提高高压缩比下的光谱图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖手工设计的图像先验，性能有限；深度学习方法缺乏物理可解释性。

Method: 建立端到端的SD-CASSI数学模型，降低计算复杂度；引入动态正则化策略，结合RGB/全色参考图像的归一化梯度约束，构建具有严格凸优化保证的TV子梯度相似函数；设计自适应参考生成和更新机制。

Result: 有效保持了空间-光谱结构一致性，并在各种重建场景中表现出稳健的性能。

Conclusion: 为计算光谱成像建立了一个可解释的数学基础。

Abstract: Spectral imaging technology has long-faced fundamental challenges in
balancing spectral, spatial, and temporal resolutions. While compressive
sensing-based Coded Aperture Snapshot Spectral Imaging (CASSI) mitigates this
trade-off through optical encoding, high compression ratios result in ill-posed
reconstruction problems. Traditional model-based methods exhibit limited
performance due to reliance on handcrafted inherent image priors, while deep
learning approaches are constrained by their black-box nature, which
compromises physical interpretability. To address these limitations, we propose
a dual-camera CASSI reconstruction framework that integrates total variation
(TV) subgradient theory. By establishing an end-to-end SD-CASSI mathematical
model, we reduce the computational complexity of solving the inverse problem
and provide a mathematically well-founded framework for analyzing multi-camera
systems. A dynamic regularization strategy is introduced, incorporating
normalized gradient constraints from RGB/panchromatic-derived reference images,
which constructs a TV subgradient similarity function with strict convex
optimization guarantees. Leveraging spatial priors from auxiliary cameras, an
adaptive reference generation and updating mechanism is designed to provide
subgradient guidance. Experimental results demonstrate that the proposed method
effectively preserves spatial-spectral structural consistency. The theoretical
framework establishes an interpretable mathematical foundation for
computational spectral imaging, demonstrating robust performance across diverse
reconstruction scenarios. The source code is available at
https://github.com/bestwishes43/ADMM-TVDS.

</details>


### [66] [Lightweight Metadata-Aware Mixture-of-Experts Masked Autoencoder for Earth Observation](https://arxiv.org/abs/2509.10919)
*Mohanad Albughdadi*

Main category: cs.CV

TL;DR: 提出了一种小型元数据感知混合专家掩码自动编码器 (MoE-MAE)，参数仅为 250 万。


<details>
  <summary>Details</summary>
Motivation: 现有地球观测模型计算成本高，限制了其可访问性和在下游任务中的重用。

Method: 该模型结合了稀疏专家路由与地理时间条件，将图像与纬度/经度和季节/每日循环编码相结合。在 BigEarthNet-Landsat 数据集上预训练 MoE-MAE，并使用线性探针评估来自其冻结编码器的嵌入。

Result: 尽管模型尺寸很小，但它可以与更大的架构竞争，表明元数据感知预训练提高了迁移和标签效率。在缺乏显式元数据的 EuroSAT-Landsat 数据集上进行评估，仍然观察到与具有数亿个参数的模型相比具有竞争力的性能。

Conclusion: 紧凑型元数据感知 MoE-MAE 是迈向未来 EO 基础模型的高效且可扩展的一步。

Abstract: Recent advances in Earth Observation have focused on large-scale foundation
models. However, these models are computationally expensive, limiting their
accessibility and reuse for downstream tasks. In this work, we investigate
compact architectures as a practical pathway toward smaller general-purpose EO
models. We propose a Metadata-aware Mixture-of-Experts Masked Autoencoder
(MoE-MAE) with only 2.5M parameters. The model combines sparse expert routing
with geo-temporal conditioning, incorporating imagery alongside
latitude/longitude and seasonal/daily cyclic encodings. We pretrain the MoE-MAE
on the BigEarthNet-Landsat dataset and evaluate embeddings from its frozen
encoder using linear probes. Despite its small size, the model competes with
much larger architectures, demonstrating that metadata-aware pretraining
improves transfer and label efficiency. To further assess generalization, we
evaluate on the EuroSAT-Landsat dataset, which lacks explicit metadata, and
still observe competitive performance compared to models with hundreds of
millions of parameters. These results suggest that compact, metadata-aware
MoE-MAEs are an efficient and scalable step toward future EO foundation models.

</details>


### [67] [Simulating Sinogram-Domain Motion and Correcting Image-Domain Artifacts Using Deep Learning in HR-pQCT Bone Imaging](https://arxiv.org/abs/2509.10961)
*Farhan Sadik,Christopher L. Newman,Stuart J. Warden,Rachel K. Surowiec*

Main category: cs.CV

TL;DR: 论文提出了一种用于校正高分辨率外周定量计算机断层扫描 (HR-pQCT) 图像中运动伪影的方法，该方法基于 Edge-enhanced Self-attention Wasserstein Generative Adversarial Network with Gradient Penalty (ESWGAN-GP)。


<details>
  <summary>Details</summary>
Motivation: 刚性运动伪影阻碍了对骨微结构在体评估。

Method: 该方法优化了传统的基于正弦图的方法来模拟 HR-pQCT 图像中的运动伪影，并使用 ESWGAN-GP 网络进行运动校正。

Result: ESWGAN-GP 在模拟数据集和真实世界数据集中均取得了较好的效果。

Conclusion: 该方法是朝着在 HR-pQCT 中实施基于深度学习的运动校正的重要一步。

Abstract: Rigid-motion artifacts, such as cortical bone streaking and trabecular
smearing, hinder in vivo assessment of bone microstructures in high-resolution
peripheral quantitative computed tomography (HR-pQCT). Despite various motion
grading techniques, no motion correction methods exist due to the lack of
standardized degradation models. We optimize a conventional sinogram-based
method to simulate motion artifacts in HR-pQCT images, creating paired datasets
of motion-corrupted images and their corresponding ground truth, which enables
seamless integration into supervised learning frameworks for motion correction.
As such, we propose an Edge-enhanced Self-attention Wasserstein Generative
Adversarial Network with Gradient Penalty (ESWGAN-GP) to address motion
artifacts in both simulated (source) and real-world (target) datasets. The
model incorporates edge-enhancing skip connections to preserve trabecular edges
and self-attention mechanisms to capture long-range dependencies, facilitating
motion correction. A visual geometry group (VGG)-based perceptual loss is used
to reconstruct fine micro-structural features. The ESWGAN-GP achieves a mean
signal-to-noise ratio (SNR) of 26.78, structural similarity index measure
(SSIM) of 0.81, and visual information fidelity (VIF) of 0.76 for the source
dataset, while showing improved performance on the target dataset with an SNR
of 29.31, SSIM of 0.87, and VIF of 0.81. The proposed methods address a
simplified representation of real-world motion that may not fully capture the
complexity of in vivo motion artifacts. Nevertheless, because motion artifacts
present one of the foremost challenges to more widespread adoption of this
modality, these methods represent an important initial step toward implementing
deep learning-based motion correction in HR-pQCT.

</details>


### [68] [Gaze Authentication: Factors Influencing Authentication Performance](https://arxiv.org/abs/2509.10969)
*Dillon Lohr,Michael J Proulx,Mehedi Hasan Raju,Oleg V Komogortsev*

Main category: cs.CV

TL;DR: 本文研究了影响最先进的基于注视的身份验证性能的关键因素。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于深入了解如何提升基于注视的身份验证的性能。

Method: 使用最先进的神经网络架构，分析眼动信号质量、眼动校准的各个方面以及对估计的原始注视进行简单过滤的影响。

Result: 发现使用相同的校准目标深度进行眼动追踪校准、融合校准的和非校准的注视以及提高眼动信号质量均能增强身份验证性能。同时发现简单的三样本移动平均滤波器通常会略微降低身份验证性能。

Conclusion: 研究结果对优化基于注视的身份验证系统具有指导意义，但也指出了一些例外情况。

Abstract: This paper examines the key factors that influence the performance of
state-of-the-art gaze-based authentication. Experiments were conducted on a
large-scale, in-house dataset comprising 8,849 subjects collected with Meta
Quest Pro equivalent hardware running a video oculography-driven gaze
estimation pipeline at 72Hz. The state-of-the-art neural network architecture
was employed to study the influence of the following factors on authentication
performance: eye tracking signal quality, various aspects of eye tracking
calibration, and simple filtering on estimated raw gaze. We found that using
the same calibration target depth for eye tracking calibration, fusing
calibrated and non-calibrated gaze, and improving eye tracking signal quality
all enhance authentication performance. We also found that a simple
three-sample moving average filter slightly reduces authentication performance
in general. While these findings hold true for the most part, some exceptions
were noted.

</details>


### [69] [TrueSkin: Towards Fair and Accurate Skin Tone Recognition and Generation](https://arxiv.org/abs/2509.10980)
*Haoming Lu*

Main category: cs.CV

TL;DR: 提出了一个名为 TrueSkin 的数据集，用于解决现有模型在肤色识别和生成方面的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型和图像生成模型在识别和合成肤色方面存在困难，且缺乏全面的数据集。

Method: 构建了一个包含7299张图像的 TrueSkin 数据集，并使用该数据集对现有识别和生成方法进行基准测试。

Result: 发现 LMM 倾向于将中间肤色错误分类为较浅的肤色，而生成模型难以准确生成指定的肤色。使用 TrueSkin 训练识别模型可将分类准确率提高 20% 以上，并且可以显著提高图像生成模型中的肤色保真度。

Conclusion: 强调了像 TrueSkin 这样全面的数据集的必要性，它不仅可以作为评估现有模型的基准，还可以作为宝贵的训练资源，以提高肤色识别和生成任务的公平性和准确性。

Abstract: Skin tone recognition and generation play important roles in model fairness,
healthcare, and generative AI, yet they remain challenging due to the lack of
comprehensive datasets and robust methodologies. Compared to other human image
analysis tasks, state-of-the-art large multimodal models (LMMs) and image
generation models struggle to recognize and synthesize skin tones accurately.
To address this, we introduce TrueSkin, a dataset with 7299 images
systematically categorized into 6 classes, collected under diverse lighting
conditions, camera angles, and capture settings. Using TrueSkin, we benchmark
existing recognition and generation approaches, revealing substantial biases:
LMMs tend to misclassify intermediate skin tones as lighter ones, whereas
generative models struggle to accurately produce specified skin tones when
influenced by inherent biases from unrelated attributes in the prompts, such as
hairstyle or environmental context. We further demonstrate that training a
recognition model on TrueSkin improves classification accuracy by more than
20\% compared to LMMs and conventional approaches, and fine-tuning with
TrueSkin significantly improves skin tone fidelity in image generation models.
Our findings highlight the need for comprehensive datasets like TrueSkin, which
not only serves as a benchmark for evaluating existing models but also provides
a valuable training resource to enhance fairness and accuracy in skin tone
recognition and generation tasks.

</details>


### [70] [Policy-Driven Transfer Learning in Resource-Limited Animal Monitoring](https://arxiv.org/abs/2509.10995)
*Nisha Pillai,Aditi Virupakshaiah,Harrison W. Smith,Amanda J. Ashworth,Prasanna Gowda,Phillip R. Owens,Adam R. Rivers,Bindu Nanduri,Mahalingam Ramkumar*

Main category: cs.CV

TL;DR: 提出了一种基于强化学习的迁移学习框架，用于自动选择最适合动物检测任务的预训练模型。


<details>
  <summary>Details</summary>
Motivation: 在野生动物保护和牲畜管理中，动物健康监测和种群管理至关重要，越来越多地依赖于自动检测和跟踪系统。无人机系统结合计算机视觉为非侵入式动物监测提供了有希望的解决方案，但标记训练数据的有限可用性仍然是开发有效的深度学习模型的障碍。预训练神经网络架构选择困难。

Method: 采用基于强化学习的迁移学习框架，该框架采用上限置信区间算法自动选择最合适的预训练模型。

Result: 该框架实现了更高的检测率，同时需要的计算时间明显少于传统方法。

Conclusion: 该方法系统地评估和排序候选模型，简化了模型选择过程。

Abstract: Animal health monitoring and population management are critical aspects of
wildlife conservation and livestock management that increasingly rely on
automated detection and tracking systems. While Unmanned Aerial Vehicle (UAV)
based systems combined with computer vision offer promising solutions for
non-invasive animal monitoring across challenging terrains, limited
availability of labeled training data remains an obstacle in developing
effective deep learning (DL) models for these applications. Transfer learning
has emerged as a potential solution, allowing models trained on large datasets
to be adapted for resource-limited scenarios such as those with limited data.
However, the vast landscape of pre-trained neural network architectures makes
it challenging to select optimal models, particularly for researchers new to
the field. In this paper, we propose a reinforcement learning (RL)-based
transfer learning framework that employs an upper confidence bound (UCB)
algorithm to automatically select the most suitable pre-trained model for
animal detection tasks. Our approach systematically evaluates and ranks
candidate models based on their performance, streamlining the model selection
process. Experimental results demonstrate that our framework achieves a higher
detection rate while requiring significantly less computational time compared
to traditional methods.

</details>


### [71] [Improving Fungi Prototype Representations for Few-Shot Classification](https://arxiv.org/abs/2509.11020)
*Abdarahmane Traore,Éric Hervet,Andy Couturier*

Main category: cs.CV

TL;DR: FungiCLEF 2025 focuses on automatic fungal species recognition using field data, addressing challenges like imbalanced data and limited samples for rare species.


<details>
  <summary>Details</summary>
Motivation: Accurate fungal identification tools are crucial for mycologists and citizen scientists to improve biodiversity monitoring.

Method: A robust deep learning method based on prototypical networks is proposed to enhance prototype representations for few-shot fungal classification.

Result: The proposed method outperforms the competition baseline by over 30 percentage points in Recall@5 on both public and private leaderboards.

Conclusion: The method shows strong potential for accurate identification of both common and rare fungal species, supporting FungiCLEF 2025's objectives.

Abstract: The FungiCLEF 2025 competition addresses the challenge of automatic fungal
species recognition using realistic, field-collected observational data.
Accurate identification tools support both mycologists and citizen scientists,
greatly enhancing large-scale biodiversity monitoring. Effective recognition
systems in this context must handle highly imbalanced class distributions and
provide reliable performance even when very few training samples are available
for many species, especially rare and under-documented taxa that are often
missing from standard training sets. According to competition organizers, about
20\% of all verified fungi observations, representing nearly 20,000 instances,
are associated with these rarely recorded species. To tackle this challenge, we
propose a robust deep learning method based on prototypical networks, which
enhances prototype representations for few-shot fungal classification. Our
prototypical network approach exceeds the competition baseline by more than 30
percentage points in Recall@5 on both the public (PB) and private (PR)
leaderboards. This demonstrates strong potential for accurately identifying
both common and rare fungal species, supporting the main objectives of
FungiCLEF 2025.

</details>


### [72] [Cluster-Level Sparse Multi-Instance Learning for Whole-Slide Images](https://arxiv.org/abs/2509.11034)
*Yuedi Zhang,Zhixiang Xia,Guosheng Yin,Bin Liu*

Main category: cs.CV

TL;DR: 提出了一种新的多示例学习框架，通过聚类和稀疏化来提高在病理学图像分析等任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统MIL方法在处理冗余实例和缺乏有效排除非信息实例的机制方面存在不足，影响了鲁棒性和可解释性。

Method: 该方法首先进行全局聚类，然后在每个包内进行局部聚类，计算簇内注意力得分，并应用稀疏正则化来选择性地保留相关簇。

Result: 在两个公开的组织病理学基准数据集上取得了state-of-the-art的性能。

Conclusion: 该方法增强了对噪声实例的鲁棒性，提高了可解释性，并降低了计算复杂度。

Abstract: Multi-Instance Learning (MIL) is pivotal for analyzing complex, weakly
labeled datasets, such as whole-slide images (WSIs) in computational pathology,
where bags comprise unordered collections of instances with sparse diagnostic
relevance. Traditional MIL approaches, including early statistical methods and
recent attention-based frameworks, struggle with instance redundancy and lack
explicit mechanisms for discarding non-informative instances, limiting their
robustness and interpretability. We propose Cluster-level Sparse MIL (csMIL), a
novel framework that integrates global-local instance clustering,
within-cluster attention, and cluster-level sparsity induction to address these
challenges. Our csMIL first performs global clustering across all bags to
establish $K$ cluster centers, followed by local clustering within each bag to
assign cluster labels. Attention scores are computed within each cluster, and
sparse regularization is applied to cluster weights, enabling the selective
retention of diagnostically relevant clusters while discarding irrelevant ones.
This approach enhances robustness to noisy instances, improves interpretability
by identifying critical regions, and reduces computational complexity.
Theoretical analysis demonstrates that csMIL requires $O(s log K)$ bags to
recover $s$ relevant clusters, aligning with compressed sensing principles.
Empirically, csMIL achieves state-of-the-art performance on two public
histopathology benchmarks (CAMELYON16, TCGA-NSCLC).

</details>


### [73] [Action Hints: Semantic Typicality and Context Uniqueness for Generalizable Skeleton-based Video Anomaly Detection](https://arxiv.org/abs/2509.11058)
*Canhui Tang,Sanping Zhou,Haoyue Shi,Le Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的零样本视频异常检测框架，通过动作典型性和独特性学习来释放骨骼数据的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的方法只学习低层次的骨骼表征，并依赖于领域限制的正常边界，这不能很好地推广到具有不同正常和异常行为模式的新场景。

Method: 首先，我们引入了一个语言引导的语义典型性建模模块，将骨骼片段投影到动作语义空间，并提炼LLM在训练过程中对典型正常和异常行为的知识。其次，我们提出了一个测试时上下文独特性分析模块，以精细地分析骨骼片段之间的时空差异，然后推导出场景自适应边界。

Result: 我们的方法在四个大型VAD数据集上实现了最先进的结果：ShanghaiTech、UBnormal、NWPU和UCF-Crime，具有超过100个未见过的监控场景。

Conclusion: 该方法在零样本视频异常检测任务中，无需目标域训练数据，即可在多个数据集上实现最先进的性能。

Abstract: Zero-Shot Video Anomaly Detection (ZS-VAD) requires temporally localizing
anomalies without target domain training data, which is a crucial task due to
various practical concerns, e.g., data privacy or new surveillance deployments.
Skeleton-based approach has inherent generalizable advantages in achieving
ZS-VAD as it eliminates domain disparities both in background and human
appearance. However, existing methods only learn low-level skeleton
representation and rely on the domain-limited normality boundary, which cannot
generalize well to new scenes with different normal and abnormal behavior
patterns. In this paper, we propose a novel zero-shot video anomaly detection
framework, unlocking the potential of skeleton data via action typicality and
uniqueness learning. Firstly, we introduce a language-guided semantic
typicality modeling module that projects skeleton snippets into action semantic
space and distills LLM's knowledge of typical normal and abnormal behaviors
during training. Secondly, we propose a test-time context uniqueness analysis
module to finely analyze the spatio-temporal differences between skeleton
snippets and then derive scene-adaptive boundaries. Without using any training
samples from the target domain, our method achieves state-of-the-art results
against skeleton-based methods on four large-scale VAD datasets: ShanghaiTech,
UBnormal, NWPU, and UCF-Crime, featuring over 100 unseen surveillance scenes.

</details>


### [74] [Organoid Tracker: A SAM2-Powered Platform for Zero-shot Cyst Analysis in Human Kidney Organoid Videos](https://arxiv.org/abs/2509.11063)
*Xiaoyu Huang,Lauren M Maxson,Trang Nguyen,Cheng Jack Song,Yuankai Huo*

Main category: cs.CV

TL;DR: 开发了一个名为 Organoid Tracker 的图形界面平台，用于分析多囊肾病 (PKD) 的肾脏类器官视频数据。


<details>
  <summary>Details</summary>
Motivation: 当前手动分析方法粗糙，缺乏像素级和纵向信息。

Method: 利用 Segment Anything Model 2 (SAM2) 实现零样本分割和自动化分析。

Result: 可以量化囊肿形成率、生长速度和形态变化等关键指标，并生成综合报告。

Conclusion: Organoid Tracker 为改进和加速肾脏发育、PKD 建模和治疗发现的研究提供了一个强大的解决方案。

Abstract: Recent advances in organoid models have revolutionized the study of human
kidney disease mechanisms and drug discovery by enabling scalable,
cost-effective research without the need for animal sacrifice. Here, we present
a kidney organoid platform optimized for efficient screening in polycystic
kidney disease (PKD). While these systems generate rich spatial-temporal
microscopy video datasets, current manual approaches to analysis remain limited
to coarse classifications (e.g., hit vs. non-hit), often missing valuable
pixel-level and longitudinal information. To help overcome this bottleneck, we
developed Organoid Tracker, a graphical user interface (GUI) platform designed
with a modular plugin architecture, which empowers researchers to extract
detailed, quantitative metrics without programming expertise. Built on the
cutting-edge vision foundation model Segment Anything Model 2 (SAM2), Organoid
Tracker enables zero-shot segmentation and automated analysis of
spatial-temporal microscopy videos. It quantifies key metrics such as cyst
formation rate, growth velocity, and morphological changes, while generating
comprehensive reports. By providing an extensible, open-source framework,
Organoid Tracker offers a powerful solution for improving and accelerating
research in kidney development, PKD modeling, and therapeutic discovery. The
platform is publicly available as open-source software at
https://github.com/hrlblab/OrganoidTracker.

</details>


### [75] [The System Description of CPS Team for Track on Driving with Language of CVPR 2024 Autonomous Grand Challenge](https://arxiv.org/abs/2509.11071)
*Jinghan Peng,Jingwen Wang,Xing Yu,Dehui Du*

Main category: cs.CV

TL;DR: 本报告概述了我们使用视觉语言模型系统参加 CVPR 2024 自动驾驶挑战赛的方法，在 Driving with Language 赛道中排名第一。


<details>
  <summary>Details</summary>
Motivation: 利用视觉语言模型解决自动驾驶中的语言理解问题。

Method: 使用 DriveLM-nuScenes 数据集训练 LLaVA 模型，通过 LoRA 和 DoRA 方法进行微调，并整合开源深度估计模型的深度信息。采用 Chain-of-Thought 推理方法。

Result: 在验证集排行榜上取得了 0.7799 的最高分。

Conclusion: 通过综合方法，在 CVPR 2024 自动驾驶挑战赛的 Driving with Language 赛道中获得了第一名。

Abstract: This report outlines our approach using vision language model systems for the
Driving with Language track of the CVPR 2024 Autonomous Grand Challenge. We
have exclusively utilized the DriveLM-nuScenes dataset for training our models.
Our systems are built on the LLaVA models, which we enhanced through
fine-tuning with the LoRA and DoRA methods. Additionally, we have integrated
depth information from open-source depth estimation models to enrich the
training and inference processes. For inference, particularly with
multiple-choice and yes/no questions, we adopted a Chain-of-Thought reasoning
approach to improve the accuracy of the results. This comprehensive methodology
enabled us to achieve a top score of 0.7799 on the validation set leaderboard,
ranking 1st on the leaderboard.

</details>


### [76] [Mars Traversability Prediction: A Multi-modal Self-supervised Approach for Costmap Generation](https://arxiv.org/abs/2509.11082)
*Zongwu Xie,Kaijie Yun,Yang Liu,Yiming Ji,Han Li*

Main category: cs.CV

TL;DR: 本文提出了一个用于预测行星漫游车可通行性成本图的鲁棒多模态框架，该模型融合了相机和激光雷达数据，并使用IMU导出的标签进行自监督训练。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在预测行星漫游车的可通行性成本图。

Method: 该模型融合了相机和激光雷达数据，使用DINOv3的图像编码器，基于FiLM的传感器融合，以及结合Huber和smoothness项的优化损失函数进行自监督训练。

Result: 实验结果表明，即使在移除图像颜色、遮挡输入、添加噪声等情况下，MAE/MSE只有轻微变化，表明几何信息主导了学习到的成本，且模型具有高度的鲁棒性。

Conclusion: 本文的主要贡献包括：一个高保真、可复现的仿真环境；一个基于自监督IMU的标签生成流程；以及一个强大的多模态BEV成本图预测模型。

Abstract: We present a robust multi-modal framework for predicting traversability
costmaps for planetary rovers. Our model fuses camera and LiDAR data to produce
a bird's-eye-view (BEV) terrain costmap, trained self-supervised using
IMU-derived labels. Key updates include a DINOv3-based image encoder,
FiLM-based sensor fusion, and an optimization loss combining Huber and
smoothness terms. Experimental ablations (removing image color, occluding
inputs, adding noise) show only minor changes in MAE/MSE (e.g. MAE increases
from ~0.0775 to 0.0915 when LiDAR is sparsified), indicating that geometry
dominates the learned cost and the model is highly robust. We attribute the
small performance differences to the IMU labeling primarily reflecting terrain
geometry rather than semantics and to limited data diversity. Unlike prior work
claiming large gains, we emphasize our contributions: (1) a high-fidelity,
reproducible simulation environment; (2) a self-supervised IMU-based labeling
pipeline; and (3) a strong multi-modal BEV costmap prediction model. We discuss
limitations and future work such as domain generalization and dataset
expansion.

</details>


### [77] [End-to-End Visual Autonomous Parking via Control-Aided Attention](https://arxiv.org/abs/2509.11090)
*Chao Chen,Shunyu Yao,Yuanwu He,Tao Feng,Ruojing Song,Yuliang Guo,Xinyu Huang,Chenxu Wu,Ren Liu,Chen Feng*

Main category: cs.CV

TL;DR: 提出了一种名为CAA-Policy的端到端模仿学习系统，通过控制信号引导视觉注意力的学习，从而实现精确停车。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端学习方法在感知和控制之间缺乏有效的协同作用，并且基于Transformer的自注意力机制会产生不稳定和时序上不一致的空间注意力。

Method: 提出了一种新颖的控制辅助注意力（CAA）机制，并以自监督的方式训练注意力模块，使用来自控制输出的反向传播梯度，而不是来自训练损失。

Result: 在CARLA模拟器中的大量实验表明，CAA-Policy始终优于端到端学习基线和模块化BEV分割+混合A*管道，实现了卓越的准确性、鲁棒性和可解释性。

Conclusion: CAA-Policy通过控制信号引导视觉注意力的学习，从而实现了精确停车，并且具有更好的鲁棒性和泛化能力。

Abstract: Precise parking requires an end-to-end system where perception adaptively
provides policy-relevant details-especially in critical areas where fine
control decisions are essential. End-to-end learning offers a unified framework
by directly mapping sensor inputs to control actions, but existing approaches
lack effective synergy between perception and control. We find that
transformer-based self-attention, when used alone, tends to produce unstable
and temporally inconsistent spatial attention, which undermines the reliability
of downstream policy decisions over time. Instead, we propose CAA-Policy, an
end-to-end imitation learning system that allows control signal to guide the
learning of visual attention via a novel Control-Aided Attention (CAA)
mechanism. For the first time, we train such an attention module in a
self-supervised manner, using backpropagated gradients from the control outputs
instead of from the training loss. This strategy encourages the attention to
focus on visual features that induce high variance in action outputs, rather
than merely minimizing the training loss-a shift we demonstrate leads to a more
robust and generalizable policy. To further enhance stability, CAA-Policy
integrates short-horizon waypoint prediction as an auxiliary task, and
introduces a separately trained motion prediction module to robustly track the
target spot over time. Extensive experiments in the CARLA simulator show that
\titlevariable~consistently surpasses both the end-to-end learning baseline and
the modular BEV segmentation + hybrid A* pipeline, achieving superior accuracy,
robustness, and interpretability. Code is released at
https://github.com/Joechencc/CAAPolicy.

</details>


### [78] [PanoLora: Bridging Perspective and Panoramic Video Generation with LoRA Adaptation](https://arxiv.org/abs/2509.11092)
*Zeyu Dong,Yuyang Yin,Yuqi Li,Eric Li,Hao-Xiang Guo,Yikai Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的360度全景视频生成方法，该方法通过LoRA有效地调整了预训练的视频扩散模型，实现了高质量的全景生成。


<details>
  <summary>Details</summary>
Motivation: 现有的全景视频生成方法由于全景和传统透视投影之间的差异，难以适应，并且效率低下，效果欠佳。

Method: 将全景视频生成视为透视视图的适配问题，并使用LoRA对预训练的视频扩散模型进行微调。

Result: 该方法仅使用约1,000个视频进行微调，即可实现高质量的全景生成，并在视觉质量、左右一致性和运动多样性方面优于现有技术。

Conclusion: LoRA可以有效地模拟全景和透视投影之间的转换，并且该方法能够保持适当的投影几何形状。

Abstract: Generating high-quality 360{\deg} panoramic videos remains a significant
challenge due to the fundamental differences between panoramic and traditional
perspective-view projections. While perspective videos rely on a single
viewpoint with a limited field of view, panoramic content requires rendering
the full surrounding environment, making it difficult for standard video
generation models to adapt. Existing solutions often introduce complex
architectures or large-scale training, leading to inefficiency and suboptimal
results. Motivated by the success of Low-Rank Adaptation (LoRA) in style
transfer tasks, we propose treating panoramic video generation as an adaptation
problem from perspective views. Through theoretical analysis, we demonstrate
that LoRA can effectively model the transformation between these projections
when its rank exceeds the degrees of freedom in the task. Our approach
efficiently fine-tunes a pretrained video diffusion model using only
approximately 1,000 videos while achieving high-quality panoramic generation.
Experimental results demonstrate that our method maintains proper projection
geometry and surpasses previous state-of-the-art approaches in visual quality,
left-right consistency, and motion diversity.

</details>


### [79] [SMILE: A Super-resolution Guided Multi-task Learning Method for Hyperspectral Unmixing](https://arxiv.org/abs/2509.11093)
*Ruiying Li,Bin Pan,Qiaoying Qu,Xia Xu,Zhenwei Shi*

Main category: cs.CV

TL;DR: 本文提出了一种超分辨率引导的多任务学习高光谱解混方法(SMILE)，以解决低空间分辨率下的高光谱解混问题。


<details>
  <summary>Details</summary>
Motivation: 直接整合超分辨率和解混可能存在任务亲和性未验证和解混收敛性无法保证的问题。

Method: 该方法通过学习共享和特定表示，从超分辨率中概括出积极信息到解混中。此外，为了保证收敛性，通过证明解混的最优解的可达性，提供了可达性定理。

Result: 在合成和真实数据集上的实验证实了该方法的有效性。

Conclusion: SMILE的主要贡献包括提供渐进的理论支持，并设计了一个新的超分辨率引导下的解混框架。

Abstract: The performance of hyperspectral unmixing may be constrained by low spatial
resolution, which can be enhanced using super-resolution in a multitask
learning way. However, integrating super-resolution and unmixing directly may
suffer two challenges: Task affinity is not verified, and the convergence of
unmixing is not guaranteed. To address the above issues, in this paper, we
provide theoretical analysis and propose super-resolution guided multi-task
learning method for hyperspectral unmixing (SMILE). The provided theoretical
analysis validates feasibility of multitask learning way and verifies task
affinity, which consists of relationship and existence theorems by proving the
positive guidance of super-resolution. The proposed framework generalizes
positive information from super-resolution to unmixing by learning both shared
and specific representations. Moreover, to guarantee the convergence, we
provide the accessibility theorem by proving the optimal solution of unmixing.
The major contributions of SMILE include providing progressive theoretical
support, and designing a new framework for unmixing under the guidance of
super-resolution. Our experiments on both synthetic and real datasets have
substantiate the usefulness of our work.

</details>


### [80] [A Copula-Guided Temporal Dependency Method for Multitemporal Hyperspectral Images Unmixing](https://arxiv.org/abs/2509.11096)
*Ruiying Li,Bin Pan,Qiaoying Qu,Xia Xu,Zhenwei Shi*

Main category: cs.CV

TL;DR: 本文提出了一种基于 copula 的时序高光谱解混方法 (Cog-TD)，以解决现有方法在建模时间依赖性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在建模时间依赖性方面存在局限性，无法捕捉动态材料的演变。

Method: 提出了一个 copula 引导的时序依赖性方法 (Cog-TD)，包括定义新的数学模型、构建 copula 引导的框架以及提供两个关键模块。

Result: 在合成和真实世界数据集上的实验结果表明了该方法的有效性。

Conclusion: 本文重新定义了具有时间依赖性的 MTHU 问题，提出了一个 copula 引导的框架，开发了两个关键模块，并提供了理论支持。

Abstract: Multitemporal hyperspectral unmixing (MTHU) aims to model variable endmembers
and dynamical abundances, which emphasizes the critical temporal information.
However, existing methods have limitations in modeling temporal dependency,
thus fail to capture the dynamical material evolution. Motivated by the ability
of copula theory in modeling dependency structure explicitly, in this paper, we
propose a copula-guided temporal dependency method (Cog-TD) for multitemporal
hyperspectral unmixing. Cog-TD defines new mathematical model, constructs
copula-guided framework and provides two key modules with theoretical support.
The mathematical model provides explicit formulations for MTHU problem
definition, which describes temporal dependency structure by incorporating
copula theory. The copula-guided framework is constructed for utilizing copula
function, which estimates dynamical endmembers and abundances with temporal
dependency. The key modules consist of copula function estimation and temporal
dependency guidance, which computes and employs temporal information to guide
unmixing process. Moreover, the theoretical support demonstrates that estimated
copula function is valid and the represented temporal dependency exists in
hyperspectral images. The major contributions of this paper include redefining
MTHU problem with temporal dependency, proposing a copula-guided framework,
developing two key modules and providing theoretical support. Our experimental
results on both synthetic and real-world datasets demonstrate the utility of
the proposed method.

</details>


### [81] [3DAeroRelief: The first 3D Benchmark UAV Dataset for Post-Disaster Assessment](https://arxiv.org/abs/2509.11097)
*Nhut Le,Ehsan Karimi,Maryam Rahnemoonfar*

Main category: cs.CV

TL;DR: 提出了一个用于灾后评估的3D基准数据集3DAeroRelief，该数据集使用无人机在飓风灾区收集，包含密集的3D点云，并进行了语义标注。


<details>
  <summary>Details</summary>
Motivation: 现有的3D基准数据集主要关注城市或室内场景，很少关注受灾地区。2D图像缺乏深度信息，且易受遮挡。

Method: 使用低成本无人机(UAVs)在飓风受损地区收集数据，通过Structure-from-Motion和Multi-View Stereo技术重建密集的3D点云，并通过手动2D标记投影到3D空间生成语义标注。

Result: 评估了几种最先进的3D分割模型，突出了灾害响应中3D场景理解的挑战和机遇。

Conclusion: 该数据集为在灾后场景的实际应用中推进鲁棒的3D视觉系统提供了一个有价值的资源。

Abstract: Timely assessment of structural damage is critical for disaster response and
recovery. However, most prior work in natural disaster analysis relies on 2D
imagery, which lacks depth, suffers from occlusions, and provides limited
spatial context. 3D semantic segmentation offers a richer alternative, but
existing 3D benchmarks focus mainly on urban or indoor scenes, with little
attention to disaster-affected areas. To address this gap, we present
3DAeroRelief--the first 3D benchmark dataset specifically designed for
post-disaster assessment. Collected using low-cost unmanned aerial vehicles
(UAVs) over hurricane-damaged regions, the dataset features dense 3D point
clouds reconstructed via Structure-from-Motion and Multi-View Stereo
techniques. Semantic annotations were produced through manual 2D labeling and
projected into 3D space. Unlike existing datasets, 3DAeroRelief captures 3D
large-scale outdoor environments with fine-grained structural damage in
real-world disaster contexts. UAVs enable affordable, flexible, and safe data
collection in hazardous areas, making them particularly well-suited for
emergency scenarios. To demonstrate the utility of 3DAeroRelief, we evaluate
several state-of-the-art 3D segmentation models on the dataset to highlight
both the challenges and opportunities of 3D scene understanding in disaster
response. Our dataset serves as a valuable resource for advancing robust 3D
vision systems in real-world applications for post-disaster scenarios.

</details>


### [82] [Filling the Gaps: A Multitask Hybrid Multiscale Generative Framework for Missing Modality in Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2509.11102)
*Nhi Kieu,Kien Nguyen,Arnold Wiliem,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

TL;DR: 提出了GEMMNet，一个用于解决遥感语义分割中多模态数据缺失问题的新型生成增强多模态学习网络。


<details>
  <summary>Details</summary>
Motivation: 多模态学习在各种领域表现出显著的性能提升，但在现实场景中，由于传感器故障和恶劣天气条件，多模态信号容易缺失，从而严重影响模型的运行和性能。现有的生成模型在处理多模态遥感数据的异构性方面存在局限性，并且容易过度依赖主导模态，从而影响模型在缺失模态条件下的鲁棒性。

Method: 提出了一个名为GEMMNet的生成增强多模态学习网络，它包含三个关键组件：混合特征提取器（HyFEx）、具有多尺度感知的混合融合（HyFMA）和互补损失（CoLoss）方案。

Result: GEMMNet在两个具有挑战性的语义分割遥感数据集（Vaihingen和Potsdam）上优于生成基线AE、cGAN以及最先进的非生成方法mmformer和shaspec。

Conclusion: GEMMNet有效地解决了遥感语义分割中多模态数据缺失的问题，并在性能上优于现有方法。

Abstract: Multimodal learning has shown significant performance boost compared to
ordinary unimodal models across various domains. However, in real-world
scenarios, multimodal signals are susceptible to missing because of sensor
failures and adverse weather conditions, which drastically deteriorates models'
operation and performance. Generative models such as AutoEncoder (AE) and
Generative Adversarial Network (GAN) are intuitive solutions aiming to
reconstruct missing modality from available ones. Yet, their efficacy in remote
sensing semantic segmentation remains underexplored. In this paper, we first
examine the limitations of existing generative approaches in handling the
heterogeneity of multimodal remote sensing data. They inadequately capture
semantic context in complex scenes with large intra-class and small inter-class
variation. In addition, traditional generative models are susceptible to heavy
dependence on the dominant modality, introducing bias that affects model
robustness under missing modality conditions. To tackle these limitations, we
propose a novel Generative-Enhanced MultiModal learning Network (GEMMNet) with
three key components: (1) Hybrid Feature Extractor (HyFEx) to effectively learn
modality-specific representations, (2) Hybrid Fusion with Multiscale Awareness
(HyFMA) to capture modality-synergistic semantic context across scales and (3)
Complementary Loss (CoLoss) scheme to alleviate the inherent bias by
encouraging consistency across modalities and tasks. Our method, GEMMNet,
outperforms both generative baselines AE, cGAN (conditional GAN), and
state-of-the-art non-generative approaches - mmformer and shaspec - on two
challenging semantic segmentation remote sensing datasets (Vaihingen and
Potsdam). Source code is made available.

</details>


### [83] [WildSmoke: Ready-to-Use Dynamic 3D Smoke Assets from a Single Video in the Wild](https://arxiv.org/abs/2509.11114)
*Yuqiu Liu,Jialin Song,Manolis Savva,Wuyang Chen*

Main category: cs.CV

TL;DR: 提出了一个从单个真实视频中提取和重建动态3D烟雾资产的流程，并进一步整合交互式模拟以进行烟雾设计和编辑。


<details>
  <summary>Details</summary>
Motivation: 现有流体重建技术严重依赖于精心控制的干净实验室环境，而对在野外捕获的真实世界视频的探索很大程度上不足。重建真实世界视频中的烟雾存在三个关键挑战：背景去除的烟雾提取、烟雾粒子和相机姿势的初始化以及推断多视图视频。

Method: 设计了针对性技术，包括背景去除的烟雾提取、烟雾粒子和相机姿势的初始化以及推断多视图视频。

Result: 该方法不仅优于以往的重建和生成方法，具有高质量的烟雾重建（在真实视频上平均PSNR +2.22），而且还能够通过模拟烟雾资产来实现对流体动力学的多样化和逼真编辑。

Conclusion: 提供模型、数据和4D烟雾资产。

Abstract: We propose a pipeline to extract and reconstruct dynamic 3D smoke assets from
a single in-the-wild video, and further integrate interactive simulation for
smoke design and editing. Recent developments in 3D vision have significantly
improved reconstructing and rendering fluid dynamics, supporting realistic and
temporally consistent view synthesis. However, current fluid reconstructions
rely heavily on carefully controlled clean lab environments, whereas real-world
videos captured in the wild are largely underexplored. We pinpoint three key
challenges of reconstructing smoke in real-world videos and design targeted
techniques, including smoke extraction with background removal, initialization
of smoke particles and camera poses, and inferring multi-view videos. Our
method not only outperforms previous reconstruction and generation methods with
high-quality smoke reconstructions (+2.22 average PSNR on wild videos), but
also enables diverse and realistic editing of fluid dynamics by simulating our
smoke assets. We provide our models, data, and 4D smoke assets at
[https://autumnyq.github.io/WildSmoke](https://autumnyq.github.io/WildSmoke).

</details>


### [84] [SVR-GS: Spatially Variant Regularization for Probabilistic Masks in 3D Gaussian Splatting](https://arxiv.org/abs/2509.11116)
*Ashkan Taghipour,Vahid Naghshin,Benjamin Southwell,Farid Boussaid,Hamid Laga,Mohammed Bennamoun*

Main category: cs.CV

TL;DR: SVR-GS reduces the number of Gaussians in 3DGS while maintaining image quality.


<details>
  <summary>Details</summary>
Motivation: Existing mask-based pruning in 3DGS is misaligned with the local per-pixel reconstruction loss.

Method: Introduces a spatially variant regularizer (SVR-GS) that renders a per-pixel spatial mask from each Gaussian's effective contribution along the ray.

Result: SVR-GS reduces the number of Gaussians by 1.79x compared to MaskGS and 5.63x compared to 3DGS, with minimal PSNR drops.

Conclusion: SVR-GS creates smaller, faster, and more memory-efficient models suitable for real-time applications.

Abstract: 3D Gaussian Splatting (3DGS) enables fast, high-quality novel view synthesis
but typically relies on densification followed by pruning to optimize the
number of Gaussians. Existing mask-based pruning, such as MaskGS, regularizes
the global mean of the mask, which is misaligned with the local per-pixel
(per-ray) reconstruction loss that determines image quality along individual
camera rays. This paper introduces SVR-GS, a spatially variant regularizer that
renders a per-pixel spatial mask from each Gaussian's effective contribution
along the ray, thereby applying sparsity pressure where it matters: on
low-importance Gaussians. We explore three spatial-mask aggregation strategies,
implement them in CUDA, and conduct a gradient analysis to motivate our final
design. Extensive experiments on Tanks\&Temples, Deep Blending, and Mip-NeRF360
datasets demonstrate that, on average across the three datasets, the proposed
SVR-GS reduces the number of Gaussians by 1.79\(\times\) compared to MaskGS and
5.63\(\times\) compared to 3DGS, while incurring only 0.50 dB and 0.40 dB PSNR
drops, respectively. These gains translate into significantly smaller, faster,
and more memory-efficient models, making them well-suited for real-time
applications such as robotics, AR/VR, and mobile perception.

</details>


### [85] [No Mesh, No Problem: Estimating Coral Volume and Surface from Sparse Multi-View Images](https://arxiv.org/abs/2509.11164)
*Diego Eustachio Farchione,Ramzi Idoughi,Peter Wonka*

Main category: cs.CV

TL;DR: 提出了一种新的学习框架，用于预测珊瑚类物体的3D体积和表面积。


<details>
  <summary>Details</summary>
Motivation: 有效的珊瑚礁监测需要量化珊瑚的生长，通过精确的体积和表面积估计，这是一项具有挑战性的任务，因为珊瑚的形态复杂。

Method: 该方法利用预训练模块(VGGT)从每个视图中提取密集点图;这些地图被合并成一个统一的点云，并用每个视图的置信度分数进行丰富。然后将得到的云输入到两个平行的DGCNN解码器头部，它们共同输出珊瑚的体积和表面积，以及它们相应的置信度估计。

Result: 该方法实现了具有竞争力的准确性，并能很好地推广到未知的形态。

Conclusion: 该框架为直接从稀疏图像集中有效和可扩展的珊瑚几何估计铺平了道路，在珊瑚生长分析和珊瑚礁监测方面具有潜在的应用。

Abstract: Effective reef monitoring requires the quantification of coral growth via
accurate volumetric and surface area estimates, which is a challenging task due
to the complex morphology of corals. We propose a novel, lightweight, and
scalable learning framework that addresses this challenge by predicting the 3D
volume and surface area of coral-like objects from 2D multi-view RGB images.
Our approach utilizes a pre-trained module (VGGT) to extract dense point maps
from each view; these maps are merged into a unified point cloud and enriched
with per-view confidence scores. The resulting cloud is fed to two parallel
DGCNN decoder heads, which jointly output the volume and the surface area of
the coral, as well as their corresponding confidence estimate. To enhance
prediction stability and provide uncertainty estimates, we introduce a
composite loss function based on Gaussian negative log-likelihood in both real
and log domains. Our method achieves competitive accuracy and generalizes well
to unseen morphologies. This framework paves the way for efficient and scalable
coral geometry estimation directly from a sparse set of images, with potential
applications in coral growth analysis and reef monitoring.

</details>


### [86] [Traffic-MLLM: A Spatio-Temporal MLLM with Retrieval-Augmented Generation for Causal Inference in Traffic](https://arxiv.org/abs/2509.11165)
*Waikit Xiu,Qiang Lu,Xiying Li,Chen Hu,Shengbo Sun*

Main category: cs.CV

TL;DR: 提出了一种名为Traffic-MLLM的多模态大型语言模型，用于细粒度的交通分析。


<details>
  <summary>Details</summary>
Motivation: 现有方法在准确建模时空因果关系和整合领域知识方面面临挑战，限制了它们在复杂场景中的有效性。

Method: 该模型基于Qwen2.5-VL主干，利用高质量的交通专用多模态数据集，并使用Low-Rank Adaptation (LoRA)进行轻量级微调。引入了一种创新的知识提示模块，将Chain-of-Thought (CoT)推理与Retrieval-Augmented Generation (RAG)融合。

Result: 在TrafficQA和DriveQA基准测试中，Traffic-MLLM取得了最先进的性能，验证了其处理多模态交通数据的卓越能力。它还表现出卓越的零样本推理和跨场景泛化能力。

Conclusion: Traffic-MLLM在处理复杂交通场景中的时空因果关系和整合领域知识方面具有优势，并在多个基准测试中取得了SOTA性能。

Abstract: As intelligent transportation systems advance, traffic video understanding
plays an increasingly pivotal role in comprehensive scene perception and causal
analysis. Yet, existing approaches face notable challenges in accurately
modeling spatiotemporal causality and integrating domain-specific knowledge,
limiting their effectiveness in complex scenarios. To address these
limitations, we propose Traffic-MLLM, a multimodal large language model
tailored for fine-grained traffic analysis. Built on the Qwen2.5-VL backbone,
our model leverages high-quality traffic-specific multimodal datasets and uses
Low-Rank Adaptation (LoRA) for lightweight fine-tuning, significantly enhancing
its capacity to model continuous spatiotemporal features in video sequences.
Furthermore, we introduce an innovative knowledge prompting module fusing
Chain-of-Thought (CoT) reasoning with Retrieval-Augmented Generation (RAG),
enabling precise injection of detailed traffic regulations and domain knowledge
into the inference process. This design markedly boosts the model's logical
reasoning and knowledge adaptation capabilities. Experimental results on
TrafficQA and DriveQA benchmarks show Traffic-MLLM achieves state-of-the-art
performance, validating its superior ability to process multimodal traffic
data. It also exhibits remarkable zero-shot reasoning and cross-scenario
generalization capabilities.

</details>


### [87] [Multispectral-NeRF:a multispectral modeling approach based on neural radiance fields](https://arxiv.org/abs/2509.11169)
*Hong Zhang,Fei Guo,Zihan Xie,Dizhao Yao*

Main category: cs.CV

TL;DR: 本文提出了一种名为 Multispectral-NeRF 的增强神经架构，用于有效地整合多光谱信息，以解决传统方法中价格昂贵、精度低和几何特征差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的三维重建技术依赖于RGB光谱信息，而现有的多光谱数据集成方法存在价格昂贵、精度低和几何特征差的问题。NeRF虽然有效，但无法考虑多波段信息。

Method: 本文通过扩展隐藏层维度以适应6波段光谱输入，重新设计残差函数以优化重建图像和参考图像之间的光谱差异计算，并调整数据压缩模块以满足多光谱图像增加的位深度要求，从而提出Multispectral-NeRF。

Result: 实验结果表明，Multispectral-NeRF 成功处理了多波段光谱特征，同时准确地保留了原始场景的光谱特征。

Conclusion: 本文提出的Multispectral-NeRF能够有效处理多光谱信息，实现高精度和高质量的重建结果。

Abstract: 3D reconstruction technology generates three-dimensional representations of
real-world objects, scenes, or environments using sensor data such as 2D
images, with extensive applications in robotics, autonomous vehicles, and
virtual reality systems. Traditional 3D reconstruction techniques based on 2D
images typically relies on RGB spectral information. With advances in sensor
technology, additional spectral bands beyond RGB have been increasingly
incorporated into 3D reconstruction workflows. Existing methods that integrate
these expanded spectral data often suffer from expensive scheme prices, low
accuracy and poor geometric features. Three - dimensional reconstruction based
on NeRF can effectively address the various issues in current multispectral 3D
reconstruction methods, producing high - precision and high - quality
reconstruction results. However, currently, NeRF and some improved models such
as NeRFacto are trained on three - band data and cannot take into account the
multi - band information. To address this problem, we propose
Multispectral-NeRF, an enhanced neural architecture derived from NeRF that can
effectively integrates multispectral information. Our technical contributions
comprise threefold modifications: Expanding hidden layer dimensionality to
accommodate 6-band spectral inputs; Redesigning residual functions to optimize
spectral discrepancy calculations between reconstructed and reference images;
Adapting data compression modules to address the increased bit-depth
requirements of multispectral imagery. Experimental results confirm that
Multispectral-NeRF successfully processes multi-band spectral features while
accurately preserving the original scenes' spectral characteristics.

</details>


### [88] [SPHERE: Semantic-PHysical Engaged REpresentation for 3D Semantic Scene Completion](https://arxiv.org/abs/2509.11171)
*Zhiwen Yang,Yuxin Peng*

Main category: cs.CV

TL;DR: 提出了一个名为SPHERE的新框架，用于相机视角的3D语义场景补全（SSC），该框架结合了体素和高斯表示，以共同利用语义和物理信息。


<details>
  <summary>Details</summary>
Motivation: 现有的基于体素和平面SSC方法难以捕捉到真实的几何细节，而NeRF和3DGS等神经重建方法虽然具有优越的物理感知能力，但在处理大规模、复杂的自动驾驶场景时，计算成本高，收敛速度慢，导致语义精度较差。

Method: 该方法首先利用语义引导的高斯初始化（SGI）模块，利用双分支3D场景表示来定位焦点体素作为锚点，以指导高效的高斯初始化。然后，物理感知谐波增强（PHE）模块结合语义球谐函数来建模物理感知的上下文细节，并通过焦点分布对齐来提高语义-几何一致性。

Result: 在SemanticKITTI和SSCBench-KITTI-360基准上的大量实验和分析验证了SPHERE的有效性。

Conclusion: SPHERE框架能够生成具有真实细节的SSC结果。

Abstract: Camera-based 3D Semantic Scene Completion (SSC) is a critical task in
autonomous driving systems, assessing voxel-level geometry and semantics for
holistic scene perception. While existing voxel-based and plane-based SSC
methods have achieved considerable progress, they struggle to capture physical
regularities for realistic geometric details. On the other hand, neural
reconstruction methods like NeRF and 3DGS demonstrate superior physical
awareness, but suffer from high computational cost and slow convergence when
handling large-scale, complex autonomous driving scenes, leading to inferior
semantic accuracy. To address these issues, we propose the Semantic-PHysical
Engaged REpresentation (SPHERE) for camera-based SSC, which integrates voxel
and Gaussian representations for joint exploitation of semantic and physical
information. First, the Semantic-guided Gaussian Initialization (SGI) module
leverages dual-branch 3D scene representations to locate focal voxels as
anchors to guide efficient Gaussian initialization. Then, the Physical-aware
Harmonics Enhancement (PHE) module incorporates semantic spherical harmonics to
model physical-aware contextual details and promote semantic-geometry
consistency through focal distribution alignment, generating SSC results with
realistic details. Extensive experiments and analyses on the popular
SemanticKITTI and SSCBench-KITTI-360 benchmarks validate the effectiveness of
SPHERE. The code is available at
https://github.com/PKU-ICST-MIPL/SPHERE_ACMMM2025.

</details>


### [89] [StegOT: Trade-offs in Steganography via Optimal Transport](https://arxiv.org/abs/2509.11178)
*Chengde Lin,Xuezhu Gong,Shuxue Ding,Mingzhe Yang,Xijun Lu,Chengjun Mo*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为StegOT的基于自动编码器的隐写模型，该模型结合了最优传输理论，旨在解决现有模型中存在的模式崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有隐写模型容易出现模式崩溃，导致载体图像和秘密图像之间的信息失衡，影响后续提取。

Method: 设计了多通道最优传输（MCOT）模块来转换特征分布，将多峰转化为单峰，以实现信息的权衡。

Result: 实验表明，该模型不仅实现了载体图像和秘密图像之间的权衡，还提高了隐写图像和恢复图像的质量。

Conclusion: 该论文提出了一种新的隐写模型，有效解决了模式崩溃问题，并在图像质量和信息隐藏方面取得了较好的效果。

Abstract: Image hiding is often referred to as steganography, which aims to hide a
secret image in a cover image of the same resolution. Many steganography models
are based on genera-tive adversarial networks (GANs) and variational
autoencoders (VAEs). However, most existing models suffer from mode collapse.
Mode collapse will lead to an information imbalance between the cover and
secret images in the stego image and further affect the subsequent extraction.
To address these challenges, this paper proposes StegOT, an autoencoder-based
steganography model incorporating optimal transport theory. We designed the
multiple channel optimal transport (MCOT) module to transform the feature
distribution, which exhibits multiple peaks, into a single peak to achieve the
trade-off of information. Experiments demonstrate that we not only achieve a
trade-off between the cover and secret images but also enhance the quality of
both the stego and recovery images. The source code will be released on
https://github.com/Rss1124/StegOT.

</details>


### [90] [The Impact of Skin Tone Label Granularity on the Performance and Fairness of AI Based Dermatology Image Classification Models](https://arxiv.org/abs/2509.11184)
*Partha Shah,Durva Sankhe,Maariyah Rashid,Zakaa Khaled,Esther Puyol-Antón,Tiarna Lee,Maram Alqarni,Sweta Rai,Andrew P. King*

Main category: cs.CV

TL;DR: 研究AI皮肤病变分类模型在不同FST粒度下的性能和偏差。


<details>
  <summary>Details</summary>
Motivation: AI模型易受肤色偏差影响，FST量表对浅肤色分类更细，需研究FST粒度对AI模型的影响。

Method: 训练多个AI模型，使用不同FST粒度的数据进行良性/恶性病变分类。

Result: 使用三分组（FST 1/2, 3/4, 5/6）训练的模型比使用FST平衡数据训练的通用模型效果更好；降低FST粒度（从1/2和3/4到1/2/3/4）会降低性能。

Conclusion: FST分组粒度对病变分类模型训练很重要，应考虑使用更能代表人类肤色多样性的替代量表。

Abstract: Artificial intelligence (AI) models to automatically classify skin lesions
from dermatology images have shown promising performance but also
susceptibility to bias by skin tone. The most common way of representing skin
tone information is the Fitzpatrick Skin Tone (FST) scale. The FST scale has
been criticised for having greater granularity in its skin tone categories for
lighter-skinned subjects. This paper conducts an investigation of the impact
(on performance and bias) on AI classification models of granularity in the FST
scale. By training multiple AI models to classify benign vs. malignant lesions
using FST-specific data of differing granularity, we show that: (i) when
training models using FST-specific data based on three groups (FST 1/2, 3/4 and
5/6), performance is generally better for models trained on FST-specific data
compared to a general model trained on FST-balanced data; (ii) reducing the
granularity of FST scale information (from 1/2 and 3/4 to 1/2/3/4) can have a
detrimental effect on performance. Our results highlight the importance of the
granularity of FST groups when training lesion classification models. Given the
question marks over possible human biases in the choice of categories in the
FST scale, this paper provides evidence for a move away from the FST scale in
fair AI research and a transition to an alternative scale that better
represents the diversity of human skin tones.

</details>


### [91] [Scaling Up Forest Vision with Synthetic Data](https://arxiv.org/abs/2509.11201)
*Yihang She,Andrew Blake,David Coomes,Srinivasan Keshav*

Main category: cs.CV

TL;DR: 本文提出了一种新的合成数据生成流程，用于森林视觉任务，并生成了一个大规模、多样化、带注释的 3D 森林数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的公共 3D 森林数据集不足以构建稳健的树木分割系统。受合成数据在自动驾驶等领域的成功启发，研究是否类似方法可以帮助树木分割。

Method: 使用合成数据进行预训练，然后仅需要最少的真实森林样地注释进行微调。集成了游戏引擎的进步和基于物理的激光雷达模拟。

Result: 合成数据可以大大减少对标记真实数据的需求。在仅对小于 0.1 公顷的单个真实森林样地进行微调后，预训练模型实现的分割与在全尺寸真实数据上训练的模型具有竞争力。

Conclusion: 物理、多样性和规模是成功使用合成数据的关键因素，为未来更强大的 3D 森林视觉系统铺平了道路。

Abstract: Accurate tree segmentation is a key step in extracting individual tree
metrics from forest laser scans, and is essential to understanding ecosystem
functions in carbon cycling and beyond. Over the past decade, tree segmentation
algorithms have advanced rapidly due to developments in AI. However existing,
public, 3D forest datasets are not large enough to build robust tree
segmentation systems. Motivated by the success of synthetic data in other
domains such as self-driving, we investigate whether similar approaches can
help with tree segmentation. In place of expensive field data collection and
annotation, we use synthetic data during pretraining, and then require only
minimal, real forest plot annotation for fine-tuning.
  We have developed a new synthetic data generation pipeline to do this for
forest vision tasks, integrating advances in game-engines with physics-based
LiDAR simulation. As a result, we have produced a comprehensive, diverse,
annotated 3D forest dataset on an unprecedented scale. Extensive experiments
with a state-of-the-art tree segmentation algorithm and a popular real dataset
show that our synthetic data can substantially reduce the need for labelled
real data. After fine-tuning on just a single, real, forest plot of less than
0.1 hectare, the pretrained model achieves segmentations that are competitive
with a model trained on the full scale real data. We have also identified
critical factors for successful use of synthetic data: physics, diversity, and
scale, paving the way for more robust 3D forest vision systems in the future.
Our data generation pipeline and the resulting dataset are available at
https://github.com/yihshe/CAMP3D.git.

</details>


### [92] [Beyond Sliders: Mastering the Art of Diffusion-based Image Manipulation](https://arxiv.org/abs/2509.11213)
*Yufei Tang,Daiheng Gao,Pingyu Wu,Wenbo Zhou,Bang Zhang,Weiming Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为Beyond Sliders的图像处理框架，集成了GAN和扩散模型，以实现对各种图像类别进行复杂的图像处理。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理no-AIGC图像，特别是现实世界中捕获的图像时，往往会失败。为了弥补这一差距。

Method: 集成了GAN和扩散模型，通过细粒度的文本和视觉指导，以对抗的方式改进图像。

Result: 在图像质量和真实感方面有显著提高。大量的实验验证证实了Beyond Sliders在各种应用中的鲁棒性和多功能性。

Conclusion: Beyond Sliders框架在各种图像类别的图像处理方面表现出强大的能力。

Abstract: In the realm of image generation, the quest for realism and customization has
never been more pressing. While existing methods like concept sliders have made
strides, they often falter when it comes to no-AIGC images, particularly images
captured in real world settings. To bridge this gap, we introduce Beyond
Sliders, an innovative framework that integrates GANs and diffusion models to
facilitate sophisticated image manipulation across diverse image categories.
Improved upon concept sliders, our method refines the image through fine
grained guidance both textual and visual in an adversarial manner, leading to a
marked enhancement in image quality and realism. Extensive experimental
validation confirms the robustness and versatility of Beyond Sliders across a
spectrum of applications.

</details>


### [93] [Geometrically Constrained and Token-Based Probabilistic Spatial Transformers](https://arxiv.org/abs/2509.11218)
*Johann Schmidt,Sebastian Stober*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的基于空间变换网络（STN）的细粒度图像分类方法，以解决几何变化带来的问题。


<details>
  <summary>Details</summary>
Motivation: 细粒度图像分类容易受到物体方向、尺度和透视畸变等几何变化的影响。

Method: 该方法对STN进行了概率化的、组件式的扩展，将仿射变换分解为旋转、缩放和剪切，并使用高斯变分后验对每个分量进行建模，在推理过程中进行基于采样的规范化。同时，设计了一种新的分量式对齐损失，利用增强参数来指导空间对齐。

Result: 在具有挑战性的蛾分类基准测试中，该方法始终优于其他STN。

Conclusion: 该方法可以有效提高细粒度图像分类的鲁棒性。

Abstract: Fine-grained visual classification (FGVC) remains highly sensitive to
geometric variability, where objects appear under arbitrary orientations,
scales, and perspective distortions. While equivariant architectures address
this issue, they typically require substantial computational resources and
restrict the hypothesis space. We revisit Spatial Transformer Networks (STNs)
as a canonicalization tool for transformer-based vision pipelines, emphasizing
their flexibility, backbone-agnostic nature, and lack of architectural
constraints. We propose a probabilistic, component-wise extension that improves
robustness. Specifically, we decompose affine transformations into rotation,
scaling, and shearing, and regress each component under geometric constraints
using a shared localization encoder. To capture uncertainty, we model each
component with a Gaussian variational posterior and perform sampling-based
canonicalization during inference.A novel component-wise alignment loss
leverages augmentation parameters to guide spatial alignment. Experiments on
challenging moth classification benchmarks demonstrate that our method
consistently improves robustness compared to other STNs.

</details>


### [94] [CCoMAML: Efficient Cattle Identification Using Cooperative Model-Agnostic Meta-Learning](https://arxiv.org/abs/2509.11219)
*Rabin Dulal,Lihong Zheng,Ashad Kabir*

Main category: cs.CV

TL;DR: 本文提出了一种基于少量样本学习的奶牛实时识别框架，该框架使用合作模型无关元学习（CCoMAML）和多头注意力特征融合（MHAFF）作为特征提取模型。


<details>
  <summary>Details</summary>
Motivation: 现有的基于射频识别（RFID）耳标的牲畜识别系统容易因丢失、损坏、篡改和易受外部攻击而失效。生物识别技术，如使用类似于人类指纹的牛鼻子图案，已成为一种有前途的解决方案。然而，深度学习模型面临着数据可用性有限、数据收集中断以及需要频繁模型重新训练的动态牛群组成等重大挑战。

Method: 本文提出了一种新的少量样本学习框架，用于使用合作模型无关元学习（CCoMAML）和多头注意力特征融合（MHAFF）作为特征提取模型的实时奶牛识别。

Result: 所提出的 CCoMAML 与 MHAFF 相比，具有卓越的奶牛识别性能，F1 分数分别为 98.46% 和 97.91%。

Conclusion: 本文提出的方法在奶牛识别方面优于当前最先进的少量样本学习技术。

Abstract: Cattle identification is critical for efficient livestock farming management,
currently reliant on radio-frequency identification (RFID) ear tags. However,
RFID-based systems are prone to failure due to loss, damage, tampering, and
vulnerability to external attacks. As a robust alternative, biometric
identification using cattle muzzle patterns similar to human fingerprints has
emerged as a promising solution. Deep learning techniques have demonstrated
success in leveraging these unique patterns for accurate identification. But
deep learning models face significant challenges, including limited data
availability, disruptions during data collection, and dynamic herd compositions
that require frequent model retraining. To address these limitations, this
paper proposes a novel few-shot learning framework for real-time cattle
identification using Cooperative Model-Agnostic Meta-Learning (CCoMAML) with
Multi-Head Attention Feature Fusion (MHAFF) as a feature extractor model. This
model offers great model adaptability to new data through efficient learning
from few data samples without retraining. The proposed approach has been
rigorously evaluated against current state-of-the-art few-shot learning
techniques applied in cattle identification. Comprehensive experimental results
demonstrate that our proposed CCoMAML with MHAFF has superior cattle
identification performance with 98.46% and 97.91% F1 scores.

</details>


### [95] [ANROT-HELANet: Adverserially and Naturally Robust Attention-Based Aggregation Network via The Hellinger Distance for Few-Shot Classification](https://arxiv.org/abs/2509.11220)
*Gao Yu Lee,Tanmoy Dam,Md Meftahul Ferdaus,Daniel Puiu Poenar,Vu N. Duong*

Main category: cs.CV

TL;DR: 提出了一种新的Few-Shot Learning (FSL) 方法，ANROT-HELANet，它在FSL的鲁棒性和性能方面都达到了state-of-the-art。


<details>
  <summary>Details</summary>
Motivation: 现有的基于贝叶斯的估计方法在使用Kullback-Leibler (KL) 散度时，容易受到对抗性攻击和自然噪声的影响。

Method: 该方法实现了一种基于Hellinger距离的对抗性和自然鲁棒的特征类聚合方案，并引入了一种新的Hellinger相似性对比损失函数，该函数推广了余弦相似性对比损失，用于变分few-shot推理场景。

Result: 在miniImageNet的1-shot和5-shot场景中，分别实现了1.20%和1.40%的收益。在图像重建质量方面，FID得分为2.75，优于传统的VAE (3.43)和WAE (3.38)方法。

Conclusion: ANROT-HELANet结合了基于Hellinger距离的特征聚合、注意力机制和新的损失函数，在保持对抗性和自然扰动的鲁棒性的同时，建立了新的state-of-the-art性能。

Abstract: Few-Shot Learning (FSL), which involves learning to generalize using only a
few data samples, has demonstrated promising and superior performances to
ordinary CNN methods. While Bayesian based estimation approaches using
Kullback-Leibler (KL) divergence have shown improvements, they remain
vulnerable to adversarial attacks and natural noises. We introduce
ANROT-HELANet, an Adversarially and Naturally RObusT Hellinger Aggregation
Network that significantly advances the state-of-the-art in FSL robustness and
performance. Our approach implements an adversarially and naturally robust
Hellinger distance-based feature class aggregation scheme, demonstrating
resilience to adversarial perturbations up to $\epsilon=0.30$ and Gaussian
noise up to $\sigma=0.30$. The network achieves substantial improvements across
benchmark datasets, including gains of 1.20\% and 1.40\% for 1-shot and 5-shot
scenarios on miniImageNet respectively. We introduce a novel Hellinger
Similarity contrastive loss function that generalizes cosine similarity
contrastive loss for variational few-shot inference scenarios. Our approach
also achieves superior image reconstruction quality with a FID score of 2.75,
outperforming traditional VAE (3.43) and WAE (3.38) approaches. Extensive
experiments conducted on four few-shot benchmarked datasets verify that
ANROT-HELANet's combination of Hellinger distance-based feature aggregation,
attention mechanisms, and our novel loss function establishes new
state-of-the-art performance while maintaining robustness against both
adversarial and natural perturbations. Our code repository will be available at
https://github.com/GreedYLearner1146/ANROT-HELANet/tree/main.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [96] [Situation Model of the Transport, Transport Emissions and Meteorological Conditions](https://arxiv.org/abs/2509.10541)
*V. Benes,M. Svitek,A. Michalikova,M. Melicherik*

Main category: cs.AI

TL;DR: 本文研究城市空气污染问题，重点关注交通排放与气象条件的关系，以及如何利用这些知识来改进城市规划和环境保护。


<details>
  <summary>Details</summary>
Motivation: 研究城市空气污染及其减排的可能性，这是当今社会面临的重要因素之一。

Method: 利用模糊推理系统（FIS）建立模型，预测不同条件下排放的变化。

Result: 该模型基于在捷克共和国布拉格测量的交通、气象和排放数据。

Conclusion: 旨在为城市规划者和政策制定者提供关于如何更有效地规划和管理城市交通，同时兼顾环境保护的见解。

Abstract: Air pollution in cities and the possibilities of reducing this pollution
represents one of the most important factors that today's society has to deal
with. This paper focuses on a systemic approach to traffic emissions with their
relation to meteorological conditions, analyzing the effect of weather on the
quantity and dispersion of traffic emissions in a city. Using fuzzy inference
systems (FIS) the model for prediction of changes in emissions depending on
various conditions is developed. The proposed model is based on traffic,
meteorology and emission data measured in Prague, Czech Republic. The main
objective of the work is to provide insight into how urban planners and
policymakers can plan and manage urban transport more effectively with
environmental protection in mind.

</details>


### [97] [ZapGPT: Free-form Language Prompting for Simulated Cellular Control](https://arxiv.org/abs/2509.10660)
*Nam H. Le,Patrick Erickson,Yanbo Zhang,Michael Levin,Josh Bongard*

Main category: cs.AI

TL;DR: 本文提出了一种新的方法，通过自由形式的自然语言提示来引导人工或生物集体行为，无需特定任务的调整或精心设计的评估指标。


<details>
  <summary>Details</summary>
Motivation: 弥合人类语言在传达意图方面的能力与大多数人工或生物系统缺乏对其进行有意义的解释或响应机制之间的差距。

Method: 使用两个AI模型：一个将命令式提示转换为对模拟细胞的干预，另一个对提示描述细胞动力学的程度进行评分；前者AI模型通过进化来提高后者的评分。

Result: 证明了简单agent的集体行为可以被自由形式的语言提示所引导，并且该系统可以泛化到未见过的提示。

Conclusion: 该系统将自然语言作为控制层，为口头或书面提示可以指导计算、机器人或生物系统实现所需行为的未来提供了可能。

Abstract: Human language is one of the most expressive tools for conveying intent, yet
most artificial or biological systems lack mechanisms to interpret or respond
meaningfully to it. Bridging this gap could enable more natural forms of
control over complex, decentralized systems. In AI and artificial life, recent
work explores how language can specify high-level goals, but most systems still
depend on engineered rewards, task-specific supervision, or rigid command sets,
limiting generalization to novel instructions. Similar constraints apply in
synthetic biology and bioengineering, where the locus of control is often
genomic rather than environmental perturbation.
  A key open question is whether artificial or biological collectives can be
guided by free-form natural language alone, without task-specific tuning or
carefully designed evaluation metrics. We provide one possible answer here by
showing, for the first time, that simple agents' collective behavior can be
guided by free-form language prompts: one AI model transforms an imperative
prompt into an intervention that is applied to simulated cells; a second AI
model scores how well the prompt describes the resulting cellular dynamics; and
the former AI model is evolved to improve the scores generated by the latter.
  Unlike previous work, our method does not require engineered fitness
functions or domain-specific prompt design. We show that the evolved system
generalizes to unseen prompts without retraining. By treating natural language
as a control layer, the system suggests a future in which spoken or written
prompts could direct computational, robotic, or biological systems to desired
behaviors. This work provides a concrete step toward this vision of AI-biology
partnerships, in which language replaces mathematical objective functions,
fixed rules, and domain-specific programming.

</details>


### [98] [Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration](https://arxiv.org/abs/2509.10704)
*Xingchen Wan,Han Zhou,Ruoxi Sun,Hootan Nakhost,Ke Jiang,Rajarishi Sinha,Sercan Ö. Arık*

Main category: cs.AI

TL;DR: Maestro是一个自进化的图像生成系统，通过迭代改进prompt来提升T2I模型生成的图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有的T2I模型依赖大量人工干预，需要手动迭代prompt工程，可用性差。

Method: 引入Maestro系统，包含自批判和自进化两个关键创新。自批判使用多模态LLM作为评论员，识别图像弱点并提供可解释的编辑信号；自进化利用多模态LLM进行图像对比，进化符合用户意图的prompt候选。

Result: 在复杂T2I任务上的大量实验表明，Maestro显著提高了图像质量，效果随着MLLM组件的进步而提升。

Conclusion: Maestro为自改进的T2I生成提供了一条鲁棒、可解释和有效的途径。

Abstract: Text-to-image (T2I) models, while offering immense creative potential, are
highly reliant on human intervention, posing significant usability challenges
that often necessitate manual, iterative prompt engineering over often
underspecified prompts. This paper introduces Maestro, a novel self-evolving
image generation system that enables T2I models to autonomously self-improve
generated images through iterative evolution of prompts, using only an initial
prompt. Maestro incorporates two key innovations: 1) self-critique, where
specialized multimodal LLM (MLLM) agents act as 'critics' to identify
weaknesses in generated images, correct for under-specification, and provide
interpretable edit signals, which are then integrated by a 'verifier' agent
while preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge
for head-to-head comparisons between iteratively generated images, eschewing
problematic images, and evolving creative prompt candidates that align with
user intents. Extensive experiments on complex T2I tasks using black-box models
demonstrate that Maestro significantly improves image quality over initial
prompts and state-of-the-art automated methods, with effectiveness scaling with
more advanced MLLM components. This work presents a robust, interpretable, and
effective pathway towards self-improving T2I generation.

</details>


### [99] [Understanding AI Evaluation Patterns: How Different GPT Models Assess Vision-Language Descriptions](https://arxiv.org/abs/2509.10707)
*Sajjad Abdoli,Rudi Cilibrasi,Rima Al-Shikh*

Main category: cs.AI

TL;DR: 分析了AI系统评估其他AI输出时的评估行为，以防止偏差。


<details>
  <summary>Details</summary>
Motivation: 了解AI评估行为对于防止偏差至关重要，尤其是在AI系统越来越多地评估其他AI输出的情况下。

Method: 使用NVIDIA的Describe Anything Model生成视觉语言描述，并由三个GPT变体（GPT-4o、GPT-4o-mini、GPT-5）进行评估。使用Gemini 2.5 Pro作为独立的提问生成器进行对照实验。

Result: GPT-4o-mini表现出系统一致性，GPT-4o擅长错误检测，而GPT-5表现出极端的保守性。所有GPT模型都表现出2:1的偏见，倾向于负面评估而不是正面确认。GPT模型聚集在一起具有很高的相似性，而Gemini表现出明显不同的评估策略。

Conclusion: 评估能力并不随通用能力而扩展，稳健的AI评估需要不同的架构视角。

Abstract: As AI systems increasingly evaluate other AI outputs, understanding their
assessment behavior becomes crucial for preventing cascading biases. This study
analyzes vision-language descriptions generated by NVIDIA's Describe Anything
Model and evaluated by three GPT variants (GPT-4o, GPT-4o-mini, GPT-5) to
uncover distinct "evaluation personalities" the underlying assessment
strategies and biases each model demonstrates. GPT-4o-mini exhibits systematic
consistency with minimal variance, GPT-4o excels at error detection, while
GPT-5 shows extreme conservatism with high variability. Controlled experiments
using Gemini 2.5 Pro as an independent question generator validate that these
personalities are inherent model properties rather than artifacts. Cross-family
analysis through semantic similarity of generated questions reveals significant
divergence: GPT models cluster together with high similarity while Gemini
exhibits markedly different evaluation strategies. All GPT models demonstrate a
consistent 2:1 bias favoring negative assessment over positive confirmation,
though this pattern appears family-specific rather than universal across AI
architectures. These findings suggest that evaluation competence does not scale
with general capability and that robust AI assessment requires diverse
architectural perspectives.

</details>


### [100] [AI Answer Engine Citation Behavior An Empirical Analysis of the GEO16 Framework](https://arxiv.org/abs/2509.10762)
*Arlen Kumar,Leanid Palkhouski*

Main category: cs.AI

TL;DR: 这篇论文介绍了一个名为GEO-16的审计框架，用于评估网页质量，并分析了搜索引擎在生成答案时引用网页的质量差异。


<details>
  <summary>Details</summary>
Motivation: 研究动机是了解AI答案引擎如何通过引用网页来提供领域知识。

Method: 使用GEO-16框架对三个搜索引擎（Brave Summary, Google AI Overviews, and Perplexity）引用的1702个网页进行了审计，并分析了1100个独立URL。

Result: 结果表明，不同引擎引用的网页质量存在差异，Metadata和Freshness、Semantic HTML和Structured Data等因素与引用关系最密切。网页总体质量是引用情况的有力预测指标。

Conclusion: 该研究提出了一个实用的发布者指南，并讨论了局限性、有效性威胁和可重复性考虑因素。

Abstract: AI answer engines increasingly mediate access to domain knowledge by
generating responses and citing web sources. We introduce GEO-16, a 16 pillar
auditing framework that converts on page quality signals into banded pillar
scores and a normalized GEO score G that ranges from 0 to 1. Using 70 product
intent prompts, we collected 1,702 citations across three engines (Brave
Summary, Google AI Overviews, and Perplexity) and audited 1,100 unique URLs. In
our corpus, the engines differed in the GEO quality of the pages they cited,
and pillars related to Metadata and Freshness, Semantic HTML, and Structured
Data showed the strongest associations with citation. Logistic models with
domain clustered standard errors indicate that overall page quality is a strong
predictor of citation, and simple operating points (for example, G at least
0.70 combined with at least 12 pillar hits) align with substantially higher
citation rates in our data. We report per engine contrasts, vertical effects,
threshold analysis, and diagnostics, then translate findings into a practical
playbook for publishers. The study is observational and focuses on English
language B2B SaaS pages; we discuss limitations, threats to validity, and
reproducibility considerations.

</details>


### [101] [AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise](https://arxiv.org/abs/2509.10769)
*Tara Bogavelli,Roshnee Sharma,Hari Subramani*

Main category: cs.AI

TL;DR: 该研究旨在通过综合的企业特定基准，评估 18 种不同的代理配置在最先进的大型语言模型中的表现，从而填补对复杂多代理系统中不同设计维度如何交互的有限的实证理解。


<details>
  <summary>Details</summary>
Motivation: 研究动机是目前对代理架构的各个组成部分的研究是孤立的，缺乏对复杂多代理系统中不同设计维度如何相互作用的实证理解。

Method: 该研究通过一个综合的企业特定基准，评估了 18 种不同的代理配置在最先进的大型语言模型中的表现。研究考察了四个关键的代理系统维度：编排策略、代理提示实现（ReAct 与函数调用）、记忆架构和思维工具集成。

Result: 该基准测试揭示了特定于模型的架构偏好，并挑战了代理 AI 系统中流行的“一刀切”模式。它还揭示了企业任务中代理整体性能的显着弱点，得分最高的模型在更复杂的任务中仅达到 35.3% 的成功率，在更简单的任务中达到 70.8% 的成功率。

Conclusion: 研究结果旨在通过支持更多基于经验的关于架构组件和模型选择的决策，为未来代理系统的设计提供信息。

Abstract: While individual components of agentic architectures have been studied in
isolation, there remains limited empirical understanding of how different
design dimensions interact within complex multi-agent systems. This study aims
to address these gaps by providing a comprehensive enterprise-specific
benchmark evaluating 18 distinct agentic configurations across state-of-the-art
large language models. We examine four critical agentic system dimensions:
orchestration strategy, agent prompt implementation (ReAct versus function
calling), memory architecture, and thinking tool integration. Our benchmark
reveals significant model-specific architectural preferences that challenge the
prevalent one-size-fits-all paradigm in agentic AI systems. It also reveals
significant weaknesses in overall agentic performance on enterprise tasks with
the highest scoring models achieving a maximum of only 35.3\% success on the
more complex task and 70.8\% on the simpler task. We hope these findings inform
the design of future agentic systems by enabling more empirically backed
decisions regarding architectural components and model selection.

</details>


### [102] [LLM Enhancement with Domain Expert Mental Model to Reduce LLM Hallucination with Causal Prompt Engineering](https://arxiv.org/abs/2509.10818)
*Boris Kovalerchuk,Brent D. Fegley*

Main category: cs.AI

TL;DR: 大型语言模型(llm)在决策支持方面表现出潜力，但由于缺乏训练数据和幻觉，它们的能力有限。检索增强生成(rag)通过整合外部信息检索来增强llm，但它可能无法访问所有必要的资源或关键的缺失信息。本文探讨了如何使用优化的人机对话和单调布尔函数及k值函数，通过优化的人机对话和单调布尔函数及k值函数来发现决策的计算上易于处理的个人专家心理模型(emm)。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(llm)在决策支持方面显示出巨大的前景。然而，llm还不能解决训练数据中的缺失问题，从而导致幻觉。检索增强生成(rag)通过整合外部信息检索来增强llm，减少幻觉并提高准确性。然而，rag及相关方法只是部分解决方案，因为它们可能无法访问所有必要的来源或关键的缺失信息。即使是日常问题也经常挑战llm的能力。

Method: 该技术基于优化的人机对话和单调布尔和k值函数，以发现决策的计算上易于处理的个人专家心理模型(emm)。我们的llm提示工程emm算法有四个步骤：(1)因素识别，(2)因素的层次结构，(3)生成广义专家心理模型规范，以及(4)从该规范生成详细的广义专家心理模型。

Result: 本文探讨了llm如何使决策更有效率，使用一个正在运行的例子来评估是否回应提案征集。

Conclusion: 针对缺少关键信息的任务，llm是不够的，因为许多现有系统在可用文档中表示得很差。

Abstract: Difficult decision-making problems abound in various disciplines and domains.
The proliferation of generative techniques, especially large language models
(LLMs), has excited interest in using them for decision support. However, LLMs
cannot yet resolve missingness in their training data, leading to
hallucinations. Retrieval-Augmented Generation (RAG) enhances LLMs by
incorporating external information retrieval, reducing hallucinations and
improving accuracy. Yet, RAG and related methods are only partial solutions, as
they may lack access to all necessary sources or key missing information. Even
everyday issues often challenge LLMs' abilities. Submitting longer prompts with
context and examples is one approach to address knowledge gaps, but designing
effective prompts is non-trivial and may not capture complex mental models of
domain experts. For tasks with missing critical information, LLMs are
insufficient, as are many existing systems poorly represented in available
documents. This paper explores how LLMs can make decision-making more
efficient, using a running example of evaluating whether to respond to a call
for proposals. We propose a technology based on optimized human-machine
dialogue and monotone Boolean and k-valued functions to discover a
computationally tractable personal expert mental model (EMM) of
decision-making. Our EMM algorithm for LLM prompt engineering has four steps:
(1) factor identification, (2) hierarchical structuring of factors, (3)
generating a generalized expert mental model specification, and (4) generating
a detailed generalized expert mental model from that specification.

</details>


### [103] [From Grounding to Skolemization: A Logic-Constrained Vector Symbolic Architecture for Complex Query Answering](https://arxiv.org/abs/2509.10837)
*Yuyin Lu,Hegang Chen,Yanghui Rao*

Main category: cs.AI

TL;DR: 本文提出了一种新的神经符号框架，用于解决不完整知识图上的复杂查询应答问题，该框架在逻辑健全性和计算效率之间取得了平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的复杂查询应答方法在逻辑健全性和计算效率之间存在权衡，要么计算成本高，要么逻辑一致性差。

Method: 本文提出了逻辑约束向量符号架构（LVSA），它结合了可微 Skolemization 模块、神经否定器和逻辑约束驱动的优化协议。

Result: LVSA 在理论上保证了对所有 EFO1 查询的通用性。在经验上，它优于最先进的 Skolemization 方法，并且与基于 Grounding 的基线相比，降低了几个数量级的推理成本。

Conclusion: LVSA 框架能够有效解决不完整知识图上的复杂查询应答问题，并在逻辑健全性和计算效率方面取得了显著的提升。

Abstract: Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs),
typically formalized as reasoning with Existential First-Order predicate logic
with one free variable (EFO$_1$), faces a fundamental trade-off between logical
soundness and computational efficiency. This work establishes the
Grounding-Skolemization dichotomy for systematically analyzing CQA methods
through the lens of formal logic. While Grounding-based methods inherently
suffer from combinatorial explosion, most Skolemization-based methods neglect
to explicitly model Skolem functions and compromise logical consistency. To
address these limitations, we propose the Logic-constrained Vector Symbolic
Architecture (LVSA), a neuro-symbolic framework that unifies a differentiable
Skolemization module and a neural negator, as well as a logical
constraint-driven optimization protocol to harmonize geometric and logical
requirements. Theoretically, LVSA guarantees universality for all EFO$_1$
queries. Empirically, it outperforms state-of-the-art Skolemization-based
methods and reduces inference costs by orders of magnitude compared to
Grounding-based baselines.

</details>


### [104] [Is the `Agent' Paradigm a Limiting Framework for Next-Generation Intelligent Systems?](https://arxiv.org/abs/2509.10875)
*Jesse Gardner,Vladimir A. Baulin*

Main category: cs.AI

TL;DR: 本文重新评估了以智能体为中心的范式在人工智能研究中的必要性和最优性，认为其存在概念模糊和以人类为中心的偏见，可能限制了人工智能的发展。


<details>
  <summary>Details</summary>
Motivation: 批判性地重新评估人工智能研究中以智能体为中心的范式，认为其可能存在局限性。

Method: 通过系统回顾相关文献，对各种人工智能框架中的智能体范式进行解构，重点分析了定义和衡量自主性和目标导向性等属性方面的挑战。

Result: 论证了许多人工智能系统的“智能体”框架虽然在启发式方面有用，但具有误导性，可能会掩盖底层计算机制，尤其是在大型语言模型（LLM）中。

Conclusion: 得出结论：研究受复杂系统、生物学和非常规计算启发的非智能体和系统框架，对于朝着稳健、可扩展和潜在的非人类形态的通用智能发展至关重要。

Abstract: The concept of the 'agent' has profoundly shaped Artificial Intelligence (AI)
research, guiding development from foundational theories to contemporary
applications like Large Language Model (LLM)-based systems. This paper
critically re-evaluates the necessity and optimality of this agent-centric
paradigm. We argue that its persistent conceptual ambiguities and inherent
anthropocentric biases may represent a limiting framework. We distinguish
between agentic systems (AI inspired by agency, often semi-autonomous, e.g.,
LLM-based agents), agential systems (fully autonomous, self-producing systems,
currently only biological), and non-agentic systems (tools without the
impression of agency). Our analysis, based on a systematic review of relevant
literature, deconstructs the agent paradigm across various AI frameworks,
highlighting challenges in defining and measuring properties like autonomy and
goal-directedness. We argue that the 'agentic' framing of many AI systems,
while heuristically useful, can be misleading and may obscure the underlying
computational mechanisms, particularly in Large Language Models (LLMs). As an
alternative, we propose a shift in focus towards frameworks grounded in
system-level dynamics, world modeling, and material intelligence. We conclude
that investigating non-agentic and systemic frameworks, inspired by complex
systems, biology, and unconventional computing, is essential for advancing
towards robust, scalable, and potentially non-anthropomorphic forms of general
intelligence. This requires not only new architectures but also a fundamental
reconsideration of our understanding of intelligence itself, moving beyond the
agent metaphor.

</details>


### [105] [Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding](https://arxiv.org/abs/2509.10931)
*Seongho Joo,Hyukhun Koh,Kyomin Jung*

Main category: cs.AI

TL;DR: 这篇论文提出了一种名为HaPLa的新的jailbreak攻击技术，该技术可以有效地攻击大型语言模型（LLMs）。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在有害用途中的潜在滥用，并探讨利用LLMs架构和学习范式中内在弱点的通用jailbreak攻击。

Method: 该方法包含两种主要策略：1) abductive framing，指示LLMs推断实现有害活动的合理中间步骤；2) symbolic encoding，一种轻量级且灵活的方法，旨在混淆有害内容。

Result: 实验结果表明，HaPLa在GPT系列模型上实现了超过95%的攻击成功率，在所有目标模型上实现了70%的攻击成功率。

Conclusion: 研究表明，在不显著降低LLMs响应良性查询的帮助性的前提下，安全地调整LLMs仍然是一个根本性的挑战。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, but their potential misuse for harmful purposes remains a
significant concern. To strengthen defenses against such vulnerabilities, it is
essential to investigate universal jailbreak attacks that exploit intrinsic
weaknesses in the architecture and learning paradigms of LLMs. In response, we
propose \textbf{H}armful \textbf{P}rompt \textbf{La}undering (HaPLa), a novel
and broadly applicable jailbreaking technique that requires only black-box
access to target models. HaPLa incorporates two primary strategies: 1)
\textit{abductive framing}, which instructs LLMs to infer plausible
intermediate steps toward harmful activities, rather than directly responding
to explicit harmful queries; and 2) \textit{symbolic encoding}, a lightweight
and flexible approach designed to obfuscate harmful content, given that current
LLMs remain sensitive primarily to explicit harmful keywords. Experimental
results show that HaPLa achieves over 95% attack success rate on GPT-series
models and 70% across all targets. Further analysis with diverse symbolic
encoding rules also reveals a fundamental challenge: it remains difficult to
safely tune LLMs without significantly diminishing their helpfulness in
responding to benign queries.

</details>


### [106] [Public Data Assisted Differentially Private In-Context Learning](https://arxiv.org/abs/2509.10932)
*Seongho Joo,Hyukhun Koh,Kyomin Jung*

Main category: cs.AI

TL;DR: 这篇论文提出了一种在保护隐私的同时提高大型语言模型上下文学习效用的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文学习方法存在隐私泄露风险，而差分隐私会降低模型效用。

Method: 该论文将任务相关的公共数据纳入上下文学习框架，并保持差分隐私保证。

Result: 实验表明，该方法显著提高了私有上下文学习的效用，并对成员推理攻击具有鲁棒性。

Conclusion: 该方法在隐私保护和模型效用之间取得了有效的平衡。

Abstract: In-context learning (ICL) in Large Language Models (LLMs) has shown
remarkable performance across various tasks without requiring fine-tuning.
However, recent studies have highlighted the risk of private data leakage
through the prompt in ICL, especially when LLMs are exposed to malicious
attacks. While differential privacy (DP) provides strong privacy guarantees, it
often significantly reduces the utility of in-context learning (ICL). To
address this challenge, we incorporate task-related public data into the ICL
framework while maintaining the DP guarantee. Based on this approach, we
propose a private in-context learning algorithm that effectively balances
privacy protection and model utility. Through experiments, we demonstrate that
our approach significantly improves the utility of private ICL with the
assistance of public data. Additionally, we show that our method is robust
against membership inference attacks, demonstrating empirical privacy
protection.

</details>


### [107] [Enhancing Computational Cognitive Architectures with LLMs: A Case Study](https://arxiv.org/abs/2509.10972)
*Ron Sun*

Main category: cs.AI

TL;DR: 将LLM融入认知架构，以应对真实世界的复杂性和心理现实主义。


<details>
  <summary>Details</summary>
Motivation: 目前的认知架构计算能力有限，而LLM在计算上更强大。将LLM融入认知架构成为一项重要任务。

Method: 将Clarion认知架构和LLM进行协同组合。利用Clarion的内隐-外显二分法，实现Clarion和LLM的无缝集成。

Result: LLM的计算能力与Clarion的心理学特性相结合。

Conclusion: 讨论了Clarion认知架构和LLM的协同组合，作为一个案例研究。

Abstract: Computational cognitive architectures are broadly scoped models of the human
mind that combine different psychological functionalities (as well as often
different computational methods for these different functionalities) into one
unified framework. They structure them in a psychologically plausible and
validated way. However, such models thus far have only limited computational
capabilities, mostly limited by the computational tools and techniques that
were adopted. More recently, LLMs have proved to be more capable
computationally than any other tools. Thus, in order to deal with both
real-world complexity and psychological realism at the same time, incorporating
LLMs into cognitive architectures naturally becomes an important task. In the
present article, a synergistic combination of the Clarion cognitive
architecture and LLMs is discussed as a case study. The implicit-explicit
dichotomy that is fundamental to Clarion is leveraged for a seamless
integration of Clarion and LLMs. As a result, computational power of LLMs is
combined with psychological nicety of Clarion.

</details>


### [108] [Rethinking Human Preference Evaluation of LLM Rationales](https://arxiv.org/abs/2509.11026)
*Ziang Li,Manasi Ganti,Zixian Ma,Helena Vasconcelos,Qijia He,Ranjay Krishna*

Main category: cs.AI

TL;DR: 这篇论文提出了一种更细粒度的评估LLM生成的原因的方法，通过定义关键属性并使用自动指标、LLM判断和人工标注进行评估，以克服二元比较的局限性。


<details>
  <summary>Details</summary>
Motivation: 评估LLM生成的理由仍然具有挑战性，因为现有的二元偏好判断方法过于粗糙，无法提供深入的见解。

Method: 1. 从现有文献中识别出一组关键的理由属性。2. 使用自动指标、LLM判断和人工标注对这些属性进行评估。3. 使用SHAP分析两个标准的人类偏好数据集，以确定哪些属性最能解释人类的偏好结果。4. 使用特定于属性的ELO评分重新评估模型生成的理由。

Result: 细粒度的属性评估可以更好地表征理由的质量，并指导未来的研究。

Conclusion: 该研究表明，基于属性的评估能够更细致地评价LLM生成的原因，并为未来的研究提供更可解释和可靠的评估实践。

Abstract: Large language models (LLMs) often generate natural language rationales --
free-form explanations that help improve performance on complex reasoning tasks
and enhance interpretability for human users. However, evaluating these
rationales remains challenging. While recent work has relied on binary
preference judgments from humans or LLM judges, such evaluations are often
opaque and coarse-grained, offering limited insight into what makes one
rationale better than another. In this work, we rethink preference evaluation
for LLM-generated rationales by asking: (1) What attributes define good
rationales? (2) Can human preferences be explained by these attributes? (3) Can
attribute-based evaluation overcome the limitations of binary comparisons? We
identify a set of key rationale attributes from prior literature and assess
them using automatic metrics, LLM judgments, and human annotations. We then
analyze two standard human preference datasets MT Bench and Chatbot Arena using
SHAP to identify which attributes best explain human preference outcomes.
Finally, we re-evaluate model-generated rationales using attribute-specific ELO
scores, revealing more nuanced model comparisons and insights. Our findings
suggest that fine-grained attribute evaluations can better characterize
rationale quality and guide future research toward more interpretable and
reliable evaluation practices.

</details>


### [109] [Free-MAD: Consensus-Free Multi-Agent Debate](https://arxiv.org/abs/2509.11035)
*Yu Cui,Hang Fu,Haibin Zhang,Licheng Wang,Cong Zuo*

Main category: cs.AI

TL;DR: 提出了一种新的多智能体辩论框架Free-MAD，它只需要一轮辩论，并引入了基于分数的决策机制和反从众机制，以提高推理性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体辩论方法依赖于多轮交互以达成共识，存在token开销大、易受错误影响、多数投票引入随机性和不公平性等问题。

Method: 提出了Free-MAD框架，该框架通过基于分数的决策机制评估整个辩论轨迹，并引入反从众机制来减轻多数人的过度影响。

Result: 在八个基准数据集上的实验表明，Free-MAD显著提高了推理性能，同时只需要一轮辩论，降低了token成本。与现有的MAD方法相比，Free-MAD在实际攻击场景中表现出更好的鲁棒性。

Conclusion: Free-MAD框架能够有效解决现有MAD方法中存在的局限性，并在推理性能和鲁棒性方面取得了显著提升。

Abstract: Multi-agent debate (MAD) is an emerging approach to improving the reasoning
capabilities of large language models (LLMs). Existing MAD methods rely on
multiple rounds of interaction among agents to reach consensus, and the final
output is selected by majority voting in the last round. However, this
consensus-based design faces several limitations. First, multiple rounds of
communication increases token overhead and limits scalability. Second, due to
the inherent conformity of LLMs, agents that initially produce correct
responses may be influenced by incorrect ones during the debate process,
causing error propagation. Third, majority voting introduces randomness and
unfairness in the decision-making phase, and can degrade the reasoning
performance.
  To address these issues, we propose \textsc{Free-MAD}, a novel MAD framework
that eliminates the need for consensus among agents. \textsc{Free-MAD}
introduces a novel score-based decision mechanism that evaluates the entire
debate trajectory rather than relying on the last round only. This mechanism
tracks how each agent's reasoning evolves, enabling more accurate and fair
outcomes. In addition, \textsc{Free-MAD} reconstructs the debate phase by
introducing anti-conformity, a mechanism that enables agents to mitigate
excessive influence from the majority. Experiments on eight benchmark datasets
demonstrate that \textsc{Free-MAD} significantly improves reasoning performance
while requiring only a single-round debate and thus reducing token costs. We
also show that compared to existing MAD approaches, \textsc{Free-MAD} exhibits
improved robustness in real-world attack scenarios.

</details>


### [110] [Agentic Lybic: Multi-Agent Execution System with Tiered Reasoning and Orchestration](https://arxiv.org/abs/2509.11067)
*Liangxuan Guo,Bin Zhu,Qingqian Tao,Kangning Liu,Xun Zhao,Xianzhe Qin,Jin Gao,Guangfu Hao*

Main category: cs.AI

TL;DR: Agentic Lybic is a multi-agent system for desktop automation that uses a finite-state machine (FSM) for dynamic orchestration, improving coordination and quality control.


<details>
  <summary>Details</summary>
Motivation: Existing autonomous agents struggle with complex multi-step desktop automation tasks due to poor coordination and inadequate quality control.

Method: The system uses a finite-state machine (FSM) to route tasks between four components: a Controller, a Manager, three Workers (Technician, Operator, Analyst), and an Evaluator. This enables dynamic selection of the optimal execution strategy and adaptive replanning.

Result: Agentic Lybic achieves a state-of-the-art 57.07% success rate on the OSWorld benchmark, outperforming existing methods.

Conclusion: Principled multi-agent orchestration with continuous quality control provides superior reliability for generalized desktop automation in complex computing environments.

Abstract: Autonomous agents for desktop automation struggle with complex multi-step
tasks due to poor coordination and inadequate quality control. We introduce
\textsc{Agentic Lybic}, a novel multi-agent system where the entire
architecture operates as a finite-state machine (FSM). This core innovation
enables dynamic orchestration. Our system comprises four components: a
Controller, a Manager, three Workers (Technician for code-based operations,
Operator for GUI interactions, and Analyst for decision support), and an
Evaluator. The critical mechanism is the FSM-based routing between these
components, which provides flexibility and generalization by dynamically
selecting the optimal execution strategy for each subtask. This principled
orchestration, combined with robust quality gating, enables adaptive replanning
and error recovery. Evaluated officially on the OSWorld benchmark,
\textsc{Agentic Lybic} achieves a state-of-the-art 57.07\% success rate in 50
steps, substantially outperforming existing methods. Results demonstrate that
principled multi-agent orchestration with continuous quality control provides
superior reliability for generalized desktop automation in complex computing
environments.

</details>


### [111] [Tractable Asymmetric Verification for Large Language Models via Deterministic Replicability](https://arxiv.org/abs/2509.11068)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.AI

TL;DR: 本研究提出了一种验证框架，用于验证大型语言模型（LLM）输出的真实性，确保其由声称的模型生成，而非伪造或由低劣模型生成。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型迅速发展为动态多智能体系统，如何在计算上建立信任成为挑战，尤其是一个智能体如何验证另一个智能体的输出是否真实。

Method: 该框架基于确定性可复制性原则，允许多个验证者概率性地审计LLM输出的小型随机片段，从而有效分配验证工作量。

Result: 模拟结果表明，有针对性的验证比完全重新生成快12倍以上，并且可以通过调整参数来调整检测概率。

Conclusion: 该研究建立了一种可审计LLM系统的可行机制，为负责任的AI奠定了基础，并为未来研究更复杂的异构多智能体系统奠定了基石。

Abstract: The landscape of Large Language Models (LLMs) shifts rapidly towards dynamic,
multi-agent systems. This introduces a fundamental challenge in establishing
computational trust, specifically how one agent can verify that another's
output was genuinely produced by a claimed LLM, and not falsified or generated
by a cheaper or inferior model. To address this challenge, this paper proposes
a verification framework that achieves tractable asymmetric effort, where the
cost to verify a computation is substantially lower than the cost to perform
it. Our approach is built upon the principle of deterministic replicability, a
property inherent to autoregressive models that strictly necessitates a
computationally homogeneous environment where all agents operate on identical
hardware and software stacks. Within this defined context, our framework
enables multiple validators to probabilistically audit small, random segments
of an LLM's output and it distributes the verification workload effectively.
The simulations demonstrated that targeted verification can be over 12 times
faster than full regeneration, with tunable parameters to adjust the detection
probability. By establishing a tractable mechanism for auditable LLM systems,
our work offers a foundational layer for responsible AI and serves as a
cornerstone for future research into the more complex, heterogeneous
multi-agent systems.

</details>


### [112] [Patient-Zero: A Unified Framework for Real-Record-Free Patient Agent Generation](https://arxiv.org/abs/2509.11078)
*Yunghwei Lai,Weizhi Ma,Yang Liu*

Main category: cs.AI

TL;DR: 提出了一种名为 Patient-Zero 的现实患者生成框架，该框架无需真实医疗记录即可生成合成数据，从而缓解了医疗领域的数据收集挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要利用大型语言模型来重写和完成现有的医疗记录，但仍然存在数据隐私、准确性和多样性的局限性，并且缺乏像真实患者一样互动的能力。

Method: Patient-Zero 引入了一种医学对齐的多步骤生成架构，该架构通过分层医学知识注入来构建全面的患者记录，而无需真实的医疗记录。然后，为了优化虚拟患者与人类的互动能力，Patient-Zero 设计了一种动态更新机制，以提高一致性和会话性能。

Result: 实验结果表明，该模型在准确性、多样性和一致性方面取得了良好的性能。在使用生成的虚拟患者进行训练后，现有模型在 MedQA 数据集上显示出显着改进。

Conclusion: Patient-Zero 框架能够生成上下文多样的患者记录，同时保持严格的医学连贯性，并由自适应对话策略和实时临床合理性验证提供支持。

Abstract: Synthetic data generation using large language models (LLMs) has emerged as a
promising solution across various domains, particularly in medical field, to
mitigate data collection challenges. However, existing studies mainly utilize
LLMs to rewrite and complete existing medical records, where the limitations in
data privacy, accuracy, and diversity sill exist, and additionally lack the
ability to interact like real patients. To address these issues, we propose a
realistic patient generation framework, Patient-Zero, which requires no real
medical records. Patient-Zero first introduces a medically-aligned multi-step
generation architecture, which builds comprehensive patient records through
hierarchical medical knowledge injection without real medical records. Then, to
optimize the virtual patient's interaction abilities with humans, Patient-Zero
designs a dynamic updating mechanism to improve the consistency and
conversational performance. Our framework enables the generation of
contextually diverse patient records while maintaining strict medical
coherence, supported by adaptive dialogue strategies and real-time clinical
plausibility verification. Experimental results demonstrate that our model
achieves good performance in accuracy, diversity, and consistency. After
training with our generated virtual patients, existing models show significant
improvements on the MedQA dataset.

</details>


### [113] [Difficulty-Aware Agent Orchestration in LLM-Powered Workflows](https://arxiv.org/abs/2509.11079)
*Jinwei Su,Yinghui Xia,Qizhen Lan,Xinyuan Song,Yang Jingsong,Lewei He,Tianyu Shi*

Main category: cs.AI

TL;DR: 提出了一个名为 DAAO 的动态框架，该框架能够根据输入查询的难度调整工作流程深度、算子选择和 LLM 分配。


<details>
  <summary>Details</summary>
Motivation: 现有的多代理框架通常依赖于静态或任务级的工作流程，这导致简单查询过度处理或复杂查询性能不足，同时也忽略了异构 LLM 之间的效率-性能权衡。

Method: DAAO 包含三个相互依赖的模块：用于难度估计的变分自编码器 (VAE)、模块化算子分配器以及成本和性能感知的 LLM 路由器。

Result: DAAO 在六个基准测试中优于先前的多代理系统，在准确性和推理效率方面均有提升。

Conclusion: DAAO 能够实现细粒度的、特定于查询的推理策略。

Abstract: Large Language Model (LLM)-based agentic systems have shown strong
capabilities across various tasks. However, existing multi-agent frameworks
often rely on static or task-level workflows, which either over-process simple
queries or underperform on complex ones, while also neglecting the
efficiency-performance trade-offs across heterogeneous LLMs. To address these
limitations, we propose Difficulty-Aware Agentic Orchestration (DAAO), a
dynamic framework that adapts workflow depth, operator selection, and LLM
assignment based on the difficulty of each input query. DAAO comprises three
interdependent modules: a variational autoencoder (VAE) for difficulty
estimation, a modular operator allocator, and a cost- and performance-aware LLM
router. By leveraging heterogeneous LLMs and dynamically tailoring workflows,
DAAO enables fine-grained, query-specific reasoning strategies. DAAO
outperforms prior multi-agent systems in both accuracy and inference efficiency
across six benchmarks. We will release our code and implementation details upon
publication.

</details>


### [114] [Neural cellular automata: applications to biology and beyond classical AI](https://arxiv.org/abs/2509.11131)
*Benedikt Hartl,Michael Levin,Léo Pio-Lopez*

Main category: cs.AI

TL;DR: 神经元胞自动机(NCA)是一种强大的建模生物自组织的框架，它通过可训练、可区分的更新规则扩展了经典的基于规则的系统，这些规则捕捉了生物物质的适应性自调节动力学。


<details>
  <summary>Details</summary>
Motivation: NCA 可以模拟分子、细胞、组织和系统层面的过程，为进化、发育、再生、衰老、形态发生和机器人控制提供多尺度能力架构视角。

Method: 将人工神经网络(ANN)嵌入为局部决策中心和局部代理之间的交互规则。

Result: NCA不仅重现了生物启发的目标模式，而且推广到新的条件，表现出对扰动的鲁棒性和开放式适应和推理的能力。

Conclusion: NCA 构成了一种统一的计算精简范例，它不仅弥合了多尺度生物学的基本见解与现代生成人工智能，而且有潜力设计出真正受生物启发的集体智能，能够进行分层推理和控制。

Abstract: Neural Cellular Automata (NCA) represent a powerful framework for modeling
biological self-organization, extending classical rule-based systems with
trainable, differentiable (or evolvable) update rules that capture the adaptive
self-regulatory dynamics of living matter. By embedding Artificial Neural
Networks (ANNs) as local decision-making centers and interaction rules between
localized agents, NCA can simulate processes across molecular, cellular,
tissue, and system-level scales, offering a multiscale competency architecture
perspective on evolution, development, regeneration, aging, morphogenesis, and
robotic control. These models not only reproduce biologically inspired target
patterns but also generalize to novel conditions, demonstrating robustness to
perturbations and the capacity for open-ended adaptation and reasoning. Given
their immense success in recent developments, we here review current literature
of NCAs that are relevant primarily for biological or bioengineering
applications. Moreover, we emphasize that beyond biology, NCAs display robust
and generalizing goal-directed dynamics without centralized control, e.g., in
controlling or regenerating composite robotic morphologies or even on
cutting-edge reasoning tasks such as ARC-AGI-1. In addition, the same
principles of iterative state-refinement is reminiscent to modern generative
Artificial Intelligence (AI), such as probabilistic diffusion models. Their
governing self-regulatory behavior is constraint to fully localized
interactions, yet their collective behavior scales into coordinated
system-level outcomes. We thus argue that NCAs constitute a unifying
computationally lean paradigm that not only bridges fundamental insights from
multiscale biology with modern generative AI, but have the potential to design
truly bio-inspired collective intelligence capable of hierarchical reasoning
and control.

</details>


### [115] [AlignKT: Explicitly Modeling Knowledge State for Knowledge Tracing with Ideal State Alignment](https://arxiv.org/abs/2509.11135)
*Jing Xiao,Chang You,Zhiyu Chen*

Main category: cs.AI

TL;DR: AlignKT模型通过显式建模稳定的知识状态，提升知识追踪的可解释性和教学支持。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型忽略知识状态本身，导致可解释性降低和教学支持不足。

Method: 采用frontend-to-backend架构显式建模稳定知识状态，并使用对比学习模块增强对齐过程的鲁棒性。

Result: AlignKT在三个真实世界数据集上优于七个基线模型，并在其中两个数据集上实现了最先进的结果。

Conclusion: AlignKT模型通过对齐初步知识状态与理想知识状态，实现了更好的性能和可解释性。

Abstract: Knowledge Tracing (KT) serves as a fundamental component of Intelligent
Tutoring Systems (ITS), enabling these systems to monitor and understand
learners' progress by modeling their knowledge state. However, many existing KT
models primarily focus on fitting the sequences of learners' interactions, and
often overlook the knowledge state itself. This limitation leads to reduced
interpretability and insufficient instructional support from the ITS. To
address this challenge, we propose AlignKT, which employs a frontend-to-backend
architecture to explicitly model a stable knowledge state. In this approach,
the preliminary knowledge state is aligned with an additional criterion.
Specifically, we define an ideal knowledge state based on pedagogical theories
as the alignment criterion, providing a foundation for interpretability. We
utilize five encoders to implement this set-up, and incorporate a contrastive
learning module to enhance the robustness of the alignment process. Through
extensive experiments, AlignKT demonstrates superior performance, outperforming
seven KT baselines on three real-world datasets. It achieves state-of-the-art
results on two of these datasets and exhibits competitive performance on the
third. The code of this work is available at
https://github.com/SCNU203/AlignKT.

</details>


### [116] [AI-Generated Content in Cross-Domain Applications: Research Trends, Challenges and Propositions](https://arxiv.org/abs/2509.11151)
*Jianxin Li,Liang Qu,Taotao Cai,Zhixue Zhao,Nur Al Hasan Haldar,Aneesh Krishna,Xiangjie Kong,Flavio Romero Macau,Tanmoy Chakraborty,Aniket Deroy,Binshan Lin,Karen Blackmore,Nasimul Noman,Jingxian Cheng,Ningning Cui,Jianliang Xu*

Main category: cs.AI

TL;DR: 本文概述了人工智能生成内容（AIGC）的最新进展和新兴挑战，并提供了跨领域视角。


<details>
  <summary>Details</summary>
Motivation: 探讨AIGC在不同领域的最新进展和挑战，填补研究空白。

Method: 汇集了16位来自多个学科的学者，提供跨领域视角。

Result: 对AIGC进行了全面概述，介绍了其在不同领域的社会影响，并讨论了关键技术挑战。

Conclusion: 旨在为读者提供关于AIGC的跨领域视角，深入了解当前的研究趋势、持续存在的挑战和未来的发展方向。

Abstract: Artificial Intelligence Generated Content (AIGC) has rapidly emerged with the
capability to generate different forms of content, including text, images,
videos, and other modalities, which can achieve a quality similar to content
created by humans. As a result, AIGC is now widely applied across various
domains such as digital marketing, education, and public health, and has shown
promising results by enhancing content creation efficiency and improving
information delivery. However, there are few studies that explore the latest
progress and emerging challenges of AIGC across different domains. To bridge
this gap, this paper brings together 16 scholars from multiple disciplines to
provide a cross-domain perspective on the trends and challenges of AIGC.
Specifically, the contributions of this paper are threefold: (1) It first
provides a broader overview of AIGC, spanning the training techniques of
Generative AI, detection methods, and both the spread and use of AI-generated
content across digital platforms. (2) It then introduces the societal impacts
of AIGC across diverse domains, along with a review of existing methods
employed in these contexts. (3) Finally, it discusses the key technical
challenges and presents research propositions to guide future work. Through
these contributions, this vision paper seeks to offer readers a cross-domain
perspective on AIGC, providing insights into its current research trends,
ongoing challenges, and future directions.

</details>


### [117] [VideoAgent: Personalized Synthesis of Scientific Videos](https://arxiv.org/abs/2509.11253)
*Xiao Liang,Bangxin Li,Zixuan Chen,Hanyue Zheng,Zhi Ma,Di Wang,Cong Tian,Quan Wang*

Main category: cs.AI

TL;DR: 提出了一种新的多智能体框架VideoAgent，通过对话界面合成个性化的科学视频。


<details>
  <summary>Details</summary>
Motivation: 现有的文档自动化主要集中于海报和幻灯片等静态媒体，缺乏个性化的动态编排和多模态内容同步机制。

Method: VideoAgent将源论文解析为细粒度的资产库，并在用户需求的指导下，编排叙事流程，合成静态幻灯片和动态动画来解释复杂的概念。

Result: 该方法明显优于现有的商业科学视频生成服务，并在科学交流中接近人类水平的质量。

Conclusion: 介绍了SciVidEval，这是第一个用于此任务的综合套件，它结合了多模态内容质量和同步的自动指标与基于视频测验的人工评估，以衡量知识转移。

Abstract: Automating the generation of scientific videos is a crucial yet challenging
task for effective knowledge dissemination. However, existing works on document
automation primarily focus on static media such as posters and slides, lacking
mechanisms for personalized dynamic orchestration and multimodal content
synchronization. To address these challenges, we introduce VideoAgent, a novel
multi-agent framework that synthesizes personalized scientific videos through a
conversational interface. VideoAgent parses a source paper into a fine-grained
asset library and, guided by user requirements, orchestrates a narrative flow
that synthesizes both static slides and dynamic animations to explain complex
concepts. To enable rigorous evaluation, we also propose SciVidEval, the first
comprehensive suite for this task, which combines automated metrics for
multimodal content quality and synchronization with a Video-Quiz-based human
evaluation to measure knowledge transfer. Extensive experiments demonstrate
that our method significantly outperforms existing commercial scientific video
generation services and approaches human-level quality in scientific
communication.

</details>


### [118] [Prompts to Proxies: Emulating Human Preferences via a Compact LLM Ensemble](https://arxiv.org/abs/2509.11311)
*Bingchen Wang,Zi-Yu Khoo,Bryan Kian Hsiang Low*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的对齐框架，将LLM视为人类调查对象的代理，以经济高效且可操纵的方式解决社会科学中的两个紧迫挑战：调查部署成本上升和调查响应数据中日益增长的人口失衡。


<details>
  <summary>Details</summary>
Motivation: 社会科学面临调查部署成本上升和调查响应数据中日益增长的人口失衡。

Method: 1.  将LLM视为人类调查对象的代理。2.  将对齐定义为一个两阶段问题：构建模拟合理受访者配置文件的多样化代理角色（称为禀赋），并选择代表性子集以基于观察数据近似真实人口。3.  引入P2P系统，该系统使用结构化提示工程、基于熵的抽样和基于回归的选择来引导LLM代理实现代表性行为模式。

Result: 在真实世界的观点调查数据集上证明了该方法的有效性，表明对齐的代理人群体可以高保真地重现汇总响应模式，并表现出大量的响应多样性，即使没有人口统计条件。

Conclusion: 该框架为研究多元对齐的实施提供了一个试验平台，并提高了社会科学研究中的数据效率。

Abstract: Large language models (LLMs) have demonstrated promise in emulating
human-like responses across a wide range of tasks. In this paper, we propose a
novel alignment framework that treats LLMs as agent proxies for human survey
respondents, affording a cost-effective and steerable solution to two pressing
challenges in the social sciences: the rising cost of survey deployment and the
growing demographic imbalance in survey response data. Drawing inspiration from
the theory of revealed preference, we formulate alignment as a two-stage
problem: constructing diverse agent personas called endowments that simulate
plausible respondent profiles, and selecting a representative subset to
approximate a ground-truth population based on observed data. To implement the
paradigm, we introduce P2P, a system that steers LLM agents toward
representative behavioral patterns using structured prompt engineering,
entropy-based sampling, and regression-based selection. Unlike
personalization-heavy approaches, our alignment approach is
demographic-agnostic and relies only on aggregate survey results, offering
better generalizability and parsimony. Beyond improving data efficiency in
social science research, our framework offers a testbed for studying the
operationalization of pluralistic alignment. We demonstrate the efficacy of our
approach on real-world opinion survey datasets, showing that our aligned agent
populations can reproduce aggregate response patterns with high fidelity and
exhibit substantial response diversity, even without demographic conditioning.

</details>


### [119] [Decoding Plastic Toxicity: An Intelligent Framework for Conflict-Aware Relational Metapath Extraction from Scientific Abstracts](https://arxiv.org/abs/2509.11330)
*Sudeshna Jana,Manjira Sinha,Tirthankar Dasgupta*

Main category: cs.AI

TL;DR: 该研究提出了一种利用大型语言模型从科学摘要中提取关系元路径的新框架，以追踪污染物传播的暴露途径和生物系统。


<details>
  <summary>Details</summary>
Motivation: 塑料的广泛使用及其在环境中的持久性导致微塑料和纳米塑料在空气、水和土壤中积累，对健康构成严重风险，包括呼吸系统、胃肠道和神经系统疾病。

Method: 该系统识别并连接不同背景下的实体，构建结构化的关系元路径，这些元路径被聚合成毒性轨迹图，从而追踪污染物通过暴露途径和生物系统的传播。此外，为了确保一致性和可靠性，该研究还纳入了一个动态证据协调模块，以解决因不断发展或相互矛盾的研究结果而产生的语义冲突。

Result: 该方法在从嘈杂的科学文本中提取可靠的、高实用性的关系知识方面表现出强大的性能，并为挖掘特定领域语料库中复杂的因果结构提供了一个可扩展的解决方案。

Conclusion: 大型语言模型可以有效地从科学摘要中提取关系元路径，以追踪污染物传播的暴露途径和生物系统，并具有良好的性能和可扩展性。

Abstract: The widespread use of plastics and their persistence in the environment have
led to the accumulation of micro- and nano-plastics across air, water, and
soil, posing serious health risks including respiratory, gastrointestinal, and
neurological disorders. We propose a novel framework that leverages large
language models to extract relational metapaths, multi-hop semantic chains
linking pollutant sources to health impacts, from scientific abstracts. Our
system identifies and connects entities across diverse contexts to construct
structured relational metapaths, which are aggregated into a Toxicity
Trajectory Graph that traces pollutant propagation through exposure routes and
biological systems. Moreover, to ensure consistency and reliability, we
incorporate a dynamic evidence reconciliation module that resolves semantic
conflicts arising from evolving or contradictory research findings. Our
approach demonstrates strong performance in extracting reliable, high-utility
relational knowledge from noisy scientific text and offers a scalable solution
for mining complex cause-effect structures in domain-specific corpora.

</details>


### [120] [The power of dynamic causality in observer-based design for soft sensor applications](https://arxiv.org/abs/2509.11336)
*William Farlessyost,Sebastian Oberst,Shweta Singh*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的框架，通过动态因果分析优化基于观测器的软传感器。


<details>
  <summary>Details</summary>
Motivation: 传统传感器选择方法依赖于线性化可观测性指标或统计相关性，无法捕捉复杂系统的时序演化。

Method: 该方法利用液体时间常数(LTC)网络，一种具有输入相关时间常数的连续时间神经架构，系统地识别和修剪对状态估计具有最小因果影响的传感器输入。该方法采用迭代工作流程：在候选输入上训练LTC观测器，通过受控扰动分析量化每个输入的因果影响，移除影响可忽略不计的输入，并重新训练直到性能下降。

Result: 在三个代表不同物理领域的机械测试平台上证明了该方法：一个谐波强制弹簧-质量-阻尼系统，一个非线性连续搅拌釜反应器，以及一个遵循Lotka-Volterra模型结构的捕食者-猎物模型，但具有季节性强制和增加的复杂性。结果表明，我们的因果关系引导修剪始终识别与底层物理对齐的最小传感器集，同时提高预测精度。

Conclusion: 该框架自动区分基本物理测量与噪声，并确定派生的交互项何时提供互补与冗余信息。除了计算效率外，该方法通过将传感器选择决策建立在动态因果关系而非静态相关性之上，从而增强了解释性，为过程工程、生态监测和农业领域的软传感应用提供了显著的益处。

Abstract: This paper introduces a novel framework for optimizing observer-based soft
sensors through dynamic causality analysis. Traditional approaches to sensor
selection often rely on linearized observability indices or statistical
correlations that fail to capture the temporal evolution of complex systems. We
address this gap by leveraging liquid-time constant (LTC) networks,
continuous-time neural architectures with input-dependent time constants, to
systematically identify and prune sensor inputs with minimal causal influence
on state estimation. Our methodology implements an iterative workflow: training
an LTC observer on candidate inputs, quantifying each input's causal impact
through controlled perturbation analysis, removing inputs with negligible
effect, and retraining until performance degradation occurs. We demonstrate
this approach on three mechanistic testbeds representing distinct physical
domains: a harmonically forced spring-mass-damper system, a nonlinear
continuous stirred-tank reactor, and a predator-prey model following the
structure of the Lotka-Volterra model, but with seasonal forcing and added
complexity. Results show that our causality-guided pruning consistently
identifies minimal sensor sets that align with underlying physics while
improving prediction accuracy. The framework automatically distinguishes
essential physical measurements from noise and determines when derived
interaction terms provide complementary versus redundant information. Beyond
computational efficiency, this approach enhances interpretability by grounding
sensor selection decisions in dynamic causal relationships rather than static
correlations, offering significant benefits for soft sensing applications
across process engineering, ecological monitoring, and agricultural domains.

</details>


### [121] [MAPGD: Multi-Agent Prompt Gradient Descent for Collaborative Prompt Optimization](https://arxiv.org/abs/2509.11361)
*Yichen Han,Bojun Liu,Zhengpeng zhou,Guanyu Liu,Zeng Zhang,Yang Yang,Wenli Wang,Isaac N Shi,Yunyan,Lewei He,Tianyu Shi*

Main category: cs.AI

TL;DR: 提出了一种新的prompt优化框架MAPGD，该框架集成了多智能体协作与基于梯度的优化。


<details>
  <summary>Details</summary>
Motivation: 现有的prompt工程方法依赖于单一的优化轨迹，限制了适应性和效率，同时存在视角狭窄、梯度冲突和计算成本高等问题。

Method: MAPGD框架具有以下特点：用于任务清晰、示例选择、格式设计和文体改进的专业代理；用于解决冲突的语义梯度协调；用于高效探索-利用的基于bandit的候选选择；以及理论收敛保证。

Result: 在分类、生成和推理任务上的实验表明，MAPGD在准确性和效率方面优于单智能体和随机基线。

Conclusion: 梯度融合、智能体专业化和冲突解决的好处得到了证实，为鲁棒和可解释的prompt优化提供了一种统一的、受梯度启发的multi-agent方法。

Abstract: Prompt engineering is crucial for leveraging large language models (LLMs),
but existing methods often rely on a single optimization trajectory, limiting
adaptability and efficiency while suffering from narrow perspectives, gradient
conflicts, and high computational cost. We propose MAPGD (Multi-Agent Prompt
Gradient Descent), a framework integrating multi-agent collaboration with
gradient-based optimization. MAPGD features specialized agents for task
clarity, example selection, format design, and stylistic refinement; semantic
gradient coordination to resolve conflicts; bandit-based candidate selection
for efficient exploration-exploitation; and theoretical convergence guarantees.
Experiments on classification, generation, and reasoning tasks show MAPGD
outperforms single-agent and random baselines in accuracy and efficiency.
Ablations confirm the benefits of gradient fusion, agent specialization, and
conflict resolution, providing a unified, gradient-inspired multi-agent
approach to robust and interpretable prompt optimization.

</details>


### [122] [Securing AI Agents: Implementing Role-Based Access Control for Industrial Applications](https://arxiv.org/abs/2509.11431)
*Aadil Gani Ganie*

Main category: cs.AI

TL;DR: AI agents enhance decision-making and process optimization in industries, but they are vulnerable to security threats like prompt injection attacks. This paper proposes a Role-Based Access Control (RBAC) framework to address these challenges, focusing on on-premises implementations.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are constrained by static training data and require fine-tuning for specific tasks. AI agents mitigate these limitations by accessing external tools and real-time data, transforming operations in industrial settings. However, they are vulnerable to security threats, including prompt injection attacks.

Method: The paper proposes a framework for integrating Role-Based Access Control (RBAC) into AI agents.

Result: The framework aims to support the effective and scalable deployment of AI agents, with a focus on on-premises implementations.

Conclusion: The paper introduces an RBAC framework to enhance the security and reliability of AI agents, particularly in on-premises industrial applications, by addressing vulnerabilities like prompt injection attacks.

Abstract: The emergence of Large Language Models (LLMs) has significantly advanced
solutions across various domains, from political science to software
development. However, these models are constrained by their training data,
which is static and limited to information available up to a specific date.
Additionally, their generalized nature often necessitates fine-tuning --
whether for classification or instructional purposes -- to effectively perform
specific downstream tasks. AI agents, leveraging LLMs as their core, mitigate
some of these limitations by accessing external tools and real-time data,
enabling applications such as live weather reporting and data analysis. In
industrial settings, AI agents are transforming operations by enhancing
decision-making, predictive maintenance, and process optimization. For example,
in manufacturing, AI agents enable near-autonomous systems that boost
productivity and support real-time decision-making. Despite these advancements,
AI agents remain vulnerable to security threats, including prompt injection
attacks, which pose significant risks to their integrity and reliability. To
address these challenges, this paper proposes a framework for integrating
Role-Based Access Control (RBAC) into AI agents, providing a robust security
guardrail. This framework aims to support the effective and scalable deployment
of AI agents, with a focus on on-premises implementations.

</details>


### [123] [Knowledge-Guided Adaptive Mixture of Experts for Precipitation Prediction](https://arxiv.org/abs/2509.11459)
*Chen Jiang,Kofi Osei,Sai Deepthi Yeddula,Dongji Feng,Wei-Shinn Ku*

Main category: cs.AI

TL;DR: 提出了一种用于降水率预测的自适应混合专家(MoE)模型，该模型利用动态路由将输入分配给最相关的专家，每个专家专注于特定的模态或时空模式。


<details>
  <summary>Details</summary>
Motivation: 由于气候系统的复杂性和多源观测数据的异构性，准确的降水预测具有挑战性。传统深度学习模型难以有效整合空间和时间分辨率不同的多源数据。

Method: 提出了一种自适应混合专家(MoE)模型，该模型针对降水率预测进行了定制。该模型中的每个专家都专注于特定的模态或时空模式。此外，还加入了一个动态路由器，用于学习将输入分配给最相关的专家。

Result: 自适应MoE显著优于所有基线模型，并且还引入了一个交互式web可视化工具，使用户能够直观地探索历史天气模式。

Conclusion: 该模型提高了预测精度和可解释性，该工具旨在支持气候敏感部门的利益相关者进行决策。

Abstract: Accurate precipitation forecasting is indispensable in agriculture, disaster
management, and sustainable strategies. However, predicting rainfall has been
challenging due to the complexity of climate systems and the heterogeneous
nature of multi-source observational data, including radar, satellite imagery,
and surface-level measurements. The multi-source data vary in spatial and
temporal resolution, and they carry domain-specific features, making it
challenging for effective integration in conventional deep learning models.
Previous research has explored various machine learning techniques for weather
prediction; however, most struggle with the integration of data with
heterogeneous modalities. To address these limitations, we propose an Adaptive
Mixture of Experts (MoE) model tailored for precipitation rate prediction. Each
expert within the model specializes in a specific modality or spatio-temporal
pattern. We also incorporated a dynamic router that learns to assign inputs to
the most relevant experts. Our results show that this modular design enhances
predictive accuracy and interpretability. In addition to the modeling
framework, we introduced an interactive web-based visualization tool that
enables users to intuitively explore historical weather patterns over time and
space. The tool was designed to support decision-making for stakeholders in
climate-sensitive sectors. We evaluated our approach using a curated multimodal
climate dataset capturing real-world conditions during Hurricane Ian in 2022.
The benchmark results show that the Adaptive MoE significantly outperformed all
the baselines.

</details>


### [124] [Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs](https://arxiv.org/abs/2509.11480)
*Amir Taherin,Juyi Lin,Arash Akbari,Arman Akbari,Pu Zhao,Weiwei Chen,David Kaeli,Yanzhi Wang*

Main category: cs.AI

TL;DR: 本文针对机器人控制领域，探讨了视觉-语言-动作（VLA）模型在不同硬件平台和功率预算下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前对VLA模型在不同架构、硬件平台以及功耗方面的性能扩展理解不足。

Method: 在边缘和数据中心GPU平台上，评估了五个代表性的VLA模型，包括最新的基线模型和两个新提出的架构。使用LIBERO基准测试，测量了在不同边缘功率约束和高性能数据中心GPU配置下的精度、延迟、吞吐量和峰值内存使用等系统级指标。

Result: 研究结果表明：1）架构选择（如动作标记化和模型主干大小）强烈影响吞吐量和内存占用；2）功率受限的边缘设备表现出非线性性能下降，但某些配置可以匹配或超过旧的数据中心GPU；3）高吞吐量变体可以在不显著降低精度的情况下实现。

Conclusion: 研究结果为在各种部署约束下选择和优化VLA模型提供了可操作的见解，并挑战了当前关于数据中心硬件在机器人推理方面优越性的假设。

Abstract: Vision-Language-Action (VLA) models have emerged as powerful generalist
policies for robotic control, yet their performance scaling across model
architectures and hardware platforms, as well as their associated power
budgets, remain poorly understood. This work presents an evaluation of five
representative VLA models -- spanning state-of-the-art baselines and two newly
proposed architectures -- targeting edge and datacenter GPU platforms. Using
the LIBERO benchmark, we measure accuracy alongside system-level metrics,
including latency, throughput, and peak memory usage, under varying edge power
constraints and high-performance datacenter GPU configurations. Our results
identify distinct scaling trends: (1) architectural choices, such as action
tokenization and model backbone size, strongly influence throughput and memory
footprint; (2) power-constrained edge devices exhibit non-linear performance
degradation, with some configurations matching or exceeding older datacenter
GPUs; and (3) high-throughput variants can be achieved without significant
accuracy loss. These findings provide actionable insights when selecting and
optimizing VLAs across a range of deployment constraints. Our work challenges
current assumptions about the superiority of datacenter hardware for robotic
inference.

</details>


### [125] [MedicalOS: An LLM Agent based Operating System for Digital Healthcare](https://arxiv.org/abs/2509.11507)
*Jared Zhu,Junde Wu*

Main category: cs.AI

TL;DR: MedicalOS是一个专为医疗保健设计的基于代理的操作系统，旨在通过将人类指令转换为预定义的数字医疗保健命令来简化临床工作流程。


<details>
  <summary>Details</summary>
Motivation: 当前的数字健康技术系统难以学习和使用，导致临床医生花费大量时间在管理任务上，而不是照顾患者。大型语言模型（LLM）在编码和计算机操作方面的兴起，为通过自然语言指示代理与操作系统和软件交互提供了潜力。

Method: 提出了MedicalOS，一个统一的基于代理的操作系统，作为医疗保健领域特定的抽象层。它将人类指令转换为预定义的数字医疗保健命令，例如患者查询、历史检索、检查管理、报告生成、转诊、治疗计划等。

Result: 在22个专业的214个患者案例中验证了MedicalOS，证明了其高诊断准确性和信心，临床上合理的检查请求，以及结构化报告和药物建议的一致生成。

Conclusion: MedicalOS是推进临床实践中工作流程自动化的一个值得信赖且可扩展的基础。

Abstract: Decades' advances in digital health technologies, such as electronic health
records, have largely streamlined routine clinical processes. Yet, most these
systems are still hard to learn and use: Clinicians often face the burden of
managing multiple tools, repeating manual actions for each patient, navigating
complicated UI trees to locate functions, and spending significant time on
administration instead of caring for patients. The recent rise of large
language model (LLM) based agents demonstrates exceptional capability in coding
and computer operation, revealing the potential for humans to interact with
operating systems and software not by direct manipulation, but by instructing
agents through natural language. This shift highlights the need for an
abstraction layer, an agent-computer interface, that translates human language
into machine-executable commands. In digital healthcare, however, requires a
more domain-specific abstractions that strictly follow trusted clinical
guidelines and procedural standards to ensure safety, transparency, and
compliance. To address this need, we present \textbf{MedicalOS}, a unified
agent-based operational system designed as such a domain-specific abstract
layer for healthcare. It translates human instructions into pre-defined digital
healthcare commands, such as patient inquiry, history retrieval, exam
management, report generation, referrals, treatment planning, that we wrapped
as off-the-shelf tools using machine languages (e.g., Python, APIs, MCP,
Linux). We empirically validate MedicalOS on 214 patient cases across 22
specialties, demonstrating high diagnostic accuracy and confidence, clinically
sound examination requests, and consistent generation of structured reports and
medication recommendations. These results highlight MedicalOS as a trustworthy
and scalable foundation for advancing workflow automation in clinical practice.

</details>


### [126] [Task Decoding based on Eye Movements using Synthetic Data Augmentation](https://arxiv.org/abs/2509.11547)
*Shanmuka Sadhu,Arca Baran,Preeti Pandey,Ayush Kumar*

Main category: cs.AI

TL;DR: 本文利用合成数据增强眼动数据，从而提高任务解码的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习算法在基于眼动数据的解码任务中表现不一，为了验证 Yarbus 的假设，即可以从眼动中解码观察者的任务。

Method: 使用 CTGAN 及其变体 CopulaGAN 和 Gretel AI 合成数据生成器生成合成数据样本，并结合真实数据进行训练。

Result: 通过增加更多的眼动数据，结合额外的合成数据，即使使用传统的机器学习算法，分类精度也得到了提高。任务解码精度从使用随机森林的 28.1% 提高到使用 Inception Time 的 82%（当添加五倍的数据时）。

Conclusion: 通过各种算法和真实数据与合成数据的组合验证了该结论，表明解码精度随着生成数据对真实数据的增强而提高。

Abstract: Machine learning has been extensively used in various applications related to
eye-tracking research. Understanding eye movement is one of the most
significant subsets of eye-tracking research that reveals the scanning pattern
of an individual. Researchers have thoroughly analyzed eye movement data to
understand various eye-tracking applications, such as attention mechanisms,
navigational behavior, task understanding, etc. The outcome of traditional
machine learning algorithms used for decoding tasks based on eye movement data
has received a mixed reaction to Yarbus' claim that it is possible to decode
the observer's task from their eye movements. In this paper, to support the
hypothesis by Yarbus, we are decoding tasks categories while generating
synthetic data samples using well-known Synthetic Data Generators CTGAN and its
variations such as CopulaGAN and Gretel AI Synthetic Data generators on
available data from an in-person user study. Our results show that augmenting
more eye movement data combined with additional synthetically generated
improves classification accuracy even with traditional machine learning
algorithms. We see a significant improvement in task decoding accuracy from
28.1% using Random Forest to 82% using Inception Time when five times more data
is added in addition to the 320 real eye movement dataset sample. Our proposed
framework outperforms all the available studies on this dataset because of the
use of additional synthetic datasets. We validated our claim with various
algorithms and combinations of real and synthetic data to show how decoding
accuracy increases with the increase in the augmentation of generated data to
real data.

</details>


### [127] [Formal Reasoning for Intelligent QA Systems: A Case Study in the Educational Domain](https://arxiv.org/abs/2509.11572)
*Tuan Bui,An Nguyen,Phat Thai,Minh Hua,Ngan Pham L. N.,Ngan Pham T. B.,Dung Le,Long Nguyen,Thanh-Tung Tran,Thang Bui,Tho Quan*

Main category: cs.AI

TL;DR: 本文提出了一种神经符号框架，将大型语言模型与模型检查相结合，以支持属性验证，从而提高推理的忠实性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在许多推理任务中表现出色，但其推理轨迹通常不忠实，并且难以处理动态、基于状态的推理。将大型语言模型与符号引擎相结合的方法虽然提高了可靠性，但仍然局限于静态逻辑形式。

Method: MCFR将自然语言翻译成形式规范，并在转换模型上验证它们。

Result: MCFR 提高了推理的忠实性和可解释性。

Conclusion: MCFR 为高风险封闭域应用中可验证的问答提供了一条可行的途径。

Abstract: Reasoning is essential for closed-domain QA systems in which procedural
correctness and policy compliance are critical. While large language models
(LLMs) have shown strong performance on many reasoning tasks, recent work
reveals that their reasoning traces are often unfaithful - serving more as
plausible justifications than as causally grounded derivations. Efforts to
combine LLMs with symbolic engines (e.g., Prover9, Z3) have improved
reliability but remain limited to static forms of logic, struggling with
dynamic, state-based reasoning such as multi-step progressions and conditional
transitions.
  In this paper, we propose MCFR (Model Checking for Formal Reasoning), a
neuro-symbolic framework that integrates LLMs with model checking to support
property verification. MCFR translates natural language into formal
specifications and verifies them over transition models. To support evaluation,
we introduce EduMC-QA, a benchmark dataset grounded in real academic
procedures. Our results show that MCFR improves reasoning faithfulness and
interpretability, offering a viable path toward verifiable QA in high-stakes
closed-domain applications. In addition to evaluating MCFR, we compare its
performance with state-of-the-art LLMs such as ChatGPT, DeepSeek, and Claude to
contextualize its effectiveness.

</details>


### [128] [A Survey of Reasoning and Agentic Systems in Time Series with Large Language Models](https://arxiv.org/abs/2509.11575)
*Ching Chang,Yidan Shi,Defu Cao,Wei Yang,Jeehyun Hwang,Haixin Wang,Jiacheng Pang,Wei Wang,Yan Liu,Wen-Chih Peng,Tien-Fu Chen*

Main category: cs.AI

TL;DR: 本文概述了时间序列推理，其中时间被视为主要轴，并将中间证据直接纳入答案。


<details>
  <summary>Details</summary>
Motivation: 探索时间序列推理在各个领域的应用，包括传统时间序列分析、解释和理解、因果推理和决策制定以及时间序列生成。

Method: 本文根据推理拓扑（包括直接推理、线性链推理和分支结构推理）组织了文献，并结合了领域的主要目标。

Result: 本文回顾了跨领域的方法和系统，展示了每种拓扑的功能以及在保真度或稳健性方面的不足。

Conclusion: 未来的进展可能取决于将推理质量与效用联系起来的基准，以及在转变感知、流媒体和长远设置下权衡成本和风险的闭环试验台。

Abstract: Time series reasoning treats time as a first-class axis and incorporates
intermediate evidence directly into the answer. This survey defines the problem
and organizes the literature by reasoning topology with three families: direct
reasoning in one step, linear chain reasoning with explicit intermediates, and
branch-structured reasoning that explores, revises, and aggregates. The
topology is crossed with the main objectives of the field, including
traditional time series analysis, explanation and understanding, causal
inference and decision making, and time series generation, while a compact tag
set spans these axes and captures decomposition and verification, ensembling,
tool use, knowledge access, multimodality, agent loops, and LLM alignment
regimes. Methods and systems are reviewed across domains, showing what each
topology enables and where it breaks down in faithfulness or robustness, along
with curated datasets, benchmarks, and resources that support study and
deployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey).
Evaluation practices that keep evidence visible and temporally aligned are
highlighted, and guidance is distilled on matching topology to uncertainty,
grounding with observable artifacts, planning for shift and streaming, and
treating cost and latency as design budgets. We emphasize that reasoning
structures must balance capacity for grounding and self-correction against
computational cost and reproducibility, while future progress will likely
depend on benchmarks that tie reasoning quality to utility and on closed-loop
testbeds that trade off cost and risk under shift-aware, streaming, and
long-horizon settings. Taken together, these directions mark a shift from
narrow accuracy toward reliability at scale, enabling systems that not only
analyze but also understand, explain, and act on dynamic worlds with traceable
evidence and credible outcomes.

</details>


### [129] [AMLNet: A Knowledge-Based Multi-Agent Framework to Generate and Detect Realistic Money Laundering Transactions](https://arxiv.org/abs/2509.11595)
*Sabin Huda,Ernest Foo,Zahra Jadidi,MA Hakim Newton,Abdul Sattar*

Main category: cs.AI

TL;DR: 提出了AMLNet，一个知识驱动的多代理框架，用于生成反洗钱(AML)交易数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的反洗钱研究受限于缺乏公开且符合法规的交易数据集。

Method: 该框架包含一个regulation-aware的交易生成器和一个集成的检测管道，用于生成合成交易数据。

Result: 生成了1,090,173条合成交易，其中约0.16%为洗钱阳性。在AUSTRAC规则覆盖率方面，监管一致性达到75%，综合技术保真度评分为0.75。检测集成在AMLNet的内部测试分区上实现了0.90的F1值。

Conclusion: 发布了AMLNet数据集(版本1.0)，以促进可重复且具有法规意识的反洗钱实验。

Abstract: Anti-money laundering (AML) research is constrained by the lack of publicly
shareable, regulation-aligned transaction datasets. We present AMLNet, a
knowledge-based multi-agent framework with two coordinated units: a
regulation-aware transaction generator and an ensemble detection pipeline. The
generator produces 1,090,173 synthetic transactions (approximately 0.16\%
laundering-positive) spanning core laundering phases (placement, layering,
integration) and advanced typologies (e.g., structuring, adaptive threshold
behavior). Regulatory alignment reaches 75\% based on AUSTRAC rule coverage
(Section 4.2), while a composite technical fidelity score of 0.75 summarizes
temporal, structural, and behavioral realism components (Section 4.4). The
detection ensemble achieves F1 0.90 (precision 0.84, recall 0.97) on the
internal test partitions of AMLNet and adapts to the external SynthAML dataset,
indicating architectural generalizability across different synthetic generation
paradigms. We provide multi-dimensional evaluation (regulatory, temporal,
network, behavioral) and release the dataset (Version 1.0,
https://doi.org/10.5281/zenodo.16736515), to advance reproducible and
regulation-conscious AML experimentation.

</details>


### [130] [Adapting and Evaluating Multimodal Large Language Models for Adolescent Idiopathic Scoliosis Self-Management: A Divide and Conquer Framework](https://arxiv.org/abs/2509.11645)
*Zhaolong Wu,Pu Luo,Jason Pui Yin Cheung,Teng Zhang*

Main category: cs.AI

TL;DR: 本研究评估了多模态大型语言模型（MLLM）在青少年特发性脊柱侧弯（AIS）自我管理中的应用，发现其在解释复杂脊柱X光片和理解AIS护理知识方面存在局限性。


<details>
  <summary>Details</summary>
Motivation: 旨在探索MLLM在AIS自我管理中的应用潜力。

Method: 构建包含3000张X光片和诊断文本的数据库，通过视觉问答、领域知识评估和患者教育咨询评估三个任务评估五个MLLM，并采用脊柱关键点提示和检索增强生成（RAG）技术进行优化。

Result: 视觉提示的有效性因架构而异，RAG显著提高了模型在知识评估任务中的性能。模型在脊柱畸形位置和方向检测方面的准确率较低（分别为0.55和0.13）。

Conclusion: 目前的MLLM还远不能在AIS护理中实现个性化辅助，主要挑战在于准确检测脊柱畸形位置和方向的能力。

Abstract: This study presents the first comprehensive evaluation of Multimodal Large
Language Models (MLLMs) for Adolescent Idiopathic Scoliosis (AIS)
self-management. We constructed a database of approximately 3,000
anteroposterior X-rays with diagnostic texts and evaluated five MLLMs through a
`Divide and Conquer' framework consisting of a visual question-answering task,
a domain knowledge assessment task, and a patient education counseling
assessment task. Our investigation revealed limitations of MLLMs' ability in
interpreting complex spinal radiographs and comprehending AIS care knowledge.
To address these, we pioneered enhancing MLLMs with spinal keypoint prompting
and compiled an AIS knowledge base for retrieval augmented generation (RAG),
respectively. Results showed varying effectiveness of visual prompting across
different architectures, while RAG substantially improved models' performances
on the knowledge assessment task. Our findings indicate current MLLMs are far
from capable in realizing personalized assistant in AIS care. The greatest
challenge lies in their abilities to obtain accurate detections of spinal
deformity locations (best accuracy: 0.55) and directions (best accuracy: 0.13).

</details>


### [131] [HeLoFusion: An Efficient and Scalable Encoder for Modeling Heterogeneous and Multi-Scale Interactions in Trajectory Prediction](https://arxiv.org/abs/2509.11719)
*Bingqing Wei,Lianmin Chen,Zhongyu Xia,Yongtao Wang*

Main category: cs.AI

TL;DR: 提出了 HeLoFusion，一个用于在自动驾驶中预测多智能体轨迹的高效且可扩展的编码器。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉复杂社交动态的全部丰富性，特别是多尺度交互的共存和异构智能体的多样化行为。

Method: 构建以每个智能体为中心的局部、多尺度图，并通过聚合-分解消息传递方案和特定类型的特征网络来处理智能体异构性。

Result: 在 Waymo Open Motion 数据集上实现了最先进的性能，为关键指标（包括 Soft mAP 和 minADE）设置了新的基准。

Conclusion: 显式建模多尺度和异构交互的局部架构是推进运动预测的一种非常有效的策略。

Abstract: Multi-agent trajectory prediction in autonomous driving requires a
comprehensive understanding of complex social dynamics. Existing methods,
however, often struggle to capture the full richness of these dynamics,
particularly the co-existence of multi-scale interactions and the diverse
behaviors of heterogeneous agents. To address these challenges, this paper
introduces HeLoFusion, an efficient and scalable encoder for modeling
heterogeneous and multi-scale agent interactions. Instead of relying on global
context, HeLoFusion constructs local, multi-scale graphs centered on each
agent, allowing it to effectively model both direct pairwise dependencies and
complex group-wise interactions (\textit{e.g.}, platooning vehicles or
pedestrian crowds). Furthermore, HeLoFusion tackles the critical challenge of
agent heterogeneity through an aggregation-decomposition message-passing scheme
and type-specific feature networks, enabling it to learn nuanced,
type-dependent interaction patterns. This locality-focused approach enables a
principled representation of multi-level social context, yielding powerful and
expressive agent embeddings. On the challenging Waymo Open Motion Dataset,
HeLoFusion achieves state-of-the-art performance, setting new benchmarks for
key metrics including Soft mAP and minADE. Our work demonstrates that a
locality-grounded architecture, which explicitly models multi-scale and
heterogeneous interactions, is a highly effective strategy for advancing motion
forecasting.

</details>


### [132] [Learning Representations in Video Game Agents with Supervised Contrastive Imitation Learning](https://arxiv.org/abs/2509.11880)
*Carlos Celemin,Joseph Brennan,Pierluigi Vito Amadori,Tim Bradley*

Main category: cs.AI

TL;DR: 本文提出了一种将监督对比学习 (SupCon) 应用于模仿学习 (IL) 的新方法，重点是为视频游戏环境中的智能体学习更有效的状态表示。


<details>
  <summary>Details</summary>
Motivation: 目标是获得observation的潜在表示，从而更好地捕捉与动作相关的因素，从而更好地建模从observation到demonstrator执行的动作的因果关系。

Method: 提出了一种将 SupCon 损失与连续输出空间集成的方法，使 SupCon 能够在不受环境动作类型约束的情况下运行。

Result: 在 3D 游戏 Astro Bot 和 Returnal 以及多个 2D Atari 游戏上的实验表明，与仅使用监督动作预测损失函数训练的基线模型相比，表示质量有所提高，学习收敛速度更快，泛化能力更好。

Conclusion: 本文表明，所提出的方法可以提高模仿学习的性能。

Abstract: This paper introduces a novel application of Supervised Contrastive Learning
(SupCon) to Imitation Learning (IL), with a focus on learning more effective
state representations for agents in video game environments. The goal is to
obtain latent representations of the observations that capture better the
action-relevant factors, thereby modeling better the cause-effect relationship
from the observations that are mapped to the actions performed by the
demonstrator, for example, the player jumps whenever an obstacle appears ahead.
We propose an approach to integrate the SupCon loss with continuous output
spaces, enabling SupCon to operate without constraints regarding the type of
actions of the environment. Experiments on the 3D games Astro Bot and Returnal,
and multiple 2D Atari games show improved representation quality, faster
learning convergence, and better generalization compared to baseline models
trained only with supervised action prediction loss functions.

</details>


### [133] [EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models](https://arxiv.org/abs/2509.11914)
*Yiqun Yao,Naitong Yu,Xiang Li,Xin Jiang,Xuezhi Fang,Wenjia Ma,Xuying Meng,Jing Li,Aixin Sun,Yequan Wang*

Main category: cs.AI

TL;DR: EgoMem is a lifelong memory agent for real-time omnimodal streams.


<details>
  <summary>Details</summary>
Motivation: To enable real-time models to recognize multiple users, provide personalized responses, and maintain long-term knowledge of users.

Method: EgoMem operates with three asynchronous processes: retrieval, omnimodal dialog, and memory management.

Result: Retrieval and memory management modules achieve over 95% accuracy. Fact-consistency scores above 87% in real-time personalized dialogs.

Conclusion: EgoMem establishes a strong baseline for future research.

Abstract: We introduce EgoMem, the first lifelong memory agent tailored for full-duplex
models that process real-time omnimodal streams. EgoMem enables real-time
models to recognize multiple users directly from raw audiovisual streams, to
provide personalized response, and to maintain long-term knowledge of users'
facts, preferences, and social relationships extracted from audiovisual
history. EgoMem operates with three asynchronous processes: (i) a retrieval
process that dynamically identifies user via face and voice, and gathers
relevant context from a long-term memory; (ii) an omnimodal dialog process that
generates personalized audio responses based on the retrieved context; and
(iii) a memory management process that automatically detects dialog boundaries
from omnimodal streams, and extracts necessary information to update the
long-term memory. Unlike existing memory agents for LLMs, EgoMem relies
entirely on raw audiovisual streams, making it especially suitable for
lifelong, real-time, and embodied scenarios. Experimental results demonstrate
that EgoMem's retrieval and memory management modules achieve over 95% accuracy
on the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot,
the system achieves fact-consistency scores above 87% in real-time personalized
dialogs, establishing a strong baseline for future research.

</details>


### [134] [BuildingGym: An open-source toolbox for AI-based building energy management using reinforcement learning](https://arxiv.org/abs/2509.11922)
*Xilei Dai,Ruotian Chen,Songze Guan,Wen-Tai Li,Chau Yuen*

Main category: cs.AI

TL;DR: BuildingGym是一个开源工具，旨在为建筑能源管理中的常见挑战训练强化学习控制策略，它集成了EnergyPlus作为其核心模拟器，并提供内置的强化学习算法。


<details>
  <summary>Details</summary>
Motivation: 缺乏灵活的框架来在建筑能源管理中的各种控制问题中实施强化学习。

Method: 提出了BuildingGym，一个开源工具，它集成了EnergyPlus作为其核心模拟器，并能够接受外部信号作为控制输入。

Result: 内置算法在冷却负荷管理任务中表现出强大的性能。

Conclusion: BuildingGym通过允许轻松配置和替换强化学习算法、模拟器和控制环境或问题，弥合了建筑经理和人工智能专家之间的差距。

Abstract: Reinforcement learning (RL) has proven effective for AI-based building energy
management. However, there is a lack of flexible framework to implement RL
across various control problems in building energy management. To address this
gap, we propose BuildingGym, an open-source tool designed as a
research-friendly and flexible framework for training RL control strategies for
common challenges in building energy management. BuildingGym integrates
EnergyPlus as its core simulator, making it suitable for both system-level and
room-level control. Additionally, BuildingGym is able to accept external
signals as control inputs instead of taking the building as a stand-alone
entity. This feature makes BuildingGym applicable for more flexible
environments, e.g. smart grid and EVs community. The tool provides several
built-in RL algorithms for control strategy training, simplifying the process
for building managers to obtain optimal control strategies. Users can achieve
this by following a few straightforward steps to configure BuildingGym for
optimization control for common problems in the building energy management
field. Moreover, AI specialists can easily implement and test state-of-the-art
control algorithms within the platform. BuildingGym bridges the gap between
building managers and AI specialists by allowing for the easy configuration and
replacement of RL algorithms, simulators, and control environments or problems.
With BuildingGym, we efficiently set up training tasks for cooling load
management, targeting both constant and dynamic cooling load management. The
built-in algorithms demonstrated strong performance across both tasks,
highlighting the effectiveness of BuildingGym in optimizing cooling strategies.

</details>


### [135] [Neuromorphic Intelligence](https://arxiv.org/abs/2509.11940)
*Marcel van Gerven*

Main category: cs.AI

TL;DR: 神经形态计算旨在模仿人脑的效率，灵活性和适应性，但挑战在于找到一个统一的理论框架。


<details>
  <summary>Details</summary>
Motivation: 传统数字方法依赖于大量的计算和能源，神经形态系统利用大脑启发的计算原理来实现更高的能源效率。通过利用人工智能，神经科学，物理学，化学和材料科学的见解，神经形态计算有望提供可持续，透明且广泛可访问的智能系统。

Method: 动态系统理论提供了一个基础。它以微分演算为基础，为自然和人工基质中的推理，学习和控制建模提供了一种有原则的语言。在这种框架内，噪声可以被用作学习的资源，而微分遗传编程能够发现实现适应性行为的动力系统。

Result: 智能行为来自物理基质的动力学。

Conclusion: 拥抱这种观点为新兴的神经形态智能铺平了道路，从而推动了人工智能的科学和可持续性。

Abstract: Neuromorphic computing seeks to replicate the remarkable efficiency,
flexibility, and adaptability of the human brain in artificial systems. Unlike
conventional digital approaches, which depend on massive computational and
energy resources, neuromorphic systems exploit brain-inspired principles of
computation to achieve orders of magnitude greater energy efficiency. By
drawing on insights from artificial intelligence, neuroscience, physics,
chemistry, and materials science, neuromorphic computing promises to deliver
intelligent systems that are sustainable, transparent, and widely accessible. A
central challenge, however, is to identify a unifying theoretical framework
capable of bridging these diverse disciplines. We argue that dynamical systems
theory provides such a foundation. Rooted in differential calculus, it offers a
principled language for modeling inference, learning, and control in both
natural and artificial substrates. Within this framework, noise can be
harnessed as a resource for learning, while differential genetic programming
enables the discovery of dynamical systems that implement adaptive behaviors.
Embracing this perspective paves the way toward emergent neuromorphic
intelligence, where intelligent behavior arises from the dynamics of physical
substrates, advancing both the science and sustainability of AI.

</details>


### [136] [How to Evaluate Medical AI](https://arxiv.org/abs/2509.11941)
*Ilia Kopanichuk,Petr Anokhin,Vladimir Shaposhnikov,Vladimir Makharev,Ekaterina Tsapieva,Iaroslav Bespalov,Dmitry V. Dylov,Ivan Oseledets*

Main category: cs.AI

TL;DR: This paper introduces new metrics, Relative Precision and Recall of Algorithmic Diagnostics (RPAD and RRAD), for evaluating AI in medical diagnosis by comparing AI outputs against multiple expert opinions and normalizing performance against inter-expert disagreement.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics fail to account for variability in expert judgments, leading to inconsistent AI performance assessments. Inter-rater agreement statistics lack interpretability.

Method: The paper introduces RPAD and RRAD metrics. The study uses 360 medical dialogues, comparing large language models (LLMs) against a panel of physicians. The methodology allows for free-form diagnosis and automated identification with high accuracy.

Result: Top-performing models achieve consistency on par with or exceeding expert consensus. Expert judgments exhibit significant variability, often greater than that between AI and humans. A remarkable 98% accuracy becomes attainable in the automated methodology for establishing the identity of free-form clinical diagnoses.

Conclusion: The study supports the need to adopt relative metrics in medical AI due to the limitations of absolute metrics and the significant variability in expert judgments.

Abstract: The integration of artificial intelligence (AI) into medical diagnostic
workflows requires robust and consistent evaluation methods to ensure
reliability, clinical relevance, and the inherent variability in expert
judgments. Traditional metrics like precision and recall often fail to account
for the inherent variability in expert judgments, leading to inconsistent
assessments of AI performance. Inter-rater agreement statistics like Cohen's
Kappa are more reliable but they lack interpretability. We introduce Relative
Precision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new
evaluation metrics that compare AI outputs against multiple expert opinions
rather than a single reference. By normalizing performance against inter-expert
disagreement, these metrics provide a more stable and realistic measure of the
quality of predicted diagnosis. In addition to the comprehensive analysis of
diagnostic quality measures, our study contains a very important side result.
Our evaluation methodology allows us to avoid selecting diagnoses from a
limited list when evaluating a given case. Instead, both the models being
tested and the examiners verifying them arrive at a free-form diagnosis. In
this automated methodology for establishing the identity of free-form clinical
diagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our
approach using 360 medical dialogues, comparing multiple large language models
(LLMs) against a panel of physicians. Large-scale study shows that
top-performing models, such as DeepSeek-V3, achieve consistency on par with or
exceeding expert consensus. Moreover, we demonstrate that expert judgments
exhibit significant variability - often greater than that between AI and
humans. This finding underscores the limitations of any absolute metrics and
supports the need to adopt relative metrics in medical AI.

</details>


### [137] [Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics](https://arxiv.org/abs/2509.11943)
*Antonin Sulc,Thorsten Hellert*

Main category: cs.AI

TL;DR: 本文介绍了一种神经符号多智能体架构，其中单个智能体的信念状态正式表示为 Kripke 模型，使他们能够使用模态逻辑的形式语言来推理已知的可能性和必然性概念。


<details>
  <summary>Details</summary>
Motivation: 扩展模型和数据集的范例已经产生了显着的涌现能力，我们认为扩展这些环境中智能体推理的结构、保真度和逻辑一致性是人工智能研究的一个关键但尚未充分探索的维度。

Method: 本文介绍了一种神经符号多智能体架构，其中单个智能体的信念状态正式表示为 Kripke 模型。 我们利用不可变的、特定于领域的知识来进行推理信息，这些信息被编码为对正确诊断至关重要的逻辑约束。 在提出的模型中，我们展示了主动引导 LM 假设生成的约束，有效地防止它们得出物理或逻辑上站不住脚的结论。

Result: 在一个高保真模拟粒子加速器环境中，我们的系统通过将 LM 强大的语义直觉与模态逻辑和事实世界模型的严格、可验证的验证相结合，成功地诊断出复杂的、级联的故障。

Conclusion: 本文展示了一条通往更强大、更可靠和更可验证的自主代理的可行途径。

Abstract: The development of intelligent agents, particularly those powered by language
models (LMs), has shown the critical role in various environments that require
intelligent and autonomous decision. Environments are not passive testing
grounds and they represent the data required for agents to learn and exhibit
very challenging conditions that require adaptive, complex and autonomous
capacity to make decisions. While the paradigm of scaling models and datasets
has led to remarkable emergent capabilities, we argue that scaling the
structure, fidelity, and logical consistency of agent reasoning within these
environments is a crucial, yet underexplored, dimension of AI research. This
paper introduces a neuro-symbolic multi-agent architecture where the belief
states of individual agents are formally represented as Kripke models. This
foundational choice enables them to reason about known concepts of
\emph{possibility} and \emph{necessity} using the formal language of modal
logic. In this work, we use of immutable, domain-specific knowledge to make
infere information, which is encoded as logical constraints essential for
proper diagnosis. In the proposed model, we show constraints that actively
guide the hypothesis generation of LMs, effectively preventing them from
reaching physically or logically untenable conclusions. In a high-fidelity
simulated particle accelerator environment, our system successfully diagnoses
complex, cascading failures by combining the powerful semantic intuition of LMs
with the rigorous, verifiable validation of modal logic and a factual world
model and showcasing a viable path toward more robust, reliable, and verifiable
autonomous agents.

</details>


### [138] [Agentic Temporal Graph of Reasoning with Multimodal Language Models: A Potential AI Aid to Healthcare](https://arxiv.org/abs/2509.11944)
*Susanta Mitra*

Main category: cs.AI

TL;DR: 本文提出了一种基于时间图的推理过程，以解决医疗领域中多模态医学推理的挑战，并辅助医疗专业人员进行正确的诊断。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态推理模型在医疗领域的应用有限，并且在诊断方面存在推理不正确的问题。

Method: 提出了一种基于时间图的推理过程，该过程通过有向图建模，可以进行回溯、细化推理内容、创建或删除现有原因，以达到最佳推荐或答案。此外，该框架还提供了任务分配和交叉验证机制，以进一步提高推理输出的准确性。

Result: 一些基本的实验和分析结果证明了所提出的初步方法的新颖性和实用性。

Conclusion: 所提出的多智能体时间推理框架能够跟踪和分析患者的健康和疾病进展，并提高推理输出的准确性。

Abstract: Healthcare and medicine are multimodal disciplines that deal with multimodal
data for reasoning and diagnosing multiple diseases. Although some multimodal
reasoning models have emerged for reasoning complex tasks in scientific
domains, their applications in the healthcare domain remain limited and fall
short in correct reasoning for diagnosis. To address the challenges of
multimodal medical reasoning for correct diagnosis and assist the healthcare
professionals, a novel temporal graph-based reasoning process modelled through
a directed graph has been proposed in the current work. It helps in
accommodating dynamic changes in reasons through backtracking, refining the
reasoning content, and creating new or deleting existing reasons to reach the
best recommendation or answer. Again, consideration of multimodal data at
different time points can enable tracking and analysis of patient health and
disease progression. Moreover, the proposed multi-agent temporal reasoning
framework provides task distributions and a cross-validation mechanism to
further enhance the accuracy of reasoning outputs. A few basic experiments and
analysis results justify the novelty and practical utility of the proposed
preliminary approach.

</details>


### [139] [MusicSwarm: Biologically Inspired Intelligence for Music Composition](https://arxiv.org/abs/2509.11973)
*Markus J. Buehler*

Main category: cs.AI

TL;DR: 去中心化的模型群可以通过群体协作的方式创作出连贯的音乐作品，且无需更新权重。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过去中心化的方式，利用多个相同的冻结预训练模型创作音乐。

Method: 构建一个去中心化的模型群，其中每个模型通过感知和传递音乐的和谐、节奏和结构等信息进行协作。

Result: 实验结果表明，该模型群在音乐质量、多样性和结构变化方面均优于集中式多智能体系统，并在创造力指标上表现更佳。

Conclusion: 通过将专业化从参数更新转移到互动规则、共享记忆和动态共识，MusicSwarm为长期的创意结构提供了一条计算和数据高效的途径，并且可以立即转移到音乐以外的协作写作、设计和科学发现等领域。

Abstract: We show that coherent, long-form musical composition can emerge from a
decentralized swarm of identical, frozen foundation models that coordinate via
stigmergic, peer-to-peer signals, without any weight updates. We compare a
centralized multi-agent system with a global critic to a fully decentralized
swarm in which bar-wise agents sense and deposit harmonic, rhythmic, and
structural cues, adapt short-term memory, and reach consensus. Across symbolic,
audio, and graph-theoretic analyses, the swarm yields superior quality while
delivering greater diversity and structural variety and leads across creativity
metrics. The dynamics contract toward a stable configuration of complementary
roles, and self-similarity networks reveal a small-world architecture with
efficient long-range connectivity and specialized bridging motifs, clarifying
how local novelties consolidate into global musical form. By shifting
specialization from parameter updates to interaction rules, shared memory, and
dynamic consensus, MusicSwarm provides a compute- and data-efficient route to
long-horizon creative structure that is immediately transferable beyond music
to collaborative writing, design, and scientific discovery.

</details>


### [140] [Human-AI Use Patterns for Decision-Making in Disaster Scenarios: A Systematic Review](https://arxiv.org/abs/2509.12034)
*Emmanuel Adjei Domfeh,Christopher L. Dancy*

Main category: cs.AI

TL;DR: 本研究综述了人机协作在灾害管理决策中的应用。


<details>
  <summary>Details</summary>
Motivation: 灾害场景下，及时和充分知情的决策至关重要，但常受不确定性、动态环境和有限资源的挑战。

Method: 分析了51篇同行评审研究，识别人机决策支持系统、任务与资源协调、信任与透明度、模拟与训练四大类。

Result: AI系统可增强情境感知、提高响应效率并支持复杂决策，但也存在可扩展性、可解释性和系统互操作性方面的局限性。

Conclusion: 强调需要自适应、值得信赖和具有上下文感知的人机系统，以提高灾害复原能力和公平的恢复结果。

Abstract: In high-stakes disaster scenarios, timely and informed decision-making is
critical yet often challenged by uncertainty, dynamic environments, and limited
resources. This paper presents a systematic review of Human-AI collaboration
patterns that support decision-making across all disaster management phases.
Drawing from 51 peer-reviewed studies, we identify four major categories:
Human-AI Decision Support Systems, Task and Resource Coordination, Trust and
Transparency, and Simulation and Training. Within these, we analyze
sub-patterns such as cognitive-augmented intelligence, multi-agent
coordination, explainable AI, and virtual training environments. Our review
highlights how AI systems may enhance situational awareness, improves response
efficiency, and support complex decision-making, while also surfacing critical
limitations in scalability, interpretability, and system interoperability. We
conclude by outlining key challenges and future research directions,
emphasizing the need for adaptive, trustworthy, and context-aware Human-AI
systems to improve disaster resilience and equitable recovery outcomes.

</details>


### [141] [When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for Cross-Modal Safety in Multimodal Large Language Models](https://arxiv.org/abs/2509.12060)
*Wei Cai,Shujuan Liu,Jian Zhao,Ziyan Shi,Yusheng Zhao,Yuchen Yuan,Tianle Zhang,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: MLLMs have safety issues due to long-chain reasoning with multimodal data.


<details>
  <summary>Details</summary>
Motivation: MLLMs are vulnerable to implicit reasoning risks where unimodal inputs combine into harmful outputs.

Method: Introduce a dataset (SSUI) and a training framework (SRPO) to align MLLM reasoning with safety values.

Result: SRPO-trained models achieve state-of-the-art safety results.

Conclusion: The SRPO framework effectively improves the safety of MLLMs by aligning their reasoning paths with human safety values.

Abstract: Multimodal Large Language Models (MLLMs) are susceptible to the implicit
reasoning risk, wherein innocuous unimodal inputs synergistically assemble into
risky multimodal data that produce harmful outputs. We attribute this
vulnerability to the difficulty of MLLMs maintaining safety alignment through
long-chain reasoning. To address this issue, we introduce
Safe-Semantics-but-Unsafe-Interpretation (SSUI), the first dataset featuring
interpretable reasoning paths tailored for such a cross-modal challenge. A
novel training framework, Safety-aware Reasoning Path Optimization (SRPO), is
also designed based on the SSUI dataset to align the MLLM's internal reasoning
process with human safety values. Experimental results show that our
SRPO-trained models achieve state-of-the-art results on key safety benchmarks,
including the proposed Reasoning Path Benchmark (RSBench), significantly
outperforming both open-source and top-tier commercial MLLMs.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [142] [Dynamic read & write optimization with TurtleKV](https://arxiv.org/abs/2509.10714)
*Tony Astolfi,Vidya Silai,Darby Huye,Lan Liu,Raja R. Sambasivan,Johes Bater*

Main category: cs.DB

TL;DR: 提出了一种新的键值存储系统TurtleKV，它可以在读写性能之间动态地进行权衡，并且在各种工作负载下都优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有键值存储在读写性能、内存使用之间存在权衡，并且难以动态调整以适应变化的工作负载。

Method: 设计了一种新的无偏数据结构，可以通过动态调整内存使用来优化读写性能。

Result: 在YCSB测试中，TurtleKV的写吞吐量比RocksDB高8倍，读吞吐量高5倍，空间放大相似。与SplinterDB相比，TurtleKV在点查询上提高了40%，范围扫描上提高了6倍，写性能相似，空间放大减少了50%。

Conclusion: TurtleKV通过动态地在读写性能之间进行权衡，实现了比现有系统更好的性能。

Abstract: High read and write performance is important for generic key/value stores,
which are fundamental to modern applications and databases. Yet, achieving high
performance for both reads and writes is challenging due to traditionally
limited memory and the pick-any-two-out-of-three tradeoff between memory use,
read performance, and write performance. Existing state-of-the-art approaches
limit memory usage and chose a primary dimension (reads or writes) for which to
optimize their on-disk structures. They recover performance in the remaining
dimension by other mechanisms. This approach limits databases' maximum
performance in the remaining dimension and their dynamic (online) tunability to
respond to changing workloads. We explore a different approach that dynamically
trades memory for read or write performance as needed. We present TurtleKV,
which includes a novel unbiased data structure for on-disk storage. It includes
a knob that dynamically increases memory reserved for increasing read or write
performance. When evaluated on YCSB, TurtleKV achieves up to 8x the write
throughput of industry-leader RocksDB and up to 5x the read throughput while
incurring similar space amplification. Compared to the state-of-the-art system
SplinterDB, TurtleKV runs up to 40% better on point queries, up to 6x better on
range scans and achieves similar write performance, while incurring 50% less
space amplification.

</details>


### [143] [The Space-Time Complexity of Sum-Product Queries](https://arxiv.org/abs/2509.11920)
*Kyle Deeds,Timo Camillo Merkl,Reinhard Pichler,Dan Suciu*

Main category: cs.DB

TL;DR: 这篇论文关注的是查询评估中的空间复杂度问题，而这个问题在很大程度上被忽略了。


<details>
  <summary>Details</summary>
Motivation: 在查询评估中，虽然时间复杂度得到了持续的改进，但空间复杂度却在很大程度上被忽视了。这在具有严格预定义空间约束的设置中尤其具有挑战性。

Method: 论文研究了合取查询（CQ）以及更广泛的求和-乘积查询（SPQ）的组合时空复杂度。提出了几种用于评估SPQ的空间效率算法。

Result: 论文表明，几乎总能以渐近更低的空间复杂度实现最优时间复杂度，而传统方法无法做到这一点。

Conclusion: 论文提出了空间高效的算法，可以在保证时间复杂度的前提下，降低空间复杂度。

Abstract: While extensive research on query evaluation has achieved consistent
improvements in the time complexity of algorithms, the space complexity of
query evaluation has been largely ignored. This is a particular challenge in
settings with strict pre-defined space constraints. In this paper, we examine
the combined space-time complexity of conjunctive queries (CQs) and, more
generally, of sum-product queries (SPQs). We propose several classes of
space-efficient algorithms for evaluating SPQs, and we show that the optimal
time complexity is almost always achievable with asymptotically lower space
complexity than traditional approaches.

</details>


### [144] [Query Answering under Volume-Based Diversity Functions](https://arxiv.org/abs/2509.11929)
*Marcelo Arenas,Timo Camillo Merkl,Reinhard Pichler,Cristian Riveros*

Main category: cs.DB

TL;DR: 本文提出了一种新的基于体积而非距离的元组多样性计算方法，并提供了一个定义基于体积的多样性函数的框架。


<details>
  <summary>Details</summary>
Motivation: 当查询评估产生过多的元组时，一种新的查询应答方法是检索它们的一个多样化子集。现有的方法是使用元组之间的距离函数来测量多样性，但这种方法可能会显示出一些不直观的行为，并且找到固定大小的最大多样性子集通常是难以处理的。

Method: 提出了一种基于体积的元组多样性计算方法。

Result: 证明了对于任何基于体积的多样性函数，总是可以计算出一个(1-1/e)-近似值。

Conclusion: 在组合复杂性方面，将基于体积的多样性函数下的CQ评估与解决方案的排序枚举联系起来，找到了在多项式时间内可以计算出(1-1/e)-近似值的一般条件。

Abstract: When query evaluation produces too many tuples, a new approach in query
answering is to retrieve a diverse subset of them. The standard approach for
measuring the diversity of a set of tuples is to use a distance function
between tuples, which measures the dissimilarity between them, to then
aggregate the pairwise distances of the set into a score (e.g., by using sum or
min aggregation). However, as we will point out in this work, the resulting
diversity measures may display some unintuitive behavior. Moreover, even in
very simple settings, finding a maximally diverse subset of the answers of
fixed size is, in general, intractable and little is known about approximations
apart from some hand-picked distance-aggregator pairs.
  In this work, we introduce a novel approach for computing the diversity of
tuples based on volume instead of distance. We present a framework for defining
volume-based diversity functions and provide several examples of these measures
applied to relational data. Although query answering of conjunctive queries
(CQ) under this setting is intractable in general, we show that one can always
compute a (1-1/e)-approximation for any volume-based diversity function.
Furthermore, in terms of combined complexity, we connect the evaluation of CQs
under volume-based diversity functions with the ranked enumeration of
solutions, finding general conditions under which a (1-1/e)-approximation can
be computed in polynomial time.

</details>


### [145] [SAQ: Pushing the Limits of Vector Quantization through Code Adjustment and Dimension Segmentation](https://arxiv.org/abs/2509.12086)
*Hui Li,Shiyuan Deng,Xiao Yan,Xiangyu Zhi,James Cheng*

Main category: cs.DB

TL;DR: 提出了一种新的向量量化（VQ）方法，称为SAQ，以提高近似最近邻搜索（ANNS）的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的VQ方法在编码效率和量化精度之间难以平衡。

Method: SAQ采用维度分割技术，优先处理具有较大值的重要维度段，并使用动态规划算法优化维度分割和位分配。此外，还设计了一种代码调整技术来加速向量编码。

Result: SAQ在量化误差方面最多减少80%，编码速度比Extended RabitQ快80倍以上。

Conclusion: SAQ在经典方法和最新的方法上都表现出优越性。

Abstract: Approximate Nearest Neighbor Search (ANNS) plays a critical role in
applications such as search engines, recommender systems, and RAG for LLMs.
Vector quantization (VQ), a crucial technique for ANNS, is commonly used to
reduce space overhead and accelerate distance computations. However, despite
significant research advances, state-of-the-art VQ methods still face
challenges in balancing encoding efficiency and quantization accuracy. To
address these limitations, we propose a novel VQ method called SAQ. To improve
accuracy, SAQ employs a new dimension segmentation technique to strategically
partition PCA-projected vectors into segments along their dimensions. By
prioritizing leading dimension segments with larger magnitudes, SAQ allocates
more bits to high-impact segments, optimizing the use of the available space
quota. An efficient dynamic programming algorithm is developed to optimize
dimension segmentation and bit allocation, ensuring minimal quantization error.
To speed up vector encoding, SAQ devises a code adjustment technique to first
quantize each dimension independently and then progressively refine quantized
vectors using a coordinate-descent-like approach to avoid exhaustive
enumeration. Extensive experiments demonstrate SAQ's superiority over classical
methods (e.g., PQ, PCA) and recent state-of-the-art approaches (e.g., LVQ,
Extended RabitQ). SAQ achieves up to 80% reduction in quantization error and
accelerates encoding speed by over 80x compared to Extended RabitQ.

</details>


### [146] [Towards a Standard for JSON Document Databases](https://arxiv.org/abs/2509.12189)
*Elena Botoeva,Julien Corman*

Main category: cs.DB

TL;DR: 本文对MongoDB聚合框架进行了形式化，旨在为查询JSON文档数据库的行业标准奠定基础。


<details>
  <summary>Details</summary>
Motivation: 确定一个可作为行业标准的起点，用于查询JSON文档数据库的片段。

Method: 提供了一组选定操作符的语法和形式语义，并展示了该片段与已知关系查询语言的关系。

Result: 解释了我们的语义与当前MongoDB实现的不同之处，并证明了我们的选择是合理的。

Conclusion: 提供了一组可用于查询优化的代数转换。

Abstract: In this technical report, we present a formalisation of the MongoDB
aggregation framework. Our aim is to identify a fragment that could serve as
the starting point for an industry-wide standard for querying JSON document
databases. We provide a syntax and formal semantics for a set of selected
operators, We show how this fragment relates to known relational query
languages. We explain how our semantics differs from the current implementation
of MongoDB, and justify our choices. We provide a set of algebraic
transformations that can be used for query optimisation.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [147] [DSRAG: A Domain-Specific Retrieval Framework Based on Document-derived Multimodal Knowledge Graph](https://arxiv.org/abs/2509.10467)
*Mengzheng Yang,Yanfei Ren,David Osei Opoku,Ruochang Li,Peng Ren,Chunxiao Xing*

Main category: cs.IR

TL;DR: 这篇论文提出了一种名为DSRAG的领域特定检索增强生成框架，旨在提高领域特定问答的性能。


<details>
  <summary>Details</summary>
Motivation: 通用大型语言模型在领域特定任务中存在知识幻觉和领域适应性不足的问题。检索增强生成(RAG)通过整合外部知识来提高准确性和相关性，但传统RAG在领域知识准确性和上下文建模方面仍面临限制。

Method: 该方法利用领域特定文档构建多模态知识图谱，并引入语义修剪和结构化子图检索机制，结合知识图谱上下文和向量检索结果来指导语言模型生成更可靠的响应。

Result: 使用Langfuse多维评分机制进行的评估表明，该方法在领域特定问答方面表现出色。

Conclusion: 验证了将多模态知识图谱与检索增强生成相结合的有效性。

Abstract: Current general-purpose large language models (LLMs) commonly exhibit
knowledge hallucination and insufficient domain-specific adaptability in
domain-specific tasks, limiting their effectiveness in specialized question
answering scenarios. Retrieval-augmented generation (RAG) effectively tackles
these challenges by integrating external knowledge to enhance accuracy and
relevance. However, traditional RAG still faces limitations in domain knowledge
accuracy and context modeling.To enhance domain-specific question answering
performance, this work focuses on a graph-based RAG framework, emphasizing the
critical role of knowledge graph quality during the generation process. We
propose DSRAG (Domain-Specific RAG), a multimodal knowledge graph-driven
retrieval-augmented generation framework designed for domain-specific
applications. Our approach leverages domain-specific documents as the primary
knowledge source, integrating heterogeneous information such as text, images,
and tables to construct a multimodal knowledge graph covering both conceptual
and instance layers. Building on this foundation, we introduce semantic pruning
and structured subgraph retrieval mechanisms, combining knowledge graph context
and vector retrieval results to guide the language model towards producing more
reliable responses. Evaluations using the Langfuse multidimensional scoring
mechanism show that our method excels in domain-specific question answering,
validating the efficacy of integrating multimodal knowledge graphs with
retrieval-augmented generation.

</details>


### [148] [Learning Decomposed Contextual Token Representations from Pretrained and Collaborative Signals for Generative Recommendation](https://arxiv.org/abs/2509.10468)
*Yifan Liu,Yaokun Liu,Zelin Li,Zhenrui Yue,Gyuseok Lee,Ruichen Yao,Yang Zhang,Dong Wang*

Main category: cs.IR

TL;DR: 提出了一种名为DECOR的统一框架，通过上下文 Token 组合和分解嵌入融合来保留预训练的语义，同时增强 Token 嵌入的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式推荐器采用两阶段范例，但这两个阶段针对不同的目标进行了优化，导致目标不一致，具体表现为次优的静态 Token 化和丢弃预训练的语义。

Method: 学习分解的上下文 Token 表示 (DECOR)，它引入了上下文 Token 组合来细化基于用户交互上下文的 Token 嵌入，以及分解嵌入融合，将预训练的代码本嵌入与新学习的协作嵌入集成。

Result: 在三个真实世界数据集上的实验表明，DECOR 在推荐性能方面始终优于最先进的基线。

Conclusion: DECOR 能够有效提升推荐性能，代码将在发表后公开。

Abstract: Recent advances in generative recommenders adopt a two-stage paradigm: items
are first tokenized into semantic IDs using a pretrained tokenizer, and then
large language models (LLMs) are trained to generate the next item via
sequence-to-sequence modeling. However, these two stages are optimized for
different objectives: semantic reconstruction during tokenizer pretraining
versus user interaction modeling during recommender training. This objective
misalignment leads to two key limitations: (i) suboptimal static tokenization,
where fixed token assignments fail to reflect diverse usage contexts; and (ii)
discarded pretrained semantics, where pretrained knowledge - typically from
language model embeddings - is overwritten during recommender training on user
interactions. To address these limitations, we propose to learn DEcomposed
COntextual Token Representations (DECOR), a unified framework that preserves
pretrained semantics while enhancing the adaptability of token embeddings.
DECOR introduces contextualized token composition to refine token embeddings
based on user interaction context, and decomposed embedding fusion that
integrates pretrained codebook embeddings with newly learned collaborative
embeddings. Experiments on three real-world datasets demonstrate that DECOR
consistently outperforms state-of-the-art baselines in recommendation
performance. Our code will be made available upon publication.

</details>


### [149] [Real-Time RAG for the Identification of Supply Chain Vulnerabilities](https://arxiv.org/abs/2509.10469)
*Jesse Ponnock,Grace Kenneally,Michael Robert Briggs,Elinor Yeo,Tyrone Patterson III,Nicholas Kinberg,Matthew Kalinowski,David Hechtman*

Main category: cs.IR

TL;DR: 本研究提出了一种新的供应链分析方法，该方法集成了新兴的检索增强生成（RAG）预处理和检索技术与先进的网络抓取技术，旨在减少将新信息纳入增强型LLM 的延迟，从而能够及时分析供应链中断。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）提供了前所未有的分析机会，但是，它们的知识库受到模型最后训练日期的限制，这使得这些能力无法被任务影响依赖于新兴和及时信息的组织使用。

Method: 通过将新兴的检索增强生成（RAG）预处理和检索技术与先进的网络抓取技术相结合，集成到LLM中，从而减少延迟，实现及时性分析。

Result: 结果表明，在将 RAG 系统应用于供应链分析时，微调嵌入检索模型始终提供最显着的性能提升，从而突出了检索质量的关键重要性。自适应迭代检索（可根据上下文动态调整检索深度）进一步提高了性能，尤其是在复杂的供应链查询中。相反，微调 LLM 产生的改进有限，资源成本更高，而诸如下行查询抽象之类的技术在实践中明显优于上行抽象。

Conclusion: 该研究表明，通过集成RAG技术和网络抓取技术，可以减少将新信息纳入LLM的延迟，从而实现及时分析供应链中断。

Abstract: New technologies in generative AI can enable deeper analysis into our
nation's supply chains but truly informative insights require the continual
updating and aggregation of massive data in a timely manner. Large Language
Models (LLMs) offer unprecedented analytical opportunities however, their
knowledge base is constrained to the models' last training date, rendering
these capabilities unusable for organizations whose mission impacts rely on
emerging and timely information. This research proposes an innovative approach
to supply chain analysis by integrating emerging Retrieval-Augmented Generation
(RAG) preprocessing and retrieval techniques with advanced web-scraping
technologies. Our method aims to reduce latency in incorporating new
information into an augmented-LLM, enabling timely analysis of supply chain
disruptors. Through experimentation, this study evaluates the combinatorial
effects of these techniques towards timeliness and quality trade-offs. Our
results suggest that in applying RAG systems to supply chain analysis,
fine-tuning the embedding retrieval model consistently provides the most
significant performance gains, underscoring the critical importance of
retrieval quality. Adaptive iterative retrieval, which dynamically adjusts
retrieval depth based on context, further enhances performance, especially on
complex supply chain queries. Conversely, fine-tuning the LLM yields limited
improvements and higher resource costs, while techniques such as downward query
abstraction significantly outperforms upward abstraction in practice.

</details>


### [150] [ReFineG: Synergizing Small Supervised Models and LLMs for Low-Resource Grounded Multimodal NER](https://arxiv.org/abs/2509.10975)
*Jielong Tang,Shuang Wang,Zhenxing Wang,Jianxing Yu,Jian Yin*

Main category: cs.IR

TL;DR: 提出了一种名为ReFineG的三阶段协同框架，用于低资源GMNER，集成了小型监督模型和冻结的MLLM。


<details>
  <summary>Details</summary>
Motivation: 现有的GMNER监督方法依赖昂贵的多模态注释，并且在低资源领域表现不佳。MLLM显示出强大的泛化能力，但存在领域知识冲突，为特定领域的实体产生冗余或不正确的提及。

Method: 1. 领域感知NER数据合成策略，将LLM知识转移到具有监督训练的小型模型，同时避免领域知识冲突。2. 基于不确定性的机制，保留来自监督模型的置信预测，并将不确定的预测委托给MLLM。3. 多模态上下文选择算法，通过类比推理增强视觉基础。

Result: 在CCKS2025 GMNER共享任务中，ReFineG排名第二，在线排行榜上的F1得分为0.6461。

Conclusion: ReFineG在有限的注释下证明了其有效性。

Abstract: Grounded Multimodal Named Entity Recognition (GMNER) extends traditional NER
by jointly detecting textual mentions and grounding them to visual regions.
While existing supervised methods achieve strong performance, they rely on
costly multimodal annotations and often underperform in low-resource domains.
Multimodal Large Language Models (MLLMs) show strong generalization but suffer
from Domain Knowledge Conflict, producing redundant or incorrect mentions for
domain-specific entities. To address these challenges, we propose ReFineG, a
three-stage collaborative framework that integrates small supervised models
with frozen MLLMs for low-resource GMNER. In the Training Stage, a domain-aware
NER data synthesis strategy transfers LLM knowledge to small models with
supervised training while avoiding domain knowledge conflicts. In the
Refinement Stage, an uncertainty-based mechanism retains confident predictions
from supervised models and delegates uncertain ones to the MLLM. In the
Grounding Stage, a multimodal context selection algorithm enhances visual
grounding through analogical reasoning. In the CCKS2025 GMNER Shared Task,
ReFineG ranked second with an F1 score of 0.6461 on the online leaderboard,
demonstrating its effectiveness with limited annotations.

</details>


### [151] [Membership Inference Attacks on Recommender System: A Survey](https://arxiv.org/abs/2509.11080)
*Jiajie He,Yuechun Gu,Keke Chen,Xintong Chen*

Main category: cs.IR

TL;DR: 本文对推荐系统中的成员推理攻击 (MIA) 进行了全面的调查。


<details>
  <summary>Details</summary>
Motivation: 推荐系统容易受到 MIA 的攻击，可能导致隐私泄露。虽然 MIA 在其他机器学习任务中有效，但传统 MIA 不适合推荐系统。目前还没有关于推荐系统 MIA 的系统性调查。

Method: 本文对推荐系统 MIA 进行了全面的回顾，探讨了设计原则、挑战、攻击和防御，并提供了一个统一的分类法，根据其特征对不同的推荐系统 MIA 进行了分类，并讨论了它们的优缺点。

Result: 本文对推荐系统 MIA 进行了全面的调查，为研究界提供参考，并为该研究领域以外的研究人员提供了清晰的描述。

Conclusion: 本文基于调查中发现的局限性和差距，指出了未来有希望的研究方向，以启发希望跟踪该领域的研究人员。

Abstract: Recommender systems (RecSys) have been widely applied to various
applications, including E-commerce, finance, healthcare, social media and have
become increasingly influential in shaping user behavior and decision-making,
highlighting their growing impact in various domains. However, recent studies
have shown that RecSys are vulnerable to membership inference attacks (MIAs),
which aim to infer whether user interaction record was used to train a target
model or not. MIAs on RecSys models can directly lead to a privacy breach. For
example, via identifying the fact that a purchase record that has been used to
train a RecSys associated with a specific user, an attacker can infer that
user's special quirks. In recent years, MIAs have been shown to be effective on
other ML tasks, e.g., classification models and natural language processing.
However, traditional MIAs are ill-suited for RecSys due to the unseen posterior
probability. Although MIAs on RecSys form a newly emerging and rapidly growing
research area, there has been no systematic survey on this topic yet. In this
article, we conduct the first comprehensive survey on RecSys MIAs. This survey
offers a comprehensive review of the latest advancements in RecSys MIAs,
exploring the design principles, challenges, attack and defense associated with
this emerging field. We provide a unified taxonomy that categorizes different
RecSys MIAs based on their characterizations and discuss their pros and cons.
Based on the limitations and gaps identified in this survey, we point out
several promising future research directions to inspire the researchers who
wish to follow this area. This survey not only serves as a reference for the
research community but also provides a clear description for researchers
outside this research domain.

</details>


### [152] [SPARK: Adaptive Low-Rank Knowledge Graph Modeling in Hybrid Geometric Spaces for Recommendation](https://arxiv.org/abs/2509.11094)
*Binhao Wang,Yutian Xiao,Maolin Wang,Zhiqi Li,Tianshuo Wei,Ruocheng Guo,Xiangyu Zhao*

Main category: cs.IR

TL;DR: SPARK是一个用于知识图谱增强推荐系统的多阶段框架，通过降噪、混合几何GNN和自适应融合策略，显著提升长尾物品推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱推荐系统存在噪声、稀疏性和欧几里德几何局限性问题，尤其影响长尾实体的表示学习，且缺乏针对物品流行度的自适应多源信号融合。

Method: SPARK首先使用Tucker分解降噪知识图谱，然后用SVD初始化的混合几何GNN在欧几里德和双曲空间学习表示，并采用物品流行度感知的自适应融合策略，最后使用对比学习对齐多源表示。

Result: SPARK在长尾物品推荐方面显著优于现有方法。

Conclusion: SPARK为知识增强推荐提供了一种稳健且有原则的方法，尤其在改进长尾物品推荐方面表现出色。

Abstract: Knowledge Graphs (KGs) enhance recommender systems but face challenges from
inherent noise, sparsity, and Euclidean geometry's inadequacy for complex
relational structures, critically impairing representation learning, especially
for long-tail entities. Existing methods also often lack adaptive multi-source
signal fusion tailored to item popularity. This paper introduces SPARK, a novel
multi-stage framework systematically tackling these issues. SPARK first employs
Tucker low-rank decomposition to denoise KGs and generate robust entity
representations. Subsequently, an SVD-initialized hybrid geometric GNN
concurrently learns representations in Euclidean and Hyperbolic spaces; the
latter is strategically leveraged for its aptitude in modeling hierarchical
structures, effectively capturing semantic features of sparse, long-tail items.
A core contribution is an item popularity-aware adaptive fusion strategy that
dynamically weights signals from collaborative filtering, refined KG
embeddings, and diverse geometric spaces for precise modeling of both
mainstream and long-tail items. Finally, contrastive learning aligns these
multi-source representations. Extensive experiments demonstrate SPARK's
significant superiority over state-of-the-art methods, particularly in
improving long-tail item recommendation, offering a robust, principled approach
to knowledge-enhanced recommendation. Implementation code is available at
https://github.com/Applied-Machine-Learning-Lab/SPARK.

</details>


### [153] [Understanding the Information Cocoon: A Multidimensional Assessment and Analysis of News Recommendation Systems](https://arxiv.org/abs/2509.11139)
*Xin Wang,Xiaowen Huang,Jitao Sang*

Main category: cs.IR

TL;DR: 本研究提出了一个多维框架，用于评估个性化新闻推荐系统中的信息茧房问题，并设计了缓解策略。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化新闻推荐系统会造成信息茧房，加剧用户偏见和社会两极分化，并且缺乏全面的评估框架。

Method: 通过主题多样性（包括主题类别数量和类别信息熵）和点击重复来评估个体同质化；通过网络密度和社区开放性来评估群体极化。在真实数据集上进行了多轮实验，评估了七种算法。

Result: 揭示了关键见解，并设计了五种轻量级的缓解策略。

Conclusion: 本研究建立了首个统一的信息茧房度量框架，并为伦理推荐系统提供了可部署的解决方案。

Abstract: Personalized news recommendation systems inadvertently create information
cocoons--homogeneous information bubbles that reinforce user biases and amplify
societal polarization. To address the lack of comprehensive assessment
frameworks in prior research, we propose a multidimensional analysis that
evaluates cocoons through dual perspectives: (1) Individual homogenization via
topic diversity (including the number of topic categories and category
information entropy) and click repetition; (2) Group polarization via network
density and community openness. Through multi-round experiments on real-world
datasets, we benchmark seven algorithms and reveal critical insights.
Furthermore, we design five lightweight mitigation strategies. This work
establishes the first unified metric framework for information cocoons and
delivers deployable solutions for ethical recommendation systems.

</details>


### [154] [Do Large Language Models Favor Recent Content? A Study on Recency Bias in LLM-Based Reranking](https://arxiv.org/abs/2509.11353)
*Hanpei Fang,Sijie Tao,Nuo Chen,Kai-Xin Chang,Tetsuya Sakai*

Main category: cs.IR

TL;DR: 大型语言模型（LLM）在信息系统中越来越多地被部署，包括用作信息检索管道中的第二阶段重排序器，但它们对近因偏差的敏感性却很少受到关注。本研究调查了LLM是否会隐式地偏爱较新的文档，通过在TREC深度学习通道检索集合中添加人工发布日期。


<details>
  <summary>Details</summary>
Motivation: 研究LLM是否会隐式地偏爱较新的文档。

Method: 通过在TREC深度学习通道检索集合中添加人工发布日期，观察LLM的排序结果。

Result: 新鲜的通道始终被提升，使前10名的平均出版年份提前最多4.78年，并在我们的列表式重排序实验中将单个项目移动了95个排名。在成对偏好实验中，注入日期后，LLM在两个具有相同相关性级别的通道之间的偏好可以平均逆转高达25%。

Conclusion: 这些发现为LLM中普遍存在的近因偏差提供了定量证据，并强调了有效的偏差缓解策略的重要性。

Abstract: Large language models (LLMs) are increasingly deployed in information
systems, including being used as second-stage rerankers in information
retrieval pipelines, yet their susceptibility to recency bias has received
little attention. We investigate whether LLMs implicitly favour newer documents
by prepending artificial publication dates to passages in the TREC Deep
Learning passage retrieval collections in 2021 (DL21) and 2022 (DL22). Across
seven models, GPT-3.5-turbo, GPT-4o, GPT-4, LLaMA-3 8B/70B, and Qwen-2.5
7B/72B, "fresh" passages are consistently promoted, shifting the Top-10's mean
publication year forward by up to 4.78 years and moving individual items by as
many as 95 ranks in our listwise reranking experiments. Although larger models
attenuate the effect, none eliminate it. We also observe that the preference of
LLMs between two passages with an identical relevance level can be reversed by
up to 25% on average after date injection in our pairwise preference
experiments. These findings provide quantitative evidence of a pervasive
recency bias in LLMs and highlight the importance of effective bias-mitigation
strategies.

</details>


### [155] [Decoding in Latent Spaces for Efficient Inference in LLM-based Recommendation](https://arxiv.org/abs/2509.11524)
*Chengbing Wang,Yang Zhang,Zhicheng Wang,Tianhao Shi,Keqin Bao,Fuli Feng,Tat-Seng Chua*

Main category: cs.IR

TL;DR: 提出了一种新的方法来绕过语言空间解码，通过在潜在空间中直接匹配候选项目和LLM的内部思想表示，从而减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）以生成方式进行推荐的微调已经取得了有希望的结果，但由于语言空间中的自回归解码，遇到了显著的推理开销。

Method: 提出了Light Latent-space Decoding (L2D)，一种有效的潜在空间解码方法。L2D通过使用反映LLM内部思想的测试序列的隐藏状态来表示用户偏好的项目，并从标有相应候选项目的训练序列的隐藏状态中获得候选项目表示。然后，它匹配这两种类型的表示来解码项目，从而实现潜在空间解码。

Result: 大量的经验结果表明，L2D比语言空间解码快10倍以上，同时保持或提高性能。

Conclusion: L2D能够在不改变LLM的生成调整范式的情况下实现高效解码，从而保持性能。

Abstract: Fine-tuning large language models (LLMs) for recommendation in a generative
manner has delivered promising results, but encounters significant inference
overhead due to autoregressive decoding in the language space. This work
explores bypassing language-space decoding by directly matching candidate items
with the LLM's internal thought representations in the latent space,
eliminating the time-consuming autoregressive process to reduce computational
costs. Towards this, we introduce Light Latent-space Decoding (L2D), an
effective and efficient latent-space decoding method. L2D represents
user-preferred items by using the hidden states of test sequences reflecting
the LLM's internal thought, and obtains candidate item representations from the
hidden states of training sequences labeled with the corresponding candidate
items. It then matches the two types of representations to decode items,
achieving latent-space decoding. In this way, it enables efficient decoding
without altering the LLM's generative tuning paradigm, thereby preserving
performance. Extensive empirical results demonstrate that L2D is more than 10x
faster than language-space decoding while maintaining or enhancing performance.

</details>


### [156] [Data-Driven Analysis of Text-Conditioned AI-Generated Music: A Case Study with Suno and Udio](https://arxiv.org/abs/2509.11824)
*Luca Casini,Laura Cros Vila,David Dalmazzo,Anna-Kaisa Kaila,Bob L. T. Sturm*

Main category: cs.IR

TL;DR: 分析了Suno和Udio两个AI音乐平台的用户生成歌曲，时间范围是2024年5月至10月。


<details>
  <summary>Details</summary>
Motivation: 了解AI音乐平台的使用方式以及用户的主题灵感。

Method: 使用文本嵌入模型、降维和聚类方法分析提示、标签和歌词，并自动注释和展示数据。

Result: 揭示了歌词中的突出主题、语言偏好、提示策略以及通过元标签引导模型的尝试。

Conclusion: 为了促进对AI生成音乐的文化实践的音乐学研究，分享了代码和资源。

Abstract: Online AI platforms for creating music from text prompts (AI music), such as
Suno and Udio, are now being used by hundreds of thousands of users. Some AI
music is appearing in advertising, and even charting, in multiple countries.
How are these platforms being used? What subjects are inspiring their users?
This article answers these questions for Suno and Udio using a large collection
of songs generated by users of these platforms from May to October 2024. Using
a combination of state-of-the-art text embedding models, dimensionality
reduction and clustering methods, we analyze the prompts, tags and lyrics, and
automatically annotate and display the processed data in interactive plots. Our
results reveal prominent themes in lyrics, language preference, prompting
strategies, as well as peculiar attempts at steering models through the use of
metatags. To promote the musicological study of the developing cultural
practice of AI-generated music we share our code and resources.

</details>


### [157] [AEFS: Adaptive Early Feature Selection for Deep Recommender Systems](https://arxiv.org/abs/2509.12076)
*Fan Hu,Gaofeng Lu,Jun Chen,Chaonan Guo,Yuekui Yang,Xirong Li*

Main category: cs.IR

TL;DR: 提出了一种新的自适应早期特征选择（AEFS）方法，该方法在保持性能的同时，显著减少了嵌入层的激活参数。


<details>
  <summary>Details</summary>
Motivation: 现有自适应后期特征选择方法在大型推荐系统中效率低下，因为它们需要处理稀疏且参数庞大的嵌入层。

Method: AEFS 采用双模型架构，包含一个用于特征选择的辅助模型和一个用于预测的主模型，并通过两个协同训练损失约束来确保两个模型之间的有效对齐。

Result: 在三个基准数据集上的实验表明，AEFS 在匹配当前最先进的自适应后期特征选择方法性能的同时，显著减少了 37.5% 的嵌入层激活参数。

Conclusion: AEFS 是一种有效的特征选择方法，可以在大型推荐系统中提高效率并减少计算成本。

Abstract: Feature selection has emerged as a crucial technique in refining recommender
systems. Recent advancements leveraging Automated Machine Learning (AutoML) has
drawn significant attention, particularly in two main categories: early feature
selection and late feature selection, differentiated by whether the selection
occurs before or after the embedding layer. The early feature selection selects
a fixed subset of features and retrains the model, while the late feature
selection, known as adaptive feature selection, dynamically adjusts feature
choices for each data instance, recognizing the variability in feature
significance. Although adaptive feature selection has shown remarkable
improvements in performance, its main drawback lies in its post-embedding layer
feature selection. This process often becomes cumbersome and inefficient in
large-scale recommender systems with billions of ID-type features, leading to a
highly sparse and parameter-heavy embedding layer. To overcome this, we
introduce Adaptive Early Feature Selection (AEFS), a very simple method that
not only adaptively selects informative features for each instance, but also
significantly reduces the activated parameters of the embedding layer. AEFS
employs a dual-model architecture, encompassing an auxiliary model dedicated to
feature selection and a main model responsible for prediction. To ensure
effective alignment between these two models, we incorporate two collaborative
training loss constraints. Our extensive experiments on three benchmark
datasets validate the efficiency and effectiveness of our approach. Notably,
AEFS matches the performance of current state-of-theart Adaptive Late Feature
Selection methods while achieving a significant reduction of 37. 5% in the
activated parameters of the embedding layer. AEFS is open-source at
https://github. com/fly-dragon211/AEFS .

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [158] [The 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real): Methods and Results](https://arxiv.org/abs/2509.10463)
*Qiuyu Chen,Xin Jin,Yue Song,Xihui Liu,Shuai Yang,Tao Yang,Ziqiang Li,Jianguo Huang,Yuntao Wei,Ba'ao Xie,Nicu Sebe,Wenjun,Zeng,Jooyeol Yun,Davide Abati,Mohamed Omran,Jaegul Choo,Amir Habibian,Auke Wiggers,Masato Kobayashi,Ning Ding,Toru Tamaki,Marzieh Gheisari,Auguste Genovesio,Yuheng Chen,Dingkun Liu,Xinyao Yang,Xinping Xu,Baicheng Chen,Dongrui Wu,Junhao Geng,Lexiang Lv,Jianxin Lin,Hanzhe Liang,Jie Zhou,Xuanxin Chen,Jinbao Wang,Can Gao,Zhangyi Wang,Zongze Li,Bihan Wen,Yixin Gao,Xiaohan Pan,Xin Li,Zhibo Chen,Baorui Peng,Zhongming Chen,Haoran Jin*

Main category: cs.LG

TL;DR: 本次研讨会旨在弥合解耦表征学习 (DRL) 的理论前景与其在实际场景中的应用之间的差距，超越合成基准。


<details>
  <summary>Details</summary>
Motivation: 探索模型在可控生成等实际应用中的鲁棒性、可解释性和泛化能力的进步。

Method:  workshop 接受了 9 篇论文，涵盖了广泛的主题，包括整合新的归纳偏置（例如，语言）、将扩散模型应用于 DRL、3D 感知解耦以及将 DRL 扩展到自动驾驶和脑电图分析等专门领域。

Result: 总结详细介绍了研讨会的目标、已接受论文的主题，并概述了作者提出的方法。

Conclusion: 本次研讨会旨在弥合解耦表征学习的理论与实际应用之间的差距，并探索其在实际场景中的应用。

Abstract: This paper reviews the 1st International Workshop on Disentangled
Representation Learning for Controllable Generation (DRL4Real), held in
conjunction with ICCV 2025. The workshop aimed to bridge the gap between the
theoretical promise of Disentangled Representation Learning (DRL) and its
application in realistic scenarios, moving beyond synthetic benchmarks.
DRL4Real focused on evaluating DRL methods in practical applications such as
controllable generation, exploring advancements in model robustness,
interpretability, and generalization. The workshop accepted 9 papers covering a
broad range of topics, including the integration of novel inductive biases
(e.g., language), the application of diffusion models to DRL, 3D-aware
disentanglement, and the expansion of DRL into specialized domains like
autonomous driving and EEG analysis. This summary details the workshop's
objectives, the themes of the accepted papers, and provides an overview of the
methodologies proposed by the authors.

</details>


### [159] [Moment Estimates and DeepRitz Methods on Learning Diffusion Systems with Non-gradient Drifts](https://arxiv.org/abs/2509.10495)
*Fanze Kong,Chen-Chih Lai,Yubin Lu*

Main category: cs.LG

TL;DR: 本文提出了一种数据驱动的两阶段方法，用于学习广义扩散系统中漂移分解的保守-耗散动力学。


<details>
  <summary>Details</summary>
Motivation: 研究复杂开放系统中普遍存在的保守-耗散动力学。

Method: 矩深度里兹方法（Moment-DeepRitz Method）。

Result: 该方法对噪声数据具有鲁棒性，适用于粗糙势和振荡旋转。

Conclusion: 通过数值实验证明了该方法的有效性。

Abstract: Conservative-dissipative dynamics are ubiquitous across a variety of complex
open systems. We propose a data-driven two-phase method, the Moment-DeepRitz
Method, for learning drift decompositions in generalized diffusion systems
involving conservative-dissipative dynamics. The method is robust to noisy
data, adaptable to rough potentials and oscillatory rotations. We demonstrate
its effectiveness through several numerical experiments.

</details>


### [160] [SOH-KLSTM: A Hybrid Kolmogorov-Arnold Network and LSTM Model for Enhanced Lithium-Ion Battery Health Monitoring](https://arxiv.org/abs/2509.10496)
*Imen Jarraya,Safa Ben Atitallah,Fatimah Alahmeda,Mohamed Abdelkadera,Maha Drissa,Fatma Abdelhadic,Anis Koubaaa*

Main category: cs.LG

TL;DR: 提出了一种新的锂电池健康状态 (SOH) 预测框架 (SOH-KLSTM)，该框架使用 Kolmogorov-Arnold 网络 (KAN) 集成的 LSTM 候选细胞状态来进行锂电池健康监测。


<details>
  <summary>Details</summary>
Motivation: 精确可靠的锂电池健康状态 (SOH) 估计对于确保电动汽车、无人驾驶飞行器、消费电子产品和可再生能源存储系统等应用的寿命、安全性和最佳性能至关重要。传统的 SOH 估计技术无法有效地表示电池退化的非线性和时间特性。

Method: 使用 Kolmogorov-Arnold 网络 (KAN) 集成的 LSTM 候选细胞状态，提出了一种新的 SOH 预测框架 (SOH-KLSTM)。

Result: 未提及

Conclusion: 该混合方法结合了 LSTM 学习长期依赖关系以进行准确时间序列预测的能力，以及 KAN 的非线性逼近能力，以有效捕获锂电池中复杂的退化行为。

Abstract: Accurate and reliable State Of Health (SOH) estimation for Lithium (Li)
batteries is critical to ensure the longevity, safety, and optimal performance
of applications like electric vehicles, unmanned aerial vehicles, consumer
electronics, and renewable energy storage systems. Conventional SOH estimation
techniques fail to represent the non-linear and temporal aspects of battery
degradation effectively. In this study, we propose a novel SOH prediction
framework (SOH-KLSTM) using Kolmogorov-Arnold Network (KAN)-Integrated
Candidate Cell State in LSTM for Li batteries Health Monitoring. This hybrid
approach combines the ability of LSTM to learn long-term dependencies for
accurate time series predictions with KAN's non-linear approximation
capabilities to effectively capture complex degradation behaviors in Lithium
batteries.

</details>


### [161] [Exploring Multi-view Symbolic Regression methods in physical sciences](https://arxiv.org/abs/2509.10500)
*Etienne Russeil,Fabrício Olivetti de França,Konstantin Malanchev,Guillaume Moinard,Maxime Cherrey*

Main category: cs.LG

TL;DR: 本文对多视角符号回归（MvSR）在Operon、PySR、phy-SO和eggp等不同实现中的性能进行了测试和比较，发现它们在实现良好准确性的同时，通常能提出参数较少的解决方案。研究还发现了一些能够更频繁地生成更好模型的特征，并为未来的MvSR发展提供了指导。


<details>
  <summary>Details</summary>
Motivation: 通过数学函数描述世界有助于科学家更好地理解不同现象的内在机制。传统方法是从第一原理和细致观察中推导出新方程，而现代方法是使用符号回归（SR）自动化部分过程。MvSR是一种有趣的扩展，它寻找能够描述同一现象生成的多个数据集的参数函数，从而有助于缓解过拟合和数据稀缺的常见问题。

Method: 本文在不同的真实世界数据集上测试和比较了Operon、PySR、phy-SO和eggp中支持的MvSR。

Result: 所有 tested MvSR 方法通常都能实现良好的准确性，同时提出参数较少的解决方案。某些特征能够更频繁地生成更好的模型。

Conclusion: 本文为未来的MvSR发展提供了指导。

Abstract: Describing the world behavior through mathematical functions help scientists
to achieve a better understanding of the inner mechanisms of different
phenomena. Traditionally, this is done by deriving new equations from first
principles and careful observations. A modern alternative is to automate part
of this process with symbolic regression (SR). The SR algorithms search for a
function that adequately fits the observed data while trying to enforce
sparsity, in the hopes of generating an interpretable equation. A particularly
interesting extension to these algorithms is the Multi-view Symbolic Regression
(MvSR). It searches for a parametric function capable of describing multiple
datasets generated by the same phenomena, which helps to mitigate the common
problems of overfitting and data scarcity. Recently, multiple implementations
added support to MvSR with small differences between them. In this paper, we
test and compare MvSR as supported in Operon, PySR, phy-SO, and eggp, in
different real-world datasets. We show that they all often achieve good
accuracy while proposing solutions with only few free parameters. However, we
find that certain features enable a more frequent generation of better models.
We conclude by providing guidelines for future MvSR developments.

</details>


### [162] [From Noise to Precision: A Diffusion-Driven Approach to Zero-Inflated Precipitation Prediction](https://arxiv.org/abs/2509.10501)
*Wentao Gao,Jiuyong Li,Lin Liu,Thuc Duy Le,Xiongren Chen,Xiaojing Du,Jixue Liu,Yanchang Zhao,Yun Chen*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的降水预测框架ZIDF，用于解决零膨胀数据带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于降水预测中存在大量零值和稀疏的非零事件，零膨胀数据对降水预测提出了重大挑战。

Method: 该框架结合了高斯扰动平滑零膨胀分布、基于Transformer的预测捕捉时间模式以及基于扩散的去噪恢复原始数据结构。

Result: 实验结果表明，ZIDF比多个最先进的降水预测模型有显著的性能改进，相对于基线非平稳Transformer，MSE降低了高达56.7%，MAE降低了21.1%。

Conclusion: ZIDF能够稳健地处理稀疏时间序列数据，并具有推广到其他零膨胀是关键挑战的领域的潜力。

Abstract: Zero-inflated data pose significant challenges in precipitation forecasting
due to the predominance of zeros with sparse non-zero events. To address this,
we propose the Zero Inflation Diffusion Framework (ZIDF), which integrates
Gaussian perturbation for smoothing zero-inflated distributions,
Transformer-based prediction for capturing temporal patterns, and
diffusion-based denoising to restore the original data structure. In our
experiments, we use observational precipitation data collected from South
Australia along with synthetically generated zero-inflated data. Results show
that ZIDF demonstrates significant performance improvements over multiple
state-of-the-art precipitation forecasting models, achieving up to 56.7\%
reduction in MSE and 21.1\% reduction in MAE relative to the baseline
Non-stationary Transformer. These findings highlight ZIDF's ability to robustly
handle sparse time series data and suggest its potential generalizability to
other domains where zero inflation is a key challenge.

</details>


### [163] [FEDEXCHANGE: Bridging the Domain Gap in Federated Object Detection for Free](https://arxiv.org/abs/2509.10503)
*Haolin Yuan,Jingtao Li,Weiming Zhuang,Chen Chen,Lingjuan Lyu*

Main category: cs.LG

TL;DR: 提出了一种新的联邦对象检测框架，称为FEDEXCHANGE，它通过服务器端的动态模型交换策略来弥合领域差距，而无需引入额外的本地计算开销。


<details>
  <summary>Details</summary>
Motivation: 跨域联邦对象检测由于环境、天气和其他领域特定因素的显著差异而面临泛化挑战，并且现有方法忽略了边缘设备的硬件约束，引入了高计算成本的本地训练正则化。

Method: FEDEXCHANGE采用服务器端动态模型交换策略，允许服务器在模型聚合和模型交换之间交替。在交换轮次中，FEDEXCHANGE基于距离度量对本地模型进行聚类和交换，从而允许本地模型从各种领域学习。

Result: FEDEXCHANGE提高了联邦对象检测的性能，在雨天等具有挑战性的领域中实现了1.6倍更好的平均精度，同时与基线方法相比，仅需要0.8倍的计算资源。

Conclusion: FEDEXCHANGE通过服务器端操作，使客户端能够在不增加额外计算开销的情况下，提高跨域利用率。

Abstract: Federated Object Detection (FOD) enables clients to collaboratively train a
global object detection model without accessing their local data from diverse
domains. However, significant variations in environment, weather, and other
domain specific factors hinder performance, making cross domain generalization
a key challenge. Existing FOD methods often overlook the hardware constraints
of edge devices and introduce local training regularizations that incur high
computational costs, limiting real-world applicability. In this paper, we
propose FEDEXCHANGE, a novel FOD framework that bridges domain gaps without
introducing additional local computational overhead. FEDEXCHANGE employs a
server side dynamic model exchange strategy that enables each client to gain
insights from other clients' domain data without direct data sharing.
Specifically, FEDEXCHANGE allows the server to alternate between model
aggregation and model exchange. During aggregation rounds, the server
aggregates all local models as usual. In exchange rounds, FEDEXCHANGE clusters
and exchanges local models based on distance measures, allowing local models to
learn from a variety of domains. As all operations are performed on the server
side, clients can achieve improved cross domain utility without any additional
computational overhead. Extensive evaluations demonstrate that FEDEXCHANGE
enhances FOD performance, achieving 1.6X better mean average precision in
challenging domains, such as rainy conditions, while requiring only 0.8X the
computational resources compared to baseline methods.

</details>


### [164] [Retrosynthesis Planning via Worst-path Policy Optimisation in Tree-structured MDPs](https://arxiv.org/abs/2509.10504)
*Mianchu Wang,Giovanni Montana*

Main category: cs.LG

TL;DR: 论文提出了一种新的逆合成规划方法，该方法将逆合成视为树状马尔可夫决策过程中的最坏路径优化问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常优化分支的平均性能，而未能解决合成路线中“最薄弱环节”的敏感性问题。

Method: 论文提出了一种名为交互式逆合成规划 (InterRetro) 的方法，该方法与树 MDP 交互，学习最坏路径结果的价值函数，并通过自我模仿来改进其策略，优先强化过去具有高估计优势的决策。

Result: InterRetro 在 Retro*-190 基准测试中解决了 100% 的目标，将合成路线缩短了 4.9%，并且仅使用 10% 的训练数据就实现了有希望的性能。

Conclusion: 该方法代表了计算逆合成规划的重大进展。

Abstract: Retrosynthesis planning aims to decompose target molecules into available
building blocks, forming a synthesis tree where each internal node represents
an intermediate compound and each leaf ideally corresponds to a purchasable
reactant. However, this tree becomes invalid if any leaf node is not a valid
building block, making the planning process vulnerable to the "weakest link" in
the synthetic route. Existing methods often optimise for average performance
across branches, failing to account for this worst-case sensitivity. In this
paper, we reframe retrosynthesis as a worst-path optimisation problem within
tree-structured Markov Decision Processes (MDPs). We prove that this
formulation admits a unique optimal solution and offers monotonic improvement
guarantees. Building on this insight, we introduce Interactive Retrosynthesis
Planning (InterRetro), a method that interacts with the tree MDP, learns a
value function for worst-path outcomes, and improves its policy through
self-imitation, preferentially reinforcing past decisions with high estimated
advantage. Empirically, InterRetro achieves state-of-the-art results, solving
100% of targets on the Retro*-190 benchmark, shortening synthetic routes by
4.9%, and achieving promising performance using only 10% of the training data -
representing a significant advance in computational retrosynthesis planning.

</details>


### [165] [AttnBoost: Retail Supply Chain Sales Insights via Gradient Boosting Perspective](https://arxiv.org/abs/2509.10506)
*Muxin Ge,Hanyu Ma,Yiyang Wu,Xiaoli Ma,Yadi Liu,Ye Aung Moe,Weizheng Xie*

Main category: cs.LG

TL;DR: 本文提出了一种名为AttnBoost的可解释学习框架，通过将特征级别的注意力机制整合到boosting过程中，以提高预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 由于零售供应链中嘈杂、异构的特征和快速变化的消费者行为，预测产品需求是一个复杂的挑战。传统的梯度提升决策树(GBDT)在结构化数据上提供了强大的预测性能，但它们通常缺乏自适应机制来识别和强调变化条件下最相关的特征。

Method: 该模型在每个boosting循环中通过一个轻量级的注意力机制动态调整特征重要性，使其能够专注于促销、定价和季节性趋势等高影响变量。

Result: 在大型零售销售数据集上的评估表明，AttnBoost优于标准的机器学习和深度表格模型，同时为供应链管理人员提供了可操作的见解。消融研究证实了注意力模块在减轻过拟合和提高可解释性方面的效用。

Conclusion: 研究结果表明，注意力引导的boosting代表了现实世界预测应用中可解释和可扩展人工智能的一个有希望的方向。

Abstract: Forecasting product demand in retail supply chains presents a complex
challenge due to noisy, heterogeneous features and rapidly shifting consumer
behavior. While traditional gradient boosting decision trees (GBDT) offer
strong predictive performance on structured data, they often lack adaptive
mechanisms to identify and emphasize the most relevant features under changing
conditions. In this work, we propose AttnBoost, an interpretable learning
framework that integrates feature-level attention into the boosting process to
enhance both predictive accuracy and explainability. Specifically, the model
dynamically adjusts feature importance during each boosting round via a
lightweight attention mechanism, allowing it to focus on high-impact variables
such as promotions, pricing, and seasonal trends. We evaluate AttnBoost on a
large-scale retail sales dataset and demonstrate that it outperforms standard
machine learning and deep tabular models, while also providing actionable
insights for supply chain managers. An ablation study confirms the utility of
the attention module in mitigating overfitting and improving interpretability.
Our results suggest that attention-guided boosting represents a promising
direction for interpretable and scalable AI in real-world forecasting
applications.

</details>


### [166] [The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models from Recursive Selective Feedback](https://arxiv.org/abs/2509.10509)
*Sai Teja Reddy Adapala*

Main category: cs.LG

TL;DR: 本文研究了递归训练的大型语言模型 (LLM) 的稳定性问题，发现选择性反馈机制可以逆转模型退化，提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前理论预测模型在用自身输出进行训练时会发生模型崩溃。本文旨在挑战这一观点。

Method: 通过引入选择性反馈机制，并在 Gemma 2B 模型上进行实验。

Result: 实验表明，选择性反馈机制可以逆转模型退化，使模型在复杂摘要任务上的性能得到显著提高。在五个代际中，质量过滤条件下的 ROUGE-L F1 得分提高了 6.6%，而未过滤的对照组下降了 3.5%，随机过滤的对照组下降了 4.2%。

Conclusion: 研究结果表明，系统弹性可能是 LLM 在简单选择压力下涌现的属性，这为开发更安全、更强大的 AI 系统提供了一个有力的、可扩展的原则。

Abstract: The stability of recursively trained large language models (LLMs) is a
foundational problem for AI safety. Prevailing theory predicts model collapse,
a progressive degradation when models are trained on their own output. We
challenge this narrative by introducing a selective feedback mechanism.
Contrary to expectation, instead of merely slowing decay, our experiments
provide strong evidence that this pressure reverses it, inducing a
statistically significant performance improvement in a Gemma 2B model on a
complex summarization task. We name this phenomenon the Anti-Ouroboros Effect.
We contrast this with a foundational experiment using a simple classifier,
where the theoretical degenerative loop was validated, highlighting the unique
dynamics of high-dimensional models. Our findings establish that systemic
resilience can be an emergent property of LLMs under simple selection pressure,
suggesting a powerful and scalable principle for developing safer and more
robust AI systems. Across five generations, a quality-filtered condition
improved by 6.6% in ROUGE-L F1 score, whereas an unfiltered control degraded by
3.5% and a random-filter control degraded by 4.2%

</details>


### [167] [LogGuardQ: A Cognitive-Enhanced Reinforcement Learning Framework for Cybersecurity Anomaly Detection in Security Logs](https://arxiv.org/abs/2509.10511)
*Umberto Gonçalves de Sousa*

Main category: cs.LG

TL;DR: LogGuardQ，一种新型强化学习框架，通过结合认知科学和强化学习，在动态环境中实现了高效、稳定和自适应的学习。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法在动态环境中探索效率、稳定性和适应性方面存在困难。LogGuardQ旨在解决这些问题。

Method: LogGuardQ整合了受人类认知启发的双记忆系统和由温度衰减和好奇心驱动的自适应探索策略。

Result: LogGuardQ在模拟访问日志数据集上实现了96.0%的检测率，精度为0.4776，召回率为0.9996，F1-score为0.6450，平均步数为5.0。统计测试证实了其性能优势。

Conclusion: LogGuardQ通过桥接认知科学和强化学习，为不确定环境中的自适应学习提供了一种可扩展的方法，在网络安全、入侵检测和不确定性下的决策方面具有潜在的应用。

Abstract: Reinforcement learning (RL) has transformed sequential decision-making, but
traditional algorithms like Deep Q-Networks (DQNs) and Proximal Policy
Optimization (PPO) often struggle with efficient exploration, stability, and
adaptability in dynamic environments. This study presents LogGuardQ (Adaptive
Log Guard with Cognitive enhancement), a novel framework that integrates a
dual-memory system inspired by human cognition and adaptive exploration
strategies driven by temperature decay and curiosity. Evaluated on a dataset of
1,000,000 simulated access logs with 47.9% anomalies over 20,000 episodes,
LogGuardQ achieves a 96.0% detection rate (versus 93.0% for DQN and 47.1% for
PPO), with precision of 0.4776, recall of 0.9996, and an F1-score of 0.6450.
The mean reward is 20.34 \pm 44.63 across all episodes (versus 18.80 \pm 43.98
for DQN and -0.17 \pm 23.79 for PPO), with an average of 5.0 steps per episode
(constant across models). Graphical analyses, including learning curves
smoothed with a Savgol filter (window=501, polynomial=2), variance trends,
action distributions, and cumulative detections, demonstrate LogGuardQ's
superior stability and efficiency. Statistical tests (Mann-Whitney U) confirm
significant performance advantages (e.g., p = 0.0002 vs. DQN with negligible
effect size, p < 0.0001 vs. PPO with medium effect size, and p < 0.0001 for DQN
vs. PPO with small effect size). By bridging cognitive science and RL,
LogGuardQ offers a scalable approach to adaptive learning in uncertain
environments, with potential applications in cybersecurity, intrusion
detection, and decision-making under uncertainty.

</details>


### [168] [A Service-Oriented Adaptive Hierarchical Incentive Mechanism for Federated Learning](https://arxiv.org/abs/2509.10512)
*Jiaxing Cao,Yuzhou Gao,Jiwei Huang*

Main category: cs.LG

TL;DR: 本文提出了一种自适应激励机制，旨在最大化联邦学习中任务发布者、本地模型所有者和worker的效用。


<details>
  <summary>Details</summary>
Motivation: 联邦学习有时会面临训练数据不足的问题，因此需要招募工人来收集数据。

Method: 本文从服务导向的角度出发，建立了任务发布者和本地模型所有者之间的Stackelberg博弈，并采用多智能体马尔可夫决策过程来描述本地模型所有者和工人之间的交互。

Result: 通过大量的数值实验验证了该方法的有效性。

Conclusion: 设计了一种自适应搜索最优策略算法，以稳定各参与者的策略并解决耦合问题。

Abstract: Recently, federated learning (FL) has emerged as a novel framework for
distributed model training. In FL, the task publisher (TP) releases tasks, and
local model owners (LMOs) use their local data to train models. Sometimes, FL
suffers from the lack of training data, and thus workers are recruited for
gathering data. To this end, this paper proposes an adaptive incentive
mechanism from a service-oriented perspective, with the objective of maximizing
the utilities of TP, LMOs and workers. Specifically, a Stackelberg game is
theoretically established between the LMOs and TP, positioning TP as the leader
and the LMOs as followers. An analytical Nash equilibrium solution is derived
to maximize their utilities. The interaction between LMOs and workers is
formulated by a multi-agent Markov decision process (MAMDP), with the optimal
strategy identified via deep reinforcement learning (DRL). Additionally, an
Adaptively Searching the Optimal Strategy Algorithm (ASOSA) is designed to
stabilize the strategies of each participant and solve the coupling problems.
Extensive numerical experiments are conducted to validate the efficacy of the
proposed method.

</details>


### [169] [Mixture-of-Clustered-Experts: Advancing Expert Specialization and Generalization in Instruction Tuning](https://arxiv.org/abs/2509.10513)
*Sugyeong Eo,Jungjun Lee,Chanjun Park,Heuiseok Lim*

Main category: cs.LG

TL;DR: 提出了一种新的MoE架构MoCE，通过双阶段路由机制，先按序列级特征进行专家组路由，再在组内激活top-k专家，以提升模型性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: MoE模型在处理输入异构性大的instruction tuning场景时，专家 специализация 不足，影响性能和泛化能力。

Method: 提出Mixture-of-Clustered-Experts (MoCE)，使用双阶段路由机制：序列级专家组路由和token级top-k专家激活。

Result: MoCE在多个benchmark上优于现有模型，并具有更强的泛化能力。

Conclusion: MoCE具有鲁棒性和有效性。

Abstract: A sparse Mixture-of-Experts (MoE) architecture has emerged as a highly
scalable solution by conditionally activating sub-modules without a
proportional increase in computational costs. However, improving expert
specialization to enhance performance and generalization remains a challenge
for MoE, especially in instruction tuning scenarios characterized by
significant input heterogeneity. In this work, we propose the
Mixture-of-Clustered-Experts (MoCE) to address this limitation through a
dual-stage routing mechanism. The first stage in the mechanism performs expert
group routing based on sequence-level features, while the second stage
activates the top-$k$ experts within the group at the token level. This
approach enables the effective partitioning of heterogeneous inputs based on
their knowledge requirements, encouraging expert group specialization while
maintaining the advantages of token-level routing. We evaluate MoCE across a
comprehensive set of benchmarks, demonstrating its consistent superiority over
strong baselines and its enhanced generalization capabilities. Detailed
analysis further highlights the robustness and effectiveness of MoCE.

</details>


### [170] [A Differential Manifold Perspective and Universality Analysis of Continuous Attractors in Artificial Neural Networks](https://arxiv.org/abs/2509.10514)
*Shaoxin Tian,Hongkai Liu,Yuying Yang,Jiali Yu,Zizheng Miao,Xuming Huang,Zhishuai Liu,Zhang Yi*

Main category: cs.LG

TL;DR: 本研究提出了一个统一的框架，用于分析人工神经网络中连续吸引子的性质。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏统一的框架来分析不同动态系统中连续吸引子的性质，限制了跨架构的泛化能力。

Method: 从微分流形的角度研究人工神经网络中的连续吸引子。

Result: 验证了与先前结论的兼容性，阐明了连续吸引子现象与局部雅可比矩阵的特征值之间的联系，并证明了常见分类模型和数据集中奇异值分层的普遍性。

Conclusion: 连续吸引子可能普遍存在于通用神经网络中，提出的框架提供了一个有希望的基础，因为它揭示了特征值和奇异值之间存在密切的数学联系。

Abstract: Continuous attractors are critical for information processing in both
biological and artificial neural systems, with implications for spatial
navigation, memory, and deep learning optimization. However, existing research
lacks a unified framework to analyze their properties across diverse dynamical
systems, limiting cross-architectural generalizability. This study establishes
a novel framework from the perspective of differential manifolds to investigate
continuous attractors in artificial neural networks. It verifies compatibility
with prior conclusions, elucidates links between continuous attractor phenomena
and eigenvalues of the local Jacobian matrix, and demonstrates the universality
of singular value stratification in common classification models and datasets.
These findings suggest continuous attractors may be ubiquitous in general
neural networks, highlighting the need for a general theory, with the proposed
framework offering a promising foundation given the close mathematical
connection between eigenvalues and singular values.

</details>


### [171] [Adaptive Preference Optimization with Uncertainty-aware Utility Anchor](https://arxiv.org/abs/2509.10515)
*Xiaobo Wang,Zixia Jia,Jiaqi Li,Qi Liu,Zilong Zheng*

Main category: cs.LG

TL;DR: 提出了一种新的离线偏好优化框架UAPO，通过引入锚定函数来估计偏好数据标注带来的不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有的DPO方法依赖于Bradley-Terry (BT) 奖励建模，存在需要成对训练数据、模型分布偏移和人类理性假设等局限性。

Method: 提出了一个通用的离线偏好优化框架，名为Adaptive Preference Optimization with Utility Anchor (UAPO)，它引入了一个锚定函数来估计偏好数据标注中的不确定性。

Result: 实验结果表明，UAPO在不严格依赖数据配对的情况下，实现了有竞争力的结果。

Conclusion: UAPO为更灵活、有效的偏好优化方法铺平了道路，因为它不依赖于成对数据，并且在训练过程中更鲁棒。

Abstract: Offline preference optimization methods are efficient for large language
models (LLMs) alignment. Direct Preference optimization (DPO)-like learning,
one of the most popular approaches, stands out for its efficiency in reward
modeling. However, these methods typically follow the convention to use
Bradley-Terry (BT) reward modeling that faces several critical assumptions,
including the requirement for pairwise training data, model distribution
shifting, human rationality assumption, etc. To address these limitations, we
propose a general framework for offline preference optimization methods,
Adaptive Preference Optimization with Utility Anchor (UAPO), which introduces
an anchoring function to estimate the uncertainties brought from preference
data annotation. Our method enables training even in scenarios where the data
is unpaired, significantly enhancing data utilization efficiency. Moreover, the
anchor design makes UAPO more robust in the training process. Experimental
results demonstrate that UAPO achieves competitive outcomes without the strict
dependency on data pairing, paving the way for more flexible and effective
preference optimization methods.

</details>


### [172] [Privacy-Preserving Personalization in Education: A Federated Recommender System for Student Performance Prediction](https://arxiv.org/abs/2509.10516)
*Rodrigo Tertulino*

Main category: cs.LG

TL;DR: 提出了一种新颖的隐私保护推荐系统，以解决教育平台中的个性化隐私困境。


<details>
  <summary>Details</summary>
Motivation: 传统的推荐系统依赖于集中式数据，这与现代数据保护法规不相容。因此，需要一个保护隐私的推荐系统。

Method: 使用联邦学习(FL)和深度神经网络(DNN)，以及来自大型ASSISTments教育数据集的丰富特征。

Result: FedProx在处理异构学生数据方面比标准FedAvg基线更稳定有效。优化的联邦模型实现了76.28%的高性能F1分数，相当于强大的集中式XGBoost模型的82.85%。

Conclusion: 联邦方法可以在不集中敏感学生数据的情况下提供高效的内容推荐。

Abstract: The increasing digitalization of education presents unprecedented
opportunities for data-driven personalization, yet it introduces significant
student data privacy challenges. Conventional recommender systems rely on
centralized data, a paradigm often incompatible with modern data protection
regulations. A novel privacy-preserving recommender system is proposed and
evaluated to address this critical issue using Federated Learning (FL). The
approach utilizes a Deep Neural Network (DNN) with rich, engineered features
from the large-scale ASSISTments educational dataset. A rigorous comparative
analysis of federated aggregation strategies was conducted, identifying FedProx
as a significantly more stable and effective method for handling heterogeneous
student data than the standard FedAvg baseline. The optimized federated model
achieves a high-performance F1-Score of 76.28\%, corresponding to 82.85\% of
the performance of a powerful, centralized XGBoost model. These findings
validate that a federated approach can provide highly effective content
recommendations without centralizing sensitive student data. Consequently, our
work presents a viable and robust solution to the personalization-privacy
dilemma in modern educational platforms.

</details>


### [173] [A Comparative Benchmark of Federated Learning Strategies for Mortality Prediction on Heterogeneous and Imbalanced Clinical Data](https://arxiv.org/abs/2509.10517)
*Rodrigo Tertulino*

Main category: cs.LG

TL;DR: 本文针对医院死亡率预测问题，评估了五种联邦学习策略（FedAvg, FedProx, FedAdagrad, FedAdam, FedCluster）在非独立同分布（non-IID）和不平衡数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界临床数据的隐私限制和统计异质性阻碍了机器学习模型在预测住院死亡率方面的应用。联邦学习(FL)提供了一种保护隐私的解决方案，但其在非独立同分布和不平衡条件下的性能需要严格考察。

Method: 使用MIMIC-IV数据集，通过按临床护理单元划分数据来模拟真实的非独立同分布环境。采用SMOTE-Tomek技术处理每个客户端本地训练数据的固有类别不平衡问题。在50轮通信中，对五种联邦学习策略进行了比较基准测试。

Result: 基于正则化的FedProx策略始终优于其他方法，实现了最高的F1分数0.8831，同时保持了稳定的收敛性。虽然基线FedAvg的计算效率最高，但其预测性能却明显较低。

Conclusion: 基于正则化的联邦学习算法（如FedProx）为异构和不平衡的临床预测任务提供了一种比标准或服务器端自适应聚合方法更强大有效的解决方案。

Abstract: Machine learning models hold significant potential for predicting in-hospital
mortality, yet data privacy constraints and the statistical heterogeneity of
real-world clinical data often hamper their development. Federated Learning
(FL) offers a privacy-preserving solution, but its performance under
non-Independent and Identically Distributed (non-IID) and imbalanced conditions
requires rigorous investigation. The study presents a comparative benchmark of
five federated learning strategies: FedAvg, FedProx, FedAdagrad, FedAdam, and
FedCluster for mortality prediction. Using the large-scale MIMIC-IV dataset, we
simulate a realistic non-IID environment by partitioning data by clinical care
unit. To address the inherent class imbalance of the task, the SMOTE-Tomek
technique is applied to each client's local training data. Our experiments,
conducted over 50 communication rounds, reveal that the regularization-based
strategy, FedProx, consistently outperformed other methods, achieving the
highest F1-Score of 0.8831 while maintaining stable convergence. While the
baseline FedAvg was the most computationally efficient, its predictive
performance was substantially lower. Our findings indicate that
regularization-based FL algorithms like FedProx offer a more robust and
effective solution for heterogeneous and imbalanced clinical prediction tasks
than standard or server-side adaptive aggregation methods. The work provides a
crucial empirical benchmark for selecting appropriate FL strategies for
real-world healthcare applications.

</details>


### [174] [Holographic Knowledge Manifolds: A Novel Pipeline for Continual Learning Without Catastrophic Forgetting in Large Language Models](https://arxiv.org/abs/2509.10518)
*Justin Arndt*

Main category: cs.LG

TL;DR: 介绍了全息知识流形（HKM），这是一个四阶段的流程，可以在AI知识表示中实现零灾难性遗忘，同时保持最小的内存增长和高效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决AI知识表示中的灾难性遗忘问题，并降低存储和计算成本。

Method: 利用分形量化、概率纠缠和动态衍射削片等技术来压缩知识。

Result: 在WikiText和FB15k数据集上的实验表明，HKM实现了0%的遗忘，3倍的压缩率，并在消费级GPU硬件上减少了53%的训练时间。成本分析预测，在petabyte规模上，5年内可节省9240万美元，并减少21.2%的能源消耗和33%的碳足迹。

Conclusion: HKM为公共大型语言模型（LLM）带来了一种新的范式，使其能够在不重新训练的情况下进行“永久”适应。未来的扩展可能进一步降低微调成本。

Abstract: We introduce the Holographic Knowledge Manifold (HKM), a four-phase pipeline
that achieves zero catastrophic forgetting in AI knowledge representation while
maintaining minimal memory growth and high efficiency. Leveraging fractal
quantization, probabilistic entanglement, and dynamic diffraction chipping, HKM
compresses knowledge substrates by 3x with 67% storage savings, integrates
holographically at 100%, and supports over 1,020 updates with 1% growth per
increment. In experiments on combined WikiText and FB15k datasets (scaled to
2,997 nodes), we demonstrate industry-leading performance: 0% forgetting
(infinite improvement over GEM baselines), 3x compression, and 53% training
time reduction on consumer GPU hardware. Hypothetical cost analyses project
$92.4M savings over 5 years at petabyte scale, with 21.2% energy reduction and
33% lower carbon footprint. This work hypothesizes a paradigm shift for public
large language models (LLMs), enabling "eternal" adaptation without retraining.
Future extensions to multimodal fusion and quantum hardware could further
democratize scalable AI, potentially reducing fine-tuning costs by 60-80% for
models like Llama-3 or Grok-4. Code, datasets, and full results are publicly
available for reproducibility.

</details>


### [175] [Gradient Estimation Methods of Approximate Multipliers for High-Accuracy Retraining of Deep Learning Models](https://arxiv.org/abs/2509.10519)
*Chang Meng,Wayne Burleson,Giovanni De Micheli*

Main category: cs.LG

TL;DR: 这篇论文提出使用近似乘法器(AppMults)来减少深度学习加速器的面积、延迟和功耗，但是AppMults会引入误差，需要重新训练来恢复精度。论文提出了两种方法LUT-2D和LUT-1D来获得更精确的AppMults梯度，从而优化重训练过程。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常使用精确乘法器(AccMult)的梯度来估计AppMult的梯度，这可能导致次优的重训练结果。

Method: 论文提出了两种方法：LUT-2D使用二维查找表(LUTs)来描述AppMult梯度，LUT-1D是LUT-2D的紧凑和更高效的变体，使用一维LUTs来存储梯度值。

Result: 实验结果表明，在CIFAR-10数据集上，LUT-2D和LUT-1D方法分别将重训练精度平均提高了3.83%和3.72%。在ImageNet数据集上，LUT-1D方法将视觉Transformer模型的重训练精度平均提高了23.69%。

Conclusion: 论文提出的LUT-2D和LUT-1D方法可以更精确地获得AppMults梯度, 从而有效提高深度学习模型的重训练精度。

Abstract: Approximate multipliers (AppMults) are widely used in deep learning
accelerators to reduce their area, delay, and power consumption. However,
AppMults introduce arithmetic errors into deep learning models, necessitating a
retraining process to recover accuracy. A key step in retraining is computing
the gradient of the AppMult, i.e., the partial derivative of the approximate
product with respect to each input operand. Existing approaches typically
estimate this gradient using that of the accurate multiplier (AccMult), which
can lead to suboptimal retraining results. To address this, we propose two
methods to obtain more precise gradients of AppMults. The first, called LUT-2D,
characterizes the AppMult gradient with 2-dimensional lookup tables (LUTs),
providing fine-grained estimation and achieving the highest retraining
accuracy. The second, called LUT-1D, is a compact and more efficient variant
that stores gradient values in 1-dimensional LUTs, achieving comparable
retraining accuracy with shorter runtime. Experimental results show that on
CIFAR-10 with convolutional neural networks, our LUT-2D and LUT-1D methods
improve retraining accuracy by 3.83% and 3.72% on average, respectively. On
ImageNet with vision transformer models, our LUT-1D method improves retraining
accuracy by 23.69% on average, compared to a state-of-the-art retraining
framework.

</details>


### [176] [Offline Contextual Bandit with Counterfactual Sample Identification](https://arxiv.org/abs/2509.10520)
*Alexandre Gilotte,Otmane Sakhi,Imad Aouali,Benjamin Heymann*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的上下文bandit方法，通过识别导致成功结果的行为来学习，而不是直接预测奖励。


<details>
  <summary>Details</summary>
Motivation: 现有的直接奖励模型容易受到混淆因素的影响，难以分离行为和上下文的影响。

Method: 该方法通过将实际行为与在相同上下文中从日志策略中抽样的反事实行为进行比较，来识别导致成功结果的行为。

Result: 该方法在合成实验和实际部署中均优于直接模型。

Conclusion: 提出的反事实样本识别方法在理论上是合理的，并且在实践中表现更好。

Abstract: In production systems, contextual bandit approaches often rely on direct
reward models that take both action and context as input. However, these models
can suffer from confounding, making it difficult to isolate the effect of the
action from that of the context. We present \emph{Counterfactual Sample
Identification}, a new approach that re-frames the problem: rather than
predicting reward, it learns to recognize which action led to a successful
(binary) outcome by comparing it to a counterfactual action sampled from the
logging policy under the same context. The method is theoretically grounded and
consistently outperforms direct models in both synthetic experiments and
real-world deployments.

</details>


### [177] [Variational Gaussian Mixture Manifold Models for Client-Specific Federated Personalization](https://arxiv.org/abs/2509.10521)
*Sai Puppala,Ismail Hossain,Md Jahangir Alam,Sajedul Talukder*

Main category: cs.LG

TL;DR: VGM$^2$ 是一个面向几何的 PFL 框架，它学习特定于客户端的参数 UMAP 嵌入，使用混合关系标记对潜在的成对距离进行建模，并仅交换变分、不确定性感知标记统计信息。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化联邦学习 (PFL) 方法在标签倾斜和非平稳性下经常失败，因为它们使用单一的全局参数化，忽略了特定于客户端的几何结构。

Method: 该方法包括：(i) 学习客户端特定的参数 UMAP 嵌入；(ii) 使用混合关系标记对潜在的成对距离进行建模，以区分相同和不同类别的样本对；(iii) 仅交换变分、不确定性感知标记统计信息。每个客户端维护一个关于标记权重、均值和方差的 Dirichlet-Normal-Inverse-Gamma (Dir-NIG) 后验；服务器通过共轭矩匹配进行聚合，以形成指导后续轮次的全局先验。

Result: 在具有非独立同分布 (non-IID) 标签分片的八个视觉数据集上，VGM$^2$ 实现了与强基线相比具有竞争力或更优越的测试 F1 分数，同时仅传输少量几何摘要。

Conclusion: VGM$^2$ 通过安全聚合和可选的差分隐私噪声来加强隐私，并提供成员推理压力测试。代码和配置将发布以确保完全的可重复性。

Abstract: Personalized federated learning (PFL) often fails under label skew and
non-stationarity because a single global parameterization ignores
client-specific geometry. We introduce VGM$^2$ (Variational Gaussian Mixture
Manifold), a geometry-centric PFL framework that (i) learns client-specific
parametric UMAP embeddings, (ii) models latent pairwise distances with mixture
relation markers for same and different class pairs, and (iii) exchanges only
variational, uncertainty-aware marker statistics. Each client maintains a
Dirichlet-Normal-Inverse-Gamma (Dir-NIG) posterior over marker weights, means,
and variances; the server aggregates via conjugate moment matching to form
global priors that guide subsequent rounds. We prove that this aggregation
minimizes the summed reverse Kullback-Leibler divergence from client posteriors
within the conjugate family, yielding stability under heterogeneity. We further
incorporate a calibration term for distance-to-similarity mapping and report
communication and compute budgets. Across eight vision datasets with non-IID
label shards, VGM$^2$ achieves competitive or superior test F1 scores compared
to strong baselines while communicating only small geometry summaries. Privacy
is strengthened through secure aggregation and optional differential privacy
noise, and we provide a membership-inference stress test. Code and
configurations will be released to ensure full reproducibility.

</details>


### [178] [Multimodal Deep Learning for ATCO Command Lifecycle Modeling and Workload Prediction](https://arxiv.org/abs/2509.10522)
*Kaizhen Tan*

Main category: cs.LG

TL;DR: 本文提出了一种多模态深度学习框架，用于估计空中交通管制员（ATCO）指令生命周期中的两个关键参数：指令和飞机动作之间的时间偏移以及指令持续时间。


<details>
  <summary>Details</summary>
Motivation: 在密集的空域中，空中交通管制员 (ATCO) 发出高强度的语音指令，准确的工作量建模对于安全和效率至关重要。

Method: 构建了一个高质量的数据集，使用滑动窗口和基于直方图的方法检测机动点。开发了一种 CNN-Transformer 集成模型，用于准确、通用和可解释的预测。

Result: 通过将轨迹与语音命令联系起来，这项工作提供了第一个支持智能命令生成的模型，并为工作量评估、人员配置和调度提供了实际价值。

Conclusion: 该模型可以支持智能命令生成，并为工作量评估、人员配置和调度提供实际价值。

Abstract: Air traffic controllers (ATCOs) issue high-intensity voice commands in dense
airspace, where accurate workload modeling is critical for safety and
efficiency. This paper proposes a multimodal deep learning framework that
integrates structured data, trajectory sequences, and image features to
estimate two key parameters in the ATCO command lifecycle: the time offset
between a command and the resulting aircraft maneuver, and the command
duration. A high-quality dataset was constructed, with maneuver points detected
using sliding window and histogram-based methods. A CNN-Transformer ensemble
model was developed for accurate, generalizable, and interpretable predictions.
By linking trajectories to voice commands, this work offers the first model of
its kind to support intelligent command generation and provides practical value
for workload assessment, staffing, and scheduling.

</details>


### [179] [From Predictions to Explanations: Explainable AI for Autism Diagnosis and Identification of Critical Brain Regions](https://arxiv.org/abs/2509.10523)
*Kush Gupta,Amir Aly,Emmanuel Ifeachor,Rohit Shankar*

Main category: cs.LG

TL;DR: 本文提出了一种结合深度学习和可解释AI的计算机辅助自闭症诊断框架，利用跨域迁移学习解决数据稀缺问题，并通过三种可解释AI技术识别关键脑区。


<details>
  <summary>Details</summary>
Motivation: 自闭症谱系障碍(ASD)是一种以非典型大脑成熟为特征的神经发育疾病。然而，迁移学习范式在机器学习中的应用仍然有限。

Method: 该框架包含两个模块：第一个模块利用通过跨域迁移学习微调的深度学习模型进行ASD分类；第二个模块侧重于解释模型决策并识别关键脑区，采用三种可解释AI (XAI) 技术。

Result: 该框架表明，跨域迁移学习可以有效解决ASD研究中的数据稀缺问题。通过应用三种已建立的可解释性技术，该方法揭示了模型如何做出诊断决策，并识别出与ASD最相关的脑区。

Conclusion: 研究结果与已建立的神经生物学证据进行了比较，突出了强一致性，并加强了所提出方法的临床相关性。

Abstract: Autism spectrum disorder (ASD) is a neurodevelopmental condition
characterized by atypical brain maturation. However, the adaptation of transfer
learning paradigms in machine learning for ASD research remains notably
limited. In this study, we propose a computer-aided diagnostic framework with
two modules. This chapter presents a two-module framework combining deep
learning and explainable AI for ASD diagnosis. The first module leverages a
deep learning model fine-tuned through cross-domain transfer learning for ASD
classification. The second module focuses on interpreting the model decisions
and identifying critical brain regions. To achieve this, we employed three
explainable AI (XAI) techniques: saliency mapping, Gradient-weighted Class
Activation Mapping, and SHapley Additive exPlanations (SHAP) analysis. This
framework demonstrates that cross-domain transfer learning can effectively
address data scarcity in ASD research. In addition, by applying three
established explainability techniques, the approach reveals how the model makes
diagnostic decisions and identifies brain regions most associated with ASD.
These findings were compared against established neurobiological evidence,
highlighting strong alignment and reinforcing the clinical relevance of the
proposed approach.

</details>


### [180] [Resource-Aware Neural Network Pruning Using Graph-based Reinforcement Learning](https://arxiv.org/abs/2509.10526)
*Dieter Balemans,Thomas Huybrechts,Jan Steckel,Siegfried Mercelis*

Main category: cs.LG

TL;DR: 本文提出了一种新的神经网络剪枝方法，通过将基于图的观察空间集成到 AutoML 框架中，以解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法依赖于手工设计的启发式方法和局部优化，导致次优性能和低效的剪枝策略。本文旨在通过引入网络结构的全局视图来克服这些限制。

Method: 该框架使用图注意力网络 (GAT) 编码器处理网络图表示，并生成丰富的嵌入。此外，将连续剪枝比率转换为细粒度的二元动作空间，使 agent 能够直接从数据中学习最佳通道重要性标准。这些贡献在约束马尔可夫决策过程 (CMDP) 框架中建模。

Result: 在 CIFAR-10、CIFAR-100 和 ImageNet 等基准数据集上进行了大量实验，结果表明该方法始终优于传统剪枝技术，显示出最先进的结果，同时学习特定于任务的剪枝策略。

Conclusion: 该方法能够识别功能冗余连接，超越了简单的权重幅度考虑。

Abstract: This paper presents a novel approach to neural network pruning by integrating
a graph-based observation space into an AutoML framework to address the
limitations of existing methods. Traditional pruning approaches often depend on
hand-crafted heuristics and local optimization perspectives, which can lead to
suboptimal performance and inefficient pruning strategies. Our framework
transforms the pruning process by introducing a graph representation of the
target neural network that captures complete topological relationships between
layers and channels, replacing the limited layer-wise observation space with a
global view of network structure. The core innovations include a Graph
Attention Network (GAT) encoder that processes the network's graph
representation and generates a rich embedding. Additionally, for the action
space we transition from continuous pruning ratios to fine-grained binary
action spaces which enables the agent to learn optimal channel importance
criteria directly from data, moving away from predefined scoring functions.
These contributions are modelled within a Constrained Markov Decision Process
(CMDP) framework, allowing the agent to make informed pruning decisions while
adhering to resource constraints such as target compression rates. For this, we
design a self-competition reward system that encourages the agent to outperform
its previous best performance while satisfying the defined constraints. We
demonstrate the effectiveness of our approach through extensive experiments on
benchmark datasets including CIFAR-10, CIFAR-100, and ImageNet. The experiments
show that our method consistently outperforms traditional pruning techniques,
showing state-of-the-art results while learning task-specific pruning
strategies that identify functionally redundant connections beyond simple
weight magnitude considerations.

</details>


### [181] [STM-Graph: A Python Framework for Spatio-Temporal Mapping and Graph Neural Network Predictions](https://arxiv.org/abs/2509.10528)
*Amirhossein Ghaffari,Huong Nguyen,Lauri Lovén,Ekaterina Gilman*

Main category: cs.LG

TL;DR: STM-Graph: A Python framework for transforming spatio-temporal urban event data into graph representations for GNN training and prediction.


<details>
  <summary>Details</summary>
Motivation: Urban spatio-temporal data's dynamic and complex nature poses challenges for predictive analytics.

Method: Transforms spatio-temporal urban event data into graph representations suitable for GNN training using diverse spatial mapping methods, OpenStreetMap data, and multiple GNN models.

Result: STM-Graph integrates spatial mapping methods, urban features, GNN models, visualization tools, and a GUI, facilitating experimentation and benchmarking. Includes links to source code.

Conclusion: STM-Graph is a modular and extensible framework valuable for urban computing researchers and practitioners, allowing integration of new mapping methods and custom models.

Abstract: Urban spatio-temporal data present unique challenges for predictive analytics
due to their dynamic and complex nature. We introduce STM-Graph, an open-source
Python framework that transforms raw spatio-temporal urban event data into
graph representations suitable for Graph Neural Network (GNN) training and
prediction. STM-Graph integrates diverse spatial mapping methods, urban
features from OpenStreetMap, multiple GNN models, comprehensive visualization
tools, and a graphical user interface (GUI) suitable for professional and
non-professional users. This modular and extensible framework facilitates rapid
experimentation and benchmarking. It allows integration of new mapping methods
and custom models, making it a valuable resource for researchers and
practitioners in urban computing. The source code of the framework and GUI are
available at: https://github.com/Ahghaffari/stm_graph and
https://github.com/tuminguyen/stm_graph_gui.

</details>


### [182] [Mitigating Catastrophic Forgetting and Mode Collapse in Text-to-Image Diffusion via Latent Replay](https://arxiv.org/abs/2509.10529)
*Aoi Otani*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为Latent Replay的持续学习方法，用于解决文本到图像扩散模型中的灾难性遗忘和模式崩溃问题。该方法通过存储模型内部结构中的紧凑、高级特征表示来实现，类似于海马体存储神经活动模式而非原始感觉输入的过程。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型在持续学习过程中面临灾难性遗忘和模式崩溃的挑战，这限制了它们在个性化应用中的发展。

Method: 该论文将神经科学启发的Latent Replay方法应用于扩散模型。Latent Replay仅保留从模型内部结构中提取的紧凑、高级特征表示，而不是存储大量图像。

Result: 实验结果表明，Latent Replay在保持模型多功能性方面显著优于现有方法。在学习所有概念后，该方法在最早的概念上保留了77.59%的图像对齐率，比基线方法高14%，同时保持了输出的多样性。

Conclusion: Latent Replay能够为生成式AI模型实现高效的持续学习，为能够随着用户需求演变而无需过度计算成本的个性化文本到图像模型铺平了道路。

Abstract: Continual learning -- the ability to acquire knowledge incrementally without
forgetting previous skills -- is fundamental to natural intelligence. While the
human brain excels at this, artificial neural networks struggle with
"catastrophic forgetting," where learning new tasks erases previously acquired
knowledge. This challenge is particularly severe for text-to-image diffusion
models, which generate images from textual prompts. Additionally, these models
face "mode collapse," where their outputs become increasingly repetitive over
time. To address these challenges, we apply Latent Replay, a
neuroscience-inspired approach, to diffusion models. Traditional replay methods
mitigate forgetting by storing and revisiting past examples, typically
requiring large collections of images. Latent Replay instead retains only
compact, high-level feature representations extracted from the model's internal
architecture. This mirrors the hippocampal process of storing neural activity
patterns rather than raw sensory inputs, reducing memory usage while preserving
critical information. Through experiments with five sequentially learned visual
concepts, we demonstrate that Latent Replay significantly outperforms existing
methods in maintaining model versatility. After learning all concepts, our
approach retained 77.59% Image Alignment (IA) on the earliest concept, 14%
higher than baseline methods, while maintaining diverse outputs. Surprisingly,
random selection of stored latent examples outperforms similarity-based
strategies. Our findings suggest that Latent Replay enables efficient continual
learning for generative AI models, paving the way for personalized
text-to-image models that evolve with user needs without excessive
computational costs.

</details>


### [183] [Dynamic Adaptive Shared Experts with Grouped Multi-Head Attention Mixture of Experts](https://arxiv.org/abs/2509.10530)
*Cheng Li,Jiexiong Liu,Yixuan Chen,Jie ji*

Main category: cs.LG

TL;DR: 提出了一种新的MoE架构Transformer模型，用于提升长序列建模能力。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型在计算效率和捕捉长距离依赖关系方面存在不足，尤其是在专家资源分配的动态适应性方面。

Method: 提出了动态自适应共享专家和分组多头注意力混合模型(DASG-MoE)，集成了分组多头注意力(GMHA)、双尺度共享专家结构(DSSE)和分层自适应动态路由(ADR)机制。

Result: 在多个长序列基准数据集上的实验表明，DASG-MoE模型优于最先进的模型。

Conclusion: DASG-MoE模型通过动态调整专家资源分配，有效提升了长序列建模能力。

Abstract: Transformer models based on the Mixture of Experts (MoE) architecture have
made significant progress in long-sequence modeling, but existing models still
have shortcomings in computational efficiency and the ability to capture
long-range dependencies, especially in terms of the dynamic adaptability of
expert resource allocation. In this paper, we propose a Dynamic Adaptive Shared
Expert and Grouped Multi-Head Attention Hybrid Model (DASG-MoE) to enhance
long-sequence modeling capabilities by integrating three modules. First, we
employ the Grouped Multi-Head Attention (GMHA) mechanism to effectively reduce
the computational complexity of long sequences. By parallel processing through
sequence grouping, local sliding window attention, and feature aggregation, we
address long-range dependency issues and the model's lack of generalization for
local information. Second, we design a Dual-Scale Shared Expert Structure
(DSSE), where shallow experts use lightweight computations to quickly respond
to low-dimensional features, while deep experts process high-dimensional
complex semantics through pre-training transfer and post-training optimization,
achieving a dynamic balance between efficiency and accuracy. Third, we propose
a hierarchical Adaptive Dynamic Routing (ADR) mechanism that dynamically
selects expert levels based on feature complexity and task requirements, and
optimizes resource allocation through a local expert activation strategy.
Experiments on multiple long-sequence benchmark datasets demonstrate that our
DASG-MoE model outperforms state-of-the-art models.

</details>


### [184] [FinXplore: An Adaptive Deep Reinforcement Learning Framework for Balancing and Discovering Investment Opportunities](https://arxiv.org/abs/2509.10531)
*Himanshu Choudhary,Arishi Orra,Manoj Thakur*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的投资组合优化方法，该方法利用深度强化学习 (DRL) 来动态平衡现有资产的配置和探索扩展投资领域中的新机会。


<details>
  <summary>Details</summary>
Motivation: 现有的基于 DRL 的投资组合优化方法通常仅限于在预定义的投资范围内分配资产，而忽略了探索新机会。为了解决这个问题，这篇研究引入了一个投资环境，将利用现有资产与探索扩展领域中的新投资机会相结合。

Method: 该方法利用两个 DRL 代理，一个代理在现有领域内分配资产，另一个代理协助探索扩展领域中的新机会，从而动态平衡这些目标以适应不断变化的市场。

Result: 在两个真实世界市场数据集上的实验表明，该方法优于现有技术水平的投资组合策略和基线方法。

Conclusion: 该研究提出了一种有效的投资组合优化方法，该方法能够动态适应市场变化并提高投资组合表现。

Abstract: Portfolio optimization is essential for balancing risk and return in
financial decision-making. Deep Reinforcement Learning (DRL) has stood out as a
cutting-edge tool for portfolio optimization that learns dynamic asset
allocation using trial-and-error interactions. However, most DRL-based methods
are restricted to allocating assets within a pre-defined investment universe
and overlook exploring new opportunities. This study introduces an investment
landscape that integrates exploiting existing assets with exploring new
investment opportunities in an extended universe. The proposed approach
leverages two DRL agents and dynamically balances these objectives to adapt to
evolving markets while enhancing portfolio performance. One agent allocates
assets within the existing universe, while another assists in exploring new
opportunities in the extended universe. The effciency of the proposed
methodology is determined using two real-world market data sets. The
experiments demonstrate the superiority of the suggested approach against the
state-of-the-art portfolio strategies and baseline methods.

</details>


### [185] [Decoupling the "What" and "Where" With Polar Coordinate Positional Embeddings](https://arxiv.org/abs/2509.10534)
*Anand Gopalakrishnan,Robert Csordás,Jürgen Schmidhuber,Michael C. Mozer*

Main category: cs.LG

TL;DR: RoPE位置嵌入会将内容和位置信息缠绕在一起，这会影响性能。PoPE通过消除这种混淆来改进RoPE。


<details>
  <summary>Details</summary>
Motivation: 流行的RoPE旋转位置嵌入会将内容和位置信息缠绕在一起，这会影响性能，尤其是在需要对这两个因素进行独立匹配时。

Method: 提出了一种改进的RoPE，称为极坐标位置嵌入（PoPE），它可以消除内容-位置混淆。

Result: 在仅需要按位置或内容进行索引的诊断任务中，PoPE远优于RoPE。在音乐、基因组和自然语言领域的自回归序列建模中，使用PoPE作为位置编码方案的Transformer优于使用RoPE的基线。

Conclusion: PoPE表现出强大的零样本长度外推能力，而RoPE的性能在测试时会随着更长的序列而显着降低，而无需微调或使用位置插值方法。

Abstract: The attention mechanism in a Transformer architecture matches key to query
based on both content -- the what -- and position in a sequence -- the where.
We present an analysis indicating that what and where are entangled in the
popular RoPE rotary position embedding. This entanglement can impair
performance particularly when decisions require independent matches on these
two factors. We propose an improvement to RoPE, which we call Polar Coordinate
Position Embeddings or PoPE, that eliminates the what-where confound. PoPE is
far superior on a diagnostic task requiring indexing solely by position or by
content. On autoregressive sequence modeling in music, genomic, and natural
language domains, Transformers using PoPE as the positional encoding scheme
outperform baselines using RoPE with respect to evaluation loss (perplexity)
and downstream task performance. On language modeling, these gains persist
across model scale, from 124M to 774M parameters. Crucially, PoPE shows strong
zero-shot length extrapolation capabilities, whereas RoPE's performance
degrades significantly on longer sequences at test time without fine tuning or
the use of position-interpolation methods.

</details>


### [186] [Semantic-guided LoRA Parameters Generation](https://arxiv.org/abs/2509.10535)
*Miaoge Li,Yang Chen,Zhijie Rao,Can Jiang,Jingcai Guo*

Main category: cs.LG

TL;DR: 提出了SG-LoRA，一个无需额外训练或访问用户数据即可高效生成用户特定LoRA参数的框架。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，边缘用户通常表现出特定于任务的偏好，这些偏好难以通过在封闭世界假设下训练的统一模型来处理。由于其成本密集型性质以及对来自边缘的原始数据利用的隐私担忧，为每个用户重新训练/微调模型也是不切实际的。

Method: SG-LoRA使用任务描述作为语义桥梁，测量它们与共享嵌入空间中一组已知专家任务的接近程度。基于这种语义指导，它对目标任务的LoRA参数分布进行建模，以生成用于新任务的高性能参数。

Result: 在多个具有挑战性的任务上进行的大量实验证实了SG-LoRA的卓越性能和显著适应性。

Conclusion: SG-LoRA通过提取杰出的LoRA专家的知识来实现与个人意图相符的LoRA模型的实时构建，同时为个性化模型适应提供了一种新颖的零样本开放世界设置中的隐私保护解决方案。

Abstract: Low-Rank Adaptation (LoRA) has demonstrated strong generalization
capabilities across a variety of tasks for efficiently fine-tuning AI models,
especially on resource-constrained edges. However, in real-world applications,
edge users often exhibit task-specific preferences that are difficult to handle
with a unified model trained under a closed-world assumption, and the challenge
may further increase when there are significant domain shifts between training
and deployment. Meanwhile, retraining/fine-tuning models for each user is also
impractical due to its cost-intensive nature and privacy concerns over raw data
utilization from edges. To address these challenges, we propose Semantic-guided
LoRA Parameter Generation (SG-LoRA), the first of its kind framework to
efficiently produce user-specific LoRA parameters without any additional
training on user tasks or access to user-specific data. Concretely, SG-LoRA
uses task descriptions as the semantic bridge, measuring their proximity to a
set of known expert tasks in a shared embedding space. Based on this semantic
guidance, it models the target task's LoRA parameter distribution to generate
high-performing parameters for novel tasks. SG-LoRA enables the real-time
construction of LoRA models aligned with individual intents by distilling
knowledge from prominent LoRA experts and, meanwhile, offering a
privacy-preserving solution for personalized model adaptation in a novel
zero-shot open-world setting proposed in this work. Extensive experiments on
multiple challenging tasks confirm the superior performance and remarkable
adaptability of SG-LoRA. Code is available at
https://github.com/keepgoingjkg/SG-LoRA.

</details>


### [187] [Contextuality, Holonomy and Discrete Fiber Bundles in Group-Valued Boltzmann Machines](https://arxiv.org/abs/2509.10536)
*Jean-Pierre Magnot*

Main category: cs.LG

TL;DR: 本文提出了一种受限玻尔兹曼机（RBM）的几何扩展，允许权重采用抽象群中的值，从而能够对复杂的关系结构进行建模。


<details>
  <summary>Details</summary>
Motivation: 动机是传统RBM无法处理复杂关系结构，例如射影变换、旋量动力学和函数对称性。

Method: 通过引入基于群值完整性的背景性指标，该指标量化了由局部权重引起的全局不一致性或“曲率”。

Result: 建立了与层理论背景性、规范理论和非交换几何的联系，并在有限和无限维度上提供了数值和图解示例。

Conclusion: 该框架为人工智能开辟了新的方向，从曲率感知学习架构到不确定或对抗环境中的拓扑正则化。

Abstract: We propose a geometric extension of restricted Boltzmann machines (RBMs) by
allowing weights to take values in abstract groups such as \(
\mathrm{GL}_n(\mathbb{R}) \), \( \mathrm{SU}(2) \), or even
infinite-dimensional operator groups. This generalization enables the modeling
of complex relational structures, including projective transformations, spinor
dynamics, and functional symmetries, with direct applications to vision,
language, and quantum learning.
  A central contribution of this work is the introduction of a
\emph{contextuality index} based on group-valued holonomies computed along
cycles in the RBM graph. This index quantifies the global inconsistency or
"curvature" induced by local weights, generalizing classical notions of
coherence, consistency, and geometric flatness. We establish links with
sheaf-theoretic contextuality, gauge theory, and noncommutative geometry, and
provide numerical and diagrammatic examples in both finite and infinite
dimensions.
  This framework opens novel directions in AI, from curvature-aware learning
architectures to topological regularization in uncertain or adversarial
environments.

</details>


### [188] [On Using Large-Batches in Federated Learning](https://arxiv.org/abs/2509.10537)
*Sahil Tyagi*

Main category: cs.LG

TL;DR: 本文旨在解决联邦学习中大批量训练导致的泛化性能下降问题，通过探索大小批量训练的折衷，以期兼顾大批量训练的并行扩展性和小批量训练的良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在资源受限的设备上训练深度网络至关重要，尤其是在数据隐私和局部性至关重要的情况下。然而，频繁同步设置下的大规模设备集群上的联邦学习可能会因大批量训练而导致泛化性能下降。

Method: 探索大小批量训练的折衷，并寻找新的方向，以同时享受大批量训练的并行扩展和小批量训练的良好泛化性。

Result: 实验结果表明，在相同迭代次数下，所提出的方法在ResNet50和VGG11模型中分别比小批量训练获得了约32.33%和3.74%的测试精度提升。

Conclusion: 所提出的方法能够有效提高联邦学习中大批量训练的泛化性能。

Abstract: Efficient Federated learning (FL) is crucial for training deep networks over
devices with limited compute resources and bounded networks. With the advent of
big data, devices either generate or collect multimodal data to train either
generic or local-context aware networks, particularly when data privacy and
locality is vital. FL algorithms generally trade-off between parallel and
statistical performance, improving model quality at the cost of higher
communication frequency, or vice versa. Under frequent synchronization
settings, FL over a large cluster of devices may perform more work per-training
iteration by processing a larger global batch-size, thus attaining considerable
training speedup. However, this may result in poor test performance (i.e., low
test loss or accuracy) due to generalization degradation issues associated with
large-batch training. To address these challenges with large-batches, this work
proposes our vision of exploiting the trade-offs between small and large-batch
training, and explore new directions to enjoy both the parallel scaling of
large-batches and good generalizability of small-batch training. For the same
number of iterations, we observe that our proposed large-batch training
technique attains about 32.33% and 3.74% higher test accuracy than small-batch
training in ResNet50 and VGG11 models respectively.

</details>


### [189] [DualAlign: Generating Clinically Grounded Synthetic Data](https://arxiv.org/abs/2509.10538)
*Rumeng Li,Xun Wang,Hong Yu*

Main category: cs.LG

TL;DR: 提出了 DualAlign 框架，用于生成更真实、更具临床意义的合成临床数据。


<details>
  <summary>Details</summary>
Motivation: 真实世界 EHR 的隐私限制、带注释的罕见病数据的可用性有限以及观察数据集中的系统性偏差，使得合成临床数据在推动医疗保健领域的人工智能发展方面变得越来越重要。

Method: 通过双重对齐来增强统计保真度和临床合理性：(1) 统计对齐，根据患者的人口统计学和风险因素进行生成；(2) 语义对齐，结合真实世界的症状轨迹来指导内容生成。

Result: 以阿尔茨海默病 (AD) 作为案例研究，DualAlign 产生的内容更贴近真实临床文档。使用 DualAlign 生成的数据和人工注释数据对 LLaMA 3.1-8B 模型进行微调，与仅在黄金数据或无指导的合成基线上训练的模型相比，性能显着提升。

Conclusion: DualAlign 虽然不能完全捕捉纵向复杂性，但它为生成临床上有根据的、保护隐私的合成数据以支持低资源临床文本分析提供了一种实用的方法。

Abstract: Synthetic clinical data are increasingly important for advancing AI in
healthcare, given strict privacy constraints on real-world EHRs, limited
availability of annotated rare-condition data, and systemic biases in
observational datasets. While large language models (LLMs) can generate fluent
clinical text, producing synthetic data that is both realistic and clinically
meaningful remains challenging. We introduce DualAlign, a framework that
enhances statistical fidelity and clinical plausibility through dual alignment:
(1) statistical alignment, which conditions generation on patient demographics
and risk factors; and (2) semantic alignment, which incorporates real-world
symptom trajectories to guide content generation. Using Alzheimer's disease
(AD) as a case study, DualAlign produces context-grounded symptom-level
sentences that better reflect real-world clinical documentation. Fine-tuning an
LLaMA 3.1-8B model with a combination of DualAlign-generated and
human-annotated data yields substantial performance gains over models trained
on gold data alone or unguided synthetic baselines. While DualAlign does not
fully capture longitudinal complexity, it offers a practical approach for
generating clinically grounded, privacy-preserving synthetic data to support
low-resource clinical text analysis.

</details>


### [190] [GTS_Forecaster: a novel deep learning based geodetic time series forecasting toolbox with python](https://arxiv.org/abs/2509.10560)
*Xuechen Liang,Xiaoxing He,Shengdao Wang,Jean-Philippe Montillet,Zhengkai Huang,Gaël Kermarrec,Shunqiang Hu,Yu Zhou,Jiahui Huang*

Main category: cs.LG

TL;DR: GTS Forecaster是一个用于大地测量时间序列预测的开源Python包，集成了深度学习模型（KAN、GNNGRU、TimeGNN）和预处理工具（异常值检测、KTIF填补算法），支持GNSS、SSH和TG数据集的预测、可视化和评估。


<details>
  <summary>Details</summary>
Motivation: 大地测量时间序列（GNSS、卫星测高SSH、验潮站TG记录）对于监测地表形变和海平面变化至关重要。准确预测这些变量可以加强早期预警系统，并支持地震、滑坡、沿海风暴潮和长期海平面的灾害缓解。

Method: GTS Forecaster集成了先进的深度学习模型，包括kernel attention networks (KAN), graph neural network-based gated recurrent units (GNNGRU), 和 time-aware graph neural networks (TimeGNN)，以有效地模拟非线性时空模式。该软件包还提供了强大的预处理工具，包括异常值检测和基于强化学习的插值算法KTIF。

Result: GTS Forecaster目前支持GNSS、SSH和TG数据集的预测、可视化和评估，并且适用于一般时间序列应用。

Conclusion: GTS Forecaster通过结合先进的模型和一个可访问的界面，促进了深度学习在大地测量预测任务中的应用。

Abstract: Geodetic time series -- such as Global Navigation Satellite System (GNSS)
positions, satellite altimetry-derived sea surface height (SSH), and tide gauge
(TG) records -- is essential for monitoring surface deformation and sea level
change. Accurate forecasts of these variables can enhance early warning systems
and support hazard mitigation for earthquakes, landslides, coastal storm surge,
and long-term sea level. However, the nonlinear, non-stationary, and incomplete
nature of such variables presents significant challenges for classic models,
which often fail to capture long-term dependencies and complex spatiotemporal
dynamics. We introduce GTS Forecaster, an open-source Python package for
geodetic time series forecasting. It integrates advanced deep learning models
-- including kernel attention networks (KAN), graph neural network-based gated
recurrent units (GNNGRU), and time-aware graph neural networks (TimeGNN) -- to
effectively model nonlinear spatial-temporal patterns. The package also
provides robust preprocessing tools, including outlier detection and a
reinforcement learning-based gap-filling algorithm, the Kalman-TransFusion
Interpolation Framework (KTIF). GTS Forecaster currently supports forecasting,
visualization, and evaluation of GNSS, SSH, and TG datasets, and is adaptable
to general time series applications. By combining cutting-edge models with an
accessible interface, it facilitates the application of deep learning in
geodetic forecasting tasks.

</details>


### [191] [SME-TEAM: Leveraging Trust and Ethics for Secure and Responsible Use of AI and LLMs in SMEs](https://arxiv.org/abs/2509.10594)
*Iqbal H. Sarker,Helge Janicke,Ahmad Mohsin,Leandros Maglaras*

Main category: cs.LG

TL;DR: 本研究提出了一个多阶段框架，旨在将信任和道德原则嵌入到人工智能生命周期中，以便在中小企业中安全且负责任地使用人工智能。


<details>
  <summary>Details</summary>
Motivation: 中小企业在采用人工智能和大型语言模型时面临技术、道德和信任问题。

Method: 该框架围绕数据、算法、人工监督和模型架构这四大支柱构建，将理论道德原则与运营实践相结合。

Result: 该框架可以增强人工智能在各种中小企业应用中的能力。

Conclusion: 本研究为负责任地采用人工智能提供了一个结构化的路线图，将信任和道德视为中小企业韧性、竞争力和可持续创新的催化剂。

Abstract: Artificial Intelligence (AI) and Large Language Models (LLMs) are reshaping
today's business practices, however, their adoption within small and
medium-sized enterprises (SMEs) raises significant technical, ethical and trust
issues. This paper proposes a structured, multi-phased framework designed to
embed trust and ethical principles throughout the AI lifecycle for their secure
and responsible use in SMEs. Structured around four pillars, i.e., Data,
Algorithms, Human oversight, and Model Architecture, the framework bridges
theoretical ethical principles with operational practice, enhancing AI
capabilities in diverse SME applications. Ultimately, this paper offers a
structured roadmap for responsible AI adoption, framing trust and ethics as a
catalyst for resilience, competitiveness, and sustainable innovation in SMEs.

</details>


### [192] [pySigLib -- Fast Signature-Based Computations on CPU and GPU](https://arxiv.org/abs/2509.10613)
*Daniil Shmelev,Cristopher Salvi*

Main category: cs.LG

TL;DR: 介绍了一个高性能Python库pySigLib，用于计算签名和签名核，特别是在量化金融领域。


<details>
  <summary>Details</summary>
Motivation: 现有的签名方法在处理大规模数据集和长序列时存在扩展性问题。

Method: 开发了一个在CPU和GPU上优化的签名和签名核的Python库，并提出了一种新的签名核微分方案。

Result: pySigLib库提供了高效的签名计算，并且新的微分方案能够以更少的运行时间提供准确的梯度。

Conclusion: pySigLib库为大规模签名计算提供了一个高效的软件栈，并改进了签名核的微分方法。

Abstract: Signature-based methods have recently gained significant traction in machine
learning for sequential data. In particular, signature kernels have emerged as
powerful discriminators and training losses for generative models on
time-series, notably in quantitative finance. However, existing implementations
do not scale to the dataset sizes and sequence lengths encountered in practice.
We present pySigLib, a high-performance Python library offering optimised
implementations of signatures and signature kernels on CPU and GPU, fully
compatible with PyTorch's automatic differentiation. Beyond an efficient
software stack for large-scale signature-based computation, we introduce a
novel differentiation scheme for signature kernels that delivers accurate
gradients at a fraction of the runtime of existing libraries.

</details>


### [193] [Optimal Multimarginal Schrödinger Bridge: Minimum Spanning Tree over Measure-valued Vertices](https://arxiv.org/abs/2509.10626)
*Georgiy A. Bondar,Abhishek Halder*

Main category: cs.LG

TL;DR: 本文解决了寻找最优Multimarginal Schrodinger Bridge的问题，该问题旨在所有可能的图结构上找到最优耦合。


<details>
  <summary>Details</summary>
Motivation: Multimarginal Schrodinger Bridge (MSB) 旨在找到具有已知统计数据和已知相关结构的随机向量集合之间的最优耦合。在 MSB 公式中，这种相关结构被
emph{a priori} 指定为具有测度值顶点的无向连通图。

Method: 寻找最优 MSB 的问题转化为求解测度值顶点的最小生成树问题。该问题分两步解决：首先，构建一个完整的图，其边权重等于相应双边 SB 的最优值之和以及端点的熵。第二步，解决该完整加权图上的标准最小生成树问题。

Result: 计算最优 MSB 相当于解决测度值顶点的最小生成树问题。

Conclusion: 数值实验验证了所提出的解决方案。

Abstract: The Multimarginal Schr\"odinger Bridge (MSB) finds the optimal coupling among
a collection of random vectors with known statistics and a known correlation
structure. In the MSB formulation, this correlation structure is specified
\emph{a priori} as an undirected connected graph with measure-valued vertices.
In this work, we formulate and solve the problem of finding the optimal MSB in
the sense we seek the optimal coupling over all possible graph structures. We
find that computing the optimal MSB amounts to solving the minimum spanning
tree problem over measure-valued vertices. We show that the resulting problem
can be solved in two steps. The first step constructs a complete graph with
edge weight equal to a sum of the optimal value of the corresponding bimarginal
SB and the entropies of the endpoints. The second step solves a standard
minimum spanning tree problem over that complete weighted graph. Numerical
experiments illustrate the proposed solution.

</details>


### [194] [Interpretable neural network system identification method for two families of second-order systems based on characteristic curves](https://arxiv.org/abs/2509.10632)
*Federico J. Gonzalez,Luis P. Lara*

Main category: cs.LG

TL;DR: 提出了一种统一的数据驱动框架，将控制微分方程的数学结构与神经网络的灵活性相结合，以进行非线性系统辨识。


<details>
  <summary>Details</summary>
Motivation: 非线性系统辨识经常需要在可解释性和灵活性之间进行权衡，通常需要结合物理约束。

Method: 该方法的核心是特征曲线 (CC) 的概念，它代表了系统的各个非线性函数（例如，摩擦和恢复分量）。每个 CC 都由一个专用 NN 建模，从而实现系统方程的模块化和可解释的表示。

Result: 结果表明，所有三种方法都非常适合具有简单多项式非线性的系统，例如范德波尔振荡器。相比之下，NN-CC 在建模具有复杂非线性和不连续性的系统（例如在粘滑系统中观察到的那些系统）方面表现出卓越的性能。

Conclusion: 这项工作的关键贡献是证明了基于 CC 的框架，特别是 NN-CC 方法，可以捕获复杂的非线性，同时通过 CC 的显式表示来保持可解释性。

Abstract: Nonlinear system identification often involves a fundamental trade-off
between interpretability and flexibility, often requiring the incorporation of
physical constraints. We propose a unified data-driven framework that combines
the mathematical structure of the governing differential equations with the
flexibility of neural networks (NNs). At the core of our approach is the
concept of characteristic curves (CCs), which represent individual nonlinear
functions (e.g., friction and restoring components) of the system. Each CC is
modeled by a dedicated NN, enabling a modular and interpretable representation
of the system equation. To demonstrate the versatility of the CC-based
formalism, we introduce three identification strategies: (1) SINDy-CC, which
extends the sparse regression approach of SINDy by incorporating the
mathematical structure of the governing equations as constraints; (2) Poly-CC,
which represents each CC using high-degree polynomials; and (3) NN-CC, which
uses NNs without requiring prior assumptions about basis functions. Our results
show that all three approaches are well-suited for systems with simple
polynomial nonlinearities, such as the van der Pol oscillator. In contrast,
NN-CC demonstrates superior performance in modeling systems with complex
nonlinearities and discontinuities, such as those observed in stick-slip
systems. The key contribution of this work is to demonstrate that the CC-based
framework, particularly the NN-CC approach, can capture complex nonlinearities
while maintaining interpretability through the explicit representation of the
CCs. This balance makes it well-suited for modeling systems with
discontinuities and complex nonlinearities that are challenging to assess using
traditional polynomial or sparse regression methods, providing a powerful tool
for nonlinear system identification.

</details>


### [195] [Accurate and Private Diagnosis of Rare Genetic Syndromes from Facial Images with Federated Deep Learning](https://arxiv.org/abs/2509.10635)
*Ali Burak Ünal,Cem Ata Baykara,Peter Krawitz,Mete Akgün*

Main category: cs.LG

TL;DR: 本研究提出了一种联邦GestaltMatcher服务，该服务允许多家医院在不共享患者图像的情况下协作训练全局特征提取器。


<details>
  <summary>Details</summary>
Motivation: 由于患者数据分散在各个机构且受严格的隐私法规约束，因此集中式数据集限制了GestaltMatcher的进一步发展。

Method: 该方法基于一种跨silo的水平联邦学习框架，并将患者数据映射到一个共享的潜在空间。此外，一个隐私保护的内核矩阵计算框架被用于综合征推断和发现。

Result: 实验表明，该联邦服务保留了超过90%的集中性能，并且对不同的silo数量和异构数据分布都具有鲁棒性。

Conclusion: 提出的联邦GestaltMatcher服务能够在保护患者隐私的同时，实现高效的特征提取和综合征推断。

Abstract: Machine learning has shown promise in facial dysmorphology, where
characteristic facial features provide diagnostic clues for rare genetic
disorders. GestaltMatcher, a leading framework in this field, has demonstrated
clinical utility across multiple studies, but its reliance on centralized
datasets limits further development, as patient data are siloed across
institutions and subject to strict privacy regulations. We introduce a
federated GestaltMatcher service based on a cross-silo horizontal federated
learning framework, which allows hospitals to collaboratively train a global
ensemble feature extractor without sharing patient images. Patient data are
mapped into a shared latent space, and a privacy-preserving kernel matrix
computation framework enables syndrome inference and discovery while
safeguarding confidentiality. New participants can directly benefit from and
contribute to the system by adopting the global feature extractor and kernel
configuration from previous training rounds. Experiments show that the
federated service retains over 90% of centralized performance and remains
robust to both varying silo numbers and heterogeneous data distributions.

</details>


### [196] [Test-Time Warmup for Multimodal Large Language Models](https://arxiv.org/abs/2509.10641)
*Nikita Rajaneesh,Thomas Zollo,Richard Zemel*

Main category: cs.LG

TL;DR: MLLMs are not living up to their full potential due to limited training data.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with complex reasoning tasks because they are trained on relatively small multimodal datasets, despite having individual components pretrained on massive datasets.

Method: Proposes a Test-Time Warmup method that adapts the MLLM per test instance using weakly supervised auxiliary tasks, instead of relying on large labeled datasets for fine-tuning.

Result: Improved performance on MMMU, VQA-Rad, and GQA datasets using the Llama-Vision-Instruct model.

Conclusion: Warming up before inference enhances MLLMs' robustness across diverse reasoning tasks.

Abstract: Multimodal Large Language Models (MLLMs) hold great promise for advanced
reasoning at the intersection of text and images, yet they have not fully
realized this potential. MLLMs typically integrate an LLM, a vision encoder,
and a connector that maps the vision encoder's embeddings into the LLM's text
embedding space. Although each component is pretrained on massive datasets with
billions of samples, the entire multimodal model is typically trained on only
thousands (or a few million) samples, which can result in weak performance on
complex reasoning tasks. To address these shortcomings, instead of relying on
extensive labeled datasets for fine-tuning, we propose a Test-Time Warmup
method that adapts the MLLM per test instance by leveraging data from weakly
supervised auxiliary tasks. With our approach, we observe a relative
performance improvement of 4.03% on MMMU, 5.28% on VQA-Rad, and 1.63% on GQA on
the Llama-Vision-Instruct model. Our method demonstrates that 'warming up'
before inference can enhance MLLMs' robustness across diverse reasoning tasks.

</details>


### [197] [Self-Supervised Goal-Reaching Results in Multi-Agent Cooperation and Exploration](https://arxiv.org/abs/2509.10656)
*Chirayu Nimonkar,Shlok Shah,Catherine Ji,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 提出了一种基于自监督目标到达技术的多智能体强化学习方法，通过最大化访问特定目标的可能性来促进智能体之间的合作和长期推理。


<details>
  <summary>Details</summary>
Motivation: 设计奖励函数以引导自主智能体实现特定目标下的协作和长期推理非常具有挑战性。

Method: 智能体不直接最大化标量奖励，而是最大化访问特定目标的可能性。人类用户可以通过指定单个目标状态来定义任务，而无需实现复杂的奖励函数。

Result: 在MARL基准测试中，该方法优于其他具有相同稀疏奖励信号的方法。即使没有明确的探索机制，自监督多智能体目标到达也能在其他方法无法成功的情况下实现涌现合作和探索。

Conclusion: 自监督目标到达技术可以有效地用于多智能体合作学习，即使在奖励信号稀疏的情况下也能实现良好的性能。

Abstract: For groups of autonomous agents to achieve a particular goal, they must
engage in coordination and long-horizon reasoning. However, designing reward
functions to elicit such behavior is challenging. In this paper, we study how
self-supervised goal-reaching techniques can be leveraged to enable agents to
cooperate. The key idea is that, rather than have agents maximize some scalar
reward, agents aim to maximize the likelihood of visiting a certain goal. This
problem setting enables human users to specify tasks via a single goal state
rather than implementing a complex reward function. While the feedback signal
is quite sparse, we will demonstrate that self-supervised goal-reaching
techniques enable agents to learn from such feedback. On MARL benchmarks, our
proposed method outperforms alternative approaches that have access to the same
sparse reward signal as our method. While our method has no explicit mechanism
for exploration, we observe that self-supervised multi-agent goal-reaching
leads to emergent cooperation and exploration in settings where alternative
approaches never witness a single successful trial.

</details>


### [198] [M4GN: Mesh-based Multi-segment Hierarchical Graph Network for Dynamic Simulations](https://arxiv.org/abs/2509.10659)
*Bo Lei,Victor M. Castillo,Yeping Hu*

Main category: cs.LG

TL;DR: M4GN: a three-tier, segment-centric hierarchical network, which improves prediction accuracy and inference speed.


<details>
  <summary>Details</summary>
Motivation: Deep message passing incurs high cost and over-smoothing on large, long-range meshes; hierarchical GNNs shorten propagation paths but still face two key obstacles: (i) building coarse graphs that respect mesh topology, geometry, and physical discontinuities, and (ii) maintaining fine-scale accuracy without sacrificing the speed gained from coarsening.

Method: M4GN begins with a hybrid segmentation strategy that pairs a fast graph partitioner with a superpixel-style refinement guided by modal-decomposition features, producing contiguous segments of dynamically consistent nodes. These segments are encoded by a permutation-invariant aggregator. The resulting information bridges a micro-level GNN, which captures local dynamics, and a macro-level transformer that reasons efficiently across segments, achieving a principled balance between accuracy and efficiency.

Result: M4GN improves prediction accuracy by up to 56% while achieving up to 22% faster inference than state-of-the-art baselines.

Conclusion: M4GN improves prediction accuracy and inference speed compared to state-of-the-art baselines on multiple representative benchmark datasets.

Abstract: Mesh-based graph neural networks (GNNs) have become effective surrogates for
PDE simulations, yet their deep message passing incurs high cost and
over-smoothing on large, long-range meshes; hierarchical GNNs shorten
propagation paths but still face two key obstacles: (i) building coarse graphs
that respect mesh topology, geometry, and physical discontinuities, and (ii)
maintaining fine-scale accuracy without sacrificing the speed gained from
coarsening. We tackle these challenges with M4GN, a three-tier, segment-centric
hierarchical network. M4GN begins with a hybrid segmentation strategy that
pairs a fast graph partitioner with a superpixel-style refinement guided by
modal-decomposition features, producing contiguous segments of dynamically
consistent nodes. These segments are encoded by a permutation-invariant
aggregator, avoiding the order sensitivity and quadratic cost of aggregation
approaches used in prior works. The resulting information bridges a micro-level
GNN, which captures local dynamics, and a macro-level transformer that reasons
efficiently across segments, achieving a principled balance between accuracy
and efficiency. Evaluated on multiple representative benchmark datasets, M4GN
improves prediction accuracy by up to 56% while achieving up to 22% faster
inference than state-of-the-art baselines.

</details>


### [199] [Least-Ambiguous Multi-Label Classifier](https://arxiv.org/abs/2509.10689)
*Misgina Tsighe Hagos,Claes Lundström*

Main category: cs.LG

TL;DR: 本文提出了一种用于单正多标签学习 (SPMLL) 的模型无关方法，该方法利用共形预测来生成校准的集合值输出，从而在测试时实现可靠的多标签预测。


<details>
  <summary>Details</summary>
Motivation: 在多标签学习中，收集完整的标签注释成本高且费力。许多数据集只为每个训练实例注释一个正标签，尽管存在多个相关标签。这种情况被称为单正多标签学习 (SPMLL)，由于其极端形式的部分监督，提出了一个重大挑战。

Method: 我们提出了一种模型无关的 SPMLL 方法，该方法利用共形预测来生成校准的集合值输出。

Result: 我们在 12 个基准数据集上评估了我们的方法，证明了相对于现有基线的持续改进和实际适用性。

Conclusion: 我们的方法弥合了单标签训练和多标签评估之间的监督差距，而无需依赖标签分布假设。

Abstract: Multi-label learning often requires identifying all relevant labels for
training instances, but collecting full label annotations is costly and
labor-intensive. In many datasets, only a single positive label is annotated
per training instance, despite the presence of multiple relevant labels. This
setting, known as single-positive multi-label learning (SPMLL), presents a
significant challenge due to its extreme form of partial supervision. We
propose a model-agnostic approach to SPMLL that draws on conformal prediction
to produce calibrated set-valued outputs, enabling reliable multi-label
predictions at test time. Our method bridges the supervision gap between
single-label training and multi-label evaluation without relying on label
distribution assumptions. We evaluate our approach on 12 benchmark datasets,
demonstrating consistent improvements over existing baselines and practical
applicability.

</details>


### [200] [Learning Concave Bid Shading Strategies in Online Auctions via Measure-valued Proximal Optimization](https://arxiv.org/abs/2509.10693)
*Iman Nodozi,Djordje Gligorijevic,Abhishek Halder*

Main category: cs.LG

TL;DR: 本文提出了一种用于头价拍卖的出价阴影策略，作为一种测量值优化问题。


<details>
  <summary>Details</summary>
Motivation: 考虑出价阴影的标准参数形式，并将问题表述为阴影参数联合分布上的凸优化。

Method: 在每次拍卖后，通过具有数据驱动能量泛函的正则化 Wasserstein-proximal 更新来调整阴影参数分布。该能量泛函取决于上下文，即发布者/用户属性，例如域、广告位类型、设备或位置。

Result: 所提出的算法鼓励出价分布将更多权重放在具有更高预期盈余的值上，即，其中获胜概率和价值差距都很大。我们表明，由此产生的测量值凸优化问题允许封闭形式的解决方案。

Conclusion: 数值例子说明了所提出的方法。

Abstract: This work proposes a bid shading strategy for first-price auctions as a
measure-valued optimization problem. We consider a standard parametric form for
bid shading and formulate the problem as convex optimization over the joint
distribution of shading parameters. After each auction, the shading parameter
distribution is adapted via a regularized Wasserstein-proximal update with a
data-driven energy functional. This energy functional is conditional on the
context, i.e., on publisher/user attributes such as domain, ad slot type,
device, or location. The proposed algorithm encourages the bid distribution to
place more weight on values with higher expected surplus, i.e., where the win
probability and the value gap are both large. We show that the resulting
measure-valued convex optimization problem admits a closed form solution. A
numerical example illustrates the proposed method.

</details>


### [201] [Verifying Computational Graphs in Production-Grade Distributed Machine Learning Frameworks](https://arxiv.org/abs/2509.10694)
*Kahfi S. Zulkifli,Wenbo Qian,Shaowei Zhu,Yuan Zhou,Zhen Zhang,Chang Lou*

Main category: cs.LG

TL;DR: Scalify是一个轻量级框架，通过验证计算图的语义等价性来暴露静默错误，从而提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习框架在支持大型模型时，由于并行性和优化技术的引入，产生了新的复杂性，导致模型性能下降。现有的解决方案要么是临时的，要么对于生产环境来说成本太高。

Method: Scalify使用等式饱和和Datalog风格的推理来验证计算图的语义等价性。为了扩展规模，Scalify使用并行重写和层记忆来分割图，重用重写模板，并使用关系推理和符号双射推理来增强等式饱和。

Result: Scalify能够在普通机器上几分钟内验证像Llama-3.1-405B这样大的模型，并在Amazon的生产机器学习框架中发现了五个未知的错误。

Conclusion: Scalify能够有效地验证大型模型并发现潜在的错误，为调试提供可操作的指导。

Abstract: Modern machine learning frameworks support very large models by incorporating
parallelism and optimization techniques. Yet, these very techniques add new
layers of complexity, introducing silent errors that severely degrade model
performance. Existing solutions are either ad hoc or too costly for production.
  We present Scalify, a lightweight framework that exposes silent errors by
verifying semantic equivalence of computational graphs using equality
saturation and Datalog-style reasoning. To scale, Scalify partitions graphs
with parallel rewriting and layer memoization, reuses rewrite templates, and
augments equality saturation with relational reasoning and symbolic bijection
inference. It further localizes discrepancies to precise code sites, turning
verification results into actionable debugging guidance. Scalify verifies
models as large as Llama-3.1-405B within minutes on a commodity machine and
exposed five unknown bugs in Amazon production machine learning frameworks.

</details>


### [202] [Kalman Bayesian Transformer](https://arxiv.org/abs/2509.10695)
*Haoming Jing,Oren Wright,José M. F. Moura,Yorie Nakahira*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，通过在贝叶斯框架内将顺序微调构建为后验推理问题，从而实现稳健且数据高效的顺序学习。


<details>
  <summary>Details</summary>
Motivation: 在数据顺序到达时，顺序微调transformer很有用，尤其是在分布发生变化时。与批量学习不同，顺序学习要求通过平衡预训练模型中的新信息和先前学习的知识来稳定训练，尽管数据量很小。当训练需要在延迟关键型环境中完成，并且学习还必须量化不确定性并由不确定性调节时，这一挑战会进一步复杂化。

Method: 我们的方法集成了随机变量的闭式矩传播、卡尔曼贝叶斯神经网络和softmax函数矩的泰勒近似。

Result: 通过明确地将预训练模型作为先验考虑，并根据量化的不确定性自适应地平衡它们与新信息，我们的方法实现了稳健且数据高效的顺序学习。通过涉及决策transformer对以分布偏移和有限内存资源为特征的任务进行顺序适应的数值模拟，证明了我们方法的有效性。

Conclusion: 本文提出了一种新颖的贝叶斯框架下的顺序微调方法，通过整合矩传播、卡尔曼滤波和泰勒近似，实现了在分布偏移和资源受限情况下的稳健学习。

Abstract: Sequential fine-tuning of transformers is useful when new data arrive
sequentially, especially with shifting distributions. Unlike batch learning,
sequential learning demands that training be stabilized despite a small amount
of data by balancing new information and previously learned knowledge in the
pre-trained models. This challenge is further complicated when training is to
be completed in latency-critical environments and learning must additionally
quantify and be mediated by uncertainty. Motivated by these challenges, we
propose a novel method that frames sequential fine-tuning as a posterior
inference problem within a Bayesian framework. Our approach integrates
closed-form moment propagation of random variables, Kalman Bayesian Neural
Networks, and Taylor approximations of the moments of softmax functions. By
explicitly accounting for pre-trained models as priors and adaptively balancing
them against new information based on quantified uncertainty, our method
achieves robust and data-efficient sequential learning. The effectiveness of
our method is demonstrated through numerical simulations involving sequential
adaptation of a decision transformer to tasks characterized by distribution
shifts and limited memory resources.

</details>


### [203] [CrunchLLM: Multitask LLMs for Structured Business Reasoning and Outcome Prediction](https://arxiv.org/abs/2509.10698)
*Rabeya Tus Sadia,Qiang Cheng*

Main category: cs.LG

TL;DR: CrunchLLM: a domain-adapted LLM framework for startup success prediction, integrating structured and unstructured data with fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Predicting startup success (acquisition or IPO) is critical but challenging due to heterogeneous data.

Method: Integrates structured company attributes with unstructured text, uses parameter-efficient fine-tuning and prompt optimization to specialize LLMs.

Result: Achieves over 80% accuracy in predicting startup success on Crunchbase, outperforming traditional methods and baseline LLMs.

Conclusion: Domain-aware fine-tuning and data fusion advance predictive modeling; CrunchLLM offers a framework and tool for venture capital and innovation policy.

Abstract: Predicting the success of start-up companies, defined as achieving an exit
through acquisition or IPO, is a critical problem in entrepreneurship and
innovation research. Datasets such as Crunchbase provide both structured
information (e.g., funding rounds, industries, investor networks) and
unstructured text (e.g., company descriptions), but effectively leveraging this
heterogeneous data for prediction remains challenging. Traditional machine
learning approaches often rely only on structured features and achieve moderate
accuracy, while large language models (LLMs) offer rich reasoning abilities but
struggle to adapt directly to domain-specific business data. We present
\textbf{CrunchLLM}, a domain-adapted LLM framework for startup success
prediction. CrunchLLM integrates structured company attributes with
unstructured textual narratives and applies parameter-efficient fine-tuning
strategies alongside prompt optimization to specialize foundation models for
entrepreneurship data. Our approach achieves accuracy exceeding 80\% on
Crunchbase startup success prediction, significantly outperforming traditional
classifiers and baseline LLMs. Beyond predictive performance, CrunchLLM
provides interpretable reasoning traces that justify its predictions, enhancing
transparency and trustworthiness for financial and policy decision makers. This
work demonstrates how adapting LLMs with domain-aware fine-tuning and
structured--unstructured data fusion can advance predictive modeling of
entrepreneurial outcomes. CrunchLLM contributes a methodological framework and
a practical tool for data-driven decision making in venture capital and
innovation policy.

</details>
