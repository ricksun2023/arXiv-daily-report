{"id": "2506.15833", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2506.15833", "abs": "https://arxiv.org/abs/2506.15833", "authors": ["Kevin Foley", "Shaghayegh Agah", "Kavya Priyanka Kakinada"], "title": "Architecture is All You Need: Improving LLM Recommenders by Dropping the Text", "comment": "7 pages, 1 figure", "summary": "In recent years, there has been an explosion of interest in the applications\nof large pre-trained language models (PLMs) to recommender systems, with many\nstudies showing strong performance of PLMs on common benchmark datasets.\nPLM-based recommender models benefit from flexible and customizable prompting,\nan unlimited vocabulary of recommendable items, and general ``world knowledge''\nacquired through pre-training on massive text corpora. While PLM-based\nrecommenders show promise in settings where data is limited, they are hard to\nimplement in practice due to their large size and computational cost.\nAdditionally, fine-tuning PLMs to improve performance on collaborative signals\nmay degrade the model's capacity for world knowledge and generalizability. We\npropose a recommender model that uses the architecture of large language models\n(LLMs) while reducing layer count and dimensions and replacing the text-based\nsubword tokenization of a typical LLM with discrete tokens that uniquely\nrepresent individual content items. We find that this simplified approach\nsubstantially outperforms both traditional sequential recommender models and\nPLM-based recommender models at a tiny fraction of the size and computational\ncomplexity of PLM-based models. Our results suggest that the principal benefit\nof LLMs in recommender systems is their architecture, rather than the world\nknowledge acquired during extensive pre-training.", "AI": {"tldr": "We propose a recommender model that uses the architecture of large language models (LLMs) while reducing layer count and dimensions and replacing the text-based subword tokenization of a typical LLM with discrete tokens that uniquely represent individual content items. We find that this simplified approach substantially outperforms both traditional sequential recommender models and PLM-based recommender models at a tiny fraction of the size and computational complexity of PLM-based models.", "motivation": "PLM-based recommenders show promise in settings where data is limited, they are hard to implement in practice due to their large size and computational cost. Additionally, fine-tuning PLMs to improve performance on collaborative signals may degrade the model's capacity for world knowledge and generalizability", "method": "a recommender model that uses the architecture of large language models (LLMs) while reducing layer count and dimensions and replacing the text-based subword tokenization of a typical LLM with discrete tokens that uniquely represent individual content items", "result": "this simplified approach substantially outperforms both traditional sequential recommender models and PLM-based recommender models at a tiny fraction of the size and computational complexity of PLM-based models", "conclusion": "the principal benefit of LLMs in recommender systems is their architecture, rather than the world knowledge acquired during extensive pre-training"}}
{"id": "2506.15862", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15862", "abs": "https://arxiv.org/abs/2506.15862", "authors": ["Jushaan Singh Kalra", "Xinran Zhao", "To Eun Kim", "Fengyu Cai", "Fernando Diaz", "Tongshuang Wu"], "title": "MoR: Better Handling Diverse Queries with a Mixture of Sparse, Dense, and Human Retrievers", "comment": "19 pages, 3 figures", "summary": "Retrieval-augmented Generation (RAG) is powerful, but its effectiveness\nhinges on which retrievers we use and how. Different retrievers offer distinct,\noften complementary signals: BM25 captures lexical matches; dense retrievers,\nsemantic similarity. Yet in practice, we typically fix a single retriever based\non heuristics, which fails to generalize across diverse information needs. Can\nwe dynamically select and integrate multiple retrievers for each individual\nquery, without the need for manual selection? In our work, we validate this\nintuition with quantitative analysis and introduce mixture of retrievers: a\nzero-shot, weighted combination of heterogeneous retrievers. Extensive\nexperiments show that such mixtures are effective and efficient: Despite\ntotaling just 0.8B parameters, this mixture outperforms every individual\nretriever and even larger 7B models by +10.8% and +3.9% on average,\nrespectively. Further analysis also shows that this mixture framework can help\nincorporate specialized non-oracle human information sources as retrievers to\nachieve good collaboration, with a 58.9% relative performance improvement over\nsimulated humans alone.", "AI": {"tldr": "The paper introduces a mixture of retrievers that dynamically combines heterogeneous retrievers for improved RAG performance, outperforming single retrievers and larger models.", "motivation": "Retrieval-augmented Generation (RAG) effectiveness depends on the choice of retrievers, but typically a single retriever is fixed, failing to generalize across diverse information needs. The paper explores dynamically selecting and integrating multiple retrievers for each query.", "method": "Introduced mixture of retrievers: a zero-shot, weighted combination of heterogeneous retrievers.", "result": "The mixture outperforms every individual retriever and even larger 7B models by +10.8% and +3.9% on average, respectively. Achieves a 58.9% relative performance improvement over simulated humans alone when incorporating specialized non-oracle human information sources.", "conclusion": "Mixture of retrievers is effective and efficient, outperforming individual retrievers and larger models. It also facilitates collaboration with human information sources."}}
{"id": "2506.16003", "categories": ["cs.IR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.16003", "abs": "https://arxiv.org/abs/2506.16003", "authors": ["Tan Loc Nguyen", "Tin T. Tran"], "title": "SEP-GCN: Leveraging Similar Edge Pairs with Temporal and Spatial Contexts for Location-Based Recommender Systems", "comment": "Accepted for ACM SIGIR Conference on Innovative Concepts and Theories\n  in Information Retrieval (ICTIR) 2025, Padua, Itay", "summary": "Recommender systems play a crucial role in enabling personalized content\ndelivery amidst the challenges of information overload and human mobility.\nAlthough conventional methods often rely on interaction matrices or graph-based\nretrieval, recent approaches have sought to exploit contextual signals such as\ntime and location. However, most existing models focus on node-level\nrepresentation or isolated edge attributes, underutilizing the relational\nstructure between interactions. We propose SEP-GCN, a novel graph-based\nrecommendation framework that learns from pairs of contextually similar\ninteraction edges, each representing a user-item check-in event. By identifying\nedge pairs that occur within similar temporal windows or geographic proximity,\nSEP-GCN augments the user-item graph with contextual similarity links. These\nlinks bridge distant but semantically related interactions, enabling improved\nlong-range information propagation. The enriched graph is processed via an\nedge-aware convolutional mechanism that integrates contextual similarity into\nthe message-passing process. This allows SEP-GCN to model user preferences more\naccurately and robustly, especially in sparse or dynamic environments.\nExperiments on benchmark data sets show that SEP-GCN consistently outperforms\nstrong baselines in both predictive accuracy and robustness.", "AI": {"tldr": "SEP-GCN\u662f\u4e00\u79cd\u5229\u7528\u4e0a\u4e0b\u6587\u76f8\u4f3c\u6027\u94fe\u63a5\u589e\u5f3a\u7528\u6237-\u9879\u76ee\u56fe\u7684\u65b0\u578b\u56fe\u63a8\u8350\u6846\u67b6\uff0c\u5728\u63a8\u8350\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u63a8\u8350\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u4ea4\u4e92\u77e9\u9635\u6216\u57fa\u4e8e\u56fe\u7684\u68c0\u7d22\uff0c\u4f46\u6700\u8fd1\u7684\u65b9\u6cd5\u8bd5\u56fe\u5229\u7528\u65f6\u95f4\u548c\u4f4d\u7f6e\u7b49\u4e0a\u4e0b\u6587\u4fe1\u53f7\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u6a21\u578b\u4fa7\u91cd\u4e8e\u8282\u70b9\u7ea7\u8868\u793a\u6216\u5b64\u7acb\u7684\u8fb9\u7f18\u5c5e\u6027\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u4ea4\u4e92\u4e4b\u95f4\u7684\u5173\u7cfb\u7ed3\u6784\u3002", "method": "SEP-GCN\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u56fe\u7684\u63a8\u8350\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u6210\u5bf9\u7684\u4e0a\u4e0b\u6587\u76f8\u4f3c\u7684\u4ea4\u4e92\u8fb9\u4e2d\u5b66\u4e60\uff0c\u6bcf\u4e2a\u4ea4\u4e92\u8fb9\u4ee3\u8868\u4e00\u4e2a\u7528\u6237-\u9879\u76ee\u7b7e\u5165\u4e8b\u4ef6\u3002\u901a\u8fc7\u8bc6\u522b\u5728\u76f8\u4f3c\u7684\u65f6\u95f4\u7a97\u53e3\u6216\u5730\u7406\u4f4d\u7f6e\u8303\u56f4\u5185\u53d1\u751f\u7684\u8fb9\u7f18\u5bf9\uff0cSEP-GCN\u4f7f\u7528\u4e0a\u4e0b\u6587\u76f8\u4f3c\u6027\u94fe\u63a5\u6765\u589e\u5f3a\u7528\u6237-\u9879\u76ee\u56fe\u3002\u7136\u540e\u901a\u8fc7\u8fb9\u7f18\u611f\u77e5\u5377\u79ef\u673a\u5236\u5904\u7406\u4e30\u5bcc\u7684\u56fe\uff0c\u8be5\u673a\u5236\u5c06\u4e0a\u4e0b\u6587\u76f8\u4f3c\u6027\u96c6\u6210\u5230\u6d88\u606f\u4f20\u9012\u8fc7\u7a0b\u4e2d\u3002", "result": "SEP-GCN\u53ef\u4ee5\u66f4\u51c6\u786e\uff0c\u66f4\u7a33\u5065\u5730\u5bf9\u7528\u6237\u504f\u597d\u8fdb\u884c\u5efa\u6a21\uff0c\u5c24\u5176\u662f\u5728\u7a00\u758f\u6216\u52a8\u6001\u73af\u5883\u4e2d\u3002\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e", "conclusion": "SEP-GCN\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u3002"}}
{"id": "2506.15831", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2506.15831", "abs": "https://arxiv.org/abs/2506.15831", "authors": ["Jongjun Park", "Fei Chiang", "Mostafa Milani"], "title": "Adaptive Anomaly Detection in the Presence of Concept Drift: Extended Report", "comment": "Extended version (to be updated)", "summary": "Data changes to reflect evolving user behaviour, preferences, and changes in\nthe environment. Such changes may occur due to expected shifts in the data\ndistribution, i.e., concept drift, or unexpected anomalous changes. The\npresence of concept drift poses challenges for anomaly detection in time\nseries. While anomalies are caused by undesirable changes in the data,\ndifferentiating abnormal changes from varying normal behaviours is difficult\ndue to differing frequencies of occurrence, varying time intervals when normal\npatterns occur. Differentiating between concept drift and anomalies is critical\nfor accurate analysis as studies have shown that the compounding effects of\nerror propagation in downstream data analysis tasks lead to lower detection\naccuracy and increased overhead due to unnecessary model updates.\nUnfortunately, existing work has largely explored anomaly detection and concept\ndrift detection in isolation. We develop AnDri, a system for Anomaly detection\nin the presence of Drift, which adjusts the normal patterns temporally, and\ndistinguish abnormal subsequences and new concepts. Moreover, it introduces a\nnew clustering method, Adjacent Hierarchical Clustering (AHC), which groups\nsimilar subsequences while respecting their temporal locality.", "AI": {"tldr": "Data changes to reflect evolving user behaviour, preferences, and changes in\nthe environment. Differentiating between concept drift and anomalies is critical\nfor accurate analysis", "motivation": "The\npresence of concept drift poses challenges for anomaly detection in time\nseries. Differentiating between concept drift and anomalies is critical\nfor accurate analysis as studies have shown that the compounding effects of\nerror propagation in downstream data analysis tasks lead to lower detection\naccuracy and increased overhead due to unnecessary model updates.Unfortunately, existing work has largely explored anomaly detection and concept\ndrift detection in isolation.", "method": "It introduces a\nnew clustering method, Adjacent Hierarchical Clustering (AHC), which groups\nsimilar subsequences while respecting their temporal locality.", "result": "differentiating abnormal changes from varying normal behaviours is difficult\ndue to differing frequencies of occurrence, varying time intervals when normal\npatterns occur.", "conclusion": "We develop AnDri, a system for Anomaly detection\nin the presence of Drift, which adjusts the normal patterns temporally, and\ndistinguish abnormal subsequences and new concepts."}}
{"id": "2506.16114", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16114", "abs": "https://arxiv.org/abs/2506.16114", "authors": ["Yejing Wang", "Shengyu Zhou", "Jinyu Lu", "Qidong Liu", "Xinhang Li", "Wenlin Zhang", "Feng Li", "Pengjie Wang", "Jian Xu", "Bo Zheng", "Xiangyu Zhao"], "title": "GFlowGR: Fine-tuning Generative Recommendation Frameworks with Generative Flow Networks", "comment": null, "summary": "Generative recommendations (GR), which usually include item tokenizers and\ngenerative Large Language Models (LLMs), have demonstrated remarkable success\nacross a wide range of scenarios. The majority of existing research efforts\nprimarily concentrate on developing powerful item tokenizers or advancing LLM\ndecoding strategies to attain superior performance. However, the critical\nfine-tuning step in GR frameworks, which is essential for adapting LLMs to\nrecommendation data, remains largely unexplored. Current approaches\npredominantly rely on either the next-token prediction loss of supervised\nfine-tuning (SFT) or recommendationspecific direct preference optimization\n(DPO) strategies. Both methods ignore the exploration of possible positive\nunobserved samples, which is commonly referred to as the exposure bias problem.\nTo mitigate this problem, this paper treats the GR as a multi-step generation\ntask and constructs a GFlowNets-based fine-tuning framework (GFlowGR). The\nproposed framework integrates collaborative knowledge from traditional\nrecommender systems to create an adaptive trajectory sampler and a\ncomprehensive reward model. Leveraging the diverse generation property of\nGFlowNets, along with sampling and heuristic weighting techniques, GFlowGR\nemerges as a promising approach to mitigate the exposure bias problem.\nExtensive empirical results on two real-world datasets and with two different\nGR backbones highlight the effectiveness and robustness of GFlowGR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGFlowNets\u7684\u5fae\u8c03\u6846\u67b6(GFlowGR)\uff0c\u4ee5\u51cf\u8f7b\u751f\u6210\u5f0f\u63a8\u8350\u4e2d\u7684\u66b4\u9732\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7684\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u76d1\u7763\u5fae\u8c03(SFT)\u7684\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u635f\u5931\u6216\u7279\u5b9a\u4e8e\u63a8\u8350\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316(DPO)\u7b56\u7565\u3002\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u5ffd\u7565\u4e86\u5bf9\u53ef\u80fd\u7684\u79ef\u6781\u672a\u89c2\u5bdf\u5230\u7684\u6837\u672c\u7684\u63a2\u7d22\uff0c\u8fd9\u901a\u5e38\u88ab\u79f0\u4e3a\u66b4\u9732\u504f\u5dee\u95ee\u9898\u3002", "method": "\u5c06GR\u89c6\u4e3a\u591a\u6b65\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u6784\u5efa\u4e00\u4e2a\u57fa\u4e8eGFlowNets\u7684\u5fae\u8c03\u6846\u67b6(GFlowGR)\u3002\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u6765\u81ea\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u7684\u534f\u4f5c\u77e5\u8bc6\uff0c\u4ee5\u521b\u5efa\u4e00\u4e2a\u81ea\u9002\u5e94\u8f68\u8ff9\u91c7\u6837\u5668\u548c\u4e00\u4e2a\u7efc\u5408\u5956\u52b1\u6a21\u578b\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u4e0d\u540c\u7684GR\u4e3b\u5e72\u4e0a\u7684\u5927\u91cf\u7ecf\u9a8c\u7ed3\u679c\u7a81\u51fa\u4e86GFlowGR\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "GFlowGR\u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u7f13\u89e3\u66b4\u9732\u504f\u5dee\u95ee\u9898\u3002\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u4e0d\u540c\u7684GR\u4e3b\u5e72\u4e0a\u7684\u5927\u91cf\u7ecf\u9a8c\u7ed3\u679c\u7a81\u51fa\u4e86GFlowGR\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.15848", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2506.15848", "abs": "https://arxiv.org/abs/2506.15848", "authors": ["Jiazhen Peng", "Zheng Qu", "Xiaoye Miao", "Rong Zhu"], "title": "Delta: A Learned Mixed Cost-based Query Optimization Framework", "comment": null, "summary": "Query optimizer is a crucial module for database management systems. Existing\noptimizers exhibit two flawed paradigms: (1) cost-based optimizers use dynamic\nprogramming with cost models but face search space explosion and heuristic\npruning constraints; (2) value-based ones train value networks to enable\nefficient beam search, but incur higher training costs and lower accuracy. They\nalso lack mechanisms to detect queries where they may perform poorly. To\ndetermine more efficient plans, we propose Delta, a mixed cost-based query\noptimization framework that consists of a compatible query detector and a\ntwo-stage planner. Delta first employs a Mahalanobis distancebased detector to\npreemptively filter out incompatible queries where the planner might perform\npoorly. For compatible queries, Delta activates its two-stage mixed cost-based\nplanner. Stage I serves as a coarse-grained filter to generate high-quality\ncandidate plans based on the value network via beam search, relaxing precision\nrequirements and narrowing the search space. Stage II employs a fine-grained\nranker to determine the best plan from the candidate plans based on a learned\ncost model. Moreover, to reduce training costs, we reuse and augment the\ntraining data from stage I to train the model in stage II. Experimental results\non three workloads demonstrate that Delta identifies higher-quality plans,\nachieving an average 2.34x speedup over PostgreSQL and outperforming the\nstate-of-the-art learned methods by 2.21x.", "AI": {"tldr": "Delta, a mixed cost-based query optimization framework, identifies higher-quality plans and achieves significant speedups compared to existing methods.", "motivation": "Existing optimizers exhibit two flawed paradigms: (1) cost-based optimizers use dynamic programming with cost models but face search space explosion and heuristic pruning constraints; (2) value-based ones train value networks to enable efficient beam search, but incur higher training costs and lower accuracy. They also lack mechanisms to detect queries where they may perform poorly.", "method": "Delta, a mixed cost-based query optimization framework that consists of a compatible query detector and a two-stage planner. Delta first employs a Mahalanobis distancebased detector to preemptively filter out incompatible queries where the planner might perform poorly. For compatible queries, Delta activates its two-stage mixed cost-based planner. Stage I serves as a coarse-grained filter to generate high-quality candidate plans based on the value network via beam search, relaxing precision requirements and narrowing the search space. Stage II employs a fine-grained ranker to determine the best plan from the candidate plans based on a learned cost model. Moreover, to reduce training costs, we reuse and augment the training data from stage I to train the model in stage II.", "result": "achieving an average 2.34x speedup over PostgreSQL and outperforming the state-of-the-art learned methods by 2.21x.", "conclusion": "Delta identifies higher-quality plans, achieving an average 2.34x speedup over PostgreSQL and outperforming the state-of-the-art learned methods by 2.21x."}}
{"id": "2506.15732", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15732", "abs": "https://arxiv.org/abs/2506.15732", "authors": ["Khurram Yamin", "Gaurav Ghosal", "Bryan Wilder"], "title": "LLMs Struggle to Perform Counterfactual Reasoning with Parametric Knowledge", "comment": "ICML 2025 Workshop on Scaling up Intervention Models", "summary": "Large Language Models have been shown to contain extensive world knowledge in\ntheir parameters, enabling impressive performance on many knowledge intensive\ntasks. However, when deployed in novel settings, LLMs often encounter\nsituations where they must integrate parametric knowledge with new or\nunfamiliar information. In this work, we explore whether LLMs can combine\nknowledge in-context with their parametric knowledge through the lens of\ncounterfactual reasoning. Through synthetic and real experiments in multi-hop\nreasoning problems, we show that LLMs generally struggle with counterfactual\nreasoning, often resorting to exclusively using their parametric knowledge.\nMoreover, we show that simple post-hoc finetuning can struggle to instill\ncounterfactual reasoning ability -- often leading to degradation in stored\nparametric knowledge. Ultimately, our work reveals important limitations of\ncurrent LLM's abilities to re-purpose parametric knowledge in novel settings.", "AI": {"tldr": "LLMs struggle with counterfactual reasoning and integrating new information with existing knowledge, and finetuning doesn't always help.", "motivation": "LLMs often encounter situations where they must integrate parametric knowledge with new or unfamiliar information.", "method": "synthetic and real experiments in multi-hop reasoning problems", "result": "LLMs generally struggle with counterfactual reasoning, often resorting to exclusively using their parametric knowledge. simple post-hoc finetuning can struggle to instill counterfactual reasoning ability -- often leading to degradation in stored parametric knowledge.", "conclusion": "current LLM's abilities to re-purpose parametric knowledge in novel settings have limitations."}}
