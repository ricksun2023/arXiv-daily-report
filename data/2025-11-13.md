<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 2]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Where did you get that? Towards Summarization Attribution for Analysts](https://arxiv.org/abs/2511.08589)
*Violet B,John M. Conroy,Sean Lynch,Danielle M,Neil P. Molino,Aaron Wiechmann,Julia S. Yang*

Main category: cs.CL

TL;DR: 这篇论文探讨了自动归因方法，旨在将摘要中的每个句子链接到源文本的部分，可能位于一个或多个文档中。


<details>
  <summary>Details</summary>
Motivation: 分析师需要归因，因为在不了解信息来源的情况下，任何内容都无法报告。本文侧重于自动归因方法。

Method: 我们探索使用混合摘要，即提取式摘要的自动释义，以简化归因。我们还使用自定义拓扑来识别不同类别的归因相关错误的比例。

Result: 无

Conclusion: 无

Abstract: Analysts require attribution, as nothing can be reported without knowing the source of the information. In this paper, we will focus on automatic methods for attribution, linking each sentence in the summary to a portion of the source text, which may be in one or more documents. We explore using a hybrid summarization, i.e., an automatic paraphrase of an extractive summary, to ease attribution. We also use a custom topology to identify the proportion of different categories of attribution-related errors.

</details>


### [2] [Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning](https://arxiv.org/abs/2511.09109)
*Wenda Wei,Yu-An Liu,Ruqing Zhang,Jiafeng Guo,Lixin Su,Shuaiqiang Wang,Dawei Yin,Maarten de Rijke,Xueqi Cheng*

Main category: cs.CL

TL;DR: Bi-RAR: A retrieval-augmented reasoning framework for complex, multi-step reasoning that mitigates reward hacking and improves response quality.


<details>
  <summary>Details</summary>
Motivation: Existing RAG methods are limited in complex reasoning and lack explicit guidance for intermediate steps, leading to reward hacking and degraded response quality.

Method: Proposes Bi-RAR, which evaluates each intermediate step jointly in both forward and backward directions using a bidirectional information distance grounded in Kolmogorov complexity.

Result: Bi-RAR surpasses previous methods on seven question answering benchmarks and enables efficient interaction and reasoning with the search engine.

Conclusion: Bi-RAR is effective for complex reasoning by providing bidirectional evaluation of intermediate steps and optimizing reasoning with multi-objective reinforcement learning.

Abstract: Retrieval-augmented generation (RAG) has proven to be effective in mitigating hallucinations in large language models, yet its effectiveness remains limited in complex, multi-step reasoning scenarios.Recent efforts have incorporated search-based interactions into RAG, enabling iterative reasoning with real-time retrieval. Most approaches rely on outcome-based supervision, offering no explicit guidance for intermediate steps. This often leads to reward hacking and degraded response quality. We propose Bi-RAR, a novel retrieval-augmented reasoning framework that evaluates each intermediate step jointly in both forward and backward directions. To assess the information completeness of each step, we introduce a bidirectional information distance grounded in Kolmogorov complexity, approximated via language model generation probabilities. This quantification measures both how far the current reasoning is from the answer and how well it addresses the question. To optimize reasoning under these bidirectional signals, we adopt a multi-objective reinforcement learning framework with a cascading reward structure that emphasizes early trajectory alignment. Empirical results on seven question answering benchmarks demonstrate that Bi-RAR surpasses previous methods and enables efficient interaction and reasoning with the search engine during training and inference.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [3] [Case Study: Transformer-Based Solution for the Automatic Digitization of Gas Plants](https://arxiv.org/abs/2511.08609)
*I. Bailo,F. Buonora,G. Ciarfaglia,L. T. Consoli,A. Evangelista,M. Gabusi,M. Ghiani,C. Petracca Ciavarella,F. Picariello,F. Sarcina,F. Tuosto,V. Zullo,L. Airoldi,G. Bruno,D. D. Gobbo,S. Pezzenati,G. A. Tona*

Main category: cs.CV

TL;DR: 本研究利用生成式人工智能模型，自动化意大利天然气运输公司SNAM能源基础设施的工厂结构采集，实现能源基础设施的数字化。


<details>
  <summary>Details</summary>
Motivation: 能源转型和生态可持续发展离不开数字化、创新和新技术工具，为了简化MGM用户的日常工作，实现工厂信息的自动化提取。

Method: 该方案以工厂的P&ID图（PDF格式）为输入，采用OCR、视觉LLM、目标检测、关系推理和优化算法，输出结构化的设计数据概览和工厂的层级框架。通过扩展最先进的场景图生成模型，引入全新的Transformer架构，加深对工厂组件之间复杂关系的分析。

Result: 文本信息提取准确率达到91%，组件识别准确率达到93%，层级结构提取准确率约为80%。

Conclusion: 通过协同使用人工智能技术，克服了因数据缺乏标准化而导致的多样性障碍。

Abstract: The energy transition is a key theme of the last decades to determine a future of eco-sustainability, and an area of such importance cannot disregard digitization, innovation and the new technological tools available. This is the context in which the Generative Artificial Intelligence models described in this paper are positioned, developed by Engineering Ingegneria Informatica SpA in order to automate the plant structures acquisition of SNAM energy infrastructure, a leading gas transportation company in Italy and Europe. The digitization of a gas plant consists in registering all its relevant information through the interpretation of the related documentation. The aim of this work is therefore to design an effective solution based on Artificial Intelligence techniques to automate the extraction of the information necessary for the digitization of a plant, in order to streamline the daily work of MGM users. The solution received the P&ID of the plant as input, each one in pdf format, and uses OCR, Vision LLM, Object Detection, Relational Reasoning and optimization algorithms to return an output consisting of two sets of information: a structured overview of the relevant design data and the hierarchical framework of the plant. To achieve convincing results, we extend a state-of-the-art model for Scene Graph Generation introducing a brand new Transformer architecture with the aim of deepening the analysis of the complex relations between the plant's components. The synergistic use of the listed AI-based technologies allowed to overcome many obstacles arising from the high variety of data, due to the lack of standardization. An accuracy of 91\% has been achieved in the extraction of textual information relating to design data. Regarding the plants topology, 93\% of components are correctly identified and the hierarchical structure is extracted with an accuracy around 80\%.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [4] [Bridging Natural Language and ASP: A Hybrid Approach Using LLMs and AMR Parsing](https://arxiv.org/abs/2511.08715)
*Connar Hite,Sean Saud,Raef Taha,Nayim Rahman,Tanvir Atahary,Scott Douglass,Tarek Taha*

Main category: cs.AI

TL;DR: 本文提出了一种使用LLM和抽象意义表示(AMR)图将无约束英语翻译成ASP程序的新方法，用于解决逻辑谜题。


<details>
  <summary>Details</summary>
Motivation: 许多不熟悉编程语言的人需要与代码交互，而ASP是一种描述和解决组合问题的强大工具，但需要用户学习其工作原理和语法。

Method: 该系统利用LLM简化自然语言句子，识别关键词，并生成简单的事实。然后从简化的语言中解析AMR图，并用于系统地生成ASP约束。

Result: 该系统成功地创建了一个完整的ASP程序，解决了组合逻辑问题。

Conclusion: 这种方法是在创建一个更轻量级、可解释的系统方面迈出的重要一步，该系统可以将自然语言转换为解决复杂的逻辑问题。

Abstract: Answer Set Programming (ASP) is a declarative programming paradigm based on logic programming and non-monotonic reasoning. It is a tremendously powerful tool for describing and solving combinatorial problems. Like any other language, ASP requires users to learn how it works and the syntax involved. It is becoming increasingly required for those unfamiliar with programming languages to interact with code. This paper proposes a novel method of translating unconstrained English into ASP programs for logic puzzles using an LLM and Abstract Meaning Representation (AMR) graphs. Everything from ASP rules, facts, and constraints is generated to fully represent and solve the desired problem. Example logic puzzles are used to demonstrate the capabilities of the system. While most current methods rely entirely on an LLM, our system minimizes the role of the LLM only to complete straightforward tasks. The LLM is used to simplify natural language sentences, identify keywords, and generate simple facts. The AMR graphs are then parsed from simplified language and used to generate ASP constraints systematically. The system successfully creates an entire ASP program that solves a combinatorial logic problem. This approach is a significant first step in creating a lighter-weight, explainable system that converts natural language to solve complex logic problems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [5] [FlashMap: A Flash Optimized Key-Value Store](https://arxiv.org/abs/2511.08826)
*Zonglin Guo,Tony Givargis*

Main category: cs.DB

TL;DR: 这篇论文介绍了一个名为FlashMap的键值存储系统，它针对基于闪存的固态硬盘(ssd)进行了优化，实现了卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 现代计算越来越需要响应能力和可扩展性，键值存储已成为工业和研究领域数据基础设施的关键组成部分。

Method: 介绍 FlashMap 键值存储系统。

Result: FlashMap实现了出色的吞吐量，平均每秒可进行1980万次插入和2380万次随机查找(有效负载为100字节)。

Conclusion: FlashMap是一个高性能的键值存储系统。

Abstract: Key-value stores are a fundamental class of NoSQL databases that offer a simple yet powerful model for data storage and retrieval, representing information as pairs of unique keys and associated values. Their minimal structure enables exceptionally fast access times, scalability, and flexibility in storing diverse data types, making them ideal for high-performance applications such as caching, session management, and distributed systems. As modern computing increasingly demands responsiveness and scalability, key-value stores have become a critical component of the data infrastructure in both industry and research contexts. In this work, we present FlashMap, a high-performance key-value store optimized for Flash-based solid-state drives (SSDs). Experiments show that FlashMap achieves outstanding throughput, averaging 19.8 million inserts and 23.8 million random lookups per second with a 100-byte payload, all on a single data center-grade server.

</details>


### [6] [Contextual Graph Embeddings: Accounting for Data Characteristics in Heterogeneous Data Integration](https://arxiv.org/abs/2511.09001)
*Yuka Haruki,Shigeru Ishikura,Kazuya Demachi,Teruaki Hayashi*

Main category: cs.DB

TL;DR: 本研究提出了一种上下文图嵌入技术，用于整合表格数据的结构细节和上下文元素，以提高模式匹配和实体解析的效果。


<details>
  <summary>Details</summary>
Motivation: 当前数据集成过程中模式匹配和实体解析等关键任务非常耗时，且数据集特征对匹配效果的影响以及不同方法的组合应用研究不足。

Method: 提出一种上下文图嵌入技术，整合表格数据的结构细节和上下文元素。

Result: 在不同属性的数据集上进行的测试表明，该方法始终优于现有的基于图的方法，尤其是在高比例数值或大量缺失数据的困难场景中。

Conclusion: 上下文嵌入增强了匹配可靠性，数据集特征显著影响集成结果。

Abstract: As organizations continue to access diverse datasets, the demand for effective data integration has increased. Key tasks in this process, such as schema matching and entity resolution, are essential but often require significant effort. Although previous studies have aimed to automate these tasks, the influence of dataset characteristics on the matching effectiveness has not been thoroughly examined, and combinations of different methods remain limited. This study introduces a contextual graph embedding technique that integrates structural details from tabular data and contextual elements such as column descriptions and external knowledge. Tests conducted on datasets with varying properties such as domain specificity, data size, missing rate, and overlap rate showed that our approach consistently surpassed existing graph-based methods, especially in difficult scenarios such those with a high proportion of numerical values or significant missing data. However, we identified specific failure cases, such as columns that were semantically similar but distinct, which remains a challenge for our method. The study highlights two main insights: (i) contextual embeddings enhance the matching reliability, and (ii) dataset characteristics significantly affect the integration outcomes. These contributions can advance the development of practical data integration systems that can support real-world enterprise applications.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [7] [Efficient Model-Agnostic Continual Learning for Next POI Recommendation](https://arxiv.org/abs/2511.08941)
*Chenhao Wang,Shanshan Feng,Lisi Chen,Fan Li,Shuo Shang*

Main category: cs.IR

TL;DR: 提出了一种名为GIRAM的框架，用于解决持续的下一POI推荐任务，该框架能够动态适应用户兴趣的变化，同时保持效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态数据集和固定模型，无法适应用户行为随时间的变化。

Method: GIRAM框架包含兴趣记忆、上下文感知键编码模块、生成键检索模块和自适应兴趣更新与融合模块。

Result: 在三个真实世界数据集上的实验表明，GIRAM始终优于现有方法，同时保持较高的更新时间和内存消耗效率。

Conclusion: GIRAM框架能够有效地解决持续的下一POI推荐任务，并具有实际部署的潜力。

Abstract: Next point-of-interest (POI) recommendation improves personalized location-based services by predicting users' next destinations based on their historical check-ins. However, most existing methods rely on static datasets and fixed models, limiting their ability to adapt to changes in user behavior over time. To address this limitation, we explore a novel task termed continual next POI recommendation, where models dynamically adapt to evolving user interests through continual updates. This task is particularly challenging, as it requires capturing shifting user behaviors while retaining previously learned knowledge. Moreover, it is essential to ensure efficiency in update time and memory usage for real-world deployment. To this end, we propose GIRAM (Generative Key-based Interest Retrieval and Adaptive Modeling), an efficient, model-agnostic framework that integrates context-aware sustained interests with recent interests. GIRAM comprises four components: (1) an interest memory to preserve historical preferences; (2) a context-aware key encoding module for unified interest key representation; (3) a generative key-based retrieval module to identify diverse and relevant sustained interests; and (4) an adaptive interest update and fusion module to update the interest memory and balance sustained and recent interests. In particular, GIRAM can be seamlessly integrated with existing next POI recommendation models. Experiments on three real-world datasets demonstrate that GIRAM consistently outperforms state-of-the-art methods while maintaining high efficiency in both update time and memory consumption.

</details>


### [8] [NeuroCLIP: Brain-Inspired Prompt Tuning for EEG-to-Image Multimodal Contrastive Learning](https://arxiv.org/abs/2511.09250)
*Jiyuan Wang,Li Zhang,Haipeng Lin,Qile Liu,Gan Huang,Ziyu Li,Zhen Liang,Xia Wu*

Main category: cs.IR

TL;DR: NeuroCLIP: 通过prompt tuning框架，针对脑电图(EEG)到图像的对比学习，提升了zero-shot图像检索的准确率，并展示了在跨被试条件下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了CLIP模型对神经表征的适应性以及脑电图-图像对齐中固有的生理-符号差距。

Method: 1) 设计双流视觉嵌入管道，结合动态滤波和token级融合，生成实例级自适应提示；2) 引入视觉提示token到脑电图-图像对齐中，作为全局模态级提示；3) 提出改进的对比损失函数，更好地模拟脑电信号中的语义模糊和跨模态噪声。

Result: 在THINGS-EEG2数据集上，NeuroCLIP在zero-shot图像检索中达到63.2%的Top-1准确率，超过了之前最好的方法12.3%，并在跨被试条件下表现出强大的泛化能力（+4.6% Top-1）。

Conclusion: 生理学感知的prompt tuning具有桥接脑信号和视觉语义的潜力。

Abstract: Recent advances in brain-inspired artificial intelligence have sought to align neural signals with visual semantics using multimodal models such as CLIP. However, existing methods often treat CLIP as a static feature extractor, overlooking its adaptability to neural representations and the inherent physiological-symbolic gap in EEG-image alignment. To address these challenges, we present NeuroCLIP, a prompt tuning framework tailored for EEG-to-image contrastive learning. Our approach introduces three core innovations: (1) We design a dual-stream visual embedding pipeline that combines dynamic filtering and token-level fusion to generate instance-level adaptive prompts, which guide the adjustment of patch embedding tokens based on image content, thereby enabling fine-grained modulation of visual representations under neural constraints; (2) We are the first to introduce visual prompt tokens into EEG-image alignment, acting as global, modality-level prompts that work in conjunction with instance-level adjustments. These visual prompt tokens are inserted into the Transformer architecture to facilitate neural-aware adaptation and parameter optimization at a global level; (3) Inspired by neuroscientific principles of human visual encoding, we propose a refined contrastive loss that better model the semantic ambiguity and cross-modal noise present in EEG signals. On the THINGS-EEG2 dataset, NeuroCLIP achieves a Top-1 accuracy of 63.2% in zero-shot image retrieval, surpassing the previous best method by +12.3%, and demonstrates strong generalization under inter-subject conditions (+4.6% Top-1), highlighting the potential of physiology-aware prompt tuning for bridging brain signals and visual semantics.

</details>


### [9] [Sim4IA-Bench: A User Simulation Benchmark Suite for Next Query and Utterance Prediction](https://arxiv.org/abs/2511.09329)
*Andreas Konstantin Kruff,Christin Katharina Kreutz,Timo Breuer,Philipp Schaer,Krisztian Balog*

Main category: cs.IR

TL;DR: 提出了一个名为Sim4IA-Bench的模拟基准测试套件，用于预测下一个查询和话语。


<details>
  <summary>Details</summary>
Motivation: 缺乏已建立的衡量标准和基准，难以评估模拟器是否准确反映了真实用户行为。

Method: 该数据集包含来自CORE搜索引擎的160个真实搜索会话。对于其中的70个会话，提供了多达62个模拟器运行，分为Task A和Task B，其中不同的方法预测用户下一个搜索查询或话语。

Result: Sim4IA-Bench为评估和比较用户模拟方法以及开发新的模拟器有效性度量提供了一个基础。还引入了一种新的度量来评估此任务中的下一个查询预测。

Conclusion: 该套件是第一个将真实搜索会话与模拟的下一个查询预测联系起来的公开基准，旨在促进可重复的研究，并激发对信息访问的现实和可解释的用户模拟的进一步工作。

Abstract: Validating user simulation is a difficult task due to the lack of established measures and benchmarks, which makes it challenging to assess whether a simulator accurately reflects real user behavior. As part of the Sim4IA Micro-Shared Task at the Sim4IA Workshop, SIGIR 2025, we present Sim4IA-Bench, a simulation benchmark suit for the prediction of the next queries and utterances, the first of its kind in the IR community. Our dataset as part of the suite comprises 160 real-world search sessions from the CORE search engine. For 70 of these sessions, up to 62 simulator runs are available, divided into Task A and Task B, in which different approaches predicted users next search queries or utterances. Sim4IA-Bench provides a basis for evaluating and comparing user simulation approaches and for developing new measures of simulator validity. Although modest in size, the suite represents the first publicly available benchmark that links real search sessions with simulated next-query predictions. In addition to serving as a testbed for next query prediction, it also enables exploratory studies on query reformulation behavior, intent drift, and interaction-aware retrieval evaluation. We also introduce a new measure for evaluating next-query predictions in this task. By making the suite publicly available, we aim to promote reproducible research and stimulate further work on realistic and explainable user simulation for information access: https://github.com/irgroup/Sim4IA-Bench.

</details>


### [10] [Practical RAG Evaluation: A Rarity-Aware Set-Based Metric and Cost-Latency-Quality Trade-offs](https://arxiv.org/abs/2511.09545)
*Etienne Dallaire*

Main category: cs.IR

TL;DR: 本论文针对生产RAG中的猜测游戏问题，提出了RA-nWG@K指标、rag-gs黄金数据集管道以及全面的基准测试，以解决传统IR指标不适用、缺乏标准化黄金数据集、缺少端到端基准测试以及embedding模型处理命名实体和会话噪声不透明等问题。


<details>
  <summary>Details</summary>
Motivation: 传统排序中心IR指标不适用于RAG，缺乏标准化和可复现的黄金数据集构建方法，缺少端到端、基于语料库的基准测试，并且现有embedding模型处理命名实体和会话噪声的能力不明确。

Method: 提出了RA-nWG@K指标，一种稀有性感知、按查询归一化的集合评分；rag-gs黄金数据集管道，采用Plackett-Luce列表式优化；构建了基于科学论文语料库的综合基准测试，涵盖密集检索、混合检索、embedding模型、交叉编码器重排序、ANN和量化；以及通过身份破坏和格式消融来量化命名实体识别信号和会话噪声敏感性的诊断方法。

Result: RA-nWG@K指标和rag-gs黄金数据集管道的迭代更新优于单次LLM排序，综合基准测试提供了关于生产RAG的全面评估，并通过诊断方法量化了命名实体识别信号和会话噪声敏感性。

Conclusion: 本论文提供了一套完整的工具，包括RA-nWG@K指标、rag-gs黄金数据集管道、综合基准测试和诊断方法，为从业者提供Pareto指导和可审计的保障措施，以支持可重现的、预算/SLA感知的决策。

Abstract: This paper addresses the guessing game in building production RAG. Classical rank-centric IR metrics (nDCG/MAP/MRR) are a poor fit for RAG, where LLMs consume a set of passages rather than a browsed list; position discounts and prevalence-blind aggregation miss what matters: whether the prompt at cutoff K contains the decisive evidence. Second, there is no standardized, reproducible way to build and audit golden sets. Third, leaderboards exist but lack end-to-end, on-corpus benchmarking that reflects production trade-offs. Fourth, how state-of-the-art embedding models handle proper-name identity signals and conversational noise remains opaque. To address these, we contribute: (1) RA-nWG@K, a rarity-aware, per-query-normalized set score, and operational ceilings via the pool-restricted oracle ceiling (PROC) and the percentage of PROC (%PROC) to separate retrieval from ordering headroom within a Cost-Latency-Quality (CLQ) lens; (2) rag-gs (MIT), a lean golden-set pipeline with Plackett-Luce listwise refinement whose iterative updates outperform single-shot LLM ranking; (3) a comprehensive benchmark on a production RAG (scientific-papers corpus) spanning dense retrieval, hybrid dense+BM25, embedding models and dimensions, cross-encoder rerankers, ANN (HNSW), and quantization; and (4) targeted diagnostics that quantify proper-name identity signal and conversational-noise sensitivity via identity-destroying and formatting ablations. Together, these components provide practitioner Pareto guidance and auditable guardrails to support reproducible, budget/SLA-aware decisions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [11] [A Lightweight CNN-Attention-BiLSTM Architecture for Multi-Class Arrhythmia Classification on Standard and Wearable ECGs](https://arxiv.org/abs/2511.08650)
*Vamsikrishna Thota,Hardik Prajapati,Yuvraj Joshi,Shubhangi Rathi*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级的深度学习模型，用于从12导联和单导联心电图中分类心律失常。


<details>
  <summary>Details</summary>
Motivation: 及时诊断和干预需要早期和准确地检测心律失常。

Method: 该模型结合了1D卷积神经网络(CNN)、注意机制和双向长短期记忆(BiLSTM)。使用类别加权损失来解决类别不平衡问题。

Result: 在CPSC 2018数据集上评估，该模型显示出优于基线模型的准确性和F1分数。

Conclusion: 该模型只有94.5万个参数，非常适合在可穿戴健康监测系统中进行实时部署。

Abstract: Early and accurate detection of cardiac arrhythmias is vital for timely diagnosis and intervention. We propose a lightweight deep learning model combining 1D Convolutional Neural Networks (CNN), attention mechanisms, and Bidirectional Long Short-Term Memory (BiLSTM) for classifying arrhythmias from both 12-lead and single-lead ECGs. Evaluated on the CPSC 2018 dataset, the model addresses class imbalance using a class-weighted loss and demonstrates superior accuracy and F1- scores over baseline models. With only 0.945 million parameters, our model is well-suited for real-time deployment in wearable health monitoring systems.

</details>
