{"id": "2506.19993", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19993", "abs": "https://arxiv.org/abs/2506.19993", "authors": ["Haochen Zhang", "Tianyi Zhang", "Junze Yin", "Oren Gal", "Anshumali Shrivastava", "Vladimir Braverman"], "title": "CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender Systems", "comment": "Accepted by ACL 2025 Findings", "summary": "Recommender systems play a pivotal role in providing relevant content to\nusers. With the rapid development of large language models (LLMs), researchers\nhave begun utilizing LLMs to build more powerful recommender systems. However,\nexisting approaches that focus on aligning LLMs with recommendation tasks do\nnot fully leverage their sequential information processing capabilities,\nleading to suboptimal performance.\n  In this paper, we propose a novel system called compressed vocabulary\nexpansion (CoVE). In CoVE, each item is assigned a unique ID within the\nexpanded vocabulary. Our framework effectively capitalizes on sequence\nunderstanding abilities of LLMs, significantly enhancing their performance on\nrecommendation tasks. Additionally, we compress the embedding layer, making\nCoVE practical for large-scale industrial applications. The effectiveness and\nperformance of CoVE are demonstrated through comprehensive experiments on\nmultiple recommendation datasets and comparisons with prior works. Our code can\nbe found at https://github.com/HaochenZhang717/CoVE-official-Repo.", "AI": {"tldr": "This paper introduces CoVE, a novel recommendation system that leverages the sequence understanding abilities of LLMs by assigning unique IDs to items within an expanded vocabulary and compressing the embedding layer for practical large-scale use.", "motivation": "Existing approaches that focus on aligning LLMs with recommendation tasks do not fully leverage their sequential information processing capabilities, leading to suboptimal performance.", "method": "In CoVE, each item is assigned a unique ID within the expanded vocabulary. Our framework effectively capitalizes on sequence understanding abilities of LLMs, significantly enhancing their performance on recommendation tasks. Additionally, we compress the embedding layer, making CoVE practical for large-scale industrial applications.", "result": "significantly enhancing their performance on recommendation tasks", "conclusion": "The effectiveness and performance of CoVE are demonstrated through comprehensive experiments on multiple recommendation datasets and comparisons with prior works."}}
{"id": "2506.20051", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2506.20051", "abs": "https://arxiv.org/abs/2506.20051", "authors": ["Jia-Huei Ju", "Suzan Verberne", "Maarten de Rijke", "Andrew Yates"], "title": "Controlled Retrieval-augmented Context Evaluation for Long-form RAG", "comment": null, "summary": "Retrieval-augmented generation (RAG) enhances large language models by\nincorporating context retrieved from external knowledge sources. While the\neffectiveness of the retrieval module is typically evaluated with\nrelevance-based ranking metrics, such metrics may be insufficient to reflect\nthe retrieval's impact on the final RAG result, especially in long-form\ngeneration scenarios. We argue that providing a comprehensive\nretrieval-augmented context is important for long-form RAG tasks like report\ngeneration and propose metrics for assessing the context independent of\ngeneration. We introduce CRUX, a \\textbf{C}ontrolled\n\\textbf{R}etrieval-a\\textbf{U}gmented conte\\textbf{X}t evaluation framework\ndesigned to directly assess retrieval-augmented contexts. This framework uses\nhuman-written summaries to control the information scope of knowledge, enabling\nus to measure how well the context covers information essential for long-form\ngeneration. CRUX uses question-based evaluation to assess RAG's retrieval in a\nfine-grained manner. Empirical results show that CRUX offers more reflective\nand diagnostic evaluation. Our findings also reveal substantial room for\nimprovement in current retrieval methods, pointing to promising directions for\nadvancing RAG's retrieval. Our data and code are publicly available to support\nand advance future research on retrieval.", "AI": {"tldr": "CRUX\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30RAG\u68c0\u7d22\u6a21\u5757\u7684\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u4eba\u5de5\u7f16\u5199\u7684\u6458\u8981\u6765\u63a7\u5236\u77e5\u8bc6\u7684\u4fe1\u606f\u8303\u56f4\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u95ee\u9898\u7684\u8bc4\u4f30\u6765\u7ec6\u7c92\u5ea6\u5730\u8bc4\u4f30RAG\u7684\u68c0\u7d22\u3002", "motivation": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u901a\u8fc7\u7ed3\u5408\u4ece\u5916\u90e8\u77e5\u8bc6\u6e90\u68c0\u7d22\u5230\u7684\u4e0a\u4e0b\u6587\u6765\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u867d\u7136\u68c0\u7d22\u6a21\u5757\u7684\u6709\u6548\u6027\u901a\u5e38\u4f7f\u7528\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u6392\u5e8f\u6307\u6807\u6765\u8bc4\u4f30\uff0c\u4f46\u8fd9\u4e9b\u6307\u6807\u53ef\u80fd\u4e0d\u8db3\u4ee5\u53cd\u6620\u68c0\u7d22\u5bf9\u6700\u7ec8RAG\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u957f\u683c\u5f0f\u751f\u6210\u573a\u666f\u4e2d\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\u63d0\u4f9b\u5168\u9762\u7684\u68c0\u7d22\u589e\u5f3a\u4e0a\u4e0b\u6587\u5bf9\u4e8e\u62a5\u544a\u751f\u6210\u7b49\u957f\u683c\u5f0fRAG\u4efb\u52a1\u975e\u5e38\u91cd\u8981\uff0c\u5e76\u63d0\u51fa\u4e86\u7528\u4e8e\u8bc4\u4f30\u72ec\u7acb\u4e8e\u751f\u6210\u7684\u4e0a\u4e0b\u6587\u7684\u6307\u6807\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86CRUX\uff0c\u8fd9\u662f\u4e00\u4e2a\u65e8\u5728\u76f4\u63a5\u8bc4\u4f30\u68c0\u7d22\u589e\u5f3a\u4e0a\u4e0b\u6587\u7684\u53d7\u63a7\u68c0\u7d22\u589e\u5f3a\u4e0a\u4e0b\u6587\u8bc4\u4f30\u6846\u67b6\u3002\u8be5\u6846\u67b6\u4f7f\u7528\u4eba\u5de5\u7f16\u5199\u7684\u6458\u8981\u6765\u63a7\u5236\u77e5\u8bc6\u7684\u4fe1\u606f\u8303\u56f4\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u8861\u91cf\u4e0a\u4e0b\u6587\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u6db5\u76d6\u4e86\u957f\u683c\u5f0f\u751f\u6210\u5fc5\u4e0d\u53ef\u5c11\u7684\u4fe1\u606f\u3002CRUX\u4f7f\u7528\u57fa\u4e8e\u95ee\u9898\u7684\u8bc4\u4f30\u6765\u7ec6\u7c92\u5ea6\u5730\u8bc4\u4f30RAG\u7684\u68c0\u7d22\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cCRUX\u63d0\u4f9b\u66f4\u5177\u53cd\u601d\u6027\u548c\u8bca\u65ad\u6027\u7684\u8bc4\u4f30\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8fd8\u8868\u660e\uff0c\u5f53\u524d\u7684\u68c0\u7d22\u65b9\u6cd5\u4ecd\u6709\u5f88\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\uff0c\u4e3a\u63a8\u8fdbRAG\u7684\u68c0\u7d22\u6307\u660e\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002", "conclusion": "CRUX\u63d0\u4f9b\u66f4\u5177\u53cd\u601d\u6027\u548c\u8bca\u65ad\u6027\u7684\u8bc4\u4f30\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u7684\u68c0\u7d22\u65b9\u6cd5\u4ecd\u6709\u5f88\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\uff0c\u4e3a\u63a8\u8fdbRAG\u7684\u68c0\u7d22\u6307\u660e\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002\u6211\u4eec\u7684\u6570\u636e\u548c\u4ee3\u7801\u662f\u516c\u5f00\u7684\uff0c\u4ee5\u652f\u6301\u548c\u63a8\u8fdb\u672a\u6765\u5bf9\u68c0\u7d22\u7684\u7814\u7a76\u3002"}}
{"id": "2506.20070", "categories": ["cs.IR", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.20070", "abs": "https://arxiv.org/abs/2506.20070", "authors": ["KMA Solaiman", "Bharat Bhargava"], "title": "Multimodal Information Retrieval for Open World with Edit Distance Weak Supervision", "comment": "Submitted to ICDE'24. An earlier version of this paper appeared on\n  TechRxiv: https://www.techrxiv.org/doi/full/10.36227/techrxiv.21990284.v1,\n  uploaded on February 05, 2023", "summary": "Existing multi-media retrieval models either rely on creating a common\nsubspace with modality-specific representation models or require schema mapping\namong modalities to measure similarities among multi-media data. Our goal is to\navoid the annotation overhead incurred from considering retrieval as a\nsupervised classification task and re-use the pretrained encoders in large\nlanguage models and vision tasks. We propose \"FemmIR\", a framework to retrieve\nmultimodal results relevant to information needs expressed with multimodal\nqueries by example without any similarity label. Such identification is\nnecessary for real-world applications where data annotations are scarce and\nsatisfactory performance is required without fine-tuning with a common\nframework across applications. We curate a new dataset called MuQNOL for\nbenchmarking progress on this task. Our technique is based on weak supervision\nintroduced through edit distance between samples: graph edit distance can be\nmodified to consider the cost of replacing a data sample in terms of its\nproperties, and relevance can be measured through the implicit signal from the\namount of edit cost among the objects. Unlike metric learning or encoding\nnetworks, FemmIR re-uses the high-level properties and maintains the property\nvalue and relationship constraints with a multi-level interaction score between\ndata samples and the query example provided by the user. We empirically\nevaluate FemmIR on a missing person use case with MuQNOL. FemmIR performs\ncomparably to similar retrieval systems in delivering on-demand retrieval\nresults with exact and approximate similarities while using the existing\nproperty identifiers in the system.", "AI": {"tldr": "FemmIR is a framework to retrieve multimodal results relevant to information needs expressed with multimodal queries by example without any similarity label, re-using the pretrained encoders in large language models and vision tasks. A new dataset called MuQNOL is curated for benchmarking progress on this task. FemmIR performs comparably to similar retrieval systems.", "motivation": "Existing multi-media retrieval models either rely on creating a common subspace with modality-specific representation models or require schema mapping among modalities to measure similarities among multi-media data. The goal is to avoid the annotation overhead incurred from considering retrieval as a supervised classification task and re-use the pretrained encoders in large language models and vision tasks. Identification is necessary for real-world applications where data annotations are scarce and satisfactory performance is required without fine-tuning with a common framework across applications.", "method": "FemmIR re-uses the high-level properties and maintains the property value and relationship constraints with a multi-level interaction score between data samples and the query example provided by the user. The technique is based on weak supervision introduced through edit distance between samples: graph edit distance can be modified to consider the cost of replacing a data sample in terms of its properties, and relevance can be measured through the implicit signal from the amount of edit cost among the objects.", "result": "FemmIR performs comparably to similar retrieval systems in delivering on-demand retrieval results with exact and approximate similarities while using the existing property identifiers in the system. A new dataset called MuQNOL is curated for benchmarking progress on this task.", "conclusion": "FemmIR performs comparably to similar retrieval systems in delivering on-demand retrieval results with exact and approximate similarities while using the existing property identifiers in the system."}}
{"id": "2506.20330", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2506.20330", "abs": "https://arxiv.org/abs/2506.20330", "authors": ["Zhigong Zhou", "Ning Ding", "Xiaochuan Fan", "Yue Shang", "Yiming Qiu", "Jingwei Zhuo", "Zhiwei Ge", "Songlin Wang", "Lin Liu", "Sulong Xu", "Han Zhang"], "title": "Semantic-enhanced Modality-asymmetric Retrieval for Online E-commerce Search", "comment": "published in sigir2023", "summary": "Semantic retrieval, which retrieves semantically matched items given a\ntextual query, has been an essential component to enhance system effectiveness\nin e-commerce search. In this paper, we study the multimodal retrieval problem,\nwhere the visual information (e.g, image) of item is leveraged as supplementary\nof textual information to enrich item representation and further improve\nretrieval performance. Though learning from cross-modality data has been\nstudied extensively in tasks such as visual question answering or media\nsummarization, multimodal retrieval remains a non-trivial and unsolved problem\nespecially in the asymmetric scenario where the query is unimodal while the\nitem is multimodal. In this paper, we propose a novel model named SMAR, which\nstands for Semantic-enhanced Modality-Asymmetric Retrieval, to tackle the\nproblem of modality fusion and alignment in this kind of asymmetric scenario.\nExtensive experimental results on an industrial dataset show that the proposed\nmodel outperforms baseline models significantly in retrieval accuracy. We have\nopen sourced our industrial dataset for the sake of reproducibility and future\nresearch works.", "AI": {"tldr": "This paper introduces SMAR, a new model for multimodal retrieval that improves retrieval accuracy by fusing textual and visual information in an asymmetric scenario. The model outperforms existing methods on an industrial dataset, which has been made publicly available.", "motivation": "Multimodal retrieval remains a non-trivial and unsolved problem especially in the asymmetric scenario where the query is unimodal while the item is multimodal.", "method": "A novel model named SMAR (Semantic-enhanced Modality-Asymmetric Retrieval) is proposed to tackle the problem of modality fusion and alignment in an asymmetric scenario where the query is unimodal while the item is multimodal.", "result": "Extensive experimental results on an industrial dataset show that the proposed model outperforms baseline models significantly in retrieval accuracy. The industrial dataset is open sourced.", "conclusion": "The proposed SMAR model outperforms baseline models significantly in retrieval accuracy on an industrial dataset."}}
{"id": "2506.20010", "categories": ["cs.DB", "H.2.4"], "pdf": "https://arxiv.org/pdf/2506.20010", "abs": "https://arxiv.org/abs/2506.20010", "authors": ["Shu Lin", "Arunprasad P. Marathe", "Per-\u0226ke Larson", "Chong Chen", "Calvin Sun", "Paul Lee", "Weidong Yu"], "title": "Near Data Processing in Taurus Database", "comment": null, "summary": "Huawei's cloud-native database system GaussDB for MySQL (also known as\nTaurus) stores data in a separate storage layer consisting of a pool of storage\nservers. Each server has considerable compute power making it possible to push\ndata reduction operations (selection, projection, and aggregation) close to\nstorage. This paper describes the design and implementation of near data\nprocessing (NDP) in Taurus. NDP has several benefits: it reduces the amount of\ndata shipped over the network; frees up CPU capacity in the compute layer; and\nreduces query run time, thereby enabling higher system throughput. Experiments\nwith the TPCH benchmark (100 GB) showed that 18 out of 22 queries benefited\nfrom NDP; data shipped was reduced by 63 percent; and CPU time by 50 percent.\nOn Q15 the impact was even higher: data shipped was reduced by 98 percent; CPU\ntime by 91 percent; and run time by 80 percent.", "AI": {"tldr": "\u534e\u4e3aGaussDB for MySQL (Taurus) \u901a\u8fc7\u8fd1\u6570\u636e\u5904\u7406 (NDP) \u51cf\u5c11\u6570\u636e\u4f20\u8f93\u548cCPU\u4f7f\u7528\uff0c\u4ece\u800c\u63d0\u9ad8\u67e5\u8be2\u6027\u80fd\u3002", "motivation": "\u5c06\u6570\u636e\u7f29\u51cf\u64cd\u4f5c\uff08\u9009\u62e9\u3001\u6295\u5f71\u548c\u805a\u5408\uff09\u63a8\u9001\u5230\u9760\u8fd1\u5b58\u50a8\u7684\u4f4d\u7f6e\u53ef\u4ee5\u51cf\u5c11\u7f51\u7edc\u4f20\u8f93\u7684\u6570\u636e\u91cf\uff0c\u91ca\u653e\u8ba1\u7b97\u5c42\u7684CPU\u5bb9\u91cf\uff0c\u5e76\u7f29\u77ed\u67e5\u8be2\u8fd0\u884c\u65f6\u95f4\u3002", "method": "\u5728\u534e\u4e3a\u4e91\u539f\u751f\u6570\u636e\u5e93\u7cfb\u7edfGaussDB for MySQL (Taurus) \u4e2d\u8bbe\u8ba1\u548c\u5b9e\u73b0\u4e86\u8fd1\u6570\u636e\u5904\u7406 (NDP)\u3002", "result": "TPCH\u57fa\u51c6\u6d4b\u8bd5\uff08100 GB\uff09\u8868\u660e\uff0c22\u4e2a\u67e5\u8be2\u4e2d\u670918\u4e2a\u53d7\u76ca\u4e8eNDP\uff1b\u6570\u636e\u4f20\u8f93\u91cf\u51cf\u5c11\u4e8663\uff05\uff1bCPU\u65f6\u95f4\u51cf\u5c11\u4e8650\uff05\u3002\u5728Q15\u4e0a\uff0c\u5f71\u54cd\u751a\u81f3\u66f4\u9ad8\uff1a\u6570\u636e\u4f20\u8f93\u91cf\u51cf\u5c11\u4e8698\uff05\uff1bCPU\u65f6\u95f4\u51cf\u5c11\u4e8691\uff05\uff1b\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\u4e8680\uff05\u3002", "conclusion": "NDP\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u91cf\uff0c\u964d\u4f4eCPU\u65f6\u95f4\uff0c\u5e76\u7f29\u77ed\u67e5\u8be2\u8fd0\u884c\u65f6\u95f4\uff0c\u4ece\u800c\u63d0\u9ad8\u7cfb\u7edf\u541e\u5410\u91cf\u3002"}}
{"id": "2506.19952", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19952", "abs": "https://arxiv.org/abs/2506.19952", "authors": ["Deepon Halder", "Thanmay Jayakumar", "Raj Dabre"], "title": "CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation", "comment": null, "summary": "Large language models (LLMs), despite their ability to perform few-shot\nmachine translation (MT), often lag behind dedicated MT systems trained on\nparallel corpora, which are crucial for high quality machine translation (MT).\nHowever, parallel corpora are often scarce or non-existent for low-resource\nlanguages. In this paper, we propose CycleDistill, a bootstrapping approach\nleveraging LLMs and few-shot translation to obtain high-quality MT systems.\nCycleDistill involves iteratively generating synthetic parallel corpora from\nmonolingual corpora via zero- or few-shot MT, which is then used to fine-tune\nthe model that was used for generating said data for MT. CycleDistill does not\nneed parallel corpora beyond 1 to 4 few-shot examples, and in our experiments\nfocusing on three Indian languages, by relying solely on monolingual corpora,\nit can achieve high-quality machine translation, improving upon a few-shot\nbaseline model by over 20-30 chrF points on average in the first iteration. We\nalso study the effect of leveraging softmax activations during the distillation\nprocess and observe mild improvements in translation quality.", "AI": {"tldr": "CycleDistill\u662f\u4e00\u79cd\u81ea\u4e3e\u65b9\u6cd5\uff0c\u5b83\u5229\u7528LLM\u548cfew-shot\u7ffb\u8bd1\u6765\u83b7\u5f97\u9ad8\u8d28\u91cf\u7684MT\u7cfb\u7edf\uff0c\u5e76\u4e14\u53ea\u9700\u8981\u5c11\u91cf\u7684few-shot\u793a\u4f8b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\uff0c\u5c3d\u7ba1\u5b83\u4eec\u5177\u6709\u6267\u884cfew-shot\u673a\u5668\u7ffb\u8bd1(MT)\u7684\u80fd\u529b\uff0c\u4f46\u901a\u5e38\u843d\u540e\u4e8e\u5728\u5e73\u884c\u8bed\u6599\u5e93\u4e0a\u8bad\u7ec3\u7684\u4e13\u7528MT\u7cfb\u7edf\uff0c\u8fd9\u5bf9\u4e8e\u9ad8\u8d28\u91cf\u7684\u673a\u5668\u7ffb\u8bd1(MT)\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u6765\u8bf4\uff0c\u5e73\u884c\u8bed\u6599\u5e93\u901a\u5e38\u662f\u7a00\u7f3a\u7684\u6216\u4e0d\u5b58\u5728\u7684\u3002", "method": "CycleDistill\uff0c\u4e00\u79cd\u5229\u7528LLM\u548cfew-shot\u7ffb\u8bd1\u7684\u81ea\u4e3e\u65b9\u6cd5\uff0c\u4ee5\u83b7\u5f97\u9ad8\u8d28\u91cf\u7684MT\u7cfb\u7edf\u3002CycleDistill\u6d89\u53ca\u901a\u8fc7zero- or few-shot MT\u4ece\u5355\u8bed\u8bed\u6599\u5e93\u4e2d\u8fed\u4ee3\u751f\u6210\u5408\u6210\u5e73\u884c\u8bed\u6599\u5e93\uff0c\u7136\u540e\u7528\u4e8e\u5fae\u8c03\u7528\u4e8e\u751f\u6210\u6240\u8ff0\u6570\u636e\u7684MT\u6a21\u578b\u3002", "result": "\u4ec5\u4f9d\u8d56\u4e8e\u5355\u8bed\u8bed\u6599\u5e93\uff0c\u5b83\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u673a\u5668\u7ffb\u8bd1\uff0c\u5728\u7b2c\u4e00\u6b21\u8fed\u4ee3\u4e2d\uff0c\u5728few-shot\u57fa\u7ebf\u6a21\u578b\u7684\u57fa\u7840\u4e0a\uff0c\u5e73\u5747\u63d0\u9ad8\u4e8620-30\u4e2achrF\u70b9\u3002\u6211\u4eec\u8fd8\u7814\u7a76\u4e86\u5728\u84b8\u998f\u8fc7\u7a0b\u4e2d\u5229\u7528softmax\u6fc0\u6d3b\u7684\u5f71\u54cd\uff0c\u5e76\u89c2\u5bdf\u5230\u7ffb\u8bd1\u8d28\u91cf\u7684\u8f7b\u5fae\u63d0\u9ad8\u3002", "conclusion": "CycleDistill\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u673a\u5668\u7ffb\u8bd1\uff0c\u5728\u7b2c\u4e00\u6b21\u8fed\u4ee3\u4e2d\uff0c\u5728few-shot\u57fa\u7ebf\u6a21\u578b\u7684\u57fa\u7840\u4e0a\uff0c\u5e73\u5747\u63d0\u9ad8\u4e8620-30\u4e2achrF\u70b9"}}
{"id": "2506.19939", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19939", "abs": "https://arxiv.org/abs/2506.19939", "authors": ["Aryan Singh Dalal", "Sidharth Rai", "Rahul Singh", "Treman Singh Kaloya", "Rahul Harsha Cheppally", "Ajay Sharda"], "title": "Computer Vision based Automated Quantification of Agricultural Sprayers Boom Displacement", "comment": "Under publication process for COMPAG", "summary": "Application rate errors when using self-propelled agricultural sprayers for\nagricultural production remain a concern. Among other factors, spray boom\ninstability is one of the major contributors to application errors. Spray\nbooms' width of 38m, combined with 30 kph driving speeds, varying terrain, and\nmachine dynamics when maneuvering complex field boundaries, make controls of\nthese booms very complex. However, there is no quantitative knowledge on the\nextent of boom movement to systematically develop a solution that might include\nboom designs and responsive boom control systems. Therefore, this study was\nconducted to develop an automated computer vision system to quantify the boom\nmovement of various agricultural sprayers. A computer vision system was\ndeveloped to track a target on the edge of the sprayer boom in real time. YOLO\nV7, V8, and V11 neural network models were trained to track the boom's\nmovements in field operations to quantify effective displacement in the\nvertical and transverse directions. An inclinometer sensor was mounted on the\nboom to capture boom angles and validate the neural network model output. The\nresults showed that the model could detect the target with more than 90 percent\naccuracy, and distance estimates of the target on the boom were within 0.026 m\nof the inclinometer sensor data. This system can quantify the boom movement on\nthe current sprayer and potentially on any other sprayer with minor\nmodifications. The data can be used to make design improvements to make sprayer\nbooms more stable and achieve greater application accuracy.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u52a8\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u6765\u91cf\u5316\u5404\u79cd\u519c\u4e1a\u55b7\u96fe\u5668\u7684\u55b7\u6746\u8fd0\u52a8\u3002", "motivation": "\u4f7f\u7528\u81ea\u8d70\u5f0f\u519c\u4e1a\u55b7\u96fe\u5668\u8fdb\u884c\u519c\u4e1a\u751f\u4ea7\u65f6\u7684\u65bd\u7528\u7387\u8bef\u5dee\u4ecd\u7136\u4ee4\u4eba\u62c5\u5fe7\u3002 \u5728\u4f17\u591a\u56e0\u7d20\u4e2d\uff0c\u55b7\u6746\u4e0d\u7a33\u5b9a\u662f\u9020\u6210\u65bd\u7528\u8bef\u5dee\u7684\u4e3b\u8981\u539f\u56e0\u4e4b\u4e00\u3002 38 \u7c73\u7684\u55b7\u6746\u5bbd\u5ea6\uff0c\u52a0\u4e0a 30 \u516c\u91cc/\u5c0f\u65f6\u7684\u884c\u9a76\u901f\u5ea6\u3001\u53d8\u5316\u7684\u5730\u5f62\u4ee5\u53ca\u5728\u64cd\u7eb5\u590d\u6742\u7684\u7530\u5730\u8fb9\u754c\u65f6\u7684\u673a\u5668\u52a8\u529b\uff0c\u4f7f\u5f97\u8fd9\u4e9b\u55b7\u6746\u7684\u63a7\u5236\u975e\u5e38\u590d\u6742\u3002 \u7136\u800c\uff0c\u6ca1\u6709\u5173\u4e8e\u55b7\u6746\u8fd0\u52a8\u7a0b\u5ea6\u7684\u5b9a\u91cf\u77e5\u8bc6\u6765\u7cfb\u7edf\u5730\u5f00\u53d1\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u89e3\u51b3\u65b9\u6848\u53ef\u80fd\u5305\u62ec\u55b7\u6746\u8bbe\u8ba1\u548c\u54cd\u5e94\u5f0f\u55b7\u6746\u63a7\u5236\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u6765\u5b9e\u65f6\u8ddf\u8e2a\u55b7\u6746\u8fb9\u7f18\u7684\u76ee\u6807\u3002 \u8bad\u7ec3\u4e86 YOLO V7\u3001V8 \u548c V11 \u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u4ee5\u8ddf\u8e2a\u7530\u95f4\u4f5c\u4e1a\u4e2d\u55b7\u6746\u7684\u8fd0\u52a8\uff0c\u4ece\u800c\u91cf\u5316\u5782\u76f4\u548c\u6a2a\u5411\u7684\u6709\u6548\u4f4d\u79fb\u3002 \u5728\u55b7\u6746\u4e0a\u5b89\u88c5\u4e86\u4e00\u4e2a\u503e\u89d2\u8ba1\u4f20\u611f\u5668\uff0c\u4ee5\u6355\u83b7\u55b7\u6746\u89d2\u5ea6\u5e76\u9a8c\u8bc1\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u51fa\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u68c0\u6d4b\u5230\u76ee\u6807\uff0c\u51c6\u786e\u7387\u8d85\u8fc7 90%\uff0c\u5e76\u4e14\u55b7\u6746\u4e0a\u76ee\u6807\u7684\u8ddd\u79bb\u4f30\u8ba1\u503c\u5728\u503e\u89d2\u8ba1\u4f20\u611f\u5668\u6570\u636e\u7684 0.026 \u7c73\u4ee5\u5185\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u53ef\u4ee5\u91cf\u5316\u5f53\u524d\u55b7\u96fe\u5668\u4ee5\u53ca\u53ef\u80fd\u7ecf\u8fc7\u5c11\u91cf\u4fee\u6539\u7684\u4efb\u4f55\u5176\u4ed6\u55b7\u96fe\u5668\u4e0a\u7684\u55b7\u6746\u8fd0\u52a8\u3002 \u8fd9\u4e9b\u6570\u636e\u53ef\u7528\u4e8e\u8fdb\u884c\u8bbe\u8ba1\u6539\u8fdb\uff0c\u4ee5\u4f7f\u55b7\u6746\u66f4\u52a0\u7a33\u5b9a\u5e76\u5b9e\u73b0\u66f4\u9ad8\u7684\u65bd\u7528\u7cbe\u5ea6\u3002"}}
{"id": "2506.19923", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19923", "abs": "https://arxiv.org/abs/2506.19923", "authors": ["Kaito Baba", "Chaoran Liu", "Shuhei Kurita", "Akiyoshi Sannai"], "title": "Prover Agent: An Agent-based Framework for Formal Mathematical Proofs", "comment": "22 pages, 2 figures", "summary": "We present Prover Agent, a novel AI agent for automated theorem proving that\nintegrates large language models (LLMs) with a formal proof assistant, Lean.\nProver Agent coordinates an informal reasoning LLM, a formal prover model, and\nfeedback from Lean while also generating auxiliary lemmas to assist in\ndiscovering the overall proof strategy. It achieves an 86.1% success rate on\nthe MiniF2F benchmark, establishing a new state-of-the-art among methods using\nsmall language models (SLMs) with a much lower sample budget than previous\napproaches. We also present case studies illustrating how these generated\nlemmas contribute to solving challenging problems.", "AI": {"tldr": "Prover Agent\u7ed3\u5408LLM\u548cLean\uff0c\u5728MiniF2F\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684SOTA\uff0c\u4e14\u6837\u672c\u9884\u7b97\u8f83\u4f4e\u3002", "motivation": "\u63d0\u51fa\u4e86Prover Agent\uff0c\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u7684\u65b0\u578bAI Agent\u3002", "method": "\u96c6\u6210\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u5f62\u5f0f\u5316\u8bc1\u660e\u52a9\u624bLean\u3002", "result": "Prover Agent\u80fd\u591f\u751f\u6210\u8f85\u52a9\u5f15\u7406\u4ee5\u5e2e\u52a9\u53d1\u73b0\u6574\u4f53\u8bc1\u660e\u7b56\u7565\uff0c\u5e76\u5728\u89e3\u51b3\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u4e2d\u53d1\u6325\u4f5c\u7528\u3002", "conclusion": "Prover Agent\u5b9e\u73b0\u4e86\u5728MiniF2F\u57fa\u51c6\u6d4b\u8bd5\u4e0a86.1%\u7684\u6210\u529f\u7387\uff0c\u5728\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u65b9\u6cd5\u4e2d\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2506.19882", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.19882", "abs": "https://arxiv.org/abs/2506.19882", "authors": ["Rylan Schaeffer", "Joshua Kazdan", "Yegor Denisov-Blanch", "Brando Miranda", "Matthias Gerstgrasser", "Susan Zhang", "Andreas Haupt", "Isha Gupta", "Elyas Obbad", "Jesse Dodge", "Jessica Zosa Forde", "Koustuv Sinha", "Francesco Orabona", "Sanmi Koyejo", "David Donoho"], "title": "Position: Machine Learning Conferences Should Establish a \"Refutations and Critiques\" Track", "comment": null, "summary": "Science progresses by iteratively advancing and correcting humanity's\nunderstanding of the world. In machine learning (ML) research, rapid\nadvancements have led to an explosion of publications, but have also led to\nmisleading, incorrect, flawed or perhaps even fraudulent studies being accepted\nand sometimes highlighted at ML conferences due to the fallibility of peer\nreview. While such mistakes are understandable, ML conferences do not offer\nrobust processes to help the field systematically correct when such errors are\nmade.This position paper argues that ML conferences should establish a\ndedicated \"Refutations and Critiques\" (R & C) Track. This R & C Track would\nprovide a high-profile, reputable platform to support vital research that\ncritically challenges prior research, thereby fostering a dynamic\nself-correcting research ecosystem. We discuss key considerations including\ntrack design, review principles, potential pitfalls, and provide an\nillustrative example submission concerning a recent ICLR 2025 Oral. We conclude\nthat ML conferences should create official, reputable mechanisms to help ML\nresearch self-correct.", "AI": {"tldr": "ML conferences should establish a dedicated \"Refutations and Critiques\" (R & C) Track to foster a dynamic self-correcting research ecosystem.", "motivation": "ML conferences do not offer robust processes to help the field systematically correct when such errors are made.", "method": "ML conferences should establish a dedicated \"Refutations and Critiques\" (R & C) Track.", "result": "This R & C Track would provide a high-profile, reputable platform to support vital research that critically challenges prior research, thereby fostering a dynamic self-correcting research ecosystem.", "conclusion": "ML conferences should create official, reputable mechanisms to help ML research self-correct."}}
{"id": "2506.20501", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20501", "abs": "https://arxiv.org/abs/2506.20501", "authors": ["Philipp Hager", "Onno Zoeter", "Maarten de Rijke"], "title": "Unidentified and Confounded? Understanding Two-Tower Models for Unbiased Learning to Rank", "comment": null, "summary": "Additive two-tower models are popular learning-to-rank methods for handling\nbiased user feedback in industry settings. Recent studies, however, report a\nconcerning phenomenon: training two-tower models on clicks collected by\nwell-performing production systems leads to decreased ranking performance. This\npaper investigates two recent explanations for this observation: confounding\neffects from logging policies and model identifiability issues. We\ntheoretically analyze the identifiability conditions of two-tower models,\nshowing that either document swaps across positions or overlapping feature\ndistributions are required to recover model parameters from clicks. We also\ninvestigate the effect of logging policies on two-tower models, finding that\nthey introduce no bias when models perfectly capture user behavior. However,\nlogging policies can amplify biases when models imperfectly capture user\nbehavior, particularly when prediction errors correlate with document placement\nacross positions. We propose a sample weighting technique to mitigate these\neffects and provide actionable insights for researchers and practitioners using\ntwo-tower models.", "AI": {"tldr": "\u53cc\u5854\u6a21\u578b\u5728\u751f\u4ea7\u7cfb\u7edf\u4e2d\u7528\u70b9\u51fb\u6570\u636e\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u6392\u540d\u6027\u80fd\u4e0b\u964d\u3002\u672c\u6587\u7814\u7a76\u4e86\u65e5\u5fd7\u8bb0\u5f55\u7b56\u7565\u548c\u6a21\u578b\u53ef\u8bc6\u522b\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6837\u672c\u6743\u91cd\u6280\u672f\u6765\u7f13\u89e3\u8fd9\u4e9b\u5f71\u54cd\u3002", "motivation": "\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\uff0c\u52a0\u6027\u53cc\u5854\u6a21\u578b\u662f\u5904\u7406\u6709\u504f\u5dee\u7684\u7528\u6237\u53cd\u9988\u7684\u6d41\u884c\u7684\u5b66\u4e60\u6392\u5e8f\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u62a5\u544a\u4e86\u4e00\u79cd\u4ee4\u4eba\u62c5\u5fe7\u7684\u73b0\u8c61\uff1a\u5728\u8868\u73b0\u826f\u597d\u7684\u751f\u4ea7\u7cfb\u7edf\u6536\u96c6\u7684\u70b9\u51fb\u6570\u636e\u4e0a\u8bad\u7ec3\u53cc\u5854\u6a21\u578b\u4f1a\u5bfc\u81f4\u6392\u540d\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u7406\u8bba\u5206\u6790\u4e86\u53cc\u5854\u6a21\u578b\u7684\u53ef\u8bc6\u522b\u6027\u6761\u4ef6\uff0c\u8868\u660e\u9700\u8981\u8de8\u4f4d\u7f6e\u7684\u6587\u6863\u4ea4\u6362\u6216\u91cd\u53e0\u7684\u7279\u5f81\u5206\u5e03\u624d\u80fd\u4ece\u70b9\u51fb\u4e2d\u6062\u590d\u6a21\u578b\u53c2\u6570\u3002\u6211\u4eec\u8fd8\u7814\u7a76\u4e86\u65e5\u5fd7\u8bb0\u5f55\u7b56\u7565\u5bf9\u53cc\u5854\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5f53\u6a21\u578b\u5b8c\u7f8e\u5730\u6355\u6349\u7528\u6237\u884c\u4e3a\u65f6\uff0c\u5b83\u4eec\u4e0d\u4f1a\u5f15\u5165\u504f\u5dee\u3002\u7136\u800c\uff0c\u5f53\u6a21\u578b\u4e0d\u5b8c\u7f8e\u5730\u6355\u6349\u7528\u6237\u884c\u4e3a\u65f6\uff0c\u65e5\u5fd7\u8bb0\u5f55\u7b56\u7565\u4f1a\u653e\u5927\u504f\u5dee\uff0c\u7279\u522b\u662f\u5f53\u9884\u6d4b\u8bef\u5dee\u4e0e\u8de8\u4f4d\u7f6e\u7684\u6587\u6863\u653e\u7f6e\u76f8\u5173\u65f6\u3002", "result": "\u8868\u660e\u9700\u8981\u8de8\u4f4d\u7f6e\u7684\u6587\u6863\u4ea4\u6362\u6216\u91cd\u53e0\u7684\u7279\u5f81\u5206\u5e03\u624d\u80fd\u4ece\u70b9\u51fb\u4e2d\u6062\u590d\u6a21\u578b\u53c2\u6570\u3002\u6211\u4eec\u8fd8\u7814\u7a76\u4e86\u65e5\u5fd7\u8bb0\u5f55\u7b56\u7565\u5bf9\u53cc\u5854\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5f53\u6a21\u578b\u5b8c\u7f8e\u5730\u6355\u6349\u7528\u6237\u884c\u4e3a\u65f6\uff0c\u5b83\u4eec\u4e0d\u4f1a\u5f15\u5165\u504f\u5dee\u3002\u7136\u800c\uff0c\u5f53\u6a21\u578b\u4e0d\u5b8c\u7f8e\u5730\u6355\u6349\u7528\u6237\u884c\u4e3a\u65f6\uff0c\u65e5\u5fd7\u8bb0\u5f55\u7b56\u7565\u4f1a\u653e\u5927\u504f\u5dee\uff0c\u7279\u522b\u662f\u5f53\u9884\u6d4b\u8bef\u5dee\u4e0e\u8de8\u4f4d\u7f6e\u7684\u6587\u6863\u653e\u7f6e\u76f8\u5173\u65f6\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6837\u672c\u6743\u91cd\u6280\u672f\u6765\u7f13\u89e3\u8fd9\u4e9b\u5f71\u54cd\uff0c\u5e76\u4e3a\u4f7f\u7528\u53cc\u5854\u6a21\u578b\u7684\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u4eba\u5458\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2506.20139", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20139", "abs": "https://arxiv.org/abs/2506.20139", "authors": ["Jiayong Qin", "Xianyu Zhu", "Qiyu Liu", "Guangyi Zhang", "Zhigang Cai", "Jianwei Liao", "Sha Hu", "Jingshu Peng", "Yingxia Shao", "Lei Chen"], "title": "Piecewise Linear Approximation in Learned Index Structures: Theoretical and Empirical Analysis", "comment": null, "summary": "A growing trend in the database and system communities is to augment\nconventional index structures, such as B+-trees, with machine learning (ML)\nmodels. Among these, error-bounded Piecewise Linear Approximation\n($\\epsilon$-PLA) has emerged as a popular choice due to its simplicity and\neffectiveness. Despite its central role in many learned indexes, the design and\nanalysis of $\\epsilon$-PLA fitting algorithms remain underexplored. In this\npaper, we revisit $\\epsilon$-PLA from both theoretical and empirical\nperspectives, with a focus on its application in learned index structures. We\nfirst establish a fundamentally improved lower bound of $\\Omega(\\kappa \\cdot\n\\epsilon^2)$ on the expected segment coverage for existing $\\epsilon$-PLA\nfitting algorithms, where $\\kappa$ is a data-dependent constant. We then\npresent a comprehensive benchmark of state-of-the-art $\\epsilon$-PLA algorithms\nwhen used in different learned data structures. Our results highlight key\ntrade-offs among model accuracy, model size, and query performance, providing\nactionable guidelines for the principled design of future learned data\nstructures.", "AI": {"tldr": "This paper revisits error-bounded Piecewise Linear Approximation ($\\\\&epsilon$-PLA) for learned indexes, providing a comprehensive benchmark and actionable guidelines for future designs.", "motivation": "Conventional index structures, such as B+-trees, are being augmented with machine learning (ML) models, and error-bounded Piecewise Linear Approximation ($\\\\&epsilon$-PLA) has emerged as a popular choice. However, the design and analysis of $\\epsilon$-PLA fitting algorithms remain underexplored.", "method": "The paper revisits $\\epsilon$-PLA from both theoretical and empirical perspectives, establishing a fundamentally improved lower bound of $\\Omega(\\kappa \nobreakdash \\cdot \\epsilon^2)$ on the expected segment coverage for existing $\\epsilon$-PLA fitting algorithms and presenting a comprehensive benchmark of state-of-the-art $\\epsilon$-PLA algorithms.", "result": "The paper establishes a fundamentally improved lower bound of $\\Omega(\\kappa \\cdot \\epsilon^2)$ on the expected segment coverage for existing $\\epsilon$-PLA fitting algorithms and highlights key trade-offs among model accuracy, model size, and query performance.", "conclusion": "This paper provides actionable guidelines for the principled design of future learned data structures by highlighting key trade-offs among model accuracy, model size, and query performance."}}
{"id": "2506.19967", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19967", "abs": "https://arxiv.org/abs/2506.19967", "authors": ["Travis Thompson", "Seung-Hwan Lim", "Paul Liu", "Ruoying He", "Dongkuan Xu"], "title": "Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs", "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive capabilities in\nlanguage understanding and generation, yet they continue to underperform on\nknowledge-intensive reasoning tasks due to limited access to structured context\nand multi-hop information. Retrieval-Augmented Generation (RAG) partially\nmitigates this by grounding generation in retrieved context, but conventional\nRAG and GraphRAG methods often fail to capture relational structure across\nnodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel\nframework that enhances LLM-based graph reasoning by applying inference-time\ncompute scaling. Our method combines sequential scaling with deep\nchain-of-thought graph traversal, and parallel scaling with majority voting\nover sampled trajectories within an interleaved reasoning-execution loop.\nExperiments on the GRBench benchmark demonstrate that our approach\nsignificantly improves multi-hop question answering performance, achieving\nsubstantial gains over both traditional GraphRAG and prior graph traversal\nbaselines. These findings suggest that inference-time scaling is a practical\nand architecture-agnostic solution for structured knowledge reasoning with LLMs", "AI": {"tldr": "Inference-Scaled GraphRAG enhances LLM-based graph reasoning by applying inference-time compute scaling, improving multi-hop question answering performance.", "motivation": "LLMs underperform on knowledge-intensive reasoning tasks due to limited access to structured context and multi-hop information. Conventional RAG and GraphRAG methods often fail to capture relational structure across nodes in knowledge graphs.", "method": "Inference-Scaled GraphRAG, combines sequential scaling with deep chain-of-thought graph traversal, and parallel scaling with majority voting over sampled trajectories within an interleaved reasoning-execution loop.", "result": "Achieves substantial gains over both traditional GraphRAG and prior graph traversal baselines on the GRBench benchmark.", "conclusion": "Inference-time scaling significantly improves multi-hop question answering performance on structured knowledge reasoning tasks with LLMs."}}
{"id": "2506.19955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19955", "abs": "https://arxiv.org/abs/2506.19955", "authors": ["Yiming Ma", "Victor Sanchez", "Tanaya Guha"], "title": "EBC-ZIP: Improving Blockwise Crowd Counting with Zero-Inflated Poisson Regression", "comment": null, "summary": "Density map estimation has become the mainstream paradigm in crowd counting.\nHowever, most existing methods overlook the extreme sparsity of ground-truth\ndensity maps. In real-world crowd scenes, the vast majority of spatial regions\n(often over 95%) contain no people, leading to heavily imbalanced count\ndistributions. Ignoring this imbalance can bias models toward overestimating\ndense regions and underperforming in sparse areas. Furthermore, most loss\nfunctions used in density estimation are majorly based on MSE and implicitly\nassume Gaussian distributions, which are ill-suited for modeling discrete,\nnon-negative count data. In this paper, we propose EBC-ZIP, a crowd counting\nframework that models the spatial distribution of counts using a Zero-Inflated\nPoisson (ZIP) regression formulation. Our approach replaces the traditional\nregression loss with the negative log-likelihood of the ZIP distribution,\nenabling better handling of zero-heavy distributions while preserving count\naccuracy. Built upon the recently proposed Enhanced Block Classification (EBC)\nframework, EBC-ZIP inherits EBC's advantages in preserving the discreteness of\ntargets and ensuring training stability, while further improving performance\nthrough a more principled probabilistic loss. We also evaluate EBC-ZIP with\nbackbones of varying computational complexity to assess its scalability.\nExtensive experiments on four crowd counting benchmarks demonstrate that\nEBC-ZIP consistently outperforms EBC and achieves state-of-the-art results.", "AI": {"tldr": "EBC-ZIP \u662f\u4e00\u4e2a\u4eba\u7fa4\u8ba1\u6570\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u96f6\u81a8\u80c0\u6cca\u677e (ZIP) \u56de\u5f52\u6765\u66f4\u597d\u5730\u5904\u7406\u96f6\u6743\u91cd\u5206\u5e03\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u6570\u51c6\u786e\u6027\uff0c\u4f18\u4e8e EBC \u5e76\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86ground-truth\u5bc6\u5ea6\u56fe\u7684\u6781\u7aef\u7a00\u758f\u6027\uff0c\u5e76\u4e14\u5927\u591a\u6570\u5bc6\u5ea6\u4f30\u8ba1\u4e2d\u4f7f\u7528\u7684\u635f\u5931\u51fd\u6570\u4e3b\u8981\u57fa\u4e8e MSE \u5e76\u4e14\u9690\u542b\u5730\u5047\u8bbe\u9ad8\u65af\u5206\u5e03\uff0c\u8fd9\u4e0d\u9002\u5408\u5efa\u6a21\u79bb\u6563\u7684\u975e\u8d1f\u8ba1\u6570\u6570\u636e\u3002", "method": "\u8be5\u65b9\u6cd5\u4f7f\u7528\u96f6\u81a8\u80c0\u6cca\u677e (ZIP) \u56de\u5f52\u516c\u5f0f\u5bf9\u8ba1\u6570\u7a7a\u95f4\u5206\u5e03\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u7528 ZIP \u5206\u5e03\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136\u4ee3\u66ff\u4f20\u7edf\u56de\u5f52\u635f\u5931\u3002", "result": "EBC-ZIP \u59cb\u7ec8\u4f18\u4e8e EBC\uff0c\u5e76\u5728\u56db\u4e2a\u4eba\u7fa4\u8ba1\u6570\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "EBC-ZIP\u5728\u56db\u4e2a\u4eba\u7fa4\u8ba1\u6570\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8eEBC\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002"}}
{"id": "2506.19977", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19977", "abs": "https://arxiv.org/abs/2506.19977", "authors": ["Deng Pan", "Keerthiram Murugesan", "Nuno Moniz", "Nitesh Chawla"], "title": "Context Attribution with Multi-Armed Bandit Optimization", "comment": null, "summary": "Understanding which parts of the retrieved context contribute to a large\nlanguage model's generated answer is essential for building interpretable and\ntrustworthy generative QA systems. We propose a novel framework that formulates\ncontext attribution as a combinatorial multi-armed bandit (CMAB) problem. Each\ncontext segment is treated as a bandit arm, and we employ Combinatorial\nThompson Sampling (CTS) to efficiently explore the exponentially large space of\ncontext subsets under a limited query budget. Our method defines a reward\nfunction based on normalized token likelihoods, capturing how well a subset of\nsegments supports the original model response. Unlike traditional\nperturbation-based attribution methods such as SHAP, which sample subsets\nuniformly and incur high computational costs, our approach adaptively balances\nexploration and exploitation by leveraging posterior estimates of segment\nrelevance. This leads to substantially improved query efficiency while\nmaintaining high attribution fidelity. Extensive experiments on diverse\ndatasets and LLMs demonstrate that our method achieves competitive attribution\nquality with fewer model queries.", "AI": {"tldr": "This paper presents a novel framework for context attribution in generative QA systems using a combinatorial multi-armed bandit approach, achieving improved query efficiency and high attribution fidelity compared to traditional methods.", "motivation": "Understanding which parts of the retrieved context contribute to a large language model's generated answer is essential for building interpretable and trustworthy generative QA systems.", "method": "The paper formulates context attribution as a combinatorial multi-armed bandit (CMAB) problem and employs Combinatorial Thompson Sampling (CTS).", "result": "The method demonstrates substantially improved query efficiency while maintaining high attribution fidelity.", "conclusion": "The proposed method achieves competitive attribution quality with fewer model queries."}}
{"id": "2506.19883", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19883", "abs": "https://arxiv.org/abs/2506.19883", "authors": ["Zhuqing Liu", "Chaosheng Dong", "Michinari Momma", "Simone Shao", "Shaoyuan Xu", "Yan Gao", "Haibo Yang", "Jia Liu"], "title": "STIMULUS: Achieving Fast Convergence and Low Sample Complexity in Stochastic Multi-Objective Learning", "comment": null, "summary": "Recently, multi-objective optimization (MOO) has gained attention for its\nbroad applications in ML, operations research, and engineering. However, MOO\nalgorithm design remains in its infancy and many existing MOO methods suffer\nfrom unsatisfactory convergence rate and sample complexity performance. To\naddress this challenge, in this paper, we propose an algorithm called STIMULUS(\nstochastic path-integrated multi-gradient recursive e\\ulstimator), a new and\nrobust approach for solving MOO problems. Different from the traditional\nmethods, STIMULUS introduces a simple yet powerful recursive framework for\nupdating stochastic gradient estimates to improve convergence performance with\nlow sample complexity. In addition, we introduce an enhanced version of\nSTIMULUS, termed STIMULUS-M, which incorporates a momentum term to further\nexpedite convergence. We establish $O(1/T)$ convergence rates of the proposed\nmethods for non-convex settings and $O (\\exp{-\\mu T})$ for strongly convex\nsettings, where $T$ is the total number of iteration rounds. Additionally, we\nachieve the state-of-the-art $O \\left(n+\\sqrt{n}\\epsilon^{-1}\\right)$ sample\ncomplexities for non-convex settings and $O\\left(n+ \\sqrt{n} \\ln\n({\\mu/\\epsilon})\\right)$ for strongly convex settings, where $\\epsilon>0$ is a\ndesired stationarity error. Moreover, to alleviate the periodic full gradient\nevaluation requirement in STIMULUS and STIMULUS-M, we further propose enhanced\nversions with adaptive batching called STIMULUS+/ STIMULUS-M+ and provide their\ntheoretical analysis.", "AI": {"tldr": "This paper introduces STIMULUS, a new MOO algorithm with improved convergence and sample complexity, along with enhanced versions STIMULUS-M, STIMULUS+, and STIMULUS-M+.", "motivation": "Existing multi-objective optimization (MOO) methods suffer from unsatisfactory convergence rate and sample complexity performance.", "method": "The paper proposes an algorithm called STIMULUS (stochastic path-integrated multi-gradient recursive estimator), a recursive framework for updating stochastic gradient estimates. An enhanced version, STIMULUS-M, incorporates a momentum term. Adaptive batching versions, STIMULUS+ and STIMULUS-M+, are also proposed.", "result": "The proposed methods achieve improved convergence performance with low sample complexity. Enhanced versions with adaptive batching alleviate the periodic full gradient evaluation requirement.", "conclusion": "This paper establishes convergence rates of O(1/T) for non-convex settings and O(exp{-\u03bcT}) for strongly convex settings. It also achieves state-of-the-art sample complexities of O(n+\u221an\u03f5^{-1}) for non-convex settings and O(n+ \u221an ln(\u03bc/\u03f5)) for strongly convex settings."}}
{"id": "2506.20041", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.20041", "abs": "https://arxiv.org/abs/2506.20041", "authors": ["Soheil Abadifard", "Fazli Can"], "title": "LSH-DynED: A Dynamic Ensemble Framework with LSH-Based Undersampling for Evolving Multi-Class Imbalanced Classification", "comment": null, "summary": "The classification of imbalanced data streams, which have unequal class\ndistributions, is a key difficulty in machine learning, especially when dealing\nwith multiple classes. While binary imbalanced data stream classification tasks\nhave received considerable attention, only a few studies have focused on\nmulti-class imbalanced data streams. Effectively managing the dynamic imbalance\nratio is a key challenge in this domain. This study introduces a novel, robust,\nand resilient approach to address these challenges by integrating Locality\nSensitive Hashing with Random Hyperplane Projections (LSH-RHP) into the Dynamic\nEnsemble Diversification (DynED) framework. To the best of our knowledge, we\npresent the first application of LSH-RHP for undersampling in the context of\nimbalanced non-stationary data streams. The proposed method undersamples the\nmajority classes by utilizing LSH-RHP, provides a balanced training set, and\nimproves the ensemble's prediction performance. We conduct comprehensive\nexperiments on 23 real-world and ten semi-synthetic datasets and compare\nLSH-DynED with 15 state-of-the-art methods. The results reveal that LSH-DynED\noutperforms other approaches in terms of both Kappa and mG-Mean effectiveness\nmeasures, demonstrating its capability in dealing with multi-class imbalanced\nnon-stationary data streams. Notably, LSH-DynED performs well in large-scale,\nhigh-dimensional datasets with considerable class imbalances and demonstrates\nadaptation and robustness in real-world circumstances. To motivate our design,\nwe review existing methods for imbalanced data streams, outline key challenges,\nand offer guidance for future work. For the reproducibility of our results, we\nhave made our implementation available on GitHub.", "AI": {"tldr": "LSH-DynED\u662f\u4e00\u79cd\u7528\u4e8e\u5904\u7406\u591a\u7c7b\u4e0d\u5e73\u8861\u6570\u636e\u6d41\u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7LSH-RHP\u6b20\u91c7\u6837\u591a\u6570\u7c7b\uff0c\u5e76\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u591a\u7c7b\u4e0d\u5e73\u8861\u6570\u636e\u6d41\u7684\u5206\u7c7b\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u96be\u70b9\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u591a\u4e2a\u7c7b\u522b\u65f6\u3002\u6709\u6548\u7ba1\u7406\u52a8\u6001\u4e0d\u5e73\u8861\u6bd4\u7387\u662f\u8be5\u9886\u57df\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u8be5\u7814\u7a76\u5c06\u5c40\u90e8\u654f\u611f\u54c8\u5e0c\u4e0e\u968f\u673a\u8d85\u5e73\u9762\u6295\u5f71\uff08LSH-RHP\uff09\u96c6\u6210\u5230\u52a8\u6001\u96c6\u6210\u591a\u6837\u5316\uff08DynED\uff09\u6846\u67b6\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u3001\u7a33\u5065\u4e14\u6709\u5f39\u6027\u7684\u65b9\u6cd5\u3002\u5229\u7528LSH-RHP\u5bf9\u591a\u6570\u7c7b\u8fdb\u884c\u6b20\u91c7\u6837\uff0c\u63d0\u4f9b\u5e73\u8861\u7684\u8bad\u7ec3\u96c6\u3002", "result": "LSH-DynED\u5728Kappa\u548cmG-Mean\u6709\u6548\u6027\u6307\u6807\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5904\u7406\u591a\u7c7b\u4e0d\u5e73\u8861\u975e\u5e73\u7a33\u6570\u636e\u6d41\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "LSH-DynED\u5728\u5904\u7406\u591a\u7c7b\u4e0d\u5e73\u8861\u975e\u5e73\u7a33\u6570\u636e\u6d41\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u3001\u9ad8\u7ef4\u5ea6\u3001\u5177\u6709\u663e\u8457\u7c7b\u4e0d\u5e73\u8861\u7684\u6570\u636e\u96c6\u4e2d\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.20023", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2506.20023", "abs": "https://arxiv.org/abs/2506.20023", "authors": ["Ryan Hildebrant", "Rahul Bhope", "Sharad Mehrotra", "Christopher Tull", "Nalini Venkatasubramanian"], "title": "DIM-SUM: Dynamic IMputation for Smart Utility Management", "comment": null, "summary": "Time series imputation models have traditionally been developed using\ncomplete datasets with artificial masking patterns to simulate missing values.\nHowever, in real-world infrastructure monitoring, practitioners often encounter\ndatasets where large amounts of data are missing and follow complex,\nheterogeneous patterns. We introduce DIM-SUM, a preprocessing framework for\ntraining robust imputation models that bridges the gap between artificially\nmasked training data and real missing patterns. DIM-SUM combines pattern\nclustering and adaptive masking strategies with theoretical learning guarantees\nto handle diverse missing patterns actually observed in the data. Through\nextensive experiments on over 2 billion readings from California water\ndistricts, electricity datasets, and benchmarks, we demonstrate that DIM-SUM\noutperforms traditional methods by reaching similar accuracy with lower\nprocessing time and significantly less training data. When compared against a\nlarge pre-trained model, DIM-SUM averages 2x higher accuracy with significantly\nless inference time.", "AI": {"tldr": "DIM-SUM is a preprocessing framework for training robust imputation models that bridges the gap between artificially masked training data and real missing patterns.", "motivation": "Practitioners often encounter datasets where large amounts of data are missing and follow complex, heterogeneous patterns.", "method": "DIM-SUM combines pattern clustering and adaptive masking strategies with theoretical learning guarantees to handle diverse missing patterns actually observed in the data.", "result": "DIM-SUM outperforms traditional methods by reaching similar accuracy with lower processing time and significantly less training data. When compared against a large pre-trained model, DIM-SUM averages 2x higher accuracy with significantly less inference time.", "conclusion": "DIM-SUM outperforms traditional methods by reaching similar accuracy with lower processing time and significantly less training data. When compared against a large pre-trained model, DIM-SUM averages 2x higher accuracy with significantly less inference time."}}
{"id": "2506.19998", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19998", "abs": "https://arxiv.org/abs/2506.19998", "authors": ["Xinyi Ni", "Haonan Jian", "Qiuyang Wang", "Vedanshi Chetan Shah", "Pengyu Hong"], "title": "Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation", "comment": null, "summary": "REST APIs play important roles in enriching the action space of web agents,\nyet most API-based agents rely on curated and uniform toolsets that do not\nreflect the complexity of real-world APIs. Building tool-using agents for\narbitrary domains remains a major challenge, as it requires reading\nunstructured API documentation, testing APIs and inferring correct parameters.\nWe propose Doc2Agent, a scalable pipeline to build agents that can call\nPython-based tools generated from API documentation. Doc2Agent generates\nexecutable tools from API documentations and iteratively refines them using a\ncode agent. We evaluate our approach on real-world APIs, WebArena APIs, and\nresearch APIs, producing validated tools. We achieved a 55\\% relative\nperformance improvement with 90\\% lower cost compared to direct API calling on\nWebArena benchmark. A domain-specific agent built for glycomaterial science\nfurther demonstrates the pipeline's adaptability to complex, knowledge-rich\ntasks. Doc2Agent offers a generalizable solution for building tool agents from\nunstructured API documentation at scale.", "AI": {"tldr": "Doc2Agent \u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u6784\u5efa\u53ef\u4ee5\u8c03\u7528\u4ece API \u6587\u6863\u751f\u6210\u7684\u57fa\u4e8e Python \u7684\u5de5\u5177\u7684\u4ee3\u7406\u3002", "motivation": "\u5927\u591a\u6570\u57fa\u4e8e API \u7684\u4ee3\u7406\u4f9d\u8d56\u4e8e\u7cbe\u9009\u548c\u7edf\u4e00\u7684\u5de5\u5177\u96c6\uff0c\u8fd9\u4e9b\u5de5\u5177\u96c6\u4e0d\u80fd\u53cd\u6620\u771f\u5b9e\u4e16\u754c API \u7684\u590d\u6742\u6027\u3002\u4e3a\u4efb\u610f\u9886\u57df\u6784\u5efa\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u9605\u8bfb\u975e\u7ed3\u6784\u5316\u7684 API \u6587\u6863\uff0c\u6d4b\u8bd5 API \u5e76\u63a8\u65ad\u6b63\u786e\u7684\u53c2\u6570\u3002", "method": "Doc2Agent \u751f\u6210\u53ef\u4ece API \u6587\u6863\u6267\u884c\u7684\u5de5\u5177\uff0c\u5e76\u4f7f\u7528\u4ee3\u7801\u4ee3\u7406\u8fed\u4ee3\u5730\u6539\u8fdb\u5b83\u4eec\u3002", "result": "\u5728 WebArena \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e0e\u76f4\u63a5 API \u8c03\u7528\u76f8\u6bd4\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86 55% \u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\uff0c\u6210\u672c\u964d\u4f4e\u4e86 90%\u3002\u4e3a\u7cd6\u6750\u6599\u79d1\u5b66\u6784\u5efa\u7684\u7279\u5b9a\u9886\u57df\u4ee3\u7406\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u8be5\u7ba1\u9053\u5bf9\u590d\u6742\u3001\u77e5\u8bc6\u4e30\u5bcc\u7684\u4efb\u52a1\u7684\u9002\u5e94\u6027\u3002", "conclusion": "Doc2Agent \u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u5730\u4ece\u975e\u7ed3\u6784\u5316 API \u6587\u6863\u6784\u5efa\u5de5\u5177\u4ee3\u7406\u3002"}}
{"id": "2506.20066", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20066", "abs": "https://arxiv.org/abs/2506.20066", "authors": ["Hsiang-Wei Huang", "Wenhao Chai", "Kuang-Ming Chen", "Cheng-Yen Yang", "Jenq-Neng Hwang"], "title": "ToSA: Token Merging with Spatial Awareness", "comment": "Accepted by IROS 2025", "summary": "Token merging has emerged as an effective strategy to accelerate Vision\nTransformers (ViT) by reducing computational costs. However, existing methods\nprimarily rely on the visual token's feature similarity for token merging,\noverlooking the potential of integrating spatial information, which can serve\nas a reliable criterion for token merging in the early layers of ViT, where the\nvisual tokens only possess weak visual information. In this paper, we propose\nToSA, a novel token merging method that combines both semantic and spatial\nawareness to guide the token merging process. ToSA leverages the depth image as\ninput to generate pseudo spatial tokens, which serve as auxiliary spatial\ninformation for the visual token merging process. With the introduced spatial\nawareness, ToSA achieves a more informed merging strategy that better preserves\ncritical scene structure. Experimental results demonstrate that ToSA\noutperforms previous token merging methods across multiple benchmarks on visual\nand embodied question answering while largely reducing the runtime of the ViT,\nmaking it an efficient solution for ViT acceleration. The code will be\navailable at: https://github.com/hsiangwei0903/ToSA", "AI": {"tldr": "ToSA: a novel token merging method that combines both semantic and spatial awareness to guide the token merging process.", "motivation": "Existing methods primarily rely on the visual token's feature similarity for token merging, overlooking the potential of integrating spatial information, which can serve as a reliable criterion for token merging in the early layers of ViT, where the visual tokens only possess weak visual information.", "method": "ToSA leverages the depth image as input to generate pseudo spatial tokens, which serve as auxiliary spatial information for the visual token merging process. With the introduced spatial awareness, ToSA achieves a more informed merging strategy that better preserves critical scene structure.", "result": "ToSA achieves a more informed merging strategy that better preserves critical scene structure.", "conclusion": "ToSA outperforms previous token merging methods across multiple benchmarks on visual and embodied question answering while largely reducing the runtime of the ViT, making it an efficient solution for ViT acceleration."}}
{"id": "2506.20008", "categories": ["cs.AI", "cs.PL", "cs.SE", "68T50, 81P68, 68T07, 68T20", "I.2.7; I.2.2"], "pdf": "https://arxiv.org/pdf/2506.20008", "abs": "https://arxiv.org/abs/2506.20008", "authors": ["Abdul Basit", "Minghao Shao", "Haider Asif", "Nouhaila Innan", "Muhammad Kashif", "Alberto Marchisio", "Muhammad Shafique"], "title": "QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges", "comment": "8 pages, 6 figures, 3 tables, submitted to QAI 2025", "summary": "Recent advances in Large Language Models (LLMs) have demonstrated strong\npotential in code generation, yet their effectiveness in quantum computing\nremains underexplored. This paper benchmarks LLMs for PennyLane-based quantum\ncode generation using real-world challenges from the Quantum Hackathon (QHack).\nWe introduce QHackBench, a novel benchmark dataset derived from QHack\ncompetitions, and evaluate model performance under vanilla prompting and\nRetrieval-Augmented Generation (RAG). Our structured evaluation framework\nassesses functional correctness, syntactic validity, and execution success\nacross varying challenge difficulties. Results indicate that RAG-enhanced\nmodels, supplemented with an augmented PennyLane dataset, approximately\ngenerate similar results as the standard prompting, particularly in complex\nquantum algorithms. Additionally, we introduce a multi-agent evaluation\npipeline that iteratively refines incorrect solutions, further enhancing\nexecution success rates. To foster further research, we commit to publicly\nreleasing QHackBench, along with our evaluation framework and experimental\nresults, enabling continued advancements in AI-assisted quantum programming.", "AI": {"tldr": "This paper benchmarks LLMs for quantum code generation, introduces a new dataset (QHackBench), and finds that RAG-enhanced models perform well. They also introduce a multi-agent evaluation pipeline that improves execution success rates and will release their dataset and evaluation framework.", "motivation": "LLMs have demonstrated strong potential in code generation, yet their effectiveness in quantum computing remains underexplored.", "method": "LLMs for PennyLane-based quantum code generation using real-world challenges from the Quantum Hackathon (QHack). QHackBench, a novel benchmark dataset derived from QHack competitions, and evaluate model performance under vanilla prompting and Retrieval-Augmented Generation (RAG). A multi-agent evaluation pipeline that iteratively refines incorrect solutions", "result": "RAG-enhanced models, supplemented with an augmented PennyLane dataset, generate similar results as the standard prompting, particularly in complex quantum algorithms.", "conclusion": "RAG-enhanced models generate similar results as standard prompting, particularly in complex quantum algorithms. A multi-agent evaluation pipeline iteratively refines incorrect solutions, enhancing execution success rates. The QHackBench dataset, evaluation framework, and experimental results are publicly released."}}
{"id": "2506.19885", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.19885", "abs": "https://arxiv.org/abs/2506.19885", "authors": ["Jing Lu", "Xuan Wu", "Yizhun Tian", "Songhan Fan", "Yali Fang"], "title": "FlightKooba: A Fast Interpretable FTP Model", "comment": "7 figures", "summary": "The Koopman theory is a powerful and effective modeling tool for converting\nnonlinear systems into linear representations, and flight trajectory prediction\n(FTP) is a complex nonlinear system. However, current models applying the\nKoopman theory to FTP tasks are not very effective, model interpretability is\nindeed an issue, and the Koopman operators are computationally intensive,\nresulting in long training times. To address this issue, this paper proposes a\nnew modeling and control framework based on the HIPPO method, the Koopman\ntheory, and state space equations from cybernetics: FlightKooba. Inspired by\nthe idea of structural state space equations, FlightKooba directly constructs\nthe Koopman operators from data. This makes the framework highly interpretable\nand significantly reduces the number of trainable parameters in the module,\nthereby greatly reducing training time. Experiments have demonstrated the\nsuperiority of the FlightKooba modeling method in terms of time and memory\nconsumption (training time comparable to the Mamba module without using\nCUDA-level acceleration; memory reduced by more than 50% on most datasets, with\na tenfold reduction in the number of parameters), essentially completing the\nFTP task. It provides a new method for the fast computation of the Koopman\noperators, opening up new possibilities for the combination of time series\nforecasting and control.", "AI": {"tldr": "proposes FlightKooba, a new modeling and control framework based on the HIPPO method, the Koopman theory, and state space equations from cybernetics to address the issue that current models applying the Koopman theory to FTP tasks are not very effective, model interpretability is indeed an issue, and the Koopman operators are computationally intensive, resulting in long training times", "motivation": "current models applying the Koopman theory to FTP tasks are not very effective, model interpretability is indeed an issue, and the Koopman operators are computationally intensive, resulting in long training times", "method": "a new modeling and control framework based on the HIPPO method, the Koopman theory, and state space equations from cybernetics: FlightKooba. Inspired by the idea of structural state space equations, FlightKooba directly constructs the Koopman operators from data.", "result": "Experiments have demonstrated the superiority of the FlightKooba modeling method in terms of time and memory consumption (training time comparable to the Mamba module without using CUDA-level acceleration; memory reduced by more than 50% on most datasets, with a tenfold reduction in the number of parameters), essentially completing the FTP task.", "conclusion": "It provides a new method for the fast computation of the Koopman operators, opening up new possibilities for the combination of time series forecasting and control."}}
{"id": "2506.20476", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.20476", "abs": "https://arxiv.org/abs/2506.20476", "authors": ["Tong Zhou"], "title": "Knowledge-Aware Diverse Reranking for Cross-Source Question Answering", "comment": null, "summary": "This paper presents Team Marikarp's solution for the SIGIR 2025 LiveRAG\ncompetition. The competition's evaluation set, automatically generated by\nDataMorgana from internet corpora, encompassed a wide range of target topics,\nquestion types, question formulations, audience types, and knowledge\norganization methods. It offered a fair evaluation of retrieving\nquestion-relevant supporting documents from a 15M documents subset of the\nFineWeb corpus. Our proposed knowledge-aware diverse reranking RAG pipeline\nachieved first place in the competition.", "AI": {"tldr": "Team Marikarp's knowledge-aware diverse reranking RAG pipeline won the SIGIR 2025 LiveRAG competition.", "motivation": "fair evaluation of retrieving question-relevant supporting documents from a 15M documents subset of the FineWeb corpus.", "method": "knowledge-aware diverse reranking RAG pipeline", "result": "achieved first place in the competition.", "conclusion": "The knowledge-aware diverse reranking RAG pipeline achieved first place in the competition."}}
{"id": "2506.20326", "categories": ["cs.CV", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2506.20326", "abs": "https://arxiv.org/abs/2506.20326", "authors": ["Sergio Torres Aguilar"], "title": "From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents", "comment": null, "summary": "Robust Document Layout Analysis (DLA) is critical for the automated\nprocessing and understanding of historical documents with complex page\norganizations. This paper benchmarks five state-of-the-art object detection\narchitectures on three annotated datasets representing a spectrum of\ncodicological complexity: The e-NDP, a corpus of Parisian medieval registers\n(1326-1504); CATMuS, a diverse multiclass dataset derived from various medieval\nand modern sources (ca.12th-17th centuries) and HORAE, a corpus of decorated\nbooks of hours (ca.13th-16th centuries). We evaluate two Transformer-based\nmodels (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and\nYOLO-World). Our findings reveal significant performance variations dependent\non model architecture, data set characteristics, and bounding box\nrepresentation. In the e-NDP dataset, Co-DETR achieves state-of-the-art results\n(0.752 mAP@.50:.95), closely followed by YOLOv11X-OBB (0.721). Conversely, on\nthe more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB\nsignificantly outperforms all other models (0.564 and 0.568, respectively).\nThis study unequivocally demonstrates that using Oriented Bounding Boxes (OBB)\nis not a minor refinement but a fundamental requirement for accurately modeling\nthe non-Cartesian nature of historical manuscripts. We conclude that a key\ntrade-off exists between the global context awareness of Transformers, ideal\nfor structured layouts, and the superior generalization of CNN-OBB models for\nvisually diverse and complex documents.", "AI": {"tldr": "Compares object detection architectures for document layout analysis on historical documents, finding that CNN-OBB models outperform Transformers on complex layouts.", "motivation": "Robust Document Layout Analysis (DLA) is critical for the automated processing and understanding of historical documents with complex page organizations.", "method": "Benchmarks five state-of-the-art object detection architectures on three annotated datasets: e-NDP, CATMuS, and HORAE. Evaluates two Transformer-based models (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and YOLO-World).", "result": "Significant performance variations dependent on model architecture, data set characteristics, and bounding box representation. On the e-NDP dataset, Co-DETR achieves state-of-the-art results, while on the more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB significantly outperforms all other models. Using Oriented Bounding Boxes (OBB) is a fundamental requirement for accurately modeling the non-Cartesian nature of historical manuscripts.", "conclusion": "A key trade-off exists between the global context awareness of Transformers, ideal for structured layouts, and the superior generalization of CNN-OBB models for visually diverse and complex documents."}}
{"id": "2506.19999", "categories": ["cs.LG", "cs.CL", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2506.19999", "abs": "https://arxiv.org/abs/2506.19999", "authors": ["Francesco Ignazio Re", "Andreas Opedal", "Glib Manaiev", "Mario Giulianelli", "Ryan Cotterell"], "title": "A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior", "comment": "ACL 2025", "summary": "Reading is a process that unfolds across space and time, alternating between\nfixations where a reader focuses on a specific point in space, and saccades\nwhere a reader rapidly shifts their focus to a new point. An ansatz of\npsycholinguistics is that modeling a reader's fixations and saccades yields\ninsight into their online sentence processing. However, standard approaches to\nsuch modeling rely on aggregated eye-tracking measurements and models that\nimpose strong assumptions, ignoring much of the spatio-temporal dynamics that\noccur during reading. In this paper, we propose a more general probabilistic\nmodel of reading behavior, based on a marked spatio-temporal point process,\nthat captures not only how long fixations last, but also where they land in\nspace and when they take place in time. The saccades are modeled using a Hawkes\nprocess, which captures how each fixation excites the probability of a new\nfixation occurring near it in time and space. The duration time of fixation\nevents is modeled as a function of fixation-specific predictors convolved\nacross time, thus capturing spillover effects. Empirically, our Hawkes process\nmodel exhibits a better fit to human saccades than baselines. With respect to\nfixation durations, we observe that incorporating contextual surprisal as a\npredictor results in only a marginal improvement in the model's predictive\naccuracy. This finding suggests that surprisal theory struggles to explain\nfine-grained eye movements.", "AI": {"tldr": "propose a more general probabilistic model of reading behavior, based on a marked spatio-temporal point process, that captures not only how long fixations last, but also where they land in space and when they take place in time", "motivation": "modeling a reader's fixations and saccades yields insight into their online sentence processing. However, standard approaches to such modeling rely on aggregated eye-tracking measurements and models that impose strong assumptions, ignoring much of the spatio-temporal dynamics that occur during reading", "method": "a more general probabilistic model of reading behavior, based on a marked spatio-temporal point process", "result": "Hawkes process model exhibits a better fit to human saccades than baselines", "conclusion": "incorporating contextual surprisal as a predictor results in only a marginal improvement in the model's predictive accuracy. This finding suggests that surprisal theory struggles to explain fine-grained eye movements."}}
{"id": "2506.20103", "categories": ["cs.CV", "cs.AI", "I.4"], "pdf": "https://arxiv.org/pdf/2506.20103", "abs": "https://arxiv.org/abs/2506.20103", "authors": ["Jiahao Lin", "Weixuan Peng", "Bojia Zi", "Yifeng Gao", "Xianbiao Qi", "Xingjun Ma", "Yu-Gang Jiang"], "title": "BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos", "comment": "7 page,4 figures,2 tables", "summary": "Recent advances in deep generative models have led to significant progress in\nvideo generation, yet the fidelity of AI-generated videos remains limited.\nSynthesized content often exhibits visual artifacts such as temporally\ninconsistent motion, physically implausible trajectories, unnatural object\ndeformations, and local blurring that undermine realism and user trust.\nAccurate detection and spatial localization of these artifacts are crucial for\nboth automated quality control and for guiding the development of improved\ngenerative models. However, the research community currently lacks a\ncomprehensive benchmark specifically designed for artifact localization in AI\ngenerated videos. Existing datasets either restrict themselves to video or\nframe level detection or lack the fine-grained spatial annotations necessary\nfor evaluating localization methods. To address this gap, we introduce\nBrokenVideos, a benchmark dataset of 3,254 AI-generated videos with\nmeticulously annotated, pixel-level masks highlighting regions of visual\ncorruption. Each annotation is validated through detailed human inspection to\nensure high quality ground truth. Our experiments show that training state of\nthe art artifact detection models and multi modal large language models (MLLMs)\non BrokenVideos significantly improves their ability to localize corrupted\nregions. Through extensive evaluation, we demonstrate that BrokenVideos\nestablishes a critical foundation for benchmarking and advancing research on\nartifact localization in generative video models. The dataset is available at:\nhttps://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.", "AI": {"tldr": "Introduce BrokenVideos, a benchmark dataset for artifact localization in AI-generated videos, to address the lack of comprehensive benchmark.", "motivation": "Synthesized content often exhibits visual artifacts such as temporally inconsistent motion, physically implausible trajectories, unnatural object deformations, and local blurring that undermine realism and user trust. Accurate detection and spatial localization of these artifacts are crucial for both automated quality control and for guiding the development of improved generative models. However, the research community currently lacks a comprehensive benchmark specifically designed for artifact localization in AI generated videos.", "method": "introduce BrokenVideos, a benchmark dataset of 3,254 AI-generated videos with meticulously annotated, pixel-level masks highlighting regions of visual corruption. Each annotation is validated through detailed human inspection", "result": "training state of the art artifact detection models and multi modal large language models (MLLMs) on BrokenVideos significantly improves their ability to localize corrupted regions", "conclusion": "BrokenVideos establishes a critical foundation for benchmarking and advancing research on artifact localization in generative video models."}}
{"id": "2506.20009", "categories": ["cs.AI", "cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2506.20009", "abs": "https://arxiv.org/abs/2506.20009", "authors": ["Konstantinos Vrettos", "Michail E. Klontzas"], "title": "Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks", "comment": "18 pages, 3 Figures", "summary": "Background The increasing adoption of Artificial Intelligence (AI) in\nhealthcare has sparked growing concerns about its environmental and ethical\nimplications. Commercial Large Language Models (LLMs), such as ChatGPT and\nDeepSeek, require substantial resources, while the utilization of these systems\nfor medical purposes raises critical issues regarding patient privacy and\nsafety. Methods We developed a customizable Retrieval-Augmented Generation\n(RAG) framework for medical tasks, which monitors its energy usage and CO2\nemissions. This system was then used to create RAGs based on various\nopen-source LLMs. The tested models included both general purpose models like\nllama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs\nperformance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs\no4-mini model. A dataset of medical questions was used for the evaluation.\nResults Custom RAG models outperformed commercial models in accuracy and energy\nconsumption. The RAG model built on llama3.1:8B achieved the highest accuracy\n(58.5%) and was significantly better than other models, including o4-mini and\nDeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption\nand CO2 footprint among all models, with a Performance per kWh of 0.52 and a\ntotal CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x\ntimes more accuracy points per kWh and 172% less electricity usage while\nmaintaining higher accuracy. Conclusion Our study demonstrates that local LLMs\ncan be leveraged to develop RAGs that outperform commercial, online LLMs in\nmedical tasks, while having a smaller environmental impact. Our modular\nframework promotes sustainable AI development, reducing electricity usage and\naligning with the UNs Sustainable Development Goals.", "AI": {"tldr": "\u672c\u5730 LLM \u53ef\u4ee5\u6784\u5efa\u6bd4\u5546\u4e1a\u6a21\u578b\u66f4\u597d\u3001\u66f4\u73af\u4fdd\u7684\u533b\u7597 RAG\u3002", "motivation": "\u533b\u7597\u4fdd\u5065\u4e2d\u8d8a\u6765\u8d8a\u591a\u5730\u91c7\u7528\u4eba\u5de5\u667a\u80fd (AI) \u5f15\u53d1\u4e86\u5bf9\u5176\u73af\u5883\u548c\u4f26\u7406\u5f71\u54cd\u7684\u65e5\u76ca\u5173\u6ce8\u3002ChatGPT \u548c DeepSeek \u7b49\u5546\u4e1a\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u9700\u8981\u5927\u91cf\u8d44\u6e90\uff0c\u800c\u5c06\u8fd9\u4e9b\u7cfb\u7edf\u7528\u4e8e\u533b\u7597\u76ee\u7684\u4f1a\u5f15\u53d1\u6709\u5173\u60a3\u8005\u9690\u79c1\u548c\u5b89\u5168\u7684\u5173\u952e\u95ee\u9898\u3002", "method": "\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u4e8e\u533b\u7597\u4efb\u52a1\u7684\u53ef\u5b9a\u5236\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u76d1\u63a7\u5176\u80fd\u6e90\u4f7f\u7528\u548c CO2 \u6392\u653e\u3002\u7136\u540e\uff0c\u8be5\u7cfb\u7edf\u7528\u4e8e\u521b\u5efa\u57fa\u4e8e\u5404\u79cd\u5f00\u6e90 LLM \u7684 RAG\u3002\u6d4b\u8bd5\u7684\u6a21\u578b\u5305\u62ec\u901a\u7528\u6a21\u578b\uff08\u5982 llama3.1:8b\uff09\u548c\u533b\u5b66\u9886\u57df\u7279\u5b9a\u6a21\u578b\uff08medgemma-4b-it\uff09\u3002\u6211\u4eec\u5c06\u6700\u4f73 RAG \u7684\u6027\u80fd\u548c\u80fd\u8017\u4e0e DeepSeekV3-R1 \u548c OpenAI \u7684 o4-mini \u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u4f7f\u7528\u533b\u5b66\u95ee\u9898\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9a\u5236 RAG \u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u80fd\u8017\u65b9\u9762\u4f18\u4e8e\u5546\u4e1a\u6a21\u578b\u3002\u57fa\u4e8e llama3.1:8B \u6784\u5efa\u7684 RAG \u6a21\u578b\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u51c6\u786e\u7387 (58.5%)\uff0c\u5e76\u4e14\u660e\u663e\u4f18\u4e8e\u5305\u62ec o4-mini \u548c DeepSeekV3-R1 \u5728\u5185\u7684\u5176\u4ed6\u6a21\u578b\u3002llama3.1-RAG \u5728\u6240\u6709\u6a21\u578b\u4e2d\u4e5f\u8868\u73b0\u51fa\u6700\u4f4e\u7684\u80fd\u8017\u548c CO2 \u6392\u653e\uff0c\u6bcf\u5343\u74e6\u65f6\u7684\u6027\u80fd\u4e3a 0.52\uff0c\u603b CO2 \u6392\u653e\u91cf\u4e3a 473 \u514b\u3002\u4e0e o4-mini \u76f8\u6bd4\uff0cllama3.1-RAG \u5728\u4fdd\u6301\u66f4\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u6bcf\u5343\u74e6\u65f6\u5b9e\u73b0\u4e86 2.7 \u500d\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\uff0c\u5e76\u51cf\u5c11\u4e86 172% \u7684\u7535\u529b\u4f7f\u7528\u3002", "conclusion": "\u672c\u5730 LLM \u53ef\u7528\u4e8e\u5f00\u53d1 RAG\uff0c\u5176\u5728\u533b\u7597\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4f18\u4e8e\u5546\u4e1a\u5728\u7ebf LLM\uff0c\u540c\u65f6\u73af\u5883\u5f71\u54cd\u66f4\u5c0f\u3002\u6211\u4eec\u7684\u6a21\u5757\u5316\u6846\u67b6\u4fc3\u8fdb\u4e86\u53ef\u6301\u7eed AI \u5f00\u53d1\uff0c\u51cf\u5c11\u4e86\u7535\u529b\u4f7f\u7528\uff0c\u5e76\u4e0e\u8054\u5408\u56fd\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\u76f8\u4e00\u81f4\u3002"}}
{"id": "2506.19890", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19890", "abs": "https://arxiv.org/abs/2506.19890", "authors": ["Ziru Zhang", "Jiadong Yu", "Danny H. K. Tsang"], "title": "Causal-Aware Intelligent QoE Optimization for VR Interaction with Adaptive Keyframe Extraction", "comment": null, "summary": "The optimization of quality of experience (QoE) in multi-user virtual reality\n(VR) interactions demands a delicate balance between ultra-low latency,\nhigh-fidelity motion synchronization, and equitable resource allocation. While\nadaptive keyframe extraction mitigates transmission overhead, existing\napproaches often overlook the causal relationships among allocated bandwidth,\nCPU frequency, and user perception, limiting QoE gains. This paper proposes an\nintelligent framework to maximize QoE by integrating adaptive keyframe\nextraction with causal-aware reinforcement learning (RL). First, a novel QoE\nmetric is formulated using the Weber-Fechner Law, combining perceptual\nsensitivity, attention-driven priorities, and motion reconstruction accuracy.\nThe QoE optimization problem is then modeled as a mixed integer programming\n(MIP) task, jointly optimizing keyframe ratios, bandwidth, and computational\nresources under horizon-fairness constraints. We propose Partial State Causal\nDeep Deterministic Policy Gradient (PS-CDDPG), which integrates the Deep\nDeterministic Policy Gradient (DDPG) method with causal influence detection. By\nleveraging causal information regarding how QoE is influenced and determined by\nvarious actions, we explore actions guided by weights calculated from causal\ninference (CI), which in turn improves training efficiency. Experiments\nconducted with the CMU Motion Capture Database demonstrate that our framework\nsignificantly reduces interactive latency, enhances QoE, and maintains\nfairness, achieving superior performance compared to benchmark methods.", "AI": {"tldr": "This paper introduces an intelligent framework that maximizes QoE in multi-user VR by integrating adaptive keyframe extraction with causal-aware reinforcement learning. Experiments show it reduces latency, enhances QoE, and maintains fairness.", "motivation": "The optimization of quality of experience (QoE) in multi-user virtual reality (VR) interactions demands a delicate balance between ultra-low latency, high-fidelity motion synchronization, and equitable resource allocation. Existing approaches often overlook the causal relationships among allocated bandwidth, CPU frequency, and user perception, limiting QoE gains.", "method": "The paper proposes Partial State Causal Deep Deterministic Policy Gradient (PS-CDDPG), which integrates the Deep Deterministic Policy Gradient (DDPG) method with causal influence detection. The QoE optimization problem is modeled as a mixed integer programming (MIP) task, jointly optimizing keyframe ratios, bandwidth, and computational resources under horizon-fairness constraints.", "result": "The framework significantly reduces interactive latency, enhances QoE, and maintains fairness, achieving superior performance compared to benchmark methods.", "conclusion": "The proposed framework significantly reduces interactive latency, enhances QoE, and maintains fairness, achieving superior performance compared to benchmark methods."}}
{"id": "2506.20495", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.20495", "abs": "https://arxiv.org/abs/2506.20495", "authors": ["Haoze Wu", "Yunzhi Yao", "Wenhao Yu", "Huajun Chen", "Ningyu Zhang"], "title": "ReCode: Updating Code API Knowledge with Reinforcement Learning", "comment": "Work in progress", "summary": "Large Language Models (LLMs) exhibit remarkable code generation capabilities\nbut falter when adapting to frequent updates in external library APIs. This\ncritical limitation, stemming from reliance on outdated API knowledge from\ntheir training data, even with access to current documentation, impedes\nreliable code generation in dynamic environments. To tackle this issue, we\npropose ReCode (rule-based Reinforcement learning for Code Update), a novel\nframework that mimics human programmer adaptation to API changes. Specifically,\nwe construct a dataset of approximately 2,000 data entries to train the LLMs to\nperform version migration based on updated information. Then, we introduce a\nmodified string similarity metric for code evaluation as the reward for\nreinforcement learning. Our experiments demonstrate that ReCode substantially\nboosts LLMs' code generation performance in dynamic API scenarios, especially\non the unseen CodeUpdateArena task. Crucially, compared to supervised\nfine-tuning, ReCode has less impact on LLMs' general code generation abilities.\nWe apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and\nDAPO), all achieving consistent improvements. Notably, after training,\nQwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned\nmodel and the reasoning model with the same architecture. Code is available at\nhttps://github.com/zjunlp/ReCode.", "AI": {"tldr": "ReCode\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001API\u573a\u666f\u4e0b\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u4e14\u5bf9\u6a21\u578b\u901a\u7528\u4ee3\u7801\u751f\u6210\u80fd\u529b\u5f71\u54cd\u8f83\u5c0f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9002\u5e94\u5916\u90e8\u5e93API\u7684\u9891\u7e41\u66f4\u65b0\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u8fc7\u65f6\u7684API\u77e5\u8bc6\uff0c\u5373\u4f7f\u53ef\u4ee5\u8bbf\u95ee\u6700\u65b0\u7684\u6587\u6863\u3002", "method": "\u63d0\u51faReCode\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6a21\u4eff\u4eba\u7c7b\u7a0b\u5e8f\u5458\u9002\u5e94API\u53d8\u5316\uff0c\u6784\u5efa\u5305\u542b\u7ea62000\u6761\u6570\u636e\u7684\u6570\u636e\u96c6\u4ee5\u8bad\u7ec3LLM\u6267\u884c\u7248\u672c\u8fc1\u79fb\uff0c\u5e76\u5f15\u5165\u6539\u8fdb\u7684\u5b57\u7b26\u4e32\u76f8\u4f3c\u6027\u5ea6\u91cf\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u3002", "result": "ReCode\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u52a8\u6001API\u573a\u666f\u4e2d\u7684\u4ee3\u7801\u751f\u6210\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u672a\u89c1\u8fc7\u7684CodeUpdateArena\u4efb\u52a1\u4e0a\u3002\u4e0e\u76d1\u7763\u5fae\u8c03\u76f8\u6bd4\uff0cReCode\u5bf9LLM\u7684\u901a\u7528\u4ee3\u7801\u751f\u6210\u80fd\u529b\u5f71\u54cd\u8f83\u5c0f\u3002\u5728\u5404\u79cdLLM\u548c\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e0a\u7684\u5e94\u7528\u5747\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u6539\u8fdb\u3002", "conclusion": "ReCode\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001API\u573a\u666f\u4e0b\u7684\u4ee3\u7801\u751f\u6210\u6027\u80fd\uff0c\u4e14\u5bf9\u6a21\u578b\u901a\u7528\u4ee3\u7801\u751f\u6210\u80fd\u529b\u5f71\u54cd\u8f83\u5c0f\u3002Qwen2.5-Coder-7B\u5728\u8bad\u7ec3\u540e\u8d85\u8fc7\u4e8632B\u53c2\u6570\u7684\u4ee3\u7801\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u3002"}}
{"id": "2506.20073", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20073", "abs": "https://arxiv.org/abs/2506.20073", "authors": ["Kethmi Hirushini Hettige", "Jiahao Ji", "Cheng Long", "Shili Xiang", "Gao Cong", "Jingyuan Wang"], "title": "A Modular Multitask Reasoning Framework Integrating Spatio-temporal Models and LLMs", "comment": null, "summary": "Spatio-temporal data mining plays a pivotal role in informed decision making\nacross diverse domains. However, existing models are often restricted to narrow\ntasks, lacking the capacity for multi-task inference and complex long-form\nreasoning that require generation of in-depth, explanatory outputs. These\nlimitations restrict their applicability to real-world, multi-faceted decision\nscenarios. In this work, we introduce STReason, a novel framework that\nintegrates the reasoning strengths of large language models (LLMs) with the\nanalytical capabilities of spatio-temporal models for multi-task inference and\nexecution. Without requiring task-specific finetuning, STReason leverages\nin-context learning to decompose complex natural language queries into modular,\ninterpretable programs, which are then systematically executed to generate both\nsolutions and detailed rationales. To facilitate rigorous evaluation, we\nconstruct a new benchmark dataset and propose a unified evaluation framework\nwith metrics specifically designed for long-form spatio-temporal reasoning.\nExperimental results show that STReason significantly outperforms advanced LLM\nbaselines across all metrics, particularly excelling in complex,\nreasoning-intensive spatio-temporal scenarios. Human evaluations further\nvalidate STReason's credibility and practical utility, demonstrating its\npotential to reduce expert workload and broaden the applicability to real-world\nspatio-temporal tasks. We believe STReason provides a promising direction for\ndeveloping more capable and generalizable spatio-temporal reasoning systems.", "AI": {"tldr": "STReason\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u5b83\u96c6\u6210\u4e86LLM\u548c\u65f6\u7a7a\u6a21\u578b\uff0c\u7528\u4e8e\u591a\u4efb\u52a1\u63a8\u7406\u548c\u6267\u884c\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u65f6\u7a7a\u6570\u636e\u6316\u6398\u6a21\u578b\u901a\u5e38\u4ec5\u9650\u4e8e\u72ed\u7a84\u7684\u4efb\u52a1\uff0c\u7f3a\u4e4f\u591a\u4efb\u52a1\u63a8\u7406\u548c\u590d\u6742\u7684\u957f\u7bc7\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "STReason\u6846\u67b6\u96c6\u6210\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\u4e0e\u65f6\u7a7a\u6a21\u578b\u7684\u5206\u6790\u80fd\u529b\uff0c\u7528\u4e8e\u591a\u4efb\u52a1\u63a8\u7406\u548c\u6267\u884c\u3002\u5b83\u5229\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u5c06\u590d\u6742\u7684\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u5206\u89e3\u4e3a\u6a21\u5757\u5316\u7684\u3001\u53ef\u89e3\u91ca\u7684\u7a0b\u5e8f\uff0c\u7136\u540e\u7cfb\u7edf\u5730\u6267\u884c\u8fd9\u4e9b\u7a0b\u5e8f\u4ee5\u751f\u6210\u89e3\u51b3\u65b9\u6848\u548c\u8be6\u7ec6\u7684\u57fa\u672c\u539f\u7406\uff0c\u800c\u65e0\u9700\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u5fae\u8c03\u3002", "result": "STReason\u663e\u8457\u4f18\u4e8e\u5148\u8fdb\u7684LLM\u57fa\u7ebf\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u7684\u3001\u63a8\u7406\u5bc6\u96c6\u7684\u65f6\u7a7a\u573a\u666f\u4e2d\u3002\u4eba\u5de5\u8bc4\u4f30\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86STReason\u7684\u53ef\u4fe1\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "STReason\u5728\u590d\u6742\u65f6\u7a7a\u63a8\u7406\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u5148\u8fdb\u7684LLM\u57fa\u7ebf\uff0c\u5e76\u901a\u8fc7\u4e86\u4eba\u5de5\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u5176\u53ef\u4fe1\u5ea6\u548c\u5b9e\u7528\u6027\uff0c\u8868\u660e\u5176\u6709\u6f5c\u529b\u51cf\u5c11\u4e13\u5bb6\u5de5\u4f5c\u91cf\u5e76\u62d3\u5bbd\u5176\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2506.20134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20134", "abs": "https://arxiv.org/abs/2506.20134", "authors": ["Ningwei Xie", "Zizi Tian", "Lei Yang", "Xiao-Ping Zhang", "Meng Guo", "Jie Li"], "title": "From 2D to 3D Cognition: A Brief Survey of General World Models", "comment": null, "summary": "World models have garnered increasing attention in the development of\nartificial general intelligence (AGI), serving as computational frameworks for\nlearning representations of the external world and forecasting future states.\nWhile early efforts focused on 2D visual perception and simulation, recent\n3D-aware generative world models have demonstrated the ability to synthesize\ngeometrically consistent, interactive 3D environments, marking a shift toward\n3D spatial cognition. Despite rapid progress, the field lacks systematic\nanalysis to categorize emerging techniques and clarify their roles in advancing\n3D cognitive world models. This survey addresses this need by introducing a\nconceptual framework, providing a structured and forward-looking review of\nworld models transitioning from 2D perception to 3D cognition. Within this\nframework, we highlight two key technological drivers, particularly advances in\n3D representations and the incorporation of world knowledge, as fundamental\npillars. Building on these, we dissect three core cognitive capabilities that\nunderpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning,\nand 3D spatial interaction. We further examine the deployment of these\ncapabilities in real-world applications, including embodied AI, autonomous\ndriving, digital twin, and gaming/VR. Finally, we identify challenges across\ndata, modeling, and deployment, and outline future directions for advancing\nmore robust and generalizable 3D world models.", "AI": {"tldr": "This survey reviews 3D world models, focusing on 3D representations, world knowledge, 3D physical scene generation, 3D spatial reasoning, and 3D spatial interaction. It also examines applications and future directions.", "motivation": "The field lacks systematic analysis to categorize emerging techniques and clarify their roles in advancing 3D cognitive world models.", "method": "introduce a conceptual framework, providing a structured and forward-looking review of world models transitioning from 2D perception to 3D cognition. Within this framework, we highlight two key technological drivers, particularly advances in 3D representations and the incorporation of world knowledge, as fundamental pillars. Building on these, we dissect three core cognitive capabilities that underpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning, and 3D spatial interaction.", "result": "3D-aware generative world models have demonstrated the ability to synthesize geometrically consistent, interactive 3D environments, marking a shift toward 3D spatial cognition.", "conclusion": "This survey identifies challenges across data, modeling, and deployment, and outline future directions for advancing more robust and generalizable 3D world models."}}
{"id": "2506.20018", "categories": ["cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2506.20018", "abs": "https://arxiv.org/abs/2506.20018", "authors": ["Zechun Deng", "Ziwei Liu", "Ziqian Bi", "Junhao Song", "Chia Xin Liang", "Joe Yeong", "Junfeng Hao"], "title": "Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models", "comment": null, "summary": "This paper investigates real-time decision support systems that leverage\nlow-latency AI models, bringing together recent progress in holistic AI-driven\ndecision tools, integration with Edge-IoT technologies, and approaches for\neffective human-AI teamwork. It looks into how large language models can assist\ndecision-making, especially when resources are limited. The research also\nexamines the effects of technical developments such as DeLLMa, methods for\ncompressing models, and improvements for analytics on edge devices, while also\naddressing issues like limited resources and the need for adaptable frameworks.\nThrough a detailed review, the paper offers practical perspectives on\ndevelopment strategies and areas of application, adding to the field by\npointing out opportunities for more efficient and flexible AI-supported\nsystems. The conclusions set the stage for future breakthroughs in this\nfast-changing area, highlighting how AI can reshape real-time decision support.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u4f4e\u5ef6\u8fdf AI \u6a21\u578b\u7684\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u7740\u773c\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5728\u8d44\u6e90\u6709\u9650\u65f6\u534f\u52a9\u51b3\u7b56\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7a81\u7834\u94fa\u5e73\u9053\u8def\u3002", "motivation": "\u7814\u7a76\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5229\u7528\u4f4e\u5ef6\u8fdf AI \u6a21\u578b\uff0c\u6c47\u96c6\u4e86\u6574\u4f53 AI \u9a71\u52a8\u7684\u51b3\u7b56\u5de5\u5177\u3001\u4e0e Edge-IoT \u6280\u672f\u7684\u96c6\u6210\u4ee5\u53ca\u6709\u6548\u7684\u4eba\u5de5\u667a\u80fd\u56e2\u961f\u5408\u4f5c\u65b9\u6cd5\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u7740\u773c\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5728\u8d44\u6e90\u6709\u9650\u65f6\u534f\u52a9\u51b3\u7b56\u3002", "method": "\u901a\u8fc7\u8be6\u7ec6\u7684\u56de\u987e\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8e\u5f00\u53d1\u7b56\u7565\u548c\u5e94\u7528\u9886\u57df\u7684\u5b9e\u8df5\u89c2\u70b9\u3002", "result": "\u8003\u5bdf\u4e86 DeLLMa \u7b49\u6280\u672f\u53d1\u5c55\u7684\u5f71\u54cd\u3001\u538b\u7f29\u6a21\u578b\u7684\u65b9\u6cd5\u4ee5\u53ca\u8fb9\u7f18\u8bbe\u5907\u5206\u6790\u7684\u6539\u8fdb\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u8d44\u6e90\u6709\u9650\u548c\u9700\u8981\u9002\u5e94\u6027\u6846\u67b6\u7b49\u95ee\u9898\u3002", "conclusion": "AI \u6709\u53ef\u80fd\u91cd\u5851\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\uff0c\u4e3a\u672a\u6765\u7a81\u7834\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2506.19891", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19891", "abs": "https://arxiv.org/abs/2506.19891", "authors": ["Qinghui Gong", "Xue Yang", "Xiaohu Tang"], "title": "Orthogonal Soft Pruning for Efficient Class Unlearning", "comment": "11 pages,3 figures", "summary": "Machine unlearning aims to selectively remove class-specific knowledge from\npretrained neural networks to satisfy privacy regulations such as the GDPR.\nExisting methods typically face a trade-off between unlearning speed and\npreservation of predictive accuracy, often incurring either high computational\noverhead or significant performance degradation on retained classes. In this\npaper, we propose a novel class-aware soft pruning framework leveraging\northogonal convolutional kernel regularization to achieve rapid and precise\nforgetting with millisecond-level response times. By enforcing orthogonality\nconstraints during training, our method decorrelates convolutional filters and\ndisentangles feature representations, while efficiently identifying\nclass-specific channels through activation difference analysis. Extensive\nevaluations across multiple architectures and datasets demonstrate stable\npruning with near-instant execution, complete forgetting of targeted classes,\nand minimal accuracy loss on retained data. Experiments on CIFAR-10, CIFAR-100,\nand TinyImageNet confirm that our approach substantially reduces membership\ninference attack risks and accelerates unlearning by orders of magnitude\ncompared to state-of-the-art baselines. This framework provides an efficient,\npractical solution for real-time machine unlearning in Machine Learning as a\nService (MLaaS) scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b63\u4ea4\u5377\u79ef\u6838\u6b63\u5219\u5316\u7684\u7c7b\u611f\u77e5\u8f6f\u526a\u679d\u6846\u67b6\uff0c\u7528\u4e8e\u5feb\u901f\u800c\u7cbe\u786e\u7684\u673a\u5668\u5378\u8f7d\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u673a\u5668\u5378\u8f7d\u65e8\u5728\u6709\u9009\u62e9\u5730\u4ece\u9884\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u4e2d\u5220\u9664\u7279\u5b9a\u4e8e\u7c7b\u7684\u77e5\u8bc6\uff0c\u4ee5\u6ee1\u8db3 GDPR \u7b49\u9690\u79c1\u6cd5\u89c4\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9762\u4e34\u5378\u8f7d\u901f\u5ea6\u548c\u9884\u6d4b\u7cbe\u5ea6\u4fdd\u6301\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u901a\u5e38\u4f1a\u4ea7\u751f\u9ad8\u8ba1\u7b97\u5f00\u9500\u6216\u4fdd\u7559\u7c7b\u522b\u7684\u663e\u7740\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7c7b\u611f\u77e5\u8f6f\u526a\u679d\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u6b63\u4ea4\u5377\u79ef\u6838\u6b63\u5219\u5316\u6765\u5b9e\u73b0\u5feb\u901f\u800c\u7cbe\u786e\u7684\u9057\u5fd8\uff0c\u54cd\u5e94\u65f6\u95f4\u8fbe\u5230\u6beb\u79d2\u7ea7\u3002", "result": "\u5728\u591a\u4e2a\u67b6\u6784\u548c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u7a33\u5b9a\u7684\u526a\u679d\u548c\u63a5\u8fd1\u77ac\u65f6\u7684\u6267\u884c\u901f\u5ea6\uff0c\u53ef\u4ee5\u5b8c\u5168\u5fd8\u8bb0\u76ee\u6807\u7c7b\u522b\uff0c\u5e76\u4e14\u4fdd\u7559\u6570\u636e\u7684\u7cbe\u5ea6\u635f\u5931\u6700\u5c0f\u3002\u5728 CIFAR-10\u3001CIFAR-100 \u548c TinyImageNet \u4e0a\u7684\u5b9e\u9a8c\u8bc1\u5b9e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5927\u5927\u964d\u4f4e\u4e86\u6210\u5458\u63a8\u7406\u653b\u51fb\u98ce\u9669\uff0c\u5e76\u5c06\u5378\u8f7d\u901f\u5ea6\u63d0\u9ad8\u4e86\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u673a\u5668\u5b66\u4e60\u5373\u670d\u52a1 (MLaaS) \u573a\u666f\u4e2d\u7684\u5b9e\u65f6\u673a\u5668\u5378\u8f7d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.20081", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20081", "abs": "https://arxiv.org/abs/2506.20081", "authors": ["Dhruv Gupta", "Gayathri Ganesh Lakshmy", "Yiqing Xie"], "title": "SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization", "comment": null, "summary": "Retrieval-Augmented Code Generation (RACG) is a critical technique for\nenhancing code generation by retrieving relevant information. In this work, we\nconduct an in-depth analysis of code retrieval by systematically masking\nspecific features while preserving code functionality. Our discoveries include:\n(1) although trained on code, current retrievers heavily rely on surface-level\ntextual features (e.g., docstrings, identifier names), and (2) they exhibit a\nstrong bias towards well-documented code, even if the documentation is\nirrelevant.Based on our discoveries, we propose SACL, a framework that enriches\ntextual information and reduces bias by augmenting code or structural knowledge\nwith semantic information. Extensive experiments show that SACL substantially\nimproves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on HumanEval /\nMBPP / SWE-Bench-Lite), which also leads to better code generation performance\n(e.g., by 4.88% Pass@1 on HumanEval).", "AI": {"tldr": "The paper analyzes code retrieval, finds issues with current methods, and proposes SACL to improve performance.", "motivation": "current retrievers heavily rely on surface-level textual features and exhibit a strong bias towards well-documented code, even if the documentation is irrelevant", "method": "a framework that enriches textual information and reduces bias by augmenting code or structural knowledge with semantic information", "result": "SACL substantially improves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on HumanEval / MBPP / SWE-Bench-Lite), which also leads to better code generation performance (e.g., by 4.88% Pass@1 on HumanEval).", "conclusion": "SACL improves code retrieval and code generation performance."}}
{"id": "2506.20151", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20151", "abs": "https://arxiv.org/abs/2506.20151", "authors": ["Haipeng Fan", "Shiyuan Zhang", "Baohunesitu", "Zihang Guo", "Huaiwen Zhang"], "title": "EAR: Erasing Concepts from Unified Autoregressive Models", "comment": "11 pages, 7 figures, 1 tables", "summary": "Autoregressive (AR) models have achieved unified and strong performance\nacross both visual understanding and image generation tasks. However, removing\nundesired concepts from AR models while maintaining overall generation quality\nremains an open challenge. In this paper, we propose Erasure Autoregressive\nModel (EAR), a fine-tuning method for effective and utility-preserving concept\nerasure in AR models. Specifically, we introduce Windowed Gradient Accumulation\n(WGA) strategy to align patch-level decoding with erasure objectives, and\nThresholded Loss Masking (TLM) strategy to protect content unrelated to the\ntarget concept during fine-tuning. Furthermore, we propose a novel benchmark,\nErase Concept Generator and Visual Filter (ECGVF), aim at provide a more\nrigorous and comprehensive foundation for evaluating concept erasure in AR\nmodels. Specifically, we first employ structured templates across diverse large\nlanguage models (LLMs) to pre-generate a large-scale corpus of\ntarget-replacement concept prompt pairs. Subsequently, we generate images from\nthese prompts and subject them to rigorous filtering via a visual classifier to\nensure concept fidelity and alignment. Extensive experimental results conducted\non the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR\nachieves marked improvements in both erasure effectiveness and model utility\npreservation. Code is available at: https://github.com/immc-lab/ear/", "AI": {"tldr": "propose Erasure Autoregressive Model (EAR), a fine-tuning method for effective and utility-preserving concept erasure in AR models, and propose a novel benchmark, Erase Concept Generator and Visual Filter (ECGVF), aim at provide a more rigorous and comprehensive foundation for evaluating concept erasure in AR models", "motivation": "removing undesired concepts from AR models while maintaining overall generation quality remains an open challenge", "method": "introduce Windowed Gradient Accumulation (WGA) strategy to align patch-level decoding with erasure objectives, and Thresholded Loss Masking (TLM) strategy to protect content unrelated to the target concept during fine-tuning", "result": "Extensive experimental results conducted on the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR achieves marked improvements in both erasure effectiveness and model utility preservation.", "conclusion": "EAR achieves marked improvements in both erasure effectiveness and model utility preservation."}}
{"id": "2506.20020", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20020", "abs": "https://arxiv.org/abs/2506.20020", "authors": ["Saloni Dash", "Am\u00e9lie Reymond", "Emma S. Spiro", "Aylin Caliskan"], "title": "Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning", "comment": null, "summary": "Reasoning in humans is prone to biases due to underlying motivations like\nidentity protection, that undermine rational decision-making and judgment. This\nmotivated reasoning at a collective level can be detrimental to society when\ndebating critical issues such as human-driven climate change or vaccine safety,\nand can further aggravate political polarization. Prior studies have reported\nthat large language models (LLMs) are also susceptible to human-like cognitive\nbiases, however, the extent to which LLMs selectively reason toward\nidentity-congruent conclusions remains largely unexplored. Here, we investigate\nwhether assigning 8 personas across 4 political and socio-demographic\nattributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and\nproprietary) across two reasoning tasks from human-subject studies -- veracity\ndiscernment of misinformation headlines and evaluation of numeric scientific\nevidence -- we find that persona-assigned LLMs have up to 9% reduced veracity\ndiscernment relative to models without personas. Political personas\nspecifically, are up to 90% more likely to correctly evaluate scientific\nevidence on gun control when the ground truth is congruent with their induced\npolitical identity. Prompt-based debiasing methods are largely ineffective at\nmitigating these effects. Taken together, our empirical findings are the first\nto suggest that persona-assigned LLMs exhibit human-like motivated reasoning\nthat is hard to mitigate through conventional debiasing prompts -- raising\nconcerns of exacerbating identity-congruent reasoning in both LLMs and humans.", "AI": {"tldr": "LLMs with assigned personas exhibit human-like motivated reasoning, which is difficult to mitigate and may worsen identity-based reasoning in both LLMs and humans.", "motivation": "Motivated reasoning at a collective level can be detrimental to society, and prior studies have reported that large language models (LLMs) are also susceptible to human-like cognitive biases, however, the extent to which LLMs selectively reason toward identity-congruent conclusions remains largely unexplored.", "method": "Assigning 8 personas across 4 political and socio-demographic attributes to induce motivated reasoning in LLMs, and testing 8 LLMs across two reasoning tasks from human-subject studies.", "result": "Persona-assigned LLMs have up to 9% reduced veracity discernment relative to models without personas. Political personas specifically, are up to 90% more likely to correctly evaluate scientific evidence on gun control when the ground truth is congruent with their induced political identity. Prompt-based debiasing methods are largely ineffective at mitigating these effects.", "conclusion": "Persona-assigned LLMs exhibit human-like motivated reasoning that is hard to mitigate through conventional debiasing prompts, raising concerns of exacerbating identity-congruent reasoning in both LLMs and humans."}}
{"id": "2506.19893", "categories": ["cs.LG", "cs.AI", "cs.IT", "eess.IV", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.19893", "abs": "https://arxiv.org/abs/2506.19893", "authors": ["Jingzhi Hu", "Geoffrey Ye Li"], "title": "Distillation-Enabled Knowledge Alignment for Generative Semantic Communications in AIGC Provisioning Tasks", "comment": null, "summary": "Due to the surging amount of AI-generated content (AIGC), its provisioning to\nedges and mobile users from the cloud incurs substantial traffic on networks.\nGenerative semantic communication (GSC) offers a promising solution by\ntransmitting highly compact information, i.e., prompt text and latent\nrepresentations, instead of high-dimensional AIGC data. However, GSC relies on\nthe alignment between the knowledge in the cloud generative AI (GAI) and that\npossessed by the edges and users, and between the knowledge for wireless\ntransmission and that of actual channels, which remains challenging. In this\npaper, we propose DeKA-g, a distillation-enabled knowledge alignment algorithm\nfor GSC systems. The core idea is to distill the generation knowledge from the\ncloud-GAI into low-rank matrices, which can be incorporated by the edge and\nused to adapt the transmission knowledge to diverse wireless channel\nconditions. DeKA-g comprises two novel methods: metaword-aided knowledge\ndistillation (MAKD) and variable-rate grouped SNR adaptation (VGSA). For MAKD,\nan optimized metaword is employed to enhance the efficiency of knowledge\ndistillation, while VGSA enables efficient adaptation to diverse compression\nrates and SNR ranges. From simulation results, DeKA-g improves the alignment\nbetween the edge-generated images and the cloud-generated ones by 44%.\nMoreover, it adapts to compression rates with 116% higher efficiency than the\nbaseline and enhances the performance in low-SNR conditions by 28%.", "AI": {"tldr": "This paper proposes DeKA-g, a distillation-enabled knowledge alignment algorithm for GSC systems, which improves the alignment between edge and cloud generated images, adapts to compression rates more efficiently, and enhances performance in low-SNR conditions.", "motivation": "Due to the surging amount of AI-generated content (AIGC), its provisioning to edges and mobile users from the cloud incurs substantial traffic on networks. Generative semantic communication (GSC) offers a promising solution by transmitting highly compact information. However, GSC relies on the alignment between the knowledge in the cloud generative AI (GAI) and that possessed by the edges and users, and between the knowledge for wireless transmission and that of actual channels, which remains challenging.", "method": "DeKA-g, a distillation-enabled knowledge alignment algorithm for GSC systems, comprises two novel methods: metaword-aided knowledge distillation (MAKD) and variable-rate grouped SNR adaptation (VGSA).", "result": "DeKA-g improves the alignment between the edge-generated images and the cloud-generated ones by 44%. Moreover, it adapts to compression rates with 116% higher efficiency than the baseline and enhances the performance in low-SNR conditions by 28%.", "conclusion": "DeKA-g improves the alignment between the edge-generated images and the cloud-generated ones by 44%. Moreover, it adapts to compression rates with 116% higher efficiency than the baseline and enhances the performance in low-SNR conditions by 28%."}}
{"id": "2506.20083", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20083", "abs": "https://arxiv.org/abs/2506.20083", "authors": ["Yingji Zhang", "Danilo S. Carvalho", "Andr\u00e9 Freitas"], "title": "Bridging Compositional and Distributional Semantics: A Survey on Latent Semantic Geometry via AutoEncoder", "comment": "In progress", "summary": "Integrating compositional and symbolic properties into current distributional\nsemantic spaces can enhance the interpretability, controllability,\ncompositionality, and generalisation capabilities of Transformer-based\nauto-regressive language models (LMs). In this survey, we offer a novel\nperspective on latent space geometry through the lens of compositional\nsemantics, a direction we refer to as \\textit{semantic representation\nlearning}. This direction enables a bridge between symbolic and distributional\nsemantics, helping to mitigate the gap between them. We review and compare\nthree mainstream autoencoder architectures-Variational AutoEncoder (VAE),\nVector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE)-and examine the\ndistinctive latent geometries they induce in relation to semantic structure and\ninterpretability.", "AI": {"tldr": "This survey explores how integrating compositional and symbolic properties into language models can improve their performance. It focuses on the latent space geometry and reviews three autoencoder architectures (VAE, VQVAE, SAE).", "motivation": "Integrating compositional and symbolic properties into current distributional semantic spaces can enhance the interpretability, controllability, compositionality, and generalisation capabilities of Transformer-based auto-regressive language models (LMs). Enable a bridge between symbolic and distributional semantics, helping to mitigate the gap between them.", "method": "Review and compare three mainstream autoencoder architectures-Variational AutoEncoder (VAE), Vector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE).", "result": "a novel perspective on latent space geometry through the lens of compositional semantics, a direction we refer to as semantic representation learning", "conclusion": "This survey reviews and compares three mainstream autoencoder architectures (VAE, VQVAE, and SAE) and examines the distinctive latent geometries they induce in relation to semantic structure and interpretability."}}
{"id": "2506.20152", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.20152", "abs": "https://arxiv.org/abs/2506.20152", "authors": ["Deepak Ghimire", "Kilho Lee", "Seong-heum Kim"], "title": "Loss-Aware Automatic Selection of Structured Pruning Criteria for Deep Neural Network Acceleration", "comment": null, "summary": "Structured pruning is a well-established technique for compressing neural\nnetworks, making it suitable for deployment in resource-limited edge devices.\nThis paper presents an efficient Loss-Aware Automatic Selection of Structured\nPruning Criteria (LAASP) for slimming and accelerating deep neural networks.\nThe majority of pruning methodologies employ a sequential process consisting of\nthree stages: 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed\npruning technique adopts a pruning-while-training approach that eliminates the\nfirst stage and integrates the second and third stages into a single cycle. The\nautomatic selection of magnitude or similarity-based filter pruning criteria\nfrom a specified pool of criteria and the specific pruning layer at each\npruning iteration is guided by the network's overall loss on a small subset of\nthe training data. To mitigate the abrupt accuracy drop due to pruning, the\nnetwork is retrained briefly after each reduction of a predefined number of\nfloating-point operations (FLOPs). The optimal pruning rates for each layer in\nthe network are automatically determined, eliminating the need for manual\nallocation of fixed or variable pruning rates for each layer. Experiments on\nthe VGGNet and ResNet models on the CIFAR-10 and ImageNet benchmark datasets\ndemonstrate the effectiveness of the proposed method. In particular, the\nResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the\ntop-1 accuracy compared to state-of-the-art methods while reducing the network\nFLOPs by 52\\%. Furthermore, the ResNet50 model on the ImageNet dataset reduces\nFLOPs by more than 42\\% with a negligible 0.33\\% drop in top-5 accuracy. The\nsource code of this paper is publicly available online -\nhttps://github.com/ghimiredhikura/laasp.", "AI": {"tldr": "This paper presents LAASP, a pruning-while-training method for compressing neural networks. It automatically selects pruning criteria based on loss and achieves good accuracy and FLOPs reduction on CIFAR-10 and ImageNet.", "motivation": "The paper addresses the need for efficient structured pruning techniques to compress neural networks for deployment on resource-limited edge devices.", "method": "The paper introduces Loss-Aware Automatic Selection of Structured Pruning Criteria (LAASP), a pruning-while-training approach that integrates pruning and fine-tuning into a single cycle. It uses the network's loss on a small training subset to automatically select pruning criteria and layers, and retrains the network after reducing a predefined number of FLOPs.", "result": "Experiments on VGGNet and ResNet models on CIFAR-10 and ImageNet show that LAASP improves accuracy and reduces FLOPs. Specifically, ResNet56 and ResNet110 on CIFAR-10 improve top-1 accuracy, while ResNet50 on ImageNet reduces FLOPs by over 42% with a minimal accuracy drop.", "conclusion": "The proposed LAASP method demonstrates significant improvements in top-1 accuracy on CIFAR-10 with ResNet56 and ResNet110, and reduces FLOPs by over 42% on ImageNet with ResNet50 with a negligible accuracy drop."}}
{"id": "2506.20059", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20059", "abs": "https://arxiv.org/abs/2506.20059", "authors": ["Weijieying Ren", "Tianxiang Zhao", "Lei Wang", "Tianchun Wang", "Vasant Honavar"], "title": "DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have led to remarkable\nprogresses in medical consultation. However, existing medical LLMs overlook the\nessential role of Electronic Health Records (EHR) and focus primarily on\ndiagnosis recommendation, limiting their clinical applicability. We propose\nDiaLLM, the first medical LLM that integrates heterogeneous EHR data into\nclinically grounded dialogues, enabling clinical test recommendation, result\ninterpretation, and diagnosis prediction to better align with real-world\nmedical practice. To construct clinically grounded dialogues from EHR, we\ndesign a Clinical Test Reference (CTR) strategy that maps each clinical code to\nits corresponding description and classifies test results as \"normal\" or\n\"abnormal\". Additionally, DiaLLM employs a reinforcement learning framework for\nevidence acquisition and automated diagnosis. To handle the large action space,\nwe introduce a reject sampling strategy to reduce redundancy and improve\nexploration efficiency. Furthermore, a confirmation reward and a\nclass-sensitive diagnosis reward are designed to guide accurate diagnosis\nprediction. Extensive experimental results demonstrate that DiaLLM outperforms\nbaselines in clinical test recommendation and diagnosis prediction.", "AI": {"tldr": "DiaLLM\u662f\u4e00\u4e2a\u5c06EHR\u6570\u636e\u6574\u5408\u5230\u4e34\u5e8a\u5bf9\u8bdd\u4e2d\u7684\u533b\u7597LLM\uff0c\u7528\u4e8e\u4e34\u5e8a\u6d4b\u8bd5\u63a8\u8350\u548c\u8bca\u65ad\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u7597LLM\u5ffd\u7565\u4e86\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55(EHR)\u7684\u91cd\u8981\u4f5c\u7528\uff0c\u4e3b\u8981\u5173\u6ce8\u8bca\u65ad\u63a8\u8350\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u4e34\u5e8a\u9002\u7528\u6027\u3002", "method": "\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e34\u5e8a\u6d4b\u8bd5\u53c2\u8003(CTR)\u7b56\u7565\uff0c\u5c06\u6bcf\u4e2a\u4e34\u5e8a\u4ee3\u7801\u6620\u5c04\u5230\u5176\u5bf9\u5e94\u7684\u63cf\u8ff0\uff0c\u5e76\u5c06\u6d4b\u8bd5\u7ed3\u679c\u5206\u7c7b\u4e3a\u201c\u6b63\u5e38\u201d\u6216\u201c\u5f02\u5e38\u201d\u3002\u6b64\u5916\uff0cDiaLLM\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8fdb\u884c\u8bc1\u636e\u83b7\u53d6\u548c\u81ea\u52a8\u8bca\u65ad\u3002\u4e3a\u4e86\u5904\u7406\u5927\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u62d2\u7edd\u62bd\u6837\u7b56\u7565\u6765\u51cf\u5c11\u5197\u4f59\u548c\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u3002\u6b64\u5916\uff0c\u786e\u8ba4\u5956\u52b1\u548c\u7c7b\u522b\u654f\u611f\u7684\u8bca\u65ad\u5956\u52b1\u65e8\u5728\u6307\u5bfc\u51c6\u786e\u7684\u8bca\u65ad\u9884\u6d4b\u3002", "result": "DiaLLM\u662f\u7b2c\u4e00\u4e2a\u5c06\u5f02\u6784EHR\u6570\u636e\u96c6\u6210\u5230\u4e34\u5e8a\u5bf9\u8bdd\u4e2d\u7684\u533b\u7597LLM\uff0c\u80fd\u591f\u8fdb\u884c\u4e34\u5e8a\u6d4b\u8bd5\u63a8\u8350\u3001\u7ed3\u679c\u89e3\u91ca\u548c\u8bca\u65ad\u9884\u6d4b\uff0c\u4ee5\u66f4\u597d\u5730\u4e0e\u771f\u5b9e\u4e16\u754c\u7684\u533b\u7597\u5b9e\u8df5\u76f8\u4e00\u81f4\u3002", "conclusion": "DiaLLM\u5728\u4e34\u5e8a\u6d4b\u8bd5\u63a8\u8350\u548c\u8bca\u65ad\u9884\u6d4b\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002"}}
{"id": "2506.19894", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.19894", "abs": "https://arxiv.org/abs/2506.19894", "authors": ["Antoine Pesenti", "Aidan OSullivan"], "title": "Explaining deep neural network models for electricity price forecasting with XAI", "comment": null, "summary": "Electricity markets are highly complex, involving lots of interactions and\ncomplex dependencies that make it hard to understand the inner workings of the\nmarket and what is driving prices. Econometric methods have been developed for\nthis, white-box models, however, they are not as powerful as deep neural\nnetwork models (DNN). In this paper, we use a DNN to forecast the price and\nthen use XAI methods to understand the factors driving the price dynamics in\nthe market. The objective is to increase our understanding of how different\nelectricity markets work. To do that, we apply explainable methods such as SHAP\nand Gradient, combined with visual techniques like heatmaps (saliency maps) to\nanalyse the behaviour and contributions of various features across five\nelectricity markets. We introduce the novel concepts of SSHAP values and SSHAP\nlines to enhance the complex representation of high-dimensional tabular models.", "AI": {"tldr": "Use a DNN and XAI methods to understand the factors driving the price dynamics in the market.", "motivation": "The objective is to increase our understanding of how different electricity markets work.", "method": "Use a DNN to forecast the price and then use XAI methods", "result": "increase our understanding of how different electricity markets work", "conclusion": "Use SHAP and Gradient to analyse the behaviour and contributions of various features across five electricity markets, introducing SSHAP values and SSHAP lines."}}
{"id": "2506.20093", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20093", "abs": "https://arxiv.org/abs/2506.20093", "authors": ["Yilin Wang", "Peixuan Lei", "Jie Song", "Yuzhe Hao", "Tao Chen", "Yuxuan Zhang", "Lei Jia", "Yuanxiang Li", "Zhongyu Wei"], "title": "ITFormer: Bridging Time Series and Natural Language for Multi-Modal QA with Large-Scale Multitask Dataset", "comment": null, "summary": "Time-series data are critical in diverse applications, such as industrial\nmonitoring, medical diagnostics, and climate research. However, effectively\nintegrating these high-dimensional temporal signals with natural language for\ndynamic, interactive tasks remains a significant challenge. To address this, we\nintroduce the Time-Series Question Answering (Time-Series QA) task and release\nEngineMT-QA, the first large-scale, multi-task, temporal-textual QA dataset\ndesigned to capture complex interactions between time-series signals and\nnatural language. Building on this resource, we propose the Instruct Time\nTransformer (ITFormer), a novel framework that bridges time-series encoders\nwith frozen large language models (LLMs). ITFormer effectively extracts,\naligns, and fuses temporal and textual features, achieving a strong improvement\nin QA accuracy over strong baselines with fewer than 1\\% additional trainable\nparameters. By combining computational efficiency with robust cross-modal\nmodeling, our work establishes a adaptable paradigm for integrating temporal\ndata with natural language, paving the way for new research and applications in\nmulti-modal AI. More details about the project, including datasets and code,\nare available at: https://pandalin98.github.io/itformer_site/", "AI": {"tldr": "Introduces EngineMT-QA, a large-scale time-series QA dataset, and ITFormer, a framework that bridges time-series encoders with frozen LLMs for improved QA accuracy.", "motivation": "Effectively integrating high-dimensional temporal signals with natural language for dynamic, interactive tasks is a significant challenge.", "method": "The paper proposes the Instruct Time Transformer (ITFormer) framework.", "result": "ITFormer achieves a strong improvement in QA accuracy over strong baselines with fewer than 1% additional trainable parameters.", "conclusion": "The paper introduces ITFormer, a framework that integrates time-series encoders with frozen large language models, achieving improved QA accuracy with minimal additional trainable parameters. This establishes a new paradigm for integrating temporal data with natural language."}}
{"id": "2506.20155", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20155", "abs": "https://arxiv.org/abs/2506.20155", "authors": ["Avadhoot Jadhav", "Ashutosh Srivastava", "Abhinav Java", "Silky Singh", "Tarun Ram Menta", "Surgan Jandial", "Balaji Krishnamurthy"], "title": "Towards Efficient Exemplar Based Image Editing with Multimodal VLMs", "comment": "Accepted at ECCV 2024 (AI4VA Workshop)", "summary": "Text-to-Image Diffusion models have enabled a wide array of image editing\napplications. However, capturing all types of edits through text alone can be\nchallenging and cumbersome. The ambiguous nature of certain image edits is\nbetter expressed through an exemplar pair, i.e., a pair of images depicting an\nimage before and after an edit respectively. In this work, we tackle\nexemplar-based image editing -- the task of transferring an edit from an\nexemplar pair to a content image(s), by leveraging pretrained text-to-image\ndiffusion models and multimodal VLMs. Even though our end-to-end pipeline is\noptimization-free, our experiments demonstrate that it still outperforms\nbaselines on multiple types of edits while being ~4x faster.", "AI": {"tldr": "This paper tackles exemplar-based image editing by transferring an edit from an exemplar pair to a content image(s), by leveraging pretrained text-to-image diffusion models and multimodal VLMs.", "motivation": "Capturing all types of edits through text alone can be challenging and cumbersome. The ambiguous nature of certain image edits is better expressed through an exemplar pair, i.e., a pair of images depicting an image before and after an edit respectively.", "method": "leveraging pretrained text-to-image diffusion models and multimodal VLMs", "result": "Experiments demonstrate that it still outperforms baselines on multiple types of edits while being ~4x faster.", "conclusion": "The proposed optimization-free pipeline outperforms baselines on multiple types of edits while being ~4x faster."}}
{"id": "2506.20130", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20130", "abs": "https://arxiv.org/abs/2506.20130", "authors": ["Adrien Bibal", "Steven N. Minton", "Deborah Khider", "Yolanda Gil"], "title": "AI Copilots for Reproducibility in Science: A Case Study", "comment": null, "summary": "Open science initiatives seek to make research outputs more transparent,\naccessible, and reusable, but ensuring that published findings can be\nindependently reproduced remains a persistent challenge. This paper introduces\nOpenPub, an AI-powered platform that supports researchers, reviewers, and\nreaders through a suite of modular copilots focused on key open science tasks.\nIn this work, we present the Reproducibility Copilot, which analyzes\nmanuscripts, code, and supplementary materials to generate structured Jupyter\nNotebooks and recommendations aimed at facilitating computational, or \"rote\",\nreproducibility. We conducted feasibility tests using previously studied\nresearch papers with known reproducibility benchmarks. Results indicate that\nOpenPub can substantially reduce reproduction time - from over 30 hours to\nabout 1 hour - while achieving high coverage of figures, tables, and results\nsuitable for computational reproduction. The system systematically detects\nbarriers to reproducibility, including missing hyperparameters, undocumented\npreprocessing steps, and incomplete or inaccessible datasets. These findings\nsuggest that AI-driven tools can meaningfully reduce the burden of\nreproducibility efforts and contribute to more transparent and verifiable\nscientific communication. The modular copilot architecture also provides a\nfoundation for extending AI assistance to additional open science objectives\nbeyond reproducibility.", "AI": {"tldr": "OpenPub, an AI-powered platform, introduces the Reproducibility Copilot, which analyzes research papers to generate structured Jupyter Notebooks and recommendations aimed at facilitating computational reproducibility. It reduces reproduction time from over 30 hours to about 1 hour.", "motivation": "Ensuring that published findings can be independently reproduced remains a persistent challenge.", "method": "The Reproducibility Copilot analyzes manuscripts, code, and supplementary materials to generate structured Jupyter Notebooks and recommendations aimed at facilitating computational, or \"rote\", reproducibility. Feasibility tests were conducted using previously studied research papers with known reproducibility benchmarks.", "result": "OpenPub can substantially reduce reproduction time - from over 30 hours to about 1 hour - while achieving high coverage of figures, tables, and results suitable for computational reproduction. The system systematically detects barriers to reproducibility, including missing hyperparameters, undocumented preprocessing steps, and incomplete or inaccessible datasets.", "conclusion": "AI-driven tools can reduce the burden of reproducibility efforts and contribute to more transparent and verifiable scientific communication. The modular copilot architecture also provides a foundation for extending AI assistance to additional open science objectives beyond reproducibility."}}
{"id": "2506.19895", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19895", "abs": "https://arxiv.org/abs/2506.19895", "authors": ["Miguel N. Font", "Jos\u00e9 L. Jorro-Aragoneses", "Carlos M. Ala\u00edz"], "title": "A Framework for Uncertainty Quantification Based on Nearest Neighbors Across Layers", "comment": "This paper has been accepted for presentation at ICANN 2025\n  (International Conference on Artificial Neural Networks) and will appear in\n  the conference proceedings published by Springer Nature in the Lecture Notes\n  in Computer Science (LNCS) series. The final authenticated version will be\n  available on the publisher website", "summary": "Neural Networks have high accuracy in solving problems where it is difficult\nto detect patterns or create a logical model. However, these algorithms\nsometimes return wrong solutions, which become problematic in high-risk domains\nlike medical diagnosis or autonomous driving. One strategy to detect and\nmitigate these errors is the measurement of the uncertainty over neural network\ndecisions. In this paper, we present a novel post-hoc framework for measuring\nthe uncertainty of a decision based on retrieved training cases that have a\nsimilar activation vector to the query for each layer. Based on these retrieved\ncases, we propose two new metrics: Decision Change and Layer Uncertainty, which\ncapture changes in nearest-neighbor class distributions across layers. We\nevaluated our approach in a classification model for two datasets: CIFAR-10 and\nMNIST. The results show that these metrics enhance uncertainty estimation,\nespecially in challenging classification tasks, outperforming softmax-based\nconfidence.", "AI": {"tldr": "This paper introduces a new way to measure uncertainty in neural networks, which can help detect errors in critical applications. The approach uses similar training cases to assess the reliability of a decision, showing improved performance over existing methods on CIFAR-10 and MNIST datasets.", "motivation": "Neural Networks sometimes return wrong solutions, which become problematic in high-risk domains like medical diagnosis or autonomous driving. One strategy to detect and mitigate these errors is the measurement of the uncertainty over neural network decisions.", "method": "a novel post-hoc framework for measuring the uncertainty of a decision based on retrieved training cases that have a similar activation vector to the query for each layer. Based on these retrieved cases, we propose two new metrics: Decision Change and Layer Uncertainty, which capture changes in nearest-neighbor class distributions across layers.", "result": "We evaluated our approach in a classification model for two datasets: CIFAR-10 and MNIST.", "conclusion": "The proposed metrics enhance uncertainty estimation, especially in challenging classification tasks, outperforming softmax-based confidence."}}
{"id": "2506.20100", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20100", "abs": "https://arxiv.org/abs/2506.20100", "authors": ["Vardhan Dongre", "Chi Gui", "Shubham Garg", "Hooshang Nayyeri", "Gokhan Tur", "Dilek Hakkani-T\u00fcr", "Vikram S. Adve"], "title": "MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations", "comment": "66 pages, 32 figures, 23 tables", "summary": "We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning\nand decision-making in consultative interaction settings. Designed for the\nagriculture domain, MIRAGE captures the full complexity of expert consultations\nby combining natural user queries, expert-authored responses, and image-based\ncontext, offering a high-fidelity benchmark for evaluating models on grounded\nreasoning, clarification strategies, and long-form generation in a real-world,\nknowledge-intensive domain. Grounded in over 35,000 real user-expert\ninteractions and curated through a carefully designed multi-step pipeline,\nMIRAGE spans diverse crop health, pest diagnosis, and crop management\nscenarios. The benchmark includes more than 7,000 unique biological entities,\ncovering plant species, pests, and diseases, making it one of the most\ntaxonomically diverse benchmarks available for vision-language models, grounded\nin the real world. Unlike existing benchmarks that rely on well-specified user\ninputs and closed-set taxonomies, MIRAGE features underspecified, context-rich\nscenarios with open-world settings, requiring models to infer latent knowledge\ngaps, handle rare entities, and either proactively guide the interaction or\nrespond. Project Page: https://mirage-benchmark.github.io", "AI": {"tldr": "MIRAGE\u662f\u4e00\u4e2a\u65b0\u7684\u519c\u4e1a\u9886\u57df\u7684\u591a\u6a21\u6001\u4e13\u5bb6\u7ea7\u63a8\u7406\u548c\u51b3\u7b56\u57fa\u51c6\uff0c\u5b83\u7ed3\u5408\u4e86\u81ea\u7136\u7528\u6237\u67e5\u8be2\u3001\u4e13\u5bb6\u54cd\u5e94\u548c\u56fe\u50cf\u4e0a\u4e0b\u6587\uff0c\u4ee5\u8bc4\u4f30\u6a21\u578b\u5728\u5b9e\u9645\u3001\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u7684\u63a8\u7406\u3001\u6f84\u6e05\u7b56\u7565\u548c\u957f\u6587\u672c\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u4f9d\u8d56\u4e8e\u660e\u786e\u7684\u7528\u6237\u8f93\u5165\u548c\u5c01\u95ed\u7684\u5206\u7c7b\uff0c\u800cMIRAGE\u5177\u6709\u4e0d\u660e\u786e\u7684\u3001\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u573a\u666f\u4e0e\u5f00\u653e\u4e16\u754c\u7684\u8bbe\u7f6e\u3002", "method": "MIRAGE\u901a\u8fc7\u7ed3\u5408\u81ea\u7136\u7684\u7528\u6237\u67e5\u8be2\u3001\u4e13\u5bb6\u7f16\u5199\u7684\u54cd\u5e94\u548c\u57fa\u4e8e\u56fe\u50cf\u7684\u4e0a\u4e0b\u6587\u6765\u6355\u83b7\u4e13\u5bb6\u54a8\u8be2\u7684\u5b8c\u6574\u590d\u6742\u6027\u3002", "result": "MIRAGE\u8de8\u8d8a\u4e86\u591a\u79cd\u4f5c\u7269\u5065\u5eb7\u3001\u75c5\u866b\u5bb3\u8bca\u65ad\u548c\u4f5c\u7269\u7ba1\u7406\u573a\u666f\uff0c\u5305\u62ec7,000\u591a\u79cd\u72ec\u7279\u7684\u751f\u7269\u5b9e\u4f53\u3002", "conclusion": "MIRAGE\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\u5b83\u5177\u6709\u5f00\u653e\u4e16\u754c\u7684\u8bbe\u7f6e\uff0c\u9700\u8981\u6a21\u578b\u6765\u63a8\u65ad\u6f5c\u5728\u7684\u77e5\u8bc6\u5dee\u8ddd\uff0c\u5904\u7406\u7f55\u89c1\u7684\u5b9e\u4f53\uff0c\u5e76\u4e3b\u52a8\u5730\u6307\u5bfc\u4ea4\u4e92\u6216\u54cd\u5e94\u3002"}}
{"id": "2506.20168", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20168", "abs": "https://arxiv.org/abs/2506.20168", "authors": ["Zhentao He", "Can Zhang", "Ziheng Wu", "Zhenghao Chen", "Yufei Zhan", "Yifan Li", "Zhao Zhang", "Xian Wang", "Minghui Qiu"], "title": "Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models", "comment": null, "summary": "Recent advancements in multimodal large language models have enhanced\ndocument understanding by integrating textual and visual information. However,\nexisting models exhibit incompleteness within their paradigm in real-world\nscenarios, particularly under visual degradation. In such conditions, the\ncurrent response paradigm often fails to adequately perceive visual degradation\nand ambiguity, leading to overreliance on linguistic priors or misaligned\nvisual-textual reasoning. This difficulty in recognizing uncertainty frequently\nresults in the generation of hallucinatory content, especially when a precise\nanswer is not feasible. To better demonstrate and analyze this phenomenon and\nproblem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR\nhallucination in degraded document understanding. This dataset includes test\nsamples spanning identity cards and invoices, with simulated real-world\ndegradations for OCR reliability. This setup allows for evaluating models'\ncapacity, under degraded input, to distinguish reliable visual information and\nanswer accordingly, thereby highlighting the challenge of avoiding\nhallucination on uncertain data. To achieve vision-faithful reasoning and\nthereby avoid the aforementioned issues, we further introduce a GRPO-based\nframework featuring a novel reward mechanism. By incorporating a self-awareness\nof visual uncertainty and an analysis method that initiates refusal to answer\nto increase task difficulty within our supervised fine-tuning and reinforcement\nlearning framework, we successfully mitigated hallucinations in ambiguous\nregions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model\nachieves a 22\\% absolute improvement in hallucination-free accuracy over GPT-4o\non KIE-HVQA and there is no significant performance drop in standard tasks,\nhighlighting both effectiveness and robustness.", "AI": {"tldr": "This paper addresses the problem of hallucination in multimodal models when processing degraded documents. It introduces a new benchmark dataset (KIE-HVQA) and a GRPO-based framework to improve accuracy and reduce hallucination, achieving significant improvements over GPT-4o.", "motivation": "Existing multimodal large language models exhibit incompleteness in real-world scenarios, particularly under visual degradation, leading to overreliance on linguistic priors or misaligned visual-textual reasoning and the generation of hallucinatory content.", "method": "The paper proposes KIE-HVQA, a benchmark dataset for evaluating OCR hallucination in degraded document understanding. It also introduces a GRPO-based framework featuring a novel reward mechanism with self-awareness of visual uncertainty and a refusal-to-answer strategy.", "result": "Experiments on Qwen2.5-VL demonstrate that the proposed 7B-parameter model achieves a 22% absolute improvement in hallucination-free accuracy over GPT-4o on KIE-HVQA, without significant performance drop in standard tasks.", "conclusion": "The paper introduces a GRPO-based framework with a novel reward mechanism to mitigate hallucinations in ambiguous regions, achieving a 22% absolute improvement in hallucination-free accuracy over GPT-4o on KIE-HVQA with a 7B-parameter model, while maintaining performance on standard tasks."}}
{"id": "2506.20249", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.20249", "abs": "https://arxiv.org/abs/2506.20249", "authors": ["Junyan Cheng", "Peter Clark", "Kyle Richardson"], "title": "Language Modeling by Language Models", "comment": null, "summary": "Can we leverage LLMs to model the process of discovering novel language model\n(LM) architectures? Inspired by real research, we propose a multi-agent LLM\napproach that simulates the conventional stages of research, from ideation and\nliterature search (proposal stage) to design implementation (code generation),\ngenerative pre-training, and downstream evaluation (verification). Using ideas\nfrom scaling laws, our system, Genesys, employs a Ladder of Scales approach;\nnew designs are proposed, adversarially reviewed, implemented, and selectively\nverified at increasingly larger model scales (14M$\\sim$350M parameters) with a\nnarrowing budget (the number of models we can train at each scale). To help\nmake discovery efficient and factorizable, Genesys uses a novel genetic\nprogramming backbone, which we show has empirical advantages over commonly used\ndirect prompt generation workflows (e.g., $\\sim$86\\% percentage point\nimprovement in successful design generation, a key bottleneck). We report\nexperiments involving 1,162 newly discovered designs (1,062 fully verified\nthrough pre-training) and find the best designs to be highly competitive with\nknown architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common\nbenchmarks). We couple these results with comprehensive system-level ablations\nand formal results, which give broader insights into the design of effective\nautonomous discovery systems.", "AI": {"tldr": "Genesys, a multi-agent LLM system, discovers novel LM architectures using genetic programming and scaling laws, achieving competitive performance with known architectures.", "motivation": "Leverage LLMs to model the process of discovering novel language model architectures, inspired by real research.", "method": "A multi-agent LLM approach that simulates conventional research stages, using a Ladder of Scales approach and a novel genetic programming backbone.", "result": "1,162 newly discovered designs (1,062 fully verified through pre-training), with significant improvement in successful design generation compared to direct prompt generation workflows.", "conclusion": "The best discovered designs are highly competitive with known architectures, outperforming GPT2 and Mamba2 on 6/9 common benchmarks. Comprehensive system-level ablations and formal results give broader insights into the design of effective autonomous discovery systems."}}
{"id": "2506.19929", "categories": ["cs.LG", "I.2.6"], "pdf": "https://arxiv.org/pdf/2506.19929", "abs": "https://arxiv.org/abs/2506.19929", "authors": ["Efe \u00c7ak\u0131r", "Patrick Dumond"], "title": "A Comparative Analysis of Reinforcement Learning and Conventional Deep Learning Approaches for Bearing Fault Diagnosis", "comment": "5 pages, 5 figures. To appear in the Proceedings of the Canadian\n  Society for Mechanical Engineering (CSME) Congress 2025", "summary": "Bearing faults in rotating machinery can lead to significant operational\ndisruptions and maintenance costs. Modern methods for bearing fault diagnosis\nrely heavily on vibration analysis and machine learning techniques, which often\nrequire extensive labeled data and may not adapt well to dynamic environments.\nThis study explores the feasibility of reinforcement learning (RL),\nspecifically Deep Q-Networks (DQNs), for bearing fault classification tasks in\nmachine condition monitoring to enhance the accuracy and adaptability of\nbearing fault diagnosis. The results demonstrate that while RL models developed\nin this study can match the performance of traditional supervised learning\nmodels under controlled conditions, they excel in adaptability when equipped\nwith optimized reward structures. However, their computational demands\nhighlight areas for further improvement. These findings demonstrate RL's\npotential to complement traditional methods, paving the way for adaptive\ndiagnostic frameworks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u8f74\u627f\u6545\u969c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u5728\u9002\u5e94\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u9700\u6c42\u8f83\u9ad8\u3002", "motivation": "\u65cb\u8f6c\u673a\u68b0\u4e2d\u7684\u8f74\u627f\u6545\u969c\u53ef\u80fd\u5bfc\u81f4\u91cd\u5927\u7684\u8fd0\u8425\u4e2d\u65ad\u548c\u7ef4\u62a4\u6210\u672c\u3002\u7528\u4e8e\u8f74\u627f\u6545\u969c\u8bca\u65ad\u7684\u73b0\u4ee3\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u632f\u52a8\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u6807\u8bb0\u6570\u636e\uff0c\u5e76\u4e14\u53ef\u80fd\u65e0\u6cd5\u5f88\u597d\u5730\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002", "method": "\u5f3a\u5316\u5b66\u4e60 (RL)\uff0c\u7279\u522b\u662f\u6df1\u5ea6 Q \u7f51\u7edc (DQN)", "result": "\u7814\u7a76\u8868\u660e\uff0c\u867d\u7136\u672c\u7814\u7a76\u4e2d\u5f00\u53d1\u7684\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u53ef\u4ee5\u5728\u53d7\u63a7\u6761\u4ef6\u4e0b\u4e0e\u4f20\u7edf\u7684\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u4f46\u5b83\u4eec\u5728\u914d\u5907\u4f18\u5316\u7684\u5956\u52b1\u7ed3\u6784\u65f6\uff0c\u5728\u9002\u5e94\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "RL\u6a21\u578b\u5728\u53d7\u63a7\u6761\u4ef6\u4e0b\u53ef\u4ee5\u4e0e\u4f20\u7edf\u7684\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u5e76\u4e14\u5728\u914d\u5907\u4f18\u5316\u7684\u5956\u52b1\u7ed3\u6784\u65f6\uff0c\u5b83\u4eec\u5728\u9002\u5e94\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u7136\u800c\uff0c\u5b83\u4eec\u7684\u8ba1\u7b97\u9700\u6c42\u7a81\u51fa\u4e86\u8fdb\u4e00\u6b65\u6539\u8fdb\u7684\u9886\u57df\u3002\u8fd9\u4e9b\u53d1\u73b0\u8bc1\u660e\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u8865\u5145\u4f20\u7edf\u65b9\u6cd5\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u81ea\u9002\u5e94\u8bca\u65ad\u6846\u67b6\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2506.20112", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2506.20112", "abs": "https://arxiv.org/abs/2506.20112", "authors": ["Songsoo Kim", "Seungtae Lee", "See Young Lee", "Joonho Kim", "Keechan Kan", "Dukyong Yoon"], "title": "A Multi-Pass Large Language Model Framework for Precise and Efficient Radiology Report Error Detection", "comment": "29 pages, 5 figures, 4 tables. Code available at\n  https://github.com/radssk/mp-rred", "summary": "Background: The positive predictive value (PPV) of large language model\n(LLM)-based proofreading for radiology reports is limited due to the low error\nprevalence. Purpose: To assess whether a three-pass LLM framework enhances PPV\nand reduces operational costs compared with baseline approaches. Materials and\nMethods: A retrospective analysis was performed on 1,000 consecutive radiology\nreports (250 each: radiography, ultrasonography, CT, MRI) from the MIMIC-III\ndatabase. Two external datasets (CheXpert and Open-i) were validation sets.\nThree LLM frameworks were tested: (1) single-prompt detector; (2) extractor\nplus detector; and (3) extractor, detector, and false-positive verifier.\nPrecision was measured by PPV and absolute true positive rate (aTPR).\nEfficiency was calculated from model inference charges and reviewer\nremuneration. Statistical significance was tested using cluster bootstrap,\nexact McNemar tests, and Holm-Bonferroni correction. Results: Framework PPV\nincreased from 0.063 (95% CI, 0.036-0.101, Framework 1) to 0.079 (0.049-0.118,\nFramework 2), and significantly to 0.159 (0.090-0.252, Framework 3; P<.001 vs.\nbaselines). aTPR remained stable (0.012-0.014; P>=.84). Operational costs per\n1,000 reports dropped to USD 5.58 (Framework 3) from USD 9.72 (Framework 1) and\nUSD 6.85 (Framework 2), reflecting reductions of 42.6% and 18.5%, respectively.\nHuman-reviewed reports decreased from 192 to 88. External validation supported\nFramework 3's superior PPV (CheXpert 0.133, Open-i 0.105) and stable aTPR\n(0.007). Conclusion: A three-pass LLM framework significantly enhanced PPV and\nreduced operational costs, maintaining detection performance, providing an\neffective strategy for AI-assisted radiology report quality assurance.", "AI": {"tldr": "A three-pass LLM framework improves the accuracy and efficiency of AI-assisted radiology report quality assurance.", "motivation": "The positive predictive value (PPV) of large language model (LLM)-based proofreading for radiology reports is limited due to the low error prevalence.", "method": "A retrospective analysis was performed on 1,000 consecutive radiology reports (250 each: radiography, ultrasonography, CT, MRI) from the MIMIC-III database. Two external datasets (CheXpert and Open-i) were validation sets. Three LLM frameworks were tested: (1) single-prompt detector; (2) extractor plus detector; and (3) extractor, detector, and false-positive verifier.", "result": "Framework PPV increased from 0.063 (95% CI, 0.036-0.101, Framework 1) to 0.079 (0.049-0.118, Framework 2), and significantly to 0.159 (0.090-0.252, Framework 3; P<.001 vs. baselines). aTPR remained stable (0.012-0.014; P>=.84). Operational costs per 1,000 reports dropped to USD 5.58 (Framework 3) from USD 9.72 (Framework 1) and USD 6.85 (Framework 2), reflecting reductions of 42.6% and 18.5%, respectively. Human-reviewed reports decreased from 192 to 88. External validation supported Framework 3's superior PPV (CheXpert 0.133, Open-i 0.105) and stable aTPR (0.007).", "conclusion": "A three-pass LLM framework significantly enhanced PPV and reduced operational costs, maintaining detection performance, providing an effective strategy for AI-assisted radiology report quality assurance."}}
{"id": "2506.20174", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20174", "abs": "https://arxiv.org/abs/2506.20174", "authors": ["Man Duc Chuc"], "title": "Towards Scalable and Generalizable Earth Observation Data Mining via Foundation Model Composition", "comment": null, "summary": "Foundation models are rapidly transforming Earth Observation data mining by\nenabling generalizable and scalable solutions for key tasks such as scene\nclassification and semantic segmentation. While most efforts in the geospatial\ndomain have focused on developing large models trained from scratch using\nmassive Earth Observation datasets, an alternative strategy that remains\nunderexplored is the reuse and combination of existing pretrained models. In\nthis study, we investigate whether foundation models pretrained on remote\nsensing and general vision datasets can be effectively combined to improve\nperformance across a diverse set of key Earth Observation tasks. Using the\nGEO-Bench benchmark, we evaluate several prominent models, including Prithvi,\nHiera, and DOFA, on eleven datasets covering a range of spatial resolutions,\nsensor modalities, and task types. The results show that feature-level\nensembling of smaller pretrained models can match or exceed the performance of\nmuch larger models, while requiring less training time and computational\nresources. Moreover, the study highlights the potential of applying knowledge\ndistillation to transfer the strengths of ensembles into more compact models,\noffering a practical path for deploying foundation models in real-world Earth\nObservation applications.", "AI": {"tldr": "feature-level ensembling of smaller pretrained models can match or exceed the performance of much larger models, while requiring less training time and computational resources", "motivation": "investigate whether foundation models pretrained on remote sensing and general vision datasets can be effectively combined to improve performance across a diverse set of key Earth Observation tasks", "method": "evaluate several prominent models, including Prithvi, Hiera, and DOFA, on eleven datasets covering a range of spatial resolutions, sensor modalities, and task types", "result": "feature-level ensembling of smaller pretrained models can match or exceed the performance of much larger models", "conclusion": "feature-level ensembling of smaller pretrained models can match or exceed the performance of much larger models, while requiring less training time and computational resources. Moreover, the study highlights the potential of applying knowledge distillation to transfer the strengths of ensembles into more compact models, offering a practical path for deploying foundation models in real-world Earth Observation applications."}}
{"id": "2506.20274", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20274", "abs": "https://arxiv.org/abs/2506.20274", "authors": ["Liya Wang", "David Yi", "Damien Jose", "John Passarelli", "James Gao", "Jordan Leventis", "Kang Li"], "title": "Enterprise Large Language Model Evaluation Benchmark", "comment": "Submitted to MLNLP 2025 at https://csity2025.org/mlnlp/index", "summary": "Large Language Models (LLMs) ) have demonstrated promise in boosting\nproductivity across AI-powered tools, yet existing benchmarks like Massive\nMultitask Language Understanding (MMLU) inadequately assess enterprise-specific\ntask complexities. We propose a 14-task framework grounded in Bloom's Taxonomy\nto holistically evaluate LLM capabilities in enterprise contexts. To address\nchallenges of noisy data and costly annotation, we develop a scalable pipeline\ncombining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented\ngeneration (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six\nleading models shows open-source contenders like DeepSeek R1 rival proprietary\nmodels in reasoning tasks but lag in judgment-based scenarios, likely due to\noverthinking. Our benchmark reveals critical enterprise performance gaps and\noffers actionable insights for model optimization. This work provides\nenterprises a blueprint for tailored evaluations and advances practical LLM\ndeployment.", "AI": {"tldr": "This paper introduces a new benchmark for evaluating LLMs in enterprise contexts, revealing performance gaps and providing insights for model optimization.", "motivation": "Existing benchmarks like Massive Multitask Language Understanding (MMLU) inadequately assess enterprise-specific task complexities.", "method": "We propose a 14-task framework grounded in Bloom's Taxonomy to holistically evaluate LLM capabilities in enterprise contexts. To address challenges of noisy data and costly annotation, we develop a scalable pipeline combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented generation (CRAG), curating a robust 9,700-sample benchmark.", "result": "Evaluation of six leading models shows open-source contenders like DeepSeek R1 rival proprietary models in reasoning tasks but lag in judgment-based scenarios, likely due to overthinking.", "conclusion": "The benchmark reveals critical enterprise performance gaps and offers actionable insights for model optimization. This work provides enterprises a blueprint for tailored evaluations and advances practical LLM deployment."}}
{"id": "2506.19935", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.19935", "abs": "https://arxiv.org/abs/2506.19935", "authors": ["Shuchen Xue", "Tianyu Xie", "Tianyang Hu", "Zijin Feng", "Jiacheng Sun", "Kenji Kawaguchi", "Zhenguo Li", "Zhi-Ming Ma"], "title": "Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture", "comment": null, "summary": "Large language models (LLMs) predominantly use autoregressive (AR)\napproaches, but masked diffusion models (MDMs) are emerging as viable\nalternatives. A key challenge in comparing AR and MDM paradigms is their\ntypical architectural difference: AR models are often decoder-only, while MDMs\nhave largely been encoder-only. This practice of changing both the modeling\nparadigm and architecture simultaneously makes direct comparisons unfair, as\nit's hard to distinguish whether observed differences stem from the paradigm\nitself or the architectural shift. This research evaluates MDMs within a\ndecoder-only framework to: (1) equitably compare MDM (as Any-Order AR, or\nAO-AR) and standard AR paradigms. Our investigation suggests that the standard\nAO-AR objective, which averages over all token permutations, may benefit from\nrefinement, as many permutations appear less informative compared to the\nlanguage's inherent left-to-right structure. (2) Investigate architectural\ninfluences (decoder-only vs. encoder-only) within MDMs. We demonstrate that\nwhile encoder-only MDMs model a simpler conditional probability space,\ndecoder-only MDMs can achieve dramatic generation speedups ($\\sim25\\times$) and\ncomparable perplexity with temperature annealing despite modeling a vastly\nlarger space, highlighting key trade-offs. This work thus decouples core\nparadigm differences from architectural influences, offering insights for\nfuture model design. Code is available at https://github.com/scxue/AO-GPT-MDM.", "AI": {"tldr": "This paper fairly compares autoregressive (AR) and masked diffusion models (MDMs) by evaluating MDMs within a decoder-only framework, and investigates architectural influences within MDMs.", "motivation": "A key challenge in comparing AR and MDM paradigms is their typical architectural difference: AR models are often decoder-only, while MDMs have largely been encoder-only. This practice of changing both the modeling paradigm and architecture simultaneously makes direct comparisons unfair.", "method": "This research evaluates MDMs within a decoder-only framework to equitably compare MDM (as Any-Order AR, or AO-AR) and standard AR paradigms, and Investigate architectural influences (decoder-only vs. encoder-only) within MDMs.", "result": "decoder-only MDMs can achieve dramatic generation speedups and comparable perplexity with temperature annealing despite modeling a vastly larger space, highlighting key trade-offs. The standard AO-AR objective, which averages over all token permutations, may benefit from refinement, as many permutations appear less informative compared to the language's inherent left-to-right structure.", "conclusion": "This work decouples core paradigm differences from architectural influences, offering insights for future model design."}}
{"id": "2506.20119", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20119", "abs": "https://arxiv.org/abs/2506.20119", "authors": ["Masaki Uto", "Yuma Ito"], "title": "Leveraging AI Graders for Missing Score Imputation to Achieve Accurate Ability Estimation in Constructed-Response Tests", "comment": "Accepted to EvalLAC'25: 2nd Workshop on Automatic Evaluation of\n  Learning and Assessment Content, held at AIED 2025, Palermo, Italy. This is\n  the camera-ready version submitted to CEUR Workshop Proceedings", "summary": "Evaluating the abilities of learners is a fundamental objective in the field\nof education. In particular, there is an increasing need to assess higher-order\nabilities such as expressive skills and logical thinking. Constructed-response\ntests such as short-answer and essay-based questions have become widely used as\na method to meet this demand. Although these tests are effective, they require\nsubstantial manual grading, making them both labor-intensive and costly. Item\nresponse theory (IRT) provides a promising solution by enabling the estimation\nof ability from incomplete score data, where human raters grade only a subset\nof answers provided by learners across multiple test items. However, the\naccuracy of ability estimation declines as the proportion of missing scores\nincreases. Although data augmentation techniques for imputing missing scores\nhave been explored in order to address this limitation, they often struggle\nwith inaccuracy for sparse or heterogeneous data. To overcome these challenges,\nthis study proposes a novel method for imputing missing scores by leveraging\nautomated scoring technologies for accurate IRT-based ability estimation. The\nproposed method achieves high accuracy in ability estimation while markedly\nreducing manual grading workload.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u81ea\u52a8\u8bc4\u5206\u6280\u672f\u8fdb\u884cIRT\u80fd\u529b\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u4eba\u5de5\u8bc4\u5206\u5de5\u4f5c\u91cf\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u8bc4\u4f30\u5b66\u4e60\u8005\u7684\u80fd\u529b\u662f\u6559\u80b2\u9886\u57df\u7684\u4e00\u9879\u57fa\u672c\u76ee\u6807\u3002\u7279\u522b\u662f\uff0c\u8d8a\u6765\u8d8a\u9700\u8981\u8bc4\u4f30\u8bf8\u5982\u8868\u8fbe\u80fd\u529b\u548c\u903b\u8f91\u601d\u7ef4\u7b49\u9ad8\u9636\u80fd\u529b\u3002\u867d\u7136\u7b80\u7b54\u9898\u548c\u8bba\u6587\u9898\u7b49\u6784\u5efa\u53cd\u5e94\u6d4b\u8bd5\u662f\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u4f46\u5b83\u4eec\u9700\u8981\u5927\u91cf\u7684\u4eba\u5de5\u8bc4\u5206\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u65e2\u52b3\u52a8\u5bc6\u96c6\u53c8\u6210\u672c\u9ad8\u6602\u3002\u968f\u7740\u7f3a\u5931\u5206\u6570\u6bd4\u4f8b\u7684\u589e\u52a0\uff0c\u80fd\u529b\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u4f1a\u4e0b\u964d\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u81ea\u52a8\u8bc4\u5206\u6280\u672f\u6765\u586b\u8865\u7f3a\u5931\u5206\u6570\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u80fd\u529b\u4f30\u8ba1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u8bc4\u5206\u5de5\u4f5c\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u81ea\u52a8\u8bc4\u5206\u6280\u672f\u6765\u586b\u8865\u7f3a\u5931\u5206\u6570\uff0c\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u57fa\u4e8eIRT\u7684\u80fd\u529b\u4f30\u8ba1\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u8bc4\u5206\u5de5\u4f5c\u91cf\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u80fd\u529b\u4f30\u8ba1\u3002"}}
{"id": "2506.20179", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.20179", "abs": "https://arxiv.org/abs/2506.20179", "authors": ["Enzhe Zhao", "Zhichang Guo", "Yao Li", "Fanghui Song", "Boying Wu"], "title": "Progressive Alignment Degradation Learning for Pansharpening", "comment": "13 pages, 9 figures", "summary": "Deep learning-based pansharpening has been shown to effectively generate\nhigh-resolution multispectral (HRMS) images. To create supervised ground-truth\nHRMS images, synthetic data generated using the Wald protocol is commonly\nemployed. This protocol assumes that networks trained on artificial\nlow-resolution data will perform equally well on high-resolution data. However,\nwell-trained models typically exhibit a trade-off in performance between\nreduced-resolution and full-resolution datasets. In this paper, we delve into\nthe Wald protocol and find that its inaccurate approximation of real-world\ndegradation patterns limits the generalization of deep pansharpening models. To\naddress this issue, we propose the Progressive Alignment Degradation Module\n(PADM), which uses mutual iteration between two sub-networks, PAlignNet and\nPDegradeNet, to adaptively learn accurate degradation processes without relying\non predefined operators. Building on this, we introduce HFreqdiff, which embeds\nhigh-frequency details into a diffusion framework and incorporates CFB and BACM\nmodules for frequency-selective detail extraction and precise reverse process\nlearning. These innovations enable effective integration of high-resolution\npanchromatic and multispectral images, significantly enhancing spatial\nsharpness and quality. Experiments and ablation studies demonstrate the\nproposed method's superior performance compared to state-of-the-art techniques.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60 pansharpening \u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u66f4\u771f\u5b9e\u7684\u964d\u7ea7\u8fc7\u7a0b\u548c\u5d4c\u5165\u9ad8\u9891\u7ec6\u8282\u6765\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684 pansharpening \u901a\u5e38\u4f7f\u7528 Wald \u534f\u8bae\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46 Wald \u534f\u8bae\u5bf9\u771f\u5b9e\u4e16\u754c\u9000\u5316\u6a21\u5f0f\u7684\u8fd1\u4f3c\u4e0d\u51c6\u786e\uff0c\u9650\u5236\u4e86\u6df1\u5ea6 pansharpening \u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u6e10\u8fdb\u5bf9\u9f50\u9000\u5316\u6a21\u5757 (PADM) \u548c HFreqdiff \u6846\u67b6\uff0cPADM \u4f7f\u7528\u4e24\u4e2a\u5b50\u7f51\u7edc\u76f8\u4e92\u8fed\u4ee3\u5b66\u4e60\u7cbe\u786e\u7684\u9000\u5316\u8fc7\u7a0b\uff0cHFreqdiff \u5c06\u9ad8\u9891\u7ec6\u8282\u5d4c\u5165\u5230\u6269\u6563\u6846\u67b6\u4e2d\uff0c\u5e76\u7ed3\u5408 CFB \u548c BACM \u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0e\u6700\u5148\u8fdb\u7684\u6280\u672f\u76f8\u6bd4\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60 pansharpening \u65b9\u6cd5\uff0c\u901a\u8fc7 PADM \u5b66\u4e60\u7cbe\u786e\u7684\u964d\u7ea7\u8fc7\u7a0b\uff0c\u5e76\u5f15\u5165 HFreqdiff \u5d4c\u5165\u9ad8\u9891\u7ec6\u8282\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u7a7a\u95f4\u6e05\u6670\u5ea6\u548c\u8d28\u91cf\u3002"}}
{"id": "2506.20332", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20332", "abs": "https://arxiv.org/abs/2506.20332", "authors": ["Jihao Gu", "Qihang Ai", "Yingyao Wang", "Pi Bu", "Jingxuan Xing", "Zekun Zhu", "Wei Jiang", "Ziming Wang", "Yingxiu Zhao", "Ming-Liang Zhang", "Jun Song", "Yuning Jiang", "Bo Zheng"], "title": "Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards", "comment": "14 pages, 12 figures", "summary": "Vision-language model-based mobile agents have gained the ability to not only\nunderstand complex instructions and mobile screenshots, but also optimize their\naction outputs via thinking and reasoning, benefiting from reinforcement\nlearning, such as Group Relative Policy Optimization (GRPO). However, existing\nresearch centers on offline reinforcement learning training or online\noptimization using action-level rewards, which limits the agent's dynamic\ninteraction with the environment. This often results in agents settling into\nlocal optima, thereby weakening their ability for exploration and error action\ncorrection. To address these challenges, we introduce an approach called\nMobile-R1, which employs interactive multi-turn reinforcement learning with\ntask-level rewards for mobile agents. Our training framework consists of three\nstages: initial format finetuning, single-step online training via action-level\nreward, followed by online training via task-level reward based on multi-turn\ntrajectories. This strategy is designed to enhance the exploration and error\ncorrection capabilities of Mobile-R1, leading to significant performance\nimprovements. Moreover, we have collected a dataset covering 28 Chinese\napplications with 24,521 high-quality manual annotations and established a new\nbenchmark with 500 trajectories. We will open source all resources, including\nthe dataset, benchmark, model weight, and codes:\nhttps://mobile-r1.github.io/Mobile-R1/.", "AI": {"tldr": "Mobile-R1 \u4f7f\u7528\u4ea4\u4e92\u5f0f\u7684\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u548c\u4efb\u52a1\u7ea7\u522b\u7684\u5956\u52b1\u6765\u6539\u8fdb\u79fb\u52a8\u4ee3\u7406\uff0c\u5e76\u901a\u8fc7\u65b0\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u96c6\u4e2d\u4e8e\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6216\u4f7f\u7528\u52a8\u4f5c\u7ea7\u522b\u5956\u52b1\u7684\u5728\u7ebf\u4f18\u5316\uff0c\u8fd9\u9650\u5236\u4e86\u667a\u80fd\u4f53\u4e0e\u73af\u5883\u7684\u52a8\u6001\u4ea4\u4e92\u3002\u8fd9\u901a\u5e38\u5bfc\u81f4\u667a\u80fd\u4f53\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff0c\u4ece\u800c\u524a\u5f31\u4e86\u5b83\u4eec\u7684\u63a2\u7d22\u548c\u9519\u8bef\u884c\u52a8\u7ea0\u6b63\u80fd\u529b\u3002", "method": "Mobile-R1\uff0c\u5b83\u91c7\u7528\u4ea4\u4e92\u5f0f\u7684\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u4e0e\u4efb\u52a1\u7ea7\u522b\u7684\u5956\u52b1\u3002", "result": "\u6536\u96c6\u4e86\u4e00\u4e2a\u5305\u542b28\u4e2a\u4e2d\u56fd\u5e94\u7528\u7a0b\u5e8f\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b24,521\u4e2a\u9ad8\u8d28\u91cf\u7684\u624b\u52a8\u6ce8\u91ca\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u5305\u542b500\u4e2a\u8f68\u8ff9\u7684\u65b0\u57fa\u51c6\u3002", "conclusion": "Mobile-R1\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u7684\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u548c\u4efb\u52a1\u7ea7\u522b\u7684\u5956\u52b1\uff0c\u589e\u5f3a\u4e86\u63a2\u7d22\u548c\u7ea0\u9519\u80fd\u529b\u3002"}}
{"id": "2506.19937", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19937", "abs": "https://arxiv.org/abs/2506.19937", "authors": ["Tomas M. Bosschieter", "Luis Franca", "Jessica Wolk", "Yiyuan Wu", "Bella Mehta", "Joseph Dehoney", "Orsolya Kiss", "Fiona C. Baker", "Qingyu Zhao", "Rich Caruana", "Kilian M. Pohl"], "title": "The Most Important Features in Generalized Additive Models Might Be Groups of Features", "comment": null, "summary": "While analyzing the importance of features has become ubiquitous in\ninterpretable machine learning, the joint signal from a group of related\nfeatures is sometimes overlooked or inadvertently excluded. Neglecting the\njoint signal could bypass a critical insight: in many instances, the most\nsignificant predictors are not isolated features, but rather the combined\neffect of groups of features. This can be especially problematic for datasets\nthat contain natural groupings of features, including multimodal datasets. This\npaper introduces a novel approach to determine the importance of a group of\nfeatures for Generalized Additive Models (GAMs) that is efficient, requires no\nmodel retraining, allows defining groups posthoc, permits overlapping groups,\nand remains meaningful in high-dimensional settings. Moreover, this definition\noffers a parallel with explained variation in statistics. We showcase\nproperties of our method on three synthetic experiments that illustrate the\nbehavior of group importance across various data regimes. We then demonstrate\nthe importance of groups of features in identifying depressive symptoms from a\nmultimodal neuroscience dataset, and study the importance of social\ndeterminants of health after total hip arthroplasty. These two case studies\nreveal that analyzing group importance offers a more accurate, holistic view of\nthe medical issues compared to a single-feature analysis.", "AI": {"tldr": "This paper introduces a novel approach to determine the importance of a group of features for Generalized Additive Models (GAMs) that is efficient and remains meaningful in high-dimensional settings. Analyzing group importance offers a more accurate, holistic view compared to a single-feature analysis.", "motivation": "The joint signal from a group of related features is sometimes overlooked or inadvertently excluded, which could bypass a critical insight that the most significant predictors are not isolated features, but rather the combined effect of groups of features. This can be especially problematic for datasets that contain natural groupings of features, including multimodal datasets.", "method": "This paper introduces a novel approach to determine the importance of a group of features for Generalized Additive Models (GAMs) that is efficient, requires no model retraining, allows defining groups posthoc, permits overlapping groups, and remains meaningful in high-dimensional settings.", "result": "Showcase properties of our method on three synthetic experiments that illustrate the behavior of group importance across various data regimes. Demonstrate the importance of groups of features in identifying depressive symptoms from a multimodal neuroscience dataset, and study the importance of social determinants of health after total hip arthroplasty.", "conclusion": "Analyzing group importance offers a more accurate, holistic view of the medical issues compared to a single-feature analysis."}}
{"id": "2506.20128", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20128", "abs": "https://arxiv.org/abs/2506.20128", "authors": ["Aashiq Muhamed"], "title": "CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG Evaluation", "comment": "Accepted at LLM4Eval @ SIGIR 2025", "summary": "RAG systems enhance LLMs by incorporating external knowledge, which is\ncrucial for domains that demand factual accuracy and up-to-date information.\nHowever, evaluating the multifaceted quality of RAG outputs, spanning aspects\nsuch as contextual coherence, query relevance, factual correctness, and\ninformational completeness, poses significant challenges. Existing evaluation\nmethods often rely on simple lexical overlap metrics, which are inadequate for\ncapturing these nuances, or involve complex multi-stage pipelines with\nintermediate steps like claim extraction or require finetuning specialized\njudge models, hindering practical efficiency. To address these limitations, we\npropose CCRS (Contextual Coherence and Relevance Score), a novel suite of five\nmetrics that utilizes a single, powerful, pretrained LLM as a zero-shot,\nend-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance\n(QR), Information Density (ID), Answer Correctness (AC), and Information Recall\n(IR). We apply CCRS to evaluate six diverse RAG system configurations on the\nchallenging BioASQ dataset. Our analysis demonstrates that CCRS effectively\ndiscriminates between system performances, confirming, for instance, that the\nMistral-7B reader outperforms Llama variants. We provide a detailed analysis of\nCCRS metric properties, including score distributions, convergent/discriminant\nvalidity, tie rates, population statistics, and discriminative power. Compared\nto the complex RAGChecker framework, CCRS offers comparable or superior\ndiscriminative power for key aspects like recall and faithfulness, while being\nsignificantly more computationally efficient. CCRS thus provides a practical,\ncomprehensive, and efficient framework for evaluating and iteratively improving\nRAG systems.", "AI": {"tldr": "CCRS: A novel suite of five metrics that utilizes a single, powerful, pretrained LLM as a zero-shot, end-to-end judge, offering a practical, comprehensive, and efficient framework for evaluating and iteratively improving RAG systems.", "motivation": "Evaluating the multifaceted quality of RAG outputs poses significant challenges. Existing evaluation methods often rely on simple lexical overlap metrics, which are inadequate for capturing these nuances, or involve complex multi-stage pipelines, hindering practical efficiency.", "method": "We propose CCRS (Contextual Coherence and Relevance Score), a novel suite of five metrics that utilizes a single, powerful, pretrained LLM as a zero-shot, end-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance (QR), Information Density (ID), Answer Correctness (AC), and Information Recall (IR).", "result": "CCRS effectively discriminates between system performances, confirming, for instance, that the Mistral-7B reader outperforms Llama variants. Compared to the complex RAGChecker framework, CCRS offers comparable or superior discriminative power for key aspects like recall and faithfulness, while being significantly more computationally efficient.", "conclusion": "CCRS offers a practical, comprehensive, and efficient framework for evaluating and iteratively improving RAG systems."}}
{"id": "2506.20214", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.20214", "abs": "https://arxiv.org/abs/2506.20214", "authors": ["Yanzhe Chen", "Huasong Zhong", "Yan Li", "Zhenheng Yang"], "title": "UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation", "comment": "19 pages, 5 figures", "summary": "Unified multimodal large language models (MLLMs) have shown promise in\njointly advancing multimodal understanding and generation, with visual\ncodebooks discretizing images into tokens for autoregressive modeling. Existing\ncodebook-based methods either rely on small vocabularies (~16K entries) that\nlack fine-grained semantics or naively scale up, resulting in low token\nutilization and unstable training. We propose UniCode$^2$, a cascaded codebook\nframework enabling large-scale, semantically aligned, and stable visual\ntokenization. By clustering millions of SigLIP sequence embeddings, we build a\n500K-entry codebook that preserves vision-language alignment while expanding\ncapacity. Stability is ensured via a cascaded design: a frozen codebook anchors\nthe embedding space, and a trainable codebook refines task-specific semantics.\nThis decoupling promotes high utilization and robust learning. Moreover, the\nalignment of our visual tokens with textual semantics enables seamless\nintegration with pretrained diffusion decoders, supporting high-quality visual\nsynthesis with minimal adaptation. UniCode^2 delivers strong performance across\ndiverse benchmarks, demonstrating the viability of scaling visual token spaces\nwithout sacrificing stability, semantics, or modularity.", "AI": {"tldr": "UniCode^2 \u662f\u4e00\u79cd\u7ea7\u8054\u4ee3\u7801\u672c\u6846\u67b6\uff0c\u652f\u6301\u5927\u89c4\u6a21\u3001\u8bed\u4e49\u5bf9\u9f50\u548c\u7a33\u5b9a\u7684\u89c6\u89c9\u6807\u8bb0\u5316\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u4ee3\u7801\u672c\u7684\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u4e8e\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7684\u5c0f\u8bcd\u6c47\u8868\uff08\u7ea6 16K \u6761\u76ee\uff09\uff0c\u8981\u4e48\u5929\u771f\u5730\u6269\u5927\u89c4\u6a21\uff0c\u5bfc\u81f4token\u5229\u7528\u7387\u4f4e\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "method": "\u901a\u8fc7\u5bf9\u6570\u767e\u4e07 SigLIP \u5e8f\u5217\u5d4c\u5165\u8fdb\u884c\u805a\u7c7b\uff0c\u6784\u5efa\u4e00\u4e2a 50 \u4e07\u6761\u76ee\u7684\u4ee3\u7801\u672c\uff0c\u8be5\u4ee3\u7801\u672c\u4fdd\u7559\u4e86\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\uff0c\u540c\u65f6\u6269\u5c55\u4e86\u5bb9\u91cf\u3002\u901a\u8fc7\u7ea7\u8054\u8bbe\u8ba1\u786e\u4fdd\u7a33\u5b9a\u6027\uff1a\u51bb\u7ed3\u7684\u4ee3\u7801\u672c\u951a\u5b9a\u5d4c\u5165\u7a7a\u95f4\uff0c\u53ef\u8bad\u7ec3\u7684\u4ee3\u7801\u672c\u7ec6\u5316\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u8bed\u4e49\u3002\u8fd9\u79cd\u89e3\u8026\u63d0\u9ad8\u4e86\u5229\u7528\u7387\u548c\u7a33\u5065\u7684\u5b66\u4e60\u80fd\u529b\u3002", "result": "UniCode^2 \u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u652f\u6301\u9ad8\u8d28\u91cf\u7684\u89c6\u89c9\u5408\u6210\uff0c\u5e76\u4e14\u4e0e\u9884\u8bad\u7ec3\u7684\u6269\u6563\u89e3\u7801\u5668\u65e0\u7f1d\u96c6\u6210\u3002", "conclusion": "UniCode^2\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5728\u4e0d\u727a\u7272\u7a33\u5b9a\u6027\u3001\u8bed\u4e49\u6216\u6a21\u5757\u5316\u7684\u60c5\u51b5\u4e0b\u6269\u5c55\u89c6\u89c9\u6807\u8bb0\u7a7a\u95f4\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2506.20357", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20357", "abs": "https://arxiv.org/abs/2506.20357", "authors": ["Sungwon Han", "Sungkyu Park", "Seungeon Lee"], "title": "Tabular Feature Discovery With Reasoning Type Exploration", "comment": null, "summary": "Feature engineering for tabular data remains a critical yet challenging step\nin machine learning. Recently, large language models (LLMs) have been used to\nautomatically generate new features by leveraging their vast knowledge.\nHowever, existing LLM-based approaches often produce overly simple or\nrepetitive features, partly due to inherent biases in the transformations the\nLLM chooses and the lack of structured reasoning guidance during generation. In\nthis paper, we propose a novel method REFeat, which guides an LLM to discover\ndiverse and informative features by leveraging multiple types of reasoning to\nsteer the feature generation process. Experiments on 59 benchmark datasets\ndemonstrate that our approach not only achieves higher predictive accuracy on\naverage, but also discovers more diverse and meaningful features. These results\nhighlight the promise of incorporating rich reasoning paradigms and adaptive\nstrategy selection into LLM-driven feature discovery for tabular data.", "AI": {"tldr": "This paper introduces REFeat, a new method that guides large language models (LLMs) to discover diverse and informative features for tabular data by leveraging multiple types of reasoning. It achieves higher predictive accuracy and discovers more meaningful features on 59 benchmark datasets.", "motivation": "Feature engineering for tabular data remains a critical yet challenging step in machine learning. Existing LLM-based approaches often produce overly simple or repetitive features, partly due to inherent biases in the transformations the LLM chooses and the lack of structured reasoning guidance during generation.", "method": "The paper proposes a novel method REFeat, which guides an LLM to discover diverse and informative features by leveraging multiple types of reasoning to steer the feature generation process.", "result": "Experiments on 59 benchmark datasets demonstrate that the approach not only achieves higher predictive accuracy on average, but also discovers more diverse and meaningful features.", "conclusion": "This paper demonstrates the promise of incorporating rich reasoning paradigms and adaptive strategy selection into LLM-driven feature discovery for tabular data."}}
{"id": "2506.19992", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19992", "abs": "https://arxiv.org/abs/2506.19992", "authors": ["Gabor Petnehazi", "Bernadett Aradi"], "title": "HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization", "comment": null, "summary": "The explosive growth of complex datasets across various modalities\nnecessitates advanced analytical tools that not only group data effectively but\nalso provide human-understandable insights into the discovered structures. We\nintroduce HERCULES (Hierarchical Embedding-based Recursive Clustering Using\nLLMs for Efficient Summarization), a novel algorithm and Python package\ndesigned for hierarchical k-means clustering of diverse data types, including\ntext, images, and numeric data (processed one modality per run). HERCULES\nconstructs a cluster hierarchy by recursively applying k-means clustering,\nstarting from individual data points at level 0. A key innovation is its deep\nintegration of Large Language Models (LLMs) to generate semantically rich\ntitles and descriptions for clusters at each level of the hierarchy,\nsignificantly enhancing interpretability. The algorithm supports two main\nrepresentation modes: `direct' mode, which clusters based on original data\nembeddings or scaled numeric features, and `description' mode, which clusters\nbased on embeddings derived from LLM-generated summaries. Users can provide a\n`topic\\_seed' to guide LLM-generated summaries towards specific themes. An\ninteractive visualization tool facilitates thorough analysis and understanding\nof the clustering results. We demonstrate HERCULES's capabilities and discuss\nits potential for extracting meaningful, hierarchical knowledge from complex\ndatasets.", "AI": {"tldr": "HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization, a novel algorithm and Python package designed for hierarchical k-means clustering of diverse data types", "motivation": "The explosive growth of complex datasets across various modalities necessitates advanced analytical tools that not only group data effectively but also provide human-understandable insights into the discovered structures.", "method": "a novel algorithm and Python package designed for hierarchical k-means clustering of diverse data types, including text, images, and numeric data (processed one modality per run).", "result": "constructs a cluster hierarchy by recursively applying k-means clustering, starting from individual data points at level 0. A key innovation is its deep integration of Large Language Models (LLMs) to generate semantically rich titles and descriptions for clusters at each level of the hierarchy, significantly enhancing interpretability.", "conclusion": "demonstrate HERCULES's capabilities and discuss its potential for extracting meaningful, hierarchical knowledge from complex datasets."}}
{"id": "2506.20160", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20160", "abs": "https://arxiv.org/abs/2506.20160", "authors": ["Ruosen Li", "Ziming Luo", "Quan Zhang", "Ruochen Li", "Ben Zhou", "Ali Payani", "Xinya Du"], "title": "AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control", "comment": null, "summary": "Large reasoning models (LRMs) achieve impressive reasoning capabilities by\ngenerating lengthy chain-of-thoughts, but this \"overthinking\" incurs high\nlatency and cost without commensurate accuracy gains. In this work, we\nintroduce AALC, a lightweight, accuracy-aware length reward integrated into\nreinforcement learning that dynamically balances correctness and brevity during\ntraining. By incorporating validation accuracy into the reward and employing a\nsmooth, dynamically scheduled length penalty, AALC delays length penalty until\ntarget performance is met. Through extensive experiments across standard and\nout-of-distribution math benchmarks, we show that our approach reduces response\nlength by over 50% while maintaining or even improving the original accuracy.\nFurthermore, qualitative analysis reveals that our method curbs redundant\nreasoning patterns such as excessive subgoal setting and verification, leading\nto structurally refined outputs rather than naive truncation. We also identify\nthat efficiency gains are accompanied by reduced interpretability: models\ntrained with AALC omit some narrative framing and explanatory context. These\nfindings highlight the potential of reward-based strategies to guide LRMs\ntoward more efficient, generalizable reasoning paths.", "AI": {"tldr": "AALC \u51cf\u5c11\u4e86 LRM \u7684\u54cd\u5e94\u957f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u4f46\u964d\u4f4e\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b (LRM) \u901a\u8fc7\u751f\u6210\u5197\u957f\u7684\u601d\u7ef4\u94fe\u6765\u5b9e\u73b0\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u662f\u8fd9\u79cd\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u4f1a\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u548c\u9ad8\u6210\u672c\uff0c\u800c\u6ca1\u6709\u83b7\u5f97\u76f8\u5e94\u7684\u51c6\u786e\u6027\u3002", "method": "\u5f15\u5165 AALC\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u3001\u7cbe\u5ea6\u611f\u77e5\u7684\u957f\u5ea6\u5956\u52b1\uff0c\u5b83\u88ab\u96c6\u6210\u5230\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u53ef\u4ee5\u5728\u8bad\u7ec3\u671f\u95f4\u52a8\u6001\u5730\u5e73\u8861\u6b63\u786e\u6027\u548c\u7b80\u6d01\u6027\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u5c06\u54cd\u5e94\u957f\u5ea6\u51cf\u5c11 50% \u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u9ad8\u539f\u59cb\u7cbe\u5ea6\u3002\u6b64\u5916\uff0c\u5b9a\u6027\u5206\u6790\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u6291\u5236\u5197\u4f59\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u4f8b\u5982\u8fc7\u5ea6\u7684\u5b50\u76ee\u6807\u8bbe\u7f6e\u548c\u9a8c\u8bc1\uff0c\u4ece\u800c\u4ea7\u751f\u7ed3\u6784\u4e0a\u66f4\u7cbe\u7ec6\u7684\u8f93\u51fa\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u7684\u622a\u65ad\u3002", "conclusion": "reward-based strategies \u53ef\u4ee5\u5f15\u5bfc LRMs \u627e\u5230\u66f4\u9ad8\u6548\u3001\u66f4\u901a\u7528\u7684\u63a8\u7406\u8def\u5f84\u3002"}}
{"id": "2506.20222", "categories": ["cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.20222", "abs": "https://arxiv.org/abs/2506.20222", "authors": ["Pujing Yang", "Guangyi Zhang", "Yunlong Cai", "Lei Yu", "Guanding Yu"], "title": "Dynamic Bandwidth Allocation for Hybrid Event-RGB Transmission", "comment": null, "summary": "Event cameras asynchronously capture pixel-level intensity changes with\nextremely low latency. They are increasingly used in conjunction with RGB\ncameras for a wide range of vision-related applications. However, a major\nchallenge in these hybrid systems lies in the transmission of the large volume\nof triggered events and RGB images. To address this, we propose a transmission\nscheme that retains efficient reconstruction performance of both sources while\naccomplishing real-time deblurring in parallel. Conventional RGB cameras and\nevent cameras typically capture the same scene in different ways, often\nresulting in significant redundant information across their outputs. To address\nthis, we develop a joint event and image (E-I) transmission framework to\neliminate redundancy and thereby optimize channel bandwidth utilization. Our\napproach employs Bayesian modeling and the information bottleneck method to\ndisentangle the shared and domain-specific information within the E-I inputs.\nThis disentangled information bottleneck framework ensures both the compactness\nand informativeness of extracted shared and domain-specific information.\nMoreover, it adaptively allocates transmission bandwidth based on scene\ndynamics, i.e., more symbols are allocated to events for dynamic details or to\nimages for static information. Simulation results demonstrate that the proposed\nscheme not only achieves superior reconstruction quality compared to\nconventional systems but also delivers enhanced deblurring performance.", "AI": {"tldr": "A joint event and image transmission framework is proposed to eliminate redundancy and optimize channel bandwidth utilization. It adaptively allocates transmission bandwidth based on scene dynamics and achieves superior reconstruction quality and enhanced deblurring performance.", "motivation": "A major challenge in hybrid systems with event cameras and RGB cameras lies in the transmission of the large volume of triggered events and RGB images. Conventional RGB cameras and event cameras typically capture the same scene in different ways, often resulting in significant redundant information across their outputs.", "method": "A joint event and image (E-I) transmission framework is developed to eliminate redundancy and thereby optimize channel bandwidth utilization. Bayesian modeling and the information bottleneck method are employed to disentangle the shared and domain-specific information within the E-I inputs. This disentangled information bottleneck framework ensures both the compactness and informativeness of extracted shared and domain-specific information. It adaptively allocates transmission bandwidth based on scene dynamics.", "result": "The proposed scheme achieves superior reconstruction quality compared to conventional systems and delivers enhanced deblurring performance.", "conclusion": "The proposed scheme achieves superior reconstruction quality compared to conventional systems and delivers enhanced deblurring performance."}}
{"id": "2506.20384", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20384", "abs": "https://arxiv.org/abs/2506.20384", "authors": ["Dror Ivry", "Oran Nahum"], "title": "Paladin-mini: A Compact and Efficient Grounding Model Excelling in Real-World Scenarios", "comment": "6 pages, 2 figures", "summary": "This paper introduces two significant contributions to address the issue of\ngrounding claims in a given context. Grounding means that given a context\n(document) and a claim, there's at least one supportive evidence for the claim\nin the document. We will introduce Paladin-mini, a compact (3.8B parameters)\nopen-source classifier model (used for labeling data as grounded or ungrounded)\nengineered for robust performance in real-world scenarios, and the\ngrounding-benchmark, a new evaluation dataset designed to assess performance on\ncritical reasoning tasks. We'll also demonstrate the results of Paladin-mini\nwith benchmarks against the current State-of-the-art and share clear and\nreproducible results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5c0f\u578b\u7684\u5f00\u6e90\u6a21\u578bPaladin-mini\u548c\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u6570\u636e\u96c6grounding-benchmark\uff0c\u7528\u4e8e\u89e3\u51b3\u548c\u8bc4\u4f30\u58f0\u660e\u7684grounding\u95ee\u9898\u3002", "motivation": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e24\u4e2a\u91cd\u8981\u7684\u8d21\u732e\uff0c\u4ee5\u89e3\u51b3\u5728\u7ed9\u5b9a\u4e0a\u4e0b\u6587\u4e2dgrounding\u58f0\u660e\u7684\u95ee\u9898\u3002Grounding\u610f\u5473\u7740\u7ed9\u5b9a\u4e00\u4e2a\u4e0a\u4e0b\u6587\uff08\u6587\u6863\uff09\u548c\u4e00\u4e2a\u58f0\u660e\uff0c\u6587\u6863\u4e2d\u81f3\u5c11\u6709\u4e00\u4e2a\u652f\u6301\u8be5\u58f0\u660e\u7684\u8bc1\u636e\u3002", "method": "\u4ecb\u7ecd\u4e86Paladin-mini\uff0c\u4e00\u4e2a\u7d27\u51d1\u7684\uff083.8B\u53c2\u6570\uff09\u5f00\u6e90\u5206\u7c7b\u5668\u6a21\u578b\uff08\u7528\u4e8e\u5c06\u6570\u636e\u6807\u8bb0\u4e3a\u5df2grounding\u6216\u672agrounding\uff09\u3002", "result": "Paladin-mini\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0cgrounding-benchmark\u662f\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u65e8\u5728\u8bc4\u4f30\u5173\u952e\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "\u5c55\u793a\u4e86Paladin-mini\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u6280\u672f\u7684\u5bf9\u6bd4\u7ed3\u679c\uff0c\u5e76\u5206\u4eab\u4e86\u6e05\u6670\u4e14\u53ef\u590d\u73b0\u7684\u7ed3\u679c\u3002"}}
{"id": "2506.19997", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19997", "abs": "https://arxiv.org/abs/2506.19997", "authors": ["Geonwoo Cho", "Jaegyun Im", "Jihwan Lee", "Hojun Yi", "Sejin Kim", "Sundong Kim"], "title": "TRACED: Transition-aware Regret Approximation with Co-learnability for Environment Design", "comment": null, "summary": "Generalizing deep reinforcement learning agents to unseen environments\nremains a significant challenge. One promising solution is Unsupervised\nEnvironment Design (UED), a co-evolutionary framework in which a teacher\nadaptively generates tasks with high learning potential, while a student learns\na robust policy from this evolving curriculum. Existing UED methods typically\nmeasure learning potential via regret, the gap between optimal and current\nperformance, approximated solely by value-function loss. Building on these\napproaches, we introduce the transition prediction error as an additional term\nin our regret approximation. To capture how training on one task affects\nperformance on others, we further propose a lightweight metric called\nco-learnability. By combining these two measures, we present Transition-aware\nRegret Approximation with Co-learnability for Environment Design (TRACED).\nEmpirical evaluations show that TRACED yields curricula that improve zero-shot\ngeneralization across multiple benchmarks while requiring up to 2x fewer\nenvironment interactions than strong baselines. Ablation studies confirm that\nthe transition prediction error drives rapid complexity ramp-up and that\nco-learnability delivers additional gains when paired with the transition\nprediction error. These results demonstrate how refined regret approximation\nand explicit modeling of task relationships can be leveraged for\nsample-efficient curriculum design in UED.", "AI": {"tldr": "TRACED\u662f\u4e00\u79cd\u65b0\u7684UED\u65b9\u6cd5\uff0c\u5b83\u4f7f\u7528\u8f6c\u79fb\u9884\u6d4b\u8bef\u5dee\u548c\u5171\u540c\u5b66\u4e60\u6027\u6765\u6539\u8fdb\u8bfe\u7a0b\u8bbe\u8ba1\uff0c\u4ece\u800c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u6cdb\u5316\u5230\u672a\u89c1\u73af\u5883\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7684UED\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7regret\u6765\u8861\u91cf\u5b66\u4e60\u6f5c\u529b\uff0c\u800cregret\u4ec5\u901a\u8fc7\u4ef7\u503c\u51fd\u6570\u635f\u5931\u6765\u8fd1\u4f3c\u3002", "method": "TRACED\uff1a\u7ed3\u5408\u4e86\u8f6c\u79fb\u9884\u6d4b\u8bef\u5dee\u548c\u5171\u540c\u5b66\u4e60\u6027\u5ea6\u91cf\u7684\u9057\u61be\u8fd1\u4f3c\u65b9\u6cd5\u3002", "result": "TRACED\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u6bd4\u5f3a\u5927\u7684\u57fa\u7ebf\u65b9\u6cd5\u51cf\u5c11\u4e86\u9ad8\u8fbe2\u500d\u7684\u73af\u5883\u4ea4\u4e92\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\uff0c\u8f6c\u79fb\u9884\u6d4b\u8bef\u5dee\u9a71\u52a8\u4e86\u5feb\u901f\u7684\u590d\u6742\u6027\u63d0\u5347\uff0c\u5e76\u4e14\u5f53\u4e0e\u8f6c\u79fb\u9884\u6d4b\u8bef\u5dee\u914d\u5bf9\u65f6\uff0c\u5171\u540c\u5b66\u4e60\u6027\u63d0\u4f9b\u4e86\u989d\u5916\u7684\u6536\u76ca\u3002", "conclusion": "TRACED\u901a\u8fc7\u6539\u8fdb\u7684\u9057\u61be\u8fd1\u4f3c\u548c\u663e\u5f0f\u5efa\u6a21\u4efb\u52a1\u5173\u7cfb\uff0c\u5b9e\u73b0\u4e86UED\u4e2d\u6837\u672c\u9ad8\u6548\u7684\u8bfe\u7a0b\u8bbe\u8ba1\u3002"}}
{"id": "2506.20167", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20167", "abs": "https://arxiv.org/abs/2506.20167", "authors": ["Fengze Li", "Yue Wang", "Yangle Liu", "Ming Huang", "Dou Hong", "Jieming Ma"], "title": "SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction with LLMs", "comment": null, "summary": "Multivariate time series forecasting requires models to simultaneously\ncapture variable-wise structural dependencies and generalize across diverse\ntasks. While structural encoders are effective in modeling feature\ninteractions, they lack the capacity to support semantic-level reasoning or\ntask adaptation. Conversely, large language models (LLMs) possess strong\ngeneralization capabilities but remain incompatible with raw time series\ninputs. This gap limits the development of unified, transferable prediction\nsystems. Therefore, we introduce SEED, a structural encoder for\nembedding-driven decoding, which integrates four stages: a token-aware encoder\nfor patch extraction, a projection module that aligns patches with language\nmodel embeddings, a semantic reprogramming mechanism that maps patches to\ntask-aware prototypes, and a frozen language model for prediction. This modular\narchitecture decouples representation learning from inference, enabling\nefficient alignment between numerical patterns and semantic reasoning.\nEmpirical results demonstrate that the proposed method achieves consistent\nimprovements over strong baselines, and comparative studies on various datasets\nconfirm SEED's role in addressing the structural-semantic modeling gap.", "AI": {"tldr": "SEED: a structural encoder for embedding-driven decoding, which integrates four stages: a token-aware encoder for patch extraction, a projection module that aligns patches with language model embeddings, a semantic reprogramming mechanism that maps patches to task-aware prototypes, and a frozen language model for prediction.", "motivation": "Multivariate time series forecasting requires models to simultaneously capture variable-wise structural dependencies and generalize across diverse tasks. Structural encoders lack the capacity to support semantic-level reasoning or task adaptation, while large language models (LLMs) remain incompatible with raw time series inputs.", "method": "a structural encoder for embedding-driven decoding, which integrates four stages: a token-aware encoder for patch extraction, a projection module that aligns patches with language model embeddings, a semantic reprogramming mechanism that maps patches to task-aware prototypes, and a frozen language model for prediction", "result": "achieves consistent improvements over strong baselines", "conclusion": "The proposed SEED method achieves consistent improvements over strong baselines and addresses the structural-semantic modeling gap."}}
{"id": "2506.20254", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20254", "abs": "https://arxiv.org/abs/2506.20254", "authors": ["Kun Yuan", "Tingxuan Chen", "Shi Li", "Joel L. Lavanchy", "Christian Heiliger", "Ege \u00d6zsoy", "Yiming Huang", "Long Bai", "Nassir Navab", "Vinkle Srivastav", "Hongliang Ren", "Nicolas Padoy"], "title": "Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and Task-graph Guided Refinement", "comment": "Accepted by MICCAI 2025", "summary": "The complexity and diversity of surgical workflows, driven by heterogeneous\noperating room settings, institutional protocols, and anatomical variability,\npresent a significant challenge in developing generalizable models for\ncross-institutional and cross-procedural surgical understanding. While recent\nsurgical foundation models pretrained on large-scale vision-language data offer\npromising transferability, their zero-shot performance remains constrained by\ndomain shifts, limiting their utility in unseen surgical environments. To\naddress this, we introduce Surgical Phase Anywhere (SPA), a lightweight\nframework for versatile surgical workflow understanding that adapts foundation\nmodels to institutional settings with minimal annotation. SPA leverages\nfew-shot spatial adaptation to align multi-modal embeddings with\ninstitution-specific surgical scenes and phases. It also ensures temporal\nconsistency through diffusion modeling, which encodes task-graph priors derived\nfrom institutional procedure protocols. Finally, SPA employs dynamic test-time\nadaptation, exploiting the mutual agreement between multi-modal phase\nprediction streams to adapt the model to a given test video in a\nself-supervised manner, enhancing the reliability under test-time distribution\nshifts. SPA is a lightweight adaptation framework, allowing hospitals to\nrapidly customize phase recognition models by defining phases in natural\nlanguage text, annotating a few images with the phase labels, and providing a\ntask graph defining phase transitions. The experimental results show that the\nSPA framework achieves state-of-the-art performance in few-shot surgical phase\nrecognition across multiple institutions and procedures, even outperforming\nfull-shot models with 32-shot labeled data. Code is available at\nhttps://github.com/CAMMA-public/SPA", "AI": {"tldr": "SPA\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u7528\u624b\u672f\u5de5\u4f5c\u6d41\u7a0b\u7406\u89e3\uff0c\u901a\u8fc7\u6700\u5c11\u7684\u6ce8\u91ca\u5c06\u57fa\u7840\u6a21\u578b\u8c03\u6574\u5230\u673a\u6784\u8bbe\u7f6e\u3002", "motivation": "\u5728\u5f00\u53d1\u7528\u4e8e\u8de8\u673a\u6784\u548c\u8de8\u7a0b\u5e8f\u624b\u672f\u7406\u89e3\u7684\u901a\u7528\u6a21\u578b\u65f6\uff0c\u624b\u672f\u5de5\u4f5c\u6d41\u7a0b\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u63d0\u51fa\u4e86\u5de8\u5927\u7684\u6311\u6218\u3002\u6700\u8fd1\u5728\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u7684\u624b\u672f\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u4f46\u5b83\u4eec\u7684\u96f6\u6837\u672c\u6027\u80fd\u4ecd\u7136\u53d7\u5230\u57df\u8f6c\u79fb\u7684\u9650\u5236\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u770b\u4e0d\u89c1\u7684\u624b\u672f\u73af\u5883\u4e2d\u7684\u6548\u7528\u3002", "method": "SPA\u5229\u7528\u5c11\u6837\u672c\u7a7a\u95f4\u81ea\u9002\u5e94\u6765\u5bf9\u9f50\u591a\u6a21\u6001\u5d4c\u5165\u4e0e\u7279\u5b9a\u673a\u6784\u7684\u624b\u672f\u573a\u666f\u548c\u9636\u6bb5\u3002\u5b83\u8fd8\u901a\u8fc7\u6269\u6563\u5efa\u6a21\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u6269\u6563\u5efa\u6a21\u7f16\u7801\u4e86\u6765\u81ea\u673a\u6784\u7a0b\u5e8f\u534f\u8bae\u7684\u4efb\u52a1\u56fe\u5148\u9a8c\u3002\u6700\u540e\uff0cSPA\u91c7\u7528\u52a8\u6001\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\uff0c\u5229\u7528\u591a\u6a21\u6001\u9636\u6bb5\u9884\u6d4b\u6d41\u4e4b\u95f4\u7684\u76f8\u4e92\u534f\u8bae\uff0c\u4ee5\u81ea\u76d1\u7763\u7684\u65b9\u5f0f\u4f7f\u6a21\u578b\u9002\u5e94\u7ed9\u5b9a\u7684\u6d4b\u8bd5\u89c6\u9891\uff0c\u4ece\u800c\u63d0\u9ad8\u6d4b\u8bd5\u65f6\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u53ef\u9760\u6027\u3002", "result": "SPA\u6846\u67b6\u5728\u8de8\u673a\u6784\u548c\u7a0b\u5e8f\u7684\u5c11\u6837\u672c\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u751a\u81f3\u4f18\u4e8e\u5177\u670932\u4e2a\u6837\u672c\u6570\u636e\u7684\u5168\u6837\u672c\u6a21\u578b\u3002", "conclusion": "SPA\u6846\u67b6\u5728\u8de8\u673a\u6784\u548c\u7a0b\u5e8f\u7684\u5c11\u6837\u672c\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u751a\u81f3\u4f18\u4e8e\u5177\u670932\u4e2a\u6837\u672c\u6570\u636e\u7684\u5168\u6837\u672c\u6a21\u578b\u3002"}}
{"id": "2506.20401", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20401", "abs": "https://arxiv.org/abs/2506.20401", "authors": ["Jinchun Du", "Bojie Shen", "Muhammad Aamir Cheema", "Adel N. Toosi"], "title": "Smart Ride and Delivery Services with Electric Vehicles: Leveraging Bidirectional Charging for Profit Optimisation", "comment": null, "summary": "With the rising popularity of electric vehicles (EVs), modern service\nsystems, such as ride-hailing delivery services, are increasingly integrating\nEVs into their operations. Unlike conventional vehicles, EVs often have a\nshorter driving range, necessitating careful consideration of charging when\nfulfilling requests. With recent advances in Vehicle-to-Grid (V2G) technology -\nallowing EVs to also discharge energy back to the grid - new opportunities and\ncomplexities emerge. We introduce the Electric Vehicle Orienteering Problem\nwith V2G (EVOP-V2G): a profit-maximization problem where EV drivers must select\ncustomer requests or orders while managing when and where to charge or\ndischarge. This involves navigating dynamic electricity prices, charging\nstation selection, and route constraints. We formulate the problem as a Mixed\nInteger Programming (MIP) model and propose two near-optimal metaheuristic\nalgorithms: one evolutionary (EA) and the other based on large neighborhood\nsearch (LNS). Experiments on real-world data show our methods can double driver\nprofits compared to baselines, while maintaining near-optimal performance on\nsmall instances and excellent scalability on larger ones. Our work highlights a\npromising path toward smarter, more profitable EV-based mobility systems that\nactively support the energy grid.", "AI": {"tldr": "This paper introduces the Electric Vehicle Orienteering Problem with V2G (EVOP-V2G): a profit-maximization problem where EV drivers must select customer requests or orders while managing when and where to charge or discharge. Two metaheuristic algorithms are proposed and the experiments on real-world data show that the proposed methods can double driver profits compared to baselines.", "motivation": "With the rising popularity of electric vehicles (EVs), modern service systems, such as ride-hailing delivery services, are increasingly integrating EVs into their operations. Unlike conventional vehicles, EVs often have a shorter driving range, necessitating careful consideration of charging when fulfilling requests. With recent advances in Vehicle-to-Grid (V2G) technology - allowing EVs to also discharge energy back to the grid - new opportunities and complexities emerge.", "method": "The problem is formulated as a Mixed Integer Programming (MIP) model and propose two near-optimal metaheuristic algorithms: one evolutionary (EA) and the other based on large neighborhood search (LNS).", "result": "The proposed methods can double driver profits compared to baselines, while maintaining near-optimal performance on small instances and excellent scalability on larger ones.", "conclusion": "The experiments on real-world data show that the proposed methods can double driver profits compared to baselines, while maintaining near-optimal performance on small instances and excellent scalability on larger ones. The work highlights a promising path toward smarter, more profitable EV-based mobility systems that actively support the energy grid."}}
{"id": "2506.20015", "categories": ["cs.LG", "cs.IT", "cs.NE", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.20015", "abs": "https://arxiv.org/abs/2506.20015", "authors": ["Dengyu Wu", "Jiechen Chen", "H. Vincent Poor", "Bipin Rajendran", "Osvaldo Simeone"], "title": "Neuromorphic Wireless Split Computing with Resonate-and-Fire Neurons", "comment": null, "summary": "Neuromorphic computing offers an energy-efficient alternative to conventional\ndeep learning accelerators for real-time time-series processing. However, many\nedge applications, such as wireless sensing and audio recognition, generate\nstreaming signals with rich spectral features that are not effectively captured\nby conventional leaky integrate-and-fire (LIF) spiking neurons. This paper\ninvestigates a wireless split computing architecture that employs\nresonate-and-fire (RF) neurons with oscillatory dynamics to process time-domain\nsignals directly, eliminating the need for costly spectral pre-processing. By\nresonating at tunable frequencies, RF neurons extract time-localized spectral\nfeatures while maintaining low spiking activity. This temporal sparsity\ntranslates into significant savings in both computation and transmission\nenergy. Assuming an OFDM-based analog wireless interface for spike\ntransmission, we present a complete system design and evaluate its performance\non audio classification and modulation classification tasks. Experimental\nresults show that the proposed RF-SNN architecture achieves comparable accuracy\nto conventional LIF-SNNs and ANNs, while substantially reducing spike rates and\ntotal energy consumption during inference and communication.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u632f\u53d1\u5c04(RF)\u795e\u7ecf\u5143\u7684\u65e0\u7ebf\u5206\u5272\u8ba1\u7b97\u67b6\u6784\uff0c\u7528\u4e8e\u76f4\u63a5\u5904\u7406\u65f6\u57df\u4fe1\u53f7\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u80fd\u8017\uff0c\u5e76\u5728\u97f3\u9891\u548c\u8c03\u5236\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "motivation": "\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u4e3a\u5b9e\u65f6\u65f6\u95f4\u5e8f\u5217\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u8282\u80fd\u7684\u66ff\u4ee3\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u52a0\u901f\u5668\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u8bb8\u591a\u8fb9\u7f18\u5e94\u7528\uff0c\u5982\u65e0\u7ebf\u4f20\u611f\u548c\u97f3\u9891\u8bc6\u522b\uff0c\u4ea7\u751f\u5177\u6709\u4e30\u5bcc\u9891\u8c31\u7279\u5f81\u7684\u6d41\u4fe1\u53f7\uff0c\u800c\u4f20\u7edf\u7684leaky integrate-and-fire (LIF)\u8109\u51b2\u795e\u7ecf\u5143\u65e0\u6cd5\u6709\u6548\u5730\u6355\u6349\u8fd9\u4e9b\u7279\u5f81\u3002", "method": "\u65e0\u7ebf\u5206\u5272\u8ba1\u7b97\u67b6\u6784\uff0c\u91c7\u7528\u5177\u6709\u632f\u8361\u52a8\u6001\u7684\u5171\u632f\u53d1\u5c04(RF)\u795e\u7ecf\u5143\u76f4\u63a5\u5904\u7406\u65f6\u57df\u4fe1\u53f7\uff0c\u65e0\u9700\u6602\u8d35\u7684\u9891\u8c31\u9884\u5904\u7406\u3002", "result": "\u6240\u63d0\u51fa\u7684RF-SNN\u67b6\u6784\u5b9e\u73b0\u4e86\u4e0e\u4f20\u7edfLIF-SNN\u548cANN\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8109\u51b2\u901f\u7387\u548c\u63a8\u7406\u53ca\u901a\u4fe1\u8fc7\u7a0b\u4e2d\u7684\u603b\u80fd\u8017\u3002", "conclusion": "RF-SNN\u67b6\u6784\u5728\u63a8\u7406\u548c\u901a\u4fe1\u8fc7\u7a0b\u4e2d\uff0c\u4e0e\u4f20\u7edf\u7684LIF-SNN\u548cANN\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8109\u51b2\u901f\u7387\u548c\u603b\u80fd\u8017\u3002"}}
{"id": "2506.20178", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20178", "abs": "https://arxiv.org/abs/2506.20178", "authors": ["Zhiyuan Wang", "Jinhao Duan", "Qingni Wang", "Xiaofeng Zhu", "Tianlong Chen", "Xiaoshuang Shi", "Kaidi Xu"], "title": "COIN: Uncertainty-Guarding Selective Question Answering for Foundation Models with Provable Risk Guarantees", "comment": null, "summary": "Uncertainty quantification (UQ) for foundation models is essential to\nidentify and mitigate potential hallucinations in automatically generated text.\nHowever, heuristic UQ approaches lack formal guarantees for key metrics such as\nthe false discovery rate (FDR) in selective prediction. Previous work adopts\nthe split conformal prediction (SCP) framework to ensure desired coverage of\nadmissible answers by constructing prediction sets, but these sets often\ncontain incorrect candidates, limiting their practical utility. To address\nthis, we propose COIN, an uncertainty-guarding selection framework that\ncalibrates statistically valid thresholds to filter a single generated answer\nper question under user-specified FDR constraints. COIN estimates the empirical\nerror rate on a calibration set and applies confidence interval methods such as\nClopper-Pearson to establish a high-probability upper bound on the true error\nrate (i.e., FDR). This enables the selection of the largest uncertainty\nthreshold that ensures FDR control on test data while significantly increasing\nsample retention. We demonstrate COIN's robustness in risk control, strong\ntest-time power in retaining admissible answers, and predictive efficiency\nunder limited calibration data across both general and multimodal text\ngeneration tasks. Furthermore, we show that employing alternative upper bound\nconstructions and UQ strategies can further boost COIN's power performance,\nwhich underscores its extensibility and adaptability to diverse application\nscenarios.", "AI": {"tldr": "COIN is proposed to control false discovery rate (FDR) in text generation by calibrating uncertainty thresholds. It improves sample retention and can be further boosted by alternative methods.", "motivation": "Uncertainty quantification (UQ) for foundation models is essential to identify and mitigate potential hallucinations in automatically generated text. However, heuristic UQ approaches lack formal guarantees for key metrics such as the false discovery rate (FDR) in selective prediction. Previous work adopts the split conformal prediction (SCP) framework to ensure desired coverage of admissible answers by constructing prediction sets, but these sets often contain incorrect candidates, limiting their practical utility.", "method": "We propose COIN, an uncertainty-guarding selection framework that calibrates statistically valid thresholds to filter a single generated answer per question under user-specified FDR constraints. COIN estimates the empirical error rate on a calibration set and applies confidence interval methods such as Clopper-Pearson to establish a high-probability upper bound on the true error rate.", "result": "COIN ensures FDR control on test data while significantly increasing sample retention. COIN's power performance can be boosted by employing alternative upper bound constructions and UQ strategies.", "conclusion": "COIN is robust in risk control, strong test-time power in retaining admissible answers, and predictive efficiency under limited calibration data. Employing alternative upper bound constructions and UQ strategies can further boost COIN's power performance."}}
{"id": "2506.20255", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20255", "abs": "https://arxiv.org/abs/2506.20255", "authors": ["Ayush Lodh", "Ritabrata Chakraborty", "Shivakumara Palaiahnakote", "Umapada Pal"], "title": "A Transformer Based Handwriting Recognition System Jointly Using Online and Offline Features", "comment": "15 pages, 7 figures", "summary": "We posit that handwriting recognition benefits from complementary cues\ncarried by the rasterized complex glyph and the pen's trajectory, yet most\nsystems exploit only one modality. We introduce an end-to-end network that\nperforms early fusion of offline images and online stroke data within a shared\nlatent space. A patch encoder converts the grayscale crop into fixed-length\nvisual tokens, while a lightweight transformer embeds the $(x, y, \\text{pen})$\nsequence. Learnable latent queries attend jointly to both token streams,\nyielding context-enhanced stroke embeddings that are pooled and decoded under a\ncross-entropy loss objective. Because integration occurs before any high-level\nclassification, temporal cues reinforce each other during representation\nlearning, producing stronger writer independence. Comprehensive experiments on\nIAMOn-DB and VNOn-DB demonstrate that our approach achieves state-of-the-art\naccuracy, exceeding previous bests by up to 1\\%. Our study also shows\nadaptation of this pipeline with gesturification on the ISI-Air dataset. Our\ncode can be found here.", "AI": {"tldr": "early fusion of offline images and online stroke data within a shared latent space to achieve state-of-the-art accuracy in handwriting recognition.", "motivation": "handwriting recognition benefits from complementary cues carried by the rasterized complex glyph and the pen's trajectory, yet most systems exploit only one modality.", "method": "an end-to-end network that performs early fusion of offline images and online stroke data within a shared latent space. A patch encoder converts the grayscale crop into fixed-length visual tokens, while a lightweight transformer embeds the $(x, y, \t ext{pen})$ sequence. Learnable latent queries attend jointly to both token streams, yielding context-enhanced stroke embeddings that are pooled and decoded under a cross-entropy loss objective.", "result": "achieves state-of-the-art accuracy, exceeding previous bests by up to 1%. adaptation of this pipeline with gesturification on the ISI-Air dataset.", "conclusion": "The approach achieves state-of-the-art accuracy, exceeding previous bests by up to 1%."}}
{"id": "2506.20404", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20404", "abs": "https://arxiv.org/abs/2506.20404", "authors": ["Riccardo Lo Bianco", "Willem van Jaarsveld", "Remco Dijkman"], "title": "GymPN: A Library for Decision-Making in Process Management Systems", "comment": null, "summary": "Process management systems support key decisions about the way work is\nallocated in organizations. This includes decisions on which task to perform\nnext, when to execute the task, and who to assign the task to. Suitable\nsoftware tools are required to support these decisions in a way that is optimal\nfor the organization. This paper presents a software library, called GymPN,\nthat supports optimal decision-making in business processes using Deep\nReinforcement Learning. GymPN builds on previous work that supports task\nassignment in business processes, introducing two key novelties: support for\npartial process observability and the ability to model multiple decisions in a\nbusiness process. These novel elements address fundamental limitations of\nprevious work and thus enable the representation of more realistic process\ndecisions. We evaluate the library on eight typical business process\ndecision-making problem patterns, showing that GymPN allows for easy modeling\nof the desired problems, as well as learning optimal decision policies.", "AI": {"tldr": "This paper presents GymPN, a software library that supports optimal decision-making in business processes using Deep Reinforcement Learning. It addresses limitations of previous work by supporting partial process observability and modeling multiple decisions, and demonstrates its effectiveness on typical business process decision-making problems.", "motivation": "Suitable software tools are required to support these decisions in a way that is optimal for the organization.", "method": "a software library, called GymPN, that supports optimal decision-making in business processes using Deep Reinforcement Learning", "result": "introducing two key novelties: support for partial process observability and the ability to model multiple decisions in a business process", "conclusion": "GymPN allows for easy modeling of the desired problems, as well as learning optimal decision policies."}}
{"id": "2506.20016", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20016", "abs": "https://arxiv.org/abs/2506.20016", "authors": ["Shanika Iroshi Nanayakkara", "Shiva Raj Pokhrel"], "title": "New Insights on Unfolding and Fine-tuning Quantum Federated Learning", "comment": "12 pages, 9 figures, 7 Tables, Submitted to IEEE/ACM journal 2025", "summary": "Client heterogeneity poses significant challenges to the performance of\nQuantum Federated Learning (QFL). To overcome these limitations, we propose a\nnew approach leveraging deep unfolding, which enables clients to autonomously\noptimize hyperparameters, such as learning rates and regularization factors,\nbased on their specific training behavior. This dynamic adaptation mitigates\noverfitting and ensures robust optimization in highly heterogeneous\nenvironments where standard aggregation methods often fail. Our framework\nachieves approximately 90% accuracy, significantly outperforming traditional\nmethods, which typically yield around 55% accuracy, as demonstrated through\nreal-time training on IBM quantum hardware and Qiskit Aer simulators. By\ndeveloping self adaptive fine tuning, the proposed method proves particularly\neffective in critical applications such as gene expression analysis and cancer\ndetection, enhancing diagnostic precision and predictive modeling within\nquantum systems. Our results are attributed to convergence-aware, learnable\noptimization steps intrinsic to the deep unfolded framework, which maintains\nthe generalization. Hence, this study addresses the core limitations of\nconventional QFL, streamlining its applicability to any complex challenges such\nas healthcare and genomic research.", "AI": {"tldr": "propose a new approach leveraging deep unfolding, which enables clients to autonomously optimize hyperparameters, such as learning rates and regularization factors, based on their specific training behavior to overcome the limitations of Client heterogeneity in Quantum Federated Learning (QFL).", "motivation": "Client heterogeneity poses significant challenges to the performance of Quantum Federated Learning (QFL).", "method": "a new approach leveraging deep unfolding, which enables clients to autonomously optimize hyperparameters", "result": "achieves approximately 90% accuracy, significantly outperforming traditional methods, which typically yield around 55% accuracy", "conclusion": "This study addresses the core limitations of conventional QFL, streamlining its applicability to any complex challenges such as healthcare and genomic research."}}
{"id": "2506.20199", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20199", "abs": "https://arxiv.org/abs/2506.20199", "authors": ["Mengqi Wang", "Tiantian Feng", "Shrikanth Narayanan"], "title": "How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?", "comment": null, "summary": "Large language models (LLMs) have enabled a wide variety of real-world\napplications in various domains. However, creating a high-performing\napplication with high accuracy remains challenging, particularly for subjective\ntasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this\nstudy investigates approaches to improving conversational emotion recognition\n(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples\nin in-context learning (ICL) to enhance CER. We propose various strategies\nbased on random and augmented example retrieval and also analyze the impact of\nconversational context on CER accuracy. Experiments were conducted on the three\ndatasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented\nexample retrieval consistently outperforms other techniques under investigation\nacross all datasets, highlighting the importance of retrieving coherent\ntargeted examples and enhancing them through paraphrasing.", "AI": {"tldr": "This study explores how to retrieve high-quality examples in in-context learning (ICL) to enhance CER. Augmented example retrieval consistently outperforms other techniques.", "motivation": "Creating a high-performing application with high accuracy remains challenging, particularly for subjective tasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this study investigates approaches to improving conversational emotion recognition (CER) by LLMs.", "method": "We propose various strategies based on random and augmented example retrieval and also analyze the impact of conversational context on CER accuracy.", "result": "Experiments were conducted on the three datasets including IEMOCAP, MELD and EmoryNLP.", "conclusion": "Augmented example retrieval consistently outperforms other techniques under investigation across all datasets, highlighting the importance of retrieving coherent targeted examples and enhancing them through paraphrasing."}}
{"id": "2506.20263", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20263", "abs": "https://arxiv.org/abs/2506.20263", "authors": ["Ning Luo", "Meiyin Hu", "Huan Wan", "Yanyan Yang", "Zhuohang Jiang", "Xin Wei"], "title": "Hierarchical Mask-Enhanced Dual Reconstruction Network for Few-Shot Fine-Grained Image Classification", "comment": null, "summary": "Few-shot fine-grained image classification (FS-FGIC) presents a significant\nchallenge, requiring models to distinguish visually similar subclasses with\nlimited labeled examples. Existing methods have critical limitations:\nmetric-based methods lose spatial information and misalign local features,\nwhile reconstruction-based methods fail to utilize hierarchical feature\ninformation and lack mechanisms to focus on discriminative regions. We propose\nthe Hierarchical Mask-enhanced Dual Reconstruction Network (HMDRN), which\nintegrates dual-layer feature reconstruction with mask-enhanced feature\nprocessing to improve fine-grained classification. HMDRN incorporates a\ndual-layer feature reconstruction and fusion module that leverages\ncomplementary visual information from different network hierarchies. Through\nlearnable fusion weights, the model balances high-level semantic\nrepresentations from the last layer with mid-level structural details from the\npenultimate layer. Additionally, we design a spatial binary mask-enhanced\ntransformer self-reconstruction module that processes query features through\nadaptive thresholding while maintaining complete support features, enhancing\nfocus on discriminative regions while filtering background noise. Extensive\nexperiments on three challenging fine-grained datasets demonstrate that HMDRN\nconsistently outperforms state-of-the-art methods across Conv-4 and ResNet-12\nbackbone architectures. Comprehensive ablation studies validate the\neffectiveness of each proposed component, revealing that dual-layer\nreconstruction enhances inter-class discrimination while mask-enhanced\ntransformation reduces intra-class variations. Visualization results provide\nevidence of HMDRN's superior feature reconstruction capabilities.", "AI": {"tldr": "HMDRN integrates dual-layer feature reconstruction with mask-enhanced feature processing to improve fine-grained classification.", "motivation": "Existing methods have critical limitations: metric-based methods lose spatial information and misalign local features, while reconstruction-based methods fail to utilize hierarchical feature information and lack mechanisms to focus on discriminative regions.", "method": "Hierarchical Mask-enhanced Dual Reconstruction Network (HMDRN), which integrates dual-layer feature reconstruction with mask-enhanced feature processing", "result": "dual-layer reconstruction enhances inter-class discrimination while mask-enhanced transformation reduces intra-class variations.", "conclusion": "HMDRN consistently outperforms state-of-the-art methods across Conv-4 and ResNet-12 backbone architectures."}}
{"id": "2506.20486", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20486", "abs": "https://arxiv.org/abs/2506.20486", "authors": ["Salvatore Milite", "Giulio Caravagna", "Andrea Sottoriva"], "title": "Mixtures of Neural Cellular Automata: A Stochastic Framework for Growth Modelling and Self-Organization", "comment": null, "summary": "Neural Cellular Automata (NCAs) are a promising new approach to model\nself-organizing processes, with potential applications in life science.\nHowever, their deterministic nature limits their ability to capture the\nstochasticity of real-world biological and physical systems.\n  We propose the Mixture of Neural Cellular Automata (MNCA), a novel framework\nincorporating the idea of mixture models into the NCA paradigm. By combining\nprobabilistic rule assignments with intrinsic noise, MNCAs can model diverse\nlocal behaviors and reproduce the stochastic dynamics observed in biological\nprocesses.\n  We evaluate the effectiveness of MNCAs in three key domains: (1) synthetic\nsimulations of tissue growth and differentiation, (2) image morphogenesis\nrobustness, and (3) microscopy image segmentation. Results show that MNCAs\nachieve superior robustness to perturbations, better recapitulate real\nbiological growth patterns, and provide interpretable rule segmentation. These\nfindings position MNCAs as a promising tool for modeling stochastic dynamical\nsystems and studying self-growth processes.", "AI": {"tldr": "MNCAs, a novel framework incorporating the idea of mixture models into the NCA paradigm, can model diverse local behaviors and reproduce the stochastic dynamics observed in biological processes.", "motivation": "NCAs' deterministic nature limits their ability to capture the stochasticity of real-world biological and physical systems.", "method": "We propose the Mixture of Neural Cellular Automata (MNCA), a novel framework incorporating the idea of mixture models into the NCA paradigm. By combining probabilistic rule assignments with intrinsic noise, MNCAs can model diverse local behaviors and reproduce the stochastic dynamics observed in biological processes.", "result": "MNCAs achieve superior robustness to perturbations, better recapitulate real biological growth patterns, and provide interpretable rule segmentation.", "conclusion": "MNCAs are a promising tool for modeling stochastic dynamical systems and studying self-growth processes."}}
{"id": "2506.20203", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20203", "abs": "https://arxiv.org/abs/2506.20203", "authors": ["Petra Baran\u010d\u00edkov\u00e1", "Ond\u0159ej Bojar"], "title": "Intrinsic vs. Extrinsic Evaluation of Czech Sentence Embeddings: Semantic Relevance Doesn't Help with MT Evaluation", "comment": null, "summary": "In this paper, we compare Czech-specific and multilingual sentence embedding\nmodels through intrinsic and extrinsic evaluation paradigms. For intrinsic\nevaluation, we employ Costra, a complex sentence transformation dataset, and\nseveral Semantic Textual Similarity (STS) benchmarks to assess the ability of\nthe embeddings to capture linguistic phenomena such as semantic similarity,\ntemporal aspects, and stylistic variations. In the extrinsic evaluation, we\nfine-tune each embedding model using COMET-based metrics for machine\ntranslation evaluation.\n  Our experiments reveal an interesting disconnect: models that excel in\nintrinsic semantic similarity tests do not consistently yield superior\nperformance on downstream translation evaluation tasks. Conversely, models with\nseemingly over-smoothed embedding spaces can, through fine-tuning, achieve\nexcellent results. These findings highlight the complex relationship between\nsemantic property probes and downstream task, emphasizing the need for more\nresearch into 'operationalizable semantics' in sentence embeddings, or more\nin-depth downstream tasks datasets (here translation evaluation)", "AI": {"tldr": "compared Czech-specific and multilingual sentence embedding models through intrinsic and extrinsic evaluation paradigms, found that models that excel in intrinsic semantic similarity tests do not consistently yield superior performance on downstream translation evaluation tasks.", "motivation": "comparing Czech-specific and multilingual sentence embedding models", "method": "compare Czech-specific and multilingual sentence embedding models through intrinsic and extrinsic evaluation paradigms. intrinsic evaluation: Costra, a complex sentence transformation dataset, and several Semantic Textual Similarity (STS) benchmarks. extrinsic evaluation: fine-tune each embedding model using COMET-based metrics for machine translation evaluation.", "result": "models that excel in intrinsic semantic similarity tests do not consistently yield superior performance on downstream translation evaluation tasks. models with seemingly over-smoothed embedding spaces can, through fine-tuning, achieve excellent results.", "conclusion": "Models that excel in intrinsic semantic similarity tests do not consistently yield superior performance on downstream translation evaluation tasks. Models with seemingly over-smoothed embedding spaces can, through fine-tuning, achieve excellent results. The complex relationship between semantic property probes and downstream task, emphasizing the need for more research into 'operationalizable semantics' in sentence embeddings, or more in-depth downstream tasks datasets (here translation evaluation)"}}
{"id": "2506.20272", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20272", "abs": "https://arxiv.org/abs/2506.20272", "authors": ["Juan Jos\u00e9 Murillo-Fuentes", "Pablo M. Olmos", "Laura Alba-Carcel\u00e9n"], "title": "Forensic Study of Paintings Through the Comparison of Fabrics", "comment": null, "summary": "The study of canvas fabrics in works of art is a crucial tool for\nauthentication, attribution and conservation. Traditional methods are based on\nthread density map matching, which cannot be applied when canvases do not come\nfrom contiguous positions on a roll. This paper presents a novel approach based\non deep learning to assess the similarity of textiles. We introduce an\nautomatic tool that evaluates the similarity between canvases without relying\non thread density maps. A Siamese deep learning model is designed and trained\nto compare pairs of images by exploiting the feature representations learned\nfrom the scans. In addition, a similarity estimation method is proposed,\naggregating predictions from multiple pairs of cloth samples to provide a\nrobust similarity score. Our approach is applied to canvases from the Museo\nNacional del Prado, corroborating the hypothesis that plain weave canvases,\nwidely used in painting, can be effectively compared even when their thread\ndensities are similar. The results demonstrate the feasibility and accuracy of\nthe proposed method, opening new avenues for the analysis of masterpieces.", "AI": {"tldr": "A novel deep learning approach is presented to assess the similarity of canvas fabrics in artworks, overcoming limitations of traditional methods and enabling analysis of non-contiguous pieces.", "motivation": "Traditional methods for analyzing canvas fabrics in art rely on thread density map matching, which is ineffective for non-contiguous canvas pieces.", "method": "A Siamese deep learning model is designed and trained to compare pairs of images, with a similarity estimation method aggregating predictions from multiple pairs of cloth samples.", "result": "The approach is applied to canvases from the Museo Nacional del Prado, corroborating the hypothesis that plain weave canvases can be effectively compared even with similar thread densities.", "conclusion": "The proposed deep learning method demonstrates feasibility and accuracy in comparing plain weave canvases, opening new avenues for analyzing masterpieces."}}
{"id": "2506.20504", "categories": ["cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2506.20504", "abs": "https://arxiv.org/abs/2506.20504", "authors": ["Konstantin Demin", "Taylor Webb", "Eric Elmoznino", "Hakwan Lau"], "title": "Engineering Sentience", "comment": null, "summary": "We spell out a definition of sentience that may be useful for designing and\nbuilding it in machines. We propose that for sentience to be meaningful for AI,\nit must be fleshed out in functional, computational terms, in enough detail to\nallow for implementation. Yet, this notion of sentience must also reflect\nsomething essentially 'subjective', beyond just having the general capacity to\nencode perceptual content. For this specific functional notion of sentience to\noccur, we propose that certain sensory signals need to be both assertoric\n(persistent) and qualitative. To illustrate the definition in more concrete\nterms, we sketch out some ways for potential implementation, given current\ntechnology. Understanding what it takes for artificial agents to be\nfunctionally sentient can also help us avoid creating them inadvertently, or at\nleast, realize that we have created them in a timely manner.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u611f\u77e5\u7684\u5b9a\u4e49\uff0c\u5b83\u53ef\u80fd\u6709\u52a9\u4e8e\u5728\u673a\u5668\u4e2d\u8bbe\u8ba1\u548c\u6784\u5efa\u5b83\u3002", "motivation": "\u6211\u4eec\u8be6\u7ec6\u9610\u8ff0\u4e86\u611f\u77e5\u7684\u5b9a\u4e49\uff0c\u8fd9\u53ef\u80fd\u6709\u52a9\u4e8e\u5728\u673a\u5668\u4e2d\u8bbe\u8ba1\u548c\u6784\u5efa\u5b83\u3002", "method": "\u6211\u4eec\u63d0\u51fa\uff0c\u4e3a\u4e86\u4f7f\u611f\u77e5\u5bf9\u4eba\u5de5\u667a\u80fd\u6709\u610f\u4e49\uff0c\u5b83\u5fc5\u987b\u4ee5\u529f\u80fd\u6027\u7684\u3001\u8ba1\u7b97\u6027\u7684\u672f\u8bed\u6765\u5145\u5b9e\uff0c\u8981\u6709\u8db3\u591f\u7684\u7ec6\u8282\u4ee5\u5141\u8bb8\u5b9e\u65bd\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u611f\u77e5\u6982\u5ff5\u4e5f\u5fc5\u987b\u53cd\u6620\u4e00\u4e9b\u672c\u8d28\u4e0a\u201c\u4e3b\u89c2\u201d\u7684\u4e1c\u897f\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5177\u6709\u7f16\u7801\u611f\u77e5\u5185\u5bb9\u7684\u4e00\u822c\u80fd\u529b\u3002\u4e3a\u4e86\u4f7f\u8fd9\u79cd\u7279\u5b9a\u7684\u529f\u80fd\u6027\u611f\u77e5\u6982\u5ff5\u53d1\u751f\uff0c\u6211\u4eec\u63d0\u51fa\u67d0\u4e9b\u611f\u5b98\u4fe1\u53f7\u9700\u8981\u65e2\u662f\u65ad\u8a00\u6027\u7684\uff08\u6301\u4e45\u7684\uff09\u53c8\u662f\u5b9a\u6027\u7684\u3002", "result": "\u4e3a\u4e86\u7528\u66f4\u5177\u4f53\u7684\u672f\u8bed\u6765\u8bf4\u660e\u8fd9\u4e2a\u5b9a\u4e49\uff0c\u6211\u4eec\u6982\u8ff0\u4e86\u4e00\u4e9b\u5728\u5f53\u524d\u6280\u672f\u6761\u4ef6\u4e0b\u6f5c\u5728\u7684\u5b9e\u73b0\u65b9\u6cd5\u3002", "conclusion": "\u7406\u89e3\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u5728\u529f\u80fd\u4e0a\u5177\u6709\u611f\u77e5\u80fd\u529b\u9700\u8981\u4ec0\u4e48\uff0c\u4e5f\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u907f\u514d\u610f\u5916\u5730\u521b\u9020\u5b83\u4eec\uff0c\u6216\u8005\u81f3\u5c11\u53ca\u65f6\u610f\u8bc6\u5230\u6211\u4eec\u5df2\u7ecf\u521b\u9020\u4e86\u5b83\u4eec\u3002"}}
{"id": "2506.20024", "categories": ["cs.LG", "cs.AI", "physics.ao-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.20024", "abs": "https://arxiv.org/abs/2506.20024", "authors": ["Salva R\u00fchling Cachay", "Miika Aittala", "Karsten Kreis", "Noah Brenowitz", "Arash Vahdat", "Morteza Mardani", "Rose Yu"], "title": "Elucidated Rolling Diffusion Models for Probabilistic Weather Forecasting", "comment": null, "summary": "Diffusion models are a powerful tool for probabilistic forecasting, yet most\napplications in high-dimensional chaotic systems predict future snapshots\none-by-one. This common approach struggles to model complex temporal\ndependencies and fails to explicitly account for the progressive growth of\nuncertainty inherent to such systems. While rolling diffusion frameworks, which\napply increasing noise to forecasts at longer lead times, have been proposed to\naddress this, their integration with state-of-the-art, high-fidelity diffusion\ntechniques remains a significant challenge. We tackle this problem by\nintroducing Elucidated Rolling Diffusion Models (ERDM), the first framework to\nsuccessfully unify a rolling forecast structure with the principled, performant\ndesign of Elucidated Diffusion Models (EDM). To do this, we adapt the core EDM\ncomponents-its noise schedule, network preconditioning, and Heun sampler-to the\nrolling forecast setting. The success of this integration is driven by three\nkey contributions: (i) a novel loss weighting scheme that focuses model\ncapacity on the mid-range forecast horizons where determinism gives way to\nstochasticity; (ii) an efficient initialization strategy using a pre-trained\nEDM for the initial window; and (iii) a bespoke hybrid sequence architecture\nfor robust spatiotemporal feature extraction under progressive denoising. On 2D\nNavier-Stokes simulations and ERA5 global weather forecasting at 1.5^\\circ\nresolution, ERDM consistently outperforms key diffusion-based baselines,\nincluding conditional autoregressive EDM. ERDM offers a flexible and powerful\ngeneral framework for tackling diffusion-based sequence generation problems\nwhere modeling escalating uncertainty is paramount. Code is available at:\nhttps://github.com/salvaRC/erdm", "AI": {"tldr": "ERDM\u662f\u4e00\u79cd\u65b0\u7684\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u6eda\u52a8\u9884\u6d4b\u548cEDM\uff0c\u5728\u5e8f\u5217\u751f\u6210\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u6a21\u62df\u4e0d\u786e\u5b9a\u6027\u589e\u957f\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u5728\u9884\u6d4b\u9ad8\u7ef4\u6df7\u6c8c\u7cfb\u7edf\u65f6\uff0c\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u5e76\u4e14\u4e0d\u80fd\u663e\u5f0f\u5730\u89e3\u91ca\u4e0d\u786e\u5b9a\u6027\u7684\u589e\u957f\u3002", "method": "\u63d0\u51fa\u4e86Elucidated Rolling Diffusion Models (ERDM)\uff0c\u5b83\u5c06\u6eda\u52a8\u9884\u6d4b\u7ed3\u6784\u4e0eElucidated Diffusion Models (EDM)\u7684\u8bbe\u8ba1\u7edf\u4e00\u8d77\u6765\uff0c\u5e76\u8c03\u6574\u4e86EDM\u7684\u6838\u5fc3\u7ec4\u4ef6\uff0c\u5305\u62ec\u566a\u58f0\u8c03\u5ea6\u3001\u7f51\u7edc\u9884\u5904\u7406\u548cHeun\u91c7\u6837\u5668\u3002", "result": "ERDM\u57282D Navier-Stokes\u6a21\u62df\u548cERA5\u5168\u7403\u5929\u6c14\u9884\u6d4b\u4e2d\uff0c\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u57fa\u4e8e\u6269\u6563\u7684\u6a21\u578b\u3002", "conclusion": "ERDM\u5728\u6a21\u62df\u548c\u5929\u6c14\u9884\u6d4b\u4e2d\u4f18\u4e8e\u5176\u4ed6\u6269\u6563\u6a21\u578b\uff0c\u4e3a\u6a21\u62df\u4e0d\u786e\u5b9a\u6027\u589e\u957f\u7684\u5e8f\u5217\u751f\u6210\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u800c\u5f3a\u5927\u7684\u6846\u67b6\u3002"}}
{"id": "2506.20209", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20209", "abs": "https://arxiv.org/abs/2506.20209", "authors": ["Benedetta Muscato", "Lucia Passaro", "Gizem Gezici", "Fosca Giannotti"], "title": "Perspectives in Play: A Multi-Perspective Approach for More Inclusive NLP Systems", "comment": null, "summary": "In the realm of Natural Language Processing (NLP), common approaches for\nhandling human disagreement consist of aggregating annotators' viewpoints to\nestablish a single ground truth. However, prior studies show that disregarding\nindividual opinions can lead can lead to the side effect of underrepresenting\nminority perspectives, especially in subjective tasks, where annotators may\nsystematically disagree because of their preferences. Recognizing that labels\nreflect the diverse backgrounds, life experiences, and values of individuals,\nthis study proposes a new multi-perspective approach using soft labels to\nencourage the development of the next generation of perspective aware models,\nmore inclusive and pluralistic. We conduct an extensive analysis across diverse\nsubjective text classification tasks, including hate speech, irony, abusive\nlanguage, and stance detection, to highlight the importance of capturing human\ndisagreements, often overlooked by traditional aggregation methods. Results\nshow that the multi-perspective approach not only better approximates human\nlabel distributions, as measured by Jensen-Shannon Divergence (JSD), but also\nachieves superior classification performance (higher F1 scores), outperforming\ntraditional approaches. However, our approach exhibits lower confidence in\ntasks like irony and stance detection, likely due to the inherent subjectivity\npresent in the texts. Lastly, leveraging Explainable AI (XAI), we explore model\nuncertainty and uncover meaningful insights into model predictions.", "AI": {"tldr": "This paper introduces a multi-perspective approach using soft labels to capture human disagreements in subjective NLP tasks. It outperforms traditional methods but shows lower confidence in highly subjective tasks, with XAI providing insights.", "motivation": "Disregarding individual opinions in NLP can underrepresent minority perspectives, especially in subjective tasks.", "method": "A new multi-perspective approach using soft labels.", "result": "The multi-perspective approach better approximates human label distributions (JSD) and achieves superior classification performance (F1 scores), but shows lower confidence in irony and stance detection. XAI provides insights into model predictions.", "conclusion": "The multi-perspective approach, while better approximating human label distributions and achieving superior classification performance, exhibits lower confidence in highly subjective tasks. XAI is used to explore model uncertainty and uncover insights."}}
{"id": "2506.20279", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20279", "abs": "https://arxiv.org/abs/2506.20279", "authors": ["Changliang Xia", "Chengyou Jia", "Zhuohang Dang", "Minnan Luo"], "title": "From Ideal to Real: Unified and Data-Efficient Dense Prediction for Real-World Scenarios", "comment": null, "summary": "Dense prediction tasks hold significant importance of computer vision, aiming\nto learn pixel-wise annotated label for an input image. Despite advances in\nthis field, existing methods primarily focus on idealized conditions, with\nlimited generalization to real-world scenarios and facing the challenging\nscarcity of real-world data. To systematically study this problem, we first\nintroduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction\ntasks that correspond to urgent real-world applications, featuring unified\nevaluation across tasks. Then, we propose DenseDiT, which maximally exploits\ngenerative models' visual priors to perform diverse real-world dense prediction\ntasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism\nand two lightweight branches that adaptively integrate multi-scale context,\nworking with less than 0.1% additional parameters. Evaluations on DenseWorld\nreveal significant performance drops in existing general and specialized\nbaselines, highlighting their limited real-world generalization. In contrast,\nDenseDiT achieves superior results using less than 0.01% training data of\nbaselines, underscoring its practical value for real-world deployment. Our\ndata, and checkpoints and codes are available at\nhttps://xcltql666.github.io/DenseDiTProj", "AI": {"tldr": "The paper introduces DenseWorld, a benchmark for real-world dense prediction tasks, and DenseDiT, a model that leverages generative priors for improved performance with minimal additional parameters and training data.", "motivation": "Existing methods primarily focus on idealized conditions, with limited generalization to real-world scenarios and facing the challenging scarcity of real-world data.", "method": "We propose DenseDiT, which maximally exploits generative models' visual priors to perform diverse real-world dense prediction tasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism and two lightweight branches that adaptively integrate multi-scale context, working with less than 0.1% additional parameters.", "result": "Evaluations on DenseWorld reveal significant performance drops in existing general and specialized baselines, highlighting their limited real-world generalization.", "conclusion": "DenseDiT achieves superior results using less than 0.01% training data of baselines, underscoring its practical value for real-world deployment."}}
{"id": "2506.20531", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.20531", "abs": "https://arxiv.org/abs/2506.20531", "authors": ["Wenbin Gan", "Minh-Son Dao", "Koji Zettsu"], "title": "Case-based Reasoning Augmented Large Language Model Framework for Decision Making in Realistic Safety-Critical Driving Scenarios", "comment": "12 pages, 10 figures, under-review conference", "summary": "Driving in safety-critical scenarios requires quick, context-aware\ndecision-making grounded in both situational understanding and experiential\nreasoning. Large Language Models (LLMs), with their powerful general-purpose\nreasoning capabilities, offer a promising foundation for such decision-making.\nHowever, their direct application to autonomous driving remains limited due to\nchallenges in domain adaptation, contextual grounding, and the lack of\nexperiential knowledge needed to make reliable and interpretable decisions in\ndynamic, high-risk environments. To address this gap, this paper presents a\nCase-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for\nevasive maneuver decision-making in complex risk scenarios. Our approach\nintegrates semantic scene understanding from dashcam video inputs with the\nretrieval of relevant past driving cases, enabling LLMs to generate maneuver\nrecommendations that are both context-sensitive and human-aligned. Experiments\nacross multiple open-source LLMs show that our framework improves decision\naccuracy, justification quality, and alignment with human expert behavior.\nRisk-aware prompting strategies further enhance performance across diverse risk\ntypes, while similarity-based case retrieval consistently outperforms random\nsampling in guiding in-context learning. Case studies further demonstrate the\nframework's robustness in challenging real-world conditions, underscoring its\npotential as an adaptive and trustworthy decision-support tool for intelligent\ndriving systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86CBR-LLM\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u8bed\u4e49\u573a\u666f\u7406\u89e3\u548c\u68c0\u7d22\u76f8\u5173\u9a7e\u9a76\u6848\u4f8b\uff0c\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u98ce\u9669\u573a\u666f\u4e2d\u89c4\u907f\u673a\u52a8\u51b3\u7b56\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u5177\u6709\u5f3a\u5927\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u57fa\u7840\u3002\u7136\u800c\uff0c\u7531\u4e8e\u9886\u57df\u9002\u5e94\u3001\u60c5\u5883\u57fa\u7840\u4ee5\u53ca\u7f3a\u4e4f\u5728\u52a8\u6001\u3001\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u505a\u51fa\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u51b3\u7b56\u6240\u9700\u7684\u7ecf\u9a8c\u77e5\u8bc6\uff0c\u5b83\u4eec\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u76f4\u63a5\u5e94\u7528\u4ecd\u7136\u53d7\u5230\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6848\u4f8b\u63a8\u7406\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b(CBR-LLM)\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u590d\u6742\u98ce\u9669\u573a\u666f\u4e2d\u8fdb\u884c\u89c4\u907f\u673a\u52a8\u51b3\u7b56\u3002", "result": "\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u51b3\u7b56\u7684\u51c6\u786e\u6027\u3001\u7406\u7531\u8d28\u91cf\u4ee5\u53ca\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u884c\u4e3a\u7684\u4e00\u81f4\u6027\u3002\u57fa\u4e8e\u98ce\u9669\u7684\u63d0\u793a\u7b56\u7565\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u5404\u79cd\u98ce\u9669\u7c7b\u578b\u7684\u6027\u80fd\uff0c\u800c\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u6848\u4f8b\u68c0\u7d22\u59cb\u7ec8\u4f18\u4e8e\u968f\u673a\u62bd\u6837\u3002", "conclusion": "CBR-LLM\u6846\u67b6\u5728\u590d\u6742\u98ce\u9669\u573a\u666f\u4e2d\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u6709\u6f5c\u529b\u4f5c\u4e3a\u667a\u80fd\u9a7e\u9a76\u7cfb\u7edf\u7684\u81ea\u9002\u5e94\u548c\u53ef\u4fe1\u8d56\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\u3002"}}
{"id": "2506.20025", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.20025", "abs": "https://arxiv.org/abs/2506.20025", "authors": ["Nathan Stromberg", "Christos Thrampoulidis", "Lalitha Sankar"], "title": "Thumb on the Scale: Optimal Loss Weighting in Last Layer Retraining", "comment": null, "summary": "While machine learning models become more capable in discriminative tasks at\nscale, their ability to overcome biases introduced by training data has come\nunder increasing scrutiny. Previous results suggest that there are two extremes\nof parameterization with very different behaviors: the population\n(underparameterized) setting where loss weighting is optimal and the separable\noverparameterized setting where loss weighting is ineffective at ensuring equal\nperformance across classes. This work explores the regime of last layer\nretraining (LLR) in which the unseen limited (retraining) data is frequently\ninseparable and the model proportionately sized, falling between the two\naforementioned extremes. We show, in theory and practice, that loss weighting\nis still effective in this regime, but that these weights \\emph{must} take into\naccount the relative overparameterization of the model.", "AI": {"tldr": "loss weighting is effective in the last layer retraining regime if weights take into account the relative overparameterization of the model", "motivation": "machine learning models' ability to overcome biases introduced by training data has come under increasing scrutiny. Previous results suggest that there are two extremes of parameterization with very different behaviors", "method": "theoretically and empirically explore the regime of last layer retraining", "result": "loss weighting is still effective in the last layer retraining regime", "conclusion": "loss weighting is still effective in the last layer retraining regime, but that these weights must take into account the relative overparameterization of the model"}}
{"id": "2506.20241", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20241", "abs": "https://arxiv.org/abs/2506.20241", "authors": ["Yubo Dong", "Hehe Fan"], "title": "Enhancing Large Language Models through Structured Reasoning", "comment": "Preprint. Under review", "summary": "Recent Large Language Models (LLMs) have significantly advanced natural\nlanguage processing and automated decision-making. However, these models still\nencounter difficulties when performing complex reasoning tasks involving\nlogical deduction and systematic planning, primarily due to their reliance on\nimplicit statistical relationships without structured knowledge\nrepresentation.Inspired by cognitive science and neurosymbolic AI, we introduce\na novel approach to enhance LLMs through explicit structured reasoning. First,\nwe convert unstructured data into structured formats by explicitly annotating\nreasoning steps. We then employ this structured dataset to train LLMs through\nSupervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning\ncapabilities of LLMs using Group Relative Policy Optimization (GRPO),\nincorporating two innovative algorithms--MAX-Flow and Longest Common\nSubsequence (LCS)--which notably improve reasoning effectiveness and reduce\ncomputational complexity. Experimental results from fine-tuning a\nDeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust\nperformance across various scenarios, and improved compatibility with\noptimization techniques, validating the efficacy of structured reasoning\nintegration in LLMs.", "AI": {"tldr": "\u901a\u8fc7\u663e\u5f0f\u7ed3\u6784\u5316\u63a8\u7406\u6765\u589e\u5f3aLLM\uff0c\u4ece\u800c\u63d0\u9ad8\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u6700\u8fd1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u81ea\u52a8\u51b3\u7b56\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u6267\u884c\u6d89\u53ca\u903b\u8f91\u63a8\u6f14\u548c\u7cfb\u7edf\u89c4\u5212\u7684\u590d\u6742\u63a8\u7406\u4efb\u52a1\u65f6\u4ecd\u7136\u9047\u5230\u56f0\u96be\uff0c\u8fd9\u4e3b\u8981\u662f\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u4e8e\u6ca1\u6709\u7ed3\u6784\u5316\u77e5\u8bc6\u8868\u793a\u7684\u9690\u5f0f\u7edf\u8ba1\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u663e\u5f0f\u6ce8\u91ca\u63a8\u7406\u6b65\u9aa4\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u6570\u636e\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u683c\u5f0f\uff0c\u4f7f\u7528\u7ed3\u6784\u5316\u6570\u636e\u96c6\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u8bad\u7ec3LLM\u3002\u4f7f\u7528GRPO\u589e\u5f3aLLM\u7684\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\uff0c\u5305\u542bMAX-Flow\u548cLCS\u7b97\u6cd5\u3002", "result": "\u5bf9DeepSeek-R1-Distill-Qwen-1.5B\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5176\u63a8\u7406\u7b80\u6d01\uff0c\u5728\u5404\u79cd\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u9ad8\u4e86\u4e0e\u4f18\u5316\u6280\u672f\u7684\u517c\u5bb9\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7ed3\u6784\u5316\u63a8\u7406\u6574\u5408\u5230LLM\u4e2d\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2506.20293", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.20293", "abs": "https://arxiv.org/abs/2506.20293", "authors": ["Kunjing Yang", "Libin Zheng", "Minru Bai", "Ting Lu", "Leyuan Fang"], "title": "Breaking Spatial Boundaries: Spectral-Domain Registration Guided Hyperspectral and Multispectral Blind Fusion", "comment": null, "summary": "The blind fusion of unregistered hyperspectral images (HSIs) and\nmultispectral images (MSIs) has attracted growing attention recently. To\naddress the registration challenge, most existing methods employ spatial\ntransformations on the HSI to achieve alignment with the MSI. However, due to\nthe substantial differences in spatial resolution of the images, the\nperformance of these methods is often unsatisfactory. Moreover, the\nregistration process tends to be time-consuming when dealing with large-sized\nimages in remote sensing. To address these issues, we propose tackling the\nregistration problem from the spectral domain. Initially, a lightweight\nSpectral Prior Learning (SPL) network is developed to extract spectral features\nfrom the HSI and enhance the spectral resolution of the MSI. Following this,\nthe obtained image undergoes spatial downsampling to produce the registered\nHSI. In this process, subspace representation and cyclic training strategy are\nemployed to improve spectral accuracy of the registered HSI obtained. Next, we\npropose a blind sparse fusion (BSF) method, which utilizes group sparsity\nregularization to equivalently promote the low-rankness of the image. This\napproach not only circumvents the need for rank estimation, but also reduces\ncomputational complexity. Then, we employ the Proximal Alternating Optimization\n(PAO) algorithm to solve the BSF model, and present its convergence analysis.\nFinally, extensive numerical experiments on simulated and real datasets are\nconducted to verify the effectiveness of our method in registration and fusion.\nWe also demonstrate its efficacy in enhancing classification performance.", "AI": {"tldr": "This paper proposes a spectral domain approach to address the registration challenge in blind fusion of unregistered hyperspectral and multispectral images. A lightweight Spectral Prior Learning network and a blind sparse fusion method are used. Numerical experiments verify the effectiveness of the method in registration and fusion, and also demonstrate its efficacy in enhancing classification performance.", "motivation": "Existing methods employ spatial transformations on the HSI to achieve alignment with the MSI. However, due to the substantial differences in spatial resolution of the images, the performance of these methods is often unsatisfactory. Moreover, the registration process tends to be time-consuming when dealing with large-sized images in remote sensing.", "method": "A lightweight Spectral Prior Learning (SPL) network is developed to extract spectral features from the HSI and enhance the spectral resolution of the MSI. Following this, the obtained image undergoes spatial downsampling to produce the registered HSI. Next, a blind sparse fusion (BSF) method, which utilizes group sparsity regularization to equivalently promote the low-rankness of the image is proposed. Then, the Proximal Alternating Optimization (PAO) algorithm to solve the BSF model is employed.", "result": "The proposed method tackles the registration problem from the spectral domain. Subspace representation and cyclic training strategy are employed to improve spectral accuracy of the registered HSI obtained. The approach not only circumvents the need for rank estimation, but also reduces computational complexity.", "conclusion": "The effectiveness of the proposed method in registration and fusion is verified through numerical experiments on simulated and real datasets. The efficacy in enhancing classification performance is also demonstrated."}}
{"id": "2506.20598", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.20598", "abs": "https://arxiv.org/abs/2506.20598", "authors": ["Alexander D. Kalian", "Jaewook Lee", "Stefan P. Johannesson", "Lennart Otte", "Christer Hogstrand", "Miao Guo"], "title": "Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges", "comment": null, "summary": "The global demand for sustainable protein sources has accelerated the need\nfor intelligent tools that can rapidly process and synthesise domain-specific\nscientific knowledge. In this study, we present a proof-of-concept multi-agent\nArtificial Intelligence (AI) framework designed to support sustainable protein\nproduction research, with an initial focus on microbial protein sources. Our\nRetrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based\nLLM agents: (1) a literature search agent that retrieves relevant scientific\nliterature on microbial protein production for a specified microbial strain,\nand (2) an information extraction agent that processes the retrieved content to\nextract relevant biological and chemical information. Two parallel\nmethodologies, fine-tuning and prompt engineering, were explored for agent\noptimisation. Both methods demonstrated effectiveness at improving the\nperformance of the information extraction agent in terms of transformer-based\ncosine similarity scores between obtained and ideal outputs. Mean cosine\nsimilarity scores were increased by up to 25%, while universally reaching mean\nscores of $\\geq 0.89$ against ideal output text. Fine-tuning overall improved\nthe mean scores to a greater extent (consistently of $\\geq 0.94$) compared to\nprompt engineering, although lower statistical uncertainties were observed with\nthe latter approach. A user interface was developed and published for enabling\nthe use of the multi-agent AI system, alongside preliminary exploration of\nadditional chemical safety-based search capabilities", "AI": {"tldr": "A multi-agent AI framework with literature search and information extraction agents was developed for sustainable protein production research, focusing on microbial protein sources. Fine-tuning and prompt engineering were used to optimize the agents.", "motivation": "The increasing global demand for sustainable protein sources requires intelligent tools to process and synthesize scientific knowledge rapidly.", "method": "A RAG-oriented system with two GPT-based LLM agents: a literature search agent and an information extraction agent. Agent optimization was explored through fine-tuning and prompt engineering.", "result": "Both fine-tuning and prompt engineering improved the information extraction agent's performance, increasing mean cosine similarity scores by up to 25% and universally reaching scores \u2265 0.89. Fine-tuning consistently achieved mean scores \u2265 0.94.", "conclusion": "Fine-tuning and prompt engineering enhance the information extraction agent's performance, with fine-tuning achieving higher mean cosine similarity scores (\u2265 0.94) compared to prompt engineering, although the latter shows lower statistical uncertainties. A user interface was developed for the multi-agent AI system, and chemical safety search capabilities were explored."}}
{"id": "2506.20031", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20031", "abs": "https://arxiv.org/abs/2506.20031", "authors": ["Prithvi Poddar", "Ehsan Tarkesh Esfahani", "Karthik Dantu", "Souma Chowdhury"], "title": "Automated Generation of Diverse Courses of Actions for Multi-Agent Operations using Binary Optimization and Graph Learning", "comment": null, "summary": "Operations in disaster response, search \\& rescue, and military missions that\ninvolve multiple agents demand automated processes to support the planning of\nthe courses of action (COA). Moreover, traverse-affecting changes in the\nenvironment (rain, snow, blockades, etc.) may impact the expected performance\nof a COA, making it desirable to have a pool of COAs that are diverse in task\ndistributions across agents. Further, variations in agent capabilities, which\ncould be human crews and/or autonomous systems, present practical opportunities\nand computational challenges to the planning process. This paper presents a new\ntheoretical formulation and computational framework to generate such diverse\npools of COAs for operations with soft variations in agent-task compatibility.\nKey to the problem formulation is a graph abstraction of the task space and the\npool of COAs itself to quantify its diversity. Formulating the COAs as a\ncentralized multi-robot task allocation problem, a genetic algorithm is used\nfor (order-ignoring) allocations of tasks to each agent that jointly maximize\ndiversity within the COA pool and overall compatibility of the agent-task\nmappings. A graph neural network is trained using a policy gradient approach to\nthen perform single agent task sequencing in each COA, which maximizes\ncompletion rates adaptive to task features. Our tests of the COA generation\nprocess in a simulated environment demonstrate significant performance gain\nover a random walk baseline, small optimality gap in task sequencing, and\nexecution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task\noperations.", "AI": {"tldr": "This paper presents a new theoretical formulation and computational framework to generate diverse pools of COAs for operations with soft variations in agent-task compatibility. A genetic algorithm is used for allocations of tasks to each agent that jointly maximize diversity within the COA pool and overall compatibility of the agent-task mappings. A graph neural network is trained using a policy gradient approach to then perform single agent task sequencing in each COA, which maximizes completion rates adaptive to task features.", "motivation": "Operations in disaster response, search & rescue, and military missions that involve multiple agents demand automated processes to support the planning of the courses of action (COA). Moreover, traverse-affecting changes in the environment (rain, snow, blockades, etc.) may impact the expected performance of a COA, making it desirable to have a pool of COAs that are diverse in task distributions across agents. Further, variations in agent capabilities, which could be human crews and/or autonomous systems, present practical opportunities and computational challenges to the planning process.", "method": "Formulating the COAs as a centralized multi-robot task allocation problem, a genetic algorithm is used for (order-ignoring) allocations of tasks to each agent that jointly maximize diversity within the COA pool and overall compatibility of the agent-task mappings. A graph neural network is trained using a policy gradient approach to then perform single agent task sequencing in each COA, which maximizes completion rates adaptive to task features.", "result": "significant performance gain over a random walk baseline, small optimality gap in task sequencing, and execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task operations.", "conclusion": "Our tests of the COA generation process in a simulated environment demonstrate significant performance gain over a random walk baseline, small optimality gap in task sequencing, and execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task operations."}}
{"id": "2506.20243", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.20243", "abs": "https://arxiv.org/abs/2506.20243", "authors": ["Papa S\u00e9ga Wade", "Mihai Andries", "Ioannis Kanellos", "Thierry Moudenc"], "title": "CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment", "comment": "5 pages, accepted for presentation at EUSIPCO 2025", "summary": "Automatic fluency assessment (AFA) remains challenging, particularly in\ncapturing speech rhythm, pauses, and disfluencies in non-native speakers. We\nintroduce a chunk-based approach integrating self-supervised learning (SSL)\nmodels (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths\nin phonetic, prosodic, and noisy speech modeling, with a hierarchical\nCNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero\nvoice activity detection (Silero-VAD), enabling fine-grained temporal analysis\nwhile mitigating over-segmentation artifacts. SSL embeddings are fused via a\nlearnable weighted mechanism, balancing acoustic and linguistic features, and\nenriched with chunk-level fluency markers (e.g., speech rate, pause durations,\nn-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies\nacross chunks. Evaluated on Avalinguo and Speechocean762, our approach improves\nF1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines\non Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on\nAvalinguo, surpassing Pyannote.audio-based segmentation baselines. These\nfindings highlight chunk-based multi-SSL fusion for robust fluency evaluation,\nthough future work should explore generalization to dialects with irregular\nprosody.", "AI": {"tldr": "A chunk-based multi-SSL fusion method improves automatic fluency assessment (AFA) for non-native speakers.", "motivation": "Automatic fluency assessment (AFA) remains challenging, particularly in capturing speech rhythm, pauses, and disfluencies in non-native speakers.", "method": "Chunk-based approach integrating self-supervised learning (SSL) models (Wav2Vec2, HuBERT, and WavLM) with a hierarchical CNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero voice activity detection (Silero-VAD).", "result": "Improves F1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines on Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on Avalinguo, surpassing Pyannote.audio-based segmentation baselines.", "conclusion": "Chunk-based multi-SSL fusion improves fluency evaluation, but generalization to dialects with irregular prosody needs further exploration."}}
{"id": "2506.20294", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20294", "abs": "https://arxiv.org/abs/2506.20294", "authors": ["Shunqi Mao", "Wei Guo", "Chaoyi Zhang", "Weidong Cai"], "title": "Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag Explorations", "comment": "10 pages, 3 figures, 2 tables", "summary": "Diffusion models have shown strong performance in conditional generation by\nprogressively denoising Gaussian noise toward a target data distribution. This\ndenoising process can be interpreted as a form of hill climbing in a learned\nlatent space, where the model iteratively refines the sample toward regions of\nhigher probability. However, diffusion models often converge to local optima\nthat are locally visually coherent yet globally inconsistent or conditionally\nmisaligned, due to latent space complexity and suboptimal initialization. Prior\nefforts attempted to address this by strengthening guidance signals or\nmanipulating the initial noise distribution. We introduce Controlled Random\nZigzag Sampling (Ctrl-Z Sampling), a novel sampling strategy designed to detect\nand escape such local maxima during conditional generation. The method first\nidentifies potential local maxima using a reward model. Upon detection, it\ninjects noise and reverts to a previous, noisier state to escape the current\noptimization plateau. The reward model then evaluates candidate trajectories,\naccepting only those that offer improvement, while progressively deeper retreat\nenables stronger escapes when nearby alternatives fail. This controlled random\nzigzag process allows dynamic alternation between forward refinement and\nbackward exploration, enhancing both alignment and visual quality in the\ngenerated outputs. The proposed Ctrl-Z Sampling is model-agnostic and\ncompatible with existing diffusion frameworks. Experimental results show that\nCtrl-Z Sampling substantially improves generation quality with only around 7.6X\nincrease in function evaluations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Ctrl-Z \u91c7\u6837\u7684\u65b0\u91c7\u6837\u7b56\u7565\uff0c\u901a\u8fc7\u5728\u5c40\u90e8\u6700\u5927\u503c\u5904\u6ce8\u5165\u566a\u58f0\u5e76\u56de\u9000\u5230\u4e4b\u524d\u7684\u72b6\u6001\u6765\u9003\u79bb\u5c40\u90e8\u6700\u4f18\uff0c\u4ece\u800c\u63d0\u9ad8\u6761\u4ef6\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u6761\u4ef6\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u9010\u6b65\u53bb\u566a\u9ad8\u65af\u566a\u58f0\u6765\u5b9e\u73b0\u76ee\u6807\u6570\u636e\u5206\u5e03\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6f5c\u5728\u7a7a\u95f4\u7684\u590d\u6742\u6027\u548c\u6b21\u4f18\u521d\u59cb\u5316\uff0c\u6269\u6563\u6a21\u578b\u901a\u5e38\u4f1a\u6536\u655b\u5230\u5c40\u90e8\u6700\u4f18\uff0c\u8fd9\u4e9b\u5c40\u90e8\u6700\u4f18\u5728\u5c40\u90e8\u89c6\u89c9\u4e0a\u662f\u8fde\u8d2f\u7684\uff0c\u4f46\u5728\u5168\u5c40\u4e0a\u662f\u4e0d\u4e00\u81f4\u7684\u6216\u6761\u4ef6\u9519\u4f4d\u7684\u3002", "method": "\u5f15\u5165\u4e86\u53d7\u63a7\u968f\u673a Zigzag \u91c7\u6837 (Ctrl-Z \u91c7\u6837)\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u91c7\u6837\u7b56\u7565\uff0c\u65e8\u5728\u68c0\u6d4b\u548c\u9003\u79bb\u6761\u4ef6\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u5c40\u90e8\u6700\u5927\u503c\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u4f7f\u7528\u5956\u52b1\u6a21\u578b\u8bc6\u522b\u6f5c\u5728\u7684\u5c40\u90e8\u6700\u5927\u503c\u3002\u68c0\u6d4b\u5230\u5c40\u90e8\u6700\u5927\u503c\u540e\uff0c\u5b83\u4f1a\u6ce8\u5165\u566a\u58f0\u5e76\u6062\u590d\u5230\u4e4b\u524d\u7684\u3001\u566a\u58f0\u66f4\u5927\u7684\u72b6\u6001\uff0c\u4ee5\u9003\u79bb\u5f53\u524d\u7684\u4f18\u5316\u5e73\u53f0\u3002\u7136\u540e\uff0c\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u5019\u9009\u8f68\u8ff9\uff0c\u4ec5\u63a5\u53d7\u90a3\u4e9b\u63d0\u4f9b\u6539\u8fdb\u7684\u8f68\u8ff9\uff0c\u800c\u9010\u6e10\u52a0\u6df1\u7684\u64a4\u9000\u53ef\u4ee5\u5728\u9644\u8fd1\u7684\u66ff\u4ee3\u65b9\u6848\u5931\u8d25\u65f6\u5b9e\u73b0\u66f4\u5f3a\u7684\u9003\u9038\u3002", "result": "Ctrl-Z Sampling \u662f\u4e00\u79cd\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u4e0e\u73b0\u6709\u7684\u6269\u6563\u6846\u67b6\u517c\u5bb9\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e", "conclusion": "Ctrl-Z Sampling \u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u4e14\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\u4ec5\u589e\u52a0\u7ea6 7.6 \u500d\u3002"}}
{"id": "2506.20600", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20600", "abs": "https://arxiv.org/abs/2506.20600", "authors": ["Wengxi Li", "Roy Pea", "Nick Haber", "Hari Subramonyam"], "title": "CogGen: A Learner-Centered Generative AI Architecture for Intelligent Tutoring with Programming Video", "comment": null, "summary": "We introduce CogGen, a learner-centered AI architecture that transforms\nprogramming videos into interactive, adaptive learning experiences by\nintegrating student modeling with generative AI tutoring based on the Cognitive\nApprenticeship framework. The architecture consists of three components: (1)\nvideo segmentation by learning goals, (2) a conversational tutoring engine\napplying Cognitive Apprenticeship strategies, and (3) a student model using\nBayesian Knowledge Tracing to adapt instruction. Our technical evaluation\ndemonstrates effective video segmentation accuracy and strong pedagogical\nalignment across knowledge, method, action, and interaction layers. Ablation\nstudies confirm the necessity of each component in generating effective\nguidance. This work advances AI-powered tutoring by bridging structured student\nmodeling with interactive AI conversations, offering a scalable approach to\nenhancing video-based programming education.", "AI": {"tldr": "CogGen: Learner-centered AI architecture transforms programming videos into interactive, adaptive learning experiences.", "motivation": "transform programming videos into interactive, adaptive learning experiences", "method": "a learner-centered AI architecture that transforms programming videos into interactive, adaptive learning experiences by integrating student modeling with generative AI tutoring based on the Cognitive Apprenticeship framework. The architecture consists of three components: (1) video segmentation by learning goals, (2) a conversational tutoring engine applying Cognitive Apprenticeship strategies, and (3) a student model using Bayesian Knowledge Tracing to adapt instruction.", "result": "demonstrates effective video segmentation accuracy and strong pedagogical alignment across knowledge, method, action, and interaction layers. Ablation studies confirm the necessity of each component in generating effective guidance.", "conclusion": "This work advances AI-powered tutoring by bridging structured student modeling with interactive AI conversations, offering a scalable approach to enhancing video-based programming education."}}
{"id": "2506.20037", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.20037", "abs": "https://arxiv.org/abs/2506.20037", "authors": ["Mohammad M Maheri", "Alex Davidson", "Hamed Haddadi"], "title": "Verifiable Unlearning on Edge", "comment": "This paper has been accepted to the IEEE European Symposium on\n  Security and Privacy (EuroS&P) 2025", "summary": "Machine learning providers commonly distribute global models to edge devices,\nwhich subsequently personalize these models using local data. However, issues\nsuch as copyright infringements, biases, or regulatory requirements may require\nthe verifiable removal of certain data samples across all edge devices.\nEnsuring that edge devices correctly execute such unlearning operations is\ncritical to maintaining integrity.\n  In this work, we introduce a verification framework leveraging zero-knowledge\nproofs, specifically zk-SNARKs, to confirm data unlearning on personalized\nedge-device models without compromising privacy. We have developed algorithms\nexplicitly designed to facilitate unlearning operations that are compatible\nwith efficient zk-SNARK proof generation, ensuring minimal computational and\nmemory overhead suitable for constrained edge environments. Furthermore, our\napproach carefully preserves personalized enhancements on edge devices,\nmaintaining model performance post-unlearning.\n  Our results affirm the practicality and effectiveness of this verification\nframework, demonstrating verifiable unlearning with minimal degradation in\npersonalization-induced performance improvements. Our methodology ensures\nverifiable, privacy-preserving, and effective machine unlearning across edge\ndevices.", "AI": {"tldr": "This paper presents a zk-SNARKs based verification framework for verifiable and privacy-preserving machine unlearning on edge devices, with minimal impact on model performance.", "motivation": "The motivation is to address the need for verifiable removal of data samples across edge devices due to copyright, bias, or regulatory issues.", "method": "The authors introduce a verification framework using zero-knowledge proofs (zk-SNARKs) and develop algorithms compatible with efficient zk-SNARK proof generation.", "result": "The results affirm the practicality and effectiveness of the framework, showing verifiable unlearning with minimal degradation in personalization-induced performance improvements.", "conclusion": "This paper demonstrates a practical and effective verification framework for machine unlearning on edge devices, ensuring minimal performance degradation."}}
{"id": "2506.20269", "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2506.20269", "abs": "https://arxiv.org/abs/2506.20269", "authors": ["Kai-Robin Lange", "Tobias Schmidt", "Matthias Reccius", "Henrik M\u00fcller", "Michael Roos", "Carsten Jentsch"], "title": "Narrative Shift Detection: A Hybrid Approach of Dynamic Topic Models and Large Language Models", "comment": "14 pages, 1 figure", "summary": "With rapidly evolving media narratives, it has become increasingly critical\nto not just extract narratives from a given corpus but rather investigate, how\nthey develop over time. While popular narrative extraction methods such as\nLarge Language Models do well in capturing typical narrative elements or even\nthe complex structure of a narrative, applying them to an entire corpus comes\nwith obstacles, such as a high financial or computational cost. We propose a\ncombination of the language understanding capabilities of Large Language Models\nwith the large scale applicability of topic models to dynamically model\nnarrative shifts across time using the Narrative Policy Framework. We apply a\ntopic model and a corresponding change point detection method to find changes\nthat concern a specific topic of interest. Using this model, we filter our\ncorpus for documents that are particularly representative of that change and\nfeed them into a Large Language Model that interprets the change that happened\nin an automated fashion and distinguishes between content and narrative shifts.\nWe employ our pipeline on a corpus of The Wall Street Journal news paper\narticles from 2009 to 2023. Our findings indicate that a Large Language Model\ncan efficiently extract a narrative shift if one exists at a given point in\ntime, but does not perform as well when having to decide whether a shift in\ncontent or a narrative shift took place.", "AI": {"tldr": "This paper proposes a combination of Large Language Models and topic models to dynamically model narrative shifts across time. The findings indicate that a Large Language Model can efficiently extract a narrative shift if one exists at a given point in time, but does not perform as well when having to decide whether a shift in content or a narrative shift took place.", "motivation": "With rapidly evolving media narratives, it has become increasingly critical to not just extract narratives from a given corpus but rather investigate, how they develop over time. While popular narrative extraction methods such as Large Language Models do well in capturing typical narrative elements or even the complex structure of a narrative, applying them to an entire corpus comes with obstacles, such as a high financial or computational cost.", "method": "combination of the language understanding capabilities of Large Language Models with the large scale applicability of topic models to dynamically model narrative shifts across time using the Narrative Policy Framework. We apply a topic model and a corresponding change point detection method to find changes that concern a specific topic of interest. Using this model, we filter our corpus for documents that are particularly representative of that change and feed them into a Large Language Model that interprets the change that happened in an automated fashion and distinguishes between content and narrative shifts.", "result": "Our findings indicate that a Large Language Model can efficiently extract a narrative shift if one exists at a given point in time, but does not perform as well when having to decide whether a shift in content or a narrative shift took place.", "conclusion": "A Large Language Model can efficiently extract a narrative shift if one exists at a given point in time, but does not perform as well when having to decide whether a shift in content or a narrative shift took place."}}
{"id": "2506.20302", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20302", "abs": "https://arxiv.org/abs/2506.20302", "authors": ["Abbas Anwar", "Mohammad Shullar", "Ali Arshad Nasir", "Mudassir Masood", "Saeed Anwar"], "title": "TDiR: Transformer based Diffusion for Image Restoration Tasks", "comment": null, "summary": "Images captured in challenging environments often experience various forms of\ndegradation, including noise, color cast, blur, and light scattering. These\neffects significantly reduce image quality, hindering their applicability in\ndownstream tasks such as object detection, mapping, and classification. Our\ntransformer-based diffusion model was developed to address image restoration\ntasks, aiming to improve the quality of degraded images. This model was\nevaluated against existing deep learning methodologies across multiple quality\nmetrics for underwater image enhancement, denoising, and deraining on publicly\navailable datasets. Our findings demonstrate that the diffusion model, combined\nwith transformers, surpasses current methods in performance. The results of our\nmodel highlight the efficacy of diffusion models and transformers in improving\nthe quality of degraded images, consequently expanding their utility in\ndownstream tasks that require high-fidelity visual data.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8etransformer\u7684\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u56fe\u50cf\u6062\u590d\u4efb\u52a1\uff0c\u65e8\u5728\u63d0\u9ad8\u9000\u5316\u56fe\u50cf\u7684\u8d28\u91cf\u3002", "motivation": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u73af\u5883\u4e2d\u62cd\u6444\u7684\u56fe\u50cf\u901a\u5e38\u4f1a\u7ecf\u5386\u5404\u79cd\u5f62\u5f0f\u7684\u9000\u5316\uff0c\u5305\u62ec\u566a\u58f0\u3001\u8272\u5f69\u5931\u771f\u3001\u6a21\u7cca\u548c\u5149\u6563\u5c04\u3002\u8fd9\u4e9b\u5f71\u54cd\u663e\u8457\u964d\u4f4e\u4e86\u56fe\u50cf\u8d28\u91cf\uff0c\u963b\u788d\u4e86\u5b83\u4eec\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u76ee\u6807\u68c0\u6d4b\u3001\u6620\u5c04\u548c\u5206\u7c7b\uff09\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "transformer-based diffusion\u6a21\u578b", "result": "\u8be5\u6269\u6563\u6a21\u578b\u7ed3\u5408transformer\uff0c\u5728\u6027\u80fd\u4e0a\u8d85\u8fc7\u4e86\u5f53\u524d\u7684\u65b9\u6cd5\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u548ctransformer\u7684\u7ed3\u5408\u5728\u63d0\u9ad8\u9000\u5316\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u975e\u5e38\u6709\u6548\uff0c\u4ece\u800c\u6269\u5927\u4e86\u5b83\u4eec\u5728\u9700\u8981\u9ad8\u4fdd\u771f\u89c6\u89c9\u6570\u636e\u7684\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6548\u7528\u3002"}}
{"id": "2506.20608", "categories": ["cs.AI", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2506.20608", "abs": "https://arxiv.org/abs/2506.20608", "authors": ["Barry Smith", "Junchao Zhang", "Hong Zhang", "Lois Curfman McInnes", "Murat Keceli", "Archit Vasan", "Satish Balay", "Toby Isaac", "Le Chen", "Venkatram Vishwanath"], "title": "AI Assistants to Enhance and Exploit the PETSc Knowledge Base", "comment": null, "summary": "Generative AI, especially through large language models (LLMs), is\ntransforming how technical knowledge can be accessed, reused, and extended.\nPETSc, a widely used numerical library for high-performance scientific\ncomputing, has accumulated a rich but fragmented knowledge base over its three\ndecades of development, spanning source code, documentation, mailing lists,\nGitLab issues, Discord conversations, technical papers, and more. Much of this\nknowledge remains informal and inaccessible to users and new developers. To\nactivate and utilize this knowledge base more effectively, the PETSc team has\nbegun building an LLM-powered system that combines PETSc content with custom\nLLM tools -- including retrieval-augmented generation (RAG), reranking\nalgorithms, and chatbots -- to assist users, support developers, and propose\nupdates to formal documentation. This paper presents initial experiences\ndesigning and evaluating these tools, focusing on system architecture, using\nRAG and reranking for PETSc-specific information, evaluation methodologies for\nvarious LLMs and embedding models, and user interface design. Leveraging the\nArgonne Leadership Computing Facility resources, we analyze how LLM responses\ncan enhance the development and use of numerical software, with an initial\nfocus on scalable Krylov solvers. Our goal is to establish an extensible\nframework for knowledge-centered AI in scientific software, enabling scalable\nsupport, enriched documentation, and enhanced workflows for research and\ndevelopment. We conclude by outlining directions for expanding this system into\na robust, evolving platform that advances software ecosystems to accelerate\nscientific discovery.", "AI": {"tldr": "The PETSc team is building an LLM-powered system to improve access to and utilization of their knowledge base, aiming to enhance software development and scientific discovery.", "motivation": "Much of PETSc's knowledge remains informal and inaccessible to users and new developers.", "method": "The PETSc team is building an LLM-powered system that combines PETSc content with custom LLM tools, including retrieval-augmented generation (RAG), reranking algorithms, and chatbots.", "result": "The paper presents initial experiences designing and evaluating these tools, focusing on system architecture, using RAG and reranking for PETSc-specific information, evaluation methodologies, and user interface design. LLM responses can enhance the development and use of numerical software.", "conclusion": "The paper outlines directions for expanding the LLM-powered system into a robust platform for advancing software ecosystems and accelerating scientific discovery."}}
