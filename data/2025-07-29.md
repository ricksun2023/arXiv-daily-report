<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Advancing Mental Disorder Detection: A Comparative Evaluation of Transformer and LSTM Architectures on Social Media](https://arxiv.org/abs/2507.19511)
*Khalid Hasan,Jamil Saquer,Mukulika Ghosh*

Main category: cs.CL

TL;DR: Transformer models excel in mental health disorder classification, with RoBERTa leading, but LSTM with BERT embeddings provides a good balance of performance and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The rising prevalence of mental health disorders necessitates automated tools for early detection and monitoring.

Method: Comprehensive evaluation of transformer models (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against LSTM using different text embedding techniques on a large annotated Reddit dataset.

Result: RoBERTa achieved the highest classification performance (99.54% F1 score on the hold-out test set and 96.05% F1 score on the external test set), while LSTM models with BERT embeddings achieved F1 scores exceeding 94% on the external dataset.

Conclusion: Transformer-based models, especially RoBERTa, are effective for mental health monitoring, but LSTM models with BERT embeddings offer a competitive, less computationally intensive alternative.

Abstract: The rising prevalence of mental health disorders necessitates the development
of robust, automated tools for early detection and monitoring. Recent advances
in Natural Language Processing (NLP), particularly transformer-based
architectures, have demonstrated significant potential in text analysis. This
study provides a comprehensive evaluation of state-of-the-art transformer
models (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against Long Short-Term
Memory (LSTM) based approaches using different text embedding techniques for
mental health disorder classification on Reddit. We construct a large annotated
dataset, validating its reliability through statistical judgmental analysis and
topic modeling. Experimental results demonstrate the superior performance of
transformer models over traditional deep-learning approaches. RoBERTa achieved
the highest classification performance, with a 99.54% F1 score on the hold-out
test set and a 96.05% F1 score on the external test set. Notably, LSTM models
augmented with BERT embeddings proved highly competitive, achieving F1 scores
exceeding 94% on the external dataset while requiring significantly fewer
computational resources. These findings highlight the effectiveness of
transformer-based models for real-time, scalable mental health monitoring. We
discuss the implications for clinical applications and digital mental health
interventions, offering insights into the capabilities and limitations of
state-of-the-art NLP methodologies in mental disorder detection.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [Tuning adaptive gamma correction (TAGC) for enhancing images in low ligh](https://arxiv.org/abs/2507.19574)
*Ghufran Abualhail Alhamzawi,Ali Saeed Alfoudi,Ali Hakem Alsaeedi,Suha Mohammed Hadi,Amjed Abbas Ahmed,Md. Riad Hassan,Nurhizam Safie Mohd Satar,Waeel Yahya Yasseen*

Main category: cs.CV

TL;DR: This paper presents a tuning adaptive gamma correction (TAGC) model for enhancing low-light images, which calculates the adaptive gamma coefficient automatically. The model effectively improves low-light images while maintaining details, natural contrast, and correct color distribution.


<details>
  <summary>Details</summary>
Motivation: Enhancing images in low-light conditions is an important challenge in computer vision. Insufficient illumination negatively affects the quality of images, resulting in low contrast, intensive noise, and blurred details.

Method: tuning adaptive gamma correction (TAGC). The model is based on analyzing the color luminance of the low-light image and calculating the average color to determine the adaptive gamma coefficient. The gamma value is calculated automatically and adaptively at different illumination levels suitable for the image without human intervention or manual adjustment.

Result: tuning adaptive gamma correction model has effectively improved low-light images while maintaining details, natural contrast, and correct color distribution. It also provides natural visual quality.

Conclusion: tuning adaptive gamma correction model has effectively improved low-light images while maintaining details, natural contrast, and correct color distribution. It also provides natural visual quality. It can be considered a more efficient solution for processing low-light images in multiple applications such as night surveillance, improving the quality of medical images, and photography in low-light environments.

Abstract: Enhancing images in low-light conditions is an important challenge in
computer vision. Insufficient illumination negatively affects the quality of
images, resulting in low contrast, intensive noise, and blurred details. This
paper presents a model for enhancing low-light images called tuning adaptive
gamma correction (TAGC). The model is based on analyzing the color luminance of
the low-light image and calculating the average color to determine the adaptive
gamma coefficient. The gamma value is calculated automatically and adaptively
at different illumination levels suitable for the image without human
intervention or manual adjustment. Based on qualitative and quantitative
evaluation, tuning adaptive gamma correction model has effectively improved
low-light images while maintaining details, natural contrast, and correct color
distribution. It also provides natural visual quality. It can be considered a
more efficient solution for processing low-light images in multiple
applications such as night surveillance, improving the quality of medical
images, and photography in low-light environments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation](https://arxiv.org/abs/2507.19489)
*Simone Bendazzoli,Sanna Persson,Mehdi Astaraki,Sebastian Pettersson,Vitali Grozman,Rodrigo Moreno*

Main category: cs.AI

TL;DR: MAIA 是一个开源平台，旨在促进临床医生、研究人员和 AI 开发人员之间的跨学科协作，加速人工智能研究转化为有影响力的临床解决方案。


<details>
  <summary>Details</summary>
Motivation: 人工智能 (AI) 集成到临床工作流程中需要强大的协作平台，这些平台能够弥合技术创新和实际医疗保健应用之间的差距。

Method: 介绍了 MAIA（医疗人工智能助手），这是一个旨在促进临床医生、研究人员和人工智能开发人员之间跨学科协作的开源平台。MAIA 构建于 Kubernetes 之上，提供了一个模块化、可扩展的环境，其中集成了用于数据管理、模型开发、注释、部署和临床反馈的工具。

Result: MAIA 具有项目隔离、CI/CD 自动化、与高性能计算基础设施和临床工作流程集成的关键特性。MAIA 支持医学影像人工智能中的实际用例，并在学术和临床环境中进行部署。

Conclusion: MAIA通过促进协作和互操作性，旨在加速人工智能研究转化为有影响力的临床解决方案，同时促进可重复性、透明度和以用户为中心的设计。我们在 KTH 皇家理工学院和 Karolinska 大学医院的不同项目中展示了 MAIA 的使用。

Abstract: The integration of Artificial Intelligence (AI) into clinical workflows
requires robust collaborative platforms that are able to bridge the gap between
technical innovation and practical healthcare applications. This paper
introduces MAIA (Medical Artificial Intelligence Assistant), an open-source
platform designed to facilitate interdisciplinary collaboration among
clinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a
modular, scalable environment with integrated tools for data management, model
development, annotation, deployment, and clinical feedback. Key features
include project isolation, CI/CD automation, integration with high-computing
infrastructures and in clinical workflows. MAIA supports real-world use cases
in medical imaging AI, with deployments in both academic and clinical
environments. By promoting collaborations and interoperability, MAIA aims to
accelerate the translation of AI research into impactful clinical solutions
while promoting reproducibility, transparency, and user-centered design. We
showcase the use of MAIA with different projects, both at KTH Royal Institute
of Technology and Karolinska University Hospital.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [4] [CleANN: Efficient Full Dynamism in Graph-based Approximate Nearest Neighbor Search](https://arxiv.org/abs/2507.19802)
*Ziyu Zhang,Yuanhao Wei,Joshua Engels,Julian Shun*

Main category: cs.DB

TL;DR: CleANN is a concurrent ANNS index that achieves high efficiency and maintains quality under full dynamism by using workload-aware linking, query-adaptive neighborhood consolidation, and semi-lazy memory cleaning.


<details>
  <summary>Details</summary>
Motivation: Existing dynamic graph-based indexes suffer from query quality degradation and expensive graph structure updates.

Method: The CleANN system consists of three main components: (1) workload-aware linking of diverse search tree descendants to combat distribution shift; (2)query-adaptive on-the-fly neighborhood consolidation to efficiently handle deleted nodes; and (3) semi-lazy memory cleaning to clean up stale information in the data structure and reduce the work spent by the first two components.

Result: CleANN has query quality at least as good as if the index had been built statically using the corresponding data. CleANN achieves 7-1200x throughput improvement on million-scale real-world datasets.

Conclusion: CleANN is the first concurrent ANNS index to achieve high efficiency while maintaining quality under full dynamism.

Abstract: Approximate nearest neighbor search (ANNS) has become a quintessential
algorithmic problem for various other foundational data tasks for AI workloads.
Graph-based ANNS indexes have superb empirical trade-offs in indexing cost,
query efficiency, and query approximation quality. Most existing graph-based
indexes are designed for the static scenario, where there are no updates to the
data after the index is constructed. However, full dynamism (insertions,
deletions, and searches) is crucial to providing up-to-date responses in
applications using vector databases. It is desirable that the index efficiently
supports updates and search queries concurrently. Existing dynamic graph-based
indexes suffer from at least one of the following problems: (1) the query
quality degrades as updates happen; and (2) the graph structure updates used to
maintain the index quality upon updates are global and thus expensive. To solve
these problems, we propose the CleANN system which consists of three main
components: (1) workload-aware linking of diverse search tree descendants to
combat distribution shift; (2)query-adaptive on-the-fly neighborhood
consolidation to efficiently handle deleted nodes; and (3) semi-lazy memory
cleaning to clean up stale information in the data structure and reduce the
work spent by the first two components. We evaluate CleANN on 7 diverse
datasets on fully dynamic workloads and find that CleANN has query quality at
least as good as if the index had been built statically using the corresponding
data. In the in-memory setting using 56 hyper-threads, with all types of
queries running concurrently, at the same recall level, CleANN achieves 7-1200x
throughput improvement on million-scale real-world datasets. To the best of our
knowledge, CleANN is the first concurrent ANNS index to achieve such efficiency
while maintaining quality under full dynamism.

</details>


### [5] [TIMEST: Temporal Information Motif Estimator Using Sampling Trees](https://arxiv.org/abs/2507.20441)
*Yunjie Pan,Omkar Bhalerao,C. Seshadhri,Nishil Talati*

Main category: cs.DB

TL;DR: TIMEST是一种通用、快速且准确的估计算法，用于计算时间网络中任意大小的时间主题。


<details>
  <summary>Details</summary>
Motivation: 现实世界网络中的边通常具有时间戳，因此需要时间主题挖掘。时间主题是一种更丰富的结构，它对主题的边施加时间约束。时间主题已被用于分析社交网络、金融交易和生物网络。时间图中的主题计数特别具有挑战性。具有数百万条边的图可能具有数万亿个时间主题，因为同一条边可能以多个时间戳出现。可能性存在组合爆炸，并且最先进的算法无法管理具有四个以上顶点的主题。

Method: 提出了一种时间跨越树采样器，该采样器利用加权采样来生成目标时间主题的子结构。该方法仔细地选取了主题的一个时间约束子集，这些约束可以被联合和有效地采样。TIMEST使用随机估计技术来获得精确的主题计数估计。

Result: TIMEST算法在时间和精度上都优于以前的算法。我们的CPU实现比最先进的精确算法的GPU实现平均快28倍，比SOTA近似算法快6倍，同时在大多数情况下始终显示小于5%的误差。

Conclusion: TIMEST算法在时间和精度上都优于以前的算法。与最先进的精确算法的GPU实现相比，我们的CPU实现平均加速了28倍，与SOTA近似算法相比，加速了6倍，同时在大多数情况下始终显示小于5%的误差。

Abstract: The mining of pattern subgraphs, known as motifs, is a core task in the field
of graph mining. Edges in real-world networks often have timestamps, so there
is a need for temporal motif mining. A temporal motif is a richer structure
that imposes timing constraints on the edges of the motif. Temporal motifs have
been used to analyze social networks, financial transactions, and biological
networks.
  Motif counting in temporal graphs is particularly challenging. A graph with
millions of edges can have trillions of temporal motifs, since the same edge
can occur with multiple timestamps. There is a combinatorial explosion of
possibilities, and state-of-the-art algorithms cannot manage motifs with more
than four vertices.
  In this work, we present TIMEST: a general, fast, and accurate estimation
algorithm to count temporal motifs of arbitrary sizes in temporal networks. Our
approach introduces a temporal spanning tree sampler that leverages weighted
sampling to generate substructures of target temporal motifs. This method
carefully takes a subset of temporal constraints of the motif that can be
jointly and efficiently sampled. TIMEST uses randomized estimation techniques
to obtain accurate estimates of motif counts.
  We give theoretical guarantees on the running time and approximation
guarantees of TIMEST. We perform an extensive experimental evaluation and show
that TIMEST is both faster and more accurate than previous algorithms. Our CPU
implementation exhibits an average speedup of 28x over state-of-the-art GPU
implementation of the exact algorithm, and 6x speedup over SOTA approximate
algorithms while consistently showcasing less than 5% error in most cases. For
example, TIMEST can count the number of instances of a financial fraud temporal
motif in four minutes with 0.6% error, while exact methods take more than two
days.

</details>


### [6] [A Functional Data Model and Query Language is All You Need](https://arxiv.org/abs/2507.20671)
*Jens Dittrich*

Main category: cs.DB

TL;DR: FDM and FQL are proposed as a modern QL that solves problems of SQL, are more expressive, and integrate smoothly into existing programming languages.


<details>
  <summary>Details</summary>
Motivation: to come up with a modern QL that solves problems of SQL (NULL-values, impedance mismatch, SQL injection, missing querying capabilities for updates, etc.)

Method: propose the vision of a functional data model (FDM) and an associated functional query language (FQL)

Result: FDM and FQL are much more expressive than the relational model and SQL. QL and PL become the "same thing", thus opening up some interesting holistic optimization opportunities between compilers and databases

Conclusion: FQL integrates smoothly into existing programming languages and allows developers to stick with their familiar abstractions.

Abstract: We propose the vision of a functional data model (FDM) and an associated
functional query language (FQL). Our proposal has far-reaching consequences: we
show a path to come up with a modern QL that solves (almost if not) all
problems of SQL (NULL-values, impedance mismatch, SQL injection, missing
querying capabilities for updates, etc.). FDM and FQL are much more expressive
than the relational model and SQL. In addition, in contrast to SQL, FQL
integrates smoothly into existing programming languages. In our approach both
QL and PL become the "same thing", thus opening up some interesting holistic
optimization opportunities between compilers and databases. In FQL, we also do
not need to force application developers to switch to unfamiliar programming
paradigms (like SQL or datalog): developers can stick with the abstractions
provided by their programming language.

</details>


### [7] [MVIAnalyzer: A Holistic Approach to Analyze Missing Value Imputation](https://arxiv.org/abs/2507.20815)
*Valerie Restat,Kai Tejkl,Uta Störl*

Main category: cs.DB

TL;DR: This paper introduces MVIAnalyzer, a framework for analyzing missing value imputation methods, and demonstrates its application on various datasets, highlighting the possibilities and limitations of different MVI methods.


<details>
  <summary>Details</summary>
Motivation: Missing values often limit the usage of data analysis or cause falsification of results. Therefore, methods of missing value imputation (MVI) are of great significance. However, in general, there is no universal, fair MVI method for different tasks. This work thus places MVI in the overall context of data analysis.

Method: We present the MVIAnalyzer, a generic framework for a holistic analysis of MVI. It considers the overall process up to the application and analysis of machine learning methods. The associated software is provided and can be used by other researchers for their own analyses. To this end, it further includes a missing value simulation with consideration of relevant parameters. The application of the MVIAnalyzer is demonstrated on data with different characteristics.

Result: An evaluation of the results shows the possibilities and limitations of different MVI methods.

Conclusion: An evaluation of the results shows the possibilities and limitations of different MVI methods. Since MVI is a very complex topic with different influencing variables, this paper additionally illustrates how the analysis can be supported by visualizations.

Abstract: Missing values often limit the usage of data analysis or cause falsification
of results. Therefore, methods of missing value imputation (MVI) are of great
significance. However, in general, there is no universal, fair MVI method for
different tasks. This work thus places MVI in the overall context of data
analysis. For this purpose, we present the MVIAnalyzer, a generic framework for
a holistic analysis of MVI. It considers the overall process up to the
application and analysis of machine learning methods. The associated software
is provided and can be used by other researchers for their own analyses. To
this end, it further includes a missing value simulation with consideration of
relevant parameters. The application of the MVIAnalyzer is demonstrated on data
with different characteristics. An evaluation of the results shows the
possibilities and limitations of different MVI methods. Since MVI is a very
complex topic with different influencing variables, this paper additionally
illustrates how the analysis can be supported by visualizations.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [8] [A Unified Framework for Interactive Visual Graph Matching via Attribute-Structure Synchronization](https://arxiv.org/abs/2507.19750)
*Yuhua Liu,Haoxuan Wang,Jiajia Kou,Ling Sun,Heyu Wang,Yongheng Wang,Yigang Wang,Jinchang Lic,Zhiguang Zhou*

Main category: cs.IR

TL;DR: a novel framework for interactive visual graph matching


<details>
  <summary>Details</summary>
Motivation: it is crucial for graph retrieval tools to make full use of the attribute information in addition to structural information.

Method: an attribute-structure synchronization method is developed for representing structural and attribute features in a unified embedding space based on Canonical Correlation Analysis (CCA).

Result: provides users with intuitive visual query interfaces for traversing, filtering and searching for the target graph in the embedding space conveniently. With the designed interfaces, the users can also specify a new target graph with desired structural and semantic features. Besides, evaluation views are designed for easy validation and interpretation of the matching results.

Conclusion: Case studies and quantitative comparisons on real-world datasets have demonstrated the superiorities of our proposed framework in graph matching and large graph exploration.

Abstract: In traditional graph retrieval tools, graph matching is commonly used to
retrieve desired graphs from extensive graph datasets according to their
structural similarities. However, in real applications, graph nodes have
numerous attributes which also contain valuable information for evaluating
similarities between graphs. Thus, to achieve superior graph matching results,
it is crucial for graph retrieval tools to make full use of the attribute
information in addition to structural information. We propose a novel framework
for interactive visual graph matching. In the proposed framework, an
attribute-structure synchronization method is developed for representing
structural and attribute features in a unified embedding space based on
Canonical Correlation Analysis (CCA). To support fast and interactive matching,
\revise{our method} provides users with intuitive visual query interfaces for
traversing, filtering and searching for the target graph in the embedding space
conveniently. With the designed interfaces, the users can also specify a new
target graph with desired structural and semantic features. Besides, evaluation
views are designed for easy validation and interpretation of the matching
results. Case studies and quantitative comparisons on real-world datasets have
demonstrated the superiorities of our proposed framework in graph matching and
large graph exploration.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [Beyond 9-to-5: A Generative Model for Augmenting Mobility Data of Underrepresented Shift Workers](https://arxiv.org/abs/2507.19510)
*Haoxuan Ma,Xishun Liao,Yifan Liu,Chris Stanford,Jiaqi Ma*

Main category: cs.LG

TL;DR: 本文提出了一种基于transformer的方法，利用GPS数据生成轮班工人的完整活动模式，以解决传统交通数据集中对他们的低估问题，从而实现精确和包容的交通规划。


<details>
  <summary>Details</summary>
Motivation: 本文关注城市交通建模中的一个关键差距，即轮班工人。轮班工人占工业化社会劳动力的15-20%，但在传统的交通调查和规划中却被系统性地低估了。

Method: 我们引入了一种新颖的基于transformer的方法，该方法利用碎片化的GPS轨迹数据来生成行为上有效的非标准工作时间个体的完整活动模式。我们的方法采用了周期感知时间嵌入和以转换为中心的损失函数，专门用于捕获轮班工人的独特活动节奏，并减轻传统交通数据集中固有的偏差。

Result: 结果表明，生成的数据与洛杉矶县的GPS数据实现了显著的分布对齐（所有评估指标的平均JSD < 0.02）。

Conclusion: 该研究通过将不完整的GPS轨迹转换为完整的、有代表性的活动模式，为交通规划者提供了一个强大的数据增强工具，以填补在理解城市人口全天候出行需求方面的关键空白，从而实现精确和包容的交通规划。

Abstract: This paper addresses a critical gap in urban mobility modeling by focusing on
shift workers, a population segment comprising 15-20% of the workforce in
industrialized societies yet systematically underrepresented in traditional
transportation surveys and planning. This underrepresentation is revealed in
this study by a comparative analysis of GPS and survey data, highlighting stark
differences between the bimodal temporal patterns of shift workers and the
conventional 9-to-5 schedules recorded in surveys. To address this bias, we
introduce a novel transformer-based approach that leverages fragmented GPS
trajectory data to generate complete, behaviorally valid activity patterns for
individuals working non-standard hours. Our method employs periodaware temporal
embeddings and a transition-focused loss function specifically designed to
capture the unique activity rhythms of shift workers and mitigate the inherent
biases in conventional transportation datasets. Evaluation shows that the
generated data achieves remarkable distributional alignment with GPS data from
Los Angeles County (Average JSD < 0.02 for all evaluation metrics). By
transforming incomplete GPS traces into complete, representative activity
patterns, our approach provides transportation planners with a powerful data
augmentation tool to fill critical gaps in understanding the 24/7 mobility
needs of urban populations, enabling precise and inclusive transportation
planning.

</details>
