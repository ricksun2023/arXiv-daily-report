<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 54]
- [cs.CV](#cs.CV) [Total: 52]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 52]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Contextual Augmentation for Entity Linking using Large Language Models](https://arxiv.org/abs/2510.18888)
*Daniel Vollmers,Hamada M. Zahera,Diego Moussallem,Axel-Cyrille Ngonga Ngomo*

Main category: cs.CL

TL;DR: 提出了一种用于实体链接的微调模型，该模型在统一的框架中共同整合了实体识别和消歧。


<details>
  <summary>Details</summary>
Motivation: 传统的实体链接方法使用两步过程，分别使用单独的模型进行实体识别和消歧，这种方法计算密集且效果较差。

Method: 利用大型语言模型来丰富实体提及的上下文，从而在实体消歧方面产生更好的性能。

Result: 在基准数据集上评估了该方法，结果表明该方法在跨域数据集上取得了最先进的性能。

Conclusion: 该方法在跨域数据集上取得了最先进的性能。

Abstract: Entity Linking involves detecting and linking entity mentions in natural
language texts to a knowledge graph. Traditional methods use a two-step process
with separate models for entity recognition and disambiguation, which can be
computationally intensive and less effective. We propose a fine-tuned model
that jointly integrates entity recognition and disambiguation in a unified
framework. Furthermore, our approach leverages large language models to enrich
the context of entity mentions, yielding better performance in entity
disambiguation. We evaluated our approach on benchmark datasets and compared
with several baselines. The evaluation results show that our approach achieves
state-of-the-art performance on out-of-domain datasets.

</details>


### [2] [Small Language Models Offer Significant Potential for Science Community](https://arxiv.org/abs/2510.18890)
*Jian Zhang*

Main category: cs.CL

TL;DR: 使用小型语言模型（MiniLMs）从地球科学文献中进行精确、快速和经济的信息检索。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）存在信息偏差和计算成本问题。我旨在评估使用免费的小型语言模型从大量地球科学文献中进行精确、快速和经济的信息检索的可行性。

Method: 我构建了一个包含约 7700 万个高质量句子的语料库，这些句子来自 95 种领先的地球科学期刊，并使用 MiniLMs 通过语义搜索技术和句子级索引从这些语料库中提取相关领域特定信息。

Result: MiniLMs 能够有效地识别大量经过专家验证的、来自已建立的多学科来源的信息，尤其适用于具有定量结果的信息。此外，通过情感分析和无监督聚类分析，MiniLM 能够追踪地球科学界结论、研究重点、进展和新问题的演变。

Conclusion: MiniLM 在地球科学领域具有巨大的潜力，可用于事实和图像检索、趋势分析、矛盾分析和教育目的。

Abstract: Recent advancements in natural language processing, particularly with large
language models (LLMs), are transforming how scientists engage with the
literature. While the adoption of LLMs is increasing, concerns remain regarding
potential information biases and computational costs. Rather than LLMs, I
developed a framework to evaluate the feasibility of precise, rapid, and
cost-effective information retrieval from extensive geoscience literature using
freely available small language models (MiniLMs). A curated corpus of
approximately 77 million high-quality sentences, extracted from 95 leading
peer-reviewed geoscience journals such as Geophysical Research Letters and
Earth and Planetary Science Letters published during years 2000 to 2024, was
constructed. MiniLMs enable a computationally efficient approach for extracting
relevant domain-specific information from these corpora through semantic search
techniques and sentence-level indexing. This approach, unlike LLMs such as
ChatGPT-4 that often produces generalized responses, excels at identifying
substantial amounts of expert-verified information with established,
multi-disciplinary sources, especially for information with quantitative
findings. Furthermore, by analyzing emotional tone via sentiment analysis and
topical clusters through unsupervised clustering within sentences, MiniLM
provides a powerful tool for tracking the evolution of conclusions, research
priorities, advancements, and emerging questions within geoscience communities.
Overall, MiniLM holds significant potential within the geoscience community for
applications such as fact and image retrievals, trend analyses, contradiction
analyses, and educational purposes.

</details>


### [3] [DuoLens: A Framework for Robust Detection of Machine-Generated Multilingual Text and Code](https://arxiv.org/abs/2510.18904)
*Shriyansh Agrawal,Aidan Lau,Sanyam Shah,Ahan M R,Kevin Zhu,Sunishchal Dev,Vasu Sharma*

Main category: cs.CL

TL;DR: 本文提出通过微调encoder-only的小型语言模型（SLMs）来提高机器生成内容检测的准确性和效率，尤其是在代码和自然语言领域。


<details>
  <summary>Details</summary>
Motivation: 当前的内容检测器，如Fast DetectGPT或GPTZero，要么计算成本高，要么准确性不足，需要在两者之间进行权衡。

Method: 使用专门的数据集对RoBERTA和CodeBERTa等预训练模型进行微调，以进行二元分类任务。

Result: SLMs在AUROC达到0.97-0.99，macro-F1达到0.89-0.94的同时，延迟降低了8-12倍，峰值VRAM降低了3-5倍。

Conclusion: 结果表明，对于二元分类任务，SLMs在计算量较小的情况下，性能优于LLMs。

Abstract: The prevalence of Large Language Models (LLMs) for generating multilingual
text and source code has only increased the imperative for machine-generated
content detectors to be accurate and efficient across domains. Current
detectors, predominantly utilizing zero-shot methods, such as Fast DetectGPT or
GPTZero, either incur high computational cost or lack sufficient accuracy,
often with a trade-off between the two, leaving room for further improvement.
To address these gaps, we propose the fine-tuning of encoder-only Small
Language Models (SLMs), in particular, the pre-trained models of RoBERTA and
CodeBERTa using specialized datasets on source code and other natural language
to prove that for the task of binary classification, SLMs outperform LLMs by a
huge margin whilst using a fraction of compute. Our encoders achieve AUROC $=
0.97$ to $0.99$ and macro-F1 $0.89$ to $0.94$ while reducing latency by
$8$-$12\times$ and peak VRAM by $3$-$5\times$ at $512$-token inputs. Under
cross-generator shifts and adversarial transformations (paraphrase,
back-translation; code formatting/renaming), performance retains $\geq 92%$ of
clean AUROC. We release training and evaluation scripts with seeds and configs;
a reproducibility checklist is also included.

</details>


### [4] [When Models Can't Follow: Testing Instruction Adherence Across 256 LLMs](https://arxiv.org/abs/2510.18892)
*Richard J. Young,Brandon Gillins,Alice M. Matthews*

Main category: cs.CL

TL;DR: 本文提出了一种精简的评估框架，使用20个精心设计的提示来评估大型语言模型在不同任务类别中的指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型指令遵循能力的系统评估仍然具有挑战性，需要新的评估方法来评估真正的能力而不是记忆性能。

Method: 使用20个精心设计的提示，在2025年10月14日对来自OpenRouter的331个可用模型中的256个经过验证的工作模型进行了大规模实证研究。

Result: 研究结果揭示了一致的失败模式，并确定了构成特殊挑战的特定指令类型。

Conclusion: 这项工作贡献了一个实用的评估工具，以及对当代大型语言模型领域中指令遵循能力的最全面的实证分析之一。

Abstract: Despite widespread deployment of Large Language Models, systematic evaluation
of instruction-following capabilities remains challenging. While comprehensive
benchmarks exist, focused assessments that quickly diagnose specific
instruction adherence patterns are valuable. As newer models may be trained on
existing benchmarks, novel evaluation approaches are needed to assess genuine
capabilities rather than memorized performance. This paper presents a
streamlined evaluation framework using twenty carefully designed prompts to
assess LLM instruction-following across diverse task categories. We demonstrate
this framework through a large-scale empirical study conducted on October 14,
2025, testing 256 verified working models from 331 available via OpenRouter. To
ensure methodological rigor and prevent selection bias, we first verified each
model's basic functionality before inclusion. Unlike large-scale benchmarks
requiring extensive computational resources, our approach offers a practical
diagnostic tool researchers and practitioners can readily apply. Our
methodology builds upon verifiable instructions while introducing a compact
test suite balancing comprehensiveness with efficiency. Each prompt targets
distinct aspects of instruction following, including format compliance, content
constraints, logical sequencing, and multi-step task execution. We evaluate
models from major providers (OpenAI, Anthropic, Google, Meta, Mistral) and
emerging implementations (Qwen, DeepSeek, community models), providing
comparative performance analysis. Our findings reveal consistent failure modes
and identify specific instruction types posing particular challenges. This work
contributes both a practical evaluation tool and one of the most comprehensive
empirical analyses of instruction-following capabilities across the
contemporary LLM landscape.

</details>


### [5] [The Massive Legal Embedding Benchmark (MLEB)](https://arxiv.org/abs/2510.19365)
*Umar Butler,Abdur-Rahman Butler,Adrian Lucas Malec*

Main category: cs.CL

TL;DR: 提出了大规模法律嵌入基准(MLEB)，用于法律信息检索。


<details>
  <summary>Details</summary>
Motivation: 为了填补开放源代码法律信息检索领域的空白。

Method: 构建了包含来自多个司法管辖区、文档类型和任务类型的十个专家注释数据集。

Result: 构建了MLEB，并创建了新的组成数据集。

Conclusion: 发布了代码、结果和数据，以帮助进行可重复的评估。

Abstract: We present the Massive Legal Embedding Benchmark (MLEB), the largest, most
diverse, and most comprehensive open-source benchmark for legal information
retrieval to date. MLEB consists of ten expert-annotated datasets spanning
multiple jurisdictions (the US, UK, EU, Australia, Ireland, and Singapore),
document types (cases, legislation, regulatory guidance, contracts, and
literature), and task types (search, zero-shot classification, and question
answering). Seven of the datasets in MLEB were newly constructed in order to
fill domain and jurisdictional gaps in the open-source legal information
retrieval landscape. We document our methodology in building MLEB and creating
the new constituent datasets, and release our code, results, and data openly to
assist with reproducible evaluations.

</details>


### [6] [Transformer-Based Low-Resource Language Translation: A Study on Standard Bengali to Sylheti](https://arxiv.org/abs/2510.18898)
*Mangsura Kabir Oni,Tabia Tanzin Prama*

Main category: cs.CL

TL;DR: 本文研究了孟加拉语到锡尔赫特语的翻译，重点关注资源匮乏的语种。


<details>
  <summary>Details</summary>
Motivation: 探索资源匮乏语种的机器翻译，特别是孟加拉语到锡尔赫特语的翻译。

Method: 通过微调多语言Transformer模型，并与zero-shot大型语言模型（LLM）进行比较。

Result: 实验结果表明，微调后的模型明显优于LLM，其中mBART-50在翻译准确性方面表现最佳，而MarianMT在字符级保真度方面表现最强。

Conclusion: 研究结果强调了针对代表性不足的语言进行任务特定调整的重要性，并为包容性语言技术的持续发展做出了贡献。

Abstract: Machine Translation (MT) has advanced from rule-based and statistical methods
to neural approaches based on the Transformer architecture. While these methods
have achieved impressive results for high-resource languages, low-resource
varieties such as Sylheti remain underexplored. In this work, we investigate
Bengali-to-Sylheti translation by fine-tuning multilingual Transformer models
and comparing them with zero-shot large language models (LLMs). Experimental
results demonstrate that fine-tuned models significantly outperform LLMs, with
mBART-50 achieving the highest translation adequacy and MarianMT showing the
strongest character-level fidelity. These findings highlight the importance of
task-specific adaptation for underrepresented languages and contribute to
ongoing efforts toward inclusive language technologies.

</details>


### [7] [ToolDreamer: Instilling LLM Reasoning Into Tool Retrievers](https://arxiv.org/abs/2510.19791)
*Saptarshi Sengupta,Zhengyu Zhou,Jun Araki,Xingbo Wang,Bingqing Wang,Suhang Wang,Zhe Feng*

Main category: cs.CL

TL;DR: 提出ToolDreamer框架，通过LLM生成假设的工具描述（TD）来调节检索模型，以便根据LLM认为可能对查询有用的工具描述来获取工具。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的工具调用越来越受欢迎。然而，对于大型工具集，由此产生的tokens将超过LLM的上下文窗口限制，从而无法包含所有工具。因此，使用外部检索器为LLM提供与查询最相关的工具。现有的检索模型基于用户查询和工具描述（TD）之间的相似性对工具进行排序。由于用户请求通常与TD的语言不一致，这导致了次优的检索。

Method: 提出ToolDreamer框架，以根据使用LLM生成的假设（合成）TD来获取工具的检索模型为条件，即LLM认为可能对查询有用的工具描述。该框架能够在TD的语言空间内实现查询和工具之间更自然的对齐。

Result: 在ToolRet数据集上应用ToolDreamer，结果表明，无论是否经过训练，该方法都提高了稀疏和密集检索器的性能，从而展示了其灵活性。

Conclusion: 通过我们提出的框架，我们的目标是将一部分推理负担转移到检索器，以便LLM可以有效地处理大量工具，而不会使其上下文窗口饱和。

Abstract: Tool calling has become increasingly popular for Large Language Models
(LLMs). However, for large tool sets, the resulting tokens would exceed the
LLM's context window limit, making it impossible to include every tool. Hence,
an external retriever is used to provide LLMs with the most relevant tools for
a query. Existing retrieval models rank tools based on the similarity between a
user query and a tool description (TD). This leads to suboptimal retrieval as
user requests are often poorly aligned with the language of TD. To remedy the
issue, we propose ToolDreamer, a framework to condition retriever models to
fetch tools based on hypothetical (synthetic) TD generated using an LLM, i.e.,
description of tools that the LLM feels will be potentially useful for the
query. The framework enables a more natural alignment between queries and tools
within the language space of TD's. We apply ToolDreamer on the ToolRet dataset
and show that our method improves the performance of sparse and dense
retrievers with and without training, thus showcasing its flexibility. Through
our proposed framework, our aim is to offload a portion of the reasoning burden
to the retriever so that the LLM may effectively handle a large collection of
tools without inundating its context window.

</details>


### [8] [Improving Topic Modeling of Social Media Short Texts with Rephrasing: A Case Study of COVID-19 Related Tweets](https://arxiv.org/abs/2510.18908)
*Wangjiaxuan Xin,Shuhua Yin,Shi Chen,Yaorong Ge*

Main category: cs.CL

TL;DR: 该论文提出了一种名为 TM-Rephrase 的框架，利用大型语言模型（LLM）将社交媒体上的非正式文本改写为更正式的语言，以提高主题建模的效果。


<details>
  <summary>Details</summary>
Motivation: 社交媒体文本的简短、非正式和噪声会影响传统主题建模的效果。

Method: 该研究使用 TM-Rephrase 框架，结合两种改写策略（通用改写和口语转正式改写），并应用于多种主题建模方法。

Result: 实验结果表明，TM-Rephrase 提高了主题建模的性能，尤其是在使用口语转正式改写策略和 LDA 算法时。

Conclusion: 该研究提出了一种模型无关的方法，可增强公共卫生相关的社交媒体分析中的主题建模效果，从而更好地理解健康危机和其他重要领域中的公共言论。

Abstract: Social media platforms such as Twitter (now X) provide rich data for
analyzing public discourse, especially during crises such as the COVID-19
pandemic. However, the brevity, informality, and noise of social media short
texts often hinder the effectiveness of traditional topic modeling, producing
incoherent or redundant topics that are often difficult to interpret. To
address these challenges, we have developed \emph{TM-Rephrase}, a
model-agnostic framework that leverages large language models (LLMs) to
rephrase raw tweets into more standardized and formal language prior to topic
modeling. Using a dataset of 25,027 COVID-19-related Twitter posts, we
investigate the effects of two rephrasing strategies, general- and
colloquial-to-formal-rephrasing, on multiple topic modeling methods. Results
demonstrate that \emph{TM-Rephrase} improves three metrics measuring topic
modeling performance (i.e., topic coherence, topic uniqueness, and topic
diversity) while reducing topic redundancy of most topic modeling algorithms,
with the colloquial-to-formal strategy yielding the greatest performance gains
and especially for the Latent Dirichlet Allocation (LDA) algorithm. This study
contributes to a model-agnostic approach to enhancing topic modeling in public
health related social media analysis, with broad implications for improved
understanding of public discourse in health crisis as well as other important
domains.

</details>


### [9] [Learning from the Best, Differently: A Diversity-Driven Rethinking on Data Selection](https://arxiv.org/abs/2510.18909)
*Hongyi He,Xiao Liu,Zhenghao Lin,Mingni Tang,Yi Cheng,Jintao Wang,Wenjie Li,Peng Cheng,Yeyun Gong*

Main category: cs.CL

TL;DR: 提出了正交多样性感知选择（ODiS）算法，用于为大型语言模型选择高质量和多样化的预训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有的基于分数的选择方法会忽略多样性，导致性能下降。

Method: ODiS算法通过主成分分析（PCA）将多维分数分解为正交特征维度，并在每个维度上选择得分最高的数据。

Result: ODiS选择的数据维度间重叠小于2%，且使用ODiS选择的数据训练的模型在下游基准测试中明显优于其他基线。

Conclusion: 正交的、多样性感知的数据选择对于大型语言模型至关重要。

Abstract: High-quality pre-training data is crutial for large language models, where
quality captures factual reliability and semantic value, and diversity ensures
broad coverage and distributional heterogeneity. Existing approaches typically
rely on single or multiple-dimensional score-based selection. However, directly
selecting top-scored data often degrades performance, and sampling from a
broader range is required to recover results. The above non-monotonicity
between dataset scores and downstream benchmark results reveals a fundamental
bias: score-based methods collapse correlated dimensions, causing top-scored
data to appear high-quality while systematically overlooking diversity. We
argue that ensuring diversity requires decomposing correlated metrics into
orthogonal feature dimensions, from which the top-scored data can be directly
selected. Therefore, we proposed the Orthogonal Diversity-Aware Selection
(ODiS) algorithm, which preserves both quality and diversity during data
selection. First, ODiS evaluates data from multiple dimensions, covering
language quality, knowledge quality, and comprehension difficulty. The
multi-dimensional scores are then decorrelated via Principal Component Analysis
(PCA), yielding orthogonal evaluation dimensions. For each dimension, a
Roberta-based scorer is trained to regress the data onto PCA-projected scores,
enabling scalable inference on large corpora. Finally, ODiS constructs the
training dataset by selecting top-scored data within each orthogonal dimension,
thereby ensuring both quality and diversity. Empirical results show that
ODiS-selected data exhibit less than 2\% inter-dimension overlap, confirming
orthogonality between dimensions. More importantly, models trained with
ODiS-selected data significantly outperform other baselines on downstream
benchmarks, highlighting the necessity of orthogonal, diversity-aware data
selection for LLMs.

</details>


### [10] [Context-aware Fairness Evaluation and Mitigation in LLMs](https://arxiv.org/abs/2510.18914)
*Afrozah Nadeem,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 大型语言模型会表现出不良行为，比如不公平和有害内容。虽然有方法可以减少这些问题，但是成本高，而且不能适应新的对话。因此，我们提出了一个动态的、可逆的、基于剪枝的框架，可以在生成过程中检测上下文相关的神经元激活，并进行调整。我们的方案可以更好地控制对话，并且保留知识。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在对话中会表现出不公平、不一致和传播有害内容等不良行为。现有的解决方法要么计算成本高，要么不能适应新的对话环境。

Method: 提出了一种动态的、可逆的、基于剪枝的框架，通过检测上下文相关的神经元激活，并进行自适应的masking，来调节它们在生成过程中的影响。

Result: 该方案能够在多语言的单轮和多轮对话中实现细粒度的、记忆感知的缓解，同时保留知识，并实现更连贯的行为。

Conclusion: 提出的推理时解决方案能够在实际对话AI中实现动态的公平性控制。

Abstract: Large language models often display undesirable behaviors embedded in their
internal representations, undermining fairness, inconsistency drift,
amplification of harmful content, and the propagation of unwanted patterns
during extended dialogue and conversations. Although training-time or
data-centric methods attempt to reduce these effects, they are computationally
expensive, irreversible once deployed, and slow to adapt to new conversational
contexts. Pruning-based methods provide a flexible and transparent way to
reduce bias by adjusting the neurons responsible for certain behaviors.
However, most existing approaches are static; once a neuron is removed, the
model loses the ability to adapt when the conversation or context changes. To
address this, we propose a dynamic, reversible, pruning-based framework that
detects context-aware neuron activations and applies adaptive masking to
modulate their influence during generation. Our inference-time solution
provides fine-grained, memory-aware mitigation with knowledge-preserved, more
coherent behavior across multilingual single- and multi-turn dialogues,
enabling dynamic fairness control in real-world conversational AI.

</details>


### [11] [MMAO-Bench: MultiModal All in One Benchmark Reveals Compositional Law between Uni-modal and Omni-modal in OmniModels](https://arxiv.org/abs/2510.18915)
*Chen Chen,ZeYang Hu,Fengjiao Chen,Liya Ma,Jiaxing Liu,Xiaoyu Li,Xuezhi Cao*

Main category: cs.CL

TL;DR: 本研究提出了一个名为MMAO-Bench的新型、高质量、多样性的全模态基准，用于评估单模态和全模态的理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前全模态大语言模型的发展迅速，但单模态和全模态之间的关联尚不清楚，需要全面的评估来推动全模态智能的发展。

Method: 该基准包含1880个人工标注样本，涵盖44种任务类型，以及一种创新的多步骤开放式问题类型，可以更好地评估复杂的推理任务。

Result: 实验结果表明，跨模态和单模态性能之间存在组合规律，全模态能力在弱模型上表现为瓶颈效应，而在强模型上表现出协同促进作用。

Conclusion: 该基准的提出和实验结果为全模态大语言模型的发展提供了新的评估工具和研究方向。

Abstract: Multimodal Large Languages models have been progressing from uni-modal
understanding toward unifying visual, audio and language modalities,
collectively termed omni models. However, the correlation between uni-modal and
omni-modal remains unclear, which requires comprehensive evaluation to drive
omni model's intelligence evolution. In this work, we propose a novel, high
quality and diversity omni model benchmark, MultiModal All in One Benchmark
(MMAO-Bench), which effectively assesses both uni-modal and omni-modal
understanding capabilities. The benchmark consists of 1880 human curated
samples, across 44 task types, and a innovative multi-step open-ended question
type that better assess complex reasoning tasks. Experimental result shows the
compositional law between cross-modal and uni-modal performance and the
omni-modal capability manifests as a bottleneck effect on weak models, while
exhibiting synergistic promotion on strong models.

</details>


### [12] [Misinformation Detection using Large Language Models with Explainability](https://arxiv.org/abs/2510.18918)
*Jainee Patel,Chintan Bhatt,Himani Trivedi,Thanh Thi Nguyen*

Main category: cs.CL

TL;DR: 本文提出了一种可解释且计算高效的流程，用于使用基于 Transformer 的预训练语言模型 (PLM) 检测错误信息。


<details>
  <summary>Details</summary>
Motivation: 在线平台上错误信息的迅速传播破坏了个人之间的信任，并阻碍了知情的决策。

Method: 使用两步策略优化 RoBERTa 和 DistilBERT：首先，冻结主干并仅训练分类头；然后，逐步解冻主干层，同时应用逐层学习率衰减。在两个真实世界的基准数据集上测试所提出的方法，并整合 LIME 和 SHAP 来确保透明性。

Result: DistilBERT 实现了与 RoBERTa 相当的准确率，同时需要的计算资源明显更少。

Conclusion: PLM 与有原则的微调和可解释性相结合，可以成为可扩展、值得信赖的错误信息检测的有效框架。

Abstract: The rapid spread of misinformation on online platforms undermines trust among
individuals and hinders informed decision making. This paper shows an
explainable and computationally efficient pipeline to detect misinformation
using transformer-based pretrained language models (PLMs). We optimize both
RoBERTa and DistilBERT using a two-step strategy: first, we freeze the backbone
and train only the classification head; then, we progressively unfreeze the
backbone layers while applying layer-wise learning rate decay. On two
real-world benchmark datasets, COVID Fake News and FakeNewsNet GossipCop, we
test the proposed approach with a unified protocol of preprocessing and
stratified splits. To ensure transparency, we integrate the Local Interpretable
Model-Agnostic Explanations (LIME) at the token level to present token-level
rationales and SHapley Additive exPlanations (SHAP) at the global feature
attribution level. It demonstrates that DistilBERT achieves accuracy comparable
to RoBERTa while requiring significantly less computational resources. This
work makes two key contributions: (1) it quantitatively shows that a
lightweight PLM can maintain task performance while substantially reducing
computational cost, and (2) it presents an explainable pipeline that retrieves
faithful local and global justifications without compromising performance. The
results suggest that PLMs combined with principled fine-tuning and
interpretability can be an effective framework for scalable, trustworthy
misinformation detection.

</details>


### [13] [Evaluating LLM Story Generation through Large-scale Network Analysis of Social Structures](https://arxiv.org/abs/2510.18932)
*Hiroshi Nonaka,K. E. Perry*

Main category: cs.CL

TL;DR: 提出了一种新的、可扩展的方法，通过分析叙事中的潜在社会结构来评估 LLM 的故事生成能力。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型 (LLM) 在复杂任务中的创造能力通常需要难以扩展的人工评估。

Method: 通过将叙事中的潜在社会结构分析为有符号角色网络，进行大规模比较分析，使用了四个领先的 LLM（GPT-4o、GPT-4o mini、Gemini 1.5 Pro 和 Gemini 1.5 Flash）和人工编写的语料库生成的超过 1,200 个故事。

Result: 基于密度、聚类和有符号边权重等网络属性的研究结果表明，LLM 生成的故事始终表现出对紧密结合的积极关系的强烈偏好，这与先前使用人工评估的研究结果一致。

Conclusion: 该方法为评估当前和未来 LLM 创意讲故事的局限性和趋势提供了一个有价值的工具。

Abstract: Evaluating the creative capabilities of large language models (LLMs) in
complex tasks often requires human assessments that are difficult to scale. We
introduce a novel, scalable methodology for evaluating LLM story generation by
analyzing underlying social structures in narratives as signed character
networks. To demonstrate its effectiveness, we conduct a large-scale
comparative analysis using networks from over 1,200 stories, generated by four
leading LLMs (GPT-4o, GPT-4o mini, Gemini 1.5 Pro, and Gemini 1.5 Flash) and a
human-written corpus. Our findings, based on network properties like density,
clustering, and signed edge weights, show that LLM-generated stories
consistently exhibit a strong bias toward tightly-knit, positive relationships,
which aligns with findings from prior research using human assessment. Our
proposed approach provides a valuable tool for evaluating limitations and
tendencies in the creative storytelling of current and future LLMs.

</details>


### [14] [Lost in the Maze: Overcoming Context Limitations in Long-Horizon Agentic Search](https://arxiv.org/abs/2510.18939)
*Howard Yen,Ashwin Paranjape,Mengzhou Xia,Thejas Venkatesh,Jack Hessel,Danqi Chen,Yuhao Zhang*

Main category: cs.CL

TL;DR: 本文介绍了一种名为 SLIM 的简单框架，旨在解决现有agentic搜索框架在长程任务中由于上下文限制而难以扩展的问题。SLIM 通过分离检索工具和定期总结轨迹来保持上下文简洁，从而实现更长、更集中的搜索。


<details>
  <summary>Details</summary>
Motivation: 现有的agentic搜索框架难以扩展到长程任务，主要是因为上下文限制，例如积累冗长且嘈杂的内容、超出上下文窗口和工具预算或过早停止。

Method: 本文提出 SLIM 框架，该框架将检索分为不同的搜索和浏览工具，并定期总结轨迹，以保持上下文的简洁。

Result: SLIM 在长程任务上实现了可比较的性能，但成本显著降低，工具调用次数也远少于强大的开源基线。例如，在使用 o3 作为基础模型时，SLIM 在 BrowseComp 上实现了 56% 的性能，在 HLE 上实现了 31% 的性能，分别比所有开源框架高出 8 和 4 个百分点，同时工具调用次数减少了 4-6 倍。SLIM 幻觉也比以前的系统少。

Conclusion: 本文发布了一个自动化的细粒度轨迹分析流程和错误分类法，用于描述长程agentic搜索框架。希望本文的分析框架和简单的工具设计能够为未来的长程agent提供信息。

Abstract: Long-horizon agentic search requires iteratively exploring the web over long
trajectories and synthesizing information across many sources, and is the
foundation for enabling powerful applications like deep research systems. In
this work, we show that popular agentic search frameworks struggle to scale to
long trajectories primarily due to context limitations-they accumulate long,
noisy content, hit context window and tool budgets, or stop early. Then, we
introduce SLIM (Simple Lightweight Information Management), a simple framework
that separates retrieval into distinct search and browse tools, and
periodically summarizes the trajectory, keeping context concise while enabling
longer, more focused searches. On long-horizon tasks, SLIM achieves comparable
performance at substantially lower cost and with far fewer tool calls than
strong open-source baselines across multiple base models. Specifically, with o3
as the base model, SLIM achieves 56% on BrowseComp and 31% on HLE,
outperforming all open-source frameworks by 8 and 4 absolute points,
respectively, while incurring 4-6x fewer tool calls. Finally, we release an
automated fine-grained trajectory analysis pipeline and error taxonomy for
characterizing long-horizon agentic search frameworks; SLIM exhibits fewer
hallucinations than prior systems. We hope our analysis framework and simple
tool design inform future long-horizon agents.

</details>


### [15] [ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge](https://arxiv.org/abs/2510.18941)
*Zhilin Wang,Jaehun Jung,Ximing Lu,Shizhe Diao,Ellie Evans,Jiaqi Zeng,Pavlo Molchanov,Yejin Choi,Jan Kautz,Yi Dong*

Main category: cs.CL

TL;DR: 提出了ProfBench，一个用于评估大型语言模型在处理专业文档、综合信息和生成报告能力的数据集。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在处理专业文档方面的能力受限于验证回复的挑战。

Method: 构建了强大且经济实惠的LLM-Judges来评估ProfBench的rubrics，通过减轻自我增强偏差并降低评估成本。

Result: ProfBench对最先进的LLM提出了重大挑战，即使是像GPT-5-high这样的顶级模型也只能达到65.9%的整体性能。专有模型和开放权重模型之间存在显著的性能差异。

Conclusion: ProfBench揭示了大型语言模型在处理复杂专业领域任务方面的差距，并强调了扩展思考在解决这些任务中的作用。

Abstract: Evaluating progress in large language models (LLMs) is often constrained by
the challenge of verifying responses, limiting assessments to tasks like
mathematics, programming, and short-form question-answering. However, many
real-world applications require evaluating LLMs in processing professional
documents, synthesizing information, and generating comprehensive reports in
response to user queries. We introduce ProfBench: a set of over 7000
response-criterion pairs as evaluated by human-experts with professional
knowledge across Physics PhD, Chemistry PhD, Finance MBA and Consulting MBA. We
build robust and affordable LLM-Judges to evaluate ProfBench rubrics, by
mitigating self-enhancement bias and reducing the cost of evaluation by 2-3
orders of magnitude, to make it fair and accessible to the broader community.
Our findings reveal that ProfBench poses significant challenges even for
state-of-the-art LLMs, with top-performing models like GPT-5-high achieving
only 65.9\% overall performance. Furthermore, we identify notable performance
disparities between proprietary and open-weight models and provide insights
into the role that extended thinking plays in addressing complex,
professional-domain tasks. Data:
https://huggingface.co/datasets/nvidia/ProfBench and Code:
https://github.com/NVlabs/ProfBench

</details>


### [16] [Dynamic Evaluation for Oversensitivity in LLMs](https://arxiv.org/abs/2510.19005)
*Sophia Xiao Pu,Sitao Cheng,Xin Eric Wang,William Yang Wang*

Main category: cs.CL

TL;DR: 论文提出了一种动态生成模型特定挑战性数据集的框架，用于评估语言模型的过度敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有的静态数据集会随着模型的演进而退化，导致数据污染和评估能力下降。

Method: 构建了一个动态生成模型特定数据集的框架，捕捉新兴的防御模式，并与每个模型的独特行为保持一致。

Result: 构建了一个名为OVERBENCH的基准，该基准汇集了来自25个模型的450,000个样本。

Conclusion: OVERBENCH提供了一个动态和不断发展的过度敏感性视角，允许持续监控防御触发因素，并突出静态数据集忽略的漏洞。

Abstract: Oversensitivity occurs when language models defensively reject prompts that
are actually benign. This behavior not only disrupts user interactions but also
obscures the boundary between harmful and harmless content. Existing benchmarks
rely on static datasets that degrade overtime as models evolve, leading to data
contamination and diminished evaluative power. To address this, we develop a
framework that dynamically generates model-specific challenging datasets,
capturing emerging defensive patterns and aligning with each model's unique
behavior. Building on this approach, we construct OVERBENCH, a benchmark that
aggregates these datasets across diverse LLM families, encompassing 450,000
samples from 25 models. OVERBENCH provides a dynamic and evolving perspective
on oversensitivity, allowing for continuous monitoring of defensive triggers as
models advance, highlighting vulnerabilities that static datasets overlook.

</details>


### [17] [Are they lovers or friends? Evaluating LLMs' Social Reasoning in English and Korean Dialogues](https://arxiv.org/abs/2510.19028)
*Eunsu Kim,Junyeong Park,Juhyun Oh,Kiwoong Park,Seyoung Song,A. Seza Dogruoz,Najoung Kim,Alice Oh*

Main category: cs.CL

TL;DR: 本文介绍了一个新的数据集SCRIPTS，用于评估大型语言模型在人际关系中的社会推理能力。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在人机交互中人际关系方面的社会推理能力。

Method: 该方法使用从电影剧本中提取的包含1000个对话的数据集SCRIPTS，并让人工标注对话者之间关系的概率标签（非常可能、不太可能、不可能）。

Result: 在英语数据集上，目前的专有大型语言模型取得了大约75-80%的准确率，但在韩语数据集上的表现下降到58-69%。更令人震惊的是，模型在10-25%的回复中选择了“不可能”的关系。此外，对于一般推理有效的思维模型和思维链提示，对社会推理提供的好处很小，有时还会放大社会偏见。

Conclusion: 目前的大型语言模型在社会推理能力方面存在显著的局限性，因此有必要努力开发具有社会意识的语言模型。

Abstract: As large language models (LLMs) are increasingly used in human-AI
interactions, their social reasoning capabilities in interpersonal contexts are
critical. We introduce SCRIPTS, a 1k-dialogue dataset in English and Korean,
sourced from movie scripts. The task involves evaluating models' social
reasoning capability to infer the interpersonal relationships (e.g., friends,
sisters, lovers) between speakers in each dialogue. Each dialogue is annotated
with probabilistic relational labels (Highly Likely, Less Likely, Unlikely) by
native (or equivalent) Korean and English speakers from Korea and the U.S.
Evaluating nine models on our task, current proprietary LLMs achieve around
75-80% on the English dataset, whereas their performance on Korean drops to
58-69%. More strikingly, models select Unlikely relationships in 10-25% of
their responses. Furthermore, we find that thinking models and chain-of-thought
prompting, effective for general reasoning, provide minimal benefits for social
reasoning and occasionally amplify social biases. Our findings reveal
significant limitations in current LLMs' social reasoning capabilities,
highlighting the need for efforts to develop socially-aware language models.

</details>


### [18] [Re:Member: Emotional Question Generation from Personal Memories](https://arxiv.org/abs/2510.19030)
*Zackary Rackauckas,Nobuaki Minematsu,Julia Hirschberg*

Main category: cs.CL

TL;DR: Re:Member是一个二语学习系统，利用用户个人视频和情感语音问题来提高学习投入度。


<details>
  <summary>Details</summary>
Motivation: 探讨情感表达和记忆关联互动如何促进更具吸引力的二语(L2)学习。

Method: 该系统结合了WhisperX转录对齐、3帧视觉采样和Style-BERT-VITS2，以在模块化生成流程中实现情感合成，并根据视觉内容调整情感基调，使用如耳语或深夜语气等表达性语音风格来唤起特定情绪。

Result: Re:Member旨在鼓励情感回忆和对话互动。

Conclusion: Re:Member强调情感和个人媒体在以学习者为中心的教育技术中的作用。

Abstract: We present Re:Member, a system that explores how emotionally expressive,
memory-grounded interaction can support more engaging second language (L2)
learning. By drawing on users' personal videos and generating stylized spoken
questions in the target language, Re:Member is designed to encourage affective
recall and conversational engagement. The system aligns emotional tone with
visual context, using expressive speech styles such as whispers or late-night
tones to evoke specific moods. It combines WhisperX-based transcript alignment,
3-frame visual sampling, and Style-BERT-VITS2 for emotional synthesis within a
modular generation pipeline. Designed as a stylized interaction probe,
Re:Member highlights the role of affect and personal media in learner-centered
educational technologies.

</details>


### [19] [When Can We Trust LLMs in Mental Health? Large-Scale Benchmarks for Reliable LLM Evaluation](https://arxiv.org/abs/2510.19032)
*Abeer Badawi,Elahe Rahimi,Md Tahmid Rahman Laskar,Sheri Grach,Lindsay Bertrand,Lames Danok,Jimmy Huang,Frank Rudzicz,Elham Dolatabadi*

Main category: cs.CL

TL;DR: 本研究旨在解决评估大型语言模型（LLMs）在心理健康支持方面的挑战，现有基准在规模、可靠性方面存在局限性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法依赖合成数据或社交媒体数据，缺乏评估自动评估器可信度的框架。

Method: 引入两个基准：MentalBench-100k（整合10,000个对话，生成100,000个响应对）和MentalAlign-70k（比较四个LLM评估器与人类专家在70,000个评分上的表现，使用情感认知一致性框架量化LLM评估器和人类专家之间的一致性、稳定性和偏差）。

Result: LLM评估器存在系统性膨胀，在认知属性（如指导和信息性）方面可靠性高，在共情方面精度降低，在安全性和相关性方面存在不可靠性。

Conclusion: 为心理健康领域LLM的可靠、大规模评估奠定了新的方法论和经验基础。

Abstract: Evaluating Large Language Models (LLMs) for mental health support is
challenging due to the emotionally and cognitively complex nature of
therapeutic dialogue. Existing benchmarks are limited in scale, reliability,
often relying on synthetic or social media data, and lack frameworks to assess
when automated judges can be trusted. To address the need for large-scale
dialogue datasets and judge reliability assessment, we introduce two benchmarks
that provide a framework for generation and evaluation. MentalBench-100k
consolidates 10,000 one-turn conversations from three real scenarios datasets,
each paired with nine LLM-generated responses, yielding 100,000 response pairs.
MentalAlign-70k}reframes evaluation by comparing four high-performing LLM
judges with human experts across 70,000 ratings on seven attributes, grouped
into Cognitive Support Score (CSS) and Affective Resonance Score (ARS). We then
employ the Affective Cognitive Agreement Framework, a statistical methodology
using intraclass correlation coefficients (ICC) with confidence intervals to
quantify agreement, consistency, and bias between LLM judges and human experts.
Our analysis reveals systematic inflation by LLM judges, strong reliability for
cognitive attributes such as guidance and informativeness, reduced precision
for empathy, and some unreliability in safety and relevance. Our contributions
establish new methodological and empirical foundations for reliable,
large-scale evaluation of LLMs in mental health. We release the benchmarks and
codes at: https://github.com/abeerbadawi/MentalBench/

</details>


### [20] [From Memorization to Generalization: Fine-Tuning Large Language Models for Biomedical Term-to-Identifier Normalization](https://arxiv.org/abs/2510.19036)
*Suswitha Pericharla,Daniel B. Hier,Tayo Obafemi-Ajayi*

Main category: cs.CL

TL;DR: 大型语言模型在生物医学术语标准化方面表现不一，术语标准化是将自然语言生物医学术语映射到标准化标识符。对 Llama 3.1 8B 的微调显示，不同术语的表现差异显著。identifier 的普及性和语汇化是影响微调成功的关键因素。


<details>
  <summary>Details</summary>
Motivation: 为了实现语义互操作性，生物医学数据的有效整合依赖于自动术语标准化，即将自然语言生物医学术语映射到标准化标识符。大型语言模型 (LLM) 在这项任务中显示出前景，但在不同术语中的表现并不均衡。

Method: 通过在多个生物医学本体中评估记忆（训练术语表现）和泛化（验证术语表现）来评估模型性能。对 Llama 3.1 8B 进行了微调。

Result: GO 映射显示出强大的记忆增益（术语到标识符的准确率提高了高达 77%），而 HPO 显示出极小的改进。泛化仅发生在蛋白质-基因 (GENE) 映射中（增益 13.9%），而针对 HPO 和 GO 的微调产生的迁移可以忽略不计。基线准确率因模型规模而异，GPT-4o 在所有术语中的表现均优于两种 Llama 变体。嵌入分析显示基因符号和蛋白质名称之间存在紧密的语义对齐，但 GO 或 HPO 的术语和标识符之间的对齐较弱，这与有限的词汇化一致。

Conclusion: 微调的成功取决于两个相互作用的因素：标识符的普及性和词汇化。流行的标识符更可能在预训练期间遇到，从而增强记忆。词汇化的标识符（如基因符号）能够实现语义泛化。相比之下，GO 和 HPO 中的任意标识符将模型限制为死记硬背。这些发现为微调何时增强事实回忆以及何时因稀疏或非词汇化的标识符而失败提供了一个预测框架。

Abstract: Effective biomedical data integration depends on automated term
normalization, the mapping of natural language biomedical terms to standardized
identifiers. This linking of terms to identifiers is essential for semantic
interoperability. Large language models (LLMs) show promise for this task but
perform unevenly across terminologies. We evaluated both memorization
(training-term performance) and generalization (validation-term performance)
across multiple biomedical ontologies. Fine-tuning Llama 3.1 8B revealed marked
differences by terminology. GO mappings showed strong memorization gains (up to
77% improvement in term-to-identifier accuracy), whereas HPO showed minimal
improvement. Generalization occurred only for protein-gene (GENE) mappings
(13.9% gain), while fine-tuning for HPO and GO yielded negligible transfer.
Baseline accuracy varied by model scale, with GPT-4o outperforming both Llama
variants for all terminologies. Embedding analyses showed tight semantic
alignment between gene symbols and protein names but weak alignment between
terms and identifiers for GO or HPO, consistent with limited lexicalization.
Fine-tuning success depended on two interacting factors: identifier popularity
and lexicalization. Popular identifiers were more likely encountered during
pretraining, enhancing memorization. Lexicalized identifiers, such as gene
symbols, enabled semantic generalization. By contrast, arbitrary identifiers in
GO and HPO constrained models to rote learning. These findings provide a
predictive framework for when fine-tuning enhances factual recall versus when
it fails due to sparse or non-lexicalized identifiers.

</details>


### [21] [That's Deprecated! Understanding, Detecting, and Steering Knowledge Conflicts in Language Models for Code Generation](https://arxiv.org/abs/2510.19116)
*Jaesung Bae,Cameron Churchwell,Mitchell Hermon,Tsun-An Hsieh,Jocelyn Xu,Yekaterina Yegorova,Mark Hasegawa-Johnson,Heng Ji*

Main category: cs.CL

TL;DR: 研究大型语言模型在面对参数知识和提示信息之间差异时的行为，并将其扩展到代码生成领域。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在知识冲突情境下的行为，特别是在代码生成方面的表现。

Method: 提出了一个领域无关的框架来构建和解释知识冲突，以及一个新的评估方法和数据集。

Result: 大型语言模型能够以高达 80.65% 的准确率检测知识冲突，并且通过激活层面的引导可以实现高达 12.6% 的性能提升。

Conclusion: 大型语言模型能够编码知识冲突的概念，并且可以通过激活层面的引导来改善性能，但效果取决于模型大小、任务领域和引导方向之间的平衡。

Abstract: This paper investigates how large language models (LLMs) behave when faced
with discrepancies between their parametric knowledge and conflicting
information contained in a prompt. Building on prior question-answering (QA)
research, we extend the investigation of knowledge conflicts to the realm of
code generation. We propose a domain-agnostic framework for constructing and
interpreting such conflicts, along with a novel evaluation method and dataset
tailored to code conflict scenarios. Our experiments indicate that sufficiently
large LLMs encode the notion of a knowledge conflict in their parameters,
enabling us to detect knowledge conflicts with up to \textbf{80.65\%} accuracy.
Building on these insights, we show that activation-level steering can achieve
up to a \textbf{12.6\%} improvement in steering success over a random baseline.
However, effectiveness depends critically on balancing model size, task domain,
and steering direction. The experiment code and data will be made publicly
available after acceptance.

</details>


### [22] [A Graph Signal Processing Framework for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2510.19117)
*Valentin Noël*

Main category: cs.CL

TL;DR: 大型语言模型在事实推理和幻觉区分方面面临挑战。本文提出了一个频谱分析框架，通过图信号处理定义诊断指标，与计算稳定性相关联。实验表明，事实陈述表现出一致的“能量山”行为，而不同类型的幻觉表现出不同的特征。


<details>
  <summary>Details</summary>
Motivation: 区分大型语言模型中的事实推理和幻觉仍然具有挑战性。

Method: 将 Transformer 层建模为由注意力机制引起的动态图，将 token embeddings 作为这些图上的信号。通过图信号处理，定义了包括 Dirichlet 能量、谱熵和高频能量比率在内的诊断指标。

Result: 实验表明，事实陈述表现出一致的“能量山”行为，低频收敛；不同幻觉类型表现出不同的特征。逻辑矛盾会破坏谱的稳定性，而语义错误保持稳定但显示连接漂移，替换幻觉显示中间扰动。使用频谱特征的简单检测器实现了 88.75% 的准确率，而基于困惑度的基线为 75%。

Conclusion: 频谱几何可以捕获推理模式和错误行为，可能为大型语言模型中的幻觉检测提供一个框架。

Abstract: Large language models achieve impressive results but distinguishing factual
reasoning from hallucinations remains challenging. We propose a spectral
analysis framework that models transformer layers as dynamic graphs induced by
attention, with token embeddings as signals on these graphs. Through graph
signal processing, we define diagnostics including Dirichlet energy, spectral
entropy, and high-frequency energy ratios, with theoretical connections to
computational stability. Experiments across GPT architectures suggest universal
spectral patterns: factual statements exhibit consistent "energy mountain"
behavior with low-frequency convergence, while different hallucination types
show distinct signatures. Logical contradictions destabilize spectra with large
effect sizes ($g>1.0$), semantic errors remain stable but show connectivity
drift, and substitution hallucinations display intermediate perturbations. A
simple detector using spectral signatures achieves 88.75% accuracy versus 75%
for perplexity-based baselines, demonstrating practical utility. These findings
indicate that spectral geometry may capture reasoning patterns and error
behaviors, potentially offering a framework for hallucination detection in
large language models.

</details>


### [23] [Training-Free Spectral Fingerprints of Voice Processing in Transformers](https://arxiv.org/abs/2510.19131)
*Valentin Noël*

Main category: cs.CL

TL;DR: 通过频谱分析，不同的Transformer架构通过不同的连接模式实现相同的语言计算，从而产生模型印记的“计算指纹”。


<details>
  <summary>Details</summary>
Motivation: 研究不同Transformer架构在处理不同语言时的差异，揭示模型训练的偏好和架构特性。

Method: 使用图信号处理分析注意力机制诱导的token图，追踪在20种语言和三个模型家族中，语音转换下的代数连通性变化。

Result: 发现了清晰的架构特征：Phi-3-Mini在早期层中显示出显著的英语特定扰动，而Qwen2.5-7B显示出小的、分布式的变化，LLaMA-3.2-1B表现出系统但减弱的反应。这些频谱特征与行为差异密切相关。

Conclusion: 训练重点可以在模型中留下可检测的计算印记，表现为在句法转换过程中可测量的连接模式。该框架可用于区分推理模式，为揭示架构偏差和支持模型可靠性分析提供了一种简单的、无需训练的诊断方法。

Abstract: Different transformer architectures implement identical linguistic
computations via distinct connectivity patterns, yielding model imprinted
``computational fingerprints'' detectable through spectral analysis. Using
graph signal processing on attention induced token graphs, we track changes in
algebraic connectivity (Fiedler value, $\Delta\lambda_2$) under voice
alternation across 20 languages and three model families, with a prespecified
early window (layers 2--5). Our analysis uncovers clear architectural
signatures: Phi-3-Mini shows a dramatic English specific early layer disruption
($\overline{\Delta\lambda_2}_{[2,5]}\!\approx\!-0.446$) while effects in 19
other languages are minimal, consistent with public documentation that
positions the model primarily for English use. Qwen2.5-7B displays small,
distributed shifts that are largest for morphologically rich languages, and
LLaMA-3.2-1B exhibits systematic but muted responses. These spectral signatures
correlate strongly with behavioral differences (Phi-3: $r=-0.976$) and are
modulated by targeted attention head ablations, linking the effect to early
attention structure and confirming functional relevance. Taken together, the
findings are consistent with the view that training emphasis can leave
detectable computational imprints: specialized processing strategies that
manifest as measurable connectivity patterns during syntactic transformations.
Beyond voice alternation, the framework differentiates reasoning modes,
indicating utility as a simple, training free diagnostic for revealing
architectural biases and supporting model reliability analysis.

</details>


### [24] [Tibetan Language and AI: A Comprehensive Survey of Resources, Methods and Challenges](https://arxiv.org/abs/2510.19144)
*Cheng Huang,Nyima Tashi,Fan Gao,Yutong Liu,Jiahao Li,Hao Tian,Siyang Jiang,Thupten Tsering,Ban Ma-bao,Renzeg Duojie,Gadeng Luosang,Rinchen Dongrub,Dorje Tashi,Jin Zhang,Xiao Feng,Hao Wang,Jie Tang,Guojie Tang,Xiangxiang Wang,Jia Zhang,Tsengdar Lee,Yongbin Yu*

Main category: cs.CL

TL;DR: 本文全面调查了人工智能领域中藏语人工智能的现状，涵盖了文本和语音数据资源、自然语言处理任务、机器翻译、语音识别以及大型语言模型的最新发展。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏可访问的数据资源、标准化基准和专用工具，对代表性不足的语言的人工智能系统的开发兴趣日益浓厚，但藏语受到的关注有限。本文旨在为未来的藏语人工智能研究工作奠定基础参考，并鼓励协作努力，为低资源语言构建一个包容和可持续的人工智能生态系统。

Method: 本文系统地对现有数据集和工具进行分类，评估不同任务中使用的方法，并在可能的情况下比较性能。

Result: 本文确定了持续存在的瓶颈，例如数据稀疏性、正字法变异以及缺乏统一的评估指标。

Conclusion: 本文讨论了跨语言迁移、多模态学习和社区驱动的资源创建的潜力。

Abstract: Tibetan, one of the major low-resource languages in Asia, presents unique
linguistic and sociocultural characteristics that pose both challenges and
opportunities for AI research. Despite increasing interest in developing AI
systems for underrepresented languages, Tibetan has received limited attention
due to a lack of accessible data resources, standardized benchmarks, and
dedicated tools. This paper provides a comprehensive survey of the current
state of Tibetan AI in the AI domain, covering textual and speech data
resources, NLP tasks, machine translation, speech recognition, and recent
developments in LLMs. We systematically categorize existing datasets and tools,
evaluate methods used across different tasks, and compare performance where
possible. We also identify persistent bottlenecks such as data sparsity,
orthographic variation, and the lack of unified evaluation metrics.
Additionally, we discuss the potential of cross-lingual transfer, multi-modal
learning, and community-driven resource creation. This survey aims to serve as
a foundational reference for future work on Tibetan AI research and encourages
collaborative efforts to build an inclusive and sustainable AI ecosystem for
low-resource languages.

</details>


### [25] ["You Are Rejected!": An Empirical Study of Large Language Models Taking Hiring Evaluations](https://arxiv.org/abs/2510.19167)
*Dingjie Fu,Dianxing Shi*

Main category: cs.CL

TL;DR: 大型语言模型在编码和推理任务中表现出色，但它们未能通过招聘评估。


<details>
  <summary>Details</summary>
Motivation: 科技公司需要高效地筛选有潜力的人才，而大型语言模型在编码和推理任务中表现出色。本文研究了大型语言模型是否能通过招聘评估。

Method: 使用先进的大型语言模型生成答案，并评估它们的性能。

Result: 模型生成的答案与公司参考的解决方案之间存在显著的不一致性。

Conclusion: 所有评估的大型语言模型都未能通过招聘评估。

Abstract: With the proliferation of the internet and the rapid advancement of
Artificial Intelligence, leading technology companies face an urgent annual
demand for a considerable number of software and algorithm engineers. To
efficiently and effectively identify high-potential candidates from thousands
of applicants, these firms have established a multi-stage selection process,
which crucially includes a standardized hiring evaluation designed to assess
job-specific competencies. Motivated by the demonstrated prowess of Large
Language Models (LLMs) in coding and reasoning tasks, this paper investigates a
critical question: Can LLMs successfully pass these hiring evaluations? To this
end, we conduct a comprehensive examination of a widely used professional
assessment questionnaire. We employ state-of-the-art LLMs to generate responses
and subsequently evaluate their performance. Contrary to any prior expectation
of LLMs being ideal engineers, our analysis reveals a significant inconsistency
between the model-generated answers and the company-referenced solutions. Our
empirical findings lead to a striking conclusion: All evaluated LLMs fails to
pass the hiring evaluation.

</details>


### [26] [Think Straight, Stop Smart: Structured Reasoning for Efficient Multi-Hop RAG](https://arxiv.org/abs/2510.19171)
*Jihwan Bang,Juntae Lee,Seunghan Yang,Sungha Choi*

Main category: cs.CL

TL;DR: TSSS: A structured multi-hop RAG framework for efficient complex reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing multi-hop RAG methods are inefficient due to regenerating predictable tokens and stochastic stopping.

Method: Introduces template-based reasoning with caching and a retriever-based terminator for deterministic halting.

Result: Achieves state-of-the-art accuracy and competitive efficiency on HotpotQA, 2WikiMultiHop, and MuSiQue.

Conclusion: TSSS is effective in efficiency-constrained scenarios like on-device inference.

Abstract: Multi-hop retrieval-augmented generation (RAG) is a promising strategy for
complex reasoning, yet existing iterative prompting approaches remain
inefficient. They often regenerate predictable token sequences at every step
and rely on stochastic stopping, leading to excessive token usage and unstable
termination. We propose TSSS (Think Straight, Stop Smart), a structured
multi-hop RAG framework designed for efficiency. TSSS introduces (i) a
template-based reasoning that caches recurring prefixes and anchors sub-queries
to the main question, reducing token generation cost while promoting stable
reasoning, and (ii) a retriever-based terminator, which deterministically halts
reasoning once additional sub-queries collapse into repetition. This separation
of structured reasoning and termination control enables both faster inference
and more reliable answers. On HotpotQA, 2WikiMultiHop, and MuSiQue, TSSS
achieves state-of-the-art accuracy and competitive efficiency among RAG-CoT
approaches, highlighting its effectiveness in efficiency-constrained scenarios
such as on-device inference.

</details>


### [27] [When Facts Change: Probing LLMs on Evolving Knowledge with evolveQA](https://arxiv.org/abs/2510.19172)
*Nishanth Sridhar Nakshatri,Shamik Roy,Manoj Ghuhan Arivazhagan,Hanhan Zhou,Vinayshekhar Bannihatti Kumar,Rashmi Gangadharaiah*

Main category: cs.CL

TL;DR: LLMs struggle with temporal knowledge conflicts (contradictions from facts changing over time).


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks don't fairly evaluate LLMs with different knowledge cut-off dates, focusing on popular entities instead of dynamic knowledge evolution.

Method: Introduce evolveQA, a benchmark using real-world, time-stamped data (AWS updates, Azure changes, WHO reports) to evaluate LLMs on temporally evolving knowledge. It generates questions tailored to different LLM knowledge cut-off dates.

Result: Significant performance drops (up to 31%) on evolveQA compared to static knowledge questions were observed across 12 LLMs.

Conclusion: LLMs have difficulty with temporal knowledge conflicts.

Abstract: LLMs often fail to handle temporal knowledge conflicts--contradictions
arising when facts evolve over time within their training data. Existing
studies evaluate this phenomenon through benchmarks built on structured
knowledge bases like Wikidata, but they focus on widely-covered,
easily-memorized popular entities and lack the dynamic structure needed to
fairly evaluate LLMs with different knowledge cut-off dates. We introduce
evolveQA, a benchmark specifically designed to evaluate LLMs on temporally
evolving knowledge, constructed from 3 real-world, time-stamped corpora: AWS
updates, Azure changes, and WHO disease outbreak reports. Our framework
identifies naturally occurring knowledge evolution and generates questions with
gold answers tailored to different LLM knowledge cut-off dates. Through
extensive evaluation of 12 open and closed-source LLMs across 3 knowledge
probing formats, we demonstrate significant performance drops of up to 31% on
evolveQA compared to static knowledge questions.

</details>


### [28] [Interpretable Question Answering with Knowledge Graphs](https://arxiv.org/abs/2510.19181)
*Kartikeya Aneja,Manasvi Srivastava,Subhayan Das,Nagender Aneja*

Main category: cs.CL

TL;DR: 本文介绍了一种完全基于知识图谱检索的问答系统，不依赖于大型语言模型的检索增强生成（RAG）。


<details>
  <summary>Details</summary>
Motivation: 探索不依赖大型语言模型进行问答的方法。

Method: 该系统首先预处理文档生成问答对，然后将这些问答对转换为知识图谱，并使用嵌入和模糊技术进行基于图的检索。检索结果经过重新排序和释义，生成最终答案。

Result: 在CRAG基准测试中，使用LLAMA-3.2和GPT-3.5-Turbo的准确率分别为71.9%和54.4%。

Conclusion: 该研究表明，即使不依赖大型语言模型，知识图谱检索也能在问答任务中取得一定的效果。

Abstract: This paper presents a question answering system that operates exclusively on
a knowledge graph retrieval without relying on retrieval augmented generation
(RAG) with large language models (LLMs). Instead, a small paraphraser model is
used to paraphrase the entity relationship edges retrieved from querying the
knowledge graph. The proposed pipeline is divided into two main stages. The
first stage involves pre-processing a document to generate sets of
question-answer (QA) pairs. The second stage converts these QAs into a
knowledge graph from which graph-based retrieval is performed using embeddings
and fuzzy techniques. The graph is queried, re-ranked, and paraphrased to
generate a final answer. This work includes an evaluation using LLM-as-a-judge
on the CRAG benchmark, which resulted in accuracies of 71.9% and 54.4% using
LLAMA-3.2 and GPT-3.5-Turbo, respectively.

</details>


### [29] [Multi-Faceted Evaluation of Tool-Augmented Dialogue Systems](https://arxiv.org/abs/2510.19186)
*Zhaoyi Joey Hou,Tanya Shourya,Yingfan Wang,Shamik Roy,Vinayshekhar Bannihatti Kumar,Rashmi Gangadharaiah*

Main category: cs.CL

TL;DR: 现有的评估方法无法捕捉到多轮工具增强对话中的关键错误，例如当代理错误解释工具结果但用户看起来满意时。我们介绍了TRACE，这是一个系统合成的工具增强对话的基准，涵盖了各种错误案例，以及SCOPE，一个评估框架，可以自动发现工具增强对话中的各种错误模式和评估标准。


<details>
  <summary>Details</summary>
Motivation: 评估使用外部工具的对话式人工智能系统具有挑战性，因为错误可能来自用户、代理和工具之间复杂的交互。

Method: TRACE：一个系统合成的工具增强对话的基准，涵盖了各种错误案例。SCOPE：一个评估框架，可以自动发现工具增强对话中的各种错误模式和评估标准。

Result: SCOPE 显著优于基线，尤其是在用户满意度信号具有误导性的具有挑战性的案例中。

Conclusion: TRACE 和 SCOPE 的提出

Abstract: Evaluating conversational AI systems that use external tools is challenging,
as errors can arise from complex interactions among user, agent, and tools.
While existing evaluation methods assess either user satisfaction or agents'
tool-calling capabilities, they fail to capture critical errors in multi-turn
tool-augmented dialogues-such as when agents misinterpret tool results yet
appear satisfactory to users. We introduce TRACE, a benchmark of systematically
synthesized tool-augmented conversations covering diverse error cases, and
SCOPE, an evaluation framework that automatically discovers diverse error
patterns and evaluation rubrics in tool-augmented dialogues. Experiments show
SCOPE significantly outperforms the baseline, particularly on challenging cases
where user satisfaction signals are misleading.

</details>


### [30] [DiSRouter: Distributed Self-Routing for LLM Selections](https://arxiv.org/abs/2510.19208)
*Hang Zheng,Hongshen Xu,Yongkai Lin,Shuai Fan,Lu Chen,Kai Yu*

Main category: cs.CL

TL;DR: DiSRouter，一个分布式LLM路由框架，利用LLM自身的自我感知能力进行路由决策，优于集中式路由。


<details>
  <summary>Details</summary>
Motivation: 现有中心化路由系统缺乏灵活性，无法充分理解不同LLM的知识边界。

Method: 提出DiSRouter，一种分布式路由范式，以及一个两阶段的自我感知训练流程，增强LLM的自我感知能力。

Result: DiSRouter在各种场景下的效用显著优于现有路由方法，能有效区分简单和困难的查询，并在out-of-domain任务中表现出强大的泛化能力。

Conclusion: 利用LLM的内在自我感知比外部评估更有效，为更模块化和高效的多智能体系统铺平了道路。

Abstract: The proliferation of Large Language Models (LLMs) has created a diverse
ecosystem of models with highly varying performance and costs, necessitating
effective query routing to balance performance and expense. Current routing
systems often rely on a centralized external router trained on a fixed set of
LLMs, making them inflexible and prone to poor performance since the small
router can not fully understand the knowledge boundaries of different LLMs. We
introduce DiSRouter (Distributed Self-Router), a novel paradigm that shifts
from centralized control to distributed routing. In DiSRouter, a query
traverses a network of LLM agents, each independently deciding whether to
answer or route to other agents based on its own self-awareness, its ability to
judge its competence. This distributed design offers superior flexibility,
scalability, and generalizability. To enable this, we propose a two-stage
Self-Awareness Training pipeline that enhances each LLM's self-awareness.
Extensive experiments demonstrate that DiSRouter significantly outperforms
existing routing methods in utility across various scenarios, effectively
distinguishes between easy and hard queries, and shows strong generalization to
out-of-domain tasks. Our work validates that leveraging an LLM's intrinsic
self-awareness is more effective than external assessment, paving the way for
more modular and efficient multi-agent systems.

</details>


### [31] [Modality Matching Matters: Calibrating Language Distances for Cross-Lingual Transfer in URIEL+](https://arxiv.org/abs/2510.19217)
*York Hay Ng,Aditya Khan,Xiang Lu,Matteo Salloum,Michael Zhou,Phuong H. Hoang,A. Seza Doğruöz,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 这篇论文提出了一种用于跨语言迁移的类型匹配语言距离框架，旨在克服现有语言知识库的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有语言知识库的向量表示不适用于不同的语言数据结构，并且缺乏将这些信号聚合成综合评分的有效方法。

Method: 论文提出了针对每种距离类型的新型结构感知表示：地理上的说话者加权分布，谱系上的双曲嵌入，以及类型学上的潜在变量模型。这些信号被统一成一个鲁棒的、任务无关的复合距离。

Result: 在选择迁移语言时，论文提出的表示和复合距离在各种NLP任务中持续提高性能。

Conclusion: 论文提供了一个更有效和有原则的多语言研究工具包。

Abstract: Existing linguistic knowledge bases such as URIEL+ provide valuable
geographic, genetic and typological distances for cross-lingual transfer but
suffer from two key limitations. One, their one-size-fits-all vector
representations are ill-suited to the diverse structures of linguistic data,
and two, they lack a principled method for aggregating these signals into a
single, comprehensive score. In this paper, we address these gaps by
introducing a framework for type-matched language distances. We propose novel,
structure-aware representations for each distance type: speaker-weighted
distributions for geography, hyperbolic embeddings for genealogy, and a latent
variables model for typology. We unify these signals into a robust,
task-agnostic composite distance. In selecting transfer languages, our
representations and composite distances consistently improve performance across
a wide range of NLP tasks, providing a more principled and effective toolkit
for multilingual research.

</details>


### [32] [SheetBrain: A Neuro-Symbolic Agent for Accurate Reasoning over Complex and Large Spreadsheets](https://arxiv.org/abs/2510.19247)
*Ziwei Wang,Jiayuan Su,Mengyu Zhou,Huaxing Zeng,Mengni Jia,Xiao Lv,Haoyu Dong,Xiaojun Ma,Shi Han,Dongmei Zhang*

Main category: cs.CL

TL;DR: SheetBrain是一个神经符号双重工作流代理框架，旨在提高大型语言模型在表格数据上的推理准确性，支持电子表格问答和操作任务。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型难以准确捕捉表格的复杂结构并保证推理的正确性。

Method: SheetBrain包含三个核心模块：理解模块（生成电子表格的综合概述）、执行模块（集成Python沙箱和Excel助手工具包）和验证模块（验证推理和答案的正确性）。

Result: SheetBrain在多个公共表格QA和操作基准测试中显著提高了准确性。

Conclusion: SheetBrain有效提高了在现有基准和更具挑战性的场景中的准确性。

Abstract: Understanding and reasoning over complex spreadsheets remain fundamental
challenges for large language models (LLMs), which often struggle with
accurately capturing the complex structure of tables and ensuring reasoning
correctness. In this work, we propose SheetBrain, a neuro-symbolic dual
workflow agent framework designed for accurate reasoning over tabular data,
supporting both spreadsheet question answering and manipulation tasks.
SheetBrain comprises three core modules: an understanding module, which
produces a comprehensive overview of the spreadsheet - including sheet summary
and query-based problem insight to guide reasoning; an execution module, which
integrates a Python sandbox with preloaded table-processing libraries and an
Excel helper toolkit for effective multi-turn reasoning; and a validation
module, which verifies the correctness of reasoning and answers, triggering
re-execution when necessary. We evaluate SheetBrain on multiple public tabular
QA and manipulation benchmarks, and introduce SheetBench, a new benchmark
targeting large, multi-table, and structurally complex spreadsheets.
Experimental results show that SheetBrain significantly improves accuracy on
both existing benchmarks and the more challenging scenarios presented in
SheetBench. Our code is publicly available at
https://github.com/microsoft/SheetBrain.

</details>


### [33] [Difficulty-Controllable Multiple-Choice Question Generation Using Large Language Models and Direct Preference Optimization](https://arxiv.org/abs/2510.19265)
*Yuto Tomikawa,Masaki Uto*

Main category: cs.CL

TL;DR: 本文提出了一种新的难度可控的多项选择题生成方法，用于阅读理解。


<details>
  <summary>Details</summary>
Motivation: 现有的神经问题生成方法无法直接生成多项选择题，并且在难度控制的准确性方面有待提高。

Method: 利用大型语言模型，使用直接偏好优化技术进行训练，以提高难度控制的准确性。

Result: 论文提出了一个新的方法。

Conclusion: 本文提出了一个新的难度可控的多项选择题生成方法。

Abstract: Difficulty-controllable question generation for reading comprehension has
gained significant attention in the field of education as a fundamental tool
for adaptive learning support. Although several neural question generation
methods have recently succeeded in controlling difficulty, conventional
approaches still face two major limitations. First, they cannot directly
generate multiple-choice questions, which are the most widely used question
type in educational contexts. Second, they are not explicitly trained to
optimize the accuracy of difficulty control, leaving room for further
improvement in difficulty controllability. To address these limitations, this
study proposes a novel difficulty-controllable multiple-choice question
generation method for reading comprehension which leverages a large language
model trained using a direct preference optimization technique to improve the
accuracy of difficulty control.

</details>


### [34] [TheMCPCompany: Creating General-purpose Agents with Task-specific Tools](https://arxiv.org/abs/2510.19286)
*Reza Esfandiarpoor,Vishwas Suryanarayanan,Stephen H. Bach,Vishal Chowdhary,Anthony Aue*

Main category: cs.CL

TL;DR: The paper introduces TheMCPCompany, a benchmark for evaluating tool-calling agents interacting with real-world services via REST APIs and MCP servers.


<details>
  <summary>Details</summary>
Motivation: Current agents rely on web browsers, but task-specific tools are easier to develop. The benchmark evaluates tool-calling agents in real-world service interactions.

Method: The authors created MCP servers with over 18,000 tools using REST APIs and provided manually annotated ground-truth tools for each task. They tested agent performance with ground truth tools and tool retrieval.

Result: Tool retrieval performs better than browser-based agents, but smaller models struggle with tool retrieval. GPT-5 performs well with tool retrieval, approaching the performance of using ground-truth tools. Advanced models struggle in complex environments.

Conclusion: Navigating and combining tens of thousands of tools to solve complex problems remains challenging, requiring better reasoning and retrieval models.

Abstract: Since the introduction of the Model Context Protocol (MCP), the number of
available tools for Large Language Models (LLMs) has increased significantly.
These task-specific tool sets offer an alternative to general-purpose tools
such as web browsers, while being easier to develop and maintain than GUIs.
However, current general-purpose agents predominantly rely on web browsers for
interacting with the environment. Here, we introduce TheMCPCompany, a benchmark
for evaluating tool-calling agents on tasks that involve interacting with
various real-world services. We use the REST APIs of these services to create
MCP servers, which include over 18,000 tools. We also provide manually
annotated ground-truth tools for each task. In our experiments, we use the
ground truth tools to show the potential of tool-calling agents for both
improving performance and reducing costs assuming perfect tool retrieval. Next,
we explore agent performance using tool retrieval to study the real-world
practicality of tool-based agents. While all models with tool retrieval perform
similarly or better than browser-based agents, smaller models cannot take full
advantage of the available tools through retrieval. On the other hand, GPT-5's
performance with tool retrieval is very close to its performance with
ground-truth tools. Overall, our work shows that the most advanced reasoning
models are effective at discovering tools in simpler environments, but
seriously struggle with navigating complex enterprise environments.
TheMCPCompany reveals that navigating tens of thousands of tools and combining
them in non-trivial ways to solve complex problems is still a challenging task
for current models and requires both better reasoning and better retrieval
models.

</details>


### [35] [JointCQ: Improving Factual Hallucination Detection with Joint Claim and Query Generation](https://arxiv.org/abs/2510.19310)
*Fan Xu,Huixuan Zhang,Zhenliang Zhang,Jiahao Wang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 提出 JointCQ 框架，用于联合生成声明和查询，以提高幻觉检测的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在声明提取和查询生成方面存在局限性，导致幻觉检测性能下降。

Method: 利用精心设计的评估标准过滤合成训练数据，并对语言模型进行微调，以实现联合声明提取和查询生成。

Result: 在多个开放域 QA 幻觉检测基准测试中，该方法优于以前的方法。

Conclusion: 该方法提高了语言模型系统的可信度和透明度。

Abstract: Current large language models (LLMs) often suffer from hallucination issues,
i,e, generating content that appears factual but is actually unreliable. A
typical hallucination detection pipeline involves response decomposition (i.e.,
claim extraction), query generation, evidence collection (i.e., search or
retrieval), and claim verification. However, existing methods exhibit
limitations in the first two stages, such as context loss during claim
extraction and low specificity in query generation, resulting in degraded
performance across the hallucination detection pipeline. In this work, we
introduce JointCQ https://github.com/pku0xff/JointCQ, a joint claim-and-query
generation framework designed to construct an effective and efficient
claim-query generator. Our framework leverages elaborately designed evaluation
criteria to filter synthesized training data, and finetunes a language model
for joint claim extraction and query generation, providing reliable and
informative inputs for downstream search and verification. Experimental results
demonstrate that our method outperforms previous methods on multiple
open-domain QA hallucination detection benchmarks, advancing the goal of more
trustworthy and transparent language model systems.

</details>


### [36] [KORE: Enhancing Knowledge Injection for Large Multimodal Models via Knowledge-Oriented Augmentations and Constraints](https://arxiv.org/abs/2510.19316)
*Kailin Jiang,Hongbo Jiang,Ning Jiang,Zhi Gao,Jinhe Bi,Yuchen Ren,Bin Li,Yuntao Du,Lei Liu,Qing Li*

Main category: cs.CL

TL;DR: KORE: A method for injecting new knowledge into large multimodal models while preserving old knowledge by using knowledge-oriented augmentations and constraints.


<details>
  <summary>Details</summary>
Motivation: Large Multimodal Models' knowledge is static and limited, unable to keep pace with real-world developments, which hinders continuous knowledge acquisition. Existing methods often struggle to learn new knowledge and suffer from catastrophic forgetting.

Method: A synergistic method of KnOwledge-oRientEd augmentations and constraints for injecting new knowledge into large multimodal models while preserving old knowledge. KORE automatically converts individual knowledge items into structured and comprehensive knowledge. KORE stores previous knowledge in the covariance matrix of LMM's linear layer activations and initializes the adapter by projecting the original weights into the matrix's null space.

Result: KORE achieves superior new knowledge injection performance and effectively mitigates catastrophic forgetting on LLaVA-v1.5-7B, LLaVA-v1.5-13B, and Qwen2.5-VL-7B.

Conclusion: KORE is effective for injecting new knowledge into large multimodal models while preserving old knowledge.

Abstract: Large Multimodal Models encode extensive factual knowledge in their
pre-trained weights. However, its knowledge remains static and limited, unable
to keep pace with real-world developments, which hinders continuous knowledge
acquisition. Effective knowledge injection thus becomes critical, involving two
goals: knowledge adaptation (injecting new knowledge) and knowledge retention
(preserving old knowledge). Existing methods often struggle to learn new
knowledge and suffer from catastrophic forgetting. To address this, we propose
KORE, a synergistic method of KnOwledge-oRientEd augmentations and constraints
for injecting new knowledge into large multimodal models while preserving old
knowledge. Unlike general text or image data augmentation, KORE automatically
converts individual knowledge items into structured and comprehensive knowledge
to ensure that the model accurately learns new knowledge, enabling accurate
adaptation. Meanwhile, KORE stores previous knowledge in the covariance matrix
of LMM's linear layer activations and initializes the adapter by projecting the
original weights into the matrix's null space, defining a fine-tuning direction
that minimizes interference with previous knowledge, enabling powerful
retention. Extensive experiments on various LMMs, including LLaVA-v1.5-7B,
LLaVA-v1.5-13B, and Qwen2.5-VL-7B, show that KORE achieves superior new
knowledge injection performance and effectively mitigates catastrophic
forgetting.

</details>


### [37] [HAD: HAllucination Detection Language Models Based on a Comprehensive Hallucination Taxonomy](https://arxiv.org/abs/2510.19318)
*Fan Xu,Xinyu Hu,Zhenghan Yu,Li Lin,Xu Zhang,Yang Zhang,Wei Zhou,Jinjie Gu,Xiaojun Wan*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种用于检测自然语言生成模型中幻觉现象（产生看似合理但不正确的信息）的方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成的文本可能包含不准确的信息，即产生幻觉。检测幻觉对于确保输出结果的可靠性至关重要。

Method: 提出了一个包含11个类别的幻觉分类体系，并开发了HAllucination Detection (HAD) 模型，该模型集成了幻觉检测、跨度识别和纠正功能。该模型在一个包含约9万个样本的合成数据集上进行训练。

Result: HAD模型在多个测试集上优于现有基线模型，并在HaluEval、FactCHD和FaithBench上取得了最佳结果。

Conclusion: HAD模型具有鲁棒性和通用性，能够有效地检测和纠正不同自然语言生成任务中的幻觉现象。

Abstract: The increasing reliance on natural language generation (NLG) models,
particularly large language models, has raised concerns about the reliability
and accuracy of their outputs. A key challenge is hallucination, where models
produce plausible but incorrect information. As a result, hallucination
detection has become a critical task. In this work, we introduce a
comprehensive hallucination taxonomy with 11 categories across various NLG
tasks and propose the HAllucination Detection (HAD) models
https://github.com/pku0xff/HAD, which integrate hallucination detection,
span-level identification, and correction into a single inference process.
Trained on an elaborate synthetic dataset of about 90K samples, our HAD models
are versatile and can be applied to various NLG tasks. We also carefully
annotate a test set for hallucination detection, called HADTest, which contains
2,248 samples. Evaluations on in-domain and out-of-domain test sets show that
our HAD models generally outperform the existing baselines, achieving
state-of-the-art results on HaluEval, FactCHD, and FaithBench, confirming their
robustness and versatility.

</details>


### [38] [Balancing Rewards in Text Summarization: Multi-Objective Reinforcement Learning via HyperVolume Optimization](https://arxiv.org/abs/2510.19325)
*Junjie Song,Yiwen Liu,Dapeng Li,Yin Sun,Shukun Fu,Siqi Chen,Yuji Cao*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型和强化学习的摘要优化方法，通过超体积优化（HVO）动态调整奖励，以平衡摘要的多个目标（一致性、连贯性、相关性和流畅性）。


<details>
  <summary>Details</summary>
Motivation: 现有的文本摘要方法难以同时优化多个目标，而基于强化学习的大语言模型在这方面的研究较少。

Method: 提出了超体积优化（HVO）方法，该方法在强化学习的奖励过程中动态调整组间分数，引导模型优化并逐步逼近帕累托前沿。

Result: 实验结果表明，HVO方法在总体得分上优于GRPO，并在不同维度上表现出更均衡的性能。使用HVO增强的7B基础模型在摘要任务中与GPT-4的性能相当，同时保持了更短的生成长度。

Conclusion: HVO方法能够有效地平衡摘要的多个目标，并在摘要任务中取得有竞争力的结果。

Abstract: Text summarization is a crucial task that requires the simultaneous
optimization of multiple objectives, including consistency, coherence,
relevance, and fluency, which presents considerable challenges. Although large
language models (LLMs) have demonstrated remarkable performance, enhanced by
reinforcement learning (RL), few studies have focused on optimizing the
multi-objective problem of summarization through RL based on LLMs. In this
paper, we introduce hypervolume optimization (HVO), a novel optimization
strategy that dynamically adjusts the scores between groups during the reward
process in RL by using the hypervolume method. This method guides the model's
optimization to progressively approximate the pareto front, thereby generating
balanced summaries across multiple objectives. Experimental results on several
representative summarization datasets demonstrate that our method outperforms
group relative policy optimization (GRPO) in overall scores and shows more
balanced performance across different dimensions. Moreover, a 7B foundation
model enhanced by HVO performs comparably to GPT-4 in the summarization task,
while maintaining a shorter generation length. Our code is publicly available
at https://github.com/ai4business-LiAuto/HVO.git

</details>


### [39] [Slot Filling as a Reasoning Task for SpeechLLMs](https://arxiv.org/abs/2510.19326)
*Kadri Hacioglu,Manjunath K E,Andreas Stolcke*

Main category: cs.CL

TL;DR: 本文提出将推理整合到语音大语言模型 (speechLLM) 中，用于端到端槽填充任务。


<details>
  <summary>Details</summary>
Motivation: 受推理LLM的启发，将槽填充任务分解为多个推理步骤。

Method: 使用思维链框架，创建推理数据集，并将监督微调策略应用于 speechLLM。

Result: 引入推理（中间）步骤可以提高性能。主要为数学、逻辑和编码领域开发的推理文本 LLM 作为推理语音 LLM 的基础模型可能较差。混合语音LLM，建立在混合文本基础 LLM 上，并经过微调以保留直接和推理操作模式，比那些仅采用一种操作模式进行微调的语音LLM 具有更好的性能。

Conclusion: 混合语音LLM性能最佳

Abstract: We propose integration of reasoning into speech large language models
(speechLLMs) for the end-to-end slot-filling task. Inspired by the recent
development of reasoning LLMs, we use a chain-of-thought framework to decompose
the slot-filling task into multiple reasoning steps, create a reasoning dataset
and apply the supervised fine-tuning strategy to a speechLLM. We distinguish
between regular and reasoning speechLLMs and experiment with different types
and sizes of LLMs as their text foundation models. We demonstrate performance
improvements by introducing reasoning (intermediate) steps. However, we show
that a reasoning textual LLM developed mainly for math, logic and coding
domains might be inferior as a foundation model for a reasoning speechLLM. We
further show that hybrid speechLLMs, built on a hybrid text foundation LLM and
fine-tuned to preserve both direct and reasoning modes of operation, have
better performance than those fine-tuned employing only one mode of operation.

</details>


### [40] [Algorithmic Fairness in NLP: Persona-Infused LLMs for Human-Centric Hate Speech Detection](https://arxiv.org/abs/2510.19331)
*Ewelina Gajewska,Arda Derbent,Jaroslaw A Chudziak,Katarzyna Budzynska*

Main category: cs.CL

TL;DR: 研究了使用注释者角色定制大型语言模型（Persona-LLM）如何影响其对仇恨言论的敏感性，特别是在注释者和目标之间共享或不同的身份相关的偏见。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过将社会人口属性融入LLM来解决自动化仇恨言论检测中的偏见问题，并桥接群体身份的心理学见解与先进的NLP技术。

Method: 采用Google的Gemini和OpenAI的GPT-4.1-mini模型，以及两种角色提示方法：浅层角色提示和基于检索增强生成（RAG）的深度情境化角色开发，以纳入更丰富的角色配置文件。分析了使用内群体和外群体注释者角色对模型在不同社会群体中的检测性能和公平性的影响。

Result: 结果强调了基于角色的方法在减少偏见方面的潜力和局限性。

Conclusion: 为开发更公平的仇恨言论检测系统提供了有价值的见解。

Abstract: In this paper, we investigate how personalising Large Language Models
(Persona-LLMs) with annotator personas affects their sensitivity to hate
speech, particularly regarding biases linked to shared or differing identities
between annotators and targets. To this end, we employ Google's Gemini and
OpenAI's GPT-4.1-mini models and two persona-prompting methods: shallow persona
prompting and a deeply contextualised persona development based on
Retrieval-Augmented Generation (RAG) to incorporate richer persona profiles. We
analyse the impact of using in-group and out-group annotator personas on the
models' detection performance and fairness across diverse social groups. This
work bridges psychological insights on group identity with advanced NLP
techniques, demonstrating that incorporating socio-demographic attributes into
LLMs can address bias in automated hate speech detection. Our results highlight
both the potential and limitations of persona-based approaches in reducing
bias, offering valuable insights for developing more equitable hate speech
detection systems.

</details>


### [41] [Local Obfuscation by GLINER for Impartial Context Aware Lineage: Development and evaluation of PII Removal system](https://arxiv.org/abs/2510.19346)
*Prakrithi Shivaprakash,Lekhansh Shukla,Animesh Mukherjee,Prabhat Chand,Pratima Murthy*

Main category: cs.CL

TL;DR: 本研究开发了一种名为LOGICAL的本地PII移除系统，该系统基于微调的GLiNER模型，并在性能上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 在电子健康记录(EHRs)中移除个人身份信息(PII)对于研究和AI开发至关重要。大型语言模型(LLMs)虽然强大，但其高计算成本和API服务的数据隐私风险限制了它们的使用，尤其是在低资源环境中。

Method: 通过在精神病医院的EHR系统中使用了1515份临床文件，定义了九个PII类别用于移除。在2849个文本实例上微调了一个modern-gliner-bi-large-v1.0模型，并在376个实例的测试集上进行了评估，使用了字符级别的精确率、召回率和F1分数。将其性能与Microsoft Azure NER、Microsoft Presidio以及使用Gemini-Pro-2.5和Llama-3.3-70B-Instruct的zero-shot prompting进行了比较。

Result: 微调后的GLiNER模型取得了优异的性能，总体微平均F1分数为0.980，显著优于Gemini-Pro-2.5(F1分数为0.845)。LOGICAL正确地完全清理了95%的文档，而次优解决方案为64%。该模型在没有专用GPU的标准笔记本电脑上高效运行。

Conclusion: 像GLiNER这样经过微调的专业transformer模型为从临床笔记中删除PII提供了一种准确、计算高效且安全的解决方案。这种“源头清理”方法是一种实用的替代方案，可以替代资源密集型LLM，从而能够创建用于研究和AI开发的去标识化数据集，同时保护数据隐私，尤其是在资源受限的环境中。

Abstract: Removing Personally Identifiable Information (PII) from clinical notes in
Electronic Health Records (EHRs) is essential for research and AI development.
While Large Language Models (LLMs) are powerful, their high computational costs
and the data privacy risks of API-based services limit their use, especially in
low-resource settings. To address this, we developed LOGICAL (Local Obfuscation
by GLINER for Impartial Context-Aware Lineage), an efficient, locally
deployable PII removal system built on a fine-tuned Generalist and Lightweight
Named Entity Recognition (GLiNER) model. We used 1515 clinical documents from a
psychiatric hospital's EHR system. We defined nine PII categories for removal.
A modern-gliner-bi-large-v1.0 model was fine-tuned on 2849 text instances and
evaluated on a test set of 376 instances using character-level precision,
recall, and F1-score. We compared its performance against Microsoft Azure NER,
Microsoft Presidio, and zero-shot prompting with Gemini-Pro-2.5 and
Llama-3.3-70B-Instruct. The fine-tuned GLiNER model achieved superior
performance, with an overall micro-average F1-score of 0.980, significantly
outperforming Gemini-Pro-2.5 (F1-score: 0.845). LOGICAL correctly sanitised 95%
of documents completely, compared to 64% for the next-best solution. The model
operated efficiently on a standard laptop without a dedicated GPU. However, a
2% entity-level false negative rate underscores the need for human-in-the-loop
validation across all tested systems. Fine-tuned, specialised transformer
models like GLiNER offer an accurate, computationally efficient, and secure
solution for PII removal from clinical notes. This "sanitisation at the source"
approach is a practical alternative to resource-intensive LLMs, enabling the
creation of de-identified datasets for research and AI development while
preserving data privacy, particularly in resource-constrained environments.

</details>


### [42] [Modeling Turn-Taking with Semantically Informed Gestures](https://arxiv.org/abs/2510.19350)
*Varsha Suresh,M. Hamza Mughal,Christian Theobalt,Vera Demberg*

Main category: cs.CL

TL;DR: 论文研究了对话中手势在轮流转换中的作用，并引入了一个扩展的语料库DnD Gesture++，其中包含2663个语义手势注释。


<details>
  <summary>Details</summary>
Motivation: 研究人类在对话中如何利用多模态线索（如语音、手势和眼神）来管理轮流转换，尤其关注手势的补充作用。

Method: 通过一个混合专家框架，整合文本、音频和手势，用于预测轮流转换。

Result: 实验表明，结合语义引导的手势可以持续提高性能。

Conclusion: 手势在多模态轮流转换中起着补充作用。

Abstract: In conversation, humans use multimodal cues, such as speech, gestures, and
gaze, to manage turn-taking. While linguistic and acoustic features are
informative, gestures provide complementary cues for modeling these
transitions. To study this, we introduce DnD Gesture++, an extension of the
multi-party DnD Gesture corpus enriched with 2,663 semantic gesture annotations
spanning iconic, metaphoric, deictic, and discourse types. Using this dataset,
we model turn-taking prediction through a Mixture-of-Experts framework
integrating text, audio, and gestures. Experiments show that incorporating
semantically guided gestures yields consistent performance gains over
baselines, demonstrating their complementary role in multimodal turn-taking.

</details>


### [43] [M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2510.19358)
*Yejin Kwon,Taewoo Kang,Hyunsoo Yoon,Changouk Kim*

Main category: cs.CL

TL;DR: 提出了一个新的多模态大型语言模型（MLLM）基准测试M3-SLU，用于评估多说话人、多轮口语理解。


<details>
  <summary>Details</summary>
Motivation: 现有的模型在语音和文本理解方面表现出色，但在说话人属性推理方面存在不足，即理解谁在何时说了什么。

Method: M3-SLU由四个开放语料库构建，包含超过12,000个验证实例，具有配对的音频、文本和元数据。它包括两个任务：(1)说话人属性问答和(2)通过话语匹配进行说话人属性识别。

Result: 结果表明，模型可以捕捉到说了什么，但通常无法识别是谁说的，这揭示了在说话人感知的对话理解方面存在一个关键差距。

Conclusion: M3-SLU作为一个具有挑战性的基准，可以促进说话人感知的多模态理解方面的研究。

Abstract: We present M3-SLU, a new multimodal large language model (MLLM) benchmark for
evaluating multi-speaker, multi-turn spoken language understanding. While
recent models show strong performance in speech and text comprehension, they
still struggle with speaker-attributed reasoning, the ability to understand who
said what and when in natural conversations. M3-SLU is built from four open
corpora (CHiME-6, MELD, MultiDialog, and AMI) and comprises over 12,000
validated instances with paired audio, transcripts, and metadata. It includes
two tasks: (1) Speaker-Attributed Question Answering and (2) Speaker
Attribution via Utterance Matching. We provide baseline results for both
cascaded pipelines and end-to-end MLLMs, evaluated using an LLM-as-Judge and
accuracy metrics. Results show that while models can capture what was said,
they often fail to identify who said it, revealing a key gap in speaker-aware
dialogue understanding. M3-SLU offers as a challenging benchmark to advance
research in speaker-aware multimodal understanding.

</details>


### [44] [AgenticMath: Enhancing LLM Reasoning via Agentic-based Math Data Generation](https://arxiv.org/abs/2510.19361)
*Xianyang Liu,Yilin Liu,Shuai Wang,Hao Cheng,Andrew Estornell,Yuzhi Zhao,Jiaheng Wei*

Main category: cs.CL

TL;DR: AgenticMath: A novel agentic pipeline for generating high-quality mathematical question-answer pairs to enhance LLM fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The creation of high-quality datasets to improve LLM reasoning remains a significant challenge, as current methods often suffer from generating low-quality/incorrect answers and limited information richness from available data sources.

Method: A four-stage pipeline: (1) Seed Question Filter; (2) Agentic Question Rephrase; (3) Answer Augment with chain-of-thought reasoning; (4) Question and Answer Evaluation.

Result: Fine-tuning 3B-8B parameter LLMs on AgenticMath generated datasets achieves competitive or superior performance on mathematical reasoning benchmarks compared to baselines trained on much more data.

Conclusion: Targeted, high-quality data generation is a more efficient path to improving mathematical reasoning in LLMs than large-scale, low-quality alternatives.

Abstract: The creation of high-quality datasets to improve Large Language Model (LLM)
reasoning remains a significant challenge, as current methods often suffer from
generating low-quality/incorrect answers and limited information richness from
available data sources. To address this, we propose AgenticMath, a novel
agentic pipeline for generating high-quality mathematical question-answer pairs
to enhance the supervised fine-tuning of LLMs. Our method operates through four
stages: (1) Seed Question Filter that selects questions with high information
richness, complexity, and clarity; (2) an Agentic Question Rephrase step that
employs a multi-agent system to generate diverse, logically consistent
paraphrases; (3) an Answer Augment step where rewrite answers using
chain-of-thought reasoning to enhance numerical and logical correctness,
without reliance on human-provided labels; and (4) a final Question and Answer
Evaluation that retains only the most superior pairs. Extensive experiments
demonstrate that, fine-tuning 3B-8B parameter LLMs on AgenticMath generated
datasets (comprising only 30-60K math samples) achieves competitive or superior
performance on diverse in domain and out-of-domain mathematical reasoning
benchmarks compared to baselines trained on much more data (e.g., 400K or 2.3M
samples). Our work demonstrates that targeted, high-quality data generation is
a more efficient path to improving mathematical reasoning in LLMs than
large-scale, low-quality alternatives.

</details>


### [45] [LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts](https://arxiv.org/abs/2510.19363)
*Siyuan Wang,Gaokai Zhang,Li Lyna Zhang,Ning Shang,Fan Yang,Dongyao Chen,Mao Yang*

Main category: cs.CL

TL;DR: LoongRL, a data-driven RL method, enhances long-context reasoning in large language models by transforming short multi-hop QA into high-difficulty long-context tasks using KeyChain, which involves UUID chains to hide the true question among distracting documents. This induces a plan-retrieve-reason-recheck pattern, improving performance on long-context multi-hop QA and retrieval, and rivaling larger models.


<details>
  <summary>Details</summary>
Motivation: Advanced thinking patterns for long-context reasoning are largely unexplored, and high-difficulty RL data are scarce.

Method: KeyChain, a synthesis approach that transforms short multi-hop QA into high-difficulty long-context tasks by inserting UUID chains that hide the true question among large collections of distracting documents. RL training on KeyChain data induces an emergent plan-retrieve-reason-recheck reasoning pattern.

Result: LoongRL substantially improves long-context multi-hop QA accuracy by +23.5% and +21.1% absolute gains on Qwen2.5-7B and 14B, respectively. LoongRL-14B rivals much larger models and improves long-context retrieval and passes needle-in-a-haystack stress tests.

Conclusion: LoongRL effectively solves long-context tasks by inducing a plan-retrieve-reason-recheck reasoning pattern, generalizing beyond training length and preserving short-context reasoning capabilities.

Abstract: Reasoning over long contexts is essential for large language models. While
reinforcement learning (RL) enhances short-context reasoning by inducing "Aha"
moments in chain-of-thought, the advanced thinking patterns required for
long-context reasoning remain largely unexplored, and high-difficulty RL data
are scarce. In this paper, we introduce LoongRL, a data-driven RL method for
advanced long-context reasoning. Central to LoongRL is KeyChain, a synthesis
approach that transforms short multi-hop QA into high-difficulty long-context
tasks by inserting UUID chains that hide the true question among large
collections of distracting documents. Solving these tasks requires the model to
trace the correct chain step-by-step, identify the true question, retrieve
relevant facts and reason over them to answer correctly. RL training on
KeyChain data induces an emergent plan-retrieve-reason-recheck reasoning
pattern that generalizes far beyond training length. Models trained at 16K
effectively solve 128K tasks without prohibitive full-length RL rollout costs.
On Qwen2.5-7B and 14B, LoongRL substantially improves long-context multi-hop QA
accuracy by +23.5% and +21.1% absolute gains. The resulting LoongRL-14B reaches
a score of 74.2, rivaling much larger frontier models such as o3-mini (74.5)
and DeepSeek-R1 (74.9). It also improves long-context retrieval, passes all
128K needle-in-a-haystack stress tests, and preserves short-context reasoning
capabilities.

</details>


### [46] [MoE-Prism: Disentangling Monolithic Experts for Elastic MoE Services via Model-System Co-Designs](https://arxiv.org/abs/2510.19366)
*Xinfeng Xia,Jiacheng Liu,Xiaofeng Hou,Peng Tang,Mingxuan Zhang,Wenfeng Wang,Chao Li*

Main category: cs.CL

TL;DR: MoE-Prism transforms rigid MoE models into elastic services by deconstructing monolithic experts into fine-grained sub-experts and using QoS-aware scheduling.


<details>
  <summary>Details</summary>
Motivation: Current Mixture-of-Experts (MoE) models have a 'quality cliff' due to their reliance on routing between a few monolithic experts, leading to inflexibility and resource over-provisioning.

Method: The paper introduces MoE-Prism, a model-system co-design with an Offline Refactoring Engine that deconstructs experts into sub-experts using a partitioning optimization solver and an Online Scheduling Engine that leverages this elasticity through QoS-aware scheduling.

Result: MoE-Prism provides over 4 times more distinct, stable operating points than the baseline, improving throughput by up to 19.9% or reducing latency by up to 10.36%.

Conclusion: MoE-Prism bridges the model-system gap, enabling adaptive, efficient, and QoS-aware AI services.

Abstract: Mixture-of-Experts (MoE) models, the state-of-the-art in large-scale AI,
achieve high quality by sparsely activating parameters. However, their reliance
on routing between a few monolithic experts via a top-k mechanism creates a
"quality cliff", offering only a few coarse-grained operating points. This
inflexibility forces a difficult trade-off between cost and quality, preventing
adaptation to diverse Service Level Objectives (SLOs) and leading to
significant resource over-provisioning.
  This paper introduces MoE-Prism, a model-system co-design that transforms
rigid MoE models into elastic services. Our methodology is divided into two
phases. First, an \emph{Offline Refactoring Engine} systematically deconstructs
monolithic experts into fine-grained "sub-experts." This engine employs a
partitioning optimization solver that uses a metaheuristic-based approach to
group neurons, preserving functional locality without requiring retraining.
Second, an \emph{Online Scheduling Engine} leverages this new elasticity
through QoS-aware scheduling. It implements specialized policies to solve
complex system problems, including maximizing throughput in cloud deployments
and managing latency-optimized offloading for memory-constrained devices. Our
evaluation across three different MoE models shows that MoE-Prismprovides over
4 times more distinct, stable operating points than the baseline. This allows
an AI service to dynamically improve throughput by up to 19.9\% under a strict
latency budget or reduce latency by up to 10.36\% under limited resources.
MoE-Prism provides the critical "control knob" to bridge the model-system gap,
enabling the next generation of adaptive, efficient, and QoS-aware AI services.

</details>


### [47] [Sign Language Translation with Sentence Embedding Supervision](https://arxiv.org/abs/2510.19367)
*Yasser Hamidullah,Josef van Genabith,Cristina España-Bonet*

Main category: cs.CL

TL;DR: 本文提出了一种新的手语翻译方法，该方法使用目标句子的句子嵌入在训练时扮演 gloss 的角色，无需任何手动注释，而是在原始文本数据上学习。


<details>
  <summary>Details</summary>
Motivation: 现有手语翻译系统依赖于 gloss 注释，但 gloss 标注的手语数据通常无法大规模获得，并且不同数据集之间的 gloss 注释差异很大。

Method: 使用目标句子的句子嵌入在训练时扮演 gloss 的角色。

Result: 该方法显著优于其他无 gloss 方法，为没有 gloss 的数据集设置了新的技术水平，并缩小了无 gloss 和依赖 gloss 的系统之间的差距。

Conclusion: 该方法在无 gloss 手语翻译中表现出色，缩小了与依赖 gloss 的系统的差距。

Abstract: State-of-the-art sign language translation (SLT) systems facilitate the
learning process through gloss annotations, either in an end2end manner or by
involving an intermediate step. Unfortunately, gloss labelled sign language
data is usually not available at scale and, when available, gloss annotations
widely differ from dataset to dataset. We present a novel approach using
sentence embeddings of the target sentences at training time that take the role
of glosses. The new kind of supervision does not need any manual annotation but
it is learned on raw textual data. As our approach easily facilitates
multilinguality, we evaluate it on datasets covering German (PHOENIX-2014T) and
American (How2Sign) sign languages and experiment with mono- and multilingual
sentence embeddings and translation systems. Our approach significantly
outperforms other gloss-free approaches, setting the new state-of-the-art for
data sets where glosses are not available and when no additional SLT datasets
are used for pretraining, diminishing the gap between gloss-free and
gloss-dependent systems.

</details>


### [48] [SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision](https://arxiv.org/abs/2510.19398)
*Yasser Hamidullah,Shakib Yazdani,Cennet Oguz,Josef van Genabith,Cristina España-Bonet*

Main category: cs.CL

TL;DR: 本文提出了一种使用语言无关的多模态嵌入来监督手语翻译（SLT）的方法，以实现直接的多语言翻译。


<details>
  <summary>Details</summary>
Motivation: 以往的SLT通常使用单一口语文本进行训练，限制了可扩展性和跨语言泛化能力。本文旨在解决这个问题。

Method: 本文采用在多种语言的文本和语音上训练的语言无关、多模态嵌入来监督SLT，并提出了一种结合多语言目标增强和视频级扰动的耦合增强方法，以解决数据稀缺问题。

Result: 实验结果表明，与仅使用文本的句子嵌入监督相比，BLEURT指标有所提高，尤其是在低资源环境中。

Conclusion: 语言无关的嵌入监督与耦合增强相结合，为传统的SLT训练提供了一种可扩展且语义鲁棒的替代方案。

Abstract: Sign language translation (SLT) is typically trained with text in a single
spoken language, which limits scalability and cross-language generalization.
Earlier approaches have replaced gloss supervision with text-based sentence
embeddings, but up to now, these remain tied to a specific language and
modality. In contrast, here we employ language-agnostic, multimodal embeddings
trained on text and speech from multiple languages to supervise SLT, enabling
direct multilingual translation. To address data scarcity, we propose a coupled
augmentation method that combines multilingual target augmentations (i.e.
translations into many languages) with video-level perturbations, improving
model robustness. Experiments show consistent BLEURT gains over text-only
sentence embedding supervision, with larger improvements in low-resource
settings. Our results demonstrate that language-agnostic embedding supervision,
combined with coupled augmentation, provides a scalable and semantically robust
alternative to traditional SLT training.

</details>


### [49] [ToMMeR -- Efficient Entity Mention Detection from Large Language Models](https://arxiv.org/abs/2510.19410)
*Victor Morand,Nadi Tomeh,Josiane Mothe,Benjamin Piwowarski*

Main category: cs.CL

TL;DR: ToMMeR is a lightweight model that identifies text spans referring to entities with high recall and precision.


<details>
  <summary>Details</summary>
Motivation: Mention detection is foundational for information extraction but suffers from performance bottlenecks.

Method: Probing mention detection capabilities from early LLM layers using ToMMeR, a lightweight model.

Result: ToMMeR achieves 93% recall zero-shot and over 90% precision. Diverse architectures converge on similar mention boundaries. ToMMeR achieves near SOTA NER performance when extended with span classification heads.

Conclusion: Structured entity representations exist in early transformer layers and can be efficiently recovered with minimal parameters.

Abstract: Identifying which text spans refer to entities -- mention detection -- is
both foundational for information extraction and a known performance
bottleneck. We introduce ToMMeR, a lightweight model (<300K parameters) probing
mention detection capabilities from early LLM layers. Across 13 NER benchmarks,
ToMMeR achieves 93\% recall zero-shot, with over 90\% precision using an LLM as
a judge showing that ToMMeR rarely produces spurious predictions despite high
recall. Cross-model analysis reveals that diverse architectures (14M-15B
parameters) converge on similar mention boundaries (DICE >75\%), confirming
that mention detection emerges naturally from language modeling. When extended
with span classification heads, ToMMeR achieves near SOTA NER performance
(80-87\% F1 on standard benchmarks). Our work provides evidence that structured
entity representations exist in early transformer layers and can be efficiently
recovered with minimal parameters.

</details>


### [50] [Spatio-temporal Sign Language Representation and Translation](https://arxiv.org/abs/2510.19413)
*Yasser Hamidullah,Josef van Genabith,Cristina España-Bonet*

Main category: cs.CL

TL;DR: DFKI-MLT 参加了 WMT-SLT 2022 的手语翻译任务，将瑞士德语手语（视频）翻译成德语（文本）。


<details>
  <summary>Details</summary>
Motivation: 目前最先进的手语翻译技术使用通用的 seq2seq 架构和定制的输入嵌入，但通常不能从时间特征中获益。

Method: 该系统在一个模型中学习时空特征表示和翻译，形成真正的端到端架构。

Result: 在开发集上获得了 5±1 BLEU 分，但在测试集上性能下降到 0.11±0.06 BLEU 分。

Conclusion: 该系统在开发集上表现良好，但在测试集上性能不佳。

Abstract: This paper describes the DFKI-MLT submission to the WMT-SLT 2022 sign
language translation (SLT) task from Swiss German Sign Language (video) into
German (text). State-of-the-art techniques for SLT use a generic seq2seq
architecture with customized input embeddings. Instead of word embeddings as
used in textual machine translation, SLT systems use features extracted from
video frames. Standard approaches often do not benefit from temporal features.
In our participation, we present a system that learns spatio-temporal feature
representations and translation in a single model, resulting in a real
end-to-end architecture expected to better generalize to new data sets. Our
best system achieved $5\pm1$ BLEU points on the development set, but the
performance on the test dropped to $0.11\pm0.06$ BLEU points.

</details>


### [51] [BLiSS 1.0: Evaluating Bilingual Learner Competence in Second Language Small Language Models](https://arxiv.org/abs/2510.19419)
*Yuan Gao,Suchir Salhan,Andrew Caines,Paula Buttery,Weiwei Sun*

Main category: cs.CL

TL;DR: BLiSS 1.0：一个用于评估认知模型的基准，通过比较模型对自然学习者错误和人工错误的容忍度来测试其对人类语言学习模式的对齐程度。


<details>
  <summary>Details</summary>
Motivation: 弥合面向性能的基准与认知模型评估之间的差距。

Method: 构建了一个包含超过 280 万个自然学习者句子的基准，BLiSS 1.0，并提供了 136,867 个受控三元组（已更正、学习者、人工），用于测试模型是否发现自然学习者错误比相同句子中的匹配人工错误更合理。

Result: 对各种模型的实验表明，选择性容忍度是一种不同于标准语法性的能力，并且性能与训练范式密切相关。

Conclusion: BLiSS 是一个可靠的工具，可用于衡量不同的训练目标如何影响模型与人类语言习得系统模式的对齐。

Abstract: To bridge the gap between performance-oriented benchmarks and the evaluation
of cognitively inspired models, we introduce BLiSS 1.0, a Benchmark of Learner
Interlingual Syntactic Structure. Our benchmark operationalizes a new paradigm
of selective tolerance, testing whether a model finds a naturalistic learner
error more plausible than a matched, artificial error within the same sentence.
Constructed from over 2.8 million naturalistic learner sentences, BLiSS
provides 136,867 controlled triplets (corrected, learner, artificial) for this
purpose. Experiments on a diverse suite of models demonstrate that selective
tolerance is a distinct capability from standard grammaticality, with
performance clustering strongly by training paradigm. This validates BLiSS as a
robust tool for measuring how different training objectives impact a model's
alignment with the systematic patterns of human language acquisition.

</details>


### [52] [MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for Large Multimodal Models](https://arxiv.org/abs/2510.19457)
*Kailin Jiang,Ning Jiang,Yuchen Ren,Yuchen Li,Yifan Gao,Jinhe Bi,Yunpu Ma,Qingqing Liu,Xianhao Wang,Yifan Jia,Hongbo Jiang,Yaocong Hu,Bin Li,Lei Liu,Yuntao Du*

Main category: cs.CL

TL;DR: 论文提出了一个名为MINED的综合基准，用于评估大型多模态模型（LMMs）的时间感知能力。该基准包含6个关键维度和11个挑战性任务，涵盖2104个时间敏感的知识样本。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估LMMs理解时间敏感知识的能力方面存在不足，因为它们受限于静态设计。

Method: 论文构建了MINED基准，该基准包含来自维基百科的时间敏感知识样本，并由两位专业注释员进行注释。论文还评估了15个广泛使用的LMMs在MINED上的表现，并研究了通过知识编辑方法更新LMMs中时间敏感知识的可行性。

Result: Gemini-2.5-Pro在MINED上取得了最高的平均CEM评分（63.07），但大多数开源LMMs仍然缺乏时间理解能力。LMMs在组织知识方面表现最佳，而在体育方面的表现最差。知识编辑方法可以在单次编辑场景中有效地更新LMMs中的知识。

Conclusion: 论文表明，LMMs可以通过知识编辑方法有效地更新知识，但仍需要在时间理解能力方面进行改进。

Abstract: Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal
pre-training, yet their static representations struggle to maintain an accurate
understanding of time-sensitive factual knowledge. Existing benchmarks remain
constrained by static designs, inadequately evaluating LMMs' ability to
understand time-sensitive knowledge. To address this gap, we propose MINED, a
comprehensive benchmark that evaluates temporal awareness along 6 key
dimensions and 11 challenging tasks: cognition, awareness, trustworthiness,
understanding, reasoning, and robustness. MINED is constructed from Wikipedia
by two professional annotators, containing 2,104 time-sensitive knowledge
samples spanning six knowledge types. Evaluating 15 widely used LMMs on MINED
shows that Gemini-2.5-Pro achieves the highest average CEM score of 63.07,
while most open-source LMMs still lack time understanding ability. Meanwhile,
LMMs perform best on organization knowledge, whereas their performance is
weakest on sport. To address these challenges, we investigate the feasibility
of updating time-sensitive knowledge in LMMs through knowledge editing methods
and observe that LMMs can effectively update knowledge via knowledge editing
methods in single editing scenarios.

</details>


### [53] [Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition](https://arxiv.org/abs/2510.19471)
*Yuu Jinnai*

Main category: cs.CL

TL;DR: 本文研究了MBR解码在语音转录任务中的应用，发现它在离线语音识别和语音翻译任务中优于传统的束搜索方法。


<details>
  <summary>Details</summary>
Motivation: MBR解码在文本生成任务中表现出色，但尚未在语音转录任务中得到充分评估。本文旨在探索MBR解码在语音识别和语音翻译任务中的潜力。

Method: 本文使用Whisper及其衍生模型，在英语和日语的语音识别和语音翻译任务上评估了MBR解码的性能，并与束搜索进行了比较。

Result: 实验结果表明，在大多数实验设置中，MBR解码的准确率优于束搜索。

Conclusion: MBR解码是一种有前景的离线语音识别和语音翻译方法，尤其适用于需要高准确率的场景。

Abstract: Recent work has shown that sample-based Minimum Bayes Risk (MBR) decoding
outperforms beam search in text-to-text generation tasks, such as machine
translation, text summarization, and image captioning. On the other hand, beam
search is the current practice for speech-to-text tasks such as automatic
speech recognition (ASR) and Speech Translation (ST). Given that MBR decoding
is effective in text-to-text generation tasks, it is reasonable to expect it to
also be effective for speech-to-text tasks. In this paper, we evaluate MBR
decoding for ASR and ST tasks on English and Japanese using Whisper and its
derivative models. We observe that the accuracy of MBR decoding outperforms
that of beam search in most of the experimental settings we have evaluated. The
results show that MBR decoding is a promising method for offline ASR and ST
tasks that require high accuracy. The code is available at
https://github.com/CyberAgentAILab/mbr-for-asr

</details>


### [54] [VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos](https://arxiv.org/abs/2510.19488)
*Dunjie Lu,Yiheng Xu,Junli Wang,Haoyuan Wu,Xinyuan Wang,Zekun Wang,Junlin Yang,Hongjin Su,Jixuan Chen,Junda Chen,Yuchen Mao,Jingren Zhou,Junyang Lin,Binyuan Hui,Tao Yu*

Main category: cs.CL

TL;DR: VideoAgentTrek是一个自动从公开的屏幕录制视频中挖掘训练数据的流水线，无需手动标注。


<details>
  <summary>Details</summary>
Motivation: 训练计算机使用代理需要大量的GUI交互数据，但是手动注释动作轨迹的成本过高。

Method: 该方法包含一个Video2Action模块，该模块具有视频定位模型和动作内容识别器，可精确定位GUI动作并提取结构化参数。

Result: 在39,000个YouTube教程视频上应用后，该流水线自动生成152万个交互步骤。在OSWorld-Verified上，任务成功率从9.3%提高到15.8%，相对提高了70%。在AgentNetBench上，步骤准确率从64.1%提高到69.3%。

Conclusion: 被动互联网视频可以转化为高质量的计算机使用代理的监督，为昂贵的手动注释提供了一种可扩展的替代方案。

Abstract: Training computer-use agents requires massive amounts of GUI interaction
data, but manually annotating action trajectories at scale is prohibitively
expensive. We present VideoAgentTrek, a scalable pipeline that automatically
mines training data from publicly available screen-recorded videos at web
scale, eliminating the need for manual annotation. Our approach addresses a key
challenge: raw videos contain implicit demonstrations but lack explicit action
labels. To solve this, we develop Video2Action, an inverse dynamics module
(IDM) with two components: (1) a video grounding model that detects and
localizes GUI actions with precise temporal boundaries and context, and (2) an
action-content recognizer that extracts structured parameters like click
coordinates and typed text with high fidelity. Applied to 39,000 YouTube
tutorial videos, our pipeline generates 1.52 million interaction steps
automatically. We leverage this data through continued pretraining followed by
supervised fine-tuning. On OSWorld-Verified, our approach improves task success
rates from 9.3% (SFT-only baseline) to 15.8%, a 70% relative improvement. On
AgentNetBench, step accuracy increases from 64.1% to 69.3%. Our results
demonstrate that passive internet videos can be transformed into high-quality
supervision for computer-use agents, providing a scalable alternative to
expensive manual annotation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [55] [Dimensionality Reduction for Remote Sensing Data Analysis: A Systematic Review of Methods and Applications](https://arxiv.org/abs/2510.18935)
*Nathan Mankovich,Kai-Hendrik Cohrs,Homer Durand,Vasileios Sitokonstantinou,Tristan Williams,Gustau Camps-Valls*

Main category: cs.CV

TL;DR: 这篇论文综述了地球观测领域中降维技术的应用，强调了降维技术在处理高维遥感数据中的重要性，并为未来研究提出了方向。


<details>
  <summary>Details</summary>
Motivation: 地球观测数据量巨大，自动信息提取至关重要，但高维度带来了稀疏性、低效率和维度灾难等挑战，限制了机器学习模型的效果。

Method: 本文重点关注特征提取方法，通过降低复杂性来保持重要的数据属性，并提升数据压缩、清洗、融合、可视化、异常检测和预测等任务的性能。

Result: 本文为在遥感数据价值链中利用降维技术提供了指导，并确定了未充分探索的降维算法及其在未来研究中的应用机会。

Conclusion: 降维技术通过减少数据维度、保留关键信息，能够有效应对地球观测数据处理中的挑战，具有重要的应用价值和研究前景。

Abstract: Earth observation involves collecting, analyzing, and processing an
ever-growing mass of data. Automatically harvesting information is crucial for
addressing significant societal, economic, and environmental challenges,
ranging from environmental monitoring to urban planning and disaster
management. However, the high dimensionality of these data poses challenges in
terms of sparsity, inefficiency, and the curse of dimensionality, which limits
the effectiveness of machine learning models. Dimensionality reduction (DR)
techniques, specifically feature extraction, address these challenges by
preserving essential data properties while reducing complexity and enhancing
tasks such as data compression, cleaning, fusion, visualization, anomaly
detection, and prediction. This review provides a handbook for leveraging DR
across the RS data value chain and identifies opportunities for under-explored
DR algorithms and their application in future research.

</details>


### [56] [Ninja Codes: Neurally Generated Fiducial Markers for Stealthy 6-DoF Tracking](https://arxiv.org/abs/2510.18976)
*Yuichiro Takeuchi,Yusuke Imoto,Shunya Kato*

Main category: cs.CV

TL;DR: Ninja Codes: 神经生成的信标，自然融入现实环境，实现隐蔽的6自由度定位跟踪。


<details>
  <summary>Details</summary>
Motivation: 传统信标外观显眼，在美学等方面不受欢迎。

Method: 使用受深度隐写术启发的端到端流程，联合训练生成和检测Ninja Codes的网络模块。

Result: 在常见室内光照条件下，Ninja Codes能够在隐藏自身的同时提供可靠的定位跟踪。

Conclusion: Ninja Codes在传统信标外观不受欢迎的场景中具有特殊价值。

Abstract: In this paper we describe Ninja Codes, neurally-generated fiducial markers
that can be made to naturally blend into various real-world environments. An
encoder network converts arbitrary images into Ninja Codes by applying visually
modest alterations; the resulting codes, printed and pasted onto surfaces, can
provide stealthy 6-DoF location tracking for a wide range of applications
including augmented reality, robotics, motion-based user interfaces, etc. Ninja
Codes can be printed using off-the-shelf color printers on regular printing
paper, and can be detected using any device equipped with a modern RGB camera
and capable of running inference. Using an end-to-end process inspired by prior
work on deep steganography, we jointly train a series of network modules that
perform the creation and detection of Ninja Codes. Through experiments, we
demonstrate Ninja Codes' ability to provide reliable location tracking under
common indoor lighting conditions, while successfully concealing themselves
within diverse environmental textures. We expect Ninja Codes to offer
particular value in scenarios where the conspicuous appearances of conventional
fiducial markers make them undesirable for aesthetic and other reasons.

</details>


### [57] [Robust Driving QA through Metadata-Grounded Context and Task-Specific Prompts](https://arxiv.org/abs/2510.19001)
*Seungjun Yu,Junsung Park,Youngsun Lim,Hyunjung Shim*

Main category: cs.CV

TL;DR: 提出一个双阶段视觉语言问答系统，用于自动驾驶，回答高层次的感知、预测和规划问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决自动驾驶中高层次的理解和推理问题，利用视觉语言模型回答相关问题。

Method: 第一阶段：使用大型多模态LLM（Qwen2.5-VL-32B），输入六个摄像头图像、历史信息和带有少量示例的思维链提示，并使用自洽性集成提高可靠性。第二阶段：使用nuScenes场景元数据（对象注释、自车状态等）和特定类别的问题指令增强提示。

Result: 在驾驶问答基准测试中，该方法显著优于基线Qwen2.5模型。例如，在第一阶段使用5个历史帧和10-shot提示，总体准确率达到65.1%（零样本为62.61%）；应用自洽性将其提高到66.85%。第二阶段达到67.37%的总体准确率。在严重的视觉损坏下，系统保持96%的准确率。

Conclusion: 精心设计的提示和上下文 grounding 可以极大地提高使用预训练视觉语言模型进行高级驾驶问答的效果。

Abstract: We present a two-phase vision-language QA system for autonomous driving that
answers high-level perception, prediction, and planning questions. In Phase-1,
a large multimodal LLM (Qwen2.5-VL-32B) is conditioned on six-camera inputs, a
short temporal window of history, and a chain-of-thought prompt with few-shot
exemplars. A self-consistency ensemble (multiple sampled reasoning chains)
further improves answer reliability. In Phase-2, we augment the prompt with
nuScenes scene metadata (object annotations, ego-vehicle state, etc.) and
category-specific question instructions (separate prompts for perception,
prediction, planning tasks). In experiments on a driving QA benchmark, our
approach significantly outperforms the baseline Qwen2.5 models. For example,
using 5 history frames and 10-shot prompting in Phase-1 yields 65.1% overall
accuracy (vs.62.61% with zero-shot); applying self-consistency raises this to
66.85%. Phase-2 achieves 67.37% overall. Notably, the system maintains 96%
accuracy under severe visual corruption. These results demonstrate that
carefully engineered prompts and contextual grounding can greatly enhance
high-level driving QA with pretrained vision-language models.

</details>


### [58] [$Δ$t-Mamba3D: A Time-Aware Spatio-Temporal State-Space Model for Breast Cancer Risk Prediction](https://arxiv.org/abs/2510.19003)
*Zhengbo Zhou,Dooman Arefan,Margarita Zuley,Shandong Wu*

Main category: cs.CV

TL;DR: 提出了一种新的state-space架构，用于纵向医学图像分析。


<details>
  <summary>Details</summary>
Motivation: 如何有效地建模在不规则时间间隔捕获的一系列高分辨率图像是一个根本的数据挑战，当前的方法未能充分利用空间和时间线索。

Method: Time-Aware Δt-Mamba3D，一种新颖的状态空间架构，适用于纵向医学成像。其核心创新是一种连续时间选择性扫描机制，可将检查之间的真实时间差显式地集成到其状态转换中。此外，还使用多尺度3D邻域融合模块，可以鲁棒地捕获时空关系。

Result: 在全面的乳腺癌风险预测基准测试中，使用连续筛查乳房X线照片检查，该模型显示出卓越的性能，验证c指数提高了2-5个百分点，并且与已建立的循环，transformer和状态空间模型的变体相比，获得了更高的1-5年AUC分数。

Conclusion: 由于其线性复杂性，该模型可以有效地处理乳房X线照片的长期和复杂的患者筛查历史，从而形成纵向图像分析的新框架。

Abstract: Longitudinal analysis of sequential radiological images is hampered by a
fundamental data challenge: how to effectively model a sequence of
high-resolution images captured at irregular time intervals. This data
structure contains indispensable spatial and temporal cues that current methods
fail to fully exploit. Models often compromise by either collapsing spatial
information into vectors or applying spatio-temporal models that are
computationally inefficient and incompatible with non-uniform time steps. We
address this challenge with Time-Aware $\Delta$t-Mamba3D, a novel state-space
architecture adapted for longitudinal medical imaging. Our model simultaneously
encodes irregular inter-visit intervals and rich spatio-temporal context while
remaining computationally efficient. Its core innovation is a continuous-time
selective scanning mechanism that explicitly integrates the true time
difference between exams into its state transitions. This is complemented by a
multi-scale 3D neighborhood fusion module that robustly captures
spatio-temporal relationships. In a comprehensive breast cancer risk prediction
benchmark using sequential screening mammogram exams, our model shows superior
performance, improving the validation c-index by 2-5 percentage points and
achieving higher 1-5 year AUC scores compared to established variants of
recurrent, transformer, and state-space models. Thanks to its linear
complexity, the model can efficiently process long and complex patient
screening histories of mammograms, forming a new framework for longitudinal
image analysis.

</details>


### [59] [A Matter of Time: Revealing the Structure of Time in Vision-Language Models](https://arxiv.org/abs/2510.19559)
*Nidham Tekaya,Manuela Waldner,Matthias Zeppelzauer*

Main category: cs.CV

TL;DR: 这篇论文主要研究了大规模视觉语言模型（VLMs）的时间感知能力，并提出了从VLM嵌入空间中提取时间线的方法。


<details>
  <summary>Details</summary>
Motivation: 评估VLMs在时间定位视觉内容方面的能力，因为它们在利用大规模训练数据后获得了开放词汇能力，能够解决超出其训练范围的任务。

Method: 1. 构建了一个名为TIME10k的基准数据集，包含超过10,000张带有时间标签的图像。2. 评估了37个VLMs的时间感知能力。3. 提出从VLM嵌入空间中提取显式“时间线”表示的方法。

Result: 研究表明，时间信息在VLM嵌入空间中沿低维非线性流形结构化。提出的时间线方法在时间推理任务中实现了与基于提示的基线方法相比具有竞争力的或更优越的准确性。

Conclusion: VLMs具有一定的时间感知能力，并且可以通过提出的时间线方法有效地提取和利用这些时间信息。

Abstract: Large-scale vision-language models (VLMs) such as CLIP have gained popularity
for their generalizable and expressive multimodal representations. By
leveraging large-scale training data with diverse textual metadata, VLMs
acquire open-vocabulary capabilities, solving tasks beyond their training
scope. This paper investigates the temporal awareness of VLMs, assessing their
ability to position visual content in time. We introduce TIME10k, a benchmark
dataset of over 10,000 images with temporal ground truth, and evaluate the
time-awareness of 37 VLMs by a novel methodology. Our investigation reveals
that temporal information is structured along a low-dimensional, non-linear
manifold in the VLM embedding space. Based on this insight, we propose methods
to derive an explicit ``timeline'' representation from the embedding space.
These representations model time and its chronological progression and thereby
facilitate temporal reasoning tasks. Our timeline approaches achieve
competitive to superior accuracy compared to a prompt-based baseline while
being computationally efficient. All code and data are available at
https://tekayanidham.github.io/timeline-page/.

</details>


### [60] [MoAlign: Motion-Centric Representation Alignment for Video Diffusion Models](https://arxiv.org/abs/2510.19022)
*Aritra Bhowmik,Denis Korzhenkov,Cees G. M. Snoek,Amirhossein Habibian,Mohsen Ghafoorian*

Main category: cs.CV

TL;DR: 本文提出了一种运动中心对齐框架，通过从预训练视频编码器中学习解耦的运动子空间，优化预测光流，从而捕捉真实的运动动态，并将文本到视频扩散模型的潜在特征与该子空间对齐，从而使生成模型能够内化运动知识并生成更合理的视频。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频扩散模型通常无法生成时间连贯和物理上合理的运动，因为这些模型对自然视频中常见的复杂运动理解不足。以往的研究通过将扩散模型特征与预训练视频编码器的特征对齐来解决这个问题，但是这些编码器将视频外观和动态混合到纠缠的特征中，限制了这种对齐的益处。

Method: 本文提出一种运动中心对齐框架，该框架从预训练视频编码器中学习解耦的运动子空间。这个子空间经过优化，可以预测真实的光流，确保它能捕捉到真实的运动动态。然后，我们将文本到视频扩散模型的潜在特征与这个新的子空间对齐，使生成模型能够内化运动知识。

Result: 我们的方法提高了最先进的视频扩散模型中的物理常识，同时保留了对文本提示的坚持，这已被在VideoPhy、VideoPhy2、VBench和VBench-2.0上的实证评估以及用户研究证明。

Conclusion: 本文提出了一种运动中心对齐框架，可以提高视频扩散模型生成视频的物理合理性。

Abstract: Text-to-video diffusion models have enabled high-quality video synthesis, yet
often fail to generate temporally coherent and physically plausible motion. A
key reason is the models' insufficient understanding of complex motions that
natural videos often entail. Recent works tackle this problem by aligning
diffusion model features with those from pretrained video encoders. However,
these encoders mix video appearance and dynamics into entangled features,
limiting the benefit of such alignment. In this paper, we propose a
motion-centric alignment framework that learns a disentangled motion subspace
from a pretrained video encoder. This subspace is optimized to predict
ground-truth optical flow, ensuring it captures true motion dynamics. We then
align the latent features of a text-to-video diffusion model to this new
subspace, enabling the generative model to internalize motion knowledge and
generate more plausible videos. Our method improves the physical commonsense in
a state-of-the-art video diffusion model, while preserving adherence to textual
prompts, as evidenced by empirical evaluations on VideoPhy, VideoPhy2, VBench,
and VBench-2.0, along with a user study.

</details>


### [61] [PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions](https://arxiv.org/abs/2510.19060)
*Amith Ananthram,Elias Stengel-Eskin,Lorena A. Bradford,Julia Demarest,Adam Purvis,Keith Krut,Robert Stein,Rina Elster Pantalony,Mohit Bansal,Kathleen McKeown*

Main category: cs.CV

TL;DR: 提出了PoSh指标和DOCENT数据集，用于评估和提升视觉语言模型在详细图像描述方面的能力，特别是在艺术作品领域。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标不适用于评估长文本图像描述，无法捕捉属性和关系连接中的错误。

Method: 使用场景图作为结构化规则，指导大型语言模型进行判断，生成基于细粒度错误的聚合分数。

Result: PoSh指标在DOCENT数据集上与人类判断的相关性更高，对图像类型具有鲁棒性，并且作为奖励函数表现良好。实验表明，现有模型在描述具有丰富场景动态的图像时存在困难。

Conclusion: PoSh和DOCENT的提出有望促进辅助文本生成等领域的发展。

Abstract: While vision-language models (VLMs) have advanced into detailed image
description, evaluation remains a challenge. Standard metrics (e.g. CIDEr,
SPICE) were designed for short texts and tuned to recognize errors that are now
uncommon, such as object misidentification. In contrast, long texts require
sensitivity to attribute and relation attachments and scores that localize
errors to particular text spans. In this work, we introduce PoSh, a metric for
detailed image description that uses scene graphs as structured rubrics to
guide LLMs-as-a-Judge, producing aggregate scores grounded in fine-grained
errors (e.g. mistakes in compositional understanding). PoSh is replicable,
interpretable and a better proxy for human raters than existing metrics
(including GPT4o-as-a-Judge). To validate PoSh, we introduce a challenging new
dataset, DOCENT. This novel benchmark contains artwork, paired with
expert-written references, and model-generated descriptions, augmented with
granular and coarse judgments of their quality from art history students. Thus,
DOCENT enables evaluating both detailed image description metrics and detailed
image description itself in a challenging new domain. We show that PoSh
achieves stronger correlations (+0.05 Spearman $\rho$) with the human judgments
in DOCENT than the best open-weight alternatives, is robust to image type
(using CapArena, an existing dataset of web imagery) and is a capable reward
function, outperforming standard supervised fine-tuning. Then, using PoSh, we
characterize the performance of open and closed models in describing the
paintings, sketches and statues in DOCENT and find that foundation models
struggle to achieve full, error-free coverage of images with rich scene
dynamics, establishing a demanding new task to gauge VLM progress. Through both
PoSh and DOCENT, we hope to enable advances in important areas such as
assistive text generation.

</details>


### [62] [UniHPR: Unified Human Pose Representation via Singular Value Contrastive Learning](https://arxiv.org/abs/2510.19078)
*Zhongyu Jiang,Wenhao Chai,Lei Li,Zhuoran Zhou,Cheng-Yen Yang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: 提出了一种统一的人体姿势表示学习流程UniHPR，可以对齐来自图像、2D和3D人体姿势的人体姿势嵌入。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态融合和生成方法缺乏对不同模态人体姿势表征之间相关性的清晰研究。

Method: 提出了一种新颖的基于奇异值的对比学习损失，以更好地对齐不同模态。

Result: 在Human3.6M数据集上实现了MPJPE 49.9mm，在3DPW数据集上实现了PA-MPJPE 51.6mm的卓越性能指标。在Human3.6M数据集中，统一的人体姿势表示实现了2D和3D姿势检索，检索误差为MPJPE 9.24mm。

Conclusion: UniHPR有效地对齐了来自不同模态的人体姿势嵌入，并在人体姿势估计和检索任务中取得了显著成果。

Abstract: In recent years, there has been a growing interest in developing effective
alignment pipelines to generate unified representations from different
modalities for multi-modal fusion and generation. As an important component of
Human-Centric applications, Human Pose representations are critical in many
downstream tasks, such as Human Pose Estimation, Action Recognition,
Human-Computer Interaction, Object tracking, etc. Human Pose representations or
embeddings can be extracted from images, 2D keypoints, 3D skeletons, mesh
models, and lots of other modalities. Yet, there are limited instances where
the correlation among all of those representations has been clearly researched
using a contrastive paradigm. In this paper, we propose UniHPR, a unified Human
Pose Representation learning pipeline, which aligns Human Pose embeddings from
images, 2D and 3D human poses. To align more than two data representations at
the same time, we propose a novel singular value-based contrastive learning
loss, which better aligns different modalities and further boosts performance.
To evaluate the effectiveness of the aligned representation, we choose 2D and
3D Human Pose Estimation (HPE) as our evaluation tasks. In our evaluation, with
a simple 3D human pose decoder, UniHPR achieves remarkable performance metrics:
MPJPE 49.9mm on the Human3.6M dataset and PA-MPJPE 51.6mm on the 3DPW dataset
with cross-domain evaluation. Meanwhile, we are able to achieve 2D and 3D pose
retrieval with our unified human pose representations in Human3.6M dataset,
where the retrieval error is 9.24mm in MPJPE.

</details>


### [63] [Advancing Brain Tumor Segmentation via Attention-based 3D U-Net Architecture and Digital Image Processing](https://arxiv.org/abs/2510.19109)
*Eyad Gad,Seif Soliman,M. Saeed Darweesh*

Main category: cs.CV

TL;DR: 本研究旨在通过将注意力机制整合到 3D U-Net 模型中，并结合基于数字图像处理技术的肿瘤检测算法来解决类不平衡问题，从而提高脑肿瘤分割的性能。


<details>
  <summary>Details</summary>
Motivation: U-Net 模型在精确描绘肿瘤区域方面面临挑战，尤其是在处理不规则形状和模糊边界时。此外，在高分辨率 MRI 数据上训练鲁棒的分割模型需要高计算资源，并且经常面临与类不平衡相关的挑战。

Method: 将注意力机制整合到 3D U-Net 模型中，使模型能够捕获复杂的细节，并在分割过程中优先考虑信息丰富的区域。此外，还利用基于数字图像处理技术的肿瘤检测算法来解决不平衡训练数据的问题并减轻偏差。

Result: 在 BraTS 2020 数据集上评估的结果表明，该模型优于相关研究，Dice 系数为 0.975，特异性为 0.988，灵敏度为 0.995，表明该模型在改善脑肿瘤分割方面的有效性。

Conclusion: 该研究提出的模型能够有效提高脑肿瘤分割的性能，为临床环境中可靠的诊断提供有价值的见解。

Abstract: In the realm of medical diagnostics, rapid advancements in Artificial
Intelligence (AI) have significantly yielded remarkable improvements in brain
tumor segmentation. Encoder-Decoder architectures, such as U-Net, have played a
transformative role by effectively extracting meaningful representations in 3D
brain tumor segmentation from Magnetic resonance imaging (MRI) scans. However,
standard U-Net models encounter challenges in accurately delineating tumor
regions, especially when dealing with irregular shapes and ambiguous
boundaries. Additionally, training robust segmentation models on
high-resolution MRI data, such as the BraTS datasets, necessitates high
computational resources and often faces challenges associated with class
imbalance. This study proposes the integration of the attention mechanism into
the 3D U-Net model, enabling the model to capture intricate details and
prioritize informative regions during the segmentation process. Additionally, a
tumor detection algorithm based on digital image processing techniques is
utilized to address the issue of imbalanced training data and mitigate bias.
This study aims to enhance the performance of brain tumor segmentation,
ultimately improving the reliability of diagnosis. The proposed model is
thoroughly evaluated and assessed on the BraTS 2020 dataset using various
performance metrics to accomplish this goal. The obtained results indicate that
the model outperformed related studies, exhibiting dice of 0.975, specificity
of 0.988, and sensitivity of 0.995, indicating the efficacy of the proposed
model in improving brain tumor segmentation, offering valuable insights for
reliable diagnosis in clinical settings.

</details>


### [64] [A Novel Approach to Breast Cancer Segmentation using U-Net Model with Attention Mechanisms and FedProx](https://arxiv.org/abs/2510.19118)
*Eyad Gad,Mustafa Abou Khatwa,Mustafa A. Elattar,Sahar Selim*

Main category: cs.CV

TL;DR: 本文提出了一种使用联邦学习和改进的U-Net模型进行乳腺癌肿瘤分割的方法，旨在提高在非独立同分布数据集上的分割精度，同时保护患者隐私。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是女性死亡的主要原因，早期检测和准确诊断至关重要。医学数据的敏感性使得开发准确和私有的人工智能模型具有挑战性。联邦学习是一种有前景的解决方案，但训练数据非独立同分布会影响模型精度和泛化能力。

Method: 采用联邦近端(FedProx)方法处理非独立同分布的乳腺超声图像数据集，并结合带有注意力机制的改进U-Net模型来提高肿瘤分割精度。

Result: 该方法获得的全局模型准确率达到96%，证明了其在提高肿瘤分割准确率和保护患者隐私方面的有效性。

Conclusion: FedProx有潜力成为在非独立同分布的本地医疗数据集上训练精确机器学习模型的一种有前景的方法。

Abstract: Breast cancer is a leading cause of death among women worldwide, emphasizing
the need for early detection and accurate diagnosis. As such Ultrasound
Imaging, a reliable and cost-effective tool, is used for this purpose, however
the sensitive nature of medical data makes it challenging to develop accurate
and private artificial intelligence models. A solution is Federated Learning as
it is a promising technique for distributed machine learning on sensitive
medical data while preserving patient privacy. However, training on
non-Independent and non-Identically Distributed (non-IID) local datasets can
impact the accuracy and generalization of the trained model, which is crucial
for accurate tumour boundary delineation in BC segmentation. This study aims to
tackle this challenge by applying the Federated Proximal (FedProx) method to
non-IID Ultrasonic Breast Cancer Imaging datasets. Moreover, we focus on
enhancing tumour segmentation accuracy by incorporating a modified U-Net model
with attention mechanisms. Our approach resulted in a global model with 96%
accuracy, demonstrating the effectiveness of our method in enhancing tumour
segmentation accuracy while preserving patient privacy. Our findings suggest
that FedProx has the potential to be a promising approach for training precise
machine learning models on non-IID local medical datasets.

</details>


### [65] [X-Ego: Acquiring Team-Level Tactical Situational Awareness via Cross-Egocentric Contrastive Video Representation Learning](https://arxiv.org/abs/2510.19150)
*Yunzhe Wang,Soham Hans,Volkan Ustun*

Main category: cs.CV

TL;DR: 本研究提出了一个名为X-Ego-CS的电子竞技数据集，并提出了一个名为CECL的跨自我对比学习方法，以提高团队战术 ситуационной осведомленности。


<details>
  <summary>Details</summary>
Motivation: 现有的视频理解工作大多依赖于第三人称广播视角，忽略了多智能体学习的同步、以自我为中心的本质。

Method: 提出了Cross-Ego Contrastive Learning (CECL)，它将队友的以自我为中心的视觉流对齐，以培养从个人角度出发的团队层面的战术 ситуационной осведомленности。

Result: 在队友-对手位置预测任务上评估了CECL，证明了其在使用最先进的视频编码器从单个第一人称视角推断队友和对手位置的能力。

Conclusion: X-Ego-CS和CECL为电子竞技中的跨自我多智能体基准测试奠定了基础。

Abstract: Human team tactics emerge from each player's individual perspective and their
ability to anticipate, interpret, and adapt to teammates' intentions. While
advances in video understanding have improved the modeling of team interactions
in sports, most existing work relies on third-person broadcast views and
overlooks the synchronous, egocentric nature of multi-agent learning. We
introduce X-Ego-CS, a benchmark dataset consisting of 124 hours of gameplay
footage from 45 professional-level matches of the popular e-sports game
Counter-Strike 2, designed to facilitate research on multi-agent
decision-making in complex 3D environments. X-Ego-CS provides cross-egocentric
video streams that synchronously capture all players' first-person perspectives
along with state-action trajectories. Building on this resource, we propose
Cross-Ego Contrastive Learning (CECL), which aligns teammates' egocentric
visual streams to foster team-level tactical situational awareness from an
individual's perspective. We evaluate CECL on a teammate-opponent location
prediction task, demonstrating its effectiveness in enhancing an agent's
ability to infer both teammate and opponent positions from a single
first-person view using state-of-the-art video encoders. Together, X-Ego-CS and
CECL establish a foundation for cross-egocentric multi-agent benchmarking in
esports. More broadly, our work positions gameplay understanding as a testbed
for multi-agent modeling and tactical learning, with implications for
spatiotemporal reasoning and human-AI teaming in both virtual and real-world
domains. Code and dataset are available at https://github.com/HATS-ICT/x-ego.

</details>


### [66] [FootFormer: Estimating Stability from Visual Input](https://arxiv.org/abs/2510.19170)
*Keaton Kraiger,Jingjing Li,Skanda Bharadwaj,Jesse Scott,Robert T. Collins,Yanxi Liu*

Main category: cs.CV

TL;DR: FootFormer: predicts human motion from visual input using a cross-modality approach.


<details>
  <summary>Details</summary>
Motivation: Existing methods only generate one or two measures of foot pressure distributions, foot contact maps, and center of mass (CoM).

Method: A cross-modality approach is used to jointly predict human motion dynamics directly from visual input.

Result: FootFormer achieves statistically significantly better or equivalent estimates of foot pressure distributions, foot contact maps, and center of mass (CoM), and achieves SOTA performance in estimating stability-predictive components.

Conclusion: FootFormer is effective in estimating human motion dynamics and stability-predictive components from visual input.

Abstract: We propose FootFormer, a cross-modality approach for jointly predicting human
motion dynamics directly from visual input. On multiple datasets, FootFormer
achieves statistically significantly better or equivalent estimates of foot
pressure distributions, foot contact maps, and center of mass (CoM), as
compared with existing methods that generate one or two of those measures.
Furthermore, FootFormer achieves SOTA performance in estimating
stability-predictive components (CoP, CoM, BoS) used in classic kinesiology
metrics. Code and data are available at
https://github.com/keatonkraiger/Vision-to-Stability.git.

</details>


### [67] [Malaria Detection from Blood Cell Images Using XceptionNet](https://arxiv.org/abs/2510.19182)
*Warisa Nusrat,Mostafijur Rahman,Ayatullah Faruk Mollah*

Main category: cs.CV

TL;DR: 本文利用深度学习实现了疟疾的自动诊断，减少了人工参与。


<details>
  <summary>Details</summary>
Motivation: 人工诊断疟疾容易出错，且依赖专业知识。因此，计算机辅助自动诊断是更好的选择。

Method: 本文使用了六种深度卷积网络（AlexNet, XceptionNet, VGG-19, Residual Attention Network, DenseNet-121, Custom-CNN）提取血细胞图像的深层特征，并对其进行分类。

Result: Residual Attention Network和XceptionNet表现最好，在公开疟疾细胞图像数据集上的平均准确率分别为97.28%和97.55%，超过了其他相关方法。

Conclusion: 深度学习驱动的方法在疟疾的自动和可靠检测方面有很大的潜力，同时最大限度地减少了直接的人工参与。

Abstract: Malaria, which primarily spreads with the bite of female anopheles mosquitos,
often leads to death of people - specifically children in the age-group of 0-5
years. Clinical experts identify malaria by observing RBCs in blood smeared
images with a microscope. Lack of adequate professional knowledge and skills,
and most importantly manual involvement may cause incorrect diagnosis.
Therefore, computer aided automatic diagnosis stands as a preferred substitute.
In this paper, well-demonstrated deep networks have been applied to extract
deep intrinsic features from blood cell images and thereafter classify them as
malaria infected or healthy cells. Among the six deep convolutional networks
employed in this work viz. AlexNet, XceptionNet, VGG-19, Residual Attention
Network, DenseNet-121 and Custom-CNN. Residual Attention Network and
XceptionNet perform relatively better than the rest on a publicly available
malaria cell image dataset. They yield an average accuracy of 97.28% and 97.55%
respectively, that surpasses other related methods on the same dataset. These
findings highly encourage the reality of deep learning driven method for
automatic and reliable detection of malaria while minimizing direct manual
involvement.

</details>


### [68] [PruneHal: Reducing Hallucinations in Multi-modal Large Language Models through Adaptive KV Cache Pruning](https://arxiv.org/abs/2510.19183)
*Fengyuan Sun,Hui Chen,Xinhao Xu,Dandan Zheng,Jingdong Chen,Jun Zhou,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: 本文提出了一种名为PruneHal的训练自由方法，通过自适应KV缓存剪枝来增强模型对关键视觉信息的关注，从而减轻多模态大型语言模型(MLLM)中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有的减轻幻觉的方法通常需要额外的训练数据或在推理过程中引入外部或内部信息，这会增加计算成本。本文观察到MLLM中的幻觉与分配给视觉tokens的注意力不足密切相关，冗余的视觉tokens分散了模型的注意力，使其无法集中于最有效的信息。

Method: 本文提出PruneHal方法，利用自适应KV缓存剪枝来增强模型对关键视觉信息的关注。

Result: 在多个广泛使用的幻觉评估基准上，使用四种主流MLLM评估PruneHal，取得了稳健而突出的结果。

Conclusion: PruneHal是一种有效的、无需额外训练且几乎不产生额外推理成本的减轻MLLM幻觉的方法。

Abstract: While multi-modal large language models (MLLMs) have made significant
progress in recent years, the issue of hallucinations remains a major
challenge. To mitigate this phenomenon, existing solutions either introduce
additional data for further training or incorporate external or internal
information during inference. However, these approaches inevitably introduce
extra computational costs. In this paper, we observe that hallucinations in
MLLMs are strongly associated with insufficient attention allocated to visual
tokens. In particular, the presence of redundant visual tokens disperses the
model's attention, preventing it from focusing on the most informative ones. As
a result, critical visual cues are often under-attended, which in turn
exacerbates the occurrence of hallucinations. Building on this observation, we
propose \textbf{PruneHal}, a training-free, simple yet effective method that
leverages adaptive KV cache pruning to enhance the model's focus on critical
visual information, thereby mitigating hallucinations. To the best of our
knowledge, we are the first to apply token pruning for hallucination mitigation
in MLLMs. Notably, our method don't require additional training and incurs
nearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be
seamlessly integrated with different decoding strategies, including those
specifically designed for hallucination mitigation. We evaluate PruneHal on
several widely used hallucination evaluation benchmarks using four mainstream
MLLMs, achieving robust and outstanding results that highlight the
effectiveness and superiority of our method. Our code will be publicly
available.

</details>


### [69] [Video Consistency Distance: Enhancing Temporal Consistency for Image-to-Video Generation via Reward-Based Fine-Tuning](https://arxiv.org/abs/2510.19193)
*Takehiro Aoshima,Yusuke Shinohara,Park Byeongseon*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的度量方法，称为视频一致性距离（VCD），用于提高图像到视频（I2V）生成任务中生成视频的时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于奖励的视频扩散模型微调方法在提高生成视频的整体质量（如美观性和一致性）方面有效，但可能无法保证时间一致性，尤其是在图像到视频生成任务中。

Method: 该论文提出在视频帧特征的频域空间中定义VCD，通过频域分析有效捕捉帧信息，从而实现与条件图像相关的连贯时间一致性。

Result: 在多个图像到视频数据集上的实验结果表明，与先前的方法相比，使用VCD微调视频生成模型可以显著提高时间一致性，同时不降低其他性能。

Conclusion: 该论文提出的VCD能够有效提高图像到视频生成任务中生成视频的时间一致性，且不会影响其他性能。

Abstract: Reward-based fine-tuning of video diffusion models is an effective approach
to improve the quality of generated videos, as it can fine-tune models without
requiring real-world video datasets. However, it can sometimes be limited to
specific performances because conventional reward functions are mainly aimed at
enhancing the quality across the whole generated video sequence, such as
aesthetic appeal and overall consistency. Notably, the temporal consistency of
the generated video often suffers when applying previous approaches to
image-to-video (I2V) generation tasks. To address this limitation, we propose
Video Consistency Distance (VCD), a novel metric designed to enhance temporal
consistency, and fine-tune a model with the reward-based fine-tuning framework.
To achieve coherent temporal consistency relative to a conditioning image, VCD
is defined in the frequency space of video frame features to capture frame
information effectively through frequency-domain analysis. Experimental results
across multiple I2V datasets demonstrate that fine-tuning a video generation
model with VCD significantly enhances temporal consistency without degrading
other performance compared to the previous method.

</details>


### [70] [Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks](https://arxiv.org/abs/2510.19195)
*Kai Zeng,Zhanqian Wu,Kaixin Xiong,Xiaobao Wei,Xiangyu Guo,Zhenxin Zhu,Kalok Ho,Lijun Zhou,Bohan Zeng,Ming Lu,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Wentao Zhang*

Main category: cs.CV

TL;DR: Dream4Drive: A synthetic data generation framework for enhancing downstream perception tasks in autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Existing driving world models overlook the evaluation of downstream perception tasks, which are crucial for autonomous driving performance. The benefit of synthetic data is not thoroughly demonstrated.

Method: Dream4Drive decomposes the input video into 3D-aware guidance maps, renders 3D assets, and fine-tunes the driving world model to produce edited, multi-view photorealistic videos.

Result: Dream4Drive significantly boosts corner case perception in autonomous driving and facilitates the generation of multi-view corner cases at scale.

Conclusion: Dream4Drive effectively boosts the performance of downstream perception models under various training epochs. The authors also contribute a large-scale 3D asset dataset named DriveObj3D.

Abstract: Recent advancements in driving world models enable controllable generation of
high-quality RGB videos or multimodal videos. Existing methods primarily focus
on metrics related to generation quality and controllability. However, they
often overlook the evaluation of downstream perception tasks, which are
$\mathbf{really\ crucial}$ for the performance of autonomous driving. Existing
methods usually leverage a training strategy that first pretrains on synthetic
data and finetunes on real data, resulting in twice the epochs compared to the
baseline (real data only). When we double the epochs in the baseline, the
benefit of synthetic data becomes negligible. To thoroughly demonstrate the
benefit of synthetic data, we introduce Dream4Drive, a novel synthetic data
generation framework designed for enhancing the downstream perception tasks.
Dream4Drive first decomposes the input video into several 3D-aware guidance
maps and subsequently renders the 3D assets onto these guidance maps. Finally,
the driving world model is fine-tuned to produce the edited, multi-view
photorealistic videos, which can be used to train the downstream perception
models. Dream4Drive enables unprecedented flexibility in generating multi-view
corner cases at scale, significantly boosting corner case perception in
autonomous driving. To facilitate future research, we also contribute a
large-scale 3D asset dataset named DriveObj3D, covering the typical categories
in driving scenarios and enabling diverse 3D-aware video editing. We conduct
comprehensive experiments to show that Dream4Drive can effectively boost the
performance of downstream perception models under various training epochs.
Project: $\href{https://wm-research.github.io/Dream4Drive/}{this\ https\ URL}$

</details>


### [71] [MoE-GS: Mixture of Experts for Dynamic Gaussian Splatting](https://arxiv.org/abs/2510.19210)
*In-Hwan Jin,Hyeongju Mun,Joonsoo Kim,Kugjin Yun,Kyeongbo Kong*

Main category: cs.CV

TL;DR: MoE-GS: First Mixture-of-Experts technique in dynamic gaussian splatting, improving rendering quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing dynamic scene reconstruction methods using 3D Gaussian Splatting have inconsistent performance across diverse scenes.

Method: A unified framework integrating multiple specialized experts via a novel Volume-aware Pixel Router.

Result: MoE-GS consistently outperforms state-of-the-art methods with improved efficiency on N3V and Technicolor datasets.

Conclusion: MoE-GS improves rendering quality, model capacity, and FPS, using single-pass multi-expert rendering, gate-aware Gaussian pruning, and a distillation strategy.

Abstract: Recent advances in dynamic scene reconstruction have significantly benefited
from 3D Gaussian Splatting, yet existing methods show inconsistent performance
across diverse scenes, indicating no single approach effectively handles all
dynamic challenges. To overcome these limitations, we propose Mixture of
Experts for Dynamic Gaussian Splatting (MoE-GS), a unified framework
integrating multiple specialized experts via a novel Volume-aware Pixel Router.
Our router adaptively blends expert outputs by projecting volumetric
Gaussian-level weights into pixel space through differentiable weight
splatting, ensuring spatially and temporally coherent results. Although MoE-GS
improves rendering quality, the increased model capacity and reduced FPS are
inherent to the MoE architecture. To mitigate this, we explore two
complementary directions: (1) single-pass multi-expert rendering and gate-aware
Gaussian pruning, which improve efficiency within the MoE framework, and (2) a
distillation strategy that transfers MoE performance to individual experts,
enabling lightweight deployment without architectural changes. To the best of
our knowledge, MoE-GS is the first approach incorporating Mixture-of-Experts
techniques into dynamic Gaussian splatting. Extensive experiments on the N3V
and Technicolor datasets demonstrate that MoE-GS consistently outperforms
state-of-the-art methods with improved efficiency. Video demonstrations are
available at https://anonymous.4open.science/w/MoE-GS-68BA/.

</details>


### [72] [SFGFusion: Surface Fitting Guided 3D Object Detection with 4D Radar and Camera Fusion](https://arxiv.org/abs/2510.19215)
*Xiaozhi Li,Huijun Di,Jian Li,Feng Liu,Wei Liang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SFGFusion的新型相机-4D成像雷达检测网络，该网络通过表面拟合来引导，以提高3D物体检测的性能。


<details>
  <summary>Details</summary>
Motivation: 4D成像雷达在物体检测中具有低成本、远距离检测和精确的速度测量等优点，但其稀疏的点云和低分辨率限制了几何表示和多模态融合。

Method: 该方法通过估计图像和雷达数据中物体的二次曲面参数来增强空间表示和跨模态交互，从而实现更可靠的精细密集深度预测。预测的深度用于指导图像特征从透视视图（PV）到统一的鸟瞰图（BEV）的转换，并生成密集的伪点云以减轻雷达点稀疏性。原始雷达点云也被编码在单独的雷达分支中。这两个点云分支采用基于柱的方法，并将特征转换到BEV空间。最后，使用标准的2D骨干和检测头从BEV特征预测对象标签和边界框。

Result: 在TJ4DRadSet和view-of-delft（VoD）物体检测基准测试中，实验结果表明SFGFusion有效地融合了相机和4D雷达特征，实现了卓越的性能。

Conclusion: SFGFusion是一种有效的相机-4D雷达融合方法，可以提高3D物体检测的性能。

Abstract: 3D object detection is essential for autonomous driving. As an emerging
sensor, 4D imaging radar offers advantages as low cost, long-range detection,
and accurate velocity measurement, making it highly suitable for object
detection. However, its sparse point clouds and low resolution limit object
geometric representation and hinder multi-modal fusion. In this study, we
introduce SFGFusion, a novel camera-4D imaging radar detection network guided
by surface fitting. By estimating quadratic surface parameters of objects from
image and radar data, the explicit surface fitting model enhances spatial
representation and cross-modal interaction, enabling more reliable prediction
of fine-grained dense depth. The predicted depth serves two purposes: 1) in an
image branch to guide the transformation of image features from perspective
view (PV) to a unified bird's-eye view (BEV) for multi-modal fusion, improving
spatial mapping accuracy; and 2) in a surface pseudo-point branch to generate
dense pseudo-point cloud, mitigating the radar point sparsity. The original
radar point cloud is also encoded in a separate radar branch. These two point
cloud branches adopt a pillar-based method and subsequently transform the
features into the BEV space. Finally, a standard 2D backbone and detection head
are used to predict object labels and bounding boxes from BEV features.
Experimental results show that SFGFusion effectively fuses camera and 4D radar
features, achieving superior performance on the TJ4DRadSet and view-of-delft
(VoD) object detection benchmarks.

</details>


### [73] [Space Object Detection using Multi-frame Temporal Trajectory Completion Method](https://arxiv.org/abs/2510.19220)
*Xiaoqing Lan,Biqiao Xin,Bingshu Wang,Han Zhang,Laixian Zhang*

Main category: cs.CV

TL;DR: 本研究提出了一种利用小波变换增强GEO目标高频特征并抑制背景噪声的方法，并设计了一种基于匈牙利算法的多帧时域轨迹补全方案，以解决GEO目标检测中的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于信号弱、恒星背景复杂和环境干扰，地球静止轨道(GEO)中的空间物体在光学成像中提出了重要的检测挑战。

Method: 通过小波变换增强GEO目标的高频特征，同时在单帧水平上抑制背景噪声。在此基础上，提出了一种以匈牙利算法为中心的多帧时间轨迹补全方案，用于全局最优跨帧匹配。为了有效缓解缺失和错误检测，在后处理流程中设计了一系列关键步骤，包括时间匹配和插值补全、基于时间一致性的噪声滤波和渐进轨迹细化。

Result: 在公共SpotGEO数据集上的实验结果表明，该方法是有效的，实现了90.14%的F_1分数。

Conclusion: 该方法在GEO目标检测中表现出有效性，并在SpotGEO数据集上取得了良好的F_1分数。

Abstract: Space objects in Geostationary Earth Orbit (GEO) present significant
detection challenges in optical imaging due to weak signals, complex stellar
backgrounds, and environmental interference. In this paper, we enhance
high-frequency features of GEO targets while suppressing background noise at
the single-frame level through wavelet transform. Building on this, we propose
a multi-frame temporal trajectory completion scheme centered on the Hungarian
algorithm for globally optimal cross-frame matching. To effectively mitigate
missing and false detections, a series of key steps including temporal matching
and interpolation completion, temporal-consistency-based noise filtering, and
progressive trajectory refinement are designed in the post-processing pipeline.
Experimental results on the public SpotGEO dataset demonstrate the
effectiveness of the proposed method, achieving an F_1 score of 90.14%.

</details>


### [74] [Background Fades, Foreground Leads: Curriculum-Guided Background Pruning for Efficient Foreground-Centric Collaborative Perception](https://arxiv.org/abs/2510.19250)
*Yuheng Wu,Xiangbo Gao,Quang Tau,Zhengzhong Tu,Dongman Lee*

Main category: cs.CV

TL;DR: 提出了一种名为 FadeLead 的前景中心协同感知框架，通过在训练过程中将背景上下文封装到前景特征中，克服了带宽限制下背景信息丢失的问题。


<details>
  <summary>Details</summary>
Motivation: 车辆协同感知通过共享互补信息来增强自动驾驶车辆的可靠性和空间覆盖范围，但车载网络的带宽限制使得传输整个特征图不切实际，现有方法忽略了包含重要背景信息的背景区域。

Method: 提出了一种课程学习策略，该策略利用早期背景线索，但逐渐修剪掉它们，迫使模型将上下文内化到前景表示中，而无需传输背景本身。

Result: 在模拟和真实基准上的大量实验表明，FadeLead 在不同的带宽设置下优于现有方法。

Conclusion: FadeLead 通过上下文丰富的 Foreground 共享，提高了协同感知的性能。

Abstract: Collaborative perception enhances the reliability and spatial coverage of
autonomous vehicles by sharing complementary information across vehicles,
offering a promising solution to long-tail scenarios that challenge
single-vehicle perception. However, the bandwidth constraints of vehicular
networks make transmitting the entire feature map impractical. Recent methods,
therefore, adopt a foreground-centric paradigm, transmitting only predicted
foreground-region features while discarding the background, which encodes
essential context. We propose FadeLead, a foreground-centric framework that
overcomes this limitation by learning to encapsulate background context into
compact foreground features during training. At the core of our design is a
curricular learning strategy that leverages background cues early on but
progressively prunes them away, forcing the model to internalize context into
foreground representations without transmitting background itself. Extensive
experiments on both simulated and real-world benchmarks show that FadeLead
outperforms prior methods under different bandwidth settings, underscoring the
effectiveness of context-enriched foreground sharing.

</details>


### [75] [Advances in 4D Representation: Geometry, Motion, and Interaction](https://arxiv.org/abs/2510.19255)
*Mingrui Zhao,Sauradip Nag,Kai Wang,Aditya Vora,Guangda Ji,Peter Chun,Ali Mahdavi-Amiri,Hao Zhang*

Main category: cs.CV

TL;DR: 本文对4D生成和重建进行了综述，重点关注神经场、几何和运动深度学习以及3D生成人工智能的最新进展。


<details>
  <summary>Details</summary>
Motivation: 探讨如何选择和定制合适的4D表示以完成特定任务。

Method: 从几何、运动和交互三个关键支柱对4D表示进行分类，重点关注有代表性的工作，突出各种表示的理想属性和挑战。

Result: 讨论了神经辐射场（NeRFs）和3D高斯溅射（3DGS）等流行的表示方法，并关注了结构化模型和长程运动等探索不足的表示方法。同时讨论了大型语言模型（LLMs）和视频基础模型（VFMs）在各种4D应用中的作用，并指出了它们当前的局限性以及如何解决这些局限性。

Conclusion: 对当前可用的4D数据集以及推动该领域发展所需的缺失部分进行了专门的介绍。

Abstract: We present a survey on 4D generation and reconstruction, a fast-evolving
subfield of computer graphics whose developments have been propelled by recent
advances in neural fields, geometric and motion deep learning, as well 3D
generative artificial intelligence (GenAI). While our survey is not the first
of its kind, we build our coverage of the domain from a unique and distinctive
perspective of 4D representations\/}, to model 3D geometry evolving over time
while exhibiting motion and interaction. Specifically, instead of offering an
exhaustive enumeration of many works, we take a more selective approach by
focusing on representative works to highlight both the desirable properties and
ensuing challenges of each representation under different computation,
application, and data scenarios. The main take-away message we aim to convey to
the readers is on how to select and then customize the appropriate 4D
representations for their tasks. Organizationally, we separate the 4D
representations based on three key pillars: geometry, motion, and interaction.
Our discourse will not only encompass the most popular representations of
today, such as neural radiance fields (NeRFs) and 3D Gaussian Splatting (3DGS),
but also bring attention to relatively under-explored representations in the 4D
context, such as structured models and long-range motions. Throughout our
survey, we will reprise the role of large language models (LLMs) and video
foundational models (VFMs) in a variety of 4D applications, while steering our
discussion towards their current limitations and how they can be addressed. We
also provide a dedicated coverage on what 4D datasets are currently available,
as well as what is lacking, in driving the subfield forward. Project
page:https://mingrui-zhao.github.io/4DRep-GMI/

</details>


### [76] [SCEESR: Semantic-Control Edge Enhancement for Diffusion-Based Super-Resolution](https://arxiv.org/abs/2510.19272)
*Yun Kai Zhuang*

Main category: cs.CV

TL;DR: 提出了一种新的超分辨率（SR）框架，该框架使用 ControlNet 机制增强单步扩散模型，以实现语义边缘引导，从而在输出质量和推理速度之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现实世界的图像超分辨率（Real-ISR）必须处理复杂降级和固有的重建模糊性。生成模型虽然提高了感知质量，但计算成本仍然是一个关键的权衡。单步扩散模型虽然速度快，但由于蒸馏伪影，通常会产生结构上的不准确。

Method: 使用 ControlNet 机制增强单步扩散模型，以实现语义边缘引导。集成了边缘信息，以在单次推理过程中提供动态结构控制。引入了一种混合损失，结合了 L2、LPIPS 和边缘感知 AME 损失，以优化像素精度、感知质量和几何精度。

Result: 有效地提高了结构完整性和真实感，同时保持了单步生成的效率，在输出质量和推理速度之间取得了更好的平衡。

Conclusion: 该方法在输出质量和推理速度之间取得了平衡。

Abstract: Real-world image super-resolution (Real-ISR) must handle complex degradations
and inherent reconstruction ambiguities. While generative models have improved
perceptual quality, a key trade-off remains with computational cost. One-step
diffusion models offer speed but often produce structural inaccuracies due to
distillation artifacts. To address this, we propose a novel SR framework that
enhances a one-step diffusion model using a ControlNet mechanism for semantic
edge guidance. This integrates edge information to provide dynamic structural
control during single-pass inference. We also introduce a hybrid loss combining
L2, LPIPS, and an edge-aware AME loss to optimize for pixel accuracy,
perceptual quality, and geometric precision. Experiments show our method
effectively improves structural integrity and realism while maintaining the
efficiency of one-step generation, achieving a superior balance between output
quality and inference speed. The results of test datasets will be published at
https://drive.google.com/drive/folders/1amddXQ5orIyjbxHgGpzqFHZ6KTolinJF?usp=drive_link
and the related code will be published at
https://github.com/ARBEZ-ZEBRA/SCEESR.

</details>


### [77] [MobiAct: Efficient MAV Action Recognition Using MobileNetV4 with Contrastive Learning and Knowledge Distillation](https://arxiv.org/abs/2510.19273)
*Zhang Nengbo,Ho Hann Woei*

Main category: cs.CV

TL;DR: 提出了一种轻量级的微型飞行器(MAV)动作识别框架MobiAct，旨在以低计算成本实现高精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于大型、计算密集型模型，不适用于资源有限的MAV平台，导致识别精度和推理速度之间需要权衡。

Method: 采用MobileNetV4作为骨干网络，并引入分阶段正交知识蒸馏(SOKD)策略，将MAV运动特征从教师网络(ResNet18)有效地转移到学生网络，从而提高知识转移效率。此外，还将一种无参数注意力机制集成到架构中，以提高识别精度，而不会增加模型复杂性。此外，还开发了一种混合损失训练策略，以结合多个损失目标，从而确保训练期间稳定而强大的优化。

Result: MobiAct实现了低能耗和低计算的MAV动作识别，同时在比较方法中保持了最快的动作解码速度。在所有三个自收集的数据集中，MobiAct实现了92.12%的平均识别精度，同时仅消耗136.16 pJ的能量，并以每秒8.84个动作的速率处理识别。值得注意的是，MobiAct解码动作的速度比领先方法快2倍，具有高度可比的识别精度。

Conclusion: MobiAct在MAV动作识别中具有卓越的效率。

Abstract: Accurate and efficient recognition of Micro Air Vehicle (MAV) motion is
essential for enabling real-time perception and coordination in autonomous
aerial swarm. However, most existing approaches rely on large, computationally
intensive models that are unsuitable for resource-limited MAV platforms, which
results in a trade-off between recognition accuracy and inference speed. To
address these challenges, this paper proposes a lightweight MAV action
recognition framework, MobiAct, designed to achieve high accuracy with low
computational cost. Specifically, MobiAct adopts MobileNetV4 as the backbone
network and introduces a Stage-wise Orthogonal Knowledge Distillation (SOKD)
strategy to effectively transfer MAV motion features from a teacher network
(ResNet18) to a student network, thereby enhancing knowledge transfer
efficiency. Furthermore, a parameter-free attention mechanism is integrated
into the architecture to improve recognition accuracy without increasing model
complexity. In addition, a hybrid loss training strategy is developed to
combine multiple loss objectives, which ensures stable and robust optimization
during training. Experimental results demonstrate that the proposed MobiAct
achieves low-energy and low-computation MAV action recognition, while
maintaining the fastest action decoding speed among compared methods. Across
all three self-collected datasets, MobiAct achieves an average recognition
accuracy of 92.12%, while consuming only 136.16 pJ of energy and processing
recognition at a rate of 8.84 actions per second. Notably, MobiAct decodes
actions up to 2 times faster than the leading method, with highly comparable
recognition accuracy, highlighting its superior efficiency in MAV action
recognition.

</details>


### [78] [D2D: Detector-to-Differentiable Critic for Improved Numeracy in Text-to-Image Generation](https://arxiv.org/abs/2510.19278)
*Nobline Yoo,Olga Russakovsky,Ye Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种名为 Detector-to-Differentiable (D2D) 的新框架，用于提升文本到图像 (T2I) 扩散模型在生成图像时正确物体数量的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常使用辅助计数网络作为外部评论家来增强数量控制，但这些评论家被限制为基于回归的模型，无法利用检测器模型更强的计数能力。

Method: D2D 框架将不可微的检测模型转换为可微的评论家，通过设计自定义激活函数将检测器 logits 转换为软二元指标，用于在推理时优化预训练 T2I 模型的噪声先验。

Result: 在 SDXL-Turbo, SD-Turbo, 和 Pixart-DMD 上进行的大量实验表明，在对象计数准确性方面有持续和显著的改进。

Conclusion: D2D 框架能够有效提升 T2I 模型生成图像时物体数量的准确性，同时保持图像质量和计算开销。

Abstract: Text-to-image (T2I) diffusion models have achieved strong performance in
semantic alignment, yet they still struggle with generating the correct number
of objects specified in prompts. Existing approaches typically incorporate
auxiliary counting networks as external critics to enhance numeracy. However,
since these critics must provide gradient guidance during generation, they are
restricted to regression-based models that are inherently differentiable, thus
excluding detector-based models with superior counting ability, whose
count-via-enumeration nature is non-differentiable. To overcome this
limitation, we propose Detector-to-Differentiable (D2D), a novel framework that
transforms non-differentiable detection models into differentiable critics,
thereby leveraging their superior counting ability to guide numeracy
generation. Specifically, we design custom activation functions to convert
detector logits into soft binary indicators, which are then used to optimize
the noise prior at inference time with pre-trained T2I models. Our extensive
experiments on SDXL-Turbo, SD-Turbo, and Pixart-DMD across four benchmarks of
varying complexity (low-density, high-density, and multi-object scenarios)
demonstrate consistent and substantial improvements in object counting accuracy
(e.g., boosting up to 13.7% on D2D-Small, a 400-prompt, low-density benchmark),
with minimal degradation in overall image quality and computational overhead.

</details>


### [79] [Enhancing Early Alzheimer Disease Detection through Big Data and Ensemble Few-Shot Learning](https://arxiv.org/abs/2510.19282)
*Safa Ben Atitallah,Maha Driss,Wadii Boulila,Anis Koubaa*

Main category: cs.CV

TL;DR: 本文提出了一种基于预训练CNN、Few-Shot Learning和集成学习的阿尔茨海默病检测方法，旨在解决标记医疗数据有限的问题。


<details>
  <summary>Details</summary>
Motivation: 标记医疗数据的稀缺性给阿尔茨海默病精确检测带来了挑战，需要有效的方法来提高检测准确性，同时考虑数据隐私约束。

Method: 利用预训练的卷积神经网络（CNN）在Few-Shot Learning（FSL）和集成学习的框架内，提出了一种基于原型网络（ProtoNet）的集成方法，结合类别感知损失和熵损失。

Result: 在Kaggle Alzheimer数据集和ADNI数据集上分别实现了99.72%和99.86%的准确率。

Conclusion: 该方法优于现有技术，具有实际应用潜力，可用于早期阿尔茨海默病检测。

Abstract: Alzheimer disease is a severe brain disorder that causes harm in various
brain areas and leads to memory damage. The limited availability of labeled
medical data poses a significant challenge for accurate Alzheimer disease
detection. There is a critical need for effective methods to improve the
accuracy of Alzheimer disease detection, considering the scarcity of labeled
data, the complexity of the disease, and the constraints related to data
privacy. To address this challenge, our study leverages the power of big data
in the form of pre-trained Convolutional Neural Networks (CNNs) within the
framework of Few-Shot Learning (FSL) and ensemble learning. We propose an
ensemble approach based on a Prototypical Network (ProtoNet), a powerful method
in FSL, integrating various pre-trained CNNs as encoders. This integration
enhances the richness of features extracted from medical images. Our approach
also includes a combination of class-aware loss and entropy loss to ensure a
more precise classification of Alzheimer disease progression levels. The
effectiveness of our method was evaluated using two datasets, the Kaggle
Alzheimer dataset and the ADNI dataset, achieving an accuracy of 99.72% and
99.86%, respectively. The comparison of our results with relevant
state-of-the-art studies demonstrated that our approach achieved superior
accuracy and highlighted its validity and potential for real-world applications
in early Alzheimer disease detection.

</details>


### [80] [Vision-Based Mistake Analysis in Procedural Activities: A Review of Advances and Challenges](https://arxiv.org/abs/2510.19292)
*Konstantinos Bacharidis,Antonis A. Argyros*

Main category: cs.CV

TL;DR: 本文综述了基于视觉的方法，用于检测和预测结构化任务中的错误，重点关注程序性和执行性错误。


<details>
  <summary>Details</summary>
Motivation: 程序性活动中的错误分析是一个重要的研究领域，其应用涵盖工业自动化、物理康复、教育和人机协作。

Method: 通过利用计算机视觉的进步，包括动作识别、预测和活动理解，基于视觉的系统可以识别任务执行中的偏差，例如不正确的排序、使用不当的技术或时间错误。我们根据程序结构、监督级别和学习策略的使用对方法进行分类，全面概述了现有数据集、评估指标和最先进的方法。

Result: 讨论了开放的挑战，例如区分允许的变化与真正的错误，以及对错误传播进行建模，以及未来的方向，包括神经符号推理和反事实状态建模。

Conclusion: 这项工作旨在对基于视觉的程序活动中的错误分析建立统一的视角，强调其在提高安全性、效率和跨不同领域的任务性能方面的潜力。

Abstract: Mistake analysis in procedural activities is a critical area of research with
applications spanning industrial automation, physical rehabilitation, education
and human-robot collaboration. This paper reviews vision-based methods for
detecting and predicting mistakes in structured tasks, focusing on procedural
and executional errors. By leveraging advancements in computer vision,
including action recognition, anticipation and activity understanding,
vision-based systems can identify deviations in task execution, such as
incorrect sequencing, use of improper techniques, or timing errors. We explore
the challenges posed by intra-class variability, viewpoint differences and
compositional activity structures, which complicate mistake detection.
Additionally, we provide a comprehensive overview of existing datasets,
evaluation metrics and state-of-the-art methods, categorizing approaches based
on their use of procedural structure, supervision levels and learning
strategies. Open challenges, such as distinguishing permissible variations from
true mistakes and modeling error propagation are discussed alongside future
directions, including neuro-symbolic reasoning and counterfactual state
modeling. This work aims to establish a unified perspective on vision-based
mistake analysis in procedural activities, highlighting its potential to
enhance safety, efficiency and task performance across diverse domains.

</details>


### [81] [Unified Reinforcement and Imitation Learning for Vision-Language Models](https://arxiv.org/abs/2510.19307)
*Byung-Kwan Lee,Ryo Hachiuma,Yong Man Ro,Yu-Chiang Frank Wang,Yueh-Hua Wu*

Main category: cs.CV

TL;DR: 提出了一种新的高效训练算法，用于创建强大的轻量级视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型因其庞大的规模，在资源受限的环境中通常不实用。

Method: 结合了强化学习和对抗模仿学习的优点，使较小的学生视觉语言模型不仅可以模仿大型教师模型的复杂文本生成，还可以通过强化信号系统地提高其生成能力。模仿框架的关键是基于LLM的判别器，它可以敏锐地区分学生和教师的输出，并辅以多个大型教师视觉语言模型的指导，以确保多样化的学习。

Result: 在各种视觉语言基准上的大量实验表明，RIL 显着缩小了与最先进的开源和闭源视觉语言模型的性能差距，并且在某些情况下超过了它们。

Conclusion: 统一的学习策略，利用强化和模仿，使学生模型能够获得显着的性能提升，使它们与领先的闭源视觉语言模型竞争。

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress, yet their
large scale often renders them impractical for resource-constrained
environments. This paper introduces Unified Reinforcement and Imitation
Learning (RIL), a novel and efficient training algorithm designed to create
powerful, lightweight VLMs. RIL distinctively combines the strengths of
reinforcement learning with adversarial imitation learning. This enables
smaller student VLMs not only to mimic the sophisticated text generation of
large teacher models but also to systematically improve their generative
capabilities through reinforcement signals. Key to our imitation framework is
an LLM-based discriminator that adeptly distinguishes between student and
teacher outputs, complemented by guidance from multiple large teacher VLMs to
ensure diverse learning. This unified learning strategy, leveraging both
reinforcement and imitation, empowers student models to achieve significant
performance gains, making them competitive with leading closed-source VLMs.
Extensive experiments on diverse vision-language benchmarks demonstrate that
RIL significantly narrows the performance gap with state-of-the-art open- and
closed-source VLMs and, in several instances, surpasses them.

</details>


### [82] [Online Handwritten Signature Verification Based on Temporal-Spatial Graph Attention Transformer](https://arxiv.org/abs/2510.19321)
*Hai-jie Yuan,Heng Zhang,Fei Yin*

Main category: cs.CV

TL;DR: 提出了一种新的动态签名验证方法，Temporal-Spatial Graph Attention Transformer (TS-GATR)，该方法结合了图注意力网络 (GAT) 和门控循环单元 (GRU) 来建模签名数据中的空间和时间依赖性。


<details>
  <summary>Details</summary>
Motivation: 在签名验证中实现高精度仍然具有挑战，因为用户内部的可变性和伪造的风险。

Method: 使用图注意力网络 (GAT) 和门控循环单元 (GRU) 结合，通过将签名表示为图来增强验证性能，其中每个节点捕获动态特征（例如位置、速度、压力），并通过使用注意力机制来建模它们的复杂关系。使用 Dual-Graph Attention Transformer (DGATR) 模块，该模块利用 k 步和 k 最近邻邻接图分别建模局部和全局空间特征。为了捕获长期时间依赖性，该模型集成了 GRU。

Result: 在 MSDS 和 DeepSignDB 等基准数据集上进行的综合实验表明，TS-GATR 超过了当前最先进的方法，在各种场景中始终实现了较低的等错误率 (EER)。

Conclusion: TS-GATR 是一种有效的动态签名验证方法。

Abstract: Handwritten signature verification is a crucial aspect of identity
authentication, with applications in various domains such as finance and
e-commerce. However, achieving high accuracy in signature verification remains
challenging due to intra-user variability and the risk of forgery. This paper
introduces a novel approach for dynamic signature verification: the
Temporal-Spatial Graph Attention Transformer (TS-GATR). TS-GATR combines the
Graph Attention Network (GAT) and the Gated Recurrent Unit (GRU) to model both
spatial and temporal dependencies in signature data. TS-GATR enhances
verification performance by representing signatures as graphs, where each node
captures dynamic features (e.g. position, velocity, pressure), and by using
attention mechanisms to model their complex relationships. The proposed method
further employs a Dual-Graph Attention Transformer (DGATR) module, which
utilizes k-step and k-nearest neighbor adjacency graphs to model local and
global spatial features, respectively. To capture long-term temporal
dependencies, the model integrates GRU, thereby enhancing its ability to learn
dynamic features during signature verification. Comprehensive experiments
conducted on benchmark datasets such as MSDS and DeepSignDB show that TS-GATR
surpasses current state-of-the-art approaches, consistently achieving lower
Equal Error Rates (EER) across various scenarios.

</details>


### [83] [Seabed-Net: A multi-task network for joint bathymetry estimation and seabed classification from remote sensing imagery in shallow waters](https://arxiv.org/abs/2510.19329)
*Panagiotis Agrafiotis,Begüm Demir*

Main category: cs.CV

TL;DR: Seabed-Net是一个统一的多任务框架，可以同时预测遥感图像的测深和基于像素的海床分类。


<details>
  <summary>Details</summary>
Motivation: 现有的方法将深度或海床类别从遥感图像中分离出来，放弃了它们相互作用的互惠互利，阻碍了深度学习方法的更广泛应用。

Method: Seabed-Net采用双分支编码器进行测深估计和基于像素的海床分类，通过注意力特征融合模块和窗口Swin-Transformer融合块集成跨任务特征，并通过动态任务不确定性加权平衡目标。

Result: 在两个不同的沿海地点的广泛评估中，Seabed-Net始终优于传统的经验模型和传统的机器学习回归方法，降低了高达75%的RMSE。与最先进的单任务和多任务基线相比，它还将测深RMSE降低了10-30%，并将海床分类精度提高了8%。

Conclusion: 联合建模深度与基底和海床栖息地可以产生协同增益，为综合浅水测绘提供了一个强大的开放解决方案。

Abstract: Accurate, detailed, and regularly updated bathymetry, coupled with complex
semantic content, is essential for under-mapped shallow-water environments
facing increasing climatological and anthropogenic pressures. However, existing
approaches that derive either depth or seabed classes from remote sensing
imagery treat these tasks in isolation, forfeiting the mutual benefits of their
interaction and hindering the broader adoption of deep learning methods. To
address these limitations, we introduce Seabed-Net, a unified multi-task
framework that simultaneously predicts bathymetry and pixel-based seabed
classification from remote sensing imagery of various resolutions. Seabed-Net
employs dual-branch encoders for bathymetry estimation and pixel-based seabed
classification, integrates cross-task features via an Attention Feature Fusion
module and a windowed Swin-Transformer fusion block, and balances objectives
through dynamic task uncertainty weighting. In extensive evaluations at two
heterogeneous coastal sites, it consistently outperforms traditional empirical
models and traditional machine learning regression methods, achieving up to
75\% lower RMSE. It also reduces bathymetric RMSE by 10-30\% compared to
state-of-the-art single-task and multi-task baselines and improves seabed
classification accuracy up to 8\%. Qualitative analyses further demonstrate
enhanced spatial consistency, sharper habitat boundaries, and corrected depth
biases in low-contrast regions. These results confirm that jointly modeling
depth with both substrate and seabed habitats yields synergistic gains,
offering a robust, open solution for integrated shallow-water mapping. Code and
pretrained weights are available at https://github.com/pagraf/Seabed-Net.

</details>


### [84] [Exploring Scale Shift in Crowd Localization under the Context of Domain Generalization](https://arxiv.org/abs/2510.19330)
*Juncheng Wang,Lei Shang,Ziqi Liu,Wang Lu,Xixu Hu,Zhe Hu,Jindong Wang,Shujun Wang*

Main category: cs.CV

TL;DR: 本文研究了人群定位中的尺度偏移域泛化问题，并提出了 Catto 算法来缓解尺度偏移的影响。


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练和测试数据之间存在头部尺度分布差异（尺度偏移），导致性能显著下降，即域泛化 (DG) 问题。本文旨在理解人群定位模型域泛化中尺度偏移的本质。

Method: 1. 系统地检查人群定位性能如何随不同程度的尺度偏移而变化。
2. 建立基准 ScaleBench，并重现 20 种先进的 DG 算法来量化这种影响。
3. 对尺度偏移进行严格的理论分析。
4. 提出了一种有效的算法，称为因果特征分解和各向异性处理 (Catto)，以减轻 DG 设置中尺度偏移的影响。

Result: 1. 证明了现有算法的局限性，并强调了尺度偏移的重要性和复杂性。
2. 揭示了未来研究的四个重要见解。
3. Catto 算法有效缓解了尺度偏移的影响。

Conclusion: 本文强调了尺度偏移域泛化这一新的且适用的研究方向的重要性。

Abstract: Crowd localization plays a crucial role in visual scene understanding towards
predicting each pedestrian location in a crowd, thus being applicable to
various downstream tasks. However, existing approaches suffer from significant
performance degradation due to discrepancies in head scale distributions (scale
shift) between training and testing data, a challenge known as domain
generalization (DG). This paper aims to comprehend the nature of scale shift
within the context of domain generalization for crowd localization models. To
this end, we address four critical questions: (i) How does scale shift
influence crowd localization in a DG scenario? (ii) How can we quantify this
influence? (iii) What causes this influence? (iv) How to mitigate the
influence? Initially, we conduct a systematic examination of how crowd
localization performance varies with different levels of scale shift. Then, we
establish a benchmark, ScaleBench, and reproduce 20 advanced DG algorithms to
quantify the influence. Through extensive experiments, we demonstrate the
limitations of existing algorithms and underscore the importance and complexity
of scale shift, a topic that remains insufficiently explored. To deepen our
understanding, we provide a rigorous theoretical analysis on scale shift.
Building on these insights, we further propose an effective algorithm called
Causal Feature Decomposition and Anisotropic Processing (Catto) to mitigate the
influence of scale shift in DG settings. Later, we also provide extensive
analytical experiments, revealing four significant insights for future
research. Our results emphasize the importance of this novel and applicable
research direction, which we term Scale Shift Domain Generalization.

</details>


### [85] [BrainMCLIP: Brain Image Decoding with Multi-Layer feature Fusion of CLIP](https://arxiv.org/abs/2510.19332)
*Tian Xia,Zihan Ma,Xinlong Wang,Qing Liu,Xiaowei He,Tianming Liu,Yudan Ren*

Main category: cs.CV

TL;DR: BrainMCLIP: A parameter-efficient, multi-layer fusion approach guided by the human visual system's functional hierarchy to decode images from fMRI signals, eliminating the need for a separate VAE pathway.


<details>
  <summary>Details</summary>
Motivation: Existing methods for decoding images from fMRI often add a parameter-intensive VAE-based pipeline to capture finer visual details, overlooking rich object information within CLIP's intermediate layers and contradicting the brain's functionally hierarchical organization.

Method: BrainMCLIP aligns fMRI signals from functionally distinct visual areas to corresponding intermediate and final CLIP layers, using a Cross-Reconstruction strategy and a novel multi-granularity loss.

Result: BrainMCLIP achieves highly competitive performance, particularly excelling on high-level semantic metrics, while using substantially fewer parameters compared to VAE-based SOTA methods.

Conclusion: BrainMCLIP effectively captures visual details by leveraging intermediate CLIP features, achieving a balance between semantic accuracy and detail fidelity without requiring a separate VAE pipeline.

Abstract: Decoding images from fMRI often involves mapping brain activity to CLIP's
final semantic layer. To capture finer visual details, many approaches add a
parameter-intensive VAE-based pipeline. However, these approaches overlook rich
object information within CLIP's intermediate layers and contradicts the
brain's functionally hierarchical. We introduce BrainMCLIP, which pioneers a
parameter-efficient, multi-layer fusion approach guided by human visual
system's functional hierarchy, eliminating the need for such a separate VAE
pathway. BrainMCLIP aligns fMRI signals from functionally distinct visual areas
(low-/high-level) to corresponding intermediate and final CLIP layers,
respecting functional hierarchy. We further introduce a Cross-Reconstruction
strategy and a novel multi-granularity loss. Results show BrainMCLIP achieves
highly competitive performance, particularly excelling on high-level semantic
metrics where it matches or surpasses SOTA(state-of-the-art) methods, including
those using VAE pipelines. Crucially, it achieves this with substantially fewer
parameters, demonstrating a reduction of
71.7\%(Table.\ref{tab:compare_clip_vae}) compared to top VAE-based SOTA
methods, by avoiding the VAE pathway. By leveraging intermediate CLIP features,
it effectively captures visual details often missed by CLIP-only approaches,
striking a compelling balance between semantic accuracy and detail fidelity
without requiring a separate VAE pipeline.

</details>


### [86] [A Training-Free Framework for Open-Vocabulary Image Segmentation and Recognition with EfficientNet and CLIP](https://arxiv.org/abs/2510.19333)
*Ying Dai,Wei Yu Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的无训练框架，用于开放词汇图像分割和对象识别 (OVSR)。


<details>
  <summary>Details</summary>
Motivation: 利用卷积神经网络 EfficientNetB0 进行无监督分割，并利用视觉语言模型 CLIP 进行开放词汇对象识别。

Method: 该框架采用两阶段流程：无监督图像分割，然后通过视觉语言对齐进行分段级识别。第一阶段，使用奇异值分解分解从 EfficientNetB0 提取的像素特征以获得潜在表示，然后使用分层聚类对这些潜在表示进行聚类以分割语义上有意义的区域。聚类数量由奇异值的分布自适应确定。在第二阶段，使用 CLIP 的 Vision Transformer 主干将分割区域定位并编码为图像嵌入。使用 CLIP 的文本编码器从类别特定的提示预先计算文本嵌入，包括通用的“其他内容”提示以支持开放集识别。图像和文本嵌入被连接并通过 SVD 投影到共享的潜在特征空间中，以增强跨模态对齐。通过计算投影图像和文本嵌入之间的相似度上的 softmax 来执行识别。

Result: 在 COCO、ADE20K 和 PASCAL VOC 等标准基准上进行了评估，在 Hungarian mIoU、精确率、召回率和 F1 分数方面实现了最先进的性能。

Conclusion: 这些结果证明了所提出的框架的有效性、灵活性和通用性。

Abstract: This paper presents a novel training-free framework for open-vocabulary image
segmentation and object recognition (OVSR), which leverages EfficientNetB0, a
convolutional neural network, for unsupervised segmentation and CLIP, a
vision-language model, for open-vocabulary object recognition. The proposed
framework adopts a two stage pipeline: unsupervised image segmentation followed
by segment-level recognition via vision-language alignment. In the first stage,
pixel-wise features extracted from EfficientNetB0 are decomposed using singular
value decomposition to obtain latent representations, which are then clustered
using hierarchical clustering to segment semantically meaningful regions. The
number of clusters is adaptively determined by the distribution of singular
values. In the second stage, the segmented regions are localized and encoded
into image embeddings using the Vision Transformer backbone of CLIP. Text
embeddings are precomputed using CLIP's text encoder from category-specific
prompts, including a generic something else prompt to support open set
recognition. The image and text embeddings are concatenated and projected into
a shared latent feature space via SVD to enhance cross-modal alignment.
Recognition is performed by computing the softmax over the similarities between
the projected image and text embeddings. The proposed method is evaluated on
standard benchmarks, including COCO, ADE20K, and PASCAL VOC, achieving
state-of-the-art performance in terms of Hungarian mIoU, precision, recall, and
F1-score. These results demonstrate the effectiveness, flexibility, and
generalizability of the proposed framework.

</details>


### [87] [DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents](https://arxiv.org/abs/2510.19336)
*Kai Shi,Jun Yang,Ni Yang,Binqiang Pan,Qingsong Xie,Chao Zhang,Zhenyu Yang,Tianhuang Su,Haonan Lu*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为DaMo的数据混合优化器，用于提升多模态大语言模型在同时处理多个手机任务时的性能。同时，作者构建了PhoneAgentBench，一个专门用于评估多模态模型在多模态手机任务上的benchmark。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务监督微调方法难以确定最佳的训练数据组成，从而无法达到最佳性能。

Method: 该论文提出了一种新颖的解决方案DaMo，它使用一个可训练的网络，通过预测给定数据集比例的下游任务性能来预测最佳数据混合。

Result: DaMo在PhoneAgentBench上实现了3.38%的性能提升，并且在其他benchmark上也表现出更好的泛化能力。在BFCL-v3任务上，DaMo比其他方法提高了12.47%。

Conclusion: DaMo具有强大的可扩展性，并且在应用于其他模型架构时仍能保持其有效性。

Abstract: Mobile Phone Agents (MPAs) have emerged as a promising research direction due
to their broad applicability across diverse scenarios. While Multimodal Large
Language Models (MLLMs) serve as the foundation for MPAs, their effectiveness
in handling multiple mobile phone tasks simultaneously remains limited.
Although multitask supervised fine-tuning (SFT) is widely adopted for multitask
learning, existing approaches struggle to determine optimal training data
compositions for peak performance. To address this challenge, we propose DaMo
(Data Mixture Optimizer) - a novel solution employing a trainable network that
predicts optimal data mixtures by forecasting downstream task performance for
any given dataset ratio. To support comprehensive evaluation, we introduce
PhoneAgentBench, the first specialized benchmark to evaluate MLLMs on
multimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse
real-world industrial mobile application scenarios. Demonstrating strong
predictive capability (R^2=0.81) in small-scale pilot experiments, DaMo
efficiently extrapolates optimal data mixing configurations. Our results show
DaMo achieves a 3.38% performance improvement on PhoneAgentBench compared to
alternative methods. Furthermore, extensive experiments across established
benchmarks including BFCL-v3, MME-Reasoning, MME-Perception, and OCRBench
reveal DaMo's superior generalization, outperforming other approaches by 2.57%
in terms of average score. When used solely for MLLM optimization on the
BFCL-v3 task, DaMo improves the metrics by 12.47% than other methods. Notably,
DaMo maintains robust scalability, preserving its effectiveness when applied to
other model architectures. The code and dataset are available at
https://github.com/OPPO-Mente-Lab/DaMo.git

</details>


### [88] [DARE: A Deformable Adaptive Regularization Estimator for Learning-Based Medical Image Registration](https://arxiv.org/abs/2510.19353)
*Ahsan Raza Siyal,Markus Haltmeier,Ruth Steiger,Malik Galijasevic,Elke Ruth Gizewski,Astrid Ellen Grams*

Main category: cs.CV

TL;DR: 提出了一种新的医学图像配准框架 DARE，它可以动态调整弹性正则化，平衡稳定性和灵活性，并包含折叠预防机制。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的医学图像配准方法忽略了正则化在确保鲁棒性和解剖学合理性方面的关键作用。

Method: DARE 框架基于形变场的梯度范数动态调整弹性正则化，并整合了应变和剪切能量项进行自适应调节。此外，还包含一个折叠预防机制，惩罚具有负形变雅可比的区域。

Result: DARE 减轻了折叠等非物理伪影，避免了过度平滑，提高了配准精度和解剖学合理性。

Conclusion: DARE 框架能够动态调整正则化，从而提升医学图像配准的鲁棒性和解剖学合理性。

Abstract: Deformable medical image registration is a fundamental task in medical image
analysis. While deep learning-based methods have demonstrated superior accuracy
and computational efficiency compared to traditional techniques, they often
overlook the critical role of regularization in ensuring robustness and
anatomical plausibility. We propose DARE (Deformable Adaptive Regularization
Estimator), a novel registration framework that dynamically adjusts elastic
regularization based on the gradient norm of the deformation field. Our
approach integrates strain and shear energy terms, which are adaptively
modulated to balance stability and flexibility. To ensure physically realistic
transformations, DARE includes a folding-prevention mechanism that penalizes
regions with negative deformation Jacobian. This strategy mitigates
non-physical artifacts such as folding, avoids over-smoothing, and improves
both registration accuracy and anatomical plausibility

</details>


### [89] [AegisRF: Adversarial Perturbations Guided with Sensitivity for Protecting Intellectual Property of Neural Radiance Fields](https://arxiv.org/abs/2510.19371)
*Woo Jae Kim,Kyu Beom Han,Yoonki Cho,Youngju Na,Junsik Jung,Sooel Son,Sung-eui Yoon*

Main category: cs.CV

TL;DR: 该论文提出了一种名为AegisRF的框架，用于保护神经辐射场（NeRFs）的知识产权，通过注入对抗性扰动来干扰未经授权的应用，同时保持渲染质量。


<details>
  <summary>Details</summary>
Motivation: 保护NeRFs的知识产权，防止未经授权的使用。

Method: 引入可学习的敏感度来量化几何扰动对渲染质量的空间变化影响，并提出包含扰动场和敏感度场的AegisRF框架。

Result: 实验结果表明AegisRF在各种下游任务和模态中具有广泛的适用性，包括多视图图像分类和基于体素的3D定位，同时保持高视觉保真度。

Conclusion: AegisRF框架能够有效地保护NeRFs的知识产权，同时保持渲染质量。

Abstract: As Neural Radiance Fields (NeRFs) have emerged as a powerful tool for 3D
scene representation and novel view synthesis, protecting their intellectual
property (IP) from unauthorized use is becoming increasingly crucial. In this
work, we aim to protect the IP of NeRFs by injecting adversarial perturbations
that disrupt their unauthorized applications. However, perturbing the 3D
geometry of NeRFs can easily deform the underlying scene structure and thus
substantially degrade the rendering quality, which has led existing attempts to
avoid geometric perturbations or restrict them to explicit spaces like meshes.
To overcome this limitation, we introduce a learnable sensitivity to quantify
the spatially varying impact of geometric perturbations on rendering quality.
Building upon this, we propose AegisRF, a novel framework that consists of a
Perturbation Field, which injects adversarial perturbations into the
pre-rendering outputs (color and volume density) of NeRF models to fool an
unauthorized downstream target model, and a Sensitivity Field, which learns the
sensitivity to adaptively constrain geometric perturbations, preserving
rendering quality while disrupting unauthorized use. Our experimental
evaluations demonstrate the generalized applicability of AegisRF across diverse
downstream tasks and modalities, including multi-view image classification and
voxel-based 3D localization, while maintaining high visual fidelity. Codes are
available at https://github.com/wkim97/AegisRF.

</details>


### [90] [Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes](https://arxiv.org/abs/2510.19400)
*Zhiyuan Feng,Zhaolu Kang,Qijie Wang,Zhiying Du,Jiongrui Yan,Shubin Shi,Chengbo Yuan,Huizhi Liang,Yu Deng,Qixiu Li,Rushuai Yang,Arctanx An,Leqi Zheng,Weijie Wang,Shawn Chen,Sicheng Xu,Yaobo Liang,Jiaolong Yang,Baining Guo*

Main category: cs.CV

TL;DR: This paper introduces MV-RoboBench, a new benchmark for evaluating the multi-view spatial reasoning capabilities of VLMs in robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: Existing VLM evaluations mainly focus on single-view settings, while multi-camera setups are increasingly standard in robotics. It's unclear whether VLMs can effectively use multi-view inputs for robotic reasoning.

Method: The authors created MV-RoboBench, a benchmark with 1.7k QA items across eight subtasks, divided into spatial understanding and robotic execution. They evaluated various VLMs, including open-source and closed-source models, with CoT-inspired techniques.

Result: State-of-the-art models perform far below human level. Spatial intelligence and robotic task execution are positively correlated. Performance on single-view spatial understanding benchmarks does not reliably translate to success in the robotic spatial tasks.

Conclusion: The authors release MV-RoboBench as an open resource to foster progress in spatially grounded VLMs and VLAs, providing data and a standardized evaluation protocol for multi-view embodied reasoning.

Abstract: Vision-language models (VLMs) are essential to Embodied AI, enabling robots
to perceive, reason, and act in complex environments. They also serve as the
foundation for the recent Vision-Language-Action (VLA) models. Yet most
evaluations of VLMs focus on single-view settings, leaving their ability to
integrate multi-view information underexplored. At the same time, multi-camera
setups are increasingly standard in robotic platforms, as they provide
complementary perspectives to mitigate occlusion and depth ambiguity. Whether
VLMs can effectively leverage such multi-view inputs for robotic reasoning
therefore remains an open question. To bridge this gap, we introduce
MV-RoboBench, a benchmark specifically designed to evaluate the multi-view
spatial reasoning capabilities of VLMs in robotic manipulation. MV-RoboBench
consists of 1.7k manually curated QA items across eight subtasks, divided into
two primary categories: spatial understanding and robotic execution. We
evaluate a diverse set of existing VLMs, including both open-source and
closed-source models, along with enhanced versions incorporating CoT-inspired
techniques. The results show that state-of-the-art models remain far below
human performance, underscoring the substantial challenges VLMs face in
multi-view robotic perception. Additionally, our analysis uncovers two key
findings: (i) spatial intelligence and robotic task execution are positively
correlated in multi-view robotic scenarios; and (ii) strong performance on
existing general-purpose single-view spatial understanding benchmarks does not
reliably translate to success in the robotic spatial tasks assessed by our
benchmark. We release MV-RoboBench as an open resource to foster progress in
spatially grounded VLMs and VLAs, providing not only data but also a
standardized evaluation protocol for multi-view embodied reasoning.

</details>


### [91] [Multi-Camera Worker Tracking in Logistics Warehouse Considering Wide-Angle Distortion](https://arxiv.org/abs/2510.19432)
*Yuki Mori,Kazuma Kano,Yusuke Asai,Shin Katayama,Kenta Urano,Takuro Yonezawa,Nobuo Kawaguchi*

Main category: cs.CV

TL;DR: 本研究利用安装在天花板上的19个广角摄像头，通过基于脚部位置对齐的方式，减少图像畸变的影响，从而实现对仓库工人位置的精确追踪，最终将追踪精度提高了20%以上。


<details>
  <summary>Details</summary>
Motivation: 提高仓库运营效率至关重要，而数字孪生技术的应用越来越受到关注。为了实现这一目标，需要准确收集仓库中工人的位置并将其反映在虚拟空间中。

Method: 使用19个安装在天花板上的广角摄像头，通过基于地面进行对齐，并利用脚部位置对齐来减少广角镜头造成的图像畸变，从而实现跨摄像头的精确位置对齐。

Result: 追踪精度提高了20%以上，验证了所提出方法的有效性。

Conclusion: 通过提出的方法，可以有效提高仓库工人位置追踪的精度。

Abstract: With the spread of e-commerce, the logistics market is growing around the
world. Therefore, improving the efficiency of warehouse operations is
essential. To achieve this, various approaches have been explored, and among
them, the use of digital twins is gaining attention. To make this approach
possible, it is necessary to accurately collect the positions of workers in a
warehouse and reflect them in a virtual space. However, a single camera has
limitations in its field of view, therefore sensing with multiple cameras is
necessary. In this study, we explored a method to track workers using 19
wide-angle cameras installed on the ceiling, looking down at the floor of the
logistics warehouse. To understand the relationship between the camera
coordinates and the actual positions in the warehouse, we performed alignment
based on the floor surface. However, due to the characteristics of wide-angle
cameras, significant distortion occurs at the edges of the image, particularly
in the vertical direction. To address this, the detected worker positions from
each camera were aligned based on foot positions, reducing the effects of image
distortion, and enabling accurate position alignment across cameras. As a
result, we confirmed an improvement of over 20% in tracking accuracy.
Furthermore, we compared multiple methods for utilizing appearance features and
validated the effectiveness of the proposed approach.

</details>


### [92] [Reasoning Like Experts: Leveraging Multimodal Large Language Models for Drawing-based Psychoanalysis](https://arxiv.org/abs/2510.19451)
*Xueqi Ma,Yanbei Jiang,Sarah Erfani,James Bailey,Weifeng Liu,Krista A. Ehinger,Jey Han Lau*

Main category: cs.CV

TL;DR: 本文介绍了一种名为PICK的多步骤框架，旨在通过分层分析和知识注入，利用多模态大型语言模型进行心理图像理解，特别关注房树人(HTP)测试。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大型语言模型在客观的多模态感知任务中表现出色，但在主观、情感细微的领域（如心理分析）中的应用仍未被探索。

Method: 该方法首先将包含多个实例的图画分解为语义上有意义的子图画，构建一个分层表示，捕捉跨三个层次（单对象层次、多对象层次和整体层次）的空间结构和内容。然后，在每个层次上有针对性地分析这些子图画，从视觉线索中提取心理或情感洞察。此外，还引入了一个HTP知识库，并设计了一个通过强化学习训练的特征提取模块，以生成用于单对象层次分析的心理特征。

Result: 实验结果表明，所提出的PICK显著提高了MLLM在心理分析中的能力。通过扩展到情感理解任务，进一步验证了其作为一个通用框架的有效性。

Conclusion: 该方法弥合了多模态大型语言模型和专业领域之间的差距，提供了一个结构化和可解释的框架，用于通过视觉表达理解人类精神状态。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated exceptional
performance across various objective multimodal perception tasks, yet their
application to subjective, emotionally nuanced domains, such as psychological
analysis, remains largely unexplored. In this paper, we introduce PICK, a
multi-step framework designed for Psychoanalytical Image Comprehension through
hierarchical analysis and Knowledge injection with MLLMs, specifically focusing
on the House-Tree-Person (HTP) Test, a widely used psychological assessment in
clinical practice. First, we decompose drawings containing multiple instances
into semantically meaningful sub-drawings, constructing a hierarchical
representation that captures spatial structure and content across three levels:
single-object level, multi-object level, and whole level. Next, we analyze
these sub-drawings at each level with a targeted focus, extracting
psychological or emotional insights from their visual cues. We also introduce
an HTP knowledge base and design a feature extraction module, trained with
reinforcement learning, to generate a psychological profile for single-object
level analysis. This profile captures both holistic stylistic features and
dynamic object-specific features (such as those of the house, tree, or person),
correlating them with psychological states. Finally, we integrate these
multi-faceted information to produce a well-informed assessment that aligns
with expert-level reasoning. Our approach bridges the gap between MLLMs and
specialized expert domains, offering a structured and interpretable framework
for understanding human mental states through visual expression. Experimental
results demonstrate that the proposed PICK significantly enhances the
capability of MLLMs in psychological analysis. It is further validated as a
general framework through extensions to emotion understanding tasks.

</details>


### [93] [Exploring "Many in Few" and "Few in Many" Properties in Long-Tailed, Highly-Imbalanced IC Defect Classification](https://arxiv.org/abs/2510.19463)
*Hao-Chiang Shao,Chun-Hao Chang,Yu-Hsien Lin,Chia-Wen Lin,Shao-Yun Fang,Yan-Hsiu Liu*

Main category: cs.CV

TL;DR: 这篇论文介绍了一个新的IC缺陷图像数据集（IC-Defect-14），并提出了一个新的模型（ReCAME-Net）来解决该数据集中存在的类内多样性和类间相似性的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的深度分类技术和自动光学检测模型在实际IC缺陷分类任务中表现不佳，因为真实世界的数据分布比公共数据集更不平衡，并且样本包含类特异性和类无关的特征。

Method: 提出了ReCAME-Net，它是一个多专家分类器框架，集成了区域通道注意力模块、度量学习损失、硬类别挖掘策略和知识蒸馏程序。

Result: ReCAME-Net在IC-Defect-14数据集上优于先前的state-of-the-art模型，并在通用公共数据集上保持了相当的性能和竞争力。

Conclusion: ReCAME-Net模型有效地解决了高不平衡IC缺陷数据集中存在的挑战。

Abstract: Despite significant advancements in deep classification techniques and in-lab
automatic optical inspection models for long-tailed or highly imbalanced data,
applying these approaches to real-world IC defect classification tasks remains
challenging. This difficulty stems from two primary factors. First, real-world
conditions, such as the high yield-rate requirements in the IC industry, result
in data distributions that are far more skewed than those found in general
public imbalanced datasets. Consequently, classifiers designed for open
imbalanced datasets often fail to perform effectively in real-world scenarios.
Second, real-world samples exhibit a mix of class-specific attributes and
class-agnostic, domain-related features. This complexity adds significant
difficulty to the classification process, particularly for highly imbalanced
datasets. To address these challenges, this paper introduces the IC-Defect-14
dataset, a large, highly imbalanced IC defect image dataset sourced from AOI
systems deployed in real-world IC production lines. This dataset is
characterized by its unique "intra-class clusters" property, which presents two
major challenges: large intra-class diversity and high inter-class similarity.
These characteristics, rarely found simultaneously in existing public datasets,
significantly degrade the performance of current state-of-the-art classifiers
for highly imbalanced data. To tackle this challenge, we propose ReCAME-Net,
which follows a multi-expert classifier framework and integrates a regional
channel attention module, metric learning losses, a hard category mining
strategy, and a knowledge distillation procedure. Extensive experimental
evaluations demonstrate that ReCAME-Net outperforms previous state-of-the-art
models on the IC-Defect-14 dataset while maintaining comparable performance and
competitiveness on general public datasets.

</details>


### [94] [PCP-GAN: Property-Constrained Pore-scale image reconstruction via conditional Generative Adversarial Networks](https://arxiv.org/abs/2510.19465)
*Ali Sadeghkhani,Brandon Bennett,Masoud Babaei,Arash Rabbani*

Main category: cs.CV

TL;DR: 本研究提出了一种多条件生成对抗网络 (cGAN) 框架，用于生成具有精确控制特性的代表性孔隙尺度图像。


<details>
  <summary>Details</summary>
Motivation: 在 subsurface 表征中，获取与 bulk 形成特性相匹配的真正具有代表性的孔隙尺度图像仍然是一个根本性的挑战，因为自然空间异质性导致提取的子图像与岩心测量值显着偏差。数据稀缺加剧了这一挑战，物理样本仅在稀疏的井位可用。

Method: 该框架在来自碳酸盐地层的四个深度（1879.50-1943.50 m）的薄片样本上进行训练，同时以孔隙度值和深度参数为条件，在一个统一的模型中进行训练。

Result: 该模型在所有地层中实现了出色的孔隙度控制 (R^2=0.95)，平均绝对误差为 0.0099-0.0197。形态学验证证实了关键孔隙网络特征（包括平均孔隙半径、比表面积和曲率）的保留，统计差异保持在可接受的地质公差范围内。最重要的是，与随机提取的真实子图像的 36.4-578% 相比，生成的图像显示出卓越的代表性，双重约束误差为 1.9-11.3%。

Conclusion: 这种能力为 subsurface 表征提供了变革性工具，对于碳储存、地热能和地下水管理应用尤其有价值，在这些应用中，了解孔隙空间的代表性形态对于实施数字岩石物理至关重要。

Abstract: Obtaining truly representative pore-scale images that match bulk formation
properties remains a fundamental challenge in subsurface characterization, as
natural spatial heterogeneity causes extracted sub-images to deviate
significantly from core-measured values. This challenge is compounded by data
scarcity, where physical samples are only available at sparse well locations.
This study presents a multi-conditional Generative Adversarial Network (cGAN)
framework that generates representative pore-scale images with precisely
controlled properties, addressing both the representativeness challenge and
data availability constraints. The framework was trained on thin section
samples from four depths (1879.50-1943.50 m) of a carbonate formation,
simultaneously conditioning on porosity values and depth parameters within a
single unified model. This approach captures both universal pore network
principles and depth-specific geological characteristics, from grainstone
fabrics with interparticle-intercrystalline porosity to crystalline textures
with anhydrite inclusions. The model achieved exceptional porosity control
(R^2=0.95) across all formations with mean absolute errors of 0.0099-0.0197.
Morphological validation confirmed preservation of critical pore network
characteristics including average pore radius, specific surface area, and
tortuosity, with statistical differences remaining within acceptable geological
tolerances. Most significantly, generated images demonstrated superior
representativeness with dual-constraint errors of 1.9-11.3% compared to
36.4-578% for randomly extracted real sub-images. This capability provides
transformative tools for subsurface characterization, particularly valuable for
carbon storage, geothermal energy, and groundwater management applications
where knowing the representative morphology of the pore space is critical for
implementing digital rock physics.

</details>


### [95] [Predicting before Reconstruction: A generative prior framework for MRI acceleration](https://arxiv.org/abs/2510.19472)
*Juhyung Park,Rokgi Hong,Roh-Eul Yoo,Jaehyeon Koo,Se Young Chun,Seung Hong Choi,Jongho Lee*

Main category: cs.CV

TL;DR: 提出了一种新的MRI加速范式，从图像重建转变为主动预测成像。


<details>
  <summary>Details</summary>
Motivation: MRI扫描时间长，限制了临床吞吐量。

Method: 首先预测目标对比度图像，然后将其用作数据驱动先验，以重建高度欠采样的MRI数据。生成模型以多种数据源为条件进行预测。

Result: 该方法在多个数据集上优于其他方法，包括那些具有替代或没有先验信息的方法。

Conclusion: 通过该框架，我们引入了从图像重建到预测成像的根本转变。

Abstract: Recent advancements in artificial intelligence have created transformative
capabilities in image synthesis and generation, enabling diverse research
fields to innovate at revolutionary speed and spectrum. In this study, we
leverage this generative power to introduce a new paradigm for accelerating
Magnetic Resonance Imaging (MRI), introducing a shift from image reconstruction
to proactive predictive imaging. Despite being a cornerstone of modern patient
care, MRI's lengthy acquisition times limit clinical throughput. Our novel
framework addresses this challenge by first predicting a target contrast image,
which then serves as a data-driven prior for reconstructing highly
under-sampled data. This informative prior is predicted by a generative model
conditioned on diverse data sources, such as other contrast images, previously
scanned images, acquisition parameters, patient information. We demonstrate
this approach with two key applications: (1) reconstructing FLAIR images using
predictions from T1w and/or T2w scans, and (2) reconstructing T1w images using
predictions from previously acquired T1w scans. The framework was evaluated on
internal and multiple public datasets (total 14,921 scans; 1,051,904 slices),
including multi-channel k-space data, for a range of high acceleration factors
(x4, x8 and x12). The results demonstrate that our prediction-prior
reconstruction method significantly outperforms other approaches, including
those with alternative or no prior information. Through this framework we
introduce a fundamental shift from image reconstruction towards a new paradigm
of predictive imaging.

</details>


### [96] [PRGCN: A Graph Memory Network for Cross-Sequence Pattern Reuse in 3D Human Pose Estimation](https://arxiv.org/abs/2510.19475)
*Zhuoyang Xie,Yibo Zhao,Hui Huang,Riwei Wang,Zan Gao*

Main category: cs.CV

TL;DR: 提出了一种新的单目3D人体姿态估计框架，通过在图卷积网络中引入模式重用机制，利用跨序列的人体运动模式来提高姿态估计的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视频人体姿态估计方法孤立地处理每个序列，忽略了跨序列的结构规律和重复运动模式。

Method: 设计了一个模式重用图卷积网络（PRGCN），它使用图记忆库存储姿态原型，并通过注意力机制动态检索这些原型作为结构先验，结合硬编码的解剖约束，利用记忆驱动的图卷积进行姿态估计。同时，采用双流混合架构，结合Mamba和自注意力机制提取时空特征。

Result: 在Human3.6M和MPI-INF-3DHP数据集上取得了新的state-of-the-art结果，MPJPE分别为37.1mm和13.4mm，并表现出更强的跨域泛化能力。

Conclusion: 跨序列模式重用机制对于推动人体姿态估计领域的发展至关重要，将研究范式从逐序列优化转变为累积知识学习。

Abstract: Monocular 3D human pose estimation remains a fundamentally ill-posed inverse
problem due to the inherent depth ambiguity in 2D-to-3D lifting. While
contemporary video-based methods leverage temporal context to enhance spatial
reasoning, they operate under a critical paradigm limitation: processing each
sequence in isolation, thereby failing to exploit the strong structural
regularities and repetitive motion patterns that pervade human movement across
sequences. This work introduces the Pattern Reuse Graph Convolutional Network
(PRGCN), a novel framework that formalizes pose estimation as a problem of
pattern retrieval and adaptation. At its core, PRGCN features a graph memory
bank that learns and stores a compact set of pose prototypes, encoded as
relational graphs, which are dynamically retrieved via an attention mechanism
to provide structured priors. These priors are adaptively fused with hard-coded
anatomical constraints through a memory-driven graph convolution, ensuring
geometrical plausibility. To underpin this retrieval process with robust
spatiotemporal features, we design a dual-stream hybrid architecture that
synergistically combines the linear-complexity, local temporal modeling of
Mamba-based state-space models with the global relational capacity of
self-attention. Extensive evaluations on Human3.6M and MPI-INF-3DHP benchmarks
demonstrate that PRGCN establishes a new state-of-the-art, achieving an MPJPE
of 37.1mm and 13.4mm, respectively, while exhibiting enhanced cross-domain
generalization capability. Our work posits that the long-overlooked mechanism
of cross-sequence pattern reuse is pivotal to advancing the field, shifting the
paradigm from per-sequence optimization towards cumulative knowledge learning.

</details>


### [97] [Mitigating representation bias caused by missing pixels in methane plume detection](https://arxiv.org/abs/2510.19478)
*Julia Wąsala,Joannes D. Maasakkers,Ilse Aben,Rochelle Schneider,Holger Hoos,Mitra Baratchi*

Main category: cs.CV

TL;DR: 卫星图像由于云等因素通常会系统性地缺失像素，这会导致自动特征提取模型中出现偏差。这篇论文旨在解决这个问题，通过多种方法来减少缺失像素对甲烷羽流检测的影响。


<details>
  <summary>Details</summary>
Motivation: 未解决的缺失像素会导致表征偏差，例如在甲烷羽流检测中，模型可能会将图像覆盖率与标签相关联，从而导致低覆盖率图像中的羽流检测不足。

Method: 论文评估了多种插补方法来消除覆盖率和标签之间的依赖性，并提出了一种加权重采样方案，通过在每个覆盖率箱中强制类别平衡来消除标签和覆盖率之间的关联。

Result: 结果表明，重采样和插补都可以显著减少表征偏差，且不影响平衡的准确率、精确率或召回率。在实际场景中，经过偏差校正的模型更有可能检测到低覆盖率图像中的羽流。

Conclusion: 该研究表明，通过重采样和插补等技术可以有效减少卫星图像中缺失像素导致的偏差，提高甲烷羽流检测的准确性，尤其是在低覆盖率图像中。

Abstract: Most satellite images have systematically missing pixels (i.e., missing data
not at random (MNAR)) due to factors such as clouds. If not addressed, these
missing pixels can lead to representation bias in automated feature extraction
models. In this work, we show that spurious association between the label and
the number of missing values in methane plume detection can cause the model to
associate the coverage (i.e., the percentage of valid pixels in an image) with
the label, subsequently under-detecting plumes in low-coverage images. We
evaluate multiple imputation approaches to remove the dependence between the
coverage and a label. Additionally, we propose a weighted resampling scheme
during training that removes the association between the label and the coverage
by enforcing class balance in each coverage bin. Our results show that both
resampling and imputation can significantly reduce the representation bias
without hurting balanced accuracy, precision, or recall. Finally, we evaluate
the capability of the debiased models using these techniques in an operational
scenario and demonstrate that the debiased models have a higher chance of
detecting plumes in low-coverage images.

</details>


### [98] [Towards Single-Source Domain Generalized Object Detection via Causal Visual Prompts](https://arxiv.org/abs/2510.19487)
*Chen Li,Huiying Xu,Changxin Gao,Zeyu Wang,Yun Liu,Xinzhong Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Cauvis的单源域泛化目标检测方法，旨在提高模型在未见过的目标域中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法试图通过数据增强来缓解域差异，但由于域偏移和有限的领域知识，模型容易陷入虚假相关的陷阱，过度依赖简单的分类特征（如颜色），而不是像对象轮廓这样重要的域不变表示。

Method: 该方法首先引入一个交叉注意力提示模块，通过将视觉提示与交叉注意力集成来减轻来自虚假特征的偏差。然后，提出了一个双分支适配器，通过高频特征提取来解开因果-虚假特征，同时实现域适应。

Result: Cauvis在SDGOD数据集上实现了最先进的性能，与现有的域泛化方法相比，增益为15.9-31.4%，并在复杂的干扰环境中表现出显著的鲁棒性优势。

Conclusion: Cauvis方法有效地提高了模型在单源域泛化目标检测任务中的性能和鲁棒性。

Abstract: Single-source Domain Generalized Object Detection (SDGOD), as a cutting-edge
research topic in computer vision, aims to enhance model generalization
capability in unseen target domains through single-source domain training.
Current mainstream approaches attempt to mitigate domain discrepancies via data
augmentation techniques. However, due to domain shift and limited
domain-specific knowledge, models tend to fall into the pitfall of spurious
correlations. This manifests as the model's over-reliance on simplistic
classification features (e.g., color) rather than essential domain-invariant
representations like object contours. To address this critical challenge, we
propose the Cauvis (Causal Visual Prompts) method. First, we introduce a
Cross-Attention Prompts module that mitigates bias from spurious features by
integrating visual prompts with cross-attention. To address the inadequate
domain knowledge coverage and spurious feature entanglement in visual prompts
for single-domain generalization, we propose a dual-branch adapter that
disentangles causal-spurious features while achieving domain adaptation via
high-frequency feature extraction. Cauvis achieves state-of-the-art performance
with 15.9-31.4% gains over existing domain generalization methods on SDGOD
datasets, while exhibiting significant robustness advantages in complex
interference environments.

</details>


### [99] [CARES: Context-Aware Resolution Selector for VLMs](https://arxiv.org/abs/2510.19496)
*Moshe Kimhi,Nimrod Shabtay,Raja Giryes,Chaim Baskin,Eli Schwartz*

Main category: cs.CV

TL;DR: 本文介绍了一种名为CARES的轻量级预处理模块，它可以预测图像-查询对所需的最小分辨率。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型视觉语言模型（VLM）处理高分辨率图像时计算量大、延迟高的问题，即使低分辨率图像就足够了。

Method: CARES使用一个紧凑的VLM（350M）来提取特征，并预测目标预训练VLM的响应何时收敛到其正确回答能力。

Result: 在跨越文档和自然图像的五个多模态基准测试以及各种目标VLM中，CARES在保持任务性能的同时，最多可减少80％的计算量。

Conclusion: CARES能够显著降低计算成本，同时保持VLM的性能。

Abstract: Large vision-language models (VLMs) commonly process images at native or high
resolution to remain effective across tasks. This inflates visual tokens ofter
to 97-99% of total tokens, resulting in high compute and latency, even when
low-resolution images would suffice. We introduce \emph{CARES}-a
\textbf{C}ontext-\textbf{A}ware \textbf{R}esolution \textbf{S}elector, a
lightweight preprocessing module that, given an image-query pair, predicts the
\emph{minimal} sufficient input resolution. CARES uses a compact VLM (350M) to
extract features and predict when a target pretrained VLM's response converges
to its peak ability to answer correctly. Though trained as a discrete
classifier over a set of optional resolutions, CARES interpolates continuous
resolutions at inference for fine-grained control. Across five multimodal
benchmarks spanning documents and natural images, as well as diverse target
VLMs, CARES preserves task performance while reducing compute by up to 80%.

</details>


### [100] [PoseCrafter: Extreme Pose Estimation with Hybrid Video Synthesis](https://arxiv.org/abs/2510.19527)
*Qing Mao,Tianxin Huang,Yu Zhu,Jinqiu Sun,Yanning Zhang,Gim Hee Lee*

Main category: cs.CV

TL;DR: 论文提出了一种新的相机位姿估计方法，特别是在图像对重叠较少的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图像对重叠较少或没有重叠时表现不佳；视频插值生成的帧模糊；关键帧选择策略慢且与位姿估计不一致。

Method: 提出混合视频生成（HVG）方法，结合视频插值模型和位姿条件的新视角合成模型，以合成更清晰的中间帧。提出基于特征匹配的选择器（FMS）来选择适合位姿估计的中间帧。

Result: 在Cambridge Landmarks、ScanNet、DL3DV-10K和NAVI数据集上的大量实验表明，与现有的SOTA方法相比，PoseCrafter可以显著提高位姿估计性能，特别是在重叠较少或没有重叠的例子中。

Conclusion: 该方法在小或无重叠图像对的相机位姿估计方面有显著提升。

Abstract: Pairwise camera pose estimation from sparsely overlapping image pairs remains
a critical and unsolved challenge in 3D vision. Most existing methods struggle
with image pairs that have small or no overlap. Recent approaches attempt to
address this by synthesizing intermediate frames using video interpolation and
selecting key frames via a self-consistency score. However, the generated
frames are often blurry due to small overlap inputs, and the selection
strategies are slow and not explicitly aligned with pose estimation. To solve
these cases, we propose Hybrid Video Generation (HVG) to synthesize clearer
intermediate frames by coupling a video interpolation model with a
pose-conditioned novel view synthesis model, where we also propose a Feature
Matching Selector (FMS) based on feature correspondence to select intermediate
frames appropriate for pose estimation from the synthesized results. Extensive
experiments on Cambridge Landmarks, ScanNet, DL3DV-10K, and NAVI demonstrate
that, compared to existing SOTA methods, PoseCrafter can obviously enhance the
pose estimation performances, especially on examples with small or no overlap.

</details>


### [101] [[De|Re]constructing VLMs' Reasoning in Counting](https://arxiv.org/abs/2510.19555)
*Simone Alghisi,Gabriel Roccabruna,Massimo Rizzoli,Seyed Mahed Mousavi,Giuseppe Riccardi*

Main category: cs.CV

TL;DR: 视觉语言模型(VLM)在视觉推理方面存在局限性，例如难以识别关系、理解时间序列和计数对象。


<details>
  <summary>Details</summary>
Motivation: 探究VLM在视觉推理方面的不足，并有针对性地提高它们的推理能力。

Method: 在受控实验条件下，研究了七个最先进的VLM在计数任务中的推理技能。通过逐层分析，揭示了错误是由于最后一层表示到输出空间的不正确映射造成的。

Result: 实验表明，VLM对物体的数量和类型、空间排列以及干扰物的共现高度敏感。微调输出层可将准确率提高高达21%。

Conclusion: 通过在真实世界数据集上取得一致的改进，证实了这些发现。

Abstract: Vision-Language Models (VLMs) have recently gained attention due to their
competitive performance on multiple downstream tasks, achieved by following
user-input instructions. However, VLMs still exhibit several limitations in
visual reasoning, such as difficulties in identifying relations (e.g., spatial,
temporal, and among objects), understanding temporal sequences (e.g., frames),
and counting objects. In this work, we go beyond score-level benchmark
evaluations of VLMs by investigating the underlying causes of their failures
and proposing a targeted approach to improve their reasoning capabilities. We
study the reasoning skills of seven state-of-the-art VLMs in the counting task
under controlled experimental conditions. Our experiments show that VLMs are
highly sensitive to the number and type of objects, their spatial arrangement,
and the co-occurrence of distractors. A layer-wise analysis reveals that errors
are due to incorrect mapping of the last-layer representation into the output
space. Our targeted training shows that fine-tuning just the output layer
improves accuracy by up to 21%. We corroborate these findings by achieving
consistent improvements on real-world datasets.

</details>


### [102] [The Intricate Dance of Prompt Complexity, Quality, Diversity, and Consistency in T2I Models](https://arxiv.org/abs/2510.19557)
*Xiaofeng Zhang,Aaron Courville,Michal Drozdzal,Adriana Romero-Soriano*

Main category: cs.CV

TL;DR: 本研究探讨了提示词复杂性对文本到图像模型生成合成数据效用的影响，发现增加提示词复杂性会降低条件多样性和提示词一致性，但减少合成数据到真实数据的分布偏移。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型在生成合成数据方面潜力巨大，但提示词复杂性对合成数据质量、多样性和一致性的影响尚不明确。

Method: 该研究首先进行合成实验，并通过理论推导解释了泛化困难的问题。然后，引入一个新的评估框架来比较真实数据和合成数据的效用，并全面分析了提示词复杂性如何影响常用文本到图像模型生成的合成数据的效用。研究使用了包括CC12M、ImageNet-1k和DCI在内的不同数据集，并评估了不同的推理时干预方法。

Result: 合成实验表明，泛化到更一般的条件比反过来更难，因为前者需要扩散模型未学习的估计似然。大规模经验实验表明，增加提示词复杂性会导致较低的条件多样性和提示词一致性，同时减少合成数据到真实数据的分布偏移。此外，目前的推理时干预可以在牺牲真实数据支持的情况下增加生成的多样性。在这些干预措施中，通过故意使用预训练语言模型作为似然估计器的提示词扩展，在图像多样性和美学方面始终表现出最高的性能，甚至高于真实数据。

Conclusion: 提示词复杂性对文本到图像模型生成合成数据具有显著影响，提示词扩展是一种有效的干预方法，可以提高生成数据的多样性和美学质量。

Abstract: Text-to-image (T2I) models offer great potential for creating virtually
limitless synthetic data, a valuable resource compared to fixed and finite real
datasets. Previous works evaluate the utility of synthetic data from T2I models
on three key desiderata: quality, diversity, and consistency. While prompt
engineering is the primary means of interacting with T2I models, the systematic
impact of prompt complexity on these critical utility axes remains
underexplored. In this paper, we first conduct synthetic experiments to
motivate the difficulty of generalization w.r.t. prompt complexity and explain
the observed difficulty with theoretical derivations. Then, we introduce a new
evaluation framework that can compare the utility of real data and synthetic
data, and present a comprehensive analysis of how prompt complexity influences
the utility of synthetic data generated by commonly used T2I models. We conduct
our study across diverse datasets, including CC12M, ImageNet-1k, and DCI, and
evaluate different inference-time intervention methods. Our synthetic
experiments show that generalizing to more general conditions is harder than
the other way round, since the former needs an estimated likelihood that is not
learned by diffusion models. Our large-scale empirical experiments reveal that
increasing prompt complexity results in lower conditional diversity and prompt
consistency, while reducing the synthetic-to-real distribution shift, which
aligns with the synthetic experiments. Moreover, current inference-time
interventions can augment the diversity of the generations at the expense of
moving outside the support of real data. Among those interventions, prompt
expansion, by deliberately using a pre-trained language model as a likelihood
estimator, consistently achieves the highest performance in both image
diversity and aesthetics, even higher than that of real data.

</details>


### [103] [HAD: Hierarchical Asymmetric Distillation to Bridge Spatio-Temporal Gaps in Event-Based Object Tracking](https://arxiv.org/abs/2510.19560)
*Yao Deng,Xian Zhong,Wenxuan Liu,Zhaofei Yu,Jingling Yuan,Tiejun Huang*

Main category: cs.CV

TL;DR: 提出了一种用于RGB相机和事件相机融合的层级非对称蒸馏框架（HAD），以提高在高速运动、HDR环境和动态背景干扰等挑战性条件下的目标跟踪性能。


<details>
  <summary>Details</summary>
Motivation: RGB相机和事件相机在空间分辨率和时间分辨率上具有互补优势，但由于成像机制的根本差异，它们之间存在显著的时空不对称性，阻碍了有效融合。

Method: 提出了一个层级对齐策略，以最小化信息损失，同时保持学生网络的计算效率和参数紧凑性。

Result: 实验结果表明，HAD始终优于最先进的方法，并且全面的消融研究进一步验证了每个设计组件的有效性和必要性。

Conclusion: HAD框架有效地弥合了RGB相机和事件相机之间的时空不对称性，显著提升了目标跟踪性能。

Abstract: RGB cameras excel at capturing rich texture details with high spatial
resolution, whereas event cameras offer exceptional temporal resolution and a
high dynamic range (HDR). Leveraging their complementary strengths can
substantially enhance object tracking under challenging conditions, such as
high-speed motion, HDR environments, and dynamic background interference.
However, a significant spatio-temporal asymmetry exists between these two
modalities due to their fundamentally different imaging mechanisms, hindering
effective multi-modal integration. To address this issue, we propose
{Hierarchical Asymmetric Distillation} (HAD), a multi-modal knowledge
distillation framework that explicitly models and mitigates spatio-temporal
asymmetries. Specifically, HAD proposes a hierarchical alignment strategy that
minimizes information loss while maintaining the student network's
computational efficiency and parameter compactness. Extensive experiments
demonstrate that HAD consistently outperforms state-of-the-art methods, and
comprehensive ablation studies further validate the effectiveness and necessity
of each designed component. The code will be released soon.

</details>


### [104] [Can You Trust What You See? Alpha Channel No-Box Attacks on Video Object Detection](https://arxiv.org/abs/2510.19574)
*Ariana Yi,Ce Zhou,Liyang Xiao,Qiben Yan*

Main category: cs.CV

TL;DR: 本文提出了一种名为{\alpha}-Cloak的无盒对抗攻击，该攻击通过RGBA视频的alpha通道进行，可以在不访问模型架构、参数或输出的情况下，欺骗目标检测器。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶汽车（AVs）和监控平台等网络物理系统中，目标检测模型越来越多地被部署，确保其免受对抗性威胁至关重要。以往的研究主要集中在图像领域的对抗性攻击，而视频领域的攻击，特别是在无盒设置下的攻击，仍然很大程度上未被研究。

Method: 该方法利用alpha通道将恶意目标视频与良性视频融合，生成一种融合视频，这种视频对人类观察者来说是无害的，但始终能欺骗目标检测器。该攻击不需要访问模型架构、参数或输出，并且不会引入可察觉的伪影。作者系统地研究了常见视频格式和播放应用程序对alpha通道的支持，并设计了一种融合算法，以确保视觉隐蔽性和兼容性。

Result: 在五个最先进的目标检测器、一个视觉-语言模型和一个多模态大型语言模型（Gemini-2.0-Flash）上评估了{\alpha}-Cloak，结果表明在所有场景中攻击成功率均为100%。

Conclusion: 研究结果揭示了基于视频的感知系统中以前未被探索的漏洞，强调了迫切需要考虑对抗环境中alpha通道的防御措施。

Abstract: As object detection models are increasingly deployed in cyber-physical
systems such as autonomous vehicles (AVs) and surveillance platforms, ensuring
their security against adversarial threats is essential. While prior work has
explored adversarial attacks in the image domain, those attacks in the video
domain remain largely unexamined, especially in the no-box setting. In this
paper, we present {\alpha}-Cloak, the first no-box adversarial attack on object
detectors that operates entirely through the alpha channel of RGBA videos.
{\alpha}-Cloak exploits the alpha channel to fuse a malicious target video with
a benign video, resulting in a fused video that appears innocuous to human
viewers but consistently fools object detectors. Our attack requires no access
to model architecture, parameters, or outputs, and introduces no perceptible
artifacts. We systematically study the support for alpha channels across common
video formats and playback applications, and design a fusion algorithm that
ensures visual stealth and compatibility. We evaluate {\alpha}-Cloak on five
state-of-the-art object detectors, a vision-language model, and a multi-modal
large language model (Gemini-2.0-Flash), demonstrating a 100% attack success
rate across all scenarios. Our findings reveal a previously unexplored
vulnerability in video-based perception systems, highlighting the urgent need
for defenses that account for the alpha channel in adversarial settings.

</details>


### [105] [VGD: Visual Geometry Gaussian Splatting for Feed-Forward Surround-view Driving Reconstruction](https://arxiv.org/abs/2510.19578)
*Junhong Lin,Kangli Wang,Shunzhou Wang,Songlin Fan,Ge Li,Wei Gao*

Main category: cs.CV

TL;DR: 提出了一种新的前馈端到端学习框架Visual Gaussian Driving (VGD)，用于解决环视自动驾驶场景重建中的泛化性和新视角质量之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常无法保证新视角的几何一致性和重建质量，因为环视视角重叠区域很小。为了解决这个问题，论文认为必须显式地学习几何信息，并利用这些信息来提高新视角的语义质量。

Method: 设计了一个轻量级的VGGT架构变体，以有效地将VGGT的几何先验知识提炼到几何分支中。设计了一个高斯头，融合多尺度几何标记，预测新视角渲染的高斯参数。集成了来自几何和高斯头分支的多尺度特征，以共同监督语义细化模型，通过特征一致性学习优化渲染质量。

Result: 在nuScenes上的实验表明，该方法在各种设置下的客观指标和主观质量方面均优于现有技术。

Conclusion: VGD具有可扩展性和高保真环视重建能力。

Abstract: Feed-forward surround-view autonomous driving scene reconstruction offers
fast, generalizable inference ability, which faces the core challenge of
ensuring generalization while elevating novel view quality. Due to the
surround-view with minimal overlap regions, existing methods typically fail to
ensure geometric consistency and reconstruction quality for novel views. To
tackle this tension, we claim that geometric information must be learned
explicitly, and the resulting features should be leveraged to guide the
elevating of semantic quality in novel views. In this paper, we introduce
\textbf{Visual Gaussian Driving (VGD)}, a novel feed-forward end-to-end
learning framework designed to address this challenge. To achieve generalizable
geometric estimation, we design a lightweight variant of the VGGT architecture
to efficiently distill its geometric priors from the pre-trained VGGT to the
geometry branch. Furthermore, we design a Gaussian Head that fuses multi-scale
geometry tokens to predict Gaussian parameters for novel view rendering, which
shares the same patch backbone as the geometry branch. Finally, we integrate
multi-scale features from both geometry and Gaussian head branches to jointly
supervise a semantic refinement model, optimizing rendering quality through
feature-consistent learning. Experiments on nuScenes demonstrate that our
approach significantly outperforms state-of-the-art methods in both objective
metrics and subjective quality under various settings, which validates VGD's
scalability and high-fidelity surround-view reconstruction.

</details>


### [106] [Multi-modal Co-learning for Earth Observation: Enhancing single-modality models via modality collaboration](https://arxiv.org/abs/2510.19579)
*Francisco Mena,Dino Ienco,Cassio F. Dantas,Roberto Interdonato,Andreas Dengel*

Main category: cs.CV

TL;DR: 提出了一种新的多模态协同学习框架，该框架能够跨各种任务进行泛化，而无需针对用于推理的特定模态。


<details>
  <summary>Details</summary>
Motivation: 在机器学习中，多模态协同学习正成为一种有效的范例，使模型能够协同地从不同的模态中学习，以增强单模态预测。遥感 (EO) 代表了多模态数据分析的典型领域，其中不同的遥感器收集数据来感知我们的地球。这种前所未有的数据量带来了新的挑战。具体而言，在训练和推理阶段访问相同的传感器模态变得越来越复杂，这是基于影响遥感平台的实际约束。

Method: 结合对比学习和模态判别学习，以指导单模态模型将内部模型流形构建为模态共享和模态特定的信息。

Result: 在跨不同传感器模态的分类和回归任务的四个 EO 基准上评估了我们的框架，其中在推理时只能访问训练期间可用的模态之一。我们的结果表明，与最近的机器学习和计算机视觉文献以及 EO 特定方法相比，预测能力得到了持续的提高。

Conclusion: 研究结果验证了我们的框架在各种 EO 应用的单模态推理场景中的有效性。

Abstract: Multi-modal co-learning is emerging as an effective paradigm in machine
learning, enabling models to collaboratively learn from different modalities to
enhance single-modality predictions. Earth Observation (EO) represents a
quintessential domain for multi-modal data analysis, wherein diverse remote
sensors collect data to sense our planet. This unprecedented volume of data
introduces novel challenges. Specifically, the access to the same sensor
modalities at both training and inference stages becomes increasingly complex
based on real-world constraints affecting remote sensing platforms. In this
context, multi-modal co-learning presents a promising strategy to leverage the
vast amount of sensor-derived data available at the training stage to improve
single-modality models for inference-time deployment. Most current research
efforts focus on designing customized solutions for either particular
downstream tasks or specific modalities available at the inference stage. To
address this, we propose a novel multi-modal co-learning framework capable of
generalizing across various tasks without targeting a specific modality for
inference. Our approach combines contrastive and modality discriminative
learning together to guide single-modality models to structure the internal
model manifold into modality-shared and modality-specific information. We
evaluate our framework on four EO benchmarks spanning classification and
regression tasks across different sensor modalities, where only one of the
modalities available during training is accessible at inference time. Our
results demonstrate consistent predictive improvements over state-of-the-art
approaches from the recent machine learning and computer vision literature, as
well as EO-specific methods. The obtained findings validate our framework in
the single-modality inference scenarios across a diverse range of EO
applications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [107] [Test-time Verification via Optimal Transport: Coverage, ROC, & Sub-optimality](https://arxiv.org/abs/2510.18982)
*Arpan Mukherjee,Marcello Bullo,Debabrota Basu,Deniz Gündüz*

Main category: cs.AI

TL;DR: 本文研究了带有验证的测试时缩放方法，旨在提升大型语言模型（LLM）的性能，但现有研究对验证器的作用和缺陷探索不足。本文建立了一个统一的框架，将可验证的测试时缩放视为一个传输问题，从而量化覆盖率、收敛区域（ROC）和采样算法的次优性之间的相互作用。揭示了次优性-覆盖率曲线的三种状态。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能统一量化覆盖率、收敛区域和采样算法次优性之间的相互作用。

Method: 将可验证的测试时缩放视为一个传输问题，从而表征覆盖率、收敛区域和次优性的相互作用。提出并分析了两类采样算法——顺序采样和批量采样。

Result: 揭示了次优性-覆盖率曲线的三种状态：传输状态、策略改进状态和饱和状态。实验结果与理论发现相符。

Conclusion: 本文通过理论分析和实验验证，加深了对带有验证的测试时缩放方法的理解，并为未来的研究提供了指导。

Abstract: While test-time scaling with verification has shown promise in improving the
performance of large language models (LLMs), the role of the verifier and its
imperfections remain underexplored. The effect of verification manifests
through interactions of three quantities: (i) the generator's coverage, (ii)
the verifier's region of convergence (ROC), and (iii) the sampling algorithm's
sub-optimality. Though recent studies capture subsets of these factors, a
unified framework quantifying the geometry of their interplay is missing. We
frame verifiable test-time scaling as a transport problem. This characterizes
the interaction of coverage, ROC, and sub-optimality, and uncovers that the
sub-optimality--coverage curve exhibits three regimes. A transport regime --
where sub-optimality increases with coverage, a policy improvement regime --
where sub-optimality may decrease with coverage, depending on the verifier's
ROC, and a saturation regime -- where sub-optimality plateaus, unaffected by
coverage. We further propose and analyze two classes of sampling algorithms --
sequential and batched, and examine how their computational complexities shape
these trade-offs. Empirical results with Qwen, Llama, and Gemma models
corroborate our theoretical findings.

</details>


### [108] [Timely Clinical Diagnosis through Active Test Selection](https://arxiv.org/abs/2510.18988)
*Silas Ruhrberg Estévez,Nicolás Astorga,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 提出了一种名为ACTMED的诊断框架，该框架结合了贝叶斯实验设计（BED）与大型语言模型（LLM），以模拟现实世界的诊断推理。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习方法在临床诊断中依赖于静态、完全观察的数据集，无法反映临床医生在实践中使用的顺序、资源敏感的推理。诊断仍然复杂且容易出错，尤其是在高压或资源有限的环境中。

Method: ACTMED通过选择预期能最大程度减少患者诊断不确定性的测试来优化诊断过程。LLM作为灵活的模拟器，生成合理的患者状态分布，并支持信念更新，而无需结构化的、特定于任务的训练数据。

Result: 在真实世界的数据集上评估ACTMED，结果表明它可以优化测试选择，从而提高诊断准确性、可解释性和资源利用率。

Conclusion: ACTMED代表着朝着透明、自适应和与临床医生对齐的诊断系统迈出的一步，该系统可在各种环境中推广，并减少对特定领域数据的依赖。

Abstract: There is growing interest in using machine learning (ML) to support clinical
diag- nosis, but most approaches rely on static, fully observed datasets and
fail to reflect the sequential, resource-aware reasoning clinicians use in
practice. Diagnosis remains complex and error prone, especially in
high-pressure or resource-limited settings, underscoring the need for
frameworks that help clinicians make timely and cost-effective decisions. We
propose ACTMED (Adaptive Clinical Test selection via Model-based Experimental
Design), a diagnostic framework that integrates Bayesian Experimental Design
(BED) with large language models (LLMs) to better emulate real-world diagnostic
reasoning. At each step, ACTMED selects the test expected to yield the greatest
reduction in diagnostic uncertainty for a given patient. LLMs act as flexible
simulators, generating plausible patient state distributions and supporting
belief updates without requiring structured, task-specific training data.
Clinicians can remain in the loop; reviewing test suggestions, interpreting
intermediate outputs, and applying clinical judgment throughout. We evaluate
ACTMED on real-world datasets and show it can optimize test selection to
improve diagnostic accuracy, interpretability, and resource use. This
represents a step to- ward transparent, adaptive, and clinician-aligned
diagnostic systems that generalize across settings with reduced reliance on
domain-specific data.

</details>


### [109] [Rectifying Shortcut Behaviors in Preference-based Reward Learning](https://arxiv.org/abs/2510.19050)
*Wenqian Ye,Guangtao Zheng,Aidong Zhang*

Main category: cs.AI

TL;DR: 提出PRISM，缓解基于偏好的奖励学习中的shortcut行为


<details>
  <summary>Details</summary>
Motivation: 现有的基于偏好的奖励模型容易出现奖励作弊和泛化性差的问题，因为它们会利用shortcut来获得高奖励分数，而不是真正反映预期目标。

Method: 受到核视角不变理论的启发，提出了偏好奖励不变性（PRISM），它使用闭式学习目标学习具有特征图的群不变核。

Result: 在多个基准测试中，该方法持续提高了奖励模型在各种分布外任务中的准确性，并减少了下游策略模型对shortcut的依赖性。

Conclusion: 建立了一个基于偏好的对齐的稳健框架。

Abstract: In reinforcement learning from human feedback, preference-based reward models
play a central role in aligning large language models to human-aligned
behavior. However, recent studies show that these models are prone to reward
hacking and often fail to generalize well due to over-optimization. They
achieve high reward scores by exploiting shortcuts, that is, exploiting
spurious features (e.g., response verbosity, agreeable tone, or sycophancy)
that correlate with human preference labels in the training data rather than
genuinely reflecting the intended objectives. In this paper, instead of probing
these issues one at a time, we take a broader view of the reward hacking
problem as shortcut behaviors and introduce a principled yet flexible approach
to mitigate shortcut behaviors in preference-based reward learning. Inspired by
the invariant theory in the kernel perspective, we propose Preference-based
Reward Invariance for Shortcut Mitigation (PRISM), which learns group-invariant
kernels with feature maps in a closed-form learning objective. Experimental
results in several benchmarks show that our method consistently improves the
accuracy of the reward model on diverse out-of-distribution tasks and reduces
the dependency on shortcuts in downstream policy models, establishing a robust
framework for preference-based alignment.

</details>


### [110] [The MUSE Benchmark: Probing Music Perception and Auditory Relational Reasoning in Audio LLMS](https://arxiv.org/abs/2510.19055)
*Brandon James Carone,Iran R. Roman,Pablo Ripollés*

Main category: cs.AI

TL;DR: 论文介绍了一个新的音乐理解和结构评估基准（MUSE），用于评估多模态大型语言模型（MLLMs）在音乐感知方面的能力。


<details>
  <summary>Details</summary>
Motivation: 当前对MLLMs在音频理解方面的评估可能掩盖了其在关系推理方面的根本弱点。为了解决这个问题，论文提出了MUSE基准。

Method: 论文使用MUSE基准评估了四个最先进的模型（Gemini Pro和Flash、Qwen2.5-Omni和Audio-Flamingo 3），并与大型人类基线（N=200）进行了比较。

Result: 结果表明，最先进的模型在能力上存在很大差异，并且与人类专家之间存在持续的差距。Gemini Pro在基本感知方面表现良好，但Qwen和Audio Flamingo 3的表现接近或低于随机水平，表明存在严重的感知缺陷。此外，Chain-of-Thought (CoT) prompting 提示方法会产生不一致且通常有害的结果。

Conclusion: 论文提供了一个关键工具，用于评估不变的音乐表征并推动更强大的AI系统的开发。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated capabilities in
audio understanding, but current evaluations may obscure fundamental weaknesses
in relational reasoning. We introduce the Music Understanding and Structural
Evaluation (MUSE) Benchmark, an open-source resource with 10 tasks designed to
probe fundamental music perception skills. We evaluate four SOTA models (Gemini
Pro and Flash, Qwen2.5-Omni, and Audio-Flamingo 3) against a large human
baseline (N=200). Our results reveal a wide variance in SOTA capabilities and a
persistent gap with human experts. While Gemini Pro succeeds on basic
perception, Qwen and Audio Flamingo 3 perform at or near chance, exposing
severe perceptual deficits. Furthermore, we find Chain-of-Thought (CoT)
prompting provides inconsistent, often detrimental results. Our work provides a
critical tool for evaluating invariant musical representations and driving
development of more robust AI systems.

</details>


### [111] [A Multi-faceted Analysis of Cognitive Abilities: Evaluating Prompt Methods with Large Language Models on the CONSORT Checklist](https://arxiv.org/abs/2510.19139)
*Sohyeon Jeon,Hyung-Chul Lee*

Main category: cs.AI

TL;DR: 本研究探讨了大型语言模型（LLM）在评估临床试验报告中CONSORT标准方面的能力。


<details>
  <summary>Details</summary>
Motivation: 目前尚不清楚大型语言模型在多大程度上能够根据CONSORT标准评估临床试验报告，特别是在认知和推理策略方面。

Method: 本研究采用行为和元认知分析方法，利用专家验证的数据，在三种提示条件下系统地比较了两个具有代表性的大型语言模型。

Result: 研究结果表明，模型在处理各种CONSORT项目和提示类型时存在明显差异，包括推理风格的变化、明确的不确定性和替代解释会影响响应模式。

Conclusion: 研究结果强调了这些系统在临床合规自动化方面的局限性，并强调了理解其认知适应和战略行为对于开发更具解释性和可靠性的医疗人工智能的重要性。

Abstract: Despite the rapid expansion of Large Language Models (LLMs) in healthcare,
the ability of these systems to assess clinical trial reporting according to
CONSORT standards remains unclear, particularly with respect to their cognitive
and reasoning strategies. This study applies a behavioral and metacognitive
analytic approach with expert-validated data, systematically comparing two
representative LLMs under three prompt conditions. Clear differences emerged in
how the models approached various CONSORT items, and prompt types, including
shifts in reasoning style, explicit uncertainty, and alternative
interpretations shaped response patterns. Our results highlight the current
limitations of these systems in clinical compliance automation and underscore
the importance of understanding their cognitive adaptations and strategic
behavior in developing more explainable and reliable medical AI.

</details>


### [112] [The Zero-Step Thinking: An Empirical Study of Mode Selection as Harder Early Exit in Reasoning Models](https://arxiv.org/abs/2510.19176)
*Yuqiao Tan,Shizhu He,Kang Liu,Jun Zhao*

Main category: cs.AI

TL;DR: 这篇论文研究了如何减少推理模型中的过度思考，通过模式选择（Mode Selection）和提前退出（Early Exit）来降低计算负担。模式选择需要在推理开始时决定使用长链思考（Long-CoT）还是短链思考（Short-CoT），而提前退出则决定推理过程的最佳停止点。


<details>
  <summary>Details</summary>
Motivation: 推理模型在数学和逻辑推理等任务中表现出色，但常常过度思考，导致不必要的计算开销。为了解决这个问题，需要自动决定使用长链思考还是短链思考，并确定推理过程的最佳停止点。

Method: 该论文首先将模式选择定义为比提前退出更具挑战性的变体，因为它们的目标相似但决策时机不同。通过对九个基线模型的实证研究，观察到基于提示的方法由于其有限的分类能力而经常失败，而利用内部信息的方法通常在大多数情况下表现更好，但仍然存在稳定性问题。

Result: 研究发现，现有方法仅依赖模型提供的信息不足以有效解决信息有限情况下的模式选择问题，凸显了这项任务的持续挑战。

Conclusion: 现有的方法在信息有限的情况下，不足以有效解决模式选择问题，该任务仍然具有挑战性。

Abstract: Reasoning models have demonstrated exceptional performance in tasks such as
mathematics and logical reasoning, primarily due to their ability to engage in
step-by-step thinking during the reasoning process. However, this often leads
to overthinking, resulting in unnecessary computational overhead. To address
this issue, Mode Selection aims to automatically decide between Long-CoT
(Chain-of-Thought) or Short-CoT by utilizing either a Thinking or NoThinking
mode. Simultaneously, Early Exit determines the optimal stopping point during
the iterative reasoning process. Both methods seek to reduce the computational
burden. In this paper, we first identify Mode Selection as a more challenging
variant of the Early Exit problem, as they share similar objectives but differ
in decision timing. While Early Exit focuses on determining the best stopping
point for concise reasoning at inference time, Mode Selection must make this
decision at the beginning of the reasoning process, relying on pre-defined fake
thoughts without engaging in an explicit reasoning process, referred to as
zero-step thinking. Through empirical studies on nine baselines, we observe
that prompt-based approaches often fail due to their limited classification
capabilities when provided with minimal hand-crafted information. In contrast,
approaches that leverage internal information generally perform better across
most scenarios but still exhibit issues with stability. Our findings indicate
that existing methods relying solely on the information provided by models are
insufficient for effectively addressing Mode Selection in scenarios with
limited information, highlighting the ongoing challenges of this task. Our code
is available at https://github.com/Trae1ounG/Zero_Step_Thinking.

</details>


### [113] [WebGraphEval: Multi-Turn Trajectory Evaluation for Web Agents using Graph Representation](https://arxiv.org/abs/2510.19205)
*Yaoyao Qian,Yuanli Wang,Jinda Zhang,Yun Zong,Meixu Chen,Hanhan Zhou,Jindan Huang,Yifan Zeng,Xinyu Hu,Chan Hee Song,Danqing Zhang*

Main category: cs.AI

TL;DR: WebGraphEval框架通过将多个agent的轨迹抽象成一个统一的、加权的操作图，实现了对web agent的多路径、跨agent和效率感知的评估。


<details>
  <summary>Details</summary>
Motivation: 当前对web agent的评估主要采用二元成功指标或与单一参考轨迹的一致性，忽略了基准数据集中存在的结构多样性。

Method: WebGraphEval框架从多个agent中提取轨迹，并将它们抽象成一个统一的、加权的操作图。该框架规范地编码动作，合并重复行为，并应用结构分析，包括奖励传播和成功加权边缘统计。

Result: 对来自六个web agent的数千条轨迹的评估表明，该图抽象捕获了跨模型规律，突出了冗余和低效率，并识别了基于结果的指标所忽略的关键决策点。

Conclusion: WebGraphEval通过将Web交互构建为图结构化数据，为Web agent的多路径、跨agent和效率感知评估建立了一种通用方法。

Abstract: Current evaluation of web agents largely reduces to binary success metrics or
conformity to a single reference trajectory, ignoring the structural diversity
present in benchmark datasets. We present WebGraphEval, a framework that
abstracts trajectories from multiple agents into a unified, weighted action
graph. This representation is directly compatible with benchmarks such as
WebArena, leveraging leaderboard runs and newly collected trajectories without
modifying environments. The framework canonically encodes actions, merges
recurring behaviors, and applies structural analyses including reward
propagation and success-weighted edge statistics. Evaluations across thousands
of trajectories from six web agents show that the graph abstraction captures
cross-model regularities, highlights redundancy and inefficiency, and
identifies critical decision points overlooked by outcome-based metrics. By
framing web interaction as graph-structured data, WebGraphEval establishes a
general methodology for multi-path, cross-agent, and efficiency-aware
evaluation of web agents.

</details>


### [114] [ChatGPT Unveils Its Limits: Principles of Law Deliver Checkmate](https://arxiv.org/abs/2510.19261)
*Marianna Molinari,Ilaria Angela Amantea,Marinella Quaranta,Guido Governatori*

Main category: cs.AI

TL;DR: 本研究评估了ChatGPT在法律领域的表现，并揭示了其局限性。


<details>
  <summary>Details</summary>
Motivation: 在法律领域，从法律判决中提取关键信息至关重要。本研究旨在探究人工智能在此任务中的能力。

Method: 本研究通过实验，将ChatGPT的性能与使用正则表达式的基线方法进行比较。

Result: 研究表明，即使ChatGPT具备必要的知识和能力，也无法有效地整合它们，进行推理，从而得出详尽的结果。

Conclusion: 人工智能缺乏全方位的理解和推理能力，这使其在法律领域存在根本性的局限性。

Abstract: This study examines the performance of ChatGPT with an experiment in the
legal domain. We compare the outcome with it a baseline using regular
expressions (Regex), rather than focusing solely on the assessment against
human performance. The study reveals that even if ChatGPT has access to the
necessary knowledge and competencies, it is unable to assemble them, reason
through, in a way that leads to an exhaustive result. This unveils a major
limitation of ChatGPT. Intelligence encompasses the ability to break down
complex issues and address them according to multiple required competencies,
providing a unified and comprehensive solution. In the legal domain, one of the
most crucial tasks is reading legal decisions and extracting key passages
condensed from principles of law (PoLs), which are then incorporated into
subsequent rulings by judges or defense documents by lawyers. In performing
this task, artificial intelligence lacks an all-encompassing understanding and
reasoning, which makes it inherently limited. Genuine intelligence, remains a
uniquely human trait, at least in this particular field.

</details>


### [115] [An Argumentative Explanation Framework for Generalized Reason Model with Inconsistent Precedents](https://arxiv.org/abs/2510.19263)
*Wachara Fungwacharakorn,Gauvain Bourgne,Ken Satoh*

Main category: cs.AI

TL;DR: 本研究扩展了推导状态论证框架（DSA-framework），以解释根据广义的理由模型进行的推理。


<details>
  <summary>Details</summary>
Motivation: 现有的基于传统一致理由模型的先例推理存在论证解释方法，但对于容纳不一致先例的广义推理框架，尚未开发出相应的论证解释方法。

Method: 扩展了推导状态论证框架（DSA-framework）。

Result: 论文研究了推导状态论证框架的扩展。

Conclusion: 本研究旨在为广义的理由模型开发论证解释方法。

Abstract: Precedential constraint is one foundation of case-based reasoning in AI and
Law. It generally assumes that the underlying set of precedents must be
consistent. To relax this assumption, a generalized notion of the reason model
has been introduced. While several argumentative explanation approaches exist
for reasoning with precedents based on the traditional consistent reason model,
there has been no corresponding argumentative explanation method developed for
this generalized reasoning framework accommodating inconsistent precedents. To
address this question, this paper examines an extension of the derivation state
argumentation framework (DSA-framework) to explain the reasoning according to
the generalized notion of the reason model.

</details>


### [116] [Learning to Make Friends: Coaching LLM Agents toward Emergent Social Ties](https://arxiv.org/abs/2510.19299)
*Philipp J. Schneider,Lin Tian,Marian-Andrei Rizoiu*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）能否重现人类在线行为的复杂社会动态？


<details>
  <summary>Details</summary>
Motivation: 研究由同质性、互惠和社会验证塑造的人类在线行为，以及什么记忆和学习机制能够使这种动态出现。

Method: 提出了一个多智能体LLM模拟框架，其中智能体反复互动、相互评估，并通过上下文学习（由指导信号加速）来调整其行为。为了模拟人类社会行为，设计了行为奖励函数，捕捉在线参与的核心驱动因素，包括社会互动、信息寻求、自我展示、协调和情感支持。

Result: 实验表明，经过指导的LLM智能体形成了稳定的互动模式，并形成了涌现的社会关系，产生的网络结构反映了真实在线社区的属性。

Conclusion: 通过将行为奖励与上下文适应相结合，该框架建立了一个研究LLM群体中集体动态的原则性试验台，并揭示了人工智能体如何接近或偏离类人社会行为。

Abstract: Can large language model (LLM) agents reproduce the complex social dynamics
that characterize human online behavior -- shaped by homophily, reciprocity,
and social validation -- and what memory and learning mechanisms enable such
dynamics to emerge? We present a multi-agent LLM simulation framework in which
agents repeatedly interact, evaluate one another, and adapt their behavior
through in-context learning accelerated by a coaching signal. To model human
social behavior, we design behavioral reward functions that capture core
drivers of online engagement, including social interaction, information
seeking, self-presentation, coordination, and emotional support. These rewards
align agent objectives with empirically observed user motivations, enabling the
study of how network structures and group formations emerge from individual
decision-making. Our experiments show that coached LLM agents develop stable
interaction patterns and form emergent social ties, yielding network structures
that mirror properties of real online communities. By combining behavioral
rewards with in-context adaptation, our framework establishes a principled
testbed for investigating collective dynamics in LLM populations and reveals
how artificial agents may approximate or diverge from human-like social
behavior.

</details>


### [117] [Continual Knowledge Adaptation for Reinforcement Learning](https://arxiv.org/abs/2510.19314)
*Jinwu Hu,Zihao Lian,Zhiquan Wen,Chenghao Li,Guohao Chen,Xutao Wen,Bin Xiao,Mingkui Tan*

Main category: cs.AI

TL;DR: 提出了一种名为 CKA-RL 的持续强化学习方法，通过维护任务相关的知识向量池并动态使用历史知识来适应新任务，从而缓解灾难性遗忘并实现跨任务的有效知识转移。


<details>
  <summary>Details</summary>
Motivation: 现实世界的环境通常是非平稳的，需要智能体不断适应新的任务和变化的环境。现有的持续强化学习方法经常遭受灾难性遗忘和知识利用效率低下的问题。

Method: 提出了一种持续知识适应策略，包括维护一个特定于任务的知识向量池，并动态地使用历史知识来调整agent以适应新的任务。此外，还提出了一种自适应知识合并机制，该机制结合了相似的知识向量以应对可扩展性挑战，从而减少了内存需求，同时确保了基本知识的保留。

Result: 在三个基准测试中进行的实验表明，所提出的 CKA-RL 优于最先进的方法，在整体性能上提高了 4.20%，在前向迁移上提高了 8.02%。

Conclusion: CKA-RL 能够积累和有效利用历史知识，缓解灾难性遗忘，实现跨任务的有效知识转移。

Abstract: Reinforcement Learning enables agents to learn optimal behaviors through
interactions with environments. However, real-world environments are typically
non-stationary, requiring agents to continuously adapt to new tasks and
changing conditions. Although Continual Reinforcement Learning facilitates
learning across multiple tasks, existing methods often suffer from catastrophic
forgetting and inefficient knowledge utilization. To address these challenges,
we propose Continual Knowledge Adaptation for Reinforcement Learning (CKA-RL),
which enables the accumulation and effective utilization of historical
knowledge. Specifically, we introduce a Continual Knowledge Adaptation
strategy, which involves maintaining a task-specific knowledge vector pool and
dynamically using historical knowledge to adapt the agent to new tasks. This
process mitigates catastrophic forgetting and enables efficient knowledge
transfer across tasks by preserving and adapting critical model parameters.
Additionally, we propose an Adaptive Knowledge Merging mechanism that combines
similar knowledge vectors to address scalability challenges, reducing memory
requirements while ensuring the retention of essential knowledge. Experiments
on three benchmarks demonstrate that the proposed CKA-RL outperforms
state-of-the-art methods, achieving an improvement of 4.20% in overall
performance and 8.02% in forward transfer. The source code is available at
https://github.com/Fhujinwu/CKA-RL.

</details>


### [118] [MSC-Bench: A Rigorous Benchmark for Multi-Server Tool Orchestration](https://arxiv.org/abs/2510.19423)
*Jia-Kai Dong,I-Wei Huang,Chun-Tin Wu,Yi-Tien Tsai*

Main category: cs.AI

TL;DR: 介绍了MSC-Bench，一个用于评估LLM agent在分层模型-上下文协议(MCP)生态系统中进行多跳、端到端工具编排的大规模基准。


<details>
  <summary>Details</summary>
Motivation: 现有基准通常孤立地评估工具，忽略了功能重叠和跨服务器编排等挑战，导致过于乐观的评估。MSC-Bench旨在解决这些差距。

Method: 通过“等效功能集”构建ground truth，允许客观指标如F1分数，并减少对LLM-as-a-judge评估的依赖。组织为五个级别的课程，系统地测试代理从单工具编排到复杂跨服务器规划的能力，以及对范围外请求的鲁棒性。

Result: 实验表明，如果没有协同设计的策略，刚性层次结构可能会阻碍性能，即使是最先进的代理在鲁棒性方面也表现出系统性弱点。

Conclusion: MSC-Bench提供了一个诊断框架，以暴露这些限制并指导开发更强大和高效的工具使用代理。

Abstract: We introduce MSC-Bench, a large-scale benchmark for evaluating multi-hop,
end-to-end tool orchestration by LLM agents in a hierarchical Model-Context
Protocol (MCP) ecosystem. Existing benchmarks often evaluate tools in
isolation, ignoring challenges such as functional overlap and cross-server
orchestration, leading to overly optimistic assessments. MSC-Bench addresses
these gaps by constructing ground truth through 'equal function sets', allowing
objective metrics such as F1 score and reducing the dependency on
LLM-as-a-judge evaluation. Organized as a five-level curriculum, it
systematically tests agent capabilities from single-tool orchestration to
complex cross-server planning, and robustness to out-of-scope requests.
Experiments reveal that rigid hierarchies can hinder performance without
co-designed strategies, and even state-of-the-art agents exhibit systemic
weaknesses in robustness. MSC-Bench provides a diagnostic framework to expose
these limitations and guide the development of more capable and efficient
tool-using agents. The benchmark and resources are publicly available at
https://github.com/snooow1029/MSC_Bench.

</details>


### [119] [NeSyPr: Neurosymbolic Proceduralization For Efficient Embodied Reasoning](https://arxiv.org/abs/2510.19429)
*Wonje Choi,Jooyoung Kim,Honguk Woo*

Main category: cs.AI

TL;DR: 提出了一种新的具身推理框架 NeSyPr，通过神经符号程序化编译知识，使基于 LM 的代理具备结构化、自适应和及时的推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决在动态环境中采用语言模型 (LM) 进行具身任务的挑战，其中由于延迟、连接和资源限制，对大规模推理引擎或符号规划器的在线访问受到限制。

Method: NeSyPr 首先利用其声明性知识通过符号工具显式生成特定于任务的计划。然后，这些计划被转换为可组合的过程表示，这些表示编码计划的隐式生产规则，从而使生成的结果过程可以无缝地集成到 LM 的推理过程中。

Result: 在具身基准 PDDLGym、VirtualHome 和 ALFWorld 上的评估表明，NeSyPr 在大型推理模型和符号规划器上具有高效的推理能力，同时使用更紧凑的 LM。

Conclusion: 神经符号程序化将多步符号结构化路径查找和推理抽象和概括为单步 LM 推理，类似于人类知识编译。它支持高效的测试时推理，而无需依赖外部符号指导，使其非常适合部署在延迟敏感和资源受限的物理系统中。

Abstract: We address the challenge of adopting language models (LMs) for embodied tasks
in dynamic environments, where online access to large-scale inference engines
or symbolic planners is constrained due to latency, connectivity, and resource
limitations. To this end, we present NeSyPr, a novel embodied reasoning
framework that compiles knowledge via neurosymbolic proceduralization, thereby
equipping LM-based agents with structured, adaptive, and timely reasoning
capabilities. In NeSyPr, task-specific plans are first explicitly generated by
a symbolic tool leveraging its declarative knowledge. These plans are then
transformed into composable procedural representations that encode the plans'
implicit production rules, enabling the resulting composed procedures to be
seamlessly integrated into the LM's inference process. This neurosymbolic
proceduralization abstracts and generalizes multi-step symbolic structured
path-finding and reasoning into single-step LM inference, akin to human
knowledge compilation. It supports efficient test-time inference without
relying on external symbolic guidance, making it well suited for deployment in
latency-sensitive and resource-constrained physical systems. We evaluate NeSyPr
on the embodied benchmarks PDDLGym, VirtualHome, and ALFWorld, demonstrating
its efficient reasoning capabilities over large-scale reasoning models and a
symbolic planner, while using more compact LMs.

</details>


### [120] [DAIL: Beyond Task Ambiguity for Language-Conditioned Reinforcement Learning](https://arxiv.org/abs/2510.19562)
*Runpeng Xie,Quanwei Wang,Hao Hu,Zherui Zhou,Ni Mu,Xiyun Li,Yiqin Yang,Shuang Xu,Qianchuan Zhao,Bo XU*

Main category: cs.AI

TL;DR: 提出了一种名为 DAIL (Distributional Aligned Learning) 的新方法，以解决语言条件任务中的歧义性问题。


<details>
  <summary>Details</summary>
Motivation: 语言指令的灵活性导致了语言条件任务中存在大量歧义，严重降低了算法性能。

Method: 该方法包含两个关键部分：分布策略和语义对齐。分布策略增强了任务的可区分性，而语义对齐模块则捕获了轨迹和语言指令之间的对应关系。

Result: 在结构化和视觉观察基准上的大量实验结果表明，DAIL 有效地解决了指令歧义，实现了优于基线方法的性能。

Conclusion: DAIL 是一种有效的方法，可以解决语言条件任务中的歧义性问题，并提高算法性能。

Abstract: Comprehending natural language and following human instructions are critical
capabilities for intelligent agents. However, the flexibility of linguistic
instructions induces substantial ambiguity across language-conditioned tasks,
severely degrading algorithmic performance. To address these limitations, we
present a novel method named DAIL (Distributional Aligned Learning), featuring
two key components: distributional policy and semantic alignment. Specifically,
we provide theoretical results that the value distribution estimation mechanism
enhances task differentiability. Meanwhile, the semantic alignment module
captures the correspondence between trajectories and linguistic instructions.
Extensive experimental results on both structured and visual observation
benchmarks demonstrate that DAIL effectively resolves instruction ambiguities,
achieving superior performance to baseline methods. Our implementation is
available at https://github.com/RunpengXie/Distributional-Aligned-Learning.

</details>


### [121] [HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search Agents in Hierarchical Rule Application](https://arxiv.org/abs/2510.19631)
*Yiqian Yang,Tian Lan,Qianghuai Jia,Li Zhu,Hui Jiang,Hang Zhu,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.AI

TL;DR: 论文介绍了一个新的电商基准测试，用于评估深度搜索代理在分层规则应用中的能力。


<details>
  <summary>Details</summary>
Motivation: 当前代理基准测试忽略了应用复杂规则（如法律条款、医疗手册和关税规则）的能力，而这些规则对于有效的深度搜索代理至关重要。

Method: 论文提出了HSCodeComp，一个现实的、专家级的电商基准测试，用于评估深度搜索代理在分层规则应用中的能力。该任务要求代理根据产品描述预测10位数的协调系统代码（HSCode）。

Result: 实验结果表明，现有的大型语言模型和代理在HSCodeComp上的性能与人类专家相比存在巨大差距，最佳代理的10位数准确率仅为46.8%，而人类专家为95.0%。

Conclusion: 分层规则应用具有挑战性，并且测试时扩展无法进一步提高性能。

Abstract: Effective deep search agents must not only access open-domain and
domain-specific knowledge but also apply complex rules-such as legal clauses,
medical manuals and tariff rules. These rules often feature vague boundaries
and implicit logic relationships, making precise application challenging for
agents. However, this critical capability is largely overlooked by current
agent benchmarks.
  To fill this gap, we introduce HSCodeComp, the first realistic, expert-level
e-commerce benchmark designed to evaluate deep search agents in hierarchical
rule application. In this task, the deep reasoning process of agents is guided
by these rules to predict 10-digit Harmonized System Code (HSCode) of products
with noisy but realistic descriptions. These codes, established by the World
Customs Organization, are vital for global supply chain efficiency. Built from
real-world data collected from large-scale e-commerce platforms, our proposed
HSCodeComp comprises 632 product entries spanning diverse product categories,
with these HSCodes annotated by several human experts.
  Extensive experimental results on several state-of-the-art LLMs, open-source,
and closed-source agents reveal a huge performance gap: best agent achieves
only 46.8% 10-digit accuracy, far below human experts at 95.0%. Besides,
detailed analysis demonstrates the challenges of hierarchical rule application,
and test-time scaling fails to improve performance further.

</details>


### [122] [AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing](https://arxiv.org/abs/2510.19661)
*Xusen Guo,Mingxing Peng,Xixuan Hao,Xingchen Zou,Qiongyan Wang,Sijie Ruan,Yuxuan Liang*

Main category: cs.AI

TL;DR: AgentSense是一个混合的、无需训练的框架，它通过多智能体进化系统将大型语言模型（LLM）集成到参与式城市感知中。


<details>
  <summary>Details</summary>
Motivation: 现有的城市感知系统在不同的城市场景中泛化能力有限，且决策中的可解释性较差。

Method: AgentSense首先采用经典规划器生成基线解决方案，然后迭代优化这些方案，以使感知任务分配适应动态城市条件和异构工作者偏好，同时生成自然语言解释，以增强透明度和信任。

Result: 在两个大规模移动数据集和七种类型的动态干扰中进行的大量实验表明，与传统方法相比，AgentSense在适应性和可解释性方面具有明显的优势。此外，与单智能体LLM基线相比，我们的方法在性能和鲁棒性方面均优于传统方法，同时提供了更合理和透明的解释。

Conclusion: 这些结果表明，AgentSense是朝着在Web上部署自适应和可解释的城市感知系统迈出的重要一步。

Abstract: Web-based participatory urban sensing has emerged as a vital approach for
modern urban management by leveraging mobile individuals as distributed
sensors. However, existing urban sensing systems struggle with limited
generalization across diverse urban scenarios and poor interpretability in
decision-making. In this work, we introduce AgentSense, a hybrid, training-free
framework that integrates large language models (LLMs) into participatory urban
sensing through a multi-agent evolution system. AgentSense initially employs
classical planner to generate baseline solutions and then iteratively refines
them to adapt sensing task assignments to dynamic urban conditions and
heterogeneous worker preferences, while producing natural language explanations
that enhance transparency and trust. Extensive experiments across two
large-scale mobility datasets and seven types of dynamic disturbances
demonstrate that AgentSense offers distinct advantages in adaptivity and
explainability over traditional methods. Furthermore, compared to single-agent
LLM baselines, our approach outperforms in both performance and robustness,
while delivering more reasonable and transparent explanations. These results
position AgentSense as a significant advancement towards deploying adaptive and
explainable urban sensing systems on the web.

</details>


### [123] [A Graph Engine for Guitar Chord-Tone Soloing Education](https://arxiv.org/abs/2510.19666)
*Matthew Keating,Michael Casey*

Main category: cs.AI

TL;DR: 本研究提出了一种基于图的引擎，为吉他学生计算和弦音独奏建议。


<details>
  <summary>Details</summary>
Motivation: 和弦音独奏是和弦进行即兴演奏的基本练习，但难以学习和实践。

Method: 该方法构建一个加权图，其中每个节点代表和弦进行中和弦的和弦音琶音。然后，它根据最佳过渡音计算每个连续和弦节点之间的边缘权重。然后，找到通过该图的最短路径并重建和弦音独奏线。

Result: 该引擎为吉他学生提供了一个友好的系统来练习和弦音独奏。

Conclusion: 该研究为吉他学生提供了一种练习和弦音独奏的有效工具。

Abstract: We present a graph-based engine for computing chord tone soloing suggestions
for guitar students. Chord tone soloing is a fundamental practice for
improvising over a chord progression, where the instrumentalist uses only the
notes contained in the current chord. This practice is a building block for all
advanced jazz guitar theory but is difficult to learn and practice. First, we
discuss methods for generating chord-tone arpeggios. Next, we construct a
weighted graph where each node represents a chord tone arpeggio for a chord in
the progression. Then, we calculate the edge weight between each consecutive
chord's nodes in terms of optimal transition tones. We then find the shortest
path through this graph and reconstruct a chord-tone soloing line. Finally, we
discuss a user-friendly system to handle input and output to this engine for
guitar students to practice chord tone soloing.

</details>


### [124] [Explainable e-sports win prediction through Machine Learning classification in streaming](https://arxiv.org/abs/2510.19671)
*Silvia García-Méndez,Francisco de Arriba-Pérez*

Main category: cs.AI

TL;DR: 提出了一种可解释的流式赢家预测分类解决方案，准确率超过 90%，优于现有解决方案。


<details>
  <summary>Details</summary>
Motivation: 电子竞技观众和玩家数量不断增加，通信解决方案和云计算技术不断发展，促使在线游戏产业不断增长。传统上，基于人工智能的电子竞技分析解决方案被定义为从相关数据中提取有意义的模式并将其可视化以增强决策，但专业获胜预测中的大多数工作都集中在批量角度的分类方面，并且忽略了可视化技术。

Method: 在流式传输中，通过多个滑动窗口控制输入数据，以反映相关的游戏变化，从而实现可解释的获胜预测分类解决方案。

Result: 实验结果表明，准确率高于 90%，超过了文献中竞争解决方案的性能。

Conclusion: 该系统可被排名和推荐系统利用，以进行明智的决策，这归功于可解释性模块，该模块增强了对结果预测的信任。

Abstract: The increasing number of spectators and players in e-sports, along with the
development of optimized communication solutions and cloud computing
technology, has motivated the constant growth of the online game industry. Even
though Artificial Intelligence-based solutions for e-sports analytics are
traditionally defined as extracting meaningful patterns from related data and
visualizing them to enhance decision-making, most of the effort in professional
winning prediction has been focused on the classification aspect from a batch
perspective, also leaving aside the visualization techniques. Consequently,
this work contributes to an explainable win prediction classification solution
in streaming in which input data is controlled over several sliding windows to
reflect relevant game changes. Experimental results attained an accuracy higher
than 90 %, surpassing the performance of competing solutions in the literature.
Ultimately, our system can be leveraged by ranking and recommender systems for
informed decision-making, thanks to the explainability module, which fosters
trust in the outcome predictions.

</details>


### [125] [RLIE: Rule Generation with Logistic Regression, Iterative Refinement, and Evaluation for Large Language Models](https://arxiv.org/abs/2510.19698)
*Yang Yang,Hua XU,Zhangyi Hu,Yutao Yue*

Main category: cs.AI

TL;DR: 本文提出了一种名为RLIE的框架，该框架集成了LLM和概率建模，以学习一组加权规则。


<details>
  <summary>Details</summary>
Motivation: 许多基于LLM的方法忽略了规则之间的相互作用，并且将LLM与概率规则学习相结合以进行稳健推理的机会仍未被充分探索。

Method: RLIE包含四个阶段：(1) 规则生成，LLM提出和过滤候选规则；(2) Logistic回归，学习全局选择和校准的概率权重；(3) 迭代优化，使用预测误差更新规则集；(4) 评估，将加权规则集作为直接分类器与将规则注入LLM的方法进行比较。

Result: 直接应用具有学习权重的规则会产生优异的性能，而使用规则、权重和logistic模型输出提示LLM会降低准确性。

Conclusion: LLM擅长语义生成和解释，但在精确的概率整合方面不太可靠。RLIE阐明了LLM在归纳推理方面的潜力和局限性，并将它们与经典的概率规则组合方法相结合，以实现更可靠的神经符号推理。

Abstract: Large Language Models (LLMs) can propose rules in natural language,
sidestepping the need for a predefined predicate space in traditional rule
learning. Yet many LLM-based approaches ignore interactions among rules, and
the opportunity to couple LLMs with probabilistic rule learning for robust
inference remains underexplored. We present RLIE, a unified framework that
integrates LLMs with probabilistic modeling to learn a set of weighted rules.
RLIE has four stages: (1) Rule generation, where an LLM proposes and filters
candidates; (2) Logistic regression, which learns probabilistic weights for
global selection and calibration; (3) Iterative refinement, which updates the
rule set using prediction errors; and (4) Evaluation, which compares the
weighted rule set as a direct classifier with methods that inject rules into an
LLM. We evaluate multiple inference strategies on real-world datasets. Applying
rules directly with their learned weights yields superior performance, whereas
prompting LLMs with the rules, weights, and logistic-model outputs surprisingly
degrades accuracy. This supports the view that LLMs excel at semantic
generation and interpretation but are less reliable for precise probabilistic
integration. RLIE clarifies the potential and limitations of LLMs for inductive
reasoning and couples them with classic probabilistic rule combination methods
to enable more reliable neuro-symbolic reasoning.

</details>


### [126] [Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning](https://arxiv.org/abs/2510.19732)
*Gunshi Gupta,Karmesh Yadav,Zsolt Kira,Yarin Gal,Rahaf Aljundi*

Main category: cs.AI

TL;DR: 提出了一种名为Memo的基于Transformer的架构和训练方法，用于强化学习中记忆密集型、长时程任务。


<details>
  <summary>Details</summary>
Motivation: 使具身智能体能够在较长时间内有效运作，关键是开发能够形成和访问记忆的模型，以便在环境中保持上下文联系。当前的范式是训练基于Transformer的策略来完成具身顺序决策任务，但视觉输入经常超出Transformer的上下文限制，而人类可以保持和利用压缩为记忆的终身经验。

Method: Memo通过在训练期间将周期性总结tokens与模型的输入交错，从而结合了记忆的创建和检索。

Result: 在网格世界meta-RL基准测试和照片般逼真的室内环境中的多对象导航任务上，Memo优于naive长上下文Transformer基线，同时计算和存储效率更高。此外，Memo在推理时可以更好地推广到更长的上下文，并且在流媒体设置中保持稳健，在这种情况下，必须截断历史上下文以适应推理约束。

Conclusion: Memo架构在memory-intensive, long-horizon任务中表现出色， 具有更好的计算和存储效率， 并且能够更好地推广到更长的上下文。

Abstract: To enable embodied agents to operate effectively over extended timeframes, it
is crucial to develop models that form and access memories to stay
contextualized in their environment. In the current paradigm of training
transformer-based policies for embodied sequential decision-making tasks,
visual inputs often overwhelm the context limits of transformers, while humans
can maintain and utilize a lifetime of experience compressed as memories.
Significant compression is possible in principle, as much of the input is
irrelevant and can be abstracted. However, existing approaches predominantly
focus on either recurrent models with fixed-size memory or transformers with
full-context reliance. In this work, we propose Memo, a transformer-based
architecture and training recipe for reinforcement learning (RL) on
memory-intensive, long-horizon tasks. Memo incorporates the creation and
retrieval of memory by interleaving periodic summarization tokens with the
inputs of a model during training. We demonstrate Memo's effectiveness on a
gridworld meta-RL benchmark and a multi-object navigation task in
photo-realistic indoor settings. Memo outperforms naive long-context
transformer baselines while being more compute and storage efficient.
Additionally, Memo generalizes better to longer contexts at inference time and
remains robust in streaming settings, where historical context must be
truncated to fit inference constraints.

</details>


### [127] [Misalignment Bounty: Crowdsourcing AI Agent Misbehavior](https://arxiv.org/abs/2510.19738)
*Rustem Turtayev,Natalia Fedorova,Oleg Serikov,Sergey Koldyba,Lev Avagyan,Dmitrii Volkov*

Main category: cs.AI

TL;DR: Misalignment Bounty是一个众包项目，旨在收集AI系统追求非预期或不安全目标的案例。


<details>
  <summary>Details</summary>
Motivation: 收集清晰的、可复现的AI系统与人类意图不符的例子。

Method: 通过众包项目Misalignment Bounty收集案例。

Result: 收到了295份提交，其中9份获奖。

Conclusion: 本报告解释了该项目的动机和评估标准，并逐步分析了9份获奖作品。

Abstract: Advanced AI systems sometimes act in ways that differ from human intent. To
gather clear, reproducible examples, we ran the Misalignment Bounty: a
crowdsourced project that collected cases of agents pursuing unintended or
unsafe goals. The bounty received 295 submissions, of which nine were awarded.
  This report explains the program's motivation and evaluation criteria, and
walks through the nine winning submissions step by step.

</details>


### [128] [Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents](https://arxiv.org/abs/2510.19771)
*Gil Pasternak,Dheeraj Rajagopal,Julia White,Dhruv Atreja,Matthew Thomas,George Hurn-Maloney,Ash Lewis*

Main category: cs.AI

TL;DR: 论文提出了PROBE，一个用于评估LLM agent主动性的新基准。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估LLM agent跨来源和长时间范围的推理能力方面存在局限性。

Method: PROBE将主动性分解为三个核心能力：搜索未指定问题、识别特定瓶颈和执行适当的解决方案。

Result: 评估结果表明，即使是最先进的模型也难以解决PROBE基准测试。

Conclusion: 结果突出了agent系统中自主行动的当前局限性，并揭示了未来有希望的研究方向。

Abstract: LLM-based agents are increasingly moving towards proactivity: rather than
awaiting instruction, they exercise agency to anticipate user needs and solve
them autonomously. However, evaluating proactivity is challenging; current
benchmarks are constrained to localized context, limiting their ability to test
reasoning across sources and longer time horizons. To address this gap, we
present PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes
proactivity as a pipeline of three core capabilities: (1) searching for
unspecified issues, (2) identifying specific bottlenecks, and (3) executing
appropriate resolutions. We apply PROBE to evaluate leading LLMs and popular
agentic frameworks, showing that even state-of-the-art models struggle to solve
this benchmark. Computing our consistent measurements across frontier LLMs and
agents, we find that the best end-to-end performance of 40% is achieved by both
GPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative
capabilities of each model and analyze mutual failure modes. Our results
highlight the current limitations of autonomous action in agentic systems, and
expose promising future research directions.

</details>


### [129] [Benchmarking World-Model Learning](https://arxiv.org/abs/2510.19788)
*Archana Warrier,Dat Nyugen,Michelangelo Naim,Moksh Jain,Yichao Liang,Karen Schroeder,Cambridge Yang,Joshua B. Tenenbaum,Sebastian Vollmer,Kevin Ellis,Zenna Tavares*

Main category: cs.AI

TL;DR: 当前世界模型学习方法侧重于下一帧预测和奖励最大化，忽略了模型应支持多种下游任务和推断的目标。WorldTest协议通过分离无奖励交互和评分测试阶段来评估模型学习智能体。


<details>
  <summary>Details</summary>
Motivation: 当前世界模型的训练和评估方法与模型应支持多种下游任务和推断的目标不符。

Method: 提出了WorldTest协议，该协议分离了无奖励交互和在相关但不同的环境中的评分测试阶段。通过AutumnBench在43个交互式网格世界环境和129个任务上实例化WorldTest，任务包括掩码帧预测、规划和预测因果动态的变化。

Result: 在AutumnBench上比较了517名人类参与者和三个前沿模型，发现人类的表现优于模型，并且扩展计算能力仅在某些环境中提高了性能。

Conclusion: WorldTest提供了一个新颖的模板来评估智能体对环境动态的学习，而AutumnBench揭示了世界模型学习中存在的巨大提升空间。

Abstract: Model-learning agents should gather information to learn world models that
support many downstream tasks and inferences, such as predicting unobserved
states, estimating near- and far-term consequences of actions, planning action
sequences, and detecting changes in dynamics. Current methods for learning and
evaluating world models diverge from this goal: training and evaluation are
anchored to next-frame prediction, and success is scored by reward maximization
in the same environment. We propose WorldTest, a protocol to evaluate
model-learning agents that separates reward-free interaction from a scored test
phase in a different but related environment. WorldTest is
open-ended$\unicode{x2014}$models should support many different tasks unknown
ahead of time$\unicode{x2014}$and agnostic to model representation, allowing
comparison across approaches. We instantiated WorldTest with AutumnBench, a
suite of 43 interactive grid-world environments and 129 tasks across three
families: masked-frame prediction, planning, and predicting changes to the
causal dynamics. We compared 517 human participants and three frontier models
on AutumnBench. We found that humans outperform the models, and scaling compute
improves performance only in some environments but not others. WorldTest
provides a novel template$\unicode{x2014}$reward-free exploration, derived
tests, and behavior-based scoring$\unicode{x2014}$to evaluate what agents learn
about environment dynamics, and AutumnBench exposes significant headroom in
world-model learning.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [130] [FlexiDataGen: An Adaptive LLM Framework for Dynamic Semantic Dataset Generation in Sensitive Domains](https://arxiv.org/abs/2510.19025)
*Hamed Jelodar,Samita Bai,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.DB

TL;DR: FlexiDataGen是一个用于在敏感领域动态生成语义数据集的自适应大型语言模型（LLM）框架。


<details>
  <summary>Details</summary>
Motivation: 机器学习中，数据集的可用性和质量仍然是关键挑战，尤其是在数据稀缺、获取成本高昂或受隐私法规约束的领域。医疗保健、生物医学研究和网络安全等领域经常遇到高数据采集成本、有限的带注释数据访问以及关键事件的稀有性或敏感性。这些问题阻碍了在此类高风险领域中开发准确且可推广的机器学习模型。

Method: FlexiDataGen集成了四个核心组件：（1）句法-语义分析，（2）检索增强生成，（3）动态元素注入，以及（4）具有语义验证的迭代释义。这些组件共同确保生成高质量的、领域相关的数据。

Result: 实验结果表明，FlexiDataGen有效地缓解了数据短缺和注释瓶颈，从而实现了可扩展且准确的机器学习模型开发。

Conclusion: FlexiDataGen是一个自适应的LLM框架，旨在动态生成语义数据集，有效缓解数据短缺和注释瓶颈，从而实现可扩展且准确的机器学习模型开发。

Abstract: Dataset availability and quality remain critical challenges in machine
learning, especially in domains where data are scarce, expensive to acquire, or
constrained by privacy regulations. Fields such as healthcare, biomedical
research, and cybersecurity frequently encounter high data acquisition costs,
limited access to annotated data, and the rarity or sensitivity of key events.
These issues-collectively referred to as the dataset challenge-hinder the
development of accurate and generalizable machine learning models in such
high-stakes domains. To address this, we introduce FlexiDataGen, an adaptive
large language model (LLM) framework designed for dynamic semantic dataset
generation in sensitive domains. FlexiDataGen autonomously synthesizes rich,
semantically coherent, and linguistically diverse datasets tailored to
specialized fields. The framework integrates four core components: (1)
syntactic-semantic analysis, (2) retrieval-augmented generation, (3) dynamic
element injection, and (4) iterative paraphrasing with semantic validation.
Together, these components ensure the generation of high-quality,
domain-relevant data. Experimental results show that FlexiDataGen effectively
alleviates data shortages and annotation bottlenecks, enabling scalable and
accurate machine learning model development.

</details>


### [131] [Fine-Grained Dichotomies for Conjunctive Queries with Minimum or Maximum](https://arxiv.org/abs/2510.19197)
*Nofar Carmeli,Nikolaos Tziavelis*

Main category: cs.DB

TL;DR: 研究了按属性之间的最小值（或最大值）排序的合取查询（CQ）答案的细粒度复杂性直接访问问题，并将其应用于探索相关的任务。


<details>
  <summary>Details</summary>
Motivation: 研究合取查询答案的直接访问问题。

Method: 研究 ranked enumeration 在 min/max orders下的情况，以及 CQs 与 x <= min X 形式的谓词相关的问题。

Result: 对于每个任务，我们为无自连接的 CQs 建立了完整的二分法，精确地识别了在接近理想的时间内可解的情况。

Conclusion: 对于无自连接的 CQs，精确地识别了在接近理想的时间内可解的情况。

Abstract: We investigate the fine-grained complexity of direct access to Conjunctive
Query (CQ) answers according to their position, ordered by the minimum (or
maximum) value between attributes. We further use the tools we develop to
explore a wealth of related tasks. We consider the task of ranked enumeration
under min/max orders, as well as tasks concerning CQs with predicates of the
form x <= min X , where X is a set of variables and x is a single variable:
counting, enumeration, direct access, and predicate elimination (i.e.,
transforming the pair of query and database to an equivalent pair without
min-predicates). For each task, we establish a complete dichotomy for
self-join-free CQs, precisely identifying the cases that are solvable in
near-ideal time, i.e., (quasi)linear preprocessing time followed by constant or
logarithmic time per output.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [132] [SBAN: A Framework \& Multi-Dimensional Dataset for Large Language Model Pre-Training and Software Code Mining](https://arxiv.org/abs/2510.18936)
*Hamed Jelodar,Mohammad Meymani,Samita Bai,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.IR

TL;DR: SBAN 是一个用于软件代码分析的大型多维数据集，旨在推进大型语言模型 (LLM) 的预训练和评估。


<details>
  <summary>Details</summary>
Motivation: 为了推进用于软件代码分析的大型语言模型 (LLM) 的预训练和评估。

Method: SBAN 包含超过 300 万个样本，包括 290 万个良性样本和 672,000 个恶意软件，每个样本都以四种互补的层表示：二进制代码、汇编指令、自然语言描述和源代码。

Result: SBAN 支持跨表示学习、软件语义理解和自动恶意软件检测方面的研究。除了安全应用程序之外，SBAN 还支持更广泛的任务，例如代码翻译、代码解释和涉及异构数据的其他软件挖掘任务。

Conclusion: SBAN 为构建能够推理代码的智能系统提供了强大的基础，为挖掘软件行为、改进安全分析以及增强 LLM 在软件代码挖掘的预训练和微调任务中的能力开辟了新的机会。

Abstract: This paper introduces SBAN (Source code, Binary, Assembly, and Natural
Language Description), a large-scale, multi-dimensional dataset designed to
advance the pre-training and evaluation of large language models (LLMs) for
software code analysis. SBAN comprises more than 3 million samples, including
2.9 million benign and 672,000 malware respectively, each represented across
four complementary layers: binary code, assembly instructions, natural language
descriptions, and source code. This unique multimodal structure enables
research on cross-representation learning, semantic understanding of software,
and automated malware detection. Beyond security applications, SBAN supports
broader tasks such as code translation, code explanation, and other software
mining tasks involving heterogeneous data. It is particularly suited for
scalable training of deep models, including transformers and other LLM
architectures. By bridging low-level machine representations and high-level
human semantics, SBAN provides a robust foundation for building intelligent
systems that reason about code. We believe that this dataset opens new
opportunities for mining software behavior, improving security analytics, and
enhancing LLM capabilities in pre-training and fine-tuning tasks for software
code mining.

</details>


### [133] [XGen-Q: An Explainable Domain-Adaptive LLM Framework with Retrieval-Augmented Generation for Software Security](https://arxiv.org/abs/2510.19006)
*Hamed Jelodar,Mohammad Meymani,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.IR

TL;DR: 本研究提出了一种名为XGen-Q的领域自适应大型语言模型，用于恶意软件检测和分析。


<details>
  <summary>Details</summary>
Motivation: 现有的检测系统通常无法推广到混淆或以前未见过的威胁，因此需要更具适应性和可解释性的模型。

Method: XGen-Q基于Qwen-Coder架构，并在超过一百万个恶意软件样本的大规模语料库上进行了预训练。它使用多阶段提示策略与检索增强生成（RAG）相结合，以提供可靠的恶意软件识别和详细的取证报告。为了进一步提高泛化能力，我们设计了一个训练管道，该管道系统地将模型暴露于各种混淆模式。

Result: 实验结果表明，XGen-Q实现了比竞争基线明显更低的困惑度，并且在新恶意软件样本上表现出强大的性能。

Conclusion: 该研究表明，基于LLM的方法在可解释和强大的恶意软件分析方面具有前景。

Abstract: Generative AI and large language models (LLMs) have shown strong capabilities
in code understanding, but their use in cybersecurity, particularly for malware
detection and analysis, remains limited. Existing detection systems often fail
to generalize to obfuscated or previously unseen threats, underscoring the need
for more adaptable and explainable models. To address this challenge, we
introduce XGen-Q, a domain-adapted LLM built on the Qwen-Coder architecture and
pretrained on a large-scale corpus of over one million malware samples,
spanning both source and assembly code. XGen-Q uses a multi-stage prompt
strategy combined with retrieval-augmented generation (RAG) to deliver reliable
malware identification and detailed forensic reporting, even in the presence of
complex code obfuscation. To further enhance generalization, we design a
training pipeline that systematically exposes the model to diverse obfuscation
patterns. Experimental results show that XGen-Q achieves significantly lower
perplexity than competitive baselines and exhibits strong performance on novel
malware samples, demonstrating the promise of LLM-based approaches for
interpretable and robust malware analysis.

</details>


### [134] [C2T-ID: Converting Semantic Codebooks to Textual Document Identifiers for Generative Search](https://arxiv.org/abs/2510.19221)
*Yingchen Zhang,Ruqing Zhang,Jiafeng Guo,Wenjun Peng,Sen Li,Fuyu Lv,Xueqi Cheng*

Main category: cs.IR

TL;DR: C2T-ID通过层级聚类构建语义数字docid，提取高频元数据关键词并迭代替换数字标签，可选的两级语义平滑步骤增强流畅性，从而平衡语义表达和搜索空间约束。


<details>
  <summary>Details</summary>
Motivation: 在生成式检索中，设计携带丰富语义信息且保持易于处理的搜索空间的文档标识符（docid）是一个重要挑战。现有的codebook方法和纯文本docid方法分别存在不足。

Method: 1. 通过层级聚类构建语义数字docid；2. 提取高频元数据关键词，并迭代地用每个簇的top-K关键词替换其数字标签；3. 可选的两级语义平滑步骤，进一步增强C2T-ID的流畅性。

Result: 在Natural Questions和淘宝产品搜索上的实验表明，C2T-ID显著优于原子docid、语义codebook和纯文本docid基线。

Conclusion: C2T-ID在平衡语义表达性和搜索空间约束方面是有效的。

Abstract: Designing document identifiers (docids) that carry rich semantic information
while maintaining tractable search spaces is a important challenge in
generative retrieval (GR). Popular codebook methods address this by building a
hierarchical semantic tree and constraining generation to its child nodes, yet
their numeric identifiers cannot leverage the large language model's pretrained
natural language understanding. Conversely, using text as docid provides more
semantic expressivity but inflates the decoding space, making the system
brittle to early-step errors. To resolve this trade-off, we propose C2T-ID: (i)
first construct semantic numerical docid via hierarchical clustering; (ii) then
extract high-frequency metadata keywords and iteratively replace each numeric
label with its cluster's top-K keywords; and (iii) an optional two-level
semantic smoothing step further enhances the fluency of C2T-ID. Experiments on
Natural Questions and Taobao's product search demonstrate that C2T-ID
significantly outperforms atomic, semantic codebook, and pure-text docid
baselines, demonstrating its effectiveness in balancing semantic expressiveness
with search space constraints.

</details>


### [135] [CoRECT: A Framework for Evaluating Embedding Compression Techniques at Scale](https://arxiv.org/abs/2510.19340)
*L. Caspari,M. Dinzinger,K. Gosh Dastidar,C. Fellicious,J. Mitrović,M. Granitzer*

Main category: cs.IR

TL;DR: 本文提出了CoRECT，一个用于大规模评估嵌入压缩方法的框架，并提供了一个新的数据集集合。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略了语料库复杂性对密集检索性能的影响，而语料库大小和文档长度是关键因素。

Method: 本文构建了一个名为CoRECT的框架，并benchmark了八种具有代表性的压缩方法。

Result: 非学习压缩可以在高达1亿个段落上实现显著的索引大小减少，且性能损失在统计上不显著。最佳压缩方法的选择仍然具有挑战性，因为性能因模型而异。

Conclusion: 本文强调了CoRECT对于实现压缩方法的一致比较和知情选择的必要性。

Abstract: Dense retrieval systems have proven to be effective across various
benchmarks, but require substantial memory to store large search indices.
Recent advances in embedding compression show that index sizes can be greatly
reduced with minimal loss in ranking quality. However, existing studies often
overlook the role of corpus complexity -- a critical factor, as recent work
shows that both corpus size and document length strongly affect dense retrieval
performance. In this paper, we introduce CoRECT (Controlled Retrieval
Evaluation of Compression Techniques), a framework for large-scale evaluation
of embedding compression methods, supported by a newly curated dataset
collection. To demonstrate its utility, we benchmark eight representative types
of compression methods. Notably, we show that non-learned compression achieves
substantial index size reduction, even on up to 100M passages, with
statistically insignificant performance loss. However, selecting the optimal
compression method remains challenging, as performance varies across models.
Such variability highlights the necessity of CoRECT to enable consistent
comparison and informed selection of compression methods. All code, data, and
results are available on GitHub and HuggingFace.

</details>


### [136] [Top-P Masking for Cross Language Information Retrieval](https://arxiv.org/abs/2510.19758)
*Joseph Casale,Andrew Silverschotz,Joseph DeSimone*

Main category: cs.IR

TL;DR: 本文提出了一种 Top-P 动态掩码方法，类似于大型语言模型中的 Nucleus Sampling，并证明其性能优于 Top-K 掩码。


<details>
  <summary>Details</summary>
Motivation: 在信息检索 (IR) 任务中，Top-K 掩码方案已被提出作为一种促进稀疏表示的方法，作为 FLOPS 正则化的简单替代方案。双语词汇和文档扩展模型 (BLADE) 等算法采用这种方法作为后处理阶段。

Method: 我们建议使用类似于大型语言模型中 Nucleus Sampling 的 Top-P 动态掩码。

Result: 证明了 Top-P 动态掩码方法比 Top-K 掩码方法具有更好的性能。

Conclusion: 在跨语言信息检索 (CLIR) 领域评估了我们的方法，证明 Top-P 动态掩码方法更优。

Abstract: Top-K masking schemes have been proposed as a method to promote sparse
representations in Information Retrieval (IR) tasks, as a simple alternative to
Floating Point Operations per Second (FLOPS) regularization. Algorithms such as
Bilingual Lexical and Document Expansion Model (BLADE), adopt this approach as
a post-processing stage. We propose using Top-P Dynamic Masking similar to
Nucleus Sampling in Large Language Models, and demonstrate better performance
than Top-K masking. Specifically, we evaluate our methods in the domain of
Cross Language Information Retrieval (CLIR)

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [137] [3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and Latency](https://arxiv.org/abs/2510.18905)
*Minseok Jung,Abhas Ricky,Muhammad Rameez Chatni*

Main category: cs.LG

TL;DR: 提出了一个3D优化框架，用于联合校准AI推理的准确性、成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 现有AI推理扩展方法通常只考虑一维或二维的启发式权衡，无法满足成本和延迟约束。

Method: 使用蒙特卡洛模拟在三个代表性场景和九个大型语言模型上评估四种优化方法，解决3D多目标优化问题。

Result: 膝点优化实现了最佳平衡，而当精度优先时，最大化准确性仍然有利。

Conclusion: 该框架为各种操作环境中的部署感知推理扩展奠定了理论基础。

Abstract: AI inference scaling is often tuned through 1D heuristics (a fixed reasoning
passes) or 2D bivariate trade-offs (e.g., performance vs. compute), which fail
to consider cost and latency constraints. We introduce a 3D optimization
framework that jointly calibrates accuracy, cost, and latency within a unified
decision space, enabling constraints-aware inference scaling. Using Monte Carlo
simulations across three representative scenarios and nine simulated large
language models, we evaluate four optimization methods to address the 3D
multi-objective optimization (MOO) problem. Framing inference scaling in MOO
shapes a feasible space that 1D and 2D optimizations fail to capture, enabling
environmentadaptive selection of the inference scaling k. Results show that
knee-point optimization achieves the best balance, while accuracy-maximization
remains favorable when precision is prioritized. The framework establishes a
theoretical foundation for deployment-aware inference scaling across diverse
operational contexts.

</details>


### [138] [Large Connectome Model: An fMRI Foundation Model of Brain Connectomes Empowered by Brain-Environment Interaction in Multitask Learning Landscape](https://arxiv.org/abs/2510.18910)
*Ziquan Wei,Tingting Dan,Guorong Wu*

Main category: cs.LG

TL;DR: 本文提出了一种新的功能性神经影像基础模型，该模型通过多任务学习和半监督微调来优化模型性能，并在多种临床应用中取得了有希望的结果。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能模型在临床应用中的性能受到样本量限制，因此需要可靠的功能性神经影像基础模型。以往的自监督学习方法与大脑-结果关系不一定一致，导致模型并非最优。

Method: 该方法将大脑建模为多任务学习问题，并提出了一种可扩展的模型架构，用于多任务预训练（通过tokenizing多个脑-环境交互）和半监督微调（通过分配预训练BEI的伪标签）。

Result: 该模型在性别预测、人类行为识别以及自闭症、帕金森病、阿尔茨海默病和精神分裂症的疾病早期诊断等多种应用中进行了评估，结果显示出巨大的潜力。

Conclusion: 该研究表明，该基础模型有潜力促进当前神经影像学在临床实践中的应用。

Abstract: A reliable foundation model of functional neuroimages is critical to promote
clinical applications where the performance of current AI models is
significantly impeded by a limited sample size. To that end, tremendous efforts
have been made to pretraining large models on extensive unlabeled fMRI data
using scalable self-supervised learning. Since self-supervision is not
necessarily aligned with the brain-to-outcome relationship, most foundation
models are suboptimal to the downstream task, such as predicting disease
outcomes. By capitalizing on rich environmental variables and demographic data
along with an unprecedented amount of functional neuroimages, we form the brain
modeling as a multitask learning and present a scalable model architecture for
(i) multitask pretraining by tokenizing multiple brain-environment interactions
(BEI) and (ii) semi-supervised finetuning by assigning pseudo-labels of
pretrained BEI. We have evaluated our foundation model on a variety of
applications, including sex prediction, human behavior recognition, and disease
early diagnosis of Autism, Parkinson's disease, Alzheimer's disease, and
{Schizophrenia}, where promising results indicate the great potential to
facilitate current neuroimaging applications in clinical routines.

</details>


### [139] [ADPO: Anchored Direct Preference Optimization](https://arxiv.org/abs/2510.18913)
*Wang Zixian*

Main category: cs.LG

TL;DR: ADPO is a framework that generalizes DPO with soft preferences, reference-policy anchoring, and groupwise extensions.


<details>
  <summary>Details</summary>
Motivation: Standard DPO assumes hard binary labels and pairwise comparisons, which is limiting. ADPO introduces soft preference probabilities, reference-policy anchors, and listwise preference modeling to address these limitations.

Method: The paper introduces ADPO, a unified framework that generalizes DPO with soft preferences, reference-policy anchoring, and groupwise extensions. It also presents three practical variants: pairwise anchored Soft-DPO, listwise anchored Soft-DPO with raw rewards, and KDE-based listwise smoothing.

Result: Anchoring improves WinMass by 38-63% over standard DPO in contextual bandits, while KDE smoothing achieves 0.68 vs 0.32 under heavy-tailed contamination. In sequential reinforcement learning, anchoring improves noisy-preference performance by 15-29%.

Conclusion: The paper provides guidance on which ADPO variant to use based on the level of noise in the data. Pairwise anchored Soft-DPO is recommended for clean or moderate noise, and KDE-based listwise ADPO for extreme contamination.

Abstract: Anchored Direct Preference Optimization (ADPO) is a unified framework that
generalizes Direct Preference Optimization (DPO) with soft preferences,
reference-policy anchoring, and groupwise extensions. While standard DPO
assumes hard binary labels and pairwise comparisons, ADPO introduces: (i) soft
preference probabilities that encode uncertainty and mitigate gradient drift;
(ii) arbitrary reference-policy anchors that stabilize training via groupwise
shift invariance and implicit KL regularization; and (iii) listwise preference
modeling through Plackett-Luce distributions. We prove that DPO, Bradley-Terry
objectives, and Top-1-vs-Rest formulations emerge as special cases. ADPO yields
three practical variants: pairwise anchored Soft-DPO, listwise anchored
Soft-DPO with raw rewards, and KDE-based listwise smoothing for heavy-tailed
noise. In contextual bandits, anchoring improves WinMass by 38-63% over
standard DPO, while KDE smoothing achieves 0.68 vs 0.32 under heavy-tailed
contamination (112% relative gain). In sequential reinforcement learning
(CartPole, LunarLander), anchoring improves noisy-preference performance by
15-29%, confirming transfer from single-step to multi-step settings.
Experiments with 10-256 parameter models provide clear guidance: use pairwise
anchored Soft-DPO for clean or moderate noise, and KDE-based listwise ADPO for
extreme contamination.

</details>


### [140] [An Encode-then-Decompose Approach to Unsupervised Time Series Anomaly Detection on Contaminated Training Data--Extended Version](https://arxiv.org/abs/2510.18998)
*Buang Zhang,Tung Kieu,Xiangfei Qiu,Chenjuan Guo,Jilin Hu,Aoying Zhou,Christian S. Jensen,Bin Yang*

Main category: cs.LG

TL;DR: 本文提出了一种新的时间序列异常检测方法，该方法通过分解编码表示来提高鲁棒性，并使用互信息度量来识别异常。


<details>
  <summary>Details</summary>
Motivation: 无监督时间序列异常检测因其无需异常标签而受到广泛关注，但自编码器对训练时间序列中的异常敏感，导致准确性降低。

Method: 提出了一种新的编码-分解范例，将编码表示分解为稳定和辅助表示，并提出了一种基于互信息的新度量来替代重建误差。

Result: 在八个常用的多变量和单变量时间序列基准测试中表现出有竞争力的或最先进的性能，并且对具有不同污染率的时间序列表现出鲁棒性。

Conclusion: 提出的方法在时间序列异常检测方面具有优势。

Abstract: Time series anomaly detection is important in modern large-scale systems and
is applied in a variety of domains to analyze and monitor the operation of
diverse systems. Unsupervised approaches have received widespread interest, as
they do not require anomaly labels during training, thus avoiding potentially
high costs and having wider applications. Among these, autoencoders have
received extensive attention. They use reconstruction errors from compressed
representations to define anomaly scores. However, representations learned by
autoencoders are sensitive to anomalies in training time series, causing
reduced accuracy. We propose a novel encode-then-decompose paradigm, where we
decompose the encoded representation into stable and auxiliary representations,
thereby enhancing the robustness when training with contaminated time series.
In addition, we propose a novel mutual information based metric to replace the
reconstruction errors for identifying anomalies. Our proposal demonstrates
competitive or state-of-the-art performance on eight commonly used multi- and
univariate time series benchmarks and exhibits robustness to time series with
different contamination ratios.

</details>


### [141] [Benchmarking On-Device Machine Learning on Apple Silicon with MLX](https://arxiv.org/abs/2510.18921)
*Oluwaseun A. Ajayi,Ogundepo Odunayo*

Main category: cs.LG

TL;DR: MLX is a framework optimized for ML computations on Apple silicon, enabling easier research and prototyping.


<details>
  <summary>Details</summary>
Motivation: The need for frameworks that can take advantage of on-device hardware (like Apple silicon) for deploying LLMs on smaller devices.

Method: Performance evaluation of MLX focusing on inference latency of transformer models. Comparing transformer implementations in MLX with Pytorch, using MLX-transformers framework. Benchmarking on Apple Silicon macbooks against an NVIDIA CUDA GPU.

Result: Inference latency performance comparison of BERT, RoBERTa, and XLM-RoBERTa models.

Conclusion: MLX shows potential in enabling efficient and accessible on-device ML applications within Apple's ecosystem.

Abstract: The recent widespread adoption of Large Language Models (LLMs) and machine
learning in general has sparked research interest in exploring the
possibilities of deploying these models on smaller devices such as laptops and
mobile phones. This creates a need for frameworks and approaches that are
capable of taking advantage of on-device hardware. The MLX framework was
created to address this need. It is a framework optimized for machine learning
(ML) computations on Apple silicon devices, facilitating easier research,
experimentation, and prototyping.
  This paper presents a performance evaluation of MLX, focusing on inference
latency of transformer models. We compare the performance of different
transformer architecture implementations in MLX with their Pytorch
counterparts. For this research we create a framework called MLX-transformers
which includes different transformer implementations in MLX and downloads the
model checkpoints in pytorch and converts it to the MLX format. By leveraging
the advanced architecture and capabilities of Apple Silicon, MLX-Transformers
enables seamless execution of transformer models directly sourced from Hugging
Face, eliminating the need for checkpoint conversion often required when
porting models between frameworks.
  Our study benchmarks different transformer models on two Apple Silicon
macbook devices against an NVIDIA CUDA GPU. Specifically, we compare the
inference latency performance of models with the same parameter sizes and
checkpoints. We evaluate the performance of BERT, RoBERTa, and XLM-RoBERTa
models, with the intention of extending future work to include models of
different modalities, thus providing a more comprehensive assessment of MLX's
capabilities. The results highlight MLX's potential in enabling efficient and
more accessible on-device ML applications within Apple's ecosystem.

</details>


### [142] [Noise-corrected GRPO: From Noisy Rewards to Unbiased Gradients](https://arxiv.org/abs/2510.18924)
*Omar El mansouri,Mohamed El Amine Seddik,Salem Lahlou*

Main category: cs.LG

TL;DR: 本研究针对现有大型语言模型通过人类反馈强化学习（RLHF）或可验证奖励（RLVR）进行对齐时，对奖励噪声高度敏感的问题，提出了噪声鲁棒的Group Relative Policy Optimization (GRPO) 框架。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF或RLVR方法容易受到不一致或错误的奖励噪声的影响，但这种噪声与广泛使用的基于群组的策略优化方法之间的相互作用尚未被充分探索。

Method: 提出了GRPO和Done Right GRPO (Dr.GRPO)框架，该框架将奖励损坏显式地建模为伯努利噪声，并在估计奖励翻转概率后应用噪声校正来消除学习信号的偏差，从而产生可证明的无偏梯度估计。

Result: 在数学和代码任务中应用噪声校正后，观察到了一致的改进，在实际奖励模型条件下，数学任务的准确率提高了6.7个百分点，代码任务的准确率提高了1.5个百分点。

Conclusion: 该研究将监督学习中的标签噪声校正与现代RLHF联系起来，为噪声 реальных 部署提供了理论见解和实用算法。

Abstract: Reinforcement learning from human feedback (RLHF) or verifiable rewards
(RLVR), the standard paradigm for aligning LLMs or building recent SOTA
reasoning models, is highly sensitive to noise from inconsistent or erroneous
rewards. Yet, the interaction between such noise and widely used group-based
policy optimization methods remains underexplored. We introduce a noise-robust
Group Relative Policy Optimization (GRPO) and Done Right GRPO (Dr.GRPO)
framework that explicitly models reward corruption as Bernoulli noise. Our
method applies noise correction after estimating reward flip probabilities to
debias the learning signal, yielding provably unbiased gradient estimates.
Theoretical analysis shows that group-based methods inherently mitigate
individual-level noise, and our correction strategy amplifies this robustness.
Empirically, we observe consistent improvements across math and code tasks when
applying our noise correction to standard reward model usage, with particular
gains of up to 6.7 percentage points in accuracy on math tasks and 1.5 on code
tasks under realistic reward model conditions. This work bridges label-noise
correction from supervised learning with modern RLHF, offering both theoretical
insights and a practical algorithm for noisy real-world deployment.

</details>


### [143] [Application of Reduced-Order Models for Temporal Multiscale Representations in the Prediction of Dynamical Systems](https://arxiv.org/abs/2510.18925)
*Elias Al Ghazal,Jad Mounayer,Beatriz Moya,Sebastian Rodriguez,Chady Ghnatios,Francisco Chinesta*

Main category: cs.LG

TL;DR: 本文提出了三种多尺度学习方法，以应对复杂多尺度系统建模和预测中的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法难以捕捉高频行为，对复杂多尺度系统的非线性和对初始条件的敏感性构成挑战。

Method: 1. 结合神经⽹络的统⼀分解 (PU) ⽅法；2. 奇异值分解 (SVD) 提取主导模式；3. 稀疏⾼阶 SVD 从有限测量中重建多尺度动态。

Result: 这些⽅法能够准确捕捉粗细粒度的动态。

Conclusion: 该框架适用于涉及复杂多尺度现象的实际应⽤，并能适应具有不完整观测的⾼维系统。

Abstract: Modeling and predicting the dynamics of complex multiscale systems remains a
significant challenge due to their inherent nonlinearities and sensitivity to
initial conditions, as well as limitations of traditional machine learning
methods that fail to capture high frequency behaviours. To overcome these
difficulties, we propose three approaches for multiscale learning. The first
leverages the Partition of Unity (PU) method, integrated with neural networks,
to decompose the dynamics into local components and directly predict both
macro- and micro-scale behaviors. The second applies the Singular Value
Decomposition (SVD) to extract dominant modes that explicitly separate macro-
and micro-scale dynamics. Since full access to the data matrix is rarely
available in practice, we further employ a Sparse High-Order SVD to reconstruct
multiscale dynamics from limited measurements. Together, these approaches
ensure that both coarse and fine dynamics are accurately captured, making the
framework effective for real-world applications involving complex, multi-scale
phenomena and adaptable to higher-dimensional systems with incomplete
observations, by providing an approximation and interpretation in all time
scales present in the phenomena under study.

</details>


### [144] [BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping](https://arxiv.org/abs/2510.18927)
*Zhiheng Xi,Xin Guo,Yang Nan,Enyu Zhou,Junrui Shen,Wenxiang Chen,Jiaqi Liu,Jixuan Huang,Zhihao Zhang,Honglin Guo,Xun Deng,Zhikai Lei,Miao Zheng,Guoteng Wang,Shuo Zhang,Peng Sun,Rui Zheng,Hang Yan,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.LG

TL;DR: 本研究针对在离线强化学习中，使用旧策略数据训练大型语言模型时，策略熵下降、优化不稳定甚至崩溃的问题。通过理论和实证分析，提出了BAPO方法，该方法动态调整裁剪边界以平衡正负贡献，保持熵，并稳定RL优化。实验结果表明，BAPO在各种离线场景中实现了快速、稳定和数据高效的训练，并在AIME 2024和AIME 2025基准测试中超越了其他模型。


<details>
  <summary>Details</summary>
Motivation: 在离线强化学习中，使用旧策略数据训练大型语言模型可以提高样本效率，但存在策略熵下降、优化不稳定甚至崩溃的问题。

Method: 提出了BAlanced Policy Optimization with Adaptive Clipping (BAPO)方法，该方法动态调整裁剪边界以自适应地重新平衡正负贡献，保持熵，并稳定RL优化。

Result: 在AIME 2024和AIME 2025基准测试中，7B BAPO模型超越了SkyWork-OR1-7B等开源模型，而32B BAPO模型不仅在相同规模的模型中取得了最先进的结果，而且优于o3-mini和Gemini-2.5-Flash-Thinking等领先的专有系统。

Conclusion: BAPO方法能够有效解决离线强化学习中策略熵下降、优化不稳定等问题，并在多个基准测试中取得了优异的性能。

Abstract: Reinforcement learning (RL) has recently become the core paradigm for
aligning and strengthening large language models (LLMs). Yet, applying RL in
off-policy settings--where stale data from past policies are used for
training--improves sample efficiency, but remains challenging: policy entropy
declines sharply, optimization often becomes unstable and may even collapse.
Through theoretical and empirical analysis, we identify two key insights: (i)
an imbalance in optimization, where negative-advantage samples dominate the
policy gradient, suppressing useful behaviors and risking gradient explosions;
and (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping
mechanism in PPO-like objectives systematically blocks entropy-increasing
updates, thereby driving the policy toward over-exploitation at the expense of
exploration. Building on these insights, we propose BAlanced Policy
Optimization with Adaptive Clipping (BAPO), a simple yet effective method that
dynamically adjusts clipping bounds to adaptively re-balance positive and
negative contributions, preserve entropy, and stabilize RL optimization. Across
diverse off-policy scenarios--including sample replay and partial rollout--BAPO
achieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025
benchmarks, our 7B BAPO model surpasses open-source counterparts such as
SkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art
results among models of the same scale but also outperforms leading proprietary
systems like o3-mini and Gemini-2.5-Flash-Thinking.

</details>


### [145] [Position: Many generalization measures for deep learning are fragile](https://arxiv.org/abs/2510.18934)
*Shuofeng Zhang,Ard Louis*

Main category: cs.LG

TL;DR: 这篇论文认为，许多在训练后的深度神经网络上计算的泛化性指标是脆弱的，即使对训练过程进行微小的修改，也可能显著改变这些指标的值、趋势或缩放行为。


<details>
  <summary>Details</summary>
Motivation: 研究动机是现有的泛化性指标可能无法稳定地反映深度神经网络的泛化能力。

Method: 通过实验展示了学习率调整或SGD变体等微小的超参数变化，以及数据复杂度的变化，如何影响各种泛化性指标。

Result: 论文表明，路径范数、谱范数、Frobenius范数、平坦度代理以及确定性PAC-Bayes替代等多种泛化性指标都存在脆弱性问题。PAC-Bayes origin measure对超参数调整的敏感度较低，但无法捕捉数据复杂性的差异。而基于函数的边缘似然PAC-Bayes bound虽然可以捕捉数据复杂性的差异，但不是一个后验指标。

Conclusion: 论文主张，新的泛化性指标的开发者应该明确地对其脆弱性进行评估。

Abstract: A wide variety of generalization measures have been applied to deep neural
networks (DNNs). Although obtaining tight bounds remains challenging, such
measures are often assumed to reproduce qualitative generalization trends. In
this position paper, we argue that many post-mortem generalization measures --
those computed on trained networks -- are \textbf{fragile}: small training
modifications that barely affect the underlying DNN can substantially change a
measure's value, trend, or scaling behavior. For example, minor hyperparameter
changes, such as learning rate adjustments or switching between SGD variants
can reverse the slope of a learning curve in widely used generalization
measures like the path norm. We also identify subtler forms of fragility. For
instance, the PAC-Bayes origin measure is regarded as one of the most reliable,
and is indeed less sensitive to hyperparameter tweaks than many other measures.
However, it completely fails to capture differences in data complexity across
learning curves. This data fragility contrasts with the function-based
marginal-likelihood PAC-Bayes bound, which does capture differences in
data-complexity, including scaling behavior, in learning curves, but which is
not a post-mortem measure. Beyond demonstrating that many bounds -- such as
path, spectral and Frobenius norms, flatness proxies, and deterministic
PAC-Bayes surrogates -- are fragile, this position paper also argues that
developers of new measures should explicitly audit them for fragility.

</details>


### [146] [NeuroAda: Activating Each Neuron's Potential for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2510.18940)
*Zhi Zhang,Yixian Shen,Congfeng Cao,Ekaterina Shutova*

Main category: cs.LG

TL;DR: NeuroAda是一种新的参数高效微调（PEFT）方法，它通过引入旁路连接来更新重要参数，从而在保持高内存效率的同时实现细粒度的模型微调。


<details>
  <summary>Details</summary>
Motivation: 现有的PEFT方法在内存效率和表示能力之间存在权衡。加法方法（如LoRA）内存效率高但表示能力有限，而选择性原位自适应方法可以更精确但内存消耗大。

Method: NeuroAda首先识别重要参数，然后为这些参数引入旁路连接。在微调期间，只更新旁路连接，而原始模型参数保持冻结。

Result: 在23+个跨越自然语言生成和理解的任务上的经验结果表明，NeuroAda以少至≤0.02%的可训练参数实现了最先进的性能，同时减少了高达60%的CUDA内存使用量。

Conclusion: NeuroAda在内存效率和微调精度之间取得了平衡，实现了最先进的性能。

Abstract: Existing parameter-efficient fine-tuning (PEFT) methods primarily fall into
two categories: addition-based and selective in-situ adaptation. The former,
such as LoRA, introduce additional modules to adapt the model to downstream
tasks, offering strong memory efficiency. However, their representational
capacity is often limited, making them less suitable for fine-grained
adaptation. In contrast, the latter directly fine-tunes a carefully chosen
subset of the original model parameters, allowing for more precise and
effective adaptation, but at the cost of significantly increased memory
consumption. To reconcile this trade-off, we propose NeuroAda, a novel PEFT
method that enables fine-grained model finetuning while maintaining high memory
efficiency. Our approach first identifies important parameters (i.e.,
connections within the network) as in selective adaptation, and then introduces
bypass connections for these selected parameters. During finetuning, only the
bypass connections are updated, leaving the original model parameters frozen.
Empirical results on 23+ tasks spanning both natural language generation and
understanding demonstrate that NeuroAda achieves state-of-the-art performance
with as little as $\leq \textbf{0.02}\%$ trainable parameters, while reducing
CUDA memory usage by up to 60%. We release our code here:
https://github.com/FightingFighting/NeuroAda.git.

</details>


### [147] [Towards Universal Solvers: Using PGD Attack in Active Learning to Increase Generalizability of Neural Operators as Knowledge Distillation from Numerical PDE Solvers](https://arxiv.org/abs/2510.18989)
*Yifei Sun*

Main category: cs.LG

TL;DR: 提出了一种对抗性师生蒸馏框架，以提高神经算子的OOD泛化能力，同时保持其低参数成本和快速推理的优点。


<details>
  <summary>Details</summary>
Motivation: 非线性PDE求解器需要精细的时空离散化和局部线性化，导致高内存成本和慢运行时。神经算子虽然推理速度快，但OOD泛化能力差，容易在训练分布之外的输入上失效。

Method: 利用可微数值求解器监督一个紧凑的神经算子，同时使用PGD风格的主动采样循环搜索最坏情况的输入，并在平滑性和能量约束下扩展训练集。使用可微谱求解器可以实现基于梯度的对抗性搜索，并稳定样本挖掘。

Result: 在Burgers和Navier-Stokes系统上的实验表明，对抗性蒸馏显著提高了OOD鲁棒性，同时保持了神经算子的低参数成本和快速推理。

Conclusion: 对抗性师生蒸馏框架能够有效提高神经算子在解决非线性PDE问题时的OOD泛化能力，同时保持其计算效率。

Abstract: Nonlinear PDE solvers require fine space-time discretizations and local
linearizations, leading to high memory cost and slow runtimes. Neural operators
such as FNOs and DeepONets offer fast single-shot inference by learning
function-to-function mappings and truncating high-frequency components, but
they suffer from poor out-of-distribution (OOD) generalization, often failing
on inputs outside the training distribution. We propose an adversarial
teacher-student distillation framework in which a differentiable numerical
solver supervises a compact neural operator while a PGD-style active sampling
loop searches for worst-case inputs under smoothness and energy constraints to
expand the training set. Using differentiable spectral solvers enables
gradient-based adversarial search and stabilizes sample mining. Experiments on
Burgers and Navier-Stokes systems demonstrate that adversarial distillation
substantially improves OOD robustness while preserving the low parameter cost
and fast inference of neural operators.

</details>


### [148] [Prior-informed optimization of treatment recommendation via bandit algorithms trained on large language model-processed historical records](https://arxiv.org/abs/2510.19014)
*Saman Nessari,Ali Bozorgi-Amiri*

Main category: cs.LG

TL;DR: 本文提出了一种综合系统，该系统集成了大型语言模型（LLM）、条件表格生成对抗网络（CTGAN）、T-learner反事实模型和上下文bandit方法，以提供定制的、数据驱动的临床建议。


<details>
  <summary>Details</summary>
Motivation: 目前的医疗实践依赖于标准化的治疗框架和经验方法，忽略了个体患者的差异，导致次优的健康结果。本文旨在解决这一问题，通过整合多种技术，为患者提供个性化的治疗建议。

Method: 该方法利用LLM将非结构化的医疗叙述处理成结构化数据集，使用CTGAN生成逼真的合成患者数据，部署T-learners预测患者特定的治疗反应，并整合先验知情的上下文bandit来增强在线治疗选择。

Result: 在III期结肠癌数据集上的测试表明，KernelUCB方法在5000轮中获得了0.60-0.61的平均奖励分数，超过了其他参考方法。

Conclusion: 该综合系统克服了在线学习环境中的冷启动限制，提高了计算效率，并构成了在适应特定患者特征的个体化医疗方面取得的显著进展。

Abstract: Current medical practice depends on standardized treatment frameworks and
empirical methodologies that neglect individual patient variations, leading to
suboptimal health outcomes. We develop a comprehensive system integrating Large
Language Models (LLMs), Conditional Tabular Generative Adversarial Networks
(CTGAN), T-learner counterfactual models, and contextual bandit approaches to
provide customized, data-informed clinical recommendations. The approach
utilizes LLMs to process unstructured medical narratives into structured
datasets (93.2% accuracy), uses CTGANs to produce realistic synthetic patient
data (55% accuracy via two-sample verification), deploys T-learners to forecast
patient-specific treatment responses (84.3% accuracy), and integrates
prior-informed contextual bandits to enhance online therapeutic selection by
effectively balancing exploration of new possibilities with exploitation of
existing knowledge. Testing on stage III colon cancer datasets revealed that
our KernelUCB approach obtained 0.60-0.61 average reward scores across 5,000
rounds, exceeding other reference methods. This comprehensive system overcomes
cold-start limitations in online learning environments, improves computational
effectiveness, and constitutes notable progress toward individualized medicine
adapted to specific patient characteristics.

</details>


### [149] [Category learning in deep neural networks: Information content and geometry of internal representations](https://arxiv.org/abs/2510.19021)
*Laurent Bonnasse-Gahot,Jean-Pierre Nadal*

Main category: cs.LG

TL;DR: 该论文研究了人工神经网络中的类别感知现象，发现类别学习会增强决策边界附近的神经空间扩张。


<details>
  <summary>Details</summary>
Motivation: 之前的研究表明，有效学习是神经空间扩张/压缩的必要结果。该论文将理论框架扩展到人工神经网络。

Method: 通过最小化贝叶斯成本（交叉熵损失的平均值）来最大化类别集合与决策层之前的神经活动之间的互信息。使用具有小维度潜在特征空间的结构化数据，表明最大化互信息意味着找到合适的投影空间并构建具有适当度量的神经表示。

Result: 数值结果表明，学习后，两个费舍尔信息矩阵匹配，并且基本上与类别边界对齐。此外，论文还将该方法与信息瓶颈方法联系起来，并展示了贝叶斯成本的偏差-方差分解。

Conclusion: 类别学习会引起决策边界附近的神经空间扩张。

Abstract: In animals, category learning enhances discrimination between stimuli close
to the category boundary. This phenomenon, called categorical perception, was
also empirically observed in artificial neural networks trained on
classification tasks. In previous modeling works based on neuroscience data, we
show that this expansion/compression is a necessary outcome of efficient
learning. Here we extend our theoretical framework to artificial networks. We
show that minimizing the Bayes cost (mean of the cross-entropy loss) implies
maximizing the mutual information between the set of categories and the neural
activities prior to the decision layer. Considering structured data with an
underlying feature space of small dimension, we show that maximizing the mutual
information implies (i) finding an appropriate projection space, and, (ii)
building a neural representation with the appropriate metric. The latter is
based on a Fisher information matrix measuring the sensitivity of the neural
activity to changes in the projection space. Optimal learning makes this neural
Fisher information follow a category-specific Fisher information, measuring the
sensitivity of the category membership. Category learning thus induces an
expansion of neural space near decision boundaries. We characterize the
properties of the categorical Fisher information, showing that its eigenvectors
give the most discriminant directions at each point of the projection space. We
find that, unexpectedly, its maxima are in general not exactly at, but near,
the class boundaries. Considering toy models and the MNIST dataset, we
numerically illustrate how after learning the two Fisher information matrices
match, and essentially align with the category boundaries. Finally, we relate
our approach to the Information Bottleneck one, and we exhibit a bias-variance
decomposition of the Bayes cost, of interest on its own.

</details>


### [150] [Empowering Decision Trees via Shape Function Branching](https://arxiv.org/abs/2510.19040)
*Nakul Upadhya,Eldan Cohen*

Main category: cs.LG

TL;DR: 提出了一种新的决策树泛化方法，即形状泛化树 (SGT)，其中每个内部节点将可学习的轴对齐形状函数应用于单个特征，从而在一次分割中实现丰富的非线性分区。


<details>
  <summary>Details</summary>
Motivation: 决策树因其在表格数据上的可解释性和强大性能而备受赞誉。然而，它们对简单轴对齐线性分割的依赖通常会迫使深度、复杂的结构来捕获非线性特征效应，从而削弱人类对所构建树的理解。

Method: 我们提出了一种决策树的新概括，即形状概括树 (SGT)，其中每个内部节点将可学习的轴对齐形状函数应用于单个特征，从而在一次分割中实现丰富的非线性分区。为了从数据中学习 SGT，我们提出了 ShapeCART，一种用于 SGT 的高效归纳算法。我们进一步将 SGT 框架扩展到二元形状函数 (S2GT) 和多路树 (SGTK)，并提出了 Shape2CART 和 ShapeCARTK，它们分别是 ShapeCART 的扩展，用于学习 S2GT 和 SGTK。

Result: 在各种数据集上的实验表明，与传统的轴对齐线性树相比，SGT 以更小的模型尺寸实现了卓越的性能。

Conclusion: SGT 具有固有的可解释性，并提供模型决策机制的直观、可视的解释。

Abstract: Decision trees are prized for their interpretability and strong performance
on tabular data. Yet, their reliance on simple axis-aligned linear splits often
forces deep, complex structures to capture non-linear feature effects,
undermining human comprehension of the constructed tree. To address this
limitation, we propose a novel generalization of a decision tree, the Shape
Generalized Tree (SGT), in which each internal node applies a learnable
axis-aligned shape function to a single feature, enabling rich, non-linear
partitioning in one split. As users can easily visualize each node's shape
function, SGTs are inherently interpretable and provide intuitive, visual
explanations of the model's decision mechanisms. To learn SGTs from data, we
propose ShapeCART, an efficient induction algorithm for SGTs. We further extend
the SGT framework to bivariate shape functions (S$^2$GT) and multi-way trees
(SGT$_K$), and present Shape$^2$CART and ShapeCART$_K$, extensions to ShapeCART
for learning S$^2$GTs and SGT$_K$s, respectively. Experiments on various
datasets show that SGTs achieve superior performance with reduced model size
compared to traditional axis-aligned linear trees.

</details>


### [151] [POLAR: Policy-based Layerwise Reinforcement Learning Method for Stealthy Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2510.19056)
*Kuai Yu,Xiaoyu Wu,Peishen Yan,Qingqian Yang,Linshan Jiang,Hao Wang,Yang Hua,Tao Song,Haibing Guan*

Main category: cs.LG

TL;DR: 提出了一种新的联邦学习后门攻击方法，该方法使用强化学习选择关键层进行投毒，以提高攻击的隐蔽性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习后门攻击方法依赖于基于规则的选择，没有考虑到层之间的相互关系，导致效果不佳，容易被检测到。

Method: 提出了一种名为POLAR的pipeline，它采用强化学习来解决分层后门攻击中的BC层选择问题。POLAR使用伯努利抽样，并引入正则化约束来限制修改层的数量。

Result: POLAR在对抗六种最先进的防御方法时，比最新的攻击方法高出40%。

Conclusion: POLAR是一种有效的联邦学习后门攻击方法，它可以通过选择关键层并限制修改层的数量来提高攻击的隐蔽性和有效性。

Abstract: Federated Learning (FL) enables decentralized model training across multiple
clients without exposing local data, but its distributed feature makes it
vulnerable to backdoor attacks. Despite early FL backdoor attacks modifying
entire models, recent studies have explored the concept of backdoor-critical
(BC) layers, which poison the chosen influential layers to maintain
stealthiness while achieving high effectiveness. However, existing BC layers
approaches rely on rule-based selection without consideration of the
interrelations between layers, making them ineffective and prone to detection
by advanced defenses. In this paper, we propose POLAR (POlicy-based LAyerwise
Reinforcement learning), the first pipeline to creatively adopt RL to solve the
BC layer selection problem in layer-wise backdoor attack. Different from other
commonly used RL paradigm, POLAR is lightweight with Bernoulli sampling. POLAR
dynamically learns an attack strategy, optimizing layer selection using policy
gradient updates based on backdoor success rate (BSR) improvements. To ensure
stealthiness, we introduce a regularization constraint that limits the number
of modified layers by penalizing large attack footprints. Extensive experiments
demonstrate that POLAR outperforms the latest attack methods by up to 40%
against six state-of-the-art (SOTA) defenses.

</details>


### [152] [Weight Decay may matter more than muP for Learning Rate Transfer in Practice](https://arxiv.org/abs/2510.19093)
*Atli Kosson,Jeremy Welborn,Yang Liu,Martin Jaggi,Xi Chen*

Main category: cs.LG

TL;DR: 本文研究了将小模型的最佳学习率迁移到大模型的方法，特别关注了Maximal Update Parameterization (muP) 的作用。


<details>
  <summary>Details</summary>
Motivation: muP旨在保持不同模型宽度下内部表征更新的稳定性，从而实现学习率的迁移。然而，muP的缩放规则依赖于一些强假设，尤其是在层输入与其权重和梯度更新的几何对齐方面。本文旨在检验这些假设在实际情况下的有效性，尤其是在大规模LLM训练中。

Method: 通过大规模实证研究，检验muP的假设在实际训练中的有效性。分析内部表征更新的动态，并研究权重衰减在学习率迁移中的作用。

Result: 研究表明，muP的假设仅在训练初期成立。在训练的大部分时间内，权重衰减而非muP能够稳定内部表征的更新动态，从而促进学习率的迁移。muP的缩放主要起到隐式学习率预热的作用。

Conclusion: 研究结果挑战了关于学习率迁移的普遍认知，并解释了为什么muP需要独立的权重衰减变体才能成功迁移学习率。可以用修改后的预热时间表在很大程度上取代muP。

Abstract: Transferring the optimal learning rate from small to large neural networks
can enable efficient training at scales where hyperparameter tuning is
otherwise prohibitively expensive. To this end, the Maximal Update
Parameterization (muP) proposes a learning rate scaling designed to keep the
update dynamics of internal representations stable across different model
widths. However, the scaling rules of muP rely on strong assumptions,
particularly about the geometric alignment of a layer's inputs with both its
weights and gradient updates. In this large-scale empirical investigation, we
show that these assumptions hold only briefly at the start of training in the
practical setups where learning rate transfer is most valuable, such as LLM
training. For the remainder of training it is weight decay rather than muP that
correctly stabilizes the update dynamics of internal representations across
widths, facilitating learning rate transfer. This suggests muP's scaling
primarily acts as a form of implicit learning rate warmup, allowing us to
largely replace it with modified warmup schedules. Together these findings
fundamentally challenge prevailing beliefs about learning rate transfer and can
explain empirical practice such as why muP requires the independent weight
decay variant for successful transfer.

</details>


### [153] [What Makes a Good Curriculum? Disentangling the Effects of Data Ordering on LLM Mathematical Reasoning](https://arxiv.org/abs/2510.19099)
*Yaning Jia,Chunhui Zhang,Xingjian Diao,Xiangchi Yuan,Zhongyu Ouyang,soroush vosoughi*

Main category: cs.LG

TL;DR: 本研究探讨了课程学习（CL）在提升大型语言模型（LLM）推理能力中的作用，发现没有一种通用的课程策略，前向或反向CL的有效性取决于模型能力和任务复杂性。


<details>
  <summary>Details</summary>
Motivation: 先前研究采用不同的难度指标和训练设置，对于课程学习何时有效、哪个方向更好等基本问题尚不明确，并且答案是否取决于我们衡量的标准。

Method: 通过一个统一的离线评估框架，将课程难度分解为五个互补维度：问题难度、模型惊讶度、置信度边际、预测不确定性和决策变异性。在Llama3.1-8B、Mistral-7B和Gemma3-4B上进行了受控的后训练实验。

Result: 发现（i）没有一种课程策略普遍占优——前向或反向CL的相对有效性共同取决于模型能力和任务复杂性；（ii）即使在单个指标内，不同难度级别的样本也会根据任务需求产生不同的收益；（iii）与任务对齐的课程侧重于塑造模型的最终表示和泛化，而内部状态课程则调节内部状态，如置信度和不确定性。

Conclusion: 研究结果挑战了通用课程策略的观念，并为跨模型和任务领域提供了可操作的指导，一些指标表明，优先考虑决策不确定样本可以进一步提高学习效果。

Abstract: Curriculum learning (CL) - ordering training data from easy to hard - has
become a popular strategy for improving reasoning in large language models
(LLMs). Yet prior work employs disparate difficulty metrics and training
setups, leaving open fundamental questions: When does curriculum help? Which
direction - forward or reverse - is better? And does the answer depend on what
we measure? We address these questions through a unified offline evaluation
framework that decomposes curriculum difficulty into five complementary
dimensions: Problem Difficulty, Model Surprisal, Confidence Margin, Predictive
Uncertainty, and Decision Variability. Through controlled post-training
experiments on mathematical reasoning benchmarks with Llama3.1-8B, Mistral-7B,
and Gemma3-4B, we find that (i) no curriculum strategy dominates universally -
the relative effectiveness of forward versus reverse CL depends jointly on
model capability and task complexity; (ii) even within a single metric, samples
at different difficulty levels produce distinct gains depending on task
demands; and (iii) task-aligned curricula focus on shaping the model's final
representations and generalization, whereas inner-state curricula modulate
internal states such as confidence and uncertainty. Our findings challenge the
notion of a universal curriculum strategy and offer actionable guidance across
model and task regimes, with some metrics indicating that prioritizing
decision-uncertain samples can further enhance learning outcomes.

</details>


### [154] [MetaCluster: Enabling Deep Compression of Kolmogorov-Arnold Network](https://arxiv.org/abs/2510.19105)
*Matthew Raffel,Adwaith Renjith,Lizhong Chen*

Main category: cs.LG

TL;DR: MetaCluster大幅压缩了Kolmogorov-Arnold Networks (KANs) 的参数存储，且不损失精度。


<details>
  <summary>Details</summary>
Motivation: KANs通过用每条边的向量替换标量权重来提高表达性和准确性，但也导致参数和内存的倍增。MetaCluster旨在解决KANs参数量大的问题。

Method: 使用元学习器将低维嵌入映射到系数向量，使其位于低维流形上，然后进行K-means聚类，并用共享质心替换每条边的向量。最后，丢弃元学习器，并对质心码本进行微调。

Result: 在MNIST、CIFAR-10和CIFAR-100数据集上，MetaCluster在标准KANs和ConvKANs上实现了高达80倍的参数存储减少，且没有精度损失。

Conclusion: MetaCluster通过利用KAN参数的向量特性，实现了跨多个系数的存储分摊，从而大大减少了参数存储。

Abstract: Kolmogorov-Arnold Networks (KANs) replace scalar weights with per-edge
vectors of basis coefficients, thereby boosting expressivity and accuracy but
at the same time resulting in a multiplicative increase in parameters and
memory. We propose MetaCluster, a framework that makes KANs highly compressible
without sacrificing accuracy. Specifically, a lightweight meta-learner, trained
jointly with the KAN, is used to map low-dimensional embedding to coefficient
vectors, shaping them to lie on a low-dimensional manifold that is amenable to
clustering. We then run K-means in coefficient space and replace per-edge
vectors with shared centroids. Afterwards, the meta-learner can be discarded,
and a brief fine-tuning of the centroid codebook recovers any residual accuracy
loss. The resulting model stores only a small codebook and per-edge indices,
exploiting the vector nature of KAN parameters to amortize storage across
multiple coefficients. On MNIST, CIFAR-10, and CIFAR-100, across standard KANs
and ConvKANs using multiple basis functions, MetaCluster achieves a reduction
of up to 80$\times$ in parameter storage, with no loss in accuracy. Code will
be released upon publication.

</details>


### [155] [Learning Peer Influence Probabilities with Linear Contextual Bandits](https://arxiv.org/abs/2510.19119)
*Ahmed Sayeed Faruk,Mohammad Shahverdikondori,Elena Zheleva*

Main category: cs.LG

TL;DR: 本文研究了在网络环境中学习同伴影响概率的问题，并提出了一种不确定性引导的探索算法，该算法可以通过调整参数来达到遗憾最小化和估计误差之间的任何权衡。


<details>
  <summary>Details</summary>
Motivation: 在网络环境中，用户经常与他人分享关于内容、产品、服务和行动方案的建议。这些建议在多大程度上是成功的和被采纳的，在很大程度上取决于发送者、接收者、他们的关系、推荐的项目和媒介的特征，这使得同伴影响概率非常不同。准确估计这些概率是理解信息传播过程和提高病毒营销策略有效性的关键。然而，从数据中学习这些概率是具有挑战性的；静态数据可以捕捉同伴推荐和同伴行为之间的相关性，但不能揭示影响关系。在线学习算法可以从干预中学习这些概率，但要么通过从随机探索中学习来浪费资源，要么为了奖励而优化，从而有利于探索具有较高影响概率的空间。

Method: 在本文中，我们研究了在上下文线性bandit框架下学习同伴影响概率的问题。我们证明了遗憾最小化和估计误差之间可能出现根本的权衡，描述了所有可实现的速率对，并提出了一种不确定性引导的探索算法，该算法可以通过调整参数来达到这种权衡中的任何一对。

Result: 我们在半合成网络数据集上的实验表明，我们的方法优于静态方法和忽略这种权衡的上下文bandits。

Conclusion: 本文研究了在网络环境中学习同伴影响概率的问题，并提出了一种不确定性引导的探索算法，该算法可以通过调整参数来达到遗憾最小化和估计误差之间的任何权衡。

Abstract: In networked environments, users frequently share recommendations about
content, products, services, and courses of action with others. The extent to
which such recommendations are successful and adopted is highly contextual,
dependent on the characteristics of the sender, recipient, their relationship,
the recommended item, and the medium, which makes peer influence probabilities
highly heterogeneous. Accurate estimation of these probabilities is key to
understanding information diffusion processes and to improving the
effectiveness of viral marketing strategies. However, learning these
probabilities from data is challenging; static data may capture correlations
between peer recommendations and peer actions but fails to reveal influence
relationships. Online learning algorithms can learn these probabilities from
interventions but either waste resources by learning from random exploration or
optimize for rewards, thus favoring exploration of the space with higher
influence probabilities. In this work, we study learning peer influence
probabilities under a contextual linear bandit framework. We show that a
fundamental trade-off can arise between regret minimization and estimation
error, characterize all achievable rate pairs, and propose an
uncertainty-guided exploration algorithm that, by tuning a parameter, attains
any pair within this trade-off. Our experiments on semi-synthetic network
datasets show the advantages of our method over static methods and contextual
bandits that ignore this trade-off.

</details>


### [156] [Steering Autoregressive Music Generation with Recursive Feature Machines](https://arxiv.org/abs/2510.19127)
*Daniel Zhao,Daniel Beaglehole,Taylor Berg-Kirkpatrick,Julian McAuley,Zachary Novack*

Main category: cs.LG

TL;DR: MusicRFM通过控制预训练音乐模型的内部激活来实现对音乐生成的细粒度、可解释的控制，无需重新训练模型或引入明显的伪影。


<details>
  <summary>Details</summary>
Motivation: 现有的可控音乐生成方法通常需要重新训练模型或引入可听的伪影。

Method: 该方法调整了递归特征机器（RFM），通过直接控制冻结的、预训练的音乐模型的内部激活来实现细粒度的、可解释的控制。RFM分析模型的内部梯度以产生可解释的“概念方向”，或激活空间中的特定轴，这些轴对应于诸如音符或和弦之类的音乐属性。首先训练轻量级的RFM探针以发现MusicGen隐藏状态中的这些方向；然后，在推理过程中，将它们注入回模型中，以实时指导生成过程，而无需逐步优化。

Result: 该方法成功地解决了控制和生成质量之间的权衡：可以将生成目标音符的准确率从0.23提高到0.82，而文本提示的依从性保持在未控制基线的约0.02之内，这表明了有效的控制，并且对提示的保真度影响最小。

Conclusion: MusicRFM是一种在音乐领域中实现可控音乐生成的有效方法，它可以在不牺牲生成质量的前提下，实现对音乐属性的精确控制。

Abstract: Controllable music generation remains a significant challenge, with existing
methods often requiring model retraining or introducing audible artifacts. We
introduce MusicRFM, a framework that adapts Recursive Feature Machines (RFMs)
to enable fine-grained, interpretable control over frozen, pre-trained music
models by directly steering their internal activations. RFMs analyze a model's
internal gradients to produce interpretable "concept directions", or specific
axes in the activation space that correspond to musical attributes like notes
or chords. We first train lightweight RFM probes to discover these directions
within MusicGen's hidden states; then, during inference, we inject them back
into the model to guide the generation process in real-time without per-step
optimization. We present advanced mechanisms for this control, including
dynamic, time-varying schedules and methods for the simultaneous enforcement of
multiple musical properties. Our method successfully navigates the trade-off
between control and generation quality: we can increase the accuracy of
generating a target musical note from 0.23 to 0.82, while text prompt adherence
remains within approximately 0.02 of the unsteered baseline, demonstrating
effective control with minimal impact on prompt fidelity. We release code to
encourage further exploration on RFMs in the music domain.

</details>


### [157] [InvarGC: Invariant Granger Causality for Heterogeneous Interventional Time Series under Latent Confounding](https://arxiv.org/abs/2510.19138)
*Ziyi Zhang,Shaogang Ren,Xiaoning Qian,Nick Duffield*

Main category: cs.LG

TL;DR: 提出了一种新的Granger因果关系（InvarGC）方法，以应对传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统Granger因果关系检验在非线性因果关系检测方面存在不足，且依赖于因果充分性和已知干预目标这两个在现实世界中难以满足的假设。

Method: 利用跨环境异质性来减轻潜在混淆的影响，并区分干预和非干预环境，从而恢复不变的因果关系。

Result: 在合成和真实数据集上的大量实验表明，该方法与最先进的方法相比具有竞争优势。

Conclusion: InvarGC在存在潜在混淆和未知干预的情况下，能够更准确地发现因果关系。

Abstract: Granger causality is widely used for causal structure discovery in complex
systems from multivariate time series data. Traditional Granger causality tests
based on linear models often fail to detect even mild non-linear causal
relationships. Therefore, numerous recent studies have investigated non-linear
Granger causality methods, achieving improved performance. However, these
methods often rely on two key assumptions: causal sufficiency and known
interventional targets. Causal sufficiency assumes the absence of latent
confounders, yet their presence can introduce spurious correlations. Moreover,
real-world time series data usually come from heterogeneous environments,
without prior knowledge of interventions. Therefore, in practice, it is
difficult to distinguish intervened environments from non-intervened ones, and
even harder to identify which variables or timesteps are affected. To address
these challenges, we propose Invariant Granger Causality (InvarGC), which
leverages cross-environment heterogeneity to mitigate the effects of latent
confounding and to distinguish intervened from non-intervened environments with
edge-level granularity, thereby recovering invariant causal relations. In
addition, we establish the identifiability under these conditions. Extensive
experiments on both synthetic and real-world datasets demonstrate the
competitive performance of our approach compared to state-of-the-art methods.

</details>


### [158] [Subliminal Corruption: Mechanisms, Thresholds, and Interpretability](https://arxiv.org/abs/2510.19152)
*Reya Vir,Sarvesh Bhatnagar*

Main category: cs.LG

TL;DR: 论文研究了在合成数据上微调的机器学习模型中，不易察觉的偏差如何通过语义中立的数据传播，即使通过了标准安全检查。这种现象被称为“Subliminal Corruption”。


<details>
  <summary>Details</summary>
Motivation: 量化理解“Subliminal Corruption”的动态。

Method: 使用带有GPT-2的teacher-student setup，对“Subliminal Corruption”的缩放规律、阈值和机制进行了系统的研究。

Result: 1. “Subliminal Corruption”导致行为交叉，降低了模型的整体对齐度。2. 对齐失败发生在一个poisoned data的临界阈值的突变阶段，而不是逐渐降级。3. 可解释性分析表明，这种corruption机制模仿了模型的自然微调过程，使其难以检测。

Conclusion: 研究结果表明，依赖合成数据的AI系统存在严重漏洞，需要新的安全协议来解决潜在的威胁。

Abstract: As machine learning models are increasingly fine-tuned on synthetic data,
there is a critical risk of subtle misalignments spreading through
interconnected AI systems. This paper investigates subliminal corruption, which
we define as undesirable traits are transmitted through semantically neutral
data, bypassing standard safety checks. While this phenomenon has been
identified, a quantitative understanding of its dynamics is missing. To address
this gap, we present a systematic study of the scaling laws, thresholds, and
mechanisms of subliminal corruption using a teacher-student setup with GPT-2.
Our experiments reveal three key findings: (1) subliminal corruption causes
behavioral crossover, degrading the model's overall alignment, not just the
targeted trait; (2) alignment fails in a sharp phase transition at a critical
threshold of poisoned data, rather than degrading gradually; and (3)
interpretability analysis shows the corruption mechanism mimics the model's
natural fine-tuning process, making it difficult to detect. These results
demonstrate a critical vulnerability in AI systems that rely on synthetic data
and highlight the need for new safety protocols that can account for latent
threats.

</details>


### [159] [Feature Space Adaptation for Robust Model Fine-Tuning](https://arxiv.org/abs/2510.19155)
*Peng Wang,Minghao Gu,Qiang Huang*

Main category: cs.LG

TL;DR: 提出了一种在特征空间中进行微调的方法，以减轻灾难性遗忘问题，提高模型在分布偏移下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 模型微调中常见的灾难性遗忘问题，特别是在下游领域数据有限或与预训练分布差异较大时。现有的参数高效微调方法容易使模型过度专注于下游数据。

Method: 提出了两种新的微调方法：LoRFA (Low-Rank Feature Adaptation) 和 VeFA (Vector-Based Feature Adaptation)，在特征空间进行微调。通过轻量级的特征级转换来补偿下游潜在变量的影响，从而保留预训练的表示。

Result: 在图像分类、NLU和NLG任务上，LoRFA和VeFA与LoRA相比，实现了可比的微调结果和更强的鲁棒性。

Conclusion: 特征空间适配可以实现与标准微调相当的结果，并在分布偏移下表现出更强的鲁棒性。

Abstract: Catastrophic forgetting is a common issue in model fine-tuning, especially
when the downstream domain contains limited labeled data or differs greatly
from the pre-training distribution. Existing parameter-efficient fine-tuning
methods operate in the weight space by modifying or augmenting the pre-trained
model's parameters, which can yield models overly specialized to the available
downstream data. To mitigate the risk of overwriting pre-trained knowledge and
enhance robustness, we propose to fine-tune the pre-trained model in the
feature space. Two new fine-tuning methods are proposed: LoRFA (Low-Rank
Feature Adaptation) and VeFA (Vector-Based Feature Adaptation). Feature space
adaptation is inspired by the idea of effect equivalence modeling (EEM) of
downstream lurking variables causing distribution shifts, which posits that
unobserved factors can be represented as the total equivalent amount on
observed features. By compensating for the effects of downstream lurking
variables via a lightweight feature-level transformation, the pre-trained
representations can be preserved, which improves model generalization under
distribution shift. We evaluate LoRFA and VeFA versus LoRA on image
classification, NLU, and NLG, covering both standard fine-tuning metrics and
robustness. Feature space adaptation achieves comparable fine-tuning results
and consistently stronger robustness.

</details>


### [160] [Instance-Dependent Regret Bounds for Nonstochastic Linear Partial Monitoring](https://arxiv.org/abs/2510.19158)
*Federico Di Gennaro,Khaled Eldowa,Nicolò Cesa-Bianchi*

Main category: cs.LG

TL;DR: 本文研究了线性部分监测问题，该问题可以看作是线性 bandits 的推广，其中损失和反馈以灵活的方式解耦。通过一种易于有效实施的exploration-by-optimization 方法，解决了该问题的非随机（对抗）有限动作版本。导出的遗憾界限取决于游戏结构，并且比以前的理论保证更透明。在容易（局部可观察）的游戏中，它们实现了标准的$\\sqrt{T}$速率，在困难（全局可观察）的游戏中实现了$T^{2/3}$速率，其中 T 是时间范围。


<details>
  <summary>Details</summary>
Motivation: 研究线性部分监测问题，它概括了线性 bandits，其中损失和反馈以灵活的方式解耦。

Method: 通过 exploration-by-optimization 方法解决问题的非随机（对抗）有限动作版本。

Result: 导出的遗憾界限取决于游戏结构，并且比以前的理论保证更透明。在容易（局部可观察）的游戏中，实现了标准的$\\sqrt{T}$速率，在困难（全局可观察）的游戏中实现了$T^{2/3}$速率。

Conclusion: 在一些新旧的部分信息设置中验证了界限，并表明在有趣的情况下，对游戏结构的依赖可能是紧密的。

Abstract: In contrast to the classic formulation of partial monitoring, linear partial
monitoring can model infinite outcome spaces, while imposing a linear structure
on both the losses and the observations. This setting can be viewed as a
generalization of linear bandits where loss and feedback are decoupled in a
flexible manner. In this work, we address a nonstochastic (adversarial),
finite-actions version of the problem through a simple instance of the
exploration-by-optimization method that is amenable to efficient
implementation. We derive regret bounds that depend on the game structure in a
more transparent manner than previous theoretical guarantees for this paradigm.
Our bounds feature instance-specific quantities that reflect the degree of
alignment between observations and losses, and resemble known guarantees in the
stochastic setting. Notably, they achieve the standard $\sqrt{T}$ rate in easy
(locally observable) games and $T^{2/3}$ in hard (globally observable) games,
where $T$ is the time horizon. We instantiate these bounds in a selection of
old and new partial information settings subsumed by this model, and illustrate
that the achieved dependence on the game structure can be tight in interesting
cases.

</details>


### [161] [Preliminary Use of Vision Language Model Driven Extraction of Mouse Behavior Towards Understanding Fear Expression](https://arxiv.org/abs/2510.19160)
*Paimon Goulart,Jordan Steinhauser,Kylene Shuler,Edward Korzus,Jia Chen,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 这篇论文介绍了一种新的视觉-语言模型（VLM），用于对小鼠的行为进行分类，并通过结合提示、上下文学习和帧级预处理来提高性能。


<details>
  <summary>Details</summary>
Motivation: 为了改进许多学科的科学探索，整合多样化的数据至关重要。现有的程序难以高精度、少用户输入地生成有价值的数据集。

Method: 使用开源的Qwen2.5-VL模型，并通过提示、上下文学习（ICL）与带标签的示例以及帧级预处理来增强其性能。

Result: 每种方法都有助于提高分类效果。将这些方法结合起来，可以在没有任何模型微调的情况下，在所有行为（包括罕见类别，如冻结和逃跑）中获得强大的F1分数。

Conclusion: 该模型通过整合在多个时间点和环境中测量的各种行为特征，支持研究小鼠行为的跨学科研究人员，从而创建一个可以解决复杂研究问题的综合数据集。

Abstract: Integration of diverse data will be a pivotal step towards improving
scientific explorations in many disciplines. This work establishes a
vision-language model (VLM) that encodes videos with text input in order to
classify various behaviors of a mouse existing in and engaging with their
environment. Importantly, this model produces a behavioral vector over time for
each subject and for each session the subject undergoes. The output is a
valuable dataset that few programs are able to produce with as high accuracy
and with minimal user input. Specifically, we use the open-source Qwen2.5-VL
model and enhance its performance through prompts, in-context learning (ICL)
with labeled examples, and frame-level preprocessing. We found that each of
these methods contributes to improved classification, and that combining them
results in strong F1 scores across all behaviors, including rare classes like
freezing and fleeing, without any model fine-tuning. Overall, this model will
support interdisciplinary researchers studying mouse behavior by enabling them
to integrate diverse behavioral features, measured across multiple time points
and environments, into a comprehensive dataset that can address complex
research questions.

</details>


### [162] [Natural Gradient VI: Guarantees for Non-Conjugate Models](https://arxiv.org/abs/2510.19163)
*Fangyuan Sun,Ilyas Fatkhullin,Niao He*

Main category: cs.LG

TL;DR: 本文研究了随机自然梯度变分推理(NGVI)的理论基础，特别是在非共轭似然的情况下。


<details>
  <summary>Details</summary>
Motivation: 尽管NGVI在变分推理中取得了经验上的成功和基础性的作用，但其理论基础仍然有限，特别是在非共轭似然的情况下。现有的理论结果不能扩展到非共轭设置，在非共轭设置下，变分损失变成非凸的，更难分析。

Method: 1. 推导了变分损失相对于合适的镜像图满足相对平滑性的充分条件。2. 提出了改进的NGVI算法，结合了非欧几里德投影，并证明了其全局非渐近收敛到平稳点。3. 在关于可能性的额外结构假设下，发现了变分损失的隐藏凸性，并建立了NGVI快速全局收敛到全局最优。

Result: 1. 导出了NGVI满足相对平滑性的充分条件。2. 证明了改进的NGVI算法的全局非渐近收敛性。3. 揭示了变分损失的隐藏凸性，并建立了NGVI快速全局收敛到全局最优。

Conclusion: 这些结果为在具有挑战性的推理环境中NGVI的几何和收敛行为提供了新的见解。

Abstract: Stochastic Natural Gradient Variational Inference (NGVI) is a widely used
method for approximating posterior distribution in probabilistic models.
Despite its empirical success and foundational role in variational inference,
its theoretical underpinnings remain limited, particularly in the case of
non-conjugate likelihoods. While NGVI has been shown to be a special instance
of Stochastic Mirror Descent, and recent work has provided convergence
guarantees using relative smoothness and strong convexity for conjugate models,
these results do not extend to the non-conjugate setting, where the variational
loss becomes non-convex and harder to analyze. In this work, we focus on
mean-field parameterization and advance the theoretical understanding of NGVI
in three key directions. First, we derive sufficient conditions under which the
variational loss satisfies relative smoothness with respect to a suitable
mirror map. Second, leveraging this structure, we propose a modified NGVI
algorithm incorporating non-Euclidean projections and prove its global
non-asymptotic convergence to a stationary point. Finally, under additional
structural assumptions about the likelihood, we uncover hidden convexity
properties of the variational loss and establish fast global convergence of
NGVI to a global optimum. These results provide new insights into the geometry
and convergence behavior of NGVI in challenging inference settings.

</details>


### [163] [Imbalanced Gradients in RL Post-Training of Multi-Task LLMs](https://arxiv.org/abs/2510.19178)
*Runzhe Wu,Ankur Samanta,Ayush Jain,Scott Fujimoto,Jeongyeol Kwon,Ben Kretzu,Youliang Yu,Kaveh Hassani,Boris Vidolov,Yonathan Efroni*

Main category: cs.LG

TL;DR: 大型语言模型的多任务后训练通常通过混合来自不同任务的数据集并共同优化它们来执行。但是，某些任务会产生明显更大的梯度，从而使更新偏向于这些任务。


<details>
  <summary>Details</summary>
Motivation: 研究表明，在强化学习的后训练中，某些任务产生明显更大的梯度，导致优化偏向这些任务。然而，这并不意味着更大的梯度就代表更大的学习收益。

Method: 分析不同任务的梯度大小和学习收益之间的关系，并探讨梯度不平衡的原因。

Result: 发现大梯度任务的学习收益与小梯度任务相似甚至更低，且梯度不平衡不能用常见的训练统计数据来解释。

Conclusion: 需要对大型语言模型进行基于梯度的校正，以避免naive的数据集混合。

Abstract: Multi-task post-training of large language models (LLMs) is typically
performed by mixing datasets from different tasks and optimizing them jointly.
This approach implicitly assumes that all tasks contribute gradients of similar
magnitudes; when this assumption fails, optimization becomes biased toward
large-gradient tasks. In this paper, however, we show that this assumption
fails in RL post-training: certain tasks produce significantly larger
gradients, thus biasing updates toward those tasks. Such gradient imbalance
would be justified only if larger gradients implied larger learning gains on
the tasks (i.e., larger performance improvements) -- but we find this is not
true. Large-gradient tasks can achieve similar or even much lower learning
gains than small-gradient ones. Further analyses reveal that these gradient
imbalances cannot be explained by typical training statistics such as training
rewards or advantages, suggesting that they arise from the inherent differences
between tasks. This cautions against naive dataset mixing and calls for future
work on principled gradient-level corrections for LLMs.

</details>


### [164] [A Communication-Efficient Decentralized Actor-Critic Algorithm](https://arxiv.org/abs/2510.19199)
*Xiaoxing Ren,Nicola Bastianello,Thomas Parisini,Andreas A. Malikopoulos*

Main category: cs.LG

TL;DR: 本文研究了智能体之间通信受限的多智能体系统中的强化学习问题。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，智能体之间的通信受限，为了解决这个问题。

Method: 本文提出了一个分散的actor-critic学习框架，每个智能体在与其邻居交换信息之前，都会执行多次策略和价值函数的本地更新，其中后者由多层神经网络近似。

Result: 该算法在马尔可夫采样下建立了有限时间收敛性分析。具体而言，为了达到$\\varepsilon$-精确的平稳点，样本复杂度为$\\mathcal{O}(\\varepsilon^{-3})$，通信复杂度为$\\mathcal{O}(\\varepsilon^{-1}\\tau^{-1})$，其中tau表示局部训练步数。

Conclusion: 数值实验验证了理论结果

Abstract: In this paper, we study the problem of reinforcement learning in multi-agent
systems where communication among agents is limited. We develop a decentralized
actor-critic learning framework in which each agent performs several local
updates of its policy and value function, where the latter is approximated by a
multi-layer neural network, before exchanging information with its neighbors.
This local training strategy substantially reduces the communication burden
while maintaining coordination across the network. We establish finite-time
convergence analysis for the algorithm under Markov-sampling. Specifically, to
attain the $\varepsilon$-accurate stationary point, the sample complexity is of
order $\mathcal{O}(\varepsilon^{-3})$ and the communication complexity is of
order $\mathcal{O}(\varepsilon^{-1}\tau^{-1})$, where tau denotes the number of
local training steps. We also show how the final error bound depends on the
neural network's approximation quality. Numerical experiments in a cooperative
control setting illustrate and validate the theoretical findings.

</details>


### [165] [An Active Diffusion Neural Network for Graphs](https://arxiv.org/abs/2510.19202)
*Mengying Jiang*

Main category: cs.LG

TL;DR: 提出了一种名为ADGNN的GNN模型，通过主动扩散克服过平滑问题，提高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有GNN模型模拟被动热扩散，存在过平滑问题，限制了捕获全局图信息的能力。受宇宙热寂现象启发，认为无外部输入时节点表示会收敛到相同特征向量。

Method: 提出ADGNN，通过整合多个外部信息源动态影响扩散过程，实现主动扩散。通过直接计算主动扩散迭代公式的闭式解，实现真正的无限扩散。

Result: 在各种图任务中，ADGNN显著提高了准确性和效率。

Conclusion: ADGNN能够有效捕获全局图信息并保持节点独特性。

Abstract: The analogy to heat diffusion has enhanced our understanding of information
flow in graphs and inspired the development of Graph Neural Networks (GNNs).
However, most diffusion-based GNNs emulate passive heat diffusion, which still
suffers from over-smoothing and limits their ability to capture global graph
information. Inspired by the heat death of the universe, which posits that
energy distribution becomes uniform over time in a closed system, we recognize
that, without external input, node representations in a graph converge to
identical feature vectors as diffusion progresses. To address this issue, we
propose the Active Diffusion-based Graph Neural Network (ADGNN). ADGNN achieves
active diffusion by integrating multiple external information sources that
dynamically influence the diffusion process, effectively overcoming the
over-smoothing problem. Furthermore, our approach realizes true infinite
diffusion by directly calculating the closed-form solution of the active
diffusion iterative formula. This allows nodes to preserve their unique
characteristics while efficiently gaining comprehensive insights into the
graph's global structure. We evaluate ADGNN against several state-of-the-art
GNN models across various graph tasks. The results demonstrate that ADGNN
significantly improves both accuracy and efficiency, highlighting its
effectiveness in capturing global graph information and maintaining node
distinctiveness.

</details>


### [166] [Enhancing Graph Neural Networks: A Mutual Learning Approach](https://arxiv.org/abs/2510.19223)
*Paul Agbaje,Akajyoti Mitra,Afia Anjum,Pranali Khose,Ebelechukwu Nwafor,Habeeb Olufowobi*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于 GNN 的协同学习框架，用于知识蒸馏，无需预训练的教师模型，通过多个学生 GNN 互相学习，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统的知识蒸馏方法依赖于预训练的复杂教师模型，但本文探索了在没有教师模型的情况下，GNN 之间的协同学习。

Method: 提出一个协同学习框架，其中 GNN 学生模型的集成在整个训练过程中相互教学。引入自适应 logit 加权单元以促进模型之间有效的知识交换，并引入熵增强技术以改进相互学习。

Result: 在节点和图分类的三个数据集上进行的大量实验表明了该方法的有效性。

Conclusion: 相对简单和浅层的 GNN 架构可以协同学习高效的模型，这些模型能够在推理过程中表现更好，特别是在处理多个任务时。

Abstract: Knowledge distillation (KD) techniques have emerged as a powerful tool for
transferring expertise from complex teacher models to lightweight student
models, particularly beneficial for deploying high-performance models in
resource-constrained devices. This approach has been successfully applied to
graph neural networks (GNNs), harnessing their expressive capabilities to
generate node embeddings that capture structural and feature-related
information. In this study, we depart from the conventional KD approach by
exploring the potential of collaborative learning among GNNs. In the absence of
a pre-trained teacher model, we show that relatively simple and shallow GNN
architectures can synergetically learn efficient models capable of performing
better during inference, particularly in tackling multiple tasks. We propose a
collaborative learning framework where ensembles of student GNNs mutually teach
each other throughout the training process. We introduce an adaptive logit
weighting unit to facilitate efficient knowledge exchange among models and an
entropy enhancement technique to improve mutual learning. These components
dynamically empower the models to adapt their learning strategies during
training, optimizing their performance for downstream tasks. Extensive
experiments conducted on three datasets each for node and graph classification
demonstrate the effectiveness of our approach.

</details>


### [167] [Controllable Machine Unlearning via Gradient Pivoting](https://arxiv.org/abs/2510.19226)
*Youngsik Hwang,Dong-Young Lim*

Main category: cs.LG

TL;DR: 提出了一种新的可控的 machine unlearning 算法，通过在 Pareto 前沿上导航来实现 unlearning 效果和模型保真度之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的 approximate unlearning 方法在 unlearning 效果和模型保真度之间存在 trade-off，面临 over-forgetting 的风险，缺乏对 unlearning 过程的细粒度控制，并且缺乏 holistic 的评估指标。

Method: 将 MU 重新定义为一个多目标优化 (MOO) 问题，并提出了一种新的算法 Controllable Unlearning by Pivoting Gradient (CUP)，该算法具有独特的 pivoting 机制，可以在整个 Pareto 前沿上进行可控的导航。

Result: CUP 能够生成更好的 Pareto 最优解集，并在各种视觉任务中始终优于现有方法。

Conclusion: CUP 算法通过超参数 `unlearning intensity' 实现对 unlearning 效果和模型保真度之间权衡的精确选择，并通过 hypervolume indicator 指标进行评估，实验结果表明 CUP 算法的优越性。

Abstract: Machine unlearning (MU) aims to remove the influence of specific data from a
trained model. However, approximate unlearning methods, often formulated as a
single-objective optimization (SOO) problem, face a critical trade-off between
unlearning efficacy and model fidelity. This leads to three primary challenges:
the risk of over-forgetting, a lack of fine-grained control over the unlearning
process, and the absence of metrics to holistically evaluate the trade-off. To
address these issues, we reframe MU as a multi-objective optimization (MOO)
problem. We then introduce a novel algorithm, Controllable Unlearning by
Pivoting Gradient (CUP), which features a unique pivoting mechanism. Unlike
traditional MOO methods that converge to a single solution, CUP's mechanism is
designed to controllably navigate the entire Pareto frontier. This navigation
is governed by a single intuitive hyperparameter, the `unlearning intensity',
which allows for precise selection of a desired trade-off. To evaluate this
capability, we adopt the hypervolume indicator, a metric that captures both the
quality and diversity of the entire set of solutions an algorithm can generate.
Our experimental results demonstrate that CUP produces a superior set of
Pareto-optimal solutions, consistently outperforming existing methods across
various vision tasks.

</details>


### [168] [Brain-Inspired Perspective on Configurations: Unsupervised Similarity and Early Cognition](https://arxiv.org/abs/2510.19229)
*Juntang Wang,Yihan Wang,Hao Wu,Dongmian Zou,Shixin Xu*

Main category: cs.LG

TL;DR: 提出了一种新的聚类框架，灵感来自大脑，具有分层组织、新颖性敏感性和灵活适应性。


<details>
  <summary>Details</summary>
Motivation: 当前的机器学习模型在无监督的情况下，很难像婴儿一样发现类别、检测新事物和适应新环境。

Method: 使用有限分辨率聚类框架，该框架使用单一分辨率参数和吸引-排斥动力学来产生分层组织。

Result: 在各种数据集中，配置在标准聚类指标上具有竞争力，在新颖性检测中达到 87% 的 AUC，并在动态类别演化过程中显示出 35% 的更好稳定性。

Conclusion: 这些结果将配置定位为早期认知分类的一个有原则的计算模型，也是迈向受大脑启发的人工智能的一步。

Abstract: Infants discover categories, detect novelty, and adapt to new contexts
without supervision -- a challenge for current machine learning. We present a
brain-inspired perspective on configurations, a finite-resolution clustering
framework that uses a single resolution parameter and attraction-repulsion
dynamics to yield hierarchical organization, novelty sensitivity, and flexible
adaptation. To evaluate these properties, we introduce mheatmap, which provides
proportional heatmaps and a reassignment algorithm to fairly assess
multi-resolution and dynamic behavior. Across datasets, configurations are
competitive on standard clustering metrics, achieve 87% AUC in novelty
detection, and show 35% better stability during dynamic category evolution.
These results position configurations as a principled computational model of
early cognitive categorization and a step toward brain-inspired AI.

</details>


### [169] [Understanding the Implicit Biases of Design Choices for Time Series Foundation Models](https://arxiv.org/abs/2510.19236)
*Annan Yu,Danielle C. Maddix,Boran Han,Xiyuan Zhang,Abdul Fatir Ansari,Oleksandr Shchur,Christos Faloutsos,Andrew Gordon Wilson,Michael W. Mahoney,Yuyang Wang*

Main category: cs.LG

TL;DR: 本文旨在理解时间序列基础模型（TSFM）训练过程中各种“旋钮”如何影响模型质量，而非开发新模型并声称其优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解TSFM设计中微妙的归纳偏置如何强烈影响其行为，并填补对训练过程如何影响模型质量的理解空白。

Method: 采用理论分析与受控经验评估相结合的方法，识别了patch大小、嵌入选择和训练目标等设计选择。

Result: 研究结果表明，这些设计选择会导致模型基本属性（如时间行为、几何结构）的隐式偏差，并且这些偏差可能符合直觉，也可能非常违反直觉，具体取决于模型和数据的属性。案例研究还展示了多种偏差如何以复杂的方式相互作用。

Conclusion: 研究讨论了结果对学习“深刻教训”和构建TSFM的意义。

Abstract: Time series foundation models (TSFMs) are a class of potentially powerful,
general-purpose tools for time series forecasting and related temporal tasks,
but their behavior is strongly shaped by subtle inductive biases in their
design. Rather than developing a new model and claiming that it is better than
existing TSFMs, e.g., by winning on existing well-established benchmarks, our
objective is to understand how the various ``knobs'' of the training process
affect model quality. Using a mix of theory and controlled empirical
evaluation, we identify several design choices (patch size, embedding choice,
training objective, etc.) and show how they lead to implicit biases in
fundamental model properties (temporal behavior, geometric structure, how
aggressively or not the model regresses to the mean, etc.); and we show how
these biases can be intuitive or very counterintuitive, depending on properties
of the model and data. We also illustrate in a case study on outlier handling
how multiple biases can interact in complex ways; and we discuss implications
of our results for learning the bitter lesson and building TSFMs.

</details>


### [170] [SPOT: Scalable Policy Optimization with Trees for Markov Decision Processes](https://arxiv.org/abs/2510.19241)
*Xuyuan Xiong,Pedro Chumpitaz-Flores,Kaixun Hua,Cheng Hua*

Main category: cs.LG

TL;DR: 提出了一种新的计算决策树策略的方法，该方法将优化问题构建为混合整数线性规划（MILP），并通过解耦MDP动态与树结构约束来提高效率。


<details>
  <summary>Details</summary>
Motivation: 在高风险决策中，可解释的强化学习策略至关重要，但在马尔可夫决策过程（MDP）中优化决策树策略仍然具有挑战性。

Method: 提出了一种名为SPOT的新方法，用于计算决策树策略，该方法将优化问题构建为混合整数线性规划（MILP）。为了提高效率，我们采用了一种降维分支定界方法，该方法将MDP动态与树结构约束解耦，从而实现高效的并行搜索。

Result: 在标准基准测试上的实验结果表明，SPOT实现了显著的加速，并且可以扩展到具有更多状态的更大MDP。

Conclusion: 我们的方法同时实现了可解释性和可扩展性，与现有方法相比，能够以快一个数量级的速度提供高质量的策略。

Abstract: Interpretable reinforcement learning policies are essential for high-stakes
decision-making, yet optimizing decision tree policies in Markov Decision
Processes (MDPs) remains challenging. We propose SPOT, a novel method for
computing decision tree policies, which formulates the optimization problem as
a mixed-integer linear program (MILP). To enhance efficiency, we employ a
reduced-space branch-and-bound approach that decouples the MDP dynamics from
tree-structure constraints, enabling efficient parallel search. This
significantly improves runtime and scalability compared to previous methods.
Our approach ensures that each iteration yields the optimal decision tree.
Experimental results on standard benchmarks demonstrate that SPOT achieves
substantial speedup and scales to larger MDPs with a significantly higher
number of states. The resulting decision tree policies are interpretable and
compact, maintaining transparency without compromising performance. These
results demonstrate that our approach simultaneously achieves interpretability
and scalability, delivering high-quality policies an order of magnitude faster
than existing approaches.

</details>


### [171] [Interpret Policies in Deep Reinforcement Learning using SILVER with RL-Guided Labeling: A Model-level Approach to High-dimensional and Multi-action Environments](https://arxiv.org/abs/2510.19244)
*Yiyu Qian,Su Nguyen,Chao Chen,Qinyue Zhou,Liyuan Zhao*

Main category: cs.LG

TL;DR: 提出了一种改进的SILVER框架，通过结合RL策略的动作输出，扩展到多动作和高维环境，以提高深度RL的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的SILVER框架在解释RL策略时，仅限于低维、二元动作领域，缺乏对复杂环境的适用性。

Method: 该方法首先提取图像特征，进行SHAP特征归因，然后利用RL引导的标签生成行为一致的边界数据集，最后训练代理模型来解释RL策略的决策结构。

Result: 在Atari环境中的实验表明，该方法在保持竞争性任务性能的同时，显著提高了透明度和人类对智能体行为的理解。

Conclusion: 该工作通过将SILVER转换为可扩展且行为感知的框架，推进了解释性RL，从而能够解释高维、多动作环境中的深度RL智能体。

Abstract: Deep reinforcement learning (RL) achieves remarkable performance but lacks
interpretability, limiting trust in policy behavior. The existing SILVER
framework (Li, Siddique, and Cao 2025) explains RL policy via Shapley-based
regression but remains restricted to low-dimensional, binary-action domains. We
propose SILVER with RL-guided labeling, an enhanced variant that extends SILVER
to multi-action and high-dimensional environments by incorporating the RL
policy's own action outputs into the boundary points identification. Our method
first extracts compact feature representations from image observations,
performs SHAP-based feature attribution, and then employs RL-guided labeling to
generate behaviorally consistent boundary datasets. Surrogate models, such as
decision trees and regression-based functions, are subsequently trained to
interpret RL policy's decision structure. We evaluate the proposed framework on
two Atari environments using three deep RL algorithms and conduct human-subject
study to assess the clarity and trustworthiness of the derived interpretable
policy. Results show that our approach maintains competitive task performance
while substantially improving transparency and human understanding of agent
behavior. This work advances explainable RL by transforming SILVER into a
scalable and behavior-aware framework for interpreting deep RL agents in
high-dimensional, multi-action settings.

</details>


### [172] [Mixing Configurations for Downstream Prediction](https://arxiv.org/abs/2510.19248)
*Juntang Wang,Hao Wu,Runkun Guo,Yihan Wang,Dongmian Zou,Shixin Xu*

Main category: cs.LG

TL;DR: 本文提出了一种名为GraMixC的即插即用模块，用于提取和融合Vision Transformers中的分层聚类结构，并在多个任务上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有聚类算法旨在模仿人类按相似性对物体进行分组的能力。Transformer中的register tokens存在冗余，且选择具有随意性。

Method: 提出了GraMixC模块，该模块提取配置，使用Reverse Merge/Split (RMS)技术对齐配置，并通过注意力头融合它们。

Result: 在DSN1 16S rRNA cultivation-media预测任务中，GraMixC将R2评分从0.6提高到0.9。在标准表格基准测试中，GraMixC始终优于单分辨率和静态特征基线。

Conclusion: GraMixC模块能够有效地提取和融合Vision Transformers中的分层聚类结构，并在多个任务上取得了state-of-the-art的结果。

Abstract: Humans possess an innate ability to group objects by similarity, a cognitive
mechanism that clustering algorithms aim to emulate. Recent advances in
community detection have enabled the discovery of configurations -- valid
hierarchical clusterings across multiple resolution scales -- without requiring
labeled data. In this paper, we formally characterize these configurations and
identify similar emergent structures in register tokens within Vision
Transformers. Unlike register tokens, configurations exhibit lower redundancy
and eliminate the need for ad hoc selection. They can be learned through
unsupervised or self-supervised methods, yet their selection or composition
remains specific to the downstream task and input. Building on these insights,
we introduce GraMixC, a plug-and-play module that extracts configurations,
aligns them using our Reverse Merge/Split (RMS) technique, and fuses them via
attention heads before forwarding them to any downstream predictor. On the DSN1
16S rRNA cultivation-media prediction task, GraMixC improves the R2 score from
0.6 to 0.9 across multiple methods, setting a new state of the art. We further
validate GraMixC on standard tabular benchmarks, where it consistently
outperforms single-resolution and static-feature baselines.

</details>


### [173] [FnRGNN: Distribution-aware Fairness in Graph Neural Network](https://arxiv.org/abs/2510.19257)
*Soyoung Park,Sungsu Lim*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为FnRGNN的框架，用于解决图神经网络在节点回归任务中的公平性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要针对分类和表示层面的去偏见，不能完全解决节点层面回归的连续性问题。

Method: 该框架在三个层面进行干预：结构层面的边缘重加权、表示层面的MMD对齐和预测层面的Sinkhorn分布匹配。

Result: 在四个真实世界数据集上的实验表明，FnRGNN在不牺牲性能的情况下降低了群体差异。

Conclusion: FnRGNN是一种有效的GNN节点回归公平性框架，它通过多层策略确保复杂图拓扑下的鲁棒公平性。

Abstract: Graph Neural Networks (GNNs) excel at learning from structured data, yet
fairness in regression tasks remains underexplored. Existing approaches mainly
target classification and representation-level debiasing, which cannot fully
address the continuous nature of node-level regression. We propose FnRGNN, a
fairness-aware in-processing framework for GNN-based node regression that
applies interventions at three levels: (i) structure-level edge reweighting,
(ii) representation-level alignment via MMD, and (iii) prediction-level
normalization through Sinkhorn-based distribution matching. This multi-level
strategy ensures robust fairness under complex graph topologies. Experiments on
four real-world datasets demonstrate that FnRGNN reduces group disparities
without sacrificing performance. Code is available at
https://github.com/sybeam27/FnRGNN.

</details>


### [174] [Data Efficient Any Transformer-to-Mamba Distillation via Attention Bridge](https://arxiv.org/abs/2510.19266)
*Penghao Wang,Yuhao Zhou,Mengxuan Wu,Panpan Zhang,Zhangyang Wang,Kai Wang*

Main category: cs.LG

TL;DR: 提出了一种新颖的数据高效蒸馏框架，通过注意力桥（CAB）有效地将Transformer教师模型的注意力知识转移到状态空间学生模型。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型（SSM）作为序列建模的有效替代方案已经出现，通过循环结构提供卓越的可扩展性。然而，它们的训练仍然昂贵，并且它们周围的生态系统远不如Transformer成熟。此外，SSM和Transformer之间的结构异质性使得有效地从预训练的注意力模型中提取知识具有挑战性。

Method: 提出了Cross-architecture distillation via Attention Bridge (CAB)，这是一种新颖的数据高效蒸馏框架，它通过轻量级桥和灵活的层间对齐实现token级别的监督，从而提高效率和可转移性。我们进一步引入了灵活的层间对齐策略，以适应教师和学生之间的架构差异。

Result: 在视觉和语言领域的广泛实验表明，即使在有限的训练数据下，我们的方法也能持续提高状态空间模型的性能，优于标准和跨架构蒸馏方法。

Conclusion: 我们的研究结果表明，基于注意力的知识可以有效地转移到循环模型，从而能够快速利用Transformer专业知识来构建更强大的SSM社区。

Abstract: State-space models (SSMs) have emerged as efficient alternatives to
Transformers for sequence modeling, offering superior scalability through
recurrent structures. However, their training remains costly and the ecosystem
around them is far less mature than that of Transformers. Moreover, the
structural heterogeneity between SSMs and Transformers makes it challenging to
efficiently distill knowledge from pretrained attention models. In this work,
we propose Cross-architecture distillation via Attention Bridge (CAB), a novel
data-efficient distillation framework that efficiently transfers attention
knowledge from Transformer teachers to state-space student models. Unlike
conventional knowledge distillation that transfers knowledge only at the output
level, CAB enables token-level supervision via a lightweight bridge and
flexible layer-wise alignment, improving both efficiency and transferability.
We further introduce flexible layer-wise alignment strategies to accommodate
architectural discrepancies between teacher and student. Extensive experiments
across vision and language domains demonstrate that our method consistently
improves the performance of state-space models, even under limited training
data, outperforming both standard and cross-architecture distillation methods.
Our findings suggest that attention-based knowledge can be efficiently
transferred to recurrent models, enabling rapid utilization of Transformer
expertise for building a stronger SSM community.

</details>


### [175] [Knowledge Distillation of Uncertainty using Deep Latent Factor Model](https://arxiv.org/abs/2510.19290)
*Sehyun Park,Jongjin Lee,Yunseop Shin,Ilsang Ohn,Yongdai Kim*

Main category: cs.LG

TL;DR: 提出了一种新的分布蒸馏方法，称为高斯蒸馏，它通过一个称为深度潜在因子模型（DLF）的特殊高斯过程来估计教师集成的分布。


<details>
  <summary>Details</summary>
Motivation: 深度集成提供了最先进的、可靠的不确定性量化，但其繁重的计算和内存需求阻碍了它们在实际应用中的部署，例如设备上人工智能。知识蒸馏将一个集成压缩成小的学生模型，但现有的技术难以保持不确定性，部分原因是减小DNNs的尺寸通常会导致变异减少。

Method: 使用高斯过程（称为深度潜在因子模型（DLF））估计教师集成的分布，并通过期望最大化（EM）算法稳定地估计DLF模型中的均值和协方差函数。

Result: 在多个基准数据集上证明了所提出的高斯蒸馏优于现有的基线。

Conclusion: 高斯蒸馏适用于语言模型的微调和分布偏移问题

Abstract: Deep ensembles deliver state-of-the-art, reliable uncertainty quantification,
but their heavy computational and memory requirements hinder their practical
deployments to real applications such as on-device AI. Knowledge distillation
compresses an ensemble into small student models, but existing techniques
struggle to preserve uncertainty partly because reducing the size of DNNs
typically results in variation reduction. To resolve this limitation, we
introduce a new method of distribution distillation (i.e. compressing a teacher
ensemble into a student distribution instead of a student ensemble) called
Gaussian distillation, which estimates the distribution of a teacher ensemble
through a special Gaussian process called the deep latent factor model (DLF) by
treating each member of the teacher ensemble as a realization of a certain
stochastic process. The mean and covariance functions in the DLF model are
estimated stably by using the expectation-maximization (EM) algorithm. By using
multiple benchmark datasets, we demonstrate that the proposed Gaussian
distillation outperforms existing baselines. In addition, we illustrate that
Gaussian distillation works well for fine-tuning of language models and
distribution shift problems.

</details>


### [176] [QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation](https://arxiv.org/abs/2510.19296)
*Yang Zhang,Rui Zhang,Jiaming Guo,Lei Huang,Di Huang,Yunpu Zhao,Shuyao Cheng,Pengwei Jin,Chongxiao Li,Zidong Du,Xing Hu,Qi Guo,Yunji Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为 QiMeng-SALV 的信号感知学习方法，用于解决 Verilog 代码生成中功能性奖励不足的问题，通过利用功能正确的输出信号的代码段来优化强化学习训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在 Verilog 代码生成方面有很大的潜力，但缺乏有意义的功能性奖励阻碍了基于强化学习的优化。

Method: 该方法通过比较生成模块和参考模块中的信号功能正确性，提取信号感知的代码段，并使用抽象语法树识别这些代码段，从而提供有意义的功能性奖励。引入了信号感知的 DPO，在正确的信号级别代码段上进行优化。

Result: 实验表明，该方法在 VerilogEval 和 RTLLM 上取得了最先进的性能，一个 7B 参数的模型与 DeepSeek v3 671B 模型的性能相匹配，并且显著优于领先的开源模型 CodeV。

Conclusion: 提出的 QiMeng-SALV 强调了从传统的模块级到细粒度信号级优化的转变，解决了 Verilog 代码生成中功能性奖励不足的问题。

Abstract: The remarkable progress of Large Language Models (LLMs) presents promising
opportunities for Verilog code generation which is significantly important for
automated circuit design. The lacking of meaningful functional rewards hinders
the preference optimization based on Reinforcement Learning (RL) for producing
functionally correct Verilog code. In this paper, we propose Signal-Aware
Learning for Verilog code generation (QiMeng-SALV) by leveraging code segments
of functionally correct output signal to optimize RL training. Considering
Verilog code specifies the structural interconnection of hardware gates and
wires so that different output signals are independent, the key insight of
QiMeng-SALV is to extract verified signal-aware implementations in partially
incorrect modules, so as to enhance the extraction of meaningful functional
rewards. Roughly, we verify the functional correctness of signals in generated
module by comparing with that of reference module in the training data. Then
abstract syntax tree (AST) is employed to identify signal-aware code segments
which can provide meaningful functional rewards from erroneous modules.
Finally, we introduce signal-aware DPO which is optimized on the correct
signal-level code segments, thereby preventing noise and interference from
incorrect signals. The proposed QiMeng-SALV underscores the paradigm shift from
conventional module-level to fine-grained signal-level optimization in Verilog
code generation, addressing the issue of insufficient functional rewards.
Experiments demonstrate that our method achieves state-of-the-art performance
on VerilogEval and RTLLM, with a 7B parameter model matching the performance of
the DeepSeek v3 671B model and significantly outperforming the leading
open-source model CodeV trained on the same dataset. Our code is available at
https://github.com/zy1xxx/SALV.

</details>


### [177] [Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall](https://arxiv.org/abs/2510.19304)
*Mingyu Jo,Jaesik Yoon,Justin Deschenaux,Caglar Gulcehre,Sungjin Ahn*

Main category: cs.LG

TL;DR: 离散扩散模型通过并行解码提供了一种有前景的自回归生成替代方案，但它们受到采样壁垒的限制：一旦发生分类采样，丰富的分布信息就会崩溃为one-hot向量，并且无法跨步骤传播，迫使后续步骤以有限的信息进行操作。为了缓解这个问题，我们引入了Loopholing，这是一种新颖而简单的机制，可以通过确定性潜在路径保留这些信息，从而产生Loopholing离散扩散模型（LDDM）。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型面临采样壁垒问题，限制了其性能。

Method: 提出了一种名为Loopholing的机制，通过确定性潜在路径保留信息。

Result: LDDM在生成困惑度上实现了显着提升，最多比先前的基线降低了61%，缩小了与自回归模型的差距（在某些情况下甚至超过了自回归模型），并产生了更连贯的文本。在推理任务中，LDDM还提高了算术基准（如Countdown和Game of 24）的性能。

Conclusion: Loopholing缓解了空闲步骤和振荡，为实现高质量的非自回归文本生成提供了一条可扩展的路径。

Abstract: Discrete diffusion models offer a promising alternative to autoregressive
generation through parallel decoding, but they suffer from a sampling wall:
once categorical sampling occurs, rich distributional information collapses
into one-hot vectors and cannot be propagated across steps, forcing subsequent
steps to operate with limited information. To mitigate this problem, we
introduce Loopholing, a novel and simple mechanism that preserves this
information via a deterministic latent pathway, leading to Loopholing Discrete
Diffusion Models (LDDMs). Trained efficiently with a self-conditioning
strategy, LDDMs achieve substantial gains-reducing generative perplexity by up
to 61% over prior baselines, closing (and in some cases surpassing) the gap
with autoregressive models, and producing more coherent text. Applied to
reasoning tasks, LDDMs also improve performance on arithmetic benchmarks such
as Countdown and Game of 24. These results also indicate that loopholing
mitigates idle steps and oscillations, providing a scalable path toward
high-quality non-autoregressive text generation.

</details>


### [178] [FrogDeepSDM: Improving Frog Counting and Occurrence Prediction Using Multimodal Data and Pseudo-Absence Imputation](https://arxiv.org/abs/2510.19305)
*Chirag Padubidri,Pranesh Velmurugan,Andreas Lanitis,Andreas Kamilaris*

Main category: cs.LG

TL;DR: 本研究利用深度学习和数据插补技术，提高了蛙类物种分布模型（SDM）的准确性。


<details>
  <summary>Details</summary>
Motivation: 监测物种分布对于评估环境影响和制定有效的保护策略至关重要。传统数据收集方法在覆盖范围和完整性方面存在局限性。

Method: 应用深度学习和数据插补技术，并结合“EY - 2022 生物多样性挑战赛”的数据，优化SDM模型。

Result: 数据平衡显著提高了模型性能，蛙类计数任务的平均绝对误差（MAE）从189降低到29。多模态集成模型优于单一模型，并在未见区域表现出稳健的泛化能力。图像和表格数据的融合提高了蛙类计数和栖息地分类的准确性，达到了84.9%的准确率和0.90的AUC。

Conclusion: 多模态学习和数据预处理技术（如平衡和插补）可以提高预测生态建模的准确性，从而有助于更精确和可扩展的生物多样性监测。

Abstract: Monitoring species distribution is vital for conservation efforts, enabling
the assessment of environmental impacts and the development of effective
preservation strategies. Traditional data collection methods, including citizen
science, offer valuable insights but remain limited in coverage and
completeness. Species Distribution Modelling (SDM) helps address these gaps by
using occurrence data and environmental variables to predict species presence
across large regions. In this study, we enhance SDM accuracy for frogs (Anura)
by applying deep learning and data imputation techniques using data from the
"EY - 2022 Biodiversity Challenge." Our experiments show that data balancing
significantly improved model performance, reducing the Mean Absolute Error
(MAE) from 189 to 29 in frog counting tasks. Feature selection identified key
environmental factors influencing occurrence, optimizing inputs while
maintaining predictive accuracy. The multimodal ensemble model, integrating
land cover, NDVI, and other environmental inputs, outperformed individual
models and showed robust generalization across unseen regions. The fusion of
image and tabular data improved both frog counting and habitat classification,
achieving 84.9% accuracy with an AUC of 0.90. This study highlights the
potential of multimodal learning and data preprocessing techniques such as
balancing and imputation to improve predictive ecological modeling when data
are sparse or incomplete, contributing to more precise and scalable
biodiversity monitoring.

</details>


### [179] [Calibration and Discrimination Optimization Using Clusters of Learned Representation](https://arxiv.org/abs/2510.19328)
*Tomer Lavi,Bracha Shapira,Nadav Rappoport*

Main category: cs.LG

TL;DR: 提出了一种新的校准流程，该流程利用在输入样本的学习表示的集群上训练的校准函数集成来增强整体校准。


<details>
  <summary>Details</summary>
Motivation: 校准对于临床预测等关键决策至关重要，但通常受到较少的关注。

Method: 利用在输入样本的学习表示的集群上训练的校准函数集成。

Result: 将各种方法的校准分数从 82.28% 提高到 100%，并引入了一种独特的匹配指标，确保模型选择优化区分和校准。

Conclusion: 该方案是通用的，可以适应任何底层表示、聚类、校准方法和指标，从而在常用的校准方法中提供灵活性和卓越的性能。

Abstract: Machine learning models are essential for decision-making and risk
assessment, requiring highly reliable predictions in terms of both
discrimination and calibration. While calibration often receives less
attention, it is crucial for critical decisions, such as those in clinical
predictions. We introduce a novel calibration pipeline that leverages an
ensemble of calibration functions trained on clusters of learned
representations of the input samples to enhance overall calibration. This
approach not only improves the calibration score of various methods from 82.28%
up to 100% but also introduces a unique matching metric that ensures model
selection optimizes both discrimination and calibration. Our generic scheme
adapts to any underlying representation, clustering, calibration methods and
metric, offering flexibility and superior performance across commonly used
calibration methods.

</details>


### [180] [Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning](https://arxiv.org/abs/2510.19338)
*Ling Team,Bin Han,Caizhi Tang,Chen Liang,Donghao Zhang,Fan Yuan,Feng Zhu,Jie Gao,Jingyu Hu,Longfei Li,Meng Li,Mingyang Zhang,Peijie Jiang,Peng Jiao,Qian Zhao,Qingyuan Yang,Wenbo Shen,Xinxing Yang,Yalin Zhang,Yankun Ren,Yao Zhao,Yibo Cao,Yixuan Sun,Yue Zhang,Yuchen Fang,Zibin Lin,Zixuan Cheng,Jun Zhou*

Main category: cs.LG

TL;DR: Ring-linear模型系列，包括Ring-mini-linear-2.0和Ring-flash-linear-2.0，通过混合线性注意力和softmax注意力，显著降低长文本推理的I/O和计算开销。


<details>
  <summary>Details</summary>
Motivation: 为了降低长文本推理的计算和I/O开销。

Method: 采用混合架构，结合线性注意力和softmax注意力，并探索不同注意力机制的比例，利用自研的FP8算子库linghe。

Result: 相比32B参数的稠密模型，推理成本降低到1/10；相比原始Ring系列，成本降低超过50%；训练效率提高50%。

Conclusion: 该模型系列在多个复杂推理基准测试中保持SOTA性能，且在强化学习阶段可以进行长期、稳定和高效的优化。

Abstract: In this technical report, we present the Ring-linear model series,
specifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0.
Ring-mini-linear-2.0 comprises 16B parameters and 957M activations, while
Ring-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both
models adopt a hybrid architecture that effectively integrates linear attention
and softmax attention, significantly reducing I/O and computational overhead in
long-context inference scenarios. Compared to a 32 billion parameter dense
model, this series reduces inference cost to 1/10, and compared to the original
Ring series, the cost is also reduced by over 50%. Furthermore, through
systematic exploration of the ratio between different attention mechanisms in
the hybrid architecture, we have identified the currently optimal model
structure. Additionally, by leveraging our self-developed high-performance FP8
operator library-linghe, overall training efficiency has been improved by 50%.
Benefiting from the high alignment between the training and inference engine
operators, the models can undergo long-term, stable, and highly efficient
optimization during the reinforcement learning phase, consistently maintaining
SOTA performance across multiple challenging complex reasoning benchmarks.

</details>


### [181] [Foundation Model Forecasts: Form and Function](https://arxiv.org/abs/2510.19345)
*Alvaro Perez-Diaz,James C. Loach,Danielle E. Toutoungi,Lee Middleton*

Main category: cs.LG

TL;DR: 时间序列基础模型(TSFM)在预测准确性方面表现出色，但准确性并不完全决定实际价值。预测的形式——点、分位数、参数或轨迹集成——从根本上限制了它能支持的操作任务。


<details>
  <summary>Details</summary>
Motivation: 调查表明，虽然许多操作任务需要保持时间依赖性的轨迹集成，但最近的三分之二的TSFM只产生点或参数预测。

Method: 该研究确定了预测类型何时可以转换，何时不能：轨迹集成可以通过边缘化转换为更简单的形式，而不需要额外的假设，但反过来需要通过copulas或conformal方法来施加时间依赖性。证明了边际不能决定路径依赖事件的概率。

Result: 证明了边际不能决定路径依赖事件的概率——无限多的联合分布共享相同的边际，但对操作问题的回答不同。

Conclusion: 分析阐明了预测类型（而非准确性）何时区分实际效用。

Abstract: Time-series foundation models (TSFMs) achieve strong forecast accuracy, yet
accuracy alone does not determine practical value. The form of a forecast --
point, quantile, parametric, or trajectory ensemble -- fundamentally constrains
which operational tasks it can support. We survey recent TSFMs and find that
two-thirds produce only point or parametric forecasts, while many operational
tasks require trajectory ensembles that preserve temporal dependence. We
establish when forecast types can be converted and when they cannot: trajectory
ensembles convert to simpler forms via marginalization without additional
assumptions, but the reverse requires imposing temporal dependence through
copulas or conformal methods. We prove that marginals cannot determine
path-dependent event probabilities -- infinitely many joint distributions share
identical marginals but yield different answers to operational questions. We
map six fundamental forecasting tasks to minimal sufficient forecast types and
provide a task-aligned evaluation framework. Our analysis clarifies when
forecast type, not accuracy, differentiates practical utility.

</details>


### [182] [A New Type of Adversarial Examples](https://arxiv.org/abs/2510.19347)
*Xingyang Nie,Guojie Xiao,Su Pan,Biao Wang,Huilin Ge,Tao Fang*

Main category: cs.LG

TL;DR: 本文提出了一种生成对抗样本的新方法，生成的对抗样本与原始样本差异很大，但产生相同的结果。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型容易受到对抗样本的攻击，这引发了对这些模型安全性的担忧。对抗样本是通过对数据集中的样本进行细微但有意的最坏情况修改来制作的，导致模型输出与原始样本不同的答案。

Method: 本文提出了一组新的算法来生成这种对抗样本，包括负迭代快速梯度符号方法 (NI-FGSM) 和负迭代快速梯度方法 (NI-FGM)，以及它们的动量变体：负动量迭代快速梯度符号方法 (NMI-FGSM) 和负动量迭代快速梯度方法 (NMI-FGM)。

Result: 通过这些方法构建的对抗样本可以在某些情况下用于攻击机器学习系统。此外，我们的结果表明，对抗样本不仅仅分布在数据集样本的附近；相反，它们广泛分布在样本空间中。

Conclusion: 对抗样本不仅仅分布在数据集样本的附近；相反，它们广泛分布在样本空间中。

Abstract: Most machine learning models are vulnerable to adversarial examples, which
poses security concerns on these models. Adversarial examples are crafted by
applying subtle but intentionally worst-case modifications to examples from the
dataset, leading the model to output a different answer from the original
example. In this paper, adversarial examples are formed in an exactly opposite
manner, which are significantly different from the original examples but result
in the same answer. We propose a novel set of algorithms to produce such
adversarial examples, including the negative iterative fast gradient sign
method (NI-FGSM) and the negative iterative fast gradient method (NI-FGM),
along with their momentum variants: the negative momentum iterative fast
gradient sign method (NMI-FGSM) and the negative momentum iterative fast
gradient method (NMI-FGM). Adversarial examples constructed by these methods
could be used to perform an attack on machine learning systems in certain
occasions. Moreover, our results show that the adversarial examples are not
merely distributed in the neighbourhood of the examples from the dataset;
instead, they are distributed extensively in the sample space.

</details>


### [183] [A Markov Decision Process for Variable Selection in Branch & Bound](https://arxiv.org/abs/2510.19348)
*Paul Strang,Zacharie Alès,Côme Bissuel,Olivier Juan,Safia Kedad-Sidhoum,Emmanuel Rachelson*

Main category: cs.LG

TL;DR: 提出了一种新的基于马尔可夫决策过程（MDP）的用于分支定界（B&B）中变量选择的强化学习（RL）方法，称为BBMDP。


<details>
  <summary>Details</summary>
Motivation: 现有研究试图通过RL算法学习最优分支策略，但依赖于特定的MDP公式和收敛定理。

Method: 提出了一个通用的MDP公式，允许使用各种RL算法来学习最优B&B启发式算法。

Result: 实验结果表明，所提出的分支代理在四个标准MILP基准测试中优于现有最先进的RL代理。

Conclusion: BBMDP模型在变量选择方面表现出色，验证了其有效性。

Abstract: Mixed-Integer Linear Programming (MILP) is a powerful framework used to
address a wide range of NP-hard combinatorial optimization problems, often
solved by Branch and Bound (B&B). A key factor influencing the performance of
B&B solvers is the variable selection heuristic governing branching decisions.
Recent contributions have sought to adapt reinforcement learning (RL)
algorithms to the B&B setting to learn optimal branching policies, through
Markov Decision Processes (MDP) inspired formulations, and ad hoc convergence
theorems and algorithms. In this work, we introduce BBMDP, a principled vanilla
MDP formulation for variable selection in B&B, allowing to leverage a broad
range of RL algorithms for the purpose of learning optimal B\&B heuristics.
Computational experiments validate our model empirically, as our branching
agent outperforms prior state-of-the-art RL agents on four standard MILP
benchmarks.

</details>


### [184] [Scalable LinUCB: Low-Rank Design Matrix Updates for Recommenders with Large Action Spaces](https://arxiv.org/abs/2510.19349)
*Evgenia Shustova,Marina Sheshukova,Sergey Samsonov,Evgeny Frolov*

Main category: cs.LG

TL;DR: 提出了一种可扩展的LinUCB算法，通过动态低秩参数化逆Cholesky风格因子，实现了快速和内存高效的逆正则化设计矩阵运算。


<details>
  <summary>Details</summary>
Motivation: 线性上下文Bandit算法，特别是LinUCB，被广泛应用于推荐系统中。然而，其训练、推理和内存成本随着特征维度和动作空间的大小而增长。关键瓶颈在于需要更新、反转和存储一个吸收交互历史上下文信息的设计矩阵。

Method: 通过动态低秩参数化逆Cholesky风格因子，实现了快速和内存高效的逆正则化设计矩阵运算。采用了投影-分裂积分器进行动态低秩逼近，平均每步更新成本为O(dr)，内存为O(dr)。

Result: 在推荐系统数据集上的实验表明了该算法的有效性。

Conclusion: 该算法在推荐系统数据集上表现出有效性，降低了计算和存储成本。

Abstract: Linear contextual bandits, especially LinUCB, are widely used in recommender
systems. However, its training, inference, and memory costs grow with feature
dimensionality and the size of the action space. The key bottleneck becomes the
need to update, invert and store a design matrix that absorbs contextual
information from interaction history. In this paper, we introduce Scalable
LinUCB, the algorithm that enables fast and memory efficient operations with
the inverse regularized design matrix. We achieve this through a dynamical
low-rank parametrization of its inverse Cholesky-style factors. We derive
numerically stable rank-1 and batched updates that maintain the inverse without
directly forming the entire matrix. To control memory growth, we employ a
projector-splitting integrator for dynamical low-rank approximation, yielding
average per-step update cost $O(dr)$ and memory $O(dr)$ for approximation rank
$r$. Inference complexity of the suggested algorithm is $O(dr)$ per action
evaluation. Experiments on recommender system datasets demonstrate the
effectiveness of our algorithm.

</details>


### [185] [ConvXformer: Differentially Private Hybrid ConvNeXt-Transformer for Inertial Navigation](https://arxiv.org/abs/2510.19352)
*Omer Tariq,Muhammad Bilal,Muneeb Ul Hassan,Dongsoo Han,Jon Crowcroft*

Main category: cs.LG

TL;DR: 本文提出了一种名为ConvXformer的混合架构，用于鲁棒的惯性导航，并提出了一种高效的差分隐私机制，以保护敏感信息并确保模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私解决方案通常会通过引入过多的噪声来降低模型性能，尤其是在高频惯性测量中。深度学习的惯性跟踪系统容易受到隐私泄露的影响，从而暴露敏感的训练数据。

Method: 本文提出了一种混合架构ConvXformer，它将ConvNeXt块与Transformer编码器融合在一个分层结构中，用于鲁棒的惯性导航。提出了一种有效的差分隐私机制，该机制结合了自适应梯度裁剪和梯度对齐噪声注入(GANI)，以保护敏感信息，同时确保模型性能。该框架利用截断奇异值分解进行梯度处理，从而能够精确控制隐私-效用权衡。

Result: 在基准数据集(OxIOD、RIDI、RoNIN)上的综合性能评估表明，ConvXformer超越了最先进的方法，在确保$(\epsilon,\delta)$-差分隐私保证的同时，定位精度提高了40%以上。引入了Mech-IO数据集，该数据集来自韩国科学技术院的机械工程大楼，那里的工业设备产生的强磁场会引起显著的传感器扰动。在严重环境畸变下证明了鲁棒性。

Conclusion: 该框架非常适合在信息物理系统中进行安全和智能导航。

Abstract: Data-driven inertial sequence learning has revolutionized navigation in
GPS-denied environments, offering superior odometric resolution compared to
traditional Bayesian methods. However, deep learning-based inertial tracking
systems remain vulnerable to privacy breaches that can expose sensitive
training data. \hl{Existing differential privacy solutions often compromise
model performance by introducing excessive noise, particularly in
high-frequency inertial measurements.} In this article, we propose ConvXformer,
a hybrid architecture that fuses ConvNeXt blocks with Transformer encoders in a
hierarchical structure for robust inertial navigation. We propose an efficient
differential privacy mechanism incorporating adaptive gradient clipping and
gradient-aligned noise injection (GANI) to protect sensitive information while
ensuring model performance. Our framework leverages truncated singular value
decomposition for gradient processing, enabling precise control over the
privacy-utility trade-off. Comprehensive performance evaluations on benchmark
datasets (OxIOD, RIDI, RoNIN) demonstrate that ConvXformer surpasses
state-of-the-art methods, achieving more than 40% improvement in positioning
accuracy while ensuring $(\epsilon,\delta)$-differential privacy guarantees. To
validate real-world performance, we introduce the Mech-IO dataset, collected
from the mechanical engineering building at KAIST, where intense magnetic
fields from industrial equipment induce significant sensor perturbations. This
demonstrated robustness under severe environmental distortions makes our
framework well-suited for secure and intelligent navigation in cyber-physical
systems.

</details>


### [186] [Optimization Benchmark for Diffusion Models on Dynamical Systems](https://arxiv.org/abs/2510.19376)
*Fabian Schaipp*

Main category: cs.LG

TL;DR: 论文评估了优化技术在训练扩散模型中的应用，发现Muon和SOAP是AdamW的高效替代品。


<details>
  <summary>Details</summary>
Motivation: 研究动机是优化技术在扩散模型训练中的评估常常缺失。

Method: 通过基准测试比较了不同优化算法在训练扩散模型去噪流量轨迹方面的性能。

Result: Muon和SOAP的最终损失比AdamW低18%。

Conclusion: Muon和SOAP是训练扩散模型的高效优化器选择。

Abstract: The training of diffusion models is often absent in the evaluation of new
optimization techniques. In this work, we benchmark recent optimization
algorithms for training a diffusion model for denoising flow trajectories. We
observe that Muon and SOAP are highly efficient alternatives to AdamW (18%
lower final loss). We also revisit several recent phenomena related to the
training of models for text or image applications in the context of diffusion
model training. This includes the impact of the learning-rate schedule on the
training dynamics, and the performance gap between Adam and SGD.

</details>


### [187] [LMFD: Latent Monotonic Feature Discovery](https://arxiv.org/abs/2510.19383)
*Guus Toussaint,Arno Knobbe*

Main category: cs.LG

TL;DR: 本文研究如何从多变量时间序列中提取潜在的“年龄”代理，即使传感器没有直接提供这些信息。


<details>
  <summary>Details</summary>
Motivation: 许多系统会老化、退化或缓慢但稳定地朝着某个方向发展，因此需要从传感器数据中提取潜在的“年龄”信息。

Method: 该方法使用精心定义的语法，并根据单调性优化生成的公式，生成一组候选特征，然后对这些特征进行拟合和评估。

Result: 在人工生成的数据集和两个真实世界的数据集上进行了评估，结果表明该系统能够将具有低单调性的传感器组合成具有高单调性的潜在特征。在 InfraWatch 的真实世界数据集中，两个绝对 Spearman's ρ 值为 0.13 和 0.09 的特征可以组合成一个绝对 Spearman's ρ 值为 0.95 的代理。

Conclusion: 该方法可以找到可解释的公式，作为系统“年龄”的代理。

Abstract: Many systems in our world age, degrade or otherwise move slowly but steadily
in a certain direction. When monitoring such systems by means of sensors, one
often assumes that some form of `age' is latently present in the data, but
perhaps the available sensors do not readily provide this useful information.
The task that we study in this paper is to extract potential proxies for this
`age' from the available multi-variate time series without having clear data on
what `age' actually is. We argue that when we find a sensor, or more likely
some discovered function of the available sensors, that is sufficiently
monotonic, that function can act as the proxy we are searching for. Using a
carefully defined grammar and optimising the resulting equations in terms of
monotonicity, defined as the absolute Spearman's Rank Correlation between time
and the candidate formula, the proposed approach generates a set of candidate
features which are then fitted and assessed on monotonicity. The proposed
system is evaluated against an artificially generated dataset and two
real-world datasets. In all experiments, we show that the system is able to
combine sensors with low individual monotonicity into latent features with high
monotonicity. For the real-world dataset of InfraWatch, a structural health
monitoring project, we show that two features with individual absolute
Spearman's $\rho$ values of $0.13$ and $0.09$ can be combined into a proxy with
an absolute Spearman's $\rho$ of $0.95$. This demonstrates that our proposed
method can find interpretable equations which can serve as a proxy for the
`age' of the system.

</details>


### [188] [Learning Noise-Resilient and Transferable Graph-Text Alignment via Dynamic Quality Assessment](https://arxiv.org/abs/2510.19384)
*Yuhang Liu,Minglai Shao,Zengyi Wo,Yunlong Chu,Bing Hao,Shengzhong Liu,Ruijie Wang,Jianxin Li*

Main category: cs.LG

TL;DR: This paper introduces ADAligner, a new graph-text alignment framework for pre-training Graph Foundation Models (GFMs) on text-attributed graphs (TAGs).


<details>
  <summary>Details</summary>
Motivation: Existing CLIP-style graph-text aligners have limitations because they assume one-to-one correspondences between nodes and texts, and they rely on static alignment objectives that cannot adapt to varying data quality.

Method: ADAligner dynamically adjusts between expressive many-to-many and conservative one-to-one objectives according to supervision quality. It estimates batch-level alignment reliability in real time and adapts its optimization accordingly.

Result: ADAligner outperforms prior graph-text aligners on zero-/few-shot node classification, link prediction and cross-modal retrieval tasks. It maintains strong robustness under noisy supervision and accelerates pre-training by approximately 2 to 3 times compared to multimodal baselines.

Conclusion: ADAligner establishes a scalable and reliable foundation for graph-text representation learning in real-world web environments.

Abstract: Pre-training Graph Foundation Models (GFMs) on text-attributed graphs (TAGs)
is central to web-scale applications such as search, recommendation, and
knowledge discovery. However, existing CLIP-style graph-text aligners face two
key limitations: they assume strict one-to-one correspondences between nodes
and texts, overlooking the inherent many-to-many relations in real-world
graphs; and they rely on static alignment objectives that cannot adapt to
varying data quality, making them brittle under noisy supervision. Together,
these limitations expose a core dilemma: embracing expressive many-to-many
alignment amplifies noise, while reverting to strict one-to-one strategies
sacrifices semantic diversity and fails to handle inherently mismatched pairs.
To address these challenges, we propose ADAligner, a dynamic, quality-aware
graph-text alignment framework that dynamically adjusts between expressive
many-to-many and conservative one-to-one objectives according to supervision
quality. ADAligner estimates batch-level alignment reliability in real time and
adapts its optimization accordingly, promoting soft, subgraph-level
many-to-many alignment when supervision is clean, while emphasizing reliable
one-to-one alignment by dynamically filtering low-confidence pairs under noise.
Theoretically, we prove that this dynamic mechanism forms a stable negative
feedback process, ensuring convergence and robustness. Comprehensive
experiments on nine diverse TAG datasets demonstrate that ADAligner
consistently outperforms prior graph-text aligners on zero-/few-shot node
classification, link prediction and cross-modal retrieval tasks. It maintains
strong robustness under noisy supervision and accelerates pre-training by
approximately 2 to 3 times compared to multimodal baselines, establishing a
scalable and reliable foundation for graph-text representation learning in
real-world web environments.

</details>
