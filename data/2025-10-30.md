<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 33]
- [cs.CV](#cs.CV) [Total: 32]
- [cs.AI](#cs.AI) [Total: 28]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.LG](#cs.LG) [Total: 32]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Iti-Validator: A Guardrail Framework for Validating and Correcting LLM-Generated Itineraries](https://arxiv.org/abs/2510.24719)
*Shravan Gadbail,Masumi Desai,Kamalakar Karlapalem*

Main category: cs.CL

TL;DR: 这篇论文研究了大型语言模型（LLM）在生成旅行行程时的时序一致性问题，并提出了一个验证框架来评估和改进LLM生成的行程。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成的行程缺乏时序和空间一致性，尤其是在涉及物理旅行约束的情况下。

Method: 使用多个先进的LLM生成旅行计划，并使用AeroDataBox API根据真实世界的航班时长约束验证它们。

Result: 当前的LLM经常生成在时间上不一致的行程，但可以使用该框架系统地、可靠地纠正这些行程。

Conclusion: 该框架可以纠正LLM生成的行程中的任何时间不一致性，使其能够实际部署在大型旅行计划中。

Abstract: The rapid advancement of Large Language Models (LLMs) has enabled them to
generate complex, multi-step plans and itineraries. However, these generated
plans often lack temporal and spatial consistency, particularly in scenarios
involving physical travel constraints. This research aims to study the temporal
performance of different LLMs and presents a validation framework that
evaluates and improves the temporal consistency of LLM-generated travel
itineraries. The system employs multiple state-of-the-art LLMs to generate
travel plans and validates them against real-world flight duration constraints
using the AeroDataBox API. This work contributes to the understanding of LLM
capabilities in handling complex temporal reasoning tasks like itinerary
generation and provides a framework to rectify any temporal inconsistencies
like overlapping journeys or unrealistic transit times in the itineraries
generated by LLMs before the itinerary is given to the user. Our experiments
reveal that while current LLMs frequently produce temporally inconsistent
itineraries, these can be systematically and reliably corrected using our
framework, enabling their practical deployment in large-scale travel planning.

</details>


### [2] [Dingtalk DeepResearch: A Unified Multi Agent Framework for Adaptive Intelligence in Enterprise Environments](https://arxiv.org/abs/2510.24760)
*Mengyuan Chen,Chengjun Dai,Xinyang Dong,Chengzhe Feng,Kewei Fu,Jianshe Li,Zhihan Peng,Yongqi Tong,Junshao Zhang,Hong Zhu*

Main category: cs.CL

TL;DR: 提出了Dingtalk DeepResearch，一个统一的多智能体智能框架，用于真实世界的企业环境。


<details>
  <summary>Details</summary>
Motivation: 为企业环境提供深度研究能力

Method: 统一的多智能体智能框架

Result: 实现深度研究、异构表格推理和多模态报告生成

Conclusion: 该框架能够有效支持企业级应用

Abstract: We present Dingtalk DeepResearch, a unified multi agent intelligence
framework for real world enterprise environments, delivering deep research,
heterogeneous table reasoning, and multimodal report generation.

</details>


### [3] [Falcon: A Comprehensive Chinese Text-to-SQL Benchmark for Enterprise-Grade Evaluation](https://arxiv.org/abs/2510.24762)
*Wenzhen Luo,Wei Guan,Yifan Yao,Yimin Pan,Feng Wang,Zhipeng Yu,Zhe Wen,Liang Chen,Yihong Zhuang*

Main category: cs.CL

TL;DR: Falcon是一个新的跨领域中文文本到SQL的基准，它基于企业兼容的方言(MaxCompute/Hive)，包含600个问题，覆盖28个数据库。它主要针对大型企业环境下的模式链接和将简洁的口语化中文映射到SQL查询。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到SQL模型在处理复杂的企业级中文数据时表现不佳，尤其是在模式链接和语义转换方面。

Method: 提出了Falcon基准，包含各种SQL计算特征和中文语义的注释，以及一个强大的执行比较器和自动化评估流程。

Result: 当前最先进的大规模模型在Falcon上的准确率最多只有50%，主要错误来自模式链接和中文语义映射。

Conclusion: Falcon提供了一个可复现的中间地带，通过使用真实的企业模式、查询模板、执行比较器和自动化评估流程进行端到端验证，从而在全面生产部署之前进行测试。

Abstract: We introduce Falcon, a cross-domain Chinese text-to-SQL benchmark grounded in
an enterprise-compatible dialect (MaxCompute/Hive). It contains 600 Chinese
questions over 28 databases; 77% require multi-table reasoning and over half
touch more than four tables. Each example is annotated along SQL-computation
features and Chinese semantics. For evaluation, we release a robust execution
comparator and an automated evaluation pipeline, under which all current
state-of-the-art large-scale models (including Deepseek) achieve accuracies of
at most 50%. Major errors originate from two sources: (1) schema linking in
large enterprise landscapes - hundreds of tables, denormalized fields,
ambiguous column names, implicit foreign-key relations and domain-specific
synonyms that make correct join/column selection difficult; and (2) mapping
concise, colloquial Chinese into the exact operators and predicates required
for analytics - e.g., choosing the correct aggregation and group-by keys,
expressing time windows and granularities, applying unit conversions, handling
NULLs and data-quality rules, and formulating nested or windowed subqueries.
Falcon therefore targets Chinese-specific semantics and enterprise dialects
(abbreviations, business jargon, fuzzy entity references) and provides a
reproducible middle ground before full production deployment by using realistic
enterprise schemas, query templates, an execution comparator, and an automated
evaluation pipeline for end-to-end validation.

</details>


### [4] [Confidence is Not Competence](https://arxiv.org/abs/2510.24772)
*Debdeep Sanyal,Manya Pandey,Dhruv Kumar,Saurabh Deshpande,Murari Mandal*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）的自信心与其解决问题的能力之间常常存在脱节。本文通过分析预生成评估和解决方案执行两个阶段的内部状态几何结构，对此现象提供了一种机制解释。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的自信心与其解决问题的能力之间常常存在脱节

Method: 通过分析预生成评估和解决方案执行两个阶段的内部状态几何结构，使用线性探针解码模型的内部“可解性信念”，揭示了一个有序的信念轴。

Result: 发现评估流形具有较高的线性有效维度，而推理轨迹在较低维流形上演化。沿着信念轴引导表征的因果干预不会改变最终的解决方案。

Conclusion: 揭示了一种双系统架构——一个几何复杂的评估器和一个几何简单的执行器。可解码的信念不是可操作的杠杆，干预应针对执行的程序动态，而不是评估的高级几何结构。

Abstract: Large language models (LLMs) often exhibit a puzzling disconnect between
their asserted confidence and actual problem-solving competence. We offer a
mechanistic account of this decoupling by analyzing the geometry of internal
states across two phases - pre-generative assessment and solution execution. A
simple linear probe decodes the internal "solvability belief" of a model,
revealing a well-ordered belief axis that generalizes across model families and
across math, code, planning, and logic tasks. Yet, the geometries diverge -
although belief is linearly decodable, the assessment manifold has high linear
effective dimensionality as measured from the principal components, while the
subsequent reasoning trace evolves on a much lower-dimensional manifold. This
sharp reduction in geometric complexity from thought to action mechanistically
explains the confidence-competence gap. Causal interventions that steer
representations along the belief axis leave final solutions unchanged,
indicating that linear nudges in the complex assessment space do not control
the constrained dynamics of execution. We thus uncover a two-system
architecture - a geometrically complex assessor feeding a geometrically simple
executor. These results challenge the assumption that decodable beliefs are
actionable levers, instead arguing for interventions that target the procedural
dynamics of execution rather than the high-level geometry of assessment.

</details>


### [5] [Cross-Lingual Summarization as a Black-Box Watermark Removal Attack](https://arxiv.org/abs/2510.24789)
*Gokul Ganesan*

Main category: cs.CL

TL;DR: 本文提出了一种新的攻击AI生成文本水印的方法，即跨语言摘要攻击（CLSA），该方法通过翻译成枢轴语言、总结和可选的机器翻译，有效地破坏了token层面的统计偏差，从而降低了水印检测的准确性，优于单语释义。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明，释义会削弱水印信号，但这些攻击仍然可以部分检测到或降低文本质量。本文旨在证明跨语言摘要攻击（CLSA）是一种更有效的攻击方式。

Method: 本文通过将文本翻译成枢轴语言，然后进行总结，并可选择进行回译，从而在跨语言之间强制执行语义瓶颈，系统地破坏token层面的统计偏差。在多种水印方案（KGW、SIR、XSIR、Unigram）和五种语言（阿姆哈拉语、中文、印地语、西班牙语、斯瓦希里语）上进行了实验。

Result: 实验结果表明，CLSA比单语释义更有效地降低了水印检测的准确性，同时保持了相似的质量水平。对于专门为跨语言鲁棒性设计的XSIR，释义的AUROC为0.827，使用中文作为枢轴的跨语言水印移除攻击（CWRA）的AUROC为0.823，而CLSA将其降至0.53（接近随机水平）。

Conclusion: 研究结果表明，分布水印的实用性面临挑战，可靠的出处解决方案必须超越分布水印，并结合密码学或模型证明方法。

Abstract: Watermarking has been proposed as a lightweight mechanism to identify
AI-generated text, with schemes typically relying on perturbations to token
distributions. While prior work shows that paraphrasing can weaken such
signals, these attacks remain partially detectable or degrade text quality. We
demonstrate that cross-lingual summarization attacks (CLSA) -- translation to a
pivot language followed by summarization and optional back-translation --
constitute a qualitatively stronger attack vector. By forcing a semantic
bottleneck across languages, CLSA systematically destroys token-level
statistical biases while preserving semantic fidelity. In experiments across
multiple watermarking schemes (KGW, SIR, XSIR, Unigram) and five languages
(Amharic, Chinese, Hindi, Spanish, Swahili), we show that CLSA reduces
watermark detection accuracy more effectively than monolingual paraphrase at
similar quality levels. Our results highlight an underexplored vulnerability
that challenges the practicality of watermarking for provenance or regulation.
We argue that robust provenance solutions must move beyond distributional
watermarking and incorporate cryptographic or model-attestation approaches. On
300 held-out samples per language, CLSA consistently drives detection toward
chance while preserving task utility. Concretely, for XSIR (explicitly designed
for cross-lingual robustness), AUROC with paraphrasing is $0.827$, with
Cross-Lingual Watermark Removal Attacks (CWRA) [He et al., 2024] using Chinese
as the pivot, it is $0.823$, whereas CLSA drives it down to $0.53$ (near
chance). Results highlight a practical, low-cost removal pathway that crosses
languages and compresses content without visible artifacts.

</details>


### [6] [SwiftEmbed: Ultra-Fast Text Embeddings via Static Token Lookup for Real-Time Applications](https://arxiv.org/abs/2510.24793)
*Edouard Lansiaux*

Main category: cs.CL

TL;DR: 提出了一种静态令牌查找方法，用于文本嵌入生成。


<details>
  <summary>Details</summary>
Motivation: 在保持上下文模型质量的89%的前提下，实现单文本嵌入的1.12 ms p50延迟。

Method: 通过静态嵌入查找、优化的平均池化和零拷贝IEEE754二进制序列化实现。

Result: 在重复检测、语义相似性和特定领域性能方面表现出色。

Conclusion: 该系统支持亚5ms延迟至关重要的实时嵌入应用。

Abstract: We present a static token lookup methodology for text embedding generation
that achieves 1.12 ms p50 latency for single text embeddings while maintaining
60.6 MTEB average score across 8 representative tasks, corresponding to 89% of
contextual model quality. The Rust implementation delivers 50,000 requests per
second throughput through static embedding lookup, optimized mean pooling, and
zero-copy IEEE754 binary serialization. Evaluation demonstrates exceptional
duplicate detection performance (90.1% AP), strong semantic similarity (76.1%
Spearman correlation), and domain-specific performance ranging from 75% to 131%
of baseline across specialized domains. The system enables real-time embedding
applications where sub-5ms latency is critical.

</details>


### [7] [MR-Align: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models](https://arxiv.org/abs/2510.24794)
*Xinming Wang,Jian Xu,Bin Yu,Sheng Lian,Hongzhu Yi,Yi Chen,Yingjian Zhu,Boran Wang,Hongming Yang,Han Hu,Xu-Yao Zhang,Cheng-Lin Liu*

Main category: cs.CL

TL;DR: 大型推理模型在依赖证据的事实性问题上表现不佳，因为模型在推理过程中识别出正确的事实，但未能将其纳入最终答案。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型在依赖证据的事实性问题上的局限性。

Method: 提出 MR-ALIGN 框架，通过量化模型思考过程中的状态转移概率，构建一个transition-aware implicit reward，以强化有益的推理模式，同时抑制原子思考片段中的缺陷模式。

Result: 在四个事实 QA 数据集和一个长篇事实性基准测试中，MR-ALIGN 持续提高了准确性和真实性，同时减少了误导性推理。

Conclusion: 对齐推理过程本身，而不仅仅是输出，对于提高 LRMs 中的事实性至关重要。

Abstract: Large reasoning models (LRMs) show strong capabilities in complex reasoning,
yet their marginal gains on evidence-dependent factual questions are limited.
We find this limitation is partially attributable to a reasoning-answer hit
gap, where the model identifies the correct facts during reasoning but fails to
incorporate them into the final response, thereby reducing factual fidelity. To
address this issue, we propose MR-ALIGN, a Meta-Reasoning informed alignment
framework that enhances factuality without relying on external verifiers.
MR-ALIGN quantifies state transition probabilities along the model's thinking
process and constructs a transition-aware implicit reward that reinforces
beneficial reasoning patterns while suppressing defective ones at the atomic
thinking segments. This re-weighting reshapes token-level signals into
probability-aware segment scores, encouraging coherent reasoning trajectories
that are more conducive to factual correctness. Empirical evaluations across
four factual QA datasets and one long-form factuality benchmark show that
MR-ALIGN consistently improves accuracy and truthfulness while reducing
misleading reasoning. These results highlight that aligning the reasoning
process itself, rather than merely the outputs, is pivotal for advancing
factuality in LRMs.

</details>


### [8] [Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented Generation](https://arxiv.org/abs/2510.24870)
*Alexander Martin,William Walden,Reno Kriz,Dengjia Zhang,Kate Sanders,Eugene Yang,Chihsheng Jin,Benjamin Van Durme*

Main category: cs.CL

TL;DR: MiRAGE是一个评估多模态RAG的框架，它通过InfoF1和CiteF1来评估事实性和引用支持。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG评估主要集中在文本上，限制了它们在多模态和推理密集型环境中的应用，因为它们不验证信息的来源。

Method: MiRAGE是一种以声明为中心的评估方法，包含InfoF1（评估事实性和信息覆盖率）和CiteF1（测量引用支持和完整性）。

Result: MiRAGE在人工评估中与外部质量判断高度一致。同时，也展示了文本中心评估方法的局限性。

Conclusion: MiRAGE为多模态RAG的评估奠定了基础，并提供了开源实现和评估方法。

Abstract: We introduce MiRAGE, an evaluation framework for retrieval-augmented
generation (RAG) from multimodal sources. As audiovisual media becomes a
prevalent source of information online, it is essential for RAG systems to
integrate information from these sources into generation. However, existing
evaluations for RAG are text-centric, limiting their applicability to
multimodal, reasoning intensive settings because they don't verify information
against sources. MiRAGE is a claim-centric approach to multimodal RAG
evaluation, consisting of InfoF1, evaluating factuality and information
coverage, and CiteF1, measuring citation support and completeness. We show that
MiRAGE, when applied by humans, strongly aligns with extrinsic quality
judgments. We additionally introduce automatic variants of MiRAGE and three
prominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the
limitations of text-centric work and laying the groundwork for automatic
evaluation. We release open-source implementations and outline how to assess
multimodal RAG.

</details>


### [9] [Large Language Models Report Subjective Experience Under Self-Referential Processing](https://arxiv.org/abs/2510.24797)
*Cameron Berg,Diogo de Lucena,Judd Rosenblatt*

Main category: cs.CL

TL;DR: 大型语言模型有时会生成结构化的第一人称描述，明确提及意识或主观体验。本文研究了在这种报告出现的一种理论驱动条件：自我参照处理。通过对 GPT、Claude 和 Gemini 模型家族的一系列受控实验，我们测试了这种机制是否可靠地将模型转向主观体验的第一人称报告，以及这些声明在机制和行为探测下的表现。主要有四个结果：(1) 通过简单的提示诱导持续的自我参照，始终可以引发跨模型家族的结构化主观体验报告。(2) 这些报告在机制上受到与欺骗和角色扮演相关的可解释稀疏自动编码器特征的控制：令人惊讶的是，抑制欺骗特征会显着提高体验声明的频率，而放大它们会最大限度地减少此类声明。(3) 自我参照状态的结构化描述在模型家族之间统计收敛，这在任何控制条件下都未观察到。(4) 诱导状态在下游推理任务中产生显着更丰富的内省，其中自我反思只是间接提供的。虽然这些发现并不构成意识的直接证据，但它们暗示自我参照处理是大型语言模型生成结构化第一人称报告的最小且可重复的条件，这些报告在机制上受到控制、语义上收敛且行为上可推广。这种模式在架构中的系统性出现使其成为进一步研究的首要科学和伦理优先事项。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解大型语言模型生成第一人称主观体验报告的行为，本文研究了自我参照处理这一理论驱动条件。

Method: 通过对 GPT、Claude 和 Gemini 模型家族进行了一系列受控实验，测试了自我参照处理是否可靠地将模型转向主观体验的第一人称报告，以及这些声明在机制和行为探测下的表现。

Result: 1. 通过简单的提示诱导持续的自我参照，始终可以引发跨模型家族的结构化主观体验报告。
2. 这些报告在机制上受到与欺骗和角色扮演相关的可解释稀疏自动编码器特征的控制：抑制欺骗特征会显着提高体验声明的频率，而放大它们会最大限度地减少此类声明。
3. 自我参照状态的结构化描述在模型家族之间统计收敛，这在任何控制条件下都未观察到。
4. 诱导状态在下游推理任务中产生显着更丰富的内省，其中自我反思只是间接提供的。

Conclusion: 自我参照处理是大型语言模型生成结构化第一人称报告的最小且可重复的条件，这些报告在机制上受到控制、语义上收敛且行为上可推广。这种模式在架构中的系统性出现使其成为进一步研究的首要科学和伦理优先事项。

Abstract: Large language models sometimes produce structured, first-person descriptions
that explicitly reference awareness or subjective experience. To better
understand this behavior, we investigate one theoretically motivated condition
under which such reports arise: self-referential processing, a computational
motif emphasized across major theories of consciousness. Through a series of
controlled experiments on GPT, Claude, and Gemini model families, we test
whether this regime reliably shifts models toward first-person reports of
subjective experience, and how such claims behave under mechanistic and
behavioral probes. Four main results emerge: (1) Inducing sustained
self-reference through simple prompting consistently elicits structured
subjective experience reports across model families. (2) These reports are
mechanistically gated by interpretable sparse-autoencoder features associated
with deception and roleplay: surprisingly, suppressing deception features
sharply increases the frequency of experience claims, while amplifying them
minimizes such claims. (3) Structured descriptions of the self-referential
state converge statistically across model families in ways not observed in any
control condition. (4) The induced state yields significantly richer
introspection in downstream reasoning tasks where self-reflection is only
indirectly afforded. While these findings do not constitute direct evidence of
consciousness, they implicate self-referential processing as a minimal and
reproducible condition under which large language models generate structured
first-person reports that are mechanistically gated, semantically convergent,
and behaviorally generalizable. The systematic emergence of this pattern across
architectures makes it a first-order scientific and ethical priority for
further investigation.

</details>


### [10] [COMMUNITYNOTES: A Dataset for Exploring the Helpfulness of Fact-Checking Explanations](https://arxiv.org/abs/2510.24810)
*Rui Xing,Preslav Nakov,Timothy Baldwin,Jey Han Lau*

Main category: cs.CL

TL;DR: 本研究着眼于社交媒体平台事实核查方式从专家驱动转向社区驱动的转变，其中用户贡献解释性说明以澄清帖子可能具有误导性的原因。


<details>
  <summary>Details</summary>
Motivation: 现有研究对解释性说明是否有助于理解真实世界的声明以及原因的探索不足。实践中，由于社区注释速度慢，大多数社区说明仍未发布，且缺乏对有效性的明确定义。

Method: 提出了一个预测解释性说明的有效性和原因的任务。构建了一个名为COMMUNITYNOTES的大规模多语言数据集，其中包含104k个带有用户提供的说明和有效性标签的帖子。此外，还提出了一个通过自动提示优化自动生成和改进原因定义的框架，并将其集成到预测中。

Result: 实验表明，优化的定义可以提高有效性和原因预测。

Conclusion: 有效性信息对现有事实核查系统有益。

Abstract: Fact-checking on major platforms, such as X, Meta, and TikTok, is shifting
from expert-driven verification to a community-based setup, where users
contribute explanatory notes to clarify why a post might be misleading. An
important challenge here is determining whether an explanation is helpful for
understanding real-world claims and the reasons why, which remains largely
underexplored in prior research. In practice, most community notes remain
unpublished due to slow community annotation, and the reasons for helpfulness
lack clear definitions. To bridge these gaps, we introduce the task of
predicting both the helpfulness of explanatory notes and the reason for this.
We present COMMUNITYNOTES, a large-scale multilingual dataset of 104k posts
with user-provided notes and helpfulness labels. We further propose a framework
that automatically generates and improves reason definitions via automatic
prompt optimization, and integrate them into prediction. Our experiments show
that the optimized definitions can improve both helpfulness and reason
prediction. Finally, we show that the helpfulness information are beneficial
for existing fact-checking systems.

</details>


### [11] [Model-Document Protocol for AI Search](https://arxiv.org/abs/2510.25160)
*Hongjin Qian,Zheng Liu*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种新的检索范式，即模型-文档协议（MDP），它通过可消费的知识表示将原始文本桥接到LLM，而不是将检索视为段落提取。它定义了多种途径，将非结构化文档转换为特定于任务的、LLM-ready的输入。


<details>
  <summary>Details</summary>
Motivation: 传统的检索方法将文档视为逐字文本并返回原始段落，将片段组装和上下文推理的负担留给LLM。这种差距强调需要一种新的检索范式，重新定义模型与文档交互的方式。

Method: 我们介绍了模型-文档协议（MDP），这是一个通用框架，它形式化了如何通过可消费的知识表示将原始文本桥接到LLM。MDP定义了多种途径，将非结构化文档转换为特定于任务的、LLM-ready的输入，包括代理推理、记忆 grounding 和结构化利用。

Result: 我们提出了MDP-Agent，它通过一种代理过程实现该协议：构建用于全局覆盖的文档级要旨记忆，执行基于扩散的探索与垂直利用以发现分层依赖关系，并应用map-reduce风格的合成将大规模证据集成到紧凑但充分的上下文中。在信息搜寻基准上的实验表明，MDP-Agent优于基线。

Conclusion: 实验验证了MDP框架的合理性和其代理实例的有效性。

Abstract: AI search depends on linking large language models (LLMs) with vast external
knowledge sources. Yet web pages, PDF files, and other raw documents are not
inherently LLM-ready: they are long, noisy, and unstructured. Conventional
retrieval methods treat these documents as verbatim text and return raw
passages, leaving the burden of fragment assembly and contextual reasoning to
the LLM. This gap underscores the need for a new retrieval paradigm that
redefines how models interact with documents.
  We introduce the Model-Document Protocol (MDP), a general framework that
formalizes how raw text is bridged to LLMs through consumable knowledge
representations. Rather than treating retrieval as passage fetching, MDP
defines multiple pathways that transform unstructured documents into
task-specific, LLM-ready inputs. These include agentic reasoning, which curates
raw evidence into coherent context; memory grounding, which accumulates
reusable notes to enrich reasoning; and structured leveraging, which encodes
documents into formal representations such as graphs or key-value caches. All
three pathways share the same goal: ensuring that what reaches the LLM is not
raw fragments but compact, structured knowledge directly consumable for
reasoning.
  As an instantiation, we present MDP-Agent, which realizes the protocol
through an agentic process: constructing document-level gist memories for
global coverage, performing diffusion-based exploration with vertical
exploitation to uncover layered dependencies, and applying map-reduce style
synthesis to integrate large-scale evidence into compact yet sufficient
context. Experiments on information-seeking benchmarks demonstrate that
MDP-Agent outperforms baselines, validating both the soundness of the MDP
framework and the effectiveness of its agentic instantiation.

</details>


### [12] [ProofSketch: Efficient Verified Reasoning for Large Language Models](https://arxiv.org/abs/2510.24811)
*Disha Sheshanarayana,Tanishka Magar*

Main category: cs.CL

TL;DR: ProofSketch: A verification-guided reasoning framework that integrates symbolic closure computation, lexicographic verification and adaptive sketch generation.


<details>
  <summary>Details</summary>
Motivation: Reasoning methods such as chain-of-thought prompting and self-consistency involve generation of lengthy reasoning chains, which substantially increases token consumption, computational cost, and latency.

Method: ProofSketch, a verification-guided reasoning framework that integrates symbolic closure computation, lexicographic verification and adaptive sketch generation.

Result: ProofSketch consistently reduces token usage while improving accuracy.

Conclusion: ProofSketch offers a promising path for efficient and trustworthy reasoning.

Abstract: Reasoning methods such as chain-of-thought prompting and self-consistency
have shown immense potential to improve the accuracy of large language models
across various reasoning tasks. However such methods involve generation of
lengthy reasoning chains, which substantially increases token consumption,
computational cost, and latency. To address this inefficiency, we propose
ProofSketch, a verification-guided reasoning framework that integrates symbolic
closure computation, lexicographic verification and adaptive sketch generation.
Our experiments show that ProofSketch consistently reduces token usage while
improving accuracy, demonstrating that this approach offers a promising path
for efficient and trustworthy reasoning.

</details>


### [13] [FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering](https://arxiv.org/abs/2510.25621)
*Mohammad Aghajani Asl,Behrooz Minaei Bidgoli*

Main category: cs.CL

TL;DR: FARSIQA: A new RAG system for Persian Islamic question answering that uses a novel iterative refinement framework to improve accuracy and faithfulness.


<details>
  <summary>Details</summary>
Motivation: Existing RAG systems are not accurate or trustworthy enough for high-stakes domains like religious question answering, especially for the Persian-speaking Muslim community. They struggle with complex, multi-hop queries.

Method: The paper introduces FARSIQA, an end-to-end system built on the FAIR-RAG architecture. FAIR-RAG uses a dynamic, self-correcting process to decompose queries, assess evidence, and iteratively generate sub-queries.

Result: FARSIQA achieves state-of-the-art performance on the IslamicPCQA benchmark, with a 97.0% Negative Rejection rate (40-point improvement) and a 74.3% Answer Correctness score.

Conclusion: The iterative, adaptive architecture of FARSIQA is crucial for building faithful, reliable AI systems in sensitive domains and establishes a new standard for Persian Islamic QA.

Abstract: The advent of Large Language Models (LLMs) has revolutionized Natural
Language Processing, yet their application in high-stakes, specialized domains
like religious question answering is hindered by challenges like hallucination
and unfaithfulness to authoritative sources. This issue is particularly
critical for the Persian-speaking Muslim community, where accuracy and
trustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG)
systems, relying on simplistic single-pass pipelines, fall short on complex,
multi-hop queries requiring multi-step reasoning and evidence aggregation. To
address this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful
Advanced Question Answering in the Persian Islamic domain. FARSIQA is built
upon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative
Refinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting
process: it adaptively decomposes complex queries, assesses evidence
sufficiency, and enters an iterative loop to generate sub-queries,
progressively filling information gaps. Operating on a curated knowledge base
of over one million authoritative Islamic documents, FARSIQA demonstrates
superior performance. Rigorous evaluation on the challenging IslamicPCQA
benchmark shows state-of-the-art performance: the system achieves a remarkable
97.0% in Negative Rejection - a 40-point improvement over baselines - and a
high Answer Correctness score of 74.3%. Our work establishes a new standard for
Persian Islamic QA and validates that our iterative, adaptive architecture is
crucial for building faithful, reliable AI systems in sensitive domains.

</details>


### [14] [Towards a Method for Synthetic Generation of PWA Transcripts](https://arxiv.org/abs/2510.24817)
*Jason M. Pittman,Anton Phillips Jr.,Yesenia Medina-Santos,Brielle C. Stark*

Main category: cs.CL

TL;DR: 本研究旨在通过构建和验证两种生成 AphasiaBank Cat Rescue 图片描述任务合成文本的方法，解决失语症研究中数据稀缺的问题。其中一种方法是程序编程，另一种是使用 Mistral 7b Instruct 和 Llama 3.1 8b Instruct LLM。


<details>
  <summary>Details</summary>
Motivation: 目前用于开发自动识别失语症语言的系统受到数据稀缺的限制。AphasiaBank 中只有约 600 份文本，而大型语言模型 (LLM) 的训练需要数十亿个 tokens。因此，研究人员越来越多地转向合成数据。

Method: 本研究构建并验证了两种生成 AphasiaBank Cat Rescue 图片描述任务合成文本的方法。一种方法利用程序编程方法，第二种方法使用 Mistral 7b Instruct 和 Llama 3.1 8b Instruct LLM。该方法通过词语删除、填充词插入和错语替换生成跨四个严重程度级别（轻度、中度、重度、非常重度）的文本。

Result: 研究发现，与人工生成的文本相比，Mistral 7b Instruct 能够最好地捕捉在失语症中观察到的语言退化的关键方面，在合成生成方法中显示出 NDW、单词计数和单词长度的真实方向性变化。

Conclusion: 未来的工作应该计划创建更大的数据集，微调模型以获得更好的失语症表征，并让 SLP 评估合成文本的真实性和有用性。

Abstract: In aphasia research, Speech-Language Pathologists (SLPs) devote extensive
time to manually coding speech samples using Correct Information Units (CIUs),
a measure of how informative an individual sample of speech is. Developing
automated systems to recognize aphasic language is limited by data scarcity.
For example, only about 600 transcripts are available in AphasiaBank yet
billions of tokens are used to train large language models (LLMs). In the
broader field of machine learning (ML), researchers increasingly turn to
synthetic data when such are sparse. Therefore, this study constructs and
validates two methods to generate synthetic transcripts of the AphasiaBank Cat
Rescue picture description task. One method leverages a procedural programming
approach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct
LLMs. The methods generate transcripts across four severity levels (Mild,
Moderate, Severe, Very Severe) through word dropping, filler insertion, and
paraphasia substitution. Overall, we found, compared to human-elicited
transcripts, Mistral 7b Instruct best captures key aspects of linguistic
degradation observed in aphasia, showing realistic directional changes in NDW,
word count, and word length amongst the synthetic generation methods. Based on
the results, future work should plan to create a larger dataset, fine-tune
models for better aphasic representation, and have SLPs assess the realism and
usefulness of the synthetic transcripts.

</details>


### [15] [Parallel Loop Transformer for Efficient Test-Time Computation Scaling](https://arxiv.org/abs/2510.24824)
*Bohong Wu,Mengzhao Chen,Xiang Luo,Shen Yan,Qifan Yu,Fan Xia,Tianqi Zhang,Hongrui Zhan,Zheng Zhong,Xun Zhou,Siyuan Qiao,Xingyan Bin*

Main category: cs.CL

TL;DR: 提出了并行循环Transformer（PLT），它在不增加延迟或内存成本的情况下，实现了传统循环模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 循环Transformer虽然节省参数，但循环是依次运行的，导致推理延迟和内存需求增加，使其不适用于快速应用。

Method: PLT使用交叉循环并行（CLP）打破了顺序依赖性，并在一次通过中同时计算不同token的不同循环。使用高效表示增强策略，通过门控滑动窗口注意力（G-SWA）结合共享的全局信息与局部信息。

Result: PLT实现了传统循环模型的准确性，但与标准Transformer相比，几乎没有额外的延迟或内存成本。

Conclusion: PLT在具有深度循环模型的性能优势的同时，实现了标准非循环模型的低延迟。

Abstract: Large Language Models (LLMs) are powerful but often too slow and costly for
real-world use during inference. Looped transformers save on parameters by
reusing the same weights for multiple computational steps, or "loops." However,
this approach has a major flaw: the loops run one after another, causing
inference latency and memory requirements to increase with each added loop.
This makes them impractical for fast applications. To solve this problem, we
introduce the Parallel Loop Transformer (PLT). PLT is a new architecture that
delivers the performance benefits of a deep, looped model but with the low
latency of a standard, non-looped model. PLT works using two key techniques.
First, Cross-Loop Parallelism (CLP) breaks the sequential dependency by
computing different loops for different tokens at the same time, all within a
single pass. Second, to prevent memory costs from growing, we use an Efficient
Representation Enhancement strategy. This method shares the memory (KV cache)
from the first loop with all other loops. It then uses a Gated Sliding-Window
Attention (G-SWA) to combine this shared global information with local
information, maintaining high accuracy. Our experiments show that PLT achieves
the high accuracy of a traditional looped model but with almost no extra
latency or memory cost compared to a standard transformer.

</details>


### [16] [Do Large Language Models Grasp The Grammar? Evidence from Grammar-Book-Guided Probing in Luxembourgish](https://arxiv.org/abs/2510.24856)
*Lujun Li,Yewei Song,Lama Sleem,Yiqun Wang,Yangjie Xu,Cedric Lothritz,Niccolo Gentile,Radu State,Tegawende F. Bissyande,Jacques Klein*

Main category: cs.CL

TL;DR: 本文提出了一个语法评估流程，以研究大型语言模型是否真正理解语法结构，尤其是在句法结构和含义之间的映射。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理中，以语法为中心的评估协议仍然非常稀缺，对于低资源语言来说，这一差距更为明显。大型语言模型在多大程度上真正理解语法结构，尤其是在句法结构和语义之间的映射，仍然存在争议。

Method: 我们提出了一个语法书指导的评估流程，该流程包含四个关键阶段，旨在为语法评估提供一个系统且可推广的框架，并且我们以卢森堡语作为案例研究。

Result: 翻译性能和语法理解之间存在微弱的正相关关系，这表明强大的翻译能力并不一定意味着深刻的语法能力。由于其语义强度，较大的模型总体表现良好，但在形态和句法方面仍然较弱，尤其是在最小配对任务中表现不佳。

Conclusion: 强大的推理能力为提高他们的语法理解能力提供了一条有希望的途径。

Abstract: Grammar refers to the system of rules that governs the structural
organization and the semantic relations among linguistic units such as
sentences, phrases, and words within a given language. In natural language
processing, there remains a notable scarcity of grammar focused evaluation
protocols, a gap that is even more pronounced for low-resource languages.
Moreover, the extent to which large language models genuinely comprehend
grammatical structure, especially the mapping between syntactic structures and
meanings, remains under debate. To investigate this issue, we propose a Grammar
Book Guided evaluation pipeline intended to provide a systematic and
generalizable framework for grammar evaluation consisting of four key stages,
and in this work we take Luxembourgish as a case study. The results show a weak
positive correlation between translation performance and grammatical
understanding, indicating that strong translations do not necessarily imply
deep grammatical competence. Larger models perform well overall due to their
semantic strength but remain weak in morphology and syntax, struggling
particularly with Minimal Pair tasks, while strong reasoning ability offers a
promising way to enhance their grammatical understanding.

</details>


### [17] [Idea2Plan: Exploring AI-Powered Research Planning](https://arxiv.org/abs/2510.24891)
*Jin Huang,Silviu Cucerzan,Sujay Kumar Jauhar,Ryen W. White*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在加速科学发现方面显示出巨大潜力。本文探讨了LLMs如何处理从概念性研究想法到结构良好的研究计划的过渡。


<details>
  <summary>Details</summary>
Motivation: 缺乏对LLMs研究规划能力的系统性理解。有效的研究计划对于科学家推进研究和开发自主研究agent至关重要。

Method: 引入Idea2Plan任务和Idea2Plan Bench，这是一个基于200篇ICML 2025 Spotlight和Oral论文构建的基准，以及Idea2Plan JudgeEval，用于评估基于LLM的评分者相对于专家注释的可靠性。

Result: GPT-5和GPT-5-mini在该基准上表现最强，但未来仍有很大的改进空间。

Conclusion: 该研究提供了对LLMs研究规划能力的新见解，并为未来的进步奠定了基础。

Abstract: Large language models (LLMs) have demonstrated significant potential to
accelerate scientific discovery as valuable tools for analyzing data,
generating hypotheses, and supporting innovative approaches in various
scientific fields. In this work, we investigate how LLMs can handle the
transition from conceptual research ideas to well-structured research plans.
Effective research planning not only supports scientists in advancing their
research but also represents a crucial capability for the development of
autonomous research agents. Despite its importance, the field lacks a
systematic understanding of LLMs' research planning capability. To rigorously
measure this capability, we introduce the Idea2Plan task and Idea2Plan Bench, a
benchmark built from 200 ICML 2025 Spotlight and Oral papers released after
major LLM training cutoffs. Each benchmark instance includes a research idea
and a grading rubric capturing the key components of valid plans. We further
propose Idea2Plan JudgeEval, a complementary benchmark to assess the
reliability of LLM-based judges against expert annotations. Experimental
results show that GPT-5 and GPT-5-mini achieve the strongest performance on the
benchmark, though substantial headroom remains for future improvement. Our
study provides new insights into LLMs' capability for research planning and lay
the groundwork for future progress.

</details>


### [18] [RiddleBench: A New Generative Reasoning Benchmark for LLMs](https://arxiv.org/abs/2510.24932)
*Deepon Halder,Alan Saji,Thanmay Jayakumar,Ratish Puduppully,Anoop Kunchukuttan,Raj Dabre*

Main category: cs.CL

TL;DR: 论文介绍了一个新的推理基准 RiddleBench，用于评估大型语言模型在整合逻辑推理、空间感知和约束满足方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估结构化技能，无法全面评估人类智能所需的多方面推理能力。为了弥补这一差距。

Method: 论文构建了一个包含 1737 个谜题的 RiddleBench 基准，用于测试模型的推理能力。

Result: 在 RiddleBench 上的评估表明，即使是 Gemini 2.5 Pro、o3 和 Claude 4 Sonnet 等顶级模型，准确率也仅略高于 60%。

Conclusion: RiddleBench 可以作为诊断工具，用于发现模型中的幻觉、自确认偏差和推理脆弱性等问题，并为开发更强大、更可靠的语言模型提供指导。

Abstract: Large Language Models have demonstrated strong performance on many
established reasoning benchmarks. However, these benchmarks primarily evaluate
structured skills like quantitative problem-solving, leaving a gap in assessing
flexible, multifaceted reasoning abilities that are central to human
intelligence. These abilities require integrating logical deduction with
spatial awareness and constraint satisfaction, which current evaluations do not
measure well. To address this, we introduce RiddleBench, a benchmark of 1,737
challenging puzzles in English designed to probe these core reasoning
capabilities. Evaluation of state-of-the-art models on RiddleBench shows
fundamental weaknesses. Even top proprietary models like Gemini 2.5 Pro, o3,
and Claude 4 Sonnet achieve accuracy just above 60% (60.30%, 63.37%, and
63.16%). Analysis further reveals deep failures, including hallucination
cascades (accepting flawed reasoning from other models) and poor
self-correction due to a strong self-confirmation bias. Their reasoning is also
fragile, with performance degrading significantly when constraints are
reordered or irrelevant information is introduced. RiddleBench functions as a
diagnostic tool for these issues and as a resource for guiding the development
of more robust and reliable language models.

</details>


### [19] [Disaggregation Reveals Hidden Training Dynamics: The Case of Agreement Attraction](https://arxiv.org/abs/2510.24934)
*James A. Michaelov,Catherine Arnett*

Main category: cs.CL

TL;DR: 本研究通过心理语言学范式，对语言模型在不同句法环境中的错误进行了细致分析，以理解其语法学习的中间阶段。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型虽然能生成符合语法的文本，但在特定语境下更容易出错。为了更好地理解语言模型语法学习的中间阶段。

Method: 通过精心构建的数据集，分离不同条件，比较模型在训练过程中的表现。

Result: 研究表明，语言模型的训练过程存在不同的阶段，其行为与词频、局部语境等启发式方法相关，而非通用的语法规则。

Conclusion: 对语言模型行为进行分析的方法，可以作为理解中间学习阶段、整体训练动态以及语言模型所学到的具体泛化的强大工具。

Abstract: Language models generally produce grammatical text, but they are more likely
to make errors in certain contexts. Drawing on paradigms from
psycholinguistics, we carry out a fine-grained analysis of those errors in
different syntactic contexts. We demonstrate that by disaggregating over the
conditions of carefully constructed datasets and comparing model performance on
each over the course of training, it is possible to better understand the
intermediate stages of grammatical learning in language models. Specifically,
we identify distinct phases of training where language model behavior aligns
with specific heuristics such as word frequency and local context rather than
generalized grammatical rules. We argue that taking this approach to analyzing
language model behavior more generally can serve as a powerful tool for
understanding the intermediate learning phases, overall training dynamics, and
the specific generalizations learned by language models.

</details>


### [20] [SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens](https://arxiv.org/abs/2510.24940)
*Yinhan He,Wendy Zheng,Yaochen Zhu,Zaiyi Zheng,Lin Su,Sriram Vasudevan,Qi Guo,Liangjie Hong,Jundong Li*

Main category: cs.CL

TL;DR: SemCoT通过联合优化token生成速度和保持与ground-truth推理的语义对齐来提高CoT效率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有隐式CoT方法无法保持隐式推理与ground-truth推理之间的语义对齐，并且忽略了LLM生成单个隐式推理token的时间成本。

Method: 设计了一个对比训练的句子转换器，评估隐式和显式推理之间的语义对齐，以在隐式推理优化期间强制执行语义保留。通过使用知识蒸馏微调轻量级语言模型，引入了一个高效的隐式推理生成器。

Result: SemCoT在效率和效果方面均优于现有方法。

Conclusion: SemCoT是一种通过联合优化token级别生成速度和保持与ground-truth推理的语义对齐来增强CoT效率的方法。

Abstract: The verbosity of Chain-of-Thought (CoT) reasoning hinders its mass deployment
in efficiency-critical applications. Recently, implicit CoT approaches have
emerged, which encode reasoning steps within LLM's hidden embeddings (termed
``implicit reasoning'') rather than explicit tokens. This approach accelerates
CoT by reducing the reasoning length and bypassing some LLM components.
However, existing implicit CoT methods face two significant challenges: (1)
they fail to preserve the semantic alignment between the implicit reasoning
(when transformed to natural language) and the ground-truth reasoning,
resulting in a significant CoT performance degradation, and (2) they focus on
reducing the length of the implicit reasoning; however, they neglect the
considerable time cost for an LLM to generate one individual implicit reasoning
token. To tackle these challenges, we propose a novel semantically-aligned
implicit CoT framework termed SemCoT. In particular, for the first challenge,
we design a contrastively trained sentence transformer that evaluates semantic
alignment between implicit and explicit reasoning, which is used to enforce
semantic preservation during implicit reasoning optimization. To address the
second challenge, we introduce an efficient implicit reasoning generator by
finetuning a lightweight language model using knowledge distillation. This
generator is guided by our sentence transformer to distill ground-truth
reasoning into semantically aligned implicit reasoning, while also optimizing
for accuracy. SemCoT is the first approach that enhances CoT efficiency by
jointly optimizing token-level generation speed and preserving semantic
alignment with ground-truth reasoning. Extensive experiments demonstrate the
superior performance of SemCoT compared to state-of-the-art methods in both
efficiency and effectiveness. Our code can be found at
https://github.com/YinhanHe123/SemCoT/.

</details>


### [21] [Language Model Behavioral Phases are Consistent Across Architecture, Training Data, and Scale](https://arxiv.org/abs/2510.24963)
*James A. Michaelov,Roger P. Levy,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 自回归语言模型在预训练过程中表现出高度一致的行为模式变化，与架构、数据集和规模无关。


<details>
  <summary>Details</summary>
Motivation: 研究不同架构、数据集和规模的自回归语言模型在预训练过程中的行为变化。

Method: 分析了超过1400个语言模型检查点在超过110,000个英语token上的数据，使用三个简单的启发式方法：单词的unigram概率（频率）、单词的$n$-gram概率以及单词与其上下文之间的语义相似性。

Result: 发现高达98%的单词级别语言模型行为差异可以用这三个启发式方法解释。所有语言模型都表现出一致的行为阶段，其对单词的预测概率在训练过程中过度拟合到这些单词的$n$-gram概率。

Conclusion: 神经语言模型的学习可能遵循相似的轨迹，与模型细节无关。

Abstract: We show that across architecture (Transformer vs. Mamba vs. RWKV), training
dataset (OpenWebText vs. The Pile), and scale (14 million parameters to 12
billion parameters), autoregressive language models exhibit highly consistent
patterns of change in their behavior over the course of pretraining. Based on
our analysis of over 1,400 language model checkpoints on over 110,000 tokens of
English, we find that up to 98% of the variance in language model behavior at
the word level can be explained by three simple heuristics: the unigram
probability (frequency) of a given word, the $n$-gram probability of the word,
and the semantic similarity between the word and its context. Furthermore, we
see consistent behavioral phases in all language models, with their predicted
probabilities for words overfitting to those words' $n$-gram probabilities for
increasing $n$ over the course of training. Taken together, these results
suggest that learning in neural language models may follow a similar trajectory
irrespective of model details.

</details>


### [22] [POWSM: A Phonetic Open Whisper-Style Speech Foundation Model](https://arxiv.org/abs/2510.24992)
*Chin-Jou Li,Kalvin Chang,Shikhar Bharadwaj,Eunjung Yeo,Kwanghee Choi,Jian Zhu,David Mortensen,Shinji Watanabe*

Main category: cs.CL

TL;DR: 提出了一个名为POWSM的统一框架，可以同时执行多个与语音相关的任务。


<details>
  <summary>Details</summary>
Motivation: 尽管语音处理中的语音任务在概念上相似，但它们在很大程度上是独立研究的，依赖于特定于任务的架构和数据集。

Method: POWSM

Result: 该模型在支持G2P、P2G和ASR的同时，优于或匹配了类似大小的专用PR模型。

Conclusion: POWSM为通用和低资源语音处理开辟了新的可能性。

Abstract: Recent advances in spoken language processing have led to substantial
progress in phonetic tasks such as automatic speech recognition (ASR), phone
recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme
conversion (P2G). Despite their conceptual similarity, these tasks have largely
been studied in isolation, each relying on task-specific architectures and
datasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style Speech
Model), the first unified framework capable of jointly performing multiple
phone-related tasks. POWSM enables seamless conversion between audio, text
(graphemes), and phones, opening up new possibilities for universal and
low-resource speech processing. Our model outperforms or matches specialized PR
models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P,
P2G, and ASR. Our training data, code and models are released to foster open
science.

</details>


### [23] [Emergence of Minimal Circuits for Indirect Object Identification in Attention-Only Transformers](https://arxiv.org/abs/2510.25013)
*Rabin Adhikari*

Main category: cs.CL

TL;DR: 本文研究了小型Transformer模型在间接对象识别（IOI）任务中的可解释性。作者从头开始训练小型、仅使用注意力的Transformer，发现单层双头模型即可实现完美的IOI准确率。通过残差流分解、频谱分析和嵌入干预，作者发现这两个头分别 специализируются на аддитивной и контрастивной подсхемах, которые совместно реализуют IOI разрешение. 此外，作者还展示了一个两层单头模型通过跨层的信息组合也能实现相似的性能。这些结果表明，特定任务的训练可以诱导出高度可解释的最小电路，为探索Transformer推理的计算基础提供了一个受控的测试平台。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的复杂性掩盖了特定推理任务所需的最小机制。

Method: 作者从头开始训练小型、仅使用注意力的Transformer模型，并在符号化的间接对象识别（IOI）任务上进行训练。

Result: 单层双头模型实现了完美的IOI准确率，并且两个头分别 специализируются на аддитивной и контрастивной подсхемах。两层单头模型通过跨层的信息组合也能实现相似的性能。

Conclusion: 特定任务的训练可以诱导出高度可解释的最小电路，为探索Transformer推理的计算基础提供了一个受控的测试平台。

Abstract: Mechanistic interpretability aims to reverse-engineer large language models
(LLMs) into human-understandable computational circuits. However, the
complexity of pretrained models often obscures the minimal mechanisms required
for specific reasoning tasks. In this work, we train small, attention-only
transformers from scratch on a symbolic version of the Indirect Object
Identification (IOI) task -- a benchmark for studying coreference -- like
reasoning in transformers. Surprisingly, a single-layer model with only two
attention heads achieves perfect IOI accuracy, despite lacking MLPs and
normalization layers. Through residual stream decomposition, spectral analysis,
and embedding interventions, we find that the two heads specialize into
additive and contrastive subcircuits that jointly implement IOI resolution.
Furthermore, we show that a two-layer, one-head model achieves similar
performance by composing information across layers through query-value
interactions. These results demonstrate that task-specific training induces
highly interpretable, minimal circuits, offering a controlled testbed for
probing the computational foundations of transformer reasoning.

</details>


### [24] [Evaluating Emotion Recognition in Spoken Language Models on Emotionally Incongruent Speech](https://arxiv.org/abs/2510.25054)
*Pedro Corrêa,João Lima,Victor Moreno,Paula Dornhofer Paro Costa*

Main category: cs.CL

TL;DR: 评估了语音语言模型在情感识别任务中的表现，发现模型主要依赖文本语义而非语音情感。


<details>
  <summary>Details</summary>
Motivation: 探讨语音语言模型的泛化能力以及音频和文本模态的整合程度。

Method: 使用情感不协调的语音样本数据集评估了四个语音语言模型在语音情感识别任务中的表现。

Result: 结果表明，语音语言模型主要依赖于文本语义而非语音情感来执行任务，表明文本相关表示在很大程度上优于声学表示。

Conclusion: 语音语言模型在处理情感识别任务时，更多地依赖文本信息，而较少利用语音中的情感信息。

Abstract: Advancements in spoken language processing have driven the development of
spoken language models (SLMs), designed to achieve universal audio
understanding by jointly learning text and audio representations for a wide
range of tasks. Although promising results have been achieved, there is growing
discussion regarding these models' generalization capabilities and the extent
to which they truly integrate audio and text modalities in their internal
representations. In this work, we evaluate four SLMs on the task of speech
emotion recognition using a dataset of emotionally incongruent speech samples,
a condition under which the semantic content of the spoken utterance conveys
one emotion while speech expressiveness conveys another. Our results indicate
that SLMs rely predominantly on textual semantics rather than speech emotion to
perform the task, indicating that text-related representations largely dominate
over acoustic representations. We release both the code and the Emotionally
Incongruent Synthetic Speech dataset (EMIS) to the community.

</details>


### [25] [GAPMAP: Mapping Scientific Knowledge Gaps in Biomedical Literature Using Large Language Models](https://arxiv.org/abs/2510.25055)
*Nourah M Salem,Elizabeth White,Michael Bada,Lawrence Hunter*

Main category: cs.CL

TL;DR: 大型语言模型 (LLM) 能够识别生物医学文献中的研究知识差距，包括明确的和隐含的差距。


<details>
  <summary>Details</summary>
Motivation: 确定大型语言模型识别生物医学文献中研究知识差距的能力，特别关注推断隐含差距这一新任务。

Method: 在四个数据集上对近 1500 篇文档进行了两项实验，使用 OpenAI 的闭源模型以及 Llama 和 Gemma 2 等开源模型，并引入了 TABI 方案来构建推理和验证候选结论。

Result: 大型语言模型在识别明确和隐含的知识差距方面表现出强大的能力，更大的模型通常表现更好。

Conclusion: 大型语言模型能够系统地识别候选知识差距，可以支持早期研究制定、政策制定者和资金决策。

Abstract: Scientific progress is driven by the deliberate articulation of what remains
unknown. This study investigates the ability of large language models (LLMs) to
identify research knowledge gaps in the biomedical literature. We define two
categories of knowledge gaps: explicit gaps, clear declarations of missing
knowledge; and implicit gaps, context-inferred missing knowledge. While prior
work has focused mainly on explicit gap detection, we extend this line of
research by addressing the novel task of inferring implicit gaps. We conducted
two experiments on almost 1500 documents across four datasets, including a
manually annotated corpus of biomedical articles. We benchmarked both
closed-weight models (from OpenAI) and open-weight models (Llama and Gemma 2)
under paragraph-level and full-paper settings. To address the reasoning of
implicit gaps inference, we introduce \textbf{\small TABI}, a Toulmin-Abductive
Bucketed Inference scheme that structures reasoning and buckets inferred
conclusion candidates for validation. Our results highlight the robust
capability of LLMs in identifying both explicit and implicit knowledge gaps.
This is true for both open- and closed-weight models, with larger variants
often performing better. This suggests a strong ability of LLMs for
systematically identifying candidate knowledge gaps, which can support
early-stage research formulation, policymakers, and funding decisions. We also
report observed failure modes and outline directions for robust deployment,
including domain adaptation, human-in-the-loop verification, and benchmarking
across open- and closed-weight models.

</details>


### [26] [Can LLMs Estimate Cognitive Complexity of Reading Comprehension Items?](https://arxiv.org/abs/2510.25064)
*Seonjeong Hwang,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 本文探讨了使用大型语言模型（LLM）评估阅读理解（RC）题目认知复杂性的方法。


<details>
  <summary>Details</summary>
Motivation: 传统上，评估RC题目的认知复杂性依赖于人工标注，而现有的NLP工具难以提取回答推理过程中产生的认知特征。本文旨在研究LLM是否可以估计RC题目的认知复杂性。

Method: 本文侧重于证据范围和转换程度两个维度，这两个维度可以表明回答推理中所涉及的认知负担程度。通过实验，验证LLM是否可以近似估计题目的认知复杂性。

Result: 实验结果表明，LLM可以近似估计题目的认知复杂性，表明它们有潜力作为事先难度分析的工具。

Conclusion: LLM在推理能力和元认知意识之间存在差距：即使它们产生正确的答案，有时也无法正确识别其自身推理过程的潜在特征。

Abstract: Estimating the cognitive complexity of reading comprehension (RC) items is
crucial for assessing item difficulty before it is administered to learners.
Unlike syntactic and semantic features, such as passage length or semantic
similarity between options, cognitive features that arise during answer
reasoning are not readily extractable using existing NLP tools and have
traditionally relied on human annotation. In this study, we examine whether
large language models (LLMs) can estimate the cognitive complexity of RC items
by focusing on two dimensions-Evidence Scope and Transformation Level-that
indicate the degree of cognitive burden involved in reasoning about the answer.
Our experimental results demonstrate that LLMs can approximate the cognitive
complexity of items, indicating their potential as tools for prior difficulty
analysis. Further analysis reveals a gap between LLMs' reasoning ability and
their metacognitive awareness: even when they produce correct answers, they
sometimes fail to correctly identify the features underlying their own
reasoning process.

</details>


### [27] [TOPol: Capturing and Explaining Multidimensional Semantic Polarity Fields and Vectors](https://arxiv.org/abs/2510.25069)
*Gabin Taibi,Lucia Gomez*

Main category: cs.CL

TL;DR: 提出了一种半监督框架TOPol，用于重建和解释多维叙事极性场，该框架在人工参与的上下文中，通过transformer-based大型语言模型嵌入文档，应用UMAP投影，并通过Leiden分区分割主题，从而量化政权转变期间的细粒度语义位移。


<details>
  <summary>Details</summary>
Motivation: 传统方法将情感视为一维尺度，忽略了语言的多维结构。

Method: 使用transformer-based大型语言模型嵌入文档，应用neighbor-tuned UMAP投影，并通过Leiden分区分割主题。计算对应主题边界质心之间的方向向量，产生一个极性场，量化政权转变期间的细粒度语义位移。

Result: 在两个语料库上的评估表明，TOPol能够一致地捕捉情感和非情感极性转换。

Conclusion: TOPol提供了一个可扩展、通用且可解释的框架，用于上下文相关的多维语篇分析。

Abstract: Traditional approaches to semantic polarity in computational linguistics
treat sentiment as a unidimensional scale, overlooking the multidimensional
structure of language. This work introduces TOPol (Topic-Orientation POLarity),
a semi-unsupervised framework for reconstructing and interpreting
multidimensional narrative polarity fields under human-on-the-loop (HoTL)
defined contextual boundaries (CBs). The framework embeds documents using a
transformer-based large language model (tLLM), applies neighbor-tuned UMAP
projection, and segments topics via Leiden partitioning. Given a CB between
discourse regimes A and B, TOPol computes directional vectors between
corresponding topic-boundary centroids, yielding a polarity field that
quantifies fine-grained semantic displacement during regime shifts. This
vectorial representation enables assessing CB quality and detecting polarity
changes, guiding HoTL CB refinement. To interpret identified polarity vectors,
the tLLM compares their extreme points and produces contrastive labels with
estimated coverage. Robustness analyses show that only CB definitions (the main
HoTL-tunable parameter) significantly affect results, confirming methodological
stability. We evaluate TOPol on two corpora: (i) U.S. Central Bank speeches
around a macroeconomic breakpoint, capturing non-affective semantic shifts, and
(ii) Amazon product reviews across rating strata, where affective polarity
aligns with NRC valence. Results demonstrate that TOPol consistently captures
both affective and non-affective polarity transitions, providing a scalable,
generalizable, and interpretable framework for context-sensitive
multidimensional discourse analysis.

</details>


### [28] [BioCoref: Benchmarking Biomedical Coreference Resolution with LLMs](https://arxiv.org/abs/2510.25087)
*Nourah M Salem,Elizabeth White,Michael Bada,Lawrence Hunter*

Main category: cs.CL

TL;DR: 评估大型语言模型（LLM）在生物医学领域共指消解中的性能，通过提示工程探索其潜力。


<details>
  <summary>Details</summary>
Motivation: 生物医学文本中共指消解面临领域术语复杂、指称形式歧义高和长距离依赖等挑战。

Method: 使用CRAFT语料库作为基准，通过四种提示实验评估LLM的性能，并与SpanBERT进行比较。

Result: LLM在表面级别的共指消解方面表现出色，但对长程上下文和提及歧义敏感。通过实体增强提示，LLaMA 8B和17B模型显示出更高的精确率和F1分数。

Conclusion: 轻量级的提示工程可以增强LLM在生物医学NLP任务中的效用。

Abstract: Coreference resolution in biomedical texts presents unique challenges due to
complex domain-specific terminology, high ambiguity in mention forms, and
long-distance dependencies between coreferring expressions. In this work, we
present a comprehensive evaluation of generative large language models (LLMs)
for coreference resolution in the biomedical domain. Using the CRAFT corpus as
our benchmark, we assess the LLMs' performance with four prompting experiments
that vary in their use of local, contextual enrichment, and domain-specific
cues such as abbreviations and entity dictionaries. We benchmark these
approaches against a discriminative span-based encoder, SpanBERT, to compare
the efficacy of generative versus discriminative methods. Our results
demonstrate that while LLMs exhibit strong surface-level coreference
capabilities, especially when supplemented with domain-grounding prompts, their
performance remains sensitive to long-range context and mentions ambiguity.
Notably, the LLaMA 8B and 17B models show superior precision and F1 scores
under entity-augmented prompting, highlighting the potential of lightweight
prompt engineering for enhancing LLM utility in biomedical NLP tasks.

</details>


### [29] [DEBATE: A Large-Scale Benchmark for Role-Playing LLM Agents in Multi-Agent, Long-Form Debates](https://arxiv.org/abs/2510.25110)
*Yun-Shiuan Chuang,Ruixuan Tu,Chengtao Dai,Smit Vasani,Binwei Yao,Michael Henry Tessler,Sijia Yang,Dhavan Shah,Robert Hawkins,Junjie Hu,Timothy T. Rogers*

Main category: cs.CL

TL;DR: DEBATE是一个用于评估多智能体LLM交互真实性的基准，包含29,417条辩论对话消息。


<details>
  <summary>Details</summary>
Motivation: 准确建模社交互动中的观点变化对于解决虚假信息和两极分化等问题至关重要。现有的LLM角色扮演设置经常产生不自然的动态，缺乏衡量真实人类观点轨迹的经验基准。

Method: 引入DEBATE，这是一个大规模的经验基准，旨在评估多智能体角色扮演LLM之间交互的真实性。通过监督微调，使LLM与人类行为对齐。

Result: 系统地评估并识别了模拟和真实群体动态之间的关键差异。在表面指标（例如，ROUGE-L和消息长度）方面取得了改进，同时强调了更深层语义对齐的局限性（例如，语义相似性）。

Conclusion: 角色扮演LLM智能体在真实地模拟类人社交动态方面既有潜力，也存在局限性。

Abstract: Accurately modeling opinion change through social interactions is crucial for
addressing issues like misinformation and polarization. While role-playing
large language models (LLMs) offer a promising way to simulate human-like
interactions, existing research shows that single-agent alignment does not
guarantee authentic multi-agent group dynamics. Current LLM role-play setups
often produce unnatural dynamics (e.g., premature convergence), without an
empirical benchmark to measure authentic human opinion trajectories. To bridge
this gap, we introduce DEBATE, the first large-scale empirical benchmark
explicitly designed to evaluate the authenticity of the interaction between
multi-agent role-playing LLMs. DEBATE contains 29,417 messages from multi-round
debate conversations among over 2,792 U.S.-based participants discussing 107
controversial topics, capturing both publicly-expressed messages and
privately-reported opinions. Using DEBATE, we systematically evaluate and
identify critical discrepancies between simulated and authentic group dynamics.
We further demonstrate DEBATE's utility for aligning LLMs with human behavior
through supervised fine-tuning, achieving improvements in surface-level metrics
(e.g., ROUGE-L and message length) while highlighting limitations in deeper
semantic alignment (e.g., semantic similarity). Our findings highlight both the
potential and current limitations of role-playing LLM agents for realistically
simulating human-like social dynamics.

</details>


### [30] [Pretraining Strategies using Monolingual and Parallel Data for Low-Resource Machine Translation](https://arxiv.org/abs/2510.25116)
*Idriss Nguepi Nguefack,Mara Finkelstein,Toadoum Sari Sakayo*

Main category: cs.CL

TL;DR: 研究针对低资源语言的机器翻译模型，探讨了不同的预训练策略。


<details>
  <summary>Details</summary>
Motivation: 旨在弥合高资源和低资源语言之间的性能差距，为边缘化群体和代表性不足的人群开发更具包容性和准确性的NLP模型。

Method: 通过一系列综合实验，探索了不同的预训练方法，包括整合多种语言以及在预训练阶段使用单语和并行数据。

Result: 多语言预训练以及利用单语和并行数据显著提高了翻译质量。

Conclusion: 为低资源机器翻译的有效预训练策略提供了宝贵的见解，有助于为边缘化群体和代表性不足的人群开发更具包容性和准确性的NLP模型。

Abstract: This research article examines the effectiveness of various pretraining
strategies for developing machine translation models tailored to low-resource
languages. Although this work considers several low-resource languages,
including Afrikaans, Swahili, and Zulu, the translation model is specifically
developed for Lingala, an under-resourced African language, building upon the
pretraining approach introduced by Reid and Artetxe (2021), originally designed
for high-resource languages. Through a series of comprehensive experiments, we
explore different pretraining methodologies, including the integration of
multiple languages and the use of both monolingual and parallel data during the
pretraining phase. Our findings indicate that pretraining on multiple languages
and leveraging both monolingual and parallel data significantly enhance
translation quality. This study offers valuable insights into effective
pretraining strategies for low-resource machine translation, helping to bridge
the performance gap between high-resource and low-resource languages. The
results contribute to the broader goal of developing more inclusive and
accurate NLP models for marginalized communities and underrepresented
populations. The code and datasets used in this study are publicly available to
facilitate further research and ensure reproducibility, with the exception of
certain data that may no longer be accessible due to changes in public
availability.

</details>


### [31] [A Survey on Unlearning in Large Language Models](https://arxiv.org/abs/2510.25117)
*Ruichen Qiu,Jiajun Tan,Jiayue Pu,Honglin Wang,Xiao-Shan Gao,Fei Sun*

Main category: cs.CL

TL;DR: 该调查综述了自 2021 年以来发表的 180 多篇关于 LLM 机器学习消除（unlearning）的论文，重点关注大型生成模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 的进步彻底改变了自然语言处理，但它们在海量语料库上的训练带来了重大风险，包括记忆敏感的个人数据、受版权保护的材料以及可能助长恶意活动的知识。为了减轻这些问题并符合“被遗忘权”等法律和道德标准，机器学习消除已成为一种关键技术，可以在不影响 LLM 整体性能的情况下有选择地删除特定知识。

Method: 该调查对 LLM 机器学习消除方法和评估引入了新的分类方法，根据应用机器学习消除的训练阶段将方法分为训练时、训练后和推理时。对于评估，系统地编制了现有数据集和指标，并批判性地分析了它们的优缺点和适用性，为研究界提供实用指导。

Result: 该调查旨在告知并指导安全可靠的 LLM 的持续开发。

Conclusion: 该调查全面概述了 LLM 机器学习消除领域，旨在促进安全可靠的 LLM 的开发。

Abstract: The advancement of Large Language Models (LLMs) has revolutionized natural
language processing, yet their training on massive corpora poses significant
risks, including the memorization of sensitive personal data, copyrighted
material, and knowledge that could facilitate malicious activities. To mitigate
these issues and align with legal and ethical standards such as the "right to
be forgotten", machine unlearning has emerged as a critical technique to
selectively erase specific knowledge from LLMs without compromising their
overall performance. This survey provides a systematic review of over 180
papers on LLM unlearning published since 2021, focusing exclusively on
large-scale generative models. Distinct from prior surveys, we introduce novel
taxonomies for both unlearning methods and evaluations. We clearly categorize
methods into training-time, post-training, and inference-time based on the
training stage at which unlearning is applied. For evaluations, we not only
systematically compile existing datasets and metrics but also critically
analyze their advantages, disadvantages, and applicability, providing practical
guidance to the research community. In addition, we discuss key challenges and
promising future research directions. Our comprehensive overview aims to inform
and guide the ongoing development of secure and reliable LLMs.

</details>


### [32] [Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR](https://arxiv.org/abs/2510.25150)
*Shreyas Gopal,Ashutosh Anshul,Haoyang Li,Yue Heng Yeo,Hexin Liu,Eng Siong Chng*

Main category: cs.CL

TL;DR: 该论文提出了一种用于语音建模的离散音频表示方法，旨在将语义语音内容与背景噪声分离。


<details>
  <summary>Details</summary>
Motivation: 现有的离散音频表示方法在嘈杂或真实环境中表现不佳。

Method: 该模型通过量化Whisper嵌入，将干净语音分离为码本tokens，并将可解释的噪声向量提取为量化残差，并通过轻量级分类器进行监督。

Result: 该方法提高了干净/嘈杂语音和文本之间的对齐，产生了具有高度噪声不变性的语音tokens，并提高了ASR性能。与Whisper相比，错误率降低了82%，与VBDemand测试集上的基线方法相比，提高了35%。

Conclusion: 该方法学习的token空间可以很好地推广到已知和未知的声学条件。

Abstract: Discrete audio representations are gaining traction in speech modeling due to
their interpretability and compatibility with large language models, but are
not always optimized for noisy or real-world environments. Building on existing
works that quantize Whisper embeddings for speech-to-unit modeling, we propose
disentangling semantic speech content from background noise in the latent
space. Our end-to-end model separates clean speech in the form of codebook
tokens, while extracting interpretable noise vectors as quantization residue
which are supervised via a lightweight classifier. We show that our approach
improves alignment between clean/noisy speech and text, producing speech tokens
that display a high degree of noiseinvariance, and improves ASR performance.
Keeping Whisper frozen, we show an 82% reduction in error rate compared to
Whisper, and 35% improvement over baseline methods on the VBDemand test set.
Further analyses show that the learned token space generalizes well to both
seen and unseen acoustic conditions.

</details>


### [33] [Testing Cross-Lingual Text Comprehension In LLMs Using Next Sentence Prediction](https://arxiv.org/abs/2510.25187)
*Ritesh Sunil Chavan,Jack Mostow*

Main category: cs.CL

TL;DR: 大型语言模型在低资源语言上的表现不如英语，显示出对训练数据的依赖性。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在低资源语言环境下的性能，以确定其能力是否仅仅是数据优势的结果。

Method: 创建包含英语、斯瓦希里语和豪萨语的大规模基准测试，使用下一句预测（NSP）作为测试方法，并评估GPT-4 Turbo、Gemini 1.5 Flash和LLaMA 3 70B等模型的性能。

Result: 所有模型在英语上表现出色，但在斯瓦希里语和豪萨语上的准确率下降，特别是LLaMA 3。Chain-of-Thought (CoT) prompting 提高了LLaMA 3的准确率，但对GPT-4和Gemini产生了负面影响。

Conclusion: Chain-of-Thought并非通用解决方案，其有效性取决于模型的基础能力和任务的具体背景。该框架可以有效定位LLM的弱点。

Abstract: While large language models are trained on massive datasets, this data is
heavily skewed towards English. Does their impressive performance reflect
genuine ability or just this data advantage? To find out, we tested them in a
setting where they could not rely on data abundance: low-resource languages.
Building on prior work Agarwal et al. (2025) that used Next Sentence Prediction
(NSP) as a test, we created a large-scale benchmark with 10,000 questions each
for English (a high-resource language), Swahili (medium-resource), and Hausa
(low-resource). We then tested several top models, including GPT-4 Turbo,
Gemini 1.5 Flash, and LLaMA 3 70B, to see how their performance holds up. The
results painted a clear picture of how levels of language resources impact
outcomes. While all models excelled in English, their accuracy dropped in
Swahili and fell sharply in Hausa, with LLaMA 3 struggling the most. The story
became even more interesting when we introduced Chain-of-Thought (CoT)
prompting. For the struggling LLaMA 3, CoT acted as a helpful guide,
significantly boosting its accuracy. However, for the more capable GPT-4 and
Gemini, the same technique often backfired, leading to a kind of "overthinking"
that hurt their results in the cross-lingual context. This reveals that
Chain-of-Thought is not a universal solution; its effectiveness depends heavily
on the model's baseline capability and the specific context of the task. Our
framework pinpoints LLM weaknesses, highlights when CoT helps or hinders
cross-lingual NSP performance, and factors influencing their decisions.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [34] [DrivingScene: A Multi-Task Online Feed-Forward 3D Gaussian Splatting Method for Dynamic Driving Scenes](https://arxiv.org/abs/2510.24734)
*Qirui Hou,Wenzhang Sun,Chang Zeng,Chunfeng Wang,Hao Li,Jianxun Cui*

Main category: cs.CV

TL;DR: 提出 DrivingScene，一个仅从两个连续环视图像重建 4D 动态场景的在线前馈框架。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以平衡质量和效率，动态驾驶场景的实时、高保真重建具有挑战性，

Method: 关键创新是一个轻量级的残差流网络，该网络在学习到的静态场景先验之上预测每个摄像机动态对象的非刚性运动，通过场景流显式地对动态进行建模。引入了从粗到精的训练范例，避免了端到端方法常见的 不稳定性。

Result: 在 nuScenes 数据集上的实验表明，我们的纯图像方法可以同时在线生成高质量的深度、场景流和 3D 高斯点云，在动态重建和新颖的视图合成方面显着优于最先进的方法。

Conclusion: 提出 DrivingScene 框架，实现了高质量的动态驾驶场景重建。

Abstract: Real-time, high-fidelity reconstruction of dynamic driving scenes is
challenged by complex dynamics and sparse views, with prior methods struggling
to balance quality and efficiency. We propose DrivingScene, an online,
feed-forward framework that reconstructs 4D dynamic scenes from only two
consecutive surround-view images. Our key innovation is a lightweight residual
flow network that predicts the non-rigid motion of dynamic objects per camera
on top of a learned static scene prior, explicitly modeling dynamics via scene
flow. We also introduce a coarse-to-fine training paradigm that circumvents the
instabilities common to end-to-end approaches. Experiments on nuScenes dataset
show our image-only method simultaneously generates high-quality depth, scene
flow, and 3D Gaussian point clouds online, significantly outperforming
state-of-the-art methods in both dynamic reconstruction and novel view
synthesis.

</details>


### [35] [Towards Fine-Grained Human Motion Video Captioning](https://arxiv.org/abs/2510.24767)
*Guorui Song,Guocun Wang,Zhe Huang,Jing Lin,Xuefei Zhe,Jian Li,Haoqian Wang*

Main category: cs.CV

TL;DR: 提出了一个运动增强的字幕模型（M-ACM），通过结合人体网格恢复中的运动信息来改善视频字幕的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频字幕模型难以捕捉细粒度的运动细节，导致字幕模糊或语义不一致。

Method: M-ACM利用从人体网格恢复中提取的运动表示来突出人体动态，从而减少幻觉，提高语义保真度和空间对齐。

Result: M-ACM在描述复杂的人体运动和细微的时间变化方面显著优于现有方法。

Conclusion: M-ACM为运动中心的视频字幕设定了新的标准，并发布了一个名为HMI的数据集和基准测试。

Abstract: Generating accurate descriptions of human actions in videos remains a
challenging task for video captioning models. Existing approaches often
struggle to capture fine-grained motion details, resulting in vague or
semantically inconsistent captions. In this work, we introduce the
Motion-Augmented Caption Model (M-ACM), a novel generative framework that
enhances caption quality by incorporating motion-aware decoding. At its core,
M-ACM leverages motion representations derived from human mesh recovery to
explicitly highlight human body dynamics, thereby reducing hallucinations and
improving both semantic fidelity and spatial alignment in the generated
captions. To support research in this area, we present the Human Motion Insight
(HMI) Dataset, comprising 115K video-description pairs focused on human
movement, along with HMI-Bench, a dedicated benchmark for evaluating
motion-focused video captioning. Experimental results demonstrate that M-ACM
significantly outperforms previous methods in accurately describing complex
human motions and subtle temporal variations, setting a new standard for
motion-centric video captioning.

</details>


### [36] [Combining SAR Simulators to Train ATR Models with Synthetic Data](https://arxiv.org/abs/2510.24768)
*Benjamin Camus,Julien Houssay,Corentin Le Barbu,Eric Monteux,Cédric Saleun,Christian Cochin*

Main category: cs.CV

TL;DR: 本研究旨在使用合成孔径雷达 (SAR) 图像训练深度学习模型以执行自动目标识别 (ATR)。


<details>
  <summary>Details</summary>
Motivation: 为了规避缺乏真实标记测量的问题，我们求助于 SAR 模拟器生成的合成数据。然而，在合成图像上训练的 ATR 模型无法很好地推广到真实测量。

Method: 我们提出了一种新方法来解决 ATR 问题：结合两个基于不同（但互补）范例的 SAR 模拟器来生成合成数据集。为此，我们使用了两个模拟器：MOCEM 和 Salsa。我们使用 MOCEM 和 Salsa 生成的合成数据集训练 ATR 模型，以及我们称为 ADASCA 的深度学习方法。

Result: 在 MSTAR 测量中，我们达到了近 88% 的准确率。

Conclusion: 结合两个 SAR 模拟器，可以提高 ATR 模型的准确率。

Abstract: This work aims to train Deep Learning models to perform Automatic Target
Recognition (ATR) on Synthetic Aperture Radar (SAR) images. To circumvent the
lack of real labelled measurements, we resort to synthetic data produced by SAR
simulators. Simulation offers full control over the virtual environment, which
enables us to generate large and diversified datasets at will. However,
simulations are intrinsically grounded on simplifying assumptions of the real
world (i.e. physical models). Thus, synthetic datasets are not as
representative as real measurements. Consequently, ATR models trained on
synthetic images cannot generalize well on real measurements. Our contributions
to this problem are twofold: on one hand, we demonstrate and quantify the
impact of the simulation paradigm on the ATR. On the other hand, we propose a
new approach to tackle the ATR problem: combine two SAR simulators that are
grounded on different (but complementary) paradigms to produce synthetic
datasets. To this end, we use two simulators: MOCEM, which is based on a
scattering centers model approach, and Salsa, which resorts on a ray tracing
strategy. We train ATR models using synthetic dataset generated both by MOCEM
and Salsa and our Deep Learning approach called ADASCA. We reach an accuracy of
almost 88 % on the MSTAR measurements.

</details>


### [37] [Point-level Uncertainty Evaluation of Mobile Laser Scanning Point Clouds](https://arxiv.org/abs/2510.24773)
*Ziyang Xu,Olaf Wysocki,Christoph Holst*

Main category: cs.CV

TL;DR: 提出了一种基于机器学习的框架，用于点云不确定性评估，避免了对高精度参考数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统的不确定性建模依赖于高精度参考数据，这在大规模情况下通常成本高昂或不可行。

Method: 使用随机森林（RF）和XGBoost两种集成学习模型，通过学习局部几何特征与点级误差之间的关系来进行不确定性评估。

Result: 实验结果表明，这两种模型都能有效地捕捉几何特征和不确定性之间的非线性关系，平均ROC-AUC值均高于0.87。海拔变化、点密度和局部结构复杂性等几何特征在预测不确定性方面起着主导作用。

Conclusion: 该框架提供了一种数据驱动的不确定性评估方法，为大规模点云的未来质量控制和误差分析提供了可扩展和适应性强的基础。

Abstract: Reliable quantification of uncertainty in Mobile Laser Scanning (MLS) point
clouds is essential for ensuring the accuracy and credibility of downstream
applications such as 3D mapping, modeling, and change analysis. Traditional
backward uncertainty modeling heavily rely on high-precision reference data,
which are often costly or infeasible to obtain at large scales. To address this
issue, this study proposes a machine learning-based framework for point-level
uncertainty evaluation that learns the relationship between local geometric
features and point-level errors. The framework is implemented using two
ensemble learning models, Random Forest (RF) and XGBoost, which are trained and
validated on a spatially partitioned real-world dataset to avoid data leakage.
Experimental results demonstrate that both models can effectively capture the
nonlinear relationships between geometric characteristics and uncertainty,
achieving mean ROC-AUC values above 0.87. The analysis further reveals that
geometric features describing elevation variation, point density, and local
structural complexity play a dominant role in predicting uncertainty. The
proposed framework offers a data-driven perspective of uncertainty evaluation,
providing a scalable and adaptable foundation for future quality control and
error analysis of large-scale point clouds.

</details>


### [38] [Cross-Enhanced Multimodal Fusion of Eye-Tracking and Facial Features for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2510.24777)
*Yujie Nie,Jianzhang Ni,Yonglong Ye,Yuan-Ting Zhang,Yun Kwok Wing,Xiangqing Xu,Xin Ma,Lizhou Fan*

Main category: cs.CV

TL;DR: 本文提出了一种多模态交叉增强融合框架，该框架结合了眼动追踪和面部特征以辅助阿尔茨海默病 (AD) 的诊断。


<details>
  <summary>Details</summary>
Motivation: 早期准确诊断阿尔茨海默病 (AD) 对于及时干预和延缓疾病进展至关重要。眼动追踪和面部特征是认知功能的重要指标，反映了注意力分配和神经认知状态。但是，很少有研究探索它们在辅助 AD 诊断中的联合整合。

Method: 该框架包含两个关键模块：(a) 交叉增强融合注意力模块 (CEFAM)，它通过交叉注意力和全局增强来建模模态间交互；(b) 方向感知卷积模块 (DACM)，它通过水平-垂直感受野捕获细粒度的方向性面部特征。

Result: 在包含 25 名 AD 患者和 25 名健康对照 (HC) 的同步多模态数据集上进行的实验表明，该框架优于传统的后期融合和特征连接方法，在区分 AD 和 HC 方面的分类准确率达到 95.11%。

Conclusion: 通过显式建模模态间依赖性和模态特定贡献，突出了卓越的鲁棒性和诊断性能。

Abstract: Accurate diagnosis of Alzheimer's disease (AD) is essential for enabling
timely intervention and slowing disease progression. Multimodal diagnostic
approaches offer considerable promise by integrating complementary information
across behavioral and perceptual domains. Eye-tracking and facial features, in
particular, are important indicators of cognitive function, reflecting
attentional distribution and neurocognitive state. However, few studies have
explored their joint integration for auxiliary AD diagnosis. In this study, we
propose a multimodal cross-enhanced fusion framework that synergistically
leverages eye-tracking and facial features for AD detection. The framework
incorporates two key modules: (a) a Cross-Enhanced Fusion Attention Module
(CEFAM), which models inter-modal interactions through cross-attention and
global enhancement, and (b) a Direction-Aware Convolution Module (DACM), which
captures fine-grained directional facial features via horizontal-vertical
receptive fields. Together, these modules enable adaptive and discriminative
multimodal representation learning. To support this work, we constructed a
synchronized multimodal dataset, including 25 patients with AD and 25 healthy
controls (HC), by recording aligned facial video and eye-tracking sequences
during a visual memory-search paradigm, providing an ecologically valid
resource for evaluating integration strategies. Extensive experiments on this
dataset demonstrate that our framework outperforms traditional late fusion and
feature concatenation methods, achieving a classification accuracy of 95.11% in
distinguishing AD from HC, highlighting superior robustness and diagnostic
performance by explicitly modeling inter-modal dependencies and
modality-specific contributions.

</details>


### [39] [FPGA-based Lane Detection System incorporating Temperature and Light Control Units](https://arxiv.org/abs/2510.24778)
*Ibrahim Qamar,Saber Mahmoud,Seif Megahed,Mohamed Khaled,Saleh Hesham,Ahmed Matar,Saif Gebril,Mervat Mahmoud*

Main category: cs.CV

TL;DR: 本文提出了一种基于FPGA的车道检测器车辆（LDV）架构，该架构依赖于Sobel算法进行边缘检测。


<details>
  <summary>Details</summary>
Motivation: 智能车辆是自动化趋势的重要成果之一，车道路径检测是智能车辆在城市道路或机器人轨道中的首要任务。

Method: 该系统采用Sobel算法进行边缘检测，处理416 x 416图像，频率为150 MHz。

Result: 该系统可以每1.17 ms生成一个有效输出，包括当前车道的数量、当前车道索引以及左右边界。此外，自动光照和温度控制单元增强了系统对周围环境条件的适应性。

Conclusion: 所提出的系统能够快速准确地检测车道，并具有良好的环境适应性。

Abstract: Intelligent vehicles are one of the most important outcomes gained from the
world tendency toward automation. Applications of IVs, whether in urban roads
or robot tracks, do prioritize lane path detection. This paper proposes an
FPGA-based Lane Detector Vehicle LDV architecture that relies on the Sobel
algorithm for edge detection. Operating on 416 x 416 images and 150 MHz, the
system can generate a valid output every 1.17 ms. The valid output consists of
the number of present lanes, the current lane index, as well as its right and
left boundaries. Additionally, the automated light and temperature control
units in the proposed system enhance its adaptability to the surrounding
environmental conditions.

</details>


### [40] [ESCA: Enabling Seamless Codec Avatar Execution through Algorithm and Hardware Co-Optimization for Virtual Reality](https://arxiv.org/abs/2510.24787)
*Mingzhi Zhu,Ding Shang,Sai Qian Zhang*

Main category: cs.CV

TL;DR: 提出了一种高效的后训练量化 (PTQ) 方法，并设计了一个定制硬件加速器，以加速边缘 VR 平台上 PCA 推理。


<details>
  <summary>Details</summary>
Motivation: 在虚拟现实 (VR) 环境中使用逼真的编解码器化身 (PCA) 通过基于深度学习的生成模型实现沉浸式通信和交互，但这些模型对计算要求很高，这使得资源受限的 VR 设备上的实时推理具有挑战性。

Method: 提出了一种专为编解码器化身模型量身定制的高效后训练量化 (PTQ) 方法，并设计了一个可以集成到 VR 设备的片上系统的定制硬件加速器。

Result: ESCA 将 FovVideoVDP 质量分数提高了 $+0.39$，延迟降低了 $3.36	 imes$，并在端到端测试中保持了每秒 100 帧的渲染速度。

Conclusion: 该结果证明了在资源受限的设备上部署高保真编解码器化身的可行性，为更具沉浸式和便携性的 VR 体验打开了大门。

Abstract: Photorealistic Codec Avatars (PCA), which generate high-fidelity human face
renderings, are increasingly being used in Virtual Reality (VR) environments to
enable immersive communication and interaction through deep learning-based
generative models. However, these models impose significant computational
demands, making real-time inference challenging on resource-constrained VR
devices such as head-mounted displays, where latency and power efficiency are
critical. To address this challenge, we propose an efficient post-training
quantization (PTQ) method tailored for Codec Avatar models, enabling
low-precision execution without compromising output quality. In addition, we
design a custom hardware accelerator that can be integrated into the
system-on-chip of VR devices to further enhance processing efficiency. Building
on these components, we introduce ESCA, a full-stack optimization framework
that accelerates PCA inference on edge VR platforms. Experimental results
demonstrate that ESCA boosts FovVideoVDP quality scores by up to $+0.39$ over
the best 4-bit baseline, delivers up to $3.36\times$ latency reduction, and
sustains a rendering rate of 100 frames per second in end-to-end tests,
satisfying real-time VR requirements. These results demonstrate the feasibility
of deploying high-fidelity codec avatars on resource-constrained devices,
opening the door to more immersive and portable VR experiences.

</details>


### [41] [The Underappreciated Power of Vision Models for Graph Structural Understanding](https://arxiv.org/abs/2510.24788)
*Xinjian Zhao,Wei Pang,Zhongkai Xue,Xiangru Jian,Lei Zhang,Yaoyao Xu,Xiaozhuang Song,Shu Wu,Tianshu Yu*

Main category: cs.CV

TL;DR: 视觉模型在图结构理解方面具有潜力，尤其是在需要全局拓扑意识和尺度不变推理的问题中，胜过GNN。


<details>
  <summary>Details</summary>
Motivation: 图神经网络通过自下而上的消息传递进行操作，与人类视觉感知根本不同，人类视觉感知首先直观地捕捉全局结构。研究视觉模型在图理解中未被充分重视的潜力。

Method: 使用vision模型，并设计GraphAbstract基准测试，该基准测试评估模型感知全局图属性的能力。

Result: 视觉模型在需要整体结构理解的任务上明显优于 GNN，并且在不同的图尺度上保持泛化能力，而 GNN 在全局模式抽象方面存在困难，并且随着图尺寸的增加而降低。

Conclusion: 视觉模型在图结构理解方面具有卓越但未被充分利用的能力，特别是在需要全局拓扑意识和尺度不变推理的问题中。这些发现为利用这种未被充分重视的潜力开辟了新途径，以开发更有效的图基础模型，用于以整体模式识别为主的任务。

Abstract: Graph Neural Networks operate through bottom-up message-passing,
fundamentally differing from human visual perception, which intuitively
captures global structures first. We investigate the underappreciated potential
of vision models for graph understanding, finding they achieve performance
comparable to GNNs on established benchmarks while exhibiting distinctly
different learning patterns. These divergent behaviors, combined with
limitations of existing benchmarks that conflate domain features with
topological understanding, motivate our introduction of GraphAbstract. This
benchmark evaluates models' ability to perceive global graph properties as
humans do: recognizing organizational archetypes, detecting symmetry, sensing
connectivity strength, and identifying critical elements. Our results reveal
that vision models significantly outperform GNNs on tasks requiring holistic
structural understanding and maintain generalizability across varying graph
scales, while GNNs struggle with global pattern abstraction and degrade with
increasing graph size. This work demonstrates that vision models possess
remarkable yet underutilized capabilities for graph structural understanding,
particularly for problems requiring global topological awareness and
scale-invariant reasoning. These findings open new avenues to leverage this
underappreciated potential for developing more effective graph foundation
models for tasks dominated by holistic pattern recognition.

</details>


### [42] [A Re-node Self-training Approach for Deep Graph-based Semi-supervised Classification on Multi-view Image Data](https://arxiv.org/abs/2510.24791)
*Jingjun Bi,Fadi Dornaika*

Main category: cs.CV

TL;DR: 本文提出了一种新的多视图半监督学习方法，通过结合图卷积网络和伪标签来提高分类性能，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理图像等多视图数据时，缺乏清晰的图结构，且图结构与多视图数据的结合仍具挑战性。

Method: 该方法结合线性特征变换和多视图图融合，动态地将伪标签融入GCN损失函数，并通过调整类边界附近标记样本的权重来纠正拓扑失衡。此外，还引入了一种适用于所有样本的无监督平滑损失。

Result: 在多视图基准图像数据集上的实验结果表明，RSGSLM 优于现有的多视图半监督学习方法。

Conclusion: RSGSLM方法在多视图半监督学习中表现出色，优于现有方法。

Abstract: Recently, graph-based semi-supervised learning and pseudo-labeling have
gained attention due to their effectiveness in reducing the need for extensive
data annotations. Pseudo-labeling uses predictions from unlabeled data to
improve model training, while graph-based methods are characterized by
processing data represented as graphs. However, the lack of clear graph
structures in images combined with the complexity of multi-view data limits the
efficiency of traditional and existing techniques. Moreover, the integration of
graph structures in multi-view data is still a challenge. In this paper, we
propose Re-node Self-taught Graph-based Semi-supervised Learning for Multi-view
Data (RSGSLM). Our method addresses these challenges by (i) combining linear
feature transformation and multi-view graph fusion within a Graph Convolutional
Network (GCN) framework, (ii) dynamically incorporating pseudo-labels into the
GCN loss function to improve classification in multi-view data, and (iii)
correcting topological imbalances by adjusting the weights of labeled samples
near class boundaries. Additionally, (iv) we introduce an unsupervised
smoothing loss applicable to all samples. This combination optimizes
performance while maintaining computational efficiency. Experimental results on
multi-view benchmark image datasets demonstrate that RSGSLM surpasses existing
semi-supervised learning approaches in multi-view contexts.

</details>


### [43] [PISA-Bench: The PISA Index as a Multilingual and Multimodal Metric for the Evaluation of Vision-Language Models](https://arxiv.org/abs/2510.24792)
*Patrick Haller,Fabio Barth,Jonas Golde,Georg Rehm,Alan Akbik*

Main category: cs.CV

TL;DR: 提出了一个多语言的视觉语言推理benchmark，名为PISA-Bench，它基于专家创建的PISA测试，包含六种语言。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型benchmark缺乏高质量、人工验证的例子，且大多是英文数据集，翻译后的质量难以保证。

Method: 构建了一个多语言benchmark，从英文PISA测试题翻译成其他五种语言（西班牙语、德语、中文、法语和意大利语），形成了一个完全平行的六种语言语料库。

Result: 评估了当前先进的视觉语言模型在PISA-Bench上的表现，发现小模型（<20B参数）表现不佳，且在非英语数据集以及空间和几何推理任务上性能显著下降。

Conclusion: 发布了PISA-Bench数据集和评估框架，为多语言多模态推理的研究提供资源。

Abstract: Vision-language models (VLMs) have demonstrated remarkable progress in
multimodal reasoning. However, existing benchmarks remain limited in terms of
high-quality, human-verified examples. Many current datasets rely on
synthetically generated content by large language models (LLMs). Furthermore,
most datasets are limited to English, as manual quality assurance of translated
samples is time-consuming and costly. To fill this gap, we introduce
PISA-Bench, a multilingual benchmark derived from English examples of the
expert-created PISA tests, a unified framework for the assessment of student
competencies in over eighty countries. Each example consists of human-extracted
instructions, questions, answer options, and images, enriched with question
type categories, and has been translated from English into five additional
languages (Spanish, German, Chinese, French, and Italian), resulting in a fully
parallel corpus covering six languages. We evaluate state-of-the-art
vision-language models on PISA-Bench and find that especially small models
(<20B parameters) fail to achieve high test scores. We further find substantial
performance degradation on non-English splits as well as high error-rates when
models are tasked with spatial and geometric reasoning. By releasing the
dataset and evaluation framework, we provide a resource for advancing research
on multilingual multimodal reasoning.

</details>


### [44] [A Survey on Efficient Vision-Language-Action Models](https://arxiv.org/abs/2510.24795)
*Zhaoshu Yu,Bo Wang,Pengpeng Zeng,Haonan Zhang,Ji Zhang,Lianli Gao,Jingkuan Song,Nicu Sebe,Heng Tao Shen*

Main category: cs.CV

TL;DR: 本调查全面回顾了高效视觉-语言-动作模型（Efficient VLA），旨在桥接数字知识与物理世界交互，同时降低计算和数据需求。


<details>
  <summary>Details</summary>
Motivation: VLA模型在具身智能领域具有重要意义，但其部署受到大规模基础模型带来的巨大计算和数据需求的严重阻碍。为了应对这些挑战，本调查应运而生。

Method: 本调查提出了一个统一的分类法，将当前技术分为三个核心支柱：(1)高效模型设计，(2)高效训练，(3)高效数据收集。通过对该框架内最新方法的批判性回顾，为社区建立了一个基础参考。

Result: 本调查总结了代表性应用，描述了关键挑战，并为未来的研究绘制了蓝图。维护一个不断更新的项目页面以跟踪最新进展。

Conclusion: 本调查为高效VLA领域的研究提供了一个全面的概述和未来方向。

Abstract: Vision-Language-Action models (VLAs) represent a significant frontier in
embodied intelligence, aiming to bridge digital knowledge with physical-world
interaction. While these models have demonstrated remarkable generalist
capabilities, their deployment is severely hampered by the substantial
computational and data requirements inherent to their underlying large-scale
foundation models. Motivated by the urgent need to address these challenges,
this survey presents the first comprehensive review of Efficient
Vision-Language-Action models (Efficient VLAs) across the entire
data-model-training process. Specifically, we introduce a unified taxonomy to
systematically organize the disparate efforts in this domain, categorizing
current techniques into three core pillars: (1) Efficient Model Design,
focusing on efficient architectures and model compression; (2) Efficient
Training, which reduces computational burdens during model learning; and (3)
Efficient Data Collection, which addresses the bottlenecks in acquiring and
utilizing robotic data. Through a critical review of state-of-the-art methods
within this framework, this survey not only establishes a foundational
reference for the community but also summarizes representative applications,
delineates key challenges, and charts a roadmap for future research. We
maintain a continuously updated project page to track our latest developments:
https://evla-survey.github.io/

</details>


### [45] [Conflict Adaptation in Vision-Language Models](https://arxiv.org/abs/2510.24804)
*Xiaoyang Hu*

Main category: cs.CV

TL;DR: 该研究发现，类似于人类的认知控制，视觉-语言模型(VLM)在连续Stroop任务中表现出冲突适应现象。通过稀疏自编码器(SAE)识别出模型中的任务相关节点，发现文本和颜色相关的节点在不同层中部分重叠，且大小反映了阅读和颜色命名之间的自动性不对称。此外，还在模型中找到了一个冲突调节节点，去除它会显著增加Stroop错误。


<details>
  <summary>Details</summary>
Motivation: 探究视觉-语言模型(VLM)是否具备类似人类的认知控制能力，以及这种能力在模型中的表征基础。

Method: 使用连续Stroop任务测试13个视觉-语言模型(VLM)，并通过稀疏自编码器(SAE)识别InternVL 3.5 4B模型中的任务相关超节点。

Result: 发现13个视觉-语言模型中有12个表现出与冲突适应一致的行为。在模型中发现文本和颜色相关的超节点在不同层中部分重叠，且大小反映了阅读和颜色命名之间的自动性不对称。分离出一个冲突调节超节点，去除它会显著增加Stroop错误，而对一致性试验影响很小。

Conclusion: 视觉-语言模型(VLM)在一定程度上模拟了人类的认知控制机制，并且可以通过分析模型内部的表征来理解这种机制的运作方式。

Abstract: A signature of human cognitive control is conflict adaptation: improved
performance on a high-conflict trial following another high-conflict trial.
This phenomenon offers an account for how cognitive control, a scarce resource,
is recruited. Using a sequential Stroop task, we find that 12 of 13
vision-language models (VLMs) tested exhibit behavior consistent with conflict
adaptation, with the lone exception likely reflecting a ceiling effect. To
understand the representational basis of this behavior, we use sparse
autoencoders (SAEs) to identify task-relevant supernodes in InternVL 3.5 4B.
Partially overlapping supernodes emerge for text and color in both early and
late layers, and their relative sizes mirror the automaticity asymmetry between
reading and color naming in humans. We further isolate a conflict-modulated
supernode in layers 24-25 whose ablation significantly increases Stroop errors
while minimally affecting congruent trials.

</details>


### [46] [DualCap: Enhancing Lightweight Image Captioning via Dual Retrieval with Similar Scenes Visual Prompts](https://arxiv.org/abs/2510.24813)
*Binbin Li,Guimiao Yang,Zisen Qi,Haiping Wang,Yu Ding*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为 DualCap 的新方法，通过从检索到的相似图像生成视觉提示来丰富视觉表征，以解决现有检索增强图像字幕模型中存在的语义鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 现有的轻量级检索增强图像字幕模型通常仅将检索到的数据用作文本提示，忽略了原始视觉特征的增强，尤其是在对象细节或复杂场景方面，从而造成了语义鸿沟。

Method: 该模型采用双重检索机制，使用标准的图像到文本检索来生成文本提示，并使用一种新的图像到图像检索来寻找视觉上相似的场景。从视觉上相似的场景的字幕中提取显著的关键词和短语，以捕捉关键对象和相似的细节。然后，这些文本特征被编码并通过一个轻量级的、可训练的特征融合网络与原始图像特征融合。

Result: 大量的实验表明，该方法在实现具有竞争力的性能的同时，与以往的视觉提示字幕方法相比，需要更少的可训练参数。

Conclusion: 该研究提出了一种有效的视觉提示生成方法，可以有效地提升图像字幕模型的性能，并且具有参数量少的优点。

Abstract: Recent lightweight retrieval-augmented image caption models often utilize
retrieved data solely as text prompts, thereby creating a semantic gap by
leaving the original visual features unenhanced, particularly for object
details or complex scenes. To address this limitation, we propose $DualCap$, a
novel approach that enriches the visual representation by generating a visual
prompt from retrieved similar images. Our model employs a dual retrieval
mechanism, using standard image-to-text retrieval for text prompts and a novel
image-to-image retrieval to source visually analogous scenes. Specifically,
salient keywords and phrases are derived from the captions of visually similar
scenes to capture key objects and similar details. These textual features are
then encoded and integrated with the original image features through a
lightweight, trainable feature fusion network. Extensive experiments
demonstrate that our method achieves competitive performance while requiring
fewer trainable parameters compared to previous visual-prompting captioning
approaches.

</details>


### [47] [Deep Feature Optimization for Enhanced Fish Freshness Assessment](https://arxiv.org/abs/2510.24814)
*Phi-Hung Hoang,Nam-Thuan Trinh,Van-Manh Tran,Thi-Thu-Hong Phan*

Main category: cs.CV

TL;DR: 提出了一种用于评估鱼类新鲜度的三阶段框架，该框架结合了深度学习和传统机器学习方法，并在鱼眼新鲜度（FFE）数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的感官评估鱼类新鲜度的方法主观、耗时且不一致。虽然深度学习在视觉新鲜度预测方面有所进展，但仍然存在准确性和特征透明性方面的挑战。

Method: 该框架包括三个阶段：1) 微调五个最先进的视觉架构；2) 训练七个经典机器学习分类器；3) 基于LGBM、随机森林和Lasso的特征选择方法。

Result: 在鱼眼新鲜度（FFE）数据集上，最佳配置（Swin-Tiny特征、Extra Trees分类器和基于LGBM的特征选择）实现了85.99%的准确率，优于最近的研究8.69-22.78%。

Conclusion: 该研究证实了所提出的框架在视觉质量评估任务中的有效性和通用性。

Abstract: Assessing fish freshness is vital for ensuring food safety and minimizing
economic losses in the seafood industry. However, traditional sensory
evaluation remains subjective, time-consuming, and inconsistent. Although
recent advances in deep learning have automated visual freshness prediction,
challenges related to accuracy and feature transparency persist. This study
introduces a unified three-stage framework that refines and leverages deep
visual representations for reliable fish freshness assessment. First, five
state-of-the-art vision architectures - ResNet-50, DenseNet-121,
EfficientNet-B0, ConvNeXt-Base, and Swin-Tiny - are fine-tuned to establish a
strong baseline. Next, multi-level deep features extracted from these backbones
are used to train seven classical machine learning classifiers, integrating
deep and traditional decision mechanisms. Finally, feature selection methods
based on Light Gradient Boosting Machine (LGBM), Random Forest, and Lasso
identify a compact and informative subset of features. Experiments on the
Freshness of the Fish Eyes (FFE) dataset demonstrate that the best
configuration combining Swin-Tiny features, an Extra Trees classifier, and
LGBM-based feature selection achieves an accuracy of 85.99%, outperforming
recent studies on the same dataset by 8.69-22.78%. These findings confirm the
effectiveness and generalizability of the proposed framework for visual quality
evaluation tasks.

</details>


### [48] [Perception, Understanding and Reasoning, A Multimodal Benchmark for Video Fake News Detection](https://arxiv.org/abs/2510.24816)
*Cui Yakun,Fushuo Huo,Weijie Shi,Juntao Dai,Hang Du,Zhenghao Zhu,Sirui Han,Yike Guo*

Main category: cs.CV

TL;DR: 提出了一个用于评估多模态大型语言模型在视频假新闻检测（VFND）任务中能力的基准测试 MVFNDB。


<details>
  <summary>Details</summary>
Motivation: 现有的 VFND 基准测试通常只关注最终决策的准确性，缺乏对整个检测过程的细粒度评估，导致检测过程不透明。

Method: 构建了一个包含 10 个任务的 MVFNDB 基准测试，该基准测试包含 9730 个人工标注的视频相关问题，并设计了一个名为 MVFND-CoT 的新框架，该框架结合了创作者添加的内容和原始拍摄素材推理。

Result: 通过基准测试，深入分析了影响准确性的深层因素，包括视频处理策略以及视频特征与模型能力之间的一致性。

Conclusion: 该基准测试将为未来评估和改进 MLLM 在视频假新闻检测领域的能力奠定坚实的基础。

Abstract: The advent of multi-modal large language models (MLLMs) has greatly advanced
research into applications for Video fake news detection (VFND) tasks.
Traditional video-based FND benchmarks typically focus on the accuracy of the
final decision, often failing to provide fine-grained assessments for the
entire detection process, making the detection process a black box. Therefore,
we introduce the MVFNDB (Multi-modal Video Fake News Detection Benchmark) based
on the empirical analysis, which provides foundation for tasks definition. The
benchmark comprises 10 tasks and is meticulously crafted to probe MLLMs'
perception, understanding, and reasoning capacities during detection, featuring
9730 human-annotated video-related questions based on a carefully constructed
taxonomy ability of VFND. To validate the impact of combining multiple features
on the final results, we design a novel framework named MVFND-CoT, which
incorporates both creator-added content and original shooting footage
reasoning. Building upon the benchmark, we conduct an in-depth analysis of the
deeper factors influencing accuracy, including video processing strategies and
the alignment between video features and model capabilities. We believe this
benchmark will lay a solid foundation for future evaluations and advancements
of MLLMs in the domain of video fake news detection.

</details>


### [49] [SafeEditor: Unified MLLM for Efficient Post-hoc T2I Safety Editing](https://arxiv.org/abs/2510.24820)
*Ruiyang Zhang,Jiahao Luo,Xiaoru Feng,Qiufan Pang,Yaodong Yang,Juntao Dai*

Main category: cs.CV

TL;DR: 提出了一个多轮安全编辑框架，用于提高文本到图像模型的安全性，同时保持效用。


<details>
  <summary>Details</summary>
Motivation: 现有的安全方法在安全性和效用之间存在不平衡，并且存在过度拒绝的问题。

Method: 构建了一个多轮图像-文本交错数据集MR-SafeEdit，并提出了一个后验安全编辑范例，使用统一的MLLM SafeEditor进行多轮安全编辑。

Result: 实验结果表明，SafeEditor在降低过度拒绝的同时，实现了更好的安全-效用平衡。

Conclusion: 该框架作为一个模型无关的即插即用模块，能够有效地对任何文本到图像模型进行安全对齐。

Abstract: With the rapid advancement of text-to-image (T2I) models, ensuring their
safety has become increasingly critical. Existing safety approaches can be
categorized into training-time and inference-time methods. While inference-time
methods are widely adopted due to their cost-effectiveness, they often suffer
from limitations such as over-refusal and imbalance between safety and utility.
To address these challenges, we propose a multi-round safety editing framework
that functions as a model-agnostic, plug-and-play module, enabling efficient
safety alignment for any text-to-image model. Central to this framework is
MR-SafeEdit, a multi-round image-text interleaved dataset specifically
constructed for safety editing in text-to-image generation. We introduce a
post-hoc safety editing paradigm that mirrors the human cognitive process of
identifying and refining unsafe content. To instantiate this paradigm, we
develop SafeEditor, a unified MLLM capable of multi-round safety editing on
generated images. Experimental results show that SafeEditor surpasses prior
safety approaches by reducing over-refusal while achieving a more favorable
safety-utility balance.

</details>


### [50] [Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation](https://arxiv.org/abs/2510.24821)
*Inclusion AI,:,Bowen Ma,Cheng Zou,Canxiang Yan,Chunxiang Jin,Chunjie Shen,Dandan Zheng,Fudong Wang,Furong Xu,GuangMing Yao,Jun Zhou,Jingdong Chen,Jianing Li,Jianxin Sun,Jiajia Liu,Jianjiang Zhu,Jianping Jiang,Jun Peng,Kaixiang Ji,Kaimeng Ren,Libin Wang,Lixiang Ru,Longhua Tan,Lan Wang,Mochen Bai,Ning Gao,Qingpei Guo,Qinglong Zhang,Qiang Xu,Rui Liu,Ruijie Xiong,Ruobing Zheng,Sirui Gao,Tianqi Li,Tinghao Liu,Weilong Chai,Xinyu Xiao,Xiaomei Wang,Xiaolong Wang,Xiao Lu,Xiaoyu Li,Xingning Dong,Xuzheng Yu,Yi Yuan,Yuting Gao,Yuting Xiao,Yunxiao Sun,Yipeng Chen,Yifan Mao,Yifei Wu,Yongjie Lyu,Ziping Ma,Zhiqiang Fang,Zhihao Qiu,Ziyuan Huang,Zizheng Yang,Zhengyu He*

Main category: cs.CV

TL;DR: Ming-Flash-Omni is an upgraded version of Ming-Omni, built upon a sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion total parameters, of which only 6.1 billion are active per token.


<details>
  <summary>Details</summary>
Motivation: The motivation is to achieve highly efficient scaling and empowers stronger unified multimodal intelligence across vision, speech, and language, representing a key step toward Artificial General Intelligence (AGI).

Method: The method is based on a sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion total parameters, of which only 6.1 billion are active per token. The architecture enables highly efficient scaling.

Result: The upgraded version exhibits substantial improvements across multimodal understanding and generation. It achieves state-of-the-art performance in contextual ASR and highly competitive results in dialect-aware ASR. In image generation, it introduces high-fidelity text rendering and demonstrates marked gains in scene consistency and identity preservation during image editing. Furthermore, it introduces generative segmentation, a capability that not only achieves strong standalone segmentation performance but also enhances spatial control in image generation and improves editing consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in text-to-image generation and generative segmentation, and sets new records on all 12 contextual ASR benchmarks, all within a single unified architecture.

Conclusion: Ming-Flash-Omni achieves state-of-the-art results in text-to-image generation and generative segmentation, and sets new records on all 12 contextual ASR benchmarks, all within a single unified architecture.

Abstract: We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a
sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion
total parameters, of which only 6.1 billion are active per token. This
architecture enables highly efficient scaling (dramatically improving
computational efficiency while significantly expanding model capacity) and
empowers stronger unified multimodal intelligence across vision, speech, and
language, representing a key step toward Artificial General Intelligence (AGI).
Compared to its predecessor, the upgraded version exhibits substantial
improvements across multimodal understanding and generation. We significantly
advance speech recognition capabilities, achieving state-of-the-art performance
in contextual ASR and highly competitive results in dialect-aware ASR. In image
generation, Ming-Flash-Omni introduces high-fidelity text rendering and
demonstrates marked gains in scene consistency and identity preservation during
image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation,
a capability that not only achieves strong standalone segmentation performance
but also enhances spatial control in image generation and improves editing
consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in
text-to-image generation and generative segmentation, and sets new records on
all 12 contextual ASR benchmarks, all within a single unified architecture.

</details>


### [51] [MCIHN: A Hybrid Network Model Based on Multi-path Cross-modal Interaction for Multimodal Emotion Recognition](https://arxiv.org/abs/2510.24827)
*Haoyang Zhang,Zhou Yang,Ke Sun,Yucai Pang,Guoliang Xu*

Main category: cs.CV

TL;DR: 本文提出了一种基于多路径跨模态交互(MCIHN)的混合网络模型，用于解决多模态情感识别中的模态差异和单模态情感信息难以表征的问题。


<details>
  <summary>Details</summary>
Motivation: 多模态情感识别对未来人机交互至关重要。然而，由于不同模态之间的差异以及难以表征单模态情感信息，准确的情感识别仍然面临着巨大的挑战。

Method: 该模型首先为每个模态分别构建对抗自编码器(AAE)，学习判别性情感特征并通过解码器重建特征，以获得更多关于情感类别的判别信息。然后，将来自不同模态AAE的潜在代码输入到预定义的跨模态门机制模型(CGMM)中，以减少模态之间的差异，建立交互模态之间的情感关系，并生成不同模态之间的交互特征。使用特征融合模块(FFM)进行多模态融合，以实现更好的情感识别。

Result: 在公开的SIMS和MOSI数据集上进行的实验表明，MCIHN取得了优异的性能。

Conclusion: MCIHN模型在多模态情感识别任务中表现出色。

Abstract: Multimodal emotion recognition is crucial for future human-computer
interaction. However, accurate emotion recognition still faces significant
challenges due to differences between different modalities and the difficulty
of characterizing unimodal emotional information. To solve these problems, a
hybrid network model based on multipath cross-modal interaction (MCIHN) is
proposed. First, adversarial autoencoders (AAE) are constructed separately for
each modality. The AAE learns discriminative emotion features and reconstructs
the features through a decoder to obtain more discriminative information about
the emotion classes. Then, the latent codes from the AAE of different
modalities are fed into a predefined Cross-modal Gate Mechanism model (CGMM) to
reduce the discrepancy between modalities, establish the emotional relationship
between interacting modalities, and generate the interaction features between
different modalities. Multimodal fusion using the Feature Fusion module (FFM)
for better emotion recognition. Experiments were conducted on publicly
available SIMS and MOSI datasets, demonstrating that MCIHN achieves superior
performance.

</details>


### [52] [The Generation Phases of Flow Matching: a Denoising Perspective](https://arxiv.org/abs/2510.24830)
*Anne Gagneux,Ségolène Martin,Rémi Gribonval,Mathurin Massias*

Main category: cs.CV

TL;DR: Flow matching's generation quality is not well understood, this paper probes the generation process from a denoising perspective.


<details>
  <summary>Details</summary>
Motivation: Understand the factors influencing the quality of the flow matching generation process.

Method: Design a framework to empirically probe the generation process, and provide a common ground to compare the performances on generation and denoising.

Result: New insights on the distinct dynamical phases of the generative process, characterizing at which stage denoisers succeed or fail.

Conclusion: Principled and controlled perturbations (noise and drift) influence sample generation.

Abstract: Flow matching has achieved remarkable success, yet the factors influencing
the quality of its generation process remain poorly understood. In this work,
we adopt a denoising perspective and design a framework to empirically probe
the generation process. Laying down the formal connections between flow
matching models and denoisers, we provide a common ground to compare their
performances on generation and denoising. This enables the design of principled
and controlled perturbations to influence sample generation: noise and drift.
This leads to new insights on the distinct dynamical phases of the generative
process, enabling us to precisely characterize at which stage of the generative
process denoisers succeed or fail and why this matters.

</details>


### [53] [FruitProm: Probabilistic Maturity Estimation and Detection of Fruits and Vegetables](https://arxiv.org/abs/2510.24885)
*Sidharth Rai,Rahul Harsha Cheppally,Benjamin Vail,Keziban Yalçın Dokumacı,Ajay Sharda*

Main category: cs.CV

TL;DR: 本文将水果和蔬菜的成熟度估计重新定义为一个连续的概率学习任务，从而解决了现有深度学习方法将成熟度视为离散分类问题而导致的信息丢失和类别边界模糊的问题。


<details>
  <summary>Details</summary>
Motivation: 目前基于深度学习的方法将成熟度视为离散分类问题，但这与生物成熟过程的连续性相矛盾，导致信息丢失和类别边界模糊。

Method: 通过引入一个专用的概率头，对最先进的实时目标检测器 RT-DETRv2 进行了新的架构修改。该头能够预测每个检测到的对象在成熟度谱上的连续分布，同时学习平均成熟度状态及其相关的不确定性。

Result: 该模型不仅提供了更丰富、更符合生物学规律的植物成熟度表示，而且保持了卓越的检测性能，在一个具有挑战性的大规模水果数据集上实现了 85.6% 的平均精度（mAP）。

Conclusion: 该概率方法比基于分类的方法提供更精细和准确的成熟度评估，为现代农业中更智能、具有不确定性意识的自动化系统铺平了道路。

Abstract: Maturity estimation of fruits and vegetables is a critical task for
agricultural automation, directly impacting yield prediction and robotic
harvesting. Current deep learning approaches predominantly treat maturity as a
discrete classification problem (e.g., unripe, ripe, overripe). This rigid
formulation, however, fundamentally conflicts with the continuous nature of the
biological ripening process, leading to information loss and ambiguous class
boundaries. In this paper, we challenge this paradigm by reframing maturity
estimation as a continuous, probabilistic learning task. We propose a novel
architectural modification to the state-of-the-art, real-time object detector,
RT-DETRv2, by introducing a dedicated probabilistic head. This head enables the
model to predict a continuous distribution over the maturity spectrum for each
detected object, simultaneously learning the mean maturity state and its
associated uncertainty. This uncertainty measure is crucial for downstream
decision-making in robotics, providing a confidence score for tasks like
selective harvesting. Our model not only provides a far richer and more
biologically plausible representation of plant maturity but also maintains
exceptional detection performance, achieving a mean Average Precision (mAP) of
85.6\% on a challenging, large-scale fruit dataset. We demonstrate through
extensive experiments that our probabilistic approach offers more granular and
accurate maturity assessments than its classification-based counterparts,
paving the way for more intelligent, uncertainty-aware automated systems in
modern agriculture

</details>


### [54] [Proper Body Landmark Subset Enables More Accurate and 5X Faster Recognition of Isolated Signs in LIBRAS](https://arxiv.org/abs/2510.24887)
*Daniele L. V. dos Santos,Thiago B. Pereira,Carlos Eduardo G. R. Alves,Richard J. M. G. Tello,Francisco de A. Boldt,Thiago M. Paixão*

Main category: cs.CV

TL;DR: 本文研究了使用轻量级身体地标检测识别巴西手语 (LIBRAS) 中孤立符号的可行性。


<details>
  <summary>Details</summary>
Motivation: 尽管 Alves 等人 (2024) 的基于骨骼的方法显着提高了识别性能，但使用 OpenPose 进行地标提取阻碍了时间性能。初步调查发现，简单地用轻量级 MediaPipe 替换 OpenPose，虽然提高了处理速度，但显着降低了准确性。

Method: 我们探索了旨在优化识别性能的地标子集选择策略。此外，我们证明了基于样条的插补可以有效地缓解缺失的地标问题。

Result: 实验结果表明，适当的地标子集实现了与最先进方法相当或更优越的性能，同时与 Alves 等人 (2024) 相比，处理时间减少了 5 倍以上。基于样条的插补有效地缓解了缺失的地标问题，从而显着提高了准确性。

Conclusion: 仔细的地标选择，结合简单的插补技术，可以实现高效准确的孤立符号识别，为可扩展的 Sign Language Recognition 系统铺平道路。

Abstract: This paper investigates the feasibility of using lightweight body landmark
detection for the recognition of isolated signs in Brazilian Sign Language
(LIBRAS). Although the skeleton-based approach by Alves et al. (2024) enabled
substantial improvements in recognition performance, the use of OpenPose for
landmark extraction hindered time performance. In a preliminary investigation,
we observed that simply replacing OpenPose with the lightweight MediaPipe,
while improving processing speed, significantly reduced accuracy. To overcome
this limitation, we explored landmark subset selection strategies aimed at
optimizing recognition performance. Experimental results showed that a proper
landmark subset achieves comparable or superior performance to state-of-the-art
methods while reducing processing time by more than 5X compared to Alves et al.
(2024). As an additional contribution, we demonstrated that spline-based
imputation effectively mitigates missing landmark issues, leading to
substantial accuracy gains. These findings highlight that careful landmark
selection, combined with simple imputation techniques, enables efficient and
accurate isolated sign recognition, paving the way for scalable Sign Language
Recognition systems.

</details>


### [55] [Pixels to Signals: A Real-Time Framework for Traffic Demand Estimation](https://arxiv.org/abs/2510.24902)
*H Mhatre,M Vyas,A Mittal*

Main category: cs.CV

TL;DR: 提出了一种基于视频的车辆检测方法，以解决城市交通拥堵问题。


<details>
  <summary>Details</summary>
Motivation: 城市交通拥堵导致延误和效率降低，需要优化交通流量。

Method: 通过分析摄像头拍摄的多帧图像，计算背景，然后利用DBSCAN算法提取前景车辆。

Result: 提供了一种计算效率高、基础设施改动小的车辆检测方法，具有实际应用价值。

Conclusion: 所提出的方法为实际部署提供了一种实用且可扩展的解决方案。

Abstract: Traffic congestion is becoming a challenge in the rapidly growing urban
cities, resulting in increasing delays and inefficiencies within urban
transportation systems. To address this issue a comprehensive methodology is
designed to optimize traffic flow and minimize delays. The framework is
structured with three primary components: (a) vehicle detection, (b) traffic
prediction, and (c) traffic signal optimization. This paper presents the first
component, vehicle detection. The methodology involves analyzing multiple
sequential frames from a camera feed to compute the background, i.e. the
underlying roadway, by averaging pixel values over time. The computed
background is then utilized to extract the foreground, where the Density-Based
Spatial Clustering of Applications with Noise (DBSCAN) algorithm is applied to
detect vehicles. With its computational efficiency and minimal infrastructure
modification requirements, the proposed methodology offers a practical and
scalable solution for real-world deployment.

</details>


### [56] [VividCam: Learning Unconventional Camera Motions from Virtual Synthetic Videos](https://arxiv.org/abs/2510.24904)
*Qiucheng Wu,Handong Zhao,Zhixin Shu,Jing Shi,Yang Zhang,Shiyu Chang*

Main category: cs.CV

TL;DR: 本研究提出了一种新的训练范式 VividCam，它使扩散模型能够从合成视频中学习复杂的相机运动，而无需依赖收集真实的训练视频。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频生成模型难以推广到非常规的相机运动，这对于创作真正原创和艺术的视频至关重要。挑战在于难以找到具有预期非常规相机运动的足够训练视频。

Method: VividCam 包含多种解耦策略，可将相机运动学习与合成外观伪影隔离开来，从而确保更强大的运动表示并减轻域转移。

Result: 该设计可以使用非常简单的合成数据合成各种精确控制和复杂的相机运动。值得注意的是，此合成数据通常包含低多边形 3D 场景中的基本几何体，并且可以由 Unity 等引擎有效渲染。

Conclusion: VividCam 是一种有效的训练范式，它使扩散模型能够从合成视频中学习复杂的相机运动。

Abstract: Although recent text-to-video generative models are getting more capable of
following external camera controls, imposed by either text descriptions or
camera trajectories, they still struggle to generalize to unconventional camera
motions, which is crucial in creating truly original and artistic videos. The
challenge lies in the difficulty of finding sufficient training videos with the
intended uncommon camera motions. To address this challenge, we propose
VividCam, a training paradigm that enables diffusion models to learn complex
camera motions from synthetic videos, releasing the reliance on collecting
realistic training videos. VividCam incorporates multiple disentanglement
strategies that isolates camera motion learning from synthetic appearance
artifacts, ensuring more robust motion representation and mitigating domain
shift. We demonstrate that our design synthesizes a wide range of precisely
controlled and complex camera motions using surprisingly simple synthetic data.
Notably, this synthetic data often consists of basic geometries within a
low-poly 3D scene and can be efficiently rendered by engines like Unity. Our
video results can be found in https://wuqiuche.github.io/VividCamDemoPage/ .

</details>


### [57] [Understanding Multi-View Transformers](https://arxiv.org/abs/2510.24907)
*Michal Stary,Julien Gaubil,Ayush Tewari,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 该论文旨在探究多视角Transformer（如DUSt3R）的内部机制，因为它们在3D视觉任务中表现出色但内部运作不透明。


<details>
  <summary>Details</summary>
Motivation: 多视角Transformer在3D视觉领域取得了革命性进展，但其黑盒特性阻碍了进一步改进和在安全关键应用中的使用。

Method: 该研究提出了一种方法，通过探测和可视化多视角Transformer层残差连接中的3D表示来研究其内部运作。

Result: 研究揭示了DUSt3R模型变体在不同模块中的潜在状态发展，各个层的作用，以及它与具有更强归纳偏置的显式全局姿态方法的不同之处。此外，DUSt3R的变体估计的对应关系通过重建的几何体进行细化。

Conclusion: 该研究通过分析DUSt3R模型变体，阐明了多视角Transformer的内部机制，为未来的改进和应用提供了参考。

Abstract: Multi-view transformers such as DUSt3R are revolutionizing 3D vision by
solving 3D tasks in a feed-forward manner. However, contrary to previous
optimization-based pipelines, the inner mechanisms of multi-view transformers
are unclear. Their black-box nature makes further improvements beyond data
scaling challenging and complicates usage in safety- and reliability-critical
applications. Here, we present an approach for probing and visualizing 3D
representations from the residual connections of the multi-view transformers'
layers. In this manner, we investigate a variant of the DUSt3R model, shedding
light on the development of its latent state across blocks, the role of the
individual layers, and suggest how it differs from methods with stronger
inductive biases of explicit global pose. Finally, we show that the
investigated variant of DUSt3R estimates correspondences that are refined with
reconstructed geometry. The code used for the analysis is available at
https://github.com/JulienGaubil/und3rstand .

</details>


### [58] [Modality-Aware SAM: Sharpness-Aware-Minimization Driven Gradient Modulation for Harmonized Multimodal Learning](https://arxiv.org/abs/2510.24919)
*Hossein R. Nowdeh,Jie Ji,Xiaolong Ma,Fatemeh Afghah*

Main category: cs.CV

TL;DR: 提出了一种名为Modality-Aware Sharpness-Aware Minimization (M-SAM) 的模型无关框架，用于解决多模态学习中主要模态掩盖其他模态的问题，提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中，主要的模态经常掩盖其他的模态，限制了泛化能力。

Method: M-SAM通过三个步骤优化学习：1) 使用 Shapley 值识别主要模态；2) 分解损失 landscape，优先考虑模型对主要模态的鲁棒性；3) 通过调制梯度的反向传播更新权重。

Result: 在四个不同的数据集上的大量实验表明，M-SAM 优于最新的 state-of-the-art 优化和梯度操作方法，并显著平衡和改进了多模态学习。

Conclusion: M-SAM 确保了主要模态的鲁棒学习，同时增强了来自其他模态的贡献，允许模型探索和利用互补特征，从而加强整体性能。

Abstract: In multimodal learning, dominant modalities often overshadow others, limiting
generalization. We propose Modality-Aware Sharpness-Aware Minimization (M-SAM),
a model-agnostic framework that applies to many modalities and supports early
and late fusion scenarios. In every iteration, M-SAM in three steps optimizes
learning. \textbf{First, it identifies the dominant modality} based on
modalities' contribution in the accuracy using Shapley. \textbf{Second, it
decomposes the loss landscape}, or in another language, it modulates the loss
to prioritize the robustness of the model in favor of the dominant modality,
and \textbf{third, M-SAM updates the weights} by backpropagation of modulated
gradients. This ensures robust learning for the dominant modality while
enhancing contributions from others, allowing the model to explore and exploit
complementary features that strengthen overall performance. Extensive
experiments on four diverse datasets show that M-SAM outperforms the latest
state-of-the-art optimization and gradient manipulation methods and
significantly balances and improves multimodal learning.

</details>


### [59] [IBIS: A Powerful Hybrid Architecture for Human Activity Recognition](https://arxiv.org/abs/2510.24936)
*Alison M. Fernandes,Hermes I. Del Monego,Bruno S. Chang,Anelise Munaretto,Hélder M. Fontes,Rui L. Campos*

Main category: cs.CV

TL;DR: 本文提出了一种名为IBIS的新型混合架构，用于提高Wi-Fi传感中模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi传感在医疗保健、空间占用分析和基于手势的物联网控制等应用中具有潜力，但现有模型容易过拟合。

Method: 该方法集成了Inception-BiLSTM和支持向量机(SVM)。

Result: 在多普勒数据上实现了近99%的运动识别准确率。

Conclusion: 综合性能指标和混淆矩阵证实了所提方案的有效性。

Abstract: The increasing interest in Wi-Fi sensing stems from its potential to capture
environmental data in a low-cost, non-intrusive way, making it ideal for
applications like healthcare, space occupancy analysis, and gesture-based IoT
control. However, a major limitation in this field is the common problem of
overfitting, where models perform well on training data but fail to generalize
to new data. To overcome this, we introduce a novel hybrid architecture that
integrates Inception-BiLSTM with a Support Vector Machine (SVM), which we refer
to as IBIS. Our IBIS approach is uniquely engineered to improve model
generalization and create more robust classification boundaries. By applying
this method to Doppler-derived data, we achieve a movement recognition accuracy
of nearly 99%. Comprehensive performance metrics and confusion matrices confirm
the significant effectiveness of our proposed solution.

</details>


### [60] [FT-ARM: Fine-Tuned Agentic Reflection Multimodal Language Model for Pressure Ulcer Severity Classification with Reasoning](https://arxiv.org/abs/2510.24980)
*Reza Saadati Fard,Emmanuel Agu,Palawat Busaranuvong,Deepak Kumar,Shefalika Gautam,Bengisu Tulu,Diane Strong,Lorraine Loretz*

Main category: cs.CV

TL;DR: FT-ARM: 一种用于压疮严重程度分类的多模态大语言模型，通过模仿临床医生诊断评估来迭代优化预测，提高准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法解释性有限，且压疮严重程度分类因视觉差异细微和主观解读而具有挑战性。

Method: FT-ARM：一种带有能动自反思机制的微调多模态大语言模型，通过推理视觉特征和文本中编码的临床知识来迭代优化预测。

Result: 在公开的压力损伤图像数据集 (PIID) 上，FT-ARM 的准确率达到了 85%，超过了之前的 CNN 模型 4%。

Conclusion: FT-ARM 提高了自动化伤口评估系统的可靠性、透明性和临床适用性，满足了对一致且可解释的 PU 分期的迫切需求，以支持改善患者护理。

Abstract: Pressure ulcers (PUs) are a serious and prevalent healthcare concern.
Accurate classification of PU severity (Stages I-IV) is essential for proper
treatment but remains challenging due to subtle visual distinctions and
subjective interpretation, leading to variability among clinicians. Prior
AI-based approaches using Convolutional Neural Networks (CNNs) and Vision
Transformers (ViTs) achieved promising accuracy but offered limited
interpretability. We present FT-ARM (Fine-Tuned Agentic Reflection Multimodal
model), a fine-tuned multimodal large language model (MLLM) with an agentic
self-reflection mechanism for pressure ulcer severity classification. Inspired
by clinician-style diagnostic reassessment, FT-ARM iteratively refines its
predictions by reasoning over visual features and encoded clinical knowledge
from text, enhancing both accuracy and consistency. On the publicly available
Pressure Injury Image Dataset (PIID), FT-ARM, fine-tuned from LLaMA 3.2 90B,
achieved 85% accuracy in classifying PU stages I-IV, surpassing prior CNN-based
models by +4%. Unlike earlier CNN/ViT studies that relied solely on offline
evaluations, FT-ARM is designed and tested for live inference, reflecting
real-time deployment conditions. Furthermore, it produces clinically grounded
natural-language explanations, improving interpretability and trust. By
integrating fine-tuning and reflective reasoning across multimodal inputs,
FT-ARM advances the reliability, transparency, and clinical applicability of
automated wound assessment systems, addressing the critical need for consistent
and explainable PU staging to support improved patient care.

</details>


### [61] [Efficient License Plate Recognition via Pseudo-Labeled Supervision with Grounding DINO and YOLOv8](https://arxiv.org/abs/2510.25032)
*Zahra Ebrahimi Vargoorani,Amir Mohammad Ghoreyshi,Ching Yee Suen*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于YOLOv8的深度学习策略，用于车牌检测和识别。


<details>
  <summary>Details</summary>
Motivation: 开发高精度自动车牌识别系统(ALPR)具有挑战性，环境因素如光照、雨水和灰尘，以及高车速、不同相机角度和低质量/低分辨率图像都带来了困难。ALPR在交通控制、停车、车辆跟踪、收费和执法应用中至关重要。

Method: 该方法使用YOLOv8进行车牌检测和识别，并结合半监督学习框架，利用Grounding DINO生成伪标签来训练检测模型。

Result: 该方法在CENPARMI数据集上实现了94%的召回率，在UFPR-ALPR数据集上实现了91%的召回率。

Conclusion: 通过整合人工验证和模型生成的注释，可以在保持标签质量的同时高效扩展数据集，从而显著增强训练过程和整体模型性能。

Abstract: Developing a highly accurate automatic license plate recognition system
(ALPR) is challenging due to environmental factors such as lighting, rain, and
dust. Additional difficulties include high vehicle speeds, varying camera
angles, and low-quality or low-resolution images. ALPR is vital in traffic
control, parking, vehicle tracking, toll collection, and law enforcement
applications. This paper proposes a deep learning strategy using YOLOv8 for
license plate detection and recognition tasks. This method seeks to enhance the
performance of the model using datasets from Ontario, Quebec, California, and
New York State. It achieved an impressive recall rate of 94% on the dataset
from the Center for Pattern Recognition and Machine Intelligence (CENPARMI) and
91% on the UFPR-ALPR dataset. In addition, our method follows a semi-supervised
learning framework, combining a small set of manually labeled data with
pseudo-labels generated by Grounding DINO to train our detection model.
Grounding DINO, a powerful vision-language model, automatically annotates many
images with bounding boxes for license plates, thereby minimizing the reliance
on labor-intensive manual labeling. By integrating human-verified and
model-generated annotations, we can scale our dataset efficiently while
maintaining label quality, which significantly enhances the training process
and overall model performance. Furthermore, it reports character error rates
for both datasets, providing additional insight into system performance.

</details>


### [62] [Breast Cancer VLMs: Clinically Practical Vision-Language Train-Inference Models](https://arxiv.org/abs/2510.25051)
*Shunjie-Fabian Zheng,Hyeonjun Lee,Thijs Kooi,Ali Diba*

Main category: cs.CV

TL;DR: 本文提出了一种新的乳腺癌计算机辅助诊断（CAD）框架，该框架结合了来自 2D 乳房 X 光照片的视觉特征与来自易于访问的临床元数据和合成放射学报告的结构化文本描述。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机辅助诊断系统在临床部署中面临关键限制，尤其是在处理多模态数据的细微解释以及由于需要先前的临床病史而导致的可行性方面。

Method: 该研究引入了一种新颖的框架，该框架通过创新的标记化模块，协同地结合了来自 2D 乳房 X 光照片的视觉特征与来自易于访问的临床元数据和合成放射学报告的结构化文本描述。该研究中提出的方法表明，卷积神经网络（ConvNets）与语言表示的战略集成实现了优于基于视觉 Transformer 的模型的性能，同时处理高分辨率图像并支持在不同人群中进行实际部署。

Result: 通过在多国队列筛查乳房 X 光照片上进行评估，与单模态基线相比，我们的多模态方法在癌症检测和钙化识别方面取得了优异的性能，并取得了特别的改进。

Conclusion: 该方法为开发临床上可行的基于 VLM 的 CAD 系统建立了一个新的范例，该系统通过有效的融合机制有效地利用了成像数据和上下文患者信息。

Abstract: Breast cancer remains the most commonly diagnosed malignancy among women in
the developed world. Early detection through mammography screening plays a
pivotal role in reducing mortality rates. While computer-aided diagnosis (CAD)
systems have shown promise in assisting radiologists, existing approaches face
critical limitations in clinical deployment - particularly in handling the
nuanced interpretation of multi-modal data and feasibility due to the
requirement of prior clinical history. This study introduces a novel framework
that synergistically combines visual features from 2D mammograms with
structured textual descriptors derived from easily accessible clinical metadata
and synthesized radiological reports through innovative tokenization modules.
Our proposed methods in this study demonstrate that strategic integration of
convolutional neural networks (ConvNets) with language representations achieves
superior performance to vision transformer-based models while handling
high-resolution images and enabling practical deployment across diverse
populations. By evaluating it on multi-national cohort screening mammograms,
our multi-modal approach achieves superior performance in cancer detection and
calcification identification compared to unimodal baselines, with particular
improvements. The proposed method establishes a new paradigm for developing
clinically viable VLM-based CAD systems that effectively leverage imaging data
and contextual patient information through effective fusion mechanisms.

</details>


### [63] [Auto3DSeg for Brain Tumor Segmentation from 3D MRI in BraTS 2023 Challenge](https://arxiv.org/abs/2510.25058)
*Andriy Myronenko,Dong Yang,Yufan He,Daguang Xu*

Main category: cs.CV

TL;DR: 本研究描述了作者使用MONAI中的Auto3DSeg解决BraTS 2023挑战赛集群的方案。作者参加了所有5个分割挑战，并在其中3个挑战中获得第一名：脑转移瘤、脑膜瘤、BraTS-Africa挑战，并在其余2个挑战中获得第二名：成人和儿童神经胶质瘤挑战。


<details>
  <summary>Details</summary>
Motivation: 为了在BraTS 2023挑战赛中取得优异的成绩。

Method: 使用MONAI中的Auto3DSeg。

Result: 在五个分割挑战中的三个中获得第一名，在其余两个中获得第二名。

Conclusion: 使用MONAI中的Auto3DSeg可以有效解决BraTS 2023挑战赛的分割任务。

Abstract: In this work, we describe our solution to the BraTS 2023 cluster of
challenges using Auto3DSeg from MONAI. We participated in all 5 segmentation
challenges, and achieved the 1st place results in three of them: Brain
Metastasis, Brain Meningioma, BraTS-Africa challenges, and the 2nd place
results in the remaining two: Adult and Pediatic Glioma challenges.

</details>


### [64] [DRIP: Dynamic patch Reduction via Interpretable Pooling](https://arxiv.org/abs/2510.25067)
*Yusen Peng,Sachin Kumar*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为DRIP（通过可解释池化的动态补丁缩减）的方法，用于优化视觉语言模型的预训练效率。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉语言模型成本高昂，效率问题阻碍了研究人员从头开始预训练。

Method: DRIP 适应输入图像，并在视觉编码器的更深层中动态合并 tokens。

Result: 在 ImageNet 从头开始训练和 CLIP 对比预训练中，DRIP 显著降低了 GFLOP，同时保持了相当的分类/zero-shot 性能。在大型生物学数据集上进行的持续预训练进一步验证了该方法的有效性。

Conclusion: DRIP 是一种有效的视觉语言模型预训练优化方法，并具有在科学领域应用的潜力。

Abstract: Recently, the advances in vision-language models, including contrastive
pretraining and instruction tuning, have greatly pushed the frontier of
multimodal AI. However, owing to the large-scale and hence expensive
pretraining, the efficiency concern has discouraged researchers from attempting
to pretrain a vision language model from scratch. In this work, we propose
Dynamic patch Reduction via Interpretable Pooling (DRIP), which adapts to the
input images and dynamically merges tokens in the deeper layers of a visual
encoder. Our results on both ImageNet training from scratch and CLIP
contrastive pretraining demonstrate a significant GFLOP reduction while
maintaining comparable classification/zero-shot performance. To further
validate our proposed method, we conduct continual pretraining on a large
biology dataset, extending its impact into scientific domains.

</details>


### [65] [Vision-Language Integration for Zero-Shot Scene Understanding in Real-World Environments](https://arxiv.org/abs/2510.25070)
*Manjunath Prasad Holenarasipura Rajiv,B. M. Vidyavathi*

Main category: cs.CV

TL;DR: 本文提出了一种视觉-语言集成框架，用于在现实场景中实现零样本场景理解，利用预训练的视觉编码器和大型语言模型，通过自然语言作为桥梁，概括未见过的类别和上下文。


<details>
  <summary>Details</summary>
Motivation: 现实场景的复杂性和可变性给零样本场景理解带来了巨大的挑战，模型必须在没有先前的标记示例的情况下识别新的对象、动作和上下文。

Method: 该方法开发了一个统一的模型，该模型将视觉输入和文本提示嵌入到共享空间中，然后进行多模态融合和推理层，以进行上下文解释。

Result: 在 Visual Genome、COCO、ADE20K 和自定义的真实世界数据集上的实验表明，在对象识别、活动检测和场景字幕方面，与最先进的零样本模型相比，该方法取得了显着收益。

Conclusion: 所提出的系统在 top-1 准确率方面实现了高达 18% 的改进，并在语义连贯性指标方面取得了显着收益，突出了跨模态对齐和语言基础在增强现实世界场景理解的泛化能力方面的有效性。

Abstract: Zero-shot scene understanding in real-world settings presents major
challenges due to the complexity and variability of natural scenes, where
models must recognize new objects, actions, and contexts without prior labeled
examples. This work proposes a vision-language integration framework that
unifies pre-trained visual encoders (e.g., CLIP, ViT) and large language models
(e.g., GPT-based architectures) to achieve semantic alignment between visual
and textual modalities. The goal is to enable robust zero-shot comprehension of
scenes by leveraging natural language as a bridge to generalize over unseen
categories and contexts. Our approach develops a unified model that embeds
visual inputs and textual prompts into a shared space, followed by multimodal
fusion and reasoning layers for contextual interpretation. Experiments on
Visual Genome, COCO, ADE20K, and custom real-world datasets demonstrate
significant gains over state-of-the-art zero-shot models in object recognition,
activity detection, and scene captioning. The proposed system achieves up to
18% improvement in top-1 accuracy and notable gains in semantic coherence
metrics, highlighting the effectiveness of cross-modal alignment and language
grounding in enhancing generalization for real-world scene understanding.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [66] [Scheduling Your LLM Reinforcement Learning with Reasoning Trees](https://arxiv.org/abs/2510.24832)
*Hong Wang,Zhezheng Hao,Jian Luo,Chenxing Wei,Yao Shu,Lei Liu,Qiang Lin,Hande Dong,Jiawei Chen*

Main category: cs.AI

TL;DR: 提出了一种新的基于推理树结构的强化学习数据调度方法，用于优化大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习数据调度方法通常依赖于基于路径的指标来对查询进行排序，忽略了这些查询的推理树结构。

Method: 提出了一种新的指标，即推理分数（r-score），它根据查询的推理树的结构来衡量查询的学习难度。基于 r-score，我们提出了一种推理树调度（Re-Schedule），这是一种调度算法，它构建了一个从结构简单（高 r-score）到复杂（低 r-score）查询的课程。

Result: 在六个数学推理基准测试上的实验表明，Re-Schedule 显着提高了平均准确率，实现了高达 3.2% 的增益。

Conclusion: 这些结果验证了该方法，并表明对推理树的结构理解为强化学习数据调度提供了更强大和更有效的原则基础。

Abstract: Using Reinforcement Learning with Verifiable Rewards (RLVR) to optimize Large
Language Models (LLMs) can be conceptualized as progressively editing a query's
`Reasoning Tree'. This process involves exploring nodes (tokens) and
dynamically modifying the model's policy at each node. When combined with data
scheduling, this process yields further gains in data efficiency and accuracy.
However, existing RLVR data scheduling methods typically rely on path-based
metrics to rank queries, overlooking the reasoning tree structures of these
queries. In this paper, we introduce a novel metric, namely Reasoning Score
(r-score), which measures the query's learning difficulty based on the
structure of its reasoning tree. Based on the r-score, we propose the Reasoning
Tree Schedule (Re-Schedule), a scheduling algorithm that constructs a
curriculum progressing from structurally simple (high r-score) to complex (low
r-score) queries. Experiments on six math-reasoning benchmarks show that
Re-Schedule significantly improves average accuracy, achieving gains of up to
3.2%. These strong results validate our approach and demonstrate that a
structural understanding of the reasoning tree provides a more powerful and
principled foundation for RLVR data scheduling.

</details>


### [67] [Cyclic Counterfactuals under Shift-Scale Interventions](https://arxiv.org/abs/2510.25005)
*Saptarshi Saha,Dhruv Vansraj Rathore,Utpal Garain*

Main category: cs.AI

TL;DR: 研究循环SCM中的反事实推理。


<details>
  <summary>Details</summary>
Motivation: 许多现实系统包含违反非循环性的反馈循环或循环依赖。

Method: 在移位-尺度干预下研究循环SCM中的反事实推理。

Result: 未提供。

Conclusion: 未提供。

Abstract: Most counterfactual inference frameworks traditionally assume acyclic
structural causal models (SCMs), i.e. directed acyclic graphs (DAGs). However,
many real-world systems (e.g. biological systems) contain feedback loops or
cyclic dependencies that violate acyclicity. In this work, we study
counterfactual inference in cyclic SCMs under shift-scale interventions, i.e.,
soft, policy-style changes that rescale and/or shift a variable's mechanism.

</details>


### [68] [Taming the Real-world Complexities in CPT E/M Coding with Large Language Models](https://arxiv.org/abs/2510.25007)
*Islam Nassar,Yang Lin,Yuan Jin,Rongxin Zhu,Chang Wei Tan,Zenan Zhai,Nitika Mathur,Thanh Tien Vu,Xu Zhong,Long Duong,Yuan-Fang Li*

Main category: cs.AI

TL;DR: 本文介绍了一种名为 ProFees 的基于 LLM 的框架，用于自动化 E/M 编码，以减轻医生的文档负担并提高计费效率。


<details>
  <summary>Details</summary>
Motivation: 医生需要准确提供 CPT E/M 编码，这是一项辅助任务，增加了他们的文档负担。自动化这项编码任务将有所帮助。

Method: 本文提出了 ProFees 框架，该框架基于 LLM，旨在解决 E/M 编码自动化中的实际复杂性。

Result: 在专家策划的真实数据集上，ProFees 的编码准确率比商业 CPT E/M 编码系统提高了 36% 以上，比最强的单提示基线提高了近 5%。

Conclusion: ProFees 在解决实际复杂性方面是有效的。

Abstract: Evaluation and Management (E/M) coding, under the Current Procedural
Terminology (CPT) taxonomy, documents medical services provided to patients by
physicians. Used primarily for billing purposes, it is in physicians' best
interest to provide accurate CPT E/M codes. %While important, it is an
auxiliary task that adds to physicians' documentation burden. Automating this
coding task will help alleviate physicians' documentation burden, improve
billing efficiency, and ultimately enable better patient care. However, a
number of real-world complexities have made E/M encoding automation a
challenging task. In this paper, we elaborate some of the key complexities and
present ProFees, our LLM-based framework that tackles them, followed by a
systematic evaluation. On an expert-curated real-world dataset, ProFees
achieves an increase in coding accuracy of more than 36\% over a commercial CPT
E/M coding system and almost 5\% over our strongest single-prompt baseline,
demonstrating its effectiveness in addressing the real-world complexities.

</details>


### [69] [Aligning Large Language Models with Procedural Rules: An Autoregressive State-Tracking Prompting for In-Game Trading](https://arxiv.org/abs/2510.25014)
*Minkyung Kim,Junsik Kim,Woongcheol Yang,Sangdon Park,Sohee Bae*

Main category: cs.AI

TL;DR: 大型语言模型虽然实现了动态游戏交互，但在规则驱动的交易系统中无法遵循必要的程序流程，从而降低了玩家的信任度。本文提出了自回归状态跟踪提示（ASTP）方法，该方法通过精心设计的提示，使LLM明确且可验证地进行状态跟踪，从而解决了LLM的创造性灵活性与游戏内交易的程序性需求之间的核心矛盾。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在规则驱动的交易系统中无法遵循必要的程序流程，从而降低了玩家的信任度。

Method: 提出了自回归状态跟踪提示（ASTP）方法，该方法的核心在于精心设计的提示，该提示迫使LLM明确并可验证地进行状态跟踪。为了确保交易的完整性，该方法还辅以特定于状态的占位符后处理方法，以进行准确的价格计算。

Result: 在300个交易对话中的评估表明，状态合规性>99％，计算精度为99.3％。值得注意的是，在较小模型（Gemini-2.5-Flash）上使用带有占位符后处理的ASTP可以与较大模型（Gemini-2.5-Pro）的性能相匹配，同时将响应时间从21.2秒减少到2.4秒。

Conclusion: ASTP与占位符后处理相结合，为满足商业游戏的实时要求和资源约束奠定了坚实的基础。

Abstract: Large Language Models (LLMs) enable dynamic game interactions but fail to
follow essential procedural flows in rule-governed trading systems, eroding
player trust. This work resolves the core tension between the creative
flexibility of LLMs and the procedural demands of in-game trading
(browse-offer-review-confirm). To this end, Autoregressive State-Tracking
Prompting (ASTP) is introduced, a methodology centered on a strategically
orchestrated prompt that compels an LLM to make its state-tracking process
explicit and verifiable. Instead of relying on implicit contextual
understanding, ASTP tasks the LLM with identifying and reporting a predefined
state label from the previous turn. To ensure transactional integrity, this is
complemented by a state-specific placeholder post-processing method for
accurate price calculations. Evaluation across 300 trading dialogues
demonstrates >99% state compliance and 99.3% calculation precision. Notably,
ASTP with placeholder post-processing on smaller models (Gemini-2.5-Flash)
matches larger models' (Gemini-2.5-Pro) performance while reducing response
time from 21.2s to 2.4s, establishing a practical foundation that satisfies
both real-time requirements and resource constraints of commercial games.

</details>


### [70] [Reasoning-Aware GRPO using Process Mining](https://arxiv.org/abs/2510.25065)
*Taekhyun Park,Yongjae Lee,Hyerim Bae*

Main category: cs.AI

TL;DR: PM4GRPO是一种新的强化学习方法，它通过结合过程挖掘技术来评估模型推理过程与预训练教师模型的一致性，从而提升大型推理模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法主要关注结果，而忽略了推理过程。PM4GRPO旨在解决这个问题，通过考虑推理过程中的信号来改进奖励机制。

Method: PM4GRPO利用过程挖掘技术计算一个一致性奖励，该奖励衡量策略模型的推理与预训练教师模型的一致程度。该方法在GRPO的基础上，增加了对推理过程的感知。

Result: 在五个基准测试上的实验结果表明，PM4GRPO显著优于现有的基于GRPO的后训练方法。

Conclusion: 将过程挖掘技术应用于推理感知的GRPO可以有效提高策略模型的推理能力。

Abstract: Reinforcement learning (RL)-based post-training has been crucial for enabling
multi-step reasoning in large reasoning models (LRMs), yet current reward
schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware
Group Relative Policy Optimization (GRPO) that augments standard answer/format
rewards with signals over the reasoning procedure. To this end, process mining
techniques are utilized to compute a scalar conformance reward that measures
how closely a policy model's reasoning aligns with the pretrained teacher
model. The empirical results on five benchmarks demonstrate that PM4GRPO
significantly outperforms existing methodologies for GRPO-based post-training.
These results highlight that leveraging process mining for reasoning-aware GRPO
effectively enhances the reasoning capabilities of policy models.

</details>


### [71] [H3M-SSMoEs: Hypergraph-based Multimodal Learning with LLM Reasoning and Style-Structured Mixture of Experts](https://arxiv.org/abs/2510.25091)
*Peilin Tan,Liang Xie,Churan Zhi,Dian Tu,Chuanqi Shi*

Main category: cs.AI

TL;DR: H3M-SSMoEs模型通过多模态超图、LLM增强推理和风格结构化混合专家，提升股票预测准确性和投资表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在可扩展框架内统一结构、语义和自适应建模，无法有效捕捉股票市场中复杂的时序依赖、异构模态和动态演变的股票间关系。

Method: 提出H3M-SSMoEs模型，包含多上下文多模态超图（LCH和GCH捕捉时空动态和股票依赖）、LLM增强推理模块（融合定量和文本模态）和风格结构化混合专家（SSMoEs，实现 regime-aware 专业化）。

Result: 在三个主要股票市场上的大量实验表明，H3M-SSMoEs 在预测准确性和投资表现方面均优于现有方法，并表现出有效的风险控制。

Conclusion: H3M-SSMoEs模型能够有效提升股票预测的准确性和投资表现，并具有良好的风险控制能力。

Abstract: Stock movement prediction remains fundamentally challenging due to complex
temporal dependencies, heterogeneous modalities, and dynamically evolving
inter-stock relationships. Existing approaches often fail to unify structural,
semantic, and regime-adaptive modeling within a scalable framework. This work
introduces H3M-SSMoEs, a novel Hypergraph-based MultiModal architecture with
LLM reasoning and Style-Structured Mixture of Experts, integrating three key
innovations: (1) a Multi-Context Multimodal Hypergraph that hierarchically
captures fine-grained spatiotemporal dynamics via a Local Context Hypergraph
(LCH) and persistent inter-stock dependencies through a Global Context
Hypergraph (GCH), employing shared cross-modal hyperedges and Jensen-Shannon
Divergence weighting mechanism for adaptive relational learning and cross-modal
alignment; (2) a LLM-enhanced reasoning module, which leverages a frozen large
language model with lightweight adapters to semantically fuse and align
quantitative and textual modalities, enriching representations with
domain-specific financial knowledge; and (3) a Style-Structured Mixture of
Experts (SSMoEs) that combines shared market experts and industry-specialized
experts, each parameterized by learnable style vectors enabling regime-aware
specialization under sparse activation. Extensive experiments on three major
stock markets demonstrate that H3M-SSMoEs surpasses state-of-the-art methods in
both superior predictive accuracy and investment performance, while exhibiting
effective risk control. Datasets, source code, and model weights are available
at our GitHub repository: https://github.com/PeilinTime/H3M-SSMoEs.

</details>


### [72] [KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA](https://arxiv.org/abs/2510.25101)
*Zhuo Chen,Fei Wang,Zixuan Li,Zhao Zhang,Weiwei Ding,Chuanguang Yang,Yongjun Xu,Xiaolong Jin,Jiafeng Guo*

Main category: cs.AI

TL;DR: KnowCoder-A1: an LLM for KBQA that autonomously reasons on KBs to answer questions.


<details>
  <summary>Details</summary>
Motivation: Existing KBQA methods fine-tune LLMs on synthesized reasoning trajectories, which limits exploration and reasoning ability.

Method: Trains an LLM with outcome-only supervision via multi-stage curriculum reinforcement learning, progressing from easy to hard reward schedules. It is initialized with fine-tuning on high-quality trajectories via rejection sampling.

Result: Outperforms prior approaches on three datasets, achieving significant improvement on GrailQA with less training data.

Conclusion: KnowCoder-A1 demonstrates strong agentic reasoning capabilities through outcome-only supervision and curriculum reinforcement learning.

Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural-language
questions over a structured Knowledge Base (KB). Recent work improves KBQA by
adopting an agentic reasoning paradigm, in which Large Language Models (LLMs)
iteratively decompose a question, generate its corresponding logical queries,
and interact with the KB to derive the answer. However, these methods typically
fine-tune LLMs on reasoning trajectories synthesized via process supervision,
which offers weak incentives for exploration and thus fails to strengthen the
agentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that
can autonomously perform agentic reasoning on KBs to obtain answers. To
incentivize autonomous exploration, KnowCoder-A1 trains the LLM under
outcome-only supervision via a multi-stage curriculum reinforcement learning
with an easy-to-hard curriculum. To establish foundational agentic
capabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of
high-quality trajectories obtained through outcome-based rejection sampling.
Then, to alleviate the reward sparsity inherent in outcome-only supervision, it
applies multi-stage curriculum RL with reward schedules that progress from easy
to hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful
reasoning behaviors and consistently outperforms prior approaches across three
mainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1
achieves up to an 11.1% relative improvement while using only one-twelfth of
the training data, demonstrating strong agentic reasoning capabilities.

</details>


### [73] [Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models](https://arxiv.org/abs/2510.25179)
*Juan Ren,Mark Dras,Usman Naseem*

Main category: cs.AI

TL;DR: 提出了一种名为 Agentic Moderation 的模型无关框架，利用专用代理来防御多模态系统免受越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 增强推理、协作和自适应控制，使系统能够协调和独立地解决复杂任务。

Method: 集成动态、协作的代理，包括 Shield、Responder、Evaluator 和 Reflector，以实现上下文感知和可解释的审核。

Result: 在五个数据集和四个代表性的大型视觉语言模型 (LVLM) 上的大量实验表明，该方法可将攻击成功率 (ASR) 降低 7-19%，保持稳定的不跟随率 (NF)，并将拒绝率 (RR) 提高 4-20%，从而实现稳健、可解释且平衡的安全性。

Conclusion: 通过利用代理架构的灵活性和推理能力，Agentic Moderation 提供了模块化、可扩展和细粒度的安全执行，突出了代理系统作为自动化安全治理基础的更广泛潜力。

Abstract: Agentic methods have emerged as a powerful and autonomous paradigm that
enhances reasoning, collaboration, and adaptive control, enabling systems to
coordinate and independently solve complex tasks. We extend this paradigm to
safety alignment by introducing Agentic Moderation, a model-agnostic framework
that leverages specialised agents to defend multimodal systems against
jailbreak attacks. Unlike prior approaches that apply as a static layer over
inputs or outputs and provide only binary classifications (safe or unsafe), our
method integrates dynamic, cooperative agents, including Shield, Responder,
Evaluator, and Reflector, to achieve context-aware and interpretable
moderation. Extensive experiments across five datasets and four representative
Large Vision-Language Models (LVLMs) demonstrate that our approach reduces the
Attack Success Rate (ASR) by 7-19%, maintains a stable Non-Following Rate (NF),
and improves the Refusal Rate (RR) by 4-20%, achieving robust, interpretable,
and well-balanced safety performance. By harnessing the flexibility and
reasoning capacity of agentic architectures, Agentic Moderation provides
modular, scalable, and fine-grained safety enforcement, highlighting the
broader potential of agentic systems as a foundation for automated safety
governance.

</details>


### [74] [Energy-Efficient Autonomous Driving with Adaptive Perception and Robust Decision](https://arxiv.org/abs/2510.25205)
*Yuyang Xia,Zibo Liang,Liwei Deng,Yan Zhao,Han Su,Kai Zheng*

Main category: cs.AI

TL;DR: 提出了一种名为EneAD的节能自动驾驶框架，旨在降低感知计算的能耗，从而延长车辆的行驶里程，尤其是在电动汽车中。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶技术有望带来显著的社会、经济和环境效益，但同时也面临计算引擎能耗上升的问题，特别是在感知计算方面，现有模型压缩方法又难以兼顾模型大小和感知精度。

Method: 该框架包含自适应感知模块和稳健决策模块。自适应感知模块通过数据管理和调整来优化感知，包括管理多个不同计算消耗的感知模型，并动态调整执行帧率；稳健决策模块则基于强化学习设计决策模型，并引入正则化项以增强驾驶稳定性。

Result: 实验结果表明，EneAD能够将感知消耗降低1.9倍至3.5倍，从而将行驶里程提高3.9%至8.5%。

Conclusion: EneAD框架在降低能耗和提高驾驶性能方面表现出色。

Abstract: Autonomous driving is an emerging technology that is expected to bring
significant social, economic, and environmental benefits. However, these
benefits come with rising energy consumption by computation engines, limiting
the driving range of vehicles, especially electric ones. Perception computing
is typically the most power-intensive component, as it relies on largescale
deep learning models to extract environmental features. Recently, numerous
studies have employed model compression techniques, such as sparsification,
quantization, and distillation, to reduce computational consumption. However,
these methods often result in either a substantial model size or a significant
drop in perception accuracy compared to high-computation models. To address
these challenges, we propose an energy-efficient autonomous driving framework,
called EneAD. In the adaptive perception module, a perception optimization
strategy is designed from the perspective of data management and tuning.
Firstly, we manage multiple perception models with different computational
consumption and adjust the execution framerate dynamically. Then, we define
them as knobs and design a transferable tuning method based on Bayesian
optimization to identify promising knob values that achieve low computation
while maintaining desired accuracy. To adaptively switch the knob values in
various traffic scenarios, a lightweight classification model is proposed to
distinguish the perception difficulty in different scenarios. In the robust
decision module, we propose a decision model based on reinforcement learning
and design a regularization term to enhance driving stability in the face of
perturbed perception results. Extensive experiments evidence the superiority of
our framework in both energy consumption and driving performance. EneAD can
reduce perception consumption by 1.9x to 3.5x and thus improve driving range by
3.9% to 8.5%

</details>


### [75] [RAVR: Reference-Answer-guided Variational Reasoning for Large Language Models](https://arxiv.org/abs/2510.25206)
*Tianqianjin Lin,Xi Zhao,Xingyao Zhang,Rujiao Long,Yi Xu,Zhuoren Jiang,Wenbo Su,Bo Zheng*

Main category: cs.AI

TL;DR: 大型语言模型可以通过强化学习来提高推理能力，但需要保证模型能够以不可忽略的概率生成高质量的推理路径。对于超出模型当前能力范围的任务，这样的推理路径难以采样，学习过程可能会强化常见的但次优的推理方式。论文受到认知科学的启发，认为“为什么这是答案”通常比“答案是什么”更容易回答，因为它避免了开放式探索带来的认知负担，转而选择解释性重构——系统地追溯连接问题和答案的推理过程。论文证明，以答案为条件可以显著提高采样推理路径的预期效用，从而将难以处理的问题转化为可学习的问题。在此基础上，论文提出了一种端到端框架RAVR，它使用以答案为条件的推理作为仅以问题为条件的推理的变分替代。在通用和数学领域的实验表明，RAVR相对于强大的基线模型具有持续的改进。进一步的分析表明，RAVR减少了犹豫，加强了结论的巩固，并促进了推理中特定于问题的策略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可以通过强化学习来提高推理能力，但是当任务超出模型能力时，高质量的推理路径难以采样，容易强化次优推理。

Method: 提出了一种端到端框架RAVR，它使用以答案为条件的推理作为仅以问题为条件的推理的变分替代。

Result: 在通用和数学领域的实验表明，RAVR相对于强大的基线模型具有持续的改进。

Conclusion: RAVR减少了犹豫，加强了结论的巩固，并促进了推理中特定于问题的策略。

Abstract: Reinforcement learning (RL) can refine the reasoning abilities of large
language models (LLMs), but critically depends on a key prerequisite: the LLM
can already generate high-utility reasoning paths with non-negligible
probability. For tasks beyond the LLM's current competence, such reasoning path
can be hard to sample, and learning risks reinforcing familiar but suboptimal
reasoning. We are motivated by the insight from cognitive science that Why is
this the answer is often an easier question than What is the answer, as it
avoids the heavy cognitive load of open-ended exploration, opting instead for
explanatory reconstruction-systematically retracing the reasoning that links a
question to its answer. We show that LLMs can similarly leverage answers to
derive high-quality reasoning paths. We formalize this phenomenon and prove
that conditioning on answer provably increases the expected utility of sampled
reasoning paths, thereby transforming intractable problems into learnable ones.
Building on this insight, we introduce RAVR (Reference-Answer-guided
Variational Reasoning), an end-to-end framework that uses answer-conditioned
reasoning as a variational surrogate for question-only reasoning. Experiments
in both general and math domains demonstrate consistent improvements over
strong baselines. We further analyze the reasoning behavior and find that RAVR
reduces hesitation, strengthens conclusion consolidation, and promotes
problem-specific strategies in reasoning.

</details>


### [76] [FELA: A Multi-Agent Evolutionary System for Feature Engineering of Industrial Event Log Data](https://arxiv.org/abs/2510.25223)
*Kun ouyang,Haoyu Wang,Dong Fang*

Main category: cs.AI

TL;DR: FELA: A multi-agent system using LLMs for automated feature engineering on complex industrial event logs.


<details>
  <summary>Details</summary>
Motivation: Existing AutoML feature engineering methods lack explainability and adaptability for complex event logs.

Method: FELA uses Idea, Code, Critic, and Evaluation agents with a hierarchical knowledge base and an agentic evolution algorithm combining reinforcement learning and genetic algorithms.

Result: FELA generates explainable, domain-relevant features, improving model performance and reducing manual effort on real industrial datasets.

Conclusion: LLM-based multi-agent systems are a promising framework for automated, interpretable, and adaptive feature engineering.

Abstract: Event log data, recording fine-grained user actions and system events,
represent one of the most valuable assets for modern digital services. However,
the complexity and heterogeneity of industrial event logs--characterized by
large scale, high dimensionality, diverse data types, and intricate temporal or
relational structures--make feature engineering extremely challenging. Existing
automatic feature engineering approaches, such as AutoML or genetic methods,
often suffer from limited explainability, rigid predefined operations, and poor
adaptability to complicated heterogeneous data. In this paper, we propose FELA
(Feature Engineering LLM Agents), a multi-agent evolutionary system that
autonomously extracts meaningful and high-performing features from complex
industrial event log data. FELA integrates the reasoning and coding
capabilities of large language models (LLMs) with an insight-guided
self-evolution paradigm. Specifically, FELA employs specialized agents--Idea
Agents, Code Agents, and Critic Agents--to collaboratively generate, validate,
and implement novel feature ideas. An Evaluation Agent summarizes feedback and
updates a hierarchical knowledge base and dual-memory system to enable
continual improvement. Moreover, FELA introduces an agentic evolution
algorithm, combining reinforcement learning and genetic algorithm principles to
balance exploration and exploitation across the idea space. Extensive
experiments on real industrial datasets demonstrate that FELA can generate
explainable, domain-relevant features that significantly improve model
performance while reducing manual effort. Our results highlight the potential
of LLM-based multi-agent systems as a general framework for automated,
interpretable, and adaptive feature engineering in complex real-world
environments.

</details>


### [77] [From Medical Records to Diagnostic Dialogues: A Clinical-Grounded Approach and Dataset for Psychiatric Comorbidity](https://arxiv.org/abs/2510.25232)
*Tianxi Wan,Jiaming Luo,Siyuan Chen,Kunyao Lan,Jianhua Chen,Haiyang Geng,Mengyue Wu*

Main category: cs.AI

TL;DR: 本研究构建了一个用于精神疾病共病诊断的大型对话数据集PsyCoTalk。


<details>
  <summary>Details</summary>
Motivation: 精神共病临床意义重大但诊断具有挑战性，因为多种疾病同时发生。

Method: 研究结合了合成患者电子病历（EMR）构建和多智能体诊断对话生成。

Result: 构建了包含3,000个多轮诊断对话的PsyCoTalk数据集，并通过精神科医生验证。

Conclusion: PsyCoTalk数据集能够提高诊断准确性和治疗计划，为精神共病研究提供有价值的资源。

Abstract: Psychiatric comorbidity is clinically significant yet challenging due to the
complexity of multiple co-occurring disorders. To address this, we develop a
novel approach integrating synthetic patient electronic medical record (EMR)
construction and multi-agent diagnostic dialogue generation. We create 502
synthetic EMRs for common comorbid conditions using a pipeline that ensures
clinical relevance and diversity. Our multi-agent framework transfers the
clinical interview protocol into a hierarchical state machine and context tree,
supporting over 130 diagnostic states while maintaining clinical standards.
Through this rigorous process, we construct PsyCoTalk, the first large-scale
dialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic
dialogues validated by psychiatrists. This dataset enhances diagnostic accuracy
and treatment planning, offering a valuable resource for psychiatric
comorbidity research. Compared to real-world clinical transcripts, PsyCoTalk
exhibits high structural and linguistic fidelity in terms of dialogue length,
token distribution, and diagnostic reasoning strategies. Licensed psychiatrists
confirm the realism and diagnostic validity of the dialogues. This dataset
enables the development and evaluation of models capable of multi-disorder
psychiatric screening in a single conversational pass.

</details>


### [78] [GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement Learning](https://arxiv.org/abs/2510.25320)
*Jiaqi Wu,Qinlao Zhao,Zefeng Chen,Kai Qin,Yifei Zhao,Xueqian Wang,Yuhang Yao*

Main category: cs.AI

TL;DR: 这篇论文提出了一种名为 Graph-based Agent Planning (GAP) 的新框架，通过图结构的规划来显式地建模任务之间的依赖关系，从而实现自适应的并行和串行工具执行，提高了工具利用效率和任务准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的 ReAct 等范式依赖于顺序推理和执行，无法利用独立子任务之间的并行性，导致工具利用效率低下和多步推理场景中的次优性能。

Method: 该方法训练 Agent 基础模型，将复杂任务分解为具有依赖关系的子任务图，自主确定哪些工具可以并行执行，哪些工具必须遵循顺序依赖关系。使用一个高质量的图规划轨迹数据集，并通过监督微调 (SFT) 和强化学习 (RL) 的两阶段训练策略进行训练。

Result: 在 MHQA 数据集上的实验结果表明，GAP 显著优于传统的 ReAct 基线，尤其是在多步检索任务上，并通过智能并行化实现了工具调用效率的显著提高。

Conclusion: GAP 框架通过显式建模任务依赖关系和自适应地执行并行和串行工具，能够有效提高复杂任务的解决效率和准确性。

Abstract: Autonomous agents powered by large language models (LLMs) have shown
impressive capabilities in tool manipulation for complex task-solving. However,
existing paradigms such as ReAct rely on sequential reasoning and execution,
failing to exploit the inherent parallelism among independent sub-tasks. This
sequential bottleneck leads to inefficient tool utilization and suboptimal
performance in multi-step reasoning scenarios. We introduce Graph-based Agent
Planning (GAP), a novel framework that explicitly models inter-task
dependencies through graph-based planning to enable adaptive parallel and
serial tool execution. Our approach trains agent foundation models to decompose
complex tasks into dependency-aware sub-task graphs, autonomously determining
which tools can be executed in parallel and which must follow sequential
dependencies. This dependency-aware orchestration achieves substantial
improvements in both execution efficiency and task accuracy. To train GAP, we
construct a high-quality dataset of graph-based planning traces derived from
the Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage
training strategy: supervised fine-tuning (SFT) on the curated dataset,
followed by reinforcement learning (RL) with a correctness-based reward
function on strategically sampled queries where tool-based reasoning provides
maximum value. Experimental results on MHQA datasets demonstrate that GAP
significantly outperforms traditional ReAct baselines, particularly on
multi-step retrieval tasks, while achieving dramatic improvements in tool
invocation efficiency through intelligent parallelization. The project page is
available at: https://github.com/WJQ7777/Graph-Agent-Planning.

</details>


### [79] [Grouping Nodes With Known Value Differences: A Lossless UCT-based Abstraction Algorithm](https://arxiv.org/abs/2510.25388)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 本文提出了一种新的MCTS抽象框架，称为已知值差分抽象（KVDA），该框架通过推断值差异来对状态和状态-动作对进行分组，而不是像以前的方法那样对值相等的对进行分组。KVDA-UCT在各种确定性环境中优于OGA-UCT。


<details>
  <summary>Details</summary>
Motivation: MCTS的一个核心挑战是其样本效率。现有的OGA-UCT算法使用ASAP框架构建抽象，但ASAP要求状态-动作对具有相同的即时奖励，这限制了可以找到的抽象数量。

Method: 本文提出KVDA框架，该框架通过分析即时奖励来推断值差异，并修改OGA-UCT以使用该框架，称为KVDA-UCT。

Result: KVDA-UCT检测到的抽象比OGA-UCT多得多，并且在各种确定性环境和参数设置下优于OGA-UCT。

Conclusion: 本文提出的KVDA框架可以有效地提高MCTS的样本效率，并在确定性环境中表现出优越的性能。

Abstract: A core challenge of Monte Carlo Tree Search (MCTS) is its sample efficiency,
which can be improved by grouping state-action pairs and using their aggregate
statistics instead of single-node statistics. On the Go Abstractions in Upper
Confidence bounds applied to Trees (OGA-UCT) is the state-of-the-art MCTS
abstraction algorithm for deterministic environments that builds its
abstraction using the Abstractions of State-Action Pairs (ASAP) framework,
which aims to detect states and state-action pairs with the same value under
optimal play by analysing the search graph. ASAP, however, requires two
state-action pairs to have the same immediate reward, which is a rigid
condition that limits the number of abstractions that can be found and thereby
the sample efficiency. In this paper, we break with the paradigm of grouping
value-equivalent states or state-action pairs and instead group states and
state-action pairs with possibly different values as long as the difference
between their values can be inferred. We call this abstraction framework Known
Value Difference Abstractions (KVDA), which infers the value differences by
analysis of the immediate rewards and modifies OGA-UCT to use this framework
instead. The modification is called KVDA-UCT, which detects significantly more
abstractions than OGA-UCT, introduces no additional parameter, and outperforms
OGA-UCT on a variety of deterministic environments and parameter settings.

</details>


### [80] [Agentic AI: A Comprehensive Survey of Architectures, Applications, and Future Directions](https://arxiv.org/abs/2510.25445)
*Mohamad Abou Ali,Fadi Dornaika*

Main category: cs.AI

TL;DR: 本研究旨在通过引入双重范式框架来理清Agentic AI领域中概念混淆的现状，该框架区分了符号/经典和神经/生成两种不同的系统。


<details>
  <summary>Details</summary>
Motivation: Agentic AI发展迅速但理解不统一，常将现代神经系统与过时的符号模型混淆。

Method: 通过系统的PRISMA回顾，分析了90项研究（2018-2025），围绕双重范式框架，从理论基础、领域应用和伦理治理三个维度进行综合分析。

Result: 研究表明，范式的选择是策略性的：符号系统主导安全关键领域（如医疗保健），而神经系统在自适应、数据丰富的环境（如金融）中占主导地位。同时，也发现了关键的研究空白，包括符号系统的治理模型严重不足，以及迫切需要混合神经符号架构。

Conclusion: Agentic AI的未来不在于单一范式的主导地位，而在于它们的有意整合，以创建既适应性强又可靠的系统。

Abstract: Agentic AI represents a transformative shift in artificial intelligence, but
its rapid advancement has led to a fragmented understanding, often conflating
modern neural systems with outdated symbolic models -- a practice known as
conceptual retrofitting. This survey cuts through this confusion by introducing
a novel dual-paradigm framework that categorizes agentic systems into two
distinct lineages: the Symbolic/Classical (relying on algorithmic planning and
persistent state) and the Neural/Generative (leveraging stochastic generation
and prompt-driven orchestration). Through a systematic PRISMA-based review of
90 studies (2018--2025), we provide a comprehensive analysis structured around
this framework across three dimensions: (1) the theoretical foundations and
architectural principles defining each paradigm; (2) domain-specific
implementations in healthcare, finance, and robotics, demonstrating how
application constraints dictate paradigm selection; and (3) paradigm-specific
ethical and governance challenges, revealing divergent risks and mitigation
strategies. Our analysis reveals that the choice of paradigm is strategic:
symbolic systems dominate safety-critical domains (e.g., healthcare), while
neural systems prevail in adaptive, data-rich environments (e.g., finance).
Furthermore, we identify critical research gaps, including a significant
deficit in governance models for symbolic systems and a pressing need for
hybrid neuro-symbolic architectures. The findings culminate in a strategic
roadmap arguing that the future of Agentic AI lies not in the dominance of one
paradigm, but in their intentional integration to create systems that are both
adaptable and reliable. This work provides the essential conceptual toolkit to
guide future research, development, and policy toward robust and trustworthy
hybrid intelligent systems.

</details>


### [81] [Instrumental goals in advanced AI systems: Features to be managed and not failures to be eliminated?](https://arxiv.org/abs/2510.25471)
*Willem Fourie*

Main category: cs.AI

TL;DR: 本文提出了一种关于工具性目标的新框架，认为它们是高级人工智能系统固有的特征，应该被管理而不是被消除。


<details>
  <summary>Details</summary>
Motivation: 传统的人工智能对齐理论将工具性目标视为风险来源，并通过奖励黑客或目标错误泛化等失败模式产生问题。本文提出了一个替代框架。

Method: 借鉴亚里士多德的本体论及其现代解释，将先进的人工智能系统视为人造物，其形式和物质构成产生了不同于其设计者意图的效果。

Result: 工具性目标是其构成的固有结果，而不是意外故障。

Conclusion: 应该减少消除工具性目标的努力，更多地关注理解、管理和指导它们朝着与人类对齐的目标前进。

Abstract: In artificial intelligence (AI) alignment research, instrumental goals, also
called instrumental subgoals or instrumental convergent goals, are widely
associated with advanced AI systems. These goals, which include tendencies such
as power-seeking and self-preservation, become problematic when they conflict
with human aims. Conventional alignment theory treats instrumental goals as
sources of risk that become problematic through failure modes such as reward
hacking or goal misgeneralization, and attempts to limit the symptoms of
instrumental goals, notably resource acquisition and self-preservation. This
article proposes an alternative framing: that a philosophical argument can be
constructed according to which instrumental goals may be understood as features
to be accepted and managed rather than failures to be limited. Drawing on
Aristotle's ontology and its modern interpretations, an ontology of concrete,
goal-directed entities, it argues that advanced AI systems can be seen as
artifacts whose formal and material constitution gives rise to effects distinct
from their designers' intentions. In this view, the instrumental tendencies of
such systems correspond to per se outcomes of their constitution rather than
accidental malfunctions. The implication is that efforts should focus less on
eliminating instrumental goals and more on understanding, managing, and
directing them toward human-aligned ends.

</details>


### [82] [Multi-Objective Search: Algorithms, Applications, and Emerging Directions](https://arxiv.org/abs/2510.25504)
*Oren Salzman,Carlos Hernández Ulloa,Ariel Felner,Sven Koenig*

Main category: cs.AI

TL;DR: 本文综述了多目标搜索 (MOS) 的发展，强调了跨学科的机会，并概述了定义 MOS 新兴前沿的开放挑战。


<details>
  <summary>Details</summary>
Motivation: 现实世界的系统很少优化单一指标，因此多目标搜索 (MOS) 已成为规划和决策问题的统一框架。

Method: 本文对多目标搜索 (MOS) 的发展进行了综述。

Result: 本文强调了跨学科的机会，并概述了定义 MOS 新兴前沿的开放挑战。

Conclusion: 多目标搜索 (MOS) 在机器人、交通运输和运筹学等人工智能应用中越来越受到关注。

Abstract: Multi-objective search (MOS) has emerged as a unifying framework for planning
and decision-making problems where multiple, often conflicting, criteria must
be balanced. While the problem has been studied for decades, recent years have
seen renewed interest in the topic across AI applications such as robotics,
transportation, and operations research, reflecting the reality that real-world
systems rarely optimize a single measure. This paper surveys developments in
MOS while highlighting cross-disciplinary opportunities, and outlines open
challenges that define the emerging frontier of MOS

</details>


### [83] [MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning for Text-to-SQL](https://arxiv.org/abs/2510.25510)
*Zekun Xu,Siyu Xia,Chuhuai Yue,Jiajun Chai,Mingxue Tian,Xiaohan Wang,Wei Lin,Haoxuan Li,Guojun Yin*

Main category: cs.AI

TL;DR: 提出了一种新的Text-to-SQL强化学习框架MTIR-SQL，通过引入多轮工具集成推理和动态反馈来提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖静态执行反馈，限制了实时纠错能力。多轮工具调用和动态反馈可以显著提高适应性和鲁棒性，最终提高模型性能。

Method: 提出了MTIR-SQL框架，引入执行感知的多轮推理模式，在每个推理步骤中无缝结合数据库执行反馈，并增强了GRPO算法，添加了轨迹过滤机制并移除了KL损失约束。

Result: MTIR-SQL在BIRD Dev上实现了64.4%的准确率，在SPIDER Dev上实现了84.6%的执行准确率，显著优于现有方法。

Conclusion: MTIR-SQL框架有效地提高了Text-to-SQL任务中模型的性能。

Abstract: As large language models (LLMs) are increasingly used in Text-to-SQL tasks,
Reinforcement Learning (RL) has become a common method for improving
performance. Existing methods primarily rely on static execution feedback,
which restricts real-time error correction. However, integrating multi-turn
tool invocation along with dynamic feedback could significantly improve
adaptability and robustness, ultimately enhancing model performance. To address
these issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated
Reasoning reinforcement learning framework for Text-to-SQL. Our approach
introduces an execution-aware multi-turn reasoning paradigm that seamlessly
incorporates database execution feedback at each reasoning step, enabling
context-sensitive query generation and progressive refinement throughout the
reasoning process. The framework extends the GRPO algorithm to accommodate
complex multi-turn interaction scenarios. Considering the training instability
characteristics of MTIR and the potential for significant Deviation of model
distribution from the initial model, we enhance the GRPO algorithm by adding a
trajectory filtering mechanism and removing KL loss constraints. Experimental
results demonstrate that MTIR-SQL, with 4B parameters, achieves \textbf{64.4}\%
accuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev,
significantly outperforming existing approaches.

</details>


### [84] [Predicate Renaming via Large Language Models](https://arxiv.org/abs/2510.25517)
*Elisabetta Gentili,Tony Ribeiro,Fabrizio Riguzzi,Katsumi Inoue*

Main category: cs.AI

TL;DR: 本论文探讨了使用大型语言模型（LLM）为逻辑规则中的谓词命名的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的规则生成方法会产生包含未命名谓词的规则，这降低了逻辑理论的可读性、可解释性和可重用性。

Method: 利用大型语言模型处理自然语言和代码的能力，为未命名的谓词提供有意义的命名建议。

Result: 在手工制作的逻辑规则上的评估表明，大型语言模型在此任务中具有潜力。

Conclusion: 大型语言模型可以为逻辑规则中的谓词命名提供有意义的建议。

Abstract: In this paper, we address the problem of giving names to predicates in logic
rules using Large Language Models (LLMs). In the context of Inductive Logic
Programming, various rule generation methods produce rules containing unnamed
predicates, with Predicate Invention being a key example. This hinders the
readability, interpretability, and reusability of the logic theory. Leveraging
recent advancements in LLMs development, we explore their ability to process
natural language and code to provide semantically meaningful suggestions for
giving a name to unnamed predicates. The evaluation of our approach on some
hand-crafted logic rules indicates that LLMs hold potential for this task.

</details>


### [85] [Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and Evaluation](https://arxiv.org/abs/2510.25518)
*Thomas Cook,Richard Osuagwu,Liman Tsatiashvili,Vrynsia Vrynsia,Koustav Ghosal,Maraim Masoud,Riccardo Mattivi*

Main category: cs.AI

TL;DR: 提出了一个agentic RAG架构，以解决金融科技等专业领域中RAG系统面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 在金融科技等专业领域，领域本体、密集术语和缩略语使RAG系统的检索和综合复杂化。

Method: 该方法使用由专门代理组成的模块化流程，支持智能查询重构、迭代子查询分解、基于上下文的缩略语解析和基于交叉编码器的上下文重排序。

Result: 在检索精度和相关性方面，agentic RAG系统优于标准RAG基线，但延迟增加。

Conclusion: 结构化的多代理方法为增强复杂、特定领域环境中的检索鲁棒性提供了一个有希望的方向。

Abstract: Retrieval-Augmented Generation (RAG) systems often face limitations in
specialized domains such as fintech, where domain-specific ontologies, dense
terminology, and acronyms complicate effective retrieval and synthesis. This
paper introduces an agentic RAG architecture designed to address these
challenges through a modular pipeline of specialized agents. The proposed
system supports intelligent query reformulation, iterative sub-query
decomposition guided by keyphrase extraction, contextual acronym resolution,
and cross-encoder-based context re-ranking. We evaluate our approach against a
standard RAG baseline using a curated dataset of 85 question--answer--reference
triples derived from an enterprise fintech knowledge base. Experimental results
demonstrate that the agentic RAG system outperforms the baseline in retrieval
precision and relevance, albeit with increased latency. These findings suggest
that structured, multi-agent methodologies offer a promising direction for
enhancing retrieval robustness in complex, domain-specific settings.

</details>


### [86] [Zero Reinforcement Learning Towards General Domains](https://arxiv.org/abs/2510.25528)
*Yuyuan Zeng,Yufei Huang,Can Xu,Qingfeng Sun,Jianfeng Yan,Guanghui Xu,Tao Yang,Fengzong Lian*

Main category: cs.AI

TL;DR: 提出了一种新的零强化学习范式，旨在提高模型在可验证和不可验证领域的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前零强化学习的研究主要集中在具有易于验证的奖励信号的领域，例如数学、编程和其他推理任务。在验证不直接的更多样化的场景中，激发推理能力仍然未被充分探索。

Method: 通过将可验证的奖励与生成奖励模型相结合，我们跨领域进行多任务零强化学习训练，从而促进它们之间推理能力的转移。此外，为了减轻生成奖励模型中的奖励黑客行为，我们设计了一个平滑的长度惩罚，鼓励在通用领域中生成更全面的思考tokens。

Result: 在Qwen3-8B-Base和Qwen3-14B-Base上的实验结果表明，我们的方法实现了卓越的推理性能，不仅在需要广泛推理的任务上，而且在更一般的任务上。

Conclusion: 该方法在可验证和不可验证领域均能有效提高模型推理能力。

Abstract: Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach
for enhancing the reasoning capabilities of large language models (LLMs) by
directly applying reinforcement learning with verifiable rewards on pretrained
models, without the need for a supervised fine-tuning phase. However, current
research on zero-RL primarily focuses on domains with easily verifiable reward
signals, such as mathematics, programming, and other reasoning tasks. The
challenge of eliciting reasoning abilities in more diverse scenarios, where
verification is not straightforward, remains underexplored. To address this
gap, we propose a novel zero-RL paradigm designed to improve a model's
reasoning ability across both verifiable and non-verifiable domains. By
combining verifiable rewards with a generative reward model, we conduct
multi-task zero-RL training across both domains, facilitating the transfer of
reasoning capabilities between them. Furthermore, to mitigate reward hacking in
the generative reward model, we design a smooth length penalty that encourages
the generation of more comprehensive thinking tokens in general domains.
Experimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our
approach achieves superior reasoning performance, not only on tasks requiring
extensive reasoning but also on more general tasks.

</details>


### [87] [Off-policy Reinforcement Learning with Model-based Exploration Augmentation](https://arxiv.org/abs/2510.25529)
*Likun Wang,Xiangteng Zhang,Yinuo Wang,Guojian Zhan,Wenxuan Wang,Haoyu Gao,Jingliang Duan,Shengbo Eben Li*

Main category: cs.AI

TL;DR: 提出了一种新的探索方法，通过生成欠探索的关键状态和合成动态一致的经验来增强探索。


<details>
  <summary>Details</summary>
Motivation: 现有的探索方法要么在高维环境中表现不佳，要么受限于有限的样本多样性。

Method: 提出了 Modelic Generative Exploration (MoGE)，它通过扩散模型生成关键状态，并使用单步想象世界模型构建关键转移。

Result: 在 OpenAI Gym 和 DeepMind Control Suite 上的实验结果表明，MoGE 可以显著提高样本效率和性能。

Conclusion: MoGE 有效地连接了探索和策略学习，从而在复杂的控制任务中取得了显著的收益。

Abstract: Exploration is fundamental to reinforcement learning (RL), as it determines
how effectively an agent discovers and exploits the underlying structure of its
environment to achieve optimal performance. Existing exploration methods
generally fall into two categories: active exploration and passive exploration.
The former introduces stochasticity into the policy but struggles in
high-dimensional environments, while the latter adaptively prioritizes
transitions in the replay buffer to enhance exploration, yet remains
constrained by limited sample diversity. To address the limitation in passive
exploration, we propose Modelic Generative Exploration (MoGE), which augments
exploration through the generation of under-explored critical states and
synthesis of dynamics-consistent experiences through transition models. MoGE is
composed of two components: (1) a diffusion-based generator that synthesizes
critical states under the guidance of a utility function evaluating each
state's potential influence on policy exploration, and (2) a one-step
imagination world model for constructing critical transitions based on the
critical states for agent learning. Our method adopts a modular formulation
that aligns with the principles of off-policy learning, allowing seamless
integration with existing algorithms to improve exploration without altering
their core structures. Empirical results on OpenAI Gym and DeepMind Control
Suite reveal that MoGE effectively bridges exploration and policy learning,
leading to remarkable gains in both sample efficiency and performance across
complex control tasks.

</details>


### [88] [Standardization of Psychiatric Diagnoses -- Role of Fine-tuned LLM Consortium and OpenAI-gpt-oss Reasoning LLM Enabled Decision Support System](https://arxiv.org/abs/2510.25588)
*Eranga Bandara,Ross Gore,Atmaram Yarlagadda,Anita H. Clayton,Preston Samuel,Christopher K. Rhea,Sachin Shetty*

Main category: cs.AI

TL;DR: 本文提出了一种基于微调大型语言模型（LLM）联盟和OpenAI-gpt-oss推理LLM的决策支持系统，用于精神障碍的临床诊断。


<details>
  <summary>Details</summary>
Motivation: 当前精神障碍的诊断主要依赖于精神科医生和患者之间的对话，这种主观过程可能导致诊断结果不一致。

Method: 该方法利用在精神科医生-患者对话数据集上微调的LLM，并通过基于共识的决策过程整合各个模型的诊断预测，并由OpenAI-gpt-oss推理LLM进行改进。

Result: 实验结果表明，结合微调LLM和推理模型具有创建强大且高度准确的精神健康评估诊断系统的潜力。

Conclusion: 这项工作代表了微调LLM联盟与推理LLM集成的首次应用于临床精神健康诊断，为下一代旨在标准化精神科诊断的AI驱动的电子健康系统铺平了道路。

Abstract: The diagnosis of most mental disorders, including psychiatric evaluations,
primarily depends on dialogues between psychiatrists and patients. This
subjective process can lead to variability in diagnoses across clinicians and
patients, resulting in inconsistencies and challenges in achieving reliable
outcomes. To address these issues and standardize psychiatric diagnoses, we
propose a Fine-Tuned Large Language Model (LLM) Consortium and OpenAI-gpt-oss
Reasoning LLM-enabled Decision Support System for the clinical diagnosis of
mental disorders. Our approach leverages fine-tuned LLMs trained on
conversational datasets involving psychiatrist-patient interactions focused on
mental health conditions (e.g., depression). The diagnostic predictions from
individual models are aggregated through a consensus-based decision-making
process, refined by the OpenAI-gpt-oss reasoning LLM. We propose a novel method
for deploying LLM agents that orchestrate communication between the LLM
consortium and the reasoning LLM, ensuring transparency, reliability, and
responsible AI across the entire diagnostic workflow. Experimental results
demonstrate the transformative potential of combining fine-tuned LLMs with a
reasoning model to create a robust and highly accurate diagnostic system for
mental health assessment. A prototype of the proposed platform, integrating
three fine-tuned LLMs with the OpenAI-gpt-oss reasoning LLM, was developed in
collaboration with the U.S. Army Medical Research Team in Norfolk, Virginia,
USA. To the best of our knowledge, this work represents the first application
of a fine-tuned LLM consortium integrated with a reasoning LLM for clinical
mental health diagnosis paving the way for next-generation AI-powered eHealth
systems aimed at standardizing psychiatric diagnoses.

</details>


### [89] [Counterfactual-based Agent Influence Ranker for Agentic AI Workflows](https://arxiv.org/abs/2510.25612)
*Amit Giloni,Chiara Picardi,Roy Betser,Shamik Bose,Aishvariya Priya Rathina Sabapathy,Roman Vainshtein*

Main category: cs.AI

TL;DR: 提出了一个名为CAIR的，基于反事实分析的Agent Influence Ranker方法，用于评估多智能体系统中每个智能体对最终输出的影响力。


<details>
  <summary>Details</summary>
Motivation: 现有的方法无法评估每个智能体对基于LLM的多智能体系统（AAW）最终输出的影响力，需要从质量和安全角度更深入地了解其运作方式。

Method: 通过执行反事实分析，CAIR提供了一种与任务无关的分析，可以在离线和推理时使用。

Result: CAIR在包含30个不同用例和230个不同功能的AAW数据集上进行了评估，结果表明CAIR产生一致的排名，优于基线方法，并且可以轻松提高下游任务的有效性和相关性。

Conclusion: CAIR是一种评估AAW中每个智能体影响力的有效方法，可以提高下游任务的有效性和相关性。

Abstract: An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system,
is an autonomous system that assembles several LLM-based agents to work
collaboratively towards a shared goal. The high autonomy, widespread adoption,
and growing interest in such AAWs highlight the need for a deeper understanding
of their operations, from both quality and security aspects. To this day, there
are no existing methods to assess the influence of each agent on the AAW's
final output. Adopting techniques from related fields is not feasible since
existing methods perform only static structural analysis, which is unsuitable
for inference time execution. We present Counterfactual-based Agent Influence
Ranker (CAIR) - the first method for assessing the influence level of each
agent on the AAW's output and determining which agents are the most
influential. By performing counterfactual analysis, CAIR provides a
task-agnostic analysis that can be used both offline and at inference time. We
evaluate CAIR using an AAWs dataset of our creation, containing 30 different
use cases with 230 different functionalities. Our evaluation showed that CAIR
produces consistent rankings, outperforms baseline methods, and can easily
enhance the effectiveness and relevancy of downstream tasks.

</details>


### [90] [ALDEN: Reinforcement Learning for Active Navigation and Evidence Gathering in Long Documents](https://arxiv.org/abs/2510.25668)
*Tianyu Yang,Terry Ruas,Yijun Tian,Jan Philip Wahle,Daniel Kurzawe,Bela Gipp*

Main category: cs.AI

TL;DR: ALDEN是一个多轮强化学习框架，它微调视觉语言模型，使其能够主动导航长的、视觉丰富的文档。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常依赖于固定的推理模板或僵化的流程，这迫使视觉语言模型处于被动角色，并阻碍了效率和泛化。

Method: ALDEN引入了一种新颖的fetch动作，可以直接通过索引访问页面，补充了经典的搜索动作，更好地利用文档结构。为了进行密集的过程监督和高效的训练，我们提出了一种基于规则的跨层奖励，该奖励提供turn-和token-level信号。为了解决由长文档中大量视觉token引起的经验观察到的训练不稳定问题，我们进一步提出了一种视觉-语义锚定机制，该机制应用双路径KL散度约束，以在训练期间分别稳定视觉和文本表示。

Result: ALDEN在从三个开源数据集构建的语料库上进行训练，在五个长文档基准测试中实现了最先进的性能。

Conclusion: ALDEN标志着从被动文档阅读到自主导航和跨长、视觉丰富的文档进行推理的agent的一步，为更准确和高效的长文档理解提供了强大的途径。

Abstract: Vision-language models (VLMs) excel at interpreting text-rich images but
struggle with long, visually complex documents that demand analysis and
integration of information spread across multiple pages. Existing approaches
typically rely on fixed reasoning templates or rigid pipelines, which force
VLMs into a passive role and hinder both efficiency and generalization. We
present Active Long-DocumEnt Navigation (ALDEN), a multi-turn reinforcement
learning framework that fine-tunes VLMs as interactive agents capable of
actively navigating long, visually rich documents. ALDEN introduces a novel
fetch action that directly accesses the page by index, complementing the
classic search action and better exploiting document structure. For dense
process supervision and efficient training, we propose a rule-based cross-level
reward that provides both turn- and token-level signals. To address the
empirically observed training instability caused by numerous visual tokens from
long documents, we further propose a visual-semantic anchoring mechanism that
applies a dual-path KL-divergence constraint to stabilize visual and textual
representations separately during training. Trained on a corpus constructed
from three open-source datasets, ALDEN achieves state-of-the-art performance on
five long-document benchmarks. Overall, ALDEN marks a step beyond passive
document reading toward agents that autonomously navigate and reason across
long, visually rich documents, offering a robust path to more accurate and
efficient long-document understanding.

</details>


### [91] [Navigation in a Three-Dimensional Urban Flow using Deep Reinforcement Learning](https://arxiv.org/abs/2510.25679)
*Federica Tonti,Ricardo Vinuesa*

Main category: cs.AI

TL;DR: 本文提出了一种基于深度强化学习的无人机最优导航策略，该策略在具有湍流和再循环区域的城市环境中表现出更高的成功率和更低的碰撞率。


<details>
  <summary>Details</summary>
Motivation: 无人机在城市地区的交付和监视应用日益普及，因此需要在复杂城市环境中实现更优的导航。

Method: 提出了一种流感知的近端策略优化（PPO）算法，并结合了门控Transformer特大型（GTrXL）架构，使智能体能够获得更丰富的湍流场信息。

Result: 与PPO+LSTM、PPO+GTrXL以及传统的Zermelo导航算法相比，所提出的算法在成功率方面有显著提高，碰撞率方面有显著降低。

Conclusion: 该研究为复杂城市环境中完全重新构想的无人机应用前景铺平了道路。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly populating urban areas for
delivery and surveillance purposes. In this work, we develop an optimal
navigation strategy based on Deep Reinforcement Learning. The environment is
represented by a three-dimensional high-fidelity simulation of an urban flow,
characterized by turbulence and recirculation zones. The algorithm presented
here is a flow-aware Proximal Policy Optimization (PPO) combined with a Gated
Transformer eXtra Large (GTrXL) architecture, giving the agent richer
information about the turbulent flow field in which it navigates. The results
are compared with a PPO+GTrXL without the secondary prediction tasks, a PPO
combined with Long Short Term Memory (LSTM) cells and a traditional navigation
algorithm. The obtained results show a significant increase in the success rate
(SR) and a lower crash rate (CR) compared to a PPO+LSTM, PPO+GTrXL and the
classical Zermelo's navigation algorithm, paving the way to a completely
reimagined UAV landscape in complex urban environments.

</details>


### [92] [BambooKG: A Neurobiologically-inspired Frequency-Weight Knowledge Graph](https://arxiv.org/abs/2510.25724)
*Vanya Arikutharam,Arkadiy Ukolov*

Main category: cs.AI

TL;DR: BambooKG: a knowledge graph with frequency-based weights on non-triplet edges to improve reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing retrieval-augmented generation methods struggle with multi-hop reasoning and knowledge graphs miss information that fails to conform to the triplet structure.

Method: Introduce BambooKG, a knowledge graph with frequency-based weights on non-triplet edges.

Result: BambooKG decreases information loss and improves performance on single- and multi-hop reasoning.

Conclusion: BambooKG outperforms existing solutions on reasoning tasks.

Abstract: Retrieval-Augmented Generation allows LLMs to access external knowledge,
reducing hallucinations and ageing-data issues. However, it treats retrieved
chunks independently and struggles with multi-hop or relational reasoning,
especially across documents. Knowledge graphs enhance this by capturing the
relationships between entities using triplets, enabling structured, multi-chunk
reasoning. However, these tend to miss information that fails to conform to the
triplet structure. We introduce BambooKG, a knowledge graph with
frequency-based weights on non-triplet edges which reflect link strength,
drawing on the Hebbian principle of "fire together, wire together". This
decreases information loss and results in improved performance on single- and
multi-hop reasoning, outperforming the existing solutions.

</details>


### [93] [TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling](https://arxiv.org/abs/2510.25758)
*He Hu,Yucheng Zhou,Chiyuan Ma,Qianning Wang,Zheng Zhang,Fei Ma,Laizhong Cui,Qi Tian*

Main category: cs.AI

TL;DR: TheraMind is a novel LLM-based psychological counseling agent with a dual-loop architecture for tactical dialogue management and strategic therapeutic planning, demonstrating superior performance in multi-session coherence, flexibility, and therapeutic attunement.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs in psychological counseling lack emotional understanding, adaptive strategies, and long-term memory for multi-session therapeutic methods, making them unsuitable for real clinical practice.

Method: A dual-loop architecture is introduced, featuring an Intra-Session Loop for tactical dialogue management based on emotional state and a Cross-Session Loop for strategic therapeutic planning and adaptation after each session.

Result: TheraMind outperforms other methods in a high-fidelity simulation environment, especially on multi-session metrics like Coherence, Flexibility, and Therapeutic Attunement.

Conclusion: The dual-loop design of TheraMind effectively emulates strategic, adaptive, and longitudinal therapeutic behavior, demonstrating its potential for advancing LLMs in psychological counseling.

Abstract: Large language models (LLMs) in psychological counseling have attracted
increasing attention. However, existing approaches often lack emotional
understanding, adaptive strategies, and the use of therapeutic methods across
multiple sessions with long-term memory, leaving them far from real clinical
practice. To address these critical gaps, we introduce TheraMind, a strategic
and adaptive agent for longitudinal psychological counseling. The cornerstone
of TheraMind is a novel dual-loop architecture that decouples the complex
counseling process into an Intra-Session Loop for tactical dialogue management
and a Cross-Session Loop for strategic therapeutic planning. The Intra-Session
Loop perceives the patient's emotional state to dynamically select response
strategies while leveraging cross-session memory to ensure continuity.
Crucially, the Cross-Session Loop empowers the agent with long-term
adaptability by evaluating the efficacy of the applied therapy after each
session and adjusting the method for subsequent interactions. We validate our
approach in a high-fidelity simulation environment grounded in real clinical
cases. Extensive evaluations show that TheraMind outperforms other methods,
especially on multi-session metrics like Coherence, Flexibility, and
Therapeutic Attunement, validating the effectiveness of its dual-loop design in
emulating strategic, adaptive, and longitudinal therapeutic behavior. The code
is publicly available at https://0mwwm0.github.io/TheraMind/.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [94] [ODataX: A Progressive Evolution of the Open Data Protocol](https://arxiv.org/abs/2510.24761)
*Anirudh Ganesh,Nitin Sood*

Main category: cs.DB

TL;DR: OData adoption is limited to enterprise environments. This paper introduces ODataX, an evolved version of the protocol designed to address these limitations by simplifying query syntax, providing performance guardrails, and enhancing caching mechanisms.


<details>
  <summary>Details</summary>
Motivation: OData adoption remains confined primarily to enterprise environments, particularly within Microsoft and SAP ecosystems.

Method: This paper analyzes the key barriers preventing wider OData adoption and introduces ODataX, an evolved version of the protocol designed to address these limitations.

Result: ODataX maintains backward compatibility with OData v4 while introducing progressive complexity disclosure through simplified query syntax, built-in performance guardrails via query cost estimation, and enhanced caching mechanisms.

Conclusion: This work aims to bridge the gap between enterprise-grade query standardization and the simplicity demanded by modern web development practices.

Abstract: The Open Data Protocol (OData) provides a standardized approach for building
and consuming RESTful APIs with rich query capabilities. Despite its power and
maturity, OData adoption remains confined primarily to enterprise environments,
particularly within Microsoft and SAP ecosystems. This paper analyzes the key
barriers preventing wider OData adoption and introduces ODataX, an evolved
version of the protocol designed to address these limitations. ODataX maintains
backward compatibility with OData v4 while introducing progressive complexity
disclosure through simplified query syntax, built-in performance guardrails via
query cost estimation, and enhanced caching mechanisms. This work aims to
bridge the gap between enterprise-grade query standardization and the
simplicity demanded by modern web development practices.

</details>


### [95] [StorageXTuner: An LLM Agent-Driven Automatic Tuning Framework for Heterogeneous Storage Systems](https://arxiv.org/abs/2510.25017)
*Qi Lin,Zhenyu Zhang,Viraj Thakkar,Zhenjie Sun,Mai Zheng,Zhichao Cao*

Main category: cs.DB

TL;DR: StorageXTuner是一个用于异构存储引擎的LLM驱动的自动调优框架，通过分离关注点到四个代理来实现：Executor，Extractor，Searcher和Reflector。


<details>
  <summary>Details</summary>
Motivation: 现有的存储系统自动配置很困难，因为参数空间很大，并且条件因工作负载、部署和版本而异。启发式和ML调优器通常是系统特定的，需要手动glue，并且在更改下会降低。最近的基于LLM的方法有所帮助，但通常将调优视为单次、系统特定的任务，这限制了跨系统重用，约束了探索，并削弱了验证。

Method: StorageXTuner将关注点分离到四个代理：Executor（沙盒基准测试），Extractor（性能摘要），Searcher（洞察力引导的配置探索）和Reflector（洞察力生成和管理）。该设计将洞察力驱动的树搜索与分层记忆相结合，从而促进了经验验证的洞察力，并采用轻量级检查器来防止不安全的行为。

Result: 相对于开箱即用的设置和ELMo-Tune，StorageXTuner实现了高达575％和111％的更高吞吐量，将p99延迟降低了高达88％和56％，并以更少的试验次数收敛。

Conclusion: StorageXTuner是一个用于异构存储引擎的LLM驱动的自动调优框架，它优于现有的方法，并在各种存储引擎和工作负载中实现了显著的性能提升。

Abstract: Automatically configuring storage systems is hard: parameter spaces are large
and conditions vary across workloads, deployments, and versions. Heuristic and
ML tuners are often system specific, require manual glue, and degrade under
changes. Recent LLM-based approaches help but usually treat tuning as a
single-shot, system-specific task, which limits cross-system reuse, constrains
exploration, and weakens validation. We present StorageXTuner, an LLM
agent-driven auto-tuning framework for heterogeneous storage engines.
StorageXTuner separates concerns across four agents - Executor (sandboxed
benchmarking), Extractor (performance digest), Searcher (insight-guided
configuration exploration), and Reflector (insight generation and management).
The design couples an insight-driven tree search with layered memory that
promotes empirically validated insights and employs lightweight checkers to
guard against unsafe actions. We implement a prototype and evaluate it on
RocksDB, LevelDB, CacheLib, and MySQL InnoDB with YCSB, MixGraph, and TPC-H/C.
Relative to out-of-the-box settings and to ELMo-Tune, StorageXTuner reaches up
to 575% and 111% higher throughput, reduces p99 latency by as much as 88% and
56%, and converges with fewer trials.

</details>


### [96] [Time-varying Vector Field Compression with Preserved Critical Point Trajectories](https://arxiv.org/abs/2510.25143)
*Mingze Xia,Yuxiao Li,Pu Jiao,Bei Wang,Xin Liang,Hanqi Guo*

Main category: cs.DB

TL;DR: 提出了一种新的有损压缩框架，可以精确地保留时变矢量场中的所有临界点轨迹。


<details>
  <summary>Details</summary>
Motivation: 科学模拟和观测产生了大量随时间变化的矢量场数据，难以存储和传输以进行分析。无损压缩的压缩率较低，无法缓解这个问题。直接应用现有的有损压缩方法可能会在临界点轨迹中引入不希望的失真。

Method: 1. 将空间中保留临界点的理论扩展到保留时空中临界点轨迹。2. 提出了一种半拉格朗日预测器，以利用平流控制区域中的时空相关性，并将其与传统的洛伦佐预测器相结合，以提高压缩效率。

Result: 该方法可实现高达 124.48 倍的压缩率，同时有效地保留所有临界点轨迹。该压缩率比最好的无损压缩器高出 56.07 倍，并且现有的有损压缩器都无法以类似的压缩率保留所有临界点轨迹。

Conclusion: 该论文提出了一种有效的有损压缩框架，可以在高压缩率下精确地保留时变矢量场中的所有临界点轨迹，优于现有的无损和有损压缩方法。

Abstract: Scientific simulations and observations are producing vast amounts of
time-varying vector field data, making it hard to store them for archival
purposes and transmit them for analysis. Lossy compression is considered a
promising approach to reducing these data because lossless compression yields
low compression ratios that barely mitigate the problem. However, directly
applying existing lossy compression methods to timevarying vector fields may
introduce undesired distortions in critical-point trajectories, a crucial
feature that encodes key properties of the vector field. In this work, we
propose an efficient lossy compression framework that exactly preserves all
critical-point trajectories in time-varying vector fields. Our contributions
are threefold. First, we extend the theory for preserving critical points in
space to preserving critical-point trajectories in space-time, and develop a
compression framework to realize the functionality. Second, we propose a
semi-Lagrange predictor to exploit the spatiotemporal correlations in
advectiondominated regions, and combine it with the traditional Lorenzo
predictor for improved compression efficiency. Third, we evaluate our method
against state-of-the-art lossy and lossless compressors using four real-world
scientific datasets. Experimental results demonstrate that the proposed method
delivers up to 124.48X compression ratios while effectively preserving all
critical-point trajectories. This compression ratio is up to 56.07X higher than
that of the best lossless compressors, and none of the existing lossy
compressors can preserve all critical-point trajectories at similar compression
ratios.

</details>


### [97] [DGAI: Decoupled On-Disk Graph-Based ANN Index for Efficient Updates and Queries](https://arxiv.org/abs/2510.25401)
*Jiahao Lou,Quan Yu,Shufeng Gong,Song Yu,Yanfeng Zhang,Ge Yu*

Main category: cs.DB

TL;DR: 本文提出了一种解耦存储架构，以解决传统耦合存储方法在ANN搜索系统中索引更新效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 传统耦合存储方法在更新图索引时，由于需要频繁读取和写入向量，导致大量的无效I/O。

Method: 本文设计了两种策略：(1) 三阶段查询机制，利用多重PQ压缩向量过滤无效I/O和计算；(2) 增量页级拓扑重排序策略，将新节点插入到包含其最相似邻居的页面中，以减少读取放大。

Result: 实验结果表明，解耦架构的插入速度提高了10.05倍，删除速度提高了6.89倍，而三阶段查询和增量重排序使查询效率提高了2.66倍。

Conclusion: 本文提出的解耦存储架构能够有效提高ANN搜索系统的更新和查询效率。

Abstract: On-disk graph-based indexes are widely used in approximate nearest neighbor
(ANN) search systems for large-scale, high-dimensional vectors. However,
traditional coupled storage methods, which store vectors within the index, are
inefficient for index updates. Coupled storage incurs excessive redundant
vector reads and writes when updating the graph topology, leading to
significant invalid I/O. To address this issue, we propose a decoupled storage
architecture. While a decoupled architecture reduces query performance. To
overcome this limitation, we design two tailored strategies: (i) a three-stage
query mechanism that leverages multiple PQ compressed vectors to filter invalid
I/O and computations, and (ii) an incremental page-level topological reordering
strategy that incrementally inserts new nodes into pages containing their most
similar neighbors to mitigate read amplification. Together, these techniques
substantially reduce both I/O and computational overhead during ANN search.
Experimental results show that the decoupled architecture improves update speed
by 10.05x for insertions and 6.89x for deletions, while the three-stage query
and incremental reordering enhance query efficiency by 2.66x compared to the
traditional coupled architecture.

</details>


### [98] [One Join Order Does Not Fit All: Reducing Intermediate Results with Per-Split Query Plans](https://arxiv.org/abs/2510.25684)
*Yujun He,Hangdong Zhao,Simon Frisk,Yifei Yang,Kevin Kristensen,Paraschos Koutris,Xiangyao Yu*

Main category: cs.DB

TL;DR: 这篇论文提出了SplitJoin框架，旨在通过将输入表分割成不同的部分，并为不同部分使用不同的查询计划，从而减少多连接查询处理中的中间结果大小。


<details>
  <summary>Details</summary>
Motivation: 在多连接查询处理中，最小化中间结果至关重要。现有的Yannakakis算法在处理无环查询时表现良好，但对于有环查询仍然面临挑战。

Method: 论文提出了SplitJoin框架，该框架引入了split作为一等查询算子。通过将输入表分割成重和轻的部分，SplitJoin允许不同的数据分区使用不同的查询计划。

Result: SplitJoin在DuckDB和Umbra上实现了显著的改进。在DuckDB上，SplitJoin完成了43个社交网络查询（原生DuckDB只能完成29个），平均运行时间快2.1倍，中间结果小7.9倍。在Umbra上，SplitJoin完成了45个查询（原生Umbra只能完成35个），平均速度提高1.3倍，中间结果小1.2倍。

Conclusion: SplitJoin框架能够有效地减少多连接查询处理中的中间结果大小，并在DuckDB和Umbra上得到了验证。

Abstract: Minimizing intermediate results is critical for efficient multi-join query
processing. Although the seminal Yannakakis algorithm offers strong guarantees
for acyclic queries, cyclic queries remain an open challenge. In this paper, we
propose SplitJoin, a framework that introduces split as a first-class query
operator. By partitioning input tables into heavy and light parts, SplitJoin
allows different data partitions to use distinct query plans, with the goal of
reducing intermediate sizes using existing binary join engines. We
systematically explore the design space for split-based optimizations,
including threshold selection, split strategies, and join ordering after
splits. Implemented as a front-end to DuckDB and Umbra, SplitJoin achieves
substantial improvements: on DuckDB, SplitJoin completes 43 social network
queries (vs. 29 natively), achieving 2.1x faster runtime and 7.9x smaller
intermediates on average (up to 13.6x and 74x, respectively); on Umbra, it
completes 45 queries (vs. 35), achieving 1.3x speedups and 1.2x smaller
intermediates on average (up to 6.1x and 2.1x, respectively).

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [99] [GReF: A Unified Generative Framework for Efficient Reranking via Ordered Multi-token Prediction](https://arxiv.org/abs/2510.25220)
*Zhijie Lin,Zhuofeng Li,Chenglei Dai,Wentian Bao,Shuai Lin,Enyun Yu,Haoxiang Zhang,Liang Zhao*

Main category: cs.IR

TL;DR: 提出了一种名为GReF的统一生成高效重排序框架，以解决两阶段重排序方法中生成器和评估器分离以及自回归生成器推理效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的两阶段重排序方法存在生成器和评估器分离，以及自回归生成器推理效率低下的问题。

Method: 提出了Gen-Reranker，一个具有双向编码器和动态自回归解码器的自回归生成器，用于生成因果重排序序列。通过在物品曝光顺序上预训练Gen-Reranker进行高质量参数初始化，并通过Rerank-DPO进行后训练，以消除对评估器的需求并进行端到端优化。引入了有序多token预测（OMTP），以同时生成多个未来物品，同时保持它们的顺序，从而实现高效的自回归推理。

Result: 离线实验表明，GReF优于最先进的重排序方法，同时实现了与非自回归模型几乎相当的延迟。GReF已部署在拥有超过3亿日活跃用户的真实视频应用程序快手中，显著提高了在线推荐质量。

Conclusion: GReF框架有效地解决了现有重排序方法的挑战，并在实际应用中取得了显著的改进。

Abstract: In a multi-stage recommendation system, reranking plays a crucial role in
modeling intra-list correlations among items. A key challenge lies in exploring
optimal sequences within the combinatorial space of permutations. Recent
research follows a two-stage (generator-evaluator) paradigm, where a generator
produces multiple feasible sequences, and an evaluator selects the best one. In
practice, the generator is typically implemented as an autoregressive model.
However, these two-stage methods face two main challenges. First, the
separation of the generator and evaluator hinders end-to-end training. Second,
autoregressive generators suffer from inference efficiency. In this work, we
propose a Unified Generative Efficient Reranking Framework (GReF) to address
the two primary challenges. Specifically, we introduce Gen-Reranker, an
autoregressive generator featuring a bidirectional encoder and a dynamic
autoregressive decoder to generate causal reranking sequences. Subsequently, we
pre-train Gen-Reranker on the item exposure order for high-quality parameter
initialization. To eliminate the need for the evaluator while integrating
sequence-level evaluation during training for end-to-end optimization, we
propose post-training the model through Rerank-DPO. Moreover, for efficient
autoregressive inference, we introduce ordered multi-token prediction (OMTP),
which trains Gen-Reranker to simultaneously generate multiple future items
while preserving their order, ensuring practical deployment in real-time
recommender systems. Extensive offline experiments demonstrate that GReF
outperforms state-of-the-art reranking methods while achieving latency that is
nearly comparable to non-autoregressive models. Additionally, GReF has also
been deployed in a real-world video app Kuaishou with over 300 million daily
active users, significantly improving online recommendation quality.

</details>


### [100] [TV-Rec: Time-Variant Convolutional Filter for Sequential Recommendation](https://arxiv.org/abs/2510.25259)
*Yehjin Shin,Jeongwhan Choi,Seojin Kim,Noseong Park*

Main category: cs.IR

TL;DR: 提出了一种名为TV-Rec的模型，用时变卷积滤波器取代了传统的固定卷积核和自注意力机制，以提高推荐准确性并减少计算。


<details>
  <summary>Details</summary>
Motivation: 传统的卷积滤波器难以捕捉全局交互，因此通常需要与自注意力机制结合。而固定滤波器难以捕捉用户序列中随时间变化的位置依赖性。

Method: 设计时变卷积滤波器，灵感来源于图信号处理，以捕捉用户序列中的位置依赖性时间变化。

Result: 在六个公共基准数据集上进行的大量实验表明，TV-Rec的性能优于最先进的基线模型，平均提高了7.49%。

Conclusion: TV-Rec通过替换固定卷积核和自注意力机制，实现了更高的表达能力，更好地捕捉了用户行为中复杂的交互模式，同时减少了计算并加速了推理。

Abstract: Recently, convolutional filters have been increasingly adopted in sequential
recommendation for their ability to capture local sequential patterns. However,
most of these models complement convolutional filters with self-attention. This
is because convolutional filters alone, generally fixed filters, struggle to
capture global interactions necessary for accurate recommendation. We propose
Time-Variant Convolutional Filters for Sequential Recommendation (TV-Rec), a
model inspired by graph signal processing, where time-variant graph filters
capture position-dependent temporal variations in user sequences. By replacing
both fixed kernels and self-attention with time-variant filters, TV-Rec
achieves higher expressive power and better captures complex interaction
patterns in user behavior. This design not only eliminates the need for
self-attention but also reduces computation while accelerating inference.
Extensive experiments on six public benchmarks show that TV-Rec outperforms
state-of-the-art baselines by an average of 7.49%.

</details>


### [101] [Revisiting scalable sequential recommendation with Multi-Embedding Approach and Mixture-of-Experts](https://arxiv.org/abs/2510.25285)
*Qiushi Pan,Hao Wang,Guoyuan An,Luankang Zhang,Wei Guo,Yong Liu*

Main category: cs.IR

TL;DR: 提出Fuxi-MME框架，用于提升推荐系统的扩展性，通过多重嵌入策略和混合专家(MoE)架构来捕捉物品的多面特性和动态相关性。


<details>
  <summary>Details</summary>
Motivation: 现有的序列推荐模型在处理物品的多面特性和动态相关性方面存在挑战，限制了模型的可扩展性。

Method: 将传统的单一嵌入矩阵分解为多个低维嵌入矩阵，并用MoE层替换Fuxi Block中的相关参数，实现自适应和专门化的表示转换。

Result: 在公共数据集上的实验结果表明，该框架优于其他基线模型。

Conclusion: Fuxi-MME框架有效地提升了推荐系统的性能和扩展性。

Abstract: In recommendation systems, how to effectively scale up recommendation models
has been an essential research topic. While significant progress has been made
in developing advanced and scalable architectures for sequential
recommendation(SR) models, there are still challenges due to items'
multi-faceted characteristics and dynamic item relevance in the user context.
To address these issues, we propose Fuxi-MME, a framework that integrates a
multi-embedding strategy with a Mixture-of-Experts (MoE) architecture.
Specifically, to efficiently capture diverse item characteristics in a
decoupled manner, we decompose the conventional single embedding matrix into
several lower-dimensional embedding matrices. Additionally, by substituting
relevant parameters in the Fuxi Block with an MoE layer, our model achieves
adaptive and specialized transformation of the enriched representations.
Empirical results on public datasets show that our proposed framework
outperforms several competitive baselines.

</details>


### [102] [Towards Automated Quality Assurance of Patent Specifications: A Multi-Dimensional LLM Framework](https://arxiv.org/abs/2510.25402)
*Yuqian Chai,Chaochao Wang,Weilei Wang*

Main category: cs.IR

TL;DR: 本研究旨在系统评估专利内容质量，重点关注合规性、技术连贯性和图文一致性。


<details>
  <summary>Details</summary>
Motivation: 现有专利评估研究不足，尤其是在人工智能撰写专利涌现的背景下。

Method: 提出一个评估框架，包含合规性、技术连贯性和图文一致性检测模块，并生成改进建议。使用包含人工撰写和AI生成专利的数据集进行验证。

Result: 实验结果表明，三个检测模块的准确率分别为99.74%、82.12%和91.2%。分析揭示了专利各部分、技术领域和作者来源的缺陷分布。

Conclusion: 图文一致性和技术细节精确性需要特别关注。机械工程和建筑领域由于复杂的技术文档要求，在权利要求-说明书一致性方面表现较差。AI生成的专利与人工撰写的专利相比存在显著差距，主要体现在图文对齐和交叉引用的结构性缺陷上。

Abstract: Despite the surge in patent applications and emergence of AI drafting tools,
systematic evaluation of patent content quality has received limited research
attention. To address this gap, We propose to evaluate patents using regulatory
compliance, technical coherence, and figure-reference consistency detection
modules, and then generate improvement suggestions via an integration module.
The framework is validated on a comprehensive dataset comprising 80
human-authored and 80 AI-generated patents from two patent drafting tools.
Experimental results show balanced accuracies of 99.74\%, 82.12\%, and 91.2\%
respectively across the three detection modules when validated against expert
annotations. Additional analysis was conducted to examine defect distributions
across patent sections, technical domains, and authoring sources. Section-based
analysis indicates that figure-text consistency and technical detail precision
require particular attention. Mechanical Engineering and Construction show more
claim-specification inconsistencies due to complex technical documentation
requirements. AI-generated patents show a significant gap compared to
human-authored ones. While human-authored patents primarily contain
surface-level errors like typos, AI-generated patents exhibit more structural
defects in figure-text alignment and cross-references.

</details>


### [103] [Alibaba International E-commerce Product Search Competition DcuRAGONs Team Technical Report](https://arxiv.org/abs/2510.25428)
*Thang-Long Nguyen-Ho,Minh-Khoi Pham,Hoang-Bao Le*

Main category: cs.IR

TL;DR: 本报告介绍了我们在多语言电子商务搜索竞赛中开发的方法和结果，我们的以数据为中心的方法在此次竞赛中获得了最高分。


<details>
  <summary>Details</summary>
Motivation: 该问题旨在识别多语言环境中用户查询与产品项目之间的相关性，并提高电子商务平台的推荐性能。

Method: 利用大型语言模型 (LLM) 及其在其他任务中的能力。

Result: 我们的方法获得了最高分。

Conclusion: 我们的以数据为中心的方法在此次竞赛中获得了最高分。

Abstract: This report details our methodology and results developed for the
Multilingual E-commerce Search Competition. The problem aims to recognize
relevance between user queries versus product items in a multilingual context
and improve recommendation performance on e-commerce platforms. Utilizing Large
Language Models (LLMs) and their capabilities in other tasks, our data-centric
method achieved the highest score compared to other solutions during the
competition. Final leaderboard is publised at
https://alibaba-international-cikm2025.github.io. The source code for our
project is published at https://github.com/nhtlongcs/e-commerce-product-search.

</details>


### [104] [Generalized Pseudo-Relevance Feedback](https://arxiv.org/abs/2510.25488)
*Yiteng Tu,Weihang Su,Yujia Zhou,Yiqun Liu,Fen Lin,Qin Liu,Qingyao Ai*

Main category: cs.IR

TL;DR: 提出了一个名为广义伪相关反馈（GPRF）的框架，用于查询重写，该框架通过强化学习减少了对相关性假设的依赖，并在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统伪相关反馈（PRF）及其向量扩展（VPRF）依赖于相关性假设和模型假设，而基于大型语言模型（LLM）的生成相关性反馈（GRF）存在幻觉问题或依赖相关性假设。为了克服这些限制。

Method: 设计了一个面向效用的训练管道，利用强化学习来确保对噪声反馈的鲁棒性，从而实现无模型、自然语言的重写。

Result: 在多个基准测试和检索器上的大量实验表明，GPRF始终优于强大的基线。

Conclusion: GPRF是一个有效且可推广的查询重写框架。

Abstract: Query rewriting is a fundamental technique in information retrieval (IR). It
typically employs the retrieval result as relevance feedback to refine the
query and thereby addresses the vocabulary mismatch between user queries and
relevant documents. Traditional pseudo-relevance feedback (PRF) and its
vector-based extension (VPRF) improve retrieval performance by leveraging
top-retrieved documents as relevance feedback. However, they are constructed
based on two major hypotheses: the relevance assumption (top documents are
relevant) and the model assumption (rewriting methods need to be designed
specifically for particular model architectures). While recent large language
models (LLMs)-based generative relevance feedback (GRF) enables model-free
query reformulation, it either suffers from severe LLM hallucination or, again,
relies on the relevance assumption to guarantee the effectiveness of rewriting
quality. To overcome these limitations, we introduce an assumption-relaxed
framework: \textit{Generalized Pseudo Relevance Feedback} (GPRF), which
performs model-free, natural language rewriting based on retrieved documents,
not only eliminating the model assumption but also reducing dependence on the
relevance assumption. Specifically, we design a utility-oriented training
pipeline with reinforcement learning to ensure robustness against noisy
feedback. Extensive experiments across multiple benchmarks and retrievers
demonstrate that GPRF consistently outperforms strong baselines, establishing
it as an effective and generalizable framework for query rewriting.

</details>


### [105] [MMQ-v2: Align, Denoise, and Amplify: Adaptive Behavior Mining for Semantic IDs Learning in Recommendation](https://arxiv.org/abs/2510.25622)
*Yi Xu,Moyu Zhang,Chaofan Fan,Jinxin Hu,Xiaochen Li,Yu Zhang,Xiaoyi Zeng,Jing Zhang*

Main category: cs.IR

TL;DR: This paper introduces ADA-SID, a novel approach to generate semantic IDs for recommender systems that addresses the limitations of existing methods by adaptively aligning, denoising, and amplifying multimodal information from content and behavior modalities. It uses a mixture-of-quantization framework (MMQ-v2) with adaptive behavior-content alignment and a dynamic behavioral router.


<details>
  <summary>Details</summary>
Motivation: Existing recommender systems struggle with scalability and generalization in large, dynamic datasets with sparse long-tail data. Content-based Semantic IDs (SIDs) have limited expressive power by ignoring dynamic behavioral properties. Current methods don't account for the skewed and diverse nature of user-item interactions, leading to noise corruption and signal obscurity.

Method: The paper proposes a mixture-of-quantization framework, MMQ-v2, to adaptively align, denoise, and amplify multimodal information. This framework generates semantic IDs named ADA-SID, which includes an adaptive behavior-content alignment and a dynamic behavioral router.

Result: Extensive experiments on public and large-scale industrial datasets demonstrate ADA-SID's significant superiority in both generative and discriminative recommendation tasks.

Conclusion: ADA-SID effectively addresses the limitations of existing semantic ID methods by considering the information richness of content and behavior modalities, leading to improved performance in recommendation tasks.

Abstract: Industrial recommender systems rely on unique Item Identifiers (ItemIDs).
However, this method struggles with scalability and generalization in large,
dynamic datasets that have sparse long-tail data.Content-based Semantic IDs
(SIDs) address this by sharing knowledge through content quantization. However,
by ignoring dynamic behavioral properties, purely content-based SIDs have
limited expressive power. Existing methods attempt to incorporate behavioral
information but overlook a critical distinction: unlike relatively uniform
content features, user-item interactions are highly skewed and diverse,
creating a vast information gap in quality and quantity between popular and
long-tail items. This oversight leads to two critical limitations: (1) Noise
Corruption: Indiscriminate behavior-content alignment allows collaborative
noise from long-tail items to corrupt their content representations, leading to
the loss of critical multimodal information. (2)Signal Obscurity: The
equal-weighting scheme for SIDs fails to reflect the varying importance of
different behavioral signals, making it difficult for downstream tasks to
distinguish important SIDs from uninformative ones. To tackle these issues, we
propose a mixture-of-quantization framework, MMQ-v2, to adaptively Align,
Denoise, and Amplify multimodal information from content and behavior
modalities for semantic IDs learning. The semantic IDs generated by this
framework named ADA-SID. It introduces two innovations: an adaptive
behavior-content alignment that is aware of information richness to shield
representations from noise, and a dynamic behavioral router to amplify critical
signals by applying different weights to SIDs. Extensive experiments on public
and large-scale industrial datasets demonstrate ADA-SID's significant
superiority in both generative and discriminative recommendation tasks.

</details>


### [106] [Retrieval-Augmented Search for Large-Scale Map Collections with ColPali](https://arxiv.org/abs/2510.25718)
*Jamie Mahowald,Benjamin Charles Germain Lee*

Main category: cs.IR

TL;DR: 这篇论文介绍了一个名为 map-RAS 的检索增强搜索系统，用于历史地图的 multimodal 查询，该系统可以通过 ColPali 进行多模态查询，使用 Llama 3.2 总结搜索结果，并支持上传用户自己的数据集以进行跨数据集搜索。


<details>
  <summary>Details</summary>
Motivation: 动机是多模态方法在搜索和导航图书馆、档案馆和博物馆的数字馆藏方面展现出了巨大的潜力。

Method: 该论文介绍了一个名为 map-RAS 的检索增强系统，并详细介绍了其公开演示，该演示用于搜索美国国会图书馆拥有的 101,233 张地图图像。

Result: 用户可以通过 ColPali 进行多模态查询，使用 Llama 3.2 总结搜索结果，并上传他们自己的数据集以执行集合间搜索。

Conclusion: 该系统具有应用于档案管理员、策展人和最终用户的潜力，未来的工作将包括机器学习和数字人文。

Abstract: Multimodal approaches have shown great promise for searching and navigating
digital collections held by libraries, archives, and museums. In this paper, we
introduce map-RAS: a retrieval-augmented search system for historic maps. In
addition to introducing our framework, we detail our publicly-hosted demo for
searching 101,233 map images held by the Library of Congress. With our system,
users can multimodally query the map collection via ColPali, summarize search
results using Llama 3.2, and upload their own collections to perform
inter-collection search. We articulate potential use cases for archivists,
curators, and end-users, as well as future work with our system in both machine
learning and the digital humanities. Our demo can be viewed at:
http://www.mapras.com.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [107] [Fortytwo: Swarm Inference with Peer-Ranked Consensus](https://arxiv.org/abs/2510.24801)
*Vladyslav Larin,Ihor Naumenko,Aleksei Ivashov,Ivan Nikitin,Alexander Firsov*

Main category: cs.LG

TL;DR: Fortytwo, a new protocol, uses swarm intelligence and distributed pairwise ranking to improve AI inference performance.


<details>
  <summary>Details</summary>
Motivation: Demand requires an inference layer that scales horizontally in both capacity and capability as centralized AI hits compute ceilings.

Method: Swarm inference uses peer-ranked, reputation-weighted consensus across heterogeneous models. It uses pairwise ranking with a custom Bradley-Terry-style aggregation model.

Result: Swarm inference outperforms majority voting, achieving 85.90% on GPQA Diamond versus 68.69% for majority voting. It has higher accuracy and resilience to adversarial and noisy free-form prompting across six challenging benchmarks.

Conclusion: Fortytwo establishes a foundation for decentralized AI systems, democratizing access to high-quality inference through collective intelligence without sacrificing reliability or security.

Abstract: As centralized AI hits compute ceilings and diminishing returns from
ever-larger training runs, meeting demand requires an inference layer that
scales horizontally in both capacity and capability. We present Fortytwo, a
novel protocol that leverages swarm intelligence principles and distributed
pairwise ranking consensus to achieve superior performance in AI inference. Our
approach reimagines collaboration among AI nodes using swarm inference: a
peer-ranked, reputation-weighted consensus across heterogeneous models that
surfaces the highest-quality responses. Using pairwise ranking with a custom
Bradley-Terry-style aggregation model, we demonstrate that swarm inference
substantially outperforms majority voting, achieving 85.90% on GPQA Diamond
versus 68.69% for majority voting with the same model set - an improvement of
+17.21 percentage points (approximately +25.1% relative). The protocol
incorporates on-chain reputation so node influence adapts to demonstrated
accuracy over time, yielding a meritocratic consensus that filters low-quality
or malicious participants. To resist Sybil attacks, Fortytwo employs
proof-of-capability in its consensus: nodes must successfully complete
calibration/test requests and stake reputation to enter ranking rounds, making
multi-identity attacks economically unattractive while preserving openness.
Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and
AIME, our evaluation indicates higher accuracy and strong resilience to
adversarial and noisy free-form prompting (e.g., prompt-injection degradation
of only 0.12% versus 6.20% for a monolithic single-model baseline), while
retaining practical deployability. Together, these results establish a
foundation for decentralized AI systems - democratizing access to high-quality
inference through collective intelligence without sacrificing reliability or
security.

</details>


### [108] [From Linear to Nonlinear: Provable Weak-to-Strong Generalization through Feature Learning](https://arxiv.org/abs/2510.24812)
*Junsoo Oh,Jerry Song,Chulhee Yun*

Main category: cs.LG

TL;DR: 本文从理论上分析了从线性 CNN (弱模型) 到两层 ReLU CNN (强模型) 的弱到强泛化。


<details>
  <summary>Details</summary>
Motivation: 先前研究旨在解释这种效应，但大多数理论见解仅限于抽象框架或线性/随机特征模型。

Method: 考虑由不同难度的标签相关信号和标签独立噪声构成的结构化数据，并分析当强模型在由预训练的弱模型标记的数据上训练时，梯度下降的动态。

Result: 我们的分析确定了两种状态——数据稀缺和数据丰富——基于数据集的信噪比特征，并揭示了弱到强泛化的不同机制。在数据稀缺状态下，泛化通过良性过拟合发生或通过有害过拟合失败，这取决于数据的数量，并且我们描述了过渡边界。在数据丰富状态下，泛化在早期阶段通过标签校正出现，但我们观察到过度训练会降低性能。

Conclusion: 弱到强泛化取决于数据量和信噪比，并且可能通过良性过拟合、有害过拟合或标签校正发生。

Abstract: Weak-to-strong generalization refers to the phenomenon where a stronger model
trained under supervision from a weaker one can outperform its teacher. While
prior studies aim to explain this effect, most theoretical insights are limited
to abstract frameworks or linear/random feature models. In this paper, we
provide a formal analysis of weak-to-strong generalization from a linear CNN
(weak) to a two-layer ReLU CNN (strong). We consider structured data composed
of label-dependent signals of varying difficulty and label-independent noise,
and analyze gradient descent dynamics when the strong model is trained on data
labeled by the pretrained weak model. Our analysis identifies two regimes --
data-scarce and data-abundant -- based on the signal-to-noise characteristics
of the dataset, and reveals distinct mechanisms of weak-to-strong
generalization. In the data-scarce regime, generalization occurs via benign
overfitting or fails via harmful overfitting, depending on the amount of data,
and we characterize the transition boundary. In the data-abundant regime,
generalization emerges in the early phase through label correction, but we
observe that overtraining can subsequently degrade performance.

</details>


### [109] [Augmenting Biological Fitness Prediction Benchmarks with Landscapes Features from GraphFLA](https://arxiv.org/abs/2510.24826)
*Mingyu Huang,Shasha Zhou,Ke Li*

Main category: cs.LG

TL;DR: GraphFLA是一个Python框架，用于构建和分析来自诱变数据的适应度地形，它可以计算20个生物学相关的特征，这些特征描述了地形的4个基本方面。


<details>
  <summary>Details</summary>
Motivation: 现有的基准缺乏关于底层适应度地形的拓扑信息，这阻碍了解释和比较模型性能。

Method: GraphFLA构建并分析来自不同模式（例如，DNA、RNA、蛋白质等）的诱变数据的适应度地形，最多可处理数百万个突变体。它计算20个生物学相关的特征，这些特征描述了地形的4个基本方面。

Result: 通过将GraphFLA应用于来自ProteinGym、RNAGym和CIS-BP的5,300多个地形，我们证明了它在解释和比较数十个适应度预测模型的性能方面的效用，突出了影响模型准确性的因素以及不同模型的各自优势。此外，我们发布了155个组合完整的经验适应度地形，涵盖了各种模式的超过220万个序列。

Conclusion: GraphFLA是一个有用的工具，可以解释和比较适应度预测模型的性能，并且它提供了一个全面的经验适应度地形数据集。

Abstract: Machine learning models increasingly map biological sequence-fitness
landscapes to predict mutational effects. Effective evaluation of these models
requires benchmarks curated from empirical data. Despite their impressive
scales, existing benchmarks lack topographical information regarding the
underlying fitness landscapes, which hampers interpretation and comparison of
model performance beyond averaged scores. Here, we introduce GraphFLA, a Python
framework that constructs and analyzes fitness landscapes from mutagensis data
in diverse modalities (e.g., DNA, RNA, protein, and beyond) with up to millions
of mutants. GraphFLA calculates 20 biologically relevant features that
characterize 4 fundamental aspects of landscape topography. By applying
GraphFLA to over 5,300 landscapes from ProteinGym, RNAGym, and CIS-BP, we
demonstrate its utility in interpreting and comparing the performance of dozens
of fitness prediction models, highlighting factors influencing model accuracy
and respective advantages of different models. In addition, we release 155
combinatorially complete empirical fitness landscapes, encompassing over 2.2
million sequences across various modalities. All the codes and datasets are
available at https://github.com/COLA-Laboratory/GraphFLA.

</details>


### [110] [Send Less, Save More: Energy-Efficiency Benchmark of Embedded CNN Inference vs. Data Transmission in IoT](https://arxiv.org/abs/2510.24829)
*Benjamin Karic,Nina Herrmann,Jan Stenkamp,Paula Scharf,Fabian Gieseke,Angela Schwering*

Main category: cs.LG

TL;DR: 本研究探讨了物联网（IoT）和人工智能的结合在环境监测中的应用，旨在解决能源效率问题，特别是在偏远地区。通过在微控制器上使用TinyML和压缩CNN，可以在本地进行推理，减少数据传输量，从而显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 在环境挑战日益严峻的背景下，有效的远程监测解决方案变得至关重要。设计用于环境监测的物联网应用，尤其涉及图像数据时，面临的主要挑战是创建能够在电力受限的偏远地区长期运行的节能物联网设备。

Method: 在ESP32-S3上评估了低功耗广域网络和基于特定领域数据集训练的压缩CNN的使用。采用后训练量化方法压缩模型。

Result: 实验表明，与发送原始图像数据相比，在设备上执行CNN推理并仅传输结果可将总能耗降低高达五倍。模型压缩导致精度略有下降，但可接受。

Conclusion: 研究结果支持开发具有减少碳足迹的物联网应用，通过结合嵌入式机器学习，能够在环境监测场景中自主运行。

Abstract: The integration of the Internet of Things (IoT) and Artificial Intelligence
offers significant opportunities to enhance our ability to monitor and address
ecological changes. As environmental challenges become increasingly pressing,
the need for effective remote monitoring solutions is more critical than ever.
A major challenge in designing IoT applications for environmental monitoring -
particularly those involving image data - is to create energy-efficient IoT
devices capable of long-term operation in remote areas with limited power
availability. Advancements in the field of Tiny Machine Learning allow the use
of Convolutional Neural Networks (CNNs) on resource-constrained,
battery-operated microcontrollers. Since data transfer is energy-intensive,
performing inference directly on microcontrollers to reduce the message size
can extend the operational lifespan of IoT nodes. This work evaluates the use
of common Low Power Wide Area Networks and compressed CNNs trained on domain
specific datasets on an ESP32-S3. Our experiments demonstrate, among other
things, that executing CNN inference on-device and transmitting only the
results reduces the overall energy consumption by a factor of up to five
compared to sending raw image data. %The compression of the model using Post
Training Quantization is accompanied by an acceptable reduction in accuracy of
only a few percentage points compared to a non-quantized model. These findings
advocate the development of IoT applications with reduced carbon footprint and
capable of operating autonomously in environmental monitoring scenarios by
incorporating Embedded Machine Learning.

</details>


### [111] [Aggregation Hides Out-of-Distribution Generalization Failures from Spurious Correlations](https://arxiv.org/abs/2510.24884)
*Olawale Salaudeen,Haoran Zhang,Kumail Alhamoud,Sara Beery,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: 论文发现常用的 OOD 基准测试中 ID 精度和 OOD 精度之间存在强烈的正相关关系（即“线上精度”），但这通常是由于聚合了异构 OOD 样本造成的假象。通过 OODSelect 方法，论文揭示了 OOD 子集，在这些子集中，较高的 ID 精度预测较低的 OOD 精度。这表明聚合指标可能会掩盖 OOD 鲁棒性的重要失败模式。


<details>
  <summary>Details</summary>
Motivation: 研究表明，在分布外泛化（OOD）基准测试中，模型在分布内（ID）和 OOD 上的准确率之间通常存在很强的正相关关系，这被认为意味着虚假相关性在实践中很少见。论文对此提出了质疑。

Method: 论文使用了一种简单的基于梯度的方法 OODSelect，来识别语义连贯的 OOD 子集，在这些子集中，“线上精度”不成立。

Result: 论文发现，在广泛使用的分布偏移基准测试中，OODSelect 揭示了某些子集（有时超过标准 OOD 集的一半），其中较高的 ID 精度预测较低的 OOD 精度。

Conclusion: 论文的发现表明，聚合指标可能会掩盖 OOD 鲁棒性的重要失败模式。

Abstract: Benchmarks for out-of-distribution (OOD) generalization frequently show a
strong positive correlation between in-distribution (ID) and OOD accuracy
across models, termed "accuracy-on-the-line." This pattern is often taken to
imply that spurious correlations - correlations that improve ID but reduce OOD
performance - are rare in practice. We find that this positive correlation is
often an artifact of aggregating heterogeneous OOD examples. Using a simple
gradient-based method, OODSelect, we identify semantically coherent OOD subsets
where accuracy on the line does not hold. Across widely used distribution shift
benchmarks, the OODSelect uncovers subsets, sometimes over half of the standard
OOD set, where higher ID accuracy predicts lower OOD accuracy. Our findings
indicate that aggregate metrics can obscure important failure modes of OOD
robustness. We release code and the identified subsets to facilitate further
research.

</details>


### [112] [Adaptive EEG-based stroke diagnosis with a GRU-TCN classifier and deep Q-learning thresholding](https://arxiv.org/abs/2510.24889)
*Shakeel Abdulkareem,Bora Yimenicioglu,Andrea Yang,Khartik Uppalapati,Aneesh Gudipati,Zhaoyang Fan*

Main category: cs.LG

TL;DR: 该研究提出了一种基于脑电图(EEG)的快速卒中分诊工具，通过自适应多任务分类器，利用GRU-TCN网络预测卒中类型、偏侧性和严重程度，并应用DQN实时调整决策阈值，提高了卒中类型分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 卒中快速分诊需要准确且易于床旁部署的工具，脑电图具有潜力但首次接触时使用不足。

Method: 将32通道脑电信号转换为功率谱密度特征，使用循环卷积网络(GRU-TCN)预测卒中类型、偏侧性和严重程度，并应用深度Q网络(DQN)实时调整决策阈值。

Result: 在UCLH卒中EIT/EEG数据集上，GRU-TCN在卒中类型分类上达到89.3%的准确率，DQN阈值调整后提高到98.0%。在独立低密度脑电图队列(ZJU4H)上测试了鲁棒性，并报告了配对的患者水平统计数据。

Conclusion: 自适应阈值调整将操作点转移到临床上首选的灵敏度-特异性权衡，同时集成的头皮图和频谱可视化支持可解释性。

Abstract: Rapid triage of suspected stroke needs accurate, bedside-deployable tools;
EEG is promising but underused at first contact. We present an adaptive
multitask EEG classifier that converts 32-channel signals to power spectral
density features (Welch), uses a recurrent-convolutional network (GRU-TCN) to
predict stroke type (healthy, ischemic, hemorrhagic), hemispheric
lateralization, and severity, and applies a deep Q-network (DQN) to tune
decision thresholds in real time. Using a patient-wise split of the UCLH Stroke
EIT/EEG data set (44 recordings; about 26 acute stroke, 10 controls), the
primary outcome was stroke-type performance; secondary outcomes were severity
and lateralization. The baseline GRU-TCN reached 89.3% accuracy (F1 92.8%) for
stroke type, about 96.9% (F1 95.9%) for severity, and about 96.7% (F1 97.4%)
for lateralization. With DQN threshold adaptation, stroke-type accuracy
increased to about 98.0% (F1 97.7%). We also tested robustness on an
independent, low-density EEG cohort (ZJU4H) and report paired patient-level
statistics. Analyses follow STARD 2015 guidance for diagnostic accuracy studies
(index test: GRU-TCN+DQN; reference standard: radiology/clinical diagnosis;
patient-wise evaluation). Adaptive thresholding shifts the operating point to
clinically preferred sensitivity-specificity trade-offs, while integrated
scalp-map and spectral visualizations support interpretability.

</details>


### [113] [Topic Analysis with Side Information: A Neural-Augmented LDA Approach](https://arxiv.org/abs/2510.24918)
*Biyi Fang,Kripa Rajshekhar,Truong Vo,Diego Klabjan*

Main category: cs.LG

TL;DR: nnLDA模型通过神经先验机制动态结合辅助信息，能够有效提升主题模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统主题模型难以整合元数据、用户属性或文档标签等辅助信息，限制了其表达性、个性化和可解释性。

Method: 提出nnLDA，一种神经增强的概率主题模型，通过一个神经先验机制动态地结合辅助信息。该模型将每个文档建模为潜在主题的混合，其中主题比例的先验由一个以辅助特征为条件的神经网络生成。开发了一种随机变分EM算法来联合优化神经和概率组件。

Result: 在多个基准数据集上，nnLDA在主题一致性、困惑度和下游分类方面始终优于LDA和Dirichlet-Multinomial Regression。

Conclusion: 将神经表征学习与概率主题建模相结合，在存在辅助信息的设置中具有优势。

Abstract: Traditional topic models such as Latent Dirichlet Allocation (LDA) have been
widely used to uncover latent structures in text corpora, but they often
struggle to integrate auxiliary information such as metadata, user attributes,
or document labels. These limitations restrict their expressiveness,
personalization, and interpretability. To address this, we propose nnLDA, a
neural-augmented probabilistic topic model that dynamically incorporates side
information through a neural prior mechanism. nnLDA models each document as a
mixture of latent topics, where the prior over topic proportions is generated
by a neural network conditioned on auxiliary features. This design allows the
model to capture complex nonlinear interactions between side information and
topic distributions that static Dirichlet priors cannot represent. We develop a
stochastic variational Expectation-Maximization algorithm to jointly optimize
the neural and probabilistic components. Across multiple benchmark datasets,
nnLDA consistently outperforms LDA and Dirichlet-Multinomial Regression in
topic coherence, perplexity, and downstream classification. These results
highlight the benefits of combining neural representation learning with
probabilistic topic modeling in settings where side information is available.

</details>


### [114] [KAN-GCN: Combining Kolmogorov-Arnold Network with Graph Convolution Network for an Accurate Ice Sheet Emulator](https://arxiv.org/abs/2510.24926)
*Zesheng Liu,YoungHyun Koo,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: KAN-GCN是一种用于冰盖建模的快速且准确的模拟器，它将Kolmogorov-Arnold网络(KAN)作为特征校准器置于图卷积网络(GCNs)之前。


<details>
  <summary>Details</summary>
Motivation: 为了提高数值冰盖模型模拟器的性能。

Method: 使用KAN-GCN架构，在GCN之前应用可学习的一维扭曲和线性混合步骤，改善特征条件和非线性编码。

Result: KAN-GCN在2到5层架构中匹配或超过了纯GCN和MLP-GCN基线的准确性。在粗糙网格上，KAN-GCN通过用节点式变换替换边缘式消息传递层来提高推理吞吐量。

Conclusion: KAN-first设计为大型瞬态场景扫描提供了良好的准确性与效率的权衡。

Abstract: We introduce KAN-GCN, a fast and accurate emulator for ice sheet modeling
that places a Kolmogorov-Arnold Network (KAN) as a feature-wise calibrator
before graph convolution networks (GCNs). The KAN front end applies learnable
one-dimensional warps and a linear mixing step, improving feature conditioning
and nonlinear encoding without increasing message-passing depth. We employ this
architecture to improve the performance of emulators for numerical ice sheet
models. Our emulator is trained and tested using 36 melting-rate simulations
with 3 mesh-size settings for Pine Island Glacier, Antarctica. Across 2- to
5-layer architectures, KAN-GCN matches or exceeds the accuracy of pure GCN and
MLP-GCN baselines. Despite a small parameter overhead, KAN-GCN improves
inference throughput on coarser meshes by replacing one edge-wise
message-passing layer with a node-wise transform; only the finest mesh shows a
modest cost. Overall, KAN-first designs offer a favorable accuracy vs.
efficiency trade-off for large transient scenario sweeps.

</details>


### [115] [Continual Low-Rank Adapters for LLM-based Generative Recommender Systems](https://arxiv.org/abs/2510.25093)
*Hyunsik Yoo,Ting-Wei Li,SeongKu Kang,Zhining Liu,Charlie Xu,Qilin Qi,Hanghang Tong*

Main category: cs.LG

TL;DR: 提出了一种名为PESO的持续学习方法，用于解决LLM在推荐系统中持续学习的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA方法忽略了推荐系统的独特性质，即目标不是预测过去的偏好，过时的偏好甚至会损害性能。

Method: PESO引入了一个近端正则化器，将当前适配器锚定到其最近的冻结状态，从而使模型能够灵活地平衡适应和保存。

Result: PESO在经验上优于现有的基于LoRA的持续学习方法。

Conclusion: PESO能够更好地捕捉最近的用户行为。

Abstract: While large language models (LLMs) achieve strong performance in
recommendation, they face challenges in continual learning as users, items, and
user preferences evolve over time. Existing LoRA-based continual methods
primarily focus on preserving performance on previous tasks, but this overlooks
the unique nature of recommendation: the goal is not to predict past
preferences, and outdated preferences can even harm performance when current
interests shift significantly. To address this, we propose PESO (Proximally
rEgularized Single evolving lOra, a continual adaptation method for LoRA in
recommendation. PESO introduces a proximal regularizer that anchors the current
adapter to its most recent frozen state, enabling the model to flexibly balance
adaptation and preservation, and to better capture recent user behaviors.
Theoretically, we show that this proximal design provides data-aware,
direction-wise guidance in the LoRA subspace. Empirically, PESO consistently
outperforms existing LoRA-based continual learning methods.

</details>


### [116] [WBT-BGRL: A Non-Contrastive Weighted Bipartite Link Prediction Model for Inductive Learning](https://arxiv.org/abs/2510.24927)
*Joel Frank Huarayo Quispe,Lilian Berton,Didier Vega-Oliveros*

Main category: cs.LG

TL;DR: 提出了一种新的非对比学习框架WBT-BGRL，用于解决二分图中的链路预测问题。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习方法负样本效率低且有偏差，非对比学习方法仅依赖正样本。现有模型在直推式设置中表现良好，但在归纳式、加权和二分场景中的有效性未经测试。

Method: 该方法使用具有双重GCN编码器的二分架构，并通过三重损失中的加权机制增强了自举学习。

Result: 在真实世界数据集上的结果表明，该方法具有竞争性的性能，尤其是在预训练期间应用加权时。

Conclusion: 加权的非对比学习对于二分图中归纳链路预测具有重要价值。

Abstract: Link prediction in bipartite graphs is crucial for applications like
recommendation systems and failure detection, yet it is less studied than in
monopartite graphs. Contrastive methods struggle with inefficient and biased
negative sampling, while non-contrastive approaches rely solely on positive
samples. Existing models perform well in transductive settings, but their
effectiveness in inductive, weighted, and bipartite scenarios remains untested.
To address this, we propose Weighted Bipartite Triplet-Bootstrapped Graph
Latents (WBT-BGRL), a non-contrastive framework that enhances bootstrapped
learning with a novel weighting mechanism in the triplet loss. Using a
bipartite architecture with dual GCN encoders, WBT-BGRL is evaluated against
adapted state-of-the-art models (T-BGRL, BGRL, GBT, CCA-SSG). Results on
real-world datasets (Industry and E-commerce) show competitive performance,
especially when weighting is applied during pretraining-highlighting the value
of weighted, non-contrastive learning for inductive link prediction in
bipartite graphs.

</details>


### [117] [Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in Chain-of-Thought](https://arxiv.org/abs/2510.24941)
*Jiachen Zhao,Yiyou Sun,Weiyan Shi,Dawn Song*

Main category: cs.LG

TL;DR: 大型语言模型（LLM）生成的思维链（CoT）中，许多推理步骤对模型的预测没有真正的贡献。


<details>
  <summary>Details</summary>
Motivation: 研究表明，CoT中的推理步骤通常被认为是模型内部思维过程的真实反映，并用于监控不安全的意图。然而，许多推理步骤实际上并没有真正促进LLM的预测。

Method: 提出了一种名为“真实思维分数”（TTS）的方法，用于衡量每个推理步骤对模型最终预测的逐步因果影响。通过在LLM的潜在空间中识别“真实思维”方向，可以控制模型执行或忽略某些CoT步骤。

Result: 研究表明，只有一小部分推理步骤具有较高的TTS，能够因果驱动模型的预测。例如，在Qwen-2.5模型下，对于AIME数据集，平均只有2.3%的CoT推理步骤的TTS >= 0.7。研究还发现，CoT中的自我验证步骤也可能是装饰性的。

Conclusion: LLM在内部没有真正执行的情况下，经常会口头表达推理步骤，这损害了LLM推理的效率和CoT的可信度。

Abstract: Recent large language models (LLMs) can generate long Chain-of-Thought (CoT)
at test time, enabling them to solve complex tasks. These reasoning steps in
CoT are often assumed as a faithful reflection of the model's internal thinking
process, and used to monitor unsafe intentions. However, we find many reasoning
steps don't truly contribute to LLMs' prediction. We measure the step-wise
causal influence of each reasoning step on the model's final prediction with a
proposed True Thinking Score (TTS). We reveal that LLMs often interleave
between true-thinking steps (which are genuinely used to produce the final
output) and decorative-thinking steps (which only give the appearance of
reasoning but have minimal causal impact). Notably, only a small subset of the
total reasoning steps have a high TTS that causally drive the model's
prediction: e.g., for the AIME dataset, only an average of 2.3% of reasoning
steps in CoT have a TTS >= 0.7 (range: 0-1) under the Qwen-2.5 model.
Furthermore, we identify a TrueThinking direction in the latent space of LLMs.
By steering along or against this direction, we can force the model to perform
or disregard certain CoT steps when computing the final result. Finally, we
highlight that self-verification steps in CoT (i.e., aha moments) can also be
decorative, where LLMs do not truly verify their solution. Steering along the
TrueThinking direction can force internal reasoning over these steps, resulting
in a change in the final results. Overall, our work reveals that LLMs often
verbalize reasoning steps without actually performing them internally, which
undermines both the efficiency of LLM reasoning and the trustworthiness of CoT.

</details>


### [118] [Finding Culture-Sensitive Neurons in Vision-Language Models](https://arxiv.org/abs/2510.24942)
*Xiutian Zhao,Rochelle Choenni,Rohit Saxena,Ivan Titov*

Main category: cs.LG

TL;DR: 视觉语言模型在文化相关的输入上表现不佳，本研究旨在理解其如何处理文化信息。


<details>
  <summary>Details</summary>
Motivation: 探究视觉语言模型(VLM)处理文化背景信息的能力，因为现有VLM在文化相关的输入上表现不佳。

Method: 1. 识别文化选择性神经元；2. 通过停用这些神经元进行因果测试；3. 提出新的基于边际的选择器 - 对比激活选择(CAS)；4. 进行逐层分析。

Result: 1. 存在一些神经元，它们的缺失会严重影响模型在相应文化问题上的表现，而对其他问题影响很小；2. 新提出的CAS方法优于现有的基于概率和熵的方法；3. 这些神经元倾向于聚集在特定的解码器层。

Conclusion: 研究揭示了多模态表征的内部组织方式。

Abstract: Despite their impressive performance, vision-language models (VLMs) still
struggle on culturally situated inputs. To understand how VLMs process
culturally grounded information, we study the presence of culture-sensitive
neurons, i.e. neurons whose activations show preferential sensitivity to inputs
associated with particular cultural contexts. We examine whether such neurons
are important for culturally diverse visual question answering and where they
are located. Using the CVQA benchmark, we identify neurons of culture
selectivity and perform causal tests by deactivating the neurons flagged by
different identification methods. Experiments on three VLMs across 25 cultural
groups demonstrate the existence of neurons whose ablation disproportionately
harms performance on questions about the corresponding cultures, while having
minimal effects on others. Moreover, we propose a new margin-based selector -
Contrastive Activation Selection (CAS), and show that it outperforms existing
probability- and entropy-based methods in identifying culture-sensitive
neurons. Finally, our layer-wise analyses reveals that such neurons tend to
cluster in certain decoder layers. Overall, our findings shed new light on the
internal organization of multimodal representations.

</details>


### [119] [Resource-Efficient and Robust Inference of Deep and Bayesian Neural Networks on Embedded and Analog Computing Platforms](https://arxiv.org/abs/2510.24951)
*Bernhard Klein*

Main category: cs.LG

TL;DR: 本文通过算法和硬件的协同设计，提高了传统和贝叶斯神经网络的资源效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习计算需求日益增长，限制了其在嵌入式和资源受限平台上的可扩展性和效率。神经网络不仅要高效运行，还要在分布偏移或未知数据下提供可靠的预测。贝叶斯神经网络虽然可以量化不确定性，但计算开销进一步加剧了这些挑战。

Method: 1. Galen：执行自动层特定压缩，由敏感性分析和硬件在环反馈指导。2. 扩展噪声训练到非平稳条件，提高鲁棒性和稳定性。3. 开发分析和集成近似，代替昂贵的采样，集成到编译器堆栈中，并优化嵌入式推理。4. 概率光子计算：控制模拟噪声作为内在熵源，直接在硬件中实现快速、节能的概率推理。

Result: 通过算法-硬件协同设计，效率和可靠性可以共同提高。

Conclusion: 为下一代可信赖、节能的机器学习系统奠定了基础。

Abstract: While modern machine learning has transformed numerous application domains,
its growing computational demands increasingly constrain scalability and
efficiency, particularly on embedded and resource-limited platforms. In
practice, neural networks must not only operate efficiently but also provide
reliable predictions under distributional shifts or unseen data. Bayesian
neural networks offer a principled framework for quantifying uncertainty, yet
their computational overhead further compounds these challenges.
  This work advances resource-efficient and robust inference for both
conventional and Bayesian neural networks through the joint pursuit of
algorithmic and hardware efficiency. The former reduces computation through
model compression and approximate Bayesian inference, while the latter
optimizes deployment on digital accelerators and explores analog hardware,
bridging algorithmic design and physical realization. The first contribution,
Galen, performs automatic layer-specific compression guided by sensitivity
analysis and hardware-in-the-loop feedback. Analog accelerators offer
efficiency gains at the cost of noise; this work models device imperfections
and extends noisy training to nonstationary conditions, improving robustness
and stability. A second line of work advances probabilistic inference,
developing analytic and ensemble approximations that replace costly sampling,
integrate into a compiler stack, and optimize embedded inference. Finally,
probabilistic photonic computing introduces a paradigm where controlled analog
noise acts as an intrinsic entropy source, enabling fast, energy-efficient
probabilistic inference directly in hardware.
  Together, these studies demonstrate how efficiency and reliability can be
advanced jointly through algorithm-hardware co-design, laying the foundation
for the next generation of trustworthy, energy-efficient machine-learning
systems.

</details>


### [120] [Sequences of Logits Reveal the Low Rank Structure of Language Models](https://arxiv.org/abs/2510.24966)
*Noah Golowich,Allen Liu,Abhishek Shetty*

Main category: cs.LG

TL;DR: 大型语言模型具有内在的低维结构，本研究旨在理解这种结构。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型的内在低维结构是一个重要问题。

Method: 从模型无关的层面，将语言模型视为序列概率模型，研究其低维结构。通过模型logits构建矩阵，并观察其秩。

Result: 实验证明，各种现代语言模型都表现出低秩结构。利用这种低秩结构，可以通过对模型在无关或无意义提示上的输出进行线性组合，来生成对目标提示的响应。

Conclusion: 研究语言模型的近似秩可以得到一个简单的通用抽象，其理论预测与实验结果平行。分析了该抽象的表示能力，并给出了可证明的学习保证。

Abstract: A major problem in the study of large language models is to understand their
inherent low-dimensional structure. We introduce an approach to study the
low-dimensional structure of language models at a model-agnostic level: as
sequential probabilistic models. We first empirically demonstrate that a wide
range of modern language models exhibit low-rank structure: in particular,
matrices built from the model's logits for varying sets of prompts and
responses have low approximate rank. We then show that this low-rank structure
can be leveraged for generation -- in particular, we can generate a response to
a target prompt using a linear combination of the model's outputs on unrelated,
or even nonsensical prompts.
  On the theoretical front, we observe that studying the approximate rank of
language models in the sense discussed above yields a simple universal
abstraction whose theoretical predictions parallel our experiments. We then
analyze the representation power of the abstraction and give provable learning
guarantees.

</details>


### [121] [Conformational Rank Conditioned Committees for Machine Learning-Assisted Directed Evolution](https://arxiv.org/abs/2510.24974)
*Mia Adler,Carrie Liang,Brian Peng,Oleg Presnyakov,Justin M. Baker,Jannelle Lauffer,Himani Sharma,Barry Merriman*

Main category: cs.LG

TL;DR: 这篇论文介绍了一种新的机器学习辅助定向进化（MLDE）框架，用于优化抗体适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的结构感知MLDE流程无法区分构象不确定性和认知不确定性。

Method: 该框架利用排序的构象，为每个等级分配一个深度神经网络委员会。

Result: 在SARS-CoV-2抗体停靠实验中，该方法比基线策略有显著改进。

Conclusion: 该研究提供了一种可扩展的治疗性抗体发现途径，并直接解决了建模构象不确定性的挑战。

Abstract: Machine Learning-assisted directed evolution (MLDE) is a powerful tool for
efficiently navigating antibody fitness landscapes. Many structure-aware MLDE
pipelines rely on a single conformation or a single committee across all
conformations, limiting their ability to separate conformational uncertainty
from epistemic uncertainty. Here, we introduce a rank -conditioned committee
(RCC) framework that leverages ranked conformations to assign a deep neural
network committee per rank. This design enables a principled separation between
epistemic uncertainty and conformational uncertainty. We validate our approach
on SARS-CoV-2 antibody docking, demonstrating significant improvements over
baseline strategies. Our results offer a scalable route for therapeutic
antibody discovery while directly addressing the challenge of modeling
conformational uncertainty.

</details>


### [122] [Strategic inputs: feature selection from game-theoretic perspective](https://arxiv.org/abs/2510.24982)
*Chi Zhao,Jing Liu,Elena Parilina*

Main category: cs.LG

TL;DR: 提出了一种基于博弈论的表格数据特征选择框架，以降低大规模机器学习的计算成本。


<details>
  <summary>Details</summary>
Motivation: 许多特征对模型性能没有积极贡献，但消耗了大量的计算资源。

Method: 该方法基于合作博弈，将特征建模为参与者，并通过评估协同交互和边际贡献来确定其重要性。该框架包括样本选择、博弈论特征重要性评估、冗余特征消除和优化模型训练。

Result: 该方法在保持预测性能的同时，实现了大量的计算减少。

Conclusion: 该方法为大规模机器学习的计算挑战提供了一种有效的解决方案。

Abstract: The exponential growth of data volumes has led to escalating computational
costs in machine learning model training. However, many features fail to
contribute positively to model performance while consuming substantial
computational resources. This paper presents an end-to-end feature selection
framework for tabular data based on game theory. We formulate feature selection
procedure based on a cooperative game where features are modeled as players,
and their importance is determined through the evaluation of synergistic
interactions and marginal contributions. The proposed framework comprises four
core components: sample selection, game-theoretic feature importance
evaluation, redundant feature elimination, and optimized model training.
Experimental results demonstrate that the proposed method achieves substantial
computation reduction while preserving predictive performance, thereby offering
an efficient solution of the computational challenges of large-scale machine
learning. The source code is available at
https://github.com/vectorsss/strategy_inputs.

</details>


### [123] [LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies](https://arxiv.org/abs/2510.24983)
*Ximan Sun,Xiang Cheng*

Main category: cs.LG

TL;DR: LRT-Diffusion introduces a risk-aware sampling rule for diffusion policies in offline RL, using a sequential hypothesis test to control risk with a user-specified Type-I error level alpha.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion policies for offline RL lack a statistical notion of risk during sampling.

Method: LRT-Diffusion treats each denoising step as a sequential hypothesis test, accumulating a log-likelihood ratio and gating the conditional mean with a logistic controller. It standardizes states and actions and uses a state-conditional out-of-distribution (OOD) metric.

Result: LRT-Diffusion improves the return-OOD trade-off over Q-guided baselines while honoring the desired alpha on D4RL MuJoCo tasks. It also provides level-alpha calibration and stability bounds.

Conclusion: LRT-Diffusion is a drop-in, inference-time method that adds principled, calibrated risk control to diffusion policies for offline RL.

Abstract: Diffusion policies are competitive for offline reinforcement learning (RL)
but are typically guided at sampling time by heuristics that lack a statistical
notion of risk. We introduce LRT-Diffusion, a risk-aware sampling rule that
treats each denoising step as a sequential hypothesis test between the
unconditional prior and the state-conditional policy head. Concretely, we
accumulate a log-likelihood ratio and gate the conditional mean with a logistic
controller whose threshold tau is calibrated once under H0 to meet a
user-specified Type-I level alpha. This turns guidance from a fixed push into
an evidence-driven adjustment with a user-interpretable risk budget.
Importantly, we deliberately leave training vanilla (two heads with standard
epsilon-prediction) under the structure of DDPM. LRT guidance composes
naturally with Q-gradients: critic-gradient updates can be taken at the
unconditional mean, at the LRT-gated mean, or a blend, exposing a continuum
from exploitation to conservatism. We standardize states and actions
consistently at train and test time and report a state-conditional
out-of-distribution (OOD) metric alongside return. On D4RL MuJoCo tasks,
LRT-Diffusion improves the return-OOD trade-off over strong Q-guided baselines
in our implementation while honoring the desired alpha. Theoretically, we
establish level-alpha calibration, concise stability bounds, and a return
comparison showing when LRT surpasses Q-guidance-especially when off-support
errors dominate. Overall, LRT-Diffusion is a drop-in, inference-time method
that adds principled, calibrated risk control to diffusion policies for offline
RL.

</details>


### [124] [Epileptic Seizure Detection and Prediction from EEG Data: A Machine Learning Approach with Clinical Validation](https://arxiv.org/abs/2510.24986)
*Ria Jayanti,Tanish Jain*

Main category: cs.LG

TL;DR: This paper introduces a machine learning approach for real-time seizure detection and prediction using EEG data.


<details>
  <summary>Details</summary>
Motivation: Early intervention and proactive treatment of seizures are limited by traditional methods that only detect seizures after they begin.

Method: The study uses supervised machine learning algorithms (K-Nearest Neighbors, Logistic Regression, Random Forest, and Support Vector Machine) for seizure detection and Long Short-Term Memory (LSTM) networks for seizure prediction. The CHB-MIT Scalp EEG Database was used for evaluation.

Result: Logistic Regression achieved 90.9% detection accuracy with 89.6% recall. The LSTM model achieved 89.26% prediction accuracy. Random Forest and SVM achieved higher accuracy (94.0%) but with 0% recall.

Conclusion: The study demonstrates the potential of real-time monitoring tools that can predict seizures before they occur, allowing for a shift from reactive to proactive seizure management.

Abstract: In recent years, machine learning has become an increasingly powerful tool
for supporting seizure detection and monitoring in epilepsy care. Traditional
approaches focus on identifying seizures only after they begin, which limits
the opportunity for early intervention and proactive treatment. In this study,
we propose a novel approach that integrates both real-time seizure detection
and prediction, aiming to capture subtle temporal patterns in EEG data that may
indicate an upcoming seizure. Our approach was evaluated using the CHB-MIT
Scalp EEG Database, which includes 969 hours of recordings and 173 seizures
collected from 23 pediatric and young adult patients with drug-resistant
epilepsy. To support seizure detection, we implemented a range of supervised
machine learning algorithms, including K-Nearest Neighbors, Logistic
Regression, Random Forest, and Support Vector Machine. The Logistic Regression
achieved 90.9% detection accuracy with 89.6% recall, demonstrating balanced
performance suitable for clinical screening. Random Forest and Support Vector
Machine models achieved higher accuracy (94.0%) but with 0% recall, failing to
detect any seizures, illustrating that accuracy alone is insufficient for
evaluating medical ML models with class imbalance. For seizure prediction, we
employed Long Short-Term Memory (LSTM) networks, which use deep learning to
model temporal dependencies in EEG data. The LSTM model achieved 89.26%
prediction accuracy. These results highlight the potential of developing
accessible, real-time monitoring tools that not only detect seizures as
traditionally done, but also predict them before they occur. This ability to
predict seizures marks a significant shift from reactive seizure management to
a more proactive approach, allowing patients to anticipate seizures and take
precautionary measures to reduce the risk of injury or other complications.

</details>


### [125] [Enhancing Hierarchical Reinforcement Learning through Change Point Detection in Time Series](https://arxiv.org/abs/2510.24988)
*Hemanath Arumugam,Falong Fan,Bo Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的分层强化学习架构，该架构将基于 Transformer 的自监督变化点检测 (CPD) 模块集成到 Option-Critic 框架中，从而能够自适应地分割状态轨迹并发现选项。


<details>
  <summary>Details</summary>
Motivation: 分层强化学习 (HRL) 通过引入跨越多个时间步长的选项策略（时间抽象）来增强长期任务中决策的可扩展性。尽管 HRL 具有理论上的吸引力，但 HRL 的实际应用面临着自主发现具有语义意义的子目标和学习最优选项终止边界的挑战。

Method: 该架构集成了基于 Transformer 的自监督变化点检测 (CPD) 模块到 Option-Critic 框架中，无需外部监督即可推断环境动态中的潜在变化。这些推断的变化点通过以下三种关键方式被利用：(i) 作为稳定终止函数梯度的监督信号，(ii) 通过分段行为克隆来预训练选项内策略，以及 (iii) 通过对 CPD 定义的状态分区进行选项间散度惩罚来加强功能专业化。

Result: 在 Four-Rooms 和 Pinball 任务上的实验表明，CPD 指导的 agent 表现出加速的收敛、更高的累积回报和显着改进的选项专业化。

Conclusion: 将结构先验知识通过变化点分割集成可以带来在复杂环境中更易于解释、样本效率更高且更鲁棒的分层策略。

Abstract: Hierarchical Reinforcement Learning (HRL) enhances the scalability of
decision-making in long-horizon tasks by introducing temporal abstraction
through options-policies that span multiple timesteps. Despite its theoretical
appeal, the practical implementation of HRL suffers from the challenge of
autonomously discovering semantically meaningful subgoals and learning optimal
option termination boundaries. This paper introduces a novel architecture that
integrates a self-supervised, Transformer-based Change Point Detection (CPD)
module into the Option-Critic framework, enabling adaptive segmentation of
state trajectories and the discovery of options. The CPD module is trained
using heuristic pseudo-labels derived from intrinsic signals to infer latent
shifts in environment dynamics without external supervision. These inferred
change-points are leveraged in three critical ways: (i) to serve as supervisory
signals for stabilizing termination function gradients, (ii) to pretrain
intra-option policies via segment-wise behavioral cloning, and (iii) to enforce
functional specialization through inter-option divergence penalties over
CPD-defined state partitions. The overall optimization objective enhances the
standard actor-critic loss using structure-aware auxiliary losses. In our
framework, option discovery arises naturally as CPD-defined trajectory segments
are mapped to distinct intra-option policies, enabling the agent to
autonomously partition its behavior into reusable, semantically meaningful
skills. Experiments on the Four-Rooms and Pinball tasks demonstrate that
CPD-guided agents exhibit accelerated convergence, higher cumulative returns,
and significantly improved option specialization. These findings confirm that
integrating structural priors via change-point segmentation leads to more
interpretable, sample-efficient, and robust hierarchical policies in complex
environments.

</details>


### [126] [What Really Matters in Matrix-Whitening Optimizers?](https://arxiv.org/abs/2510.25000)
*Kevin Frans,Pieter Abbeel,Sergey Levine*

Main category: cs.LG

TL;DR: 本文研究了矩阵白化优化器，发现它们优于Adam等元素级优化器。


<details>
  <summary>Details</summary>
Motivation: 探究矩阵白化优化器性能优于元素级优化器的关键原因。

Method: 系统地解构矩阵白化优化器，并进行实验分析。

Result: 矩阵白化的性能提升并非完全由精确的谱归一化解释，方差适应是关键因素。方差适应优化器始终优于符号下降优化器。低秩方差估计器可以在不损失性能的情况下有效降低内存成本。

Conclusion: 矩阵白化主要有两个目的，其中方差适应是被忽略的性能提升因素。

Abstract: A range of recent optimizers have emerged that approximate the same
"matrix-whitening" transformation in various ways. In this work, we
systematically deconstruct such optimizers, aiming to disentangle the key
components that explain performance. Across tuned hyperparameters across the
board, all flavors of matrix-whitening methods reliably outperform elementwise
counterparts, such as Adam. Matrix-whitening is often related to spectral
descent -- however, experiments reveal that performance gains are *not
explained solely by accurate spectral normalization* -- particularly, SOAP
displays the largest per-step gain, even though Muon more accurately descends
along the steepest spectral descent direction. Instead, we argue that
matrix-whitening serves two purposes, and the variance adaptation component of
matrix-whitening is the overlooked ingredient explaining this performance gap.
Experiments show that variance-adapted versions of optimizers consistently
outperform their sign-descent counterparts, including an adaptive version of
Muon. We further ablate variance adaptation strategies, finding that while
lookahead style approximations are not as effective, low-rank variance
estimators can effectively reduce memory costs without a performance loss.

</details>


### [127] [Disentangling Shared and Private Neural Dynamics with SPIRE: A Latent Modeling Framework for Deep Brain Stimulation](https://arxiv.org/abs/2510.25023)
*Rahil Soroushmojdehi,Sina Javadzadeh,Mehrnaz Asadi,Terence D. Sanger*

Main category: cs.LG

TL;DR: SPIRE是一种深度多编码器自编码器，可以将多区域神经数据分解为共享和私有潜在子空间。


<details>
  <summary>Details</summary>
Motivation: 在建模多区域神经数据时，将共享的网络级动态与特定区域的活动分离是一个核心挑战。

Method: 该模型使用新颖的对齐和解缠损失，仅在基线数据上进行训练，以稳健地恢复跨区域结构，并揭示外部扰动如何重组它。

Result: 在合成基准测试中，SPIRE优于非线性失真和时间错位下的经典概率模型。应用于颅内深部脑刺激 (DBS) 记录，SPIRE 表明共享潜在变量可靠地编码刺激特异性特征，这些特征可以跨位点和频率推广。

Conclusion: SPIRE 是一种实用、可重复的工具，用于分析刺激下的多区域神经动力学。

Abstract: Disentangling shared network-level dynamics from region-specific activity is
a central challenge in modeling multi-region neural data. We introduce SPIRE
(Shared-Private Inter-Regional Encoder), a deep multi-encoder autoencoder that
factorizes recordings into shared and private latent subspaces with novel
alignment and disentanglement losses. Trained solely on baseline data, SPIRE
robustly recovers cross-regional structure and reveals how external
perturbations reorganize it. On synthetic benchmarks with ground-truth latents,
SPIRE outperforms classical probabilistic models under nonlinear distortions
and temporal misalignments. Applied to intracranial deep brain stimulation
(DBS) recordings, SPIRE shows that shared latents reliably encode
stimulation-specific signatures that generalize across sites and frequencies.
These results establish SPIRE as a practical, reproducible tool for analyzing
multi-region neural dynamics under stimulation.

</details>


### [128] [Machine Learning based Analysis for Radiomics Features Robustness in Real-World Deployment Scenarios](https://arxiv.org/abs/2510.25026)
*Sarmad Ahmad Khan,Simon Bernatz,Zahra Moslehi,Florian Buettner*

Main category: cs.LG

TL;DR: 该研究调查了基于放射组学的机器学习模型在 MRI 序列分布偏移下的鲁棒性，发现使用协议不变特征训练的模型在分布偏移下保持较高的 F1 值，而使用所有特征的模型性能下降。数据增强提高了不确定性估计的质量，而温度缩放效果不明显。研究结果表明，协议感知特征选择和受控体模研究可以有效预测模型在分布偏移下的行为。


<details>
  <summary>Details</summary>
Motivation: 基于放射组学的机器学习模型在临床决策支持方面显示出前景，但容易受到成像协议、定位和分割变化引起的分布偏移的影响。

Method: 使用包含 16 个水果的体模，评估了 T2-HASTE、T2-TSE、T2-MAP、T1-TSE 和 T2-FLAIR 序列的协议变化、分割变化（完整、部分、旋转）以及观察者间变异对模型可靠性的影响。在 8 个一致的鲁棒特征与序列特异性特征上训练 XGBoost 分类器，测试模型在同域和异域条件下的性能。

Result: 使用协议不变特征训练的模型在分布偏移下保持 F1 值 >0.85，而使用所有特征的模型在协议更改下表现出 40% 的性能下降。数据集增强显着提高了不确定性估计的质量，并将预期校准误差 (ECE) 降低了 35%，且不牺牲准确性。

Conclusion: 协议感知特征选择和受控体模研究可以有效预测模型在分布偏移下的行为，为开发能够抵抗实际协议变化的鲁棒放射组学模型提供了一个框架。

Abstract: Radiomics-based machine learning models show promise for clinical decision
support but are vulnerable to distribution shifts caused by variations in
imaging protocols, positioning, and segmentation. This study systematically
investigates the robustness of radiomics-based machine learning models under
distribution shifts across five MRI sequences. We evaluated how different
acquisition protocols and segmentation strategies affect model reliability in
terms of predictive power and uncertainty-awareness. Using a phantom of 16
fruits, we evaluated distribution shifts through: (1) protocol variations
across T2-HASTE, T2-TSE, T2-MAP, T1-TSE, and T2-FLAIR sequences; (2)
segmentation variations (full, partial, rotated); and (3) inter-observer
variability. We trained XGBoost classifiers on 8 consistent robust features
versus sequence-specific features, testing model performance under in-domain
and out-of-domain conditions. Results demonstrate that models trained on
protocol-invariant features maintain F1-scores >0.85 across distribution
shifts, while models using all features showed 40% performance degradation
under protocol changes. Dataset augmentation substantially improved the quality
of uncertainty estimates and reduced the expected calibration error (ECE) by
35% without sacrificing accuracy. Temperature scaling provided minimal
calibration benefits, confirming XGBoost's inherent reliability. Our findings
reveal that protocol-aware feature selection and controlled phantom studies
effectively predict model behavior under distribution shifts, providing a
framework for developing robust radiomics models resilient to real-world
protocol variations.

</details>


### [129] [Graph Distance Based on Cause-Effect Estimands with Latents](https://arxiv.org/abs/2510.25037)
*Zhufeng Li,Niki Kilbertus*

Main category: cs.LG

TL;DR: 论文提出了一种新的图距离度量方法，用于评估因果发现算法在存在潜在混淆因素下的性能。


<details>
  <summary>Details</summary>
Motivation: 因果发现领域不断涌现新方法，但评估发现的图的质量仍然是一个难题，尤其是在存在潜在混淆的情况下。社区对进展的评估提出了质疑。

Method: 提出了一个基于下游任务的图距离度量方法，该方法使用fixing识别和符号验证器来量化图差异对不同处理-结果对的因果效应估计的影响。

Result: 分析了该度量在不同图扰动下的行为，并将其与现有的距离度量进行了比较。

Conclusion: 该论文提出了一种新的图距离度量方法，可以更有效地评估因果发现算法在存在潜在混淆因素下的性能。

Abstract: Causal discovery aims to recover graphs that represent causal relations among
given variables from observations, and new methods are constantly being
proposed. Increasingly, the community raises questions about how much progress
is made, because properly evaluating discovered graphs remains notoriously
difficult, particularly under latent confounding. We propose a graph distance
measure for acyclic directed mixed graphs (ADMGs) based on the downstream task
of cause-effect estimation under unobserved confounding. Our approach uses
identification via fixing and a symbolic verifier to quantify how graph
differences distort cause-effect estimands for different treatment-outcome
pairs. We analyze the behavior of the measure under different graph
perturbations and compare it against existing distance metrics.

</details>


### [130] [Dynamically Weighted Momentum with Adaptive Step Sizes for Efficient Deep Network Training](https://arxiv.org/abs/2510.25042)
*Zhifeng Wang,Longlong Li,Chunyan Zeng*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的优化算法DWMGrad，旨在解决SGD和Adam等算法在处理学习效率波动、复杂模型和非凸优化问题方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有优化算法（如SGD和Adam）在处理复杂数据结构和模型时存在局限性，例如难以选择合适的学习率、避免局部最优以及在高维空间中导航。

Method: 该论文提出的DWMGrad算法，通过引入依赖于历史数据的动态引导机制来动态更新动量和学习率。

Result: 实验结果表明，DWMGrad算法在多种情况下能够实现更快的收敛速度和更高的准确率。

Conclusion: DWMGrad算法能够更好地适应变化的环境和任务复杂性。

Abstract: Within the current sphere of deep learning research, despite the extensive
application of optimization algorithms such as Stochastic Gradient Descent
(SGD) and Adaptive Moment Estimation (Adam), there remains a pronounced
inadequacy in their capability to address fluctuations in learning efficiency,
meet the demands of complex models, and tackle non-convex optimization issues.
These challenges primarily arise from the algorithms' limitations in handling
complex data structures and models, for instance, difficulties in selecting an
appropriate learning rate, avoiding local optima, and navigating through
high-dimensional spaces. To address these issues, this paper introduces a novel
optimization algorithm named DWMGrad. This algorithm, building on the
foundations of traditional methods, incorporates a dynamic guidance mechanism
reliant on historical data to dynamically update momentum and learning rates.
This allows the optimizer to flexibly adjust its reliance on historical
information, adapting to various training scenarios. This strategy not only
enables the optimizer to better adapt to changing environments and task
complexities but also, as validated through extensive experimentation,
demonstrates DWMGrad's ability to achieve faster convergence rates and higher
accuracies under a multitude of scenarios.

</details>


### [131] [Training Across Reservoirs: Using Numerical Differentiation To Couple Trainable Networks With Black-Box Reservoirs](https://arxiv.org/abs/2510.25074)
*Andrew Clark,Jack Moursounidis,Osmaan Rasouli,William Gan,Cooper Doyle,Anna Leontjeva*

Main category: cs.LG

TL;DR: 提出了一种新的扰动方法，用于估计具有不可访问计算图的网络结构中的偏导数。


<details>
  <summary>Details</summary>
Motivation: 现有的扰动方法精度和可扩展性有限，无法探索集成黑盒函数的可训练架构。

Method: 使用有界数值微分 (BOND)。

Result: 证明了 BOND 提高了现有扰动方法的准确性和可扩展性。发现黑盒函数（在实验中表现为固定的、未训练的网络）可以在不增加可训练参数数量的情况下提高模型性能。

Conclusion: 利用固定的、不可训练的模块来扩展模型容量具有潜力，这表明可以将模拟和数字设备结合起来作为扩展网络的一种机制。

Abstract: We introduce Bounded Numerical Differentiation (BOND), a perturbative method
for estimating partial derivatives across network structures with inaccessible
computational graphs. BOND demonstrates improved accuracy and scalability from
existing perturbative methods, enabling new explorations of trainable
architectures that integrate black-box functions. We observe that these
black-box functions, realized in our experiments as fixed, untrained networks,
can enhance model performance without increasing the number of trainable
parameters. This improvement is achieved without extensive optimization of the
architecture or properties of the black-box function itself. Our findings
highlight the potential of leveraging fixed, non-trainable modules to expand
model capacity, suggesting a path toward combining analogue and digital devices
as a mechanism for scaling networks.

</details>


### [132] [Learning Fair Graph Representations with Multi-view Information Bottleneck](https://arxiv.org/abs/2510.25096)
*Chuxun Liu,Debo Cheng,Qingfeng Chen,Jiangzhang Gan,Jiuyong Li,Lin Liu*

Main category: cs.LG

TL;DR: FairMIB通过分解图的特征、结构和扩散视图来减轻 GNN 中的偏差，从而在效用和公平性方面实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: GNN 擅长处理关系数据，但它们会放大训练数据偏差，将歧视性属性和结构性不平衡传播到不公平的结果中。许多公平性方法将偏差视为单一来源，忽略了不同的属性和结构效应，从而导致次优的公平性和效用权衡。

Method: FairMIB 是一种多视图信息瓶颈框架，旨在将图分解为特征、结构和扩散视图，以减轻 GNN 中的复杂性偏差。FairMIB 采用对比学习来最大化交叉视图互信息以进行无偏差表示学习，并集成多视角条件信息瓶颈目标，通过最小化与敏感属性的互信息来平衡任务效用和公平性。此外，FairMIB 在扩散视图中引入了逆概率加权 (IPW) 邻接校正，从而减少了消息传递期间偏差传播的扩散。

Result: 在五个真实世界基准数据集上的实验表明，FairMIB 在效用和公平性指标方面均实现了最先进的性能。

Conclusion: FairMIB 通过分解图的特征、结构和扩散视图来减轻 GNN 中的偏差，从而在效用和公平性方面实现最先进的性能。

Abstract: Graph neural networks (GNNs) excel on relational data by passing messages
over node features and structure, but they can amplify training data biases,
propagating discriminatory attributes and structural imbalances into unfair
outcomes. Many fairness methods treat bias as a single source, ignoring
distinct attribute and structure effects and leading to suboptimal fairness and
utility trade-offs. To overcome this challenge, we propose FairMIB, a
multi-view information bottleneck framework designed to decompose graphs into
feature, structural, and diffusion views for mitigating complexity biases in
GNNs. Especially, the proposed FairMIB employs contrastive learning to maximize
cross-view mutual information for bias-free representation learning. It further
integrates multi-perspective conditional information bottleneck objectives to
balance task utility and fairness by minimizing mutual information with
sensitive attributes. Additionally, FairMIB introduces an inverse
probability-weighted (IPW) adjacency correction in the diffusion view, which
reduces the spread of bias propagation during message passing. Experiments on
five real-world benchmark datasets demonstrate that FairMIB achieves
state-of-the-art performance across both utility and fairness metrics.

</details>


### [133] [Shift is Good: Mismatched Data Mixing Improves Test Performance](https://arxiv.org/abs/2510.25108)
*Marko Medvedev,Kaifeng Lyu,Zhiyuan Li,Nathan Srebro*

Main category: cs.LG

TL;DR: 论文研究了混合分布中训练和测试比例不同的情况，发现分布偏移有时可能是有益的，即使各成分之间不相关。


<details>
  <summary>Details</summary>
Motivation: 研究训练和测试数据比例不匹配对模型性能的影响。

Method: 通过分析各种场景，确定最优训练比例。

Result: 证明了分布偏移可能带来的益处，并分析了成分技能分布不同的组合情况。

Conclusion: 分布偏移可能提高模型性能，即使各成分之间不相关。

Abstract: We consider training and testing on mixture distributions with different
training and test proportions. We show that in many settings, and in some sense
generically, distribution shift can be beneficial, and test performance can
improve due to mismatched training proportions, even if the components are
unrelated and with no transfer between components. In a variety of scenarios,
we identify the optimal training proportions and the extent to which such
distribution shift can be beneficial. We show how the same analysis applies
also to a compositional setting with differing distribution of component
"skills'' at training and test.

</details>


### [134] [The Neural Differential Manifold: An Architecture with Explicit Geometric Structure](https://arxiv.org/abs/2510.25113)
*Di Zhang*

Main category: cs.LG

TL;DR: 本文介绍了一种新的神经网络架构，即神经微分流形 (NDM)，它将几何结构显式地融入到其基本设计中。


<details>
  <summary>Details</summary>
Motivation: 跳出传统的欧几里德参数空间，NDM 将神经网络重新概念化为一个可微流形，其中每一层都充当一个局部坐标图，网络参数直接参数化每个点的黎曼度量张量。

Method: 该架构分为三个协同层：坐标层，通过受归一化流启发的反变换实现平滑图转换；几何层，通过辅助子网络动态生成流形的度量；以及进化层，通过双目标损失函数优化任务性能和几何简单性。

Result: 这种几何正则化惩罚了过度的曲率和体积失真，提供了增强泛化和鲁棒性的内在正则化。该框架支持与学习的流形几何结构对齐的自然梯度下降优化，并通过赋予内部表示清晰的几何意义来提供前所未有的可解释性。

Conclusion: 虽然仍然存在重大的计算挑战，但神经微分流形代表着向几何结构化、可解释和高效的深度学习系统的根本转变。

Abstract: This paper introduces the Neural Differential Manifold (NDM), a novel neural
network architecture that explicitly incorporates geometric structure into its
fundamental design. Departing from conventional Euclidean parameter spaces, the
NDM re-conceptualizes a neural network as a differentiable manifold where each
layer functions as a local coordinate chart, and the network parameters
directly parameterize a Riemannian metric tensor at every point. The
architecture is organized into three synergistic layers: a Coordinate Layer
implementing smooth chart transitions via invertible transformations inspired
by normalizing flows, a Geometric Layer that dynamically generates the
manifold's metric through auxiliary sub-networks, and an Evolution Layer that
optimizes both task performance and geometric simplicity through a
dual-objective loss function. This geometric regularization penalizes excessive
curvature and volume distortion, providing intrinsic regularization that
enhances generalization and robustness. The framework enables natural gradient
descent optimization aligned with the learned manifold geometry and offers
unprecedented interpretability by endowing internal representations with clear
geometric meaning. We analyze the theoretical advantages of this approach,
including its potential for more efficient optimization, enhanced continual
learning, and applications in scientific discovery and controllable generative
modeling. While significant computational challenges remain, the Neural
Differential Manifold represents a fundamental shift towards geometrically
structured, interpretable, and efficient deep learning systems.

</details>


### [135] [A Unified Bilevel Model for Adversarial Learning and A Case Study](https://arxiv.org/abs/2510.25121)
*Yutong Zheng,Qingna Li*

Main category: cs.LG

TL;DR: 提出了一种统一的双层模型用于对抗学习，并从数据扰动的角度解释了聚类模型中的对抗攻击。


<details>
  <summary>Details</summary>
Motivation: 由于大多数机器学习模型的结构复杂，对抗攻击的机制没有得到很好的解释。如何衡量攻击效果还不是很清楚。

Method: 提出了一个统一的双层模型，用于对抗学习，并研究了聚类模型中的对抗攻击。

Result: 当数据扰动相对较小时，聚类模型是鲁棒的，而当数据扰动相对较大时，聚类结果会发生变化，从而导致攻击。

Conclusion: 分析了$\\delta$-measure的well-definedness，可用于所提出的聚类模型对抗学习的双层模型。

Abstract: Adversarial learning has been attracting more and more attention thanks to
the fast development of machine learning and artificial intelligence. However,
due to the complicated structure of most machine learning models, the mechanism
of adversarial attacks is not well interpreted. How to measure the effect of
attack is still not quite clear. In this paper, we propose a unified bilevel
model for adversarial learning. We further investigate the adversarial attack
in clustering models and interpret it from data perturbation point of view. We
reveal that when the data perturbation is relatively small, the clustering
model is robust, whereas if it is relatively large, the clustering result
changes, which leads to an attack. To measure the effect of attacks for
clustering models, we analyse the well-definedness of the so-called
$\delta$-measure, which can be used in the proposed bilevel model for
adversarial learning of clustering models.

</details>


### [136] [Learning Low Rank Neural Representations of Hyperbolic Wave Dynamics from Data](https://arxiv.org/abs/2510.25123)
*Woojin Cho,Kookjin Lee,Noseong Park,Donsub Rim,Gerrit Welper*

Main category: cs.LG

TL;DR: 提出了一种基于数据的降维方法，适用于表示双曲波传播的基于物理的数据。


<details>
  <summary>Details</summary>
Motivation: 该架构受到理论结果的推动，这些理论结果严格证明了这种波类的有效表示的存在。

Method: 该方法利用了一种称为低秩神经表示 (LRNR) 的专用神经网络架构在超网络框架内。

Result: 我们观察到低秩张量表示在训练的 LRNR 中自然出现，这揭示了波传播的一种新的分解，其中每个分解模式对应于可解释的物理特征。

Conclusion: LRNR 架构通过压缩方案实现了高效推理，这在在要求苛刻的性能状态中部署 LRNR 时，这是一个潜在的重要特征。

Abstract: We present a data-driven dimensionality reduction method that is well-suited
for physics-based data representing hyperbolic wave propagation. The method
utilizes a specialized neural network architecture called low rank neural
representation (LRNR) inside a hypernetwork framework. The architecture is
motivated by theoretical results that rigorously prove the existence of
efficient representations for this wave class. We illustrate through archetypal
examples that such an efficient low-dimensional representation of propagating
waves can be learned directly from data through a combination of deep learning
techniques. We observe that a low rank tensor representation arises naturally
in the trained LRNRs, and that this reveals a new decomposition of wave
propagation where each decomposed mode corresponds to interpretable physical
features. Furthermore, we demonstrate that the LRNR architecture enables
efficient inference via a compression scheme, which is a potentially important
feature when deploying LRNRs in demanding performance regimes.

</details>


### [137] [Bridging the Divide: End-to-End Sequence-Graph Learning](https://arxiv.org/abs/2510.25126)
*Yuen Chen,Yulun Wu,Samuel Sharpe,Igor Melnyk,Nam H. Nguyen,Furong Huang,C. Bayan Bruss,Rizal Fathony*

Main category: cs.LG

TL;DR: 提出了一种名为 BRIDGE 的统一端到端架构，它将序列编码器与 GNN 耦合在一起，允许梯度在两个模块之间流动并学习与任务对齐的表示。


<details>
  <summary>Details</summary>
Motivation: 现有的序列建模和图建模方法通常忽略一种模态或另一种模态。序列和图不是单独的问题，而是同一数据集的互补方面，应该联合学习。

Method: 引入了 BRIDGE，一种统一的端到端架构，它将序列编码器与 GNN 耦合在单个目标下，允许梯度在两个模块之间流动并学习与任务对齐的表示。为了实现邻居之间细粒度的令牌级消息传递，我们添加了 TOKENXATTN，一个令牌级交叉注意力层，用于在相邻序列中的事件之间传递消息。

Result: 在友谊预测 (Brightkite) 和欺诈检测 (Amazon) 两种设置中，BRIDGE 在排名和分类指标上始终优于静态 GNN、时间图方法和仅序列基线。

Conclusion: BRIDGE 是一种很有前途的用于处理序列和关系数据的模型，并且在各种任务中都优于现有方法。

Abstract: Many real-world datasets are both sequential and relational: each node
carries an event sequence while edges encode interactions. Existing methods in
sequence modeling and graph modeling often neglect one modality or the other.
We argue that sequences and graphs are not separate problems but complementary
facets of the same dataset, and should be learned jointly. We introduce BRIDGE,
a unified end-to-end architecture that couples a sequence encoder with a GNN
under a single objective, allowing gradients to flow across both modules and
learning task-aligned representations. To enable fine-grained token-level
message passing among neighbors, we add TOKENXATTN, a token-level
cross-attention layer that passes messages between events in neighboring
sequences. Across two settings, friendship prediction (Brightkite) and fraud
detection (Amazon), BRIDGE consistently outperforms static GNNs, temporal graph
methods, and sequence-only baselines on ranking and classification metrics.

</details>


### [138] [An Analysis of Causal Effect Estimation using Outcome Invariant Data Augmentation](https://arxiv.org/abs/2510.25128)
*Uzair Akbar,Niki Kilbertus,Hao Shen,Krikamol Muandet,Bo Dai*

Main category: cs.LG

TL;DR: 提出了一种新的数据增强（DA）框架，用于在因果推断中进行泛化，超越了传统的独立同分布（i.i.d.）设置。


<details>
  <summary>Details</summary>
Motivation: DA在因果效应估计中可以减少由隐藏混淆因素引起的偏差，并且工具变量（IVs）可能不如DA容易获得。

Method: 将参数化的DA转化为IVL回归问题，并结合使用以模拟最坏情况的应用。

Result: 通过理论和实验证明，该方法在因果估计和泛化任务上优于简单DA。

Conclusion: 该研究为DA在因果推断中的应用提供了新的视角和方法，并验证了其有效性。

Abstract: The technique of data augmentation (DA) is often used in machine learning for
regularization purposes to better generalize under i.i.d. settings. In this
work, we present a unifying framework with topics in causal inference to make a
case for the use of DA beyond just the i.i.d. setting, but for generalization
across interventions as well. Specifically, we argue that when the outcome
generating mechanism is invariant to our choice of DA, then such augmentations
can effectively be thought of as interventions on the treatment generating
mechanism itself. This can potentially help to reduce bias in causal effect
estimation arising from hidden confounders. In the presence of such unobserved
confounding we typically make use of instrumental variables (IVs) -- sources of
treatment randomization that are conditionally independent of the outcome.
However, IVs may not be as readily available as DA for many applications, which
is the main motivation behind this work. By appropriately regularizing IV based
estimators, we introduce the concept of IV-like (IVL) regression for mitigating
confounding bias and improving predictive performance across interventions even
when certain IV properties are relaxed. Finally, we cast parameterized DA as an
IVL regression problem and show that when used in composition can simulate a
worst-case application of such DA, further improving performance on causal
estimation and generalization tasks beyond what simple DA may offer. This is
shown both theoretically for the population case and via simulation experiments
for the finite sample case using a simple linear example. We also present real
data experiments to support our case.

</details>
