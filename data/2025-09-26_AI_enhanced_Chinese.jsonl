{"id": "2509.20617", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20617", "abs": "https://arxiv.org/abs/2509.20617", "authors": ["Eric Fithian", "Kirill Skobelev"], "title": "DELM: a Python toolkit for Data Extraction with Language Models", "comment": null, "summary": "Large Language Models (LLMs) have become powerful tools for annotating\nunstructured data. However, most existing workflows rely on ad hoc scripts,\nmaking reproducibility, robustness, and systematic evaluation difficult. To\naddress these challenges, we introduce DELM (Data Extraction with Language\nModels), an open-source Python toolkit designed for rapid experimental\niteration of LLM-based data extraction pipelines and for quantifying the\ntrade-offs between them. DELM minimizes boilerplate code and offers a modular\nframework with structured outputs, built-in validation, flexible data-loading\nand scoring strategies, and efficient batch processing. It also includes robust\nsupport for working with LLM APIs, featuring retry logic, result caching,\ndetailed cost tracking, and comprehensive configuration management. We showcase\nDELM's capabilities through two case studies: one featuring a novel prompt\noptimization algorithm, and another illustrating how DELM quantifies trade-offs\nbetween cost and coverage when selecting keywords to decide which paragraphs to\npass to an LLM. DELM is available at\n\\href{https://github.com/Center-for-Applied-AI/delm}{\\texttt{github.com/Center-for-Applied-AI/delm}}.", "AI": {"tldr": "DELM\u662f\u4e00\u4e2a\u7528\u4e8eLLM\u6570\u636e\u63d0\u53d6\u6d41\u7a0b\u7684Python\u5de5\u5177\u5305\uff0c\u65e8\u5728\u63d0\u9ad8\u53ef\u91cd\u590d\u6027\u3001\u9c81\u68d2\u6027\u548c\u7cfb\u7edf\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u6d41\u7a0b\u4f9d\u8d56\u4e8e\u4e34\u65f6\u811a\u672c\uff0c\u5bfc\u81f4\u53ef\u91cd\u590d\u6027\u3001\u9c81\u68d2\u6027\u548c\u7cfb\u7edf\u8bc4\u4f30\u5b58\u5728\u56f0\u96be\u3002", "method": "DELM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5177\u6709\u7ed3\u6784\u5316\u8f93\u51fa\u3001\u5185\u7f6e\u9a8c\u8bc1\u3001\u7075\u6d3b\u7684\u6570\u636e\u52a0\u8f7d\u548c\u8bc4\u5206\u7b56\u7565\u4ee5\u53ca\u9ad8\u6548\u7684\u6279\u5904\u7406\u3002", "result": "\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86DELM\u7684\u529f\u80fd\uff1a\u4e00\u4e2a\u5c55\u793a\u4e86\u65b0\u7684prompt\u4f18\u5316\u7b97\u6cd5\uff0c\u53e6\u4e00\u4e2a\u5c55\u793a\u4e86DELM\u5982\u4f55\u91cf\u5316\u6210\u672c\u548c\u8986\u76d6\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "DELM\u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u53ef\u7528\u4e8e\u5feb\u901f\u5b9e\u9a8c\u8fed\u4ee3LLM\u6570\u636e\u63d0\u53d6pipeline\u5e76\u91cf\u5316\u5b83\u4eec\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2509.20769", "categories": ["cs.IR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20769", "abs": "https://arxiv.org/abs/2509.20769", "authors": ["Tuo Zhang", "Yuechun Sun", "Ruiliang Liu"], "title": "Provenance Analysis of Archaeological Artifacts via Multimodal RAG Systems", "comment": null, "summary": "In this work, we present a retrieval-augmented generation (RAG)-based system\nfor provenance analysis of archaeological artifacts, designed to support expert\nreasoning by integrating multimodal retrieval and large vision-language models\n(VLMs). The system constructs a dual-modal knowledge base from reference texts\nand images, enabling raw visual, edge-enhanced, and semantic retrieval to\nidentify stylistically similar objects. Retrieved candidates are synthesized by\nthe VLM to generate structured inferences, including chronological,\ngeographical, and cultural attributions, alongside interpretive justifications.\nWe evaluate the system on a set of Eastern Eurasian Bronze Age artifacts from\nthe British Museum. Expert evaluation demonstrates that the system produces\nmeaningful and interpretable outputs, offering scholars concrete starting\npoints for analysis and significantly alleviating the cognitive burden of\nnavigating vast comparative corpora.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eRAG\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u8003\u53e4\u6587\u7269\u6eaf\u6e90\u5206\u6790\uff0c\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u68c0\u7d22\u548c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6765\u652f\u6301\u4e13\u5bb6\u63a8\u7406\u3002", "motivation": "\u8be5\u7cfb\u7edf\u65e8\u5728\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u68c0\u7d22\u548c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u51cf\u8f7b\u4e13\u5bb6\u5728\u5904\u7406\u5927\u91cf\u6bd4\u8f83\u8bed\u6599\u5e93\u65f6\u7684\u8ba4\u77e5\u8d1f\u62c5\uff0c\u5e76\u4e3a\u5206\u6790\u63d0\u4f9b\u5177\u4f53\u7684\u8d77\u70b9\u3002", "method": "\u8be5\u7cfb\u7edf\u6784\u5efa\u4e86\u4e00\u4e2a\u6765\u81ea\u53c2\u8003\u6587\u672c\u548c\u56fe\u50cf\u7684\u53cc\u6a21\u6001\u77e5\u8bc6\u5e93\uff0c\u652f\u6301\u539f\u59cb\u89c6\u89c9\u3001\u8fb9\u7f18\u589e\u5f3a\u548c\u8bed\u4e49\u68c0\u7d22\uff0c\u4ee5\u8bc6\u522b\u98ce\u683c\u76f8\u4f3c\u7684\u7269\u4f53\u3002\u68c0\u7d22\u5230\u7684\u5019\u9009\u5bf9\u8c61\u7531VLM\u5408\u6210\uff0c\u4ee5\u751f\u6210\u7ed3\u6784\u5316\u7684\u63a8\u8bba\uff0c\u5305\u62ec\u5e74\u4ee3\u3001\u5730\u7406\u548c\u6587\u5316\u5f52\u5c5e\uff0c\u4ee5\u53ca\u89e3\u91ca\u6027\u7406\u7531\u3002", "result": "\u5728\u5bf9\u5927\u82f1\u535a\u7269\u9986\u7684\u6b27\u4e9a\u4e1c\u90e8\u9752\u94dc\u65f6\u4ee3\u6587\u7269\u8fdb\u884c\u8bc4\u4f30\u540e\uff0c\u4e13\u5bb6\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u4ea7\u751f\u4e86\u6709\u610f\u4e49\u4e14\u53ef\u89e3\u91ca\u7684\u8f93\u51fa\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u5b66\u8005\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u5206\u6790\u8d77\u70b9\uff0c\u5e76\u663e\u8457\u51cf\u8f7b\u4e86\u5bfc\u822a\u5927\u578b\u6bd4\u8f83\u8bed\u6599\u5e93\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002"}}
{"id": "2509.20804", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20804", "abs": "https://arxiv.org/abs/2509.20804", "authors": ["Meng Yuan", "Justin Zobel"], "title": "Performance Consistency of Learning Methods for Information Retrieval Tasks", "comment": null, "summary": "A range of approaches have been proposed for estimating the accuracy or\nrobustness of the measured performance of IR methods. One is to use\nbootstrapping of test sets, which, as we confirm, provides an estimate of\nvariation in performance. For IR methods that rely on a seed, such as those\nthat involve machine learning, another approach is to use a random set of seeds\nto examine performance variation. Using three different IR tasks we have used\nsuch randomness to examine a range of traditional statistical learning models\nand transformer-based learning models. While the statistical models are stable,\nthe transformer models show huge variation as seeds are changed. In 9 of 11\ncases the F1-scores (in the range 0.0--1.0) had a standard deviation of over\n0.075; while 7 of 11 precision values (also in the range 0.0--1.0) had a\nstandard deviation of over 0.125. This is in a context where differences of\nless than 0.02 have been used as evidence of method improvement. Our findings\nhighlight the vulnerability of transformer models to training instabilities and\nmoreover raise questions about the reliability of previous results, thus\nunderscoring the need for rigorous evaluation practices.", "AI": {"tldr": "Transformer\u6a21\u578b\u5728\u4e0d\u540c\u79cd\u5b50\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\u5f88\u5927\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9\u5176\u53ef\u9760\u6027\u7684\u8d28\u7591\u3002", "motivation": "\u8bc4\u4f30\u4fe1\u606f\u68c0\u7d22\uff08IR\uff09\u65b9\u6cd5\u6027\u80fd\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u968f\u673a\u79cd\u5b50\u8bc4\u4f30\u4f20\u7edf\u7edf\u8ba1\u5b66\u4e60\u6a21\u578b\u548c\u57fa\u4e8eTransformer\u7684\u5b66\u4e60\u6a21\u578b\u5728\u4e09\u4e2a\u4e0d\u540c\u7684IR\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u53d8\u5316\u3002", "result": "\u7edf\u8ba1\u6a21\u578b\u8868\u73b0\u7a33\u5b9a\uff0c\u4f46Transformer\u6a21\u578b\u5728\u6539\u53d8\u79cd\u5b50\u65f6\u8868\u73b0\u51fa\u5de8\u5927\u5dee\u5f02\u3002\u572811\u4e2a\u6848\u4f8b\u4e2d\uff0c\u67099\u4e2a\u6848\u4f8b\u7684F1\u5206\u6570\u6807\u51c6\u5dee\u8d85\u8fc70.075\uff0c7\u4e2a\u6848\u4f8b\u7684\u7cbe\u786e\u5ea6\u6807\u51c6\u5dee\u8d85\u8fc70.125\u3002", "conclusion": "Transformer\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\uff0c\u4e4b\u524d\u7684\u7814\u7a76\u7ed3\u679c\u53ef\u80fd\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2509.20781", "categories": ["cs.LG", "cs.DB", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.20781", "abs": "https://arxiv.org/abs/2509.20781", "authors": ["Alireza Heidari", "Amirhossein Ahmad", "Wei Zhang", "Ying Xiong"], "title": "Sig2Model: A Boosting-Driven Model for Updatable Learned Indexes", "comment": "22 pages, 11 figures", "summary": "Learned Indexes (LIs) represent a paradigm shift from traditional index\nstructures by employing machine learning models to approximate the cumulative\ndistribution function (CDF) of sorted data. While LIs achieve remarkable\nefficiency for static datasets, their performance degrades under dynamic\nupdates: maintaining the CDF invariant (sum of F(k) equals 1) requires global\nmodel retraining, which blocks queries and limits the queries-per-second (QPS)\nmetric. Current approaches fail to address these retraining costs effectively,\nrendering them unsuitable for real-world workloads with frequent updates. In\nthis paper, we present Sig2Model, an efficient and adaptive learned index that\nminimizes retraining cost through three key techniques: (1) a sigmoid boosting\napproximation technique that dynamically adjusts the index model by\napproximating update-induced shifts in data distribution with localized sigmoid\nfunctions while preserving bounded error guarantees and deferring full\nretraining; (2) proactive update training via Gaussian mixture models (GMMs)\nthat identifies high-update-probability regions for strategic placeholder\nallocation to speed up updates; and (3) a neural joint optimization framework\nthat continuously refines both the sigmoid ensemble and GMM parameters via\ngradient-based learning. We evaluate Sig2Model against state-of-the-art\nupdatable learned indexes on real-world and synthetic workloads, and show that\nSig2Model reduces retraining cost by up to 20x, achieves up to 3x higher QPS,\nand uses up to 1000x less memory.", "AI": {"tldr": "Sig2Model: An efficient and adaptive learned index that minimizes retraining cost through sigmoid boosting, proactive update training, and neural joint optimization.", "motivation": "Learned Indexes (LIs) achieve remarkable efficiency for static datasets, their performance degrades under dynamic updates: maintaining the CDF invariant requires global model retraining, which blocks queries and limits the queries-per-second (QPS) metric. Current approaches fail to address these retraining costs effectively, rendering them unsuitable for real-world workloads with frequent updates.", "method": "a sigmoid boosting approximation technique that dynamically adjusts the index model by approximating update-induced shifts in data distribution with localized sigmoid functions while preserving bounded error guarantees and deferring full retraining; proactive update training via Gaussian mixture models (GMMs) that identifies high-update-probability regions for strategic placeholder allocation to speed up updates; and a neural joint optimization framework that continuously refines both the sigmoid ensemble and GMM parameters via gradient-based learning.", "result": "Sig2Model reduces retraining cost by up to 20x, achieves up to 3x higher QPS, and uses up to 1000x less memory.", "conclusion": "Sig2Model is an efficient and adaptive learned index that minimizes retraining cost and improves performance on dynamic datasets."}}
{"id": "2509.20883", "categories": ["cs.IR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20883", "abs": "https://arxiv.org/abs/2509.20883", "authors": ["Hua Zong", "Qingtao Zeng", "Zhengxiong Zhou", "Zhihua Han", "Zhensong Yan", "Mingjie Liu", "Hechen Sun", "Jiawei Liu", "Yiwen Hu", "Qi Wang", "YiHan Xian", "Wenjie Guo", "Houyuan Xiang", "Zhiyuan Zeng", "Xiangrong Sheng", "Bencheng Yan", "Nan Hu", "Yuheng Huang", "Jinqing Lian", "Ziru Xu", "Yan Zhang", "Ju Huang", "Siran Yang", "Huimin Yi", "Jiamang Wang", "Pengjie Wang", "Han Zhu", "Jian Wu", "Dan Ou", "Jian Xu", "Haihong Tang", "Yuning Jiang", "Bo Zheng", "Lin Qu"], "title": "RecIS: Sparse to Dense, A Unified Training Framework for Recommendation Models", "comment": null, "summary": "In this paper, we propose RecIS, a unified Sparse-Dense training framework\ndesigned to achieve two primary goals: 1. Unified Framework To create a Unified\nsparse-dense training framework based on the PyTorch ecosystem that meets the\ntraining needs of industrial-grade recommendation models that integrated with\nlarge models. 2.System Optimization To optimize the sparse component, offering\nsuperior efficiency over the TensorFlow-based recommendation models. The dense\ncomponent, meanwhile, leverages existing optimization technologies within the\nPyTorch ecosystem. Currently, RecIS is being used in Alibaba for numerous\nlarge-model enhanced recommendation training tasks, and some traditional sparse\nmodels have also begun training in it.", "AI": {"tldr": "RecIS is a unified sparse-dense training framework for industrial-grade recommendation models.", "motivation": "To create a unified sparse-dense training framework and optimize the sparse component for better efficiency.", "method": "A unified sparse-dense training framework based on PyTorch.", "result": "RecIS is used in Alibaba for large-model enhanced recommendation training tasks and some traditional sparse models.", "conclusion": "RecIS meets the training needs of industrial-grade recommendation models and offers superior efficiency."}}
{"id": "2509.20367", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.20367", "abs": "https://arxiv.org/abs/2509.20367", "authors": ["Leyi Ouyang"], "title": "Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models", "comment": "2 Figures, 7 Tables, 1 Algorithm", "summary": "Diplomatic events consistently prompt widespread public discussion and\ndebate. Public sentiment plays a critical role in diplomacy, as a good\nsentiment provides vital support for policy implementation, helps resolve\ninternational issues, and shapes a nation's international image. Traditional\nmethods for gauging public sentiment, such as large-scale surveys or manual\ncontent analysis of media, are typically time-consuming, labor-intensive, and\nlack the capacity for forward-looking analysis. We propose a novel framework\nthat identifies specific modifications for diplomatic event narratives to shift\npublic sentiment from negative to neutral or positive. First, we train a\nlanguage model to predict public reaction towards diplomatic events. To this\nend, we construct a dataset comprising descriptions of diplomatic events and\ntheir associated public discussions. Second, guided by communication theories\nand in collaboration with domain experts, we predetermined several textual\nfeatures for modification, ensuring that any alterations changed the event's\nnarrative framing while preserving its core facts.We develop a counterfactual\ngeneration algorithm that employs a large language model to systematically\nproduce modified versions of an original text. The results show that this\nframework successfully shifted public sentiment to a more favorable state with\na 70\\% success rate. This framework can therefore serve as a practical tool for\ndiplomats, policymakers, and communication specialists, offering data-driven\ninsights on how to frame diplomatic initiatives or report on events to foster a\nmore desirable public sentiment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4fee\u6539\u5916\u4ea4\u4e8b\u4ef6\u7684\u53d9\u8ff0\u65b9\u5f0f\u6765\u8f6c\u53d8\u516c\u4f17\u60c5\u7eea\uff0c\u4f7f\u5176\u4ece\u8d1f\u9762\u53d8\u4e3a\u4e2d\u6027\u6216\u6b63\u9762\u3002", "motivation": "\u516c\u4f17\u60c5\u7eea\u5728\u5916\u4ea4\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u826f\u597d\u7684\u516c\u4f17\u60c5\u7eea\u4e3a\u653f\u7b56\u5b9e\u65bd\u63d0\u4f9b\u91cd\u8981\u652f\u6301\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3\u56fd\u9645\u95ee\u9898\uff0c\u5851\u9020\u56fd\u5bb6\u5f62\u8c61\u3002\u4f20\u7edf\u7684\u60c5\u7eea\u8861\u91cf\u65b9\u6cd5\u8017\u65f6\u3001\u8d39\u529b\uff0c\u4e14\u7f3a\u4e4f\u524d\u77bb\u6027\u5206\u6790\u80fd\u529b\u3002", "method": "1. \u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u6765\u9884\u6d4b\u516c\u4f17\u5bf9\u5916\u4ea4\u4e8b\u4ef6\u7684\u53cd\u5e94\u30022. \u6784\u5efa\u5305\u542b\u5916\u4ea4\u4e8b\u4ef6\u63cf\u8ff0\u53ca\u5176\u76f8\u5173\u516c\u4f17\u8ba8\u8bba\u7684\u6570\u636e\u96c6\u30023. \u786e\u5b9a\u51e0\u4e2a\u7528\u4e8e\u4fee\u6539\u7684\u6587\u672c\u7279\u5f81\uff0c\u786e\u4fdd\u4efb\u4f55\u6539\u53d8\u90fd\u80fd\u6539\u53d8\u4e8b\u4ef6\u7684\u53d9\u4e8b\u6846\u67b6\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u6838\u5fc3\u4e8b\u5b9e\u30024. \u5f00\u53d1\u4e00\u79cd\u53cd\u4e8b\u5b9e\u751f\u6210\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u7cfb\u7edf\u5730\u751f\u6210\u539f\u59cb\u6587\u672c\u7684\u4fee\u6539\u7248\u672c\u3002", "result": "\u8be5\u6846\u67b6\u6210\u529f\u5730\u5c06\u516c\u4f17\u60c5\u7eea\u8f6c\u53d8\u4e3a\u66f4\u6709\u5229\u7684\u72b6\u6001\uff0c\u6210\u529f\u7387\u8fbe70%\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u4ee5\u4f5c\u4e3a\u5916\u4ea4\u5b98\u3001\u653f\u7b56\u5236\u5b9a\u8005\u548c\u4f20\u64ad\u4e13\u5bb6\u7684\u5b9e\u7528\u5de5\u5177\uff0c\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u89c1\u89e3\uff0c\u8bf4\u660e\u5982\u4f55\u6784\u5efa\u5916\u4ea4\u4e3e\u63aa\u6216\u62a5\u544a\u4e8b\u4ef6\uff0c\u4ee5\u57f9\u517b\u66f4\u7406\u60f3\u7684\u516c\u4f17\u60c5\u7eea\u3002"}}
{"id": "2509.20379", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20379", "abs": "https://arxiv.org/abs/2509.20379", "authors": ["Ofir Azachi", "Kfir Eliyahu", "Eyal El Ani", "Rom Himelstein", "Roi Reichart", "Yuval Pinter", "Nitay Calderon"], "title": "Leveraging NTPs for Efficient Hallucination Detection in VLMs", "comment": null, "summary": "Hallucinations of vision-language models (VLMs), which are misalignments\nbetween visual content and generated text, undermine the reliability of VLMs.\nOne common approach for detecting them employs the same VLM, or a different\none, to assess generated outputs. This process is computationally intensive and\nincreases model latency. In this paper, we explore an efficient on-the-fly\nmethod for hallucination detection by training traditional ML models over\nsignals based on the VLM's next-token probabilities (NTPs). NTPs provide a\ndirect quantification of model uncertainty. We hypothesize that high\nuncertainty (i.e., a low NTP value) is strongly associated with hallucinations.\nTo test this, we introduce a dataset of 1,400 human-annotated statements\nderived from VLM-generated content, each labeled as hallucinated or not, and\nuse it to test our NTP-based lightweight method. Our results demonstrate that\nNTP-based features are valuable predictors of hallucinations, enabling fast and\nsimple ML models to achieve performance comparable to that of strong VLMs.\nFurthermore, augmenting these NTPs with linguistic NTPs, computed by feeding\nonly the generated text back into the VLM, enhances hallucination detection\nperformance. Finally, integrating hallucination prediction scores from VLMs\ninto the NTP-based models led to better performance than using either VLMs or\nNTPs alone. We hope this study paves the way for simple, lightweight solutions\nthat enhance the reliability of VLMs.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u4e0b\u4e00token\u6982\u7387 (NTP) \u7684\u8f7b\u91cf\u7ea7\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8 VLM \u7684\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u8ba1\u7b97\u5bc6\u96c6\u4e14\u589e\u52a0\u6a21\u578b\u5ef6\u8fdf\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u8bad\u7ec3\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5229\u7528VLM\u7684NTPs\u4f5c\u4e3a\u4fe1\u53f7\uff0c\u5e76\u7ed3\u5408\u8bed\u8a00NTPs\u548cVLM\u7684\u5e7b\u89c9\u9884\u6d4b\u5206\u6570\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eNTP\u7684\u7279\u5f81\u53ef\u4ee5\u6709\u6548\u5730\u9884\u6d4b\u5e7b\u89c9\uff0c\u5e76\u4e14\u8be5\u65b9\u6cd5\u7684\u6027\u80fd\u4e0e\u5f3a\u5927\u7684VLM\u76f8\u5f53\u3002\u7ed3\u5408\u8bed\u8a00NTPs\u548cVLM\u7684\u5e7b\u89c9\u9884\u6d4b\u5206\u6570\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u7b80\u5355\u3001\u8f7b\u91cf\u7ea7\u7684\u89e3\u51b3\u65b9\u6848\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u4ece\u800c\u589e\u5f3a\u4e86VLM\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.20364", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.20364", "abs": "https://arxiv.org/abs/2509.20364", "authors": ["Thomas J Sheffler"], "title": "An Approach to Checking Correctness for Agentic Systems", "comment": "15 pages, 5 figures", "summary": "This paper presents a temporal expression language for monitoring AI agent\nbehavior, enabling systematic error-detection of LLM-based agentic systems that\nexhibit variable outputs due to stochastic generation processes. Drawing from\ntemporal logic techniques used in hardware verification, this approach monitors\nexecution traces of agent tool calls and state transitions to detect deviations\nfrom expected behavioral patterns. Current error-detection approaches rely\nprimarily on text matching of inputs and outputs, which proves fragile due to\nthe natural language variability inherent in LLM responses. The proposed method\ninstead focuses on the sequence of agent actions -- such as tool invocations\nand inter-agent communications -- allowing verification of system behavior\nindependent of specific textual outputs. The temporal expression language\nprovides assertions that capture correct behavioral patterns across multiple\nexecution scenarios. These assertions serve dual purposes: validating prompt\nengineering and guardrail effectiveness during development, and providing\nregression testing when agents are updated with new LLMs or modified logic. The\napproach is demonstrated using a three-agent system, where agents coordinate to\nsolve multi-step reasoning tasks. When powered by large, capable models, all\ntemporal assertions were satisfied across many test runs. However, when smaller\nmodels were substituted in two of the three agents, executions violated\nbehavioral assertions, primarily due to improper tool sequencing and failed\ncoordination handoffs. The temporal expressions successfully flagged these\nanomalies, demonstrating the method's effectiveness for detecting behavioral\nregressions in production agentic systems. This approach provides a foundation\nfor systematic monitoring of AI agent reliability as these systems become\nincreasingly deployed in critical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u76d1\u63a7AI agent\u884c\u4e3a\u7684\u65f6\u5e8f\u8868\u8fbe\u5f0f\u8bed\u8a00\uff0c\u4ee5\u7cfb\u7edf\u5730\u68c0\u6d4b\u57fa\u4e8eLLM\u7684agent\u7cfb\u7edf\u7684\u9519\u8bef\u3002", "motivation": "\u5f53\u524d\u9519\u8bef\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u8f93\u5165\u548c\u8f93\u51fa\u7684\u6587\u672c\u5339\u914d\uff0c\u7531\u4e8eLLM\u54cd\u5e94\u4e2d\u56fa\u6709\u7684\u81ea\u7136\u8bed\u8a00\u53ef\u53d8\u6027\uff0c\u8fd9\u79cd\u65b9\u6cd5\u663e\u5f97\u8106\u5f31\u3002\u672c\u6587\u65b9\u6cd5\u4fa7\u91cd\u4e8eagent\u52a8\u4f5c\u5e8f\u5217\uff0c\u5141\u8bb8\u72ec\u7acb\u4e8e\u7279\u5b9a\u6587\u672c\u8f93\u51fa\u9a8c\u8bc1\u7cfb\u7edf\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u65f6\u5e8f\u903b\u8f91\u6280\u672f\uff0c\u76d1\u63a7agent\u5de5\u5177\u8c03\u7528\u548c\u72b6\u6001\u8f6c\u6362\u7684\u6267\u884c\u8f68\u8ff9\uff0c\u4ee5\u68c0\u6d4b\u4e0e\u9884\u671f\u884c\u4e3a\u6a21\u5f0f\u7684\u504f\u5dee\u3002", "result": "\u5f53\u4f7f\u7528\u5927\u578b\u6a21\u578b\u65f6\uff0c\u6240\u6709\u65f6\u5e8f\u65ad\u8a00\u5728\u8bb8\u591a\u6d4b\u8bd5\u8fd0\u884c\u4e2d\u90fd\u5f97\u5230\u6ee1\u8db3\u3002\u5f53\u8f83\u5c0f\u7684\u6a21\u578b\u88ab\u66ff\u6362\u65f6\uff0c\u6267\u884c\u8fdd\u53cd\u4e86\u884c\u4e3a\u65ad\u8a00\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u4e0d\u6b63\u786e\u7684\u5de5\u5177\u6392\u5e8f\u548c\u5931\u8d25\u7684\u534f\u8c03\u5207\u6362\u3002\u65f6\u5e8f\u8868\u8fbe\u5f0f\u6210\u529f\u5730\u6807\u8bb0\u4e86\u8fd9\u4e9b\u5f02\u5e38\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7cfb\u7edf\u5730\u76d1\u63a7AI agent\u53ef\u9760\u6027\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u5728\u5173\u952e\u5e94\u7528\u4e2d\u3002"}}
{"id": "2509.20408", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20408", "abs": "https://arxiv.org/abs/2509.20408", "authors": ["Leo Maxime Brunswic", "Haozhi Wang", "Shuang Luo", "Jianye Hao", "Amir Rasouli", "Yinchuan Li"], "title": "A Theory of Multi-Agent Generative Flow Networks", "comment": "Accepted at SPIGM Workshop NeurIPS 2025", "summary": "Generative flow networks utilize a flow-matching loss to learn a stochastic\npolicy for generating objects from a sequence of actions, such that the\nprobability of generating a pattern can be proportional to the corresponding\ngiven reward. However, a theoretical framework for multi-agent generative flow\nnetworks (MA-GFlowNets) has not yet been proposed. In this paper, we propose\nthe theory framework of MA-GFlowNets, which can be applied to multiple agents\nto generate objects collaboratively through a series of joint actions. We\nfurther propose four algorithms: a centralized flow network for centralized\ntraining of MA-GFlowNets, an independent flow network for decentralized\nexecution, a joint flow network for achieving centralized training with\ndecentralized execution, and its updated conditional version. Joint Flow\ntraining is based on a local-global principle allowing to train a collection of\n(local) GFN as a unique (global) GFN. This principle provides a loss of\nreasonable complexity and allows to leverage usual results on GFN to provide\ntheoretical guarantees that the independent policies generate samples with\nprobability proportional to the reward function. Experimental results\ndemonstrate the superiority of the proposed framework compared to reinforcement\nlearning and MCMC-based methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u667a\u80fd\u4f53\u751f\u6210\u6d41\u7f51\u7edc\uff08MA-GFlowNets\uff09\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u4e2a\u667a\u80fd\u4f53\u534f\u4f5c\u751f\u6210\u5bf9\u8c61\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5bf9\u591a\u667a\u80fd\u4f53\u751f\u6210\u6d41\u7f51\u7edc\u7684\u7406\u8bba\u6846\u67b6\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u56db\u79cd\u7b97\u6cd5\uff1a\u96c6\u4e2d\u5f0f\u6d41\u7f51\u7edc\u3001\u72ec\u7acb\u6d41\u7f51\u7edc\u3001\u8054\u5408\u6d41\u7f51\u7edc\u53ca\u5176\u6761\u4ef6\u7248\u672c\u3002\u8054\u5408\u6d41\u8bad\u7ec3\u57fa\u4e8e\u5c40\u90e8-\u5168\u5c40\u539f\u5219\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u4f18\u4e8e\u5f3a\u5316\u5b66\u4e60\u548c\u57fa\u4e8eMCMC\u7684\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u4e3a\u591a\u667a\u80fd\u4f53\u751f\u6210\u5bf9\u8c61\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u7b97\u6cd5\u3002"}}
{"id": "2509.20904", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20904", "abs": "https://arxiv.org/abs/2509.20904", "authors": ["Kairui Fu", "Tao Zhang", "Shuwen Xiao", "Ziyang Wang", "Xinming Zhang", "Chenchi Zhang", "Yuliang Yan", "Junjun Zheng", "Yu Li", "Zhihong Chen", "Jian Wu", "Xiangheng Kong", "Shengyu Zhang", "Kun Kuang", "Yuning Jiang", "Bo Zheng"], "title": "FORGE: Forming Semantic Identifiers for Generative Retrieval in Industrial Datasets", "comment": null, "summary": "Semantic identifiers (SIDs) have gained increasing attention in generative\nretrieval (GR) due to their meaningful semantic discriminability. However,\ncurrent research on SIDs faces three main challenges: (1) the absence of\nlarge-scale public datasets with multimodal features, (2) limited investigation\ninto optimization strategies for SID generation, which typically rely on costly\nGR training for evaluation, and (3) slow online convergence in industrial\ndeployment. To address these challenges, we propose FORGE, a comprehensive\nbenchmark for FOrming semantic identifieR in Generative rEtrieval with\nindustrial datasets. Specifically, FORGE is equipped with a dataset comprising\n14 billion user interactions and multimodal features of 250 million items\nsampled from Taobao, one of the biggest e-commerce platforms in China.\nLeveraging this dataset, FORGE explores several optimizations to enhance the\nSID construction and validates their effectiveness via offline experiments\nacross different settings and tasks. Further online analysis conducted on our\nplatform, which serves over 300 million users daily, reveals a 0.35% increase\nin transaction count, highlighting the practical impact of our method.\nRegarding the expensive SID validation accompanied by the full training of GRs,\nwe propose two novel metrics of SID that correlate positively with\nrecommendation performance, enabling convenient evaluations without any GR\ntraining. For real-world applications, FORGE introduces an offline pretraining\nschema that reduces online convergence by half. The code and data are available\nat https://github.com/selous123/al_sid.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86FORGE\uff0c\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u68c0\u7d22\u4e2d\u8bed\u4e49\u6807\u8bc6\u7b26\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u5b83\u5305\u542b\u4e00\u4e2a\u6765\u81ea\u6dd8\u5b9d\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u63a2\u7d22\u4e86SID\u6784\u5efa\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u6807\u8bc6\u7b26\u7814\u7a76\u9762\u4e34\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7f3a\u5931\u3001SID\u751f\u6210\u4f18\u5316\u7b56\u7565\u7814\u7a76\u4e0d\u8db3\u4ee5\u53ca\u5728\u7ebf\u6536\u655b\u901f\u5ea6\u6162\u7b49\u6311\u6218\u3002", "method": "FORGE\u914d\u5907\u4e86\u4e00\u4e2a\u5305\u542b140\u4ebf\u7528\u6237\u4ea4\u4e92\u6570\u636e\u548c2.5\u4ebf\u5546\u54c1\u591a\u6a21\u6001\u7279\u5f81\u7684\u6570\u636e\u96c6\uff0c\u5e76\u63a2\u7d22\u4e86\u591a\u79cd\u4f18\u5316\u65b9\u6cd5\u6765\u589e\u5f3aSID\u7684\u6784\u5efa\u3002", "result": "\u79bb\u7ebf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86FORGE\u7684\u6709\u6548\u6027\uff0c\u5728\u7ebf\u5206\u6790\u663e\u793a\u4ea4\u6613\u6570\u91cf\u589e\u52a0\u4e860.35%\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u4e0e\u63a8\u8350\u6027\u80fd\u6b63\u76f8\u5173\u7684SID\u65b0\u6307\u6807\uff0c\u65e0\u9700GR\u8bad\u7ec3\u5373\u53ef\u8fdb\u884c\u8bc4\u4f30\u3002\u79bb\u7ebf\u9884\u8bad\u7ec3\u6a21\u5f0f\u5c06\u5728\u7ebf\u6536\u655b\u901f\u5ea6\u964d\u4f4e\u4e86\u4e00\u534a\u3002", "conclusion": "FORGE\u901a\u8fc7\u63d0\u4f9b\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3001\u4f18\u5316SID\u6784\u5efa\u65b9\u6cd5\u548c\u52a0\u901f\u5728\u7ebf\u6536\u655b\uff0c\u4e3a\u751f\u6210\u68c0\u7d22\u4e2d\u7684\u8bed\u4e49\u6807\u8bc6\u7b26\u7814\u7a76\u505a\u51fa\u4e86\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2509.20373", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20373", "abs": "https://arxiv.org/abs/2509.20373", "authors": ["Shreya G. Upadhyay", "Carlos Busso", "Chi-Chun Lee"], "title": "Speaker Style-Aware Phoneme Anchoring for Improved Cross-Lingual Speech Emotion Recognition", "comment": null, "summary": "Cross-lingual speech emotion recognition (SER) remains a challenging task due\nto differences in phonetic variability and speaker-specific expressive styles\nacross languages. Effectively capturing emotion under such diverse conditions\nrequires a framework that can align the externalization of emotions across\ndifferent speakers and languages. To address this problem, we propose a\nspeaker-style aware phoneme anchoring framework that aligns emotional\nexpression at the phonetic and speaker levels. Our method builds\nemotion-specific speaker communities via graph-based clustering to capture\nshared speaker traits. Using these groups, we apply dual-space anchoring in\nspeaker and phonetic spaces to enable better emotion transfer across languages.\nEvaluations on the MSP-Podcast (English) and BIIC-Podcast (Taiwanese Mandarin)\ncorpora demonstrate improved generalization over competitive baselines and\nprovide valuable insights into the commonalities in cross-lingual emotion\nrepresentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u8bed\u8a00\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\uff08SER\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5728\u97f3\u7d20\u548c\u8bf4\u8bdd\u4eba\u5c42\u9762\u8fdb\u884c\u5bf9\u9f50\uff0c\u4ee5\u89e3\u51b3\u4e0d\u540c\u8bed\u8a00\u5728\u8bed\u97f3\u53d8\u5f02\u6027\u548c\u8bf4\u8bdd\u4eba\u8868\u8fbe\u98ce\u683c\u4e0a\u7684\u5dee\u5f02\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u8de8\u8bed\u8a00\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\uff08SER\uff09\u7531\u4e8e\u4e0d\u540c\u8bed\u8a00\u5728\u8bed\u97f3\u53d8\u5f02\u6027\u548c\u8bf4\u8bdd\u4eba\u8868\u8fbe\u98ce\u683c\u4e0a\u7684\u5dee\u5f02\u800c\u4ecd\u7136\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u9700\u8981\u5728\u8fd9\u6837\u7684\u4e0d\u540c\u6761\u4ef6\u4e0b\u6709\u6548\u5730\u6355\u6349\u60c5\u611f\uff0c\u8fd9\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u5bf9\u9f50\u4e0d\u540c\u8bf4\u8bdd\u4eba\u548c\u8bed\u8a00\u7684\u60c5\u611f\u5916\u5316\u7684\u6846\u67b6\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bf4\u8bdd\u4eba\u98ce\u683c\u611f\u77e5\u7684\u97f3\u7d20\u951a\u5b9a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u97f3\u7d20\u548c\u8bf4\u8bdd\u4eba\u5c42\u9762\u8fdb\u884c\u60c5\u611f\u8868\u8fbe\u7684\u5bf9\u9f50\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u57fa\u4e8e\u56fe\u7684\u805a\u7c7b\u6784\u5efa\u7279\u5b9a\u4e8e\u60c5\u611f\u7684\u8bf4\u8bdd\u4eba\u793e\u533a\uff0c\u4ee5\u6355\u6349\u5171\u4eab\u7684\u8bf4\u8bdd\u4eba\u7279\u5f81\u3002\u5229\u7528\u8fd9\u4e9b\u7fa4\u4f53\uff0c\u6211\u4eec\u5728\u8bf4\u8bdd\u4eba\u548c\u8bed\u97f3\u7a7a\u95f4\u4e2d\u5e94\u7528\u53cc\u7a7a\u95f4\u951a\u5b9a\uff0c\u4ee5\u5b9e\u73b0\u8de8\u8bed\u8a00\u7684\u66f4\u597d\u7684\u60c5\u611f\u8f6c\u79fb\u3002", "result": "\u5728MSP-Podcast\uff08\u82f1\u8bed\uff09\u548cBIIC-Podcast\uff08\u53f0\u6e7e\u666e\u901a\u8bdd\uff09\u8bed\u6599\u5e93\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u7ade\u4e89\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u6240\u63d0\u9ad8\uff0c\u5e76\u4e3a\u8de8\u8bed\u8a00\u60c5\u611f\u8868\u793a\u7684\u5171\u6027\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bf4\u8bdd\u4eba\u98ce\u683c\u611f\u77e5\u7684\u97f3\u7d20\u951a\u5b9a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u97f3\u7d20\u548c\u8bf4\u8bdd\u4eba\u5c42\u9762\u8fdb\u884c\u60c5\u611f\u8868\u8fbe\u7684\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.20420", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20420", "abs": "https://arxiv.org/abs/2509.20420", "authors": ["Elias N. Zois", "Moises Diaz", "Salem Said", "Miguel A. Ferrer"], "title": "Quasi-Synthetic Riemannian Data Generation for Writer-Independent Offline Signature Verification", "comment": "9 pages, 3 figures", "summary": "Offline handwritten signature verification remains a challenging task,\nparticularly in writer-independent settings where models must generalize across\nunseen individuals. Recent developments have highlighted the advantage of\ngeometrically inspired representations, such as covariance descriptors on\nRiemannian manifolds. However, past or present, handcrafted or data-driven\nmethods usually depend on real-world signature datasets for classifier\ntraining. We introduce a quasi-synthetic data generation framework leveraging\nthe Riemannian geometry of Symmetric Positive Definite matrices (SPD). A small\nset of genuine samples in the SPD space is the seed to a Riemannian Gaussian\nMixture which identifies Riemannian centers as synthetic writers and variances\nas their properties. Riemannian Gaussian sampling on each center generates\npositive as well as negative synthetic SPD populations. A metric learning\nframework utilizes pairs of similar and dissimilar SPD points, subsequently\ntesting it over on real-world datasets. Experiments conducted on two popular\nsignature datasets, encompassing Western and Asian writing styles, demonstrate\nthe efficacy of the proposed approach under both intra- and cross- dataset\nevaluation protocols. The results indicate that our quasi-synthetic approach\nachieves low error rates, highlighting the potential of generating synthetic\ndata in Riemannian spaces for writer-independent signature verification\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u51c6\u5408\u6210\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635 (SPD) \u7684\u9ece\u66fc\u51e0\u4f55\uff0c\u901a\u8fc7\u9ece\u66fc\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u751f\u6210\u6b63\u8d1f\u6837\u672c\uff0c\u7528\u4e8e\u79bb\u7ebf\u624b\u5199\u7b7e\u540d\u9a8c\u8bc1\u3002", "motivation": "\u79bb\u7ebf\u624b\u5199\u7b7e\u540d\u9a8c\u8bc1\u5728\u4f5c\u8005\u72ec\u7acb\u7684\u73af\u5883\u4e2d\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u8fc7\u53bb\u7684\u6216\u73b0\u5728\u7684\u624b\u5de5\u6216\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u771f\u5b9e\u4e16\u754c\u7684\u7b7e\u540d\u6570\u636e\u96c6\u8fdb\u884c\u5206\u7c7b\u5668\u8bad\u7ec3\u3002", "method": "\u5229\u7528 SPD \u7a7a\u95f4\u7684\u9ece\u66fc\u51e0\u4f55\uff0c\u4ee5 SPD \u7a7a\u95f4\u4e2d\u7684\u4e00\u5c0f\u7ec4\u771f\u5b9e\u6837\u672c\u4f5c\u4e3a\u79cd\u5b50\uff0c\u901a\u8fc7\u9ece\u66fc\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u8bc6\u522b\u9ece\u66fc\u4e2d\u5fc3\u4f5c\u4e3a\u5408\u6210\u4f5c\u8005\uff0c\u5e76\u4ee5\u65b9\u5dee\u4f5c\u4e3a\u5176\u5c5e\u6027\u3002\u5728\u6bcf\u4e2a\u4e2d\u5fc3\u4e0a\u8fdb\u884c\u9ece\u66fc\u9ad8\u65af\u62bd\u6837\uff0c\u751f\u6210\u6b63\u7684\u548c\u8d1f\u7684\u5408\u6210 SPD \u7fa4\u4f53\u3002\u5229\u7528\u5ea6\u91cf\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u6210\u5bf9\u7684\u76f8\u4f3c\u548c\u4e0d\u76f8\u4f3c\u7684 SPD \u70b9\u8fdb\u884c\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5728\u5305\u542b\u897f\u65b9\u548c\u4e9a\u6d32\u4e66\u5199\u98ce\u683c\u7684\u4e24\u4e2a\u6d41\u884c\u7684\u7b7e\u540d\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5185\u90e8\u548c\u4ea4\u53c9\u6570\u636e\u96c6\u8bc4\u4f30\u534f\u8bae\u4e0b\u5747\u6709\u6548\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u51c6\u5408\u6210\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4f4e\u9519\u8bef\u7387\u3002", "conclusion": "\u5728\u9ece\u66fc\u7a7a\u95f4\u4e2d\u751f\u6210\u5408\u6210\u6570\u636e\u5bf9\u4e8e\u4f5c\u8005\u72ec\u7acb\u7684\u7b7e\u540d\u9a8c\u8bc1\u7cfb\u7edf\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2509.20368", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20368", "abs": "https://arxiv.org/abs/2509.20368", "authors": ["Theo Uscidda", "Matthew Trager", "Michael Kleinman", "Aditya Chattopadhyay", "Wei Xia", "Stefano Soatto"], "title": "LATTS: Locally Adaptive Test-Time Scaling", "comment": null, "summary": "One common strategy for improving the performance of Large Language Models\n(LLMs) on downstream tasks involves using a \\emph{verifier model} to either\nselect the best answer from a pool of candidates or to steer the\nauto-regressive generation process towards better outputs. This class of\nmethods typically results in improved accuracy at the cost of increased\ncomputation at test-time, a paradigm known as \\emph{test-time scaling}.\nHowever, most existing approaches increase computation uniformly across all\nsamples and generation steps, without considering the complexity of individual\ninstances, leading to inefficient resource use. We address this limitation by\nproposing an approach, called \\emph{Locally Adaptive Test-Time Scaling\n(LATTS)}, that allocates variable compute across generation steps.\nSpecifically, at each generation step, LATTS employs a verifier-based\nacceptance criterion to decide whether to resample, backtrack, restart, or stop\nthe generation process. This criterion effectively adjusts the per-step\ncomputational effort based on a precise notion of \\emph{local difficulty}\nderived from the verifier model. Empirical results show that LATTS achieves\nsignificantly superior accuracy--compute tradeoffs compared to standard\nverifier-based methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Locally Adaptive Test-Time Scaling (LATTS) \u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6839\u636e\u9a8c\u8bc1\u5668\u6a21\u578b\u5f97\u51fa\u7684\u5c40\u90e8\u96be\u5ea6\u6765\u8c03\u6574\u6bcf\u6b65\u7684\u8ba1\u7b97\u91cf\uff0c\u4ece\u800c\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u66f4\u597d\u7684\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6539\u8fdb\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u65b9\u6cd5\u901a\u5e38\u5728\u6240\u6709\u6837\u672c\u548c\u751f\u6210\u6b65\u9aa4\u4e2d\u7edf\u4e00\u589e\u52a0\u8ba1\u7b97\u91cf\uff0c\u5bfc\u81f4\u8d44\u6e90\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\u3002", "method": "LATTS \u5728\u6bcf\u4e2a\u751f\u6210\u6b65\u9aa4\u91c7\u7528\u57fa\u4e8e\u9a8c\u8bc1\u5668\u7684\u63a5\u53d7\u6807\u51c6\uff0c\u4ee5\u51b3\u5b9a\u662f\u5426\u91cd\u65b0\u91c7\u6837\u3001\u56de\u6eaf\u3001\u91cd\u542f\u6216\u505c\u6b62\u751f\u6210\u8fc7\u7a0b\u3002\u8be5\u6807\u51c6\u6839\u636e\u9a8c\u8bc1\u5668\u6a21\u578b\u5f97\u51fa\u7684\u5c40\u90e8\u96be\u5ea6\u6765\u6709\u6548\u8c03\u6574\u6bcf\u6b65\u7684\u8ba1\u7b97\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6807\u51c6\u7684\u57fa\u4e8e\u9a8c\u8bc1\u5668\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cLATTS \u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u663e\u8457\u7684\u4f18\u52bf\u3002", "conclusion": "LATTS \u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u7684\u5229\u7528\u7387\u3002"}}
{"id": "2509.20416", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20416", "abs": "https://arxiv.org/abs/2509.20416", "authors": ["Haiduo Huang", "Jiangcheng Song", "Wenzhe Zhao", "Pengju Ren"], "title": "FastEagle: Cascaded Drafting for Accelerating Speculative Decoding", "comment": null, "summary": "Speculative decoding accelerates generation by drafting candidates and\nverifying them in parallel, yet state-of-the-art drafters (e.g., EAGLE) still\nrequire N sequential passes to propose N tokens. We present FastEagle, a\nnon-autoregressive cascaded drafter that emits an entire draft in a single\nforward pass. FastEagle replaces temporal steps with a lightweight layer\ncascade and trains with layer-wise supervision to mitigate error accumulation.\nCoupled with a constrained draft tree that preserves lossless verification\ncost, FastEagle delivers substantial wall-clock speedups over strong\nautoregressive drafters while maintaining competitive acceptance behavior.\nAcross multiple LLMs (Vicuna-13B, LLaMA-Instruct 3.x, and\nDeepSeek-R1-Distill-LLaMA) and tasks (MT-Bench, HumanEval, GSM8K, CNN/DM,\nAlpaca), FastEagle consistently outperforms EAGLE-3 in speedup under both\ngreedy and stochastic decoding, with comparable average acceptance lengths.\nThese results indicate that removing sequential dependencies in drafting is a\npractical path toward lossless LLM inference acceleration.", "AI": {"tldr": "FastEagle\u662f\u4e00\u4e2a\u975e\u81ea\u56de\u5f52\u7684\u8349\u7a3f\u5668\uff0c\u5b83\u53ef\u4ee5\u5728\u4e00\u4e2a\u524d\u5411\u8fc7\u7a0b\u4e2d\u751f\u6210\u6574\u4e2a\u8349\u7a3f\uff0c\u4ece\u800c\u52a0\u901f\u751f\u6210\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u7684\u8349\u7a3f\u5668\u9700\u8981N\u4e2a\u8fde\u7eed\u7684\u6b65\u9aa4\u6765\u751f\u6210N\u4e2atoken\uff0c\u6548\u7387\u8f83\u4f4e\u3002", "method": "FastEagle\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7684\u5c42\u53e0\u7ed3\u6784\u4ee3\u66ff\u65f6\u95f4\u6b65\uff0c\u5e76\u901a\u8fc7\u5c42\u7ea7\u7684\u76d1\u7763\u8bad\u7ec3\u6765\u51cf\u5c11\u8bef\u5dee\u7d2f\u79ef\u3002", "result": "FastEagle\u5728\u591a\u4e2aLLM\u548c\u4efb\u52a1\u4e2d\uff0c\u59cb\u7ec8\u4f18\u4e8eEAGLE-3\uff0c\u5e76\u4e14\u5177\u6709\u76f8\u5f53\u7684\u5e73\u5747\u63a5\u53d7\u957f\u5ea6\u3002", "conclusion": "\u5728\u8349\u7a3f\u4e2d\u79fb\u9664\u987a\u5e8f\u4f9d\u8d56\u6027\u662f\u5b9e\u73b0\u65e0\u635fLLM\u63a8\u7406\u52a0\u901f\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2509.20940", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20940", "abs": "https://arxiv.org/abs/2509.20940", "authors": ["Su Liu", "Bin Bi", "Jan Bakus", "Paritosh Kumar Velalam", "Vijay Yella", "Vinod Hegde"], "title": "Markup Language Modeling for Web Document Understanding", "comment": null, "summary": "Web information extraction (WIE) is an important part of many e-commerce\nsystems, supporting tasks like customer analysis and product recommendation. In\nthis work, we look at the problem of building up-to-date product databases by\nextracting detailed information from shopping review websites. We fine-tuned\nMarkupLM on product data gathered from review sites of different sizes and then\ndeveloped a variant we call MarkupLM++, which extends predictions to internal\nnodes of the DOM tree. Our experiments show that using larger and more diverse\ntraining sets improves extraction accuracy overall. We also find that including\ninternal nodes helps with some product attributes, although it leads to a\nslight drop in overall performance. The final model reached a precision of\n0.906, recall of 0.724, and an F1 score of 0.805.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u4ece\u8d2d\u7269\u8bc4\u8bba\u7f51\u7ad9\u63d0\u53d6\u8be6\u7ec6\u7684\u4ea7\u54c1\u4fe1\u606f\uff0c\u4ee5\u6784\u5efa\u6700\u65b0\u7684\u4ea7\u54c1\u6570\u636e\u5e93\u3002", "motivation": "\u7535\u5b50\u5546\u52a1\u7cfb\u7edf\u4e2d\uff0c\u7f51\u7edc\u4fe1\u606f\u63d0\u53d6\u5728\u5ba2\u6237\u5206\u6790\u548c\u4ea7\u54c1\u63a8\u8350\u7b49\u4efb\u52a1\u4e2d\u626e\u6f14\u7740\u91cd\u8981\u89d2\u8272\u3002\u672c\u6587\u7740\u773c\u4e8e\u901a\u8fc7\u4ece\u8d2d\u7269\u8bc4\u8bba\u7f51\u7ad9\u63d0\u53d6\u8be6\u7ec6\u4fe1\u606f\u6765\u6784\u5efa\u6700\u65b0\u7684\u4ea7\u54c1\u6570\u636e\u5e93\u3002", "method": "\u672c\u6587\u5728\u4ece\u4e0d\u540c\u89c4\u6a21\u7684\u8bc4\u8bba\u7f51\u7ad9\u6536\u96c6\u7684\u4ea7\u54c1\u6570\u636e\u4e0a\u5fae\u8c03\u4e86MarkupLM\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aMarkupLM++\u7684\u53d8\u4f53\uff0c\u8be5\u53d8\u4f53\u5c06\u9884\u6d4b\u6269\u5c55\u5230DOM\u6811\u7684\u5185\u90e8\u8282\u70b9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u66f4\u5927\u3001\u66f4\u591a\u6837\u5316\u7684\u8bad\u7ec3\u96c6\u53ef\u4ee5\u63d0\u9ad8\u6574\u4f53\u63d0\u53d6\u7cbe\u5ea6\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u5305\u542b\u5185\u90e8\u8282\u70b9\u6709\u52a9\u4e8e\u67d0\u4e9b\u4ea7\u54c1\u5c5e\u6027\u7684\u63d0\u53d6\uff0c\u4f46\u4f1a\u5bfc\u81f4\u6574\u4f53\u6027\u80fd\u7565\u6709\u4e0b\u964d\u3002\u6700\u7ec8\u6a21\u578b\u7684\u7cbe\u786e\u7387\u4e3a0.906\uff0c\u53ec\u56de\u7387\u4e3a0.724\uff0cF1\u5f97\u5206\u4e3a0.805\u3002", "conclusion": "\u672c\u6587\u8868\u660e\uff0c\u4f7f\u7528\u66f4\u5927\u3001\u66f4\u591a\u6837\u5316\u7684\u8bad\u7ec3\u96c6\u53ef\u4ee5\u63d0\u9ad8\u6574\u4f53\u63d0\u53d6\u7cbe\u5ea6\uff0c\u5305\u542b\u5185\u90e8\u8282\u70b9\u6709\u52a9\u4e8e\u67d0\u4e9b\u4ea7\u54c1\u5c5e\u6027\u7684\u63d0\u53d6\u3002"}}
{"id": "2509.20374", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20374", "abs": "https://arxiv.org/abs/2509.20374", "authors": ["Nithin Somasekharan", "Ling Yue", "Yadi Cao", "Weichao Li", "Patrick Emami", "Pochinapeddi Sai Bhargav", "Anurag Acharya", "Xingyu Xie", "Shaowu Pan"], "title": "CFD-LLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong performance across\ngeneral NLP tasks, but their utility in automating numerical experiments of\ncomplex physical system -- a critical and labor-intensive component -- remains\nunderexplored. As the major workhorse of computational science over the past\ndecades, Computational Fluid Dynamics (CFD) offers a uniquely challenging\ntestbed for evaluating the scientific capabilities of LLMs. We introduce\nCFDLLMBench, a benchmark suite comprising three complementary components --\nCFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM\nperformance across three key competencies: graduate-level CFD knowledge,\nnumerical and physical reasoning of CFD, and context-dependent implementation\nof CFD workflows. Grounded in real-world CFD practices, our benchmark combines\na detailed task taxonomy with a rigorous evaluation framework to deliver\nreproducible results and quantify LLM performance across code executability,\nsolution accuracy, and numerical convergence behavior. CFDLLMBench establishes\na solid foundation for the development and evaluation of LLM-driven automation\nof numerical experiments for complex physical systems. Code and data are\navailable at https://github.com/NREL-Theseus/cfdllmbench/.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aCFDLLMBench\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\uff08CFD\uff09\u9886\u57df\u7684\u6027\u80fd\u3002", "motivation": "\u8bc4\u4f30LLM\u5728\u81ea\u52a8\u5316\u590d\u6742\u7269\u7406\u7cfb\u7edf\u6570\u503c\u5b9e\u9a8c\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\uff08CFD\uff09\u8fd9\u4e00\u5173\u952e\u4e14\u52b3\u52a8\u5bc6\u96c6\u578b\u9886\u57df\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u5305\u542b\u4e09\u4e2a\u4e92\u8865\u7ec4\u4ef6\uff1aCFDQuery\u3001CFDCodeBench\u548cFoamBench\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30LLM\u5728CFD\u77e5\u8bc6\u3001\u6570\u503c\u548c\u7269\u7406\u63a8\u7406\u4ee5\u53caCFD\u5de5\u4f5c\u6d41\u7a0b\u7684\u4e0a\u4e0b\u6587\u76f8\u5173\u5b9e\u65bd\u65b9\u9762\u7684\u6027\u80fd\u3002", "result": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u5408\u4e86\u8be6\u7ec6\u7684\u4efb\u52a1\u5206\u7c7b\u548c\u4e25\u683c\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u63d0\u4f9b\u53ef\u91cd\u73b0\u7684\u7ed3\u679c\uff0c\u5e76\u91cf\u5316LLM\u5728\u4ee3\u7801\u53ef\u6267\u884c\u6027\u3001\u89e3\u51b3\u65b9\u6848\u51c6\u786e\u6027\u548c\u6570\u503c\u6536\u655b\u884c\u4e3a\u65b9\u9762\u7684\u6027\u80fd\u3002", "conclusion": "CFDLLMBench\u4e3a\u5f00\u53d1\u548c\u8bc4\u4f30LLM\u9a71\u52a8\u7684\u590d\u6742\u7269\u7406\u7cfb\u7edf\u6570\u503c\u5b9e\u9a8c\u81ea\u52a8\u5316\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2509.20427", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20427", "abs": "https://arxiv.org/abs/2509.20427", "authors": ["Team Seedream", "Yunpeng Chen", "Yu Gao", "Lixue Gong", "Meng Guo", "Qiushan Guo", "Zhiyao Guo", "Xiaoxia Hou", "Weilin Huang", "Yixuan Huang", "Xiaowen Jian", "Huafeng Kuang", "Zhichao Lai", "Fanshi Li", "Liang Li", "Xiaochen Lian", "Chao Liao", "Liyang Liu", "Wei Liu", "Yanzuo Lu", "Zhengxiong Luo", "Tongtong Ou", "Guang Shi", "Yichun Shi", "Shiqi Sun", "Yu Tian", "Zhi Tian", "Peng Wang", "Rui Wang", "Xun Wang", "Ye Wang", "Guofeng Wu", "Jie Wu", "Wenxu Wu", "Yonghui Wu", "Xin Xia", "Xuefeng Xiao", "Shuang Xu", "Xin Yan", "Ceyuan Yang", "Jianchao Yang", "Zhonghua Zhai", "Chenlin Zhang", "Heng Zhang", "Qi Zhang", "Xinyu Zhang", "Yuwei Zhang", "Shijia Zhao", "Wenliang Zhao", "Wenjia Zhu"], "title": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "comment": "Seedream 4.0 Technical Report", "summary": "We introduce Seedream 4.0, an efficient and high-performance multimodal image\ngeneration system that unifies text-to-image (T2I) synthesis, image editing,\nand multi-image composition within a single framework. We develop a highly\nefficient diffusion transformer with a powerful VAE which also can reduce the\nnumber of image tokens considerably. This allows for efficient training of our\nmodel, and enables it to fast generate native high-resolution images (e.g.,\n1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning\ndiverse taxonomies and knowledge-centric concepts. Comprehensive data\ncollection across hundreds of vertical scenarios, coupled with optimized\nstrategies, ensures stable and large-scale training, with strong\ngeneralization. By incorporating a carefully fine-tuned VLM model, we perform\nmulti-modal post-training for training both T2I and image editing tasks\njointly. For inference acceleration, we integrate adversarial distillation,\ndistribution matching, and quantization, as well as speculative decoding. It\nachieves an inference time of up to 1.8 seconds for generating a 2K image\n(without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream\n4.0 can achieve state-of-the-art results on both T2I and multimodal image\nediting. In particular, it demonstrates exceptional multimodal capabilities in\ncomplex tasks, including precise image editing and in-context reasoning, and\nalso allows for multi-image reference, and can generate multiple output images.\nThis extends traditional T2I systems into an more interactive and\nmultidimensional creative tool, pushing the boundary of generative AI for both\ncreativity and professional applications. Seedream 4.0 is now accessible on\nhttps://www.volcengine.com/experience/ark?launch=seedream.", "AI": {"tldr": "Seedream 4.0\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u9ad8\u6027\u80fd\u7684\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210\u7cfb\u7edf\uff0c\u7edf\u4e00\u4e86\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u3001\u56fe\u50cf\u7f16\u8f91\u548c\u591a\u56fe\u50cf\u7ec4\u5408\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u7684\u6548\u7387\u548c\u8d28\u91cf\uff0c\u5e76\u6269\u5c55\u5230\u66f4\u590d\u6742\u7684\u591a\u6a21\u6001\u4efb\u52a1\u3002", "method": "\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u6269\u6563Transformer\u548c\u5f3a\u5927\u7684VAE\uff0c\u5e76\u7ed3\u5408\u591a\u6a21\u6001\u540e\u8bad\u7ec3\u3001\u5bf9\u6297\u84b8\u998f\u3001\u5206\u5e03\u5339\u914d\u3001\u91cf\u5316\u548c\u63a8\u6d4b\u89e3\u7801\u7b49\u6280\u672f\u3002", "result": "\u5b9e\u73b0\u4e86\u5feb\u901f\u751f\u6210\u539f\u751f\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff08\u59821K-4K\uff09\uff0c\u5e76\u5728T2I\u548c\u591a\u6a21\u6001\u56fe\u50cf\u7f16\u8f91\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "Seedream 4.0\u5c06\u4f20\u7edf\u7684T2I\u7cfb\u7edf\u6269\u5c55\u4e3a\u66f4\u5177\u4ea4\u4e92\u6027\u548c\u591a\u7ef4\u5ea6\u7684\u521b\u610f\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u751f\u6210\u4eba\u5de5\u667a\u80fd\u5728\u521b\u610f\u548c\u4e13\u4e1a\u5e94\u7528\u65b9\u9762\u7684\u754c\u9650\u3002"}}
{"id": "2509.20370", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20370", "abs": "https://arxiv.org/abs/2509.20370", "authors": ["MZ Naser"], "title": "Philosophy-informed Machine Learning", "comment": null, "summary": "Philosophy-informed machine learning (PhIML) directly infuses core ideas from\nanalytic philosophy into ML model architectures, objectives, and evaluation\nprotocols. Therefore, PhIML promises new capabilities through models that\nrespect philosophical concepts and values by design. From this lens, this paper\nreviews conceptual foundations to demonstrate philosophical gains and\nalignment. In addition, we present case studies on how ML users/designers can\nadopt PhIML as an agnostic post-hoc tool or intrinsically build it into ML\nmodel architectures. Finally, this paper sheds light on open technical barriers\nalongside philosophical, practical, and governance challenges and outlines a\nresearch roadmap toward safe, philosophy-aware, and ethically responsible\nPhIML.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u54f2\u5b66\u6307\u5bfc\u7684\u673a\u5668\u5b66\u4e60(PhIML)\uff0c\u5b83\u5c06\u5206\u6790\u54f2\u5b66\u7684\u6838\u5fc3\u601d\u60f3\u76f4\u63a5\u6ce8\u5165\u5230ML\u6a21\u578b\u67b6\u6784\u3001\u76ee\u6807\u548c\u8bc4\u4f30\u534f\u8bae\u4e2d\u3002", "motivation": "\u901a\u8fc7\u5c0a\u91cd\u8bbe\u8ba1\u7684\u54f2\u5b66\u6982\u5ff5\u548c\u4ef7\u503c\u7684\u6a21\u578b\uff0cPhIML\u6709\u671b\u5b9e\u73b0\u65b0\u7684\u529f\u80fd\u3002\u672c\u6587\u65e8\u5728\u4ece\u6982\u5ff5\u57fa\u7840\u7684\u89d2\u5ea6\uff0c\u5c55\u793a\u54f2\u5b66\u4e0a\u7684\u6536\u76ca\u548c\u4e00\u81f4\u6027\u3002", "method": "\u672c\u6587\u56de\u987e\u4e86\u6982\u5ff5\u57fa\u7840\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86ML\u7528\u6237/\u8bbe\u8ba1\u8005\u5982\u4f55\u91c7\u7528PhIML\u4f5c\u4e3a\u4e0d\u53ef\u77e5\u7684\u4e8b\u540e\u5de5\u5177\u6216\u5c06\u5176\u5185\u5728\u6784\u5efa\u5230ML\u6a21\u578b\u67b6\u6784\u4e2d\u3002", "result": "\u672c\u6587\u91cd\u70b9\u4ecb\u7ecd\u4e86\u5f00\u653e\u7684\u6280\u672f\u969c\u788d\uff0c\u4ee5\u53ca\u54f2\u5b66\u3001\u5b9e\u8df5\u548c\u6cbb\u7406\u65b9\u9762\u7684\u6311\u6218\uff0c\u5e76\u6982\u8ff0\u4e86\u901a\u5411\u5b89\u5168\u3001\u5177\u6709\u54f2\u5b66\u610f\u8bc6\u548c\u5bf9\u4f26\u7406\u8d1f\u8d23\u7684PhIML\u7684\u7814\u7a76\u8def\u7ebf\u56fe\u3002", "conclusion": "PhIML\u901a\u8fc7\u5c06\u54f2\u5b66\u601d\u60f3\u878d\u5165\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u6709\u671b\u5728\u5b89\u5168\u548c\u4f26\u7406\u65b9\u9762\u5b9e\u73b0\u65b0\u7684\u7a81\u7834\uff0c\u4f46\u4ecd\u9762\u4e34\u6280\u672f\u3001\u54f2\u5b66\u548c\u5b9e\u8df5\u7b49\u591a\u91cd\u6311\u6218\u3002"}}
{"id": "2509.20422", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2509.20422", "abs": "https://arxiv.org/abs/2509.20422", "authors": ["Yiling Ma", "Nathan Luke Abraham", "Stefan Versick", "Roland Ruhnke", "Andrea Schneidereit", "Ulrike Niemeier", "Felix Back", "Peter Braesicke", "Peer Nowack"], "title": "mloz: A Highly Efficient Machine Learning-Based Ozone Parameterization for Climate Sensitivity Simulations", "comment": null, "summary": "Atmospheric ozone is a crucial absorber of solar radiation and an important\ngreenhouse gas. However, most climate models participating in the Coupled Model\nIntercomparison Project (CMIP) still lack an interactive representation of\nozone due to the high computational costs of atmospheric chemistry schemes.\nHere, we introduce a machine learning parameterization (mloz) to interactively\nmodel daily ozone variability and trends across the troposphere and\nstratosphere in standard climate sensitivity simulations, including two-way\ninteractions of ozone with the Quasi-Biennial Oscillation. We demonstrate its\nhigh fidelity on decadal timescales and its flexible use online across two\ndifferent climate models -- the UK Earth System Model (UKESM) and the German\nICOsahedral Nonhydrostatic (ICON) model. With atmospheric temperature profile\ninformation as the only input, mloz produces stable ozone predictions around 31\ntimes faster than the chemistry scheme in UKESM, contributing less than 4\npercent of the respective total climate model runtimes. In particular, we also\ndemonstrate its transferability to different climate models without chemistry\nschemes by transferring the parameterization from UKESM to ICON. This\nhighlights the potential for widespread adoption in CMIP-level climate models\nthat lack interactive chemistry for future climate change assessments,\nparticularly when focusing on climate sensitivity simulations, where ozone\ntrends and variability are known to significantly modulate atmospheric feedback\nprocesses.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u53c2\u6570\u5316\u65b9\u6cd5\uff08mloz\uff09\uff0c\u7528\u4e8e\u5728\u6c14\u5019\u6a21\u578b\u4e2d\u6a21\u62df\u81ed\u6c27\u53d8\u5316\u548c\u8d8b\u52bf\u3002", "motivation": "\u76ee\u524d\u6c14\u5019\u6a21\u578b\u7531\u4e8e\u5927\u6c14\u5316\u5b66\u65b9\u6848\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u901a\u5e38\u7f3a\u4e4f\u5bf9\u81ed\u6c27\u7684\u4ea4\u4e92\u8868\u793a\u3002", "method": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u53c2\u6570\u5316\u65b9\u6cd5\uff08mloz\uff09\uff0c\u4ee5\u5927\u6c14\u6e29\u5ea6\u5256\u9762\u4fe1\u606f\u4f5c\u4e3a\u552f\u4e00\u8f93\u5165\uff0c\u6a21\u62df\u5bf9\u6d41\u5c42\u548c\u5e73\u6d41\u5c42\u7684\u81ed\u6c27\u53d8\u5316\u548c\u8d8b\u52bf\u3002", "result": "mloz\u7684\u9884\u6d4b\u901f\u5ea6\u6bd4UKESM\u4e2d\u7684\u5316\u5b66\u65b9\u6848\u5feb\u7ea631\u500d\uff0c\u4e14\u5728\u6c14\u5019\u6a21\u578b\u603b\u8fd0\u884c\u65f6\u95f4\u4e2d\u8d21\u732e\u5c0f\u4e8e4%\u3002", "conclusion": "mloz\u5177\u6709\u9ad8\u4fdd\u771f\u5ea6\u3001\u7075\u6d3b\u6027\u548c\u53ef\u79fb\u690d\u6027\uff0c\u6709\u6f5c\u529b\u5728\u7f3a\u4e4f\u4ea4\u4e92\u5316\u5b66\u7684CMIP\u7ea7\u522b\u6c14\u5019\u6a21\u578b\u4e2d\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2509.20989", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20989", "abs": "https://arxiv.org/abs/2509.20989", "authors": ["Zhangchi Zhu", "Wei Zhang"], "title": "Rejuvenating Cross-Entropy Loss in Knowledge Distillation for Recommender Systems", "comment": null, "summary": "This paper analyzes Cross-Entropy (CE) loss in knowledge distillation (KD)\nfor recommender systems. KD for recommender systems targets at distilling\nrankings, especially among items most likely to be preferred, and can only be\ncomputed on a small subset of items. Considering these features, we reveal the\nconnection between CE loss and NDCG in the field of KD. We prove that when\nperforming KD on an item subset, minimizing CE loss maximizes the lower bound\nof NDCG, only if an assumption of closure is satisfied. It requires that the\nitem subset consists of the student's top items. However, this contradicts our\ngoal of distilling rankings of the teacher's top items. We empirically\ndemonstrate the vast gap between these two kinds of top items. To bridge the\ngap between our goal and theoretical support, we propose Rejuvenated\nCross-Entropy for Knowledge Distillation (RCE-KD). It splits the top items\ngiven by the teacher into two subsets based on whether they are highly ranked\nby the student. For the subset that defies the condition, a sampling strategy\nis devised to use teacher-student collaboration to approximate our assumption\nof closure. We also combine the losses on the two subsets adaptively. Extensive\nexperiments demonstrate the effectiveness of our method. Our code is available\nat https://anonymous.4open.science/r/RCE-KD.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u63a8\u8350\u7cfb\u7edf\u4e2d\u77e5\u8bc6\u84b8\u998f\u7684\u4ea4\u53c9\u71b5\uff08CE\uff09\u635f\u5931\uff0c\u5e76\u63ed\u793a\u4e86CE\u635f\u5931\u4e0eNDCG\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "motivation": "\u77e5\u8bc6\u84b8\u998f\u65e8\u5728\u63d0\u70bc\u6392\u5e8f\uff0c\u5c24\u5176\u662f\u5728\u6700\u6709\u53ef\u80fd\u88ab\u504f\u597d\u7684\u9879\u76ee\u4e4b\u95f4\uff0c\u5e76\u4e14\u53ea\u80fd\u5728\u9879\u76ee\u7684\u5c0f\u5b50\u96c6\u4e0a\u8ba1\u7b97\u3002\u4f5c\u8005\u53d1\u73b0\u5b66\u751f\u6a21\u578btop\u9879\u76ee\u548c\u8001\u5e08\u6a21\u578btop\u9879\u76ee\u4e4b\u95f4\u5b58\u5728\u5de8\u5927\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e86\u7528\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u590d\u5174\u4ea4\u53c9\u71b5\uff08RCE-KD\uff09\u3002\u5b83\u6839\u636e\u6559\u5e08\u7ed9\u51fa\u7684top\u9879\u76ee\u662f\u5426\u88ab\u5b66\u751f\u9ad8\u5ea6\u6392\u5e8f\uff0c\u5c06top\u9879\u76ee\u5206\u6210\u4e24\u4e2a\u5b50\u96c6\u3002\u5bf9\u4e8e\u8fdd\u53cd\u6761\u4ef6\u7684\u5b50\u96c6\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u62bd\u6837\u7b56\u7565\uff0c\u4ee5\u4f7f\u7528\u5e08\u751f\u534f\u4f5c\u6765\u8fd1\u4f3c\u6211\u4eec\u7684\u5c01\u95ed\u5047\u8bbe\u3002\u6211\u4eec\u8fd8\u81ea\u9002\u5e94\u5730\u7ed3\u5408\u4e86\u4e24\u4e2a\u5b50\u96c6\u4e0a\u7684\u635f\u5931\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u63d0\u9ad8\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2509.20375", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20375", "abs": "https://arxiv.org/abs/2509.20375", "authors": ["Sharanya Parimanoharan", "Ruwan D. Nawarathna"], "title": "Assessing Classical Machine Learning and Transformer-based Approaches for Detecting AI-Generated Research Text", "comment": null, "summary": "The rapid adoption of large language models (LLMs) such as ChatGPT has\nblurred the line between human and AI-generated texts, raising urgent questions\nabout academic integrity, intellectual property, and the spread of\nmisinformation. Thus, reliable AI-text detection is needed for fair assessment\nto safeguard human authenticity and cultivate trust in digital communication.\nIn this study, we investigate how well current machine learning (ML) approaches\ncan distinguish ChatGPT-3.5-generated texts from human-written texts employing\na labeled data set of 250 pairs of abstracts from a wide range of research\ntopics. We test and compare both classical (Logistic Regression armed with\nclassical Bag-of-Words, POS, and TF-IDF features) and transformer-based (BERT\naugmented with N-grams, DistilBERT, BERT with a lightweight custom classifier,\nand LSTM-based N-gram models) ML detection techniques. As we aim to assess each\nmodel's performance in detecting AI-generated research texts, we also aim to\ntest whether an ensemble of these models can outperform any single detector.\nResults show DistilBERT achieves the overall best performance, while Logistic\nRegression and BERT-Custom offer solid, balanced alternatives; LSTM- and\nBERT-N-gram approaches lag. The max voting ensemble of the three best models\nfails to surpass DistilBERT itself, highlighting the primacy of a single\ntransformer-based representation over mere model diversity. By comprehensively\nassessing the strengths and weaknesses of these AI-text detection approaches,\nthis work lays a foundation for more robust transformer frameworks with larger,\nricher datasets to keep pace with ever-improving generative AI models.", "AI": {"tldr": "\u63a2\u8ba8\u4e86\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u68c0\u6d4bChatGPT\u751f\u6210\u7684\u6587\u672c\u4e0e\u4eba\u7c7b\u64b0\u5199\u6587\u672c\u7684\u533a\u522b\uff0c\u65e8\u5728\u89e3\u51b3\u5b66\u672f\u8bda\u4fe1\u3001\u77e5\u8bc6\u4ea7\u6743\u548c\u9519\u8bef\u4fe1\u606f\u4f20\u64ad\u7b49\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT\uff09\u7684\u5feb\u901f\u666e\u53ca\u6a21\u7cca\u4e86\u4eba\u7c7b\u548cAI\u751f\u6210\u6587\u672c\u4e4b\u95f4\u7684\u754c\u9650\uff0c\u5f15\u53d1\u4e86\u5bf9\u5b66\u672f\u8bda\u4fe1\u3001\u77e5\u8bc6\u4ea7\u6743\u548c\u9519\u8bef\u4fe1\u606f\u4f20\u64ad\u7684\u62c5\u5fe7\u3002\u56e0\u6b64\uff0c\u9700\u8981\u53ef\u9760\u7684AI\u6587\u672c\u68c0\u6d4b\u6280\u672f\u6765\u8fdb\u884c\u516c\u5e73\u8bc4\u4f30\uff0c\u7ef4\u62a4\u4eba\u7c7b\u539f\u521b\u6027\uff0c\u5e76\u5728\u6570\u5b57\u901a\u4fe1\u4e2d\u57f9\u517b\u4fe1\u4efb\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86\u5305\u542b250\u5bf9\u6765\u81ea\u4e0d\u540c\u7814\u7a76\u4e3b\u9898\u7684\u6458\u8981\u7684\u6807\u8bb0\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u5e76\u6bd4\u8f83\u4e86\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u903b\u8f91\u56de\u5f52\u4e0e\u8bcd\u888b\u6a21\u578b\u3001\u8bcd\u6027\u6807\u6ce8\u548cTF-IDF\u7279\u5f81\uff09\u548c\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\uff08BERT\u4e0eN-grams\u3001DistilBERT\u3001\u5e26\u6709\u8f7b\u91cf\u7ea7\u81ea\u5b9a\u4e49\u5206\u7c7b\u5668\u7684BERT\u548c\u57fa\u4e8eLSTM\u7684N-gram\u6a21\u578b\uff09\u3002", "result": "DistilBERT\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u603b\u4f53\u6027\u80fd\uff0c\u800c\u903b\u8f91\u56de\u5f52\u548cBERT-Custom\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u5e73\u8861\u7684\u66ff\u4ee3\u65b9\u6848\uff1bLSTM\u548cBERT-N-gram\u65b9\u6cd5\u5219\u76f8\u5bf9\u843d\u540e\u3002\u6700\u4f73\u6a21\u578b\u7684\u6700\u5927\u6295\u7968\u96c6\u6210\u672a\u80fd\u8d85\u8fc7DistilBERT\u672c\u8eab\u3002", "conclusion": "\u8be5\u7814\u7a76\u5168\u9762\u8bc4\u4f30\u4e86\u5404\u79cdAI\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u4e3a\u6784\u5efa\u66f4\u5f3a\u5927\u7684Transformer\u6846\u67b6\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4ee5\u4fbf\u5229\u7528\u66f4\u5927\u3001\u66f4\u4e30\u5bcc\u7684\u6570\u636e\u96c6\u8ddf\u4e0a\u4e0d\u65ad\u6539\u8fdb\u7684\u751f\u6210\u5f0fAI\u6a21\u578b\u3002"}}
{"id": "2509.20474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20474", "abs": "https://arxiv.org/abs/2509.20474", "authors": ["Samia Saeed", "Khuram Naveed"], "title": "A Contrastive Learning Framework for Breast Cancer Detection", "comment": null, "summary": "Breast cancer, the second leading cause of cancer-related deaths globally,\naccounts for a quarter of all cancer cases [1]. To lower this death rate, it is\ncrucial to detect tumors early, as early-stage detection significantly improves\ntreatment outcomes. Advances in non-invasive imaging techniques have made early\ndetection possible through computer-aided detection (CAD) systems which rely on\ntraditional image analysis to identify malignancies. However, there is a\ngrowing shift towards deep learning methods due to their superior\neffectiveness. Despite their potential, deep learning methods often struggle\nwith accuracy due to the limited availability of large-labeled datasets for\ntraining. To address this issue, our study introduces a Contrastive Learning\n(CL) framework, which excels with smaller labeled datasets. In this regard, we\ntrain Resnet-50 in semi supervised CL approach using similarity index on a\nlarge amount of unlabeled mammogram data. In this regard, we use various\naugmentation and transformations which help improve the performance of our\napproach. Finally, we tune our model on a small set of labelled data that\noutperforms the existing state of the art. Specifically, we observed a 96.7%\naccuracy in detecting breast cancer on benchmark datasets INbreast and MIAS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60(CL)\u7684\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u4e73\u817a\u764c\u68c0\u6d4b\u4e2d\u56e0\u6807\u8bb0\u6570\u636e\u96c6\u6709\u9650\u800c\u5bfc\u81f4\u7684\u51c6\u786e\u6027\u95ee\u9898\u3002", "motivation": "\u65e9\u671f\u68c0\u6d4b\u5bf9\u4e8e\u6539\u5584\u4e73\u817a\u764c\u6cbb\u7597\u6548\u679c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7531\u4e8e\u7f3a\u4e4f\u5927\u578b\u6807\u8bb0\u6570\u636e\u96c6\u800c\u96be\u4ee5\u4fdd\u8bc1\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u534a\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u8bad\u7ec3Resnet-50\uff0c\u5229\u7528\u5927\u91cf\u672a\u6807\u8bb0\u7684\u4e73\u623fX\u5149\u7247\u6570\u636e\u548c\u76f8\u4f3c\u6027\u6307\u6807\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6INbreast\u548cMIAS\u4e0a\uff0c\u4e73\u817a\u764c\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe\u523096.7%\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u6c34\u5e73\u3002", "conclusion": "\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u4e73\u817a\u764c\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2509.20493", "categories": ["cs.AI", "cs.CL", "cs.DL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.20493", "abs": "https://arxiv.org/abs/2509.20493", "authors": ["Paris Koloveas", "Serafeim Chatzopoulos", "Thanasis Vergoulis", "Christos Tryfonopoulos"], "title": "InsightGUIDE: An Opinionated AI Assistant for Guided Critical Reading of Scientific Literature", "comment": "Accepted for publication on ICTAI 2025", "summary": "The proliferation of scientific literature presents an increasingly\nsignificant challenge for researchers. While Large Language Models (LLMs) offer\npromise, existing tools often provide verbose summaries that risk replacing,\nrather than assisting, the reading of the source material. This paper\nintroduces InsightGUIDE, a novel AI-powered tool designed to function as a\nreading assistant, not a replacement. Our system provides concise, structured\ninsights that act as a \"map\" to a paper's key elements by embedding an expert's\nreading methodology directly into its core AI logic. We present the system's\narchitecture, its prompt-driven methodology, and a qualitative case study\ncomparing its output to a general-purpose LLM. The results demonstrate that\nInsightGUIDE produces more structured and actionable guidance, serving as a\nmore effective tool for the modern researcher.", "AI": {"tldr": "InsightGUIDE is a new AI tool that provides concise, structured insights to help researchers understand scientific papers, acting as a reading assistant rather than a replacement.", "motivation": "The increasing amount of scientific literature makes it challenging for researchers to stay informed. Existing LLMs provide summaries that are too verbose.", "method": "The paper introduces InsightGUIDE, an AI-powered tool with a prompt-driven methodology. It embeds an expert's reading methodology into its core AI logic to provide structured insights.", "result": "InsightGUIDE produces more structured and actionable guidance compared to general-purpose LLMs.", "conclusion": "InsightGUIDE is a more effective tool for researchers by providing concise and structured insights, acting as a reading assistant."}}
{"id": "2509.20454", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20454", "abs": "https://arxiv.org/abs/2509.20454", "authors": ["Kay Fuhrmeister", "Arne Pelzer", "Fabian Radke", "Julia Lechinger", "Mahzad Gharleghi", "Thomas K\u00f6llmer", "Insa Wolf"], "title": "Bridging Privacy and Utility: Synthesizing anonymized EEG with constraining utility functions", "comment": null, "summary": "Electroencephalography (EEG) is widely used for recording brain activity and\nhas seen numerous applications in machine learning, such as detecting sleep\nstages and neurological disorders. Several studies have successfully shown the\npotential of EEG data for re-identification and leakage of other personal\ninformation. Therefore, the increasing availability of EEG consumer devices\nraises concerns about user privacy, motivating us to investigate how to\nsafeguard this sensitive data while retaining its utility for EEG applications.\nTo address this challenge, we propose a transformer-based autoencoder to create\nEEG data that does not allow for subject re-identification while still\nretaining its utility for specific machine learning tasks. We apply our\napproach to automatic sleep staging by evaluating the re-identification and\nutility potential of EEG data before and after anonymization. The results show\nthat the re-identifiability of the EEG signal can be substantially reduced\nwhile preserving its utility for machine learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8etransformer\u7684\u81ea\u7f16\u7801\u5668\uff0c\u7528\u4e8e\u521b\u5efa\u8111\u7535\u6570\u636e\uff0c\u8be5\u6570\u636e\u4e0d\u5141\u8bb8\u4e3b\u4f53\u91cd\u65b0\u8bc6\u522b\uff0c\u540c\u65f6\u4ecd\u4fdd\u6301\u5176\u7528\u4e8e\u7279\u5b9a\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u7684\u6548\u7528\u3002", "motivation": "\u8111\u7535\u56fe(EEG)\u88ab\u5e7f\u6cdb\u7528\u4e8e\u8bb0\u5f55\u5927\u8111\u6d3b\u52a8\uff0c\u5e76\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u6709\u8bb8\u591a\u5e94\u7528\uff0c\u4f8b\u5982\u68c0\u6d4b\u7761\u7720\u9636\u6bb5\u548c\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u3002\u4e00\u4e9b\u7814\u7a76\u6210\u529f\u5730\u8868\u660e\u4e86\u8111\u7535\u6570\u636e\u5728\u91cd\u65b0\u8bc6\u522b\u548c\u6cc4\u9732\u5176\u4ed6\u4e2a\u4eba\u4fe1\u606f\u65b9\u9762\u7684\u6f5c\u529b\u3002\u56e0\u6b64\uff0c\u8111\u7535\u6d88\u8d39\u8bbe\u5907\u7684\u65e5\u76ca\u666e\u53ca\u5f15\u8d77\u4e86\u4eba\u4eec\u5bf9\u7528\u6237\u9690\u79c1\u7684\u62c5\u5fe7\uff0c\u4fc3\u4f7f\u6211\u4eec\u7814\u7a76\u5982\u4f55\u5728\u4fdd\u62a4\u8fd9\u79cd\u654f\u611f\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u5728\u8111\u7535\u5e94\u7528\u4e2d\u7684\u6548\u7528\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8etransformer\u7684\u81ea\u7f16\u7801\u5668\u6765\u521b\u5efa\u8111\u7535\u6570\u636e\uff0c\u8be5\u6570\u636e\u4e0d\u5141\u8bb8\u4e3b\u4f53\u91cd\u65b0\u8bc6\u522b\uff0c\u540c\u65f6\u4ecd\u4fdd\u6301\u5176\u7528\u4e8e\u7279\u5b9a\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u7684\u6548\u7528\u3002\u6211\u4eec\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u5e94\u7528\u4e8e\u81ea\u52a8\u7761\u7720\u5206\u671f\uff0c\u901a\u8fc7\u8bc4\u4f30\u533f\u540d\u5316\u524d\u540e\u8111\u7535\u6570\u636e\u7684\u91cd\u65b0\u8bc6\u522b\u548c\u6548\u7528\u6f5c\u529b\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8111\u7535\u4fe1\u53f7\u7684\u53ef\u91cd\u65b0\u8bc6\u522b\u6027\u53ef\u4ee5\u5927\u5927\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6548\u7528\u3002", "conclusion": "\u8111\u7535\u4fe1\u53f7\u7684\u53ef\u91cd\u65b0\u8bc6\u522b\u6027\u53ef\u4ee5\u5927\u5927\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6548\u7528\u3002"}}
{"id": "2509.21179", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21179", "abs": "https://arxiv.org/abs/2509.21179", "authors": ["Huimin Yan", "Longfei Xu", "Junjie Sun", "Ni Ou", "Wei Luo", "Xing Tan", "Ran Cheng", "Kaikui Liu", "Xiangxiang Chu"], "title": "IntSR: An Integrated Generative Framework for Search and Recommendation", "comment": null, "summary": "Generative recommendation has emerged as a promising paradigm, demonstrating\nremarkable results in both academic benchmarks and industrial applications.\nHowever, existing systems predominantly focus on unifying retrieval and ranking\nwhile neglecting the integration of search and recommendation (S&R) tasks. What\nmakes search and recommendation different is how queries are formed: search\nuses explicit user requests, while recommendation relies on implicit user\ninterests. As for retrieval versus ranking, the distinction comes down to\nwhether the queries are the target items themselves. Recognizing the query as\ncentral element, we propose IntSR, an integrated generative framework for S&R.\nIntSR integrates these disparate tasks using distinct query modalities. It also\naddresses the increased computational complexity associated with integrated S&R\nbehaviors and the erroneous pattern learning introduced by a dynamically\nchanging corpus. IntSR has been successfully deployed across various scenarios\nin Amap, leading to substantial improvements in digital asset's GMV(+3.02%),\nPOI recommendation's CTR(+2.76%), and travel mode suggestion's ACC(+5.13%).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u641c\u7d22\u548c\u63a8\u8350\u7684\u96c6\u6210\u751f\u6210\u6846\u67b6 IntSR\uff0c\u8be5\u6846\u67b6\u5728 Amap \u7684\u5404\u79cd\u573a\u666f\u4e2d\u6210\u529f\u90e8\u7f72\uff0c\u5e76\u5728\u6570\u5b57\u8d44\u4ea7\u7684 GMV\u3001POI \u63a8\u8350\u7684 CTR \u548c\u51fa\u884c\u65b9\u5f0f\u5efa\u8bae\u7684 ACC \u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u4e3b\u8981\u4fa7\u91cd\u4e8e\u7edf\u4e00\u68c0\u7d22\u548c\u6392\u5e8f\uff0c\u800c\u5ffd\u7565\u4e86\u641c\u7d22\u548c\u63a8\u8350 (S&R) \u4efb\u52a1\u7684\u96c6\u6210\u3002\u641c\u7d22\u4f7f\u7528\u663e\u5f0f\u7528\u6237\u8bf7\u6c42\uff0c\u800c\u63a8\u8350\u4f9d\u8d56\u4e8e\u9690\u5f0f\u7528\u6237\u5174\u8da3\u3002\u68c0\u7d22\u4e0e\u6392\u5e8f\u7684\u533a\u522b\u5728\u4e8e\u67e5\u8be2\u662f\u5426\u4e3a\u76ee\u6807\u9879\u76ee\u672c\u8eab\u3002", "method": "IntSR \u4f7f\u7528\u4e0d\u540c\u7684\u67e5\u8be2\u6a21\u6001\u96c6\u6210\u4e86\u8fd9\u4e9b\u4e0d\u540c\u7684\u4efb\u52a1\u3002\u5b83\u8fd8\u89e3\u51b3\u4e86\u4e0e\u96c6\u6210 S&R \u884c\u4e3a\u76f8\u5173\u7684\u8ba1\u7b97\u590d\u6742\u6027\u589e\u52a0\u4ee5\u53ca\u52a8\u6001\u53d8\u5316\u7684\u8bed\u6599\u5e93\u5f15\u5165\u7684\u9519\u8bef\u6a21\u5f0f\u5b66\u4e60\u95ee\u9898\u3002", "result": "IntSR \u5df2\u5728 Amap \u7684\u5404\u79cd\u573a\u666f\u4e2d\u6210\u529f\u90e8\u7f72\uff0c\u4ece\u800c\u663e\u7740\u63d0\u9ad8\u4e86\u6570\u5b57\u8d44\u4ea7\u7684 GMV (+3.02%)\u3001POI \u63a8\u8350\u7684 CTR (+2.76%) \u548c\u51fa\u884c\u65b9\u5f0f\u5efa\u8bae\u7684 ACC (+5.13%)\u3002", "conclusion": "IntSR \u662f\u4e00\u79cd\u7528\u4e8e S&R \u7684\u96c6\u6210\u751f\u6210\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u4e0d\u540c\u7684\u67e5\u8be2\u6a21\u6001\u96c6\u6210\u4e0d\u540c\u7684\u4efb\u52a1\uff0c\u5e76\u89e3\u51b3\u4e86\u4e0e\u96c6\u6210 S&R \u884c\u4e3a\u76f8\u5173\u7684\u8ba1\u7b97\u590d\u6742\u6027\u589e\u52a0\u4ee5\u53ca\u52a8\u6001\u53d8\u5316\u7684\u8bed\u6599\u5e93\u5f15\u5165\u7684\u9519\u8bef\u6a21\u5f0f\u5b66\u4e60\u95ee\u9898\u3002"}}
{"id": "2509.20376", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20376", "abs": "https://arxiv.org/abs/2509.20376", "authors": ["Haoxuan Li", "Zhen Wen", "Qiqi Jiang", "Chenxiao Li", "Yuwei Wu", "Yuchen Yang", "Yiyao Wang", "Xiuqi Huang", "Minfeng Zhu", "Wei Chen"], "title": "ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable performance across a\nwide range of natural language tasks. Understanding how LLMs internally\nrepresent knowledge remains a significant challenge. Despite Sparse\nAutoencoders (SAEs) have emerged as a promising technique for extracting\ninterpretable features from LLMs, SAE features do not inherently align with\nhuman-understandable concepts, making their interpretation cumbersome and\nlabor-intensive. To bridge the gap between SAE features and human concepts, we\npresent ConceptViz, a visual analytics system designed for exploring concepts\nin LLMs. ConceptViz implements a novel dentification => Interpretation =>\nValidation pipeline, enabling users to query SAEs using concepts of interest,\ninteractively explore concept-to-feature alignments, and validate the\ncorrespondences through model behavior verification. We demonstrate the\neffectiveness of ConceptViz through two usage scenarios and a user study. Our\nresults show that ConceptViz enhances interpretability research by streamlining\nthe discovery and validation of meaningful concept representations in LLMs,\nultimately aiding researchers in building more accurate mental models of LLM\nfeatures. Our code and user guide are publicly available at\nhttps://github.com/Happy-Hippo209/ConceptViz.", "AI": {"tldr": "ConceptViz is a visual analytics system designed for exploring concepts in LLMs, enabling users to query SAEs using concepts of interest, interactively explore concept-to-feature alignments, and validate the correspondences through model behavior verification.", "motivation": "Understanding how LLMs internally represent knowledge remains a significant challenge. Sparse Autoencoders (SAEs) have emerged as a promising technique for extracting interpretable features from LLMs, but SAE features do not inherently align with human-understandable concepts, making their interpretation cumbersome and labor-intensive.", "method": "ConceptViz implements a novel Identification => Interpretation => Validation pipeline, enabling users to query SAEs using concepts of interest, interactively explore concept-to-feature alignments, and validate the correspondences through model behavior verification.", "result": "ConceptViz enhances interpretability research by streamlining the discovery and validation of meaningful concept representations in LLMs, ultimately aiding researchers in building more accurate mental models of LLM features.The effectiveness of ConceptViz is demonstrated through two usage scenarios and a user study.", "conclusion": "ConceptViz is effective in streamlining the discovery and validation of meaningful concept representations in LLMs, ultimately aiding researchers in building more accurate mental models of LLM features."}}
{"id": "2509.20479", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20479", "abs": "https://arxiv.org/abs/2509.20479", "authors": ["Simon Baeuerle", "Pratik Khanna", "Nils Friederich", "Angelo Jovin Yamachui Sitcheu", "Damir Shakirov", "Andreas Steimer", "Ralf Mikut"], "title": "Are Foundation Models Ready for Industrial Defect Recognition? A Reality Check on Real-World Data", "comment": null, "summary": "Foundation Models (FMs) have shown impressive performance on various text and\nimage processing tasks. They can generalize across domains and datasets in a\nzero-shot setting. This could make them suitable for automated quality\ninspection during series manufacturing, where various types of images are being\nevaluated for many different products. Replacing tedious labeling tasks with a\nsimple text prompt to describe anomalies and utilizing the same models across\nmany products would save significant efforts during model setup and\nimplementation. This is a strong advantage over supervised Artificial\nIntelligence (AI) models, which are trained for individual applications and\nrequire labeled training data. We test multiple recent FMs on both custom\nreal-world industrial image data and public image data. We show that all of\nthose models fail on our real-world data, while the very same models perform\nwell on public benchmark datasets.", "AI": {"tldr": "\u5927\u578b\u6a21\u578b\u5728\u6587\u672c\u548c\u56fe\u50cf\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5b9e\u9645\u5de5\u4e1a\u56fe\u50cf\u6570\u636e\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u5728\u7cfb\u5217\u5236\u9020\u4e2d\uff0c\u4f7f\u7528\u5927\u578b\u6a21\u578b\u8fdb\u884c\u81ea\u52a8\u8d28\u91cf\u68c0\u6d4b\u53ef\u4ee5\u8282\u7701\u5927\u91cf\u5de5\u4f5c\uff0c\u5e76\u4e14\u53ef\u4ee5\u66ff\u4ee3\u7e41\u7410\u7684\u6807\u6ce8\u4efb\u52a1\u3002", "method": "\u5728\u5b9a\u5236\u7684\u771f\u5b9e\u5de5\u4e1a\u56fe\u50cf\u6570\u636e\u548c\u516c\u5171\u56fe\u50cf\u6570\u636e\u4e0a\u6d4b\u8bd5\u591a\u4e2a\u6700\u65b0\u7684\u5927\u578b\u6a21\u578b\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u5747\u5931\u8d25\uff0c\u4f46\u5728\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u5927\u578b\u6a21\u578b\u5728\u5b9e\u9645\u5de5\u4e1a\u56fe\u50cf\u6570\u636e\u4e0a\u7684zero-shot\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002"}}
{"id": "2509.20513", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20513", "abs": "https://arxiv.org/abs/2509.20513", "authors": ["Samer Alshaer", "Ala Khalifeh", "Roman Obermaisser"], "title": "Reconstruction-Based Adaptive Scheduling Using AI Inferences in Safety-Critical Systems", "comment": "14 pages, 10 figures", "summary": "Adaptive scheduling is crucial for ensuring the reliability and safety of\ntime-triggered systems (TTS) in dynamic operational environments. Scheduling\nframeworks face significant challenges, including message collisions, locked\nloops from incorrect precedence handling, and the generation of incomplete or\ninvalid schedules, which can compromise system safety and performance. To\naddress these challenges, this paper presents a novel reconstruction framework\ndesigned to dynamically validate and assemble schedules. The proposed\nreconstruction models operate by systematically transforming AI-generated or\nheuristically derived scheduling priorities into fully executable schedules,\nensuring adherence to critical system constraints such as precedence rules and\ncollision-free communication. It incorporates robust safety checks, efficient\nallocation algorithms, and recovery mechanisms to handle unexpected context\nevents, including hardware failures and mode transitions. Comprehensive\nexperiments were conducted across multiple performance profiles, including\nmakespan minimisation, workload balancing, and energy efficiency, to validate\nthe operational effectiveness of the reconstruction models. Results demonstrate\nthat the proposed framework significantly enhances system adaptability,\noperational integrity, and runtime performance while maintaining computational\nefficiency. Overall, this work contributes a practical and scalable solution to\nthe problem of safe schedule generation in safety-critical TTS, enabling\nreliable and flexible real-time scheduling even under highly dynamic and\nuncertain operational conditions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cd\u6784\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u9a8c\u8bc1\u548c\u7ec4\u88c5\u8c03\u5ea6\uff0c\u4ee5\u786e\u4fdd\u65f6\u95f4\u89e6\u53d1\u7cfb\u7edf\uff08TTS\uff09\u5728\u52a8\u6001\u64cd\u4f5c\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5728\u52a8\u6001\u64cd\u4f5c\u73af\u5883\u4e2d\uff0c\u65f6\u95f4\u89e6\u53d1\u7cfb\u7edf\uff08TTS\uff09\u7684\u8c03\u5ea6\u6846\u67b6\u9762\u4e34\u6d88\u606f\u51b2\u7a81\u3001\u4e0d\u6b63\u786e\u7684\u4f18\u5148\u7ea7\u5904\u7406\u5bfc\u81f4\u7684\u9501\u6b7b\u5faa\u73af\u4ee5\u53ca\u751f\u6210\u4e0d\u5b8c\u6574\u6216\u65e0\u6548\u7684\u8c03\u5ea6\u7b49\u6311\u6218\uff0c\u8fd9\u4e9b\u90fd\u4f1a\u635f\u5bb3\u7cfb\u7edf\u5b89\u5168\u548c\u6027\u80fd\u3002", "method": "\u8be5\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u5730\u5c06AI\u751f\u6210\u6216\u542f\u53d1\u5f0f\u5bfc\u51fa\u7684\u8c03\u5ea6\u4f18\u5148\u7ea7\u8f6c\u6362\u4e3a\u5b8c\u5168\u53ef\u6267\u884c\u7684\u8c03\u5ea6\u6765\u8fd0\u884c\uff0c\u786e\u4fdd\u7b26\u5408\u5173\u952e\u7cfb\u7edf\u7ea6\u675f\uff0c\u5982\u4f18\u5148\u7ea7\u89c4\u5219\u548c\u65e0\u51b2\u7a81\u901a\u4fe1\u3002\u5b83\u7ed3\u5408\u4e86\u5f3a\u5927\u7684\u5b89\u5168\u68c0\u67e5\u3001\u9ad8\u6548\u7684\u5206\u914d\u7b97\u6cd5\u548c\u6062\u590d\u673a\u5236\u6765\u5904\u7406\u610f\u5916\u7684\u4e0a\u4e0b\u6587\u4e8b\u4ef6\uff0c\u5305\u62ec\u786c\u4ef6\u6545\u969c\u548c\u6a21\u5f0f\u8f6c\u6362\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u3001\u64cd\u4f5c\u5b8c\u6574\u6027\u548c\u8fd0\u884c\u65f6\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5b89\u5168\u5173\u952e\u578bTTS\u4e2d\u7684\u5b89\u5168\u8c03\u5ea6\u751f\u6210\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5373\u4f7f\u5728\u9ad8\u5ea6\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u7684\u64cd\u4f5c\u6761\u4ef6\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u53ef\u9760\u548c\u7075\u6d3b\u7684\u5b9e\u65f6\u8c03\u5ea6\u3002"}}
{"id": "2509.20463", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20463", "abs": "https://arxiv.org/abs/2509.20463", "authors": ["Tue Do", "Varun Chandrasekaran", "Daniel Alabi"], "title": "Efficiently Attacking Memorization Scores", "comment": null, "summary": "Influence estimation tools -- such as memorization scores -- are widely used\nto understand model behavior, attribute training data, and inform dataset\ncuration. However, recent applications in data valuation and responsible\nmachine learning raise the question: can these scores themselves be\nadversarially manipulated? In this work, we present a systematic study of the\nfeasibility of attacking memorization-based influence estimators. We\ncharacterize attacks for producing highly memorized samples as highly sensitive\nqueries in the regime where a trained algorithm is accurate. Our attack\n(calculating the pseudoinverse of the input) is practical, requiring only\nblack-box access to model outputs and incur modest computational overhead. We\nempirically validate our attack across a wide suite of image classification\ntasks, showing that even state-of-the-art proxies are vulnerable to targeted\nscore manipulations. In addition, we provide a theoretical analysis of the\nstability of memorization scores under adversarial perturbations, revealing\nconditions under which influence estimates are inherently fragile. Our findings\nhighlight critical vulnerabilities in influence-based attribution and suggest\nthe need for robust defenses. All code can be found at\nhttps://anonymous.4open.science/r/MemAttack-5413/", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u8bb0\u5fc6\u7684Influence Estimation\u65b9\u6cd5\u662f\u5426\u53ef\u4ee5\u88ab\u5bf9\u6297\u653b\u51fb\uff0c\u53d1\u73b0\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u4ee3\u7406\u4e5f\u5bb9\u6613\u53d7\u5230\u76ee\u6807\u5206\u6570\u64cd\u7eb5\u3002", "motivation": "\u5728\u6570\u636e\u4f30\u503c\u548c\u8d1f\u8d23\u4efb\u7684\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u63d0\u51fa\u4e86\u4e00\u4e2a\u95ee\u9898\uff1a\u8fd9\u4e9b\u5206\u6570\u672c\u8eab\u662f\u5426\u53ef\u4ee5\u88ab\u5bf9\u6297\u6027\u64cd\u7eb5\uff1f", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u7684\u7814\u7a76\uff0c\u7814\u7a76\u653b\u51fb\u57fa\u4e8e\u8bb0\u5fc6\u7684Influence Estimator\u7684\u53ef\u884c\u6027\u3002\u6211\u4eec\u7684\u653b\u51fb\u53ea\u9700\u8981\u5bf9\u6a21\u578b\u8f93\u51fa\u8fdb\u884c\u9ed1\u76d2\u8bbf\u95ee\uff0c\u5e76\u4e14\u8ba1\u7b97\u5f00\u9500\u9002\u4e2d(\u8ba1\u7b97\u8f93\u5165\u7684\u4f2a\u9006)\u3002", "result": "\u6211\u4eec\u901a\u8fc7\u5927\u91cf\u7684\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u653b\u51fb\uff0c\u8868\u660e\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u4ee3\u7406\u4e5f\u5bb9\u6613\u53d7\u5230\u76ee\u6807\u5206\u6570\u64cd\u7eb5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5bf9\u5bf9\u6297\u6270\u52a8\u4e0b\u8bb0\u5fc6\u5206\u6570\u7684\u7a33\u5b9a\u6027\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5f71\u54cd\u4f30\u8ba1\u672c\u8d28\u4e0a\u662f\u8106\u5f31\u7684\u6761\u4ef6\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u57fa\u4e8e\u5f71\u54cd\u7684\u5f52\u56e0\u4e2d\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u5e76\u5efa\u8bae\u9700\u8981\u5f3a\u5927\u7684\u9632\u5fa1\u3002"}}
{"id": "2509.21317", "categories": ["cs.IR", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.21317", "abs": "https://arxiv.org/abs/2509.21317", "authors": ["Jiakai Tang", "Yujie Luo", "Xunke Xi", "Fei Sun", "Xueyang Feng", "Sunhao Dai", "Chao Yi", "Dian Chen", "Zhujin Gao", "Yang Li", "Xu Chen", "Wen Chen", "Jian Wu", "Yuning Jiang", "Bo Zheng"], "title": "Interactive Recommendation Agent with Active User Commands", "comment": "Under Review", "summary": "Traditional recommender systems rely on passive feedback mechanisms that\nlimit users to simple choices such as like and dislike. However, these\ncoarse-grained signals fail to capture users' nuanced behavior motivations and\nintentions. In turn, current systems cannot also distinguish which specific\nitem attributes drive user satisfaction or dissatisfaction, resulting in\ninaccurate preference modeling. These fundamental limitations create a\npersistent gap between user intentions and system interpretations, ultimately\nundermining user satisfaction and harming system effectiveness.\n  To address these limitations, we introduce the Interactive Recommendation\nFeed (IRF), a pioneering paradigm that enables natural language commands within\nmainstream recommendation feeds. Unlike traditional systems that confine users\nto passive implicit behavioral influence, IRF empowers active explicit control\nover recommendation policies through real-time linguistic commands. To support\nthis paradigm, we develop RecBot, a dual-agent architecture where a Parser\nAgent transforms linguistic expressions into structured preferences and a\nPlanner Agent dynamically orchestrates adaptive tool chains for on-the-fly\npolicy adjustment. To enable practical deployment, we employ\nsimulation-augmented knowledge distillation to achieve efficient performance\nwhile maintaining strong reasoning capabilities. Through extensive offline and\nlong-term online experiments, RecBot shows significant improvements in both\nuser satisfaction and business outcomes.", "AI": {"tldr": "\u4f20\u7edf\u7684\u63a8\u8350\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u88ab\u52a8\u7684\u53cd\u9988\u673a\u5236\uff0c\u65e0\u6cd5\u6355\u6349\u7528\u6237\u7ec6\u81f4\u7684\u884c\u4e3a\u52a8\u673a\u548c\u610f\u56fe\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4ea4\u4e92\u5f0f\u63a8\u8350Feed\uff08IRF\uff09\uff0c\u5b83\u652f\u6301\u5728\u4e3b\u6d41\u63a8\u8350Feed\u4e2d\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u547d\u4ee4\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u53cc\u4ee3\u7406\u67b6\u6784RecBot\uff0cParser Agent\u5c06\u8bed\u8a00\u8868\u8fbe\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u504f\u597d\uff0cPlanner Agent\u52a8\u6001\u5730\u534f\u8c03\u81ea\u9002\u5e94\u5de5\u5177\u94fe\u4ee5\u8fdb\u884c\u5373\u65f6\u7b56\u7565\u8c03\u6574\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u79bb\u7ebf\u548c\u957f\u671f\u5728\u7ebf\u5b9e\u9a8c\uff0cRecBot\u5728\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u4e1a\u52a1\u6210\u679c\u65b9\u9762\u5747\u663e\u793a\u51fa\u663e\u7740\u6539\u5584\u3002", "motivation": "\u4f20\u7edf\u7684\u63a8\u8350\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u88ab\u52a8\u7684\u53cd\u9988\u673a\u5236\uff0c\u65e0\u6cd5\u6355\u6349\u7528\u6237\u7ec6\u81f4\u7684\u884c\u4e3a\u52a8\u673a\u548c\u610f\u56fe\uff0c\u5bfc\u81f4\u7528\u6237\u610f\u56fe\u548c\u7cfb\u7edf\u89e3\u91ca\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u6700\u7ec8\u635f\u5bb3\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u7cfb\u7edf\u6709\u6548\u6027\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4ea4\u4e92\u5f0f\u63a8\u8350Feed\uff08IRF\uff09\uff0c\u5b83\u652f\u6301\u5728\u4e3b\u6d41\u63a8\u8350Feed\u4e2d\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u547d\u4ee4\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u53cc\u4ee3\u7406\u67b6\u6784RecBot\uff0cParser Agent\u5c06\u8bed\u8a00\u8868\u8fbe\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u504f\u597d\uff0cPlanner Agent\u52a8\u6001\u5730\u534f\u8c03\u81ea\u9002\u5e94\u5de5\u5177\u94fe\u4ee5\u8fdb\u884c\u5373\u65f6\u7b56\u7565\u8c03\u6574\u3002\u4e3a\u4e86\u5b9e\u73b0\u5b9e\u9645\u90e8\u7f72\uff0c\u6211\u4eec\u91c7\u7528\u6a21\u62df\u589e\u5f3a\u77e5\u8bc6\u84b8\u998f\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u79bb\u7ebf\u548c\u957f\u671f\u5728\u7ebf\u5b9e\u9a8c\uff0cRecBot\u5728\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u4e1a\u52a1\u6210\u679c\u65b9\u9762\u5747\u663e\u793a\u51fa\u663e\u7740\u6539\u5584\u3002", "conclusion": "\u4ea4\u4e92\u5f0f\u63a8\u8350Feed\uff08IRF\uff09\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u8303\u4f8b\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u4e1a\u52a1\u6210\u679c\u3002"}}
{"id": "2509.20377", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20377", "abs": "https://arxiv.org/abs/2509.20377", "authors": ["Tomoaki Isoda"], "title": "SKILL-RAG: Self-Knowledge Induced Learning and Filtering for Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has significantly improved the\nperformance of large language models (LLMs) on knowledge-intensive tasks in\nrecent years. However, since retrieval systems may return irrelevant content,\nincorporating such information into the model often leads to hallucinations.\nThus, identifying and filtering out unhelpful retrieved content is a key\nchallenge for improving RAG performance.To better integrate the internal\nknowledge of the model with external knowledge from retrieval, it is essential\nto understand what the model \"knows\" and \"does not know\" (which is also called\n\"self-knowledge\"). Based on this insight, we propose SKILL-RAG (Self-Knowledge\nInduced Learning and Filtering for RAG), a novel method that leverages the\nmodel's self-knowledge to determine which retrieved documents are beneficial\nfor answering a given query. We design a reinforcement learning-based training\nframework to explicitly elicit self-knowledge from the model and employs\nsentence-level granularity to filter out irrelevant content while preserving\nuseful knowledge.We evaluate SKILL-RAG using Llama2-7B and Qwen3-8B on several\nquestion answering benchmarks. Experimental results demonstrate that SKILL-RAG\nnot only improves generation quality but also significantly reduces the number\nof input documents, validating the importance of self-knowledge in guiding the\nselection of high-quality retrievals.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSKILL-RAG\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u6a21\u578b\u7684\u81ea\u6211\u77e5\u8bc6\u6765\u5224\u65ad\u54ea\u4e9b\u68c0\u7d22\u5230\u7684\u6587\u6863\u5bf9\u56de\u7b54\u7ed9\u5b9a\u7684\u95ee\u9898\u662f\u6709\u76ca\u7684\u3002", "motivation": "\u68c0\u7d22\u7cfb\u7edf\u53ef\u80fd\u4f1a\u8fd4\u56de\u4e0d\u76f8\u5173\u7684\u5185\u5bb9\uff0c\u5c06\u8fd9\u4e9b\u4fe1\u606f\u7eb3\u5165\u6a21\u578b\u901a\u5e38\u4f1a\u5bfc\u81f4\u5e7b\u89c9\u3002\u56e0\u6b64\uff0c\u8bc6\u522b\u548c\u8fc7\u6ee4\u6389\u65e0\u7528\u7684\u68c0\u7d22\u5185\u5bb9\u662f\u63d0\u9ad8RAG\u6027\u80fd\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u4ee5\u660e\u786e\u5730\u4ece\u6a21\u578b\u4e2d\u63d0\u53d6\u81ea\u6211\u77e5\u8bc6\uff0c\u5e76\u91c7\u7528\u53e5\u5b50\u7ea7\u7684\u7c92\u5ea6\u6765\u8fc7\u6ee4\u6389\u4e0d\u76f8\u5173\u7684\u5185\u5bb9\uff0c\u540c\u65f6\u4fdd\u7559\u6709\u7528\u7684\u77e5\u8bc6\u3002", "result": "\u5728\u591a\u4e2a\u95ee\u7b54\u57fa\u51c6\u4e0a\u4f7f\u7528Llama2-7B\u548cQwen3-8B\u8bc4\u4f30SKILL-RAG\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSKILL-RAG\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u800c\u4e14\u663e\u8457\u51cf\u5c11\u4e86\u8f93\u5165\u6587\u6863\u7684\u6570\u91cf\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u81ea\u6211\u77e5\u8bc6\u5728\u6307\u5bfc\u9009\u62e9\u9ad8\u8d28\u91cf\u68c0\u7d22\u7ed3\u679c\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.20481", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20481", "abs": "https://arxiv.org/abs/2509.20481", "authors": ["Jing Li", "Oskar Bartosz", "Chengyu Wang", "Michal Wnuczynski", "Dilshan Godaliyadda", "Michael Polley"], "title": "Shared Neural Space: Unified Precomputed Feature Encoding for Multi-Task and Cross Domain Vision", "comment": null, "summary": "The majority of AI models in imaging and vision are customized to perform on\nspecific high-precision task. However, this strategy is inefficient for\napplications with a series of modular tasks, since each requires a mapping into\na disparate latent domain. To address this inefficiency, we proposed a\nuniversal Neural Space (NS), where an encoder-decoder framework pre-computes\nfeatures across vision and imaging tasks. Our encoder learns transformation\naware, generalizable representations, which enable multiple downstream AI\nmodules to share the same feature space. This architecture reduces redundancy,\nimproves generalization across domain shift, and establishes a foundation for\neffecient multi-task vision pipelines. Furthermore, as opposed to larger\ntransformer backbones, our backbone is lightweight and CNN-based, allowing for\nwider across hardware. We furthur demonstrate that imaging and vision modules,\nsuch as demosaicing, denoising, depth estimation and semantic segmentation can\nbe performed efficiently in the NS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u795e\u7ecf\u7a7a\u95f4\uff08NS\uff09\uff0c\u5176\u4e2d\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6846\u67b6\u9884\u5148\u8ba1\u7b97\u89c6\u89c9\u548c\u56fe\u50cf\u4efb\u52a1\u7684\u7279\u5f81\u3002", "motivation": "\u9488\u5bf9\u5927\u591a\u6570\u6210\u50cf\u548c\u89c6\u89c9\u4e2d\u7684AI\u6a21\u578b\u88ab\u5b9a\u5236\u4e3a\u6267\u884c\u7279\u5b9a\u7684\u9ad8\u7cbe\u5ea6\u4efb\u52a1\uff0c\u4f46\u8fd9\u79cd\u7b56\u7565\u5bf9\u4e8e\u5177\u6709\u4e00\u7cfb\u5217\u6a21\u5757\u5316\u4efb\u52a1\u7684\u5e94\u7528\u7a0b\u5e8f\u6765\u8bf4\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u90fd\u9700\u8981\u6620\u5c04\u5230\u4e0d\u540c\u7684\u6f5c\u5728\u57df\u3002", "method": "\u4f7f\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6846\u67b6\u9884\u5148\u8ba1\u7b97\u89c6\u89c9\u548c\u56fe\u50cf\u4efb\u52a1\u7684\u7279\u5f81\u3002\u7f16\u7801\u5668\u5b66\u4e60\u5177\u6709\u8f6c\u6362\u610f\u8bc6\u7684\u3001\u53ef\u6cdb\u5316\u7684\u8868\u793a\uff0c\u8fd9\u4f7f\u5f97\u591a\u4e2a\u4e0b\u6e38AI\u6a21\u5757\u53ef\u4ee5\u5171\u4eab\u76f8\u540c\u7684\u7279\u5f81\u7a7a\u95f4\u3002\u8be5backbone\u662f\u8f7b\u91cf\u7ea7\u7684\u548c\u57fa\u4e8eCNN\u7684\u3002", "result": "\u6210\u50cf\u548c\u89c6\u89c9\u6a21\u5757\uff0c\u5982demosaicing\u3001denoising\u3001\u6df1\u5ea6\u4f30\u8ba1\u548c\u8bed\u4e49\u5206\u5272\u53ef\u4ee5\u5728NS\u4e2d\u6709\u6548\u5730\u6267\u884c\u3002", "conclusion": "\u8be5\u67b6\u6784\u51cf\u5c11\u4e86\u5197\u4f59\uff0c\u63d0\u9ad8\u4e86\u8de8\u57df\u8f6c\u79fb\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e3a\u9ad8\u6548\u7684\u591a\u4efb\u52a1\u89c6\u89c9\u7ba1\u9053\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.20520", "categories": ["cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20520", "abs": "https://arxiv.org/abs/2509.20520", "authors": ["Samer Alshaer", "Ala Khalifeh", "Roman Obermaisser"], "title": "Adaptive Approach to Enhance Machine Learning Scheduling Algorithms During Runtime Using Reinforcement Learning in Metascheduling Applications", "comment": "18 pages, 21 figures", "summary": "Metascheduling in time-triggered architectures has been crucial in adapting\nto dynamic and unpredictable environments, ensuring the reliability and\nefficiency of task execution. However, traditional approaches face significant\nchallenges when training Artificial Intelligence (AI) scheduling inferences\noffline, particularly due to the complexities involved in constructing a\ncomprehensive Multi-Schedule Graph (MSG) that accounts for all possible\nscenarios. The process of generating an MSG that captures the vast probability\nspace, especially when considering context events like hardware failures, slack\nvariations, or mode changes, is resource-intensive and often infeasible. To\naddress these challenges, we propose an adaptive online learning unit\nintegrated within the metascheduler to enhance performance in real-time. The\nprimary motivation for developing this unit stems from the limitations of\noffline training, where the MSG created is inherently a subset of the complete\nspace, focusing only on the most probable and critical context events. In the\nonline mode, Reinforcement Learning (RL) plays a pivotal role by continuously\nexploring and discovering new scheduling solutions, thus expanding the MSG and\nenhancing system performance over time. This dynamic adaptation allows the\nsystem to handle unexpected events and complex scheduling scenarios more\neffectively. Several RL models were implemented within the online learning\nunit, each designed to address specific challenges in scheduling. These models\nnot only facilitate the discovery of new solutions but also optimize existing\nschedulers, particularly when stricter deadlines or new performance criteria\nare introduced. By continuously refining the AI inferences through real-time\ntraining, the system remains flexible and capable of meeting evolving demands,\nthus ensuring robustness and efficiency in large-scale, safety-critical\nenvironments.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u5728\u5143\u8c03\u5ea6\u5668\u4e2d\u7684\u81ea\u9002\u5e94\u5728\u7ebf\u5b66\u4e60\u5355\u5143\uff0c\u4ee5\u589e\u5f3a\u5b9e\u65f6\u6027\u80fd\uff0c\u89e3\u51b3\u79bb\u7ebf\u8bad\u7ec3AI\u8c03\u5ea6\u63a8\u7406\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u79bb\u7ebf\u8bad\u7ec3\u65b9\u6cd5\u96be\u4ee5\u6784\u5efa\u5305\u542b\u6240\u6709\u53ef\u80fd\u573a\u666f\u7684\u5b8c\u6574\u591a\u8c03\u5ea6\u56fe\uff08MSG\uff09\uff0c\u5c24\u5176\u662f\u5728\u8003\u8651\u786c\u4ef6\u6545\u969c\u3001\u677e\u5f1b\u53d8\u5316\u6216\u6a21\u5f0f\u6539\u53d8\u7b49\u4e0a\u4e0b\u6587\u4e8b\u4ef6\u65f6\uff0c\u751f\u6210MSG\u7684\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u5e38\u5e38\u4e0d\u53ef\u884c\u3002", "method": "\u8be5\u65b9\u6cd5\u5728\u5728\u7ebf\u6a21\u5f0f\u4e0b\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6301\u7eed\u63a2\u7d22\u548c\u53d1\u73b0\u65b0\u7684\u8c03\u5ea6\u65b9\u6848\uff0c\u4ece\u800c\u6269\u5c55MSG\u5e76\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u5b9e\u65f6\u8bad\u7ec3\u4e0d\u65ad\u6539\u8fdbAI\u63a8\u7406\uff0c\u7cfb\u7edf\u4fdd\u6301\u7075\u6d3b\u6027\uff0c\u80fd\u591f\u6ee1\u8db3\u4e0d\u65ad\u53d8\u5316\u7684\u9700\u6c42\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u786e\u4fdd\u4e86\u5927\u578b\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2509.20478", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20478", "abs": "https://arxiv.org/abs/2509.20478", "authors": ["Vivek Myers", "Bill Chunyuan Zheng", "Benjamin Eysenbach", "Sergey Levine"], "title": "Offline Goal-conditioned Reinforcement Learning with Quasimetric Representations", "comment": null, "summary": "Approaches for goal-conditioned reinforcement learning (GCRL) often use\nlearned state representations to extract goal-reaching policies. Two frameworks\nfor representation structure have yielded particularly effective GCRL\nalgorithms: (1) *contrastive representations*, in which methods learn\n\"successor features\" with a contrastive objective that performs inference over\nfuture outcomes, and (2) *temporal distances*, which link the (quasimetric)\ndistance in representation space to the transit time from states to goals. We\npropose an approach that unifies these two frameworks, using the structure of a\nquasimetric representation space (triangle inequality) with the right\nadditional constraints to learn successor representations that enable optimal\ngoal-reaching. Unlike past work, our approach is able to exploit a\n**quasimetric** distance parameterization to learn **optimal** goal-reaching\ndistances, even with **suboptimal** data and in **stochastic** environments.\nThis gives us the best of both worlds: we retain the stability and long-horizon\ncapabilities of Monte Carlo contrastive RL methods, while getting the free\nstitching capabilities of quasimetric network parameterizations. On existing\noffline GCRL benchmarks, our representation learning objective improves\nperformance on stitching tasks where methods based on contrastive learning\nstruggle, and on noisy, high-dimensional environments where methods based on\nquasimetric networks struggle.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\uff08GCRL\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7edf\u4e00\u4e86\u5bf9\u6bd4\u8868\u793a\u548c\u65f6\u95f4\u8ddd\u79bb\u4e24\u79cd\u6846\u67b6\uff0c\u5229\u7528\u62df\u5ea6\u91cf\u8868\u793a\u7a7a\u95f4\u7684\u7ed3\u6784\u6765\u5b66\u4e60\u80fd\u591f\u5b9e\u73b0\u6700\u4f73\u76ee\u6807\u5230\u8fbe\u7684\u540e\u7ee7\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u7684GCRL\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u5b66\u4e60\u5230\u7684\u72b6\u6001\u8868\u793a\u6765\u63d0\u53d6\u76ee\u6807\u5230\u8fbe\u7b56\u7565\u3002\u5bf9\u6bd4\u8868\u793a\u548c\u65f6\u95f4\u8ddd\u79bb\u662f\u4e24\u79cd\u6709\u6548\u7684\u8868\u793a\u7ed3\u6784\u6846\u67b6\uff0c\u4f46\u524d\u8005\u5728\u62fc\u63a5\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u540e\u8005\u5728\u9ad8\u7ef4\u566a\u58f0\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u62df\u5ea6\u91cf\u8868\u793a\u7a7a\u95f4\u7684\u7ed3\u6784\uff08\u4e09\u89d2\u4e0d\u7b49\u5f0f\uff09\uff0c\u5e76\u65bd\u52a0\u989d\u5916\u7684\u7ea6\u675f\uff0c\u4ee5\u5b66\u4e60\u540e\u7ee7\u8868\u793a\uff0c\u4ece\u800c\u5b9e\u73b0\u6700\u4f73\u76ee\u6807\u5230\u8fbe\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u6b21\u4f18\u6570\u636e\u548c\u968f\u673a\u73af\u5883\u4e2d\u5b66\u4e60\u6700\u4f73\u76ee\u6807\u5230\u8fbe\u8ddd\u79bb\uff0c\u5e76\u4e14\u5728\u5bf9\u6bd4\u5b66\u4e60\u96be\u4ee5\u5904\u7406\u7684\u62fc\u63a5\u4efb\u52a1\u548c\u62df\u5ea6\u91cf\u7f51\u7edc\u96be\u4ee5\u5904\u7406\u7684\u9ad8\u7ef4\u566a\u58f0\u73af\u5883\u4e2d\uff0c\u6027\u80fd\u5747\u6709\u6240\u63d0\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u8499\u7279\u5361\u6d1b\u5bf9\u6bd4RL\u65b9\u6cd5\u7684\u7a33\u5b9a\u6027\u548c\u957f\u65f6\u7a0b\u80fd\u529b\uff0c\u4ee5\u53ca\u62df\u5ea6\u91cf\u7f51\u7edc\u53c2\u6570\u5316\u7684\u81ea\u7531\u62fc\u63a5\u80fd\u529b\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684GCRL\u6027\u80fd\u3002"}}
{"id": "2509.20567", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20567", "abs": "https://arxiv.org/abs/2509.20567", "authors": ["Ayan Sar", "Pranav Singh Puri", "Sumit Aich", "Tanupriya Choudhury", "Abhijit Kumar"], "title": "SwasthLLM: a Unified Cross-Lingual, Multi-Task, and Meta-Learning Zero-Shot Framework for Medical Diagnosis Using Contrastive Representations", "comment": "Submitted to International Conference on Big Data 2025", "summary": "In multilingual healthcare environments, automatic disease diagnosis from\nclinical text remains a challenging task due to the scarcity of annotated\nmedical data in low-resource languages and the linguistic variability across\npopulations. This paper proposes SwasthLLM, a unified, zero-shot,\ncross-lingual, and multi-task learning framework for medical diagnosis that\noperates effectively across English, Hindi, and Bengali without requiring\nlanguage-specific fine-tuning. At its core, SwasthLLM leverages the\nmultilingual XLM-RoBERTa encoder augmented with a language-aware attention\nmechanism and a disease classification head, enabling the model to extract\nmedically relevant information regardless of the language structure. To align\nsemantic representations across languages, a Siamese contrastive learning\nmodule is introduced, ensuring that equivalent medical texts in different\nlanguages produce similar embeddings. Further, a translation consistency module\nand a contrastive projection head reinforce language-invariant representation\nlearning. SwasthLLM is trained using a multi-task learning strategy, jointly\noptimizing disease classification, translation alignment, and contrastive\nlearning objectives. Additionally, we employ Model-Agnostic Meta-Learning\n(MAML) to equip the model with rapid adaptation capabilities for unseen\nlanguages or tasks with minimal data. Our phased training pipeline emphasizes\nrobust representation alignment before task-specific fine-tuning. Extensive\nevaluation shows that SwasthLLM achieves high diagnostic performance, with a\ntest accuracy of 97.22% and an F1-score of 97.17% in supervised settings.\nCrucially, in zero-shot scenarios, it attains 92.78% accuracy on Hindi and\n73.33% accuracy on Bengali medical text, demonstrating strong generalization in\nlow-resource contexts.", "AI": {"tldr": "SwasthLLM: A multilingual, zero-shot, multi-task learning framework for medical diagnosis.", "motivation": "Automatic disease diagnosis from clinical text in multilingual healthcare is challenging due to data scarcity in low-resource languages and linguistic variability.", "method": "Utilizes XLM-RoBERTa with language-aware attention, Siamese contrastive learning, translation consistency, contrastive projection head, and MAML for rapid adaptation.", "result": "Achieves 97.22% accuracy and 97.17% F1-score in supervised settings, and 92.78% accuracy in Hindi and 73.33% in Bengali in zero-shot scenarios.", "conclusion": "SwasthLLM demonstrates strong generalization in low-resource contexts for medical diagnosis."}}
{"id": "2509.20378", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20378", "abs": "https://arxiv.org/abs/2509.20378", "authors": ["Sirui Wang", "Andong Chen", "Tiejun Zhao"], "title": "Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation", "comment": null, "summary": "Emotional text-to-speech (E-TTS) is central to creating natural and\ntrustworthy human-computer interaction. Existing systems typically rely on\nsentence-level control through predefined labels, reference audio, or natural\nlanguage prompts. While effective for global emotion expression, these\napproaches fail to capture dynamic shifts within a sentence. To address this\nlimitation, we introduce Emo-FiLM, a fine-grained emotion modeling framework\nfor LLM-based TTS. Emo-FiLM aligns frame-level features from emotion2vec to\nwords to obtain word-level emotion annotations, and maps them through a\nFeature-wise Linear Modulation (FiLM) layer, enabling word-level emotion\ncontrol by directly modulating text embeddings. To support evaluation, we\nconstruct the Fine-grained Emotion Dynamics Dataset (FEDD) with detailed\nannotations of emotional transitions. Experiments show that Emo-FiLM\noutperforms existing approaches on both global and fine-grained tasks,\ndemonstrating its effectiveness and generality for expressive speech synthesis.", "AI": {"tldr": "Emo-FiLM: A novel framework for fine-grained emotion control in LLM-based TTS, enabling word-level emotion modulation using emotion2vec and FiLM layers.", "motivation": "Existing E-TTS systems lack the ability to capture dynamic emotion shifts within a sentence, relying on global emotion control.", "method": "Emo-FiLM aligns frame-level features from emotion2vec to words for word-level emotion annotations, and uses a Feature-wise Linear Modulation (FiLM) layer to modulate text embeddings for word-level emotion control. A new dataset, FEDD, was created for evaluation.", "result": "Emo-FiLM outperforms existing methods on both global and fine-grained emotion tasks.", "conclusion": "Emo-FiLM is effective and general for expressive speech synthesis."}}
{"id": "2509.20484", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20484", "abs": "https://arxiv.org/abs/2509.20484", "authors": ["Dani Manjah", "Tim Bary", "Beno\u00eet G\u00e9rin", "Beno\u00eet Macq", "Christophe de Vleeschouwer"], "title": "Data-Efficient Stream-Based Active Distillation for Scalable Edge Model Deployment", "comment": "6 pages, 3 figures, 2 algorithms, presented at SEEDS Workshop (ICIP\n  2025)", "summary": "Edge camera-based systems are continuously expanding, facing ever-evolving\nenvironments that require regular model updates. In practice, complex teacher\nmodels are run on a central server to annotate data, which is then used to\ntrain smaller models tailored to the edge devices with limited computational\npower. This work explores how to select the most useful images for training to\nmaximize model quality while keeping transmission costs low. Our work shows\nthat, for a similar training load (i.e., iterations), a high-confidence\nstream-based strategy coupled with a diversity-based approach produces a\nhigh-quality model with minimal dataset queries.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u9009\u62e9\u6700\u6709\u7528\u7684\u56fe\u50cf\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u6a21\u578b\u8d28\u91cf\uff0c\u540c\u65f6\u964d\u4f4e\u4f20\u8f93\u6210\u672c\u3002", "motivation": "\u8fb9\u7f18\u76f8\u673a\u7cfb\u7edf\u4e0d\u65ad\u6269\u5c55\uff0c\u9762\u4e34\u7740\u4e0d\u65ad\u53d1\u5c55\u7684\u73af\u5883\uff0c\u9700\u8981\u5b9a\u671f\u66f4\u65b0\u6a21\u578b\u3002\u901a\u5e38\uff0c\u590d\u6742\u7684\u6559\u5e08\u6a21\u578b\u5728\u4e2d\u592e\u670d\u52a1\u5668\u4e0a\u8fd0\u884c\u4ee5\u6ce8\u91ca\u6570\u636e\uff0c\u7136\u540e\u7528\u4e8e\u8bad\u7ec3\u9488\u5bf9\u8ba1\u7b97\u80fd\u529b\u6709\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u91cf\u8eab\u5b9a\u5236\u7684\u8f83\u5c0f\u6a21\u578b\u3002", "method": "\u9ad8\u7f6e\u4fe1\u5ea6\u6d41\u5f0f\u7b56\u7565\u4e0e\u57fa\u4e8e\u591a\u6837\u6027\u7684\u65b9\u6cd5\u76f8\u7ed3\u5408\u3002", "result": "\u5bf9\u4e8e\u7c7b\u4f3c\u7684\u8bad\u7ec3\u8d1f\u8f7d\uff08\u5373\u8fed\u4ee3\uff09\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4ee5\u6700\u5c11\u7684\u6570\u636e\u96c6\u67e5\u8be2\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6a21\u578b\u3002", "conclusion": "\u9ad8\u7f6e\u4fe1\u5ea6\u6d41\u5f0f\u7b56\u7565\u4e0e\u57fa\u4e8e\u591a\u6837\u6027\u7684\u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u4ee5\u6700\u5c11\u7684\u6570\u636e\u96c6\u67e5\u8be2\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6a21\u578b\u3002"}}
{"id": "2509.20523", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20523", "abs": "https://arxiv.org/abs/2509.20523", "authors": ["Pawel Trajdos", "Marek Kurzynski"], "title": "A Compound Classification System Based on Fuzzy Relations Applied to the Noise-Tolerant Control of a Bionic Hand via EMG Signal Recognition", "comment": null, "summary": "Modern anthropomorphic upper limb bioprostheses are typically controlled by\nelectromyographic (EMG) biosignals using a pattern recognition scheme.\nUnfortunately, there are many factors originating from the human source of\nobjects to be classified and from the human-prosthesis interface that make it\ndifficult to obtain an acceptable classification quality. One of these factors\nis the high susceptibility of biosignals to contamination, which can\nconsiderably reduce the quality of classification of a recognition system.\n  In the paper, the authors propose a new recognition system intended for EMG\nbased control of the hand prosthesis with detection of contaminated biosignals\nin order to mitigate the adverse effect of contaminations. The system consists\nof two ensembles: the set of one-class classifiers (OCC) to assess the degree\nof contamination of individual channels and the ensemble of K-nearest\nneighbours (KNN) classifier to recognise the patient's intent. For all\nrecognition systems, an original, coherent fuzzy model was developed, which\nallows the use of a uniform soft (fuzzy) decision scheme throughout the\nrecognition process. The experimental evaluation was conducted using real\nbiosignals from a public repository. The goal was to provide an experimental\ncomparative analysis of the parameters and procedures of the developed method\non which the quality of the recognition system depends. The proposed fuzzy\nrecognition system was also compared with similar systems described in the\nliterature.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u808c\u7535\uff08EMG\uff09\u63a7\u5236\u624b\u90e8\u5047\u80a2\u8bc6\u522b\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u68c0\u6d4b\u53d7\u6c61\u67d3\u7684\u751f\u7269\u4fe1\u53f7\uff0c\u4ee5\u51cf\u8f7b\u6c61\u67d3\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "motivation": "\u808c\u7535\u751f\u7269\u4fe1\u53f7\u6613\u53d7\u6c61\u67d3\uff0c\u8fd9\u4f1a\u964d\u4f4e\u8bc6\u522b\u7cfb\u7edf\u7684\u5206\u7c7b\u8d28\u91cf\u3002", "method": "\u8be5\u7cfb\u7edf\u7531\u4e24\u4e2a\u96c6\u6210\u7ec4\u6210\uff1a\u5355\u7c7b\u5206\u7c7b\u5668\uff08OCC\uff09\u96c6\u5408\uff0c\u7528\u4e8e\u8bc4\u4f30\u5404\u4e2a\u901a\u9053\u7684\u6c61\u67d3\u7a0b\u5ea6\uff1bK\u8fd1\u90bb\uff08KNN\uff09\u5206\u7c7b\u5668\u96c6\u5408\uff0c\u7528\u4e8e\u8bc6\u522b\u60a3\u8005\u7684\u610f\u56fe\u3002\u4e3a\u6240\u6709\u8bc6\u522b\u7cfb\u7edf\u5f00\u53d1\u4e86\u4e00\u4e2a\u539f\u521b\u7684\u3001\u8fde\u8d2f\u7684\u6a21\u7cca\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5141\u8bb8\u5728\u6574\u4e2a\u8bc6\u522b\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u7edf\u4e00\u7684\u8f6f\uff08\u6a21\u7cca\uff09\u51b3\u7b56\u65b9\u6848\u3002", "result": "\u5bf9\u6240\u5f00\u53d1\u65b9\u6cd5\u7684\u53c2\u6570\u548c\u7a0b\u5e8f\u8fdb\u884c\u4e86\u5b9e\u9a8c\u6bd4\u8f83\u5206\u6790\uff0c\u5e76\u5c06\u5176\u4e0e\u6587\u732e\u4e2d\u63cf\u8ff0\u7684\u7c7b\u4f3c\u7cfb\u7edf\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u7cca\u8bc6\u522b\u7cfb\u7edf\u80fd\u591f\u68c0\u6d4b\u53d7\u6c61\u67d3\u7684\u751f\u7269\u4fe1\u53f7\uff0c\u4ece\u800c\u63d0\u9ad8\u808c\u7535\u63a7\u5236\u624b\u90e8\u5047\u80a2\u7684\u6027\u80fd\u3002"}}
{"id": "2509.20489", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20489", "abs": "https://arxiv.org/abs/2509.20489", "authors": ["D. Darankoum", "C. Habermacher", "J. Volle", "S. Grudinin"], "title": "CoSupFormer : A Contrastive Supervised learning approach for EEG signal Classification", "comment": "20 pages (14 pages Main text and 6 pages Supplementary Material)", "summary": "Electroencephalography signals (EEGs) contain rich multi-scale information\ncrucial for understanding brain states, with potential applications in\ndiagnosing and advancing the drug development landscape. However, extracting\nmeaningful features from raw EEG signals while handling noise and channel\nvariability remains a major challenge. This work proposes a novel end-to-end\ndeep-learning framework that addresses these issues through several key\ninnovations. First, we designed an encoder capable of explicitly capturing\nmulti-scale frequency oscillations covering a wide range of features for\ndifferent EEG-related tasks. Secondly, to model complex dependencies and handle\nthe high temporal resolution of EEGs, we introduced an attention-based encoder\nthat simultaneously learns interactions across EEG channels and within\nlocalized {\\em patches} of individual channels. We integrated a dedicated\ngating network on top of the attention encoder to dynamically filter out noisy\nand non-informative channels, enhancing the reliability of EEG data. The entire\nencoding process is guided by a novel loss function, which leverages supervised\nand contrastive learning, significantly improving model generalization. We\nvalidated our approach in multiple applications, ranging from the\nclassification of effects across multiple Central Nervous System (CNS)\ndisorders treatments to the diagnosis of Parkinson's and Alzheimer's disease.\nOur results demonstrate that the proposed learning paradigm can extract\nbiologically meaningful patterns from raw EEG signals across different species,\nautonomously select high-quality channels, and achieve robust generalization\nthrough innovative architectural and loss design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u8111\u7535\u4fe1\u53f7\u4e2d\u7684\u566a\u58f0\u548c\u901a\u9053\u53d8\u5f02\u6027\uff0c\u5e76\u63d0\u53d6\u6709\u610f\u4e49\u7684\u7279\u5f81\u3002", "motivation": "\u8111\u7535\u4fe1\u53f7\u5305\u542b\u4e30\u5bcc\u7684\u591a\u5c3a\u5ea6\u4fe1\u606f\uff0c\u5bf9\u4e8e\u7406\u89e3\u5927\u8111\u72b6\u6001\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u5728\u8bca\u65ad\u548c\u63a8\u8fdb\u836f\u7269\u5f00\u53d1\u9886\u57df\u5177\u6709\u6f5c\u5728\u5e94\u7528\u3002\u7136\u800c\uff0c\u4ece\u539f\u59cb\u8111\u7535\u4fe1\u53f7\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u7279\u5f81\uff0c\u540c\u65f6\u5904\u7406\u566a\u58f0\u548c\u901a\u9053\u53d8\u5f02\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u7684\u6311\u6218\u3002", "method": "\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u80fd\u591f\u663e\u5f0f\u6355\u83b7\u591a\u5c3a\u5ea6\u9891\u7387\u632f\u8361\u7684\u7f16\u7801\u5668\uff1b\u4e00\u4e2a\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7f16\u7801\u5668\uff0c\u53ef\u4ee5\u540c\u65f6\u5b66\u4e60\u8111\u7535\u901a\u9053\u4e4b\u95f4\u548c\u5c40\u90e8\u901a\u9053\u5185\u7684\u76f8\u4e92\u4f5c\u7528\uff1b\u4e00\u4e2a\u95e8\u63a7\u7f51\u7edc\uff0c\u7528\u4e8e\u52a8\u6001\u8fc7\u6ee4\u6389\u566a\u58f0\u548c\u975e\u4fe1\u606f\u901a\u9053\uff1b\u4ee5\u53ca\u4e00\u79cd\u5229\u7528\u76d1\u7763\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u65b0\u578b\u635f\u5931\u51fd\u6570\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u5305\u62ec\u591a\u79cd\u4e2d\u67a2\u795e\u7ecf\u7cfb\u7edf\uff08CNS\uff09\u75be\u75c5\u6cbb\u7597\u6548\u679c\u7684\u5206\u7c7b\u4ee5\u53ca\u5e15\u91d1\u68ee\u75c5\u548c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u8bca\u65ad\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5b66\u4e60\u8303\u5f0f\u53ef\u4ee5\u4ece\u4e0d\u540c\u7269\u79cd\u7684\u539f\u59cb\u8111\u7535\u4fe1\u53f7\u4e2d\u63d0\u53d6\u5177\u6709\u751f\u7269\u5b66\u610f\u4e49\u7684\u6a21\u5f0f\uff0c\u81ea\u4e3b\u9009\u62e9\u9ad8\u8d28\u91cf\u901a\u9053\uff0c\u5e76\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u548c\u635f\u5931\u8bbe\u8ba1\u5b9e\u73b0\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.20577", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20577", "abs": "https://arxiv.org/abs/2509.20577", "authors": ["Sampurna Roy", "Ayan Sar", "Anurag Kaushish", "Kanav Gupta", "Tanupriya Choudhury", "Abhijit Kumar"], "title": "Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures", "comment": "Submitted in IEEE International Conference on Big Data 2025", "summary": "Contemporary transformer architectures apply identical processing depth to\nall inputs, creating inefficiencies and limiting reasoning quality. Simple\nfactual queries are subjected to the same multilayered computation as complex\nlogical problems, wasting resources while constraining deep inference. To\novercome this, we came up with a concept of Dynamic Reasoning Chains through\nDepth Specialised Mixture of Experts (DS-MoE), a modular framework that extends\nthe Mixture of Experts paradigm from width-based to depth specialised\ncomputation. DS-MoE introduces expert modules optimised for distinct reasoning\ndepths, shallow pattern recognition, compositional reasoning, logical\ninference, memory integration, and meta-cognitive supervision. A learned\nrouting network dynamically assembles custom reasoning chains, activating only\nthe necessary experts to match input complexity. The dataset on which we\ntrained and evaluated DS-MoE is on The Pile, an 800GB corpus covering diverse\ndomains such as scientific papers, legal texts, programming code, and web\ncontent, enabling systematic assessment across reasoning depths. Experimental\nresults demonstrate that DS-MoE achieves up to 16 per cent computational\nsavings and 35 per cent faster inference compared to uniform-depth\ntransformers, while delivering 2.8 per cent higher accuracy on complex\nmulti-step reasoning benchmarks. Furthermore, routing decisions yield\ninterpretable reasoning chains, enhancing transparency and scalability. These\nfindings establish DS-MoE as a significant advancement in adaptive neural\narchitectures, demonstrating that depth-specialised modular processing can\nsimultaneously improve efficiency, reasoning quality, and interpretability in\nlarge-scale language models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aDS-MoE\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u6df1\u5ea6\u4e13\u4e1a\u5316\u7684\u6df7\u5408\u4e13\u5bb6\u6765\u521b\u5efa\u52a8\u6001\u63a8\u7406\u94fe\uff0c\u4ece\u800c\u514b\u670d\u4e86\u73b0\u6709transformer\u67b6\u6784\u7684\u4f4e\u6548\u95ee\u9898\u3002", "motivation": "\u73b0\u6709transformer\u67b6\u6784\u5bf9\u6240\u6709\u8f93\u5165\u5e94\u7528\u76f8\u540c\u7684\u5904\u7406\u6df1\u5ea6\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u5e76\u9650\u5236\u4e86\u63a8\u7406\u8d28\u91cf\u3002\u7b80\u5355\u7684\u67e5\u8be2\u548c\u590d\u6742\u7684\u903b\u8f91\u95ee\u9898\u53d7\u5230\u76f8\u540c\u7684\u591a\u5c42\u8ba1\u7b97\uff0c\u6d6a\u8d39\u8d44\u6e90\u3002", "method": "\u63d0\u51fa\u4e86\u6df1\u5ea6\u4e13\u4e1a\u5316\u7684\u6df7\u5408\u4e13\u5bb6(DS-MoE)\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u6df7\u5408\u4e13\u5bb6\u8303\u5f0f\u4ece\u57fa\u4e8e\u5bbd\u5ea6\u7684\u8ba1\u7b97\u6269\u5c55\u5230\u6df1\u5ea6\u4e13\u4e1a\u5316\u7684\u8ba1\u7b97\u3002DS-MoE\u5f15\u5165\u4e86\u9488\u5bf9\u4e0d\u540c\u63a8\u7406\u6df1\u5ea6\u4f18\u5316\u7684\u4e13\u5bb6\u6a21\u5757\uff0c\u5e76\u901a\u8fc7\u5b66\u4e60\u5230\u7684\u8def\u7531\u7f51\u7edc\u52a8\u6001\u7ec4\u88c5\u81ea\u5b9a\u4e49\u63a8\u7406\u94fe\u3002", "result": "DS-MoE\u5b9e\u73b0\u4e86\u9ad8\u8fbe16%\u7684\u8ba1\u7b97\u8282\u7701\u548c35%\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u5728\u590d\u6742\u7684\u591a\u6b65\u9aa4\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u4f9b\u4e862.8%\u7684\u66f4\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "DS-MoE\u662f\u81ea\u9002\u5e94\u795e\u7ecf\u67b6\u6784\u7684\u4e00\u4e2a\u91cd\u5927\u8fdb\u6b65\uff0c\u8bc1\u660e\u4e86\u6df1\u5ea6\u4e13\u4e1a\u5316\u7684\u6a21\u5757\u5316\u5904\u7406\u53ef\u4ee5\u540c\u65f6\u63d0\u9ad8\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\u3001\u63a8\u7406\u8d28\u91cf\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.20381", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20381", "abs": "https://arxiv.org/abs/2509.20381", "authors": ["Jianyu Wen", "Jingyun Wang", "Cilin Yan", "Jiayin Cai", "Xiaolong Jiang", "Ying Zhang"], "title": "USB-Rec: An Effective Framework for Improving Conversational Recommendation Capability of Large Language Model", "comment": "Accepted by Recsys'25", "summary": "Recently, Large Language Models (LLMs) have been widely employed in\nConversational Recommender Systems (CRSs). Unlike traditional language model\napproaches that focus on training, all existing LLMs-based approaches are\nmainly centered around how to leverage the summarization and analysis\ncapabilities of LLMs while ignoring the issue of training. Therefore, in this\nwork, we propose an integrated training-inference framework,\nUser-Simulator-Based framework (USB-Rec), for improving the performance of LLMs\nin conversational recommendation at the model level. Firstly, we design a\nLLM-based Preference Optimization (PO) dataset construction strategy for RL\ntraining, which helps the LLMs understand the strategies and methods in\nconversational recommendation. Secondly, we propose a Self-Enhancement Strategy\n(SES) at the inference stage to further exploit the conversational\nrecommendation potential obtained from RL training. Extensive experiments on\nvarious datasets demonstrate that our method consistently outperforms previous\nstate-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6539\u8fdbLLM\u5728\u5bf9\u8bdd\u63a8\u8350\u4e2d\u6027\u80fd\u7684\u96c6\u6210\u8bad\u7ec3-\u63a8\u7406\u6846\u67b6\uff08USB-Rec\uff09\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u5982\u4f55\u5229\u7528LLM\u7684\u603b\u7ed3\u548c\u5206\u6790\u80fd\u529b\uff0c\u800c\u5ffd\u7565\u4e86\u8bad\u7ec3\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u504f\u597d\u4f18\u5316\uff08PO\uff09\u6570\u636e\u96c6\u6784\u5efa\u7b56\u7565\u7528\u4e8eRL\u8bad\u7ec3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u63a8\u7406\u9636\u6bb5\u7684\u81ea\u589e\u5f3a\u7b56\u7565\uff08SES\uff09\u3002", "result": "\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u4ee5\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3-\u63a8\u7406\u6846\u67b6\uff0c\u53ef\u4ee5\u63d0\u9ad8LLM\u5728\u5bf9\u8bdd\u63a8\u8350\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2509.20524", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20524", "abs": "https://arxiv.org/abs/2509.20524", "authors": ["Julien Han", "Shuwen Qiu", "Qi Li", "Xingzi Xu", "Mehmet Saygin Seyfioglu", "Kavosh Asadi", "Karim Bouyarmane"], "title": "InstructVTON: Optimal Auto-Masking and Natural-Language-Guided Interactive Style Control for Inpainting-Based Virtual Try-On", "comment": "Submitted to CVPR 2025 and Published at CVPR 2025 AI for Content\n  Creation workshop", "summary": "We present InstructVTON, an instruction-following interactive virtual try-on\nsystem that allows fine-grained and complex styling control of the resulting\ngeneration, guided by natural language, on single or multiple garments. A\ncomputationally efficient and scalable formulation of virtual try-on formulates\nthe problem as an image-guided or image-conditioned inpainting task. These\ninpainting-based virtual try-on models commonly use a binary mask to control\nthe generation layout. Producing a mask that yields desirable result is\ndifficult, requires background knowledge, might be model dependent, and in some\ncases impossible with the masking-based approach (e.g. trying on a long-sleeve\nshirt with \"sleeves rolled up\" styling on a person wearing long-sleeve shirt\nwith sleeves down, where the mask will necessarily cover the entire sleeve).\nInstructVTON leverages Vision Language Models (VLMs) and image segmentation\nmodels for automated binary mask generation. These masks are generated based on\nuser-provided images and free-text style instructions. InstructVTON simplifies\nthe end-user experience by removing the necessity of a precisely drawn mask,\nand by automating execution of multiple rounds of image generation for try-on\nscenarios that cannot be achieved with masking-based virtual try-on models\nalone. We show that InstructVTON is interoperable with existing virtual try-on\nmodels to achieve state-of-the-art results with styling control.", "AI": {"tldr": "InstructVTON\u662f\u4e00\u4e2a\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u63a7\u5236\u7684\u4ea4\u4e92\u5f0f\u865a\u62df\u8bd5\u7a7f\u7cfb\u7edf\uff0c\u53ef\u4ee5\u5bf9\u5355\u4ef6\u6216\u591a\u4ef6\u670d\u88c5\u8fdb\u884c\u7ec6\u7c92\u5ea6\u548c\u590d\u6742\u7684\u98ce\u683c\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u63a9\u7801\u7684\u865a\u62df\u8bd5\u7a7f\u6a21\u578b\u5b58\u5728\u751f\u6210\u7406\u60f3\u7ed3\u679c\u56f0\u96be\u3001\u9700\u8981\u80cc\u666f\u77e5\u8bc6\u3001\u53ef\u80fd\u4f9d\u8d56\u6a21\u578b\u4ee5\u53ca\u67d0\u4e9b\u60c5\u51b5\u4e0b\u65e0\u6cd5\u5b9e\u73b0\u7684\u95ee\u9898\uff08\u4f8b\u5982\uff0c\u5728\u7a7f\u7740\u957f\u8896\u886c\u886b\u7684\u4eba\u8eab\u4e0a\u8bd5\u7a7f\u5377\u8d77\u8896\u5b50\u7684\u957f\u8896\u886c\u886b\uff09\u3002", "method": "InstructVTON\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u548c\u56fe\u50cf\u5206\u5272\u6a21\u578b\u81ea\u52a8\u751f\u6210\u4e8c\u5143\u63a9\u7801\uff0c\u8fd9\u4e9b\u63a9\u7801\u57fa\u4e8e\u7528\u6237\u63d0\u4f9b\u7684\u56fe\u50cf\u548c\u81ea\u7531\u6587\u672c\u98ce\u683c\u6307\u4ee4\u751f\u6210\u3002", "result": "InstructVTON\u7b80\u5316\u4e86\u6700\u7ec8\u7528\u6237\u4f53\u9a8c\uff0c\u65e0\u9700\u7cbe\u786e\u7ed8\u5236\u63a9\u7801\uff0c\u5e76\u81ea\u52a8\u6267\u884c\u591a\u8f6e\u56fe\u50cf\u751f\u6210\uff0c\u4ee5\u5b9e\u73b0\u4ec5\u4f7f\u7528\u57fa\u4e8e\u63a9\u7801\u7684\u865a\u62df\u8bd5\u7a7f\u6a21\u578b\u65e0\u6cd5\u5b9e\u73b0\u7684\u8bd5\u7a7f\u573a\u666f\u3002InstructVTON\u53ef\u4e0e\u73b0\u6709\u7684\u865a\u62df\u8bd5\u7a7f\u6a21\u578b\u4e92\u64cd\u4f5c\uff0c\u4ee5\u5b9e\u73b0\u5177\u6709\u98ce\u683c\u63a7\u5236\u7684\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "InstructVTON\u901a\u8fc7\u6307\u4ee4\u63a7\u5236\uff0c\u7b80\u5316\u4e86\u865a\u62df\u8bd5\u7a7f\u6d41\u7a0b\uff0c\u5e76\u63d0\u9ad8\u4e86\u8bd5\u7a7f\u6548\u679c\u3002"}}
{"id": "2509.20562", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20562", "abs": "https://arxiv.org/abs/2509.20562", "authors": ["Yubin Ge", "Salvatore Romeo", "Jason Cai", "Monica Sunkara", "Yi Zhang"], "title": "SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection", "comment": "Accepted at EMNLP 2025 Main Conference", "summary": "Despite the rapid advancements in LLM agents, they still face the challenge\nof generating meaningful reflections due to inadequate error analysis and a\nreliance on rare successful trajectories, especially in complex tasks. In this\nwork, we propose SAMULE, a new framework for self-learning agents powered by a\nretrospective language model that is trained based on Multi-Level Reflection\nSynthesis. It first synthesizes high-quality reflections across three\ncomplementary levels: Single-Trajectory Learning (micro-level) for detailed\nerror correction; Intra-Task Learning (meso-level) to build error taxonomies\nacross multiple trials of the same task, and Inter-Task Learning (macro-level)\nto extract transferable insights based on same typed errors from diverse task\nfailures. Then we fine-tune a language model serving as the retrospective model\nto generate reflections during inference. We further extend our framework to\ninteractive settings through a foresight-based reflection mechanism, enabling\nagents to proactively reflect and adapt during user interactions by comparing\npredicted and actual responses. Extensive experiments on three challenging\nbenchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our\napproach significantly outperforms reflection-based baselines. Our results\nhighlight the critical role of well-designed reflection synthesis and\nfailure-centric learning in building self-improving LLM agents.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAMULE\u7684\u81ea\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3LLM Agent\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u751f\u6210\u6709\u610f\u4e49\u7684\u53cd\u601d\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684LLM Agent\u7531\u4e8e\u9519\u8bef\u5206\u6790\u4e0d\u8db3\u548c\u4f9d\u8d56\u7f55\u89c1\u7684\u6210\u529f\u8f68\u8ff9\uff0c\u96be\u4ee5\u751f\u6210\u6709\u610f\u4e49\u7684\u53cd\u601d\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u3002", "method": "\u8be5\u6846\u67b6\u57fa\u4e8e\u591a\u5c42\u6b21\u53cd\u601d\u5408\u6210\u8bad\u7ec3\u56de\u987e\u6027\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u62ec\u5355\u8f68\u8ff9\u5b66\u4e60\uff08\u5fae\u89c2\u5c42\u9762\uff09\u3001\u4efb\u52a1\u5185\u5b66\u4e60\uff08\u4e2d\u89c2\u5c42\u9762\uff09\u548c\u4efb\u52a1\u95f4\u5b66\u4e60\uff08\u5b8f\u89c2\u5c42\u9762\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u57fa\u4e8e\u524d\u77bb\u7684\u53cd\u601d\u673a\u5236\u6269\u5c55\u5230\u4ea4\u4e92\u5f0f\u8bbe\u7f6e\u3002", "result": "\u5728TravelPlanner\u3001NATURAL PLAN\u548cTau-bench\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSAMULE\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u53cd\u601d\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u53cd\u601d\u5408\u6210\u548c\u4ee5\u5931\u8d25\u4e3a\u4e2d\u5fc3\u7684\u5b66\u4e60\u5728\u6784\u5efa\u81ea\u6211\u5b8c\u5584\u7684LLM Agent\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2509.20501", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20501", "abs": "https://arxiv.org/abs/2509.20501", "authors": ["Kishor Datta Gupta", "Mohd Ariful Haque", "Marufa Kamal", "Ahmed Rafi Hasan", "Md. Mahfuzur Rahman", "Roy George"], "title": "Beyond Visual Similarity: Rule-Guided Multimodal Clustering with explicit domain rules", "comment": "12 pages, 9 figures", "summary": "Traditional clustering techniques often rely solely on similarity in the\ninput data, limiting their ability to capture structural or semantic\nconstraints that are critical in many domains. We introduce the Domain Aware\nRule Triggered Variational Autoencoder (DARTVAE), a rule guided multimodal\nclustering framework that incorporates domain specific constraints directly\ninto the representation learning process. DARTVAE extends the VAE architecture\nby embedding explicit rules, semantic representations, and data driven features\ninto a unified latent space, while enforcing constraint compliance through rule\nconsistency and violation penalties in the loss function. Unlike conventional\nclustering methods that rely only on visual similarity or apply rules as post\nhoc filters, DARTVAE treats rules as first class learning signals. The rules\nare generated by LLMs, structured into knowledge graphs, and enforced through a\nloss function combining reconstruction, KL divergence, consistency, and\nviolation penalties. Experiments on aircraft and automotive datasets\ndemonstrate that rule guided clustering produces more operationally meaningful\nand interpretable clusters for example, isolating UAVs, unifying stealth\naircraft, or separating SUVs from sedans while improving traditional clustering\nmetrics. However, the framework faces challenges: LLM generated rules may\nhallucinate or conflict, excessive rules risk overfitting, and scaling to\ncomplex domains increases computational and consistency difficulties. By\ncombining rule encodings with learned representations, DARTVAE achieves more\nmeaningful and consistent clustering outcomes than purely data driven models,\nhighlighting the utility of constraint guided multimodal clustering for\ncomplex, knowledge intensive settings.", "AI": {"tldr": "DARTVAE: A rule-guided multimodal clustering framework incorporating domain-specific constraints into representation learning.", "motivation": "Traditional clustering techniques lack the ability to capture structural or semantic constraints.", "method": "Extends VAE architecture by embedding explicit rules, semantic representations, and data-driven features into a unified latent space, enforcing constraint compliance through rule consistency and violation penalties in the loss function. Rules are generated by LLMs and structured into knowledge graphs.", "result": "Rule-guided clustering produces more operationally meaningful and interpretable clusters, improving traditional clustering metrics on aircraft and automotive datasets.", "conclusion": "DARTVAE achieves more meaningful and consistent clustering outcomes than purely data-driven models, highlighting the utility of constraint-guided multimodal clustering for complex, knowledge-intensive settings. Challenges include LLM hallucination, rule conflicts, overfitting, and scaling difficulties."}}
{"id": "2509.20581", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20581", "abs": "https://arxiv.org/abs/2509.20581", "authors": ["Ayan Sar", "Sampurna Roy", "Kanav Gupta", "Anurag Kaushish", "Tanupriya Choudhury", "Abhijit Kumar"], "title": "Hierarchical Resolution Transformers: A Wavelet-Inspired Architecture for Multi-Scale Language Understanding", "comment": "Submitted in IEEE International Conference on Big Data 2025", "summary": "Transformer architectures have achieved state-of-the-art performance across\nnatural language tasks, yet they fundamentally misrepresent the hierarchical\nnature of human language by processing text as flat token sequences. This\nresults in quadratic computational cost, weak computational cost, weak\ncompositional generalization, and inadequate discourse-level modeling. We\npropose Hierarchical Resolution Transformer (HRT), a novel wavelet-inspired\nneural architecture that processes language simultaneously across multiple\nresolutions, from characters to discourse-level units. HRT constructs a\nmulti-resolution attention, enabling bottom-up composition and top-down\ncontextualization. By employing exponential sequence reduction across scales,\nHRT achieves O(nlogn) complexity, offering significant efficiency improvements\nover standard transformers. We evaluated HRT on a diverse suite of benchmarks,\nincluding GLUE, SuperGLUE, Long Range Arena, and WikiText-103, and results\ndemonstrated that HRT outperforms standard transformer baselines by an average\nof +3.8% on GLUE, +4.5% on SuperGLUE, and +6.1% on Long Range Arena, while\nreducing memory usage by 42% and inference latency by 37% compared to BERT and\nGPT style models of similar parameter count. Ablation studies confirm the\neffectiveness of cross-resolution attention and scale-specialized modules,\nshowing that each contributes independently to both efficiency and accuracy.\nOur findings establish HRT as the first architecture to align computational\nstructure with the hierarchical organization of human language, demonstrating\nthat multi-scale, wavelet-inspired processing yields both theoretical\nefficiency gains and practical improvements in language understanding.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5206\u5c42\u5206\u8fa8\u7387Transformer\uff08HRT\uff09\uff0c\u4e00\u79cd\u53d7\u5c0f\u6ce2\u542f\u53d1\u7684\u795e\u7ecf\u67b6\u6784\uff0c\u53ef\u4ee5\u5728\u591a\u4e2a\u5206\u8fa8\u7387\u4e0a\u540c\u65f6\u5904\u7406\u8bed\u8a00\u3002", "motivation": "Transformer\u67b6\u6784\u5728\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u4ece\u6839\u672c\u4e0a\u9519\u8bef\u5730\u8868\u793a\u4e86\u4eba\u7c7b\u8bed\u8a00\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u5c06\u6587\u672c\u5904\u7406\u4e3a\u6241\u5e73\u7684token\u5e8f\u5217\uff0c\u5bfc\u81f4\u4e8c\u6b21\u8ba1\u7b97\u6210\u672c\u3001\u5f31\u7ec4\u5408\u6cdb\u5316\u548c\u4e0d\u5145\u5206\u7684\u7bc7\u7ae0\u7ea7\u5efa\u6a21\u3002", "method": "HRT\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u5206\u8fa8\u7387\u6ce8\u610f\u529b\uff0c\u5b9e\u73b0\u4e86\u81ea\u4e0b\u800c\u4e0a\u7684\u7ec4\u5408\u548c\u81ea\u4e0a\u800c\u4e0b\u7684\u8bed\u5883\u5316\u3002\u901a\u8fc7\u5728\u4e0d\u540c\u5c3a\u5ea6\u4e0a\u91c7\u7528\u6307\u6570\u5e8f\u5217\u7f29\u51cf\uff0cHRT\u5b9e\u73b0\u4e86O(nlogn)\u7684\u590d\u6742\u5ea6\u3002", "result": "\u5728GLUE\u3001SuperGLUE\u3001Long Range Arena\u548cWikiText-103\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHRT\u7684\u6027\u80fd\u4f18\u4e8e\u6807\u51c6Transformer\u57fa\u7ebf\uff0c\u5728GLUE\u4e0a\u5e73\u5747\u63d0\u9ad8+3.8%\uff0c\u5728SuperGLUE\u4e0a\u63d0\u9ad8+4.5%\uff0c\u5728Long Range Arena\u4e0a\u63d0\u9ad8+6.1%\uff0c\u540c\u65f6\u4e0e\u7c7b\u4f3c\u53c2\u6570\u8ba1\u6570\u7684BERT\u548cGPT\u98ce\u683c\u6a21\u578b\u76f8\u6bd4\uff0c\u5185\u5b58\u4f7f\u7528\u91cf\u51cf\u5c11\u4e8642%\uff0c\u63a8\u7406\u5ef6\u8fdf\u51cf\u5c11\u4e8637%\u3002", "conclusion": "HRT\u662f\u7b2c\u4e00\u4e2a\u5c06\u8ba1\u7b97\u7ed3\u6784\u4e0e\u4eba\u7c7b\u8bed\u8a00\u7684\u5c42\u6b21\u7ec4\u7ec7\u5bf9\u9f50\u7684\u67b6\u6784\uff0c\u8bc1\u660e\u4e86\u591a\u5c3a\u5ea6\u3001\u53d7\u5c0f\u6ce2\u542f\u53d1\u7684\u5904\u7406\u53ef\u4ee5\u4ea7\u751f\u7406\u8bba\u4e0a\u7684\u6548\u7387\u63d0\u5347\u548c\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u7684\u5b9e\u9645\u6539\u8fdb\u3002"}}
{"id": "2509.20461", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20461", "abs": "https://arxiv.org/abs/2509.20461", "authors": ["Bruce Kuwahara", "Chen-Yuan Lin", "Xiao Shi Huang", "Kin Kwan Leung", "Jullian Arta Yapeter", "Ilya Stanevich", "Felipe Perez", "Jesse C. Cresswell"], "title": "Document Summarization with Conformal Importance Guarantees", "comment": "NeurIPS 2025. Code is available at\n  https://github.com/layer6ai-labs/conformal-importance-summarization", "summary": "Automatic summarization systems have advanced rapidly with large language\nmodels (LLMs), yet they still lack reliable guarantees on inclusion of critical\ncontent in high-stakes domains like healthcare, law, and finance. In this work,\nwe introduce Conformal Importance Summarization, the first framework for\nimportance-preserving summary generation which uses conformal prediction to\nprovide rigorous, distribution-free coverage guarantees. By calibrating\nthresholds on sentence-level importance scores, we enable extractive document\nsummarization with user-specified coverage and recall rates over critical\ncontent. Our method is model-agnostic, requires only a small calibration set,\nand seamlessly integrates with existing black-box LLMs. Experiments on\nestablished summarization benchmarks demonstrate that Conformal Importance\nSummarization achieves the theoretically assured information coverage rate. Our\nwork suggests that Conformal Importance Summarization can be combined with\nexisting techniques to achieve reliable, controllable automatic summarization,\npaving the way for safer deployment of AI summarization tools in critical\napplications. Code is available at\nhttps://github.com/layer6ai-labs/conformal-importance-summarization.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a Conformal Importance Summarization \u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u4fdd\u7559\u91cd\u8981\u6027\u7684\u6458\u8981\uff0c\u8be5\u6846\u67b6\u4f7f\u7528 conformal prediction \u6765\u63d0\u4f9b\u4e25\u683c\u7684\u3001\u65e0\u5206\u5e03\u7684\u8986\u76d6\u4fdd\u8bc1\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u6458\u8981\u7cfb\u7edf\uff0c\u5c24\u5176\u662f\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7cfb\u7edf\uff0c\u5728\u533b\u7597\u4fdd\u5065\u3001\u6cd5\u5f8b\u548c\u91d1\u878d\u7b49\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u7f3a\u4e4f\u5bf9\u5173\u952e\u5185\u5bb9\u5305\u542b\u7684\u53ef\u9760\u4fdd\u8bc1\u3002", "method": "\u901a\u8fc7\u6821\u51c6\u53e5\u5b50\u7ea7\u522b\u91cd\u8981\u6027\u5206\u6570\u7684\u9608\u503c\uff0c\u5b9e\u73b0\u53ef\u63d0\u53d6\u6587\u6863\u6458\u8981\uff0c\u5e76\u5177\u6709\u7528\u6237\u6307\u5b9a\u7684\u8986\u76d6\u7387\u548c\u5173\u952e\u5185\u5bb9\u53ec\u56de\u7387\u3002\u8be5\u65b9\u6cd5\u662f\u6a21\u578b\u65e0\u5173\u7684\uff0c\u53ea\u9700\u8981\u4e00\u4e2a\u5c0f\u7684\u6821\u51c6\u96c6\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u73b0\u6709\u7684\u9ed1\u76d2 LLM \u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728\u5df2\u5efa\u7acb\u7684\u6458\u8981\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cConformal Importance Summarization \u5b9e\u73b0\u4e86\u7406\u8bba\u4e0a\u4fdd\u8bc1\u7684\u4fe1\u606f\u8986\u76d6\u7387\u3002", "conclusion": "Conformal Importance Summarization \u53ef\u4ee5\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u4ee5\u5b9e\u73b0\u53ef\u9760\u3001\u53ef\u63a7\u7684\u81ea\u52a8\u6458\u8981\uff0c\u4e3a\u5728\u5173\u952e\u5e94\u7528\u4e2d\u66f4\u5b89\u5168\u5730\u90e8\u7f72 AI \u6458\u8981\u5de5\u5177\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2509.20537", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20537", "abs": "https://arxiv.org/abs/2509.20537", "authors": ["Dana A Abdullah", "Dana Rasul Hamad", "Bishar Rasheed Ibrahim", "Sirwan Abdulwahid Aula", "Aso Khaleel Ameen", "Sabat Salih Hamadamin"], "title": "Innovative Deep Learning Architecture for Enhanced Altered Fingerprint Recognition", "comment": null, "summary": "Altered fingerprint recognition (AFR) is challenging for biometric\nverification in applications such as border control, forensics, and fiscal\nadmission. Adversaries can deliberately modify ridge patterns to evade\ndetection, so robust recognition of altered prints is essential. We present\nDeepAFRNet, a deep learning recognition model that matches and recognizes\ndistorted fingerprint samples. The approach uses a VGG16 backbone to extract\nhigh-dimensional features and cosine similarity to compare embeddings. We\nevaluate on the SOCOFing Real-Altered subset with three difficulty levels\n(Easy, Medium, Hard). With strict thresholds, DeepAFRNet achieves accuracies of\n96.7 percent, 98.76 percent, and 99.54 percent for the three levels. A\nthreshold-sensitivity study shows that relaxing the threshold from 0.92 to 0.72\nsharply degrades accuracy to 7.86 percent, 27.05 percent, and 29.51 percent,\nunderscoring the importance of threshold selection in biometric systems. By\nusing real altered samples and reporting per-level metrics, DeepAFRNet\naddresses limitations of prior work based on synthetic alterations or limited\nverification protocols, and indicates readiness for real-world deployments\nwhere both security and recognition resilience are critical.", "AI": {"tldr": "DeepAFRNet\u662f\u4e00\u79cd\u7528\u4e8e\u8bc6\u522b\u626d\u66f2\u6307\u7eb9\u6837\u672c\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5b83\u4f7f\u7528VGG16\u63d0\u53d6\u7279\u5f81\u5e76\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8fdb\u884c\u6bd4\u8f83\u3002", "motivation": "\u5728\u8fb9\u5883\u63a7\u5236\u3001\u53d6\u8bc1\u548c\u8d22\u653f\u8bb8\u53ef\u7b49\u5e94\u7528\u4e2d\uff0c\u6539\u53d8\u7684\u6307\u7eb9\u8bc6\u522b(AFR)\u5177\u6709\u6311\u6218\u6027\u3002\u5bf9\u6297\u8005\u53ef\u4ee5\u6545\u610f\u4fee\u6539\u810a\u7ebf\u6a21\u5f0f\u4ee5\u9003\u907f\u68c0\u6d4b\uff0c\u56e0\u6b64\u5bf9\u6539\u53d8\u7684\u6307\u7eb9\u8fdb\u884c\u9c81\u68d2\u8bc6\u522b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8be5\u65b9\u6cd5\u4f7f\u7528VGG16\u9aa8\u5e72\u7f51\u7edc\u63d0\u53d6\u9ad8\u7ef4\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6765\u6bd4\u8f83\u5d4c\u5165\u3002", "result": "\u5728\u4e25\u683c\u7684\u9608\u503c\u4e0b\uff0cDeepAFRNet\u5728\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b(\u7b80\u5355\u3001\u4e2d\u7b49\u3001\u56f0\u96be)\u4e0a\u5206\u522b\u5b9e\u73b0\u4e8696.7%\u300198.76%\u548c99.54%\u7684\u51c6\u786e\u7387\u3002\u9608\u503c\u654f\u611f\u6027\u7814\u7a76\u8868\u660e\uff0c\u5c06\u9608\u503c\u4ece0.92\u653e\u5bbd\u52300.72\u4f1a\u4f7f\u51c6\u786e\u7387\u6025\u5267\u4e0b\u964d\u52307.86%\u300127.05%\u548c29.51%\uff0c\u8fd9 \u043f\u043e\u0434\u0447\u0435\u0440\u043a\u0438\u0432\u0430\u0435\u0442 \u4e86\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u4e2d\u9608\u503c\u9009\u62e9\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528\u771f\u5b9e\u7684\u6539\u53d8\u6837\u672c\u5e76\u62a5\u544a\u6bcf\u4e2a\u7ea7\u522b\u7684\u6307\u6807\uff0cDeepAFRNet\u89e3\u51b3\u4e86\u57fa\u4e8e\u5408\u6210\u6539\u53d8\u6216\u6709\u9650\u9a8c\u8bc1\u534f\u8bae\u7684\u5148\u524d\u5de5\u4f5c\u7684\u5c40\u9650\u6027\uff0c\u5e76\u8868\u660e\u5df2\u51c6\u5907\u597d\u5728\u5b89\u5168\u6027\u548c\u8bc6\u522b\u5f39\u6027\u90fd\u81f3\u5173\u91cd\u8981\u7684\u5b9e\u9645\u90e8\u7f72\u4e2d\u5e94\u7528\u3002"}}
{"id": "2509.20640", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20640", "abs": "https://arxiv.org/abs/2509.20640", "authors": ["Oluwakemi T. Olayinka", "Sumeet Jeswani", "Divine Iloh"], "title": "Adaptive Cybersecurity Architecture for Digital Product Ecosystems Using Agentic AI", "comment": null, "summary": "Traditional static cybersecurity models often struggle with scalability,\nreal-time detection, and contextual responsiveness in the current digital\nproduct ecosystems which include cloud services, application programming\ninterfaces (APIs), mobile platforms, and edge devices. This study introduces\nautonomous goal driven agents capable of dynamic learning and context-aware\ndecision making as part of an adaptive cybersecurity architecture driven by\nagentic artificial intelligence (AI). To facilitate autonomous threat\nmitigation, proactive policy enforcement, and real-time anomaly detection, this\nframework integrates agentic AI across the key ecosystem layers. Behavioral\nbaselining, decentralized risk scoring, and federated threat intelligence\nsharing are important features. The capacity of the system to identify zero-day\nattacks and dynamically modify access policies was demonstrated through native\ncloud simulations. The evaluation results show increased adaptability,\ndecreased response latency, and improved detection accuracy. The architecture\nprovides an intelligent and scalable blueprint for safeguarding complex digital\ninfrastructure and is compatible with zero-trust models, thereby supporting the\nadherence to international cybersecurity regulations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eagentic AI\u7684\u81ea\u9002\u5e94\u7f51\u7edc\u5b89\u5168\u67b6\u6784\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u9759\u6001\u6a21\u578b\u5728\u53ef\u6269\u5c55\u6027\u3001\u5b9e\u65f6\u68c0\u6d4b\u548c\u4e0a\u4e0b\u6587\u54cd\u5e94\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u7684\u9759\u6001\u7f51\u7edc\u5b89\u5168\u6a21\u578b\u96be\u4ee5\u9002\u5e94\u5f53\u524d\u6570\u5b57\u4ea7\u54c1\u751f\u6001\u7cfb\u7edf\uff08\u5305\u62ec\u4e91\u670d\u52a1\u3001API\u3001\u79fb\u52a8\u5e73\u53f0\u548c\u8fb9\u7f18\u8bbe\u5907\uff09\u7684\u53ef\u6269\u5c55\u6027\u3001\u5b9e\u65f6\u68c0\u6d4b\u548c\u4e0a\u4e0b\u6587\u54cd\u5e94\u9700\u6c42\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u7531\u81ea\u4e3b\u76ee\u6807\u9a71\u52a8\u7684agent\uff0c\u8be5agent\u80fd\u591f\u8fdb\u884c\u52a8\u6001\u5b66\u4e60\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u51b3\u7b56\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u81ea\u9002\u5e94\u7f51\u7edc\u5b89\u5168\u67b6\u6784\u7684\u5173\u952e\u751f\u6001\u5c42\u4e2d\uff0c\u5b9e\u73b0\u81ea\u4e3b\u5a01\u80c1\u7f13\u89e3\u3001\u4e3b\u52a8\u7b56\u7565\u6267\u884c\u548c\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\u3002\u8be5\u67b6\u6784\u5177\u6709\u884c\u4e3a\u57fa\u7ebf\u3001\u53bb\u4e2d\u5fc3\u5316\u98ce\u9669\u8bc4\u5206\u548c\u8054\u90a6\u5a01\u80c1\u60c5\u62a5\u5171\u4eab\u7b49\u91cd\u8981\u529f\u80fd\u3002", "result": "\u901a\u8fc7\u539f\u751f\u4e91\u6a21\u62df\uff0c\u7cfb\u7edf\u5c55\u793a\u4e86\u8bc6\u522b\u96f6\u65e5\u653b\u51fb\u548c\u52a8\u6001\u4fee\u6539\u8bbf\u95ee\u7b56\u7565\u7684\u80fd\u529b\u3002\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u67b6\u6784\u63d0\u9ad8\u4e86\u9002\u5e94\u6027\uff0c\u964d\u4f4e\u4e86\u54cd\u5e94\u5ef6\u8fdf\uff0c\u5e76\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u4fdd\u62a4\u590d\u6742\u7684\u6570\u5b57\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u4e00\u4e2a\u667a\u80fd\u4e14\u53ef\u6269\u5c55\u7684\u84dd\u56fe\uff0c\u5e76\u4e14\u4e0e\u96f6\u4fe1\u4efb\u6a21\u578b\u517c\u5bb9\uff0c\u4ece\u800c\u652f\u6301\u9075\u5b88\u56fd\u9645\u7f51\u7edc\u5b89\u5168\u6cd5\u89c4\u3002"}}
{"id": "2509.20503", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20503", "abs": "https://arxiv.org/abs/2509.20503", "authors": ["Evgenii Egorov", "Hanno Ackermann", "Markus Nagel", "Hong Cai"], "title": "Myosotis: structured computation for attention like layer", "comment": null, "summary": "Attention layers apply a sequence-to-sequence mapping whose parameters depend\non the pairwise interactions of the input elements. However, without any\nstructural assumptions, memory and compute scale quadratically with the\nsequence length. The two main ways to mitigate this are to introduce sparsity\nby ignoring a sufficient amount of pairwise interactions or to introduce\nrecurrent dependence along them, as SSM does. Although both approaches are\nreasonable, they both have disadvantages. We propose a novel algorithm that\ncombines the advantages of both concepts. Our idea is based on the efficient\ninversion of tree-structured matrices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u7ed3\u5408\u4e86\u7a00\u758f\u6027\u548c\u5faa\u73af\u4f9d\u8d56\u6027\u7684\u4f18\u70b9\uff0c\u4ee5\u89e3\u51b3\u6ce8\u610f\u529b\u5c42\u4e2d\u5185\u5b58\u548c\u8ba1\u7b97\u968f\u5e8f\u5217\u957f\u5ea6\u4e8c\u6b21\u589e\u957f\u7684\u95ee\u9898\u3002", "motivation": "\u6ce8\u610f\u529b\u5c42\u7684\u53c2\u6570\u53d6\u51b3\u4e8e\u8f93\u5165\u5143\u7d20\u7684\u6210\u5bf9\u4ea4\u4e92\uff0c\u4f46\u5982\u679c\u6ca1\u6709\u7ed3\u6784\u5047\u8bbe\uff0c\u5185\u5b58\u548c\u8ba1\u7b97\u4f1a\u968f\u5e8f\u5217\u957f\u5ea6\u4e8c\u6b21\u589e\u957f\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u8981\u4e48\u5f15\u5165\u7a00\u758f\u6027\uff0c\u8981\u4e48\u5f15\u5165\u5faa\u73af\u4f9d\u8d56\u6027\uff0c\u4f46\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u6709\u7f3a\u70b9\u3002", "method": "\u8be5\u7b97\u6cd5\u57fa\u4e8e\u6811\u7ed3\u6784\u77e9\u9635\u7684\u6709\u6548\u53cd\u6f14\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5", "conclusion": "\u8be5\u7b97\u6cd5\u7ed3\u5408\u4e86\u7a00\u758f\u6027\u548c\u5faa\u73af\u4f9d\u8d56\u6027\u7684\u4f18\u70b9\u3002"}}
{"id": "2509.20805", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20805", "abs": "https://arxiv.org/abs/2509.20805", "authors": ["Genki Kusano"], "title": "Few-Shot and Training-Free Review Generation via Conversational Prompting", "comment": null, "summary": "Personalized review generation helps businesses understand user preferences,\nyet most existing approaches assume extensive review histories of the target\nuser or require additional model training. Real-world applications often face\nfew-shot and training-free situations, where only a few user reviews are\navailable and fine-tuning is infeasible. It is well known that large language\nmodels (LLMs) can address such low-resource settings, but their effectiveness\ndepends on prompt engineering. In this paper, we propose Conversational\nPrompting, a lightweight method that reformulates user reviews as multi-turn\nconversations. Its simple variant, Simple Conversational Prompting (SCP),\nrelies solely on the user's own reviews, while the contrastive variant,\nContrastive Conversational Prompting (CCP), inserts reviews from other users or\nLLMs as incorrect replies and then asks the model to correct them, encouraging\nthe model to produce text in the user's style. Experiments on eight product\ndomains and five LLMs showed that the conventional non-conversational prompt\noften produced reviews similar to those written by random users, based on\ntext-based metrics such as ROUGE-L and BERTScore, and application-oriented\ntasks like user identity matching and sentiment analysis. In contrast, both SCP\nand CCP produced reviews much closer to those of the target user, even when\neach user had only two reviews. CCP brings further improvements when\nhigh-quality negative examples are available, whereas SCP remains competitive\nwhen such data cannot be collected. These results suggest that conversational\nprompting offers a practical solution for review generation under few-shot and\ntraining-free constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5bf9\u8bdd\u63d0\u793a\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5c11\u6837\u672c\u548c\u65e0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u4e2a\u6027\u5316\u8bc4\u8bba\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u76ee\u6807\u7528\u6237\u6709\u5927\u91cf\u7684\u8bc4\u8bba\u5386\u53f2\u6216\u9700\u8981\u989d\u5916\u7684\u6a21\u578b\u8bad\u7ec3\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7528\u6237\u8bc4\u8bba\u8f83\u5c11\u4e14\u96be\u4ee5\u8fdb\u884c\u5fae\u8c03\u3002", "method": "\u5c06\u7528\u6237\u8bc4\u8bba\u91cd\u65b0\u6784\u5efa\u4e3a\u591a\u8f6e\u5bf9\u8bdd\uff0c\u5305\u62ec\u7b80\u5355\u5bf9\u8bdd\u63d0\u793a\uff08SCP\uff09\u548c\u5bf9\u6bd4\u5bf9\u8bdd\u63d0\u793a\uff08CCP\uff09\uff0c\u540e\u8005\u901a\u8fc7\u63d2\u5165\u5176\u4ed6\u7528\u6237\u7684\u8bc4\u8bba\u6216LLM\u751f\u6210\u7684\u8bc4\u8bba\u4f5c\u4e3a\u9519\u8bef\u56de\u590d\uff0c\u9f13\u52b1\u6a21\u578b\u4ee5\u7528\u6237\u7684\u98ce\u683c\u751f\u6210\u6587\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f20\u7edf\u7684\u975e\u5bf9\u8bdd\u63d0\u793a\u751f\u6210\u7684\u8bc4\u8bba\u4e0e\u968f\u673a\u7528\u6237\u76f8\u4f3c\uff0c\u800cSCP\u548cCCP\u751f\u6210\u7684\u8bc4\u8bba\u66f4\u63a5\u8fd1\u76ee\u6807\u7528\u6237\uff0c\u5c24\u5176\u662f\u5728\u7528\u6237\u53ea\u6709\u5c11\u91cf\u8bc4\u8bba\u65f6\u3002CCP\u5728\u6709\u9ad8\u8d28\u91cf\u8d1f\u6837\u672c\u65f6\u6548\u679c\u66f4\u597d\uff0c\u800cSCP\u5728\u7f3a\u4e4f\u6b64\u7c7b\u6570\u636e\u65f6\u4ecd\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u5bf9\u8bdd\u63d0\u793a\u4e3a\u5c11\u6837\u672c\u548c\u65e0\u8bad\u7ec3\u7ea6\u675f\u4e0b\u7684\u8bc4\u8bba\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.20467", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20467", "abs": "https://arxiv.org/abs/2509.20467", "authors": ["Henrik Vatndal", "Vinay Setty"], "title": "ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos", "comment": null, "summary": "Short-form video platforms like TikTok present unique challenges for\nmisinformation detection due to their multimodal, dynamic, and noisy content.\nWe present ShortCheck, a modular, inference-only pipeline with a user-friendly\ninterface that automatically identifies checkworthy short-form videos to help\nhuman fact-checkers. The system integrates speech transcription, OCR, object\nand deepfake detection, video-to-text summarization, and claim verification.\nShortCheck is validated by evaluating it on two manually annotated datasets\nwith TikTok videos in a multilingual setting. The pipeline achieves promising\nresults with F1-weighted score over 70\\%.", "AI": {"tldr": "ShortCheck\u662f\u4e00\u4e2a\u81ea\u52a8\u8bc6\u522b\u503c\u5f97\u4eba\u5de5\u5ba1\u6838\u7684\u77ed\u89c6\u9891\u7684pipeline\uff0c\u96c6\u6210\u4e86\u8bed\u97f3\u8f6c\u5f55\u3001OCR\u3001\u7269\u4f53\u548cdeepfake\u68c0\u6d4b\u3001\u89c6\u9891\u6587\u672c\u6458\u8981\u548c\u58f0\u660e\u9a8c\u8bc1\u3002", "motivation": "\u77ed\u89c6\u9891\u5e73\u53f0\u4e0a\u7684\u9519\u8bef\u4fe1\u606f\u68c0\u6d4b\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5185\u5bb9\u662f\u591a\u6a21\u6001\u3001\u52a8\u6001\u548c\u5608\u6742\u7684\u3002", "method": "ShortCheck\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u3001\u4ec5\u63a8\u7406\u7684pipeline\uff0c\u5177\u6709\u7528\u6237\u53cb\u597d\u7684\u754c\u9762\u3002", "result": "\u8be5pipeline\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684TikTok\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0cF1-weighted score\u8d85\u8fc770%\u3002", "conclusion": "ShortCheck\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002"}}
{"id": "2509.20579", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20579", "abs": "https://arxiv.org/abs/2509.20579", "authors": ["Hanna Yurchyk", "Wei-Di Chang", "Gregory Dudek", "David Meger"], "title": "Large Pre-Trained Models for Bimanual Manipulation in 3D", "comment": "Accepted to 2025 IEEE-RAS 24th International Conference on Humanoid\n  Robots", "summary": "We investigate the integration of attention maps from a pre-trained Vision\nTransformer into voxel representations to enhance bimanual robotic\nmanipulation. Specifically, we extract attention maps from DINOv2, a\nself-supervised ViT model, and interpret them as pixel-level saliency scores\nover RGB images. These maps are lifted into a 3D voxel grid, resulting in\nvoxel-level semantic cues that are incorporated into a behavior cloning policy.\nWhen integrated into a state-of-the-art voxel-based policy, our\nattention-guided featurization yields an average absolute improvement of 8.2%\nand a relative gain of 21.9% across all tasks in the RLBench bimanual\nbenchmark.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5c06\u9884\u8bad\u7ec3 Vision Transformer \u7684\u6ce8\u610f\u529b\u56fe\u6574\u5408\u5230\u4f53\u7d20\u8868\u793a\u4e2d\uff0c\u4ee5\u589e\u5f3a\u53cc\u81c2\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "motivation": "\u5229\u7528 DINOv2 \u7684\u6ce8\u610f\u529b\u56fe\u4f5c\u4e3a\u50cf\u7d20\u7ea7\u663e\u8457\u6027\u5f97\u5206\uff0c\u63d0\u5347\u53cc\u81c2\u64cd\u4f5c\u6027\u80fd\u3002", "method": "\u63d0\u53d6 DINOv2 \u7684\u6ce8\u610f\u529b\u56fe\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u4f53\u7d20\u7f51\u683c\u4e2d\u7684\u8bed\u4e49\u7ebf\u7d22\uff0c\u5e76\u6574\u5408\u5230\u884c\u4e3a\u514b\u9686\u7b56\u7565\u4e2d\u3002", "result": "\u5728 RLBench \u53cc\u81c2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u7279\u5f81\u5316\u65b9\u6cd5\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u7684\u5e73\u5747\u7edd\u5bf9\u6539\u8fdb\u4e3a 8.2%\uff0c\u76f8\u5bf9\u589e\u76ca\u4e3a 21.9%\u3002", "conclusion": "\u5c06\u9884\u8bad\u7ec3 ViT \u7684\u6ce8\u610f\u529b\u56fe\u878d\u5165\u4f53\u7d20\u8868\u793a\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u53cc\u81c2\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u3002"}}
{"id": "2509.20652", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20652", "abs": "https://arxiv.org/abs/2509.20652", "authors": ["Po-Yu Liang", "Yong Zhang", "Tatiana Hwa", "Aaron Byers"], "title": "Accelerate Creation of Product Claims Using Generative AI", "comment": "This paper has been accepted at the GenProCC workshop (NeurIPS 2025)", "summary": "The benefit claims of a product is a critical driver of consumers' purchase\nbehavior. Creating product claims is an intense task that requires substantial\ntime and funding. We have developed the $\\textbf{Claim Advisor}$ web\napplication to accelerate claim creations using in-context learning and\nfine-tuning of large language models (LLM). $\\textbf{Claim Advisor}$ was\ndesigned to disrupt the speed and economics of claim search, generation,\noptimization, and simulation. It has three functions: (1) semantically\nsearching and identifying existing claims and/or visuals that resonate with the\nvoice of consumers; (2) generating and/or optimizing claims based on a product\ndescription and a consumer profile; and (3) ranking generated and/or manually\ncreated claims using simulations via synthetic consumers. Applications in a\nconsumer packaged goods (CPG) company have shown very promising results. We\nbelieve that this capability is broadly useful and applicable across product\ncategories and industries. We share our learning to encourage the research and\napplication of generative AI in different industries.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3a Claim Advisor \u7684\u7f51\u7edc\u5e94\u7528\u7a0b\u5e8f\uff0c\u65e8\u5728\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u52a0\u901f\u4ea7\u54c1\u58f0\u660e\u7684\u521b\u5efa\u8fc7\u7a0b\u3002", "motivation": "\u4ea7\u54c1\u58f0\u660e\u662f\u5f71\u54cd\u6d88\u8d39\u8005\u8d2d\u4e70\u884c\u4e3a\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4f46\u521b\u5efa\u4ea7\u54c1\u58f0\u660e\u9700\u8981\u5927\u91cf\u7684\u65f6\u95f4\u548c\u8d44\u91d1\u6295\u5165\u3002", "method": "\u8be5\u5e94\u7528\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5fae\u8c03\u6765\u5b9e\u73b0\uff0c\u5305\u542b\u8bed\u4e49\u641c\u7d22\u3001\u58f0\u660e\u751f\u6210\u4e0e\u4f18\u5316\u3001\u4ee5\u53ca\u4f7f\u7528\u5408\u6210\u6d88\u8d39\u8005\u8fdb\u884c\u6a21\u62df\u6392\u5e8f\u4e09\u4e2a\u529f\u80fd\u3002", "result": "\u5728\u4e00\u5bb6\u6d88\u8d39\u54c1\u516c\u53f8\u4e2d\u7684\u5e94\u7528\u663e\u793a\u51fa\u975e\u5e38\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002", "conclusion": "\u8fd9\u9879\u6280\u672f\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\uff0c\u6211\u4eec\u5206\u4eab\u7814\u7a76\u6210\u679c\u4ee5\u9f13\u52b1\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u4e0d\u540c\u884c\u4e1a\u4e2d\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2509.20507", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20507", "abs": "https://arxiv.org/abs/2509.20507", "authors": ["Liya Gaynutdinova", "Petr Havl\u00e1sek", "Ond\u0159ej Roko\u0161", "Fleur Hendriks", "Martin Do\u0161k\u00e1\u0159"], "title": "Auto-Regressive U-Net for Full-Field Prediction of Shrinkage-Induced Damage in Concrete", "comment": null, "summary": "This paper introduces a deep learning approach for predicting time-dependent\nfull-field damage in concrete. The study uses an auto-regressive U-Net model to\npredict the evolution of the scalar damage field in a unit cell given\nmicrostructural geometry and evolution of an imposed shrinkage profile. By\nsequentially using the predicted damage output as input for subsequent\npredictions, the model facilitates the continuous assessment of damage\nprogression. Complementarily, a convolutional neural network (CNN) utilises the\ndamage estimations to forecast key mechanical properties, including observed\nshrinkage and residual stiffness. The proposed dual-network architecture\ndemonstrates high computational efficiency and robust predictive performance on\nthe synthesised datasets. The approach reduces the computational load\ntraditionally associated with full-field damage evaluations and is used to gain\ninsights into the relationship between aggregate properties, such as shape,\nsize, and distribution, and the effective shrinkage and reduction in stiffness.\nUltimately, this can help to optimize concrete mix designs, leading to improved\ndurability and reduced internal damage.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u6df7\u51dd\u571f\u4e2d\u968f\u65f6\u95f4\u53d8\u5316\u7684\u5b8c\u6574\u635f\u4f24\u573a\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u9884\u6d4b\u6df7\u51dd\u571f\u4e2d\u968f\u65f6\u95f4\u53d8\u5316\u7684\u5b8c\u6574\u635f\u4f24\u573a\uff0c\u5e76\u4f18\u5316\u6df7\u51dd\u571f\u7684\u6df7\u5408\u7269\u8bbe\u8ba1\uff0c\u4ece\u800c\u63d0\u9ad8\u8010\u4e45\u6027\u5e76\u51cf\u5c11\u5185\u90e8\u635f\u4f24\u3002", "method": "\u4f7f\u7528\u81ea\u56de\u5f52 U-Net \u6a21\u578b\u9884\u6d4b\u5355\u5143\u683c\u4e2d\u6807\u91cf\u635f\u4f24\u573a\u7684\u6f14\u53d8\uff0c\u5e76\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u5229\u7528\u635f\u4f24\u4f30\u8ba1\u6765\u9884\u6d4b\u5173\u952e\u7684\u673a\u68b0\u6027\u80fd\uff0c\u5305\u62ec\u89c2\u6d4b\u5230\u7684\u6536\u7f29\u7387\u548c\u6b8b\u4f59\u521a\u5ea6\u3002", "result": "\u6240\u63d0\u51fa\u7684\u53cc\u7f51\u7edc\u67b6\u6784\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f88\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u548c\u5f3a\u5927\u7684\u9884\u6d4b\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u4e0e\u5168\u573a\u635f\u4f24\u8bc4\u4f30\u76f8\u5173\u7684\u8ba1\u7b97\u8d1f\u8377\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5e2e\u52a9\u4f18\u5316\u6df7\u51dd\u571f\u6df7\u5408\u7269\u8bbe\u8ba1\uff0c\u4ece\u800c\u63d0\u9ad8\u8010\u4e45\u6027\u5e76\u51cf\u5c11\u5185\u90e8\u635f\u4f24\u3002"}}
{"id": "2509.21106", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.21106", "abs": "https://arxiv.org/abs/2509.21106", "authors": ["Hyunseo Kim", "Sangam Lee", "Kwangwook Seo", "Dongha Lee"], "title": "BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback", "comment": "Work in progress", "summary": "Search-augmented large language models (LLMs) have advanced\ninformation-seeking tasks by integrating retrieval into generation, reducing\nusers' cognitive burden compared to traditional search systems. Yet they remain\ninsufficient for fully addressing diverse user needs, which requires\nrecognizing how the same query can reflect different intents across users and\ndelivering information in preferred forms. While recent systems such as ChatGPT\nand Gemini attempt personalization by leveraging user histories, systematic\nevaluation of such personalization is under-explored. To address this gap, we\npropose BESPOKE, the realistic benchmark for evaluating personalization in\nsearch-augmented LLMs. BESPOKE is designed to be both realistic, by collecting\nauthentic chat and search histories directly from humans, and diagnostic, by\npairing responses with fine-grained preference scores and feedback. The\nbenchmark is constructed through long-term, deeply engaged human annotation,\nwhere human annotators contributed their own histories, authored queries with\ndetailed information needs, and evaluated responses with scores and diagnostic\nfeedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key\nrequirements for effective personalization in information-seeking tasks,\nproviding a foundation for fine-grained evaluation of personalized\nsearch-augmented LLMs. Our code and data are available at\nhttps://augustinlib.github.io/BESPOKE/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86BESPOKE\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u641c\u7d22\u589e\u5f3a\u578bLLM\u4e2d\u4e2a\u6027\u5316\u7684\u73b0\u5b9e\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u7684\u641c\u7d22\u589e\u5f3a\u578bLLM\u5728\u6ee1\u8db3\u4e0d\u540c\u7528\u6237\u9700\u6c42\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u56e0\u4e3a\u5b83\u4eec\u672a\u80fd\u5145\u5206\u8bc6\u522b\u540c\u4e00\u67e5\u8be2\u80cc\u540e\u4e0d\u540c\u7528\u6237\u7684\u610f\u56fe\uff0c\u5e76\u4e14\u4e0d\u80fd\u4ee5\u7528\u6237\u504f\u597d\u7684\u5f62\u5f0f\u4f20\u9012\u4fe1\u606f\u3002\u5bf9ChatGPT\u548cGemini\u7b49\u7cfb\u7edf\u7684\u4e2a\u6027\u5316\u80fd\u529b\u7f3a\u4e4f\u7cfb\u7edf\u7684\u8bc4\u4f30\u3002", "method": "\u672c\u6587\u901a\u8fc7\u6536\u96c6\u6765\u81ea\u4eba\u7c7b\u7684\u771f\u5b9e\u804a\u5929\u548c\u641c\u7d22\u5386\u53f2\u6765\u8bbe\u8ba1BESPOKE\u57fa\u51c6\uff0c\u5e76\u901a\u8fc7\u5c06\u54cd\u5e94\u4e0e\u7ec6\u7c92\u5ea6\u7684\u504f\u597d\u5206\u6570\u548c\u53cd\u9988\u914d\u5bf9\u6765\u5b9e\u73b0\u8bca\u65ad\u6027\u8bc4\u4f30\u3002", "result": "\u672c\u6587\u5229\u7528BESPOKE\u8fdb\u884c\u4e86\u7cfb\u7edf\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5728\u4fe1\u606f\u641c\u5bfb\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6709\u6548\u4e2a\u6027\u5316\u7684\u5173\u952e\u9700\u6c42\uff0c\u4e3a\u4e2a\u6027\u5316\u641c\u7d22\u589e\u5f3a\u578bLLM\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "BESPOKE\u662f\u4e00\u4e2a\u73b0\u5b9e\u7684\u57fa\u51c6\uff0c\u53ef\u4ee5\u6709\u6548\u8bc4\u4f30\u641c\u7d22\u589e\u5f3a\u578bLLM\u4e2d\u7684\u4e2a\u6027\u5316\u80fd\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u65b9\u5411\u3002"}}
{"id": "2509.20502", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20502", "abs": "https://arxiv.org/abs/2509.20502", "authors": ["Xiao Wang", "Jia Wang", "Yijie Wang", "Pengtao Dang", "Sha Cao", "Chi Zhang"], "title": "MARS: toward more efficient multi-agent collaboration for LLM reasoning", "comment": null, "summary": "Large language models (LLMs) have achieved impressive results in natural\nlanguage understanding, yet their reasoning capabilities remain limited when\noperating as single agents. Multi-Agent Debate (MAD) has been proposed to\naddress this limitation by enabling collaborative reasoning among multiple\nmodels in a round-table debate manner. While effective, MAD introduces\nsubstantial computational overhead due to the number of agents involved and the\nfrequent communication required. In this paper, we propose MARS (Multi-Agent\nReview System), a role-based collaboration framework inspired by the review\nprocess. In MARS, an author agent generates an initial solution, reviewer\nagents provide decisions and comments independently, and a meta-reviewer\nintegrates the feedback to make the final decision and guide further revision.\nThis design enhances reasoning quality while avoiding costly\nreviewer-to-reviewer interactions, thereby controlling token consumption and\ninference time. We compared MARS with both MAD and other state-of-the-art\nreasoning strategies across multiple benchmarks. Extensive experiments with\ndifferent LLMs show that MARS matches the accuracy of MAD while reducing both\ntoken usage and inference time by approximately 50\\%. Code is available at\nhttps://github.com/xwang97/MARS.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u540d\u4e3aMARS\uff0c\u65e8\u5728\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff08MAD\uff09\u65b9\u6cd5\u867d\u7136\u6709\u6548\uff0c\u4f46\u7531\u4e8e\u667a\u80fd\u4f53\u6570\u91cf\u4f17\u591a\u548c\u9891\u7e41\u901a\u4fe1\uff0c\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u89d2\u8272\u7684\u534f\u4f5c\u6846\u67b6MARS\uff0c\u5176\u4e2d\u4f5c\u8005\u667a\u80fd\u4f53\u751f\u6210\u521d\u59cb\u89e3\u51b3\u65b9\u6848\uff0c\u8bc4\u5ba1\u8005\u667a\u80fd\u4f53\u72ec\u7acb\u63d0\u4f9b\u51b3\u7b56\u548c\u8bc4\u8bba\uff0c\u5143\u8bc4\u5ba1\u8005\u6574\u5408\u53cd\u9988\u4ee5\u505a\u51fa\u6700\u7ec8\u51b3\u7b56\u5e76\u6307\u5bfc\u8fdb\u4e00\u6b65\u4fee\u8ba2\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMARS\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0eMAD\u7684\u51c6\u786e\u7387\u76f8\u5339\u914d\uff0c\u540c\u65f6token\u4f7f\u7528\u91cf\u548c\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u4e86\u7ea650%\u3002", "conclusion": "MARS\u80fd\u591f\u5728\u63d0\u9ad8\u63a8\u7406\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u6709\u6548\u63a7\u5236token\u6d88\u8017\u548c\u63a8\u7406\u65f6\u95f4\u3002"}}
{"id": "2509.20580", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20580", "abs": "https://arxiv.org/abs/2509.20580", "authors": ["Xinyang Mu", "Yuzhen Lu", "Boyang Deng"], "title": "A Comparative Benchmark of Real-time Detectors for Blueberry Detection towards Precision Orchard Management", "comment": "19 pages, 6 figures, 4 tables. Abstract abridged due to arXiv's 1920\n  character limit", "summary": "Blueberry detection in natural environments remains challenging due to\nvariable lighting, occlusions, and motion blur due to environmental factors and\nimaging devices. Deep learning-based object detectors promise to address these\nchallenges, but they demand a large-scale, diverse dataset that captures the\nreal-world complexities. Moreover, deploying these models in practical\nscenarios often requires the right accuracy/speed/memory trade-off in model\nselection. This study presents a novel comparative benchmark analysis of\nadvanced real-time object detectors, including YOLO (You Only Look Once)\n(v8-v12) and RT-DETR (Real-Time Detection Transformers) (v1-v2) families,\nconsisting of 36 model variants, evaluated on a newly curated dataset for\nblueberry detection. This dataset comprises 661 canopy images collected with\nsmartphones during the 2022-2023 seasons, consisting of 85,879 labelled\ninstances (including 36,256 ripe and 49,623 unripe blueberries) across a wide\nrange of lighting conditions, occlusions, and fruit maturity stages. Among the\nYOLO models, YOLOv12m achieved the best accuracy with a mAP@50 of 93.3%, while\nRT-DETRv2-X obtained a mAP@50 of 93.6%, the highest among all the RT-DETR\nvariants. The inference time varied with the model scale and complexity, and\nthe mid-sized models appeared to offer a good accuracy-speed balance. To\nfurther enhance detection performance, all the models were fine-tuned using\nUnbiased Mean Teacher-based semi-supervised learning (SSL) on a separate set of\n1,035 unlabeled images acquired by a ground-based machine vision platform in\n2024. This resulted in accuracy gains ranging from -1.4% to 2.9%, with\nRT-DETR-v2-X achieving the best mAP@50 of 94.8%. More in-depth research into\nSSL is needed to better leverage cross-domain unlabeled data. Both the dataset\nand software programs of this study are made publicly available to support\nfurther research.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u81ea\u7136\u73af\u5883\u4e0b\u84dd\u8393\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u5305\u542b\u5404\u79cd\u5149\u7167\u3001\u906e\u6321\u548c\u6210\u719f\u9636\u6bb5\u7684\u5927\u89c4\u6a21\u84dd\u8393\u6570\u636e\u96c6\uff0c\u5e76\u5bf9 YOLO \u548c RT-DETR \u7cfb\u5217\u7684 36 \u4e2a\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u548c\u6bd4\u8f83\u5206\u6790\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u7d22\u4e86\u4f7f\u7528\u65e0\u504f Mean Teacher \u534a\u76d1\u7763\u5b66\u4e60 (SSL) \u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5728\u81ea\u7136\u73af\u5883\u4e2d\u68c0\u6d4b\u84dd\u8393\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5b58\u5728\u53ef\u53d8\u7684\u5149\u7167\u3001\u906e\u6321\u548c\u8fd0\u52a8\u6a21\u7cca\u7b49\u95ee\u9898\u3002\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u76ee\u6807\u68c0\u6d4b\u5668\u6709\u6f5c\u529b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u9700\u8981\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u3002\u540c\u65f6\uff0c\u5728\u5b9e\u9645\u90e8\u7f72\u8fd9\u4e9b\u6a21\u578b\u65f6\uff0c\u9700\u8981\u5728\u51c6\u786e\u6027\u3001\u901f\u5ea6\u548c\u5185\u5b58\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002", "method": "1. \u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b 661 \u5f20\u51a0\u5c42\u56fe\u50cf\u7684\u84dd\u8393\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u56fe\u50cf\u662f\u5728 2022-2023 \u5b63\u4f7f\u7528\u667a\u80fd\u624b\u673a\u62cd\u6444\u7684\uff0c\u5305\u542b 85,879 \u4e2a\u6807\u8bb0\u5b9e\u4f8b\u30022. \u5bf9 YOLO (v8-v12) \u548c RT-DETR (v1-v2) \u7cfb\u5217\u7684 36 \u4e2a\u6a21\u578b\u53d8\u4f53\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u548c\u6bd4\u8f83\u5206\u6790\u30023. \u4f7f\u7528\u65e0\u504f Mean Teacher \u534a\u76d1\u7763\u5b66\u4e60 (SSL) \u5bf9\u6240\u6709\u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\u3002", "result": "YOLOv12m \u7684 mAP@50 \u8fbe\u5230 93.3%\uff0cRT-DETRv2-X \u7684 mAP@50 \u8fbe\u5230 93.6%\u3002\u901a\u8fc7\u4f7f\u7528 SSL \u8fdb\u884c\u5fae\u8c03\uff0cRT-DETR-v2-X \u7684 mAP@50 \u8fdb\u4e00\u6b65\u63d0\u9ad8\u5230 94.8%\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u84dd\u8393\u6570\u636e\u96c6\uff0c\u5e76\u5bf9 YOLO \u548c RT-DETR \u7cfb\u5217\u7684\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u5206\u6790\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e2d\u7b49\u89c4\u6a21\u7684\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u901f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7 SSL \u5fae\u8c03\u8fdb\u4e00\u6b65\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.20707", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20707", "abs": "https://arxiv.org/abs/2509.20707", "authors": ["Junjie Cui", "Peilong Wang", "Jason Holmes", "Leshan Sun", "Michael L. Hinni", "Barbara A. Pockaj", "Sujay A. Vora", "Terence T. Sio", "William W. Wong", "Nathan Y. Yu", "Steven E. Schild", "Joshua R. Niska", "Sameer R. Keole", "Jean-Claude M. Rwigema", "Samir H. Patel", "Lisa A. McGee", "Carlos A. Vargas", "Wei Liu"], "title": "An Automated Retrieval-Augmented Generation LLaMA-4 109B-based System for Evaluating Radiotherapy Treatment Plans", "comment": "16 pages, 4 figures. Submitted to npj Digital Medicine", "summary": "Purpose: To develop a retrieval-augmented generation (RAG) system powered by\nLLaMA-4 109B for automated, protocol-aware, and interpretable evaluation of\nradiotherapy treatment plans.\n  Methods and Materials: We curated a multi-protocol dataset of 614\nradiotherapy plans across four disease sites and constructed a knowledge base\ncontaining normalized dose metrics and protocol-defined constraints. The RAG\nsystem integrates three core modules: a retrieval engine optimized across five\nSentenceTransformer backbones, a percentile prediction component based on\ncohort similarity, and a clinical constraint checker. These tools are directed\nby a large language model (LLM) using a multi-step prompt-driven reasoning\npipeline to produce concise, grounded evaluations.\n  Results: Retrieval hyperparameters were optimized using Gaussian Process on a\nscalarized loss function combining root mean squared error (RMSE), mean\nabsolute error (MAE), and clinically motivated accuracy thresholds. The best\nconfiguration, based on all-MiniLM-L6-v2, achieved perfect nearest-neighbor\naccuracy within a 5-percentile-point margin and a sub-2pt MAE. When tested\nend-to-end, the RAG system achieved 100% agreement with the computed values by\nstandalone retrieval and constraint-checking modules on both percentile\nestimates and constraint identification, confirming reliable execution of all\nretrieval, prediction and checking steps.\n  Conclusion: Our findings highlight the feasibility of combining structured\npopulation-based scoring with modular tool-augmented reasoning for transparent,\nscalable plan evaluation in radiation therapy. The system offers traceable\noutputs, minimizes hallucination, and demonstrates robustness across protocols.\nFuture directions include clinician-led validation, and improved domain-adapted\nretrieval models to enhance real-world integration.", "AI": {"tldr": "\u5f00\u53d1\u4e00\u4e2a\u57fa\u4e8eLLaMA-4 109B\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u3001\u534f\u8bae\u611f\u77e5\u548c\u53ef\u89e3\u91ca\u7684\u653e\u5c04\u6cbb\u7597\u8ba1\u5212\u8bc4\u4f30\u3002", "motivation": "\u5728\u653e\u5c04\u6cbb\u7597\u8ba1\u5212\u8bc4\u4f30\u4e2d\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u3001\u534f\u8bae\u611f\u77e5\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u6784\u5efa\u5305\u542b\u6807\u51c6\u5316\u5242\u91cf\u6307\u6807\u548c\u534f\u8bae\u5b9a\u4e49\u7ea6\u675f\u7684\u77e5\u8bc6\u5e93\uff0c\u5e76\u96c6\u6210\u68c0\u7d22\u5f15\u64ce\u3001\u767e\u5206\u4f4d\u9884\u6d4b\u7ec4\u4ef6\u548c\u4e34\u5e8a\u7ea6\u675f\u68c0\u67e5\u5668\uff0c\u901a\u8fc7\u591a\u6b65\u9aa4\u63d0\u793a\u9a71\u52a8\u7684\u63a8\u7406\u6d41\u7a0b\uff0c\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6307\u5bfc\uff0c\u751f\u6210\u7b80\u6d01\u3001\u6709\u6839\u636e\u7684\u8bc4\u4f30\u3002", "result": "RAG\u7cfb\u7edf\u5728\u767e\u5206\u4f4d\u4f30\u8ba1\u548c\u7ea6\u675f\u8bc6\u522b\u65b9\u9762\u4e0e\u72ec\u7acb\u68c0\u7d22\u548c\u7ea6\u675f\u68c0\u67e5\u6a21\u5757\u8ba1\u7b97\u7684\u503c\u8fbe\u6210100%\u7684\u4e00\u81f4\uff0c\u786e\u8ba4\u4e86\u6240\u6709\u68c0\u7d22\u3001\u9884\u6d4b\u548c\u68c0\u67e5\u6b65\u9aa4\u7684\u53ef\u9760\u6267\u884c\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5c06\u57fa\u4e8e\u7ed3\u6784\u5316\u4eba\u7fa4\u7684\u8bc4\u5206\u4e0e\u6a21\u5757\u5316\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u76f8\u7ed3\u5408\uff0c\u53ef\u5b9e\u73b0\u900f\u660e\u3001\u53ef\u6269\u5c55\u7684\u653e\u5c04\u6cbb\u7597\u8ba1\u5212\u8bc4\u4f30\u3002"}}
{"id": "2509.20509", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20509", "abs": "https://arxiv.org/abs/2509.20509", "authors": ["Luca Serfilippi", "Giorgio Franceschelli", "Antonio Corradi", "Mirco Musolesi"], "title": "Complexity-Driven Policy Optimization", "comment": null, "summary": "Policy gradient methods often balance exploitation and exploration via\nentropy maximization. However, maximizing entropy pushes the policy towards a\nuniform random distribution, which represents an unstructured and sometimes\ninefficient exploration strategy. In this work, we propose replacing the\nentropy bonus with a more robust complexity bonus. In particular, we adopt a\nmeasure of complexity, defined as the product of Shannon entropy and\ndisequilibrium, where the latter quantifies the distance from the uniform\ndistribution. This regularizer encourages policies that balance stochasticity\n(high entropy) with structure (high disequilibrium), guiding agents toward\nregimes where useful, non-trivial behaviors can emerge. Such behaviors arise\nbecause the regularizer suppresses both extremes, e.g., maximal disorder and\ncomplete order, creating pressure for agents to discover structured yet\nadaptable strategies. Starting from Proximal Policy Optimization (PPO), we\nintroduce Complexity-Driven Policy Optimization (CDPO), a new learning\nalgorithm that replaces entropy with complexity. We show empirically across a\nrange of discrete action space tasks that CDPO is more robust to the choice of\nthe complexity coefficient than PPO is with the entropy coefficient, especially\nin environments requiring greater exploration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0cCDPO\uff0c\u5b83\u4f7f\u7528\u590d\u6742\u6027\u5956\u52b1\u4ee3\u66ff\u71b5\u5956\u52b1\uff0c\u4ee5\u5e73\u8861\u63a2\u7d22\u548c\u5229\u7528\uff0c\u9f13\u52b1\u667a\u80fd\u4f53\u53d1\u73b0\u7ed3\u6784\u5316\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u7b56\u7565\u3002", "motivation": "\u6700\u5927\u5316\u71b5\u4f1a\u4f7f\u7b56\u7565\u8d8b\u5411\u4e8e\u5747\u5300\u968f\u673a\u5206\u5e03\uff0c\u8fd9\u4ee3\u8868\u7740\u4e00\u79cd\u975e\u7ed3\u6784\u5316\u4e14\u6709\u65f6\u6548\u7387\u4f4e\u4e0b\u7684\u63a2\u7d22\u7b56\u7565\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u7528\u9999\u519c\u71b5\u548c\u4e0d\u5e73\u8861\u7684\u4e58\u79ef\u6765\u8861\u91cf\u590d\u6742\u6027\uff0c\u4e0d\u5e73\u8861\u91cf\u5316\u4e86\u4e0e\u5747\u5300\u5206\u5e03\u7684\u8ddd\u79bb\u3002\u7528\u590d\u6742\u6027\u4ee3\u66ff\u71b5\uff0c\u63d0\u51faCDPO\u7b97\u6cd5\u3002", "result": "\u5728\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u4efb\u52a1\u4e2d\uff0cCDPO \u6bd4 PPO \u5bf9\u590d\u6742\u6027\u7cfb\u6570\u7684\u9009\u62e9\u66f4\u7a33\u5065\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u66f4\u5927\u63a2\u7d22\u7684\u73af\u5883\u4e2d\u3002", "conclusion": "CDPO \u662f\u4e00\u79cd\u66f4\u7a33\u5065\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5b83\u53ef\u4ee5\u901a\u8fc7\u5e73\u8861\u968f\u673a\u6027\u548c\u7ed3\u6784\u6765\u9f13\u52b1\u667a\u80fd\u4f53\u53d1\u73b0\u7ed3\u6784\u5316\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u7b56\u7565\u3002"}}
{"id": "2509.21151", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.21151", "abs": "https://arxiv.org/abs/2509.21151", "authors": ["Lei Hei", "Tingjing Liao", "Yingxin Pei", "Yiyang Qi", "Jiaqi Wang", "Ruiting Li", "Feiliang Ren"], "title": "Retrieval over Classification: Integrating Relation Semantics for Multimodal Relation Extraction", "comment": "Accepted by EMNLP 2025 Main Conference", "summary": "Relation extraction (RE) aims to identify semantic relations between entities\nin unstructured text. Although recent work extends traditional RE to multimodal\nscenarios, most approaches still adopt classification-based paradigms with\nfused multimodal features, representing relations as discrete labels. This\nparadigm has two significant limitations: (1) it overlooks structural\nconstraints like entity types and positional cues, and (2) it lacks semantic\nexpressiveness for fine-grained relation understanding. We propose\n\\underline{R}etrieval \\underline{O}ver \\underline{C}lassification (ROC), a\nnovel framework that reformulates multimodal RE as a retrieval task driven by\nrelation semantics. ROC integrates entity type and positional information\nthrough a multimodal encoder, expands relation labels into natural language\ndescriptions using a large language model, and aligns entity-relation pairs via\nsemantic similarity-based contrastive learning. Experiments show that our\nmethod achieves state-of-the-art performance on the benchmark datasets MNRE and\nMORE and exhibits stronger robustness and interpretability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u5173\u7cfb\u62bd\u53d6\u6846\u67b6ROC\uff0c\u5b83\u5c06\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u8f6c\u5316\u4e3a\u68c0\u7d22\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u5b9e\u4f53-\u5173\u7cfb\u5bf9\u3002", "motivation": "\u4f20\u7edf\u7684\u591a\u6a21\u6001\u5173\u7cfb\u62bd\u53d6\u65b9\u6cd5\u5ffd\u7565\u4e86\u7ed3\u6784\u7ea6\u675f\uff08\u5982\u5b9e\u4f53\u7c7b\u578b\u548c\u4f4d\u7f6e\u4fe1\u606f\uff09\uff0c\u5e76\u4e14\u7f3a\u4e4f\u5bf9\u7ec6\u7c92\u5ea6\u5173\u7cfb\u7406\u89e3\u7684\u8bed\u4e49\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u68c0\u7d22\u4f18\u4e8e\u5206\u7c7b(ROC)\u7684\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u7f16\u7801\u5668\u6574\u5408\u5b9e\u4f53\u7c7b\u578b\u548c\u4f4d\u7f6e\u4fe1\u606f\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c06\u5173\u7cfb\u6807\u7b7e\u6269\u5c55\u4e3a\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u7684\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u5b9e\u4f53-\u5173\u7cfb\u5bf9\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6MNRE\u548cMORE\u4e0a\u5b9e\u73b0\u4e86state-of-the-art\u7684\u6027\u80fd\uff0c\u5e76\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u4e0a\u5177\u6709\u6709\u6548\u6027\u3002"}}
{"id": "2509.20557", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20557", "abs": "https://arxiv.org/abs/2509.20557", "authors": ["Hannah Liu", "Junghyun Min", "Ethan Yue Heng Cheung", "Shou-Yi Hung", "Syed Mekael Wasti", "Runtong Liang", "Shiyao Qian", "Shizhao Zheng", "Elsie Chan", "Ka Ieng Charlotte Lo", "Wing Yu Yip", "Richard Tzong-Han Tsai", "En-Shiun Annie Lee"], "title": "SiniticMTError: A Machine Translation Dataset with Error Annotations for Sinitic Languages", "comment": "Work in progress. 14 pages, 4 figures, 5 tables", "summary": "Despite major advances in machine translation (MT) in recent years, progress\nremains limited for many low-resource languages that lack large-scale training\ndata and linguistic resources. Cantonese and Wu Chinese are two Sinitic\nexamples, although each enjoys more than 80 million speakers around the world.\nIn this paper, we introduce SiniticMTError, a novel dataset that builds on\nexisting parallel corpora to provide error span, error type, and error severity\nannotations in machine-translated examples from English to Mandarin, Cantonese,\nand Wu Chinese. Our dataset serves as a resource for the MT community to\nutilize in fine-tuning models with error detection capabilities, supporting\nresearch on translation quality estimation, error-aware generation, and\nlow-resource language evaluation. We report our rigorous annotation process by\nnative speakers, with analyses on inter-annotator agreement, iterative\nfeedback, and patterns in error type and severity.", "AI": {"tldr": "SiniticMTError\u662f\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\uff0c\u5b83\u5efa\u7acb\u5728\u73b0\u6709\u7684\u5e73\u884c\u8bed\u6599\u5e93\u4e4b\u4e0a\uff0c\u4ee5\u63d0\u4f9b\u4ece\u82f1\u8bed\u5230\u666e\u901a\u8bdd\u3001\u7ca4\u8bed\u548c\u5434\u8bed\u7684\u673a\u5668\u7ffb\u8bd1\u793a\u4f8b\u4e2d\u7684\u9519\u8bef\u8de8\u5ea6\u3001\u9519\u8bef\u7c7b\u578b\u548c\u9519\u8bef\u4e25\u91cd\u7a0b\u5ea6\u6ce8\u91ca\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u673a\u5668\u7ffb\u8bd1\uff08MT\uff09\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u5bf9\u4e8e\u8bb8\u591a\u7f3a\u4e4f\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u548c\u8bed\u8a00\u8d44\u6e90\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u6765\u8bf4\uff0c\u8fdb\u5c55\u4ecd\u7136\u6709\u9650\u3002\u7ca4\u8bed\u548c\u5434\u8bed\u662f\u6c49\u85cf\u8bed\u7cfb\u7684\u4e24\u4e2a\u4f8b\u5b50\uff0c\u5c3d\u7ba1\u6bcf\u79cd\u8bed\u8a00\u5728\u4e16\u754c\u5404\u5730\u90fd\u6709\u8d85\u8fc7 8000 \u4e07\u7684\u4f7f\u7528\u8005\u3002", "method": "\u6211\u4eec\u4ecb\u7ecd\u4e86 SiniticMTError\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\uff0c\u5b83\u5efa\u7acb\u5728\u73b0\u6709\u7684\u5e73\u884c\u8bed\u6599\u5e93\u4e4b\u4e0a\uff0c\u4ee5\u63d0\u4f9b\u673a\u5668\u7ffb\u8bd1\u793a\u4f8b\u4e2d\u7684\u9519\u8bef\u8de8\u5ea6\u3001\u9519\u8bef\u7c7b\u578b\u548c\u9519\u8bef\u4e25\u91cd\u7a0b\u5ea6\u6ce8\u91ca\uff0c\u4ece\u82f1\u8bed\u5230\u666e\u901a\u8bdd\u3001\u7ca4\u8bed\u548c\u5434\u8bed\u3002", "result": "\u6211\u4eec\u62a5\u544a\u4e86\u6bcd\u8bed\u4eba\u58eb\u7684\u4e25\u683c\u6ce8\u91ca\u8fc7\u7a0b\uff0c\u5e76\u5206\u6790\u4e86\u6ce8\u91ca\u8005\u95f4\u534f\u8bae\u3001\u8fed\u4ee3\u53cd\u9988\u4ee5\u53ca\u9519\u8bef\u7c7b\u578b\u548c\u4e25\u91cd\u7a0b\u5ea6\u7684\u6a21\u5f0f\u3002", "conclusion": "\u6211\u4eec\u7684\u6570\u636e\u96c6\u53ef\u4ee5\u4f5c\u4e3a MT \u793e\u533a\u7684\u8d44\u6e90\uff0c\u7528\u4e8e\u5fae\u8c03\u5177\u6709\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\u7684\u6a21\u578b\uff0c\u652f\u6301\u7ffb\u8bd1\u8d28\u91cf\u4f30\u8ba1\u3001\u9519\u8bef\u611f\u77e5\u751f\u6210\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u8bc4\u4f30\u7684\u7814\u7a76\u3002"}}
{"id": "2509.20585", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20585", "abs": "https://arxiv.org/abs/2509.20585", "authors": ["Farbod Bigdeli", "Mohsen Mohammadagha", "Ali Bigdeli"], "title": "Region-of-Interest Augmentation for Mammography Classification under Patient-Level Cross-Validation", "comment": "5 pages, 5 figures, 2 tables", "summary": "Breast cancer screening with mammography remains central to early detection\nand mortality reduction. Deep learning has shown strong potential for\nautomating mammogram interpretation, yet limited-resolution datasets and small\nsample sizes continue to restrict performance. We revisit the Mini-DDSM dataset\n(9,684 images; 2,414 patients) and introduce a lightweight region-of-interest\n(ROI) augmentation strategy. During training, full images are probabilistically\nreplaced with random ROI crops sampled from a precomputed, label-free\nbounding-box bank, with optional jitter to increase variability. We evaluate\nunder strict patient-level cross-validation and report ROC-AUC, PR-AUC, and\ntraining-time efficiency metrics (throughput and GPU memory). Because ROI\naugmentation is training-only, inference-time cost remains unchanged. On\nMini-DDSM, ROI augmentation (best: p_roi = 0.10, alpha = 0.10) yields modest\naverage ROC-AUC gains, with performance varying across folds; PR-AUC is flat to\nslightly lower. These results demonstrate that simple, data-centric ROI\nstrategies can enhance mammography classification in constrained settings\nwithout requiring additional labels or architectural modifications.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684ROI\u589e\u5f3a\u7b56\u7565\uff0c\u7528\u4e8e\u63d0\u9ad8\u4e73\u817a\u764c\u7b5b\u67e5\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u5728\u4e73\u817a\u764c\u94bc\u9776\u56fe\u50cf\u5224\u8bfb\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u5206\u8fa8\u7387\u548c\u6837\u672c\u91cf\u3002", "method": "\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u671f\u95f4\uff0c\u5c06\u5b8c\u6574\u56fe\u50cf\u66ff\u6362\u4e3a\u4ece\u9884\u5148\u8ba1\u7b97\u7684\u65e0\u6807\u7b7e\u8fb9\u754c\u6846\u5e93\u4e2d\u62bd\u6837\u7684\u968f\u673aROI\u88c1\u526a\uff0c\u5e76\u53ef\u9009\u62e9\u6296\u52a8\u4ee5\u589e\u52a0\u53d8\u5f02\u6027\u3002", "result": "\u5728Mini-DDSM\u6570\u636e\u96c6\u4e0a\uff0cROI\u589e\u5f3a\u5728ROC-AUC\u65b9\u9762\u7565\u6709\u63d0\u5347\uff0cPR-AUC\u57fa\u672c\u6301\u5e73\u6216\u7565\u6709\u4e0b\u964d\u3002", "conclusion": "\u7b80\u5355\u7684\u6570\u636e\u9a71\u52a8\u7684ROI\u7b56\u7565\u53ef\u4ee5\u5728\u53d7\u9650\u73af\u5883\u4e2d\u589e\u5f3a\u4e73\u817a\u94bc\u9776\u56fe\u50cf\u5206\u7c7b\uff0c\u65e0\u9700\u989d\u5916\u6807\u7b7e\u6216\u67b6\u6784\u4fee\u6539\u3002"}}
{"id": "2509.20729", "categories": ["cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.20729", "abs": "https://arxiv.org/abs/2509.20729", "authors": ["Jiazheng Sun", "Te Yang", "Jiayang Niu", "Mingxuan Li", "Yongyong Lu", "Ruimeng Yang", "Xin Peng"], "title": "Fairy: Interactive Mobile Assistant to Real-world Tasks via LMM-based Multi-agent", "comment": "20 pages, 12 figures", "summary": "Large multi-modal models (LMMs) have advanced mobile GUI agents. However,\nexisting methods struggle with real-world scenarios involving diverse app\ninterfaces and evolving user needs. End-to-end methods relying on model's\ncommonsense often fail on long-tail apps, and agents without user interaction\nact unilaterally, harming user experience. To address these limitations, we\npropose Fairy, an interactive multi-agent mobile assistant capable of\ncontinuously accumulating app knowledge and self-evolving during usage. Fairy\nenables cross-app collaboration, interactive execution, and continual learning\nthrough three core modules:(i) a Global Task Planner that decomposes user tasks\ninto sub-tasks from a cross-app view; (ii) an App-Level Executor that refines\nsub-tasks into steps and actions based on long- and short-term memory,\nachieving precise execution and user interaction via four core agents operating\nin dual loops; and (iii) a Self-Learner that consolidates execution experience\ninto App Map and Tricks. To evaluate Fairy, we introduce RealMobile-Eval, a\nreal-world benchmark with a comprehensive metric suite, and LMM-based agents\nfor automated scoring. Experiments show that Fairy with GPT-4o backbone\noutperforms the previous SoTA by improving user requirement completion by 33.7%\nand reducing redundant steps by 58.5%, showing the effectiveness of its\ninteraction and self-learning.", "AI": {"tldr": "Fairy, an interactive multi-agent mobile assistant, continuously learns and evolves app knowledge during usage, enabling cross-app collaboration and interactive execution.", "motivation": "Existing methods struggle with diverse app interfaces and evolving user needs in real-world scenarios, often failing on long-tail apps and lacking user interaction.", "method": "Fairy uses a Global Task Planner, an App-Level Executor with dual-loop agents, and a Self-Learner to decompose tasks, refine sub-tasks, and consolidate experience.", "result": "Fairy with GPT-4o outperforms previous state-of-the-art methods, improving user requirement completion by 33.7% and reducing redundant steps by 58.5%.", "conclusion": "Fairy's interaction and self-learning mechanisms are effective in real-world mobile app scenarios, as demonstrated by the RealMobile-Eval benchmark."}}
