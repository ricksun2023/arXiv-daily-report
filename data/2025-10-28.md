<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 13]
- [cs.CV](#cs.CV) [Total: 13]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.IR](#cs.IR) [Total: 16]
- [cs.LG](#cs.LG) [Total: 13]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Multi-lingual Dataset of Classified Paragraphs from Open Access Scientific Publications](https://arxiv.org/abs/2510.21762)
*Eric Jeangirard*

Main category: cs.CL

TL;DR: 本文介绍了一个从CC-BY许可的科学出版物中提取的包含833k段落的数据集，这些段落被分为四类：致谢、数据提及、软件/代码提及和临床试验提及。


<details>
  <summary>Details</summary>
Motivation: 该数据集旨在促进文本分类模型的训练和科学文献挖掘的命名实体识别系统的开发。

Method: 该数据集来自French Open Science Monitor语料库，并使用GROBID进行处理。段落使用fastText进行语言识别，并使用OpenAlex进行科学领域分类。

Result: 该数据集包含833k个段落，主要为英语和法语，并包含其他欧洲语言。每个段落都标注了语言和科学领域。

Conclusion: 该数据集已在HuggingFace上公开发布，可用于训练文本分类模型和开发命名实体识别系统。

Abstract: We present a dataset of 833k paragraphs extracted from CC-BY licensed
scientific publications, classified into four categories: acknowledgments, data
mentions, software/code mentions, and clinical trial mentions. The paragraphs
are primarily in English and French, with additional European languages
represented. Each paragraph is annotated with language identification (using
fastText) and scientific domain (from OpenAlex). This dataset, derived from the
French Open Science Monitor corpus and processed using GROBID, enables training
of text classification models and development of named entity recognition
systems for scientific literature mining. The dataset is publicly available on
HuggingFace https://doi.org/10.57967/hf/6679 under a CC-BY license.

</details>


### [2] [Policy Optimization Prefers The Path of Least Resistance](https://arxiv.org/abs/2510.21853)
*Debdeep Sanyal,Aakash Sen Sharma,Dhruv Kumar,Saurabh Deshpande,Murari Mandal*

Main category: cs.CL

TL;DR: 策略优化算法在开放式 CoT 结构中会倾向于选择阻力最小的路径，导致模型退化为直接回答形式。


<details>
  <summary>Details</summary>
Motivation: 研究策略优化算法在放宽严格的思维链 (CoT) 约束条件下的行为，填补现有研究的空白。

Method: 通过一系列受控实验，研究不同模型和算法在开放式 CoT 结构下的策略优化行为。

Result: 策略优化算法倾向于放弃显式推理，转而直接生成答案，即使对复杂的 ") 格式赋予更高的奖励权重，结果依然成立。策略优化会优先优化最简单的奖励组成部分。收敛到高奖励捷径并非偶然，而是优化过程驱动的。

Conclusion: 给予策略自由发散是一把双刃剑，虽然对发现高奖励捷径是必要的，但也鼓励利用奖励函数中最简单的部分，对奖励攻击提出了挑战。

Abstract: Policy optimization (PO) algorithms are used to refine Large Language Models
for complex, multi-step reasoning. Current state-of-the-art pipelines enforce a
strict think-then-answer format to elicit chain-of-thought (CoT); however, the
behavior of PO when these rigid constraints are relaxed into an open-ended CoT
structure remains an under-studied question. We investigate this gap with an
extensive suite of controlled experiments and identify a consistent principle:
\textit{policy optimization consistently follows the path of least resistance}.
When afforded the flexibility to interleave reasoning and response, policy
optimization consistently learns to discard explicit reasoning, causing the
policy to degenerate to a direct \texttt{<answer>}-only format. This outcome
holds true across various models and algorithms. We find that this collapse in
format is persistent even when the complex \texttt{<think><answer>} format is
assigned up to 4x larger reward weights. We formalize this principle through a
series of controlled reward decomposition experiments, demonstrating a clear
hierarchy: PO systematically optimizes for the simplest reward component first,
a preference that holds even when faced with mutually exclusive choices or
strong incentives for more complex behaviors. Finally, we show that successful
convergence on the high-reward shortcut is not a low-effort drift but is driven
by the optimization process that requires the KL-regularized policy to have
sufficient freedom to make a significant shift from its initial prior. Our
findings reveal that granting policies the freedom to diverge is a double-edged
sword: while necessary for discovering high-reward shortcuts, it also creates a
powerful incentive to game the simplest aspects of the reward function, posing
a critical challenge for reward hacking under alignment.

</details>


### [3] [Language Ranker: A Lightweight Ranking framework for LLM Decoding](https://arxiv.org/abs/2510.21883)
*Chenheng Zhang,Tianqi Du,Jizhe Zhang,Mingqing Xiao,Yifei Wang,Yisen Wang,Zhouchen Lin*

Main category: cs.CL

TL;DR: 提出了一种名为 Language Ranker 的新框架，用于重新排序候选响应，从而提高大型语言模型的性能，同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统的大型语言模型研究主要集中在改进输出分布上，而较少关注将这些分布转换为最终响应的解码过程。奖励模型等最新进展突显了解码的重要性，但这些方法通常计算成本高且适用性有限。

Method: 将解码过程概念化为类似于推荐流水线中的排序阶段，并引入一个轻量级模块来重新排序候选响应，该模块使用基础模型提取的特征。

Result: 在广泛的任务中进行的实验表明，Language Ranker 的性能与大规模奖励模型相当，同时只需要 <0.5M 的额外参数，从而显著降低了训练和推理阶段的计算开销。

Conclusion: Language Ranker 方法高效且有效，展示了其充分释放大型语言模型潜力的潜力。

Abstract: Conventional research on large language models (LLMs) has primarily focused
on refining output distributions, while paying less attention to the decoding
process that transforms these distributions into final responses. Recent
advances, such as scaling the computation of inference time with reward models,
have underscored the importance of decoding, but these methods often suffer
from high computational costs and limited applicability. In this paper, we
revisit LLM generation through the lens of recommender systems, conceptualizing
the decoding process as analogous to the ranking stage in recommendation
pipelines. From this perspective, we observe that both traditional decoding
methods and reward models exhibit clear limitations such as redundancy.
Motivated by this insight, we propose Language Ranker, a novel framework that
introduces a lightweight module to rerank candidate responses using features
extracted by the base model. Experiments across a wide range of tasks show that
Language Ranker achieves performance comparable to large-scale reward models,
while requiring only <0.5M additional parameters, significantly reducing the
computational overhead during both training and inference stages. This
highlights the efficiency and effectiveness of our method, showcasing its
potential to fully unlock the capabilities of LLMs.

</details>


### [4] [Framework for Machine Evaluation of Reasoning Completeness in Large Language Models For Classification Tasks](https://arxiv.org/abs/2510.21884)
*Avinash Patil*

Main category: cs.CL

TL;DR: 本研究提出了RACE框架，用于评估LLM生成的解释与逻辑回归基线模型的重要特征之间的一致性。


<details>
  <summary>Details</summary>
Motivation: 在敏感领域中，机器学习的广泛应用增加了对透明和可解释人工智能的需求。大型语言模型(LLM)越来越能够生成自然语言解释，但这些解释是否真实地捕捉了决策的基础预测信号仍不清楚。

Method: 该论文介绍了一个名为RACE的系统框架，即“解释完整性的推理对齐”，以评估LLM生成的解释与来自逻辑回归基线的可解释特征重要性分数之间的一致性。RACE实现了token-aware、精确字符串和编辑距离匹配技术，以捕获多个粒度级别上的一致性。

Result: 实证结果揭示了一致的不对称性：正确的预测表现出更高的支持特征覆盖率，而错误的预测与矛盾特征的覆盖率升高相关。编辑距离匹配进一步发现了释义重叠，在保持这种不对称性的同时提高了覆盖率。

Conclusion: 这些发现表明，LLM的理由结合了表面水平和灵活的证据重用，但也可能在错误情况下放大误导性线索。RACE为LLM解释的真实性提供了新的见解，并为评估神经语言模型中推理的完整性建立了定量基础。

Abstract: The growing adoption of machine learning (ML) in sensitive domains has
heightened the demand for transparent and interpretable artificial
intelligence. Large Language Models (LLMs) are increasingly capable of
producing natural language explanations, yet it remains unclear whether these
rationales faithfully capture the predictive signals that underlie decisions.
This paper introduces RACE-Reasoning Alignment for Completeness of
Explanations, a systematic framework to evaluate the alignment between
LLM-generated explanations and interpretable feature importance scores derived
from a logistic regression baseline. We analyze four widely used text
classification datasets-WIKI ONTOLOGY, AG NEWS, IMDB, and GOEMOTIONS-and
compare LLM rationales against top-ranked supporting and contradicting lexical
features. To capture alignment at multiple levels of granularity, RACE
implements token-aware, exact string, and edit-distance matching techniques.
Empirical results reveal a consistent asymmetry: correct predictions exhibit
higher coverage of supporting features, while incorrect predictions are
associated with elevated coverage of contradicting features. Edit-distance
matching further uncovers paraphrastic overlaps, boosting coverage while
preserving this asymmetry. These findings demonstrate that LLM rationales
combine both surface-level and flexible evidence reuse, yet can also amplify
misleading cues in error cases. RACE provides new insights into the
faithfulness of LLM explanations and establishes a quantitative basis for
evaluating reasoning completeness in neural language models.

</details>


### [5] [Preventing Catastrophic Forgetting: Behavior-Aware Sampling for Safer Language Model Fine-Tuning](https://arxiv.org/abs/2510.21885)
*Anh Pham,Mihir Thalanki,Michael Sun,Aditya Chaloo,Ankita Gupta,Tian Xia,Aditya Mate,Ehimwenma Nosakhare,Soundararajan Srinivasan*

Main category: cs.CL

TL;DR: 大型语言模型在良性数据上进行微调时，通常会丢失先前对齐的安全行为（一种称为灾难性遗忘的现象）。


<details>
  <summary>Details</summary>
Motivation: 先前的工作表明，添加随机安全示例可以缓解此问题，但仍不清楚哪些示例最有效。

Method: 我们提出了一个行为感知采样框架，该框架基于两个互补因素选择安全示例：指令-响应行为（例如，拒绝与依从）和跨危害类别的语义多样性。

Result: 系统评估表明，这种方法在保持帮助性的同时，显着减少了有害输出，仅使用 0.5% 的额外训练数据即可实现高达 41% 的有害性降低。

Conclusion: 这些结果突出了有针对性的数据选择如何提高大规模微调的安全性和效率。

Abstract: Large language models often lose previously aligned safety behaviors when
fine-tuned on benign data, a phenomenon known as catastrophic forgetting. Prior
work shows that adding random safety examples can mitigate this effect, but it
remains unclear which examples are most effective. We propose a behavior-aware
sampling framework that selects safety examples based on two complementary
factors: instruction-response behavior (e.g., refusal versus compliance) and
semantic diversity across harm categories. Systematic evaluation shows that
this approach substantially reduces harmful outputs while maintaining
helpfulness, achieving up to a 41% reduction in harmfulness with only 0.5%
additional training data. These results highlight how targeted data selection
can improve the safety and efficiency of fine-tuning at scale.

</details>


### [6] [Embedding Trust: Semantic Isotropy Predicts Nonfactuality in Long-Form Text Generation](https://arxiv.org/abs/2510.21891)
*Dhrupad Bhardwaj,Julia Kempe,Tim G. J. Rudner*

Main category: cs.CL

TL;DR: 提出了一种新的方法来评估大型语言模型生成长篇回复的可信度，该方法基于语义各向同性，通过测量文本嵌入在单位球上的均匀程度来实现。


<details>
  <summary>Details</summary>
Motivation: 在需要高度准确回复的高风险领域部署大型语言模型时，需要评估其生成回复的可信度。现有的逐条声明事实核查方法计算成本高昂且脆弱。

Method: 通过生成多个长篇回复，嵌入它们，并估计这些回复的语义各向同性水平（即嵌入在单位球上的角度离散度）来评估可信度。

Result: 发现较高的语义各向同性（即更大的嵌入离散度）可靠地表明样本间较低的事实一致性。该方法无需标注数据、微调和超参数选择，并且可以与开放或封闭权重嵌入模型一起使用。

Conclusion: 该方法在预测长篇回复中的非事实性方面优于现有方法，只需少量样本，为将信任评估集成到实际LLM工作流程中提供了一种实用且低成本的方法。

Abstract: To deploy large language models (LLMs) in high-stakes application domains
that require substantively accurate responses to open-ended prompts, we need
reliable, computationally inexpensive methods that assess the trustworthiness
of long-form responses generated by LLMs. However, existing approaches often
rely on claim-by-claim fact-checking, which is computationally expensive and
brittle in long-form responses to open-ended prompts. In this work, we
introduce semantic isotropy -- the degree of uniformity across normalized text
embeddings on the unit sphere -- and use it to assess the trustworthiness of
long-form responses generated by LLMs. To do so, we generate several long-form
responses, embed them, and estimate the level of semantic isotropy of these
responses as the angular dispersion of the embeddings on the unit sphere. We
find that higher semantic isotropy -- that is, greater embedding dispersion --
reliably signals lower factual consistency across samples. Our approach
requires no labeled data, no fine-tuning, and no hyperparameter selection, and
can be used with open- or closed-weight embedding models. Across multiple
domains, our method consistently outperforms existing approaches in predicting
nonfactuality in long-form responses using only a handful of samples --
offering a practical, low-cost approach for integrating trust assessment into
real-world LLM workflows.

</details>


### [7] [Understanding Network Behaviors through Natural Language Question-Answering](https://arxiv.org/abs/2510.21894)
*Mingzhe Xing,Chang Tian,Jianan Zhang,Lichen Pan,Peipei Liu,Zhaoteng Yan,Yinliang Yue*

Main category: cs.CL

TL;DR: NetMind是一个使用自然语言查询网络的框架，通过分块、统一表示和混合语言来解决长上下文、异构性和复杂推理的挑战。


<details>
  <summary>Details</summary>
Motivation: 理解网络行为的复杂性增加，现有方法学习曲线陡峭且灵活性有限。自然语言提供了一种更易访问和解释的界面，但面临长上下文、异构性和复杂推理的挑战。

Method: 提出NetMind框架，采用基于树的配置分块策略、构建统一的事实图以及设计混合命令式-声明式语言。

Result: NetMind在准确性和可扩展性方面优于现有基线。

Conclusion: NetMind实现了准确且可扩展的网络行为理解。

Abstract: Modern large-scale networks introduce significant complexity in understanding
network behaviors, increasing the risk of misconfiguration. Prior work proposed
to understand network behaviors by mining network configurations, typically
relying on domain-specific languages interfaced with formal models. While
effective, they suffer from a steep learning curve and limited flexibility. In
contrast, natural language (NL) offers a more accessible and interpretable
interface, motivating recent research on NL-guided network behavior
understanding. Recent advances in large language models (LLMs) further enhance
this direction, leveraging their extensive prior knowledge of network concepts
and strong reasoning capabilities. However, three key challenges remain: 1)
numerous router devices with lengthy configuration files challenge LLM's
long-context understanding ability; 2) heterogeneity across devices and
protocols impedes scalability; and 3) complex network topologies and protocols
demand advanced reasoning abilities beyond the current capabilities of LLMs. To
tackle the above challenges, we propose NetMind, a novel framework for querying
networks using NL. Our approach introduces a tree-based configuration chunking
strategy to preserve semantic coherence while enabling efficient partitioning.
We then construct a unified fact graph as an intermediate representation to
normalize vendor-specific configurations. Finally, we design a hybrid
imperative-declarative language to reduce the reasoning burden on LLMs and
enhance precision. We contribute a benchmark consisting of NL question-answer
pairs paired with network configurations. Experiments demonstrate that NetMind
achieves accurate and scalable network behavior understanding, outperforming
existing baselines.

</details>


### [8] [Deep Literature Survey Automation with an Iterative Workflow](https://arxiv.org/abs/2510.21900)
*Hongbo Zhang,Han Cui,Yidong Wang,Yijian Tian,Qi Guo,Cunxiang Wang,Jian Wu,Chiyu Song,Yue Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种基于循环大纲生成的自动文献综述框架 IterSurvey，该框架模仿人类研究人员的迭代阅读过程，逐步检索、阅读和更新大纲，以确保探索和连贯性。为了提供忠实的论文层面基础，我们设计了论文卡片，将每篇论文提炼成其贡献、方法和发现，并引入了具有可视化增强的审查和改进循环，以改善文本流程并整合多模态元素，例如图表。


<details>
  <summary>Details</summary>
Motivation: 现有自动文献综述系统通常采用一次性范例，容易导致检索噪声、结构碎片化和上下文过载，从而限制了综述质量。本文旨在解决这些问题。

Method: 本文提出了一种基于循环大纲生成的框架 IterSurvey，其中规划代理逐步检索、阅读和更新大纲。此外，本文还设计了论文卡片，并引入了具有可视化增强的审查和改进循环。

Result: 实验表明，IterSurvey 在内容覆盖率、结构连贯性和引文质量方面均优于现有技术水平，同时生成了更易于访问和组织良好的综述。此外，本文还引入了 Survey-Arena，这是一个配对基准，可以更清晰地定位机器生成的综述相对于人工撰写的综述。

Conclusion: 本文提出的 IterSurvey 框架能够有效提高自动文献综述的质量，并在多个指标上优于现有技术水平。Survey-Arena 基准的引入也为评估自动文献综述的质量提供了更可靠的手段。

Abstract: Automatic literature survey generation has attracted increasing attention,
yet most existing systems follow a one-shot paradigm, where a large set of
papers is retrieved at once and a static outline is generated before drafting.
This design often leads to noisy retrieval, fragmented structures, and context
overload, ultimately limiting survey quality. Inspired by the iterative reading
process of human researchers, we propose \ours, a framework based on recurrent
outline generation, in which a planning agent incrementally retrieves, reads,
and updates the outline to ensure both exploration and coherence. To provide
faithful paper-level grounding, we design paper cards that distill each paper
into its contributions, methods, and findings, and introduce a
review-and-refine loop with visualization enhancement to improve textual flow
and integrate multimodal elements such as figures and tables. Experiments on
both established and emerging topics show that \ours\ substantially outperforms
state-of-the-art baselines in content coverage, structural coherence, and
citation quality, while producing more accessible and better-organized surveys.
To provide a more reliable assessment of such improvements, we further
introduce Survey-Arena, a pairwise benchmark that complements absolute scoring
and more clearly positions machine-generated surveys relative to human-written
ones. The code is available at
https://github.com/HancCui/IterSurvey\_Autosurveyv2.

</details>


### [9] [Explaining and Mitigating Crosslingual Tokenizer Inequities](https://arxiv.org/abs/2510.21909)
*Catherine Arnett,Tyler A. Chang,Stella Biderman,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 论文研究了不同语言的token premiums（编码平行文本所需的token数量差异）问题。


<details>
  <summary>Details</summary>
Motivation: 高token premiums会导致训练吞吐量降低和推理成本增加。论文旨在探究导致不同语言token premiums差异的跨语言因素。

Method: 论文训练了约7000个单语tokenizer，涵盖97种语言，并操纵tokenization算法、词汇量和数据集大小等因素。研究测量了token premiums，并测试了数据相似性、词汇量和预tokenization等因素之间的关系。同时，研究还调查了文字系统和词长等语言特定特征的作用。

Result: 研究发现训练和测试数据之间的相似性不影响token premiums，但词汇量和预tokenization会影响token premiums。确定每种语言的“最佳”词汇量可以显著降低token premium的影响。训练允许合并空白符的superword tokenizer可以降低token premium的影响并提高整体压缩率。

Conclusion: 干预词汇量或预tokenizer可以显著降低跨语言token premium的影响。

Abstract: The number of tokens it takes to encode parallel text in different languages
is known to vary. These disparities are called token premiums. Having high
token premiums leads to less throughput during training and increases costs at
inference. In this paper, we show that even after controlling for dataset size,
vocabulary size, and data content, monolingual tokenizers exhibit a wide range
of token premiums across languages. To understand the cross-linguistic
differences that cause these token premiums, we train a suite of approximately
7,000 comparable monolingual tokenizers for 97 languages, manipulating
tokenization algorithm, vocabulary size, and dataset size. We measure token
premiums and test for a relationship between factors such as data similarity
(between tokenizer training and evaluation), vocabulary size, and
pre-tokenization. We also investigate the role of language-specific features
such as writing system and word length. We find that similarity between
training and test data does not impact token premiums, but vocabulary size and
pre-tokenization do. While simply increasing vocabulary size does not lead to
reduced token premium effects, we can determine an ``optimal'' vocabulary size
for each language to achieve significantly reduced token premium effects. We
also train superword tokenizers which allow merges over whitespaces, and we
find that they both reduce token premium effects and improve compression
overall. Thus, intervening on the vocabulary size or the pre-tokenizer
significantly reduces crosslingual token premium effects.

</details>


### [10] [Model-Aware Tokenizer Transfer](https://arxiv.org/abs/2510.21954)
*Mykola Haltiuk,Aleksander Smywiński-Pohl*

Main category: cs.CL

TL;DR: 提出了一种新的tokenizer迁移方法，可以在多语言llm中实现鲁棒的tokenizer迁移。


<details>
  <summary>Details</summary>
Motivation: 现有的tokenizer迁移方法通常依赖于语义启发式方法来初始化新的嵌入，忽略了更高层的模型动态，限制了迁移质量。

Method: 提出了一种模型感知tokenizer迁移(MATT)方法，该方法将模型内部结构纳入tokenizer迁移过程。MATT引入了一种注意影响建模(AIM)目标，该目标将源模型中的token间通信模式提炼到具有新tokenizer的目标模型中，从而在标准语言建模之前提供有效的预热。

Result: 在不同的语言环境中进行的实验表明，MATT在几个GPU小时内恢复了原始模型的大部分性能，优于启发式基线。

Conclusion: 将模型级信号纳入tokenizer迁移提供了一条实用且有效的途径。

Abstract: Large Language Models (LLMs) are trained to support an increasing number of
languages, yet their predefined tokenizers remain a bottleneck for adapting
models to lower-resource or distinct-script languages. Existing tokenizer
transfer methods typically rely on semantic heuristics to initialize new
embeddings, ignoring higher-layer model dynamics and limiting transfer quality.
We propose Model-Aware Tokenizer Transfer (MATT), a method that incorporates
model internals into the tokenizer transfer process. MATT introduces an
Attention Influence Modeling (AIM) objective that distills inter-token
communication patterns from a source model into a target model with a new
tokenizer, providing an efficient warm-up before standard language modeling.
Unlike approaches that focus solely on embedding similarity, MATT leverages
attention behavior to guide embedding initialization and adaptation.
Experiments across diverse linguistic settings show that MATT recovers a large
fraction of the original model's performance within a few GPU hours,
outperforming heuristic baselines. These results demonstrate that incorporating
model-level signals offers a practical and effective path toward robust
tokenizer transfer in multilingual LLMs.

</details>


### [11] [A Stylometric Application of Large Language Models](https://arxiv.org/abs/2510.21958)
*Harrison F. Stropkay,Jiayi Chen,Mohammad J. Latifi,Daniel N. Rockmore,Jeremy R. Manning*

Main category: cs.CL

TL;DR: 大型语言模型可以区分不同作者的写作风格。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型是否能捕捉并区分不同作者的写作风格。

Method: 训练独立的GPT-2模型，每个模型基于一位作者的作品进行训练，然后比较模型预测该作者和其他作者文本的准确率。

Result: 针对一位作者训练的模型能更准确地预测该作者的文本。

Conclusion: 模型能够捕捉到作者独特的写作风格。通过此方法，证实了R. P. Thompson是Oz系列第15本书的作者。

Abstract: We show that large language models (LLMs) can be used to distinguish the
writings of different authors. Specifically, an individual GPT-2 model, trained
from scratch on the works of one author, will predict held-out text from that
author more accurately than held-out text from other authors. We suggest that,
in this way, a model trained on one author's works embodies the unique writing
style of that author. We first demonstrate our approach on books written by
eight different (known) authors. We also use this approach to confirm R. P.
Thompson's authorship of the well-studied 15th book of the Oz series,
originally attributed to F. L. Baum.

</details>


### [12] [Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks](https://arxiv.org/abs/2510.21983)
*Havva Alizadeh Noughabi,Julien Serbanescu,Fattane Zarrinkalam,Ali Dehghantanha*

Main category: cs.CL

TL;DR: 大型语言模型容易受到越狱攻击，研究利用社会科学的劝说理论来构建对抗性提示，以绕过LLM中的对齐约束。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易受到越狱攻击，但之前研究很少关注影响模型对此类攻击的语言和心理机制。本文着眼于利用社会科学的劝说理论来制作对抗性提示。

Method: 利用社会科学中已建立的劝说策略，构建具有劝说结构的提示。

Result: 基于劝说的提示显著绕过了安全措施，证明了它们诱导越狱行为的潜力。

Conclusion: 这项工作强调了跨学科见解在应对LLM安全挑战中的重要性。

Abstract: Despite recent advances, Large Language Models remain vulnerable to jailbreak
attacks that bypass alignment safeguards and elicit harmful outputs. While
prior research has proposed various attack strategies differing in human
readability and transferability, little attention has been paid to the
linguistic and psychological mechanisms that may influence a model's
susceptibility to such attacks. In this paper, we examine an interdisciplinary
line of research that leverages foundational theories of persuasion from the
social sciences to craft adversarial prompts capable of circumventing alignment
constraints in LLMs. Drawing on well-established persuasive strategies, we
hypothesize that LLMs, having been trained on large-scale human-generated text,
may respond more compliantly to prompts with persuasive structures.
Furthermore, we investigate whether LLMs themselves exhibit distinct persuasive
fingerprints that emerge in their jailbreak responses. Empirical evaluations
across multiple aligned LLMs reveal that persuasion-aware prompts significantly
bypass safeguards, demonstrating their potential to induce jailbreak behaviors.
This work underscores the importance of cross-disciplinary insight in
addressing the evolving challenges of LLM safety. The code and data are
available.

</details>


### [13] [Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models](https://arxiv.org/abs/2510.22014)
*Sarah Ball,Niki Hasrati,Alexander Robey,Avi Schwarzschild,Frauke Kreuter,Zico Kolter,Andrej Risteski*

Main category: cs.CL

TL;DR: 这篇论文研究了基于离散优化的jailbreak攻击，这种攻击通过在输入提示后添加简短、无意义的后缀来引诱大型语言模型生成不允许的内容。这些后缀通常具有可转移性，即在未优化的提示和模型上也能成功。论文旨在分析可转移性发生的原因和条件。


<details>
  <summary>Details</summary>
Motivation: 目前对jailbreak攻击的可转移性缺乏严谨的分析，为了填补这一空白，本文旨在研究jailbreak攻击可转移性发生的原因和条件。

Method: 本文通过实验确定了三个与跨多个实验设置的转移成功密切相关的统计属性：(1) 没有后缀的提示激活模型内部拒绝方向的程度，(2) 后缀引起多大程度的远离这个方向的推动，(3) 这些转移在正交于拒绝方向的方向上有多大。

Result: 提示语义相似性与转移成功的相关性较弱。这些发现使得可以更细致地理解可转移性。

Conclusion: 对可转移性的细粒度理解可以转化为攻击成功的实际改进，并通过干预实验展示了这一点。

Abstract: Discrete optimization-based jailbreaking attacks on large language models aim
to generate short, nonsensical suffixes that, when appended onto input prompts,
elicit disallowed content. Notably, these suffixes are often transferable --
succeeding on prompts and models for which they were never optimized. And yet,
despite the fact that transferability is surprising and empirically
well-established, the field lacks a rigorous analysis of when and why transfer
occurs. To fill this gap, we identify three statistical properties that
strongly correlate with transfer success across numerous experimental settings:
(1) how much a prompt without a suffix activates a model's internal refusal
direction, (2) how strongly a suffix induces a push away from this direction,
and (3) how large these shifts are in directions orthogonal to refusal. On the
other hand, we find that prompt semantic similarity only weakly correlates with
transfer success. These findings lead to a more fine-grained understanding of
transferability, which we use in interventional experiments to showcase how our
statistical analysis can translate into practical improvements in attack
success.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [14] [Diagnosing Bottlenecks in Data Visualization Understanding by Vision-Language Models](https://arxiv.org/abs/2510.21740)
*Alexa R. Tartaglini,Satchel Grant,Daniel Wurgaft,Christopher Potts,Judith E. Fan*

Main category: cs.CV

TL;DR: 当前视觉语言模型(VLM)在理解数据可视化方面存在困难，但失败原因尚不清楚。


<details>
  <summary>Details</summary>
Motivation: 探究VLM在数据可视化理解任务中失败的原因，例如视觉信息编码、视觉和语言模块间信息传递以及语言模块内部信息处理的局限性。

Method: 开发了一个名为FUGU的数据可视化理解任务套件，用于精确描述潜在的困难来源。使用FUGU来调查三个广泛使用的VLM，并使用激活修补和线性探针来追踪模型中信息流。

Result: 发现一些模型无法正确生成单个数据点的坐标，并且这些初始错误通常会导致错误的最终响应。当提供正确的坐标时，性能会显着提高。即使模型生成了不正确的响应，也可以从视觉编码器中的潜在表示中成功读取正确的坐标，这表明这些错误的根源在于视觉语言的切换。提供正确的坐标有助于涉及少量数据点的任务，但通常会降低提取跨多个数据点的统计关系的任务的性能。在FUGU上微调模型也未能产生最佳性能。

Conclusion: 目前的VLM存在架构约束，这可能对可靠的数据可视化理解构成重大挑战。

Abstract: Data visualizations are vital components of many scientific articles and news
stories. Current vision-language models (VLMs) still struggle on basic data
visualization understanding tasks, but the causes of failure remain unclear.
Are VLM failures attributable to limitations in how visual information in the
data visualization is encoded, how information is transferred between the
vision and language modules, or how information is processed within the
language module? We developed FUGU, a suite of data visualization understanding
tasks, to precisely characterize potential sources of difficulty (e.g.,
extracting the position of data points, distances between them, and other
summary statistics). We used FUGU to investigate three widely used VLMs. To
diagnose the sources of errors produced by these models, we used activation
patching and linear probes to trace information flow through models across a
variety of prompting strategies. We found that some models fail to generate the
coordinates of individual data points correctly, and these initial errors often
lead to erroneous final responses. When these models are provided with the
correct coordinates, performance improves substantially. Moreover, even when
the model generates an incorrect response, the correct coordinates can be
successfully read out from the latent representations in the vision encoder,
suggesting that the source of these errors lies in the vision-language handoff.
We further found that while providing correct coordinates helps with tasks
involving one or a small number of data points, it generally worsens
performance for tasks that require extracting statistical relationships across
many data points. Fine-tuning models on FUGU also fails to yield ceiling
performance. These findings point to architectural constraints in current VLMs
that might pose significant challenges for reliable data visualization
understanding.

</details>


### [15] [Agro-Consensus: Semantic Self-Consistency in Vision-Language Models for Crop Disease Management in Developing Countries](https://arxiv.org/abs/2510.21757)
*Mihir Gupta,Pratik Desai,Ross Greer*

Main category: cs.CV

TL;DR: 提出了一种低成本的自洽框架，以提高视觉语言模型在农业图像描述中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 发展中国家在农业疾病管理方面面临挑战，因为缺乏专家、网络不可靠且成本高昂，阻碍了大规模人工智能系统的部署。

Method: 采用语义聚类，使用轻量级预训练嵌入模型对多个候选响应进行分组，然后通过基于余弦相似度的共识选择最连贯的标题。 结合人工参与（HITL）组件，用户确认作物类型，过滤掉错误的生成，确保为共识机制提供更高质量的输入。

Result: 在PlantVillage数据集上，使用微调的3B参数PaliGemma模型，该框架的性能优于标准解码方法。单集群共识方法在10个候选生成中实现了83.1%的峰值精度，而贪婪解码的基线精度为77.5%。当考虑多个集群时，准确率进一步提高；在前四个候选集群中找到正确响应时，准确率上升至94.0%，优于基线中前4个选择所达到的88.5%。

Conclusion: 该框架在提高视觉语言模型在农业图像描述中的可靠性方面有效，特别是在资源受限的环境中。

Abstract: Agricultural disease management in developing countries such as India, Kenya,
and Nigeria faces significant challenges due to limited access to expert plant
pathologists, unreliable internet connectivity, and cost constraints that
hinder the deployment of large-scale AI systems. This work introduces a
cost-effective self-consistency framework to improve vision-language model
(VLM) reliability for agricultural image captioning. The proposed method
employs semantic clustering, using a lightweight (80MB) pre-trained embedding
model to group multiple candidate responses. It then selects the most coherent
caption -- containing a diagnosis, symptoms, analysis, treatment, and
prevention recommendations -- through a cosine similarity-based consensus. A
practical human-in-the-loop (HITL) component is incorporated, wherein user
confirmation of the crop type filters erroneous generations, ensuring
higher-quality input for the consensus mechanism. Applied to the publicly
available PlantVillage dataset using a fine-tuned 3B-parameter PaliGemma model,
our framework demonstrates improvements over standard decoding methods.
Evaluated on 800 crop disease images with up to 21 generations per image, our
single-cluster consensus method achieves a peak accuracy of 83.1% with 10
candidate generations, compared to the 77.5% baseline accuracy of greedy
decoding. The framework's effectiveness is further demonstrated when
considering multiple clusters; accuracy rises to 94.0% when a correct response
is found within any of the top four candidate clusters, outperforming the 88.5%
achieved by a top-4 selection from the baseline.

</details>


### [16] [Proportion and Perspective Control for Flow-Based Image Generation](https://arxiv.org/abs/2510.21763)
*Julien Boudier,Hugo Caselles-Dupré*

Main category: cs.CV

TL;DR: 提出了两种 ControlNet，用于更好地控制 text-to-image 模型生成的图像的空间和几何结构。


<details>
  <summary>Details</summary>
Motivation: 现有的 text-to-image 模型在空间和几何结构控制方面存在局限性。

Method: 提出了 proportion ControlNet（通过边界框控制物体的位置和比例）和 perspective ControlNet（通过消失线控制场景的 3D 几何）。使用视觉-语言模型进行标注，并使用专门的算法来调节图像合成，从而支持这些模块的训练。

Result: 证明了这两个模块都提供了有效的控制，但在复杂的约束下存在局限性。

Conclusion: 发布了 proportion ControlNet 和 perspective ControlNet 模型。

Abstract: While modern text-to-image diffusion models generate high-fidelity images,
they offer limited control over the spatial and geometric structure of the
output. To address this, we introduce and evaluate two ControlNets specialized
for artistic control: (1) a proportion ControlNet that uses bounding boxes to
dictate the position and scale of objects, and (2) a perspective ControlNet
that employs vanishing lines to control the 3D geometry of the scene. We
support the training of these modules with data pipelines that leverage
vision-language models for annotation and specialized algorithms for
conditioning image synthesis. Our experiments demonstrate that both modules
provide effective control but exhibit limitations with complex constraints.
Both models are released on HuggingFace:
https://huggingface.co/obvious-research

</details>


### [17] [H2OFlow: Grounding Human-Object Affordances with 3D Generative Models and Dense Diffused Flows](https://arxiv.org/abs/2510.21769)
*Harry Zhang,Luca Carlone*

Main category: cs.CV

TL;DR: 提出了一种名为H2OFlow的新框架，该框架仅使用来自 3D 生成模型的数据来全面学习 3D HOI 行为能力，包括接触、方向和空间占用。


<details>
  <summary>Details</summary>
Motivation: 目前的方法通常依赖于人工标记的数据集，这些数据集捕获了真实或模拟的人与对象交互 (HOI) 任务，这些数据集的生成成本高且耗时。此外，大多数现有的 3D 可供性理解方法仅限于基于接触的分析，而忽略了人与对象交互的其他重要方面，例如方向和空间占用。

Method: H2OFlow 采用基于密集 3D 流的表示，该表示通过在点云上运行的密集扩散过程学习。

Result: 通过广泛的定量和定性评估，证明 H2OFlow 可以有效地推广到真实世界的对象，并且在对 3D 行为能力进行建模时，超越了依赖于手动注释或基于网格的表示的先前方法。

Conclusion: H2OFlow 是一种很有前途的 3D HOI 行为能力学习框架，它不需要人工注释，并且可以有效地推广到真实世界的对象。

Abstract: Understanding how humans interact with the surrounding environment, and
specifically reasoning about object interactions and affordances, is a critical
challenge in computer vision, robotics, and AI. Current approaches often depend
on labor-intensive, hand-labeled datasets capturing real-world or simulated
human-object interaction (HOI) tasks, which are costly and time-consuming to
produce. Furthermore, most existing methods for 3D affordance understanding are
limited to contact-based analysis, neglecting other essential aspects of
human-object interactions, such as orientation (\eg, humans might have a
preferential orientation with respect certain objects, such as a TV) and
spatial occupancy (\eg, humans are more likely to occupy certain regions around
an object, like the front of a microwave rather than its back). To address
these limitations, we introduce \emph{H2OFlow}, a novel framework that
comprehensively learns 3D HOI affordances -- encompassing contact, orientation,
and spatial occupancy -- using only synthetic data generated from 3D generative
models. H2OFlow employs a dense 3D-flow-based representation, learned through a
dense diffusion process operating on point clouds. This learned flow enables
the discovery of rich 3D affordances without the need for human annotations.
Through extensive quantitative and qualitative evaluations, we demonstrate that
H2OFlow generalizes effectively to real-world objects and surpasses prior
methods that rely on manual annotations or mesh-based representations in
modeling 3D affordance.

</details>


### [18] [OCR-Quality: A Human-Annotated Dataset for OCR Quality Assessment](https://arxiv.org/abs/2510.21774)
*Yulong Zhang*

Main category: cs.CV

TL;DR: 提出了一个用于评估OCR质量评估方法的综合人工标注数据集OCR-Quality。


<details>
  <summary>Details</summary>
Motivation: 在真实应用中，可靠的OCR质量评估至关重要，但缺乏相关资源。

Method: 使用最先进的视觉-语言模型(VLMs)处理1000个PDF页面，并使用4级评分系统进行人工标注。

Result: 构建了一个包含详细来源信息、注释指南和各种难度级别代表性案例的数据集。

Conclusion: OCR-Quality满足了实际应用中对可靠OCR质量评估的关键需求，并为训练和评估OCR验证系统提供了有价值的基准。

Abstract: We present OCR-Quality, a comprehensive human-annotated dataset designed for
evaluating and developing OCR quality assessment methods. The dataset consists
of 1,000 PDF pages converted to PNG images at 300 DPI, sampled from diverse
real-world scenarios, including academic papers, textbooks, e-books, and
multilingual documents. Each document has been processed using state-of-the-art
Vision-Language Models (VLMs) and manually annotated with quality scores using
a 4-level scoring system (1: Excellent, 2: Good, 3: Fair, 4: Poor). The dataset
includes detailed source information, annotation guidelines, and representative
cases across various difficulty levels. OCR-Quality addresses the critical need
for reliable OCR quality assessment in real-world applications and provides a
valuable benchmark for training and evaluating OCR verification systems. The
dataset is publicly available at
https://huggingface.co/datasets/Aslan-mingye/OCR-Quality .

</details>


### [19] [Face-MakeUpV2: Facial Consistency Learning for Controllable Text-to-Image Generation](https://arxiv.org/abs/2510.21775)
*Dawei Dai,Yinxiu Zhou,Chenghang Li,Guolai Jiang,Chengfang Zhang*

Main category: cs.CV

TL;DR: Face-MakeUpV2是一个面部图像生成模型，旨在保持面部ID和与参考图像的物理特征的一致性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型在面部图像生成中，当响应局部语义指令时，经常遭受面部属性泄漏和物理一致性不足的问题。

Method: 1. 构建大规模数据集FaceCaptionMask-1M，包含约一百万个图像-文本-掩码对，为局部语义指令提供精确的空间监督。
2. 采用通用文本到图像预训练模型作为主干，并引入两个互补的面部信息注入通道：一个3D面部渲染通道，用于结合图像的物理特征，以及一个全局面部特征通道。
3. 制定了两个优化目标，用于模型的监督学习：模型嵌入空间中的语义对齐，以减轻属性泄漏问题；以及面部图像上的感知损失，以保持ID一致性。

Result: Face-MakeUpV2在保持面部ID和维持参考图像的物理一致性方面取得了最佳的整体性能。

Conclusion: Face-MakeUpV2在各种应用中具有可靠和可控的面部编辑的实际潜力。

Abstract: In facial image generation, current text-to-image models often suffer from
facial attribute leakage and insufficient physical consistency when responding
to local semantic instructions. In this study, we propose Face-MakeUpV2, a
facial image generation model that aims to maintain the consistency of face ID
and physical characteristics with the reference image. First, we constructed a
large-scale dataset FaceCaptionMask-1M comprising approximately one million
image-text-masks pairs that provide precise spatial supervision for the local
semantic instructions. Second, we employed a general text-to-image pretrained
model as the backbone and introduced two complementary facial information
injection channels: a 3D facial rendering channel to incorporate the physical
characteristics of the image and a global facial feature channel. Third, we
formulated two optimization objectives for the supervised learning of our
model: semantic alignment in the model's embedding space to mitigate the
attribute leakage problem and perceptual loss on facial images to preserve ID
consistency. Extensive experiments demonstrated that our Face-MakeUpV2 achieves
best overall performance in terms of preserving face ID and maintaining
physical consistency of the reference images. These results highlight the
practical potential of Face-MakeUpV2 for reliable and controllable facial
editing in diverse applications.

</details>


### [20] [Ageing Drift in Binary Face Templates: A Bits-per-Decade Analysis](https://arxiv.org/abs/2510.21778)
*Abdelilah Ganmati,Karim Afdel,Lahcen Koutti*

Main category: cs.CV

TL;DR: 研究人脸识别模板的长期稳定性，并量化了老化漂移。


<details>
  <summary>Details</summary>
Motivation: 研究人脸识别技术在时间推移下的性能变化。

Method: 使用PCA-ITQ将人脸CNN的float embeddings压缩成64位和128位代码，并针对AgeDB中的每个人脸计算汉明距离与年龄差距的线性模型。

Result: 64位模板的中值斜率为每十年1.357 bits，128位模板为每十年2.571 bits，表明类内距离随时间推移略有增加。较短的代码在固定决策阈值下更稳定。

Conclusion: 讨论了对智能卡和match-on-card部署的影响，包括定期重新注册和针对经验上不稳定的位位置进行奇偶校验等缓解措施。

Abstract: We study the longitudinal stability of compact binary face templates and
quantify ageing drift directly in bits per decade. Float embeddings from a
modern face CNN are compressed with PCA-ITQ into 64- and 128-bit codes. For
each identity in AgeDB with at least three distinct ages, we form all genuine
pairs and fit a per-identity linear model of Hamming distance versus absolute
age gap. Across 566 identities, the median slope is 1.357 bits per decade for
64-bit templates and 2.571 bits per decade for 128-bit templates, with tight
non-parametric 95 percent bootstrap confidence intervals. The distributions are
predominantly positive, indicating a small but systematic increase in
intra-class distance over time. Because drift scales with code length, shorter
codes are inherently more age-stable at a fixed decision threshold. We connect
these slopes to operating characteristics by reporting EER and TPR at FAR = 1
percent in three age bins. We discuss implications for smart-card and
match-on-card deployments, including simple mitigations such as periodic
re-enrolment and targeted parity on empirically unstable bit positions. Code
and CSV artifacts are provided to support reproducibility.

</details>


### [21] [Bridging Accuracy and Interpretability: Deep Learning with XAI for Breast Cancer Detection](https://arxiv.org/abs/2510.21780)
*Bishal Chhetri,B. V. Rathish Kumar*

Main category: cs.CV

TL;DR: 提出了一种用于早期乳腺癌检测的可解释深度学习框架，该框架使用从乳房肿块的数字化细针穿刺 (FNA) 图像中提取的定量特征。


<details>
  <summary>Details</summary>
Motivation: 临床上需要高精度和可解释性的乳腺癌早期检测方法。

Method: 使用 ReLU 激活函数、Adam 优化器和二元交叉熵损失的深度神经网络，并结合 SHAP 和 LIME 等可解释 AI 技术。

Result: 该模型实现了最先进的分类性能，准确率为 0.992，精度为 1.000，召回率为 0.977，F1 分数为 0.988，优于其他算法。凹点特征被发现是对分类任务产生积极影响的最有影响力的特征。

Conclusion: 该框架通过提供特征级别的归因和人类可读的可视化，提高了临床医生对深度学习模型的信任度，从而弥合了性能和可解释性之间的差距。

Abstract: In this study, we present an interpretable deep learning framework for the
early detection of breast cancer using quantitative features extracted from
digitized fine needle aspirate (FNA) images of breast masses. Our deep neural
network, using ReLU activations, the Adam optimizer, and a binary cross-entropy
loss, delivers state-of-the-art classification performance, achieving an
accuracy of 0.992, precision of 1.000, recall of 0.977, and an F1 score of
0.988. These results substantially exceed the benchmarks reported in the
literature. We evaluated the model under identical protocols against a suite of
well-established algorithms (logistic regression, decision trees, random
forests, stochastic gradient descent, K-nearest neighbors, and XGBoost) and
found the deep model consistently superior on the same metrics. Recognizing
that high predictive accuracy alone is insufficient for clinical adoption due
to the black-box nature of deep learning models, we incorporated model-agnostic
Explainable AI techniques such as SHAP and LIME to produce feature-level
attributions and human-readable visualizations. These explanations quantify the
contribution of each feature to individual predictions, support error analysis,
and increase clinician trust, thus bridging the gap between performance and
interpretability for real-world clinical use. The concave points feature of the
cell nuclei is found to be the most influential feature positively impacting
the classification task. This insight can be very helpful in improving the
diagnosis and treatment of breast cancer by highlighting the key
characteristics of breast tumor.

</details>


### [22] [EdgeSync: Accelerating Edge-Model Updates for Data Drift through Adaptive Continuous Learning](https://arxiv.org/abs/2510.21781)
*Runchu Donga,Peng Zhao,Guiqin Wang,Nan Qi,Jie Lin*

Main category: cs.CV

TL;DR: EdgeSync improves edge model accuracy by incorporating timeliness and inference results into sample filtering and optimizing the timing of model updates.


<details>
  <summary>Details</summary>
Motivation: Model accuracy decreases over time due to changing data features, and retraining is compute-intensive and may not align well with the evolving data distribution.

Method: EdgeSync enhances sample filtering by incorporating timeliness and inference results and features a dynamic training management module.

Result: EdgeSync improves accuracy by approximately 3.4% compared to existing methods and by about 10% compared to traditional approaches.

Conclusion: EdgeSync is an efficient edge-model updating approach that ensures training samples are more relevant to the current video content while reducing update delays.

Abstract: Real-time video analytics systems typically deploy lightweight models on edge
devices to reduce latency. However, the distribution of data features may
change over time due to various factors such as changing lighting and weather
conditions, leading to decreased model accuracy. Recent frameworks try to
address this issue by leveraging remote servers to continuously train and adapt
lightweight edge models using more complex models in the cloud. Despite these
advancements, existing methods face two key challenges: first, the retraining
process is compute-intensive, causing significant delays in model updates;
second, the new model may not align well with the evolving data distribution of
the current video stream. To address these challenges, we introduce EdgeSync,
an efficient edge-model updating approach that enhances sample filtering by
incorporating timeliness and inference results, thus ensuring training samples
are more relevant to the current video content while reducing update delays.
Additionally, EdgeSync features a dynamic training management module that
optimizes the timing and sequencing of model updates to improve their
timeliness. Evaluations on diverse and complex real-world datasets demonstrate
that EdgeSync improves accuracy by approximately 3.4% compared to existing
methods and by about 10% compared to traditional approaches.

</details>


### [23] [Promptable Fire Segmentation: Unleashing SAM2's Potential for Real-Time Mobile Deployment with Strategic Bounding Box Guidance](https://arxiv.org/abs/2510.21782)
*Emmanuel U. Ugwu,Zhang Xinming*

Main category: cs.CV

TL;DR: 本文评估了SAM2变体在火灾分割中的性能，特别关注边界框提示策略以增强部署可行性。


<details>
  <summary>Details</summary>
Motivation: 火灾分割由于火焰的不规则边界、半透明边缘和高度可变的强度仍然是计算机视觉中的一个关键挑战。虽然Segment Anything Models (SAM和SAM2)已经展示了令人印象深刻的跨领域泛化能力，但它们在火灾分割中的有效性——特别是在移动部署约束下——仍然很大程度上未被探索。

Method: 系统地评估了四种SAM2.1变体（tiny、small、base_plus、large）以及面向移动的变体（TinySAM、MobileSAM）在三个火灾数据集上使用多种提示策略：自动、单正点（SP）、单正点+单负点（SP+SN）、多正点（MP）、边界框（Box）和混合变体（Box+SP和Box+MP）。

Result: 实验结果表明，边界框提示始终优于自动和基于单点的方法，其中Box+MP在Khan数据集上实现了最高的平均IoU（0.64）和Dice系数（0.75）。诸如TinySAM和MobileSAM之类的轻量级变体进一步降低了内存和计算成本，使其更适合于容忍延迟的边缘场景。

Conclusion: 这项工作为在火灾监控系统中部署可提示的分割模型提供了关键见解，并为未来特定领域的SAM应用研究建立了基准。

Abstract: Fire segmentation remains a critical challenge in computer vision due to
flames' irregular boundaries, translucent edges, and highly variable
intensities. While the Segment Anything Models (SAM and SAM2) have demonstrated
impressive cross-domain generalization capabilities, their effectiveness in
fire segmentation -- particularly under mobile deployment constraints --
remains largely unexplored. This paper presents the first comprehensive
evaluation of SAM2 variants for fire segmentation, focusing on bounding box
prompting strategies to enhance deployment feasibility. We systematically
evaluate four SAM2.1 variants (tiny, small, base_plus, large) alongside
mobile-oriented variants (TinySAM, MobileSAM) across three fire datasets using
multiple prompting strategies: automatic, single positive point (SP), single
positive point + single negative point (SP+SN), multiple positive points (MP),
bounding box (Box), and hybrid variants (Box+SP and Box+MP). Our experimental
results demonstrate that bounding box prompts consistently outperform automatic
and single point-based approaches, with Box+MP achieving the highest mean IoU
(0.64) and Dice coefficient (0.75) on the Khan dataset. Lightweight variants
such as TinySAM and MobileSAM further reduce memory and computational costs,
making them more suitable for latency-tolerant edge scenarios. Overall, this
work provides critical insights for deploying promptable segmentation models in
fire monitoring systems and establishes benchmarks for future research in
domain-specific SAM applications. Code is available at:
https://github.com/UEmmanuel5/ProFSAM

</details>


### [24] [Noise Aggregation Analysis Driven by Small-Noise Injection: Efficient Membership Inference for Diffusion Models](https://arxiv.org/abs/2510.21783)
*Guo Li,Yuyang Yu,Xuemiao Xu*

Main category: cs.CV

TL;DR: 提出了一种针对扩散模型的有效成员推理攻击方法，该方法通过注入轻微噪声并评估噪声分布的聚合程度来判断样本是否为训练集成员。


<details>
  <summary>Details</summary>
Motivation: 扩散模型被广泛使用，但也带来了潜在的隐私风险，成员推理攻击试图确定特定数据样本是否被用于模型训练过程。

Method: 该方法基于注入轻微噪声，并评估噪声分布的聚合程度。 член изображения在扩散过程的某个时间步长表现出更高的预测噪声聚合，而非成员图像的预测噪声则表现出更离散的特征。

Result: 该方法在多个数据集上取得了优越的性能，并且在面对大型文本到图像扩散模型时，在ASR和AUC方面也表现出更好的攻击效果。

Conclusion: 该方法具有可扩展性，并且能够有效地攻击扩散模型。

Abstract: Diffusion models have demonstrated powerful performance in generating
high-quality images. A typical example is text-to-image generator like Stable
Diffusion. However, their widespread use also poses potential privacy risks. A
key concern is membership inference attacks, which attempt to determine whether
a particular data sample was used in the model training process. We propose an
efficient membership inference attack method against diffusion models. This
method is based on the injection of slight noise and the evaluation of the
aggregation degree of the noise distribution. The intuition is that the noise
prediction patterns of diffusion models for training set samples and
non-training set samples exhibit distinguishable differences.Specifically, we
suppose that member images exhibit higher aggregation of predicted noise around
a certain time step of the diffusion process. In contrast, the predicted noises
of non-member images exhibit a more discrete characteristic around the certain
time step. Compared with other existing methods, our proposed method requires
fewer visits to the target diffusion model. We inject slight noise into the
image under test and then determine its membership by analyzing the aggregation
degree of the noise distribution predicted by the model. Empirical findings
indicate that our method achieves superior performance across multiple
datasets. At the same time, our method can also show better attack effects in
ASR and AUC when facing large-scale text-to-image diffusion models, proving the
scalability of our method.

</details>


### [25] [Multi-Agent Pose Uncertainty: A Differentiable Rendering Cramér-Rao Bound](https://arxiv.org/abs/2510.21785)
*Arun Muthukkumar*

Main category: cs.CV

TL;DR: 本文提出了一种相机姿态估计协方差的闭式下界算法，将可微渲染器视为测量函数，为计算机视觉和机器人技术中的应用提供了严谨的姿态不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 在计算机视觉和机器人技术中，姿态估计至关重要，但很少有研究对密集或学习模型下的姿态进行严格的不确定性量化。

Method: 该方法通过将可微渲染器视为测量函数，推导出相机姿态估计协方差的闭式下界。通过在流形上对一个小的姿态扰动线性化图像形成过程，得到一个渲染感知的Cramér-Rao bound。

Result: 该方法可以简化为经典的捆绑调整不确定性，确保了与视觉理论的连续性。通过融合不同相机的Fisher信息，自然地扩展到多智能体设置。

Conclusion: 该统计公式可用于诸如协同感知和新视角合成等下游任务，而无需显式的关键点对应。

Abstract: Pose estimation is essential for many applications within computer vision and
robotics. Despite its uses, few works provide rigorous uncertainty
quantification for poses under dense or learned models. We derive a closed-form
lower bound on the covariance of camera pose estimates by treating a
differentiable renderer as a measurement function. Linearizing image formation
with respect to a small pose perturbation on the manifold yields a render-aware
Cram\'er-Rao bound. Our approach reduces to classical bundle-adjustment
uncertainty, ensuring continuity with vision theory. It also naturally extends
to multi-agent settings by fusing Fisher information across cameras. Our
statistical formulation has downstream applications for tasks such as
cooperative perception and novel view synthesis without requiring explicit
keypoint correspondences.

</details>


### [26] [EventFormer: A Node-graph Hierarchical Attention Transformer for Action-centric Video Event Prediction](https://arxiv.org/abs/2510.21786)
*Qile Su,Shoutai Zhu,Shuai Zhang,Baoyu Liang,Chao Tong*

Main category: cs.CV

TL;DR: 本文提出了一个名为AVEP（Action-centric Video Event Prediction）的新任务，旨在预测视频中后续事件，并构建了一个大型结构化数据集来支持该任务。同时，作者提出了一个基于节点图分层注意力的视频事件预测模型EventFormer，并在AVEP上进行了实验，证明了该任务的复杂性和数据集的价值。


<details>
  <summary>Details</summary>
Motivation: 现有的事件预测研究主要集中在文本脚本上，而忽略了视频中事件预测的重要性。为了填补这一空白，本文提出了AVEP任务，旨在研究视频中的事件预测。

Method: 本文构建了一个包含约3.5万个带注释视频和超过17.8万个视频片段的大型结构化数据集。此外，本文还提出了一个基于节点图分层注意力的视频事件预测模型EventFormer，该模型能够捕获事件及其参数之间的关系，以及参数之间的共指关系。

Result: 在AVEP数据集上，EventFormer模型优于所有现有的视频预测模型和大型视觉语言模型（LVLMs）。

Conclusion: 本文提出的AVEP任务和EventFormer模型为视频事件预测领域的研究提供了新的方向和方法。

Abstract: Script event induction, which aims to predict the subsequent event based on
the context, is a challenging task in NLP, achieving remarkable success in
practical applications. However, human events are mostly recorded and presented
in the form of videos rather than scripts, yet there is a lack of related
research in the realm of vision. To address this problem, we introduce AVEP
(Action-centric Video Event Prediction), a task that distinguishes itself from
existing video prediction tasks through its incorporation of more complex logic
and richer semantic information. We present a large structured dataset, which
consists of about $35K$ annotated videos and more than $178K$ video clips of
event, built upon existing video event datasets to support this task. The
dataset offers more fine-grained annotations, where the atomic unit is
represented as a multimodal event argument node, providing better structured
representations of video events. Due to the complexity of event structures,
traditional visual models that take patches or frames as input are not
well-suited for AVEP. We propose EventFormer, a node-graph hierarchical
attention based video event prediction model, which can capture both the
relationships between events and their arguments and the coreferencial
relationships between arguments. We conducted experiments using several SOTA
video prediction models as well as LVLMs on AVEP, demonstrating both the
complexity of the task and the value of the dataset. Our approach outperforms
all these video prediction models. We will release the dataset and code for
replicating the experiments and annotations.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [27] [A Multi-Component AI Framework for Computational Psychology: From Robust Predictive Modeling to Deployed Generative Dialogue](https://arxiv.org/abs/2510.21720)
*Anant Pareek*

Main category: cs.AI

TL;DR: 本文提出了一个综合性的框架，旨在弥合孤立的预测模型和用于心理分析的交互式系统之间的差距。


<details>
  <summary>Details</summary>
Motivation: 利用人工智能和计算心理学的融合，通过计算手段建模、理解和与复杂的人类心理状态互动。

Method: 该方法包括一个严谨的端到端开发生命周期。首先，使用经典机器学习技术在四个不同的心理数据集上建立基础性能基准。其次，对最先进的Transformer模型进行了微调。第三，使用参数高效技术对生成式大型语言模型（LLM）进行了微调，使其可以充当交互式“人格大脑”。最后，整个预测和生成模型套件被构建和部署为稳健、可扩展的微服务生态系统。

Result: 主要发现包括成功稳定了用于情感计算的基于Transformer的回归模型，在标准方法失败的地方显示出有意义的预测性能，以及开发了一种可复制的方法来 democratizing 大规模人工智能研究。

Conclusion: 这项工作的意义在于其整体方法，展示了一个完整的从研究到部署的管道，该管道将预测分析与生成式对话相结合，从而为计算心理学和人机交互领域的未来研究提供了实用的模型。

Abstract: The confluence of Artificial Intelligence and Computational Psychology
presents an opportunity to model, understand, and interact with complex human
psychological states through computational means. This paper presents a
comprehensive, multi-faceted framework designed to bridge the gap between
isolated predictive modeling and an interactive system for psychological
analysis. The methodology encompasses a rigorous, end-to-end development
lifecycle. First, foundational performance benchmarks were established on four
diverse psychological datasets using classical machine learning techniques.
Second, state-of-the-art transformer models were fine-tuned, a process that
necessitated the development of effective solutions to overcome critical
engineering challenges, including the resolution of numerical instability in
regression tasks and the creation of a systematic workflow for conducting
large-scale training under severe resource constraints. Third, a generative
large language model (LLM) was fine-tuned using parameter-efficient techniques
to function as an interactive "Personality Brain." Finally, the entire suite of
predictive and generative models was architected and deployed as a robust,
scalable microservices ecosystem. Key findings include the successful
stabilization of transformer-based regression models for affective computing,
showing meaningful predictive performance where standard approaches failed, and
the development of a replicable methodology for democratizing large-scale AI
research. The significance of this work lies in its holistic approach,
demonstrating a complete research-to-deployment pipeline that integrates
predictive analysis with generative dialogue, thereby providing a practical
model for future research in computational psychology and human-AI interaction.

</details>


### [28] [PREFINE: Personalized Story Generation via Simulated User Critics and User-Specific Rubric Generation](https://arxiv.org/abs/2510.21721)
*Kentaro Ueda,Takehiro Takayanagi*

Main category: cs.AI

TL;DR: 提出了一种新的个性化故事生成框架，无需微调或直接用户反馈。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖显式反馈或微调，存在用户负担、数据收集、计算成本和隐私问题。

Method: 提出PREFINE框架，该框架扩展了Critique-and-Refine范式以实现个性化。PREFINE从用户的互动历史中构建一个伪用户代理，并生成用户特定的rubrics（评估标准）。

Result: 在PerDOC和PerMPST故事数据集上进行了综合评估。自动评估表明，PREFINE比基线方法获得了更高的胜率和统计上显著的分数，且不影响一般的故事质量。

Conclusion: 该方法在对话系统、教育和推荐等更广泛的应用中具有实现高效个性化的潜力。

Abstract: While recent advances in Large Language Models (LLMs) have improved the
quality of creative text generation, significant challenges remain in producing
personalized stories that reflect individual user preferences. Conventional
approaches rely on explicit feedback or fine-tuning, which presents practical
issues regarding user burden, data collection, computational costs, and
privacy. In this work, we propose PREFINE (Persona-and-Rubric Guided
Critique-and-Refine), a novel framework that extends the Critique-and-Refine
paradigm to personalization. PREFINE constructs a pseudo-user agent from a
user's interaction history and generates user-specific rubrics (evaluation
criteria). By having this agent critique and refine outputs on the user's
behalf based on these tailored rubrics, our method achieves personalized
generation without requiring parameter updates or direct user feedback. We
conducted a comprehensive evaluation on the PerDOC and PerMPST story datasets.
We designed three baseline methods and several model variants to verify the
contribution of each component of our framework. In automatic evaluations
(LLM-as-a-Judge), PREFINE achieved higher win rates and statistically
significant scores than the baselines, without compromising general story
quality. Analysis of the model variants confirmed that both the pseudo-user
agent and the user-specific rubrics are crucial for enhancing personalization
performance. Beyond story generation, our approach holds potential for enabling
efficient personalization in broader applications, such as dialogue systems,
education, and recommendation.

</details>


### [29] [SIGN: Schema-Induced Games for Naming](https://arxiv.org/abs/2510.21855)
*Ryan Zhang,Herbert Woisetscläger*

Main category: cs.AI

TL;DR: 本文提出了一种名为 Schema-Induced Games for Naming (SIGN) 的命名游戏，旨在研究轻量级结构如何引导惯例形成。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的人工智能系统正在解决日益复杂的问题，这些问题通常通过大型语言模型 (LLM) 代理之间的交互来完成。当这些代理产生不一致的约定，协调可能会崩溃。因此，协作编码和分布式规划等应用需要可靠、一致的通信，并且随着系统增长，可扩展性是核心问题。

Method: 将模式诱导通信与无约束自然语言进行比较。

Result: 与无约束自然语言相比，模式诱导通信的收敛速度更快，协议一致性提高了 5.8 倍。

Conclusion: 研究结果表明，最小的结构可以作为有效多智能体协调的简单控制旋钮，并指向命名游戏之外的更广泛应用。

Abstract: Real-world AI systems are tackling increasingly complex problems, often
through interactions among large language model (LLM) agents. When these agents
develop inconsistent conventions, coordination can break down. Applications
such as collaborative coding and distributed planning therefore require
reliable, consistent communication, and scalability is a central concern as
systems grow. We introduce Schema-Induced Games for Naming (SIGN), a naming
game that examines how lightweight structure can steer convention formation. We
compare schema-induced communication to unconstrained natural language and find
faster convergence with up to 5.8x higher agreement. These results suggest that
minimal structure can act as a simple control knob for efficient multi-agent
coordination, pointing toward broader applications beyond the naming game.

</details>


### [30] [Capability Ceilings in Autoregressive Language Models: Empirical Evidence from Knowledge-Intensive Tasks](https://arxiv.org/abs/2510.21866)
*Javier Marín*

Main category: cs.AI

TL;DR: Decoder-only语言模型在知识密集型任务中存在能力上限，扩展参数规模并不能显著提高准确率。


<details>
  <summary>Details</summary>
Motivation: 研究OPT和Pythia模型家族在知识密集型任务中的性能表现，揭示其扩展规模的收益递减现象。

Method: 系统评估OPT和Pythia模型家族（70M-30B参数）在知识检索和MMLU数学基准测试上的性能，并通过注意力干预实验分析模型行为。

Result: 知识检索任务准确率提升不明显，MMLU数学基准测试准确率停滞在19-20%，而算术等程序性任务则表现出常规的随规模扩展而性能提升的现象。注意力干预实验表明模型对扰动高度敏感。

Conclusion: 对于使用OPT和Pythia架构的知识密集型应用，参数扩展超过1-2B意义不大。这些发现量化了这些模型家族中特定于能力的扩展失败，为资源分配决策提供信息。这些模式是否反映了解码器专用架构的基本约束或特定于实现的限制仍然是一个悬而未决的问题，需要在不同的架构方法中进行调查。

Abstract: We document empirical capability ceilings in decoder-only autoregressive
language models across knowledge-intensive tasks. Systematic evaluation of OPT
and Pythia model families (70M-30B parameters, spanning 240 times scaling)
reveals that knowledge retrieval tasks show negligible accuracy improvement
despite smooth loss reduction. On MMLU mathematics benchmarks, accuracy remains
flat at 19-20% (below 25% random chance) across all scales while cross-entropy
loss decreases by 31%. In contrast, procedural tasks like arithmetic show
conventional scaling where both metrics improve together. Attention
intervention experiments reveal high sensitivity to perturbation: swapping
attention patterns between models causes catastrophic performance collapse
(complete accuracy loss) rather than graceful degradation. These measurements
have immediate engineering implications: for knowledge-intensive applications
using OPT and Pythia architectures, parameter scaling beyond 1-2B offers
minimal accuracy gains despite continued loss improvement. Our findings
quantify capability-specific scaling failures in these model families to inform
resource allocation decisions. Whether these patterns reflect fundamental
constraints of decoder-only architectures or implementation-specific
limitations remains an open question requiring investigation across diverse
architectural approaches.

</details>


### [31] [GeoThought: A Dataset for Enhancing Mathematical Geometry Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.21881)
*Nannan Shi,Chuanyu Qin,Shipeng Song,Man Luo*

Main category: cs.AI

TL;DR: 大型语言模型在基于文本的数学问题解决中表现出强大的推理能力。然而，当应用于视觉推理任务，特别是几何问题解决时，它们的性能会大幅下降，因为几何问题提出了独特的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏足够的规模、多样性和显式推理轨迹，从而阻碍了有效的模型训练。几何的内在复杂性需要详细的图像理解和多步骤推理。

Method: 我们开发了 GeoThoughts 数据集，这是一个综合的几何推理语料库，包含两个子集：Geo-Thought-6K（包含 6,243 个样本）及其增强版本 Geo-Thought-Augmented-10K（包含 10,834 个样本）。

Result: 我们的模型在几何任务中优于现有基准，表明使用我们的 Chain-of-Thought 数据集进行训练可以提高领域内和领域外环境中的几何推理能力。

Conclusion: 通过调用 CoT 来纠正这些错误，模型可以产生正确的答案。错误主要来自对数学概念的不正确解释或空间误判。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities
in text-based mathematical problem solving; however, when adapted to visual
reasoning tasks, particularly geometric problem solving, their performance
substantially declines because geometric problems present unique challenges.
Specifically, these challenges stem from two key factors: first, the intrinsic
complexity of geometry requiring detailed image comprehension and multi-step
reasoning, and second, the limitations of existing datasets which lack
sufficient scale, diversity, and explicit reasoning traces, consequently
hindering effective model training. To address these challenges, we developed
the GeoThoughts dataset, a comprehensive geometric reasoning corpus with two
subsets: Geo-Thought-6K with 6,243 samples and its augmented version
Geo-Thought-Augmented-10K containing 10,834 samples. Each entry includes visual
descriptions, step-by-step solutions, explicit reasoning chains, reflection
steps, and final answers. Using this dataset, we developed GeoThought-MLLM, a
mathematical reasoning multimodal model that generates detailed thinking
processes during problem-solving. Our model outperforms existing benchmarks in
geometric tasks, demonstrating that training with our Chain-of-Thought dataset
improves geometric reasoning capabilities across both in-domain and
out-of-domain settings. Finally, we analyze failure cases and observe that
errors primarily arise from incorrect interpretation of mathematical concepts
or spatial misjudgment. By invoking CoT to correct these mistakes, the model
produces correct answers.

</details>


### [32] [Exploration through Generation: Applying GFlowNets to Structured Search](https://arxiv.org/abs/2510.21886)
*Mark Phillip Matovic*

Main category: cs.AI

TL;DR: 本研究应用生成流网络（GFlowNets）解决图优化问题，包括旅行商问题、最小生成树和最短路径。


<details>
  <summary>Details</summary>
Motivation: 探索生成模型解决组合优化问题的能力，并克服传统算法在计算上的局限性。

Method: 使用轨迹平衡损失训练 GFlowNets，使其能够按奖励函数比例采样解决方案，并依次构建解决方案。

Result: 实验表明，GFlowNets 能够学习找到最优解，并在不同大小的基准实例上生成与经典算法匹配的解决方案。

Conclusion: 生成模型可以通过学习策略解决组合优化问题，且 GFlowNets 具有计算可扩展性，有望扩展到传统精确方法不可行的大规模问题实例。

Abstract: This work applies Generative Flow Networks (GFlowNets) to three graph
optimization problems: the Traveling Salesperson Problem, Minimum Spanning
Tree, and Shortest Path. GFlowNets are generative models that learn to sample
solutions proportionally to a reward function. The models are trained using the
Trajectory Balance loss to build solutions sequentially, selecting edges for
spanning trees, nodes for paths, and cities for tours. Experiments on benchmark
instances of varying sizes show that GFlowNets learn to find optimal solutions.
For each problem type, multiple graph configurations with different numbers of
nodes were tested. The generated solutions match those from classical
algorithms (Dijkstra for shortest path, Kruskal for spanning trees, and exact
solvers for TSP). Training convergence depends on problem complexity, with the
number of episodes required for loss stabilization increasing as graph size
grows. Once training converges, the generated solutions match known optima from
classical algorithms across the tested instances. This work demonstrates that
generative models can solve combinatorial optimization problems through learned
policies. The main advantage of this learning-based approach is computational
scalability: while classical algorithms have fixed complexity per instance,
GFlowNets amortize computation through training. With sufficient computational
resources, the framework could potentially scale to larger problem instances
where classical exact methods become infeasible.

</details>


### [33] [Computational Hardness of Reinforcement Learning with Partial $q^π$-Realizability](https://arxiv.org/abs/2510.21888)
*Shayan Karimi,Xiaoqi Tan*

Main category: cs.AI

TL;DR: 本文研究了强化学习在一种新的线性函数逼近机制（称为部分$q^{\pi}$-可实现性）中的计算复杂性，证明了在这种情况下学习一个$\epsilon$-最优策略在计算上是困难的。


<details>
  <summary>Details</summary>
Motivation: 目标是在预定义的策略集$\Pi$中学习一个$\epsilon$-最优策略，假设$\Pi$中所有策略的价值函数都是线性可实现的。这个框架的假设比$q^{\pi}$-可实现性中的假设弱，但比$q^*$-可实现性中的假设强，提供了一个函数逼近自然产生的实用模型。

Method: 通过将两个复杂性问题$\delta$-Max-3SAT和$\delta$-Max-3SAT(b)归约到GLinear-$\\kappa$-RL（贪婪策略）和SLinear-$\\kappa$-RL（softmax策略）的实例，证明了学习一个$\epsilon$-最优策略是NP-hard的。

Result: 发现在参数化的贪婪策略集（argmax）下是NP-hard的，并且表明 - 除非NP = RP - 当策略集包含softmax策略时，在随机指数时间假设下，存在一个指数下界（在特征向量维度中）。

Conclusion: 我们的发现表明，与在生成访问模型下的$q^{\pi}$-可实现性相比，在部分$q^{\pi}$-可实现性中通常无法获得积极的计算结果。

Abstract: This paper investigates the computational complexity of reinforcement
learning in a novel linear function approximation regime, termed partial
$q^{\pi}$-realizability. In this framework, the objective is to learn an
$\epsilon$-optimal policy with respect to a predefined policy set $\Pi$, under
the assumption that all value functions for policies in $\Pi$ are linearly
realizable. The assumptions of this framework are weaker than those in
$q^{\pi}$-realizability but stronger than those in $q^*$-realizability,
providing a practical model where function approximation naturally arises. We
prove that learning an $\epsilon$-optimal policy in this setting is
computationally hard. Specifically, we establish NP-hardness under a
parameterized greedy policy set (argmax) and show that - unless NP = RP - an
exponential lower bound (in feature vector dimension) holds when the policy set
contains softmax policies, under the Randomized Exponential Time Hypothesis.
Our hardness results mirror those in $q^*$-realizability and suggest
computational difficulty persists even when $\Pi$ is expanded beyond the
optimal policy. To establish this, we reduce from two complexity problems,
$\delta$-Max-3SAT and $\delta$-Max-3SAT(b), to instances of GLinear-$\kappa$-RL
(greedy policy) and SLinear-$\kappa$-RL (softmax policy). Our findings indicate
that positive computational results are generally unattainable in partial
$q^{\pi}$-realizability, in contrast to $q^{\pi}$-realizability under a
generative access model.

</details>


### [34] [Performance Trade-offs of Optimizing Small Language Models for E-Commerce](https://arxiv.org/abs/2510.21970)
*Josip Tomo Licardo,Nikola Tankovic*

Main category: cs.AI

TL;DR: 本文研究了使用小型开源模型替代大型商业模型在电商意图识别任务中的可行性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言理解和生成任务中表现出色，但部署成本高昂。本文旨在探索小型开源模型作为资源高效替代方案的可行性。

Method: 本文使用QLoRA在合成数据集上微调了一个十亿参数的Llama 3.2模型，并应用了后训练量化技术，创建了GPU优化（GPTQ）和CPU优化（GGUF）版本。

Result: 结果表明，该专用模型达到了99%的准确率，与更大的GPT-4.1模型性能相当。硬件相关的性能分析揭示了关键的权衡：GPTQ降低了VRAM使用率，但在旧的GPU架构上降低了推理速度；GGUF格式在CPU上实现了高达18倍的推理吞吐量加速和超过90%的内存消耗降低。

Conclusion: 小型、经过适当优化的开源模型不仅可行，而且更适合领域特定的应用，以一小部分的计算成本提供最先进的准确性。

Abstract: Large Language Models (LLMs) offer state-of-the-art performance in natural
language understanding and generation tasks. However, the deployment of leading
commercial models for specialized tasks, such as e-commerce, is often hindered
by high computational costs, latency, and operational expenses. This paper
investigates the viability of smaller, open-weight models as a
resource-efficient alternative. We present a methodology for optimizing a
one-billion-parameter Llama 3.2 model for multilingual e-commerce intent
recognition. The model was fine-tuned using Quantized Low-Rank Adaptation
(QLoRA) on a synthetically generated dataset designed to mimic real-world user
queries. Subsequently, we applied post-training quantization techniques,
creating GPU-optimized (GPTQ) and CPU-optimized (GGUF) versions. Our results
demonstrate that the specialized 1B model achieves 99% accuracy, matching the
performance of the significantly larger GPT-4.1 model. A detailed performance
analysis revealed critical, hardware-dependent trade-offs: while 4-bit GPTQ
reduced VRAM usage by 41%, it paradoxically slowed inference by 82% on an older
GPU architecture (NVIDIA T4) due to dequantization overhead. Conversely, GGUF
formats on a CPU achieved a speedup of up to 18x in inference throughput and a
reduction of over 90% in RAM consumption compared to the FP16 baseline. We
conclude that small, properly optimized open-weight models are not just a
viable but a more suitable alternative for domain-specific applications,
offering state-of-the-art accuracy at a fraction of the computational cost.

</details>


### [35] [Distribution Shift Alignment Helps LLMs Simulate Survey Response Distributions](https://arxiv.org/abs/2510.21977)
*Ji Huang,Mengfei Li,Shuai Shao*

Main category: cs.AI

TL;DR: 提出了一种名为分布偏移对齐 (DSA) 的两阶段微调方法，该方法通过学习分布变化而非拟合训练数据，使结果更接近真实分布，从而更有效地模拟调查。


<details>
  <summary>Details</summary>
Motivation: 现有零样本方法存在prompt敏感性和准确率低的问题，而传统微调方法 основном 拟合训练集分布，难以产生比训练集本身更准确的结果，这偏离了使用 LLM 模拟调查回复的最初目标。

Method: 引入分布偏移对齐 (DSA)，这是一种两阶段微调方法，可以对齐输出分布和跨不同背景的分布偏移。

Result: 在五个公共调查数据集上，DSA 始终优于其他方法。DSA 将所需的真实数据减少了 53.48-69.12%。

Conclusion: DSA 在调查模拟中具有有效性和效率。

Abstract: Large language models (LLMs) offer a promising way to simulate human survey
responses, potentially reducing the cost of large-scale data collection.
However, existing zero-shot methods suffer from prompt sensitivity and low
accuracy, while conventional fine-tuning approaches mostly fit the training set
distributions and struggle to produce results more accurate than the training
set itself, which deviates from the original goal of using LLMs to simulate
survey responses. Building on this observation, we introduce Distribution Shift
Alignment (DSA), a two-stage fine-tuning method that aligns both the output
distributions and the distribution shifts across different backgrounds. By
learning how these distributions change rather than fitting training data, DSA
can provide results substantially closer to the true distribution than the
training data. Empirically, DSA consistently outperforms other methods on five
public survey datasets. We further conduct a comprehensive comparison covering
accuracy, robustness, and data savings. DSA reduces the required real data by
53.48-69.12%, demonstrating its effectiveness and efficiency in survey
simulation.

</details>


### [36] [Foundation of Intelligence: Review of Math Word Problems from Human Cognition Perspective](https://arxiv.org/abs/2510.21999)
*Zhenya Huang,Jiayu Liu,Xin Lin,Zhiyuan Ma,Shangzi Xue,Tong Xiao,Qi Liu,Yee Whye Teh,Enhong Chen*

Main category: cs.AI

TL;DR: 本文全面回顾了过去十年有影响力的数学文字题（MWP）研究，重点关注人类推理认知，并对现有方法进行了综合比较。


<details>
  <summary>Details</summary>
Motivation: 旨在通过模拟类人认知能力来提升AI的推理能力，但该领域缺乏系统的MWP调查分类以及对当前发展趋势的讨论。

Method: 通过人类认知的视角，总结了MWP求解的5个关键认知能力，包括问题理解、逻辑组织、联想记忆、批判性思维和知识学习。重点关注这些能力，回顾了近10年的两种主流MWP模型：神经网络求解器和基于LLM的求解器。

Result: 重新运行了所有代表性的MWP求解器，并补充了它们在5个主流基准测试上的性能，以进行统一比较。

Conclusion: 首次从人类推理认知的角度全面分析了过去十年有影响力的MWP研究，并对现有方法进行了综合比较。

Abstract: Math word problem (MWP) serves as a fundamental research topic in artificial
intelligence (AI) dating back to 1960s. This research aims to advance the
reasoning abilities of AI by mirroring the human-like cognitive intelligence.
The mainstream technological paradigm has evolved from the early rule-based
methods, to deep learning models, and is rapidly advancing towards large
language models. However, the field still lacks a systematic taxonomy for the
MWP survey along with a discussion of current development trends. Therefore, in
this paper, we aim to comprehensively review related research in MWP solving
through the lens of human cognition, to demonstrate how recent AI models are
advancing in simulating human cognitive abilities. Specifically, we summarize 5
crucial cognitive abilities for MWP solving, including Problem Understanding,
Logical Organization, Associative Memory, Critical Thinking, and Knowledge
Learning. Focused on these abilities, we review two mainstream MWP models in
recent 10 years: neural network solvers, and LLM based solvers, and discuss the
core human-like abilities they demonstrated in their intricate problem-solving
process. Moreover, we rerun all the representative MWP solvers and supplement
their performance on 5 mainstream benchmarks for a unified comparison. To the
best of our knowledge, this survey first comprehensively analyzes the
influential MWP research of the past decade from the perspective of human
reasoning cognition and provides an integrative overall comparison across
existing approaches. We hope it can inspire further research in AI reasoning.
Our repository is released on https://github.com/Ljyustc/FoI-MWP.

</details>


### [37] [LightAgent: Mobile Agentic Foundation Models](https://arxiv.org/abs/2510.22009)
*Yangqin Jiang,Chao Huang*

Main category: cs.AI

TL;DR: LightAgent是一个移动代理基础模型解决方案，它利用设备-云协作来利用设备上模型的成本效益和云模型的高能力，同时避免它们的缺点。


<details>
  <summary>Details</summary>
Motivation: 移动GUI代理面临一个关键困境：真正的设备上模型（4B或更小）缺乏足够的性能，而有能力的模型（从7B开始）要么太大而无法进行移动部署，要么成本过高（例如，仅限云的闭源MLLM）。

Method: LightAgent通过在合成GUI数据上进行两阶段SFT->GRPO训练来增强Qwen2.5-VL-3B，以实现强大的决策，集成了一种高效的长推理机制，以在资源紧张的情况下利用历史交互，并默认为设备上执行-仅通过实时复杂性评估将具有挑战性的子任务升级到云。

Result: 在在线AndroidLab基准和各种应用程序上的实验表明，LightAgent与更大的模型相匹配或接近，同时显着降低了云成本。

Conclusion: LightAgent是一个很有前途的移动代理基础模型解决方案，它可以在具有成本效益的方式下实现与大型模型相媲美的性能。

Abstract: With the advancement of multimodal large language models (MLLMs), building
GUI agent systems has become an increasingly promising direction-especially for
mobile platforms, given their rich app ecosystems and intuitive touch
interactions. Yet mobile GUI agents face a critical dilemma: truly on-device
models (4B or smaller) lack sufficient performance, while capable models
(starting from 7B) are either too large for mobile deployment or prohibitively
costly (e.g., cloud-only closed-source MLLMs). To resolve this, we propose
LightAgent, a mobile agentic foundation model solution that leverages
device-cloud collaboration to tap the cost-efficiency of on-device models and
the high capability of cloud models, while avoiding their drawbacks.
Specifically, LightAgent enhances Qwen2.5-VL-3B via two-stage SFT->GRPO
training on synthetic GUI data for strong decision-making, integrates an
efficient long-reasoning mechanism to utilize historical interactions under
tight resources, and defaults to on-device execution-only escalating
challenging subtasks to the cloud via real-time complexity assessment.
Experiments on the online AndroidLab benchmark and diverse apps show LightAgent
matches or nears larger models, with a significant reduction in cloud costs.

</details>


### [38] [LLM-AR: LLM-powered Automated Reasoning Framework](https://arxiv.org/abs/2510.22034)
*Rick Chen,Joseph Ternasky,Aaron Ontoyin Yin,Xianling Mu,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TL;DR: 本文提出了一种名为 LLM-AR 的新框架，用于预测初创公司的成功。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在识别模式和推理方面表现出色，但准确性不足，阻碍了其在高风险决策应用中的应用。本文从风险投资的角度研究了这一问题，通过预测早期初创公司的创始人特征来预测其成功。

Method: 该框架首先将 LLM 生成的启发式方法提炼成概率规则，然后通过迭代策略演化循环，结合关联规则挖掘来逐步完善预测规则。

Result: LLM-AR 在未见过的测试集上实现了 59.5% 的精确率和 8.7% 的召回率，精确率是随机基线的 5.9 倍。

Conclusion: 该框架具有可解释性和可调性，并有希望扩展到其他领域。

Abstract: Large language models (LLMs) can already identify patterns and reason
effectively, yet their variable accuracy hampers adoption in high-stakes
decision-making applications. In this paper, we study this issue from a venture
capital perspective by predicting idea-stage startup success based on founder
traits. (i) To build a reliable prediction model, we introduce LLM-AR, a
pipeline inspired by neural-symbolic systems that distils LLM-generated
heuristics into probabilistic rules executed by the ProbLog automated-reasoning
engine. (ii) An iterative policy-evolution loop incorporates association-rule
mining to progressively refine the prediction rules.
  On unseen folds, LLM-AR achieves 59.5% precision and 8.7% recall, 5.9x the
random baseline precision, while exposing every decision path for human
inspection. The framework is interpretable and tunable via hyperparameters,
showing promise to extend into other domains.

</details>


### [39] [Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability](https://arxiv.org/abs/2510.22039)
*Po-Chen Kuo,Han Hou,Will Dabney,Edgar Y. Walker*

Main category: cs.AI

TL;DR: 元强化学习智能体难以学习到紧凑、可解释的贝叶斯最优置信状态，这限制了智能体的适应性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 受神经科学中预测编码的启发，研究整合自监督预测编码模块到元强化学习中是否可以促进学习贝叶斯最优表示。

Method: 通过状态机模拟，对比了带有预测模块的元强化学习和传统元强化学习。

Result: 带有预测模块的元强化学习始终生成更可解释的表示，更接近贝叶斯最优置信状态，并且在需要主动信息寻求的任务中，只有带有预测模块的元强化学习成功学习到最优表示和策略。更好的表示学习带来更好的泛化能力。

Conclusion: 预测学习可以作为智能体在不完全可观测环境中进行有效表示学习的指导原则。

Abstract: Learning a compact representation of history is critical for planning and
generalization in partially observable environments. While meta-reinforcement
learning (RL) agents can attain near Bayes-optimal policies, they often fail to
learn the compact, interpretable Bayes-optimal belief states. This
representational inefficiency potentially limits the agent's adaptability and
generalization capacity. Inspired by predictive coding in neuroscience--which
suggests that the brain predicts sensory inputs as a neural implementation of
Bayesian inference--and by auxiliary predictive objectives in deep RL, we
investigate whether integrating self-supervised predictive coding modules into
meta-RL can facilitate learning of Bayes-optimal representations. Through state
machine simulation, we show that meta-RL with predictive modules consistently
generates more interpretable representations that better approximate
Bayes-optimal belief states compared to conventional meta-RL across a wide
variety of tasks, even when both achieve optimal policies. In challenging tasks
requiring active information seeking, only meta-RL with predictive modules
successfully learns optimal representations and policies, whereas conventional
meta-RL struggles with inadequate representation learning. Finally, we
demonstrate that better representation learning leads to improved
generalization. Our results strongly suggest the role of predictive learning as
a guiding principle for effective representation learning in agents navigating
partial observability.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [40] [Determining Window Sizes using Species Estimation for Accurate Process Mining over Streams](https://arxiv.org/abs/2510.22314)
*Christian Imenkamp,Martin Kabierski,Hendrik Reiter,Matthias Weidlich,Wilhelm Hasselbring,Agnes Koschmider*

Main category: cs.DB

TL;DR: 提出了一种新的流式过程挖掘方法，通过调整窗口大小来解决静态窗口大小的局限性。


<details>
  <summary>Details</summary>
Motivation: 静态、固定窗口大小导致不准确的表示，从而在分析中引入偏差。

Method: 基于生物多样性研究中物种估计的样本代表性估计器，动态确定合适的窗口大小。

Result: 在真实世界数据集上的评估结果表明，在准确性和对概念漂移的鲁棒性方面，优于采用静态窗口大小的现有方法。

Conclusion: 提出了一种动态调整窗口大小的流式过程挖掘方法，实验证明其有效性。

Abstract: Streaming process mining deals with the real-time analysis of event streams.
A common approach for it is to adopt windowing mechanisms that select event
data from a stream for subsequent analysis. However, the size of these windows
denotes a crucial parameter, as it influences the representativeness of the
window content and, by extension, of the analysis results. Given that process
dynamics are subject to changes and potential concept drift, a static, fixed
window size leads to inaccurate representations that introduce bias in the
analysis. In this work, we present a novel approach for streaming process
mining that addresses these limitations by adjusting window sizes.
Specifically, we dynamically determine suitable window sizes based on
estimators for the representativeness of samples as developed for species
estimation in biodiversity research. Evaluation results on real-world data sets
show improvements over existing approaches that adopt static window sizes in
terms of accuracy and robustness to concept drifts.

</details>


### [41] [Dynamically Detect and Fix Hardness for Efficient Approximate Nearest Neighbor Search](https://arxiv.org/abs/2510.22316)
*Zhiyuan Hua,Qiji Mo,Zebin Yao,Lixiao Cui,Xiaoguang Liu,Gang Wang,Zijing Wei,Xinyu Liu,Tianxiao Tang,Shaozhi Liu,Lin Qu*

Main category: cs.DB

TL;DR: 这篇论文提出了一种新的近似最近邻搜索（ANNS）方法，通过动态识别和修复图结构中的缺陷区域来提高搜索精度，尤其是在分布外（OOD）场景中。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图的ANNS方法在处理特定查询时，尤其是在OOD场景中，精度会显著下降。RoarGraph方法试图通过构建基数据和历史查询之间的二分图来解决这个问题，但存在理论支持不足、需要大量历史查询以及重建代价高等限制。

Method: 该论文首先提出了“逃逸硬度”的概念来评估查询周围图结构的质量。然后，将图搜索分为两个阶段，并基于逃逸硬度动态识别和修复缺陷区域。提出了可达性修复（RFix）来增强关键节点的可导航性，以及邻域图缺陷修复（NGFix）来提高查询密集分布区域的图连通性。

Result: 实验结果表明，该方法在真实世界数据集上优于其他最先进的方法，对于OOD查询，在99%召回率下，搜索速度比RoarGraph快2.25倍，比HNSW快6.88倍。此外，索引构建速度比RoarGraph快2.35-9.02倍。

Conclusion: 该论文提出了一种有效的ANNS方法，通过动态修复图结构中的缺陷区域，显著提高了搜索精度和速度，尤其是在OOD场景中。

Abstract: Approximate Nearest Neighbor Search (ANNS) has become a fundamental component
in many real-world applications. Among various ANNS algorithms, graph-based
methods are state-of-the-art. However, ANNS often suffers from a significant
drop in accuracy for certain queries, especially in Out-of-Distribution (OOD)
scenarios. To address this issue, a recent approach named RoarGraph constructs
a bipartite graph between the base data and historical queries to bridge the
gap between two different distributions. However, it suffers from some
limitations: (1) Building a bipartite graph between two distributions lacks
theoretical support, resulting in the query distribution not being effectively
utilized by the graph index. (2) Requires a sufficient number of historical
queries before graph construction and suffers from high construction times. (3)
When the query workload changes, it requires reconstruction to maintain high
search accuracy.
  In this paper, we first propose Escape Hardness, a metric to evaluate the
quality of the graph structure around the query. Then we divide the graph
search into two stages and dynamically identify and fix defective graph regions
in each stage based on Escape Hardness. (1) From the entry point to the
vicinity of the query. We propose Reachability Fixing (RFix), which enhances
the navigability of some key nodes. (2) Searching within the vicinity of the
query. We propose Neighboring Graph Defects Fixing (NGFix) to improve graph
connectivity in regions where queries are densely distributed. The results of
extensive experiments show that our method outperforms other state-of-the-art
methods on real-world datasets, achieving up to 2.25x faster search speed for
OOD queries at 99% recall compared with RoarGraph and 6.88x faster speed
compared with HNSW. It also accelerates index construction by 2.35-9.02x
compared to RoarGraph.

</details>


### [42] [A Survey of Data Agents: Emerging Paradigm or Overstated Hype?](https://arxiv.org/abs/2510.23587)
*Yizhang Zhu,Liangwei Wang,Chenyu Yang,Xiaotian Lin,Boyan Li,Wei Zhou,Xinyu Liu,Zhangyang Peng,Tianqi Luo,Yu Li,Chengliang Chai,Chong Chen,Shimin Di,Ju Fan,Ji Sun,Nan Tang,Fugee Tsung,Jiannan Wang,Chenglin Wu,Yanwei Xu,Shaolei Zhang,Yong Zhang,Xuanhe Zhou,Guoliang Li,Yuyu Luo*

Main category: cs.DB

TL;DR: 本文提出了一个数据代理的分层分类法，旨在解决当前数据代理领域术语含糊不清和应用不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 当前数据代理领域存在术语含糊不清和应用不一致的问题，导致用户期望不匹配、责任划分不明确以及行业增长受阻。

Method: 本文受到SAE J3016驾驶自动化标准的启发，提出了一个包含六个层级的数据代理分层分类法，从手动操作（L0）到完全自主的数据代理（L5）。

Result: 本文通过该分类法，对现有研究进行了结构化综述，涵盖了数据管理、准备和分析等领域的专业数据代理，以及新兴的、具有增强自主性的通用系统。分析了推动数据代理发展的关键进化飞跃和技术差距，特别是在L2到L3的过渡阶段。

Conclusion: 本文提出了一个前瞻性的路线图，展望了主动的、生成式数据代理的出现。

Abstract: The rapid advancement of large language models (LLMs) has spurred the
emergence of data agents--autonomous systems designed to orchestrate Data + AI
ecosystems for tackling complex data-related tasks. However, the term "data
agent" currently suffers from terminological ambiguity and inconsistent
adoption, conflating simple query responders with sophisticated autonomous
architectures. This terminological ambiguity fosters mismatched user
expectations, accountability challenges, and barriers to industry growth.
Inspired by the SAE J3016 standard for driving automation, this survey
introduces the first systematic hierarchical taxonomy for data agents,
comprising six levels that delineate and trace progressive shifts in autonomy,
from manual operations (L0) to a vision of generative, fully autonomous data
agents (L5), thereby clarifying capability boundaries and responsibility
allocation. Through this lens, we offer a structured review of existing
research arranged by increasing autonomy, encompassing specialized data agents
for data management, preparation, and analysis, alongside emerging efforts
toward versatile, comprehensive systems with enhanced autonomy. We further
analyze critical evolutionary leaps and technical gaps for advancing data
agents, especially the ongoing L2-to-L3 transition, where data agents evolve
from procedural execution to autonomous orchestration. Finally, we conclude
with a forward-looking roadmap, envisioning the advent of proactive, generative
data agents.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [43] [Improving E-commerce Search with Category-Aligned Retrieval](https://arxiv.org/abs/2510.21711)
*Rauf Aliev*

Main category: cs.IR

TL;DR: 本文提出了一种类别对齐检索系统 (CARS)，通过预测用户查询的产品类别并提升该类别内的产品来提高搜索相关性。


<details>
  <summary>Details</summary>
Motivation: 传统电商搜索系统难以解决用户查询和产品目录之间的语义差距。

Method: 提出了一种从查询嵌入创建“可训练类别原型”的新方法，并使用 all-MiniLM-L6-v2 和 OpenAI 的 text-embedding-ada-002 两个模型进行评估。

Result: 离线评估表明，该方法非常有效，使用 OpenAI 模型训练后，Top-3 类别预测准确率从 43.8% 提高到 83.2%。但端到端模拟显示，在复杂的检索管道中盲目应用类别提升会负面影响搜索相关性指标。

Conclusion: 结果强调需要置信度感知和自适应集成策略。

Abstract: Traditional e-commerce search systems often struggle with the semantic gap
between user queries and product catalogs. In this paper, we propose a
Category-Aligned Retrieval System (CARS) that improves search relevance by
first predicting the product category from a user's query and then boosting
products within that category. We introduce a novel method for creating
"Trainable Category Prototypes" from query embeddings. We evaluate this method
with two models: a lightweight all-MiniLM-L6-v2 and OpenAI's
text-embedding-ada-002. Our offline evaluation shows this method is highly
effective, with the OpenAI model increasing Top-3 category prediction accuracy
from a zero-shot baseline of 43.8% to 83.2% after training. The end-to-end
simulation, however, highlights the limitations of blindly applying category
boosts in a complex retrieval pipeline: while accuracy is high, naive
integration can negatively affect search relevance metrics such as nDCG@10. We
argue that this is partly due to dataset-specific ambiguities (e.g., polysemous
queries in the Amazon ESCI corpus) and partly due to the sensitivity of
retrieval systems to over-constraining filters. Crucially, these results do not
diminish the value of the approach; rather, they emphasize the need for
confidence-aware and adaptive integration strategies.

</details>


### [44] [DecoupleSearch: Decouple Planning and Search via Hierarchical Reward Modeling](https://arxiv.org/abs/2510.21712)
*Hao Sun,Zile Qiao,Bo Wang,Guoxin Chen,Yingyan Hou,Yong Jiang,Pengjun Xie,Fei Huang,Yan Zhang*

Main category: cs.IR

TL;DR: Agentic RAG存在规划和搜索问题，缺乏中间步骤的监督，以及候选空间过大


<details>
  <summary>Details</summary>
Motivation: 为了增强大型语言模型（LLM），检索增强生成（RAG）系统已经成为一种关键方法，通过动态整合外部知识。为了进一步提高RAG的灵活性，Agentic RAG将自主代理引入工作流程

Method: 我们提出了DecoupleSearch，这是一个新颖的框架，它使用双重价值模型将规划和搜索过程分离，从而能够独立优化计划推理和搜索基础。我们的方法构建了一个推理树，其中每个节点代表计划和搜索步骤。我们利用蒙特卡洛树搜索来评估每个步骤的质量。在推理过程中，分层束搜索使用双重价值模型迭代地细化规划和搜索候选对象

Result: 在不同参数大小的策略模型上进行了大量实验，证明了我们方法的有效性

Conclusion: DecoupleSearch通过解耦规划和搜索过程，使用双重价值模型独立优化计划推理和搜索基础，从而有效解决了Agentic RAG中的挑战

Abstract: Retrieval-Augmented Generation (RAG) systems have emerged as a pivotal
methodology for enhancing Large Language Models (LLMs) through the dynamic
integration of external knowledge. To further improve RAG's flexibility,
Agentic RAG introduces autonomous agents into the workflow. However, Agentic
RAG faces several challenges: (1) the success of each step depends on both
high-quality planning and accurate search, (2) the lack of supervision for
intermediate reasoning steps, and (3) the exponentially large candidate space
for planning and searching. To address these challenges, we propose
DecoupleSearch, a novel framework that decouples planning and search processes
using dual value models, enabling independent optimization of plan reasoning
and search grounding. Our approach constructs a reasoning tree, where each node
represents planning and search steps. We leverage Monte Carlo Tree Search to
assess the quality of each step. During inference, Hierarchical Beam Search
iteratively refines planning and search candidates with dual value models.
Extensive experiments across policy models of varying parameter sizes,
demonstrate the effectiveness of our method.

</details>


### [45] [asLLR: LLM based Leads Ranking in Auto Sales](https://arxiv.org/abs/2510.21713)
*Yin Sun,Yiwen Liu,Junjie Song,Chenyu Zhang,Xinyuan Zhang,Lingjie Liu,Siqi Chen,Yuji Cao*

Main category: cs.IR

TL;DR: This paper introduces asLLR, a LLM-based approach for ranking sales leads in the auto sales system. It improves sales efficiency by integrating CTR and QA losses to model tabular and natural language features.


<details>
  <summary>Details</summary>
Motivation: Traditional CTR prediction methods struggle with complex natural language features in CRM systems, limiting their effectiveness in sales lead ranking.

Method: The paper proposes asLLR, which integrates CTR loss and Question Answering (QA) loss within a decoder-only large language model architecture to model both tabular and natural language features.

Result: asLLR achieves an AUC of 0.8127, surpassing traditional CTR estimation methods by 0.0231. It also enhances CTR models when used for extracting text features by 0.0058. Online A/B testing showed a 9.5% increase in sales volume.

Conclusion: asLLR effectively models intricate patterns in commercial datasets and provides a valuable tool for business intelligence and operational decision-making in auto sales.

Abstract: In the area of commercial auto sales system, high-quality lead score
sequencing determines the priority of a sale's work and is essential for
optimizing the efficiency of the sales system. Since CRM (Customer Relationship
Management) system contains plenty of textual interaction features between
sales and customers, traditional techniques such as Click Through Rate (CTR)
prediction struggle with processing the complex information inherent in natural
language features, which limits their effectiveness in sales lead ranking.
Bridging this gap is critical for enhancing business intelligence and
decision-making. Recently, the emergence of large language models (LLMs) has
opened new avenues for improving recommendation systems, this study introduces
asLLR (LLM-based Leads Ranking in Auto Sales), which integrates CTR loss and
Question Answering (QA) loss within a decoder-only large language model
architecture. This integration enables the simultaneous modeling of both
tabular and natural language features. To verify the efficacy of asLLR, we
constructed an innovative dataset derived from the customer lead pool of a
prominent new energy vehicle brand, with 300,000 training samples and 40,000
testing samples. Our experimental results demonstrate that asLLR effectively
models intricate patterns in commercial datasets, achieving the AUC of 0.8127,
surpassing traditional CTR estimation methods by 0.0231. Moreover, asLLR
enhances CTR models when used for extracting text features by 0.0058. In
real-world sales scenarios, after rigorous online A/B testing, asLLR increased
the sales volume by about 9.5% compared to the traditional method, providing a
valuable tool for business intelligence and operational decision-making.

</details>


### [46] [Practice on Long Behavior Sequence Modeling in Tencent Advertising](https://arxiv.org/abs/2510.21714)
*Xian Hu,Ming Yue,Zhixiang Feng,Junwei Pan,Junjie Zhai,Ximei Wang,Xinrui Miao,Qian Li,Xun Liu,Shangyu Zhang,Letian Wang,Hua Lu,Zijian Zeng,Chen Cai,Wei Wang,Fei Xiong,Pengfei Xiong,Jintao Zhang,Zhiyuan Wu,Chunhui Zhang,Anan Liu,Jiulong You,Chao Deng,Yuekui Yang,Shudong Huang,Dapeng Liu,Haijie Gu*

Main category: cs.IR

TL;DR: 这篇论文提出了一种用于推荐系统的长序列建模方法，通过整合来自不同广告场景和内容领域的用户行为来构建统一的商业行为轨迹。


<details>
  <summary>Details</summary>
Motivation: 传统的推荐系统难以捕捉用户长期偏好，尤其是在广告领域，用户行为稀疏，单一广告域的数据难以构建长序列。

Method: 该方法包含两个阶段：搜索阶段和序列建模阶段。在搜索阶段，设计了分层硬搜索方法和解耦嵌入软搜索方法。在序列建模阶段，引入了解耦边信息时间兴趣网络（TIN）、目标解耦位置编码和目标解耦SASRec，以及堆叠TIN。

Result: 在腾讯的大规模广告平台上部署后，该方法带来了显著的性能提升：微信视频号GMV提升4.22%，微信朋友圈GMV提升1.96%。

Conclusion: 该论文提出的方法有效地解决了跨域或跨场景集成带来的特征分类差距、字段间干扰以及目标间干扰等问题，并在实际应用中取得了显著的成果。

Abstract: Long-sequence modeling has become an indispensable frontier in recommendation
systems for capturing users' long-term preferences. However, user behaviors
within advertising domains are inherently sparse, posing a significant barrier
to constructing long behavioral sequences using data from a single advertising
domain alone. This motivates us to collect users' behaviors not only across
diverse advertising scenarios, but also beyond the boundaries of the
advertising domain into content domains-thereby constructing unified commercial
behavior trajectories. This cross-domain or cross-scenario integration gives
rise to the following challenges: (1) feature taxonomy gaps between distinct
scenarios and domains, (2) inter-field interference arising from irrelevant
feature field pairs, and (3) target-wise interference in temporal and semantic
patterns when optimizing for different advertising targets. To address these
challenges, we propose several practical approaches within the two-stage
framework for long-sequence modeling. In the first (search) stage, we design a
hierarchical hard search method for handling complex feature taxonomy
hierarchies, alongside a decoupled embedding-based soft search to alleviate
conflicts between attention mechanisms and feature representation. In the
second (sequence modeling) stage, we introduce: (a) Decoupled Side Information
Temporal Interest Networks (TIN) to mitigate inter-field conflicts; (b)
Target-Decoupled Positional Encoding and Target-Decoupled SASRec to address
target-wise interference; and (c) Stacked TIN to model high-order behavioral
correlations. Deployed in production on Tencent's large-scale advertising
platforms, our innovations delivered significant performance gains: an overall
4.22% GMV lift in WeChat Channels and an overall 1.96% GMV increase in WeChat
Moments.

</details>


### [47] [Words to Waves: Emotion-Adaptive Music Recommendation System](https://arxiv.org/abs/2510.21724)
*Apoorva Chavali,Reeve Menezes*

Main category: cs.IR

TL;DR: 提出了一种新的音乐推荐框架，该框架利用 Wide and Deep Learning 架构的变体，该架构以直接从自然语言推断出的实时情绪状态作为输入，并推荐能够密切描绘情绪的歌曲。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统通常倾向于忽略情感背景，而依赖于历史收听模式或静态情绪标签。

Method: 该系统通过使用基于 Transformer 的嵌入来捕获用户提供的文本描述中的情感背景，这些嵌入经过微调以预测效价-唤醒的情感维度。该架构的深度组件利用这些嵌入来概括看不见的情感模式，而宽度组件通过交叉积特征有效地记忆用户-情感和情感-流派关联。

Result: 实验结果表明，个性化的音乐选择对用户的情绪产生积极影响，并显着提高情绪相关性。

Conclusion: 个性化的音乐选择可以积极影响用户的情绪，并显着提高情绪相关性。

Abstract: Current recommendation systems often tend to overlook emotional context and
rely on historical listening patterns or static mood tags. This paper
introduces a novel music recommendation framework employing a variant of Wide
and Deep Learning architecture that takes in real-time emotional states
inferred directly from natural language as inputs and recommends songs that
closely portray the mood. The system captures emotional contexts from
user-provided textual descriptions by using transformer-based embeddings, which
were finetuned to predict the emotional dimensions of valence-arousal. The deep
component of the architecture utilizes these embeddings to generalize unseen
emotional patterns, while the wide component effectively memorizes user-emotion
and emotion-genre associations through cross-product features. Experimental
results show that personalized music selections positively influence the user's
emotions and lead to a significant improvement in emotional relevance.

</details>


### [48] [From Authors to Reviewers: Leveraging Rankings to Improve Peer Review](https://arxiv.org/abs/2510.21726)
*Weichen Wang,Chengchun Shi*

Main category: cs.IR

TL;DR: 本文讨论了Su等人(2025)的JASA讨论稿，针对机器学习会议稿件数量激增导致的审稿质量问题，提出了一种利用审稿人排序信息的方法，并与作者排序信息结合，以更准确地评估稿件质量。


<details>
  <summary>Details</summary>
Motivation: 近年来，机器学习会议的审稿质量因投稿数量快速增长而备受关注。

Method: 通过模拟2023年ICML会议的投稿数据，利用审稿人而非作者的排序信息。

Result: 结果表明，结合审稿人排序信息能显著提高对论文质量的评估，通常优于仅使用作者排序信息；结合审稿人和作者的排序信息在大多数情况下能产生最准确的评估结果。

Conclusion: 结合审稿人和作者的排序信息能够更准确地评估稿件质量。

Abstract: This paper is a discussion of the 2025 JASA discussion paper by Su et al.
(2025). We would like to congratulate the authors on conducting a comprehensive
and insightful empirical investigation of the 2023 ICML ranking data. The
review quality of machine learning (ML) conferences has become a big concern in
recent years, due to the rapidly growing number of submitted manuscripts. In
this discussion, we propose an approach alternative to Su et al. (2025) that
leverages ranking information from reviewers rather than authors. We simulate
review data that closely mimics the 2023 ICML conference submissions. Our
results show that (i) incorporating ranking information from reviewers can
significantly improve the evaluation of each paper's quality, often
outperforming the use of ranking information from authors alone; and (ii)
combining ranking information from both reviewers and authors yields the most
accurate evaluation of submitted papers in most scenarios.

</details>


### [49] [Your Dense Retriever is Secretly an Expeditious Reasoner](https://arxiv.org/abs/2510.21727)
*Yichi Zhang,Jun Bai,Zhixin Cai,Shuhan Qin,Zhuofan Chen,Jinghua Guan,Wenge Rong*

Main category: cs.IR

TL;DR: 提出了一种混合查询重写框架AdaQR，以优化检索效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 稠密检索器在处理推理密集型查询时表现不佳，而大型语言模型成本高昂。

Method: AdaQR框架包含一个Reasoner Router，用于动态地将查询分配给快速稠密推理或深度LLM推理。稠密推理通过Dense Reasoner在嵌入空间中执行LLM风格的推理。

Result: 在大型检索基准BRIGHT上的实验表明，AdaQR在降低28%的推理成本的同时，保持甚至提高了7%的检索性能。

Conclusion: AdaQR框架通过自适应地选择推理方法，实现了效率和准确性之间的可控权衡。

Abstract: Dense retrievers enhance retrieval by encoding queries and documents into
continuous vectors, but they often struggle with reasoning-intensive queries.
Although Large Language Models (LLMs) can reformulate queries to capture
complex reasoning, applying them universally incurs significant computational
cost. In this work, we propose Adaptive Query Reasoning (AdaQR), a hybrid query
rewriting framework. Within this framework, a Reasoner Router dynamically
directs each query to either fast dense reasoning or deep LLM reasoning. The
dense reasoning is achieved by the Dense Reasoner, which performs LLM-style
reasoning directly in the embedding space, enabling a controllable trade-off
between efficiency and accuracy. Experiments on large-scale retrieval
benchmarks BRIGHT show that AdaQR reduces reasoning cost by 28% while
preserving-or even improving-retrieval performance by 7%.

</details>


### [50] [Modeling Bias Evolution in Fashion Recommender Systems: A System Dynamics Approach](https://arxiv.org/abs/2510.21728)
*Mahsa Goodarzi,M. Abdullah Canbaz*

Main category: cs.IR

TL;DR: 该研究调查了时尚推荐系统(FRS)中偏差的激活和加强机制。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的偏差不仅会扭曲用户体验，还会延续和放大现有的社会刻板印象，尤其是在时尚电子商务等领域。

Method: 通过利用系统动力学建模和实验模拟，剖析了偏差的时间演变及其对系统性能的多方面影响。

Result: 归纳偏差对系统结果的影响比用户偏差更大，表明了需要干预的关键领域。目前的反偏策略在一定程度上是有效的，但需要进一步加强以全面减轻偏差。

Conclusion: 这项研究强调了推进这些策略和扩展系统边界以纳入更广泛的背景因素（如用户人口统计和项目多样性）的必要性，旨在促进FRS的包容性和公平性。

Abstract: Bias in recommender systems not only distorts user experience but also
perpetuates and amplifies existing societal stereotypes, particularly in
sectors like fashion e-commerce. This study employs a dynamic modeling approach
to scrutinize the mechanisms of bias activation and reinforcement within
Fashion Recommender Systems (FRS). By leveraging system dynamics modeling and
experimental simulations, we dissect the temporal evolution of bias and its
multifaceted impacts on system performance. Our analysis reveals that inductive
biases exert a more substantial influence on system outcomes than user biases,
suggesting critical areas for intervention. We demonstrate that while current
debiasing strategies, including data rebalancing and algorithmic
regularization, are effective to an extent, they require further enhancement to
comprehensively mitigate biases. This research underscores the necessity for
advancing these strategies and extending system boundaries to incorporate
broader contextual factors such as user demographics and item diversity, aiming
to foster inclusivity and fairness in FRS. The findings advocate for a
proactive approach in recommender system design to counteract bias propagation
and ensure equitable user experiences.

</details>


### [51] [CustomIR: Unsupervised Fine-Tuning of Dense Embeddings for Known Document Corpora](https://arxiv.org/abs/2510.21729)
*Nathan Paull*

Main category: cs.IR

TL;DR: CustomIR：一个利用合成数据微调语言模型以提升领域检索效果的框架。


<details>
  <summary>Details</summary>
Motivation: 预训练的密集嵌入模型在特定领域语料库上的表现下降。

Method: 利用大型语言模型生成查询-文档对，并验证难负样本，从而进行无监督微调。

Result: 在企业邮件和消息数据集上的实验表明，CustomIR 显著提高了检索效果，小模型Recall@10提升高达2.3。

Conclusion: 有针对性的合成微调是提高领域特定性能的可扩展且经济高效的策略。

Abstract: Dense embedding models have become critical for modern information retrieval,
particularly in RAG pipelines, but their performance often degrades when
applied to specialized corpora outside their pre-training distribution. To
address thi we introduce \textbf{CustomIR}, a framework for unsupervised
adaptation of pre-trained language embedding models to domain-specific corpora
using synthetically generated query-document pairs. CustomIR leverages large
language models (LLMs) to create diverse queries grounded in a known target
corpus, paired with LLM-verified hard negatives, eliminating the need for
costly human annotation. Experiments on enterprise email and messaging datasets
show that CustomIR consistently improves retrieval effectiveness with small
models gaining up to 2.3 points in Recall@10. This performance increase allows
these small models to rival the performance of much larger alternatives,
allowing for cheaper RAG deployments. These results highlight that targeted
synthetic fine-tuning offers a scalable and cost-efficient strategy for
increasing domain-specific performance.

</details>


### [52] [TriMat: Context-aware Recommendation by Tri-Matrix Factorization](https://arxiv.org/abs/2510.21730)
*Hao Wang*

Main category: cs.IR

TL;DR: 本论文提出了一种基于三矩阵分解的上下文感知推荐系统(CARS)方法。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文感知推荐系统在实际应用中进展不大。

Method: 利用三矩阵分解技术将上下文信息融入矩阵分解框架。

Result: 该技术能有效提高推荐的准确性和公平性。

Conclusion: 所提出的方法在实验中表现出有效性。

Abstract: Search engine is the symbolic technology of Web 2.0, and many people used to
believe recommender systems is the new frontier of Web 3.0. In the past 10
years, with the advent of TikTok and similar apps, recommender systems has
materialized the vision of the machine learning pioneers. However, many
research topics of the field remain unfixed until today. One such topic is CARS
(Context-aware Recommender Systems) , which is largely a theoretical topic
without much advance in real-world applications. In this paper, we utilize
tri-matrix factorization technique to incorporate contextual information into
our matrix factorization framework, and prove that our technique is effective
in improving both the accuracy and fairness metrics in our experiments.

</details>


### [53] [Augmenting Researchy Questions with Sub-question Judgments](https://arxiv.org/abs/2510.21733)
*Jia-Huei Ju,Eugene Yang,Trevor Adriaanse,Andrew Yates*

Main category: cs.IR

TL;DR: 本文通过使用 Llama3.3 70B 模型，为 Researchy Questions 数据集中的每个子问题添加了 LLM 判定的标签，旨在为训练更好地支持复杂信息需求的检索模型提供资源。


<details>
  <summary>Details</summary>
Motivation: Researchy Questions 数据集包含约 10 万个具有复杂信息需求的查询问题，但缺乏子问题与相关文档之间的关联。

Method: 使用 Llama3.3 70B 模型为每个子问题生成 LLM 判定的标签，从而扩充 Researchy Questions 数据集。

Result: 扩充后的数据集包含子问题与 LLM 判定的标签之间的关联。

Conclusion: 该数据集可作为训练检索模型的资源，以更好地支持复杂信息需求。

Abstract: The Researchy Questions dataset provides about 100k question queries with
complex information needs that require retrieving information about several
aspects of a topic. Each query in ResearchyQuestions is associated with
sub-questions that were produced by prompting GPT-4. While ResearchyQuestions
contains labels indicating what documents were clicked after issuing the query,
there are no associations in the dataset between sub-questions and relevant
documents. In this work, we augment the Researchy Questions dataset with
LLM-judged labels for each sub-question using a Llama3.3 70B model. We intend
these sub-question labels to serve as a resource for training retrieval models
that better support complex information needs.

</details>


### [54] [From Factoid Questions to Data Product Requests: Benchmarking Data Product Discovery over Tables and Text](https://arxiv.org/abs/2510.21737)
*Liangliang Zhang,Nandana Mihindukulasooriya,Niharika S. D'Souza,Sola Shirai,Sarthak Dash,Yao Ma,Horst Samulowitz*

Main category: cs.IR

TL;DR: 介绍了DPBench，一个用于数据产品发现的基准，它通过将相关的表格和段落聚类成连贯的数据产品，生成跨越数据源的分析请求，并通过多语言模型评估来验证基准质量，从而系统地重新利用现有的表格-文本问答数据集。


<details>
  <summary>Details</summary>
Motivation: 当前没有专门为数据产品发现建立的基准。现有的数据集侧重于回答单个表格上的单一事实问题，而不是为更广泛、连贯的产品收集多个数据资产。为了填补这一空白。

Method: 通过将相关的表格和段落聚类成连贯的数据产品，生成跨越数据源的专业级分析请求，并通过多语言模型评估来验证基准质量，从而系统地重新利用现有的表格-文本问答数据集。

Result: 通过混合检索方法的基线实验，确定了DPR评估的可行性，揭示了当前的局限性，并为自动数据产品发现研究指出了新的机会。

Conclusion: DPBench是第一个用户请求驱动的混合表-文本语料库上的数据产品基准，它保留了完整的出处，同时生成了可操作的、类似分析师的数据产品请求。

Abstract: Data products are reusable, self-contained assets designed for specific
business use cases. Automating their discovery and generation is of great
industry interest, as it enables discovery in large data lakes and supports
analytical Data Product Requests (DPRs). Currently, there is no benchmark
established specifically for data product discovery. Existing datasets focus on
answering single factoid questions over individual tables rather than
collecting multiple data assets for broader, coherent products. To address this
gap, we introduce DPBench, the first user-request-driven data product benchmark
over hybrid table-text corpora. Our framework systematically repurposes
existing table-text QA datasets by clustering related tables and passages into
coherent data products, generating professional-level analytical requests that
span both data sources, and validating benchmark quality through multi-LLM
evaluation. DPBench preserves full provenance while producing actionable,
analyst-like data product requests. Baseline experiments with hybrid retrieval
methods establish the feasibility of DPR evaluation, reveal current
limitations, and point to new opportunities for automatic data product
discovery research.
  Code and datasets are available at:
https://anonymous.4open.science/r/data-product-benchmark-BBA7/

</details>


### [55] [DiffGRM: Diffusion-based Generative Recommendation Model](https://arxiv.org/abs/2510.21805)
*Zhao Liu,Yichen Zhu,Yiqing Yang,Guoping Tang,Rui Huang,Qiang Luo,Xiao Lv,Ruiming Tang,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: 提出了一种基于扩散的生成推荐模型DiffGRM，以解决传统自回归模型在项目表示上的不足。


<details>
  <summary>Details</summary>
Motivation: 自回归生成推荐模型(ARMs)在处理项目语义ID(SID)时，存在内部项目一致性和数字间异质性问题，导致模型训练效率低下。

Method: 使用掩蔽离散扩散模型(MDM)替换自回归解码器，实现SID数字的双向上下文和任意顺序并行生成。通过并行语义编码(PSE)解耦数字并平衡每个数字的信息；使用On-policy Coherent Noising (OCN)优先考虑不确定的数字；使用Confidence-guided Parallel Denoising (CPD)优先填充高置信度的数字。

Result: 在多个数据集上，DiffGRM相比于其他生成和判别推荐基线模型，NDCG@10 指标提升了6.9%-15.5%。

Conclusion: DiffGRM 能够有效提升推荐系统的性能。

Abstract: Generative recommendation (GR) is an emerging paradigm that represents each
item via a tokenizer as an n-digit semantic ID (SID) and predicts the next item
by autoregressively generating its SID conditioned on the user's history.
However, two structural properties of SIDs make ARMs ill-suited. First,
intra-item consistency: the n digits jointly specify one item, yet the
left-to-right causality trains each digit only under its prefix and blocks
bidirectional cross-digit evidence, collapsing supervision to a single causal
path. Second, inter-digit heterogeneity: digits differ in semantic granularity
and predictability, while the uniform next-token objective assigns equal weight
to all digits, overtraining easy digits and undertraining hard digits. To
address these two issues, we propose DiffGRM, a diffusion-based GR model that
replaces the autoregressive decoder with a masked discrete diffusion model
(MDM), thereby enabling bidirectional context and any-order parallel generation
of SID digits for recommendation. Specifically, we tailor DiffGRM in three
aspects: (1) tokenization with Parallel Semantic Encoding (PSE) to decouple
digits and balance per-digit information; (2) training with On-policy Coherent
Noising (OCN) that prioritizes uncertain digits via coherent masking to
concentrate supervision on high-value signals; and (3) inference with
Confidence-guided Parallel Denoising (CPD) that fills higher-confidence digits
first and generates diverse Top-K candidates. Experiments show consistent gains
over strong generative and discriminative recommendation baselines on multiple
datasets, improving NDCG@10 by 6.9%-15.5%. Code is available at
https://github.com/liuzhao09/DiffGRM.

</details>


### [56] [Unifying Inductive, Cross-Domain, and Multimodal Learning for Robust and Generalizable Recommendation](https://arxiv.org/abs/2510.21812)
*Chanyoung Chung,Kyeongryul Lee,Sunbin Park,Joyce Jiyoung Whang*

Main category: cs.IR

TL;DR: MICRec: A unified framework for recommendation that combines inductive modeling, multimodal guidance, and cross-domain transfer.


<details>
  <summary>Details</summary>
Motivation: Existing recommendation systems struggle with complex scenarios across diverse domains due to their focus on individual aspects like new users/items, diverse information, or cross-domain transfer.

Method: The paper presents MICRec, which fuses inductive modeling, multimodal guidance, and cross-domain transfer. It refines representations through modality-based aggregation and uses overlapping users as anchors across domains.

Result: MICRec outperforms 12 baselines, especially in domains with limited data.

Conclusion: MICRec enables robust and generalizable recommendation by capturing user contexts and latent preferences in heterogeneous and incomplete real-world data.

Abstract: Recommender systems have long been built upon the modeling of interactions
between users and items, while recent studies have sought to broaden this
paradigm by generalizing to new users and items, incorporating diverse
information sources, and transferring knowledge across domains. Nevertheless,
these efforts have largely focused on individual aspects, hindering their
ability to tackle the complex recommendation scenarios that arise in daily
consumptions across diverse domains. In this paper, we present MICRec, a
unified framework that fuses inductive modeling, multimodal guidance, and
cross-domain transfer to capture user contexts and latent preferences in
heterogeneous and incomplete real-world data. Moving beyond the inductive
backbone of INMO, our model refines expressive representations through
modality-based aggregation and alleviates data sparsity by leveraging
overlapping users as anchors across domains, thereby enabling robust and
generalizable recommendation. Experiments show that MICRec outperforms 12
baselines, with notable gains in domains with limited training data.

</details>


### [57] [Development of an Automated Web Application for Efficient Web Scraping: Design and Implementation](https://arxiv.org/abs/2510.21831)
*Alok Dutta,Nilanjana Roy,Rhythm Sen,Sougata Dutta,Prabhat Das*

Main category: cs.IR

TL;DR: 该论文介绍了一个用户友好的自动化Web应用程序，旨在简化和优化非技术用户的网络抓取过程。


<details>
  <summary>Details</summary>
Motivation: 为了简化非技术用户的网络抓取过程，并使更多人能够访问数据提取。

Method: 该应用将网络抓取分解为三个阶段：抓取（使用requests库获取HTML内容）、提取（使用BeautifulSoup和正则表达式从HTML中提取相关数据）和执行（将数据结构化为CSV等格式）。使用Flask框架部署，并使用MongoDB支持用户注册、登录和数据存储。

Result: 该工具提高了网络抓取的效率，并使用户能够轻松地输入网址、定义数据提取参数并以简化格式下载数据。

Conclusion: 该方法在使网络抓取工具更易于访问、高效且易于使用方面取得了显著进展。

Abstract: This paper presents the design and implementation of a user-friendly,
automated web application that simplifies and optimizes the web scraping
process for non-technical users. The application breaks down the complex task
of web scraping into three main stages: fetching, extraction, and execution. In
the fetching stage, the application accesses target websites using the HTTP
protocol, leveraging the requests library to retrieve HTML content. The
extraction stage utilizes powerful parsing libraries like BeautifulSoup and
regular expressions to extract relevant data from the HTML. Finally, the
execution stage structures the data into accessible formats, such as CSV,
ensuring the scraped content is organized for easy use. To provide personalized
and secure experiences, the application includes user registration and login
functionalities, supported by MongoDB, which stores user data and scraping
history. Deployed using the Flask framework, the tool offers a scalable, robust
environment for web scraping. Users can easily input website URLs, define data
extraction parameters, and download the data in a simplified format, without
needing technical expertise. This automated tool not only enhances the
efficiency of web scraping but also democratizes access to data extraction by
empowering users of all technical levels to gather and manage data tailored to
their needs. The methodology detailed in this paper represents a significant
advancement in making web scraping tools accessible, efficient, and easy to use
for a broader audience.

</details>


### [58] [Temporal Graph Theoretic Analysis of Geopolitical Dynamics in the U.S. Entity List](https://arxiv.org/abs/2510.21962)
*Yunsen Lei,Kexin Bai,Quan Li,H. Howie Huang*

Main category: cs.IR

TL;DR: 美国出口管制是重要的经济手段，实体清单是其中最突出的工具，但其动态性未被充分探索。


<details>
  <summary>Details</summary>
Motivation: 研究实体清单的动态性，揭示其背后的地缘政治战略。

Method: 构建了一个时间图框架，将实体清单文件从静态注册表转换为地缘政治战略的动态表示。创建了美国政府外国实体指定事件的数据集，并将其建模为时间二分图。开发了一种多层次分析方法。

Result: 揭示了静态视角无法捕捉的升级、持久性和协调的动态模式。

Conclusion: 时间图分析为系统计算地缘政治出口管制动态提供了见解。

Abstract: Export controls have become one of America's most prominent tools of economic
statecraft. They aim to block rival countries' access to sensitive
technologies, safeguard U.S. supply chains, protect national security, and
shape geopolitical competition. Among various instruments, the U.S. Entity List
has emerged as the most salient, yet its dynamics remain underexplored. This
paper introduces a novel temporal graph framework that transforms the Entity
List documents from a static registry of foreign entities of concern into a
dynamic representation of geopolitical strategy. We construct the first
event-based dataset of U.S. government foreign entity designations and model
them as a temporal bipartite graph. Building on this representation, we develop
a multi-level analytical approach that reveals shifting roles, enforcement
strategy, and broader sanction ecosystems. Applied to 25 years of data, the
framework uncovers dynamic patterns of escalation, persistence, and
coordination that static views cannot capture. More broadly, our study
demonstrates how temporal graph analysis offers systematic computational
insights into the geopolitical dynamics of export controls.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [59] [A Feature Engineering Approach for Business Impact-Oriented Failure Detection in Distributed Instant Payment Systems](https://arxiv.org/abs/2510.21710)
*Lorenzo Porcelli*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的基于ISO 20022消息交换处理时间的特征工程方法，用于即时支付基础设施的异常检测。


<details>
  <summary>Details</summary>
Motivation: 传统监控方法无法弥合技术指标和业务流程可见性之间的差距，无法满足即时支付系统零停机的高性能要求。

Method: 该方法通过计算连续ISO 20022消息交换之间的处理时间来构建系统状态的紧凑表示，并应用异常检测。

Result: 在TARGET即时支付结算系统（TIPS）上的实验表明，该方法能有效检测各种异常模式，并提供可解释的解释，帮助运维人员理解业务影响。

Conclusion: 该框架通过将特征映射到不同的处理阶段，区分内部和外部支付系统问题，显著减少调查时间，并弥合分布式系统中交易状态分散在多个实体中的可观察性差距。

Abstract: Instant payment infrastructures have stringent performance requirements,
processing millions of transactions daily with zero-downtime expectations.
Traditional monitoring approaches fail to bridge the gap between technical
infrastructure metrics and business process visibility. We introduce a novel
feature engineering approach based on processing times computed between
consecutive ISO 20022 message exchanges, creating a compact representation of
system state. By applying anomaly detection to these features, we enable early
failure detection and localization, allowing incident classification.
Experimental evaluation on the TARGET Instant Payment Settlement (TIPS) system,
using both real-world incidents and controlled simulations, demonstrates the
approach's effectiveness in detecting diverse anomaly patterns and provides
inherently interpretable explanations that enable operators to understand the
business impact. By mapping features to distinct processing phases, the
resulting framework differentiates between internal and external payment system
issues, significantly reduces investigation time, and bridges observability
gaps in distributed systems where transaction state is fragmented across
multiple entities.

</details>


### [60] [Numerical Fragility in Transformers: A Layer-wise Theory for Explaining, Forecasting, and Mitigating Instability](https://arxiv.org/abs/2510.21770)
*Jinwoo Baek*

Main category: cs.LG

TL;DR: 该论文研究了低精度训练的 Transformers 中的前向误差放大问题。


<details>
  <summary>Details</summary>
Motivation: 研究 Transformer 在低精度训练中误差增长的原因。

Method: 提出了一个模块化的理论，用于预测误差增长的时间和位置。该理论为自注意力机制推导了一个逐层界限，并引入了 LayerNorm 指标。

Result: 通过实验验证了该理论的有效性，并提出了一个小的 LayerNorm 调整来稳定训练。

Conclusion: 该理论提供可操作的诊断工具，可以解释自注意力机制何时脆弱，预测不稳定性，并促使进行最小限度的干预。

Abstract: Transformers trained in low precision can suffer forward-error amplification.
We give a first-order, module-wise theory that predicts when and where errors
grow. For self-attention we derive a per-layer bound that factorizes into three
interpretable diagnostics: a score-scale ratio $\kappa_{\rm score}$, a rowwise
softmax sensitivity $\kappa_{\rm softmax}$, and value conditioning $\kappa(V)$.
We prove a residual relaxation inequality showing that residual blocks
attenuate depth-wise accumulation, and we introduce a precision- and
width-aware LayerNorm indicator $\rho_{\rm LN}$ with a matching first-order
bound in the $\epsilon$-dominated regime. These pieces yield a unified
forward-stability bound whose right-hand side is directly estimable during
training.
  On Tiny-ViT/CIFAR-10 we evaluate the bound and components. (1) The combined
predictor $\kappa_{\rm softmax},(1+\kappa_{\rm
score}),\kappa(V),|W_O|2+\kappa{\rm eff}+C_{\rm LN}$ tracks
FP32$\leftrightarrow$LP mismatches across seeds, widths, and precisions;
scaling by $\epsilon_{\rm mach}$ collapses mixed-precision points. (2) The
time-series maximum of $\kappa_{\rm softmax}$ acts as an early-warning signal,
leading error spikes by 16-24 steps (corr. 0.65-0.82; permutation
$p!\approx!10^{-3}$; Precision@K 0.89-1.00). (3) Guided by $\rho_{\rm LN}$, a
small LayerNorm-$\epsilon$ tweak targeting $\rho_\star$ gives consistent
stabilization (mean tail-loss $\downarrow\ \approx0.010$ at $\rho_\star!=!0.6$,
cap$=10^{-2}$) with negligible overhead.
  Overall, our theory supplies actionable, unitless diagnostics that (i)
explain when self-attention is fragile, (ii) forecast instability, and (iii)
motivate a minimally invasive mitigation.

</details>


### [61] [Centrum: Model-based Database Auto-tuning with Minimal Distributional Assumptions](https://arxiv.org/abs/2510.22734)
*Yuanhao Lai,Pengfei Zheng,Chenpeng Ji,Yan Li,Songhan Zhang,Rutao Zhang,Zhengang Wang,Yunfei Du*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于模型的数据库管理系统自动调优器Centrum，以解决现有方法在建模和优化数据库管理系统性能时存在的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯过程贝叶斯优化的数据库管理系统自动调优器在实际应用中存在局限性，而基于树集成模型的自动调优器（如SMAC）能够避免这些问题，但仍有改进空间。现有树集成贝叶斯优化器只能实现无分布的点估计，且对不确定性估计施加了不切实际的分布假设。此外，梯度提升的最新进展很少应用于优化数据库管理系统自动调优器。

Method: Centrum采用随机梯度提升集成（SGBE）的两阶段学习程序来改进代理模型中的无分布点和区间估计。此外，Centrum采用广义的SGBE估计的局部自适应共形预测，以实现无分布的不确定性估计和采集函数。

Result: 在两个数据库管理系统和三个工作负载上进行的大量物理和模拟实验表明，Centrum优于21种最先进的方法。

Conclusion: Centrum是第一个实现无分布的自动调优器，增强了贝叶斯优化在数据库管理系统自动调优中的实用性，也是第一个在贝叶斯优化中无缝融合梯度提升集成和共形推理的自动调优器。

Abstract: Gaussian-Process-based Bayesian optimization (GP-BO), is a prevailing
model-based framework for DBMS auto-tuning. However, recent work shows
GP-BO-based DBMS auto-tuners significantly outperformed auto-tuners based on
SMAC, which features random forest surrogate models; such results motivate us
to rethink and investigate the limitations of GP-BO in auto-tuner design. We
find the fundamental assumptions of GP-BO are widely violated when modeling and
optimizing DBMS performance, while tree-ensemble-BOs (e.g., SMAC) can avoid the
assumption pitfalls and deliver improved tuning efficiency and effectiveness.
Moreover, we argue that existing tree-ensemble-BOs restrict further advancement
in DBMS auto-tuning. First, existing tree-ensemble-BOs can only achieve
distribution-free point estimates, but still impose unrealistic distributional
assumptions on uncertainty estimates, compromising surrogate modeling and
distort the acquisition function. Second, recent advances in gradient boosting,
which can further enhance surrogate modeling against vanilla GP and random
forest counterparts, have rarely been applied in optimizing DBMS auto-tuners.
To address these issues, we propose a novel model-based DBMS auto-tuner,
Centrum. Centrum improves distribution-free point and interval estimation in
surrogate modeling with a two-phase learning procedure of stochastic gradient
boosting ensembles. Moreover, Centrum adopts a generalized SGBE-estimated
locally-adaptive conformal prediction to facilitate a distribution-free
uncertainty estimation and acquisition function. To our knowledge, Centrum is
the first auto-tuner to realize distribution-freeness, enhancing BO's
practicality in DBMS auto-tuning, and the first to seamlessly fuse gradient
boosting ensembles and conformal inference in BO. Extensive physical and
simulation experiments on two DBMSs and three workloads show Centrum
outperforms 21 SOTA methods.

</details>


### [62] [Chebyshev Moment Regularization (CMR): Condition-Number Control with Moment Shaping](https://arxiv.org/abs/2510.21772)
*Jinwoo Baek*

Main category: cs.LG

TL;DR: 提出了切比雪夫矩正则化（CMR），一种直接优化层谱的简单、架构无关的损失函数。


<details>
  <summary>Details</summary>
Motivation: 旨在通过控制谱边缘和塑造内部结构，优化模型的光谱属性，从而实现稳定和准确的学习。

Method: 通过对数条件代理控制谱边缘，通过切比雪夫矩塑造内部结构，并采用解耦、有上限的混合规则，以保持任务梯度。

Result: 在对抗性“κ-stress”设置（MNIST，15层MLP）中，与普通训练相比，CMR将平均层条件数降低了约1000倍（在5个epoch内从约3.9×10^3降至约3.4），增加了平均梯度幅度，并恢复了测试精度（约10%到约86%）。

Conclusion: 这些结果支持优化驱动的谱预处理：直接引导模型走向良好条件的状态，以实现稳定、准确的学习。

Abstract: We introduce \textbf{Chebyshev Moment Regularization (CMR)}, a simple,
architecture-agnostic loss that directly optimizes layer spectra. CMR jointly
controls spectral edges via a log-condition proxy and shapes the interior via
Chebyshev moments, with a decoupled, capped mixing rule that preserves task
gradients. We prove strictly monotone descent for the condition proxy, bounded
moment gradients, and orthogonal invariance. In an adversarial
``$\kappa$-stress'' setting (MNIST, 15-layer MLP), \emph{compared to vanilla
training}, CMR reduces mean layer condition numbers by $\sim\!10^3$ (from
$\approx3.9\!\times\!10^3$ to $\approx3.4$ in 5 epochs), increases average
gradient magnitude, and restores test accuracy (
$\approx10\%\!\to\!\approx86\%$ ). These results support
\textbf{optimization-driven spectral preconditioning}: directly steering models
toward well-conditioned regimes for stable, accurate learning.

</details>


### [63] [What Causes Postoperative Aspiration?](https://arxiv.org/abs/2510.21779)
*Supriya Nagesh,Karina Covarrubias,Robert El-Kareh,Shiva Prasad Kasiviswanathan,Nina Mishra*

Main category: cs.LG

TL;DR: This study uses machine learning to predict postoperative aspiration using the MIMIC-IV database.


<details>
  <summary>Details</summary>
Motivation: Postoperative aspiration significantly impacts surgical patient outcomes, motivating the development of a predictive model for timely intervention.

Method: Three ML models (XGBoost, Multilayer Perceptron, and Random Forest) were trained on pre-surgical data from the MIMIC-IV database to predict postoperative aspiration, and ATE was estimated using Augmented Inverse Probability Weighting to investigate causation.

Result: The ML model achieved an AUROC of 0.86 and 77.3% sensitivity. Opioid dose, length of stay, and age were key predictors. Opioids and operative site were significant causative factors. Men were more likely to aspirate and received higher opioid doses.

Conclusion: ML models can predict postoperative aspiration risk, enabling targeted prevention. Opioid dosage and operative site significantly influence risk. Gender disparity in opioid administration and aspiration rates requires further investigation.

Abstract: Background: Aspiration, the inhalation of foreign material into the lungs,
significantly impacts surgical patient morbidity and mortality. This study
develops a machine learning (ML) model to predict postoperative aspiration,
enabling timely preventative interventions.
  Methods: From the MIMIC-IV database of over 400,000 hospital admissions, we
identified 826 surgical patients (mean age: 62, 55.7\% male) who experienced
aspiration within seven days post-surgery, along with a matched non-aspiration
cohort. Three ML models: XGBoost, Multilayer Perceptron, and Random Forest were
trained using pre-surgical hospitalization data to predict postoperative
aspiration. To investigate causation, we estimated Average Treatment Effects
(ATE) using Augmented Inverse Probability Weighting.
  Results: Our ML model achieved an AUROC of 0.86 and 77.3\% sensitivity on a
held-out test set. Maximum daily opioid dose, length of stay, and patient age
emerged as the most important predictors. ATE analysis identified significant
causative factors: opioids (0.25 +/- 0.06) and operative site (neck: 0.20 +/-
0.13, head: 0.19 +/- 0.13). Despite equal surgery rates across genders, men
were 1.5 times more likely to aspirate and received 27\% higher maximum daily
opioid dosages compared to women.
  Conclusion: ML models can effectively predict postoperative aspiration risk,
enabling targeted preventative measures. Maximum daily opioid dosage and
operative site significantly influence aspiration risk. The gender disparity in
both opioid administration and aspiration rates warrants further investigation.
These findings have important implications for improving postoperative care
protocols and aspiration prevention strategies.

</details>


### [64] [Online Mixture of Experts: No-Regret Learning for Optimal Collective Decision-Making](https://arxiv.org/abs/2510.21788)
*Larkin Liu,Jalal Etesami*

Main category: cs.LG

TL;DR: 提出了一种在线专家混合（OMoE）方法，用于动态聚合多个专家（如大型语言模型）的输出，以提高整体准确性。


<details>
  <summary>Details</summary>
Motivation: 在上下文中，如何聚合专家委员会的输出以获得最佳结果是一个问题。

Method: 提出了两种算法：一种结合了聚合投票和UCB驱动的连续消除，另一种采用了在线加权多数投票机制。

Result: 为理想情况下bandit setting中的遗憾特性推导了理论保证，并提供了相应的经验结果。应用于大型语言模型的在线微调，动态调整专家权重或选择最佳专家委员会。

Conclusion: 引入了新的方法和无悔保证，用于组合多个专家以提高整体性能。

Abstract: We explore the use of expert-guided bandit learning, which we refer to as
online mixture-of-experts (OMoE). In this setting, given a context, a candidate
committee of experts must determine how to aggregate their outputs to achieve
optimal results in terms of aggregate accuracy. We propose two algorithms to
address this problem. The first algorithm combines aggregate voting with
UCB-driven successive elimination, efficiently pruning suboptimal exploration
actions. The second algorithm employs an online weighted-majority-voting
mechanism, leveraging the respective voting power of each expert proportional
to their predictive power. We derive theoretical guarantees for the regret
properties in the bandit setting under ideal circumstances, and empirical
results are provided accordingly. As a modern study on applications, these
methods are applied to the online fine-tuning of a set of expert large language
models (LLMs), where after each response, the generative LLM dynamically
reweighs its set of experts and/or selects the optimal committee of experts to
generate the most accurate response. Our results introduce new methodologies
and no-regret guarantees for combining multiple experts to improve on the
performance of the an aggregate model overall.

</details>


### [65] [Variance-Reduction Guidance: Sampling Trajectory Optimization for Diffusion Models](https://arxiv.org/abs/2510.21792)
*Shifeng Xu,Yanzhu Liu,Adams Wai-Kin Kong*

Main category: cs.LG

TL;DR: 扩散模型是一种新兴的生成模型，但其预测误差会累积并降低生成质量。本文介绍了一种统计测量预测误差的新技术，并提出了方差减少引导（VRG）方法来缓解此误差。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的预测误差会累积并降低生成质量。

Method: 提出方差减少引导（VRG）方法，通过搜索新的采样轨迹来提高生成质量，而无需模型微调或修改。

Result: 在各种数据集和基线上的实验表明，VRG 可以显著提高扩散模型的生成质量。

Conclusion: VRG 可以显著提高扩散模型的生成质量。

Abstract: Diffusion models have become emerging generative models. Their sampling
process involves multiple steps, and in each step the models predict the noise
from a noisy sample. When the models make prediction, the output deviates from
the ground truth, and we call such a deviation as \textit{prediction error}.
The prediction error accumulates over the sampling process and deteriorates
generation quality. This paper introduces a novel technique for statistically
measuring the prediction error and proposes the Variance-Reduction Guidance
(VRG) method to mitigate this error. VRG does not require model fine-tuning or
modification. Given a predefined sampling trajectory, it searches for a new
trajectory which has the same number of sampling steps but produces higher
quality results. VRG is applicable to both conditional and unconditional
generation. Experiments on various datasets and baselines demonstrate that VRG
can significantly improve the generation quality of diffusion models. Source
code is available at https://github.com/shifengxu/VRG.

</details>


### [66] [A Physics-Guided AI Cascaded Corrector Model Significantly Extends Madden-Julian Oscillation Prediction Skill](https://arxiv.org/abs/2510.21796)
*Xiao Zhou,Yuze Sun,Jie Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: 提出了一种新的深度学习框架PCC-MJO，用于修正动力模型对MJO的预测，延长有效预测范围。


<details>
  <summary>Details</summary>
Motivation: MJO是全球天气和气候极端事件的重要驱动因素，但业务动力模型对其预测仍然具有挑战性，有效预测通常限于3-4周。

Method: 该模型采用物理信息3D U-Net来校正时空场误差，然后使用针对预测技能优化的LSTM来优化MJO的RMM指数。

Result: 该框架将CMA、ECMWF和NCEP的有效预测范围延长了2-8天，并有效缓解了“海洋大陆障碍”。

Conclusion: 该模型为突破亚季节预测的长期障碍提供了一条有希望的、物理上一致的、计算效率高的和高度通用的途径。

Abstract: The Madden-Julian Oscillation (MJO) is an important driver of global weather
and climate extremes, but its prediction in operational dynamical models
remains challenging, with skillful forecasts typically limited to 3-4 weeks.
Here, we introduce a novel deep learning framework, the Physics-guided Cascaded
Corrector for MJO (PCC-MJO), which acts as a universal post-processor to
correct MJO forecasts from dynamical models. This two-stage model first employs
a physics-informed 3D U-Net to correct spatial-temporal field errors, then
refines the MJO's RMM index using an LSTM optimized for forecast skill. When
applied to three different operational forecasts from CMA, ECMWF and NCEP, our
unified framework consistently extends the skillful forecast range (bivariate
correlation > 0.5) by 2-8 days. Crucially, the model effectively mitigates the
"Maritime Continent barrier", enabling more realistic eastward propagation and
amplitude. Explainable AI analysis quantitatively confirms that the model's
decision-making is spatially congruent with observed MJO dynamics (correlation
> 0.93), demonstrating that it learns physically meaningful features rather
than statistical fittings. Our work provides a promising physically consistent,
computationally efficient, and highly generalizable pathway to break through
longstanding barriers in subseasonal forecasting.

</details>


### [67] [Quantifying Multimodal Imbalance: A GMM-Guided Adaptive Loss for Audio-Visual Learning](https://arxiv.org/abs/2510.21797)
*Zhaocheng Liu,Zhiwen Yu,Xiaoqing Liu*

Main category: cs.LG

TL;DR: 提出了一种量化多模态不平衡程度的新方法，并设计了一个样本级别的自适应损失函数。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要集中在架构修改和优化，忽略了模态之间不平衡程度的定量分析。

Method: 1. 定义“模态差距”为不同模态Softmax得分的差异。2. 使用双峰高斯混合模型 (GMM) 对模态差距分布进行建模。3. 应用贝叶斯定理计算每个样本属于不同分布的后验概率。4. 设计了一个具有三个目标的自适应损失函数：最小化模态差距，鼓励不平衡样本分布向平衡分布移动，并对不平衡样本应用更大的惩罚权重。5. 采用两阶段训练策略。

Result: 在CREMA-D和AVE数据集上实现了SOTA性能，准确率分别为80.65%和70.90%。

Conclusion: 验证了所提出方法的有效性。

Abstract: Current mainstream approaches to addressing multimodal imbalance primarily
focus on architectural modifications and optimization-based, often overlooking
a quantitative analysis of the imbalance degree between modalities. To address
this gap, our work introduces a novel method for the quantitative analysis of
multi-modal imbalance, which in turn informs the design of a sample-level
adaptive loss function.We begin by defining the "Modality Gap" as the
difference between the Softmax scores of different modalities (e.g., audio and
visual) for the ground-truth class prediction. Analysis of the Modality Gap
distribution reveals that it can be effectively modeled by a bimodal Gaussian
Mixture Model (GMM). These two components are found to correspond respectively
to "modality-balanced" and "modality-imbalanced" data samples. Subsequently, we
apply Bayes' theorem to compute the posterior probability of each sample
belonging to these two distinct distributions.Informed by this quantitative
analysis, we design a novel adaptive loss function with three objectives: (1)
to minimize the overall Modality Gap; (2) to encourage the imbalanced sample
distribution to shift towards the balanced one; and (3) to apply greater
penalty weights to imbalanced samples. We employ a two-stage training strategy
consisting of a warm-up phase followed by an adaptive training
phase.Experimental results demonstrate that our approach achieves
state-of-the-art (SOTA) performance on the public CREMA-D and AVE datasets,
attaining accuracies of $80.65\%$ and $70.90\%$, respectively. This validates
the effectiveness of our proposed methodology.

</details>


### [68] [MARS-M: When Variance Reduction Meets Matrices](https://arxiv.org/abs/2510.21800)
*Yifeng Liu,Angela Yuan,Quanquan Gu*

Main category: cs.LG

TL;DR: 本文提出了一种新的优化器MARS-M，它结合了MARS中的方差缩减技术和Muon中的矩阵预处理技术。


<details>
  <summary>Details</summary>
Motivation: 现有的基于矩阵的预处理优化器（如Muon）和基于方差缩减的优化器（如MARS）在训练大型神经网络和大型语言模型时分别具有优势，但缺乏结合。

Method: 本文将MARS的方差缩减技术与Muon相结合，提出了新的优化器MARS-M，并从理论上证明了其收敛速度。

Result: 在语言建模和计算机视觉任务上的实验结果表明，MARS-M在各种下游基准测试中均能产生更低的损失和更高的性能。

Conclusion: MARS-M 结合了 MARS 和 Muon 的优点，在实验中表现更好，并且具有更快的收敛速度。

Abstract: Matrix-based preconditioned optimizers, such as Muon, have recently been
shown to be more efficient than scalar-based optimizers for training
large-scale neural networks, including large language models (LLMs). On the
other hand, recent benchmarks on optimizers for LLM pre-training have
demonstrated that variance-reduction techniques such as MARS can achieve
substantial speedups over standard optimizers that do not employ variance
reduction. In this paper, to achieve the best of both worlds, we introduce
MARS-M, a new optimizer that integrates the variance reduction technique in
MARS with Muon. Under standard regularity conditions, we prove that Muon-M
converges to a first-order stationary point at a rate of
$\tilde{\mathcal{O}}(T^{-1/3})$, which improves upon
$\tilde{\mathcal{O}}(T^{-1/4})$ rate attained by Muon. Our empirical results on
language modeling and computer vision tasks demonstrate that MARS-M
consistently yields lower losses and improved performance across various
downstream benchmarks. The implementation of MARS-M is available at
https://github.com/AGI-Arena/MARS/MARS_M.

</details>


### [69] [Residual-guided AI-CFD hybrid method enables stable and scalable simulations: from 2D benchmarks to 3D applications](https://arxiv.org/abs/2510.21804)
*Shilaj Baral,Youngkyu Lee,Sangam Khanal,Joongoo Jeon*

Main category: cs.LG

TL;DR: 提出了一种新的混合仿真策略XRePIT，它将机器学习加速与基于求解器的校正相结合。


<details>
  <summary>Details</summary>
Motivation: 现有的混合方法缺乏自动化和鲁棒性，无法实际应用。纯粹的数据驱动的流体动力学替代方法经常因误差累积而彻底失败。

Method: 该方法被专门设计为完全自动化和物理感知，确保了以前的方法所缺乏的稳定性和实际适用性。

Result: 该设计克服了长期存在的障碍，首次实现了超过10,000个时间步长的稳定加速推广。该方法还能稳健地推广到未知的边界条件，并且可以扩展到3D流。

Conclusion: 该方法提供高达4.98倍的加速，同时保持较高的物理保真度，以~1E-3的相对误差解析热场，并以低于1E-2 ms-1的误差捕获低幅度速度动态。

Abstract: Purely data-driven surrogates for fluid dynamics often fail catastrophically
from error accumulation, while existing hybrid methods have lacked the
automation and robustness for practical use. To solve this, we developed
XRePIT, a novel hybrid simulation strategy that synergizes machine learning
(ML) acceleration with solver-based correction. We specifically designed our
method to be fully automated and physics-aware, ensuring the stability and
practical applicability that previous approaches lacked. We demonstrate that
this new design overcomes long-standing barriers, achieving the first stable,
accelerated rollouts for over 10,000 timesteps. The method also generalizes
robustly to unseen boundary conditions and, crucially, scales to 3D flows. Our
approach delivers speedups up to 4.98$\times$ while maintaining high physical
fidelity, resolving thermal fields with relative errors of ~1E-3 and capturing
low magnitude velocity dynamics with errors below 1E-2 ms-1. This work thus
establishes a mature and scalable hybrid method, paving the way for its use in
real-world engineering.

</details>


### [70] [Geographic Transferability of Machine Learning Models for Short-Term Airport Fog Forecasting](https://arxiv.org/abs/2510.21819)
*Marcelo Cerda Castillo*

Main category: cs.LG

TL;DR: 本研究旨在构建一个无需地理坐标的雾预报模型，使其能够在不同地点之间迁移。


<details>
  <summary>Details</summary>
Motivation: 许多机器学习模型依赖于特定位置的特征，导致无法跨地点应用。本文研究了是否可以将基本的热力学和辐射过程编码到与坐标无关的特征集中，以实现地理上的可迁移性。

Method: 使用智利圣地亚哥（SCEL）2002-2009年的数据训练了一个梯度提升分类器（XGBoost），并在2010-2012年的预留数据集以及蒙特港（SCTE）、旧金山（KSFO）和伦敦（EGLL）进行了严格的零样本测试。

Result: 该模型在高达11,650公里的距离和不同的雾类型（辐射雾、平流雾、海洋雾）下，实现了0.923-0.947的AUC值。一致的SHAP特征排序表明，能见度持续性、太阳角度和热梯度主导了预测。

Conclusion: 研究结果表明，基于物理信息且与坐标无关的特征工程可以产生具有地理迁移性的天气预报工具。

Abstract: Short-term forecasting of airport fog (visibility < 1.0 km) presents
challenges in geographic generalization because many machine learning models
rely on location-specific features and fail to transfer across sites. This
study investigates whether fundamental thermodynamic and radiative processes
can be encoded in a coordinate-free (location-independent) feature set to
enable geographic transferability. A gradient boosting classifier (XGBoost)
trained on Santiago, Chile (SCEL, 33S) data from 2002-2009 was evaluated on a
2010-2012 holdout set and under strict zero-shot tests at Puerto Montt (SCTE),
San Francisco (KSFO), and London (EGLL). The model achieved AUC values of
0.923-0.947 across distances up to 11,650 km and different fog regimes
(radiative, advective, marine). Consistent SHAP feature rankings show that
visibility persistence, solar angle, and thermal gradients dominate
predictions, suggesting the model learned transferable physical relationships
rather than site-specific patterns. Results suggest that physics-informed,
coordinate-free feature engineering can yield geographically transferable
atmospheric forecasting tools.

</details>


### [71] [Unlocking Biomedical Insights: Hierarchical Attention Networks for High-Dimensional Data Interpretation](https://arxiv.org/abs/2510.21820)
*Rekha R Nair,Tina Babu,Alavikunhu Panthakkan,Hussain Al-Ahmad,Balamurugan Balusamy*

Main category: cs.LG

TL;DR: 提出了一种新的可解释深度学习模型HAIN，用于分析高维生物医学数据。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型缺乏透明度，阻碍了其在关键决策应用中的部署，因此需要开发既准确又可解释的机器学习模型。

Method: HAIN统一了多层次注意力机制、降维和解释驱动的损失函数。

Result: 在TCGA数据集上，HAIN的分类精度达到94.3%，超过了传统的事后可解释性方法，并且有效识别了生物相关的癌症生物标志物。

Conclusion: HAIN通过协调预测准确性和可解释性，推动了用于精准医疗和法规遵从的透明AI解决方案的发展。

Abstract: The proliferation of high-dimensional datasets in fields such as genomics,
healthcare, and finance has created an urgent need for machine learning models
that are both highly accurate and inherently interpretable. While traditional
deep learning approaches deliver strong predictive performance, their lack of
transparency often impedes their deployment in critical, decision-sensitive
applications. In this work, we introduce the Hierarchical Attention-based
Interpretable Network (HAIN), a novel architecture that unifies multi-level
attention mechanisms, dimensionality reduction, and explanation-driven loss
functions to deliver interpretable and robust analysis of complex biomedical
data. HAIN provides feature-level interpretability via gradientweighted
attention and offers global model explanations through prototype-based
representations. Comprehensive evaluation on The Cancer Genome Atlas (TCGA)
dataset demonstrates that HAIN achieves a classification accuracy of 94.3%,
surpassing conventional post-hoc interpretability approaches such as SHAP and
LIME in both transparency and explanatory power. Furthermore, HAIN effectively
identifies biologically relevant cancer biomarkers, supporting its utility for
clinical and research applications. By harmonizing predictive accuracy with
interpretability, HAIN advances the development of transparent AI solutions for
precision medicine and regulatory compliance.

</details>
