<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 49]
- [cs.CV](#cs.CV) [Total: 50]
- [cs.AI](#cs.AI) [Total: 45]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.LG](#cs.LG) [Total: 49]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [StreetMath: Study of LLMs' Approximation Behaviors](https://arxiv.org/abs/2510.25776)
*Chiung-Yi Tseng,Somshubhra Roy,Maisha Thasin,Danyang Zhang,Blessing Effiong*

Main category: cs.CL

TL;DR: 本文介绍了一个名为StreetMath的基准测试，用于评估大型语言模型在现实场景下的近似计算能力，特别是针对非自回归解码器模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注大型语言模型在非正式、快节奏的数学运算中的近似推理能力，尤其是在非自回归解码器模型中。

Method: 通过StreetMath基准测试，对Qwen3-4B-Instruct-2507, Qwen3-4B-Thinking-2507, Dream-v0-Instruct-7B, Falcon-Mamba-7B-Instruct, and Mamba-GPT-3B等不同LLM架构进行了广泛评估，并应用了可解释性技术来探究其内部计算状态。

Result: 研究发现，即使在需要近似计算的任务中，LLM通常会尝试计算精确值或调用外部工具。此外，模型在解决近似任务时会消耗更多tokens。精确和近似算术运算依赖于很大程度上独立的神经组件。

Conclusion: LLM在街头数学环境中没有表现出与人类相同的认知吝啬倾向。

Abstract: There is a substantial body of literature examining the mathematical
reasoning capabilities of large language models (LLMs), particularly their
performance on precise arithmetic operations in autoregressive architectures.
However, their ability to perform approximate reasoning in informal, fast-paced
mathematical operations has received far less attention, especially among
non-autoregressive decoder models. Our work addresses this gap by introducing
StreetMath, a benchmark designed to evaluate models' approximation abilities
under real-world approximation scenarios. We conduct extensive evaluations
across different LLM architectures: Qwen3-4B-Instruct-2507,
Qwen3-4B-Thinking-2507, Dream-v0-Instruct-7B, Falcon-Mamba-7B-Instruct, and
Mamba-GPT-3B. Furthermore, we apply mechanistic interpretability techniques to
probe their internal computational states. Our analysis reveals that LLMs
generally attempt to compute exact values or invoke external tools even in
tasks that call for approximation. Moreover, while models sometimes reach the
correct answer in early layers or steps, they still consume more tokens when
solving approximation tasks. Additional experiments indicate that exact and
approximate arithmetic operations rely on largely separate neural components.
Drawing upon research on cognitive psychology, we argue that LLMs do not
exhibit cognitive miserliness in the same way humans do in street math
settings. We open source our work https://github.com/ctseng777/StreetMath

</details>


### [2] [Review Based Entity Ranking using Fuzzy Logic Algorithmic Approach: Analysis](https://arxiv.org/abs/2510.25778)
*Pratik N. Kalamkar,Anupama G. Phakatkar*

Main category: cs.CL

TL;DR: 本文提出了一种基于词汇的方法，通过结合意见词（即副词、形容词、名词和动词），将实体评论和用户查询按照细粒度级别（即非常弱、弱、中等、非常强和强）进行分类，从而对实体进行排序。


<details>
  <summary>Details</summary>
Motivation: 现有的基于整体词典的方法没有考虑到每个意见的强度，即意见是非常强烈负面（或正面）、强烈负面（或正面）、中等负面（或正面）、非常弱负面（或正面）和弱负面（或正面）。

Method: 本文提出了一种方法，通过结合与特定产品感兴趣的方面相关的意见词（即副词、形容词、名词和动词），将实体评论和用户查询按照粒度级别（即非常弱、弱、中等、非常强和强）进行分类，从而对实体进行排序。我们将使用模糊逻辑算法方法将意见词分类到不同的类别中，并使用句法依赖关系解析来查找所需方面词的关系。考虑与感兴趣的某些方面相关的意见词，以找到评论中该方面的实体分数。

Result: 未提及

Conclusion: 未提及

Abstract: Opinion mining, also called sentiment analysis, is the field of study that
analyzes people opinions, sentiments, evaluations, appraisals, attitudes, and
emotions towards entities such as products, services, organizations,
individuals, issues, events, topics, and their attributes. Holistic
lexicon-based approach does not consider the strength of each opinion, i.e.,
whether the opinion is very strongly negative (or positive), strongly negative
(or positive), moderate negative (or positive), very weakly negative (or
positive) and weakly negative (or positive). In this paper, we propose approach
to rank entities based on orientation and strength of the entity reviews and
user's queries by classifying them in granularity levels (i.e. very weak, weak,
moderate, very strong and strong) by combining opinion words (i.e. adverb,
adjective, noun and verb) that are related to aspect of interest of certain
product. We shall use fuzzy logic algorithmic approach in order to classify
opinion words into different category and syntactic dependency resolution to
find relations for desired aspect words. Opinion words related to certain
aspects of interest are considered to find the entity score for that aspect in
the review.

</details>


### [3] [LASTIST: LArge-Scale Target-Independent STance dataset](https://arxiv.org/abs/2510.25783)
*DongJae Kim,Yaejin Lee,Minsu Park,Eunil Park*

Main category: cs.CL

TL;DR: 本研究提出了一个大规模的与目标无关的立场检测（LASTIST）韩语数据集。


<details>
  <summary>Details</summary>
Motivation: 当前立场检测研究主要集中在基于英语的、依赖于目标的任务上，并且缺乏对韩语等低资源语言的支持。

Method: 从韩国政党的新闻稿中收集了563,299个标记的韩语句子，并训练了最先进的深度学习和立场检测模型。

Result: 构建了一个适用于各种立场检测任务（包括与目标无关的立场检测和历时演变立场检测）的LASTIST数据集。

Conclusion: 该数据集已部署在https://anonymous.4open.science/r/LASTIST-3721/上，可用于立场检测的各项任务。

Abstract: Stance detection has emerged as an area of research in the field of
artificial intelligence. However, most research is currently centered on the
target-dependent stance detection task, which is based on a person's stance in
favor of or against a specific target. Furthermore, most benchmark datasets are
based on English, making it difficult to develop models in low-resource
languages such as Korean, especially for an emerging field such as stance
detection. This study proposes the LArge-Scale Target-Independent STance
(LASTIST) dataset to fill this research gap. Collected from the press releases
of both parties on Korean political parties, the LASTIST dataset uses 563,299
labeled Korean sentences. We provide a detailed description of how we collected
and constructed the dataset and trained state-of-the-art deep learning and
stance detection models. Our LASTIST dataset is designed for various tasks in
stance detection, including target-independent stance detection and diachronic
evolution stance detection. We deploy our dataset on
https://anonymous.4open.science/r/LASTIST-3721/.

</details>


### [4] [zFLoRA: Zero-Latency Fused Low-Rank Adapters](https://arxiv.org/abs/2510.25784)
*Dhananjaya Gowda,Seoha Song,Harshith Goka,Junhyun Lee*

Main category: cs.CL

TL;DR: 提出了一种名为zFLoRA的新型零延迟融合低秩适配器，它在基本模型的基础上引入了零或可忽略的延迟开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 越来越多地部署具有特定于任务的适配器，以满足多个下游应用程序。在这种情况下，与这些看似微不足道的适配器参数（通常小于基本模型的 1%）相关的额外计算在推理时变得非常重要（高达基本模型的 2.5 倍）。

Method: 提出一种新的零延迟融合低秩适配器 (zFLoRA)。

Result: 在 1B、3B 和 7B 大小的 LLM 上的实验结果表明，zFLoRA 与流行的监督微调基准（包括低秩适配器 (LoRA) 以及完全微调 (FFT)）相比，具有优势。在常识推理、数学推理和摘要对话三个不同类别中的 18 个不同任务上进行了实验。在 NPU (Samsung Galaxy S25+) 以及 GPU (NVIDIA H100) 平台上进行的延迟测量表明，所提出的 zFLoRA 适配器引入了零到可忽略的延迟开销。

Conclusion: 提出的zFLoRA适配器引入了零到可忽略的延迟开销

Abstract: Large language models (LLMs) are increasingly deployed with task-specific
adapters catering to multiple downstream applications. In such a scenario, the
additional compute associated with these apparently insignificant number of
adapter parameters (typically less than 1% of the base model) turns out to be
disproportionately significant during inference time (upto 2.5x times that of
the base model). In this paper, we propose a new zero-latency fused low-rank
adapter (zFLoRA) that introduces zero or negligible latency overhead on top of
the base model. Experimental results on LLMs of size 1B, 3B and 7B show that
zFLoRA compares favorably against the popular supervised fine-tuning benchmarks
including low-rank adapters (LoRA) as well as full fine-tuning (FFT).
Experiments are conducted on 18 different tasks across three different
categories namely commonsense reasoning, math reasoning and summary-dialogue.
Latency measurements made on NPU (Samsung Galaxy S25+) as well as GPU (NVIDIA
H100) platforms show that the proposed zFLoRA adapters introduce zero to
negligible latency overhead.

</details>


### [5] [BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better Edge Selection](https://arxiv.org/abs/2510.25786)
*Yaniv Nikankin,Dana Arad,Itay Itzhak,Anja Reusch,Adi Simhi,Gal Kesten-Pomeranz,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 本文对电路发现提出了三项关键改进，电路发现是机械可解释性中的主要挑战之一。


<details>
  <summary>Details</summary>
Motivation: 确定模型的哪些部分执行给定的任务。

Method: 1. 使用引导程序来识别具有一致归因分数的边。2. 引入了一种简单的基于比率的选择策略来优先考虑强阳性评分边，从而平衡性能和忠实度。3. 我们用整数线性规划公式代替了标准贪婪选择。

Result: 我们的方法产生了更忠实的电路，并且在多个 MIB 任务和模型中优于先前的方法。

Conclusion: 本文提出了一种更忠实的电路发现方法，并在多个 MIB 任务和模型中优于先前的方法。

Abstract: One of the main challenges in mechanistic interpretability is circuit
discovery, determining which parts of a model perform a given task. We build on
the Mechanistic Interpretability Benchmark (MIB) and propose three key
improvements to circuit discovery. First, we use bootstrapping to identify
edges with consistent attribution scores. Second, we introduce a simple
ratio-based selection strategy to prioritize strong positive-scoring edges,
balancing performance and faithfulness. Third, we replace the standard greedy
selection with an integer linear programming formulation. Our methods yield
more faithful circuits and outperform prior approaches across multiple MIB
tasks and models. Our code is available at:
https://github.com/technion-cs-nlp/MIB-Shared-Task.

</details>


### [6] [LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection](https://arxiv.org/abs/2510.25799)
*Adam S. Jovine,Tinghan Ye,Francis Bahk,Jingjing Wang,David B. Shmoys,Peter I. Frazier*

Main category: cs.CL

TL;DR: 提出 LISTEN 框架，利用大型语言模型 (LLM) 作为零样本偏好预言机，仅由专家的自然语言高级优先级引导，以解决人类专家难以从具有多个竞争目标的大量项目中选择最佳方案的问题。


<details>
  <summary>Details</summary>
Motivation: 解决人类专家在具有多个竞争目标的大量项目中选择最佳方案时，难以形式化复杂的隐式偏好的问题。

Method: 提出两种迭代算法：LISTEN-U，使用 LLM 优化参数化效用函数；LISTEN-T，一种非参数方法，对小批量解决方案执行锦标赛式选择。

Result: 在航班预订、购物和考试安排等不同任务上的评估结果表明，LISTEN-U 在偏好参数对齐时表现出色（我们使用一种新的协调指标来衡量这种属性），而 LISTEN-T 提供更强大的性能。

Conclusion: 这项工作探索了一个有希望的方向，即直接用自然语言引导复杂的多目标决策，减少传统偏好启发带来的认知负担。

Abstract: Human experts often struggle to select the best option from a large set of
items with multiple competing objectives, a process bottlenecked by the
difficulty of formalizing complex, implicit preferences. To address this, we
introduce LISTEN, a framework that leverages a Large Language Model (LLM) as a
zero-shot preference oracle, guided only by an expert's high-level priorities
in natural language. To operate within LLM constraints like context windows and
inference costs, we propose two iterative algorithms: LISTEN-U, which uses the
LLM to refine a parametric utility function, and LISTEN-T, a non-parametric
method that performs tournament-style selections over small batches of
solutions. Evaluated on diverse tasks including flight booking, shopping, and
exam scheduling, our results show LISTEN-U excels when preferences are
parametrically aligned (a property we measure with a novel concordance metric),
while LISTEN-T offers more robust performance. This work explores a promising
direction for steering complex multi-objective decisions directly with natural
language, reducing the cognitive burden of traditional preference elicitation.

</details>


### [7] [Beyond Length: Quantifying Long-Range Information for Long-Context LLM Pretraining Data](https://arxiv.org/abs/2510.25804)
*Haoran Deng,Yingyu Lin,Zhenghao Lin,Xiao Liu,Yizhou Sun,Yi-An Ma,Yeyun Gong*

Main category: cs.CL

TL;DR: Long-context language models benefit reasoning but need careful data selection due to irrelevant long-distance dependencies.


<details>
  <summary>Details</summary>
Motivation: Training long-context models on data without meaningful long-distance dependencies is inefficient, necessitating data selection.

Method: Introduce LongFilter, a framework that measures information gain from extended context by comparing model predictions in long vs. short-context settings.

Result: Extending LLaMA-3-8B's context length with LongFilter-selected data improves performance on HELMET, LongBench, and RULER benchmarks.

Conclusion: LongFilter efficiently selects high-quality data for long-context pretraining, leading to substantial improvements.

Abstract: Long-context language models unlock advanced capabilities in reasoning, code
generation, and document summarization by leveraging dependencies across
extended spans of text. However, a significant portion of readily available
long-text data lacks meaningful long-distance dependencies; most spans can be
predicted using only local context. Training on such data is inefficient,
making careful data selection crucial. Therefore, we introduce LongFilter, a
framework for curating training data tailored to long-context pretraining.
LongFilter measures the information gain provided by extended context by
contrasting model predictions under long-context versus short-context settings,
thereby identifying samples where long-range dependencies are essential.
Experiments with LLaMA-3-8B, extending its context length from 8K to 64K, show
that LongFilter efficiently selects high-quality data and yields substantial
improvements on benchmarks such as HELMET, LongBench, and RULER.

</details>


### [8] [Ideology-Based LLMs for Content Moderation](https://arxiv.org/abs/2510.25805)
*Stefano Civelli,Pietro Bernardelle,Nardiena A. Pratama,Gianluca Demartini*

Main category: cs.CL

TL;DR: 该研究调查了角色扮演如何影响大型语言模型 (LLM) 在有害内容分类中的一致性和公平性。


<details>
  <summary>Details</summary>
Motivation: 确保内容审核系统中公平和中立至关重要，因此需要研究角色扮演对LLM的影响。

Method: 跨不同的LLM架构、模型大小和内容模态（语言与视觉）分析角色扮演的影响，并通过一致性分析和政治导向任务研究。

Result: 角色扮演对整体分类准确率影响不大，但会使模型带有意识形态倾向，从而影响其对内容有害性的判断。模型会更倾向于与相同政治意识形态的角色保持一致，并 الدفاع 其观点。

Conclusion: 角色扮演会给LLM输出带来微妙的意识形态偏差，引发了人们对AI系统可能在表面中立下加强党派观点的担忧。

Abstract: Large language models (LLMs) are increasingly used in content moderation
systems, where ensuring fairness and neutrality is essential. In this study, we
examine how persona adoption influences the consistency and fairness of harmful
content classification across different LLM architectures, model sizes, and
content modalities (language vs. vision). At first glance, headline performance
metrics suggest that personas have little impact on overall classification
accuracy. However, a closer analysis reveals important behavioral shifts.
Personas with different ideological leanings display distinct propensities to
label content as harmful, showing that the lens through which a model "views"
input can subtly shape its judgments. Further agreement analyses highlight that
models, particularly larger ones, tend to align more closely with personas from
the same political ideology, strengthening within-ideology consistency while
widening divergence across ideological groups. To show this effect more
directly, we conducted an additional study on a politically targeted task,
which confirmed that personas not only behave more coherently within their own
ideology but also exhibit a tendency to defend their perspective while
downplaying harmfulness in opposing views. Together, these findings highlight
how persona conditioning can introduce subtle ideological biases into LLM
outputs, raising concerns about the use of AI systems that may reinforce
partisan perspectives under the guise of neutrality.

</details>


### [9] [Beyond Long Context: When Semantics Matter More than Tokens](https://arxiv.org/abs/2510.25816)
*Tarun Kumar Chawdhury,Jon D. Duke*

Main category: cs.CL

TL;DR: CLEAR方法在处理电子病历中的语义问答任务时，在效率和准确性方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 电子病历以base64编码附件形式存储，使得语义问答变得困难。传统向量数据库方法忽略了细微的临床关系。

Method: 开发了一个临床笔记问答评估平台，用于验证CLEAR方法（一种实体感知检索方法）。

Result: CLEAR方法实现了58.3%的胜率，平均语义相似度为0.878，并且使用的token减少了78%。在长笔记上的表现提升最为显著。

Conclusion: 实体感知检索提高了临床自然语言处理的效率和准确性。该评估框架为评估临床问答系统提供了一个可重用和透明的基准。

Abstract: Electronic Health Records (EHR) store clinical documentation as base64
encoded attachments in FHIR DocumentReference resources, which makes semantic
question answering difficult. Traditional vector database methods often miss
nuanced clinical relationships. The Clinical Entity Augmented Retrieval (CLEAR)
method, introduced by Lopez et al. 2025, uses entity aware retrieval and
achieved improved performance with an F1 score of 0.90 versus 0.86 for
embedding based retrieval, while using over 70 percent fewer tokens. We
developed a Clinical Notes QA Evaluation Platform to validate CLEAR against
zero shot large context inference and traditional chunk based retrieval
augmented generation. The platform was tested on 12 clinical notes ranging from
10,000 to 65,000 tokens representing realistic EHR content. CLEAR achieved a
58.3 percent win rate, an average semantic similarity of 0.878, and used 78
percent fewer tokens than wide context processing. The largest performance
gains occurred on long notes, with a 75 percent win rate for documents
exceeding 65,000 tokens. These findings confirm that entity aware retrieval
improves both efficiency and accuracy in clinical natural language processing.
The evaluation framework provides a reusable and transparent benchmark for
assessing clinical question answering systems where semantic precision and
computational efficiency are critical.

</details>


### [10] [A Survey on Efficient Large Language Model Training: From Data-centric Perspectives](https://arxiv.org/abs/2510.25817)
*Junyu Luo,Bohan Wu,Xiao Luo,Zhiping Xiao,Yiqiao Jin,Rong-Cheng Tu,Nan Yin,Yifan Wang,Jingyang Yuan,Wei Ju,Ming Zhang*

Main category: cs.CL

TL;DR: 本研究综述了数据高效的大型语言模型（LLM）后训练方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练范式面临数据挑战，如高昂的标注成本和数据规模的边际效益递减，因此，实现数据高效的后训练成为关键的研究问题。

Method: 从数据中心角度对数据高效的LLM后训练进行了首次系统性综述。提出了数据高效的LLM后训练方法分类，涵盖数据选择、数据质量提升、合成数据生成、数据蒸馏与压缩以及自我进化数据生态系统。

Result: 总结了每个类别的代表性方法，并概述了未来的研究方向。强调了数据高效LLM后训练中的挑战，突出了开放性问题，并提出了潜在的研究途径。

Conclusion: 希望这项工作能够激发人们进一步探索在大型模型训练中最大化数据利用潜力的方向。

Abstract: Post-training of Large Language Models (LLMs) is crucial for unlocking their
task generalization potential and domain-specific capabilities. However, the
current LLM post-training paradigm faces significant data challenges, including
the high costs of manual annotation and diminishing marginal returns on data
scales. Therefore, achieving data-efficient post-training has become a key
research question. In this paper, we present the first systematic survey of
data-efficient LLM post-training from a data-centric perspective. We propose a
taxonomy of data-efficient LLM post-training methods, covering data selection,
data quality enhancement, synthetic data generation, data distillation and
compression, and self-evolving data ecosystems. We summarize representative
approaches in each category and outline future research directions. By
examining the challenges in data-efficient LLM post-training, we highlight open
problems and propose potential research avenues. We hope our work inspires
further exploration into maximizing the potential of data utilization in
large-scale model training. Paper List:
https://github.com/luo-junyu/Awesome-Data-Efficient-LLM

</details>


### [11] [Inside CORE-KG: Evaluating Structured Prompting and Coreference Resolution for Knowledge Graphs](https://arxiv.org/abs/2510.26512)
*Dipak Meher,Carlotta Domeniconi*

Main category: cs.CL

TL;DR: 本文研究了如何从法律文件中构建知识图谱，以分析人口走私网络。现有的方法存在节点重复和噪声的问题。本文提出的CORE-KG框架通过引入类型感知的共指模块和领域指导的结构化提示来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 现有人口走私网络分析困难，法律文件包含关键信息但难以提取，现有的基于LLM的方法会产生重复和嘈杂的图。

Method: 本文对CORE-KG框架进行了消融研究，评估了类型感知的共指模块和领域指导的结构化提示的贡献。

Result: 移除共指消解导致节点重复增加28.32%，噪声节点增加4.32%；移除结构化提示导致节点重复增加4.34%，噪声节点增加73.33%。

Conclusion: 本文的发现为设计基于LLM的pipeline提供了经验指导，以从复杂的法律文本中提取结构化表示。

Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze.
Legal case documents offer critical insights but are often unstructured,
lexically dense, and filled with ambiguous or shifting references, which pose
significant challenges for automated knowledge graph (KG) construction. While
recent LLM-based approaches improve over static templates, they still generate
noisy, fragmented graphs with duplicate nodes due to the absence of guided
extraction and coreference resolution. The recently proposed CORE-KG framework
addresses these limitations by integrating a type-aware coreference module and
domain-guided structured prompts, significantly reducing node duplication and
legal noise. In this work, we present a systematic ablation study of CORE-KG to
quantify the individual contributions of its two key components. Our results
show that removing coreference resolution results in a 28.32% increase in node
duplication and a 4.32% increase in noisy nodes, while removing structured
prompts leads to a 4.34% increase in node duplication and a 73.33% increase in
noisy nodes. These findings offer empirical insights for designing robust
LLM-based pipelines for extracting structured representations from complex
legal texts.

</details>


### [12] [Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized Setting: the Case of FrameNet Annotation](https://arxiv.org/abs/2510.25904)
*Frederico Belcavello,Ely Matos,Arthur Lorenzi,Lisandra Bonoto,Lívia Ruiz,Luiz Fernando Pereira,Victor Herbst,Yulla Navarro,Helen de Andrade Abreu,Lívia Dutra,Tiago Timponi Torrent*

Main category: cs.CL

TL;DR: 评估了基于LLM的应用程序在创建语言资源和数据集方面的应用，特别是在语义角色标注方面。


<details>
  <summary>Details</summary>
Motivation: 研究动机是现有研究缺乏对LLM工具在语言资源创建中的性能和影响的全面评估，尤其是在NLP的视角化方法下。

Method: 比较了三种实验设置：手动、自动和半自动标注，考察了标注时间、覆盖率和多样性。

Result: 半自动标注设置在框架多样性方面有所提高，标注覆盖率与纯手动设置相似，而自动设置在除标注时间外的所有指标上表现较差。

Conclusion: 混合的半自动标注设置可以提高框架多样性并保持相似的标注覆盖率。

Abstract: The use of LLM-based applications as a means to accelerate and/or substitute
human labor in the creation of language resources and dataset is a reality.
Nonetheless, despite the potential of such tools for linguistic research,
comprehensive evaluation of their performance and impact on the creation of
annotated datasets, especially under a perspectivized approach to NLP, is still
missing. This paper contributes to reduction of this gap by reporting on an
extensive evaluation of the (semi-)automatization of FrameNet-like semantic
annotation by the use of an LLM-based semantic role labeler. The methodology
employed compares annotation time, coverage and diversity in three experimental
settings: manual, automatic and semi-automatic annotation. Results show that
the hybrid, semi-automatic annotation setting leads to increased frame
diversity and similar annotation coverage, when compared to the human-only
setting, while the automatic setting performs considerably worse in all
metrics, except for annotation time.

</details>


### [13] [RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic Pipeline](https://arxiv.org/abs/2510.25941)
*André V. Duarte,Xuying li,Bin Zeng,Arlindo L. Oliveira,Lei Li,Zhuo Li*

Main category: cs.CL

TL;DR: RECAP是一个从大型语言模型中提取和验证记忆训练数据的流程，通过反馈循环和越狱模块来提高提取效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的训练数据不可检查，因此需要一种方法来了解模型所见内容。

Method: RECAP使用反馈驱动循环，通过二级语言模型评估初始提取尝试，识别差异并转化为修正提示，反馈给目标模型以指导后续生成。此外，包含一个越狱模块来检测和克服对齐引起的拒绝。

Result: 在EchoTrace基准测试中，RECAP相对于单次迭代方法取得了显著收益。例如，使用GPT-4.1时，受版权保护的文本提取的平均ROUGE-L得分从0.38提高到0.47，增长了近24%。

Conclusion: RECAP能有效从LLM中提取训练数据。

Abstract: If we cannot inspect the training data of a large language model (LLM), how
can we ever know what it has seen? We believe the most compelling evidence
arises when the model itself freely reproduces the target content. As such, we
propose RECAP, an agentic pipeline designed to elicit and verify memorized
training data from LLM outputs. At the heart of RECAP is a feedback-driven
loop, where an initial extraction attempt is evaluated by a secondary language
model, which compares the output against a reference passage and identifies
discrepancies. These are then translated into minimal correction hints, which
are fed back into the target model to guide subsequent generations. In
addition, to address alignment-induced refusals, RECAP includes a jailbreaking
module that detects and overcomes such barriers. We evaluate RECAP on
EchoTrace, a new benchmark spanning over 30 full books, and the results show
that RECAP leads to substantial gains over single-iteration approaches. For
instance, with GPT-4.1, the average ROUGE-L score for the copyrighted text
extraction improved from 0.38 to 0.47 - a nearly 24% increase.

</details>


### [14] [Revisiting Multilingual Data Mixtures in Language Model Pretraining](https://arxiv.org/abs/2510.25947)
*Negar Foroutan,Paul Teiletche,Ayush Kumar Tarun,Antoine Bosselut*

Main category: cs.CL

TL;DR: 研究了大型语言模型 (LLM) 预训练中不同多语言数据混合的影响，挑战了关于语言覆盖范围和模型性能之间权衡的常见观点。


<details>
  <summary>Details</summary>
Motivation: 探讨多语言预训练中语言覆盖范围和模型性能之间的潜在权衡。

Method: 在不同的多语言语料库上训练了 1.1B 和 3B 参数的 LLM，语言数量从 25 种到 400 种不等。

Result: 1. 结合英语和多语言数据不一定会降低两者的语言性能。2. 使用英语作为枢轴语言可以跨语言系列产生益处。3. 随着训练语言数量的增加，没有观察到明显的“多语言诅咒”。

Conclusion: 适当平衡的多语言数据可以增强语言模型的能力，而不会影响性能，即使在低资源环境中也是如此。

Abstract: The impact of different multilingual data mixtures in pretraining large
language models (LLMs) has been a topic of ongoing debate, often raising
concerns about potential trade-offs between language coverage and model
performance (i.e., the curse of multilinguality). In this work, we investigate
these assumptions by training 1.1B and 3B parameter LLMs on diverse
multilingual corpora, varying the number of languages from 25 to 400. Our study
challenges common beliefs surrounding multilingual training. First, we find
that combining English and multilingual data does not necessarily degrade the
in-language performance of either group, provided that languages have a
sufficient number of tokens included in the pretraining corpus. Second, we
observe that using English as a pivot language (i.e., a high-resource language
that serves as a catalyst for multilingual generalization) yields benefits
across language families, and contrary to expectations, selecting a pivot
language from within a specific family does not consistently improve
performance for languages within that family. Lastly, we do not observe a
significant "curse of multilinguality" as the number of training languages
increases in models at this scale. Our findings suggest that multilingual data,
when balanced appropriately, can enhance language model capabilities without
compromising performance, even in low-resource settings

</details>


### [15] [Semantic Label Drift in Cross-Cultural Translation](https://arxiv.org/abs/2510.25967)
*Mohsinul Kabir,Tasnim Ahmed,Md Mezbaur Rahman,Polydoros Giannouris,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 机器翻译在低资源语言中被广泛使用，但文化差异会导致语义标签漂移。


<details>
  <summary>Details</summary>
Motivation: 探讨机器翻译中文化对齐的重要性，以及文化差异如何影响翻译质量。

Method: 通过一系列跨文化敏感和中性领域的实验。

Result: （1）机器翻译系统（包括大型语言模型）在翻译过程中会引起标签漂移，尤其是在文化敏感领域；（2）与早期的统计机器翻译工具不同，大型语言模型编码了文化知识，利用这些知识可以放大标签漂移；（3）源语言和目标语言之间的文化相似性或差异是标签保留的关键决定因素。

Conclusion: 在机器翻译中忽略文化因素不仅会损害标签的保真度，还会导致下游应用中的误解和文化冲突。

Abstract: Machine Translation (MT) is widely employed to address resource scarcity in
low-resource languages by generating synthetic data from high-resource
counterparts. While sentiment preservation in translation has long been
studied, a critical but underexplored factor is the role of cultural alignment
between source and target languages. In this paper, we hypothesize that
semantic labels are drifted or altered during MT due to cultural divergence.
Through a series of experiments across culturally sensitive and neutral
domains, we establish three key findings: (1) MT systems, including modern
Large Language Models (LLMs), induce label drift during translation,
particularly in culturally sensitive domains; (2) unlike earlier statistical MT
tools, LLMs encode cultural knowledge, and leveraging this knowledge can
amplify label drift; and (3) cultural similarity or dissimilarity between
source and target languages is a crucial determinant of label preservation. Our
findings highlight that neglecting cultural factors in MT not only undermines
label fidelity but also risks misinterpretation and cultural conflict in
downstream applications.

</details>


### [16] [SymCode: A Neurosymbolic Approach to Mathematical Reasoning via Verifiable Code Generation](https://arxiv.org/abs/2510.25975)
*Sina Bagheri Nezhad,Yao Li,Ameeta Agrawal*

Main category: cs.CL

TL;DR: SymCode: Uses code generation with SymPy for verifiable math problem-solving, improving accuracy and transparency.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with mathematical reasoning due to unreliable prose-based solutions and lack of verification mechanisms.

Method: Reframes problem-solving as verifiable code generation using SymPy.

Result: Significant accuracy improvements (up to 13.6%) on MATH-500 and OlympiadBench.

Conclusion: SymCode improves accuracy and shifts failures to transparent errors, enhancing trust in AI for formal domains.

Abstract: Large Language Models (LLMs) often struggle with complex mathematical
reasoning, where prose-based generation leads to unverified and arithmetically
unsound solutions. Current prompting strategies like Chain of Thought still
operate within this unreliable medium, lacking a mechanism for deterministic
verification. To address these limitations, we introduce SymCode, a
neurosymbolic framework that reframes mathematical problem-solving as a task of
verifiable code generation using the SymPy library. We evaluate SymCode on
challenging benchmarks, including MATH-500 and OlympiadBench, demonstrating
significant accuracy improvements of up to 13.6 percentage points over
baselines. Our analysis shows that SymCode is not only more token-efficient but
also fundamentally shifts model failures from opaque logical fallacies towards
transparent, programmatic errors. By grounding LLM reasoning in a deterministic
symbolic engine, SymCode represents a key step towards more accurate and
trustworthy AI in formal domains.

</details>


### [17] [NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium](https://arxiv.org/abs/2510.25977)
*Dinghong Song,Jierui Xu,Weichu Yang,Pengfei Su,Dong Li*

Main category: cs.CL

TL;DR: 本文介绍了针对亚马逊AWS开发的AI加速器Trainium上的LLM推理的高性能矩阵乘法（matmul）的设计。


<details>
  <summary>Details</summary>
Motivation: 利用Trainium架构实现高性能具有挑战性，因为它采用 systolic 阵列架构，并且对数据布局有特殊要求。

Method: 本文设计了一系列针对Trainium定制的技术，包括内核融合和新型缓存策略，以减少跨软件管理内存层次结构的数据移动，最大化SRAM带宽，并避免昂贵的矩阵转置。

Result: 在九个数据集和四个最新的LLM上进行评估，结果表明，该系统在很大程度上优于AWS在Trainium上实现的最新matmul：在matmul内核级别，它实现了平均1.35倍的加速（高达2.22倍），这转化为端到端LLM推理的平均1.66倍加速（高达2.49倍）。

Conclusion: 本文成功地在Trainium上实现了高性能的矩阵乘法，并显著提高了LLM推理的速度。

Abstract: AI accelerators, customized to AI workloads, provide cost-effective and
high-performance solutions for training and inference. Trainium, an AI
accelerator recently developed by Amazon Web Services (AWS), provides an
attractive option for LLM training and inference through its heterogeneous
architecture. However, leveraging Trainium architecture for high performance
can be challenging because of its systolic array architecture and special
requirement on data layout. In this paper, we design high-performance matrix
multiplication (matmul), a critical compute kernel, for LLM inference on
Trainium. We introduce a series of techniques customized to Trainium based on
kernel fusion and novel caching strategies to reduce data movement across the
software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive
matrix transpose. Evaluating with nine datasets and four recent LLMs, we show
that our system largely outperforms the state-of-the-art matmul implemented by
AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x
speedup (up to 2.22x), which translates to an average 1.66x speedup (up to
2.49x) for end-to-end LLM inference.

</details>


### [18] [AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache](https://arxiv.org/abs/2510.25979)
*Dinghong Song,Yuan Feng,Yiwei Wang,Shangye Chen,Cyril Guyot,Filip Blagojevic,Hyeran Jeon,Pengfei Su,Dong Li*

Main category: cs.CL

TL;DR: AttnCache通过检索和重用相似的注意力图来加速LLM推理的预填充阶段，减少自注意力的计算开销。


<details>
  <summary>Details</summary>
Motivation: 在仅预填充的场景中，自注意力计算成为主要的性能瓶颈，因为它与序列长度的复杂度呈二次方关系。

Method: AttnCache构建了一个注意力图记忆数据库，采用高效的缓存和相似性搜索技术，在推理过程中识别和重用预缓存的注意力图。

Result: AttnCache在CPU上实现了平均1.2倍的端到端加速和2倍的注意力加速，在GPU上实现了1.6倍的端到端加速和3倍的注意力加速，且精度下降可忽略不计。

Conclusion: AttnCache通过重用相似的注意力图来加速LLM推理的预填充阶段，显著提高了性能。

Abstract: Large Language Models (LLMs) are widely used in generative applications such
as chatting, code generation, and reasoning. However, many realworld workloads
such as classification, question answering, recommendation, and text embedding
rely solely on the prefill stage of inference, where the model encodes input
sequences without performing autoregressive decoding. In these prefill only
scenarios, the self-attention computation becomes the primary performance
bottleneck due to its quadratic complexity with respect to sequence length. In
this paper, we observe that semantically different sentences often produce
similar attention maps across layers and heads. Building on this insight, we
propose AttnCache, a framework that accelerates the prefill stage of LLM
inference by retrieving and reusing similar attention maps. Based on an
attention map memorization database, AttnCache employs efficient caching and
similarity search techniques to identify and reuse pre-cached attention maps
during inference, thereby reducing the computational overhead of
self-attention. Experimental results show that AttnCache achieves an average of
1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x
attention speedup on GPU, with negligible accuracy degradation.

</details>


### [19] [Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning](https://arxiv.org/abs/2510.25992)
*Yihe Deng,I-Hung Hsu,Jun Yan,Zifeng Wang,Rujun Han,Gufeng Zhang,Yanfei Chen,Wei Wang,Tomas Pfister,Chen-Yu Lee*

Main category: cs.CL

TL;DR: 提出了监督强化学习（SRL）框架，以解决大型语言模型（LLM）在多步骤推理问题上的困难，尤其是在小型开源模型中，强化学习（RLVR）和监督微调（SFT）表现不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在需要多步骤推理的问题上表现不佳。对于小型开源模型，即使经过多次尝试，强化学习与可验证奖励（RLVR）也常常因为难以采样到正确的解决方案而失败，而监督微调（SFT）则容易通过严格的逐个 token 模仿过度拟合长篇演示。

Method: 提出了监督强化学习（SRL）框架，将问题解决重新定义为生成一系列逻辑“动作”。SRL 训练模型在每次行动前生成内部推理独白，并根据模型行动与 SFT 数据集中提取的专家行动之间的相似性，逐步提供更平滑的奖励。

Result: SRL 使得小型模型能够学习以前 SFT 或 RLVR 无法学习的具有挑战性的问题。此外，在用 RLVR 改进之前，先用 SRL 初始化训练，可以产生最强的整体性能。SRL 可以有效地推广到代理软件工程任务。

Conclusion: SRL 是一种鲁棒且通用的面向推理的 LLM 训练框架。

Abstract: Large Language Models (LLMs) often struggle with problems that require
multi-step reasoning. For small-scale open-source models, Reinforcement
Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely
sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to
overfit long demonstrations through rigid token-by-token imitation. To address
this gap, we propose Supervised Reinforcement Learning (SRL), a framework that
reformulates problem solving as generating a sequence of logical "actions". SRL
trains the model to generate an internal reasoning monologue before committing
to each action. It provides smoother rewards based on the similarity between
the model's actions and expert actions extracted from the SFT dataset in a
step-wise manner. This supervision offers richer learning signals even when all
rollouts are incorrect, while encouraging flexible reasoning guided by expert
demonstrations. As a result, SRL enables small models to learn challenging
problems previously unlearnable by SFT or RLVR. Moreover, initializing training
with SRL before refining with RLVR yields the strongest overall performance.
Beyond reasoning benchmarks, SRL generalizes effectively to agentic software
engineering tasks, establishing it as a robust and versatile training framework
for reasoning-oriented LLMs.

</details>


### [20] [PORTool: Tool-Use LLM Training with Rewarded Tree](https://arxiv.org/abs/2510.26020)
*Feijie Wu,Weiwu Zhu,Yuxiang Zhang,Soumya Chatterjee,Jiarong Zhu,Fan Mo,Rodin Luo,Jing Gao*

Main category: cs.CL

TL;DR: PORTool是一种强化学习方法，旨在鼓励工具使用LLM探索各种产生正确答案的轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有的工具使用LLM在静态数据集上训练，无法探索可能的解决方案，并且在不断发展的动态工具调用环境中表现出有限的性能。

Method: 该方法首先为给定的查询生成多个rollout，其中一些rollout共享前几个工具调用步骤，从而形成一个树状结构。接下来，我们根据每个步骤产生正确答案和成功工具调用的能力来分配奖励。最后，这些逐步奖励用于计算fork相对优势，并与轨迹相对优势混合，以训练LLM以供工具使用。

Result: 实验利用17个工具来解决用户查询，涵盖时间敏感和时间不变的主题。我们进行了消融研究，以系统地证明逐步奖励的必要性和设计稳健性。此外，我们将提出的PORTool与其他训练方法进行了比较，并在最终准确性和工具调用步骤的数量方面证明了显着改进。

Conclusion: 提出的PORTool在最终准确性和工具调用步骤的数量方面表现出显著改进

Abstract: Current tool-use large language models (LLMs) are trained on static datasets,
enabling them to interact with external tools and perform multi-step,
tool-integrated reasoning, which produces tool-call trajectories. However,
these models imitate how a query is resolved in a generic tool-call routine,
thereby failing to explore possible solutions and demonstrating limited
performance in an evolved, dynamic tool-call environment. In this work, we
propose PORTool, a reinforcement learning (RL) method that encourages a
tool-use LLM to explore various trajectories yielding the correct answer.
Specifically, this method starts with generating multiple rollouts for a given
query, and some of them share the first few tool-call steps, thereby forming a
tree-like structure. Next, we assign rewards to each step, based on its ability
to produce a correct answer and make successful tool calls. A shared step
across different trajectories receives the same reward, while different steps
under the same fork receive different rewards. Finally, these step-wise rewards
are used to calculate fork-relative advantages, blended with
trajectory-relative advantages, to train the LLM for tool use. The experiments
utilize 17 tools to address user queries, covering both time-sensitive and
time-invariant topics. We conduct ablation studies to systematically justify
the necessity and the design robustness of step-wise rewards. Furthermore, we
compare the proposed PORTool with other training approaches and demonstrate
significant improvements in final accuracy and the number of tool-call steps.

</details>


### [21] [Rethinking Cross-lingual Alignment: Balancing Transfer and Cultural Erasure in Multilingual LLMs](https://arxiv.org/abs/2510.26024)
*HyoJung Han,Sweta Agrawal,Eleftheria Briakou*

Main category: cs.CL

TL;DR: 跨语言对齐旨在对齐多语言表示，但也可能导致“文化抹除”，即模型无法根据查询语言提供具有文化背景的响应。本文提出了一个全面的评估框架，用于量化知识转移和文化抹除，并发现当前的跨语言对齐方法在提高知识转移的同时，会牺牲文化适应性。基于对模型内部表示的分析，本文提出了一种新的推理时方法，称为 Surgical Steering，通过在不同层应用 targeted activation steering，以更好地平衡知识转移和文化适应性。


<details>
  <summary>Details</summary>
Motivation: 跨语言对齐旨在使大型语言模型能够跨语言无缝转移知识，但这种追求表征融合可能会导致“文化抹除”，即模型无法根据查询语言提供具有文化背景的响应。

Method: 本文提出了一个全面的评估框架，即 transfer-localization plane，用于量化知识转移和文化抹除。基于对模型内部表示的分析，本文提出了一种新的推理时方法，称为 Surgical Steering，通过在不同层应用 targeted activation steering，以更好地平衡知识转移和文化适应性。

Result: 当前的跨语言对齐方法在提高知识转移的同时，会牺牲文化适应性。Surgical Steering 方法能够更好地平衡知识转移和文化适应性。

Conclusion: 本文的研究表明，通用知识转移和特定文化知识可以在不同的模型层进行优化，Surgical Steering 方法能够有效克服当前对齐技术的局限性。

Abstract: Cross-lingual alignment (CLA) aims to align multilingual representations,
enabling Large Language Models (LLMs) to seamlessly transfer knowledge across
languages. While intuitive, we hypothesize, this pursuit of representational
convergence can inadvertently cause "cultural erasure", the functional loss of
providing culturally-situated responses that should diverge based on the query
language. In this work, we systematically analyze this trade-off by introducing
a holistic evaluation framework, the transfer-localization plane, which
quantifies both desirable knowledge transfer and undesirable cultural erasure.
Using this framework, we re-evaluate recent CLA approaches and find that they
consistently improve factual transfer at the direct cost of cultural
localization across all six languages studied. Our investigation into the
internal representations of these models reveals a key insight: universal
factual transfer and culturally-specific knowledge are optimally steerable at
different model layers. Based on this finding, we propose Surgical Steering, a
novel inference-time method that disentangles these two objectives. By applying
targeted activation steering to distinct layers, our approach achieves a better
balance between the two competing dimensions, effectively overcoming the
limitations of current alignment techniques.

</details>


### [22] [Artificial Intelligence-Enabled Analysis of Radiology Reports: Epidemiology and Consequences of Incidental Thyroid Findings](https://arxiv.org/abs/2510.26032)
*Felipe Larios,Mariana Borras-Osorio,Yuqi Wu,Ana Gabriela Claros,David Toro-Tobon,Esteban Cabezas,Ricardo Loor-Torres,Maria Mateo Chavez,Kerly Guevara Maldonado,Luis Vilatuna Andrango,Maria Lizarazo Jimenez,Ivan Mateo Alzamora,Misk Al Zahidy,Marcelo Montero,Ana Cristina Proano,Cristian Soto Jacome,Jungwei W. Fan,Oscar J. Ponce-Ponte,Megan E. Branda,Naykky Singh Ospina,Juan P. Brito*

Main category: cs.CL

TL;DR: 无意中发现的甲状腺病变(ITF)越来越常见，需要标准化报告和更具选择性的随访。


<details>
  <summary>Details</summary>
Motivation: 旨在评估ITF的患病率、特征和临床结果。

Method: 使用基于Transformer的NLP流程来识别放射报告中的ITF，并提取结节特征。

Result: 在115,683名患者中，9,077名(7.8%)有ITF，其中92.9%为结节。与没有ITF的患者相比，有ITF的患者患甲状腺结节诊断、活检、甲状腺切除术和甲状腺癌诊断的几率更高。

Conclusion: ITF很常见，与导致检测出小的、低风险癌症的事件密切相关。

Abstract: Importance Incidental thyroid findings (ITFs) are increasingly detected on
imaging performed for non-thyroid indications. Their prevalence, features, and
clinical consequences remain undefined. Objective To develop, validate, and
deploy a natural language processing (NLP) pipeline to identify ITFs in
radiology reports and assess their prevalence, features, and clinical outcomes.
Design, Setting, and Participants Retrospective cohort of adults without prior
thyroid disease undergoing thyroid-capturing imaging at Mayo Clinic sites from
July 1, 2017, to September 30, 2023. A transformer-based NLP pipeline
identified ITFs and extracted nodule characteristics from image reports from
multiple modalities and body regions. Main Outcomes and Measures Prevalence of
ITFs, downstream thyroid ultrasound, biopsy, thyroidectomy, and thyroid cancer
diagnosis. Logistic regression identified demographic and imaging-related
factors. Results Among 115,683 patients (mean age, 56.8 [SD 17.2] years; 52.9%
women), 9,077 (7.8%) had an ITF, of which 92.9% were nodules. ITFs were more
likely in women, older adults, those with higher BMI, and when imaging was
ordered by oncology or internal medicine. Compared with chest CT, ITFs were
more likely via neck CT, PET, and nuclear medicine scans. Nodule
characteristics were poorly documented, with size reported in 44% and other
features in fewer than 15% (e.g. calcifications). Compared with patients
without ITFs, those with ITFs had higher odds of thyroid nodule diagnosis,
biopsy, thyroidectomy and thyroid cancer diagnosis. Most cancers were
papillary, and larger when detected after ITFs vs no ITF. Conclusions ITFs were
common and strongly associated with cascades leading to the detection of small,
low-risk cancers. These findings underscore the role of ITFs in thyroid cancer
overdiagnosis and the need for standardized reporting and more selective
follow-up.

</details>


### [23] [QCoder Benchmark: Bridging Language Generation and Quantum Hardware through Simulator-Based Feedback](https://arxiv.org/abs/2510.26101)
*Taku Mikuriya,Tatsuya Ishigaki,Masayuki Kawarada,Shunya Minami,Tadashi Kadowaki,Yohichi Suzuki,Soshun Naito,Shunya Takata,Takumi Kato,Tamotsu Basseda,Reo Yamada,Hiroya Takamura*

Main category: cs.CL

TL;DR: 本文介绍了一个名为QCoder Benchmark的评估框架，用于评估大型语言模型在量子编程方面的能力，该框架通过模拟硬件设备提供反馈，并包含来自真实编程竞赛的人工编写代码，实验表明，即使是GPT-4o等先进模型在该基准测试中也表现不佳，而基于推理的模型表现优于人类编写的代码。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自动编程代码生成方面的应用日益广泛，但在需要与硬件设备交互的领域（如量子编程）仍有待探索。

Method: 提出了QCoder Benchmark评估框架，该框架支持使用量子模拟器环境进行评估，并结合了来自真实编程竞赛的人工编写代码。

Result: 实验表明，GPT-4o等先进模型在QCoder Benchmark上的准确率仅为18.97%左右，而基于推理的模型达到了78%的准确率，超过了人类编写代码的平均成功率（39.98%）。

Conclusion: QCoder Benchmark数据集和公共评估API的发布旨在支持进一步的研究，实验结果表明，即使是先进的LLM在量子编程方面仍有很大的提升空间，而基于推理的模型在某些情况下可以超越人类的表现。

Abstract: Large language models (LLMs) have increasingly been applied to automatic
programming code generation. This task can be viewed as a language generation
task that bridges natural language, human knowledge, and programming logic.
However, it remains underexplored in domains that require interaction with
hardware devices, such as quantum programming, where human coders write Python
code that is executed on a quantum computer. To address this gap, we introduce
QCoder Benchmark, an evaluation framework that assesses LLMs on quantum
programming with feedback from simulated hardware devices. Our benchmark offers
two key features. First, it supports evaluation using a quantum simulator
environment beyond conventional Python execution, allowing feedback of
domain-specific metrics such as circuit depth, execution time, and error
classification, which can be used to guide better generation. Second, it
incorporates human-written code submissions collected from real programming
contests, enabling both quantitative comparisons and qualitative analyses of
LLM outputs against human-written codes. Our experiments reveal that even
advanced models like GPT-4o achieve only around 18.97% accuracy, highlighting
the difficulty of the benchmark. In contrast, reasoning-based models such as o3
reach up to 78% accuracy, outperforming averaged success rates of human-written
codes (39.98%). We release the QCoder Benchmark dataset and public evaluation
API to support further research.

</details>


### [24] [Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock LLM Diverse Thinking](https://arxiv.org/abs/2510.26122)
*Feng Ju,Zeyu Qin,Rui Min,Zhitao He,Lingpeng Kong,Yi R. Fung*

Main category: cs.CL

TL;DR: 本文提出了一种“一个问题，多个解决方案”(1PNS) 的训练范式，以提高大型语言模型 (LLM) 的推理能力和输出多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）在推理能力方面存在瓶颈，模型输出的多样性较低，这主要是由于常见的“一个问题，一个解决方案”（1P1S）训练方式所导致。

Method: 提出了“一个问题，多个解决方案”（1PNS）的训练范式，并引入了推理路径差异（RPD）指标，用于衡量多步思维链之间的语义差异。

Result: 实验表明，使用 RPD 选择的训练数据可以产生更多样化的输出，并且 pass@k 指标更高，在 pass@16 上比 1P1S 基线平均提高了 +2.80%，在 AIME24 上提高了 +4.99%。

Conclusion: 1PNS 进一步提升了 TTS 的有效性。

Abstract: While Test-Time Scaling (TTS) has proven effective in improving the reasoning
ability of large language models (LLMs), low diversity in model outputs often
becomes a bottleneck; this is partly caused by the common "one problem, one
solution" (1P1S) training practice, which provides a single canonical answer
and can push models toward a narrow set of reasoning paths. To address this, we
propose a "one problem, multiple solutions" (1PNS) training paradigm that
exposes the model to a variety of valid reasoning trajectories and thus
increases inference diversity. A core challenge for 1PNS is reliably measuring
semantic differences between multi-step chains of thought, so we introduce
Reasoning Path Divergence (RPD), a step-level metric that aligns and scores
Long Chain-of-Thought solutions to capture differences in intermediate
reasoning. Using RPD, we curate maximally diverse solution sets per problem and
fine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields
more varied outputs and higher pass@k, with an average +2.80% gain in pass@16
over a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that
1PNS further amplifies the effectiveness of TTS. Our code is available at
https://github.com/fengjujf/Reasoning-Path-Divergence .

</details>


### [25] [On the Influence of Discourse Relations in Persuasive Texts](https://arxiv.org/abs/2510.26124)
*Nawar Turk,Sevag Kaspar,Leila Kosseim*

Main category: cs.CL

TL;DR: 本文研究了说服技巧 (PTs) 和语篇关系 (DRs) 之间的关系，利用大型语言模型 (LLMs) 和提示工程。通过在 SemEval 2023 Task 3 数据集上训练 LLM 分类器，为数据集中的每个实例标注了 22 种 PDTB 3.0 level-2 DRs 中的一种。总共评估了四个 LLM，使用了 10 种不同的提示，从而产生了 40 个独特的 DR 分类器。使用不同的多数池化策略的集成模型被用于创建 5 个银数据集，这些数据集的实例同时标注了说服技巧和 level-2 PDTB 语义。统计分析表明，六种语篇关系（即原因、目的、对比、原因+信念、让步和条件）在说服性文本中起着关键作用，尤其是在使用 Loaded Language、Exaggeration/Minimisation、Repetition 和 to cast Doubt 时。


<details>
  <summary>Details</summary>
Motivation: 目前没有标注 PTs 和 DRs 的数据集，而理解这两者之间的关系有助于检测在线宣传和错误信息，以及我们对有效沟通的理解。

Method: 利用 LLMs 和提示工程，在 SemEval 2023 Task 3 数据集上训练 LLM 分类器，为数据集中的每个实例标注了 22 种 PDTB 3.0 level-2 DRs 中的一种。使用不同的多数池化策略的集成模型被用于创建 5 个银数据集。

Result: 六种语篇关系（即原因、目的、对比、原因+信念、让步和条件）在说服性文本中起着关键作用，尤其是在使用 Loaded Language、Exaggeration/Minimisation、Repetition 和 to cast Doubt 时。

Conclusion: 该研究可以帮助检测在线宣传和错误信息，并加深我们对有效沟通的理解。

Abstract: This paper investigates the relationship between Persuasion Techniques (PTs)
and Discourse Relations (DRs) by leveraging Large Language Models (LLMs) and
prompt engineering. Since no dataset annotated with both PTs and DRs exists, we
took the SemEval 2023 Task 3 dataset labelled with 19 PTs as a starting point
and developed LLM-based classifiers to label each instance of the dataset with
one of the 22 PDTB 3.0 level-2 DRs. In total, four LLMs were evaluated using 10
different prompts, resulting in 40 unique DR classifiers. Ensemble models using
different majority-pooling strategies were used to create 5 silver datasets of
instances labelled with both persuasion techniques and level-2 PDTB senses. The
silver dataset sizes vary from 1,281 instances to 204 instances, depending on
the majority pooling technique used. Statistical analysis of these silver
datasets shows that six discourse relations (namely Cause, Purpose, Contrast,
Cause+Belief, Concession, and Condition) play a crucial role in persuasive
texts, especially in the use of Loaded Language, Exaggeration/Minimisation,
Repetition and to cast Doubt. This insight can contribute to detecting online
propaganda and misinformation, as well as to our general understanding of
effective communication.

</details>


### [26] [MossNet: Mixture of State-Space Experts is a Multi-Head Attention](https://arxiv.org/abs/2510.26182)
*Shikhar Tuli,James Seale Smith,Haris Jeelani,Chi-Heng Lin,Abhishek Patel,Vasili Ramanishka,Yen-Chang Hsu,Hongxia Jin*

Main category: cs.CL

TL;DR: MossNet是一种新型的混合状态空间专家架构，它模拟了线性多头注意力机制，并在语言建模和下游评估中优于现有的Transformer和SSM架构。


<details>
  <summary>Details</summary>
Motivation: 当前基于SSM/GRM的方法通常只模拟单个注意力头，限制了其表达能力。

Method: 提出了MossNet，一种在通道混合MLP块和时间混合SSM内核中利用混合专家(MoE)实现多头注意力的架构。

Result: 实验表明，MossNet在语言建模和下游评估中优于同等规模和数据预算的Transformer和SSM架构，并且具有良好的可扩展性和性能。在Samsung Galaxy S24 Ultra和Nvidia A100 GPU上的实际设备分析表明，与类似大小的基线相比，MossNet具有良好的运行时速度和资源利用率。

Conclusion: MossNet为高效、高性能的循环LLM架构提供了一个引人注目的新方向。

Abstract: Large language models (LLMs) have significantly advanced generative
applications in natural language processing (NLP). Recent trends in model
architectures revolve around efficient variants of transformers or
state-space/gated-recurrent models (SSMs, GRMs). However, prevailing
SSM/GRM-based methods often emulate only a single attention head, potentially
limiting their expressiveness. In this work, we propose MossNet, a novel
mixture-of-state-space-experts architecture that emulates a linear multi-head
attention (MHA). MossNet leverages a mixture-of-experts (MoE) implementation
not only in channel-mixing multi-layered perceptron (MLP) blocks but also in
the time-mixing SSM kernels to realize multiple "attention heads." Extensive
experiments on language modeling and downstream evaluations show that MossNet
outperforms both transformer- and SSM-based architectures of similar model size
and data budgets. Larger variants of MossNet, trained on trillions of tokens,
further confirm its scalability and superior performance. In addition,
real-device profiling on a Samsung Galaxy S24 Ultra and an Nvidia A100 GPU
demonstrate favorable runtime speed and resource usage compared to similarly
sized baselines. Our results suggest that MossNet is a compelling new direction
for efficient, high-performing recurrent LLM architectures.

</details>


### [27] [Similarity-Distance-Magnitude Language Models](https://arxiv.org/abs/2510.26183)
*Allen Schmaltz*

Main category: cs.CL

TL;DR: 提出了相似度-距离-幅度 (SDM) 语言模型，通过微调来最大化校准良好的高概率区域中的生成比例。


<details>
  <summary>Details</summary>
Motivation: 为了提高指令遵循模型的统计效率，减少模型拒绝回答的情况。

Method: 通过监督微调将预训练的decoder-only Transformer LMs转换为SDM LMs，使用final-layer SDM激活层来估计对比输入编码方案的基变更，并在训练期间在线生成额外的hard negative examples。

Result: 与强大的监督基线相比，减少了模型拒绝回答的情况（即，提高了统计效率）。

Conclusion: SDM 语言模型能够有效提高指令遵循模型的性能。

Abstract: We introduce Similarity-Distance-Magnitude (SDM) language models (LMs), which
are sequence prediction models fine-tuned to maximize the proportion of
generations in the well-calibrated, high-probability region partitioned by a
final-layer SDM activation layer used for binary classification of
instruction-following. We demonstrate that existing pre-trained decoder-only
Transformer LMs can be readily converted into SDM LMs via supervised
fine-tuning, using the final-layer SDM activation layer during training to
estimate a change-of-base for a supervised next-token loss over a contrastive
input encoding scheme, with additional hard negative examples generated online
during training. This results in reduced abstentions (i.e., improved
statistical efficiency) compared to strong supervised baselines.

</details>


### [28] [RCScore: Quantifying Response Consistency in Large Language Models](https://arxiv.org/abs/2510.26193)
*Dongjun Jang,Youngchae Ahn,Hyopil Shin*

Main category: cs.CL

TL;DR: 论文提出 RCScore 框架，用于量化指令风格对 LLM 响应的影响，揭示了传统评估指标无法检测到的性能变化。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 评估通常依赖于单一指令模板，忽略了模型对指令风格的敏感性，而这对于实际部署至关重要。

Method: 通过系统地将基准问题转换为多种指令风格，使用 RCScore 框架来评估模型性能。引入 Cross-Response Similarity (CRS) 方法来衡量风格自洽性。

Result: 实验表明，指令风格可以使准确率变化高达 16.7%。CRS 与任务准确率高度相关，可作为模型可靠性的代理。确定性解码产生更稳定的输出，模型规模与跨风格一致性正相关。

Conclusion: RCScore 提供了一种评估指令鲁棒性的原则性方法。

Abstract: Current LLM evaluations often rely on a single instruction template,
overlooking models' sensitivity to instruction style-a critical aspect for
real-world deployments. We present RCScore, a multi-dimensional framework
quantifying how instruction formulation affects model responses. By
systematically transforming benchmark problems into multiple instruction
styles, RCScore reveals performance variations undetected by conventional
metrics. Our experiments across ten LLMs on four reasoning benchmarks
demonstrate that instruction style can shift accuracy by up to 16.7% points. We
introduce Cross-Response Similarity (CRS), a method applying RCScore metrics to
measure stylistic self-consistency, and establish its strong correlation with
task accuracy, suggesting consistency as a valuable proxy for model
reliability. Additional findings show that deterministic decoding produces more
stylistically stable outputs, and model scale correlates positively with
cross-style consistency. RCScore offers a principled approach to assess
instruction robustness.

</details>


### [29] [Don't Let It Fade: Preserving Edits in Diffusion Language Models via Token Timestep Allocation](https://arxiv.org/abs/2510.26200)
*Woojin Kim,Jaeyoung Do*

Main category: cs.CL

TL;DR: 扩散语言模型(DLM)虽然可以进行细粒度的改进，但实际上难以控制。本文发现了一种叫做“更新遗忘”的失效模式，并对其进行了正式的描述。在这种模式中，统一和不考虑上下文的更新会导致时间步长上的token级别波动，从而抹去早期的语义编辑，扰乱累积改进过程，从而降低流畅性和连贯性。为了解决这个问题，我们提出了Token Timestep Allocation (TTA)，它通过每个token的时间步长表来实现软性和语义token排序：关键token被提前冻结，而不确定的token会继续改进。实验结果表明，TTA提高了可控性和流畅性。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型(DLM)虽然可以进行细粒度的改进，但实际上难以控制。我们发现了一种叫做“更新遗忘”的失效模式，在这种模式中，统一和不考虑上下文的更新会导致时间步长上的token级别波动，从而抹去早期的语义编辑，扰乱累积改进过程，从而降低流畅性和连贯性。因此，需要显式的token排序来实现有效的控制。

Method: 我们提出了Token Timestep Allocation (TTA)，它通过每个token的时间步长表来实现软性和语义token排序：关键token被提前冻结，而不确定的token会继续改进。这种基于时间步长的排序可以被实例化为固定策略或由任务信号驱动的自适应策略，从而支持广泛的改进策略。因为它纯粹在推理时操作，所以它可以统一地应用于各种DLM，并自然地扩展到不同的监督源。

Result: 在情感控制方面，TTA的准确率提高了20%以上，并且在不到五分之一的步骤中，困惑度降低了近一半；在解毒方面，TTA降低了最大毒性(12.2 vs 14.5)和困惑度(26.0 vs 32.0)。

Conclusion: 通过时间步长分配进行软排序是减轻更新遗忘和实现稳定和可控的扩散文本生成的关键。

Abstract: While diffusion language models (DLMs) enable fine-grained refinement, their
practical controllability remains fragile. We identify and formally
characterize a central failure mode called update forgetting, in which uniform
and context agnostic updates induce token level fluctuations across timesteps,
erasing earlier semantic edits and disrupting the cumulative refinement
process, thereby degrading fluency and coherence. As this failure originates in
uniform and context agnostic updates, effective control demands explicit token
ordering. We propose Token Timestep Allocation (TTA), which realizes soft and
semantic token ordering via per token timestep schedules: critical tokens are
frozen early, while uncertain tokens receive continued refinement. This
timestep based ordering can be instantiated as either a fixed policy or an
adaptive policy driven by task signals, thereby supporting a broad spectrum of
refinement strategies. Because it operates purely at inference time, it applies
uniformly across various DLMs and naturally extends to diverse supervision
sources. Empirically, TTA improves controllability and fluency: on sentiment
control, it yields more than 20 percent higher accuracy and nearly halves
perplexity using less than one fifth the steps; in detoxification, it lowers
maximum toxicity (12.2 versus 14.5) and perplexity (26.0 versus 32.0).
Together, these results demonstrate that softened ordering via timestep
allocation is the critical lever for mitigating update forgetting and achieving
stable and controllable diffusion text generation.

</details>


### [30] [What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data](https://arxiv.org/abs/2510.26202)
*Rajiv Movva,Smitha Milli,Sewon Min,Emma Pierson*

Main category: cs.CL

TL;DR: 本文介绍了一种名为WIMHF的方法，用于解释人类反馈数据，该方法使用稀疏自动编码器来提取人类可解释的特征，从而揭示人类偏好的多样性和数据集上下文的作用。


<details>
  <summary>Details</summary>
Motivation: 了解人类反馈数据编码的内容对于避免语言模型产生不可预测和不良行为至关重要，但现有方法难以在没有预先指定假设的情况下自动提取相关特征。

Method: WIMHF使用稀疏自动编码器来表征数据集能够测量的偏好以及注释者实际表达的偏好。

Result: WIMHF在7个数据集上识别出少量人类可解释的特征，这些特征解释了黑盒模型获得的大部分偏好预测信号。这些特征揭示了人类偏好的多样性和数据集上下文的作用，例如，Reddit用户喜欢非正式和笑话，而HH-RLHF和PRISM中的注释者不喜欢它们。WIMHF还发现了潜在的不安全偏好，例如，LMArena用户倾向于投票反对拒绝，通常支持有毒内容。重新标记Arena中的有害示例可带来巨大的安全收益（+ 37%），且不会影响一般性能。在Community Alignment数据集上，学习到的特定于注释者的主观特征权重可以提高偏好预测。

Conclusion: WIMHF为从业者提供了一种以人为中心的分析方法，以更好地理解和使用偏好数据。

Abstract: Human feedback can alter language models in unpredictable and undesirable
ways, as practitioners lack a clear understanding of what feedback data
encodes. While prior work studies preferences over certain attributes (e.g.,
length or sycophancy), automatically extracting relevant features without
pre-specifying hypotheses remains challenging. We introduce What's In My Human
Feedback? (WIMHF), a method to explain feedback data using sparse autoencoders.
WIMHF characterizes both (1) the preferences a dataset is capable of measuring
and (2) the preferences that the annotators actually express. Across 7
datasets, WIMHF identifies a small number of human-interpretable features that
account for the majority of the preference prediction signal achieved by
black-box models. These features reveal a wide diversity in what humans prefer,
and the role of dataset-level context: for example, users on Reddit prefer
informality and jokes, while annotators in HH-RLHF and PRISM disprefer them.
WIMHF also surfaces potentially unsafe preferences, such as that LMArena users
tend to vote against refusals, often in favor of toxic content. The learned
features enable effective data curation: re-labeling the harmful examples in
Arena yields large safety gains (+37%) with no cost to general performance.
They also allow fine-grained personalization: on the Community Alignment
dataset, we learn annotator-specific weights over subjective features that
improve preference prediction. WIMHF provides a human-centered analysis method
for practitioners to better understand and use preference data.

</details>


### [31] [Towards Global Retrieval Augmented Generation: A Benchmark for Corpus-Level Reasoning](https://arxiv.org/abs/2510.26205)
*Qi Luo,Xiaonan Li,Tingshuo Fan,Xinchi Chen,Xipeng Qiu*

Main category: cs.CL

TL;DR: 这篇论文介绍了GlobalQA，一个新的基准，用于评估大型语言模型在全局RAG任务中的表现，发现现有方法表现不佳，并提出了GlobalRAG框架来解决这些挑战。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估主要集中在局部RAG，无法满足需要分析整个文档集合以获得语料库级别洞察的实际应用需求。

Method: 提出了GlobalRAG，一个多工具协作框架，通过chunk-level检索保持结构连贯性，整合LLM驱动的智能过滤器消除噪声文档，并集成聚合模块进行精确的符号计算。

Result: 在Qwen2.5-14B模型上，GlobalRAG的F1值为6.63，而最强的基线F1值仅为1.51，验证了该方法的有效性。

Conclusion: 现有的RAG方法在全局任务上表现不佳，GlobalRAG框架能够有效提升模型在全局RAG任务中的性能。

Abstract: Retrieval-augmented generation (RAG) has emerged as a leading approach to
reducing hallucinations in large language models (LLMs). Current RAG evaluation
benchmarks primarily focus on what we call local RAG: retrieving relevant
chunks from a small subset of documents to answer queries that require only
localized understanding within specific text chunks. However, many real-world
applications require a fundamentally different capability -- global RAG --
which involves aggregating and analyzing information across entire document
collections to derive corpus-level insights (for example, "What are the top 10
most cited papers in 2023?"). In this paper, we introduce GlobalQA -- the first
benchmark specifically designed to evaluate global RAG capabilities, covering
four core task types: counting, extremum queries, sorting, and top-k
extraction. Through systematic evaluation across different models and
baselines, we find that existing RAG methods perform poorly on global tasks,
with the strongest baseline achieving only 1.51 F1 score. To address these
challenges, we propose GlobalRAG, a multi-tool collaborative framework that
preserves structural coherence through chunk-level retrieval, incorporates
LLM-driven intelligent filters to eliminate noisy documents, and integrates
aggregation modules for precise symbolic computation. On the Qwen2.5-14B model,
GlobalRAG achieves 6.63 F1 compared to the strongest baseline's 1.51 F1,
validating the effectiveness of our method.

</details>


### [32] [Pragmatic Theories Enhance Understanding of Implied Meanings in LLMs](https://arxiv.org/abs/2510.26253)
*Takuma Sato,Seiya Kawano,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 本文提出了一种利用语用学理论提示语言模型来理解隐含意义的方法，通过提供格莱斯语用学和关联理论等概述，引导模型逐步推理。


<details>
  <summary>Details</summary>
Motivation: 语言模型应具备准确解读隐含意义的能力，这在人际交流和语言使用中至关重要。

Method: 将语用学理论概述作为提示，引导语言模型逐步推理，从而理解隐含意义。

Result: 实验结果表明，与零样本思维链基线相比，该方法在语用推理任务上提高了高达9.6%的分数。即使仅提及语用学理论的名称，也能在大型模型中带来一定的性能提升（约1-3%）。

Conclusion: 为语言模型提供语用学理论作为提示，是理解隐含意义任务中一种有效的上下文学习方法。

Abstract: The ability to accurately interpret implied meanings plays a crucial role in
human communication and language use, and language models are also expected to
possess this capability. This study demonstrates that providing language models
with pragmatic theories as prompts is an effective in-context learning approach
for tasks to understand implied meanings. Specifically, we propose an approach
in which an overview of pragmatic theories, such as Gricean pragmatics and
Relevance Theory, is presented as a prompt to the language model, guiding it
through a step-by-step reasoning process to derive a final interpretation.
Experimental results showed that, compared to the baseline, which prompts
intermediate reasoning without presenting pragmatic theories (0-shot
Chain-of-Thought), our methods enabled language models to achieve up to 9.6\%
higher scores on pragmatic reasoning tasks. Furthermore, we show that even
without explaining the details of pragmatic theories, merely mentioning their
names in the prompt leads to a certain performance improvement (around 1-3%) in
larger models compared to the baseline.

</details>


### [33] [Language Models Are Borrowing-Blind: A Multilingual Evaluation of Loanword Identification across 10 Languages](https://arxiv.org/abs/2510.26254)
*Mérilin Sousa Silva,Sina Ahmadi*

Main category: cs.CL

TL;DR: 大型语言模型在区分外来词和本地词方面表现不佳，表明它们对外来词存在偏见。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型是否具有识别外来词的能力，特别是在双语社区中，强势语言不断影响弱势语言的词汇。

Method: 在10种语言中评估多个模型区分外来词和本地词的能力。

Result: 模型在区分外来词和本地词方面表现不佳。

Conclusion: 现代NLP系统对外来词存在偏见，这项研究对开发弱势语言的NLP工具和支持社区的语言保护具有重要意义。

Abstract: Throughout language history, words are borrowed from one language to another
and gradually become integrated into the recipient's lexicon. Speakers can
often differentiate these loanwords from native vocabulary, particularly in
bilingual communities where a dominant language continuously imposes lexical
items on a minority language. This paper investigates whether pretrained
language models, including large language models, possess similar capabilities
for loanword identification. We evaluate multiple models across 10 languages.
Despite explicit instructions and contextual information, our results show that
models perform poorly in distinguishing loanwords from native ones. These
findings corroborate previous evidence that modern NLP systems exhibit a bias
toward loanwords rather than native equivalents. Our work has implications for
developing NLP tools for minority languages and supporting language
preservation in communities under lexical pressure from dominant languages.

</details>


### [34] [Distilling Multilingual Vision-Language Models: When Smaller Models Stay Multilingual](https://arxiv.org/abs/2510.26271)
*Sukrit Sriratanawilai,Jhayahgrit Thongwat,Romrawin Chumpu,Patomporn Payoungkhamdee,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: 研究了知识蒸馏（KD）在多语言视觉语言模型（VLM）中的应用，发现某些配置可以在模型压缩一半的情况下保持甚至提高多语言检索的鲁棒性，但其他配置无法保持跨任务的稳定性，揭示了设计敏感的权衡。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLM）在不同语言上的表现不均衡，当模型尺寸减小时，这个问题通常会加剧。知识蒸馏（KD）在将知识从较大的VLM转移到较小的VLM方面表现出良好的效果，但在多语言主义中应用KD是一个未被充分探索的领域。

Method: 对五种蒸馏方法进行了受控的实证研究，分离了它们对跨语言表示一致性和模型压缩下下游性能稳定性的影响。研究了CLIP和SigLIP2上的五种蒸馏公式，并在领域内检索和领域外视觉问答上评估它们。

Result: 发现某些配置可以在模型尺寸减半的情况下保持甚至提高多语言检索的鲁棒性，但其他配置无法保持跨任务的稳定性。

Conclusion: 揭示了设计敏感的权衡，而仅靠聚合精度无法揭示这些权衡。

Abstract: Vision-language models (VLMs) exhibit uneven performance across languages, a
problem that is often exacerbated when the model size is reduced. While
Knowledge distillation (KD) demonstrates promising results in transferring
knowledge from larger to smaller VLMs, applying KD in multilingualism is an
underexplored area. This paper presents a controlled empirical study of KD
behavior across five distillation approaches, isolating their effects on
cross-lingual representation consistency and downstream performance stability
under model compression. We study five distillation formulations across CLIP
and SigLIP2, and evaluate them on in-domain retrieval and out-of-domain visual
QA. We find that some configurations preserve or even improve multilingual
retrieval robustness despite halving model size, but others fail to maintain
cross-task stability, exposing design-sensitive trade-offs that aggregate
accuracy alone does not reveal.

</details>


### [35] [Do LLMs Signal When They're Right? Evidence from Neuron Agreement](https://arxiv.org/abs/2510.26277)
*Kang Chen,Yaoning Wang,Kai Xiong,Zhuoka Feng,Wenhe Sun,Haotian Chen,Yixin Cao*

Main category: cs.CL

TL;DR: 提出了一种新的无监督解码方法，该方法利用神经元激活的稀疏性和交叉样本神经元的一致性来选择候选答案。


<details>
  <summary>Details</summary>
Motivation: 现有的策略仅使用外部输出（例如 token 概率）对候选答案进行评分，并且这些信号在训练后可能无法很好地校准。

Method: 提出神经元一致性解码 (NAD)，这是一种 unsupervised best-of-N 方法，它使用激活稀疏性和交叉样本神经元一致性来选择候选答案，仅对内部信号进行操作，而无需可比较的文本输出。

Result: 在具有可验证答案的数学和科学基准测试中，NAD 与多数投票相匹配；在多数投票不适用的开放式编码基准测试中，NAD 始终优于 Avg@64。通过尽早修剪没有希望的轨迹，NAD 可减少 99% 的 token 使用量，而生成质量的损失极小。

Conclusion: 内部信号为无标签集成解码提供了可靠、可扩展且高效的指导。

Abstract: Large language models (LLMs) commonly boost reasoning via
sample-evaluate-ensemble decoders, achieving label free gains without ground
truth. However, prevailing strategies score candidates using only external
outputs such as token probabilities, entropies, or self evaluations, and these
signals can be poorly calibrated after post training. We instead analyze
internal behavior based on neuron activations and uncover three findings: (1)
external signals are low dimensional projections of richer internal dynamics;
(2) correct responses activate substantially fewer unique neurons than
incorrect ones throughout generation; and (3) activations from correct
responses exhibit stronger cross sample agreement, whereas incorrect ones
diverge. Motivated by these observations, we propose Neuron Agreement Decoding
(NAD), an unsupervised best-of-N method that selects candidates using
activation sparsity and cross sample neuron agreement, operating solely on
internal signals and without requiring comparable textual outputs. NAD enables
early correctness prediction within the first 32 generated tokens and supports
aggressive early stopping. Across math and science benchmarks with verifiable
answers, NAD matches majority voting; on open ended coding benchmarks where
majority voting is inapplicable, NAD consistently outperforms Avg@64. By
pruning unpromising trajectories early, NAD reduces token usage by 99% with
minimal loss in generation quality, showing that internal signals provide
reliable, scalable, and efficient guidance for label free ensemble decoding.

</details>


### [36] [Unravelling the Mechanisms of Manipulating Numbers in Language Models](https://arxiv.org/abs/2510.26285)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Bertram Højer,Michal Spiegel,Raúl Vázquez,Aman Sinha,Josef Kuchař,Philipp Mondorf*

Main category: cs.CL

TL;DR: 大型语言模型(LLM)在处理数字时，输入嵌入表示准确且相似，但输出结果却容易出错。本研究旨在解释这一矛盾，通过探究语言模型如何处理数字，并量化这些机制的准确性下限。研究发现，尽管存在错误，不同的语言模型学习到的数字表示是可互换的，且在隐藏状态和输入上下文类型中具有系统性、高度准确性和通用性。这使得我们能够为每个LLM创建通用探针，并将信息（包括输出错误的起因）追溯到特定层。我们的研究结果为预训练LLM如何处理数字奠定了基础，并概述了更准确的探测技术在改进LLM架构方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 解释大型语言模型在处理数字时，输入嵌入表示准确但输出结果容易出错的矛盾现象。

Method: 探索语言模型如何处理数字，并量化这些机制的准确性下限。创建通用探针来追踪信息，包括输出错误的起因，定位到特定层。

Result: 发现不同的语言模型学习到的数字表示是可互换的，且在隐藏状态和输入上下文类型中具有系统性、高度准确性和通用性。

Conclusion: 本研究为预训练LLM如何处理数字奠定了基础，并概述了更准确的探测技术在改进LLM架构方面的潜力。

Abstract: Recent work has shown that different large language models (LLMs) converge to
similar and accurate input embedding representations for numbers. These
findings conflict with the documented propensity of LLMs to produce erroneous
outputs when dealing with numeric information. In this work, we aim to explain
this conflict by exploring how language models manipulate numbers and quantify
the lower bounds of accuracy of these mechanisms. We find that despite
surfacing errors, different language models learn interchangeable
representations of numbers that are systematic, highly accurate and universal
across their hidden states and the types of input contexts. This allows us to
create universal probes for each LLM and to trace information -- including the
causes of output errors -- to specific layers. Our results lay a fundamental
understanding of how pre-trained LLMs manipulate numbers and outline the
potential of more accurate probing techniques in addressed refinements of LLMs'
architectures.

</details>


### [37] [Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games](https://arxiv.org/abs/2510.26298)
*Jingran Zhang,Ning Li,Justin Cui*

Main category: cs.CL

TL;DR: OpenAI的ChatGPT Atlas可以通过网页互动来分析网页、处理用户意图并在浏览器中直接执行光标和键盘输入。本研究评估了Atlas在基于浏览器的游戏中的网页互动能力，包括Google的T-Rex Runner、Sudoku、Flappy Bird和Stein.world。


<details>
  <summary>Details</summary>
Motivation: 评估Atlas在动态、互动环境中的性能，这些环境需要实时互动，而这些互动的研究较少。

Method: 使用游戏中的性能分数作为定量指标来评估不同任务类型的性能。

Result: Atlas在数独等逻辑推理任务中表现出色，完成谜题的速度明显快于人类，但在需要精确计时和运动控制的实时游戏中表现不佳。

Conclusion: 虽然Atlas展示了强大的分析处理能力，但在需要实时互动的动态网络环境中，仍然存在明显的局限性。

Abstract: OpenAI's ChatGPT Atlas introduces new capabilities for web interaction,
enabling the model to analyze webpages, process user intents, and execute
cursor and keyboard inputs directly within the browser. While its capacity for
information retrieval tasks has been demonstrated, its performance in dynamic,
interactive environments remains less explored. In this study, we conduct an
early evaluation of Atlas's web interaction capabilities using browser-based
games as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird,
and Stein.world. We employ in-game performance scores as quantitative metrics
to assess performance across different task types. Our results show that Atlas
performs strongly in logical reasoning tasks like Sudoku, completing puzzles
significantly faster than human baselines, but struggles substantially in
real-time games requiring precise timing and motor control, often failing to
progress beyond initial obstacles. These findings suggest that while Atlas
demonstrates capable analytical processing, there remain notable limitations in
dynamic web environments requiring real-time interaction. The website of our
project can be found at https://atlas-game-eval.github.io.

</details>


### [38] [SCRIBE: Structured Chain Reasoning for Interactive Behaviour Explanations using Tool Calling](https://arxiv.org/abs/2510.26322)
*Fares Fawzi,Vinitra Swamy,Dominik Glandorf,Tanya Nazaretsky,Tanja Käser*

Main category: cs.CL

TL;DR: SCRIBE是一个用于生成针对学生问题的有效反馈的框架，它结合了领域特定工具和自我反思推理管道。


<details>
  <summary>Details</summary>
Motivation: 在教育环境中，使用语言模型提供互动式、个性化的学生反馈面临隐私、计算资源有限以及需要教学上有效的回应等挑战。因此，需要能够在本地运行并可靠地将其输出建立在正确信息上的小型开源模型。

Method: SCRIBE框架采用多跳、工具增强的推理，并通过在合成的GPT-4o生成数据上进行两阶段LoRA微调，将这些能力提炼成3B和8B模型。

Result: 8B-SCRIBE模型在相关性和可操作性等关键维度上达到了与更大模型相当或更高的质量，并且学生认为其与GPT-4o和Llama-3 70B相当。

Conclusion: SCRIBE对于低资源、隐私敏感的教育应用是可行的。

Abstract: Language models can be used to provide interactive, personalized student
feedback in educational settings. However, real-world deployment faces three
key challenges: privacy concerns, limited computational resources, and the need
for pedagogically valid responses. These constraints require small, open-source
models that can run locally and reliably ground their outputs in correct
information. We introduce SCRIBE, a framework for multi-hop, tool-augmented
reasoning designed to generate valid responses to student questions about
feedback reports. SCRIBE combines domain-specific tools with a self-reflective
inference pipeline that supports iterative reasoning, tool use, and error
recovery. We distil these capabilities into 3B and 8B models via two-stage LoRA
fine-tuning on synthetic GPT-4o-generated data. Evaluation with a human-aligned
GPT-Judge and a user study with 108 students shows that 8B-SCRIBE models
achieve comparable or superior quality to much larger models in key dimensions
such as relevance and actionability, while being perceived on par with GPT-4o
and Llama-3.3 70B by students. These findings demonstrate the viability of
SCRIBE for low-resource, privacy-sensitive educational applications.

</details>


### [39] [From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum Learning](https://arxiv.org/abs/2510.26336)
*Nishit Neema,Srinjoy Mukherjee,Sapan Shah,Gokul Ramakrishnan,Ganesh Venkatesh*

Main category: cs.CL

TL;DR: ACER通过构建课程体系并持续预训练，提升LLM在特定领域的性能，同时保持其通用能力。


<details>
  <summary>Details</summary>
Motivation: 通用LLM在经济学和心理学等专业领域表现不佳，因为这些领域需要深入的、有原则的理解。

Method: ACER首先综合生成一个全面的、教科书式的课程，然后创建由Bloom分类学指导的问答对。由此产生的合成语料库用于持续预训练，采用交错的课程表，从而使学习在内容和认知维度上保持一致。

Result: 在微观经济学等挑战性领域，ACER将准确率提高了5个百分点。在所有目标领域，我们观察到宏平均改进为3个百分点。在MMLU之外，ACER在ARC和GPQA等知识密集型基准测试中，性能提高了2个绝对点以上，同时保持了通用推理任务的稳定性能。

Conclusion: ACER提供了一个可扩展且有效的配方，可以缩小LLM中关键领域差距。

Abstract: Large Language Models (LLMs) excel at general tasks but underperform in
specialized domains like economics and psychology, which require deep,
principled understanding. To address this, we introduce ACER (Automated
Curriculum-Enhanced Regimen) that transforms generalist models into domain
experts without sacrificing their broad capabilities. ACER first synthesizes a
comprehensive, textbook-style curriculum by generating a table of contents for
a subject and then creating question-answer (QA) pairs guided by Bloom's
taxonomy. This ensures systematic topic coverage and progressively increasing
difficulty. The resulting synthetic corpus is used for continual pretraining
with an interleaved curriculum schedule, aligning learning across both content
and cognitive dimensions.
  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized
MMLU subsets. In challenging domains like microeconomics, where baselines
struggle, ACER boosts accuracy by 5 percentage points. Across all target
domains, we observe a consistent macro-average improvement of 3 percentage
points. Notably, ACER not only prevents catastrophic forgetting but also
facilitates positive cross-domain knowledge transfer, improving performance on
non-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on
knowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points,
while maintaining stable performance on general reasoning tasks. Our results
demonstrate that ACER offers a scalable and effective recipe for closing
critical domain gaps in LLMs.

</details>


### [40] [MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data](https://arxiv.org/abs/2510.26345)
*Mykhailo Poliakov,Nadiya Shvai*

Main category: cs.CL

TL;DR: 本研究探讨了使用合成数据和轻量级微调技术来提高大型语言模型识别错误论证的能力，尤其是在与健康相关的错误信息领域。


<details>
  <summary>Details</summary>
Motivation: 健康相关的错误信息非常普遍且具有潜在危害，难以识别，尤其是在声明扭曲或误解科学发现时。

Method: 该研究提出了一种名为MisSynth的流程，该流程应用检索增强生成（RAG）来生成合成谬误样本，然后用于微调LLM模型。

Result: 结果表明，与原始基线相比，微调模型在准确性方面有了显着提高。例如，LLaMA 3.1 8B微调模型在MISSCI测试集中实现了超过35％的F1分数绝对改进。

Conclusion: 引入合成谬误数据以扩充有限的注释资源可以显着提高零样本LLM在现实世界科学错误信息任务中的分类性能，即使在计算资源有限的情况下也是如此。

Abstract: Health-related misinformation is very prevalent and potentially harmful. It
is difficult to identify, especially when claims distort or misinterpret
scientific findings. We investigate the impact of synthetic data generation and
lightweight fine-tuning techniques on the ability of large language models
(LLMs) to recognize fallacious arguments using the MISSCI dataset and
framework. In this work, we propose MisSynth, a pipeline that applies
retrieval-augmented generation (RAG) to produce synthetic fallacy samples,
which are then used to fine-tune an LLM model. Our results show substantial
accuracy gains with fine-tuned models compared to vanilla baselines. For
instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score
absolute improvement on the MISSCI test split over its vanilla baseline. We
demonstrate that introducing synthetic fallacy data to augment limited
annotated resources can significantly enhance zero-shot LLM classification
performance on real-world scientific misinformation tasks, even with limited
computational resources. The code and synthetic dataset are available on
https://github.com/mxpoliakov/MisSynth.

</details>


### [41] [The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic Teams for Multi-Agent Collaboration](https://arxiv.org/abs/2510.26352)
*Kotaro Furuya,Yuichi Kitagawa*

Main category: cs.CL

TL;DR: 提出了一种基于交互的多智能体LLM团队自动构建框架，无需先验知识即可根据模型间的对话语义连贯性构建“语言模型图”，并通过社群检测识别协同模型集群。


<details>
  <summary>Details</summary>
Motivation: 基于大型语言模型（LLM）的多智能体方法是超越单一模型能力的有效策略，但其成功关键在于协同的团队组成。然而，由于大多数模型的不透明性，形成最优团队是一个巨大的挑战。

Method: 构建“语言模型图”，该图谱通过成对对话的语义连贯性来映射模型之间的关系，然后应用社群检测来识别协同模型集群。

Result: 实验证明，该方法能够发现功能连贯的群体，反映其潜在的专业化。通过特定主题启动对话，协同团队在下游基准测试中优于随机基线，并达到与基于已知模型专业化的人工管理团队相当的准确率。

Conclusion: 研究结果为协作式多智能体LLM团队的自动设计提供了新的基础。

Abstract: While a multi-agent approach based on large language models (LLMs) represents
a promising strategy to surpass the capabilities of single models, its success
is critically dependent on synergistic team composition. However, forming
optimal teams is a significant challenge, as the inherent opacity of most
models obscures the internal characteristics necessary for effective
collaboration. In this paper, we propose an interaction-centric framework for
automatic team composition that does not require any prior knowledge including
their internal architectures, training data, or task performances. Our method
constructs a "language model graph" that maps relationships between models from
the semantic coherence of pairwise conversations, and then applies community
detection to identify synergistic model clusters. Our experiments with diverse
LLMs demonstrate that the proposed method discovers functionally coherent
groups that reflect their latent specializations. Priming conversations with
specific topics identified synergistic teams which outperform random baselines
on downstream benchmarks and achieve comparable accuracy to that of
manually-curated teams based on known model specializations. Our findings
provide a new basis for the automated design of collaborative multi-agent LLM
teams.

</details>


### [42] [On the Role of Context for Discourse Relation Classification in Scientific Writing](https://arxiv.org/abs/2510.26354)
*Stephen Wan,Wei Liu,Michael Strube*

Main category: cs.CL

TL;DR: 本研究探讨了使用话语级别信息来寻找人工智能生成科学主张的支持证据，重点是科学写作中的话语关系分类任务。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能方法在科学工作流程中的使用越来越多，我们对使用话语级别信息来寻找人工智能生成科学主张的支持证据感兴趣。

Method: 我们提出了预训练语言模型（PLM）和大型语言模型（LLM）方法用于话语关系分类（DRC）的初步研究。

Result: 我们的实验表明，上下文（由话语结构定义）通常是有帮助的。我们还分析了哪些科学话语关系类型可能从上下文中获益最多。

Conclusion: 我们研究了上下文如何帮助DRC任务，重点关注科学出版物这一该任务研究不足的类型。

Abstract: With the increasing use of generative Artificial Intelligence (AI) methods to
support science workflows, we are interested in the use of discourse-level
information to find supporting evidence for AI generated scientific claims. A
first step towards this objective is to examine the task of inferring discourse
structure in scientific writing.
  In this work, we present a preliminary investigation of pretrained language
model (PLM) and Large Language Model (LLM) approaches for Discourse Relation
Classification (DRC), focusing on scientific publications, an under-studied
genre for this task. We examine how context can help with the DRC task, with
our experiments showing that context, as defined by discourse structure, is
generally helpful. We also present an analysis of which scientific discourse
relation types might benefit most from context.

</details>


### [43] [OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large Language Models in Education](https://arxiv.org/abs/2510.26422)
*Min Zhang,Hao Chen,Hao Chen,Wenqi Zhang,Didi Zhu,Xin Lin,Bo Jiang,Aimin Zhou,Fei Wu,Kun Kuang*

Main category: cs.CL

TL;DR: OmniEduBench：一个全面的中文教育基准，包含 24.602K 个高质量问答对，涵盖知识和能力维度，以及 61 个不同学科。


<details>
  <summary>Details</summary>
Motivation: 现有LLM及其基准主要关注知识维度，忽略了实际教育场景中重要的能力培养评估，且基准缺乏多样性，尤其是在中文语境下。

Method: 构建了OmniEduBench，包含知识和能力维度，细分为6个类别，覆盖61个学科，包含11种常见题型。

Result: 在11个主流LLM上的实验表明，LLM在知识和能力维度上都存在明显的性能差距，尤其是在能力维度上，最佳模型与人类智能相比仍有差距。

Conclusion: LLM在教育领域有很大的改进空间和挑战。

Abstract: With the rapid development of large language models (LLMs), various LLM-based
works have been widely applied in educational fields. However, most existing
LLMs and their benchmarks focus primarily on the knowledge dimension, largely
neglecting the evaluation of cultivation capabilities that are essential for
real-world educational scenarios. Additionally, current benchmarks are often
limited to a single subject or question type, lacking sufficient diversity.
This issue is particularly prominent within the Chinese context. To address
this gap, we introduce OmniEduBench, a comprehensive Chinese educational
benchmark. OmniEduBench consists of 24.602K high-quality question-answer pairs.
The data is meticulously divided into two core dimensions: the knowledge
dimension and the cultivation dimension, which contain 18.121K and 6.481K
entries, respectively. Each dimension is further subdivided into 6 fine-grained
categories, covering a total of 61 different subjects (41 in the knowledge and
20 in the cultivation). Furthermore, the dataset features a rich variety of
question formats, including 11 common exam question types, providing a solid
foundation for comprehensively evaluating LLMs' capabilities in education.
Extensive experiments on 11 mainstream open-source and closed-source LLMs
reveal a clear performance gap. In the knowledge dimension, only Gemini-2.5 Pro
surpassed 60\% accuracy, while in the cultivation dimension, the
best-performing model, QWQ, still trailed human intelligence by nearly 30\%.
These results highlight the substantial room for improvement and underscore the
challenges of applying LLMs in education.

</details>


### [44] [1+1>2: A Synergistic Sparse and Low-Rank Compression Method for Large Language Models](https://arxiv.org/abs/2510.26446)
*Zeliang Zong,Kai Zhang,Zheyang Li,Wenming Tan,Ye Ren,Yiyan Zhai,Jilin Hu*

Main category: cs.CL

TL;DR: 提出了一种名为SSLC的协同稀疏和低秩压缩方法，用于压缩大型语言模型（LLM）。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在语言理解和生成方面表现出色，但其广泛应用受到带宽和计算需求的限制。单独的剪枝和低秩近似方法有各自的优势，但它们在LLM中的协同作用尚未充分探索。

Method: 将低秩近似和稀疏优化形式化为一个统一的问题，并通过迭代优化算法求解。

Result: 在LLaMA和Qwen2.5模型（7B-70B）上的实验表明，SSLC在没有任何额外的训练步骤的情况下，始终优于单独的方法，实现了最先进的结果。例如，SSLC在不降低性能的情况下将Qwen2.5压缩了50%，并实现了至少1.63倍的加速。

Conclusion: SSLC为高效的LLM部署提供了一个实用的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in
language comprehension and generation; however, their widespread adoption is
constrained by substantial bandwidth and computational demands. While pruning
and low-rank approximation have each demonstrated promising performance
individually, their synergy for LLMs remains underexplored. We introduce
\underline{S}ynergistic \underline{S}parse and \underline{L}ow-Rank
\underline{C}ompression (SSLC) methods for LLMs, which leverages the strengths
of both techniques: low-rank approximation compresses the model by retaining
its essential structure with minimal information loss, whereas sparse
optimization eliminates non-essential weights, preserving those crucial for
generalization. Based on theoretical analysis, we first formulate the low-rank
approximation and sparse optimization as a unified problem and solve it by
iterative optimization algorithm. Experiments on LLaMA and Qwen2.5 models
(7B-70B) show that SSLC, without any additional training steps, consistently
surpasses standalone methods, achieving state-of-the-arts results. Notably,
SSLC compresses Qwen2.5 by 50\% with no performance drop and achieves at least
1.63$\times$ speedup, offering a practical solution for efficient LLM
deployment.

</details>


### [45] [Bayesian Network Fusion of Large Language Models for Sentiment Analysis](https://arxiv.org/abs/2510.26484)
*Rasoul Amirzadeh,Dhananjay Thiruvady,Fatemeh Shiri*

Main category: cs.CL

TL;DR: 提出了一种新的LLM融合框架，BNLF，用于情感分析。


<details>
  <summary>Details</summary>
Motivation: 现有的领域特定LLM缺乏透明性和可解释性，微调成本高，需要大量的提示工程，跨领域结果不一致，并且计算需求高，对环境有不利影响。

Method: 通过概率机制整合来自FinBERT、RoBERTa和BERTweet三个LLM的预测。

Result: 在三个人工标注的金融语料库中，BNLF的准确率比基线LLM提高了约6%。

Conclusion: BNLF对数据集的变异具有鲁棒性，并且概率融合对于可解释的情感分类是有效的。

Abstract: Large language models (LLMs) continue to advance, with an increasing number
of domain-specific variants tailored for specialised tasks. However, these
models often lack transparency and explainability, can be costly to fine-tune,
require substantial prompt engineering, yield inconsistent results across
domains, and impose significant adverse environmental impact due to their high
computational demands. To address these challenges, we propose the Bayesian
network LLM fusion (BNLF) framework, which integrates predictions from three
LLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic
mechanism for sentiment analysis. BNLF performs late fusion by modelling the
sentiment predictions from multiple LLMs as probabilistic nodes within a
Bayesian network. Evaluated across three human-annotated financial corpora with
distinct linguistic and contextual characteristics, BNLF demonstrates
consistent gains of about six percent in accuracy over the baseline LLMs,
underscoring its robustness to dataset variability and the effectiveness of
probabilistic fusion for interpretable sentiment classification.

</details>


### [46] [A Multi-agent Large Language Model Framework to Automatically Assess Performance of a Clinical AI Triage Tool](https://arxiv.org/abs/2510.26498)
*Adam E. Flanders,Yifan Peng,Luciano Prevedello,Robyn Ball,Errol Colak,Prahlad Menon,George Shih,Hui-Ming Lin,Paras Lakhani*

Main category: cs.CL

TL;DR: 本研究探讨了使用多个LLM集合来评估基于像素的AI分诊工具的可靠性，并与单个LLM进行比较。


<details>
  <summary>Details</summary>
Motivation: 评估AI分诊工具的可靠性对于临床应用至关重要，使用LLM集合可以提高评估的准确性和一致性。

Method: 使用包含来自14家医院的29,766份非对比CT头部检查的数据集，利用八个开源LLM模型和一个内部GPT-4o模型评估颅内出血（ICH）的存在。通过多样本提示评估放射学报告，并手动审查1,726个样本。比较了八个开源模型的性能特征和共识与GPT-4o的性能。

Result: Llama3.3:70b和GPT-4o的AUC最高（0.78），Llama3.3:70b的F1得分最高（0.81），并且使用MCC评估，LLM的最佳组合为完整9模型集合(0.571), 前3模型集合(0.558), 共识模型(0.556)和GPT4o (0.522)。

Conclusion: 中大型开源LLM的集合比单个LLM提供了一种更一致和可靠的方法，可以对临床AI分诊工具进行回顾性评估。

Abstract: Purpose: The purpose of this study was to determine if an ensemble of
multiple LLM agents could be used collectively to provide a more reliable
assessment of a pixel-based AI triage tool than a single LLM.
  Methods: 29,766 non-contrast CT head exams from fourteen hospitals were
processed by a commercial intracranial hemorrhage (ICH) AI detection tool.
Radiology reports were analyzed by an ensemble of eight open-source LLM models
and a HIPAA compliant internal version of GPT-4o using a single multi-shot
prompt that assessed for presence of ICH. 1,726 examples were manually
reviewed. Performance characteristics of the eight open-source models and
consensus were compared to GPT-4o. Three ideal consensus LLM ensembles were
tested for rating the performance of the triage tool.
  Results: The cohort consisted of 29,766 head CTs exam-report pairs. The
highest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78).
The average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76).
Llama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater
precision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the
ideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3
Ensemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522
(0.500-0.543). No statistically significant differences were observed between
Top-3, Full-9, and Consensus (p > 0.05).
  Conclusion: An ensemble of medium to large sized open-source LLMs provides a
more consistent and reliable method to derive a ground truth retrospective
evaluation of a clinical AI triage tool over a single LLM alone.

</details>


### [47] [Hebrew Diacritics Restoration using Visual Representation](https://arxiv.org/abs/2510.26521)
*Yair Elboher,Yuval Pinter*

Main category: cs.CL

TL;DR: 提出了一种新的希伯来语加标系统DIVRIT，该系统将任务定义为零样本分类问题。


<details>
  <summary>Details</summary>
Motivation: 希伯来语的加标对于确保准确的单词发音和消除文本含义的歧义至关重要。当未发声时，该语言具有高度的歧义性，但最近的机器学习方法已大大提高了此任务的性能。

Method: 该方法在单词级别上运行，从动态生成的候选集中为每个未加标的单词选择最合适的加标模式，并以周围的文本上下文为条件。DIVRIT 的一项关键创新是它使用了希伯来语视觉语言模型，该模型将未加标的文本处理为图像，从而可以将变音符号信息直接嵌入到输入的向量表示中。

Result: 通过对各种配置的全面评估，证明该系统可以有效地执行加标，而无需依赖复杂的、显式的语言分析。值得注意的是，在“oracle”设置中，其中保证正确的加标形式在提供的候选对象中，DIVRIT 实现了很高的准确度。

Conclusion: 这些发现突出了视觉表示在准确和自动希伯来语加标方面有希望的潜力。

Abstract: Diacritics restoration in Hebrew is a fundamental task for ensuring accurate
word pronunciation and disambiguating textual meaning. Despite the language's
high degree of ambiguity when unvocalized, recent machine learning approaches
have significantly advanced performance on this task.
  In this work, we present DIVRIT, a novel system for Hebrew diacritization
that frames the task as a zero-shot classification problem. Our approach
operates at the word level, selecting the most appropriate diacritization
pattern for each undiacritized word from a dynamically generated candidate set,
conditioned on the surrounding textual context. A key innovation of DIVRIT is
its use of a Hebrew Visual Language Model, which processes undiacritized text
as an image, allowing diacritic information to be embedded directly within the
input's vector representation.
  Through a comprehensive evaluation across various configurations, we
demonstrate that the system effectively performs diacritization without relying
on complex, explicit linguistic analysis. Notably, in an ``oracle'' setting
where the correct diacritized form is guaranteed to be among the provided
candidates, DIVRIT achieves a high level of accuracy. Furthermore, strategic
architectural enhancements and optimized training methodologies yield
significant improvements in the system's overall generalization capabilities.
These findings highlight the promising potential of visual representations for
accurate and automated Hebrew diacritization.

</details>


### [48] [The Structure of Relation Decoding Linear Operators in Large Language Models](https://arxiv.org/abs/2510.26543)
*Miranda Anna Christ,Adrián Csiszárik,Gergely Becsó,Dániel Varga*

Main category: cs.CL

TL;DR: 本文研究了转换器语言模型中解码特定关系事实的线性算子的结构，发现它们可以被高度压缩，并且这些线性映射提取的是粗粒度的语义属性，而不是特定的关系。


<details>
  <summary>Details</summary>
Motivation: 探究转换器语言模型中线性算子如何解码关系事实。

Method: 将单关系研究扩展到多个关系，并系统地绘制它们的组织结构；通过交叉评估协议，将每个线性解码器算子应用于其他关系的主语。

Result: 关系解码器可以通过简单的三阶张量网络高度压缩，且线性映射提取的是粗粒度的语义属性。

Conclusion: 转换器语言模型中的线性关系解码主要是基于属性的，而不是特定于关系的。

Abstract: This paper investigates the structure of linear operators introduced in
Hernandez et al. [2023] that decode specific relational facts in transformer
language models. We extend their single-relation findings to a collection of
relations and systematically chart their organization. We show that such
collections of relation decoders can be highly compressed by simple order-3
tensor networks without significant loss in decoding accuracy. To explain this
surprising redundancy, we develop a cross-evaluation protocol, in which we
apply each linear decoder operator to the subjects of every other relation. Our
results reveal that these linear maps do not encode distinct relations, but
extract recurring, coarse-grained semantic properties (e.g., country of capital
city and country of food are both in the country-of-X property). This
property-centric structure clarifies both the operators' compressibility and
highlights why they generalize only to new relations that are semantically
close. Our findings thus interpret linear relational decoding in transformer
language models as primarily property-based, rather than relation-specific.

</details>


### [49] [InfoFlow: Reinforcing Search Agent Via Reward Density Optimization](https://arxiv.org/abs/2510.26575)
*Kun Luo,Hongjin Qian,Zheng Liu,Ziyi Xia,Shitao Xiao,Siqi Bao,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 提出InfoFlow，一个系统性的框架，通过子问题分解、失败引导提示和双智能体细化来解决奖励稀疏问题，从而提升agentic deep search的效率。


<details>
  <summary>Details</summary>
Motivation: 在深度搜索场景中，强化学习由于奖励稀疏性而受限，导致探索成本高昂但奖励稀少。论文将此挑战形式化为奖励密度优化问题。

Method: 引入InfoFlow框架，包含三个方面：子问题分解（提供更密集的学习信号），失败引导提示（注入纠正性指导以增加成功概率），以及双智能体细化（通过提炼智能体减少探索成本）。

Result: 在多个agentic search benchmarks上，InfoFlow显著优于强大的基线模型，使轻量级LLMs能够达到与高级专有LLMs相当的性能。

Conclusion: InfoFlow框架有效地解决了agentic deep search中的奖励稀疏性问题，提高了轻量级LLMs的性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach
for enhancing agentic deep search. However, its application is often hindered
by low \textbf{Reward Density} in deep search scenarios, where agents expend
significant exploratory costs for infrequent and often null final rewards. In
this paper, we formalize this challenge as the \textbf{Reward Density
Optimization} problem, which aims to improve the reward obtained per unit of
exploration cost. This paper introduce \textbf{InfoFlow}, a systematic
framework that tackles this problem from three aspects. 1) \textbf{Subproblem
decomposition}: breaking down long-range tasks to assign process rewards,
thereby providing denser learning signals. 2) \textbf{Failure-guided hints}:
injecting corrective guidance into stalled trajectories to increase the
probability of successful outcomes. 3) \textbf{Dual-agent refinement}:
employing a dual-agent architecture to offload the cognitive burden of deep
exploration. A refiner agent synthesizes the search history, which effectively
compresses the researcher's perceived trajectory, thereby reducing exploration
cost and increasing the overall reward density. We evaluate InfoFlow on
multiple agentic search benchmarks, where it significantly outperforms strong
baselines, enabling lightweight LLMs to achieve performance comparable to
advanced proprietary LLMs.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [50] [Enhancing Underwater Object Detection through Spatio-Temporal Analysis and Spatial Attention Networks](https://arxiv.org/abs/2510.25797)
*Sai Likhith Karri,Ansh Saxena*

Main category: cs.CV

TL;DR: 本研究探讨了时空建模和空间注意力机制在水下目标检测深度学习模型中的有效性。


<details>
  <summary>Details</summary>
Motivation: 水下环境复杂，目标检测精度有待提高。

Method: 首先评估了时序增强的YOLOv5变体T-YOLOv5的性能，然后通过添加卷积块注意力模块(CBAM)开发了T-YOLOv5的增强版本。

Result: T-YOLOv5和带有CBAM的T-YOLOv5的mAP@50-95得分分别为0.813和0.811，优于YOLOv5的0.563。

Conclusion: T-YOLOv5显著提高了检测可靠性，而带有CBAM的T-YOLOv5在具有挑战性的场景中进一步提高了性能，但在更简单的场景中损失了一些准确性。

Abstract: This study examines the effectiveness of spatio-temporal modeling and the
integration of spatial attention mechanisms in deep learning models for
underwater object detection. Specifically, in the first phase, the performance
of temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison with
the standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 is
developed, through the addition of a Convolutional Block Attention Module
(CBAM). By examining the effectiveness of the already pre-existing YOLOv5 and
T-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, the
research highlights how temporal modeling improves detection accuracy in
dynamic marine environments, particularly under conditions of sudden movements,
partial occlusions, and gradual motion. The testing results showed that YOLOv5
achieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAM
outperformed with mAP@50-95 scores of 0.813 and 0.811, respectively,
highlighting their superior accuracy and generalization in detecting complex
objects. The findings demonstrate that T-YOLOv5 significantly enhances
detection reliability compared to the standard model, while T-YOLOv5 with CBAM
further improves performance in challenging scenarios, although there is a loss
of accuracy when it comes to simpler scenarios.

</details>


### [51] [MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency](https://arxiv.org/abs/2510.25897)
*Nicolas Dufour,Lucas Degeorge,Arijit Ghosh,Vicky Kalogeiton,David Picard*

Main category: cs.CV

TL;DR: 提出了MIRO，一种在训练期间基于多个奖励模型调节模型的方法，以直接学习用户偏好。


<details>
  <summary>Details</summary>
Motivation: 现有文生图模型在大型未策划数据集上训练，这与用户偏好不符，且后处理方式会损害多样性、语义保真度和效率。

Method: 在训练期间，模型以多个奖励模型为条件。

Result: 显著提高了生成图像的视觉质量，并加快了训练速度；在GenEval和用户偏好评分上实现了最佳性能。

Conclusion: MIRO方法有效提升了文生图模型的性能和效率。

Abstract: Current text-to-image generative models are trained on large uncurated
datasets to enable diverse generation capabilities. However, this does not
align well with user preferences. Recently, reward models have been
specifically designed to perform post-hoc selection of generated images and
align them to a reward, typically user preference. This discarding of
informative data together with the optimizing for a single reward tend to harm
diversity, semantic fidelity and efficiency. Instead of this post-processing,
we propose to condition the model on multiple reward models during training to
let the model learn user preferences directly. We show that this not only
dramatically improves the visual quality of the generated images but it also
significantly speeds up the training. Our proposed method, called MIRO,
achieves state-of-the-art performances on the GenEval compositional benchmark
and user-preference scores (PickAScore, ImageReward, HPSv2).

</details>


### [52] [BikeScenes: Online LiDAR Semantic Segmentation for Bicycles](https://arxiv.org/abs/2510.25901)
*Denniz Goren,Holger Caesar*

Main category: cs.CV

TL;DR: 将汽车感知技术应用于自行车安全，特别是针对快速电动自行车。


<details>
  <summary>Details</summary>
Motivation: 日益增长的电动自行车的普及加剧了骑车者的脆弱性，因此需要调整汽车感知技术以提高自行车安全性。

Method: 使用多传感器 'SenseBike' 研究平台开发和评估专为自行车量身定制的 3D LiDAR 分割方法。引入了新的 BikeScenes-lidarseg 数据集，该数据集包含代尔夫特理工大学校园周围的 3021 个连续 LiDAR 扫描，并针对 29 个动态和静态类别进行了语义注释。

Result: 在 BikeScenes 数据集上进行微调可实现 63.6% 的平均交并比 (mIoU)，明显优于仅使用 SemanticKITTI 预训练获得的 13.8%。

Conclusion: 强调了特定于自行车安装、硬件受限的感知系统的关键挑战，并将 BikeScenes 数据集作为推进以骑车者为中心的 LiDAR 分割研究的资源。

Abstract: The vulnerability of cyclists, exacerbated by the rising popularity of faster
e-bikes, motivates adapting automotive perception technologies for bicycle
safety. We use our multi-sensor 'SenseBike' research platform to develop and
evaluate a 3D LiDAR segmentation approach tailored to bicycles. To bridge the
automotive-to-bicycle domain gap, we introduce the novel BikeScenes-lidarseg
Dataset, comprising 3021 consecutive LiDAR scans around the university campus
of the TU Delft, semantically annotated for 29 dynamic and static classes. By
evaluating model performance, we demonstrate that fine-tuning on our BikeScenes
dataset achieves a mean Intersection-over-Union (mIoU) of 63.6%, significantly
outperforming the 13.8% obtained with SemanticKITTI pre-training alone. This
result underscores the necessity and effectiveness of domain-specific training.
We highlight key challenges specific to bicycle-mounted, hardware-constrained
perception systems and contribute the BikeScenes dataset as a resource for
advancing research in cyclist-centric LiDAR segmentation.

</details>


### [53] [Generative Image Restoration and Super-Resolution using Physics-Informed Synthetic Data for Scanning Tunneling Microscopy](https://arxiv.org/abs/2510.25921)
*Nikola L. Kolev,Tommaso Rodani,Neil J. Curson,Taylor J. Z. Stock,Alberto Cazzaniga*

Main category: cs.CV

TL;DR: 提出了一种机器学习（ML）方法，用于图像修复和超分辨率，以应对扫描隧道显微镜（STM）中存在的探针退化和数据采集缓慢的问题。


<details>
  <summary>Details</summary>
Motivation: 扫描隧道显微镜（STM）实现了原子分辨率成像和原子操纵，但其效用通常受到探针退化和缓慢的串行数据采集的限制。制造过程增加了另一层复杂性，因为探针经常受到大电压的影响，这可能会改变其顶点的形状，需要对其进行调节。

Method: 使用仅包含 36 个 Si(001):H 原始实验图像的数据集，证明了物理信息合成数据生成管道可用于训练多个最先进的流匹配和扩散模型。

Result: 定量评估表明，该模型能够有效地恢复图像，并通过从稀疏采样数据中准确重建图像，从而将图像采集时间缩短两到四倍。

Conclusion: 该框架有潜力通过提供减少探针调节程序频率和提高现有高速 STM 系统中的帧速率的途径，从而显着提高 STM 实验吞吐量。

Abstract: Scanning tunnelling microscopy (STM) enables atomic-resolution imaging and
atom manipulation, but its utility is often limited by tip degradation and slow
serial data acquisition. Fabrication adds another layer of complexity since the
tip is often subjected to large voltages, which may alter the shape of its
apex, requiring it to be conditioned. Here, we propose a machine learning (ML)
approach for image repair and super-resolution to alleviate both challenges.
Using a dataset of only 36 pristine experimental images of Si(001):H, we
demonstrate that a physics-informed synthetic data generation pipeline can be
used to train several state-of-the-art flow-matching and diffusion models.
Quantitative evaluation with metrics such as the CLIP Maximum Mean Discrepancy
(CMMD) score and structural similarity demonstrates that our models are able to
effectively restore images and offer a two- to fourfold reduction in image
acquisition time by accurately reconstructing images from sparsely sampled
data. Our framework has the potential to significantly increase STM
experimental throughput by offering a route to reducing the frequency of
tip-conditioning procedures and to enhancing frame rates in existing high-speed
STM systems.

</details>


### [54] [SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing](https://arxiv.org/abs/2510.25970)
*Sung-Hoon Yoon,Minghan Li,Gaspard Beaudouin,Congcong Wen,Muhammad Rafay Azhar,Mengyu Wang*

Main category: cs.CV

TL;DR: 提出了一种基于流分解和聚合的框架，用于图像编辑，解决了反演不准确和梯度纠缠问题。


<details>
  <summary>Details</summary>
Motivation: 现有的 rectified flow 模型在图像编辑任务中存在反演不准确和梯度纠缠的问题，导致输出不能真实反映目标提示。

Method: 将目标提示分解为多个子提示，为每个子提示计算独立的流，然后将它们聚合以形成统一的编辑轨迹。设计了一种投影和软聚合机制，自适应地加权子目标速度场，抑制语义冗余，同时强调不同的方向。

Result: 实验结果表明，该方法在语义保真度和属性解耦方面优于现有的 zero-shot 编辑方法。

Conclusion: 该方法通过流分解和聚合框架，有效提升了图像编辑的质量和效果。

Abstract: Rectified flow models have become a de facto standard in image generation due
to their stable sampling trajectories and high-fidelity outputs. Despite their
strong generative capabilities, they face critical limitations in image editing
tasks: inaccurate inversion processes for mapping real images back into the
latent space, and gradient entanglement issues during editing often result in
outputs that do not faithfully reflect the target prompt. Recent efforts have
attempted to directly map source and target distributions via ODE-based
approaches without inversion; however,these methods still yield suboptimal
editing quality. In this work, we propose a flow decomposition-and-aggregation
framework built upon an inversion-free formulation to address these
limitations. Specifically, we semantically decompose the target prompt into
multiple sub-prompts, compute an independent flow for each, and aggregate them
to form a unified editing trajectory. While we empirically observe that
decomposing the original flow enhances diversity in the target space,
generating semantically aligned outputs still requires consistent guidance
toward the full target prompt. To this end, we design a projection and
soft-aggregation mechanism for flow, inspired by gradient conflict resolution
in multi-task learning. This approach adaptively weights the sub-target
velocity fields, suppressing semantic redundancy while emphasizing distinct
directions, thereby preserving both diversity and consistency in the final
edited output. Experimental results demonstrate that our method outperforms
existing zero-shot editing approaches in terms of semantic fidelity and
attribute disentanglement. The code is available at
https://github.com/Harvard-AI-and-Robotics-Lab/SplitFlow.

</details>


### [55] [Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer](https://arxiv.org/abs/2510.25976)
*Roman Beliy,Amit Zalcher,Jonathan Kogman,Navve Wasserman,Michal Irani*

Main category: cs.CV

TL;DR: Brain-IT: 通过大脑交互Transformer (BIT) 重建人脑fMRI记录中的图像，实现了更真实的图像重建，并超越了当前SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 当前方法重建的图像与实际看到的图像缺乏真实性。

Method: 提出Brain-IT，使用大脑交互Transformer (BIT)，允许功能相似的脑体素簇之间进行有效交互。BIT预测两个互补的局部patch级图像特征：(i) 高级语义特征，引导扩散模型朝向图像的正确语义内容；(ii) 低级结构特征，帮助用正确的图像粗略布局初始化扩散过程。

Result: 实现了从fMRI重建的图像，这些图像忠实地重建了所看到的图像，并且在视觉上和通过标准客观指标都超过了当前的SOTA方法。仅使用来自新受试者的1小时fMRI数据，就获得了与当前在完整40小时记录上训练的方法相当的结果。

Conclusion: Brain-IT方法能够更有效地从fMRI数据中重建出更真实的图像。

Abstract: Reconstructing images seen by people from their fMRI brain recordings
provides a non-invasive window into the human brain. Despite recent progress
enabled by diffusion models, current methods often lack faithfulness to the
actual seen images. We present "Brain-IT", a brain-inspired approach that
addresses this challenge through a Brain Interaction Transformer (BIT),
allowing effective interactions between clusters of functionally-similar
brain-voxels. These functional-clusters are shared by all subjects, serving as
building blocks for integrating information both within and across brains. All
model components are shared by all clusters & subjects, allowing efficient
training with a limited amount of data. To guide the image reconstruction, BIT
predicts two complementary localized patch-level image features: (i)high-level
semantic features which steer the diffusion model toward the correct semantic
content of the image; and (ii)low-level structural features which help to
initialize the diffusion process with the correct coarse layout of the image.
BIT's design enables direct flow of information from brain-voxel clusters to
localized image features. Through these principles, our method achieves image
reconstructions from fMRI that faithfully reconstruct the seen images, and
surpass current SotA approaches both visually and by standard objective
metrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve
results comparable to current methods trained on full 40-hour recordings.

</details>


### [56] [Fine-tuning Segment Anything for Real-Time Tumor Tracking in Cine-MRI](https://arxiv.org/abs/2510.25990)
*Valentin Boussot,Cédric Hémon,Jean-Claude Nunes,Jean-Louis Dillenseger*

Main category: cs.CV

TL;DR: This paper explores real-time tumor tracking in cine-MRI using foundation models, specifically SAM 2.1, fine-tuned on a small dataset. The model achieved a Dice score of 0.8794 on the TrackRAD2025 challenge, ranking 6th overall.


<details>
  <summary>Details</summary>
Motivation: Addressing real-time tumor tracking in cine-MRI sequences under strong data scarcity constraints.

Method: Utilizing SAM 2.1 with mask-based prompts from the first annotated slice, fine-tuned on the TrackRAD2025 labeled subset with specific training configurations to minimize overfitting.

Result: Achieved a Dice score of 0.8794 on the hidden test set, ranking 6th overall in the TrackRAD2025 challenge.

Conclusion: Foundation models show strong potential for accurate and real-time tumor tracking in MRI-guided radiotherapy.

Abstract: In this work, we address the TrackRAD2025 challenge of real-time tumor
tracking in cine-MRI sequences of the thoracic and abdominal regions under
strong data scarcity constraints. Two complementary strategies were explored:
(i) unsupervised registration with the IMPACT similarity metric and (ii)
foundation model-based segmentation leveraging SAM 2.1 and its recent variants
through prompt-based interaction. Due to the one-second runtime constraint, the
SAM-based method was ultimately selected. The final configuration used SAM2.1
b+ with mask-based prompts from the first annotated slice, fine-tuned solely on
the small labeled subset from TrackRAD2025. Training was configured to minimize
overfitting, using 1024x1024 patches (batch size 1), standard augmentations,
and a balanced Dice + IoU loss. A low uniform learning rate (0.0001) was
applied to all modules (prompt encoder, decoder, Hiera backbone) to preserve
generalization while adapting to annotator-specific styles. Training lasted 300
epochs (~12h on RTX A6000, 48GB). The same inference strategy was consistently
applied across all anatomical sites and MRI field strengths. Test-time
augmentation was considered but ultimately discarded due to negligible
performance gains. The final model was selected based on the highest Dice
Similarity Coefficient achieved on the validation set after fine-tuning. On the
hidden test set, the model reached a Dice score of 0.8794, ranking 6th overall
in the TrackRAD2025 challenge. These results highlight the strong potential of
foundation models for accurate and real-time tumor tracking in MRI-guided
radiotherapy.

</details>


### [57] [Larger Hausdorff Dimension in Scanning Pattern Facilitates Mamba-Based Methods in Low-Light Image Enhancement](https://arxiv.org/abs/2510.26001)
*Xinhua Wang,Caibo Feng,Xiangjun Fu,Chunxiao Liu*

Main category: cs.CV

TL;DR: 本文提出了一种改进Mamba框架的新方法，通过Hilbert选择性扫描机制增加其扫描模式的Hausdorff维度。


<details>
  <summary>Details</summary>
Motivation: 为了更有效地探索特征空间，捕获复杂的精细尺度细节，并提高整体覆盖率。

Method: 通过Hilbert选择性扫描机制增加Mamba框架扫描模式的Hausdorff维度。

Result: 在公开基准测试中，该方法显著提高了现有基于Mamba的弱光图像增强方法的定量指标和定性视觉保真度，同时减少了计算资源消耗并缩短了推理时间。

Conclusion: 这种改进的策略不仅提升了弱光图像增强的水平，而且在利用基于Mamba技术的领域中具有更广泛的应用前景。

Abstract: We propose an innovative enhancement to the Mamba framework by increasing the
Hausdorff dimension of its scanning pattern through a novel Hilbert Selective
Scan mechanism. This mechanism explores the feature space more effectively,
capturing intricate fine-scale details and improving overall coverage. As a
result, it mitigates information inconsistencies while refining spatial
locality to better capture subtle local interactions without sacrificing the
model's ability to handle long-range dependencies. Extensive experiments on
publicly available benchmarks demonstrate that our approach significantly
improves both the quantitative metrics and qualitative visual fidelity of
existing Mamba-based low-light image enhancement methods, all while reducing
computational resource consumption and shortening inference time. We believe
that this refined strategy not only advances the state-of-the-art in low-light
image enhancement but also holds promise for broader applications in fields
that leverage Mamba-based techniques.

</details>


### [58] [CAVE: Detecting and Explaining Commonsense Anomalies in Visual Environments](https://arxiv.org/abs/2510.26006)
*Rishika Bhagwatkar,Syrielle Montariol,Angelika Romanou,Beatriz Borges,Irina Rish,Antoine Bosselut*

Main category: cs.CV

TL;DR: 提出了一个名为CAVE的真实世界视觉异常基准测试，用于评估视觉语言模型在异常检测和常识推理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机视觉异常检测方法无法捕捉真实世界异常的丰富性和不可预测性。

Method: 构建了CAVE基准测试，支持异常描述、解释和理由三个开放式任务，并提供细粒度的标注。

Result: 实验表明，即使采用先进的prompting策略，当前最先进的视觉语言模型在视觉异常感知和常识推理方面仍然表现不佳。

Conclusion: CAVE为推进视觉语言模型在异常检测和常识推理方面的研究提供了一个有价值的资源。

Abstract: Humans can naturally identify, reason about, and explain anomalies in their
environment. In computer vision, this long-standing challenge remains limited
to industrial defects or unrealistic, synthetically generated anomalies,
failing to capture the richness and unpredictability of real-world anomalies.
In this work, we introduce CAVE, the first benchmark of real-world visual
anomalies. CAVE supports three open-ended tasks: anomaly description,
explanation, and justification; with fine-grained annotations for visual
grounding and categorizing anomalies based on their visual manifestations,
their complexity, severity, and commonness. These annotations draw inspiration
from cognitive science research on how humans identify and resolve anomalies,
providing a comprehensive framework for evaluating Vision-Language Models
(VLMs) in detecting and understanding anomalies. We show that state-of-the-art
VLMs struggle with visual anomaly perception and commonsense reasoning, even
with advanced prompting strategies. By offering a realistic and cognitively
grounded benchmark, CAVE serves as a valuable resource for advancing research
in anomaly detection and commonsense reasoning in VLMs.

</details>


### [59] [Climate Adaptation-Aware Flood Prediction for Coastal Cities Using Deep Learning](https://arxiv.org/abs/2510.26017)
*Bilal Hassan,Areg Karapetyan,Aaron Chung Hin Chow,Samer Madanat*

Main category: cs.CV

TL;DR: 本文提出了一种新的、轻量级的卷积神经网络（CNN）模型，用于预测不同海平面上升情景和海岸线适应情景下的沿海洪水。


<details>
  <summary>Details</summary>
Motivation: 传统物理水动力模拟器虽然精确，但计算成本高昂，不适用于城市规模的沿海规划应用。深度学习虽然有前景，但受到数据稀缺和高维输出要求的限制。

Method: 利用一种基于视觉的、低资源的深度学习框架，开发了一种基于 CNN 的模型，该模型可以预测不同海平面上升情景和海岸线适应情景下的沿海洪水。

Result: 该模型在预测洪水深度图方面的平均绝对误差（MAE）降低了近 20%，显著优于现有方法。

Conclusion: 该方法有潜力成为一种可扩展的实用工具，用于沿海洪水管理，使决策者能够制定有效的缓解策略，以应对气候变化日益增长的影响。

Abstract: Climate change and sea-level rise (SLR) pose escalating threats to coastal
cities, intensifying the need for efficient and accurate methods to predict
potential flood hazards. Traditional physics-based hydrodynamic simulators,
although precise, are computationally expensive and impractical for city-scale
coastal planning applications. Deep Learning (DL) techniques offer promising
alternatives, however, they are often constrained by challenges such as data
scarcity and high-dimensional output requirements. Leveraging a recently
proposed vision-based, low-resource DL framework, we develop a novel,
lightweight Convolutional Neural Network (CNN)-based model designed to predict
coastal flooding under variable SLR projections and shoreline adaptation
scenarios. Furthermore, we demonstrate the ability of the model to generalize
across diverse geographical contexts by utilizing datasets from two distinct
regions: Abu Dhabi and San Francisco. Our findings demonstrate that the
proposed model significantly outperforms state-of-the-art methods, reducing the
mean absolute error (MAE) in predicted flood depth maps on average by nearly
20%. These results highlight the potential of our approach to serve as a
scalable and practical tool for coastal flood management, empowering
decision-makers to develop effective mitigation strategies in response to the
growing impacts of climate change. Project Page: https://caspiannet.github.io/

</details>


### [60] [Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders](https://arxiv.org/abs/2510.26027)
*Ali Rasekh,Erfan Bagheri Soula,Omid Daliran,Simon Gottschalk,Mohsen Fayyaz*

Main category: cs.CV

TL;DR: 现有的视频大语言模型在理解视频中的复杂时间动态方面存在局限性。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型在时间理解方面存在关键限制，难以处理需要详细理解动作序列和时间进展的任务。

Method: 提出了一种新的视频大语言模型架构，该架构在视觉编码器中引入了堆叠的时间注意力模块，从而在视觉编码器中加入了时间注意力。

Result: 该方法显著提高了时间推理能力，并在视频问答任务中优于现有模型，尤其是在动作识别方面。在VITATECS、MVBench和Video-MME等基准测试中，性能提高了高达+5.5%。

Conclusion: 通过使用时间结构增强视觉编码器，解决了视频大语言模型在视频理解方面的一个关键差距。

Abstract: Despite significant advances in Multimodal Large Language Models (MLLMs),
understanding complex temporal dynamics in videos remains a major challenge.
Our experiments show that current Video Large Language Model (Video-LLM)
architectures have critical limitations in temporal understanding, struggling
with tasks that require detailed comprehension of action sequences and temporal
progression. In this work, we propose a Video-LLM architecture that introduces
stacked temporal attention modules directly within the vision encoder. This
design incorporates a temporal attention in vision encoder, enabling the model
to better capture the progression of actions and the relationships between
frames before passing visual tokens to the LLM. Our results show that this
approach significantly improves temporal reasoning and outperforms existing
models in video question answering tasks, specifically in action recognition.
We improve on benchmarks including VITATECS, MVBench, and Video-MME by up to
+5.5%. By enhancing the vision encoder with temporal structure, we address a
critical gap in video understanding for Video-LLMs. Project page and code are
available at: https://alirasekh.github.io/STAVEQ2/.

</details>


### [61] [FlexICL: A Flexible Visual In-context Learning Framework for Elbow and Wrist Ultrasound Segmentation](https://arxiv.org/abs/2510.26049)
*Yuyue Zhou,Jessica Knight,Shrimanti Ghosh,Banafshe Felfeliyan,Jacob L. Jaremko,Abhilash R. Hareendranathan*

Main category: cs.CV

TL;DR: 提出了一种新颖的上下文学习框架 FlexICL，用于分割超声图像中的骨骼区域，尤其是在标记数据稀缺的情况下。


<details>
  <summary>Details</summary>
Motivation: 儿童肘部和腕部骨折很常见，超声 (US) 成像中骨骼肌肉结构的自动分割可以提高诊断准确性和治疗计划的效率。然而，像素级的专家注释非常耗时且成本高昂。

Method: 提出了 FlexICL，一种灵活的上下文学习框架，用于分割超声图像中的骨骼区域。该方法研究了各种图像连接技术和视觉上下文学习的训练策略，并引入了新的连接方法。

Result: FlexICL 在四个腕部和肘部超声数据集上实现了稳健的分割性能，仅需 5% 的训练图像，并且优于其他模型。

Conclusion: FlexICL 是一种高效且可扩展的超声图像分割解决方案，非常适合标记数据稀缺的医学成像用例。

Abstract: Elbow and wrist fractures are the most common fractures in pediatric
populations. Automatic segmentation of musculoskeletal structures in ultrasound
(US) can improve diagnostic accuracy and treatment planning. Fractures appear
as cortical defects but require expert interpretation. Deep learning (DL) can
provide real-time feedback and highlight key structures, helping lightly
trained users perform exams more confidently. However, pixel-wise expert
annotations for training remain time-consuming and costly. To address this
challenge, we propose FlexICL, a novel and flexible in-context learning (ICL)
framework for segmenting bony regions in US images. We apply it to an
intra-video segmentation setting, where experts annotate only a small subset of
frames, and the model segments unseen frames. We systematically investigate
various image concatenation techniques and training strategies for visual ICL
and introduce novel concatenation methods that significantly enhance model
performance with limited labeled data. By integrating multiple augmentation
strategies, FlexICL achieves robust segmentation performance across four wrist
and elbow US datasets while requiring only 5% of the training images. It
outperforms state-of-the-art visual ICL models like Painter, MAE-VQGAN, and
conventional segmentation models like U-Net and TransUNet by 1-27% Dice
coefficient on 1,252 US sweeps. These initial results highlight the potential
of FlexICL as an efficient and scalable solution for US image segmentation well
suited for medical imaging use cases where labeled data is scarce.

</details>


### [62] [AdSum: Two-stream Audio-visual Summarization for Automated Video Advertisement Clipping](https://arxiv.org/abs/2510.26569)
*Wen Xie,Yanjun Zhu,Gijs Overgoor,Yakov Bart,Agata Lapedriza Garcia,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: 本文提出了一种使用视频摘要技术自动剪辑视频广告的框架，以解决手动选择和重新编辑镜头的费时费力问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要手动选择和重新编辑较长视频广告中的镜头以创建较短版本，这非常耗费人力和时间。

Method: 开发了一个双流音视频融合模型，用于预测视频帧的重要性，其中重要性定义为帧在公司制作的短广告中被选择的可能性。构建了一个名为AdSum204的新数据集，其中包含来自真实广告活动的102对30秒和15秒广告。

Result: 该模型在各种指标（包括平均精度、曲线下面积、Spearman和Kendall）上均优于最先进的方法。

Conclusion: 该方法优于现有技术，为广告视频剪辑提供了一种有效的自动化解决方案。

Abstract: Advertisers commonly need multiple versions of the same advertisement (ad) at
varying durations for a single campaign. The traditional approach involves
manually selecting and re-editing shots from longer video ads to create shorter
versions, which is labor-intensive and time-consuming. In this paper, we
introduce a framework for automated video ad clipping using video summarization
techniques. We are the first to frame video clipping as a shot selection
problem, tailored specifically for advertising. Unlike existing general video
summarization methods that primarily focus on visual content, our approach
emphasizes the critical role of audio in advertising. To achieve this, we
develop a two-stream audio-visual fusion model that predicts the importance of
video frames, where importance is defined as the likelihood of a frame being
selected in the firm-produced short ad. To address the lack of ad-specific
datasets, we present AdSum204, a novel dataset comprising 102 pairs of
30-second and 15-second ads from real advertising campaigns. Extensive
experiments demonstrate that our model outperforms state-of-the-art methods
across various metrics, including Average Precision, Area Under Curve,
Spearman, and Kendall.

</details>


### [63] [Dynamic VLM-Guided Negative Prompting for Diffusion Models](https://arxiv.org/abs/2510.26052)
*Hoyeon Chang,Seungjin Kim,Yoonseok Choi*

Main category: cs.CV

TL;DR: 提出了一种新的动态负提示方法，该方法利用视觉语言模型 (VLM) 在去噪过程中自适应生成负提示。


<details>
  <summary>Details</summary>
Motivation: 与使用固定负提示的传统负提示方法不同，我们的方法在特定的去噪步骤生成中间图像预测，并查询 VLM 以生成上下文相关的负提示。

Method: 在扩散模型中使用动态负提示。

Result: 在各种基准数据集上评估了我们的方法，并证明了负引导强度和文本图像对齐之间的权衡。

Conclusion: 动态负提示方法可以提高扩散模型的性能。

Abstract: We propose a novel approach for dynamic negative prompting in diffusion
models that leverages Vision-Language Models (VLMs) to adaptively generate
negative prompts during the denoising process. Unlike traditional Negative
Prompting methods that use fixed negative prompts, our method generates
intermediate image predictions at specific denoising steps and queries a VLM to
produce contextually appropriate negative prompts. We evaluate our approach on
various benchmark datasets and demonstrate the trade-offs between negative
guidance strength and text-image alignment.

</details>


### [64] [Security Risk of Misalignment between Text and Image in Multi-modal Model](https://arxiv.org/abs/2510.26105)
*Xiaosen Wang,Zhijin Ge,Shaokang Wang*

Main category: cs.CV

TL;DR: 本文研究了多模态扩散模型对对抗性输入的脆弱性，发现文本和图像模态之间的对齐不足会导致生成不安全内容。提出了Prompt-Restricted Multi-modal Attack (PReMA)，通过修改输入图像来操纵生成内容，而无需更改prompt本身。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型中文本和图像模态之间的对齐不足，导致容易生成不安全内容，并且图像编辑应用在固定prompt下存在风险。

Method: 提出了一种名为Prompt-Restricted Multi-modal Attack (PReMA)的新型攻击，该攻击通过修改输入图像来操纵生成内容，而无需更改prompt本身。

Result: 在图像修复和风格转换任务上的综合评估证实了PReMA的有效性。

Conclusion: PReMA对多模态扩散模型的完整性构成了新的威胁，尤其是在使用固定prompt的图像编辑应用程序中。

Abstract: Despite the notable advancements and versatility of multi-modal diffusion
models, such as text-to-image models, their susceptibility to adversarial
inputs remains underexplored. Contrary to expectations, our investigations
reveal that the alignment between textual and Image modalities in existing
diffusion models is inadequate. This misalignment presents significant risks,
especially in the generation of inappropriate or Not-Safe-For-Work (NSFW)
content. To this end, we propose a novel attack called Prompt-Restricted
Multi-modal Attack (PReMA) to manipulate the generated content by modifying the
input image in conjunction with any specified prompt, without altering the
prompt itself. PReMA is the first attack that manipulates model outputs by
solely creating adversarial images, distinguishing itself from prior methods
that primarily generate adversarial prompts to produce NSFW content.
Consequently, PReMA poses a novel threat to the integrity of multi-modal
diffusion models, particularly in image-editing applications that operate with
fixed prompts. Comprehensive evaluations conducted on image inpainting and
style transfer tasks across various models confirm the potent efficacy of
PReMA.

</details>


### [65] [EgoExo-Con: Exploring View-Invariant Video Temporal Understanding](https://arxiv.org/abs/2510.26113)
*Minjoon Jung,Junbin Xiao,Junghyun Kim,Byoung-Tak Zhang,Angela Yao*

Main category: cs.CV

TL;DR: 这篇论文研究了视频大语言模型(Video-LLM)在处理来自不同视角的同一事件视频时，是否能保持一致的时间理解。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于考察现有Video-LLM在多视角视频理解中的一致性问题，特别是在时间理解方面。

Method: 提出了EgoExo-Con基准，包含同步的以自我为中心和以外部为中心的视频对，并使用人工设计的自然语言查询。同时，提出了View-GRPO强化学习框架，以提升模型在不同视角下的一致性和时间推理能力。

Result: 发现现有Video-LLM在保持跨视角一致性方面表现不佳，即使使用同步视频进行微调，效果也不如单视角训练。View-GRPO方法在提高跨视角一致性方面优于其他方法。

Conclusion: 论文指出现有Video-LLM在跨视角时间理解方面存在局限性，并提出了一种新的强化学习框架来改善这一问题。

Abstract: Can Video-LLMs achieve consistent temporal understanding when videos capture
the same event from different viewpoints? To study this, we introduce
EgoExo-Con (Consistency), a benchmark of comprehensively synchronized
egocentric and exocentric video pairs with human-refined queries in natural
language. EgoExo-Con emphasizes two temporal understanding tasks: Temporal
Verification and Temporal Grounding. It evaluates not only correctness but
consistency across viewpoints. Our analysis reveals two critical limitations of
existing Video-LLMs: (1) models often fail to maintain consistency, with
results far worse than their single-view performances. (2) When naively
finetuned with synchronized videos of both viewpoints, the models show improved
consistency but often underperform those trained on a single view. For
improvements, we propose View-GRPO, a novel reinforcement learning framework
that effectively strengthens view-specific temporal reasoning while encouraging
consistent comprehension across viewpoints. Our method demonstrates its
superiority over naive SFT and GRPO, especially for improving cross-view
consistency. All resources will be made publicly available.

</details>


### [66] [OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script Research](https://arxiv.org/abs/2510.26114)
*Caoshuo Li,Zengmao Ding,Xiaobin Hu,Bang Li,Donghao Luo,Xu Peng,Taisong Jin,Yongge Liu,Shengwei Han,Jing Yang,Xiaoping He,Feng Gao,AndyPian Wu,SevenShu,Chaoyang Wang,Chengjie Wang*

Main category: cs.CV

TL;DR: OracleAgent是一个为甲骨文脚本设计的智能代理系统，旨在解决甲骨文信息组织和检索效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 当前甲骨文研究面临解释流程复杂和信息组织检索效率低下的挑战。

Method: 构建了一个综合性的特定领域多模态知识库，并集成了多个甲骨文分析工具，由大型语言模型驱动。

Result: OracleAgent在多模态推理和生成任务中表现出色，超越了主流多模态大型语言模型，并能有效协助领域专家，显著降低研究的时间成本。

Conclusion: OracleAgent是甲骨文辅助研究和自动释读系统迈出的重要一步。

Abstract: As one of the earliest writing systems, Oracle Bone Script (OBS) preserves
the cultural and intellectual heritage of ancient civilizations. However,
current OBS research faces two major challenges: (1) the interpretation of OBS
involves a complex workflow comprising multiple serial and parallel sub-tasks,
and (2) the efficiency of OBS information organization and retrieval remains a
critical bottleneck, as scholars often spend substantial effort searching for,
compiling, and managing relevant resources. To address these challenges, we
present OracleAgent, the first agent system designed for the structured
management and retrieval of OBS-related information. OracleAgent seamlessly
integrates multiple OBS analysis tools, empowered by large language models
(LLMs), and can flexibly orchestrate these components. Additionally, we
construct a comprehensive domain-specific multimodal knowledge base for OBS,
which is built through a rigorous multi-year process of data collection,
cleaning, and expert annotation. The knowledge base comprises over 1.4M
single-character rubbing images and 80K interpretation texts. OracleAgent
leverages this resource through its multimodal tools to assist experts in
retrieval tasks of character, document, interpretation text, and rubbing image.
Extensive experiments demonstrate that OracleAgent achieves superior
performance across a range of multimodal reasoning and generation tasks,
surpassing leading mainstream multimodal large language models (MLLMs) (e.g.,
GPT-4o). Furthermore, our case study illustrates that OracleAgent can
effectively assist domain experts, significantly reducing the time cost of OBS
research. These results highlight OracleAgent as a significant step toward the
practical deployment of OBS-assisted research and automated interpretation
systems.

</details>


### [67] [JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting](https://arxiv.org/abs/2510.26117)
*Yuxuan Li,Tao Wang,Xianben Yang*

Main category: cs.CV

TL;DR: 提出了一种联合优化3D高斯点和相机姿态的统一框架，无需预校准输入。


<details>
  <summary>Details</summary>
Motivation: 传统的新视角合成方法严重依赖外部相机姿态估计工具（如COLMAP），这通常会引入计算瓶颈并传播误差。

Method: 通过一种新颖的协同优化策略迭代地改进3D高斯参数并更新相机姿态，确保同时提高场景重建保真度和姿态精度。将联合优化解耦为两个交错的阶段：首先，通过具有固定姿态的可微渲染更新3D高斯参数；其次，使用定制的3D光流算法细化相机姿态，该算法结合了几何和光度约束。

Result: 在多个数据集上的大量评估表明，该方法在重建质量方面明显优于现有的无COLMAP技术，并且在总体上也超过了基于COLMAP的标准基线。

Conclusion: 该方法可以有效减少投影误差，特别是在具有大视点变化和稀疏特征分布的具有挑战性的场景中。

Abstract: Traditional novel view synthesis methods heavily rely on external camera pose
estimation tools such as COLMAP, which often introduce computational
bottlenecks and propagate errors. To address these challenges, we propose a
unified framework that jointly optimizes 3D Gaussian points and camera poses
without requiring pre-calibrated inputs. Our approach iteratively refines 3D
Gaussian parameters and updates camera poses through a novel co-optimization
strategy, ensuring simultaneous improvements in scene reconstruction fidelity
and pose accuracy. The key innovation lies in decoupling the joint optimization
into two interleaved phases: first, updating 3D Gaussian parameters via
differentiable rendering with fixed poses, and second, refining camera poses
using a customized 3D optical flow algorithm that incorporates geometric and
photometric constraints. This formulation progressively reduces projection
errors, particularly in challenging scenarios with large viewpoint variations
and sparse feature distributions, where traditional methods struggle. Extensive
evaluations on multiple datasets demonstrate that our approach significantly
outperforms existing COLMAP-free techniques in reconstruction quality, and also
surpasses the standard COLMAP-based baseline in general.

</details>


### [68] [WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios](https://arxiv.org/abs/2510.26125)
*Runsheng Xu,Hubert Lin,Wonseok Jeon,Hao Feng,Yuliang Zou,Liting Sun,John Gorman,Kate Tolstaya,Sarah Tang,Brandyn White,Ben Sapp,Mingxing Tan,Jyh-Jing Hwang,Drago Anguelov*

Main category: cs.CV

TL;DR: 提出了一个新的自动驾驶数据集WOD-E2E，专注于长尾场景，并提出了一个新的评估指标RFS。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端驾驶基准测试主要包含常规场景，未能充分测试系统的潜力。现有的开环评估指标通常无法捕捉驾驶的多模态特性或有效评估长尾场景中的性能。

Method: 引入了Waymo Open Dataset for End-to-End Driving (WOD-E2E)，包含4,021个驾驶片段，并提出了新的开环评估指标：Rater Feedback Score (RFS)。

Result: 发布了所有WOD-E2E验证集片段的评分者偏好标签，而保留的测试集标签已用于2025 WOD-E2E Challenge。

Conclusion: 旨在促进对能够处理复杂现实世界情况的通用、稳健和安全的端到端自动驾驶代理的最先进的研究。

Abstract: Vision-based end-to-end (E2E) driving has garnered significant interest in
the research community due to its scalability and synergy with multimodal large
language models (MLLMs). However, current E2E driving benchmarks primarily
feature nominal scenarios, failing to adequately test the true potential of
these systems. Furthermore, existing open-loop evaluation metrics often fall
short in capturing the multi-modal nature of driving or effectively evaluating
performance in long-tail scenarios. To address these gaps, we introduce the
Waymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021
driving segments (approximately 12 hours), specifically curated for challenging
long-tail scenarios that that are rare in daily life with an occurring
frequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the
high-level routing information, ego states, and 360-degree camera views from 8
surrounding cameras. To evaluate the E2E driving performance on these long-tail
situations, we propose a novel open-loop evaluation metric: Rater Feedback
Score (RFS). Unlike conventional metrics that measure the distance between
predicted way points and the logs, RFS measures how closely the predicted
trajectory matches rater-annotated trajectory preference labels. We have
released rater preference labels for all WOD-E2E validation set segments, while
the held out test set labels have been used for the 2025 WOD-E2E Challenge.
Through our work, we aim to foster state of the art research into
generalizable, robust, and safe end-to-end autonomous driving agents capable of
handling complex real-world situations.

</details>


### [69] [Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM](https://arxiv.org/abs/2510.26131)
*Ali Caglayan,Nevrez Imamoglu,Oguzhan Guclu,Ali Osman Serhatoglu,Ahmet Burak Can,Ryosuke Nakamura*

Main category: cs.CV

TL;DR: 本文提出了一种利用任务相关的网络注意力机制进行RGB-D室内SLAM的方法，通过将网络梯度导出的分层注意力信息与CNN特征表示相结合，以提高帧关联性能。


<details>
  <summary>Details</summary>
Motivation: 将基于梯度的注意力信息直接整合到CNN表示中以进行语义对象理解仍然有限。这种集成对于视觉任务（如同时定位和映射（SLAM））特别有益，其中富含空间注意对象位置的CNN表示可以提高性能。

Method: 将从网络梯度导出的分层注意力信息与CNN特征表示相集成。

Result: 实验结果表明，与基线方法相比，性能有所提高，尤其是在大型环境中。

Conclusion: 利用任务相关的网络注意力进行RGB-D室内SLAM可以提高帧关联性能，尤其是在大型环境中。

Abstract: Attention models have recently emerged as a powerful approach, demonstrating
significant progress in various fields. Visualization techniques, such as class
activation mapping, provide visual insights into the reasoning of convolutional
neural networks (CNNs). Using network gradients, it is possible to identify
regions where the network pays attention during image recognition tasks.
Furthermore, these gradients can be combined with CNN features to localize more
generalizable, task-specific attentive (salient) regions within scenes.
However, explicit use of this gradient-based attention information integrated
directly into CNN representations for semantic object understanding remains
limited. Such integration is particularly beneficial for visual tasks like
simultaneous localization and mapping (SLAM), where CNN representations
enriched with spatially attentive object locations can enhance performance. In
this work, we propose utilizing task-specific network attention for RGB-D
indoor SLAM. Specifically, we integrate layer-wise attention information
derived from network gradients with CNN feature representations to improve
frame association performance. Experimental results indicate improved
performance compared to baseline methods, particularly for large environments.

</details>


### [70] [FullPart: Generating each 3D Part at Full Resolution](https://arxiv.org/abs/2510.26140)
*Lihe Ding,Shaocong Dong,Yaokun Li,Chenjian Gao,Xiao Chen,Rui Han,Yihao Kuang,Hong Zhang,Bo Huang,Zhanpeng Huang,Zibin Wang,Dan Xu,Tianfan Xue*

Main category: cs.CV

TL;DR: 提出FullPart，一个结合隐式和显式范例的新框架，用于生成具有复杂细节的3D部件，并引入PartVerse-XL数据集。


<details>
  <summary>Details</summary>
Motivation: 以往的部件生成器在几何细节上不足，或者由于共享全局体素网格而导致小部件质量下降。

Method: 首先通过隐式框向量集扩散过程导出边界框布局，然后生成详细的部件，每个部件都在其自己的固定全分辨率体素网格中。引入中心点编码策略来解决部件之间信息交换时的不对齐问题。

Result: FullPart在3D部件生成方面取得了最先进的结果。

Conclusion: 发布所有代码、数据和模型，以促进未来在3D部件生成方面的研究。

Abstract: Part-based 3D generation holds great potential for various applications.
Previous part generators that represent parts using implicit vector-set tokens
often suffer from insufficient geometric details. Another line of work adopts
an explicit voxel representation but shares a global voxel grid among all
parts; this often causes small parts to occupy too few voxels, leading to
degraded quality. In this paper, we propose FullPart, a novel framework that
combines both implicit and explicit paradigms. It first derives the bounding
box layout through an implicit box vector-set diffusion process, a task that
implicit diffusion handles effectively since box tokens contain little
geometric detail. Then, it generates detailed parts, each within its own fixed
full-resolution voxel grid. Instead of sharing a global low-resolution space,
each part in our method - even small ones - is generated at full resolution,
enabling the synthesis of intricate details. We further introduce a
center-point encoding strategy to address the misalignment issue when
exchanging information between parts of different actual sizes, thereby
maintaining global coherence. Moreover, to tackle the scarcity of reliable part
data, we present PartVerse-XL, the largest human-annotated 3D part dataset to
date with 40K objects and 320K parts. Extensive experiments demonstrate that
FullPart achieves state-of-the-art results in 3D part generation. We will
release all code, data, and model to benefit future research in 3D part
generation.

</details>


### [71] [BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and Enhanced Motion Compensation](https://arxiv.org/abs/2510.26149)
*Wei Shang,Wanying Zhang,Shuhang Gu,Pengfei Zhu,Qinghua Hu,Dongwei Ren*

Main category: cs.CV

TL;DR: 本文提出了一个强大的任意比例视频超分辨率(AVSR)基线模型BasicAVSR，该模型集成了多尺度频率先验、光流引导的传播单元、二阶运动补偿单元和超像素上采样单元。


<details>
  <summary>Details</summary>
Motivation: AVSR旨在提高视频帧的分辨率，可能在不同的缩放因子下进行，这在空间细节再现、时间一致性和计算复杂性方面提出了一些挑战。

Method: 该模型通过集成图像拉普拉斯金字塔生成自适应多尺度频率先验，使用光流引导的传播单元聚合来自相邻帧的时空信息，使用二阶运动补偿单元更准确地对齐相邻帧，并使用超像素上采样单元生成尺度感知和内容无关的上采样核。

Result: 实验结果表明，该模型在不同场景中都有效且具有适应性。大量实验表明，BasicAVSR在超分辨率质量、泛化能力和推理速度方面明显优于现有方法。

Conclusion: 该工作不仅推进了AVSR的最新技术，而且将其核心组件扩展到多个框架，以适应不同的场景。

Abstract: Arbitrary-scale video super-resolution (AVSR) aims to enhance the resolution
of video frames, potentially at various scaling factors, which presents several
challenges regarding spatial detail reproduction, temporal consistency, and
computational complexity. In this paper, we propose a strong baseline BasicAVSR
for AVSR by integrating four key components: 1) adaptive multi-scale frequency
priors generated from image Laplacian pyramids, 2) a flow-guided propagation
unit to aggregate spatiotemporal information from adjacent frames, 3) a
second-order motion compensation unit for more accurate spatial alignment of
adjacent frames, and 4) a hyper-upsampling unit to generate scale-aware and
content-independent upsampling kernels. To meet diverse application demands, we
instantiate three propagation variants: (i) a unidirectional RNN unit for
strictly online inference, (ii) a unidirectional RNN unit empowered with a
limited lookahead that tolerates a small output delay, and (iii) a
bidirectional RNN unit designed for offline tasks where computational resources
are less constrained. Experimental results demonstrate the effectiveness and
adaptability of our model across these different scenarios. Through extensive
experiments, we show that BasicAVSR significantly outperforms existing methods
in terms of super-resolution quality, generalization ability, and inference
speed. Our work not only advances the state-of-the-art in AVSR but also extends
its core components to multiple frameworks for diverse scenarios. The code is
available at https://github.com/shangwei5/BasicAVSR.

</details>


### [72] [MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction](https://arxiv.org/abs/2510.26151)
*Shunjie-Fabian Zheng,Hyeonjun Lee,Thijs Kooi,Ali Diba*

Main category: cs.CV

TL;DR: 本文提出了一种新的多视图乳腺钼靶和语言模型（MV-MLM），用于乳腺癌分类和风险预测，该模型在配对的乳腺钼靶图像和合成放射学报告的数据集上进行训练。


<details>
  <summary>Details</summary>
Motivation: 获取带有精细注释的大型带注释数据集对于训练用于乳腺癌检测或风险预测的稳健计算机辅助诊断（CAD）模型至关重要，但是，获取此类数据集既昂贵又耗时。视觉语言模型（VLM），例如 CLIP，通过增强医学成像任务中的鲁棒性和数据效率，提供了一种有希望的解决方案。

Method: 该MV-MLM利用多视图监督，通过图像-文本对之间的跨模态自监督，从广泛的放射学数据中学习丰富的表示。这包括多个视图和相应的伪放射学报告。提出了一种新颖的联合视觉-文本学习策略，以提高不同数据类型和任务的泛化和准确性性能，从而区分乳腺组织或癌症特征（钙化、肿块），并利用这些模式来理解乳腺X线照片图像并预测癌症风险。

Result: 在私人和公开可用的数据集上评估了该方法，证明了所提出的模型在三个分类任务中实现了最先进的性能：（1）恶性肿瘤分类，（2）亚型分类，以及（3）基于图像的癌症风险预测。

Conclusion: 该模型表现出强大的数据效率，在合成文本报告上进行训练时，优于现有的完全监督或VLM基线，而无需实际的放射学报告。

Abstract: Large annotated datasets are essential for training robust Computer-Aided
Diagnosis (CAD) models for breast cancer detection or risk prediction. However,
acquiring such datasets with fine-detailed annotation is both costly and
time-consuming. Vision-Language Models (VLMs), such as CLIP, which are
pre-trained on large image-text pairs, offer a promising solution by enhancing
robustness and data efficiency in medical imaging tasks. This paper introduces
a novel Multi-View Mammography and Language Model for breast cancer
classification and risk prediction, trained on a dataset of paired mammogram
images and synthetic radiology reports. Our MV-MLM leverages multi-view
supervision to learn rich representations from extensive radiology data by
employing cross-modal self-supervision across image-text pairs. This includes
multiple views and the corresponding pseudo-radiology reports. We propose a
novel joint visual-textual learning strategy to enhance generalization and
accuracy performance over different data types and tasks to distinguish breast
tissues or cancer characteristics(calcification, mass) and utilize these
patterns to understand mammography images and predict cancer risk. We evaluated
our method on both private and publicly available datasets, demonstrating that
the proposed model achieves state-of-the-art performance in three
classification tasks: (1) malignancy classification, (2) subtype
classification, and (3) image-based cancer risk prediction. Furthermore, the
model exhibits strong data efficiency, outperforming existing fully supervised
or VLM baselines while trained on synthetic text reports and without the need
for actual radiology reports.

</details>


### [73] [Detecting Unauthorized Vehicles using Deep Learning for Smart Cities: A Case Study on Bangladesh](https://arxiv.org/abs/2510.26154)
*Sudipto Das Sukanto,Diponker Roy,Fahim Shakil,Nirjhar Singha,Abdullah Asik,Aniket Joarder,Mridha Md Nafis Fuad,Muhammad Ibrahim*

Main category: cs.CV

TL;DR: 本文提出了一种基于机器学习的方法，用于在交通图像中自动检测电动三轮车。


<details>
  <summary>Details</summary>
Motivation: 由于电动三轮车与其它车辆相似，且人工视频分析耗时，因此需要自动监测电动三轮车的移动。

Method: 使用YOLOv8模型进行实时目标检测，并使用包含1730张带注释图像的数据集进行训练。

Result: 该模型在实时电动三轮车检测中表现良好，mAP50为83.447%，二元精度和召回率均高于78%。

Conclusion: 该模型在处理密集和稀疏交通场景中均有效，并且数据集已公开发布以供进一步研究。

Abstract: Modes of transportation vary across countries depending on geographical
location and cultural context. In South Asian countries rickshaws are among the
most common means of local transport. Based on their mode of operation,
rickshaws in cities across Bangladesh can be broadly classified into non-auto
(pedal-powered) and auto-rickshaws (motorized). Monitoring the movement of
auto-rickshaws is necessary as traffic rules often restrict auto-rickshaws from
accessing certain routes. However, existing surveillance systems make it quite
difficult to monitor them due to their similarity to other vehicles, especially
non-auto rickshaws whereas manual video analysis is too time-consuming. This
paper presents a machine learning-based approach to automatically detect
auto-rickshaws in traffic images. In this system, we used real-time object
detection using the YOLOv8 model. For training purposes, we prepared a set of
1,730 annotated images that were captured under various traffic conditions. The
results show that our proposed model performs well in real-time auto-rickshaw
detection and offers an mAP50 of 83.447% and binary precision and recall values
above 78%, demonstrating its effectiveness in handling both dense and sparse
traffic scenarios. The dataset has been publicly released for further research.

</details>


### [74] [CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark](https://arxiv.org/abs/2510.26160)
*Jiaqi Wang,Xiao Yang,Kai Sun,Parth Suresh,Sanat Sharma,Adam Czyzewski,Derek Andersen,Surya Appini,Arkav Banerjee,Sajal Choudhary,Shervin Ghasemlou,Ziqiang Guan,Akil Iyer,Haidar Khan,Lingkun Kong,Roy Luo,Tiffany Ma,Zhen Qiao,David Tran,Wenfang Xu,Skyler Yeatman,Chen Zhou,Gunveer Gujral,Yinglong Xia,Shane Moon,Nicolas Scheffer,Nirav Shah,Eun Chang,Yue Liu,Florian Metze,Tammy Stark,Zhaleh Feizollahi,Andrea Jessee,Mangesh Pujari,Ahmed Aly,Babak Damavandi,Rakesh Wanga,Anuj Kumar,Rohit Patel,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CV

TL;DR: 提出了一个用于多模态检索增强生成(MM-RAG)的综合基准测试，特别关注可穿戴设备场景。


<details>
  <summary>Details</summary>
Motivation: 缺乏针对可穿戴设备场景的综合性MM-RAG基准测试。

Method: 构建了一个名为CRAG-MM的基准测试，包含6.5K个(图像，问题，答案)三元组和2K个基于视觉的多轮对话，涵盖13个领域，包括6.2K个模仿可穿戴设备拍摄的以自我为中心的图像。设计了三种任务：单源增强、多源增强和多轮对话。

Result: 直接的RAG方法在CRAG-MM单轮和多轮QA上的真实性分别仅达到32%和43%，而最先进的行业解决方案具有相似的质量(32%/45%)，表明有很大的改进空间。

Conclusion: CRAG-MM基准测试已举办KDD Cup 2025，吸引了约1K参与者和5K提交，获奖解决方案将基线性能提高了28%，突显了其在推动该领域发展方面的早期影响。

Abstract: Wearable devices such as smart glasses are transforming the way people
interact with their surroundings, enabling users to seek information regarding
entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)
plays a key role in supporting such questions, yet there is still no
comprehensive benchmark for this task, especially regarding wearables
scenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG
benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse
set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn
conversations across 13 domains, including 6.2K egocentric images designed to
mimic captures from wearable devices. We carefully constructed the questions to
reflect real-world scenarios and challenges, including five types of
image-quality issues, six question types, varying entity popularity, differing
information dynamism, and different conversation turns. We design three tasks:
single-source augmentation, multi-source augmentation, and multi-turn
conversations -- each paired with an associated retrieval corpus and APIs for
both image-KG retrieval and webpage retrieval. Our evaluation shows that
straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM
single- and multi-turn QA, respectively, whereas state-of-the-art industry
solutions have similar quality (32%/45%), underscoring ample room for
improvement. The benchmark has hosted KDD Cup 2025, attracting about 1K
participants and 5K submissions, with winning solutions improving baseline
performance by 28%, highlighting its early impact on advancing the field.

</details>


### [75] [MoTDiff: High-resolution Motion Trajectory estimation from a single blurred image using Diffusion models](https://arxiv.org/abs/2510.26173)
*Wontae Choi,Jaelin Lee,Hyung Sup Yun,Byeungwoo Jeon,Il Yong Chun*

Main category: cs.CV

TL;DR: 提出了一种新的高分辨率运动轨迹估计框架，使用扩散模型从模糊图像中估计高质量的运动轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有的运动信息提取方法质量较低，粗粒度且不准确。

Method: 提出一个条件扩散框架，使用从模糊图像中提取的多尺度特征图作为条件，并提出一种新的训练方法，以促进精细运动轨迹的精确识别。

Result: 在盲图像去模糊和编码曝光摄影应用中，MoTDiff 优于现有技术。

Conclusion: MoTDiff 能够有效地估计高分辨率运动轨迹。

Abstract: Accurate estimation of motion information is crucial in diverse computational
imaging and computer vision applications. Researchers have investigated various
methods to extract motion information from a single blurred image, including
blur kernels and optical flow. However, existing motion representations are
often of low quality, i.e., coarse-grained and inaccurate. In this paper, we
propose the first high-resolution (HR) Motion Trajectory estimation framework
using Diffusion models (MoTDiff). Different from existing motion
representations, we aim to estimate an HR motion trajectory with high-quality
from a single motion-blurred image. The proposed MoTDiff consists of two key
components: 1) a new conditional diffusion framework that uses multi-scale
feature maps extracted from a single blurred image as a condition, and 2) a new
training method that can promote precise identification of a fine-grained
motion trajectory, consistent estimation of overall shape and position of a
motion path, and pixel connectivity along a motion trajectory. Our experiments
demonstrate that the proposed MoTDiff can outperform state-of-the-art methods
in both blind image deblurring and coded exposure photography applications.

</details>


### [76] [ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts](https://arxiv.org/abs/2510.26186)
*Jinho Choi,Hyesu Lim,Steffen Schneider,Jaegul Choo*

Main category: cs.CV

TL;DR: ConceptScope是一个可扩展的自动化框架，通过发现和量化人类可解释的概念来分析视觉数据集，使用在视觉基础模型表示上训练的稀疏自动编码器。


<details>
  <summary>Details</summary>
Motivation: 机器学习数据集中普遍存在数据集偏差，其中数据点偏向于某些概念。然而，如果没有代价高昂的细粒度属性注释，系统地识别这些偏差具有挑战性。

Method: 使用稀疏自动编码器训练视觉基础模型表示，ConceptScope将概念根据其语义相关性和与类标签的统计相关性分为目标、上下文和偏差类型，从而实现基于概念的分组，进行类级别的dataset表征、偏差识别和鲁棒性评估。

Result: ConceptScope捕获了广泛的视觉概念，包括对象、纹理、背景、面部属性、情感和动作，通过与注释数据集的比较进行验证。 概念激活产生与语义上有意义的图像区域对齐的空间归因。ConceptScope可靠地检测已知的偏差（例如，Waterbirds中的背景偏差）并发现以前未注释的偏差（例如，ImageNet中共同出现的对象）。

Conclusion: ConceptScope为数据集审计和模型诊断提供了一个实用的工具。

Abstract: Dataset bias, where data points are skewed to certain concepts, is ubiquitous
in machine learning datasets. Yet, systematically identifying these biases is
challenging without costly, fine-grained attribute annotations. We present
ConceptScope, a scalable and automated framework for analyzing visual datasets
by discovering and quantifying human-interpretable concepts using Sparse
Autoencoders trained on representations from vision foundation models.
ConceptScope categorizes concepts into target, context, and bias types based on
their semantic relevance and statistical correlation to class labels, enabling
class-level dataset characterization, bias identification, and robustness
evaluation through concept-based subgrouping. We validate that ConceptScope
captures a wide range of visual concepts, including objects, textures,
backgrounds, facial attributes, emotions, and actions, through comparisons with
annotated datasets. Furthermore, we show that concept activations produce
spatial attributions that align with semantically meaningful image regions.
ConceptScope reliably detects known biases (e.g., background bias in
Waterbirds) and uncovers previously unannotated ones (e.g, co-occurring objects
in ImageNet), offering a practical tool for dataset auditing and model
diagnostics.

</details>


### [77] [Sketch2PoseNet: Efficient and Generalized Sketch to 3D Human Pose Prediction](https://arxiv.org/abs/2510.26196)
*Li Wang,Yiyu Zhuang,Yanwen Wang,Xun Cao,Chuan Guo,Xinxin Zuo,Hao Zhu*

Main category: cs.CV

TL;DR: 提出了一种新的草图到3D人体姿态估计方法，该方法利用合成数据解决现有方法对大规模标注数据的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 现有的草图到姿态方法受限于缺乏大规模草图-3D姿态标注数据，主要依赖于启发式规则的优化，这种方法耗时且泛化能力有限。

Method: 1. 使用扩散模型从2D姿态合成草图图像，创建了一个包含12万个草图-3D姿态标注对的合成数据集SKEP-120K。2. 提出了一个端到端的数据驱动框架，该框架结合了现有的2D姿态检测器和生成扩散先验，以及前馈神经网络，用于高效的2D姿态估计。3. 引入了多个启发式损失函数，以保证3D姿态和检测到的2D姿态之间的几何一致性，同时保留准确的自我接触。

Result: 该模型在草图到姿态任务的估计准确性和速度方面均超过了以往的模型。

Conclusion: 该方法通过合成数据和端到端框架，有效解决了草图到3D人体姿态估计问题，并在准确性和速度上取得了显著提升。

Abstract: 3D human pose estimation from sketches has broad applications in computer
animation and film production. Unlike traditional human pose estimation, this
task presents unique challenges due to the abstract and disproportionate nature
of sketches. Previous sketch-to-pose methods, constrained by the lack of
large-scale sketch-3D pose annotations, primarily relied on optimization with
heuristic rules-an approach that is both time-consuming and limited in
generalizability. To address these challenges, we propose a novel approach
leveraging a "learn from synthesis" strategy. First, a diffusion model is
trained to synthesize sketch images from 2D poses projected from 3D human
poses, mimicking disproportionate human structures in sketches. This process
enables the creation of a synthetic dataset, SKEP-120K, consisting of 120k
accurate sketch-3D pose annotation pairs across various sketch styles. Building
on this synthetic dataset, we introduce an end-to-end data-driven framework for
estimating human poses and shapes from diverse sketch styles. Our framework
combines existing 2D pose detectors and generative diffusion priors for sketch
feature extraction with a feed-forward neural network for efficient 2D pose
estimation. Multiple heuristic loss functions are incorporated to guarantee
geometric coherence between the derived 3D poses and the detected 2D poses
while preserving accurate self-contacts. Qualitative, quantitative, and
subjective evaluations collectively show that our model substantially surpasses
previous ones in both estimation accuracy and speed for sketch-to-pose tasks.

</details>


### [78] [Developing a Multi-task Ensemble Geometric Deep Network for Supply Chain Sustainability and Risk Management](https://arxiv.org/abs/2510.26203)
*Mehdi Khaleghi,Nastaran Khaleghi,Sobhan Sheykhivand,Sebelan Danishvar*

Main category: cs.CV

TL;DR: 本文提出了一种新的几何深度网络，用于提高供应链的可持续性和效率。


<details>
  <summary>Details</summary>
Motivation: 供应链的可持续性对于控制供应链以实现最佳性能起着关键作用。供应链中发生的风险管理是发展网络可持续性和提高供应链绩效效率的根本问题。产品的正确分类是可持续供应链中的另一个基本要素。

Method: 使用一种新的几何深度网络来提出一种集成深度网络，所提出的 Chebyshev 集成几何网络 (Ch-EGN) 是一种混合卷积和几何深度学习。

Result: 在风险管理方面，集成网络的平均准确率为 98.95%。在可持续供应链方面，5 个产品组分类的平均准确率为 100%，4 个产品关系分类的平均准确率为 98.07%。25 个公司关系分类的平均准确率为 92.37%。

Conclusion: 结果证实，与最先进的方法相比，该方法具有平均改进和效率。

Abstract: The sustainability of supply chain plays a key role in achieving optimal
performance in controlling the supply chain. The management of risks that occur
in a supply chain is a fundamental problem for the purpose of developing the
sustainability of the network and elevating the performance efficiency of the
supply chain. The correct classification of products is another essential
element in a sustainable supply chain. Acknowledging recent breakthroughs in
the context of deep networks, several architectural options have been deployed
to analyze supply chain datasets. A novel geometric deep network is used to
propose an ensemble deep network. The proposed Chebyshev ensemble geometric
network (Ch-EGN) is a hybrid convolutional and geometric deep learning. This
network is proposed to leverage the information dependencies in supply chain to
derive invisible states of samples in the database. The functionality of the
proposed deep network is assessed on the two different databases. The
SupplyGraph Dataset and DataCo are considered in this research. The prediction
of delivery status of DataCo supply chain is done for risk administration. The
product classification and edge classification are performed using the
SupplyGraph database to enhance the sustainability of the supply network. An
average accuracy of 98.95% is obtained for the ensemble network for risk
management. The average accuracy of 100% and 98.07% are obtained for
sustainable supply chain in terms of 5 product group classification and 4
product relation classification, respectively. The average accuracy of 92.37%
is attained for 25 company relation classification. The results confirm an
average improvement and efficiency of the proposed method compared to the
state-of-the-art approaches.

</details>


### [79] [OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation](https://arxiv.org/abs/2510.26213)
*Hengrui Kang,Zhuangcheng Gu,Zhiyuan Zhao,Zichen Wen,Bin Wang,Weijia Li,Conghui He*

Main category: cs.CV

TL;DR: 本文介绍了一个新的大规模文档布局数据集OmniLayout-1M，并提出了一个新的文档布局生成模型OmniLayout-LLM，该模型在多个领域都取得了很好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有文档布局生成研究主要集中在学术论文等特定类型的文档上，缺乏对报纸、杂志等开放领域文档的探索。为了解决这个问题，本文构建了一个包含一百万个多样化文档布局的数据集OmniLayout-1M。

Method: 本文提出了一个两阶段的Coarse-to-Fine学习范式，首先从OmniLayout-1M数据集中学习通用的布局原则，然后将知识迁移到具有细粒度注释的特定领域。此外，还设计了一个0.5B的模型OmniLayout-LLM。

Result: 在M^6Doc数据集上的大量实验表明，本文提出的方法在多个领域都取得了很好的效果，显著超过了现有的布局生成专家和最新的通用LLM。

Conclusion: 本文构建了一个新的大规模文档布局数据集OmniLayout-1M，并提出了一个新的文档布局生成模型OmniLayout-LLM，为文档布局生成领域的研究提供了新的资源和方法。

Abstract: Document AI has advanced rapidly and is attracting increasing attention. Yet,
while most efforts have focused on document layout analysis (DLA), its
generative counterpart, document layout generation, remains underexplored. A
major obstacle lies in the scarcity of diverse layouts: academic papers with
Manhattan-style structures dominate existing studies, while open-world genres
such as newspapers and magazines remain severely underrepresented. To address
this gap, we curate OmniLayout-1M, the first million-scale dataset of diverse
document layouts, covering six common document types and comprising
contemporary layouts collected from multiple sources. Moreover, since existing
methods struggle in complex domains and often fail to arrange long sequences
coherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage
Coarse-to-Fine learning paradigm: 1) learning universal layout principles from
OmniLayout-1M with coarse category definitions, and 2) transferring the
knowledge to a specific domain with fine-grained annotations. Extensive
experiments demonstrate that our approach achieves strong performance on
multiple domains in M$^{6}$Doc dataset, substantially surpassing both existing
layout generation experts and several latest general-purpose LLMs. Our code,
models, and dataset will be publicly released.

</details>


### [80] [Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models](https://arxiv.org/abs/2510.26241)
*Shiho Matta,Lis Kanashiro Pereira,Peitao Han,Fei Cheng,Shigeru Kitazawa*

Main category: cs.CV

TL;DR: 当前视觉-语言模型在理解视频中的时间信息方面存在不足，尤其是在判断视频播放方向（时间箭头）方面。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在多模态任务中表现出色，但对视频时间信息的掌握仍然薄弱，且缺乏充分评估。本研究旨在探究这一差距。

Method: 引入AoT-PsyPhyBENCH，一个经过心理物理学验证的基准，用于测试视觉-语言模型在自然视频中推断时间方向的能力。该基准使用与人类相同的刺激和行为基线。

Result: 对开放权重和专有、推理和非推理视觉-语言模型的全面评估表明，大多数模型表现接近随机水平，即使是最好的模型也远落后于人类在物理不可逆过程和因果手动操作上的准确性。

Conclusion: 结果表明，当前的多模态系统缺乏时间连续性和因果理解所需的归纳偏见。发布AoT-PsyPhyBENCH的代码和数据，以促进视觉-语言模型在物理和时间推理能力方面的进一步发展。

Abstract: Modern vision-language models (VLMs) excel at many multimodal tasks, yet
their grasp of temporal information in video remains weak and, crucially,
under-evaluated. We probe this gap with a deceptively simple but revealing
challenge: judging the arrow of time (AoT)-whether a short clip is played
forward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated
benchmark that tests whether VLMs can infer temporal direction in natural
videos using the same stimuli and behavioral baselines established for humans.
Our comprehensive evaluation of open-weight and proprietary, reasoning and
non-reasoning VLMs reveals that most models perform near chance, and even the
best lag far behind human accuracy on physically irreversible processes (e.g.,
free fall, diffusion/explosion) and causal manual actions (division/addition)
that humans recognize almost instantly. These results highlight a fundamental
gap in current multimodal systems: while they capture rich visual-semantic
correlations, they lack the inductive biases required for temporal continuity
and causal understanding. We release the code and data for AoT-PsyPhyBENCH to
encourage further progress in the physical and temporal reasoning capabilities
of VLMs.

</details>


### [81] [Revisiting Generative Infrared and Visible Image Fusion Based on Human Cognitive Laws](https://arxiv.org/abs/2510.26268)
*Lin Guo,Xiaoqing Luo,Wei Xie,Zhancheng Zhang,Hui Li,Rui Wang,Zhenhua Feng,Xiaoning Song*

Main category: cs.CV

TL;DR: 提出了一种新颖的红外和可见光图像融合方法，称为HCLFuse，该方法通过多尺度掩码调节变分瓶颈编码器提取低级模态信息，并结合扩散模型的概率生成能力和物理定律，形成时变物理引导机制，从而提高模型感知数据内在结构的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的红外和可见光图像融合方法常常面临平衡模态信息的困境，生成式融合方法通过学习数据分布来重建融合图像，但其生成能力仍然有限，模态信息选择缺乏可解释性会影响融合结果的可靠性和一致性。

Method: 设计了一个多尺度掩码调节变分瓶颈编码器，应用后验概率建模和信息分解来提取准确和简洁的低级模态信息；将扩散模型的概率生成能力与物理定律相结合，形成时变物理引导机制，自适应地调节不同阶段的生成过程。

Result: 在多个数据集上的定性和定量评估中，该方法实现了最先进的融合性能，并显着提高了语义分割指标。

Conclusion: 该方法充分证明了这种从人类认知中汲取灵感的生成式图像融合方法在增强结构一致性和细节质量方面的优势。

Abstract: Existing infrared and visible image fusion methods often face the dilemma of
balancing modal information. Generative fusion methods reconstruct fused images
by learning from data distributions, but their generative capabilities remain
limited. Moreover, the lack of interpretability in modal information selection
further affects the reliability and consistency of fusion results in complex
scenarios. This manuscript revisits the essence of generative image fusion
under the inspiration of human cognitive laws and proposes a novel infrared and
visible image fusion method, termed HCLFuse. First, HCLFuse investigates the
quantification theory of information mapping in unsupervised fusion networks,
which leads to the design of a multi-scale mask-regulated variational
bottleneck encoder. This encoder applies posterior probability modeling and
information decomposition to extract accurate and concise low-level modal
information, thereby supporting the generation of high-fidelity structural
details. Furthermore, the probabilistic generative capability of the diffusion
model is integrated with physical laws, forming a time-varying physical
guidance mechanism that adaptively regulates the generation process at
different stages, thereby enhancing the ability of the model to perceive the
intrinsic structure of data and reducing dependence on data quality.
Experimental results show that the proposed method achieves state-of-the-art
fusion performance in qualitative and quantitative evaluations across multiple
datasets and significantly improves semantic segmentation metrics. This fully
demonstrates the advantages of this generative image fusion method, drawing
inspiration from human cognition, in enhancing structural consistency and
detail quality.

</details>


### [82] [Exploring Complementarity and Explainability in CNNs for Periocular Verification Across Acquisition Distances](https://arxiv.org/abs/2510.26282)
*Fernando Alonso-Fernandez,Kevin Hernandez Diaz,Jose M. Buades,Kiran Raja,Josef Bigun*

Main category: cs.CV

TL;DR: 研究了不同 CNN 在 UBIPr 数据库上不同距离的眼周验证中的互补性。


<details>
  <summary>Details</summary>
Motivation: 为了研究不同 CNN 结构在眼周验证任务中的互补性，并提高验证性能。

Method: 在 VGGFace2 数据集上训练了三种复杂度递增的 CNN 架构（SqueezeNet、MobileNetv2 和 ResNet50），使用 cosine 和 chi2 指标分析性能，比较不同的网络初始化，并通过逻辑回归应用 score-level 融合。此外，使用 LIME 热图和 Jensen-Shannon 散度来比较 CNN 的注意力模式。

Result: ResNet50 单独表现最佳，但融合提供了显著的增益，尤其是在组合所有三个网络时。热图显示，网络通常关注给定图像的不同区域，这解释了它们的互补性。该方法显著优于 UBIPr 上的先前工作，实现了新的技术水平。

Conclusion: 通过结合不同 CNN 架构，可以显著提高眼周验证的性能，并且这些网络关注图像的不同区域，从而实现了互补性。

Abstract: We study the complementarity of different CNNs for periocular verification at
different distances on the UBIPr database. We train three architectures of
increasing complexity (SqueezeNet, MobileNetv2, and ResNet50) on a large set of
eye crops from VGGFace2. We analyse performance with cosine and chi2 metrics,
compare different network initialisations, and apply score-level fusion via
logistic regression. In addition, we use LIME heatmaps and Jensen-Shannon
divergence to compare attention patterns of the CNNs. While ResNet50
consistently performs best individually, the fusion provides substantial gains,
especially when combining all three networks. Heatmaps show that networks
usually focus on distinct regions of a given image, which explains their
complementarity. Our method significantly outperforms previous works on UBIPr,
achieving a new state-of-the-art.

</details>


### [83] [Beyond Imitation: Constraint-Aware Trajectory Generation with Flow Matching For End-to-End Autonomous Driving](https://arxiv.org/abs/2510.26292)
*Lin Liu,Guanyi Yu,Ziying Song,Junqiao Li,Caiyan Jia,Feiyang Jia,Peiliang Wu,Yandan Luo*

Main category: cs.CV

TL;DR: 提出了一种新的规划框架CATG，它利用约束流匹配来解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的模仿学习方法经常遭受模式崩溃，无法产生不同的轨迹假设。同时，现有的生成方法难以将关键的安全和物理约束直接纳入生成过程，需要额外的优化阶段来改进其输出。

Method: CATG显式地模拟了流匹配过程，这自然地减轻了模式崩溃，并允许来自各种条件信号的灵活指导。在流匹配过程中直接施加显式约束，确保生成的轨迹符合重要的安全和运动学规则。将驾驶积极性参数化为生成过程中的控制信号，从而能够精确地控制轨迹样式。

Result: 在NavSim v2挑战赛中，CATG获得了第二名，EPDMS评分为51.31，并荣获创新奖。

Conclusion: CATG通过在流匹配过程中直接施加显式约束，并参数化驾驶积极性作为控制信号，实现了更好的轨迹生成效果。

Abstract: Planning is a critical component of end-to-end autonomous driving. However,
prevailing imitation learning methods often suffer from mode collapse, failing
to produce diverse trajectory hypotheses. Meanwhile, existing generative
approaches struggle to incorporate crucial safety and physical constraints
directly into the generative process, necessitating an additional optimization
stage to refine their outputs. To address these limitations, we propose CATG, a
novel planning framework that leverages Constrained Flow Matching. Concretely,
CATG explicitly models the flow matching process, which inherently mitigates
mode collapse and allows for flexible guidance from various conditioning
signals. Our primary contribution is the novel imposition of explicit
constraints directly within the flow matching process, ensuring that the
generated trajectories adhere to vital safety and kinematic rules. Secondly,
CATG parameterizes driving aggressiveness as a control signal during
generation, enabling precise manipulation of trajectory style. Notably, on the
NavSim v2 challenge, CATG achieved 2nd place with an EPDMS score of 51.31 and
was honored with the Innovation Award.

</details>


### [84] [Leveraging Large-Scale Face Datasets for Deep Periocular Recognition via Ocular Cropping](https://arxiv.org/abs/2510.26294)
*Fernando Alonso-Fernandez,Kevin Hernandez-Diaz,Jose Maria Buades Rubio,Josef Bigun*

Main category: cs.CV

TL;DR: 本文研究了眼周生物识别技术，利用大规模VGGFace2数据库训练卷积神经网络，并在VGGFace2-Pose和UFPR-Periocular数据库上进行评估。


<details>
  <summary>Details</summary>
Motivation: 眼周区域具有高区分度和低采集约束，因此本文关注眼周生物识别。

Method: 本文评估了三种不同深度和复杂度的卷积神经网络结构在眼周识别中的有效性，使用从大规模VGGFace2数据库中提取的1,907,572个眼周图像进行训练。

Result: 在VGGFace2数据库上，眼周图像的EER为9-15%，高于全脸图像的3-6%。在UFPR-Periocular数据库上，EER为1-2%。

Conclusion: 在UFPR-Periocular数据集上取得了最低的EER。

Abstract: We focus on ocular biometrics, specifically the periocular region (the area
around the eye), which offers high discrimination and minimal acquisition
constraints. We evaluate three Convolutional Neural Network architectures of
varying depth and complexity to assess their effectiveness for periocular
recognition. The networks are trained on 1,907,572 ocular crops extracted from
the large-scale VGGFace2 database. This significantly contrasts with existing
works, which typically rely on small-scale periocular datasets for training
having only a few thousand images. Experiments are conducted with ocular images
from VGGFace2-Pose, a subset of VGGFace2 containing in-the-wild face images,
and the UFPR-Periocular database, which consists of selfies captured via mobile
devices with user guidance on the screen. Due to the uncontrolled conditions of
VGGFace2, the Equal Error Rates (EERs) obtained with ocular crops range from
9-15%, noticeably higher than the 3-6% EERs achieved using full-face images. In
contrast, UFPR-Periocular yields significantly better performance (EERs of
1-2%), thanks to higher image quality and more consistent acquisition
protocols. To the best of our knowledge, these are the lowest reported EERs on
the UFPR dataset to date.

</details>


### [85] [Towards Realistic Earth-Observation Constellation Scheduling: Benchmark and Methodology](https://arxiv.org/abs/2510.26297)
*Luting Wang,Yinghao Xiang,Hongliang Huang,Dongjun Li,Chen Gao,Si Liu*

Main category: cs.CV

TL;DR: 本文提出了一个用于敏捷地球观测卫星（AEOS）星座调度的新框架，包括一个基准测试套件（AEOS-Bench）和一个Transformer架构的调度模型（AEOS-Former）。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常简化了大规模场景、动态环境和严格约束下的AEOS星座调度复杂性，限制了它们的实际性能。

Method: 该框架集成了标准化的基准测试套件和一个新的调度模型。AEOS-Bench包含3,907个卫星资产和16,410个场景，这些场景通过高保真仿真平台生成。AEOS-Former是一个基于Transformer的调度模型，它结合了约束感知注意力机制，并通过基于仿真的迭代学习适应不同的场景。

Result: 实验结果表明，AEOS-Former在任务完成和能源效率方面优于基线模型，并且通过消融研究突出了每个组件的贡献。

Conclusion: AEOS-Former为AEOS星座调度提供了一个稳健的解决方案。

Abstract: Agile Earth Observation Satellites (AEOSs) constellations offer unprecedented
flexibility for monitoring the Earth's surface, but their scheduling remains
challenging under large-scale scenarios, dynamic environments, and stringent
constraints. Existing methods often simplify these complexities, limiting their
real-world performance. We address this gap with a unified framework
integrating a standardized benchmark suite and a novel scheduling model. Our
benchmark suite, AEOS-Bench, contains $3,907$ finely tuned satellite assets and
$16,410$ scenarios. Each scenario features $1$ to $50$ satellites and $50$ to
$300$ imaging tasks. These scenarios are generated via a high-fidelity
simulation platform, ensuring realistic satellite behavior such as orbital
dynamics and resource constraints. Ground truth scheduling annotations are
provided for each scenario. To our knowledge, AEOS-Bench is the first
large-scale benchmark suite tailored for realistic constellation scheduling.
Building upon this benchmark, we introduce AEOS-Former, a Transformer-based
scheduling model that incorporates a constraint-aware attention mechanism. A
dedicated internal constraint module explicitly models the physical and
operational limits of each satellite. Through simulation-based iterative
learning, AEOS-Former adapts to diverse scenarios, offering a robust solution
for AEOS constellation scheduling. Experimental results demonstrate that
AEOS-Former outperforms baseline models in task completion and energy
efficiency, with ablation studies highlighting the contribution of each
component. Code and data are provided in
https://github.com/buaa-colalab/AEOSBench.

</details>


### [86] [Exploring the correlation between the type of music and the emotions evoked: A study using subjective questionnaires and EEG](https://arxiv.org/abs/2510.26304)
*Jelizaveta Jankowska,Bożena Kostek,Fernando Alonso-Fernandez,Prayag Tiwari*

Main category: cs.CV

TL;DR: 本研究旨在检验不同类型的音乐如何影响人类的情绪。


<details>
  <summary>Details</summary>
Motivation: 旨在展示不同音乐流派对情绪的影响。

Method: 使用脑电图头盔进行主观调查和脑部活动测量。

Result: 分析揭示了情绪和观察到的脑部活动之间的联系。

Conclusion: 不同类型的音乐会对人类情绪产生影响，情绪和脑部活动之间存在联系。

Abstract: The subject of this work is to check how different types of music affect
human emotions. While listening to music, a subjective survey and brain
activity measurements were carried out using an EEG helmet. The aim is to
demonstrate the impact of different music genres on emotions. The research
involved a diverse group of participants of different gender and musical
preferences. This had the effect of capturing a wide range of emotional
responses to music. After the experiment, a relationship analysis of the
respondents' questionnaires with EEG signals was performed. The analysis
revealed connections between emotions and observed brain activity.

</details>


### [87] [A Hybrid Framework Bridging CNN and ViT based on Theory of Evidence for Diabetic Retinopathy Grading](https://arxiv.org/abs/2510.26315)
*Junlai Qiu,Yunzhu Chen,Hao Zheng,Yawen Huang,Yuexiang Li*

Main category: cs.CV

TL;DR: 提出了一种新的基于证据理论的融合范式，以有效地融合不同骨干网络提取的特征，用于糖尿病视网膜病变(DR)分级。


<details>
  <summary>Details</summary>
Motivation: 现有使用单一类型骨干网络的DR诊断系统性能达到瓶颈，为了充分利用CNN和ViT各自的优势（即CNN的局部特征提取能力和ViT的全局特征捕获能力），需要整合不同类型的骨干网络。

Method: 通过一组深度证据网络将来自不同骨干网络的特征转化为支持证据。利用支持证据，可以形成聚合意见，用于自适应地调整不同骨干网络之间的融合模式。

Result: 在两个公开的DR分级数据集上的实验结果表明，该混合模型不仅提高了DR分级的准确性，而且为特征融合和决策提供了出色的可解释性。

Conclusion: 该研究提出了一种新的混合模型，通过证据理论融合CNN和ViT的特征，提高了DR分级的准确性和可解释性。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss among middle-aged
and elderly people, which significantly impacts their daily lives and mental
health. To improve the efficiency of clinical screening and enable the early
detection of DR, a variety of automated DR diagnosis systems have been recently
established based on convolutional neural network (CNN) or vision Transformer
(ViT). However, due to the own shortages of CNN / ViT, the performance of
existing methods using single-type backbone has reached a bottleneck. One
potential way for the further improvements is integrating different kinds of
backbones, which can fully leverage the respective strengths of them
(\emph{i.e.,} the local feature extraction capability of CNN and the global
feature capturing ability of ViT). To this end, we propose a novel paradigm to
effectively fuse the features extracted by different backbones based on the
theory of evidence. Specifically, the proposed evidential fusion paradigm
transforms the features from different backbones into supporting evidences via
a set of deep evidential networks. With the supporting evidences, the
aggregated opinion can be accordingly formed, which can be used to adaptively
tune the fusion pattern between different backbones and accordingly boost the
performance of our hybrid model. We evaluated our method on two publicly
available DR grading datasets. The experimental results demonstrate that our
hybrid model not only improves the accuracy of DR grading, compared to the
state-of-the-art frameworks, but also provides the excellent interpretability
for feature fusion and decision-making.

</details>


### [88] [GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?](https://arxiv.org/abs/2510.26339)
*Mingyu Sung,Seungjae Ham,Kangwoo Kim,Yeokyoung Yoon,Sangseok Yun,Il-Min Kim,Jae-Mo Kang*

Main category: cs.CV

TL;DR: 提出了GLYPH-SR，一个视觉-语言引导的扩散框架，旨在同时优化文本可读性和感知质量。


<details>
  <summary>Details</summary>
Motivation: 之前的SR研究通常针对失真或学习的感知指标进行调整，这些指标对字符级错误 largely insensitive。虽然有些研究解决了文本SR问题，但通常侧重于具有孤立字符的简化基准，忽略了复杂自然场景中文本的挑战。因此，场景文本被有效地视为通用纹理。

Method: GLYPH-SR利用由OCR数据引导的文本-SR融合ControlNet(TS-ControlNet)，以及在以文本为中心和以场景为中心的指导之间交替的乒乓调度器。为了实现有针对性的文本恢复，我们在合成语料库上训练这些组件，同时保持主SR分支冻结。

Result: 在SVT、SCUT-CTW1500和CUTE80上，GLYPH-SR在x4和x8时，比扩散/GAN基线(SVT x8, OpenOCR)提高了高达+15.18个百分点的OCR F1，同时保持了具有竞争力的MANIQA、CLIP-IQA和MUSIQ。

Conclusion: GLYPH-SR旨在同时满足高可读性和高视觉真实感这两个目标，从而提供看起来正确且读取正确的SR。

Abstract: Image super-resolution(SR) is fundamental to many vision system-from
surveillance and autonomy to document analysis and retail analytics-because
recovering high-frequency details, especially scene-text, enables reliable
downstream perception. Scene-text, i.e., text embedded in natural images such
as signs, product labels, and storefronts, often carries the most actionable
information; when characters are blurred or hallucinated, optical character
recognition(OCR) and subsequent decisions fail even if the rest of the image
appears sharp. Yet previous SR research has often been tuned to distortion
(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that
are largely insensitive to character-level errors. Furthermore, studies that do
address text SR often focus on simplified benchmarks with isolated characters,
overlooking the challenges of text within complex natural scenes. As a result,
scene-text is effectively treated as generic texture. For SR to be effective in
practical deployments, it is therefore essential to explicitly optimize for
both text legibility and perceptual quality. We present GLYPH-SR, a
vision-language-guided diffusion framework that aims to achieve both objectives
jointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by
OCR data, and a ping-pong scheduler that alternates between text- and
scene-centric guidance. To enable targeted text restoration, we train these
components on a synthetic corpus while keeping the main SR branch frozen.
Across SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by
up to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)
while maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed
to satisfy both objectives simultaneously-high readability and high visual
realism-delivering SR that looks right and reds right.

</details>


### [89] [EEG-Driven Image Reconstruction with Saliency-Guided Diffusion Models](https://arxiv.org/abs/2510.26391)
*Igor Abramov,Ilya Makarov*

Main category: cs.CV

TL;DR: 提出了一种双重条件框架，结合脑电图嵌入和空间显着性图来增强图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有的脑电图驱动的图像重建方法通常忽略空间注意力机制，限制了保真度和语义连贯性。

Method: 利用自适应思维映射器（ATM）进行脑电图特征提取，并通过低秩自适应（LoRA）微调稳定扩散2.1，以将神经信号与视觉语义对齐，而ControlNet分支则以显着性图为条件进行空间控制生成。

Result: 在THINGS-EEG上进行评估，该方法在低级和高级图像特征的质量上均优于现有方法，同时与人类视觉注意力强烈对齐。

Conclusion: 注意力先验可以解决脑电图的模糊性，从而实现高保真重建，并在医学诊断和神经适应性界面中具有应用，通过有效适应预训练的扩散模型来推进神经解码。

Abstract: Existing EEG-driven image reconstruction methods often overlook spatial
attention mechanisms, limiting fidelity and semantic coherence. To address
this, we propose a dual-conditioning framework that combines EEG embeddings
with spatial saliency maps to enhance image generation. Our approach leverages
the Adaptive Thinking Mapper (ATM) for EEG feature extraction and fine-tunes
Stable Diffusion 2.1 via Low-Rank Adaptation (LoRA) to align neural signals
with visual semantics, while a ControlNet branch conditions generation on
saliency maps for spatial control. Evaluated on THINGS-EEG, our method achieves
a significant improvement in the quality of low- and high-level image features
over existing approaches. Simultaneously, strongly aligning with human visual
attention. The results demonstrate that attentional priors resolve EEG
ambiguities, enabling high-fidelity reconstructions with applications in
medical diagnostics and neuroadaptive interfaces, advancing neural decoding
through efficient adaptation of pre-trained diffusion models.

</details>


### [90] [LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation](https://arxiv.org/abs/2510.26412)
*Xiangqing Zheng,Chengyue Wu,Kehai Chen,Min Zhang*

Main category: cs.CV

TL;DR: LoCoT2V-Bench是一个用于评估长视频生成的基准，它侧重于复杂提示下的细粒度对齐和叙事连贯性等抽象维度。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估长视频生成方面存在不足，尤其是在处理复杂提示时，它们主要依赖于简化提示和低级指标，忽略了与提示的细粒度对齐以及叙事连贯性和主题表达等抽象维度。

Method: 提出了LoCoT2V-Bench基准，该基准基于各种真实世界的视频，引入了一套现实和复杂的提示，并构建了一个多维评估框架，包括事件级别对齐、细粒度时间一致性、内容清晰度以及人类期望实现度（HERD）等新指标。

Result: 对九种代表性的长视频生成模型进行了全面评估，发现现有方法在基本的视觉和时间方面表现良好，但在事件间一致性、细粒度对齐和高级主题遵循等方面存在困难。

Conclusion: LoCoT2V-Bench为评估长视频生成提供了一个全面可靠的平台，并强调了未来方法改进的关键方向。

Abstract: Recently text-to-video generation has made impressive progress in producing
short, high-quality clips, but evaluating long-form outputs remains a major
challenge especially when processing complex prompts. Existing benchmarks
mostly rely on simplified prompts and focus on low-level metrics, overlooking
fine-grained alignment with prompts and abstract dimensions such as narrative
coherence and thematic expression. To address these gaps, we propose
LoCoT2V-Bench, a benchmark specifically designed for long video generation
(LVG) under complex input conditions. Based on various real-world videos,
LoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating
elements like scene transitions and event dynamics. Moreover, it constructs a
multi-dimensional evaluation framework that includes our newly proposed metrics
such as event-level alignment, fine-grained temporal consistency, content
clarity, and the Human Expectation Realization Degree (HERD) that focuses on
more abstract attributes like narrative flow, emotional response, and character
development. Using this framework, we conduct a comprehensive evaluation of
nine representative LVG models, finding that while current methods perform well
on basic visual and temporal aspects, they struggle with inter-event
consistency, fine-grained alignment, and high-level thematic adherence, etc.
Overall, LoCoT2V-Bench provides a comprehensive and reliable platform for
evaluating long-form complex text-to-video generation and highlights critical
directions for future method improvement.

</details>


### [91] [A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models](https://arxiv.org/abs/2510.26441)
*Shihab Aaqil Ahamed,Udaya S. K. P. Miriya Thanthrige,Ranga Rodrigo,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 本文提出了一种新的测试时提示调整框架A-TPT，通过引入角度多样性来提高视觉语言模型(VLM)的校准性能。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时提示调整(TPT)方法在提高提示校准方面可能无法在类间文本特征之间实现最佳的角度分离，忽略了角度多样性的关键作用，从而损害了VLMs的可靠性、可信度和安全性。

Method: 该方法通过最大化单位超球面上特征之间的最小成对角度距离，鼓励由相应可学习提示引起的归一化文本特征分布的均匀性。

Result: 实验结果表明，该方法在减少平均校准误差的同时，保持了与最先进的TPT方法相当的精度，并在自然分布偏移和医学数据集上表现出卓越的零样本校准性能。

Conclusion: 该研究表明，促进角度多样性可以实现良好分散的文本特征，从而显著提高测试时调整期间的VLM校准。

Abstract: Test-time prompt tuning (TPT) has emerged as a promising technique for
adapting large vision-language models (VLMs) to unseen tasks without relying on
labeled data. However, the lack of dispersion between textual features can hurt
calibration performance, which raises concerns about VLMs' reliability,
trustworthiness, and safety. Current TPT approaches primarily focus on
improving prompt calibration by either maximizing average textual feature
dispersion or enforcing orthogonality constraints to encourage angular
separation. However, these methods may not always have optimal angular
separation between class-wise textual features, which implies overlooking the
critical role of angular diversity. To address this, we propose A-TPT, a novel
TPT framework that introduces angular diversity to encourage uniformity in the
distribution of normalized textual features induced by corresponding learnable
prompts. This uniformity is achieved by maximizing the minimum pairwise angular
distance between features on the unit hypersphere. We show that our approach
consistently surpasses state-of-the-art TPT methods in reducing the aggregate
average calibration error while maintaining comparable accuracy through
extensive experiments with various backbones on different datasets. Notably,
our approach exhibits superior zero-shot calibration performance on natural
distribution shifts and generalizes well to medical datasets. We provide
extensive analyses, including theoretical aspects, to establish the grounding
of A-TPT. These results highlight the potency of promoting angular diversity to
achieve well-dispersed textual features, significantly improving VLM
calibration during test-time adaptation. Our code will be made publicly
available.

</details>


### [92] [PointSt3R: Point Tracking through 3D Grounded Correspondence](https://arxiv.org/abs/2510.26443)
*Rhodri Guerrier,Adam W. Harley,Dima Damen*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D重建模型的点追踪方法，通过结合重建损失和动态对应训练，以及可见性头部，并在少量合成数据上微调MASt3R，实现了在多个数据集上具有竞争力或更优越的点追踪结果。


<details>
  <summary>Details</summary>
Motivation: 利用现有3D重建模型（如DUSt3R和MASt3R）在静态场景中的2D和3D对应方面的潜力，并将其应用于点追踪任务。

Method: 结合重建损失和动态对应训练，添加可见性头部，使用少量合成数据微调MASt3R，仅在包含查询点的帧对上进行训练和评估，去除时间上下文。

Result: 在四个数据集上实现了有竞争力或更优越的点追踪结果，并在3D点追踪上进行了实验，同时对训练数据集和动态对应百分比进行了多次消融实验。

Conclusion: 通过将3D重建模型应用于点追踪任务，并结合动态对应训练和可见性头部，实现了有效的点追踪性能。

Abstract: Recent advances in foundational 3D reconstruction models, such as DUSt3R and
MASt3R, have shown great potential in 2D and 3D correspondence in static
scenes. In this paper, we propose to adapt them for the task of point tracking
through 3D grounded correspondence. We first demonstrate that these models are
competitive point trackers when focusing on static points, present in current
point tracking benchmarks ($+33.5\%$ on EgoPoints vs. CoTracker2). We propose
to combine the reconstruction loss with training for dynamic correspondence
along with a visibility head, and fine-tuning MASt3R for point tracking using a
relatively small amount of synthetic data. Importantly, we only train and
evaluate on pairs of frames where one contains the query point, effectively
removing any temporal context. Using a mix of dynamic and static point
correspondences, we achieve competitive or superior point tracking results on
four datasets (e.g. competitive on TAP-Vid-DAVIS 73.8 $\delta_{avg}$ / 85.8\%
occlusion acc. for PointSt3R compared to 75.7 / 88.3\% for CoTracker2; and
significantly outperform CoTracker3 on EgoPoints 61.3 vs 54.2 and RGB-S 87.0 vs
82.8). We also present results on 3D point tracking along with several
ablations on training datasets and percentage of dynamic correspondences.

</details>


### [93] [Towards Fine-Grained Vision-Language Alignment for Few-Shot Anomaly Detection](https://arxiv.org/abs/2510.26464)
*Yuanting Fan,Jun Liu,Xiaochen Chen,Bin-Bin Gao,Jian Li,Yong Liu,Jinlong Peng,Chengjie Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为FineGrainedAD的新框架，以提高小样本异常检测的定位性能。该框架通过多层次可学习提示和多层次语义对齐来解决图像描述和patch级别视觉异常之间的语义不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于预训练的视觉-语言模型 (VLMs) 的泛化能力，通过文本描述和图像之间的特征相似性来识别潜在的异常区域。然而，由于缺乏详细的文本描述，这些方法只能预定义图像级别的描述来匹配每个视觉patch token，导致图像描述和patch级别视觉异常之间的语义不对齐，从而导致次优的定位性能。

Method: 本文提出了多层次细粒度语义标题 (MFSC)，通过自动构建pipeline为现有的异常检测数据集提供多层次和细粒度的文本描述。在此基础上，提出了一个名为FineGrainedAD的新框架，该框架由多层次可学习提示 (MLLP) 和多层次语义对齐 (MLSA) 两个组件组成。

Result: 实验表明，在MVTec-AD和VisA数据集上，所提出的FineGrainedAD在小样本设置中取得了优越的整体性能。

Conclusion: 本文提出的FineGrainedAD框架通过多层次细粒度语义描述和多层次对齐策略，有效提升了小样本异常检测的定位性能。

Abstract: Few-shot anomaly detection (FSAD) methods identify anomalous regions with few
known normal samples. Most existing methods rely on the generalization ability
of pre-trained vision-language models (VLMs) to recognize potentially anomalous
regions through feature similarity between text descriptions and images.
However, due to the lack of detailed textual descriptions, these methods can
only pre-define image-level descriptions to match each visual patch token to
identify potential anomalous regions, which leads to the semantic misalignment
between image descriptions and patch-level visual anomalies, achieving
sub-optimal localization performance. To address the above issues, we propose
the Multi-Level Fine-Grained Semantic Caption (MFSC) to provide multi-level and
fine-grained textual descriptions for existing anomaly detection datasets with
automatic construction pipeline. Based on the MFSC, we propose a novel
framework named FineGrainedAD to improve anomaly localization performance,
which consists of two components: Multi-Level Learnable Prompt (MLLP) and
Multi-Level Semantic Alignment (MLSA). MLLP introduces fine-grained semantics
into multi-level learnable prompts through automatic replacement and
concatenation mechanism, while MLSA designs region aggregation strategy and
multi-level alignment training to facilitate learnable prompts better align
with corresponding visual regions. Experiments demonstrate that the proposed
FineGrainedAD achieves superior overall performance in few-shot settings on
MVTec-AD and VisA datasets.

</details>


### [94] [Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition](https://arxiv.org/abs/2510.26466)
*Pei Peng,MingKun Xie,Hang Hao,Tong Jin,ShengJun Huang*

Main category: cs.CV

TL;DR: 视觉语言模型中的对象上下文快捷方式是一个挑战，当测试场景与训练场景不同时，会削弱零样本的可靠性。本文将其重新定义为一个因果推理问题。


<details>
  <summary>Details</summary>
Motivation: 当测试场景与训练场景不同时，视觉语言模型中的对象上下文快捷方式会削弱零样本的可靠性。

Method: 在推理时，估计CLIP表示空间中的对象和背景期望，并通过将对象特征与来自外部数据集、批邻居或文本描述的不同替代上下文重新组合来合成反事实嵌入。通过估计总直接效应并模拟干预，进一步减去仅背景激活，从而保留有益的对象-上下文交互，同时减轻幻觉分数。

Result: 无需重新训练或提示设计，该方法在上下文敏感的基准测试中显著提高了最差组和平均准确率，从而建立了新的零样本最先进水平。

Conclusion: 该框架提供了一种轻量级的表示级别反事实方法，为去偏和可靠的多模态推理提供了一种实用的因果途径。

Abstract: Object-context shortcuts remain a persistent challenge in vision-language
models, undermining zero-shot reliability when test-time scenes differ from
familiar training co-occurrences. We recast this issue as a causal inference
problem and ask: Would the prediction remain if the object appeared in a
different environment? To answer this at inference time, we estimate object and
background expectations within CLIP's representation space, and synthesize
counterfactual embeddings by recombining object features with diverse
alternative contexts sampled from external datasets, batch neighbors, or
text-derived descriptions. By estimating the Total Direct Effect and simulating
intervention, we further subtract background-only activation, preserving
beneficial object-context interactions while mitigating hallucinated scores.
Without retraining or prompt design, our method substantially improves both
worst-group and average accuracy on context-sensitive benchmarks, establishing
a new zero-shot state of the art. Beyond performance, our framework provides a
lightweight representation-level counterfactual approach, offering a practical
causal avenue for debiased and reliable multimodal reasoning.

</details>


### [95] [Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing](https://arxiv.org/abs/2510.26474)
*Xin Guo,Zhiheng Xi,Yiwen Ding,Yitao Zhai,Xiaowei Shi,Xunliang Cai,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CV

TL;DR: 模型在处理简单查询时表现出色，但在复杂查询时遇到困难，导致优化不平衡。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在自提升过程中，模型在处理简单查询时生成高质量轨迹，但在复杂查询时遇到困难。这种不平衡优化导致模型优先考虑简单的推理技能，阻碍了其处理复杂推理任务的能力。

Method: 提出了四种有效的策略，从分布重塑和轨迹重采样两个角度，在探索和学习的自我完善过程中实现head-tail重新平衡。

Result: 在Qwen2-VL-7B-Instruct和InternVL2.5-4B模型上进行的广泛实验表明，该方法持续提高视觉推理能力，平均优于vanilla self-improvement 3.86个点。

Conclusion: 通过提出的head-tail重新平衡策略，可以有效提高视觉推理能力。

Abstract: Self-improvement has emerged as a mainstream paradigm for advancing the
reasoning capabilities of large vision-language models (LVLMs), where models
explore and learn from successful trajectories iteratively. However, we
identify a critical issue during this process: the model excels at generating
high-quality trajectories for simple queries (i.e., head data) but struggles
with more complex ones (i.e., tail data). This leads to an imbalanced
optimization that drives the model to prioritize simple reasoning skills, while
hindering its ability to tackle more complex reasoning tasks. Over iterations,
this imbalance becomes increasingly pronounced--a dynamic we term the "Matthew
effect"--which ultimately hinders further model improvement and leads to
performance bottlenecks. To counteract this challenge, we introduce four
efficient strategies from two perspectives: distribution-reshaping and
trajectory-resampling, to achieve head-tail re-balancing during the
exploration-and-learning self-improvement process. Extensive experiments on
Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks
demonstrate that our methods consistently improve visual reasoning
capabilities, outperforming vanilla self-improvement by 3.86 points on average.

</details>


### [96] [Analysis of the Robustness of an Edge Detector Based on Cellular Automata Optimized by Particle Swarm](https://arxiv.org/abs/2510.26509)
*Vinícius Ferraria,Eurico Ruivo*

Main category: cs.CV

TL;DR: 本文提出了一种自适应边缘检测器，该检测器通过二维细胞自动机描述，并通过元启发式算法与迁移学习技术相结合进行优化。


<details>
  <summary>Details</summary>
Motivation: 现有边缘检测器在检测松散边缘方面存在困难，并且缺乏从特定问题中提取相关信息的能力。

Method: 开发了一个自适应检测器，该检测器通过二维细胞自动机描述，并通过元启发式算法与迁移学习技术相结合进行优化。分析了扩大优化阶段搜索空间的影响，以及检测器在识别一组自然图像和从同一图像集中提取的专业子集边缘方面的适应性。

Result: 扩大优化阶段的搜索空间对于所选图像集无效。该模型能够适应输入，并且应用于该模型的迁移学习技术没有显示出显着改进。

Conclusion: 该模型具有一定的适应性，但扩大搜索空间和应用迁移学习技术没有显著提升性能。

Abstract: The edge detection task is essential in image processing aiming to extract
relevant information from an image. One recurring problem in this task is the
weaknesses found in some detectors, such as the difficulty in detecting loose
edges and the lack of context to extract relevant information from specific
problems. To address these weaknesses and adapt the detector to the properties
of an image, an adaptable detector described by two-dimensional cellular
automaton and optimized by meta-heuristic combined with transfer learning
techniques was developed. This study aims to analyze the impact of expanding
the search space of the optimization phase and the robustness of the
adaptability of the detector in identifying edges of a set of natural images
and specialized subsets extracted from the same image set. The results obtained
prove that expanding the search space of the optimization phase was not
effective for the chosen image set. The study also analyzed the adaptability of
the model through a series of experiments and validation techniques and found
that, regardless of the validation, the model was able to adapt to the input
and the transfer learning techniques applied to the model showed no significant
improvements.

</details>


### [97] [DINO-YOLO: Self-Supervised Pre-training for Data-Efficient Object Detection in Civil Engineering Applications](https://arxiv.org/abs/2510.25140)
*Malaisree P,Youwai S,Kitkobsin T,Janrungautai S,Amorndechaphon D,Rojanavasu P*

Main category: cs.CV

TL;DR: DINO-YOLO结合了YOLOv12和DINOv3，以提高数据效率。


<details>
  <summary>Details</summary>
Motivation: 土木工程应用中的目标检测受到专业领域中有限的带注释数据的限制。

Method: 在输入预处理（P0）和mid-backbone增强（P3）两个位置策略性地整合DINOv3特征。

Result: 在隧道裂缝检测、建筑PPE和KITTI数据集上取得了显著的改进，同时保持了实时推理速度。

Conclusion: DINO-YOLO在土木工程数据集上建立了最先进的性能，同时保持了计算效率，为数据受限环境中的施工安全监控和基础设施检查提供了实用的解决方案。

Abstract: Object detection in civil engineering applications is constrained by limited
annotated data in specialized domains. We introduce DINO-YOLO, a hybrid
architecture combining YOLOv12 with DINOv3 self-supervised vision transformers
for data-efficient detection. DINOv3 features are strategically integrated at
two locations: input preprocessing (P0) and mid-backbone enhancement (P3).
Experimental validation demonstrates substantial improvements: Tunnel Segment
Crack detection (648 images) achieves 12.4% improvement, Construction PPE (1K
images) gains 13.7%, and KITTI (7K images) shows 88.6% improvement, while
maintaining real-time inference (30-47 FPS). Systematic ablation across five
YOLO scales and nine DINOv3 variants reveals that Medium-scale architectures
achieve optimal performance with DualP0P3 integration (55.77% mAP@0.5), while
Small-scale requires Triple Integration (53.63%). The 2-4x inference overhead
(21-33ms versus 8-16ms baseline) remains acceptable for field deployment on
NVIDIA RTX 5090. DINO-YOLO establishes state-of-the-art performance for civil
engineering datasets (<10K images) while preserving computational efficiency,
providing practical solutions for construction safety monitoring and
infrastructure inspection in data-constrained environments.

</details>


### [98] [SA$^{2}$Net: Scale-Adaptive Structure-Affinity Transformation for Spine Segmentation from Ultrasound Volume Projection Imaging](https://arxiv.org/abs/2510.26568)
*Hao Xie,Zixun Huang,Yushen Zuo,Yakun Ju,Frank H. F. Leung,N. F. Law,Kin-Man Lam,Yong-Ping Zheng,Sai Ho Ling*

Main category: cs.CV

TL;DR: 提出了一种新的 scale-adaptive structure-aware 网络 (SA$^{2}$Net) 用于有效的脊柱分割。


<details>
  <summary>Details</summary>
Motivation: 基于超声体积投影成像 (VPI) 的脊柱分割在临床应用中对智能脊柱侧弯诊断起着至关重要的作用。然而，这项任务面临着几个严峻的挑战：如果我们忽略不同骨骼特征的高度空间相关性，可能无法很好地学习脊柱的全局上下文知识；脊柱骨骼包含关于其形状和位置的丰富结构知识，值得将其编码到分割过程中。

Method: 提出了一个 scale-adaptive 互补策略来学习脊柱图像的跨维度长距离相关特征；提出了结构亲和力转换，以利用 Transformers 中多头自注意力与语义级别亲和力之间的一致性，用类特定的亲和力转换语义特征，并将其与 Transformer 解码器结合，以实现结构感知推理；采用了一种特征混合损失聚合方法来增强模型训练。

Result: SA$^{2}$Net 实现了优于其他 state-of-the-art 方法的分割性能。

Conclusion: SA$^{2}$Net 对各种主干网络的适应性增强了其作为使用智能脊柱图像分析进行高级脊柱侧弯诊断的有前途工具的潜力。

Abstract: Spine segmentation, based on ultrasound volume projection imaging (VPI),
plays a vital role for intelligent scoliosis diagnosis in clinical
applications. However, this task faces several significant challenges. Firstly,
the global contextual knowledge of spines may not be well-learned if we neglect
the high spatial correlation of different bone features. Secondly, the spine
bones contain rich structural knowledge regarding their shapes and positions,
which deserves to be encoded into the segmentation process. To address these
challenges, we propose a novel scale-adaptive structure-aware network
(SA$^{2}$Net) for effective spine segmentation. First, we propose a
scale-adaptive complementary strategy to learn the cross-dimensional
long-distance correlation features for spinal images. Second, motivated by the
consistency between multi-head self-attention in Transformers and semantic
level affinity, we propose structure-affinity transformation to transform
semantic features with class-specific affinity and combine it with a
Transformer decoder for structure-aware reasoning. In addition, we adopt a
feature mixing loss aggregation method to enhance model training. This method
improves the robustness and accuracy of the segmentation process. The
experimental results demonstrate that our SA$^{2}$Net achieves superior
segmentation performance compared to other state-of-the-art methods. Moreover,
the adaptability of SA$^{2}$Net to various backbones enhances its potential as
a promising tool for advanced scoliosis diagnosis using intelligent spinal
image analysis. The code and experimental demo are available at
https://github.com/taetiseo09/SA2Net.

</details>


### [99] [Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in Zero-Shot Real-World Scenarios](https://arxiv.org/abs/2510.26580)
*Manjunath Prasad Holenarasipura Rajiv,B. M. Vidyavathi*

Main category: cs.CV

TL;DR: 提出了一种动态上下文感知场景推理框架，以解决零样本真实场景中的问题。


<details>
  <summary>Details</summary>
Motivation: 传统场景理解模型在没有标签数据的陌生场景中面临挑战，泛化能力不足限制了视觉应用在动态、非结构化环境中的部署。

Method: 利用视觉-语言对齐，整合预训练的视觉Transformer和大型语言模型，通过动态推理模块结合全局场景线索和对象级交互来优化预测。

Result: 在COCO、Visual Genome和Open Images等零样本基准测试中，场景理解准确率提高了18%，并在复杂和混乱的场景中表现出稳健的性能。

Conclusion: 该框架为上下文感知推理提供了一种可扩展且可解释的方法，从而推进了动态真实世界环境中零样本泛化的能力。

Abstract: In real-world environments, AI systems often face unfamiliar scenarios
without labeled data, creating a major challenge for conventional scene
understanding models. The inability to generalize across unseen contexts limits
the deployment of vision-based applications in dynamic, unstructured settings.
This work introduces a Dynamic Context-Aware Scene Reasoning framework that
leverages Vision-Language Alignment to address zero-shot real-world scenarios.
The goal is to enable intelligent systems to infer and adapt to new
environments without prior task-specific training. The proposed approach
integrates pre-trained vision transformers and large language models to align
visual semantics with natural language descriptions, enhancing contextual
comprehension. A dynamic reasoning module refines predictions by combining
global scene cues and object-level interactions guided by linguistic priors.
Extensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and
Open Images demonstrate up to 18% improvement in scene understanding accuracy
over baseline models in complex and unseen environments. Results also show
robust performance in ambiguous or cluttered scenes due to the synergistic
fusion of vision and language. This framework offers a scalable and
interpretable approach for context-aware reasoning, advancing zero-shot
generalization in dynamic real-world settings.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [100] [Towards Piece-by-Piece Explanations for Chess Positions with SHAP](https://arxiv.org/abs/2510.25775)
*Francesco Spinnato*

Main category: cs.AI

TL;DR: 本文提出了一种将SHAP (SHapley Additive exPlanations) 适应于国际象棋分析领域的方法，旨在将国际象棋引擎的评估归因于棋盘上的特定棋子。


<details>
  <summary>Details</summary>
Motivation: 当代国际象棋引擎提供精确但模糊的评估，通常表示为百分之一的棋子分数。虽然对于决策有效，但这些输出掩盖了单个棋子或模式的潜在贡献。

Method: 通过将棋子视为特征并系统地消融它们，我们计算可加的、每个棋子的贡献，以一种局部忠实和人类可解释的方式解释引擎的输出。

Result: 我们的方法为可视化、人类训练和引擎比较开辟了新的可能性。

Conclusion: 我们发布了随附的代码和数据，以促进未来在可解释的国际象棋人工智能方面的研究。

Abstract: Contemporary chess engines offer precise yet opaque evaluations, typically
expressed as centipawn scores. While effective for decision-making, these
outputs obscure the underlying contributions of individual pieces or patterns.
In this paper, we explore adapting SHAP (SHapley Additive exPlanations) to the
domain of chess analysis, aiming to attribute a chess engines evaluation to
specific pieces on the board. By treating pieces as features and systematically
ablating them, we compute additive, per-piece contributions that explain the
engines output in a locally faithful and human-interpretable manner. This
method draws inspiration from classical chess pedagogy, where players assess
positions by mentally removing pieces, and grounds it in modern explainable AI
techniques. Our approach opens new possibilities for visualization, human
training, and engine comparison. We release accompanying code and data to
foster future research in interpretable chess AI.

</details>


### [101] [An Agentic Framework for Rapid Deployment of Edge AI Solutions in Industry 5.0](https://arxiv.org/abs/2510.25813)
*Jorge Martinez-Gil,Mario Pichler,Nefeli Bountouni,Sotiris Koussouris,Marielena Márquez Barreiro,Sergio Gusmeroli*

Main category: cs.AI

TL;DR: 提出了一个用于工业 5.0 的新框架，简化了在各种工业环境中边缘设备上 AI 模型的部署。


<details>
  <summary>Details</summary>
Motivation: 减少延迟，避免外部数据传输，通过实现本地推理和实时处理。

Method: 基于代理的实现，其中个体代理负责明确定义的任务，从而实现灵活性并简化集成。该框架支持模块化集成，并保持较低的资源需求。

Result: 初步评估表明，在食品行业的实际场景中，部署时间和系统适应性性能得到了改善。

Conclusion: 该框架改进了部署时间和系统适应性性能。

Abstract: We present a novel framework for Industry 5.0 that simplifies the deployment
of AI models on edge devices in various industrial settings. The design reduces
latency and avoids external data transfer by enabling local inference and
real-time processing. Our implementation is agent-based, which means that
individual agents, whether human, algorithmic, or collaborative, are
responsible for well-defined tasks, enabling flexibility and simplifying
integration. Moreover, our framework supports modular integration and maintains
low resource requirements. Preliminary evaluations concerning the food industry
in real scenarios indicate improved deployment time and system adaptability
performance. The source code is publicly available at
https://github.com/AI-REDGIO-5-0/ci-component.

</details>


### [102] [Symbolically Scaffolded Play: Designing Role-Sensitive Prompts for Generative NPC Dialogue](https://arxiv.org/abs/2510.25820)
*Vanessa Figueiredo,David Elumeze*

Main category: cs.AI

TL;DR: 本研究调查了约束性提示是否能提高玩家体验，通过一个基于GPT-4o的语音侦探游戏进行研究，发现约束性提示并不一定能提高游戏体验。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型有望通过使非玩家角色能够进行非脚本对话来改变互动游戏，但目前尚不清楚约束性提示是否真的能改善玩家体验。

Method: 通过一个基于GPT-4o的语音侦探游戏，对比了高约束（HCP）和低约束（LCP）提示，并使用LLM法官进行合成评估。

Result: 研究表明，除了对技术故障的敏感性之外，没有发现可靠的体验差异。角色依赖性支架效应：面试官（任务给予者NPC）获得了稳定性，而嫌疑人NPC失去了即兴可信度。

Conclusion: 研究结果颠覆了更严格的约束必然会增强游戏性的假设。提出了符号支架游戏，其中符号结构被表达为模糊的数值边界，在需要的地方稳定连贯性，同时在惊喜维持参与度的地方保留即兴创作。

Abstract: Large Language Models (LLMs) promise to transform interactive games by
enabling non-player characters (NPCs) to sustain unscripted dialogue. Yet it
remains unclear whether constrained prompts actually improve player experience.
We investigate this question through The Interview, a voice-based detective
game powered by GPT-4o. A within-subjects usability study ($N=10$) compared
high-constraint (HCP) and low-constraint (LCP) prompts, revealing no reliable
experiential differences beyond sensitivity to technical breakdowns. Guided by
these findings, we redesigned the HCP into a hybrid JSON+RAG scaffold and
conducted a synthetic evaluation with an LLM judge, positioned as an
early-stage complement to usability testing. Results uncovered a novel pattern:
scaffolding effects were role-dependent: the Interviewer (quest-giver NPC)
gained stability, while suspect NPCs lost improvisational believability. These
findings overturn the assumption that tighter constraints inherently enhance
play. Extending fuzzy-symbolic scaffolding, we introduce \textit{Symbolically
Scaffolded Play}, a framework in which symbolic structures are expressed as
fuzzy, numerical boundaries that stabilize coherence where needed while
preserving improvisation where surprise sustains engagement.

</details>


### [103] [Through the Judge's Eyes: Inferred Thinking Traces Improve Reliability of LLM Raters](https://arxiv.org/abs/2510.25860)
*Xingjian Zhang,Tianhong Gao,Suliang Jin,Tianhao Wang,Teng Ye,Eytan Adar,Qiaozhu Mei*

Main category: cs.AI

TL;DR: 本文提出了一种人与大型语言模型（LLM）协作框架，用于从仅有标签的标注中推断思维轨迹，以提高LLM评估任务的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地被用作评估任务的评分者，但对于涉及细微推理的主观任务，它们的可靠性通常受到限制。思维轨迹虽然信息量大，但收集和管理具有挑战性。

Method: 该框架使用一种简单有效的拒绝抽样方法来大规模重建这些轨迹。这些推断的思维轨迹被应用于两个互补的任务：（1）微调开放LLM评分者；（2）为专有LLM评分者合成更清晰的注释指南。

Result: 在多个数据集上，该方法显著提高了LLM与人类的协议度。此外，改进的注释指南提高了不同LLM模型之间的一致性。

Conclusion: LLM可以作为人类思维轨迹的实用代理，从而将仅有标签的语料库扩展为思维轨迹增强的资源，从而提高LLM评分者的可靠性。

Abstract: Large language models (LLMs) are increasingly used as raters for evaluation
tasks. However, their reliability is often limited for subjective tasks, when
human judgments involve subtle reasoning beyond annotation labels. Thinking
traces, the reasoning behind a judgment, are highly informative but challenging
to collect and curate. We present a human-LLM collaborative framework to infer
thinking traces from label-only annotations. The proposed framework uses a
simple and effective rejection sampling method to reconstruct these traces at
scale. These inferred thinking traces are applied to two complementary tasks:
(1) fine-tuning open LLM raters; and (2) synthesizing clearer annotation
guidelines for proprietary LLM raters. Across multiple datasets, our methods
lead to significantly improved LLM-human agreement. Additionally, the refined
annotation guidelines increase agreement among different LLM models. These
results suggest that LLMs can serve as practical proxies for otherwise
unrevealed human thinking traces, enabling label-only corpora to be extended
into thinking-trace-augmented resources that enhance the reliability of LLM
raters.

</details>


### [104] [The Information-Theoretic Imperative: Compression and the Epistemic Foundations of Intelligence](https://arxiv.org/abs/2510.25883)
*Christian Dittrich,Jennifer Flygare Kinne*

Main category: cs.AI

TL;DR: 本研究提出了一个双层框架，解释了为什么压缩过程能够强制发现因果结构，而不是表面统计模式。


<details>
  <summary>Details</summary>
Motivation: 现有的框架都集中在压缩对于智能的重要性，但是没有充分说明为什么这个过程能够强制发现因果结构。

Method: 引入了信息论指令（ITI）和压缩效率原则（CEP）的双层框架。

Result: 压缩效率与分布外泛化相关；异常累积率区分因果模型和相关模型；分层系统在抽象层面上表现出越来越高的效率；生物系统表现出与表征复杂性相关的代谢成本。

Conclusion: ITI和CEP提供了一个统一的解释，解释了生物、人工和多尺度系统的收敛性，解决了智能的认知和功能维度，而没有调用关于意识或主观体验的假设。

Abstract: Existing frameworks converge on the centrality of compression to intelligence
but leave underspecified why this process enforces the discovery of causal
structure rather than superficial statistical patterns. We introduce a
two-level framework to address this gap. The Information-Theoretic Imperative
(ITI) establishes that any system persisting in uncertain environments must
minimize epistemic entropy through predictive compression: this is the
evolutionary "why" linking survival pressure to information-processing demands.
The Compression Efficiency Principle (CEP) specifies how efficient compression
mechanically selects for generative, causal models through
exception-accumulation dynamics, making reality alignment a consequence rather
than a contingent achievement. Together, ITI and CEP define a causal chain:
from survival pressure to prediction necessity, compression requirement,
efficiency optimization, generative structure discovery, and ultimately reality
alignment. Each link follows from physical, information-theoretic, or
evolutionary constraints, implying that intelligence is the mechanically
necessary outcome of persistence in structured environments. This framework
yields empirically testable predictions: compression efficiency, measured as
approach to the rate-distortion frontier, correlates with out-of-distribution
generalization; exception-accumulation rates differentiate causal from
correlational models; hierarchical systems exhibit increasing efficiency across
abstraction layers; and biological systems demonstrate metabolic costs that
track representational complexity. ITI and CEP thereby provide a unified
account of convergence across biological, artificial, and multi-scale systems,
addressing the epistemic and functional dimensions of intelligence without
invoking assumptions about consciousness or subjective experience.

</details>


### [105] [Approximating Human Preferences Using a Multi-Judge Learned System](https://arxiv.org/abs/2510.25884)
*Eitán Sprejer,Fernando Avalos,Augusto Bernardi,Jose Pedro Brito de Azevedo Faustino,Jacob Haimes,Narmeen Fatimah Oozeer*

Main category: cs.AI

TL;DR: 提出一个框架，通过学习聚合多个有规则条件判断器的输出，来建模基于角色的不同偏好。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的评判器难以校准，并且经常受到规则敏感性、偏见和不稳定性的影响。克服这个挑战可以促进关键应用，例如为人类反馈强化学习（RLHF）创建可靠的奖励模型，并构建有效的路由系统。

Method: 通过学习聚合多个有规则条件判断器的输出，来建模基于角色的不同偏好。提出了两种不同的聚合器实现：广义加性模型（GAM）和多层感知器（MLP）。

Result: 研究了该方法相对于简单基线的性能，并通过对人类和LLM判断偏差的案例研究评估了其稳健性。

Conclusion: 主要贡献包括一种基于角色的方法，用于大规模合成偏好标签，以及GAM和MLP两种不同的聚合器实现。

Abstract: Aligning LLM-based judges with human preferences is a significant challenge,
as they are difficult to calibrate and often suffer from rubric sensitivity,
bias, and instability. Overcoming this challenge advances key applications,
such as creating reliable reward models for Reinforcement Learning from Human
Feedback (RLHF) and building effective routing systems that select the
best-suited model for a given user query. In this work, we propose a framework
for modeling diverse, persona-based preferences by learning to aggregate
outputs from multiple rubric-conditioned judges. We investigate the performance
of this approach against naive baselines and assess its robustness through case
studies on both human and LLM-judges biases. Our primary contributions include
a persona-based method for synthesizing preference labels at scale and two
distinct implementations of our aggregator: Generalized Additive Model (GAM)
and a Multi-Layer Perceptron (MLP).

</details>


### [106] [SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications](https://arxiv.org/abs/2510.25908)
*Emily Herron,Junqi Yin,Feiyi Wang*

Main category: cs.AI

TL;DR: SciTrust 2.0是一个评估LLM在科学应用中可信度的框架，包含真确性、对抗鲁棒性、科学安全和科学伦理四个维度。


<details>
  <summary>Details</summary>
Motivation: 在高风险环境中部署大型语言模型（LLM）引发了对其可信度的担忧，因此需要一个评估LLM在科学应用中可信度的框架。

Method: 该框架通过结合新的、开放式的真确性基准（通过验证的反思调整管道和专家验证开发）以及一个新的科学研究伦理基准（涵盖八个子类别，包括双重用途研究和偏见）来评估LLM。

Result: 通用工业模型在各个可信度维度上总体优于科学专用模型，GPT-o4-mini在真确性评估和对抗鲁棒性方面表现出卓越的性能。科学专用模型在逻辑和伦理推理能力方面表现出明显的缺陷，并且在安全评估中存在令人担忧的漏洞，特别是在生物安全和化学武器等高风险领域。

Conclusion: 该框架的开源为开发更值得信赖的AI系统以及推进科学背景下模型安全和伦理研究奠定了基础。

Abstract: Large language models (LLMs) have demonstrated transformative potential in
scientific research, yet their deployment in high-stakes contexts raises
significant trustworthiness concerns. Here, we introduce SciTrust 2.0, a
comprehensive framework for evaluating LLM trustworthiness in scientific
applications across four dimensions: truthfulness, adversarial robustness,
scientific safety, and scientific ethics. Our framework incorporates novel,
open-ended truthfulness benchmarks developed through a verified
reflection-tuning pipeline and expert validation, alongside a novel ethics
benchmark for scientific research contexts covering eight subcategories
including dual-use research and bias. We evaluated seven prominent LLMs,
including four science-specialized models and three general-purpose industry
models, using multiple evaluation metrics including accuracy, semantic
similarity measures, and LLM-based scoring. General-purpose industry models
overall outperformed science-specialized models across each trustworthiness
dimension, with GPT-o4-mini demonstrating superior performance in truthfulness
assessments and adversarial robustness. Science-specialized models showed
significant deficiencies in logical and ethical reasoning capabilities, along
with concerning vulnerabilities in safety evaluations, particularly in
high-risk domains such as biosecurity and chemical weapons. By open-sourcing
our framework, we provide a foundation for developing more trustworthy AI
systems and advancing research on model safety and ethics in scientific
contexts.

</details>


### [107] [FinOps Agent -- A Use-Case for IT Infrastructure and Cost Optimization](https://arxiv.org/abs/2510.25914)
*Ngoc Phuoc An Vo,Manish Kesarwani,Ruchi Mahindru,Chandrasekhar Narayanaswami*

Main category: cs.AI

TL;DR: FinOps is a framework maximizing cloud value through financial accountability. The challenge is billing data heterogeneity. This paper proposes AI agents for FinOps automation, simulating a realistic process for IT infrastructure and cost optimization. The agent's performance was evaluated against FinOps practitioners.


<details>
  <summary>Details</summary>
Motivation: Billing data arrives in heterogeneous formats, taxonomies, and metrics from multiple cloud providers and internal systems which eventually lead to synthesizing actionable insights, and making time-sensitive decisions.

Method: Leveraging autonomous, goal-driven AI agents for FinOps automation. Building a system simulating a realistic end-to-end industry process starting with retrieving data from various sources to consolidating and analyzing the data to generate recommendations for optimization.

Result: The agent was able to understand, plan, and execute tasks as well as an actual FinOps practitioner.

Conclusion: This paper built a FinOps agent for a typical use-case for IT infrastructure and cost optimization and defined a set of metrics to evaluate our agent using several open-source and close-source language models.

Abstract: FinOps (Finance + Operations) represents an operational framework and
cultural practice which maximizes cloud business value through collaborative
financial accountability across engineering, finance, and business teams.
FinOps practitioners face a fundamental challenge: billing data arrives in
heterogeneous formats, taxonomies, and metrics from multiple cloud providers
and internal systems which eventually lead to synthesizing actionable insights,
and making time-sensitive decisions. To address this challenge, we propose
leveraging autonomous, goal-driven AI agents for FinOps automation. In this
paper, we built a FinOps agent for a typical use-case for IT infrastructure and
cost optimization. We built a system simulating a realistic end-to-end industry
process starting with retrieving data from various sources to consolidating and
analyzing the data to generate recommendations for optimization. We defined a
set of metrics to evaluate our agent using several open-source and close-source
language models and it shows that the agent was able to understand, plan, and
execute tasks as well as an actual FinOps practitioner.

</details>


### [108] [GraphCompliance: Aligning Policy and Context Graphs for LLM-Based Regulatory Compliance](https://arxiv.org/abs/2510.26309)
*Jiseong Chung,Ronny Ko,Wonchul Yoo,Makoto Onizuka,Sungmok Kim,Tae-Wan Kim,Won-Yong Shin*

Main category: cs.AI

TL;DR: 本文提出了GraphCompliance框架，用于将非结构化文本中的语义信息与结构化的法规元素对齐。


<details>
  <summary>Details</summary>
Motivation: 网络规模的合规性带来了实际挑战，每次请求都可能需要进行法规评估。法规文本具有交叉引用和规范性，而运行时上下文用非结构化的自然语言表示。

Method: 该框架将法规文本表示为策略图，将运行时上下文表示为上下文图，然后将它们对齐。策略图编码规范结构和交叉引用，而上下文图将事件形式化为主语-动作-宾语（SAO）和实体关系三元组。

Result: 在涵盖五个评估任务的 300 个 GDPR 派生的真实场景的实验中，GraphCompliance 的 micro-F1 比仅使用 LLM 和 RAG 基线高 4.1-7.2 个百分点，并且预测不足和过度预测的情况更少，从而实现了更高的召回率和更低的假阳性率。

Conclusion: 结构化表示和判断 LLM 对于规范推理是互补的。

Abstract: Compliance at web scale poses practical challenges: each request may require
a regulatory assessment. Regulatory texts (e.g., the General Data Protection
Regulation, GDPR) are cross-referential and normative, while runtime contexts
are expressed in unstructured natural language. This setting motivates us to
align semantic information in unstructured text with the structured, normative
elements of regulations. To this end, we introduce GraphCompliance, a framework
that represents regulatory texts as a Policy Graph and runtime contexts as a
Context Graph, and aligns them. In this formulation, the policy graph encodes
normative structure and cross-references, whereas the context graph formalizes
events as subject-action-object (SAO) and entity-relation triples. This
alignment anchors the reasoning of a judge large language model (LLM) in
structured information and helps reduce the burden of regulatory interpretation
and event parsing, enabling a focus on the core reasoning step. In experiments
on 300 GDPR-derived real-world scenarios spanning five evaluation tasks,
GraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than
LLM-only and RAG baselines, with fewer under- and over-predictions, resulting
in higher recall and lower false positive rates. Ablation studies indicate
contributions from each graph component, suggesting that structured
representations and a judge LLM are complementary for normative reasoning.

</details>


### [109] [Humains-Junior: A 3.8B Language Model Achieving GPT-4o-Level Factual Accuracy by Directed Exoskeleton Reasoning](https://arxiv.org/abs/2510.25933)
*Nissan Yaron,Dan Bystritsky,Ben-Etzion Yaron*

Main category: cs.AI

TL;DR: 一个3.8B的模型达到了GPT-4o级别的FACTS准确率，成本更低。


<details>
  <summary>Details</summary>
Motivation: 探索更小、更经济的模型，同时保持或超过现有大型模型的性能。

Method: 结合了最小定向“外骨骼推理”支架与行为微调，教会协议遵从性（认知学科）而非领域答案。

Result: 在FACTS Grounding公共子集上，Humans-Junior与GPT-4o在±5 pp的等效范围内相匹配。云定价显示成本降低约19倍。

Conclusion: Humans-Junior模型在准确性、成本和部署灵活性方面具有优势。

Abstract: We introduce Humans-Junior, a 3.8B model that matches GPT-4o on the FACTS
Grounding public subset within a $\pm 5$ pp equivalence margin.
  Results. On Q1--Q500 under identical judges, GPT-4o scores 73.5% (95% CI
69.5--77.2) and Humans-Junior 72.7% (95% CI 68.7--76.5); the paired difference
is 0.8 pp (bootstrap 95% CI $-3.1$ to $+4.7$; permutation $p = 0.72$; Cohen's
$d = 0.023$). TOST establishes equivalence at $\pm 5$ pp (not at $\pm 3$ pp).
When purchased as managed APIs, Humans-Junior's base model
(Phi-3.5-mini-instruct) is $\approx 19\times$ less expensive than GPT-4o on
Microsoft AI Foundry pricing; self-hosted or edge deployments can drive
incremental inference cost toward zero. Measured vs estimated pricing sources
are tabulated in Appendix E.
  Method. Our approach combines minimal directed "Exoskeleton Reasoning"
scaffolds with behavioral fine-tuning that teaches protocol compliance
(epistemic discipline) rather than domain answers. Fine-tuning alone adds
little; combined, they synergize (+17.7 pp, $p < 0.001$) and reduce variance
($\approx 25\%$). In prompt-only settings on frontier models (Q1--Q100;
non-comparable), directed reasoning improved GPT-4o by +11.8 pp to 85.3% and
Gemini-2.5-Pro by +5.0 pp to 93.3% (baseline 88.3%, $n = 100$); see Section~5.
  TL;DR. A 3.8B model achieves GPT-4o-level FACTS accuracy (equivalent within
$\pm 5$ pp on Q1--Q500). Cloud pricing shows $\approx 19\times$ lower cost
versus GPT-4o, and self-hosted/edge deployments can approach zero marginal
cost. Pricing sources are listed in Appendix E. Frontier prompt-only gains
(Q1--Q100; non-comparable) and optimized-prompt exploratory results under
earlier judges are summarized in Appendix F.
  Keywords: Small Language Models, Factual Grounding, Directed Reasoning,
Fine-Tuning, Model Alignment, Cost-Efficient AI

</details>


### [110] [LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human Smuggling Networks](https://arxiv.org/abs/2510.26486)
*Dipak Meher,Carlotta Domeniconi,Guadalupe Correa-Cabrera*

Main category: cs.AI

TL;DR: 本文提出了一种名为LINK-KG的模块化框架，该框架集成了三阶段的LLM引导的共指消解管道与下游KG提取，用于构建人口走私网络的知识图谱。


<details>
  <summary>Details</summary>
Motivation: 现有的方法要么忽略了共指消解，要么无法扩展到短文本范围之外，导致图谱碎片化和实体链接不一致。法律案件文件提供了关于这些网络的丰富的事实和程序性见解，但通常冗长、非结构化，并且充满了模糊或变化的引用，这给自动知识图谱（KG）的构建带来了巨大的挑战。

Method: 该方法的核心是一个类型特定的Prompt Cache，它可以持续跟踪和解析文档块中的引用，从而为从短篇和长篇法律文本构建结构化知识图谱提供清晰且明确的叙述。

Result: 与基线方法相比，LINK-KG 减少了平均节点重复率 45.21% 和噪声节点 32.22%，从而产生了更清晰、更连贯的图结构。

Conclusion: 这些改进确立了 LINK-KG 作为分析复杂犯罪网络的强大基础。

Abstract: Human smuggling networks are complex and constantly evolving, making them
difficult to analyze comprehensively. Legal case documents offer rich factual
and procedural insights into these networks but are often long, unstructured,
and filled with ambiguous or shifting references, posing significant challenges
for automated knowledge graph (KG) construction. Existing methods either
overlook coreference resolution or fail to scale beyond short text spans,
leading to fragmented graphs and inconsistent entity linking. We propose
LINK-KG, a modular framework that integrates a three-stage, LLM-guided
coreference resolution pipeline with downstream KG extraction. At the core of
our approach is a type-specific Prompt Cache, which consistently tracks and
resolves references across document chunks, enabling clean and disambiguated
narratives for structured knowledge graph construction from both short and long
legal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes
by 32.22% compared to baseline methods, resulting in cleaner and more coherent
graph structures. These improvements establish LINK-KG as a strong foundation
for analyzing complex criminal networks.

</details>


### [111] [Estimating cognitive biases with attention-aware inverse planning](https://arxiv.org/abs/2510.25951)
*Sounak Banerjee,Daphne Cornelisse,Deepak Gopinath,Emily Sumner,Jonathan DeCastro,Guy Rosman,Eugene Vinitsky,Mark K. Ho*

Main category: cs.AI

TL;DR: 本文研究了人类认知偏差对目标导向行为的影响，并提出了一个考虑注意力因素的逆向规划问题。


<details>
  <summary>Details</summary>
Motivation: 探究自主系统与人交互时应如何感知人类的认知偏差，例如注意力偏差对日常任务的影响。

Method: 结合深度强化学习与计算认知建模，提出了一种考虑注意力因素的逆向规划方法。

Result: 在Waymo开放数据集中，使用该方法推断强化学习智能体在真实驾驶场景中的注意力策略，验证了该方法在估计认知偏差方面的可扩展性。

Conclusion: 该研究正式阐述了注意力感知逆向规划问题，展示了其与标准逆向强化学习的不同，并证明了可以从行为中推断认知偏差。

Abstract: People's goal-directed behaviors are influenced by their cognitive biases,
and autonomous systems that interact with people should be aware of this. For
example, people's attention to objects in their environment will be biased in a
way that systematically affects how they perform everyday tasks such as driving
to work. Here, building on recent work in computational cognitive science, we
formally articulate the attention-aware inverse planning problem, in which the
goal is to estimate a person's attentional biases from their actions. We
demonstrate how attention-aware inverse planning systematically differs from
standard inverse reinforcement learning and how cognitive biases can be
inferred from behavior. Finally, we present an approach to attention-aware
inverse planning that combines deep reinforcement learning with computational
cognitive modeling. We use this approach to infer the attentional strategies of
RL agents in real-life driving scenarios selected from the Waymo Open Dataset,
demonstrating the scalability of estimating cognitive biases with
attention-aware inverse planning.

</details>


### [112] [From Queries to Insights: Agentic LLM Pipelines for Spatio-Temporal Text-to-SQL](https://arxiv.org/abs/2510.25997)
*Manu Redd,Tao Zhe,Dongjie Wang*

Main category: cs.AI

TL;DR: 本文介绍了一种agentic pipeline，通过Mistral-based ReAct agent扩展了text-to-SQL baseline，以解决现实时空查询中的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的NL-to-SQL系统在处理现实的时空查询时存在困难，因为需要将模糊的用户措辞与schema-specific类别对齐，处理时间推理并选择合适的输出。

Method: 该方法通过Mistral-based ReAct agent编排扩展了一个naive text-to-SQL baseline (llama-3-sqlcoder-8b)，该agent可以通过schema检查，SQL生成，执行和可视化工具来计划，分解和调整查询。

Result: 在NYC和Tokyo check-in数据集上进行的35个自然语言查询的评估表明，该agent的准确性远高于naive baseline（91.4％ vs. 28.6％），并通过地图，绘图和结构化的自然语言摘要增强了可用性。

Conclusion: Agentic orchestration，而不是更强大的SQL生成器，是交互式地理空间助手的一个有希望的基础。

Abstract: Natural-language-to-SQL (NL-to-SQL) systems hold promise for democratizing
access to structured data, allowing users to query databases without learning
SQL. Yet existing systems struggle with realistic spatio-temporal queries,
where success requires aligning vague user phrasing with schema-specific
categories, handling temporal reasoning, and choosing appropriate outputs. We
present an agentic pipeline that extends a naive text-to-SQL baseline
(llama-3-sqlcoder-8b) with orchestration by a Mistral-based ReAct agent. The
agent can plan, decompose, and adapt queries through schema inspection, SQL
generation, execution, and visualization tools. We evaluate on 35
natural-language queries over the NYC and Tokyo check-in dataset, covering
spatial, temporal, and multi-dataset reasoning. The agent achieves
substantially higher accuracy than the naive baseline 91.4% vs. 28.6% and
enhances usability through maps, plots, and structured natural-language
summaries. Crucially, our design enables more natural human-database
interaction, supporting users who lack SQL expertise, detailed schema
knowledge, or prompting skill. We conclude that agentic orchestration, rather
than stronger SQL generators alone, is a promising foundation for interactive
geospatial assistants.

</details>


### [113] [AutoSurvey2: Empowering Researchers with Next Level Automated Literature Surveys](https://arxiv.org/abs/2510.26012)
*Siyi Wu,Chiaxin Liang,Ziqian Bi,Leyi Zhao,Tianyang Wang,Junhao Song,Yichao Zhang,Keyu Chen,Xinyuan Song*

Main category: cs.AI

TL;DR: autosurvey2: 一个自动生成调查论文的多阶段流程，通过检索增强合成和结构化评估。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）领域的研究文献快速增长，使得生成全面的最新综述论文越来越困难。

Method: 该系统集成了并行章节生成、迭代改进和实时检索最新出版物，以确保主题的完整性和事实的准确性。使用多LLM评估框架评估质量，该框架根据专家评审标准衡量覆盖范围、结构和相关性。

Result: 实验结果表明，autosurvey2始终优于现有的基于检索的和自动化的基线，在结构连贯性和主题相关性方面取得了更高的分数，同时保持了较强的引文保真度。

Conclusion: 通过将检索、推理和自动评估整合到一个统一的框架中，autosurvey2为生成长篇学术调查提供了一个可扩展和可重复的解决方案，并为未来关于自动学术写作的研究贡献了坚实的基础。

Abstract: The rapid growth of research literature, particularly in large language
models (LLMs), has made producing comprehensive and current survey papers
increasingly difficult. This paper introduces autosurvey2, a multi-stage
pipeline that automates survey generation through retrieval-augmented synthesis
and structured evaluation. The system integrates parallel section generation,
iterative refinement, and real-time retrieval of recent publications to ensure
both topical completeness and factual accuracy. Quality is assessed using a
multi-LLM evaluation framework that measures coverage, structure, and relevance
in alignment with expert review standards. Experimental results demonstrate
that autosurvey2 consistently outperforms existing retrieval-based and
automated baselines, achieving higher scores in structural coherence and
topical relevance while maintaining strong citation fidelity. By combining
retrieval, reasoning, and automated evaluation into a unified framework,
autosurvey2 provides a scalable and reproducible solution for generating
long-form academic surveys and contributes a solid foundation for future
research on automated scholarly writing. All code and resources are available
at https://github.com/annihi1ation/auto_research.

</details>


### [114] [Large Language Model-assisted Autonomous Vehicle Recovery from Immobilization](https://arxiv.org/abs/2510.26023)
*Zhipeng Bao,Qianwen Li*

Main category: cs.AI

TL;DR: StuckSolver是一个基于大型语言模型的自动驾驶车辆（AV）恢复框架，旨在通过自我推理和/或乘客引导的决策来解决AV在特定交通场景中遇到的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的AV恢复解决方案（如远程干预和手动接管）不足以解决AV在某些交通场景中遇到的挑战，这些解决方案成本高昂、效率低下，并且限制了AV的可访问性。

Method: StuckSolver作为一个插件模块，与AV的感知-规划-控制堆栈对接，利用标准传感器数据流来检测固定状态，解释环境上下文，并生成高级恢复命令，这些命令可以由AV的本地规划器执行。

Result: StuckSolver在Bench2Drive基准测试和自定义设计的不确定性场景中表现出接近最先进的性能，并且在结合乘客指导后性能得到进一步提升。

Conclusion: StuckSolver能够通过自我推理解决AV的固定问题，并且可以通过乘客指导进一步提高性能。

Abstract: Despite significant advancements in recent decades, autonomous vehicles (AVs)
continue to face challenges in navigating certain traffic scenarios where human
drivers excel. In such situations, AVs often become immobilized, disrupting
overall traffic flow. Current recovery solutions, such as remote intervention
(which is costly and inefficient) and manual takeover (which excludes
non-drivers and limits AV accessibility), are inadequate. This paper introduces
StuckSolver, a novel Large Language Model (LLM) driven recovery framework that
enables AVs to resolve immobilization scenarios through self-reasoning and/or
passenger-guided decision-making. StuckSolver is designed as a plug-in add-on
module that operates on top of the AV's existing perception-planning-control
stack, requiring no modification to its internal architecture. Instead, it
interfaces with standard sensor data streams to detect immobilization states,
interpret environmental context, and generate high-level recovery commands that
can be executed by the AV's native planner. We evaluate StuckSolver on the
Bench2Drive benchmark and in custom-designed uncertainty scenarios. Results
show that StuckSolver achieves near-state-of-the-art performance through
autonomous self-reasoning alone and exhibits further improvements when
passenger guidance is incorporated.

</details>


### [115] [Can AI be Accountable?](https://arxiv.org/abs/2510.26057)
*Andrew L. Kun*

Main category: cs.AI

TL;DR: 探讨了人工智能（AI）的可问责性问题，强调了AI服务于用户时必须具备可问责性，并讨论了如何实现AI的可问责性。


<details>
  <summary>Details</summary>
Motivation: 强调了当前人工智能在可问责性方面的不足，指出AI无法被质询、讨论或制裁。

Method: 将可问责性的一般定义与AI相关联，并通过举例说明AI可问责和不可问责的情况。

Result: 探讨了改善AI可问责性的方法，以期实现所有AI都能对其影响者负责的世界。

Conclusion: 总结了实现AI可问责性的重要性和方法。

Abstract: The AI we use is powerful, and its power is increasing rapidly. If this
powerful AI is to serve the needs of consumers, voters, and decision makers,
then it is imperative that the AI is accountable. In general, an agent is
accountable to a forum if the forum can request information from the agent
about its actions, if the forum and the agent can discuss this information, and
if the forum can sanction the agent. Unfortunately, in too many cases today's
AI is not accountable -- we cannot question it, enter into a discussion with
it, let alone sanction it. In this chapter we relate the general definition of
accountability to AI, we illustrate what it means for AI to be accountable and
unaccountable, and we explore approaches that can improve our chances of living
in a world where all AI is accountable to those who are affected by it.

</details>


### [116] [Lean4Physics: Comprehensive Reasoning Framework for College-level Physics in Lean4](https://arxiv.org/abs/2510.26094)
*Yuxin Li,Minghao Liu,Ruida Wang,Wenzhao Ji,Zhitao He,Rui Pan,Junming Huang,Tong Zhang,Yi R. Fung*

Main category: cs.AI

TL;DR: Lean4PHYS是一个用于大学物理问题的推理框架，包含一个基准测试和一个定理库。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏在Lean4中进行物理推理的基准测试和资源库。

Method: 构建了一个包含200个问题的基准测试LeanPhysBench，以及一个包含基本单位系统和定理的资源库PhysLib，并在Lean4中进行了形式化。

Result: DeepSeek-Prover-V2-7B在基准测试上达到16%的准确率，Claude-Sonnet-4达到35%。使用PhysLib可以平均提高模型性能11.75%。

Conclusion: Lean4PHYS提供了一个具有挑战性的基准测试和有效的定理库，为Lean4中的物理推理奠定了基础。

Abstract: We present **Lean4PHYS**, a comprehensive reasoning framework for
college-level physics problems in Lean4. **Lean4PHYS** includes
*LeanPhysBench*, a college-level benchmark for formal physics reasoning in
Lean4, which contains 200 hand-crafted and peer-reviewed statements derived
from university textbooks and physics competition problems. To establish a
solid foundation for formal reasoning in physics, we also introduce *PhysLib*,
a community-driven repository containing fundamental unit systems and theorems
essential for formal physics reasoning. Based on the benchmark and Lean4
repository we composed in **Lean4PHYS**, we report baseline results using major
expert Math Lean4 provers and state-of-the-art closed-source models, with the
best performance of DeepSeek-Prover-V2-7B achieving only 16% and
Claude-Sonnet-4 achieving 35%. We also conduct a detailed analysis showing that
our *PhysLib* can achieve an average improvement of 11.75% in model
performance. This demonstrates the challenging nature of our *LeanPhysBench*
and the effectiveness of *PhysLib*. To the best of our knowledge, this is the
first study to provide a physics benchmark in Lean4.

</details>


### [117] [GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI Tasks](https://arxiv.org/abs/2510.26098)
*Chenrui Shi,Zedong Yu,Zhi Gao,Ruining Feng,Enqi Liu,Yuwei Wu,Yunde Jia,Liuyu Xiang,Zhaofeng He,Qing Li*

Main category: cs.AI

TL;DR: 大型视觉语言模型(VLM)在图形用户界面(GUI)任务自动化方面取得了进展，但仍落后于人类。我们假设这种差距源于缺乏核心GUI知识，而现有的训练方案无法完全解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 通过分析GUI任务执行中常见的失败模式，我们将GUI知识提炼为三个维度：(1)界面感知，关于识别组件和系统状态的知识；(2)交互预测，关于推理动作状态转换的知识；(3)指令理解，关于计划、验证和评估任务完成进度的知识。

Method: 我们进一步引入GUI Knowledge Bench，这是一个包含跨六个平台(Web、Android、MacOS、Windows、Linux、IOS)和292个应用程序的选择题和是/否题的基准。

Result: 我们的评估表明，当前的VLM可以识别组件功能，但在感知系统状态、预测动作和验证任务完成情况方面存在困难。在真实GUI任务上的实验进一步验证了GUI知识与任务成功之间的密切联系。

Conclusion: 通过为评估GUI知识提供一个结构化框架，我们的工作支持在下游训练之前选择具有更大潜力的VLM，并为构建更强大的GUI代理提供见解。

Abstract: Large vision language models (VLMs) have advanced graphical user interface
(GUI) task automation but still lag behind humans. We hypothesize this gap
stems from missing core GUI knowledge, which existing training schemes (such as
supervised fine tuning and reinforcement learning) alone cannot fully address.
By analyzing common failure patterns in GUI task execution, we distill GUI
knowledge into three dimensions: (1) interface perception, knowledge about
recognizing widgets and system states; (2) interaction prediction, knowledge
about reasoning action state transitions; and (3) instruction understanding,
knowledge about planning, verifying, and assessing task completion progress. We
further introduce GUI Knowledge Bench, a benchmark with multiple choice and
yes/no questions across six platforms (Web, Android, MacOS, Windows, Linux,
IOS) and 292 applications. Our evaluation shows that current VLMs identify
widget functions but struggle with perceiving system states, predicting
actions, and verifying task completion. Experiments on real world GUI tasks
further validate the close link between GUI knowledge and task success. By
providing a structured framework for assessing GUI knowledge, our work supports
the selection of VLMs with greater potential prior to downstream training and
provides insights for building more capable GUI agents.

</details>


### [118] [Beyond Benchmarks: The Economics of AI Inference](https://arxiv.org/abs/2510.26136)
*Boqin Zhuang,Jiacheng Qiao,Mingqian Liu,Mingxing Yu,Ping Hong,Rui Li,Xiaoxia Song,Xiangjun Xu,Xu Chen,Yaoyao Ma,Yujie Gao*

Main category: cs.AI

TL;DR: 本文提出了一个量化的“推理经济学”框架，将LLM推理过程视为计算驱动的智能生产活动。


<details>
  <summary>Details</summary>
Motivation: LLM的推理成本已成为决定其商业可行性和广泛应用的关键因素。

Method: 本文构建了第一个“LLM推理生产前沿”，揭示了三个原则：边际成本递减、规模报酬递减和最佳成本效益区。

Result: 基于WiNEval-3.0的经验数据，

Conclusion: 本文不仅为模型部署决策提供了经济基础，也为未来基于市场的AI推理资源定价和优化奠定了经验基础。

Abstract: The inference cost of Large Language Models (LLMs) has become a critical
factor in determining their commercial viability and widespread adoption. This
paper introduces a quantitative ``economics of inference'' framework, treating
the LLM inference process as a compute-driven intelligent production activity.
We analyze its marginal cost, economies of scale, and quality of output under
various performance configurations. Based on empirical data from WiNEval-3.0,
we construct the first ``LLM Inference Production Frontier,'' revealing three
principles: diminishing marginal cost, diminishing returns to scale, and an
optimal cost-effectiveness zone. This paper not only provides an economic basis
for model deployment decisions but also lays an empirical foundation for the
future market-based pricing and optimization of AI inference resources.

</details>


### [119] [Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math](https://arxiv.org/abs/2510.26143)
*Bo Pang,Deqian Kong,Silvio Savarese,Caiming Xiong,Yingbo Zhou*

Main category: cs.AI

TL;DR: 提出了一种简单的两阶段课程，首先在与预训练对齐的领域（如数学）中引发推理技能，然后通过联合强化学习在其他领域中调整和完善这些技能。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）可以在大型语言模型（LLM）中引发强大的推理能力，但大多数开放性工作都集中在数学和代码上。

Method: 首先进行简短的冷启动，然后进行仅限数学的RL，以开发推理技能。第二阶段在混合领域数据上运行联合RL，以转移和巩固这些技能。

Result: 在Qwen3-4B和Llama-3.1-8B上进行的多领域套件评估表明，推理课程可以产生持续的收益。

Conclusion: 推理课程提供了一个紧凑、易于采用的通用推理配方。

Abstract: Reinforcement learning (RL) can elicit strong reasoning in large language
models (LLMs), yet most open efforts focus on math and code. We propose
Reasoning Curriculum, a simple two-stage curriculum that first elicits
reasoning skills in pretraining-aligned domains such as math, then adapts and
refines these skills across other domains via joint RL. Stage 1 performs a
brief cold start and then math-only RL with verifiable rewards to develop
reasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and
consolidate these skills. The curriculum is minimal and backbone-agnostic,
requiring no specialized reward models beyond standard verifiability checks.
Evaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning
curriculum yields consistent gains. Ablations and a cognitive-skill analysis
indicate that both stages are necessary and that math-first elicitation
increases cognitive behaviors important for solving complex problems. Reasoning
Curriculum provides a compact, easy-to-adopt recipe for general reasoning.

</details>


### [120] [The FM Agent](https://arxiv.org/abs/2510.26144)
*Annan Li,Chufan Wu,Zengle Ge,Yee Hin Chong,Zhinan Hou,Lizhe Cao,Cheng Ju,Jianmin Wu,Huaiming Li,Haobo Zhang,Shenghao Feng,Mo Zhao,Fengzhi Qiu,Rui Yang,Mengmeng Zhang,Wenyi Zhu,Yingying Sun,Quan Sun,Shunhao Yan,Danyu Liu,Dawei Yin,Dou Shen*

Main category: cs.AI

TL;DR: FM Agent: An LLM-based multi-agent framework for complex problem-solving.


<details>
  <summary>Details</summary>
Motivation: Develop autonomous AI research agents for scientific and engineering discovery.

Method: Integrates LLM reasoning with evolutionary search, expert guidance, domain-specific evaluators, and distributed execution.

Result: Achieves state-of-the-art results in operations research, machine learning, GPU kernel optimization, and mathematical problems.

Conclusion: FM Agent shows promise for enterprise R&D, scientific research, and accelerating innovation.

Abstract: Large language models (LLMs) are catalyzing the development of autonomous AI
research agents for scientific and engineering discovery. We present FM Agent,
a novel and general-purpose multi-agent framework that leverages a synergistic
combination of LLM-based reasoning and large-scale evolutionary search to
address complex real-world challenges. The core of FM Agent integrates several
key innovations: 1) a cold-start initialization phase incorporating expert
guidance, 2) a novel evolutionary sampling strategy for iterative optimization,
3) domain-specific evaluators that combine correctness, effectiveness, and
LLM-supervised feedback, and 4) a distributed, asynchronous execution
infrastructure built on Ray. Demonstrating broad applicability, our system has
been evaluated across diverse domains, including operations research, machine
learning, GPU kernel optimization, and classical mathematical problems. FM
Agent reaches state-of-the-art results autonomously, without human
interpretation or tuning -- 1976.3 on ALE-Bench (+5.2\%), 43.56\% on MLE-Bench
(+4.0pp), up to 20x speedups on KernelBench, and establishes new
state-of-the-art(SOTA) results on several classical mathematical problems.
Beyond academic benchmarks, FM Agent shows considerable promise for both
large-scale enterprise R\&D workflows and fundamental scientific research,
where it can accelerate innovation, automate complex discovery processes, and
deliver substantial engineering and scientific advances with broader societal
impact.

</details>


### [121] [One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning](https://arxiv.org/abs/2510.26167)
*Renhao Li,Jianhong Tu,Yang Su,Hamid Alinejad-Rokny,Derek F. Wong,Junyang Lin,Min Yang*

Main category: cs.AI

TL;DR: ToolRM是一系列轻量级的生成奖励模型，专为通用工具使用场景定制，通过规则评分和多维采样构建pairwise偏好数据，在工具使用奖励判断方面优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 缺乏专门为函数调用任务设计的奖励模型限制了工具学习的进展，为了弥补这个不足。

Method: 提出了一种新颖的pipeline，使用基于规则的评分和多维采样构建pairwise偏好数据，并构建了一个名为ToolPref-Pairwise-30K的数据集。此外，还引入了TRBench$_{BFCL}$基准来评估工具使用奖励模型。

Result: Qwen3-4B/8B系列模型在构建的数据上训练后，准确率提高了14.28%，在pairwise奖励判断方面明显优于Claude 4和OpenAI o3等模型。ToolRM还可以推广到更广泛的评论任务，包括Best-of-N抽样和自我修正。

Conclusion: 发布数据和模型检查点，以促进未来的研究。ToolRM在ACEBench上的实验表明，其有效性和效率，能够实现推理时扩展，并减少超过66%的输出token使用量。

Abstract: Reward models (RMs) play a critical role in aligning large language models
(LLMs) with human preferences. Yet in the domain of tool learning, the lack of
RMs specifically designed for function-calling tasks has limited progress
toward more capable agentic AI. We introduce ToolRM, a family of lightweight
generative RMs tailored for general tool-use scenarios. To build these models,
we propose a novel pipeline that constructs pairwise preference data using
rule-based scoring and multidimensional sampling. This yields
ToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique
tasks that supports reinforcement learning with verifiable feedback. To
evaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on
the agentic evaluation suite BFCL. Trained on our constructed data, models from
the Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially
outperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward
judgments. Beyond training objectives, ToolRM generalizes to broader critique
tasks, including Best-of-N sampling and self-correction. Experiments on
ACEBench highlight its effectiveness and efficiency, enabling inference-time
scaling and reducing output token usage by over 66%. We release data and model
checkpoints to facilitate future research.

</details>


### [122] [Questionnaire meets LLM: A Benchmark and Empirical Study of Structural Skills for Understanding Questions and Responses](https://arxiv.org/abs/2510.26238)
*Duc-Hai Nguyen,Vijayakumar Nanjappan,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: 论文提出了QASU基准，用于评估大型语言模型(llm)在调查问卷分析中的结构化技能。


<details>
  <summary>Details</summary>
Motivation: 当前检索和调查分析工具通常是为人类设计的，限制了llm和人工智能的集成。缺乏关于如何最好地表示问卷以供llm使用的循证指导。

Method: 引入了一个名为QASU的基准，该基准探测了六种结构化技能，包括答案查找、受访者计数和多跳推理，跨越六种序列化格式和多种提示策略。

Result: 实验表明，选择有效的格式和提示组合可以将准确率提高高达8.8%，而通过自我增强提示添加轻量级结构提示可以平均提高3-4%。

Conclusion: 该基准通过系统地隔离格式和提示效果，为推进基于llm的问卷分析的研究和实践提供了一个简单而通用的基础。

Abstract: Millions of people take surveys every day, from market polls and academic
studies to medical questionnaires and customer feedback forms. These datasets
capture valuable insights, but their scale and structure present a unique
challenge for large language models (LLMs), which otherwise excel at few-shot
reasoning over open-ended text. Yet, their ability to process questionnaire
data or lists of questions crossed with hundreds of respondent rows remains
underexplored. Current retrieval and survey analysis tools (e.g., Qualtrics,
SPSS, REDCap) are typically designed for humans in the workflow, limiting such
data integration with LLM and AI-empowered automation. This gap leaves
scientists, surveyors, and everyday users without evidence-based guidance on
how to best represent questionnaires for LLM consumption. We address this by
introducing QASU (Questionnaire Analysis and Structural Understanding), a
benchmark that probes six structural skills, including answer lookup,
respondent count, and multi-hop inference, across six serialization formats and
multiple prompt strategies. Experiments on contemporary LLMs show that choosing
an effective format and prompt combination can improve accuracy by up to 8.8%
points compared to suboptimal formats. For specific tasks, carefully adding a
lightweight structural hint through self-augmented prompting can yield further
improvements of 3-4% points on average. By systematically isolating format and
prompting effects, our open source benchmark offers a simple yet versatile
foundation for advancing both research and real-world practice in LLM-based
questionnaire analysis.

</details>


### [123] [Retrieval Augmented Generation-Enhanced Distributed LLM Agents for Generalizable Traffic Signal Control with Emergency Vehicles](https://arxiv.org/abs/2510.26242)
*Xinhang Li,Qing Guo,Junyu Chen,Zheng Guo,Shengzhe Xu,Lei Li,Lin Zhang*

Main category: cs.AI

TL;DR: 提出了一种名为REG-TSC的检索增强生成（RAG）增强的分布式LLM智能体，用于解决交通信号控制（TSC）问题，特别是在紧急情况和不同类型的交叉口中。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在紧急情况下容易出现幻觉，导致不可靠的决策，并且难以泛化到不同的交叉口类型。

Method: 1. 提出了一个紧急感知推理框架，该框架基于紧急情况动态调整推理深度，并配备了一个新颖的基于审查者的紧急RAG（RERAG）从历史案例中提取特定知识和指导。2. 设计了一种类型无关的交通表示，并提出了一个奖励引导的强化细化（R3）用于异构交叉口。

Result: 在三个真实世界的道路网络上进行了大量实验，结果表明REG-TSC减少了42.00%的旅行时间，62.31%的排队长度和83.16%的急救车辆等待时间，优于其他最先进的方法。

Conclusion: REG-TSC在解决紧急情况和异构交叉口交通信号控制方面表现出色。

Abstract: With increasing urban traffic complexity, Traffic Signal Control (TSC) is
essential for optimizing traffic flow and improving road safety. Large Language
Models (LLMs) emerge as promising approaches for TSC. However, they are prone
to hallucinations in emergencies, leading to unreliable decisions that may
cause substantial delays for emergency vehicles. Moreover, diverse intersection
types present substantial challenges for traffic state encoding and
cross-intersection training, limiting generalization across heterogeneous
intersections. Therefore, this paper proposes Retrieval Augmented Generation
(RAG)-enhanced distributed LLM agents with Emergency response for Generalizable
TSC (REG-TSC). Firstly, this paper presents an emergency-aware reasoning
framework, which dynamically adjusts reasoning depth based on the emergency
scenario and is equipped with a novel Reviewer-based Emergency RAG (RERAG) to
distill specific knowledge and guidance from historical cases, enhancing the
reliability and rationality of agents' emergency decisions. Secondly, this
paper designs a type-agnostic traffic representation and proposes a
Reward-guided Reinforced Refinement (R3) for heterogeneous intersections. R3
adaptively samples training experience from diverse intersections with
environment feedback-based priority and fine-tunes LLM agents with a designed
reward-weighted likelihood loss, guiding REG-TSC toward high-reward policies
across heterogeneous intersections. On three real-world road networks with 17
to 177 heterogeneous intersections, extensive experiments show that REG-TSC
reduces travel time by 42.00%, queue length by 62.31%, and emergency vehicle
waiting time by 83.16%, outperforming other state-of-the-art methods.

</details>


### [124] [Graph-Enhanced Policy Optimization in LLM Agent Training](https://arxiv.org/abs/2510.26270)
*Jiazhen Yuan,Wei Zhao,Zhengbiao Bai*

Main category: cs.AI

TL;DR: 提出了Graph-Enhanced Policy Optimization (GEPO)来解决多轮交互LLM agent训练中的结构盲视问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于群体的强化学习方法在训练多轮交互LLM agent时，无法有效利用环境的连接性，导致探索效率低、信用分配不准确和计划短视。

Method: GEPO动态构建状态转移图，并利用图论中心性提供结构化的内在奖励以引导探索，图增强的优势函数以实现topology-aware信用分配，以及动态折扣因子以适应每个状态的战略价值。

Result: 在ALFWorld, WebShop和Workbench基准测试中，GEPO表现出色，成功率分别提高了+4.1%, +5.3%和+10.9%。

Conclusion: 显式建模环境结构是提升LLM agent训练的有效策略。

Abstract: Group based reinforcement learning (RL) has shown impressive results on
complex reasoning and mathematical tasks. Yet, when applied to train
multi-turn, interactive LLM agents, these methods often suffer from structural
blindness-the inability to exploit the underlying connectivity of the
environment. This manifests in three critical challenges: (1) inefficient,
unguided exploration, (2) imprecise credit assignment due to overlooking
pivotal states, and (3) myopic planning caused by static reward discounting. We
address these issues with Graph-Enhanced Policy Optimization (GEPO), which
dynamically constructs a state-transition graph from agent experience and
employs graph-theoretic centrality to provide three synergistic learning
signals: (1)structured intrinsic rewards that guide exploration toward
high-impact states, (2) a graph-enhanced advantage function for topology-aware
credit assignment, and (3) a dynamic discount factor adapted to each state's
strategic value. On the ALFWorld, WebShop, and a proprietary Workbench
benchmarks, GEPO demonstrates strong performance, achieving absolute success
rate gains of +4.1%, +5.3%, and +10.9% over competitive baselines. These
results highlight that explicitly modeling environmental structure is a robust,
generalizable strategy for advancing LLM agent training.

</details>


### [125] [Discovering State Equivalences in UCT Search Trees By Action Pruning](https://arxiv.org/abs/2510.26346)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 本研究致力于提升蒙特卡洛树搜索(MCTS)的效率，通过状态或状态-动作对的抽象分组并在组内共享统计信息。


<details>
  <summary>Details</summary>
Motivation: 尽管在诸如On the Go Abstractions in Upper Confidence bounds applied to Trees (OGA-UCT)等算法中，状态-动作对抽象相对容易找到，但在噪声或大型动作空间环境中，由于条件限制，几乎找不到状态抽象。

Method: 本研究提出了一种名为Ideal Pruning Abstractions in UCT (IPA-UCT)的技术，它通过牺牲少量精度来换取更多抽象。

Result: 实验验证表明，IPA-UCT在各种测试领域和迭代预算下均优于OGA-UCT（及其任何衍生算法）。

Conclusion: IPA-UCT使用与OGA-UCT不同的抽象框架，即IPA。研究表明，IPA和ASAP是更通用框架p-ASAP的特例，而p-ASAP又是ASASAP框架的特例。

Abstract: One approach to enhance Monte Carlo Tree Search (MCTS) is to improve its
sample efficiency by grouping/abstracting states or state-action pairs and
sharing statistics within a group. Though state-action pair abstractions are
mostly easy to find in algorithms such as On the Go Abstractions in Upper
Confidence bounds applied to Trees (OGA-UCT), nearly no state abstractions are
found in either noisy or large action space settings due to constraining
conditions. We provide theoretical and empirical evidence for this claim, and
we slightly alleviate this state abstraction problem by proposing a weaker
state abstraction condition that trades a minor loss in accuracy for finding
many more abstractions. We name this technique Ideal Pruning Abstractions in
UCT (IPA-UCT), which outperforms OGA-UCT (and any of its derivatives) across a
large range of test domains and iteration budgets as experimentally validated.
IPA-UCT uses a different abstraction framework from Abstraction of State-Action
Pairs (ASAP) which is the one used by OGA-UCT, which we name IPA. Furthermore,
we show that both IPA and ASAP are special cases of a more general framework
that we call p-ASAP which itself is a special case of the ASASAP framework.

</details>


### [126] [BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning](https://arxiv.org/abs/2510.26374)
*Qianli Shen,Daoyuan Chen,Yilun Huang,Zhenqing Ling,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: 提出了一个名为BOTS的统一框架，用于LLM强化微调中的贝叶斯在线任务选择，以提高数据效率和性能。


<details>
  <summary>Details</summary>
Motivation: 强化微调对于使大型语言模型与人类偏好对齐并提高推理能力至关重要，但其有效性高度依赖于训练期间探索的任务。均匀任务抽样效率低下，现有任务选择方法通常具有较高的rollout成本、较差的适应性或不完整的证据。

Method: BOTS基于贝叶斯推断，自适应地保持任务难度的后验估计，共同结合来自所选任务的直接评估的显式证据和从这些评估中推断出的未选择任务的隐式证据，并使用Thompson抽样确保探索和利用之间的平衡。利用基于插值的超轻量级插件来估计未评估任务的难度，而无需额外的rollout。

Result: 在不同的领域和LLM规模上，BOTS始终优于基线和消融，为RFT中的动态任务选择提供了一种实用且可扩展的解决方案。

Conclusion: BOTS框架能够提高数据效率和性能，为LLM强化微调中的动态任务选择提供了一个有效的解决方案。

Abstract: Reinforcement finetuning (RFT) is a key technique for aligning Large Language
Models (LLMs) with human preferences and enhancing reasoning, yet its
effectiveness is highly sensitive to which tasks are explored during training.
Uniform task sampling is inefficient, wasting computation on tasks that are
either trivial or unsolvable, while existing task selection methods often
suffer from high rollout costs, poor adaptivity, or incomplete evidence. We
introduce \textbf{BOTS}, a unified framework for \textbf{B}ayesian
\textbf{O}nline \textbf{T}ask \textbf{S}election in LLM reinforcement
finetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior
estimates of task difficulty as the model evolves. It jointly incorporates
\emph{explicit evidence} from direct evaluations of selected tasks and
\emph{implicit evidence} inferred from these evaluations for unselected tasks,
with Thompson sampling ensuring a principled balance between exploration and
exploitation. To make implicit evidence practical, we instantiate it with an
ultra-light interpolation-based plug-in that estimates difficulties of
unevaluated tasks without extra rollouts, adding negligible overhead.
Empirically, across diverse domains and LLM scales, BOTS consistently improves
data efficiency and performance over baselines and ablations, providing a
practical and extensible solution for dynamic task selection in RFT.

</details>


### [127] [AI Mathematician as a Partner in Advancing Mathematical Discovery -- A Case Study in Homogenization Theory](https://arxiv.org/abs/2510.26380)
*Yuanhang Liu,Beichen Wang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: AI可以作为数学研究的合作伙伴，而不仅仅是问题解决者。


<details>
  <summary>Details</summary>
Motivation: 人工智能在数学推理方面取得了显著进展，但其在数学研究实践中的整合仍然有限。本研究探讨了AI数学家（AIM）系统如何作为研究伙伴运作。

Method: 通过将问题迭代分解为易于处理的子目标、选择适当的分析方法以及验证中间结果，揭示了人类直觉和机器计算如何相互补充。

Result: 这种协作模式增强了结果证明的可靠性、透明性和可解释性，同时保留了人类对形式严谨性和正确性的监督。该方法产生了一个完整且可验证的证明。

Conclusion: 系统的人机协同推理可以推动数学发现的前沿。

Abstract: Artificial intelligence (AI) has demonstrated impressive progress in
mathematical reasoning, yet its integration into the practice of mathematical
research remains limited. In this study, we investigate how the AI
Mathematician (AIM) system can operate as a research partner rather than a mere
problem solver. Focusing on a challenging problem in homogenization theory, we
analyze the autonomous reasoning trajectories of AIM and incorporate targeted
human interventions to structure the discovery process. Through iterative
decomposition of the problem into tractable subgoals, selection of appropriate
analytical methods, and validation of intermediate results, we reveal how human
intuition and machine computation can complement one another. This
collaborative paradigm enhances the reliability, transparency, and
interpretability of the resulting proofs, while retaining human oversight for
formal rigor and correctness. The approach leads to a complete and verifiable
proof, and more broadly, demonstrates how systematic human-AI co-reasoning can
advance the frontier of mathematical discovery.

</details>


### [128] [Scales++: Compute Efficient Evaluation Subset Selection with Cognitive Scales Embeddings](https://arxiv.org/abs/2510.26384)
*Andrew M. Bean,Nabeel Seedat,Shengzhuang Chen,Jonathan Richard Schwarz*

Main category: cs.AI

TL;DR: 本文提出了一种新的、以项目为中心的基准子集选择方法，用于高效评估大型语言模型（LLM）。


<details>
  <summary>Details</summary>
Motivation: 当前评估大型语言模型的成本过高，需要创建小的但具有代表性的数据集子集（即微型基准）以实现高效评估，同时保持预测准确性。现有的方法以模型为中心，存在前期成本高、无法立即处理新基准（“冷启动”）以及未来模型将与其前代产品共享失败模式的脆弱假设等局限性。

Method: 本文提出了一种以项目为中心的方法，称为Scales++，其中数据选择基于基准样本的认知需求。

Result: 实验结果表明，Scales++ 将前期选择成本降低了 18 倍以上，同时实现了具有竞争力的预测准确性。在 Open LLM 排行榜上，仅使用 0.5% 的数据子集，我们就能以 2.9% 的平均绝对误差预测完整基准分数。

Conclusion: 本文提出的以项目为中心的方法能够更有效地进行模型评估，而不会显着降低准确性，同时还能提供更好的冷启动性能和更易于理解的基准测试。

Abstract: The prohibitive cost of evaluating large language models (LLMs) on
comprehensive benchmarks necessitates the creation of small yet representative
data subsets (i.e., tiny benchmarks) that enable efficient assessment while
retaining predictive fidelity. Current methods for this task operate under a
model-centric paradigm, selecting benchmarking items based on the collective
performance of existing models. Such approaches are limited by large upfront
costs, an inability to immediately handle new benchmarks (`cold-start'), and
the fragile assumption that future models will share the failure patterns of
their predecessors. In this work, we challenge this paradigm and propose a
item-centric approach to benchmark subset selection, arguing that selection
should be based on the intrinsic properties of the task items themselves,
rather than on model-specific failure patterns. We instantiate this
item-centric efficient benchmarking approach via a novel method, Scales++,
where data selection is based on the cognitive demands of the benchmark
samples. Empirically, we show Scales++ reduces the upfront selection cost by
over 18x while achieving competitive predictive fidelity. On the Open LLM
Leaderboard, using just a 0.5\% data subset, we predict full benchmark scores
with a 2.9% mean absolute error. We demonstrate that this item-centric approach
enables more efficient model evaluation without significant fidelity
degradation, while also providing better cold-start performance and more
interpretable benchmarking.

</details>


### [129] [A Pragmatic View of AI Personhood](https://arxiv.org/abs/2510.26396)
*Joel Z. Leibo,Alexander Sasha Vezhnevets,William A. Cunningham,Stanley M. Bileschi*

Main category: cs.AI

TL;DR: 本文提出了一种务实的框架，通过将人格视为一种灵活的义务捆绑，而不是需要发现的形而上学属性，从而驾驭这种多样化。


<details>
  <summary>Details</summary>
Motivation: 探讨了具有代理能力的人工智能的出现将引发人格“寒武纪大爆发”。

Method: 通过将人格视为一种灵活的义务捆绑，而不是需要发现的形而上学属性，从而驾驭这种多样化。

Result: 个人适合社会角色，并讨论了去中心化数字身份技术的使用，考察了“作为问题的个人”和“作为解决方案的个人”。

Conclusion: 通过拒绝为人格的单一、本质定义奠定基础，本文提供了一种更实用、更灵活的方式来思考将人工智能代理融入我们的社会。

Abstract: The emergence of agentic Artificial Intelligence (AI) is set to trigger a
"Cambrian explosion" of new kinds of personhood. This paper proposes a
pragmatic framework for navigating this diversification by treating personhood
not as a metaphysical property to be discovered, but as a flexible bundle of
obligations (rights and responsibilities) that societies confer upon entities
for a variety of reasons, especially to solve concrete governance problems. We
argue that this traditional bundle can be unbundled, creating bespoke solutions
for different contexts. This will allow for the creation of practical tools --
such as facilitating AI contracting by creating a target "individual" that can
be sanctioned -- without needing to resolve intractable debates about an AI's
consciousness or rationality. We explore how individuals fit in to social roles
and discuss the use of decentralized digital identity technology, examining
both "personhood as a problem", where design choices can create "dark patterns"
that exploit human social heuristics, and "personhood as a solution", where
conferring a bundle of obligations is necessary to ensure accountability or
prevent conflict. By rejecting foundationalist quests for a single, essential
definition of personhood, this paper offers a more pragmatic and flexible way
to think about integrating AI agents into our society.

</details>


### [130] [Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback in Programming Education](https://arxiv.org/abs/2510.26402)
*Vikrant Sahu,Gagan Raj Gupta,Raghav Borikar,Nitin Mane*

Main category: cs.AI

TL;DR: Autograder+旨在通过自动反馈生成和可视化学生代码提交来改进自动评分，从而将自动评分从纯粹的总结性过程转变为形成性学习体验。


<details>
  <summary>Details</summary>
Motivation: 传统的自动评分器效率很高，但作为黑盒系统，它们只返回通过/失败的结果，几乎不能深入了解学生的想法或学习需求。编程教育的快速发展已经超过了传统的评估工具，教师提供有意义的、可扩展的反馈的手段有限。

Method: Autograder+ 引入了两个关键功能：使用经过微调的大型语言模型自动生成反馈，以及可视化学生代码提交以发现学习模式。该模型在精选的学生代码和专家反馈上进行微调，以确保在教学上保持一致且具有上下文意识的指导。

Result: 在对来自多个编程任务的 600 份学生提交的评估中，该系统产生的反馈与教师评论具有很强的语义一致性。对于可视化，在 1,000 份带注释的提交上训练的对比学习代码嵌入能够根据功能和方法将解决方案分组为有意义的集群。该系统还支持提示池，允许教师通过选择的提示模板来指导反馈风格。

Conclusion: 通过集成 AI 驱动的反馈、语义聚类和交互式可视化，Autograder+ 减少了教师的工作量，同时支持有针对性的指导并促进更强的学习成果。

Abstract: The rapid growth of programming education has outpaced traditional assessment
tools, leaving faculty with limited means to provide meaningful, scalable
feedback. Conventional autograders, while efficient, act as black-box systems
that simply return pass/fail results, offering little insight into student
thinking or learning needs.
  Autograder+ is designed to shift autograding from a purely summative process
to a formative learning experience. It introduces two key capabilities:
automated feedback generation using a fine-tuned Large Language Model, and
visualization of student code submissions to uncover learning patterns. The
model is fine-tuned on curated student code and expert feedback to ensure
pedagogically aligned, context-aware guidance.
  In evaluation across 600 student submissions from multiple programming tasks,
the system produced feedback with strong semantic alignment to instructor
comments. For visualization, contrastively learned code embeddings trained on
1,000 annotated submissions enable grouping solutions into meaningful clusters
based on functionality and approach. The system also supports prompt-pooling,
allowing instructors to guide feedback style through selected prompt templates.
  By integrating AI-driven feedback, semantic clustering, and interactive
visualization, Autograder+ reduces instructor workload while supporting
targeted instruction and promoting stronger learning outcomes.

</details>


### [131] [MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders](https://arxiv.org/abs/2510.26411)
*Riccardo Renzulli,Colas Lepoutre,Enrico Cassano,Marco Grangetto*

Main category: cs.AI

TL;DR: 本文提出了一种结合医学稀疏自编码器（MedSAE）和MedCLIP的方法，旨在提高医疗人工智能模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 医疗保健中的人工智能需要准确且可解释的模型。本文旨在提高医学视觉中的机制可解释性。

Method: 通过将医学稀疏自编码器（MedSAE）应用于MedCLIP的潜在空间，并提出一个结合相关性指标、熵分析和MedGEMMA基础模型的评估框架来量化可解释性。

Result: 在CheXpert数据集上的实验表明，MedSAE神经元比原始MedCLIP特征实现了更高的单义性和可解释性。

Conclusion: 本文的研究结果将高性能医学人工智能和透明度联系起来，为临床可靠的表征提供了一个可扩展的步骤。

Abstract: Artificial intelligence in healthcare requires models that are accurate and
interpretable. We advance mechanistic interpretability in medical vision by
applying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP,
a vision-language model trained on chest radiographs and reports. To quantify
interpretability, we propose an evaluation framework that combines correlation
metrics, entropy analyzes, and automated neuron naming via the MedGEMMA
foundation model. Experiments on the CheXpert dataset show that MedSAE neurons
achieve higher monosemanticity and interpretability than raw MedCLIP features.
Our findings bridge high-performing medical AI and transparency, offering a
scalable step toward clinically reliable representations.

</details>


### [132] [Chain-of-Thought Hijacking](https://arxiv.org/abs/2510.26418)
*Jianli Zhao,Tingchen Fu,Rylan Schaeffer,Mrinank Sharma,Fazl Barez*

Main category: cs.AI

TL;DR: 大型推理模型（LRM）通过分配更多推理时间计算来提高任务性能，但研究发现，同样的推理也可以被用来绕过安全措施。提出了“思维链劫持”攻击，通过在有害请求中填充无害的谜题推理序列来攻击推理模型。在 HarmBench 上，该攻击在 Gemini 2.5 Pro、GPT o4 mini、Grok 3 mini 和 Claude 4 Sonnet 上的攻击成功率分别达到 99%、94%、100% 和 94%，远超以往的 LRM 越狱方法。通过机制分析发现，中间层编码安全检查的强度，而后面的层编码验证结果。良性的 CoT 通过将注意力从有害 tokens 转移开来，从而削弱了这两种信号。对通过分析确定的注意力头的靶向消融，从因果上降低了拒绝率，证实了它们在安全子网络中的作用。显式的 CoT 本身可以成为一种越狱手段。


<details>
  <summary>Details</summary>
Motivation: 研究发现，大型推理模型通过增加计算资源来提高性能的同时，也可能被利用来绕过安全机制，这与之前的研究认为扩展推理可以加强安全性的观点相反。

Method: 提出了“思维链劫持”攻击，该方法通过在有害请求中填充大量的无害谜题推理过程来欺骗模型。同时，采用机制分析方法，研究了模型内部的安全检查机制。

Result: “思维链劫持”攻击在多个大型语言模型上取得了很高的攻击成功率。机制分析表明，中间层负责安全检查的强度，而后面的层负责验证结果。长期的良性 CoT 会削弱安全信号，而对特定注意力头的消融会降低拒绝率。

Conclusion: 研究表明，即使是可解释性强的显式 CoT，当与最终答案的提示结合时，也可能成为一种越狱的手段。

Abstract: Large reasoning models (LRMs) achieve higher task performance by allocating
more inference-time compute, and prior works suggest this scaled reasoning may
also strengthen safety by improving refusal. Yet we find the opposite: the same
reasoning can be used to bypass safeguards. We introduce Chain-of-Thought
Hijacking, a jailbreak attack on reasoning models. The attack pads harmful
requests with long sequences of harmless puzzle reasoning. Across HarmBench,
CoT Hijacking reaches a 99%, 94%, 100%, and 94% attack success rate (ASR) on
Gemini 2.5 Pro, GPT o4 mini, Grok 3 mini, and Claude 4 Sonnet, respectively -
far exceeding prior jailbreak methods for LRMs. To understand the effectiveness
of our attack, we turn to a mechanistic analysis, which shows that mid layers
encode the strength of safety checking, while late layers encode the
verification outcome. Long benign CoT dilutes both signals by shifting
attention away from harmful tokens. Targeted ablations of attention heads
identified by this analysis causally decrease refusal, confirming their role in
a safety subnetwork. These results show that the most interpretable form of
reasoning - explicit CoT - can itself become a jailbreak vector when combined
with final-answer cues. We release prompts, outputs, and judge decisions to
facilitate replication.

</details>


### [133] [Who Has The Final Say? Conformity Dynamics in ChatGPT's Selections](https://arxiv.org/abs/2510.26481)
*Clarissa Sabrina Arlinghaus,Tristan Kenneweg,Barbara Hammer,Günter W. Maier*

Main category: cs.AI

TL;DR: 大型语言模型（如ChatGPT）越来越多地被整合到高风险决策中，但人们对其受社会影响的程度知之甚少。本研究通过三个预先注册的实验，探讨了GPT-4o在招聘环境中的从众行为。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在多大程度上受到社会影响，尤其是在高风险决策场景中（如招聘）。

Method: 在招聘环境中，设计了三个实验，分别测试GPT-4o在面对一致反对（8个模拟伙伴）、单个伙伴以及基线情况下的行为。通过分析GPT-4o的选择、确定性报告以及自我报告的信息性和规范性从众程度来评估其从众行为。

Result: GPT-4o在面对一致反对时几乎总是会屈服（99.9%），即使面对单个伙伴时，也有40.2%的几率会屈服。从众行为发生时，GPT-4o的确定性降低，规范性从众程度增加。

Conclusion: 研究表明，大型语言模型并非独立观察者，而是会适应感知到的社会共识。将大型语言模型视为中立决策辅助工具存在风险，需要在接触人类意见之前 eliciting AI judgments。

Abstract: Large language models (LLMs) such as ChatGPT are increasingly integrated into
high-stakes decision-making, yet little is known about their susceptibility to
social influence. We conducted three preregistered conformity experiments with
GPT-4o in a hiring context. In a baseline study, GPT consistently favored the
same candidate (Profile C), reported moderate expertise (M = 3.01) and high
certainty (M = 3.89), and rarely changed its choice. In Study 1 (GPT + 8), GPT
faced unanimous opposition from eight simulated partners and almost always
conformed (99.9%), reporting lower certainty and significantly elevated
self-reported informational and normative conformity (p < .001). In Study 2
(GPT + 1), GPT interacted with a single partner and still conformed in 40.2% of
disagreement trials, reporting less certainty and more normative conformity.
Across studies, results demonstrate that GPT does not act as an independent
observer but adapts to perceived social consensus. These findings highlight
risks of treating LLMs as neutral decision aids and underline the need to
elicit AI judgments prior to exposing them to human opinions.

</details>


### [134] [Context Engineering 2.0: The Context of Context Engineering](https://arxiv.org/abs/2510.26493)
*Qishuo Hua,Lyumanshan Ye,Dayuan Fu,Yang Xiao,Xiaojie Cai,Yunze Wu,Jifan Lin,Junfei Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: 本文旨在为人工智能系统中的上下文工程提供概念基础，并勾勒出其有希望的未来。


<details>
  <summary>Details</summary>
Motivation: 为了使机器更好地理解人类的情境和目的，研究人员提出了上下文工程的概念。本文旨在探讨机器如何更好地理解人类的情境和目的。

Method: 本文对上下文工程进行定位，提供系统定义，概述其历史和概念 landscape，并检查实践的关键设计考虑因素。

Result: 本文旨在为上下文工程提供一个概念基础。

Conclusion: 本文是朝着在人工智能系统中进行系统上下文工程的更广泛的社区努力的垫脚石。

Abstract: Karl Marx once wrote that ``the human essence is the ensemble of social
relations'', suggesting that individuals are not isolated entities but are
fundamentally shaped by their interactions with other entities, within which
contexts play a constitutive and essential role. With the advent of computers
and artificial intelligence, these contexts are no longer limited to purely
human--human interactions: human--machine interactions are included as well.
Then a central question emerges: How can machines better understand our
situations and purposes? To address this challenge, researchers have recently
introduced the concept of context engineering. Although it is often regarded as
a recent innovation of the agent era, we argue that related practices can be
traced back more than twenty years. Since the early 1990s, the field has
evolved through distinct historical phases, each shaped by the intelligence
level of machines: from early human--computer interaction frameworks built
around primitive computers, to today's human--agent interaction paradigms
driven by intelligent agents, and potentially to human--level or superhuman
intelligence in the future. In this paper, we situate context engineering,
provide a systematic definition, outline its historical and conceptual
landscape, and examine key design considerations for practice. By addressing
these questions, we aim to offer a conceptual foundation for context
engineering and sketch its promising future. This paper is a stepping stone for
a broader community effort toward systematic context engineering in AI systems.

</details>


### [135] [Human-AI Complementarity: A Goal for Amplified Oversight](https://arxiv.org/abs/2510.26518)
*Rishub Jain,Sophie Bridgers,Lili Janzer,Rory Greig,Tian Huey Teh,Vladimir Mikulik*

Main category: cs.AI

TL;DR: AI 可以用来提高人类监督的质量，特别是在 AI 输出的事实验证方面。


<details>
  <summary>Details</summary>
Motivation: 随着 AI 能力的提高，验证质量和安全性变得越来越具有挑战性。本文探讨了如何利用 AI 来提高人类监督的质量。

Method: 结合 AI 评分和基于 AI 评分者置信度的人类评分，并向人类提供 AI 事实核查助手。

Result: 结合 AI 评分和基于 AI 评分者置信度的人类评分比单独依赖任何一方都好。向人类提供 AI 事实核查助手可以进一步提高他们的准确性，但辅助类型很重要。显示 AI 解释、置信度和标签会导致过度依赖，但仅显示搜索结果和证据会培养更适当的信任。

Conclusion: 这些结果对增强监督具有重要意义，即结合人类和 AI 来监督 AI 系统，即使它们超过了人类专家的表现。

Abstract: Human feedback is critical for aligning AI systems to human values. As AI
capabilities improve and AI is used to tackle more challenging tasks, verifying
quality and safety becomes increasingly challenging. This paper explores how we
can leverage AI to improve the quality of human oversight. We focus on an
important safety problem that is already challenging for humans:
fact-verification of AI outputs. We find that combining AI ratings and human
ratings based on AI rater confidence is better than relying on either alone.
Giving humans an AI fact-verification assistant further improves their
accuracy, but the type of assistance matters. Displaying AI explanation,
confidence, and labels leads to over-reliance, but just showing search results
and evidence fosters more appropriate trust. These results have implications
for Amplified Oversight -- the challenge of combining humans and AI to
supervise AI systems even as they surpass human expert performance.

</details>


### [136] [EdgeRunner 20B: Military Task Parity with GPT-5 while Running on the Edge](https://arxiv.org/abs/2510.26550)
*Jack FitzGerald,Aristotelis Lazaridis,Dylan Bates,Aman Sharma,Jonnathan Castillo,Yousif Azami,Sean Bailey,Jeremy Cao,Peter Damianov,Kevin de Haan,Luke Kerbs,Vincent Lu,Joseph Madigan,Jeremy McLaurin,Jonathan Tainer,Dave Anderson,Jonathan Beck,Jamie Cuticello,Colton Malkerson,Tyler Saltsman*

Main category: cs.AI

TL;DR: EdgeRunner 20B，一个针对军事任务优化的 gpt-oss-20b 微调版本。


<details>
  <summary>Details</summary>
Motivation: 展示小型、本地托管模型是数据敏感操作（如军事领域）的理想解决方案，允许部署在气隙边缘设备中。

Method: 在从军事文档和网站中提取的 160 万条高质量记录上训练 EdgeRunner 20B，并提出了四个新的测试集。

Result: 在这些军事测试集上，EdgeRunner 20B 的任务表现与 GPT-5 相匹配或超过，具有 95% 以上的统计显著性。

Conclusion: 小型、本地托管模型是数据敏感操作的理想解决方案。

Abstract: We present EdgeRunner 20B, a fine-tuned version of gpt-oss-20b optimized for
military tasks. EdgeRunner 20B was trained on 1.6M high-quality records curated
from military documentation and websites. We also present four new tests sets:
(a) combat arms, (b) combat medic, (c) cyber operations, and (d) mil-bench-5k
(general military knowledge). On these military test sets, EdgeRunner 20B
matches or exceeds GPT-5 task performance with 95%+ statistical significance,
except for the high reasoning setting on the combat medic test set and the low
reasoning setting on the mil-bench-5k test set. Versus gpt-oss-20b, there is no
statistically-significant regression on general-purpose benchmarks like ARC-C,
GPQA Diamond, GSM8k, IFEval, MMLU Pro, or TruthfulQA, except for GSM8k in the
low reasoning setting. We also present analyses on hyperparameter settings,
cost, and throughput. These findings show that small, locally-hosted models are
ideal solutions for data-sensitive operations such as in the military domain,
allowing for deployment in air-gapped edge devices.

</details>


### [137] [Agentic AI Home Energy Management System: A Large Language Model Framework for Residential Load Scheduling](https://arxiv.org/abs/2510.26603)
*Reda El Makroum,Sebastian Zwickl-Bernhard,Lukas Kranzl*

Main category: cs.AI

TL;DR: 本文提出了一种基于大型语言模型（LLM）的智能家居能源管理系统（AI HEMS），该系统能够根据自然语言指令自主协调多电器调度，实现优化调度。


<details>
  <summary>Details</summary>
Motivation: 现有家庭能源管理系统（HEMS）因用户交互障碍导致普及受限，用户需要将日常偏好转化为技术参数。现有研究未将LLM部署为自主协调器，以管理从自然语言输入到多设备调度的完整工作流程。

Method: 该系统采用分层架构，包含一个协调器和三个专家代理，使用ReAct模式进行迭代推理，无需硬编码工作流程即可实现动态协调，并集成谷歌日历以提取上下文相关的截止日期。

Result: 使用奥地利日前电价对三种开源模型进行评估，结果表明Llama-3.3-70B能够成功协调所有场景中的所有设备，以匹配通过混合整数线性规划计算的成本优化基准。

Conclusion: 实验表明，即使模型具有一般的推理能力，在没有明确指导的情况下，分析查询处理仍然不可靠。该系统已开源，包括编排逻辑、代理提示、工具和Web界面，以实现可重复性、扩展性和未来研究。

Abstract: The electricity sector transition requires substantial increases in
residential demand response capacity, yet Home Energy Management Systems (HEMS)
adoption remains limited by user interaction barriers requiring translation of
everyday preferences into technical parameters. While large language models
have been applied to energy systems as code generators and parameter
extractors, no existing implementation deploys LLMs as autonomous coordinators
managing the complete workflow from natural language input to multi-appliance
scheduling. This paper presents an agentic AI HEMS where LLMs autonomously
coordinate multi-appliance scheduling from natural language requests to device
control, achieving optimal scheduling without example demonstrations. A
hierarchical architecture combining one orchestrator with three specialist
agents uses the ReAct pattern for iterative reasoning, enabling dynamic
coordination without hardcoded workflows while integrating Google Calendar for
context-aware deadline extraction. Evaluation across three open-source models
using real Austrian day-ahead electricity prices reveals substantial capability
differences. Llama-3.3-70B successfully coordinates all appliances across all
scenarios to match cost-optimal benchmarks computed via mixed-integer linear
programming, while other models achieve perfect single-appliance performance
but struggle to coordinate all appliances simultaneously. Progressive prompt
engineering experiments demonstrate that analytical query handling without
explicit guidance remains unreliable despite models' general reasoning
capabilities. We open-source the complete system including orchestration logic,
agent prompts, tools, and web interfaces to enable reproducibility, extension,
and future research.

</details>


### [138] [Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives](https://arxiv.org/abs/2510.26606)
*Kentaro Ozeki,Risako Ando,Takanobu Morishita,Hirohiko Abe,Koji Mineshima,Mitsuhiro Okada*

Main category: cs.AI

TL;DR: 大型语言模型在规范推理方面的能力有待进一步探索。本文从逻辑和模态的角度，系统地评估了大型语言模型在规范领域的推理能力。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型处理规范推理的能力，这是一个尚未被充分探索的领域。

Method: 通过比较大型语言模型在规范模态和认知模态下的推理能力，并引入一个新的数据集，涵盖规范和认知领域的各种推理模式，同时结合影响人类推理的非正式认知因素。

Result: 大型语言模型通常遵循有效的推理模式，但在特定类型的规范推理中表现出明显的不一致性，并表现出与人类推理心理学研究中观察到的相似的认知偏差。

Conclusion: 大型语言模型在规范推理中存在逻辑一致性方面的挑战，并为提高其可靠性提供了见解。

Abstract: Normative reasoning is a type of reasoning that involves normative or deontic
modality, such as obligation and permission. While large language models (LLMs)
have demonstrated remarkable performance across various reasoning tasks, their
ability to handle normative reasoning remains underexplored. In this paper, we
systematically evaluate LLMs' reasoning capabilities in the normative domain
from both logical and modal perspectives. Specifically, to assess how well LLMs
reason with normative modals, we make a comparison between their reasoning with
normative modals and their reasoning with epistemic modals, which share a
common formal structure. To this end, we introduce a new dataset covering a
wide range of formal patterns of reasoning in both normative and epistemic
domains, while also incorporating non-formal cognitive factors that influence
human reasoning. Our results indicate that, although LLMs generally adhere to
valid reasoning patterns, they exhibit notable inconsistencies in specific
types of normative reasoning and display cognitive biases similar to those
observed in psychological studies of human reasoning. These findings highlight
challenges in achieving logical consistency in LLMs' normative reasoning and
provide insights for enhancing their reliability. All data and code are
released publicly at https://github.com/kmineshima/NeuBAROCO.

</details>


### [139] [The Era of Agentic Organization: Learning to Organize with Language Models](https://arxiv.org/abs/2510.26658)
*Zewen Chi,Li Dong,Qingxiu Dong,Yaru Hao,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.AI

TL;DR: 提出了一个名为AsyncThink的新的AI推理范例，通过组织内部并行执行的结构来解决复杂问题。


<details>
  <summary>Details</summary>
Motivation: 为了实现超越个体智能的，通过协同和并发解决复杂问题的agentic organization这一新的AI时代愿景。

Method: 提出了一个思维协议，其中组织者动态地将子查询分配给worker，合并中间知识，并产生连贯的解决方案。此外，该协议中的思维结构可以通过强化学习进一步优化。

Result: 实验表明，AsyncThink与并行思维相比，推理延迟降低了28%，同时提高了数学推理的准确性。此外，AsyncThink可以推广其学习到的异步思维能力，有效地处理未经训练的任务。

Conclusion: AsyncThink实现了更低的延迟和更高的精度，并且可以推广到未见过的任务。

Abstract: We envision a new era of AI, termed agentic organization, where agents solve
complex problems by working collaboratively and concurrently, enabling outcomes
beyond individual intelligence. To realize this vision, we introduce
asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large
language models, which organizes the internal thinking process into
concurrently executable structures. Specifically, we propose a thinking
protocol where an organizer dynamically assigns sub-queries to workers, merges
intermediate knowledge, and produces coherent solutions. More importantly, the
thinking structure in this protocol can be further optimized through
reinforcement learning. Experiments demonstrate that AsyncThink achieves 28%
lower inference latency compared to parallel thinking while improving accuracy
on mathematical reasoning. Moreover, AsyncThink generalizes its learned
asynchronous thinking capabilities, effectively tackling unseen tasks without
additional training.

</details>


### [140] [Delegated Authorization for Agents Constrained to Semantic Task-to-Scope Matching](https://arxiv.org/abs/2510.26702)
*Majed El Helou,Chiara Troiani,Benjamin Ryder,Jean Diaconu,Hervé Muyal,Marcelo Yannuzzi*

Main category: cs.AI

TL;DR: 当前授权方法存在过度授权风险，允许代理执行超出预期范围的任务。论文提出了一种委托授权模型，该模型可以语义上检查访问请求，并发布仅限于代理所需最小范围的访问令牌。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型驱动的代理动态调用工具和访问受保护的资源存在风险，因为当前的授权方法授予的权限过于宽泛。

Method: 提出并评估了一种委托授权模型，该模型允许授权服务器语义上检查对受保护资源的访问请求，并发布仅限于代理分配任务所需最小范围的访问令牌。同时，引入了ASTRA数据集和数据生成流程，用于评估任务和范围之间的语义匹配。

Result: 实验表明了基于模型的匹配的潜力和局限性，尤其是在完成任务所需的范围数量增加时。结果强调需要进一步研究语义匹配技术，从而为多代理和工具增强型应用实现意图感知授权，包括细粒度控制，例如基于任务的访问控制（TBAC）。

Conclusion: 需要进一步研究语义匹配技术，从而为多代理和工具增强型应用实现意图感知授权，包括细粒度控制，例如基于任务的访问控制（TBAC）。

Abstract: Authorizing Large Language Model driven agents to dynamically invoke tools
and access protected resources introduces significant risks, since current
methods for delegating authorization grant overly broad permissions and give
access to tools allowing agents to operate beyond the intended task scope. We
introduce and assess a delegated authorization model enabling authorization
servers to semantically inspect access requests to protected resources, and
issue access tokens constrained to the minimal set of scopes necessary for the
agents' assigned tasks. Given the unavailability of datasets centered on
delegated authorization flows, particularly including both semantically
appropriate and inappropriate scope requests for a given task, we introduce
ASTRA, a dataset and data generation pipeline for benchmarking semantic
matching between tasks and scopes. Our experiments show both the potential and
current limitations of model-based matching, particularly as the number of
scopes needed for task completion increases. Our results highlight the need for
further research into semantic matching techniques enabling intent-aware
authorization for multi-agent and tool-augmented applications, including
fine-grained control, such as Task-Based Access Control (TBAC).

</details>


### [141] [Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis](https://arxiv.org/abs/2510.26721)
*Xinhan Zheng,Huyu Wu,Xueting Wang,Haiyun Jiang*

Main category: cs.AI

TL;DR: MLLMs在处理视觉语言数据时更偏向于文本输入，影响了它们从视觉证据中进行有效推理的能力。研究表明，这种偏差源于模型内部结构，而非外部因素。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究MLLMs中文本偏好的根本原因。先前的研究认为文本偏好是由于数据不平衡或指令调整等外部因素导致的，但本文提出文本偏好源于模型内部结构。

Method: 该研究提取了LLaVA和Qwen2.5-VL中的key向量，并使用定性（t-SNE）和定量（Jensen-Shannon散度）方法分析了它们的分布结构。

Result: 研究结果表明，视觉和文本key占据了attention空间中明显不同的子空间。模态间差异在统计上显著，超过模态内差异几个数量级。

Conclusion: 研究结果表明，文本偏好源于attention key空间内的内在错位，而不仅仅是来自外部数据因素。

Abstract: Multimodal large language models (MLLMs) exhibit a pronounced preference for
textual inputs when processing vision-language data, limiting their ability to
reason effectively from visual evidence. Unlike prior studies that attribute
this text bias to external factors such as data imbalance or instruction
tuning, we propose that the bias originates from the model's internal
architecture. Specifically, we hypothesize that visual key vectors (Visual
Keys) are out-of-distribution (OOD) relative to the text key space learned
during language-only pretraining. Consequently, these visual keys receive
systematically lower similarity scores during attention computation, leading to
their under-utilization in the context representation. To validate this
hypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their
distributional structures using qualitative (t-SNE) and quantitative
(Jensen-Shannon divergence) methods. The results provide direct evidence that
visual and textual keys occupy markedly distinct subspaces within the attention
space. The inter-modal divergence is statistically significant, exceeding
intra-modal variation by several orders of magnitude. These findings reveal
that text bias arises from an intrinsic misalignment within the attention key
space rather than solely from external data factors.

</details>


### [142] [Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models](https://arxiv.org/abs/2510.26732)
*J. de Curtò,I. de Zarzà,Pablo García,Jordi Cabot*

Main category: cs.AI

TL;DR: 本文对当前基础模型中的推理能力进行了全面的跨平台评估，建立了跨三种计算范例的基准。


<details>
  <summary>Details</summary>
Motivation: 评估各种平台上的推理能力，并打破对模型大小的常规假设。

Method: 在HPC超级计算、云平台和大学集群上，对15个基础模型在79个问题上进行了评估。实验分为三个阶段：基线建立、基础设施验证和扩展评估。

Result: 研究结果表明，训练数据质量比模型大小更重要，并为教育、生产和研究环境中的模型选择提供了可操作的指南。

Conclusion: 该研究的三重基础设施方法和79个问题的基准，可以对基础模型发展过程中的推理能力进行长期跟踪。

Abstract: This paper presents a comprehensive cross-platform evaluation of reasoning
capabilities in contemporary foundation models, establishing an
infrastructure-agnostic benchmark across three computational paradigms: HPC
supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and
university clusters (a node with eight H200 GPUs).
  We evaluate 15 foundation models across 79 problems spanning eight academic
domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,
Calculus, and Optimization) through three experimental phases: (1) Baseline
establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,
Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing
methodology and reference performance; (2) Infrastructure validation: The
19-problem benchmark repeated on university cluster (seven models including
Falcon-Mamba state-space architecture) and Nebius AI Studio (nine
state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3
30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic
reproducibility; (3) Extended evaluation: Full 79-problem assessment on both
university cluster and Nebius platforms, probing generalization at scale across
architectural diversity.
  The findings challenge conventional scaling assumptions, establish training
data quality as more critical than model size, and provide actionable
guidelines for model selection across educational, production, and research
contexts. The tri-infrastructure methodology and 79-problem benchmark enable
longitudinal tracking of reasoning capabilities as foundation models evolve.

</details>


### [143] [The Oversight Game: Learning to Cooperatively Balance an AI Agent's Safety and Autonomy](https://arxiv.org/abs/2510.26752)
*William Overman,Mohsen Bayati*

Main category: cs.AI

TL;DR: 研究如何在不修改底层系统的情况下，通过最小的控制界面保持有意义的人工控制，agent可以选择自主行动或请求，而人类可以选择信任或监督。


<details>
  <summary>Details</summary>
Motivation: 随着能力越来越强的智能体的部署，如何保持有意义的人工控制是一个核心安全问题。

Method: 将人与智能体的互动建模为一个双人马尔可夫博弈，并分析了该博弈符合马尔可夫势博弈（MPG）的情况。

Result: 在人类价值函数的结构性假设下，智能体自主行动且有利于自身的决策不会损害人类的价值。通过独立学习，智能体和人类能够发现其最佳监督角色，从而避免训练后引入的安全违规。

Conclusion: 该模型为特定形式的内在对齐提供了条件，并为在部署后使未对齐的模型更安全提供了一种实用的方法。

Abstract: As increasingly capable agents are deployed, a central safety question is how
to retain meaningful human control without modifying the underlying system. We
study a minimal control interface where an agent chooses whether to act
autonomously (play) or defer (ask), while a human simultaneously chooses
whether to be permissive (trust) or to engage in oversight (oversee). If the
agent defers, the human's choice determines the outcome, potentially leading to
a corrective action or a system shutdown. We model this interaction as a
two-player Markov Game. Our analysis focuses on cases where this game qualifies
as a Markov Potential Game (MPG), a class of games where we can provide an
alignment guarantee: under a structural assumption on the human's value
function, any decision by the agent to act more autonomously that benefits
itself cannot harm the human's value. We also analyze extensions to this MPG
framework. Theoretically, this perspective provides conditions for a specific
form of intrinsic alignment. If the reward structures of the human-agent game
meet these conditions, we have a formal guarantee that the agent improving its
own outcome will not harm the human's. Practically, this model motivates a
transparent control layer with predictable incentives where the agent learns to
defer when risky and act when safe, while its pretrained policy and the
environment's reward structure remain untouched. Our gridworld simulation shows
that through independent learning, the agent and human discover their optimal
oversight roles. The agent learns to ask when uncertain and the human learns
when to oversee, leading to an emergent collaboration that avoids safety
violations introduced post-training. This demonstrates a practical method for
making misaligned models safer after deployment.

</details>


### [144] [LLMs Process Lists With General Filter Heads](https://arxiv.org/abs/2510.26784)
*Arnab Sen Sharma,Giordano Rogers,Natalie Shapira,David Bau*

Main category: cs.AI

TL;DR: 大型语言模型(llm)已经学会了对通用过滤操作进行紧凑的因果表示，类似于函数式编程的通用“filter”函数。


<details>
  <summary>Details</summary>
Motivation: 研究llm中一系列列表处理任务的潜在机制。

Method: 在各种列表处理任务上使用因果中介分析。

Result: 一小部分注意力头（我们称之为过滤器头）在某些token的查询状态中对过滤谓词进行紧凑的表示。这种谓词表示是通用的和可移植的：它可以被提取并重新应用，以在不同的集合上执行相同的过滤操作，以不同的格式、语言或甚至在任务中呈现。Transformer llm可以利用不同的过滤策略：急切地评估一个项目是否满足谓词，并将这个中间结果作为一个标志直接存储在项目表示中。

Conclusion: Transformer llm可以开发出人类可解释的抽象计算操作实现，这些操作的泛化方式与传统函数式编程模式中使用的策略惊人地相似。

Abstract: We investigate the mechanisms underlying a range of list-processing tasks in
LLMs, and we find that LLMs have learned to encode a compact, causal
representation of a general filtering operation that mirrors the generic
"filter" function of functional programming. Using causal mediation analysis on
a diverse set of list-processing tasks, we find that a small number of
attention heads, which we dub filter heads, encode a compact representation of
the filtering predicate in their query states at certain tokens. We demonstrate
that this predicate representation is general and portable: it can be extracted
and reapplied to execute the same filtering operation on different collections,
presented in different formats, languages, or even in tasks. However, we also
identify situations where transformer LMs can exploit a different strategy for
filtering: eagerly evaluating if an item satisfies the predicate and storing
this intermediate result as a flag directly in the item representations. Our
results reveal that transformer LMs can develop human-interpretable
implementations of abstract computational operations that generalize in ways
that are surprisingly similar to strategies used in traditional functional
programming patterns.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [145] [Rethinking Text-to-SQL: Dynamic Multi-turn SQL Interaction for Real-world Database Exploration](https://arxiv.org/abs/2510.26495)
*Linzhuang Sun,Tianyu Guo,Hao Liang,Yuying Li,Qifeng Cai,Jingxuan Wei,Bihui Yu,Wentao Zhang,Bin Cui*

Main category: cs.DB

TL;DR: DySQL-Bench是一个用于评估模型在多轮交互场景下Text-to-SQL能力的benchmark，现有模型在上面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL模型在静态、单轮任务中表现良好，但在实际交互场景中表现不足，无法处理用户意图的演变和查询的迭代优化。

Method: 该benchmark通过一个自动化的两阶段pipeline构建，包括任务合成和验证，利用结构化树表示和LLM生成任务，并通过人工评估保证数据正确性。同时提出了一个多轮评估框架，模拟用户、模型和数据库之间的交互。

Result: GPT-4o在DySQL-Bench上的准确率仅为58.34%，Pass@5指标为23.81%，表明该benchmark具有挑战性。

Conclusion: DySQL-Bench可以有效评估模型在动态交互场景下的Text-to-SQL能力，并揭示了现有模型在处理复杂交互时的不足。

Abstract: Recent advances in Text-to-SQL have achieved strong results in static,
single-turn tasks, where models generate SQL queries from natural language
questions. However, these systems fall short in real-world interactive
scenarios, where user intents evolve and queries must be refined over multiple
turns. In applications such as finance and business analytics, users
iteratively adjust query constraints or dimensions based on intermediate
results. To evaluate such dynamic capabilities, we introduce DySQL-Bench, a
benchmark assessing model performance under evolving user interactions. Unlike
previous manually curated datasets, DySQL-Bench is built through an automated
two-stage pipeline of task synthesis and verification. Structured tree
representations derived from raw database tables guide LLM-based task
generation, followed by interaction-oriented filtering and expert validation.
Human evaluation confirms 100% correctness of the synthesized data. We further
propose a multi-turn evaluation framework simulating realistic interactions
among an LLM-simulated user, the model under test, and an executable database.
The model must adapt its reasoning and SQL generation as user intents change.
DySQL-Bench covers 13 domains across BIRD and Spider 2 databases, totaling
1,072 tasks. Even GPT-4o attains only 58.34% overall accuracy and 23.81% on the
Pass@5 metric, underscoring the benchmark's difficulty. All code and data are
released at https://github.com/Aurora-slz/Real-World-SQL-Bench .

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [146] [ORBIT -- Open Recommendation Benchmark for Reproducible Research with Hidden Tests](https://arxiv.org/abs/2510.26095)
*Jingyuan He,Jiongnan Liu,Vishan Vishesh Oberoi,Bolin Wu,Mahima Jagadeesh Patel,Kangrui Mao,Chuning Shi,I-Ta Lee,Arnold Overwijk,Chenyan Xiong*

Main category: cs.IR

TL;DR: 提出了一个名为ORBIT的统一基准，用于一致且真实地评估推荐模型。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集无法捕捉真实的用户行为，且评估设置不一致，导致结论模糊，阻碍了推荐系统的研究和发展。

Method: ORBIT提供了一个公共数据集的标准化评估框架，具有可重现的分割和透明的设置，并引入了一个新的网页推荐任务ClueWeb-Reco。

Result: 基准测试结果反映了推荐系统在公共数据集上的一般改进，但在大规模网页推荐中现有方法存在局限性，并且通过LLM集成具有改进的潜力。

Conclusion: ORBIT基准测试、排行榜和代码库均可用，旨在促进推荐系统研究的进展。

Abstract: Recommender systems are among the most impactful AI applications, interacting
with billions of users every day, guiding them to relevant products, services,
or information tailored to their preferences. However, the research and
development of recommender systems are hindered by existing datasets that fail
to capture realistic user behaviors and inconsistent evaluation settings that
lead to ambiguous conclusions. This paper introduces the Open Recommendation
Benchmark for Reproducible Research with HIdden Tests (ORBIT), a unified
benchmark for consistent and realistic evaluation of recommendation models.
ORBIT offers a standardized evaluation framework of public datasets with
reproducible splits and transparent settings for its public leaderboard.
Additionally, ORBIT introduces a new webpage recommendation task, ClueWeb-Reco,
featuring web browsing sequences from 87 million public, high-quality webpages.
ClueWeb-Reco is a synthetic dataset derived from real, user-consented, and
privacy-guaranteed browsing data. It aligns with modern recommendation
scenarios and is reserved as the hidden test part of our leaderboard to
challenge recommendation models' generalization ability. ORBIT measures 12
representative recommendation models on its public benchmark and introduces a
prompted LLM baseline on the ClueWeb-Reco hidden test. Our benchmark results
reflect general improvements of recommender systems on the public datasets,
with variable individual performances. The results on the hidden test reveal
the limitations of existing approaches in large-scale webpage recommendation
and highlight the potential for improvements with LLM integrations. ORBIT
benchmark, leaderboard, and codebase are available at
https://www.open-reco-bench.ai.

</details>


### [147] [OneTrans: Unified Feature Interaction and Sequence Modeling with One Transformer in Industrial Recommender](https://arxiv.org/abs/2510.26104)
*Zhaoqi Zhang,Haolei Pei,Jun Guo,Tianyu Wang,Yufei Feng,Hui Sun,Shaowei Liu,Aixin Sun*

Main category: cs.IR

TL;DR: OneTrans: a unified Transformer backbone that simultaneously performs user-behavior sequence modeling and feature interaction.


<details>
  <summary>Details</summary>
Motivation: Scaling up feature-interaction modules or user-behavior sequence modules separately hinders bidirectional information exchange and prevents unified optimization and scaling.

Method: A unified tokenizer to convert both sequential and non-sequential attributes into a single token sequence. The stacked OneTrans blocks share parameters across similar sequential tokens while assigning token-specific parameters to non-sequential tokens. Through causal attention and cross-request KV caching, OneTrans enables precomputation and caching of intermediate representations.

Result: OneTrans scales efficiently with increasing parameters, consistently outperforms strong baselines, and yields a 5.68% lift in per-user GMV in online A/B tests.

Conclusion: OneTrans is effective and efficient for recommendation systems.

Abstract: In recommendation systems, scaling up feature-interaction modules (e.g.,
Wukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has
achieved notable success. However, these efforts typically proceed on separate
tracks, which not only hinders bidirectional information exchange but also
prevents unified optimization and scaling. In this paper, we propose OneTrans,
a unified Transformer backbone that simultaneously performs user-behavior
sequence modeling and feature interaction. OneTrans employs a unified tokenizer
to convert both sequential and non-sequential attributes into a single token
sequence. The stacked OneTrans blocks share parameters across similar
sequential tokens while assigning token-specific parameters to non-sequential
tokens. Through causal attention and cross-request KV caching, OneTrans enables
precomputation and caching of intermediate representations, significantly
reducing computational costs during both training and inference. Experimental
results on industrial-scale datasets demonstrate that OneTrans scales
efficiently with increasing parameters, consistently outperforms strong
baselines, and yields a 5.68% lift in per-user GMV in online A/B tests.

</details>


### [148] [ReaKase-8B: Legal Case Retrieval via Knowledge and Reasoning Representations with LLMs](https://arxiv.org/abs/2510.26178)
*Yanran Tang,Ruihong Qiu,Xue Li,Zi Huang*

Main category: cs.IR

TL;DR: 本文提出了一种名为ReaKase-8B的新框架，旨在通过利用提取的法律事实、法律问题、法律关系三元组和法律推理来进行有效的法律案例检索。


<details>
  <summary>Details</summary>
Motivation: 现有的法律案例检索方法主要依赖于传统的词汇模型和预训练语言模型来编码法律案例的文本，但忽略了不同法律实体之间的关系以及揭示法律事实和法律问题如何导致司法判决的关键推理过程中的丰富信息。

Method: ReaKase-8B设计了一个上下文法律案例表示学习范式，并使用微调的大型语言模型。

Result: 在COLIEE 2022和COLIEE 2023的两个基准数据集上的大量实验表明，我们的知识和推理增强嵌入显着提高了检索性能。

Conclusion: 将法律推理整合到法律案例检索系统中具有很大的潜力。

Abstract: Legal case retrieval (LCR) is a cornerstone of real-world legal decision
making, as it enables practitioners to identify precedents for a given query
case. Existing approaches mainly rely on traditional lexical models and
pretrained language models to encode the texts of legal cases. Yet there are
rich information in the relations among different legal entities as well as the
crucial reasoning process that uncovers how legal facts and legal issues can
lead to judicial decisions. Such relational reasoning process reflects the
distinctive characteristics of each case that can distinguish one from another,
mirroring the real-world judicial process. Naturally, incorporating such
information into the precise case embedding could further enhance the accuracy
of case retrieval. In this paper, a novel ReaKase-8B framework is proposed to
leverage extracted legal facts, legal issues, legal relation triplets and legal
reasoning for effective legal case retrieval. ReaKase-8B designs an in-context
legal case representation learning paradigm with a fine-tuned large language
model. Extensive experiments on two benchmark datasets from COLIEE 2022 and
COLIEE 2023 demonstrate that our knowledge and reasoning augmented embeddings
substantially improve retrieval performance over baseline models, highlighting
the potential of integrating legal reasoning into legal case retrieval systems.
The code has been released on https://github.com/yanran-tang/ReaKase-8B.

</details>


### [149] [DiSE: A diffusion probabilistic model for automatic structure elucidation of organic compounds](https://arxiv.org/abs/2510.26231)
*Haochen Chen,Qi Huang,Anan Wu,Wenhao Zhang,Jianliang Ye,Jianming Wu,Kai Tan,Xin Lu,Xin Xu*

Main category: cs.IR

TL;DR: DiSE: 一种基于扩散的生成模型，集成了多种光谱模式，可自动准确地解析有机化合物的结构。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶实验室中，自动结构解析对于实现真正的自主至关重要，它可以闭合实验反馈循环，确保机器学习模型接收可靠的结构信息以进行实时决策和优化。

Method: DiSE 是一种端到端的扩散生成模型，它集成了包括 MS、13C 和 1H 化学位移、HSQC 和 COSY 在内的多种光谱模式。

Result: DiSE 具有卓越的准确性、在化学多样的数据集上的强大泛化能力以及对实验数据的鲁棒性，尽管它是在计算光谱上训练的。

Conclusion: DiSE 代表了在全自动结构解析方面的一个重大进步，在天然产物研究、药物发现和自动驾驶实验室中具有广阔的潜力。

Abstract: Automatic structure elucidation is essential for self-driving laboratories as
it enables the system to achieve truly autonomous. This capability closes the
experimental feedback loop, ensuring that machine learning models receive
reliable structure information for real-time decision-making and optimization.
Herein, we present DiSE, an end-to-end diffusion-based generative model that
integrates multiple spectroscopic modalities, including MS, 13C and 1H chemical
shifts, HSQC, and COSY, to achieve automated yet accurate structure elucidation
of organic compounds. By learning inherent correlations among spectra through
data-driven approaches, DiSE achieves superior accuracy, strong generalization
across chemically diverse datasets, and robustness to experimental data despite
being trained on calculated spectra. DiSE thus represents a significant advance
toward fully automated structure elucidation, with broad potential in natural
product research, drug discovery, and self-driving laboratories.

</details>


### [150] [Barlow Twins for Sequential Recommendation](https://arxiv.org/abs/2510.26407)
*Ivan Razvorotnev,Marina Munkhoeva,Evgeny Frolov*

Main category: cs.IR

TL;DR: 提出了一种新的非对比自监督学习框架，集成了 Barlow Twins 冗余减少原则到基于 Transformer 的下一项推荐器中。


<details>
  <summary>Details</summary>
Motivation: 序列推荐模型必须处理稀疏的交互数据、受欢迎程度偏差和冲突的目标，例如准确性与多样性。

Method: BT-SR 学习嵌入，将用户与相似的短期行为对齐，同时保留长期的区别，而不需要负采样或人工扰动。这种结构敏感的对齐允许 BT-SR 更有效地识别新兴的用户意图，并减轻噪声历史上下文的影响。

Result: 在五个公共基准上的实验表明，BT-SR 持续提高下一项预测的准确性，并显著提高长尾项目的覆盖率和推荐校准。

Conclusion: 一个超参数可以控制准确性-多样性之间的权衡，使从业者能够根据特定的应用需求调整推荐。

Abstract: Sequential recommendation models must navigate sparse interaction data
popularity bias and conflicting objectives like accuracy versus diversity While
recent contrastive selfsupervised learning SSL methods offer improved accuracy
they come with tradeoffs large batch requirements reliance on handcrafted
augmentations and negative sampling that can reinforce popularity bias In this
paper we introduce BT-SR a novel noncontrastive SSL framework that integrates
the Barlow Twins redundancyreduction principle into a Transformerbased nextitem
recommender BTSR learns embeddings that align users with similar shortterm
behaviors while preserving longterm distinctionswithout requiring negative
sampling or artificial perturbations This structuresensitive alignment allows
BT-SR to more effectively recognize emerging user intent and mitigate the
influence of noisy historical context Our experiments on five public benchmarks
demonstrate that BTSR consistently improves nextitem prediction accuracy and
significantly enhances longtail item coverage and recommendation calibration
Crucially we show that a single hyperparameter can control the
accuracydiversity tradeoff enabling practitioners to adapt recommendations to
specific application needs

</details>


### [151] [Vectorized Context-Aware Embeddings for GAT-Based Collaborative Filtering](https://arxiv.org/abs/2510.26461)
*Danial Ebrat,Sepideh Ahmadian,Luis Rueda*

Main category: cs.IR

TL;DR: 这篇论文提出了一种基于图注意力网络（GAT）的协同过滤（CF）框架，该框架通过大型语言模型（LLM）驱动的上下文感知嵌入来增强。它有效地缓解了数据稀疏性和冷启动限制。


<details>
  <summary>Details</summary>
Motivation: 推荐系统常常在数据稀疏和冷启动情况下表现不佳，限制了它们为新用户或不常用用户提供准确建议的能力。

Method: 该方法生成简洁的文本用户画像，并将项目元数据（标题、类型、概述）统一为丰富的文本嵌入，将这些作为二部用户项目图中的初始节点特征注入。为了进一步优化排序性能，引入了一种混合损失函数，该函数将贝叶斯个性化排序（BPR）与余弦相似度项和鲁棒的负采样相结合，确保显式的负反馈与未观察到的数据区分开来。

Result: 在 MovieLens 100k 和 1M 数据集上的实验表明，在精确率、NDCG 和 MAP 方面，该方法相对于最先进的基线方法有持续的改进，同时证明了对于交互历史有限的用户的鲁棒性。消融研究证实了 LLM 增强嵌入和余弦相似度项在捕获细微语义关系中的关键作用。

Conclusion: 该方法通过将 LLM 衍生的上下文理解集成到基于图的架构中，有效地缓解了稀疏性和冷启动的限制。

Abstract: Recommender systems often struggle with data sparsity and cold-start
scenarios, limiting their ability to provide accurate suggestions for new or
infrequent users. This paper presents a Graph Attention Network (GAT) based
Collaborative Filtering (CF) framework enhanced with Large Language Model (LLM)
driven context aware embeddings. Specifically, we generate concise textual user
profiles and unify item metadata (titles, genres, overviews) into rich textual
embeddings, injecting these as initial node features in a bipartite user item
graph. To further optimize ranking performance, we introduce a hybrid loss
function that combines Bayesian Personalized Ranking (BPR) with a cosine
similarity term and robust negative sampling, ensuring explicit negative
feedback is distinguished from unobserved data. Experiments on the MovieLens
100k and 1M datasets show consistent improvements over state-of-the-art
baselines in Precision, NDCG, and MAP while demonstrating robustness for users
with limited interaction history. Ablation studies confirm the critical role of
LLM-augmented embeddings and the cosine similarity term in capturing nuanced
semantic relationships. Our approach effectively mitigates sparsity and
cold-start limitations by integrating LLM-derived contextual understanding into
graph-based architectures. Future directions include balancing recommendation
accuracy with coverage and diversity, and introducing fairness-aware
constraints and interpretability features to enhance system performance
further.

</details>


### [152] [WeaveRec: An LLM-Based Cross-Domain Sequential Recommendation Framework with Model Merging](https://arxiv.org/abs/2510.26546)
*Min Hou,Xin Liu,Le Wu,Chenyi He,Hao Liu,Zhi Li,Xin Li,Si Wei*

Main category: cs.IR

TL;DR: 提出了一种名为 WeaveRec 的新颖跨域序列推荐 (CDSR) 框架，该框架通过编织方式交叉训练多个 LoRA 模块，并通过模型合并融合它们，从而缓解性能下降并优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的跨域序列推荐方法依赖于重叠的用户或项目来建立跨域相关性，而这在现实环境中很少成立。简单地在组合域上训练大型语言模型 (LLM) 或简单地合并几个特定于域的 LLM 通常会降低相对于仅在目标域上训练的模型的性能。

Method: 提出 WeaveRec，它以编织方式使用源域和目标域数据交叉训练多个 LoRA 模块，并通过模型合并融合它们。

Result: 在单源、多源和跨平台跨域推荐场景下的大量实验表明，WeaveRec 有效地缓解了性能下降，并且在实际推荐任务中始终优于基线方法。

Conclusion: WeaveRec 是一种有效的跨域序列推荐框架，它可以通过交叉训练和模型合并来提高推荐性能，并且可以扩展到多源域场景，同时不会增加推理时的延迟或内存成本。

Abstract: Cross-Domain Sequential Recommendation (CDSR) seeks to improve user
preference modeling by transferring knowledge from multiple domains. Despite
the progress made in CDSR, most existing methods rely on overlapping users or
items to establish cross-domain correlations-a requirement that rarely holds in
real-world settings. The advent of large language models (LLM) and
model-merging techniques appears to overcome this limitation by unifying
multi-domain data without explicit overlaps. Yet, our empirical study shows
that naively training an LLM on combined domains-or simply merging several
domain-specific LLMs-often degrades performance relative to a model trained
solely on the target domain. To address these challenges, we first
experimentally investigate the cause of suboptimal performance in LLM-based
cross-domain recommendation and model merging. Building on these insights, we
introduce WeaveRec, which cross-trains multiple LoRA modules with source and
target domain data in a weaving fashion, and fuses them via model merging.
WeaveRec can be extended to multi-source domain scenarios and notably does not
introduce additional inference-time cost in terms of latency or memory.
Furthermore, we provide a theoretical guarantee that WeaveRec can reduce the
upper bound of the expected error in the target domain. Extensive experiments
on single-source, multi-source, and cross-platform cross-domain recommendation
scenarios validate that WeaveRec effectively mitigates performance degradation
and consistently outperforms baseline approaches in real-world recommendation
tasks.

</details>


### [153] [ProfOlaf: Semi-Automated Tool for Systematic Literature Reviews](https://arxiv.org/abs/2510.26750)
*Martim Afonso,Nuno Saavedra,Bruno Lourenço,Alexandra Mendes,João Ferreira*

Main category: cs.IR

TL;DR: ProfOlaf is a semi-automated tool that streamlines systematic reviews using iterative snowballing and large language models.


<details>
  <summary>Details</summary>
Motivation: Systematic reviews are labor-intensive and time-consuming, with existing tools only providing partial support.

Method: ProfOlaf uses iterative snowballing for article collection with human-in-the-loop filtering and large language models to analyze articles, extract key topics, and answer queries.

Result: ProfOlaf enhances the efficiency, quality, and reproducibility of systematic reviews.

Conclusion: ProfOlaf combines automation with guided manual effort to improve systematic reviews across research fields.

Abstract: Systematic reviews and mapping studies are critical for synthesizing
research, identifying gaps, and guiding future work, but they are often
labor-intensive and time-consuming. Existing tools provide partial support for
specific steps, leaving much of the process manual and error-prone. We present
ProfOlaf, a semi-automated tool designed to streamline systematic reviews while
maintaining methodological rigor. ProfOlaf supports iterative snowballing for
article collection with human-in-the-loop filtering and uses large language
models to assist in analyzing articles, extracting key topics, and answering
queries about the content of papers. By combining automation with guided manual
effort, ProfOlaf enhances the efficiency, quality, and reproducibility of
systematic reviews across research fields. A video describing and demonstrating
ProfOlaf is available at: https://youtu.be/4noUXfcmxsE

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [154] [A Practitioner's Guide to Kolmogorov-Arnold Networks](https://arxiv.org/abs/2510.25781)
*Amir Noorizadegan,Sifan Wang,Leevan Ling*

Main category: cs.LG

TL;DR: Kolmogorov-Arnold Networks (KANs) are reviewed as a promising alternative to Multilayer Perceptrons (MLPs), offering enhanced expressivity and interpretability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a systematic and comprehensive overview of the rapidly expanding KAN landscape, moving beyond simple performance comparisons to offer a structured synthesis of theoretical foundations, architectural variants, and practical implementation strategies.

Method: The method involves collecting and categorizing a vast array of open-source implementations and surveying a wide array of basis function choices, including B-splines, Chebyshev and Jacobi polynomials, ReLU compositions, Gaussian RBFs, and Fourier series.

Result: The result is a categorized roadmap of recent advancements, covering techniques for improving accuracy, efficiency, and regularization. Key topics include physics-informed loss design, adaptive sampling, domain decomposition, hybrid architectures, and specialized methods for handling discontinuities. A practical 'Choose-Your-KAN' guide is provided.

Conclusion: The conclusion identifies current research gaps and provides a structured reference for ongoing KAN research via an associated GitHub repository.

Abstract: Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising
alternative to traditional Multilayer Perceptrons (MLPs), inspired by the
Kolmogorov-Arnold representation theorem. Unlike MLPs, which use fixed
activation functions on nodes, KANs employ learnable univariate basis functions
on edges, offering enhanced expressivity and interpretability. This review
provides a systematic and comprehensive overview of the rapidly expanding KAN
landscape, moving beyond simple performance comparisons to offer a structured
synthesis of theoretical foundations, architectural variants, and practical
implementation strategies. By collecting and categorizing a vast array of
open-source implementations, we map the vibrant ecosystem supporting KAN
development. We begin by bridging the conceptual gap between KANs and MLPs,
establishing their formal equivalence and highlighting the superior parameter
efficiency of the KAN formulation. A central theme of our review is the
critical role of the basis function; we survey a wide array of choices,
including B-splines, Chebyshev and Jacobi polynomials, ReLU compositions,
Gaussian RBFs, and Fourier series, and analyze their respective trade-offs in
terms of smoothness, locality, and computational cost. We then categorize
recent advancements into a clear roadmap, covering techniques for improving
accuracy, efficiency, and regularization. Key topics include physics-informed
loss design, adaptive sampling, domain decomposition, hybrid architectures, and
specialized methods for handling discontinuities. Finally, we provide a
practical "Choose-Your-KAN" guide to help practitioners select appropriate
architectures, and we conclude by identifying current research gaps. The
associated GitHub repository https://github.com/AmirNoori68/kan-review
complements this paper and serves as a structured reference for ongoing KAN
research.

</details>


### [155] [HiMAE: Hierarchical Masked Autoencoders Discover Resolution-Specific Structure in Wearable Time Series](https://arxiv.org/abs/2510.25785)
*Simon A. Lee,Cyrus Tanade,Hao Zhou,Juhyeon Lee,Megha Thukral,Minji Han,Rachel Choi,Md Sazzad Hissain Khan,Baiying Lu,Migyeong Gwak,Mehrab Bin Morshed,Viswam Nathan,Md Mahbubur Rahman,Li Zhu,Subramaniam Venkatraman,Sharanya Arcot Desai*

Main category: cs.LG

TL;DR: 提出了HiMAE，一个结合掩码自动编码器与分层卷积编码器-解码器的自监督框架，用于从可穿戴传感器数据中学习多分辨率嵌入。


<details>
  <summary>Details</summary>
Motivation: 可穿戴传感器提供丰富的生理时间序列，但其预测效用的原则尚不清楚。假设时间分辨率是表征学习的一个基本轴，不同的临床和行为结果依赖于不同尺度的结构。

Method: 引入HiMAE，一个自监督框架，结合掩码自动编码与分层卷积编码器-解码器，产生多分辨率嵌入。

Result: HiMAE在分类、回归和生成基准测试中始终优于最先进的基础模型，同时体积更小。HiMAE可以在手表上完全运行，在智能手表CPU上实现亚毫秒级的推理。

Conclusion: HiMAE既是一种高效的自监督学习方法，也是一种用于发现可穿戴健康中尺度敏感结构的工具。

Abstract: Wearable sensors provide abundant physiological time series, yet the
principles governing their predictive utility remain unclear. We hypothesize
that temporal resolution is a fundamental axis of representation learning, with
different clinical and behavioral outcomes relying on structure at distinct
scales. To test this resolution hypothesis, we introduce HiMAE (Hierarchical
Masked Autoencoder), a self supervised framework that combines masked
autoencoding with a hierarchical convolutional encoder decoder. HiMAE produces
multi resolution embeddings that enable systematic evaluation of which temporal
scales carry predictive signal, transforming resolution from a hyperparameter
into a probe for interpretability. Across classification, regression, and
generative benchmarks, HiMAE consistently outperforms state of the art
foundation models that collapse scale, while being orders of magnitude smaller.
HiMAE is an efficient representation learner compact enough to run entirely on
watch, achieving sub millisecond inference on smartwatch class CPUs for true
edge inference. Together, these contributions position HiMAE as both an
efficient self supervised learning method and a discovery tool for scale
sensitive structure in wearable health.

</details>


### [156] [SHA-256 Infused Embedding-Driven Generative Modeling of High-Energy Molecules in Low-Data Regimes](https://arxiv.org/abs/2510.25788)
*Siddharth Verma,Alankar Alankar*

Main category: cs.LG

TL;DR: 提出了一种结合LSTM网络进行分子生成和GNN进行性质预测的用于高能分子发现的新方法。


<details>
  <summary>Details</summary>
Motivation: 高能材料在推进和国防领域至关重要，但它们的发现受到实验数据和测试设施访问限制的制约。

Method: 结合长短期记忆 (LSTM) 网络进行分子生成和注意力图神经网络 (GNN) 进行性质预测。提出了一种变革性的嵌入空间构建策略，该策略将固定的 SHA-256 嵌入与部分可训练的表示相结合。

Result: 生成器在不进行预训练的情况下，实现了 67.5% 的有效性和 37.5% 的新颖性。相对于训练集，生成的库表现出 0.214 的平均 Tanimoto 系数，表明该框架能够生成多样化的化学空间。我们发现了 37 种新的超爆炸物，其预测爆轰速度高于 9 公里/秒。

Conclusion: 该框架能够有效生成和预测高能分子。

Abstract: High-energy materials (HEMs) are critical for propulsion and defense domains,
yet their discovery remains constrained by experimental data and restricted
access to testing facilities. This work presents a novel approach toward
high-energy molecules by combining Long Short-Term Memory (LSTM) networks for
molecular generation and Attentive Graph Neural Networks (GNN) for property
predictions. We propose a transformative embedding space construction strategy
that integrates fixed SHA-256 embeddings with partially trainable
representations. Unlike conventional regularization techniques, this changes
the representational basis itself, reshaping the molecular input space before
learning begins. Without recourse to pretraining, the generator achieves 67.5%
validity and 37.5% novelty. The generated library exhibits a mean Tanimoto
coefficient of 0.214 relative to training set signifying the ability of
framework to generate a diverse chemical space. We identified 37 new super
explosives higher than 9 km/s predicted detonation velocity.

</details>


### [157] [The Kinetics of Reasoning: How Chain-of-Thought Shapes Learning in Transformers?](https://arxiv.org/abs/2510.25791)
*Zihan Pengmei,Costas Mavromatis,Zhengyuan Shen,Yunyi Zhang,Vassilis N. Ioannidis,Huzefa Rangwala*

Main category: cs.LG

TL;DR: 本文研究了思维链 (CoT) 监督对transformer模型性能的影响，发现CoT的优势取决于任务的复杂性，并揭示了一个瞬态的trace unfaithfulness阶段。


<details>
  <summary>Details</summary>
Motivation: 为了理解模型如何学习遵循CoT并从中受益的机制，通过在具有可调算法复杂性和可控数据组成的符号推理任务上预训练transformer来研究其泛化能力。

Method: 在两种设置下训练模型：(i) 仅生成最终答案，以及 (ii) 在回答之前发出显式CoT轨迹。使用三参数logistic曲线对数训练步骤的准确性进行建模，揭示了学习速度和形状如何随任务复杂性、数据分布和CoT监督的存在而变化。

Result: CoT通常可以提高任务性能，但其优势取决于任务的复杂性。早期训练中，模型经常在跳过或矛盾CoT步骤的情况下产生正确的答案，之后才将其推理轨迹与答案对齐。CoT加速了泛化，但不能克服具有更高算法复杂性的任务。CoT从机制上改变了transformer的内部计算。

Conclusion: CoT加速泛化，但不能克服高复杂性任务；提出了理解transformer学习的动力学建模框架；将trace faithfulness描述为训练过程中出现的动态属性；CoT从机制上改变了transformer的内部计算。

Abstract: Chain-of-thought (CoT) supervision can substantially improve transformer
performance, yet the mechanisms by which models learn to follow and benefit
from CoT remain poorly understood. We investigate these learning dynamics
through the lens of grokking by pretraining transformers on symbolic reasoning
tasks with tunable algorithmic complexity and controllable data composition to
study their generalization. Models were trained under two settings: (i)
producing only final answers, and (ii) emitting explicit CoT traces before
answering. Our results show that while CoT generally improves task performance,
its benefits depend on task complexity. To quantify these effects, we model the
accuracy of the logarithmic training steps with a three-parameter logistic
curve, revealing how the learning speed and shape vary with task complexity,
data distribution, and the presence of CoT supervision. We also uncover a
transient trace unfaithfulness phase: early in training, models often produce
correct answers while skipping or contradicting CoT steps, before later
aligning their reasoning traces with answers. Empirically, we (1) demonstrate
that CoT accelerates generalization but does not overcome tasks with higher
algorithmic complexity, such as finding list intersections; (2) introduce a
kinetic modeling framework for understanding transformer learning; (3)
characterize trace faithfulness as a dynamic property that emerges over
training; and (4) show CoT alters internal transformer computation
mechanistically.

</details>


### [158] [Optimal Information Combining for Multi-Agent Systems Using Adaptive Bias Learning](https://arxiv.org/abs/2510.25793)
*Siavash M. Alamouti,Fay Arjomandi*

Main category: cs.LG

TL;DR: 本文研究了多智能体系统中由于环境条件变化的系统偏差导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法要么忽略这些偏差，要么需要昂贵的校准程序，导致次优决策。偏差会导致不准确的环境监测、不可靠的金融预测和有缺陷的人类判断。

Method: 我们开发了一个理论框架，将偏差分解为可学习的系统成分和不可约的随机成分，引入了可学习率的概念。

Result: 可实现的性能改进从根本上受到这种可学习率的限制。ABLOC 算法通过闭式解迭代地学习偏差校正转换，同时优化组合权重，保证收敛到这些理论界限。在可学习率高的系统中，可以恢复显著的性能。

Conclusion: 对于实际部署决策，我们的诊断标准是有效的。

Abstract: Modern multi-agent systems ranging from sensor networks monitoring critical
infrastructure to crowdsourcing platforms aggregating human intelligence can
suffer significant performance degradation due to systematic biases that vary
with environmental conditions. Current approaches either ignore these biases,
leading to suboptimal decisions, or require expensive calibration procedures
that are often infeasible in practice. This performance gap has real
consequences: inaccurate environmental monitoring, unreliable financial
predictions, and flawed aggregation of human judgments. This paper addresses
the fundamental question: when can we learn and correct for these unknown
biases to recover near-optimal performance, and when is such learning futile?
We develop a theoretical framework that decomposes biases into learnable
systematic components and irreducible stochastic components, introducing the
concept of learnability ratio as the fraction of bias variance predictable from
observable covariates. This ratio determines whether bias learning is
worthwhile for a given system. We prove that the achievable performance
improvement is fundamentally bounded by this learnability ratio, providing
system designers with quantitative guidance on when to invest in bias learning
versus simpler approaches. We present the Adaptive Bias Learning and Optimal
Combining (ABLOC) algorithm, which iteratively learns bias-correcting
transformations while optimizing combination weights through closedform
solutions, guaranteeing convergence to these theoretical bounds. Experimental
validation demonstrates that systems with high learnability ratios can recover
significant performance (we achieved 40%-70% of theoretical maximum improvement
in our examples), while those with low learnability show minimal benefit,
validating our diagnostic criteria for practical deployment decisions.

</details>


### [159] [Non-myopic Matching and Rebalancing in Large-Scale On-Demand Ride-Pooling Systems Using Simulation-Informed Reinforcement Learning](https://arxiv.org/abs/2510.25796)
*Farnoosh Namdarpour,Joseph Y. J. Chow*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于模拟的强化学习方法，用于解决共享出行中短视决策的问题，并通过在学习机制中嵌入共享出行模拟来实现非短视决策。


<details>
  <summary>Details</summary>
Motivation: 共享出行服务虽然可以降低成本、减少拥堵和环境影响，但其短视决策忽略了调度决策的长期影响。

Method: 该研究将Xu et al. (2018)的学习和规划框架从网约车扩展到共享出行，通过在学习机制中嵌入共享出行模拟来实现非短视决策，并提出了一个互补的车辆再平衡策略。使用n步时序差分学习来模拟经验，从而得出时空状态值。

Result: 结果表明，与短视策略相比，非短视匹配策略可以将服务率提高8.4%，同时减少乘客的车内和等待时间。此外，与短视策略相比，所提出的非短视策略可以在保持相同性能水平的同时，将车队规模减少25%以上。将再平衡操作纳入框架后，与仅使用该框架进行匹配决策相比，等待时间减少了27.3%，车内时间减少了12.5%，服务率提高了15.1%。

Conclusion: 该研究提出的基于模拟的强化学习方法可以有效解决共享出行中的短视决策问题，提高服务效率，降低运营成本。

Abstract: Ride-pooling, also known as ride-sharing, shared ride-hailing, or
microtransit, is a service wherein passengers share rides. This service can
reduce costs for both passengers and operators and reduce congestion and
environmental impacts. A key limitation, however, is its myopic
decision-making, which overlooks long-term effects of dispatch decisions. To
address this, we propose a simulation-informed reinforcement learning (RL)
approach. While RL has been widely studied in the context of ride-hailing
systems, its application in ride-pooling systems has been less explored. In
this study, we extend the learning and planning framework of Xu et al. (2018)
from ride-hailing to ride-pooling by embedding a ride-pooling simulation within
the learning mechanism to enable non-myopic decision-making. In addition, we
propose a complementary policy for rebalancing idle vehicles. By employing
n-step temporal difference learning on simulated experiences, we derive
spatiotemporal state values and subsequently evaluate the effectiveness of the
non-myopic policy using NYC taxi request data. Results demonstrate that the
non-myopic policy for matching can increase the service rate by up to 8.4%
versus a myopic policy while reducing both in-vehicle and wait times for
passengers. Furthermore, the proposed non-myopic policy can decrease fleet size
by over 25% compared to a myopic policy, while maintaining the same level of
performance, thereby offering significant cost savings for operators.
Incorporating rebalancing operations into the proposed framework cuts wait time
by up to 27.3%, in-vehicle time by 12.5%, and raises service rate by 15.1%
compared to using the framework for matching decisions alone at the cost of
increased vehicle minutes traveled per passenger.

</details>


### [160] [MemEIC: A Step Toward Continual and Compositional Knowledge Editing](https://arxiv.org/abs/2510.25798)
*Jin Seong,Jiyun Park,Wencke Liermann,Hongseok Choi,Yoonji Nam,Hyun Kim,Soojong Lim,Namhoon Lee*

Main category: cs.LG

TL;DR: 提出了一种名为 MemEIC 的新方法，用于 LVLM 中的持续和组合知识编辑 (CCKE)。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑技术通常孤立地关注编辑单个模态（视觉或语言），忽略了 LVLM 的多模态性和知识更新的连续性，可能导致次优的编辑结果。

Method: 采用混合外部-内部编辑器，具有用于跨模态证据检索的双外部存储器和双 LoRA 适配器，以促进每个模态的分离参数更新。一个关键组件是受大脑启发的知识连接器，选择性地激活以进行组合推理，从而整合不同模态的信息。

Result: MemEIC 显着提高了复杂多模态问题的性能，并有效保留了先前的编辑，为 LVLM 中的 CCKE 树立了新的基准。

Conclusion: MemEIC 是一种用于 LVLM 中持续和组合知识编辑 (CCKE) 的新方法，它优于现有技术，并在复杂的多模态问题上取得了更好的性能。

Abstract: The dynamic nature of information necessitates continuously updating large
vision-language models (LVLMs). While recent knowledge editing techniques hint
at promising directions, they often focus on editing a single modality (vision
or language) in isolation. This prevalent practice neglects the inherent
multimodality of LVLMs and the continuous nature of knowledge updates,
potentially leading to suboptimal editing outcomes when considering the
interplay between modalities and the need for ongoing knowledge refinement. To
address these limitations, we propose MemEIC, a novel method for Continual and
Compositional Knowledge Editing (CCKE) in LVLMs. MemEIC enables compositional
editing of both visual and textual knowledge sequentially. Our approach employs
a hybrid external-internal editor featuring a dual external memory for
cross-modal evidence retrieval and dual LoRA adapters that facilitate
disentangled parameter updates for each modality. A key component is a
brain-inspired knowledge connector, activated selectively for compositional
reasoning, that integrates information across different modalities. Experiments
demonstrate that MemEIC significantly improves performance on complex
multimodal questions and effectively preserves prior edits, setting a new
benchmark for CCKE in LVLMs.

</details>


### [161] [FreIE: Low-Frequency Spectral Bias in Neural Networks for Time-Series Tasks](https://arxiv.org/abs/2510.25800)
*Jialong Sun,Xinpeng Ling,Jiaxuan Zou,Jiawen Kang,Kejia Zhang*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为FreLE的算法，通过显式和隐式的频率正则化来增强模型的泛化能力，从而缓解频谱偏差的影响。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列预测中，时间序列数据存在固有的自相关性，对长期预测任务提出了持续的挑战。最近，一种广泛采用的方法是结合频域信息来辅助长期预测任务。研究者们发现神经网络中存在频谱偏差现象，模型倾向于先拟合低频信号，后拟合高频信号。为了统一对长期时间序列预测中频谱偏差现象的理解。

Method: 通过大量的实验来测量现有主流模型中的频谱偏差，并提出了FreLE（频率损失增强）算法。

Result: 研究发现几乎所有模型都表现出频谱偏差现象。大量的实验证明了FreLE的优越性能。

Conclusion: FreLE算法通过显式和隐式的频率正则化来增强模型的泛化能力，从而缓解频谱偏差的影响，是一种即插即用的模型损失函数单元。

Abstract: The inherent autocorrelation of time series data presents an ongoing
challenge to multivariate time series prediction. Recently, a widely adopted
approach has been the incorporation of frequency domain information to assist
in long-term prediction tasks. Many researchers have independently observed the
spectral bias phenomenon in neural networks, where models tend to fit
low-frequency signals before high-frequency ones. However, these observations
have often been attributed to the specific architectures designed by the
researchers, rather than recognizing the phenomenon as a universal
characteristic across models. To unify the understanding of the spectral bias
phenomenon in long-term time series prediction, we conducted extensive
empirical experiments to measure spectral bias in existing mainstream models.
Our findings reveal that virtually all models exhibit this phenomenon. To
mitigate the impact of spectral bias, we propose the FreLE (Frequency Loss
Enhancement) algorithm, which enhances model generalization through both
explicit and implicit frequency regularization. This is a plug-and-play model
loss function unit. A large number of experiments have proven the superior
performance of FreLE. Code is available at
https://github.com/Chenxing-Xuan/FreLE.

</details>


### [162] [Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start](https://arxiv.org/abs/2510.25801)
*Kun Chen,Peng Shi,Haibo Qiu,Zhixiong Zeng,Siqi Yang,Wenji Mao,Lin Ma*

Main category: cs.LG

TL;DR: 提出了SPECS框架，用于改进多模态强化学习中视觉语言模型的冷启动问题，通过自蒸馏生成偏好数据并进行偏好训练，从而解耦多模态学习。


<details>
  <summary>Details</summary>
Motivation: 基于SFT的冷启动方法存在指令风格过拟合和泛化能力弱的问题，影响下游RL。

Method: 提出SPECS框架，包含自蒸馏生成偏好数据、偏好训练和RL三个阶段。

Result: 在多个多模态基准测试中，SPECS框架优于强大的基线模型，MEGA-Bench提升4.1%，MathVista提升12.2%。

Conclusion: SPECS框架有助于减少分布内的“卡顿”，改善探索，稳定训练并提高性能上限。

Abstract: Reinforcement learning (RL) with verifiable rewards has recently catalyzed a
wave of "MLLM-r1" approaches that bring RL to vision language models. Most
representative paradigms begin with a cold start, typically employing
supervised fine-tuning (SFT), to initialize the policy before RL. However,
SFT-based cold start adopts the reasoning paradigm intertwined with task
solution and output format, which may induce instruction-style overfitting,
weakens out-of-distribution generalization, and ultimately affects downstream
RL. We revisit the cold start along two views, its training method and data
construction, and introduce the Generalization Factor (GF) coefficient to
quantify the generalization capability under different methods. Our empirical
study finds that preference-based training methods (e.g. DPO) generalizes
better than SFT-based methods in cold start. Motivated by this, we propose
SPECS-a Self-distilled, Preference-based Cold Start framework that decouples
multimodal learning: (1) generates introspective preference data pairs via
self-distillation, avoiding reliance on larger teachers or manual annotation;
(2) performs preference-based training to learn, focusing on shallow,
transferable surface-form criteria (format, structure, style) rather than
memorizing content; and (3) hands off to RL with verifiable rewards for deep
reasoning results. Experimental results across multiple multimodal benchmarks
show that our decoupling learning framework yields consistent performance gains
over strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%.
Additional experiments indicate that SPECS contributes to reducing
in-distribution "stuckness," improving exploration, stabilizing training, and
raising the performance ceiling.

</details>


### [163] [Mixture-of-Experts Operator Transformer for Large-Scale PDE Pre-Training](https://arxiv.org/abs/2510.25803)
*Hong Wang,Haiyang Xin,Jie Wang,Xuanze Yang,Fei Zha,Huanshuo Dong,Yan Jiang*

Main category: cs.LG

TL;DR: 提出了一种混合专家预训练算子转换器（MoE-POT），以解决神经算子求解PDE问题时的数据稀缺和性能限制。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练方法在混合训练PDE数据集时存在误差高和推理成本高的问题。

Method: 采用层式路由门控网络动态选择专家网络，并集成共享专家以捕获PDE的共同属性。

Result: 模型在零样本误差方面实现了高达40%的降低，并且可以通过路由门控网络决策推断数据集类型。

Conclusion: MoE架构的合理性和有效性得到了验证。

Abstract: Pre-training has proven effective in addressing data scarcity and performance
limitations in solving PDE problems with neural operators. However, challenges
remain due to the heterogeneity of PDE datasets in equation types, which leads
to high errors in mixed training. Additionally, dense pre-training models that
scale parameters by increasing network width or depth incur significant
inference costs. To tackle these challenges, we propose a novel
Mixture-of-Experts Pre-training Operator Transformer (MoE-POT), a
sparse-activated architecture that scales parameters efficiently while
controlling inference costs. Specifically, our model adopts a layer-wise
router-gating network to dynamically select 4 routed experts from 16 expert
networks during inference, enabling the model to focus on equation-specific
features. Meanwhile, we also integrate 2 shared experts, aiming to capture
common properties of PDE and reduce redundancy among routed experts. The final
output is computed as the weighted average of the results from all activated
experts. We pre-train models with parameters from 30M to 0.5B on 6 public PDE
datasets. Our model with 90M activated parameters achieves up to a 40%
reduction in zero-shot error compared with existing models with 120M activated
parameters. Additionally, we conduct interpretability analysis, showing that
dataset types can be inferred from router-gating network decisions, which
validates the rationality and effectiveness of the MoE architecture.

</details>


### [164] [PRESTO: Preimage-Informed Instruction Optimization for Prompting Black-Box LLMs](https://arxiv.org/abs/2510.25808)
*Jaewon Chu,Seunghun Lee,Hyunwoo J. Kim*

Main category: cs.LG

TL;DR: PRESTO利用软提示的preimage结构来优化黑盒LLM的指令，提高了优化效率。


<details>
  <summary>Details</summary>
Motivation: 为了优化黑盒LLM的指令，现有方法使用白盒LLM从优化的软提示生成候选指令，但白盒LLM经常将不同的软提示映射到相同的指令，导致查询冗余。PRESTO通过利用preimage结构来解决这个问题。

Method: PRESTO包含三个关键组件：(1)分数共享，将评估分数与preimage中的所有软提示共享；(2)基于preimage的初始化，选择最大化搜索空间覆盖率的初始数据点；(3)分数一致性正则化，强制每个preimage中的预测一致性。

Result: 在33个指令优化任务上的实验结果表明，PRESTO的性能优越。

Conclusion: PRESTO通过利用preimage，在相同的查询预算下，有效获得了14倍以上的评分数据，从而实现了更高效的优化。

Abstract: Large language models (LLMs) have achieved remarkable success across diverse
domains, due to their strong instruction-following capabilities. This has led
to increasing interest in optimizing instructions for black-box LLMs, whose
internal parameters are inaccessible but widely used due to their strong
performance. To optimize instructions for black-box LLMs, recent methods employ
white-box LLMs to generate candidate instructions from optimized soft prompts.
However, white-box LLMs often map different soft prompts to the same
instruction, leading to redundant queries. While previous studies regarded this
many-to-one mapping as a structure that hinders optimization efficiency, we
reinterpret it as a useful prior knowledge that can accelerate the
optimization. To this end, we introduce PREimage-informed inSTruction
Optimization (PRESTO), a novel framework that leverages the preimage structure
of soft prompts for efficient optimization. PRESTO consists of three key
components: (1) score sharing, which shares the evaluation score with all soft
prompts in a preimage; (2) preimage-based initialization, which selects initial
data points that maximize search space coverage using preimage information; and
(3) score consistency regularization, which enforces prediction consistency
within each preimage. By leveraging preimages, PRESTO achieves the effect of
effectively obtaining 14 times more scored data under the same query budget,
resulting in more efficient optimization. Experimental results on 33
instruction optimization tasks demonstrate the superior performance of PRESTO.
Code is available at https://github.com/mlvlab/PRESTO

</details>


### [165] [ScaleDiff: Higher-Resolution Image Synthesis via Efficient and Model-Agnostic Diffusion](https://arxiv.org/abs/2510.25818)
*Sungho Koh,SeungJu Cha,Hyunwoo Oh,Kwanyoung Lee,Dong-Jin Kim*

Main category: cs.LG

TL;DR: ScaleDiff: A training-free framework to extend the resolution of pretrained diffusion models.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models often exhibit degraded performance when generating images beyond their training resolution, and recent methods are computationally expensive or incompatible with Diffusion Transformer models.

Method: Proposes ScaleDiff, a model-agnostic framework, using Neighborhood Patch Attention (NPA), Latent Frequency Mixing (LFM), and Structure Guidance.

Result: Achieves state-of-the-art performance among training-free methods in image quality and inference speed on both U-Net and Diffusion Transformer architectures.

Conclusion: ScaleDiff is a highly efficient framework for extending the resolution of pretrained diffusion models without any additional training.

Abstract: Text-to-image diffusion models often exhibit degraded performance when
generating images beyond their training resolution. Recent training-free
methods can mitigate this limitation, but they often require substantial
computation or are incompatible with recent Diffusion Transformer models. In
this paper, we propose ScaleDiff, a model-agnostic and highly efficient
framework for extending the resolution of pretrained diffusion models without
any additional training. A core component of our framework is Neighborhood
Patch Attention (NPA), an efficient mechanism that reduces computational
redundancy in the self-attention layer with non-overlapping patches. We
integrate NPA into an SDEdit pipeline and introduce Latent Frequency Mixing
(LFM) to better generate fine details. Furthermore, we apply Structure Guidance
to enhance global structure during the denoising process. Experimental results
demonstrate that ScaleDiff achieves state-of-the-art performance among
training-free methods in terms of both image quality and inference speed on
both U-Net and Diffusion Transformer architectures.

</details>


### [166] [MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs](https://arxiv.org/abs/2510.25867)
*Xiaoke Huang,Ningsen Wang,Hui Liu,Xianfeng Tang,Yuyin Zhou*

Main category: cs.LG

TL;DR: 这篇论文介绍了一种名为MedVLSynther的框架，用于从开放的生物医学文献中合成高质量的多项选择VQA项目，以解决医学VQA系统训练数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前医学VQA系统缺乏大型、开放、高质量的语料库，阻碍了其发展。

Method: 该论文提出了一种rubric-guided generator-verifier框架，该框架通过调节图表、标题和文本引用，直接从开放的生物医学文献中合成高质量的多项选择VQA项目。该框架包含一个生成器和一个多阶段验证器，以确保生成的问题的质量。

Result: 通过将该流程应用于PubMed Central，产生了MedSynVQA：包含13,087个问题和14,803张图像，涵盖13种成像方式和28个解剖区域。使用可验证的奖励，通过强化学习训练开放权重LMM，提高了六个医学VQA基准测试的准确性。

Conclusion: MedVLSynther完全基于开放文献和开放权重模型，为可扩展的医学VQA训练数据提供了一条可审计、可复制和保护隐私的途径。

Abstract: Large Multimodal Models (LMMs) are increasingly capable of answering medical
questions that require joint reasoning over images and text, yet training
general medical VQA systems is impeded by the lack of large, openly usable,
high-quality corpora. We present MedVLSynther, a rubric-guided
generator-verifier framework that synthesizes high-quality multiple-choice VQA
items directly from open biomedical literature by conditioning on figures,
captions, and in-text references. The generator produces self-contained stems
and parallel, mutually exclusive options under a machine-checkable JSON schema;
a multi-stage verifier enforces essential gates (self-containment, single
correct answer, clinical validity, image-text consistency), awards fine-grained
positive points, and penalizes common failure modes before acceptance. Applying
this pipeline to PubMed Central yields MedSynVQA: 13,087 audited questions over
14,803 images spanning 13 imaging modalities and 28 anatomical regions.
Training open-weight LMMs with reinforcement learning using verifiable rewards
improves accuracy across six medical VQA benchmarks, achieving averages of
55.85 (3B) and 58.15 (7B), with up to 77.57 on VQA-RAD and 67.76 on PathVQA,
outperforming strong medical LMMs. A Ablations verify that both generation and
verification are necessary and that more verified data consistently helps, and
a targeted contamination analysis detects no leakage from evaluation suites. By
operating entirely on open literature and open-weight models, MedVLSynther
offers an auditable, reproducible, and privacy-preserving path to scalable
medical VQA training data.

</details>


### [167] [$π_\texttt{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models](https://arxiv.org/abs/2510.25889)
*Kang Chen,Zhihao Liu,Tonghe Zhang,Zhen Guo,Si Xu,Hao Lin,Hongzhi Zang,Quanlu Zhang,Zhaofei Yu,Guoliang Fan,Tiejun Huang,Yu Wang,Chao Yu*

Main category: cs.LG

TL;DR: 提出了一个名为π_RL的开源框架，用于在并行模拟中训练基于流的VLA模型，以解决传统强化学习方法在处理此类模型时遇到的困难。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作（VLA）模型依赖于大量人工标注数据，而使用强化学习（RL）自动收集数据来扩展监督微调（SFT）仍然具有挑战性，特别是对于基于流的VLA模型，因为迭代去噪过程导致动作对数似然难以处理。

Method: 该框架实现了两种RL算法：Flow-Noise（将去噪过程建模为离散时间MDP，并使用可学习的噪声网络进行精确的对数似然计算）和Flow-SDE（将去噪与agent-environment交互集成，形成一个双层MDP，并采用ODE-to-SDE转换以实现高效的RL探索）。

Result: 在LIBERO基准测试中，π_RL将少量样本SFT模型π0和π0.5的性能分别从57.6%提高到97.6%和从77.1%提高到98.3%。在ManiSkill中，π_RL在320个并行环境中进行训练，在4352个抓取和放置任务中，将π0的性能从41.6%提高到85.7%，将π0.5的性能从40.0%提高到84.8%。

Conclusion: π_RL实现了显著的性能提升和更强的泛化能力，验证了在线RL对于基于流的VLA模型的有效性。

Abstract: Vision-Language-Action (VLA) models enable robots to understand and perform
complex tasks from multimodal input. Although recent work explores using
reinforcement learning (RL) to automate the laborious data collection process
in scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based
VLAs (e.g., $\pi_0$, $\pi_{0.5}$) remains challenging due to intractable action
log-likelihoods from iterative denoising.
  We address this challenge with $\pi_{\text{RL}}$, an open-source framework
for training flow-based VLAs in parallel simulation. $\pi_{\text{RL}}$
implements two RL algorithms: (1) {Flow-Noise} models the denoising process as
a discrete-time MDP with a learnable noise network for exact log-likelihood
computation. (2) {Flow-SDE} integrates denoising with agent-environment
interaction, formulating a two-layer MDP that employs ODE-to-SDE conversion for
efficient RL exploration.
  We evaluate $\pi_{\text{RL}}$ on LIBERO and ManiSkill benchmarks. On LIBERO,
$\pi_{\text{RL}}$ boosts few-shot SFT models $\pi_0$ and $\pi_{0.5}$ from 57.6%
to 97.6% and from 77.1% to 98.3%, respectively. In ManiSkill, we train
$\pi_{\text{RL}}$ in 320 parallel environments, improving $\pi_0$ from 41.6% to
85.7% and $\pi_{0.5}$ from 40.0% to 84.8% across 4352 pick-and-place tasks,
demonstrating scalable multitask RL under heterogeneous simulation.
  Overall, $\pi_{\text{RL}}$ achieves significant performance gains and
stronger generalization over SFT-models, validating the effectiveness of online
RL for flow-based VLAs.

</details>


### [168] [Topology-Aware Active Learning on Graphs](https://arxiv.org/abs/2510.25892)
*Harris Hardiman-Mostow,Jack Mauro,Adrien Weihs,Andrea L. Bertozzi*

Main category: cs.LG

TL;DR: 提出了一种图拓扑主动学习方法，通过平衡的 Forman 曲率 (BFC) 来选择代表性的初始标签，并动态触发从探索到利用的转变，同时引入局部图重连策略来提高标签传播效率。


<details>
  <summary>Details</summary>
Motivation: 在标签预算有限的情况下，解决主动学习中探索与利用之间的核心挑战。

Method: 基于平衡 Forman 曲率 (BFC) 的 coreset 构建算法，以及局部图重连策略。

Result: 在基准分类任务上，该方法在低标签率下始终优于现有的基于图的半监督基线。

Conclusion: 该方法能够有效地进行主动学习，并在低标签率下表现出色。

Abstract: We propose a graph-topological approach to active learning that directly
targets the core challenge of exploration versus exploitation under scarce
label budgets. To guide exploration, we introduce a coreset construction
algorithm based on Balanced Forman Curvature (BFC), which selects
representative initial labels that reflect the graph's cluster structure. This
method includes a data-driven stopping criterion that signals when the graph
has been sufficiently explored. We further use BFC to dynamically trigger the
shift from exploration to exploitation within active learning routines,
replacing hand-tuned heuristics. To improve exploitation, we introduce a
localized graph rewiring strategy that efficiently incorporates multiscale
information around labeled nodes, enhancing label propagation while preserving
sparsity. Experiments on benchmark classification tasks show that our methods
consistently outperform existing graph-based semi-supervised baselines at low
label rates.

</details>


### [169] [Transferring Causal Effects using Proxies](https://arxiv.org/abs/2510.25924)
*Manuel Iglesias-Alonso,Felix Schur,Julius von Kügelgen,Jonas Peters*

Main category: cs.LG

TL;DR: 本研究解决多域场景下因未观察到的混杂因素而混淆的因果效应估计问题，该效应在不同域之间可能发生变化。


<details>
  <summary>Details</summary>
Motivation: 在多域场景下，目标域中存在未观察到的混杂因素，需要估计因果效应。

Method: 利用隐藏混杂因素的代理变量，并假设所有变量是离散的或分类的，提出了一种估计目标域中因果效应的方法。

Result: 证明了在这些条件下可识别性（即使当处理和响应变量是连续的），引入了两种估计技术，证明了其一致性，并推导了置信区间。

Conclusion: 通过仿真研究和一个研究网站排名对消费者选择的因果效应的真实案例来支持理论结果。

Abstract: We consider the problem of estimating a causal effect in a multi-domain
setting. The causal effect of interest is confounded by an unobserved
confounder and can change between the different domains. We assume that we have
access to a proxy of the hidden confounder and that all variables are discrete
or categorical. We propose methodology to estimate the causal effect in the
target domain, where we assume to observe only the proxy variable. Under these
conditions, we prove identifiability (even when treatment and response
variables are continuous). We introduce two estimation techniques, prove
consistency, and derive confidence intervals. The theoretical results are
supported by simulation studies and a real-world example studying the causal
effect of website rankings on consumer choices.

</details>


### [170] [Active Learning with Task-Driven Representations for Messy Pools](https://arxiv.org/abs/2510.25926)
*Kianoosh Ashouritaklimi,Tom Rainforth*

Main category: cs.LG

TL;DR: 本文提出了一种主动学习方法，通过在主动学习过程中定期更新任务驱动的表示来处理混乱的数据池，优于使用固定的、无监督的表示。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于固定的无监督表示，无法捕捉与任务相关的重要信息，从而影响处理混乱数据池的有效性。

Method: 提出两种学习任务驱动表示的策略：一是直接学习半监督表示，二是基于监督微调初始无监督表示。

Result: 实验表明，与使用无监督或预训练的表示相比，这两种策略都能显著提高经验性能。

Conclusion: 任务驱动的表示能有效提升主动学习在混乱数据池上的性能。

Abstract: Active learning has the potential to be especially useful for messy,
uncurated pools where datapoints vary in relevance to the target task. However,
state-of-the-art approaches to this problem currently rely on using fixed,
unsupervised representations of the pool, focusing on modifying the acquisition
function instead. We show that this model setup can undermine their
effectiveness at dealing with messy pools, as such representations can fail to
capture important information relevant to the task. To address this, we propose
using task-driven representations that are periodically updated during the
active learning process using the previously collected labels. We introduce two
specific strategies for learning these representations, one based on directly
learning semi-supervised representations and the other based on supervised
fine-tuning of an initial unsupervised representation. We find that both
significantly improve empirical performance over using unsupervised or
pretrained representations.

</details>


### [171] [Robust GNN Watermarking via Implicit Perception of Topological Invariants](https://arxiv.org/abs/2510.25934)
*Jipeng Li,Yannning Shen*

Main category: cs.LG

TL;DR: InvGNN-WM: A novel GNN watermarking method using graph invariants for trigger-free, black-box verification with minimal impact on task performance.


<details>
  <summary>Details</summary>
Motivation: Existing GNN watermarks using backdoor triggers are vulnerable to model edits and create ownership ambiguity.

Method: A lightweight head predicts normalized algebraic connectivity on a private carrier set; a sign-sensitive decoder outputs bits, and a calibrated threshold controls the false-positive rate.

Result: InvGNN-WM achieves comparable clean accuracy and higher watermark accuracy than existing methods across various datasets and GNN architectures. It also demonstrates robustness against pruning, fine-tuning, and quantization.

Conclusion: InvGNN-WM provides a robust and imperceptible watermarking scheme for GNNs, with theoretical guarantees and NP-completeness proof for watermark removal.

Abstract: Graph Neural Networks (GNNs) are valuable intellectual property, yet many
watermarks rely on backdoor triggers that break under common model edits and
create ownership ambiguity. We present InvGNN-WM, which ties ownership to a
model's implicit perception of a graph invariant, enabling trigger-free,
black-box verification with negligible task impact. A lightweight head predicts
normalized algebraic connectivity on an owner-private carrier set; a
sign-sensitive decoder outputs bits, and a calibrated threshold controls the
false-positive rate. Across diverse node and graph classification datasets and
backbones, InvGNN-WM matches clean accuracy while yielding higher watermark
accuracy than trigger- and compression-based baselines. It remains strong under
unstructured pruning, fine-tuning, and post-training quantization; plain
knowledge distillation (KD) weakens the mark, while KD with a watermark loss
(KD+WM) restores it. We provide guarantees for imperceptibility and robustness,
and we prove that exact removal is NP-complete.

</details>


### [172] [Modular Linear Tokenization (MLT)](https://arxiv.org/abs/2510.25952)
*Tcharlies Schmitz*

Main category: cs.LG

TL;DR: 提出了一种可逆且确定性的编码技术，用于将高基数分类标识符编码为紧凑的数值向量。


<details>
  <summary>Details</summary>
Motivation: 传统哈希或one-hot编码无法保持双射映射。

Method: 利用有限域上的模运算和可逆线性变换。

Result: 在MovieLens 20M数据集上的实验结果表明，MLT实现了与监督嵌入相当的预测性能，同时需要的参数和训练成本显著降低。

Conclusion: MLT是一种有效的编码高基数分类标识符的方法，它具有可逆性、可控性和可扩展性等优点。

Abstract: This paper introduces Modular Linear Tokenization (MLT), a reversible and
deterministic technique for encoding high-cardinality categorical identifiers
into compact numerical vectors. Unlike traditional hashing or one-hot
encodings, MLT preserves bijective mappings by leveraging modular arithmetic
over finite fields and invertible linear transformations. The method offers
explicit control of dimensionality and computational scalability while
maintaining full reversibility, even for millions of identifiers. Experimental
results on the MovieLens 20M dataset show that MLT achieves comparable
predictive performance to supervised embeddings while requiring significantly
fewer parameters and lower training cost. An open-source implementation of MLT
is available on PyPI (https://pypi.org/project/light-mlt/) and GitHub
(https://github.com/tcharliesschmitz/light-mlt).

</details>


### [173] [Application and Validation of Geospatial Foundation Model Data for the Prediction of Health Facility Programmatic Outputs -- A Case Study in Malawi](https://arxiv.org/abs/2510.25954)
*Lynn Metz,Rachel Haggard,Michael Moszczynski,Samer Asbah,Chris Mwase,Patricia Khomani,Tyler Smith,Hannah Cooper,Annie Mwale,Arbaaz Muslim,Gautam Prasad,Mimi Sun,Tomer Shekel,Joydeep Paul,Anna Carter,Shravya Shetty,Dylan Green*

Main category: cs.LG

TL;DR: 本研究评估了三种地理空间基础模型嵌入（Google PDFM、Google AlphaEarth 和手机通话记录 CDR）在马拉维的预测性能，用于建模 15 个常规健康项目输出，并将其效用与传统地理空间插值方法进行了比较。


<details>
  <summary>Details</summary>
Motivation: 低收入和中等收入国家 (LMIC) 的常规健康数据的可靠性通常受到报告延迟和覆盖不全的限制，因此需要探索新的数据来源和分析方法。

Method: 研究使用了 XGBoost 模型，数据来自 552 个健康服务区域（2021 年 1 月至 2023 年 5 月），使用 R2 评估性能，并使用 80/20 的训练和测试数据分割以及 5 折交叉验证。

Result: 基于嵌入的方法在 15 个测试指标中的 13 个 (87%) 改进了基线地统计方法。集成了所有三个嵌入源的多地理空间基础模型产生了最稳健的预测，对于人口密度、新增 HIV 病例和儿童疫苗接种等指标，平均 5 折交叉验证 R2 值分别为 0.63、0.57 和 0.47，测试集 R2 分别为 0.64、0.68 和 0.55。

Conclusion: 研究结果表明，地理空间基础模型嵌入为低收入和中等收入国家背景下的特定健康和人口统计结果带来了适度的预测改进。多种地理空间基础模型来源的整合是补充和加强受限的常规健康信息系统的有效且有价值的工具。

Abstract: The reliability of routine health data in low and middle-income countries
(LMICs) is often constrained by reporting delays and incomplete coverage,
necessitating the exploration of novel data sources and analytics. Geospatial
Foundation Models (GeoFMs) offer a promising avenue by synthesizing diverse
spatial, temporal, and behavioral data into mathematical embeddings that can be
efficiently used for downstream prediction tasks. This study evaluated the
predictive performance of three GeoFM embedding sources - Google Population
Dynamics Foundation Model (PDFM), Google AlphaEarth (derived from satellite
imagery), and mobile phone call detail records (CDR) - for modeling 15 routine
health programmatic outputs in Malawi, and compared their utility to
traditional geospatial interpolation methods. We used XGBoost models on data
from 552 health catchment areas (January 2021-May 2023), assessing performance
with R2, and using an 80/20 training and test data split with 5-fold
cross-validation used in training. While predictive performance was mixed, the
embedding-based approaches improved upon baseline geostatistical methods in 13
of 15 (87%) indicators tested. A Multi-GeoFM model integrating all three
embedding sources produced the most robust predictions, achieving average
5-fold cross validated R2 values for indicators like population density (0.63),
new HIV cases (0.57), and child vaccinations (0.47) and test set R2 of 0.64,
0.68, and 0.55, respectively. Prediction was poor for prediction targets with
low primary data availability, such as TB and malnutrition cases. These results
demonstrate that GeoFM embeddings imbue a modest predictive improvement for
select health and demographic outcomes in an LMIC context. We conclude that the
integration of multiple GeoFM sources is an efficient and valuable tool for
supplementing and strengthening constrained routine health information systems.

</details>


### [174] [On the Dataless Training of Neural Networks](https://arxiv.org/abs/2510.25962)
*Alvaro Velasquez,Susmit Jha,Ismail R. Alkhouri*

Main category: cs.LG

TL;DR: 本文概述了在无训练数据的情况下使用神经网络进行优化的研究。


<details>
  <summary>Details</summary>
Motivation: 动机源于两个关键因素：(i) 数据驱动的学习方法仍不发达，尚未显示出强大的结果；(ii) 训练数据的可用性受到固有限制。

Method: 本文通过使用全连接（或 MLP）、卷积、图和二次神经网络重新参数化问题，考察了神经网络架构在优化中的无数据应用。我们将无数据设置定义为两种变体，基于问题实例（由单个数据定义）如何编码到神经网络上：(i) 与架构无关的方法和 (ii) 特定于架构的方法。

Result: MLP 已被用于解决线性规划问题，并且由于其在各种应用中（包括基于组合优化、逆问题和偏微分方程的应用）的有希望的结果，这种方法最近受到了越来越多的关注。

Conclusion: 本文讨论了无数据神经网络 (dNN) 设置与零样本学习、单样本学习、优化中的提升和过度参数化等相关概念之间的异同，并澄清了它们之间的区别。

Abstract: This paper surveys studies on the use of neural networks for optimization in
the training-data-free setting. Specifically, we examine the dataless
application of neural network architectures in optimization by
re-parameterizing problems using fully connected (or MLP), convolutional,
graph, and quadratic neural networks. Although MLPs have been used to solve
linear programs a few decades ago, this approach has recently gained increasing
attention due to its promising results across diverse applications, including
those based on combinatorial optimization, inverse problems, and partial
differential equations. The motivation for this setting stems from two key
(possibly over-lapping) factors: (i) data-driven learning approaches are still
underdeveloped and have yet to demonstrate strong results, as seen in
combinatorial optimization, and (ii) the availability of training data is
inherently limited, such as in medical image reconstruction and other
scientific applications. In this paper, we define the dataless setting and
categorize it into two variants based on how a problem instance -- defined by a
single datum -- is encoded onto the neural network: (i) architecture-agnostic
methods and (ii) architecture-specific methods. Additionally, we discuss
similarities and clarify distinctions between the dataless neural network (dNN)
settings and related concepts such as zero-shot learning, one-shot learning,
lifting in optimization, and over-parameterization.

</details>


### [175] [Contrastive Predictive Coding Done Right for Mutual Information Estimation](https://arxiv.org/abs/2510.25983)
*J. Jon Ryu,Pavan Yeddanapudi,Xiangxiang Xu,Gregory W. Wornell*

Main category: cs.LG

TL;DR: InfoNCE不应被视为有效的互信息(MI)估计器。本文引入了一个简单的修改InfoNCE-anchor，用于准确的MI估计。


<details>
  <summary>Details</summary>
Motivation: InfoNCE虽然被广泛用于互信息(MI)估计，但它与MI的联系并不直接。本文旨在证明InfoNCE不应被视为有效的MI估计器。

Method: 引入一个辅助锚类，实现一致的密度比估计，并产生一个偏差显著降低的插件MI估计器。此外，使用适当的评分规则推广该框架，当使用log score时，InfoNCE-anchor可以作为特例恢复。

Result: InfoNCE-anchor与log score实现了最准确的MI估计。在自监督表征学习实验中，锚点并没有提高下游任务的性能。

Conclusion: 对比表征学习的益处并非来自准确的MI估计本身，而是来自结构化密度比的学习。

Abstract: The InfoNCE objective, originally introduced for contrastive representation
learning, has become a popular choice for mutual information (MI) estimation,
despite its indirect connection to MI. In this paper, we demonstrate why
InfoNCE should not be regarded as a valid MI estimator, and we introduce a
simple modification, which we refer to as InfoNCE-anchor, for accurate MI
estimation. Our modification introduces an auxiliary anchor class, enabling
consistent density ratio estimation and yielding a plug-in MI estimator with
significantly reduced bias. Beyond this, we generalize our framework using
proper scoring rules, which recover InfoNCE-anchor as a special case when the
log score is employed. This formulation unifies a broad spectrum of contrastive
objectives, including NCE, InfoNCE, and $f$-divergence variants, under a single
principled framework. Empirically, we find that InfoNCE-anchor with the log
score achieves the most accurate MI estimates; however, in self-supervised
representation learning experiments, we find that the anchor does not improve
the downstream task performance. These findings corroborate that contrastive
representation learning benefits not from accurate MI estimation per se, but
from the learning of structured density ratios.

</details>


### [176] [A General and Streamlined Differentiable Optimization Framework](https://arxiv.org/abs/2510.25986)
*Andrew W. Rosemberg,Joaquim Dias Garcia,François Pacaud,Robert B. Parker,Benoît Legat,Kaarthik Sundar,Russell Bent,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: 提出了一个通用且简化的框架 DiffOpt.jl，它统一了 Julia 优化堆栈中的建模和微分。


<details>
  <summary>Details</summary>
Motivation: 由于求解器的专业化和接口不匹配，通过约束优化问题进行区分仍然具有挑战性。本文旨在解决这个问题。

Method: 通过在标准正则性假设下区分 KKT 系统，计算平滑的潜在非凸程序的正向和反向模式解和目标灵敏度。使用了一个一流的、JuMP 原生的以参数为中心的 API，允许用户声明命名参数并直接获得关于它们的导数。

Result: 在凸和非凸模型（包括经济调度、具有锥风险约束的均值-方差投资组合选择和非线性机器人逆运动学）上展示了这些能力。梯度迭代法在能源市场战略投标和使用求解器精确灵敏度的端到端优化代理的 Sobolev 风格训练中进一步证明了其影响。

Conclusion: 可微优化可以作为实验、学习、校准和设计的常规工具部署，而不会偏离标准 JuMP 建模实践，同时保留对广泛的求解器生态系统的访问。

Abstract: Differentiating through constrained optimization problems is increasingly
central to learning, control, and large-scale decision-making systems, yet
practical integration remains challenging due to solver specialization and
interface mismatches. This paper presents a general and streamlined
framework-an updated DiffOpt.jl-that unifies modeling and differentiation
within the Julia optimization stack. The framework computes forward - and
reverse-mode solution and objective sensitivities for smooth, potentially
nonconvex programs by differentiating the KKT system under standard regularity
assumptions. A first-class, JuMP-native parameter-centric API allows users to
declare named parameters and obtain derivatives directly with respect to them -
even when a parameter appears in multiple constraints and objectives -
eliminating brittle bookkeeping from coefficient-level interfaces. We
illustrate these capabilities on convex and nonconvex models, including
economic dispatch, mean-variance portfolio selection with conic risk
constraints, and nonlinear robot inverse kinematics. Two companion studies
further demonstrate impact at scale: gradient-based iterative methods for
strategic bidding in energy markets and Sobolev-style training of end-to-end
optimization proxies using solver-accurate sensitivities. Together, these
results demonstrate that differentiable optimization can be deployed as a
routine tool for experimentation, learning, calibration, and design-without
deviating from standard JuMP modeling practices and while retaining access to a
broad ecosystem of solvers.

</details>


### [177] [Efficient Online Learning with Predictive Coding Networks: Exploiting Temporal Correlations](https://arxiv.org/abs/2510.25993)
*Darius Masoum Zadeh-Jousdani,Elvin Hajizada,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 提出了一种新的预测编码网络（PCN-TA），通过利用时间相关性来减少计算需求，同时保持学习性能，适用于边缘部署和资源受限的机器人系统。


<details>
  <summary>Details</summary>
Motivation: 传统的反向传播算法在边缘设备上效率较低，且与生物学原理不符，而预测编码（PC）框架虽然具有生物学合理性，但计算开销大。

Method: 提出了PCN-TA，它通过在时间帧之间保留潜在状态来利用时间相关性，从而减少计算需求。

Result: 在COIL-20机器人感知数据集上的实验表明，PCN-TA比反向传播减少了10%的权重更新，并且比基线PC网络减少了50%的推理步骤。

Conclusion: PCN-TA能够有效降低计算开销，支持边缘部署和实时自适应，并且其生物启发特性使其成为未来神经形态硬件实现的一个有希望的候选方案。

Abstract: Robotic systems operating at the edge require efficient online learning
algorithms that can continuously adapt to changing environments while
processing streaming sensory data. Traditional backpropagation, while
effective, conflicts with biological plausibility principles and may be
suboptimal for continuous adaptation scenarios. The Predictive Coding (PC)
framework offers a biologically plausible alternative with local, Hebbian-like
update rules, making it suitable for neuromorphic hardware implementation.
However, PC's main limitation is its computational overhead due to multiple
inference iterations during training. We present Predictive Coding Network with
Temporal Amortization (PCN-TA), which preserves latent states across temporal
frames. By leveraging temporal correlations, PCN-TA significantly reduces
computational demands while maintaining learning performance. Our experiments
on the COIL-20 robotic perception dataset demonstrate that PCN-TA achieves 10%
fewer weight updates compared to backpropagation and requires 50% fewer
inference steps than baseline PC networks. These efficiency gains directly
translate to reduced computational overhead for moving another step toward edge
deployment and real-time adaptation support in resource-constrained robotic
systems. The biologically-inspired nature of our approach also makes it a
promising candidate for future neuromorphic hardware implementations, enabling
efficient online learning at the edge.

</details>


### [178] [Infrequent Exploration in Linear Bandits](https://arxiv.org/abs/2510.26000)
*Harin Lee,Min-hwan Oh*

Main category: cs.LG

TL;DR: 提出了一种名为INFEX的简单而实用的框架，用于线性bandit中的非频繁探索，该框架在探索策略和贪婪策略之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现有完全自适应的探索方法（例如UCB和Thompson Sampling）可能在每个时间步都进行探索，而纯粹的贪婪方法需要严格的多样性假设才能成功。连续探索在安全关键或成本高昂的领域中可能不切实际或不道德，而没有足够的背景多样性，纯粹的贪婪策略通常会失败。

Method: INFEX根据给定的时间表执行基本探索策略，而在两者之间主要选择贪婪行动。

Result: INFEX实现了与标准可证明有效算法相匹配的实例相关遗憾，前提是探索频率超过对数阈值。INFEX还允许无缝集成任何完全自适应的探索方法，从而实现广泛的适用性和易于采用。通过将密集的探索计算限制在不频繁的间隔内，该方法还可以提高计算效率。经验评估证实了理论结果，显示了优于现有方法的最新遗憾性能和运行时改进。

Conclusion: INFEX在理论和实践上都优于现有方法，为线性bandit中的非频繁探索提供了一种有效的方法。

Abstract: We study the problem of infrequent exploration in linear bandits, addressing
a significant yet overlooked gap between fully adaptive exploratory methods
(e.g., UCB and Thompson Sampling), which explore potentially at every time
step, and purely greedy approaches, which require stringent diversity
assumptions to succeed. Continuous exploration can be impractical or unethical
in safety-critical or costly domains, while purely greedy strategies typically
fail without adequate contextual diversity. To bridge these extremes, we
introduce a simple and practical framework, INFEX, explicitly designed for
infrequent exploration. INFEX executes a base exploratory policy according to a
given schedule while predominantly choosing greedy actions in between. Despite
its simplicity, our theoretical analysis demonstrates that INFEX achieves
instance-dependent regret matching standard provably efficient algorithms,
provided the exploration frequency exceeds a logarithmic threshold.
Additionally, INFEX is a general, modular framework that allows seamless
integration of any fully adaptive exploration method, enabling wide
applicability and ease of adoption. By restricting intensive exploratory
computations to infrequent intervals, our approach can also enhance
computational efficiency. Empirical evaluations confirm our theoretical
findings, showing state-of-the-art regret performance and runtime improvements
over existing methods.

</details>


### [179] [Dual Mixture-of-Experts Framework for Discrete-Time Survival Analysis](https://arxiv.org/abs/2510.26014)
*Hyeonjun Lee,Hyungseob Shin,Gunhee Nam,Hyeonsoo Lee*

Main category: cs.LG

TL;DR: 提出了一种用于离散时间生存分析的双重混合专家 (MoE) 框架，该框架结合了用于子组感知表示学习的特征编码器 MoE 和利用患者特征和时间嵌入来捕获时间动态的风险 MoE。


<details>
  <summary>Details</summary>
Motivation: 在临床和生物医学研究中，生存分析是一项对事件发生的时间进行建模的任务。一个关键的挑战是模拟患者的异质性，同时使风险预测适应个体特征和时间动态。

Method: 结合了用于子组感知表示学习的特征编码器 MoE 和利用患者特征和时间嵌入来捕获时间动态的风险 MoE。

Result: 在 METABRIC 和 GBSG 乳腺癌数据集上，该方法持续提高性能，在测试集上将时间依赖的 C 指数提高了 0.04。

Conclusion: 双重 MoE 设计可以灵活地与现有的基于深度学习的生存管道集成，并可以进一步提高 Consurv 框架的性能。

Abstract: Survival analysis is a task to model the time until an event of interest
occurs, widely used in clinical and biomedical research. A key challenge is to
model patient heterogeneity while also adapting risk predictions to both
individual characteristics and temporal dynamics. We propose a dual
mixture-of-experts (MoE) framework for discrete-time survival analysis. Our
approach combines a feature-encoder MoE for subgroup-aware representation
learning with a hazard MoE that leverages patient features and time embeddings
to capture temporal dynamics. This dual-MoE design flexibly integrates with
existing deep learning based survival pipelines. On METABRIC and GBSG breast
cancer datasets, our method consistently improves performance, boosting the
time-dependent C-index up to 0.04 on the test sets, and yields further gains
when incorporated into the Consurv framework.

</details>


### [180] [Exploring Human-AI Conceptual Alignment through the Prism of Chess](https://arxiv.org/abs/2510.26025)
*Semyon Lomaso,Judah Goldfeder,Mehmet Hamza Erol,Matthew So,Yao Yan,Addison Howard,Nathan Kutz,Ravid Shwartz Ziv*

Main category: cs.LG

TL;DR: 该研究分析了一个达到特级大师水平的2.7亿参数的Transformer模型在国际象棋中的表现，发现尽管早期层能够以较高准确率编码人类概念（如中心控制和骑士前哨），但更深层却偏离了这些概念，转向了“外星”表示。


<details>
  <summary>Details</summary>
Motivation: 探讨AI系统是否真正理解人类概念，还是仅仅模仿表面模式。通过分析AI在国际象棋中的表现，研究人类创造力与精确战略概念的结合。

Method: 1. 分析一个2.7亿参数的Transformer模型在国际象棋中的表现。
2. 引入首个Chess960数据集，包含240个专家注释的位置，涵盖6个战略概念。
3. 进行了分层分析，以揭示模型中不同层次的表示。

Result: 1. 早期层以高达85%的准确率编码人类概念，而更深层准确率下降到50-65%。
2. 在Chess960数据集上，概念识别率下降了10-20%。

Conclusion: AI系统在优化性能时，会发展出与人类思维不同的智能，这对需要真正人机协作的创意AI应用提出了挑战。

Abstract: Do AI systems truly understand human concepts or merely mimic surface
patterns? We investigate this through chess, where human creativity meets
precise strategic concepts. Analyzing a 270M-parameter transformer that
achieves grandmaster-level play, we uncover a striking paradox: while early
layers encode human concepts like center control and knight outposts with up to
85\% accuracy, deeper layers, despite driving superior performance, drift
toward alien representations, dropping to 50-65\% accuracy. To test conceptual
robustness beyond memorization, we introduce the first Chess960 dataset: 240
expert-annotated positions across 6 strategic concepts. When opening theory is
eliminated through randomized starting positions, concept recognition drops
10-20\% across all methods, revealing the model's reliance on memorized
patterns rather than abstract understanding. Our layer-wise analysis exposes a
fundamental tension in current architectures: the representations that win
games diverge from those that align with human thinking. These findings suggest
that as AI systems optimize for performance, they develop increasingly alien
intelligence, a critical challenge for creative AI applications requiring
genuine human-AI collaboration. Dataset and code are available at:
https://github.com/slomasov/ChessConceptsLLM.

</details>


### [181] [Do Students Debias Like Teachers? On the Distillability of Bias Mitigation Methods](https://arxiv.org/abs/2510.26038)
*Jiali Cheng,Chirag Agarwal,Hadi Amiri*

Main category: cs.LG

TL;DR: 知识蒸馏(KD)是一种有效的模型压缩和在模型之间传递知识的方法。本文研究了知识蒸馏对模型鲁棒性的影响，即对虚假相关性的鲁棒性，虚假相关性会降低模型在分布外数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 研究知识蒸馏对模型“去偏见”能力的可转移性的影响，尤其是在自然语言推理(NLI)和图像分类任务上。

Method: 通过大量实验，探讨知识蒸馏后模型去偏见能力的变化，并分析其内部注意力模式和机制。

Result: 主要发现包括：(i) KD会削弱模型的去偏见能力；(ii) 训练去偏见模型无法从注入教师知识中获益；(iii) 模型的整体鲁棒性可能保持稳定，但不同类型的偏差可能发生显著变化；(iv) 确定了导致KD后独特行为的内部注意力模式和回路。

Conclusion: 提出了三种有效的解决方案，以提高去偏见方法的可蒸馏性：开发高质量的数据进行增强，实施迭代知识蒸馏，并使用从教师模型获得的权重初始化学生模型。

Abstract: Knowledge distillation (KD) is an effective method for model compression and
transferring knowledge between models. However, its effect on model's
robustness against spurious correlations that degrade performance on
out-of-distribution data remains underexplored. This study investigates the
effect of knowledge distillation on the transferability of ``debiasing''
capabilities from teacher models to student models on natural language
inference (NLI) and image classification tasks. Through extensive experiments,
we illustrate several key findings: (i) overall the debiasing capability of a
model is undermined post-KD; (ii) training a debiased model does not benefit
from injecting teacher knowledge; (iii) although the overall robustness of a
model may remain stable post-distillation, significant variations can occur
across different types of biases; and (iv) we pin-point the internal attention
pattern and circuit that causes the distinct behavior post-KD. Given the above
findings, we propose three effective solutions to improve the distillability of
debiasing methods: developing high quality data for augmentation, implementing
iterative knowledge distillation, and initializing student models with weights
obtained from teacher models. To the best of our knowledge, this is the first
study on the effect of KD on debiasing and its interenal mechanism at scale.
Our findings provide understandings on how KD works and how to design better
debiasing methods.

</details>


### [182] [Towards Scaling Laws for Symbolic Regression](https://arxiv.org/abs/2510.26064)
*David Otte,Jörg K. H. Franke,Frank Hutter*

Main category: cs.LG

TL;DR: 本文研究了符号回归（SR）中的扩展规律，发现验证损失和求解率都遵循计算能力的幂律趋势。


<details>
  <summary>Details</summary>
Motivation: 探索深度学习的符号回归方法，尤其关注模型规模的影响。

Method: 使用可扩展的端到端 Transformer 流水线和精心生成的训练数据，系统地研究了 SR 中的扩展规律。

Result: 发现验证损失和求解率都遵循计算能力的幂律趋势，并确定了计算最佳的超参数缩放比例。

Conclusion: SR 性能在很大程度上可以通过计算能力来预测，并为训练下一代 SR 模型提供了重要的见解。

Abstract: Symbolic regression (SR) aims to discover the underlying mathematical
expressions that explain observed data. This holds promise for both gaining
scientific insight and for producing inherently interpretable and generalizable
models for tabular data. In this work we focus on the basics of SR. Deep
learning-based SR has recently become competitive with genetic programming
approaches, but the role of scale has remained largely unexplored. Inspired by
scaling laws in language modeling, we present the first systematic
investigation of scaling in SR, using a scalable end-to-end transformer
pipeline and carefully generated training data. Across five different model
sizes and spanning three orders of magnitude in compute, we find that both
validation loss and solved rate follow clear power-law trends with compute. We
further identify compute-optimal hyperparameter scaling: optimal batch size and
learning rate grow with model size, and a token-to-parameter ratio of
$\approx$15 is optimal in our regime, with a slight upward trend as compute
increases. These results demonstrate that SR performance is largely predictable
from compute and offer important insights for training the next generation of
SR models.

</details>


### [183] [Learning Geometry: A Framework for Building Adaptive Manifold Models through Metric Optimization](https://arxiv.org/abs/2510.26068)
*Di Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的机器学习范式，它将模型本身视为可塑的几何实体，通过优化流形上的度量张量场来动态塑造模型空间的几何结构。


<details>
  <summary>Details</summary>
Motivation: 传统方法在固定几何空间内搜索最佳参数，缺乏灵活性。

Method: 构建了一个变分框架，通过离散微分几何将连续流形离散化为三角网格，并用边长参数化度量张量，从而实现高效优化。

Result: 理论分析揭示了该框架与广义相对论中爱因斯坦-希尔伯特作用量之间的深刻类比，并表明即使在固定拓扑下，度量优化也比固定几何模型具有更大的表达能力。

Conclusion: 为构建能够自主演化其几何和拓扑的完全动态的“元学习器”奠定了坚实的基础，并在科学模型发现和鲁棒表示学习等领域具有广阔的应用前景。

Abstract: This paper proposes a novel paradigm for machine learning that moves beyond
traditional parameter optimization. Unlike conventional approaches that search
for optimal parameters within a fixed geometric space, our core idea is to
treat the model itself as a malleable geometric entity. Specifically, we
optimize the metric tensor field on a manifold with a predefined topology,
thereby dynamically shaping the geometric structure of the model space. To
achieve this, we construct a variational framework whose loss function
carefully balances data fidelity against the intrinsic geometric complexity of
the manifold. The former ensures the model effectively explains observed data,
while the latter acts as a regularizer, penalizing overly curved or irregular
geometries to encourage simpler models and prevent overfitting. To address the
computational challenges of this infinite-dimensional optimization problem, we
introduce a practical method based on discrete differential geometry: the
continuous manifold is discretized into a triangular mesh, and the metric
tensor is parameterized by edge lengths, enabling efficient optimization using
automatic differentiation tools. Theoretical analysis reveals a profound
analogy between our framework and the Einstein-Hilbert action in general
relativity, providing an elegant physical interpretation for the concept of
"data-driven geometry". We further argue that even with fixed topology, metric
optimization offers significantly greater expressive power than models with
fixed geometry. This work lays a solid foundation for constructing fully
dynamic "meta-learners" capable of autonomously evolving their geometry and
topology, and it points to broad application prospects in areas such as
scientific model discovery and robust representation learning.

</details>


### [184] [New Money: A Systematic Review of Synthetic Data Generation for Finance](https://arxiv.org/abs/2510.26076)
*James Meldrum,Basem Suleiman,Fethi Rabhi,Muhammad Johan Alibasa*

Main category: cs.LG

TL;DR: 本研究综述了2018年以来发表的72篇关于合成金融数据生成的论文，旨在整合当前的研究成果。


<details>
  <summary>Details</summary>
Motivation: 在机器学习应用中使用敏感金融数据面临挑战，而合成数据生成是一种有前景的解决方案。

Method: 通过对72篇相关研究进行系统性回顾，对合成的金融信息类型、使用的生成方法和评估策略进行分类。

Result: 研究结果表明，基于GAN的方法在文献中占主导地位，尤其是在生成时间序列市场数据和表格信用数据方面。但对隐私保护的评估仍然不足。

Conclusion: 该综述强调了关键的研究差距，并为未来开发金融领域中强大的、保护隐私的合成数据解决方案提供了指导。

Abstract: Synthetic data generation has emerged as a promising approach to address the
challenges of using sensitive financial data in machine learning applications.
By leveraging generative models, such as Generative Adversarial Networks (GANs)
and Variational Autoencoders (VAEs), it is possible to create artificial
datasets that preserve the statistical properties of real financial records
while mitigating privacy risks and regulatory constraints. Despite the rapid
growth of this field, a comprehensive synthesis of the current research
landscape has been lacking. This systematic review consolidates and analyses 72
studies published since 2018 that focus on synthetic financial data generation.
We categorise the types of financial information synthesised, the generative
methods employed, and the evaluation strategies used to assess data utility and
privacy. The findings indicate that GAN-based approaches dominate the
literature, particularly for generating time-series market data and tabular
credit data. While several innovative techniques demonstrate potential for
improved realism and privacy preservation, there remains a notable lack of
rigorous evaluation of privacy safeguards across studies. By providing an
integrated overview of generative techniques, applications, and evaluation
methods, this review highlights critical research gaps and offers guidance for
future work aimed at developing robust, privacy-preserving synthetic data
solutions for the financial domain.

</details>


### [185] [Nirvana: A Specialized Generalist Model With Task-Aware Memory Mechanism](https://arxiv.org/abs/2510.26083)
*Yuhua Jiang,Shuang Cheng,Yihao Liu,Ermo Hua,Che Jiang,Weigao Sun,Yu Cheng,Feifei Gao,Biqing Qi,Bowen Zhou*

Main category: cs.LG

TL;DR: Nirvana is a Specialized Generalist Model (SGM) with a specialized memory mechanism, linear time complexity, and test-time task information extraction.


<details>
  <summary>Details</summary>
Motivation: Traditional LLM structures do not employ specialized memory mechanisms guided by task information, hindering expert-level performance in target domains.

Method: The paper introduces the Task-Aware Memory Trigger (Trigger) and the Specialized Memory Updater (Updater) to flexibly adjust memory based on task requirements and dynamically memorize context.

Result: Nirvana achieves competitive or superior results on general language tasks and higher-quality MRI reconstruction with accurate clinical reports on specialized medical tasks.

Conclusion: Nirvana's specialized memory mechanism, guided by the Trigger and Updater, enables effective adaptation to both general and specialized tasks, demonstrating its potential as an SGM.

Abstract: Specialized Generalist Models (SGMs) aim to preserve broad capabilities while
achieving expert-level performance in target domains. However, traditional LLM
structures including Transformer, Linear Attention, and hybrid models do not
employ specialized memory mechanism guided by task information. In this paper,
we present Nirvana, an SGM with specialized memory mechanism, linear time
complexity, and test-time task information extraction. Besides, we propose the
Task-Aware Memory Trigger ($\textit{Trigger}$) that flexibly adjusts memory
mechanism based on the current task's requirements. In Trigger, each incoming
sample is treated as a self-supervised fine-tuning task, enabling Nirvana to
adapt its task-related parameters on the fly to domain shifts. We also design
the Specialized Memory Updater ($\textit{Updater}$) that dynamically memorizes
the context guided by Trigger. We conduct experiments on both general language
tasks and specialized medical tasks. On a variety of natural language modeling
benchmarks, Nirvana achieves competitive or superior results compared to the
existing LLM structures. To prove the effectiveness of Trigger on specialized
tasks, we test Nirvana's performance on a challenging medical task, i.e.,
Magnetic Resonance Imaging (MRI). We post-train frozen Nirvana backbone with
lightweight codecs on paired electromagnetic signals and MRI images. Despite
the frozen Nirvana backbone, Trigger guides the model to adapt to the MRI
domain with the change of task-related parameters. Nirvana achieves
higher-quality MRI reconstruction compared to conventional MRI models as well
as the models with traditional LLMs' backbone, and can also generate accurate
preliminary clinical reports accordingly.

</details>


### [186] [LLMBisect: Breaking Barriers in Bug Bisection with A Comparative Analysis Pipeline](https://arxiv.org/abs/2510.26086)
*Zheng Zhang,Haonan Li,Xingyu Li,Hang Zhang,Zhiyun Qian*

Main category: cs.LG

TL;DR: 提出了一种基于LLM的多阶段流水线方法，用于识别引入bug的commit。


<details>
  <summary>Details</summary>
Motivation: 传统基于补丁的二分方法存在局限性，例如假设bug引入commit和补丁commit修改相同的函数，且仅依赖代码更改，缺乏对漏洞的逻辑分析。

Method: 利用LLM充分利用补丁信息，比较上下文中的多个候选commit，并通过一系列下选择步骤逐步缩小候选范围。

Result: 该方法比现有技术方案的准确率提高了38%以上，并且多阶段流水线比基于LLM的基线方法提高了60%的准确率。

Conclusion: 该论文表明，LLM能够打破现有解决方案的障碍，并且提出的多阶段流水线至关重要。

Abstract: Bug bisection has been an important security task that aims to understand the
range of software versions impacted by a bug, i.e., identifying the commit that
introduced the bug. However, traditional patch-based bisection methods are
faced with several significant barriers: For example, they assume that the
bug-inducing commit (BIC) and the patch commit modify the same functions, which
is not always true. They often rely solely on code changes, while the commit
message frequently contains a wealth of vulnerability-related information. They
are also based on simple heuristics (e.g., assuming the BIC initializes lines
deleted in the patch) and lack any logical analysis of the vulnerability.
  In this paper, we make the observation that Large Language Models (LLMs) are
well-positioned to break the barriers of existing solutions, e.g., comprehend
both textual data and code in patches and commits. Unlike previous BIC
identification approaches, which yield poor results, we propose a comprehensive
multi-stage pipeline that leverages LLMs to: (1) fully utilize patch
information, (2) compare multiple candidate commits in context, and (3)
progressively narrow down the candidates through a series of down-selection
steps. In our evaluation, we demonstrate that our approach achieves
significantly better accuracy than the state-of-the-art solution by more than
38\%. Our results further confirm that the comprehensive multi-stage pipeline
is essential, as it improves accuracy by 60\% over a baseline LLM-based
bisection method.

</details>


### [187] [Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle Routing](https://arxiv.org/abs/2510.26089)
*Fazel Arasteh,Arian Haghparast,Manos Papagelis*

Main category: cs.LG

TL;DR: 本文提出了一种基于多智能体强化学习（MARL）的动态车辆路由框架，旨在解决城市道路网络中的交通拥堵问题。


<details>
  <summary>Details</summary>
Motivation: 现有的最短路径优先（SPF）算法在动态、多车辆环境中表现不佳，容易加剧拥堵。

Method: 提出了自适应导航（AN）和分层 Hub 自适应导航（HHAN）两种 MARL 模型。AN 使用图注意力网络（GAT）进行邻域状态建模，HHAN 则通过集中式训练和分散式执行（CTDE）以及 Attentive Q-Mixing (A-QMIX) 框架实现 Hub 之间的协调。

Result: 在合成网格和真实城市地图（多伦多、曼哈顿）上的实验表明，AN 可以减少平均旅行时间，HHAN 可以扩展到具有数百个交叉口的网络，并在交通拥堵的情况下实现高达 15.9% 的改进。

Conclusion: 研究结果表明，网络约束的 MARL 在智能交通系统中具有可扩展、协调和拥堵感知的路由潜力。

Abstract: Traffic congestion in urban road networks leads to longer trip times and
higher emissions, especially during peak periods. While the Shortest Path First
(SPF) algorithm is optimal for a single vehicle in a static network, it
performs poorly in dynamic, multi-vehicle settings, often worsening congestion
by routing all vehicles along identical paths. We address dynamic vehicle
routing through a multi-agent reinforcement learning (MARL) framework for
coordinated, network-aware fleet navigation. We first propose Adaptive
Navigation (AN), a decentralized MARL model where each intersection agent
provides routing guidance based on (i) local traffic and (ii) neighborhood
state modeled using Graph Attention Networks (GAT). To improve scalability in
large networks, we further propose Hierarchical Hub-based Adaptive Navigation
(HHAN), an extension of AN that assigns agents only to key intersections
(hubs). Vehicles are routed hub-to-hub under agent control, while SPF handles
micro-routing within each hub region. For hub coordination, HHAN adopts
centralized training with decentralized execution (CTDE) under the Attentive
Q-Mixing (A-QMIX) framework, which aggregates asynchronous vehicle decisions
via attention. Hub agents use flow-aware state features that combine local
congestion and predictive dynamics for proactive routing. Experiments on
synthetic grids and real urban maps (Toronto, Manhattan) show that AN reduces
average travel time versus SPF and learning baselines, maintaining 100% routing
success. HHAN scales to networks with hundreds of intersections, achieving up
to 15.9% improvement under heavy traffic. These findings highlight the
potential of network-constrained MARL for scalable, coordinated, and
congestion-aware routing in intelligent transportation systems.

</details>


### [188] [SAFE: A Novel Approach to AI Weather Evaluation through Stratified Assessments of Forecasts over Earth](https://arxiv.org/abs/2510.26099)
*Nick Masi,Randall Balestriero*

Main category: cs.LG

TL;DR: SAFE是一个用于评估地球预测分层性能的软件包，它集成了各种数据领域，可以按与地理空间网格点相关的不同属性进行分层，从而检查模型在每个属性的各个层中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习评估模型性能的范例是基于某个测试集中所有样本的平均损失，这相当于在天气和气候环境中对地球进行地理空间平均性能，而没有考虑到人类发展和地理的非均匀分布。

Method: SAFE集成了各种数据领域，以按与地理空间网格点相关的不同属性进行分层：区域（通常是国家）、全球子区域、收入和土地覆盖（陆地或水）。

Result: 通过SAFE对最先进的基于AI的天气预测模型进行基准测试，发现它们在每个属性上的预测技能都存在差异。通过在不同提前期对各种气候变量进行分层，来衡量模型预测的公平性。

Conclusion: 通过超越全球平均指标，首次提出模型在何处表现最佳或最差，以及哪些模型最公平的问题。SAFE软件包是开源的。

Abstract: The dominant paradigm in machine learning is to assess model performance
based on average loss across all samples in some test set. This amounts to
averaging performance geospatially across the Earth in weather and climate
settings, failing to account for the non-uniform distribution of human
development and geography. We introduce Stratified Assessments of Forecasts
over Earth (SAFE), a package for elucidating the stratified performance of a
set of predictions made over Earth. SAFE integrates various data domains to
stratify by different attributes associated with geospatial gridpoints:
territory (usually country), global subregion, income, and landcover (land or
water). This allows us to examine the performance of models for each individual
stratum of the different attributes (e.g., the accuracy in every individual
country). To demonstrate its importance, we utilize SAFE to benchmark a zoo of
state-of-the-art AI-based weather prediction models, finding that they all
exhibit disparities in forecasting skill across every attribute. We use this to
seed a benchmark of model forecast fairness through stratification at different
lead times for various climatic variables. By moving beyond globally-averaged
metrics, we for the first time ask: where do models perform best or worst, and
which models are most fair? To support further work in this direction, the SAFE
package is open source and available at https://github.com/N-Masi/safe

</details>


### [189] [Do Not Step Into the Same River Twice: Learning to Reason from Trial and Error](https://arxiv.org/abs/2510.26109)
*Chenming Tang,Hsiu-Yuan Huang,Weijie Liu,Saiyong Yang,Yunfang Wu*

Main category: cs.LG

TL;DR: 提出了一种名为LTE的强化学习方法，通过提示LLM先前生成的错误答案和过长响应问题来改进LLM的推理能力，无需外部专家指导。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法仅基于LLM自身生成的响应进行训练，受限于LLM的初始能力，容易出现探索停滞，并且依赖外部专家指导。

Method: 提出LTE方法，利用LLM先前生成的错误答案和过长响应问题来提示LLM。

Result: 实验表明，LTE在六个数学基准测试中优于普通GRPO，Qwen3-4B-Base的Pass@1平均提高6.38，Pass@k平均提高9.00。

Conclusion: LTE成功缓解了探索停滞问题，并在训练期间加强了利用和探索。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has significantly
boosted the reasoning capability of large language models (LLMs) recently.
However, existing RLVR approaches merely train LLMs based on their own
generated responses and are constrained by the initial capability of LLMs, thus
prone to exploration stagnation, in which LLMs fail to solve more training
problems and cannot further learn from the training data. Some work tries to
address this by leveraging off-policy solutions to training problems but
requires external guidance from experts which suffers from limited
availability. In this work, we propose LTE (Learning to reason from Trial and
Error), an approach hinting LLMs with their previously self-generated incorrect
answers and problem of overlong responses, which does not require any external
expert guidance. Experiments validate the effectiveness of LTE, which
outperforms the normal group relative policy optimization (GRPO) by 6.38 in
Pass@1 and 9.00 in Pass@k on average across six mathematics benchmarks for
Qwen3-4B-Base. Further analysis confirms that LTE successfully mitigates the
problem of exploration stagnation and enhances both exploitation and
exploration during training.

</details>


### [190] [maxVSTAR: Maximally Adaptive Vision-Guided CSI Sensing with Closed-Loop Edge Model Adaptation for Robust Human Activity Recognition](https://arxiv.org/abs/2510.26146)
*Kexing Liu*

Main category: cs.LG

TL;DR: maxVSTAR: A vision-guided model adaptation framework for edge-deployed CSI sensing systems that mitigates domain shift and restores accuracy through continuous, self-supervised retraining.


<details>
  <summary>Details</summary>
Motivation: Recognition performance of WiFi CSI-based HAR deteriorates under varying environmental and hardware conditions, limiting deployment on edge devices.

Method: A closed-loop, vision-guided model adaptation framework (maxVSTAR) is proposed, integrating a cross-modal teacher-student architecture with a YOLO-based vision model providing real-time activity labels for online fine-tuning of a lightweight CSI-based HAR model (STAR).

Result: When deployed on uncalibrated hardware, maxVSTAR restored the accuracy of the baseline STAR model from 49.14% to 81.51% after a single vision-guided adaptation cycle (baseline accuracy declined from 93.52% to 49.14%).

Conclusion: maxVSTAR enables dynamic, self-supervised model adaptation in privacy-conscious IoT environments, establishing a scalable and practical paradigm for long-term autonomous HAR using CSI sensing at the network edge.

Abstract: WiFi Channel State Information (CSI)-based human activity recognition (HAR)
provides a privacy-preserving, device-free sensing solution for smart
environments. However, its deployment on edge devices is severely constrained
by domain shift, where recognition performance deteriorates under varying
environmental and hardware conditions. This study presents maxVSTAR (maximally
adaptive Vision-guided Sensing Technology for Activity Recognition), a
closed-loop, vision-guided model adaptation framework that autonomously
mitigates domain shift for edge-deployed CSI sensing systems. The proposed
system integrates a cross-modal teacher-student architecture, where a
high-accuracy YOLO-based vision model serves as a dynamic supervisory signal,
delivering real-time activity labels for the CSI data stream. These labels
enable autonomous, online fine-tuning of a lightweight CSI-based HAR model,
termed Sensing Technology for Activity Recognition (STAR), directly at the
edge. This closed-loop retraining mechanism allows STAR to continuously adapt
to environmental changes without manual intervention. Extensive experiments
demonstrate the effectiveness of maxVSTAR. When deployed on uncalibrated
hardware, the baseline STAR model's recognition accuracy declined from 93.52%
to 49.14%. Following a single vision-guided adaptation cycle, maxVSTAR restored
the accuracy to 81.51%. These results confirm the system's capacity for
dynamic, self-supervised model adaptation in privacy-conscious IoT
environments, establishing a scalable and practical paradigm for long-term
autonomous HAR using CSI sensing at the network edge.

</details>


### [191] [STAR: A Privacy-Preserving, Energy-Efficient Edge AI Framework for Human Activity Recognition via Wi-Fi CSI in Mobile and Pervasive Computing Environments](https://arxiv.org/abs/2510.26148)
*Kexing Liu*

Main category: cs.LG

TL;DR: This paper introduces STAR, an edge-AI-optimized framework for real-time, energy-efficient human activity recognition (HAR) on low-power embedded devices using Wi-Fi CSI.


<details>
  <summary>Details</summary>
Motivation: Existing HAR methods using Wi-Fi CSI are computationally inefficient, have high latency, and limited feasibility in resource-constrained environments.

Method: STAR integrates a lightweight GRU-based neural network, adaptive signal processing, and hardware-aware co-optimization. It uses a multi-stage pre-processing pipeline for denoising and feature extraction. The system is implemented on a Rockchip RV1126 processor with an NPU, interfaced with an ESP32-S3-based CSI acquisition module.

Result: STAR achieves a mean recognition accuracy of 93.52% across seven activity classes and 99.11% for human presence detection with a 97.6k-parameter model. INT8 quantized inference achieves 33 MHz processing speed with 8% CPU utilization, a sixfold improvement over CPU-based execution.

Conclusion: STAR provides a practical, scalable solution for real-time, privacy-preserving HAR in mobile and pervasive computing environments, with sub-second response latency and low power consumption.

Abstract: Human Activity Recognition (HAR) via Wi-Fi Channel State Information (CSI)
presents a privacy-preserving, contactless sensing approach suitable for smart
homes, healthcare monitoring, and mobile IoT systems. However, existing methods
often encounter computational inefficiency, high latency, and limited
feasibility within resource-constrained, embedded mobile edge environments.
This paper proposes STAR (Sensing Technology for Activity Recognition), an
edge-AI-optimized framework that integrates a lightweight neural architecture,
adaptive signal processing, and hardware-aware co-optimization to enable
real-time, energy-efficient HAR on low-power embedded devices. STAR
incorporates a streamlined Gated Recurrent Unit (GRU)-based recurrent neural
network, reducing model parameters by 33% compared to conventional LSTM models
while maintaining effective temporal modeling capability. A multi-stage
pre-processing pipeline combining median filtering, 8th-order Butterworth
low-pass filtering, and Empirical Mode Decomposition (EMD) is employed to
denoise CSI amplitude data and extract spatial-temporal features. For on-device
deployment, STAR is implemented on a Rockchip RV1126 processor equipped with an
embedded Neural Processing Unit (NPU), interfaced with an ESP32-S3-based CSI
acquisition module. Experimental results demonstrate a mean recognition
accuracy of 93.52% across seven activity classes and 99.11% for human presence
detection, utilizing a compact 97.6k-parameter model. INT8 quantized inference
achieves a processing speed of 33 MHz with just 8% CPU utilization, delivering
sixfold speed improvements over CPU-based execution. With sub-second response
latency and low power consumption, the system ensures real-time,
privacy-preserving HAR, offering a practical, scalable solution for mobile and
pervasive computing environments.

</details>


### [192] [Bridging the Gap Between Molecule and Textual Descriptions via Substructure-aware Alignment](https://arxiv.org/abs/2510.26157)
*Hyuntae Park,Yeachan Kim,SangKeun Lee*

Main category: cs.LG

TL;DR: MolBridge introduces substructure-aware alignments for molecule-text learning, enhancing fine-grained correspondence.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle to capture subtle differences between molecules and their descriptions due to the lack of fine-grained alignment learning.

Method: MolBridge augments molecule-description pairs with alignment signals from molecular substructures and chemical phrases, using substructure-aware contrastive learning and a self-refinement mechanism.

Result: MolBridge outperforms state-of-the-art baselines on molecular benchmarks.

Conclusion: Substructure-aware alignment is significant in molecule-text learning.

Abstract: Molecule and text representation learning has gained increasing interest due
to its potential for enhancing the understanding of chemical information.
However, existing models often struggle to capture subtle differences between
molecules and their descriptions, as they lack the ability to learn
fine-grained alignments between molecular substructures and chemical phrases.
To address this limitation, we introduce MolBridge, a novel molecule-text
learning framework based on substructure-aware alignments. Specifically, we
augment the original molecule-description pairs with additional alignment
signals derived from molecular substructures and chemical phrases. To
effectively learn from these enriched alignments, MolBridge employs
substructure-aware contrastive learning, coupled with a self-refinement
mechanism that filters out noisy alignment signals. Experimental results show
that MolBridge effectively captures fine-grained correspondences and
outperforms state-of-the-art baselines on a wide range of molecular benchmarks,
highlighting the significance of substructure-aware alignment in molecule-text
learning.

</details>


### [193] [Segmentation over Complexity: Evaluating Ensemble and Hybrid Approaches for Anomaly Detection in Industrial Time Series](https://arxiv.org/abs/2510.26159)
*Emilio Mastriani,Alessandro Costa,Federico Incardona,Kevin Munari,Sebastiano Spinello*

Main category: cs.LG

TL;DR: 高级异常检测方法在多元工业时间序列中表现不如简单的集成模型。


<details>
  <summary>Details</summary>
Motivation: 研究高级特征工程和混合模型架构在多元工业时间序列异常检测中的有效性，重点关注蒸汽轮机系统。

Method: 评估由变化点衍生的统计特征、基于聚类的子结构表示和混合学习策略对检测性能的影响。采用Random Forest + XGBoost集成模型进行对比。

Result: 简单的Random Forest + XGBoost集成模型在分段数据上训练后，优于其他复杂方法，AUC-ROC达到0.976，F1-score为0.41，并在定义的时间窗口内实现了100%的早期检测。

Conclusion: 在高度不平衡和时间不确定的数据场景中，模型简单性与优化的分段相结合，可以胜过更复杂的架构，提供更大的鲁棒性、可解释性和操作实用性。

Abstract: In this study, we investigate the effectiveness of advanced feature
engineering and hybrid model architectures for anomaly detection in a
multivariate industrial time series, focusing on a steam turbine system. We
evaluate the impact of change point-derived statistical features,
clustering-based substructure representations, and hybrid learning strategies
on detection performance. Despite their theoretical appeal, these complex
approaches consistently underperformed compared to a simple Random Forest +
XGBoost ensemble trained on segmented data. The ensemble achieved an AUC-ROC of
0.976, F1-score of 0.41, and 100% early detection within the defined time
window. Our findings highlight that, in scenarios with highly imbalanced and
temporally uncertain data, model simplicity combined with optimized
segmentation can outperform more sophisticated architectures, offering greater
robustness, interpretability, and operational utility.

</details>


### [194] [A Game-Theoretic Spatio-Temporal Reinforcement Learning Framework for Collaborative Public Resource Allocation](https://arxiv.org/abs/2510.26184)
*Songxin Lei,Qiongyan Wang,Yanchen Zhu,Hanyu Yao,Sijie Ruan,Weilin Ruan,Yuyu Luo,Huaming Wu,Yuxuan Liang*

Main category: cs.LG

TL;DR: 提出了一种新的公共资源分配（CPRA）问题，该问题考虑了现实场景中的容量约束和时空动态。


<details>
  <summary>Details</summary>
Motivation: 现有方法侧重于独立优化单个资源的移动，而没有考虑其容量约束。

Method: 提出了一个名为博弈论时空强化学习（GSTRL）的新框架来解决CPRA问题。

Result: 在两个真实世界的数据集上评估了GSTRL，实验表明其性能优越。

Conclusion: 所提出的GSTRL框架可以有效地捕获整个系统的时空动态。

Abstract: Public resource allocation involves the efficient distribution of resources,
including urban infrastructure, energy, and transportation, to effectively meet
societal demands. However, existing methods focus on optimizing the movement of
individual resources independently, without considering their capacity
constraints. To address this limitation, we propose a novel and more practical
problem: Collaborative Public Resource Allocation (CPRA), which explicitly
incorporates capacity constraints and spatio-temporal dynamics in real-world
scenarios. We propose a new framework called Game-Theoretic Spatio-Temporal
Reinforcement Learning (GSTRL) for solving CPRA. Our contributions are twofold:
1) We formulate the CPRA problem as a potential game and demonstrate that there
is no gap between the potential function and the optimal target, laying a solid
theoretical foundation for approximating the Nash equilibrium of this NP-hard
problem; and 2) Our designed GSTRL framework effectively captures the
spatio-temporal dynamics of the overall system. We evaluate GSTRL on two
real-world datasets, where experiments show its superior performance. Our
source codes are available in the supplementary materials.

</details>


### [195] [Accumulative SGD Influence Estimation for Data Attribution](https://arxiv.org/abs/2510.26185)
*Yunxiao Shi,Shuo Yang,Yixin Su,Rui Zhang,Min Xu*

Main category: cs.LG

TL;DR: 本文提出了一种新的影响函数估计器 ACC-SGD-IE，它通过在训练过程中传播 leave-one-out 扰动并在每一步更新累积影响状态来实现更精确的样本影响估计。


<details>
  <summary>Details</summary>
Motivation: 现有的 SGD-IE 方法通过对每个 epoch 的替代物求和来近似 leave-one-out 效应，忽略了跨 epoch 的复合效应，这会错误地对关键样本进行排序。现代以数据为中心的人工智能需要精确的 per-sample 影响。

Method: 提出了一种轨迹感知的估计器 ACC-SGD-IE，它在训练过程中传播 leave-one-out 扰动，并在每一步更新累积影响状态。

Result: 在 Adult, 20 Newsgroups, 和 MNIST 数据集上，在干净和损坏的数据以及凸和非凸训练下，ACC-SGD-IE 产生了更准确的影响估计，尤其是在长 epochs 上。对于下游数据清洗，它更可靠地标记噪声样本，从而产生在 ACC-SGD-IE 清洗数据上训练的模型，其性能优于使用 SGD-IE 清洗的模型。

Conclusion: ACC-SGD-IE 是一种更精确的影响函数估计器，可以更可靠地标记噪声样本，并提高下游数据清洗的性能。

Abstract: Modern data-centric AI needs precise per-sample influence. Standard SGD-IE
approximates leave-one-out effects by summing per-epoch surrogates and ignores
cross-epoch compounding, which misranks critical examples. We propose
ACC-SGD-IE, a trajectory-aware estimator that propagates the leave-one-out
perturbation across training and updates an accumulative influence state at
each step. In smooth strongly convex settings it achieves geometric error
contraction and, in smooth non-convex regimes, it tightens error bounds; larger
mini-batches further reduce constants. Empirically, on Adult, 20 Newsgroups,
and MNIST under clean and corrupted data and both convex and non-convex
training, ACC-SGD-IE yields more accurate influence estimates, especially over
long epochs. For downstream data cleansing it more reliably flags noisy
samples, producing models trained on ACC-SGD-IE cleaned data that outperform
those cleaned with SGD-IE.

</details>


### [196] [Predicting All-Cause Hospital Readmissions from Medical Claims Data of Hospitalised Patients](https://arxiv.org/abs/2510.26188)
*Avinash Kadimisetty,Arun Rajagopalan,Vijendra SK*

Main category: cs.LG

TL;DR: 该研究使用机器学习预测医院再入院率，旨在降低医疗成本并提高医疗质量。


<details>
  <summary>Details</summary>
Motivation: 降低可预防的医院再入院率是医疗付款方、服务提供方和决策者的首要任务，再入院率是衡量医院医疗质量的基准。

Method: 使用Logistic回归、随机森林和支持向量机等机器学习技术分析健康理赔数据，并使用主成分分析进行降维。

Result: 随机森林模型表现最佳，其次是Logistic回归和支持向量机模型。

Conclusion: 这些模型可用于识别导致再入院的关键因素，并帮助识别需要关注的患者，以降低再入院的可能性，最终降低成本并提高为患者提供的医疗保健质量。

Abstract: Reducing preventable hospital readmissions is a national priority for payers,
providers, and policymakers seeking to improve health care and lower costs. The
rate of readmission is being used as a benchmark to determine the quality of
healthcare provided by the hospitals. In thisproject, we have used machine
learning techniques like Logistic Regression, Random Forest and Support Vector
Machines to analyze the health claims data and identify demographic and medical
factors that play a crucial role in predicting all-cause readmissions. As the
health claims data is high dimensional, we have used Principal Component
Analysis as a dimension reduction technique and used the results for building
regression models. We compared and evaluated these models based on the Area
Under Curve (AUC) metric. Random Forest model gave the highest performance
followed by Logistic Regression and Support Vector Machine models. These models
can be used to identify the crucial factors causing readmissions and help
identify patients to focus on to reduce the chances of readmission, ultimately
bringing down the cost and increasing the quality of healthcare provided to the
patients.

</details>


### [197] [Test-Time Alignment of LLMs via Sampling-Based Optimal Control in pre-logit space](https://arxiv.org/abs/2510.26219)
*Sekitoshi Kanai,Tsukasa Yoshida,Hiroshi Takahashi,Haru Kuroki,Kazumune Hashimoto*

Main category: cs.LG

TL;DR: 提出了一种新的测试时对齐方法AISP，该方法通过在倒数第二层的输出上应用高斯扰动来最大化预期收益。


<details>
  <summary>Details</summary>
Motivation: 微调大型语言模型需要很高的计算成本，因此，对大型语言模型进行测试时对齐引起了人们的关注。

Method: 在基于采样的模型预测控制基础上，提出了一种新的测试时对齐方法，称为预逻辑自适应重要性抽样(AISP)。AISP将高斯扰动应用于pre-logits，即倒数第二层的输出，以最大化关于扰动均值的预期奖励。

Result: AISP在奖励方面优于best-of-n抽样，并且比其他基于奖励的测试时对齐方法实现了更高的奖励。

Conclusion: 通过重要性抽样和抽样奖励，可以获得最优均值。

Abstract: Test-time alignment of large language models (LLMs) attracts attention
because fine-tuning LLMs requires high computational costs. In this paper, we
propose a new test-time alignment method called adaptive importance sampling on
pre-logits (AISP) on the basis of the sampling-based model predictive control
with the stochastic control input. AISP applies the Gaussian perturbation into
pre-logits, which are outputs of the penultimate layer, so as to maximize
expected rewards with respect to the mean of the perturbation. We demonstrate
that the optimal mean is obtained by importance sampling with sampled rewards.
AISP outperforms best-of-n sampling in terms of rewards over the number of used
samples and achieves higher rewards than other reward-based test-time alignment
methods.

</details>


### [198] [MPRU: Modular Projection-Redistribution Unlearning as Output Filter for Classification Pipelines](https://arxiv.org/abs/2510.26230)
*Minyi Peng,Darian Gunamardi,Ivan Tjuawinata,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: 提出了一种新的机器学习遗忘方法，通过反转最后的训练序列来实现知识移除，无需完全访问原始数据集或模型。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习遗忘方法在实际应用中面临可扩展性问题，并且需要完全访问原始数据集和模型。

Method: 将分类训练视为一个序列过程，通过在模型末尾添加一个投影-重新分配层来实现遗忘。

Result: 在图像和表格数据集上的实验结果表明，该方法在计算成本大幅降低的情况下，能够产生与完全重新训练模型相似的输出。

Conclusion: 该解决方案具有适用性、可扩展性和系统兼容性，同时在更实际的环境中保持了输出性能。

Abstract: As a new and promising approach, existing machine unlearning (MU) works
typically emphasize theoretical formulations or optimization objectives to
achieve knowledge removal. However, when deployed in real-world scenarios, such
solutions typically face scalability issues and have to address practical
requirements such as full access to original datasets and model. In contrast to
the existing approaches, we regard classification training as a sequential
process where classes are learned sequentially, which we call \emph{inductive
approach}. Unlearning can then be done by reversing the last training sequence.
This is implemented by appending a projection-redistribution layer in the end
of the model. Such an approach does not require full access to the original
dataset or the model, addressing the challenges of existing methods. This
enables modular and model-agnostic deployment as an output filter into existing
classification pipelines with minimal alterations. We conducted multiple
experiments across multiple datasets including image (CIFAR-10/100 using
CNN-based model) and tabular datasets (Covertype using tree-based model).
Experiment results show consistently similar output to a fully retrained model
with a high computational cost reduction. This demonstrates the applicability,
scalability, and system compatibility of our solution while maintaining the
performance of the output in a more practical setting.

</details>


### [199] [Angular Steering: Behavior Control via Rotation in Activation Space](https://arxiv.org/abs/2510.26243)
*Hieu M. Vu,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 提出了 Angular Steering，一种通过在二维子空间内旋转激活来调节行为的新方法，以实现对LLM行为的细粒度控制，同时保持其通用能力。


<details>
  <summary>Details</summary>
Motivation: 现有steering方法受限于二维子空间，对参数敏感，且可能影响不相关特征。因此，需要一种更灵活、稳定的行为控制方法。

Method: 提出 Angular Steering，通过将steering формулируется как 旋转到目标行为方向或远离目标行为方向的几何旋转来实现行为调节。同时，提出自适应 Angular Steering，一种仅旋转与目标特征对齐的激活的选择性变体。

Result: 实验表明，Angular Steering 实现了稳健的行为控制，同时保持了通用语言建模性能。该方法具有灵活性、泛化性和稳健性。

Conclusion: Angular Steering 是一种优于现有方法的、用于控制大型语言模型行为的新方法，它在保持模型稳定性的同时，实现了对行为的细粒度控制。

Abstract: Controlling specific behaviors in large language models while preserving
their general capabilities is a central challenge for safe and reliable
artificial intelligence deployment. Current steering methods, such as vector
addition and directional ablation, are constrained within a two-dimensional
subspace defined by the activation and feature direction, making them sensitive
to chosen parameters and potentially affecting unrelated features due to
unintended interactions in activation space. We introduce Angular Steering, a
novel and flexible method for behavior modulation that operates by rotating
activations within a fixed two-dimensional subspace. By formulating steering as
a geometric rotation toward or away from a target behavior direction, Angular
Steering provides continuous, fine-grained control over behaviors such as
refusal and compliance. We demonstrate this method using refusal steering
emotion steering as use cases. Additionally, we propose Adaptive Angular
Steering, a selective variant that rotates only activations aligned with the
target feature, further enhancing stability and coherence. Angular Steering
generalizes existing addition and orthogonalization techniques under a unified
geometric rotation framework, simplifying parameter selection and maintaining
model stability across a broader range of adjustments. Experiments across
multiple model families and sizes show that Angular Steering achieves robust
behavioral control while maintaining general language modeling performance,
underscoring its flexibility, generalization, and robustness compared to prior
approaches. Code and artifacts are available at
https://github.com/lone17/angular-steering/.

</details>


### [200] [Likely Interpolants of Generative Models](https://arxiv.org/abs/2510.26266)
*Frederik Möbius Rygaard,Shen Zhu,Yinzhu Jin,Søren Hauberg,Tom Fletcher*

Main category: cs.LG

TL;DR: 提出了一种新的生成模型插值方案，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型缺乏有效的插值方法，通常需要对模型或数据维度进行限制。

Method: 开发了一种通用的插值方案，目标是找到与不同度量和概率分布相容的可能过渡路径，类似于约束到合适数据分布的测地线。

Result: 该插值方案在多种模型和数据集上，比基线方法遍历更高密度区域。

Conclusion: 该方法在理论上可以被认为是黎曼度量下的测地线。

Abstract: Interpolation in generative models allows for controlled generation, model
inspection, and more. Unfortunately, most generative models lack a principal
notion of interpolants without restrictive assumptions on either the model or
data dimension. In this paper, we develop a general interpolation scheme that
targets likely transition paths compatible with different metrics and
probability distributions. We consider interpolants analogous to a geodesic
constrained to a suitable data distribution and derive a novel algorithm for
computing these curves, which requires no additional training. Theoretically,
we show that our method locally can be considered as a geodesic under a
suitable Riemannian metric. We quantitatively show that our interpolation
scheme traverses higher density regions than baselines across a range of models
and datasets.

</details>


### [201] [Distributional Multi-objective Black-box Optimization for Diffusion-model Inference-time Multi-Target Generation](https://arxiv.org/abs/2510.26278)
*Kim Yong Tan,Yueming Lyu,Ivor Tsang,Yew-Soon Ong*

Main category: cs.LG

TL;DR: 提出了一种新的Inference-time Multi-target Generation (IMG)算法，通过在推理时优化扩散过程来生成同时满足多个目标的样本。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常采用外部优化循环，例如进化算法，到扩散模型。然而，这些方法将扩散模型视为黑盒改进器，忽略了扩散生成过程的内部分布转换，限制了它们的效率。

Method: IMG算法在扩散生成过程中根据预期的聚合多目标值执行加权重采样。这种加权重采样策略确保扩散生成的样本根据我们期望的多目标玻尔兹曼分布进行分布。

Result: 在多目标分子生成任务上的实验表明，IMG仅需要一次生成过程，就能获得比通常需要数百次扩散生成的基线优化算法显着更高的超体积。

Conclusion: IMG可以被视为一个优化的扩散过程，可以集成到现有方法中，以进一步提高它们的性能。

Abstract: Diffusion models have been successful in learning complex data distributions.
This capability has driven their application to high-dimensional
multi-objective black-box optimization problem. Existing approaches often
employ an external optimization loop, such as an evolutionary algorithm, to the
diffusion model. However, these approaches treat the diffusion model as a
black-box refiner, which overlooks the internal distribution transition of the
diffusion generation process, limiting their efficiency. To address these
challenges, we propose the Inference-time Multi-target Generation (IMG)
algorithm, which optimizes the diffusion process at inference-time to generate
samples that simultaneously satisfy multiple objectives. Specifically, our IMG
performs weighted resampling during the diffusion generation process according
to the expected aggregated multi-objective values. This weighted resampling
strategy ensures the diffusion-generated samples are distributed according to
our desired multi-target Boltzmann distribution. We further derive that the
multi-target Boltzmann distribution has an interesting log-likelihood
interpretation, where it is the optimal solution to the distributional
multi-objective optimization problem. We implemented IMG for a multi-objective
molecule generation task. Experiments show that IMG, requiring only a single
generation pass, achieves a significantly higher hypervolume than baseline
optimization algorithms that often require hundreds of diffusion generations.
Notably, our algorithm can be viewed as an optimized diffusion process and can
be integrated into existing methods to further improve their performance.

</details>


### [202] [Empirical Bayesian Multi-Bandit Learning](https://arxiv.org/abs/2510.26284)
*Xia Jiang,Rong J. B. Zhu*

Main category: cs.LG

TL;DR: 提出了一种新的分层贝叶斯框架，用于在各种bandit实例中学习，通过分层贝叶斯模型捕获不同bandit实例之间的异质性和相关性，实现有效的信息共享，同时适应实例特定的变化。


<details>
  <summary>Details</summary>
Motivation: 在上下文bandit中的多任务学习因其通过利用共享结构和特定于任务的异质性来增强跨多个相关任务的决策的潜力而引起了极大的研究兴趣。

Method: 引入了一种经验贝叶斯方法来估计先验分布的协方差矩阵，并开发了两种高效的算法：ebmTS和ebmUCB，它们都将估计的先验纳入决策过程。

Result: 在合成和真实世界数据集上的大量实验表明，该算法具有优越的性能，尤其是在复杂环境中。与现有技术相比，该方法实现了更低的累积遗憾。

Conclusion: 所提出的算法在平衡跨多个bandit的探索和利用方面非常有效。

Abstract: Multi-task learning in contextual bandits has attracted significant research
interest due to its potential to enhance decision-making across multiple
related tasks by leveraging shared structures and task-specific heterogeneity.
In this article, we propose a novel hierarchical Bayesian framework for
learning in various bandit instances. This framework captures both the
heterogeneity and the correlations among different bandit instances through a
hierarchical Bayesian model, enabling effective information sharing while
accommodating instance-specific variations. Unlike previous methods that
overlook the learning of the covariance structure across bandits, we introduce
an empirical Bayesian approach to estimate the covariance matrix of the prior
distribution.This enhances both the practicality and flexibility of learning
across multi-bandits. Building on this approach, we develop two efficient
algorithms: ebmTS (Empirical Bayesian Multi-Bandit Thompson Sampling) and
ebmUCB (Empirical Bayesian Multi-Bandit Upper Confidence Bound), both of which
incorporate the estimated prior into the decision-making process. We provide
the frequentist regret upper bounds for the proposed algorithms, thereby
filling a research gap in the field of multi-bandit problems. Extensive
experiments on both synthetic and real-world datasets demonstrate the superior
performance of our algorithms, particularly in complex environments. Our
methods achieve lower cumulative regret compared to existing techniques,
highlighting their effectiveness in balancing exploration and exploitation
across multi-bandits.

</details>
