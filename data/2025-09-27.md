<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 3]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models](https://arxiv.org/abs/2509.20367)
*Leyi Ouyang*

Main category: cs.CL

TL;DR: 提出了一种新框架，通过修改外交事件叙事以转变公众情绪，利用语言模型预测公众反应，并生成反事实文本。


<details>
  <summary>Details</summary>
Motivation: 公众情绪在外交中至关重要，传统评估方法耗时且缺乏前瞻性。

Method: 训练语言模型预测公众反应，与领域专家合作确定文本特征进行修改，使用反事实生成算法系统地产生修改版本。

Result: 该框架成功地将公众情绪转变为更有利的状态，成功率达70%。

Conclusion: 该框架可为外交官、政策制定者和传播专家提供实用工具，以促进更理想的公众情绪。

Abstract: Diplomatic events consistently prompt widespread public discussion and
debate. Public sentiment plays a critical role in diplomacy, as a good
sentiment provides vital support for policy implementation, helps resolve
international issues, and shapes a nation's international image. Traditional
methods for gauging public sentiment, such as large-scale surveys or manual
content analysis of media, are typically time-consuming, labor-intensive, and
lack the capacity for forward-looking analysis. We propose a novel framework
that identifies specific modifications for diplomatic event narratives to shift
public sentiment from negative to neutral or positive. First, we train a
language model to predict public reaction towards diplomatic events. To this
end, we construct a dataset comprising descriptions of diplomatic events and
their associated public discussions. Second, guided by communication theories
and in collaboration with domain experts, we predetermined several textual
features for modification, ensuring that any alterations changed the event's
narrative framing while preserving its core facts.We develop a counterfactual
generation algorithm that employs a large language model to systematically
produce modified versions of an original text. The results show that this
framework successfully shifted public sentiment to a more favorable state with
a 70\% success rate. This framework can therefore serve as a practical tool for
diplomats, policymakers, and communication specialists, offering data-driven
insights on how to frame diplomatic initiatives or report on events to foster a
more desirable public sentiment.

</details>


### [2] [Speaker Style-Aware Phoneme Anchoring for Improved Cross-Lingual Speech Emotion Recognition](https://arxiv.org/abs/2509.20373)
*Shreya G. Upadhyay,Carlos Busso,Chi-Chun Lee*

Main category: cs.CL

TL;DR: 本文提出了一种跨语言语音情感识别框架，该框架通过在音标和说话人层面进行对齐，从而有效捕捉不同语言和说话人之间的情感。


<details>
  <summary>Details</summary>
Motivation: 跨语言语音情感识别由于不同语言在语音变异性和说话人特定表达风格上的差异而具有挑战性。需要在各种条件下有效捕捉情感，这需要一个能够对齐不同说话人和语言情感外化的框架。

Method: 该方法通过基于图的聚类构建特定于情感的说话人社区，以捕获共享的说话人特征。利用这些组，我们在说话人和语音空间中应用双空间锚定，以实现跨语言的更好情感转移。

Result: 在 MSP-Podcast (英语) 和 BIIC-Podcast (台湾普通话) 语料库上的评估表明，与竞争基线相比，泛化能力有所提高，并为跨语言情感表征的共性提供了有价值的见解。

Conclusion: 所提出的说话人风格感知音素锚定框架能够有效提升跨语言语音情感识别的性能。

Abstract: Cross-lingual speech emotion recognition (SER) remains a challenging task due
to differences in phonetic variability and speaker-specific expressive styles
across languages. Effectively capturing emotion under such diverse conditions
requires a framework that can align the externalization of emotions across
different speakers and languages. To address this problem, we propose a
speaker-style aware phoneme anchoring framework that aligns emotional
expression at the phonetic and speaker levels. Our method builds
emotion-specific speaker communities via graph-based clustering to capture
shared speaker traits. Using these groups, we apply dual-space anchoring in
speaker and phonetic spaces to enable better emotion transfer across languages.
Evaluations on the MSP-Podcast (English) and BIIC-Podcast (Taiwanese Mandarin)
corpora demonstrate improved generalization over competitive baselines and
provide valuable insights into the commonalities in cross-lingual emotion
representation.

</details>


### [3] [CFD-LLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics](https://arxiv.org/abs/2509.20374)
*Nithin Somasekharan,Ling Yue,Yadi Cao,Weichao Li,Patrick Emami,Pochinapeddi Sai Bhargav,Anurag Acharya,Xingyu Xie,Shaowu Pan*

Main category: cs.CL

TL;DR: 本文介绍了一个名为CFDLLMBench的基准测试套件，用于评估大型语言模型（LLM）在计算流体动力学（CFD）领域的性能。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLM）在自动化复杂物理系统数值实验中的应用，特别是在计算流体动力学（CFD）这一关键且劳动密集型领域。

Method: 设计了CFDLLMBench基准测试套件，包含CFDQuery、CFDCodeBench和FoamBench三个互补组件，全面评估LLM在CFD知识、数值和物理推理以及工作流程实现方面的能力。

Result: CFDLLMBench基准测试套件能够提供可重现的结果，并量化LLM在代码可执行性、解决方案准确性和数值收敛行为方面的性能。

Conclusion: CFDLLMBench为开发和评估LLM驱动的复杂物理系统数值实验自动化奠定了坚实的基础。

Abstract: Large Language Models (LLMs) have demonstrated strong performance across
general NLP tasks, but their utility in automating numerical experiments of
complex physical system -- a critical and labor-intensive component -- remains
underexplored. As the major workhorse of computational science over the past
decades, Computational Fluid Dynamics (CFD) offers a uniquely challenging
testbed for evaluating the scientific capabilities of LLMs. We introduce
CFDLLMBench, a benchmark suite comprising three complementary components --
CFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM
performance across three key competencies: graduate-level CFD knowledge,
numerical and physical reasoning of CFD, and context-dependent implementation
of CFD workflows. Grounded in real-world CFD practices, our benchmark combines
a detailed task taxonomy with a rigorous evaluation framework to deliver
reproducible results and quantify LLM performance across code executability,
solution accuracy, and numerical convergence behavior. CFDLLMBench establishes
a solid foundation for the development and evaluation of LLM-driven automation
of numerical experiments for complex physical systems. Code and data are
available at https://github.com/NREL-Theseus/cfdllmbench/.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [Leveraging NTPs for Efficient Hallucination Detection in VLMs](https://arxiv.org/abs/2509.20379)
*Ofir Azachi,Kfir Eliyahu,Eyal El Ani,Rom Himelstein,Roi Reichart,Yuval Pinter,Nitay Calderon*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于视觉语言模型（VLM）的 next-token probabilities (NTPs) 的轻量级幻觉检测方法，旨在解决 VLM 中视觉内容和生成文本之间的不对齐问题。


<details>
  <summary>Details</summary>
Motivation: VLM 容易产生幻觉，影响其可靠性，而使用 VLM 检测幻觉计算成本高、延迟大。

Method: 通过训练传统机器学习模型，利用 VLM 的 NTPs 作为信号，实现对幻觉的快速检测。还引入了一个包含 1400 个人工标注语句的数据集来测试该方法。

Result: 基于 NTP 的特征可以有效预测幻觉，简单 ML 模型可以达到与强大的 VLM 相当的性能。结合语言 NTPs 和将 VLM 的幻觉预测分数整合到基于 NTP 的模型中，可以进一步提高性能。

Conclusion: 该研究提出了一种简单轻量级的解决方案，有望提高 VLM 的可靠性。

Abstract: Hallucinations of vision-language models (VLMs), which are misalignments
between visual content and generated text, undermine the reliability of VLMs.
One common approach for detecting them employs the same VLM, or a different
one, to assess generated outputs. This process is computationally intensive and
increases model latency. In this paper, we explore an efficient on-the-fly
method for hallucination detection by training traditional ML models over
signals based on the VLM's next-token probabilities (NTPs). NTPs provide a
direct quantification of model uncertainty. We hypothesize that high
uncertainty (i.e., a low NTP value) is strongly associated with hallucinations.
To test this, we introduce a dataset of 1,400 human-annotated statements
derived from VLM-generated content, each labeled as hallucinated or not, and
use it to test our NTP-based lightweight method. Our results demonstrate that
NTP-based features are valuable predictors of hallucinations, enabling fast and
simple ML models to achieve performance comparable to that of strong VLMs.
Furthermore, augmenting these NTPs with linguistic NTPs, computed by feeding
only the generated text back into the VLM, enhances hallucination detection
performance. Finally, integrating hallucination prediction scores from VLMs
into the NTP-based models led to better performance than using either VLMs or
NTPs alone. We hope this study paves the way for simple, lightweight solutions
that enhance the reliability of VLMs.

</details>


### [5] [Quasi-Synthetic Riemannian Data Generation for Writer-Independent Offline Signature Verification](https://arxiv.org/abs/2509.20420)
*Elias N. Zois,Moises Diaz,Salem Said,Miguel A. Ferrer*

Main category: cs.CV

TL;DR: 提出了一种准合成数据生成框架，利用对称正定矩阵 (SPD) 的黎曼几何，在黎曼空间中生成合成数据，用于离线手写签名验证。


<details>
  <summary>Details</summary>
Motivation: 离线手写签名验证在作者独立的环境中仍然具有挑战性，因为模型必须在未见过的个体之间进行泛化。过去的方法通常依赖于真实世界的签名数据集进行分类器训练。

Method: 利用 SPD 空间的黎曼几何，以 SPD 空间中的一小组真实样本作为种子，构建黎曼高斯混合模型，该模型将黎曼中心识别为合成作者，并将方差识别为其属性。在每个中心上进行黎曼高斯抽样，生成正的和负的合成 SPD 样本。利用度量学习框架，使用成对的相似和不相似的 SPD 点进行训练，然后在真实数据集上进行测试。

Result: 在两个流行的签名数据集（包括西方和亚洲书写风格）上进行的实验表明，所提出的方法在内部和交叉数据集评估协议下均有效。准合成方法实现了低错误率。

Conclusion: 在黎曼空间中生成合成数据对于作者独立的签名验证系统具有潜力。

Abstract: Offline handwritten signature verification remains a challenging task,
particularly in writer-independent settings where models must generalize across
unseen individuals. Recent developments have highlighted the advantage of
geometrically inspired representations, such as covariance descriptors on
Riemannian manifolds. However, past or present, handcrafted or data-driven
methods usually depend on real-world signature datasets for classifier
training. We introduce a quasi-synthetic data generation framework leveraging
the Riemannian geometry of Symmetric Positive Definite matrices (SPD). A small
set of genuine samples in the SPD space is the seed to a Riemannian Gaussian
Mixture which identifies Riemannian centers as synthetic writers and variances
as their properties. Riemannian Gaussian sampling on each center generates
positive as well as negative synthetic SPD populations. A metric learning
framework utilizes pairs of similar and dissimilar SPD points, subsequently
testing it over on real-world datasets. Experiments conducted on two popular
signature datasets, encompassing Western and Asian writing styles, demonstrate
the efficacy of the proposed approach under both intra- and cross- dataset
evaluation protocols. The results indicate that our quasi-synthetic approach
achieves low error rates, highlighting the potential of generating synthetic
data in Riemannian spaces for writer-independent signature verification
systems.

</details>


### [6] [Seedream 4.0: Toward Next-generation Multimodal Image Generation](https://arxiv.org/abs/2509.20427)
*Team Seedream,Yunpeng Chen,Yu Gao,Lixue Gong,Meng Guo,Qiushan Guo,Zhiyao Guo,Xiaoxia Hou,Weilin Huang,Yixuan Huang,Xiaowen Jian,Huafeng Kuang,Zhichao Lai,Fanshi Li,Liang Li,Xiaochen Lian,Chao Liao,Liyang Liu,Wei Liu,Yanzuo Lu,Zhengxiong Luo,Tongtong Ou,Guang Shi,Yichun Shi,Shiqi Sun,Yu Tian,Zhi Tian,Peng Wang,Rui Wang,Xun Wang,Ye Wang,Guofeng Wu,Jie Wu,Wenxu Wu,Yonghui Wu,Xin Xia,Xuefeng Xiao,Shuang Xu,Xin Yan,Ceyuan Yang,Jianchao Yang,Zhonghua Zhai,Chenlin Zhang,Heng Zhang,Qi Zhang,Xinyu Zhang,Yuwei Zhang,Shijia Zhao,Wenliang Zhao,Wenjia Zhu*

Main category: cs.CV

TL;DR: Seedream 4.0是一个高效、高性能的多模态图像生成系统，统一了文本到图像的合成、图像编辑和多图像组合。


<details>
  <summary>Details</summary>
Motivation: 为了创建一个统一的框架，能够同时处理文本到图像的生成，图像编辑和多图像组合任务。

Method: 开发了一个高效的扩散transformer和一个强大的VAE，并结合多模态后训练、对抗蒸馏、分布匹配、量化和推测解码等技术。

Result: 实现了快速生成原生高分辨率图像（如1K-4K），并在T2I和多模态图像编辑方面达到了最先进的结果，生成2K图像的推理时间仅为1.8秒。

Conclusion: Seedream 4.0将传统的T2I系统扩展为一个更具互动性和多维度的创意工具，推动了生成人工智能在创意和专业应用方面的边界。

Abstract: We introduce Seedream 4.0, an efficient and high-performance multimodal image
generation system that unifies text-to-image (T2I) synthesis, image editing,
and multi-image composition within a single framework. We develop a highly
efficient diffusion transformer with a powerful VAE which also can reduce the
number of image tokens considerably. This allows for efficient training of our
model, and enables it to fast generate native high-resolution images (e.g.,
1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning
diverse taxonomies and knowledge-centric concepts. Comprehensive data
collection across hundreds of vertical scenarios, coupled with optimized
strategies, ensures stable and large-scale training, with strong
generalization. By incorporating a carefully fine-tuned VLM model, we perform
multi-modal post-training for training both T2I and image editing tasks
jointly. For inference acceleration, we integrate adversarial distillation,
distribution matching, and quantization, as well as speculative decoding. It
achieves an inference time of up to 1.8 seconds for generating a 2K image
(without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream
4.0 can achieve state-of-the-art results on both T2I and multimodal image
editing. In particular, it demonstrates exceptional multimodal capabilities in
complex tasks, including precise image editing and in-context reasoning, and
also allows for multi-image reference, and can generate multiple output images.
This extends traditional T2I systems into an more interactive and
multidimensional creative tool, pushing the boundary of generative AI for both
creativity and professional applications. Seedream 4.0 is now accessible on
https://www.volcengine.com/experience/ark?launch=seedream.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [An Approach to Checking Correctness for Agentic Systems](https://arxiv.org/abs/2509.20364)
*Thomas J Sheffler*

Main category: cs.AI

TL;DR: 提出了一种时间表达式语言，用于监控AI代理行为，实现对基于LLM的代理系统的系统性错误检测。


<details>
  <summary>Details</summary>
Motivation: 当前错误检测方法主要依赖于输入和输出的文本匹配，由于LLM响应中固有的自然语言可变性，这种方法显得脆弱。本文提出了一种方法，专注于代理行为的序列，允许独立于特定文本输出验证系统行为。

Method: 使用时间逻辑技术，监控代理工具调用和状态转换的执行轨迹，以检测与预期行为模式的偏差。

Result: 当使用大型模型时，所有时间断言都满足。当较小模型被替换时，执行违反了行为断言，主要是由于不正确的工具排序和失败的协调切换。

Conclusion: 该方法为系统性监控AI代理的可靠性奠定了基础，因为这些系统越来越多地部署在关键应用中。

Abstract: This paper presents a temporal expression language for monitoring AI agent
behavior, enabling systematic error-detection of LLM-based agentic systems that
exhibit variable outputs due to stochastic generation processes. Drawing from
temporal logic techniques used in hardware verification, this approach monitors
execution traces of agent tool calls and state transitions to detect deviations
from expected behavioral patterns. Current error-detection approaches rely
primarily on text matching of inputs and outputs, which proves fragile due to
the natural language variability inherent in LLM responses. The proposed method
instead focuses on the sequence of agent actions -- such as tool invocations
and inter-agent communications -- allowing verification of system behavior
independent of specific textual outputs. The temporal expression language
provides assertions that capture correct behavioral patterns across multiple
execution scenarios. These assertions serve dual purposes: validating prompt
engineering and guardrail effectiveness during development, and providing
regression testing when agents are updated with new LLMs or modified logic. The
approach is demonstrated using a three-agent system, where agents coordinate to
solve multi-step reasoning tasks. When powered by large, capable models, all
temporal assertions were satisfied across many test runs. However, when smaller
models were substituted in two of the three agents, executions violated
behavioral assertions, primarily due to improper tool sequencing and failed
coordination handoffs. The temporal expressions successfully flagged these
anomalies, demonstrating the method's effectiveness for detecting behavioral
regressions in production agentic systems. This approach provides a foundation
for systematic monitoring of AI agent reliability as these systems become
increasingly deployed in critical applications.

</details>


### [8] [LATTS: Locally Adaptive Test-Time Scaling](https://arxiv.org/abs/2509.20368)
*Theo Uscidda,Matthew Trager,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: 提出了一种名为 Locally Adaptive Test-Time Scaling (LATTS) 的方法，它根据验证器模型衍生的局部难度来调整每步的计算量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在所有样本和生成步骤中统一增加计算量，而没有考虑个体实例的复杂性，导致资源使用效率低下。

Method: 在每个生成步骤中，LATTS 采用基于验证器的接受标准来决定是否重新采样、回溯、重新启动或停止生成过程。

Result: 实验结果表明，与标准的基于验证器的方法相比，LATTS 实现了明显更高的精度-计算权衡。

Conclusion: LATTS 能够更有效地利用计算资源，并在提高准确性的同时降低计算成本。

Abstract: One common strategy for improving the performance of Large Language Models
(LLMs) on downstream tasks involves using a \emph{verifier model} to either
select the best answer from a pool of candidates or to steer the
auto-regressive generation process towards better outputs. This class of
methods typically results in improved accuracy at the cost of increased
computation at test-time, a paradigm known as \emph{test-time scaling}.
However, most existing approaches increase computation uniformly across all
samples and generation steps, without considering the complexity of individual
instances, leading to inefficient resource use. We address this limitation by
proposing an approach, called \emph{Locally Adaptive Test-Time Scaling
(LATTS)}, that allocates variable compute across generation steps.
Specifically, at each generation step, LATTS employs a verifier-based
acceptance criterion to decide whether to resample, backtrack, restart, or stop
the generation process. This criterion effectively adjusts the per-step
computational effort based on a precise notion of \emph{local difficulty}
derived from the verifier model. Empirical results show that LATTS achieves
significantly superior accuracy--compute tradeoffs compared to standard
verifier-based methods.

</details>


### [9] [Philosophy-informed Machine Learning](https://arxiv.org/abs/2509.20370)
*MZ Naser*

Main category: cs.AI

TL;DR: 本文综述了哲学指导的机器学习(PhIML)，它将分析哲学的核心思想直接注入到ML模型架构、目标和评估协议中。


<details>
  <summary>Details</summary>
Motivation: 通过尊重哲学概念和价值的模型，PhIML有望带来新的能力。

Method: 本文回顾了概念基础，以展示哲学上的收益和一致性。此外，我们还介绍了ML用户/设计者如何采用PhIML作为不可知的事后工具或将其内在构建到ML模型架构中的案例研究。

Result: 本文揭示了开放的技术障碍以及哲学、实践和治理挑战。

Conclusion: 本文概述了迈向安全、具有哲学意识和对道德负责的PhIML的研究路线图。

Abstract: Philosophy-informed machine learning (PhIML) directly infuses core ideas from
analytic philosophy into ML model architectures, objectives, and evaluation
protocols. Therefore, PhIML promises new capabilities through models that
respect philosophical concepts and values by design. From this lens, this paper
reviews conceptual foundations to demonstrate philosophical gains and
alignment. In addition, we present case studies on how ML users/designers can
adopt PhIML as an agnostic post-hoc tool or intrinsically build it into ML
model architectures. Finally, this paper sheds light on open technical barriers
alongside philosophical, practical, and governance challenges and outlines a
research roadmap toward safe, philosophy-aware, and ethically responsible
PhIML.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [10] [DELM: a Python toolkit for Data Extraction with Language Models](https://arxiv.org/abs/2509.20617)
*Eric Fithian,Kirill Skobelev*

Main category: cs.IR

TL;DR: DELM是一个用于LLM数据提取的Python工具包，旨在提高可重复性、鲁棒性和系统评估。


<details>
  <summary>Details</summary>
Motivation: 现有工作流程依赖于临时脚本，导致可重复性、鲁棒性和系统评估方面存在困难。

Method: DELM提供了一个模块化框架，具有结构化输出、内置验证、灵活的数据加载和评分策略以及高效的批处理。

Result: DELM通过案例研究展示了其能力，包括提示优化算法和成本与覆盖率之间的权衡。

Conclusion: DELM是一个开源工具包，可用于快速实验LLM数据提取管道，并量化它们之间的权衡。

Abstract: Large Language Models (LLMs) have become powerful tools for annotating
unstructured data. However, most existing workflows rely on ad hoc scripts,
making reproducibility, robustness, and systematic evaluation difficult. To
address these challenges, we introduce DELM (Data Extraction with Language
Models), an open-source Python toolkit designed for rapid experimental
iteration of LLM-based data extraction pipelines and for quantifying the
trade-offs between them. DELM minimizes boilerplate code and offers a modular
framework with structured outputs, built-in validation, flexible data-loading
and scoring strategies, and efficient batch processing. It also includes robust
support for working with LLM APIs, featuring retry logic, result caching,
detailed cost tracking, and comprehensive configuration management. We showcase
DELM's capabilities through two case studies: one featuring a novel prompt
optimization algorithm, and another illustrating how DELM quantifies trade-offs
between cost and coverage when selecting keywords to decide which paragraphs to
pass to an LLM. DELM is available at
\href{https://github.com/Center-for-Applied-AI/delm}{\texttt{github.com/Center-for-Applied-AI/delm}}.

</details>


### [11] [Provenance Analysis of Archaeological Artifacts via Multimodal RAG Systems](https://arxiv.org/abs/2509.20769)
*Tuo Zhang,Yuechun Sun,Ruiliang Liu*

Main category: cs.IR

TL;DR: 该论文提出了一个基于检索增强生成（RAG）的系统，用于考古文物的溯源分析。


<details>
  <summary>Details</summary>
Motivation: 旨在通过整合多模态检索和大型视觉-语言模型（VLMs）来支持专家推理，减轻学者浏览大量比较语料库的认知负担。

Method: 该系统构建了一个来自参考文本和图像的双模态知识库，支持原始视觉、边缘增强和语义检索以识别风格相似的物体。检索到的候选对象由 VLM 合成，以生成结构化的推论，包括年代、地理和文化归属，以及解释性理由。

Result: 在来自大英博物馆的欧亚青铜时代文物上进行的评估表明，该系统产生了有意义且可解释的输出。

Conclusion: 该系统为学者提供了具体的分析起点，显著减轻了认知负担。

Abstract: In this work, we present a retrieval-augmented generation (RAG)-based system
for provenance analysis of archaeological artifacts, designed to support expert
reasoning by integrating multimodal retrieval and large vision-language models
(VLMs). The system constructs a dual-modal knowledge base from reference texts
and images, enabling raw visual, edge-enhanced, and semantic retrieval to
identify stylistically similar objects. Retrieved candidates are synthesized by
the VLM to generate structured inferences, including chronological,
geographical, and cultural attributions, alongside interpretive justifications.
We evaluate the system on a set of Eastern Eurasian Bronze Age artifacts from
the British Museum. Expert evaluation demonstrates that the system produces
meaningful and interpretable outputs, offering scholars concrete starting
points for analysis and significantly alleviating the cognitive burden of
navigating vast comparative corpora.

</details>


### [12] [Performance Consistency of Learning Methods for Information Retrieval Tasks](https://arxiv.org/abs/2509.20804)
*Meng Yuan,Justin Zobel*

Main category: cs.IR

TL;DR: 该论文探讨了信息检索（IR）方法性能评估的准确性和鲁棒性问题，发现基于 Transformer 的模型对训练不稳定非常敏感，质疑了以往结果的可靠性，强调了严格评估实践的必要性。


<details>
  <summary>Details</summary>
Motivation: 评估信息检索（IR）方法测量性能的准确性或鲁棒性。

Method: 使用随机种子检查各种传统统计学习模型和基于 Transformer 的学习模型的性能变化。

Result: 统计模型稳定，而 Transformer 模型随着种子的变化表现出巨大差异。在 9/11 的情况下，F1 分数（范围 0.0--1.0）的标准偏差超过 0.075；而 7/11 的精确度值（也在 0.0--1.0 范围内）的标准偏差超过 0.125。

Conclusion: Transformer 模型容易受到训练不稳定性的影响，对先前结果的可靠性提出了质疑，因此强调需要严格的评估实践。

Abstract: A range of approaches have been proposed for estimating the accuracy or
robustness of the measured performance of IR methods. One is to use
bootstrapping of test sets, which, as we confirm, provides an estimate of
variation in performance. For IR methods that rely on a seed, such as those
that involve machine learning, another approach is to use a random set of seeds
to examine performance variation. Using three different IR tasks we have used
such randomness to examine a range of traditional statistical learning models
and transformer-based learning models. While the statistical models are stable,
the transformer models show huge variation as seeds are changed. In 9 of 11
cases the F1-scores (in the range 0.0--1.0) had a standard deviation of over
0.075; while 7 of 11 precision values (also in the range 0.0--1.0) had a
standard deviation of over 0.125. This is in a context where differences of
less than 0.02 have been used as evidence of method improvement. Our findings
highlight the vulnerability of transformer models to training instabilities and
moreover raise questions about the reliability of previous results, thus
underscoring the need for rigorous evaluation practices.

</details>


### [13] [RecIS: Sparse to Dense, A Unified Training Framework for Recommendation Models](https://arxiv.org/abs/2509.20883)
*Hua Zong,Qingtao Zeng,Zhengxiong Zhou,Zhihua Han,Zhensong Yan,Mingjie Liu,Hechen Sun,Jiawei Liu,Yiwen Hu,Qi Wang,YiHan Xian,Wenjie Guo,Houyuan Xiang,Zhiyuan Zeng,Xiangrong Sheng,Bencheng Yan,Nan Hu,Yuheng Huang,Jinqing Lian,Ziru Xu,Yan Zhang,Ju Huang,Siran Yang,Huimin Yi,Jiamang Wang,Pengjie Wang,Han Zhu,Jian Wu,Dan Ou,Jian Xu,Haihong Tang,Yuning Jiang,Bo Zheng,Lin Qu*

Main category: cs.IR

TL;DR: RecIS: A unified sparse-dense training framework.


<details>
  <summary>Details</summary>
Motivation: To create a unified sparse-dense training framework based on the PyTorch ecosystem that meets the training needs of industrial-grade recommendation models that integrated with large models and to optimize the sparse component, offering superior efficiency over the TensorFlow-based recommendation models.

Method: A unified Sparse-Dense training framework

Result: RecIS is being used in Alibaba for numerous large-model enhanced recommendation training tasks, and some traditional sparse models have also begun training in it.

Conclusion: RecIS is a unified sparse-dense training framework designed to achieve two primary goals: 1. Unified Framework 2.System Optimization

Abstract: In this paper, we propose RecIS, a unified Sparse-Dense training framework
designed to achieve two primary goals: 1. Unified Framework To create a Unified
sparse-dense training framework based on the PyTorch ecosystem that meets the
training needs of industrial-grade recommendation models that integrated with
large models. 2.System Optimization To optimize the sparse component, offering
superior efficiency over the TensorFlow-based recommendation models. The dense
component, meanwhile, leverages existing optimization technologies within the
PyTorch ecosystem. Currently, RecIS is being used in Alibaba for numerous
large-model enhanced recommendation training tasks, and some traditional sparse
models have also begun training in it.

</details>


### [14] [FORGE: Forming Semantic Identifiers for Generative Retrieval in Industrial Datasets](https://arxiv.org/abs/2509.20904)
*Kairui Fu,Tao Zhang,Shuwen Xiao,Ziyang Wang,Xinming Zhang,Chenchi Zhang,Yuliang Yan,Junjun Zheng,Yu Li,Zhihong Chen,Jian Wu,Xiangheng Kong,Shengyu Zhang,Kun Kuang,Yuning Jiang,Bo Zheng*

Main category: cs.IR

TL;DR: 本研究提出了FORGE，一个用于生成式检索中语义标识符（SID）的综合基准，包含大规模数据集和优化策略。


<details>
  <summary>Details</summary>
Motivation: 当前SID研究面临缺乏大规模多模态数据集、SID生成优化策略研究不足以及在线收敛速度慢等挑战。

Method: 构建了包含140亿用户交互和2.5亿商品多模态特征的数据集，并探索了多种优化方法来提升SID构建。

Result: 离线实验验证了优化方法的有效性，在线分析显示交易数量增加了0.35%。提出了两种与推荐性能正相关的SID新指标，无需GR训练即可进行评估。离线预训练模式将在线收敛速度缩短了一半。

Conclusion: FORGE通过提供大规模数据集、优化策略和评估指标，有效提升了生成式检索中语义标识符的性能和效率，并在实际应用中取得了显著效果。

Abstract: Semantic identifiers (SIDs) have gained increasing attention in generative
retrieval (GR) due to their meaningful semantic discriminability. However,
current research on SIDs faces three main challenges: (1) the absence of
large-scale public datasets with multimodal features, (2) limited investigation
into optimization strategies for SID generation, which typically rely on costly
GR training for evaluation, and (3) slow online convergence in industrial
deployment. To address these challenges, we propose FORGE, a comprehensive
benchmark for FOrming semantic identifieR in Generative rEtrieval with
industrial datasets. Specifically, FORGE is equipped with a dataset comprising
14 billion user interactions and multimodal features of 250 million items
sampled from Taobao, one of the biggest e-commerce platforms in China.
Leveraging this dataset, FORGE explores several optimizations to enhance the
SID construction and validates their effectiveness via offline experiments
across different settings and tasks. Further online analysis conducted on our
platform, which serves over 300 million users daily, reveals a 0.35% increase
in transaction count, highlighting the practical impact of our method.
Regarding the expensive SID validation accompanied by the full training of GRs,
we propose two novel metrics of SID that correlate positively with
recommendation performance, enabling convenient evaluations without any GR
training. For real-world applications, FORGE introduces an offline pretraining
schema that reduces online convergence by half. The code and data are available
at https://github.com/selous123/al_sid.

</details>


### [15] [Markup Language Modeling for Web Document Understanding](https://arxiv.org/abs/2509.20940)
*Su Liu,Bin Bi,Jan Bakus,Paritosh Kumar Velalam,Vijay Yella,Vinod Hegde*

Main category: cs.IR

TL;DR: 本文介绍了一种用于从购物评论网站提取详细产品信息的方法，以构建最新的产品数据库。


<details>
  <summary>Details</summary>
Motivation: 构建最新的产品数据库，支持客户分析和产品推荐等任务。

Method: 在从不同大小的评论网站收集的产品数据上微调 MarkupLM，并开发了一个名为 MarkupLM++ 的变体，该变体将预测扩展到 DOM 树的内部节点。

Result: 使用更大、更多样化的训练集可以提高整体提取准确率。包含内部节点有助于某些产品属性的提取，但会导致整体性能略有下降。最终模型的精确率为 0.906，召回率为 0.724，F1 值为 0.805。

Conclusion: 该模型在产品信息提取方面表现出良好的性能，但仍有改进空间。

Abstract: Web information extraction (WIE) is an important part of many e-commerce
systems, supporting tasks like customer analysis and product recommendation. In
this work, we look at the problem of building up-to-date product databases by
extracting detailed information from shopping review websites. We fine-tuned
MarkupLM on product data gathered from review sites of different sizes and then
developed a variant we call MarkupLM++, which extends predictions to internal
nodes of the DOM tree. Our experiments show that using larger and more diverse
training sets improves extraction accuracy overall. We also find that including
internal nodes helps with some product attributes, although it leads to a
slight drop in overall performance. The final model reached a precision of
0.906, recall of 0.724, and an F1 score of 0.805.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [16] [A Theory of Multi-Agent Generative Flow Networks](https://arxiv.org/abs/2509.20408)
*Leo Maxime Brunswic,Haozhi Wang,Shuang Luo,Jianye Hao,Amir Rasouli,Yinchuan Li*

Main category: cs.LG

TL;DR: 本文提出了多智能体生成流网络（MA-GFlowNets）的理论框架，用于多个智能体通过一系列联合动作协同生成对象。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏针对多智能体生成流网络的理论框架。

Method: 提出了四种算法：集中式流网络、独立流网络、联合流网络及其条件版本。联合流训练基于局部-全局原则。

Result: 实验结果表明，与强化学习和基于MCMC的方法相比，所提出的框架具有优越性。

Conclusion: 所提出的MA-GFlowNets框架有效，并在实验中表现出优越性。

Abstract: Generative flow networks utilize a flow-matching loss to learn a stochastic
policy for generating objects from a sequence of actions, such that the
probability of generating a pattern can be proportional to the corresponding
given reward. However, a theoretical framework for multi-agent generative flow
networks (MA-GFlowNets) has not yet been proposed. In this paper, we propose
the theory framework of MA-GFlowNets, which can be applied to multiple agents
to generate objects collaboratively through a series of joint actions. We
further propose four algorithms: a centralized flow network for centralized
training of MA-GFlowNets, an independent flow network for decentralized
execution, a joint flow network for achieving centralized training with
decentralized execution, and its updated conditional version. Joint Flow
training is based on a local-global principle allowing to train a collection of
(local) GFN as a unique (global) GFN. This principle provides a loss of
reasonable complexity and allows to leverage usual results on GFN to provide
theoretical guarantees that the independent policies generate samples with
probability proportional to the reward function. Experimental results
demonstrate the superiority of the proposed framework compared to reinforcement
learning and MCMC-based methods.

</details>


### [17] [Sig2Model: A Boosting-Driven Model for Updatable Learned Indexes](https://arxiv.org/abs/2509.20781)
*Alireza Heidari,Amirhossein Ahmad,Wei Zhang,Ying Xiong*

Main category: cs.LG

TL;DR: Sig2Model: An efficient and adaptive learned index that minimizes retraining cost through sigmoid boosting, proactive update training, and neural joint optimization.


<details>
  <summary>Details</summary>
Motivation: Current learned indexes degrade under dynamic updates because maintaining the CDF invariant requires global model retraining, which blocks queries and limits the queries-per-second (QPS) metric. Current approaches fail to address these retraining costs effectively.

Method: The paper presents Sig2Model, which uses three key techniques: sigmoid boosting approximation, proactive update training via Gaussian mixture models (GMMs), and a neural joint optimization framework.

Result: Sig2Model reduces retraining cost by up to 20x, achieves up to 3x higher QPS, and uses up to 1000x less memory compared to state-of-the-art updatable learned indexes.

Conclusion: Sig2Model is an efficient and adaptive learned index that minimizes retraining cost and outperforms existing approaches in dynamic update scenarios.

Abstract: Learned Indexes (LIs) represent a paradigm shift from traditional index
structures by employing machine learning models to approximate the cumulative
distribution function (CDF) of sorted data. While LIs achieve remarkable
efficiency for static datasets, their performance degrades under dynamic
updates: maintaining the CDF invariant (sum of F(k) equals 1) requires global
model retraining, which blocks queries and limits the queries-per-second (QPS)
metric. Current approaches fail to address these retraining costs effectively,
rendering them unsuitable for real-world workloads with frequent updates. In
this paper, we present Sig2Model, an efficient and adaptive learned index that
minimizes retraining cost through three key techniques: (1) a sigmoid boosting
approximation technique that dynamically adjusts the index model by
approximating update-induced shifts in data distribution with localized sigmoid
functions while preserving bounded error guarantees and deferring full
retraining; (2) proactive update training via Gaussian mixture models (GMMs)
that identifies high-update-probability regions for strategic placeholder
allocation to speed up updates; and (3) a neural joint optimization framework
that continuously refines both the sigmoid ensemble and GMM parameters via
gradient-based learning. We evaluate Sig2Model against state-of-the-art
updatable learned indexes on real-world and synthetic workloads, and show that
Sig2Model reduces retraining cost by up to 20x, achieves up to 3x higher QPS,
and uses up to 1000x less memory.

</details>


### [18] [FastEagle: Cascaded Drafting for Accelerating Speculative Decoding](https://arxiv.org/abs/2509.20416)
*Haiduo Huang,Jiangcheng Song,Wenzhe Zhao,Pengju Ren*

Main category: cs.LG

TL;DR: FastEagle是一种非自回归的级联drafter，它可以在单个前向传播中生成整个草稿，从而加速生成过程。


<details>
  <summary>Details</summary>
Motivation: 目前最好的drafter（例如EAGLE）仍然需要N个连续的pass来提议N个token，这限制了生成速度。

Method: FastEagle用轻量级的层级联替换了时间步，并使用层级监督进行训练，以减少误差累积。此外，FastEagle还采用了约束草稿树，以保持无损的验证成本。

Result: FastEagle在多个LLM和任务中，始终优于EAGLE-3，并且具有相当的平均接受长度。

Conclusion: 在drafting中移除顺序依赖性是实现无损LLM推理加速的一种可行方法。

Abstract: Speculative decoding accelerates generation by drafting candidates and
verifying them in parallel, yet state-of-the-art drafters (e.g., EAGLE) still
require N sequential passes to propose N tokens. We present FastEagle, a
non-autoregressive cascaded drafter that emits an entire draft in a single
forward pass. FastEagle replaces temporal steps with a lightweight layer
cascade and trains with layer-wise supervision to mitigate error accumulation.
Coupled with a constrained draft tree that preserves lossless verification
cost, FastEagle delivers substantial wall-clock speedups over strong
autoregressive drafters while maintaining competitive acceptance behavior.
Across multiple LLMs (Vicuna-13B, LLaMA-Instruct 3.x, and
DeepSeek-R1-Distill-LLaMA) and tasks (MT-Bench, HumanEval, GSM8K, CNN/DM,
Alpaca), FastEagle consistently outperforms EAGLE-3 in speedup under both
greedy and stochastic decoding, with comparable average acceptance lengths.
These results indicate that removing sequential dependencies in drafting is a
practical path toward lossless LLM inference acceleration.

</details>


### [19] [mloz: A Highly Efficient Machine Learning-Based Ozone Parameterization for Climate Sensitivity Simulations](https://arxiv.org/abs/2509.20422)
*Yiling Ma,Nathan Luke Abraham,Stefan Versick,Roland Ruhnke,Andrea Schneidereit,Ulrike Niemeier,Felix Back,Peter Braesicke,Peer Nowack*

Main category: cs.LG

TL;DR: 提出了一种机器学习参数化方法（mloz）来模拟对流层和平流层的每日臭氧变化和趋势。


<details>
  <summary>Details</summary>
Motivation: 由于大气化学方案的高计算成本，许多气候模型缺乏交互式臭氧表示。

Method: 使用大气温度剖面信息作为输入，mloz 在线模拟臭氧，并与 UKESM 和 ICON 模型集成。

Result: mloz 的速度比 UKESM 中的化学方案快 31 倍，且在气候模型运行时间中占比不到 4%。

Conclusion: mloz 具有高保真度、可移植性，并有潜力被广泛应用于缺乏交互式化学过程的 CMIP 级别气候模型中。

Abstract: Atmospheric ozone is a crucial absorber of solar radiation and an important
greenhouse gas. However, most climate models participating in the Coupled Model
Intercomparison Project (CMIP) still lack an interactive representation of
ozone due to the high computational costs of atmospheric chemistry schemes.
Here, we introduce a machine learning parameterization (mloz) to interactively
model daily ozone variability and trends across the troposphere and
stratosphere in standard climate sensitivity simulations, including two-way
interactions of ozone with the Quasi-Biennial Oscillation. We demonstrate its
high fidelity on decadal timescales and its flexible use online across two
different climate models -- the UK Earth System Model (UKESM) and the German
ICOsahedral Nonhydrostatic (ICON) model. With atmospheric temperature profile
information as the only input, mloz produces stable ozone predictions around 31
times faster than the chemistry scheme in UKESM, contributing less than 4
percent of the respective total climate model runtimes. In particular, we also
demonstrate its transferability to different climate models without chemistry
schemes by transferring the parameterization from UKESM to ICON. This
highlights the potential for widespread adoption in CMIP-level climate models
that lack interactive chemistry for future climate change assessments,
particularly when focusing on climate sensitivity simulations, where ozone
trends and variability are known to significantly modulate atmospheric feedback
processes.

</details>
