<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 31]
- [cs.CV](#cs.CV) [Total: 34]
- [cs.AI](#cs.AI) [Total: 21]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.LG](#cs.LG) [Total: 34]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Unifying Scheme for Extractive Content Selection Tasks](https://arxiv.org/abs/2507.16922)
*Shmuel Amar,Ori Shapira,Aviv Slobodkin,Ido Dagan*

Main category: cs.CL

TL;DR: 该论文提出了一个统一的instruction-guided content selection框架，并提供了一个基准测试和数据集来促进该框架的使用。


<details>
  <summary>Details</summary>
Motivation: 现有的NLP内容选择任务研究孤立，缺乏统一的建模方法、数据集和评估指标。

Method: 该论文提出了instruction-guided content selection (IGCS) 框架，使用instruction来指导语言模型进行内容选择。

Result: 该论文提出的框架和数据集能够提升内容选择任务的性能，即使在没有目标任务的专用训练的情况下。

Conclusion: 该论文提出了instruction-guided content selection (IGCS) 框架，并证明了其在统一内容选择任务中的有效性。同时，论文提供了一个统一的基准测试和大型通用合成数据集，并解决了通用推理时间问题。

Abstract: A broad range of NLP tasks involve selecting relevant text spans from given
source texts. Despite this shared objective, such \textit{content selection}
tasks have traditionally been studied in isolation, each with its own modeling
approaches, datasets, and evaluation metrics. In this work, we propose
\textit{instruction-guided content selection (IGCS)} as a beneficial unified
framework for such settings, where the task definition and any
instance-specific request are encapsulated as instructions to a language model.
To promote this framework, we introduce \igcsbench{}, the first unified
benchmark covering diverse content selection tasks. Further, we create a large
generic synthetic dataset that can be leveraged for diverse content selection
tasks, and show that transfer learning with these datasets often boosts
performance, whether dedicated training for the targeted task is available or
not. Finally, we address generic inference time issues that arise in LLM-based
modeling of content selection, assess a generic evaluation metric, and overall
propose the utility of our resources and methods for future content selection
models. Models and datasets available at https://github.com/shmuelamar/igcs.

</details>


### [2] [AI-based Clinical Decision Support for Primary Care: A Real-World Study](https://arxiv.org/abs/2507.16947)
*Robert Korom,Sarah Kiptinness,Najib Adan,Kassim Said,Catherine Ithuli,Oliver Rotich,Boniface Kimani,Irene King'ori,Stellah Kamau,Elizabeth Atemba,Muna Aden,Preston Bowman,Michael Sharman,Rebecca Soskin Hicks,Rebecca Distler,Johannes Heidecke,Rahul K. Arora,Karan Singhal*

Main category: cs.CL

TL;DR: AI Consult通过识别潜在的文档和临床决策错误，充当临床医生的安全保障，从而减少了诊断和治疗错误。


<details>
  <summary>Details</summary>
Motivation: 评估基于大型语言模型的临床决策支持在实时护理中的影响。

Method: 进行了一项质量改进研究，比较了15家诊所的临床医生在使用或不使用AI Consult的情况下进行的39,849次患者就诊的结果。由独立医生对就诊进行评估，以识别临床错误。

Result: 使用AI Consult的临床医生犯的错误相对较少：诊断错误减少16%，治疗错误减少13%。

Conclusion: LLM临床决策支持工具具有减少实际环境中错误的潜力，并为推进负责任的采用提供了实用的框架。

Abstract: We evaluate the impact of large language model-based clinical decision
support in live care. In partnership with Penda Health, a network of primary
care clinics in Nairobi, Kenya, we studied AI Consult, a tool that serves as a
safety net for clinicians by identifying potential documentation and clinical
decision-making errors. AI Consult integrates into clinician workflows,
activating only when needed and preserving clinician autonomy. We conducted a
quality improvement study, comparing outcomes for 39,849 patient visits
performed by clinicians with or without access to AI Consult across 15 clinics.
Visits were rated by independent physicians to identify clinical errors.
Clinicians with access to AI Consult made relatively fewer errors: 16% fewer
diagnostic errors and 13% fewer treatment errors. In absolute terms, the
introduction of AI Consult would avert diagnostic errors in 22,000 visits and
treatment errors in 29,000 visits annually at Penda alone. In a survey of
clinicians with AI Consult, all clinicians said that AI Consult improved the
quality of care they delivered, with 75% saying the effect was "substantial".
These results required a clinical workflow-aligned AI Consult implementation
and active deployment to encourage clinician uptake. We hope this study
demonstrates the potential for LLM-based clinical decision support tools to
reduce errors in real-world settings and provides a practical framework for
advancing responsible adoption.

</details>


### [3] [Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs](https://arxiv.org/abs/2507.16951)
*Shuyuan Lin,Lei Duan,Philip Hughes,Yuxuan Sheng*

Main category: cs.CL

TL;DR: SALU 是一种用于检测LLM中无法回答的问题的新方法，优于现有技术，并且不太可能产生幻觉。


<details>
  <summary>Details</summary>
Motivation: 会话信息检索 (CIR) 系统在提供对信息的直观访问的同时，面临着一个重大挑战：可靠地处理无法回答的问题，以防止产生误导性或幻觉内容。传统方法通常依赖于外部分类器，这可能会导致与核心生成大型语言模型 (LLM) 不一致。

Method: 提出了一种新颖的方法，即用于不可回答性的自我感知LLM (SALU)，该方法将不可回答性检测直接集成在LLM的生成过程中。它使用多任务学习框架进行训练，用于标准问答 (QA) 和不可回答查询的显式弃权生成。至关重要的是，它结合了置信度评分引导的强化学习与人工反馈 (RLHF) 阶段，该阶段明确惩罚幻觉反应并奖励适当的弃权，从而培养对知识边界的内在自我意识。

Result: 通过在我们定制的 C-IR_Answerability 数据集上进行的大量实验，SALU 在正确回答或放弃回答问题的整体准确性方面始终优于强大的基线，包括混合 LLM 分类器系统。人工评估进一步证实了 SALU 的卓越可靠性，在事实性、适当弃权方面取得了高分，最重要的是，幻觉显着减少。

Conclusion: SALU在事实性、适当弃权方面表现出色，并显著减少了幻觉，展示了其可靠性。

Abstract: Conversational Information Retrieval (CIR) systems, while offering intuitive
access to information, face a significant challenge: reliably handling
unanswerable questions to prevent the generation of misleading or hallucinated
content. Traditional approaches often rely on external classifiers, which can
introduce inconsistencies with the core generative Large Language Models
(LLMs). This paper introduces Self-Aware LLM for Unanswerability (SALU), a
novel approach that deeply integrates unanswerability detection directly within
the LLM's generative process. SALU is trained using a multi-task learning
framework for both standard Question Answering (QA) and explicit abstention
generation for unanswerable queries. Crucially, it incorporates a
confidence-score-guided reinforcement learning with human feedback (RLHF)
phase, which explicitly penalizes hallucinated responses and rewards
appropriate abstentions, fostering intrinsic self-awareness of knowledge
boundaries. Through extensive experiments on our custom-built
C-IR_Answerability dataset, SALU consistently outperforms strong baselines,
including hybrid LLM-classifier systems, in overall accuracy for correctly
answering or abstaining from questions. Human evaluation further confirms
SALU's superior reliability, achieving high scores in factuality, appropriate
abstention, and, most importantly, a dramatic reduction in hallucination,
demonstrating its ability to robustly "know when to say 'I don't know'."

</details>


### [4] [Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning](https://arxiv.org/abs/2507.16971)
*Aleksandr Perevalov,Andreas Both*

Main category: cs.CL

TL;DR: The paper introduces mKGQAgent, a framework that converts natural language questions into SPARQL queries using a coordinated LLM agent workflow. The approach achieved first place in the Text2SPARQL challenge 2025.


<details>
  <summary>Details</summary>
Motivation: Accessing knowledge via multilingual natural-language interfaces is one of the emerging challenges in the field of information retrieval and related ones. Structured knowledge stored in knowledge graphs can be queried via a specific query language (e.g., SPARQL). Therefore, one needs to transform natural-language input into a query to fulfill an information need.

Method: introducing mKGQAgent, a human-inspired framework that breaks down the task of converting natural language questions into SPARQL queries into modular, interpretable subtasks. By leveraging a coordinated LLM agent workflow for planning, entity linking, and query refinement - guided by an experience pool for in-context learning

Result: Evaluated on the DBpedia- and Corporate-based KGQA benchmarks within the Text2SPARQL challenge 2025, our approach took first place among the other participants.

Conclusion: This work opens new avenues for developing human-like reasoning systems in multilingual semantic parsing.

Abstract: Accessing knowledge via multilingual natural-language interfaces is one of
the emerging challenges in the field of information retrieval and related ones.
Structured knowledge stored in knowledge graphs can be queried via a specific
query language (e.g., SPARQL). Therefore, one needs to transform
natural-language input into a query to fulfill an information need. Prior
approaches mostly focused on combining components (e.g., rule-based or
neural-based) that solve downstream tasks and come up with an answer at the
end. We introduce mKGQAgent, a human-inspired framework that breaks down the
task of converting natural language questions into SPARQL queries into modular,
interpretable subtasks. By leveraging a coordinated LLM agent workflow for
planning, entity linking, and query refinement - guided by an experience pool
for in-context learning - mKGQAgent efficiently handles multilingual KGQA.
Evaluated on the DBpedia- and Corporate-based KGQA benchmarks within the
Text2SPARQL challenge 2025, our approach took first place among the other
participants. This work opens new avenues for developing human-like reasoning
systems in multilingual semantic parsing.

</details>


### [5] [Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain](https://arxiv.org/abs/2507.16974)
*Rishemjit Kaur,Arshdeep Singh Bhankhar,Surangika Ranathunga,Jashanpreet Singh Salh,Sudhir Rajput,Vidhi,Kashish Mahendra,Bhavika Berwal,Ritesh Kumar*

Main category: cs.CL

TL;DR: Fine-tuning language-specific LLMs with synthetic multilingual agricultural data improves accuracy and relevance for farmers in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: Enabling farmers to access accurate agriculture-related information in their native languages is crucial, but general-purpose LLMs lack precision in local and multilingual contexts due to insufficient domain-specific training and scarcity of high-quality, region-specific datasets.

Method: The study generates multilingual synthetic agricultural datasets (English, Hindi, Punjabi) from agriculture-specific documents and fine-tunes language-specific LLMs.

Result: The fine-tuned models showed significant improvements in factual accuracy, relevance, and agricultural consensus compared to their baseline counterparts on curated multilingual datasets.

Conclusion: This study demonstrates that fine-tuning language-specific LLMs with synthetic, multilingual agricultural datasets significantly improves their performance in factual accuracy, relevance, and agricultural consensus, especially in multilingual and low-resource settings. This approach helps bridge the knowledge gap in AI-driven agricultural solutions for diverse linguistic communities.

Abstract: Enabling farmers to access accurate agriculture-related information in their
native languages in a timely manner is crucial for the success of the
agriculture field. Although large language models (LLMs) can be used to
implement Question Answering (QA) systems, simply using publicly available
general-purpose LLMs in agriculture typically offer generic advisories, lacking
precision in local and multilingual contexts due to insufficient
domain-specific training and scarcity of high-quality, region-specific
datasets. Our study addresses these limitations by generating multilingual
synthetic agricultural datasets (English, Hindi, Punjabi) from
agriculture-specific documents and fine-tuning language-specific LLMs. Our
evaluation on curated multilingual datasets demonstrates significant
improvements in factual accuracy, relevance, and agricultural consensus for the
fine-tuned models compared to their baseline counterparts. These results
highlight the efficacy of synthetic data-driven, language-specific fine-tuning
as an effective strategy to improve the performance of LLMs in agriculture,
especially in multilingual and low-resource settings. By enabling more accurate
and localized agricultural advisory services, this study provides a meaningful
step toward bridging the knowledge gap in AI-driven agricultural solutions for
diverse linguistic communities.

</details>


### [6] [Obscured but Not Erased: Evaluating Nationality Bias in LLMs via Name-Based Bias Benchmarks](https://arxiv.org/abs/2507.16989)
*Giulio Pelosio,Devesh Batra,Noémie Bovey,Robert Hankache,Cristovao Iglesias,Greig Cowan,Raad Khraishi*

Main category: cs.CL

TL;DR: 即使没有明确的国籍信息，大型语言模型仍然存在偏见，小型模型的偏见问题更严重。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 即使在没有明确的人口统计标记的情况下，也可能表现出对特定国籍的潜在偏见。该研究旨在调查用文化指示性名称替换明确的国籍标签的影响，因为这种情况更贴近真实的 LLM 应用。

Method: 使用基于名称的基准测试方法，该方法源自 QA 的偏差基准测试 (BBQ) 数据集，通过用文化相关的名称替换明确的国籍标签来研究其影响。

Result: 小型模型不如大型模型准确，并且表现出更大的偏差。例如，在基于名称的数据集和模糊的上下文中（未揭示正确的选择），Claude Haiku 表现出最差的刻板印象偏差分数为 9%，而其较大的对应模型 Claude Sonnet 仅为 3.5%，后者在准确性方面也优于前者 117.7%。此外，我们发现小型模型在这些模糊的上下文中保留了大部分现有错误。例如，在用名称替换明确的国籍引用后，GPT-4o 保留了 68% 的错误率，而 GPT-4o-mini 则为 76%，其他模型提供商也有类似的发现。

Conclusion: 小型模型在准确性方面表现较差，并且比大型模型表现出更大的偏差。即使在用文化相关的名字替换明确的国籍标签后，大型语言模型中仍然存在偏差，这对在全球背景下开发和部署人工智能系统具有深远的影响。

Abstract: Large Language Models (LLMs) can exhibit latent biases towards specific
nationalities even when explicit demographic markers are not present. In this
work, we introduce a novel name-based benchmarking approach derived from the
Bias Benchmark for QA (BBQ) dataset to investigate the impact of substituting
explicit nationality labels with culturally indicative names, a scenario more
reflective of real-world LLM applications. Our novel approach examines how this
substitution affects both bias magnitude and accuracy across a spectrum of LLMs
from industry leaders such as OpenAI, Google, and Anthropic. Our experiments
show that small models are less accurate and exhibit more bias compared to
their larger counterparts. For instance, on our name-based dataset and in the
ambiguous context (where the correct choice is not revealed), Claude Haiku
exhibited the worst stereotypical bias scores of 9%, compared to only 3.5% for
its larger counterpart, Claude Sonnet, where the latter also outperformed it by
117.7% in accuracy. Additionally, we find that small models retain a larger
portion of existing errors in these ambiguous contexts. For example, after
substituting names for explicit nationality references, GPT-4o retains 68% of
the error rate versus 76% for GPT-4o-mini, with similar findings for other
model providers, in the ambiguous context. Our research highlights the stubborn
resilience of biases in LLMs, underscoring their profound implications for the
development and deployment of AI systems in diverse, global contexts.

</details>


### [7] [Multi-Label Classification with Generative AI Models in Healthcare: A Case Study of Suicidality and Risk Factors](https://arxiv.org/abs/2507.17009)
*Ming Huang,Zehan Li,Yan Hu,Wanjing Wang,Andrew Wen,Scott Lane,Salih Selek,Lokesh Shahani,Rodrigo Machado-Vieira,Jair Soares,Hua Xu,Hongfang Liu*

Main category: cs.CL

TL;DR: This paper uses generative LLMs (GPT-3.5 and GPT-4.5) for multi-label classification of suicidality-related factors in electronic health records, achieving high accuracy and revealing error patterns.


<details>
  <summary>Details</summary>
Motivation: Early identification of suicidality-related factors (SrFs), including SI, SA, exposure to suicide (ES), and non-suicidal self-injury (NSSI), is critical for timely intervention. Prior studies have applied AI to detect SrFs in clinical notes, but most treat suicidality as a binary classification task, overlooking the complexity of cooccurring risk factors.

Method: The study explores the use of generative large language models (LLMs), specifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs from psychiatric electronic health records (EHRs). A novel end to end generative MLC pipeline is presented, along with advanced evaluation methods, including label set level metrics and a multilabel confusion matrix for error analysis.

Result: Finetuned GPT-3.5 achieved top performance with 0.94 partial match accuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior performance across label sets, including rare or minority label sets, indicating a more balanced and robust performance. Systematic error patterns were revealed, such as the conflation of SI and SA, and highlight the models tendency toward cautious over labeling.

Conclusion: This study demonstrates the feasibility of using generative AI for complex clinical classification tasks and provides a blueprint for structuring unstructured EHR data to support large scale clinical research and evidence based medicine.

Abstract: Suicide remains a pressing global health crisis, with over 720,000 deaths
annually and millions more affected by suicide ideation (SI) and suicide
attempts (SA). Early identification of suicidality-related factors (SrFs),
including SI, SA, exposure to suicide (ES), and non-suicidal self-injury
(NSSI), is critical for timely intervention. While prior studies have applied
AI to detect SrFs in clinical notes, most treat suicidality as a binary
classification task, overlooking the complexity of cooccurring risk factors.
This study explores the use of generative large language models (LLMs),
specifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs
from psychiatric electronic health records (EHRs). We present a novel end to
end generative MLC pipeline and introduce advanced evaluation methods,
including label set level metrics and a multilabel confusion matrix for error
analysis. Finetuned GPT-3.5 achieved top performance with 0.94 partial match
accuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior
performance across label sets, including rare or minority label sets,
indicating a more balanced and robust performance. Our findings reveal
systematic error patterns, such as the conflation of SI and SA, and highlight
the models tendency toward cautious over labeling. This work not only
demonstrates the feasibility of using generative AI for complex clinical
classification tasks but also provides a blueprint for structuring unstructured
EHR data to support large scale clinical research and evidence based medicine.

</details>


### [8] [Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?](https://arxiv.org/abs/2507.17015)
*Arduin Findeis,Floris Weers,Guoli Yin,Ke Ye,Ruoming Pang,Tom Gunter*

Main category: cs.CL

TL;DR: 使用工具来增强标准 AI 注释器系统，以提高在长篇事实、数学和代码任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 在某些领域，高质量的成对比较可能难以获得——来自 AI 和人类。例如，对于包含许多事实陈述的回复，注释者可能会不成比例地权衡写作质量而不是潜在的事实。

Method: 使用网络搜索和代码执行，基于外部验证来支持自身，独立于 LLM 的内部知识和偏差。

Result: 外部工具确实可以在许多情况下提高性能，但并非所有情况。我们的实验强调了性能对简单参数（例如，提示）的敏感性，以及需要改进的（非饱和）注释器基准。

Conclusion: 外部工具可以在许多情况下提高性能，但并非所有情况。性能对简单参数（例如，提示）的敏感性，以及需要改进的（非饱和）注释器基准。

Abstract: Pairwise preferences over model responses are widely collected to evaluate
and provide feedback to large language models (LLMs). Given two alternative
model responses to the same input, a human or AI annotator selects the "better"
response. This approach can provide feedback for domains where other hard-coded
metrics are difficult to obtain (e.g., chat response quality), thereby helping
model evaluation or training. However, for some domains high-quality pairwise
comparisons can be tricky to obtain - from AI and humans. For example, for
responses with many factual statements, annotators may disproportionately weigh
writing quality rather than underlying facts. In this work, we explore
augmenting standard AI annotator systems with additional tools to improve
performance on three challenging response domains: long-form factual, math and
code tasks. We propose a tool-using agentic system to provide higher quality
feedback on these domains. Our system uses web-search and code execution to
ground itself based on external validation, independent of the LLM's internal
knowledge and biases. We provide extensive experimental results evaluating our
method across the three targeted response domains as well as general annotation
tasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as
three new datasets for domains with saturated pre-existing datasets. Our
results indicate that external tools can indeed improve performance in many,
but not all, cases. More generally, our experiments highlight the sensitivity
of performance to simple parameters (e.g., prompt) and the need for improved
(non-saturated) annotator benchmarks. We share our code at
https://github.com/apple/ml-agent-evaluator.

</details>


### [9] [Evolutionary Feature-wise Thresholding for Binary Representation of NLP Embeddings](https://arxiv.org/abs/2507.17025)
*Soumen Sinha,Shahryar Rahnamayan,Azam Asilian Bidgoli*

Main category: cs.CL

TL;DR: This paper introduces a coordinate search-based optimization framework that identifies the optimal threshold for each feature, enhancing performance across various features in binary representations for NLP embeddings.


<details>
  <summary>Details</summary>
Motivation: Efficient text embedding is crucial for large-scale NLP applications, where storage and computational efficiency are key concerns. Using binary representations (barcodes) instead of real-valued features can be used for NLP embeddings.

Method: A coordinate search-based optimization framework is used to identify the optimal threshold for each feature in NLP embeddings.

Result: Optimal barcode representations show promising results in various NLP applications, outperforming traditional binarization methods in accuracy. The technique is versatile and can be applied to any features, not just limited to NLP embeddings.

Conclusion: The proposed coordinate search-based optimization framework identifies optimal feature-specific thresholds for binary encoding, leading to improved performance in various NLP applications.

Abstract: Efficient text embedding is crucial for large-scale natural language
processing (NLP) applications, where storage and computational efficiency are
key concerns. In this paper, we explore how using binary representations
(barcodes) instead of real-valued features can be used for NLP embeddings
derived from machine learning models such as BERT. Thresholding is a common
method for converting continuous embeddings into binary representations, often
using a fixed threshold across all features. We propose a Coordinate
Search-based optimization framework that instead identifies the optimal
threshold for each feature, demonstrating that feature-specific thresholds lead
to improved performance in binary encoding. This ensures that the binary
representations are both accurate and efficient, enhancing performance across
various features. Our optimal barcode representations have shown promising
results in various NLP applications, demonstrating their potential to transform
text representation. We conducted extensive experiments and statistical tests
on different NLP tasks and datasets to evaluate our approach and compare it to
other thresholding methods. Binary embeddings generated using using optimal
thresholds found by our method outperform traditional binarization methods in
accuracy. This technique for generating binary representations is versatile and
can be applied to any features, not just limited to NLP embeddings, making it
useful for a wide range of domains in machine learning applications.

</details>


### [10] [CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards](https://arxiv.org/abs/2507.17147)
*Cheng Liu,Yifei Lu,Fanghua Ye,Jian Li,Xingyu Chen,Feiliang Ren,Zhaopeng Tu,Xiaolong Li*

Main category: cs.CL

TL;DR: CogDual 是一种新颖的角色扮演语言代理，它采用“认知-然后-响应”的推理范式，通过共同建模外部情境意识和内部自我意识来生成响应，从而提高角色一致性和上下文对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖于提示工程或监督微调来使模型模仿特定场景中的角色行为，但通常忽略驱动这些行为的潜在认知机制。

Method: 采用“认知-然后-响应”的推理范式，共同建模外部情境意识和内部自我意识。

Result: CogDual 生成的响应具有改进的角色一致性和上下文对齐。

Conclusion: CogDual在多个角色扮演任务中始终优于现有基线，并且能够有效地泛化。

Abstract: Role-Playing Language Agents (RPLAs) have emerged as a significant
application direction for Large Language Models (LLMs). Existing approaches
typically rely on prompt engineering or supervised fine-tuning to enable models
to imitate character behaviors in specific scenarios, but often neglect the
underlying \emph{cognitive} mechanisms driving these behaviors. Inspired by
cognitive psychology, we introduce \textbf{CogDual}, a novel RPLA adopting a
\textit{cognize-then-respond } reasoning paradigm. By jointly modeling external
situational awareness and internal self-awareness, CogDual generates responses
with improved character consistency and contextual alignment. To further
optimize the performance, we employ reinforcement learning with two
general-purpose reward schemes designed for open-domain text generation.
Extensive experiments on the CoSER benchmark, as well as Cross-MR and
LifeChoice, demonstrate that CogDual consistently outperforms existing
baselines and generalizes effectively across diverse role-playing tasks.

</details>


### [11] [SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs](https://arxiv.org/abs/2507.17178)
*Zhiqiang Liu,Enpei Niu,Yin Hua,Mengshu Sun,Lei Liang,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: SKA-Bench: a new benchmark for evaluating structured knowledge understanding in LLMs. It reveals challenges in noise robustness, order insensitivity, information integration, and negative rejection.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations for SK understanding are non-rigorous (i.e., lacking evaluations of specific capabilities) and focus on a single type of SK. Therefore, we aim to propose a more comprehensive and rigorous structured knowledge understanding benchmark to diagnose the shortcomings of LLMs.

Method: We introduce SKA-Bench, a Structured Knowledge Augmented QA Benchmark that encompasses four widely used structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a three-stage pipeline to construct SKA-Bench instances, which includes a question, an answer, positive knowledge units, and noisy knowledge units. To evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we expand the instances into four fundamental ability testbeds: Noise Robustness, Order Insensitivity, Information Integration, and Negative Rejection.

Result: Empirical evaluations on 8 representative LLMs, including the advanced DeepSeek-R1, indicate that existing LLMs still face significant challenges in understanding structured knowledge

Conclusion: Existing LLMs still face significant challenges in understanding structured knowledge, and their performance is influenced by factors such as the amount of noise, the order of knowledge units, and hallucination phenomenon.

Abstract: Although large language models (LLMs) have made significant progress in
understanding Structured Knowledge (SK) like KG and Table, existing evaluations
for SK understanding are non-rigorous (i.e., lacking evaluations of specific
capabilities) and focus on a single type of SK. Therefore, we aim to propose a
more comprehensive and rigorous structured knowledge understanding benchmark to
diagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a
Structured Knowledge Augmented QA Benchmark that encompasses four widely used
structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a
three-stage pipeline to construct SKA-Bench instances, which includes a
question, an answer, positive knowledge units, and noisy knowledge units. To
evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we
expand the instances into four fundamental ability testbeds: Noise Robustness,
Order Insensitivity, Information Integration, and Negative Rejection. Empirical
evaluations on 8 representative LLMs, including the advanced DeepSeek-R1,
indicate that existing LLMs still face significant challenges in understanding
structured knowledge, and their performance is influenced by factors such as
the amount of noise, the order of knowledge units, and hallucination
phenomenon. Our dataset and code are available at
https://github.com/Lza12a/SKA-Bench.

</details>


### [12] [FinGAIA: An End-to-End Benchmark for Evaluating AI Agents in Finance](https://arxiv.org/abs/2507.17186)
*Lingfeng Zeng,Fangqi Lou,Zixuan Wang,Jiajie Xu,Jinyi Niu,Mengping Li,Yifan Dong,Qi Qi,Wei Zhang,Ziwei Yang,Jun Han,Ruilun Feng,Ruiqi Hu,Lejie Zhang,Zhengbo Feng,Yicheng Ren,Xin Guo,Zhaowei Liu,Dongpo Cheng,Weige Cai,Liwen Zhang*

Main category: cs.CL

TL;DR: FinGAIA is introduced to evaluate AI agents in finance. Current agents lag experts, and key failure patterns are identified.


<details>
  <summary>Details</summary>
Motivation: The multi-step, multi-tool collaboration capabilities of AI agents in the financial sector are underexplored.

Method: The paper introduces FinGAIA, a benchmark comprising 407 tasks across seven financial sub-domains, organized into three levels of scenario depth. Ten mainstream AI agents were evaluated in a zero-shot setting.

Result: The best-performing agent, ChatGPT, achieved 48.9% accuracy, lagging financial experts. Error analysis revealed five recurring failure patterns.

Conclusion: This paper introduces FinGAIA, a benchmark for evaluating AI agents in the financial domain, and identifies areas for future research based on error analysis of current agents.

Abstract: The booming development of AI agents presents unprecedented opportunities for
automating complex tasks across various domains. However, their multi-step,
multi-tool collaboration capabilities in the financial sector remain
underexplored. This paper introduces FinGAIA, an end-to-end benchmark designed
to evaluate the practical abilities of AI agents in the financial domain.
FinGAIA comprises 407 meticulously crafted tasks, spanning seven major
financial sub-domains: securities, funds, banking, insurance, futures, trusts,
and asset management. These tasks are organized into three hierarchical levels
of scenario depth: basic business analysis, asset decision support, and
strategic risk management. We evaluated 10 mainstream AI agents in a zero-shot
setting. The best-performing agent, ChatGPT, achieved an overall accuracy of
48.9\%, which, while superior to non-professionals, still lags financial
experts by over 35 percentage points. Error analysis has revealed five
recurring failure patterns: Cross-modal Alignment Deficiency, Financial
Terminological Bias, Operational Process Awareness Barrier, among others. These
patterns point to crucial directions for future research. Our work provides the
first agent benchmark closely related to the financial domain, aiming to
objectively assess and promote the development of agents in this crucial field.
Partial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA.

</details>


### [13] [The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models](https://arxiv.org/abs/2507.17216)
*Giuseppe Russo,Debora Nozza,Paul Röttger,Dirk Hovy*

Main category: cs.CL

TL;DR: LLMs don't align well with human moral judgments, but a new method called Dynamic Moral Profiling (DMP) helps close this gap.


<details>
  <summary>Details</summary>
Motivation: Understanding how closely LLMs align with human moral judgments is crucial as people increasingly rely on them for moral advice, which may influence human decisions.

Method: Introducing the Moral Dilemma Dataset, a benchmark of 1,618 real-world moral dilemmas, and comparing the distributions of LLM and human judgments using a 60-value taxonomy. Also introducing Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method.

Result: LLMs reproduce human judgments only under high consensus, with alignment deteriorating sharply when human disagreement increases. DMP improves alignment by 64.3% and enhances value diversity.

Conclusion: LLMs exhibit a pluralistic moral gap compared to human moral judgments, but the proposed Dynamic Moral Profiling (DMP) method can improve alignment and value diversity.

Abstract: People increasingly rely on Large Language Models (LLMs) for moral advice,
which may influence humans' decisions. Yet, little is known about how closely
LLMs align with human moral judgments. To address this, we introduce the Moral
Dilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a
distribution of human moral judgments consisting of a binary evaluation and a
free-text rationale. We treat this problem as a pluralistic distributional
alignment task, comparing the distributions of LLM and human judgments across
dilemmas. We find that models reproduce human judgments only under high
consensus; alignment deteriorates sharply when human disagreement increases. In
parallel, using a 60-value taxonomy built from 3,783 value expressions
extracted from rationales, we show that LLMs rely on a narrower set of moral
values than humans. These findings reveal a pluralistic moral gap: a mismatch
in both the distribution and diversity of values expressed. To close this gap,
we introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method
that conditions model outputs on human-derived value profiles. DMP improves
alignment by 64.3% and enhances value diversity, offering a step toward more
pluralistic and human-aligned moral guidance from LLMs.

</details>


### [14] [CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings](https://arxiv.org/abs/2507.17234)
*Kyeongkyu Lee,Seonghwan Yoon,Hongki Lim*

Main category: cs.CL

TL;DR: CLARIFID通过模仿放射科医生的工作流程并优化诊断的正确性来自动生成放射报告，从而优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动生成放射学报告有潜力减轻放射科医生的大量工作负担，但目前的方法难以提供临床上可靠的结论。特别是，大多数先前的方法侧重于生成流畅的文本，而没有有效确保报告的事实正确性，并且通常依赖于单视图图像，限制了诊断的全面性。

Method: CLARIFID，一个新颖的框架，通过镜像专家的两步工作流程，直接优化诊断正确性。具体来说，CLARIFID (1) 通过section-aware预训练学习从“发现”到“印象”的逻辑流程，(2) 使用近端策略优化进行微调，其中“印象”部分的CheXbert F1评分作为奖励，(3) 实施推理感知解码，在合成“印象”之前完成“发现”，以及 (4) 通过基于视觉转换器的多视图编码器融合多个胸部X光片视图。

Result: 该方法实现了卓越的临床疗效，并在标准NLG指标和临床感知评分方面优于现有的基线。

Conclusion: 该方法在MIMIC-CXR数据集上实现了卓越的临床疗效，并在标准NLG指标和临床感知评分方面优于现有的基线。

Abstract: Automatic generation of radiology reports has the potential to alleviate
radiologists' significant workload, yet current methods struggle to deliver
clinically reliable conclusions. In particular, most prior approaches focus on
producing fluent text without effectively ensuring the factual correctness of
the reports and often rely on single-view images, limiting diagnostic
comprehensiveness. We propose CLARIFID, a novel framework that directly
optimizes diagnostic correctness by mirroring the two-step workflow of experts.
Specifically, CLARIFID (1) learns the logical flow from Findings to Impression
through section-aware pretraining, (2) is fine-tuned with Proximal Policy
Optimization in which the CheXbert F1 score of the Impression section serves as
the reward, (3) enforces reasoning-aware decoding that completes "Findings"
before synthesizing the "Impression", and (4) fuses multiple chest X-ray views
via a vision-transformer-based multi-view encoder. During inference, we apply a
reasoning-aware next-token forcing strategy followed by report-level
re-ranking, ensuring that the model first produces a comprehensive Findings
section before synthesizing the Impression and thereby preserving coherent
clinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate
that our method achieves superior clinical efficacy and outperforms existing
baselines on both standard NLG metrics and clinically aware scores.

</details>


### [15] [Millions of $\text{GeAR}$-s: Extending GraphRAG to Millions of Documents](https://arxiv.org/abs/2507.17399)
*Zhili Shen,Chenxin Diao,Pascual Merita,Pavlos Vougiouklis,Jeff Z. Pan*

Main category: cs.CL

TL;DR: 本文调整了GeAR并在SIGIR 2025 LiveRAG Challenge上进行了测试。


<details>
  <summary>Details</summary>
Motivation: 最近的研究已经探索了基于图的方法来进行检索增强生成，利用结构化或半结构化的信息（例如从文档中提取的实体及其关系）来增强检索。然而，这些方法通常被设计用来解决特定的任务，例如多跳问答和以查询为中心的摘要，因此，它们在更广泛的数据集上的普遍适用性的证据有限。

Method: 调整最先进的基于图的RAG解决方案: GeAR

Result: 探索GeAR在SIGIR 2025 LiveRAG Challenge上的性能和局限性。

Conclusion: 这篇论文旨在调整最先进的基于图的RAG解决方案GeAR，并探索其在SIGIR 2025 LiveRAG Challenge上的性能和局限性。

Abstract: Recent studies have explored graph-based approaches to retrieval-augmented
generation, leveraging structured or semi-structured information -- such as
entities and their relations extracted from documents -- to enhance retrieval.
However, these methods are typically designed to address specific tasks, such
as multi-hop question answering and query-focused summarisation, and therefore,
there is limited evidence of their general applicability across broader
datasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG
solution: $\text{GeAR}$ and explore its performance and limitations on the
SIGIR 2025 LiveRAG Challenge.

</details>


### [16] [Triple X: A LLM-Based Multilingual Speech Recognition System for the INTERSPEECH2025 MLC-SLM Challenge](https://arxiv.org/abs/2507.17288)
*Miaomiao Gao,Xiaoxiao Xiang,Yiwen Guo*

Main category: cs.CL

TL;DR: Triple X speech recognition system uses encoder-adapter-LLM architecture and multi-stage training to achieve competitive WER performance in multilingual conversational scenarios, ranking second in the MLC-SLM Challenge.


<details>
  <summary>Details</summary>
Motivation: optimizing speech recognition accuracy in multilingual conversational scenarios

Method: an innovative encoder-adapter-LLM architecture and a meticulously designed multi-stage training strategy leveraging extensive multilingual audio datasets

Result: achieves competitive Word Error Rate (WER) performance on both dev and test sets

Conclusion: The system achieves competitive WER performance on both dev and test sets, obtaining second place in the challenge ranking.

Abstract: This paper describes our Triple X speech recognition system submitted to Task
1 of the Multi-Lingual Conversational Speech Language Modeling (MLC-SLM)
Challenge. Our work focuses on optimizing speech recognition accuracy in
multilingual conversational scenarios through an innovative encoder-adapter-LLM
architecture. This framework harnesses the powerful reasoning capabilities of
text-based large language models while incorporating domain-specific
adaptations. To further enhance multilingual recognition performance, we
adopted a meticulously designed multi-stage training strategy leveraging
extensive multilingual audio datasets. Experimental results demonstrate that
our approach achieves competitive Word Error Rate (WER) performance on both dev
and test sets, obtaining second place in the challenge ranking.

</details>


### [17] [Investigating Subjective Factors of Argument Strength: Storytelling, Emotions, and Hedging](https://arxiv.org/abs/2507.17409)
*Carlotta Quensel,Neele Falk,Gabriella Lapesa*

Main category: cs.CL

TL;DR: 研究了情感、故事叙述和对冲等主观因素对论证强度的影响。


<details>
  <summary>Details</summary>
Motivation: 缺乏对主观特征与论证强度之间关系的大规模分析。

Method: 回归分析

Result: 故事叙述和对冲对客观和主观的论证质量有相反的影响，而情感的影响取决于其修辞运用而非领域。

Conclusion: 故事叙述和对冲对客观和主观的论证质量有相反的影响，而情感的影响取决于其修辞运用而非领域。

Abstract: In assessing argument strength, the notions of what makes a good argument are
manifold. With the broader trend towards treating subjectivity as an asset and
not a problem in NLP, new dimensions of argument quality are studied. Although
studies on individual subjective features like personal stories exist, there is
a lack of large-scale analyses of the relation between these features and
argument strength. To address this gap, we conduct regression analysis to
quantify the impact of subjective factors $-$ emotions, storytelling, and
hedging $-$ on two standard datasets annotated for objective argument quality
and subjective persuasion. As such, our contribution is twofold: at the level
of contributed resources, as there are no datasets annotated with all studied
dimensions, this work compares and evaluates automated annotation methods for
each subjective feature. At the level of novel insights, our regression
analysis uncovers different patterns of impact of subjective features on the
two facets of argument strength encoded in the datasets. Our results show that
storytelling and hedging have contrasting effects on objective and subjective
argument quality, while the influence of emotions depends on their rhetoric
utilization rather than the domain.

</details>


### [18] [Each to Their Own: Exploring the Optimal Embedding in RAG](https://arxiv.org/abs/2507.17442)
*Shiting Chen,Zijian Zhao,Jinsong Chen*

Main category: cs.CL

TL;DR: This paper introduces Confident RAG, a method that leverages multiple embedding models to enhance RAG performance by selecting the most confident responses, achieving improvements over vanilla LLMs and RAG.


<details>
  <summary>Details</summary>
Motivation: Methods for incorporating up-to-date information into LLMs or adding external knowledge to construct domain-specific models have garnered wide attention. Retrieval-Augmented Generation (RAG) is notable for its low cost and minimal effort for parameter tuning. However, the variant embedding models used in RAG exhibit different benefits across various areas, often leading to different similarity calculation results and, consequently, varying response quality from LLMs.

Method: We propose and examine two approaches to enhance RAG by combining the benefits of multiple embedding models, named Mixture-Embedding RAG and Confident RAG.

Result: Mixture-Embedding RAG does not outperform vanilla RAG. Confident RAG generates responses multiple times using different embedding models and then selects the responses with the highest confidence level, demonstrating average improvements of approximately 10% and 5% over vanilla LLMs and RAG, respectively. The consistent results across different LLMs and embedding models indicate that Confident RAG is an efficient plug-and-play approach for various domains.

Conclusion: Confident RAG is an efficient plug-and-play approach for various domains, demonstrating average improvements of approximately 10% and 5% over vanilla LLMs and RAG, respectively.

Abstract: Recently, as Large Language Models (LLMs) have fundamentally impacted various
fields, the methods for incorporating up-to-date information into LLMs or
adding external knowledge to construct domain-specific models have garnered
wide attention. Retrieval-Augmented Generation (RAG), serving as an
inference-time scaling method, is notable for its low cost and minimal effort
for parameter tuning. However, due to heterogeneous training data and model
architecture, the variant embedding models used in RAG exhibit different
benefits across various areas, often leading to different similarity
calculation results and, consequently, varying response quality from LLMs. To
address this problem, we propose and examine two approaches to enhance RAG by
combining the benefits of multiple embedding models, named Mixture-Embedding
RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects
retrievals from multiple embedding models based on standardized similarity;
however, it does not outperform vanilla RAG. In contrast, Confident RAG
generates responses multiple times using different embedding models and then
selects the responses with the highest confidence level, demonstrating average
improvements of approximately 10% and 5% over vanilla LLMs and RAG,
respectively. The consistent results across different LLMs and embedding models
indicate that Confident RAG is an efficient plug-and-play approach for various
domains. We will release our code upon publication.

</details>


### [19] [MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs](https://arxiv.org/abs/2507.17476)
*Alexander R. Fabbri,Diego Mares,Jorge Flores,Meher Mankikar,Ernesto Hernandez,Dean Lee,Bing Liu,Chen Xing*

Main category: cs.CL

TL;DR: This paper introduces MultiNRC, a new benchmark to evaluate LLMs on native multilingual reasoning, and finds that current LLMs are not good at it.


<details>
  <summary>Details</summary>
Motivation: evaluation of such LLMs' multilingual reasoning capability across diverse languages and cultural contexts remains limited. Existing multilingual reasoning benchmarks are typically constructed by translating existing English reasoning benchmarks, biasing these benchmarks towards reasoning problems with context in English language/cultures

Method: introduce the Multilingual Native Reasoning Challenge (MultiNRC), a benchmark designed to assess LLMs on more than 1,000 native, linguistic and culturally grounded reasoning questions written by native speakers in French, Spanish, and Chinese

Result: current LLMs are still not good at native multilingual reasoning, with none scoring above 50% on MultiNRC

Conclusion: current LLMs are still not good at native multilingual reasoning, with none scoring above 50% on MultiNRC

Abstract: Although recent Large Language Models (LLMs) have shown rapid improvement on
reasoning benchmarks in English, the evaluation of such LLMs' multilingual
reasoning capability across diverse languages and cultural contexts remains
limited. Existing multilingual reasoning benchmarks are typically constructed
by translating existing English reasoning benchmarks, biasing these benchmarks
towards reasoning problems with context in English language/cultures. In this
work, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a
benchmark designed to assess LLMs on more than 1,000 native, linguistic and
culturally grounded reasoning questions written by native speakers in French,
Spanish, and Chinese. MultiNRC covers four core reasoning categories:
language-specific linguistic reasoning, wordplay & riddles, cultural/tradition
reasoning, and math reasoning with cultural relevance. For cultural/tradition
reasoning and math reasoning with cultural relevance, we also provide English
equivalent translations of the multilingual questions by manual translation
from native speakers fluent in English. This set of English equivalents can
provide a direct comparison of LLM reasoning capacity in other languages vs.
English on the same reasoning questions. We systematically evaluate current 14
leading LLMs covering most LLM families on MultiNRC and its English equivalent
set. The results show that (1) current LLMs are still not good at native
multilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs
exhibit distinct strengths and weaknesses in handling linguistic, cultural, and
logical reasoning tasks; (3) Most models perform substantially better in math
reasoning in English compared to in original languages (+10%), indicating
persistent challenges with culturally grounded knowledge.

</details>


### [20] [Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice](https://arxiv.org/abs/2507.17527)
*Shanbo Cheng,Yu Bao,Zhichao Huang,Yu Lu,Ningxin Peng,Lu Xu,Runsheng Yu,Rong Cao,Ting Han,Zeyang Li,Sitong Liu,Shengtao Ma,Shiguang Pan,Jiongchen Xiao,Nuo Xu,Meng Yang,Rong Ye,Yiming Yu,Ruofei Zhang,Wanyi Zhang,Wenhao Zhu,Liehao Zou,Lu Lu,Yuxuan Wang,Yonghui Wu*

Main category: cs.CL

TL;DR: Seed-LiveInterpret 2.0 是一种端到端口译模型，可提供高保真、超低延迟的语音生成和语音克隆功能。


<details>
  <summary>Details</summary>
Motivation: 口译是翻译行业最艰巨的领域之一，产品级的自动系统长期以来一直受到难以解决的挑战的困扰：转录和翻译质量差、缺乏实时语音生成、多说话人混淆和翻译后的语音膨胀，尤其是在长篇论述中。

Method:  duplex speech-to-speech understanding-generating framework

Result: Seed-LiveInterpret 2.0在翻译质量上明显优于商业SI解决方案，同时将克隆语音的平均延迟从近10秒降至接近实时的3秒，降幅接近70%，大大提高了实用性。

Conclusion: Seed-LiveInterpret 2.0在复杂场景下，经人工翻译验证，正确率超过70%。

Abstract: Simultaneous Interpretation (SI) represents one of the most daunting
frontiers in the translation industry, with product-level automatic systems
long plagued by intractable challenges: subpar transcription and translation
quality, lack of real-time speech generation, multi-speaker confusion, and
translated speech inflation, especially in long-form discourses. In this study,
we introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers
high-fidelity, ultra-low-latency speech-to-speech generation with voice cloning
capabilities. As a fully operational product-level solution, Seed-LiveInterpret
2.0 tackles these challenges head-on through our novel duplex speech-to-speech
understanding-generating framework. Experimental results demonstrate that
through large-scale pretraining and reinforcement learning, the model achieves
a significantly better balance between translation accuracy and latency,
validated by human interpreters to exceed 70% correctness in complex scenarios.
Notably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by
significant margins in translation quality, while slashing the average latency
of cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is
around a near 70% reduction that drastically enhances practical usability.

</details>


### [21] [Synthetic Voice Data for Automatic Speech Recognition in African Languages](https://arxiv.org/abs/2507.17578)
*Brian DeRenzi,Anna Dixon,Mohamed Aymane Farhi,Christian Resch*

Main category: cs.CL

TL;DR: 该研究探索了使用大型合成语音语料库来改进非洲语言的 ASR，结果好坏参半，并强调需要改进评估方法。


<details>
  <summary>Details</summary>
Motivation: 非洲 2300 多种语言中的大多数仍然无法使用语音技术。

Method: 该研究采用了一个三步过程：LLM 驱动的文本创建、TTS 语音合成和 ASR 微调。

Result: 为创建合成文本的十种语言中的八种实现了高于 5/7 的可读性分数。在 Hausa 语中，经过微调的 Wav2Vec-BERT-2.0 模型在 250 小时的真实数据和 250 小时的合成数据上训练，与仅使用 500 小时的真实数据基线相匹配，而 579 小时的真实数据和 450 小时到 993 小时的合成数据创造了最佳性能。Chichewa 语的 WER 提高了约 6.5%，真实数据与合成数据的比例为 1:2；Dholuo 语的 1:1 比例在某些评估数据上显示出类似的改进，但在其他数据上则没有。

Conclusion: 该研究表明，合成语音数据可以有效地用于非洲语言的 ASR 改进，但结果因语言和数据集而异。他们强调需要更强大的评估协议和更准确的评估数据，并公开发布了所有数据和模型，以促进进一步的研究。

Abstract: Speech technology remains out of reach for most of the over 2300 languages in
Africa. We present the first systematic assessment of large-scale synthetic
voice corpora for African ASR. We apply a three-step process: LLM-driven text
creation, TTS voice synthesis, and ASR fine-tuning. Eight out of ten languages
for which we create synthetic text achieved readability scores above 5 out of
7. We evaluated ASR improvement for three (Hausa, Dholuo, Chichewa) and created
more than 2,500 hours of synthetic voice data at below 1% of the cost of real
data. Fine-tuned Wav2Vec-BERT-2.0 models trained on 250h real and 250h
synthetic Hausa matched a 500h real-data-only baseline, while 579h real and
450h to 993h synthetic data created the best performance. We also present
gender-disaggregated ASR performance evaluation. For very low-resource
languages, gains varied: Chichewa WER improved about 6.5% relative with a 1:2
real-to-synthetic ratio; a 1:1 ratio for Dholuo showed similar improvements on
some evaluation data, but not on others. Investigating intercoder reliability,
ASR errors and evaluation datasets revealed the need for more robust reviewer
protocols and more accurate evaluation data. All data and models are publicly
released to invite further work to improve synthetic data for African
languages.

</details>


### [22] [A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)](https://arxiv.org/abs/2507.17618)
*Bowen Zheng,Ming Ma,Zhongqiao Lin,Tianming Yang*

Main category: cs.CL

TL;DR: This paper proposes SPADE, a novel decoding method, and a hybrid early-exit algorithm to reduce the inference costs of large language models without compromising accuracy.


<details>
  <summary>Details</summary>
Motivation: Large language models are computationally expensive, and early-exit algorithms often suffer from poor performance due to misalignment between intermediate and output layer representations.

Method: The paper proposes SPADE (SPace Alignment DEcoding), a novel decoding method that aligns intermediate layer representations with the output layer by propagating a minimally reduced sequence. It further optimizes the early-exit decision-making process by training a linear approximation of SPADE that computes entropy-based confidence metrics.

Result: The proposed approach significantly reduces inference costs without compromising accuracy.

Conclusion: This paper introduces a hybrid early-exit algorithm that reduces inference costs without compromising accuracy by monitoring confidence levels and stopping inference at intermediate layers while using SPADE to generate high-quality outputs.

Abstract: Large language models are computationally expensive due to their deep
structures. Prior research has shown that intermediate layers contain
sufficient information to generate accurate answers, leading to the development
of early-exit algorithms that reduce inference costs by terminating computation
at earlier layers. However, these methods often suffer from poor performance
due to misalignment between intermediate and output layer representations that
lead to decoding inaccuracy. To address these challenges, we propose SPADE
(SPace Alignment DEcoding), a novel decoding method that aligns intermediate
layer representations with the output layer by propagating a minimally reduced
sequence consisting of only the start token and the answer token. We further
optimize the early-exit decision-making process by training a linear
approximation of SPADE that computes entropy-based confidence metrics. Putting
them together, we create a hybrid early-exit algorithm that monitors confidence
levels and stops inference at intermediate layers while using SPADE to generate
high-quality outputs. This approach significantly reduces inference costs
without compromising accuracy, offering a scalable and efficient solution for
deploying large language models in real-world applications.

</details>


### [23] [Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach](https://arxiv.org/abs/2507.10330)
*Mohammed Bouri,Adnane Saoud*

Main category: cs.CL

TL;DR: This paper introduces a GBM-based regularization technique to improve the adversarial robustness of NLP models, particularly recurrent networks and SSMs, achieving up to 8.8% improvement over existing methods.


<details>
  <summary>Details</summary>
Motivation: NLP models are vulnerable to adversarial attacks, and the robustness of recurrent networks and modern state space models (SSMs) remains understudied.

Method: A novel regularization technique based on Growth Bound Matrices (GBM) is introduced to improve NLP model robustness.

Result: The method enhances resilience against word substitution attacks, improves generalization on clean text, and provides a systematic analysis of SSM (S4) robustness. Extensive experiments show improved adversarial robustness by up to 8.8% over baselines.

Conclusion: The proposed GBM-based regularization technique improves adversarial robustness by up to 8.8% over existing baselines, demonstrating its effectiveness in adversarial defense.

Abstract: Despite advancements in Natural Language Processing (NLP), models remain
vulnerable to adversarial attacks, such as synonym substitutions. While prior
work has focused on improving robustness for feed-forward and convolutional
architectures, the robustness of recurrent networks and modern state space
models (SSMs), such as S4, remains understudied. These architectures pose
unique challenges due to their sequential processing and complex parameter
dynamics. In this paper, we introduce a novel regularization technique based on
Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the
impact of input perturbations on model outputs. We focus on computing the GBM
for three architectures: Long Short-Term Memory (LSTM), State Space models
(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance
resilience against word substitution attacks, (2) improve generalization on
clean text, and (3) providing the first systematic analysis of SSM (S4)
robustness. Extensive experiments across multiple architectures and benchmark
datasets demonstrate that our method improves adversarial robustness by up to
8.8% over existing baselines. These results highlight the effectiveness of our
approach, outperforming several state-of-the-art methods in adversarial
defense. Codes are available at https://github.com/BouriMohammed/GBM

</details>


### [24] [WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training](https://arxiv.org/abs/2507.17634)
*Changxin Tian,Jiapeng Wang,Qian Zhao,Kunlong Chen,Jia Liu,Ziqi Liu,Jiaxin Mao,Wayne Xin Zhao,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 提出了 WSM 框架，该框架通过模型平均模拟各种衰减策略，并在多个基准测试中优于 WSD 方法。


<details>
  <summary>Details</summary>
Motivation: 学习率 (LR) 调度的最新进展表明，无衰减方法在消除传统衰减阶段的同时保持竞争性能的有效性。模型合并技术已成为该领域特别有前途的解决方案。

Method: 提出了 Warmup-Stable and Merge (WSM)，这是一个通用框架，它在学习率衰减和模型合并之间建立了正式的联系。

Result: 确定合并持续时间（检查点聚合的训练窗口）是影响模型性能的最关键因素，超过了检查点间隔和合并数量的重要性。

Conclusion: WSM在多个基准测试中始终优于广泛采用的 WSD 方法，在 MATH 上实现了 +3.5% 的显着改进，在 HumanEval 上实现了 +2.9% 的改进，在 MMLU-Pro 上实现了 +5.5% 的改进。性能优势扩展到监督微调场景，突显了 WSM 在长期模型改进方面的潜力。

Abstract: Recent advances in learning rate (LR) scheduling have demonstrated the
effectiveness of decay-free approaches that eliminate the traditional decay
phase while maintaining competitive performance. Model merging techniques have
emerged as particularly promising solutions in this domain. We present
Warmup-Stable and Merge (WSM), a general framework that establishes a formal
connection between learning rate decay and model merging. WSM provides a
unified theoretical foundation for emulating various decay strategies-including
cosine decay, linear decay and inverse square root decay-as principled model
averaging schemes, while remaining fully compatible with diverse optimization
methods. Through extensive experiments, we identify merge duration-the training
window for checkpoint aggregation-as the most critical factor influencing model
performance, surpassing the importance of both checkpoint interval and merge
quantity. Our framework consistently outperforms the widely-adopted
Warmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving
significant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on
MMLU-Pro. The performance advantages extend to supervised fine-tuning
scenarios, highlighting WSM's potential for long-term model refinement.

</details>


### [25] [Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries](https://arxiv.org/abs/2507.17636)
*Victor Hartman,Petter Törnberg*

Main category: cs.CL

TL;DR: 本研究利用大型语言模型分析了欧洲议员的推文，发现执政党较少使用负面信息，而极端和民粹主义政党更多地参与负面宣传。


<details>
  <summary>Details</summary>
Motivation: 实证研究受到现有分类方法的高成本和有限可扩展性的限制。

Method: 引入零样本大型语言模型（LLM）作为跨语言负面竞选分类的新方法，并利用此方法分析了19个欧洲国家议员在2017年至2022年间发布的1800万条推文。

Result: 大型语言模型（LLM）在十种语言的基准数据集上取得了与以母语为母语的人类编码员相当的性能，并且优于传统的监督机器学习方法。

Conclusion: 研究结果揭示了一致的跨国模式：执政党不太可能使用负面信息，而意识形态极端和民粹主义政党——尤其是极右翼政党——则会进行明显更高程度的负面宣传。这些发现加深了我们对政党层面特征如何在多党制系统中塑造战略沟通的理解。

Abstract: Negative campaigning is a central feature of political competition, yet
empirical research has been limited by the high cost and limited scalability of
existing classification methods. This study makes two key contributions. First,
it introduces zero-shot Large Language Models (LLMs) as a novel approach for
cross-lingual classification of negative campaigning. Using benchmark datasets
in ten languages, we demonstrate that LLMs achieve performance on par with
native-speaking human coders and outperform conventional supervised machine
learning approaches. Second, we leverage this novel method to conduct the
largest cross-national study of negative campaigning to date, analyzing 18
million tweets posted by parliamentarians in 19 European countries between 2017
and 2022. The results reveal consistent cross-national patterns: governing
parties are less likely to use negative messaging, while ideologically extreme
and populist parties -- particularly those on the radical right -- engage in
significantly higher levels of negativity. These findings advance our
understanding of how party-level characteristics shape strategic communication
in multiparty systems. More broadly, the study demonstrates the potential of
LLMs to enable scalable, transparent, and replicable research in political
communication across linguistic and cultural contexts.

</details>


### [26] [Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models](https://arxiv.org/abs/2507.17702)
*Changxin Tian,Kunlong Chen,Jia Liu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: This paper introduces Efficiency Leverage (EL) to quantify the computational advantage of MoE models and derives scaling laws for efficient MoE model scaling, validated by the Ling-mini-beta model.


<details>
  <summary>Details</summary>
Motivation: predicting the model capacity of a given MoE configurations remains an unresolved problem

Method: introducing Efficiency Leverage (EL), a metric quantifying the computational advantage of an MoE model over a dense equivalent and conducting a large-scale empirical study, training over 300 models up to 28B parameters

Result: EL is primarily driven by the expert activation ratio and the total compute budget, both following predictable power laws, while expert granularity acts as a non-linear modulator with a clear optimal range. Ling-mini-beta matched the performance of the 6.1B dense model while consuming over 7x fewer computational resources, thereby confirming the accuracy of our scaling laws.

Conclusion: This work provides a principled and empirically-grounded foundation for the scaling of efficient MoE models.

Abstract: Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large
Language Models (LLMs) efficiently by decoupling total parameters from
computational cost. However, this decoupling creates a critical challenge:
predicting the model capacity of a given MoE configurations (e.g., expert
activation ratio and granularity) remains an unresolved problem. To address
this gap, we introduce Efficiency Leverage (EL), a metric quantifying the
computational advantage of an MoE model over a dense equivalent. We conduct a
large-scale empirical study, training over 300 models up to 28B parameters, to
systematically investigate the relationship between MoE architectural
configurations and EL. Our findings reveal that EL is primarily driven by the
expert activation ratio and the total compute budget, both following
predictable power laws, while expert granularity acts as a non-linear modulator
with a clear optimal range. We integrate these discoveries into a unified
scaling law that accurately predicts the EL of an MoE architecture based on its
configuration. To validate our derived scaling laws, we designed and trained
Ling-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active
parameters, alongside a 6.1B dense model for comparison. When trained on an
identical 1T high-quality token dataset, Ling-mini-beta matched the performance
of the 6.1B dense model while consuming over 7x fewer computational resources,
thereby confirming the accuracy of our scaling laws. This work provides a
principled and empirically-grounded foundation for the scaling of efficient MoE
models.

</details>


### [27] [TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in Languages of West Asia and North Africa](https://arxiv.org/abs/2507.17709)
*Parker Riley,Siamak Shakeri,Waleed Ammar,Jonathan H. Clark*

Main category: cs.CL

TL;DR: TyDi QA-WANA: a question-answering dataset consisting of 28K examples divided among 10 language varieties of western Asia and northern Africa.


<details>
  <summary>Details</summary>
Motivation: The data collection process was designed to elicit information-seeking questions, where the asker is genuinely curious to know the answer.

Method: The data was collected directly in each language variety, without the use of translation, in order to avoid issues of cultural relevance.

Result: We present performance of two baseline models

Conclusion: The authors release their code and data to facilitate further improvement by the research community.

Abstract: We present TyDi QA-WANA, a question-answering dataset consisting of 28K
examples divided among 10 language varieties of western Asia and northern
Africa. The data collection process was designed to elicit information-seeking
questions, where the asker is genuinely curious to know the answer. Each
question in paired with an entire article that may or may not contain the
answer; the relatively large size of the articles results in a task suitable
for evaluating models' abilities to utilize large text contexts in answering
questions. Furthermore, the data was collected directly in each language
variety, without the use of translation, in order to avoid issues of cultural
relevance. We present performance of two baseline models, and release our code
and data to facilitate further improvement by the research community.

</details>


### [28] [From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes](https://arxiv.org/abs/2507.17717)
*Karen Zhou,John Giorgi,Pranav Mani,Peng Xu,Davis Liang,Chenhao Tan*

Main category: cs.CL

TL;DR: This paper proposes a new method to evaluate the quality of AI-generated clinical notes by distilling real user feedback into structured checklists, which outperforms existing methods and aligns well with clinician preferences.


<details>
  <summary>Details</summary>
Motivation: Existing automated metrics often fail to align with real-world physician preferences, and evaluating their quality remains a challenge due to high subjectivity and limited scalability of expert review.

Method: a pipeline that systematically distills real user feedback into structured checklists for note evaluation. These checklists are designed to be interpretable, grounded in human feedback, and enforceable by LLM-based evaluators

Result: our feedback-derived checklist outperforms baseline approaches in our offline evaluations in coverage, diversity, and predictive power for human ratings. Extensive experiments confirm the checklist's robustness to quality-degrading perturbations, significant alignment with clinician preferences, and practical value as an evaluation methodology.

Conclusion: The checklist can help identify notes likely to fall below our chosen quality thresholds in offline research settings.

Abstract: AI-generated clinical notes are increasingly used in healthcare, but
evaluating their quality remains a challenge due to high subjectivity and
limited scalability of expert review. Existing automated metrics often fail to
align with real-world physician preferences. To address this, we propose a
pipeline that systematically distills real user feedback into structured
checklists for note evaluation. These checklists are designed to be
interpretable, grounded in human feedback, and enforceable by LLM-based
evaluators. Using deidentified data from over 21,000 clinical encounters,
prepared in accordance with the HIPAA safe harbor standard, from a deployed AI
medical scribe system, we show that our feedback-derived checklist outperforms
baseline approaches in our offline evaluations in coverage, diversity, and
predictive power for human ratings. Extensive experiments confirm the
checklist's robustness to quality-degrading perturbations, significant
alignment with clinician preferences, and practical value as an evaluation
methodology. In offline research settings, the checklist can help identify
notes likely to fall below our chosen quality thresholds.

</details>


### [29] [AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer](https://arxiv.org/abs/2507.17718)
*Danny D. Leybzon,Shreyas Tirumala,Nishant Jain,Summer Gillen,Michael Jackson,Cameron McPhee,Jennifer Schmidt*

Main category: cs.CL

TL;DR: This paper introduces and tests an AI system for conducting quantitative surveys, finding that shorter surveys and more responsive AI interviewers improve completion rates, reduce break-off rates, and increase respondent satisfaction.


<details>
  <summary>Details</summary>
Motivation: With the rise of voice-enabled artificial intelligence (AI) systems, quantitative survey researchers have access to a new data-collection mode: AI telephone surveying. By using AI to conduct phone interviews, researchers can scale quantitative studies while balancing the dual goals of human-like interactivity and methodological rigor. Unlike earlier efforts that used interactive voice response (IVR) technology to automate these surveys, voice AI enables a more natural and adaptive respondent experience as it is more robust to interruptions, corrections, and other idiosyncrasies of human speech.

Method: We built and tested an AI system to conduct quantitative surveys based on large language models (LLM), automatic speech recognition (ASR), and speech synthesis technologies. The system was specifically designed for quantitative research, and strictly adhered to research best practices like question order randomization, answer order randomization, and exact wording.  To validate the system's effectiveness, we deployed it to conduct two pilot surveys with the SSRS Opinion Panel and followed-up with a separate human-administered survey to assess respondent experiences. We measured three key metrics: the survey completion rates, break-off rates, and respondent satisfaction scores.

Result: Our results suggest that shorter instruments and more responsive AI interviewers may contribute to improvements across all three metrics studied.

Conclusion: Shorter instruments and more responsive AI interviewers may contribute to improvements across all three metrics studied.

Abstract: With the rise of voice-enabled artificial intelligence (AI) systems,
quantitative survey researchers have access to a new data-collection mode: AI
telephone surveying. By using AI to conduct phone interviews, researchers can
scale quantitative studies while balancing the dual goals of human-like
interactivity and methodological rigor. Unlike earlier efforts that used
interactive voice response (IVR) technology to automate these surveys, voice AI
enables a more natural and adaptive respondent experience as it is more robust
to interruptions, corrections, and other idiosyncrasies of human speech.
  We built and tested an AI system to conduct quantitative surveys based on
large language models (LLM), automatic speech recognition (ASR), and speech
synthesis technologies. The system was specifically designed for quantitative
research, and strictly adhered to research best practices like question order
randomization, answer order randomization, and exact wording.
  To validate the system's effectiveness, we deployed it to conduct two pilot
surveys with the SSRS Opinion Panel and followed-up with a separate
human-administered survey to assess respondent experiences. We measured three
key metrics: the survey completion rates, break-off rates, and respondent
satisfaction scores. Our results suggest that shorter instruments and more
responsive AI interviewers may contribute to improvements across all three
metrics studied.

</details>


### [30] [Megrez2 Technical Report](https://arxiv.org/abs/2507.17728)
*Boxun Li,Yadong Li,Zhiyuan Li,Congyi Liu,Weilin Liu,Guowei Niu,Zheyue Tan,Haiyang Xu,Zhuyu Yao,Tao Yuan,Dong Zhou,Yueqing Zhuang,Bo Zhao,Guohao Dai,Yu Wang*

Main category: cs.CL

TL;DR: Megrez2 is a lightweight language model architecture with cross-layer expert sharing and pre-gated routing. Megrez2-Preview achieves competitive performance with fewer parameters, making it suitable for resource-constrained applications.


<details>
  <summary>Details</summary>
Motivation: To create a lightweight and high-performance language model architecture optimized for device native deployment.

Method: Introduces a novel cross-layer expert sharing mechanism and incorporates pre-gated routing.

Result: Megrez2-Preview, with 3B activated and 7.5B stored parameters, shows strong performance in language understanding, instruction following, mathematical reasoning, and code generation.

Conclusion: Megrez2-Preview demonstrates competitive or superior performance compared to larger models. The Megrez2 architecture achieves a balance between accuracy, efficiency, and deployability.

Abstract: We present Megrez2, a novel lightweight and high-performance language model
architecture optimized for device native deployment. Megrez2 introduces a novel
cross-layer expert sharing mechanism, which significantly reduces total
parameter count by reusing expert modules across adjacent transformer layers
while maintaining most of the model's capacity. It also incorporates pre-gated
routing, enabling memory-efficient expert loading and faster inference. As the
first instantiation of the Megrez2 architecture, we introduce the
Megrez2-Preview model, which is pre-trained on a 5-trillion-token corpus and
further enhanced through supervised fine-tuning and reinforcement learning with
verifiable rewards. With only 3B activated and 7.5B stored parameters,
Megrez2-Preview demonstrates competitive or superior performance compared to
larger models on a wide range of tasks, including language understanding,
instruction following, mathematical reasoning, and code generation. These
results highlight the effectiveness of the Megrez2 architecture to achieve a
balance between accuracy, efficiency, and deployability, making it a strong
candidate for real-world, resource-constrained applications.

</details>


### [31] [Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks](https://arxiv.org/abs/2507.17747)
*Linbo Cao,Jinman Zhao*

Main category: cs.CL

TL;DR: 提出了一种基于辩论的评估范式，以提高 QA 评估的难度和减少数据污染，实验结果表明该方法有效且稳健。


<details>
  <summary>Details</summary>
Motivation: 随着前沿语言模型越来越多地饱和标准 QA 基准，人们仍然担心数据污染、记忆和不断升级的数据集创建成本。

Method: 一种基于辩论的评估范式，将任何现有的 QA 数据集转换为结构化的对抗性辩论。

Result: 实证结果验证了该方法的稳健性及其 против 数据污染的有效性——在测试问题上微调的 Llama 3.1 模型显示出显着的准确性提高（50% -> 82%），但在辩论中的表现更差。结果还表明，即使是较弱的评委也可以可靠地区分较强的辩手，突出了基于辩论的评估如何扩展到未来更有能力的系统，同时保持创建新基准成本的一小部分。

Conclusion: 该框架强调“在测试集上进行预训练不再是你所需要的全部”，为衡量高级语言模型的真正推理能力提供了一条可持续的路径。

Abstract: As frontier language models increasingly saturate standard QA benchmarks,
concerns about data contamination, memorization, and escalating dataset
creation costs persist. We propose a debate-driven evaluation paradigm that
transforms any existing QA dataset into structured adversarial debates--where
one model is given the official answer to defend, and another constructs and
defends an alternative answer--adjudicated by a judge model blind to the
correct solution. By forcing multi-round argumentation, this approach
substantially increases difficulty while penalizing shallow memorization, yet
reuses QA items to reduce curation overhead. We make two main contributions:
(1) an evaluation pipeline to systematically convert QA tasks into debate-based
assessments, and (2) a public benchmark that demonstrates our paradigm's
effectiveness on a subset of MMLU-Pro questions, complete with standardized
protocols and reference models. Empirical results validate the robustness of
the method and its effectiveness against data contamination--a Llama 3.1 model
fine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%)
but performed worse in debates. Results also show that even weaker judges can
reliably differentiate stronger debaters, highlighting how debate-based
evaluation can scale to future, more capable systems while maintaining a
fraction of the cost of creating new benchmarks. Overall, our framework
underscores that "pretraining on the test set is no longer all you need,"
offering a sustainable path for measuring the genuine reasoning ability of
advanced language models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [32] [Post-Disaster Affected Area Segmentation with a Vision Transformer (ViT)-based EVAP Model using Sentinel-2 and Formosat-5 Imagery](https://arxiv.org/abs/2507.16849)
*Yi-Shan Chu,Hsuan-Cheng Wei*

Main category: cs.CV

TL;DR: 提出了一种基于ViT的深度学习框架，用于改进灾害影响区域的分割，从而为灾害测绘提供可扩展的方法。


<details>
  <summary>Details</summary>
Motivation: 旨在支持和增强台湾太空中心 (TASA) 开发的突发增值产品 (EVAP)，以改进遥感图像中受灾害影响区域的分割。

Method: 基于视觉Transformer (ViT) 的深度学习框架

Result: 该框架提高了分割结果的平滑性和可靠性

Conclusion: 该框架通过改进分割结果的平滑性和可靠性，为在缺乏准确地面实况时进行灾害测绘提供了一种可扩展的方法。

Abstract: We propose a vision transformer (ViT)-based deep learning framework to refine
disaster-affected area segmentation from remote sensing imagery, aiming to
support and enhance the Emergent Value Added Product (EVAP) developed by the
Taiwan Space Agency (TASA). The process starts with a small set of manually
annotated regions. We then apply principal component analysis (PCA)-based
feature space analysis and construct a confidence index (CI) to expand these
labels, producing a weakly supervised training set. These expanded labels are
then used to train ViT-based encoder-decoder models with multi-band inputs from
Sentinel-2 and Formosat-5 imagery. Our architecture supports multiple decoder
variants and multi-stage loss strategies to improve performance under limited
supervision. During the evaluation, model predictions are compared with
higher-resolution EVAP output to assess spatial coherence and segmentation
consistency. Case studies on the 2022 Poyang Lake drought and the 2023 Rhodes
wildfire demonstrate that our framework improves the smoothness and reliability
of segmentation results, offering a scalable approach for disaster mapping when
accurate ground truth is unavailable.

</details>


### [33] [Toward a Real-Time Framework for Accurate Monocular 3D Human Pose Estimation with Geometric Priors](https://arxiv.org/abs/2507.16850)
*Mohamed Adjel*

Main category: cs.CV

TL;DR: combines real-time 2D keypoint detection with geometry-aware 2D-to-3D lifting, explicitly leveraging known camera intrinsics and subject-specific anatomical priors to enable fast, personalized, and accurate 3D pose estimation from monocular images without requiring specialized hardware.


<details>
  <summary>Details</summary>
Motivation: Monocular 3D human pose estimation remains a challenging and ill-posed problem, particularly in real-time settings and unconstrained environments. While direct imageto-3D approaches require large annotated datasets and heavy models, 2D-to-3D lifting offers a more lightweight and flexible alternative-especially when enhanced with prior knowledge.

Method: combines real-time 2D keypoint detection with geometry-aware 2D-to-3D lifting, explicitly leveraging known camera intrinsics and subject-specific anatomical priors. builds on recent advances in self-calibration and biomechanically-constrained inverse kinematics to generate large-scale, plausible 2D-3D training pairs from MoCap and synthetic datasets.

Result: enable fast, personalized, and accurate 3D pose estimation from monocular images without requiring specialized hardware.

Conclusion: This proposal aims to foster discussion on bridging data-driven learning and model-based priors to improve accuracy, interpretability, and deployability of 3D human motion capture on edge devices in the wild.

Abstract: Monocular 3D human pose estimation remains a challenging and ill-posed
problem, particularly in real-time settings and unconstrained environments.
While direct imageto-3D approaches require large annotated datasets and heavy
models, 2D-to-3D lifting offers a more lightweight and flexible
alternative-especially when enhanced with prior knowledge. In this work, we
propose a framework that combines real-time 2D keypoint detection with
geometry-aware 2D-to-3D lifting, explicitly leveraging known camera intrinsics
and subject-specific anatomical priors. Our approach builds on recent advances
in self-calibration and biomechanically-constrained inverse kinematics to
generate large-scale, plausible 2D-3D training pairs from MoCap and synthetic
datasets. We discuss how these ingredients can enable fast, personalized, and
accurate 3D pose estimation from monocular images without requiring specialized
hardware. This proposal aims to foster discussion on bridging data-driven
learning and model-based priors to improve accuracy, interpretability, and
deployability of 3D human motion capture on edge devices in the wild.

</details>


### [34] [Coarse-to-fine crack cue for robust crack detection](https://arxiv.org/abs/2507.16851)
*Zelong Liu,Yuliang Gu,Zhichao Sun,Huachao Zhu,Xin Xiao,Bo Du,Laurent Najman,Yongchao Xu*

Main category: cs.CV

TL;DR: CrackCue, a novel coarse-to-fine crack cue generation method, leverages the thin structure property of cracks to improve the generalization ability and robustness of crack detection networks.


<details>
  <summary>Details</summary>
Motivation: deep learning-based methods still struggle in generalizing to unseen domains. The thin structure property of cracks is usually overlooked by previous methods.

Method: a novel method for robust crack detection based on coarse-to-fine crack cue generation. Specifically, we first employ a simple max-pooling and upsampling operation on the crack image. This results in a coarse crack-free background, based on which a fine crack-free background can be obtained via a reconstruction network. The difference between the original image and fine crack-free background provides a fine crack cue.

Result: This fine cue embeds robust crack prior information which is unaffected by complex backgrounds, shadow, and varied lighting.

Conclusion: The proposed CrackCue significantly improves the generalization ability and robustness of the baseline methods.

Abstract: Crack detection is an important task in computer vision. Despite impressive
in-dataset performance, deep learning-based methods still struggle in
generalizing to unseen domains. The thin structure property of cracks is
usually overlooked by previous methods. In this work, we introduce CrackCue, a
novel method for robust crack detection based on coarse-to-fine crack cue
generation. The core concept lies on leveraging the thin structure property to
generate a robust crack cue, guiding the crack detection. Specifically, we
first employ a simple max-pooling and upsampling operation on the crack image.
This results in a coarse crack-free background, based on which a fine
crack-free background can be obtained via a reconstruction network. The
difference between the original image and fine crack-free background provides a
fine crack cue. This fine cue embeds robust crack prior information which is
unaffected by complex backgrounds, shadow, and varied lighting. As a
plug-and-play method, we incorporate the proposed CrackCue into three advanced
crack detection networks. Extensive experimental results demonstrate that the
proposed CrackCue significantly improves the generalization ability and
robustness of the baseline methods. The source code will be publicly available.

</details>


### [35] [CLAMP: Contrastive Learning with Adaptive Multi-loss and Progressive Fusion for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.16854)
*Xiaoqiang He*

Main category: cs.CV

TL;DR: 本文提出了一种用于多模态情感分析的对比学习框架，该框架具有自适应多重损失和渐进式注意力融合(CLAMP)，并在公共基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的方法面临着诸如跨模态对齐噪声和细粒度表示中的一致性不足等挑战。全局模态对齐方法通常忽略方面术语及其对应的局部视觉区域之间的联系，弥合文本和图像之间的表示差距仍然是一个挑战。

Method: 本文介绍了一种端到端的对比学习框架，该框架具有自适应多重损失和渐进式注意力融合(CLAMP)。该框架由三个新颖的模块组成：渐进式注意力融合网络、多任务对比学习和自适应多重损失聚合。

Result: 渐进式注意力融合网络通过分层、多阶段的跨模态交互来增强文本特征和图像区域之间的细粒度对齐，从而有效地抑制不相关的视觉噪声。其次，多任务对比学习结合了全局模态对比和局部粒度对齐，以增强跨模态表示一致性。自适应多重损失聚合采用基于动态不确定性的加权机制来根据每个任务的不确定性校准损失贡献，从而减轻梯度干扰。

Conclusion: CLAMP在标准公共基准上始终优于绝大多数现有最先进的方法。

Abstract: Multimodal aspect-based sentiment analysis(MABSA) seeks to identify aspect
terms within paired image-text data and determine their fine grained sentiment
polarities, representing a fundamental task for improving the effectiveness of
applications such as product review systems and public opinion monitoring.
Existing methods face challenges such as cross modal alignment noise and
insufficient consistency in fine-grained representations. While global modality
alignment methods often overlook the connection between aspect terms and their
corresponding local visual regions, bridging the representation gap between
text and images remains a challenge. To address these limitations, this paper
introduces an end to end Contrastive Learning framework with Adaptive
Multi-loss and Progressive Attention Fusion(CLAMP). The framework is composed
of three novel modules: Progressive Attention Fusion network, Multi-task
Contrastive Learning, and Adaptive Multi-loss Aggregation. The Progressive
Attention Fusion network enhances fine-grained alignment between textual
features and image regions via hierarchical, multi-stage cross modal
interactions, effectively suppressing irrelevant visual noise. Secondly,
multi-task contrastive learning combines global modal contrast and local
granularity alignment to enhance cross modal representation consistency.
Adaptive Multi-loss Aggregation employs a dynamic uncertainty based weighting
mechanism to calibrate loss contributions according to each task's uncertainty,
thereby mitigating gradient interference. Evaluation on standard public
benchmarks demonstrates that CLAMP consistently outperforms the vast majority
of existing state of the art methods.

</details>


### [36] [SIA: Enhancing Safety via Intent Awareness for Vision-Language Models](https://arxiv.org/abs/2507.16856)
*Youngjin Na,Sangheon Jeong,Youngwan Lee*

Main category: cs.CV

TL;DR: SIA 是一种免训练的提示工程框架，通过意图感知来主动检测和缓解多模态输入中的有害意图，从而提高视觉语言模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型 (VLM) 越来越多地部署在现实世界的应用中，图像和文本之间微妙的相互作用产生了新的安全风险。特别地，看似无害的输入可以结合起来揭示有害的意图，从而导致不安全的模型响应。尽管人们越来越关注多模态安全性，但先前基于事后过滤或静态拒绝提示的方法难以检测到这种潜在的风险，尤其是在危害性仅从输入的组合中出现时。

Method: SIA 采用了一个三阶段的推理过程：(1) 通过标题进行视觉抽象，(2) 通过少样本链式思考提示进行意图推断，以及 (3) 意图条件下的响应细化。

Result: SIA 实现了显著的安全性改进，优于先前的方法。

Conclusion: SIA通过在 safety-critical benchmarks 上的大量实验表明，其在安全性方面取得了显著提高，优于先前的方法。尽管SIA在 MMStar 上显示出一般推理准确性的略微降低，但相应的安全收益突出了意图感知推理在使 VLM 与以人为本的价值观保持一致方面的价值。

Abstract: As vision-language models (VLMs) are increasingly deployed in real-world
applications, new safety risks arise from the subtle interplay between images
and text. In particular, seemingly innocuous inputs can combine to reveal
harmful intent, leading to unsafe model responses. Despite increasing attention
to multimodal safety, previous approaches based on post hoc filtering or static
refusal prompts struggle to detect such latent risks, especially when
harmfulness emerges only from the combination of inputs. We propose SIA (Safety
via Intent Awareness), a training-free prompt engineering framework that
proactively detects and mitigates harmful intent in multimodal inputs. SIA
employs a three-stage reasoning process: (1) visual abstraction via captioning,
(2) intent inference through few-shot chain-of-thought prompting, and (3)
intent-conditioned response refinement. Rather than relying on predefined rules
or classifiers, SIA dynamically adapts to the implicit intent inferred from the
image-text pair. Through extensive experiments on safety-critical benchmarks
including SIUO, MM-SafetyBench, and HoliSafe, we demonstrate that SIA achieves
substantial safety improvements, outperforming prior methods. Although SIA
shows a minor reduction in general reasoning accuracy on MMStar, the
corresponding safety gains highlight the value of intent-aware reasoning in
aligning VLMs with human-centric values.

</details>


### [37] [Look Before You Fuse: 2D-Guided Cross-Modal Alignment for Robust 3D Detection](https://arxiv.org/abs/2507.16861)
*Xiang Li*

Main category: cs.CV

TL;DR: This paper introduces a method to pre-align cross-modal features before fusion using 2D object priors to address the misalignment between camera and LiDAR features, which achieves state-of-the-art performance on nuScenes validation dataset.


<details>
  <summary>Details</summary>
Motivation: utilize 2D object priors to pre-align cross-modal features before fusion

Method: propose Prior Guided Depth Calibration (PGDC), Discontinuity Aware Geometric Fusion (DAGF) and Structural Guidance Depth Modulator (SGDM)

Result: achieves state-of-the-art performance on nuScenes validation dataset, with its mAP and NDS reaching 71.5% and 73.6% respectively

Conclusion: The proposed method achieves state-of-the-art performance on nuScenes validation dataset, with its mAP and NDS reaching 71.5% and 73.6% respectively.

Abstract: Integrating LiDAR and camera inputs into a unified Bird's-Eye-View (BEV)
representation is crucial for enhancing 3D perception capabilities of
autonomous vehicles. However, current methods are often affected by
misalignment between camera and LiDAR features. This misalignment leads to
inaccurate depth supervision in camera branch and erroneous fusion during
cross-modal feature aggregation. The root cause of this misalignment lies in
projection errors, stemming from minor extrinsic calibration inaccuracies and
rolling shutter effect of LiDAR during vehicle motion. In this work, our key
insight is that these projection errors are predominantly concentrated at
object-background boundaries, which are readily identified by 2D detectors.
Based on this, our main motivation is to utilize 2D object priors to pre-align
cross-modal features before fusion. To address local misalignment, we propose
Prior Guided Depth Calibration (PGDC), which leverages 2D priors to correct
local misalignment and preserve correct cross-modal feature pairs. To resolve
global misalignment, we introduce Discontinuity Aware Geometric Fusion (DAGF)
to process calibrated results from PGDC, suppressing noise and explicitly
enhancing sharp transitions at object-background boundaries. To effectively
utilize these transition-aware depth representations, we incorporate Structural
Guidance Depth Modulator (SGDM), using a gated attention mechanism to
efficiently fuse aligned depth and image features. Our proposed method achieves
state-of-the-art performance on nuScenes validation dataset, with its mAP and
NDS reaching 71.5% and 73.6% respectively.

</details>


### [38] [Pixels, Patterns, but No Poetry: To See The World like Humans](https://arxiv.org/abs/2507.16863)
*Hongcheng Gao,Zihao Huang,Lin Xu,Jingyi Tang,Xinhao Li,Yue Liu,Haoyang Li,Taihang Hu,Minhua Lin,Xinlong Yang,Ge Wu,Balong Bi,Hongyu Chen,Wentao Zhang*

Main category: cs.CV

TL;DR: This paper shifts focus from reasoning to perception and introduces a new benchmark to evaluate MLLMs' performance on synthetic images. The findings reveal that state-of-the-art MLLMs exhibit catastrophic failures on perceptual tasks trivial for humans.


<details>
  <summary>Details</summary>
Motivation: Achieving human-like perception and reasoning in Multimodal Large Language Models (MLLMs) remains a central challenge. Recent research has primarily focused on enhancing reasoning capabilities in MLLMs, a fundamental question persists: Can Multimodal Large Language Models truly perceive the world as humans do?

Method: introduce the Turing Eye Test (TET), a challenging perception-oriented benchmark comprising four diagnostic tasks that evaluate MLLMs' performance on synthetic images that humans process intuitively

Result: Both in-context learning and training on language backbone-effective for previous benchmarks-fail to improve performance on our tasks

Conclusion: state-of-the-art MLLMs exhibit catastrophic failures on perceptual tasks trivial for humans. Fine-tuning the vision tower enables rapid adaptation.

Abstract: Achieving human-like perception and reasoning in Multimodal Large Language
Models (MLLMs) remains a central challenge in artificial intelligence. While
recent research has primarily focused on enhancing reasoning capabilities in
MLLMs, a fundamental question persists: Can Multimodal Large Language Models
truly perceive the world as humans do? This paper shifts focus from reasoning
to perception. Rather than constructing benchmarks specifically for reasoning,
we introduce the Turing Eye Test (TET), a challenging perception-oriented
benchmark comprising four diagnostic tasks that evaluate MLLMs' performance on
synthetic images that humans process intuitively. Our findings reveal that
state-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks
trivial for humans. Both in-context learning and training on language
backbone-effective for previous benchmarks-fail to improve performance on our
tasks, while fine-tuning the vision tower enables rapid adaptation, suggesting
that our benchmark poses challenges for vision tower generalization rather than
for the knowledge and reasoning capabilities of the language backbone-a key gap
between current MLLMs and human perception. We release a representative subset
of TET tasks in this version, and will introduce more diverse tasks and methods
to enhance visual generalization in future work.

</details>


### [39] [HIPPO-Video: Simulating Watch Histories with Large Language Models for Personalized Video Highlighting](https://arxiv.org/abs/2507.16873)
*Jeongeun Lee,Youngjae Yu,Dongha Lee*

Main category: cs.CV

TL;DR: HIPPO-Video是一个用于个性化视频高亮显示的新数据集，它使用基于LLM的用户模拟器生成真实的观看历史，反映了不同的用户偏好。


<details>
  <summary>Details</summary>
Motivation: 用户偏好是高度可变的和复杂的，因此个性化视频高亮显示至关重要。然而，现有的视频数据集通常缺乏个性化，依赖于孤立的视频或简单的文本查询，无法捕捉用户行为的复杂性。

Method: 提出了HiPHer方法，该方法利用个性化的观看历史来预测偏好条件下的分段显著性得分。

Result: 该数据集包括2,040个（观看历史，显着性得分）对，涵盖170个语义类别的20,400个视频。HiPHer方法优于现有的通用和基于查询的方法。

Conclusion: HiPHer方法在用户中心视频高亮显示方面优于现有方法，展示了其在实际场景中的潜力。

Abstract: The exponential growth of video content has made personalized video
highlighting an essential task, as user preferences are highly variable and
complex. Existing video datasets, however, often lack personalization, relying
on isolated videos or simple text queries that fail to capture the intricacies
of user behavior. In this work, we introduce HIPPO-Video, a novel dataset for
personalized video highlighting, created using an LLM-based user simulator to
generate realistic watch histories reflecting diverse user preferences. The
dataset includes 2,040 (watch history, saliency score) pairs, covering 20,400
videos across 170 semantic categories. To validate our dataset, we propose
HiPHer, a method that leverages these personalized watch histories to predict
preference-conditioned segment-wise saliency scores. Through extensive
experiments, we demonstrate that our method outperforms existing generic and
query-based approaches, showcasing its potential for highly user-centric video
highlighting in real-world scenarios.

</details>


### [40] [ReMeREC: Relation-aware and Multi-entity Referring Expression Comprehension](https://arxiv.org/abs/2507.16877)
*Yizhi Hu,Zezhao Tian,Xingqun Qi,Chen Su,Bingkun Yang,Junhui Yin,Muyi Sun,Man Zhang,Zhenan Sun*

Main category: cs.CV

TL;DR: 提出了ReMeREC框架和ReMeX数据集，以解决多实体场景中实体间关系的问题，并在四个基准数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法处理单实体定位，但它们通常忽略多实体场景中复杂的实体间关系，限制了它们的准确性和可靠性。缺乏高质量的数据集与细粒度的、配对的图像-文本-关系注释阻碍了进一步的进展。

Method: 提出了ReMeREC框架，该框架联合利用视觉和文本线索来定位多个实体，同时对它们的相互关系进行建模。引入了文本自适应多实体感知器(TMP)，动态地从细粒度的文本线索中推断实体的数量和范围，从而产生独特的表示。实体间关系推理器(EIR)增强了关系推理和全局场景理解。构建了一个小规模的辅助数据集EntityText，使用大型语言模型生成。

Result: ReMeREC在多实体定位和关系预测方面实现了最先进的性能，大幅超过了现有方法。

Conclusion: ReMeREC在多实体定位和关系预测方面实现了最先进的性能，大幅超过了现有方法。

Abstract: Referring Expression Comprehension (REC) aims to localize specified entities
or regions in an image based on natural language descriptions. While existing
methods handle single-entity localization, they often ignore complex
inter-entity relationships in multi-entity scenes, limiting their accuracy and
reliability. Additionally, the lack of high-quality datasets with fine-grained,
paired image-text-relation annotations hinders further progress. To address
this challenge, we first construct a relation-aware, multi-entity REC dataset
called ReMeX, which includes detailed relationship and textual annotations. We
then propose ReMeREC, a novel framework that jointly leverages visual and
textual cues to localize multiple entities while modeling their
inter-relations. To address the semantic ambiguity caused by implicit entity
boundaries in language, we introduce the Text-adaptive Multi-entity Perceptron
(TMP), which dynamically infers both the quantity and span of entities from
fine-grained textual cues, producing distinctive representations. Additionally,
our Entity Inter-relationship Reasoner (EIR) enhances relational reasoning and
global scene understanding. To further improve language comprehension for
fine-grained prompts, we also construct a small-scale auxiliary dataset,
EntityText, generated using large language models. Experiments on four
benchmark datasets show that ReMeREC achieves state-of-the-art performance in
multi-entity grounding and relation prediction, outperforming existing
approaches by a large margin.

</details>


### [41] [CausalStep: A Benchmark for Explicit Stepwise Causal Reasoning in Videos](https://arxiv.org/abs/2507.16878)
*Xuchen Li,Xuzhao Li,Shiyu Hu,Kaiqi Huang,Wentao Zhang*

Main category: cs.CV

TL;DR: CausalStep 是一个用于视频中显式逐步因果推理的基准。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准主要评估浅层理解和推理，并允许模型利用全局上下文，无法严格评估真正的因果和逐步推理。因此，实现稳健的视频推理仍然是一个重大挑战。

Method: CausalStep 将视频分割成因果关联的单元，并执行严格的逐步问答 (QA) 协议，需要按顺序回答并防止快捷方式解决方案。每个问题都包含基于错误类型分类法精心构建的干扰项，以确保诊断价值。

Result: 对领先的专有模型和开源模型以及人类基线的实验表明，当前模型与人类水平的逐步推理之间存在显着差距。

Conclusion: CausalStep 提供了一个严格的基准，以推动鲁棒和可解释的视频推理方面的进展。

Abstract: Recent advances in large language models (LLMs) have improved reasoning in
text and image domains, yet achieving robust video reasoning remains a
significant challenge. Existing video benchmarks mainly assess shallow
understanding and reasoning and allow models to exploit global context, failing
to rigorously evaluate true causal and stepwise reasoning. We present
CausalStep, a benchmark designed for explicit stepwise causal reasoning in
videos. CausalStep segments videos into causally linked units and enforces a
strict stepwise question-answer (QA) protocol, requiring sequential answers and
preventing shortcut solutions. Each question includes carefully constructed
distractors based on error type taxonomy to ensure diagnostic value. The
benchmark features 100 videos across six categories and 1,852 multiple-choice
QA pairs. We introduce seven diagnostic metrics for comprehensive evaluation,
enabling precise diagnosis of causal reasoning capabilities. Experiments with
leading proprietary and open-source models, as well as human baselines, reveal
a significant gap between current models and human-level stepwise reasoning.
CausalStep provides a rigorous benchmark to drive progress in robust and
interpretable video reasoning.

</details>


### [42] [Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less Local Than Assumed](https://arxiv.org/abs/2507.16880)
*Antoni Kowalczuk,Dominik Hintersdorf,Lukas Struppek,Kristian Kersting,Adam Dziedzic,Franziska Boenisch*

Main category: cs.CV

TL;DR: 文本到图像扩散模型可能无意中记忆和复制训练数据，缓解措施是不够的，并且强调需要真正删除记忆内容的方法。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型（DM）在图像生成方面取得了显著的成功。然而，由于它们可能无意中记忆和复制训练数据，因此对数据隐私和知识产权的担忧仍然存在。最近的缓解工作主要集中在识别和修剪负责触发复制的权重，基于记忆可以被定位的假设。

Method: 我们证明，即使在修剪后，对输入提示的文本嵌入进行微小的调整也足以重新触发数据复制，突出了这些防御措施的脆弱性。此外，我们通过证明复制可以从文本嵌入空间内的不同位置触发，并且遵循模型中的不同路径，从而挑战了记忆局部性的基本假设。

Result: 我们的研究表明，现有的缓解策略是不够的，并且强调需要真正删除记忆内容的方法，而不是试图抑制它的检索。作为第一步，我们引入了一种新的对抗性微调方法，该方法迭代地搜索复制触发器并更新模型以提高鲁棒性。通过我们的研究，我们为文本到图像DM中的记忆本质提供了新的见解，并为构建更值得信赖和合规的生成人工智能奠定了基础。

Conclusion: 现有的缓解策略是不够的，并且强调需要真正删除记忆内容的方法，而不是试图抑制它的检索。作为第一步，我们引入了一种新的对抗性微调方法，该方法迭代地搜索复制触发器并更新模型以提高鲁棒性。

Abstract: Text-to-image diffusion models (DMs) have achieved remarkable success in
image generation. However, concerns about data privacy and intellectual
property remain due to their potential to inadvertently memorize and replicate
training data. Recent mitigation efforts have focused on identifying and
pruning weights responsible for triggering replication, based on the assumption
that memorization can be localized. Our research assesses the robustness of
these pruning-based approaches. We demonstrate that even after pruning, minor
adjustments to text embeddings of input prompts are sufficient to re-trigger
data replication, highlighting the fragility of these defenses. Furthermore, we
challenge the fundamental assumption of memorization locality, by showing that
replication can be triggered from diverse locations within the text embedding
space, and follows different paths in the model. Our findings indicate that
existing mitigation strategies are insufficient and underscore the need for
methods that truly remove memorized content, rather than attempting to suppress
its retrieval. As a first step in this direction, we introduce a novel
adversarial fine-tuning method that iteratively searches for replication
triggers and updates the model to increase robustness. Through our research, we
provide fresh insights into the nature of memorization in text-to-image DMs and
a foundation for building more trustworthy and compliant generative AI.

</details>


### [43] [Sparser2Sparse: Single-shot Sparser-to-Sparse Learning for Spatial Transcriptomics Imputation with Natural Image Co-learning](https://arxiv.org/abs/2507.16886)
*Yaoyu Fang,Jiahe Qian,Xinkun Wang,Lee A. Cooper,Bo Zhou*

Main category: cs.CV

TL;DR: S2S-ST：一种新颖的ST插补框架，它只需要一个单一且低成本的稀疏采样ST数据集以及广泛可用的自然图像进行共同训练。


<details>
  <summary>Details</summary>
Motivation: 高分辨率ST数据的成本高昂和稀缺仍然是重大挑战。

Method: Single-shot Sparser-to-Sparse (S2S-ST)

Result: 在包括乳腺癌、肝脏和淋巴组织在内的多种组织类型的大量实验表明，我们的方法在插补准确性方面优于最先进的方法。

Conclusion: S2S-ST通过从稀疏输入实现强大的ST重建，显著减少了对昂贵的高分辨率数据的依赖，促进了在生物医学研究和临床应用中更广泛的采用。

Abstract: Spatial transcriptomics (ST) has revolutionized biomedical research by
enabling high resolution gene expression profiling within tissues. However, the
high cost and scarcity of high resolution ST data remain significant
challenges. We present Single-shot Sparser-to-Sparse (S2S-ST), a novel
framework for accurate ST imputation that requires only a single and low-cost
sparsely sampled ST dataset alongside widely available natural images for
co-training. Our approach integrates three key innovations: (1) a
sparser-to-sparse self-supervised learning strategy that leverages intrinsic
spatial patterns in ST data, (2) cross-domain co-learning with natural images
to enhance feature representation, and (3) a Cascaded Data Consistent
Imputation Network (CDCIN) that iteratively refines predictions while
preserving sampled gene data fidelity. Extensive experiments on diverse tissue
types, including breast cancer, liver, and lymphoid tissue, demonstrate that
our method outperforms state-of-the-art approaches in imputation accuracy. By
enabling robust ST reconstruction from sparse inputs, our framework
significantly reduces reliance on costly high resolution data, facilitating
potential broader adoption in biomedical research and clinical applications.

</details>


### [44] [AURA: A Multi-Modal Medical Agent for Understanding, Reasoning & Annotation](https://arxiv.org/abs/2507.16940)
*Nima Fathi,Amar Kumar,Tal Arbel*

Main category: cs.CV

TL;DR: AURA is the first visual linguistic explainability agent for medical images, enabling interactive analysis and decision support.


<details>
  <summary>Details</summary>
Motivation: Applying LLM-based agentic systems to medical imaging is still in its infancy, despite their promise in other domains.

Method: AURA, the first visual linguistic explainability agent, leverages Qwen-32B and integrates a modular toolbox comprising segmentation, counterfactual image-generation, and evaluation tools.

Result: AURA enables dynamic interactions, contextual explanations, and hypothesis testing, representing a significant advancement toward more transparent, adaptable, and clinically aligned AI systems.

Conclusion: Agentic AI systems hold significant promise for transforming medical image analysis from static predictions to interactive decision support.

Abstract: Recent advancements in Large Language Models (LLMs) have catalyzed a paradigm
shift from static prediction systems to agentic AI agents capable of reasoning,
interacting with tools, and adapting to complex tasks. While LLM-based agentic
systems have shown promise across many domains, their application to medical
imaging remains in its infancy. In this work, we introduce AURA, the first
visual linguistic explainability agent designed specifically for comprehensive
analysis, explanation, and evaluation of medical images. By enabling dynamic
interactions, contextual explanations, and hypothesis testing, AURA represents
a significant advancement toward more transparent, adaptable, and clinically
aligned AI systems. We highlight the promise of agentic AI in transforming
medical image analysis from static predictions to interactive decision support.
Leveraging Qwen-32B, an LLM-based architecture, AURA integrates a modular
toolbox comprising: (i) a segmentation suite with phase grounding, pathology
segmentation, and anatomy segmentation to localize clinically meaningful
regions; (ii) a counterfactual image-generation module that supports reasoning
through image-level explanations; and (iii) a set of evaluation tools including
pixel-wise difference-map analysis, classification, and advanced
state-of-the-art components to assess diagnostic relevance and visual
interpretability.

</details>


### [45] [Toward Long-Tailed Online Anomaly Detection through Class-Agnostic Concepts](https://arxiv.org/abs/2507.16946)
*Chiao-An Yang,Kuan-Chuan Peng,Raymond A. Yeh*

Main category: cs.CV

TL;DR: Proposes a class-agnostic framework for long-tailed online anomaly detection (LTOAD), outperforming existing methods in both offline and online settings.


<details>
  <summary>Details</summary>
Motivation: Extending anomaly detection (AD) to a realistic setting by considering the novel task of long-tailed online AD (LTOAD). Offline state-of-the-art LTAD methods cannot be directly applied to the online setting because LTAD is class-aware, requiring class labels not available online.

Method: A class-agnostic framework for LTAD adapted to the online learning setting.

Result: Outperforms the SOTA baselines in most offline LTAD settings, including both the industrial manufacturing and the medical domain. +4.63% image-AUROC on MVTec. +0.53% image-AUROC compared to baselines in the most challenging long-tailed online setting.

Conclusion: The proposed class-agnostic framework outperforms state-of-the-art baselines in most offline long-tailed anomaly detection (LTAD) settings, achieving +4.63% image-AUROC on MVTec and +0.53% in the long-tailed online setting.

Abstract: Anomaly detection (AD) identifies the defect regions of a given image. Recent
works have studied AD, focusing on learning AD without abnormal images, with
long-tailed distributed training data, and using a unified model for all
classes. In addition, online AD learning has also been explored. In this work,
we expand in both directions to a realistic setting by considering the novel
task of long-tailed online AD (LTOAD). We first identified that the offline
state-of-the-art LTAD methods cannot be directly applied to the online setting.
Specifically, LTAD is class-aware, requiring class labels that are not
available in the online setting. To address this challenge, we propose a
class-agnostic framework for LTAD and then adapt it to our online learning
setting. Our method outperforms the SOTA baselines in most offline LTAD
settings, including both the industrial manufacturing and the medical domain.
In particular, we observe +4.63% image-AUROC on MVTec even compared to methods
that have access to class labels and the number of classes. In the most
challenging long-tailed online setting, we achieve +0.53% image-AUROC compared
to baselines. Our LTOAD benchmark is released here:
https://doi.org/10.5281/zenodo.16283852 .

</details>


### [46] [Divisive Decisions: Improving Salience-Based Training for Generalization in Binary Classification Tasks](https://arxiv.org/abs/2507.17000)
*Jacob Piland,Chris Sweet,Adam Czajka*

Main category: cs.CV

TL;DR: 提出了利用真假类CAM的显著性引导训练方法，并在多个二元分类任务中验证了其有效性，提高了模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 以往的研究忽略了假类CAM，也就是模型对于不正确标签类获得的显著性。我们假设在二元任务中，真和假CAM应该在人类识别的重要分类特征上有所不同（并在人类显著性地图中有所体现）。

Method: 提出了三种新的显著性引导训练方法，这些方法将真类和假类模型CAM都纳入训练策略中，以及一种用于识别重要特征的新颖的post-hoc工具。

Result: 在几个不同的二元封闭集和开放集分类任务（包括合成人脸检测、生物特征呈现攻击检测和胸部X射线扫描异常分类）中，对所有引入的方法进行了评估，发现提出的方法提高了深度学习模型的泛化能力。

Conclusion: 提出的方法在深度学习模型上，相比于传统的（仅使用真类CAM）显著性引导训练方法，提高了泛化能力。

Abstract: Existing saliency-guided training approaches improve model generalization by
incorporating a loss term that compares the model's class activation map (CAM)
for a sample's true-class ({\it i.e.}, correct-label class) against a human
reference saliency map. However, prior work has ignored the false-class CAM(s),
that is the model's saliency obtained for incorrect-label class. We hypothesize
that in binary tasks the true and false CAMs should diverge on the important
classification features identified by humans (and reflected in human saliency
maps). We use this hypothesis to motivate three new saliency-guided training
methods incorporating both true- and false-class model's CAM into the training
strategy and a novel post-hoc tool for identifying important features. We
evaluate all introduced methods on several diverse binary close-set and
open-set classification tasks, including synthetic face detection, biometric
presentation attack detection, and classification of anomalies in chest X-ray
scans, and find that the proposed methods improve generalization capabilities
of deep learning models over traditional (true-class CAM only) saliency-guided
training approaches. We offer source codes and model weights\footnote{GitHub
repository link removed to preserve anonymity} to support reproducible
research.

</details>


### [47] [HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic Learning](https://arxiv.org/abs/2507.17402)
*Li Jun,Wang Jinpeng,Tan Chaolei,Lian Niu,Chen Long,Zhang Min,Wang Yaowei,Xia Shu-Tao,Chen Bin*

Main category: cs.CV

TL;DR: HLFormer：首个用于部分相关视频检索的双曲建模框架，通过双曲空间学习来补偿欧几里得空间的次优层次建模能力，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的部分相关视频检索 (PRVR) 方法在欧几里得空间中存在几何失真，有时会错误地表示视频的内在层次结构，并忽略某些层次语义，最终导致次优的时间建模。

Method: 提出了一个双曲建模框架 HLFormer，它结合了 Lorentz 注意力块和欧几里得注意力块以在混合空间中编码视频嵌入，并使用均值引导自适应交互模块来动态融合特征。此外，引入了偏序保持损失，以通过洛伦兹锥约束来加强“文本 < 视频”的层次结构。

Result: 大量实验表明，HLFormer 优于现有技术水平的方法。

Conclusion: HLFormer 在 PRVR 任务上超越了现有最佳方法。

Abstract: Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of
matching untrimmed videos with text queries describing only partial content.
Existing methods suffer from geometric distortion in Euclidean space that
sometimes misrepresents the intrinsic hierarchical structure of videos and
overlooks certain hierarchical semantics, ultimately leading to suboptimal
temporal modeling. To address this issue, we propose the first hyperbolic
modeling framework for PRVR, namely HLFormer, which leverages hyperbolic space
learning to compensate for the suboptimal hierarchical modeling capabilities of
Euclidean space. Specifically, HLFormer integrates the Lorentz Attention Block
and Euclidean Attention Block to encode video embeddings in hybrid spaces,
using the Mean-Guided Adaptive Interaction Module to dynamically fuse features.
Additionally, we introduce a Partial Order Preservation Loss to enforce "text <
video" hierarchy through Lorentzian cone constraints. This approach further
enhances cross-modal matching by reinforcing partial relevance between video
content and text queries. Extensive experiments show that HLFormer outperforms
state-of-the-art methods. Code is released at
https://github.com/lijun2005/ICCV25-HLFormer.

</details>


### [48] [Bringing Balance to Hand Shape Classification: Mitigating Data Imbalance Through Generative Models](https://arxiv.org/abs/2507.17008)
*Gaston Gustavo Rios,Pedro Dal Bianco,Franco Ronchetti,Facundo Quiroga,Oscar Stanchi,Santiago Ponte Ahón,Waldo Hasperué*

Main category: cs.CV

TL;DR: 利用生成对抗网络（GAN）生成合成数据来增强手形分类器的训练数据，从而提高精度并解决数据集不平衡的问题。


<details>
  <summary>Details</summary>
Motivation: 大多数手语手形数据集受到严重限制且不平衡，给有效的模型训练带来了重大挑战。在本文中，我们探索了通过生成合成数据来增强手形分类器的训练数据的有效性。

Method: 使用在RWTH German sign language手形数据集上训练的EfficientNet分类器，应用不同的策略来组合生成和真实图像。比较了两种用于数据生成的生成对抗网络（GAN）架构：ReACGAN和SPADE。

Result: 所提出的技术将RWTH数据集上当前最先进的精度提高了5%。实现了与单源训练分类器相当的性能，而无需重新训练生成器。

Conclusion: 该方法在RWTH数据集上将当前最先进的精度提高了5%，解决了小型和不平衡数据集的局限性。此外，该方法证明了通过利用在广泛的HaGRID数据集上训练的基于姿势的生成，能够推广到不同的手语数据集。在不需要重新训练生成器的情况下，实现了与单源训练分类器相当的性能。

Abstract: Most sign language handshape datasets are severely limited and unbalanced,
posing significant challenges to effective model training. In this paper, we
explore the effectiveness of augmenting the training data of a handshape
classifier by generating synthetic data. We use an EfficientNet classifier
trained on the RWTH German sign language handshape dataset, which is small and
heavily unbalanced, applying different strategies to combine generated and real
images. We compare two Generative Adversarial Networks (GAN) architectures for
data generation: ReACGAN, which uses label information to condition the data
generation process through an auxiliary classifier, and SPADE, which utilizes
spatially-adaptive normalization to condition the generation on pose
information. ReACGAN allows for the generation of realistic images that align
with specific handshape labels, while SPADE focuses on generating images with
accurate spatial handshape configurations. Our proposed techniques improve the
current state-of-the-art accuracy on the RWTH dataset by 5%, addressing the
limitations of small and unbalanced datasets. Additionally, our method
demonstrates the capability to generalize across different sign language
datasets by leveraging pose-based generation trained on the extensive HaGRID
dataset. We achieve comparable performance to single-source trained classifiers
without the need for retraining the generator.

</details>


### [49] [Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for Tumor Flagging and Staging](https://arxiv.org/abs/2507.17412)
*Farnaz Khun Jush,Steffen Vogler,Matthias Lenga*

Main category: cs.CV

TL;DR: 本文介绍了C-MIR，一种用于体积医学图像的新型重排序方法，它在肿瘤标记方面表现出令人鼓舞的改进，尤其是在结肠癌和肺癌方面。


<details>
  <summary>Details</summary>
Motivation: 医学图像的体积不断增加，给放射科医生检索相关病例带来了挑战。基于内容的图像检索（CBIR）系统为有效访问相似病例提供了潜力，但缺乏标准化评估和综合研究。

Method: 引入C-MIR，一种新颖的体积重排序方法，它调整了ColBERT的上下文后期交互机制以用于3D医学成像。

Result: 我们的评估突出了C-MIR的显着优势。我们证明了后期交互原理成功适应于体积医学图像，从而实现了有效的上下文感知重排序。一个关键发现是C-MIR能够有效地定位感兴趣区域，无需对数据集进行预分割，并为依赖昂贵数据丰富步骤的系统提供了一种计算高效的替代方案。

Conclusion: C-MIR在肿瘤标记方面表现出令人鼓舞的改进，尤其是在结肠癌和肺癌方面（p<0.05）。C-MIR还显示出改善肿瘤分期的潜力，值得进一步探索其能力。最终，我们的工作旨在弥合先进检索技术与它们在医疗保健中的实际应用之间的差距，为改进诊断过程铺平道路。

Abstract: The increasing volume of medical images poses challenges for radiologists in
retrieving relevant cases. Content-based image retrieval (CBIR) systems offer
potential for efficient access to similar cases, yet lack standardized
evaluation and comprehensive studies. Building on prior studies for tumor
characterization via CBIR, this study advances CBIR research for volumetric
medical images through three key contributions: (1) a framework eliminating
reliance on pre-segmented data and organ-specific datasets, aligning with large
and unstructured image archiving systems, i.e. PACS in clinical practice; (2)
introduction of C-MIR, a novel volumetric re-ranking method adapting ColBERT's
contextualized late interaction mechanism for 3D medical imaging; (3)
comprehensive evaluation across four tumor sites using three feature extractors
and three database configurations. Our evaluations highlight the significant
advantages of C-MIR. We demonstrate the successful adaptation of the late
interaction principle to volumetric medical images, enabling effective
context-aware re-ranking. A key finding is C-MIR's ability to effectively
localize the region of interest, eliminating the need for pre-segmentation of
datasets and offering a computationally efficient alternative to systems
relying on expensive data enrichment steps. C-MIR demonstrates promising
improvements in tumor flagging, achieving improved performance, particularly
for colon and lung tumors (p<0.05). C-MIR also shows potential for improving
tumor staging, warranting further exploration of its capabilities. Ultimately,
our work seeks to bridge the gap between advanced retrieval techniques and
their practical applications in healthcare, paving the way for improved
diagnostic processes.

</details>


### [50] [Transformer Based Building Boundary Reconstruction using Attraction Field Maps](https://arxiv.org/abs/2507.17038)
*Muhammad Kamran,Mohammad Moein Sheikholeslami,Andreas Wichmann,Gunho Sohn*

Main category: cs.CV

TL;DR: This paper introduces a novel GCN-based deep learning methodology for automated building footprint extraction from satellite images. The model, Decoupled-PolyGCN, outperforms existing methods by 6% in AP and 10% in AR.


<details>
  <summary>Details</summary>
Motivation: Reconstructing spatial maps from satellite imagery is a complex computer vision task, requiring the creation of high-level object representations, such as primitives, to accurately capture the built environment. Consequently, high-quality spatial maps often rely on labor-intensive and manual processes.

Method: This paper introduces a novel deep learning methodology leveraging Graph Convolutional Networks (GCNs) to address these challenges in building footprint reconstruction. The proposed approach enhances performance by incorporating geometric regularity into building boundaries, integrating multi-scale and multi-resolution features, and embedding Attraction Field Maps into the network.

Result: The proposed approach enhances performance by incorporating geometric regularity into building boundaries, integrating multi-scale and multi-resolution features, and embedding Attraction Field Maps into the network. Decoupled-PolyGCN, outperforms existing methods by 6% in AP and 10% in AR

Conclusion: The Decoupled-PolyGCN model outperforms existing methods by 6% in AP and 10% in AR, demonstrating its ability to deliver accurate and regularized building footprints across diverse and challenging scenarios.

Abstract: In recent years, the number of remote satellites orbiting the Earth has grown
significantly, streaming vast amounts of high-resolution visual data to support
diverse applications across civil, public, and military domains. Among these
applications, the generation and updating of spatial maps of the built
environment have become critical due to the extensive coverage and detailed
imagery provided by satellites. However, reconstructing spatial maps from
satellite imagery is a complex computer vision task, requiring the creation of
high-level object representations, such as primitives, to accurately capture
the built environment. While the past decade has witnessed remarkable
advancements in object detection and representation using visual data,
primitives-based object representation remains a persistent challenge in
computer vision. Consequently, high-quality spatial maps often rely on
labor-intensive and manual processes. This paper introduces a novel deep
learning methodology leveraging Graph Convolutional Networks (GCNs) to address
these challenges in building footprint reconstruction. The proposed approach
enhances performance by incorporating geometric regularity into building
boundaries, integrating multi-scale and multi-resolution features, and
embedding Attraction Field Maps into the network. These innovations provide a
scalable and precise solution for automated building footprint extraction from
a single satellite image, paving the way for impactful applications in urban
planning, disaster management, and large-scale spatial analysis. Our model,
Decoupled-PolyGCN, outperforms existing methods by 6% in AP and 10% in AR,
demonstrating its ability to deliver accurate and regularized building
footprints across diverse and challenging scenarios.

</details>


### [51] [Controllable Hybrid Captioner for Improved Long-form Video Understanding](https://arxiv.org/abs/2507.17047)
*Kuleen Sasse,Efsun Sarioglu Kayi,Arun Reddy*

Main category: cs.CV

TL;DR: The paper introduces a system that uses a video captioner and a LLM to answer questions about videos. It improves captioning by adding scene descriptions and fine-tuning the captioner to produce both action and scene captions.


<details>
  <summary>Details</summary>
Motivation: Video data is dense and high-dimensional. Text-based summaries offer a more compact representation for query-relevant content and enable reasoning over video content using LLMs.

Method: The authors rely on the progressive construction of a text-based memory by a video captioner on shorter video chunks. They incorporate static scene descriptions using LLaVA VLM and fine-tune the LaViLa video captioner.

Result: The authors improved the quality of the activity log by enriching it with static scene descriptions. They explored different ways of partitioning the video into meaningful segments and incorporated static scene descriptions using LLaVA VLM. The fine-tuned LaViLa video captioner produces both action and scene captions.

Conclusion: The authors successfully fine-tuned the LaViLa video captioner to produce both action and scene captions, significantly improving the efficiency of the captioning pipeline. They introduced a controllable hybrid captioner that alternates between different types of captions based on scene changes detected in the video.

Abstract: Video data, especially long-form video, is extremely dense and
high-dimensional. Text-based summaries of video content offer a way to
represent query-relevant content in a much more compact manner than raw video.
In addition, textual representations are easily ingested by state-of-the-art
large language models (LLMs), which enable reasoning over video content to
answer complex natural language queries. To solve this issue, we rely on the
progressive construction of a text-based memory by a video captioner operating
on shorter chunks of the video, where spatio-temporal modeling is
computationally feasible. We explore ways to improve the quality of the
activity log comprised solely of short video captions. Because the video
captions tend to be focused on human actions, and questions may pertain to
other information in the scene, we seek to enrich the memory with static scene
descriptions using Vision Language Models (VLMs). Our video understanding
system relies on the LaViLa video captioner in combination with a LLM to answer
questions about videos. We first explored different ways of partitioning the
video into meaningful segments such that the textual descriptions more
accurately reflect the structure of the video content. Furthermore, we
incorporated static scene descriptions into the captioning pipeline using LLaVA
VLM, resulting in a more detailed and complete caption log and expanding the
space of questions that are answerable from the textual memory. Finally, we
have successfully fine-tuned the LaViLa video captioner to produce both action
and scene captions, significantly improving the efficiency of the captioning
pipeline compared to using separate captioning models for the two tasks. Our
model, controllable hybrid captioner, can alternate between different types of
captions according to special input tokens that signals scene changes detected
in the video.

</details>


### [52] [Toward Scalable Video Narration: A Training-free Approach Using Multimodal Large Language Models](https://arxiv.org/abs/2507.17050)
*Tz-Ying Wu,Tahani Trigui,Sharath Nittur Sridhar,Anand Bodas,Subarna Tripathi*

Main category: cs.CV

TL;DR: VideoNarrator, a training-free pipeline, improves video narration by reducing hallucinations and enhancing temporal alignment using MLLMs and VLMs.


<details>
  <summary>Details</summary>
Motivation: MLLMs often struggle with temporally aligned narrations and tend to hallucinate, particularly in unfamiliar scenarios.

Method: A training-free pipeline called VideoNarrator is introduced, leveraging off-the-shelf MLLMs and VLMs as caption generators, context providers, or caption verifiers.

Result: Experimental results demonstrate that the approach significantly enhances the quality and accuracy of video narrations, effectively reducing hallucinations and improving temporal alignment.

Conclusion: The synergistic interaction of MLLMs and VLMs enhances video narration quality, reduces hallucinations, and improves temporal alignment, benefiting downstream tasks and potential applications in advertising and marketing.

Abstract: In this paper, we introduce VideoNarrator, a novel training-free pipeline
designed to generate dense video captions that offer a structured snapshot of
video content. These captions offer detailed narrations with precise
timestamps, capturing the nuances present in each segment of the video. Despite
advancements in multimodal large language models (MLLMs) for video
comprehension, these models often struggle with temporally aligned narrations
and tend to hallucinate, particularly in unfamiliar scenarios. VideoNarrator
addresses these challenges by leveraging a flexible pipeline where
off-the-shelf MLLMs and visual-language models (VLMs) can function as caption
generators, context providers, or caption verifiers. Our experimental results
demonstrate that the synergistic interaction of these components significantly
enhances the quality and accuracy of video narrations, effectively reducing
hallucinations and improving temporal alignment. This structured approach not
only enhances video understanding but also facilitates downstream tasks such as
video summarization and video question answering, and can be potentially
extended for advertising and marketing applications.

</details>


### [53] [Few-Shot Learning in Video and 3D Object Detection: A Survey](https://arxiv.org/abs/2507.17079)
*Md Meftahul Ferdaus,Kendall N. Niles,Joe Tom,Mahdi Abdelguerfi,Elias Ioup*

Main category: cs.CV

TL;DR: FSL reduces annotation needs for video and 3D object detection by leveraging spatiotemporal structure and specialized point cloud networks.


<details>
  <summary>Details</summary>
Motivation: Few-shot learning (FSL) enables object detection models to recognize novel classes given only a few annotated examples, thereby reducing expensive manual data labeling.

Method: This survey examines recent FSL advances for video and 3D object detection.

Result: FSL for video is valuable since annotating objects across frames is more laborious than for static images. Few-shot 3D detection enables practical autonomous driving deployment by minimizing costly 3D annotation needs.

Conclusion: FSL shows promise for reducing annotation requirements and enabling real-world video, 3D, and other applications by efficiently leveraging information across feature, temporal, and data modalities.

Abstract: Few-shot learning (FSL) enables object detection models to recognize novel
classes given only a few annotated examples, thereby reducing expensive manual
data labeling. This survey examines recent FSL advances for video and 3D object
detection. For video, FSL is especially valuable since annotating objects
across frames is more laborious than for static images. By propagating
information across frames, techniques like tube proposals and temporal matching
networks can detect new classes from a couple examples, efficiently leveraging
spatiotemporal structure. FSL for 3D detection from LiDAR or depth data faces
challenges like sparsity and lack of texture. Solutions integrate FSL with
specialized point cloud networks and losses tailored for class imbalance.
Few-shot 3D detection enables practical autonomous driving deployment by
minimizing costly 3D annotation needs. Core issues in both domains include
balancing generalization and overfitting, integrating prototype matching, and
handling data modality properties. In summary, FSL shows promise for reducing
annotation requirements and enabling real-world video, 3D, and other
applications by efficiently leveraging information across feature, temporal,
and data modalities. By comprehensively surveying recent advancements, this
paper illuminates FSL's potential to minimize supervision needs and enable
deployment across video, 3D, and other real-world applications.

</details>


### [54] [SDGOCC: Semantic and Depth-Guided Bird's-Eye View Transformation for 3D Multimodal Occupancy Prediction](https://arxiv.org/abs/2507.17083)
*Zaipeng Duan,Chenxu Dang,Xuzhong Hu,Pei An,Junfeng Ding,Jie Zhan,Yunbiao Xu,Jie Ma*

Main category: cs.CV

TL;DR: 提出了一种新颖的多模态 occupancy prediction 网络 SDG-OCC，通过联合语义和深度引导的视图转换和主动蒸馏，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态 3D occupancy prediction 方法大多是单模态的：基于相机的方法缺乏深度信息，而基于 LiDAR 的方法难以应对遮挡。

Method: 提出了一种新颖的多模态 occupancy prediction 网络，称为 SDG-OCC，它结合了联合语义和深度引导的视图转换，以及 fusion-to-occupancy-driven 的主动蒸馏。

Result: 通过集成像素语义和共点深度，构建精确的深度分布；从多模态数据中提取丰富的语义信息，并基于 LiDAR 识别的区域，选择性地将知识转移到图像特征。

Conclusion: 实现了在 Occ3D-nuScenes 数据集上的最先进性能，并在更具挑战性的 SurroundOcc-nuScenes 数据集上表现出可比的性能，证明了其有效性和鲁棒性。

Abstract: Multimodal 3D occupancy prediction has garnered significant attention for its
potential in autonomous driving. However, most existing approaches are
single-modality: camera-based methods lack depth information, while LiDAR-based
methods struggle with occlusions. Current lightweight methods primarily rely on
the Lift-Splat-Shoot (LSS) pipeline, which suffers from inaccurate depth
estimation and fails to fully exploit the geometric and semantic information of
3D LiDAR points. Therefore, we propose a novel multimodal occupancy prediction
network called SDG-OCC, which incorporates a joint semantic and depth-guided
view transformation coupled with a fusion-to-occupancy-driven active
distillation. The enhanced view transformation constructs accurate depth
distributions by integrating pixel semantics and co-point depth through
diffusion and bilinear discretization. The fusion-to-occupancy-driven active
distillation extracts rich semantic information from multimodal data and
selectively transfers knowledge to image features based on LiDAR-identified
regions. Finally, for optimal performance, we introduce SDG-Fusion, which uses
fusion alone, and SDG-KL, which integrates both fusion and distillation for
faster inference. Our method achieves state-of-the-art (SOTA) performance with
real-time processing on the Occ3D-nuScenes dataset and shows comparable
performance on the more challenging SurroundOcc-nuScenes dataset, demonstrating
its effectiveness and robustness. The code will be released at
https://github.com/DzpLab/SDGOCC.

</details>


### [55] [FedVLM: Scalable Personalized Vision-Language Models through Federated Learning](https://arxiv.org/abs/2507.17088)
*Arkajyoti Mitra,Afia Anjum,Paul Agbaje,Mert Pesé,Habeeb Olufowobi*

Main category: cs.CV

TL;DR: This paper introduces FedVLM, a federated learning framework with personalized LoRA (pLoRA) to fine-tune vision-language models in decentralized, heterogeneous data environments. pLoRA improves performance by 24.5% compared to standard LoRA.


<details>
  <summary>Details</summary>
Motivation: fine-tuning these models at scale remains challenging, particularly in federated environments where data is decentralized and non-iid across clients. Existing parameter-efficient tuning methods like LoRA (Low-Rank Adaptation) reduce computational overhead but struggle with heterogeneous client data, leading to suboptimal generalization

Method: a federated LoRA fine-tuning framework that enables decentralized adaptation of VLMs while preserving model privacy and reducing reliance on centralized training. To further tackle data heterogeneity, we introduce personalized LoRA (pLoRA), which dynamically adapts LoRA parameters to each client's unique data distribution

Result: pLoRA improves client-specific performance by 24.5% over standard LoRA, demonstrating superior adaptation in non-iid settings

Conclusion: FedVLM provides a scalable and efficient solution for fine-tuning VLMs in federated settings, advancing personalized adaptation in distributed learning scenarios.

Abstract: Vision-language models (VLMs) demonstrate impressive zero-shot and few-shot
learning capabilities, making them essential for several downstream tasks.
However, fine-tuning these models at scale remains challenging, particularly in
federated environments where data is decentralized and non-iid across clients.
Existing parameter-efficient tuning methods like LoRA (Low-Rank Adaptation)
reduce computational overhead but struggle with heterogeneous client data,
leading to suboptimal generalization. To address these challenges, we propose
FedVLM, a federated LoRA fine-tuning framework that enables decentralized
adaptation of VLMs while preserving model privacy and reducing reliance on
centralized training. To further tackle data heterogeneity, we introduce
personalized LoRA (pLoRA), which dynamically adapts LoRA parameters to each
client's unique data distribution, significantly improving local adaptation
while maintaining global model aggregation. Experiments on the RLAIF-V dataset
show that pLoRA improves client-specific performance by 24.5% over standard
LoRA, demonstrating superior adaptation in non-iid settings. FedVLM provides a
scalable and efficient solution for fine-tuning VLMs in federated settings,
advancing personalized adaptation in distributed learning scenarios.

</details>


### [56] [IONext: Unlocking the Next Era of Inertial Odometry](https://arxiv.org/abs/2507.17089)
*Shanshan Zhang,Siyue Wang,Tianshui Wen,Qi Zhang,Ziheng Zhou,Lingxiang Zheng,Yu Yang*

Main category: cs.CV

TL;DR: IONext 是一种新型 CNN 架构，用于惯性里程计，它优于现有技术。


<details>
  <summary>Details</summary>
Motivation: Transformer在建模长程依赖关系方面表现出色，但它们对局部、细粒度运动变化的敏感性有限，并且缺乏固有的归纳偏差，这通常会阻碍定位精度和泛化。最近的研究表明，将大核卷积和受 Transformer 启发的架构设计融入 CNN 可以有效地扩展感受野，从而提高全局运动感知。

Method: 我们提出了一个名为双翼自适应动态混合器 (DADM) 的新型基于 CNN 的模块，该模块自适应地捕获来自动态输入的全局运动模式和局部、细粒度的运动特征。为了进一步改进时间建模，我们引入了时空门控单元 (STGU)，它在时间域中选择性地提取具有代表性和与任务相关的运动特征。在此基础上，我们提出了一个新的基于 CNN 的惯性里程计骨干网络，名为下一代惯性里程计 (IONext)。

Result: 我们提出了一个新的基于 CNN 的惯性里程计骨干网络，名为下一代惯性里程计 (IONext)。在六个公共数据集上进行了大量实验，结果表明 IONext 始终优于最先进的 (SOTA) Transformer 和基于 CNN 的方法。例如，在 RNIN 数据集上，与代表性模型 iMOT 相比，IONext 将平均 ATE 降低了 10%，将平均 RTE 降低了 12%。

Conclusion: IONext在六个公共数据集上始终优于最先进的 Transformer 和 CNN 方法。例如，在 RNIN 数据集上，与代表性模型 iMOT 相比，IONext 将平均 ATE 降低了 10%，将平均 RTE 降低了 12%。

Abstract: Researchers have increasingly adopted Transformer-based models for inertial
odometry. While Transformers excel at modeling long-range dependencies, their
limited sensitivity to local, fine-grained motion variations and lack of
inherent inductive biases often hinder localization accuracy and
generalization. Recent studies have shown that incorporating large-kernel
convolutions and Transformer-inspired architectural designs into CNN can
effectively expand the receptive field, thereby improving global motion
perception. Motivated by these insights, we propose a novel CNN-based module
called the Dual-wing Adaptive Dynamic Mixer (DADM), which adaptively captures
both global motion patterns and local, fine-grained motion features from
dynamic inputs. This module dynamically generates selective weights based on
the input, enabling efficient multi-scale feature aggregation. To further
improve temporal modeling, we introduce the Spatio-Temporal Gating Unit (STGU),
which selectively extracts representative and task-relevant motion features in
the temporal domain. This unit addresses the limitations of temporal modeling
observed in existing CNN approaches. Built upon DADM and STGU, we present a new
CNN-based inertial odometry backbone, named Next Era of Inertial Odometry
(IONext). Extensive experiments on six public datasets demonstrate that IONext
consistently outperforms state-of-the-art (SOTA) Transformer- and CNN-based
methods. For instance, on the RNIN dataset, IONext reduces the average ATE by
10% and the average RTE by 12% compared to the representative model iMOT.

</details>


### [57] [Robust Five-Class and binary Diabetic Retinopathy Classification Using Transfer Learning and Data Augmentation](https://arxiv.org/abs/2507.17121)
*Faisal Ahmed,Mohammad Alfrad Nobel Bhuiyan*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的糖尿病视网膜病变(DR)分类框架，该框架利用迁移学习和数据增强技术，在二元分类和五类分类任务中均取得了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变(DR)是全球范围内导致视力丧失的主要原因，通过自动视网膜图像分析进行早期诊断可以显著降低失明的风险。

Method: 利用迁移学习和广泛的数据增强，来解决类别不平衡和有限的训练数据带来的挑战。评估了一系列预训练的卷积神经网络架构，包括ResNet和EfficientNet的变体，在APTOS 2019数据集上进行。

Result: 对于二元分类，所提出的模型达到了98.9%的最先进的准确率，精确率为98.6%，召回率为99.3%，F1分数为98.9%，AUC为99.4%。在更具挑战性的五类严重程度分类任务中，该模型获得了84.6%的竞争性准确率和94.1%的AUC，优于几种现有方法。研究结果还表明，EfficientNet-B0和ResNet34在两项任务中提供了准确性和计算效率之间的最佳权衡。

Conclusion: 结合类别平衡增强和迁移学习对于高性能DR诊断非常有效。该框架为DR筛查提供了一个可扩展且准确的解决方案，具有在实际临床环境中部署的潜力。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, and
early diagnosis through automated retinal image analysis can significantly
reduce the risk of blindness. This paper presents a robust deep learning
framework for both binary and five-class DR classification, leveraging transfer
learning and extensive data augmentation to address the challenges of class
imbalance and limited training data. We evaluate a range of pretrained
convolutional neural network architectures, including variants of ResNet and
EfficientNet, on the APTOS 2019 dataset.
  For binary classification, our proposed model achieves a state-of-the-art
accuracy of 98.9%, with a precision of 98.6%, recall of 99.3%, F1-score of
98.9%, and an AUC of 99.4%. In the more challenging five-class severity
classification task, our model obtains a competitive accuracy of 84.6% and an
AUC of 94.1%, outperforming several existing approaches. Our findings also
demonstrate that EfficientNet-B0 and ResNet34 offer optimal trade-offs between
accuracy and computational efficiency across both tasks.
  These results underscore the effectiveness of combining class-balanced
augmentation with transfer learning for high-performance DR diagnosis. The
proposed framework provides a scalable and accurate solution for DR screening,
with potential for deployment in real-world clinical environments.

</details>


### [58] [ScSAM: Debiasing Morphology and Distributional Variability in Subcellular Semantic Segmentation](https://arxiv.org/abs/2507.17149)
*Bo Fang,Jianan Fan,Dongnan Liu,Hang Chang,Gerald J. Shami,Filip Braet,Weidong Cai*

Main category: cs.CV

TL;DR: ScSAM通过融合预训练的SAM和MAE来增强特征的鲁棒性，并在不同的亚细胞图像数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 亚细胞成分在形态和分布上的显著变异性对基于学习的细胞器分割模型提出了长期挑战，显著增加了有偏见的特征学习的风险。现有的方法通常依赖于单一的映射关系，忽略了特征的多样性，从而导致有偏见的训练。SAM专注于全局上下文理解，通常忽略细粒度的空间细节，因此难以捕捉细微的结构变化和应对倾斜的数据分布。

Method: 融合预训练的SAM与MAE引导的细胞先验知识，缓解数据不平衡带来的训练偏差。设计了一个特征对齐和融合模块，将预训练的嵌入对齐到同一特征空间，并有效地结合不同的表示。提出了一个基于余弦相似度矩阵的类提示编码器来激活类特异性特征，以识别亚细胞类别。

Result: ScSAM提高了特征的鲁棒性。

Conclusion: ScSAM在不同的亚细胞图像数据集上优于现有方法。

Abstract: The significant morphological and distributional variability among
subcellular components poses a long-standing challenge for learning-based
organelle segmentation models, significantly increasing the risk of biased
feature learning. Existing methods often rely on single mapping relationships,
overlooking feature diversity and thereby inducing biased training. Although
the Segment Anything Model (SAM) provides rich feature representations, its
application to subcellular scenarios is hindered by two key challenges: (1) The
variability in subcellular morphology and distribution creates gaps in the
label space, leading the model to learn spurious or biased features. (2) SAM
focuses on global contextual understanding and often ignores fine-grained
spatial details, making it challenging to capture subtle structural alterations
and cope with skewed data distributions. To address these challenges, we
introduce ScSAM, a method that enhances feature robustness by fusing
pre-trained SAM with Masked Autoencoder (MAE)-guided cellular prior knowledge
to alleviate training bias from data imbalance. Specifically, we design a
feature alignment and fusion module to align pre-trained embeddings to the same
feature space and efficiently combine different representations. Moreover, we
present a cosine similarity matrix-based class prompt encoder to activate
class-specific features to recognize subcellular categories. Extensive
experiments on diverse subcellular image datasets demonstrate that ScSAM
outperforms state-of-the-art methods.

</details>


### [59] [UNICE: Training A Universal Image Contrast Enhancer](https://arxiv.org/abs/2507.17157)
*Ruodai Cui,Lei Zhang*

Main category: cs.CV

TL;DR: UNICE 是一种通用图像对比度增强器，它通过从单个 sRGB 图像生成多曝光序列 (MES) 并将其融合来增强图像。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以为各种对比度增强任务学习一种通用模型。现有图像对比度增强方法在不同任务中的泛化性能较差。

Method: 该方法训练一个网络从单个 sRGB 图像生成一个 MES，然后训练另一个网络将生成的 MES 融合为增强图像。

Result: UNICE 不需要昂贵的人工标注，并且在不同任务中表现出比现有图像对比度增强方法更强的泛化性能。

Conclusion: UNICE在不同任务中表现出比现有图像对比度增强方法更强的泛化性能，甚至在多个无参考图像质量指标中优于手动创建的 ground-truth。

Abstract: Existing image contrast enhancement methods are typically designed for
specific tasks such as under-/over-exposure correction, low-light and backlit
image enhancement, etc. The learned models, however, exhibit poor
generalization performance across different tasks, even across different
datasets of a specific task. It is important to explore whether we can learn a
universal and generalized model for various contrast enhancement tasks. In this
work, we observe that the common key factor of these tasks lies in the need of
exposure and contrast adjustment, which can be well-addressed if high-dynamic
range (HDR) inputs are available. We hence collect 46,928 HDR raw images from
public sources, and render 328,496 sRGB images to build multi-exposure
sequences (MES) and the corresponding pseudo sRGB ground-truths via
multi-exposure fusion. Consequently, we train a network to generate an MES from
a single sRGB image, followed by training another network to fuse the generated
MES into an enhanced image. Our proposed method, namely UNiversal Image
Contrast Enhancer (UNICE), is free of costly human labeling. However, it
demonstrates significantly stronger generalization performance than existing
image contrast enhancement methods across and within different tasks, even
outperforming manually created ground-truths in multiple no-reference image
quality metrics. The dataset, code and model are available at
https://github.com/BeyondHeaven/UNICE.

</details>


### [60] [DOOMGAN:High-Fidelity Dynamic Identity Obfuscation Ocular Generative Morphing](https://arxiv.org/abs/2507.17158)
*Bharath Krishnamurthy,Ajita Rattani*

Main category: cs.CV

TL;DR: This paper introduces DOOMGAN, a novel method for generating morphing attacks on visible-spectrum ocular biometrics. DOOMGAN outperforms existing methods and a new dataset is released.


<details>
  <summary>Details</summary>
Motivation: morphing attacks, synthetic biometric traits created by blending features from multiple individuals, threaten biometric system integrity. While extensively studied for near-infrared iris and face biometrics, morphing in visible-spectrum ocular data remains underexplored. Simulating such attacks demands advanced generation models that handle uncontrolled conditions while preserving detailed ocular features like iris boundaries and periocular textures

Method: landmark-driven encoding of visible ocular anatomy, attention-guided generation for realistic morph synthesis, and dynamic weighting of multi-faceted losses for optimized convergence

Result: DOOMGAN achieves over 20% higher attack success rates than baseline methods under stringent thresholds, along with 20% better elliptical iris structure generation and 30% improved gaze consistency

Conclusion: DOOMGAN achieves over 20% higher attack success rates than baseline methods under stringent thresholds, along with 20% better elliptical iris structure generation and 30% improved gaze consistency. We also release the first comprehensive ocular morphing dataset to support further research in this domain.

Abstract: Ocular biometrics in the visible spectrum have emerged as a prominent
modality due to their high accuracy, resistance to spoofing, and non-invasive
nature. However, morphing attacks, synthetic biometric traits created by
blending features from multiple individuals, threaten biometric system
integrity. While extensively studied for near-infrared iris and face
biometrics, morphing in visible-spectrum ocular data remains underexplored.
Simulating such attacks demands advanced generation models that handle
uncontrolled conditions while preserving detailed ocular features like iris
boundaries and periocular textures. To address this gap, we introduce DOOMGAN,
that encompasses landmark-driven encoding of visible ocular anatomy,
attention-guided generation for realistic morph synthesis, and dynamic
weighting of multi-faceted losses for optimized convergence. DOOMGAN achieves
over 20% higher attack success rates than baseline methods under stringent
thresholds, along with 20% better elliptical iris structure generation and 30%
improved gaze consistency. We also release the first comprehensive ocular
morphing dataset to support further research in this domain.

</details>


### [61] [Multi-Scale PCB Defect Detection with YOLOv8 Network Improved via Pruning and Lightweight Network](https://arxiv.org/abs/2507.17176)
*Li Pingzhen,Xu Sheng,Chen Jing,Su Chengyue*

Main category: cs.CV

TL;DR: This paper improves YOLOv8 for PCB defect detection using lightweighting and pruning techniques, achieving higher accuracy and speed compared to the original YOLOv8n.


<details>
  <summary>Details</summary>
Motivation: Traditional PCB defect detection models struggle to balance accuracy and computational cost, failing to meet the demands for high accuracy and real-time detection of tiny defects due to high PCB design density and production speed.

Method: The YOLOv8 model is improved using a comprehensive strategy of tiny target sensitivity, network lightweighting, and adaptive pruning. This involves optimizing the backbone network with a Ghost-HGNetv2 structure, integrating C2f-Faster in the neck section, designing a new GCDetect detection head, using an Inner-MPDIoU boundary loss function, and pruning the model with an optimized adaptive pruning rate.

Result: The improved model achieves a mAP0.5 of 99.32% and a mAP0.5:0.9 of 75.18% on a publicly available PCB defect dataset, which is 10.13% higher compared to YOLOv8n.

Conclusion: The improved YOLOv8 model demonstrates advantages in both accuracy and speed on a PCB defect dataset, achieving a mAP0.5 of 99.32% and mAP0.5:0.9 of 75.18%, a 10.13% improvement compared to YOLOv8n.

Abstract: With the high density of printed circuit board (PCB) design and the high
speed of production, the traditional PCB defect detection model is difficult to
take into account the accuracy and computational cost, and cannot meet the
requirements of high accuracy and real-time detection of tiny defects.
Therefore, in this paper, a multi-scale PCB defect detection method is improved
with YOLOv8 using a comprehensive strategy of tiny target sensitivity strategy,
network lightweighting and adaptive pruning, which is able to improve the
detection speed and accuracy by optimizing the backbone network, the neck
network and the detection head, the loss function and the adaptive pruning
rate. Firstly, a Ghost-HGNetv2 structure with fewer parameters is used in the
backbone network, and multilevel features are used to extract image semantic
features to discover accurate defects. Secondly, we integrate C2f-Faster with
small number of parameters in the neck section to enhance the ability of
multi-level feature fusion. Next, in the Head part, we design a new GCDetect
detection head, which allows the prediction of bounding boxes and categories to
share the weights of GroupConv, and uses a small number of grouping
convolutions to accomplish the regression and classification tasks, which
significantly reduces the number of parameters while maintaining the accuracy
of detection. We also design the Inner-MPDIoU boundary loss function to improve
the detection and localization of tiny targets. Finally, the model was pruned
by an optimized adaptive pruning rate to further reduce the complexity of the
model. Experimental results show that the model exhibits advantages in terms of
accuracy and speed. On the publicly available PCB defect dataset, mAP0.5
reaches 99.32% and mAP0.5:0.9 reaches 75.18%, which is 10.13% higher compared
to YOLOv8n.

</details>


### [62] [Hierarchical Fusion and Joint Aggregation: A Multi-Level Feature Representation Method for AIGC Image Quality Assessment](https://arxiv.org/abs/2507.17182)
*Linghe Meng,Jiarun Song*

Main category: cs.CV

TL;DR: This paper proposes a multi-level visual representation paradigm to address the limitations of single-level visual features in assessing the quality of AI-generated content (AIGC).


<details>
  <summary>Details</summary>
Motivation: Existing methods generally rely on single-level visual features, limiting their ability to capture complex distortions in AIGC images. To address this limitation

Method: a multi-level visual representation paradigm is proposed with three stages, namely multi-level feature extraction, hierarchical fusion, and joint aggregation. Based on this paradigm, two networks are developed. Specifically, the Multi-Level Global-Local Fusion Network (MGLF-Net) is designed for the perceptual quality assessment, extracting complementary local and global features via dual CNN and Transformer visual backbones. The Multi-Level Prompt-Embedded Fusion Network (MPEF-Net) targets Text-to-Image correspondence by embedding prompt semantics into the visual feature fusion process at each feature level.

Result: outstanding performance on both tasks

Conclusion: Experiments on benchmarks demonstrate outstanding performance on both tasks, validating the effectiveness of the proposed multi-level visual assessment paradigm.

Abstract: The quality assessment of AI-generated content (AIGC) faces multi-dimensional
challenges, that span from low-level visual perception to high-level semantic
understanding. Existing methods generally rely on single-level visual features,
limiting their ability to capture complex distortions in AIGC images. To
address this limitation, a multi-level visual representation paradigm is
proposed with three stages, namely multi-level feature extraction, hierarchical
fusion, and joint aggregation. Based on this paradigm, two networks are
developed. Specifically, the Multi-Level Global-Local Fusion Network (MGLF-Net)
is designed for the perceptual quality assessment, extracting complementary
local and global features via dual CNN and Transformer visual backbones. The
Multi-Level Prompt-Embedded Fusion Network (MPEF-Net) targets Text-to-Image
correspondence by embedding prompt semantics into the visual feature fusion
process at each feature level. The fused multi-level features are then
aggregated for final evaluation. Experiments on benchmarks demonstrate
outstanding performance on both tasks, validating the effectiveness of the
proposed multi-level visual assessment paradigm.

</details>


### [63] [Asymmetric Lesion Detection with Geometric Patterns and CNN-SVM Classification](https://arxiv.org/abs/2507.17185)
*M. A. Rasel,Sameem Abdul Kareem,Zhenli Kwan,Nik Aimee Azizah Faheem,Winn Hui Han,Rebecca Kai Jan Choong,Shin Shen Yong,Unaizah Obaidellah*

Main category: cs.CV

TL;DR: 使用监督学习图像处理算法分析病变形状，使用卷积神经网络提取特征，训练支持向量机分类器，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 病变形状提供了对皮肤疾病的重要见解。在临床实践的方法中，不对称病变形状是诊断黑色素瘤的标准之一。

Method: 提出了一种支持技术，即监督学习图像处理算法，以分析病变形状的几何图案。

Result: 几何实验检测率99%，CNN实验Kappa Score 94%, Macro F1-score 95%, Weighted F1-score 97%。

Conclusion: 使用卷积神经网络提取形状、颜色和纹理特征，并训练多类支持向量机分类器，优于现有方法。在基于几何的实验中，皮肤不对称病变的检测率达到 99.00%。在基于 CNN 的实验中，分类病变形状（不对称、半对称和对称）的最佳性能为 94% Kappa Score、95% Macro F1-score 和 97% Weighted F1-score。

Abstract: In dermoscopic images, which allow visualization of surface skin structures
not visible to the naked eye, lesion shape offers vital insights into skin
diseases. In clinically practiced methods, asymmetric lesion shape is one of
the criteria for diagnosing melanoma. Initially, we labeled data for a
non-annotated dataset with symmetrical information based on clinical
assessments. Subsequently, we propose a supporting technique, a supervised
learning image processing algorithm, to analyze the geometrical pattern of
lesion shape, aiding non-experts in understanding the criteria of an asymmetric
lesion. We then utilize a pre-trained convolutional neural network (CNN) to
extract shape, color, and texture features from dermoscopic images for training
a multiclass support vector machine (SVM) classifier, outperforming
state-of-the-art methods from the literature. In the geometry-based experiment,
we achieved a 99.00% detection rate for dermatological asymmetric lesions. In
the CNN-based experiment, the best performance is found with 94% Kappa Score,
95% Macro F1-score, and 97% Weighted F1-score for classifying lesion shapes
(Asymmetric, Half-Symmetric, and Symmetric).

</details>


### [64] [Vec2Face+ for Face Dataset Generation](https://arxiv.org/abs/2507.17192)
*Haiyu Wu,Jaskirat Singh,Sicong Tian,Liang Zheng,Kevin W. Bowyer*

Main category: cs.CV

TL;DR: Vec2Face+ is proposed to generate high-quality face training data with proper inter-class separability, intra-class variation and identity consistency. A synthetic face dataset is created and achieves state-of-the-art accuracy on real-world test sets.


<details>
  <summary>Details</summary>
Motivation: Large inter-class separability and intra-class attribute variation are essential for synthesizing a quality dataset. However, when increasing intra-class variation, existing methods overlook the necessity of maintaining intra-class identity consistency.

Method: A generative model Vec2Face+ is proposed, which creates images directly from image features and allows for continuous and easy control of face identities and attributes. Datasets with proper inter-class separability, intra-class variation and identity consistency are obtained by sampling vectors, using AttrOP algorithm and LoRA-based pose control.

Result: VFace10K allows an FR model to achieve state-of-the-art accuracy on seven real-world test sets. VFace100K and VFace300K yield higher accuracy than the real-world training dataset, CASIA-WebFace, on five real-world test sets. Models trained with synthetic identities are more biased than those trained with real identities.

Conclusion: A synthetic face dataset (VFace10K, VFace100K and VFace300K) is created, which allows an FR model to achieve state-of-the-art accuracy on real-world test sets. Models trained with synthetic identities are more biased than those trained with real identities, which requires future investigation.

Abstract: When synthesizing identities as face recognition training data, it is
generally believed that large inter-class separability and intra-class
attribute variation are essential for synthesizing a quality dataset. % This
belief is generally correct, and this is what we aim for. However, when
increasing intra-class variation, existing methods overlook the necessity of
maintaining intra-class identity consistency. % To address this and generate
high-quality face training data, we propose Vec2Face+, a generative model that
creates images directly from image features and allows for continuous and easy
control of face identities and attributes. Using Vec2Face+, we obtain datasets
with proper inter-class separability and intra-class variation and identity
consistency using three strategies: 1) we sample vectors sufficiently different
from others to generate well-separated identities; 2) we propose an AttrOP
algorithm for increasing general attribute variations; 3) we propose LoRA-based
pose control for generating images with profile head poses, which is more
efficient and identity-preserving than AttrOP. % Our system generates VFace10K,
a synthetic face dataset with 10K identities, which allows an FR model to
achieve state-of-the-art accuracy on seven real-world test sets. Scaling the
size to 4M and 12M images, the corresponding VFace100K and VFace300K datasets
yield higher accuracy than the real-world training dataset, CASIA-WebFace, on
five real-world test sets. This is the first time a synthetic dataset beats the
CASIA-WebFace in average accuracy. In addition, we find that only 1 out of 11
synthetic datasets outperforms random guessing (\emph{i.e., 50\%}) in twin
verification and that models trained with synthetic identities are more biased
than those trained with real identities. Both are important aspects for future
investigation.

</details>


### [65] [DesignLab: Designing Slides Through Iterative Detection and Correction](https://arxiv.org/abs/2507.17202)
*Jooyeol Yun,Heng Wang,Yotaro Shimose,Jaegul Choo,Shingo Takamatsu*

Main category: cs.CV

TL;DR: DesignLab将设计过程分解为审查和贡献两个角色，通过迭代优化幻灯片设计，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 非专业人士设计高质量演示幻灯片具有挑战性，现有自动化工具缺乏改进自身输出的能力。

Method: 将设计过程分解为设计审查员和设计贡献者两个角色，通过迭代循环不断改进幻灯片设计。

Result: DesignLab优于现有设计生成方法，包括商业工具。

Conclusion: DesignLab通过迭代优化设计，超越了现有设计生成方法，包括商业工具，能够生成更精致、专业的幻灯片。

Abstract: Designing high-quality presentation slides can be challenging for non-experts
due to the complexity involved in navigating various design choices. Numerous
automated tools can suggest layouts and color schemes, yet often lack the
ability to refine their own output, which is a key aspect in real-world
workflows. We propose DesignLab, which separates the design process into two
roles, the design reviewer, who identifies design-related issues, and the
design contributor who corrects them. This decomposition enables an iterative
loop where the reviewer continuously detects issues and the contributor
corrects them, allowing a draft to be further polished with each iteration,
reaching qualities that were unattainable. We fine-tune large language models
for these roles and simulate intermediate drafts by introducing controlled
perturbations, enabling the design reviewer learn design errors and the
contributor learn how to fix them. Our experiments show that DesignLab
outperforms existing design-generation methods, including a commercial tool, by
embracing the iterative nature of designing which can result in polished,
professional slides.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [66] [Towards Autonomous Sustainability Assessment via Multimodal AI Agents](https://arxiv.org/abs/2507.17012)
*Zhihan Zhang,Alexander Metzger,Yuxuan Mei,Felix Hähnlein,Zachary Englhardt,Tingyu Cheng,Gregory D. Abowd,Shwetak Patel,Adriana Schulz,Vikram Iyer*

Main category: cs.AI

TL;DR: 我们使用多模态 AI 代理来模拟 LCA 专家与利益相关者之间的交互，以计算电子设备的碳排放量。


<details>
  <summary>Details</summary>
Motivation: 近年来，人们对可持续性信息的兴趣猛增。然而，将产品制造到处置的环境影响 (EI) 的材料和流程映射到的生命周期评估 (LCA) 所需的数据通常不可用。

Method: 我们引入了多模态 AI 代理，这些代理模拟 LCA 专家与产品经理和工程师等利益相关者之间的交互，以计算电子设备的从摇篮到大门（生产）的碳排放。

Result: 该方法将专家的时间从数周或数月减少到一分钟以下，并缩小了数据可用性差距，同时在零专有数据的情况下，产生的碳足迹估算值在专家 LCA 的 19% 以内。此外，我们开发了一种通过将输入与具有相似描述和已知碳足迹的产品集群进行比较来直接估计 EI 的方法。这在笔记本电脑上以 3 毫秒的速度运行，电子产品的 MAPE 为 12.28%。此外，我们开发了一种数据驱动的方法来生成排放因子。我们使用未知材料的属性将其表示为相似材料的排放因子的加权总和。与人类专家选择最接近的 LCA 数据库条目相比，这提高了 MAPE 120.26%。

Conclusion: 我们分析了数据并计算了这种方法的扩展，并讨论了其对未来 LCA 工作流程的影响。

Abstract: Interest in sustainability information has surged in recent years. However,
the data required for a life cycle assessment (LCA) that maps the materials and
processes from product manufacturing to disposal into environmental impacts
(EI) are often unavailable. Here we reimagine conventional LCA by introducing
multimodal AI agents that emulate interactions between LCA experts and
stakeholders like product managers and engineers to calculate the
cradle-to-gate (production) carbon emissions of electronic devices. The AI
agents iteratively generate a detailed life-cycle inventory leveraging a custom
data abstraction and software tools that extract information from online text
and images from repair communities and government certifications. This approach
reduces weeks or months of expert time to under one minute and closes data
availability gaps while yielding carbon footprint estimates within 19% of
expert LCAs with zero proprietary data. Additionally, we develop a method to
directly estimate EI by comparing an input to a cluster of products with
similar descriptions and known carbon footprints. This runs in 3 ms on a laptop
with a MAPE of 12.28% on electronic products. Further, we develop a data-driven
method to generate emission factors. We use the properties of an unknown
material to represent it as a weighted sum of emission factors for similar
materials. Compared to human experts picking the closest LCA database entry,
this improves MAPE by 120.26%. We analyze the data and compute scaling of this
approach and discuss its implications for future LCA workflows.

</details>


### [67] [CQE under Epistemic Dependencies: Algorithms and Experiments (extended version)](https://arxiv.org/abs/2507.17487)
*Lorenzo Marconi,Flavia Ricci,Riccardo Rosati*

Main category: cs.AI

TL;DR: This paper focuses on answering Boolean unions of conjunctive queries (BUCQs) with respect to the intersection of all optimal GA censors


<details>
  <summary>Details</summary>
Motivation: investigate Controlled Query Evaluation (CQE) over ontologies, where information disclosure is regulated by epistemic dependencies (EDs)

Method: combine EDs with the notion of optimal GA censors

Result: characterize the security of this intersection-based approach and identify a class of EDs (namely, full EDs) for which it remains safe

Conclusion: answering BUCQs in the above CQE semantics is in AC^0 in data complexity by presenting a suitable, detailed first-order rewriting algorithm and experiments conducted in two different evaluation scenarios, showing the practical feasibility of our rewriting function.

Abstract: We investigate Controlled Query Evaluation (CQE) over ontologies, where
information disclosure is regulated by epistemic dependencies (EDs), a family
of logical rules recently proposed for the CQE framework. In particular, we
combine EDs with the notion of optimal GA censors, i.e. maximal sets of ground
atoms that are entailed by the ontology and can be safely revealed. We focus on
answering Boolean unions of conjunctive queries (BUCQs) with respect to the
intersection of all optimal GA censors - an approach that has been shown in
other contexts to ensure strong security guarantees with favorable
computational behavior. First, we characterize the security of this
intersection-based approach and identify a class of EDs (namely, full EDs) for
which it remains safe. Then, for a subclass of EDs and for DL-Lite_R
ontologies, we show that answering BUCQs in the above CQE semantics is in AC^0
in data complexity by presenting a suitable, detailed first-order rewriting
algorithm. Finally, we report on experiments conducted in two different
evaluation scenarios, showing the practical feasibility of our rewriting
function.

</details>


### [68] [New Mechanisms in Flex Distribution for Bounded Suboptimal Multi-Agent Path Finding](https://arxiv.org/abs/2507.17054)
*Shao-Hung Chan,Thomy Phan,Jiaoyang Li,Sven Koenig*

Main category: cs.AI

TL;DR: This paper proposes new flex distribution mechanisms for EECBS to improve its efficiency and proves their completeness and bounded-suboptimality. Experiments show that the proposed approaches outperform the original flex distribution.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the issue of reduced efficiency in Explicit Estimation Conflict-Based Search (EECBS) caused by increasing thresholds that may push the SOC beyond  w  * LB, forcing EECBS to switch among different sets of paths instead of resolving collisions on a particular set of paths.

Method: The paper proposes Conflict-Based Flex Distribution that distributes flex in proportion to the number of collisions. It also estimates the delays needed to satisfy constraints and proposes Delay-Based Flex Distribution. On top of that, it proposes Mixed-Strategy Flex Distribution, combining both in a hierarchical framework.

Result: The experiments show that the proposed approaches outperform the original (greedy) flex distribution.

Conclusion: The paper introduces Conflict-Based Flex Distribution and Delay-Based Flex Distribution to address the issue of reduced efficiency in EECBS due to switching among different sets of paths. It also proposes Mixed-Strategy Flex Distribution, combining both in a hierarchical framework. The paper proves that EECBS with these new flex distribution mechanisms is complete and bounded-suboptimal, and the experiments show that the proposed approaches outperform the original (greedy) flex distribution.

Abstract: Multi-Agent Path Finding (MAPF) is the problem of finding a set of
collision-free paths, one for each agent in a shared environment. Its objective
is to minimize the sum of path costs (SOC), where the path cost of each agent
is defined as the travel time from its start location to its target location.
Explicit Estimation Conflict-Based Search (EECBS) is the leading algorithm for
bounded-suboptimal MAPF, with the SOC of the solution being at most a
user-specified factor $w$ away from optimal. EECBS maintains sets of paths and
a lower bound $LB$ on the optimal SOC. Then, it iteratively selects a set of
paths whose SOC is at most $w \cdot LB$ and introduces constraints to resolve
collisions. For each path in a set, EECBS maintains a lower bound on its
optimal path that satisfies constraints. By finding an individually
bounded-suboptimal path with cost at most a threshold of $w$ times its lower
bound, EECBS guarantees to find a bounded-suboptimal solution. To speed up
EECBS, previous work uses flex distribution to increase the threshold. Though
EECBS with flex distribution guarantees to find a bounded-suboptimal solution,
increasing the thresholds may push the SOC beyond $w \cdot LB$, forcing EECBS
to switch among different sets of paths instead of resolving collisions on a
particular set of paths, and thus reducing efficiency. To address this issue,
we propose Conflict-Based Flex Distribution that distributes flex in proportion
to the number of collisions. We also estimate the delays needed to satisfy
constraints and propose Delay-Based Flex Distribution. On top of that, we
propose Mixed-Strategy Flex Distribution, combining both in a hierarchical
framework. We prove that EECBS with our new flex distribution mechanisms is
complete and bounded-suboptimal. Our experiments show that our approaches
outperform the original (greedy) flex distribution.

</details>


### [69] [LoRA is All You Need for Safety Alignment of Reasoning LLMs](https://arxiv.org/abs/2507.17075)
*Yihao Xue,Baharan Mirzasoleiman*

Main category: cs.AI

TL;DR: 使用LoRA进行安全对齐微调可以在不牺牲推理能力的情况下提高LLM的安全性。


<details>
  <summary>Details</summary>
Motivation: 安全对齐微调会显著降低推理能力，即“安全税”。

Method: 使用LoRA对拒绝数据集进行SFT，限制安全权重更新到低秩空间。

Result: 实验表明，该方法产生了高度安全的LLM，其安全级别与完整模型微调相当，且不影响其推理能力。LoRA引起的权重更新与初始权重的重叠小于完整模型微调。减少这种重叠的方法在某些任务上有所改进。

Conclusion: 使用LoRA进行SFT可以有效对齐模型的安全性，且不损害其推理能力。通过限制安全权重更新到低秩空间，最小化了与推理权重的干扰。

Abstract: Reasoning LLMs have demonstrated remarkable breakthroughs in solving complex
problems that were previously out of reach. To ensure LLMs do not assist with
harmful requests, safety alignment fine-tuning is necessary in the
post-training phase. However, safety alignment fine-tuning has recently been
shown to significantly degrade reasoning abilities, a phenomenon known as the
"Safety Tax". In this work, we show that using LoRA for SFT on refusal datasets
effectively aligns the model for safety without harming its reasoning
capabilities. This is because restricting the safety weight updates to a
low-rank space minimizes the interference with the reasoning weights. Our
extensive experiments across four benchmarks covering math, science, and coding
show that this approach produces highly safe LLMs -- with safety levels
comparable to full-model fine-tuning -- without compromising their reasoning
abilities. Additionally, we observe that LoRA induces weight updates with
smaller overlap with the initial weights compared to full-model fine-tuning. We
also explore methods that further reduce such overlap -- via regularization or
during weight merging -- and observe some improvement on certain tasks. We hope
this result motivates designing approaches that yield more consistent
improvements in the reasoning-safety trade-off.

</details>


### [70] [HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI Systems: A Case Study](https://arxiv.org/abs/2507.17118)
*Mandar Pitale,Jelena Frtunikj,Abhinaw Priyadershi,Vasu Singh,Maria Spence*

Main category: cs.AI

TL;DR: This paper reviews architectural solutions for AI safety in autonomous systems, evaluates safety analysis techniques, introduces HySAFE-AI, and suggests future AI safety standards.


<details>
  <summary>Details</summary>
Motivation: AI has become integral to safety-critical areas like autonomous driving systems (ADS) and robotics. The architecture of recent autonomous systems are trending toward end-to-end (E2E) monolithic architectures such as large language models (LLMs) and vision language models (VLMs).

Method: The paper reviews different architectural solutions and then evaluate the efficacy of common safety analyses such as failure modes and effect analysis (FMEA) and fault tree analysis (FTA).

Result: The paper shows how FMEA and FTA techniques can be improved for the intricate nature of the foundational models, particularly in how they form and utilize latent representations.

Conclusion: This paper introduces HySAFE-AI, a hybrid framework that adapts traditional methods to evaluate the safety of AI systems. It also offers hints of future work and suggestions to guide the evolution of future AI safety standards.

Abstract: AI has become integral to safety-critical areas like autonomous driving
systems (ADS) and robotics. The architecture of recent autonomous systems are
trending toward end-to-end (E2E) monolithic architectures such as large
language models (LLMs) and vision language models (VLMs). In this paper, we
review different architectural solutions and then evaluate the efficacy of
common safety analyses such as failure modes and effect analysis (FMEA) and
fault tree analysis (FTA). We show how these techniques can be improved for the
intricate nature of the foundational models, particularly in how they form and
utilize latent representations. We introduce HySAFE-AI, Hybrid Safety
Architectural Analysis Framework for AI Systems, a hybrid framework that adapts
traditional methods to evaluate the safety of AI systems. Lastly, we offer
hints of future work and suggestions to guide the evolution of future AI safety
standards.

</details>


### [71] [Improving LLMs' Generalized Reasoning Abilities by Graph Problems](https://arxiv.org/abs/2507.17168)
*Qifan Zhang,Nuo Chen,Zehua Li,Miao Peng,Jing Tang,Jia Li*

Main category: cs.AI

TL;DR: introduce GraphPile, the first large-scale corpus specifically designed for CPT using GPR data to enhance the general reasoning capabilities of LLMs


<details>
  <summary>Details</summary>
Motivation: LLMs performance often falters on novel and complex problems. Domain-specific continued pretraining (CPT) methods lack transferability to broader reasoning tasks.

Method: pioneer the use of Graph Problem Reasoning (GPR) to enhance the general reasoning capabilities of LLMs

Result: train GraphMind on popular base models Llama 3 and 3.1, as well as Gemma 2, achieving up to 4.9 percent higher accuracy in mathematical reasoning and up to 21.2 percent improvement in non-mathematical reasoning tasks

Conclusion: This work bridges the gap between domain-specific pretraining and universal reasoning capabilities, advancing the adaptability and robustness of LLMs.

Abstract: Large Language Models (LLMs) have made remarkable strides in reasoning tasks,
yet their performance often falters on novel and complex problems.
Domain-specific continued pretraining (CPT) methods, such as those tailored for
mathematical reasoning, have shown promise but lack transferability to broader
reasoning tasks. In this work, we pioneer the use of Graph Problem Reasoning
(GPR) to enhance the general reasoning capabilities of LLMs. GPR tasks,
spanning pathfinding, network analysis, numerical computation, and topological
reasoning, require sophisticated logical and relational reasoning, making them
ideal for teaching diverse reasoning patterns. To achieve this, we introduce
GraphPile, the first large-scale corpus specifically designed for CPT using GPR
data. Spanning 10.9 billion tokens across 23 graph tasks, the dataset includes
chain-of-thought, program-of-thought, trace of execution, and real-world graph
data. Using GraphPile, we train GraphMind on popular base models Llama 3 and
3.1, as well as Gemma 2, achieving up to 4.9 percent higher accuracy in
mathematical reasoning and up to 21.2 percent improvement in non-mathematical
reasoning tasks such as logical and commonsense reasoning. By being the first
to harness GPR for enhancing reasoning patterns and introducing the first
dataset of its kind, our work bridges the gap between domain-specific
pretraining and universal reasoning capabilities, advancing the adaptability
and robustness of LLMs.

</details>


### [72] [Our Cars Can Talk: How IoT Brings AI to Vehicles](https://arxiv.org/abs/2507.17214)
*Amod Kant Agrawal*

Main category: cs.AI

TL;DR: 本文提出了一个关于智能车辆、预测性维护和人工智能用户交互的概念和技术视角，旨在促进跨学科对话和指导未来发展。


<details>
  <summary>Details</summary>
Motivation: 将人工智能引入车辆并使其成为传感平台是实现维护从被动到主动转变的关键。现在是将人工智能副驾驶集成到车辆中的时候了，它可以同时理解机器和驾驶员的语言。

Method: 提出了一种概念和技术视角。

Result: 不适用

Conclusion: 本文旨在促进智能车辆系统、预测性维护和人工智能用户交互领域的跨学科对话，并指导未来的研究和开发。

Abstract: Bringing AI to vehicles and enabling them as sensing platforms is key to
transforming maintenance from reactive to proactive. Now is the time to
integrate AI copilots that speak both languages: machine and driver. This
article offers a conceptual and technical perspective intended to spark
interdisciplinary dialogue and guide future research and development in
intelligent vehicle systems, predictive maintenance, and AI-powered user
interaction.

</details>


### [73] [Agent Identity Evals: Measuring Agentic Identity](https://arxiv.org/abs/2507.17257)
*Elija Perrier,Michael Timothy Bennett*

Main category: cs.AI

TL;DR: This paper introduces a framework called agent identity evals (AIE) to measure and maintain the agentic identity of language model agents (LMAs) over time.


<details>
  <summary>Details</summary>
Motivation: The identifiability, continuity, persistence and consistency of LMAs can be undermined by pathologies inherited from LLMs, which can erode their reliability, trustworthiness and utility.

Method: This paper sets out formal definitions and methods that can be applied at each stage of the LMA life-cycle.

Result: AIE comprises a set of novel metrics which can integrate with other measures of performance, capability and agentic robustness to assist in the design of optimal LMA infrastructure and scaffolding such as memory and tools.

Conclusion: This paper introduces agent identity evals (AIE), a framework for measuring the degree to which an LMA system exhibit and maintain their agentic identity over time.

Abstract: Central to agentic capability and trustworthiness of language model agents
(LMAs) is the extent they maintain stable, reliable, identity over time.
However, LMAs inherit pathologies from large language models (LLMs)
(statelessness, stochasticity, sensitivity to prompts and
linguistically-intermediation) which can undermine their identifiability,
continuity, persistence and consistency. This attrition of identity can erode
their reliability, trustworthiness and utility by interfering with their
agentic capabilities such as reasoning, planning and action. To address these
challenges, we introduce \textit{agent identity evals} (AIE), a rigorous,
statistically-driven, empirical framework for measuring the degree to which an
LMA system exhibit and maintain their agentic identity over time, including
their capabilities, properties and ability to recover from state perturbations.
AIE comprises a set of novel metrics which can integrate with other measures of
performance, capability and agentic robustness to assist in the design of
optimal LMA infrastructure and scaffolding such as memory and tools. We set out
formal definitions and methods that can be applied at each stage of the LMA
life-cycle, and worked examples of how to apply them.

</details>


### [74] [Students' Feedback Requests and Interactions with the SCRIPT Chatbot: Do They Get What They Ask For?](https://arxiv.org/abs/2507.17258)
*Andreas Scholl,Natalie Kiesler*

Main category: cs.AI

TL;DR: Developed and evaluated SCRIPT, a ChatGPT-4o-mini chatbot for novice programming learners, finding that feedback requests follow a sequence and the chatbot aligns well with requested feedback types.


<details>
  <summary>Details</summary>
Motivation: Building on prior research on Generative AI (GenAI) and related tools for programming education, we developed SCRIPT, a chatbot based on ChatGPT-4o-mini, to support novice learners. SCRIPT allows for open-ended interactions and structured guidance through predefined prompts.

Method: We evaluated the tool via an experiment with 136 students from an introductory programming course at a large German university and analyzed how students interacted with SCRIPT while solving programming tasks with a focus on their feedback preferences.

Result: The results reveal that students' feedback requests seem to follow a specific sequence. Moreover, the chatbot responses aligned well with students' requested feedback types (in 75%), and it adhered to the system prompt constraints.

Conclusion: The study's insights inform the design of GenAI-based learning support systems and highlight challenges in balancing guidance and flexibility in AI-assisted tools.

Abstract: Building on prior research on Generative AI (GenAI) and related tools for
programming education, we developed SCRIPT, a chatbot based on ChatGPT-4o-mini,
to support novice learners. SCRIPT allows for open-ended interactions and
structured guidance through predefined prompts. We evaluated the tool via an
experiment with 136 students from an introductory programming course at a large
German university and analyzed how students interacted with SCRIPT while
solving programming tasks with a focus on their feedback preferences. The
results reveal that students' feedback requests seem to follow a specific
sequence. Moreover, the chatbot responses aligned well with students' requested
feedback types (in 75%), and it adhered to the system prompt constraints. These
insights inform the design of GenAI-based learning support systems and
highlight challenges in balancing guidance and flexibility in AI-assisted
tools.

</details>


### [75] [Compliance Brain Assistant: Conversational Agentic AI for Assisting Compliance Tasks in Enterprise Environments](https://arxiv.org/abs/2507.17289)
*Shitong Zhu,Chenhao Fang,Derek Larson,Neel Reddy Pochareddy,Rajeev Rao,Sophie Zeng,Yanqing Peng,Wendy Summer,Alex Goncalves,Arya Pudota,Herve Robert*

Main category: cs.AI

TL;DR: CBA通过智能路由在响应质量和延迟之间取得平衡，显著提高了企业合规任务的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 旨在提高企业环境中人员日常合规任务的效率。

Method: 设计了一个用户查询路由器，可以在快速通道模式和完全代理模式之间智能选择，以平衡响应质量和延迟。

Result: CBA在平均关键词匹配率（83.7% vs. 41.7%）和LLM-judge通过率（82.0% vs. 20.0%）上显著优于原始LLM。

Conclusion: CBA显著提升了合规任务的性能，并在平均关键词匹配率和LLM-judge通过率上优于原始LLM。路由机制在两种模式之间取得了良好的平衡。

Abstract: This paper presents Compliance Brain Assistant (CBA), a conversational,
agentic AI assistant designed to boost the efficiency of daily compliance tasks
for personnel in enterprise environments. To strike a good balance between
response quality and latency, we design a user query router that can
intelligently choose between (i) FastTrack mode: to handle simple requests that
only need additional relevant context retrieved from knowledge corpora; and
(ii) FullAgentic mode: to handle complicated requests that need composite
actions and tool invocations to proactively discover context across various
compliance artifacts, and/or involving other APIs/models for accommodating
requests. A typical example would be to start with a user query, use its
description to find a specific entity and then use the entity's information to
query other APIs for curating and enriching the final AI response.
  Our experimental evaluations compared CBA against an out-of-the-box LLM on
various real-world privacy/compliance-related queries targeting various
personas. We found that CBA substantially improved upon the vanilla LLM's
performance on metrics such as average keyword match rate (83.7% vs. 41.7%) and
LLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full
routing-based design against the `fast-track only` and `full-agentic` modes and
found that it had a better average match-rate and pass-rate while keeping the
run-time approximately the same. This finding validated our hypothesis that the
routing mechanism leads to a good trade-off between the two worlds.

</details>


### [76] [Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using Generative Adversarial Imitation Learning](https://arxiv.org/abs/2507.17418)
*Joobin Jin,Seokjun Hong,Gyeongseon Baek,Yeeun Kim,Byeongjoon Noh*

Main category: cs.AI

TL;DR: Ctx2TrajGen是一种上下文感知的轨迹生成框架，它使用GAIL合成逼真的城市驾驶行为，并在DRIFT数据集上表现出优异的性能。


<details>
  <summary>Details</summary>
Motivation: 精确建模微观车辆轨迹对于交通行为分析和自动驾驶系统至关重要。

Method: 使用GAIL合成逼真的城市驾驶行为，利用PPO和WGAN-GP解决非线性相互依赖和微观环境中的训练不稳定性，并显式地以周围车辆和道路几何形状为条件。

Result: Ctx2TrajGen生成与真实世界环境对齐的、具有交互意识的轨迹。

Conclusion: Ctx2TrajGen在DRIFT数据集上表现优于现有方法，在真实性、行为多样性和上下文保真度方面均有提升，为数据稀缺和领域转移提供了一个强大的解决方案，无需模拟。

Abstract: Precise modeling of microscopic vehicle trajectories is critical for traffic
behavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a
context-aware trajectory generation framework that synthesizes realistic urban
driving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses
nonlinear interdependencies and training instability inherent in microscopic
settings. By explicitly conditioning on surrounding vehicles and road geometry,
Ctx2TrajGen generates interaction-aware trajectories aligned with real-world
context. Experiments on the drone-captured DRIFT dataset demonstrate superior
performance over existing methods in terms of realism, behavioral diversity,
and contextual fidelity, offering a robust solution to data scarcity and domain
shift without simulation.

</details>


### [77] [An Uncertainty-Driven Adaptive Self-Alignment Framework for Large Language Models](https://arxiv.org/abs/2507.17477)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.AI

TL;DR: UDASA通过量化输出不确定性并分阶段优化模型，从而在没有人为干预的情况下提高LLM的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 在没有人为注释的情况下，实现与人类意图和安全规范的高质量对齐仍然是一个根本性的挑战。

Method: 提出了一种不确定性驱动的自适应自对齐（UDASA）框架，旨在以全自动的方式改进LLM对齐。

Result: 实验结果表明，UDASA在多个任务中优于现有的对齐方法，包括无害性、有用性、真实性和受控情绪生成，显著提高了模型性能。

Conclusion: UDASA在多个任务中优于现有的对齐方法，包括无害性、有用性、真实性和受控情绪生成，显著提高了模型性能。

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
instruction following and general-purpose reasoning. However, achieving
high-quality alignment with human intent and safety norms without human
annotations remains a fundamental challenge. In this work, we propose an
Uncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to
improve LLM alignment in a fully automated manner. UDASA first generates
multiple responses for each input and quantifies output uncertainty across
three dimensions: semantics, factuality, and value alignment. Based on these
uncertainty scores, the framework constructs preference pairs and categorizes
training samples into three stages, conservative, moderate, and exploratory,
according to their uncertainty difference. The model is then optimized
progressively across these stages. In addition, we conduct a series of
preliminary studies to validate the core design assumptions and provide strong
empirical motivation for the proposed framework. Experimental results show that
UDASA outperforms existing alignment methods across multiple tasks, including
harmlessness, helpfulness, truthfulness, and controlled sentiment generation,
significantly improving model performance.

</details>


### [78] [LTLZinc: a Benchmarking Framework for Continual Learning and Neuro-Symbolic Temporal Reasoning](https://arxiv.org/abs/2507.17482)
*Luca Salvatore Lorello,Nikolaos Manginas,Marco Lippi,Stefano Melacci*

Main category: cs.AI

TL;DR: LTLZinc 是一个基准框架，可以根据它评估神经符号和持续学习方法。


<details>
  <summary>Details</summary>
Motivation: 大多数现有的神经符号人工智能方法仅应用于静态场景，而很少探索沿时间维度进行推理的具有挑战性的环境。

Method: 我们引入了 LTLZinc，这是一个基准框架，可用于生成涵盖各种不同问题的数据集，神经符号和持续学习方法可以根据这些数据集沿时间和约束驱动的维度进行评估。

Result: 在 LTLZinc 生成的六个神经符号序列分类和四个类持续学习任务上的实验表明了时间学习和推理的挑战性，并强调了当前最先进方法的局限性。

Conclusion: LTLZinc 生成器和十个即用型任务已发布给神经符号和持续学习社区，希望能促进对统一的时间学习和推理框架的研究。

Abstract: Neuro-symbolic artificial intelligence aims to combine neural architectures
with symbolic approaches that can represent knowledge in a human-interpretable
formalism. Continual learning concerns with agents that expand their knowledge
over time, improving their skills while avoiding to forget previously learned
concepts. Most of the existing approaches for neuro-symbolic artificial
intelligence are applied to static scenarios only, and the challenging setting
where reasoning along the temporal dimension is necessary has been seldom
explored. In this work we introduce LTLZinc, a benchmarking framework that can
be used to generate datasets covering a variety of different problems, against
which neuro-symbolic and continual learning methods can be evaluated along the
temporal and constraint-driven dimensions. Our framework generates expressive
temporal reasoning and continual learning tasks from a linear temporal logic
specification over MiniZinc constraints, and arbitrary image classification
datasets. Fine-grained annotations allow multiple neural and neuro-symbolic
training settings on the same generated datasets. Experiments on six
neuro-symbolic sequence classification and four class-continual learning tasks
generated by LTLZinc, demonstrate the challenging nature of temporal learning
and reasoning, and highlight limitations of current state-of-the-art methods.
We release the LTLZinc generator and ten ready-to-use tasks to the
neuro-symbolic and continual learning communities, in the hope of fostering
research towards unified temporal learning and reasoning frameworks.

</details>


### [79] [Automated Hybrid Grounding Using Structural and Data-Driven Heuristics](https://arxiv.org/abs/2507.17493)
*Alexander Beiser,Markus Hecher,Stefan Woltran*

Main category: cs.AI

TL;DR: 本文提出了一种自动混合 grounding 方法，该方法使用数据结构启发式算法来确定何时使用 body-decoupled grounding 以及何时使用标准 bottom-up grounding。实验结果表明，该方法在难以 grounding 的场景中有所改进，而在难以解决的实例中，性能接近当前最优。


<details>
  <summary>Details</summary>
Motivation: The grounding bottleneck poses one of the key challenges that hinders the widespread adoption of Answer Set Programming in industry. Hybrid Grounding is a step in alleviating the bottleneck by combining the strength of standard bottom-up grounding with recently proposed techniques where rule bodies are decoupled during grounding. However, it has remained unclear when hybrid grounding shall use body-decoupled grounding and when to use standard bottom-up grounding.

Method: 我们引入了一种基于数据结构启发式的拆分算法，该算法可以检测何时使用 body-decoupled grounding，何时使用标准 grounding 有益。我们在规则结构和一个包含实例数据的估计程序的基础上构建了启发式算法。

Result: 在我们的原型实现上进行的实验表明，结果很有希望，这表明在难以 grounding 的场景中有所改进，而在难以解决的实例中，我们接近了最先进的性能。

Conclusion: 本文通过开发自动混合 grounding 来解决这个问题：我们引入了一种基于数据结构启发式的拆分算法，该算法可以检测何时使用 body-decoupled grounding，何时使用标准 grounding 有益。我们在规则结构和一个包含实例数据的估计程序的基础上构建了启发式算法。在我们的原型实现上进行的实验表明，结果很有希望，这表明在难以 grounding 的场景中有所改进，而在难以解决的实例中，我们接近了最先进的性能。

Abstract: The grounding bottleneck poses one of the key challenges that hinders the
widespread adoption of Answer Set Programming in industry. Hybrid Grounding is
a step in alleviating the bottleneck by combining the strength of standard
bottom-up grounding with recently proposed techniques where rule bodies are
decoupled during grounding. However, it has remained unclear when hybrid
grounding shall use body-decoupled grounding and when to use standard bottom-up
grounding. In this paper, we address this issue by developing automated hybrid
grounding: we introduce a splitting algorithm based on data-structural
heuristics that detects when to use body-decoupled grounding and when standard
grounding is beneficial. We base our heuristics on the structure of rules and
an estimation procedure that incorporates the data of the instance. The
experiments conducted on our prototypical implementation demonstrate promising
results, which show an improvement on hard-to-ground scenarios, whereas on
hard-to-solve instances we approach state-of-the-art performance.

</details>


### [80] [Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning](https://arxiv.org/abs/2507.17512)
*Yu Li,Zhuoshi Pan,Honglin Lin,Mengyuan Sun,Conghui He,Lijun Wu*

Main category: cs.AI

TL;DR: This paper investigates multi-domain reasoning in LLMs within the RLVR framework, focusing on mathematical reasoning, code generation, and logical puzzle solving. The study analyzes domain interactions, the influence of SFT on RL, and critical RL training details to improve multi-domain reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing research has concentrated on isolated reasoning domains, while real-world scenarios demand integrated application of multiple cognitive skills. The interplay among these reasoning skills under reinforcement learning remains poorly understood.

Method: Leveraging the GRPO algorithm and the Qwen-2.5-7B model family, the study evaluates models' in-domain improvements and cross-domain generalization capabilities. It also examines interactions during combined cross-domain training and compares performance differences between base and instruct models. The study explores the impacts of curriculum learning strategies, reward design variations, and language-specific factors.

Result: The results offer significant insights into the dynamics governing domain interactions, revealing key factors influencing both specialized and generalizable reasoning performance.

Conclusion: This study offers insights into domain interactions and key factors influencing specialized and generalizable reasoning performance, providing guidance for optimizing RL methodologies to foster comprehensive, multi-domain reasoning capabilities in LLMs.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing
research has predominantly concentrated on isolated reasoning domains such as
mathematical problem-solving, coding tasks, or logical reasoning. However, real
world reasoning scenarios inherently demand an integrated application of
multiple cognitive skills. Despite this, the interplay among these reasoning
skills under reinforcement learning remains poorly understood. To bridge this
gap, we present a systematic investigation of multi-domain reasoning within the
RLVR framework, explicitly focusing on three primary domains: mathematical
reasoning, code generation, and logical puzzle solving. We conduct a
comprehensive study comprising four key components: (1) Leveraging the GRPO
algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the
models' in-domain improvements and cross-domain generalization capabilities
when trained on single-domain datasets. (2) Additionally, we examine the
intricate interactions including mutual enhancements and conflicts that emerge
during combined cross-domain training. (3) To further understand the influence
of SFT on RL, we also analyze and compare performance differences between base
and instruct models under identical RL configurations. (4) Furthermore, we
delve into critical RL training details, systematically exploring the impacts
of curriculum learning strategies, variations in reward design, and
language-specific factors. Through extensive experiments, our results offer
significant insights into the dynamics governing domain interactions, revealing
key factors influencing both specialized and generalizable reasoning
performance. These findings provide valuable guidance for optimizing RL
methodologies to foster comprehensive, multi-domain reasoning capabilities in
LLMs.

</details>


### [81] [TAI Scan Tool: A RAG-Based Tool With Minimalistic Input for Trustworthy AI Self-Assessment](https://arxiv.org/abs/2507.17514)
*Athanasios Davvetas,Xenia Ziouvelou,Ypatia Dami,Alexis Kaponis,Konstantina Giouvanopoulou,Michael Papademas*

Main category: cs.AI

TL;DR: Introduces a RAG-based TAI self-assessment tool for AI Act compliance, showing promising results in predicting risk levels and retrieving relevant articles.


<details>
  <summary>Details</summary>
Motivation: Facilitating compliance with the AI Act.

Method: RAG-based TAI self-assessment tool with a two-step approach (pre-screening and assessment).

Result: Promising results in qualitative evaluation using use-case scenarios, correctly predicting risk levels and retrieving relevant articles across three distinct semantic groups. The tool's reasoning relies on comparison with the setting of high-risk systems.

Conclusion: The tool correctly predicts risk levels and retrieves relevant articles.

Abstract: This paper introduces the TAI Scan Tool, a RAG-based TAI self-assessment tool
with minimalistic input. The current version of the tool supports the legal TAI
assessment, with a particular emphasis on facilitating compliance with the AI
Act. It involves a two-step approach with a pre-screening and an assessment
phase. The assessment output of the system includes insight regarding the
risk-level of the AI system according to the AI Act, while at the same time
retrieving relevant articles to aid with compliance and notify on their
obligations. Our qualitative evaluation using use-case scenarios yields
promising results, correctly predicting risk levels while retrieving relevant
articles across three distinct semantic groups. Furthermore, interpretation of
results shows that the tool's reasoning relies on comparison with the setting
of high-risk systems, a behaviour attributed to their deployment requiring
careful consideration, and therefore frequently presented within the AI Act.

</details>


### [82] [Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning](https://arxiv.org/abs/2507.17539)
*Xinyao Liu,Diping Song*

Main category: cs.AI

TL;DR: The paper introduces FundusExpert, an ophthalmology-specific MLLM and a new dataset, FundusGen. FundusExpert shows improved performance in ophthalmic tasks by integrating positioning-diagnosis reasoning capabilities and using clinically aligned cognitive chains.


<details>
  <summary>Details</summary>
Motivation: MLLMs face challenges in ophthalmology due to fragmented annotation granularity and inconsistencies in clinical reasoning logic, hindering precise cross-modal understanding.

Method: The paper introduces FundusExpert, an ophthalmology-specific MLLM, and FundusGen, a dataset constructed through the intelligent Fundus-Engine system. A clinically aligned cognitive chain guides the model to generate interpretable reasoning paths.

Result: FundusExpert achieves the best performance in ophthalmic question-answering tasks, surpassing MedRegA by 26.6%, and excels in zero-shot report generation tasks, outperforming GPT-4o with a clinical consistency of 77.0%. The paper also reveals a scaling law between data quality and model capability.

Conclusion: This paper develops a scalable, clinically-aligned MLLM by integrating region-level localization with diagnostic reasoning chains, bridging the visual-language gap in specific MLLMs.

Abstract: Multimodal large language models (MLLMs) demonstrate significant potential in
the field of medical diagnosis. However, they face critical challenges in
specialized domains such as ophthalmology, particularly the fragmentation of
annotation granularity and inconsistencies in clinical reasoning logic, which
hinder precise cross-modal understanding. This paper introduces FundusExpert,
an ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning
capabilities, along with FundusGen, a dataset constructed through the
intelligent Fundus-Engine system. Fundus-Engine automates localization and
leverages MLLM-based semantic expansion to integrate global disease
classification, local object detection, and fine-grained feature analysis
within a single fundus image. Additionally, by constructing a clinically
aligned cognitive chain, it guides the model to generate interpretable
reasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,
achieves the best performance in ophthalmic question-answering tasks,
surpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in
zero-shot report generation tasks, achieving a clinical consistency of 77.0%,
significantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling
law between data quality and model capability ($L \propto N^{0.068}$),
demonstrating that the cognitive alignment annotations in FundusGen enhance
data utilization efficiency. By integrating region-level localization with
diagnostic reasoning chains, our work develops a scalable, clinically-aligned
MLLM and explores a pathway toward bridging the visual-language gap in specific
MLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.

</details>


### [83] [Simulating multiple human perspectives in socio-ecological systems using large language models](https://arxiv.org/abs/2507.17680)
*Yongchao Zeng,Calum Brown,Ioannis Kyriakou,Ronja Hotz,Mark Rounsevell*

Main category: cs.AI

TL;DR: HoPeS modelling framework employs LLM agents to represent various stakeholders; users can step into the agent roles to experience perspectival differences.


<details>
  <summary>Details</summary>
Motivation: Understanding socio-ecological systems requires insights from diverse stakeholder perspectives, which are often hard to access.

Method: HoPeS employs agents powered by large language models (LLMs) to represent various stakeholders

Result: discrepancies persist between the policy recommendation and implementation due to stakeholders' competing advocacies, mirroring real-world misalignment between researcher and policymaker perspectives. The user's reflection highlights the subjective feelings of frustration and disappointment as a researcher, especially due to the challenge of maintaining political neutrality while attempting to gain political influence.

Conclusion: Further system and protocol refinement are likely to enable new forms of interdisciplinary collaboration in socio-ecological simulations.

Abstract: Understanding socio-ecological systems requires insights from diverse
stakeholder perspectives, which are often hard to access. To enable
alternative, simulation-based exploration of different stakeholder
perspectives, we develop the HoPeS (Human-Oriented Perspective Shifting)
modelling framework. HoPeS employs agents powered by large language models
(LLMs) to represent various stakeholders; users can step into the agent roles
to experience perspectival differences. A simulation protocol serves as a
"scaffold" to streamline multiple perspective-taking simulations, supporting
users in reflecting on, transitioning between, and integrating across
perspectives. A prototype system is developed to demonstrate HoPeS in the
context of institutional dynamics and land use change, enabling both
narrative-driven and numerical experiments. In an illustrative experiment, a
user successively adopts the perspectives of a system observer and a researcher
- a role that analyses data from the embedded land use model to inform
evidence-based decision-making for other LLM agents representing various
institutions. Despite the user's effort to recommend technically sound
policies, discrepancies persist between the policy recommendation and
implementation due to stakeholders' competing advocacies, mirroring real-world
misalignment between researcher and policymaker perspectives. The user's
reflection highlights the subjective feelings of frustration and disappointment
as a researcher, especially due to the challenge of maintaining political
neutrality while attempting to gain political influence. Despite this, the user
exhibits high motivation to experiment with alternative narrative framing
strategies, suggesting the system's potential in exploring different
perspectives. Further system and protocol refinement are likely to enable new
forms of interdisciplinary collaboration in socio-ecological simulations.

</details>


### [84] [Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks](https://arxiv.org/abs/2507.17695)
*Ilias Chatzistefanidis,Navid Nikaein*

Main category: cs.AI

TL;DR: This paper introduces a symbiotic agent paradigm combining LLMs with real-time optimization for 6G networks, achieving improved accuracy, reduced resource usage, and better service-level agreement in 5G testbed experiments.


<details>
  <summary>Details</summary>
Motivation: LLM-based autonomous agents are expected to play a vital role in the evolution of 6G networks, by empowering real-time decision-making related to management and service provisioning to end-users. This facilitates the transition from a specialized intelligence approach to AGI-driven networks.

Method: A novel agentic paradigm that combines LLMs with real-time optimization algorithms towards Trustworthy AI, defined as symbiotic agents. Two novel agent types including: (i) Radio Access Network optimizers, and (ii) multi-agent negotiators for Service-Level Agreements (SLAs) are designed and implemented. An end-to-end architecture for AGI networks is proposed and evaluated on a 5G testbed.

Result: Symbiotic agents reduce decision errors fivefold compared to standalone LLM-based agents, while smaller language models (SLM) achieve similar accuracy with a 99.9% reduction in GPU resource overhead and in near-real-time loops of 82 ms. Reduces RAN over-utilization by approximately 44%.

Conclusion: The symbiotic paradigm is introduced as the foundation for next-generation, AGI-driven networks-systems designed to remain adaptable, efficient, and trustworthy even as LLMs advance.

Abstract: Large Language Model (LLM)-based autonomous agents are expected to play a
vital role in the evolution of 6G networks, by empowering real-time
decision-making related to management and service provisioning to end-users.
This shift facilitates the transition from a specialized intelligence approach,
where artificial intelligence (AI) algorithms handle isolated tasks, to
artificial general intelligence (AGI)-driven networks, where agents possess
broader reasoning capabilities and can manage diverse network functions. In
this paper, we introduce a novel agentic paradigm that combines LLMs with
real-time optimization algorithms towards Trustworthy AI, defined as symbiotic
agents. Optimizers at the LLM's input-level provide bounded uncertainty
steering for numerically precise tasks, whereas output-level optimizers
supervised by the LLM enable adaptive real-time control. We design and
implement two novel agent types including: (i) Radio Access Network optimizers,
and (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We
further propose an end-to-end architecture for AGI networks and evaluate it on
a 5G testbed capturing channel fluctuations from moving vehicles. Results show
that symbiotic agents reduce decision errors fivefold compared to standalone
LLM-based agents, while smaller language models (SLM) achieve similar accuracy
with a 99.9% reduction in GPU resource overhead and in near-real-time loops of
82 ms. A multi-agent demonstration for collaborative RAN on the real-world
testbed highlights significant flexibility in service-level agreement and
resource allocation, reducing RAN over-utilization by approximately 44%.
Drawing on our findings and open-source implementations, we introduce the
symbiotic paradigm as the foundation for next-generation, AGI-driven
networks-systems designed to remain adaptable, efficient, and trustworthy even
as LLMs advance.

</details>


### [85] [Thinking Isn't an Illusion: Overcoming the Limitations of Reasoning Models via Tool Augmentations](https://arxiv.org/abs/2507.17699)
*Zhao Song,Song Yue,Jiahao Zhang*

Main category: cs.AI

TL;DR: Tool-augmented LRMs outperform non-reasoning LLMs, suggesting reasoning is not an illusion.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models (LRMs) have become a central focus in today's large language model (LLM) research, recent empirical studies suggest that this thinking process may not actually enhance reasoning ability, where LLMs without explicit reasoning actually outperform LRMs on tasks with low or high complexity. In this work, we revisit these findings and investigate whether the limitations of LRMs persist when tool augmentations are introduced.

Method: incorporate two types of tools, Python interpreters and scratchpads, and evaluate three representative LLMs and their LRM counterparts on Apple's benchmark reasoning puzzles

Result: with proper tool use, LRMs consistently outperform their non-reasoning counterparts across all levels of task complexity

Conclusion: tool-augmented LRMs consistently outperform their non-reasoning counterparts across all levels of task complexity. These findings challenge the recent narrative that reasoning is an illusion and highlight the potential of tool-augmented LRMs for solving complex problems.

Abstract: Large Reasoning Models (LRMs) have become a central focus in today's large
language model (LLM) research, where models are designed to output a
step-by-step thinking process before arriving at a final answer to handle
complex reasoning tasks. Despite their promise, recent empirical studies (e.g.,
[Shojaee et al., 2025] from Apple) suggest that this thinking process may not
actually enhance reasoning ability, where LLMs without explicit reasoning
actually outperform LRMs on tasks with low or high complexity. In this work, we
revisit these findings and investigate whether the limitations of LRMs persist
when tool augmentations are introduced. We incorporate two types of tools,
Python interpreters and scratchpads, and evaluate three representative LLMs and
their LRM counterparts on Apple's benchmark reasoning puzzles. Our results show
that, with proper tool use, LRMs consistently outperform their non-reasoning
counterparts across all levels of task complexity. These findings challenge the
recent narrative that reasoning is an illusion and highlight the potential of
tool-augmented LRMs for solving complex problems.

</details>


### [86] [Online Submission and Evaluation System Design for Competition Operations](https://arxiv.org/abs/2507.17730)
*Zhe Chen,Daniel Harabor,Ryan Hechnenberger,Nathan R. Sturtevant*

Main category: cs.AI

TL;DR: This paper introduces an online competition system to automate submission and evaluation, reducing operational burden and compatibility issues. It has been successfully used in several competitions.


<details>
  <summary>Details</summary>
Motivation: Tracking the progress in these research areas is not easy, as publications appear in different venues at the same time, and many of them claim to represent the state-of-the-art. Competitions pose a significant operational burden. The organisers must manage and evaluate a large volume of submissions. Participants typically develop their solutions in diverse environments, leading to compatibility issues during the evaluation of their submissions.

Method: an online competition system that automates the submission and evaluation process

Result: allows organisers to manage large numbers of submissions efficiently, utilising isolated environments to evaluate submissions. This system has already been used successfully for several competitions

Conclusion: This paper presents an online competition system that automates the submission and evaluation process,utilising isolated environments to evaluate submissions. This system has already been used successfully for several competitions, including the Grid-Based Pathfinding Competition and the League of Robot Runners competition.

Abstract: Research communities have developed benchmark datasets across domains to
compare the performance of algorithms and techniques However, tracking the
progress in these research areas is not easy, as publications appear in
different venues at the same time, and many of them claim to represent the
state-of-the-art. To address this, research communities often organise periodic
competitions to evaluate the performance of various algorithms and techniques,
thereby tracking advancements in the field. However, these competitions pose a
significant operational burden. The organisers must manage and evaluate a large
volume of submissions. Furthermore, participants typically develop their
solutions in diverse environments, leading to compatibility issues during the
evaluation of their submissions. This paper presents an online competition
system that automates the submission and evaluation process for a competition.
The competition system allows organisers to manage large numbers of submissions
efficiently, utilising isolated environments to evaluate submissions. This
system has already been used successfully for several competitions, including
the Grid-Based Pathfinding Competition and the League of Robot Runners
competition.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [87] [Triadic First-Order Logic Queries in Temporal Networks](https://arxiv.org/abs/2507.17215)
*Omkar Bhalerao,Yunjie Pan,C. Seshadhri,Nishil Talati*

Main category: cs.DB

TL;DR: Introduces FOLTY, an efficient algorithm for mining thresholded FOL triadic queries in temporal networks, opening a new research direction in motif analysis.


<details>
  <summary>Details</summary>
Motivation: Motif counting in temporal graphs is a challenging problem, and FOL queries can mine richer information from networks compared to typical triadic queries.

Method: The algorithm FOLTY uses specialized temporal data structures for efficient implementation.

Result: FOLTY achieves running time matching the best known for temporal triangle counting and can process graphs with nearly 70M edges in less than an hour.

Conclusion: The work introduces FOLTY, an algorithm for mining thresholded FOL triadic queries, demonstrating excellent empirical performance on large graphs.

Abstract: Motif counting is a fundamental problem in network analysis, and there is a
rich literature of theoretical and applied algorithms for this problem. Given a
large input network $G$, a motif $H$ is a small "pattern" graph indicative of
special local structure. Motif/pattern mining involves finding all matches of
this pattern in the input $G$. The simplest, yet challenging, case of motif
counting is when $H$ has three vertices, often called a "triadic" query. Recent
work has focused on "temporal graph mining", where the network $G$ has edges
with timestamps (and directions) and $H$ has time constraints.
  Inspired by concepts in logic and database theory, we introduce the study of
"thresholded First Order Logic (FOL) Motif Analysis" for massive temporal
networks. A typical triadic motif query asks for the existence of three
vertices that form a desired temporal pattern. An "FOL" motif query is obtained
by having both existential and thresholded universal quantifiers. This allows
for query semantics that can mine richer information from networks. A typical
triadic query would be "find all triples of vertices $u,v,w$ such that they
form a triangle within one hour". A thresholded FOL query can express "find all
pairs $u,v$ such that for half of $w$ where $(u,w)$ formed an edge, $(v,w)$
also formed an edge within an hour".
  We design the first algorithm, FOLTY, for mining thresholded FOL triadic
queries. The theoretical running time of FOLTY matches the best known running
time for temporal triangle counting in sparse graphs. We give an efficient
implementation of FOLTY using specialized temporal data structures. FOLTY has
excellent empirical behavior, and can answer triadic FOL queries on graphs with
nearly 70M edges is less than hour on commodity hardware. Our work has the
potential to start a new research direction in the classic well-studied problem
of motif analysis.

</details>


### [88] [Unfolding Data Quality Dimensions in Practice: A Survey](https://arxiv.org/abs/2507.17507)
*Vasileios Papastergios,Lisa Ehrlinger,Anastasios Gounaris*

Main category: cs.DB

TL;DR: 弥合数据质量理论和实践之间的差距，通过系统地连接数据质量工具提供的低级功能与高级维度，揭示它们的多对多关系。


<details>
  <summary>Details</summary>
Motivation: 数据质量描述了数据满足特定需求的程度，以及适合人类和/或下游任务（例如，人工智能）使用的程度。数据质量可以通过多个高级概念（称为维度）进行评估，例如准确性、完整性、一致性或及时性。虽然对于数据质量维度存在广泛的研究和一些标准化尝试（例如，ISO/IEC 25012），但它们的实际应用通常仍不清楚。与研究工作并行，已经开发了大量工具，这些工具实现了检测和缓解特定数据质量问题（例如，缺失值或异常值）的功能。

Method: 通过检查七个开源数据质量工具，系统地连接数据质量工具提供的低级功能与高级维度，揭示它们的多对多关系。

Result: 揭示数据质量工具提供的低级功能与高级维度之间的多对多关系

Conclusion: 通过检查七个开源数据质量工具，我们提供了它们的功能和数据质量维度之间的全面映射，展示了各个功能及其变体如何部分地促进单个维度的评估。这个系统性的调查为从业者和研究人员提供了一个关于数据质量检查碎片化景象的统一视图，为跨多个维度的质量评估提供了可操作的见解。

Abstract: Data quality describes the degree to which data meet specific requirements
and are fit for use by humans and/or downstream tasks (e.g., artificial
intelligence). Data quality can be assessed across multiple high-level concepts
called dimensions, such as accuracy, completeness, consistency, or timeliness.
While extensive research and several attempts for standardization (e.g.,
ISO/IEC 25012) exist for data quality dimensions, their practical application
often remains unclear. In parallel to research endeavors, a large number of
tools have been developed that implement functionalities for the detection and
mitigation of specific data quality issues, such as missing values or outliers.
With this paper, we aim to bridge this gap between data quality theory and
practice by systematically connecting low-level functionalities offered by data
quality tools with high-level dimensions, revealing their many-to-many
relationships. Through an examination of seven open-source data quality tools,
we provide a comprehensive mapping between their functionalities and the data
quality dimensions, demonstrating how individual functionalities and their
variants partially contribute to the assessment of single dimensions. This
systematic survey provides both practitioners and researchers with a unified
view on the fragmented landscape of data quality checks, offering actionable
insights for quality assessment across multiple dimensions.

</details>


### [89] [SHINE: A Scalable HNSW Index in Disaggregated Memory](https://arxiv.org/abs/2507.17647)
*Manuel Widmoser,Daniel Kocher,Nikolaus Augsten*

Main category: cs.DB

TL;DR: 提出了一种用于分离内存的可扩展HNSW索引，通过图结构保持、缓存机制和缓存组合来提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 为了扩展到数十亿的高维向量，索引必须是分布式的。分离内存架构将计算和内存物理地分离到两个不同的硬件单元中，并且在现代数据中心中变得流行。计算节点通过RDMA网络直接访问远程内存并执行所有计算，这给分离索引带来了独特的挑战。

Method: 构建一个保留图结构的索引，达到与单机HNSW相同的精度。采用高效的缓存机制，并逻辑上结合计算节点的缓存。

Result: 该方法在评估中证实了其效率和可扩展性。

Conclusion: 提出了一种可扩展的HNSW索引，用于在分离内存中进行ANN搜索。通过缓存机制和逻辑上组合计算节点的缓存来提高缓存效率和可扩展性。

Abstract: Approximate nearest neighbor (ANN) search is a fundamental problem in
computer science for which in-memory graph-based methods, such as Hierarchical
Navigable Small World (HNSW), perform exceptionally well. To scale beyond
billions of high-dimensional vectors, the index must be distributed. The
disaggregated memory architecture physically separates compute and memory into
two distinct hardware units and has become popular in modern data centers. Both
units are connected via RDMA networks that allow compute nodes to directly
access remote memory and perform all the computations, posing unique challenges
for disaggregated indexes.
  In this work, we propose a scalable HNSW index for ANN search in
disaggregated memory. In contrast to existing distributed approaches, which
partition the graph at the cost of accuracy, our method builds a
graph-preserving index that reaches the same accuracy as a single-machine HNSW.
Continuously fetching high-dimensional vector data from remote memory leads to
severe network bandwidth limitations, which we overcome by employing an
efficient caching mechanism. Since answering a single query involves processing
numerous unique graph nodes, caching alone is not sufficient to achieve high
scalability. We logically combine the caches of the compute nodes to increase
the overall cache effectiveness and confirm the efficiency and scalability of
our method in our evaluation.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [90] [A Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing Retrieval-Augmented Generation in Large Language Models](https://arxiv.org/abs/2507.16826)
*Qikai Wei,Huansheng Ning,Chunlong Han,Jianguo Ding*

Main category: cs.IR

TL;DR: This paper introduces QMKGF, a Query-Aware Multi-Path Knowledge Graph Fusion approach, to improve Retrieval Augmented Generation by considering the connections between retrieved segments. It uses a knowledge graph and query-aware attention to enhance semantic representation and improve the quality of LLMs' generation.


<details>
  <summary>Details</summary>
Motivation: Existing RAG studies primarily focus on retrieving isolated segments using similarity-based matching methods, while overlooking the intrinsic connections between them, which hampers performance in RAG tasks.

Method: The paper proposes QMKGF, a Query-Aware Multi-Path Knowledge Graph Fusion Approach. It constructs a knowledge graph (KG) using prompt templates and LLMs, introduces a multi-path subgraph construction strategy, designs a query-aware attention reward model to score subgraph triples, and enriches the subgraph with additional triples. Finally, it expands the original query using the updated subgraph to enhance its semantic representation.

Result: QMKGF achieves a ROUGE-1 score of 64.98% on the HotpotQA dataset, surpassing the BGE-Rerank approach by 9.72 percentage points.

Conclusion: The experimental results on SQuAD, IIRC, Culture, HotpotQA, and MuSiQue datasets demonstrate the effectiveness and superiority of the QMKGF approach, which achieves a ROUGE-1 score of 64.98% on the HotpotQA dataset, surpassing the BGE-Rerank approach by 9.72 percentage points.

Abstract: Retrieval Augmented Generation (RAG) has gradually emerged as a promising
paradigm for enhancing the accuracy and factual consistency of content
generated by large language models (LLMs). However, existing RAG studies
primarily focus on retrieving isolated segments using similarity-based matching
methods, while overlooking the intrinsic connections between them. This
limitation hampers performance in RAG tasks. To address this, we propose QMKGF,
a Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing
Retrieval Augmented Generation. First, we design prompt templates and employ
general-purpose LLMs to extract entities and relations, thereby generating a
knowledge graph (KG) efficiently. Based on the constructed KG, we introduce a
multi-path subgraph construction strategy that incorporates one-hop relations,
multi-hop relations, and importance-based relations, aiming to improve the
semantic relevance between the retrieved documents and the user query.
Subsequently, we designed a query-aware attention reward model that scores
subgraph triples based on their semantic relevance to the query. Then, we
select the highest score subgraph and enrich subgraph with additional triples
from other subgraphs that are highly semantically relevant to the query.
Finally, the entities, relations, and triples within the updated subgraph are
utilised to expand the original query, thereby enhancing its semantic
representation and improving the quality of LLMs' generation. We evaluate QMKGF
on the SQuAD, IIRC, Culture, HotpotQA, and MuSiQue datasets. On the HotpotQA
dataset, our method achieves a ROUGE-1 score of 64.98\%, surpassing the
BGE-Rerank approach by 9.72 percentage points (from 55.26\% to 64.98\%).
Experimental results demonstrate the effectiveness and superiority of the QMKGF
approach.

</details>


### [91] [You Don't Bring Me Flowers: Mitigating Unwanted Recommendations Through Conformal Risk Control](https://arxiv.org/abs/2507.16829)
*Giovanni De Toni,Erasmo Purificato,Emilia Gómez,Bruno Lepri,Andrea Passerini,Cristian Consonni*

Main category: cs.IR

TL;DR: 这篇论文提出了一种新的推荐方法，可以减少用户收到的不良推荐。


<details>
  <summary>Details</summary>
Motivation: 推荐系统在塑造在线信息消费方面发挥着重要作用。虽然在个性化内容方面有效，但这些系统越来越多地因传播不相关、不需要甚至有害的推荐而受到批评。此类内容会降低用户满意度，并导致严重的社会问题，包括错误信息、激进化和用户信任度下降。

Method: 该方法是直观的、模型无关的、分布自由的，并利用一致风险控制。

Result: 在来自流行的在线视频共享平台的数据上的实验评估表明，该方法确保了以最小的努力有效和可控地减少不需要的推荐。

Conclusion: 这篇论文提出了一种使用一致风险控制来限制个性化推荐中不需要的内容的方法，并通过利用关于项目的简单二元反馈来证明其有效性。

Abstract: Recommenders are significantly shaping online information consumption. While
effective at personalizing content, these systems increasingly face criticism
for propagating irrelevant, unwanted, and even harmful recommendations. Such
content degrades user satisfaction and contributes to significant societal
issues, including misinformation, radicalization, and erosion of user trust.
Although platforms offer mechanisms to mitigate exposure to undesired content,
these mechanisms are often insufficiently effective and slow to adapt to users'
feedback. This paper introduces an intuitive, model-agnostic, and
distribution-free method that uses conformal risk control to provably bound
unwanted content in personalized recommendations by leveraging simple binary
feedback on items. We also address a limitation of traditional conformal risk
control approaches, i.e., the fact that the recommender can provide a smaller
set of recommended items, by leveraging implicit feedback on consumed items to
expand the recommendation set while ensuring robust risk mitigation. Our
experimental evaluation on data coming from a popular online video-sharing
platform demonstrates that our approach ensures an effective and controllable
reduction of unwanted recommendations with minimal effort. The source code is
available here: https://github.com/geektoni/mitigating-harm-recsys.

</details>


### [92] [LLM4MEA: Data-free Model Extraction Attacks on Sequential Recommenders via Large Language Models](https://arxiv.org/abs/2507.16969)
*Shilong Zhao,Fei Sun,Kaike Zhang,Shaoling Jing,Du Su,Zhichao Shi,Zhiyi Yin,Huawei Shen,Xueqi Cheng*

Main category: cs.IR

TL;DR: LLM4MEA 是一种利用大型语言模型生成数据的新型模型提取方法，可有效提高攻击性能并降低推荐系统漏洞的风险。


<details>
  <summary>Details</summary>
Motivation: 先前的 MEA 中的黑盒攻击由于数据选择中的随机抽样而无法暴露推荐系统漏洞，从而导致合成分布与真实分布不一致。

Method: 利用大型语言模型 (LLM) 作为类人排序器来生成数据的新型模型提取方法 LLM4MEA。

Result: LLM4MEA 将合成数据与真实数据之间的差异降低了高达 64.98%，并平均提高了 MEA 性能 44.82%。

Conclusion: LLM4MEA显著优于现有方法，在数据质量和攻击性能方面都有提升。提出了一种防御策略，并确定了可以降低 MEA 风险的关键超参数。

Abstract: Recent studies have demonstrated the vulnerability of sequential recommender
systems to Model Extraction Attacks (MEAs). MEAs collect responses from
recommender systems to replicate their functionality, enabling unauthorized
deployments and posing critical privacy and security risks. Black-box attacks
in prior MEAs are ineffective at exposing recommender system vulnerabilities
due to random sampling in data selection, which leads to misaligned synthetic
and real-world distributions. To overcome this limitation, we propose LLM4MEA,
a novel model extraction method that leverages Large Language Models (LLMs) as
human-like rankers to generate data. It generates data through interactions
between the LLM ranker and target recommender system. In each interaction, the
LLM ranker analyzes historical interactions to understand user behavior, and
selects items from recommendations with consistent preferences to extend the
interaction history, which serves as training data for MEA. Extensive
experiments demonstrate that LLM4MEA significantly outperforms existing
approaches in data quality and attack performance, reducing the divergence
between synthetic and real-world data by up to 64.98% and improving MEA
performance by 44.82% on average. From a defensive perspective, we propose a
simple yet effective defense strategy and identify key hyperparameters of
recommender systems that can mitigate the risk of MEAs.

</details>


### [93] [VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings](https://arxiv.org/abs/2507.17080)
*Ramin Giahi,Kehui Yao,Sriram Kollipara,Kai Zhao,Vahid Mirjalili,Jianpeng Xu,Topojoy Biswas,Evren Korpeoglu,Kannan Achan*

Main category: cs.IR

TL;DR: 提出VL-CLIP框架，通过视觉定位和LLM增强文本表示来改进电商推荐系统。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型（如CLIP）在电商推荐系统中面临关键挑战：1) 弱对象级对齐，2) 模糊的文本表示，3) 领域不匹配。

Method: 提出一个名为VL-CLIP的框架，通过整合视觉定位以实现细粒度的视觉理解，并利用基于LLM的代理来生成丰富的文本嵌入，从而增强CLIP嵌入。

Result: 在美国最大的电商平台之一上，CTR提高18.6%，ATC提高15.5%，GMV提高4.0%。在精确度和语义对齐方面，VL-CLIP优于CLIP、FashionCLIP和GCL等视觉-语言模型。

Conclusion: VL-CLIP通过结合视觉定位和LLM增强的文本表示，显著提高了电商推荐的检索准确率、多模态检索效果和推荐质量。

Abstract: Multimodal learning plays a critical role in e-commerce recommendation
platforms today, enabling accurate recommendations and product understanding.
However, existing vision-language models, such as CLIP, face key challenges in
e-commerce recommendation systems: 1) Weak object-level alignment, where global
image embeddings fail to capture fine-grained product attributes, leading to
suboptimal retrieval performance; 2) Ambiguous textual representations, where
product descriptions often lack contextual clarity, affecting cross-modal
matching; and 3) Domain mismatch, as generic vision-language models may not
generalize well to e-commerce-specific data. To address these limitations, we
propose a framework, VL-CLIP, that enhances CLIP embeddings by integrating
Visual Grounding for fine-grained visual understanding and an LLM-based agent
for generating enriched text embeddings. Visual Grounding refines image
representations by localizing key products, while the LLM agent enhances
textual features by disambiguating product descriptions. Our approach
significantly improves retrieval accuracy, multimodal retrieval effectiveness,
and recommendation quality across tens of millions of items on one of the
largest e-commerce platforms in the U.S., increasing CTR by 18.6%, ATC by
15.5%, and GMV by 4.0%. Additional experimental results show that our framework
outperforms vision-language models, including CLIP, FashionCLIP, and GCL, in
both precision and semantic alignment, demonstrating the potential of combining
object-aware visual grounding and LLM-enhanced text representation for robust
multimodal recommendations.

</details>


### [94] [Enhancing Transferability and Consistency in Cross-Domain Recommendations via Supervised Disentanglement](https://arxiv.org/abs/2507.17112)
*Yuhan Wang,Qing Xie,Zhifeng Bao,Mengzi Tang,Lin Li,Yongjian Liu*

Main category: cs.IR

TL;DR: This paper introduces DGCDR, a GNN-enhanced encoder-decoder framework for cross-domain recommendation that addresses the challenges of pre-separation strategies and unsupervised disentanglement objectives in existing methods.


<details>
  <summary>Details</summary>
Motivation: disentanglement-based CDR methods employing generative modeling or GNNs with contrastive objectives face two key challenges: (i) pre-separation strategies decouple features before extracting collaborative signals, disrupting intra-domain interactions and introducing noise; (ii) unsupervised disentanglement objectives lack explicit task-specific guidance, resulting in limited consistency and suboptimal alignment

Method: a GNN-enhanced encoder-decoder framework

Result: DGCDR achieves state-of-the-art performance, with improvements of up to 11.59% across key metrics. Qualitative analyses further validate its superior disentanglement quality and transferability.

Conclusion: DGCDR achieves state-of-the-art performance, with improvements of up to 11.59% across key metrics. Qualitative analyses further validate its superior disentanglement quality and transferability.

Abstract: Cross-domain recommendation (CDR) aims to alleviate the data sparsity by
transferring knowledge across domains. Disentangled representation learning
provides an effective solution to model complex user preferences by separating
intra-domain features (domain-shared and domain-specific features), thereby
enhancing robustness and interpretability. However, disentanglement-based CDR
methods employing generative modeling or GNNs with contrastive objectives face
two key challenges: (i) pre-separation strategies decouple features before
extracting collaborative signals, disrupting intra-domain interactions and
introducing noise; (ii) unsupervised disentanglement objectives lack explicit
task-specific guidance, resulting in limited consistency and suboptimal
alignment. To address these challenges, we propose DGCDR, a GNN-enhanced
encoder-decoder framework. To handle challenge (i), DGCDR first applies GNN to
extract high-order collaborative signals, providing enriched representations as
a robust foundation for disentanglement. The encoder then dynamically
disentangles features into domain-shared and -specific spaces, preserving
collaborative information during the separation process. To handle challenge
(ii), the decoder introduces an anchor-based supervision that leverages
hierarchical feature relationships to enhance intra-domain consistency and
cross-domain alignment. Extensive experiments on real-world datasets
demonstrate that DGCDR achieves state-of-the-art performance, with improvements
of up to 11.59% across key metrics. Qualitative analyses further validate its
superior disentanglement quality and transferability. Our source code and
datasets are available on GitHub for further comparison.

</details>


### [95] [R4ec: A Reasoning, Reflection, and Refinement Framework for Recommendation Systems](https://arxiv.org/abs/2507.17249)
*Hao Gu,Rui Zhong,Yu Xia,Wei Yang,Chi Lu,Peng Jiang,Kun Gai*

Main category: cs.IR

TL;DR: This paper introduces $R^{4}$ec, a reasoning, reflection, and refinement framework that enhances recommendation systems by using LLMs to mimic System-2 thinking, leading to improved accuracy and a 2.2% revenue increase in online advertising.


<details>
  <summary>Details</summary>
Motivation: Existing approaches primarily involve basic prompt techniques for knowledge acquisition, which resemble System-1 thinking and are highly sensitive to errors in the reasoning path.

Method: The paper proposes a reasoning, reflection and refinement framework ($R^{4}$ec) that evolves the recommendation system into a weak System-2 model, which includes an actor model for reasoning and a reflection model for feedback. The actor model refines its response based on the feedback through an iterative reflection and refinement process.

Result: The $R^{4}$ec framework improves recommendation systems by enabling LLMs to facilitate slow and deliberate System-2-like thinking. $R^{4}$ec shows a 2.2% increase in revenue on a large scale online advertising platform.

Conclusion: The paper demonstrates the superiority of $R^{4}$ec on Amazon-Book and MovieLens-1M datasets and shows a 2.2% increase in revenue on a large scale online advertising platform. The paper also investigates the scaling properties of the actor model and reflection model.

Abstract: Harnessing Large Language Models (LLMs) for recommendation systems has
emerged as a prominent avenue, drawing substantial research interest. However,
existing approaches primarily involve basic prompt techniques for knowledge
acquisition, which resemble System-1 thinking. This makes these methods highly
sensitive to errors in the reasoning path, where even a small mistake can lead
to an incorrect inference. To this end, in this paper, we propose $R^{4}$ec, a
reasoning, reflection and refinement framework that evolves the recommendation
system into a weak System-2 model. Specifically, we introduce two models: an
actor model that engages in reasoning, and a reflection model that judges these
responses and provides valuable feedback. Then the actor model will refine its
response based on the feedback, ultimately leading to improved responses. We
employ an iterative reflection and refinement process, enabling LLMs to
facilitate slow and deliberate System-2-like thinking. Ultimately, the final
refined knowledge will be incorporated into a recommendation backbone for
prediction. We conduct extensive experiments on Amazon-Book and MovieLens-1M
datasets to demonstrate the superiority of $R^{4}$ec. We also deploy $R^{4}$ec
on a large scale online advertising platform, showing 2.2\% increase of
revenue. Furthermore, we investigate the scaling properties of the actor model
and reflection model.

</details>


### [96] [Exploring the Potential of LLMs for Serendipity Evaluation in Recommender Systems](https://arxiv.org/abs/2507.17290)
*Li Kang,Yuhan Zhao,Li Chen*

Main category: cs.IR

TL;DR: LLMs can effectively simulate human users for serendipity evaluation in recommender systems, outperforming traditional metrics.


<details>
  <summary>Details</summary>
Motivation: Serendipity evaluation in recommender systems is challenging due to its subjective nature, and current metrics don't align with user perceptions. LLMs are revolutionizing evaluation methodologies.

Method: Meta-evaluation on two datasets from e-commerce and movie domains, comparing LLMs to proxy metrics, assessing the impact of auxiliary data, and testing multi-LLM techniques.

Result: Simplest zero-shot LLMs match or exceed conventional metrics. Multi-LLM techniques and auxiliary data improve alignment with human perspectives, achieving a 21.5% Pearson correlation with user study results.

Conclusion: LLMs can serve as potentially accurate and cost-effective evaluators for serendipity in recommender systems.

Abstract: Serendipity plays a pivotal role in enhancing user satisfaction within
recommender systems, yet its evaluation poses significant challenges due to its
inherently subjective nature and conceptual ambiguity. Current algorithmic
approaches predominantly rely on proxy metrics for indirect assessment, often
failing to align with real user perceptions, thus creating a gap. With large
language models (LLMs) increasingly revolutionizing evaluation methodologies
across various human annotation tasks, we are inspired to explore a core
research proposition: Can LLMs effectively simulate human users for serendipity
evaluation? To address this question, we conduct a meta-evaluation on two
datasets derived from real user studies in the e-commerce and movie domains,
focusing on three key aspects: the accuracy of LLMs compared to conventional
proxy metrics, the influence of auxiliary data on LLM comprehension, and the
efficacy of recently popular multi-LLM techniques. Our findings indicate that
even the simplest zero-shot LLMs achieve parity with, or surpass, the
performance of conventional metrics. Furthermore, multi-LLM techniques and the
incorporation of auxiliary data further enhance alignment with human
perspectives. Based on our findings, the optimal evaluation by LLMs yields a
Pearson correlation coefficient of 21.5\% when compared to the results of the
user study. This research implies that LLMs may serve as potentially accurate
and cost-effective evaluators, introducing a new paradigm for serendipity
evaluation in recommender systems.

</details>


### [97] [EndoFinder: Online Lesion Retrieval for Explainable Colorectal Polyp Diagnosis Leveraging Latent Scene Representations](https://arxiv.org/abs/2507.17323)
*Ruijie Yang,Yan Zhu,Peiyao Fu,Yizhe Zhang,Zhihua Wang,Quanlin Li,Pinghong Zhou,Xian Yang,Shuo Wang*

Main category: cs.IR

TL;DR: EndoFinder, an online polyp retrieval framework that leverages multi-view scene representations for explainable and scalable CRC diagnosis, outperforms existing methods in accuracy and offers a promising direction for more efficient AI-driven colonoscopy workflows.


<details>
  <summary>Details</summary>
Motivation: Colorectal cancer (CRC) remains a leading cause of cancer-related mortality, underscoring the importance of timely polyp detection and diagnosis. While deep learning models have improved optical-assisted diagnostics, they often demand extensive labeled datasets and yield "black-box" outputs with limited interpretability.

Method: develop a Polyp-aware Image Encoder by combining contrastive learning and a reconstruction task, guided by polyp segmentation masks and introduce a Scene Representation Transformer, which fuses multiple views of the polyp into a single latent representation. By discretizing this representation through a hashing layer, EndoFinder enables real-time retrieval from a compiled database of historical polyp cases

Result: EndoFinder outperforms existing methods in accuracy while providing transparent, retrieval-based insights for clinical decision-making. By contributing a novel dataset and a scalable, explainable framework, our work addresses key challenges in polyp diagnosis and offers a promising direction for more efficient AI-driven colonoscopy workflows.

Conclusion: EndoFinder outperforms existing methods in accuracy while providing transparent, retrieval-based insights for clinical decision-making, addressing key challenges in polyp diagnosis and offers a promising direction for more efficient AI-driven colonoscopy workflows.

Abstract: Colorectal cancer (CRC) remains a leading cause of cancer-related mortality,
underscoring the importance of timely polyp detection and diagnosis. While deep
learning models have improved optical-assisted diagnostics, they often demand
extensive labeled datasets and yield "black-box" outputs with limited
interpretability. In this paper, we propose EndoFinder, an online polyp
retrieval framework that leverages multi-view scene representations for
explainable and scalable CRC diagnosis. First, we develop a Polyp-aware Image
Encoder by combining contrastive learning and a reconstruction task, guided by
polyp segmentation masks. This self-supervised approach captures robust
features without relying on large-scale annotated data. Next, we treat each
polyp as a three-dimensional "scene" and introduce a Scene Representation
Transformer, which fuses multiple views of the polyp into a single latent
representation. By discretizing this representation through a hashing layer,
EndoFinder enables real-time retrieval from a compiled database of historical
polyp cases, where diagnostic information serves as interpretable references
for new queries. We evaluate EndoFinder on both public and newly collected
polyp datasets for re-identification and pathology classification. Results show
that EndoFinder outperforms existing methods in accuracy while providing
transparent, retrieval-based insights for clinical decision-making. By
contributing a novel dataset and a scalable, explainable framework, our work
addresses key challenges in polyp diagnosis and offers a promising direction
for more efficient AI-driven colonoscopy workflows. The source code is
available at https://github.com/ku262/EndoFinder-Scene.

</details>


### [98] ["Beyond the past": Leveraging Audio and Human Memory for Sequential Music Recommendation](https://arxiv.org/abs/2507.17356)
*Viet-Tran Anh,Bruno Sguerra,Gabriel Meseguer-Brocal,Lea Briand,Manuel Moussallam*

Main category: cs.IR

TL;DR: propose a model that leverages audio information to predict in advance the ACT-R-like activation of new tracks


<details>
  <summary>Details</summary>
Motivation: one limitation of using a model inspired by human memory (or the past), is that it struggles to recommend new tracks that users have not previously listened to. To bridge this gap

Method: propose a model that leverages audio information to predict in advance the ACT-R-like activation of new tracks and incorporates them into the recommendation scoring process.

Result: improve the prediction of the most relevant tracks for the next user session.

Conclusion: demonstrates the empirical effectiveness of the proposed model using proprietary data, which we publicly release along with the model's source code to foster future research in this field.

Abstract: On music streaming services, listening sessions are often composed of a
balance of familiar and new tracks. Recently, sequential recommender systems
have adopted cognitive-informed approaches, such as Adaptive Control of
Thought-Rational (ACT-R), to successfully improve the prediction of the most
relevant tracks for the next user session. However, one limitation of using a
model inspired by human memory (or the past), is that it struggles to recommend
new tracks that users have not previously listened to. To bridge this gap, here
we propose a model that leverages audio information to predict in advance the
ACT-R-like activation of new tracks and incorporates them into the
recommendation scoring process. We demonstrate the empirical effectiveness of
the proposed model using proprietary data, which we publicly release along with
the model's source code to foster future research in this field.

</details>


### [99] [Citation Recommendation using Deep Canonical Correlation Analysis](https://arxiv.org/abs/2507.17603)
*Conor McNamara,Effirul Ramlan*

Main category: cs.IR

TL;DR: 提出了一种新的引文推荐算法，该算法通过应用深度典型相关分析 (DCCA) 改进了线性典型相关分析 (CCA) 方法，深度典型相关分析 (DCCA) 是一种神经网络扩展，能够捕获科学文章的分布式文本和基于图的表示之间复杂的非线性关系


<details>
  <summary>Details</summary>
Motivation: 有效地结合多个数据视图需要融合技术，这些技术可以捕获互补信息，同时保留每个模态的独特特征

Method: 应用深度典型相关分析 (DCCA)，这是一种神经网络扩展，能够捕获科学文章的分布式文本和基于图的表示之间复杂的非线性关系

Result: 在大型 DBLP (数字书目和图书馆项目) 引用网络数据集上的实验表明，我们的方法优于最先进的基于 CCA 的方法，在 Mean Average Precision@10 方面实现了超过 11% 的相对改进，在 Precision@10 方面实现了 5% 的改进，在 Recall@10 方面实现了 7% 的改进

Conclusion: DCCA的非线性变换比CCA的线性投影产生更具表现力的潜在表示

Abstract: Recent advances in citation recommendation have improved accuracy by
leveraging multi-view representation learning to integrate the various
modalities present in scholarly documents. However, effectively combining
multiple data views requires fusion techniques that can capture complementary
information while preserving the unique characteristics of each modality. We
propose a novel citation recommendation algorithm that improves upon linear
Canonical Correlation Analysis (CCA) methods by applying Deep CCA (DCCA), a
neural network extension capable of capturing complex, non-linear relationships
between distributed textual and graph-based representations of scientific
articles. Experiments on the large-scale DBLP (Digital Bibliography & Library
Project) citation network dataset demonstrate that our approach outperforms
state-of-the-art CCA-based methods, achieving relative improvements of over 11%
in Mean Average Precision@10, 5% in Precision@10, and 7% in Recall@10. These
gains reflect more relevant citation recommendations and enhanced ranking
quality, suggesting that DCCA's non-linear transformations yield more
expressive latent representations than CCA's linear projections.

</details>


### [100] [Leave No One Behind: Fairness-Aware Cross-Domain Recommender Systems for Non-Overlapping Users](https://arxiv.org/abs/2507.17749)
*Weixin Chen,Yuhan Zhao,Li Chen,Weike Pan*

Main category: cs.IR

TL;DR: 提出了一种新颖的解决方案，该方案为非重叠目标域用户生成虚拟源域用户，从而解决 CDR 非重叠用户偏差问题。


<details>
  <summary>Details</summary>
Motivation: 重叠用户在推荐质量方面获得了显著提升，而非重叠用户受益甚微，甚至面临性能下降。这种不公平可能会削弱用户信任，并因此对业务参与度和收入产生负面影响。

Method: 利用双重注意机制来辨别重叠和非重叠用户之间的相似性，从而合成真实的虚拟用户嵌入。

Result: 在三个公共数据集上进行的综合实验表明，该方法有效地缓解了 CDR 非重叠用户偏差，且不损失整体准确性。

Conclusion: 该方法有效地缓解了 CDR 非重叠用户偏差，且不损失整体准确性。

Abstract: Cross-domain recommendation (CDR) methods predominantly leverage overlapping
users to transfer knowledge from a source domain to a target domain. However,
through empirical studies, we uncover a critical bias inherent in these
approaches: while overlapping users experience significant enhancements in
recommendation quality, non-overlapping users benefit minimally and even face
performance degradation. This unfairness may erode user trust, and,
consequently, negatively impact business engagement and revenue. To address
this issue, we propose a novel solution that generates virtual source-domain
users for non-overlapping target-domain users. Our method utilizes a dual
attention mechanism to discern similarities between overlapping and
non-overlapping users, thereby synthesizing realistic virtual user embeddings.
We further introduce a limiter component that ensures the generated virtual
users align with real-data distributions while preserving each user's unique
characteristics. Notably, our method is model-agnostic and can be seamlessly
integrated into any CDR model. Comprehensive experiments conducted on three
public datasets with five CDR baselines demonstrate that our method effectively
mitigates the CDR non-overlapping user bias, without loss of overall accuracy.
Our code is publicly available at https://github.com/WeixinChen98/VUG.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [101] [Eco-Friendly AI: Unleashing Data Power for Green Federated Learning](https://arxiv.org/abs/2507.17241)
*Mattia Sabella,Monica Vitali*

Main category: cs.LG

TL;DR: This paper introduces a data-centric approach to Green Federated Learning, reducing training data volume to minimize the environmental impact of FL tasks, with promising results in time series classification.


<details>
  <summary>Details</summary>
Motivation: The widespread adoption of AI and ML has a significant environmental impact in terms of energy consumption and carbon emissions. Reducing data transmission costs and enhancing privacy are important challenges.

Method: The paper proposes a data-centric approach to Green Federated Learning, focusing on reducing the volume of training data by analyzing federated datasets, selecting an optimal subset of data based on quality metrics, and choosing federated nodes with the lowest environmental impact. An interactive recommendation system is introduced to optimize FL configurations through data reduction.

Result: The methodology examines the influence of data-centric factors, such as data quality and volume, on FL training performance and carbon emissions. The interactive recommendation system optimizes FL configurations through data reduction, minimizing environmental impact during training.

Conclusion: The paper demonstrates promising results in reducing the environmental impact of FL tasks by applying the proposed methodology to time series classification.

Abstract: The widespread adoption of Artificial Intelligence (AI) and Machine Learning
(ML) comes with a significant environmental impact, particularly in terms of
energy consumption and carbon emissions. This pressing issue highlights the
need for innovative solutions to mitigate AI's ecological footprint. One of the
key factors influencing the energy consumption of ML model training is the size
of the training dataset. ML models are often trained on vast amounts of data
continuously generated by sensors and devices distributed across multiple
locations. To reduce data transmission costs and enhance privacy, Federated
Learning (FL) enables model training without the need to move or share raw
data. While FL offers these advantages, it also introduces challenges due to
the heterogeneity of data sources (related to volume and quality),
computational node capabilities, and environmental impact.
  This paper contributes to the advancement of Green AI by proposing a
data-centric approach to Green Federated Learning. Specifically, we focus on
reducing FL's environmental impact by minimizing the volume of training data.
Our methodology involves the analysis of the characteristics of federated
datasets, the selecting of an optimal subset of data based on quality metrics,
and the choice of the federated nodes with the lowest environmental impact. We
develop a comprehensive methodology that examines the influence of data-centric
factors, such as data quality and volume, on FL training performance and carbon
emissions. Building on these insights, we introduce an interactive
recommendation system that optimizes FL configurations through data reduction,
minimizing environmental impact during training. Applying this methodology to
time series classification has demonstrated promising results in reducing the
environmental impact of FL tasks.

</details>


### [102] [Evaluating Artificial Intelligence Algorithms for the Standardization of Transtibial Prosthetic Socket Shape Design](https://arxiv.org/abs/2507.16818)
*C. H. E. Jordaan,M. van der Stelt,T. J. J. Maal,V. M. A. Stirler,R. Leijendekkers,T. Kachman,G. A. de Jong*

Main category: cs.LG

TL;DR: This study uses AI to standardize transtibial prosthetic socket design, finding that predicting adaptations to the socket shape is more effective than predicting the final shape directly, with a random forest model achieving the best results.


<details>
  <summary>Details</summary>
Motivation: The quality of a transtibial prosthetic socket depends on the prosthetist's skills and expertise, as the fitting is performed manually. This study investigates multiple artificial intelligence (AI) approaches to help standardize transtibial prosthetic socket design.

Method: multiple artificial intelligence (AI) approaches

Result: For all algorithms, estimating the required adaptations outperformed direct prediction of the final socket shape. The random forest model applied to adaptation prediction yields the lowest error with a median surface-to-surface distance of 1.24 millimeters, a first quartile of 1.03 millimeters, and a third quartile of 1.54 millimeters.

Conclusion: Estimating the required adaptations outperformed direct prediction of the final socket shape. The random forest model applied to adaptation prediction yields the lowest error with a median surface-to-surface distance of 1.24 millimeters, a first quartile of 1.03 millimeters, and a third quartile of 1.54 millimeters.

Abstract: The quality of a transtibial prosthetic socket depends on the prosthetist's
skills and expertise, as the fitting is performed manually. This study
investigates multiple artificial intelligence (AI) approaches to help
standardize transtibial prosthetic socket design. Data from 118 patients were
collected by prosthetists working in the Dutch healthcare system. This data
consists of a three-dimensional (3D) scan of the residual limb and a
corresponding 3D model of the prosthetist-designed socket. Multiple data
pre-processing steps are performed for alignment, standardization and
optionally compression using Morphable Models and Principal Component Analysis.
Afterward, three different algorithms - a 3D neural network, Feedforward neural
network, and random forest - are developed to either predict 1) the final
socket shape or 2) the adaptations performed by a prosthetist to predict the
socket shape based on the 3D scan of the residual limb. Each algorithm's
performance was evaluated by comparing the prosthetist-designed socket with the
AI-generated socket, using two metrics in combination with the error location.
First, we measure the surface-to-surface distance to assess the overall surface
error between the AI-generated socket and the prosthetist-designed socket.
Second, distance maps between the AI-generated and prosthetist sockets are
utilized to analyze the error's location. For all algorithms, estimating the
required adaptations outperformed direct prediction of the final socket shape.
The random forest model applied to adaptation prediction yields the lowest
error with a median surface-to-surface distance of 1.24 millimeters, a first
quartile of 1.03 millimeters, and a third quartile of 1.54 millimeters.

</details>


### [103] [Exploring the Frontiers of kNN Noisy Feature Detection and Recovery for Self-Driving Labs](https://arxiv.org/abs/2507.16833)
*Qiuyu Shi,Kangming Li,Yao Fehlis,Daniel Persaud,Robert Black,Jason Hattrick-Simpers*

Main category: cs.LG

TL;DR: Developed an automated workflow to detect and correct noisy features in self-driving laboratories, showing that data quality can be enhanced through this method.


<details>
  <summary>Details</summary>
Motivation: Errors in the capture of input parameters may corrupt the features used to model system performance, compromising current and future campaigns in self-driving laboratories (SDLs).

Method: This study develops an automated workflow to systematically detect noisy features, determine sample-feature pairings that can be corrected, and finally recover the correct feature values. A systematic study is then performed to examine how dataset size, noise intensity, and feature value distribution affect both the detectability and recoverability of noisy features.

Result: High-intensity noise and large training datasets are conducive to the detection and correction of noisy features. Low-intensity noise reduces detection and recovery but can be compensated for by larger clean training data sets. Detection and correction results vary between features with continuous and dispersed feature distributions showing greater recoverability compared to features with discrete or narrow distributions.

Conclusion: This study demonstrates a model agnostic framework for rational data recovery in the presence of noise, limited data, and differing feature distributions and provides a tangible benchmark of kNN imputation in materials data sets. Ultimately, it aims to enhance data quality and experimental precision in automated materials discovery.

Abstract: Self-driving laboratories (SDLs) have shown promise to accelerate materials
discovery by integrating machine learning with automated experimental
platforms. However, errors in the capture of input parameters may corrupt the
features used to model system performance, compromising current and future
campaigns. This study develops an automated workflow to systematically detect
noisy features, determine sample-feature pairings that can be corrected, and
finally recover the correct feature values. A systematic study is then
performed to examine how dataset size, noise intensity, and feature value
distribution affect both the detectability and recoverability of noisy
features. In general, high-intensity noise and large training datasets are
conducive to the detection and correction of noisy features. Low-intensity
noise reduces detection and recovery but can be compensated for by larger clean
training data sets. Detection and correction results vary between features with
continuous and dispersed feature distributions showing greater recoverability
compared to features with discrete or narrow distributions. This systematic
study not only demonstrates a model agnostic framework for rational data
recovery in the presence of noise, limited data, and differing feature
distributions but also provides a tangible benchmark of kNN imputation in
materials data sets. Ultimately, it aims to enhance data quality and
experimental precision in automated materials discovery.

</details>


### [104] [TD-Interpreter: Enhancing the Understanding of Timing Diagrams with Visual-Language Learning](https://arxiv.org/abs/2507.16844)
*Jie He,Vincent Theo Willem Kenbeek,Zhantao Yang,Meixun Qu,Ezio Bartocci,Dejan Ničković,Radu Grosu*

Main category: cs.LG

TL;DR: TD-Interpreter is a ML tool that helps engineers understand timing diagrams using visual question-answering.


<details>
  <summary>Details</summary>
Motivation: To assist engineers in understanding complex timing diagrams from third parties during design and verification.

Method: Fine-tuned LLaVA, a 7B multimodal large language model, with a synthetic data generation workflow.

Result: TD-Interpreter is a visual question-answer environment for design and verification queries regarding timing diagrams.

Conclusion: TD-Interpreter outperforms untuned GPT-4o on timing diagram benchmarks.

Abstract: We introduce TD-Interpreter, a specialized ML tool that assists engineers in
understanding complex timing diagrams (TDs), originating from a third party,
during their design and verification process. TD-Interpreter is a visual
question-answer environment which allows engineers to input a set of TDs and
ask design and verification queries regarding these TDs. We implemented
TD-Interpreter with multimodal learning by fine-tuning LLaVA, a lightweight 7B
Multimodal Large Language Model (MLLM). To address limited training data
availability, we developed a synthetic data generation workflow that aligns
visual information with its textual interpretation. Our experimental evaluation
demonstrates the usefulness of TD-Interpreter which outperformed untuned GPT-4o
by a large margin on the evaluated benchmarks.

</details>


### [105] [Reinforcement Learning in hyperbolic space for multi-step reasoning](https://arxiv.org/abs/2507.16864)
*Tao Xu,Dung-Yang Lee,Momiao Xiong*

Main category: cs.LG

TL;DR: This paper proposes a new reinforcement learning framework using hyperbolic Transformers for multi-step reasoning, which improves accuracy and reduces computational time compared to RL with vanilla transformers.


<details>
  <summary>Details</summary>
Motivation: Conventional RL methods struggle with complex reasoning tasks due to issues such as credit assignment, high-dimensional state representations, and stability concerns.

Method: The paper introduces a new framework that integrates hyperbolic Transformers into RL for multi-step reasoning. The proposed approach leverages hyperbolic embeddings to model hierarchical structures effectively.

Result: Compared to RL with vanilla transformer, the hyperbolic RL largely improves accuracy by (32%~44%) on FrontierMath benchmark, (43%~45%) on nonlinear optimal control benchmark, while achieving impressive reduction in computational time by (16%~32%) on FrontierMath benchmark, (16%~17%) on nonlinear optimal control benchmark.

Conclusion: The paper demonstrates the potential of hyperbolic Transformers in reinforcement learning, particularly for multi-step reasoning tasks that involve hierarchical structures.

Abstract: Multi-step reasoning is a fundamental challenge in artificial intelligence,
with applications ranging from mathematical problem-solving to decision-making
in dynamic environments. Reinforcement Learning (RL) has shown promise in
enabling agents to perform multi-step reasoning by optimizing long-term
rewards. However, conventional RL methods struggle with complex reasoning tasks
due to issues such as credit assignment, high-dimensional state
representations, and stability concerns. Recent advancements in Transformer
architectures and hyperbolic geometry have provided novel solutions to these
challenges. This paper introduces a new framework that integrates hyperbolic
Transformers into RL for multi-step reasoning. The proposed approach leverages
hyperbolic embeddings to model hierarchical structures effectively. We present
theoretical insights, algorithmic details, and experimental results that
include Frontier Math and nonlinear optimal control problems. Compared to RL
with vanilla transformer, the hyperbolic RL largely improves accuracy by
(32%~44%) on FrontierMath benchmark, (43%~45%) on nonlinear optimal control
benchmark, while achieving impressive reduction in computational time by
(16%~32%) on FrontierMath benchmark, (16%~17%) on nonlinear optimal control
benchmark. Our work demonstrates the potential of hyperbolic Transformers in
reinforcement learning, particularly for multi-step reasoning tasks that
involve hierarchical structures.

</details>


### [106] [Diffusion-Modeled Reinforcement Learning for Carbon and Risk-Aware Microgrid Optimization](https://arxiv.org/abs/2507.16867)
*Yunyi Zhao,Wei Zhang,Cheng Xiang,Hongyang Du,Dusit Niyato,Shuhua Gao*

Main category: cs.LG

TL;DR: DiffCarl, a diffusion-modeled carbon- and risk-aware reinforcement learning algorithm, enables adaptive energy scheduling under uncertainty and explicitly account for carbon emissions and operational risk in multi-microgrid systems.


<details>
  <summary>Details</summary>
Motivation: Microgrid communities face significant challenges in real-time energy scheduling and optimization under uncertainty with the growing integration of renewables and increasing system complexity.

Method: DiffCarl integrates a diffusion model into a deep reinforcement learning (DRL) framework.

Result: DiffCarl outperforms classic algorithms and state-of-the-art DRL solutions, with 2.3-30.1% lower operational cost and 28.7% lower carbon emissions than those of its carbon-unaware variant, and reduces performance variability.

Conclusion: DiffCarl is a practical and forward-looking solution that adapts to different system configurations and objectives to support real-world deployment in evolving energy systems.

Abstract: This paper introduces DiffCarl, a diffusion-modeled carbon- and risk-aware
reinforcement learning algorithm for intelligent operation of multi-microgrid
systems. With the growing integration of renewables and increasing system
complexity, microgrid communities face significant challenges in real-time
energy scheduling and optimization under uncertainty. DiffCarl integrates a
diffusion model into a deep reinforcement learning (DRL) framework to enable
adaptive energy scheduling under uncertainty and explicitly account for carbon
emissions and operational risk. By learning action distributions through a
denoising generation process, DiffCarl enhances DRL policy expressiveness and
enables carbon- and risk-aware scheduling in dynamic and uncertain microgrid
environments. Extensive experimental studies demonstrate that it outperforms
classic algorithms and state-of-the-art DRL solutions, with 2.3-30.1% lower
operational cost. It also achieves 28.7% lower carbon emissions than those of
its carbon-unaware variant and reduces performance variability. These results
highlight DiffCarl as a practical and forward-looking solution. Its flexible
design allows efficient adaptation to different system configurations and
objectives to support real-world deployment in evolving energy systems.

</details>


### [107] [Navigation through Non-Compact Symmetric Spaces: a mathematical perspective on Cartan Neural Networks](https://arxiv.org/abs/2507.16871)
*Pietro Giuseppe Fré,Federico Milanesio,Guido Sanguinetti,Matteo Santoro*

Main category: cs.LG

TL;DR: This paper expands on the mathematical structures underpinning Cartan Neural Networks, detailing the geometric properties of the layers and how the maps between layers interact with such structures to make Cartan Neural Networks covariant and geometrically interpretable.


<details>
  <summary>Details</summary>
Motivation: Recent work has identified non-compact symmetric spaces U/H as a promising class of homogeneous manifolds to develop a geometrically consistent theory of neural networks

Method: detailing the geometric properties of the layers and how the maps between layers interact with such structures to make Cartan Neural Networks covariant and geometrically interpretable

Result: An initial implementation of these concepts has been presented in a twin paper under the moniker of Cartan Neural Networks, showing both the feasibility and the performance of these geometric concepts in a machine learning context

Conclusion: Together, these twin papers constitute a first step towards a fully geometrically interpretable theory of neural networks exploiting group-theoretic structures

Abstract: Recent work has identified non-compact symmetric spaces U/H as a promising
class of homogeneous manifolds to develop a geometrically consistent theory of
neural networks. An initial implementation of these concepts has been presented
in a twin paper under the moniker of Cartan Neural Networks, showing both the
feasibility and the performance of these geometric concepts in a machine
learning context. The current paper expands on the mathematical structures
underpinning Cartan Neural Networks, detailing the geometric properties of the
layers and how the maps between layers interact with such structures to make
Cartan Neural Networks covariant and geometrically interpretable. Together,
these twin papers constitute a first step towards a fully geometrically
interpretable theory of neural networks exploiting group-theoretic structures

</details>


### [108] [Confidence Optimization for Probabilistic Encoding](https://arxiv.org/abs/2507.16881)
*Pengjiu Xia,Yidian Huang,Wenchao Wei,Yuwen Tan*

Main category: cs.LG

TL;DR: The paper introduces a confidence optimization probabilistic encoding (CPE) method to improve distance reliability and enhance representation learning in neural networks by using a confidence-aware mechanism and L2 regularization.


<details>
  <summary>Details</summary>
Motivation: The randomness of Gaussian noise distorts point-based distance measurements in classification tasks when using probabilistic encoding.

Method: The authors refine probabilistic encoding with a confidence-aware mechanism to adjust distance calculations and replace the conventional KL divergence-based variance regularization with a simpler L2 regularization term to directly constrain variance.

Result: The proposed method significantly improves performance and generalization on natural language classification tasks.

Conclusion: The proposed CPE method improves performance and generalization on both the BERT and the RoBERTa model.

Abstract: Probabilistic encoding introduces Gaussian noise into neural networks,
enabling a smooth transition from deterministic to uncertain states and
enhancing generalization ability. However, the randomness of Gaussian noise
distorts point-based distance measurements in classification tasks. To mitigate
this issue, we propose a confidence optimization probabilistic encoding (CPE)
method that improves distance reliability and enhances representation learning.
Specifically, we refine probabilistic encoding with two key strategies: First,
we introduce a confidence-aware mechanism to adjust distance calculations,
ensuring consistency and reliability in probabilistic encoding classification
tasks. Second, we replace the conventional KL divergence-based variance
regularization, which relies on unreliable prior assumptions, with a simpler L2
regularization term to directly constrain variance. The method we proposed is
model-agnostic, and extensive experiments on natural language classification
tasks demonstrate that our method significantly improves performance and
generalization on both the BERT and the RoBERTa model.

</details>


### [109] [SplitMeanFlow: Interval Splitting Consistency in Few-Step Generative Modeling](https://arxiv.org/abs/2507.16884)
*Yi Guo,Wei Wang,Zhihang Yuan,Rong Cao,Kuan Chen,Zhengyang Chen,Yuanyuan Huo,Yang Zhang,Yuping Wang,Shouda Liu,Yuxuan Wang*

Main category: cs.LG

TL;DR: SplitMeanFlow 是一种新的训练框架，它通过利用定积分的加性特性，推导出一个新的、纯代数的恒等式，称为区间分割一致性，从而学习平均速度场。


<details>
  <summary>Details</summary>
Motivation: 像 Flow Matching 这样的生成模型已经取得了最先进的性能，但通常受到计算成本高的迭代采样过程的阻碍。为了解决这个问题，最近的工作集中于通过学习平均速度场来进行Few-step或 one-step 的生成，平均速度场直接将噪声映射到数据。

Method: SplitMeanFlow，一种新的训练框架，它直接将这种代数一致性作为学习目标来执行。

Result: 与 MeanFlow 相比，SplitMeanFlow 是一种更通用的基础，并且效率更高，因为它消除了对 JVP 计算的需求，从而简化了实现，更稳定的训练和更广泛的硬件兼容性。

Conclusion: SplitMeanFlow 模型在大型语音合成产品中成功部署，实现了 20 倍的加速。

Abstract: Generative models like Flow Matching have achieved state-of-the-art
performance but are often hindered by a computationally expensive iterative
sampling process. To address this, recent work has focused on few-step or
one-step generation by learning the average velocity field, which directly maps
noise to data. MeanFlow, a leading method in this area, learns this field by
enforcing a differential identity that connects the average and instantaneous
velocities. In this work, we argue that this differential formulation is a
limiting special case of a more fundamental principle. We return to the first
principles of average velocity and leverage the additivity property of definite
integrals. This leads us to derive a novel, purely algebraic identity we term
Interval Splitting Consistency. This identity establishes a self-referential
relationship for the average velocity field across different time intervals
without resorting to any differential operators. Based on this principle, we
introduce SplitMeanFlow, a new training framework that enforces this algebraic
consistency directly as a learning objective. We formally prove that the
differential identity at the core of MeanFlow is recovered by taking the limit
of our algebraic consistency as the interval split becomes infinitesimal. This
establishes SplitMeanFlow as a direct and more general foundation for learning
average velocity fields. From a practical standpoint, our algebraic approach is
significantly more efficient, as it eliminates the need for JVP computations,
resulting in simpler implementation, more stable training, and broader hardware
compatibility. One-step and two-step SplitMeanFlow models have been
successfully deployed in large-scale speech synthesis products (such as
Doubao), achieving speedups of 20x.

</details>


### [110] [SiLQ: Simple Large Language Model Quantization-Aware Training](https://arxiv.org/abs/2507.16933)
*Steven K. Esser,Jeffrey L. McKinstry,Deepika Bablani,Rathinakumar Appuswamy,Dharmendra S. Modha*

Main category: cs.LG

TL;DR: The paper introduces a quantization-aware training approach that improves accuracy and efficiency of large language models with minimal overhead and broad applicability.


<details>
  <summary>Details</summary>
Motivation: Large language models can be quantized to reduce inference time latency, model size, and energy consumption, thereby delivering a better user experience at lower cost. A challenge exists to deliver quantized models with minimal loss of accuracy in reasonable time, and in particular to do so without requiring mechanisms incompatible with specialized inference accelerators.

Method: a simple, end-to-end quantization-aware training approach

Result: outperforms the leading published quantization methods by large margins on several modern benchmarks, with both base and instruct model variants. The approach easily generalizes across different model architectures, can be applied to activations, cache, and weights, and requires the introduction of no additional operations to the model other than the quantization itself.

Conclusion: A simple, end-to-end quantization-aware training approach outperforms the leading published quantization methods by large margins on several modern benchmarks.

Abstract: Large language models can be quantized to reduce inference time latency,
model size, and energy consumption, thereby delivering a better user experience
at lower cost. A challenge exists to deliver quantized models with minimal loss
of accuracy in reasonable time, and in particular to do so without requiring
mechanisms incompatible with specialized inference accelerators. Here, we
demonstrate a simple, end-to-end quantization-aware training approach that,
with an increase in total model training budget of less than 0.1%, outperforms
the leading published quantization methods by large margins on several modern
benchmarks, with both base and instruct model variants. The approach easily
generalizes across different model architectures, can be applied to
activations, cache, and weights, and requires the introduction of no additional
operations to the model other than the quantization itself.

</details>


### [111] [Hierarchical Reinforcement Learning Framework for Adaptive Walking Control Using General Value Functions of Lower-Limb Sensor Signals](https://arxiv.org/abs/2507.16983)
*Sonny T. Jones,Grange M. Simpson,Patrick M. Pilarski,Ashley N. Dalrymple*

Main category: cs.LG

TL;DR: HRL approach breaks down the complex task of exoskeleton control adaptation into a higher-level framework for terrain strategy adaptation and a lower-level framework for providing predictive information


<details>
  <summary>Details</summary>
Motivation: enhance mobility and autonomy for individuals with motor impairments

Method: use of Hierarchical Reinforcement Learning (HRL) to develop adaptive control strategies

Result: the addition of predictions made from GVFs increased overall network accuracy. Terrain-specific performance increases were seen while walking on even ground, uneven ground, up and down ramps, and turns, terrains that are often misclassified without predictive information.

Conclusion: This work contributes new insights into the nuances of HRL and the future development of exoskeletons to facilitate safe transitioning and traversing across different walking environments.

Abstract: Rehabilitation technology is a natural setting to study the shared learning
and decision-making of human and machine agents. In this work, we explore the
use of Hierarchical Reinforcement Learning (HRL) to develop adaptive control
strategies for lower-limb exoskeletons, aiming to enhance mobility and autonomy
for individuals with motor impairments. Inspired by prominent models of
biological sensorimotor processing, our investigated HRL approach breaks down
the complex task of exoskeleton control adaptation into a higher-level
framework for terrain strategy adaptation and a lower-level framework for
providing predictive information; this latter element is implemented via the
continual learning of general value functions (GVFs). GVFs generated temporal
abstractions of future signal values from multiple wearable lower-limb sensors,
including electromyography, pressure insoles, and goniometers. We investigated
two methods for incorporating actual and predicted sensor signals into a policy
network with the intent to improve the decision-making capacity of the control
system of a lower-limb exoskeleton during ambulation across varied terrains. As
a key result, we found that the addition of predictions made from GVFs
increased overall network accuracy. Terrain-specific performance increases were
seen while walking on even ground, uneven ground, up and down ramps, and turns,
terrains that are often misclassified without predictive information. This
suggests that predictive information can aid decision-making during
uncertainty, e.g., on terrains that have a high chance of being misclassified.
This work, therefore, contributes new insights into the nuances of HRL and the
future development of exoskeletons to facilitate safe transitioning and
traversing across different walking environments.

</details>


### [112] [PyG 2.0: Scalable Learning on Real World Graphs](https://arxiv.org/abs/2507.16991)
*Matthias Fey,Jinu Sunil,Akihiro Nitta,Rishi Puri,Manan Shah,Blaž Stojanovič,Ramona Bendias,Alexandria Barghi,Vid Kocijan,Zecheng Zhang,Xinwei He,Jan Eric Lenssen,Jure Leskovec*

Main category: cs.LG

TL;DR: Pyg 2.0 is a comprehensive update that introduces substantial improvements in scalability and real-world application capabilities.


<details>
  <summary>Details</summary>
Motivation: establish itself as a leading framework for Graph Neural Networks

Method: enhanced architecture, including support for heterogeneous and temporal graphs, scalable feature/graph stores, and various optimizations

Result: enabling researchers and practitioners to tackle large-scale graph learning problems efficiently

Conclusion: PyG 2.0 introduces substantial improvements in scalability and real-world application capabilities.

Abstract: PyG (PyTorch Geometric) has evolved significantly since its initial release,
establishing itself as a leading framework for Graph Neural Networks. In this
paper, we present Pyg 2.0 (and its subsequent minor versions), a comprehensive
update that introduces substantial improvements in scalability and real-world
application capabilities. We detail the framework's enhanced architecture,
including support for heterogeneous and temporal graphs, scalable feature/graph
stores, and various optimizations, enabling researchers and practitioners to
tackle large-scale graph learning problems efficiently. Over the recent years,
PyG has been supporting graph learning in a large variety of application areas,
which we will summarize, while providing a deep dive into the important areas
of relational deep learning and large language modeling.

</details>


### [113] [Should Bias Always be Eliminated? A Principled Framework to Use Data Bias for OOD Generation](https://arxiv.org/abs/2507.17001)
*Yan Li,Guangyi Chen,Yunlong Deng,Zijian Li,Zeyu Tang,Anpeng Wu,Kun Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种新的框架，该框架策略性地利用偏差来补充推理过程中的不变表示，从而提高模型在 OOD 领域中的性能。


<details>
  <summary>Details</summary>
Motivation: 大多数现有的模型适应 OOD 领域的方法依赖于不变表示学习来消除偏差特征的影响。然而，应该始终消除偏差吗？如果不是，应该何时保留它，以及如何利用它？

Method: 该框架包含两个关键组件，以直接和间接的方式利用偏差：(1) 使用不变性作为指导，从偏差中提取预测成分；(2) 利用已识别的偏差来估计环境条件，然后使用它来探索适当的偏差感知预测器，以减轻环境差距。

Result: 该方法优于现有方法。

Conclusion: 该方法在合成数据集和标准领域泛化基准测试中均表现出色，证明了其稳健性和适应性。

Abstract: Most existing methods for adapting models to out-of-distribution (OOD)
domains rely on invariant representation learning to eliminate the influence of
biased features. However, should bias always be eliminated -- and if not, when
should it be retained, and how can it be leveraged? To address these questions,
we first present a theoretical analysis that explores the conditions under
which biased features can be identified and effectively utilized. Building on
this theoretical foundation, we introduce a novel framework that strategically
leverages bias to complement invariant representations during inference. The
framework comprises two key components that leverage bias in both direct and
indirect ways: (1) using invariance as guidance to extract predictive
ingredients from bias, and (2) exploiting identified bias to estimate the
environmental condition and then use it to explore appropriate bias-aware
predictors to alleviate environment gaps. We validate our approach through
experiments on both synthetic datasets and standard domain generalization
benchmarks. Results consistently demonstrate that our method outperforms
existing approaches, underscoring its robustness and adaptability.

</details>


### [114] [laplax -- Laplace Approximations with JAX](https://arxiv.org/abs/2507.17013)
*Tobias Weber,Bálint Mucsányi,Lenard Rommel,Thomas Christie,Lars Kasüschke,Marvin Pförtner,Philipp Hennig*

Main category: cs.LG

TL;DR: laplax is a new open-source Python package for performing Laplace approximations with jax


<details>
  <summary>Details</summary>
Motivation: quantifying weight-space uncertainty in deep neural networks, enabling the application of Bayesian tools such as predictive uncertainty and model selection via Occam's razor.

Method: a new open-source Python package for performing Laplace approximations with jax

Result: laplax is designed with a modular and purely functional architecture and minimal external dependencies

Conclusion: laplax offers a flexible and researcher-friendly framework for rapid prototyping and experimentation. Its goal is to facilitate research on Bayesian neural networks, uncertainty quantification for deep learning, and the development of improved Laplace approximation techniques.

Abstract: The Laplace approximation provides a scalable and efficient means of
quantifying weight-space uncertainty in deep neural networks, enabling the
application of Bayesian tools such as predictive uncertainty and model
selection via Occam's razor. In this work, we introduce laplax, a new
open-source Python package for performing Laplace approximations with jax.
Designed with a modular and purely functional architecture and minimal external
dependencies, laplax offers a flexible and researcher-friendly framework for
rapid prototyping and experimentation. Its goal is to facilitate research on
Bayesian neural networks, uncertainty quantification for deep learning, and the
development of improved Laplace approximation techniques.

</details>


### [115] [Causal Graph Fuzzy LLMs: A First Introduction and Applications in Time Series Forecasting](https://arxiv.org/abs/2507.17016)
*Omid Orang,Patricia O. Lucas,Gabriel I. F. Paiva,Petronio C. L. Silva,Felipe Augusto Rocha da Silva,Adriano Alonso Veloso,Frederico Gadelha Guimaraes*

Main category: cs.LG

TL;DR: This paper introduces CGF-LLM, a new LLM framework using GPT-2 with fuzzy time series and causal graphs for multivariate time series forecasting. It converts numerical data into interpretable text, enhancing semantic understanding and structural insight.


<details>
  <summary>Details</summary>
Motivation: Applying Large Language Models (LLMs) to time series forecasting (TSF) has garnered significant attention. The objective is to convert numerical time series into interpretable forms through fuzzification and causal analysis, enabling semantic understanding and structural insight for the GPT-2 model.

Method: The study uses GPT-2 combined with fuzzy time series (FTS) and causal graph to predict multivariate time series.

Result: The resulting textual representation offers a more interpretable view of the complex dynamics underlying the original time series.

Conclusion: The CGF-LLM model's effectiveness is confirmed across four multivariate time series datasets, paving promising future directions for time series forecasting using LLMs based on FTS.

Abstract: In recent years, the application of Large Language Models (LLMs) to time
series forecasting (TSF) has garnered significant attention among researchers.
This study presents a new frame of LLMs named CGF-LLM using GPT-2 combined with
fuzzy time series (FTS) and causal graph to predict multivariate time series,
marking the first such architecture in the literature. The key objective is to
convert numerical time series into interpretable forms through the parallel
application of fuzzification and causal analysis, enabling both semantic
understanding and structural insight as input for the pretrained GPT-2 model.
The resulting textual representation offers a more interpretable view of the
complex dynamics underlying the original time series. The reported results
confirm the effectiveness of our proposed LLM-based time series forecasting
model, as demonstrated across four different multivariate time series datasets.
This initiative paves promising future directions in the domain of TSF using
LLMs based on FTS.

</details>


### [116] [BiLO: Bilevel Local Operator Learning for PDE Inverse Problems. Part II: Efficient Uncertainty Quantification with Low-Rank Adaptation](https://arxiv.org/abs/2507.17019)
*Ray Zirui Zhang,Christopher E. Miles,Xiaohui Xie,John S. Lowengrub*

Main category: cs.LG

TL;DR: Extends Bilevel Local Operator Learning (BiLO) to Bayesian inference for PDE-constrained optimization, achieving accurate and efficient uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Uncertainty quantification and inverse problems governed by partial differential equations (PDEs) are central to a wide range of scientific and engineering applications.

Method: Bilevel Local Operator Learning (BiLO) for PDE-constrained optimization problems extended to the Bayesian inference framework. Achieves efficient sampling through gradient-based Markov Chain Monte Carlo (MCMC) methods and low-rank adaptation (LoRA).

Result: Improves the accuracy of both parameter inference and uncertainty quantification. Demonstrates a direct link between the tolerance for solving the lower level problem and the accuracy of the resulting uncertainty quantification.

Conclusion: The proposed method delivers accurate inference and quantification of uncertainties while maintaining high computational efficiency.

Abstract: Uncertainty quantification and inverse problems governed by partial
differential equations (PDEs) are central to a wide range of scientific and
engineering applications. In this second part of a two part series, we extend
Bilevel Local Operator Learning (BiLO) for PDE-constrained optimization
problems developed in Part 1 to the Bayesian inference framework. At the lower
level, we train a network to approximate the local solution operator by
minimizing the local operator loss with respect to the weights of the neural
network. At the upper level, we sample the PDE parameters from the posterior
distribution. We achieve efficient sampling through gradient-based Markov Chain
Monte Carlo (MCMC) methods and low-rank adaptation (LoRA). Compared with
existing methods based on Bayesian neural networks, our approach bypasses the
challenge of sampling in the high-dimensional space of neural network weights
and does not require specifying a prior distribution on the neural network
solution. Instead, uncertainty propagates naturally from the data through the
PDE constraints. By enforcing strong PDE constraints, the proposed method
improves the accuracy of both parameter inference and uncertainty
quantification. We analyze the dynamic error of the gradient in the MCMC
sampler and the static error in the posterior distribution due to inexact
minimization of the lower level problem and demonstrate a direct link between
the tolerance for solving the lower level problem and the accuracy of the
resulting uncertainty quantification. Through numerical experiments across a
variety of PDE models, we demonstrate that our method delivers accurate
inference and quantification of uncertainties while maintaining high
computational efficiency.

</details>


### [117] [Pragmatic Policy Development via Interpretable Behavior Cloning](https://arxiv.org/abs/2507.17056)
*Anton Matsson,Yaochen Rao,Heather J. Litman,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 提出了一种简单而实用的替代方案：从行为策略的可解释模型估计的每个患者状态下最常选择的动作中得出治疗策略。


<details>
  <summary>Details</summary>
Motivation: 与可解释性和评估相关的挑战限制了其在安全关键领域的实际应用。可解释性受到无约束RL策略的黑盒性质的阻碍，而评估（通常是异策略执行的）对数据收集行为策略的巨大偏差很敏感，尤其是在使用基于重要性抽样的方法时。

Method: 从每个患者状态下最常选择的动作中得出治疗策略，由行为策略的可解释模型估计。

Result: 使用基于树的模型，专门用于利用数据中的模式，获得关于治疗的状态的自然分组。树结构通过设计确保了可解释性，同时改变所考虑的动作数量控制了与行为策略的重叠程度，从而实现了可靠的异策略评估。

Conclusion: 在类风湿性关节炎和败血症护理的真实案例中，根据该框架得出的策略可以胜过当前的实践，为通过离线RL获得的策略提供可解释的替代方案。

Abstract: Offline reinforcement learning (RL) holds great promise for deriving optimal
policies from observational data, but challenges related to interpretability
and evaluation limit its practical use in safety-critical domains.
Interpretability is hindered by the black-box nature of unconstrained RL
policies, while evaluation -- typically performed off-policy -- is sensitive to
large deviations from the data-collecting behavior policy, especially when
using methods based on importance sampling. To address these challenges, we
propose a simple yet practical alternative: deriving treatment policies from
the most frequently chosen actions in each patient state, as estimated by an
interpretable model of the behavior policy. By using a tree-based model, which
is specifically designed to exploit patterns in the data, we obtain a natural
grouping of states with respect to treatment. The tree structure ensures
interpretability by design, while varying the number of actions considered
controls the degree of overlap with the behavior policy, enabling reliable
off-policy evaluation. This pragmatic approach to policy development
standardizes frequent treatment patterns, capturing the collective clinical
judgment embedded in the data. Using real-world examples in rheumatoid
arthritis and sepsis care, we demonstrate that policies derived under this
framework can outperform current practice, offering interpretable alternatives
to those obtained via offline RL.

</details>


### [118] [Risk In Context: Benchmarking Privacy Leakage of Foundation Models in Synthetic Tabular Data Generation](https://arxiv.org/abs/2507.17066)
*Jessup Byun,Xiaofeng Lin,Joshua Ward,Guang Cheng*

Main category: cs.LG

TL;DR: Foundation models for synthetic tabular data in low-data settings have high privacy risks. Prompt tweaks can improve privacy-utility tradeoff.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art generative models rely on large datasets and can overfit, leak sensitive records, and require frequent retraining in low-data settings. In-context learning repeats seed rows verbatim, introducing a new privacy risk.

Method: Benchmark of three foundation models (GPT-4o-mini, LLaMA 3.3 70B, TabPFN v2) against four baselines on 35 real-world tables from health, finance, and policy. Evaluation of statistical fidelity, downstream utility, and membership inference leakage. Factorial study of zero-cost prompt tweaks.

Result: Foundation models consistently have the highest privacy risk. LLaMA 3.3 70B reaches up to 54 percentage points higher true-positive rate at 1% FPR than the safest baseline. GPT-4o-mini and TabPFN are also highly vulnerable. Three zero-cost prompt tweaks can reduce worst-case AUC by 14 points and rare-class leakage by up to 39 points while maintaining over 90% fidelity.

Conclusion: Foundation models consistently have the highest privacy risk. CTGAN and GPT-4o-mini offer better privacy-utility tradeoffs. Three zero-cost prompt tweaks can reduce worst-case AUC and rare-class leakage while maintaining over 90% fidelity.

Abstract: Synthetic tabular data is essential for machine learning workflows,
especially for expanding small or imbalanced datasets and enabling
privacy-preserving data sharing. However, state-of-the-art generative models
(GANs, VAEs, diffusion models) rely on large datasets with thousands of
examples. In low-data settings, often the primary motivation for synthetic
data, these models can overfit, leak sensitive records, and require frequent
retraining. Recent work uses large pre-trained transformers to generate rows
via in-context learning (ICL), which needs only a few seed examples and no
parameter updates, avoiding retraining. But ICL repeats seed rows verbatim,
introducing a new privacy risk that has only been studied in text. The severity
of this risk in tabular synthesis-where a single row may identify a
person-remains unclear. We address this gap with the first benchmark of three
foundation models (GPT-4o-mini, LLaMA 3.3 70B, TabPFN v2) against four
baselines on 35 real-world tables from health, finance, and policy. We evaluate
statistical fidelity, downstream utility, and membership inference leakage.
Results show foundation models consistently have the highest privacy risk.
LLaMA 3.3 70B reaches up to 54 percentage points higher true-positive rate at
1% FPR than the safest baseline. GPT-4o-mini and TabPFN are also highly
vulnerable. We plot the privacy-utility frontier and show that CTGAN and
GPT-4o-mini offer better tradeoffs. A factorial study finds that three
zero-cost prompt tweaks-small batch size, low temperature, and using summary
statistics-can reduce worst-case AUC by 14 points and rare-class leakage by up
to 39 points while maintaining over 90% fidelity. Our benchmark offers a
practical guide for safer low-data synthesis with foundation models.

</details>


### [119] [BGM-HAN: A Hierarchical Attention Network for Accurate and Fair Decision Assessment on Semi-Structured Profiles](https://arxiv.org/abs/2507.17472)
*Junhua Liu,Roy Ka-Wei Lee,Kwan Hui Lim*

Main category: cs.LG

TL;DR: 提出了一种新的方法，通过整合分层学习和各种增强功能来增强复杂的决策工作流程。


<details>
  <summary>Details</summary>
Motivation: 高风险领域的人类决策通常依赖于专业知识和启发式方法，但容易受到难以检测的认知偏差的影响，这些偏差威胁到公平性和长期结果。

Method: BGM-HAN，一种增强的Byte-Pair编码、门控多头分层注意力网络，旨在有效地建模半结构化的申请人数据。

Result: 实验结果表明，我们提出的模型显著优于传统机器学习到大型语言模型的最先进的基线。

Conclusion: 提出的模型在真实招生数据上显著优于最先进的基线模型，为在结构、上下文和公平性重要的领域中增强决策提供了有希望的框架。

Abstract: Human decision-making in high-stakes domains often relies on expertise and
heuristics, but is vulnerable to hard-to-detect cognitive biases that threaten
fairness and long-term outcomes. This work presents a novel approach to
enhancing complex decision-making workflows through the integration of
hierarchical learning alongside various enhancements. Focusing on university
admissions as a representative high-stakes domain, we propose BGM-HAN, an
enhanced Byte-Pair Encoded, Gated Multi-head Hierarchical Attention Network,
designed to effectively model semi-structured applicant data. BGM-HAN captures
multi-level representations that are crucial for nuanced assessment, improving
both interpretability and predictive performance. Experimental results on real
admissions data demonstrate that our proposed model significantly outperforms
both state-of-the-art baselines from traditional machine learning to large
language models, offering a promising framework for augmenting decision-making
in domains where structure, context, and fairness matter. Source code is
available at: https://github.com/junhua/bgm-han.

</details>


### [120] [Advancing Robustness in Deep Reinforcement Learning with an Ensemble Defense Approach](https://arxiv.org/abs/2507.17070)
*Adithya Mohan,Dominik Rößle,Daniel Cremers,Torsten Schön*

Main category: cs.LG

TL;DR: This paper introduces an ensemble defense architecture for DRL models in autonomous driving, which significantly improves robustness against adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Existing defense mechanisms have a research gap regarding the integration of multiple defenses in autonomous driving scenarios.

Method: This paper proposes a novel ensemble-based defense architecture to mitigate adversarial attacks in autonomous driving.

Result: The ensemble method improves the mean reward from 5.87 to 18.38 (over 213% increase) and reduces the mean collision rate from 0.50 to 0.09 (an 82% decrease) in the highway scenario and merge scenario, outperforming all standalone defense strategies.

Conclusion: The proposed ensemble architecture significantly enhances the robustness of DRL models, improving mean reward and reducing collision rate compared to baseline and standalone defenses.

Abstract: Recent advancements in Deep Reinforcement Learning (DRL) have demonstrated
its applicability across various domains, including robotics, healthcare,
energy optimization, and autonomous driving. However, a critical question
remains: How robust are DRL models when exposed to adversarial attacks? While
existing defense mechanisms such as adversarial training and distillation
enhance the resilience of DRL models, there remains a significant research gap
regarding the integration of multiple defenses in autonomous driving scenarios
specifically. This paper addresses this gap by proposing a novel ensemble-based
defense architecture to mitigate adversarial attacks in autonomous driving. Our
evaluation demonstrates that the proposed architecture significantly enhances
the robustness of DRL models. Compared to the baseline under FGSM attacks, our
ensemble method improves the mean reward from 5.87 to 18.38 (over 213%
increase) and reduces the mean collision rate from 0.50 to 0.09 (an 82%
decrease) in the highway scenario and merge scenario, outperforming all
standalone defense strategies.

</details>


### [121] [Sensor Drift Compensation in Electronic-Nose-Based Gas Recognition Using Knowledge Distillation](https://arxiv.org/abs/2507.17071)
*Juntao Lin,Xianghao Zhan*

Main category: cs.LG

TL;DR: 本文提出了一种基于知识蒸馏(KD)的电子鼻传感器漂移补偿方法，该方法在性能上优于现有的DRCA方法。


<details>
  <summary>Details</summary>
Motivation: 由于环境变化和传感器老化，传感器漂移对电子鼻系统在实际部署期间的气体分类性能提出了挑战。先前的研究缺乏稳健的统计实验验证，并且可能过度补偿传感器漂移，从而失去与类别相关的方差。

Method: 提出了一种新的知识蒸馏(KD)方法，基准方法是域正则化成分分析(DRCA)，以及一种混合方法KD-DRCA。

Result: KD始终优于DRCA和KD-DRCA，准确率提高了18%，F1分数提高了15%，证明了KD在漂移补偿方面的卓越有效性。

Conclusion: 知识蒸馏(KD)在电子鼻漂移缓解方面首次应用，显著优于先前的最先进的DRCA方法，并增强了真实环境中传感器漂移补偿的可靠性。

Abstract: Due to environmental changes and sensor aging, sensor drift challenges the
performance of electronic nose systems in gas classification during real-world
deployment. Previous studies using the UCI Gas Sensor Array Drift Dataset
reported promising drift compensation results but lacked robust statistical
experimental validation and may overcompensate for sensor drift, losing
class-related variance.To address these limitations and improve sensor drift
compensation with statistical rigor, we first designed two domain adaptation
tasks based on the same electronic nose dataset: using the first batch to
predict the remaining batches, simulating a controlled laboratory setting; and
predicting the next batch using all prior batches, simulating continuous
training data updates for online training. We then systematically tested three
methods: our proposed novel Knowledge Distillation (KD) method, the benchmark
method Domain Regularized Component Analysis (DRCA), and a hybrid method
KD-DRCA, across 30 random test set partitions on the UCI dataset. We showed
that KD consistently outperformed both DRCA and KD-DRCA, achieving up to an 18%
improvement in accuracy and 15% in F1-score, demonstrating KD's superior
effectiveness in drift compensation. This is the first application of KD for
electronic nose drift mitigation, significantly outperforming the previous
state-of-the-art DRCA method and enhancing the reliability of sensor drift
compensation in real-world environments.

</details>


### [122] [ZORMS-LfD: Learning from Demonstrations with Zeroth-Order Random Matrix Search](https://arxiv.org/abs/2507.17096)
*Olivia Dry,Timothy L. Molloy,Wanxin Jin,Iman Shames*

Main category: cs.LG

TL;DR: ZORMS-LfD 是一种用于从演示中学习的零阶随机矩阵搜索方法，它在各种基准问题上与现有技术水平的方法相匹配或超过了它们的性能，并且计算时间减少了。


<details>
  <summary>Details</summary>
Motivation: 现有的最优一阶方法需要存在并计算成本、约束、动力学和学习损失相对于状态、控制和/或参数的梯度。大多数现有方法也适用于离散时间，而连续时间中的约束问题只受到粗略的关注。

Method: Zeroth-Order Random Matrix Search for Learning from Demonstrations (ZORMS-LfD)

Result: ZORMS-LfD在学习损失和计算时间方面与现有技术水平的方法相匹配或超过了它们的性能。在无约束的连续时间基准问题上，ZORMS-LfD实现了与现有技术水平的一阶方法相似的损失性能，计算时间减少了超过80%。在约束的连续时间基准问题上，没有专门的现有技术水平的方法，ZORMS-LfD被证明优于常用的无梯度Nelder-Mead优化方法。

Conclusion: ZORMS-LfD在各种基准问题上，在学习损失和计算时间方面，与现有最优方法相匹配或超过了它们的性能。在无约束的连续时间基准问题上，ZORMS-LfD实现了与现有最优一阶方法相似的损失性能，而计算时间减少了80%以上。在没有专门的最优方法的约束连续时间基准问题上，ZORMS-LfD被证明优于常用的无梯度Nelder-Mead优化方法。

Abstract: We propose Zeroth-Order Random Matrix Search for Learning from Demonstrations
(ZORMS-LfD). ZORMS-LfD enables the costs, constraints, and dynamics of
constrained optimal control problems, in both continuous and discrete time, to
be learned from expert demonstrations without requiring smoothness of the
learning-loss landscape. In contrast, existing state-of-the-art first-order
methods require the existence and computation of gradients of the costs,
constraints, dynamics, and learning loss with respect to states, controls
and/or parameters. Most existing methods are also tailored to discrete time,
with constrained problems in continuous time receiving only cursory attention.
We demonstrate that ZORMS-LfD matches or surpasses the performance of
state-of-the-art methods in terms of both learning loss and compute time across
a variety of benchmark problems. On unconstrained continuous-time benchmark
problems, ZORMS-LfD achieves similar loss performance to state-of-the-art
first-order methods with an over $80$\% reduction in compute time. On
constrained continuous-time benchmark problems where there is no specialized
state-of-the-art method, ZORMS-LfD is shown to outperform the commonly used
gradient-free Nelder-Mead optimization method.

</details>


### [123] [Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models](https://arxiv.org/abs/2507.17107)
*Andrii Balashov*

Main category: cs.LG

TL;DR: RL fine-tuning only changes a small part of the model, which works just as well and is consistent across different setups.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that RL fine-tuning requires updating most of a model's parameters for aligning large language models with complex tasks and human preferences.

Method: RL fine-tuning with PPO, DPO, SimPO, PRIME algorithms on OpenAI, Meta, and open-source LLMs, analyzing parameter updates and overlap across different seeds, datasets, and algorithms.

Result: RL fine-tuning modifies only 5-30% of weights, subnetworks updated by RL show substantial overlap, fine-tuning only this sparse subnetwork recovers full model performance.

Conclusion: RL fine-tuning consistently modifies only a small subnetwork, leaving most parameters unchanged, suggesting targeted changes near the model's original distribution.

Abstract: Reinforcement learning (RL) is a key post-pretraining step for aligning large
language models (LLMs) with complex tasks and human preferences. While it is
often assumed that RL fine-tuning requires updating most of a model's
parameters, we challenge this assumption with a surprising finding: RL
fine-tuning consistently modifies only a small subnetwork (typically 5-30% of
weights), leaving most parameters unchanged. We call this phenomenon RL-induced
parameter update sparsity. It arises naturally, without any sparsity
constraints or parameter-efficient tuning, and appears across multiple RL
algorithms (e.g., PPO, DPO, SimPO, PRIME) and model families (e.g., OpenAI,
Meta, and open-source LLMs). Moreover, the subnetworks updated by RL show
substantial overlap across different seeds, datasets, and algorithms-far
exceeding chance-suggesting a partially transferable structure in the
pretrained model. We show that fine-tuning only this sparse subnetwork recovers
full model performance and yields parameters nearly identical to the fully
fine-tuned model. Our analysis suggests this sparsity emerges because RL
operates near the model's original distribution, requiring only targeted
changes. KL penalties, gradient clipping, and on-policy dynamics have limited
effect on the sparsity pattern. These findings shed new light on how RL adapts
models: not by shifting all weights, but by focusing training on a small,
consistently updated subnetwork. This insight enables more efficient RL methods
and reframes sparsity through the lens of the lottery ticket hypothesis.

</details>


### [124] [Probabilistic Graphical Models: A Concise Tutorial](https://arxiv.org/abs/2507.17116)
*Jacqueline Maasch,Willie Neiswanger,Stefano Ermon,Volodymyr Kuleshov*

Main category: cs.LG

TL;DR: A concise introduction to probabilistic graphical modeling, covering representation, learning, and inference.


<details>
  <summary>Details</summary>
Motivation: Probabilistic graphical modeling uses probability distributions to describe the world, make predictions, and support decision-making under uncertainty. It bridges probability and graph theory, providing compact representations of joint probability distributions for probabilistic reasoning.

Method: review of basic probability and graph theory, exploration of three dominant themes: representation of multivariate distributions in graphs, algorithms for learning, and algorithms for inference.

Result: yielding powerful generative models for probabilistic reasoning.

Conclusion: This tutorial provides a concise introduction to the formalisms, methods, and applications of probabilistic graphical modeling.

Abstract: Probabilistic graphical modeling is a branch of machine learning that uses
probability distributions to describe the world, make predictions, and support
decision-making under uncertainty. Underlying this modeling framework is an
elegant body of theory that bridges two mathematical traditions: probability
and graph theory. This framework provides compact yet expressive
representations of joint probability distributions, yielding powerful
generative models for probabilistic reasoning.
  This tutorial provides a concise introduction to the formalisms, methods, and
applications of this modeling framework. After a review of basic probability
and graph theory, we explore three dominant themes: (1) the representation of
multivariate distributions in the intuitive visual language of graphs, (2)
algorithms for learning model parameters and graphical structures from data,
and (3) algorithms for inference, both exact and approximate.

</details>


### [125] [Computer Vision for Real-Time Monkeypox Diagnosis on Embedded Systems](https://arxiv.org/abs/2507.17123)
*Jacob M. Delgado-López,Ricardo A. Morell-Rodriguez,Sebastián O. Espinosa-Del Rosario,Wilfredo E. Lugo-Beauchamp*

Main category: cs.LG

TL;DR: 本研究提出了一种人工智能驱动的诊断工具，该工具利用 NVIDIA Jetson Orin Nano 上的预训练 MobileNetV2 架构，用于猴痘的二元分类。


<details>
  <summary>Details</summary>
Motivation: 在资源有限的环境中，快速诊断猴痘等传染病对于有效的控制和治疗至关重要。

Method: 利用预训练的 MobileNetV2 架构进行二元分类，使用 TensorRT 框架加速 FP32 的推理，并执行 FP16 和 INT8 格式的训练后量化。

Result: 该模型在开源猴痘皮肤病变数据集上实现了 93.07% 的 F1 分数。TensorRT 的混合精度能力实现了这些优化，从而减小了模型尺寸，提高了推理速度，并将功耗降低了大约两倍，同时保持了原始精度。功耗分析证实，优化后的模型在推理过程中使用的能量明显更少。

Conclusion: 该诊断工具是一种高效、可扩展且节能的解决方案，旨在解决服务欠发达地区的诊断挑战，为在低资源医疗环境中的更广泛应用铺平道路。

Abstract: The rapid diagnosis of infectious diseases, such as monkeypox, is crucial for
effective containment and treatment, particularly in resource-constrained
environments. This study presents an AI-driven diagnostic tool developed for
deployment on the NVIDIA Jetson Orin Nano, leveraging the pre-trained
MobileNetV2 architecture for binary classification. The model was trained on
the open-source Monkeypox Skin Lesion Dataset, achieving a 93.07% F1-Score,
which reflects a well-balanced performance in precision and recall. To optimize
the model, the TensorRT framework was used to accelerate inference for FP32 and
to perform post-training quantization for FP16 and INT8 formats. TensorRT's
mixed-precision capabilities enabled these optimizations, which reduced the
model size, increased inference speed, and lowered power consumption by
approximately a factor of two, all while maintaining the original accuracy.
Power consumption analysis confirmed that the optimized models used
significantly less energy during inference, reinforcing their suitability for
deployment in resource-constrained environments. The system was deployed with a
Wi-Fi Access Point (AP) hotspot and a web-based interface, enabling users to
upload and analyze images directly through connected devices such as mobile
phones. This setup ensures simple access and seamless connectivity, making the
tool practical for real-world applications. These advancements position the
diagnostic tool as an efficient, scalable, and energy-conscious solution to
address diagnosis challenges in underserved regions, paving the way for broader
adoption in low-resource healthcare settings.

</details>


### [126] [Model Compression Engine for Wearable Devices Skin Cancer Diagnosis](https://arxiv.org/abs/2507.17125)
*Jacob M. Delgado-López,Andrea P. Seda-Hernandez,Juan D. Guadalupe-Rosado,Luis E. Fernandez Ramirez,Miguel Giboyeaux-Camilo,Wilfredo E. Lugo-Beauchamp*

Main category: cs.LG

TL;DR: AI-driven diagnostic tool for skin cancer detection on edge devices shows high performance and energy efficiency, suitable for resource-limited settings.


<details>
  <summary>Details</summary>
Motivation: Early skin cancer detection is challenging, especially in resource-limited settings.

Method: Transfer learning with MobileNetV2, compressed with TensorRT for NVIDIA Jetson Orin Nano deployment.

Result: Achieved 87.18% F1-Score, 93.18% precision, and 81.91% recall. Model size reduced by up to 0.41, with improved inference speed and energy consumption decreased by up to 0.93 in INT8 precision.

Conclusion: Optimized AI systems can revolutionize healthcare diagnostics, bridging the gap between advanced technology and underserved regions.

Abstract: Skin cancer is one of the most prevalent and preventable types of cancer, yet
its early detection remains a challenge, particularly in resource-limited
settings where access to specialized healthcare is scarce. This study proposes
an AI-driven diagnostic tool optimized for embedded systems to address this
gap. Using transfer learning with the MobileNetV2 architecture, the model was
adapted for binary classification of skin lesions into "Skin Cancer" and
"Other." The TensorRT framework was employed to compress and optimize the model
for deployment on the NVIDIA Jetson Orin Nano, balancing performance with
energy efficiency. Comprehensive evaluations were conducted across multiple
benchmarks, including model size, inference speed, throughput, and power
consumption. The optimized models maintained their performance, achieving an
F1-Score of 87.18% with a precision of 93.18% and recall of 81.91%.
Post-compression results showed reductions in model size of up to 0.41, along
with improvements in inference speed and throughput, and a decrease in energy
consumption of up to 0.93 in INT8 precision. These findings validate the
feasibility of deploying high-performing, energy-efficient diagnostic tools on
resource-constrained edge devices. Beyond skin cancer detection, the
methodologies applied in this research have broader applications in other
medical diagnostics and domains requiring accessible, efficient AI solutions.
This study underscores the potential of optimized AI systems to revolutionize
healthcare diagnostics, thereby bridging the divide between advanced technology
and underserved regions.

</details>


### [127] [Enabling Self-Improving Agents to Learn at Test Time With Human-In-The-Loop Guidance](https://arxiv.org/abs/2507.17131)
*Yufei He,Ruoyu Li,Alex Chen,Yue Liu,Yulin Chen,Yuan Sui,Cheng Chen,Yi Zhu,Luca Luo,Frank Yang,Bryan Hooi*

Main category: cs.LG

TL;DR: ARIA is an LLM agent framework that continuously learns updated domain knowledge at test time, improving adaptability and accuracy in dynamic environments. It is deployed in TikTok Pay.


<details>
  <summary>Details</summary>
Motivation: LLM agents struggle in environments with frequently changing rules and domain knowledge, and current approaches are insufficient for adapting to new knowledge during operation.

Method: The Adaptive Reflective Interactive Agent (ARIA) framework uses structured self-dialogue to assess uncertainty, identify knowledge gaps, request explanations from experts, and update a timestamped knowledge repository.

Result: ARIA demonstrates significant improvements in adaptability and accuracy compared to baselines and is deployed within TikTok Pay serving over 150 million monthly active users.

Conclusion: The ARIA framework significantly improves adaptability and accuracy in dynamic knowledge tasks, as demonstrated by evaluations and a real-world deployment within TikTok Pay.

Abstract: Large language model (LLM) agents often struggle in environments where rules
and required domain knowledge frequently change, such as regulatory compliance
and user risk screening. Current approaches, like offline fine-tuning and
standard prompting, are insufficient because they cannot effectively adapt to
new knowledge during actual operation. To address this limitation, we propose
the Adaptive Reflective Interactive Agent (ARIA), an LLM agent framework
designed specifically to continuously learn updated domain knowledge at test
time. ARIA assesses its own uncertainty through structured self-dialogue,
proactively identifying knowledge gaps and requesting targeted explanations or
corrections from human experts. It then systematically updates an internal,
timestamped knowledge repository with provided human guidance, detecting and
resolving conflicting or outdated knowledge through comparisons and
clarification queries. We evaluate ARIA on the realistic customer due diligence
name screening task on TikTok Pay, alongside publicly available dynamic
knowledge tasks. Results demonstrate significant improvements in adaptability
and accuracy compared to baselines using standard offline fine-tuning and
existing self-improving agents. ARIA is deployed within TikTok Pay serving over
150 million monthly active users, confirming its practicality and effectiveness
for operational use in rapidly evolving environments.

</details>


### [128] [SADA: Stability-guided Adaptive Diffusion Acceleration](https://arxiv.org/abs/2507.17135)
*Ting Jiang,Yixiao Wang,Hancheng Ye,Zishan Shao,Jingwei Sun,Jingyang Zhang,Zekai Chen,Jianyi Zhang,Yiran Chen,Hai Li*

Main category: cs.LG

TL;DR: SADA accelerates sampling of ODE-based generative models with minimal fidelity degradation by unifying step-wise and token-wise sparsity decisions.


<details>
  <summary>Details</summary>
Motivation: Existing training-free acceleration strategies demonstrate low faithfulness compared to the original baseline because different prompts correspond to varying denoising trajectory, and such methods do not consider the underlying ODE formulation and its numerical solution.

Method: SADA unifies step-wise and token-wise sparsity decisions via a single stability criterion to accelerate sampling of ODE-based generative models, adaptively allocates sparsity based on the sampling trajectory, and introduces principled approximation schemes that leverage the precise gradient information from the numerical ODE solver.

Result: SADA achieves consistent >= 1.8x speedups with minimal fidelity degradation (LPIPS <= 0.10 and FID <= 4.5) compared to unmodified baselines, significantly outperforming prior methods. It accelerates ControlNet without any modifications and speeds up MusicLDM by 1.8x with ~0.01 spectrogram LPIPS.

Conclusion: SADA achieves significant speedups with minimal fidelity degradation and adapts seamlessly to other pipelines and modalities.

Abstract: Diffusion models have achieved remarkable success in generative tasks but
suffer from high computational costs due to their iterative sampling process
and quadratic attention costs. Existing training-free acceleration strategies
that reduce per-step computation cost, while effectively reducing sampling
time, demonstrate low faithfulness compared to the original baseline. We
hypothesize that this fidelity gap arises because (a) different prompts
correspond to varying denoising trajectory, and (b) such methods do not
consider the underlying ODE formulation and its numerical solution. In this
paper, we propose Stability-guided Adaptive Diffusion Acceleration (SADA), a
novel paradigm that unifies step-wise and token-wise sparsity decisions via a
single stability criterion to accelerate sampling of ODE-based generative
models (Diffusion and Flow-matching). For (a), SADA adaptively allocates
sparsity based on the sampling trajectory. For (b), SADA introduces principled
approximation schemes that leverage the precise gradient information from the
numerical ODE solver. Comprehensive evaluations on SD-2, SDXL, and Flux using
both EDM and DPM++ solvers reveal consistent $\ge 1.8\times$ speedups with
minimal fidelity degradation (LPIPS $\leq 0.10$ and FID $\leq 4.5$) compared to
unmodified baselines, significantly outperforming prior methods. Moreover, SADA
adapts seamlessly to other pipelines and modalities: It accelerates ControlNet
without any modifications and speeds up MusicLDM by $1.8\times$ with $\sim
0.01$ spectrogram LPIPS.

</details>


### [129] [PICore: Physics-Informed Unsupervised Coreset Selection for Data Efficient Neural Operator Training](https://arxiv.org/abs/2507.17151)
*Anirudh Satheesh,Anant Khandelwal,Mucong Ding,Radu Balan*

Main category: cs.LG

TL;DR: PICore是一种无监督coreset选择框架，它可以识别信息量最大的训练样本，而不需要访问ground-truth PDE解决方案。


<details>
  <summary>Details</summary>
Motivation: 训练神经算子有两个主要的瓶颈：它们需要大量的训练数据来学习这些映射，并且这些数据需要被标记，这只能通过使用数值求解器的昂贵模拟来访问。

Method: 提出了PICore，一个无监督coreset选择框架，它通过它们对算子学习的潜在贡献来选择未标记的输入，而不需要访问ground-truth PDE解决方案，利用一个物理信息损失来选择未标记的输入。

Result: PICore减少了注释成本，减少了训练时间。

Conclusion: PICore在保证精度变化不大的前提下，相对于有监督coreset选择方法，在四个不同的PDE基准和多个coreset选择策略中，训练效率平均提高了78%。

Abstract: Neural operators offer a powerful paradigm for solving partial differential
equations (PDEs) that cannot be solved analytically by learning mappings
between function spaces. However, there are two main bottlenecks in training
neural operators: they require a significant amount of training data to learn
these mappings, and this data needs to be labeled, which can only be accessed
via expensive simulations with numerical solvers. To alleviate both of these
issues simultaneously, we propose PICore, an unsupervised coreset selection
framework that identifies the most informative training samples without
requiring access to ground-truth PDE solutions. PICore leverages a
physics-informed loss to select unlabeled inputs by their potential
contribution to operator learning. After selecting a compact subset of inputs,
only those samples are simulated using numerical solvers to generate labels,
reducing annotation costs. We then train the neural operator on the reduced
labeled dataset, significantly decreasing training time as well. Across four
diverse PDE benchmarks and multiple coreset selection strategies, PICore
achieves up to 78% average increase in training efficiency relative to
supervised coreset selection methods with minimal changes in accuracy. We
provide code at https://github.com/Asatheesh6561/PICore.

</details>


### [130] [Tabular Diffusion based Actionable Counterfactual Explanations for Network Intrusion Detection](https://arxiv.org/abs/2507.17161)
*Vinura Galwaduge,Jagath Samarabandu*

Main category: cs.LG

TL;DR: 该论文提出了一种用于网络入侵检测的可操作的反事实解释框架，该框架能够生成全局规则以有效过滤攻击。


<details>
  <summary>Details</summary>
Motivation: 现代网络入侵检测系统 (NIDS) 频繁地利用复杂深度学习模型的预测能力。然而，这种深度学习方法的“黑盒”性质增加了一层不透明性，阻碍了对检测决策的正确理解和信任，并妨碍了及时采取应对此类攻击的措施。

Method: 该论文提出了一种新的基于扩散的反事实解释框架。

Result: 该论文提出的方法以更有效的方式减少了生成解释的时间，从而在测试的反事实解释算法中提供了最小的、多样化的反事实解释。这些全局反事实规则显示出有效过滤传入攻击查询的能力，这对于有效的入侵检测和防御机制至关重要。

Conclusion: 该论文提出了一种新的基于扩散的反事实解释框架，该框架能够为网络入侵攻击提供可操作的解释，并通过总结这些解释来创建一组全局规则，从而有效地过滤掉传入的攻击查询。

Abstract: Modern network intrusion detection systems (NIDS) frequently utilize the
predictive power of complex deep learning models. However, the "black-box"
nature of such deep learning methods adds a layer of opaqueness that hinders
the proper understanding of detection decisions, trust in the decisions and
prevent timely countermeasures against such attacks. Explainable AI (XAI)
methods provide a solution to this problem by providing insights into the
causes of the predictions. The majority of the existing XAI methods provide
explanations which are not convenient to convert into actionable
countermeasures. In this work, we propose a novel diffusion-based
counterfactual explanation framework that can provide actionable explanations
for network intrusion attacks. We evaluated our proposed algorithm against
several other publicly available counterfactual explanation algorithms on 3
modern network intrusion datasets. To the best of our knowledge, this work also
presents the first comparative analysis of existing counterfactual explanation
algorithms within the context of network intrusion detection systems. Our
proposed method provide minimal, diverse counterfactual explanations out of the
tested counterfactual explanation algorithms in a more efficient manner by
reducing the time to generate explanations. We also demonstrate how
counterfactual explanations can provide actionable explanations by summarizing
them to create a set of global rules. These rules are actionable not only at
instance level but also at the global level for intrusion attacks. These global
counterfactual rules show the ability to effectively filter out incoming attack
queries which is crucial for efficient intrusion detection and defense
mechanisms.

</details>


### [131] [Met$^2$Net: A Decoupled Two-Stage Spatio-Temporal Forecasting Model for Complex Meteorological Systems](https://arxiv.org/abs/2507.17189)
*Shaohan Li,Hao Yang,Min Chen,Xiaolin Qin*

Main category: cs.LG

TL;DR: Proposes an implicit two-stage training method with self-attention to improve weather prediction accuracy by addressing representation inconsistency and variable dependency issues.


<details>
  <summary>Details</summary>
Motivation: End-to-end methods for weather prediction face limitations of representation inconsistency in multivariable integration and struggle to effectively capture the dependency between variables.

Method: An implicit two-stage training method with separate encoders and decoders for each variable, along with a self-attention mechanism for multivariable fusion in the latent space.

Result: The proposed method achieves state-of-the-art performance, reducing MSE for near-surface air temperature and relative humidity predictions by 28.82% and 23.39%, respectively.

Conclusion: The proposed method achieves state-of-the-art performance, reducing MSE for near-surface air temperature and relative humidity predictions by 28.82% and 23.39%, respectively.

Abstract: The increasing frequency of extreme weather events due to global climate
change urges accurate weather prediction. Recently, great advances have been
made by the \textbf{end-to-end methods}, thanks to deep learning techniques,
but they face limitations of \textit{representation inconsistency} in
multivariable integration and struggle to effectively capture the dependency
between variables, which is required in complex weather systems. Treating
different variables as distinct modalities and applying a \textbf{two-stage
training approach} from multimodal models can partially alleviate this issue,
but due to the inconformity in training tasks between the two stages, the
results are often suboptimal. To address these challenges, we propose an
implicit two-stage training method, configuring separate encoders and decoders
for each variable. In detailed, in the first stage, the Translator is frozen
while the Encoders and Decoders learn a shared latent space, in the second
stage, the Encoders and Decoders are frozen, and the Translator captures
inter-variable interactions for prediction. Besides, by introducing a
self-attention mechanism for multivariable fusion in the latent space, the
performance achieves further improvements. Empirically, extensive experiments
show the state-of-the-art performance of our method. Specifically, it reduces
the MSE for near-surface air temperature and relative humidity predictions by
28.82\% and 23.39\%, respectively. The source code is available at
https://github.com/ShremG/Met2Net.

</details>


### [132] [Filter-And-Refine: A MLLM Based Cascade System for Industrial-Scale Video Content Moderation](https://arxiv.org/abs/2507.17204)
*Zixuan Wang,Jinghao Shi,Hanzhong Liang,Xiang Shen,Vera Wen,Zhiqian Chen,Yifan Wu,Zhixin Zhang,Hongyu Xiong*

Main category: cs.LG

TL;DR: This paper introduces an efficient and cost-effective MLLM-based approach for content moderation, improving F1 score by 66.50% and increasing moderation volume by 41% while reducing computational cost to 1.5%.


<details>
  <summary>Details</summary>
Motivation: traditional video classification models struggle with complicated scenarios such as implicit harmful content and contextual ambiguity. The high computational cost of MLLMs makes full-scale deployment impractical, and adapting generative models for discriminative classification remains an open research problem.

Method: introduce an efficient method to transform a generative MLLM into a multimodal classifier using minimal discriminative training data. propose a router-ranking cascade system that integrates MLLMs with a lightweight router model.

Result: MLLM-based approach improves F1 score by 66.50% over traditional classifiers while requiring only 2% of the fine-tuning data. The cascading deployment reduces computational cost to only 1.5% of direct full-scale deployment, and increases automatic content moderation volume by 41%.

Conclusion: MLLM-based approach improves F1 score by 66.50% over traditional classifiers while requiring only 2% of the fine-tuning data. The cascading deployment reduces computational cost to only 1.5% of direct full-scale deployment, and increases automatic content moderation volume by 41%.

Abstract: Effective content moderation is essential for video platforms to safeguard
user experience and uphold community standards. While traditional video
classification models effectively handle well-defined moderation tasks, they
struggle with complicated scenarios such as implicit harmful content and
contextual ambiguity. Multimodal large language models (MLLMs) offer a
promising solution to these limitations with their superior cross-modal
reasoning and contextual understanding. However, two key challenges hinder
their industrial adoption. First, the high computational cost of MLLMs makes
full-scale deployment impractical. Second, adapting generative models for
discriminative classification remains an open research problem. In this paper,
we first introduce an efficient method to transform a generative MLLM into a
multimodal classifier using minimal discriminative training data. To enable
industry-scale deployment, we then propose a router-ranking cascade system that
integrates MLLMs with a lightweight router model. Offline experiments
demonstrate that our MLLM-based approach improves F1 score by 66.50% over
traditional classifiers while requiring only 2% of the fine-tuning data. Online
evaluations show that our system increases automatic content moderation volume
by 41%, while the cascading deployment reduces computational cost to only 1.5%
of direct full-scale deployment.

</details>


### [133] [Dataset Distillation as Data Compression: A Rate-Utility Perspective](https://arxiv.org/abs/2507.17221)
*Youneng Bao,Yiping Liu,Zhuo Chen,Yongsheng Liang,Mu Li,Kede Ma*

Main category: cs.LG

TL;DR: 提出了一种用于数据集蒸馏的联合率-效用优化方法，该方法在相当的精度下实现了比标准蒸馏高 170 倍的压缩率，并且始终能够建立更好的率-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习越来越多地需要越来越大的数据集和模型，从而导致过高的计算和存储需求。数据集蒸馏通过将原始数据集压缩成一小组合成样本来缓解这种情况，同时保留其全部效用。然而，现有的方法要么在固定的存储预算下最大化性能，要么寻求合适的合成数据表示以消除冗余，而没有联合优化这两个目标。

Method: 提出了一种用于数据集蒸馏的联合率-效用优化方法。该方法将合成样本参数化为由极轻量级网络解码的可优化潜在代码。该方法估计量化潜在变量的香农熵作为速率度量，并将任何现有的蒸馏损失作为效用度量，并通过拉格朗日乘数进行权衡。

Result: 该方法在不同的 bpc 预算、蒸馏损失和骨干架构中，始终能够建立更好的率-效用权衡。在 CIFAR-10、CIFAR-100 和 ImageNet-128 上，该方法在相当的精度下实现了比标准蒸馏高 170 倍的压缩率。

Conclusion: 在 CIFAR-10、CIFAR-100 和 ImageNet-128 上，该方法在相当的精度下实现了比标准蒸馏高 170 倍的压缩率。在不同的 bpc 预算、蒸馏损失和骨干架构中，该方法始终能够建立更好的率-效用权衡。

Abstract: Driven by the ``scale-is-everything'' paradigm, modern machine learning
increasingly demands ever-larger datasets and models, yielding prohibitive
computational and storage requirements. Dataset distillation mitigates this by
compressing an original dataset into a small set of synthetic samples, while
preserving its full utility. Yet, existing methods either maximize performance
under fixed storage budgets or pursue suitable synthetic data representations
for redundancy removal, without jointly optimizing both objectives. In this
work, we propose a joint rate-utility optimization method for dataset
distillation. We parameterize synthetic samples as optimizable latent codes
decoded by extremely lightweight networks. We estimate the Shannon entropy of
quantized latents as the rate measure and plug any existing distillation loss
as the utility measure, trading them off via a Lagrange multiplier. To enable
fair, cross-method comparisons, we introduce bits per class (bpc), a precise
storage metric that accounts for sample, label, and decoder parameter costs. On
CIFAR-10, CIFAR-100, and ImageNet-128, our method achieves up to $170\times$
greater compression than standard distillation at comparable accuracy. Across
diverse bpc budgets, distillation losses, and backbone architectures, our
approach consistently establishes better rate-utility trade-offs.

</details>


### [134] [P3SL: Personalized Privacy-Preserving Split Learning on Heterogeneous Edge Devices](https://arxiv.org/abs/2507.17228)
*Wei Fan,JinYi Yoon,Xiaochang Li,Huajie Shao,Bo Ji*

Main category: cs.LG

TL;DR: P3SL is a Personalized Privacy-Preserving Split Learning framework designed for heterogeneous, resource-constrained edge device systems


<details>
  <summary>Details</summary>
Motivation: Split Learning (SL) encounters significant challenges in heterogeneous environments where devices vary in computing resources, communication capabilities, environmental conditions, and privacy requirements. Although recent studies have explored heterogeneous SL frameworks that optimize split points for devices with varying resource constraints, they often neglect personalized privacy requirements and local model customization under varying environmental conditions.

Method: We adopt a bi-level optimization technique that empowers clients to determine their own optimal personalized split points without sharing private sensitive information (i.e., computational resources, environmental conditions, privacy requirements) with the server.

Result: We propose P3SL, a Personalized Privacy-Preserving Split Learning framework designed for heterogeneous, resource-constrained edge device systems. The key contributions of this work are twofold. First, we design a personalized sequential split learning pipeline that allows each client to achieve customized privacy protection and maintain personalized local models tailored to their computational resources, environmental conditions, and privacy needs. Second, we adopt a bi-level optimization technique that empowers clients to determine their own optimal personalized split points without sharing private sensitive information (i.e., computational resources, environmental conditions, privacy requirements) with the server. This approach balances energy consumption and privacy leakage risks while maintaining high model accuracy.

Conclusion: We implement and evaluate P3SL on a testbed consisting of 7 devices including 4 Jetson Nano P3450 devices, 2 Raspberry Pis, and 1 laptop, using diverse model architectures and datasets under varying environmental conditions.

Abstract: Split Learning (SL) is an emerging privacy-preserving machine learning
technique that enables resource constrained edge devices to participate in
model training by partitioning a model into client-side and server-side
sub-models. While SL reduces computational overhead on edge devices, it
encounters significant challenges in heterogeneous environments where devices
vary in computing resources, communication capabilities, environmental
conditions, and privacy requirements. Although recent studies have explored
heterogeneous SL frameworks that optimize split points for devices with varying
resource constraints, they often neglect personalized privacy requirements and
local model customization under varying environmental conditions. To address
these limitations, we propose P3SL, a Personalized Privacy-Preserving Split
Learning framework designed for heterogeneous, resource-constrained edge device
systems. The key contributions of this work are twofold. First, we design a
personalized sequential split learning pipeline that allows each client to
achieve customized privacy protection and maintain personalized local models
tailored to their computational resources, environmental conditions, and
privacy needs. Second, we adopt a bi-level optimization technique that empowers
clients to determine their own optimal personalized split points without
sharing private sensitive information (i.e., computational resources,
environmental conditions, privacy requirements) with the server. This approach
balances energy consumption and privacy leakage risks while maintaining high
model accuracy. We implement and evaluate P3SL on a testbed consisting of 7
devices including 4 Jetson Nano P3450 devices, 2 Raspberry Pis, and 1 laptop,
using diverse model architectures and datasets under varying environmental
conditions.

</details>
