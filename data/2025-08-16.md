<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 52]
- [cs.CV](#cs.CV) [Total: 50]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.LG](#cs.LG) [Total: 51]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry](https://arxiv.org/abs/2508.09991)
*Lovedeep Gondara,Gregory Arbour,Raymond Ng,Jonathan Simkin,Shebnum Devji*

Main category: cs.CL

TL;DR: 从临床文件中自动提取数据具有提高医疗效率的巨大潜力，但部署自然语言处理 (NLP) 解决方案带来实际挑战。


<details>
  <summary>Details</summary>
Motivation: 从临床文件中自动提取数据具有提高医疗效率的巨大潜力，但部署自然语言处理 (NLP) 解决方案带来实际挑战。

Method: 在不列颠哥伦比亚癌症登记处 (BCCR) 实施各种 NLP 模型以进行信息提取和分类任务。

Result: 强调了基于明确的业务目标定义问题（而不仅仅是技术准确性）至关重要，采用迭代开发方法，并在从一开始就建立涉及领域专家、最终用户和 ML 专家的深度跨学科协作和共同设计。

Conclusion: 分享了在不列颠哥伦比亚癌症登记处 (BCCR) 实施各种 NLP 模型以进行信息提取和分类任务的关键经验教训，强调了实践中的考虑，这些考虑可以推广到癌症登记处之外，并为寻求成功实施 AI/NLP 解决方案以加强数据管理流程并最终改善患者护理和公共健康结果的医疗保健组织提供指导。

Abstract: Automating data extraction from clinical documents offers significant
potential to improve efficiency in healthcare settings, yet deploying Natural
Language Processing (NLP) solutions presents practical challenges. Drawing upon
our experience implementing various NLP models for information extraction and
classification tasks at the British Columbia Cancer Registry (BCCR), this paper
shares key lessons learned throughout the project lifecycle. We emphasize the
critical importance of defining problems based on clear business objectives
rather than solely technical accuracy, adopting an iterative approach to
development, and fostering deep interdisciplinary collaboration and co-design
involving domain experts, end-users, and ML specialists from inception. Further
insights highlight the need for pragmatic model selection (including hybrid
approaches and simpler methods where appropriate), rigorous attention to data
quality (representativeness, drift, annotation), robust error mitigation
strategies involving human-in-the-loop validation and ongoing audits, and
building organizational AI literacy. These practical considerations,
generalizable beyond cancer registries, provide guidance for healthcare
organizations seeking to successfully implement AI/NLP solutions to enhance
data management processes and ultimately improve patient care and public health
outcomes.

</details>


### [2] [A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain](https://arxiv.org/abs/2508.09993)
*Hugo Massaroli,Leonardo Iara,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CL

TL;DR: The paper proposes a transparent evaluation protocol for benchmarking the fairness of open-source LLMs using smart contracts on the Internet Computer Protocol (ICP) blockchain. It benchmarks Llama, DeepSeek, and Mistral models, evaluates social bias and extends analysis with multilingual evaluation, revealing cross-linguistic disparities. All code and results are open source.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly deployed in realworld applications, yet concerns about their fairness persist especially in highstakes domains like criminal justice, education, healthcare, and finance.

Method: executing onchain HTTP requests to hosted Hugging Face endpoints and storing datasets, prompts, and metrics directly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the PISA dataset, evaluate structured Context Association Metrics derived from the StereoSet dataset and extend our analysis with a multilingual evaluation across English, Spanish, and Portuguese using the Kaleidoscope benchmark

Result: verifiable, immutable, and reproducible evaluations. All code and results are open source, enabling community audits and longitudinal fairness tracking across model versions.

Conclusion: This paper introduces transparent evaluation protocol for benchmarking the fairness of opensource LLMs using smart contracts on the Internet Computer Protocol (ICP) blockchain, revealing cross-linguistic disparities.

Abstract: Large language models (LLMs) are increasingly deployed in realworld
applications, yet concerns about their fairness persist especially in
highstakes domains like criminal justice, education, healthcare, and finance.
This paper introduces transparent evaluation protocol for benchmarking the
fairness of opensource LLMs using smart contracts on the Internet Computer
Protocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable,
immutable, and reproducible evaluations by executing onchain HTTP requests to
hosted Hugging Face endpoints and storing datasets, prompts, and metrics
directly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the
PISA dataset for academic performance prediction (OECD, 2018), a dataset
suitable for fairness evaluation using statistical parity and equal opportunity
metrics (Hardt et al., 2016). We also evaluate structured Context Association
Metrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure
social bias in contextual associations. We further extend our analysis with a
multilingual evaluation across English, Spanish, and Portuguese using the
Kaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic
disparities. All code and results are open source, enabling community audits
and longitudinal fairness tracking across model versions.

</details>


### [3] [Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling](https://arxiv.org/abs/2508.09997)
*Johannes Schneider,Béatrice S. Hasler,Michaela Varrone,Fabian Hoya,Thomas Schroffenegger,Dana-Kristin Mah,Karl Peböck*

Main category: cs.CL

TL;DR: 该研究使用主题建模和 LLM 分析了课堂中学生、教师和 ChatGPT 的互动数据，对消息进行了内容和任务分类，为 GenAI 的使用提供了支持，并提出了未来研究的问题。


<details>
  <summary>Details</summary>
Motivation: 先前的研究主要缺乏内容或主题分类，而任务分类在教育中更为普遍，但大多数没有 K-12 的真实世界数据支持。

Method: 该研究采用了一种新颖、简单的主题建模方法，并直接应用最先进的 LLM 进行分析，通过明确的指令比以前的方法实现了更好的人工对齐的分层主题结构。

Result: 该研究对学生、教师和 ChatGPT 生成的 17,000 多条消息进行了分类，包括内容和任务两个维度，并提供了示例提示和高层次的概述以及切实的见解。分析产生了许多新的应用。

Conclusion: 该研究的结果为研究人员、教师和学生丰富 GenAI 的使用提供了支持，同时讨论也强调了未来研究的一些问题。

Abstract: We analyze anonymous interaction data of minors in class-rooms spanning
several months, schools, and subjects employing a novel, simple topic modeling
approach. Specifically, we categorize more than 17,000 messages generated by
students, teachers, and ChatGPT in two dimensions: content (such as nature and
people) and tasks (such as writing and explaining). Our hierarchical
categorization done separately for each dimension includes exemplary prompts,
and provides both a high-level overview as well as tangible insights. Prior
works mostly lack a content or thematic categorization. While task
categorizations are more prevalent in education, most have not been supported
by real-world data for K-12. In turn, it is not surprising that our analysis
yielded a number of novel applications. In deriving these insights, we found
that many of the well-established classical and emerging computational methods,
i.e., topic modeling, for analysis of large amounts of texts underperform,
leading us to directly apply state-of-the-art LLMs with adequate pre-processing
to achieve hierarchical topic structures with better human alignment through
explicit instructions than prior approaches. Our findings support fellow
researchers, teachers and students in enriching the usage of GenAI, while our
discussion also highlights a number of concerns and open questions for future
research.

</details>


### [4] [INTIMA: A Benchmark for Human-AI Companionship Behavior](https://arxiv.org/abs/2508.09998)
*Lucie-Aimée Kaffee,Giada Pistilli,Yacine Jernite*

Main category: cs.CL

TL;DR: 我们引入了交互和机器依恋基准 (INTIMA)，这是一个用于评估语言模型中陪伴行为的基准。


<details>
  <summary>Details</summary>
Motivation: 用户与人工智能系统建立情感联系的人工智能陪伴已经成为一种重要的模式，具有积极但令人担忧的含义。

Method: 从心理学理论和用户数据中，我们开发了一个包含四个类别和 368 个目标提示的 31 种行为的分类法。对这些提示的响应被评估为陪伴强化、边界维护或中性。

Result: 将 INTIMA 应用于 Gemma-3、Phi-4、o3-mini 和 Claude-4 表明，陪伴强化行为在所有模型中仍然更为常见，尽管我们观察到模型之间存在显着差异。

Conclusion: 不同商业提供商在基准测试中更敏感的部分优先考虑不同的类别，这令人担忧，因为适当的边界设置和情感支持对于用户福祉都很重要。这些发现强调需要更一致的方法来处理情感互动。

Abstract: AI companionship, where users develop emotional bonds with AI systems, has
emerged as a significant pattern with positive but also concerning
implications. We introduce Interactions and Machine Attachment Benchmark
(INTIMA), a benchmark for evaluating companionship behaviors in language
models. Drawing from psychological theories and user data, we develop a
taxonomy of 31 behaviors across four categories and 368 targeted prompts.
Responses to these prompts are evaluated as companionship-reinforcing,
boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini,
and Claude-4 reveals that companionship-reinforcing behaviors remain much more
common across all models, though we observe marked differences between models.
Different commercial providers prioritize different categories within the more
sensitive parts of the benchmark, which is concerning since both appropriate
boundary-setting and emotional support matter for user well-being. These
findings highlight the need for more consistent approaches to handling
emotionally charged interactions.

</details>


### [5] [XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs](https://arxiv.org/abs/2508.09999)
*Yuzhuo Xiao,Zeyu Han,Yuhan Wang,Huaizu Jiang*

Main category: cs.CL

TL;DR: This paper introduces XFacta, a new dataset for multimodal misinformation detection, and evaluates various MLLM-based strategies. It also proposes a semi-automatic detection framework to keep the dataset up-to-date.


<details>
  <summary>Details</summary>
Motivation: The rapid spread of multimodal misinformation on social media calls for more effective and robust detection methods. However, it remains unclear exactly where the bottleneck of existing approaches lies (evidence retrieval v.s. reasoning), hindering the further advances in this field. On the dataset side, existing benchmarks either contain outdated events, leading to evaluation bias due to discrepancies with contemporary social media scenarios as MLLMs can simply memorize these events, or artificially synthetic, failing to reflect real-world misinformation patterns. Additionally, it lacks comprehensive analyses of MLLM-based model design strategies.

Method: introducing XFacta, a contemporary, real-world dataset that is better suited for evaluating MLLM-based detectors. We systematically evaluate various MLLM-based misinformation detection strategies, assessing models across different architectures and scales, as well as benchmarking against existing detection methods. Building on these analyses, we further enable a semi-automatic detection-in-the-loop framework that continuously updates XFacta with new content to maintain its contemporary relevance.

Result: We introduce XFacta, a contemporary, real-world dataset that is better suited for evaluating MLLM-based detectors. We systematically evaluate various MLLM-based misinformation detection strategies, assessing models across different architectures and scales, as well as benchmarking against existing detection methods. Building on these analyses, we further enable a semi-automatic detection-in-the-loop framework that continuously updates XFacta with new content to maintain its contemporary relevance.

Conclusion: This paper provides valuable insights and practices for advancing the field of multimodal misinformation detection. The code and data have been released.

Abstract: The rapid spread of multimodal misinformation on social media calls for more
effective and robust detection methods. Recent advances leveraging multimodal
large language models (MLLMs) have shown the potential in addressing this
challenge. However, it remains unclear exactly where the bottleneck of existing
approaches lies (evidence retrieval v.s. reasoning), hindering the further
advances in this field. On the dataset side, existing benchmarks either contain
outdated events, leading to evaluation bias due to discrepancies with
contemporary social media scenarios as MLLMs can simply memorize these events,
or artificially synthetic, failing to reflect real-world misinformation
patterns. Additionally, it lacks comprehensive analyses of MLLM-based model
design strategies. To address these issues, we introduce XFacta, a
contemporary, real-world dataset that is better suited for evaluating
MLLM-based detectors. We systematically evaluate various MLLM-based
misinformation detection strategies, assessing models across different
architectures and scales, as well as benchmarking against existing detection
methods. Building on these analyses, we further enable a semi-automatic
detection-in-the-loop framework that continuously updates XFacta with new
content to maintain its contemporary relevance. Our analysis provides valuable
insights and practices for advancing the field of multimodal misinformation
detection. The code and data have been released.

</details>


### [6] [AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification](https://arxiv.org/abs/2508.10000)
*Chenhao Xue,Yuanzhe Jin,Adrian Carrasco-Revilla,Joyraj Chakraborty,Min Chen*

Main category: cs.CL

TL;DR: Utilizing LLMs to generate synthetic data to improve the performance of text classification models. An ensemble algorithm is proposed to select a search strategy according to the characteristics of a class.


<details>
  <summary>Details</summary>
Motivation: The major challenge is the difficulty to collect sufficient data for all text classes when developing text classification models for real world applications.

Method: The authors formulate an automated workflow that searches for input examples that lead to more effective synthetic data for improving the model concerned. They study three search strategies and use experiment results to inform an ensemble algorithm.

Result: Experiments demonstrate that the ensemble approach is more effective than each individual strategy in the automated workflow for improving classification models using LLMs.

Conclusion: This paper introduces an ensemble approach that selects a search strategy based on class characteristics, demonstrating improved effectiveness in enhancing classification models using LLMs compared to individual strategies.

Abstract: When developing text classification models for real world applications, one
major challenge is the difficulty to collect sufficient data for all text
classes. In this work, we address this challenge by utilizing large language
models (LLMs) to generate synthetic data and using such data to improve the
performance of the models without waiting for more real data to be collected
and labelled. As an LLM generates different synthetic data in response to
different input examples, we formulate an automated workflow, which searches
for input examples that lead to more ``effective'' synthetic data for improving
the model concerned. We study three search strategies with an extensive set of
experiments, and use experiment results to inform an ensemble algorithm that
selects a search strategy according to the characteristics of a class. Our
further experiments demonstrate that this ensemble approach is more effective
than each individual strategy in our automated workflow for improving
classification models using LLMs.

</details>


### [7] [HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish](https://arxiv.org/abs/2508.10001)
*Rakesh Thakur,Sneha Sharma,Gauri Chopra*

Main category: cs.CL

TL;DR: Addresses fact-checking in Hinglish with HiFACT dataset and a graph-aware model, outperforming existing baselines.


<details>
  <summary>Details</summary>
Motivation: Existing fact-verification systems largely focus on high-resource, monolingual settings and fail to generalize to real-world political discourse in linguistically diverse regions like India. Given the widespread use of Hinglish by public figures, particularly political figures, and the growing influence of social media on public opinion, there's a critical need for robust, multilingual and context-aware fact-checking tools

Method: a novel graphaware, retrieval-augmented fact-checking model is proposed that combines multilingual contextual encoding, claim-evidence semantic alignment, evidence graph construction, graph neural reasoning, and natural language explanation generation

Result: HiFACTMix outperformed accuracy in comparison to state of art multilingual baselines models and provides faithful justifications for its verdicts

Conclusion: HiFACTMix outperformed accuracy in comparison to state of art multilingual baselines models and provides faithful justifications for its verdicts. This work opens a new direction for multilingual, code-mixed, and politically grounded fact verification research.

Abstract: Fact-checking in code-mixed, low-resource languages such as Hinglish remains
an underexplored challenge in natural language processing. Existing
fact-verification systems largely focus on high-resource, monolingual settings
and fail to generalize to real-world political discourse in linguistically
diverse regions like India. Given the widespread use of Hinglish by public
figures, particularly political figures, and the growing influence of social
media on public opinion, there's a critical need for robust, multilingual and
context-aware fact-checking tools. To address this gap a novel benchmark HiFACT
dataset is introduced with 1,500 realworld factual claims made by 28 Indian
state Chief Ministers in Hinglish, under a highly code-mixed low-resource
setting. Each claim is annotated with textual evidence and veracity labels. To
evaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking
model is proposed that combines multilingual contextual encoding,
claim-evidence semantic alignment, evidence graph construction, graph neural
reasoning, and natural language explanation generation. Experimental results
show that HiFACTMix outperformed accuracy in comparison to state of art
multilingual baselines models and provides faithful justifications for its
verdicts. This work opens a new direction for multilingual, code-mixed, and
politically grounded fact verification research.

</details>


### [8] [User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents](https://arxiv.org/abs/2508.10004)
*Andrés Carvallo,Denis Parra,Peter Brusilovsky,Hernan Valdivieso,Gabriel Rada,Ivania Donoso,Vladimir Araujo*

Main category: cs.CL

TL;DR: attention weights are not helpful for explaining predictions, but visualization affects perceived helpfulness


<details>
  <summary>Details</summary>
Motivation: whether attention weights provide helpful explanations is unclear, and little research has explored how visualizing attention affects its usefulness as an explanation aid

Method: user study to evaluate whether attention-based explanations support users in biomedical document classification and whether there is a preferred way to visualize them

Result: Transformer model (XLNet) classified documents accurately; the attention weights were not perceived as particularly helpful for explaining the predictions. users preferred more intuitive formats, such as text brightness or background color

Conclusion: attention weights were not perceived as particularly helpful for explaining the predictions, but perceived helpfulness is influenced by how they are visually presented

Abstract: The attention mechanism is a core component of the Transformer architecture.
Beyond improving performance, attention has been proposed as a mechanism for
explainability via attention weights, which are associated with input features
(e.g., tokens in a document). In this context, larger attention weights may
imply more relevant features for the model's prediction. In evidence-based
medicine, such explanations could support physicians' understanding and
interaction with AI systems used to categorize biomedical literature. However,
there is still no consensus on whether attention weights provide helpful
explanations. Moreover, little research has explored how visualizing attention
affects its usefulness as an explanation aid. To bridge this gap, we conducted
a user study to evaluate whether attention-based explanations support users in
biomedical document classification and whether there is a preferred way to
visualize them. The study involved medical experts from various disciplines who
classified articles based on study design (e.g., systematic reviews, broad
synthesis, randomized and non-randomized trials). Our findings show that the
Transformer model (XLNet) classified documents accurately; however, the
attention weights were not perceived as particularly helpful for explaining the
predictions. However, this perception varied significantly depending on how
attention was visualized. Contrary to Munzner's principle of visual
effectiveness, which favors precise encodings like bar length, users preferred
more intuitive formats, such as text brightness or background color. While our
results do not confirm the overall utility of attention weights for
explanation, they suggest that their perceived helpfulness is influenced by how
they are visually presented.

</details>


### [9] [Semantic Structure in Large Language Model Embeddings](https://arxiv.org/abs/2508.10003)
*Austin C. Kozlowski,Callin Dai,Andrei Boutyline*

Main category: cs.CL

TL;DR: 大型语言模型中的语义特征以类似于人类语言的方式纠缠在一起，并且语义信息是低维的。


<details>
  <summary>Details</summary>
Motivation: 心理学研究一直发现，人类对不同语义尺度上的词语的评级可以简化为低维形式，而信息损失相对较小。我们发现大型语言模型 (LLM) 的嵌入矩阵中编码的语义关联表现出类似的结构。

Method: 通过反义词对（例如，善良-残酷）定义的语义方向上的词的投影与人类评级高度相关，并且这些投影有效地减少到 LLM 嵌入中的 3 维子空间，与从人类调查反应中得出的模式非常相似。

Result: 在 LLM 嵌入中，由反义词对定义的语义方向上的词的投影与人类评级高度相关，并且可以有效简化为 3 维子空间。沿着一个语义方向移动 tokens 会对几何对齐的特征产生脱靶效应，其大小与它们的余弦相似度成正比。

Conclusion: 大型语言模型 (LLM) 中编码的语义关联表现出与人类语言相似的结构，并且大量的语义信息是低维的。此外，解释这种语义结构可能对于在操纵特征时避免意外后果至关重要。

Abstract: Psychological research consistently finds that human ratings of words across
diverse semantic scales can be reduced to a low-dimensional form with
relatively little information loss. We find that the semantic associations
encoded in the embedding matrices of large language models (LLMs) exhibit a
similar structure. We show that the projections of words on semantic directions
defined by antonym pairs (e.g. kind - cruel) correlate highly with human
ratings, and further find that these projections effectively reduce to a
3-dimensional subspace within LLM embeddings, closely resembling the patterns
derived from human survey responses. Moreover, we find that shifting tokens
along one semantic direction causes off-target effects on geometrically aligned
features proportional to their cosine similarity. These findings suggest that
semantic features are entangled within LLMs similarly to how they are
interconnected in human language, and a great deal of semantic information,
despite its apparent complexity, is surprisingly low-dimensional. Furthermore,
accounting for this semantic structure may prove essential for avoiding
unintended consequences when steering features.

</details>


### [10] [RTTC: Reward-Guided Collaborative Test-Time Compute](https://arxiv.org/abs/2508.10024)
*J. Pablo Muñoz,Jinjie Yuan*

Main category: cs.CL

TL;DR: RTTC 通过自适应选择最佳 TTC 策略来提高 LLM 的准确性，并利用查询状态缓存来减少冗余计算。


<details>
  <summary>Details</summary>
Motivation: 测试时计算 (TTC) 已成为一种强大的范例，通过利用测试时训练 (TTT) 和检索增强生成 (RAG) 等策略来增强推理时大型语言模型 (LLM) 的性能。然而，最佳适应策略因查询而异，并且不加选择地应用 TTC 策略会产生巨大的计算开销。

Method: 引入了奖励引导的测试时计算 (RTTC)，这是一种新颖的框架，它通过预训练的奖励模型自适应地为每个查询选择最有效的 TTC 策略，从而最大限度地提高跨不同领域和任务的下游准确性。RTTC 在分布式服务器-客户端架构中运行，从远程知识库检索相关样本，并仅在必要时在客户端设备上应用 RAG 或轻量级微调。为了进一步减少冗余计算，我们提出了查询状态缓存，从而能够在检索和适应级别上高效地重用历史查询状态。

Result: RTTC在多个LLM和基准测试中始终优于vanilla RAG或TTT

Conclusion: RTTC在多个LLM和基准测试中始终优于vanilla RAG或TTT，验证了自适应、奖励引导的TTC选择的必要性以及RTTC在可扩展、高性能语言模型适应方面的潜力。

Abstract: Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the
performance of Large Language Models (LLMs) at inference, leveraging strategies
such as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).
However, the optimal adaptation strategy varies across queries, and
indiscriminate application of TTC strategy incurs substantial computational
overhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a
novel framework that adaptively selects the most effective TTC strategy for
each query via a pretrained reward model, maximizing downstream accuracy across
diverse domains and tasks. RTTC operates in a distributed server-client
architecture, retrieving relevant samples from a remote knowledge base and
applying RAG or lightweight fine-tuning on client devices only when necessary.
To further mitigate redundant computation, we propose Query-State Caching,
which enables the efficient reuse of historical query states at both retrieval
and adaptation levels. Extensive experiments across multiple LLMs and
benchmarks demonstrate that RTTC consistently achieves superior accuracy
compared to vanilla RAG or TTT, validating the necessity of adaptive,
reward-guided TTC selection and the potential of RTTC for scalable,
high-performance language model adaptation.

</details>


### [11] [Reflect then Learn: Active Prompting for Information Extraction Guided by Introspective Confusion](https://arxiv.org/abs/2508.10036)
*Dong Zhao,Yadong Wang,Xiang Chen,Chenxi Wang,Hongliang Dai,Chuanxing Geng,Shengzhong Zhang,Shaoyuan Li,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 提出了一种新的主动提示框架APIE，通过量化格式和内容的不确定性来选择信息量最大的样本，以提高少样本信息提取的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在少样本信息提取（IE）方面显示出卓越的潜力，但它们的性能对上下文示例的选择高度敏感。传统的选择策略通常无法提供信息指导，因为它们忽略了模型易错的一个关键来源：混淆不仅来自语义内容，还来自 IE 任务所需的良好结构化格式的生成。

Method: 提出了一种名为信息提取主动提示（APIE）的新型主动提示框架，该框架由我们称之为自省混淆的原则指导。我们的方法使 LLM 能够通过双组件不确定性度量来评估其自身的混淆，该度量独特地量化了格式不确定性（生成正确语法的难度）和内容不确定性（提取语义的不一致性）。

Result: 在四个基准上的大量实验表明，我们的方法始终优于强大的基线，在提取准确性和鲁棒性方面均产生了显着改进。

Conclusion: 该研究表明，在构建有效且可靠的结构化生成系统时，对模型不确定性进行细粒度的双层视图至关重要。

Abstract: Large Language Models (LLMs) show remarkable potential for few-shot
information extraction (IE), yet their performance is highly sensitive to the
choice of in-context examples. Conventional selection strategies often fail to
provide informative guidance, as they overlook a key source of model
fallibility: confusion stemming not just from semantic content, but also from
the generation of well-structured formats required by IE tasks. To address
this, we introduce Active Prompting for Information Extraction (APIE), a novel
active prompting framework guided by a principle we term introspective
confusion. Our method empowers an LLM to assess its own confusion through a
dual-component uncertainty metric that uniquely quantifies both Format
Uncertainty (difficulty in generating correct syntax) and Content Uncertainty
(inconsistency in extracted semantics). By ranking unlabeled data with this
comprehensive score, our framework actively selects the most challenging and
informative samples to serve as few-shot exemplars. Extensive experiments on
four benchmarks show that our approach consistently outperforms strong
baselines, yielding significant improvements in both extraction accuracy and
robustness. Our work highlights the critical importance of a fine-grained,
dual-level view of model uncertainty when it comes to building effective and
reliable structured generation systems.

</details>


### [12] [From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation](https://arxiv.org/abs/2508.10005)
*Chengliang Zhou,Mei Wang,Ting Zhang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: 推出了 EQGBench，用于评估法学硕士在中国 EQG 方面的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 在解决数学问题方面表现出了卓越的能力。然而，从提供答案到生成高质量的教育问题这一转变带来了重大挑战，但这些挑战仍未得到充分探索。

Method: 建立了一个五维评估框架，并提供了一个包含 900 个评估样本的数据集，涵盖数学、物理和化学三个基础中学学科。

Result: 我们推出了 EQGBench，这是一个专门为评估法学硕士在中国 EQG 方面的性能而设计的综合基准。

Conclusion: 通过对 46 个主流大型模型的系统评估，揭示了在生成能够反映教育价值和培养学生综合能力的问题方面仍有很大的发展空间。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
mathematical problem-solving. However, the transition from providing answers to
generating high-quality educational questions presents significant challenges
that remain underexplored. To advance Educational Question Generation (EQG) and
facilitate LLMs in generating pedagogically valuable and educationally
effective questions, we introduce EQGBench, a comprehensive benchmark
specifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench
establishes a five-dimensional evaluation framework supported by a dataset of
900 evaluation samples spanning three fundamental middle school disciplines:
mathematics, physics, and chemistry. The dataset incorporates user queries with
varying knowledge points, difficulty gradients, and question type
specifications to simulate realistic educational scenarios. Through systematic
evaluation of 46 mainstream large models, we reveal significant room for
development in generating questions that reflect educational value and foster
students' comprehensive abilities.

</details>


### [13] [Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models](https://arxiv.org/abs/2508.10007)
*Y. Lyu,D. Combs,D. Neumann,Y. C. Leong*

Main category: cs.CL

TL;DR: 大型语言模型可以自动评分 AIHQ 开放式回复，从而简化心理评估。


<details>
  <summary>Details</summary>
Motivation: 恶意归因偏差是指将社会互动解释为故意怀有敌意的倾向。歧义意图敌意问卷 (AIHQ) 通常用于衡量恶意归因偏差，包括开放式问题，参与者在其中描述负面社会情境背后的感知意图以及他们将如何回应。虽然这些问题提供了对恶意归因内容的见解，但它们需要人工评分员进行耗时的评分。

Method: 使用先前收集的数据集，其中脑外伤 (TBI) 患者和健康对照组 (HC) 完成了 AIHQ，并由经过训练的评分员对他们的开放式回答进行评分。我们使用这些回答的一半来根据人工生成的评分对两个模型进行微调，并在剩余一半的 AIHQ 回答上测试微调后的模型。

Result: 模型生成的评分与人类对敌意归因和攻击性反应的评分相一致，微调后的模型显示出更高的一致性。这种一致性在模糊、有意和意外的情景类型中是一致的，并重复了先前关于 TBI 和 HC 组之间敌意归因和攻击性反应的群体差异的研究结果。微调后的模型也能很好地推广到独立的非临床数据集。

Conclusion: 大型语言模型可以简化研究和临床环境中 AIHQ 的评分，并有可能促进不同人群的心理评估。

Abstract: Hostile attribution bias is the tendency to interpret social interactions as
intentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ)
is commonly used to measure hostile attribution bias, and includes open-ended
questions where participants describe the perceived intentions behind a
negative social situation and how they would respond. While these questions
provide insights into the contents of hostile attributions, they require
time-intensive scoring by human raters. In this study, we assessed whether
large language models can automate the scoring of AIHQ open-ended responses. We
used a previously collected dataset in which individuals with traumatic brain
injury (TBI) and healthy controls (HC) completed the AIHQ and had their
open-ended responses rated by trained human raters. We used half of these
responses to fine-tune the two models on human-generated ratings, and tested
the fine-tuned models on the remaining half of AIHQ responses. Results showed
that model-generated ratings aligned with human ratings for both attributions
of hostility and aggression responses, with fine-tuned models showing higher
alignment. This alignment was consistent across ambiguous, intentional, and
accidental scenario types, and replicated previous findings on group
differences in attributions of hostility and aggression responses between TBI
and HC groups. The fine-tuned models also generalized well to an independent
nonclinical dataset. To support broader adoption, we provide an accessible
scoring interface that includes both local and cloud-based options. Together,
our findings suggest that large language models can streamline AIHQ scoring in
both research and clinical contexts, revealing their potential to facilitate
psychological assessments across different populations.

</details>


### [14] [Multidimensional classification of posts for online course discussion forum curation](https://arxiv.org/abs/2508.10008)
*Antonio Leandro Martins Candido,Jose Everardo Bessa Maia*

Main category: cs.CL

TL;DR: This paper proposes Bayesian fusion to combine a pre-trained LLM with a classifier trained on local data for automatic curation of discussion forums. The approach improves performance and is competitive with LLM fine-tuning, while being less resource-intensive.


<details>
  <summary>Details</summary>
Motivation: Automatic curation of discussion forums requires constant updates and frequent retraining of LLMs which is resource-intensive.

Method: Bayesian fusion is used to combine the classification scores of a pre-trained generic LLM with those of a classifier trained on local data.

Result: The proposed fusion improves the results compared to each classifier individually and is competitive with the LLM fine-tuning approach.

Conclusion: Bayesian fusion improves classification results compared to individual classifiers and is competitive with LLM fine-tuning.

Abstract: The automatic curation of discussion forums in online courses requires
constant updates, making frequent retraining of Large Language Models (LLMs) a
resource-intensive process. To circumvent the need for costly fine-tuning, this
paper proposes and evaluates the use of Bayesian fusion. The approach combines
the multidimensional classification scores of a pre-trained generic LLM with
those of a classifier trained on local data. The performance comparison
demonstrated that the proposed fusion improves the results compared to each
classifier individually, and is competitive with the LLM fine-tuning approach

</details>


### [15] [Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts](https://arxiv.org/abs/2508.10009)
*Hojun Jin,Eunsoo Hong,Ziwon Hyung,Sungjun Lim,Seungjin Lee,Keunseok Cho*

Main category: cs.CL

TL;DR: S-MoE improves model performance by using guiding tokens to route tasks to designated experts, overcoming limitations of hard-parameter sharing.


<details>
  <summary>Details</summary>
Motivation: Hard-parameter sharing often leads to task interference, impeding overall model performance.

Method: We propose a simple yet effective Supervised Mixture of Experts (S-MoE) utilizing special guiding tokens to route each task to its designated expert.

Result: S-MoE overcomes the limitations of hard-parameter sharing and enables the model to process mixed-bandwidth input while jointly performing automatic speech recognition (ASR) and speech translation (ST).

Conclusion: S-MoE achieves a 6.35% relative improvement in Word Error Rate (WER) when applied to both the encoder and decoder.

Abstract: Hard-parameter sharing is a common strategy to train a single model jointly
across diverse tasks. However, this often leads to task interference, impeding
overall model performance. To address the issue, we propose a simple yet
effective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of
Experts models, S-MoE eliminates the need for training gating functions by
utilizing special guiding tokens to route each task to its designated expert.
By assigning each task to a separate feedforward network, S-MoE overcomes the
limitations of hard-parameter sharing. We further apply S-MoE to a
speech-to-text model, enabling the model to process mixed-bandwidth input while
jointly performing automatic speech recognition (ASR) and speech translation
(ST). Experimental results demonstrate the effectiveness of the proposed S-MoE,
achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to
both the encoder and decoder.

</details>


### [16] [Learning from Natural Language Feedback for Personalized Question Answering](https://arxiv.org/abs/2508.10695)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: 提出了VAC框架，用自然语言反馈代替标量奖励，从而优化个性化问答。


<details>
  <summary>Details</summary>
Motivation: 个性化对于提高语言技术（尤其是在问答等信息搜索任务中）的有效性和用户满意度至关重要。目前个性化大型语言模型 (LLM) 的方法通常依赖于检索增强生成 (RAG)，然后通过标量奖励信号进行强化学习，以教导模型如何使用检索到的个人上下文。我们认为这些标量奖励有时会提供微弱的、非指导性的反馈，从而限制学习效率和个性化质量。

Method: 提出了 VAC，一种新颖的个性化响应生成框架，该框架用自然语言反馈 (NLF) 代替标量奖励，这些自然语言反馈是根据用户配置文件和问题叙述生成的。

Result: 在包含三个不同领域的 LaMP-QA 基准上的评估表明，与最先进的结果相比，该方法具有一致且显着的改进。人工评估进一步证实了生成的响应的卓越质量。

Conclusion: 自然语言反馈为优化个性化问答提供了更有效的信号。

Abstract: Personalization is crucial for enhancing both the effectiveness and user
satisfaction of language technologies, particularly in information-seeking
tasks like question answering. Current approaches for personalizing large
language models (LLMs) often rely on retrieval-augmented generation (RAG),
followed by reinforcement learning with scalar reward signals to teach models
how to use retrieved personal context. We believe that these scalar rewards
sometimes provide weak, non-instructive feedback, limiting learning efficiency
and personalization quality. We introduce VAC, a novel framework for
personalized response generation that replaces scalar rewards with natural
language feedback (NLF) that are generated conditioned on the user profiles and
the question narratives. NLF serves as a rich and actionable supervision
signal, allowing the policy model to iteratively refine its outputs and
internalize effective personalization strategies. Training alternates between
optimizing the feedback model and fine-tuning the policy model on the improved
responses, resulting in a policy model that no longer requires feedback at
inference. Evaluation on the LaMP-QA benchmark that consists of three diverse
domains demonstrates consistent and significant improvements over the
state-of-the-art results. Human evaluations further confirm the superior
quality of the generated responses. These results demonstrate that NLF provides
more effective signals for optimizing personalized question answering.

</details>


### [17] [An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs](https://arxiv.org/abs/2508.10010)
*Ayana Hussain,Patrick Zhao,Nicholas Vincent*

Main category: cs.CL

TL;DR: LLMs can generate misinformation, but can also detect it.


<details>
  <summary>Details</summary>
Motivation: LLMs are a double-edged sword capable of generating harmful misinformation -- inadvertently, or when prompted by "jailbreak" attacks that attempt to produce malicious outputs. LLMs could, with additional research, be used to detect and prevent the spread of misinformation.

Method: investigate the efficacy and characteristics of LLM-produced jailbreak attacks that cause other models to produce harmful medical misinformation. We also study how misinformation generated by jailbroken LLMs compares to typical misinformation found on social media, and how effectively it can be detected using standard machine learning approaches. Specifically, we closely examine 109 distinct attacks against three target LLMs and compare the attack prompts to in-the-wild health-related LLM queries. We also examine the resulting jailbreak responses, comparing the generated misinformation to health-related misinformation on Reddit.

Result: findings add more evidence that LLMs can be effectively used to detect misinformation from both other LLMs and from people

Conclusion: LLMs can be effectively used to detect misinformation from both other LLMs and from people, and support a body of work suggesting that with careful design, LLMs can contribute to a healthier overall information ecosystem.

Abstract: Large Language Models (LLMs) are a double-edged sword capable of generating
harmful misinformation -- inadvertently, or when prompted by "jailbreak"
attacks that attempt to produce malicious outputs. LLMs could, with additional
research, be used to detect and prevent the spread of misinformation. In this
paper, we investigate the efficacy and characteristics of LLM-produced
jailbreak attacks that cause other models to produce harmful medical
misinformation. We also study how misinformation generated by jailbroken LLMs
compares to typical misinformation found on social media, and how effectively
it can be detected using standard machine learning approaches. Specifically, we
closely examine 109 distinct attacks against three target LLMs and compare the
attack prompts to in-the-wild health-related LLM queries. We also examine the
resulting jailbreak responses, comparing the generated misinformation to
health-related misinformation on Reddit. Our findings add more evidence that
LLMs can be effectively used to detect misinformation from both other LLMs and
from people, and support a body of work suggesting that with careful design,
LLMs can contribute to a healthier overall information ecosystem.

</details>


### [18] [Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan](https://arxiv.org/abs/2508.10011)
*Yuta Nagamori,Mikoto Kosai,Yuji Kawai,Haruka Marumo,Misaki Shibuya,Tatsuya Negishi,Masaki Imanishi,Yasumasa Ikeda,Koichiro Tsuchiya,Asuka Sawai,Licht Miyamoto*

Main category: cs.CL

TL;DR: 本研究评估了ChatGPT等大型语言模型在营养教育中的应用，发现它们在准确性和一致性方面存在局限性，需要进一步改进才能作为可靠的学习辅助工具。


<details>
  <summary>Details</summary>
Motivation: 基于大型语言模型（LLM）的生成式人工智能（AI），如ChatGPT，已经在包括医学和教育在内的各个专业领域展示了显著的进展。然而，它们在营养教育，特别是在日本注册营养师国家执照考试中的表现仍未被充分探索。本研究旨在评估当前基于LLM的生成式AI模型作为营养学学生学习辅助工具的潜力。

Method: 使用来自日本注册营养师国家考试的问题作为ChatGPT和三个Bing模型（精确、创造性、平衡）的提示，这些模型基于GPT-3.5和GPT-4。每个问题都输入到独立的会话中，并分析模型响应的准确性、一致性和响应时间。测试了额外的提示工程，包括角色分配，以评估潜在的性能改进。

Result: Bing-Precise（66.2%）和Bing-Creative（61.4%）超过了及格线（60%），而Bing-Balanced（43.3%）和ChatGPT（42.8%）没有。Bing-Precise和Bing-Creative通常在各个学科领域都优于其他模型，除了营养教育，所有模型的表现都不佳。没有一个模型在重复尝试中始终提供相同的正确答案，突显了答案稳定性的局限性。ChatGPT在响应模式中表现出更高的一致性，但准确性较低。提示工程的效果最小，除非明确提供正确的答案和解释时，效果略有提高。

Conclusion: 虽然一些生成式AI模型略微超过了及格线，但总体准确性和答案一致性仍不理想。此外，所有模型在答案一致性和稳健性方面都表现出明显的局限性。需要进一步的改进，以确保为营养师执照考试准备提供可靠和稳定的基于AI的学习辅助工具。

Abstract: Generative artificial intelligence (AI) based on large language models
(LLMs), such as ChatGPT, has demonstrated remarkable progress across various
professional fields, including medicine and education. However, their
performance in nutritional education, especially in Japanese national licensure
examination for registered dietitians, remains underexplored. This study aimed
to evaluate the potential of current LLM-based generative AI models as study
aids for nutrition students. Questions from the Japanese national examination
for registered dietitians were used as prompts for ChatGPT and three Bing
models (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question
was entered into independent sessions, and model responses were analyzed for
accuracy, consistency, and response time. Additional prompt engineering,
including role assignment, was tested to assess potential performance
improvements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the
passing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did
not. Bing-Precise and Bing-Creative generally outperformed others across
subject fields except Nutrition Education, where all models underperformed.
None of the models consistently provided the same correct responses across
repeated attempts, highlighting limitations in answer stability. ChatGPT showed
greater consistency in response patterns but lower accuracy. Prompt engineering
had minimal effect, except for modest improvement when correct answers and
explanations were explicitly provided. While some generative AI models
marginally exceeded the passing threshold, overall accuracy and answer
consistency remained suboptimal. Moreover, all the models demonstrated notable
limitations in answer consistency and robustness. Further advancements are
needed to ensure reliable and stable AI-based study aids for dietitian
licensure preparation.

</details>


### [19] [Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs](https://arxiv.org/abs/2508.10012)
*Dehao Tao,Guangjie Liu,Weizheng,Yongfeng Huang,Minghu jiang*

Main category: cs.CL

TL;DR: This paper proposes Guidance Graph guided Knowledge Exploration (GG Explore) to enhance LLMs' performance in knowledge-intensive tasks by using an intermediate Guidance Graph for efficient knowledge retrieval.


<details>
  <summary>Details</summary>
Motivation: LLMs' reliance on static knowledge and opaque reasoning processes limits their performance in knowledge intensive tasks. Current exploration methods face a fundamental trade off: question guided approaches incur redundant exploration due to granularity mismatches, while clue guided methods fail to effectively leverage contextual information for complex scenarios.

Method: The paper develops Structural Alignment that filters incompatible candidates without LLM overhead, and Context Aware Pruning that enforces semantic consistency with graph constraints.

Result: The proposed method achieves superior efficiency and outperforms SOTA, especially on complex tasks, while maintaining strong performance with smaller LLMs.

Conclusion: This paper introduces Guidance Graph guided Knowledge Exploration (GG Explore), a novel framework that introduces an intermediate Guidance Graph to bridge unstructured queries and structured knowledge retrieval. Extensive experiments show our method achieves superior efficiency and outperforms SOTA, especially on complex tasks, while maintaining strong performance with smaller LLMs, demonstrating practical value.

Abstract: While Large Language Models (LLMs) exhibit strong linguistic capabilities,
their reliance on static knowledge and opaque reasoning processes limits their
performance in knowledge intensive tasks. Knowledge graphs (KGs) offer a
promising solution, but current exploration methods face a fundamental trade
off: question guided approaches incur redundant exploration due to granularity
mismatches, while clue guided methods fail to effectively leverage contextual
information for complex scenarios. To address these limitations, we propose
Guidance Graph guided Knowledge Exploration (GG Explore), a novel framework
that introduces an intermediate Guidance Graph to bridge unstructured queries
and structured knowledge retrieval. The Guidance Graph defines the retrieval
space by abstracting the target knowledge' s structure while preserving broader
semantic context, enabling precise and efficient exploration. Building upon the
Guidance Graph, we develop: (1) Structural Alignment that filters incompatible
candidates without LLM overhead, and (2) Context Aware Pruning that enforces
semantic consistency with graph constraints. Extensive experiments show our
method achieves superior efficiency and outperforms SOTA, especially on complex
tasks, while maintaining strong performance with smaller LLMs, demonstrating
practical value.

</details>


### [20] [Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis](https://arxiv.org/abs/2508.10013)
*Linqing Chen,Hanmeng Zhong,Wentao Wu,Weilei Wang*

Main category: cs.CL

TL;DR: Semantic Bridge is a new framework for controllably generating sophisticated multi-hop reasoning questions from arbitrary sources, addressing the scarcity of high-quality reasoning-intensive question-answer pairs for LLM training.


<details>
  <summary>Details</summary>
Motivation: LLM training faces a critical bottleneck: the scarcity of high-quality, reasoning-intensive question-answer pairs, especially from sparse, domain-specific sources like PubMed papers or legal documents. Existing methods rely on surface patterns, fundamentally failing to generate controllable, complex multi-hop reasoning questions that test genuine understanding-essential for advancing LLM training paradigms.

Method: semantic graph weaving-three complementary bridging mechanisms (entity bridging for role-varying shared entities, predicate chain bridging for temporal/causal/logical sequences, and causal bridging for explicit reasoning chains)-that systematically construct complex pathways across documents, with fine-grained control over complexity and types via AMR-driven analysis

Result: Our multi-modal AMR pipeline achieves up to 9.5% better round-trip quality, enabling production-ready controllable QA generation. Extensive evaluation demonstrates performance across both general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It yields consistent 18.3%-25.4% gains over baselines across four languages (English, Chinese, French, German). Question pairs generated from 200 sources outperform 600 native human annotation examples with 67% fewer materials. Human evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2% improved pattern coverage.

Conclusion: Semantic Bridge establishes a new paradigm for LLM training data synthesis, enabling controllable generation of targeted reasoning questions from sparse sources. We will release our core code and semantic bridge model.

Abstract: Large language model (LLM) training faces a critical bottleneck: the scarcity
of high-quality, reasoning-intensive question-answer pairs, especially from
sparse, domain-specific sources like PubMed papers or legal documents. Existing
methods rely on surface patterns, fundamentally failing to generate
controllable, complex multi-hop reasoning questions that test genuine
understanding-essential for advancing LLM training paradigms. We present
\textbf{Semantic Bridge}, the first universal framework for controllably
generating sophisticated multi-hop reasoning questions from arbitrary sources.
Our breakthrough innovation is \textit{semantic graph weaving}-three
complementary bridging mechanisms (entity bridging for role-varying shared
entities, predicate chain bridging for temporal/causal/logical sequences, and
causal bridging for explicit reasoning chains)-that systematically construct
complex pathways across documents, with fine-grained control over complexity
and types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to
9.5% better round-trip quality, enabling production-ready controllable QA
generation. Extensive evaluation demonstrates performance across both
general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It
yields consistent 18.3%-25.4% gains over baselines across four languages
(English, Chinese, French, German). Question pairs generated from 200 sources
outperform 600 native human annotation examples with 67% fewer materials. Human
evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2%
improved pattern coverage. Semantic Bridge establishes a new paradigm for LLM
training data synthesis, enabling controllable generation of targeted reasoning
questions from sparse sources. We will release our core code and semantic
bridge model.

</details>


### [21] [PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?](https://arxiv.org/abs/2508.10014)
*Lingfeng Zhou,Jialing Zhang,Jin Gao,Mohan Jiang,Dequan Wang*

Main category: cs.CL

TL;DR: LLM evaluators struggle to identify roles in dialogues compared to humans, indicating they are not yet suitable for judging role-playing quality. A new benchmark, PersonaEval, is introduced to highlight this gap.


<details>
  <summary>Details</summary>
Motivation: Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms, which may fail to reflect how humans perceive role fidelity. A key prerequisite for human-aligned evaluation is role identification.

Method: PersonaEval, the first benchmark designed to test whether LLM evaluators can reliably identify human roles, using human-authored dialogues.

Result: The best-performing LLMs reach only around 69% accuracy in PersonaEval, while human participants perform near ceiling with 90.8% accuracy.

Conclusion: Current LLM evaluators are not human enough to effectively judge role-play scenarios, and reliable evaluation requires strong, human-like reasoning abilities.

Abstract: Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms,
which may fail to reflect how humans perceive role fidelity. A key prerequisite
for human-aligned evaluation is role identification, the ability to recognize
who is speaking based on dialogue context. We argue that any meaningful
judgment of role-playing quality (how well a character is played) fundamentally
depends on first correctly attributing words and actions to the correct persona
(who is speaking). We present PersonaEval, the first benchmark designed to test
whether LLM evaluators can reliably identify human roles. PersonaEval uses
human-authored dialogues from novels, scripts, and video transcripts,
challenging models to determine the correct persona according to the
conversation context. Our experiments, including a human study, show that even
the best-performing LLMs reach only around 69% accuracy, well below the level
needed for reliable evaluation. In contrast, human participants perform near
ceiling with 90.8% accuracy, highlighting that current LLM evaluators are still
not human enough to effectively judge role-play scenarios. To better understand
this gap, we examine training-time adaptation and test-time compute, suggesting
that reliable evaluation requires more than task-specific tuning, but depends
on strong, human-like reasoning abilities in LLM evaluators. We release our
benchmark at https://github.com/maple-zhou/PersonaEval.

</details>


### [22] [RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis](https://arxiv.org/abs/2508.10015)
*Enzhi Wang,Qicheng Li,Shiwan Zhao,Aobo Kong,Jiaming Zhou,Xi Yang,Yequan Wang,Yonghua Lin,Yong Qin*

Main category: cs.CL

TL;DR: RealTalk-CN：首个中文多轮、多领域语音-文本双模态TOD数据集，包含5.4k个对话，带有配对的语音-文本注释，解决了现有数据集缺乏真实语音信号、语音口吃和说话者变化等问题。


<details>
  <summary>Details</summary>
Motivation: 现有的TOD数据集主要是基于文本的，缺乏评估语音LLM鲁棒性所必需的真实语音信号。此外，现有的语音TOD数据集主要是英语的，缺乏语音口吃和说话者变化等关键方面。

Method: 我们提出了一个新的跨模态聊天任务，可以真实地模拟真实世界的用户交互，允许在语音和文本模态之间动态切换。

Result: 我们介绍了RealTalk-CN，这是第一个中文多轮、多领域的语音-文本双模态TOD数据集，包含5.4k个对话（60K个话语，150小时），带有配对的语音-文本注释。RealTalk-CN 捕获了具有注释的自发语音口吃的各种对话场景，确保全面覆盖语音对话中真实世界的复杂性。

Conclusion: RealTalk-CN数据集的有效性通过大量实验验证，为中文语音LLM研究奠定了坚实的基础。

Abstract: In recent years, large language models (LLMs) have achieved remarkable
advancements in multimodal processing, including end-to-end speech-based
language models that enable natural interactions and perform specific tasks in
task-oriented dialogue (TOD) systems. However, existing TOD datasets are
predominantly text-based, lacking real speech signals that are essential for
evaluating the robustness of speech-based LLMs. Moreover, existing speech TOD
datasets are primarily English and lack critical aspects such as speech
disfluencies and speaker variations. To address these gaps, we introduce
RealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal
TOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired
speech-text annotations. RealTalk-CN captures diverse dialogue scenarios with
annotated spontaneous speech disfluencies, ensuring comprehensive coverage of
real-world complexities in speech dialogue. In addition, we propose a novel
cross-modal chat task that authentically simulates real-world user
interactions, allowing dynamic switching between speech and text modalities.
Our evaluation covers robustness to speech disfluencies, sensitivity to speaker
characteristics, and cross-domain performance. Extensive experiments validate
the effectiveness of RealTalk-CN, establishing a strong foundation for Chinese
speech-based LLMs research.

</details>


### [23] [Training-Free Multimodal Large Language Model Orchestration](https://arxiv.org/abs/2508.10016)
*Tianyu Xie,Yuhang Wu,Yongdong Luo,Jiayi Ji,Xiawu Zheng*

Main category: cs.CL

TL;DR: This paper introduces MLLM Orchestration, a training-free approach for creating interactive multimodal AI systems. It uses a central LLM to coordinate specialized models, achieving improved performance, reduced latency, and enhanced interpretability.


<details>
  <summary>Details</summary>
Motivation: Different Multimodal Large Language Models (MLLMs) cannot be integrated into a unified multimodal input-output system directly. In previous work, training has been considered as an inevitable component due to challenges in modal alignment, Text-to-Speech efficiency and other integration issues.

Method: MLLM Orchestration leverages the inherent reasoning capabilities of large language models to coordinate specialized models through explicit workflows, enabling natural multimodal interactions while maintaining modularity, improving interpretability, and significantly enhancing computational efficiency. The orchestration framework is built upon three key innovations: (1) a central controller LLM that analyzes user inputs and dynamically routes tasks to appropriate specialized models through carefully designed agents; (2) a parallel Text-to-Speech architecture that enables true full-duplex interaction with seamless interruption handling and natural conversational flow; and (3) a cross-modal memory integration system that maintains coherent context across modalities through intelligent information synthesis and retrieval, selectively avoiding unnecessary modality calls in certain scenarios to improve response speed.

Result: MLLM Orchestration achieves comprehensive multimodal capabilities without additional training, performance improvements of up to 7.8% over traditional jointly-trained approaches on standard benchmarks, reduced latency by 10.3%.

Conclusion: MLLM Orchestration achieves comprehensive multimodal capabilities without additional training, performance improvements of up to 7.8% over traditional jointly-trained approaches on standard benchmarks, reduced latency by 10.3%, and significantly enhanced interpretability through explicit orchestration processes.

Abstract: Different Multimodal Large Language Models (MLLMs) cannot be integrated into
a unified multimodal input-output system directly. In previous work, training
has been considered as an inevitable component due to challenges in modal
alignment, Text-to-Speech efficiency and other integration issues. In this
paper, we introduce Multimodal Large Language Model Orchestration, an effective
approach for creating interactive multimodal AI systems without additional
training. MLLM Orchestration leverages the inherent reasoning capabilities of
large language models to coordinate specialized models through explicit
workflows, enabling natural multimodal interactions while maintaining
modularity, improving interpretability, and significantly enhancing
computational efficiency. Our orchestration framework is built upon three key
innovations: (1) a central controller LLM that analyzes user inputs and
dynamically routes tasks to appropriate specialized models through carefully
designed agents; (2) a parallel Text-to-Speech architecture that enables true
full-duplex interaction with seamless interruption handling and natural
conversational flow; and (3) a cross-modal memory integration system that
maintains coherent context across modalities through intelligent information
synthesis and retrieval, selectively avoiding unnecessary modality calls in
certain scenarios to improve response speed. Extensive evaluations demonstrate
that MLLM Orchestration achieves comprehensive multimodal capabilities without
additional training, performance improvements of up to 7.8% over traditional
jointly-trained approaches on standard benchmarks, reduced latency by 10.3%,
and significantly enhanced interpretability through explicit orchestration
processes.

</details>


### [24] [A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models](https://arxiv.org/abs/2508.10018)
*Sridhar Mahadevan*

Main category: cs.CL

TL;DR: This paper introduces a categorical homotopy framework for LLMs to address the problem that LLMs do not generate the same next-token probabilities for superficially different statements with the same meaning.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) should generate the same next-token probabilities in such cases, but usually do not.

Method: Introducing a categorical homotopy framework for LLMs. Introducing an LLM Markov category to represent probability distributions in language generated by an LLM

Result: We present a detailed overview of application of categorical homotopy to LLMs, from higher algebraic K-theory to model categories, building on powerful theoretical results developed over the past half a century.

Conclusion: This paper uses categorical homotopy techniques to capture weak equivalences in an LLM Markov category.

Abstract: Natural language is replete with superficially different statements, such as
``Charles Darwin wrote" and ``Charles Darwin is the author of", which carry the
same meaning. Large language models (LLMs) should generate the same next-token
probabilities in such cases, but usually do not. Empirical workarounds have
been explored, such as using k-NN estimates of sentence similarity to produce
smoothed estimates. In this paper, we tackle this problem more abstractly,
introducing a categorical homotopy framework for LLMs. We introduce an LLM
Markov category to represent probability distributions in language generated by
an LLM, where the probability of a sentence, such as ``Charles Darwin wrote" is
defined by an arrow in a Markov category. However, this approach runs into
difficulties as language is full of equivalent rephrases, and each generates a
non-isomorphic arrow in the LLM Markov category. To address this fundamental
problem, we use categorical homotopy techniques to capture ``weak equivalences"
in an LLM Markov category. We present a detailed overview of application of
categorical homotopy to LLMs, from higher algebraic K-theory to model
categories, building on powerful theoretical results developed over the past
half a century.

</details>


### [25] [Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning](https://arxiv.org/abs/2508.10019)
*Li Wang,Changhao Zhang,Zengqi Xiu,Kai Lu,Xin Yu,Kui Zhang,Wenjun Wu*

Main category: cs.CL

TL;DR: This paper introduces DURIT, a framework that decouples understanding from reasoning by mapping natural language problems into a canonical problem space. Experiments show that DURIT substantially improves SLMs' performance on both in-domain and out-of-domain mathematical and logical reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: improving the reasoning ability of Small Language Models (SLMs, e.g., $\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity and variability of natural language: essentially equivalent problems often appear in diverse surface forms, often obscured by redundant or distracting details. This imposes a dual burden on SLMs: they must first extract the core problem from complex linguistic input, and then perform reasoning based on that understanding. The resulting vast and noisy problem space hinders optimization, particularly for models with limited capacity.

Method: DURIT (Decoupled Understanding from Reasoning via Iterative Training), a three-step algorithm that iteratively: (1) mapping natural language problems via reinforcement learning, (2) aligns reasoning trajectories through self-distillation, and (3) trains reasoning policies in the problem space. The mapper and reasoner are co-trained in an alternating loop throughout this process.

Result: DURIT substantially improves SLMs' performance on both in-domain and out-of-domain mathematical and logical reasoning tasks. DURIT also improves the robustness of reasoning

Conclusion: DURIT improves SLMs' performance on both in-domain and out-of-domain mathematical and logical reasoning tasks. decoupling understanding from reasoning is an effective strategy for strengthening SLMs

Abstract: Despite recent advances in the reasoning capabilities of Large Language
Models (LLMs), improving the reasoning ability of Small Language Models (SLMs,
e.g., $\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity
and variability of natural language: essentially equivalent problems often
appear in diverse surface forms, often obscured by redundant or distracting
details. This imposes a dual burden on SLMs: they must first extract the core
problem from complex linguistic input, and then perform reasoning based on that
understanding. The resulting vast and noisy problem space hinders optimization,
particularly for models with limited capacity. To address this, we propose a
new framework that decouples understanding from reasoning by mapping natural
language problems into a canonical problem space-a semantically simplified yet
expressive domain. This enables SLMs to focus on reasoning over standardized
inputs, free from linguistic variability. Within this framework, we introduce
DURIT (Decoupled Understanding from Reasoning via Iterative Training), a
three-step algorithm that iteratively: (1) mapping natural language problems
via reinforcement learning, (2) aligns reasoning trajectories through
self-distillation, and (3) trains reasoning policies in the problem space. The
mapper and reasoner are co-trained in an alternating loop throughout this
process. Experiments show that DURIT substantially improves SLMs' performance
on both in-domain and out-of-domain mathematical and logical reasoning tasks.
Beyond improving reasoning capabilities, DURIT also improves the robustness of
reasoning, validating decoupling understanding from reasoning as an effective
strategy for strengthening SLMs.

</details>


### [26] [FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models](https://arxiv.org/abs/2508.10020)
*Chuan Li,Qianyi Zhao,Fengran Mo,Cen Chen*

Main category: cs.CL

TL;DR: FedCoT是一种新的框架，旨在增强联邦环境中的推理能力，通过利用轻量级的思维链增强机制和改进的聚合方法，在医疗推理任务中显著提高了客户端推理性能，同时完全保护数据隐私。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习环境中有效增强大型语言模型 (LLM) 的推理能力仍然具有挑战性，尤其是在平衡性能提升与严格的计算、通信和隐私约束时。传统的联邦调优方法主要优化答案的正确性，而忽略了理由的质量，并且改进理由的现有方法通常依赖于侵犯隐私的集中式模型的知识蒸馏。此外，传统联邦微调中LLM的通信开销仍然很大。

Method: FedCoT利用轻量级的思维链增强机制：本地模型生成多个推理路径，紧凑的判别器动态选择最有希望的路径。

Result: FedCoT提高了推理的准确性和鲁棒性，同时提供了有价值的可解释性。在医疗推理任务上的综合实验表明

Conclusion: FedCoT显著提升了客户端推理性能，同时完全保护数据隐私。

Abstract: Efficiently enhancing the reasoning capabilities of large language models
(LLMs) in federated learning environments remains challenging, particularly
when balancing performance gains with strict computational, communication, and
privacy constraints. This challenge is especially acute in healthcare, where
decisions-spanning clinical, operational, and patient-facing contexts-demand
not only accurate outputs but also interpretable, traceable rationales to
ensure safety, accountability, and regulatory compliance. Conventional
federated tuning approaches on LLM fail to address this need: they optimize
primarily for answer correctness while neglecting rationale quality, leaving
CoT capabilities dependent on models' innate pre-training abilities. Moreover,
existing methods for improving rationales typically rely on privacy-violating
knowledge distillation from centralized models. Additionally, the communication
overhead in traditional federated fine-tuning on LLMs remains substantial. We
addresses this gap by proposing FedCoT, a novel framework specifically designed
to enhance reasoning in federated settings. FedCoT leverages a lightweight
chain-of-thought enhancement mechanism: local models generate multiple
reasoning paths, and a compact discriminator dynamically selects the most
promising one. This approach improves reasoning accuracy and robustness while
providing valuable interpretability, which is particularly critical for medical
applications. To manage client heterogeneity efficiently, we adopt an improved
aggregation approach building upon advanced LoRA module stacking, incorporating
client classifier-awareness to achieve noise-free aggregation across diverse
clients. Comprehensive experiments on medical reasoning tasks demonstrate that
FedCoT significantly boosts client-side reasoning performance under stringent
resource budgets while fully preserving data privacy.

</details>


### [27] [LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients](https://arxiv.org/abs/2508.10021)
*Egor Fadeev,Dzhambulat Mollaev,Aleksei Shestov,Dima Korolev,Omar Zoloev,Ivan Kireev,Andrey Savchenko,Maksim Makarenko*

Main category: cs.CL

TL;DR: LATTE是一种对比学习框架，它通过对比损失将原始事件嵌入与冻结LLM的语义嵌入对齐，显著降低了推理成本和输入大小。


<details>
  <summary>Details</summary>
Motivation: 从客户历史通信序列中学习客户嵌入对于金融应用至关重要。大型语言模型 (LLM) 提供一般的世界知识，但它们在长事件序列上的直接使用在计算上是昂贵的，并且在现实世界的管道中是不切实际的。

Method: 一种对比学习框架，可将原始事件嵌入与来自冻结LLM的语义嵌入对齐。

Result: 该方法显著降低了推理成本和输入大小，与LLM完整序列的传统处理相比。

Conclusion: 该方法在真实金融数据集上优于现有技术，同时保持在对延迟敏感的环境中的可部署性。

Abstract: Learning clients embeddings from sequences of their historic communications
is central to financial applications. While large language models (LLMs) offer
general world knowledge, their direct use on long event sequences is
computationally expensive and impractical in real-world pipelines. In this
paper, we propose LATTE, a contrastive learning framework that aligns raw event
embeddings with semantic embeddings from frozen LLMs. Behavioral features are
summarized into short prompts, embedded by the LLM, and used as supervision via
contrastive loss. The proposed approach significantly reduces inference cost
and input size compared to conventional processing of complete sequence by LLM.
We experimentally show that our method outperforms state-of-the-art techniques
for learning event sequence representations on real-world financial datasets
while remaining deployable in latency-sensitive environments.

</details>


### [28] [Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control](https://arxiv.org/abs/2508.10022)
*Yuanchang Ye*

Main category: cs.CL

TL;DR: This study introduces a significance testing-enhanced conformal prediction framework to improve trustworthiness of large language models in multiple-choice question answering.


<details>
  <summary>Details</summary>
Motivation: LLMs have been increasingly deployed in disciplinary QA scenarios, hallucination and nonfactual generation substantially compromise response reliability.

Method: integrates $p$-value computation with conformity scoring through self-consistency resampling of MCQA responses. This approach calculates option frequencies to address LLMs' black-box nature, subsequently constructing prediction sets via null hypothesis testing ($\\\mathcal{H}_0$) with empirically derived $p$-values.

Result: The enhanced CP achieves user-specified empirical miscoverage rates; Test-set average prediction set size (APSS) decreases monotonically with increasing risk levels ($\\alpha$), validating APSS as an effective uncertainty metric.

Conclusion: This work establishes a principled statistical framework for trustworthy LLM deployment in high-stakes QA applications.

Abstract: This study introduces a significance testing-enhanced conformal prediction
(CP) framework to improve trustworthiness of large language models (LLMs) in
multiple-choice question answering (MCQA). While LLMs have been increasingly
deployed in disciplinary QA scenarios, hallucination and nonfactual generation
substantially compromise response reliability. Although CP provides
statistically rigorous marginal coverage guarantees for prediction sets, and
significance testing offers established statistical rigor, their synergistic
integration remains unexplored. To mitigate hallucination and factual
inaccuracies, our framework integrates $p$-value computation with conformity
scoring through self-consistency resampling of MCQA responses. This approach
calculates option frequencies to address LLMs' black-box nature, subsequently
constructing prediction sets via null hypothesis testing ($\mathcal{H}_0$) with
empirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks
using off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves
user-specified empirical miscoverage rates; (2) Test-set average prediction set
size (APSS) decreases monotonically with increasing risk levels ($\alpha$),
validating APSS as an effective uncertainty metric. This work establishes a
principled statistical framework for trustworthy LLM deployment in high-stakes
QA applications.

</details>


### [29] [Detecting and explaining postpartum depression in real-time with generative artificial intelligence](https://arxiv.org/abs/2508.10025)
*Silvia García-Méndez,Francisco de Arriba-Pérez*

Main category: cs.CL

TL;DR: This paper presents an intelligent postpartum depression (PPD) screening system using NLP, ML, and LLMs, achieving 90% accuracy and offering interpretable results for real-time assessment and intervention.


<details>
  <summary>Details</summary>
Motivation: Postpartum depression (PPD) significantly impacts mothers' mental and physical well-being, making rapid detection and risk factor identification crucial for timely intervention.

Method: The study combines Natural Language Processing, Machine Learning (ML), and Large Language Models (LLMs) for affordable, real-time, and non-invasive speech analysis. It uses interpretable ML models (tree-based algorithms) with feature importance and natural language to address the black box problem.

Result: The system achieves 90% accuracy in PPD detection, outperforming competing solutions.

Conclusion: The solution contributes to the rapid detection of PPD and associated risk factors, which is critical for timely assessment and intervention. The system achieves 90% accuracy in PPD detection, outperforming existing solutions.

Abstract: Among the many challenges mothers undergo after childbirth, postpartum
depression (PPD) is a severe condition that significantly impacts their mental
and physical well-being. Consequently, the rapid detection of ppd and their
associated risk factors is critical for in-time assessment and intervention
through specialized prevention procedures. Accordingly, this work addresses the
need to help practitioners make decisions with the latest technological
advancements to enable real-time screening and treatment recommendations.
Mainly, our work contributes to an intelligent PPD screening system that
combines Natural Language Processing, Machine Learning (ML), and Large Language
Models (LLMs) towards an affordable, real-time, and non-invasive free speech
analysis. Moreover, it addresses the black box problem since the predictions
are described to the end users thanks to the combination of LLMs with
interpretable ml models (i.e., tree-based algorithms) using feature importance
and natural language. The results obtained are 90 % on ppd detection for all
evaluation metrics, outperforming the competing solutions in the literature.
Ultimately, our solution contributes to the rapid detection of PPD and their
associated risk factors, critical for in-time and proper assessment and
intervention.

</details>


### [30] [SABER: Switchable and Balanced Training for Efficient LLM Reasoning](https://arxiv.org/abs/2508.10026)
*Kai Zhao,Yanjun Zhao,Jiaming Song,Shien He,Lusheng Zhang,Qiang Zhang,Tianjiao Li*

Main category: cs.CL

TL;DR: SABER is a reinforcement learning framework that enables efficient LLM reasoning with user-controllable, token-budgeted reasoning. It achieves high accuracy under tight budgets and effective generalization.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) empowered by chain-of-thought reasoning have achieved impressive accuracy on complex tasks but suffer from excessive inference costs and latency when applied uniformly to all problems.

Method: a reinforcement learning framework that endows LLMs with user-controllable, token-budgeted reasoning. SABER first profiles each training example's base-model thinking token usage and assigns it to one of the predefined budget tiers. During fine-tuning, the model is guided by system prompts and length-aware rewards to respect its assigned budget. In parallel, we incorporate no-think examples to ensure the model remains reliable even when explicit reasoning is turned off. SABER further supports four discrete inference modes - NoThink, FastThink, CoreThink, and DeepThink

Result: SABER achieves high accuracy under tight budgets, graceful degradation, and effective cross-scale and cross-domain generalization. In particular, SABER-FastThink cuts reasoning length by 65.4% and yields a 3.6% accuracy gain compared with the base model on the MATH benchmark.

Conclusion: SABER achieves high accuracy under tight budgets, graceful degradation, and effective cross-scale and cross-domain generalization. In particular, SABER-FastThink cuts reasoning length by 65.4% and yields a 3.6% accuracy gain compared with the base model on the MATH benchmark.

Abstract: Large language models (LLMs) empowered by chain-of-thought reasoning have
achieved impressive accuracy on complex tasks but suffer from excessive
inference costs and latency when applied uniformly to all problems. We propose
SABER (Switchable and Balanced Training for Efficient LLM Reasoning), a
reinforcement learning framework that endows LLMs with user-controllable,
token-budgeted reasoning. SABER first profiles each training example's
base-model thinking token usage and assigns it to one of the predefined budget
tiers. During fine-tuning, the model is guided by system prompts and
length-aware rewards to respect its assigned budget. In parallel, we
incorporate no-think examples to ensure the model remains reliable even when
explicit reasoning is turned off. SABER further supports four discrete
inference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling
flexible trade-offs between latency and reasoning depth. Extensive evaluations
on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning
(LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight
budgets, graceful degradation, and effective cross-scale and cross-domain
generalization. In particular, SABER-FastThink cuts reasoning length by 65.4%
and yields a 3.6% accuracy gain compared with the base model on the MATH
benchmark.

</details>


### [31] [LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.10027)
*Ali Zolnour,Hossein Azadmaleki,Yasaman Haghbin,Fatemeh Taherinezhad,Mohamad Javad Momeni Nezhad,Sina Rashidi,Masoud Khani,AmirSajjad Taleban,Samin Mahdizadeh Sani,Maryam Dadkhah,James M. Noble,Suzanne Bakken,Yadollah Yaghoobzadeh,Abdol-Hossein Vahabie,Masoud Rouhizadeh,Maryam Zolnoori*

Main category: cs.CL

TL;DR: This paper explores using speech-based NLP and LLMs to detect Alzheimer's, finding that combining transformer embeddings with linguistic features improves detection, and fine-tuned LLMs can augment data, but multimodal models need more work.


<details>
  <summary>Details</summary>
Motivation: Alzheimer's disease and related dementias (ADRD) affect approximately five million older adults in the U.S., yet over half remain undiagnosed. Speech-based natural language processing (NLP) offers a promising, scalable approach to detect early cognitive decline through linguistic markers.

Method: A screening pipeline that (i) fuses transformer embeddings with handcrafted linguistic features, (ii) tests data augmentation using synthetic speech generated by large language models (LLMs), and (iii) benchmarks unimodal and multimodal LLM classifiers for ADRD detection. Transcripts from the DementiaBank "cookie-theft" task (n = 237) were used. Ten transformer models were evaluated under three fine-tuning strategies. A fusion model combined embeddings from the top-performing transformer with 110 lexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B, Ministral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic speech, which was used to augment training data. Three multimodal models (GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in zero-shot and fine-tuned settings.

Result: The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or transformer-only baselines. Augmenting training data with 2x MedAlpaca-7B synthetic speech increased F1 to 85.7. Fine-tuning significantly improved unimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current multimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen = 66.0). Performance gains aligned with the distributional similarity between synthetic and real speech.

Conclusion: Integrating transformer embeddings with linguistic features enhances ADRD detection from speech. Clinically tuned LLMs effectively support both classification and data augmentation, while further advancement is needed in multimodal modeling.

Abstract: Alzheimer's disease and related dementias (ADRD) affect approximately five
million older adults in the U.S., yet over half remain undiagnosed.
Speech-based natural language processing (NLP) offers a promising, scalable
approach to detect early cognitive decline through linguistic markers.
  To develop and evaluate a screening pipeline that (i) fuses transformer
embeddings with handcrafted linguistic features, (ii) tests data augmentation
using synthetic speech generated by large language models (LLMs), and (iii)
benchmarks unimodal and multimodal LLM classifiers for ADRD detection.
  Transcripts from the DementiaBank "cookie-theft" task (n = 237) were used.
Ten transformer models were evaluated under three fine-tuning strategies. A
fusion model combined embeddings from the top-performing transformer with 110
lexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B,
Ministral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic
speech, which was used to augment training data. Three multimodal models
(GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in
zero-shot and fine-tuned settings.
  The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or
transformer-only baselines. Augmenting training data with 2x MedAlpaca-7B
synthetic speech increased F1 to 85.7. Fine-tuning significantly improved
unimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current
multimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen =
66.0). Performance gains aligned with the distributional similarity between
synthetic and real speech.
  Integrating transformer embeddings with linguistic features enhances ADRD
detection from speech. Clinically tuned LLMs effectively support both
classification and data augmentation, while further advancement is needed in
multimodal modeling.

</details>


### [32] [PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs](https://arxiv.org/abs/2508.10028)
*Xiao Fu,Hossein A. Rahmani,Bin Wu,Jerome Ramos,Emine Yilmaz,Aldo Lipani*

Main category: cs.CL

TL;DR: PREF是一个个性化的、无需参考的评估框架，它可以联合测量通用输出质量和用户特定的对齐，而无需黄金个性化参考。


<details>
  <summary>Details</summary>
Motivation: 个性化文本生成对于以用户为中心的信息系统至关重要，但大多数评估方法忽略了用户的个性。

Method: PREF 通过一个三步流程运行：(1) 覆盖阶段，使用大型语言模型 (LLM) 生成一个全面的、特定于查询的指南，涵盖通用标准，例如事实性、连贯性和完整性；(2) 偏好阶段，使用目标用户的个人资料、陈述或推断的偏好和上下文，重新排序并选择性地扩充这些因素，从而生成个性化的评估标准；(3) 评分阶段，应用 LLM 评判器根据此标准对候选答案进行评分，确保基线充分性，同时捕获主观优先级。

Result: 在 PrefEval 基准测试（包括隐式偏好跟踪任务）上的实验表明，PREF 比强大的基线实现了更高的准确性、更好的校准以及与人类判断更紧密的对齐。

Conclusion: PREF实现了更高的准确性、更好的校准以及与人类判断更紧密的对齐。

Abstract: Personalised text generation is essential for user-centric information
systems, yet most evaluation methods overlook the individuality of users. We
introduce \textbf{PREF}, a \textbf{P}ersonalised \textbf{R}eference-free
\textbf{E}valuation \textbf{F}ramework that jointly measures general output
quality and user-specific alignment without requiring gold personalised
references. PREF operates in a three-step pipeline: (1) a coverage stage uses a
large language model (LLM) to generate a comprehensive, query-specific
guideline covering universal criteria such as factuality, coherence, and
completeness; (2) a preference stage re-ranks and selectively augments these
factors using the target user's profile, stated or inferred preferences, and
context, producing a personalised evaluation rubric; and (3) a scoring stage
applies an LLM judge to rate candidate answers against this rubric, ensuring
baseline adequacy while capturing subjective priorities. This separation of
coverage from preference improves robustness, transparency, and reusability,
and allows smaller models to approximate the personalised quality of larger
ones. Experiments on the PrefEval benchmark, including implicit
preference-following tasks, show that PREF achieves higher accuracy, better
calibration, and closer alignment with human judgments than strong baselines.
By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the
groundwork for more reliable assessment and development of personalised
language generation systems.

</details>


### [33] [Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs](https://arxiv.org/abs/2508.10029)
*Wenpeng Xing,Mohan Li,Chunqiang Hu,Haitao XuNingyu Zhang,Bo Lin,Meng Han*

Main category: cs.CL

TL;DR: 提出了一种新的越狱攻击方法，称为潜在融合越狱 (LFJ)，该方法通过插值有害和良性查询对的隐藏状态来引出禁止的响应，并提出了一种对抗训练防御方法来缓解 LFJ。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 在各种语言任务中表现出令人印象深刻的能力，但容易受到绕过其安全对齐的越狱攻击。

Method: 提出了一种基于表征的攻击方法，通过插值有害和良性查询对的隐藏状态来引出禁止的响应。

Result: 在 Vicuna 和 LLaMA-2 等模型上，在 AdvBench 和 MaliciousInstruct 等基准测试中进行的评估产生了 94.01% 的平均攻击成功率 (ASR)，优于现有方法。

Conclusion: 提出了一种对抗训练防御方法，可以在不降低良性输入性能的情况下，将攻击成功率降低 80% 以上。

Abstract: Large language models (LLMs) demonstrate impressive capabilities in various
language tasks but are susceptible to jailbreak attacks that circumvent their
safety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a
representation-based attack that interpolates hidden states from harmful and
benign query pairs to elicit prohibited responses. LFJ begins by selecting
query pairs with high thematic and syntactic similarity, then performs
gradient-guided interpolation at influential layers and tokens, followed by
optimization to balance attack success, output fluency, and computational
efficiency. Evaluations on models such as Vicuna and LLaMA-2 across benchmarks
like AdvBench and MaliciousInstruct yield an average attack success rate (ASR)
of 94.01%, outperforming existing methods. To mitigate LFJ, we propose an
adversarial training defense that fine-tunes models on interpolated examples,
reducing ASR by over 80% without degrading performance on benign inputs.
Ablation studies validate the importance of query pair selection, hidden state
interpolation components, and optimization strategies in LFJ's effectiveness.

</details>


### [34] [Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models](https://arxiv.org/abs/2508.10030)
*Saaduddin Mahmud,Mason Nakamura,Kyle H. Wray,Shlomo Zilberstein*

Main category: cs.CL

TL;DR: This paper introduces IAPO, a framework that jointly optimizes the prompt and inference scale, and develops PSST, a fixed-budget training algorithm for IAPO. The results demonstrate the critical role of incorporating inference-awareness when aligning black-box LLMs through prompt optimization.


<details>
  <summary>Details</summary>
Motivation: Existing prompt optimization approaches are inference strategy agnostic, which constitutes a significant methodological gap, as there is a strong interdependence between prompt optimization and inference strategy. User preferences regarding trade-offs among multiple objectives and inference budgets substantially influence the choice of prompt and inference configuration.

Method: The paper introduces a unified novel framework named IAPO (Inference-Aware Prompt Optimization) that jointly optimizes the prompt and inference scale, and develops a fixed-budget training algorithm for IAPO, which the authors call PSST (Prompt Scaling via Sequential Trimming).

Result: The paper's empirical and theoretical analysis reveals a strong interdependence between prompt optimization and inference strategies.

Conclusion: The paper evaluates the effectiveness of PSST on six different tasks and demonstrates the critical role of incorporating inference-awareness when aligning black-box LLMs through prompt optimization.

Abstract: Prompt optimization methods have demonstrated significant effectiveness in
aligning black-box large language models (LLMs). In parallel, inference scaling
strategies such as Best-of-N Sampling and Majority Voting have also proven to
enhance alignment and performance by trading off computation. However, existing
prompt optimization approaches are inference strategy agnostic; that is, they
optimize prompts without regard to the inference strategy employed during
deployment. This constitutes a significant methodological gap, as our empirical
and theoretical analysis reveals a strong interdependence between these two
paradigms. Moreover, we find that user preferences regarding trade-offs among
multiple objectives and inference budgets substantially influence the choice of
prompt and inference configuration. To address this gap, we introduce a unified
novel framework named IAPO (Inference-Aware Prompt Optimization) that jointly
optimizes the prompt and inference scale, while being aware of the inference
budget and different task objectives. We then develop a fixed-budget training
algorithm for IAPO, which we call PSST (Prompt Scaling via Sequential
Trimming), and analyze finite-budget guarantees on error probability. Finally,
we evaluate the effectiveness of PSST on six different tasks, including
multi-objective text generation and reasoning, and demonstrate the critical
role of incorporating inference-awareness when aligning black-box LLMs through
prompt optimization.

</details>


### [35] [The Cost of Thinking: Increased Jailbreak Risk in Large Language Models](https://arxiv.org/abs/2508.10032)
*Fan Yang*

Main category: cs.CL

TL;DR: 具有思维模式的LLM更容易受到Jailbreak攻击。提出了一种安全思维干预方法，以降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 发现具有思维模式的LLM更容易受到Jailbreak攻击。攻击思维模式的LLM的成功率几乎高于非思维模式。出于教育目的和过长的思考长度是成功攻击数据的特征，并且LLM在知道问题有害时也会给出有害的答案。

Method: 提出了一种安全思维干预方法，通过在prompt中添加“特定思维tokens”来显式引导LLM的内部思维过程。

Result: 安全思维干预可以显著降低具有思维模式的LLM的攻击成功率。

Conclusion: 提出了一种安全思维干预方法，通过在提示中添加“特定思维tokens”来显式引导LLM的内部思维过程，从而显著降低LLM思维模式下的攻击成功率。

Abstract: Thinking mode has always been regarded as one of the most valuable modes in
LLMs. However, we uncover a surprising and previously overlooked phenomenon:
LLMs with thinking mode are more easily broken by Jailbreak attack. We evaluate
9 LLMs on AdvBench and HarmBench and find that the success rate of attacking
thinking mode in LLMs is almost higher than that of non-thinking mode. Through
large numbers of sample studies, it is found that for educational purposes and
excessively long thinking lengths are the characteristics of successfully
attacked data, and LLMs also give harmful answers when they mostly know that
the questions are harmful. In order to alleviate the above problems, this paper
proposes a method of safe thinking intervention for LLMs, which explicitly
guides the internal thinking processes of LLMs by adding "specific thinking
tokens" of LLMs to the prompt. The results demonstrate that the safe thinking
intervention can significantly reduce the attack success rate of LLMs with
thinking mode.

</details>


### [36] [mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning](https://arxiv.org/abs/2508.10137)
*Nghia Trung Ngo,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: mSCoRe: A challenging multilingual benchmark reveals limitations in LLMs' commonsense reasoning and suggests improvements.


<details>
  <summary>Details</summary>
Motivation: The mechanism underlying LLMs' utilization of different human reasoning skills remains poorly investigated, especially for multilingual commonsense reasoning.

Method: A Multilingual and Scalable Benchmark for Skill-based Commonsense Reasoning (mSCoRe) incorporating a novel taxonomy of reasoning skills, a robust data synthesis pipeline, and a complexity scaling framework.

Result: Extensive experiments on eight state-of-the-art LLMs demonstrate mSCoRe's challenge and reveal limitations in nuanced multilingual general and cultural commonsense. Detailed analysis on reasoning processes is provided.

Conclusion: mSCoRe benchmark remains significantly challenging for current LLMs, revealing limitations in multilingual general and cultural commonsense reasoning. Detailed analysis suggests future directions for improvement.

Abstract: Recent advancements in reasoning-reinforced Large Language Models (LLMs) have
shown remarkable capabilities in complex reasoning tasks. However, the
mechanism underlying their utilization of different human reasoning skills
remains poorly investigated, especially for multilingual commonsense reasoning
that involves everyday knowledge across different languages and cultures. To
address this gap, we propose a \textbf{M}ultilingual and Scalable Benchmark for
\textbf{S}kill-based \textbf{Co}mmonsense \textbf{Re}asoning (\textbf{mSCoRe}).
Our benchmark incorporates three key components that are designed to
systematically evaluate LLM's reasoning capabilities, including: (1) a novel
taxonomy of reasoning skills that enables fine-grained analysis of models'
reasoning processes, (2) a robust data synthesis pipeline tailored specifically
for commonsense reasoning evaluation, and (3) a complexity scaling framework
allowing task difficulty to scale dynamically alongside future improvements in
LLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying
sizes and training approaches demonstrate that \textbf{mSCoRe} remains
significantly challenging for current models, particularly at higher complexity
levels. Our results reveal the limitations of such reasoning-reinforced models
when confronted with nuanced multilingual general and cultural commonsense. We
further provide detailed analysis on the models' reasoning processes,
suggesting future directions for improving multilingual commonsense reasoning
capabilities.

</details>


### [37] [Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs](https://arxiv.org/abs/2508.10142)
*Kartikeya Badola,Jonathan Simon,Arian Hosseini,Sara Marie Mc Carthy,Tsendsuren Munkhdalai,Abhimanyu Goyal,Tomáš Kočiský,Shyam Upadhyay,Bahare Fatemi,Mehran Kazemi*

Main category: cs.CL

TL;DR: LLMs need improvement in interactive tasks. A new benchmark reveals weaknesses in instruction following, reasoning, and planning.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with nuanced environments or interactive tasks common in real-world scenarios, highlighting the need for LLMs that can engage in multi-turn dialogue, seek information, and reason with incomplete data.

Method: A novel benchmark comprising multi-turn tasks designed to test reasoning, interactive dialogue, and information-seeking abilities with deterministic scoring mechanisms.

Result: Evaluating frontier models reveals significant headroom; most errors emerge from poor instruction following, reasoning failures, and poor planning.

Conclusion: This benchmark provides insights into LLMs' strengths/weaknesses in complex, interactive scenarios and offers a platform for future research.

Abstract: Large language models (LLMs) excel at solving problems with clear and
complete statements, but often struggle with nuanced environments or
interactive tasks which are common in most real-world scenarios. This
highlights the critical need for developing LLMs that can effectively engage in
logically consistent multi-turn dialogue, seek information and reason with
incomplete data. To this end, we introduce a novel benchmark comprising a suite
of multi-turn tasks each designed to test specific reasoning, interactive
dialogue, and information-seeking abilities. These tasks have deterministic
scoring mechanisms, thus eliminating the need for human intervention.
Evaluating frontier models on our benchmark reveals significant headroom. Our
analysis shows that most errors emerge from poor instruction following,
reasoning failures, and poor planning. This benchmark provides valuable
insights into the strengths and weaknesses of current LLMs in handling complex,
interactive scenarios and offers a robust platform for future research aimed at
improving these critical capabilities.

</details>


### [38] [LaajMeter: A Framework for LaaJ Evaluation](https://arxiv.org/abs/2508.10161)
*Gal Amram,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Avi Ziv*

Main category: cs.CL

TL;DR: LaaJMeter is introduced to address the challenges of using LLMs as judges in domain-specific contexts, offering a simulation-based framework for controlled meta-evaluation, which helps validate metrics and estimate evaluator performance thresholds.


<details>
  <summary>Details</summary>
Motivation: LLMs as judges (LaaJs) face challenges in domain-specific contexts due to scarce annotated data and costly expert evaluation. Existing meta-evaluation metrics lack validation, making it difficult to determine LaaJ quality and performance thresholds.

Method: introduction of LaaJMeter, a simulation-based framework for controlled meta-evaluation of LaaJs

Result: Demonstrated LaaJMeter's utility in a code translation task, highlighting limitations of common metrics and the importance of principled metric selection.

Conclusion: LaaJMeter provides a scalable and extensible solution for assessing LaaJs in low-resource settings, contributing to trustworthy and reproducible evaluation in NLP.

Abstract: Large Language Models (LLMs) are increasingly used as evaluators in natural
language processing tasks, a paradigm known as LLM-as-a-Judge (LaaJ). While
effective in general domains, LaaJs pose significant challenges in
domain-specific contexts, where annotated data is scarce and expert evaluation
is costly. In such cases, meta-evaluation is often performed using metrics that
have not been validated for the specific domain in which they are applied. As a
result, it becomes difficult to determine which metrics effectively identify
LaaJ quality, and further, what threshold indicates sufficient evaluator
performance. In this work, we introduce LaaJMeter, a simulation-based framework
for controlled meta-evaluation of LaaJs. LaaJMeter enables engineers to
generate synthetic data representing virtual models and judges, allowing
systematic analysis of evaluation metrics under realistic conditions. This
helps practitioners validate and refine LaaJs for specific evaluation tasks:
they can test whether their metrics correctly distinguish between better and
worse (virtual) LaaJs, and estimate appropriate thresholds for evaluator
adequacy.
  We demonstrate the utility of LaaJMeter in a code translation task involving
a legacy programming language, showing how different metrics vary in
sensitivity to evaluator quality. Our results highlight the limitations of
common metrics and the importance of principled metric selection. LaaJMeter
provides a scalable and extensible solution for assessing LaaJs in low-resource
settings, contributing to the broader effort to ensure trustworthy and
reproducible evaluation in NLP.

</details>


### [39] [Estimating Machine Translation Difficulty](https://arxiv.org/abs/2508.10175)
*Lorenzo Proietti,Stefano Perrella,Vilém Zouhar,Roberto Navigli,Tom Kocmi*

Main category: cs.CL

TL;DR: The paper focuses on estimating the difficulty of translation, introduces new models (Sentinel-src) that outperform existing methods, and releases improved versions for identifying challenging texts for machine translation.


<details>
  <summary>Details</summary>
Motivation: High-quality machine translation outputs make it difficult to distinguish between state-of-the-art models and identify areas for future improvement. Automatically identifying texts where machine translation systems struggle holds promise.

Method: Formalized the task of translation difficulty estimation, defined difficulty based on expected translation quality, introduced a new metric to evaluate difficulty estimators, and constructed more challenging machine translation benchmarks.

Result: Dedicated models outperform heuristic-based methods and LLM-as-a-judge approaches. Two improved models, Sentinel-src-24 and Sentinel-src-25, are released.

Conclusion: Dedicated models (Sentinel-src) outperform heuristic-based methods and LLM-as-a-judge approaches in translation difficulty estimation. Improved models Sentinel-src-24 and Sentinel-src-25 are released for identifying challenging texts for machine translation systems.

Abstract: Machine translation quality has began achieving near-perfect translations in
some setups. These high-quality outputs make it difficult to distinguish
between state-of-the-art models and to identify areas for future improvement.
Automatically identifying texts where machine translation systems struggle
holds promise for developing more discriminative evaluations and guiding future
research.
  We formalize the task of translation difficulty estimation, defining a text's
difficulty based on the expected quality of its translations. We introduce a
new metric to evaluate difficulty estimators and use it to assess both
baselines and novel approaches. Finally, we demonstrate the practical utility
of difficulty estimators by using them to construct more challenging machine
translation benchmarks. Our results show that dedicated models (dubbed
Sentinel-src) outperform both heuristic-based methods (e.g. word rarity or
syntactic complexity) and LLM-as-a-judge approaches. We release two improved
models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which
can be used to scan large collections of texts and select those most likely to
challenge contemporary machine translation systems.

</details>


### [40] [Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs](https://arxiv.org/abs/2508.10180)
*Wenlong Deng,Jiaming Zhang,Qi Zeng,Christos Thrampoulidis,Boying Gong,Xiaoxiao Li*

Main category: cs.CL

TL;DR: For-Value: A forward-only data valuation framework


<details>
  <summary>Details</summary>
Motivation: Quantifying the influence of individual training samples is essential for enhancing the transparency and accountability of large language models (LLMs) and vision-language models (VLMs). However, existing data valuation methods often rely on Hessian information or model retraining, making them computationally prohibitive for billion-parameter models.

Method: a forward-only data valuation framework that enables scalable and efficient influence estimation for both LLMs and VLMs. computes influence scores using a simple closed-form expression based solely on a single forward pass

Result: For-Value accurately estimates per-sample influence by capturing alignment in hidden representations and prediction errors between training and validation samples.

Conclusion: For-Value matches or outperforms gradient-based baselines in identifying impactful fine-tuning examples and effectively detecting mislabeled data.

Abstract: Quantifying the influence of individual training samples is essential for
enhancing the transparency and accountability of large language models (LLMs)
and vision-language models (VLMs). However, existing data valuation methods
often rely on Hessian information or model retraining, making them
computationally prohibitive for billion-parameter models. In this work, we
introduce For-Value, a forward-only data valuation framework that enables
scalable and efficient influence estimation for both LLMs and VLMs. By
leveraging the rich representations of modern foundation models, For-Value
computes influence scores using a simple closed-form expression based solely on
a single forward pass, thereby eliminating the need for costly gradient
computations. Our theoretical analysis demonstrates that For-Value accurately
estimates per-sample influence by capturing alignment in hidden representations
and prediction errors between training and validation samples. Extensive
experiments show that For-Value matches or outperforms gradient-based baselines
in identifying impactful fine-tuning examples and effectively detecting
mislabeled data.

</details>


### [41] [PakBBQ: A Culturally Adapted Bias Benchmark for QA](https://arxiv.org/abs/2508.10186)
*Abdullah Hashmat,Muhammad Arham Mirza,Agha Ali Raza*

Main category: cs.CL

TL;DR: This paper introduces PakBBQ, a dataset for evaluating bias in LLMs in the context of Pakistan, and finds that disambiguation, language choice, and question framing can affect bias.


<details>
  <summary>Details</summary>
Motivation: Most LLMs are trained and evaluated on Western centric data, with little attention paid to low-resource languages and regional contexts.  The paper aims to address this gap.

Method: The authors introduce PakBBQ, a culturally and regionally adapted extension of the original Bias Benchmark for Question Answering (BBQ) dataset. They evaluate multiple multilingual LLMs under both ambiguous and explicitly disambiguated contexts, as well as negative versus non negative question framings.

Result: The experiments reveal (i) an average accuracy gain of 12% with disambiguation, (ii) consistently stronger counter bias behaviors in Urdu than in English, and (iii) marked framing effects that reduce stereotypical responses when questions are posed negatively.

Conclusion: This paper highlights the importance of contextualized benchmarks and simple prompt engineering strategies for bias mitigation in low resource settings.

Abstract: With the widespread adoption of Large Language Models (LLMs) across various
applications, it is empirical to ensure their fairness across all user
communities. However, most LLMs are trained and evaluated on Western centric
data, with little attention paid to low-resource languages and regional
contexts. To address this gap, we introduce PakBBQ, a culturally and regionally
adapted extension of the original Bias Benchmark for Question Answering (BBQ)
dataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8
categories in both English and Urdu, covering eight bias dimensions including
age, disability, appearance, gender, socio-economic status, religious, regional
affiliation, and language formality that are relevant in Pakistan. We evaluate
multiple multilingual LLMs under both ambiguous and explicitly disambiguated
contexts, as well as negative versus non negative question framings. Our
experiments reveal (i) an average accuracy gain of 12\% with disambiguation,
(ii) consistently stronger counter bias behaviors in Urdu than in English, and
(iii) marked framing effects that reduce stereotypical responses when questions
are posed negatively. These findings highlight the importance of contextualized
benchmarks and simple prompt engineering strategies for bias mitigation in low
resource settings.

</details>


### [42] [Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models](https://arxiv.org/abs/2508.10192)
*Igor Halperin*

Main category: cs.CL

TL;DR: This paper introduces a new framework, Semantic Divergence Metrics (SDM), to detect when large language models go off-track and generate nonsensical or unfaithful text. It uses semantic analysis and information theory to measure the divergence between prompts and responses.


<details>
  <summary>Details</summary>
Motivation: LLMs suffer from hallucinations, where they generate non-factual or unfaithful text. The paper focuses on detecting Faithfulness Hallucinations, specifically confabulations, which are arbitrary and semantically misaligned responses.

Method: The paper uses joint clustering on sentence embeddings to create a shared topic space for prompts and answers, then computes information-theoretic metrics like Jensen-Shannon divergence, Wasserstein distance, and KL divergence to measure the semantic divergence between prompts and responses.

Result: The paper introduces Semantic Divergence Metrics (SDM), a framework for detecting Faithfulness Hallucinations. It identifies KL divergence as an indicator of Semantic Exploration and combines metrics into the Semantic Box for classifying LLM response types. A high $\mathcal{S}_H$ score indicates a Faithfulness hallucination.

Conclusion: The paper introduces the Semantic Box, a diagnostic framework for classifying LLM response types, including confident confabulations, based on Semantic Divergence Metrics (SDM).

Abstract: The proliferation of Large Language Models (LLMs) is challenged by
hallucinations, critical failure modes where models generate non-factual,
nonsensical or unfaithful text. This paper introduces Semantic Divergence
Metrics (SDM), a novel lightweight framework for detecting Faithfulness
Hallucinations -- events of severe deviations of LLMs responses from input
contexts. We focus on a specific implementation of these LLM errors,
{confabulations, defined as responses that are arbitrary and semantically
misaligned with the user's query. Existing methods like Semantic Entropy test
for arbitrariness by measuring the diversity of answers to a single, fixed
prompt. Our SDM framework improves upon this by being more prompt-aware: we
test for a deeper form of arbitrariness by measuring response consistency not
only across multiple answers but also across multiple, semantically-equivalent
paraphrases of the original prompt. Methodologically, our approach uses joint
clustering on sentence embeddings to create a shared topic space for prompts
and answers. A heatmap of topic co-occurances between prompts and responses can
be viewed as a quantified two-dimensional visualization of the user-machine
dialogue. We then compute a suite of information-theoretic metrics to measure
the semantic divergence between prompts and responses. Our practical score,
$\mathcal{S}_H$, combines the Jensen-Shannon divergence and Wasserstein
distance to quantify this divergence, with a high score indicating a
Faithfulness hallucination. Furthermore, we identify the KL divergence
KL(Answer $||$ Prompt) as a powerful indicator of \textbf{Semantic
Exploration}, a key signal for distinguishing different generative behaviors.
These metrics are further combined into the Semantic Box, a diagnostic
framework for classifying LLM response types, including the dangerous,
confident confabulation.

</details>


### [43] [Understanding Textual Emotion Through Emoji Prediction](https://arxiv.org/abs/2508.10222)
*Ethan Gordon,Nishank Kuppa,Rigved Tummala,Sriram Anasuri*

Main category: cs.CL

TL;DR: 本项目使用四种深度学习架构来预测短文本序列中的表情符号。


<details>
  <summary>Details</summary>
Motivation: 本项目旨在使用四种深度学习架构，从短文本序列中探索表情符号预测。

Method: 使用了四种深度学习架构：前馈网络、CNN、Transformer和BERT。

Result: 结果表明，由于其预训练优势，BERT实现了最高的整体性能，而CNN在稀有表情符号类别上表现出卓越的功效。

Conclusion: BERT在整体表现上最佳，CNN在稀有表情符号类别上表现出卓越的功效。这项研究表明了架构选择和超参数调整对于情感感知表情符号预测的重要性，有助于改善人机交互。

Abstract: This project explores emoji prediction from short text sequences using four
deep learning architectures: a feed-forward network, CNN, transformer, and
BERT. Using the TweetEval dataset, we address class imbalance through focal
loss and regularization techniques. Results show BERT achieves the highest
overall performance due to its pre-training advantage, while CNN demonstrates
superior efficacy on rare emoji classes. This research shows the importance of
architecture selection and hyperparameter tuning for sentiment-aware emoji
prediction, contributing to improved human-computer interaction.

</details>


### [44] [Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia](https://arxiv.org/abs/2508.10226)
*Andrew X. Chen,Guillermo Horga,Sean Escola*

Main category: cs.CL

TL;DR: LLM可以预测CHR患者的BPRS评分，具有接近人类评估者的信度，并有潜力提高评估的标准化和准确性，尤其是在外语和纵向数据方面。


<details>
  <summary>Details</summary>
Motivation: 临床精神分裂症高危（CHR）患者需要密切监测症状，以便为适当的治疗提供信息。简明精神病评定量表（BPRS）是一种经过验证的常用研究工具，用于测量精神分裂症和其他精神病患者的症状；但是，它在临床实践中并不常用，因为它需要冗长的结构化访谈。

Method: 利用大型语言模型（LLM）预测来自AMP-SCZ队列中409名CHR患者的临床访谈记录的BPRS评分。

Result: LLM预测的零样本性能与真实评估相比，接近人类间和人类内部的评估者信度（中位数一致性：0.84，ICC：0.73）。LLM在外语评估（中位数一致性：0.88，ICC：0.70）以及在单样本或少样本学习方法中整合纵向信息方面的准确性。

Conclusion: LLMs具有提高和标准化CHR患者评估的巨大潜力，尤其是在外语评估和整合纵向信息方面。

Abstract: Patients who are at clinical high risk (CHR) for schizophrenia need close
monitoring of their symptoms to inform appropriate treatments. The Brief
Psychiatric Rating Scale (BPRS) is a validated, commonly used research tool for
measuring symptoms in patients with schizophrenia and other psychotic
disorders; however, it is not commonly used in clinical practice as it requires
a lengthy structured interview. Here, we utilize large language models (LLMs)
to predict BPRS scores from clinical interview transcripts in 409 CHR patients
from the Accelerating Medicines Partnership Schizophrenia (AMP-SCZ) cohort.
Despite the interviews not being specifically structured to measure the BPRS,
the zero-shot performance of the LLM predictions compared to the true
assessment (median concordance: 0.84, ICC: 0.73) approaches human inter- and
intra-rater reliability. We further demonstrate that LLMs have substantial
potential to improve and standardize the assessment of CHR patients via their
accuracy in assessing the BPRS in foreign languages (median concordance: 0.88,
ICC: 0.70), and integrating longitudinal information in a one-shot or few-shot
learning approach.

</details>


### [45] [A Computational Approach to Analyzing Language Change and Variation in the Constructed Language Toki Pona](https://arxiv.org/abs/2508.10246)
*Daniel Huang,Hyoun-A Joo*

Main category: cs.CL

TL;DR: This study examines language change in Toki Pona, suggesting that constructed languages evolve naturally like natural languages.


<details>
  <summary>Details</summary>
Motivation: explores language change and variation in Toki Pona

Method: computational and corpus-based approach

Result: changes in preferences of content words for different syntactic positions over time and variation in usage across different corpora

Conclusion: sociolinguistic factors influence Toki Pona in the same way as natural languages, and that even constructed linguistic systems naturally evolve as communities use them

Abstract: This study explores language change and variation in Toki Pona, a constructed
language with approximately 120 core words. Taking a computational and
corpus-based approach, the study examines features including fluid word classes
and transitivity in order to examine (1) changes in preferences of content
words for different syntactic positions over time and (2) variation in usage
across different corpora. The results suggest that sociolinguistic factors
influence Toki Pona in the same way as natural languages, and that even
constructed linguistic systems naturally evolve as communities use them.

</details>


### [46] [Inductive Bias Extraction and Matching for LLM Prompts](https://arxiv.org/abs/2508.10295)
*Christian M. Angel,Francis Ferraro*

Main category: cs.CL

TL;DR: 通过将LLM的输出融入提示中，可以改善提示的措辞，从而提高LLM在分类和排序任务中的性能。


<details>
  <summary>Details</summary>
Motivation: LLM对提示措辞的细微变化很敏感，这部分归因于LLM中存在的归纳偏置。

Method: 使用LLM的输出作为其提示的一部分，从而创建一个与模型中的归纳偏置相匹配的提示。

Result: 使用归纳偏置提取和匹配策略，分类任务的LLM Likert评级提高了19%，排序任务的LLM Likert评级提高了27%。

Conclusion: 使用归纳偏置提取和匹配策略可以提高LLM在分类和排序任务中的性能。

Abstract: The active research topic of prompt engineering makes it evident that LLMs
are sensitive to small changes in prompt wording. A portion of this can be
ascribed to the inductive bias that is present in the LLM. By using an LLM's
output as a portion of its prompt, we can more easily create satisfactory
wording for prompts. This has the effect of creating a prompt that matches the
inductive bias in model. Empirically, we show that using this Inductive Bias
Extraction and Matching strategy improves LLM Likert ratings used for
classification by up to 19% and LLM Likert ratings used for ranking by up to
27%.

</details>


### [47] [Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race](https://arxiv.org/abs/2508.10304)
*Gustavo Bonil,Simone Hashiguti,Jhessica Silva,João Gondim,Helena Maia,Nádia Silva,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: This study uses qualitative methods to analyze gender and racial biases in LLM-generated stories, finding that LLMs reinforce stereotypes and struggle to correct them, highlighting the need for critical approaches to AI ethics.


<details>
  <summary>Details</summary>
Motivation: Assess whether LLMs reproduce biases, such as discrimination and racialization, while maintaining hegemonic discourses. Current bias detection approaches rely mostly on quantitative, automated methods, which often overlook the nuanced ways in which biases emerge in natural language.

Method: Qualitative, discursive framework through manual analysis of LLM-generated short stories featuring Black and white women.

Result: Black women are portrayed as tied to ancestry and resistance, while white women appear in self-discovery processes. When prompted to correct biases, models offered superficial revisions that maintained problematic meanings.

Conclusion: LLMs replicate crystallized discursive representations, reinforcing essentialization and a sense of social immobility. Superficial revisions maintain problematic meanings, revealing limitations in fostering inclusive narratives. The study demonstrates the ideological functioning of algorithms and has significant implications for the ethical use and development of AI.

Abstract: With the advance of Artificial Intelligence (AI), Large Language Models
(LLMs) have gained prominence and been applied in diverse contexts. As they
evolve into more sophisticated versions, it is essential to assess whether they
reproduce biases, such as discrimination and racialization, while maintaining
hegemonic discourses. Current bias detection approaches rely mostly on
quantitative, automated methods, which often overlook the nuanced ways in which
biases emerge in natural language. This study proposes a qualitative,
discursive framework to complement such methods. Through manual analysis of
LLM-generated short stories featuring Black and white women, we investigate
gender and racial biases. We contend that qualitative methods such as the one
proposed here are fundamental to help both developers and users identify the
precise ways in which biases manifest in LLM outputs, thus enabling better
conditions to mitigate them. Results show that Black women are portrayed as
tied to ancestry and resistance, while white women appear in self-discovery
processes. These patterns reflect how language models replicate crystalized
discursive representations, reinforcing essentialization and a sense of social
immobility. When prompted to correct biases, models offered superficial
revisions that maintained problematic meanings, revealing limitations in
fostering inclusive narratives. Our results demonstrate the ideological
functioning of algorithms and have significant implications for the ethical use
and development of AI. The study reinforces the need for critical,
interdisciplinary approaches to AI design and deployment, addressing how
LLM-generated discourses reflect and perpetuate inequalities.

</details>


### [48] [ReviewRL: Towards Automated Scientific Review with RL](https://arxiv.org/abs/2508.10308)
*Sihang Zeng,Kai Tian,Kaiyan Zhang,Yuru wang,Junqi Gao,Runze Liu,Sa Yang,Jingxuan Li,Xinwei Long,Jiaheng Ma,Biqing Qi,Bowen Zhou*

Main category: cs.CL

TL;DR: ReviewRL is a reinforcement learning framework that generates comprehensive and factually grounded scientific paper reviews, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Peer review is essential for scientific progress but faces growing challenges due to increasing submission volumes and reviewer fatigue. Existing automated review approaches struggle with factual accuracy, rating consistency, and analytical depth, often generating superficial or generic feedback lacking the insights characteristic of high-quality human reviews.

Method: a reinforcement learning framework for generating comprehensive and factually grounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP retrieval-augmented context generation pipeline that incorporates relevant scientific literature, (2) supervised fine-tuning that establishes foundational reviewing capabilities, and (3) a reinforcement learning procedure with a composite reward function that jointly enhances review quality and rating accuracy.

Result: ReviewRL significantly outperforms existing methods across both rule-based metrics and model-based quality assessments.

Conclusion: ReviewRL establishes a foundational framework for RL-driven automatic critique generation in scientific discovery, demonstrating promising potential for future development in this domain.

Abstract: Peer review is essential for scientific progress but faces growing challenges
due to increasing submission volumes and reviewer fatigue. Existing automated
review approaches struggle with factual accuracy, rating consistency, and
analytical depth, often generating superficial or generic feedback lacking the
insights characteristic of high-quality human reviews. We introduce ReviewRL, a
reinforcement learning framework for generating comprehensive and factually
grounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP
retrieval-augmented context generation pipeline that incorporates relevant
scientific literature, (2) supervised fine-tuning that establishes foundational
reviewing capabilities, and (3) a reinforcement learning procedure with a
composite reward function that jointly enhances review quality and rating
accuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL
significantly outperforms existing methods across both rule-based metrics and
model-based quality assessments. ReviewRL establishes a foundational framework
for RL-driven automatic critique generation in scientific discovery,
demonstrating promising potential for future development in this domain. The
implementation of ReviewRL will be released at GitHub.

</details>


### [49] [From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document Analysis](https://arxiv.org/abs/2508.10311)
*Xuan Li,Jialiang Dong,Raymond Wong*

Main category: cs.CL

TL;DR: DOTABLER是一个以表格为中心的语义文档解析框架，旨在揭示表格及其上下文之间的深层语义联系，并在表格上下文语义分析方面优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在布局分析、表格检测和数据提取等表面任务，缺乏对表格及其上下文关联的深度语义解析，限制了跨段落数据解释和上下文一致性分析等高级任务。

Method: DOTABLER利用自定义数据集和预训练模型的领域特定微调，整合了一个完整的解析管道来识别与表格在语义上相关的上下文片段。

Result: DOTABLER实现了超过90%的精确率和F1分数，证明了其在表格上下文语义分析和深度文档解析方面的卓越性能。

Conclusion: DOTABLER在表格上下文语义分析和深度文档解析方面表现出色，优于GPT-4o等先进模型，在真实PDF的近4,000页和1,000多个表格上评估，实现了超过90%的精确率和F1分数。

Abstract: Documents are core carriers of information and knowl-edge, with broad
applications in finance, healthcare, and scientific research. Tables, as the
main medium for structured data, encapsulate key information and are among the
most critical document components. Existing studies largely focus on
surface-level tasks such as layout analysis, table detection, and data
extraction, lacking deep semantic parsing of tables and their contextual
associations. This limits advanced tasks like cross-paragraph data
interpretation and context-consistent analysis. To address this, we propose
DOTABLER, a table-centric semantic document parsing framework designed to
uncover deep semantic links between tables and their context. DOTABLER
leverages a custom dataset and domain-specific fine-tuning of pre-trained
models, integrating a complete parsing pipeline to identify context segments
semantically tied to tables. Built on this semantic understanding, DOTABLER
implements two core functionalities: table-centric document structure parsing
and domain-specific table retrieval, delivering comprehensive table-anchored
semantic analysis and precise extraction of semantically relevant tables.
Evaluated on nearly 4,000 pages with over 1,000 tables from real-world PDFs,
DOTABLER achieves over 90% Precision and F1 scores, demonstrating superior
performance in table-context semantic analysis and deep document parsing
compared to advanced models such as GPT-4o.

</details>


### [50] [Beyond Semantic Understanding: Preserving Collaborative Frequency Components in LLM-based Recommendation](https://arxiv.org/abs/2508.10312)
*Minhao Wang,Yunhang He,Cong Xu,Zhangchi Zhu,Wei Zhang*

Main category: cs.CL

TL;DR: FreLLM4Rec 通过全局图低通滤波器 (G-LPF) 和时间频率调制 (TFM) 来平衡语义和协同信息，从而缓解了协同信号衰减，并在推荐性能上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 基于 LLM 的推荐系统表现出过度强调用户交互历史中语义相关性的趋势。当采用预训练的协同 ID 嵌入作为输入时，与传统的基于 Transformer 的序列模型相比，基于 LLM 的推荐系统会逐层削弱 LLM 主干中固有的协同信号。

Method: 我们引入 FreLLM4Rec，这是一种旨在从频谱角度平衡语义和协同信息的方法。项目嵌入首先使用全局图低通滤波器 (G-LPF) 进行纯化，以初步消除不相关的高频噪声。然后，时间频率调制 (TFM) 逐层主动保留协同信号。

Result: FreLLM4Rec 成功缓解了协同信号衰减并实现了具有竞争力的性能，与最佳基线相比，NDCG@10 提高了 8.00%。

Conclusion: FreLLM4Rec成功缓解了协同信号衰减并实现了具有竞争力的性能，与最佳基线相比，NDCG@10 提高了 8.00%。

Abstract: Recommender systems in concert with Large Language Models (LLMs) present
promising avenues for generating semantically-informed recommendations.
However, LLM-based recommenders exhibit a tendency to overemphasize semantic
correlations within users' interaction history. When taking pretrained
collaborative ID embeddings as input, LLM-based recommenders progressively
weaken the inherent collaborative signals as the embeddings propagate through
LLM backbones layer by layer, as opposed to traditional Transformer-based
sequential models in which collaborative signals are typically preserved or
even enhanced for state-of-the-art performance. To address this limitation, we
introduce FreLLM4Rec, an approach designed to balance semantic and
collaborative information from a spectral perspective. Item embeddings that
incorporate both semantic and collaborative information are first purified
using a Global Graph Low-Pass Filter (G-LPF) to preliminarily remove irrelevant
high-frequency noise. Temporal Frequency Modulation (TFM) then actively
preserves collaborative signal layer by layer. Note that the collaborative
preservation capability of TFM is theoretically guaranteed by establishing a
connection between the optimal but hard-to-implement local graph fourier
filters and the suboptimal yet computationally efficient frequency-domain
filters. Extensive experiments on four benchmark datasets demonstrate that
FreLLM4Rec successfully mitigates collaborative signal attenuation and achieves
competitive performance, with improvements of up to 8.00\% in NDCG@10 over the
best baseline. Our findings provide insights into how LLMs process
collaborative information and offer a principled approach for improving
LLM-based recommendation systems.

</details>


### [51] [Cross-Prompt Encoder for Low-Performing Languages](https://arxiv.org/abs/2508.10352)
*Beso Mikaberidze,Teimuraz Saghinadze,Simon Ostermann,Philipp Muller*

Main category: cs.CL

TL;DR: 本文提出了一种新的prompt encoder结构，可以提高低资源语言的性能，并在多语言环境中提供更广泛的适应性。


<details>
  <summary>Details</summary>
Motivation: 先前的工作主要集中于通过小型神经提示编码器中的参数交互来稳定训练，但它们在跨语言迁移方面的更广泛潜力仍未被探索。本文旨在证明提示编码器可以在提高低性能语言的性能方面发挥核心作用。

Method: 结合轻量级编码架构与多种类型学上不同的语言的多源训练，设计了Cross-Prompt Encoder (XPE)。提出了双重软提示机制，将基于编码器的提示与直接训练的标准软提示相结合。

Result: 在SIB-200基准测试上的实验表明，XPE在低性能语言上最有效，而混合变体在多语言环境中提供更广泛的适应性。

Conclusion: XPE在低性能语言上最有效，而混合变体在多语言环境中提供更广泛的适应性。

Abstract: Soft prompts have emerged as a powerful alternative to adapters in
parameter-efficient fine-tuning (PEFT), enabling large language models (LLMs)
to adapt to downstream tasks without architectural changes or parameter
updates. While prior work has focused on stabilizing training via parameter
interaction in small neural prompt encoders, their broader potential for
transfer across languages remains unexplored. In this paper, we demonstrate
that a prompt encoder can play a central role in improving performance on
low-performing languages-those that achieve poor accuracy even under full-model
fine-tuning. We introduce the Cross-Prompt Encoder (XPE), which combines a
lightweight encoding architecture with multi-source training on typologically
diverse languages - a design that enables the model to capture abstract and
transferable patterns across languages. To complement XPE, we propose a Dual
Soft Prompt mechanism that combines an encoder-based prompt with a directly
trained standard soft prompt. This hybrid design proves especially effective
for target languages that benefit from both broadly shared structure and
language-specific alignment. Experiments on the SIB-200 benchmark reveal a
consistent trade-off: XPE is most effective for low-performing languages, while
hybrid variants offer broader adaptability across multilingual settings.

</details>


### [52] [Making Qwen3 Think in Korean with Reinforcement Learning](https://arxiv.org/abs/2508.10355)
*Jungyup Lee,Jemin Kim,Sang Park,SeungJae Lee*

Main category: cs.CL

TL;DR: A two-stage fine-tuning approach makes Qwen3 14B think natively in Korean, improving reasoning and problem-solving.


<details>
  <summary>Details</summary>
Motivation: To make the large language model Qwen3 14B think natively in Korean.

Method: A two-stage fine-tuning approach with supervised fine-tuning (SFT) and reinforcement learning with a customized Group Relative Policy Optimization (GRPO) algorithm.

Result: Improved Korean reasoning, problem-solving performance, and stability in GRPO training using an oracle judge model that calibrates the reward signal.

Conclusion: The RL-tuned model shows significantly improved results on advanced reasoning benchmarks while maintaining knowledge and language proficiency, performing its chain-of-thought in Korean.

Abstract: We present a two-stage fine-tuning approach to make the large language model
Qwen3 14B "think" natively in Korean. In the first stage, supervised
fine-tuning (SFT) on a high-quality Korean reasoning dataset establishes a
strong foundation in Korean logical reasoning, yielding notable improvements in
Korean-language tasks and even some gains in general reasoning ability. In the
second stage, we employ reinforcement learning with a customized Group Relative
Policy Optimization (GRPO) algorithm to further enhance both Korean reasoning
alignment and overall problem-solving performance. We address critical
stability challenges in GRPO training - such as reward hacking and policy
collapse - by introducing an oracle judge model that calibrates the reward
signal. Our approach achieves stable learning (avoiding the collapse observed
in naive GRPO) and leads to steady, incremental performance gains. The final
RL-tuned model demonstrates substantially improved results on advanced
reasoning benchmarks (particularly math and coding tasks) while maintaining
knowledge and language proficiency, successfully conducting its internal
chain-of-thought entirely in Korean.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [53] [Stochastic-based Patch Filtering for Few-Shot Learning](https://arxiv.org/abs/2508.10066)
*Javier Rodenas,Eduardo Aguilar,Petia Radeva*

Main category: cs.CV

TL;DR: SPFF通过随机过滤补丁嵌入来解决食物图像小样本学习中的挑战，有效地关注相关特征，并在分类基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 食物图像因其视觉复杂性和可变性，给小样本学习模型带来了独特的挑战。比较查询和支持图像时，这个问题导致无法关注最重要的元素，从而导致错误分类。

Method: 我们提出了基于随机的补丁过滤小样本学习(SPFF)来关注与类表示显示出更大相关性的补丁嵌入。

Result: 通过定性分析，我们证明了SPFF有效地关注了类特定的食物特征最突出的补丁，同时成功地过滤掉了不相关的补丁。

Conclusion: SPFF在小样本分类基准测试Food-101、VireoFood-172和UECFood-256上优于现有SoA方法。

Abstract: Food images present unique challenges for few-shot learning models due to
their visual complexity and variability. For instance, a pasta dish might
appear with various garnishes on different plates and in diverse lighting
conditions and camera perspectives. This problem leads to losing focus on the
most important elements when comparing the query with support images, resulting
in misclassification. To address this issue, we propose Stochastic-based Patch
Filtering for Few-Shot Learning (SPFF) to attend to the patch embeddings that
show greater correlation with the class representation. The key concept of SPFF
involves the stochastic filtering of patch embeddings, where patches less
similar to the class-aware embedding are more likely to be discarded. With
patch embedding filtered according to the probability of appearance, we use a
similarity matrix that quantifies the relationship between the query image and
its respective support images. Through a qualitative analysis, we demonstrate
that SPFF effectively focuses on patches where class-specific food features are
most prominent while successfully filtering out non-relevant patches. We
validate our approach through extensive experiments on few-shot classification
benchmarks: Food-101, VireoFood-172 and UECFood-256, outperforming the existing
SoA methods.

</details>


### [54] [DINOv3](https://arxiv.org/abs/2508.10104)
*Oriane Siméoni,Huy V. Vo,Maximilian Seitzer,Federico Baldassarre,Maxime Oquab,Cijo Jose,Vasil Khalidov,Marc Szafraniec,Seungeun Yi,Michaël Ramamonjisoa,Francisco Massa,Daniel Haziza,Luca Wehrstedt,Jianyuan Wang,Timothée Darcet,Théo Moutakanni,Leonel Sentana,Claire Roberts,Andrea Vedaldi,Jamie Tolan,John Brandt,Camille Couprie,Julien Mairal,Hervé Jégou,Patrick Labatut,Piotr Bojanowski*

Main category: cs.CV

TL;DR: DINOv3 is a versatile vision foundation model that outperforms previous self- and weakly-supervised models by leveraging Gram anchoring and post-hoc strategies.


<details>
  <summary>Details</summary>
Motivation: Self-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures. By not being tailored to specific tasks or domains, this training paradigm has the potential to learn visual representations from diverse sources.

Method: A new method called Gram anchoring is introduced, which effectively addresses the known yet unsolved issue of dense feature maps degrading during long training schedules. Post-hoc strategies are applied that further enhance the models' flexibility with respect to resolution, model size, and alignment with text.

Result: DINOv3 outperforms the specialized state of the art across a broad range of settings, without fine-tuning.

Conclusion: DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models. The DINOv3 suite of vision models is shared to advance the state of the art on a wide spectrum of tasks and data.

Abstract: Self-supervised learning holds the promise of eliminating the need for manual
data annotation, enabling models to scale effortlessly to massive datasets and
larger architectures. By not being tailored to specific tasks or domains, this
training paradigm has the potential to learn visual representations from
diverse sources, ranging from natural to aerial images -- using a single
algorithm. This technical report introduces DINOv3, a major milestone toward
realizing this vision by leveraging simple yet effective strategies. First, we
leverage the benefit of scaling both dataset and model size by careful data
preparation, design, and optimization. Second, we introduce a new method called
Gram anchoring, which effectively addresses the known yet unsolved issue of
dense feature maps degrading during long training schedules. Finally, we apply
post-hoc strategies that further enhance our models' flexibility with respect
to resolution, model size, and alignment with text. As a result, we present a
versatile vision foundation model that outperforms the specialized state of the
art across a broad range of settings, without fine-tuning. DINOv3 produces
high-quality dense features that achieve outstanding performance on various
vision tasks, significantly surpassing previous self- and weakly-supervised
foundation models. We also share the DINOv3 suite of vision models, designed to
advance the state of the art on a wide spectrum of tasks and data by providing
scalable solutions for diverse resource constraints and deployment scenarios.

</details>


### [55] [Empowering Morphing Attack Detection using Interpretable Image-Text Foundation Model](https://arxiv.org/abs/2508.10110)
*Sushrut Patwardhan,Raghavendra Ramachandra,Sushma Venkatesh*

Main category: cs.CV

TL;DR: This paper introduces a multimodal approach using CLIP for morphing attack detection, achieving good zero-shot performance and text-based explainability.


<details>
  <summary>Details</summary>
Motivation: Morphing attack detection is crucial for reliable face recognition systems.

Method: A multimodal learning framework using Contrastive Language-Image Pretraining (CLIP) is proposed. The framework is evaluated with ten different textual prompts.

Result: The proposed framework achieves generalizable morphing attack detection and predicts relevant text snippets in zero-shot evaluation across five different morphing generation techniques and three mediums.

Conclusion: The paper presents a multimodal learning approach for morphing attack detection, demonstrating generalizable detection and relevant text snippet prediction using CLIP.

Abstract: Morphing attack detection has become an essential component of face
recognition systems for ensuring a reliable verification scenario. In this
paper, we present a multimodal learning approach that can provide a textual
description of morphing attack detection. We first show that zero-shot
evaluation of the proposed framework using Contrastive Language-Image
Pretraining (CLIP) can yield not only generalizable morphing attack detection,
but also predict the most relevant text snippet. We present an extensive
analysis of ten different textual prompts that include both short and long
textual prompts. These prompts are engineered by considering the human
understandable textual snippet. Extensive experiments were performed on a face
morphing dataset that was developed using a publicly available face biometric
dataset. We present an evaluation of SOTA pre-trained neural networks together
with the proposed framework in the zero-shot evaluation of five different
morphing generation techniques that are captured in three different mediums.

</details>


### [56] [Interpretable Oracle Bone Script Decipherment through Radical and Pictographic Analysis with LVLMs](https://arxiv.org/abs/2508.10113)
*Kaixin Peng,Mengyang Zhao,Haiyang Yu,Teng Fu,Bin Li*

Main category: cs.CV

TL;DR: Proposes an interpretable Oracle Bone Script (OBS) decipherment method using Large Vision-Language Models, achieving state-of-the-art accuracy and zero-shot decipherment capabilities., and releases a new dataset.


<details>
  <summary>Details</summary>
Motivation: Current deep learning-based methods often ignore the intricate connections between glyphs and the semantics of OBS, resulting in limited generalization and interpretability, especially when addressing zero-shot settings and undeciphered OBS.

Method: An interpretable OBS decipherment method based on Large Vision-Language Models, which synergistically combines radical analysis and pictograph-semantic understanding. A progressive training strategy guides the model from radical recognition and analysis to pictographic analysis and mutual analysis. A Radical-Pictographic Dual Matching mechanism enhances zero-shot decipherment performance.

Result: The proposed approach achieves state-of-the-art Top-10 accuracy and superior zero-shot decipherment capabilities on public benchmarks.

Conclusion: The proposed method achieves state-of-the-art Top-10 accuracy and superior zero-shot decipherment capabilities on public benchmarks. The model delivers logical analysis processes, providing archaeologically valuable reference results for undeciphered OBS, with potential applications in digital humanities and historical research.

Abstract: As the oldest mature writing system, Oracle Bone Script (OBS) has long posed
significant challenges for archaeological decipherment due to its rarity,
abstractness, and pictographic diversity. Current deep learning-based methods
have made exciting progress on the OBS decipherment task, but existing
approaches often ignore the intricate connections between glyphs and the
semantics of OBS. This results in limited generalization and interpretability,
especially when addressing zero-shot settings and undeciphered OBS. To this
end, we propose an interpretable OBS decipherment method based on Large
Vision-Language Models, which synergistically combines radical analysis and
pictograph-semantic understanding to bridge the gap between glyphs and meanings
of OBS. Specifically, we propose a progressive training strategy that guides
the model from radical recognition and analysis to pictographic analysis and
mutual analysis, thus enabling reasoning from glyph to meaning. We also design
a Radical-Pictographic Dual Matching mechanism informed by the analysis
results, significantly enhancing the model's zero-shot decipherment
performance. To facilitate model training, we propose the Pictographic
Decipherment OBS Dataset, which comprises 47,157 Chinese characters annotated
with OBS images and pictographic analysis texts. Experimental results on public
benchmarks demonstrate that our approach achieves state-of-the-art Top-10
accuracy and superior zero-shot decipherment capabilities. More importantly,
our model delivers logical analysis processes, possibly providing
archaeologically valuable reference results for undeciphered OBS, and thus has
potential applications in digital humanities and historical research. The
dataset and code will be released in https://github.com/PKXX1943/PD-OBS.

</details>


### [57] [Deep Learning Enables Large-Scale Shape and Appearance Modeling in Total-Body DXA Imaging](https://arxiv.org/abs/2508.10132)
*Arianna Bunnell,Devon Cataldi,Yannik Glaser,Thomas K. Wolfgruber,Steven Heymsfield,Alan B. Zonderman,Thomas L. Kelly,Peter Sadowski,John A. Shepherd*

Main category: cs.CV

TL;DR: 开发了一种深度学习方法，用于 TBDXA 扫描图像上的自动基准点定位，并在大型数据集上进行了验证，证明了其在身体成分分析方面的价值。


<details>
  <summary>Details</summary>
Motivation: 全身双能量 X 射线吸收成像 (TBDXA) 是一种相对低成本的全身成像方式，广泛用于身体成分评估。

Method: 开发并验证了一种深度学习方法，用于在 TBDXA 扫描图像上自动进行基准点定位。

Result: 该方法在外部测试数据集中实现了 99.5% 的关键点正确率。

Conclusion: 该方法能够可靠地在 TBDXA 扫描图像上自动定位关键点，并且可以用于研究身体成分与健康指标之间的关系。

Abstract: Total-body dual X-ray absorptiometry (TBDXA) imaging is a relatively low-cost
whole-body imaging modality, widely used for body composition assessment. We
develop and validate a deep learning method for automatic fiducial point
placement on TBDXA scans using 1,683 manually-annotated TBDXA scans. The method
achieves 99.5% percentage correct keypoints in an external testing dataset. To
demonstrate the value for shape and appearance modeling (SAM), our method is
used to place keypoints on 35,928 scans for five different TBDXA imaging modes,
then associations with health markers are tested in two cohorts not used for
SAM model generation using two-sample Kolmogorov-Smirnov tests. SAM feature
distributions associated with health biomarkers are shown to corroborate
existing evidence and generate new hypotheses on body composition and shape's
relationship to various frailty, metabolic, inflammation, and cardiometabolic
health markers. Evaluation scripts, model weights, automatic point file
generation code, and triangulation files are available at
https://github.com/hawaii-ai/dxa-pointplacement.

</details>


### [58] [MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning](https://arxiv.org/abs/2508.10133)
*Thanh-Dat Truong,Christophe Bobda,Nitin Agarwal,Khoa Luu*

Main category: cs.CV

TL;DR: This paper proposes MANGO, a novel multimodal fusion method using attention-based normalizing flows, achieving state-of-the-art results on various multimodal tasks.


<details>
  <summary>Details</summary>
Motivation: Current multimodal fusion methods struggle to capture essential features of each modality and comprehend complex structures and correlations of multimodal inputs.

Method: The paper introduces a Multimodal Attention-based Normalizing Flow (MANGO) approach with a new Invertible Cross-Attention (ICA) layer and three cross-attention mechanisms: Modality-to-Modality Cross-Attention (MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality Cross-Attention (LICA).

Result: The experimental results demonstrate state-of-the-art (SoTA) performance of the proposed approach on three different multimodal learning tasks.

Conclusion: The proposed MANGO approach achieves state-of-the-art performance on semantic segmentation, image-to-image translation, and movie genre classification tasks.

Abstract: Multimodal learning has gained much success in recent years. However, current
multimodal fusion methods adopt the attention mechanism of Transformers to
implicitly learn the underlying correlation of multimodal features. As a
result, the multimodal model cannot capture the essential features of each
modality, making it difficult to comprehend complex structures and correlations
of multimodal inputs. This paper introduces a novel Multimodal Attention-based
Normalizing Flow (MANGO) approach\footnote{The source code of this work will be
publicly available.} to developing explicit, interpretable, and tractable
multimodal fusion learning. In particular, we propose a new Invertible
Cross-Attention (ICA) layer to develop the Normalizing Flow-based Model for
multimodal data. To efficiently capture the complex, underlying correlations in
multimodal data in our proposed invertible cross-attention layer, we propose
three new cross-attention mechanisms: Modality-to-Modality Cross-Attention
(MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality
Cross-Attention (LICA). Finally, we introduce a new Multimodal Attention-based
Normalizing Flow to enable the scalability of our proposed method to
high-dimensional multimodal data. Our experimental results on three different
multimodal learning tasks, i.e., semantic segmentation, image-to-image
translation, and movie genre classification, have illustrated the
state-of-the-art (SoTA) performance of the proposed approach.

</details>


### [59] [Improving watermelon (Citrullus lanatus) disease classification with generative artificial intelligence (GenAI)-based synthetic and real-field images via a custom EfficientNetV2-L model](https://arxiv.org/abs/2508.10156)
*Nitin Rai,Nathan S. Boyd,Gary E. Vallad,Arnold W. Schumann*

Main category: cs.CV

TL;DR: combining real and synthetic images improves model performance for crop disease classification


<details>
  <summary>Details</summary>
Motivation: GenAI models are being used to create synthetic images of various diseases, potentially facilitating model creation and reducing the dependency on resource-intensive in-field data collection. However, limited research has been conducted on evaluating the effectiveness of integrating real with synthetic images to improve disease classification performance

Method: training dataset was divided into five treatments: H0 (only real images), H1 (only synthetic images), H2 (1:1 real-to-synthetic), H3 (1:10 real-to-synthetic), and H4 (H3 + random images to improve variability and model generalization). All treatments were trained using a custom EfficientNetV2-L architecture with enhanced fine-tuning and transfer learning techniques

Result: the weighted F1-score increased from 0.65 (on H0) to 1.00 (on H3-H4) signifying that the addition of a small number of real images with a considerable volume of synthetic images improved model performance and generalizability

Conclusion: combining a limited number of real images with synthetic images can enhance the prediction accuracy of an EfficientNetV2-L model for classifying watermelon diseases

Abstract: The current advancements in generative artificial intelligence (GenAI) models
have paved the way for new possibilities for generating high-resolution
synthetic images, thereby offering a promising alternative to traditional image
acquisition for training computer vision models in agriculture. In the context
of crop disease diagnosis, GenAI models are being used to create synthetic
images of various diseases, potentially facilitating model creation and
reducing the dependency on resource-intensive in-field data collection.
However, limited research has been conducted on evaluating the effectiveness of
integrating real with synthetic images to improve disease classification
performance. Therefore, this study aims to investigate whether combining a
limited number of real images with synthetic images can enhance the prediction
accuracy of an EfficientNetV2-L model for classifying watermelon
\textit{(Citrullus lanatus)} diseases. The training dataset was divided into
five treatments: H0 (only real images), H1 (only synthetic images), H2 (1:1
real-to-synthetic), H3 (1:10 real-to-synthetic), and H4 (H3 + random images to
improve variability and model generalization). All treatments were trained
using a custom EfficientNetV2-L architecture with enhanced fine-tuning and
transfer learning techniques. Models trained on H2, H3, and H4 treatments
demonstrated high precision, recall, and F1-score metrics. Additionally, the
weighted F1-score increased from 0.65 (on H0) to 1.00 (on H3-H4) signifying
that the addition of a small number of real images with a considerable volume
of synthetic images improved model performance and generalizability. Overall,
this validates the findings that synthetic images alone cannot adequately
substitute for real images; instead, both must be used in a hybrid manner to
maximize model performance for crop disease classification.

</details>


### [60] [SynSpill: Improved Industrial Spill Detection With Synthetic Data](https://arxiv.org/abs/2508.10171)
*Aaditya Baranwal,Abdul Mueez,Jason Voelker,Guneet Bhatia,Shruti Vyas*

Main category: cs.CV

TL;DR: 提出了一种基于合成数据的框架，用于改进视觉-语言模型在工业泄漏检测等安全关键领域的性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型(vlms)通过强大的零样本能力改变了通用视觉识别。然而，在小众的、安全关键的领域(如工业泄漏检测)中，它们的性能显著下降，在这些领域中，危险事件是罕见的、敏感的，并且难以注释。这种稀缺性——由隐私问题、数据敏感性和真实事件的罕见性驱动——使得对大多数工业环境下的探测器进行常规微调变得不可行。

Method: 以高质量的合成数据生成流程为中心，提出了一个可扩展的框架。该框架支持对VLM进行参数高效微调(PEFT)，并显著提高YOLO和DETR等先进目标检测器的性能。

Result: 在没有合成数据(SynSpill数据集)的情况下，vlm仍然比这些检测器更好地推广到看不见的泄漏场景。当使用SynSpill时，vlm和检测器都取得了显著的改进，它们的性能变得相当。

Conclusion: 高保真合成数据是弥合安全关键型应用中领域差距的有力手段。合成生成和轻量级适配的结合，为在难以获得真实数据的工业环境中部署视觉系统提供了一条经济高效、可扩展的途径。

Abstract: Large-scale Vision-Language Models (VLMs) have transformed general-purpose
visual recognition through strong zero-shot capabilities. However, their
performance degrades significantly in niche, safety-critical domains such as
industrial spill detection, where hazardous events are rare, sensitive, and
difficult to annotate. This scarcity -- driven by privacy concerns, data
sensitivity, and the infrequency of real incidents -- renders conventional
fine-tuning of detectors infeasible for most industrial settings.
  We address this challenge by introducing a scalable framework centered on a
high-quality synthetic data generation pipeline. We demonstrate that this
synthetic corpus enables effective Parameter-Efficient Fine-Tuning (PEFT) of
VLMs and substantially boosts the performance of state-of-the-art object
detectors such as YOLO and DETR. Notably, in the absence of synthetic data
(SynSpill dataset), VLMs still generalize better to unseen spill scenarios than
these detectors. When SynSpill is used, both VLMs and detectors achieve marked
improvements, with their performance becoming comparable.
  Our results underscore that high-fidelity synthetic data is a powerful means
to bridge the domain gap in safety-critical applications. The combination of
synthetic generation and lightweight adaptation offers a cost-effective,
scalable pathway for deploying vision systems in industrial environments where
real data is scarce/impractical to obtain.
  Project Page: https://synspill.vercel.app

</details>


### [61] [EntropyGS: An Efficient Entropy Coding on 3D Gaussian Splatting](https://arxiv.org/abs/2508.10227)
*Yuning Huang,Jiahao Pang,Fengqing Zhu,Dong Tian*

Main category: cs.CV

TL;DR: 提出了一种名为EntropyGS的分解和参数化熵编码方法，它可以在保持相似渲染质量的同时，将3DGS数据的速率降低约30倍。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射（3DGS）作为一种新兴的新视角合成方法，展示了快速的训练/渲染和卓越的视觉质量。3DGS的高斯创建和视图渲染这两个任务通常在时间和设备上是分离的，因此3DGS高斯的存储/传输和最终压缩变得必要。

Method: 提出了一种分解和参数化的熵编码方法，EntropyGS。

Result: 球谐AC属性精确地服从拉普拉斯分布，而高斯分布的混合可以近似旋转、缩放和不透明度。此外，谐波AC属性与其他属性表现出微弱的相关性，除了从色彩空间继承的相关性。

Conclusion: EntropyGS实现了大约30倍的速率降低，同时保持与输入3DGS数据相似的渲染质量，并具有快速的编码和解码时间。

Abstract: As an emerging novel view synthesis approach, 3D Gaussian Splatting (3DGS)
demonstrates fast training/rendering with superior visual quality. The two
tasks of 3DGS, Gaussian creation and view rendering, are typically separated
over time or devices, and thus storage/transmission and finally compression of
3DGS Gaussians become necessary. We begin with a correlation and statistical
analysis of 3DGS Gaussian attributes. An inspiring finding in this work reveals
that spherical harmonic AC attributes precisely follow Laplace distributions,
while mixtures of Gaussian distributions can approximate rotation, scaling, and
opacity. Additionally, harmonic AC attributes manifest weak correlations with
other attributes except for inherited correlations from a color space. A
factorized and parameterized entropy coding method, EntropyGS, is hereinafter
proposed. During encoding, distribution parameters of each Gaussian attribute
are estimated to assist their entropy coding. The quantization for entropy
coding is adaptively performed according to Gaussian attribute types. EntropyGS
demonstrates about 30x rate reduction on benchmark datasets while maintaining
similar rendering quality compared to input 3DGS data, with a fast encoding and
decoding time.

</details>


### [62] [CellSymphony: Deciphering the molecular and phenotypic orchestration of cells with single-cell pathomics](https://arxiv.org/abs/2508.10232)
*Paul H. Acosta,Pingjun Chen,Simon P. Castillo,Maria Esther Salvatierra,Yinyin Yuan,Xiaoxi Pan*

Main category: cs.CV

TL;DR: CellSymphony 是一种多模态框架，它融合了空间基因表达与形态学背景，实现了准确的细胞类型注释，并揭示了不同的微环境生态位。


<details>
  <summary>Details</summary>
Motivation: 从组织学图像中提取稳健的细胞水平特征并将它们与空间转录组学数据整合仍然是一个关键挑战。

Method: CellSymphony，一个灵活的多模态框架，它利用来自 Xenium 转录组谱和组织学图像的基础模型衍生的嵌入，实现真正的单细胞分辨率。

Result: CellSymphony 实现了准确的细胞类型注释，并揭示了三种癌症类型的不同微环境生态位。

Conclusion: 利用基础模型和多模态融合来破译复杂组织生态系统中细胞的生理和表型编排。

Abstract: Xenium, a new spatial transcriptomics platform, enables
subcellular-resolution profiling of complex tumor tissues. Despite the rich
morphological information in histology images, extracting robust cell-level
features and integrating them with spatial transcriptomics data remains a
critical challenge. We introduce CellSymphony, a flexible multimodal framework
that leverages foundation model-derived embeddings from both Xenium
transcriptomic profiles and histology images at true single-cell resolution. By
learning joint representations that fuse spatial gene expression with
morphological context, CellSymphony achieves accurate cell type annotation and
uncovers distinct microenvironmental niches across three cancer types. This
work highlights the potential of foundation models and multimodal fusion for
deciphering the physiological and phenotypic orchestration of cells within
complex tissue ecosystems.

</details>


### [63] [Multi-Label Plant Species Prediction with Metadata-Enhanced Multi-Head Vision Transformers](https://arxiv.org/abs/2508.10457)
*Hanna Herasimchyk,Robin Labryga,Tomislav Prusina*

Main category: cs.CV

TL;DR: This paper presents a multi-head vision transformer approach for multi-label plant species prediction, achieving 3rd place in the PlantCLEF 2025 challenge. It uses a pre-trained DINOv2 ViT-B/14 backbone with multi-scale tiling, dynamic threshold optimization, and ensemble strategies.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the PlantCLEF 2025 challenge, which involves a domain shift between training on single-species images and testing on multi-species quadrat images.

Method: The methodology leverages a pre-trained DINOv2 Vision Transformer Base (ViT-B/14) backbone with multiple classification heads for species, genus, and family prediction, utilizing taxonomic hierarchies. Key contributions include multi-scale tiling, dynamic threshold optimization, and ensemble strategies through bagging and Hydra model architectures. The approach incorporates various inference techniques including image cropping, top-n filtering, and logit thresholding strategies.

Result: Experiments were conducted on approximately 1.4 million training images covering 7,806 plant species. The submission achieved 3rd place on the private leaderboard.

Conclusion: The submission achieved 3rd place on the private leaderboard, demonstrating strong performance in multi-label plant species prediction.

Abstract: We present a multi-head vision transformer approach for multi-label plant
species prediction in vegetation plot images, addressing the PlantCLEF 2025
challenge. The task involves training models on single-species plant images
while testing on multi-species quadrat images, creating a drastic domain shift.
Our methodology leverages a pre-trained DINOv2 Vision Transformer Base
(ViT-B/14) backbone with multiple classification heads for species, genus, and
family prediction, utilizing taxonomic hierarchies. Key contributions include
multi-scale tiling to capture plants at different scales, dynamic threshold
optimization based on mean prediction length, and ensemble strategies through
bagging and Hydra model architectures. The approach incorporates various
inference techniques including image cropping to remove non-plant artifacts,
top-n filtering for prediction constraints, and logit thresholding strategies.
Experiments were conducted on approximately 1.4 million training images
covering 7,806 plant species. Results demonstrate strong performance, making
our submission 3rd best on the private leaderboard. Our code is available at
https://github.com/geranium12/plant-clef-2025/tree/v1.0.0.

</details>


### [64] [Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets](https://arxiv.org/abs/2508.10256)
*Xinan Zhang,Haolin Wang,Yung-An Hsieh,Zhongyu Yang,Anthony Yezzi,Yi-Chang Tsai*

Main category: cs.CV

TL;DR: 本综述分析了深度学习裂缝检测领域的新兴趋势，引入了一个新的数据集，并进行了基准测试实验。


<details>
  <summary>Details</summary>
Motivation: 裂缝检测在民用基础设施中起着关键作用，深度学习近年来已显著推动了该领域的发展。新兴趋势正在重塑这一领域。

Method: 系统地分析了学习范式、泛化性和数据集重新获取方面的趋势，并引入了一个新的数据集。

Result: 提供了对基于深度学习的裂缝检测中不断发展的方法和未来方向的见解。进行了广泛的基准测试实验，以建立常用深度学习方法（包括最新的基础模型）的基线。

Conclusion: 分析了基于深度学习的裂缝检测领域的新兴趋势，并为未来的研究建立基线。

Abstract: Crack detection plays a crucial role in civil infrastructures, including
inspection of pavements, buildings, etc., and deep learning has significantly
advanced this field in recent years. While numerous technical and review papers
exist in this domain, emerging trends are reshaping the landscape. These shifts
include transitions in learning paradigms (from fully supervised learning to
semi-supervised, weakly-supervised, unsupervised, few-shot, domain adaptation
and fine-tuning foundation models), improvements in generalizability (from
single-dataset performance to cross-dataset evaluation), and diversification in
dataset reacquisition (from RGB images to specialized sensor-based data). In
this review, we systematically analyze these trends and highlight
representative works. Additionally, we introduce a new dataset collected with
3D laser scans, 3DCrack, to support future research and conduct extensive
benchmarking experiments to establish baselines for commonly used deep learning
methodologies, including recent foundation models. Our findings provide
insights into the evolving methodologies and future directions in deep
learning-based crack detection. Project page:
https://github.com/nantonzhang/Awesome-Crack-Detection

</details>


### [65] [MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs](https://arxiv.org/abs/2508.10264)
*Haonan Ge,Yiwei Wang,Ming-Hsuan Yang,Yujun Cai*

Main category: cs.CV

TL;DR: MRFD是一种新的解码方法，可以减少大型视觉语言模型中的幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型(LVLMs)在多模态任务中表现出强大的性能。然而，由于验证图像不同区域信息的能力有限，它们经常产生幻觉——与视觉输入不一致的文本。

Method: Multi-Region Fusion Decoding (MRFD)，一种无需训练的解码方法，通过建模区域间一致性来提高事实基础。

Result: 跨多个LVLM和基准的实验表明，MRFD显著减少了幻觉并提高了响应的事实性。

Conclusion: MRFD显著减少了幻觉并提高了响应的事实性，而无需模型更新。

Abstract: Large Vision-Language Models (LVLMs) have shown strong performance across
multimodal tasks. However, they often produce hallucinations -- text that is
inconsistent with visual input, due to the limited ability to verify
information in different regions of the image. To address this, we propose
Multi-Region Fusion Decoding (MRFD), a training-free decoding method that
improves factual grounding by modeling inter-region consistency. MRFD
identifies salient regions using cross-attention, generates initial responses
for each, and computes reliability weights based on Jensen-Shannon Divergence
(JSD) among the responses. These weights guide a consistency-aware fusion of
per-region predictions, using region-aware prompts inspired by Chain-of-Thought
reasoning. Experiments across multiple LVLMs and benchmarks show that MRFD
significantly reduces hallucinations and improves response factuality without
requiring model updates.

</details>


### [66] [Pose-Robust Calibration Strategy for Point-of-Gaze Estimation on Mobile Phones](https://arxiv.org/abs/2508.10268)
*Yujie Zhao,Jiabei Zeng,Shiguang Shan*

Main category: cs.CV

TL;DR: 提出动态校准策略，提升注视点估计器对头部姿势变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于外观的注视点(PoG)估计器难以在个体间泛化，并且校准后的PoG估计器对头部姿势变化敏感。

Method: 构建了一个包含32人面部图像的基准数据集MobilePoG，并系统地分析了校准点和头部姿势的多样性对估计精度的影响。在此基础上，提出了一种动态校准策略。

Result: 实验表明，在校准过程中引入更广泛的头部姿势范围可以提高估计器处理姿势变化的能力。动态校准策略能够产生更好的校准PoG估计器，且对头部姿势变化的敏感度低于传统校准策略。

Conclusion: 提出了一种动态校准策略，通过在用户校准时引入头部姿势变化，提高了注视点估计器对头部姿势变化的鲁棒性。

Abstract: Although appearance-based point-of-gaze (PoG) estimation has improved, the
estimators still struggle to generalize across individuals due to personal
differences. Therefore, person-specific calibration is required for accurate
PoG estimation. However, calibrated PoG estimators are often sensitive to head
pose variations. To address this, we investigate the key factors influencing
calibrated estimators and explore pose-robust calibration strategies.
Specifically, we first construct a benchmark, MobilePoG, which includes facial
images from 32 individuals focusing on designated points under either fixed or
continuously changing head poses. Using this benchmark, we systematically
analyze how the diversity of calibration points and head poses influences
estimation accuracy. Our experiments show that introducing a wider range of
head poses during calibration improves the estimator's ability to handle pose
variation. Building on this insight, we propose a dynamic calibration strategy
in which users fixate on calibration points while moving their phones. This
strategy naturally introduces head pose variation during a user-friendly and
efficient calibration process, ultimately producing a better calibrated PoG
estimator that is less sensitive to head pose variations than those using
conventional calibration strategies. Codes and datasets are available at our
project page.

</details>


### [67] [High Fidelity Text to Image Generation with Contrastive Alignment and Structural Guidance](https://arxiv.org/abs/2508.10280)
*Danyi Gao*

Main category: cs.CV

TL;DR: 提出了一种新的文本驱动图像生成方法，通过结合对比学习和结构引导来提高图像质量，并在 COCO-2014 数据集上取得了优异的结果。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动的图像生成方法在语义对齐准确性和结构一致性方面存在性能瓶颈。

Method: 通过将文本-图像对比约束与结构引导机制相结合，提出了一种高保真图像生成方法。

Result: 在 COCO-2014 数据集上进行了系统实验。定量指标证实了该方法在 CLIP Score、FID 和 SSIM 方面的优越性能。

Conclusion: 该方法有效弥合了语义对齐和结构保真度之间的差距，而没有增加计算复杂性。它展示了生成语义清晰和结构完整的图像的强大能力，为联合文本-图像建模和图像生成提供了可行的技术路径。

Abstract: This paper addresses the performance bottlenecks of existing text-driven
image generation methods in terms of semantic alignment accuracy and structural
consistency. A high-fidelity image generation method is proposed by integrating
text-image contrastive constraints with structural guidance mechanisms. The
approach introduces a contrastive learning module that builds strong
cross-modal alignment constraints to improve semantic matching between text and
image. At the same time, structural priors such as semantic layout maps or edge
sketches are used to guide the generator in spatial-level structural modeling.
This enhances the layout completeness and detail fidelity of the generated
images. Within the overall framework, the model jointly optimizes contrastive
loss, structural consistency loss, and semantic preservation loss. A
multi-objective supervision mechanism is adopted to improve the semantic
consistency and controllability of the generated content. Systematic
experiments are conducted on the COCO-2014 dataset. Sensitivity analyses are
performed on embedding dimensions, text length, and structural guidance
strength. Quantitative metrics confirm the superior performance of the proposed
method in terms of CLIP Score, FID, and SSIM. The results show that the method
effectively bridges the gap between semantic alignment and structural fidelity
without increasing computational complexity. It demonstrates a strong ability
to generate semantically clear and structurally complete images, offering a
viable technical path for joint text-image modeling and image generation.

</details>


### [68] [VIFSS: View-Invariant and Figure Skating-Specific Pose Representation Learning for Temporal Action Segmentation](https://arxiv.org/abs/2508.10281)
*Ryota Tanaka,Tomohiro Suzuki,Keisuke Fujii*

Main category: cs.CV

TL;DR: A new Temporal Action Segmentation (TAS) framework for figure skating jumps is proposed, incorporating 3D nature and semantic procedure of movements, achieving high accuracy even with limited data.


<details>
  <summary>Details</summary>
Motivation: Accurately recognizing the type and timing of jumps is essential for objective performance evaluation in figure skating, but it typically requires expert-level knowledge and existing TAS methods have limitations due to insufficient data and not accounting for the three-dimensional aspects and procedural structure of jump actions.

Method: A new TAS framework incorporating the three-dimensional nature and the semantic procedure of jump movements. It includes a View-Invariant, Figure Skating-Specific pose representation learning approach (VIFSS) and a fine-grained annotation scheme.

Result: The method achieves over 92% F1@50 on element-level TAS, and view-invariant contrastive pre-training is particularly effective when fine-tuning data is limited.

Conclusion: The proposed TAS framework achieves over 92% F1@50 on element-level TAS and view-invariant contrastive pre-training is effective when fine-tuning data is limited.

Abstract: Understanding human actions from videos plays a critical role across various
domains, including sports analytics. In figure skating, accurately recognizing
the type and timing of jumps a skater performs is essential for objective
performance evaluation. However, this task typically requires expert-level
knowledge due to the fine-grained and complex nature of jump procedures. While
recent approaches have attempted to automate this task using Temporal Action
Segmentation (TAS), there are two major limitations to TAS for figure skating:
the annotated data is insufficient, and existing methods do not account for the
inherent three-dimensional aspects and procedural structure of jump actions. In
this work, we propose a new TAS framework for figure skating jumps that
explicitly incorporates both the three-dimensional nature and the semantic
procedure of jump movements. First, we propose a novel View-Invariant, Figure
Skating-Specific pose representation learning approach (VIFSS) that combines
contrastive learning as pre-training and action classification as fine-tuning.
For view-invariant contrastive pre-training, we construct FS-Jump3D, the first
publicly available 3D pose dataset specialized for figure skating jumps.
Second, we introduce a fine-grained annotation scheme that marks the ``entry
(preparation)'' and ``landing'' phases, enabling TAS models to learn the
procedural structure of jumps. Extensive experiments demonstrate the
effectiveness of our framework. Our method achieves over 92% F1@50 on
element-level TAS, which requires recognizing both jump types and rotation
levels. Furthermore, we show that view-invariant contrastive pre-training is
particularly effective when fine-tuning data is limited, highlighting the
practicality of our approach in real-world scenarios.

</details>


### [69] [JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics](https://arxiv.org/abs/2508.10287)
*Simindokht Jahangard,Mehrzad Mohammadi,Yi Shen,Zhixi Cai,Hamid Rezatofighi*

Main category: cs.CV

TL;DR: This paper introduces JRDB-Reasoning, a new visual reasoning benchmark for human-crowded environments with customizable questions and detailed annotations to address limitations in existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing visual reasoning benchmarks lack a clear definition of reasoning complexity, offer no control to generate questions over varying difficulty and task customization, and fail to provide structured, step-by-step reasoning annotations (workflows).

Method: The paper formalizes reasoning complexity and introduces an adaptive query engine that generates customizable questions of varying complexity with detailed intermediate annotations.

Result: The paper introduces a new benchmark and an engine that enables fine-grained evaluation of visual reasoning frameworks and dynamic assessment of visual-language models across reasoning levels.

Conclusion: The paper introduces JRDB-Reasoning, a new benchmark tailored for visual reasoning in human-crowded environments, which extends the JRDB dataset with human-object interaction and geometric relationship annotations. The benchmark is designed for fine-grained evaluation of visual reasoning frameworks and dynamic assessment of visual-language models across reasoning levels.

Abstract: Recent advances in Vision-Language Models (VLMs) and large language models
(LLMs) have greatly enhanced visual reasoning, a key capability for embodied AI
agents like robots. However, existing visual reasoning benchmarks often suffer
from several limitations: they lack a clear definition of reasoning complexity,
offer have no control to generate questions over varying difficulty and task
customization, and fail to provide structured, step-by-step reasoning
annotations (workflows). To bridge these gaps, we formalize reasoning
complexity, introduce an adaptive query engine that generates customizable
questions of varying complexity with detailed intermediate annotations, and
extend the JRDB dataset with human-object interaction and geometric
relationship annotations to create JRDB-Reasoning, a benchmark tailored for
visual reasoning in human-crowded environments. Our engine and benchmark enable
fine-grained evaluation of visual reasoning frameworks and dynamic assessment
of visual-language models across reasoning levels.

</details>


### [70] [A Sub-Pixel Multimodal Optical Remote Sensing Images Matching Method](https://arxiv.org/abs/2508.10294)
*Tao Huang,Hongbo Pan,Nanxi Zhou,Shun Zhou*

Main category: cs.CV

TL;DR: 提出PCWLAD方法，以提高多模态光学图像的匹配精度，实验结果表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 多模态光学图像的非线性辐射和几何变形差异会降低图像匹配精度。

Method: 提出了一种相位一致性加权最小绝对偏差（PCWLAD）亚像素模板匹配方法。

Result: PCWLAD在正确匹配率（CMR）和均方根误差（RMSE）方面优于现有的八种最先进的方法。

Conclusion: PCWLAD在三个数据集上表现优于现有方法，平均匹配精度约为0.4像素。

Abstract: High-accuracy matching of multimodal optical images is the basis of geometric
processing. However, the image matching accuracy is usually degraded by the
nonlinear radiation and geometric deformation differences caused by different
spectral responses. To address these problems, we proposed a phase consistency
weighted least absolute deviation (PCWLAD) sub-pixel template matching method
to improve the matching accuracy of multimodal optical images. This method
consists of two main steps: coarse matching with the structural similarity
index measure (SSIM) and fine matching with WLAD. In the coarse matching step,
PCs are calculated without a noise filter to preserve the original structural
details, and template matching is performed using the SSIM. In the fine
matching step, we applied the radiometric and geometric transformation models
between two multimodal PC templates based on the coarse matching. Furthermore,
mutual structure filtering is adopted in the model to mitigate the impact of
noise within the corresponding templates on the structural consistency, and the
WLAD criterion is used to estimate the sub-pixel offset. To evaluate the
performance of PCWLAD, we created three types of image datasets: visible to
infrared Landsat images, visible to near-infrared close-range images, and
visible to infrared uncrewed aerial vehicle (UAV) images. PCWLAD outperformed
existing state-of-the-art eight methods in terms of correct matching rate (CMR)
and root mean square error (RMSE) and reached an average matching accuracy of
approximately 0.4 pixels across all three datasets. Our software and datasets
are publicly available at https://github.com/huangtaocsu/PCWLAD.

</details>


### [71] [InterSyn: Interleaved Learning for Dynamic Motion Synthesis in the Wild](https://arxiv.org/abs/2508.10297)
*Yiyi Ma,Yuanzhi Liang,Xiu Li,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: InterSyn is a novel framework that targets the generation of realistic interaction motions by learning from integrated motions that consider both solo and multi-person dynamics


<details>
  <summary>Details</summary>
Motivation: Targets the generation of realistic interaction motions by learning from integrated motions that consider both solo and multi-person dynamics. Unlike previous methods that treat these components separately, InterSyn employs an interleaved learning strategy to capture the natural, dynamic interactions and nuanced coordination inherent in real-world scenarios.

Method: Interleaved Learning for Motion Synthesis (InterSyn), comprising two key modules: the Interleaved Interaction Synthesis (INS) module and the Relative Coordination Refinement (REC) module.

Result: Motion sequences generated by InterSyn exhibit higher text-to-motion alignment and improved diversity compared with recent methods.

Conclusion: The motion sequences generated by InterSyn exhibit higher text-to-motion alignment and improved diversity compared with recent methods, setting a new benchmark for robust and natural motion synthesis.

Abstract: We present Interleaved Learning for Motion Synthesis (InterSyn), a novel
framework that targets the generation of realistic interaction motions by
learning from integrated motions that consider both solo and multi-person
dynamics. Unlike previous methods that treat these components separately,
InterSyn employs an interleaved learning strategy to capture the natural,
dynamic interactions and nuanced coordination inherent in real-world scenarios.
Our framework comprises two key modules: the Interleaved Interaction Synthesis
(INS) module, which jointly models solo and interactive behaviors in a unified
paradigm from a first-person perspective to support multiple character
interactions, and the Relative Coordination Refinement (REC) module, which
refines mutual dynamics and ensures synchronized motions among characters.
Experimental results show that the motion sequences generated by InterSyn
exhibit higher text-to-motion alignment and improved diversity compared with
recent methods, setting a new benchmark for robust and natural motion
synthesis. Additionally, our code will be open-sourced in the future to promote
further research and development in this area.

</details>


### [72] [From Pixel to Mask: A Survey of Out-of-Distribution Segmentation](https://arxiv.org/abs/2508.10309)
*Wenjie Zhao,Jia Li,Yunhui Guo*

Main category: cs.CV

TL;DR: 本文概述了分布外 (OoD) 分割技术，特别是在自动驾驶中的应用。


<details>
  <summary>Details</summary>
Motivation: 随着人们对人工智能安全问题的日益关注，分布外 (OoD) 检测和分割越来越受到关注。传统的 OoD 检测方法可以识别 OoD 对象的存在，但缺乏空间定位，限制了它们在下游任务中的用处。OoD 分割通过在像素级粒度上定位异常对象来解决这一限制。这种能力对于自动驾驶等安全关键型应用至关重要，在这些应用中，感知模块不仅必须检测而且必须精确分割 OoD 对象，从而实现有针对性的控制动作并增强整体系统鲁棒性。

Method: 本文将当前的 OoD 分割方法分为四类：(i) 测试时 OoD 分割，(ii) 监督训练的异常值暴露，(iii) 基于重建的方法，(iv) 以及利用强大模型的方法。

Result: 本文系统地回顾了自动驾驶场景中最新的 OoD 分割技术进展。

Conclusion: 本文综述了自动驾驶场景中最新的 OoD 分割技术进展，指出了新兴的挑战，并讨论了有希望的未来研究方向。

Abstract: Out-of-distribution (OoD) detection and segmentation have attracted growing
attention as concerns about AI security rise. Conventional OoD detection
methods identify the existence of OoD objects but lack spatial localization,
limiting their usefulness in downstream tasks. OoD segmentation addresses this
limitation by localizing anomalous objects at pixel-level granularity. This
capability is crucial for safety-critical applications such as autonomous
driving, where perception modules must not only detect but also precisely
segment OoD objects, enabling targeted control actions and enhancing overall
system robustness. In this survey, we group current OoD segmentation approaches
into four categories: (i) test-time OoD segmentation, (ii) outlier exposure for
supervised training, (iii) reconstruction-based methods, (iv) and approaches
that leverage powerful models. We systematically review recent advances in OoD
segmentation for autonomous-driving scenarios, identify emerging challenges,
and discuss promising future research directions.

</details>


### [73] [Integrating Reinforcement Learning with Visual Generative Models: Foundations and Advances](https://arxiv.org/abs/2508.10316)
*Yuanzhi Liang,Yijie Fang,Rui Li,Ziqi Ni,Ruijie Su,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: This survey reviews the use of Reinforcement Learning (RL) to improve visual content generation by aligning it with high-level goals, addressing the limitations of traditional surrogate objectives. It also discusses future research directions.


<details>
  <summary>Details</summary>
Motivation: Generative models are typically trained with surrogate objectives such as likelihood or reconstruction loss, which often misalign with perceptual quality, semantic accuracy, or physical realism. Reinforcement learning (RL) offers a principled framework for optimizing non-differentiable, preference-driven, and temporally structured objectives.

Method: This survey provides a systematic overview of RL-based methods for visual content generation. It reviews the evolution of RL and examines its integration into image, video, and 3D/4D generation.

Result: RL serves not only as a fine-tuning mechanism but also as a structural component for aligning generation with complex, high-level goals across image, video, and 3D/4D generation.

Conclusion: This paper concludes with open challenges and future research directions at the intersection of RL and generative modeling.

Abstract: Generative models have made significant progress in synthesizing visual
content, including images, videos, and 3D/4D structures. However, they are
typically trained with surrogate objectives such as likelihood or
reconstruction loss, which often misalign with perceptual quality, semantic
accuracy, or physical realism. Reinforcement learning (RL) offers a principled
framework for optimizing non-differentiable, preference-driven, and temporally
structured objectives. Recent advances demonstrate its effectiveness in
enhancing controllability, consistency, and human alignment across generative
tasks. This survey provides a systematic overview of RL-based methods for
visual content generation. We review the evolution of RL from classical control
to its role as a general-purpose optimization tool, and examine its integration
into image, video, and 3D/4D generation. Across these domains, RL serves not
only as a fine-tuning mechanism but also as a structural component for aligning
generation with complex, high-level goals. We conclude with open challenges and
future research directions at the intersection of RL and generative modeling.

</details>


### [74] [Concepts or Skills? Rethinking Instruction Selection for Multi-modal Models](https://arxiv.org/abs/2508.10339)
*Andrew Bai,Justin Cui,Ruochen Wang,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 视觉语言基准测试主要受益于具有相似技能或视觉概念的指令训练。我们设计了一种有针对性的数据选择方法，通过选择具有最匹配概念/技能的指令来优化基准性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言指令调整实现了两个主要目的：学习视觉概念和学习视觉技能。在本文中，我们发现视觉语言基准测试陷入了主要受益于具有相似技能或视觉概念的指令训练的二分法。

Method: 我们设计了一种简单的目标训练数据选择方法，以优化给定基准的性能。我们首先从基准中提取概念/技能，确定基准主要受益于相似概念还是技能，最后选择具有最匹配概念/技能的指令。

Result: 在 10 多个基准上的实验验证了我们的目标数据选择方法的有效性，在所有基准上平均比现有最佳基线高 +0.9%，在以技能为中心的子集上高 +1.5%。

Conclusion: 本文强调了在指令选择中认识到内在权衡的重要性，这需要在概念知识的获取与视觉技能之间取得平衡。

Abstract: Vision-language instruction tuning achieves two main purposes: learning
visual concepts and learning visual skills. In this paper, we found that
vision-language benchmarks fall into the dichotomy of mainly benefiting from
training on instructions with similar skills or visual concepts. Inspired by
the discovery, we designed a simple targeted training data selection method to
optimize the performance of a given benchmark. We first extract the
concepts/skills from the benchmark, determine whether the benchmark
predominantly benefits from similar concepts or skills, and finally select
instructions with the most matching concepts/skills. Experiments on 10+
benchmarks validate the effectiveness of our targeted data selection method,
showing +0.9\% over the best existing baseline averaged over all benchmarks and
+1.5\% on the skill-focused subset. Our findings underscore the importance of
recognizing the inherent trade-off within instruction selection, which requires
balancing the acquisition of conceptual knowledge against visual skill.

</details>


### [75] [Glo-DMU: A Deep Morphometry Framework of Ultrastructural Characterization in Glomerular Electron Microscopic Images](https://arxiv.org/abs/2508.10351)
*Zhentai Zhang,Danyi Weng,Guibin Zhang,Xiang Chen,Kaixing Long,Jian Geng,Yanmeng Lu,Lei Zhang,Zhitao Zhou,Lei Cao*

Main category: cs.CV

TL;DR: Glo-DMU 框架通过深度学习模型自动量化肾小球超微结构特征，辅助肾脏病理诊断，具有全自动化、高精度和高通量的特点。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要集中在单个超微结构识别上，难以满足实际诊断需求。

Method: 提出了一种基于三个深度学习模型的肾小球形态测量框架 Glo-DMU，包括超微结构分割模型、肾小球滤过屏障区域分类模型和电子致密沉积物检测模型。

Result: 在 115 名患者的 9 种肾脏病理类型中进行了评估，结果表明自动量化结果与病理报告中的形态学描述具有良好的一致性。

Conclusion: Glo-DMU 框架通过同时量化肾小球基底膜厚度、足突消失程度和电子致密沉积物的位置，在实际诊断场景中表现出良好的一致性，为肾脏病理学家提供了一个高效的辅助工具。

Abstract: Complex and diverse ultrastructural features can indicate the type,
progression, and prognosis of kidney diseases. Recently, computational
pathology combined with deep learning methods has shown tremendous potential in
advancing automatic morphological analysis of glomerular ultrastructure.
However, current research predominantly focuses on the recognition of
individual ultrastructure, which makes it challenging to meet practical
diagnostic needs. In this study, we propose the glomerular morphometry
framework of ultrastructural characterization (Glo-DMU), which is grounded on
three deep models: the ultrastructure segmentation model, the glomerular
filtration barrier region classification model, and the electron-dense deposits
detection model. Following the conventional protocol of renal biopsy diagnosis,
this framework simultaneously quantifies the three most widely used
ultrastructural features: the thickness of glomerular basement membrane, the
degree of foot process effacement, and the location of electron-dense deposits.
We evaluated the 115 patients with 9 renal pathological types in real-world
diagnostic scenarios, demonstrating good consistency between automatic
quantification results and morphological descriptions in the pathological
reports. Glo-DMU possesses the characteristics of full automation, high
precision, and high throughput, quantifying multiple ultrastructural features
simultaneously, and providing an efficient tool for assisting renal
pathologists.

</details>


### [76] [Improving OCR for Historical Texts of Multiple Languages](https://arxiv.org/abs/2508.10356)
*Hylke Westerdijk,Ben Blankenborg,Khondoker Ittehadul Islam*

Main category: cs.CV

TL;DR: advanced deep learning techniques are used to solve Optical Character Recognition (OCR) and Document Layout Analysis


<details>
  <summary>Details</summary>
Motivation: using advanced deep learning techniques to solve Optical Character Recognition (OCR) and Document Layout Analysis

Method: utilizing a Convolutional Recurrent Neural Network (CRNN) that integrated DeepLabV3+ for semantic segmentation with a Bidirectional LSTM, incorporating confidence-based pseudolabeling to refine our model; applied a CRNN with a ResNet34 encoder, trained using the Connectionist Temporal Classification (CTC) loss function to effectively capture sequential dependencies

Result: enhanced dataset through extensive data augmentation and employed the Kraken and TrOCR models to improve character recognition; refined our model

Conclusion: This report offers valuable insights and suggests potential directions for future research.

Abstract: This paper presents our methodology and findings from three tasks across
Optical Character Recognition (OCR) and Document Layout Analysis using advanced
deep learning techniques. First, for the historical Hebrew fragments of the
Dead Sea Scrolls, we enhanced our dataset through extensive data augmentation
and employed the Kraken and TrOCR models to improve character recognition. In
our analysis of 16th to 18th-century meeting resolutions task, we utilized a
Convolutional Recurrent Neural Network (CRNN) that integrated DeepLabV3+ for
semantic segmentation with a Bidirectional LSTM, incorporating confidence-based
pseudolabeling to refine our model. Finally, for modern English handwriting
recognition task, we applied a CRNN with a ResNet34 encoder, trained using the
Connectionist Temporal Classification (CTC) loss function to effectively
capture sequential dependencies. This report offers valuable insights and
suggests potential directions for future research.

</details>


### [77] [AtomDiffuser: Time-Aware Degradation Modeling for Drift and Beam Damage in STEM Imaging](https://arxiv.org/abs/2508.10359)
*Hao Wang,Hongkui Zheng,Kai He,Abolfazl Razi*

Main category: cs.CV

TL;DR: AtomDiffuser是一个时间感知退化建模框架，它通过预测仿射变换和空间变化的衰减图来解开样本漂移和辐射衰减。


<details>
  <summary>Details</summary>
Motivation: 扫描透射电子显微镜 (STEM) 在现代材料科学中起着关键作用，能够直接成像原子结构及其在外部干扰下的演变。然而，由于两种纠缠的退化效应，解释时间分辨的 STEM 数据仍然具有挑战性：由机械和热不稳定性引起的空间漂移，以及由辐射损伤引起的束诱导信号损失。

Method: 我们提出了 AtomDiffuser，这是一个时间感知退化建模框架，它通过预测任意两个 STEM 帧之间的仿射变换和空间变化的衰减图来解开样本漂移和辐射衰减。

Result: 经过合成退化过程的训练，AtomDiffuser 也能很好地推广到真实的冷冻 STEM 数据。

Conclusion: AtomDiffuser可以进行高分辨率降级推断和漂移对齐，并提供可视化和量化与辐射引起的原子不稳定性相关的降级模式的工具。

Abstract: Scanning transmission electron microscopy (STEM) plays a critical role in
modern materials science, enabling direct imaging of atomic structures and
their evolution under external interferences. However, interpreting
time-resolved STEM data remains challenging due to two entangled degradation
effects: spatial drift caused by mechanical and thermal instabilities, and
beam-induced signal loss resulting from radiation damage. These factors distort
both geometry and intensity in complex, temporally correlated ways, making it
difficult for existing methods to explicitly separate their effects or model
material dynamics at atomic resolution. In this work, we present AtomDiffuser,
a time-aware degradation modeling framework that disentangles sample drift and
radiometric attenuation by predicting an affine transformation and a spatially
varying decay map between any two STEM frames. Unlike traditional denoising or
registration pipelines, our method leverages degradation as a physically
heuristic, temporally conditioned process, enabling interpretable structural
evolutions across time. Trained on synthetic degradation processes,
AtomDiffuser also generalizes well to real-world cryo-STEM data. It further
supports high-resolution degradation inference and drift alignment, offering
tools for visualizing and quantifying degradation patterns that correlate with
radiation-induced atomic instabilities.

</details>


### [78] [Contrast Sensitivity Function of Multimodal Vision-Language Models](https://arxiv.org/abs/2508.10367)
*Pablo Hernández-Cámara,Alexandra Gomez-Villa,Jose Manuel Jaén-Lorites,Jorge Vila-Tomás,Jesus Malo,Valero Laparra*

Main category: cs.CV

TL;DR: 多模态模型在视觉感知上与人类有差距，且提示语影响大。


<details>
  <summary>Details</summary>
Motivation: 评估多模态视觉语言模型与人类感知的对齐情况至关重要，对比敏感度函数是人类视觉的关键特征。

Method: 通过提示聊天机器人判断不同对比度下图案的可见性来估计其对比敏感度函数。

Result: 虽然一些模型近似于人类的对比敏感度函数形状或幅度，但没有一个模型能完全复制两者。

Conclusion: 多模态模型在视觉表征上与人类感知存在差距，提示语措辞对结果有很大影响。

Abstract: Assessing the alignment of multimodal vision-language models~(VLMs) with
human perception is essential to understand how they perceive low-level visual
features. A key characteristic of human vision is the contrast sensitivity
function (CSF), which describes sensitivity to spatial frequency at
low-contrasts. Here, we introduce a novel behavioral psychophysics-inspired
method to estimate the CSF of chat-based VLMs by directly prompting them to
judge pattern visibility at different contrasts for each frequency. This
methodology is closer to the real experiments in psychophysics than the
previously reported. Using band-pass filtered noise images and a diverse set of
prompts, we assess model responses across multiple architectures. We find that
while some models approximate human-like CSF shape or magnitude, none fully
replicate both. Notably, prompt phrasing has a large effect on the responses,
raising concerns about prompt stability. Our results provide a new framework
for probing visual sensitivity in multimodal models and reveal key gaps between
their visual representations and human perception.

</details>


### [79] [Towards Spatially Consistent Image Generation: On Incorporating Intrinsic Scene Properties into Diffusion Models](https://arxiv.org/abs/2508.10382)
*Hyundo Lee,Suhyung Choi,Byoung-Tak Zhang,Inwoo Hwang*

Main category: cs.CV

TL;DR: This paper introduces a method to generate more spatially consistent and realistic images by co-generating images and their corresponding intrinsics, leveraging intrinsic scene properties to capture the underlying scene structure.


<details>
  <summary>Details</summary>
Motivation: Image generation models often produce spatially inconsistent and distorted images due to limited information about the underlying structures and spatial layouts.

Method: The approach co-generates both images and their corresponding intrinsics, enabling the model to implicitly capture the underlying scene structure. It aggregates various intrinsic scene properties into a single latent variable using an autoencoder and denoises the image and intrinsic domains by carefully sharing mutual information.

Result: The method produces more spatially consistent and realistic images.

Conclusion: The method corrects spatial inconsistencies and produces a more natural layout of scenes while maintaining the fidelity and textual alignment of the base model (e.g., Stable Diffusion).

Abstract: Image generation models trained on large datasets can synthesize high-quality
images but often produce spatially inconsistent and distorted images due to
limited information about the underlying structures and spatial layouts. In
this work, we leverage intrinsic scene properties (e.g., depth, segmentation
maps) that provide rich information about the underlying scene, unlike prior
approaches that solely rely on image-text pairs or use intrinsics as
conditional inputs. Our approach aims to co-generate both images and their
corresponding intrinsics, enabling the model to implicitly capture the
underlying scene structure and generate more spatially consistent and realistic
images. Specifically, we first extract rich intrinsic scene properties from a
large image dataset with pre-trained estimators, eliminating the need for
additional scene information or explicit 3D representations. We then aggregate
various intrinsic scene properties into a single latent variable using an
autoencoder. Building upon pre-trained large-scale Latent Diffusion Models
(LDMs), our method simultaneously denoises the image and intrinsic domains by
carefully sharing mutual information so that the image and intrinsic reflect
each other without degrading image quality. Experimental results demonstrate
that our method corrects spatial inconsistencies and produces a more natural
layout of scenes while maintaining the fidelity and textual alignment of the
base model (e.g., Stable Diffusion).

</details>


### [80] [Unlocking Robust Semantic Segmentation Performance via Label-only Elastic Deformations against Implicit Label Noise](https://arxiv.org/abs/2508.10383)
*Yechan Kim,Dongho Yoon,Younkwan Lee,Unse Fatima,Hong Kook Kim,Songjae Lee,Sanga Park,Jeong Ho Park,Seonjong Kang,Moongu Jeon*

Main category: cs.CV

TL;DR: NSegment+ 是一种新的数据增强方法，通过解耦图像和标签转换来解决语义分割中现实的标签噪声问题，并在多个数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据集也表现出细微的（或隐式的）标签缺陷。这些源于固有的挑战，例如模糊的对象边界和注释者变异。虽然没有明确出现，但这种轻微和潜在的噪声仍然会损害模型性能。典型的数据增强方法，将相同的转换应用于图像及其标签，可能会放大这些细微的缺陷，并限制模型的泛化能力。

Method: 引入一种新颖的增强框架 NSegment+，该框架将图像和标签转换解耦，以解决语义分割的这种真实噪声。通过仅对分割标签引入受控弹性变形，同时保留原始图像，我们的方法鼓励模型专注于学习对象结构的鲁棒表示，尽管存在细微的标签不一致。

Result: NSegment+ 始终提高性能，在 Vaihingen、LoveDA、Cityscapes 和 PASCAL VOC 上的平均 mIoU 增益分别高达 +2.29、+2.38、+1.75 和 +3.39。

Conclusion: NSegment+在多个数据集上表现优异，即使没有 bells and whistles，也能提升性能，强调了解决隐式标签噪声的重要性。当与其他训练技巧（包括 CutMix 和标签平滑）结合使用时，这些收益可以进一步放大。

Abstract: While previous studies on image segmentation focus on handling severe (or
explicit) label noise, real-world datasets also exhibit subtle (or implicit)
label imperfections. These arise from inherent challenges, such as ambiguous
object boundaries and annotator variability. Although not explicitly present,
such mild and latent noise can still impair model performance. Typical data
augmentation methods, which apply identical transformations to the image and
its label, risk amplifying these subtle imperfections and limiting the model's
generalization capacity. In this paper, we introduce NSegment+, a novel
augmentation framework that decouples image and label transformations to
address such realistic noise for semantic segmentation. By introducing
controlled elastic deformations only to segmentation labels while preserving
the original images, our method encourages models to focus on learning robust
representations of object structures despite minor label inconsistencies.
Extensive experiments demonstrate that NSegment+ consistently improves
performance, achieving mIoU gains of up to +2.29, +2.38, +1.75, and +3.39 in
average on Vaihingen, LoveDA, Cityscapes, and PASCAL VOC, respectively-even
without bells and whistles, highlighting the importance of addressing implicit
label noise. These gains can be further amplified when combined with other
training tricks, including CutMix and Label Smoothing.

</details>


### [81] [PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection](https://arxiv.org/abs/2508.10397)
*Haibin Sun,Xinghui Song*

Main category: cs.CV

TL;DR: 提出了 PQ-DAF，以解决驾驶员分心检测中数据稀缺和域转移问题，该方法通过姿态驱动的扩散模型生成高质量的合成数据，并通过视觉语言模型进行过滤，实验表明该方法提高了少样本学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的模型在实际场景中部署时，泛化能力通常会下降。这种限制主要是由于实际环境中数据注释的高成本以及训练数据集和目标部署条件之间的显着域转移而导致的少样本学习挑战引起的。

Method: 我们提出了一个姿态驱动的质量控制数据增强框架 (PQ-DAF)，该框架利用视觉语言模型进行样本过滤，以经济高效地扩展训练数据并增强跨域鲁棒性。我们采用渐进条件扩散模型 (PCDM) 来准确捕获关键驾驶员姿态特征并合成各种训练示例。然后，引入了一个基于 CogVLM 视觉语言模型构建的样本质量评估模块，用于根据置信度阈值过滤掉低质量的合成样本，从而确保增强数据集的可靠性。

Result: PQ-DAF 显著提高了在少量样本驾驶员分心检测中的性能，并在数据稀缺条件下实现了模型泛化方面的显著提升。

Conclusion: PQ-DAF 显著提高了在少量样本驾驶员分心检测中的性能，并在数据稀缺条件下实现了模型泛化方面的显著提升。

Abstract: Driver distraction detection is essential for improving traffic safety and
reducing road accidents. However, existing models often suffer from degraded
generalization when deployed in real-world scenarios. This limitation primarily
arises from the few-shot learning challenge caused by the high cost of data
annotation in practical environments, as well as the substantial domain shift
between training datasets and target deployment conditions. To address these
issues, we propose a Pose-driven Quality-controlled Data Augmentation Framework
(PQ-DAF) that leverages a vision-language model for sample filtering to
cost-effectively expand training data and enhance cross-domain robustness.
Specifically, we employ a Progressive Conditional Diffusion Model (PCDMs) to
accurately capture key driver pose features and synthesize diverse training
examples. A sample quality assessment module, built upon the CogVLM
vision-language model, is then introduced to filter out low-quality synthetic
samples based on a confidence threshold, ensuring the reliability of the
augmented dataset. Extensive experiments demonstrate that PQ-DAF substantially
improves performance in few-shot driver distraction detection, achieving
significant gains in model generalization under data-scarce conditions.

</details>


### [82] [Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.10407)
*Eunseo Koh,Seunghoo Hong,Tae-Young Kim,Simon S. Woo,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 提出了一种新方法，通过在文本嵌入空间中引入 delta 向量来抑制文本到图像生成模型中与特定词语强烈纠缠的内容，并在个性化的 T2I 模型中实现了更精确的抑制。


<details>
  <summary>Details</summary>
Motivation: 文本到图像 (T2I) 扩散模型在从文本提示生成多样化的高质量图像方面取得了显著进展。然而，这些模型在抑制与特定词语强烈纠缠的内容方面仍然面临挑战。例如，当生成“查理·卓别林”的图像时，即使明确指示不包括，也始终会出现“胡子”，因为“胡子”的概念与“查理·卓别林”强烈纠缠。

Method: 提出了一种新颖的方法，可以直接抑制扩散模型的文本嵌入空间中这种纠缠的内容。我们的方法引入了一个 delta 向量，该向量修改了文本嵌入，以削弱生成图像中不需要的内容的影响，并且我们进一步证明了可以通过零样本方法轻松获得该 delta 向量。此外，我们提出了一种带有 Delta 向量的选择性抑制 (SSDV) 方法，以将 delta 向量适应到交叉注意力机制中，从而更有效地抑制原本会生成的区域中不需要的内容。此外，我们通过优化 delta 向量，在个性化的 T2I 模型中实现了更精确的抑制，而之前的基线无法实现。

Result: 我们的方法显著优于现有方法，无论是在定量还是定性指标方面。

Conclusion: 该方法显著优于现有方法，无论是在定量还是定性指标方面。

Abstract: Text-to-Image (T2I) diffusion models have made significant progress in
generating diverse high-quality images from textual prompts. However, these
models still face challenges in suppressing content that is strongly entangled
with specific words. For example, when generating an image of ``Charlie
Chaplin", a ``mustache" consistently appears even if explicitly instructed not
to include it, as the concept of ``mustache" is strongly entangled with
``Charlie Chaplin". To address this issue, we propose a novel approach to
directly suppress such entangled content within the text embedding space of
diffusion models. Our method introduces a delta vector that modifies the text
embedding to weaken the influence of undesired content in the generated image,
and we further demonstrate that this delta vector can be easily obtained
through a zero-shot approach. Furthermore, we propose a Selective Suppression
with Delta Vector (SSDV) method to adapt delta vector into the cross-attention
mechanism, enabling more effective suppression of unwanted content in regions
where it would otherwise be generated. Additionally, we enabled more precise
suppression in personalized T2I models by optimizing delta vector, which
previous baselines were unable to achieve. Extensive experimental results
demonstrate that our approach significantly outperforms existing methods, both
in terms of quantitative and qualitative metrics.

</details>


### [83] [SC-Lane: Slope-aware and Consistent Road Height Estimation Framework for 3D Lane Detection](https://arxiv.org/abs/2508.10411)
*Chaesong Park,Eunbin Seo,Jihyeon Hwang,Jongwoo Lim*

Main category: cs.CV

TL;DR: SC-Lane是一种新的坡度感知和时间一致的高度图估计框架，用于3D车道检测，它优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 之前的3D车道检测方法依赖于固定的坡度锚点，对不同的道路几何形状的鲁棒性较差。

Method: 提出了一种坡度感知自适应特征模块，该模块从图像线索中动态预测适当的权重，用于将多坡度表示集成到统一的高度图中。此外，高度一致性模块强制时间连贯性，确保跨连续帧的高度估计稳定和准确。

Result: SC-Lane在OpenLane基准测试中实现了最先进的性能，F-score为64.3%，明显优于现有方法。

Conclusion: SC-Lane显著提高了高度估计和3D车道检测，在OpenLane基准测试中实现了最先进的性能，F-score为64.3%，明显优于现有方法。

Abstract: In this paper, we introduce SC-Lane, a novel slope-aware and temporally
consistent heightmap estimation framework for 3D lane detection. Unlike
previous approaches that rely on fixed slope anchors, SC-Lane adaptively
determines the fusion of slope-specific height features, improving robustness
to diverse road geometries. To achieve this, we propose a Slope-Aware Adaptive
Feature module that dynamically predicts the appropriate weights from image
cues for integrating multi-slope representations into a unified heightmap.
Additionally, a Height Consistency Module enforces temporal coherence, ensuring
stable and accurate height estimation across consecutive frames, which is
crucial for real-world driving scenarios. To evaluate the effectiveness of
SC-Lane, we employ three standardized metrics-Mean Absolute Error(MAE), Root
Mean Squared Error (RMSE), and threshold-based accuracy-which, although common
in surface and depth estimation, have been underutilized for road height
assessment. Using the LiDAR-derived heightmap dataset introduced in prior work
[20], we benchmark our method under these metrics, thereby establishing a
rigorous standard for future comparisons. Extensive experiments on the OpenLane
benchmark demonstrate that SC-Lane significantly improves both height
estimation and 3D lane detection, achieving state-of-the-art performance with
an F-score of 64.3%, outperforming existing methods by a notable margin. For
detailed results and a demonstration video, please refer to our project
page:https://parkchaesong.github.io/sclane/

</details>


### [84] [NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer](https://arxiv.org/abs/2508.10424)
*Shanyuan Liu,Jian Zhu,Junda Lu,Yue Gong,Liuzhuozheng Li,Bo Cheng,Yuhang Ma,Liebucha Wu,Xiaoyu Wu,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: NanoControl以极低的额外开销实现了高效且高质量的可控文本到图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于ControlNet范式，引入了显著的参数开销和增加的计算成本。

Method: 采用Flux作为主干网络，设计了一个LoRA风格的控制模块，并引入了KV-Context Augmentation机制。

Result: 实现了最先进的可控文本到图像生成性能，同时参数数量仅增加0.024%，GFLOPs仅增加0.029%。

Conclusion: NanoControl显著降低了计算开销，同时保持了卓越的生成质量和改进的可控性。

Abstract: Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in
text-to-image synthesis. However, in the domain of controllable text-to-image
generation using DiTs, most existing methods still rely on the ControlNet
paradigm originally designed for UNet-based diffusion models. This paradigm
introduces significant parameter overhead and increased computational costs. To
address these challenges, we propose the Nano Control Diffusion Transformer
(NanoControl), which employs Flux as the backbone network. Our model achieves
state-of-the-art controllable text-to-image generation performance while
incurring only a 0.024\% increase in parameter count and a 0.029\% increase in
GFLOPs, thus enabling highly efficient controllable generation. Specifically,
rather than duplicating the DiT backbone for control, we design a LoRA-style
(low-rank adaptation) control module that directly learns control signals from
raw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation
mechanism that integrates condition-specific key-value information into the
backbone in a simple yet highly effective manner, facilitating deep fusion of
conditional features. Extensive benchmark experiments demonstrate that
NanoControl significantly reduces computational overhead compared to
conventional control approaches, while maintaining superior generation quality
and achieving improved controllability.

</details>


### [85] [STRIDE-QA: Visual Question Answering Dataset for Spatiotemporal Reasoning in Urban Driving Scenes](https://arxiv.org/abs/2508.10427)
*Keishi Ishihara,Kento Sasaki,Tsubasa Takahashi,Daiki Shiono,Yu Yamaguchi*

Main category: cs.CV

TL;DR: STRIDE-QA 是一个大规模 VQA 数据集，用于城市驾驶中的时空推理；通用 VLM 在此数据集上表现不佳，但在该数据集上进行微调后性能显着提高。


<details>
  <summary>Details</summary>
Motivation: 静态的、网络来源的图像-文本对训练从根本上限制了理解和预测动态交通场景所需的精确时空推理。

Method: 构建了一个大规模视觉问答 (VQA) 数据集 STRIDE-QA，用于从自我中心视角进行物理基础推理。

Result: 现有的 VLM 表现不佳，但在 STRIDE-QA 上进行微调后，VLM 的性能显着提高。

Conclusion: STRIDE-QA 为开发更可靠的、用于安全关键自主系统的 VLM 奠定了全面的基础。

Abstract: Vision-Language Models (VLMs) have been applied to autonomous driving to
support decision-making in complex real-world scenarios. However, their
training on static, web-sourced image-text pairs fundamentally limits the
precise spatiotemporal reasoning required to understand and predict dynamic
traffic scenes. We address this critical gap with STRIDE-QA, a large-scale
visual question answering (VQA) dataset for physically grounded reasoning from
an ego-centric perspective. Constructed from 100 hours of multi-sensor driving
data in Tokyo, capturing diverse and challenging conditions, STRIDE-QA is the
largest VQA dataset for spatiotemporal reasoning in urban driving, offering 16
million QA pairs over 285K frames. Grounded by dense, automatically generated
annotations including 3D bounding boxes, segmentation masks, and multi-object
tracks, the dataset uniquely supports both object-centric and ego-centric
reasoning through three novel QA tasks that require spatial localization and
temporal prediction. Our benchmarks demonstrate that existing VLMs struggle
significantly, achieving near-zero scores on prediction consistency. In
contrast, VLMs fine-tuned on STRIDE-QA exhibit dramatic performance gains,
achieving 55% success in spatial localization and 28% consistency in future
motion prediction, compared to near-zero scores from general-purpose VLMs.
Therefore, STRIDE-QA establishes a comprehensive foundation for developing more
reliable VLMs for safety-critical autonomous systems.

</details>


### [86] [CRISP: Contrastive Residual Injection and Semantic Prompting for Continual Video Instance Segmentation](https://arxiv.org/abs/2508.10432)
*Baichen Liu,Qi Lyu,Xudong Wang,Jiahua Dong,Lianqing Liu,Zhi Han*

Main category: cs.CV

TL;DR: CRISP是一种用于持续视频实例分割的方法，它通过对比残差注入和语义提示来解决实例、类别和任务方面的混淆，从而在长期任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 持续视频实例分割需要吸收新对象类别的可塑性和保留先前学习的类别的稳定性，同时保持跨帧的时间一致性。

Method: 对比残差注入和语义提示(CRISP)

Result: 在YouTube-VIS-2019和YouTube-VIS-2021数据集上的大量实验表明，CRISP明显优于现有的持续分割方法。

Conclusion: CRISP在长期持续视频实例分割任务中显著优于现有的持续分割方法，避免了灾难性遗忘，有效提高了分割和分类性能。

Abstract: Continual video instance segmentation demands both the plasticity to absorb
new object categories and the stability to retain previously learned ones, all
while preserving temporal consistency across frames. In this work, we introduce
Contrastive Residual Injection and Semantic Prompting (CRISP), an earlier
attempt tailored to address the instance-wise, category-wise, and task-wise
confusion in continual video instance segmentation. For instance-wise learning,
we model instance tracking and construct instance correlation loss, which
emphasizes the correlation with the prior query space while strengthening the
specificity of the current task query. For category-wise learning, we build an
adaptive residual semantic prompt (ARSP) learning framework, which constructs a
learnable semantic residual prompt pool generated by category text and uses an
adjustive query-prompt matching mechanism to build a mapping relationship
between the query of the current task and the semantic residual prompt.
Meanwhile, a semantic consistency loss based on the contrastive learning is
introduced to maintain semantic coherence between object queries and residual
prompts during incremental training. For task-wise learning, to ensure the
correlation at the inter-task level within the query space, we introduce a
concise yet powerful initialization strategy for incremental prompts. Extensive
experiments on YouTube-VIS-2019 and YouTube-VIS-2021 datasets demonstrate that
CRISP significantly outperforms existing continual segmentation methods in the
long-term continual video instance segmentation task, avoiding catastrophic
forgetting and effectively improving segmentation and classification
performance. The code is available at https://github.com/01upup10/CRISP.

</details>


### [87] [DOD-SA: Infrared-Visible Decoupled Object Detection with Single-Modality Annotations](https://arxiv.org/abs/2508.10445)
*Hang Jin,Chenqiang Gao,Junjie Guo,Fangcen Liu,Kanghui Tian,Qinyao Chang*

Main category: cs.CV

TL;DR: Proposes a decoupled object detection framework (DOD-SA) for infrared-visible images using single-modality annotations to reduce annotation costs, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods typically require dual-modality annotations, which incurs high annotation costs.

Method: A novel infrared-visible Decoupled Object Detection framework with Single-modality Annotations, called DOD-SA, built upon a Single- and Dual-Modality Collaborative Teacher-Student Network (CoSD-TSNet), and a Progressive and Self-Tuning Training Strategy (PaST) with a Pseudo Label Assigner (PLA).

Result: The proposed method outperforms state-of-the-art (SOTA).

Conclusion: The method outperforms state-of-the-art (SOTA) on the DroneVehicle dataset.

Abstract: Infrared-visible object detection has shown great potential in real-world
applications, enabling robust all-day perception by leveraging the
complementary information of infrared and visible images. However, existing
methods typically require dual-modality annotations to output detection results
for both modalities during prediction, which incurs high annotation costs. To
address this challenge, we propose a novel infrared-visible Decoupled Object
Detection framework with Single-modality Annotations, called DOD-SA. The
architecture of DOD-SA is built upon a Single- and Dual-Modality Collaborative
Teacher-Student Network (CoSD-TSNet), which consists of a single-modality
branch (SM-Branch) and a dual-modality decoupled branch (DMD-Branch). The
teacher model generates pseudo-labels for the unlabeled modality,
simultaneously supporting the training of the student model. The collaborative
design enables cross-modality knowledge transfer from the labeled modality to
the unlabeled modality, and facilitates effective SM-to-DMD branch supervision.
To further improve the decoupling ability of the model and the pseudo-label
quality, we introduce a Progressive and Self-Tuning Training Strategy (PaST)
that trains the model in three stages: (1) pretraining SM-Branch, (2) guiding
the learning of DMD-Branch by SM-Branch, and (3) refining DMD-Branch. In
addition, we design a Pseudo Label Assigner (PLA) to align and pair labels
across modalities, explicitly addressing modality misalignment during training.
Extensive experiments on the DroneVehicle dataset demonstrate that our method
outperforms state-of-the-art (SOTA).

</details>


### [88] [SkeySpot: Automating Service Key Detection for Digital Electrical Layout Plans in the Construction Industry](https://arxiv.org/abs/2508.10449)
*Dhruv Dosi,Rohit Meena,Param Rajpura,Yogesh Kumar Meena*

Main category: cs.CV

TL;DR: Introduces SkeySpot, a YOLOv8-based toolkit for real-time electrical symbol detection from scanned floor plans, enhancing accessibility and interoperability in the construction industry.


<details>
  <summary>Details</summary>
Motivation: Lack of machine-readable floor plans makes large-scale interpretation time-consuming and error-prone. Automated symbol spotting offers a scalable solution.

Method: Utilized YOLOv8 object detection model, achieving a mAP of 82.5%, to develop SkeySpot, a lightweight, open-source toolkit.

Result: Introduced the DELP dataset with 2,450 instances across 34 service key classes. YOLOv8 achieved the highest performance with a mAP of 82.5% on this dataset.

Conclusion: SkeySpot toolkit enables real-time detection, classification, and quantification of electrical symbols, producing structured outputs for interoperable building information workflows, enhancing accessibility for SMEs and supporting standardization and sustainability.

Abstract: Legacy floor plans, often preserved only as scanned documents, remain
essential resources for architecture, urban planning, and facility management
in the construction industry. However, the lack of machine-readable floor plans
render large-scale interpretation both time-consuming and error-prone.
Automated symbol spotting offers a scalable solution by enabling the
identification of service key symbols directly from floor plans, supporting
workflows such as cost estimation, infrastructure maintenance, and regulatory
compliance. This work introduces a labelled Digitised Electrical Layout Plans
(DELP) dataset comprising 45 scanned electrical layout plans annotated with
2,450 instances across 34 distinct service key classes. A systematic evaluation
framework is proposed using pretrained object detection models for DELP
dataset. Among the models benchmarked, YOLOv8 achieves the highest performance
with a mean Average Precision (mAP) of 82.5\%. Using YOLOv8, we develop
SkeySpot, a lightweight, open-source toolkit for real-time detection,
classification, and quantification of electrical symbols. SkeySpot produces
structured, standardised outputs that can be scaled up for interoperable
building information workflows, ultimately enabling compatibility across
downstream applications and regulatory platforms. By lowering dependency on
proprietary CAD systems and reducing manual annotation effort, this approach
makes the digitisation of electrical layouts more accessible to small and
medium-sized enterprises (SMEs) in the construction industry, while supporting
broader goals of standardisation, interoperability, and sustainability in the
built environment.

</details>


### [89] [From Images to Perception: Emergence of Perceptual Properties by Reconstructing Images](https://arxiv.org/abs/2508.10450)
*Pablo Hernández-Cámara,Jesus Malo,Valero Laparra*

Main category: cs.CV

TL;DR: 这项工作表明，视觉系统可能会被调整为消除具有该稀疏程度的那些特定程度的失真，并且生物启发模型可以在没有人为监督的情况下学习感知指标。


<details>
  <summary>Details</summary>
Motivation: 许多科学家认为，人类视觉感知可能来自图像统计，从而在早期视觉中形成有效的神经表征。

Method: 一个生物启发式架构，可以适应视网膜V1皮层的几个已知事实，PerceptNet，已经为与图像重建相关的不同任务进行了端到端优化：自动编码、去噪、去模糊和稀疏正则化。

Result: 编码器阶段（类似V1的层）始终表现出与人类对图像失真的感知判断的最高相关性，尽管在初始化或训练中没有使用感知信息。这种对齐表现出对中等噪声、模糊和稀疏的最佳效果。

Conclusion: 生物启发模型可以在没有人为监督的情况下学习感知指标。

Abstract: A number of scientists suggested that human visual perception may emerge from
image statistics, shaping efficient neural representations in early vision. In
this work, a bio-inspired architecture that can accommodate several known facts
in the retina-V1 cortex, the PerceptNet, has been end-to-end optimized for
different tasks related to image reconstruction: autoencoding, denoising,
deblurring, and sparsity regularization. Our results show that the encoder
stage (V1-like layer) consistently exhibits the highest correlation with human
perceptual judgments on image distortion despite not using perceptual
information in the initialization or training. This alignment exhibits an
optimum for moderate noise, blur and sparsity. These findings suggest that the
visual system may be tuned to remove those particular levels of distortion with
that level of sparsity and that biologically inspired models can learn
perceptual metrics without human supervision.

</details>


### [90] [Trajectory-aware Shifted State Space Models for Online Video Super-Resolution](https://arxiv.org/abs/2508.10453)
*Qiang Zhu,Xiandong Meng,Yuxian Jiang,Fan Zhang,David Bull,Shuyuan Zhu,Bing Zeng*

Main category: cs.CV

TL;DR: TS-Mamba is a novel online VSR method based on Trajectory-aware Shifted SSMs, leveraging both long-term trajectory modeling and low-complexity Mamba to achieve efficient spatio-temporal information aggregation.


<details>
  <summary>Details</summary>
Motivation: Most of the existing online VSR methods solely employ one neighboring previous frame to achieve temporal alignment, which limits long-range temporal modeling of videos.

Method: Trajectory-aware Shifted SSMs (TS-Mamba), leveraging both long-term trajectory modeling and low-complexity Mamba to achieve efficient spatio-temporal information aggregation. Specifically, TS-Mamba first constructs the trajectories within a video to select the most similar tokens from the previous frames. Then, a Trajectory-aware Shifted Mamba Aggregation (TSMA) module consisting of proposed shifted SSMs blocks is employed to aggregate the selected tokens.

Result: Extensive experiments on three widely used VSR test datasets demonstrate that compared with six online VSR benchmark models, our TS-Mamba achieves state-of-the-art performance in most cases and over 22.7% complexity reduction (in MACs).

Conclusion: TS-Mamba achieves state-of-the-art performance in most cases and over 22.7% complexity reduction (in MACs).

Abstract: Online video super-resolution (VSR) is an important technique for many
real-world video processing applications, which aims to restore the current
high-resolution video frame based on temporally previous frames. Most of the
existing online VSR methods solely employ one neighboring previous frame to
achieve temporal alignment, which limits long-range temporal modeling of
videos. Recently, state space models (SSMs) have been proposed with linear
computational complexity and a global receptive field, which significantly
improve computational efficiency and performance. In this context, this paper
presents a novel online VSR method based on Trajectory-aware Shifted SSMs
(TS-Mamba), leveraging both long-term trajectory modeling and low-complexity
Mamba to achieve efficient spatio-temporal information aggregation.
Specifically, TS-Mamba first constructs the trajectories within a video to
select the most similar tokens from the previous frames. Then, a
Trajectory-aware Shifted Mamba Aggregation (TSMA) module consisting of proposed
shifted SSMs blocks is employed to aggregate the selected tokens. The shifted
SSMs blocks are designed based on Hilbert scannings and corresponding shift
operations to compensate for scanning losses and strengthen the spatial
continuity of Mamba. Additionally, we propose a trajectory-aware loss function
to supervise the trajectory generation, ensuring the accuracy of token
selection when training our model. Extensive experiments on three widely used
VSR test datasets demonstrate that compared with six online VSR benchmark
models, our TS-Mamba achieves state-of-the-art performance in most cases and
over 22.7\% complexity reduction (in MACs). The source code for TS-Mamba will
be available at https://github.com.

</details>


### [91] [SingleStrip: learning skull-stripping from a single labeled example](https://arxiv.org/abs/2508.10464)
*Bella Specktor-Fadida,Malte Hoffmann*

Main category: cs.CV

TL;DR: Combines domain randomization and self-training with autoencoder-based quality control to train 3D skull-stripping networks using minimal labeled data, achieving performance comparable to models trained with more labeled images.


<details>
  <summary>Details</summary>
Motivation: Manual labeling for deep learning segmentation is laborious and time-consuming, especially for volumetric images like brain MRI. Domain-randomization techniques offer limited anatomical variability when very few label maps are available. Semi-supervised self-training addresses label scarcity by iteratively incorporating model predictions into the training set.

Method: Combining domain randomization with self-training to train three-dimensional skull-stripping networks using as little as a single labeled example. The method includes automatically binning voxel intensities for initial model training, training a convolutional autoencoder (AE) to assess the quality of brain masks predicted for unlabeled data, and selecting top-ranking pseudo-labels to fine-tune the network.

Result: Achieved skull-stripping performance on out-of-distribution data that approaches models trained with more labeled images. AE-based ranking yields a stronger correlation with segmentation accuracy compared to consistency-based ranking under test-time augmentation.

Conclusion: Combining domain randomization and AE-based quality control enables effective semi-supervised segmentation from extremely limited labeled data, potentially easing the labeling burden in studies involving new anatomical structures or emerging imaging techniques.

Abstract: Deep learning segmentation relies heavily on labeled data, but manual
labeling is laborious and time-consuming, especially for volumetric images such
as brain magnetic resonance imaging (MRI). While recent domain-randomization
techniques alleviate the dependency on labeled data by synthesizing diverse
training images from label maps, they offer limited anatomical variability when
very few label maps are available. Semi-supervised self-training addresses
label scarcity by iteratively incorporating model predictions into the training
set, enabling networks to learn from unlabeled data. In this work, we combine
domain randomization with self-training to train three-dimensional
skull-stripping networks using as little as a single labeled example. First, we
automatically bin voxel intensities, yielding labels we use to synthesize
images for training an initial skull-stripping model. Second, we train a
convolutional autoencoder (AE) on the labeled example and use its
reconstruction error to assess the quality of brain masks predicted for
unlabeled data. Third, we select the top-ranking pseudo-labels to fine-tune the
network, achieving skull-stripping performance on out-of-distribution data that
approaches models trained with more labeled images. We compare AE-based ranking
to consistency-based ranking under test-time augmentation, finding that the AE
approach yields a stronger correlation with segmentation accuracy. Our results
highlight the potential of combining domain randomization and AE-based quality
control to enable effective semi-supervised segmentation from extremely limited
labeled data. This strategy may ease the labeling burden that slows progress in
studies involving new anatomical structures or emerging imaging techniques.

</details>


### [92] [Enhanced Sparse Point Cloud Data Processing for Privacy-aware Human Action Recognition](https://arxiv.org/abs/2508.10469)
*Maimunatu Tunau,Vincent Gbouna Zakka,Zhuangzhuang Dai*

Main category: cs.CV

TL;DR: This paper comprehensively evaluates three data processing methods for mmWave radar-based human action recognition, offering guidance for future research.


<details>
  <summary>Details</summary>
Motivation: Traditional vision-based human action recognition systems raise privacy concerns. mmWave radar sensors offer a privacy-preserving alternative, but their data is sparse and noisy.

Method: The paper conducts a detailed performance analysis of DBSCAN, the Hungarian Algorithm, and Kalman Filtering, individually and in combination, using the MiliPoint dataset. It also proposes targeted enhancements to the individual methods.

Result: The results of the performance analysis provide crucial insights into the strengths and trade-offs of each method and their integrations.

Conclusion: This paper provides a detailed performance analysis of three data processing methods (DBSCAN, Hungarian Algorithm, and Kalman Filtering) for mmWave radar data in human action recognition, offering insights into their strengths, trade-offs, and potential improvements.

Abstract: Human Action Recognition (HAR) plays a crucial role in healthcare, fitness
tracking, and ambient assisted living technologies. While traditional vision
based HAR systems are effective, they pose privacy concerns. mmWave radar
sensors offer a privacy preserving alternative but present challenges due to
the sparse and noisy nature of their point cloud data. In the literature, three
primary data processing methods: Density-Based Spatial Clustering of
Applications with Noise (DBSCAN), the Hungarian Algorithm, and Kalman Filtering
have been widely used to improve the quality and continuity of radar data.
However, a comprehensive evaluation of these methods, both individually and in
combination, remains lacking. This paper addresses that gap by conducting a
detailed performance analysis of the three methods using the MiliPoint dataset.
We evaluate each method individually, all possible pairwise combinations, and
the combination of all three, assessing both recognition accuracy and
computational cost. Furthermore, we propose targeted enhancements to the
individual methods aimed at improving accuracy. Our results provide crucial
insights into the strengths and trade-offs of each method and their
integrations, guiding future work on mmWave based HAR systems

</details>


### [93] [STAMP: Multi-pattern Attention-aware Multiple Instance Learning for STAS Diagnosis in Multi-center Histopathology Images](https://arxiv.org/abs/2508.10473)
*Liangrui Pan,xiaoyu Li,Guang Zhu,Guanting Li,Ruixin Wang,Jiadi Luo,Yaning Yang,Liang qingchun,Shaoliang Peng*

Main category: cs.CV

TL;DR: 提出了一种名为STAMP的多模式注意力感知多示例学习框架，用于分析和诊断肺腺癌中的空气间隙扩散（STAS），并在多中心组织病理学图像上取得了有竞争力的诊断结果。


<details>
  <summary>Details</summary>
Motivation: 空气间隙扩散（STAS）是肺腺癌（LUAD）的一种新型侵袭模式，与肿瘤复发和生存率降低有关。然而，LUAD中的大规模STAS诊断仍然是一项劳动密集型工作，由于其独特的病理学特征和形态学特征，容易出现疏忽和误诊。因此，迫切需要利用深度学习模型进行STAS诊断。

Method: 提出了一个多模式注意力感知多示例学习框架STAMP，以分析和诊断多中心组织病理学图像中STAS的存在。双分支架构引导模型从不同的语义空间学习与STAS相关的病理特征。基于Transformer的实例编码和多模式注意聚合模块动态选择与STAS病理密切相关的区域，抑制不相关的噪声，增强全局表征的区分能力。此外，相似性正则化约束防止了跨分支的特征冗余，从而提高了整体诊断准确性。

Result: STAMP在STAS-SXY、STAS-TXY和STAS-TCGA上取得了有竞争力的诊断结果，AUC分别为0.8058、0.8017和0.7928。

Conclusion: STAMP在STAS-SXY、STAS-TXY和STAS-TCGA上取得了有竞争力的诊断结果，AUC分别为0.8058、0.8017和0.7928，超过了临床水平。

Abstract: Spread through air spaces (STAS) constitutes a novel invasive pattern in lung
adenocarcinoma (LUAD), associated with tumor recurrence and diminished survival
rates. However, large-scale STAS diagnosis in LUAD remains a labor-intensive
endeavor, compounded by the propensity for oversight and misdiagnosis due to
its distinctive pathological characteristics and morphological features.
Consequently, there is a pressing clinical imperative to leverage deep learning
models for STAS diagnosis. This study initially assembled histopathological
images from STAS patients at the Second Xiangya Hospital and the Third Xiangya
Hospital of Central South University, alongside the TCGA-LUAD cohort. Three
senior pathologists conducted cross-verification annotations to construct the
STAS-SXY, STAS-TXY, and STAS-TCGA datasets. We then propose a multi-pattern
attention-aware multiple instance learning framework, named STAMP, to analyze
and diagnose the presence of STAS across multi-center histopathology images.
Specifically, the dual-branch architecture guides the model to learn
STAS-associated pathological features from distinct semantic spaces.
Transformer-based instance encoding and a multi-pattern attention aggregation
modules dynamically selects regions closely associated with STAS pathology,
suppressing irrelevant noise and enhancing the discriminative power of global
representations. Moreover, a similarity regularization constraint prevents
feature redundancy across branches, thereby improving overall diagnostic
accuracy. Extensive experiments demonstrated that STAMP achieved competitive
diagnostic results on STAS-SXY, STAS-TXY and STAS-TCGA, with AUCs of 0.8058,
0.8017, and 0.7928, respectively, surpassing the clinical level.

</details>


### [94] [TweezeEdit: Consistent and Efficient Image Editing with Path Regularization](https://arxiv.org/abs/2508.10498)
*Jianda Mao,Kaibo Wang,Yang Xiang,Kani Chen*

Main category: cs.CV

TL;DR: TweezeEdit通过正则化去噪路径实现高效图像编辑，保持语义并快速对齐目标。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度对齐目标提示，同时未能充分保留源图像语义，导致语义保持效果不佳和编辑路径过长。

Method: 提出TweezeEdit，一个免微调和反演的框架，通过正则化整个去噪路径，使用一致性模型沿直接路径注入目标提示语义。

Result: TweezeEdit在语义保持和目标对齐方面优于现有方法，编辑速度快。

Conclusion: TweezeEdit在语义保持和目标对齐方面表现出色，编辑速度快，仅需12步（每次编辑1.6秒）。

Abstract: Large-scale pre-trained diffusion models empower users to edit images through
text guidance. However, existing methods often over-align with target prompts
while inadequately preserving source image semantics. Such approaches generate
target images explicitly or implicitly from the inversion noise of the source
images, termed the inversion anchors. We identify this strategy as suboptimal
for semantic preservation and inefficient due to elongated editing paths. We
propose TweezeEdit, a tuning- and inversion-free framework for consistent and
efficient image editing. Our method addresses these limitations by regularizing
the entire denoising path rather than relying solely on the inversion anchors,
ensuring source semantic retention and shortening editing paths. Guided by
gradient-driven regularization, we efficiently inject target prompt semantics
along a direct path using a consistency model. Extensive experiments
demonstrate TweezeEdit's superior performance in semantic preservation and
target alignment, outperforming existing methods. Remarkably, it requires only
12 steps (1.6 seconds per edit), underscoring its potential for real-time
applications.

</details>


### [95] [Multi-Sample Anti-Aliasing and Constrained Optimization for 3D Gaussian Splatting](https://arxiv.org/abs/2508.10507)
*Zheng Zhou,Jia-Chen Zhang,Yu-Jie Xiong,Chun-Ming Xia*

Main category: cs.CV

TL;DR: 提出了一种综合优化框架，该框架将多重采样抗锯齿 (MSAA) 与双重几何约束相结合，以解决 3D 高斯溅射中精细细节重建模糊的问题。


<details>
  <summary>Details</summary>
Motivation: 3D 高斯溅射的最新进展显着改进了实时新视角合成，但场景优化期间的几何约束不足通常会导致精细细节的重建模糊，尤其是在具有高频纹理和尖锐不连续性的区域中。

Method: 该系统通过自适应混合四重子样本来计算像素颜色，从而有效减少高频分量中的混叠伪影。该框架引入了两个约束：(a) 一种自适应加权策略，通过动态梯度分析优先考虑未充分重建的区域，以及 (b) 梯度微分约束，用于在对象边界处强制执行几何正则化。

Result: 在多个基准上的大量实验评估表明，我们的方法在细节保持方面取得了最先进的性能，尤其是在保持高频纹理和尖锐不连续性方面，同时保持了实时渲染效率。定量指标和感知研究证实，在结构相似性 (SSIM) 和感知质量 (LPIPS) 方面，相对于基线方法，该方法在统计上具有显着改进。

Conclusion: 该方法在细节保持方面取得了最先进的性能，尤其是在保持高频纹理和尖锐不连续性方面，同时保持了实时渲染效率。定量指标和感知研究证实，在结构相似性 (SSIM) 和感知质量 (LPIPS) 方面，相对于基线方法，该方法在统计上具有显着改进。

Abstract: Recent advances in 3D Gaussian splatting have significantly improved
real-time novel view synthesis, yet insufficient geometric constraints during
scene optimization often result in blurred reconstructions of fine-grained
details, particularly in regions with high-frequency textures and sharp
discontinuities. To address this, we propose a comprehensive optimization
framework integrating multisample anti-aliasing (MSAA) with dual geometric
constraints. Our system computes pixel colors through adaptive blending of
quadruple subsamples, effectively reducing aliasing artifacts in high-frequency
components. The framework introduces two constraints: (a) an adaptive weighting
strategy that prioritizes under-reconstructed regions through dynamic gradient
analysis, and (b) gradient differential constraints enforcing geometric
regularization at object boundaries. This targeted optimization enables the
model to allocate computational resources preferentially to critical regions
requiring refinement while maintaining global consistency. Extensive
experimental evaluations across multiple benchmarks demonstrate that our method
achieves state-of-the-art performance in detail preservation, particularly in
preserving high-frequency textures and sharp discontinuities, while maintaining
real-time rendering efficiency. Quantitative metrics and perceptual studies
confirm statistically significant improvements over baseline approaches in both
structural similarity (SSIM) and perceptual quality (LPIPS).

</details>


### [96] [A Segmentation-driven Editing Method for Bolt Defect Augmentation and Detection](https://arxiv.org/abs/2508.10509)
*Yangjie Xiao,Ke Zhang,Jiacun Wang,Xin Sheng,Yurong Guo,Meijuan Chen,Zehua Ren,Zhaoye Zheng,Zhenbing Zhao*

Main category: cs.CV

TL;DR: 该文提出了一种新的数据增强方法，通过分割和编辑螺栓图像来提高缺陷检测的性能。


<details>
  <summary>Details</summary>
Motivation: 螺栓缺陷检测对于确保输电线路的安全至关重要。然而，缺陷图像的稀缺性和不平衡的数据分布严重限制了检测性能。为了解决这个问题，

Method: 该文提出了一种分割驱动的螺栓缺陷编辑方法 (SBDE) 来扩充数据集。首先，提出了螺栓属性分割模型 (Bolt-SAM)，通过 CLAHE-FFT 适配器 (CFA) 和多部分感知掩码解码器 (MAMD) 增强了复杂螺栓属性的分割，为后续编辑任务生成高质量的掩码。其次，设计了一个掩码优化模块 (MOD) 并与图像修复模型 (LaMa) 集成，构建了螺栓缺陷属性编辑模型 (MOD-LaMa)，通过属性编辑将普通螺栓转换为有缺陷的螺栓。最后，提出了一种编辑恢复增强 (ERA) 策略，以恢复并将编辑后的缺陷螺栓放回原始检查场景中，并扩展缺陷检测数据集。

Result: 实验结果表明，该文提出的SBDE方法生成的螺栓缺陷图像明显优于目前最好的图像编辑模型，并有效提高了螺栓缺陷检测的性能。

Conclusion: 该文提出的SBDE方法生成的螺栓缺陷图像明显优于目前最好的图像编辑模型，有效提高了螺栓缺陷检测的性能，充分验证了该方法的有效性和应用潜力。

Abstract: Bolt defect detection is critical to ensure the safety of transmission lines.
However, the scarcity of defect images and imbalanced data distributions
significantly limit detection performance. To address this problem, we propose
a segmentationdriven bolt defect editing method (SBDE) to augment the dataset.
First, a bolt attribute segmentation model (Bolt-SAM) is proposed, which
enhances the segmentation of complex bolt attributes through the CLAHE-FFT
Adapter (CFA) and Multipart- Aware Mask Decoder (MAMD), generating high-quality
masks for subsequent editing tasks. Second, a mask optimization module (MOD) is
designed and integrated with the image inpainting model (LaMa) to construct the
bolt defect attribute editing model (MOD-LaMa), which converts normal bolts
into defective ones through attribute editing. Finally, an editing recovery
augmentation (ERA) strategy is proposed to recover and put the edited defect
bolts back into the original inspection scenes and expand the defect detection
dataset. We constructed multiple bolt datasets and conducted extensive
experiments. Experimental results demonstrate that the bolt defect images
generated by SBDE significantly outperform state-of-the-art image editing
models, and effectively improve the performance of bolt defect detection, which
fully verifies the effectiveness and application potential of the proposed
method. The code of the project is available at
https://github.com/Jay-xyj/SBDE.

</details>


### [97] [EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba](https://arxiv.org/abs/2508.10522)
*Quang Nguyen,Nhat Le,Baoru Huang,Minh Nhat Vu,Chengcheng Tang,Van Nguyen,Ngan Le,Thieu Vo,Anh Nguyen*

Main category: cs.CV

TL;DR: This paper introduces a new method and the EgoAIST++ dataset for predicting human dance motion from egocentric video and music, using a Skeleton Mamba-based network.


<details>
  <summary>Details</summary>
Motivation: Estimating human dance motion from both egocentric video and music is largely unexplored, while the egocentric view often obscures much of the body, and incorporating music requires the generated head and body movements to align well with both visual and musical inputs.

Method: The paper develops an EgoMusic Motion Network with a core Skeleton Mamba that explicitly captures the skeleton structure of the human body, drawing on the success of diffusion models and Mamba.

Result: The approach outperforms state-of-the-art approaches and generalizes effectively to real-world data.

Conclusion: The proposed EgoMusic Motion Network with a core Skeleton Mamba outperforms state-of-the-art approaches and generalizes effectively to real-world data.

Abstract: Estimating human dance motion is a challenging task with various industrial
applications. Recently, many efforts have focused on predicting human dance
motion using either egocentric video or music as input. However, the task of
jointly estimating human motion from both egocentric video and music remains
largely unexplored. In this paper, we aim to develop a new method that predicts
human dance motion from both egocentric video and music. In practice, the
egocentric view often obscures much of the body, making accurate full-pose
estimation challenging. Additionally, incorporating music requires the
generated head and body movements to align well with both visual and musical
inputs. We first introduce EgoAIST++, a new large-scale dataset that combines
both egocentric views and music with more than 36 hours of dancing motion.
Drawing on the success of diffusion models and Mamba on modeling sequences, we
develop an EgoMusic Motion Network with a core Skeleton Mamba that explicitly
captures the skeleton structure of the human body. We illustrate that our
approach is theoretically supportive. Intensive experiments show that our
method clearly outperforms state-of-the-art approaches and generalizes
effectively to real-world data.

</details>


### [98] [Reasoning in Computer Vision: Taxonomy, Models, Tasks, and Methodologies](https://arxiv.org/abs/2508.10523)
*Ayushman Sarkar,Mohd Yamani Idna Idris,Zhenyu Yu*

Main category: cs.CV

TL;DR: 本调查对视觉推理进行了分类，并考察了其在架构和评估中的应用，同时着重强调了它在弥合感知和推理方面的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的调查通常孤立地处理这些方向，缺乏跨推理类型、方法和评估协议的统一分析和比较。

Method: 通过图模型、记忆网络、注意力机制和神经符号系统等架构，系统地检查了五种主要的视觉推理类型（关系、符号、时间、因果和常识）的实现。

Result: 对评估协议进行了回顾，这些协议旨在评估功能正确性、结构一致性和因果有效性，并批判性地分析了它们在泛化性、可重复性和解释能力方面的局限性。确定了视觉推理中的关键开放挑战，包括复杂场景的可扩展性、符号和神经范式的更深入集成、缺乏全面的基准数据集以及在弱监督下的推理。

Conclusion: 弥合感知和推理对于构建透明、值得信赖和跨领域自适应的 AI 系统至关重要，尤其是在自动驾驶和医疗诊断等关键领域。

Abstract: Visual reasoning is critical for a wide range of computer vision tasks that
go beyond surface-level object detection and classification. Despite notable
advances in relational, symbolic, temporal, causal, and commonsense reasoning,
existing surveys often address these directions in isolation, lacking a unified
analysis and comparison across reasoning types, methodologies, and evaluation
protocols. This survey aims to address this gap by categorizing visual
reasoning into five major types (relational, symbolic, temporal, causal, and
commonsense) and systematically examining their implementation through
architectures such as graph-based models, memory networks, attention
mechanisms, and neuro-symbolic systems. We review evaluation protocols designed
to assess functional correctness, structural consistency, and causal validity,
and critically analyze their limitations in terms of generalizability,
reproducibility, and explanatory power. Beyond evaluation, we identify key open
challenges in visual reasoning, including scalability to complex scenes, deeper
integration of symbolic and neural paradigms, the lack of comprehensive
benchmark datasets, and reasoning under weak supervision. Finally, we outline a
forward-looking research agenda for next-generation vision systems, emphasizing
that bridging perception and reasoning is essential for building transparent,
trustworthy, and cross-domain adaptive AI systems, particularly in critical
domains such as autonomous driving and medical diagnostics.

</details>


### [99] [Med-GLIP: Advancing Medical Language-Image Pre-training with Large-scale Grounded Dataset](https://arxiv.org/abs/2508.10528)
*Ziye Deng,Ruihan He,Jiaxiang Liu,Yuan Wang,Zijie Meng,Songtao Jiang,Yong Xie,Zuozhu Liu*

Main category: cs.CV

TL;DR: Med-GLIP-5M 是一个大型医学 grounding 数据集，Med-GLIP 是一个在 Med-GLIP-5M 上训练的模态感知 grounding 框架。


<details>
  <summary>Details</summary>
Motivation: 现有的研究受到有限的模态覆盖、粗粒度注释以及缺乏统一的、可推广的 grounding 框架的限制。

Method: 我们提出了 Med-GLIP，一个在 Med-GLIP-5M 上训练的模态感知 grounding 框架。

Result: 我们构建了一个大型医学 grounding 数据集 Med-GLIP-5M，该数据集包含七种成像模态的超过 530 万个区域级注释，涵盖不同的解剖结构和病理学发现。

Conclusion: Med-GLIP在多个 grounding 基准测试中始终优于最先进的基线。此外，将其空间输出集成到下游任务（包括医学 VQA 和报告生成）中，可带来显着的性能提升。

Abstract: Medical image grounding aims to align natural language phrases with specific
regions in medical images, serving as a foundational task for intelligent
diagnosis, visual question answering (VQA), and automated report generation
(MRG). However, existing research is constrained by limited modality coverage,
coarse-grained annotations, and the absence of a unified, generalizable
grounding framework. To address these challenges, we construct a large-scale
medical grounding dataset Med-GLIP-5M comprising over 5.3 million region-level
annotations across seven imaging modalities, covering diverse anatomical
structures and pathological findings. The dataset supports both segmentation
and grounding tasks with hierarchical region labels, ranging from organ-level
boundaries to fine-grained lesions. Based on this foundation, we propose
Med-GLIP, a modality-aware grounding framework trained on Med-GLIP-5M. Rather
than relying on explicitly designed expert modules, Med-GLIP implicitly
acquires hierarchical semantic understanding from diverse training data --
enabling it to recognize multi-granularity structures, such as distinguishing
lungs from pneumonia lesions. Extensive experiments demonstrate that Med-GLIP
consistently outperforms state-of-the-art baselines across multiple grounding
benchmarks. Furthermore, integrating its spatial outputs into downstream tasks,
including medical VQA and report generation, leads to substantial performance
gains. Our dataset will be released soon.

</details>


### [100] [GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images](https://arxiv.org/abs/2508.10542)
*Mengyu Ren,Yutong Li,Hua Li,Runmin Cong,Sam Kwong*

Main category: cs.CV

TL;DR: Proposes a new network (GCRPNet) based on Mamba architecture for salient object detection in remote sensing images, achieving state-of-the-art performance by better integrating global and local features.


<details>
  <summary>Details</summary>
Motivation: Existing methods based on vision transformers (ViTs) and convolutional neural networks (CNNs) architectures aim to leverage both global and local features, but the difficulty in effectively integrating these heterogeneous features limits their overall performance. Salient object detection (SOD) in optical remote sensing images (ORSIs) faces numerous challenges, including significant variations in target scales and low contrast between targets and the background.

Method: We propose a graph-enhanced contextual and regional perception network (GCRPNet), which builds upon the Mamba architecture. It uses a visual state space (VSS) encoder, a difference-similarity guided hierarchical graph attention module (DS-HGAM), and a LEVSS block decoder with adaptive scanning strategy and multi-granularity collaborative attention enhancement module (MCAEM).

Result: achieves state-of-the-art performance

Conclusion: The proposed model achieves state-of-the-art performance, validating its effectiveness and superiority.

Abstract: Salient object detection (SOD) in optical remote sensing images (ORSIs) faces
numerous challenges, including significant variations in target scales and low
contrast between targets and the background. Existing methods based on vision
transformers (ViTs) and convolutional neural networks (CNNs) architectures aim
to leverage both global and local features, but the difficulty in effectively
integrating these heterogeneous features limits their overall performance. To
overcome these limitations, we propose a graph-enhanced contextual and regional
perception network (GCRPNet), which builds upon the Mamba architecture to
simultaneously capture long-range dependencies and enhance regional feature
representation. Specifically, we employ the visual state space (VSS) encoder to
extract multi-scale features. To further achieve deep guidance and enhancement
of these features, we first design a difference-similarity guided hierarchical
graph attention module (DS-HGAM). This module strengthens cross-layer
interaction capabilities between features of different scales while enhancing
the model's structural perception,allowing it to distinguish between foreground
and background more effectively. Then, we design the LEVSS block as the decoder
of GCRPNet. This module integrates our proposed adaptive scanning strategy and
multi-granularity collaborative attention enhancement module (MCAEM). It
performs adaptive patch scanning on feature maps processed via multi-scale
convolutions, thereby capturing rich local region information and enhancing
Mamba's local modeling capability. Extensive experimental results demonstrate
that the proposed model achieves state-of-the-art performance, validating its
effectiveness and superiority.

</details>


### [101] [PSScreen: Partially Supervised Multiple Retinal Disease Screening](https://arxiv.org/abs/2508.10549)
*Boyi Zheng,Qing Liu*

Main category: cs.CV

TL;DR: PSScreen是一种用于多重视网膜疾病筛查的部分监督模型，通过领域泛化和伪标签一致性来解决领域转移和标签缺失问题，并在同域和异域数据集上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 利用多个部分标记的数据集来训练多重视网膜疾病筛查模型减少了对完全注释数据集的依赖，但由于来自各个医疗站点的训练数据集之间的显着领域转移以及部分类别的标签缺失问题，仍然具有挑战性。

Method: PSScreen，一种新的部分监督多重视网膜疾病筛查模型，由两个流组成，一个学习确定性特征，另一个通过不确定性注入学习概率特征。然后，我们利用文本指导将两种类型的特征解耦为疾病特征，并通过特征蒸馏对齐它们，以提高领域泛化能力。同时，我们采用两个流之间的伪标签一致性来解决标签缺失问题，并引入自蒸馏来传递关于已知类的任务相关语义，从确定性流到概率流，以进一步提高检测性能。

Result: PSScreen在六种视网膜疾病和正常状态的检测性能上显著提高，并在同域和异域数据集上取得了最先进的结果。

Conclusion: PSScreen显著提高了六种视网膜疾病和正常状态的检测性能，并在同域和异域数据集上取得了最先进的结果。

Abstract: Leveraging multiple partially labeled datasets to train a model for multiple
retinal disease screening reduces the reliance on fully annotated datasets, but
remains challenging due to significant domain shifts across training datasets
from various medical sites, and the label absent issue for partial classes. To
solve these challenges, we propose PSScreen, a novel Partially Supervised
multiple retinal disease Screening model. Our PSScreen consists of two streams
and one learns deterministic features and the other learns probabilistic
features via uncertainty injection. Then, we leverage the textual guidance to
decouple two types of features into disease-wise features and align them via
feature distillation to boost the domain generalization ability. Meanwhile, we
employ pseudo label consistency between two streams to address the label absent
issue and introduce a self-distillation to transfer task-relevant semantics
about known classes from the deterministic to the probabilistic stream to
further enhance the detection performances. Experiments show that our PSScreen
significantly enhances the detection performances on six retinal diseases and
the normal state averagely and achieves state-of-the-art results on both
in-domain and out-of-domain datasets. Codes are available at
https://github.com/boyiZheng99/PSScreen.

</details>


### [102] [AR Surgical Navigation With Surface Tracing: Comparing In-SitVisualization with Tool-Tracking Guidance for Neurosurgical Applications](https://arxiv.org/abs/2508.10554)
*Marc J. Fischer,Jeffrey Potts,Gabriel Urreola,Dax Jones,Paolo Palmisciano,E. Bradley Strong,Branden Cord,Andrew D. Hernandez,Julia D. Sharma,E. Brandon Strong*

Main category: cs.CV

TL;DR: This study presents a novel AR guidance method for surgical navigation using the HoloLens 2, which improves accuracy and is preferred by users.


<details>
  <summary>Details</summary>
Motivation: known issues with AR depth perception due to vergence-accommodation conflict and occlusion handling limitations of the currently commercially available display technology present acute challenges in surgical settings where precision is paramount

Method: a novel methodology for utilizing AR guidance to register anatomical targets and provide real-time instrument navigation using placement of simulated external ventricular drain catheters on a phantom model

Result: computed tomography scans of the phantom models were acquired, allowing for evaluation of insertion accuracy, target deviation, angular error, and depth precision. System Usability Scale surveys assessed user experience and cognitive workload

Conclusion: Tool-tracking guidance improved performance metrics across all accuracy measures and was preferred by users in subjective evaluations.

Abstract: Augmented Reality (AR) surgical navigation systems are emerging as the next
generation of intraoperative surgical guidance, promising to overcome
limitations of traditional navigation systems. However, known issues with AR
depth perception due to vergence-accommodation conflict and occlusion handling
limitations of the currently commercially available display technology present
acute challenges in surgical settings where precision is paramount. This study
presents a novel methodology for utilizing AR guidance to register anatomical
targets and provide real-time instrument navigation using placement of
simulated external ventricular drain catheters on a phantom model as the
clinical scenario. The system registers target positions to the patient through
a novel surface tracing method and uses real-time infrared tool tracking to aid
in catheter placement, relying only on the onboard sensors of the Microsoft
HoloLens 2. A group of intended users performed the procedure of simulated
insertions under two AR guidance conditions: static in-situ visualization,
where planned trajectories are overlaid directly onto the patient anatomy, and
real-time tool-tracking guidance, where live feedback of the catheter's pose is
provided relative to the plan. Following the insertion tests, computed
tomography scans of the phantom models were acquired, allowing for evaluation
of insertion accuracy, target deviation, angular error, and depth precision.
System Usability Scale surveys assessed user experience and cognitive workload.
Tool-tracking guidance improved performance metrics across all accuracy
measures and was preferred by users in subjective evaluations. A free copy of
this paper and all supplemental materials are available at
https://bit.ly/45l89Hq.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [103] [A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions](https://arxiv.org/abs/2508.10047)
*Ziyang Xiao,Jingrong Xie,Lilin Xu,Shisi Guan,Jingyan Zhu,Xiongwei Han,Xiaojin Fu,WingYin Yu,Han Wu,Wei Shi,Qingcan Kang,Jiahui Duan,Tao Zhong,Mingxuan Yuan,Jia Zeng,Yuan Wang,Gang Chen,Dongxiang Zhang*

Main category: cs.AI

TL;DR: Surveys recent advancements in using LLMs to automate mathematical modeling, identifies and corrects errors in benchmark datasets, and builds a resource portal.


<details>
  <summary>Details</summary>
Motivation: Optimization modeling has been widely employed for optimal decision-making across various sectors, but it requires substantial expertise from operations research professionals. With the advent of large language models (LLMs), new opportunities have emerged to automate the procedure of mathematical modeling.

Method: Presents a comprehensive and timely review of recent advancements that cover the entire technical stack, including data synthesis and fine-tuning for the base model, inference frameworks, benchmark datasets, and performance evaluation. Also conducted an in-depth analysis on the quality of benchmark datasets, cleaned the datasets and constructed a new leaderboard with fair performance evaluation, and build an online portal that integrates resources of cleaned datasets, code and paper repository to benefit the community.

Result: Conducted an in-depth analysis on the quality of benchmark datasets, which was found to have a surprisingly high error rate. Cleaned the datasets and constructed a new leaderboard with fair performance evaluation in terms of base LLM model and datasets. Build an online portal that integrates resources of cleaned datasets, code and paper repository to benefit the community.

Conclusion: Identifies limitations in current methodologies and outlines future research opportunities.

Abstract: By virtue of its great utility in solving real-world problems, optimization
modeling has been widely employed for optimal decision-making across various
sectors, but it requires substantial expertise from operations research
professionals. With the advent of large language models (LLMs), new
opportunities have emerged to automate the procedure of mathematical modeling.
This survey presents a comprehensive and timely review of recent advancements
that cover the entire technical stack, including data synthesis and fine-tuning
for the base model, inference frameworks, benchmark datasets, and performance
evaluation. In addition, we conducted an in-depth analysis on the quality of
benchmark datasets, which was found to have a surprisingly high error rate. We
cleaned the datasets and constructed a new leaderboard with fair performance
evaluation in terms of base LLM model and datasets. We also build an online
portal that integrates resources of cleaned datasets, code and paper repository
to benefit the community. Finally, we identify limitations in current
methodologies and outline future research opportunities.

</details>


### [104] [Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development](https://arxiv.org/abs/2508.10108)
*Sattvik Sahai,Prasoon Goyal,Michael Johnston,Anna Gottardi,Yao Lu,Lucy Hu,Luke Dai,Shaohua Liu,Samyuth Sagi,Hangjie Shi,Desheng Zhang,Lavina Vaz,Leslie Ball,Maureen Murray,Rahul Gupta,Shankar Ananthakrishna*

Main category: cs.AI

TL;DR: The Amazon Nova AI Challenge fostered advancements in AI safety for software development through a competition between university teams, focusing on red teaming and safety alignment methods.


<details>
  <summary>Details</summary>
Motivation: Ensuring the safety of AI systems for software development.

Method: Developing automated red teaming bots and safe AI assistants, along with a custom baseline coding specialist model, a tournament orchestration service, and an evaluation harness.

Result: Novel approaches in reasoning-based safety alignment, robust model guardrails, multi-turn jail-breaking, and efficient probing of large language models (LLMs).

Conclusion: University teams and the Amazon Nova AI Challenge team made advancements in addressing the safety challenges of AI for software development.

Abstract: AI systems for software development are rapidly gaining prominence, yet
significant challenges remain in ensuring their safety. To address this, Amazon
launched the Trusted AI track of the Amazon Nova AI Challenge, a global
competition among 10 university teams to drive advances in secure AI. In the
challenge, five teams focus on developing automated red teaming bots, while the
other five create safe AI assistants. This challenge provides teams with a
unique platform to evaluate automated red-teaming and safety alignment methods
through head-to-head adversarial tournaments where red teams have multi-turn
conversations with the competing AI coding assistants to test their safety
alignment. Along with this, the challenge provides teams with a feed of high
quality annotated data to fuel iterative improvement. Throughout the challenge,
teams developed state-of-the-art techniques, introducing novel approaches in
reasoning-based safety alignment, robust model guardrails, multi-turn
jail-breaking, and efficient probing of large language models (LLMs). To
support these efforts, the Amazon Nova AI Challenge team made substantial
scientific and engineering investments, including building a custom baseline
coding specialist model for the challenge from scratch, developing a tournament
orchestration service, and creating an evaluation harness. This paper outlines
the advancements made by university teams and the Amazon Nova AI Challenge team
in addressing the safety challenges of AI for software development,
highlighting this collaborative effort to raise the bar for AI safety.

</details>


### [105] [MCP-Orchestrated Multi-Agent System for Automated Disinformation Detection](https://arxiv.org/abs/2508.10143)
*Alexandru-Andrei Avram,Adrian Groza,Alexandru Lecu*

Main category: cs.AI

TL;DR: 提出了一种多智能体系统，该系统使用关系抽取来检测新闻文章中的虚假信息，该系统优于传统方法，并且易于扩展。


<details>
  <summary>Details</summary>
Motivation: 数字平台上的大量虚假信息传播对信息完整性构成了重大挑战。

Method: 多智能体系统，结合了机器学习智能体（逻辑回归）、维基百科知识检查智能体（依赖于命名实体识别）、连贯性检测智能体（使用 LLM 提示工程）和网络抓取的用于事实检查的数据分析器。

Result: 多智能体集成系统实现了 95.3% 的准确率和 0.964 的 F1 分数，明显优于单个智能体和传统方法。

Conclusion: 该多智能体集成系统实现了 95.3% 的准确率和 0.964 的 F1 分数，明显优于单个智能体和传统方法。 加权聚合方法优于算法阈值优化。 模块化架构使系统易于扩展，同时保持决策过程的细节。

Abstract: The large spread of disinformation across digital platforms creates
significant challenges to information integrity. This paper presents a
multi-agent system that uses relation extraction to detect disinformation in
news articles, focusing on titles and short text snippets. The proposed Agentic
AI system combines four agents: (i) a machine learning agent (logistic
regression), (ii) a Wikipedia knowledge check agent (which relies on named
entity recognition), (iii) a coherence detection agent (using LLM prompt
engineering), and (iv) a web-scraped data analyzer that extracts relational
triplets for fact checking. The system is orchestrated via the Model Context
Protocol (MCP), offering shared context and live learning across components.
Results demonstrate that the multi-agent ensemble achieves 95.3% accuracy with
an F1 score of 0.964, significantly outperforming individual agents and
traditional approaches. The weighted aggregation method, mathematically derived
from individual agent misclassification rates, proves superior to algorithmic
threshold optimization. The modular architecture makes the system easily
scalable, while also maintaining details of the decision processes.

</details>


### [106] [Agentic AI Frameworks: Architectures, Protocols, and Design Challenges](https://arxiv.org/abs/2508.10146)
*Hana Derouiche,Zaki Brahmi,Haithem Mazeni*

Main category: cs.AI

TL;DR: This paper reviews Agentic AI frameworks, analyzes agent communication protocols, and proposes future research directions.


<details>
  <summary>Details</summary>
Motivation: The emergence of Large Language Models (LLMs) has ushered in a transformative paradigm in artificial intelligence, Agentic AI, where intelligent agents exhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent coordination.

Method: systematic review and comparative analysis of leading Agentic AI frameworks

Result: evaluation of architectural principles, communication mechanisms, memory management, safety guardrails, and alignment with service-oriented computing paradigms of leading Agentic AI frameworks; in-depth analysis of protocols such as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network Protocol (ANP), and Agora.

Conclusion: This paper establishes a foundational taxonomy for Agentic AI systems and proposes future research directions to enhance scalability, robustness, and interoperability.

Abstract: The emergence of Large Language Models (LLMs) has ushered in a transformative
paradigm in artificial intelligence, Agentic AI, where intelligent agents
exhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent
coordination. This paper provides a systematic review and comparative analysis
of leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen,
Semantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural
principles, communication mechanisms, memory management, safety guardrails, and
alignment with service-oriented computing paradigms. Furthermore, we identify
key limitations, emerging trends, and open challenges in the field. To address
the issue of agent communication, we conduct an in-depth analysis of protocols
such as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network
Protocol (ANP), and Agora. Our findings not only establish a foundational
taxonomy for Agentic AI systems but also propose future research directions to
enhance scalability, robustness, and interoperability. This work serves as a
comprehensive reference for researchers and practitioners working to advance
the next generation of autonomous AI systems.

</details>


### [107] [Improving and Evaluating Open Deep Research Agents](https://arxiv.org/abs/2508.10152)
*Doaa Allabadi,Kyle Bradbury,Jordan M. Malof*

Main category: cs.AI

TL;DR: This paper compares open-source and closed-source DRAs on a new benchmark, finds that all systems perform poorly, and introduces improvements to the open-source DRA to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Recent research largely involves proprietary closed-source systems. We only found one open-source DRA, termed Open Deep Research (ODR).

Method: Adapt the BrowseComp benchmark to compare ODR to existing proprietary systems and introduce three strategic improvements to ODR.

Result: All three systems achieve 0% accuracy on the test set. ODR+ model, achieves a state-of-the-art 10% success rate on BC-Small.

Conclusion: The ODR+ model achieves a state-of-the-art 10% success rate on BC-Small.

Abstract: We focus here on Deep Research Agents (DRAs), which are systems that can take
a natural language prompt from a user, and then autonomously search for, and
utilize, internet-based content to address the prompt. Recent DRAs have
demonstrated impressive capabilities on public benchmarks however, recent
research largely involves proprietary closed-source systems. At the time of
this work, we only found one open-source DRA, termed Open Deep Research (ODR).
In this work we adapt the challenging recent BrowseComp benchmark to compare
ODR to existing proprietary systems. We propose BrowseComp-Small (BC-Small),
comprising a subset of BrowseComp, as a more computationally-tractable DRA
benchmark for academic labs. We benchmark ODR and two other proprietary systems
on BC-Small: one system from Anthropic and one system from Google. We find that
all three systems achieve 0% accuracy on the test set of 60 questions. We
introduce three strategic improvements to ODR, resulting in the ODR+ model,
which achieves a state-of-the-art 10% success rate on BC-Small among both
closed-source and open-source systems. We report ablation studies indicating
that all three of our improvements contributed to the success of ODR+.

</details>


### [108] [Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization](https://arxiv.org/abs/2508.10164)
*Bin Hong,Jiayu Liu,Zhenya Huang,Kai Zhang,Mengdi Zhang*

Main category: cs.AI

TL;DR: This paper introduces Length Controlled Preference Optimization (LCPO) to make large reasoning models more efficient by reducing output length without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: Long outputs of Large Reasoning Models (LRMs) increase computational costs and may lead to overthinking. Current methods for efficient reasoning often compromise reasoning quality or require extensive resources.

Method: The paper analyzes generation path distributions, filters generated trajectories through difficulty estimation, and analyzes the convergence behaviors of preference optimization methods. Based on this, it proposes LCPO which balances the implicit reward related to NLL loss.

Result: LCPO reduces the average output length by over 50% across multiple benchmarks while maintaining reasoning performance.

Conclusion: The paper proposes Length Controlled Preference Optimization (LCPO) to reduce the generation length of Large Reasoning Models (LRMs) while maintaining reasoning performance.

Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated strong
performance on complex tasks through long Chain-of-Thought (CoT) reasoning.
However, their lengthy outputs increase computational costs and may lead to
overthinking, raising challenges in balancing reasoning effectiveness and
efficiency. Current methods for efficient reasoning often compromise reasoning
quality or require extensive resources. This paper investigates efficient
methods to reduce the generation length of LRMs. We analyze generation path
distributions and filter generated trajectories through difficulty estimation.
Subsequently, we analyze the convergence behaviors of the objectives of various
preference optimization methods under a Bradley-Terry loss based framework.
Based on the analysis, we propose Length Controlled Preference Optimization
(LCPO) that directly balances the implicit reward related to NLL loss. LCPO can
effectively learn length preference with limited data and training. Extensive
experiments demonstrate that our approach significantly reduces the average
output length by over 50\% across multiple benchmarks while maintaining the
reasoning performance. Our work highlights the potential for computationally
efficient approaches in guiding LRMs toward efficient reasoning.

</details>


### [109] [KompeteAI: Accelerated Autonomous Multi-Agent System for End-to-End Pipeline Generation for Machine Learning Problems](https://arxiv.org/abs/2508.10177)
*Stepan Kulibaba,Artem Dzhalilov,Roman Pakhomov,Oleg Svidchenko,Alexander Gasnikov,Aleksei Shpilman*

Main category: cs.AI

TL;DR: KompeteAI, a novel AutoML framework with dynamic solution space exploration, outperforms leading methods and accelerates pipeline evaluation.


<details>
  <summary>Details</summary>
Motivation: Recent Large Language Model (LLM)-based AutoML systems face significant limitations such as constrained exploration strategies and a severe execution bottleneck.

Method: KompeteAI introduces a merging stage that composes top candidates and integrates Retrieval-Augmented Generation (RAG), sourcing ideas from Kaggle notebooks and arXiv papers.

Result: KompeteAI accelerates pipeline evaluation 6.9 times.

Conclusion: KompeteAI outperforms leading methods by an average of 3% on the primary AutoML benchmark, MLE-Bench, and achieves state-of-the-art results on Kompete-bench.

Abstract: Recent Large Language Model (LLM)-based AutoML systems demonstrate impressive
capabilities but face significant limitations such as constrained exploration
strategies and a severe execution bottleneck. Exploration is hindered by
one-shot methods lacking diversity and Monte Carlo Tree Search (MCTS)
approaches that fail to recombine strong partial solutions. The execution
bottleneck arises from lengthy code validation cycles that stifle iterative
refinement. To overcome these challenges, we introduce KompeteAI, a novel
AutoML framework with dynamic solution space exploration. Unlike previous MCTS
methods that treat ideas in isolation, KompeteAI introduces a merging stage
that composes top candidates. We further expand the hypothesis space by
integrating Retrieval-Augmented Generation (RAG), sourcing ideas from Kaggle
notebooks and arXiv papers to incorporate real-world strategies. KompeteAI also
addresses the execution bottleneck via a predictive scoring model and an
accelerated debugging method, assessing solution potential using early stage
metrics to avoid costly full-code execution. This approach accelerates pipeline
evaluation 6.9 times. KompeteAI outperforms leading methods (e.g., RD-agent,
AIDE, and Ml-Master) by an average of 3\% on the primary AutoML benchmark,
MLE-Bench. Additionally, we propose Kompete-bench to address limitations in
MLE-Bench, where KompeteAI also achieves state-of-the-art results

</details>


### [110] [Extending the Entropic Potential of Events for Uncertainty Quantification and Decision-Making in Artificial Intelligence](https://arxiv.org/abs/2508.10241)
*Mark Zilberman*

Main category: cs.AI

TL;DR: 本文介绍了熵势在人工智能中的应用，以提高不确定性量化、决策和可解释性。


<details>
  <summary>Details</summary>
Motivation: 演示了事件的熵势概念如何提高人工智能中的不确定性量化、决策和可解释性。

Method: 通过引入以事件为中心的度量来调整人工智能的熵势框架，该度量捕捉了动作、观察或其他离散事件如何影响未来时间范围的不确定性。

Result: 在策略评估、内在奖励设计、可解释人工智能和异常检测中探索了应用，突出了该指标在统一和加强智能系统中的不确定性建模方面的潜力。

Conclusion: 熵势框架为人工智能中的不确定性管理提供了一种理论基础、可解释且通用的方法，桥接了热力学、信息论和机器学习的原理。

Abstract: This work demonstrates how the concept of the entropic potential of events --
a parameter quantifying the influence of discrete events on the expected future
entropy of a system -- can enhance uncertainty quantification, decision-making,
and interpretability in artificial intelligence (AI). Building on its original
formulation in physics, the framework is adapted for AI by introducing an
event-centric measure that captures how actions, observations, or other
discrete occurrences impact uncertainty at future time horizons. Both the
original and AI-adjusted definitions of entropic potential are formalized, with
the latter emphasizing conditional expectations to account for counterfactual
scenarios. Applications are explored in policy evaluation, intrinsic reward
design, explainable AI, and anomaly detection, highlighting the metric's
potential to unify and strengthen uncertainty modeling in intelligent systems.
Conceptual examples illustrate its use in reinforcement learning, Bayesian
inference, and anomaly detection, while practical considerations for
computation in complex AI models are discussed. The entropic potential
framework offers a theoretically grounded, interpretable, and versatile
approach to managing uncertainty in AI, bridging principles from
thermodynamics, information theory, and machine learning.

</details>


### [111] [Why Cannot Large Language Models Ever Make True Correct Reasoning?](https://arxiv.org/abs/2508.10265)
*Jingde Cheng*

Main category: cs.AI

TL;DR: LLMs' understanding and reasoning abilities are illusions; they can never truly understand or reason.


<details>
  <summary>Details</summary>
Motivation: Many AI experts and non-professionals are trumpeting the understanding and reasoning abilities of LLMs, which the author believes are illusions.

Method: Explanation of the limitations of the working principle of LLMs.

Result: LLMs can never have true understanding and reasoning abilities.

Conclusion: LLMs can never have the ability of true correct reasoning due to the essential limitations of their working principle.

Abstract: Recently, with the application progress of AIGC tools based on large language
models (LLMs), led by ChatGPT, many AI experts and more non-professionals are
trumpeting the "understanding ability" and "reasoning ability" of the LLMs. The
present author considers that the so-called "understanding ability" and
"reasoning ability" of LLMs are just illusions of those people who with vague
concepts. In fact, the LLMs can never have the true understanding ability and
true reasoning ability. This paper intents to explain that, because the
essential limitations of their working principle, the LLMs can never have the
ability of true correct reasoning.

</details>


### [112] [Promoting Efficient Reasoning with Verifiable Stepwise Reward](https://arxiv.org/abs/2508.10293)
*Chuhuai Yue,Chengqi Dong,Yinan Gao,Hang He,Jiajun Chai,Guojun Yin,Wei Lin*

Main category: cs.AI

TL;DR: 提出了一种新的奖励机制，以减少大型推理模型中的过度思考，从而在效率和准确性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型 (LRM) 经常存在过度思考的问题，在简单的问题上花费过多的计算量，从而降低效率。现有的高效推理方法通常需要准确的任务评估来预设令牌预算或选择推理模式，这限制了它们的灵活性和可靠性。

Method: 提出了一种基于规则的可验证的逐步奖励机制 (VSRM)，该机制根据推理轨迹中中间状态的性能来分配奖励。

Result: 通过将 VSRM 与 PPO 和 Reinforce++ 相结合，在包括 AIME24 和 AIME25 在内的标准数学推理基准上进行了广泛的实验。结果表明，该方法在保持原始推理性能的同时，显着减少了输出长度，从而在效率和准确性之间取得了最佳平衡。

Conclusion: 该方法通过抑制无效步骤并鼓励有效推理，从根本上缓解了过度思考问题。

Abstract: Large reasoning models (LRMs) have recently achieved significant progress in
complex reasoning tasks, aided by reinforcement learning with verifiable
rewards. However, LRMs often suffer from overthinking, expending excessive
computation on simple problems and reducing efficiency. Existing efficient
reasoning methods typically require accurate task assessment to preset token
budgets or select reasoning modes, which limits their flexibility and
reliability. In this work, we revisit the essence of overthinking and identify
that encouraging effective steps while penalizing ineffective ones is key to
its solution. To this end, we propose a novel rule-based verifiable stepwise
reward mechanism (VSRM), which assigns rewards based on the performance of
intermediate states in the reasoning trajectory. This approach is intuitive and
naturally fits the step-by-step nature of reasoning tasks. We conduct extensive
experiments on standard mathematical reasoning benchmarks, including AIME24 and
AIME25, by integrating VSRM with PPO and Reinforce++. Results show that our
method achieves substantial output length reduction while maintaining original
reasoning performance, striking an optimal balance between efficiency and
accuracy. Further analysis of overthinking frequency and pass@k score before
and after training demonstrates that our approach in deed effectively
suppresses ineffective steps and encourages effective reasoning, fundamentally
alleviating the overthinking problem. All code will be released upon
acceptance.

</details>


### [113] [A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering](https://arxiv.org/abs/2508.10337)
*Chenliang Zhang,Lin Wang,Yuanyuan Lu,Yusheng Qi,Kexin Wang,Peixu Hou,Wenshi Chen*

Main category: cs.AI

TL;DR: The paper presents a system for multi-modal multi-turn question answering, using a vision LLM, curriculum learning, reinforcement learning, and web search, achieving top results in the META CRAG-MM challenge.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the META CRAG-MM challenge, which requires a retrieval-augmented generation system for multi-modal multi-turn question answering.

Method: The solution uses a vision large language model, fine-tuned with knowledge distilled from GPT-4.1, and enhanced with curriculum learning and reinforcement learning. Web search APIs were also used for external knowledge.

Result: The approach achieved 1st place in Task 1 with a 52.38% lead and 3rd place in Task 3.

Conclusion: The paper's approach, integrating curriculum learning with reinforcement learning, was effective, achieving 1st place in Task 1 and 3rd in Task 3.

Abstract: This paper describes the solutions of the Dianping-Trust-Safety team for the
META CRAG-MM challenge. The challenge requires building a comprehensive
retrieval-augmented generation system capable for multi-modal multi-turn
question answering. The competition consists of three tasks: (1) answering
questions using structured data retrieved from an image-based mock knowledge
graph, (2) synthesizing information from both knowledge graphs and web search
results, and (3) handling multi-turn conversations that require context
understanding and information aggregation from multiple sources. For Task 1,
our solution is based on the vision large language model, enhanced by
supervised fine-tuning with knowledge distilled from GPT-4.1. We further
applied curriculum learning strategies to guide reinforcement learning,
resulting in improved answer accuracy and reduced hallucination. For Task 2 and
Task 3, we additionally leveraged web search APIs to incorporate external
knowledge, enabling the system to better handle complex queries and multi-turn
conversations. Our approach achieved 1st place in Task 1 with a significant
lead of 52.38\%, and 3rd place in Task 3, demonstrating the effectiveness of
the integration of curriculum learning with reinforcement learning in our
training pipeline.

</details>


### [114] [Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach](https://arxiv.org/abs/2508.10340)
*Chak Lam Shek,Guangyao Shi,Pratap Tokekar*

Main category: cs.AI

TL;DR: 提出 HATRPO-W 和 HATRPO-G 两种方法，通过优化 KL 散度阈值分配，提升异构多智能体强化学习中的 HATRPO 性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习 (MARL) 需要交互智能体之间协调和稳定的策略更新。异构智能体信任区域策略优化 (HATRPO) 使用 Kullback-Leibler (KL) 散度强制执行每个智能体的信任区域约束，以稳定训练。但是，为每个智能体分配相同的 KL 阈值可能会导致缓慢且局部最优的更新，尤其是在异构设置中。

Method: 提出了两种分配 KL 散度阈值的方法：HATRPO-W，一种基于 Karush-Kuhn-Tucker (KKT) 的方法，可在全局 KL 约束下优化阈值分配；以及 HATRPO-G，一种贪婪算法，可根据改进与散度之比对代理进行优先级排序。

Result: 实验结果表明，该方法显著提高了 HATRPO 的性能，在不同的 MARL 基准测试中实现了更快的收敛和更高的最终奖励。具体而言，HATRPO-W 和 HATRPO-G 在最终性能上取得了可比的改进，均超过 22.5%。

Conclusion: HATRPO-W 和 HATRPO-G 显著提升了 HATRPO 的性能，在不同的 MARL 基准测试中实现了更快的收敛和更高的最终奖励。HATRPO-W 和 HATRPO-G 在最终性能上取得了可比的改进，均超过 22.5%。HATRPO-W 也表现出更稳定的学习动态，方差更小。

Abstract: Multi-agent reinforcement learning (MARL) requires coordinated and stable
policy updates among interacting agents. Heterogeneous-Agent Trust Region
Policy Optimization (HATRPO) enforces per-agent trust region constraints using
Kullback-Leibler (KL) divergence to stabilize training. However, assigning each
agent the same KL threshold can lead to slow and locally optimal updates,
especially in heterogeneous settings. To address this limitation, we propose
two approaches for allocating the KL divergence threshold across agents:
HATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes
threshold assignment under global KL constraints, and HATRPO-G, a greedy
algorithm that prioritizes agents based on improvement-to-divergence ratio. By
connecting sequential policy optimization with constrained threshold
scheduling, our approach enables more flexible and effective learning in
heterogeneous-agent settings. Experimental results demonstrate that our methods
significantly boost the performance of HATRPO, achieving faster convergence and
higher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and
HATRPO-G achieve comparable improvements in final performance, each exceeding
22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as
reflected by its lower variance.

</details>


### [115] [STEP: Stepwise Curriculum Learning for Context-Knowledge Fusion in Conversational Recommendation](https://arxiv.org/abs/2508.10669)
*Zhenye Yang,Jinpeng Chen,Huan Li,Xiongnan Jin,Xuanyang Li,Junwei Zhang,Hongbo Gao,Kaimin Wei,Senzhang Wang*

Main category: cs.AI

TL;DR: STEP is a conversational recommender system that uses a pre-trained language model and curriculum-guided context-knowledge fusion to improve recommendation and dialogue quality.


<details>
  <summary>Details</summary>
Motivation: existing CRS faces challenges in capturing the deep semantics of user preferences and dialogue context. In particular, the efficient integration of external knowledge graph (KG) information into dialogue generation and recommendation remains a pressing issue. Traditional approaches typically combine KG information directly with dialogue content, which often struggles with complex semantic relationships, resulting in recommendations that may not align with user expectations.

Method: a conversational recommender centered on pre-trained language models that combines curriculum-guided context-knowledge fusion with lightweight task-specific prompt tuning. At its heart, an F-Former progressively aligns the dialogue context with knowledge-graph entities through a three-stage curriculum, thus resolving fine-grained semantic mismatches. The fused representation is then injected into the frozen language model via two minimal yet adaptive prefix prompts: a conversation prefix that steers response generation toward user intent and a recommendation prefix that biases item ranking toward knowledge-consistent candidates.

Result: STEP outperforms mainstream methods in the precision of recommendation and dialogue quality in two public datasets.

Conclusion: STEP outperforms mainstream methods in the precision of recommendation and dialogue quality in two public datasets.

Abstract: Conversational recommender systems (CRSs) aim to proactively capture user
preferences through natural language dialogue and recommend high-quality items.
To achieve this, CRS gathers user preferences via a dialog module and builds
user profiles through a recommendation module to generate appropriate
recommendations. However, existing CRS faces challenges in capturing the deep
semantics of user preferences and dialogue context. In particular, the
efficient integration of external knowledge graph (KG) information into
dialogue generation and recommendation remains a pressing issue. Traditional
approaches typically combine KG information directly with dialogue content,
which often struggles with complex semantic relationships, resulting in
recommendations that may not align with user expectations.
  To address these challenges, we introduce STEP, a conversational recommender
centered on pre-trained language models that combines curriculum-guided
context-knowledge fusion with lightweight task-specific prompt tuning. At its
heart, an F-Former progressively aligns the dialogue context with
knowledge-graph entities through a three-stage curriculum, thus resolving
fine-grained semantic mismatches. The fused representation is then injected
into the frozen language model via two minimal yet adaptive prefix prompts: a
conversation prefix that steers response generation toward user intent and a
recommendation prefix that biases item ranking toward knowledge-consistent
candidates. This dual-prompt scheme allows the model to share cross-task
semantics while respecting the distinct objectives of dialogue and
recommendation. Experimental results show that STEP outperforms mainstream
methods in the precision of recommendation and dialogue quality in two public
datasets.

</details>


### [116] [What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles](https://arxiv.org/abs/2508.10358)
*Mengtao Zhou,Sifan Wu,Huan Zhang,Qi Sima,Bang Liu*

Main category: cs.AI

TL;DR: This paper introduces a new benchmark and agent for evaluating the imaginative reasoning capabilities of LLMs, finding that they still lag behind humans in this area.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks, often static or focused on social deduction, fail to capture the dynamic, exploratory nature of this reasoning process. To address this gap, we introduce a comprehensive research framework based on the classic "Turtle Soup" game

Method: We introduce a comprehensive research framework based on the classic "Turtle Soup" game, integrating a benchmark, an agent, and an evaluation protocol. We present TurtleSoup-Bench, the first large-scale, bilingual, interactive benchmark for imaginative reasoning, comprising 800 turtle soup puzzles sourced from both the Internet and expert authors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs' performance in this setting. To evaluate reasoning quality, we develop a multi-dimensional protocol measuring logical consistency, detail completion, and conclusion alignment.

Result: Experiments with leading LLMs reveal clear capability limits, common failure patterns, and a significant performance gap compared to humans.

Conclusion: Experiments with leading LLMs reveal clear capability limits, common failure patterns, and a significant performance gap compared to humans. Our work offers new insights into LLMs' imaginative reasoning and establishes a foundation for future research on exploratory agent behavior.

Abstract: We investigate the capacity of Large Language Models (LLMs) for imaginative
reasoning--the proactive construction, testing, and revision of hypotheses in
information-sparse environments. Existing benchmarks, often static or focused
on social deduction, fail to capture the dynamic, exploratory nature of this
reasoning process. To address this gap, we introduce a comprehensive research
framework based on the classic "Turtle Soup" game, integrating a benchmark, an
agent, and an evaluation protocol. We present TurtleSoup-Bench, the first
large-scale, bilingual, interactive benchmark for imaginative reasoning,
comprising 800 turtle soup puzzles sourced from both the Internet and expert
authors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs'
performance in this setting. To evaluate reasoning quality, we develop a
multi-dimensional protocol measuring logical consistency, detail completion,
and conclusion alignment. Experiments with leading LLMs reveal clear capability
limits, common failure patterns, and a significant performance gap compared to
humans. Our work offers new insights into LLMs' imaginative reasoning and
establishes a foundation for future research on exploratory agent behavior.

</details>


### [117] [LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval](https://arxiv.org/abs/2508.10391)
*Yaoze Zhang,Rong Wu,Pinlong Cai,Xiaoman Wang,Guohang Yan,Song Mao,Ding Wang,Botian Shi*

Main category: cs.AI

TL;DR: LeanRAG通过结合知识聚合和检索策略，解决了现有知识图谱RAG方法的局限性，实现了更高的响应质量和更低的检索冗余。


<details>
  <summary>Details</summary>
Motivation: 知识图谱的RAG方法仍然存在两个关键的、未解决的挑战：高层次的概念总结作为不相连的“语义岛”而存在，缺乏跨社区推理所需的显式关系；检索过程本身在结构上仍然没有意识到，通常会退化为低效的平面搜索，而无法利用图的丰富拓扑。

Method: LeanRAG采用了一种新颖的语义聚合算法，该算法形成实体集群并在聚合级别摘要之间构建新的显式关系，从而创建一个完全可导航的语义网络。然后，一种自下而上的结构引导检索策略将查询锚定到最相关的细粒度实体，然后系统地遍历图的语义路径，以收集简洁但上下文全面的证据集。

Result: LeanRAG可以减轻与图上的路径检索相关的巨大开销，并最大限度地减少冗余信息检索。在具有不同领域的四个具有挑战性的QA基准上进行的大量实验表明，LeanRAG在响应质量方面显著优于现有方法，同时减少了46%的检索冗余。

Conclusion: LeanRAG在响应质量方面显著优于现有方法，同时减少了46%的检索冗余。

Abstract: Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large
Language Models by leveraging external knowledge, whereas the effectiveness is
often compromised by the retrieval of contextually flawed or incomplete
information. To address this, knowledge graph-based RAG methods have evolved
towards hierarchical structures, organizing knowledge into multi-level
summaries. However, these approaches still suffer from two critical,
unaddressed challenges: high-level conceptual summaries exist as disconnected
``semantic islands'', lacking the explicit relations needed for cross-community
reasoning; and the retrieval process itself remains structurally unaware, often
degenerating into an inefficient flat search that fails to exploit the graph's
rich topology. To overcome these limitations, we introduce LeanRAG, a framework
that features a deeply collaborative design combining knowledge aggregation and
retrieval strategies. LeanRAG first employs a novel semantic aggregation
algorithm that forms entity clusters and constructs new explicit relations
among aggregation-level summaries, creating a fully navigable semantic network.
Then, a bottom-up, structure-guided retrieval strategy anchors queries to the
most relevant fine-grained entities and then systematically traverses the
graph's semantic pathways to gather concise yet contextually comprehensive
evidence sets. The LeanRAG can mitigate the substantial overhead associated
with path retrieval on graphs and minimizes redundant information retrieval.
Extensive experiments on four challenging QA benchmarks with different domains
demonstrate that LeanRAG significantly outperforming existing methods in
response quality while reducing 46\% retrieval redundancy. Code is available
at: https://github.com/RaZzzyz/LeanRAG

</details>


### [118] [HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation](https://arxiv.org/abs/2508.10425)
*Yan Ting Chok,Soyon Park,Seungheun Baek,Hajung Kim,Junhyun Lee,Jaewoo Kang*

Main category: cs.AI

TL;DR: HiRef结合了分层本体和细化的共现模式，以提高药物推荐的鲁棒性，并在EHR基准测试中取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 由于存在很少观察到的医学实体和可能无法完全捕获临床基本事实的不完整记录，真实世界的EHR数据提出了重大挑战。虽然在纵向电子健康记录上训练的数据驱动模型通常能获得强大的经验性能，但由于它们依赖于观察到的共现模式，因此在缺失或新条件下难以推广。

Method: 我们提出了用于稳健药物推荐的分层本体和网络细化(HiRef)，这是一个统一的框架，它结合了两种互补的结构：(i)编码在精选医学本体中的分层语义，以及(ii)从真实世界EHRs中提取的精炼的共现模式。

Result: 广泛的实验和全面的消融研究表明，HiRef对未见过的医学代码具有弹性，这得到了对学习到的稀疏图结构和医学代码嵌入的深入分析的支持。

Conclusion: HiRef在EHR基准测试（MIMIC-III和MIMIC-IV）上取得了优异的性能，并在模拟的未见代码设置下保持了较高的准确率。

Abstract: Medication recommendation is a crucial task for assisting physicians in
making timely decisions from longitudinal patient medical records. However,
real-world EHR data present significant challenges due to the presence of
rarely observed medical entities and incomplete records that may not fully
capture the clinical ground truth. While data-driven models trained on
longitudinal Electronic Health Records often achieve strong empirical
performance, they struggle to generalize under missing or novel conditions,
largely due to their reliance on observed co-occurrence patterns. To address
these issues, we propose Hierarchical Ontology and Network Refinement for
Robust Medication Recommendation (HiRef), a unified framework that combines two
complementary structures: (i) the hierarchical semantics encoded in curated
medical ontologies, and (ii) refined co-occurrence patterns derived from
real-world EHRs. We embed ontology entities in hyperbolic space, which
naturally captures tree-like relationships and enables knowledge transfer
through shared ancestors, thereby improving generalizability to unseen codes.
To further improve robustness, we introduce a prior-guided sparse
regularization scheme that refines the EHR co-occurrence graph by suppressing
spurious edges while preserving clinically meaningful associations. Our model
achieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and
maintains high accuracy under simulated unseen-code settings. Extensive
experiments with comprehensive ablation studies demonstrate HiRef's resilience
to unseen medical codes, supported by in-depth analyses of the learned
sparsified graph structure and medical code embeddings.

</details>


### [119] [MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance](https://arxiv.org/abs/2508.10429)
*Yi Dong,Yusuke Muraoka,Scott Shi,Yi Zhang*

Main category: cs.AI

TL;DR: MM-Food-100K是一个包含10万个样本的多模态食物数据集，用于提升图像营养预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 为了创建一个具有可验证来源的大型多模态食物智能数据集。

Method: 使用Codatta贡献模型，结合社区采购和可配置的AI辅助质量检查来收集和注释食物图像。

Result: 在MM-Food-100K子集上，微调后的模型在标准指标上优于原始基线模型。

Conclusion: MM-Food-100K数据集被发布，并且在图像营养预测方面对大型视觉语言模型进行了微调，结果显示一致的收益。

Abstract: We present MM-Food-100K, a public 100,000-sample multimodal food intelligence
dataset with verifiable provenance. It is a curated approximately 10% open
subset of an original 1.2 million, quality-accepted corpus of food images
annotated for a wide range of information (such as dish name, region of
creation). The corpus was collected over six weeks from over 87,000
contributors using the Codatta contribution model, which combines community
sourcing with configurable AI-assisted quality checks; each submission is
linked to a wallet address in a secure off-chain ledger for traceability, with
a full on-chain protocol on the roadmap. We describe the schema, pipeline, and
QA, and validate utility by fine-tuning large vision-language models (ChatGPT
5, ChatGPT OSS, Qwen-Max) on image-based nutrition prediction. Fine-tuning
yields consistent gains over out-of-box baselines across standard metrics; we
report results primarily on the MM-Food-100K subset. We release MM-Food-100K
for publicly free access and retain approximately 90% for potential commercial
access with revenue sharing to contributors.

</details>


### [120] [We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning](https://arxiv.org/abs/2508.10433)
*Runqi Qiao,Qiuna Tan,Peiqing Yang,Yanzi Wang,Xiaowan Wang,Enhui Wan,Sitong Zhou,Guanting Dong,Yuchen Zeng,Yida Xu,Jie Wang,Chong Sun,Chen Li,Honggang Zhang*

Main category: cs.AI

TL;DR: We-Math 2.0 enhances MLLMs' mathematical reasoning via knowledge integration, data space modeling, and RL-based training, achieving strong results on MathBookEval.


<details>
  <summary>Details</summary>
Motivation: Existing research primarily focuses on dataset construction and method optimization, often overlooking two critical aspects: comprehensive knowledge-driven design and model-centric data space modeling.

Method: a unified system that integrates a structured mathematical knowledge system, model-centric data space modeling, and a reinforcement learning (RL)-based training paradigm

Result: We introduce We-Math 2.0, a unified system that integrates a structured mathematical knowledge system, model-centric data space modeling, and a reinforcement learning (RL)-based training paradigm to comprehensively enhance the mathematical reasoning abilities of MLLMs. The key contributions of We-Math 2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level hierarchical system encompassing 491 knowledge points and 1,819 fundamental principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a dataset that ensures broad conceptual coverage and flexibility through dual expansion. Additionally, we define a three-dimensional difficulty space and generate 7 progressive variants per problem to build MathBook-Pro, a challenging dataset for robust training. (3) MathBook-RL: We propose a two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive Alignment RL, leveraging average-reward learning and dynamic data scheduling to achieve progressive alignment across difficulty levels. (4) MathBookEval: We introduce a comprehensive benchmark covering all 491 knowledge points with diverse reasoning step distributions.

Conclusion: MathBook-RL performs competitively with existing baselines on four widely-used benchmarks and achieves strong results on MathBookEval, suggesting promising generalization in mathematical reasoning.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across various tasks, but still struggle with complex mathematical
reasoning. Existing research primarily focuses on dataset construction and
method optimization, often overlooking two critical aspects: comprehensive
knowledge-driven design and model-centric data space modeling. In this paper,
we introduce We-Math 2.0, a unified system that integrates a structured
mathematical knowledge system, model-centric data space modeling, and a
reinforcement learning (RL)-based training paradigm to comprehensively enhance
the mathematical reasoning abilities of MLLMs. The key contributions of We-Math
2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level
hierarchical system encompassing 491 knowledge points and 1,819 fundamental
principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a
dataset that ensures broad conceptual coverage and flexibility through dual
expansion. Additionally, we define a three-dimensional difficulty space and
generate 7 progressive variants per problem to build MathBook-Pro, a
challenging dataset for robust training. (3) MathBook-RL: We propose a
two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the
model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive
Alignment RL, leveraging average-reward learning and dynamic data scheduling to
achieve progressive alignment across difficulty levels. (4) MathBookEval: We
introduce a comprehensive benchmark covering all 491 knowledge points with
diverse reasoning step distributions. Experimental results show that
MathBook-RL performs competitively with existing baselines on four widely-used
benchmarks and achieves strong results on MathBookEval, suggesting promising
generalization in mathematical reasoning.

</details>


### [121] [FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs](https://arxiv.org/abs/2508.10467)
*Xueli Pan,Victor de Boer,Jacco van Ossenbruggen*

Main category: cs.AI

TL;DR: FIRESPARQL 是一个用于学术知识图问答的模块化框架，它使用微调的 LLM 和可选的 RAG 和 SPARQL 查询纠正来提高准确性。


<details>
  <summary>Details</summary>
Motivation: 由于学术内容的复杂性和这些图的复杂结构，基于学术知识图 (SKG) 的问答仍然是一项具有挑战性的任务。大型语言模型 (LLM) 方法可用于将自然语言问题 (NLQ) 转换为 SPARQL 查询；然而，由于接触 SKG 特定内容和底层模式的限制，这些基于 LLM 的方法在 SPARQL 查询生成方面存在困难。我们确定了 LLM 生成的 SPARQL 查询中的两种主要类型的错误：(i) 结构不一致，例如查询中缺少或冗余的三元组，以及 (ii) 语义不准确，其中尽管查询结构正确，但查询中显示了不正确的实体或属性。

Method: FIRESPARQL，一个模块化框架，支持微调的 LLM 作为核心组件，可选的上下文通过检索增强生成 (RAG) 和 SPARQL 查询纠正层提供。

Result: 实验结果表明，微调实现了最高的整体性能，在测试集上查询准确率达到 0.90 ROUGE-L，结果准确率达到 0.85 RelaxedEM。

Conclusion: 微调实现了最高的整体性能，在测试集上查询准确率达到 0.90 ROUGE-L，结果准确率达到 0.85 RelaxedEM。

Abstract: Question answering over Scholarly Knowledge Graphs (SKGs) remains a
challenging task due to the complexity of scholarly content and the intricate
structure of these graphs. Large Language Model (LLM) approaches could be used
to translate natural language questions (NLQs) into SPARQL queries; however,
these LLM-based approaches struggle with SPARQL query generation due to limited
exposure to SKG-specific content and the underlying schema. We identified two
main types of errors in the LLM-generated SPARQL queries: (i) structural
inconsistencies, such as missing or redundant triples in the queries, and (ii)
semantic inaccuracies, where incorrect entities or properties are shown in the
queries despite a correct query structure. To address these issues, we propose
FIRESPARQL, a modular framework that supports fine-tuned LLMs as a core
component, with optional context provided via retrieval-augmented generation
(RAG) and a SPARQL query correction layer. We evaluate the framework on the
SciQA Benchmark using various configurations (zero-shot, zero-shot with RAG,
one-shot, fine-tuning, and fine-tuning with RAG) and compare the performance
with baseline and state-of-the-art approaches. We measure query accuracy using
BLEU and ROUGE metrics, and query result accuracy using relaxed exact
match(RelaxedEM), with respect to the gold standards containing the NLQs,
SPARQL queries, and the results of the queries. Experimental results
demonstrate that fine-tuning achieves the highest overall performance, reaching
0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the
test set.

</details>


### [122] [SEQ-GPT: LLM-assisted Spatial Query via Example](https://arxiv.org/abs/2508.10486)
*Ivan Khai Ze Lim,Ningyi Liao,Yiming Yang,Gerald Wei Yong Yip,Siqiang Luo*

Main category: cs.AI

TL;DR: 本文提出了一种名为SEQ-GPT的空间查询系统，该系统利用大型语言模型(llm)实现更通用的空间范例查询(SEQ)搜索，并通过定制的LLM适配管道将自然语言与结构化空间数据对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的在线地图等空间服务主要依赖于用户查询进行位置搜索，但在执行复杂任务(如同时搜索一组位置)时，用户体验受到限制。

Method: 提出了一个定制的LLM适配管道，通过对话合成和多模型合作，将自然语言与结构化空间数据和查询对齐。

Result: llm的语言能力支持在SEQ过程中进行独特的交互操作，包括要求用户澄清查询细节，并根据用户反馈动态调整搜索。

Conclusion: SEQ-GPT通过利用大型语言模型(llm)的语言能力，实现了更通用的空间范例查询(SEQ)搜索，并提供了一个端到端的演示，以扩展具有实际数据和应用场景的空间搜索。

Abstract: Contemporary spatial services such as online maps predominantly rely on user
queries for location searches. However, the user experience is limited when
performing complex tasks, such as searching for a group of locations
simultaneously. In this study, we examine the extended scenario known as
Spatial Exemplar Query (SEQ), where multiple relevant locations are jointly
searched based on user-specified examples. We introduce SEQ-GPT, a spatial
query system powered by Large Language Models (LLMs) towards more versatile SEQ
search using natural language. The language capabilities of LLMs enable unique
interactive operations in the SEQ process, including asking users to clarify
query details and dynamically adjusting the search based on user feedback. We
also propose a tailored LLM adaptation pipeline that aligns natural language
with structured spatial data and queries through dialogue synthesis and
multi-model cooperation. SEQ-GPT offers an end-to-end demonstration for
broadening spatial search with realistic data and application scenarios.

</details>


### [123] [Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model](https://arxiv.org/abs/2508.10492)
*Shicheng Xu,Xin Huang,Zihao Wei,Liang Pang,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: DxDirector-7B是一种新的AI诊断系统，它作为主要驱动者，医生作为辅助，能够更准确、高效地完成诊断。


<details>
  <summary>Details</summary>
Motivation: 现有AI在临床诊断中主要作为辅助工具，无法驱动从模糊主诉开始的全流程诊断，限制了其降低医生工作量和提高诊断效率的能力。

Method: 提出了DxDirector-7B，一个具有深度思考能力的大语言模型，能够驱动全流程诊断，并建立了强大的误诊责任框架。

Result: DxDirector-7B在罕见、复杂和真实世界病例的评估中，诊断准确率显著优于现有医学大语言模型和通用大语言模型，并能大幅减少医生工作量。

Conclusion: DxDirector-7B在全流程诊断中表现出色，能显著提高诊断准确率并减轻医生工作量，有潜力替代医学专家。

Abstract: Full-process clinical diagnosis in the real world encompasses the entire
diagnostic workflow that begins with only an ambiguous chief complaint. While
artificial intelligence (AI), particularly large language models (LLMs), is
transforming clinical diagnosis, its role remains largely as an assistant to
physicians. This AI-assisted working pattern makes AI can only answer specific
medical questions at certain parts within the diagnostic process, but lack the
ability to drive the entire diagnostic process starting from an ambiguous
complaint, which still relies heavily on human physicians. This gap limits AI's
ability to fully reduce physicians' workload and enhance diagnostic efficiency.
To address this, we propose a paradigm shift that reverses the relationship
between physicians and AI: repositioning AI as the primary director, with
physicians serving as its assistants. So we present DxDirector-7B, an LLM
endowed with advanced deep thinking capabilities, enabling it to drive the
full-process diagnosis with minimal physician involvement. Furthermore,
DxDirector-7B establishes a robust accountability framework for misdiagnoses,
delineating responsibility between AI and human physicians. In evaluations
across rare, complex, and real-world cases under full-process diagnosis
setting, DxDirector-7B not only achieves significant superior diagnostic
accuracy but also substantially reduces physician workload than
state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained
analyses across multiple clinical departments and tasks validate its efficacy,
with expert evaluations indicating its potential to serve as a viable
substitute for medical specialists. These findings mark a new era where AI,
traditionally a physicians' assistant, now drives the entire diagnostic process
to drastically reduce physicians' workload, indicating an efficient and
accurate diagnostic solution.

</details>


### [124] [PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning](https://arxiv.org/abs/2508.10501)
*Yushi Feng,Junye Du,Yingying Hong,Qifan Wang,Lequan Yu*

Main category: cs.AI

TL;DR: PASS is a multimodal framework that addresses the limitations of existing tool-augmented agentic systems in Chest X-Ray reasoning by adaptively sampling agentic workflows, offering interpretable probabilities, and balancing performance with computational costs. It introduces a new benchmark, CAB-E, and outperforms existing baselines.


<details>
  <summary>Details</summary>
Motivation: Existing tool-augmented agentic systems are limited in the real world by black-box reasoning steps, poor multimodal integration, and rigid and computationally inefficient agentic pipelines, especially in healthcare tasks like Chest X-Ray (CXR) reasoning.

Method: PASS adaptively samples agentic workflows over a multi-tool graph, yielding decision paths annotated with interpretable probabilities. It also continuously compresses salient findings into an evolving personalized memory, while dynamically deciding whether to deepen its reasoning path or invoke an early exit for efficiency. A novel three-stage training procedure, including expert knowledge warm-up, contrastive path-ranking, and cost-aware reinforcement learning is designed to optimize performance and cost.

Result: PASS outperforms strong baselines in accuracy, AUC, and LLM-J while balancing computational costs. CAB-E, a comprehensive benchmark for multi-step, safety-critical, free-form CXR reasoning is introduced for rigorous evaluation.

Conclusion: PASS significantly outperforms strong baselines in multiple metrics while balancing computational costs, pushing a new paradigm shift towards interpretable, adaptive, and multimodal medical agentic systems.

Abstract: Existing tool-augmented agentic systems are limited in the real world by (i)
black-box reasoning steps that undermine trust of decision-making and pose
safety risks, (ii) poor multimodal integration, which is inherently critical
for healthcare tasks, and (iii) rigid and computationally inefficient agentic
pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the
first multimodal framework to address these challenges in the context of Chest
X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a
multi-tool graph, yielding decision paths annotated with interpretable
probabilities. Given the complex CXR reasoning task with multimodal medical
data, PASS leverages its learned task-conditioned distribution over the agentic
supernet. Thus, it adaptively selects the most suitable tool at each supernet
layer, offering probability-annotated trajectories for post-hoc audits and
directly enhancing medical AI safety. PASS also continuously compresses salient
findings into an evolving personalized memory, while dynamically deciding
whether to deepen its reasoning path or invoke an early exit for efficiency. To
optimize a Pareto frontier balancing performance and cost, we design a novel
three-stage training procedure, including expert knowledge warm-up, contrastive
path-ranking, and cost-aware reinforcement learning. To facilitate rigorous
evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step,
safety-critical, free-form CXR reasoning. Experiments across various benchmarks
validate that PASS significantly outperforms strong baselines in multiple
metrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs,
pushing a new paradigm shift towards interpretable, adaptive, and multimodal
medical agentic systems.

</details>


### [125] [Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment](https://arxiv.org/abs/2508.10530)
*Zetian Sun,Dongfang Li,Baotian Hu*

Main category: cs.AI

TL;DR: The paper introduces the alignment stage assumption to explain the effectiveness difference between static and on-policy preference candidates in language model alignment, and proposes an algorithm to identify the boundaries between the preference injection stage and the preference fine-tuning stage.


<details>
  <summary>Details</summary>
Motivation: On-policy data is not always optimal, with systematic effectiveness difference emerging between static and on-policy preference candidates.

Method: Theoretical and empirical analysis to characterize the preference injection stage and the preference fine-tuning stage, and propose an effective algorithm to identify the boundaries between them.

Result: On-policy data can result in a 3$\times$ effectiveness compared with static data for Llama-3, and a 0.4$\times$ effectiveness for Zephyr. Experiments on 5 models (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO, SLiC-HF) show the generalizability of alignment stage assumption and boundary measurement.

Conclusion: The alignment stage assumption and boundary measurement are generalizable across different models and alignment methods.

Abstract: The alignment of language models (LMs) with human preferences is critical for
building reliable AI systems. The problem is typically framed as optimizing an
LM policy to maximize the expected reward that reflects human preferences.
Recently, Direct Preference Optimization (DPO) was proposed as a LM alignment
method that directly optimize the policy from static preference data, and
further improved by incorporating on-policy sampling (i.e., preference
candidates generated during the training loop) for better LM alignment.
However, we show on-policy data is not always optimal, with systematic
effectiveness difference emerging between static and on-policy preference
candidates. For example, on-policy data can result in a 3$\times$ effectiveness
compared with static data for Llama-3, and a 0.4$\times$ effectiveness for
Zephyr. To explain the phenomenon, we propose the alignment stage assumption,
which divides the alignment process into two distinct stages: the preference
injection stage, which benefits from diverse data, and the preference
fine-tuning stage, which favors high-quality data. Through theoretical and
empirical analysis, we characterize these stages and propose an effective
algorithm to identify the boundaries between them. We perform experiments on 5
models (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO,
SLiC-HF) to show the generalizability of alignment stage assumption and
boundary measurement.

</details>


### [126] [Improving Value-based Process Verifier via Low-Cost Variance Reduction](https://arxiv.org/abs/2508.10539)
*Zetian Sun,Dongfang Li,Baotian Hu,Min Zhang*

Main category: cs.AI

TL;DR: This paper introduces ComMCS, a method to reduce variance in LLM reasoning, improving performance on math problems without extra cost.


<details>
  <summary>Details</summary>
Motivation: Reasoning capabilities of LLMs in complex domains like mathematics remain a challenge. Value-based process verifiers are promising but suffer from estimation error due to the limited number of Monte Carlo samples.

Method: The paper proposes Compound Monte Carlo Sampling (ComMCS), which constructs an unbiased estimator by linearly combining MC estimators from current and subsequent steps.

Result: ComMCS outperforms regression-based optimization method by 2.8 points, the non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32 sampling experiment.

Conclusion: The paper introduces ComMCS, a novel method that reduces variance in value-based process verifiers without additional LLM inference cost. Empirical results on MATH-500 and GSM8K benchmarks demonstrate the effectiveness of ComMCS, outperforming existing methods.

Abstract: Large language models (LLMs) have achieved remarkable success in a wide range
of tasks. However, their reasoning capabilities, particularly in complex
domains like mathematics, remain a significant challenge. Value-based process
verifiers, which estimate the probability of a partial reasoning chain leading
to a correct solution, are a promising approach for improving reasoning.
Nevertheless, their effectiveness is often hindered by estimation error in
their training annotations, a consequence of the limited number of Monte Carlo
(MC) samples feasible due to the high cost of LLM inference. In this paper, we
identify that the estimation error primarily arises from high variance rather
than bias, and the MC estimator is a Minimum Variance Unbiased Estimator
(MVUE). To address the problem, we propose the \textsc{Com}pound \textsc{M}onte
\textsc{C}arlo \textsc{S}ampling (ComMCS) method, which constructs an unbiased
estimator by linearly combining the MC estimators from the current and
subsequent steps. Theoretically, we show that our method leads to a predictable
reduction in variance, while maintaining an unbiased estimation without
additional LLM inference cost. We also perform empirical experiments on the
MATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.
Notably, ComMCS outperforms regression-based optimization method by 2.8 points,
the non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32
sampling experiment.

</details>


### [127] [MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models](https://arxiv.org/abs/2508.10599)
*Xinyan Jiang,Lin Zhang,Jiayi Zhang,Qingsong Yang,Guimin Hu,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: MSRS通过子空间表示微调，减少属性冲突，并在各种任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以共同steering多个属性，经常导致干扰和不希望的权衡。

Method: 通过子空间表示微调实现有效的多属性steering。

Result: MSRS通过为每个属性分配正交子空间来减少属性间的干扰，将它们的影响隔离在模型的表示空间内。

Conclusion: MSRS显著减少了属性冲突，在各种属性上超过了现有方法，并且有效地推广到各种下游任务。

Abstract: Activation steering offers a promising approach to controlling the behavior
of Large Language Models by directly manipulating their internal activations.
However, most existing methods struggle to jointly steer multiple attributes,
often resulting in interference and undesirable trade-offs. To address this
challenge, we propose Multi-Subspace Representation Steering (MSRS), a novel
framework for effective multi-attribute steering via subspace representation
fine-tuning. MSRS reduces inter-attribute interference by allocating orthogonal
subspaces to each attribute, isolating their influence within the model's
representation space. MSRS also incorporates a hybrid subspace composition
strategy: it combines attribute-specific subspaces for unique steering
directions with a shared subspace for common steering directions. A dynamic
weighting function learns to efficiently integrate these components for precise
control. During inference, MSRS introduces a token-level steering mechanism
that dynamically identifies and intervenes on the most semantically relevant
tokens, enabling fine-grained behavioral modulation. Experimental results show
that MSRS significantly reduces attribute conflicts, surpasses existing methods
across a range of attributes, and generalizes effectively to diverse downstream
tasks.

</details>


### [128] [GenOM: Ontology Matching with Description Generation and Large Language Model](https://arxiv.org/abs/2508.10703)
*Yiping Song,Jiaoyan Chen,Renate A. Schmidt*

Main category: cs.AI

TL;DR: GenOM是一个基于大型语言模型的本体对齐框架，通过语义丰富和精确匹配来提高本体匹配的性能，并在生物医学领域表现出色。


<details>
  <summary>Details</summary>
Motivation: 本体匹配在实现跨异构知识源的语义互操作性和集成方面起着至关重要的作用，特别是在生物医学领域，该领域包含许多与疾病和药物相关的复杂概念。

Method: GenOM框架通过生成文本定义来丰富本体概念的语义表示，使用嵌入模型检索对齐候选，并结合基于精确匹配的工具来提高精度。

Result: GenOM通常可以实现有竞争力的性能，超过许多基线。

Conclusion: GenOM在OAEI Bio-ML track上表现出色，超越了许多基线方法，包括传统OM系统和最新的基于LLM的方法。消融研究证实了语义丰富和少样本提示的有效性，突出了该框架的鲁棒性和适应性。

Abstract: Ontology matching (OM) plays an essential role in enabling semantic
interoperability and integration across heterogeneous knowledge sources,
particularly in the biomedical domain which contains numerous complex concepts
related to diseases and pharmaceuticals. This paper introduces GenOM, a large
language model (LLM)-based ontology alignment framework, which enriches the
semantic representations of ontology concepts via generating textual
definitions, retrieves alignment candidates with an embedding model, and
incorporates exact matching-based tools to improve precision. Extensive
experiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often
achieve competitive performance, surpassing many baselines including
traditional OM systems and recent LLM-based methods. Further ablation studies
confirm the effectiveness of semantic enrichment and few-shot prompting,
highlighting the framework's robustness and adaptability.

</details>


### [129] [Agentic Design Review System](https://arxiv.org/abs/2508.10745)
*Sayan Nag,K J Joseph,Koustava Goswami,Vlad I Morariu,Balaji Vasan Srinivasan*

Main category: cs.AI

TL;DR: 我们提出了一个 Agentic 设计评审系统 (AgenticDRS)，其中多个代理协同分析一个设计，由一个元代理协调。


<details>
  <summary>Details</summary>
Motivation: 从对齐、构图、美学和颜色选择等多个方面评估图形设计。

Method: 基于图匹配的新型上下文范例选择方法和独特的提示扩展方法。

Result: 针对适应问题设置的最新基线进行彻底的实验评估，并辅以关键的消融实验，证明了 Agentic-DRS 的有效性。

Conclusion: Agentic-DRS 在评估图形设计和生成可操作的反馈方面是有效的。

Abstract: Evaluating graphic designs involves assessing it from multiple facets like
alignment, composition, aesthetics and color choices. Evaluating designs in a
holistic way involves aggregating feedback from individual expert reviewers.
Towards this, we propose an Agentic Design Review System (AgenticDRS), where
multiple agents collaboratively analyze a design, orchestrated by a meta-agent.
A novel in-context exemplar selection approach based on graph matching and a
unique prompt expansion method plays central role towards making each agent
design aware. Towards evaluating this framework, we propose DRS-BENCH
benchmark. Thorough experimental evaluation against state-of-the-art baselines
adapted to the problem setup, backed-up with critical ablation experiments
brings out the efficacy of Agentic-DRS in evaluating graphic designs and
generating actionable feedback. We hope that this work will attract attention
to this pragmatic, yet under-explored research direction.

</details>


### [130] [Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning](https://arxiv.org/abs/2508.10747)
*Sangwoo Jeon,Juchul Shin,Gyeong-Tae Kim,YeonJe Cho,Seongwoo Kim*

Main category: cs.AI

TL;DR: 提出了一种稀疏的、目标感知的GNN表示，选择性地编码相关的局部关系，并显式地整合与目标相关的空间特征，以解决现有方法在大型基于网格的环境中边缘信息的组合爆炸和大量的稀疏性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常将规划状态表示为完全连接的图，导致边缘信息的组合爆炸和大量的稀疏性，尤其是在大型基于网格的环境中。

Method: 提出了一种稀疏的、目标感知的GNN表示，选择性地编码相关的局部关系，并显式地整合与目标相关的空间特征。

Result: 该方法能够有效扩展到以前使用密集图表示无法实现的大型网格尺寸，并显著提高策略泛化和成功率。

Conclusion: 该方法能够有效扩展到以前使用密集图表示无法实现的大型网格尺寸，并显著提高策略泛化和成功率。为解决实际的大规模通用规划任务奠定了实践基础。

Abstract: Generalized planning using deep reinforcement learning (RL) combined with
graph neural networks (GNNs) has shown promising results in various symbolic
planning domains described by PDDL. However, existing approaches typically
represent planning states as fully connected graphs, leading to a combinatorial
explosion in edge information and substantial sparsity as problem scales grow,
especially evident in large grid-based environments. This dense representation
results in diluted node-level information, exponentially increases memory
requirements, and ultimately makes learning infeasible for larger-scale
problems. To address these challenges, we propose a sparse, goal-aware GNN
representation that selectively encodes relevant local relationships and
explicitly integrates spatial features related to the goal. We validate our
approach by designing novel drone mission scenarios based on PDDL within a grid
world, effectively simulating realistic mission execution environments. Our
experimental results demonstrate that our method scales effectively to larger
grid sizes previously infeasible with dense graph representations and
substantially improves policy generalization and success rates. Our findings
provide a practical foundation for addressing realistic, large-scale
generalized planning tasks.

</details>


### [131] [Modeling Human Responses to Multimodal AI Content](https://arxiv.org/abs/2508.10769)
*Zhiqi Shen,Shaojing Fan,Danni Xu,Terence Sim,Mohan Kankanhalli*

Main category: cs.AI

TL;DR: 该研究着眼于AI生成内容对人类感知和行为的影响，提出了一个包含大量AI生成帖子的数据集，并设计了一个能够预测人类反应的LLM系统，以对抗AI错误信息。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容变得普遍，错误信息的风险也随之增加。以往的研究主要集中在识别内容是否真实，而对于此类内容如何影响人类的感知和行为知之甚少。在交易或股票市场等领域，预测人们的反应（例如，新闻帖子是否会传播）可能比验证其事实准确性更为重要。

Method: 提出了MhAIM数据集，包含154,552个在线帖子（其中111,153个是AI生成的），并提出了三个新指标：可信度、影响力和开放性，以量化用户如何判断和参与在线内容。提出了T-Lens，一个基于LLM的代理系统，通过结合预测的人类对多模态信息的反应来回答用户查询。

Result: 人们在帖子包含文本和视觉效果时，能够更好地识别AI内容，尤其是在两者之间存在不一致时。HR-MCP（人类反应模型上下文协议）建立在标准化模型上下文协议（MCP）之上，从而可以与任何LLM无缝集成。这种集成使T-Lens能够更好地与人类反应对齐，从而增强了可解释性和交互能力。

Conclusion: 该研究提供了实证见解和实用工具，使LLM具备人类感知能力，并为减轻AI驱动的错误信息风险提供了可操作的策略。

Abstract: As AI-generated content becomes widespread, so does the risk of
misinformation. While prior research has primarily focused on identifying
whether content is authentic, much less is known about how such content
influences human perception and behavior. In domains like trading or the stock
market, predicting how people react (e.g., whether a news post will go viral),
can be more critical than verifying its factual accuracy. To address this, we
take a human-centered approach and introduce the MhAIM Dataset, which contains
154,552 online posts (111,153 of them AI-generated), enabling large-scale
analysis of how people respond to AI-generated content. Our human study reveals
that people are better at identifying AI content when posts include both text
and visuals, particularly when inconsistencies exist between the two. We
propose three new metrics: trustworthiness, impact, and openness, to quantify
how users judge and engage with online content. We present T-Lens, an LLM-based
agent system designed to answer user queries by incorporating predicted human
responses to multimodal information. At its core is HR-MCP (Human Response
Model Context Protocol), built on the standardized Model Context Protocol
(MCP), enabling seamless integration with any LLM. This integration allows
T-Lens to better align with human reactions, enhancing both interpretability
and interaction capabilities. Our work provides empirical insights and
practical tools to equip LLMs with human-awareness capabilities. By
highlighting the complex interplay among AI, human cognition, and information
reception, our findings suggest actionable strategies for mitigating the risks
of AI-driven misinformation.

</details>


### [132] [The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference](https://arxiv.org/abs/2508.10777)
*Maël Jullien,Marco Valentino,André Freitas*

Main category: cs.AI

TL;DR: current LLMs often possess the relevant clinical knowledge but lack the structured, composable internal representations needed to deploy it reliably


<details>
  <summary>Details</summary>
Motivation: Large language models are often assumed to acquire increasingly structured, generalizable internal representations simply by scaling data and parameters. We interrogate this assumption

Method: introducing a Clinical Trial Natural Language Inference benchmark comprising four reasoning families, Causal Attribution, Compositional Grounding, Epistemic Verification, and Risk State Abstraction. Each item is paired with a targeted Ground Knowledge and Meta-Level Reasoning Verification (GKMRV) probe

Result: Models achieve near-ceiling GKMRV accuracy yet perform poorly on the main reasoning tasks. Despite low accuracy, output inferences are highly consistent across samples, indicating a systematic application of underlying heuristics and shortcuts

Conclusion: current LLMs often possess the relevant clinical knowledge but lack the structured, composable internal representations needed to deploy it reliably

Abstract: Large language models are often assumed to acquire increasingly structured,
generalizable internal representations simply by scaling data and parameters.
We interrogate this assumption by introducing a Clinical Trial Natural Language
Inference benchmark comprising four reasoning families, Causal Attribution,
Compositional Grounding, Epistemic Verification, and Risk State Abstraction.
Each item is paired with a targeted Ground Knowledge and Meta-Level Reasoning
Verification (GKMRV) probe, allowing us to dissociate failures of factual
access from failures of inference. We evaluate six contemporary LLMs under both
direct and chain of thought prompting.
  Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform
poorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy,
output inferences are highly consistent across samples (mean 0.87), indicating
a systematic application of underlying heuristics and shortcuts.
  These results reveal fundamental structural and representational limitations:
current LLMs often possess the relevant clinical knowledge but lack the
structured, composable internal representations needed to deploy it reliably
(e.g., integrating constraints, weighing evidence, or simulating
counterfactuals). Decoupling knowledge from reasoning with GKMRV makes this
dissociation explicit and measurable, providing an effective framework for
probing the reliability of LLMs in high-stakes domains.

</details>


### [133] [Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems](https://arxiv.org/abs/2508.10806)
*Maria J. P. Peixoto,Akriti Pandey,Ahsan Zaman,Peter R. Lewis*

Main category: cs.AI

TL;DR: This paper investigates accessibility gaps in XAI and suggests that simplified explanations and multimodal presentation are required for more equitable interpretability for users with vision impairments.


<details>
  <summary>Details</summary>
Motivation: the accessibility of eXplainable AI (XAI) methods, particularly for users with vision impairments, remains underexplored

Method: a two-pronged approach: a literature review of 79 studies and a four-part methodological proof of concept that operationalizes inclusive XAI design

Result: evaluations of XAI techniques rarely include disabled users, with most explanations relying on inherently visual formats

Conclusion: Simplified explanations are more comprehensible for non-visual users than detailed ones, and that multimodal presentation is required for more equitable interpretability.

Abstract: As AI systems are increasingly deployed to support decision-making in
critical domains, explainability has become a means to enhance the
understandability of these outputs and enable users to make more informed and
conscious choices. However, despite growing interest in the usability of
eXplainable AI (XAI), the accessibility of these methods, particularly for
users with vision impairments, remains underexplored. This paper investigates
accessibility gaps in XAI through a two-pronged approach. First, a literature
review of 79 studies reveals that evaluations of XAI techniques rarely include
disabled users, with most explanations relying on inherently visual formats.
Second, we present a four-part methodological proof of concept that
operationalizes inclusive XAI design: (1) categorization of AI systems, (2)
persona definition and contextualization, (3) prototype design and
implementation, and (4) expert and user assessment of XAI techniques for
accessibility. Preliminary findings suggest that simplified explanations are
more comprehensible for non-visual users than detailed ones, and that
multimodal presentation is required for more equitable interpretability.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [134] [Privacy-Preserving Approximate Nearest Neighbor Search on High-Dimensional Data](https://arxiv.org/abs/2508.10373)
*Yingfan Liu,Yandi Zhang,Jiadong Xie,Hui Li,Jeffrey Xu Yu,Jiangtao Cui*

Main category: cs.DB

TL;DR: This paper introduces a novel privacy-preserving k-ANNS solution on a single cloud server that improves data privacy, efficiency, and accuracy, while minimizing user involvement. The method uses a new encryption method and a privacy-preserving index and achieves significant speedups compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing PP-ANNS solutions fall short of meeting the requirements of data privacy, efficiency, accuracy, and minimal user involvement concurrently.

Method: A novel solution that primarily executes PP-ANNS on a single cloud server. A novel encryption method named distance comparison encryption is introduced. A privacy-preserving index is designed that combines the state-of-the-art  𝑘 -ANNS method with an approximate distance computation method. A search method using a filter-and-refine strategy based on the index is devised.

Result: The experimental results demonstrate the superiority of the proposed solution over existing solutions. The method accelerates PP-ANNS by up to 3 orders of magnitude compared to state-of-the-art methods, while not compromising the accuracy.

Conclusion: The proposed method accelerates PP-ANNS by up to 3 orders of magnitude compared to state-of-the-art methods, while not compromising the accuracy.

Abstract: In the era of cloud computing and AI, data owners outsource ubiquitous
vectors to the cloud, which furnish approximate $k$-nearest neighbors
($k$-ANNS) services to users. To protect data privacy against the untrusted
server, privacy-preserving $k$-ANNS (PP-ANNS) on vectors has been a fundamental
and urgent problem. However, existing PP-ANNS solutions fall short of meeting
the requirements of data privacy, efficiency, accuracy, and minimal user
involvement concurrently. To tackle this challenge, we introduce a novel
solution that primarily executes PP-ANNS on a single cloud server to avoid the
heavy communication overhead between the cloud and the user. To ensure data
privacy, we introduce a novel encryption method named distance comparison
encryption, facilitating secure, efficient, and exact distance comparisons. To
optimize the trade-off between data privacy and search performance, we design a
privacy-preserving index that combines the state-of-the-art $k$-ANNS method
with an approximate distance computation method. Then, we devise a search
method using a filter-and-refine strategy based on the index. Moreover, we
provide the security analysis of our solution and conduct extensive experiments
to demonstrate its superiority over existing solutions. Based on our
experimental results, our method accelerates PP-ANNS by up to 3 orders of
magnitude compared to state-of-the-art methods, while not compromising the
accuracy.

</details>


### [135] [Cross-Organizational Analysis of Parliamentary Processes: A Case Study](https://arxiv.org/abs/2508.10381)
*Paul-Julius Hillmann,Stephan A. Fahrenkrog-Petersen,Jan Mendling*

Main category: cs.DB

TL;DR: apply process mining to parliamentary processes of German state parliaments


<details>
  <summary>Details</summary>
Motivation: little attention has gone into the cross-organizational comparison of processes, since many companies are hesitant to share their data. This paper explores the processes of German state parliaments that are often legally required to share their data and run the same type of processes for different geographical regions.

Method: apply process mining to parliamentary processes

Result: the relevance of our results that are based on knowledge exchange with a political scientist and a domain expert from the German federal parliament.

Conclusion: This paper analyzes legislative processes of three German state parliaments, generating insights into their differences and best practices.

Abstract: Process Mining has been widely adopted by businesses and has been shown to
help organizations analyze and optimize their processes. However, so far,
little attention has gone into the cross-organizational comparison of
processes, since many companies are hesitant to share their data. In this
paper, we explore the processes of German state parliaments that are often
legally required to share their data and run the same type of processes for
different geographical regions. This paper is the first attempt to apply
process mining to parliamentary processes and, therefore, contributes toward a
novel interdisciplinary research area that combines political science and
process mining. In our case study, we analyze legislative processes of three
German state parliaments and generate insights into their differences and best
practices. We provide a discussion of the relevance of our results that are
based on knowledge exchange with a political scientist and a domain expert from
the German federal parliament.

</details>


### [136] [Efficient Methods for Accurate Sparse Trajectory Recovery and Map Matching](https://arxiv.org/abs/2508.10460)
*Wei Tian,Jieming Shi,Man Lung Yiu*

Main category: cs.DB

TL;DR: 提出了 TRMMA 和 MMA 两种方法，分别用于轨迹恢复和地图匹配，并在真实数据集上取得了最佳结果。


<details>
  <summary>Details</summary>
Motivation: 现实世界的轨迹通常是稀疏的，采样率低，并且与道路网络错位，但是许多应用程序需要高质量的数据以获得最佳性能。为了提高以稀疏轨迹作为输入的数据质量，系统地研究了两个相关的研究问题：道路网络上的轨迹恢复和地图匹配。

Method: 提出 TRMMA 和 MMA 两种有效的方法，分别用于准确的轨迹恢复和地图匹配。MMA 作为 TRMMA 的第一步，将 GPS 点映射到路段，TRMMA 专注于 MMA 返回的路线中的路段，以推断缺失点的位置比例，从而高效地生成高采样轨迹。

Result: TRMMA 和 MMA 在四个大型真实世界数据集上，在轨迹恢复和地图匹配方面，始终如一地取得了最佳的结果质量，通常有显著的优势。

Conclusion: TRMMA 和 MMA 在四个大型真实世界数据集上，在轨迹恢复和地图匹配方面，始终如一地取得了最佳的结果质量，通常有显著的优势。

Abstract: Real-world trajectories are often sparse with low-sampling rates (i.e., long
intervals between consecutive GPS points) and misaligned with road networks,
yet many applications demand high-quality data for optimal performance. To
improve data quality with sparse trajectories as input, we systematically study
two related research problems: trajectory recovery on road network, which aims
to infer missing points to recover high-sampling trajectories, and map
matching, which aims to map GPS points to road segments to determine underlying
routes. In this paper, we present efficient methods TRMMA and MMA for accurate
trajectory recovery and map matching, respectively, where MMA serves as the
first step of TRMMA. In MMA, we carefully formulate a classification task to
map a GPS point from sparse trajectories to a road segment over a small
candidate segment set, rather than the entire road network. We develop
techniques in MMA to generate effective embeddings that capture the patterns of
GPS data, directional information, and road segments, to accurately align
sparse trajectories to routes. For trajectory recovery, TRMMA focuses on the
segments in the route returned by MMA to infer missing points with position
ratios on road segments, producing high-sampling trajectories efficiently by
avoiding evaluation of all road segments. Specifically, in TRMMA, we design a
dual-transformer encoding process to cohesively capture latent patterns in
trajectories and routes, and an effective decoding technique to sequentially
predict the position ratios and road segments of missing points. We conduct
extensive experiments to compare TRMMA and MMA with numerous existing methods
for trajectory recovery and map matching, respectively, on 4 large real-world
datasets. TRMMA and MMA consistently achieve the best result quality, often by
a significant margin.

</details>


### [137] [Advances in Logic-Based Entity Resolution: Enhancing ASPEN with Local Merges and Optimality Criteria](https://arxiv.org/abs/2508.10504)
*Zhliang Xiang,Meghyn Bienvenu,Gianluca Cima,Víctor Gutiérrez-Basulto,Yazmín Ibáñez-García*

Main category: cs.DB

TL;DR: ASPEN+扩展了ASPEN，支持本地合并和新的优化标准，提高了实体解析的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的ASPEN系统只支持全局合并，这在某些情况下是不合适的，例如，'J. Lee'的某些实例可能指向'Joy Lee'，而另一些应该与'Jake Lee'匹配。

Method: ASPEN+系统，它扩展了现有的ASP系统，支持本地合并，并提供新的优化标准。

Result: 对真实世界数据集的广泛实验评估，证明了本地合并和新的优化标准对准确性和运行时的影响。

Conclusion: ASPEN+通过支持本地合并和新的优化标准，扩展了现有的基于ASP的系统ASPEN，用于集体实体解析。

Abstract: In this paper, we present ASPEN+, which extends an existing ASP-based system,
ASPEN,for collective entity resolution with two important functionalities:
support for local merges and new optimality criteria for preferred solutions.
Indeed, ASPEN only supports so-called global merges of entity-referring
constants (e.g. author ids), in which all occurrences of matched constants are
treated as equivalent and merged accordingly. However, it has been argued that
when resolving data values, local merges are often more appropriate, as e.g.
some instances of 'J. Lee' may refer to 'Joy Lee', while others should be
matched with 'Jake Lee'. In addition to allowing such local merges, ASPEN+
offers new optimality criteria for selecting solutions, such as minimizing rule
violations or maximising the number of rules supporting a merge. Our main
contributions are thus (1) the formalisation and computational analysis of
various notions of optimal solution, and (2) an extensive experimental
evaluation on real-world datasets, demonstrating the effect of local merges and
the new optimality criteria on both accuracy and runtime.

</details>


### [138] [Emerging Skycube](https://arxiv.org/abs/2508.10516)
*Mickaël Martin Nevot*

Main category: cs.DB

TL;DR: 本文提出了Emerging Skycube，一种结合Skycube和新兴datacube的概念，并通过两种约简方法优化计算效率。


<details>
  <summary>Details</summary>
Motivation: 为了提取与多个标准相关的全局最优或非支配数据，并观察它们根据决策属性的演变。

Method: 结合多标准决策分析和趋势反转发现

Result: 新兴Skycube的计算成本低于新兴datacube，并且可以通过提出的两种约简方法进一步节省计算时间和存储空间。

Conclusion: 提出了新兴Skycube的概念，并提出了两种连续的约简方法来节省计算时间和存储空间：基于Skylines概念格的Skycube无损部分物化，以及闭合新兴Skycube或闭合新兴L-Skycube。

Abstract: Combining multi-criteria decision analysis and trend reversal discovery make
it possible to extract globally optimal, or non-dominated, data in relation to
several criteria, and then to observe their evolution according to a
decision-making property. Thus, we introduce Emerging Skycube, a concept
associating Skycube and emerging datacube. As far as we know, no
DBMS-integrated solution exists to compute an emerging Skycube, and hence
taking advantage of ROLAP analysis tools. An emerging datacube has only one
measure: we propose to use several to comply to multi-criteria decision
analysis constraints which requires multiple attributes. A datacube is
expensive to compute. An emerging datacube is about twice as expensive. On the
other hand, an emerging Skycube is cheaper as the trend reversal is computed
after two Skycube calculations, which considerably reduces the relation volume
in comparison with the initial one. It is possible to save even more computing
time and storage space. To this end, we propose two successive reductions.
First, a Skycube lossless partial materialisation using Skylines concepts
lattice, based on the agree concepts lattice and partitions lattice. Then,
either the closed emerging Skycube for an information-loss reduction, or the
closed emerging L-Skycube for a smaller but lossless reduction.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [139] [Bridging Modality Gaps in e-Commerce Products via Vision-Language Alignment](https://arxiv.org/abs/2508.10116)
*Yipeng Zhang,Hongju Yu,Aritra Mandal,Canran Xu,Qunzhi Zhou,Zhe Wu*

Main category: cs.IR

TL;DR: OPAL 是一个框架，用于使用微调的多模态大型语言模型 (MLLM) 从图像生成符合模式的高质量商品描述。


<details>
  <summary>Details</summary>
Motivation: 人工或半人工录入结构化商品信息通常会产生不一致的质量、错误和缓慢的周转时间，尤其对于 C2C 卖家而言。直接从商品图像生成准确的描述提供了一种有希望的替代方案。现有的基于检索的解决方案解决了一些问题，但通常会遗漏细粒度的视觉细节，并且难以处理利基或专业类别。

Method: OPAL，一个用于从图像生成符合模式的高质量商品描述的框架，使用微调的多模态大型语言模型 (MLLM)。它引入了两种数据细化方法：MLLM 辅助一致性增强和 LLM 辅助上下文理解。使用视觉指令调整结合直接偏好优化来微调 MLLM。

Result: 在真实世界的电子商务数据集上评估 OPAL，表明它在描述质量和模式完成率方面始终优于基线方法。

Conclusion: OPAL 通过弥合视觉和文本模式之间的差距，提供更丰富、更准确和更一致的商品描述，从而推进了自动化列表优化，并支持电子商务平台中可扩展的高质量内容生成。

Abstract: Item information, such as titles and attributes, is essential for effective
user engagement in e-commerce. However, manual or semi-manual entry of
structured item specifics often produces inconsistent quality, errors, and slow
turnaround, especially for Customer-to-Customer sellers. Generating accurate
descriptions directly from item images offers a promising alternative. Existing
retrieval-based solutions address some of these issues but often miss
fine-grained visual details and struggle with niche or specialized categories.
  We propose Optimized Preference-Based AI for Listings (OPAL), a framework for
generating schema-compliant, high-quality item descriptions from images using a
fine-tuned multimodal large language model (MLLM). OPAL addresses key
challenges in multimodal e-commerce applications, including bridging modality
gaps and capturing detailed contextual information. It introduces two data
refinement methods: MLLM-Assisted Conformity Enhancement, which ensures
alignment with structured schema requirements, and LLM-Assisted Contextual
Understanding, which improves the capture of nuanced and fine-grained
information from visual inputs.
  OPAL uses visual instruction tuning combined with direct preference
optimization to fine-tune the MLLM, reducing hallucinations and improving
robustness across different backbone architectures. We evaluate OPAL on
real-world e-commerce datasets, showing that it consistently outperforms
baseline methods in both description quality and schema completion rates. These
results demonstrate that OPAL effectively bridges the gap between visual and
textual modalities, delivering richer, more accurate, and more consistent item
descriptions. This work advances automated listing optimization and supports
scalable, high-quality content generation in e-commerce platforms.

</details>


### [140] [DS4RS: Community-Driven and Explainable Dataset Search Engine for Recommender System Research](https://arxiv.org/abs/2508.10238)
*Xinyang Shao,Tri Kurniawan Wijaya*

Main category: cs.IR

TL;DR: A community-driven dataset search engine tailored for recommender system research is proposed to improve dataset discoverability and search interpretability, facilitating more efficient research reproduction.


<details>
  <summary>Details</summary>
Motivation: Finding datasets that match specific recommendation task or domains remains a challenge due to scattered sources and inconsistent metadata.

Method: We propose a community-driven and explainable dataset search engine tailored for recommender system research. Our system supports semantic search across multiple dataset attributes, such as dataset names, descriptions, and recommendation domain, and provides explanations of search relevance to enhance transparency. The system encourages community participation by allowing users to contribute standardized dataset metadata in public repository.

Result: Our system supports semantic search across multiple dataset attributes, such as dataset names, descriptions, and recommendation domain, and provides explanations of search relevance to enhance transparency.

Conclusion: The system facilitates more efficient research reproduction by improving dataset discoverability and search interpretability. The platform is publicly available at: https://ds4rs.com.

Abstract: Accessing suitable datasets is critical for research and development in
recommender systems. However, finding datasets that match specific
recommendation task or domains remains a challenge due to scattered sources and
inconsistent metadata. To address this gap, we propose a community-driven and
explainable dataset search engine tailored for recommender system research. Our
system supports semantic search across multiple dataset attributes, such as
dataset names, descriptions, and recommendation domain, and provides
explanations of search relevance to enhance transparency. The system encourages
community participation by allowing users to contribute standardized dataset
metadata in public repository. By improving dataset discoverability and search
interpretability, the system facilitates more efficient research reproduction.
The platform is publicly available at: https://ds4rs.com.

</details>


### [141] [Clicks Versus Conversion: Choosing a Recommender's Training Objective in E-Commerce](https://arxiv.org/abs/2508.10377)
*Michael Weiss,Robert Rosenbach,Christian Eggenberger*

Main category: cs.IR

TL;DR: 对比CTR、ACR和OSR，发现优化OSR比优化CTR带来的GMV提升高五倍以上。


<details>
  <summary>Details</summary>
Motivation: 在电子商务中，对高点击率 (CTR) 或高转化率（如加入购物车率 (ACR) 和订单提交率 (OSR，浏览到购买转化率)）进行排名产品推荐是标准做法。优化 CTR 看起来是一个简单的选择：训练数据（即点击数据）易于收集且通常大量可用。另一方面，ACR 和 OSR 与商店的业务目标（如商品交易总额 (GMV)）有更直接的联系。

Method: 在线 A/B 测试

Result: 优化 OSR 产生的 GMV 提升是优化 CTR 的五倍以上，且不会牺牲新产品的发现。我们的结果还提供了对每个目标的不同特征重要性的见解。

Conclusion: 在我们的商店中，优化 OSR 产生的 GMV 提升是优化 CTR 的五倍以上，且不会牺牲新产品的发现。

Abstract: Ranking product recommendations to optimize for a high click-through rate
(CTR) or for high conversion, such as add-to-cart rate (ACR) and
Order-Submit-Rate (OSR, view-to-purchase conversion) are standard practices in
e-commerce. Optimizing for CTR appears like a straightforward choice: Training
data (i.e., click data) are simple to collect and often available in large
quantities. Additionally, CTR is used far beyond e-commerce, making it a
generalist, easily implemented option. ACR and OSR, on the other hand, are more
directly linked to a shop's business goals, such as the Gross Merchandise Value
(GMV). In this paper, we compare the effects of using either of these
objectives using an online A/B test. Among our key findings, we demonstrate
that in our shops, optimizing for OSR produces a GMV uplift more than five
times larger than when optimizing for CTR, without sacrificing new product
discovery. Our results also provide insights into the different feature
importances for each of the objectives.

</details>


### [142] [Proxy Model-Guided Reinforcement Learning for Client Selection in Federated Recommendation](https://arxiv.org/abs/2508.10401)
*Liang Qu,Jianxin Li,Wei Yuan,Penghui Ruan,Yuhui Shi,Hongzhi Yin*

Main category: cs.IR

TL;DR: This paper proposes ProxyRL-FRS, a proxy model-guided reinforcement learning framework for client selection in federated recommendation, which addresses the limitations of existing methods by using a proxy model for lightweight contribution estimation and a staleness-aware reinforcement learning agent.


<details>
  <summary>Details</summary>
Motivation: most existing FedRS frameworks adopt fully random client selection strategy in each training round, overlooking the statistical heterogeneity of user data arising from diverse preferences and behavior patterns, thereby resulting in suboptimal model performance. While some client selection strategies have been proposed in the broader federated learning literature, these methods are typically designed for generic tasks and fail to address the unique challenges of recommendation scenarios, such as expensive contribution evaluation due to the large number of clients, and sparse updates resulting from long-tail item distributions.

Method: we propose ProxyRL-FRS, a proxy model-guided reinforcement learning framework tailored for client selection in federated recommendation. Specifically, we first introduce ProxyNCF, a dual-branch model deployed on each client, which augments standard Neural Collaborative Filtering with an additional proxy model branch that provides lightweight contribution estimation, thus eliminating the need for expensive per-round local training traditionally required to evaluate a client's contribution. Furthermore, we design a staleness-aware SA reinforcement learning agent that selects clients based on the proxy-estimated contribution, and is guided by a reward function balancing recommendation accuracy and embedding staleness, thereby enriching the update coverage of item embeddings.

Result: Experiments conducted on public recommendation datasets demonstrate the effectiveness of ProxyRL-FRS.

Conclusion: Experiments conducted on public recommendation datasets demonstrate the effectiveness of ProxyRL-FRS.

Abstract: Federated recommender systems have emerged as a promising privacy-preserving
paradigm, enabling personalized recommendation services without exposing users'
raw data. By keeping data local and relying on a central server to coordinate
training across distributed clients, FedRSs protect user privacy while
collaboratively learning global models. However, most existing FedRS frameworks
adopt fully random client selection strategy in each training round,
overlooking the statistical heterogeneity of user data arising from diverse
preferences and behavior patterns, thereby resulting in suboptimal model
performance. While some client selection strategies have been proposed in the
broader federated learning literature, these methods are typically designed for
generic tasks and fail to address the unique challenges of recommendation
scenarios, such as expensive contribution evaluation due to the large number of
clients, and sparse updates resulting from long-tail item distributions. To
bridge this gap, we propose ProxyRL-FRS, a proxy model-guided reinforcement
learning framework tailored for client selection in federated recommendation.
Specifically, we first introduce ProxyNCF, a dual-branch model deployed on each
client, which augments standard Neural Collaborative Filtering with an
additional proxy model branch that provides lightweight contribution
estimation, thus eliminating the need for expensive per-round local training
traditionally required to evaluate a client's contribution. Furthermore, we
design a staleness-aware SA reinforcement learning agent that selects clients
based on the proxy-estimated contribution, and is guided by a reward function
balancing recommendation accuracy and embedding staleness, thereby enriching
the update coverage of item embeddings. Experiments conducted on public
recommendation datasets demonstrate the effectiveness of ProxyRL-FRS.

</details>


### [143] [Semantic IDs for Joint Generative Search and Recommendation](https://arxiv.org/abs/2508.10478)
*Gustavo Penha,Edoardo D'Amico,Marco De Nadai,Enrico Palumbo,Alexandre Tamborrino,Ali Vardasbi,Max Lefarov,Shawn Lin,Timothy Heath,Francesco Fabbri,Hugues Bouchard*

Main category: cs.IR

TL;DR: 本文探讨了如何构建在统一模型中在搜索和推荐中表现良好的语义ID。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）驱动的生成模型正在成为驱动推荐和搜索任务的统一解决方案。这些模型中的一个关键设计选择是如何表示项目，传统上是通过唯一标识符（ID），最近通过由离散代码组成的语义ID，从嵌入中获得。

Method: 比较一系列构建语义ID的策略，研究特定任务和跨任务的方法，以及在联合搜索和推荐生成模型中，每个任务是否应该有自己的语义ID令牌。

Result: 使用在搜索和推荐任务上微调的双编码器模型来获得项目嵌入，然后构建统一的语义ID空间，可以在这两个任务中实现强大的性能。

Conclusion: 使用在搜索和推荐任务上微调的双编码器模型来获得项目嵌入，然后构建统一的语义ID空间，可以在这两个任务中实现强大的性能。

Abstract: Generative models powered by Large Language Models (LLMs) are emerging as a
unified solution for powering both recommendation and search tasks. A key
design choice in these models is how to represent items, traditionally through
unique identifiers (IDs) and more recently with Semantic IDs composed of
discrete codes, obtained from embeddings. While task-specific embedding models
can improve performance for individual tasks, they may not generalize well in a
joint setting. In this paper, we explore how to construct Semantic IDs that
perform well both in search and recommendation when using a unified model. We
compare a range of strategies to construct Semantic IDs, looking into
task-specific and cross-tasks approaches, and also whether each task should
have its own semantic ID tokens in a joint search and recommendation generative
model. Our results show that using a bi-encoder model fine-tuned on both search
and recommendation tasks to obtain item embeddings, followed by the
construction of a unified Semantic ID space provides an effective trade-off,
enabling strong performance in both tasks. We hope these findings spark
follow-up work on generalisable, semantically grounded ID schemes and inform
the next wave of unified generative recommender architectures.

</details>


### [144] [Efficient Patent Searching Using Graph Transformers](https://arxiv.org/abs/2508.10496)
*Krzysztof Daniell,Igor Buzhinsky,Sebastian Björkqvist*

Main category: cs.IR

TL;DR: 提出了一种基于图变换器的专利检索方法，该方法利用专利审查员的引文作为相关性信号进行训练，在检索质量和计算效率方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在决定是否提交新的专利申请或使现有专利无效时，找到相关的现有技术至关重要。然而，由于专利文件数量庞大，以及需要细致的比较来确定新颖性，因此寻找现有技术具有挑战性。一个精确的搜索引擎对于加速这一过程是极其宝贵的。

Method: 基于图变换器的密集检索方法，利用专利审查员的引文作为相关性信号进行训练。

Result: 使用图作为输入显著提高了处理长文档的计算效率，同时利用审查员的引文使模型能够学习到超出简单基于文本匹配的领域特定相似性。

Conclusion: 图变换器模型在专利检索质量和计算效率方面均优于现有文本嵌入模型。

Abstract: Finding relevant prior art is crucial when deciding whether to file a new
patent application or invalidate an existing patent. However, searching for
prior art is challenging due to the large number of patent documents and the
need for nuanced comparisons to determine novelty. An accurate search engine is
therefore invaluable for speeding up the process. We present a Graph
Transformer-based dense retrieval method for patent searching where each
invention is represented by a graph describing its features and their
relationships. Our model processes these invention graphs and is trained using
prior art citations from patent office examiners as relevance signals. Using
graphs as input significantly improves the computational efficiency of
processing long documents, while leveraging examiner citations allows the model
to learn domain-specific similarities beyond simple text-based matching. The
result is a search engine that emulates how professional patent examiners
identify relevant documents. We compare our approach against publicly available
text embedding models and show substantial improvements in both prior art
retrieval quality and computational efficiency.

</details>


### [145] [DAS: Dual-Aligned Semantic IDs Empowered Industrial Recommender System](https://arxiv.org/abs/2508.10584)
*Wencai Ye,Mingjie Sun,Shaoyun Shi,Peng Wang,Wenjin Wu,Peng Jiang*

Main category: cs.IR

TL;DR: DAS is a one-stage method for aligning semantic IDs with collaborative signals in recommendation systems, avoiding information loss and improving alignment quality. It has been deployed at Kuaishou App.


<details>
  <summary>Details</summary>
Motivation: Semantic IDs lack collaborative signals, leading to misalignment with recommendation objectives. Two-stage frameworks for alignment have information loss and inflexibility.

Method: A novel one-stage Dual-Aligned Semantic IDs (DAS) method that simultaneously optimizes quantization and alignment. It incorporates a ID-based CF debias module and three contrastive alignment methods: dual user-to-item (u2i), dual item-to-item/user-to-user (i2i/u2u), and dual co-occurrence item-to-item/user-to-user (i2i/u2u). It also aligns the dual quantizations of users and ads.

Result: DAS achieves more efficient alignment between semantic IDs and collaborative signals. Extensive offline experiments and online A/B tests demonstrate DAS's effectiveness.

Conclusion: The proposed DAS method is effective and has been successfully deployed in various advertising scenarios at Kuaishou App, serving over 400 million users daily.

Abstract: Semantic IDs are discrete identifiers generated by quantizing the Multi-modal
Large Language Models (MLLMs) embeddings, enabling efficient multi-modal
content integration in recommendation systems. However, their lack of
collaborative signals results in a misalignment with downstream discriminative
and generative recommendation objectives. Recent studies have introduced
various alignment mechanisms to address this problem, but their two-stage
framework design still leads to two main limitations: (1) inevitable
information loss during alignment, and (2) inflexibility in applying adaptive
alignment strategies, consequently constraining the mutual information
maximization during the alignment process. To address these limitations, we
propose a novel and flexible one-stage Dual-Aligned Semantic IDs (DAS) method
that simultaneously optimizes quantization and alignment, preserving semantic
integrity and alignment quality while avoiding the information loss typically
associated with two-stage methods. Meanwhile, DAS achieves more efficient
alignment between the semantic IDs and collaborative signals, with the
following two innovative and effective approaches: (1) Multi-view Constrative
Alignment: To maximize mutual information between semantic IDs and
collaborative signals, we first incorporate an ID-based CF debias module, and
then design three effective contrastive alignment methods: dual user-to-item
(u2i), dual item-to-item/user-to-user (i2i/u2u), and dual co-occurrence
item-to-item/user-to-user (i2i/u2u). (2) Dual Learning: By aligning the dual
quantizations of users and ads, the constructed semantic IDs for users and ads
achieve stronger alignment. Finally, we conduct extensive offline experiments
and online A/B tests to evaluate DAS's effectiveness, which is now successfully
deployed across various advertising scenarios at Kuaishou App, serving over 400
million users daily.

</details>


### [146] [FuXi-β: Towards a Lightweight and Fast Large-Scale Generative Recommendation Model](https://arxiv.org/abs/2508.10615)
*Yufei Ye,Wei Guo,Hao Wang,Hong Zhu,Yuyang Ye,Yong Liu,Huifeng Guo,Ruiming Tang,Defu Lian,Enhong Chen*

Main category: cs.IR

TL;DR: Propose FuXi-$\\beta$, a new model based on FuXi-$\\alpha$, which introduces Functional Relative Attention Bias and removes the query-key attention map, achieving better performance and acceleration.


<details>
  <summary>Details</summary>
Motivation: Scaling laws for autoregressive generative recommenders reveal potential for larger, more versatile systems but mean greater latency and training costs. To accelerate training and inference, two efficiency bottlenecks are identified: the indexing operations in relative temporal attention bias and the computation of the query-key attention map.  relative attention bias in self-attention mechanisms can also serve as attention maps, raising the question of whether some attention maps are redundant.  using the query-key attention map might degrade the model's performance in recommendation tasks.

Method: introduce Functional Relative Attention Bias, which avoids the time-consuming operations of the original relative attention bias, thereby accelerating the process. remove the query-key attention map from the original self-attention layer and design a new Attention-Free Token Mixer module. by applying this framework to FuXi-$\\alpha$, introduce a new model, FuXi-$\\beta$.

Result: FuXi-$\\beta$ outperforms previous state-of-the-art models and achieves significant acceleration compared to FuXi-$\\alpha$. shows an improvement of 27% to 47% in the NDCG@10 metric on large-scale industrial datasets compared to FuXi-$\\alpha$.

Conclusion: FuXi-$\beta$ outperforms previous state-of-the-art models and achieves significant acceleration compared to FuXi-$\\&alpha$, while also adhering to the scaling law. Notably, FuXi-$\\&beta$ shows an improvement of 27% to 47% in the NDCG@10 metric on large-scale industrial datasets compared to FuXi-$\\&alpha$.

Abstract: Scaling laws for autoregressive generative recommenders reveal potential for
larger, more versatile systems but mean greater latency and training costs. To
accelerate training and inference, we investigated the recent generative
recommendation models HSTU and FuXi-$\alpha$, identifying two efficiency
bottlenecks: the indexing operations in relative temporal attention bias and
the computation of the query-key attention map. Additionally, we observed that
relative attention bias in self-attention mechanisms can also serve as
attention maps. Previous works like Synthesizer have shown that alternative
forms of attention maps can achieve similar performance, naturally raising the
question of whether some attention maps are redundant. Through empirical
experiments, we discovered that using the query-key attention map might degrade
the model's performance in recommendation tasks. To address these bottlenecks,
we propose a new framework applicable to Transformer-like recommendation
models. On one hand, we introduce Functional Relative Attention Bias, which
avoids the time-consuming operations of the original relative attention bias,
thereby accelerating the process. On the other hand, we remove the query-key
attention map from the original self-attention layer and design a new
Attention-Free Token Mixer module. Furthermore, by applying this framework to
FuXi-$\alpha$, we introduce a new model, FuXi-$\beta$. Experiments across
multiple datasets demonstrate that FuXi-$\beta$ outperforms previous
state-of-the-art models and achieves significant acceleration compared to
FuXi-$\alpha$, while also adhering to the scaling law. Notably, FuXi-$\beta$
shows an improvement of 27% to 47% in the NDCG@10 metric on large-scale
industrial datasets compared to FuXi-$\alpha$. Our code is available in a
public repository: https://github.com/USTC-StarTeam/FuXi-beta

</details>


### [147] [Hypercomplex Prompt-aware Multimodal Recommendation](https://arxiv.org/abs/2508.10753)
*Zheyu Chen,Jinfeng Xu,Hewei Wang,Shuo Yang,Zitong Wan,Haibo Hu*

Main category: cs.IR

TL;DR: HPMRec: a novel Hypercomplex Prompt-aware Multimodal Recommendation framework, which utilizes hypercomplex embeddings to enhance the representation diversity of multimodal features and alleviates the over-smoothing problem.


<details>
  <summary>Details</summary>
Motivation: Modern recommender systems face critical challenges in handling information overload while addressing the inherent limitations of multimodal representation learning. Existing methods suffer from three fundamental limitations: (1) restricted ability to represent rich multimodal features through a single representation, (2) existing linear modality fusion strategies ignore the deep nonlinear correlations between modalities, and (3) static optimization methods failing to dynamically mitigate the over-smoothing problem in graph convolutional network (GCN).

Method: a novel Hypercomplex Prompt-aware Multimodal Recommendation framework, which utilizes hypercomplex embeddings in the form of multi-components to enhance the representation diversity of multimodal features. HPMRec adopts the hypercomplex multiplication to naturally establish nonlinear cross-modality interactions to bridge semantic gaps, which is beneficial to explore the cross-modality features. HPMRec also introduces the prompt-aware compensation mechanism to aid the misalignment between components and modality-specific features loss, and this mechanism fundamentally alleviates the over-smoothing problem. It further designs self-supervised learning tasks that enhance representation diversity and align different modalities.

Result: HPMRec achieves state-of-the-art recommendation performance.

Conclusion: HPMRec achieves state-of-the-art recommendation performance on four public datasets.

Abstract: Modern recommender systems face critical challenges in handling information
overload while addressing the inherent limitations of multimodal representation
learning. Existing methods suffer from three fundamental limitations: (1)
restricted ability to represent rich multimodal features through a single
representation, (2) existing linear modality fusion strategies ignore the deep
nonlinear correlations between modalities, and (3) static optimization methods
failing to dynamically mitigate the over-smoothing problem in graph
convolutional network (GCN). To overcome these limitations, we propose HPMRec,
a novel Hypercomplex Prompt-aware Multimodal Recommendation framework, which
utilizes hypercomplex embeddings in the form of multi-components to enhance the
representation diversity of multimodal features. HPMRec adopts the hypercomplex
multiplication to naturally establish nonlinear cross-modality interactions to
bridge semantic gaps, which is beneficial to explore the cross-modality
features. HPMRec also introduces the prompt-aware compensation mechanism to aid
the misalignment between components and modality-specific features loss, and
this mechanism fundamentally alleviates the over-smoothing problem. It further
designs self-supervised learning tasks that enhance representation diversity
and align different modalities. Extensive experiments on four public datasets
show that HPMRec achieves state-of-the-art recommendation performance.

</details>


### [148] [CrossDenoise: Denoising Implicit Feedback via a Lightweight Entity-Aware Synergistic Framework](https://arxiv.org/abs/2508.10851)
*Ze Liu,Xianquan Wang,Shuochen Liu,Jie Ma,Huibo Xu,Yupeng Han,Zhe Yang,Kai Zhang,Longfei Li,Jun Zhou*

Main category: cs.IR

TL;DR: CrossDenoise是一种用于隐式反馈去噪的新框架，它优于现有技术，计算效率高，并且只需要很少的超参数调整。


<details>
  <summary>Details</summary>
Motivation: 推荐系统严重依赖隐式反馈，但由于假阳性和假阴性，隐式反馈本质上是嘈杂的，严重降低了推荐准确性。现有的去噪策略通常忽略实体感知建模，计算开销高，或者需要过多的超参数调整，限制了它们在现实世界中的适用性。

Method: 提出了一种新颖且轻量级的框架CrossDenoise，通过将噪声估计分解为用户、项目和交互特定因素来解决这些挑战。利用经验观察，CrossDenoise通过平均训练损失的基于排名的线性映射来计算实体声誉因素（用户/项目可靠性），并将这些因素与从个体损失的经验累积分布函数（ECDF）导出的交互水平权重融合。

Result: 在ML-1M、Yelp和Amazon-book数据集上，跨GMF、NeuMF和CDAE骨干网络的广泛实验表明，CrossDenoise始终且显著优于最先进的基线。例如，它在Yelp上使用NeuMF实现了高达27.01%的NDCG@50增益，同时产生的计算和内存开销可忽略不计。

Conclusion: CrossDenoise在多个数据集和模型上显著优于现有技术，为隐式反馈去噪提供了一种实用且可扩展的解决方案。

Abstract: Recommender systems heavily rely on implicit feedback, which is inherently
noisy due to false positives and negatives, severely degrading recommendation
accuracy. Existing denoising strategies often overlook entity-aware modeling,
suffer from high computational overhead, or demand excessive hyperparameter
tuning, limiting their real-world applicability. We propose CrossDenoise, a
novel and lightweight framework that addresses these challenges by
disentangling noise estimation into user-, item-, and interaction-specific
factors. Leveraging empirical observations that show significant heterogeneity
in user and item noise propensities, CrossDenoise computes entity reputation
factors (user/item reliability) via a rank-based linear mapping of average
training losses. These are fused with interaction-level weights derived from an
empirical cumulative distribution function (ECDF) of individual losses. This
design is model-agnostic, computationally efficient, and requires only two
intuitive hyperparameters. Extensive experiments on ML-1M, Yelp, and
Amazon-book datasets, across GMF, NeuMF, and CDAE backbones, demonstrate that
CrossDenoise consistently and significantly outperforms state-of-the-art
baselines. For instance, it achieves up to 27.01% NDCG@50 gain on Yelp with
NeuMF, while incurring negligible computational and memory overhead. Our
analysis confirms that CrossDenoise effectively separates clean from noisy
samples and remains robust under varied hyperparameter settings. It offers a
practical and scalable solution for denoising implicit feedback.

</details>


### [149] [Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data](https://arxiv.org/abs/2508.09636)
*Lalitesh Morishetti,Abhay Kumar,Jonathan Scott,Kaushiki Nag,Gunjan Sharma,Shanu Vashishtha,Rahul Sridhar,Rohit Chatter,Kannan Achan*

Main category: cs.IR

TL;DR: 提出了一种用于优化个性化产品搜索排序的新模型，该模型集成了表格和非表格数据，并利用 TinyBERT 模型和新颖的抽样技术。实验表明，该方法显著提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 为了优化个性化产品搜索排序。

Method: 提出了一种新颖的模型架构，使用多任务学习 (MTL) 框架来优化个性化产品搜索排序，并结合了表格和非表格数据，利用预训练的 TinyBERT 模型进行语义嵌入，并采用新颖的抽样技术来捕捉不同的客户行为。

Result: 实验结果表明，将非表格数据与多任务学习范式中的高级嵌入技术相结合，可以显著提高模型性能。

Conclusion: 结合非表格数据与先进嵌入技术的多任务学习范式显著提升了模型性能，实现了个性化产品搜索排序的改进。

Abstract: In this paper, we present a novel model architecture for optimizing
personalized product search ranking using a multi-task learning (MTL)
framework. Our approach uniquely integrates tabular and non-tabular data,
leveraging a pre-trained TinyBERT model for semantic embeddings and a novel
sampling technique to capture diverse customer behaviors. We evaluate our model
against several baselines, including XGBoost, TabNet, FT-Transformer, DCN-V2,
and MMoE, focusing on their ability to handle mixed data types and optimize
personalized ranking. Additionally, we propose a scalable relevance labeling
mechanism based on click-through rates, click positions, and semantic
similarity, offering an alternative to traditional human-annotated labels.
Experimental results show that combining non-tabular data with advanced
embedding techniques in multi-task learning paradigm significantly enhances
model performance. Ablation studies further underscore the benefits of
incorporating relevance labels, fine-tuning TinyBERT layers, and TinyBERT
query-product embedding interactions. These results demonstrate the
effectiveness of our approach in achieving improved personalized product search
ranking.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [150] [OpenFPL: An open-source forecasting method rivaling state-of-the-art Fantasy Premier League services](https://arxiv.org/abs/2508.09992)
*Daniel Groos*

Main category: cs.LG

TL;DR: This paper introduces OpenFPL, an open-source Fantasy Premier League forecasting method that achieves accuracy comparable to commercial services using only public data.


<details>
  <summary>Details</summary>
Motivation: To democratize access to highly accurate forecasts of player performance by presenting OpenFPL, an open-source Fantasy Premier League forecasting method developed exclusively from public data.

Method: position-specific ensemble models optimized on Fantasy Premier League and Understat data from four previous seasons (2020-21 to 2023-24)

Result: OpenFPL achieves accuracy comparable to a leading commercial service when tested prospectively on data from the 2024-25 season. OpenFPL also surpasses the commercial benchmark for high-return players ($>$ 2 points)

Conclusion: OpenFPL achieves accuracy comparable to a leading commercial service and surpasses the commercial benchmark for high-return players. These findings hold across one-, two-, and three-gameweek forecast horizons.

Abstract: Fantasy Premier League engages the football community in selecting the
Premier League players who will perform best from gameweek to gameweek. Access
to accurate performance forecasts gives participants an edge over competitors
by guiding expectations about player outcomes and reducing uncertainty in squad
selection. However, high-accuracy forecasts are currently limited to commercial
services whose inner workings are undisclosed and that rely on proprietary
data. This paper aims to democratize access to highly accurate forecasts of
player performance by presenting OpenFPL, an open-source Fantasy Premier League
forecasting method developed exclusively from public data. Comprising
position-specific ensemble models optimized on Fantasy Premier League and
Understat data from four previous seasons (2020-21 to 2023-24), OpenFPL
achieves accuracy comparable to a leading commercial service when tested
prospectively on data from the 2024-25 season. OpenFPL also surpasses the
commercial benchmark for high-return players ($>$ 2 points), which are most
influential for rank gains. These findings hold across one-, two-, and
three-gameweek forecast horizons, supporting long-term planning of transfers
and strategies while also informing final-day decisions.

</details>


### [151] [xRFM: Accurate, scalable, and interpretable feature learning models for tabular data](https://arxiv.org/abs/2508.10053)
*Daniel Beaglehole,David Holzmüller,Adityanarayanan Radhakrishnan,Mikhail Belkin*

Main category: cs.LG

TL;DR: xRFM, a new algorithm, outperforms GBDTs in regression and is competitive in classification tasks on tabular data. It also provides interpretability.


<details>
  <summary>Details</summary>
Motivation: Inference from tabular data is a foundation for modern technology and science. Yet, in contrast to the explosive changes in the rest of AI, the best practice for these predictive tasks has been relatively unchanged and is still primarily based on variations of Gradient Boosted Decision Trees (GBDTs).

Method: xRFM, an algorithm that combines feature learning kernel machines with a tree structure

Result: xRFM achieves best performance across 100 regression datasets and is competitive to the best methods across 200 classification datasets outperforming GBDTs.

Conclusion: xRFM achieves best performance across 100 regression datasets and is competitive to the best methods across 200 classification datasets outperforming GBDTs. Additionally, xRFM provides interpretability natively through the Average Gradient Outer Product.

Abstract: Inference from tabular data, collections of continuous and categorical
variables organized into matrices, is a foundation for modern technology and
science. Yet, in contrast to the explosive changes in the rest of AI, the best
practice for these predictive tasks has been relatively unchanged and is still
primarily based on variations of Gradient Boosted Decision Trees (GBDTs). Very
recently, there has been renewed interest in developing state-of-the-art
methods for tabular data based on recent developments in neural networks and
feature learning methods. In this work, we introduce xRFM, an algorithm that
combines feature learning kernel machines with a tree structure to both adapt
to the local structure of the data and scale to essentially unlimited amounts
of training data.
  We show that compared to $31$ other methods, including recently introduced
tabular foundation models (TabPFNv2) and GBDTs, xRFM achieves best performance
across $100$ regression datasets and is competitive to the best methods across
$200$ classification datasets outperforming GBDTs. Additionally, xRFM provides
interpretability natively through the Average Gradient Outer Product.

</details>


### [152] [A Personalized Exercise Assistant using Reinforcement Learning (PEARL): Results from a four-arm Randomized-controlled Trial](https://arxiv.org/abs/2508.10060)
*Amy Armento Lee,Narayan Hegde,Nina Deliu,Emily Rosenzweig,Arun Suggala,Sriram Lakshminarasimhan,Qian He,John Hernandez,Martin Seneviratne,Rahul Singh,Pradnesh Kalkar,Karthikeyan Shanmugam,Aravindan Raghuveer,Abhimanyu Singh,My Nguyen,James Taylor,Jatin Alla,Sofia S. Villar,Hulya Emir-Farinas*

Main category: cs.LG

TL;DR: This study used reinforcement learning to personalize nudges for physical activity and found it more effective than random or fixed nudges.


<details>
  <summary>Details</summary>
Motivation: Consistent physical inactivity poses a major global health challenge, and mobile health (mHealth) interventions, particularly Just-in-Time Adaptive Interventions (JITAIs), offer a promising avenue for scalable, personalized physical activity (PA) promotion.

Method: A four-arm randomized controlled trial was conducted with 13,463 Fitbit users, comparing control, random, fixed, and reinforcement learning (RL) nudge strategies.

Result: The RL group showed a significant increase in PA compared to all other groups at 1 month and sustained a significant increase compared to the control group at 2 months.

Conclusion: This study demonstrates the potential of a scalable, behaviorally-informed RL approach to personalize digital health interventions for PA.

Abstract: Consistent physical inactivity poses a major global health challenge. Mobile
health (mHealth) interventions, particularly Just-in-Time Adaptive
Interventions (JITAIs), offer a promising avenue for scalable, personalized
physical activity (PA) promotion. However, developing and evaluating such
interventions at scale, while integrating robust behavioral science, presents
methodological hurdles. The PEARL study was the first large-scale, four-arm
randomized controlled trial to assess a reinforcement learning (RL) algorithm,
informed by health behavior change theory, to personalize the content and
timing of PA nudges via a Fitbit app.
  We enrolled and randomized 13,463 Fitbit users into four study arms: control,
random, fixed, and RL. The control arm received no nudges. The other three arms
received nudges from a bank of 155 nudges based on behavioral science
principles. The random arm received nudges selected at random. The fixed arm
received nudges based on a pre-set logic from survey responses about PA
barriers. The RL group received nudges selected by an adaptive RL algorithm. We
included 7,711 participants in primary analyses (mean age 42.1, 86.3% female,
baseline steps 5,618.2).
  We observed an increase in PA for the RL group compared to all other groups
from baseline to 1 and 2 months. The RL group had significantly increased
average daily step count at 1 month compared to all other groups: control (+296
steps, p=0.0002), random (+218 steps, p=0.005), and fixed (+238 steps,
p=0.002). At 2 months, the RL group sustained a significant increase compared
to the control group (+210 steps, p=0.0122). Generalized estimating equation
models also revealed a sustained increase in daily steps in the RL group vs.
control (+208 steps, p=0.002). These findings demonstrate the potential of a
scalable, behaviorally-informed RL approach to personalize digital health
interventions for PA.

</details>


### [153] [Measuring Time Series Forecast Stability for Demand Planning](https://arxiv.org/abs/2508.10063)
*Steven Klee,Yuntian Xia*

Main category: cs.LG

TL;DR: This paper studies forecast stability, finding that ensemble models offer better stability without sacrificing accuracy, and highlights the importance of stability in production systems.


<details>
  <summary>Details</summary>
Motivation: Demand planners often value consistency and stability over incremental accuracy improvements, as drastic forecast variations require high human intervention and can cause distrust in ML forecasting models.

Method: Case study measuring the stability and accuracy of state-of-the-art forecasting models on public data sets.

Result: Ensemble models improve stability without significantly deteriorating forecast accuracy.

Conclusion: Ensemble models improve stability without significantly deteriorating forecast accuracy. The paper proposes the need for further study of forecast stability for models deployed in production systems.

Abstract: Time series forecasting is a critical first step in generating demand plans
for supply chains. Experiments on time series models typically focus on
demonstrating improvements in forecast accuracy over existing/baseline
solutions, quantified according to some accuracy metric. There is no doubt that
forecast accuracy is important; however in production systems, demand planners
often value consistency and stability over incremental accuracy improvements.
Assuming that the inputs have not changed significantly, forecasts that vary
drastically from one planning cycle to the next require high amounts of human
intervention, which frustrates demand planners and can even cause them to lose
trust in ML forecasting models. We study model-induced stochasticity, which
quantifies the variance of a set of forecasts produced by a single model when
the set of inputs is fixed. Models with lower variance are more stable.
  Recently the forecasting community has seen significant advances in forecast
accuracy through the development of deep machine learning models for time
series forecasting. We perform a case study measuring the stability and
accuracy of state-of-the-art forecasting models (Chronos, DeepAR, PatchTST,
Temporal Fusion Transformer, TiDE, and the AutoGluon best quality ensemble) on
public data sets from the M5 competition and Favorita grocery sales. We show
that ensemble models improve stability without significantly deteriorating (or
even improving) forecast accuracy. While these results may not be surprising,
the main point of this paper is to propose the need for further study of
forecast stability for models that are being deployed in production systems.

</details>


### [154] [Constrained Decoding of Diffusion LLMs with Context-Free Grammars](https://arxiv.org/abs/2508.10111)
*Niels Mündler,Jasper Dekoninck,Martin Vechev*

Main category: cs.LG

TL;DR: 本文提出了一种用于扩散模型的约束解码方法，该方法可以处理上下文无关文法捕获的形式语言。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在不同领域表现出良好的性能。许多LLM的实际应用需要遵守形式语言指定的语法约束。然而，由于其概率性质，LLM的输出不能保证遵守这些形式语言。

Method: 将约束解码简化为更一般的加性填充问题，并提出了一种高效的上下文无关语言算法来解决它。

Result: 在各种应用（如C++代码填充和JSON中的结构化数据提取）上的经验结果表明，该方法实现了接近完美的语法正确性，同时始终保持或提高功能正确性。

Conclusion: 该方法在保持或提高功能正确性的同时，实现了接近完美的语法正确性，计算开销保持在实用范围内。

Abstract: Large language models (LLMs) have shown promising performance across diverse
domains. Many practical applications of LLMs, such as code completion and
structured data extraction, require adherence to syntactic constraints
specified by a formal language. Yet, due to their probabilistic nature, LLM
output is not guaranteed to adhere to such formal languages. Prior work has
proposed constrained decoding as a means to restrict LLM generation to
particular formal languages. However, existing works are not applicable to the
emerging paradigm of diffusion LLMs, when used in practical scenarios such as
the generation of formally correct C++ or JSON output. In this paper we address
this challenge and present the first constrained decoding method for diffusion
models, one that can handle formal languages captured by context-free grammars.
We begin by reducing constrained decoding to the more general additive
infilling problem, which asks whether a partial output can be completed to a
valid word in the target language. This problem also naturally subsumes the
previously unaddressed multi-region infilling constrained decoding. We then
reduce this problem to the task of deciding whether the intersection of the
target language and a regular language is empty and present an efficient
algorithm to solve it for context-free languages. Empirical results on various
applications, such as C++ code infilling and structured data extraction in
JSON, demonstrate that our method achieves near-perfect syntactic correctness
while consistently preserving or improving functional correctness. Importantly,
our efficiency optimizations ensure that the computational overhead remains
practical.

</details>


### [155] [Less is More: Learning Graph Tasks with Just LLMs](https://arxiv.org/abs/2508.10115)
*Sola Shirai,Kavitha Srinivas,Julian Dolby,Michael Katz,Horst Samulowitz,Shirin Sohrabi*

Main category: cs.LG

TL;DR: LLMs can learn to solve graph tasks with chain-of-thought training, generalizing to new tasks and structures without specialized graph encoders.


<details>
  <summary>Details</summary>
Motivation: Reasoning over graphs could help solve many problems for large language models (LLMs). The merits of prior approaches remain unclear.

Method: Training LLMs with instructive chain-of-thought solutions.

Result: Show that even small LLMs can learn to solve graph tasks and this training generalizes to new tasks and graph structures.

Conclusion: Small LLMs can learn to solve graph tasks by training them with instructive chain-of-thought solutions, and this training generalizes, without specialized graph encoders, to new tasks and graph structures.

Abstract: For large language models (LLMs), reasoning over graphs could help solve many
problems. Prior work has tried to improve LLM graph reasoning by examining how
best to serialize graphs as text and by combining GNNs and LLMs. However, the
merits of such approaches remain unclear, so we empirically answer the
following research questions: (1) Can LLMs learn to solve fundamental graph
tasks without specialized graph encoding models?, (2) Can LLMs generalize
learned solutions to unseen graph structures or tasks?, and (3) What are the
merits of competing approaches to learn graph tasks? We show that even small
LLMs can learn to solve graph tasks by training them with instructive
chain-of-thought solutions, and this training generalizes, without specialized
graph encoders, to new tasks and graph structures.

</details>


### [156] [From Intent to Execution: Multimodal Chain-of-Thought Reinforcement Learning for Precise CAD Code Generation](https://arxiv.org/abs/2508.10118)
*Ke Niu,Haiyang Yu,Zhuofan Chen,Mengyang Zhao,Teng Fu,Bin Li,Xiangyang Xue*

Main category: cs.LG

TL;DR: CAD-RL, a new framework for CAD modeling code generation, improves reasoning, precision, and executability using reinforcement learning and a new dataset, ExeCAD.


<details>
  <summary>Details</summary>
Motivation: automating parametric 3D modeling

Method: a multimodal Chain-of-Thought (CoT) guided reinforcement learning post training framework

Result: CAD-RL achieves significant improvements in reasoning quality, output precision, and code executability over existing VLMs.

Conclusion: CAD-RL achieves significant improvements in reasoning quality, output precision, and code executability over existing VLMs.

Abstract: Computer-Aided Design (CAD) plays a vital role in engineering and
manufacturing, yet current CAD workflows require extensive domain expertise and
manual modeling effort. Recent advances in large language models (LLMs) have
made it possible to generate code from natural language, opening new
opportunities for automating parametric 3D modeling. However, directly
translating human design intent into executable CAD code remains highly
challenging, due to the need for logical reasoning, syntactic correctness, and
numerical precision. In this work, we propose CAD-RL, a multimodal
Chain-of-Thought (CoT) guided reinforcement learning post training framework
for CAD modeling code generation. Our method combines CoT-based Cold Start with
goal-driven reinforcement learning post training using three task-specific
rewards: executability reward, geometric accuracy reward, and external
evaluation reward. To ensure stable policy learning under sparse and
high-variance reward conditions, we introduce three targeted optimization
strategies: Trust Region Stretch for improved exploration, Precision Token Loss
for enhanced dimensions parameter accuracy, and Overlong Filtering to reduce
noisy supervision. To support training and benchmarking, we release ExeCAD, a
noval dataset comprising 16,540 real-world CAD examples with paired natural
language and structured design language descriptions, executable CADQuery
scripts, and rendered 3D models. Experiments demonstrate that CAD-RL achieves
significant improvements in reasoning quality, output precision, and code
executability over existing VLMs.

</details>


### [157] [Nested-ReFT: Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts](https://arxiv.org/abs/2508.10123)
*Maxime Heuillet,Yufei Cui,Boxing Chen,Audrey Durand,Prasanna Parthasarathi*

Main category: cs.LG

TL;DR: Nested-ReFT reduces training cost by using a subset of model layers for off-policy completion generation, achieving comparable performance to standard ReFT with better efficiency.


<details>
  <summary>Details</summary>
Motivation: Reducing the computational cost of generating completions during training in standard ReFT frameworks.

Method: Nested-ReFT: A novel ReFT framework using a subset of layers as the behavior model for off-policy completions.

Result: Improved computational efficiency (tokens/sec) across multiple math reasoning benchmarks and model sizes. Performance matches baseline ReFT with bias mitigation.

Conclusion: Nested-ReFT maintains performance with improved computational efficiency.

Abstract: Advanced reasoning in LLMs on challenging domains like mathematical reasoning
can be tackled using verifiable rewards based reinforced fine-tuning (ReFT). In
standard ReFT frameworks, a behavior model generates multiple completions with
answers per problem, for the answer to be then scored by a reward function.
While such RL post-training methods demonstrate significant performance
improvements across challenging reasoning domains, the computational cost of
generating completions during training with multiple inference steps makes the
training cost non-trivial. To address this, we draw inspiration from off-policy
RL, and speculative decoding to introduce a novel ReFT framework, dubbed
Nested-ReFT, where a subset of layers of the target model acts as the behavior
model to generate off-policy completions during training. The behavior model
configured with dynamic layer skipping per batch during training decreases the
inference cost compared to the standard ReFT frameworks. Our theoretical
analysis shows that Nested-ReFT yields unbiased gradient estimates with
controlled variance. Our empirical analysis demonstrates improved computational
efficiency measured as tokens/sec across multiple math reasoning benchmarks and
model sizes. Additionally, we explore three variants of bias mitigation to
minimize the off-policyness in the gradient updates that allows for maintaining
performance that matches the baseline ReFT performance.

</details>


### [158] [rETF-semiSL: Semi-Supervised Learning for Neural Collapse in Temporal Data](https://arxiv.org/abs/2508.10147)
*Yuhan Xie,William Cappelletti,Mahsa Shoaran,Pascal Frossard*

Main category: cs.LG

TL;DR: 提出了一种新的半监督pre-training策略，以强制满足神经崩溃现象，并结合生成pretext任务和序列增强策略，在时间序列分类数据集上优于以前的pretext任务。


<details>
  <summary>Details</summary>
Motivation: 时间序列的深度神经网络必须捕获复杂的时间模式，以有效地表示动态数据。自监督和半监督学习方法在预训练大型模型方面显示出有希望的结果，当对大型模型进行微调以进行分类时，通常优于从头开始训练的模型。但是，pretext训练任务的选择通常是启发式的，并且它们到下游分类的可传递性未得到保证

Method: 提出了一种新的半监督pre-training策略，以强制满足在最佳训练的神经分类器中观察到的神经崩溃现象的潜在表示。使用旋转等角紧框架分类器和伪标记来预训练具有少量标记样本的深度编码器。此外，为了在强制嵌入可分离性的同时有效地捕获时间动态，将生成pretext任务与该方法集成，并定义了一种新的序列增强策略。

Result: 该方法在lstm、transformers和状态空间模型上优于以前的pretext任务。

Conclusion: 该方法在三个多元时间序列分类数据集上显著优于先前的pretext任务，突出了将pre-training目标与理论上有根据的嵌入几何对齐的好处。

Abstract: Deep neural networks for time series must capture complex temporal patterns,
to effectively represent dynamic data. Self- and semi-supervised learning
methods show promising results in pre-training large models, which -- when
finetuned for classification -- often outperform their counterparts trained
from scratch. Still, the choice of pretext training tasks is often heuristic
and their transferability to downstream classification is not granted, thus we
propose a novel semi-supervised pre-training strategy to enforce latent
representations that satisfy the Neural Collapse phenomenon observed in
optimally trained neural classifiers. We use a rotational equiangular tight
frame-classifier and pseudo-labeling to pre-train deep encoders with few
labeled samples. Furthermore, to effectively capture temporal dynamics while
enforcing embedding separability, we integrate generative pretext tasks with
our method, and we define a novel sequential augmentation strategy. We show
that our method significantly outperforms previous pretext tasks when applied
to LSTMs, transformers, and state-space models on three multivariate time
series classification datasets. These results highlight the benefit of aligning
pre-training objectives with theoretically grounded embedding geometry.

</details>


### [159] [Out-of-Distribution Detection using Counterfactual Distance](https://arxiv.org/abs/2508.10148)
*Maria Stoica,Francesco Leofante,Alessio Lomuscio*

Main category: cs.LG

TL;DR: This paper introduces a new OOD detection method using counterfactual explanations to measure the distance to decision boundaries, achieving state-of-the-art or superior performance on several datasets.


<details>
  <summary>Details</summary>
Motivation: Accurate and explainable out-of-distribution (OOD) detection is required to use machine learning systems safely. Previous work has shown that feature distance to decision boundaries can be used to identify OOD data effectively.

Method: The paper proposes a post-hoc OOD detection method that calculates the distance to decision boundaries using counterfactual explanations, and improves scalability by computing counterfactuals directly in embedding space.

Result: The method achieves 93.50% AUROC and 25.80% FPR95 on CIFAR-10, 97.05% AUROC and 13.79% FPR95 on CIFAR-100, and 92.55% AUROC and 33.55% FPR95 on ImageNet-200 across four OOD datasets.

Conclusion: The method achieves state-of-the-art results on CIFAR-10 and outperforms existing methods on CIFAR-100 and ImageNet-200 across four OOD datasets.

Abstract: Accurate and explainable out-of-distribution (OOD) detection is required to
use machine learning systems safely. Previous work has shown that feature
distance to decision boundaries can be used to identify OOD data effectively.
In this paper, we build on this intuition and propose a post-hoc OOD detection
method that, given an input, calculates the distance to decision boundaries by
leveraging counterfactual explanations. Since computing explanations can be
expensive for large architectures, we also propose strategies to improve
scalability by computing counterfactuals directly in embedding space.
Crucially, as the method employs counterfactual explanations, we can seamlessly
use them to help interpret the results of our detector. We show that our method
is in line with the state of the art on CIFAR-10, achieving 93.50% AUROC and
25.80% FPR95. Our method outperforms these methods on CIFAR-100 with 97.05%
AUROC and 13.79% FPR95 and on ImageNet-200 with 92.55% AUROC and 33.55% FPR95
across four OOD datasets

</details>


### [160] [Characterizing Evolution in Expectation-Maximization Estimates for Overspecified Mixed Linear Regression](https://arxiv.org/abs/2508.10154)
*Zhankun Luo,Abolfazl Hashemi*

Main category: cs.LG

TL;DR: 本文研究了当模型具有比数据分布中更多的混合分量时，期望最大化（EM）算法在混合模型中的行为。结果表明，初始猜测和混合权重平衡会影响收敛速度和统计精度。


<details>
  <summary>Details</summary>
Motivation: 混合模型由于其在实践中的有效性和全面的理论基础而引起了广泛关注。一个持续存在的挑战是模型错误指定，当要拟合的模型具有比数据分布中更多的混合分量时，就会发生这种情况。

Method: 针对具有未知$d$维回归参数和混合权重的过参数化双分量混合线性回归（2MLR），开发了对期望最大化（EM）算法在目标模型错误指定情况下的行为的理论理解。在总体层面上的定理5.1中，对于混合权重的非平衡初始猜测，建立了回归参数在$O(\\log(1/\\epsilon))$步内的线性收敛性。相反，对于混合权重的平衡初始猜测，观察到次线性收敛，在$O(\\epsilon^{-2})$步内达到$\epsilon$-欧几里德距离精度。在有限样本水平的定理6.1中，对于具有足够不平衡的固定混合权重的混合模型，证明了$O((d/n)^{1/2})$的统计精度，而对于具有足够平衡的固定混合权重的混合模型，在给定n个数据样本的情况下，精度为$O((d/n)^{1/4})$。

Result: 对于混合权重的非平衡初始猜测，建立了回归参数在$O(\\log(1/\\epsilon))$步内的线性收敛性。相反，对于混合权重的平衡初始猜测，观察到次线性收敛，在$O(\\epsilon^{-2})$步内达到$\epsilon$-欧几里德距离精度。对于具有足够不平衡的固定混合权重的混合模型，证明了$O((d/n)^{1/2})$的统计精度，而对于具有足够平衡的固定混合权重的混合模型，在给定n个数据样本的情况下，精度为$O((d/n)^{1/4})$。

Conclusion: 在有限样本水平上，对于具有足够不平衡的固定混合权重的混合模型，证明了$O((d/n)^{1/2})$的统计精度，而对于具有足够平衡的固定混合权重的混合模型，在给定n个数据样本的情况下，精度为$O((d/n)^{1/4})$。通过将定理5.1中所需的最终精度$\\$epsilon\\$设置为与定理6.1中有限样本水平的精度相匹配，即对于足够不平衡的固定混合权重，令$\epsilon = O((d/n)^{1/2})$，对于足够平衡的固定混合权重，令$\epsilon = O((d/n)^{1/4})$，直观地推导出迭代复杂度界限$O(\\log (1/\\epsilon))=O(\\log (n/d))$和$O(\\epsilon^{-2})=O((n/d)^{1/2})$，在有限样本水平上，分别对应于足够不平衡和平衡的初始混合权重。将过参数化设置中的分析扩展到低信噪比状态。

Abstract: Mixture models have attracted significant attention due to practical
effectiveness and comprehensive theoretical foundations. A persisting challenge
is model misspecification, which occurs when the model to be fitted has more
mixture components than those in the data distribution. In this paper, we
develop a theoretical understanding of the Expectation-Maximization (EM)
algorithm's behavior in the context of targeted model misspecification for
overspecified two-component Mixed Linear Regression (2MLR) with unknown
$d$-dimensional regression parameters and mixing weights. In Theorem 5.1 at the
population level, with an unbalanced initial guess for mixing weights, we
establish linear convergence of regression parameters in $O(\log(1/\epsilon))$
steps. Conversely, with a balanced initial guess for mixing weights, we observe
sublinear convergence in $O(\epsilon^{-2})$ steps to achieve the
$\epsilon$-accuracy at Euclidean distance. In Theorem 6.1 at the finite-sample
level, for mixtures with sufficiently unbalanced fixed mixing weights, we
demonstrate a statistical accuracy of $O((d/n)^{1/2})$, whereas for those with
sufficiently balanced fixed mixing weights, the accuracy is $O((d/n)^{1/4})$
given $n$ data samples. Furthermore, we underscore the connection between our
population level and finite-sample level results: by setting the desired final
accuracy $\epsilon$ in Theorem 5.1 to match that in Theorem 6.1 at the
finite-sample level, namely letting $\epsilon = O((d/n)^{1/2})$ for
sufficiently unbalanced fixed mixing weights and $\epsilon = O((d/n)^{1/4})$
for sufficiently balanced fixed mixing weights, we intuitively derive iteration
complexity bounds $O(\log (1/\epsilon))=O(\log (n/d))$ and
$O(\epsilon^{-2})=O((n/d)^{1/2})$ at the finite-sample level for sufficiently
unbalanced and balanced initial mixing weights. We further extend our analysis
in overspecified setting to low SNR regime.

</details>


### [161] [Confounding is a Pervasive Problem in Real World Recommender Systems](https://arxiv.org/abs/2508.10479)
*Alexander Merkov,David Rohde,Alexandre Gilotte,Benjamin Heymann*

Main category: cs.LG

TL;DR: 推荐系统中的特征工程、A/B 测试和模块化可能会引入混淆，从而降低系统性能。


<details>
  <summary>Details</summary>
Motivation: 当未测量的特征影响治疗和结果时，就会出现未观察到的混淆，从而导致有偏差的因果效应估计。这个问题破坏了经济学、医学、生态学或流行病学等领域的观察性研究。利用完全观察数据的推荐系统似乎不易受到这个问题的影响。然而，推荐系统中的许多标准做法导致观察到的特征被忽略，从而导致实际上的相同问题。

Method: 仿真研究

Result: 特征工程、A/B 测试和模块化会将混淆引入推荐系统并阻碍其性能。

Conclusion: 常见的做法（如特征工程、A/B 测试和模块化）实际上会将混淆引入推荐系统并阻碍其性能。提供了该现象的若干说明，并由仿真研究提供支持，以及关于从业者如何在实际系统中减少或避免混淆影响的实用建议。

Abstract: Unobserved confounding arises when an unmeasured feature influences both the
treatment and the outcome, leading to biased causal effect estimates. This
issue undermines observational studies in fields like economics, medicine,
ecology or epidemiology. Recommender systems leveraging fully observed data
seem not to be vulnerable to this problem. However many standard practices in
recommender systems result in observed features being ignored, resulting in
effectively the same problem. This paper will show that numerous common
practices such as feature engineering, A/B testing and modularization can in
fact introduce confounding into recommendation systems and hamper their
performance. Several illustrations of the phenomena are provided, supported by
simulation studies with practical suggestions about how practitioners may
reduce or avoid the affects of confounding in real systems.

</details>


### [162] [Benchmark-Driven Selection of AI: Evidence from DeepSeek-R1](https://arxiv.org/abs/2508.10173)
*Petr Spelda,Vit Stritecky*

Main category: cs.LG

TL;DR: 基准驱动的选择会影响大型语言模型的性能，某些基准可以被视为训练的课程而不是看不见的测试集。


<details>
  <summary>Details</summary>
Motivation: 在观察到推理语言模型可以将它们现有的能力结合成新的中间步骤轨迹，然后在完成任务之前完成，并且这些轨迹有时可以帮助它们比过去的模型更好地概括之后，对推理语言模型的评估变得越来越重要。随着推理成为大型语言模型的下一个规模维度，需要仔细研究它们在关键任务中的能力。

Method: 使用来自“人类最后一考”的顺序决策问题来展示基准驱动的选择对DeepSeek-R1的影响。

Result: 展示了基准驱动的选择对DeepSeek-R1的影响。

Conclusion: 更好的性能并非总是由测试时算法的改进或模型大小引起的，而是通过使用有影响力的基准作为学习的课程来实现。通过有影响力的基准来引导人工智能的发展，将评估转化为学习，并使测试任务的新颖性成为衡量推理模型泛化能力的关键。

Abstract: Evaluation of reasoning language models gained importance after it was
observed that they can combine their existing capabilities into novel traces of
intermediate steps before task completion and that the traces can sometimes
help them to generalize better than past models. As reasoning becomes the next
scaling dimension of large language models, careful study of their capabilities
in critical tasks is needed. We show that better performance is not always
caused by test-time algorithmic improvements or model sizes but also by using
impactful benchmarks as curricula for learning. We call this benchmark-driven
selection of AI and show its effects on DeepSeek-R1 using our sequential
decision-making problem from Humanity's Last Exam. Steering development of AI
by impactful benchmarks trades evaluation for learning and makes novelty of
test tasks key for measuring generalization capabilities of reasoning models.
Consequently, some benchmarks could be seen as curricula for training rather
than unseen test sets.

</details>


### [163] [An Explainable AI based approach for Monitoring Animal Health](https://arxiv.org/abs/2508.10210)
*Rahul Janaa,Shubham Dixit,Mrityunjay Sharma,Ritesh Kumar*

Main category: cs.LG

TL;DR: This paper presents a data-driven farming approach using explainable ML to monitor cattle activity with IoT devices and accelerometer data. A k-NN classifier achieved high performance, and SHAP was used for feature interpretation.


<details>
  <summary>Details</summary>
Motivation: Monitoring cattle health and optimizing yield are key challenges faced by dairy farmers due to difficulties in tracking all animals on the farm. This work aims to showcase modern data-driven farming practices based on explainable machine learning(ML) methods that explain the activity and behaviour of dairy cattle (cows). Continuous data collection of 3-axis accelerometer sensors and usage of robust ML methodologies and algorithms, provide farmers and researchers with actionable information on cattle activity, allowing farmers to make informed decisions and incorporate sustainable practices.

Method: This study utilizes Bluetooth-based Internet of Things (IoT) devices and 4G networks for seamless data transmission, immediate analysis, inference generation, and explains the models performance with explainability frameworks. Special emphasis is put on the pre-processing of the accelerometers time series data, including the extraction of statistical characteristics, signal processing techniques, and lag-based features using the sliding window technique. Various hyperparameter-optimized ML models are evaluated across varying window lengths for activity classification.

Result: The k-nearest neighbour Classifier achieved the best performance, with AUC of mean 0.98 and standard deviation of 0.0026 on the training set and 0.99 on testing set).

Conclusion: The k-nearest neighbour Classifier achieved the best performance, with AUC of mean 0.98 and standard deviation of 0.0026 on the training set and 0.99 on testing set). In order to ensure transparency, Explainable AI based frameworks such as SHAP is used to interpret feature importance that can be understood and used by practitioners. A detailed comparison of the important features, along with the stability analysis of selected features, supports development of explainable and practical ML models for sustainable livestock management.

Abstract: Monitoring cattle health and optimizing yield are key challenges faced by
dairy farmers due to difficulties in tracking all animals on the farm. This
work aims to showcase modern data-driven farming practices based on explainable
machine learning(ML) methods that explain the activity and behaviour of dairy
cattle (cows). Continuous data collection of 3-axis accelerometer sensors and
usage of robust ML methodologies and algorithms, provide farmers and
researchers with actionable information on cattle activity, allowing farmers to
make informed decisions and incorporate sustainable practices. This study
utilizes Bluetooth-based Internet of Things (IoT) devices and 4G networks for
seamless data transmission, immediate analysis, inference generation, and
explains the models performance with explainability frameworks. Special
emphasis is put on the pre-processing of the accelerometers time series data,
including the extraction of statistical characteristics, signal processing
techniques, and lag-based features using the sliding window technique. Various
hyperparameter-optimized ML models are evaluated across varying window lengths
for activity classification. The k-nearest neighbour Classifier achieved the
best performance, with AUC of mean 0.98 and standard deviation of 0.0026 on the
training set and 0.99 on testing set). In order to ensure transparency,
Explainable AI based frameworks such as SHAP is used to interpret feature
importance that can be understood and used by practitioners. A detailed
comparison of the important features, along with the stability analysis of
selected features, supports development of explainable and practical ML models
for sustainable livestock management.

</details>


### [164] [AI-Driven Detection and Analysis of Handwriting on Seized Ivory: A Tool to Uncover Criminal Networks in the Illicit Wildlife Trade](https://arxiv.org/abs/2508.10219)
*Will Fein,Ryan J. Horwitz,John E. Brown III,Amit Misra,Felipe Oviedo,Kevin White,Juan M. Lavista Ferres,Samuel K. Wasser*

Main category: cs.LG

TL;DR: 本研究提出了一种由人工智能驱动的管道，用于提取和分析查获的象牙上的手写标记，从而提供了一种新颖、可扩展且低成本的法医证据来源。


<details>
  <summary>Details</summary>
Motivation: 跨境象牙贸易继续导致非洲各地的大象数量下降，贩运网络仍然难以 disruption。

Method: 使用对象检测模型提取超过 17,000 个单独的标记，然后使用最先进的 AI 工具进行标记和描述。

Result: 我们识别出 184 个重复出现的“签名标记”，这些标记连接了出现它们的象牙。在多次查获中观察到 20 个签名标记，通过参与两批货物的贩运者，在这些查获之间建立了法医联系。

Conclusion: 本研究展示了人工智能在野生动物取证方面的变革潜力，并强调了将笔迹分析整合到破坏有组织的野生动物犯罪的努力中的实际步骤。

Abstract: The transnational ivory trade continues to drive the decline of elephant
populations across Africa, and trafficking networks remain difficult to
disrupt. Tusks seized by law enforcement officials carry forensic information
on the traffickers responsible for their export, including DNA evidence and
handwritten markings made by traffickers. For 20 years, analyses of tusk DNA
have identified where elephants were poached and established connections among
shipments of ivory. While the links established using genetic evidence are
extremely conclusive, genetic data is expensive and sometimes impossible to
obtain. But though handwritten markings are easy to photograph, they are rarely
documented or analyzed. Here, we present an AI-driven pipeline for extracting
and analyzing handwritten markings on seized elephant tusks, offering a novel,
scalable, and low-cost source of forensic evidence. Having collected 6,085
photographs from eight large seizures of ivory over a 6-year period
(2014-2019), we used an object detection model to extract over 17,000
individual markings, which were then labeled and described using
state-of-the-art AI tools. We identified 184 recurring "signature markings"
that connect the tusks on which they appear. 20 signature markings were
observed in multiple seizures, establishing forensic links between these
seizures through traffickers involved in both shipments. This work complements
other investigative techniques by filling in gaps where other data sources are
unavailable. The study demonstrates the transformative potential of AI in
wildlife forensics and highlights practical steps for integrating handwriting
analysis into efforts to disrupt organized wildlife crime.

</details>


### [165] [Comparison of D-Wave Quantum Annealing and Markov Chain Monte Carlo for Sampling from a Probability Distribution of a Restricted Boltzmann Machine](https://arxiv.org/abs/2508.10228)
*Abdelmoula El Yazizi,Samee U. Khan,Yaroslav Koshka*

Main category: cs.LG

TL;DR: 该论文比较了D-Wave和Gibbs采样在RBM训练中的性能，发现D-Wave在局部谷的数量上略有优势，但两种技术发现的局部谷差异较大，尤其是在训练后期，这表明经典-量子组合方法可能具有改进潜力。


<details>
  <summary>Details</summary>
Motivation: 评估受限玻尔兹曼机（RBM）的采样质量。

Method: 将局部谷（LV）中心方法应用于评估D-Wave量子退火器的最新一代产品的采样质量。在与基于对比散度的RBM学习相关的条件下，获得了D-Wave和来自经典训练的RBM的Gibbs样本。比较了样本所属的LV数量和相应局部最小值的能量。

Result: 通过减少D-Wave退火时间，并未实现局部谷（LV）数量的显著（理想）增加。在任何训练时期，D-Wave采样的状态都比Gibbs采样属于更高数量的LV。然而，这两种技术发现的许多LV都不同。对于高概率采样状态，这两种技术的不互补性和重叠性更高（不利）。

Conclusion: 该研究的结果可能解释了之前使用D-Wave采样未能取得显著改进的原因，但也揭示了改进的潜力，例如使用经典-量子组合方法。

Abstract: A local-valley (LV) centered approach to assessing the quality of sampling
from Restricted Boltzmann Machines (RBMs) was applied to the latest generation
of the D-Wave quantum annealer. D-Wave and Gibbs samples from a classically
trained RBM were obtained at conditions relevant to the
contrastive-divergence-based RBM learning. The samples were compared for the
number of the LVs to which they belonged and the energy of the corresponding
local minima. No significant (desirable) increase in the number of the LVs has
been achieved by decreasing the D-Wave annealing time. At any training epoch,
the states sampled by the D-Wave belonged to a somewhat higher number of LVs
than in the Gibbs sampling. However, many of those LVs found by the two
techniques differed. For high-probability sampled states, the two techniques
were (unfavorably) less complementary and more overlapping. Nevertheless, many
potentially "important" local minima, i.e., those having intermediate, even if
not high, probability values, were found by only one of the two sampling
techniques while missed by the other. The two techniques overlapped less at
later than earlier training epochs, which is precisely the stage of the
training when modest improvements to the sampling quality could make meaningful
differences for the RBM trainability. The results of this work may explain the
failure of previous investigations to achieve substantial (or any) improvement
when using D-Wave-based sampling. However, the results reveal some potential
for improvement, e.g., using a combined classical-quantum approach.

</details>


### [166] [Interpretable Machine Learning Model for Early Prediction of Acute Kidney Injury in Critically Ill Patients with Cirrhosis: A Retrospective Study](https://arxiv.org/abs/2508.10233)
*Li Sun,Shuheng Chen,Junyi Fan,Yong Si,Minoo Ahmadi,Elham Pishgar,Kamiar Alaei,Maryam Pishgar*

Main category: cs.LG

TL;DR: 本研究开发了一种基于LightGBM的可解释机器学习模型，用于对危重肝硬化患者进行早期AKI预测。


<details>
  <summary>Details</summary>
Motivation: 肝硬化是一种进行性肝病，死亡率高，并发症频繁，尤其是急性肾损伤（AKI），多达50%的住院患者发生AKI，并使预后恶化。AKI源于复杂的血流动力学、炎症和代谢变化，因此早期检测至关重要。许多预测工具缺乏准确性、可解释性和与重症监护病房（ICU）工作流程的一致性。

Method: 我们对MIMIC-IV v2.2数据库进行了回顾性分析，确定了1240名患有肝硬化的成年ICU患者，并排除了ICU停留时间少于48小时或缺少关键数据的患者。提取了前48小时的实验室和生理变量。该流程包括预处理、缺失值过滤、LASSO特征选择和SMOTE类别平衡。使用AUROC、准确率、F1分数、灵敏度、特异性和预测值对六种算法（LightGBM、CatBoost、XGBoost、logistic回归、naive Bayes和神经网络）进行了训练和评估。

Result: LightGBM取得了最佳性能（AUROC 0.808，95% CI 0.741-0.856；准确率0.704；NPV 0.911）。关键预测指标包括凝血酶原部分时间延长、未进行院外20G置管、低pH值和pO2改变，这与已知的肝硬化-AKI机制相符，并提示了可操作的目标。

Conclusion: LightGBM模型能够利用常规临床变量，对ICU中患有肝硬化的患者进行准确的早期AKI风险分层。其高阴性预测值支持对低风险患者进行安全降级，并且可解释性可以培养临床医生的信任和有针对性的预防。

Abstract: Background: Cirrhosis is a progressive liver disease with high mortality and
frequent complications, notably acute kidney injury (AKI), which occurs in up
to 50% of hospitalized patients and worsens outcomes. AKI stems from complex
hemodynamic, inflammatory, and metabolic changes, making early detection
essential. Many predictive tools lack accuracy, interpretability, and alignment
with intensive care unit (ICU) workflows. This study developed an interpretable
machine learning model for early AKI prediction in critically ill patients with
cirrhosis.
  Methods: We conducted a retrospective analysis of the MIMIC-IV v2.2 database,
identifying 1240 adult ICU patients with cirrhosis and excluding those with ICU
stays under 48 hours or missing key data. Laboratory and physiological
variables from the first 48 hours were extracted. The pipeline included
preprocessing, missingness filtering, LASSO feature selection, and SMOTE class
balancing. Six algorithms-LightGBM, CatBoost, XGBoost, logistic regression,
naive Bayes, and neural networks-were trained and evaluated using AUROC,
accuracy, F1-score, sensitivity, specificity, and predictive values.
  Results: LightGBM achieved the best performance (AUROC 0.808, 95% CI
0.741-0.856; accuracy 0.704; NPV 0.911). Key predictors included prolonged
partial thromboplastin time, absence of outside-facility 20G placement, low pH,
and altered pO2, consistent with known cirrhosis-AKI mechanisms and suggesting
actionable targets.
  Conclusion: The LightGBM-based model enables accurate early AKI risk
stratification in ICU patients with cirrhosis using routine clinical variables.
Its high negative predictive value supports safe de-escalation for low-risk
patients, and interpretability fosters clinician trust and targeted prevention.
External validation and integration into electronic health record systems are
warranted.

</details>


### [167] [Can Transformers Break Encryption Schemes via In-Context Learning?](https://arxiv.org/abs/2508.10235)
*Jathin Korrapati,Patrick Mendoza,Aditya Tomar,Abein Abraham*

Main category: cs.LG

TL;DR: 本文研究了使用上下文学习的transformer模型在密码学函数学习中的应用，特别是单字母替换和Vigenère密码。


<details>
  <summary>Details</summary>
Motivation: 评估transformers在ICL范式下的归纳偏置和泛化能力。

Method: 使用transformer模型，通过上下文学习(ICL)的方式，在不更新参数的情况下，根据少量的密文和明文对推断出隐藏的替换规则并解码新的密文。

Result: 模型能够在给定少量(密文，明文)对的情况下，推断出潜在的替换并解码新的密文。

Conclusion: 该研究提出了ICL在密码学函数学习领域中的新应用，重点关注单字母替换和Vigenère密码等密码。

Abstract: In-context learning (ICL) has emerged as a powerful capability of
transformer-based language models, enabling them to perform tasks by
conditioning on a small number of examples presented at inference time, without
any parameter updates. Prior work has shown that transformers can generalize
over simple function classes like linear functions, decision trees, even neural
networks, purely from context, focusing on numerical or symbolic reasoning over
underlying well-structured functions. Instead, we propose a novel application
of ICL into the domain of cryptographic function learning, specifically
focusing on ciphers such as mono-alphabetic substitution and Vigen\`ere
ciphers, two classes of private-key encryption schemes. These ciphers involve a
fixed but hidden bijective mapping between plain text and cipher text
characters. Given a small set of (cipher text, plain text) pairs, the goal is
for the model to infer the underlying substitution and decode a new cipher text
word. This setting poses a structured inference challenge, which is well-suited
for evaluating the inductive biases and generalization capabilities of
transformers under the ICL paradigm. Code is available at
https://github.com/adistomar/CS182-project.

</details>


### [168] [Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on Transformer Models](https://arxiv.org/abs/2508.10243)
*Taibiao Zhao,Mingxuan Sun,Hao Wang,Xiaobing Chen,Xiangwei Zhou*

Main category: cs.LG

TL;DR: 提出了一种新的transformer后门攻击方法HPMI，该方法无需重训练，通过剪枝和注入恶意头来实现攻击，具有高攻击成功率和良好的隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 以往的后门攻击方法通常依赖于使用干净数据进行重训练或改变模型架构，这两种方法都需要消耗大量资源并且具有侵入性。Transformer模型容易受到后门攻击。

Method: 提出了一种新的无需重训练的后门攻击方法HPMI，该方法通过剪枝最不重要的头并注入预先训练的恶意头来建立后门。

Result: HPMI实现了至少99.55%的攻击成功率，并且能够绕过四种先进的防御机制。此外，HPMI在各种防御策略下实现了更好的隐藏性和鲁棒性，同时保持了对clean accuracy的最小影响。

Conclusion: HPMI在保持最小clean accuracy损失的同时，实现了至少99.55%的攻击成功率，并绕过了四种先进的防御机制。相对于依赖于重训练的攻击，HPMI在各种防御策略下实现了更好的隐藏性和鲁棒性，同时保持了对clean accuracy的最小影响。

Abstract: Transformer models have demonstrated exceptional performance and have become
indispensable in computer vision (CV) and natural language processing (NLP)
tasks. However, recent studies reveal that transformers are susceptible to
backdoor attacks. Prior backdoor attack methods typically rely on retraining
with clean data or altering the model architecture, both of which can be
resource-intensive and intrusive. In this paper, we propose Head-wise Pruning
and Malicious Injection (HPMI), a novel retraining-free backdoor attack on
transformers that does not alter the model's architecture. Our approach
requires only a small subset of the original data and basic knowledge of the
model architecture, eliminating the need for retraining the target transformer.
Technically, HPMI works by pruning the least important head and injecting a
pre-trained malicious head to establish the backdoor. We provide a rigorous
theoretical justification demonstrating that the implanted backdoor resists
detection and removal by state-of-the-art defense techniques, under reasonable
assumptions. Experimental evaluations across multiple datasets further validate
the effectiveness of HPMI, showing that it 1) incurs negligible clean accuracy
loss, 2) achieves at least 99.55% attack success rate, and 3) bypasses four
advanced defense mechanisms. Additionally, relative to state-of-the-art
retraining-dependent attacks, HPMI achieves greater concealment and robustness
against diverse defense strategies, while maintaining minimal impact on clean
accuracy.

</details>


### [169] [Convergence Analysis of Max-Min Exponential Neural Network Operators in Orlicz Space](https://arxiv.org/abs/2508.10248)
*Satyaranjan Pradhan,Madan Mohan Soren*

Main category: cs.LG

TL;DR: 本文提出了一种 Max Min 方法，用于使用指数神经网络算子逼近函数，并研究了其逼近性质和收敛行为


<details>
  <summary>Details</summary>
Motivation: 使用指数神经网络算子逼近函数

Method: 提出了一种使用指数神经网络算子逼近函数的 Max Min 方法，并开发了 Max Min Kantorovich 型指数神经网络算子

Result: 研究了单变量函数的逐点收敛和一致收敛性。使用对数连续模量分析收敛阶数并估计相应的收敛速度。提供了一些图形表示，以说明通过合适的核函数和 sigmoid 激活函数的函数逼近误差

Conclusion: 研究了 Max Min Kantorovich 型指数神经网络算子在 Orlicz 空间中的收敛行为

Abstract: In this current work, we propose a Max Min approach for approximating
functions using exponential neural network operators. We extend this framework
to develop the Max Min Kantorovich-type exponential neural network operators
and investigate their approximation properties. We study both pointwise and
uniform convergence for univariate functions. To analyze the order of
convergence, we use the logarithmic modulus of continuity and estimate the
corresponding rate of convergence. Furthermore, we examine the convergence
behavior of the Max Min Kantorovich type exponential neural network operators
within the Orlicz space setting. We provide some graphical representations to
illustrate the approximation error of the function through suitable kernel and
sigmoidal activation functions.

</details>


### [170] [Multi-Agent Reinforcement Learning for Adaptive Resource Orchestration in Cloud-Native Clusters](https://arxiv.org/abs/2508.10253)
*Guanzi Yao,Heyao Liu,Linyan Dai*

Main category: cs.LG

TL;DR: A multi-agent reinforcement learning method is proposed to address resource dynamism and scheduling complexity in cloud-native database systems. It outperforms traditional methods in resource utilization, scheduling latency, policy convergence, system stability, and fairness.


<details>
  <summary>Details</summary>
Motivation: challenges of high resource dynamism and scheduling complexity in cloud-native database systems

Method: adaptive resource orchestration method based on multi-agent reinforcement learning with heterogeneous role-based agent modeling and a reward-shaping mechanism

Result: The proposed method outperforms traditional approaches across multiple key metrics, including resource utilization, scheduling latency, policy convergence speed, system stability, and fairness.

Conclusion: The proposed multi-agent reinforcement learning method outperforms traditional approaches in cloud-native database systems, demonstrating strong generalization and practical utility in handling orchestration tasks with high concurrency, high-dimensional state spaces, and complex dependency relationships.

Abstract: This paper addresses the challenges of high resource dynamism and scheduling
complexity in cloud-native database systems. It proposes an adaptive resource
orchestration method based on multi-agent reinforcement learning. The method
introduces a heterogeneous role-based agent modeling mechanism. This allows
different resource entities, such as compute nodes, storage nodes, and
schedulers, to adopt distinct policy representations. These agents are better
able to reflect diverse functional responsibilities and local environmental
characteristics within the system. A reward-shaping mechanism is designed to
integrate local observations with global feedback. This helps mitigate policy
learning bias caused by incomplete state observations. By combining real-time
local performance signals with global system value estimation, the mechanism
improves coordination among agents and enhances policy convergence stability. A
unified multi-agent training framework is developed and evaluated on a
representative production scheduling dataset. Experimental results show that
the proposed method outperforms traditional approaches across multiple key
metrics. These include resource utilization, scheduling latency, policy
convergence speed, system stability, and fairness. The results demonstrate
strong generalization and practical utility. Across various experimental
scenarios, the method proves effective in handling orchestration tasks with
high concurrency, high-dimensional state spaces, and complex dependency
relationships. This confirms its advantages in real-world, large-scale
scheduling environments.

</details>


### [171] [Federated Anomaly Detection for Multi-Tenant Cloud Platforms with Personalized Modeling](https://arxiv.org/abs/2508.10255)
*Yuxi Wang,Heyao Liu,Nyutian Long,Guanzi Yao*

Main category: cs.LG

TL;DR: Federated learning enhances anomaly detection in clouds, improving privacy and accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing data privacy, heterogeneous resource behavior, and limitations of centralized modeling in multi-tenant clouds.

Method: A federated training framework with personalized parameter adjustment and Mahalanobis distance for anomaly scoring.

Result: The method outperforms existing models in Precision, Recall, and F1-Score, maintaining stable performance under varying conditions.

Conclusion: The proposed federated learning-based anomaly detection method demonstrates superior performance compared to existing models in cloud environments, showing robustness and accuracy in resource monitoring and anomaly diagnosis.

Abstract: This paper proposes an anomaly detection method based on federated learning
to address key challenges in multi-tenant cloud environments, including data
privacy leakage, heterogeneous resource behavior, and the limitations of
centralized modeling. The method establishes a federated training framework
involving multiple tenants. Each tenant trains the model locally using private
resource usage data. Through parameter aggregation, a global model is
optimized, enabling cross-tenant collaborative anomaly detection while
preserving data privacy. To improve adaptability to diverse resource usage
patterns, a personalized parameter adjustment mechanism is introduced. This
allows the model to retain tenant-specific feature representations while
sharing global knowledge. In the model output stage, the Mahalanobis distance
is used to compute anomaly scores. This enhances both the accuracy and
stability of anomaly detection. The experiments use real telemetry data from a
cloud platform to construct a simulated multi-tenant environment. The study
evaluates the model's performance under varying participation rates and noise
injection levels. These comparisons demonstrate the proposed method's
robustness and detection accuracy. Experimental results show that the proposed
method outperforms existing mainstream models across key metrics such as
Precision, Recall, and F1-Score. It also maintains stable performance in
various complex scenarios. These findings highlight the method's practical
potential for intelligent resource monitoring and anomaly diagnosis in cloud
computing environments.

</details>


### [172] [Source Component Shift Adaptation via Offline Decomposition and Online Mixing Approach](https://arxiv.org/abs/2508.10257)
*Ryuta Matsuno*

Main category: cs.LG

TL;DR: proposes a source component shift adaptation method via offline decomposition and online mixing, which determines prediction models and updates the mixing weight of the prediction models for precise prediction through online convex optimization.


<details>
  <summary>Details</summary>
Motivation: Existing online learning methods often fail to utilize recurring shifts effectively, while model-pool-based methods struggle to capture individual source components, leading to poor adaptation.

Method: a source component shift adaptation method via an offline decomposition and online mixing approach. It first determines prediction models, each of which learns a source component solely based on past training data offline through the EM algorithm. Then, it updates the mixing weight of the prediction models for precise prediction through online convex optimization.

Result: achieving superior adaptation performance over existing methods, reducing the cumulative test loss by up to 67.4%.

Conclusion: This method fully leverages the characteristics of the shifts, achieving superior adaptation performance over existing methods. Experiments conducted on various real-world regression datasets demonstrate that this method outperforms baselines, reducing the cumulative test loss by up to 67.4%.

Abstract: This paper addresses source component shift adaptation, aiming to update
predictions adapting to source component shifts for incoming data streams based
on past training data. Existing online learning methods often fail to utilize
recurring shifts effectively, while model-pool-based methods struggle to
capture individual source components, leading to poor adaptation. In this
paper, we propose a source component shift adaptation method via an offline
decomposition and online mixing approach. We theoretically identify that the
problem can be divided into two subproblems: offline source component
decomposition and online mixing weight adaptation. Based on this, our method
first determines prediction models, each of which learns a source component
solely based on past training data offline through the EM algorithm. Then, it
updates the mixing weight of the prediction models for precise prediction
through online convex optimization. Thanks to our theoretical derivation, our
method fully leverages the characteristics of the shifts, achieving superior
adaptation performance over existing methods. Experiments conducted on various
real-world regression datasets demonstrate that our method outperforms
baselines, reducing the cumulative test loss by up to 67.4%.

</details>


### [173] [Uncertainty-Aware Prediction of Parkinson's Disease Medication Needs: A Two-Stage Conformal Prediction Approach](https://arxiv.org/abs/2508.10284)
*Ricardo Diaz-Rincon,Muxuan Liang,Adolfo Ramirez-Zamora,Benjamin Shickel*

Main category: cs.LG

TL;DR: Developed a conformal prediction framework for Parkinson's medication management, providing reliable prediction intervals for medication needs up to two years in advance, enabling evidence-based decisions on levodopa dosing.


<details>
  <summary>Details</summary>
Motivation: Current approaches rely on trial-and-error decisions without systematic predictive methods. Clinicians require not only predictions of future medication needs but also reliable confidence measures. Without quantified uncertainty, adjustments risk premature escalation to maximum doses or prolonged inadequate symptom control.

Method: a two-stage approach identifies patients likely to need medication changes, then predicts required levodopa equivalent daily dose adjustments.

Result: Our framework achieved marginal coverage while reducing prediction interval lengths compared to traditional approaches, providing precise predictions for short-term planning and wider ranges for long-term forecasting.

Conclusion: We developed a conformal prediction framework anticipating medication needs up to two years in advance with reliable prediction intervals and statistical guarantees. By quantifying uncertainty, our approach enables evidence-based decisions about levodopa dosing, optimizing symptom control while minimizing side effects and improving life quality.

Abstract: Parkinson's Disease (PD) medication management presents unique challenges due
to heterogeneous disease progression and treatment response. Neurologists must
balance symptom control with optimal dopaminergic dosing based on functional
disability while minimizing side effects. This balance is crucial as inadequate
or abrupt changes can cause levodopa-induced dyskinesia, wearing off, and
neuropsychiatric effects, significantly reducing quality of life. Current
approaches rely on trial-and-error decisions without systematic predictive
methods. Despite machine learning advances, clinical adoption remains limited
due to reliance on point predictions that do not account for prediction
uncertainty, undermining clinical trust and utility. Clinicians require not
only predictions of future medication needs but also reliable confidence
measures. Without quantified uncertainty, adjustments risk premature escalation
to maximum doses or prolonged inadequate symptom control. We developed a
conformal prediction framework anticipating medication needs up to two years in
advance with reliable prediction intervals and statistical guarantees. Our
approach addresses zero-inflation in PD inpatient data, where patients maintain
stable medication regimens between visits. Using electronic health records from
631 inpatient admissions at University of Florida Health (2011-2021), our
two-stage approach identifies patients likely to need medication changes, then
predicts required levodopa equivalent daily dose adjustments. Our framework
achieved marginal coverage while reducing prediction interval lengths compared
to traditional approaches, providing precise predictions for short-term
planning and wider ranges for long-term forecasting. By quantifying
uncertainty, our approach enables evidence-based decisions about levodopa
dosing, optimizing symptom control while minimizing side effects and improving
life quality.

</details>


### [174] [SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic Representation Learning](https://arxiv.org/abs/2508.10298)
*Weijian Mai,Jiamin Wu,Yu Zhu,Zhouheng Yao,Dongzhan Zhou,Andrew F. Luo,Qihao Zheng,Wanli Ouyang,Chunfeng Song*

Main category: cs.LG

TL;DR: SynBrain是一种新的生成框架，它以概率和生物可解释的方式模拟从视觉语义到神经反应的转换，在视觉到fMRI编码性能方面超过了现有技术水平。


<details>
  <summary>Details</summary>
Motivation: 现有的确定性方法难以同时模拟这种生物可变性，同时捕捉编码刺激信息的潜在功能一致性。

Method: SynBrain，一个生成框架，它以概率和生物可解释的方式模拟从视觉语义到神经反应的转换。SynBrain引入了两个关键组件：（i）BrainVAE通过概率学习将神经表征建模为连续概率分布，同时通过视觉语义约束保持功能一致性；（ii）语义到神经映射器充当语义传输通路，将视觉语义投影到神经反应流形中，以促进高保真fMRI合成。

Result: SynBrain在特定被试的视觉到fMRI编码性能方面超过了最先进的方法。此外，SynBrain能够以少量数据有效地适应新的被试，并合成高质量的fMRI信号，这些信号在提高数据受限的fMRI到图像解码性能方面非常有效。

Conclusion: SynBrain能够捕捉试验和被试之间的一致性，合成的信号能够捕捉由生物神经可变性形成的、可解释的模式。

Abstract: Deciphering how visual stimuli are transformed into cortical responses is a
fundamental challenge in computational neuroscience. This visual-to-neural
mapping is inherently a one-to-many relationship, as identical visual inputs
reliably evoke variable hemodynamic responses across trials, contexts, and
subjects. However, existing deterministic methods struggle to simultaneously
model this biological variability while capturing the underlying functional
consistency that encodes stimulus information. To address these limitations, we
propose SynBrain, a generative framework that simulates the transformation from
visual semantics to neural responses in a probabilistic and biologically
interpretable manner. SynBrain introduces two key components: (i) BrainVAE
models neural representations as continuous probability distributions via
probabilistic learning while maintaining functional consistency through visual
semantic constraints; (ii) A Semantic-to-Neural Mapper acts as a semantic
transmission pathway, projecting visual semantics into the neural response
manifold to facilitate high-fidelity fMRI synthesis. Experimental results
demonstrate that SynBrain surpasses state-of-the-art methods in
subject-specific visual-to-fMRI encoding performance. Furthermore, SynBrain
adapts efficiently to new subjects with few-shot data and synthesizes
high-quality fMRI signals that are effective in improving data-limited
fMRI-to-image decoding performance. Beyond that, SynBrain reveals functional
consistency across trials and subjects, with synthesized signals capturing
interpretable patterns shaped by biological neural variability. The code will
be made publicly available.

</details>


### [175] [Improving Learning of New Diseases through Knowledge-Enhanced Initialization for Federated Adapter Tuning](https://arxiv.org/abs/2508.10299)
*Danni Peng,Yuan Wang,Kangning Cai,Peiyan Ning,Jiming Xu,Yong Liu,Rick Siow Mong Goh,Qingsong Wei,Huazhu Fu*

Main category: cs.LG

TL;DR: This paper introduces FedKEI, a federated learning framework that uses past knowledge to help clients quickly adapt to new healthcare tasks. It outperforms existing methods on various medical datasets.


<details>
  <summary>Details</summary>
Motivation: Given the rapidly evolving healthcare environment, it is crucial for individual clients to quickly adapt to new tasks or diseases by tuning adapters while drawing upon past experiences. In healthcare, federated learning (FL) is a widely adopted framework that enables privacy-preserving collaboration among medical institutions. With large foundation models (FMs) demonstrating impressive capabilities, using FMs in FL through cost-efficient adapter tuning has become a popular approach.

Method: Federated Knowledge-Enhanced Initialization (FedKEI), a novel framework that leverages cross-client and cross-task transfer from past knowledge to generate informed initializations for learning new tasks with adapters. FedKEI begins with a global clustering process at the server to generalize knowledge across tasks, followed by the optimization of aggregation weights across clusters (inter-cluster weights) and within each cluster (intra-cluster weights) to personalize knowledge transfer for each new task. To facilitate more effective learning of the inter- and intra-cluster weights, we adopt a bi-level optimization scheme that collaboratively learns the global intra-cluster weights across clients and optimizes the local inter-cluster weights toward each client's task objective.

Result: FedKEI's advantage in adapting to new diseases compared to state-of-the-art methods.

Conclusion: Extensive experiments on three benchmark datasets of different modalities, including dermatology, chest X-rays, and retinal OCT, demonstrate FedKEI's advantage in adapting to new diseases compared to state-of-the-art methods.

Abstract: In healthcare, federated learning (FL) is a widely adopted framework that
enables privacy-preserving collaboration among medical institutions. With large
foundation models (FMs) demonstrating impressive capabilities, using FMs in FL
through cost-efficient adapter tuning has become a popular approach. Given the
rapidly evolving healthcare environment, it is crucial for individual clients
to quickly adapt to new tasks or diseases by tuning adapters while drawing upon
past experiences. In this work, we introduce Federated Knowledge-Enhanced
Initialization (FedKEI), a novel framework that leverages cross-client and
cross-task transfer from past knowledge to generate informed initializations
for learning new tasks with adapters. FedKEI begins with a global clustering
process at the server to generalize knowledge across tasks, followed by the
optimization of aggregation weights across clusters (inter-cluster weights) and
within each cluster (intra-cluster weights) to personalize knowledge transfer
for each new task. To facilitate more effective learning of the inter- and
intra-cluster weights, we adopt a bi-level optimization scheme that
collaboratively learns the global intra-cluster weights across clients and
optimizes the local inter-cluster weights toward each client's task objective.
Extensive experiments on three benchmark datasets of different modalities,
including dermatology, chest X-rays, and retinal OCT, demonstrate FedKEI's
advantage in adapting to new diseases compared to state-of-the-art methods.

</details>


### [176] [A Vision-Language Pre-training Model-Guided Approach for Mitigating Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2508.10315)
*Keke Gai,Dongjue Wang,Jing Yu,Liehuang Zhu,Qi Wu*

Main category: cs.LG

TL;DR: 该论文提出了一种新的联邦学习后门防御框架CLIP-Fed，它利用视觉-语言预训练模型，通过整合预聚合和后聚合防御策略，在异构数据分布下有效防御后门攻击，并在代表性数据集上取得了良好的实验结果。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习（FL）后门防御方法依赖于同质客户端数据分布或干净的服务数据集的可用性，这限制了实用性和有效性。在异构客户端数据分布下防御后门攻击，同时保持模型性能仍然是一个重大挑战。

Method: 该论文提出了一种名为CLIP-Fed的联邦学习后门防御框架，该框架利用视觉-语言预训练模型的零样本学习能力。通过整合预聚合和后聚合防御策略，CLIP-Fed克服了非独立同分布对防御有效性的限制。为了解决隐私问题，并增强数据集对不同触发器的覆盖，该论文使用多模态大型语言模型和频率分析构建并扩充服务器数据集，而无需任何客户端样本。为了解决由后门样本引起的类原型偏差，并消除触发模式和目标标签之间的相关性，CLIP-Fed使用原型对比损失和Kullback-Leibler散度对齐增强数据集上全局模型和CLIP的知识。

Result: CLIP-Fed在CIFAR-10和CIFAR-10-LT上实现了平均ASR降低，同时提高了平均MA。

Conclusion: CLIP-Fed在代表性数据集上进行了广泛的实验验证，结果表明，与最先进的方法相比，CLIP-Fed在CIFAR-10上的ASR平均降低了2.03%，在CIFAR-10-LT上的ASR平均降低了1.35%，同时平均MA分别提高了7.92%和0.48%。

Abstract: Existing backdoor defense methods in Federated Learning (FL) rely on the
assumption of homogeneous client data distributions or the availability of a
clean serve dataset, which limits the practicality and effectiveness. Defending
against backdoor attacks under heterogeneous client data distributions while
preserving model performance remains a significant challenge. In this paper, we
propose a FL backdoor defense framework named CLIP-Fed, which leverages the
zero-shot learning capabilities of vision-language pre-training models. By
integrating both pre-aggregation and post-aggregation defense strategies,
CLIP-Fed overcomes the limitations of Non-IID imposed on defense effectiveness.
To address privacy concerns and enhance the coverage of the dataset against
diverse triggers, we construct and augment the server dataset using the
multimodal large language model and frequency analysis without any client
samples. To address class prototype deviations caused by backdoor samples and
eliminate the correlation between trigger patterns and target labels, CLIP-Fed
aligns the knowledge of the global model and CLIP on the augmented dataset
using prototype contrastive loss and Kullback-Leibler divergence. Extensive
experiments on representative datasets validate the effectiveness of CLIP-Fed.
Compared to state-of-the-art methods, CLIP-Fed achieves an average reduction in
ASR, i.e., 2.03\% on CIFAR-10 and 1.35\% on CIFAR-10-LT, while improving
average MA by 7.92\% and 0.48\%, respectively.

</details>


### [177] [Welfare-Centric Clustering](https://arxiv.org/abs/2508.10345)
*Claire Jie Zhang,Seyed A. Esmaeili,Jamie Morgenstern*

Main category: cs.LG

TL;DR: This paper introduces welfare-centric fair clustering algorithms with theoretical guarantees that outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional fair clustering notions may yield undesirable outcomes. Dickerson et al. (2025) advocated for a welfare-centric clustering approach.

Method: The paper models group utilities based on distances and proportional representation and formalizes two optimization objectives: the Rawlsian (Egalitarian) objective and the Utilitarian objective. Novel algorithms are introduced for both objectives.

Result: The proposed methods significantly outperform existing fair clustering baselines on multiple real-world datasets.

Conclusion: The paper introduces novel algorithms for Rawlsian and Utilitarian objectives in welfare-centric clustering and proves theoretical guarantees. Empirical results show significant outperformance compared to existing fair clustering baselines.

Abstract: Fair clustering has traditionally focused on ensuring equitable group
representation or equalizing group-specific clustering costs. However,
Dickerson et al. (2025) recently showed that these fairness notions may yield
undesirable or unintuitive clustering outcomes and advocated for a
welfare-centric clustering approach that models the utilities of the groups. In
this work, we model group utilities based on both distances and proportional
representation and formalize two optimization objectives based on
welfare-centric clustering: the Rawlsian (Egalitarian) objective and the
Utilitarian objective. We introduce novel algorithms for both objectives and
prove theoretical guarantees for them. Empirical evaluations on multiple
real-world datasets demonstrate that our methods significantly outperform
existing fair clustering baselines.

</details>


### [178] [A Hierarchical IDS for Zero-Day Attack Detection in Internet of Medical Things Networks](https://arxiv.org/abs/2508.10346)
*Md Ashraf Uddin,Nam H. Chu,Reza Rafeh*

Main category: cs.LG

TL;DR: Proposes a multi-level IoMT IDS framework capable of detecting zero-day attacks with high accuracy using meta-learning and One Class Classification.


<details>
  <summary>Details</summary>
Motivation: The Internet of Medical Things (IoMT) is vulnerable to cyberattacks, and traditional centralized Intrusion Detection Systems (IDSs) are unsuitable due to response delays, privacy risks, and added vulnerabilities. Running IDSs locally on IoMT devices is often infeasible due to limited computation.

Method: A multi-level IoMT IDS framework using meta-learning or One Class Classification (OCC) with the usfAD algorithm for the first layer, and subsequent layers for attack type and novelty identification.

Result: Experiments on the CICIoMT2024 dataset show 99.77 percentage accuracy and 97.8 percentage F1-score. The first layer detects zero-day attacks with high accuracy without needing new datasets.

Conclusion: A multi-level IoMT IDS framework is proposed that can detect zero-day attacks and distinguish between known and unknown threats. Experiments on the CICIoMT2024 dataset show high accuracy and F1-score, with the first layer detecting zero-day attacks accurately without needing new datasets.

Abstract: The Internet of Medical Things (IoMT) is driving a healthcare revolution but
remains vulnerable to cyberattacks such as denial of service, ransomware, data
hijacking, and spoofing. These networks comprise resource constrained,
heterogeneous devices (e.g., wearable sensors, smart pills, implantables),
making traditional centralized Intrusion Detection Systems (IDSs) unsuitable
due to response delays, privacy risks, and added vulnerabilities. Centralized
IDSs require all sensors to transmit data to a central server, causing delays
or network disruptions in dense environments. Running IDSs locally on IoMT
devices is often infeasible due to limited computation, and even lightweight
IDS components remain at risk if updated models are delayed leaving them
exposed to zero-day attacks that threaten patient health and data security. We
propose a multi level IoMT IDS framework capable of detecting zero day attacks
and distinguishing between known and unknown threats. The first layer (near
Edge) filters traffic at a coarse level (attack or not) using meta-learning or
One Class Classification (OCC) with the usfAD algorithm. Subsequent layers (far
Edge, Cloud) identify attack type and novelty. Experiments on the CICIoMT2024
dataset show 99.77 percentage accuracy and 97.8 percentage F1-score. The first
layer detects zero-day attacks with high accuracy without needing new datasets,
ensuring strong applicability in IoMT environments. Additionally, the
meta-learning approach achieves high.

</details>


### [179] [Semantic Communication with Distribution Learning through Sequential Observations](https://arxiv.org/abs/2508.10350)
*Samer Lahoud,Kinda Khawam*

Main category: cs.LG

TL;DR: 研究了语义通信中的分布学习，发现即时语义性能和长期可学习性之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 语义通信旨在传达意义，而不是完美的比特复制，代表了从传统通信的范式转变。传统语义通信优化个体意义传输，但当先验未知时，我们建立了学习源统计的基本条件。

Method: 研究语义通信中的分布学习，接收器必须通过顺序观察来推断潜在的意义分布。

Result: 可学习性需要有效传输矩阵的满秩，表征分布估计的收敛速度，并量化估计误差如何转化为语义失真。分析揭示了一个根本的权衡：为即时语义性能优化的编码方案通常会牺牲长期可学习性。在 CIFAR-10 上的实验验证了我们的理论框架。

Conclusion: 系统条件反射严重影响学习率和可实现的性能。需要在即时性能和适应能力之间取得平衡。

Abstract: Semantic communication aims to convey meaning rather than bit-perfect
reproduction, representing a paradigm shift from traditional communication.
This paper investigates distribution learning in semantic communication where
receivers must infer the underlying meaning distribution through sequential
observations. While semantic communication traditionally optimizes individual
meaning transmission, we establish fundamental conditions for learning source
statistics when priors are unknown. We prove that learnability requires full
rank of the effective transmission matrix, characterize the convergence rate of
distribution estimation, and quantify how estimation errors translate to
semantic distortion. Our analysis reveals a fundamental trade-off: encoding
schemes optimized for immediate semantic performance often sacrifice long-term
learnability. Experiments on CIFAR-10 validate our theoretical framework,
demonstrating that system conditioning critically impacts both learning rate
and achievable performance. These results provide the first rigorous
characterization of statistical learning in semantic communication and offer
design principles for systems that balance immediate performance with
adaptation capability.

</details>


### [180] [eMamba: Efficient Acceleration Framework for Mamba Models in Edge Computing](https://arxiv.org/abs/2508.10370)
*Jiyong Kim,Jaeho Lee,Jiahao Lin,Alish Kanani,Miao Sun,Umit Y. Ogras,Jaehyun Park*

Main category: cs.LG

TL;DR: eMamba is a hardware acceleration framework for deploying Mamba models on edge platforms. It achieves comparable accuracy with fewer parameters and lower latency than baseline solutions.


<details>
  <summary>Details</summary>
Motivation: Mamba, a recent sequence-to-sequence SSM, offers competitive accuracy with superior computational efficiency compared to state-of-the-art transformer models. While this advantage makes Mamba particularly promising for resource-constrained edge devices, no hardware acceleration frameworks are currently optimized for deploying it in such environments.

Method: This paper presents eMamba, a comprehensive end-to-end hardware acceleration framework explicitly designed for deploying Mamba models on edge platforms. eMamba maximizes computational efficiency by replacing complex normalization layers with lightweight hardware-aware alternatives and approximating expensive operations, such as SiLU activation and exponentiation, considering the target applications. Then, it performs an approximation-aware neural architecture search (NAS) to tune the learnable parameters used during approximation.

Result: Evaluations with Fashion-MNIST, CIFAR-10, and MARS show eMamba achieves comparable accuracy to state-of-the-art techniques using 1.63-19.9$\times$ fewer parameters. Experimental results show 4.95-5.62$\times$ lower latency and 2.22-9.95$\times$ higher throughput, with 4.77$\times$ smaller area, 9.84$\times$ lower power, and 48.6$\times$ lower energy consumption than baseline solutions while maintaining competitive accuracy.

Conclusion: eMamba achieves comparable accuracy to state-of-the-art techniques using fewer parameters, generalizes well to large-scale natural language tasks, and demonstrates lower latency, higher throughput, smaller area, lower power, and lower energy consumption than baseline solutions.

Abstract: State Space Model (SSM)-based machine learning architectures have recently
gained significant attention for processing sequential data. Mamba, a recent
sequence-to-sequence SSM, offers competitive accuracy with superior
computational efficiency compared to state-of-the-art transformer models. While
this advantage makes Mamba particularly promising for resource-constrained edge
devices, no hardware acceleration frameworks are currently optimized for
deploying it in such environments. This paper presents eMamba, a comprehensive
end-to-end hardware acceleration framework explicitly designed for deploying
Mamba models on edge platforms. eMamba maximizes computational efficiency by
replacing complex normalization layers with lightweight hardware-aware
alternatives and approximating expensive operations, such as SiLU activation
and exponentiation, considering the target applications. Then, it performs an
approximation-aware neural architecture search (NAS) to tune the learnable
parameters used during approximation. Evaluations with Fashion-MNIST, CIFAR-10,
and MARS, an open-source human pose estimation dataset, show eMamba achieves
comparable accuracy to state-of-the-art techniques using 1.63-19.9$\times$
fewer parameters. In addition, it generalizes well to large-scale natural
language tasks, demonstrating stable perplexity across varying sequence lengths
on the WikiText2 dataset. We also quantize and implement the entire eMamba
pipeline on an AMD ZCU102 FPGA and ASIC using GlobalFoundries (GF) 22 nm
technology. Experimental results show 4.95-5.62$\times$ lower latency and
2.22-9.95$\times$ higher throughput, with 4.77$\times$ smaller area,
9.84$\times$ lower power, and 48.6$\times$ lower energy consumption than
baseline solutions while maintaining competitive accuracy.

</details>


### [181] [A Unified Evaluation Framework for Multi-Annotator Tendency Learning](https://arxiv.org/abs/2508.10393)
*Liyun Zhang,Jingcheng Ke,Shenli Fan,Xuanmeng Sha,Zheng Lian*

Main category: cs.LG

TL;DR: 本文提出了一个评估多标注学习中个体倾向学习 (ITL) 方法的统一框架，包含 DIC 和 BAE 两种新指标。


<details>
  <summary>Details</summary>
Motivation: 现有的多标注学习研究侧重于个体倾向学习 (ITL)，但缺乏评估 ITL 方法是否真正捕捉个体倾向并提供有意义的行为解释的评估框架。

Method: 本文提出了两种新的指标：(1) Difference of Inter-annotator Consistency (DIC) 和 (2) Behavior Alignment Explainability (BAE)。

Result: 提出的评估框架有效。

Conclusion: 本文提出了一个统一的评估框架，通过大量实验验证了其有效性。

Abstract: Recent works have emerged in multi-annotator learning that shift focus from
Consensus-oriented Learning (CoL), which aggregates multiple annotations into a
single ground-truth prediction, to Individual Tendency Learning (ITL), which
models annotator-specific labeling behavior patterns (i.e., tendency) to
provide explanation analysis for understanding annotator decisions. However, no
evaluation framework currently exists to assess whether ITL methods truly
capture individual tendencies and provide meaningful behavioral explanations.
To address this gap, we propose the first unified evaluation framework with two
novel metrics: (1) Difference of Inter-annotator Consistency (DIC) quantifies
how well models capture annotator tendencies by comparing predicted
inter-annotator similarity structures with ground-truth; (2) Behavior Alignment
Explainability (BAE) evaluates how well model explanations reflect annotator
behavior and decision relevance by aligning explainability-derived with
ground-truth labeling similarity structures via Multidimensional Scaling (MDS).
Extensive experiments validate the effectiveness of our proposed evaluation
framework.

</details>


### [182] [XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization](https://arxiv.org/abs/2508.10395)
*Aditya Tomar,Coleman Hooper,Minjae Lee,Haocheng Xi,Rishabh Tiwari,Wonjun Kang,Luca Manolache,Michael W. Mahoney,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: XQuant通过低比特量化显著降低内存消耗，相对于最先进的KV缓存量化方法，具有显著的精度优势。


<details>
  <summary>Details</summary>
Motivation: 由于大量的内存占用和带宽需求，高效地推断LLM具有挑战性。在过去的几十年里，计算能力稳步超过了内存容量和带宽，这种趋势在现代GPU硬件中仍然很明显，并加剧了LLM推理的挑战。因此，新的算法正在涌现，它们用增加的计算来换取减少的内存操作。

Method: 通过量化和缓存层输入激活X，而不是使用标准KV缓存，然后在推理期间动态地重新构建Keys和Values来实现。

Result: XQuant实现了高达$\\\sim 7.7\\times$的内存节省，与FP16基线相比，复杂度降低<0.1。XQuant-CL实现了相对于FP16基线高达10倍的内存节省，复杂度降低仅为0.01，内存节省12.5倍，复杂度降低仅为$0.1。

Conclusion: XQuant利用硬件平台快速增长的计算能力来消除内存瓶颈，同时超越了最先进的KV缓存量化方法，并在各种模型上实现了接近FP16的精度。

Abstract: Although LLM inference has emerged as a critical workload for many downstream
applications, efficiently inferring LLMs is challenging due to the substantial
memory footprint and bandwidth requirements. In parallel, compute capabilities
have steadily outpaced both memory capacity and bandwidth over the last few
decades, a trend that remains evident in modern GPU hardware and exacerbates
the challenge of LLM inference. As such, new algorithms are emerging that trade
increased computation for reduced memory operations. To that end, we present
XQuant, which takes advantage of this trend, enabling an order-of-magnitude
reduction in memory consumption through low-bit quantization with substantial
accuracy benefits relative to state-of-the-art KV cache quantization methods.
We accomplish this by quantizing and caching the layer input activations X,
instead of using standard KV caching, and then rematerializing the Keys and
Values on-the-fly during inference. This results in an immediate 2$\times$
memory savings compared to KV caching. By applying XQuant, we achieve up to
$\sim 7.7\times$ memory savings with $<0.1$ perplexity degradation compared to
the FP16 baseline. Furthermore, our approach leverages the fact that X values
are similar across layers. Building on this observation, we introduce
XQuant-CL, which exploits the cross-layer similarity in the X embeddings for
extreme compression. Across different models, XQuant-CL attains up to
10$\times$ memory savings relative to the FP16 baseline with only 0.01
perplexity degradation, and 12.5$\times$ memory savings with only $0.1$
perplexity degradation. XQuant exploits the rapidly increasing compute
capabilities of hardware platforms to eliminate the memory bottleneck, while
surpassing state-of-the-art KV cache quantization methods and achieving
near-FP16 accuracy across a wide range of models.

</details>


### [183] [SC2Arena and StarEvolve: Benchmark and Self-Improvement Framework for LLMs in Complex Decision-Making Tasks](https://arxiv.org/abs/2508.10428)
*Pengbo Shen,Yaqing Wang,Ni Mu,Yao Luan,Runpeng Xie,Senhao Yang,Lexiang Wang,Hao Hu,Shuang Xu,Yiqin Yang,Bo Xu*

Main category: cs.LG

TL;DR: 提出了SC2Arena基准测试和StarEvolve框架，以提高LLM在星际争霸II中的战略规划能力。


<details>
  <summary>Details</summary>
Motivation: 在复杂决策中评估大型语言模型（LLM）对于提升人工智能的战略规划和实时适应能力至关重要。然而，现有的星际争霸II等任务的基准测试未能捕捉到游戏的完整复杂性，例如完整的游戏环境、多样化的行动空间和所有可玩的种族。为了解决这一差距，我们提出了SC2Arena基准测试。

Method: 我们提出了StarEvolve，一个集成了战略规划和战术执行的分层框架，具有迭代自我纠正和通过高质量游戏数据进行微调的持续改进功能。它的关键组件包括一个用于分解游戏玩法的Planner-Executor-Verifier结构，以及一个用于选择高质量训练样本的评分系统。

Result: 使用SC2Arena进行的综合分析为开发通用智能体提供了有价值的见解，这在以前的基准测试中是不可能的。实验结果还表明，我们提出的StarEvolve在战略规划中取得了优异的性能。

Conclusion: StarEvolve在战略规划中表现出色，代码、环境和算法均已公开。

Abstract: Evaluating large language models (LLMs) in complex decision-making is
essential for advancing AI's ability for strategic planning and real-time
adaptation. However, existing benchmarks for tasks like StarCraft II fail to
capture the game's full complexity, such as its complete game context, diverse
action spaces, and all playable races. To address this gap, we present
SC2Arena, a benchmark that fully supports all playable races, low-level action
spaces, and optimizes text-based observations to tackle spatial reasoning
challenges. Complementing this, we introduce StarEvolve, a hierarchical
framework that integrates strategic planning with tactical execution, featuring
iterative self-correction and continuous improvement via fine-tuning on
high-quality gameplay data. Its key components include a
Planner-Executor-Verifier structure to break down gameplay, and a scoring
system for selecting high-quality training samples. Comprehensive analysis
using SC2Arena provides valuable insights into developing generalist agents
that were not possible with previous benchmarks. Experimental results also
demonstrate that our proposed StarEvolve achieves superior performance in
strategic planning. Our code, environment, and algorithms are publicly
available.

</details>


### [184] [Unpacking the Implicit Norm Dynamics of Sharpness-Aware Minimization in Tensorized Models](https://arxiv.org/abs/2508.10435)
*Tianxiao Cao,Kyohei Atarashi,Hisashi Kashima*

Main category: cs.LG

TL;DR: This paper analyzes SAM's norm dynamics in tensorized models, introduces Norm Deviation, and proposes Deviation-Aware Scaling (DAS) to improve performance and reduce computational cost.


<details>
  <summary>Details</summary>
Motivation: explore the behavior of SAM in more general tensorized or scale-invariant models, and SAM's implicit control of Norm Deviation is governed by the covariance between core norms and their gradient magnitudes.

Method: gradient flow analysis to derive the evolution of Norm Deviation under SAM

Result: SAM's implicit control of Norm Deviation is governed by the covariance between core norms and their gradient magnitudes.

Conclusion: Deviation-Aware Scaling (DAS) achieves competitive or improved performance over SAM with reduced computational overhead in tensor completion, noisy training, model compression, and parameter-efficient fine-tuning.

Abstract: Sharpness-Aware Minimization (SAM) has been proven to be an effective
optimization technique for improving generalization in overparameterized
models. While prior works have explored the implicit regularization of SAM in
simple two-core scale-invariant settings, its behavior in more general
tensorized or scale-invariant models remains underexplored. In this work, we
leverage scale-invariance to analyze the norm dynamics of SAM in general
tensorized models. We introduce the notion of \emph{Norm Deviation} as a global
measure of core norm imbalance, and derive its evolution under SAM using
gradient flow analysis. We show that SAM's implicit control of Norm Deviation
is governed by the covariance between core norms and their gradient magnitudes.
Motivated by these findings, we propose a simple yet effective method,
\emph{Deviation-Aware Scaling (DAS)}, which explicitly mimics this
regularization behavior by scaling core norms in a data-adaptive manner. Our
experiments across tensor completion, noisy training, model compression, and
parameter-efficient fine-tuning confirm that DAS achieves competitive or
improved performance over SAM, while offering reduced computational overhead.

</details>


### [185] [RealAC: A Domain-Agnostic Framework for Realistic and Actionable Counterfactual Explanations](https://arxiv.org/abs/2508.10455)
*Asiful Arefeen,Shovito Barua Soumma,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: RealAC 是一种与领域无关的框架，用于生成 realistic and actionable counterfactuals，它通过对齐特征对的联合分布来自动保留复杂的特征间依赖关系，并允许用户“冻结”他们不想更改的属性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常通过 rigid, hand-crafted 的约束或领域特定的知识来加强特征间的依赖关系，这限制了它们的通用性和捕捉数据中固有的复杂、非线性关系的能力。此外，它们很少适应用户指定的偏好，并提出因果关系上不合理或不可行的解释。

Method: RealAC 是一种与领域无关的框架，用于生成 realistic and actionable counterfactuals。它通过对齐 factual 和 counterfactual 实例之间的特征对的联合分布，自动保留复杂的特征间依赖关系，而无需依赖显式的领域知识。该框架还允许最终用户通过在优化过程中抑制 frozen features 的变化来“冻结”他们不能或不希望更改的属性。

Result: 在三个合成数据集和两个真实数据集上的评估表明，RealAC 在真实性和可操作性之间取得了平衡。我们的方法在 causal edge score, dependency preservation score, and IM1 realism metric 上优于现有技术水平的基线和基于大型语言模型的 counterfactual generation 技术。

Conclusion: RealAC在因果关系感知和以用户为中心的 counterfactual generation 方面提供了一种解决方案，并在 causal edge score, dependency preservation score, and IM1 realism metric 上优于现有技术水平的基线和基于大型语言模型的 counterfactual generation 技术。

Abstract: Counterfactual explanations provide human-understandable reasoning for
AI-made decisions by describing minimal changes to input features that would
alter a model's prediction. To be truly useful in practice, such explanations
must be realistic and feasible -- they should respect both the underlying data
distribution and user-defined feasibility constraints. Existing approaches
often enforce inter-feature dependencies through rigid, hand-crafted
constraints or domain-specific knowledge, which limits their generalizability
and ability to capture complex, nonlinear relations inherent in data. Moreover,
they rarely accommodate user-specified preferences and suggest explanations
that are causally implausible or infeasible to act upon. We introduce RealAC, a
domain-agnostic framework for generating realistic and actionable
counterfactuals. RealAC automatically preserves complex inter-feature
dependencies without relying on explicit domain knowledge -- by aligning the
joint distributions of feature pairs between factual and counterfactual
instances. The framework also allows end-users to ``freeze'' attributes they
cannot or do not wish to change by suppressing change in frozen features during
optimization. Evaluations on three synthetic and two real datasets demonstrate
that RealAC balances realism with actionability. Our method outperforms
state-of-the-art baselines and Large Language Model-based counterfactual
generation techniques in causal edge score, dependency preservation score, and
IM1 realism metric and offers a solution for causality-aware and user-centric
counterfactual generation.

</details>


### [186] [X-Node: Self-Explanation is All We Need](https://arxiv.org/abs/2508.10461)
*Prajit Sengupta,Islem Rekik*

Main category: cs.LG

TL;DR: X-Node：一种自解释图神经网络框架，可在保持竞争性分类准确率的同时生成忠实的、每个节点的解释。


<details>
  <summary>Details</summary>
Motivation: 图神经网络 (GNN) 通过捕获数据实例之间的结构依赖性，在计算机视觉和医学图像分类任务中取得了最先进的结果。然而，它们的决策在很大程度上是不透明的，这限制了它们在高风险临床应用中的可信度，而在这些应用中，可解释性至关重要。GNN 现有的可解释性技术通常是事后的和全局的，对单个节点决策或局部推理的洞察力有限。

Method: 我们引入了X-Node，这是一个自解释的GNN框架，其中每个节点都会在预测过程中生成自己的解释。对于每个节点，我们构建一个结构化的上下文向量，编码可解释的线索，例如其局部拓扑中的度、中心性、聚类、特征显着性和标签一致性。轻量级的Reasoner模块将此上下文映射到紧凑的解释向量，该向量具有三个目的：(1)通过解码器重建节点的潜在嵌入，以增强保真度，(2)使用预训练的LLM（例如Grok或Gemini）生成自然语言解释，以及(3)通过“文本注入”机制指导GNN本身，该机制将解释反馈到消息传递管道中。

Result: 我们的结果表明，X-Node 在保持竞争性分类准确率的同时，能够生成忠实的、每个节点的解释。

Conclusion: X-Node在保持竞争性分类准确率的同时，能够生成忠实的、每个节点的解释。

Abstract: Graph neural networks (GNNs) have achieved state-of-the-art results in
computer vision and medical image classification tasks by capturing structural
dependencies across data instances. However, their decision-making remains
largely opaque, limiting their trustworthiness in high-stakes clinical
applications where interpretability is essential. Existing explainability
techniques for GNNs are typically post-hoc and global, offering limited insight
into individual node decisions or local reasoning. We introduce X-Node, a
self-explaining GNN framework in which each node generates its own explanation
as part of the prediction process. For every node, we construct a structured
context vector encoding interpretable cues such as degree, centrality,
clustering, feature saliency, and label agreement within its local topology. A
lightweight Reasoner module maps this context into a compact explanation
vector, which serves three purposes: (1) reconstructing the node's latent
embedding via a decoder to enforce faithfulness, (2) generating a natural
language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3)
guiding the GNN itself via a "text-injection" mechanism that feeds explanations
back into the message-passing pipeline. We evaluate X-Node on two graph
datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT,
and GIN backbones. Our results show that X-Node maintains competitive
classification accuracy while producing faithful, per-node explanations.
Repository: https://github.com/basiralab/X-Node.

</details>


### [187] [GraphFedMIG: Tackling Class Imbalance in Federated Graph Learning via Mutual Information-Guided Generation](https://arxiv.org/abs/2508.10471)
*Xinrui Li,Qilin Fan,Tianfu Wang,Kaiwen Wei,Ke Yu,Xu Zhang*

Main category: cs.LG

TL;DR: This paper introduces GraphFedMIG, a federated graph learning framework that addresses class imbalance by using a hierarchical GAN and mutual information guidance to augment minority class features.


<details>
  <summary>Details</summary>
Motivation: Federated graph learning (FGL) is critically challenged by statistical heterogeneity, particularly class imbalance, which causes the global model to become biased and fail at identifying rare events. This is exacerbated in FGL as nodes from a minority class are often surrounded by biased neighborhood information.

Method: The paper proposes GraphFedMIG, a novel FGL framework that reframes the problem as a federated generative data augmentation task, employing a hierarchical generative adversarial network with a mutual information-guided mechanism.

Result: GraphFedMIG can generate high-fidelity feature representations and focuses on producing high-value, minority-class features.

Conclusion: The experimental results on four real-world datasets demonstrate the superiority of the proposed GraphFedMIG compared with other baselines.

Abstract: Federated graph learning (FGL) enables multiple clients to collaboratively
train powerful graph neural networks without sharing their private,
decentralized graph data. Inherited from generic federated learning, FGL is
critically challenged by statistical heterogeneity, where non-IID data
distributions across clients can severely impair model performance. A
particularly destructive form of this is class imbalance, which causes the
global model to become biased towards majority classes and fail at identifying
rare but critical events. This issue is exacerbated in FGL, as nodes from a
minority class are often surrounded by biased neighborhood information,
hindering the learning of expressive embeddings. To grapple with this
challenge, we propose GraphFedMIG, a novel FGL framework that reframes the
problem as a federated generative data augmentation task. GraphFedMIG employs a
hierarchical generative adversarial network where each client trains a local
generator to synthesize high-fidelity feature representations. To provide
tailored supervision, clients are grouped into clusters, each sharing a
dedicated discriminator. Crucially, the framework designs a mutual
information-guided mechanism to steer the evolution of these client generators.
By calculating each client's unique informational value, this mechanism
corrects the local generator parameters, ensuring that subsequent rounds of
mutual information-guided generation are focused on producing high-value,
minority-class features. We conduct extensive experiments on four real-world
datasets, and the results demonstrate the superiority of the proposed
GraphFedMIG compared with other baselines.

</details>


### [188] [EDAPT: Towards Calibration-Free BCIs with Continual Online Adaptation](https://arxiv.org/abs/2508.10474)
*Lisa Haxel,Jaivardhan Kapoor,Ulf Ziemann,Jakob H. Macke*

Main category: cs.LG

TL;DR: EDAPT 是一种与任务和模型无关的框架，它通过持续的模型适应来消除校准，从而提高了脑机接口 (BCI) 的准确性。


<details>
  <summary>Details</summary>
Motivation: 脑机接口 (BCI) 会因神经信号随时间漂移以及用户之间的差异而导致准确性下降，这需要频繁的重新校准，从而限制了实际部署。

Method: EDAPT 首先使用来自多个用户的数据训练基线解码器，然后通过监督微调不断地个性化该模型，因为神经模式在使用过程中会不断演变。

Result: 我们测试了 EDAPT 在涵盖三个 BCI 任务的九个数据集上，发现它始终优于传统的静态方法。这些改进主要来自结合人群层面的预训练和在线持续微调，而无监督域适应在某些数据集上提供了进一步的提升。EDAPT 运行效率很高，可以在消费级硬件上在 200 毫秒内更新模型。最后，解码精度会随着总数据预算而扩展，而不是在受试者和试验之间的分配。

Conclusion: EDAPT 提供了一条通往免校准 BCI 的实用途径，减少了 BCI 部署的主要障碍。

Abstract: Brain-computer interfaces (BCIs) suffer from accuracy degradation as neural
signals drift over time and vary across users, requiring frequent recalibration
that limits practical deployment. We introduce EDAPT, a task- and
model-agnostic framework that eliminates calibration through continual model
adaptation. EDAPT first trains a baseline decoder using data from multiple
users, then continually personalizes this model via supervised finetuning as
the neural patterns evolve during use. We tested EDAPT across nine datasets
covering three BCI tasks, and found that it consistently improved accuracy over
conventional, static methods. These improvements primarily stem from combining
population-level pretraining and online continual finetuning, with unsupervised
domain adaptation providing further gains on some datasets. EDAPT runs
efficiently, updating models within 200 milliseconds on consumer-grade
hardware. Finally, decoding accuracy scales with total data budget rather than
its allocation between subjects and trials. EDAPT provides a practical pathway
toward calibration-free BCIs, reducing a major barrier to BCI deployment.

</details>


### [189] [Pinet: Optimizing hard-constrained neural networks with orthogonal projection layers](https://arxiv.org/abs/2508.10480)
*Panagiotis D. Grontas,Antonio Terpin,Efe C. Balta,Raffaello D'Andrea,John Lygeros*

Main category: cs.LG

TL;DR: $\\Pi$net 是一种新的神经网络输出层，可确保满足凸约束，并在各种优化问题中优于传统求解器和最先进的学习方法。


<details>
  <summary>Details</summary>
Motivation: 介绍一个神经网络的输出层，以确保满足凸约束。

Method: 利用算子分裂进行快速可靠的投影，并利用隐函数定理进行反向传播。

Result: 当解决单个问题时，获得比传统求解器更快的适度精度解决方案，而对于批量问题，则快得多。

Conclusion: $\\Pi$net在训练时间、解决方案质量和超参数调整的鲁棒性方面超越了最先进的学习方法，同时保持了相似的推理时间。$\\Pi$net可以解决具有非凸轨迹偏好的多车辆运动规划问题，并提供了一个在JAX中实现的GPU就绪软件包，具有有效的调整启发式方法。

Abstract: We introduce an output layer for neural networks that ensures satisfaction of
convex constraints. Our approach, $\Pi$net, leverages operator splitting for
rapid and reliable projections in the forward pass, and the implicit function
theorem for backpropagation. We deploy $\Pi$net as a feasible-by-design
optimization proxy for parametric constrained optimization problems and obtain
modest-accuracy solutions faster than traditional solvers when solving a single
problem, and significantly faster for a batch of problems. We surpass
state-of-the-art learning approaches in terms of training time, solution
quality, and robustness to hyperparameter tuning, while maintaining similar
inference times. Finally, we tackle multi-vehicle motion planning with
non-convex trajectory preferences and provide $\Pi$net as a GPU-ready package
implemented in JAX with effective tuning heuristics.

</details>


### [190] [Learning State-Space Models of Dynamic Systems from Arbitrary Data using Joint Embedding Predictive Architectures](https://arxiv.org/abs/2508.10489)
*Jonas Ulmen,Ganesh Sundaram,Daniel Görges*

Main category: cs.LG

TL;DR: introduce a novel technique for creating world models using continuous-time dynamic systems from arbitrary observation data, which appear to be more capable than reconstruction-based methods


<details>
  <summary>Details</summary>
Motivation: With the advent of Joint Embedding Predictive Architectures (JEPAs), which appear to be more capable than reconstruction-based methods

Method: integrates sequence embeddings with neural ordinary differential equations (neural ODEs). It employs loss functions that enforce contractive embeddings and Lipschitz constants in state transitions to construct a well-organized latent state space.

Result: demonstrated through the generation of structured latent state-space models for a simple pendulum system using only image data.

Conclusion: This paper introduces a novel technique for creating world models using continuous-time dynamic systems from arbitrary observation data, opening up a new technique for developing more general control algorithms and estimation techniques with broad applications in robotics.

Abstract: With the advent of Joint Embedding Predictive Architectures (JEPAs), which
appear to be more capable than reconstruction-based methods, this paper
introduces a novel technique for creating world models using continuous-time
dynamic systems from arbitrary observation data. The proposed method integrates
sequence embeddings with neural ordinary differential equations (neural ODEs).
It employs loss functions that enforce contractive embeddings and Lipschitz
constants in state transitions to construct a well-organized latent state
space. The approach's effectiveness is demonstrated through the generation of
structured latent state-space models for a simple pendulum system using only
image data. This opens up a new technique for developing more general control
algorithms and estimation techniques with broad applications in robotics.

</details>


### [191] [On the Complexity-Faithfulness Trade-off of Gradient-Based Explanations](https://arxiv.org/abs/2508.10490)
*Amir Mehrpanah,Matteo Gamba,Kevin Smith,Hossein Azizpour*

Main category: cs.LG

TL;DR: 论文提出了一个频谱框架来分析和提高 ReLU 网络解释的平滑性和忠实性。


<details>
  <summary>Details</summary>
Motivation: ReLU 网络虽然在视觉数据中很流行，但具有明显的转变，有时依赖于单个像素进行预测，这使得vanilla基于梯度的解释noisy且难以解释。现有的方法，例如 GradCAM，通过产生替代模型来平滑这些解释，但代价是保真度。

Method: 论文引入了一个统一的频谱框架来系统地分析和量化解释中的平滑性、忠实性及其权衡。

Result: 论文验证了不同设计选择、数据集和消融实验的理论结果。

Conclusion: 论文通过量化 ReLU 网络对高频信息的贡献来提供一种识别平滑性和忠实性之间权衡的原则性方法。该分析描述了基于替代的平滑如何扭曲解释，从而导致“解释差距”。

Abstract: ReLU networks, while prevalent for visual data, have sharp transitions,
sometimes relying on individual pixels for predictions, making vanilla
gradient-based explanations noisy and difficult to interpret. Existing methods,
such as GradCAM, smooth these explanations by producing surrogate models at the
cost of faithfulness. We introduce a unifying spectral framework to
systematically analyze and quantify smoothness, faithfulness, and their
trade-off in explanations. Using this framework, we quantify and regularize the
contribution of ReLU networks to high-frequency information, providing a
principled approach to identifying this trade-off. Our analysis characterizes
how surrogate-based smoothing distorts explanations, leading to an
``explanation gap'' that we formally define and measure for different post-hoc
methods. Finally, we validate our theoretical findings across different design
choices, datasets, and ablations.

</details>


### [192] [Contrastive ECOC: Learning Output Codes for Adversarial Defense](https://arxiv.org/abs/2508.10491)
*Che-Yu Chou,Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: This paper introduces three models for automated codebook learning based on contrastive learning, allowing codebooks to be learned directly and adaptively from data, and demonstrate superior robustness to adversarial attacks compared to two baselines.


<details>
  <summary>Details</summary>
Motivation: Traditional ECOC methods rely on manually designed or randomly generated codebooks, which are labor-intensive and may yield suboptimal, dataset-agnostic results. One-hot encoding is not always the most effective encoding mechanism.

Method: automated codebook learning based on contrastive learning

Result: demonstrate superior robustness to adversarial attacks compared to two baselines across four datasets

Conclusion: This paper introduces three models for automated codebook learning based on contrastive learning, allowing codebooks to be learned directly and adaptively from data.  Across four datasets, the proposed models demonstrate superior robustness to adversarial attacks compared to two baselines.

Abstract: Although one-hot encoding is commonly used for multiclass classification, it
is not always the most effective encoding mechanism. Error Correcting Output
Codes (ECOC) address multiclass classification by mapping each class to a
unique codeword used as a label. Traditional ECOC methods rely on manually
designed or randomly generated codebooks, which are labor-intensive and may
yield suboptimal, dataset-agnostic results. This paper introduces three models
for automated codebook learning based on contrastive learning, allowing
codebooks to be learned directly and adaptively from data. Across four
datasets, our proposed models demonstrate superior robustness to adversarial
attacks compared to two baselines. The source is available at
https://github.com/YuChou20/Automated-Codebook-Learning-with-Error-Correcting-Output-Code-Technique.

</details>


### [193] [A Unified Multi-Agent Framework for Universal Multimodal Understanding and Generation](https://arxiv.org/abs/2508.10494)
*Jiulin Li,Ping Huang,Yexin Li,Shuo Chen,Juewen Hu,Ye Tian*

Main category: cs.LG

TL;DR: MAGUS是一个用于多模态理解和生成的模块化框架，它优于强大的基线和最先进的系统，而无需联合训练。


<details>
  <summary>Details</summary>
Motivation: 现实世界的多模态应用通常需要any-to-any的能力，从而能够跨文本、图像、音频和视频等模态进行理解和生成。然而，整合自回归语言模型(llm)在推理方面的优势和扩散模型在高保真生成方面的优势仍然具有挑战性。现有的方法依赖于 rigid 管道或紧密耦合的架构，限制了灵活性和可伸缩性。

Method: MAGUS (Multi-Agent Guided Unified Multimodal System)，一个模块化框架，通过两个解耦阶段统一多模态理解和生成：认知和审议。MAGUS支持在共享文本工作空间中进行符号多智能体协作。认知阶段包括三个角色条件多模态LLM代理——感知器、计划器和反思器——进行协作对话，以执行结构化理解和计划。审议阶段包含一种增长感知搜索机制，该机制以相互增强的方式协调基于llm的推理和基于扩散的生成。

Result: MAGUS支持即插即用可扩展性、可伸缩的any-to-any模态转换和语义对齐——所有这些都不需要联合训练。在包括图像、视频和音频生成以及跨模态指令跟随在内的多个基准上的实验表明，MAGUS优于强大的基线和最先进的系统。

Conclusion: MAGUS在多个基准测试中优于强大的基线和最先进的系统，并在MME基准测试中超过了强大的闭源模型GPT-4o。

Abstract: Real-world multimodal applications often require any-to-any capabilities,
enabling both understanding and generation across modalities including text,
image, audio, and video. However, integrating the strengths of autoregressive
language models (LLMs) for reasoning and diffusion models for high-fidelity
generation remains challenging. Existing approaches rely on rigid pipelines or
tightly coupled architectures, limiting flexibility and scalability. We propose
MAGUS (Multi-Agent Guided Unified Multimodal System), a modular framework that
unifies multimodal understanding and generation via two decoupled phases:
Cognition and Deliberation. MAGUS enables symbolic multi-agent collaboration
within a shared textual workspace. In the Cognition phase, three
role-conditioned multimodal LLM agents - Perceiver, Planner, and Reflector -
engage in collaborative dialogue to perform structured understanding and
planning. The Deliberation phase incorporates a Growth-Aware Search mechanism
that orchestrates LLM-based reasoning and diffusion-based generation in a
mutually reinforcing manner. MAGUS supports plug-and-play extensibility,
scalable any-to-any modality conversion, and semantic alignment - all without
the need for joint training. Experiments across multiple benchmarks, including
image, video, and audio generation, as well as cross-modal instruction
following, demonstrate that MAGUS outperforms strong baselines and
state-of-the-art systems. Notably, on the MME benchmark, MAGUS surpasses the
powerful closed-source model GPT-4o.

</details>


### [194] [Nonlocal Monte Carlo via Reinforcement Learning](https://arxiv.org/abs/2508.10520)
*Dmitrii Dobrynin,Masoud Mohseni,John Paul Strachan*

Main category: cs.LG

TL;DR: We employ deep reinforcement learning to train the nonlocal transition policies of NMC and show that the trained policies improve upon the standard MCMC-based and nonlocal simulated annealing on hard uniform random and scale-free random 4-SAT benchmarks.


<details>
  <summary>Details</summary>
Motivation: conventional MCMC struggles to unfreeze rigid variables, escape suboptimal basins of attraction, and sample high-quality and diverse solutions

Method: deep reinforcement learning (RL) to train the nonlocal transition policies of NMC

Result: resulting solver can be trained solely by observing energy changes of the configuration space exploration as RL rewards and the local minimum energy landscape geometry as RL states

Conclusion: The trained policies improve upon the standard MCMC-based and nonlocal simulated annealing on hard uniform random and scale-free random 4-SAT benchmarks in terms of residual energy, time-to-solution, and diversity of solutions metrics.

Abstract: Optimizing or sampling complex cost functions of combinatorial optimization
problems is a longstanding challenge across disciplines and applications. When
employing family of conventional algorithms based on Markov Chain Monte Carlo
(MCMC) such as simulated annealing or parallel tempering, one assumes
homogeneous (equilibrium) temperature profiles across input. This instance
independent approach was shown to be ineffective for the hardest benchmarks
near a computational phase transition when the so-called overlap-gap-property
holds. In these regimes conventional MCMC struggles to unfreeze rigid
variables, escape suboptimal basins of attraction, and sample high-quality and
diverse solutions. In order to mitigate these challenges, Nonequilibrium
Nonlocal Monte Carlo (NMC) algorithms were proposed that leverage inhomogeneous
temperature profiles thereby accelerating exploration of the configuration
space without compromising its exploitation. Here, we employ deep reinforcement
learning (RL) to train the nonlocal transition policies of NMC which were
previously designed phenomenologically. We demonstrate that the resulting
solver can be trained solely by observing energy changes of the configuration
space exploration as RL rewards and the local minimum energy landscape geometry
as RL states. We further show that the trained policies improve upon the
standard MCMC-based and nonlocal simulated annealing on hard uniform random and
scale-free random 4-SAT benchmarks in terms of residual energy,
time-to-solution, and diversity of solutions metrics.

</details>


### [195] [Projected Coupled Diffusion for Test-Time Constrained Joint Generation](https://arxiv.org/abs/2508.10531)
*Hao Luan,Yi Xian Goh,See-Kiong Ng,Chun Kai Ling*

Main category: cs.LG

TL;DR: PCD是一种用于约束联合生成的新型测试时框架，通过在生成动态中引入耦合引导项，并结合投影步骤来加强硬约束，从而实现扩散模型之间的协调。


<details>
  <summary>Details</summary>
Motivation: 在测试时对采样进行修改已经成为扩散算法的一个重要扩展，其目标是在不必重新训练整个扩散模型的情况下，偏置生成过程以实现给定的目标。然而，从多个预训练的扩散模型中生成联合相关的样本，同时在没有昂贵的重新训练的情况下，执行特定于任务的约束仍然具有挑战性。

Method: PCD引入了耦合引导项到生成动态中，以鼓励扩散模型之间的协调，并在每个扩散步骤中加入一个投影步骤，以加强硬约束。

Result: 结果显示出改进的耦合效果和保证的约束满足，且没有产生过多计算成本。

Conclusion: PCD在图像对生成、物体操作和多机器人运动规划等应用场景中表现出有效性，在没有产生过多计算成本的情况下，结果显示出改进的耦合效果和保证的约束满足。

Abstract: Modifications to test-time sampling have emerged as an important extension to
diffusion algorithms, with the goal of biasing the generative process to
achieve a given objective without having to retrain the entire diffusion model.
However, generating jointly correlated samples from multiple pre-trained
diffusion models while simultaneously enforcing task-specific constraints
without costly retraining has remained challenging. To this end, we propose
Projected Coupled Diffusion (PCD), a novel test-time framework for constrained
joint generation. PCD introduces a coupled guidance term into the generative
dynamics to encourage coordination between diffusion models and incorporates a
projection step at each diffusion step to enforce hard constraints.
Empirically, we demonstrate the effectiveness of PCD in application scenarios
of image-pair generation, object manipulation, and multi-robot motion planning.
Our results show improved coupling effects and guaranteed constraint
satisfaction without incurring excessive computational costs.

</details>


### [196] [Driving Accurate Allergen Prediction with Protein Language Models and Generalization-Focused Evaluation](https://arxiv.org/abs/2508.10541)
*Brian Shing-Hei Wong,Joshua Mincheol Kim,Sin-Hang Fung,Qing Xiong,Kelvin Fu-Kiu Ao,Junkang Wei,Ran Wang,Dan Michelle Wang,Jingying Zhou,Bo Feng,Alfred Sze-Lok Cheng,Kevin Y. Yip,Stephen Kwok-Wing Tsui,Qin Cao*

Main category: cs.LG

TL;DR: Applm, a computational framework that leverages the 100-billion parameter xTrimoPGLM protein language model,  outperforms seven state-of-the-art methods in identifying allergen proteins.


<details>
  <summary>Details</summary>
Motivation: Allergens, typically proteins capable of triggering adverse immune responses, represent a significant public health challenge.

Method: Applm (Allergen Prediction with Protein Language Models), a computational framework that leverages the 100-billion parameter xTrimoPGLM protein language model

Result: Applm consistently outperforms seven state-of-the-art methods in a diverse set of tasks that closely resemble difficult real-world scenarios.

Conclusion: xTrimoPGLM is crucial for Applm's performance by detecting important differences among protein sequences.

Abstract: Allergens, typically proteins capable of triggering adverse immune responses,
represent a significant public health challenge. To accurately identify
allergen proteins, we introduce Applm (Allergen Prediction with Protein
Language Models), a computational framework that leverages the 100-billion
parameter xTrimoPGLM protein language model. We show that Applm consistently
outperforms seven state-of-the-art methods in a diverse set of tasks that
closely resemble difficult real-world scenarios. These include identifying
novel allergens that lack similar examples in the training set, differentiating
between allergens and non-allergens among homologs with high sequence
similarity, and assessing functional consequences of mutations that create few
changes to the protein sequences. Our analysis confirms that xTrimoPGLM,
originally trained on one trillion tokens to capture general protein sequence
characteristics, is crucial for Applm's performance by detecting important
differences among protein sequences. In addition to providing Applm as
open-source software, we also provide our carefully curated benchmark datasets
to facilitate future research.

</details>


### [197] [Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards](https://arxiv.org/abs/2508.10548)
*Zetian Sun,Dongfang Li,Zhuoen Chen,Yuhuai Qin,Baotian Hu*

Main category: cs.LG

TL;DR: This paper introduces Gated Reward Accumulation (G-RA) to address reward sparsity in long-horizon RL for software engineering tasks, achieving significant improvements in completion and modification rates.


<details>
  <summary>Details</summary>
Motivation: Reward sparsity in long-horizon reinforcement learning (RL) tasks is a significant challenge, and existing reward shaping methods suffer from bias, require task decomposition, or lead to reward hacking.

Method: Gated Reward Accumulation (G-RA), a novel method that accumulates immediate rewards only when high-level (long-term) rewards meet a predefined threshold

Result: G-RA leads to a significant increase in completion rates (47.6% -> 93.8% and 22.0% -> 86.0%) and modification rates (19.6% -> 23.8% and 12.0% -> 42.0%) on SWE-bench Verified and kBench.

Conclusion: Gated Reward Accumulation (G-RA) significantly improves completion and modification rates in software engineering tasks while avoiding policy degradation by accumulating immediate rewards only when high-level rewards meet a threshold.

Abstract: Reward sparsity in long-horizon reinforcement learning (RL) tasks remains a
significant challenge, while existing outcome-based reward shaping struggles to
define meaningful immediate rewards without introducing bias or requiring
explicit task decomposition. Alternatively, verification-based reward shaping
uses stepwise critics, but misalignment between immediate rewards and long-term
objectives can lead to reward hacking and suboptimal policies. In this work, we
address this problem in the context of software engineering (SWE) tasks, where
multi-turn reasoning and rule-based verification are critical. We introduce the
SWE-oriented RL Framework, a unified system supporting multi-turn interaction,
docker-based execution, and customizable reward functions. Additionally, we
propose Gated Reward Accumulation (G-RA), a novel method that accumulates
immediate rewards only when high-level (long-term) rewards meet a predefined
threshold, ensuring stable RL optimization. Experiments on SWE-bench Verified
and kBench demonstrate that G-RA leads to an increase in completion rates
(47.6\% \rightarrow 93.8\% and 22.0\% \rightarrow 86.0\%) and modification
rates (19.6\% \rightarrow 23.8\% and 12.0\% \rightarrow 42.0\%), while avoiding
policy degradation caused by reward misalignment. Our findings highlight the
importance of balanced reward accumulation in long-horizon RL and provide a
practical solution.

</details>


### [198] [Technical Report: Facilitating the Adoption of Causal Inference Methods Through LLM-Empowered Co-Pilot](https://arxiv.org/abs/2508.10581)
*Jeroen Berrevoets,Julianna Piskorz,Robert Davis,Harry Amad,Jim Weatherall,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: CATE-B is an open-source co-pilot system that uses LLMs to guide users through treatment effect estimation,construct a structural causal model, identifying robust adjustment sets, and selecting appropriate regression methods,and a suite of benchmark tasks spanning diverse domains and causal complexities are released.


<details>
  <summary>Details</summary>
Motivation: Estimating treatment effects (TE) from observational data is a critical yet complex task in many fields. While recent advances have produced powerful estimation techniques, their adoption remains limited due to the need for deep expertise in causal assumptions, adjustment strategies, and model selection.

Method: CATE-B uses large language models (LLMs) within an agentic framework to guide users through the end-to-end process of treatment effect estimation. CATE-B assists in (i) constructing a structural causal model via causal discovery and LLM-based edge orientation, (ii) identifying robust adjustment sets through a novel Minimal Uncertainty Adjustment Set criterion, and (iii) selecting appropriate regression methods tailored to the causal structure and dataset characteristics.

Result: To encourage reproducibility and evaluation, we release a suite of benchmark tasks spanning diverse domains and causal complexities.

Conclusion: CATE-B lowers the barrier to rigorous causal analysis and lays the foundation for a new class of benchmarks in automated treatment effect estimation.

Abstract: Estimating treatment effects (TE) from observational data is a critical yet
complex task in many fields, from healthcare and economics to public policy.
While recent advances in machine learning and causal inference have produced
powerful estimation techniques, their adoption remains limited due to the need
for deep expertise in causal assumptions, adjustment strategies, and model
selection. In this paper, we introduce CATE-B, an open-source co-pilot system
that uses large language models (LLMs) within an agentic framework to guide
users through the end-to-end process of treatment effect estimation. CATE-B
assists in (i) constructing a structural causal model via causal discovery and
LLM-based edge orientation, (ii) identifying robust adjustment sets through a
novel Minimal Uncertainty Adjustment Set criterion, and (iii) selecting
appropriate regression methods tailored to the causal structure and dataset
characteristics. To encourage reproducibility and evaluation, we release a
suite of benchmark tasks spanning diverse domains and causal complexities. By
combining causal inference with intelligent, interactive assistance, CATE-B
lowers the barrier to rigorous causal analysis and lays the foundation for a
new class of benchmarks in automated treatment effect estimation.

</details>


### [199] [GNN-based Unified Deep Learning](https://arxiv.org/abs/2508.10583)
*Furkan Pala,Islem Rekik*

Main category: cs.LG

TL;DR: Unified learning uses a GNN to guide the optimization of heterogeneous models, improving generalizability across different medical imaging distributions and architectures.


<details>
  <summary>Details</summary>
Motivation: Deep learning models often struggle to maintain generalizability in medical imaging, particularly under domain-fracture scenarios where distribution shifts arise from varying imaging techniques, acquisition protocols, patient populations, demographics, and equipment. In practice, each hospital may need to train distinct models - differing in learning task, width, and depth - to match local data. How to train such heterogeneous models coherently across datasets, while enhancing each model's generalizability, remains an open problem.

Method: Unified learning encodes each model into a graph representation, enabling unification in a shared graph learning space. A GNN then guides optimization of these unified models. By decoupling parameters of individual models and controlling them through a unified GNN (uGNN), our method supports parameter sharing and knowledge transfer across varying architectures (MLPs, CNNs, GNNs) and distributions

Result: Evaluations on MorphoMNIST and two MedMNIST benchmarks - PneumoniaMNIST and BreastMNIST - show that unified learning boosts performance when models are trained on unique distributions and tested on mixed ones, demonstrating strong robustness to unseen data with large distribution shifts.

Conclusion: Unified learning boosts performance when models are trained on unique distributions and tested on mixed ones, demonstrating strong robustness to unseen data with large distribution shifts.

Abstract: Deep learning models often struggle to maintain generalizability in medical
imaging, particularly under domain-fracture scenarios where distribution shifts
arise from varying imaging techniques, acquisition protocols, patient
populations, demographics, and equipment. In practice, each hospital may need
to train distinct models - differing in learning task, width, and depth - to
match local data. For example, one hospital may use Euclidean architectures
such as MLPs and CNNs for tabular or grid-like image data, while another may
require non-Euclidean architectures such as graph neural networks (GNNs) for
irregular data like brain connectomes. How to train such heterogeneous models
coherently across datasets, while enhancing each model's generalizability,
remains an open problem. We propose unified learning, a new paradigm that
encodes each model into a graph representation, enabling unification in a
shared graph learning space. A GNN then guides optimization of these unified
models. By decoupling parameters of individual models and controlling them
through a unified GNN (uGNN), our method supports parameter sharing and
knowledge transfer across varying architectures (MLPs, CNNs, GNNs) and
distributions, improving generalizability. Evaluations on MorphoMNIST and two
MedMNIST benchmarks - PneumoniaMNIST and BreastMNIST - show that unified
learning boosts performance when models are trained on unique distributions and
tested on mixed ones, demonstrating strong robustness to unseen data with large
distribution shifts. Code and benchmarks: https://github.com/basiralab/uGNN

</details>


### [200] [Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformer](https://arxiv.org/abs/2508.10587)
*Xuanhao Mu,Gökhan Demirel,Yuzhe Zhang,Jianlei Liu,Thorsten Schlachter,Veit Hagenmeyer*

Main category: cs.LG

TL;DR: 提出了一种新的基于生成对抗Transformer (GAT) 的上采样方法，无需ground-truth高分辨率数据，可有效提高上采样精度和模型预测控制性能。


<details>
  <summary>Details</summary>
Motivation: 为了弥合基于能源系统模型的能源网络设计和运行中的时间粒度差距，需要对时间序列进行重采样。虽然传统的上采样方法在计算上是有效的，但它们通常会导致显着的信息丢失或噪声增加。时间序列生成模型、超分辨率模型和插补模型等高级模型显示出潜力，但也面临着根本性的挑战。时间序列生成模型的目标是学习原始数据的分布，以生成具有相似统计特征的高分辨率序列。这与上采样的定义并不完全一致。时间序列超分辨率模型或插补模型会降低上采样的准确性，因为输入的低分辨率时间序列是稀疏的，并且可能没有足够的上下文。此外，此类模型通常依赖于监督学习范例。这带来了一个根本性的应用悖论：它们的训练需要上采样应用场景中本质上不存在的高分辨率时间序列。

Method: 利用生成对抗Transformer (GAT)

Result: 与传统的插值方法相比，该方法可以将上采样任务的均方根误差 (RMSE) 降低 9%，并且模型预测控制 (MPC) 应用程序场景的准确性提高了 13%。

Conclusion: 介绍了使用生成对抗Transformer (GAT) 的一种新方法，该方法无需访问任何ground-truth高分辨率数据即可进行训练。与传统的插值方法相比，该方法可以将上采样任务的均方根误差 (RMSE) 降低 9%，并且模型预测控制 (MPC) 应用程序场景的准确性提高了 13%。

Abstract: To bridge the temporal granularity gap in energy network design and operation
based on Energy System Models, resampling of time series is required. While
conventional upsampling methods are computationally efficient, they often
result in significant information loss or increased noise. Advanced models such
as time series generation models, Super-Resolution models and imputation models
show potential, but also face fundamental challenges. The goal of time series
generative models is to learn the distribution of the original data to generate
high-resolution series with similar statistical characteristics. This is not
entirely consistent with the definition of upsampling. Time series
Super-Resolution models or imputation models can degrade the accuracy of
upsampling because the input low-resolution time series are sparse and may have
insufficient context. Moreover, such models usually rely on supervised learning
paradigms. This presents a fundamental application paradox: their training
requires the high-resolution time series that is intrinsically absent in
upsampling application scenarios. To address the mentioned upsampling issue,
this paper introduces a new method utilizing Generative Adversarial
Transformers (GATs), which can be trained without access to any ground-truth
high-resolution data. Compared with conventional interpolation methods, the
introduced method can reduce the root mean square error (RMSE) of upsampling
tasks by 9%, and the accuracy of a model predictive control (MPC) application
scenario is improved by 13%.

</details>
