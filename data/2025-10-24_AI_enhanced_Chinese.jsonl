{"id": "2510.19986", "categories": ["cs.IR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19986", "abs": "https://arxiv.org/abs/2510.19986", "authors": ["Drew B. Thomas"], "title": "Automating Iconclass: LLMs and RAG for Large-Scale Classification of Religious Woodcuts", "comment": "29 pages, 7 figures. First presented at the \"Digital Humanities and\n  Artificial Intelligence\" conference at the University of Reading on 17 June\n  2024", "summary": "This paper presents a novel methodology for classifying early modern\nreligious images by using Large Language Models (LLMs) and vector databases in\ncombination with Retrieval-Augmented Generation (RAG). The approach leverages\nthe full-page context of book illustrations from the Holy Roman Empire,\nallowing the LLM to generate detailed descriptions that incorporate both visual\nand textual elements. These descriptions are then matched to relevant Iconclass\ncodes through a hybrid vector search. This method achieves 87% and 92%\nprecision at five and four levels of classification, significantly\noutperforming traditional image and keyword-based searches. By employing\nfull-page descriptions and RAG, the system enhances classification accuracy,\noffering a powerful tool for large-scale analysis of early modern visual\narchives. This interdisciplinary approach demonstrates the growing potential of\nLLMs and RAG in advancing research within art history and digital humanities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5411\u91cf\u6570\u636e\u5e93\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5bf9\u65e9\u671f\u73b0\u4ee3\u5b97\u6559\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u5229\u7528\u795e\u5723\u7f57\u9a6c\u5e1d\u56fd\u4e66\u7c4d\u63d2\u56fe\u7684\u6574\u9875\u4e0a\u4e0b\u6587\uff0c\u4f7fLLM\u80fd\u591f\u751f\u6210\u5305\u542b\u89c6\u89c9\u548c\u6587\u672c\u5143\u7d20\u7684\u8be6\u7ec6\u63cf\u8ff0\u3002", "method": "\u901a\u8fc7\u6df7\u5408\u5411\u91cf\u641c\u7d22\u5c06\u8fd9\u4e9b\u63cf\u8ff0\u4e0e\u76f8\u5173\u7684Iconclass\u4ee3\u7801\u8fdb\u884c\u5339\u914d\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e94\u4e2a\u548c\u56db\u4e2a\u5206\u7c7b\u7ea7\u522b\u4e0a\u5206\u522b\u5b9e\u73b0\u4e8687%\u548c92%\u7684\u7cbe\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u56fe\u50cf\u548c\u57fa\u4e8e\u5173\u952e\u8bcd\u7684\u641c\u7d22\u3002", "conclusion": "\u901a\u8fc7\u91c7\u7528\u6574\u9875\u63cf\u8ff0\u548cRAG\uff0c\u8be5\u7cfb\u7edf\u63d0\u9ad8\u4e86\u5206\u7c7b\u7cbe\u5ea6\uff0c\u4e3a\u5927\u89c4\u6a21\u5206\u6790\u65e9\u671f\u73b0\u4ee3\u89c6\u89c9\u6863\u6848\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
{"id": "2510.20150", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.20150", "abs": "https://arxiv.org/abs/2510.20150", "authors": ["Yaochen Zhu", "Harald Steck", "Dawen Liang", "Yinhan He", "Jundong Li", "Nathan Kallus"], "title": "Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs) are reshaping the recommender system paradigm by\nenabling users to express preferences and receive recommendations through\nconversations. Yet, aligning LLMs to the recommendation task remains\nchallenging: pretrained LLMs often generate out-of-catalog items, violate\nrequired output formats, and their ranking quality degrades sharply toward the\nend of the generated list. To this end, we propose ConvRec-R1, a two-stage\nframework for end-to-end training of LLM-based conversational recommender\nsystems. In Stage 1, we construct a behavioral-cloning dataset with a\nRemap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded\ndemonstrations from powerful blackbox LLMs to warm-start the RL training. In\nStage 2, we propose Rank-GRPO, a principled extension of group relative policy\noptimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats\neach rank in the recommendation list as the unit instead of token (too\nfine-grained) or sequence (too coarse), redefining rewards to remove non-causal\ncredit assignment and introducing a rank-level importance ratio based on the\ngeometric mean of rank-wise token probabilities to stabilize policy updates.\nExperiments on the public Reddit-v2 dataset show that ConvRec-R1 converges\nfaster and achieves higher Recall and NDCG than GRPO-style baselines. Code and\ndatasets are released at https://github.com/yaochenzhu/Rank-GRPO.", "AI": {"tldr": "ConvRec-R1\u662f\u4e00\u4e2a\u7528\u4e8e\u8bad\u7ec3\u57fa\u4e8eLLM\u7684\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3LLM\u751f\u6210\u4e0d\u7b26\u5408\u76ee\u5f55\u7684\u9879\u76ee\u3001\u8fdd\u53cd\u8f93\u51fa\u683c\u5f0f\u4ee5\u53ca\u6392\u540d\u8d28\u91cf\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684LLM\u63a8\u8350\u7cfb\u7edf\u5b58\u5728\u751f\u6210\u5185\u5bb9\u4e0d\u7b26\u5408\u76ee\u5f55\u3001\u8f93\u51fa\u683c\u5f0f\u9519\u8bef\u4ee5\u53ca\u6392\u540d\u8d28\u91cf\u4e0b\u964d\u7b49\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a1) \u4f7f\u7528Remap-Reflect-Adjust\u6d41\u6c34\u7ebf\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u3001\u57fa\u4e8e\u76ee\u5f55\u7684\u884c\u4e3a\u514b\u9686\u6570\u636e\u96c6\uff1b2) \u63d0\u51faRank-GRPO\uff0c\u4e00\u79cd\u9488\u5bf9\u6392\u540d\u5f0f\u8f93\u51fa\u4efb\u52a1\u7684GRPO\u6269\u5c55\uff0c\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u5956\u52b1\u548c\u5f15\u5165\u6392\u540d\u7ea7\u522b\u7684\u91cd\u8981\u6027\u6bd4\u7387\u6765\u4f18\u5316\u7b56\u7565\u66f4\u65b0\u3002", "result": "\u5728Reddit-v2\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cConvRec-R1\u6bd4GRPO\u57fa\u7ebf\u6536\u655b\u66f4\u5feb\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684Recall\u548cNDCG\u3002", "conclusion": "ConvRec-R1\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u8bad\u7ec3\u57fa\u4e8eLLM\u7684\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\uff0c\u5e76\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u7ed3\u679c\u3002"}}
{"id": "2510.20193", "categories": ["cs.IR", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20193", "abs": "https://arxiv.org/abs/2510.20193", "authors": ["Rahul Raja", "Arpita Vats"], "title": "Multimedia-Aware Question Answering: A Review of Retrieval and Cross-Modal Reasoning Architectures", "comment": "In Proceedings of the 2nd ACM Workshop in AI-powered Question and\n  Answering Systems (AIQAM '25), October 27-28, 2025, Dublin, Ireland. ACM, New\n  York, NY, USA, 8 pages. https://doi.org/10.1145/3746274.3760393", "summary": "Question Answering (QA) systems have traditionally relied on structured text\ndata, but the rapid growth of multimedia content (images, audio, video, and\nstructured metadata) has introduced new challenges and opportunities for\nretrieval-augmented QA. In this survey, we review recent advancements in QA\nsystems that integrate multimedia retrieval pipelines, focusing on\narchitectures that align vision, language, and audio modalities with user\nqueries. We categorize approaches based on retrieval methods, fusion\ntechniques, and answer generation strategies, and analyze benchmark datasets,\nevaluation protocols, and performance tradeoffs. Furthermore, we highlight key\nchallenges such as cross-modal alignment, latency-accuracy tradeoffs, and\nsemantic grounding, and outline open problems and future research directions\nfor building more robust and context-aware QA systems leveraging multimedia\ndata.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u6982\u8ff0\u4e86\u7ed3\u5408\u591a\u5a92\u4f53\u68c0\u7d22\u7ba1\u9053\u7684\u95ee\u7b54\u7cfb\u7edf\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "motivation": "\u4f20\u7edf\u95ee\u7b54\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\uff0c\u4f46\u591a\u5a92\u4f53\u5185\u5bb9\u7684\u5feb\u901f\u589e\u957f\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\u548c\u673a\u9047\u3002", "method": "\u5bf9\u95ee\u7b54\u7cfb\u7edf\u8fdb\u884c\u5206\u7c7b\uff0c\u91cd\u70b9\u5173\u6ce8\u68c0\u7d22\u65b9\u6cd5\u3001\u878d\u5408\u6280\u672f\u548c\u7b54\u6848\u751f\u6210\u7b56\u7565\u3002", "result": "\u5206\u6790\u4e86\u57fa\u51c6\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u534f\u8bae\u548c\u6027\u80fd\u6743\u8861\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u8de8\u6a21\u6001\u5bf9\u9f50\u3001\u5ef6\u8fdf-\u51c6\u786e\u6027\u6743\u8861\u548c\u8bed\u4e49\u57fa\u7840\u7b49\u5173\u952e\u6311\u6218\uff0c\u5e76\u6982\u8ff0\u4e86\u5f00\u653e\u6027\u95ee\u9898\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.20260", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.20260", "abs": "https://arxiv.org/abs/2510.20260", "authors": ["Changping Meng", "Hongyi Ling", "Jianling Wang", "Yifan Liu", "Shuzhou Zhang", "Dapeng Hong", "Mingyan Gao", "Onkar Dalal", "Ed Chi", "Lichan Hong", "Haokai Lu", "Ningren Han"], "title": "Balancing Fine-tuning and RAG: A Hybrid Strategy for Dynamic LLM Recommendation Updates", "comment": "RecSys 2025 Industry Track", "summary": "Large Language Models (LLMs) empower recommendation systems through their\nadvanced reasoning and planning capabilities. However, the dynamic nature of\nuser interests and content poses a significant challenge: While initial\nfine-tuning aligns LLMs with domain knowledge and user preferences, it fails to\ncapture such real-time changes, necessitating robust update mechanisms. This\npaper investigates strategies for updating LLM-powered recommenders, focusing\non the trade-offs between ongoing fine-tuning and Retrieval-Augmented\nGeneration (RAG). Using an LLM-powered user interest exploration system as a\ncase study, we perform a comparative analysis of these methods across\ndimensions like cost, agility, and knowledge incorporation. We propose a hybrid\nupdate strategy that leverages the long-term knowledge adaptation of periodic\nfine-tuning with the agility of low-cost RAG. We demonstrate through live A/B\nexperiments on a billion-user platform that this hybrid approach yields\nstatistically significant improvements in user satisfaction, offering a\npractical and cost-effective framework for maintaining high-quality LLM-powered\nrecommender systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u63a8\u8350\u7cfb\u7edf\u7684\u66f4\u65b0\u7b56\u7565\uff0c\u91cd\u70b9\u5173\u6ce8\u6301\u7eed\u5fae\u8c03\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u5176\u5148\u8fdb\u7684\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u6765\u589e\u5f3a\u63a8\u8350\u7cfb\u7edf\u3002\u7136\u800c\uff0c\u7528\u6237\u5174\u8da3\u548c\u5185\u5bb9\u7684\u52a8\u6001\u7279\u6027\u63d0\u51fa\u4e86\u4e00\u4e2a\u5de8\u5927\u7684\u6311\u6218\uff1a\u867d\u7136\u6700\u521d\u7684\u5fae\u8c03\u4f7fLLM\u4e0e\u9886\u57df\u77e5\u8bc6\u548c\u7528\u6237\u504f\u597d\u4fdd\u6301\u4e00\u81f4\uff0c\u4f46\u5b83\u672a\u80fd\u6355\u6349\u5230\u8fd9\u79cd\u5b9e\u65f6\u53d8\u5316\uff0c\u56e0\u6b64\u9700\u8981\u5f3a\u5927\u7684\u66f4\u65b0\u673a\u5236\u3002", "method": "\u4f7f\u7528LLM\u9a71\u52a8\u7684\u7528\u6237\u5174\u8da3\u63a2\u7d22\u7cfb\u7edf\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u6211\u4eec\u5bf9\u8fd9\u4e9b\u65b9\u6cd5\u5728\u6210\u672c\u3001\u654f\u6377\u6027\u548c\u77e5\u8bc6\u6574\u5408\u7b49\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u66f4\u65b0\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5229\u7528\u5468\u671f\u6027\u5fae\u8c03\u7684\u957f\u671f\u77e5\u8bc6\u9002\u5e94\u548c\u4f4e\u6210\u672cRAG\u7684\u654f\u6377\u6027\u3002", "result": "\u901a\u8fc7\u5728\u5341\u4ebf\u7528\u6237\u5e73\u53f0\u4e0a\u8fdb\u884c\u7684\u5b9e\u65f6A/B\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u6df7\u5408\u65b9\u6cd5\u5728\u7528\u6237\u6ee1\u610f\u5ea6\u65b9\u9762\u4ea7\u751f\u4e86\u5177\u6709\u7edf\u8ba1\u610f\u4e49\u7684\u6539\u8fdb\uff0c\u4e3a\u7ef4\u62a4\u9ad8\u8d28\u91cf\u7684LLM\u9a71\u52a8\u7684\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u6846\u67b6\u3002", "conclusion": "\u7ed3\u5408\u5468\u671f\u6027\u5fae\u8c03\u7684\u957f\u671f\u77e5\u8bc6\u9002\u5e94\u548c\u4f4e\u6210\u672cRAG\u7684\u654f\u6377\u6027\u7684\u6df7\u5408\u66f4\u65b0\u7b56\u7565\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347LLM\u9a71\u52a8\u7684\u63a8\u8350\u7cfb\u7edf\u7684\u7528\u6237\u6ee1\u610f\u5ea6\u3002"}}
{"id": "2510.20082", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.20082", "abs": "https://arxiv.org/abs/2510.20082", "authors": ["Yuanyuan Tian"], "title": "Query Optimization in the Wild: Realities and Trends", "comment": "6 pages, 3 figures. This paper is based on an invited talk given by\n  Yuanyuan Tian at the Special EDBT/ICDT Joint Event on Theory & Practice of\n  Query Processing in EDBT 2026\n  (https://edbticdt2025.upc.edu/?contents=special_event.html)", "summary": "For nearly half a century, the core design of query optimizers in industrial\ndatabase systems has remained remarkably stable, relying on foundational\nprinciples from System R and the Volcano/Cascades framework. However, the rise\nof cloud computing, massive data volumes, and unified data platforms has\nexposed the limitations of this traditional, monolithic architecture. Taking an\nindustrial perspective, this paper reviews the past and present of query\noptimization in production systems and identifies the challenges they face\ntoday. Then this paper highlights three key trends gaining momentum in the\nindustry that promise to address these challenges. First, a tighter feedback\nloop between query optimization and query execution is being used to improve\nthe robustness of query performance. Second, the scope of optimization is\nexpanding from a single query to entire workloads through the convergence of\nquery optimization and workload optimization. Third, and perhaps most\ntransformatively, the industry is moving from monolithic designs to composable\narchitectures that foster agility and cross-engine collaboration. Together,\nthese trends chart a clear path toward a more dynamic, holistic, and adaptable\nfuture for query optimization in practice.", "AI": {"tldr": "\u5de5\u4e1a\u754c\u6570\u636e\u5e93\u7cfb\u7edf\u4e2d\u7684\u67e5\u8be2\u4f18\u5316\u5668\u5728\u8fd1\u534a\u4e2a\u4e16\u7eaa\u91cc\uff0c\u5176\u6838\u5fc3\u8bbe\u8ba1\u4e00\u76f4\u4fdd\u6301\u7a33\u5b9a\uff0c\u4f46\u4e91\u8ba1\u7b97\u3001\u5927\u6570\u636e\u91cf\u548c\u7edf\u4e00\u6570\u636e\u5e73\u53f0\u7684\u5174\u8d77\u66b4\u9732\u4e86\u4f20\u7edf\u5355\u4f53\u67b6\u6784\u7684\u5c40\u9650\u6027\u3002\u672c\u6587\u56de\u987e\u4e86\u751f\u4ea7\u7cfb\u7edf\u4e2d\u67e5\u8be2\u4f18\u5316\u7684\u8fc7\u53bb\u548c\u73b0\u5728\uff0c\u5e76\u786e\u5b9a\u4e86\u5b83\u4eec\u4eca\u5929\u9762\u4e34\u7684\u6311\u6218\u3002\u7136\u540e\uff0c\u672c\u6587\u91cd\u70b9\u4ecb\u7ecd\u4e86\u884c\u4e1a\u4e2d\u6b63\u5728\u5174\u8d77\u7684\u4e09\u5927\u5173\u952e\u8d8b\u52bf\uff0c\u8fd9\u4e9b\u8d8b\u52bf\u6709\u671b\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff1a\u67e5\u8be2\u4f18\u5316\u548c\u67e5\u8be2\u6267\u884c\u4e4b\u95f4\u66f4\u7d27\u5bc6\u7684\u53cd\u9988\u5faa\u73af\u3001\u4f18\u5316\u8303\u56f4\u4ece\u5355\u4e2a\u67e5\u8be2\u6269\u5c55\u5230\u6574\u4e2a\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4ee5\u53ca\u4ece\u5355\u4f53\u8bbe\u8ba1\u8f6c\u5411\u53ef\u7ec4\u5408\u67b6\u6784\u3002", "motivation": "\u4e91\u8ba1\u7b97\u3001\u5927\u6570\u636e\u91cf\u548c\u7edf\u4e00\u6570\u636e\u5e73\u53f0\u7684\u5174\u8d77\u66b4\u9732\u4e86\u4f20\u7edf\u5355\u4f53\u67e5\u8be2\u4f18\u5316\u5668\u67b6\u6784\u7684\u5c40\u9650\u6027", "method": "\u56de\u987e\u751f\u4ea7\u7cfb\u7edf\u4e2d\u67e5\u8be2\u4f18\u5316\u7684\u8fc7\u53bb\u548c\u73b0\u5728\uff0c\u5e76\u786e\u5b9a\u5b83\u4eec\u4eca\u5929\u9762\u4e34\u7684\u6311\u6218\u3002\u7136\u540e\u91cd\u70b9\u4ecb\u7ecd\u4e86\u884c\u4e1a\u4e2d\u6b63\u5728\u5174\u8d77\u7684\u4e09\u5927\u5173\u952e\u8d8b\u52bf\u3002", "result": "\u786e\u5b9a\u4e86\u67e5\u8be2\u4f18\u5316\u548c\u67e5\u8be2\u6267\u884c\u4e4b\u95f4\u66f4\u7d27\u5bc6\u7684\u53cd\u9988\u5faa\u73af\u3001\u4f18\u5316\u8303\u56f4\u4ece\u5355\u4e2a\u67e5\u8be2\u6269\u5c55\u5230\u6574\u4e2a\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4ee5\u53ca\u4ece\u5355\u4f53\u8bbe\u8ba1\u8f6c\u5411\u53ef\u7ec4\u5408\u67b6\u6784\u8fd9\u4e09\u5927\u8d8b\u52bf\u3002", "conclusion": "\u8fd9\u4e9b\u8d8b\u52bf\u5171\u540c\u63cf\u7ed8\u4e86\u4e00\u4e2a\u66f4\u52a8\u6001\u3001\u6574\u4f53\u548c\u9002\u5e94\u6027\u5f3a\u7684\u67e5\u8be2\u4f18\u5316\u5b9e\u8df5\u7684\u672a\u6765\u3002"}}
{"id": "2510.19840", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19840", "abs": "https://arxiv.org/abs/2510.19840", "authors": ["Sai Teja Erukude", "Viswa Chaitanya Marella", "Suhasnadh Reddy Veluru"], "title": "Fourier-Based GAN Fingerprint Detection using ResNet50", "comment": "6 pages. Published in IEEE", "summary": "The rapid rise of photorealistic images produced from Generative Adversarial\nNetworks (GANs) poses a serious challenge for image forensics and industrial\nsystems requiring reliable content authenticity. This paper uses\nfrequency-domain analysis combined with deep learning to solve the problem of\ndistinguishing StyleGAN-generated images from real ones. Specifically, a\ntwo-dimensional Discrete Fourier Transform (2D DFT) was applied to transform\nimages into the Fourier domain, where subtle periodic artifacts become\ndetectable. A ResNet50 neural network is trained on these transformed images to\ndifferentiate between real and synthetic ones. The experiments demonstrate that\nthe frequency-domain model achieves a 92.8 percent and an AUC of 0.95,\nsignificantly outperforming the equivalent model trained on raw spatial-domain\nimages. These results indicate that the GAN-generated images have unique\nfrequency-domain signatures or \"fingerprints\". The method proposed highlights\nthe industrial potential of combining signal processing techniques and deep\nlearning to enhance digital forensics and strengthen the trustworthiness of\nindustrial AI systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u9891\u57df\u5206\u6790\u548c\u6df1\u5ea6\u5b66\u4e60\u6765\u533a\u5206StyleGAN\u751f\u6210\u7684\u56fe\u50cf\u548c\u771f\u5b9e\u56fe\u50cf\u3002", "motivation": "\u7531\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc(GANs)\u751f\u6210\u7684\u7167\u7247\u7ea7\u56fe\u50cf\u7684\u8fc5\u901f\u5d1b\u8d77\uff0c\u5bf9\u56fe\u50cf\u53d6\u8bc1\u548c\u9700\u8981\u53ef\u9760\u5185\u5bb9\u771f\u5b9e\u6027\u7684\u5de5\u4e1a\u7cfb\u7edf\u63d0\u51fa\u4e86\u4e25\u5cfb\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e8c\u7ef4\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362(2D DFT)\u5c06\u56fe\u50cf\u8f6c\u6362\u5230\u9891\u57df\uff0c\u7136\u540e\u5728\u8fd9\u4e9b\u8f6c\u6362\u540e\u7684\u56fe\u50cf\u4e0a\u8bad\u7ec3ResNet50\u795e\u7ecf\u7f51\u7edc\uff0c\u4ee5\u533a\u5206\u771f\u5b9e\u56fe\u50cf\u548c\u5408\u6210\u56fe\u50cf\u3002", "result": "\u8be5\u9891\u57df\u6a21\u578b\u8fbe\u5230\u4e8692.8%\u7684\u51c6\u786e\u7387\u548c0.95\u7684AUC\uff0c\u660e\u663e\u4f18\u4e8e\u5728\u539f\u59cb\u7a7a\u57df\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684\u540c\u7b49\u6a21\u578b\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0cGAN\u751f\u6210\u7684\u56fe\u50cf\u5177\u6709\u72ec\u7279\u7684\u9891\u57df\u7279\u5f81\u6216\u201c\u6307\u7eb9\u201d\uff0c\u8be5\u65b9\u6cd5\u7a81\u51fa\u4e86\u7ed3\u5408\u4fe1\u53f7\u5904\u7406\u6280\u672f\u548c\u6df1\u5ea6\u5b66\u4e60\u5728\u589e\u5f3a\u6570\u5b57\u53d6\u8bc1\u548c\u52a0\u5f3a\u5de5\u4e1a\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u53ef\u4fe1\u5ea6\u65b9\u9762\u7684\u5de5\u4e1a\u6f5c\u529b\u3002"}}
{"id": "2510.19858", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19858", "abs": "https://arxiv.org/abs/2510.19858", "authors": ["Jindi Wang", "Yidi Zhang", "Zhaoxing Li"], "title": "DeBERTa-KC: A Transformer-Based Classifier for Knowledge Construction in Online Learning Discourse", "comment": null, "summary": "This study presents DeBERTa-KC, a transformer-based model for automatic\nclassification of knowledge construction (KC) levels in online science learning\ndiscourse. Using comments collected from four popular YouTube science channels\n(2022--2024), a balanced corpus of 20,000 manually annotated samples was\ncreated across four KC categories: \\textit{nonKC}, \\textit{Share},\n\\textit{Explore}, and \\textit{Negotiate}. The proposed model extends DeBERTa-v3\nwith Focal Loss, Label Smoothing, and R-Drop regularization to address class\nimbalance and enhance generalization. A reproducible end-to-end pipeline was\nimplemented, encompassing data extraction, annotation, preprocessing, training,\nand evaluation. Across 10-fold stratified cross-validation, DeBERTa-KC achieved\na macro-F1 of $0.836 \\pm 0.008$, significantly out-performing both classical\nand transformer baselines ($p<0.01$). Per-category results indicate strong\nsensitivity to higher-order epistemic engagement, particularly in\n\\textit{Explore} and \\textit{Negotiate} discourse. These findings demonstrate\nthat large language models can effectively capture nuanced indicators of\nknowledge construction in informal digital learning environments, offering\nscalable, theory-informed approaches to discourse analysis and the development\nof automated tools for assessing epistemic engagement.", "AI": {"tldr": "DeBERTa-KC\u6a21\u578b\u7528\u4e8e\u81ea\u52a8\u5206\u7c7b\u5728\u7ebf\u79d1\u5b66\u5b66\u4e60\u8ba8\u8bba\u4e2d\u7684\u77e5\u8bc6\u5efa\u6784 (KC) \u5c42\u6b21\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5206\u6790\u5728\u7ebf\u79d1\u5b66\u5b66\u4e60\u8ba8\u8bba\u4e2d\u7684\u77e5\u8bc6\u5efa\u6784\u5c42\u6b21\u3002", "method": "\u8be5\u6a21\u578b\u57fa\u4e8e DeBERTa-v3\uff0c\u5e76\u91c7\u7528 Focal Loss\u3001Label Smoothing \u548c R-Drop \u6b63\u5219\u5316\u65b9\u6cd5\u3002", "result": "DeBERTa-KC \u5728 10 \u6298\u4ea4\u53c9\u9a8c\u8bc1\u4e2d\u53d6\u5f97\u4e86 0.836 \u7684 macro-F1 \u5206\u6570\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u6355\u6349\u975e\u6b63\u5f0f\u6570\u5b57\u5b66\u4e60\u73af\u5883\u4e2d\u77e5\u8bc6\u5efa\u6784\u7684\u7ec6\u5fae\u6307\u6807\u3002"}}
{"id": "2510.19835", "categories": ["cs.AI", "cs.ET", "cs.NE", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.19835", "abs": "https://arxiv.org/abs/2510.19835", "authors": ["Max B. Zhao", "Fei Li"], "title": "A Quantum-Inspired Algorithm for Solving Sudoku Puzzles and the MaxCut Problem", "comment": "29 pages, 10 figures, accepted by Quantum Information & Computation\n  on August 6, 2025", "summary": "We propose and evaluate a quantum-inspired algorithm for solving Quadratic\nUnconstrained Binary Optimization (QUBO) problems, which are mathematically\nequivalent to finding ground states of Ising spin-glass Hamiltonians. The\nalgorithm employs Matrix Product States (MPS) to compactly represent large\nsuperpositions of spin configurations and utilizes a discrete driving schedule\nto guide the MPS toward the ground state. At each step, a driver Hamiltonian --\nincorporating a transverse magnetic field -- is combined with the problem\nHamiltonian to enable spin flips and facilitate quantum tunneling. The MPS is\nupdated using the standard Density Matrix Renormalization Group (DMRG) method,\nwhich iteratively minimizes the system's energy via multiple sweeps across the\nspin chain. Despite its heuristic nature, the algorithm reliably identifies\nglobal minima, not merely near-optimal solutions, across diverse QUBO\ninstances. We first demonstrate its effectiveness on intermediate-level Sudoku\npuzzles from publicly available sources, involving over $200$ Ising spins with\nlong-range couplings dictated by constraint satisfaction. We then apply the\nalgorithm to MaxCut problems from the Biq Mac library, successfully solving\ninstances with up to $251$ nodes and $3,265$ edges. We discuss the advantages\nof this quantum-inspired approach, including its scalability, generalizability,\nand suitability for industrial-scale QUBO applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u91cf\u5b50\u542f\u53d1\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4e8c\u6b21\u65e0\u7ea6\u675f\u4e8c\u5143\u4f18\u5316\uff08QUBO\uff09\u95ee\u9898\u3002", "motivation": "\u8be5\u7b97\u6cd5\u65e8\u5728\u5bfb\u627eIsing\u81ea\u65cb\u73bb\u7483\u54c8\u5bc6\u987f\u91cf\u7684\u57fa\u6001\u3002", "method": "\u8be5\u7b97\u6cd5\u4f7f\u7528\u77e9\u9635\u4e58\u79ef\u72b6\u6001\uff08MPS\uff09\u6765\u8868\u793a\u81ea\u65cb\u914d\u7f6e\u7684\u53e0\u52a0\uff0c\u5e76\u5229\u7528\u79bb\u6563\u9a71\u52a8\u65b9\u6848\u5c06MPS\u5f15\u5bfc\u81f3\u57fa\u6001\u3002\u5728\u6bcf\u4e00\u6b65\u4e2d\uff0c\u9a71\u52a8\u54c8\u5bc6\u987f\u91cf\uff08\u5305\u542b\u6a2a\u5411\u78c1\u573a\uff09\u4e0e\u95ee\u9898\u54c8\u5bc6\u987f\u91cf\u76f8\u7ed3\u5408\uff0c\u4ee5\u5b9e\u73b0\u81ea\u65cb\u7ffb\u8f6c\u5e76\u4fc3\u8fdb\u91cf\u5b50\u96a7\u7a7f\u3002MPS\u4f7f\u7528\u5bc6\u5ea6\u77e9\u9635\u91cd\u6574\u5316\u7ec4\uff08DMRG\uff09\u65b9\u6cd5\u8fdb\u884c\u66f4\u65b0\u3002", "result": "\u8be5\u7b97\u6cd5\u5728\u5404\u79cdQUBO\u5b9e\u4f8b\u4e2d\u53ef\u9760\u5730\u8bc6\u522b\u5168\u5c40\u6700\u5c0f\u503c\u3002\u5728\u6765\u81ea\u516c\u5f00\u6765\u6e90\u7684\u4e2d\u95f4\u7ea7\u6570\u72ec\u8c1c\u9898\uff08\u6d89\u53ca\u8d85\u8fc7200\u4e2aIsing\u81ea\u65cb\uff09\u548cBiq Mac\u5e93\u4e2d\u7684MaxCut\u95ee\u9898\uff08\u6210\u529f\u89e3\u51b3\u4e86\u5177\u6709\u9ad8\u8fbe251\u4e2a\u8282\u70b9\u548c3,265\u6761\u8fb9\u7684\u5b9e\u4f8b\uff09\u4e0a\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\u3001\u901a\u7528\u6027\u548c\u9002\u7528\u4e8e\u5de5\u4e1a\u7ea7QUBO\u5e94\u7528\u7684\u4f18\u70b9\u3002"}}
{"id": "2510.19861", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19861", "abs": "https://arxiv.org/abs/2510.19861", "authors": ["Felix Michalak", "Steven Abreu"], "title": "Some Attention is All You Need for Retrieval", "comment": "16 pages, 10 figures", "summary": "We demonstrate complete functional segregation in hybrid SSM-Transformer\narchitectures: retrieval depends exclusively on self-attention layers. Across\nRecurrentGemma-2B/9B and Jamba-Mini-1.6, attention ablation causes catastrophic\nretrieval failure (0% accuracy), while SSM layers show no compensatory\nmechanisms even with improved prompting. Conversely, sparsifying attention to\njust 15% of heads maintains near-perfect retrieval while preserving 84% MMLU\nperformance, suggesting self-attention specializes primarily for retrieval\ntasks. We identify precise mechanistic requirements for retrieval: needle\ntokens must be exposed during generation and sufficient context must be\navailable during prefill or generation. This strict functional specialization\nchallenges assumptions about redundancy in hybrid architectures and suggests\nthese models operate as specialized modules rather than integrated systems,\nwith immediate implications for architecture optimization and interpretability.", "AI": {"tldr": "\u6df7\u5408 SSM-Transformer \u67b6\u6784\u4e2d\uff0c\u68c0\u7d22\u5b8c\u5168\u4f9d\u8d56\u4e8e\u81ea\u6ce8\u610f\u529b\u5c42\u3002", "motivation": "\u7814\u7a76\u6df7\u5408\u67b6\u6784\u4e2d\u4e0d\u540c\u7ec4\u4ef6\u7684\u529f\u80fd\u5206\u5de5\u3002", "method": "\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u548c\u7a00\u758f\u5316\u6ce8\u610f\u529b\u5934\u6765\u5206\u6790\u68c0\u7d22\u6027\u80fd\u3002", "result": "\u81ea\u6ce8\u610f\u529b\u6d88\u878d\u5bfc\u81f4\u68c0\u7d22\u5931\u8d25\uff0c\u800c SSM \u5c42\u6ca1\u6709\u8865\u507f\u673a\u5236\u3002\u7a00\u758f\u5316\u6ce8\u610f\u529b\u5934\u5728\u4fdd\u6301\u68c0\u7d22\u6027\u80fd\u7684\u540c\u65f6\uff0c\u57fa\u672c\u4e0d\u5f71\u54cd MMLU \u6027\u80fd\u3002", "conclusion": "\u6df7\u5408\u67b6\u6784\u4e2d\u7684\u81ea\u6ce8\u610f\u529b\u548c SSM \u5c42\u662f\u4e13\u95e8\u5316\u7684\u6a21\u5757\uff0c\u800c\u4e0d\u662f\u96c6\u6210\u7cfb\u7edf\u3002"}}
{"id": "2510.20276", "categories": ["cs.IR", "cs.HC", "cs.MA", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.20276", "abs": "https://arxiv.org/abs/2510.20276", "authors": ["Wonil Kim", "Hyeongseok Wi", "Seungsoon Park", "Taejun Kim", "Sangeun Keum", "Keunhyoung Kim", "Taewan Kim", "Jongmin Jung", "Taehyoung Kim", "Gaetan Guerrero", "Mael Le Goff", "Julie Po", "Dongjoo Moon", "Juhan Nam", "Jongpil Lee"], "title": "From Generation to Attribution: Music AI Agent Architectures for the Post-Streaming Era", "comment": "Accepted to the NeurIPS 2025 AI4Music Workshop", "summary": "Generative AI is reshaping music creation, but its rapid growth exposes\nstructural gaps in attribution, rights management, and economic models. Unlike\npast media shifts, from live performance to recordings, downloads, and\nstreaming, AI transforms the entire lifecycle of music, collapsing boundaries\nbetween creation, distribution, and monetization. However, existing streaming\nsystems, with opaque and concentrated royalty flows, are ill-equipped to handle\nthe scale and complexity of AI-driven production. We propose a content-based\nMusic AI Agent architecture that embeds attribution directly into the creative\nworkflow through block-level retrieval and agentic orchestration. Designed for\niterative, session-based interaction, the system organizes music into granular\ncomponents (Blocks) stored in BlockDB; each use triggers an Attribution Layer\nevent for transparent provenance and real-time settlement. This framework\nreframes AI from a generative tool into infrastructure for a Fair AI Media\nPlatform. By enabling fine-grained attribution, equitable compensation, and\nparticipatory engagement, it points toward a post-streaming paradigm where\nmusic functions not as a static catalog but as a collaborative and adaptive\necosystem.", "AI": {"tldr": "Generative AI is changing music creation but current systems can't handle it. The authors propose a new system for fair attribution and compensation.", "motivation": "Current streaming systems can't handle the scale and complexity of AI-driven music production.", "method": "A content-based Music AI Agent architecture that embeds attribution directly into the creative workflow through block-level retrieval and agentic orchestration.", "result": "The system enables fine-grained attribution, equitable compensation, and participatory engagement.", "conclusion": "The proposed framework reframes AI from a generative tool into infrastructure for a Fair AI Media Platform, enabling a collaborative and adaptive ecosystem."}}
{"id": "2510.20110", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.20110", "abs": "https://arxiv.org/abs/2510.20110", "authors": ["Ming Sheng", "Shuliang Wang", "Yong Zhang", "Yi Luo", "Xianbo Liu", "Zeming Li"], "title": "UREM: A High-performance Unified and Resilient Enhancement Method for Multi- and High-Dimensional Indexes", "comment": "12 pages,12 Figures", "summary": "Numerous multi- or high-dimensional indexes with distinct advantages have\nbeen proposed on various platforms to meet application requirements. To achieve\nhigher-performance queries, most indexes employ enhancement methods, including\nstructure-oriented and layout-oriented enhancement methods. Existing\nstructure-oriented methods tailored to specific indexes work well under static\nworkloads but lack generality and degrade under dynamic workloads. The\nlayout-oriented methods exhibit good generality and perform well under dynamic\nworkloads, but exhibit suboptimal performance under static workloads.\nTherefore, it is an open challenge to develop a unified and resilient\nenhancement method that can improve query performance for different indexes\nadaptively under different scenarios. In this paper, we propose UREM, which is\nthe first high-performance Unified and Resilient Enhancement Method designed\nfor both multi- and high-dimensional indexes, capable of adapting to different\nscenarios. Specifically, UREM (1) can be uniformly applied with different\nindexes on various platforms; (2) enhances the query performance of indexes by\nlayout optimization under static workloads; (3) enables indexes to stabilize\nperformance when queries shift through partial layout reorganization. We\nevaluate UREM on 20 widely used indexes. Experimental results demonstrate that\nUREM improves the query performance of multi- and high-dimensional indexes by\nup to 5.73x and 9.18x under static workloads, and by an average of 5.72x and\n9.47x under dynamic workloads. Moreover, some traditional indexes enhanced by\nUREM even achieve performance comparable to or even surpassing that of recent\nadvanced indexes.", "AI": {"tldr": "\u63d0\u51fa\u4e86UREM\uff0c\u4e00\u79cd\u7edf\u4e00\u4e14\u6709\u5f39\u6027\u7684\u589e\u5f3a\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347\u591a\u7ef4\u548c\u9ad8\u7ef4\u7d22\u5f15\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u67e5\u8be2\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u7ed3\u6784\u5bfc\u5411\u65b9\u6cd5\u5728\u9759\u6001\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u6027\u80fd\u4e0b\u964d\uff1b\u5e03\u5c40\u5bfc\u5411\u65b9\u6cd5\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\uff0c\u4f46\u5728\u9759\u6001\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u6027\u80fd\u6b20\u4f73\u3002\u56e0\u6b64\uff0c\u5f00\u53d1\u4e00\u79cd\u7edf\u4e00\u4e14\u6709\u5f39\u6027\u7684\u589e\u5f3a\u65b9\u6cd5\u6765\u63d0\u9ad8\u4e0d\u540c\u7d22\u5f15\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u67e5\u8be2\u6027\u80fd\u662f\u4e00\u4e2a\u516c\u5f00\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86UREM\uff0c\u5b83\u53ef\u4ee5\u7edf\u4e00\u5e94\u7528\u4e8e\u5404\u79cd\u5e73\u53f0\u4e0a\u7684\u4e0d\u540c\u7d22\u5f15\uff1b\u901a\u8fc7\u5e03\u5c40\u4f18\u5316\u589e\u5f3a\u9759\u6001\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u7d22\u5f15\u67e5\u8be2\u6027\u80fd\uff1b\u5e76\u901a\u8fc7\u90e8\u5206\u5e03\u5c40\u91cd\u7ec4\u4f7f\u7d22\u5f15\u5728\u67e5\u8be2\u8f6c\u79fb\u65f6\u7a33\u5b9a\u6027\u80fd\u3002", "result": "UREM\u5728\u9759\u6001\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u5c06\u591a\u7ef4\u548c\u9ad8\u7ef4\u7d22\u5f15\u7684\u67e5\u8be2\u6027\u80fd\u63d0\u9ad8\u4e86\u9ad8\u8fbe5.73\u500d\u548c9.18\u500d\uff0c\u5728\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u5e73\u5747\u63d0\u9ad8\u4e865.72\u500d\u548c9.47\u500d\u3002UREM\u589e\u5f3a\u7684\u4e00\u4e9b\u4f20\u7edf\u7d22\u5f15\u751a\u81f3\u8fbe\u5230\u4e86\u4e0e\u6700\u8fd1\u5148\u8fdb\u7d22\u5f15\u76f8\u5f53\u6216\u8d85\u8fc7\u7684\u6027\u80fd\u3002", "conclusion": "UREM\u662f\u4e00\u79cd\u9ad8\u6027\u80fd\u7684\u7edf\u4e00\u5f39\u6027\u589e\u5f3a\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u7ef4\u548c\u9ad8\u7ef4\u7d22\u5f15\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u7684\u573a\u666f\u3002"}}
{"id": "2510.19955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19955", "abs": "https://arxiv.org/abs/2510.19955", "authors": ["M\u00e1rcus Vin\u00edcius Lobo Costa", "Sherlon Almeida da Silva", "B\u00e1rbara Caroline Benato", "Leo Sampaio Ferraz Ribeiro", "Moacir Antonelli Ponti"], "title": "Transformed Multi-view 3D Shape Features with Contrastive Learning", "comment": null, "summary": "This paper addresses the challenges in representation learning of 3D shape\nfeatures by investigating state-of-the-art backbones paired with both\ncontrastive supervised and self-supervised learning objectives. Computer vision\nmethods struggle with recognizing 3D objects from 2D images, often requiring\nextensive labeled data and relying on Convolutional Neural Networks (CNNs) that\nmay overlook crucial shape relationships. Our work demonstrates that Vision\nTransformers (ViTs) based architectures, when paired with modern contrastive\nobjectives, achieve promising results in multi-view 3D analysis on our\ndownstream tasks, unifying contrastive and 3D shape understanding pipelines.\nFor example, supervised contrastive losses reached about 90.6% accuracy on\nModelNet10. The use of ViTs and contrastive learning, leveraging ViTs' ability\nto understand overall shapes and contrastive learning's effectiveness,\novercomes the need for extensive labeled data and the limitations of CNNs in\ncapturing crucial shape relationships. The success stems from capturing global\nshape semantics via ViTs and refining local discriminative features through\ncontrastive optimization. Importantly, our approach is empirical, as it is\ngrounded on extensive experimental evaluation to validate the effectiveness of\ncombining ViTs with contrastive objectives for 3D representation learning.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u9aa8\u5e72\u7f51\u7edc\u4e0e\u5bf9\u6bd4\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u76ee\u6807\u76f8\u7ed3\u5408\u6765\u5b66\u4e60 3D \u5f62\u72b6\u7279\u5f81\u7684\u6311\u6218\u3002", "motivation": "\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\u96be\u4ee5\u4ece 2D \u56fe\u50cf\u4e2d\u8bc6\u522b 3D \u5bf9\u8c61\uff0c\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u6807\u8bb0\u6570\u636e\u5e76\u4e14\u4f9d\u8d56\u4e8e\u53ef\u80fd\u5ffd\u7565\u5173\u952e\u5f62\u72b6\u5173\u7cfb\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN)\u3002", "method": "\u672c\u6587\u91c7\u7528\u57fa\u4e8e\u89c6\u89c9 Transformer (ViT) \u7684\u67b6\u6784\uff0c\u5e76\u7ed3\u5408\u73b0\u4ee3\u5bf9\u6bd4\u76ee\u6807\uff0c\u5728\u4e0b\u6e38\u4efb\u52a1\u7684\u591a\u89c6\u56fe 3D \u5206\u6790\u4e2d\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u7edf\u4e00\u4e86\u5bf9\u6bd4\u548c 3D \u5f62\u72b6\u7406\u89e3\u7ba1\u9053\u3002", "result": "\u6709\u76d1\u7763\u7684\u5bf9\u6bd4\u635f\u5931\u5728 ModelNet10 \u4e0a\u8fbe\u5230\u4e86\u7ea6 90.6% \u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u901a\u8fc7 ViT \u6355\u83b7\u5168\u5c40\u5f62\u72b6\u8bed\u4e49\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u4f18\u5316\u6539\u8fdb\u5c40\u90e8\u533a\u5206\u7279\u5f81\uff0c\u514b\u670d\u4e86\u5bf9\u5927\u91cf\u6807\u8bb0\u6570\u636e\u7684\u9700\u6c42\u4ee5\u53ca CNN \u5728\u6355\u83b7\u5173\u952e\u5f62\u72b6\u5173\u7cfb\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.19866", "categories": ["cs.CL", "cs.AI", "G.1.10; G.4; I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.19866", "abs": "https://arxiv.org/abs/2510.19866", "authors": ["Xincheng Liu"], "title": "An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson Plans Across Different Models and Prompt Frameworks in High-School Physics", "comment": "20 pages, 6 tables", "summary": "This study evaluates the pedagogical soundness and usability of AI-generated\nlesson plans across five leading large language models: ChatGPT (GPT-5), Claude\nSonnet 4.5, Gemini 2.5 Flash, DeepSeek V3.2, and Grok 4. Beyond model choice,\nthree structured prompt frameworks were tested: TAG (Task, Audience, Goal),\nRACE (Role, Audience, Context, Execution), and COSTAR (Context, Objective,\nStyle, Tone, Audience, Response Format).\n  Fifteen lesson plans were generated for a single high-school physics topic,\nThe Electromagnetic Spectrum. The lesson plans were analyzed through four\nautomated computational metrics: (1) readability and linguistic complexity, (2)\nfactual accuracy and hallucination detection, (3) standards and curriculum\nalignment, and (4) cognitive demand of learning objectives.\n  Results indicate that model selection exerted the strongest influence on\nlinguistic accessibility, with DeepSeek producing the most readable teaching\nplan (FKGL = 8.64) and Claude generating the densest language (FKGL = 19.89).\n  The prompt framework structure most strongly affected the factual accuracy\nand pedagogical completeness, with the RACE framework yielding the lowest\nhallucination index and the highest incidental alignment with NGSS curriculum\nstandards. Across all models, the learning objectives in the fifteen lesson\nplans clustered at the Remember and Understand tiers of Bloom's taxonomy. There\nwere limited higher-order verbs in the learning objectives extracted.\n  Overall, the findings suggest that readability is significantly governed by\nmodel design, while instructional reliability and curricular alignment depend\nmore on the prompt framework. The most effective configuration for lesson plans\nidentified in the results was to combine a readability-optimized model with the\nRACE framework and an explicit checklist of physics concepts, curriculum\nstandards, and higher-order objectives.", "AI": {"tldr": "This study assesses the quality of AI-generated lesson plans from five LLMs using three prompt frameworks.", "motivation": "To evaluate the pedagogical soundness and usability of AI-generated lesson plans.", "method": "Generated 15 lesson plans for a high-school physics topic using five LLMs and three prompt frameworks, then analyzed them using computational metrics: readability, factual accuracy, curriculum alignment, and cognitive demand.", "result": "Model selection most influenced readability, while prompt framework affected factual accuracy and curriculum alignment. The RACE framework yielded the lowest hallucination index. Learning objectives were mostly at the Remember and Understand tiers of Bloom's taxonomy.", "conclusion": "Readability depends on the model, while instructional reliability and curricular alignment depend on the prompt framework. Combining a readability-optimized model with the RACE framework and an explicit checklist is most effective."}}
{"id": "2510.19836", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.19836", "abs": "https://arxiv.org/abs/2510.19836", "authors": ["Eliseo Curcio"], "title": "Benchmarking Reasoning Reliability in Artificial Intelligence Models for Energy-System Analysis", "comment": null, "summary": "Artificial intelligence and machine learning are increasingly used for\nforecasting, optimization, and policy design in the energy sector, yet no\nstandardized framework exists to evaluate whether these systems reason\ncorrectly. Current validation practices focus on predictive accuracy or\ncomputational efficiency, leaving the logical integrity of analytical\nconclusions untested. This study introduces the Analytical Reliability\nBenchmark (ARB), a reproducible framework that quantifies reasoning reliability\nin large language models applied to energy system analysis. The benchmark\nintegrates five submetrics: accuracy, reasoning reliability, uncertainty\ndiscipline, policy consistency, and transparency, and evaluates model\nperformance across deterministic, probabilistic, and epistemic scenarios using\nopen technoeconomic datasets (NREL ATB 2024, DOE H2A/H2New, IEA WEO 2024). Four\nfrontier models (GPT-4/5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Llama 3 70B) were\ntested under identical factual and regulatory conditions. Results show that\nreasoning reliability can be objectively measured. GPT-4/5 and Claude 4.5\nSonnet achieved consistent and policy-compliant reasoning (Analytical\nReliability Index greater than 90), Gemini 2.5 Pro demonstrated moderate\nstability, and Llama 3 70B remained below professional thresholds. Statistical\nvalidation confirmed that these differences are significant and reproducible.\nThe ARB establishes the first quantitative method in the energy literature for\nverifying causal, probabilistic, and policy-driven reasoning in artificial\nintelligence systems, providing a reference framework for trustworthy and\ntransparent analytical applications in the global energy transition.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u5206\u6790\u53ef\u9760\u6027\u57fa\u51c6\uff08ARB\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u80fd\u6e90\u7cfb\u7edf\u5206\u6790\u4e2d\u7684\u63a8\u7406\u53ef\u9760\u6027\u3002", "motivation": "\u5f53\u524d\u80fd\u6e90\u9886\u57df\u7684\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u5b66\u4e60\u5e94\u7528\u7f3a\u4e4f\u5bf9\u5206\u6790\u7ed3\u8bba\u903b\u8f91\u5b8c\u6574\u6027\u7684\u9a8c\u8bc1\uff0c\u73b0\u6709\u9a8c\u8bc1\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u9884\u6d4b\u51c6\u786e\u6027\u6216\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u8be5\u57fa\u51c6\u6574\u5408\u4e86\u4e94\u4e2a\u5b50\u6307\u6807\uff1a\u51c6\u786e\u6027\u3001\u63a8\u7406\u53ef\u9760\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u7ea6\u675f\u3001\u653f\u7b56\u4e00\u81f4\u6027\u548c\u900f\u660e\u5ea6\uff0c\u5e76\u5728\u786e\u5b9a\u6027\u3001\u6982\u7387\u6027\u548c\u8ba4\u77e5\u60c5\u5883\u4e0b\uff0c\u4f7f\u7528\u5f00\u653e\u7684\u6280\u672f\u7ecf\u6d4e\u6570\u636e\u96c6\u5bf9\u6a21\u578b\u6027\u80fd\u8fdb\u884c\u8bc4\u4f30\u3002\u6d4b\u8bd5\u4e86\u56db\u4e2a\u524d\u6cbf\u6a21\u578b\uff08GPT-4/5\u3001Claude 4.5 Sonnet\u3001Gemini 2.5 Pro\u3001Llama 3 70B\uff09\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u63a8\u7406\u53ef\u9760\u6027\u53ef\u4ee5\u88ab\u5ba2\u89c2\u8861\u91cf\u3002GPT-4/5 \u548c Claude 4.5 Sonnet \u5b9e\u73b0\u4e86\u6301\u7eed\u4e14\u7b26\u5408\u653f\u7b56\u7684\u63a8\u7406\uff08\u5206\u6790\u53ef\u9760\u6027\u6307\u6570\u5927\u4e8e 90\uff09\uff0cGemini 2.5 Pro \u8868\u73b0\u51fa\u9002\u5ea6\u7684\u7a33\u5b9a\u6027\uff0c\u800c Llama 3 70B \u4ecd\u4f4e\u4e8e\u4e13\u4e1a\u9608\u503c\u3002\u8fd9\u4e9b\u5dee\u5f02\u5177\u6709\u663e\u8457\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002", "conclusion": "ARB \u5728\u80fd\u6e90\u6587\u732e\u4e2d\u5efa\u7acb\u4e86\u7b2c\u4e00\u4e2a\u91cf\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u9a8c\u8bc1\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u56e0\u679c\u3001\u6982\u7387\u548c\u7b56\u7565\u9a71\u52a8\u7684\u63a8\u7406\uff0c\u4e3a\u5168\u7403\u80fd\u6e90\u8f6c\u578b\u4e2d\u53ef\u4fe1\u548c\u900f\u660e\u7684\u5206\u6790\u5e94\u7528\u63d0\u4f9b\u53c2\u8003\u6846\u67b6\u3002"}}
{"id": "2510.19872", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19872", "abs": "https://arxiv.org/abs/2510.19872", "authors": ["Iman Rahmani", "Saman Yazdannik", "Morteza Tayefi", "Jafar Roshanian"], "title": "An Integrated Approach to Neural Architecture Search for Deep Q-Networks", "comment": null, "summary": "The performance of deep reinforcement learning agents is fundamentally\nconstrained by their neural network architecture, a choice traditionally made\nthrough expensive hyperparameter searches and then fixed throughout training.\nThis work investigates whether online, adaptive architecture optimization can\nescape this constraint and outperform static designs. We introduce NAS-DQN, an\nagent that integrates a learned neural architecture search controller directly\ninto the DRL training loop, enabling dynamic network reconfiguration based on\ncumulative performance feedback. We evaluate NAS-DQN against three\nfixed-architecture baselines and a random search control on a continuous\ncontrol task, conducting experiments over multiple random seeds. Our results\ndemonstrate that NAS-DQN achieves superior final performance, sample\nefficiency, and policy stability while incurring negligible computational\noverhead. Critically, the learned search strategy substantially outperforms\nboth undirected random architecture exploration and poorly-chosen fixed\ndesigns, indicating that intelligent, performance-guided search is the key\nmechanism driving success. These findings establish that architecture\nadaptation is not merely beneficial but necessary for optimal sample efficiency\nin online deep reinforcement learning, and suggest that the design of RL agents\nneed not be a static offline choice but can instead be seamlessly integrated as\na dynamic component of the learning process itself.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a NAS-DQN \u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u8c03\u6574\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u3002", "motivation": "\u4f20\u7edf\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u6027\u80fd\u53d7\u5230\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u7684\u9650\u5236\uff0c\u800c\u8fd9\u79cd\u7ed3\u6784\u901a\u5e38\u662f\u901a\u8fc7\u6602\u8d35\u7684\u8d85\u53c2\u6570\u641c\u7d22\u6765\u9009\u62e9\u7684\uff0c\u5e76\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u4e0d\u53d8\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u5728\u7ebf\u81ea\u9002\u5e94\u67b6\u6784\u4f18\u5316\u662f\u5426\u53ef\u4ee5\u6253\u7834\u8fd9\u79cd\u9650\u5236\uff0c\u5e76\u8d85\u8d8a\u9759\u6001\u8bbe\u8ba1\u3002", "method": "\u5c06\u5b66\u4e60\u5230\u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u63a7\u5236\u5668\u76f4\u63a5\u96c6\u6210\u5230 DRL \u8bad\u7ec3\u5faa\u73af\u4e2d\uff0c\u4ece\u800c\u80fd\u591f\u6839\u636e\u7d2f\u79ef\u6027\u80fd\u53cd\u9988\u8fdb\u884c\u52a8\u6001\u7f51\u7edc\u91cd\u6784\u3002", "result": "NAS-DQN \u5728\u6700\u7ec8\u6027\u80fd\u3001\u6837\u672c\u6548\u7387\u548c\u7b56\u7565\u7a33\u5b9a\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u4e09\u79cd\u56fa\u5b9a\u67b6\u6784\u57fa\u7ebf\u548c\u968f\u673a\u641c\u7d22\u5bf9\u7167\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "\u67b6\u6784\u81ea\u9002\u5e94\u4e0d\u4ec5\u6709\u76ca\uff0c\u800c\u4e14\u5bf9\u4e8e\u5728\u7ebf\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6700\u4f73\u6837\u672c\u6548\u7387\u662f\u5fc5\u8981\u7684\u3002"}}
