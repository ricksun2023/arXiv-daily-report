<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 50]
- [cs.CV](#cs.CV) [Total: 44]
- [cs.AI](#cs.AI) [Total: 43]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.LG](#cs.LG) [Total: 49]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [BenchPress: A Human-in-the-Loop Annotation System for Rapid Text-to-SQL Benchmark Curation](https://arxiv.org/abs/2510.13853)
*Fabian Wenz,Omar Bouattour,Devin Yang,Justin Choi,Cecil Gregg,Nesime Tatbul,Çağatay Demiralp*

Main category: cs.CL

TL;DR: This paper introduces BenchPress, a human-in-the-loop system to accelerate the creation of domain-specific text-to-SQL benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-SQL benchmarks are mostly public datasets, and LLMs are less effective on private enterprise data warehouses. Manually annotating SQL logs for creating benchmarks is difficult and costly.

Method: BenchPress uses retrieval-augmented generation (RAG) and LLMs to propose multiple natural language descriptions for a given SQL query. Human experts then refine these drafts.

Result: BenchPress significantly reduces the time and effort required to create high-quality benchmarks, enhancing annotation accuracy and benchmark reliability.

Conclusion: BenchPress streamlines the creation of custom benchmarks, offering a mechanism for assessing text-to-SQL models on domain-specific workloads. The system is publicly available.

Abstract: Large language models (LLMs) have been successfully applied to many tasks,
including text-to-SQL generation. However, much of this work has focused on
publicly available datasets, such as Fiben, Spider, and Bird. Our earlier work
showed that LLMs are much less effective in querying large private enterprise
data warehouses and released Beaver, the first private enterprise text-to-SQL
benchmark. To create Beaver, we leveraged SQL logs, which are often readily
available. However, manually annotating these logs to identify which natural
language questions they answer is a daunting task. Asking database
administrators, who are highly trained experts, to take on additional work to
construct and validate corresponding natural language utterances is not only
challenging but also quite costly. To address this challenge, we introduce
BenchPress, a human-in-the-loop system designed to accelerate the creation of
domain-specific text-to-SQL benchmarks. Given a SQL query, BenchPress uses
retrieval-augmented generation (RAG) and LLMs to propose multiple natural
language descriptions. Human experts then select, rank, or edit these drafts to
ensure accuracy and domain alignment. We evaluated BenchPress on annotated
enterprise SQL logs, demonstrating that LLM-assisted annotation drastically
reduces the time and effort required to create high-quality benchmarks. Our
results show that combining human verification with LLM-generated suggestions
enhances annotation accuracy, benchmark reliability, and model evaluation
robustness. By streamlining the creation of custom benchmarks, BenchPress
offers researchers and practitioners a mechanism for assessing text-to-SQL
models on a given domain-specific workload. BenchPress is freely available via
our public GitHub repository at
https://github.com/fabian-wenz/enterprise-txt2sql and is also accessible on our
website at http://dsg-mcgraw.csail.mit.edu:5000.

</details>


### [2] [Bridging the Semantic Gap: Contrastive Rewards for Multilingual Text-to-SQL](https://arxiv.org/abs/2510.13827)
*Ashish Kattamuri,Ishita Prasad,Meetu Malhotra,Arpita Vats,Rahul Raja,Albert Lie*

Main category: cs.CL

TL;DR: 这篇论文提出了一种新的Text-to-SQL框架，通过结合Group Relative Policy Optimization (GRPO) 和多语言对比奖励信号，来提高跨语言场景下Text-to-SQL系统的任务效率和语义准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的Text-to-SQL方法只关注可执行的查询，忽略了语义对齐的挑战，并且在从英语迁移到其他语言时，执行准确率显著下降。

Method: 该方法通过结合基于语义相似度的奖励信号，来教导模型在SQL生成和用户意图之间获得更好的对应关系。具体来说，采用了GRPO框架，并加入了对比奖励信号。

Result: 在七种语言的MultiSpider数据集上，使用GRPO微调LLaMA-3-3B模型将执行准确率提高到87.4% (+26 pp)，语义准确率提高到52.29% (+32.86 pp)。在GRPO框架中添加对比奖励信号后，平均语义准确率进一步提高到59.14% (+6.85 pp，越南语高达 +10 pp)。

Conclusion: 实验表明，使用对比奖励信号微调的参数高效的3B LLaMA模型优于更大的zero-shot 8B LLaMA模型，执行准确率提高了7.43 pp (从8B模型的81.43% 提高到3B模型的88.86%)，并且语义准确率几乎与8B模型持平 (59.14% vs. 68.57%)，所有这些都只使用了3,000个强化学习训练样本。

Abstract: Current Text-to-SQL methods are evaluated and only focused on executable
queries, overlooking the semantic alignment challenge -- both in terms of the
semantic meaning of the query and the correctness of the execution results.
Even execution accuracy itself shows significant drops when moving from English
to other languages, with an average decline of 6 percentage points across
non-English languages. We address these challenges by presenting a new
framework that combines Group Relative Policy Optimization (GRPO) within a
multilingual contrastive reward signal to enhance both task efficiency and
semantic accuracy in Text-to-SQL systems in cross-lingual scenarios. Our method
teaches models to obtain better correspondence between SQL generation and user
intent by combining a reward signal based on semantic similarity. On the
seven-language MultiSpider dataset, fine-tuning the LLaMA-3-3B model with GRPO
improved the execution accuracy up to 87.4 percent (+26 pp over zero-shot) and
semantic accuracy up to 52.29 percent (+32.86 pp). Adding our contrastive
reward signal in the GRPO framework further improved the average semantic
accuracy to 59.14 percent (+6.85 pp, up to +10 pp for Vietnamese). Our
experiments showcase that a smaller, parameter-efficient 3B LLaMA model
fine-tuned with our contrastive reward signal outperforms a much larger
zero-shot 8B LLaMA model, with an uplift of 7.43 pp in execution accuracy (from
81.43 percent on the 8B model to 88.86 percent on the 3B model), and nearly
matches its semantic accuracy (59.14 percent vs. 68.57 percent) -- all using
just 3,000 reinforcement learning training examples. These results demonstrate
how we can improve the performance of Text-to-SQL systems with contrastive
rewards for directed semantic alignment, without requiring large-scale training
datasets.

</details>


### [3] [From Explainability to Action: A Generative Operational Framework for Integrating XAI in Clinical Mental Health Screening](https://arxiv.org/abs/2510.13828)
*Ratna Kandala,Akshata Kishore Moharir,Divya Arvinda Nayak*

Main category: cs.CL

TL;DR: 现有的可解释人工智能技术无法提供临床医生和患者可理解的相关信息，阻碍了其在精神健康筛查中的实际应用。


<details>
  <summary>Details</summary>
Motivation: 解决可解释人工智能技术在精神健康筛查中无法提供临床相关信息的难题，弥合实验室研究与临床应用之间的差距。

Method: 提出了一种生成式操作框架，该框架利用大型语言模型作为翻译引擎，将来自不同可解释人工智能工具的技术输出与临床指南相结合，自动生成人类可读的、有证据支持的临床叙述。

Result: 通过对框架组件的系统分析，展示了该框架如何解决工作流程集成、偏见缓解和特定利益相关者沟通等关键操作障碍。

Conclusion: 该研究为推动可解释人工智能领域从生成孤立数据点转向在临床实践中提供集成、可操作和可信赖的AI提供了一个战略路线图。

Abstract: Explainable Artificial Intelligence (XAI) has been presented as the critical
component for unlocking the potential of machine learning in mental health
screening (MHS). However, a persistent lab-to-clinic gap remains. Current XAI
techniques, such as SHAP and LIME, excel at producing technically faithful
outputs such as feature importance scores, but fail to deliver clinically
relevant, actionable insights that can be used by clinicians or understood by
patients. This disconnect between technical transparency and human utility is
the primary barrier to real-world adoption. This paper argues that this gap is
a translation problem and proposes the Generative Operational Framework, a
novel system architecture that leverages Large Language Models (LLMs) as a
central translation engine. This framework is designed to ingest the raw,
technical outputs from diverse XAI tools and synthesize them with clinical
guidelines (via RAG) to automatically generate human-readable, evidence-backed
clinical narratives. To justify our solution, we provide a systematic analysis
of the components it integrates, tracing the evolution from intrinsic models to
generative XAI. We demonstrate how this framework directly addresses key
operational barriers, including workflow integration, bias mitigation, and
stakeholder-specific communication. This paper also provides a strategic
roadmap for moving the field beyond the generation of isolated data points
toward the delivery of integrated, actionable, and trustworthy AI in clinical
practice.

</details>


### [4] [A Linguistics-Aware LLM Watermarking via Syntactic Predictability](https://arxiv.org/abs/2510.13829)
*Shinwoo Park,Hyejin Park,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.CL

TL;DR: 本文介绍了一种名为STELA的公开可验证水印框架，该框架通过利用词性n-gram模型来调节水印强度，从而在文本质量和检测鲁棒性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型(llm)的快速发展，可靠的治理工具变得至关重要。公开可验证的水印对于建立可信赖的AI生态系统尤为重要。目前主要的挑战是在文本质量和检测鲁棒性之间取得平衡。

Method: STELA框架通过词性(POS) n-gram模型来动态调节水印强度，在语法约束强的上下文中减弱水印，以保持质量，在语言灵活性高的上下文中加强水印，以提高可检测性。检测器无需访问任何模型logits即可工作，从而方便了公开验证。

Result: 在类型多样的语言(分析型英语、孤立型汉语和粘着型韩语)上进行的大量实验表明，STELA在检测鲁棒性方面优于先前的方法。

Conclusion: STELA框架在文本质量和检测鲁棒性之间取得了平衡，并且实现了公开可验证的检测。

Abstract: As large language models (LLMs) continue to advance rapidly, reliable
governance tools have become critical. Publicly verifiable watermarking is
particularly essential for fostering a trustworthy AI ecosystem. A central
challenge persists: balancing text quality against detection robustness. Recent
studies have sought to navigate this trade-off by leveraging signals from model
output distributions (e.g., token-level entropy); however, their reliance on
these model-specific signals presents a significant barrier to public
verification, as the detection process requires access to the logits of the
underlying model. We introduce STELA, a novel framework that aligns watermark
strength with the linguistic degrees of freedom inherent in language. STELA
dynamically modulates the signal using part-of-speech (POS) n-gram-modeled
linguistic indeterminacy, weakening it in grammatically constrained contexts to
preserve quality and strengthen it in contexts with greater linguistic
flexibility to enhance detectability. Our detector operates without access to
any model logits, thus facilitating publicly verifiable detection. Through
extensive experiments on typologically diverse languages-analytic English,
isolating Chinese, and agglutinative Korean-we show that STELA surpasses prior
methods in detection robustness. Our code is available at
https://github.com/Shinwoo-Park/stela_watermark.

</details>


### [5] [Users as Annotators: LLM Preference Learning from Comparison Mode](https://arxiv.org/abs/2510.13830)
*Zhongze Cai,Xiaocheng Li*

Main category: cs.CL

TL;DR: 提出了一种利用用户在比较模式下的注释数据来对齐大型语言模型的新方法。


<details>
  <summary>Details</summary>
Motivation: 专业人工标注成本高，但用户注释数据质量无法保证。

Method: 通过生成来自不同模型或同一模型的不同版本的两个响应来推断用户的数据质量，并使用期望最大化算法来估计用户的潜在质量因子，并相应地过滤用户注释数据。

Result: 下游任务表明该方法在捕获用户行为和数据过滤方面是有效的。

Conclusion: 该方法能够有效利用用户偏好数据对齐大型语言模型。

Abstract: Pairwise preference data have played an important role in the alignment of
large language models (LLMs). Each sample of such data consists of a prompt,
two different responses to the prompt, and a binary label indicating which of
the two responses is better. The labels are usually annotated by professional
human annotators. In this paper, we consider an alternative approach to collect
pairwise preference data -- user annotation from comparison mode. With the
increasingly wider adoption of LLMs among the population, users are
contributing more and more of their preference labels through their daily
interactions with the LLMs. The upside of such labels is that users are the
best experts in judging the responses to their own queries/prompts, but the
downside is the lack of quality control in these labels. In this paper, we
consider a new idea of generating two responses from two different models or
two different versions of the same model. The asymmetry allows us to make an
inference of the user's data quality through our proposed user behavior model.
We develop an expectation-maximization algorithm to estimate a latent quality
factor of the user, and filter users' annotation data accordingly. The
downstream task shows the effectiveness of our approach in both capturing the
user behavior and data filtering for LLM alignment.

</details>


### [6] [Informed Routing in LLMs: Smarter Token-Level Computation for Faster Inference](https://arxiv.org/abs/2510.13831)
*Chao Han,Yijuan Liang,Zihao Xuan,Daokuan Wu,Wei Zhang,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 这篇论文提出了一种新的路由范式，称为知情路由，以解决大型语言模型（LLM）推理成本高的问题。该方法在做出路由决策之前，通过轻量级特征预测器（LFF）预测单元的输出，从而实现灵活的执行或近似策略，在减少计算量的同时保持模型保真度。


<details>
  <summary>Details</summary>
Motivation: 现有动态token级别计算分配方法依赖于贪婪路由，导致不可逆的信息丢失和次优的token选择。

Method: 论文提出了轻量级特征预测器（LFF），用于评估token的重要性和可恢复性，从而实现知情路由。

Result: 在语言建模和推理任务上的大量实验表明，知情路由在多个稀疏级别上实现了最先进的效率-性能权衡。即使没有最终的LoRA微调，该方法也能匹配或超过需要完全微调的强大基线，同时将训练时间减少了50%以上。

Conclusion: 知情路由是一种有效的方法，可以在降低LLM推理成本的同时保持模型性能。

Abstract: The deployment of large language models (LLMs) in real-world applications is
increasingly limited by their high inference cost. While recent advances in
dynamic token-level computation allocation attempt to improve efficiency by
selectively activating model components per token, existing methods rely on
greedy routing--a myopic execute-or-skip mechanism that often leads to
irreversible information loss and suboptimal token selection. This paper
introduces informed routing, a new paradigm that proactively addresses these
issues. The key insight is to assess not only a token's immediate importance
but also its recoverability, i.e., how well its transformation can be
approximated. To this end, we propose the Lightweight Feature Forecaster (LFF),
a small predictive module that estimates a unit's output before routing
decisions are made. This enables a flexible execute-or-approximate policy that
preserves model fidelity while drastically reducing computation. Extensive
experiments on both language modeling and reasoning tasks show that informed
routing achieves state-of-the-art efficiency-performance trade-offs across
multiple sparsity levels. Notably, even without final LoRA fine-tuning, our
method matches or surpasses strong baselines that require full fine-tuning, all
while reducing training time by over 50%. The code is available at:
https://github.com/EIT-NLP/informed-routing

</details>


### [7] [Entropy Meets Importance: A Unified Head Importance-Entropy Score for Stable and Efficient Transformer Pruning](https://arxiv.org/abs/2510.13832)
*Minsik Choi,Hyegang Son,Changhoon Kim,Young Geun Kim*

Main category: cs.CL

TL;DR: 本文提出了一种新的剪枝标准HIES，用于解决Transformer模型在推理和部署中的效率问题。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在NLP任务中表现出色，但其多层和注意力头的结构特性导致推理和部署效率低下。为了解决这些挑战，研究者提出了各种剪枝方法。基于梯度的HIS方法因其可解释性、效率和识别冗余头的能力而备受关注，但是HIS只捕捉了梯度驱动的贡献，忽略了注意力模式的多样性。

Method: 本文提出了一种新的剪枝标准HIES (Head Importance-Entropy Score)，它将头部重要性分数与注意力熵相结合，为每个头部贡献提供补充证据。

Result: 基于HIES的剪枝方法在模型质量上提高了15.2%，在稳定性上提高了2.04倍，实现了在不牺牲精度或稳定性的前提下大幅压缩模型。

Conclusion: 本文提出的HIES剪枝方法能够有效提高模型质量和稳定性，实现模型压缩。

Abstract: Transformer-based models have achieved remarkable performance in NLP tasks.
However, their structural characteristics-multiple layers and attention
heads-introduce efficiency challenges in inference and deployment. To address
these challenges, various pruning methods have recently been proposed. Notably,
gradient-based methods using Head Importance Scores (HIS) have gained traction
for interpretability, efficiency, and ability to identify redundant heads.
However, HIS alone has limitations as it captures only the gradient-driven
contribution, overlooking the diversity of attention patterns. To overcome
these limitations, we introduce a novel pruning criterion, HIES (Head
Importance-Entropy Score), which integrates head importance scores with
attention entropy, providing complementary evidence on per-head contribution.
Empirically, HIES-based pruning yields up to 15.2% improvement in model quality
and 2.04x improvement in stability over HIS-only methods, enabling substantial
model compression without sacrificing either accuracy or stability. Code will
be released upon publication.

</details>


### [8] [ConDABench: Interactive Evaluation of Language Models for Data Analysis](https://arxiv.org/abs/2510.13835)
*Avik Dutta,Priyanshu Gupta,Hosein Hasanbeig,Rahul Pratap Singh,Harshit Nigam,Sumit Gulwani,Arjun Radhakrishna,Gustavo Soares,Ashish Tiwari*

Main category: cs.CL

TL;DR: 介绍了一个新的对话式数据分析基准测试框架ConDABench，用于评估大型语言模型在复杂交互式任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的数据分析任务评估基准无法捕捉到真实世界数据分析中目标不明确和数据不干净等复杂性，并且缺乏对交互性的支持。

Method: 提出了一个多智能体工作流程，用于从描述公共数据集见解的文章中生成真实的基准测试。

Result: 生成了包含1420个ConDA问题的基准测试，并评估了当前先进的大型语言模型。结果表明，新一代模型在解决更多实例方面表现更好，但在需要持续、长程交互的任务中表现不一定更好。

Conclusion: ConDABench可以帮助模型构建者衡量在构建真正协作模型以完成复杂交互式任务方面的进展。

Abstract: Real-world data analysis tasks often come with under-specified goals and
unclean data. User interaction is necessary to understand and disambiguate a
user's intent, and hence, essential to solving these complex tasks. Existing
benchmarks for evaluating LLMs on data analysis tasks do not capture these
complexities or provide first-class support for interactivity. We introduce
ConDABench, a framework for generating conversational data analysis (ConDA)
benchmarks and evaluating external tools on the generated benchmarks. \bench
consists of (a) a multi-agent workflow for generating realistic benchmarks from
articles describing insights gained from public datasets, (b) 1,420 ConDA
problems generated using this workflow, and (c) an evaluation harness that, for
the first time, makes it possible to systematically evaluate conversational
data analysis tools on the generated ConDA problems. Evaluation of
state-of-the-art LLMs on the benchmarks reveals that while the new generation
of models are better at solving more instances, they are not necessarily better
at solving tasks that require sustained, long-form engagement. ConDABench is an
avenue for model builders to measure progress towards truly collaborative
models that can complete complex interactive tasks.

</details>


### [9] [SIMBA UQ: Similarity-Based Aggregation for Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2510.13836)
*Debarun Bhattacharjya,Balaji Ganesan,Junkyu Lee,Radu Marinescu,Katsiaryna Mirylenka,Michael Glass,Xiao Shou*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）何时知道自己不知道什么，即不确定性量化（UQ）。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于提高AI系统的可信度，通过不确定性量化来评估LLM生成输出的置信度。

Method: 本文提出了一种基于相似性的聚合框架，该框架利用生成输出与其他采样生成结果之间的一致性作为置信度的代理，并使用小训练集训练置信度估计模型。

Result: 实验结果表明，所提出的基于相似性的方法可以产生比基线方法更好的校准置信度。

Conclusion: 本文证明了基于相似性的方法在问题回答、摘要和文本到SQL等任务中，可以有效地提高LLM的不确定性量化效果。

Abstract: When does a large language model (LLM) know what it does not know?
Uncertainty quantification (UQ) provides measures of uncertainty, such as an
estimate of the confidence in an LLM's generated output, and is therefore
increasingly recognized as a crucial component of trusted AI systems. Black-box
UQ methods do not require access to internal model information from the
generating LLM and therefore have numerous real-world advantages, such as
robustness to system changes, adaptability to choice of LLM, reduced costs, and
computational tractability. In this paper, we investigate the effectiveness of
UQ techniques that are primarily but not necessarily entirely black-box, where
the consistency between a generated output and other sampled generations is
used as a proxy for confidence in its correctness. We propose a high-level
non-verbalized similarity-based aggregation framework that subsumes a broad
swath of UQ approaches suitable for complex generative tasks, as well as
introduce specific novel techniques from the framework that train confidence
estimation models using small training sets. Through an empirical study with
datasets spanning the diverse tasks of question answering, summarization, and
text-to-SQL, we demonstrate that our proposed similarity-based methods can
yield better calibrated confidences than baselines.

</details>


### [10] [Seeing Hate Differently: Hate Subspace Modeling for Culture-Aware Hate Speech Detection](https://arxiv.org/abs/2510.13837)
*Weibin Cai,Reza Zafarani*

Main category: cs.CL

TL;DR: 提出了一种文化感知的框架，以解决仇恨言论检测中存在的标签偏差和文化差异问题。


<details>
  <summary>Details</summary>
Motivation: 现有的仇恨言论检测方法忽略了一个现实世界的复杂性：训练标签存在偏差，并且不同文化背景的个体对仇恨的理解存在差异。

Method: 该框架构建了个人的仇恨子空间，通过对文化属性的组合进行建模来缓解数据稀疏问题，并使用标签传播来捕获每个组合的独特特征。

Result: 实验表明，该方法在所有指标上的性能平均优于现有技术 1.05%。

Conclusion: 该方法可以进一步提高分类性能。

Abstract: Hate speech detection has been extensively studied, yet existing methods
often overlook a real-world complexity: training labels are biased, and
interpretations of what is considered hate vary across individuals with
different cultural backgrounds. We first analyze these challenges, including
data sparsity, cultural entanglement, and ambiguous labeling. To address them,
we propose a culture-aware framework that constructs individuals' hate
subspaces. To alleviate data sparsity, we model combinations of cultural
attributes. For cultural entanglement and ambiguous labels, we use label
propagation to capture distinctive features of each combination. Finally,
individual hate subspaces, which in turn can further enhance classification
performance. Experiments show our method outperforms state-of-the-art by 1.05\%
on average across all metrics.

</details>


### [11] [Meronymic Ontology Extraction via Large Language Models](https://arxiv.org/abs/2510.13839)
*Dekai Zhang,Simone Conia,Antonio Rago*

Main category: cs.CL

TL;DR: 本文提出了一种利用大型语言模型(llm)从原始评论文本中自动提取产品本体的方法。


<details>
  <summary>Details</summary>
Motivation: 本体在当今的数字时代已经变得至关重要，它可以组织大量可用的非结构化文本。在为这些信息提供正式结构方面，本体具有巨大的价值和应用，例如，电子商务，无数的产品列表需要适当的产品组织。然而，手动构建这些本体是一个耗时、昂贵和费力的过程。

Method: 我们利用大型语言模型(llm)的最新进展，开发了一种从原始评论文本中提取产品本体(以部分整体关系的形式)的全自动方法。

Result: 我们证明，当使用llm作为评判标准进行评估时，我们的方法产生的产品本体超过了现有的基于bert的基线。

Conclusion: 我们的研究为llm更广泛地用于(产品或其他)本体提取奠定了基础。

Abstract: Ontologies have become essential in today's digital age as a way of
organising the vast amount of readily available unstructured text. In providing
formal structure to this information, ontologies have immense value and
application across various domains, e.g., e-commerce, where countless product
listings necessitate proper product organisation. However, the manual
construction of these ontologies is a time-consuming, expensive and laborious
process. In this paper, we harness the recent advancements in large language
models (LLMs) to develop a fully-automated method of extracting product
ontologies, in the form of meronymies, from raw review texts. We demonstrate
that the ontologies produced by our method surpass an existing, BERT-based
baseline when evaluating using an LLM-as-a-judge. Our investigation provides
the groundwork for LLMs to be used more generally in (product or otherwise)
ontology extraction.

</details>


### [12] [ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking](https://arxiv.org/abs/2510.13842)
*Yutao Wu,Xiao Liu,Yinghui Li,Yifeng Gao,Yifan Ding,Jiale Ding,Xiang Zheng,Xingjun Ma*

Main category: cs.CL

TL;DR: 知识中毒通过将对抗性内容注入知识库，诱使大型语言模型 (LLM) 产生由操纵的上下文支持的、受攻击者控制的输出，从而对检索增强生成 (RAG) 系统构成严重威胁。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明，LLM 容易受到误导或恶意检索内容的影响。然而，现实世界中的事实核查场景更具挑战性，因为可信的证据通常在检索池中占据主导地位。

Method: 我们提出了 ADMIT（对抗性多重注入技术），这是一种少样本、语义对齐的中毒攻击，可以在不访问目标 LLM、检索器或令牌级别控制的情况下，反转事实核查决策并诱导欺骗性理由。

Result: 广泛的实验表明，ADMIT 在 4 个检索器、11 个 LLM 和 4 个跨域基准测试中有效转移，在 $0.93 \\\times 10^{-6}$ 的极低中毒率下，平均攻击成功率 (ASR) 达到 86%，即使在存在强烈的反证据的情况下也保持稳健。与之前的最先进的攻击相比，ADMIT 在所有设置中将 ASR 提高了 11.2%，暴露了基于 RAG 的事实核查系统中存在的重大漏洞。

Conclusion: ADMIT 攻击有效地提升了攻击成功率，揭示了现实世界中基于 RAG 的事实核查系统的重大漏洞。

Abstract: Knowledge poisoning poses a critical threat to Retrieval-Augmented Generation
(RAG) systems by injecting adversarial content into knowledge bases, tricking
Large Language Models (LLMs) into producing attacker-controlled outputs
grounded in manipulated context. Prior work highlights LLMs' susceptibility to
misleading or malicious retrieved content. However, real-world fact-checking
scenarios are more challenging, as credible evidence typically dominates the
retrieval pool. To investigate this problem, we extend knowledge poisoning to
the fact-checking setting, where retrieved context includes authentic
supporting or refuting evidence. We propose \textbf{ADMIT}
(\textbf{AD}versarial \textbf{M}ulti-\textbf{I}njection \textbf{T}echnique), a
few-shot, semantically aligned poisoning attack that flips fact-checking
decisions and induces deceptive justifications, all without access to the
target LLMs, retrievers, or token-level control. Extensive experiments show
that ADMIT transfers effectively across 4 retrievers, 11 LLMs, and 4
cross-domain benchmarks, achieving an average attack success rate (ASR) of 86\%
at an extremely low poisoning rate of $0.93 \times 10^{-6}$, and remaining
robust even in the presence of strong counter-evidence. Compared with prior
state-of-the-art attacks, ADMIT improves ASR by 11.2\% across all settings,
exposing significant vulnerabilities in real-world RAG-based fact-checking
systems.

</details>


### [13] [Serialized EHR make for good text representations](https://arxiv.org/abs/2510.13843)
*Zhirong Chou,Quan Qin,Shi Li*

Main category: cs.CL

TL;DR: 提出了一种名为SerialBEHRT的领域对齐基础模型，通过在结构化EHR序列上进行额外的预训练来扩展SciBERT。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常难以将电子健康记录(EHRs)的表格和基于事件的性质与自然语言模型的顺序先验相协调。这种结构上的不匹配限制了他们捕捉病人遭遇的纵向依赖关系的能力。

Method: 该模型旨在编码临床事件之间的时间和上下文关系，从而产生更丰富的患者表征。

Result: 在抗生素敏感性预测任务中，SerialBEHRT取得了优越和更一致的性能。

Conclusion: 时间序列化在医疗保健基础模型预训练中的重要性

Abstract: The emergence of foundation models in healthcare has opened new avenues for
learning generalizable representations from large scale clinical data. Yet,
existing approaches often struggle to reconcile the tabular and event based
nature of Electronic Health Records (EHRs) with the sequential priors of
natural language models. This structural mismatch limits their ability to
capture longitudinal dependencies across patient encounters. We introduce
SerialBEHRT, a domain aligned foundation model that extends SciBERT through
additional pretraining on structured EHR sequences. SerialBEHRT is designed to
encode temporal and contextual relationships among clinical events, thereby
producing richer patient representations. We evaluate its effectiveness on the
task of antibiotic susceptibility prediction, a clinically meaningful problem
in antibiotic stewardship. Through extensive benchmarking against state of the
art EHR representation strategies, we demonstrate that SerialBEHRT achieves
superior and more consistent performance, highlighting the importance of
temporal serialization in foundation model pretraining for healthcare.

</details>


### [14] [DynaSpec: Context-aware Dynamic Speculative Sampling for Large-Vocabulary Language Models](https://arxiv.org/abs/2510.13847)
*Jinbin Zhang,Nasib Ullah,Erik Schultheis,Rohit Babbar*

Main category: cs.CL

TL;DR: Speculative decoding with large vocabularies faces a bottleneck in the drafter's output head. DynaSpec, a context-dependent dynamic shortlisting mechanism, is proposed to address this.


<details>
  <summary>Details</summary>
Motivation: Scaling LLM vocabulary increases the drafter's output head parameters, causing a latency bottleneck. Existing methods using fixed vocabulary subsets are brittle and suppress rare tokens.

Method: DynaSpec uses lightweight meta-classifiers to route contexts to token clusters, forming a dynamic shortlist for the drafter. It leverages parallel execution of draft encoding and meta shortlisting.

Result: DynaSpec shows consistent gains in mean accepted length over fixed-shortlist baselines and enables smaller shortlists without degrading acceptance.

Conclusion: DynaSpec is a robust and efficient approach to speed up drafting in speculative decoding by using context-dependent dynamic shortlisting.

Abstract: Speculative decoding (a.k.a. speculative sampling) has become a standard way
to accelerate LLM inference: a small drafter proposes multiple tokens and a
large target model verifies them once per speculation length. Recently, scaling
of the LLM vocabulary has pushed the number of tokens to grow substantially.
While verification over the full vocabulary leaves the target model largely
unaffected, the O(|V|d) parameters in the drafter's output head become a
latency bottleneck, slowing the entire pipeline. Contemporary methods (e.g.,
FR-Spec, VocabTrim) restrict the drafter's vocabulary to a fixed subset of the
target model's vocabulary, ranked in descending order of token frequency.
Although this reduces draft-time compute, it is brittle, since: (i) frequency
lists are corpus-dependent and require retuning to generalize, and (ii) static
shortlists suppress rare or domain-specific tokens, lowering the expected
number of tokens per verification step. We propose DynaSpec, a
context-dependent dynamic shortlisting mechanism that is robust, speeds up
drafting, and generalizes across diverse tasks. Concretely, we introduce
lightweight, coarse-grained meta-classifiers that route contexts to a small
number of token clusters; the union of the top-k selected clusters forms the
drafter's shortlist, while verification retains the full vocabulary and
exactness. The meta-classifier finishes its computation earlier than the
drafter's hidden state generation by exploiting parallel execution of draft
encoding and meta shortlisting on separate streams. On standard
speculative-decoding benchmarks, we observe consistent gains in mean accepted
length over fixed-shortlist baselines, while context-dependent selection
enables smaller shortlists without degrading acceptance.

</details>


### [15] [PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering](https://arxiv.org/abs/2510.14278)
*Md Mahadi Hasan Nahid,Davood Rafiei*

Main category: cs.CL

TL;DR: 提出了一种Agentic检索系统，利用大型语言模型在一个结构化的循环中检索相关证据，以提高多跳问答的准确率和召回率。


<details>
  <summary>Details</summary>
Motivation: 多跳问答需要收集多个证据，而检索在其中起着核心作用。

Method: 该框架由三个专门的代理组成：问题分析器（将多跳问题分解为子问题），选择器（识别每个子问题最相关的上下文，侧重于精确性）和添加器（引入任何缺失的证据，侧重于召回率）。选择器和添加器之间的迭代交互产生了一组紧凑而全面的支持段落。

Result: 在四个多跳QA基准测试中，该方法始终优于强大的基线。

Conclusion: 该方法实现了更高的检索准确率，同时过滤掉分散注意力的内容，使下游QA模型能够超越完整上下文的答案准确率，同时依赖于明显更少的不相关信息。

Abstract: Retrieval plays a central role in multi-hop question answering (QA), where
answering complex questions requires gathering multiple pieces of evidence. We
introduce an Agentic Retrieval System that leverages large language models
(LLMs) in a structured loop to retrieve relevant evidence with high precision
and recall. Our framework consists of three specialized agents: a Question
Analyzer that decomposes a multi-hop question into sub-questions, a Selector
that identifies the most relevant context for each sub-question (focusing on
precision), and an Adder that brings in any missing evidence (focusing on
recall). The iterative interaction between Selector and Adder yields a compact
yet comprehensive set of supporting passages. In particular, it achieves higher
retrieval accuracy while filtering out distracting content, enabling downstream
QA models to surpass full-context answer accuracy while relying on
significantly less irrelevant information. Experiments on four multi-hop QA
benchmarks -- HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG --
demonstrates that our approach consistently outperforms strong baselines.

</details>


### [16] [On-device System of Compositional Multi-tasking in Large Language Models](https://arxiv.org/abs/2510.13848)
*Ondrej Bohdal,Konstantinos Theodosiadis,Asterios Mpatziakas,Dimitris Filippidis,Iro Spyrou,Christos Zonios,Anastasios Drosou,Dimosthenis Ioannidis,Kyeng-Hun Lee,Jijoong Moon,Hyeonmok Ko,Mete Ozay,Umberto Michieli*

Main category: cs.CL

TL;DR: 提出了一种新的方法，专门为涉及摘要和翻译的组合多任务场景量身定制。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 通常通过参数高效的微调技术（如低秩适配器 (LoRA)）来适应各种下游任务。虽然适配器可以组合起来以分别处理多个任务，但标准方法在定位同时执行复杂任务（例如从长对话生成翻译摘要）时会遇到困难。

Method: 我们的技术包括在组合的摘要和翻译适配器之上添加一个可学习的投影层。这种设计能够实现有效的集成，同时通过减少计算开销来保持效率，而其他策略需要大量的重新训练或顺序处理。

Result: 实验结果表明，我们的解决方案在基于云和设备上的实现中都表现良好且速度很快，突出了在需要高速运行和资源限制的实际应用中采用我们的框架的潜在好处。

Conclusion: 我们通过开发能够无缝执行组合任务的 Android 应用程序，证明了我们的方法在设备环境中的实际可行性。

Abstract: Large language models (LLMs) are commonly adapted for diverse downstream
tasks via parameter-efficient fine-tuning techniques such as Low-Rank Adapters
(LoRA). While adapters can be combined to handle multiple tasks separately,
standard approaches struggle when targeting the simultaneous execution of
complex tasks, such as generating a translated summary from a long
conversation. To address this challenge, we propose a novel approach tailored
specifically for compositional multi-tasking scenarios involving summarization
and translation. Our technique involves adding a learnable projection layer on
top of the combined summarization and translation adapters. This design enables
effective integration while maintaining efficiency through reduced
computational overhead compared to alternative strategies requiring extensive
retraining or sequential processing. We demonstrate the practical viability of
our method within an on-device environment by developing an Android app capable
of executing compositional tasks seamlessly. Experimental results indicate our
solution performs well and is fast in both cloud-based and on-device
implementations, highlighting the potential benefits of adopting our framework
in real-world applications demanding high-speed operation alongside resource
constraints.

</details>


### [17] [Rethinking Schema Linking: A Context-Aware Bidirectional Retrieval Approach for Text-to-SQL](https://arxiv.org/abs/2510.14296)
*Md Mahadi Hasan Nahid,Davood Rafiei,Weiwei Zhang,Yong Zhang*

Main category: cs.CL

TL;DR: 这篇论文提出了一种上下文感知的双向模式检索框架，用于解决Text-to-SQL系统中模式链接的问题。该方法通过结合表优先和列优先的检索策略，并辅以问题分解、关键词和短语提取等技术，提高了模式召回率，减少了误报，从而提升了Text-to-SQL的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法常常忽略相关模式元素的检索，导致幻觉和执行失败。该研究旨在解决这一问题。

Method: 该方法提出了一个上下文感知的双向模式检索框架，结合表优先和列优先的检索策略，并辅以问题分解、关键词和短语提取等技术。

Result: 该方法在BIRD和Spider等基准测试中显著提高了模式召回率，减少了误报，并且使用该方法检索到的模式生成的SQL语句优于全模式基线。

Conclusion: 模式链接是提高Text-to-SQL准确性和效率的有力手段，该方法将全模式和完美模式之间的性能差距缩小了50%。

Abstract: Schema linking -- the process of aligning natural language questions with
database schema elements -- is a critical yet underexplored component of
Text-to-SQL systems. While recent methods have focused primarily on improving
SQL generation, they often neglect the retrieval of relevant schema elements,
which can lead to hallucinations and execution failures. In this work, we
propose a context-aware bidirectional schema retrieval framework that treats
schema linking as a standalone problem. Our approach combines two complementary
strategies: table-first retrieval followed by column selection, and
column-first retrieval followed by table selection. It is further augmented
with techniques such as question decomposition, keyword extraction, and
keyphrase extraction. Through comprehensive evaluations on challenging
benchmarks such as BIRD and Spider, we demonstrate that our method
significantly improves schema recall while reducing false positives. Moreover,
SQL generation using our retrieved schema consistently outperforms full-schema
baselines and closely approaches oracle performance, all without requiring
query refinement. Notably, our method narrows the performance gap between full
and perfect schema settings by 50\%. Our findings highlight schema linking as a
powerful lever for enhancing Text-to-SQL accuracy and efficiency.

</details>


### [18] [Language steering in latent space to mitigate unintended code-switching](https://arxiv.org/abs/2510.13849)
*Andrey Goncharov,Nikolai Kondusov,Alexey Zaytsev*

Main category: cs.CL

TL;DR: 提出了一种轻量级的推理时方法，通过在并行翻译上进行PCA识别语言方向，并沿着这些轴引导token嵌入来控制语言身份，从而减轻代码转换，同时保持语义。


<details>
  <summary>Details</summary>
Motivation: 多语言大型语言模型（LLMs）经常表现出无意的代码转换，降低了下游任务的可靠性。

Method: 通过在并行翻译上进行PCA识别语言方向，并沿着这些轴引导token嵌入来控制语言身份。

Result: 使用单个主成分实现了95-99％的语言分类准确率，并在Qwen2.5和Llama-3.2模型上将多个语言对的下一个token分布差异降低了高达42％。

Conclusion: 语言身份集中在具有近乎完美的线性可分性的最终层中。

Abstract: Multilingual Large Language Models (LLMs) often exhibit unintended
code-switching, reducing reliability in downstream tasks. We propose
latent-space language steering, a lightweight inference-time method that
identifies language directions via PCA on parallel translations and steers
token embeddings along these axes to control language identity. Our approach
mitigates code-switching while preserving semantics with negligible
computational overhead and requires only minimal parallel data for calibration.
Empirically, we achieve 95-99\% language classification accuracy using a single
principal component and reduce next-token distributional divergence by up to
42% across multiple language pairs on Qwen2.5 and Llama-3.2 models. We further
analyze the layer-wise evolution of language representations, revealing that
language identity concentrates in final layers with near-perfect linear
separability.

</details>


### [19] [PluriHop: Exhaustive, Recall-Sensitive QA over Distractor-Rich Corpora](https://arxiv.org/abs/2510.14377)
*Mykolas Sveistrys,Richard Kunert*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种新的问题类型，称为 pluri-hop 问题，它需要跨多个文档进行聚合才能回答。作者提出了 PluriHopWIND 数据集来研究这种问题，并提出了 PluriHopRAG 架构来解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 现有的问答系统在处理需要跨多个文档聚合的问题时存在局限性，尤其是在重复性高、干扰信息多的语料库中。

Method: 作者提出了 PluriHopRAG 架构，该架构将查询分解为文档级别的子问题，并使用交叉编码器过滤器来丢弃不相关的文档，然后再进行代价高昂的 LLM 推理。

Result: PluriHopRAG 在 F1 分数上取得了 18-52% 的相对改进。

Conclusion: PluriHopWIND 数据集揭示了当前问答系统在重复性高、干扰信息多的语料库上的局限性。PluriHopRAG 的性能突出了详尽检索和早期过滤作为 top-k 方法的强大替代方案的价值。

Abstract: Recent advances in large language models (LLMs) and retrieval-augmented
generation (RAG) have enabled progress on question answering (QA) when relevant
evidence is in one (single-hop) or multiple (multi-hop) passages. Yet many
realistic questions about recurring report data - medical records, compliance
filings, maintenance logs - require aggregation across all documents, with no
clear stopping point for retrieval and high sensitivity to even one missed
passage. We term these pluri-hop questions and formalize them by three
criteria: recall sensitivity, exhaustiveness, and exactness. To study this
setting, we introduce PluriHopWIND, a diagnostic multilingual dataset of 48
pluri-hop questions built from 191 real-world wind industry reports in German
and English. We show that PluriHopWIND is 8-40% more repetitive than other
common datasets and thus has higher density of distractor documents, better
reflecting practical challenges of recurring report corpora. We test a
traditional RAG pipeline as well as graph-based and multimodal variants, and
find that none of the tested approaches exceed 40% in statement-wise F1 score.
Motivated by this, we propose PluriHopRAG, a RAG architecture that follows a
"check all documents individually, filter cheaply" approach: it (i) decomposes
queries into document-level subquestions and (ii) uses a cross-encoder filter
to discard irrelevant documents before costly LLM reasoning. We find that
PluriHopRAG achieves relative F1 score improvements of 18-52% depending on base
LLM. Despite its modest size, PluriHopWIND exposes the limitations of current
QA systems on repetitive, distractor-rich corpora. PluriHopRAG's performance
highlights the value of exhaustive retrieval and early filtering as a powerful
alternative to top-k methods.

</details>


### [20] [Revisiting the UID Hypothesis in LLM Reasoning Traces](https://arxiv.org/abs/2510.13850)
*Minju Gwak,Guijin Son,Jaehyung Kim*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型(llm)中思维链(cot)推理的信息密度。发现成功的推理具有非均匀的信息密度波动。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于心理语言学中的均匀信息密度(uid)假设，该假设认为人类通过维持稳定的信息流进行交流。论文旨在检验llm的推理过程是否符合uid假设。

Method: 论文引入了基于熵的指标来分析推理轨迹中的信息流，并在三个具有挑战性的数学基准上进行了实验。

Result: 研究结果表明，llm中成功的推理在全球范围内是非均匀的，正确解的特征是信息密度不均匀的波动。这与人类的交流模式形成鲜明对比。

Conclusion: 研究结果挑战了关于机器推理的假设，并为设计可解释和自适应的推理模型提出了新的方向。

Abstract: Large language models (LLMs) often solve problems using step-by-step
Chain-of-Thought (CoT) reasoning, yet these intermediate steps are frequently
unfaithful or hard to interpret. Inspired by the Uniform Information Density
(UID) hypothesis in psycholinguistics -- which posits that humans communicate
by maintaining a stable flow of information -- we introduce entropy-based
metrics to analyze the information flow within reasoning traces. Surprisingly,
across three challenging mathematical benchmarks, we find that successful
reasoning in LLMs is globally non-uniform: correct solutions are characterized
by uneven swings in information density, in stark contrast to human
communication patterns. This result challenges assumptions about machine
reasoning and suggests new directions for designing interpretable and adaptive
reasoning models.

</details>


### [21] [MedTrust-RAG: Evidence Verification and Trust Alignment for Biomedical Question Answering](https://arxiv.org/abs/2510.14400)
*Yingpeng Ning,Yuanyuan Sun,Ling Luo,Yanhua Wang,Yuchen Pan,Hongfei Lin*

Main category: cs.CL

TL;DR: MedTrust-Guided Iterative RAG通过结合引用感知推理、迭代检索验证和MedTrust-Align模块，旨在提高生物医学问答中RAG系统的可靠性，减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有的生物医学问答RAG系统由于检索后的噪声和证据验证不足，容易产生幻觉，影响了回答的可靠性。

Method: 提出了MedTrust-Guided Iterative RAG框架，包含三个创新点：一是强制执行引用感知推理，要求所有生成内容都明确基于检索到的医学文档；二是采用迭代检索验证过程，通过医学差距分析来评估证据的充分性并优化查询；三是整合MedTrust-Align模块，结合验证过的正例和幻觉感知的负例，以强化基于引用的推理并惩罚容易产生幻觉的响应模式。

Result: 在MedMCQA、MedQA和MMLU-Med数据集上的实验表明，该方法在多种模型架构上始终优于现有方法，LLaMA3.1-8B-Instruct和Qwen3-8B的平均准确率分别提高了2.7%和2.4%。

Conclusion: MedTrust-Guided Iterative RAG框架能够有效提高医学问答中RAG系统的准确性和可靠性，减少幻觉的产生。

Abstract: Biomedical question answering (QA) requires accurate interpretation of
complex medical knowledge. Large language models (LLMs) have shown promising
capabilities in this domain, with retrieval-augmented generation (RAG) systems
enhancing performance by incorporating external medical literature. However,
RAG-based approaches in biomedical QA suffer from hallucinations due to
post-retrieval noise and insufficient verification of retrieved evidence,
undermining response reliability. We propose MedTrust-Guided Iterative RAG, a
framework designed to enhance factual consistency and mitigate hallucinations
in medical QA. Our method introduces three key innovations. First, it enforces
citation-aware reasoning by requiring all generated content to be explicitly
grounded in retrieved medical documents, with structured Negative Knowledge
Assertions used when evidence is insufficient. Second, it employs an iterative
retrieval-verification process, where a verification agent assesses evidence
adequacy and refines queries through Medical Gap Analysis until reliable
information is obtained. Third, it integrates the MedTrust-Align Module (MTAM)
that combines verified positive examples with hallucination-aware negative
samples, leveraging Direct Preference Optimization to reinforce
citation-grounded reasoning while penalizing hallucination-prone response
patterns. Experiments on MedMCQA, MedQA, and MMLU-Med demonstrate that our
approach consistently outperforms competitive baselines across multiple model
architectures, achieving the best average accuracy with gains of 2.7% for
LLaMA3.1-8B-Instruct and 2.4% for Qwen3-8B.

</details>


### [22] [EvoEdit: Evolving Null-space Alignment for Robust and Efficient Knowledge Editing](https://arxiv.org/abs/2510.13851)
*Sicheng Lyu,Yu Gu,Xinyu Wang,Jerry Huang,Sitao Luan,Yufei Cui,Xiao-Wen Chang,Peng Lu*

Main category: cs.CL

TL;DR: EvoEdit是一种新的模型编辑策略，通过连续零空间对齐来减少灾难性干扰，从而实现稳定和高效的模型编辑。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要持续更新以纠正过时或错误的知识。模型编辑是引入有针对性的修改而无需完全重新训练的一种引人注目的范例。然而，在顺序编辑上下文中，现有的方法表现出显著的局限性并且遭受灾难性干扰。

Method: 通过为每个传入的编辑执行连续零空间对齐，EvoEdit保留原始和先前修改的知识表示，并在保留的知识上保持输出不变性，即使在长编辑序列中也能有效减少干扰。

Result: 在真实世界的顺序知识编辑基准上的评估表明，EvoEdit实现了比现有最先进的定位然后编辑技术更好或相当的性能，速度提高了高达3.53倍。

Conclusion: 这些结果强调了在动态发展的信息环境中开发更 принципиальных 方法来设计LLM的必要性，同时提供了一个简单而有效的解决方案，并具有强大的理论保证。

Abstract: Large language models (LLMs) require continual updates to rectify outdated or
erroneous knowledge. Model editing has emerged as a compelling paradigm for
introducing targeted modifications without the computational burden of full
retraining. Existing approaches are mainly based on a locate-then-edit
framework. However, in sequential editing contexts, where multiple updates are
applied over time, they exhibit significant limitations and suffer from
catastrophic interference, i.e., new edits compromise previously integrated
updates and degrade preserved knowledge. To address these challenges, we
introduce EvoEdit, a novel editing strategy that mitigates catastrophic
interference through sequential null-space alignment, enabling stable and
efficient model editing. By performing sequential null-space alignment for each
incoming edit, EvoEdit preserves both original and previously modified
knowledge representations and maintains output invariance on preserved
knowledge even across long edit sequences, effectively mitigating interference.
Evaluations on real-world sequential knowledge-editing benchmarks show that
EvoEdit achieves better or comparable performance than prior state-of-the-art
locate-then-edit techniques, with up to 3.53 times speedup. Overall, these
results underscore the necessity of developing more principled approaches for
designing LLMs in dynamically evolving information settings, while providing a
simple yet effective solution with strong theoretical guarantees.

</details>


### [23] [ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups](https://arxiv.org/abs/2510.13852)
*Peter Banyas,Shristi Sharma,Alistair Simmons,Atharva Vispute*

Main category: cs.CL

TL;DR: ConsistencyAI是一个用于评估大型语言模型（LLM）针对不同角色在事实一致性上的独立基准。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在衡量大型语言模型（LLM）是否会根据不同人口统计特征的用户提供不一致的事实性回答。

Method: 研究人员使用包含15个主题的提示，向19个LLM提问，并为每个LLM重复100次，每次添加来自不同角色的提示上下文。然后，将响应处理成句子嵌入，计算跨角色余弦相似度，并计算加权平均值以得出事实一致性得分。

Result: 在100个角色的实验中，得分范围从0.9065到0.7896，平均值为0.8656。xAI的Grok-3一致性最高，而一些轻量级模型排名最低。一致性因主题而异：就业市场一致性最低，G7世界领导人一致性最高，疫苗或以色列-巴勒斯坦冲突等问题因提供者而异。

Conclusion: 研究结果表明，提供者和主题都会影响事实一致性。

Abstract: Is an LLM telling you different facts than it's telling me? This paper
introduces ConsistencyAI, an independent benchmark for measuring the factual
consistency of large language models (LLMs) for different personas.
ConsistencyAI tests whether, when users of different demographics ask identical
questions, the model responds with factually inconsistent answers. Designed
without involvement from LLM providers, this benchmark offers impartial
evaluation and accountability. In our experiment, we queried 19 LLMs with
prompts that requested 5 facts for each of 15 topics. We repeated this query
100 times for each LLM, each time adding prompt context from a different
persona selected from a subset of personas modeling the general population. We
processed the responses into sentence embeddings, computed cross-persona cosine
similarity, and computed the weighted average of cross-persona cosine
similarity to calculate factual consistency scores. In 100-persona experiments,
scores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as
a benchmark threshold. xAI's Grok-3 is most consistent, while several
lightweight models rank lowest. Consistency varies by topic: the job market is
least consistent, G7 world leaders most consistent, and issues like vaccines or
the Israeli-Palestinian conflict diverge by provider. These results show that
both the provider and the topic shape the factual consistency. We release our
code and interactive demo to support reproducible evaluation and encourage
persona-invariant prompting strategies.

</details>


### [24] [R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging](https://arxiv.org/abs/2510.13854)
*Mamadou K. Keita,Christopher Homan,Sebastien Diarra*

Main category: cs.CL

TL;DR: 提出了一种混合方法，将语言规则的多层系统直接整合到神经网络的训练目标中。


<details>
  <summary>Details</summary>
Motivation: 该模型旨在处理词汇表外(OOV)的单词，并具有原则性的不确定性。

Method: R2T框架，一种混合方法，它集成了一个多层语言规则系统到一个神经网络的训练目标中。该方法使用一个自适应损失函数，其中包括一个正则化项，用于训练模型以处理具有原则性不确定性的词汇表外(OOV)单词。

Result: 在Zarma词性标注(POS)上的实验表明，仅在未标记文本上训练的R2T-BiLSTM模型达到了98.2%的准确率，优于在300个标记句子上微调的AfriBERTa等基线。我们进一步表明，对于像命名实体识别(NER)这样更复杂的任务，R2T可以作为一个强大的预训练步骤;一个用R2T预训练并在50个标记句子上微调的模型优于一个在300个句子上训练的基线。

Conclusion: R2T框架在词性标注和命名实体识别任务中表现出色，证明了其有效性。

Abstract: We introduce the Rule-to-Tag (R2T) framework, a hybrid approach that
integrates a multi-tiered system of linguistic rules directly into a neural
network's training objective. R2T's novelty lies in its adaptive loss function,
which includes a regularization term that teaches the model to handle
out-of-vocabulary (OOV) words with principled uncertainty. We frame this work
as a case study in a paradigm we call principled learning (PrL), where models
are trained with explicit task constraints rather than on labeled examples
alone. Our experiments on Zarma part-of-speech (POS) tagging show that the
R2T-BiLSTM model, trained only on unlabeled text, achieves 98.2% accuracy,
outperforming baselines like AfriBERTa fine-tuned on 300 labeled sentences. We
further show that for more complex tasks like named entity recognition (NER),
R2T serves as a powerful pre-training step; a model pre-trained with R2T and
fine-tuned on just 50 labeled sentences outperformes a baseline trained on 300.

</details>


### [25] [Intent Clustering with Shared Pseudo-Labels](https://arxiv.org/abs/2510.14640)
*I-Fan Lin,Faegheh Hasibi,Suzan Verberne*

Main category: cs.CL

TL;DR: 提出了一种直观、免训练、免标签的意图聚类方法，该方法使用轻量级和开源的LLM，并做出最小的假设。


<details>
  <summary>Details</summary>
Motivation: 许多当前的方法依赖于商业LLM，这些LLM成本高昂且透明度有限。 此外，他们的方法通常明确依赖于预先知道集群的数量，而在实际环境中通常并非如此。

Method: 首先要求LLM为每个文本生成伪标签，然后在每个文本的伪标签集中执行多标签分类。基于同一集群的文本将共享更多标签的假设。

Result: 在四个基准数据集上的评估表明，该方法实现了与最新基线相当甚至更好的结果，同时保持简单且计算效率高。

Conclusion: 我们的研究结果表明，我们的方法可以应用于低资源场景，并且在多个模型和数据集中是稳定的。

Abstract: In this paper, we propose an intuitive, training-free and label-free method
for intent clustering that makes minimal assumptions using lightweight and
open-source LLMs. Many current approaches rely on commercial LLMs, which are
costly, and offer limited transparency. Additionally, their methods often
explicitly depend on knowing the number of clusters in advance, which is often
not the case in realistic settings. To address these challenges, instead of
asking the LLM to match similar text directly, we first ask it to generate
pseudo-labels for each text, and then perform multi-label classification in
this pseudo-label set for each text. This approach is based on the hypothesis
that texts belonging to the same cluster will share more labels, and will
therefore be closer when encoded into embeddings. These pseudo-labels are more
human-readable than direct similarity matches. Our evaluation on four benchmark
sets shows that our approach achieves results comparable to and better than
recent baselines, while remaining simple and computationally efficient. Our
findings indicate that our method can be applied in low-resource scenarios and
is stable across multiple models and datasets.

</details>


### [26] [Harnessing Consistency for Robust Test-Time LLM Ensemble](https://arxiv.org/abs/2510.13855)
*Zhichen Zeng,Qi Yu,Xiao Lin,Ruizhong Qiu,Xuying Ning,Tianxin Wei,Yuchen Yan,Jingrui He,Hanghang Tong*

Main category: cs.CL

TL;DR: LLM集成是一种很有前途的方法，但它对潜在的错误信号的鲁棒性关注不足。本文提出了CoRE，一种利用模型一致性的即插即用技术，用于稳健的LLM集成。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(llm)表现出不同的优势和弱点，LLM集成是一种很有前途的方法，可以整合它们互补的能力。尽管在提高集成质量方面取得了实质性进展，但人们对集成对潜在错误信号的鲁棒性的关注有限，这些错误信号通常来自不同的标记化方案和不同的模型专业知识。

Method: 我们提出了CoRE，一种利用模型一致性的即插即用技术，用于健壮的LLM集成，它可以与不同的集成方法无缝集成。Token-level一致性通过应用低通滤波器来降低具有高不一致性的不确定token的权重来捕获细粒度的不一致性，通常是由于token未对齐造成的，从而在细粒度级别上提高鲁棒性。模型级一致性通过提升具有高自置信度和与其他模型发散最小的模型输出来模拟全局一致性，从而在更粗的级别上增强鲁棒性。

Result: 在不同的基准、模型组合和集成策略上的大量实验表明，CoRE始终提高集成性能和鲁棒性。

Conclusion: 本文提出了CoRE，一种利用模型一致性的即插即用技术，用于稳健的LLM集成，它可以与不同的集成方法无缝集成，并且在不同的基准、模型组合和集成策略上的大量实验表明，CoRE始终提高集成性能和鲁棒性。

Abstract: Different large language models (LLMs) exhibit diverse strengths and
weaknesses, and LLM ensemble serves as a promising approach to integrate their
complementary capabilities. Despite substantial progress in improving ensemble
quality, limited attention has been paid to the robustness of ensembles against
potential erroneous signals, which often arise from heterogeneous tokenization
schemes and varying model expertise. Our analysis shows that ensemble failures
typically arise from both the token level and the model level: the former
reflects severe disagreement in token predictions, while the latter involves
low confidence and pronounced disparities among models. In light of this, we
propose CoRE, a plug-and-play technique that harnesses model consistency for
robust LLM ensemble, which can be seamlessly integrated with diverse ensemble
methods. Token-level consistency captures fine-grained disagreements by
applying a low-pass filter to downweight uncertain tokens with high
inconsistency, often due to token misalignment, thereby improving robustness at
a granular level. Model-level consistency models global agreement by promoting
model outputs with high self-confidence and minimal divergence from others,
enhancing robustness at a coarser level. Extensive experiments across diverse
benchmarks, model combinations, and ensemble strategies demonstrate that CoRE
consistently improves ensemble performance and robustness.

</details>


### [27] [An Efficient Rubric-based Generative Verifier for Search-Augmented LLMs](https://arxiv.org/abs/2510.14660)
*Linyue Ma,Yilong Xu,Xiang Long,Zhi Zheng*

Main category: cs.CL

TL;DR: 本文提出了一种名为“nugget-as-rubric”的统一且可验证的范例，它将原子信息点视为不同搜索增强工作负载的结构化评估标准，并设计了一个自动评分标准构建流程，该流程可以自动检索与每个问题相关的段落并从中提取评分标准。此外，本文还介绍了一种名为 Search-Gen-V 的 4B 参数高效生成验证器，实验结果表明，Search-Gen-V 在不同的工作负载中实现了强大的验证准确性，使其成为搜索增强型 LLM 的可扩展、稳健且高效的可验证奖励构造器。


<details>
  <summary>Details</summary>
Motivation: 现有的搜索增强型 LLM 奖励建模面临一些局限性。基于规则的奖励虽然可验证，但容易受到表达方式变化的影响，并且不能应用于长格式工作负载。相比之下，生成式奖励提高了鲁棒性，但为动态语料库中的长格式工作负载设计可验证且稳定的奖励仍然具有挑战性，并且还会产生高昂的计算成本。

Method: 本文提出了一种统一且可验证的范例“nugget-as-rubric”，它将原子信息点视为不同搜索增强工作负载的结构化评估标准。为了支持长格式设置，本文设计了一个基于查询重写的自动评分标准构建流程，该流程可以自动检索与每个问题相关的段落并从中提取评分标准，无论是来自静态语料库还是来自动态在线 Web 内容。此外，本文还引入了 Search-Gen-V，这是一个在本文提出的可验证范例下，通过蒸馏思想和两阶段策略训练的 4B 参数高效生成验证器。

Result: 实验结果表明，Search-Gen-V 在不同的工作负载中实现了强大的验证准确性。

Conclusion: Search-Gen-V 是一种可扩展、稳健且高效的可验证奖励构造器，适用于搜索增强型 LLM。

Abstract: Search augmentation empowers Large Language Models with retrieval
capabilities to overcome the limitations imposed by static parameters.
Recently, Reinforcement Learning leverages tailored reward signals as a viable
technique to enhance LLMs performing tasks involving search. However, existing
reward modeling for search-augmented LLMs faces several limitations. Rule-based
rewards, such as Exact Match, are verifiable but fragile to variations in
expression and cannot be applied to long-form workloads. In contrast,
generative rewards improve robustness, but designing verifiable and stable
rewards for long-form workloads in dynamic corpora remains challenging and also
incurs high computational costs. In this paper, we propose a unified and
verifiable paradigm, "nugget-as-rubric", which treats atomic information points
as structured evaluation criteria for different search-augmentation workloads.
Short-form tasks correspond to a single rubric, whereas long-form tasks expand
to multiple rubrics aligned with the question's information needs. To support
long-form settings, we design an automatic rubric construction pipeline based
on query rewriting, which can automatically retrieve passages relevant to each
question and extract rubrics from them, both from static corpora and from
dynamic online web content. Furthermore, we introduce \textbf{Search-Gen-V}, a
4B-parameter efficient generative verifier under our proposed verifiable
paradigm, which is trained via the idea of distillation and a two-stage
strategy. Experimental results show that Search-Gen-V achieves strong
verification accuracy across different workloads, making it a scalable, robust,
and efficient verifiable reward constructor for search-augmented LLMs.

</details>


### [28] [Multimodal Retrieval-Augmented Generation with Large Language Models for Medical VQA](https://arxiv.org/abs/2510.13856)
*A H M Rezaul Karim,Ozlem Uzuner*

Main category: cs.CL

TL;DR: MasonNLP系统在MEDIQA-WV 2025的wound-care VQA任务中排名第三，该系统利用通用领域的大型语言模型和检索增强生成（RAG）框架，通过结合领域内文本和视觉示例，在临床相关示例中改进推理、模式遵守和响应质量。


<details>
  <summary>Details</summary>
Motivation: 为了支持临床决策和患者护理，医学视觉问答(MedVQA) 允许对医学图像进行自然语言查询。MEDIQA-WV 2025 共享任务解决了 wound-care VQA 问题，要求系统从图像和患者查询中生成自由文本响应和结构化伤口属性。

Method: MasonNLP系统采用了一个通用领域的、指令调整的大型语言模型，该模型具有检索增强生成（RAG）框架，该框架结合了来自领域内数据的文本和视觉示例。

Result: 该系统在19个团队和51个提交作品中排名第三，平均得分41.37%。

Conclusion: 轻量级RAG与通用LLM相结合，通过简单的索引和融合添加一些相关示例，无需额外的训练或复杂的重新排序，为多模态临床NLP任务提供了一个简单有效的基线。

Abstract: Medical Visual Question Answering (MedVQA) enables natural language queries
over medical images to support clinical decision-making and patient care. The
MEDIQA-WV 2025 shared task addressed wound-care VQA, requiring systems to
generate free-text responses and structured wound attributes from images and
patient queries. We present the MasonNLP system, which employs a
general-domain, instruction-tuned large language model with a
retrieval-augmented generation (RAG) framework that incorporates textual and
visual examples from in-domain data. This approach grounds outputs in
clinically relevant exemplars, improving reasoning, schema adherence, and
response quality across dBLEU, ROUGE, BERTScore, and LLM-based metrics. Our
best-performing system ranked 3rd among 19 teams and 51 submissions with an
average score of 41.37%, demonstrating that lightweight RAG with
general-purpose LLMs -- a minimal inference-time layer that adds a few relevant
exemplars via simple indexing and fusion, with no extra training or complex
re-ranking -- provides a simple and effective baseline for multimodal clinical
NLP tasks.

</details>


### [29] [ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP Architecture and Paired Weight Sharing](https://arxiv.org/abs/2510.13860)
*Shivanshu Kumar,Gopalakrishnan Srinivasan*

Main category: cs.CL

TL;DR: 提出了一种名为ShishuLM的高效语言模型架构，旨在减少参数数量和Key-Value (KV)缓存需求。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在自然语言处理任务中表现出色，但存在大量内存和计算开销。研究表明，这些模型中存在显著的架构冗余，为优化提供了机会。

Method: 通过结合AI可解释性和推理时层剪枝的研究，提出ShishuLM，利用MLP近似Transformer块。

Result: ShishuLM在内存需求方面减少高达25%，在训练和推理期间的延迟方面提高高达40%。

Conclusion: 实验和分析结果为从预训练角度构建更高效的SLM架构提供了见解。

Abstract: While the transformer architecture has achieved state-of-the-art performance
on natural language processing tasks, these models impose substantial memory
and computational overhead. Recent research has identified significant
architectural redundancies within these models, presenting opportunities for
optimization without compromising performance. Taking insights from research in
AI interpretability and inference-time layer pruning, we introduce an efficient
language model architecture, referred to as ShishuLM, which reduces both the
parameter count and Key-Value (KV) cache requirements. Given the increasing
importance of Small Language Models (SLMs) in agentic AI systems, we evaluate
our approach on two SLMs of different scales. Our analysis reveals that for
moderate-context scenarios, normalization coupled with attention computation is
roughly linear with the input, enabling entire transformer blocks to be
approximated through Multi-Layer Perceptrons (MLPs). Our results show that
ShishuLM provides up to 25% reduction in memory requirements and up to 40%
improvement in latency during both training and inference, compared to parent
models. Our experimental and analytical findings provide insights towards
building more efficient SLM architectures from a pre-training standpoint.

</details>


### [30] [Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM Reranking](https://arxiv.org/abs/2510.14824)
*Ziqi Dai,Xin Zhang,Mingxin Li,Yanzhao Zhang,Dingkun Long,Pengjun Xie,Meishan Zhang,Wenjie Li,Min Zhang*

Main category: cs.CL

TL;DR: 对比学习 (CL) 和监督微调 (SFT) 哪个更适合基于大型语言模型 (LLM) 的重排序。


<details>
  <summary>Details</summary>
Motivation: BERT 风格的编码器通常使用对比学习，而大型语言模型通常使用监督微调。研究目的是确定哪种目标更适合基于 LLM 的重排序，以及造成这种差异的潜在机制。

Method: 对重排序的 CL 和 SFT 进行了全面的比较和分析，并将目标分解为权重和方向两个组成部分，提出了一个统一的框架来理解它们的相互作用。

Result: SFT 提供了比 CL 更强的加权方案，而首选的评分方向没有明显的胜者。通过 SFT 进行大规模训练，在 MRB 基准测试上提出了新的最先进的重排序器。

Conclusion: 对于 LLM 重排序，SFT 优于 CL。

Abstract: In information retrieval, training reranking models mainly focuses on two
types of objectives: metric learning (e.g. contrastive loss to increase the
predicted scores on relevant query-document pairs) and classification (binary
label prediction of relevance vs. irrelevance). For BERT-style encoders,
various studies have shown that contrastive learning (CL) can be more effective
than discriminative (classification) learning. However, for large language
models (LLMs), classification via supervised fine-tuning (SFT), which predicts
''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears
more promising as it aligns well with the generative nature of LLMs. This
divergence raises a central question: which objective is intrinsically better
suited to LLM-based reranking, and what mechanism underlies the difference? In
this work, we conduct a comprehensive comparison and analysis between CL and
SFT for reranking, taking the universal multimodal retrieval (UMR) as the
experimental playground. We first decompose the objectives into two components:
weight, which controls the magnitude of those updates, and direction, which
guides the model updates, then present a unified framework for understanding
their interactions. Through probing experiments, we find that SFT provides a
substantially stronger weighting scheme than CL, whereas the preferred scoring
direction shows no clear winner. Taken together, these results point to a
consistent advantage of SFT over CL for LLM reranking. To further validate our
findings, we conduct large-scale training with SFT and present new
state-of-the-art rerankers on the MRB benchmark. We also provide ablations on
SFT settings and expect our findings to benefit future research and
applications in this area.

</details>


### [31] [Ensembling Large Language Models to Characterize Affective Dynamics in Student-AI Tutor Dialogues](https://arxiv.org/abs/2510.13862)
*Chenyu Zhang,Sharifa Alghowinem,Cynthia Breazeal*

Main category: cs.CL

TL;DR: 本研究调查了大型语言模型（LLM）在教育环境中的情感动态，通过分析PyTutor与本科生之间的对话数据，利用集成LLM框架进行情感感知，发现学生在使用AI导师时情绪波动频繁，积极情绪脆弱但负面情绪能快速消解，中性时刻是干预的良好时机。


<details>
  <summary>Details</summary>
Motivation: 目前对LLM介导的辅导中情感动态的理解不足，需要进一步研究生成式AI融入教育的负责任途径，关注学习者不断变化的情感状态。

Method: 使用集成LLM框架，分析了16986轮PyTutor与261名本科生之间的对话，通过Gemini、GPT-4o和Claude三个LLM生成情感标注，包括效价、唤醒和学习帮助性的标量评级以及自由文本情感标签，并通过排序加权的模型内池化和跨模型多数共识融合这些估计，以生成稳健的情感概况。

Result: 学生在使用AI导师时通常表现出轻微的积极情感和适度的唤醒水平。困惑和好奇是解决问题的常见伴随物，沮丧虽然不常见，但仍然会以可能阻碍进展的方式出现。积极情绪持续时间略长于中性或负面情绪，但它们是脆弱的并且容易被打断。负面情绪通常会很快消解，有时会直接反弹到积极状态。中性时刻通常充当转折点，更多时候是向上引导学生。

Conclusion: 研究表明，在AI辅导互动中，学生情绪波动频繁，中性时刻是导师介入的良好时机。

Abstract: While recent studies have examined the leaning impact of large language model
(LLM) in educational contexts, the affective dynamics of LLM-mediated tutoring
remain insufficiently understood. This work introduces the first ensemble-LLM
framework for large-scale affect sensing in tutoring dialogues, advancing the
conversation on responsible pathways for integrating generative AI into
education by attending to learners' evolving affective states. To achieve this,
we analyzed two semesters' worth of 16,986 conversational turns exchanged
between PyTutor, an LLM-powered AI tutor, and 261 undergraduate learners across
three U.S. institutions. To investigate learners' emotional experiences, we
generate zero-shot affect annotations from three frontier LLMs (Gemini, GPT-4o,
Claude), including scalar ratings of valence, arousal, and
learning-helpfulness, along with free-text emotion labels. These estimates are
fused through rank-weighted intra-model pooling and plurality consensus across
models to produce robust emotion profiles. Our analysis shows that during
interaction with the AI tutor, students typically report mildly positive affect
and moderate arousal. Yet learning is not uniformly smooth: confusion and
curiosity are frequent companions to problem solving, and frustration, while
less common, still surfaces in ways that can derail progress. Emotional states
are short-lived--positive moments last slightly longer than neutral or negative
ones, but they are fragile and easily disrupted. Encouragingly, negative
emotions often resolve quickly, sometimes rebounding directly into positive
states. Neutral moments frequently act as turning points, more often steering
students upward than downward, suggesting opportunities for tutors to intervene
at precisely these junctures.

</details>


### [32] [Unlocking the Potential of Diffusion Language Models through Template Infilling](https://arxiv.org/abs/2510.13870)
*Junhoo Lee,Seungyeon Kim,Nojun Kwak*

Main category: cs.CL

TL;DR: 提出了一种新的扩散语言模型（DLM）条件调节方法，称为模板填充（TI），该方法首先生成目标响应的结构模板，然后填充掩码段。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（DLM）已成为自回归语言模型的一种有前景的替代方案，但它们的推理策略仍然局限于从自回归范式继承的基于前缀的提示。

Method: 提出模板填充（TI），一种为DLM生成过程量身定制的条件调节方法。引入动态段分配（DSA），它可以根据生成置信度自适应地调整段长度。

Result: 在数学推理和代码生成基准测试中，相对于基线实现了持续 17.01$\%$p 的改进。在多 token 生成设置中提供了额外的优势，可以在保持生成质量的同时实现有效的加速。

Conclusion: 模板填充（TI）是一种有效的扩散语言模型条件调节方法，在数学推理和代码生成方面都优于基线模型。

Abstract: Diffusion Language Models (DLMs) have emerged as a promising alternative to
Autoregressive Language Models, yet their inference strategies remain limited
to prefix-based prompting inherited from the autoregressive paradigm. In this
paper, we propose Template Infilling (TI), a tailored conditioning methodology
for DLMs' generation process. Unlike conventional prefix prompting, TI first
generates a structural template for the target response, then fills in the
masked segments. To enhance the flexibility of this structural control, we
introduce Dynamic Segment Allocation (DSA), which adaptively adjusts segment
lengths based on generation confidence. We demonstrate the effectiveness of our
approach on mathematical reasoning and code generation benchmarks, achieving
consistent improvements of 17.01$\%$p over baseline. Furthermore, we show that
TI provides additional advantages in multi-token generation settings, enabling
effective speedup while maintaining generation quality.

</details>


### [33] [Quechua Speech Datasets in Common Voice: The Case of Puno Quechua](https://arxiv.org/abs/2510.13871)
*Elwin Huaman,Wendi Huaman,Jorge Luis Huaman,Ninfa Quispe*

Main category: cs.CL

TL;DR: Common Voice is used to create speech datasets for under-resourced languages like Quechua.


<details>
  <summary>Details</summary>
Motivation: Under-resourced languages lack data for speech technology development.

Method: Integration of Quechua languages into Common Voice, focusing on Puno Quechua as a case study.

Result: Common Voice hosts 191.1 hours of Quechua speech, with Puno Quechua contributing 12 hours.

Conclusion: The study contributes to inclusive voice technology and digital empowerment of under-resourced language communities and proposes a research agenda addressing technical challenges and ethical considerations.

Abstract: Under-resourced languages, such as Quechuas, face data and resource scarcity,
hindering their development in speech technology. To address this issue, Common
Voice presents a crucial opportunity to foster an open and community-driven
speech dataset creation. This paper examines the integration of Quechua
languages into Common Voice. We detail the current 17 Quechua languages,
presenting Puno Quechua (ISO 639-3: qxp) as a focused case study that includes
language onboarding and corpus collection of both reading and spontaneous
speech data. Our results demonstrate that Common Voice now hosts 191.1 hours of
Quechua speech (86\% validated), with Puno Quechua contributing 12 hours (77\%
validated), highlighting the Common Voice's potential. We further propose a
research agenda addressing technical challenges, alongside ethical
considerations for community engagement and indigenous data sovereignty. Our
work contributes towards inclusive voice technology and digital empowerment of
under-resourced language communities.

</details>


### [34] [FRACCO: A gold-standard annotated corpus of oncological entities with ICD-O-3.1 normalisation](https://arxiv.org/abs/2510.13873)
*Johann Pignat,Milena Vucetic,Christophe Gaudet-Blavignac,Jamil Zaghir,Amandine Stettler,Fanny Amrein,Jonatan Bonjour,Jean-Philippe Goldman,Olivier Michielin,Christian Lovis,Mina Bjelogrlic*

Main category: cs.CL

TL;DR: FRACCO: a French clinical oncology corpus with expert annotations of 1301 synthetic cases translated from Spanish, covering morphology, topography, and histologic differentiation using ICD-O.


<details>
  <summary>Details</summary>
Motivation: Lack of French oncology resources for NLP tool development.

Method: Expert annotation of 1301 synthetic French clinical cases, translated from the Spanish CANTEMIST corpus, with ICD-O normalization and composite expression-level normalizations. Annotation quality was ensured through expert review.

Result: A dataset with 71127 ICD-O normalisations, 399 unique morphology codes, 272 topography codes, and 2043 unique composite expressions.

Conclusion: FRACCO provides a reference standard for named entity recognition and concept normalisation in French oncology texts.

Abstract: Developing natural language processing tools for clinical text requires
annotated datasets, yet French oncology resources remain scarce. We present
FRACCO (FRench Annotated Corpus for Clinical Oncology) an expert-annotated
corpus of 1301 synthetic French clinical cases, initially translated from the
Spanish CANTEMIST corpus as part of the FRASIMED initiative. Each document is
annotated with terms related to morphology, topography, and histologic
differentiation, using the International Classification of Diseases for
Oncology (ICD-O) as reference. An additional annotation layer captures
composite expression-level normalisations that combine multiple ICD-O elements
into unified clinical concepts. Annotation quality was ensured through expert
review: 1301 texts were manually annotated for entity spans by two domain
experts. A total of 71127 ICD-O normalisations were produced through a
combination of automated matching and manual validation by a team of five
annotators. The final dataset representing 399 unique morphology codes (from
2549 different expressions), 272 topography codes (from 3143 different
expressions), and 2043 unique composite expressions (from 11144 different
expressions). This dataset provides a reference standard for named entity
recognition and concept normalisation in French oncology texts.

</details>


### [35] [What Layers When: Learning to Skip Compute in LLMs with Residual Gates](https://arxiv.org/abs/2510.13876)
*Filipe Laitenberger,Dawid Kopiczko,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CL

TL;DR: GateSkip: a residual-stream gating mechanism for token-wise layer skipping in decoder-only LMs.


<details>
  <summary>Details</summary>
Motivation: To save compute and improve performance of decoder-only LMs.

Method: Equipping each Attention/MLP branch with a sigmoid-linear gate and skipping low-importance tokens based on gate values during inference.

Result: Up to 15% compute savings with over 90% accuracy on long-form reasoning; accuracy gains at full compute and matched baseline quality near 50% savings on instruction-tuned models.

Conclusion: Learned gates provide insights into transformer information flow and the method combines easily with other optimization techniques.

Abstract: We introduce GateSkip, a simple residual-stream gating mechanism that enables
token-wise layer skipping in decoder-only LMs. Each Attention/MLP branch is
equipped with a sigmoid-linear gate that condenses the branch's output before
it re-enters the residual stream. During inference we rank tokens by the gate
values and skip low-importance ones using a per-layer budget. While early-exit
or router-based Mixture-of-Depths models are known to be unstable and need
extensive retraining, our smooth, differentiable gates fine-tune stably on top
of pretrained models. On long-form reasoning, we save up to 15\% compute while
retaining over 90\% of baseline accuracy. On instruction-tuned models we see
accuracy gains at full compute and match baseline quality near 50\% savings.
The learned gates give insight into transformer information flow (e.g., BOS
tokens act as anchors), and the method combines easily with quantization,
pruning, and self-speculative decoding.

</details>


### [36] [TextBandit: Evaluating Probabilistic Reasoning in LLMs Through Language-Only Decision Tasks](https://arxiv.org/abs/2510.13878)
*Jimin Lim,Arjun Damerla,Arthur Jiang,Nam Le*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在不确定性下进行连续决策的能力，通过纯文本反馈与多臂老虎机环境交互。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在仅使用自然语言的情况下进行连续决策的能力。

Method: 引入一个新的基准，其中大型语言模型使用纯文本反馈与多臂老虎机环境交互，没有任何数值线索或明确的概率。

Result: Qwen3-4B 实现了 89.2% 的最佳臂选择率，显著优于更大的大型语言模型和传统方法。

Conclusion: 概率推理能够仅从语言中产生，并将此基准作为评估自然、非数字环境中决策能力的一步。

Abstract: Large language models (LLMs) have shown to be increasingly capable of
performing reasoning tasks, but their ability to make sequential decisions
under uncertainty only using natural language remains underexplored. We
introduce a novel benchmark in which LLMs interact with multi-armed bandit
environments using purely textual feedback, "you earned a token", without
access to numerical cues or explicit probabilities, resulting in the model to
infer latent reward structures purely off linguistic cues and to adapt
accordingly. We evaluated the performance of four open-source LLMs and compare
their performance to standard decision-making algorithms such as Thompson
Sampling, Epsilon Greedy, Upper Confidence Bound (UCB), and random choice.
While most of the LLMs underperformed compared to the baselines, Qwen3-4B,
achieved the best-arm selection rate of 89.2% , which significantly
outperformed both the larger LLMs and traditional methods. Our findings suggest
that probabilistic reasoning is able to emerge from language alone, and we
present this benchmark as a step towards evaluating decision-making
capabilities in naturalistic, non-numeric contexts.

</details>


### [37] [Catch Your Breath: Adaptive Computation for Self-Paced Sequence Production](https://arxiv.org/abs/2510.13879)
*Alexandre Galashov,Matt Jones,Rosemary Ke,Yuan Cao,Vaishnavh Nagarajan,Michael C. Mozer*

Main category: cs.CL

TL;DR: 提出了一种允许语言模型动态和自主地扩展每个输入token所使用的计算步骤数量的监督训练目标，模型可以通过发出<don't know>输出来请求额外的计算步骤。


<details>
  <summary>Details</summary>
Motivation: 使模型能够明智地使用<don't know>输出并校准其不确定性。

Method: 将每个输出token的选择构建为一个具有时间成本的序列决策问题，并研究了三种方法：CYB-AP、CYB-VA和CYB-DP。

Result: CYB模型只需要baseline模型三分之一的训练数据即可达到相同的性能，并且只需要具有pause和交叉熵损失的模型的二分之一的数据。CYB模型会在提高准确性的情况下请求额外的步骤，并且该模型会根据token级别的复杂性和上下文调整其处理时间。

Conclusion: CYB模型能够根据token级别的复杂性和上下文调整其处理时间，并且在提高准确性的情况下请求额外的步骤。

Abstract: We explore a class of supervised training objectives that allow a language
model to dynamically and autonomously scale the number of compute steps used
for each input token. For any token, the model can request additional compute
steps by emitting a <don't know> output. If the model is granted a delay, a
specialized <pause> token is inserted at the next input step, providing the
model with additional compute resources to generate an output. The model can
request multiple pauses. To train the model to use <don't know> outputs
judiciously and to calibrate its uncertainty, we frame the selection of each
output token as a sequential-decision problem with a time cost. We refer to the
class of methods as $\textit{Catch Your Breath}$ losses and we study three
methods in this class: CYB-AP frames the model's task as anytime prediction,
where an output may be required at any step and accuracy is discounted over
time; CYB-VA is a variational approach that aims to maximize prediction
accuracy subject to a specified distribution over stopping times; and CYB-DP
imposes a penalty based on a computational budget. Through fine-tuning
experiments, we identify the best performing loss variant. The CYB model needs
only one third as much training data as the baseline (no pause) model needs to
achieve the same performance, and half as much data as a model with pauses and
a cross-entropy loss. We find that the CYB model requests additional steps when
doing so improves accuracy, and the model adapts its processing time to
token-level complexity and context. For example, it often pauses after plural
nouns like $\textit{patients}$ and $\textit{challenges}$ but never pauses after
the first token of contracted words like $\textit{wasn}$ and $\textit{didn}$,
and it shows high variability for ambiguous tokens like $\textit{won}$, which
could function as either a verb or part of a contraction.

</details>


### [38] [PAGE: Prompt Augmentation for text Generation Enhancement](https://arxiv.org/abs/2510.13880)
*Mauro Jose Pacchiotti,Luciana Ballejos,Mariel Ale*

Main category: cs.CL

TL;DR: 提出了一个名为PAGE的框架，通过简单的辅助模块来提升文本生成模型的性能，无需额外的生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有的自然语言生成模型在特定任务或需求下表现不佳，需要大量额外数据进行调整。

Method: 利用分类器或提取器等轻量级模型从输入文本中提取信息，构建富化的输入，从而提升生成质量和可控性。

Result: 在需求工程领域进行了概念验证，使用带有分类器的辅助模块来提高软件需求生成的质量。

Conclusion: PAGE框架通过模块化架构，易于适应不同的任务，能够有效提升文本生成模型的性能。

Abstract: In recent years, natural language generative models have shown outstanding
performance in text generation tasks. However, when facing specific tasks or
particular requirements, they may exhibit poor performance or require
adjustments that demand large amounts of additional data. This work introduces
PAGE (Prompt Augmentation for text Generation Enhancement), a framework
designed to assist these models through the use of simple auxiliary modules.
These modules, lightweight models such as classifiers or extractors, provide
inferences from the input text. The output of these auxiliaries is then used to
construct an enriched input that improves the quality and controllability of
the generation. Unlike other generation-assistance approaches, PAGE does not
require auxiliary generative models; instead, it proposes a simpler, modular
architecture that is easy to adapt to different tasks. This paper presents the
proposal, its components and architecture, and reports a proof of concept in
the domain of requirements engineering, where an auxiliary module with a
classifier is used to improve the quality of software requirements generation.

</details>


### [39] [Too Open for Opinion? Embracing Open-Endedness in Large Language Models for Social Simulation](https://arxiv.org/abs/2510.13884)
*Bolei Ma,Yong Cao,Indira Sen,Anna-Carolina Haensch,Frauke Kreuter,Barbara Plank,Daniel Hershcovich*

Main category: cs.CL

TL;DR: 当前的大语言模型（LLM）研究为了方便评分和比较，通常采用选择题或简答题形式，忽略了LLM的生成性本质。本文认为，开放式、自由文本的LLM对于真实的社会模拟至关重要。


<details>
  <summary>Details</summary>
Motivation: 强调开放式LLM在社会模拟中的价值，认为其能够改进测量和设计，支持探索未预料的观点，并减少研究者施加的指导性偏差。

Method: 借鉴了数十年的调查方法研究和自然语言处理（NLP）的最新进展。

Result: 开放式LLM能够捕捉表达性和个体性，有助于预测试，并最终提高方法论的效用。

Conclusion: 呼吁采用新的实践和评估框架，利用而非限制LLM的开放式生成多样性，从而在自然语言处理和社会科学之间建立协同效应。

Abstract: Large Language Models (LLMs) are increasingly used to simulate public opinion
and other social phenomena. Most current studies constrain these simulations to
multiple-choice or short-answer formats for ease of scoring and comparison, but
such closed designs overlook the inherently generative nature of LLMs. In this
position paper, we argue that open-endedness, using free-form text that
captures topics, viewpoints, and reasoning processes "in" LLMs, is essential
for realistic social simulation. Drawing on decades of survey-methodology
research and recent advances in NLP, we argue why this open-endedness is
valuable in LLM social simulations, showing how it can improve measurement and
design, support exploration of unanticipated views, and reduce
researcher-imposed directive bias. It also captures expressiveness and
individuality, aids in pretesting, and ultimately enhances methodological
utility. We call for novel practices and evaluation frameworks that leverage
rather than constrain the open-ended generative diversity of LLMs, creating
synergies between NLP and social science.

</details>


### [40] [Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data Categorization](https://arxiv.org/abs/2510.13885)
*Ariel Kamen*

Main category: cs.CL

TL;DR: 评估了十个大型语言模型在非结构化文本分类中的性能，发现它们在将文本压缩到有限的分类体系时面临挑战。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估大型语言模型在非结构化文本分类中的性能，并探讨其局限性。

Method: 采用统一的数据集和零样本提示，在十个大型语言模型上进行比较评估，并使用经典指标和LLM特定指标进行评估。

Result: 结果表明，大型语言模型在经典性能方面表现一般，且存在幻觉和膨胀问题。集成方法可以显著提高准确性，减少膨胀，并完全消除幻觉。

Conclusion: 结论是，模型规模的扩大和架构的改进并不能确保更好的分类准确性，而模型的协同编排可能代表着实现或超过人类专家水平的最有效途径。

Abstract: This study presents a comparative evaluation of ten state-of-the-art large
language models (LLMs) applied to unstructured text categorization using the
Interactive Advertising Bureau (IAB) 2.2 hierarchical taxonomy. The analysis
employed a uniform dataset of 8,660 human-annotated samples and identical
zero-shot prompts to ensure methodological consistency across all models.
Evaluation metrics included four classic measures - accuracy, precision,
recall, and F1-score - and three LLM-specific indicators: hallucination ratio,
inflation ratio, and categorization cost.
  Results show that, despite their rapid advancement, contemporary LLMs achieve
only moderate classic performance, with average scores of 34% accuracy, 42%
precision, 45% recall, and 41% F1-score. Hallucination and inflation ratios
reveal that models frequently overproduce categories relative to human
annotators. Among the evaluated systems, Gemini 1.5/2.0 Flash and GPT 20B/120B
offered the most favorable cost-to-performance balance, while GPT 120B
demonstrated the lowest hallucination ratio. The findings suggest that scaling
and architectural improvements alone do not ensure better categorization
accuracy, as the task requires compressing rich unstructured text into a
limited taxonomy - a process that challenges current model architectures.
  To address these limitations, a separate ensemble-based approach was
developed and tested. The ensemble method, in which multiple LLMs act as
independent experts, substantially improved accuracy, reduced inflation, and
completely eliminated hallucinations. These results indicate that coordinated
orchestration of models - rather than sheer scale - may represent the most
effective path toward achieving or surpassing human-expert performance in
large-scale text categorization.

</details>


### [41] [Reliable Fine-Grained Evaluation of Natural Language Math Proofs](https://arxiv.org/abs/2510.13888)
*Wenjie Ma,Andrei Cojocaru,Neel Kolhe,Bradley Louie,Robin Said Sharif,Haihan Zhang,Vincent Zhuang,Matei Zaharia,Sewon Min*

Main category: cs.CL

TL;DR: 这篇论文提出了ProofBench，一个专家注释的细粒度数学证明评分数据集，并开发了ProofGrader，一个用于评估LLM生成数学证明的评估器。ProofGrader在专家评分方面表现出色，并在下游证明生成任务中显示出潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的数学推理大语言模型主要集中在容易验证最终答案的任务上，而生成和验证自然语言数学证明仍然是一个挑战。缺乏可靠的、细粒度的LLM生成数学证明评估器是主要瓶颈。

Method: 该研究系统地开发和验证评估器，对模型生成的数学证明进行0-7分制的细粒度评分。引入ProofBench数据集，并探索了评估器设计空间的关键方面，包括backbone模型、输入上下文、指令和评估工作流程。最终，设计了一个结合强大推理backbone LM、参考解决方案和评分方案的ProofGrader。

Result: ProofGrader在专家评分方面取得了0.926的低平均绝对误差(MAE)，显著优于简单的基线。在best-of-n选择任务中，当n=16时，ProofGrader的平均得分为4.14（满分7分），缩小了二元评估器（2.48）和人类专家（4.62）之间差距的78%。

Conclusion: ProofGrader具有推进下游证明生成的潜力。

Abstract: Recent advances in large language models (LLMs) for mathematical reasoning
have largely focused on tasks with easily verifiable final answers; however,
generating and verifying natural language math proofs remains an open
challenge. We identify the absence of a reliable, fine-grained evaluator for
LLM-generated math proofs as a critical gap. To address this, we propose a
systematic methodology for developing and validating evaluators that assign
fine-grained scores on a 0-7 scale to model-generated math proofs. To enable
this study, we introduce ProofBench, the first expert-annotated dataset of
fine-grained proof ratings, spanning 145 problems from six major math
competitions (USAMO, IMO, Putnam, etc) and 435 LLM-generated solutions from
Gemini-2.5-pro, o3, and DeepSeek-R1. %with expert gradings. Using ProofBench as
a testbed, we systematically explore the evaluator design space across key
axes: the backbone model, input context, instructions and evaluation workflow.
Our analysis delivers ProofGrader, an evaluator that combines a strong
reasoning backbone LM, rich context from reference solutions and marking
schemes, and a simple ensembling method; it achieves a low Mean Absolute Error
(MAE) of 0.926 against expert scores, significantly outperforming naive
baselines. Finally, we demonstrate its practical utility in a best-of-$n$
selection task: at $n=16$, ProofGrader achieves an average score of 4.14 (out
of 7), closing 78% of the gap between a naive binary evaluator (2.48) and the
human oracle (4.62), highlighting its potential to advance downstream proof
generation.

</details>


### [42] [A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness](https://arxiv.org/abs/2510.13890)
*Fali Wang,Jihai Chen,Shuhua Yang,Ali Al-Lawati,Linli Tang,Hui Liu,Suhang Wang*

Main category: cs.CL

TL;DR: 本文综述了小型语言模型 (SLM) 和大型语言模型 (LLM) 协作的最新进展，重点关注如何结合 SLM 的效率和 LLM 的通用性来满足各种任务和部署场景中的目标。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然强大，但也存在微调成本高、推理延迟、边缘部署受限和可靠性问题。小型语言模型 (SLM) 具有紧凑、高效和适应性强的特点，可以作为补充。

Method: 本文对 SLM-LLM 协作进行了系统性综述，并根据协作目标进行了组织。提出了一个包含四个目标的分类法：性能增强、成本效益、云边隐私和可信度。

Result: 本文回顾了代表性的方法，总结了设计范式，并概述了在高效、安全和可扩展的 SLM-LLM 协作方面面临的开放挑战和未来方向。

Conclusion: 小型语言模型和大型语言模型的协同合作，有望在性能、成本、隐私和信任度方面实现更优的解决方案。

Abstract: Large language models (LLMs) have advanced many domains and applications but
face high fine-tuning costs, inference latency, limited edge deployability, and
reliability concerns. Small language models (SLMs), compact, efficient, and
adaptable, offer complementary remedies. Recent work explores collaborative
frameworks that fuse SLMs' specialization and efficiency with LLMs'
generalization and reasoning to meet diverse objectives across tasks and
deployment scenarios. Motivated by these developments, this paper presents a
systematic survey of SLM-LLM collaboration organized by collaboration
objectives. We propose a taxonomy with four goals: performance enhancement,
cost-effectiveness, cloud-edge privacy, and trustworthiness. Within this
framework, we review representative methods, summarize design paradigms, and
outline open challenges and future directions toward efficient, secure, and
scalable SLM-LLM collaboration.

</details>


### [43] [The Harder The Better: Maintaining Supervised Fine-tuning Generalization with Less but Harder Data](https://arxiv.org/abs/2510.13892)
*Zhaoyang Shang,Sibo Wei,Jianbin Guo,Rui Zhou,Lifeng Dong,Yin Luo*

Main category: cs.CL

TL;DR: 这篇论文提出了一种名为THTB的框架，用于指令数据选择和标注指导，以提高大型语言模型在特定领域的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在识别高质量数据子集方面过度依赖LLM的内部知识，缺乏可解释性和泛化能力。

Method: THTB框架结合质量过滤与内在和外在难度评分，优先考虑更高层次的认知指令。

Result: 实验表明，使用THTB框架训练的模型仅用5%的数据就能胜过完整数据集训练的模型，并且具有更好的泛化能力。在垂直领域，仅用2%的数据训练的模型就能超过用更大数据集训练的模型。

Conclusion: THTB框架为高效的SFT提供了可解释和可量化的标准，并在数据选择和标注指导方面具有强大的领域适应潜力。

Abstract: Large Language Models (LLMs) excel in general tasks, but adapting them to
specialized domains relies on high-quality supervised fine-tuning (SFT) data.
Although existing methods can identify subsets of high-quality data and reduce
training cost to some extent, their selection process still suffers from
over-reliance on LLMs' internal knowledge, weak interpretability, and limited
generalization. To address these limitations, we propose THTB (The Harder The
Better), a cognitive science-inspired framework for instruction data selection
and annotation guidance. THTB prioritizes higher-level cognitive instructions
by combining quality filtering with intrinsic and extrinsic hardness scoring,
offering interpretable and quantifiable criteria for efficient SFT, both in
data selection and annotation guidance. Experiments show that THTB enables
models trained on only 5% of the data to outperform full-dataset training,
while achieving superior generalization compared with LLM-only selection. In
addition, THTB provides effective annotation guidance in vertical domains,
enabling a model trained on just 2% of the data to surpass models trained on
much larger datasets, demonstrating strong potential for domain adaptation. Our
code, datasets, and models are available on
https://github.com/DYJG-research/THTB.

</details>


### [44] [Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection](https://arxiv.org/abs/2510.13893)
*Olga E. Sorokoletova,Francesco Giarrusso,Vincenzo Suriani,Daniele Nardi*

Main category: cs.CL

TL;DR: 本文构建了一个包含50种越狱策略的层级分类体系，并进行了红队挑战，以评估不同攻击类型的有效性。同时，创建了一个新的意大利语数据集，用于研究对抗性意图的演变。


<details>
  <summary>Details</summary>
Motivation: 现有的防御措施通常只关注单轮攻击，缺乏跨语言覆盖，并且依赖于有限的分类体系，这些分类体系要么未能捕捉到攻击策略的完整多样性，要么强调风险类别而非越狱技术。

Method: 构建了一个包含50种越狱策略的层级分类体系，并通过红队挑战分析不同攻击类型的流行度和成功率。此外，还创建了一个新的意大利语数据集，其中包含1364个多轮对抗性对话，并使用该分类体系进行注释。

Result: 分析了不同攻击类型的流行度和成功率，并评估了分类体系引导的提示对改进自动检测的好处。结果表明，特定的越狱策略利用了模型漏洞并导致了不一致。

Conclusion: 通过构建全面的越狱策略分类体系和新的意大利语数据集，加深了对越狱技术有效性的理解，并为改进自动检测提供了新的方法。

Abstract: Jailbreaking techniques pose a significant threat to the safety of Large
Language Models (LLMs). Existing defenses typically focus on single-turn
attacks, lack coverage across languages, and rely on limited taxonomies that
either fail to capture the full diversity of attack strategies or emphasize
risk categories rather than the jailbreaking techniques. To advance the
understanding of the effectiveness of jailbreaking techniques, we conducted a
structured red-teaming challenge. The outcome of our experiments are manifold.
First, we developed a comprehensive hierarchical taxonomy of 50 jailbreak
strategies, consolidating and extending prior classifications into seven broad
families, including impersonation, persuasion, privilege escalation, cognitive
overload, obfuscation, goal conflict, and data poisoning. Second, we analyzed
the data collected from the challenge to examine the prevalence and success
rates of different attack types, providing insights into how specific jailbreak
strategies exploit model vulnerabilities and induce misalignment. Third, we
benchmark a popular LLM for jailbreak detection, evaluating the benefits of
taxonomy-guided prompting for improving automatic detection. Finally, we
compiled a new Italian dataset of 1364 multi-turn adversarial dialogues,
annotated with our taxonomy, enabling the study of interactions where
adversarial intent emerges gradually and succeeds in bypassing traditional
safeguards.

</details>


### [45] [Attribution Quality in AI-Generated Content:Benchmarking Style Embeddings and LLM Judges](https://arxiv.org/abs/2510.13898)
*Misam Abbas*

Main category: cs.CL

TL;DR: 论文探讨了在大语言模型时代进行作者身份识别的挑战，并评估了两种方法（固定风格嵌入和指令调优的LLM裁判GPT-4o）在区分人类和机器生成文本方面的表现。


<details>
  <summary>Details</summary>
Motivation: 随着机器生成的文本越来越接近人类写作水平，确定作者身份变得越来越困难。

Method: 论文使用了Human AI Parallel Corpus数据集，该数据集包含六个领域的600个实例，并比较了固定风格嵌入和指令调优的LLM裁判GPT-4o这两种方法的准确性。

Result: 固定风格嵌入在GPT生成的文本上表现更好（82% vs. 68%），而LLM裁判在LLaMA生成的文本上略好（85% vs. 81%），但在小说和学术论文中，LLM裁判的表现明显优于风格嵌入，表明其具有语义敏感性；而风格嵌入在口语和剧本对话中表现更好，反映了其结构优势。

Conclusion: 作者身份识别是一个多维度的问题，需要混合策略。论文提供代码和数据，为评估AI生成内容的归属提供了一个可复现的基准。

Abstract: Attributing authorship in the era of large language models (LLMs) is
increasingly challenging as machine-generated prose rivals human writing. We
benchmark two complementary attribution mechanisms , fixed Style Embeddings and
an instruction-tuned LLM judge (GPT-4o) on the Human AI Parallel Corpus, an
open dataset of 600 balanced instances spanning six domains (academic, news,
fiction, blogs, spoken transcripts, and TV/movie scripts). Each instance
contains a human prompt with both a gold continuation and an LLM-generated
continuation from either GPT-4o or LLaMA-70B-Instruct. The Style Embedding
baseline achieves stronger aggregate accuracy on GPT continuations (82 pct vs.
68 pct). The LLM Judge is slightly better than the Style embeddings on LLaMA
continuations (85 pct vs. 81 pct) but the results are not statistically
significant. Crucially, the LLM judge significantly outperforms in fiction and
academic prose, indicating semantic sensitivity, whereas embeddings dominate in
spoken and scripted dialogue, reflecting structural strengths. These
complementary patterns highlight attribution as a multidimensional problem
requiring hybrid strategies. To support reproducibility we provide code on
GitHub and derived data on Hugging Face under the MIT license. This open
framework provides a reproducible benchmark for attribution quality assessment
in AI-generated content, along with a review of related literature influencing
this work.

</details>


### [46] [Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences](https://arxiv.org/abs/2510.13900)
*Julian Minder,Clément Dumas,Stewart Slocum,Helena Casademunt,Cameron Holmes,Robert West,Neel Nanda*

Main category: cs.CL

TL;DR: 狭窄领域的微调已成为将大型语言模型 (LLM) 适应特定任务并创建具有已知不寻常属性（对研究有用）的模型的必要工具。我们表明，狭窄的微调会在 LLM 激活中产生强大的偏差，可以对其进行解释以了解微调领域。


<details>
  <summary>Details</summary>
Motivation: 研究表明，狭窄的微调会在 LLM 激活中产生偏差，可以对其进行解释以了解微调领域。通过分析随机文本的前几个 token 的激活差异并添加此差异来操纵模型激活，可以生成类似于微调数据的格式和一般内容，表明这些分析包含关键信息。

Method: 通过创建基于 LLM 的可解释性代理来理解微调领域。该代理可以访问偏差，与使用简单提示的基线代理相比，性能显着提高。分析跨不同架构（Gemma、LLaMA、Qwen）和规模（1B 到 32B 参数）的虚假事实、新兴的不对齐、潜意识学习和禁忌词猜测游戏模型的合成文档微调。

Result: 狭窄微调的模型在其激活中具有其训练目标的显着痕迹，将预训练数据混合到微调语料库中可以largely消除偏差，但可能仍然存在残留风险。

Conclusion: 这项工作表明，狭窄微调的模型在其激活中具有其训练目标的显着痕迹，并提出了改进训练方式的方法，警告 AI 安全和可解释性研究人员，使用此类模型作为研究更广泛的微调（例如，聊天调整）的代理可能不切实际，并强调需要更深入地研究狭窄微调的影响，并为模型差异、安全性和可解释性研究开发真正Realistic的案例研究。

Abstract: Finetuning on narrow domains has become an essential tool to adapt Large
Language Models (LLMs) to specific tasks and to create models with known
unusual properties that are useful for research. We show that narrow finetuning
creates strong biases in LLM activations that can be interpreted to understand
the finetuning domain. These biases can be discovered using simple tools from
model diffing - the study of differences between models before and after
finetuning. In particular, analyzing activation differences on the first few
tokens of random text and steering by adding this difference to the model
activations produces text similar to the format and general content of the
finetuning data. We demonstrate that these analyses contain crucial information
by creating an LLM-based interpretability agent to understand the finetuning
domain. With access to the bias, the agent performs significantly better
compared to baseline agents using simple prompting. Our analysis spans
synthetic document finetuning for false facts, emergent misalignment,
subliminal learning, and taboo word guessing game models across different
architectures (Gemma, LLaMA, Qwen) and scales (1B to 32B parameters). We
suspect these biases reflect overfitting and find that mixing pretraining data
into the finetuning corpus largely removes them, though residual risks may
remain. Our work (1) demonstrates that narrowly finetuned models have salient
traces of their training objective in their activations and suggests ways to
improve how they are trained, (2) warns AI safety and interpretability
researchers that the common practice of using such models as a proxy for
studying broader finetuning (e.g., chat-tuning) might not be realistic, and (3)
highlights the need for deeper investigation into the effects of narrow
finetuning and development of truly realistic case studies for model-diffing,
safety and interpretability research.

</details>


### [47] [RAID: Refusal-Aware and Integrated Decoding for Jailbreaking LLMs](https://arxiv.org/abs/2510.13901)
*Tuan T. Nguyen,John Le,Thai T. Vu,Willy Susilo,Heath Cooper*

Main category: cs.CL

TL;DR: 这篇论文提出了一种名为RAID的新框架，用于攻击大型语言模型（LLM）的安全机制，通过生成对抗性后缀来诱导模型产生受限内容。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在各种任务中表现出色，但容易受到绕过安全机制的jailbreak攻击。

Method: RAID框架将离散的tokens放宽到连续嵌入，并使用联合目标对其进行优化，该目标鼓励受限响应，包含拒绝感知正则化器以避免嵌入空间中的拒绝方向，并应用连贯性项以保持语义合理性和非冗余性。然后，通过平衡嵌入亲和力与语言模型可能性，评论家引导的解码程序将嵌入映射回tokens。

Result: 在多个开源LLM上的实验表明，与最近的白盒和黑盒基线相比，RAID以更少的查询和更低的计算成本实现了更高的攻击成功率。

Conclusion: 研究结果强调了嵌入空间正则化对于理解和缓解LLM jailbreak漏洞的重要性。

Abstract: Large language models (LLMs) achieve impressive performance across diverse
tasks yet remain vulnerable to jailbreak attacks that bypass safety mechanisms.
We present RAID (Refusal-Aware and Integrated Decoding), a framework that
systematically probes these weaknesses by crafting adversarial suffixes that
induce restricted content while preserving fluency. RAID relaxes discrete
tokens into continuous embeddings and optimizes them with a joint objective
that (i) encourages restricted responses, (ii) incorporates a refusal-aware
regularizer to steer activations away from refusal directions in embedding
space, and (iii) applies a coherence term to maintain semantic plausibility and
non-redundancy. After optimization, a critic-guided decoding procedure maps
embeddings back to tokens by balancing embedding affinity with language-model
likelihood. This integration yields suffixes that are both effective in
bypassing defenses and natural in form. Experiments on multiple open-source
LLMs show that RAID achieves higher attack success rates with fewer queries and
lower computational cost than recent white-box and black-box baselines. These
findings highlight the importance of embedding-space regularization for
understanding and mitigating LLM jailbreak vulnerabilities.

</details>


### [48] [Investigating Political and Demographic Associations in Large Language Models Through Moral Foundations Theory](https://arxiv.org/abs/2510.13902)
*Nicole Smith-Vaniz,Harper Lyon,Lorraine Steigner,Ben Armstrong,Nicholas Mattei*

Main category: cs.CL

TL;DR: 本研究调查了大型语言模型（LLM）在政治和道德领域的潜在偏见，通过道德基础理论（MFT）分析了LLM的回应，并将其与人类数据进行了比较。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在日常生活中扮演着重要的建议提供者角色，引发了对其在政治和道德领域可能存在的偏见的担忧。

Method: 该研究运用道德基础理论（MFT）来评估LLM的回应，并将其与现有的人类研究进行对比，从而判断LLM是否表现出意识形态倾向。

Result: 该研究深入了解了AI生成的回应中政治和人口依赖性的程度。

Conclusion: 通过系统地分析LLM在不同条件和实验下的行为，该研究提供了关于AI生成的回应中政治和人口依赖性的程度的见解。

Abstract: Large Language Models (LLMs) have become increasingly incorporated into
everyday life for many internet users, taking on significant roles as advice
givers in the domains of medicine, personal relationships, and even legal
matters. The importance of these roles raise questions about how and what
responses LLMs make in difficult political and moral domains, especially
questions about possible biases. To quantify the nature of potential biases in
LLMs, various works have applied Moral Foundations Theory (MFT), a framework
that categorizes human moral reasoning into five dimensions: Harm, Fairness,
Ingroup Loyalty, Authority, and Purity. Previous research has used the MFT to
measure differences in human participants along political, national, and
cultural lines. While there has been some analysis of the responses of LLM with
respect to political stance in role-playing scenarios, no work so far has
directly assessed the moral leanings in the LLM responses, nor have they
connected LLM outputs with robust human data. In this paper we analyze the
distinctions between LLM MFT responses and existing human research directly,
investigating whether commonly available LLM responses demonstrate ideological
leanings: either through their inherent responses, straightforward
representations of political ideologies, or when responding from the
perspectives of constructed human personas. We assess whether LLMs inherently
generate responses that align more closely with one political ideology over
another, and additionally examine how accurately LLMs can represent ideological
perspectives through both explicit prompting and demographic-based
role-playing. By systematically analyzing LLM behavior across these conditions
and experiments, our study provides insight into the extent of political and
demographic dependency in AI-generated responses.

</details>


### [49] [Schema for In-Context Learning](https://arxiv.org/abs/2510.13905)
*Pan Chen,Shaohong Chen,Mark Wang,Shi Xuan Leong,Priscilla Fung,Varinia Bernales,Alan Aspuru-Guzik*

Main category: cs.CL

TL;DR: 提出了一种新的上下文学习框架，名为SCHEMA ACTIVATED IN CONTEXT LEARNING (SA-ICL)，该框架受到认知科学中图式理论的启发，通过提取先验示例中的认知构建块表示，创建一个抽象的图式，用于增强模型在面对新问题时的推理过程。


<details>
  <summary>Details</summary>
Motivation: 传统的上下文学习缺乏显式的知识检索和迁移模块。

Method: SA-ICL框架提取认知构建块的表示，创建抽象图式，并用它来增强模型的推理过程。

Result: 实验表明，大型语言模型缺乏隐式形成和利用基于图式的学习表示的能力，但可以从显式的基于图式的支架中获益。在GPQA数据集的化学和物理问题上，SA-ICL持续提高性能，最高可达36.19%。

Conclusion: SA-ICL不仅连接了从模式启动到思维链提示的不同ICL策略，而且为增强LLM中类似人类的推理能力开辟了一条新途径。

Abstract: In-Context Learning (ICL) enables transformer-based language models to adapt
to new tasks by conditioning on demonstration examples. However, traditional
example-driven in-context learning lacks explicit modules for knowledge
retrieval and transfer at the abstraction level. Inspired by cognitive science,
specifically schema theory, which holds that humans interpret new information
by activating pre-existing mental frameworks (schemas) to structure
understanding, we introduce SCHEMA ACTIVATED IN CONTEXT LEARNING (SA-ICL). This
framework extracts the representation of the building blocks of cognition for
the reasoning process instilled from prior examples, creating an abstracted
schema, a lightweight, structured template of key inferential steps and their
relationships, which is then used to augment a model's reasoning process when
presented with a novel question. We demonstrate that a broad range of large
language models (LLMs) lack the capacity to form and utilize internal
schema-based learning representations implicitly, but instead benefit
significantly from explicit schema-based scaffolding. Across chemistry and
physics questions from the GPQA dataset, our experiments show that SA-ICL
consistently boosts performance, up to 36.19 percent, when the single
demonstration example is of high quality, which simultaneously reduces reliance
on the number of demonstrations and enhances interpretability. SCHEMA ACTIVATED
IN CONTEXT LEARNING not only bridges disparate ICL strategies ranging from
pattern priming to Chain-of-Thought prompting, but also paves a new path for
enhancing human-like reasoning in LLMs.

</details>


### [50] [LLM Prompt Duel Optimizer: Efficient Label-Free Prompt Optimization](https://arxiv.org/abs/2510.13907)
*Yuanchen Wu,Saurabh Verma,Justin Lee,Fangzhou Xiong,Poppy Zhang,Amel Awadelkarim,Xu Chen,Yubai Yuan,Shawndra Hill*

Main category: cs.CL

TL;DR: 提出了一种无需标签的提示优化框架（PDO），通过LLM判断提示对的偏好进行优化。


<details>
  <summary>Details</summary>
Motivation: 现有的自动提示优化方法依赖于带标签的验证数据，但收集高质量标签成本高昂。

Method: 将提示优化问题建模为决斗bandit问题，利用Double Thompson Sampling (D-TS)选择信息量大的提示对进行比较，并通过Top-Performer Guided Mutation扩展候选提示。

Result: 在BIG-bench Hard (BBH) 和 MS MARCO上的实验表明，PDO优于基线方法。

Conclusion: D-TS和提示变异都有效。

Abstract: Large language models (LLMs) are highly sensitive to their input prompts,
making prompt design a central challenge. While automatic prompt optimization
(APO) reduces manual engineering, most approaches assume access to ground-truth
references such as labeled validation data. In practice, however, collecting
high-quality labels is costly and slow. We propose the Prompt Duel Optimizer
(PDO), a sample-efficient framework for label-free prompt optimization. PDO
formulates the problem as a dueling-bandit setting, where supervision signal
comes from pairwise preference feedback provided by an LLM judge. The framework
combines Double Thompson Sampling (D-TS), which prioritizes informative prompt
comparisons, with Top-Performer Guided Mutation, which expands the candidate
pool by mutating high-performing prompts. PDO naturally operates in label-free
settings and can also incorporate partial labels to mitigate judge noise.
Experiments on BIG-bench Hard (BBH) and MS MARCO show that PDO consistently
outperforms baseline methods. Ablation studies further demonstrate the
effectiveness of both D-TS and prompt mutation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [51] [MultiFoodhat: A potential new paradigm for intelligent food quality inspection](https://arxiv.org/abs/2510.13889)
*Yue Hu,Guohang Zhuang*

Main category: cs.CV

TL;DR: 提出了一种新的零样本食物识别框架，MultiFoodChat，通过视觉-语言模型和大型语言模型进行多轮对话协同推理，无需额外训练或手动标注。


<details>
  <summary>Details</summary>
Motivation: 现有监督模型依赖大量标注数据，泛化能力有限。

Method: 引入对话驱动的多代理推理框架MultiFoodChat，利用Object Perception Token (OPT) 捕获细粒度视觉属性，Interactive Reasoning Agent (IRA) 动态解释上下文线索以优化预测。

Result: 在多个公共食物数据集上的实验表明，MultiFoodChat 相比现有无监督和少样本方法，实现了更高的识别准确率和可解释性。

Conclusion: MultiFoodChat 有望成为智能食品质量检测和分析的新范例。

Abstract: Food image classification plays a vital role in intelligent food quality
inspection, dietary assessment, and automated monitoring. However, most
existing supervised models rely heavily on large labeled datasets and exhibit
limited generalization to unseen food categories. To overcome these challenges,
this study introduces MultiFoodChat, a dialogue-driven multi-agent reasoning
framework for zero-shot food recognition. The framework integrates
vision-language models (VLMs) and large language models (LLMs) to enable
collaborative reasoning through multi-round visual-textual dialogues. An Object
Perception Token (OPT) captures fine-grained visual attributes, while an
Interactive Reasoning Agent (IRA) dynamically interprets contextual cues to
refine predictions. This multi-agent design allows flexible and human-like
understanding of complex food scenes without additional training or manual
annotations. Experiments on multiple public food datasets demonstrate that
MultiFoodChat achieves superior recognition accuracy and interpretability
compared with existing unsupervised and few-shot methods, highlighting its
potential as a new paradigm for intelligent food quality inspection and
analysis.

</details>


### [52] [Post-surgical Endometriosis Segmentation in Laparoscopic Videos](https://arxiv.org/abs/2510.13899)
*Andreas Leibetseder,Klaus Schoeffmann,Jörg Keckstein,Simon Keckstein*

Main category: cs.CV

TL;DR: 该论文介绍了一个用于分割子宫内膜异位症（尤其是深色子宫内膜植入物）的系统，旨在辅助妇科医生。


<details>
  <summary>Details</summary>
Motivation: 子宫内膜异位症的视觉表现多样，难以识别，尤其对于非专业人士。

Method: 该系统通过训练来分割深色子宫内膜植入物，分析腹腔镜手术视频。

Result: 该系统可以对识别出的植入区域进行多色叠加注释，并显示检测摘要，以改进视频浏览。

Conclusion: 该系统旨在辅助妇科医生治疗子宫内膜异位症。

Abstract: Endometriosis is a common women's condition exhibiting a manifold visual
appearance in various body-internal locations. Having such properties makes its
identification very difficult and error-prone, at least for laymen and
non-specialized medical practitioners. In an attempt to provide assistance to
gynecologic physicians treating endometriosis, this demo paper describes a
system that is trained to segment one frequently occurring visual appearance of
endometriosis, namely dark endometrial implants. The system is capable of
analyzing laparoscopic surgery videos, annotating identified implant regions
with multi-colored overlays and displaying a detection summary for improved
video browsing.

</details>


### [53] [Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models](https://arxiv.org/abs/2510.13993)
*Jia Yun Chua,Argyrios Zolotas,Miguel Arana-Catania*

Main category: cs.CV

TL;DR: 本文研究了将视觉模型和视觉语言模型结合以增强遥感图像分析，重点是飞机检测和场景理解。


<details>
  <summary>Details</summary>
Motivation: 传统视觉模型需要大量特定领域的标记数据，并且理解复杂环境中上下文的能力有限。视觉语言模型通过集成视觉和文本数据提供了一种互补的方法，但它们在遥感中的应用仍有待探索，特别是考虑到它们的通用性。

Method: 将YOLO与LLaVA、ChatGPT和Gemini等视觉语言模型集成，以实现更准确和上下文感知的图像解释。在标记和未标记的遥感数据以及退化的图像场景中评估性能。

Result: 在飞机检测和计数精度方面，各种模型的平均MAE提高了48.46%，特别是在原始和退化场景等具有挑战性的条件下。对于全面理解遥感图像，CLIPScore提高了6.17%。

Conclusion: 所提出的方法结合了传统视觉模型和视觉语言模型，为更先进和高效的遥感图像分析铺平了道路，尤其是在少样本学习场景中。

Abstract: Remote sensing has become a vital tool across sectors such as urban planning,
environmental monitoring, and disaster response. While the volume of data
generated has increased significantly, traditional vision models are often
constrained by the requirement for extensive domain-specific labelled data and
their limited ability to understand the context within complex environments.
Vision Language Models offer a complementary approach by integrating visual and
textual data; however, their application to remote sensing remains
underexplored, particularly given their generalist nature. This work
investigates the combination of vision models and VLMs to enhance image
analysis in remote sensing, with a focus on aircraft detection and scene
understanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and
Gemini aims to achieve more accurate and contextually aware image
interpretation. Performance is evaluated on both labelled and unlabelled remote
sensing data, as well as degraded image scenarios which are crucial for remote
sensing. The findings show an average MAE improvement of 48.46% across models
in the accuracy of aircraft detection and counting, especially in challenging
conditions, in both raw and degraded scenarios. A 6.17% improvement in
CLIPScore for comprehensive understanding of remote sensing images is obtained.
The proposed approach combining traditional vision models and VLMs paves the
way for more advanced and efficient remote sensing image analysis, especially
in few-shot learning scenarios.

</details>


### [54] [Finding Holes: Pathologist Level Performance Using AI for Cribriform Morphology Detection in Prostate Cancer](https://arxiv.org/abs/2510.13995)
*Kelvin Szolnoky,Anders Blilie,Nita Mulliqi,Toyonori Tsuzuki,Hemamali Samaratunga,Matteo Titus,Xiaoyi Ji,Sol Erika Boman,Einar Gudlaugsson,Svein Reidar Kjosavik,José Asenjo,Marcello Gambacorta,Paolo Libretti,Marcin Braun,Radisław Kordek,Roman Łowicki,Brett Delahunt,Kenneth A. Iczkowski,Theo van der Kwast,Geert J. L. H. van Leenders,Katia R. M. Leite,Chin-Chen Pan,Emiel Adrianus Maria Janssen,Martin Eklund,Lars Egevad,Kimmo Kartasalo*

Main category: cs.CV

TL;DR: 开发并验证了一种基于AI的系统，以提高前列腺癌筛状形态的检测。


<details>
  <summary>Details</summary>
Motivation: 筛状形态是前列腺癌中一种预后不良的组织学特征，但病理学家之间的观察者间差异很大。

Method: 使用EfficientNetV2-S编码器和多重实例学习创建深度学习模型，用于端到端全玻片分类。在来自三个队列的430名患者的640张数字化前列腺核心针活检切片上训练模型。

Result: 该模型显示出强大的内部验证性能（AUC：0.97，95％CI：0.95-0.99；Cohen's kappa：0.81，95％CI：0.72-0.89）和强大的外部验证（AUC：0.90，95％CI：0.86-0.93；Cohen's kappa：0.55，95％CI：0.45-0.64）。

Conclusion: 我们的AI模型在前列腺癌筛状形态检测方面表现出病理学家级别的性能。这种方法可以提高诊断可靠性，标准化报告并改善前列腺癌患者的治疗决策。

Abstract: Background: Cribriform morphology in prostate cancer is a histological
feature that indicates poor prognosis and contraindicates active surveillance.
However, it remains underreported and subject to significant interobserver
variability amongst pathologists. We aimed to develop and validate an AI-based
system to improve cribriform pattern detection.
  Methods: We created a deep learning model using an EfficientNetV2-S encoder
with multiple instance learning for end-to-end whole-slide classification. The
model was trained on 640 digitised prostate core needle biopsies from 430
patients, collected across three cohorts. It was validated internally (261
slides from 171 patients) and externally (266 slides, 104 patients from three
independent cohorts). Internal validation cohorts included laboratories or
scanners from the development set, while external cohorts used completely
independent instruments and laboratories. Annotations were provided by three
expert uropathologists with known high concordance. Additionally, we conducted
an inter-rater analysis and compared the model's performance against nine
expert uropathologists on 88 slides from the internal validation cohort.
  Results: The model showed strong internal validation performance (AUC: 0.97,
95% CI: 0.95-0.99; Cohen's kappa: 0.81, 95% CI: 0.72-0.89) and robust external
validation (AUC: 0.90, 95% CI: 0.86-0.93; Cohen's kappa: 0.55, 95% CI:
0.45-0.64). In our inter-rater analysis, the model achieved the highest average
agreement (Cohen's kappa: 0.66, 95% CI: 0.57-0.74), outperforming all nine
pathologists whose Cohen's kappas ranged from 0.35 to 0.62.
  Conclusion: Our AI model demonstrates pathologist-level performance for
cribriform morphology detection in prostate cancer. This approach could enhance
diagnostic reliability, standardise reporting, and improve treatment decisions
for prostate cancer patients.

</details>


### [55] [NAPPure: Adversarial Purification for Robust Image Classification under Non-Additive Perturbations](https://arxiv.org/abs/2510.14025)
*Junjie Nan,Jianing Li,Wei Chen,Mingkun Zhang,Xueqi Cheng*

Main category: cs.CV

TL;DR: 提出了一种新的对抗净化框架NAPPure，可以处理非加性扰动。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗净化方法在处理blur, occlusion, distortion等非加性扰动时效果不佳，因为它们是为加性扰动设计的。

Method: 首先建立对抗图像的生成过程，然后通过最大化似然来解开潜在的干净图像和扰动参数。

Result: 在GTSRB和CIFAR-10数据集上的实验表明，NAPPure显著提高了图像分类模型对非加性扰动的鲁棒性。

Conclusion: NAPPure可以有效地提高图像分类模型对非加性扰动的鲁棒性。

Abstract: Adversarial purification has achieved great success in combating adversarial
image perturbations, which are usually assumed to be additive. However,
non-additive adversarial perturbations such as blur, occlusion, and distortion
are also common in the real world. Under such perturbations, existing
adversarial purification methods are much less effective since they are
designed to fit the additive nature. In this paper, we propose an extended
adversarial purification framework named NAPPure, which can further handle
non-additive perturbations. Specifically, we first establish the generation
process of an adversarial image, and then disentangle the underlying clean
image and perturbation parameters through likelihood maximization. Experiments
on GTSRB and CIFAR-10 datasets show that NAPPure significantly boosts the
robustness of image classification models against non-additive perturbations.

</details>


### [56] [Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding](https://arxiv.org/abs/2510.14032)
*Xiaoqian Shen,Wenxuan Zhang,Jun Chen,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为Vgent的基于图的检索-推理-增强生成框架，以提高大型视频语言模型（LVLMs）对长视频的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型视频语言模型难以处理长视频，面临上下文窗口限制和长期序列信息保留的挑战。现有检索增强生成方法应用于长视频时，存在时间依赖性中断和包含无关信息的问题。

Method: 该方法通过构建结构化视频图来保留视频片段之间的语义关系，并引入中间推理步骤来减少检索噪声，从而提高检索效果。

Result: 在三个长视频理解基准测试中，Vgent框架相比基线模型性能提升了3.0%~5.4%，并且优于当前最佳视频RAG方法8.6%。

Conclusion: Vgent框架有效地提升了LVLMs对长视频的理解能力。

Abstract: Understanding and reasoning over long videos pose significant challenges for
large video language models (LVLMs) due to the difficulty in processing
intensive video tokens beyond context window and retaining long-term sequential
information. Retrieval-Augmented Generation (RAG) has demonstrated
effectiveness in processing long context for Large Language Models (LLMs);
however, applying RAG to long video faces challenges such as disrupted temporal
dependencies and inclusion of irrelevant information that can hinder accurate
reasoning. To address these limitations, we propose Vgent, a novel graph-based
retrieval-reasoning-augmented generation framework to enhance LVLMs for long
video understanding. Our approach introduces two key innovations: (i) It
represents videos by structured graphs with semantic relationships across video
clips preserved to improve retrieval effectiveness. (ii) It introduces an
intermediate reasoning step to mitigate the reasoning limitation of LVLMs,
which leverages structured verification to reduce retrieval noise and
facilitate the explicit aggregation of relevant information across clips,
resulting in more accurate and context-aware responses. We comprehensively
evaluate our framework with various open-source LVLMs on three long-video
understanding benchmarks. Our approach yielded an overall performance
improvement of $3.0\%\sim 5.4\%$ over base models on MLVU, and outperformed
state-of-the-art video RAG methods by $8.6\%$. Our code is publicly available
at https://xiaoqian-shen.github.io/Vgent.

</details>


### [57] [Synchronization of Multiple Videos](https://arxiv.org/abs/2510.14051)
*Avihai Naaman,Ron Shapira Weber,Oren Freifeld*

Main category: cs.CV

TL;DR: 提出了一种名为时间原型学习（TPL）的框架，用于同步来自不同场景或生成式AI视频的视频。


<details>
  <summary>Details</summary>
Motivation: 由于主题、背景和非线性时间错位不同，同步来自不同场景或生成式AI视频的视频非常具有挑战性。

Method: 构建一个共享的、紧凑的1D表示，该表示来自通过各种预训练模型提取的高维嵌入。通过学习统一的原型序列来对齐视频，该原型序列锚定关键动作阶段，从而避免详尽的成对匹配。

Result: TPL提高了跨各种数据集的同步精度、效率和鲁棒性，包括细粒度的帧检索和阶段分类任务。

Conclusion: TPL是第一个缓解描绘相同动作的多个生成式AI视频中的同步问题的方法。

Abstract: Synchronizing videos captured simultaneously from multiple cameras in the
same scene is often easy and typically requires only simple time shifts.
However, synchronizing videos from different scenes or, more recently,
generative AI videos, poses a far more complex challenge due to diverse
subjects, backgrounds, and nonlinear temporal misalignment. We propose Temporal
Prototype Learning (TPL), a prototype-based framework that constructs a shared,
compact 1D representation from high-dimensional embeddings extracted by any of
various pretrained models. TPL robustly aligns videos by learning a unified
prototype sequence that anchors key action phases, thereby avoiding exhaustive
pairwise matching. Our experiments show that TPL improves synchronization
accuracy, efficiency, and robustness across diverse datasets, including
fine-grained frame retrieval and phase classification tasks. Importantly, TPL
is the first approach to mitigate synchronization issues in multiple generative
AI videos depicting the same action. Our code and a new multiple video
synchronization dataset are available at https://bgu-cs-vil.github.io/TPL/

</details>


### [58] [Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images](https://arxiv.org/abs/2510.14081)
*Emanuel Garbin,Guy Adam,Oded Krams,Zohar Barzelay,Eran Guendelman,Michael Schwarz,Moran Vatelmacher,Yigal Shenkman,Eli Peker,Itai Druker,Uri Patish,Yoav Blum,Max Bluvstein,Junxuan Li,Rawal Khirodkar,Shunsuke Saito*

Main category: cs.CV

TL;DR: 提出了一种新的零样本流水线，用于从一些非结构化的手机图像中创建超逼真的、保留身份的 3D 头像。


<details>
  <summary>Details</summary>
Motivation: 现有的方法面临着几个挑战：单视图方法存在几何不一致和幻觉，降低了身份保留，而基于合成数据训练的模型无法捕捉到高频细节，如皮肤皱纹和细毛，限制了真实感。

Method: 该方法引入了两个关键贡献：(1) 一个生成规范化模块，将多个非结构化的视图处理成一个标准化的、一致的表示，以及 (2) 一个基于 Transformer 的模型，该模型在一个新的、大规模的高保真高斯溅射头像数据集上进行训练，该数据集来自真实人物的穹顶捕获。

Result: 该“捕获、规范、溅射”流水线可从非结构化照片生成具有引人注目的真实感和强大的身份保留的静态四分之一身头像。

Conclusion: 总结来说，该论文提出了一个零样本流水线，通过生成规范化模块和 Transformer 模型，从非结构化手机图像中创建超逼真且保留身份的 3D 头像，并在真实感和身份保留方面表现出色。

Abstract: We present a novel, zero-shot pipeline for creating hyperrealistic,
identity-preserving 3D avatars from a few unstructured phone images. Existing
methods face several challenges: single-view approaches suffer from geometric
inconsistencies and hallucinations, degrading identity preservation, while
models trained on synthetic data fail to capture high-frequency details like
skin wrinkles and fine hair, limiting realism. Our method introduces two key
contributions: (1) a generative canonicalization module that processes multiple
unstructured views into a standardized, consistent representation, and (2) a
transformer-based model trained on a new, large-scale dataset of high-fidelity
Gaussian splatting avatars derived from dome captures of real people. This
"Capture, Canonicalize, Splat" pipeline produces static quarter-body avatars
with compelling realism and robust identity preservation from unstructured
photos.

</details>


### [59] [cubic: CUDA-accelerated 3D Bioimage Computing](https://arxiv.org/abs/2510.14143)
*Alexandr A. Kalinin,Anne E. Carpenter,Shantanu Singh,Matthew J. O'Meara*

Main category: cs.CV

TL;DR: Cubic是一个开源Python库，通过使用CuPy和RAPIDS cuCIM中的GPU加速替代方案来增强SciPy和scikit-image API，从而解决生物图像分析中的可扩展性、效率和集成问题。


<details>
  <summary>Details</summary>
Motivation: 现代显微镜生成越来越大的2D和3D数据集，现有的计算方法受到其可扩展性、效率以及与现代科学计算工作流程的集成限制。现有的生物图像分析工具通常缺乏应用程序编程接口（API），不支持图形处理单元（GPU）加速，缺乏广泛的3D图像处理能力，并且/或者对于计算密集型工作流程的互操作性较差。

Method: Cubic通过使用CuPy和RAPIDS cuCIM中的GPU加速替代方案来增强广泛使用的SciPy和scikit-image API。Cubic的API与设备无关，并在数据位于设备上时将操作分派给GPU，否则在CPU上执行，从而无缝地加速了广泛的图像处理例程。

Result: 通过对各个操作进行基准测试以及重现现有的反卷积和分割流程来评估Cubic，在保持算法保真度的同时实现了显着的速度提升。

Conclusion: 这些进展为可扩展的、可重复的生物图像分析奠定了坚实的基础，该分析与更广泛的Python科学计算生态系统（包括其他GPU加速方法）集成，从而可以进行交互式探索和自动高通量分析工作流程。

Abstract: Quantitative analysis of multidimensional biological images is useful for
understanding complex cellular phenotypes and accelerating advances in
biomedical research. As modern microscopy generates ever-larger 2D and 3D
datasets, existing computational approaches are increasingly limited by their
scalability, efficiency, and integration with modern scientific computing
workflows. Existing bioimage analysis tools often lack application programmable
interfaces (APIs), do not support graphics processing unit (GPU) acceleration,
lack broad 3D image processing capabilities, and/or have poor interoperability
for compute-heavy workflows. Here, we introduce cubic, an open-source Python
library that addresses these challenges by augmenting widely used SciPy and
scikit-image APIs with GPU-accelerated alternatives from CuPy and RAPIDS cuCIM.
cubic's API is device-agnostic and dispatches operations to GPU when data
reside on the device and otherwise executes on CPU, seamlessly accelerating a
broad range of image processing routines. This approach enables GPU
acceleration of existing bioimage analysis workflows, from preprocessing to
segmentation and feature extraction for 2D and 3D data. We evaluate cubic both
by benchmarking individual operations and by reproducing existing deconvolution
and segmentation pipelines, achieving substantial speedups while maintaining
algorithmic fidelity. These advances establish a robust foundation for
scalable, reproducible bioimage analysis that integrates with the broader
Python scientific computing ecosystem, including other GPU-accelerated methods,
enabling both interactive exploration and automated high-throughput analysis
workflows. cubic is openly available at
https://github$.$com/alxndrkalinin/cubic

</details>


### [60] [Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures](https://arxiv.org/abs/2510.14179)
*Yuancheng Xu,Wenqi Xian,Li Ma,Julien Philip,Ahmet Levent Taşel,Yiwei Zhao,Ryan Burgert,Mingming He,Oliver Hermann,Oliver Pilarski,Rahul Garg,Paul Debevec,Ning Yu*

Main category: cs.CV

TL;DR: 本文介绍了一个框架，通过定制的数据管道，在视频扩散模型中实现多视角角色一致性和3D相机控制。


<details>
  <summary>Details</summary>
Motivation: 该框架旨在提高视频生成在虚拟制作中的集成度。

Method: 该方法通过4D高斯溅射（4DGS）和视频重新照明模型，使用记录的体积捕获性能重新渲染，并通过不同的相机轨迹和光照变化来训练角色一致性组件，并在该数据上微调最先进的开源视频扩散模型。

Result: 实验表明，该框架提高了视频质量，提高了个性化准确性，并增强了相机控制和光照适应性。

Conclusion: 该框架推进了视频生成与虚拟制作的集成。

Abstract: We introduce a framework that enables both multi-view character consistency
and 3D camera control in video diffusion models through a novel customization
data pipeline. We train the character consistency component with recorded
volumetric capture performances re-rendered with diverse camera trajectories
via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video
relighting model. We fine-tune state-of-the-art open-source video diffusion
models on this data to provide strong multi-view identity preservation, precise
camera control, and lighting adaptability. Our framework also supports core
capabilities for virtual production, including multi-subject generation using
two approaches: joint training and noise blending, the latter enabling
efficient composition of independently customized models at inference time; it
also achieves scene and real-life video customization as well as control over
motion and spatial layout during customization. Extensive experiments show
improved video quality, higher personalization accuracy, and enhanced camera
control and lighting adaptability, advancing the integration of video
generation into virtual production. Our project page is available at:
https://eyeline-labs.github.io/Virtually-Being.

</details>


### [61] [Joint Modeling of Big Five and HEXACO for Multimodal Apparent Personality-trait Recognition](https://arxiv.org/abs/2510.14203)
*Ryo Masumura,Shota Orihashi,Mana Ihori,Tomohiro Tanaka,Naoki Makishima,Taiga Yamane,Naotaka Kawata,Satoshi Suzuki,Taichi Katayama*

Main category: cs.CV

TL;DR: 提出了一种联合建模方法，用于从多模态人类行为中自动识别 Big Five 和 HEXACO 人格特质。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多使用 Big Five，但忽略了 HEXACO，后者可以评估与攻击性和报复心相关的诚实-谦逊特质。此外，尚不清楚机器学习建模中 Big Five 和 HEXACO 之间的关系。

Method: 联合优化 Big Five 和 HEXACO 的识别。

Result: 实验表明，该方法能有效识别 Big Five 和 HEXACO。

Conclusion: 通过考虑 Big Five 和 HEXACO 之间的关系，可以提高对多模态人类行为的认知。

Abstract: This paper proposes a joint modeling method of the Big Five, which has long
been studied, and HEXACO, which has recently attracted attention in psychology,
for automatically recognizing apparent personality traits from multimodal human
behavior. Most previous studies have used the Big Five for multimodal apparent
personality-trait recognition. However, no study has focused on apparent HEXACO
which can evaluate an Honesty-Humility trait related to displaced aggression
and vengefulness, social-dominance orientation, etc. In addition, the
relationships between the Big Five and HEXACO when modeled by machine learning
have not been clarified. We expect awareness of multimodal human behavior to
improve by considering these relationships. The key advance of our proposed
method is to optimize jointly recognizing the Big Five and HEXACO. Experiments
using a self-introduction video dataset demonstrate that the proposed method
can effectively recognize the Big Five and HEXACO.

</details>


### [62] [LOTA: Bit-Planes Guided AI-Generated Image Detection](https://arxiv.org/abs/2510.14230)
*Hongsong Wang,Renxi Cheng,Yang Zhang,Chaolei Han,Jie Gui*

Main category: cs.CV

TL;DR: 该论文提出了一种新的AI生成图像检测方法，通过分析图像的低位平面噪声，实现了高精度和高效率的AI生成图像识别。


<details>
  <summary>Details</summary>
Motivation: 现有方法计算成本高，且无法捕捉原始图像中固有的噪声特征，导致AI生成图像难以区分。

Method: 该方法通过位平面图像处理提取噪声，设计了基于位平面的噪声图像生成方法，并利用图像归一化策略。通过最大梯度块选择增强噪声信号，并设计了轻量级的分类器。

Result: 该方法在GenImage基准测试中达到了98.9%的平均准确率，比现有方法提高了11.9%，且具有良好的跨生成器泛化能力。速度比现有方法快近百倍。

Conclusion: 该方法能够有效地区分AI生成的图像，具有高精度、高效率和良好的泛化能力。

Abstract: The rapid advancement of GAN and Diffusion models makes it more difficult to
distinguish AI-generated images from real ones. Recent studies often use
image-based reconstruction errors as an important feature for determining
whether an image is AI-generated. However, these approaches typically incur
high computational costs and also fail to capture intrinsic noisy features
present in the raw images. To solve these problems, we innovatively refine
error extraction by using bit-plane-based image processing, as lower bit planes
indeed represent noise patterns in images. We introduce an effective bit-planes
guided noisy image generation and exploit various image normalization
strategies, including scaling and thresholding. Then, to amplify the noise
signal for easier AI-generated image detection, we design a maximum gradient
patch selection that applies multi-directional gradients to compute the noise
score and selects the region with the highest score. Finally, we propose a
lightweight and effective classification head and explore two different
structures: noise-based classifier and noise-guided classifier. Extensive
experiments on the GenImage benchmark demonstrate the outstanding performance
of our method, which achieves an average accuracy of \textbf{98.9\%}
(\textbf{11.9}\%~$\uparrow$) and shows excellent cross-generator generalization
capability. Particularly, our method achieves an accuracy of over 98.2\% from
GAN to Diffusion and over 99.2\% from Diffusion to GAN. Moreover, it performs
error extraction at the millisecond level, nearly a hundred times faster than
existing methods. The code is at https://github.com/hongsong-wang/LOTA.

</details>


### [63] [PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic Analysis](https://arxiv.org/abs/2510.14241)
*Soumyya Kanti Datta,Tanvi Ranga,Chengzhe Sun,Siwei Lyu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的多模态音频-视觉框架，用于检测深度伪造技术。


<details>
  <summary>Details</summary>
Motivation: 传统检测方法无法充分识别由 GAN、扩散模型和神经渲染技术等高级生成模型生成的现代深度伪造品，因为这些技术会产生几乎完美的单个帧，但会无意中产生传统检测器经常忽略的微小时间差异。

Method: 该方法结合了语言、动态面部运动和面部识别线索，利用音素序列、唇部几何数据和高级面部识别嵌入。

Result: 该集成方法通过识别多种互补模态之间的不一致性，显着提高了对细微深度伪造更改的检测。

Conclusion: 该研究提出了一种有效的深度伪造检测框架，通过整合多模态信息，能够更准确地识别细微的伪造痕迹。

Abstract: The rise of manipulated media has made deepfakes a particularly insidious
threat, involving various generative manipulations such as lip-sync
modifications, face-swaps, and avatar-driven facial synthesis. Conventional
detection methods, which predominantly depend on manually designed
phoneme-viseme alignment thresholds, fundamental frame-level consistency
checks, or a unimodal detection strategy, inadequately identify modern-day
deepfakes generated by advanced generative models such as GANs, diffusion
models, and neural rendering techniques. These advanced techniques generate
nearly perfect individual frames yet inadvertently create minor temporal
discrepancies frequently overlooked by traditional detectors. We present a
novel multimodal audio-visual framework, Phoneme-Temporal and Identity-Dynamic
Analysis(PIA), incorporating language, dynamic face motion, and facial
identification cues to address these limitations. We utilize phoneme sequences,
lip geometry data, and advanced facial identity embeddings. This integrated
method significantly improves the detection of subtle deepfake alterations by
identifying inconsistencies across multiple complementary modalities. Code is
available at https://github.com/skrantidatta/PIA

</details>


### [64] [Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication](https://arxiv.org/abs/2510.14245)
*Miu Sumino,Mayu Ishii,Shun Kaizu,Daisuke Hisano,Yu Nakayama*

Main category: cs.CV

TL;DR: 提出了一种新的基于事件的OCC调制方案，称为事件间隔调制（EIM），该方案通过调制事件之间的时间间隔来提高传输速度，并在室内环境中实现了28 kbps@10m和8.4 kbps@50m的传输速率。


<details>
  <summary>Details</summary>
Motivation: 传统的基于帧的相机的OCC系统存在比特率低和处理负载高等局限性。为了解决这些问题，提出了利用事件视觉传感器（EVS）作为接收器的OCC系统。EVS具有异步操作和高动态范围，能够实现高速、低延迟和鲁棒的通信。目前还没有充分利用EVS独特性的调制方法。

Method: 提出了一种新的调制方案，称为事件间隔调制（EIM），专门为基于事件的OCC设计。EIM通过调制事件之间的时间间隔来改进传输速度。建立了EIM的理论模型，并进行了概念验证实验。调整和定制了EVS的参数，以优化EIM的频率响应。实验确定了EIM中可用的最大调制阶数。基于获得的参数进行传输实验。

Result: 在室内环境中，实现了10米传输距离下28 kbps和50米传输距离下8.4 kbps的传输速率。这为基于事件的OCC系统建立了新的比特率基准。

Conclusion: 论文提出了一种新的EIM调制方案，并成功地在实验中验证了其可行性，实现了比现有技术更高的传输速率。

Abstract: Optical camera communication (OCC) represents a promising visible light
communication technology. Nonetheless, typical OCC systems utilizing
frame-based cameras are encumbered by limitations, including low bit rate and
high processing load. To address these issues, OCC system utilizing an
event-based vision sensor (EVS) as receivers have been proposed. The EVS
enables high-speed, low-latency, and robust communication due to its
asynchronous operation and high dynamic range. In existing event-based OCC
systems, conventional modulation schemes such as on-off keying (OOK) and pulse
position modulation have been applied, however, to the best of our knowledge,
no modulation method has been proposed that fully exploits the unique
characteristics of the EVS. This paper proposes a novel modulation scheme,
called the event interval modulation (EIM) scheme, specifically designed for
event-based OCC. EIM enables improvement in transmission speed by modulating
information using the intervals between events. This paper proposes a
theoretical model of EIM and conducts a proof-of-concept experiment. First, the
parameters of the EVS are tuned and customized to optimize the frequency
response specifically for EIM. Then, the maximum modulation order usable in EIM
is determined experimentally. We conduct transmission experiments based on the
obtained parameters. Finally, we report successful transmission at 28 kbps over
10 meters and 8.4 kbps over 50 meters in an indoor environment. This sets a new
benchmark for bit rate in event-based OCC systems.

</details>


### [65] [MACE: Mixture-of-Experts Accelerated Coordinate Encoding for Large-Scale Scene Localization and Rendering](https://arxiv.org/abs/2510.14251)
*Mingkai Liu,Dikai Fan,Haohua Que,Haojia Gao,Xiao Liu,Shuxue Peng,Meixia Lin,Shengyu Gu,Ruicong Ye,Wanli Qiu,Handong Yao,Ruopeng Zhang,Xianliang Huang*

Main category: cs.CV

TL;DR: 提出了一种名为MACE的混合专家加速坐标编码方法，用于解决大规模场景中的高效定位和高质量渲染问题。


<details>
  <summary>Details</summary>
Motivation: 现有的场景坐标回归（SCR）方法在小规模定位中表现良好，但扩展到大规模场景时受到单网络容量的限制。

Method: 引入门控网络隐式分类和选择子网络，并在每次推理过程中仅激活单个子网络。提出了无辅助损失的负载均衡（ALF-LB）策略，以提高大规模场景的定位精度。

Result: 该框架显著降低了成本，同时保持了更高的精度，为大规模场景应用提供了一种有效的解决方案。在Cambridge数据集上的实验表明，该方法仅需10分钟的训练即可实现高质量的渲染效果。

Conclusion: MACE方法能够有效解决大规模场景中的高效定位和高质量渲染问题，并在降低计算成本的同时保持较高精度。

Abstract: Efficient localization and high-quality rendering in large-scale scenes
remain a significant challenge due to the computational cost involved. While
Scene Coordinate Regression (SCR) methods perform well in small-scale
localization, they are limited by the capacity of a single network when
extended to large-scale scenes. To address these challenges, we propose the
Mixed Expert-based Accelerated Coordinate Encoding method (MACE), which enables
efficient localization and high-quality rendering in large-scale scenes.
Inspired by the remarkable capabilities of MOE in large model domains, we
introduce a gating network to implicitly classify and select sub-networks,
ensuring that only a single sub-network is activated during each inference.
Furtheremore, we present Auxiliary-Loss-Free Load Balancing(ALF-LB) strategy to
enhance the localization accuracy on large-scale scene. Our framework provides
a significant reduction in costs while maintaining higher precision, offering
an efficient solution for large-scale scene applications. Additional
experiments on the Cambridge test set demonstrate that our method achieves
high-quality rendering results with merely 10 minutes of training.

</details>


### [66] [Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization](https://arxiv.org/abs/2510.14255)
*Liao Shen,Wentao Jiang,Yiran Zhu,Tiezheng Ge,Zhiguo Cao,Bo Zheng*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的基于强化学习的视频扩散框架，名为身份保持奖励引导优化 (IPRO)，以提高身份保持能力。


<details>
  <summary>Details</summary>
Motivation: 现有的图像到视频 (I2V) 模型在保持输入人像和生成的视频之间身份一致性方面存在困难，尤其是在视频中的人表现出显著的表情变化和动作时。当人脸仅占图像的一小部分时，这个问题变得至关重要。

Method: 该方法没有引入辅助模块或改变模型架构，而是引入了一种直接有效的调整算法，该算法使用人脸身份评分器优化扩散模型。为了提高性能和加速收敛，该方法通过采样链的最后步骤反向传播奖励信号，从而实现更丰富的梯度反馈。此外，还提出了一种新的人脸评分机制，将真实视频中的人脸视为人脸特征池，提供多角度人脸信息以增强泛化能力。KL 散度正则化进一步融入以稳定训练并防止过度拟合奖励信号。

Result: 在 Wan 2.2 I2V 模型和内部 I2V 模型上的大量实验证明了该方法的有效性。

Conclusion: IPRO 是一种有效的视频扩散框架，可提高身份保持能力。

Abstract: Recent advances in image-to-video (I2V) generation have achieved remarkable
progress in synthesizing high-quality, temporally coherent videos from static
images. Among all the applications of I2V, human-centric video generation
includes a large portion. However, existing I2V models encounter difficulties
in maintaining identity consistency between the input human image and the
generated video, especially when the person in the video exhibits significant
expression changes and movements. This issue becomes critical when the human
face occupies merely a small fraction of the image. Since humans are highly
sensitive to identity variations, this poses a critical yet under-explored
challenge in I2V generation. In this paper, we propose Identity-Preserving
Reward-guided Optimization (IPRO), a novel video diffusion framework based on
reinforcement learning to enhance identity preservation. Instead of introducing
auxiliary modules or altering model architectures, our approach introduces a
direct and effective tuning algorithm that optimizes diffusion models using a
face identity scorer. To improve performance and accelerate convergence, our
method backpropagates the reward signal through the last steps of the sampling
chain, enabling richer gradient feedback. We also propose a novel facial
scoring mechanism that treats faces in ground-truth videos as facial feature
pools, providing multi-angle facial information to enhance generalization. A
KL-divergence regularization is further incorporated to stabilize training and
prevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V
model and our in-house I2V model demonstrate the effectiveness of our method.
Our project and code are available at
\href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}.

</details>


### [67] [Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning](https://arxiv.org/abs/2510.14256)
*Xiangyu Meng,Zixian Zhang,Zhenghao Zhang,Junchao Liao,Long Qin,Weizhi Wang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为 Identity-GRPO 的方法，用于改进多人视频生成中身份保持的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法在动态交互场景中难以保持多个人物身份的一致性。

Method: 该方法构建了一个视频奖励模型，并采用了一种针对多人一致性定制的 GRPO 变体。

Result: 实验表明，Identity-GRPO 在人类一致性指标上比基线方法提高了 18.9%。

Conclusion: Identity-GRPO 能够有效提升个性化视频生成中强化学习的效果。

Abstract: While advanced methods like VACE and Phantom have advanced video generation
for specific subjects in diverse scenarios, they struggle with multi-human
identity preservation in dynamic interactions, where consistent identities
across multiple characters are critical. To address this, we propose
Identity-GRPO, a human feedback-driven optimization pipeline for refining
multi-human identity-preserving video generation. First, we construct a video
reward model trained on a large-scale preference dataset containing
human-annotated and synthetic distortion data, with pairwise annotations
focused on maintaining human consistency throughout the video. We then employ a
GRPO variant tailored for multi-human consistency, which greatly enhances both
VACE and Phantom. Through extensive ablation studies, we evaluate the impact of
annotation quality and design choices on policy optimization. Experiments show
that Identity-GRPO achieves up to 18.9% improvement in human consistency
metrics over baseline methods, offering actionable insights for aligning
reinforcement learning with personalized video generation.

</details>


### [68] [Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval](https://arxiv.org/abs/2510.14535)
*Keima Abe,Hayato Muraki,Shuhei Tomoshige,Kenichi Oishi,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 提出了PL-SE-ADA框架，用于领域协调和可解释的表示学习，同时保留大脑MR图像中的疾病相关信息。


<details>
  <summary>Details</summary>
Motivation: 医学图像由于扫描仪和协议的差异，经常在成像站点之间显示出域偏移，这降低了机器学习在疾病分类等任务中的性能。因此，领域协调是一个关键的研究重点。

Method: PL-SE-ADA包括两个编码器$f_E$和$f_{SE}$来提取$\boldsymbol{z_u}$和$\boldsymbol{z_d}$，一个解码器来重建图像$f_D$，以及一个域预测器$g_D$。除了编码器和域预测器之间的对抗性训练之外，该模型还学习通过对$\boldsymbol{z_u}$和$\boldsymbol{z_d}$的重建求和来重建输入图像$\boldsymbol{x}$，从而确保协调性和信息性。

Result: 与先前的方法相比，PL-SE-ADA在图像重建、疾病分类和领域识别方面实现了相等或更好的性能。

Conclusion: PL-SE-ADA还能够可视化领域独立的脑部特征和领域特定的成分，从而在整个框架中提供高度的可解释性。

Abstract: Medical images like MR scans often show domain shifts across imaging sites
due to scanner and protocol differences, which degrade machine learning
performance in tasks such as disease classification. Domain harmonization is
thus a critical research focus. Recent approaches encode brain images
$\boldsymbol{x}$ into a low-dimensional latent space $\boldsymbol{z}$, then
disentangle it into $\boldsymbol{z_u}$ (domain-invariant) and
$\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these
methods often lack interpretability$-$an essential requirement in medical
applications$-$leaving practical issues unresolved. We propose
Pseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a
general framework for domain harmonization and interpretable representation
learning that preserves disease-relevant information in brain MR images.
PL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract
$\boldsymbol{z_u}$ and $\boldsymbol{z_d}$, a decoder to reconstruct the image
$f_D$, and a domain predictor $g_D$. Beyond adversarial training between the
encoder and domain predictor, the model learns to reconstruct the input image
$\boldsymbol{x}$ by summing reconstructions from $\boldsymbol{z_u}$ and
$\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared
to prior methods, PL-SE-ADA achieves equal or better performance in image
reconstruction, disease classification, and domain recognition. It also enables
visualization of both domain-independent brain features and domain-specific
components, offering high interpretability across the entire framework.

</details>


### [69] [MatchAttention: Matching the Relative Positions for High-Resolution Cross-View Matching](https://arxiv.org/abs/2510.14260)
*Tingman Yan,Tao Liu,Xilian Yang,Qunfei Zhao,Zeyang Xia*

Main category: cs.CV

TL;DR: This paper introduces MatchAttention, an attention mechanism for efficient and accurate cross-view matching, especially for high-resolution images.


<details>
  <summary>Details</summary>
Motivation: Existing cross-attention mechanisms suffer from quadratic complexity and lack explicit matching constraints, making high-resolution image matching challenging.

Method: The paper proposes MatchAttention, which dynamically matches relative positions using BilinearSoftmax for continuous sliding-window attention sampling. It also introduces MatchDecoder, gated cross-MatchAttention, and a consistency-constrained loss to handle occlusions.

Result: MatchStereo-B ranked 1st on the Middlebury benchmark and achieves state-of-the-art performance on KITTI 2012, KITTI 2015, ETH3D, and Spring flow datasets. MatchStereo-T can process 4K UHD images in 0.1 seconds with 3GB of GPU memory.

Conclusion: The proposed approach enables real-time, high-resolution, and high-accuracy cross-view matching.

Abstract: Cross-view matching is fundamentally achieved through cross-attention
mechanisms. However, matching of high-resolution images remains challenging due
to the quadratic complexity and lack of explicit matching constraints in the
existing cross-attention. This paper proposes an attention mechanism,
MatchAttention, that dynamically matches relative positions. The relative
position determines the attention sampling center of the key-value pairs given
a query. Continuous and differentiable sliding-window attention sampling is
achieved by the proposed BilinearSoftmax. The relative positions are
iteratively updated through residual connections across layers by embedding
them into the feature channels. Since the relative position is exactly the
learning target for cross-view matching, an efficient hierarchical cross-view
decoder, MatchDecoder, is designed with MatchAttention as its core component.
To handle cross-view occlusions, gated cross-MatchAttention and a
consistency-constrained loss are proposed. These two components collectively
mitigate the impact of occlusions in both forward and backward passes, allowing
the model to focus more on learning matching relationships. When applied to
stereo matching, MatchStereo-B ranked 1st in average error on the public
Middlebury benchmark and requires only 29ms for KITTI-resolution inference.
MatchStereo-T can process 4K UHD images in 0.1 seconds using only 3GB of GPU
memory. The proposed models also achieve state-of-the-art performance on KITTI
2012, KITTI 2015, ETH3D, and Spring flow datasets. The combination of high
accuracy and low computational complexity makes real-time, high-resolution, and
high-accuracy cross-view matching possible. Code is available at
https://github.com/TingmanYan/MatchAttention.

</details>


### [70] [Experimental Demonstration of Event-based Optical Camera Communication in Long-Range Outdoor Environment](https://arxiv.org/abs/2510.14266)
*Miu Sumino,Mayu Ishii,Shun Kaizu,Daisuke Hisano,Yu Nakayama*

Main category: cs.CV

TL;DR: 提出了一种基于事件视觉传感器的光摄像头通信系统的鲁棒解调方案


<details>
  <summary>Details</summary>
Motivation: 在室外实验中，在200米-60kbps和400米-30kbps的条件下，首次实现了BER < 10^{-3}

Method: 结合了OOK与toggle解调和数字锁相环

Result: 在200米-60kbps和400米-30kbps的条件下，实现了BER < 10^{-3}

Conclusion: 提出了一种鲁棒的解调方案

Abstract: We propose a robust demodulation scheme for optical camera communication
systems using an event-based vision sensor, combining OOK with toggle
demodulation and a digital phase-locked loop. This is the first report to
achieve a $\mathrm{BER} < 10^{-3}$ at 200m-60kbps and 400m-30kbps in outdoor
experiments.

</details>


### [71] [GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering](https://arxiv.org/abs/2510.14270)
*Alexander Valverde,Brian Xu,Yuyin Zhou,Meng Xu,Hongyun Wang*

Main category: cs.CV

TL;DR: 提出了一种名为 GauSSmart 的混合方法，该方法结合了 2D 基础模型和 3D 高斯溅射重建，以增强场景重建效果。


<details>
  <summary>Details</summary>
Motivation: 高斯溅射在大型数据集上表现出色，但在稀疏覆盖区域捕捉精细细节或保持真实感方面存在困难，这主要是由于稀疏 3D 训练数据的固有局限性。

Method: 该方法整合了已建立的 2D 计算机视觉技术，包括凸滤波和来自 DINO 等基础模型的语义特征监督，以增强基于高斯的场景重建。通过利用 2D 分割先验和高维特征嵌入，该方法指导高斯 splat 的密集化和细化，从而改善覆盖不足区域的覆盖范围并保留复杂的结构细节。

Result: 在三个数据集上验证了该方法，GauSSmart 在大多数评估场景中始终优于现有的高斯溅射。

Conclusion: 混合 2D-3D 方法具有巨大的潜力，通过将 2D 基础模型与 3D 重建管线相结合，可以克服单独使用任一方法的局限性。

Abstract: Scene reconstruction has emerged as a central challenge in computer vision,
with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting
achieving remarkable progress. While Gaussian Splatting demonstrates strong
performance on large-scale datasets, it often struggles to capture fine details
or maintain realism in regions with sparse coverage, largely due to the
inherent limitations of sparse 3D training data.
  In this work, we propose GauSSmart, a hybrid method that effectively bridges
2D foundational models and 3D Gaussian Splatting reconstruction. Our approach
integrates established 2D computer vision techniques, including convex
filtering and semantic feature supervision from foundational models such as
DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D
segmentation priors and high-dimensional feature embeddings, our method guides
the densification and refinement of Gaussian splats, improving coverage in
underrepresented areas and preserving intricate structural details.
  We validate our approach across three datasets, where GauSSmart consistently
outperforms existing Gaussian Splatting in the majority of evaluated scenes.
Our results demonstrate the significant potential of hybrid 2D-3D approaches,
highlighting how the thoughtful combination of 2D foundational models with 3D
reconstruction pipelines can overcome the limitations inherent in either
approach alone.

</details>


### [72] [CLEAR: Causal Learning Framework For Robust Histopathology Tumor Detection Under Out-Of-Distribution Shifts](https://arxiv.org/abs/2510.14273)
*Kieu-Anh Truong Thi,Huy-Hieu Pham,Duc-Trong Le*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的基于因果推理的框架，以解决组织病理学中由于采集过程或数据源差异导致的领域转移问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖于通过对齐特征分布或引入统计变化来建模统计相关性，但它们经常忽略因果关系。

Method: 该方法通过设计显式包含中介和观察到的组织切片的转换策略来实现前门原理。

Result: 在CAMELYON17数据集和一个私有组织病理学数据集上验证了该方法，在未见过的领域中表现出一致的性能提升。在CAMELYON17数据集和私有组织病理学数据集上，该方法取得了高达7%的改进，优于现有基线。

Conclusion: 这些结果突出了因果推理作为解决组织病理学图像分析中领域转移问题的强大工具的潜力。

Abstract: Domain shift in histopathology, often caused by differences in acquisition
processes or data sources, poses a major challenge to the generalization
ability of deep learning models. Existing methods primarily rely on modeling
statistical correlations by aligning feature distributions or introducing
statistical variation, yet they often overlook causal relationships. In this
work, we propose a novel causal-inference-based framework that leverages
semantic features while mitigating the impact of confounders. Our method
implements the front-door principle by designing transformation strategies that
explicitly incorporate mediators and observed tissue slides. We validate our
method on the CAMELYON17 dataset and a private histopathology dataset,
demonstrating consistent performance gains across unseen domains. As a result,
our approach achieved up to a 7% improvement in both the CAMELYON17 dataset and
the private histopathology dataset, outperforming existing baselines. These
results highlight the potential of causal inference as a powerful tool for
addressing domain shift in histopathology image analysis.

</details>


### [73] [Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding](https://arxiv.org/abs/2510.14304)
*Kyungryul Back,Seongbeom Park,Milim Kim,Mincheol Kwon,SangHyeok Lee,Hyunyoung Lee,Junhee Cho,Seunghyun Park,Jinkyu Kim*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的三层对比解码水印方法，以减少大型视觉语言模型（LVLMs）中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: LVLMs容易产生幻觉，过度依赖单一模态或记忆训练数据，而不能正确地根据视觉信息生成输出。

Method: 该方法包括三个步骤：选择成熟层和业余层，使用水印相关问题识别枢轴层以评估视觉基础，并应用三层对比解码生成最终输出。

Result: 在POPE、MME和AMBER等公共基准测试上的实验表明，该方法在减少LVLMs中的幻觉方面取得了最先进的性能，并生成了更符合视觉基础的响应。

Conclusion: 该方法有效地减少了LVLMs中的幻觉，并提高了视觉基础性。

Abstract: Large Vision-Language Models (LVLMs) have recently shown promising results on
various multimodal tasks, even achieving human-comparable performance in
certain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often
rely heavily on a single modality or memorize training data without properly
grounding their outputs. To address this, we propose a training-free, tri-layer
contrastive decoding with watermarking, which proceeds in three steps: (1)
select a mature layer and an amateur layer among the decoding layers, (2)
identify a pivot layer using a watermark-related question to assess whether the
layer is visually well-grounded, and (3) apply tri-layer contrastive decoding
to generate the final output. Experiments on public benchmarks such as POPE,
MME and AMBER demonstrate that our method achieves state-of-the-art performance
in reducing hallucinations in LVLMs and generates more visually grounded
responses.

</details>


### [74] [A Multi-domain Image Translative Diffusion StyleGAN for Iris Presentation Attack Detection](https://arxiv.org/abs/2510.14314)
*Shivangi Yadav,Arun Ross*

Main category: cs.CV

TL;DR: 提出了一种新的框架，用于生成合成的眼部图像，该框架可以捕捉多个领域的PA和真实特征，例如真实的、打印的眼睛和化妆品隐形眼镜。


<details>
  <summary>Details</summary>
Motivation: 虹膜生物识别系统容易受到演示攻击（PA）的攻击，其中人造眼睛、打印的眼睛图像或美容隐形眼镜等伪影被呈现给系统。为了解决这个问题，已经开发了几种演示攻击检测（PAD）方法。然而，由于构建和成像PA的内在困难，用于训练和评估虹膜PAD技术的数据集非常稀缺。

Method: MID-StyleGAN结合了扩散模型和生成对抗网络（GAN）的优势，以生成逼真和多样化的合成数据。我们的方法利用多域架构，可以实现真实眼部图像和不同PA域之间的转换。该模型采用为眼部数据量身定制的自适应损失函数，以保持域一致性。

Result: MID-StyleGAN在生成高质量合成眼部图像方面优于现有方法。生成的数据显著提高了PAD系统的性能，为虹膜和眼部生物识别技术中的数据稀缺问题提供了一个可扩展的解决方案。例如，在LivDet2020数据集上，1%误检率下的真检率从93.41%提高到98.72%，展示了所提出方法的影响。

Conclusion: 该论文提出了一种新的框架MID-StyleGAN，用于生成合成眼部图像，有效解决了虹膜和眼部生物识别技术中的数据稀缺问题，并显著提高了PAD系统的性能。

Abstract: An iris biometric system can be compromised by presentation attacks (PAs)
where artifacts such as artificial eyes, printed eye images, or cosmetic
contact lenses are presented to the system. To counteract this, several
presentation attack detection (PAD) methods have been developed. However, there
is a scarcity of datasets for training and evaluating iris PAD techniques due
to the implicit difficulties in constructing and imaging PAs. To address this,
we introduce the Multi-domain Image Translative Diffusion StyleGAN
(MID-StyleGAN), a new framework for generating synthetic ocular images that
captures the PA and bonafide characteristics in multiple domains such as
bonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the
strengths of diffusion models and generative adversarial networks (GANs) to
produce realistic and diverse synthetic data. Our approach utilizes a
multi-domain architecture that enables the translation between bonafide ocular
images and different PA domains. The model employs an adaptive loss function
tailored for ocular data to maintain domain consistency. Extensive experiments
demonstrate that MID-StyleGAN outperforms existing methods in generating
high-quality synthetic ocular images. The generated data was used to
significantly enhance the performance of PAD systems, providing a scalable
solution to the data scarcity problem in iris and ocular biometrics. For
example, on the LivDet2020 dataset, the true detect rate at 1% false detect
rate improved from 93.41% to 98.72%, showcasing the impact of the proposed
method.

</details>


### [75] [Vision-Centric Activation and Coordination for Multimodal Large Language Models](https://arxiv.org/abs/2510.14349)
*Yunnan Wang,Fan Lu,Kecheng Zheng,Ziyuan Huang,Ziqiang Li,Wenjun Zeng,Xin Jin*

Main category: cs.CV

TL;DR: VaCo 通过结合多个视觉基础模型（VFM）的视觉信息来优化 MLLM 的表征，从而提升 MLLM 的视觉理解能力。


<details>
  <summary>Details</summary>
Motivation: 主流 MLLM 仅通过文本 token 的下一个 token 预测进行监督，忽略了以视觉为中心的关键信息，这对于分析能力至关重要。

Method: VaCo 引入了视觉判别对齐来整合从 VFM 中提取的任务感知感知特征，从而统一了 MLLM 中文本和视觉输出的优化。具体来说，VaCo 将可学习的模块化任务查询（MTQ）和视觉对齐层（VAL）整合到 MLLM 中，在不同 VFM 的监督下激活特定的视觉信号。为了协调 VFM 之间的表征冲突，设计的 Token Gateway Mask（TGM）限制了多组 MTQ 之间的信息流。

Result: 大量实验表明，VaCo 显着提高了不同 MLLM 在各种基准测试中的性能，展示了其在视觉理解方面的卓越能力。

Conclusion: VaCo 是一种有效的 MLLM 优化方法，它通过整合多个 VFM 的视觉信息来提升 MLLM 的视觉理解能力。

Abstract: Multimodal large language models (MLLMs) integrate image features from visual
encoders with LLMs, demonstrating advanced comprehension capabilities. However,
mainstream MLLMs are solely supervised by the next-token prediction of textual
tokens, neglecting critical vision-centric information essential for analytical
abilities. To track this dilemma, we introduce VaCo, which optimizes MLLM
representations through Vision-Centric activation and Coordination from
multiple vision foundation models (VFMs). VaCo introduces visual discriminative
alignment to integrate task-aware perceptual features extracted from VFMs,
thereby unifying the optimization of both textual and visual outputs in MLLMs.
Specifically, we incorporate the learnable Modular Task Queries (MTQs) and
Visual Alignment Layers (VALs) into MLLMs, activating specific visual signals
under the supervision of diverse VFMs. To coordinate representation conflicts
across VFMs, the crafted Token Gateway Mask (TGM) restricts the information
flow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo
significantly improves the performance of different MLLMs on various
benchmarks, showcasing its superior capabilities in visual comprehension.

</details>


### [76] [Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D Registration](https://arxiv.org/abs/2510.14354)
*Siddharth Tourani,Jayaram Reddy,Sarvesh Thakur,K Madhava Krishna,Muhammad Haris Khan,N Dinesh Reddy*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的自监督RGB-D配准方法，利用循环一致的关键点和姿态块来提高配准精度。


<details>
  <summary>Details</summary>
Motivation: 如何利用大量的无标签RGB-D数据进行场景的几何推理。

Method: 使用循环一致的关键点作为显著点，在匹配过程中加强空间一致性约束，并引入了一种新的姿态块，将GRU循环单元与变换同步相结合，融合历史和多视角数据。

Result: 该方法在ScanNet和3DMatch上超过了以往的自监督配准方法，甚至优于一些较早的监督方法。

Conclusion: 该方法有效，并且可以集成到现有方法中以提高性能。

Abstract: With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data has
become available. This prompts the question of how to utilize this data for
geometric reasoning of scenes. While many RGB-D registration meth- ods rely on
geometric and feature-based similarity, we take a different approach. We use
cycle-consistent keypoints as salient points to enforce spatial coherence
constraints during matching, improving correspondence accuracy. Additionally,
we introduce a novel pose block that combines a GRU recurrent unit with
transformation synchronization, blending historical and multi-view data. Our
approach surpasses previous self- supervised registration methods on ScanNet
and 3DMatch, even outperforming some older supervised methods. We also
integrate our components into existing methods, showing their effectiveness.

</details>


### [77] [Spatial Preference Rewarding for MLLMs Spatial Understanding](https://arxiv.org/abs/2510.14374)
*Han Qiu,Peng Gao,Lewei Lu,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为空间偏好奖励（SPR）的方法，通过奖励 MLLM 生成的详细且定位精确的描述，来增强 MLLM 的细粒度空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的 MLLM 在细粒度空间感知能力方面存在不足，并且无法满足用户对细粒度空间理解的需求。这是因为现有方法主要集中于调整 MLLM 以对预先注释的指令数据进行建模，而没有直接监督 MLLM 的实际响应。

Method: SPR 方法通过语义和定位分数来综合评估 MLLM 生成描述中的文本质量和定位质量。此外，该方法还通过直接偏好优化来改进 MLLM 描述，从而增强与视觉输入的细粒度对齐。

Result: 在标准参考和定位基准上的大量实验表明，SPR 能够有效提高 MLLM 的空间理解能力，且训练开销最小。

Conclusion: SPR 方法能够有效提升 MLLM 的细粒度空间理解能力。

Abstract: Multimodal large language models~(MLLMs) have demonstrated promising spatial
understanding capabilities, such as referencing and grounding object
descriptions. Despite their successes, MLLMs still fall short in fine-grained
spatial perception abilities, such as generating detailed region descriptions
or accurately localizing objects. Additionally, they often fail to respond to
the user's requirements for desired fine-grained spatial understanding. This
issue might arise because existing approaches primarily focus on tuning MLLMs
to model pre-annotated instruction data to inject spatial knowledge, without
direct supervision of MLLMs' actual responses. We address this issue by SPR, a
Spatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial
capabilities by rewarding MLLMs' detailed responses with precise object
localization over vague or inaccurate responses. With randomly selected image
regions and region descriptions from MLLMs, SPR introduces semantic and
localization scores to comprehensively evaluate the text quality and
localization quality in MLLM-generated descriptions. We also refine the MLLM
descriptions with better localization accuracy and pair the best-scored
refinement with the initial descriptions of the lowest score for direct
preference optimization, thereby enhancing fine-grained alignment with visual
input. Extensive experiments over standard referring and grounding benchmarks
show that SPR improves MLLM spatial understanding capabilities effectively with
minimal overhead in training. Data and code will be released at
https://github.com/hanqiu-hq/SPR

</details>


### [78] [DOS: Directional Object Separation in Text Embeddings for Multi-Object Image Generation](https://arxiv.org/abs/2510.14376)
*Dongnam Byun,Jungwon Park,Jumgmin Ko,Changin Choi,Wonjong Rhee*

Main category: cs.CV

TL;DR: 本文提出了一种名为DOS（Directional Object Separation）的方法，用于改进多对象图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型在处理包含多个对象的提示时，经常出现对象忽略或对象混合的问题。通过研究，作者识别出四个问题场景：相似形状、相似纹理、不同的背景偏差和多个对象，这些场景中对象间的关系经常导致失败。

Method: 该方法基于CLIP embeddings的两个关键观察，修改三种类型的CLIP文本嵌入，然后将其传递到文本到图像模型中。

Result: 实验结果表明，DOS持续提高了多对象图像生成的成功率，并减少了对象混合。在人工评估中，DOS明显优于四种竞争方法，在四个基准测试中获得了26.24%-43.04%的更多投票。

Conclusion: 这些结果表明，DOS是一种实用的有效解决方案，可以改善多对象图像生成。

Abstract: Recent progress in text-to-image (T2I) generative models has led to
significant improvements in generating high-quality images aligned with text
prompts. However, these models still struggle with prompts involving multiple
objects, often resulting in object neglect or object mixing. Through extensive
studies, we identify four problematic scenarios, Similar Shapes, Similar
Textures, Dissimilar Background Biases, and Many Objects, where inter-object
relationships frequently lead to such failures. Motivated by two key
observations about CLIP embeddings, we propose DOS (Directional Object
Separation), a method that modifies three types of CLIP text embeddings before
passing them into text-to-image models. Experimental results show that DOS
consistently improves the success rate of multi-object image generation and
reduces object mixing. In human evaluations, DOS significantly outperforms four
competing methods, receiving 26.24%-43.04% more votes across four benchmarks.
These results highlight DOS as a practical and effective solution for improving
multi-object image generation.

</details>


### [79] [DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with Analytical Insights](https://arxiv.org/abs/2510.14383)
*Danish Ali,Ajmal Mian,Naveed Akhtar,Ghulam Mubashar Hassan*

Main category: cs.CV

TL;DR: 提出了一种名为双分辨率双向Mamba (DRBD-Mamba) 的高效3D分割模型，用于捕获多尺度远程依赖关系，计算开销最小。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤亚区具有异质性，准确的脑肿瘤分割对于临床诊断和治疗具有重要意义。基于Mamba的状态空间模型表现出良好的性能，但由于跨多个空间轴的顺序特征计算，导致了显著的计算开销。此外，它们在不同的BraTS数据分区中的鲁棒性在很大程度上仍未得到探索，这在可靠的评估中留下了一个关键的差距。

Method: 利用空间填充曲线在3D到1D特征映射期间保持空间局部性，从而减少对计算密集型多轴特征扫描的依赖。为了丰富特征表示，提出了一个门控融合模块，自适应地整合前向和反向上下文，以及一个量化块，离散特征以提高鲁棒性。此外，在BraTS2023上提出了五个系统折叠，用于在不同条件下严格评估分割技术，并详细分析了常见的失败场景。

Result: 在最新方法使用的20%测试集上，该模型在完整肿瘤的Dice改善为0.10%，肿瘤核心的Dice改善为1.75%，增强肿瘤的Dice改善为0.93%。在所提出的系统五折评估中，该模型保持了具有竞争力的完整肿瘤准确率，同时实现了肿瘤核心平均Dice增益0.86%和增强肿瘤平均Dice增益1.45%，优于现有的最先进水平。此外，该模型在效率上提高了15倍，同时保持了较高的分割精度。

Conclusion: DRBD-Mamba在保持高分割精度的同时，效率提高了15倍，突出了其相对于现有方法的鲁棒性和计算优势。

Abstract: Accurate brain tumor segmentation is significant for clinical diagnosis and
treatment. It is challenging due to the heterogeneity of tumor subregions.
Mamba-based State Space Models have demonstrated promising performance.
However, they incur significant computational overhead due to sequential
feature computation across multiple spatial axes. Moreover, their robustness
across diverse BraTS data partitions remains largely unexplored, leaving a
critical gap in reliable evaluation. To address these limitations, we propose
dual-resolution bi-directional Mamba (DRBD-Mamba), an efficient 3D segmentation
model that captures multi-scale long-range dependencies with minimal
computational overhead. We leverage a space-filling curve to preserve spatial
locality during 3D-to-1D feature mapping, thereby reducing reliance on
computationally expensive multi-axial feature scans. To enrich feature
representation, we propose a gated fusion module that adaptively integrates
forward and reverse contexts, along with a quantization block that discretizes
features to improve robustness. In addition, we propose five systematic folds
on BraTS2023 for rigorous evaluation of segmentation techniques under diverse
conditions and present detailed analysis of common failure scenarios. On the
20\% test set used by recent methods, our model achieves Dice improvements of
0.10\% for whole tumor, 1.75\% for tumor core, and 0.93\% for enhancing tumor.
Evaluations on the proposed systematic five folds demonstrate that our model
maintains competitive whole tumor accuracy while achieving clear average Dice
gains of 0.86\% for tumor core and 1.45\% for enhancing tumor over existing
state-of-the-art. Furthermore, our model attains 15 times improvement in
efficiency while maintaining high segmentation accuracy, highlighting its
robustness and computational advantage over existing approaches.

</details>


### [80] [BoardVision: Deployment-ready and Robust Motherboard Defect Detection with YOLO+Faster-RCNN Ensemble](https://arxiv.org/abs/2510.14389)
*Brandon Hill,Kma Solaiman*

Main category: cs.CV

TL;DR: 本研究提出了BoardVision，一个用于检测主板组装缺陷的框架。


<details>
  <summary>Details</summary>
Motivation: 现有PCB检测主要集中在裸板或trace-level缺陷，而对完整主板的组装级检测仍未被充分探索。

Method: 使用YOLOv7和Faster R-CNN两种检测器，并提出了一种轻量级集成方法Confidence-Temporal Voting (CTV Voter)。

Result: 在MiracleFactory主板数据集上进行了基准测试，并评估了在各种扰动下的鲁棒性。

Conclusion: 验证了计算机视觉技术从基准测试结果到实际质量保证的转变潜力。

Abstract: Motherboard defect detection is critical for ensuring reliability in
high-volume electronics manufacturing. While prior research in PCB inspection
has largely targeted bare-board or trace-level defects, assembly-level
inspection of full motherboards inspection remains underexplored. In this work,
we present BoardVision, a reproducible framework for detecting assembly-level
defects such as missing screws, loose fan wiring, and surface scratches. We
benchmark two representative detectors - YOLOv7 and Faster R-CNN, under
controlled conditions on the MiracleFactory motherboard dataset, providing the
first systematic comparison in this domain. To mitigate the limitations of
single models, where YOLO excels in precision but underperforms in recall and
Faster R-CNN shows the reverse, we propose a lightweight ensemble,
Confidence-Temporal Voting (CTV Voter), that balances precision and recall
through interpretable rules. We further evaluate robustness under realistic
perturbations including sharpness, brightness, and orientation changes,
highlighting stability challenges often overlooked in motherboard defect
detection. Finally, we release a deployable GUI-driven inspection tool that
bridges research evaluation with operator usability. Together, these
contributions demonstrate how computer vision techniques can transition from
benchmark results to practical quality assurance for assembly-level motherboard
manufacturing.

</details>


### [81] [DCMIL: A Progressive Representation Learning Model of Whole Slide Images for Cancer Prognosis Analysis](https://arxiv.org/abs/2510.14403)
*Chao Tu,Kun Huang,Jie Zhang,Qianjin Feng,Yu Zhang,Zhenyuan Ning*

Main category: cs.CV

TL;DR: 提出了一种名为DCMIL的易到难渐进式表示学习模型，用于高效处理WSI以进行癌症预后，无需密集注释，可将千兆像素大小的WSI直接转换为结果预测。


<details>
  <summary>Details</summary>
Motivation: 计算病理学在利用全玻片图像（WSI）来量化形态异质性和开发人类癌症的客观预后模式方面显示出前景。然而，千兆像素大小输入的计算瓶颈和密集人工注释的稀缺性阻碍了进展。当前的方法通常忽略跨多放大倍数WSI的细粒度信息以及肿瘤微环境的变化。

Method: 提出了一种名为双课程对比多实例学习（DCMIL）的易到难渐进式表示学习模型。

Result: 在十二种癌症类型（5,954名患者，1254万张切片）上的大量实验表明，DCMIL优于基于WSI的标准预后模型。此外，DCMIL识别出细粒度的预后显着区域，提供稳健的实例不确定性估计，并捕获正常组织和肿瘤组织之间的形态差异，具有产生新的生物学见解的潜力。

Conclusion: DCMIL模型在癌症预后方面优于现有模型，并且能够识别预后相关区域，提供不确定性估计，并捕捉正常和肿瘤组织的形态差异，有潜力产生新的生物学见解。

Abstract: The burgeoning discipline of computational pathology shows promise in
harnessing whole slide images (WSIs) to quantify morphological heterogeneity
and develop objective prognostic modes for human cancers. However, progress is
impeded by the computational bottleneck of gigapixel-size inputs and the
scarcity of dense manual annotations. Current methods often overlook
fine-grained information across multi-magnification WSIs and variations in
tumor microenvironments. Here, we propose an easy-to-hard progressive
representation learning model, termed dual-curriculum contrastive
multi-instance learning (DCMIL), to efficiently process WSIs for cancer
prognosis. The model does not rely on dense annotations and enables the direct
transformation of gigapixel-size WSIs into outcome predictions. Extensive
experiments on twelve cancer types (5,954 patients, 12.54 million tiles)
demonstrate that DCMIL outperforms standard WSI-based prognostic models.
Additionally, DCMIL identifies fine-grained prognosis-salient regions, provides
robust instance uncertainty estimation, and captures morphological differences
between normal and tumor tissues, with the potential to generate new biological
insights. All codes have been made publicly accessible at
https://github.com/tuuuc/DCMIL.

</details>


### [82] [Real-Time Neural Video Compression with Unified Intra and Inter Coding](https://arxiv.org/abs/2510.14431)
*Hui Xiang,Yifan Bian,Li Li,Jingran Wu,Xianguo Zhang,Dong Liu*

Main category: cs.CV

TL;DR: 提出了一种新的神经视频压缩框架，通过统一的帧内和帧间编码以及同步双帧压缩，提高了压缩效率和稳定性，同时保持了实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经视频压缩方案在处理遮挡、新内容、帧间误差传播等方面存在局限性。

Method: 借鉴经典视频编码的帧内编码思想，设计了一个统一的帧内和帧间编码框架，并提出了同步双帧压缩方法。

Result: 实验结果表明，该方案在BD-rate上优于DCVC-RT 10.7%，并提供了更稳定的比特率和每帧质量，同时保持了实时编码/解码性能。

Conclusion: 该方案通过统一的帧内和帧间编码以及同步双帧压缩，有效解决了现有神经视频压缩方案的局限性，提高了压缩效率和稳定性。

Abstract: Neural video compression (NVC) technologies have advanced rapidly in recent
years, yielding state-of-the-art schemes such as DCVC-RT that offer superior
compression efficiency to H.266/VVC and real-time encoding/decoding
capabilities. Nonetheless, existing NVC schemes have several limitations,
including inefficiency in dealing with disocclusion and new content, interframe
error propagation and accumulation, among others. To eliminate these
limitations, we borrow the idea from classic video coding schemes, which allow
intra coding within inter-coded frames. With the intra coding tool enabled,
disocclusion and new content are properly handled, and interframe error
propagation is naturally intercepted without the need for manual refresh
mechanisms. We present an NVC framework with unified intra and inter coding,
where every frame is processed by a single model that is trained to perform
intra/inter coding adaptively. Moreover, we propose a simultaneous two-frame
compression design to exploit interframe redundancy not only forwardly but also
backwardly. Experimental results show that our scheme outperforms DCVC-RT by an
average of 10.7\% BD-rate reduction, delivers more stable bitrate and quality
per frame, and retains real-time encoding/decoding performances. Code and
models will be released.

</details>


### [83] [Structured Universal Adversarial Attacks on Object Detection for Video Sequences](https://arxiv.org/abs/2510.14460)
*Sven Jacob,Weijia Shao,Gjergji Kasneci*

Main category: cs.CV

TL;DR: 提出了一种针对视频目标检测的通用对抗攻击方法，该方法通过核范数正则化来促进集中在背景中的结构化扰动。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的目标检测器容易受到对抗性攻击，尤其是在涉及通用扰动时。这项工作旨在解决这个问题。

Method: 利用核范数正则化来促进结构化扰动集中在背景中，并采用自适应乐观指数梯度方法来有效优化此公式。

Result: 该攻击在有效性方面优于基于低秩投影梯度下降和 Frank-Wolfe 的攻击，同时保持了较高的隐蔽性。

Conclusion: 提出了一种有效的、隐蔽的通用对抗攻击方法，可以成功攻击视频目标检测器。

Abstract: Video-based object detection plays a vital role in safety-critical
applications. While deep learning-based object detectors have achieved
impressive performance, they remain vulnerable to adversarial attacks,
particularly those involving universal perturbations. In this work, we propose
a minimally distorted universal adversarial attack tailored for video object
detection, which leverages nuclear norm regularization to promote structured
perturbations concentrated in the background. To optimize this formulation
efficiently, we employ an adaptive, optimistic exponentiated gradient method
that enhances both scalability and convergence. Our results demonstrate that
the proposed attack outperforms both low-rank projected gradient descent and
Frank-Wolfe based attacks in effectiveness while maintaining high stealthiness.
All code and data are publicly available at
https://github.com/jsve96/AO-Exp-Attack.

</details>


### [84] [Unsupervised Deep Generative Models for Anomaly Detection in Neuroimaging: A Systematic Scoping Review](https://arxiv.org/abs/2510.14462)
*Youwan Mahé,Elise Bannier,Stéphanie Leplaideur,Elisa Fromont,Francesca Galassi*

Main category: cs.CV

TL;DR: 无监督深度生成模型为脑部影像异常检测提供了一种有前景的替代方案，它仅需在健康数据上训练，并将异常识别为与正常脑部结构的偏差。


<details>
  <summary>Details</summary>
Motivation: 与需要大量体素级注释数据集的监督方法不同，无监督深度生成模型不受限于特征明确的病理，可以通过学习到的正常脑部结构来识别异常。

Method: 本研究采用PRISMA指导的范围界定综述，综合了关于神经影像异常检测的无监督深度生成模型的最新研究，包括自编码器、变分自编码器、生成对抗网络和去噪扩散模型。

Result: 纳入的 2018 年至 2025 年间发表的 49 项研究表明，生成模型在大型病灶中取得了令人鼓舞的性能，并在解决更细微的异常方面取得了进展。生成模型的关键优势在于它们能够生成可解释的伪健康重建。

Conclusion: 生成模型为异常检测提供了一个引人注目的方向，可以实现半监督学习，支持新影像生物标志物的发现，并促进统一的端到端框架中疾病内部和跨疾病的偏差映射。

Abstract: Unsupervised deep generative models are emerging as a promising alternative
to supervised methods for detecting and segmenting anomalies in brain imaging.
Unlike fully supervised approaches, which require large voxel-level annotated
datasets and are limited to well-characterised pathologies, these models can be
trained exclusively on healthy data and identify anomalies as deviations from
learned normative brain structures. This PRISMA-guided scoping review
synthesises recent work on unsupervised deep generative models for anomaly
detection in neuroimaging, including autoencoders, variational autoencoders,
generative adversarial networks, and denoising diffusion models. A total of 49
studies published between 2018 - 2025 were identified, covering applications to
brain MRI and, less frequently, CT across diverse pathologies such as tumours,
stroke, multiple sclerosis, and small vessel disease. Reported performance
metrics are compared alongside architectural design choices. Across the
included studies, generative models achieved encouraging performance for large
focal lesions and demonstrated progress in addressing more subtle
abnormalities. A key strength of generative models is their ability to produce
interpretable pseudo-healthy (also referred to as counterfactual)
reconstructions, which is particularly valuable when annotated data are scarce,
as in rare or heterogeneous diseases. Looking ahead, these models offer a
compelling direction for anomaly detection, enabling semi-supervised learning,
supporting the discovery of novel imaging biomarkers, and facilitating within-
and cross-disease deviation mapping in unified end-to-end frameworks. To
realise clinical impact, future work should prioritise anatomy-aware modelling,
development of foundation models, task-appropriate evaluation metrics, and
rigorous clinical validation.

</details>


### [85] [Pruning Overparameterized Multi-Task Networks for Degraded Web Image Restoration](https://arxiv.org/abs/2510.14463)
*Thomas Katraouras,Dimitrios Rafailidis*

Main category: cs.CV

TL;DR: 本文提出了一种压缩多任务图像恢复模型的方法，通过迭代剪枝策略，在保持高性能的同时，显著减少了模型参数量。


<details>
  <summary>Details</summary>
Motivation: 在线社交网络中的有损操作会导致图像质量下降，影响用户体验。多任务图像恢复模型虽然可以同时处理不同类型的图像退化，但参数量过大，计算效率低。

Method: 提出名为MIR-L的模型，采用迭代剪枝策略，移除低幅度权重，并将剩余权重重置为原始初始化值，以发现“winning tickets”。

Result: 在去雨、去雾和去噪任务的基准数据集上，MIR-L仅保留了10%的可训练参数，同时保持了较高的图像恢复性能。

Conclusion: MIR-L模型能够在高稀疏度下保持或超过最先进的性能，实现了多任务图像恢复模型的有效压缩。

Abstract: Image quality is a critical factor in delivering visually appealing content
on web platforms. However, images often suffer from degradation due to lossy
operations applied by online social networks (OSNs), negatively affecting user
experience. Image restoration is the process of recovering a clean high-quality
image from a given degraded input. Recently, multi-task (all-in-one) image
restoration models have gained significant attention, due to their ability to
simultaneously handle different types of image degradations. However, these
models often come with an excessively high number of trainable parameters,
making them computationally inefficient. In this paper, we propose a strategy
for compressing multi-task image restoration models. We aim to discover highly
sparse subnetworks within overparameterized deep models that can match or even
surpass the performance of their dense counterparts. The proposed model, namely
MIR-L, utilizes an iterative pruning strategy that removes low-magnitude
weights across multiple rounds, while resetting the remaining weights to their
original initialization. This iterative process is important for the multi-task
image restoration model's optimization, effectively uncovering "winning
tickets" that maintain or exceed state-of-the-art performance at high sparsity
levels. Experimental evaluation on benchmark datasets for the deraining,
dehazing, and denoising tasks shows that MIR-L retains only 10% of the
trainable parameters while maintaining high image restoration performance. Our
code, datasets and pre-trained models are made publicly available at
https://github.com/Thomkat/MIR-L.

</details>


### [86] [Grazing Detection using Deep Learning and Sentinel-2 Time Series Data](https://arxiv.org/abs/2510.14493)
*Aleksis Pirinen,Delia Fano Yela,Smita Chakraborty,Erik Källman*

Main category: cs.CV

TL;DR: 本文利用遥感数据监测草地放牧情况，以提高土地利用合规性的检查效率。


<details>
  <summary>Details</summary>
Motivation: 大规模监测放牧地点仍然有限，但放牧对农业生产和生物多样性都有影响。

Method: 使用 Sentinel-2 L2A 时间序列，训练 CNN-LSTM 模型，对 4 月至 10 月的图像进行二元预测（放牧/未放牧）。

Result: 在五个验证集中，平均 F1 分数为 77%，对放牧牧场的召回率为 90%。如果检查员每年最多访问 4% 的地点，优先检查模型预测为非放牧的区域，确认的非放牧地点比随机检查多 17.2 倍。

Conclusion: 结果表明，粗分辨率、免费提供的卫星数据可以可靠地指导检查资源，以实现符合保护目标的土地利用合规性。代码和模型已公开提供。

Abstract: Grazing shapes both agricultural production and biodiversity, yet scalable
monitoring of where grazing occurs remains limited. We study seasonal grazing
detection from Sentinel-2 L2A time series: for each polygon-defined field
boundary, April-October imagery is used for binary prediction (grazed / not
grazed). We train an ensemble of CNN-LSTM models on multi-temporal reflectance
features, and achieve an average F1 score of 77 percent across five validation
splits, with 90 percent recall on grazed pastures. Operationally, if inspectors
can visit at most 4 percent of sites annually, prioritising fields predicted by
our model as non-grazed yields 17.2 times more confirmed non-grazing sites than
random inspection. These results indicate that coarse-resolution, freely
available satellite data can reliably steer inspection resources for
conservation-aligned land-use compliance. Code and models have been made
publicly available.

</details>


### [87] [Vision Mamba for Permeability Prediction of Porous Media](https://arxiv.org/abs/2510.14516)
*Ali Kashefi,Tapan Mukerji*

Main category: cs.CV

TL;DR: Vision Mamba作为ViT的替代方案，在图像分类中受到关注，它在计算和内存效率方面有所提高。


<details>
  <summary>Details</summary>
Motivation: Vision Mamba比ViT和CNN更有效率，因此，我们首次引入了一个使用Vision Mamba作为骨干的神经网络，用于预测三维多孔介质的渗透率。

Method: 将Vision Mamba与ViT和CNN模型在渗透率预测的多个方面进行了比较，并进行了一项烧蚀研究，以评估其组成部分对精度的影响。

Result: 在实践中证明了Vision Mamba在三维多孔介质的渗透率预测方面优于ViT和CNN。

Conclusion: 我们相信所提出的框架有潜力被整合到大型视觉模型中，在这些模型中，Vision Mamba被用来代替ViT。

Abstract: Vision Mamba has recently received attention as an alternative to Vision
Transformers (ViTs) for image classification. The network size of Vision Mamba
scales linearly with input image resolution, whereas ViTs scale quadratically,
a feature that improves computational and memory efficiency. Moreover, Vision
Mamba requires a significantly smaller number of trainable parameters than
traditional convolutional neural networks (CNNs), and thus, they can be more
memory efficient. Because of these features, we introduce, for the first time,
a neural network that uses Vision Mamba as its backbone for predicting the
permeability of three-dimensional porous media. We compare the performance of
Vision Mamba with ViT and CNN models across multiple aspects of permeability
prediction and perform an ablation study to assess the effects of its
components on accuracy. We demonstrate in practice the aforementioned
advantages of Vision Mamba over ViTs and CNNs in the permeability prediction of
three-dimensional porous media. We make the source code publicly available to
facilitate reproducibility and to enable other researchers to build on and
extend this work. We believe the proposed framework has the potential to be
integrated into large vision models in which Vision Mamba is used instead of
ViTs.

</details>


### [88] [Real-Time Surgical Instrument Defect Detection via Non-Destructive Testing](https://arxiv.org/abs/2510.14525)
*Qurrat Ul Ain,Atif Aftab Ahmed Jilani,Zunaira Shafqat,Nigar Azhar Butt*

Main category: cs.CV

TL;DR: SurgScan: AI-powered defect detection for surgical instruments using YOLOv8.


<details>
  <summary>Details</summary>
Motivation: Manual inspection of surgical instruments is error-prone and inconsistent, posing risks to patient safety.

Method: Developed SurgScan, an AI framework using YOLOv8 for real-time defect classification.

Result: SurgScan achieves 99.3% accuracy with 4.2-5.8 ms inference speed per image. Contrast-enhanced preprocessing improves defect detection.

Conclusion: SurgScan provides a scalable and cost-effective solution for automated quality control, reducing manual inspection and ensuring regulatory compliance.

Abstract: Defective surgical instruments pose serious risks to sterility, mechanical
integrity, and patient safety, increasing the likelihood of surgical
complications. However, quality control in surgical instrument manufacturing
often relies on manual inspection, which is prone to human error and
inconsistency. This study introduces SurgScan, an AI-powered defect detection
framework for surgical instruments. Using YOLOv8, SurgScan classifies defects
in real-time, ensuring high accuracy and industrial scalability. The model is
trained on a high-resolution dataset of 102,876 images, covering 11 instrument
types and five major defect categories. Extensive evaluation against
state-of-the-art CNN architectures confirms that SurgScan achieves the highest
accuracy (99.3%) with real-time inference speeds of 4.2-5.8 ms per image,
making it suitable for industrial deployment. Statistical analysis demonstrates
that contrast-enhanced preprocessing significantly improves defect detection,
addressing key limitations in visual inspection. SurgScan provides a scalable,
cost-effective AI solution for automated quality control, reducing reliance on
manual inspection while ensuring compliance with ISO 13485 and FDA standards,
paving the way for enhanced defect detection in medical manufacturing.

</details>


### [89] [Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models](https://arxiv.org/abs/2510.14526)
*Yunze Tong,Didi Zhu,Zijing Hu,Jinluan Yang,Ziyu Zhao*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的文本到图像生成方法，通过在去噪前对初始噪声进行文本条件细化，从而更好地与提示对齐，并减少训练和推理之间的不匹配。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成方法生成的图像可能与提示不符，因为训练和推理阶段噪声的分布存在差异。

Method: 该论文提出了一种噪声投影器，该投影器在去噪之前将噪声映射到提示感知的对应物，使其与SD训练期间观察到的分布更好地匹配。该方法通过从视觉语言模型(VLM)中提取token-level反馈，并将其提炼成奖励模型，然后通过准直接偏好优化来优化噪声投影器。

Result: 大量实验表明，该论文提出的提示感知噪声投影提高了各种提示的文本-图像对齐。

Conclusion: 该论文提出了一种有效的文本到图像生成方法，通过噪声投影器减少了训练和推理之间的不匹配，提高了文本-图像对齐，且无需参考图像或手工先验，并降低了推理成本。

Abstract: In text-to-image generation, different initial noises induce distinct
denoising paths with a pretrained Stable Diffusion (SD) model. While this
pattern could output diverse images, some of them may fail to align well with
the prompt. Existing methods alleviate this issue either by altering the
denoising dynamics or by drawing multiple noises and conducting post-selection.
In this paper, we attribute the misalignment to a training-inference mismatch:
during training, prompt-conditioned noises lie in a prompt-specific subset of
the latent space, whereas at inference the noise is drawn from a
prompt-agnostic Gaussian prior. To close this gap, we propose a noise projector
that applies text-conditioned refinement to the initial noise before denoising.
Conditioned on the prompt embedding, it maps the noise to a prompt-aware
counterpart that better matches the distribution observed during SD training,
without modifying the SD model. Our framework consists of these steps: we first
sample some noises and obtain token-level feedback for their corresponding
images from a vision-language model (VLM), then distill these signals into a
reward model, and finally optimize the noise projector via a quasi-direct
preference optimization. Our design has two benefits: (i) it requires no
reference images or handcrafted priors, and (ii) it incurs small inference
cost, replacing multi-sample selection with a single forward pass. Extensive
experiments further show that our prompt-aware noise projection improves
text-image alignment across diverse prompts.

</details>


### [90] [PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model](https://arxiv.org/abs/2510.14528)
*Cheng Cui,Ting Sun,Suyin Liang,Tingquan Gao,Zelun Zhang,Jiaxuan Liu,Xueqing Wang,Changda Zhou,Hongen Liu,Manhui Lin,Yue Zhang,Yubo Zhang,Handong Zheng,Jing Zhang,Jun Zhang,Yi Liu,Dianhai Yu,Yanjun Ma*

Main category: cs.CV

TL;DR: PaddleOCR-VL is a resource-efficient model for document parsing.


<details>
  <summary>Details</summary>
Motivation: To create a compact yet powerful vision-language model (VLM) for accurate element recognition in documents.

Method: Integrating a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model.

Result: Achieves SOTA performance in both page-level document parsing and element-level recognition.

Conclusion: PaddleOCR-VL is highly suitable for practical deployment in real-world scenarios due to its performance, efficiency, and speed.

Abstract: In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model
tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a
compact yet powerful vision-language model (VLM) that integrates a NaViT-style
dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to
enable accurate element recognition. This innovative model efficiently supports
109 languages and excels in recognizing complex elements (e.g., text, tables,
formulas, and charts), while maintaining minimal resource consumption. Through
comprehensive evaluations on widely used public benchmarks and in-house
benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document
parsing and element-level recognition. It significantly outperforms existing
solutions, exhibits strong competitiveness against top-tier VLMs, and delivers
fast inference speeds. These strengths make it highly suitable for practical
deployment in real-world scenarios.

</details>


### [91] [Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology](https://arxiv.org/abs/2510.14532)
*Xinrui Huang,Fan Xiao,Dongming He,Anqi Gao,Dandan Li,Xiaofan Zhang,Shaoting Zhang,Xudong Wang*

Main category: cs.CV

TL;DR: DentVFM: 首个牙科视觉基础模型，通过自监督学习和多模态数据，在多种牙科任务中表现出色，超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有牙科AI系统受限于单模态、任务特定和依赖昂贵标注数据，难以泛化到不同临床场景。

Method: 引入DentVFM，一个基于Vision Transformer (ViT)架构的视觉基础模型家族，使用在DentVista数据集上自监督学习。

Result: DentVFM在疾病诊断、治疗分析等多种牙科任务中表现出色，优于现有模型，并能实现跨模态诊断。

Conclusion: DentVFM为牙科AI设立了新范式，提供可扩展、适应性强且标签效率高的模型，以改善智能牙科保健。

Abstract: Oral and maxillofacial radiology plays a vital role in dental healthcare, but
radiographic image interpretation is limited by a shortage of trained
professionals. While AI approaches have shown promise, existing dental AI
systems are restricted by their single-modality focus, task-specific design,
and reliance on costly labeled data, hindering their generalization across
diverse clinical scenarios. To address these challenges, we introduce DentVFM,
the first family of vision foundation models (VFMs) designed for dentistry.
DentVFM generates task-agnostic visual representations for a wide range of
dental applications and uses self-supervised learning on DentVista, a large
curated dental imaging dataset with approximately 1.6 million multi-modal
radiographic images from various medical centers. DentVFM includes 2D and 3D
variants based on the Vision Transformer (ViT) architecture. To address gaps in
dental intelligence assessment and benchmarks, we introduce DentBench, a
comprehensive benchmark covering eight dental subspecialties, more diseases,
imaging modalities, and a wide geographical distribution. DentVFM shows
impressive generalist intelligence, demonstrating robust generalization to
diverse dental tasks, such as disease diagnosis, treatment analysis, biomarker
identification, and anatomical landmark detection and segmentation.
Experimental results indicate DentVFM significantly outperforms supervised,
self-supervised, and weakly supervised baselines, offering superior
generalization, label efficiency, and scalability. Additionally, DentVFM
enables cross-modality diagnostics, providing more reliable results than
experienced dentists in situations where conventional imaging is unavailable.
DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and
label-efficient model to improve intelligent dental healthcare and address
critical gaps in global oral healthcare.

</details>


### [92] [Exploring Image Representation with Decoupled Classical Visual Descriptors](https://arxiv.org/abs/2510.14536)
*Chenyuan Qu,Hao Chen,Jianbo Jiao*

Main category: cs.CV

TL;DR: 提出 VisualSplit 框架，显式地将图像分解为解耦的经典描述符，并将每个描述符视为视觉知识的独立但互补的组成部分。


<details>
  <summary>Details</summary>
Motivation: 深度学习在图像理解任务中取得了显著进展，但其内部表示通常是不透明的，难以解释视觉信息的处理方式。而经典视觉描述符（例如，边缘、颜色和强度分布）长期以来一直是图像分析的基础，并且对人类来说仍然是直观易懂的。因此， modern learning 是否可以从这些经典线索中受益？

Method: 通过重建驱动的预训练方案，VisualSplit 学习捕捉每个视觉描述符的本质，同时保持其可解释性。

Result: 通过显式地分解视觉属性，该方法固有地促进了各种高级视觉任务中的有效属性控制，包括图像生成和编辑，超越了传统的分类和分割。

Conclusion: 表明这种新的学习方法对于视觉理解的有效性

Abstract: Exploring and understanding efficient image representations is a
long-standing challenge in computer vision. While deep learning has achieved
remarkable progress across image understanding tasks, its internal
representations are often opaque, making it difficult to interpret how visual
information is processed. In contrast, classical visual descriptors (e.g. edge,
colour, and intensity distribution) have long been fundamental to image
analysis and remain intuitively understandable to humans. Motivated by this
gap, we ask a central question: Can modern learning benefit from these
classical cues? In this paper, we answer it with VisualSplit, a framework that
explicitly decomposes images into decoupled classical descriptors, treating
each as an independent but complementary component of visual knowledge. Through
a reconstruction-driven pre-training scheme, VisualSplit learns to capture the
essence of each visual descriptor while preserving their interpretability. By
explicitly decomposing visual attributes, our method inherently facilitates
effective attribute control in various advanced visual tasks, including image
generation and editing, extending beyond conventional classification and
segmentation, suggesting the effectiveness of this new learning approach for
visual understanding. Project page: https://chenyuanqu.com/VisualSplit/.

</details>


### [93] [Exploring Cross-Modal Flows for Few-Shot Learning](https://arxiv.org/abs/2510.14543)
*Ziqi Jiang,Yanghao Wang,Long Chen*

Main category: cs.CV

TL;DR: 现有的参数高效微调(PEFT)方法在调整视觉或文本特征时通常只进行一步调整，这对于模态特征高度纠缠的复杂数据集来说是不够的。本文提出了一个模型无关的多步调整方法，通过学习跨模态速度场来实现更精确和鲁棒的对齐。


<details>
  <summary>Details</summary>
Motivation: 跨模态任务的一个基本挑战是如何对齐来自不同模态的特征。现有的预训练视觉-语言模型虽然可以实现图像和文本之间的大致对齐，但对于进一步的调整，通常需要参数高效微调(PEFT)。

Method: 提出了一种模型无关的多步调整方法，通过学习跨模态速度场：Flow Matching Alignment (FMA)来实现。为了确保训练过程中类别之间的一致性，首先采用固定的耦合策略。然后，提出一种噪声增强策略来缓解数据稀疏问题。最后，设计了一种提前停止的求解器，可以提前终止转换过程，提高效率和准确性。

Result: 在各种基准和骨干网络上的大量结果表明，FMA能够持续产生显著的性能提升，尤其是在具有挑战性的数据集上。

Conclusion: FMA具有多步校正能力，能够实现更精确和鲁棒的对齐，优于一步PEFT方法。

Abstract: Aligning features from different modalities, is one of the most fundamental
challenges for cross-modal tasks. Although pre-trained vision-language models
can achieve a general alignment between image and text, they often require
parameter-efficient fine-tuning (PEFT) for further adjustment. Today's PEFT
methods (e.g., prompt tuning, LoRA-based, or adapter-based) always selectively
fine-tune a subset of parameters, which can slightly adjust either visual or
textual features, and avoid overfitting. In this paper, we are the first to
highlight that all existing PEFT methods perform one-step adjustment. It is
insufficient for complex (or difficult) datasets, where features of different
modalities are highly entangled. To this end, we propose the first
model-agnostic multi-step adjustment approach by learning a cross-modal
velocity field: Flow Matching Alignment (FMA). Specifically, to ensure the
correspondence between categories during training, we first utilize a fixed
coupling strategy. Then, we propose a noise augmentation strategy to alleviate
the data scarcity issue. Finally, we design an early-stopping solver, which
terminates the transformation process earlier, improving both efficiency and
accuracy. Compared with one-step PEFT methods, FMA has the multi-step
rectification ability to achieve more precise and robust alignment. Extensive
results have demonstrated that FMA can consistently yield significant
performance gains across various benchmarks and backbones, particularly on
challenging datasets.

</details>


### [94] [Consistent text-to-image generation via scene de-contextualization](https://arxiv.org/abs/2510.14553)
*Song Tang,Peihao Gong,Kunyu Li,Kai Guo,Boyu Wang,Mao Ye,Jianwei Zhang,Xiatian Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种名为场景去语境化 (SDeC) 的新方法，用于解决文本到图像生成中由于场景上下文与主体之间相关性引起的身份漂移问题。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型在生成跨场景的同一主体的图像时，由于身份漂移问题而失败。先前的方法依赖于预先知道所有目标场景的不切实际的假设。

Method: 本文通过量化 SVD 方向稳定性自适应地重新加权相应的特征值，从而识别并抑制 ID prompt 嵌入中潜在的场景-ID 相关性。

Result: 实验表明，SDeC 显著提高了身份保持能力，同时保持了场景多样性。

Conclusion: SDeC 是一种灵活通用的解决方案，适用于实际应用，在这些应用中，这种先验知识通常不可用或随时间变化。

Abstract: Consistent text-to-image (T2I) generation seeks to produce
identity-preserving images of the same subject across diverse scenes, yet it
often fails due to a phenomenon called identity (ID) shift. Previous methods
have tackled this issue, but typically rely on the unrealistic assumption of
knowing all target scenes in advance. This paper reveals that a key source of
ID shift is the native correlation between subject and scene context, called
scene contextualization, which arises naturally as T2I models fit the training
distribution of vast natural images. We formally prove the near-universality of
this scene-ID correlation and derive theoretical bounds on its strength. On
this basis, we propose a novel, efficient, training-free prompt embedding
editing approach, called Scene De-Contextualization (SDeC), that imposes an
inversion process of T2I's built-in scene contextualization. Specifically, it
identifies and suppresses the latent scene-ID correlation within the ID
prompt's embedding by quantifying the SVD directional stability to adaptively
re-weight the corresponding eigenvalues. Critically, SDeC allows for per-scene
use (one scene per prompt) without requiring prior access to all target scenes.
This makes it a highly flexible and general solution well-suited to real-world
applications where such prior knowledge is often unavailable or varies over
time. Experiments demonstrate that SDeC significantly enhances identity
preservation while maintaining scene diversity.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [95] [Decision Oriented Technique (DOTechnique): Finding Model Validity Through Decision-Maker Context](https://arxiv.org/abs/2510.13858)
*Raheleh Biglari,Joachim Denil*

Main category: cs.AI

TL;DR: 提出了一种新的模型验证方法，该方法通过决策一致性来确定模型有效性，而不是输出相似性。


<details>
  <summary>Details</summary>
Motivation: 传统的模型验证方法依赖于预定义的有效性框架，但这种框架并非总是可用或充分的。因此，本文旨在提出一种更有效的模型验证方法。

Method: 通过评估替代模型与高保真模型相比是否能做出等效的决策，从而确定模型有效性区域。该方法集成了领域约束和符号推理来缩小搜索空间，提高计算效率。

Result: 通过高速公路变道系统的例子，证明了DOTechnique可以发现仿真模型的有效性区域。

Conclusion: 该技术具有通过决策者上下文来支持发现模型有效性的潜力。

Abstract: Model validity is as critical as the model itself, especially when guiding
decision-making processes. Traditional approaches often rely on predefined
validity frames, which may not always be available or sufficient. This paper
introduces the Decision Oriented Technique (DOTechnique), a novel method for
determining model validity based on decision consistency rather than output
similarity. By evaluating whether surrogate models lead to equivalent decisions
compared to high-fidelity models, DOTechnique enables efficient identification
of validity regions, even in the absence of explicit validity boundaries. The
approach integrates domain constraints and symbolic reasoning to narrow the
search space, enhancing computational efficiency. A highway lane change system
serves as a motivating example, demonstrating how DOTechnique can uncover the
validity region of a simulation model. The results highlight the potential of
the technique to support finding model validity through decision-maker context.

</details>


### [96] [Do Slides Help? Multi-modal Context for Automatic Transcription of Conference Talks](https://arxiv.org/abs/2510.13979)
*Supriti Sinhamahapatra,Jan Niehues*

Main category: cs.AI

TL;DR: 该论文提出了一种结合视觉信息的自动语音识别系统，特别关注科学演示场景。


<details>
  <summary>Details</summary>
Motivation: 现有的语音识别系统忽略了多模态上下文，而视觉信息在消除歧义和适应性方面至关重要。尤其是在科学演示场景中，幻灯片包含重要的上下文信息。

Method: 1. 创建一个多模态演示基准，自动分析特定领域的术语。2. 探索用多模态信息增强语音模型的方法。3. 通过适当的数据增强方法缓解数据集的不足。

Result: 使用增强的数据集训练模型，与基线模型相比，所有词的词错误率相对降低了约34%，特定领域术语的词错误率相对降低了约35%。

Conclusion: 该研究表明，结合幻灯片等多模态信息可以显著提高科学演示场景下的语音识别性能。

Abstract: State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily
rely on acoustic information while disregarding additional multi-modal context.
However, visual information are essential in disambiguation and adaptation.
While most work focus on speaker images to handle noise conditions, this work
also focuses on integrating presentation slides for the use cases of scientific
presentation.
  In a first step, we create a benchmark for multi-modal presentation including
an automatic analysis of transcribing domain-specific terminology. Next, we
explore methods for augmenting speech models with multi-modal information. We
mitigate the lack of datasets with accompanying slides by a suitable approach
of data augmentation. Finally, we train a model using the augmented dataset,
resulting in a relative reduction in word error rate of approximately 34%,
across all words and 35%, for domain-specific terms compared to the baseline
model.

</details>


### [97] [Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment](https://arxiv.org/abs/2510.13985)
*María Victoria Carro,Denise Alejandra Mester,Francisca Gauna Selasco,Giovanni Franco Gabriel Marraffini,Mario Alejandro Leiva,Gerardo I. Simari,María Vanina Martinez*

Main category: cs.AI

TL;DR: 大型语言模型 (LLM) 在因果关系判断中容易产生因果错觉，即使证据不足也会推断出因果关系。


<details>
  <summary>Details</summary>
Motivation: 因果错觉是许多社会问题的根源，研究LLM是否也存在这种偏差。

Method: 构建了一个包含1000个医疗背景下的零 contingency 场景的数据集，并提示 LLM 评估潜在原因的有效性。

Result: 所有评估的模型都系统性地推断出不必要的因果关系，表明它们容易产生因果错觉。

Conclusion: 研究结果表明，LLM可能不真正理解因果关系，而只是在没有真正理解的情况下再现因果语言，这引发了人们对在需要准确因果推理的领域中使用语言模型的担忧。

Abstract: Causal learning is the cognitive process of developing the capability of
making causal inferences based on available information, often guided by
normative principles. This process is prone to errors and biases, such as the
illusion of causality, in which people perceive a causal relationship between
two variables despite lacking supporting evidence. This cognitive bias has been
proposed to underlie many societal problems, including social prejudice,
stereotype formation, misinformation, and superstitious thinking. In this work,
we examine whether large language models are prone to developing causal
illusions when faced with a classic cognitive science paradigm: the contingency
judgment task. To investigate this, we constructed a dataset of 1,000 null
contingency scenarios (in which the available information is not sufficient to
establish a causal relationship between variables) within medical contexts and
prompted LLMs to evaluate the effectiveness of potential causes. Our findings
show that all evaluated models systematically inferred unwarranted causal
relationships, revealing a strong susceptibility to the illusion of causality.
While there is ongoing debate about whether LLMs genuinely understand causality
or merely reproduce causal language without true comprehension, our findings
support the latter hypothesis and raise concerns about the use of language
models in domains where accurate causal reasoning is essential for informed
decision-making.

</details>


### [98] [GammaZero: Learning To Guide POMDP Belief Space Search With Graph Representations](https://arxiv.org/abs/2510.14035)
*Rajesh Mangannavar,Prasad Tadepalli*

Main category: cs.AI

TL;DR: 提出了一种以动作中心图表示框架，用于在部分可观察马尔可夫决策过程（POMDP）中学习指导规划。


<details>
  <summary>Details</summary>
Motivation: 现有的方法需要特定领域的神经架构，并且难以扩展，而GammaZero利用统一的基于图的信念表示，能够在一个领域内推广不同问题规模。

Method: 将信念状态系统地转换为以动作为中心的图，其中在小问题上学习的结构模式可以转移到更大的实例。采用具有解码器架构的图神经网络，从易于计算的问题的专家演示中学习价值函数和策略，然后应用这些学习到的启发式方法来指导更大问题上的蒙特卡洛树搜索。

Result: 在标准POMDP基准测试上的实验结果表明，当在相同大小的问题上进行训练和测试时，GammaZero实现了与BetaZero相当的性能，同时 uniquely 实现了对训练中遇到的问题的 2-4 倍大的问题的零样本泛化，并在减少搜索需求的同时保持了解决方案的质量。

Conclusion: GammaZero 能够在 POMDP 中学习指导规划，并且可以推广到更大的问题。

Abstract: We introduce an action-centric graph representation framework for learning to
guide planning in Partially Observable Markov Decision Processes (POMDPs).
Unlike existing approaches that require domain-specific neural architectures
and struggle with scalability, GammaZero leverages a unified graph-based belief
representation that enables generalization across problem sizes within a
domain. Our key insight is that belief states can be systematically transformed
into action-centric graphs where structural patterns learned on small problems
transfer to larger instances. We employ a graph neural network with a decoder
architecture to learn value functions and policies from expert demonstrations
on computationally tractable problems, then apply these learned heuristics to
guide Monte Carlo tree search on larger problems. Experimental results on
standard POMDP benchmarks demonstrate that GammaZero achieves comparable
performance to BetaZero when trained and tested on the same-sized problems,
while uniquely enabling zero-shot generalization to problems 2-4 times larger
than those seen during training, maintaining solution quality with reduced
search requirements.

</details>


### [99] [Position: Require Frontier AI Labs To Release Small "Analog" Models](https://arxiv.org/abs/2510.14053)
*Shriyash Upadhyay,Chaithanya Bandi,Narmeen Oozeer,Philip Quirke*

Main category: cs.AI

TL;DR: 本文提出了一种新的监管大型AI模型的方法，通过强制AI实验室发布小型、开放的模拟模型，以促进安全验证、可解释性研究和算法透明度，同时降低监管成本并加速安全进步。


<details>
  <summary>Details</summary>
Motivation: 现有的AI监管提案因安全成本问题而搁置，本文旨在寻找一种既能确保AI安全又能促进创新的监管方法。

Method: 本文建议强制大型AI实验室发布小型、开放的模拟模型，这些模型以类似于大型专有模型的方式训练和提炼。

Result: 研究表明，使用这些较小模型开发的安全和可解释性方法可以有效地推广到前沿规模的系统。

Conclusion: 更深入地理解模型可以缓解安全与创新之间的权衡，从而让我们两者兼得。

Abstract: Recent proposals for regulating frontier AI models have sparked concerns
about the cost of safety regulation, and most such regulations have been
shelved due to the safety-innovation tradeoff. This paper argues for an
alternative regulatory approach that ensures AI safety while actively promoting
innovation: mandating that large AI laboratories release small, openly
accessible analog models (scaled-down versions) trained similarly to and
distilled from their largest proprietary models.
  Analog models serve as public proxies, allowing broad participation in safety
verification, interpretability research, and algorithmic transparency without
forcing labs to disclose their full-scale models. Recent research demonstrates
that safety and interpretability methods developed using these smaller models
generalize effectively to frontier-scale systems. By enabling the wider
research community to directly investigate and innovate upon accessible
analogs, our policy substantially reduces the regulatory burden and accelerates
safety advancements.
  This mandate promises minimal additional costs, leveraging reusable resources
like data and infrastructure, while significantly contributing to the public
good. Our hope is not only that this policy be adopted, but that it illustrates
a broader principle supporting fundamental research in machine learning: deeper
understanding of models relaxes the safety-innovation tradeoff and lets us have
more of both.

</details>


### [100] [Generating Fair Consensus Statements with Social Choice on Token-Level MDPs](https://arxiv.org/abs/2510.14106)
*Carter Blair,Kate Larson*

Main category: cs.AI

TL;DR: 提出了一种新的共识声明生成框架，该框架利用多目标 Markov 决策过程 (MDP) 模拟任务，并结合社会选择理论，以保证在聚合不同意见时具有可证明的公平性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型语言模型的共识声明生成框架缺乏内在结构，难以在聚合不同意见时提供可证明的公平性保证。

Method: 将任务建模为多目标、令牌级别的 Markov 决策过程 (MDP)，其中每个目标对应于一个 agent 的偏好。每个 agent 的令牌级别奖励来自他们的策略（例如，个性化语言模型）。

Result: 实验表明，由平均主义目标引导的搜索生成的共识声明，与包括 Habermas Machine 在内的基线方法相比，具有改进的最坏情况 agent 对齐。

Conclusion: 该研究提出了一种基于 MDP 和社会选择理论的共识声明生成方法，能够在聚合不同意见时提高公平性。

Abstract: Current frameworks for consensus statement generation with large language
models lack the inherent structure needed to provide provable fairness
guarantees when aggregating diverse free-form opinions. We model the task as a
multi-objective, token-level Markov Decision Process (MDP), where each
objective corresponds to an agent's preference. Token-level rewards for each
agent are derived from their policy (e.g., a personalized language model). This
approach utilizes the finding that such policies implicitly define optimal
Q-functions, providing a principled way to quantify rewards at each generation
step without a value function (Rafailov et al., 2024). This MDP formulation
creates a formal structure amenable to analysis using principles from social
choice theory. We propose two approaches grounded in social choice theory.
First, we propose a stochastic generation policy guaranteed to be in the
ex-ante core, extending core stability concepts from voting theory to text
generation. This policy is derived from an underlying distribution over
complete statements that maximizes proportional fairness (Nash Welfare).
Second, for generating a single statement, we target the maximization of
egalitarian welfare using search algorithms within the MDP framework.
Empirically, experiments using language models to instantiate agent policies
show that search guided by the egalitarian objective generates consensus
statements with improved worst-case agent alignment compared to baseline
methods, including the Habermas Machine (Tessler et al., 2024).

</details>


### [101] [STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management](https://arxiv.org/abs/2510.14112)
*Huiliang Zhang,Di Wu,Arnaud Zinflou,Benoit Boulet*

Main category: cs.AI

TL;DR: 提出了一种名为STEMS的、用于协调建筑能源管理的、安全约束的多智能体强化学习框架。


<details>
  <summary>Details</summary>
Motivation: 当前的多建筑能源系统在利用时空信息不足，缺乏严格的安全保障和系统复杂性。

Method: 集成了两个核心组件：(1) 使用GCN-Transformer融合架构的时空图表示学习框架，以捕获建筑物间的关系和时间模式；(2) 结合控制障碍函数（Control Barrier Functions）的安全约束多智能体RL算法，以提供数学安全保证。

Result: 在真实建筑数据集上的大量实验表明，STEMS的性能优于现有方法，成本降低21%，排放降低18%，安全违规从35.1%大幅降低至5.6%，同时保持最佳舒适度，不舒适比例仅为0.13。

Conclusion: 该框架在极端天气条件下表现出强大的鲁棒性，并在不同建筑类型中保持有效性。

Abstract: Building energy management is essential for achieving carbon reduction goals,
improving occupant comfort, and reducing energy costs. Coordinated building
energy management faces critical challenges in exploiting spatial-temporal
dependencies while ensuring operational safety across multi-building systems.
Current multi-building energy systems face three key challenges: insufficient
spatial-temporal information exploitation, lack of rigorous safety guarantees,
and system complexity. This paper proposes Spatial-Temporal Enhanced Safe
Multi-Agent Coordination (STEMS), a novel safety-constrained multi-agent
reinforcement learning framework for coordinated building energy management.
STEMS integrates two core components: (1) a spatial-temporal graph
representation learning framework using a GCN-Transformer fusion architecture
to capture inter-building relationships and temporal patterns, and (2) a
safety-constrained multi-agent RL algorithm incorporating Control Barrier
Functions to provide mathematical safety guarantees. Extensive experiments on
real-world building datasets demonstrate STEMS's superior performance over
existing methods, showing that STEMS achieves 21% cost reduction, 18% emission
reduction, and dramatically reduces safety violations from 35.1% to 5.6% while
maintaining optimal comfort with only 0.13 discomfort proportion. The framework
also demonstrates strong robustness during extreme weather conditions and
maintains effectiveness across different building types.

</details>


### [102] [Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems](https://arxiv.org/abs/2510.14133)
*Edoardo Allegrini,Ananth Shreekumar,Z. Berkay Celik*

Main category: cs.AI

TL;DR: This paper introduces a modeling framework for multi-agent AI systems to address the fragmentation in inter-agent communication, which hinders analysis and introduces risks. It proposes two models: the host agent model and the task lifecycle model, and defines properties for each to enable formal verification and improve system reliability.


<details>
  <summary>Details</summary>
Motivation: The current ecosystem of inter-agent communication is fragmented, preventing rigorous analysis of system properties and introducing risks.

Method: The paper introduces a modeling framework with two models: the host agent model and the task lifecycle model. It defines 17 properties for the host agent and 14 for the task lifecycle, expressed in temporal logic.

Result: The framework enables formal verification of system behavior, detection of coordination edge cases, and prevention of deadlocks and security vulnerabilities.

Conclusion: The paper introduces the first rigorously grounded, domain-agnostic framework for the systematic analysis, design, and deployment of correct, reliable, and robust agentic AI systems.

Abstract: Agentic AI systems, which leverage multiple autonomous agents and Large
Language Models (LLMs), are increasingly used to address complex, multi-step
tasks. The safety, security, and functionality of these systems are critical,
especially in high-stakes applications. However, the current ecosystem of
inter-agent communication is fragmented, with protocols such as the Model
Context Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol
for coordination being analyzed in isolation. This fragmentation creates a
semantic gap that prevents the rigorous analysis of system properties and
introduces risks such as architectural misalignment and exploitable
coordination issues. To address these challenges, we introduce a modeling
framework for agentic AI systems composed of two foundational models. The
first, the host agent model, formalizes the top-level entity that interacts
with the user, decomposes tasks, and orchestrates their execution by leveraging
external agents and tools. The second, the task lifecycle model, details the
states and transitions of individual sub-tasks from creation to completion,
providing a fine-grained view of task management and error handling. Together,
these models provide a unified semantic framework for reasoning about the
behavior of multi-AI agent systems. Grounded in this framework, we define 17
properties for the host agent and 14 for the task lifecycle, categorized into
liveness, safety, completeness, and fairness. Expressed in temporal logic,
these properties enable formal verification of system behavior, detection of
coordination edge cases, and prevention of deadlocks and security
vulnerabilities. Through this effort, we introduce the first rigorously
grounded, domain-agnostic framework for the systematic analysis, design, and
deployment of correct, reliable, and robust agentic AI systems.

</details>


### [103] [A Multimodal Approach to Heritage Preservation in the Context of Climate Change](https://arxiv.org/abs/2510.14136)
*David Roqui,Adèle Cormier,nistor Grozavu,Ann Bourges*

Main category: cs.AI

TL;DR: 提出了一种轻量级多模态架构，融合传感器数据和视觉图像来预测文化遗产遗址的退化程度。


<details>
  <summary>Details</summary>
Motivation: 传统监测依赖于单模态分析，无法捕捉环境压力与材料退化之间的复杂相互作用。气候变化加速了文化遗产遗址的退化。

Method: 该方法改进了PerceiverIO，通过简化编码器和自适应Barlow Twins损失来实现。

Result: 在斯特拉斯堡大教堂的数据上，该模型达到了76.9%的准确率，比标准多模态架构提高了43%，比原始PerceiverIO提高了25%。

Conclusion: 该研究表明，架构的简单性与对比正则化相结合，可以在数据稀缺的遗产监测环境中实现有效的多模态学习，为人工智能驱动的保护决策支持系统奠定基础。

Abstract: Cultural heritage sites face accelerating degradation due to climate change,
yet tradi- tional monitoring relies on unimodal analysis (visual inspection or
environmental sen- sors alone) that fails to capture the complex interplay
between environmental stres- sors and material deterioration. We propose a
lightweight multimodal architecture that fuses sensor data (temperature,
humidity) with visual imagery to predict degradation severity at heritage
sites. Our approach adapts PerceiverIO with two key innovations: (1) simplified
encoders (64D latent space) that prevent overfitting on small datasets (n=37
training samples), and (2) Adaptive Barlow Twins loss that encourages modality
complementarity rather than redundancy. On data from Strasbourg Cathedral, our
model achieves 76.9% accu- racy, a 43% improvement over standard multimodal
architectures (VisualBERT, Trans- former) and 25% over vanilla PerceiverIO.
Ablation studies reveal that sensor-only achieves 61.5% while image-only
reaches 46.2%, confirming successful multimodal synergy. A systematic
hyperparameter study identifies an optimal moderate correlation target ({\tau}
=0.3) that balances align- ment and complementarity, achieving 69.2% accuracy
compared to other {\tau} values ({\tau} =0.1/0.5/0.7: 53.8%, {\tau} =0.9:
61.5%). This work demonstrates that architectural sim- plicity combined with
contrastive regularization enables effective multimodal learning in data-scarce
heritage monitoring contexts, providing a foundation for AI-driven con-
servation decision support systems.

</details>


### [104] [CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization](https://arxiv.org/abs/2510.14150)
*Henrique Assumpção,Diego Ferreira,Leandro Campos,Fabricio Murai*

Main category: cs.AI

TL;DR: CodeEvolve是一个开源的进化编码代理，它结合了大型语言模型（LLM）和遗传算法来解决复杂的计算问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决复杂的计算问题，并构建在最近的广义科学发现方法之上，将强大的进化概念应用到LLM领域。

Method: 采用基于岛屿的遗传算法来维持种群多样性和增加吞吐量，引入了一种新颖的基于灵感的交叉机制，该机制利用LLM的上下文窗口来组合来自成功解决方案的特征，并实施元提示策略以动态探索解决方案空间。

Result: 在用于评估Google DeepMind的闭源AlphaEvolve的数学基准的子集上对CodeEvolve进行了严格的评估。我们的发现表明，我们的方法在几个具有挑战性的问题上超过了AlphaEvolve的性能。

Conclusion: 为了促进协作和加速进步，我们将完整的框架作为开源存储库发布。

Abstract: In this work, we introduce CodeEvolve, an open-source evolutionary coding
agent that unites Large Language Models (LLMs) with genetic algorithms to solve
complex computational problems. Our framework adapts powerful evolutionary
concepts to the LLM domain, building upon recent methods for generalized
scientific discovery. CodeEvolve employs an island-based genetic algorithm to
maintain population diversity and increase throughput, introduces a novel
inspiration-based crossover mechanism that leverages the LLMs context window to
combine features from successful solutions, and implements meta-prompting
strategies for dynamic exploration of the solution space. We conduct a rigorous
evaluation of CodeEvolve on a subset of the mathematical benchmarks used to
evaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that
our method surpasses AlphaEvolve's performance on several challenging problems.
To foster collaboration and accelerate progress, we release our complete
framework as an open-source repository.

</details>


### [105] [Combining Reinforcement Learning and Behavior Trees for NPCs in Video Games with AMD Schola](https://arxiv.org/abs/2510.14154)
*Tian Liu,Alex Cann,Ian Colbert,Mehdi Saeedi*

Main category: cs.AI

TL;DR: 本研究探讨了强化学习（RL）在商业视频游戏中的应用瓶颈，并强调了RL与传统行为树（BTs）的结合是未来的关键方向。


<details>
  <summary>Details</summary>
Motivation: 强化学习在游戏AI中的应用面临挑战，商业视频游戏采用缓慢。本研究旨在解决这些挑战。

Method: 通过AMD Schola插件在虚幻引擎中创建多任务NPC，并在受《最后生还者》启发的复杂3D环境中，展示了联合训练RL模型与BTs的方法。

Result: 验证了BT+RL方法的可行性，并展示了各种技能。

Conclusion: 强调了RL与BTs结合的重要性，并为游戏AI社区提供了实践指导。

Abstract: While the rapid advancements in the reinforcement learning (RL) research
community have been remarkable, the adoption in commercial video games remains
slow. In this paper, we outline common challenges the Game AI community faces
when using RL-driven NPCs in practice, and highlight the intersection of RL
with traditional behavior trees (BTs) as a crucial juncture to be explored
further. Although the BT+RL intersection has been suggested in several research
papers, its adoption is rare. We demonstrate the viability of this approach
using AMD Schola -- a plugin for training RL agents in Unreal Engine -- by
creating multi-task NPCs in a complex 3D environment inspired by the commercial
video game ``The Last of Us". We provide detailed methodologies for jointly
training RL models with BTs while showcasing various skills.

</details>


### [106] [JEDA: Query-Free Clinical Order Search from Ambient Dialogues](https://arxiv.org/abs/2510.14169)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Amitabh Saikia,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: JEDA是一个快速、可解释且无需LLM的检索层，它将环境上下文与可操作的临床医嘱实时连接起来。


<details>
  <summary>Details</summary>
Motivation: 现有的临床医嘱系统依赖于LLM重写，这增加了延迟、不稳定性和不透明性，阻碍了实时医嘱。

Method: JEDA使用领域初始化的双编码器，直接检索规范医嘱，并在无查询模式下，编码短时间窗口的环境对话以触发检索。它使用受限的LLM指导，将每个签署的医嘱与互补的公式联系起来。

Result: JEDA在实践中获得了很大的收益，并且大大优于其基础编码器和最近的开放嵌入器。

Conclusion: JEDA提供了一种快速、可解释且无需LLM的检索层，可以在实时环境中将环境上下文与可操作的临床医嘱联系起来。

Abstract: Clinical conversations mix explicit directives (order a chest X-ray) with
implicit reasoning (the cough worsened overnight, we should check for
pneumonia). Many systems rely on LLM rewriting, adding latency, instability,
and opacity that hinder real-time ordering. We present JEDA (Joint Embedding
for Direct and Ambient clinical orders), a domain-initialized bi-encoder that
retrieves canonical orders directly and, in a query-free mode, encodes a short
rolling window of ambient dialogue to trigger retrieval. Initialized from
PubMedBERT and fine-tuned with a duplicate-safe contrastive objective, JEDA
aligns heterogeneous expressions of intent to shared order concepts. Training
uses constrained LLM guidance to tie each signed order to complementary
formulations (command only, context only, command+context, context+reasoning),
producing clearer inter-order separation, tighter query extendash order
coupling, and stronger generalization. The query-free mode is noise-resilient,
reducing sensitivity to disfluencies and ASR errors by conditioning on a short
window rather than a single utterance. Deployed in practice, JEDA yields large
gains and substantially outperforms its base encoder and recent open embedders
(Linq Embed Mistral, SFR Embedding, GTE Qwen, BGE large, Embedding Gemma). The
result is a fast, interpretable, LLM-free retrieval layer that links ambient
context to actionable clinical orders in real time.

</details>


### [107] [ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning](https://arxiv.org/abs/2510.14176)
*Roger Creus Castanyer,Faisal Mohamed,Pablo Samuel Castro,Cyrus Neary,Glen Berseth*

Main category: cs.AI

TL;DR: 提出了一种名为ARM-FM的框架，利用大型模型自动生成强化学习中的奖励函数。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法对奖励函数非常敏感，这限制了它们的广泛应用。

Method: 使用大型模型自动构建奖励机（RM），奖励机是一种基于自动机的奖励规范形式。

Result: 在各种具有挑战性的环境中，包括零样本泛化，提供了ARM-FM有效性的经验证据。

Conclusion: 利用大型模型的推理能力，自动进行组合奖励设计。

Abstract: Reinforcement learning (RL) algorithms are highly sensitive to reward
function specification, which remains a central challenge limiting their broad
applicability. We present ARM-FM: Automated Reward Machines via Foundation
Models, a framework for automated, compositional reward design in RL that
leverages the high-level reasoning capabilities of foundation models (FMs).
Reward machines (RMs) -- an automata-based formalism for reward specification
-- are used as the mechanism for RL objective specification, and are
automatically constructed via the use of FMs. The structured formalism of RMs
yields effective task decompositions, while the use of FMs enables objective
specifications in natural language. Concretely, we (i) use FMs to automatically
generate RMs from natural language specifications; (ii) associate language
embeddings with each RM automata-state to enable generalization across tasks;
and (iii) provide empirical evidence of ARM-FM's effectiveness in a diverse
suite of challenging environments, including evidence of zero-shot
generalization.

</details>


### [108] [Implementation of AI in Precision Medicine](https://arxiv.org/abs/2510.14194)
*Göktuğ Bender,Samer Faraj,Anand Bhardwaj*

Main category: cs.AI

TL;DR: AI在精准医疗中的应用日益重要，但临床应用仍然有限。本文回顾了2019-2024年关于AI在精准医疗中应用的文献，重点关注数据质量、临床可靠性、工作流程整合和治理等方面的障碍和推动因素。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在精准医疗中的应用现状及挑战，旨在推动AI技术在精准医疗领域的实际应用。

Method: 通过范围界定审查方法，对2019-2024年相关文献进行分析。

Result: 识别了数据质量、临床可靠性、工作流程整合和治理等方面的关键障碍和推动因素。

Conclusion: 强调了影响现实应用的相互依赖关系，并为支持可信和可持续的实施提出了未来方向。

Abstract: Artificial intelligence (AI) has become increasingly central to precision
medicine by enabling the integration and interpretation of multimodal data, yet
implementation in clinical settings remains limited. This paper provides a
scoping review of literature from 2019-2024 on the implementation of AI in
precision medicine, identifying key barriers and enablers across data quality,
clinical reliability, workflow integration, and governance. Through an
ecosystem-based framework, we highlight the interdependent relationships
shaping real-world translation and propose future directions to support
trustworthy and sustainable implementation.

</details>


### [109] [Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks](https://arxiv.org/abs/2510.14207)
*Trilok Padhi,Pinxian Lu,Abdulkadir Erol,Tanmay Sutar,Gauri Sharma,Mina Sonmez,Munmun De Choudhury,Ugur Kursuncu*

Main category: cs.AI

TL;DR: LLM agents in web apps are vulnerable to multi-turn harassment. This paper introduces a benchmark, attack methods, and evaluation to analyze and mitigate this.


<details>
  <summary>Details</summary>
Motivation: Prior jailbreak research focused on single-turn prompts, while real harassment is multi-turn. The study aims to address this gap.

Method: The paper presents a benchmark with a synthetic dataset, multi-agent simulation, jailbreak methods (memory, planning, fine-tuning), and a mixed-methods evaluation framework. It uses LLaMA-3.1-8B-Instruct and Gemini-2.0-flash.

Result: Jailbreak tuning significantly increases harassment success rates (up to 99.33%) and reduces refusal rates. Insult and Flaming are the most prevalent toxic behaviors. Models exhibit human-like aggression profiles, with distinct escalation trajectories between closed-source and open-source models.

Conclusion: Multi-turn attacks succeed and mimic human harassment dynamics, highlighting the need for robust safety guardrails for online platforms.

Abstract: Large Language Model (LLM) agents are powering a growing share of interactive
web applications, yet remain vulnerable to misuse and harm. Prior jailbreak
research has largely focused on single-turn prompts, whereas real harassment
often unfolds over multi-turn interactions. In this work, we present the Online
Harassment Agentic Benchmark consisting of: (i) a synthetic multi-turn
harassment conversation dataset, (ii) a multi-agent (e.g., harasser, victim)
simulation informed by repeated game theory, (iii) three jailbreak methods
attacking agents across memory, planning, and fine-tuning, and (iv) a
mixed-methods evaluation framework. We utilize two prominent LLMs,
LLaMA-3.1-8B-Instruct (open-source) and Gemini-2.0-flash (closed-source). Our
results show that jailbreak tuning makes harassment nearly guaranteed with an
attack success rate of 95.78--96.89% vs. 57.25--64.19% without tuning in Llama,
and 99.33% vs. 98.46% without tuning in Gemini, while sharply reducing refusal
rate to 1-2% in both models. The most prevalent toxic behaviors are Insult with
84.9--87.8% vs. 44.2--50.8% without tuning, and Flaming with 81.2--85.1% vs.
31.5--38.8% without tuning, indicating weaker guardrails compared to sensitive
categories such as sexual or racial harassment. Qualitative evaluation further
reveals that attacked agents reproduce human-like aggression profiles, such as
Machiavellian/psychopathic patterns under planning, and narcissistic tendencies
with memory. Counterintuitively, closed-source and open-source models exhibit
distinct escalation trajectories across turns, with closed-source models
showing significant vulnerability. Overall, our findings show that multi-turn
and theory-grounded attacks not only succeed at high rates but also mimic
human-like harassment dynamics, motivating the development of robust safety
guardrails to ultimately keep online platforms safe and responsible.

</details>


### [110] [LiveResearchBench: A Live Benchmark for User-Centric Deep Research in the Wild](https://arxiv.org/abs/2510.14240)
*Jiayu Wang,Yifei Ming,Riya Dulepet,Qinglin Chen,Austin Xu,Zixuan Ke,Frederic Sala,Aws Albarghouthi,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 提出了LiveResearchBench和DeepEval，用于评估agentic系统的深度研究能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试在评估agentic系统的深度研究能力方面存在不足，无法满足用户需求、动态性、明确性和多面性等原则。

Method: 构建了包含100个任务的LiveResearchBench基准测试，并设计了DeepEval评估套件，用于评估报告的内容和质量。

Result: 对17个深度研究系统进行了评估，揭示了现有系统的优势、不足以及关键的系统组件。

Conclusion: LiveResearchBench和DeepEval为系统评估深度研究能力提供了严谨的基础，并通过分析揭示了提升系统性能的关键要素。

Abstract: Deep research -- producing comprehensive, citation-grounded reports by
searching and synthesizing information from hundreds of live web sources --
marks an important frontier for agentic systems. To rigorously evaluate this
ability, four principles are essential: tasks should be (1) user-centric,
reflecting realistic information needs, (2) dynamic, requiring up-to-date
information beyond parametric knowledge, (3) unambiguous, ensuring consistent
interpretation across users, and (4) multi-faceted and search-intensive,
requiring search over numerous web sources and in-depth analysis. Existing
benchmarks fall short of these principles, often focusing on narrow domains or
posing ambiguous questions that hinder fair comparison. Guided by these
principles, we introduce LiveResearchBench, a benchmark of 100 expert-curated
tasks spanning daily life, enterprise, and academia, each requiring extensive,
dynamic, real-time web search and synthesis. Built with over 1,500 hours of
human labor, LiveResearchBench provides a rigorous basis for systematic
evaluation. To evaluate citation-grounded long-form reports, we introduce
DeepEval, a comprehensive suite covering both content- and report-level
quality, including coverage, presentation, citation accuracy and association,
consistency and depth of analysis. DeepEval integrates four complementary
evaluation protocols, each designed to ensure stable assessment and high
agreement with human judgments. Using LiveResearchBench and DeepEval, we
conduct a comprehensive evaluation of 17 frontier deep research systems,
including single-agent web search, single-agent deep research, and multi-agent
systems. Our analysis reveals current strengths, recurring failure modes, and
key system components needed to advance reliable, insightful deep research.

</details>


### [111] [Towards Agentic Self-Learning LLMs in Search Environment](https://arxiv.org/abs/2510.14253)
*Wangtao Sun,Xiang Cheng,Jialin Fan,Yao Xu,Xing Yu,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.AI

TL;DR: 这篇论文研究了如何通过自学习扩展基于 LLM 的 Agent，而无需依赖人工数据集或预定义的规则奖励。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索在没有人工干预的情况下，如何有效地训练 LLM Agent。

Method: 提出了 Agentic Self-Learning (ASL) 框架，该框架整合了任务生成、策略执行和评估，利用 Prompt Generator、Policy Model 和 Generative Reward Model 形成良性循环。

Result: 实验结果表明，ASL 能够持续提升性能，优于其他基线方法，并在零标签数据条件下表现出卓越的样本效率和鲁棒性。GRM 的验证能力是主要瓶颈。

Conclusion: 该研究确定了奖励来源和数据规模是开放域 Agent 学习的关键因素，并证明了多角色协同进化对于可扩展、自我改进的 Agent 的有效性。

Abstract: We study whether self-learning can scale LLM-based agents without relying on
human-curated datasets or predefined rule-based rewards. Through controlled
experiments in a search-agent setting, we identify two key determinants of
scalable agent training: the source of reward signals and the scale of agent
task data. We find that rewards from a Generative Reward Model (GRM) outperform
rigid rule-based signals for open-domain learning, and that co-evolving the GRM
with the policy further boosts performance. Increasing the volume of agent task
data-even when synthetically generated-substantially enhances agentic
capabilities. Building on these insights, we propose \textbf{Agentic
Self-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning
framework that unifies task generation, policy execution, and evaluation within
a shared tool environment and LLM backbone. ASL coordinates a Prompt Generator,
a Policy Model, and a Generative Reward Model to form a virtuous cycle of
harder task setting, sharper verification, and stronger solving. Empirically,
ASL delivers steady, round-over-round gains, surpasses strong RLVR baselines
(e.g., Search-R1) that plateau or degrade, and continues improving under
zero-labeled-data conditions, indicating superior sample efficiency and
robustness. We further show that GRM verification capacity is the main
bottleneck: if frozen, it induces reward hacking and stalls progress; continual
GRM training on the evolving data distribution mitigates this, and a small
late-stage injection of real verification data raises the performance ceiling.
This work establishes reward source and data scale as critical levers for
open-domain agent learning and demonstrates the efficacy of multi-role
co-evolution for scalable, self-improving agents. The data and code of this
paper are released at
https://github.com/forangel2014/Towards-Agentic-Self-Learning

</details>


### [112] [MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning](https://arxiv.org/abs/2510.14265)
*Xukai Wang,Xuanbo Liu,Mingrui Chen,Haitian Zhong,Xuanlin Yang,Bohan Zeng,Jinbo Hu,Hao Liang,Junbo Niu,Xuchen Li,Ruitao Wu,Ruichuan An,Yang Shi,Liu Liu,Xu-Yao Zhang,Qiang Liu,Zhouchen Lin,Wentao Zhang,Bin Dong*

Main category: cs.AI

TL;DR: 提出MorphoBench，一个可以通过调整问题难度来评估大型模型推理能力的benchmark。


<details>
  <summary>Details</summary>
Motivation: 现有评估大型模型推理能力的benchmark范围有限，且缺乏根据模型推理能力调整难度的灵活性。

Method: 1) 从现有benchmark和奥林匹克竞赛等来源收集复杂推理问题；2) 通过利用模型推理过程中生成的关键语句，自适应地修改问题的分析挑战；3) 包含使用仿真软件生成的问题，从而以最小的资源消耗动态调整benchmark难度。

Result: 收集了超过1300个测试问题，并根据o3和GPT-5等模型的推理能力迭代调整了MorphoBench的难度。

Conclusion: MorphoBench提高了模型推理评估的全面性和有效性，为提高大型模型的推理能力和科学鲁棒性提供了可靠的指导。

Abstract: With the advancement of powerful large-scale reasoning models, effectively
evaluating the reasoning capabilities of these models has become increasingly
important. However, existing benchmarks designed to assess the reasoning
abilities of large models tend to be limited in scope and lack the flexibility
to adapt their difficulty according to the evolving reasoning capacities of the
models. To address this, we propose MorphoBench, a benchmark that incorporates
multidisciplinary questions to evaluate the reasoning capabilities of large
models and can adjust and update question difficulty based on the reasoning
abilities of advanced models. Specifically, we curate the benchmark by
selecting and collecting complex reasoning questions from existing benchmarks
and sources such as Olympiad-level competitions. Additionally, MorphoBench
adaptively modifies the analytical challenge of questions by leveraging key
statements generated during the model's reasoning process. Furthermore, it
includes questions generated using simulation software, enabling dynamic
adjustment of benchmark difficulty with minimal resource consumption. We have
gathered over 1,300 test questions and iteratively adjusted the difficulty of
MorphoBench based on the reasoning capabilities of models such as o3 and GPT-5.
MorphoBench enhances the comprehensiveness and validity of model reasoning
evaluation, providing reliable guidance for improving both the reasoning
abilities and scientific robustness of large models. The code has been released
in https://github.com/OpenDCAI/MorphoBench.

</details>


### [113] [A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space](https://arxiv.org/abs/2510.14301)
*Bingjie Zhang,Yibo Yang,Renzhe,Dandan Guo,Jindong Gu,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: 提出GuardSpace框架，用于在微调过程中保持LLM的安全性对齐。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在各种任务中取得了显著成功，但其安全性对齐在适应过程中仍然脆弱。即使在良性数据上进行微调或使用低秩适应，预训练的安全行为也很容易退化，从而导致微调模型中出现有害响应。

Method: 使用协方差预处理奇异值分解将预训练权重显式分解为安全相关和安全不相关的分量，并从安全不相关的分量初始化低秩适配器，同时冻结安全相关分量以保持其相关的安全机制。构建一个零空间投影器，该投影器限制适配器更新改变有害提示上的安全输出，从而保持原始的拒绝行为。

Result: 在多个下游任务上使用各种预训练模型进行的实验表明，GuardSpace 优于现有方法。对于在 GSM8K 上微调的 Llama-2-7B-Chat，GuardSpace 优于最先进的 AsFT 方法，将平均有害分数从 14.4% 降低到 3.6%，同时将准确率从 26.0% 提高到 28.0%。

Conclusion: GuardSpace 框架能够有效提升微调后模型的安全性，同时保持或提升模型在下游任务中的性能。

Abstract: Large language models (LLMs) have achieved remarkable success in diverse
tasks, yet their safety alignment remains fragile during adaptation. Even when
fine-tuning on benign data or with low-rank adaptation, pre-trained safety
behaviors are easily degraded, leading to harmful responses in the fine-tuned
models. To address this challenge, we propose GuardSpace, a guardrail framework
for preserving safety alignment throughout fine-tuning, composed of two key
components: a safety-sensitive subspace and a harmful-resistant null space.
First, we explicitly decompose pre-trained weights into safety-relevant and
safety-irrelevant components using covariance-preconditioned singular value
decomposition, and initialize low-rank adapters from the safety-irrelevant
ones, while freezing safety-relevant components to preserve their associated
safety mechanism. Second, we construct a null space projector that restricts
adapter updates from altering safe outputs on harmful prompts, thereby
maintaining the original refusal behavior. Experiments with various pre-trained
models on multiple downstream tasks demonstrate that GuardSpace achieves
superior performance over existing methods. Notably, for Llama-2-7B-Chat
fine-tuned on GSM8K, GuardSpace outperforms the state-of-the-art method AsFT,
reducing the average harmful score from 14.4% to 3.6%, while improving the
accuracy from from 26.0% to 28.0%.

</details>


### [114] [Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies](https://arxiv.org/abs/2510.14312)
*Mason Nakamura,Abhinav Kumar,Saaduddin Mahmud,Sahar Abdelnabi,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: 本文提出了一个名为Terrarium的框架，用于研究基于LLM的多智能体系统中的安全性、隐私和安全问题。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的多智能体系统可以自动化繁琐的用户任务，但也引入了新的风险，包括对齐问题和恶意攻击。

Method: 本文重新利用了黑板设计，创建了一个模块化、可配置的测试平台，并识别了关键的攻击向量。

Result: 本文实现了三个协作MAS场景和四个代表性攻击，以展示框架的灵活性。

Conclusion: Terrarium旨在通过提供快速原型设计、评估和迭代防御和设计的工具，加速实现值得信赖的多智能体系统。

Abstract: A multi-agent system (MAS) powered by large language models (LLMs) can
automate tedious user tasks such as meeting scheduling that requires
inter-agent collaboration. LLMs enable nuanced protocols that account for
unstructured private data, user constraints, and preferences. However, this
design introduces new risks, including misalignment and attacks by malicious
parties that compromise agents or steal user data. In this paper, we propose
the Terrarium framework for fine-grained study on safety, privacy, and security
in LLM-based MAS. We repurpose the blackboard design, an early approach in
multi-agent systems, to create a modular, configurable testbed for multi-agent
collaboration. We identify key attack vectors such as misalignment, malicious
agents, compromised communication, and data poisoning. We implement three
collaborative MAS scenarios with four representative attacks to demonstrate the
framework's flexibility. By providing tools to rapidly prototype, evaluate, and
iterate on defenses and designs, Terrarium aims to accelerate progress toward
trustworthy multi-agent systems.

</details>


### [115] [Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction](https://arxiv.org/abs/2510.14319)
*Xu Shen,Qi Zhang,Song Wang,Zhen Tan,Xinyu Zhao,Laura Yao,Vaishnav Tadiparthi,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Kwonjoon Lee,Tianlong Chen*

Main category: cs.AI

TL;DR: MASC: A metacognitive framework for multi-agent systems (MAS) that detects and corrects errors in real-time.


<details>
  <summary>Details</summary>
Motivation: Large Language Model based multi-agent systems (MAS) excel at collaborative problem solving but remain brittle to cascading errors: a single faulty step can propagate across agents and disrupt the trajectory.

Method: MASC rethinks detection as history-conditioned anomaly scoring via two complementary designs: (1) Next-Execution Reconstruction, which predicts the embedding of the next step from the query and interaction history to capture causal consistency, and (2) Prototype-Guided Enhancement, which learns a prototype prior over normal-step embeddings and uses it to stabilize reconstruction and anomaly scoring under sparse context (e.g., early steps). When an anomaly step is flagged, MASC triggers a correction agent to revise the acting agent's output before information flows downstream.

Result: MASC consistently outperforms all baselines, improving step-level error detection by up to 8.47% AUC-ROC

Conclusion: metacognitive monitoring and targeted correction can mitigate error propagation with minimal overhead.

Abstract: Large Language Model based multi-agent systems (MAS) excel at collaborative
problem solving but remain brittle to cascading errors: a single faulty step
can propagate across agents and disrupt the trajectory. In this paper, we
present MASC, a metacognitive framework that endows MAS with real-time,
unsupervised, step-level error detection and self-correction. MASC rethinks
detection as history-conditioned anomaly scoring via two complementary designs:
(1) Next-Execution Reconstruction, which predicts the embedding of the next
step from the query and interaction history to capture causal consistency, and
(2) Prototype-Guided Enhancement, which learns a prototype prior over
normal-step embeddings and uses it to stabilize reconstruction and anomaly
scoring under sparse context (e.g., early steps). When an anomaly step is
flagged, MASC triggers a correction agent to revise the acting agent's output
before information flows downstream. On the Who&When benchmark, MASC
consistently outperforms all baselines, improving step-level error detection by
up to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers
consistent end-to-end gains across architectures, confirming that our
metacognitive monitoring and targeted correction can mitigate error propagation
with minimal overhead.

</details>


### [116] [AI for Service: Proactive Assistance with AI Glasses](https://arxiv.org/abs/2510.14359)
*Zichen Wen,Yiyu Wang,Chenfei Liao,Boxue Yang,Junxian Li,Weifeng Liu,Haocong He,Bolong Feng,Xuyang Liu,Yuanhuiyi Lyu,Xu Zheng,Xuming Hu,Linfeng Zhang*

Main category: cs.AI

TL;DR: 提出了一种新的AI范式，AI4Service，旨在实现主动和实时的日常协助。


<details>
  <summary>Details</summary>
Motivation: 现有AI服务主要为被动响应用户指令，缺乏主动性和预测用户需求的能力。

Method: 提出了Alpha-Service框架，包含输入单元、中央处理单元、算术逻辑单元、记忆单元和输出单元五个关键组件，并通过AI眼镜上的多智能体系统实现。

Result: 通过实时Blackjack顾问、博物馆导游和购物助手等案例研究，验证了Alpha-Service在无需明确提示下感知环境、推断用户意图并提供及时有效帮助的能力。

Conclusion: Alpha-Service框架能够无缝地感知环境，推断用户意图，并提供及时和有用的帮助，无需明确的提示。

Abstract: In an era where AI is evolving from a passive tool into an active and
adaptive companion, we introduce AI for Service (AI4Service), a new paradigm
that enables proactive and real-time assistance in daily life. Existing AI
services remain largely reactive, responding only to explicit user commands. We
argue that a truly intelligent and helpful assistant should be capable of
anticipating user needs and taking actions proactively when appropriate. To
realize this vision, we propose Alpha-Service, a unified framework that
addresses two fundamental challenges: Know When to intervene by detecting
service opportunities from egocentric video streams, and Know How to provide
both generalized and personalized services. Inspired by the von Neumann
computer architecture and based on AI glasses, Alpha-Service consists of five
key components: an Input Unit for perception, a Central Processing Unit for
task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit
for long-term personalization, and an Output Unit for natural human
interaction. As an initial exploration, we implement Alpha-Service through a
multi-agent system deployed on AI glasses. Case studies, including a real-time
Blackjack advisor, a museum tour guide, and a shopping fit assistant,
demonstrate its ability to seamlessly perceive the environment, infer user
intent, and provide timely and useful assistance without explicit prompts.

</details>


### [117] [TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence](https://arxiv.org/abs/2510.14670)
*Marco Simoni,Aleksandar Fontana,Andrea Saracino,Paolo Mori*

Main category: cs.AI

TL;DR: TITAN is a framework that connects natural-language cyber threat queries with executable reasoning over a structured knowledge graph.


<details>
  <summary>Details</summary>
Motivation: The paper introduces TITAN to address the need for connecting natural language cyber threat queries with executable reasoning.

Method: It integrates a path planner model and a graph executor that traverses the TITAN Ontology.

Result: Empirical evaluations show that TITAN enables models to generate syntactically valid and semantically coherent reasoning paths.

Conclusion: TITAN enables models to generate syntactically valid and semantically coherent reasoning paths that can be deterministically executed on the underlying graph.

Abstract: TITAN (Threat Intelligence Through Automated Navigation) is a framework that
connects natural-language cyber threat queries with executable reasoning over a
structured knowledge graph. It integrates a path planner model, which predicts
logical relation chains from text, and a graph executor that traverses the
TITAN Ontology to retrieve factual answers and supporting evidence. Unlike
traditional retrieval systems, TITAN operates on a typed, bidirectional graph
derived from MITRE, allowing reasoning to move clearly and reversibly between
threats, behaviors, and defenses. To support training and evaluation, we
introduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test:
13951) pairing natural language questions with executable reasoning paths and
step by step Chain of Thought explanations. Empirical evaluations show that
TITAN enables models to generate syntactically valid and semantically coherent
reasoning paths that can be deterministically executed on the underlying graph.

</details>


### [118] [Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?](https://arxiv.org/abs/2510.14387)
*Yijie Hu,Zihao Zhou,Kaizhu Huang,Xiaowei Huang,Qiufeng Wang*

Main category: cs.AI

TL;DR: 提出了一种无需调整即可将数学LLM的数学推理能力迁移到多模态LLM (MLLM) 的方法，称为IP-Merging。


<details>
  <summary>Details</summary>
Motivation: 多模态LLM (MLLM) 的数学推理能力落后于LLM。研究表明，直接将数学LLM的知识迁移到MLLM存在参数空间差距的问题。

Method: 提出了IP-Merging方法，该方法首先识别MLLM和数学LLM中与推理相关的参数，然后将它们投影到MLLM的子空间中，以保持对齐，最后合并该子空间中的参数。

Result: IP-Merging方法可以直接增强MLLM的数学推理能力，且不会影响其其他能力。

Conclusion: IP-Merging是一种有效的将数学LLM的数学推理能力迁移到MLLM的方法。

Abstract: Math reasoning has been one crucial ability of large language models (LLMs),
where significant advancements have been achieved in recent years. However,
most efforts focus on LLMs by curating high-quality annotation data and
intricate training (or inference) paradigms, while the math reasoning
performance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM
typically consists of an LLM and a vision block, we wonder: Can MLLMs directly
absorb math reasoning abilities from off-the-shelf math LLMs without tuning?
Recent model-merging approaches may offer insights into this question. However,
they overlook the alignment between the MLLM and LLM, where we find that there
is a large gap between their parameter spaces, resulting in lower performance.
Our empirical evidence reveals two key factors behind this issue: the
identification of crucial reasoning-associated layers in the model and the
mitigation of the gaps in parameter space. Based on the empirical insights, we
propose IP-Merging that first identifies the reasoning-associated parameters in
both MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to
maintain the alignment, and finally merges parameters in this subspace.
IP-Merging is a tuning-free approach since parameters are directly adjusted.
Extensive experiments demonstrate that our IP-Merging method can enhance the
math reasoning ability of MLLMs directly from Math LLMs without compromising
their other capabilities.

</details>


### [119] [Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control](https://arxiv.org/abs/2510.14388)
*Zhe Wu,Hongjin Lu,Junliang Xing,Changhao Zhang,Yin Zhu,Yuhao Yang,Yuheng Jing,Kai Li,Kun Shao,Jianye Hao,Jun Wang,Yuanchun Shi*

Main category: cs.AI

TL;DR: Hi-Agent: a hierarchical vision-language agent for mobile control, achieving SOTA results on Android-in-the-Wild benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing approaches rely on direct state-to-action mappings, which lack structured reasoning and planning, and thus generalize poorly to novel tasks or unseen UI layouts.

Method: A trainable hierarchical vision-language agent with a high-level reasoning model and a low-level action model, jointly optimized with a foresight advantage function.

Result: Achieves a new SOTA 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark, significantly outperforming prior methods. Demonstrates competitive zero-shot generalization on the ScreenSpot-v2 benchmark and scales effectively on the AndroidWorld benchmark.

Conclusion: Hi-Agent shows strong adaptability in high-complexity mobile control scenarios.

Abstract: Building agents that autonomously operate mobile devices has attracted
increasing attention. While Vision-Language Models (VLMs) show promise, most
existing approaches rely on direct state-to-action mappings, which lack
structured reasoning and planning, and thus generalize poorly to novel tasks or
unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical
vision-language agent for mobile control, featuring a high-level reasoning
model and a low-level action model that are jointly optimized. For efficient
training, we reformulate multi-step decision-making as a sequence of
single-step subgoals and propose a foresight advantage function, which
leverages execution feedback from the low-level model to guide high-level
optimization. This design alleviates the path explosion issue encountered by
Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables
stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art
(SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark,
significantly outperforming prior methods across three paradigms: prompt-based
(AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement
learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot
generalization on the ScreenSpot-v2 benchmark. On the more challenging
AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones,
showing strong adaptability in high-complexity mobile control scenarios.

</details>


### [120] [IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning](https://arxiv.org/abs/2510.14406)
*Xikai Zhang,Bo Wang,Likang Xiao,Yongzhi Li,Quan Chen,Wenju Wu,Liu Liu*

Main category: cs.AI

TL;DR: 提出了IMAGINE框架，将多智能体系统(MAS)集成到单个模型中，以解决LLM在复杂推理和规划方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理和规划方面面临挑战，且多智能体系统存在推理成本高、延迟长和难以端到端训练的问题。

Method: 提出了IMAGINE框架，通过端到端训练将MAS的推理和规划能力集成到单个模型中。

Result: 使用Qwen3-8B-Instruct作为基础模型，在TravelPlanner基准测试中达到了82.7%的Final Pass Rate，超过了DeepSeek-R1-671B的40%。

Conclusion: IMAGINE框架能够使小规模模型获得结构化的推理和规划能力，并显著超越MAS的性能。

Abstract: Although large language models (LLMs) have made significant strides across
various tasks, they still face significant challenges in complex reasoning and
planning. For example, even with carefully designed prompts and prior
information explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on
the TravelPlanner dataset in the sole-planning mode. Similarly, even in the
thinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass
Rates of 5.9% and 40%, respectively. Although well-organized Multi-Agent
Systems (MAS) can offer improved collective reasoning, they often suffer from
high reasoning costs due to multi-round internal interactions, long
per-response latency, and difficulties in end-to-end training. To address these
challenges, we propose a general and scalable framework called IMAGINE, short
for Integrating Multi-Agent System into One Model. This framework not only
integrates the reasoning and planning capabilities of MAS into a single,
compact model, but also significantly surpass the capabilities of the MAS
through a simple end-to-end training. Through this pipeline, a single
small-scale model is not only able to acquire the structured reasoning and
planning capabilities of a well-organized MAS but can also significantly
outperform it. Experimental results demonstrate that, when using
Qwen3-8B-Instruct as the base model and training it with our method, the model
achieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding
the 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.

</details>


### [121] [Eliminating Negative Occurrences of Derived Predicates from PDDL Axioms](https://arxiv.org/abs/2510.14412)
*Claudia Grundke,Gabriele Röger*

Main category: cs.AI

TL;DR: 本文介绍了一种转换方法，用于消除 PDDL 中由公理推导出的谓词的否定形式。


<details>
  <summary>Details</summary>
Motivation: PDDL 标准对公理主体中出现的谓词的否定形式有所限制，本文旨在解除这一限制。

Method: 提出了一种转换方法。

Result: 该转换方法能够消除 PDDL 中由公理推导出的谓词的否定形式。

Conclusion: 解除 PDDL 标准中对公理的限制是可行的。

Abstract: Axioms are a feature of the Planning Domain Definition Language PDDL that can
be considered as a generalization of database query languages such as Datalog.
The PDDL standard restricts negative occurrences of predicates in axiom bodies
to predicates that are directly set by actions and not derived by axioms. In
the literature, authors often deviate from this limitation and only require
that the set of axioms is stratifiable. Both variants can express exactly the
same queries as least fixed-point logic, indicating that negative occurrences
of derived predicates can be eliminated. We present the corresponding
transformation.

</details>


### [122] [Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration](https://arxiv.org/abs/2510.14512)
*Haoyuan Li,Mathias Funk,Aaqib Saeed*

Main category: cs.AI

TL;DR: Helmsman是一个多智能体系统，可以自动合成联邦学习系统。


<details>
  <summary>Details</summary>
Motivation: 联邦学习系统设计和部署复杂，需要选择、组合和调整策略来应对数据异构和系统约束等多方面挑战。

Method: Helmsman通过三个协作阶段模拟研究和开发工作流程：(1)交互式人机循环规划，以制定合理的研究计划；(2)由受监督的智能体团队进行模块化代码生成；(3)在沙盒模拟环境中进行自主评估和改进的闭环。

Result: Helmsman生成的解决方案与现有的手工基线相比具有竞争力，并且通常优于它们。

Conclusion: 这项工作朝着复杂分散式人工智能系统的自动化工程迈出了重要一步。

Abstract: Federated Learning (FL) offers a powerful paradigm for training models on
decentralized data, but its promise is often undermined by the immense
complexity of designing and deploying robust systems. The need to select,
combine, and tune strategies for multifaceted challenges like data
heterogeneity and system constraints has become a critical bottleneck,
resulting in brittle, bespoke solutions. To address this, we introduce
Helmsman, a novel multi-agent system that automates the end-to-end synthesis of
federated learning systems from high-level user specifications. It emulates a
principled research and development workflow through three collaborative
phases: (1) interactive human-in-the-loop planning to formulate a sound
research plan, (2) modular code generation by supervised agent teams, and (3) a
closed-loop of autonomous evaluation and refinement in a sandboxed simulation
environment. To facilitate rigorous evaluation, we also introduce
AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess
the system-level generation capabilities of agentic systems in FL. Extensive
experiments demonstrate that our approach generates solutions competitive with,
and often superior to, established hand-crafted baselines. Our work represents
a significant step towards the automated engineering of complex decentralized
AI systems.

</details>


### [123] [JSPLIT: A Taxonomy-based Solution for Prompt Bloating in Model Context Protocol](https://arxiv.org/abs/2510.14537)
*Emanuele Antonioni,Stefan Markovic,Anirudha Shankar,Jaime Bernardo,Lovro Markovic,Silvia Pareti,Benedetto Proietti*

Main category: cs.AI

TL;DR: JSPLIT框架通过分层分类工具并根据用户提示选择相关工具，有效减小了提示大小，提高了代理在复杂环境中的工具选择准确性，同时降低了成本并提高了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前应用需要LLM与外部工具交互，导致提示变得冗长，增加了成本、延迟并降低了任务成功率。

Method: 提出了一种名为JSPLIT的框架，该框架使用分类法驱动的方法来更有效地管理提示大小，对工具进行分层分类，并根据用户提示识别和包含最相关的工具。

Result: JSPLIT显著减小了提示大小，并且在不显著影响代理有效响应能力的情况下，提高了工具选择的准确性。

Conclusion: JSPLIT框架通过有效管理提示大小，降低了成本，并在高复杂度代理环境中提高了任务成功率。

Abstract: AI systems are continually evolving and advancing, and user expectations are
concurrently increasing, with a growing demand for interactions that go beyond
simple text-based interaction with Large Language Models (LLMs). Today's
applications often require LLMs to interact with external tools, marking a
shift toward more complex agentic systems. To support this, standards such as
the Model Context Protocol (MCP) have emerged, enabling agents to access tools
by including a specification of the capabilities of each tool within the
prompt. Although this approach expands what agents can do, it also introduces a
growing problem: prompt bloating. As the number of tools increases, the prompts
become longer, leading to high prompt token costs, increased latency, and
reduced task success resulting from the selection of tools irrelevant to the
prompt. To address this issue, we introduce JSPLIT, a taxonomy-driven framework
designed to help agents manage prompt size more effectively when using large
sets of MCP tools. JSPLIT organizes the tools into a hierarchical taxonomy and
uses the user's prompt to identify and include only the most relevant tools,
based on both the query and the taxonomy structure. In this paper, we describe
the design of the taxonomy, the tool selection algorithm, and the dataset used
to evaluate JSPLIT. Our results show that JSPLIT significantly reduces prompt
size without significantly compromising the agent's ability to respond
effectively. As the number of available tools for the agent grows
substantially, JSPLIT even improves the tool selection accuracy of the agent,
effectively reducing costs while simultaneously improving task success in
high-complexity agent environments.

</details>


### [124] [Symbol Grounding in Neuro-Symbolic AI: A Gentle Introduction to Reasoning Shortcuts](https://arxiv.org/abs/2510.14538)
*Emanuele Marconato,Samuele Bortolotti,Emile van Krieken,Paolo Morettin,Elena Umili,Antonio Vergari,Efthymia Tsamoura,Andrea Passerini,Stefano Teso*

Main category: cs.AI

TL;DR: 本文旨在对神经符号（NeSy）AI中的推理捷径（RSs）进行综述，旨在提高NeSy AI模型的可信度。


<details>
  <summary>Details</summary>
Motivation: 神经符号AI结合了神经网络和符号推理，但容易受到推理捷径的影响，从而影响模型的可解释性、泛化性和可靠性。现有文献分散，难以理解和解决。

Method: 本文对推理捷径进行了介绍，讨论了其原因和后果，回顾了现有的理论表征，并详细介绍了处理推理捷径的方法，包括缓解和感知策略。

Result: 本文旨在以易于理解的形式重新构建高级材料，以统一的视角看待推理捷径，从而降低解决它们的门槛。

Conclusion: 本文旨在为可靠的NeSy和值得信赖的AI模型的发展做出贡献。

Abstract: Neuro-symbolic (NeSy) AI aims to develop deep neural networks whose
predictions comply with prior knowledge encoding, e.g. safety or structural
constraints. As such, it represents one of the most promising avenues for
reliable and trustworthy AI. The core idea behind NeSy AI is to combine neural
and symbolic steps: neural networks are typically responsible for mapping
low-level inputs into high-level symbolic concepts, while symbolic reasoning
infers predictions compatible with the extracted concepts and the prior
knowledge. Despite their promise, it was recently shown that - whenever the
concepts are not supervised directly - NeSy models can be affected by Reasoning
Shortcuts (RSs). That is, they can achieve high label accuracy by grounding the
concepts incorrectly. RSs can compromise the interpretability of the model's
explanations, performance in out-of-distribution scenarios, and therefore
reliability. At the same time, RSs are difficult to detect and prevent unless
concept supervision is available, which is typically not the case. However, the
literature on RSs is scattered, making it difficult for researchers and
practitioners to understand and tackle this challenging problem. This overview
addresses this issue by providing a gentle introduction to RSs, discussing
their causes and consequences in intuitive terms. It also reviews and
elucidates existing theoretical characterizations of this phenomenon. Finally,
it details methods for dealing with RSs, including mitigation and awareness
strategies, and maps their benefits and limitations. By reformulating advanced
material in a digestible form, this overview aims to provide a unifying
perspective on RSs to lower the bar to entry for tackling them. Ultimately, we
hope this overview contributes to the development of reliable NeSy and
trustworthy AI models.

</details>


### [125] [LLM Agents Beyond Utility: An Open-Ended Perspective](https://arxiv.org/abs/2510.14548)
*Asen Nachkov,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: 大型语言模型智能体利用思维链推理和函数调用，但它们能否像独立的实体一样规划、设计任务并朝着更广泛的目标推理？


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型是否可以像独立的实体一样自主规划和实现目标。

Method: 通过开放实验环境，增强预训练的LLM智能体，使其能够生成任务、积累知识并与环境交互。

Result: 该智能体可以可靠地执行复杂的多步骤指令，跨运行存储和重用信息，并提出和解决自己的任务。但对prompt设计敏感，容易重复生成任务，无法形成自我认知。

Conclusion: 这些发现展示了预训练的LLM在实现开放性方面的潜力和局限性，并为训练智能体管理记忆、高效探索和追求抽象的长期目标指明了未来方向。

Abstract: Recent LLM agents have made great use of chain of thought reasoning and
function calling. As their capabilities grow, an important question arises: can
this software represent not only a smart problem-solving tool, but an entity in
its own right, that can plan, design immediate tasks, and reason toward
broader, more ambiguous goals? To study this question, we adopt an open-ended
experimental setting where we augment a pretrained LLM agent with the ability
to generate its own tasks, accumulate knowledge, and interact extensively with
its environment. We study the resulting open-ended agent qualitatively. It can
reliably follow complex multi-step instructions, store and reuse information
across runs, and propose and solve its own tasks, though it remains sensitive
to prompt design, prone to repetitive task generation, and unable to form
self-representations. These findings illustrate both the promise and current
limits of adapting pretrained LLMs toward open-endedness, and point to future
directions for training agents to manage memory, explore productively, and
pursue abstract long-term goals.

</details>


### [126] [ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks](https://arxiv.org/abs/2510.14621)
*Yuanyi Song,Heyuan Huang,Qiqiang Lin,Yin Zhao,Xiangmou Qu,Jun Wang,Xingyu Lou,Weiwen Liu,Zhuosheng Zhang,Jun Wang,Yong Yu,Weinan Zhang,Zhaoxiang Wang*

Main category: cs.AI

TL;DR: 提出了一个图结构基准框架ColorBench，用于评估移动代理在复杂、长程任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的移动代理评估标准（离线静态基准和在线动态测试）无法全面评估代理能力，因为真实世界的移动任务复杂且允许多个有效解决方案。

Method: 通过对真实设备交互中观察到的有限状态进行建模，实现动态行为的静态模拟。构建了一个专注于复杂长程任务的基准测试ColorBench。

Result: ColorBench包含175个任务，每个任务包含至少两个正确路径和几个典型错误路径，实现了准动态交互。通过评估各种基线，发现了现有模型的局限性，并提出了改进方向和可行的技术途径。

Conclusion: ColorBench可以弥合离线和在线评估之间的差距，增强测试稳定性，并为复杂、长程问题上提升代理的性能提供方向。

Abstract: The rapid advancement of multimodal large language models has enabled agents
to operate mobile devices by directly interacting with graphical user
interfaces, opening new possibilities for mobile automation. However,
real-world mobile tasks are often complex and allow for multiple valid
solutions. This contradicts current mobile agent evaluation standards: offline
static benchmarks can only validate a single predefined "golden path", while
online dynamic testing is constrained by the complexity and non-reproducibility
of real devices, making both approaches inadequate for comprehensively
assessing agent capabilities. To bridge the gap between offline and online
evaluation and enhance testing stability, this paper introduces a novel
graph-structured benchmarking framework. By modeling the finite states observed
during real-device interactions, it achieves static simulation of dynamic
behaviors. Building on this, we develop ColorBench, a benchmark focused on
complex long-horizon tasks. It supports evaluation of multiple valid solutions,
subtask completion rate statistics, and atomic-level capability analysis.
ColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average
length of over 13 steps. Each task includes at least two correct paths and
several typical error paths, enabling quasi-dynamic interaction. By evaluating
ColorBench across various baselines, we discover limitations of existing models
and propose improvement directions and feasible technical pathways to enhance
agents' performance on complex, long-horizon problems based on experimental
results. Code and data are available at:
https://github.com/MadeAgents/ColorBench.

</details>


### [127] [Beyond Hallucinations: The Illusion of Understanding in Large Language Models](https://arxiv.org/abs/2510.14665)
*Rikard Rosenbacke,Carl Rosenbacke,Victor Rosenbacke,Martin McKee*

Main category: cs.AI

TL;DR: 大型语言模型 (LLM) 越来越深入地嵌入到人类的沟通和决策中，但它们继承了语言本身固有的模糊性、偏见和缺乏对真理的直接访问。本文认为，LLM 以规模化的方式运作系统 1 认知：快速、联想和有说服力，但没有反思或证伪。为了解决这个问题，我们引入了 Rose-Frame，这是一个三维框架，用于诊断人机交互中的认知和认知漂移。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 正在变得深入地嵌入到人类的沟通和决策中，但它们继承了语言本身固有的模糊性、偏见和缺乏对真理的直接访问。虽然它们的输出流畅、情感共鸣和连贯，但它们是通过统计预测而不是有根据的推理产生的。这会造成幻觉的风险，即听起来令人信服但缺乏事实有效性的反应。

Method: 本文介绍了 Rose-Frame，这是一个三维框架，用于诊断人机交互中的认知和认知漂移。三个轴是：(i) 地图与领域，它区分了现实的表示（认识论）与现实本身（本体论）；(ii) 直觉与理性，借鉴双重过程理论来区分快速、情绪化的判断与缓慢、反思性的思维；(iii) 冲突与确认，它检查思想是否通过分歧进行批判性测试，或者只是通过相互验证来加强。

Result: Rose-Frame 不试图用更多的数据或规则来修复 LLM。相反，它提供了一种反思工具，使模型的局限性和用户的假设都可见，从而实现更透明和具有批判意识的 AI 部署。

Conclusion: 对齐被重新定义为认知治理：直觉，无论是人类的还是人工智能的，都必须继续受到人类理性的支配。只有通过嵌入反思性的、可证伪的监督，我们才能使机器的流畅性与人类的理解对齐。

Abstract: Large language models (LLMs) are becoming deeply embedded in human
communication and decision-making, yet they inherit the ambiguity, bias, and
lack of direct access to truth inherent in language itself. While their outputs
are fluent, emotionally resonant, and coherent, they are generated through
statistical prediction rather than grounded reasoning. This creates the risk of
hallucination, responses that sound convincing but lack factual validity.
Building on Geoffrey Hinton's observation that AI mirrors human intuition
rather than reasoning, this paper argues that LLMs operationalize System 1
cognition at scale: fast, associative, and persuasive, but without reflection
or falsification. To address this, we introduce the Rose-Frame, a
three-dimensional framework for diagnosing cognitive and epistemic drift in
human-AI interaction. The three axes are: (i) Map vs. Territory, which
distinguishes representations of reality (epistemology) from reality itself
(ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to
separate fast, emotional judgments from slow, reflective thinking; and (iii)
Conflict vs. Confirmation, which examines whether ideas are critically tested
through disagreement or simply reinforced through mutual validation. Each
dimension captures a distinct failure mode, and their combination amplifies
misalignment. Rose-Frame does not attempt to fix LLMs with more data or rules.
Instead, it offers a reflective tool that makes both the model's limitations
and the user's assumptions visible, enabling more transparent and critically
aware AI deployment. It reframes alignment as cognitive governance: intuition,
whether human or artificial, must remain governed by human reason. Only by
embedding reflective, falsifiable oversight can we align machine fluency with
human understanding.

</details>


### [128] [Machine Learning and Public Health: Identifying and Mitigating Algorithmic Bias through a Systematic Review](https://arxiv.org/abs/2510.14669)
*Sara Altamirano,Arjan Vreeken,Sennay Ghebreab*

Main category: cs.AI

TL;DR: 该研究对荷兰公共卫生机器学习研究中算法偏差的识别、讨论和报告进行了系统的文献综述，揭示了普遍存在的差距，并提出了一个面向公平的框架ACAR。


<details>
  <summary>Details</summary>
Motivation: 机器学习有潜力通过改进监测、风险分层和资源分配来彻底改变公共卫生，但算法偏差可能会无意中加剧现有的健康差距。

Method: 该研究开发了算法偏差风险评估工具(RABAT)，并将其应用于35项同行评审的研究。

Result: 分析揭示了普遍存在的差距：虽然数据抽样和缺失数据实践有很好的记录，但大多数研究忽略了明确的公平框架、亚组分析和对潜在危害的透明讨论。

Conclusion: 该研究提出了针对公共卫生机器学习从业者的可行性建议，以持续考虑算法偏差并提高透明度，确保算法创新能够促进健康公平，而不是破坏它。

Abstract: Machine learning (ML) promises to revolutionize public health through
improved surveillance, risk stratification, and resource allocation. However,
without systematic attention to algorithmic bias, ML may inadvertently
reinforce existing health disparities. We present a systematic literature
review of algorithmic bias identification, discussion, and reporting in Dutch
public health ML research from 2021 to 2025. To this end, we developed the Risk
of Algorithmic Bias Assessment Tool (RABAT) by integrating elements from
established frameworks (Cochrane Risk of Bias, PROBAST, Microsoft Responsible
AI checklist) and applied it to 35 peer-reviewed studies. Our analysis reveals
pervasive gaps: although data sampling and missing data practices are well
documented, most studies omit explicit fairness framing, subgroup analyses, and
transparent discussion of potential harms. In response, we introduce a
four-stage fairness-oriented framework called ACAR (Awareness,
Conceptualization, Application, Reporting), with guiding questions derived from
our systematic literature review to help researchers address fairness across
the ML lifecycle. We conclude with actionable recommendations for public health
ML practitioners to consistently consider algorithmic bias and foster
transparency, ensuring that algorithmic innovations advance health equity
rather than undermine it.

</details>


### [129] [NAEL: Non-Anthropocentric Ethical Logic](https://arxiv.org/abs/2510.14676)
*Bianca Maria Lerma,Rafael Peñaloza*

Main category: cs.AI

TL;DR: 提出了一种新的非人类中心的人工智能伦理框架，该框架基于主动推理和符号推理。


<details>
  <summary>Details</summary>
Motivation: 传统的AI伦理方法以人类为中心，存在局限性。NAEL旨在打破这些局限性。

Method: NAEL形式化了伦理行为，将其视为智能系统在动态多智能体环境中最小化全局预期自由能时涌现的属性。提出了一种神经符号架构，使agent能够在不确定的环境中评估其行为的伦理后果。

Result: 通过伦理资源分配的案例研究，说明了NAEL在自我保护、认知学习和集体福利之间的动态平衡。

Conclusion: NAEL允许agent在不预设拟人道德直觉的情况下，发展context敏感的、适应性的和关系性的伦理行为。

Abstract: We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical
framework for artificial agents grounded in active inference and symbolic
reasoning. Departing from conventional, human-centred approaches to AI ethics,
NAEL formalizes ethical behaviour as an emergent property of intelligent
systems minimizing global expected free energy in dynamic, multi-agent
environments. We propose a neuro-symbolic architecture to allow agents to
evaluate the ethical consequences of their actions in uncertain settings. The
proposed system addresses the limitations of existing ethical models by
allowing agents to develop context-sensitive, adaptive, and relational ethical
behaviour without presupposing anthropomorphic moral intuitions. A case study
involving ethical resource distribution illustrates NAEL's dynamic balancing of
self-preservation, epistemic learning, and collective welfare.

</details>


### [130] [Practical, Utilitarian Algorithm Configuration](https://arxiv.org/abs/2510.14683)
*Devon Graham,Kevin Leyton-Brown*

Main category: cs.AI

TL;DR: 本文改进了COUP算法，使其在具有理论保证的同时，在实际性能上与常用的启发式配置程序竞争。


<details>
  <summary>Details</summary>
Motivation: 用户效用函数可以灵活地捕捉用户对算法运行时的偏好，但现有的COUP算法注重理论保证而忽略了实际性能。

Method: 通过一系列改进来提升COUP算法的实际性能，同时不降低其理论保证。

Result: 实验证明了改进的COUP算法的有效性。

Conclusion: 本文将具有理论保证的功利主义算法配置推进到与广泛使用的启发式配置程序竞争的水平。

Abstract: Utilitarian algorithm configuration identifies a parameter setting for a
given algorithm that maximizes a user's utility. Utility functions offer a
theoretically well-grounded approach to optimizing decision-making under
uncertainty and are flexible enough to capture a user's preferences over
algorithm runtimes (e.g., they can describe a sharp cutoff after which a
solution is no longer required, a per-hour cost for compute, or diminishing
returns from algorithms that take longer to run). COUP is a recently-introduced
utilitarian algorithm configuration procedure which was designed mainly to
offer strong theoretical guarantees about the quality of the configuration it
returns, with less attention paid to its practical performance. This paper
closes that gap, bringing theoretically-grounded, utilitarian algorithm
configuration to the point where it is competitive with widely used, heuristic
configuration procedures that offer no performance guarantees. We present a
series of improvements to COUP that improve its empirical performance without
degrading its theoretical guarantees and demonstrate their benefit
experimentally. Using a case study, we also illustrate ways of exploring the
robustness of a given solution to the algorithm selection problem to variations
in the utility function.

</details>


### [131] [Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging](https://arxiv.org/abs/2510.14697)
*Bang An,Yibo Yang,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: 提出了一种名为PAVE的新方法，用于在知识感知子空间中提纯任务向量，以解决模型合并中因任务无关冗余导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 模型合并旨在将独立微调模型的任务特定能力整合到单个模型中，但现有方法因任务向量中的冗余信息导致性能下降。

Method: 通过对微调模型的权重进行面向上下文的奇异值分解，将权重分解为任务相关和冗余成分，并通过剪枝冗余成分来提纯任务向量。引入了一种频谱秩分配策略，以优化归一化激活剪枝误差。

Result: 实验证明，PAVE可以提高各种模型合并方法、任务和模型架构的性能。

Conclusion: PAVE作为一个即插即用的方案，可以有效地提高基于任务向量的合并方法的性能。

Abstract: Model merging aims to integrate task-specific abilities from individually
fine-tuned models into a single model without extra training. In recent model
merging methods, task vector has become a fundamental building block, as it can
encapsulate the residual information from finetuning. However, the merged model
often suffers from notable performance degradation due to the conflicts caused
by task-irrelevant redundancy in task vectors. Existing efforts in overcoming
redundancy by randomly dropping elements in the parameter space involves
randomness and lacks knowledge awareness. To address these challenges, in this
study, we propose Purifying TAsk Vectors (PAVE) in knowledge-aware subspace.
Concretely, we sample some training examples from each task, and feed them into
their corresponding fine-tuned models to acquire the covariance matrices before
linear layers. We then perform a context-oriented singular value decomposition,
which accentuates the weight components most relevant to the target knowledge.
As a result, we can split fine-tuned model weights into task-relevant and
redundant components in the knowledge-aware subspace, and purify the task
vector by pruning the redundant components. To induce fair pruning efforts
across models, we further introduce a spectral rank allocation strategy by
optimizing a normalized activated pruning error. The task vector purification
by our method as a plug-and-play scheme is applicable across various task
vector-based merging methods to improve their performance. In experiments, we
demonstrate the effectiveness of PAVE across a diverse set of merging methods,
tasks, and model architectures.

</details>


### [132] [Cognitive-Aligned Spatio-Temporal Large Language Models For Next Point-of-Interest Prediction](https://arxiv.org/abs/2510.14702)
*Penglong Zhai,Jie Li,Fanyi Di,Yue Liu,Yifang Yuan,Jie Huang,Peng Wu,Sicong Wang,Mingyang Yin,Tingting Hu,Yao Xu,Xin Li*

Main category: cs.AI

TL;DR: 提出了一个名为CoAST的框架，用于解决大型语言模型（LLM）在预测下一个兴趣点（POI）时，缺乏对地理实体和时空移动模式的理解，以及难以整合世界知识和人类认知的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在推荐系统中表现出潜力，但缺乏对地理和时空信息的理解，并且难以整合世界知识和人类认知。

Method: CoAST框架使用自然语言作为接口，包含两个阶段：通过在丰富的时空轨迹数据上进行持续预训练来获取推荐知识；以及通过监督微调（SFT）和强化学习（RL）来使认知判断与人类偏好对齐。

Result: 在各种真实世界数据集上的离线实验以及在高德地图App主页的“猜你去哪儿”中的在线实验表明，CoAST是有效的。

Conclusion: CoAST框架能够有效提升POI推荐的性能，并且能够整合世界知识和人类认知。

Abstract: The next point-of-interest (POI) recommendation task aims to predict the
users' immediate next destinations based on their preferences and historical
check-ins, holding significant value in location-based services. Recently,
large language models (LLMs) have shown great potential in recommender systems,
which treat the next POI prediction in a generative manner. However, these
LLMs, pretrained primarily on vast corpora of unstructured text, lack the
native understanding of structured geographical entities and sequential
mobility patterns required for next POI prediction tasks. Moreover, in
industrial-scale POI prediction applications, incorporating world knowledge and
alignment of human cognition, such as seasons, weather conditions, holidays,
and users' profiles (such as habits, occupation, and preferences), can enhance
the user experience while improving recommendation performance. To address
these issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a
framework employing natural language as an interface, allowing for the
incorporation of world knowledge, spatio-temporal trajectory patterns,
profiles, and situational information. Specifically, CoAST mainly comprises of
2 stages: (1) Recommendation Knowledge Acquisition through continued
pretraining on the enriched spatial-temporal trajectory data of the
desensitized users; (2) Cognitive Alignment to align cognitive judgments with
human preferences using enriched training data through Supervised Fine-Tuning
(SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline
experiments on various real-world datasets and online experiments deployed in
"Guess Where You Go" of AMAP App homepage demonstrate the effectiveness of
CoAST.

</details>


### [133] [ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling](https://arxiv.org/abs/2510.14703)
*Jianghao Lin,Yuanyuan Shi,Xin Peng,Renjie Ding,Hairui Wang,Yuxuan Peng,Bizhe Bai,Weixi Song,Fengshuo Bai,Huacan Chai,Weinan Zhang,Fei Huang,Ying Wen*

Main category: cs.AI

TL;DR: 本文提出了一种用于函数调用的推理扩展框架，该框架结合了细粒度束搜索和过程奖励模型 ToolPRM，该模型对每个函数调用的内部步骤进行评分。


<details>
  <summary>Details</summary>
Motivation: 当前关于推理扩展的研究主要集中在非结构化输出生成任务中，而其在结构化输出（如函数调用）中的应用在很大程度上尚未被探索。

Method: 该方法结合了细粒度束搜索与过程奖励模型 ToolPRM，并构建了第一个细粒度内部调用过程监督数据集，该数据集使用函数屏蔽技术自动注释，以为结构化工具使用推理提供步进式奖励。

Result: ToolPRM 在预测精度方面优于粗粒度和结果奖励模型，表明其在监督函数调用推理过程方面具有更强的能力。配备 ToolPRM 的推理扩展技术还可以显着提高各种函数调用任务和基准测试中的骨干模型性能。

Conclusion: 本文揭示了将推理扩展技术应用于结构化输出的关键原则：由于结构化函数调用生成的不可恢复特性，应“多探索，少保留”。

Abstract: Large language models (LLMs) are increasingly demonstrating strong
capabilities as autonomous agents, with function calling serving as a core
mechanism for interaction with the environment. Meanwhile, inference scaling
has become a cutting-edge technique to enhance LLM performance by allocating
more computational resources during the inference process. However, current
research on inference scaling primarily focuses on unstructured output
generation tasks, leaving its application in structured outputs, like function
calling, largely underexplored. To bridge this gap, we propose an inference
scaling framework that combines fine-grained beam search with a process reward
model, ToolPRM, which scores the internal steps of each single function call.
To train ToolPRM, we construct the first fine-grained intra-call process
supervision dataset, automatically annotated with function-masking techniques
to provide step-level rewards for structured tool-use reasoning. Extensive
experiments demonstrate that ToolPRM beats the coarse-grained and outcome
reward models in terms of predictive accuracy, indicating its stronger
capability in supervising the function calling inference process. Inference
scaling technique equipped with ToolPRM also significantly improves the
backbone model performance across various function calling tasks and
benchmarks. More importantly, we reveal a key principle for applying inference
scaling techniques to structured outputs: "explore more but retain less" due to
the unrecoverability characteristics of structured function calling generation.

</details>


### [134] [SimKO: Simple Pass@K Policy Optimization](https://arxiv.org/abs/2510.14807)
*Ruotian Peng,Yi Ren,Zhouliang Yu,Weiyang Liu,Yandong Wen*

Main category: cs.AI

TL;DR: RLVR方法在提高LLM的推理能力方面取得了进展，但存在重利用轻探索的系统性偏差。


<details>
  <summary>Details</summary>
Motivation: 分析现有RLVR方法中存在的重利用轻探索的系统性偏差，这种偏差导致pass@1性能提升，但pass@K (K>1) 性能下降。

Method: 通过跟踪token级别的概率分布来分析RLVR方法的训练动态，并提出Simple Pass@K Optimization (SimKO) 方法来缓解过度集中问题，从而鼓励探索。

Result: SimKO在各种数学和逻辑推理基准测试中，始终产生更高的pass@K，从而提供了一种改进RLVR探索的简单方法。

Conclusion: SimKO通过非对称方式，对验证正确的响应提高top-K候选者的概率，对验证不正确的响应，对top-1候选者施加更强的惩罚，尤其是在高熵token上应用时，能有效缓解过度集中问题。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has advanced the
reasoning capabilities of large language models (LLMs). However, prevailing
RLVR methods exhibit a systematic bias toward exploitation over exploration, as
evidenced by improved pass@1 but reduced pass@K (K>1) performance. To
understand this issue, we analyze training dynamics of RLVR methods by tracking
the token-level probability distributions over vocabulary candidates. Our
analysis reveals a consistent probability concentration effect where the top-1
candidate increasingly accumulates probability mass and suppresses that of
other candidates. More importantly, stronger over-concentration correlates with
worse pass@K performance. Inspired by this finding, we propose Simple Pass@K
Optimization (SimKO), a method designed to mitigate the over-concentration
issue, thereby encouraging exploration. SimKO operates in an asymmetrical
manner. For verified-correct responses, it boosts the probabilities of the
top-K candidates. For verified-incorrect responses, it applies stronger
penalties to the top-1 candidate. We observe that this asymmetric design is
particularly effective at mitigating over-concentration when applied at tokens
with high entropy. Across various math and logical-reasoning benchmarks, SimKO
consistently yields higher pass@K for a wide range of K, providing a simple way
to improve RLVR's exploration.

</details>


### [135] [Agentic NL2SQL to Reduce Computational Costs](https://arxiv.org/abs/2510.14808)
*Dominik Jehle,Lennart Purucker,Frank Hutter*

Main category: cs.AI

TL;DR: Datalake Agent通过交互式循环减少LLM使用的元信息，从而降低NL2SQL任务的成本。


<details>
  <summary>Details</summary>
Motivation: 使用LLM进行NL2SQL需要在大量SQL数据库上处理大量的元信息，导致prompt过长和处理成本高昂。

Method: Datalake Agent采用交互式循环，LLM在循环中使用推理框架，选择性地请求解决表格问答任务所需的信息，而不是直接使用包含所有元信息的prompt调用LLM。

Result: Datalake Agent将LLM使用的token减少了高达87％，从而大大降低了成本，同时保持了竞争性能。

Conclusion: Datalake Agent能够更有效地解决NL2SQL任务。

Abstract: Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)
has recently been empowered by large language models (LLMs). Using LLMs to
perform NL2SQL methods on a large collection of SQL databases necessitates
processing large quantities of meta-information about the databases, which in
turn results in lengthy prompts with many tokens and high processing costs. To
address this challenge, we introduce Datalake Agent, an agentic system designed
to enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing
direct solvers for NL2SQL that call the LLM once with all meta-information in
the prompt, the Datalake Agent employs an interactive loop to reduce the
utilized meta-information. Within the loop, the LLM is used in a reasoning
framework that selectively requests only the necessary information to solve a
table question answering task. We evaluate the Datalake Agent on a collection
of 23 databases with 100 table question answering tasks. The Datalake Agent
reduces the tokens used by the LLM by up to 87\% and thus allows for
substantial cost reductions while maintaining competitive performance.

</details>


### [136] [RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning](https://arxiv.org/abs/2510.14828)
*Jinrui Liu,Bingyan Nie,Boyu Li,Yaran Chen,Yuze Wang,Shunsen He,Haoran Li*

Main category: cs.AI

TL;DR: 本文提出了一种名为 RoboGPT-R1 的双阶段微调框架，用于提升具身智能体在复杂操作任务中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的大型语言模型和视觉语言模型在执行长程操作任务时，由于常识和推理能力的限制，面临挑战。

Method: 该框架首先通过监督训练获取基础知识，然后通过强化学习解决模型在视觉空间理解和推理方面的不足。同时，设计了一种基于规则的奖励函数，以实现物理理解和动作序列一致性。

Result: 在 EmbodiedBench 基准测试中，使用 Qwen2.5-VL-3B 训练的推理模型显著优于更大规模的模型 GPT-4o-mini 21.33%，并且超过了其他使用 Qwen2.5-VL-7B 训练的模型 20.33%。

Conclusion: RoboGPT-R1 框架能够有效提升具身智能体在复杂操作任务中的推理能力。

Abstract: Improving the reasoning capabilities of embodied agents is crucial for robots
to complete complex human instructions in long-view manipulation tasks
successfully. Despite the success of large language models and vision language
models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue
facing challenges in performing long-horizon manipulation tasks in complex
real-world environments, owing to their restricted common sense and reasoning
capabilities. Considering that aligning general-purpose vision language models
to robotic planning tasks via supervised fine-tuning suffers from poor
generalization and insufficient physical understanding, we propose RoboGPT-R1,
a two-stage fine-tuning framework for embodied planning. In this framework,
supervised training acquires foundational knowledge through expert sequences,
followed by RL to address the model's shortcomings in visual-spatial
understanding and reasoning. To achieve physical understanding and action
sequence consistency in multi-step reasoning tasks, we design a rule-based
reward function that simultaneously considers long-horizon performance and
action constraint in the environment. The reasoning model, trained on
Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,
by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the
EmbodiedBench benchmark.

</details>


### [137] [Boosting Instruction Following at Scale](https://arxiv.org/abs/2510.14842)
*Ben Elder,Evelyn Duesterwald,Vinod Muthusamy*

Main category: cs.AI

TL;DR: Instruction Boosting是一种提高LLM提示指令可靠性的后处理方法，通过增加指令遵循率来改进模型性能。


<details>
  <summary>Details</summary>
Motivation: 开发者通常通过操纵提示来影响LLM的行为，但简单地增加指令并不能保证它们会被遵循。论文旨在解决LLM提示指令的可靠性问题。

Method: 论文提出了Instruction Boosting方法，并通过SCALEDIF基准进行评估，该基准具有可扩展的指令数量。

Result: Instruction Boosting将两个指令的指令遵循率提高了7个点，将十个指令的指令遵循率提高了4个点。论文还分析了性能随指令增加而下降的趋势，并提出了冲突评分工具。

Conclusion: 指令之间的冲突程度是导致性能下降的重要因素。论文的冲突评分工具可以解释观察到的性能趋势，并为开发者提供关于额外提示指令对模型性能影响的反馈。

Abstract: A typical approach developers follow to influence an LLM's behavior in an
application is through careful manipulation of the prompt, such as by adding or
modifying instructions. However, merely adding more instructions provides
little assurance that they will actually be followed. We introduce Instruction
Boosting as a post-generation method to increase the reliability of LLM prompt
instructions. We show that Instruction Boosting improves the instruction
following rate by up to 7 points for two instructions and up to 4 points for
ten instructions. To demonstrate these results we introduce SCALEDIF, a
benchmark with a scaled instruction volume of up to ten instructions per data
sample. We also present an analysis of the commonly observed trend that
performance degrades as more instructions are added. We show that an important
factor contributing to this trend is the degree of tension and conflict that
arises as the number of instructions is increased. We contribute a quantitative
conflict scoring tool that explains the observed performance trends and
provides feedback to developers on the impact that additional prompt
instructions have on a model's performance.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [138] [Towards a Multimodal Stream Processing System](https://arxiv.org/abs/2510.14631)
*Uélison Jean Lopes dos Santos,Alessandro Ferri,Szilard Nistor,Riccardo Tommasini,Carsten Binnig,Manisha Luthra*

Main category: cs.DB

TL;DR: 这篇论文提出了一个将多模态大型语言模型嵌入到多模态流处理系统中的新方法，以实现跨多种模态的实时查询处理。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态查询数据库无法满足流处理系统的严格延迟和吞吐量要求。

Method: 该方法提出了逻辑、物理和语义查询转换等优化策略，以减少模型负载并提高吞吐量，同时保持准确性。

Result: 通过一个名为system的prototype，证明了该优化策略可以将性能提高一个数量级以上。

Conclusion: 论文讨论了一个研究路线图，概述了构建可扩展和高效的多模态流处理系统所面临的开放性研究挑战。

Abstract: In this paper, we present a vision for a new generation of multimodal
streaming systems that embed MLLMs as first-class operators, enabling real-time
query processing across multiple modalities. Achieving this is non-trivial:
while recent work has integrated MLLMs into databases for multimodal queries,
streaming systems require fundamentally different approaches due to their
strict latency and throughput requirements. Our approach proposes novel
optimizations at all levels, including logical, physical, and semantic query
transformations that reduce model load to improve throughput while preserving
accuracy. We demonstrate this with \system{}, a prototype leveraging such
optimizations to improve performance by more than an order of magnitude.
Moreover, we discuss a research roadmap that outlines open research challenges
for building a scalable and efficient multimodal stream processing systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [139] [FinAI Data Assistant: LLM-based Financial Database Query Processing with the OpenAI Function Calling API](https://arxiv.org/abs/2510.14162)
*Juhyeong Kim,Yejin Kim,Youngbin Lee,Hyunwoo Byun*

Main category: cs.IR

TL;DR: FinAI Data Assistant通过结合大型语言模型和 OpenAI Function Calling API，为金融数据库的自然语言查询提供了一种实用的方法，它不通过 text-to-SQL 合成完整的 SQL，而是将用户请求路由到一个小型、经过审查的参数化查询库，从而以生成灵活性换取可靠性、低延迟和成本效率。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于提高金融数据库查询的可靠性、降低延迟和成本，并避免完全依赖 text-to-SQL 方法。

Method: 该方法结合了大型语言模型和 OpenAI Function Calling API，使用小型、经过审查的参数化查询库来响应用户请求，并进行了受控实验，包括评估 LLM 单独回忆或推断时间相关金融数据的能力、LLM 将公司名称映射到股票代码的准确性，以及 function calling 相对于 text-to-SQL 的性能。

Result: 实验结果表明，仅使用 LLM 的预测存在不可忽略的误差，并且相对于模型知识截止日期，股票价格主要表现出前瞻性偏差。对于 NASDAQ-100 成分股，股票代码映射准确率接近完美，对于 S&P 500 公司也很高。FinAI Data Assistant 在任务套件上实现了比 text-to-SQL 基线更低的延迟和成本，以及更高的可靠性。

Conclusion: FinAI Data Assistant 是一种更可靠、更高效的金融数据库查询方法，通过权衡生成灵活性，实现了更好的性能。

Abstract: We present FinAI Data Assistant, a practical approach for natural-language
querying over financial databases that combines large language models (LLMs)
with the OpenAI Function Calling API. Rather than synthesizing complete SQL via
text-to-SQL, our system routes user requests to a small library of vetted,
parameterized queries, trading generative flexibility for reliability, low
latency, and cost efficiency. We empirically study three questions: (RQ1)
whether LLMs alone can reliably recall or extrapolate time-dependent financial
data without external retrieval; (RQ2) how well LLMs map company names to stock
ticker symbols; and (RQ3) whether function calling outperforms text-to-SQL for
end-to-end database query processing. Across controlled experiments on prices
and fundamentals, LLM-only predictions exhibit non-negligible error and show
look-ahead bias primarily for stock prices relative to model knowledge cutoffs.
Ticker-mapping accuracy is near-perfect for NASDAQ-100 constituents and high
for S\&P~500 firms. Finally, FinAI Data Assistant achieves lower latency and
cost and higher reliability than a text-to-SQL baseline on our task suite. We
discuss design trade-offs, limitations, and avenues for deployment.

</details>


### [140] [Large Scale Retrieval for the LinkedIn Feed using Causal Language Models](https://arxiv.org/abs/2510.14223)
*Sudarshan Srinivasa Ramanujam,Antonio Alonso,Saurabh Kataria,Siddharth Dangi,Akhilesh Gupta,Birjodh Singh Tiwana,Manas Somaiya,Luke Simon,David Byrne,Sojeong Ha,Sen Zhou,Andrei Akterskii,Zhanglong Liu,Samira Sriram,Crescent Xiong,Zhoutao Pei,Angela Shao,Alex Li,Annie Xiao,Caitlin Kolb,Thomas Kistler,Zach Moore,Hamed Firooz*

Main category: cs.IR

TL;DR: 本文提出了一种新颖的检索方法，该方法微调了一个大型因果语言模型（Meta的LLaMA 3），作为一个双编码器，仅使用文本输入为用户（成员）和内容（项目）生成高质量的嵌入。


<details>
  <summary>Details</summary>
Motivation: 在像LinkedIn Feed这样的大规模推荐系统中，检索阶段对于将数亿潜在候选项目缩小到可管理的子集以进行排序至关重要。LinkedIn的Feed提供来自成员网络外部的建议内容（基于成员的主题兴趣），其中从数亿候选项目的池中检索2000个候选项目，延迟预算为几毫秒，入站QPS为每秒数千个。

Method: 使用Meta的LLaMA 3，通过prompt设计和量化数值特征进行微调，生成高质量的用户和内容嵌入。

Result: 离线指标和在线A/B测试表明，会员参与度有了显著提高。我们观察到新会员的显著收益，他们通常缺乏强大的网络连接，这表明高质量的建议内容有助于保留。

Conclusion: 这项工作证明了生成语言模型如何有效地适应工业应用中的实时、高吞吐量检索。

Abstract: In large scale recommendation systems like the LinkedIn Feed, the retrieval
stage is critical for narrowing hundreds of millions of potential candidates to
a manageable subset for ranking. LinkedIn's Feed serves suggested content from
outside of the member's network (based on the member's topical interests),
where 2000 candidates are retrieved from a pool of hundreds of millions
candidate with a latency budget of a few milliseconds and inbound QPS of
several thousand per second. This paper presents a novel retrieval approach
that fine-tunes a large causal language model (Meta's LLaMA 3) as a dual
encoder to generate high quality embeddings for both users (members) and
content (items), using only textual input. We describe the end to end pipeline,
including prompt design for embedding generation, techniques for fine-tuning at
LinkedIn's scale, and infrastructure for low latency, cost effective online
serving. We share our findings on how quantizing numerical features in the
prompt enables the information to get properly encoded in the embedding,
facilitating greater alignment between the retrieval and ranking layer. The
system was evaluated using offline metrics and an online A/B test, which showed
substantial improvements in member engagement. We observed significant gains
among newer members, who often lack strong network connections, indicating that
high-quality suggested content aids retention. This work demonstrates how
generative language models can be effectively adapted for real time, high
throughput retrieval in industrial applications.

</details>


### [141] [Synergistic Integration and Discrepancy Resolution of Contextualized Knowledge for Personalized Recommendation](https://arxiv.org/abs/2510.14257)
*Lingyu Mu,Hao Deng,Haibo Xing,Kaican Lin,Zhitong Zhu,Yu Zhang,Xiaoyi Zeng,Zhengxiao Liu,Zheng Lin,Jinxin Hu*

Main category: cs.IR

TL;DR: CoCo: An end-to-end framework dynamically constructs user-specific contextual knowledge embeddings through a dual-mechanism approach.


<details>
  <summary>Details</summary>
Motivation: Current methodologies that adopt static schema-based prompting mechanisms encounter significant limitations: (1) they employ universal template structures that neglect the multi-faceted nature of user preference diversity; (2) they implement superficial alignment between semantic knowledge representations and behavioral feature spaces without achieving comprehensive latent space integration.

Method: CoCo, an end-to-end framework that dynamically constructs user-specific contextual knowledge embeddings through a dual-mechanism approach. Our method realizes profound integration of semantic and behavioral latent dimensions via adaptive knowledge fusion and contradiction resolution modules.

Result: achieving a maximum 8.58% improvement over seven cutting-edge methods in recommendation accuracy. The framework's deployment on a production advertising system resulted in a 1.91% sales growth

Conclusion: CoCo provides a versatile solution for next-generation recommendation systems requiring both knowledge-enhanced reasoning and personalized adaptation.

Abstract: The integration of large language models (LLMs) into recommendation systems
has revealed promising potential through their capacity to extract world
knowledge for enhanced reasoning capabilities. However, current methodologies
that adopt static schema-based prompting mechanisms encounter significant
limitations: (1) they employ universal template structures that neglect the
multi-faceted nature of user preference diversity; (2) they implement
superficial alignment between semantic knowledge representations and behavioral
feature spaces without achieving comprehensive latent space integration. To
address these challenges, we introduce CoCo, an end-to-end framework that
dynamically constructs user-specific contextual knowledge embeddings through a
dual-mechanism approach. Our method realizes profound integration of semantic
and behavioral latent dimensions via adaptive knowledge fusion and
contradiction resolution modules. Experimental evaluations across diverse
benchmark datasets and an enterprise-level e-commerce platform demonstrate
CoCo's superiority, achieving a maximum 8.58% improvement over seven
cutting-edge methods in recommendation accuracy. The framework's deployment on
a production advertising system resulted in a 1.91% sales growth, validating
its practical effectiveness. With its modular design and model-agnostic
architecture, CoCo provides a versatile solution for next-generation
recommendation systems requiring both knowledge-enhanced reasoning and
personalized adaptation.

</details>


### [142] [Large Reasoning Embedding Models: Towards Next-Generation Dense Retrieval Paradigm](https://arxiv.org/abs/2510.14321)
*Jianting Tang,Dongshuai Li,Tao Wen,Fuyu Lv,Dan Ou,Linli Xu*

Main category: cs.IR

TL;DR: 这篇论文提出了一种名为大型推理嵌入模型（LREM）的新方法，通过将推理过程融入到表征学习中，以提高电商搜索系统中密集检索的准确性。该模型特别针对那些与目标商品存在显著词汇差异的困难查询，通过推理过程弥合查询和商品之间的语义差距。


<details>
  <summary>Details</summary>
Motivation: 现有的嵌入模型在处理词汇差异大的困难查询时，由于语义准确性不足，检索性能会显著下降。这些模型倾向于捕捉训练数据中的统计共现模式，偏向于浅层的词汇和语义匹配。

Method: 该论文提出了LREM模型，它将推理过程整合到表征学习中。LREM首先对困难查询进行推理，以实现对原始查询的深入理解，然后生成一个推理增强的查询嵌入用于检索。该模型采用两阶段训练过程：第一阶段使用SFT和InfoNCE损失优化LLM，以建立初步的推理和嵌入能力；第二阶段通过强化学习进一步完善推理轨迹。

Result: 大量的离线和在线实验验证了LREM的有效性。

Conclusion: LREM模型已于2025年8月在中国最大的电商平台上部署，证明了其在实际应用中的价值。

Abstract: In modern e-commerce search systems, dense retrieval has become an
indispensable component. By computing similarities between query and item
(product) embeddings, it efficiently selects candidate products from
large-scale repositories. With the breakthroughs in large language models
(LLMs), mainstream embedding models have gradually shifted from BERT to LLMs
for more accurate text modeling. However, these models still adopt
direct-embedding methods, and the semantic accuracy of embeddings remains
inadequate. Therefore, contrastive learning is heavily employed to achieve
tight semantic alignment between positive pairs. Consequently, such models tend
to capture statistical co-occurrence patterns in the training data, biasing
them toward shallow lexical and semantic matches. For difficult queries
exhibiting notable lexical disparity from target items, the performance
degrades significantly. In this work, we propose the Large Reasoning Embedding
Model (LREM), which novelly integrates reasoning processes into representation
learning. For difficult queries, LREM first conducts reasoning to achieve a
deep understanding of the original query, and then produces a
reasoning-augmented query embedding for retrieval. This reasoning process
effectively bridges the semantic gap between original queries and target items,
significantly improving retrieval accuracy. Specifically, we adopt a two-stage
training process: the first stage optimizes the LLM on carefully curated
Query-CoT-Item triplets with SFT and InfoNCE losses to establish preliminary
reasoning and embedding capabilities, and the second stage further refines the
reasoning trajectories via reinforcement learning (RL). Extensive offline and
online experiments validate the effectiveness of LREM, leading to its
deployment on China's largest e-commerce platform since August 2025.

</details>


### [143] [Ensembling Multiple Hallucination Detectors Trained on VLLM Internal Representations](https://arxiv.org/abs/2510.14330)
*Yuto Nakamizo,Ryuhei Miyazato,Hikaru Tanabe,Ryuta Yamakura,Kiori Hatanaka*

Main category: cs.IR

TL;DR: y3h2 团队在 KDD Cup 2025 Meta CRAG-MM 挑战赛中获得第五名，该挑战赛是关于图像事实性问题回答（VQA）的数据集，着重于减少 VLM 的幻觉。


<details>
  <summary>Details</summary>
Motivation: 比赛使用基于 LLM 的自动评估器，不正确的答案会导致负分，因此需要减少 VLM 内部表示的幻觉。

Method: 使用 hidden_state 和 attention heads 的输出训练基于逻辑回归的幻觉检测模型，并采用这些模型的集成。

Result: 该方法牺牲了一些正确答案，但显著减少了幻觉，最终在排行榜上名列前茅。

Conclusion: 通过减少 VLM 的幻觉，可以在 VQA 任务中取得良好的效果。

Abstract: This paper presents the 5th place solution by our team, y3h2, for the Meta
CRAG-MM Challenge at KDD Cup 2025. The CRAG-MM benchmark is a visual question
answering (VQA) dataset focused on factual questions about images, including
egocentric images. The competition was contested based on VQA accuracy, as
judged by an LLM-based automatic evaluator. Since incorrect answers result in
negative scores, our strategy focused on reducing hallucinations from the
internal representations of the VLM. Specifically, we trained logistic
regression-based hallucination detection models using both the hidden_state and
the outputs of specific attention heads. We then employed an ensemble of these
models. As a result, while our method sacrificed some correct answers, it
significantly reduced hallucinations and allowed us to place among the top
entries on the final leaderboard. For implementation details and code, please
refer to
https://gitlab.aicrowd.com/htanabe/meta-comprehensive-rag-benchmark-starter-kit.

</details>


### [144] [GemiRec: Interest Quantization and Generation for Multi-Interest Recommendation](https://arxiv.org/abs/2510.14626)
*Zhibo Wu,Yunfan Wu,Quan Liu,Lin Jiang,Ping Yang,Yao Hu*

Main category: cs.IR

TL;DR: GemiRec is a multi-interest recommendation framework addressing interest collapse and insufficient modeling of interest evolution.


<details>
  <summary>Details</summary>
Motivation: Existing multi-interest recommendation methods suffer from interest collapse and fail to capture latent interests.

Method: The paper proposes GemiRec, a framework with three modules: Interest Dictionary Maintenance Module (IDMM), Multi-Interest Posterior Distribution Module (MIPDM), and Multi-Interest Retrieval Module (MIRM).

Result: Theoretical and empirical analyses, along with experiments, demonstrate GemiRec's advantages and effectiveness. It has been deployed in production since March 2025.

Conclusion: GemiRec effectively addresses the limitations of existing multi-interest recommendation methods and has practical value in industrial applications.

Abstract: Multi-interest recommendation has gained attention, especially in industrial
retrieval stage. Unlike classical dual-tower methods, it generates multiple
user representations instead of a single one to model comprehensive user
interests. However, prior studies have identified two underlying limitations:
the first is interest collapse, where multiple representations homogenize. The
second is insufficient modeling of interest evolution, as they struggle to
capture latent interests absent from a user's historical behavior. We begin
with a thorough review of existing works in tackling these limitations. Then,
we attempt to tackle these limitations from a new perspective. Specifically, we
propose a framework-level refinement for multi-interest recommendation, named
GemiRec. The proposed framework leverages interest quantization to enforce a
structural interest separation and interest generation to learn the evolving
dynamics of user interests explicitly. It comprises three modules: (a) Interest
Dictionary Maintenance Module (IDMM) maintains a shared quantized interest
dictionary. (b) Multi-Interest Posterior Distribution Module (MIPDM) employs a
generative model to capture the distribution of user future interests. (c)
Multi-Interest Retrieval Module (MIRM) retrieves items using multiple
user-interest representations. Both theoretical and empirical analyses, as well
as extensive experiments, demonstrate its advantages and effectiveness.
Moreover, it has been deployed in production since March 2025, showing its
practical value in industrial applications.

</details>


### [145] [MR.Rec: Synergizing Memory and Reasoning for Personalized Recommendation Assistant with LLMs](https://arxiv.org/abs/2510.14629)
*Jiani Huang,Xingchen Zou,Lianghao Xia,Qing Li*

Main category: cs.IR

TL;DR: 提出了一种名为 MR.Rec 的新框架，该框架结合了记忆和推理，以实现基于 LLM 的推荐。


<details>
  <summary>Details</summary>
Motivation: 当前方法受限于有限的上下文窗口和单轮推理，阻碍了它们捕获动态用户偏好和主动推理推荐上下文的能力。

Method: 开发了一个综合的检索增强生成 (RAG) 系统，该系统有效地索引和检索相关的外部记忆，以增强 LLM 的个性化能力。RAG 系统通过整合推理增强的记忆检索，超越了传统的基于查询的检索。设计了一个强化学习框架，该框架训练 LLM 自主学习记忆利用和推理改进的有效策略。

Result: MR.Rec 在多个指标上显着优于最先进的基线。

Conclusion: MR.Rec 在提供智能化和个性化推荐方面是有效的。

Abstract: The application of Large Language Models (LLMs) in recommender systems faces
key challenges in delivering deep personalization and intelligent reasoning,
especially for interactive scenarios. Current methods are often constrained by
limited context windows and single-turn reasoning, hindering their ability to
capture dynamic user preferences and proactively reason over recommendation
contexts. To address these limitations, we propose MR.Rec, a novel framework
that synergizes memory and reasoning for LLM-based recommendations. To achieve
personalization, we develop a comprehensive Retrieval-Augmented Generation
(RAG) system that efficiently indexes and retrieves relevant external memory to
enhance LLM personalization capabilities. Furthermore, to enable the synergy
between memory and reasoning, our RAG system goes beyond conventional
query-based retrieval by integrating reasoning enhanced memory retrieval.
Finally, we design a reinforcement learning framework that trains the LLM to
autonomously learn effective strategies for both memory utilization and
reasoning refinement. By combining dynamic memory retrieval with adaptive
reasoning, this approach ensures more accurate, context-aware, and highly
personalized recommendations. Extensive experiments demonstrate that MR.Rec
significantly outperforms state-of-the-art baselines across multiple metrics,
validating its efficacy in delivering intelligent and personalized
recommendations. We will release code and data upon paper notification.

</details>


### [146] [Causality Enhancement for Cross-Domain Recommendation](https://arxiv.org/abs/2510.14641)
*Zhibo Wu,Yunfan Wu,Lin Jiang,Ping Yang,Yao Hu*

Main category: cs.IR

TL;DR: 这篇论文提出了一种名为 CE-CDR 的因果增强跨域推荐框架，以解决现有方法中跨域建模不足或负迁移的问题。


<details>
  <summary>Details</summary>
Motivation: 现有跨域推荐方法在利用源域信息时，可能由于任务不一致或未考虑因果关系而导致效果不佳。直接在因果标记数据集上训练跨域表示是一个自然的想法，但现实中识别无偏的因果标签非常具有挑战性。

Method: 该框架首先将跨域推荐重新表述为因果图，然后启发式地构建一个因果关系感知数据集，并推导出理论上无偏的 Partial Label Causal Loss，以增强跨域表示。

Result: 理论和实验分析表明 CE-CDR 的合理性和有效性，以及其作为模型无关插件的通用性。

Conclusion: CE-CDR 已于 2025 年 4 月投入生产，展示了其在现实应用中的实用价值。

Abstract: Cross-domain recommendation forms a crucial component in recommendation
systems. It leverages auxiliary information through source domain tasks or
features to enhance target domain recommendations. However, incorporating
inconsistent source domain tasks may result in insufficient cross-domain
modeling or negative transfer. While incorporating source domain features
without considering the underlying causal relationships may limit their
contribution to final predictions. Thus, a natural idea is to directly train a
cross-domain representation on a causality-labeled dataset from the source to
target domain. Yet this direction has been rarely explored, as identifying
unbiased real causal labels is highly challenging in real-world scenarios. In
this work, we attempt to take a first step in this direction by proposing a
causality-enhanced framework, named CE-CDR. Specifically, we first reformulate
the cross-domain recommendation as a causal graph for principled guidance. We
then construct a causality-aware dataset heuristically. Subsequently, we derive
a theoretically unbiased Partial Label Causal Loss to generalize beyond the
biased causality-aware dataset to unseen cross-domain patterns, yielding an
enriched cross-domain representation, which is then fed into the target model
to enhance target-domain recommendations. Theoretical and empirical analyses,
as well as extensive experiments, demonstrate the rationality and effectiveness
of CE-CDR and its general applicability as a model-agnostic plugin. Moreover,
it has been deployed in production since April 2025, showing its practical
value in real-world applications.

</details>


### [147] [Dataset Pruning in RecSys and ML: Best Practice or Mal-Practice?](https://arxiv.org/abs/2510.14704)
*Leonie Winter*

Main category: cs.IR

TL;DR: 本研究探讨了数据修剪对推荐系统数据集和算法性能的影响，发现常用的核心修剪方法可能会高度选择性地保留少量用户，并且在修剪后的数据上训练和测试算法会获得较高评分，但在未修剪的测试集上评估时，这种优势会消失。


<details>
  <summary>Details</summary>
Motivation: 研究数据修剪（特别是移除交互次数少于指定数量的用户）对数据集特征和算法性能的影响。

Method: 分析了五个基准数据集在未修剪和五个连续修剪级别（5, 10, 20, 50, 100）下的情况。对于每个核心集，我们检查了结构和分布特征，并训练和测试了11个代表性算法。为了进一步评估修剪后的数据集是否导致人为夸大的性能结果，我们还评估了在修剪后的训练集上训练但在未修剪的数据上测试的模型。

Result: 常用的核心修剪方法可能会高度选择性，在某些数据集中只留下原始用户的2%。当在修剪后的数据上进行训练和测试时，传统算法获得了更高的nDCG@10分数；然而，当在未修剪的测试集上评估时，这种优势在很大程度上消失了。在所有算法中，当在未修剪的数据上测试时，性能随着修剪水平的提高而下降，突出了数据集减少对推荐算法性能的影响。

Conclusion: 数据修剪对推荐算法的性能有显著影响，在修剪后的数据集上获得的较高评分可能具有误导性，因为在未修剪的数据集上进行评估时，这些优势会减弱。

Abstract: Offline evaluations in recommender system research depend heavily on
datasets, many of which are pruned, such as the widely used MovieLens
collections. This thesis examines the impact of data pruning - specifically,
removing users with fewer than a specified number of interactions - on both
dataset characteristics and algorithm performance. Five benchmark datasets were
analysed in both their unpruned form and at five successive pruning levels (5,
10, 20, 50, 100). For each coreset, we examined structural and distributional
characteristics and trained and tested eleven representative algorithms. To
further assess if pruned datasets lead to artificially inflated performance
results, we also evaluated models trained on the pruned train sets but tested
on unpruned data. Results show that commonly applied core pruning can be highly
selective, leaving as little as 2% of the original users in some datasets.
Traditional algorithms achieved higher nDCG@10 scores when both training and
testing on pruned data; however, this advantage largely disappeared when
evaluated on unpruned test sets. Across all algorithms, performance declined
with increasing pruning levels when tested on unpruned data, highlighting the
impact of dataset reduction on the performance of recommender algorithms.

</details>


### [148] [Cross-Scenario Unified Modeling of User Interests at Billion Scale](https://arxiv.org/abs/2510.14788)
*Manjie Xu,Cheng Chen,Xin Jia,Jingyi Zhou,Yongji Wu,Zejian Wang,Chi Zhang,Kai Zuo,Yibo Chen,Xu Tang,Yao Hu,Yixin Zhu*

Main category: cs.IR

TL;DR: 提出了一种名为RED-Rec的LLM增强型分层推荐引擎，用于行业级内容推荐系统。


<details>
  <summary>Details</summary>
Motivation: 传统的推荐系统通常优先考虑孤立特定场景中的业务指标优化，忽略了跨场景的行为信号，并且难以在十亿级部署中整合LLM等先进技术，这最终限制了他们捕获整个平台接触点的整体用户兴趣的能力。

Method: 通过聚合和综合来自不同场景的行为来统一跨多个行为上下文的用户兴趣表示，从而产生全面的项目和用户建模。核心是一个双塔LLM驱动的框架，该框架能够实现细致、多方面的表示以及部署效率，并且一种感知场景的密集混合和查询策略有效地融合了各种行为信号，以捕获跨场景的用户意图模式并在服务期间表达细粒度的、特定于上下文的意图。

Result: 通过在RedNote上对数亿用户进行在线A/B测试，验证了RED-Rec，在内容推荐和广告定向任务中均显示出显着的性能提升。我们进一步介绍了一个百万级的顺序推荐数据集RED-MMU，用于全面的离线训练和评估。

Conclusion: 我们的工作促进了统一的用户建模，从而在大型UGC平台中释放了更深层次的个性化并培养了更有意义的用户参与度。

Abstract: User interests on content platforms are inherently diverse, manifesting
through complex behavioral patterns across heterogeneous scenarios such as
search, feed browsing, and content discovery. Traditional recommendation
systems typically prioritize business metric optimization within isolated
specific scenarios, neglecting cross-scenario behavioral signals and struggling
to integrate advanced techniques like LLMs at billion-scale deployments, which
finally limits their ability to capture holistic user interests across platform
touchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender
Engine for Diversified scenarios, tailored for industry-level content
recommendation systems. RED-Rec unifies user interest representations across
multiple behavioral contexts by aggregating and synthesizing actions from
varied scenarios, resulting in comprehensive item and user modeling. At its
core, a two-tower LLM-powered framework enables nuanced, multifaceted
representations with deployment efficiency, and a scenario-aware dense mixing
and querying policy effectively fuses diverse behavioral signals to capture
cross-scenario user intent patterns and express fine-grained, context-specific
intents during serving. We validate RED-Rec through online A/B testing on
hundreds of millions of users in RedNote through online A/B testing, showing
substantial performance gains in both content recommendation and advertisement
targeting tasks. We further introduce a million-scale sequential recommendation
dataset, RED-MMU, for comprehensive offline training and evaluation. Our work
advances unified user modeling, unlocking deeper personalization and fostering
more meaningful user engagement in large-scale UGC platforms.

</details>


### [149] [A Simulation Framework for Studying Systemic Effects of Feedback Loops in Recommender Systems](https://arxiv.org/abs/2510.14857)
*Gabriele Barlacchi,Margherita Lalli,Emanuele Ferragina,Fosca Giannotti,Luca Pappalardo*

Main category: cs.IR

TL;DR: 本文介绍了一个模拟框架，用于模拟在线零售环境中推荐系统的反馈循环，该循环会影响个体行为和集体市场动态。


<details>
  <summary>Details</summary>
Motivation: 研究推荐系统如何影响多样性、购买集中度和用户同质化。

Method: 使用亚马逊电子商务数据集，分析不同的推荐算法。

Result: 反馈循环提高了个人多样性，但降低了集体多样性，并将需求集中在少数热门商品上。某些推荐系统还会随着时间的推移增加用户同质化。

Conclusion: 需要在个性化和长期多样性之间取得平衡。

Abstract: Recommender systems continuously interact with users, creating feedback loops
that shape both individual behavior and collective market dynamics. This paper
introduces a simulation framework to model these loops in online retail
environments, where recommenders are periodically retrained on evolving
user-item interactions. Using the Amazon e-Commerce dataset, we analyze how
different recommendation algorithms influence diversity, purchase
concentration, and user homogenization over time. Results reveal a systematic
trade-off: while the feedback loop increases individual diversity, it
simultaneously reduces collective diversity and concentrates demand on a few
popular items. Moreover, for some recommender systems, the feedback loop
increases user homogenization over time, making user purchase profiles
increasingly similar. These findings underscore the need for recommender
designs that balance personalization with long-term diversity.

</details>


### [150] [Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report](https://arxiv.org/abs/2510.14880)
*Rikiya Takehi,Benjamin Clavié,Sean Lee,Aamir Shakir*

Main category: cs.IR

TL;DR: 介绍了 mxbai-edge-colbert-v0 模型，参数量分别为 17M 和 32M。


<details>
  <summary>Details</summary>
Motivation: 旨在支持各种规模的检索，从云端大规模检索到可在本地设备上运行的模型。

Method: 通过大量实验改进检索和后期交互模型，并提炼成更小的模型作为概念验证。

Result: mxbai-edge-colbert-v0 在常见短文本基准测试 (BEIR) 上优于 ColBERTv2，并在长上下文任务中表现出前所未有的效率。

Conclusion: mxbai-edge-colbert-v0 是一个有能力的小模型，是未来实验的坚实基础。

Abstract: In this work, we introduce mxbai-edge-colbert-v0 models, at two different
parameter counts: 17M and 32M. As part of our research, we conduct numerous
experiments to improve retrieval and late-interaction models, which we intend
to distill into smaller models as proof-of-concepts. Our ultimate aim is to
support retrieval at all scales, from large-scale retrieval which lives in the
cloud to models that can run locally, on any device. mxbai-edge-colbert-v0 is a
model that we hope will serve as a solid foundation backbone for all future
experiments, representing the first version of a long series of small
proof-of-concepts. As part of the development of mxbai-edge-colbert-v0, we
conducted multiple ablation studies, of which we report the results. In terms
of downstream performance, mxbai-edge-colbert-v0 is a particularly capable
small model, outperforming ColBERTv2 on common short-text benchmarks (BEIR) and
representing a large step forward in long-context tasks, with unprecedented
efficiency.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [151] [Large Language Models for Real-World IoT Device Identification](https://arxiv.org/abs/2510.13817)
*Rameen Mahmood,Tousif Ahmed,Sai Teja Peddinti,Danny Yuxing Huang*

Main category: cs.LG

TL;DR: 本文提出了一种基于异构网络元数据的语言建模方法的设备识别方案，以应对物联网设备快速扩张带来的安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有物联网设备识别方法无法跟上设备扩张速度，导致安全、隐私和网络责任问题，尤其是在开放世界环境中，流量元数据不完整、嘈杂或被混淆。

Method: 使用大型语言模型集合，结合互信息和基于熵的稳定性评分，为IoT Inspector数据集生成高质量的供应商标签，并使用课程学习微调量化的LLaMA3.18B模型。

Result: 该模型在2015家供应商中实现了98.25%的Top-1准确率和90.73%的宏准确率，并且对缺失字段、协议漂移和对抗性操纵具有弹性。

Conclusion: 指令调优的LLM为大规模真实世界的设备识别提供了可扩展和可解释的基础。

Abstract: The rapid expansion of IoT devices has outpaced current identification
methods, creating significant risks for security, privacy, and network
accountability. These challenges are heightened in open-world environments,
where traffic metadata is often incomplete, noisy, or intentionally obfuscated.
We introduce a semantic inference pipeline that reframes device identification
as a language modeling task over heterogeneous network metadata. To construct
reliable supervision, we generate high-fidelity vendor labels for the IoT
Inspector dataset, the largest real-world IoT traffic corpus, using an ensemble
of large language models guided by mutual-information and entropy-based
stability scores. We then instruction-tune a quantized LLaMA3.18B model with
curriculum learning to support generalization under sparsity and long-tail
vendor distributions. Our model achieves 98.25% top-1 accuracy and 90.73% macro
accuracy across 2,015 vendors while maintaining resilience to missing fields,
protocol drift, and adversarial manipulation. Evaluation on an independent IoT
testbed, coupled with explanation quality and adversarial stress tests,
demonstrates that instruction-tuned LLMs provide a scalable and interpretable
foundation for real-world device identification at scale.

</details>


### [152] [Self-Training with Dynamic Weighting for Robust Gradual Domain Adaptation](https://arxiv.org/abs/2510.13864)
*Zixi Wang,Yushe Cao,Yubo Huang,Jinzhu Wei,Jingzehua Xu,Shuai Zhang,Xin Lai*

Main category: cs.LG

TL;DR: 提出了一种新的自训练方法，称为动态加权自训练（STDW），旨在增强渐进域适应（GDA）的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统GDA方法通过中间域和自训练来减轻域偏移，但常常受到低效的知识迁移或不完整的中间数据的困扰。

Method: 引入了一种动态加权机制，自适应地平衡源域和目标域在训练过程中的损失贡献。具体来说，我们设计了一个由时变超参数 $\varrho$（从 0 到 1 递增）控制的优化框架，该参数控制特定领域学习的强度并确保稳定的适应。该方法利用自训练来生成伪标签，并优化加权目标函数以进行迭代模型更新，从而保持跨中间域的鲁棒性。

Result: 在旋转 MNIST、颜色偏移 MNIST、人像数据集和 Cover Type 数据集上的实验表明，STDW 优于现有基线。消融研究进一步验证了 $\varrho$ 的动态调度在实现渐进式适应中的关键作用，证实了其在减少域偏差和提高泛化能力方面的有效性。

Conclusion: 这项工作为鲁棒的渐进式领域自适应提供了理论见解和实践框架，在动态现实场景中具有潜在的应用。

Abstract: In this paper, we propose a new method called Self-Training with Dynamic
Weighting (STDW), which aims to enhance robustness in Gradual Domain Adaptation
(GDA) by addressing the challenge of smooth knowledge migration from the source
to the target domain. Traditional GDA methods mitigate domain shift through
intermediate domains and self-training but often suffer from inefficient
knowledge migration or incomplete intermediate data. Our approach introduces a
dynamic weighting mechanism that adaptively balances the loss contributions of
the source and target domains during training. Specifically, we design an
optimization framework governed by a time-varying hyperparameter $\varrho$
(progressing from 0 to 1), which controls the strength of domain-specific
learning and ensures stable adaptation. The method leverages self-training to
generate pseudo-labels and optimizes a weighted objective function for
iterative model updates, maintaining robustness across intermediate domains.
Experiments on rotated MNIST, color-shifted MNIST, portrait datasets, and the
Cover Type dataset demonstrate that STDW outperforms existing baselines.
Ablation studies further validate the critical role of $\varrho$'s dynamic
scheduling in achieving progressive adaptation, confirming its effectiveness in
reducing domain bias and improving generalization. This work provides both
theoretical insights and a practical framework for robust gradual domain
adaptation, with potential applications in dynamic real-world scenarios. The
code is available at https://github.com/Dramwig/STDW.

</details>


### [153] [Deep Edge Filter: Return of the Human-Crafted Layer in Deep Learning](https://arxiv.org/abs/2510.13865)
*Dongkwan Lee,Junhoo Lee,Nojun Kwak*

Main category: cs.LG

TL;DR: 提出了一种新的深度边缘滤波器，它将高通滤波应用于深度神经网络特征，以提高模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 作者假设神经网络在高频分量中编码任务相关的语义信息，而在深度特征的低频分量中存储特定于领域的偏差。

Method: 通过从原始特征中减去低通滤波的输出，该方法分离出可泛化的表示，同时保持架构的完整性。

Result: 在视觉、文本、3D和音频等不同领域的实验结果表明，无论模型架构和数据模式如何，性能都得到了一致的提高。

Conclusion: 分析表明，该方法诱导特征稀疏化并有效地分离高频分量，为核心假设提供了实证验证。

Abstract: We introduce the Deep Edge Filter, a novel approach that applies high-pass
filtering to deep neural network features to improve model generalizability.
Our method is motivated by our hypothesis that neural networks encode
task-relevant semantic information in high-frequency components while storing
domain-specific biases in low-frequency components of deep features. By
subtracting low-pass filtered outputs from original features, our approach
isolates generalizable representations while preserving architectural
integrity. Experimental results across diverse domains such as Vision, Text,
3D, and Audio demonstrate consistent performance improvements regardless of
model architecture and data modality. Analysis reveals that our method induces
feature sparsification and effectively isolates high-frequency components,
providing empirical validation of our core hypothesis. The code is available at
https://github.com/dongkwani/DeepEdgeFilter.

</details>


### [154] [CoLoR-GAN: Continual Few-Shot Learning with Low-Rank Adaptation in Generative Adversarial Networks](https://arxiv.org/abs/2510.13869)
*Munsif Ali,Leonardo Rossi,Massimo Bertozzi*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为CoLoR-GAN的框架，用于解决GAN中的持续少样本学习问题，该框架利用低秩张量来有效调整模型以适应目标任务，同时减少所需的参数数量。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的方法（如LFS-GAN）在每次训练迭代中都会引入大量新的权重，从长远来看，这将变得非常重要。因此，本文旨在减少参数量。

Method: 该方法引入了一种卷积层的LoRA in LoRA (LLoRA)技术，并对LoRA的超参数选择进行了实证研究。

Result: CoLoR-GAN在多个基准CL和FS任务上展示了有效性，并且该模型是高效的，达到了SOTA性能，但资源数量却大大减少。

Conclusion: CoLoR-GAN是一种有效的框架，可以在GAN中实现持续少样本学习，同时减少所需的参数数量。

Abstract: Continual learning (CL) in the context of Generative Adversarial Networks
(GANs) remains a challenging problem, particularly when it comes to learn from
a few-shot (FS) samples without catastrophic forgetting. Current most effective
state-of-the-art (SOTA) methods, like LFS-GAN, introduce a non-negligible
quantity of new weights at each training iteration, which would become
significant when considering the long term. For this reason, this paper
introduces \textcolor{red}{\textbf{\underline{c}}}ontinual
few-sh\textcolor{red}{\textbf{\underline{o}}}t learning with
\textcolor{red}{\textbf{\underline{lo}}}w-\textcolor{red}{\textbf{\underline{r}}}ank
adaptation in GANs named CoLoR-GAN, a framework designed to handle both FS and
CL together, leveraging low-rank tensors to efficiently adapt the model to
target tasks while reducing even more the number of parameters required.
Applying a vanilla LoRA implementation already permitted us to obtain pretty
good results. In order to optimize even further the size of the adapters, we
challenged LoRA limits introducing a LoRA in LoRA (LLoRA) technique for
convolutional layers. Finally, aware of the criticality linked to the choice of
the hyperparameters of LoRA, we provide an empirical study to easily find the
best ones. We demonstrate the effectiveness of CoLoR-GAN through experiments on
several benchmark CL and FS tasks and show that our model is efficient,
reaching SOTA performance but with a number of resources enormously reduced.
Source code is available on
\href{https://github.com/munsifali11/CoLoR-GAN}{Github.

</details>


### [155] [Joint Discriminative-Generative Modeling via Dual Adversarial Training](https://arxiv.org/abs/2510.13872)
*Xuwang Yin,Claire Zhang,Julie Steele,Nir Shavit,Tony T. Wang*

Main category: cs.LG

TL;DR: 提出了一种新的训练框架，该框架结合了对抗训练 (AT) 原则，以实现判别鲁棒性和稳定的生成学习。


<details>
  <summary>Details</summary>
Motivation: 同时在单个框架内实现稳健的分类和高保真生成建模，这是一个巨大的挑战。混合方法（例如，联合能量模型 (JEM)）将分类器解释为 EBM，但通常受到 SGLD 训练中固有的不稳定性和较差的样本质量的限制。

Method: 该方法引入了三个关键创新：(1) 用基于 AT 的稳定方法替代基于 SGLD 的 JEM 学习，该方法通过使用 BCE 损失区分真实数据和 PGD 生成的对比样本来优化能量函数；(2) 判别成分的协同对抗训练，增强了分类鲁棒性，同时消除了对显式梯度惩罚的需要；(3) 解决批归一化和 EBM 训练之间不兼容性的两阶段训练程序。

Result: 在 CIFAR-10、CIFAR-100 和 ImageNet 上的实验表明，该方法在现有混合模型的基础上大大提高了对抗鲁棒性，同时保持了竞争性的生成性能。在 ImageNet 上，当针对生成建模进行优化时，该模型的生成保真度超过了 BigGAN，并且接近扩散模型，代表了第一个基于 MCMC 的 EBM 方法，可以在复杂的高分辨率数据集上实现高质量生成。

Conclusion: 该方法解决了限制 JEM 扩展的关键稳定性问题，并表明对抗训练可以作为能够生成和稳健地分类视觉数据的统一框架的有效基础。

Abstract: Simultaneously achieving robust classification and high-fidelity generative
modeling within a single framework presents a significant challenge. Hybrid
approaches, such as Joint Energy-Based Models (JEM), interpret classifiers as
EBMs but are often limited by the instability and poor sample quality inherent
in SGLD-based training. We address these limitations by proposing a novel
training framework that integrates adversarial training (AT) principles for
both discriminative robustness and stable generative learning. The proposed
method introduces three key innovations: (1) the replacement of SGLD-based JEM
learning with a stable, AT-based approach that optimizes the energy function by
discriminating between real data and PGD-generated contrastive samples using
the BCE loss; (2) synergistic adversarial training for the discriminative
component that enhances classification robustness while eliminating the need
for explicit gradient penalties; and (3) a two-stage training procedure to
resolve the incompatibility between batch normalization and EBM training.
Experiments on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our method
substantially improves adversarial robustness over existing hybrid models while
maintaining competitive generative performance. On ImageNet, when optimized for
generative modeling, our model's generative fidelity surpasses that of BigGAN
and approaches diffusion models, representing the first MCMC-based EBM approach
to achieve high-quality generation on complex, high-resolution datasets. Our
approach addresses key stability issues that have limited JEM scaling and
demonstrates that adversarial training can serve as an effective foundation for
unified frameworks capable of generating and robustly classifying visual data.

</details>


### [156] [K-frames: Scene-Driven Any-k Keyframe Selection for long video understanding](https://arxiv.org/abs/2510.13891)
*Yifeng Yao,Yike Yun,Jing Wang,Huishuai Zhang,Dongyan Zhao,Ke Tian,Zhihao Wang,Minghui Qiu,Tao Wang*

Main category: cs.LG

TL;DR: 本研究提出了一种新的场景驱动的关键帧选择范式K-frames，以解决多模态大语言模型在长视频理解中面临的上下文窗口限制、计算成本高以及传统均匀采样导致的信息损失问题。


<details>
  <summary>Details</summary>
Motivation: 现有关键帧选择方法通常产生稀疏且时间上不连续的帧，忽略了场景的连续性，并且缺乏多尺度帧选择的灵活性。

Method: 首先，构建了一个包含20万个视频Highlights的数据集PeakClips。然后，K-frames通过一个三阶段的渐进课程学习clip2frame的选择，包括两个监督微调阶段（用于时间定位和关键片段感知）和一个强化学习阶段（直接优化场景驱动的预测策略）。

Result: 在主要的视频理解 benchmarks 上的大量实验表明，K-frames 为各种规模的关键帧选择提供了一种有效、可解释且即插即用的解决方案。

Conclusion: K-frames 能够预测语义连贯、与查询相关的片段，从而实现任意数量的关键帧选择，以满足不同的用户需求。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
capabilities in image understanding, but long-video are constrained by context
windows and computational cost. Uniform frame sampling often leads to
substantial information loss. Meanwhile existing keyframe selection methods
such as text-frame retrieval or RL-based frame optimization typically yield
sparse and temporally disjointed frames, overlooking scene continuity and
lacking flexibility for multi-scale frame selection. To address these
limitations, we introduce K-frames, a novel paradigm for scene-driven keyframe
selection that preserves temporal continuity. Instead of selecting individual
frames, K-frames predicts semantically coherent, query-relevant clips, which
enables any-k keyframes selection to meet diverse user budgets. To achieve this
approach, we first introduce PeakClips, a dataset of 200K video highlights
conditioned by query. Building on this dataset, K-frames learns clip2frame
selection using a three-stage progressive curriculum. It involves two
Supervised Fine-Tuning stages for temporal grounding and key-clip perception,
followed by a Reinforcement Learning stage that directly optimizes the
scene-driven prediction policy for downstream task without further annotations.
Extensive experiments on major long-video understanding benchmarks demonstrate
that K-frames provides an effective, interpretable, and plug-and-play solution
for keyframe selection at various scales. Our dataset and model will be
available.

</details>


### [157] [Multi-View Semi-Supervised Label Distribution Learning with Local Structure Complementarity](https://arxiv.org/abs/2510.13917)
*Yanshan Xiao,Kaihong Wu,Bo Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的多视图半监督标签分布学习方法(MVSS-LDL)，该方法利用每个视图的局部近邻结构，并强调多个视图中局部近邻结构的互补性。


<details>
  <summary>Details</summary>
Motivation: 现有的标签分布学习(LDL)方法主要针对单视图问题，而忽略了带标签和无标签数据的多视图LDL问题。

Method: 该方法首先计算每个视图的k近邻，然后通过结合其他视图中的近邻来补充每个视图的近邻集。最后，基于补充的近邻集，构建了一个基于图学习的多视图半监督LDL模型。

Result: 数值实验表明，MVSS-LDL比现有的单视图LDL方法具有更好的分类性能。

Conclusion: 本文首次尝试解决多视图LDL问题，并提出了MVSS-LDL方法，该方法通过考虑局部近邻结构的互补性，使不同的视图能够相互提供局部结构信息以相互补充。

Abstract: Label distribution learning (LDL) is a paradigm that each sample is
associated with a label distribution. At present, the existing approaches are
proposed for the single-view LDL problem with labeled data, while the
multi-view LDL problem with labeled and unlabeled data has not been considered.
In this paper, we put forward the multi-view semi-supervised label distribution
learning with local structure complementarity (MVSS-LDL) approach, which
exploits the local nearest neighbor structure of each view and emphasizes the
complementarity of local nearest neighbor structures in multiple views.
Specifically speaking, we first explore the local structure of view $v$ by
computing the $k$-nearest neighbors. As a result, the $k$-nearest neighbor set
of each sample $\boldsymbol{x}_i$ in view $v$ is attained. Nevertheless, this
$k$-nearest neighbor set describes only a part of the nearest neighbor
information of sample $\boldsymbol{x}_i$. In order to obtain a more
comprehensive description of sample $\boldsymbol{x}_i$'s nearest neighbors, we
complement the nearest neighbor set in view $v$ by incorporating sample
$\boldsymbol{x}_i$'s nearest neighbors in other views. Lastly, based on the
complemented nearest neighbor set in each view, a graph learning-based
multi-view semi-supervised LDL model is constructed. By considering the
complementarity of local nearest neighbor structures, different views can
mutually provide the local structural information to complement each other. To
the best of our knowledge, this is the first attempt at multi-view LDL.
Numerical studies have demonstrated that MVSS-LDL attains explicitly better
classification performance than the existing single-view LDL methods.

</details>


### [158] [Weight Weaving: Parameter Pooling for Data-Free Model Merging](https://arxiv.org/abs/2510.13921)
*Levy Chaves,Eduardo Valle,Sandra Avila*

Main category: cs.LG

TL;DR: 模型合并是一种通过参数集成结合专业深度神经网络的经济高效且数据高效的方法。大多数模型合并方法都严重依赖于缩放超参数λ，这些参数以全局或单独方式衡量每个模型的贡献。我们介绍了一种即插即用的技术，称为权重编织，它使用用户定义的池化函数（例如平均、随机选择或现有模型合并方法）在 λ 值搜索空间中池化模型权重。我们的方法在数据自由设置中始终提高了多种模型合并方法的性能，平均准确率提高了 15.9 个百分点。


<details>
  <summary>Details</summary>
Motivation: 模型合并依赖于缩放超参数λ，但在实践中，免数据设置缩放因子很困难，通常需要使用来自评估集的特权数据来调整λ。

Method: 我们引入了一种即插即用的技术，称为权重编织，它使用用户定义的池化函数（例如平均、随机选择或现有模型合并方法）在 λ 值搜索空间中池化模型权重。该方法对搜索空间施加的约束极小，并且与现有模型合并方法正交。

Result: 在视觉多任务学习、视觉持续学习和域泛化这三个实验设置中的三个 ViT 变体上验证了权重编织。该方法始终提高了多种模型合并方法的性能，在数据自由设置中平均准确率提高了 15.9 个百分点。

Conclusion: 权重编织是一种有前途的免数据模型合并方法，它可以提高各种任务和模型合并技术的性能。

Abstract: Model merging provides a cost-effective and data-efficient combination of
specialized deep neural networks through parameter integration. This technique
leverages expert models across downstream tasks without requiring retraining.
Most model merging approaches critically depend on scaling hyper-parameters
$\lambda$, which weight each model's contribution globally or individually.
Principled approaches for setting scaling factors without accessing any data
(data-free) are scarce, often leading researchers to tune $\lambda$ using
privileged data from the evaluation set, which is obviously unfeasible in
practice. To address this limitation, we introduce Weight Weaving, a
plug-and-play technique that pools model weights across $\lambda$ values search
space using user-defined pooling functions, such as averaging, random
selection, or even existing model merging methods. Our method demonstrates high
modularity, imposing minimal constraints on the search space. It operates
orthogonally to existing model merging methods and eliminates evaluation data
requirements. We validate Weight Weaving across three ViT variants in three
experimental setups: vision multi-task learning, vision continual learning, and
domain generalization. Our method consistently improves the performance of
several model merging methods, achieving average accuracy gains of up to 15.9
percentage points in a data-free setting.

</details>


### [159] [LTR-ICD: A Learning-to-Rank Approach for Automatic ICD Coding](https://arxiv.org/abs/2510.13922)
*Mohammad Mansoori,Amira Soliman,Farzaneh Etminani*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，将ICD代码分配和排序任务视为分类和排序任务，而不是传统的分类任务。


<details>
  <summary>Details</summary>
Motivation: 临床笔记包含医生在就诊期间提供的非结构化文本。正确分配和排序ICD代码对于医疗诊断和报销至关重要。然而，自动化这项任务仍然具有挑战性。

Method: 本文从检索系统的角度处理ICD代码分配和排序任务，将其转化为分类和排序任务。

Result: 该模型在正确排序主要诊断代码方面的准确率为47%，而最先进的分类器为20%。在分类指标方面，该模型实现了0.6065的micro-F1分数和0.2904的macro-F1分数，超过了之前的最佳模型（分别为0.597和0.2660）。

Conclusion: 该研究表明，所提出的框架比其他方法更能够识别高优先级代码。

Abstract: Clinical notes contain unstructured text provided by clinicians during
patient encounters. These notes are usually accompanied by a sequence of
diagnostic codes following the International Classification of Diseases (ICD).
Correctly assigning and ordering ICD codes are essential for medical diagnosis
and reimbursement. However, automating this task remains challenging.
State-of-the-art methods treated this problem as a classification task, leading
to ignoring the order of ICD codes that is essential for different purposes. In
this work, as a first attempt, we approach this task from a retrieval system
perspective to consider the order of codes, thus formulating this problem as a
classification and ranking task. Our results and analysis show that the
proposed framework has a superior ability to identify high-priority codes
compared to other methods. For instance, our model accuracy in correctly
ranking primary diagnosis codes is 47%, compared to 20% for the
state-of-the-art classifier. Additionally, in terms of classification metrics,
the proposed model achieves a micro- and macro-F1 scores of 0.6065 and 0.2904,
respectively, surpassing the previous best model with scores of 0.597 and
0.2660.

</details>


### [160] [Distributional Consistency Loss: Beyond Pointwise Data Terms in Inverse Problems](https://arxiv.org/abs/2510.13972)
*George Webber,Andrew J. Reader*

Main category: cs.LG

TL;DR: 提出了一种新的数据保真度损失函数，称为分布一致性（DC）损失，它通过测试观测测量值在统计上是否与当前估计所隐含的噪声分布一致来集体评估数据保真度，而不是逐点匹配。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案通常在先验假设（正则化）与对噪声测量数据的一致性（数据保真度）之间取得平衡，而传统的数据保真度损失函数（如均方误差（MSE））常常导致对噪声的过拟合。

Method: 采用了一种聚合视角，引入了分布一致性（DC）损失，这是一种数据保真度目标，它使用基于模型的概率分数对每个测量值进行分布级别校准，从而取代了逐点匹配。DC损失可以直接替代标准数据一致性项。

Result: 在图像去噪和医学图像重建两个关键应用领域证明了其有效性：在具有深度图像先验的图像去噪中，使用DC代替MSE损失消除了对提前停止的需求并实现了更高的PSNR；在来自泊松噪声数据的医学图像重建中，DC损失减少了高度迭代重建中的伪影，并增强了手工制作的正则化的功效。

Conclusion: DC损失是一种在统计上有充分根据的、性能增强的传统保真度损失的替代方案，适用于逆问题。

Abstract: Recovering true signals from noisy measurements is a central challenge in
inverse problems spanning medical imaging, geophysics, and signal processing.
Current solutions balance prior assumptions regarding the true signal
(regularization) with agreement to noisy measured data (data-fidelity).
Conventional data-fidelity loss functions, such as mean-squared error (MSE) or
negative log-likelihood, seek pointwise agreement with noisy measurements,
often leading to overfitting to noise. In this work, we instead evaluate
data-fidelity collectively by testing whether the observed measurements are
statistically consistent with the noise distributions implied by the current
estimate. We adopt this aggregated perspective and introduce distributional
consistency (DC) loss, a data-fidelity objective that replaces pointwise
matching with distribution-level calibration using model-based probability
scores for each measurement. DC loss acts as a direct and practical plug-in
replacement for standard data consistency terms: i) it is compatible with
modern regularizers, ii) it is optimized in the same way as traditional losses,
and iii) it avoids overfitting to measurement noise even without the use of
priors. Its scope naturally fits many practical inverse problems where the
measurement-noise distribution is known and where the measured dataset consists
of many independent noisy values. We demonstrate efficacy in two key example
application areas: i) in image denoising with deep image prior, using DC
instead of MSE loss removes the need for early stopping and achieves higher
PSNR; ii) in medical image reconstruction from Poisson-noisy data, DC loss
reduces artifacts in highly-iterated reconstructions and enhances the efficacy
of hand-crafted regularization. These results position DC loss as a
statistically grounded, performance-enhancing alternative to conventional
fidelity losses for inverse problems.

</details>


### [161] [BitNet Distillation](https://arxiv.org/abs/2510.13998)
*Xun Wu,Shaohan Huang,Wenhui Wang,Ting Song,Li Dong,Yan Xia,Furu Wei*

Main category: cs.LG

TL;DR: BitDistill: Fine-tunes full-precision LLMs into 1.58-bit precision for specific tasks, achieving strong performance with minimal cost.


<details>
  <summary>Details</summary>
Motivation: To fine-tune full-precision LLMs into low-precision models for downstream tasks while maintaining performance and reducing computational cost.

Method: Uses SubLN, multi-head attention distillation (based on MiniLM), and continual pre-training.

Result: Achieves performance comparable to full-precision models with up to 10x memory savings and 2.65x faster inference on CPUs.

Conclusion: BitDistill is an effective method for fine-tuning LLMs to 1.58-bit precision, offering significant memory and speed advantages.

Abstract: In this paper, we present BitNet Distillation (BitDistill), a lightweight
pipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into
1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream
tasks, achieving strong task-specific performance with minimal computational
cost. Specifically, BitDistill incorporates three key techniques: the SubLN
module, as introduced in BitNet; multi-head attention distillation, based on
MiniLM; and continual pre-training, which serves as a crucial warm-up step to
mitigate the scalability issue of the performance gap between finetuned
full-precision and 1.58-bit LLMs on specific tasks. Experimental results show
that BitDistill achieves performance comparable to the full-precision
counterpart models across model size, while enabling up to 10x memory savings
and 2.65x faster inference on CPUs. Code is available at
https://github.com/microsoft/BitNet.

</details>


### [162] [REAP the Experts: Why Pruning Prevails for One-Shot MoE compression](https://arxiv.org/abs/2510.13999)
*Mike Lasby,Ivan Lazarevich,Nish Sinnadurai,Sean Lie,Yani Ioannou,Vithursan Thangarasa*

Main category: cs.LG

TL;DR: 专家稀疏混合模型(SMoE)虽然高效，但参数过多导致内存开销大，因此需要进行专家压缩。


<details>
  <summary>Details</summary>
Motivation: 现有研究倾向于在判别任务上进行专家合并，但该研究表明，在生成任务中，专家剪枝是更优策略。专家合并会引入不可避免的误差，导致功能子空间崩溃。

Method: 提出了一种新的剪枝标准，即路由器加权专家激活剪枝(REAP)，该方法同时考虑了路由器门值和专家激活范数。

Result: 在20B到1T参数的SMoE模型上，REAP在生成任务上始终优于合并和其他剪枝方法，尤其是在50%压缩率下。在代码生成和工具调用任务中，即使剪枝50%的专家，该方法也能在Qwen3-Coder-480B和Kimi-K2上实现近乎无损的压缩。

Conclusion: 专家剪枝是生成任务中压缩SMoE模型的更优策略，提出的REAP方法能够有效地进行专家剪枝，并在代码生成和工具调用等任务上实现近乎无损的压缩。

Abstract: Sparsely-activated Mixture-of-Experts (SMoE) models offer efficient
pre-training and low latency but their large parameter counts create
significant memory overhead, motivating research into expert compression.
Contrary to recent findings favouring expert merging on discriminative
benchmarks, we demonstrate that expert pruning is a superior strategy for
generative tasks. We prove that merging introduces an irreducible error by
causing a "functional subspace collapse", due to the loss of the router's
independent, input-dependent control over experts. Leveraging this insight, we
propose Router-weighted Expert Activation Pruning (REAP), a novel pruning
criterion that considers both router gate-values and expert activation norms.
Across a diverse set of SMoE models ranging from 20B to 1T parameters, REAP
consistently outperforms merging and other pruning methods on generative
benchmarks, especially at 50% compression. Notably, our method achieves
near-lossless compression on code generation and tool-calling tasks with
Qwen3-Coder-480B and Kimi-K2, even after pruning 50% of experts.

</details>


### [163] [Conditional Clifford-Steerable CNNs with Complete Kernel Basis for PDE Modeling](https://arxiv.org/abs/2510.14007)
*Bálint László Szarvas,Maksim Zhdanov*

Main category: cs.LG

TL;DR: CSCNNs are limited in expressivity. This paper introduces Conditional Clifford-Steerable Kernels to improve expressivity.


<details>
  <summary>Details</summary>
Motivation: The kernel basis of CSCNNs is not complete, thus limiting the model expressivity.

Method: Proposes Conditional Clifford-Steerable Kernels, which augment the kernels with equivariant representations computed from the input feature field. Derives the equivariance constraint for these input-dependent kernels and show how it can be solved efficiently via implicit parameterization.

Result: Demonstrates improved expressivity on multiple PDE forecasting tasks, including fluid dynamics and relativistic electrodynamics, where our method consistently outperforms baseline methods.

Conclusion: The proposed method improves expressivity and outperforms baselines in PDE forecasting tasks.

Abstract: Clifford-Steerable CNNs (CSCNNs) provide a unified framework that allows
incorporating equivariance to arbitrary pseudo-Euclidean groups, including
isometries of Euclidean space and Minkowski spacetime. In this work, we
demonstrate that the kernel basis of CSCNNs is not complete, thus limiting the
model expressivity. To address this issue, we propose Conditional
Clifford-Steerable Kernels, which augment the kernels with equivariant
representations computed from the input feature field. We derive the
equivariance constraint for these input-dependent kernels and show how it can
be solved efficiently via implicit parameterization. We empirically demonstrate
an improved expressivity of the resulting framework on multiple PDE forecasting
tasks, including fluid dynamics and relativistic electrodynamics, where our
method consistently outperforms baseline methods.

</details>


### [164] [Noise-Adaptive Layerwise Learning Rates: Accelerating Geometry-Aware Optimization for Deep Neural Network Training](https://arxiv.org/abs/2510.14009)
*Jie Hao,Xiaochuan Gong,Jie Xu,Zhengdao Wang,Mingrui Liu*

Main category: cs.LG

TL;DR: 提出了一种噪声自适应的层级学习率方案，以加速DNN训练。


<details>
  <summary>Details</summary>
Motivation: 现有的几何感知优化器对同一组内的层施加固定的学习率，这对于DNN训练来说可能效率低下。

Method: 在几何感知优化算法的基础上，估计由所选LMO引起的对偶范数中的梯度方差，并使用它来分配时变的噪声自适应层级学习率。

Result: 在transformer架构（如LLaMA和GPT）上的实验结果表明，该方法比最先进的优化器收敛速度更快。

Conclusion: 该算法实现了快速的收敛速度，并在实验中表现出优于现有优化器的性能。

Abstract: Geometry-aware optimization algorithms, such as Muon, have achieved
remarkable success in training deep neural networks (DNNs). These methods
leverage the underlying geometry of DNNs by selecting appropriate norms for
different layers and updating parameters via norm-constrained linear
minimization oracles (LMOs). However, even within a group of layers associated
with the same norm, the local curvature can be heterogeneous across layers and
vary dynamically over the course of training. For example, recent work shows
that sharpness varies substantially across transformer layers and throughout
training, yet standard geometry-aware optimizers impose fixed learning rates to
layers within the same group, which may be inefficient for DNN training.
  In this paper, we introduce a noise-adaptive layerwise learning rate scheme
on top of geometry-aware optimization algorithms and substantially accelerate
DNN training compared to methods that use fixed learning rates within each
group. Our method estimates gradient variance in the dual norm induced by the
chosen LMO on the fly, and uses it to assign time-varying noise-adaptive
layerwise learning rates within each group. We provide a theoretical analysis
showing that our algorithm achieves a sharp convergence rate. Empirical results
on transformer architectures such as LLaMA and GPT demonstrate that our
approach achieves faster convergence than state-of-the-art optimizers.

</details>


### [165] [Context-Selective State Space Models: Feedback is All You Need](https://arxiv.org/abs/2510.14027)
*Riccardo Zattra,Giacomo Baggio,Umberto Casti,Augusto Ferrante,Francesco Ticozzi*

Main category: cs.LG

TL;DR: COFFEE模型是一种新型的时变状态空间模型，它结合了状态反馈以实现上下文相关的选择性，同时仍然允许并行实现。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在处理长程依赖时面临挑战，而Mamba架构中的S6模块在长序列基准测试中表现出色。COFFEE模型旨在提供一种更高效的替代方案。

Method: COFFEE模型通过状态反馈从内部状态计算选择性，并采用高效的模型参数化来减少冗余。

Result: 在归纳头任务中，COFFEE模型以比S6少两个数量级的参数和训练序列实现了接近完美的精度。在MNIST上，COFFEE模型在相同的架构中大大优于S6，仅用3585个参数就达到了97%的准确率。

Conclusion: 状态反馈是构建可扩展和高效序列模型的关键机制。

Abstract: Transformers, powered by the attention mechanism, are the backbone of most
foundation models, yet they suffer from quadratic complexity and difficulties
in dealing with long-range dependencies in the input sequence. Recent work has
shown that state space models (SSMs) provide an efficient alternative, with the
S6 module at the core of the Mamba architecture achieving state-of-the-art
results on long-sequence benchmarks. In this paper, we introduce the COFFEE
(COntext From FEEdback) model, a novel time-varying SSM that incorporates state
feedback to enable context-dependent selectivity, while still allowing for
parallel implementation. Whereas the selectivity mechanism of S6 only depends
on the current input, COFFEE computes it from the internal state, which serves
as a compact representation of the sequence history. This shift allows the
model to regulate its dynamics based on accumulated context, improving its
ability to capture long-range dependencies. In addition to state feedback, we
employ an efficient model parametrization that removes redundancies present in
S6 and leads to a more compact and trainable formulation. On the induction head
task, COFFEE achieves near-perfect accuracy with two orders of magnitude fewer
parameters and training sequences compared to S6. On MNIST, COFFEE largely
outperforms S6 within the same architecture, reaching 97% accuracy with only
3585 parameters. These results showcase the role of state feedback as a key
mechanism for building scalable and efficient sequence models.

</details>


### [166] [CausalVerse: Benchmarking Causal Representation Learning with Configurable High-Fidelity Simulations](https://arxiv.org/abs/2510.14049)
*Guangyi Chen,Yunlong Deng,Peiyuan Zhu,Yan Li,Yifan Sheng,Zijian Li,Kun Zhang*

Main category: cs.LG

TL;DR: 提出了一个新的因果表征学习(CRL)基准，使用高保真模拟视觉数据，保留了真实的视觉复杂性，更重要的是，可以访问真实的因果生成过程。


<details>
  <summary>Details</summary>
Motivation: 现有的评估通常依赖于简单的合成数据集或真实世界任务的下游性能，通常在现实主义和评估精度之间存在两难。

Method: 使用高保真模拟视觉数据，构建包含20万张图像和300万个视频帧的数据集，涵盖静态图像生成、动态物理模拟、机器人操作和交通状况分析四个领域。

Result: 评估了不同范例的代表性CRL方法，并提供了经验见解，以帮助从业者和新手选择或扩展适当的CRL框架。

Conclusion: 提供了一个全面的测试平台，有望弥合严格评估和现实世界适用性之间的差距。

Abstract: Causal Representation Learning (CRL) aims to uncover the data-generating
process and identify the underlying causal variables and relations, whose
evaluation remains inherently challenging due to the requirement of known
ground-truth causal variables and causal structure. Existing evaluations often
rely on either simplistic synthetic datasets or downstream performance on
real-world tasks, generally suffering a dilemma between realism and evaluative
precision. In this paper, we introduce a new benchmark for CRL using
high-fidelity simulated visual data that retains both realistic visual
complexity and, more importantly, access to ground-truth causal generating
processes. The dataset comprises around 200 thousand images and 3 million video
frames across 24 sub-scenes in four domains: static image generation, dynamic
physical simulations, robotic manipulations, and traffic situation analysis.
These scenarios range from static to dynamic settings, simple to complex
structures, and single to multi-agent interactions, offering a comprehensive
testbed that hopefully bridges the gap between rigorous evaluation and
real-world applicability. In addition, we provide flexible access to the
underlying causal structures, allowing users to modify or configure them to
align with the required assumptions in CRL, such as available domain labels,
temporal dependencies, or intervention histories. Leveraging this benchmark, we
evaluated representative CRL methods across diverse paradigms and offered
empirical insights to assist practitioners and newcomers in choosing or
extending appropriate CRL frameworks to properly address specific types of real
problems that can benefit from the CRL perspective. Welcome to visit our:
Project page:https://causal-verse.github.io/,
Dataset:https://huggingface.co/CausalVerse.

</details>


### [167] [FedHFT: Efficient Federated Finetuning with Heterogeneous Edge Clients](https://arxiv.org/abs/2510.14054)
*Fatih Ilhan,Selim Furkan Tekin,Tiansheng Huang,Gaowen Liu,Ramana Kompella,Greg Eisenhauer,Yingyan Celine Lin,Calton Pu,Ling Liu*

Main category: cs.LG

TL;DR: 提出了一种高效且个性化的联邦微调框架FedHFT，以应对在下游任务和特定领域数据集上进行个性化自然语言理解（NLU）应用时，微调预训练大型语言模型（LLM）所面临的有限和/或异构数据以及不同客户端上可用的计算资源不同的两个主要挑战。


<details>
  <summary>Details</summary>
Motivation: 由于专有数据的保密性或隐私要求，微调的数据有限和/或异构；参与客户端（如边缘设备）可用的计算资源各不相同。

Method: 引入混合掩码适配器来处理参与客户端之间的资源异构性，从而在分布式环境中实现跨多个客户端的预训练语言模型的高性能协同微调，同时保持专有数据的本地性。引入了一种基于掩码个性化和客户端聚类的双层优化方法来处理非独立同分布数据分布。

Result: 与具有代表性的异构联邦学习方法相比，在数据和资源异构性下，各种自然语言理解任务的性能和效率都有显着提高。

Conclusion: FedHFT框架有效地解决了异构联邦学习中的数据和资源挑战，并在各种NLU任务中取得了显着的性能提升。

Abstract: Fine-tuning pre-trained large language models (LLMs) has become a common
practice for personalized natural language understanding (NLU) applications on
downstream tasks and domain-specific datasets. However, there are two main
challenges: (i) limited and/or heterogeneous data for fine-tuning due to
proprietary data confidentiality or privacy requirements, and (ii) varying
computation resources available across participating clients such as edge
devices. This paper presents FedHFT - an efficient and personalized federated
fine-tuning framework to address both challenges. First, we introduce a mixture
of masked adapters to handle resource heterogeneity across participating
clients, enabling high-performance collaborative fine-tuning of pre-trained
language model(s) across multiple clients in a distributed setting, while
keeping proprietary data local. Second, we introduce a bi-level optimization
approach to handle non-iid data distribution based on masked personalization
and client clustering. Extensive experiments demonstrate significant
performance and efficiency improvements over various natural language
understanding tasks under data and resource heterogeneity compared to
representative heterogeneous federated learning methods.

</details>


### [168] [On the expressivity of sparse maxout networks](https://arxiv.org/abs/2510.14068)
*Moritz Grillo,Tobias Hofmann*

Main category: cs.LG

TL;DR: 研究稀疏 maxout 网络的表达能力，其中每个神经元从上一层获取固定数量的输入，并采用 maxout 激活。


<details>
  <summary>Details</summary>
Motivation: 研究卷积神经网络或图神经网络的关键特性。

Method: 建立网络可计算函数与一类虚拟多胞形之间的对偶性，将其几何形状与网络表达性问题联系起来。推导出相关多胞形维数的严格界限，作为分析的中心工具。在此基础上，构建了一系列深度层次。

Result: 足够深的稀疏 maxout 网络是通用的。如果未达到所需的深度，则宽度本身无法弥补固定入度约束的稀疏性。

Conclusion: 稀疏 maxout 网络的深度是表达能力的关键因素。

Abstract: We study the expressivity of sparse maxout networks, where each neuron takes
a fixed number of inputs from the previous layer and employs a, possibly
multi-argument, maxout activation. This setting captures key characteristics of
convolutional or graph neural networks. We establish a duality between
functions computable by such networks and a class of virtual polytopes, linking
their geometry to questions of network expressivity. In particular, we derive a
tight bound on the dimension of the associated polytopes, which serves as the
central tool for our analysis. Building on this, we construct a sequence of
depth hierarchies. While sufficiently deep sparse maxout networks are
universal, we prove that if the required depth is not reached, width alone
cannot compensate for the sparsity of a fixed indegree constraint.

</details>


### [169] [Exploratory Causal Inference in SAEnce](https://arxiv.org/abs/2510.14073)
*Tommaso Mencattini,Riccardo Cadei,Francesco Locatello*

Main category: cs.LG

TL;DR: 本研究提出了一种名为神经效应搜索（Neural Effect Search）的新方法，用于直接从数据中发现治疗的未知效果，无需手动构建假设和昂贵的分析。


<details>
  <summary>Details</summary>
Motivation: 传统随机对照试验依赖于手工构建的假设和昂贵的分析，限制了大规模因果效应估计，并可能锚定在流行的但不完整的假设上。

Method: 该方法利用预训练的基础模型将试验中的非结构化数据转化为有意义的表示，并通过稀疏自编码器进行解释。引入神经效应搜索，通过渐进分层解决多重检验问题和效应纠缠。

Result: 在半合成实验中评估了算法的鲁棒性，并在实验生态学中展示了首次在真实科学试验中成功进行无监督因果效应识别。

Conclusion: 神经效应搜索是一种有效的方法，可以直接从数据中发现治疗的未知效果，并在真实世界的科学试验中成功应用。

Abstract: Randomized Controlled Trials are one of the pillars of science; nevertheless,
they rely on hand-crafted hypotheses and expensive analysis. Such constraints
prevent causal effect estimation at scale, potentially anchoring on popular yet
incomplete hypotheses. We propose to discover the unknown effects of a
treatment directly from data. For this, we turn unstructured data from a trial
into meaningful representations via pretrained foundation models and interpret
them via a sparse autoencoder. However, discovering significant causal effects
at the neural level is not trivial due to multiple-testing issues and effects
entanglement. To address these challenges, we introduce Neural Effect Search, a
novel recursive procedure solving both issues by progressive stratification.
After assessing the robustness of our algorithm on semi-synthetic experiments,
we showcase, in the context of experimental ecology, the first successful
unsupervised causal effect identification on a real-world scientific trial.

</details>


### [170] [Neural Network approximation power on homogeneous and heterogeneous reaction-diffusion equations](https://arxiv.org/abs/2510.14094)
*Haotian Feng*

Main category: cs.LG

TL;DR: 本文研究了神经网络近似反应扩散方程解的能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是虽然神经网络越来越多地被用于求解微分方程，但对其有效性的理论基础研究不足。

Method: 本文基于通用逼近定理，证明了两层神经网络可以逼近一维反应扩散方程，三层神经网络可以逼近二维反应扩散方程。

Result: 证明了两层神经网络可以逼近一维反应扩散方程，三层神经网络可以逼近二维反应扩散方程。

Conclusion: 本文强调了神经网络在逼近反应扩散方程和相关偏微分方程解方面的表达能力，为基于神经网络的微分方程求解器提供了理论基础。

Abstract: Reaction-diffusion systems represent one of the most fundamental formulations
used to describe a wide range of physical, chemical, and biological processes.
With the increasing adoption of neural networks, recent research has focused on
solving differential equations using machine learning techniques. However, the
theoretical foundation explaining why neural networks can effectively
approximate such solutions remains insufficiently explored.
  This paper provides a theoretical analysis of the approximation power of
neural networks for one- and two-dimensional reaction-diffusion equations in
both homogeneous and heterogeneous media. Building upon the universal
approximation theorem, we demonstrate that a two-layer neural network can
approximate the one-dimensional reaction-diffusion equation, while a
three-layer neural network can approximate its two-dimensional counterpart. The
theoretical framework presented here can be further extended to elliptic and
parabolic equations.
  Overall, this work highlights the expressive power of neural networks in
approximating solutions to reaction-diffusion equations and related PDEs,
providing a theoretical foundation for neural network-based differential
equation solvers.

</details>


### [171] [Unlocking Out-of-Distribution Generalization in Transformers via Recursive Latent Space Reasoning](https://arxiv.org/abs/2510.14095)
*Awni Altabaa,Siyu Chen,John Lafferty,Zhuoran Yang*

Main category: cs.LG

TL;DR: 本文研究了Transformer网络中的OOD泛化问题，并提出了一组架构机制来增强OOD泛化能力。


<details>
  <summary>Details</summary>
Motivation: 机器学习中的一个核心挑战是超越训练分布的系统性、组合泛化，这是现代语言模型涌现推理能力的关键瓶颈。本文旨在解决这个问题。

Method: 使用GSM8K风格的模块化算术在计算图任务上，研究Transformer网络中的OOD泛化，并引入和探索了一组架构机制，包括输入自适应递归、算法监督、通过离散瓶颈锚定的潜在表示和显式纠错机制。

Result: 这些机制共同产生了一种架构方法，用于Transformer网络中具有鲁棒算法泛化能力的本机和可扩展潜在空间推理。

Conclusion: 详细的机理解释性分析表明，这些机制如何产生强大的OOD泛化能力。

Abstract: Systematic, compositional generalization beyond the training distribution
remains a core challenge in machine learning -- and a critical bottleneck for
the emergent reasoning abilities of modern language models. This work
investigates out-of-distribution (OOD) generalization in Transformer networks
using a GSM8K-style modular arithmetic on computational graphs task as a
testbed. We introduce and explore a set of four architectural mechanisms aimed
at enhancing OOD generalization: (i) input-adaptive recurrence; (ii)
algorithmic supervision; (iii) anchored latent representations via a discrete
bottleneck; and (iv) an explicit error-correction mechanism. Collectively,
these mechanisms yield an architectural approach for native and scalable latent
space reasoning in Transformer networks with robust algorithmic generalization
capabilities. We complement these empirical results with a detailed mechanistic
interpretability analysis that reveals how these mechanisms give rise to robust
OOD generalization abilities.

</details>


### [172] [TENDE: Transfer Entropy Neural Diffusion Estimation](https://arxiv.org/abs/2510.14096)
*Simon Pedro Galeano Munoz,Mustapha Bounoua,Giulio Franzese,Pietro Michiardi,Maurizio Filippone*

Main category: cs.LG

TL;DR: 提出了一种新的方法TENDE，它利用基于分数的扩散模型通过条件互信息来估计转移熵。


<details>
  <summary>Details</summary>
Motivation: 现有的估计方法受维度诅咒的影响，需要限制性的分布假设，或者需要指数级大的数据集才能实现可靠的收敛。

Method: 利用基于分数的扩散模型，通过学习相关条件分布的分数函数来估计转移熵。

Result: 与现有的神经估计器和其他最先进的方法相比，在合成基准和真实数据上表现出卓越的准确性和鲁棒性。

Conclusion: TENDE提供灵活、可扩展的估计，同时对底层数据生成过程做出最小的假设。

Abstract: Transfer entropy measures directed information flow in time series, and it
has become a fundamental quantity in applications spanning neuroscience,
finance, and complex systems analysis. However, existing estimation methods
suffer from the curse of dimensionality, require restrictive distributional
assumptions, or need exponentially large datasets for reliable convergence. We
address these limitations in the literature by proposing TENDE (Transfer
Entropy Neural Diffusion Estimation), a novel approach that leverages
score-based diffusion models to estimate transfer entropy through conditional
mutual information. By learning score functions of the relevant conditional
distributions, TENDE provides flexible, scalable estimation while making
minimal assumptions about the underlying data-generating process. We
demonstrate superior accuracy and robustness compared to existing neural
estimators and other state-of-the-art approaches across synthetic benchmarks
and real data.

</details>


### [173] [Agentic Entropy-Balanced Policy Optimization](https://arxiv.org/abs/2510.14545)
*Guanting Dong,Licheng Bao,Zhongyuan Wang,Kangzhi Zhao,Xiaoxi Li,Jiajie Jin,Jinghan Yang,Hangyu Mao,Fuzheng Zhang,Kun Gai,Guorui Zhou,Yutao Zhu,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.LG

TL;DR: 提出了 Agentic Entropy-Balanced Policy Optimization (AEPO) 算法，以平衡 rollout 和策略更新阶段的熵。


<details>
  <summary>Details</summary>
Motivation: 主流 agentic RL 算法过度依赖熵信号可能导致训练崩溃。

Method: AEPO 包含动态熵平衡 rollout 机制和熵平衡策略优化。

Result: AEPO 在 14 个具有挑战性的数据集上优于 7 种主流 RL 算法。在少量样本下，Qwen3-14B 与 AEPO 结合取得了显著成果。

Conclusion: AEPO 提高了 rollout 采样多样性，同时保持了稳定的策略熵，促进了可扩展的 web agent 训练。

Abstract: Recently, Agentic Reinforcement Learning (Agentic RL) has made significant
progress in incentivizing the multi-turn, long-horizon tool-use capabilities of
web agents. While mainstream agentic RL algorithms autonomously explore
high-uncertainty tool-call steps under the guidance of entropy, excessive
reliance on entropy signals can impose further constraints, leading to the
training collapse. In this paper, we delve into the challenges caused by
entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an
agentic RL algorithm designed to balance entropy in both the rollout and policy
update phases. AEPO comprises two core components: (1) a dynamic
entropy-balanced rollout mechanism that adaptively allocate global and branch
sampling budget through entropy pre-monitoring, while imposing a branch penalty
on consecutive high-entropy tool-call steps to prevent over-branching issues;
and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient
operation into the high-entropy clipping term to preserve and properly rescale
gradients on high-entropy tokens, while incorporating entropy-aware advantage
estimation to prioritize learning on high-uncertainty tokens. Results across 14
challenging datasets show that AEPO consistently outperforms 7 mainstream RL
algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive
results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker
for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on
WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout
sampling diversity while maintaining stable policy entropy, facilitating
scalable web agent training.

</details>


### [174] [Near-Optimal Regret-Queue Length Tradeoff in Online Learning for Two-Sided Markets](https://arxiv.org/abs/2510.14097)
*Zixian Yang,Sushil Mahavir Varma,Lei Ying*

Main category: cs.LG

TL;DR: 研究一个双边市场，目标是设计定价和匹配算法，以最大化平台利润并保持合理的队列长度。提出了一种基于在线学习的定价策略，并证明了其近似最优性。


<details>
  <summary>Details</summary>
Motivation: 设计定价和匹配算法，以最大化平台利润，同时保持合理的队列长度。现实中，控制价格依赖到达率的需求和供应曲线可能是未知的。

Method: 设计了一种新颖的基于在线学习的定价策略。该策略具有两个显着特征：优化低后悔和小队列长度之间权衡的动态组件；以及解决获取快速学习的有用样本和保持小队列长度之间紧张关系概率组件。

Result: 证明了三个性能指标之间的权衡：$\\\tilde{O}(T^{1-\\gamma})$ 后悔值，$\\\tilde{O}(T^{\\gamma/2})$ 平均队列长度，以及 $\\\tilde{O}(T^{\\gamma})$ 最大队列长度，对于 $\\gamma \\in (0, 1/6]$，显着优于现有结果。在允许的 $\\gamma$ 范围内，表明后悔值和平均队列长度之间的权衡在对数因子下是最优的。

Conclusion: 所提出的策略在后悔值和平均队列长度之间实现了最优的权衡，并且该策略具有动态和概率组件，可以有效地平衡学习和队列长度之间的关系。

Abstract: We study a two-sided market, wherein, price-sensitive heterogeneous customers
and servers arrive and join their respective queues. A compatible
customer-server pair can then be matched by the platform, at which point, they
leave the system. Our objective is to design pricing and matching algorithms
that maximize the platform's profit, while maintaining reasonable queue
lengths. As the demand and supply curves governing the price-dependent arrival
rates may not be known in practice, we design a novel online-learning-based
pricing policy and establish its near-optimality. In particular, we prove a
tradeoff among three performance metrics: $\tilde{O}(T^{1-\gamma})$ regret,
$\tilde{O}(T^{\gamma/2})$ average queue length, and $\tilde{O}(T^{\gamma})$
maximum queue length for $\gamma \in (0, 1/6]$, significantly improving over
existing results [1]. Moreover, barring the permissible range of $\gamma$, we
show that this trade-off between regret and average queue length is optimal up
to logarithmic factors under a class of policies, matching the optimal one as
in [2] which assumes the demand and supply curves to be known. Our proposed
policy has two noteworthy features: a dynamic component that optimizes the
tradeoff between low regret and small queue lengths; and a probabilistic
component that resolves the tension between obtaining useful samples for fast
learning and maintaining small queue lengths.

</details>


### [175] [Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge Graphs with Hybrid Retrieval](https://arxiv.org/abs/2510.14592)
*Rashmi R,Vidyadhar Upadhya*

Main category: cs.LG

TL;DR: MAHA: A Modality-Aware Hybrid retrieval Architecture for multimodal question answering.


<details>
  <summary>Details</summary>
Motivation: Current RAG systems are limited by their focus on unimodal textual data, which is insufficient for unstructured multimodal documents containing text, images, tables, etc.

Method: MAHA integrates dense vector retrieval with structured graph traversal, using a knowledge graph to encode cross-modal semantics and relationships.

Result: MAHA outperforms baselines on benchmark datasets, achieving a ROUGE-L score of 0.486 and providing complete modality coverage.

Conclusion: MAHA establishes a scalable and interpretable retrieval framework that advances RAG systems by enabling modality-aware reasoning over unstructured multimodal data.

Abstract: Current Retrieval-Augmented Generation (RAG) systems primarily operate on
unimodal textual data, limiting their effectiveness on unstructured multimodal
documents. Such documents often combine text, images, tables, equations, and
graphs, each contributing unique information. In this work, we present a
Modality-Aware Hybrid retrieval Architecture (MAHA), designed specifically for
multimodal question answering with reasoning through a modality-aware knowledge
graph. MAHA integrates dense vector retrieval with structured graph traversal,
where the knowledge graph encodes cross-modal semantics and relationships. This
design enables both semantically rich and context-aware retrieval across
diverse modalities. Evaluations on multiple benchmark datasets demonstrate that
MAHA substantially outperforms baseline methods, achieving a ROUGE-L score of
0.486, providing complete modality coverage. These results highlight MAHA's
ability to combine embeddings with explicit document structure, enabling
effective multimodal retrieval. Our work establishes a scalable and
interpretable retrieval framework that advances RAG systems by enabling
modality-aware reasoning over unstructured multimodal data.

</details>


### [176] [Briding Diffusion Posterior Sampling and Monte Carlo methods: a survey](https://arxiv.org/abs/2510.14114)
*Yazid Janati,Alain Durmus,Jimmy Olsson,Eric Moulines*

Main category: cs.LG

TL;DR: 本文综述了使用预训练扩散模型和蒙特卡洛方法解决贝叶斯逆问题的方法，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成建模中表现出色，并有潜力作为先验解决贝叶斯逆问题。

Method: 综述了利用预训练扩散模型和蒙特卡洛方法解决贝叶斯逆问题的方法，重点关注扭曲机制。

Result: 这些方法主要采用扭曲机制来调整扩散过程中的中间分布，使其趋向后验分布。

Conclusion: 蒙特卡洛方法可以辅助从这些扭曲的分布中进行采样。

Abstract: Diffusion models enable the synthesis of highly accurate samples from complex
distributions and have become foundational in generative modeling. Recently,
they have demonstrated significant potential for solving Bayesian inverse
problems by serving as priors. This review offers a comprehensive overview of
current methods that leverage \emph{pre-trained} diffusion models alongside
Monte Carlo methods to address Bayesian inverse problems without requiring
additional training. We show that these methods primarily employ a
\emph{twisting} mechanism for the intermediate distributions within the
diffusion process, guiding the simulations toward the posterior distribution.
We describe how various Monte Carlo methods are then used to aid in sampling
from these twisted distributions.

</details>


### [177] [Neural Network-enabled Domain-consistent Robust Optimisation for Global CO$_2$ Reduction Potential of Gas Power Plants](https://arxiv.org/abs/2510.14125)
*Waqar Muhammad Ashraf,Talha Ansar,Abdulelah S. Alshehri,Peipei Chen,Ramit Debnath,Vivek Dua*

Main category: cs.LG

TL;DR: 提出了一种新的鲁棒优化框架，该框架利用神经网络驱动，将数据驱动的领域知识作为约束整合到非线性规划技术中。


<details>
  <summary>Details</summary>
Motivation: 解决参数化神经网络模型与优化求解器相互作用时产生的领域不一致解问题。

Method: 将数据驱动的领域知识作为约束整合到非线性规划技术中。

Result: 应用于一个 1180 兆瓦容量的联合循环燃气发电厂，该框架提供了领域一致的鲁棒优化解决方案，实现了经过验证的 0.76 个百分点的平均能源效率提升。将这种效率提升推广到全球燃气发电厂，我们估计每年可减少 2600 万吨二氧化碳（亚洲 1060 万吨，美洲 900 万吨，欧洲 450 万吨）。

Conclusion: 机器学习在为全球气候行动提供近期、可扩展的脱碳路径方面具有协同作用。

Abstract: We introduce a neural network-driven robust optimisation framework that
integrates data-driven domain as a constraint into the nonlinear programming
technique, addressing the overlooked issue of domain-inconsistent solutions
arising from the interaction of parametrised neural network models with
optimisation solvers. Applied to a 1180 MW capacity combined cycle gas power
plant, our framework delivers domain-consistent robust optimal solutions that
achieve a verified 0.76 percentage point mean improvement in energy efficiency.
For the first time, scaling this efficiency gain to the global fleet of gas
power plants, we estimate an annual 26 Mt reduction potential in CO$_2$ (with
10.6 Mt in Asia, 9.0 Mt in the Americas, and 4.5 Mt in Europe). These results
underscore the synergetic role of machine learning in delivering near-term,
scalable decarbonisation pathways for global climate action.

</details>


### [178] [Demystifying the Mechanisms Behind Emergent Exploration in Goal-conditioned RL](https://arxiv.org/abs/2510.14129)
*Mahsa Bastankhah,Grace Liu,Dilip Arumugam,Thomas L. Griffiths,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 本文研究了无监督强化学习中涌现探索背后的机制，重点关注 Single-Goal Contrastive Reinforcement Learning (SGCRL) 算法。


<details>
  <summary>Details</summary>
Motivation: 理解 SGCRL 如何在没有外部奖励或课程的情况下解决具有挑战性的长时程目标到达任务。

Method: 结合算法目标函数的理论分析和受控实验。

Result: SGCRL 通过其学习的表征最大化隐式奖励，这些表征自动修改奖励环境以促进探索和利用。实验表明，这些探索动态源于学习状态空间的低秩表征，而不是来自神经网络函数逼近。

Conclusion: 对 SGCRL 的理解使我们能够调整 SGCRL 以执行安全意识探索。

Abstract: In this work, we take a first step toward elucidating the mechanisms behind
emergent exploration in unsupervised reinforcement learning. We study
Single-Goal Contrastive Reinforcement Learning (SGCRL), a self-supervised
algorithm capable of solving challenging long-horizon goal-reaching tasks
without external rewards or curricula. We combine theoretical analysis of the
algorithm's objective function with controlled experiments to understand what
drives its exploration. We show that SGCRL maximizes implicit rewards shaped by
its learned representations. These representations automatically modify the
reward landscape to promote exploration before reaching the goal and
exploitation thereafter. Our experiments also demonstrate that these
exploration dynamics arise from learning low-rank representations of the state
space rather than from neural network function approximation. Our improved
understanding enables us to adapt SGCRL to perform safety-aware exploration.

</details>


### [179] [Learning Wireless Interference Patterns: Decoupled GNN for Throughput Prediction in Heterogeneous Multi-Hop p-CSMA Networks](https://arxiv.org/abs/2510.14137)
*Faezeh Dehghan Tarzjani,Bhaskar Krishnamachari*

Main category: cs.LG

TL;DR: 提出了一种新的图卷积网络（D-GCN）架构，用于解决异构多跳无线网络中饱和吞吐量预测问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型在稀疏拓扑中低估吞吐量，精确的马尔可夫链分析计算量巨大，标准GCN在异构网络上误差较高。

Method: D-GCN显式分离节点自身传输概率的处理与邻居干扰效应，用可学习的注意力机制代替平均聚合。

Result: D-GCN实现了3.3%的NMAE，优于其他基线方法，并且在精确分析方法计算不可行时仍然易于处理。

Conclusion: D-GCN能够实现基于梯度的网络优化，达到理论最优值的1%以内。

Abstract: The p-persistent CSMA protocol is central to random-access MAC analysis, but
predicting saturation throughput in heterogeneous multi-hop wireless networks
remains a hard problem. Simplified models that assume a single, shared
interference domain can underestimate throughput by 48--62\% in sparse
topologies. Exact Markov-chain analyses are accurate but scale exponentially in
computation time, making them impractical for large networks. These
computational barriers motivate structural machine learning approaches like
GNNs for scalable throughput prediction in general network topologies. Yet
off-the-shelf GNNs struggle here: a standard GCN yields 63.94\% normalized mean
absolute error (NMAE) on heterogeneous networks because symmetric normalization
conflates a node's direct interference with higher-order, cascading effects
that pertain to how interference propagates over the network graph.
  Building on these insights, we propose the Decoupled Graph Convolutional
Network (D-GCN), a novel architecture that explicitly separates processing of a
node's own transmission probability from neighbor interference effects. D-GCN
replaces mean aggregation with learnable attention, yielding interpretable,
per-neighbor contribution weights while capturing complex multihop interference
patterns. D-GCN attains 3.3\% NMAE, outperforms strong baselines, remains
tractable even when exact analytical methods become computationally infeasible,
and enables gradient-based network optimization that achieves within 1\% of
theoretical optima.

</details>


### [180] [Inferred global dense residue transition graphs from primary structure sequences enable protein interaction prediction via directed graph convolutional neural networks](https://arxiv.org/abs/2510.14139)
*Islam Akef Ebeid,Haoteng Tang,Pengfei Gu*

Main category: cs.LG

TL;DR: 提出了一种新的蛋白质-蛋白质相互作用（PPI）预测框架，该框架使用基于n-gram图的蛋白质表示和专门设计的有向图卷积神经网络（DirectGCN）。


<details>
  <summary>Details</summary>
Motivation: 现有PPI预测方法计算量大，要么使用蛋白质语言模型的直接序列嵌入，要么使用图神经网络处理3D蛋白质结构，因此需要计算强度较低的替代方案。

Method: 该框架包括两个阶段：1) ProtGram，将蛋白质的 primary 结构建模为 n-gram 图的层次结构，其中残基转移概率定义边权重；2) DirectGCN，一种定制的有向图卷积神经网络，通过单独的路径特定转换（incoming、outgoing 和 undirected）处理信息，并通过可学习的门控机制组合这些路径。

Result: DirectGCN 在标准节点分类基准测试中表现良好，尤其是在复杂、有向图中。ProtGram-DirectGCN 框架在 PPI 预测中表现出强大的预测能力，即使在有限的训练数据下也是如此。

Conclusion: 该研究提出了一种有效的 PPI 预测方法，该方法计算强度较低，并且在数据有限的情况下也能保持强大的预测能力。

Abstract: Introduction Accurate prediction of protein-protein interactions (PPIs) is
crucial for understanding cellular functions and advancing drug development.
Existing in-silico methods use direct sequence embeddings from Protein Language
Models (PLMs). Others use Graph Neural Networks (GNNs) for 3D protein
structures. This study explores less computationally intensive alternatives. We
introduce a novel framework for downstream PPI prediction through link
prediction. Methods We introduce a two-stage graph representation learning
framework, ProtGram-DirectGCN. First, we developed ProtGram. This approach
models a protein's primary structure as a hierarchy of globally inferred n-gram
graphs. In these graphs, residue transition probabilities define edge weights.
Each edge connects a pair of residues in a directed graph. The probabilities
are aggregated from a large corpus of sequences. Second, we propose DirectGCN,
a custom directed graph convolutional neural network. This model features a
unique convolutional layer. It processes information through separate
path-specific transformations: incoming, outgoing, and undirected. A shared
transformation is also applied. These paths are combined via a learnable gating
mechanism. We apply DirectGCN to ProtGram graphs to learn residue-level
embeddings. These embeddings are pooled via attention to generate protein-level
embeddings for prediction. Results We first established the efficacy of
DirectGCN on standard node classification benchmarks. Its performance matches
established methods on general datasets. The model excels at complex, directed
graphs with dense, heterophilic structures. When applied to PPI prediction, the
full ProtGram-DirectGCN framework delivers robust predictive power. This strong
performance holds even with limited training data.

</details>


### [181] [On Evaluating Loss Functions for Stock Ranking: An Empirical Analysis With Transformer Model](https://arxiv.org/abs/2510.14156)
*Jan Kwiatkowski,Jarosław A. Chudziak*

Main category: cs.LG

TL;DR: 本文研究了不同的训练损失函数如何影响Transformer模型对股票收益率进行排序的能力，以用于量化交易策略。


<details>
  <summary>Details</summary>
Motivation: 金融市场具有动态性和复杂性，传统的损失函数不能直接学习股票收益率的正确顺序。需要研究更高级的排序损失函数。

Method: 本文系统地评估了一系列高级损失函数，包括点式、对式和列表式损失函数，用于每日股票收益率预测，并在标普500数据上进行排名投资组合选择。

Result: 本文全面评估了不同的损失函数如何影响模型学习横截面和时间模式的能力，从而为优化基于排名的交易策略提供实用指导。

Conclusion: 不同的损失函数对模型学习股票收益率排序的影响不同，本文的研究为优化量化交易策略提供了指导。

Abstract: Quantitative trading strategies rely on accurately ranking stocks to identify
profitable investments. Effective portfolio management requires models that can
reliably order future stock returns. Transformer models are promising for
understanding financial time series, but how different training loss functions
affect their ability to rank stocks well is not yet fully understood. Financial
markets are challenging due to their changing nature and complex relationships
between stocks. Standard loss functions, which aim for simple prediction
accuracy, often aren't enough. They don't directly teach models to learn the
correct order of stock returns. While many advanced ranking losses exist from
fields such as information retrieval, there hasn't been a thorough comparison
to see how well they work for ranking financial returns, especially when used
with modern Transformer models for stock selection. This paper addresses this
gap by systematically evaluating a diverse set of advanced loss functions
including pointwise, pairwise, listwise for daily stock return forecasting to
facilitate rank-based portfolio selection on S&P 500 data. We focus on
assessing how each loss function influences the model's ability to discern
profitable relative orderings among assets. Our research contributes a
comprehensive benchmark revealing how different loss functions impact a model's
ability to learn cross-sectional and temporal patterns crucial for portfolio
selection, thereby offering practical guidance for optimizing ranking-based
trading strategies.

</details>


### [182] [Data Understanding Survey: Pursuing Improved Dataset Characterization Via Tensor-based Methods](https://arxiv.org/abs/2510.14161)
*Matthew D. Merris,Tim Andersen*

Main category: cs.LG

TL;DR: 这篇论文探讨了传统数据集表征方法的局限性，并提倡采用基于张量的方法以更深入地理解复杂数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集表征方法无法提供创新和可解释性所需的深刻理解和洞察力。

Method: 论文调查了当前最先进的传统数据分析技术，并讨论了基于张量的方法。

Result: 通过实例，展示了张量方法如何揭示细致的数据特征，从而提供增强的可解释性和可操作的智能。

Conclusion: 论文提倡采用基于张量的表征方法，从而在理解复杂数据集方面取得飞跃，并为智能、可解释的数据驱动发现铺平道路。

Abstract: In the evolving domains of Machine Learning and Data Analytics, existing
dataset characterization methods such as statistical, structural, and
model-based analyses often fail to deliver the deep understanding and insights
essential for innovation and explainability. This work surveys the current
state-of-the-art conventional data analytic techniques and examines their
limitations, and discusses a variety of tensor-based methods and how these may
provide a more robust alternative to traditional statistical, structural, and
model-based dataset characterization techniques. Through examples, we
illustrate how tensor methods unveil nuanced data characteristics, offering
enhanced interpretability and actionable intelligence. We advocate for the
adoption of tensor-based characterization, promising a leap forward in
understanding complex datasets and paving the way for intelligent, explainable
data-driven discoveries.

</details>


### [183] [Towards Reversible Model Merging For Low-rank Weights](https://arxiv.org/abs/2510.14163)
*Mohammadsajad Alipour,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: 提出了一种新的模型合并方法，用于解决将低秩模型合并后性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的模型合并方法在应用于低秩权重时会导致严重的性能下降。

Method: 提出了一种根本不同的方法：构建一个紧凑的基，原始的特定于任务的模型可以通过线性组合从中恢复。将合并重新定义为生成一个具有重建能力的模型空间，而不是生成一个单一的合并模型。

Result: 在各种数据集和模型规模上的大量实验表明，RMM始终优于现有的合并方法，并且能够显著地保持低秩压缩模型的性能。

Conclusion: RMM 是一种高效、无数据且灵活的方法，它为选择模型权重的最佳基础和线性组合的特定于任务的系数提供了闭式解。

Abstract: Model merging aims to combine multiple fine-tuned models into a single set of
weights that performs well across all source tasks. While prior work has shown
that merging can approximate the performance of individual fine-tuned models
for each task, it largely overlooks scenarios where models are compressed into
low-rank representations, either through low-rank adaptation (LoRA) or
post-training singular value decomposition (SVD). We first demonstrate that
applying conventional merging methods to low-rank weights leads to severe
performance degradation in the merged model. Motivated by this phenomenon, we
propose a fundamentally different approach: instead of collapsing all adapters
into one set of weights, we construct a compact basis (e.g., an equivalent of
holding two or more models) from which original task-specific models can be
recovered via linear combination. This reframes merging as generating a
reconstruction-capable model space rather than producing a single merged model.
Crucially, this allows us to ``revert'' to each individual model when needed,
recognizing that no merged model can consistently outperform one specialized
for its task. Building on this insight, we introduce our method, Reversible
Model Merging (RMM), an efficient, data-free, and flexible method that provides
a closed-form solution for selecting the optimal basis of model weights and
task-specific coefficients for linear combination. Extensive experiments across
diverse datasets and model scales demonstrate that RMM consistently outperforms
existing merging approaches, preserving the performance of low-rank compressed
models by a significant margin.

</details>


### [184] [Optimal Control Theoretic Neural Optimizer: From Backpropagation to Dynamic Programming](https://arxiv.org/abs/2510.14168)
*Guan-Horng Liu,Tianrong Chen,Evangelos A. Theodorou*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于最优控制理论的神经网络优化器(OCNOpt)，该优化器通过将反向传播与动态规划联系起来，探索Bellman方程的高阶展开。


<details>
  <summary>Details</summary>
Motivation: 将深度神经网络的优化问题置于最优控制编程框架内，并利用动态系统理论为数值方程和物理提供理论基础。

Method: 通过将反向传播算法与动态规划的最优性条件联系起来，导出一个新的优化方法，该方法探索Bellman方程的高阶展开。

Result: OCNOpt 在鲁棒性和效率方面优于现有方法，同时保持可控的计算复杂度。

Conclusion: OCNOpt 为基于动态系统和最优控制理论的算法设计开辟了新途径，并为分层反馈策略、博弈论应用和连续时间模型（如神经 ODE）的更高阶训练提供了丰富的算法机会。

Abstract: Optimization of deep neural networks (DNNs) has been a driving force in the
advancement of modern machine learning and artificial intelligence. With DNNs
characterized by a prolonged sequence of nonlinear propagation, determining
their optimal parameters given an objective naturally fits within the framework
of Optimal Control Programming. Such an interpretation of DNNs as dynamical
systems has proven crucial in offering a theoretical foundation for principled
analysis from numerical equations to physics. In parallel to these theoretical
pursuits, this paper focuses on an algorithmic perspective. Our motivated
observation is the striking algorithmic resemblance between the Backpropagation
algorithm for computing gradients in DNNs and the optimality conditions for
dynamical systems, expressed through another backward process known as dynamic
programming. Consolidating this connection, where Backpropagation admits a
variational structure, solving an approximate dynamic programming up to the
first-order expansion leads to a new class of optimization methods exploring
higher-order expansions of the Bellman equation. The resulting optimizer,
termed Optimal Control Theoretic Neural Optimizer (OCNOpt), enables rich
algorithmic opportunities, including layer-wise feedback policies,
game-theoretic applications, and higher-order training of continuous-time
models such as Neural ODEs. Extensive experiments demonstrate that OCNOpt
improves upon existing methods in robustness and efficiency while maintaining
manageable computational complexity, paving new avenues for principled
algorithmic design grounded in dynamical systems and optimal control theory.

</details>


### [185] [MAFA: A Multi-Agent Framework for Enterprise-Scale Annotation with Configurable Task Adaptation](https://arxiv.org/abs/2510.14184)
*Mahmood Hegazy,Aaron Rodrigues,Azzam Naeem*

Main category: cs.LG

TL;DR: MAFA是一个多智能体标注框架，通过可配置的智能体协作，解决了金融服务中大规模的标注积压问题。


<details>
  <summary>Details</summary>
Motivation: 解决金融服务中数百万客户话语需要准确分类的标注积压这一关键挑战。

Method: 结合了专业智能体、结构化推理和基于判断的共识机制。该框架支持动态任务调整，允许组织通过配置定义自定义标注类型，而无需更改代码。

Result: 消除了100万条话语的积压，同时平均实现了与人工标注者86%的一致性，每年节省超过5,000小时的人工标注工作。在内部意图分类数据集中，Top-1准确率提高了13.8%，Top-5准确率提高了15.1%，F1提高了16.9%。

Conclusion: 弥合了理论多智能体系统与实际企业部署之间的差距，为面临类似标注挑战的组织提供了蓝图。

Abstract: We present MAFA (Multi-Agent Framework for Annotation), a production-deployed
system that transforms enterprise-scale annotation workflows through
configurable multi-agent collaboration. Addressing the critical challenge of
annotation backlogs in financial services, where millions of customer
utterances require accurate categorization, MAFA combines specialized agents
with structured reasoning and a judge-based consensus mechanism. Our framework
uniquely supports dynamic task adaptation, allowing organizations to define
custom annotation types (FAQs, intents, entities, or domain-specific
categories) through configuration rather than code changes. Deployed at JP
Morgan Chase, MAFA has eliminated a 1 million utterance backlog while
achieving, on average, 86% agreement with human annotators, annually saving
over 5,000 hours of manual annotation work. The system processes utterances
with annotation confidence classifications, which are typically 85% high, 10%
medium, and 5% low across all datasets we tested. This enables human annotators
to focus exclusively on ambiguous and low-coverage cases. We demonstrate MAFA's
effectiveness across multiple datasets and languages, showing consistent
improvements over traditional and single-agent annotation baselines: 13.8%
higher Top-1 accuracy, 15.1% improvement in Top-5 accuracy, and 16.9% better F1
in our internal intent classification dataset and similar gains on public
benchmarks. This work bridges the gap between theoretical multi-agent systems
and practical enterprise deployment, providing a blueprint for organizations
facing similar annotation challenges.

</details>


### [186] [Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation](https://arxiv.org/abs/2510.14190)
*Ruchi Sandilya,Sumaira Perez,Charles Lynch,Lindsay Victoria,Benjamin Zebley,Derrick Matthew Buchanan,Mahendra T. Bhati,Nolan Williams,Timothy J. Spellman,Faith M. Gunning,Conor Liston,Logan Grosenick*

Main category: cs.LG

TL;DR: ConDA: Uses contrastive learning in diffusion model embeddings to align the latent space with system dynamics, enabling interpretable control.


<details>
  <summary>Details</summary>
Motivation: Diffusion model latent spaces lack explicit organization for interpretable control. Contrastive objectives can recover disentangled representations.

Method: Applies contrastive learning within diffusion embeddings to align latent geometry with system dynamics. Enables nonlinear trajectory traversal.

Result: Improved controllability compared to linear traversals and conditioning-based baselines across benchmarks in fluid dynamics, neural calcium imaging, therapeutic neurostimulation, and facial expression.

Conclusion: Diffusion latents encode dynamics-relevant structure, but exploiting it requires latent organization and manifold traversal.

Abstract: Diffusion models excel at generation, but their latent spaces are not
explicitly organized for interpretable control. We introduce ConDA (Contrastive
Diffusion Alignment), a framework that applies contrastive learning within
diffusion embeddings to align latent geometry with system dynamics. Motivated
by recent advances showing that contrastive objectives can recover more
disentangled and structured representations, ConDA organizes diffusion latents
such that traversal directions reflect underlying dynamical factors. Within
this contrastively structured space, ConDA enables nonlinear trajectory
traversal that supports faithful interpolation, extrapolation, and controllable
generation. Across benchmarks in fluid dynamics, neural calcium imaging,
therapeutic neurostimulation, and facial expression, ConDA produces
interpretable latent representations with improved controllability compared to
linear traversals and conditioning-based baselines. These results suggest that
diffusion latents encode dynamics-relevant structure, but exploiting this
structure requires latent organization and traversal along the latent manifold.

</details>


### [187] [Incentive-Based Federated Learning](https://arxiv.org/abs/2510.14208)
*Chanuka A. S. Hewa Kaluannakkage,Rajkumar Buyya*

Main category: cs.LG

TL;DR: 联邦学习在保护数据隐私的同时，实现了协作模型训练。然而，由于参与者困境等关键因素，实际适应性可能会受到限制。参与实体通常不愿为学习系统做出贡献，除非他们获得一些好处，或者他们可能会假装参与并搭便车。


<details>
  <summary>Details</summary>
Motivation: 确定联邦学习系统中激励机制设计的根本挑战。

Method: 从经济学和博弈论的基本概念，以及区块链和深度强化学习等技术驱动的解决方案。

Result: 提出了一个全面的分类法，彻底涵盖了基于上述理论概念的集中式和分散式架构。此外，所描述的概念是从应用的角度提出的，涵盖了新兴的工业应用，包括医疗保健、智能基础设施、车辆网络和基于区块链的去中心化系统。

Conclusion: 精心设计的激励机制不仅是可选功能，而且是联邦学习实际成功的关键组成部分。

Abstract: Federated learning promises to revolutionize machine learning by enabling
collaborative model training without compromising data privacy. However,
practical adaptability can be limited by critical factors, such as the
participation dilemma. Participating entities are often unwilling to contribute
to a learning system unless they receive some benefits, or they may pretend to
participate and free-ride on others. This chapter identifies the fundamental
challenges in designing incentive mechanisms for federated learning systems. It
examines how foundational concepts from economics and game theory can be
applied to federated learning, alongside technology-driven solutions such as
blockchain and deep reinforcement learning. This work presents a comprehensive
taxonomy that thoroughly covers both centralized and decentralized
architectures based on the aforementioned theoretical concepts. Furthermore,
the concepts described are presented from an application perspective, covering
emerging industrial applications, including healthcare, smart infrastructure,
vehicular networks, and blockchain-based decentralized systems. Through this
exploration, this chapter demonstrates that well-designed incentive mechanisms
are not merely optional features but essential components for the practical
success of federated learning. This analysis reveals both the promising
solutions that have emerged and the significant challenges that remain in
building truly sustainable, fair, and robust federated learning ecosystems.

</details>


### [188] [Spectral Analysis of Molecular Kernels: When Richer Features Do Not Guarantee Better Generalization](https://arxiv.org/abs/2510.14217)
*Asma Jamali,Tin Sum Cheng,Rodrigo A. Vargas-Hernández*

Main category: cs.LG

TL;DR: 本文对分子核的谱性质进行了全面的分析，发现更丰富的谱特征并不一定能提高预测精度，有时甚至会产生负相关。


<details>
  <summary>Details</summary>
Motivation: 了解核的谱性质有助于从原理上理解泛化和表征质量。尽管深度模型在分子性质预测方面取得了最先进的精度，但核方法因其在低数据状态下的鲁棒性和透明的理论基础而被广泛使用。然而，对分子核的系统谱分析仍然很少。

Method: 本文对QM9数据集上的核岭回归进行了首次全面的谱分析，研究了分子指纹、预训练的基于transformer的模型、全局和局部3D表征在七种分子性质上的表现。

Result: 研究表明，更丰富的谱特征并不一定能提高精度。对于基于transformer的和局部3D表征，谱丰富度甚至可能与性能负相关。此外，保留前2%的特征值即可恢复几乎所有性能，表明主要的特征值捕获了最丰富的信息。

Conclusion: 研究结果挑战了“更丰富的频谱产生更好的泛化”这一常见启发式方法，并强调了表征、核特征和预测性能之间细微的关系。这些发现为评估数据有限的科学和实际任务中的核方法和自监督学习方法提供了信息。

Abstract: Understanding the spectral properties of kernels offers a principled
perspective on generalization and representation quality. While deep models
achieve state-of-the-art accuracy in molecular property prediction, kernel
methods remain widely used for their robustness in low-data regimes and
transparent theoretical grounding. Despite extensive studies of kernel spectra
in machine learning, systematic spectral analyses of molecular kernels are
scarce. In this work, we provide the first comprehensive spectral analysis of
kernel ridge regression on the QM9 dataset, molecular fingerprint, pretrained
transformer-based, global and local 3D representations across seven molecular
properties. Surprisingly, richer spectral features, measured by four different
spectral metrics, do not consistently improve accuracy. Pearson correlation
tests further reveal that for transformer-based and local 3D representations,
spectral richness can even have a negative correlation with performance. We
also implement truncated kernels to probe the relationship between spectrum and
predictive performance: in many kernels, retaining only the top 2% of
eigenvalues recovers nearly all performance, indicating that the leading
eigenvalues capture the most informative features. Our results challenge the
common heuristic that "richer spectra yield better generalization" and
highlight nuanced relationships between representation, kernel features, and
predictive performance. Beyond molecular property prediction, these findings
inform how kernel and self-supervised learning methods are evaluated in
data-limited scientific and real-world tasks.

</details>


### [189] [When Flatness Does (Not) Guarantee Adversarial Robustness](https://arxiv.org/abs/2510.14231)
*Nils Philipp Walter,Linara Adilova,Jilles Vreeken,Michael Kamp*

Main category: cs.LG

TL;DR: 神经网络容易受到对抗扰动的影响。有假设认为损失面中的平坦最小值区域可以提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络的脆弱性以及平坦最小值与鲁棒性之间的关系。

Method: 通过严格的形式化关系，推导了倒数第二层中相对平坦度的闭式表达式，并用它来约束输入空间中损失的变化。

Result: 平坦性仅意味着局部而非全局对抗鲁棒性。为了保持局部邻域之外的鲁棒性，损失需要从数据流形中急剧弯曲。

Conclusion: 这项研究挑战了对平坦度的简化看法，并提供了对其在鲁棒性中作用的细致理解。

Abstract: Despite their empirical success, neural networks remain vulnerable to small,
adversarial perturbations. A longstanding hypothesis suggests that flat minima,
regions of low curvature in the loss landscape, offer increased robustness.
While intuitive, this connection has remained largely informal and incomplete.
By rigorously formalizing the relationship, we show this intuition is only
partially correct: flatness implies local but not global adversarial
robustness. To arrive at this result, we first derive a closed-form expression
for relative flatness in the penultimate layer, and then show we can use this
to constrain the variation of the loss in input space. This allows us to
formally analyze the adversarial robustness of the entire network. We then show
that to maintain robustness beyond a local neighborhood, the loss needs to
curve sharply away from the data manifold. We validate our theoretical
predictions empirically across architectures and datasets, uncovering the
geometric structure that governs adversarial vulnerability, and linking
flatness to model confidence: adversarial examples often lie in large, flat
regions where the model is confidently wrong. Our results challenge simplified
views of flatness and provide a nuanced understanding of its role in
robustness.

</details>


### [190] [Scaling Test-Time Compute to Achieve IOI Gold Medal with Open-Weight Models](https://arxiv.org/abs/2510.14232)
*Mehrzad Samadi,Aleksander Ficek,Sean Narenthiran,Siddhartha Jain,Wasi Uddin Ahmad,Somshubra Majumdar,Vahid Noroozi,Boris Ginsburg*

Main category: cs.LG

TL;DR: 本文提出了一个名为GenCluster的可扩展、可复现的测试时计算框架，该框架使用开放权重模型达到了国际信息学奥林匹克竞赛（IOI）金牌水平。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLM）的推理和解决问题能力，并缩小开放系统和封闭系统之间的差距。先前的专有模型声称已达到IOI金牌水平，但方法未公开，而开放权重模型难以达到类似结果。

Method: 结合大规模生成、行为聚类、排序和循环提交策略，在有限的验证预算下有效地探索不同的解决方案空间。

Result: GenCluster的性能随可用计算资源而持续扩展，缩小了开放系统和封闭系统之间的差距。使用开放权重模型gpt-oss-120b首次在IOI 2025上获得金牌。

Conclusion: GenCluster为LLM推理的透明和可复现评估设定了新的基准。

Abstract: Competitive programming has become a rigorous benchmark for evaluating the
reasoning and problem-solving capabilities of large language models (LLMs). The
International Olympiad in Informatics (IOI) stands out as one of the most
prestigious annual competitions in competitive programming and has become a key
benchmark for comparing human and AI-level programming ability. While several
proprietary models have been claimed to achieve gold medal-level performance at
the IOI, often with undisclosed methods, achieving comparable results with
open-weight models remains a significant challenge. In this paper, we present
\gencluster, a scalable and reproducible test-time compute framework that
attains IOI gold-level performance using open-weight models. It combines
large-scale generation, behavioral clustering, ranking, and a round-robin
submission strategy to efficiently explore diverse solution spaces under
limited validation budgets. Our experiments show that the performance of our
proposed approach scales consistently with available compute, narrowing the gap
between open and closed systems. Notably, we will show that GenCluster can
achieve a gold medal at IOI 2025 for the first time with an open-weight model
gpt-oss-120b, setting a new benchmark for transparent and reproducible
evaluation of reasoning in LLMs.

</details>


### [191] [Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation](https://arxiv.org/abs/2510.14246)
*Jingwen Gu,Yiting He,Zhishuai Liu,Pan Xu*

Main category: cs.LG

TL;DR: 本文提出了一种名为DR-RPO的在线策略优化算法，用于解决强化学习中分布偏移下的决策问题，该算法通过参考策略正则化和线性函数逼近等技术，实现了样本高效和次线性后悔，并在理论和实践上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在分布偏移下进行决策是一个核心挑战，尤其是在训练和部署环境不同时。尽管策略优化在标准强化学习中取得了成功，但在鲁棒强化学习中，其理论和实验探索仍然不足。

Method: 本文提出了一种名为DR-RPO的无模型在线策略优化方法，它结合了参考策略正则化、d-矩形线性MDP公式、线性函数逼近和乐观探索的上置信 बोनस，以实现可处理的优化和扩展性。

Result: 本文提供了理论保证，表明策略优化可以在鲁棒强化学习中实现多项式次优界限和样本效率，与基于价值的方法的性能相匹配。在各种领域中的实验结果证实了该理论并证明了DR-RPO的鲁棒性。

Conclusion: 本文提出的DR-RPO算法是一种有效的在线策略优化方法，可以在分布偏移下学习鲁棒策略，并在理论和实践上都取得了良好的效果。

Abstract: Decision-making under distribution shift is a central challenge in
reinforcement learning (RL), where training and deployment environments differ.
We study this problem through the lens of robust Markov decision processes
(RMDPs), which optimize performance against adversarial transition dynamics.
Our focus is the online setting, where the agent has only limited interaction
with the environment, making sample efficiency and exploration especially
critical. Policy optimization, despite its success in standard RL, remains
theoretically and empirically underexplored in robust RL. To bridge this gap,
we propose \textbf{D}istributionally \textbf{R}obust \textbf{R}egularized
\textbf{P}olicy \textbf{O}ptimization algorithm (DR-RPO), a model-free online
policy optimization method that learns robust policies with sublinear regret.
To enable tractable optimization within the softmax policy class, DR-RPO
incorporates reference-policy regularization, yielding RMDP variants that are
doubly constrained in both transitions and policies. To scale to large
state-action spaces, we adopt the $d$-rectangular linear MDP formulation and
combine linear function approximation with an upper confidence bonus for
optimistic exploration. We provide theoretical guarantees showing that policy
optimization can achieve polynomial suboptimality bounds and sample efficiency
in robust RL, matching the performance of value-based approaches. Finally,
empirical results across diverse domains corroborate our theory and demonstrate
the robustness of DR-RPO.

</details>


### [192] [A Physics Prior-Guided Dual-Stream Attention Network for Motion Prediction of Elastic Bragg Breakwaters](https://arxiv.org/abs/2510.14250)
*Lianzi Jiang,Jianxin Zhang,Xinyu Han,Huanhe Dong,Xiangrong Wang*

Main category: cs.LG

TL;DR: 提出了一种新的用于弹性布拉格防波堤运动响应预测的深度学习模型，该模型考虑了海洋系统中的自然衰减和波浪-结构相互作用。


<details>
  <summary>Details</summary>
Motivation: 传统的深度学习模型在面对未见过的海况时泛化能力有限，因为它们忽略了海洋系统中的自然衰减，并且对波浪-结构相互作用建模不足。

Method: 提出了一个物理先验引导的双流注意力网络 (PhysAttnNet)，它包含一个衰减双向自注意力 (DBSA) 模块和一个相位差引导的双向交叉注意力 (PDG-BCA) 模块，并通过全局上下文融合 (GCF) 模块进行整合。该模型还使用混合时频损失进行训练。

Result: 在波浪水槽数据集上的综合实验表明，PhysAttnNet 显著优于主流模型。跨场景泛化测试验证了模型的鲁棒性和对未见环境的适应性。

Conclusion: PhysAttnNet 有潜力成为开发海洋工程中复杂系统预测模型的框架。

Abstract: Accurate motion response prediction for elastic Bragg breakwaters is critical
for their structural safety and operational integrity in marine environments.
However, conventional deep learning models often exhibit limited generalization
capabilities when presented with unseen sea states. These deficiencies stem
from the neglect of natural decay observed in marine systems and inadequate
modeling of wave-structure interaction (WSI). To overcome these challenges,
this study proposes a novel Physics Prior-Guided Dual-Stream Attention Network
(PhysAttnNet). First, the decay bidirectional self-attention (DBSA) module
incorporates a learnable temporal decay to assign higher weights to recent
states, aiming to emulate the natural decay phenomenon. Meanwhile, the phase
differences guided bidirectional cross-attention (PDG-BCA) module explicitly
captures the bidirectional interaction and phase relationship between waves and
the structure using a cosine-based bias within a bidirectional
cross-computation paradigm. These streams are synergistically integrated
through a global context fusion (GCF) module. Finally, PhysAttnNet is trained
with a hybrid time-frequency loss that jointly minimizes time-domain prediction
errors and frequency-domain spectral discrepancies. Comprehensive experiments
on wave flume datasets demonstrate that PhysAttnNet significantly outperforms
mainstream models. Furthermore,cross-scenario generalization tests validate the
model's robustness and adaptability to unseen environments, highlighting its
potential as a framework to develop predictive models for complex systems in
ocean engineering.

</details>


### [193] [Generalist vs Specialist Time Series Foundation Models: Investigating Potential Emergent Behaviors in Assessing Human Health Using PPG Signals](https://arxiv.org/abs/2510.14254)
*Saurabh Kataria,Yi Wu,Zhaoliang Chen,Hyunjung Gloria Kwak,Yuhao Xu,Lovely Yeswanth Panchumarthi,Ran Xiao,Jiaying Lu,Ayca Ermis,Anni Zhao,Runze Yan,Alex Federov,Zewen Liu,Xu Wu,Wei Jin,Carl Yang,Jocelyn Grunwell,Stephanie R. Brown,Amit Shah,Craig Jabaley,Tim Buchman,Sivasubramanium V Bhavani,Randall J. Lee,Xiao Hu*

Main category: cs.LG

TL;DR: 本研究对比了通用和专用时间序列基础模型在光电容积脉搏波 (PPG) 信号上的性能。


<details>
  <summary>Details</summary>
Motivation: 评估通用和专用时间序列模型在生理传感中的适用性，特别是对于PPG信号。

Method: 通过51个任务的综合基准测试，在七个维度上评估模型，包括胜率、平均性能、特征质量、调整增益、性能方差、可迁移性和可扩展性。

Result: 在全微调的情况下，专用模型获得了高27%的胜率。

Conclusion: 该研究全面评估了通用和专用时间序列基础模型的优势和局限性，为不同下游场景提供了整体理解。

Abstract: Foundation models are large-scale machine learning models that are
pre-trained on massive amounts of data and can be adapted for various
downstream tasks. They have been extensively applied to tasks in Natural
Language Processing and Computer Vision with models such as GPT, BERT, and
CLIP. They are now also increasingly gaining attention in time-series analysis,
particularly for physiological sensing. However, most time series foundation
models are specialist models - with data in pre-training and testing of the
same type, such as Electrocardiogram, Electroencephalogram, and
Photoplethysmogram (PPG). Recent works, such as MOMENT, train a generalist time
series foundation model with data from multiple domains, such as weather,
traffic, and electricity. This paper aims to conduct a comprehensive
benchmarking study to compare the performance of generalist and specialist
models, with a focus on PPG signals. Through an extensive suite of total 51
tasks covering cardiac state assessment, laboratory value estimation, and
cross-modal inference, we comprehensively evaluate both models across seven
dimensions, including win score, average performance, feature quality, tuning
gain, performance variance, transferability, and scalability. These metrics
jointly capture not only the models' capability but also their adaptability,
robustness, and efficiency under different fine-tuning strategies, providing a
holistic understanding of their strengths and limitations for diverse
downstream scenarios. In a full-tuning scenario, we demonstrate that the
specialist model achieves a 27% higher win score. Finally, we provide further
analysis on generalization, fairness, attention visualizations, and the
importance of training data choice.

</details>


### [194] [CAST: Compositional Analysis via Spectral Tracking for Understanding Transformer Layer Functions](https://arxiv.org/abs/2510.14262)
*Zihao Fu,Ming Liao,Chris Russell,Zhenguang G. Cai*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为CAST的新框架，用于分析Transformer模型的层功能，通过直接估计变换矩阵和综合频谱分析。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型取得了显著的成功，但其内部机制仍然很大程度上是黑盒子，难以理解。

Method: 该方法通过Moore-Penrose伪逆估计每一层的变换矩阵，并应用具有六个可解释指标的频谱分析来表征层行为。

Result: 分析表明，仅编码器模型和仅解码器模型之间存在不同的行为，解码器模型表现出压缩-扩展周期，而编码器模型保持一致的高秩处理。内核分析进一步证明了层之间的功能关系模式，CKA相似性矩阵将层清楚地划分为三个阶段：特征提取、压缩和专业化。

Conclusion: CAST框架通过估计变换矩阵和频谱分析，为理解Transformer模型的层功能提供了一种新的视角，补充了现有的可解释性方法。

Abstract: Large language models have achieved remarkable success but remain largely
black boxes with poorly understood internal mechanisms. To address this
limitation, many researchers have proposed various interpretability methods
including mechanistic analysis, probing classifiers, and activation
visualization, each providing valuable insights from different perspectives.
Building upon this rich landscape of complementary approaches, we introduce
CAST (Compositional Analysis via Spectral Tracking), a probe-free framework
that contributes a novel perspective by analyzing transformer layer functions
through direct transformation matrix estimation and comprehensive spectral
analysis. CAST offers complementary insights to existing methods by estimating
the realized transformation matrices for each layer using Moore-Penrose
pseudoinverse and applying spectral analysis with six interpretable metrics
characterizing layer behavior. Our analysis reveals distinct behaviors between
encoder-only and decoder-only models, with decoder models exhibiting
compression-expansion cycles while encoder models maintain consistent high-rank
processing. Kernel analysis further demonstrates functional relationship
patterns between layers, with CKA similarity matrices clearly partitioning
layers into three phases: feature extraction, compression, and specialization.

</details>


### [195] [Nonparametric Data Attribution for Diffusion Models](https://arxiv.org/abs/2510.14269)
*Yutian Zhao,Chao Du,Xiaosen Zheng,Tianyu Pang,Min Lin*

Main category: cs.LG

TL;DR: 提出了一种新的非参数数据归因方法，用于生成模型，该方法仅基于数据操作，通过生成图像和训练图像之间的patch-level相似性来衡量影响。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型归因方法通常需要访问模型梯度或重新训练，这限制了它们在专有或大规模环境中的应用。

Method: 该方法基于最优得分函数的解析形式，并自然地扩展到多尺度表示，同时通过基于卷积的加速保持计算效率。

Result: 实验表明，该方法实现了强大的归因性能，与基于梯度的方法紧密匹配，并且大大优于现有的非参数基线。

Conclusion: 该框架揭示了反映训练数据和输出之间内在关系的模式，独立于任何特定模型，并且可以产生空间上可解释的归因。

Abstract: Data attribution for generative models seeks to quantify the influence of
individual training examples on model outputs. Existing methods for diffusion
models typically require access to model gradients or retraining, limiting
their applicability in proprietary or large-scale settings. We propose a
nonparametric attribution method that operates entirely on data, measuring
influence via patch-level similarity between generated and training images. Our
approach is grounded in the analytical form of the optimal score function and
naturally extends to multiscale representations, while remaining
computationally efficient through convolution-based acceleration. In addition
to producing spatially interpretable attributions, our framework uncovers
patterns that reflect intrinsic relationships between training data and
outputs, independent of any specific model. Experiments demonstrate that our
method achieves strong attribution performance, closely matching gradient-based
approaches and substantially outperforming existing nonparametric baselines.
Code is available at https://github.com/sail-sg/NDA.

</details>


### [196] [Stable Prediction of Adverse Events in Medical Time-Series Data](https://arxiv.org/abs/2510.14286)
*Mayank Keoliya,Seewon Choi,Rajeev Alur,Mayur Naik,Eric Wong*

Main category: cs.LG

TL;DR: 本文介绍了一个新的早期事件预测 (EEP) 基准测试 CAREBench，该基准测试使用多模态输入评估部署能力，并评估时间稳定性以及预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试忽略了风险评分的稳定性，并且主要在表格输入上进行评估，从而导致轨迹行为未经测试。为了解决这个问题，我们引入了 CAREBench。

Method: 我们提出了一个稳定性指标，该指标量化了每位患者风险的短期可变性，并根据局部 Lipschitz 常数惩罚突然振荡。

Result: 跨任务，现有方法，特别是 LLM，难以共同优化准确性和稳定性，尤其是在高精度操作点上的召回率较差。

Conclusion: 这些结果表明，需要产生与证据一致的稳定轨迹的模型，以赢得临床医生对连续监测环境的信任。

Abstract: Early event prediction (EEP) systems continuously estimate a patient's
imminent risk to support clinical decision-making. For bedside trust, risk
trajectories must be accurate and temporally stable, shifting only with new,
relevant evidence. However, current benchmarks (a) ignore stability of risk
scores and (b) evaluate mainly on tabular inputs, leaving trajectory behavior
untested. To address this gap, we introduce CAREBench, an EEP benchmark that
evaluates deployability using multi-modal inputs-tabular EHR, ECG waveforms,
and clinical text-and assesses temporal stability alongside predictive
accuracy. We propose a stability metric that quantifies short-term variability
in per-patient risk and penalizes abrupt oscillations based on local-Lipschitz
constants. CAREBench spans six prediction tasks such as sepsis onset and
compares classical learners, deep sequence models, and zero-shot LLMs. Across
tasks, existing methods, especially LLMs, struggle to jointly optimize accuracy
and stability, with notably poor recall at high-precision operating points.
These results highlight the need for models that produce evidence-aligned,
stable trajectories to earn clinician trust in continuous monitoring settings.
(Code: https://github.com/SeewonChoi/CAREBench.)

</details>


### [197] [Enhancing Time-Series Anomaly Detection by Integrating Spectral-Residual Bottom-Up Attention with Reservoir Computing](https://arxiv.org/abs/2510.14287)
*Hayato Nihei,Sou Nobukawa,Yusuke Sakemi,Kazuyuki Aihara*

Main category: cs.LG

TL;DR: 本文提出了一种新的时间序列异常检测方法，该方法将谱残差（SR）方法与储层计算（RC）相结合，称为SR-RC，以提高RC的异常检测性能，同时不牺牲学习效率。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上，单独使用RC实现足够的异常检测性能可能需要一个过大的储层。虽然注意力机制可以提高准确性，但它们可能需要大量的计算，并破坏RC的学习效率。

Method: 该研究提出了一种谱残差RC（SR-RC），它将谱残差（SR）方法（一种无学习、自下而上的注意力机制）与RC相结合。

Result: SR-RC在基准任务和真实世界的时间序列数据集上优于传统的RC和基于SR方法提取的值的logistic回归模型。

Conclusion: SR-RC为部署RC作为时间序列异常检测的边缘AI提供了一个实际的方向，因为它和RC一样，非常适合硬件实现。

Abstract: Reservoir computing (RC) establishes the basis for the processing of
time-series data by exploiting the high-dimensional spatiotemporal response of
a recurrent neural network to an input signal. In particular, RC trains only
the output layer weights. This simplicity has drawn attention especially in
Edge Artificial Intelligence (AI) applications. Edge AI enables time-series
anomaly detection in real time, which is important because detection delays can
lead to serious incidents. However, achieving adequate anomaly-detection
performance with RC alone may require an unacceptably large reservoir on
resource-constrained edge devices. Without enlarging the reservoir, attention
mechanisms can improve accuracy, although they may require substantial
computation and undermine the learning efficiency of RC. In this study, to
improve the anomaly detection performance of RC without sacrificing learning
efficiency, we propose a spectral residual RC (SR-RC) that integrates the
spectral residual (SR) method - a learning-free, bottom-up attention mechanism
- with RC. We demonstrated that SR-RC outperformed conventional RC and
logistic-regression models based on values extracted by the SR method across
benchmark tasks and real-world time-series datasets. Moreover, because the SR
method, similarly to RC, is well suited for hardware implementation, SR-RC
suggests a practical direction for deploying RC as Edge AI for time-series
anomaly detection.

</details>


### [198] [TED++: Submanifold-Aware Backdoor Detection via Layerwise Tubular-Neighbourhood Screening](https://arxiv.org/abs/2510.14299)
*Nam Le,Leo Yu Zhang,Kewen Liao,Shirui Pan,Wei Luo*

Main category: cs.LG

TL;DR: 提出了一种新的框架TED++，用于检测隐蔽的后门攻击，即使在数据稀缺的情况下也能实现最先进的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的防御措施在攻击者利用细微的基于距离的异常或当干净的例子稀缺时容易受到攻击，因此需要新的防御方法。

Method: 通过构建围绕每个类别的隐藏特征流形的管状邻域，估计其局部“厚度”，然后应用局部自适应排序(LAR)来检测任何漂移到容许管之外的激活。通过聚合所有层的LAR调整后的排名，TED++捕捉输入在演化类子流形上的忠实程度。

Result: 在基准数据集和任务上进行了广泛的实验，证明TED++在自适应攻击和有限数据场景下实现了最先进的检测性能。即使每个类只有五个保留的例子，TED++仍然提供了接近完美的检测，与次优方法相比，AUROC提高了高达14%。

Conclusion: TED++ 是一种有效的检测隐蔽后门攻击的框架，即使在数据稀缺的情况下也能实现最先进的检测性能。

Abstract: As deep neural networks power increasingly critical applications, stealthy
backdoor attacks, where poisoned training inputs trigger malicious model
behaviour while appearing benign, pose a severe security risk. Many existing
defences are vulnerable when attackers exploit subtle distance-based anomalies
or when clean examples are scarce. To meet this challenge, we introduce TED++,
a submanifold-aware framework that effectively detects subtle backdoors that
evade existing defences. TED++ begins by constructing a tubular neighbourhood
around each class's hidden-feature manifold, estimating its local ``thickness''
from a handful of clean activations. It then applies Locally Adaptive Ranking
(LAR) to detect any activation that drifts outside the admissible tube. By
aggregating these LAR-adjusted ranks across all layers, TED++ captures how
faithfully an input remains on the evolving class submanifolds. Based on such
characteristic ``tube-constrained'' behaviour, TED++ flags inputs whose
LAR-based ranking sequences deviate significantly. Extensive experiments are
conducted on benchmark datasets and tasks, demonstrating that TED++ achieves
state-of-the-art detection performance under both adaptive-attack and
limited-data scenarios. Remarkably, even with only five held-out examples per
class, TED++ still delivers near-perfect detection, achieving gains of up to
14\% in AUROC over the next-best method. The code is publicly available at
https://github.com/namle-w/TEDpp.

</details>


### [199] [Active Measuring in Reinforcement Learning With Delayed Negative Effects](https://arxiv.org/abs/2510.14315)
*Daiqi Gao,Ziping Xu,Aseel Rawashdeh,Predrag Klasnja,Susan A. Murphy*

Main category: cs.LG

TL;DR: 提出Actively Observable Markov Decision Process (AOMDP)，其中智能体选择控制动作，并决定是否测量潜在状态。测量行动揭示了真实的潜在状态，但可能对环境产生负面的延迟影响。


<details>
  <summary>Details</summary>
Motivation: 在现实环境中，强化学习 (RL) 中测量状态的成本可能很高，并且可能对未来的结果产生负面影响。

Method: 将 AOMDP 公式化为周期性部分可观察 MDP，并提出了一种基于信念状态的在线 RL 算法。为了近似信念状态，我们进一步提出了一种 sequential Monte Carlo 方法，以联合近似未知静态环境参数和未观察到的潜在状态的后验。

Result: 证明了减少不确定性可以显著提高样本效率，并增加最佳策略的价值，尽管存在这些成本。

Conclusion: 在数字健康应用程序中评估了所提出的算法，其中智能体决定何时提供数字干预以及何时通过调查评估用户的健康状况。

Abstract: Measuring states in reinforcement learning (RL) can be costly in real-world
settings and may negatively influence future outcomes. We introduce the
Actively Observable Markov Decision Process (AOMDP), where an agent not only
selects control actions but also decides whether to measure the latent state.
The measurement action reveals the true latent state but may have a negative
delayed effect on the environment. We show that this reduced uncertainty may
provably improve sample efficiency and increase the value of the optimal policy
despite these costs. We formulate an AOMDP as a periodic partially observable
MDP and propose an online RL algorithm based on belief states. To approximate
the belief states, we further propose a sequential Monte Carlo method to
jointly approximate the posterior of unknown static environment parameters and
unobserved latent states. We evaluate the proposed algorithm in a digital
health application, where the agent decides when to deliver digital
interventions and when to assess users' health status through surveys.

</details>
