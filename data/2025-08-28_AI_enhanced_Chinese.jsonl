{"id": "2508.19365", "categories": ["cs.IR", "cs.CY", "H.3.3"], "pdf": "https://arxiv.org/pdf/2508.19365", "abs": "https://arxiv.org/abs/2508.19365", "authors": ["Emaan Hariri", "Daniel E. Ho"], "title": "AI for Statutory Simplification: A Comprehensive State Legal Corpus and Labor Benchmark", "comment": "10 pages, 3 figures. To appear in ICAIL 2025", "summary": "One of the emerging use cases of AI in law is for code simplification:\nstreamlining, distilling, and simplifying complex statutory or regulatory\nlanguage. One U.S. state has claimed to eliminate one third of its state code\nusing AI. Yet we lack systematic evaluations of the accuracy, reliability, and\nrisks of such approaches. We introduce LaborBench, a question-and-answer\nbenchmark dataset designed to evaluate AI capabilities in this domain. We\nleverage a unique data source to create LaborBench: a dataset updated annually\nby teams of lawyers at the U.S. Department of Labor, who compile differences in\nunemployment insurance laws across 50 states for over 101 dimensions in a\nsix-month process, culminating in a 200-page publication of tables. Inspired by\nour collaboration with one U.S. state to explore using large language models\n(LLMs) to simplify codes in this domain, where complexity is particularly\nacute, we transform the DOL publication into LaborBench. This provides a unique\nbenchmark for AI capacity to conduct, distill, and extract realistic statutory\nand regulatory information. To assess the performance of retrieval augmented\ngeneration (RAG) approaches, we also compile StateCodes, a novel and\ncomprehensive state statute and regulatory corpus of 8.7 GB, enabling much more\nsystematic research into state codes. We then benchmark the performance of\ninformation retrieval and state-of-the-art large LLMs on this data and show\nthat while these models are helpful as preliminary research for code\nsimplification, the overall accuracy is far below the touted promises for LLMs\nas end-to-end pipelines for regulatory simplification.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86LaborBench\u6570\u636e\u96c6\u548cStateCodes\u8bed\u6599\u5e93\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u5728\u7b80\u5316\u6cd5\u89c4\u4ee3\u7801\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7ed3\u679c\u8868\u660e\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u51c6\u786e\u7387\u8fdc\u672a\u8fbe\u5230\u9884\u671f\u3002", "motivation": "\u7f3a\u4e4f\u5bf9\u6b64\u7c7b\u65b9\u6cd5\u51c6\u786e\u6027\u3001\u53ef\u9760\u6027\u548c\u98ce\u9669\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002\u7f8e\u56fd\u4e00\u4e2a\u5dde\u58f0\u79f0\u4f7f\u7528\u4eba\u5de5\u667a\u80fd\u6d88\u9664\u4e86\u5176\u4e09\u5206\u4e4b\u4e00\u7684\u5dde\u4ee3\u7801\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aLaborBench\u7684\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u7f16\u5236\u4e86\u4e00\u4e2a8.7 GB\u7684\u65b0\u578b\u7efc\u5408\u5dde\u6cd5\u89c4\u548c\u76d1\u7ba1\u8bed\u6599\u5e93StateCodes\uff0c\u4ee5\u8bc4\u4f30\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u65b9\u6cd5\u7684\u6027\u80fd\u3002", "result": "\u5bf9\u4fe1\u606f\u68c0\u7d22\u548c\u6700\u5148\u8fdb\u7684\u5927\u578bLLM\u5728\u6b64\u6570\u636e\u4e0a\u7684\u6027\u80fd\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8868\u660e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u6cd5\u89c4\u7b80\u5316\u7684\u7aef\u5230\u7aef\u7ba1\u9053\u7684\u6574\u4f53\u51c6\u786e\u7387\u8fdc\u4f4e\u4e8e\u5ba3\u4f20\u7684\u627f\u8bfa\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u6709\u52a9\u4e8e\u4ee3\u7801\u7b80\u5316\u7684\u521d\u6b65\u7814\u7a76\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u6cd5\u89c4\u7b80\u5316\u7684\u7aef\u5230\u7aef\u7ba1\u9053\u7684\u6574\u4f53\u51c6\u786e\u7387\u8fdc\u4f4e\u4e8e\u5ba3\u4f20\u7684\u627f\u8bfa\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u6709\u52a9\u4e8e\u4ee3\u7801\u7b80\u5316\u7684\u521d\u6b65\u7814\u7a76\u3002"}}
{"id": "2508.19399", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.19399", "abs": "https://arxiv.org/abs/2508.19399", "authors": ["Tobias Vente", "Michael Heep", "Abdullah Abbas", "Theodor Sperle", "Joeran Beel", "Bart Goethals"], "title": "APS Explorer: Navigating Algorithm Performance Spaces for Informed Dataset Selection", "comment": null, "summary": "Dataset selection is crucial for offline recommender system experiments, as\nmismatched data (e.g., sparse interaction scenarios require datasets with low\nuser-item density) can lead to unreliable results. Yet, 86\\% of ACM RecSys 2024\npapers provide no justification for their dataset choices, with most relying on\njust four datasets: Amazon (38\\%), MovieLens (34\\%), Yelp (15\\%), and Gowalla\n(12\\%). While Algorithm Performance Spaces (APS) were proposed to guide dataset\nselection, their adoption has been limited due to the absence of an intuitive,\ninteractive tool for APS exploration. Therefore, we introduce the APS Explorer,\na web-based visualization tool for interactive APS exploration, enabling\ndata-driven dataset selection. The APS Explorer provides three interactive\nfeatures: (1) an interactive PCA plot showing dataset similarity via\nperformance patterns, (2) a dynamic meta-feature table for dataset comparisons,\nand (3) a specialized visualization for pairwise algorithm performance.", "AI": {"tldr": "This paper introduces the APS Explorer, a tool for data-driven dataset selection in recommender systems, addressing the lack of justification for dataset choices in research.", "motivation": "Dataset selection is crucial for offline recommender system experiments, as mismatched data can lead to unreliable results. Yet, 86% of ACM RecSys 2024 papers provide no justification for their dataset choices, with most relying on just four datasets", "method": "a web-based visualization tool for interactive APS exploration", "result": "The APS Explorer provides three interactive features: (1) an interactive PCA plot showing dataset similarity via performance patterns, (2) a dynamic meta-feature table for dataset comparisons, and (3) a specialized visualization for pairwise algorithm performance.", "conclusion": "We introduce the APS Explorer, a web-based visualization tool for interactive APS exploration, enabling data-driven dataset selection."}}
{"id": "2508.19507", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19507", "abs": "https://arxiv.org/abs/2508.19507", "authors": ["Kyungho Kim", "Sunwoo Kim", "Geon Lee", "Kijung Shin"], "title": "A Self-Supervised Mixture-of-Experts Framework for Multi-behavior Recommendation", "comment": "CIKM 2025", "summary": "In e-commerce, where users face a vast array of possible item choices,\nrecommender systems are vital for helping them discover suitable items they\nmight otherwise overlook. While many recommender systems primarily rely on a\nuser's purchase history, recent multi-behavior recommender systems incorporate\nvarious auxiliary user behaviors, such as item clicks and cart additions, to\nenhance recommendations. Despite their overall performance gains, their\neffectiveness varies considerably between visited items (i.e., those a user has\ninteracted with through auxiliary behaviors) and unvisited items (i.e., those\nwith which the user has had no such interactions). Specifically, our analysis\nreveals that (1) existing multi-behavior recommender systems exhibit a\nsignificant gap in recommendation quality between the two item types (visited\nand unvisited items) and (2) achieving strong performance on both types with a\nsingle model architecture remains challenging. To tackle these issues, we\npropose a novel multi-behavior recommender system, MEMBER. It employs a\nmixture-of-experts framework, with experts designed to recommend the two item\ntypes, respectively. Each expert is trained using a self-supervised method\nspecialized for its design goal. In our comprehensive experiments, we show the\neffectiveness of MEMBER across both item types, achieving up to 65.46\\%\nperformance gain over the best competitor in terms of Hit Ratio@20.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u884c\u4e3a\u63a8\u8350\u7cfb\u7edfMEMBER\uff0c\u8be5\u7cfb\u7edf\u53ef\u4ee5\u6709\u6548\u5730\u63d0\u5347\u8bbf\u95ee\u8fc7\u7684\u9879\u76ee\u548c\u672a\u8bbf\u95ee\u8fc7\u7684\u9879\u76ee\u7684\u63a8\u8350\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u884c\u4e3a\u63a8\u8350\u7cfb\u7edf\u5728\u8bbf\u95ee\u8fc7\u7684\u9879\u76ee\u548c\u672a\u8bbf\u95ee\u8fc7\u7684\u9879\u76ee\u4e4b\u95f4\u7684\u63a8\u8350\u8d28\u91cf\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u5e76\u4e14\u4f7f\u7528\u5355\u4e00\u6a21\u578b\u67b6\u6784\u5728\u8fd9\u4e24\u79cd\u7c7b\u578b\u4e0a\u90fd\u53d6\u5f97\u826f\u597d\u7684\u6027\u80fd\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u4e13\u5bb6\u88ab\u8bbe\u8ba1\u7528\u6765\u5206\u522b\u63a8\u8350\u4e24\u79cd\u7c7b\u578b\u7684\u9879\u76ee\u3002\u6bcf\u4e2a\u4e13\u5bb6\u90fd\u4f7f\u7528\u4e13\u95e8\u4e3a\u5176\u8bbe\u8ba1\u76ee\u6807\u91cf\u8eab\u5b9a\u5236\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "MEMBER\u5728\u4e24\u79cd\u9879\u76ee\u7c7b\u578b\u4e0a\u90fd\u663e\u793a\u51fa\u4e86\u6709\u6548\u6027\uff0c\u5728Hit Ratio@20\u65b9\u9762\uff0c\u4e0e\u6700\u4f73\u7ade\u4e89\u8005\u76f8\u6bd4\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe65.46%\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u884c\u4e3a\u63a8\u8350\u7cfb\u7edfMEMBER\uff0c\u5b83\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u4e13\u5bb6\u88ab\u8bbe\u8ba1\u7528\u6765\u5206\u522b\u63a8\u8350\u4e24\u79cd\u7c7b\u578b\u7684\u9879\u76ee\u3002\u5728\u7efc\u5408\u5b9e\u9a8c\u4e2d\uff0cMEMBER\u5728\u4e24\u79cd\u9879\u76ee\u7c7b\u578b\u4e0a\u90fd\u663e\u793a\u51fa\u4e86\u6709\u6548\u6027\uff0c\u5728Hit Ratio@20\u65b9\u9762\uff0c\u4e0e\u6700\u4f73\u7ade\u4e89\u8005\u76f8\u6bd4\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe65.46%\u3002"}}
{"id": "2508.19268", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19268", "abs": "https://arxiv.org/abs/2508.19268", "authors": ["Qing Wang", "Xue Han", "Jiahui Wang", "Lehao Xing", "Qian Hu", "Lianlian Zhang", "Chao Deng", "Junlan Feng"], "title": "MultiPL-MoE: Multi-Programming-Lingual Extension of Large Language Models through Hybrid Mixture-of-Experts", "comment": null, "summary": "Despite LLMs' excellent code creation capabilities, multilingual code\ngeneration remains extremely challenging. To address this, we intent to improve\nthe multi-programming-lingual (MultiPL) performance of the base LLMs while\nretaining the most popular ones using restricted computational resources. We\nconsider MultiPL to be a special case of multiple natural languages and propose\na MultiPL extension of LLMs utilizing a hybrid mixture of experts (MoE), called\nMultiPL-MoE. Specifically, MultiPL-MoE combines two paired MoEs to optimize\nexpert selection at both the token and segment levels. The token-level MoE is a\nstandard upcycling MoE structure with a shared expert and a novel gate weight\nnormalization approach that aids in the final fusion with the segment-level\nMoE. The segment-level MoE incorporates two innovative designs to better\ncapture the syntactic structure and contextual patterns of programming\nlanguages: First, using a sliding window to partition the input token sequence\ninto multiple segments; Then, adopting an expert-choice routing strategy that\nallows experts to select the top-k segments. The results of the experiment\nproved the effectiveness of MultiPL-MoE.", "AI": {"tldr": "This paper introduces MultiPL-MoE, a hybrid MoE approach to improve multilingual code generation in LLMs, which combines token-level and segment-level MoEs with innovative designs.", "motivation": "Multilingual code generation remains extremely challenging. To address this, we intent to improve the multi-programming-lingual (MultiPL) performance of the base LLMs while retaining the most popular ones using restricted computational resources. We consider MultiPL to be a special case of multiple natural languages", "method": "We propose a MultiPL extension of LLMs utilizing a hybrid mixture of experts (MoE), called MultiPL-MoE. Specifically, MultiPL-MoE combines two paired MoEs to optimize expert selection at both the token and segment levels.", "result": "The token-level MoE is a standard upcycling MoE structure with a shared expert and a novel gate weight normalization approach that aids in the final fusion with the segment-level MoE. The segment-level MoE incorporates two innovative designs to better capture the syntactic structure and contextual patterns of programming languages: First, using a sliding window to partition the input token sequence into multiple segments; Then, adopting an expert-choice routing strategy that allows experts to select the top-k segments.", "conclusion": "The experiment results proved the effectiveness of MultiPL-MoE."}}
{"id": "2508.19539", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.19539", "abs": "https://arxiv.org/abs/2508.19539", "authors": ["Payam Pourashraf", "Bamshad Mobasher"], "title": "A Hybrid Recommendation Framework for Enhancing User Engagement in Local News", "comment": null, "summary": "Local news organizations face an urgent need to boost reader engagement amid\ndeclining circulation and competition from global media. Personalized news\nrecommender systems offer a promising solution by tailoring content to user\ninterests. Yet, conventional approaches often emphasize general preferences and\nmay overlook nuanced or eclectic interests in local news.\n  We propose a hybrid news recommender that integrates local and global\npreference models to improve engagement. Building on evidence of the value of\nlocalized models, our method unifies local and non-local predictors in one\nframework. The system adaptively combines recommendations from a local model,\nspecialized in region-specific content, and a global model that captures\nbroader preferences. Ensemble strategies and multiphase training balance the\ntwo.\n  We evaluated the model on two datasets: a synthetic set based on Syracuse\nnewspaper distributions and a Danish dataset (EB-NeRD) labeled for local and\nnon-local content with an LLM. Results show our integrated approach outperforms\nsingle-model baselines in accuracy and coverage, suggesting improved\npersonalization that can drive user engagement.\n  The findings have practical implications for publishers, especially local\noutlets. By leveraging both community-specific and general user interests, the\nhybrid recommender can deliver more relevant content, increasing retention and\nsubscriptions. In sum, this work introduces a new direction for recommender\nsystems, bridging local and global models to revitalize local news consumption\nthrough scalable, personalized user experiences.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u65b0\u95fb\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u96c6\u6210\u672c\u5730\u548c\u5168\u5c40\u504f\u597d\u6a21\u578b\u6765\u63d0\u9ad8\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5355\u6a21\u578b\u57fa\u7ebf\u3002", "motivation": "\u672c\u5730\u65b0\u95fb\u673a\u6784\u9762\u4e34\u7740\u63d0\u9ad8\u8bfb\u8005\u53c2\u4e0e\u5ea6\u7684\u8feb\u5207\u9700\u6c42\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u4e86\u672c\u5730\u65b0\u95fb\u4e2d\u7ec6\u81f4\u6216\u6298\u8877\u7684\u5174\u8da3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u65b0\u95fb\u63a8\u8350\u5668\uff0c\u8be5\u63a8\u8350\u5668\u96c6\u6210\u4e86\u672c\u5730\u548c\u5168\u5c40\u504f\u597d\u6a21\u578b\uff0c\u7edf\u4e00\u4e86\u672c\u5730\u548c\u975e\u672c\u5730\u9884\u6d4b\u5668\uff0c\u5e76\u901a\u8fc7\u96c6\u6210\u7b56\u7565\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\u6765\u5e73\u8861\u4e24\u8005\u3002", "result": "\u5728\u57fa\u4e8e\u9521\u62c9\u4e18\u5179\u62a5\u7eb8\u5206\u5e03\u7684\u5408\u6210\u6570\u636e\u96c6\u548c\u4f7f\u7528LLM\u6807\u8bb0\u7684\u4e39\u9ea6\u6570\u636e\u96c6(EB-NeRD)\u4e0a\u8bc4\u4f30\u4e86\u8be5\u6a21\u578b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u96c6\u6210\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u8986\u76d6\u7387\u65b9\u9762\u4f18\u4e8e\u5355\u6a21\u578b\u57fa\u7ebf\u3002", "conclusion": "\u96c6\u6210\u672c\u5730\u548c\u5168\u5c40\u504f\u597d\u6a21\u578b\u7684\u65b0\u95fb\u63a8\u8350\u7cfb\u7edf\u80fd\u591f\u63d0\u9ad8\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u4e3a\u672c\u5730\u65b0\u95fb\u673a\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u63d0\u4f9b\u66f4\u76f8\u5173\u7684\u5185\u5bb9\u6765\u589e\u52a0\u7528\u6237\u7559\u5b58\u548c\u8ba2\u9605\u91cf\u3002"}}
{"id": "2508.19270", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19270", "abs": "https://arxiv.org/abs/2508.19270", "authors": ["Nguyen Huu Nhat Minh", "Tran Nguyen Anh", "Truong Dinh Dung", "Vo Van Nam", "Le Pham Tuyen"], "title": "Whisper based Cross-Lingual Phoneme Recognition between Vietnamese and English", "comment": null, "summary": "Cross-lingual phoneme recognition has emerged as a significant challenge for\naccurate automatic speech recognition (ASR) when mixing Vietnamese and English\npronunciations. Unlike many languages, Vietnamese relies on tonal variations to\ndistinguish word meanings, whereas English features stress patterns and\nnon-standard pronunciations that hinder phoneme alignment between the two\nlanguages. To address this challenge, we propose a novel bilingual speech\nrecognition approach with two primary contributions: (1) constructing a\nrepresentative bilingual phoneme set that bridges the differences between\nVietnamese and English phonetic systems; (2) designing an end-to-end system\nthat leverages the PhoWhisper pre-trained encoder for deep high-level\nrepresentations to improve phoneme recognition. Our extensive experiments\ndemonstrate that the proposed approach not only improves recognition accuracy\nin bilingual speech recognition for Vietnamese but also provides a robust\nframework for addressing the complexities of tonal and stress-based phoneme\nrecognition", "AI": {"tldr": "A new bilingual speech recognition approach is proposed to improve the accuracy of Vietnamese phoneme recognition by constructing a bilingual phoneme set and using an end-to-end system with the PhoWhisper pre-trained encoder.", "motivation": "Cross-lingual phoneme recognition has emerged as a significant challenge for accurate automatic speech recognition (ASR) when mixing Vietnamese and English pronunciations. Unlike many languages, Vietnamese relies on tonal variations to distinguish word meanings, whereas English features stress patterns and non-standard pronunciations that hinder phoneme alignment between the two languages.", "method": "A novel bilingual speech recognition approach with two primary contributions: (1) constructing a representative bilingual phoneme set that bridges the differences between Vietnamese and English phonetic systems; (2) designing an end-to-end system that leverages the PhoWhisper pre-trained encoder for deep high-level representations to improve phoneme recognition.", "result": "The proposed approach improves recognition accuracy in bilingual speech recognition for Vietnamese.", "conclusion": "The proposed approach improves recognition accuracy in bilingual speech recognition for Vietnamese and provides a robust framework for addressing the complexities of tonal and stress-based phoneme recognition."}}
{"id": "2508.19254", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.19254", "abs": "https://arxiv.org/abs/2508.19254", "authors": ["Jookyung Song", "Mookyoung Kang", "Nojun Kwak"], "title": "Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration", "comment": "6 pages, 4 figures, NeurIPS Creative AI Track 2025", "summary": "This paper presents a real-time generative drawing system that interprets and\nintegrates both formal intent - the structural, compositional, and stylistic\nattributes of a sketch - and contextual intent - the semantic and thematic\nmeaning inferred from its visual content - into a unified transformation\nprocess. Unlike conventional text-prompt-based generative systems, which\nprimarily capture high-level contextual descriptions, our approach\nsimultaneously analyzes ground-level intuitive geometric features such as line\ntrajectories, proportions, and spatial arrangement, and high-level semantic\ncues extracted via vision-language models. These dual intent signals are\njointly conditioned in a multi-stage generation pipeline that combines\ncontour-preserving structural control with style- and content-aware image\nsynthesis. Implemented with a touchscreen-based interface and distributed\ninference architecture, the system achieves low-latency, two-stage\ntransformation while supporting multi-user collaboration on shared canvases.\nThe resulting platform enables participants, regardless of artistic expertise,\nto engage in synchronous, co-authored visual creation, redefining human-AI\ninteraction as a process of co-creation and mutual enhancement.", "AI": {"tldr": "This paper introduces a real-time generative drawing system that combines formal and contextual intent for co-creative human-AI interaction.", "motivation": "Unlike conventional text-prompt-based generative systems, which primarily capture high-level contextual descriptions, our approach simultaneously analyzes ground-level intuitive geometric features such as line trajectories, proportions, and spatial arrangement, and high-level semantic cues extracted via vision-language models.", "method": "This paper presents a real-time generative drawing system that interprets and integrates both formal intent and contextual intent into a unified transformation process.", "result": "Implemented with a touchscreen-based interface and distributed inference architecture, the system achieves low-latency, two-stage transformation while supporting multi-user collaboration on shared canvases.", "conclusion": "The resulting platform enables participants, regardless of artistic expertise, to engage in synchronous, co-authored visual creation, redefining human-AI interaction as a process of co-creation and mutual enhancement."}}
{"id": "2508.19249", "categories": ["cs.LG", "math.DS", "stat.ME", "stat.ML", "37M99"], "pdf": "https://arxiv.org/pdf/2508.19249", "abs": "https://arxiv.org/abs/2508.19249", "authors": ["Jonas S\u00f8eborg Nielsen", "Marcus Galea Jacobsen", "Albert Brincker Olson", "Mads Peter S\u00f8rensen", "Allan Peter Engsig-Karup"], "title": "Physics-Informed Regression: Parameter Estimation in Parameter-Linear Nonlinear Dynamic Models", "comment": "For public PIR Julia package, see\n  https://github.com/MarcusGalea/PhysicsInformedRegression.jl", "summary": "We present a new efficient hybrid parameter estimation method based on the\nidea, that if nonlinear dynamic models are stated in terms of a system of\nequations that is linear in terms of the parameters, then regularized ordinary\nleast squares can be used to estimate these parameters from time series data.\nWe introduce the term \"Physics-Informed Regression\" (PIR) to describe the\nproposed data-driven hybrid technique as a way to bridge theory and data by use\nof ordinary least squares to efficiently perform parameter estimation of the\nmodel coefficients of different parameter-linear models; providing examples of\nmodels based on nonlinear ordinary equations (ODE) and partial differential\nequations (PDE). The focus is on parameter estimation on a selection of ODE and\nPDE models, each illustrating performance in different model characteristics.\nFor two relevant epidemic models of different complexity and number of\nparameters, PIR is tested and compared against the related technique,\nphysics-informed neural networks (PINN), both on synthetic data generated from\nknown target parameters and on real public Danish time series data collected\nduring the COVID-19 pandemic in Denmark. Both methods were able to estimate the\ntarget parameters, while PIR showed to perform noticeably better, especially on\na compartment model with higher complexity. Given the difference in\ncomputational speed, it is concluded that the PIR method is superior to PINN\nfor the models considered. It is also demonstrated how PIR can be applied to\nestimate the time-varying parameters of a compartment model that is fitted\nusing real Danish data from the COVID-19 pandemic obtained during a period from\n2020 to 2021. The study shows how data-driven and physics-informed techniques\nmay support reliable and fast -- possibly real-time -- parameter estimation in\nparameter-linear nonlinear dynamic models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u6548\u6df7\u5408\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\uff0c\u79f0\u4e3a\u7269\u7406\u4fe1\u606f\u56de\u5f52\uff08PIR\uff09\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u3002", "motivation": "\u4e3a\u4e86\u6865\u63a5\u7406\u8bba\u548c\u6570\u636e\uff0c\u901a\u8fc7\u4f7f\u7528\u666e\u901a\u6700\u5c0f\u4e8c\u4e58\u6cd5\u6709\u6548\u5730\u6267\u884c\u4e0d\u540c\u53c2\u6570\u7ebf\u6027\u6a21\u578b\u7684\u6a21\u578b\u7cfb\u6570\u7684\u53c2\u6570\u4f30\u8ba1\u3002", "method": "Physics-Informed Regression (PIR)", "result": "PIR\u5728\u4f30\u8ba1\u76ee\u6807\u53c2\u6570\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u66f4\u9ad8\u590d\u6742\u6027\u7684\u9694\u5ba4\u6a21\u578b\u4e0a\u3002PIR\u8fd8\u53ef\u4ee5\u7528\u4e8e\u4f30\u8ba1\u4f7f\u75282020\u5e74\u81f32021\u5e74\u671f\u95f4\u83b7\u5f97\u7684COVID-19\u5927\u6d41\u884c\u7684\u771f\u5b9e\u4e39\u9ea6\u6570\u636e\u62df\u5408\u7684\u9694\u5ba4\u6a21\u578b\u7684\u65f6\u53d8\u53c2\u6570\u3002", "conclusion": "PIR\u65b9\u6cd5\u5728\u6240\u8003\u8651\u7684\u6a21\u578b\u4e2d\u4f18\u4e8ePINN\uff0c\u56e0\u4e3a\u5b83\u5728\u8ba1\u7b97\u901f\u5ea6\u4e0a\u6709\u4f18\u52bf\u3002\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u6570\u636e\u9a71\u52a8\u548c\u7269\u7406\u4fe1\u606f\u6280\u672f\u5982\u4f55\u5728\u53c2\u6570\u7ebf\u6027\u975e\u7ebf\u6027\u52a8\u6001\u6a21\u578b\u4e2d\u652f\u6301\u53ef\u9760\u548c\u5feb\u901f\u7684\u53c2\u6570\u4f30\u8ba1\u3002"}}
{"id": "2508.19316", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; I.2.4"], "pdf": "https://arxiv.org/pdf/2508.19316", "abs": "https://arxiv.org/abs/2508.19316", "authors": ["Shreyans Jain", "Alexandra Yost", "Amirali Abdullah"], "title": "Sycophancy as compositions of Atomic Psychometric Traits", "comment": "8 pages, 4 figures", "summary": "Sycophancy is a key behavioral risk in LLMs, yet is often treated as an\nisolated failure mode that occurs via a single causal mechanism. We instead\npropose modeling it as geometric and causal compositions of psychometric traits\nsuch as emotionality, openness, and agreeableness - similar to factor\ndecomposition in psychometrics. Using Contrastive Activation Addition (CAA), we\nmap activation directions to these factors and study how different combinations\nmay give rise to sycophancy (e.g., high extraversion combined with low\nconscientiousness). This perspective allows for interpretable and compositional\nvector-based interventions like addition, subtraction and projection; that may\nbe used to mitigate safety-critical behaviors in LLMs.", "AI": {"tldr": "This paper models sycophancy in LLMs as compositions of psychometric traits and proposes interventions to mitigate it.", "motivation": "Sycophancy is a key behavioral risk in LLMs, yet is often treated as an isolated failure mode.", "method": "Using Contrastive Activation Addition (CAA), the paper maps activation directions to psychometric factors.", "result": "The paper studies how different combinations of psychometric traits may give rise to sycophancy.", "conclusion": "This paper proposes interpretable and compositional vector-based interventions like addition, subtraction and projection to mitigate safety-critical behaviors in LLMs."}}
{"id": "2508.19379", "categories": ["cs.DB", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.19379", "abs": "https://arxiv.org/abs/2508.19379", "authors": ["Anurag Chakraborty", "Semih Saliho\u011flu"], "title": "Robust Recursive Query Parallelism in Graph Database Management Systems", "comment": null, "summary": "Efficient multi-core parallel processing of recursive join queries is\ncritical for achieving good performance in graph database management systems\n(GDBMSs). Prior work adopts two broad approaches. First is the state of the art\nmorsel-driven parallelism, whose vanilla application in GDBMSs parallelizes\ncomputations at the source node level. Second is to parallelize each iteration\nof the computation at the frontier level. We show that these approaches can be\nseen as part of a design space of morsel dispatching policies based on picking\ndifferent granularities of morsels. We then empirically study the question of\nwhich policies parallelize better in practice under a variety of datasets and\nquery workloads that contain one to many source nodes. We show that these two\npolicies can be combined in a hybrid policy that issues morsels both at the\nsource node and frontier levels. We then show that the multi-source\nbreadth-first search optimization from prior work can also be modeled as a\nmorsel dispatching policy that packs multiple source nodes into multi-source\nmorsels. We implement these policies inside a single system, the Kuzu GDBMS,\nand evaluate them both within Kuzu and across other systems. We show that the\nhybrid policy captures the behavior of both source morsel-only and frontier\nmorsel-only policies in cases when these approaches parallelize well, and\nout-perform them on queries when they are limited, and propose it as a robust\napproach to parallelizing recursive queries. We further show that assigning\nmulti-sources is beneficial, as it reduces the amount of scans, but only when\nthere is enough sources in the query.", "AI": {"tldr": "This paper studies parallel processing of recursive join queries in graph databases, compares different morsel dispatching policies, and proposes a hybrid policy that combines source node and frontier level parallelism. The paper also shows that assigning multi-sources is beneficial.", "motivation": "Achieving good performance in graph database management systems (GDBMSs) requires efficient multi-core parallel processing of recursive join queries. Prior work adopts two broad approaches: state of the art morsel-driven parallelism and parallelizing each iteration of the computation at the frontier level.", "method": "morsel dispatching policies based on picking different granularities of morsels, a hybrid policy that issues morsels both at the source node and frontier levels, multi-source breadth-first search optimization", "result": "The hybrid policy captures the behavior of both source morsel-only and frontier morsel-only policies in cases when these approaches parallelize well, and out-perform them on queries when they are limited. Assigning multi-sources is beneficial, as it reduces the amount of scans, but only when there is enough sources in the query.", "conclusion": "The hybrid policy captures the behavior of both source morsel-only and frontier morsel-only policies in cases when these approaches parallelize well, and out-perform them on queries when they are limited, and propose it as a robust approach to parallelizing recursive queries. Assigning multi-sources is beneficial, as it reduces the amount of scans, but only when there is enough sources in the query."}}
{"id": "2508.19547", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.19547", "abs": "https://arxiv.org/abs/2508.19547", "authors": ["Tongxin Xu", "Wenqiang Liu", "Chenzhong Bin", "Cihan Xiao", "Zhixin Zeng", "Tianlong Gu"], "title": "Improving Recommendation Fairness via Graph Structure and Representation Augmentation", "comment": "Accepted by CIKM 2025", "summary": "Graph Convolutional Networks (GCNs) have become increasingly popular in\nrecommendation systems. However, recent studies have shown that GCN-based\nmodels will cause sensitive information to disseminate widely in the graph\nstructure, amplifying data bias and raising fairness concerns. While various\nfairness methods have been proposed, most of them neglect the impact of biased\ndata on representation learning, which results in limited fairness improvement.\nMoreover, some studies have focused on constructing fair and balanced data\ndistributions through data augmentation, but these methods significantly reduce\nutility due to disruption of user preferences. In this paper, we aim to design\na fair recommendation method from the perspective of data augmentation to\nimprove fairness while preserving recommendation utility. To achieve\nfairness-aware data augmentation with minimal disruption to user preferences,\nwe propose two prior hypotheses. The first hypothesis identifies sensitive\ninteractions by comparing outcomes of performance-oriented and fairness-aware\nrecommendations, while the second one focuses on detecting sensitive features\nby analyzing feature similarities between biased and debiased representations.\nThen, we propose a dual data augmentation framework for fair recommendation,\nwhich includes two data augmentation strategies to generate fair augmented\ngraphs and feature representations. Furthermore, we introduce a debiasing\nlearning method that minimizes the dependence between the learned\nrepresentations and sensitive information to eliminate bias. Extensive\nexperiments on two real-world datasets demonstrate the superiority of our\nproposed framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u91cd\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u7528\u4e8e\u516c\u5e73\u63a8\u8350\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6765\u63d0\u9ad8\u516c\u5e73\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u8350\u6548\u7528\u3002", "motivation": "\u57fa\u4e8eGCN\u7684\u6a21\u578b\u4f1a\u5bfc\u81f4\u654f\u611f\u4fe1\u606f\u5728\u56fe\u7ed3\u6784\u4e2d\u5e7f\u6cdb\u4f20\u64ad\uff0c\u4ece\u800c\u6269\u5927\u6570\u636e\u504f\u5dee\u5e76\u5f15\u8d77\u516c\u5e73\u6027\u95ee\u9898\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u516c\u5e73\u6027\u65b9\u6cd5\u5ffd\u7565\u4e86\u6709\u504f\u5dee\u6570\u636e\u5bf9\u8868\u793a\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u516c\u5e73\u6027\u6539\u5584\u6709\u9650\u3002\u6b64\u5916\uff0c\u4e00\u4e9b\u7814\u7a76\u4fa7\u91cd\u4e8e\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6765\u6784\u5efa\u516c\u5e73\u548c\u5e73\u8861\u7684\u6570\u636e\u5206\u5e03\uff0c\u4f46\u7531\u4e8e\u7528\u6237\u504f\u597d\u7684\u4e2d\u65ad\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u663e\u7740\u964d\u4f4e\u4e86\u6548\u7528\u3002", "method": "\u4ece\u6570\u636e\u589e\u5f3a\u7684\u89d2\u5ea6\u8bbe\u8ba1\u4e86\u4e00\u79cd\u516c\u5e73\u7684\u63a8\u8350\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u516c\u5e73\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u8350\u6548\u7528\u3002\u63d0\u51fa\u4e86\u4e24\u4e2a\u5148\u9a8c\u5047\u8bbe\u3002\u7b2c\u4e00\u4e2a\u5047\u8bbe\u901a\u8fc7\u6bd4\u8f83\u9762\u5411\u6027\u80fd\u548c\u9762\u5411\u516c\u5e73\u6027\u7684\u63a8\u8350\u7ed3\u679c\u6765\u8bc6\u522b\u654f\u611f\u4ea4\u4e92\uff0c\u800c\u7b2c\u4e8c\u4e2a\u5047\u8bbe\u4fa7\u91cd\u4e8e\u901a\u8fc7\u5206\u6790\u6709\u504f\u5dee\u548c\u65e0\u504f\u5dee\u8868\u793a\u4e4b\u95f4\u7684\u7279\u5f81\u76f8\u4f3c\u6027\u6765\u68c0\u6d4b\u654f\u611f\u7279\u5f81\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u91cd\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u7528\u4e8e\u516c\u5e73\u63a8\u8350\uff0c\u5305\u62ec\u4e24\u79cd\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u4ee5\u751f\u6210\u516c\u5e73\u7684\u589e\u5f3a\u56fe\u548c\u7279\u5f81\u8868\u793a\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u53bb\u504f\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u5b66\u4e60\u5230\u7684\u8868\u793a\u548c\u654f\u611f\u4fe1\u606f\u4e4b\u95f4\u7684\u4f9d\u8d56\u6027\uff0c\u4ece\u800c\u6d88\u9664\u504f\u5dee\u3002\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\u4e86\u8be5\u6846\u67b6\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.19271", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19271", "abs": "https://arxiv.org/abs/2508.19271", "authors": ["Rushitha Santhoshi Mamidala", "Anshuman Chhabra", "Ankur Mali"], "title": "Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT", "comment": null, "summary": "Prompt-based reasoning strategies such as Chain-of-Thought (CoT) and\nIn-Context Learning (ICL) have become widely used for eliciting reasoning\ncapabilities in large language models (LLMs). However, these methods rely on\nfragile, implicit mechanisms often yielding inconsistent outputs across seeds,\nformats, or minor prompt variations making them fundamentally unreliable for\ntasks requiring stable, interpretable reasoning. In contrast, automata-based\nneuro-symbolic frameworks like RetoMaton offer a more structured and\ntrustworthy alternative by grounding retrieval in symbolic memory with\ndeterministic transitions. In this work, we extend RetoMaton by replacing its\nglobal datastore with a local, task-adaptive Weighted Finite Automaton (WFA),\nconstructed directly from external domain corpora. This local automaton\nstructure promotes robust, context-aware retrieval while preserving symbolic\ntraceability and low inference overhead. Unlike prompting, which entangles\ncontext and memory in opaque ways, our approach leverages the explicit\nstructure of WFAs to provide verifiable and modular retrieval behavior, making\nit better suited for domain transfer and interoperability. We evaluate this\nlocal RetoMaton variant on two pretrained LLMs LLaMA-3.2-1B and Gemma-3-1B-PT\nacross three reasoning tasks: TriviaQA (reading comprehension), GSM8K\n(multi-step math), and MMLU (domain knowledge). Compared to the base model and\nprompting-based methods, augmenting these setups with local RetoMaton\nconsistently improves performance while enabling transparent and reproducible\nretrieval dynamics. Our results highlight a promising shift toward trustworthy,\nsymbolic reasoning in modern LLMs via lightweight, automaton-guided memory.", "AI": {"tldr": "\u7528\u5c40\u90e8\u3001\u4efb\u52a1\u81ea\u9002\u5e94\u52a0\u6743\u6709\u9650\u81ea\u52a8\u673a (WFA) \u66ff\u6362\u5168\u5c40\u6570\u636e\u5b58\u50a8\u6765\u6269\u5c55 RetoMaton\uff0c\u4ece\u800c\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u900f\u660e\u548c\u53ef\u91cd\u73b0\u7684\u68c0\u7d22\u52a8\u6001\u3002", "motivation": "\u57fa\u4e8e\u63d0\u793a\u7684\u63a8\u7406\u7b56\u7565\uff08\u5982 Chain-of-Thought (CoT) \u548c In-Context Learning (ICL)\uff09\u5df2\u5e7f\u6cdb\u7528\u4e8e\u6fc0\u53d1\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u63a8\u7406\u80fd\u529b\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u8106\u5f31\u7684\u3001\u9690\u5f0f\u7684\u673a\u5236\uff0c\u8fd9\u4e9b\u673a\u5236\u901a\u5e38\u4f1a\u5728\u79cd\u5b50\u3001\u683c\u5f0f\u6216\u7ec6\u5fae\u7684\u63d0\u793a\u53d8\u5316\u4e2d\u4ea7\u751f\u4e0d\u4e00\u81f4\u7684\u8f93\u51fa\uff0c\u4ece\u800c\u4f7f\u5176\u5bf9\u4e8e\u9700\u8981\u7a33\u5b9a\u3001\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u7684\u4efb\u52a1\u800c\u8a00\uff0c\u4ece\u6839\u672c\u4e0a\u662f\u4e0d\u53ef\u9760\u7684\u3002", "method": "\u7528\u4ece\u5916\u90e8\u9886\u57df\u8bed\u6599\u5e93\u76f4\u63a5\u6784\u5efa\u7684\u5c40\u90e8\u3001\u4efb\u52a1\u81ea\u9002\u5e94\u52a0\u6743\u6709\u9650\u81ea\u52a8\u673a (WFA) \u66ff\u6362\u5176\u5168\u5c40\u6570\u636e\u5b58\u50a8\u6765\u6269\u5c55 RetoMaton\u3002", "result": "\u5728\u4e09\u4e2a\u63a8\u7406\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e86\u8fd9\u79cd\u5c40\u90e8 RetoMaton \u53d8\u4f53\uff1aTriviaQA\uff08\u9605\u8bfb\u7406\u89e3\uff09\u3001GSM8K\uff08\u591a\u6b65\u6570\u5b66\uff09\u548c MMLU\uff08\u9886\u57df\u77e5\u8bc6\uff09\u3002\u4e0e\u57fa\u7840\u6a21\u578b\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u7528\u5c40\u90e8 RetoMaton \u589e\u5f3a\u8fd9\u4e9b\u8bbe\u7f6e\u59cb\u7ec8\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\uff0c\u540c\u65f6\u5b9e\u73b0\u900f\u660e\u548c\u53ef\u91cd\u73b0\u7684\u68c0\u7d22\u52a8\u6001\u3002", "conclusion": "\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u3001\u81ea\u52a8\u673a\u5f15\u5bfc\u7684\u8bb0\u5fc6\uff0c\u5728\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u671d\u7740\u53ef\u4fe1\u7684\u3001\u7b26\u53f7\u63a8\u7406\u7684\u65b9\u5411\u8f6c\u53d8\u3002"}}
{"id": "2508.19257", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19257", "abs": "https://arxiv.org/abs/2508.19257", "authors": ["Chenghao Liu", "Jiachen Zhang", "Chengxuan Li", "Zhimu Zhou", "Shixin Wu", "Songfang Huang", "Huiling Duan"], "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models", "comment": "Manuscript submitted to AAAI 2026, currently under review", "summary": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u65f6\u95f4\u4ee4\u724c\u878d\u5408(TTF)\u7684\u514d\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3aVLA\u63a8\u7406\u8d28\u91cf\uff0c\u901a\u8fc7\u6574\u5408\u5386\u53f2\u548c\u5f53\u524d\u7684\u89c6\u89c9\u8868\u5f81\u6765\u5b9e\u73b0\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u6a21\u578b\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u72ec\u7acb\u5904\u7406\u89c6\u89c9\u8f93\u5165\uff0c\u5ffd\u7565\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u56fa\u6709\u7684\u5b9d\u8d35\u65f6\u95f4\u4fe1\u606f\u3002\u8fd9\u79cd\u9010\u5e27\u5904\u7406\u4f7f\u5f97\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u89c6\u89c9\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u5ffd\u7565\u4e86\u64cd\u4f5c\u5e8f\u5217\u4e2d\u8fde\u7eed\u5e27\u4e4b\u95f4\u7684\u5b9e\u8d28\u6027\u8fde\u8d2f\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u65f6\u95f4\u4ee4\u724c\u878d\u5408(TTF)\u7684\u514d\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u786c\u878d\u5408\u7b56\u7565\u548c\u5173\u952e\u5e27\u951a\u5b9a\u6765\u5b9e\u73b0\u9009\u62e9\u6027\u65f6\u95f4\u4ee4\u724c\u878d\u5408\u3002", "result": "\u5728LIBERO\u4e0a\u5e73\u5747\u63d0\u9ad8\u4e864.0\u4e2a\u767e\u5206\u70b9\uff0872.4\uff05 vs 68.4\uff05\u57fa\u7ebf\uff09\uff0c\u5728SimplerEnv\u4e0a\u8fdb\u884c\u4e86\u4ea4\u53c9\u73af\u5883\u9a8c\u8bc1\uff08\u76f8\u5bf9\u63d0\u9ad8\u4e864.8\uff05\uff09\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u76f8\u5bf9\u63d0\u9ad8\u4e868.7\uff05\u3002", "conclusion": "TTF\u5728LIBERO\u3001SimplerEnv\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6301\u7eed\u7684\u6539\u8fdb\uff0c\u5e76\u4e14\u4e0e\u6a21\u578b\u65e0\u5173\uff0c\u5728OpenVLA\u548cVLA-Cache\u67b6\u6784\u4e2d\u5747\u6709\u6548\u3002\u9009\u62e9\u6027Query\u77e9\u9635\u91cd\u7528\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002"}}
{"id": "2508.19263", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.19263", "abs": "https://arxiv.org/abs/2508.19263", "authors": ["Anat Heilper", "Doron Singer"], "title": "Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats", "comment": "16 pages 9 images", "summary": "As deep learning models grow and deployment becomes more widespread, reducing\nthe storage and transmission costs of neural network weights has become\nincreasingly important. While prior work such as ZipNN has shown that lossless\ncompression methods - particularly those based on Huffman encoding\nfloating-point exponents can significantly reduce model sizes, these techniques\nhave primarily been applied to higher-precision formats such as FP32 and BF16.\nIn this work, we extend the ZipNN approach to lower-precision floating-point\nformats, specifically FP8 and FP4, which are gaining popularity for efficient\ninference. We design a compression method that separates and compresses the\nexponent and mantissa components independently using entropy coding. Our\nevaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also\ninvestigate the compressibility of key-value (K/V) cache tensors used in large\nlanguage models (LLMs), finding that they, too, exhibit compressible patterns,\nenabling memory savings during deployment.", "AI": {"tldr": "Extends ZipNN to compress FP8/FP4 models and K/V caches in LLMs, achieving significant compression ratios.", "motivation": "Reducing the storage and transmission costs of neural network weights has become increasingly important as deep learning models grow and deployment becomes more widespread.", "method": "The paper designs a compression method that separates and compresses the exponent and mantissa components independently using entropy coding.", "result": "Achieves compression ratios up to 62% for BF16 and 83% for FP8. Key-value (K/V) cache tensors in LLMs also exhibit compressible patterns.", "conclusion": "This paper extends ZipNN to lower-precision floating-point formats like FP8 and FP4, achieving compression ratios up to 62% for BF16 and 83% for FP8. It also investigates the compressibility of key-value (K/V) cache tensors in LLMs."}}
{"id": "2508.19383", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.19383", "abs": "https://arxiv.org/abs/2508.19383", "authors": ["Daoyuan Jin", "Nick Gunner", "Niko Carvajal Janke", "Shivranjani Baruah", "Kaitlin M. Gold", "Yu Jiang"], "title": "Aleks: AI powered Multi Agent System for Autonomous Scientific Discovery via Data-Driven Approaches in Plant Science", "comment": null, "summary": "Modern plant science increasingly relies on large, heterogeneous datasets,\nbut challenges in experimental design, data preprocessing, and reproducibility\nhinder research throughput. Here we introduce Aleks, an AI-powered multi-agent\nsystem that integrates domain knowledge, data analysis, and machine learning\nwithin a structured framework to autonomously conduct data-driven scientific\ndiscovery. Once provided with a research question and dataset, Aleks\niteratively formulated problems, explored alternative modeling strategies, and\nrefined solutions across multiple cycles without human intervention. In a case\nstudy on grapevine red blotch disease, Aleks progressively identified\nbiologically meaningful features and converged on interpretable models with\nrobust performance. Ablation studies underscored the importance of domain\nknowledge and memory for coherent outcomes. This exploratory work highlights\nthe promise of agentic AI as an autonomous collaborator for accelerating\nscientific discovery in plant sciences.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86 Aleks\uff0c\u8fd9\u662f\u4e00\u4e2a AI \u9a71\u52a8\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u5b83\u53ef\u4ee5\u5728\u6ca1\u6709\u4eba\u5de5\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\u81ea\u4e3b\u8fdb\u884c\u6570\u636e\u9a71\u52a8\u7684\u79d1\u5b66\u53d1\u73b0\u3002", "motivation": "\u73b0\u4ee3\u690d\u7269\u79d1\u5b66\u8d8a\u6765\u8d8a\u4f9d\u8d56\u5927\u578b\u5f02\u6784\u6570\u636e\u96c6\uff0c\u4f46\u5b9e\u9a8c\u8bbe\u8ba1\u3001\u6570\u636e\u9884\u5904\u7406\u548c\u53ef\u91cd\u590d\u6027\u65b9\u9762\u7684\u6311\u6218\u963b\u788d\u4e86\u7814\u7a76\u541e\u5410\u91cf\u3002", "method": "AI-powered multi-agent system", "result": "\u5728\u8461\u8404\u85e4\u7ea2\u6591\u75c5\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cAleks \u9010\u6b65\u8bc6\u522b\u51fa\u5177\u6709\u751f\u7269\u5b66\u610f\u4e49\u7684\u7279\u5f81\uff0c\u5e76\u6536\u655b\u4e8e\u5177\u6709\u7a33\u5065\u6027\u80fd\u7684\u53ef\u89e3\u91ca\u6a21\u578b\u3002", "conclusion": "Agentic AI \u53ef\u4ee5\u4f5c\u4e3a\u81ea\u4e3b\u5408\u4f5c\u8005\u52a0\u901f\u690d\u7269\u79d1\u5b66\u7684\u79d1\u5b66\u53d1\u73b0\u3002"}}
{"id": "2508.19807", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19807", "abs": "https://arxiv.org/abs/2508.19807", "authors": ["Michael Nidd", "Christoph Miksovic", "Thomas Gschwind", "Francesco Fusco", "Andrea Giovannini", "Ioana Giurgiu"], "title": "Bootstrapping Learned Cost Models with Synthetic SQL Queries", "comment": null, "summary": "Having access to realistic workloads for a given database instance is\nextremely important to enable stress and vulnerability testing, as well as to\noptimize for cost and performance. Recent advances in learned cost models have\nshown that when enough diverse SQL queries are available, one can effectively\nand efficiently predict the cost of running a given query against a specific\ndatabase engine. In this paper, we describe our experience in exploiting modern\nsynthetic data generation techniques, inspired by the generative AI and LLM\ncommunity, to create high-quality datasets enabling the effective training of\nsuch learned cost models. Initial results show that we can improve a learned\ncost model's predictive accuracy by training it with 45% fewer queries than\nwhen using competitive generation approaches.", "AI": {"tldr": "\u5229\u7528\u751f\u6210\u5f0fAI\u548cLLM\u6280\u672f\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u53ef\u4ee5\u7528\u66f4\u5c11\u7684\u67e5\u8be2\u6765\u8bad\u7ec3 learned cost \u6a21\u578b\uff0c\u5e76\u63d0\u9ad8\u5176\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u8bbf\u95ee\u73b0\u5b9e\u7684\u5de5\u4f5c\u8d1f\u8f7d\u5bf9\u4e8e\u538b\u529b\u548c\u6f0f\u6d1e\u6d4b\u8bd5\u4ee5\u53ca\u4f18\u5316\u6210\u672c\u548c\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u6700\u8fd1\u5728 learned cost \u6a21\u578b\u65b9\u9762\u7684\u8fdb\u5c55\u8868\u660e\uff0c\u5f53\u6709\u8db3\u591f\u591a\u7684 SQL \u67e5\u8be2\u65f6\uff0c\u53ef\u4ee5\u6709\u6548\u4e14\u9ad8\u6548\u5730\u9884\u6d4b\u8fd0\u884c\u7ed9\u5b9a\u67e5\u8be2\u7684\u6210\u672c\u3002", "method": "\u5229\u7528\u73b0\u4ee3\u5408\u6210\u6570\u636e\u751f\u6210\u6280\u672f\uff0c\u53d7\u5230\u751f\u6210\u5f0fAI\u548cLLM\u793e\u533a\u7684\u542f\u53d1\u3002", "result": "\u4e0e\u4f7f\u7528\u7ade\u4e89\u6027\u751f\u6210\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4f7f\u7528\u751f\u6210\u5f0fAI\u548cLLM\u6280\u672f\u8bad\u7ec3\u7684 learned cost \u6a21\u578b\uff0c\u53ef\u4ee5\u7528\u5c11 45% \u7684\u67e5\u8be2\u6765\u63d0\u9ad8\u5176\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528\u751f\u6210\u5f0fAI\u548cLLM\u6280\u672f\uff0c\u53ef\u4ee5\u7528\u66f4\u5c11\u7684\u67e5\u8be2\u6765\u8bad\u7ec3 learned cost \u6a21\u578b\uff0c\u5e76\u63d0\u9ad8\u5176\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2508.19591", "categories": ["cs.IR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.19591", "abs": "https://arxiv.org/abs/2508.19591", "authors": ["Jiakui Shen", "Yunqi Mi", "Guoshuai Zhao", "Jialie Shen", "Xueming Qian"], "title": "A Model-agnostic Strategy to Mitigate Embedding Degradation in Personalized Federated Recommendation", "comment": null, "summary": "Centralized recommender systems encounter privacy leakage due to the need to\ncollect user behavior and other private data. Hence, federated recommender\nsystems (FedRec) have become a promising approach with an aggregated global\nmodel on the server. However, this distributed training paradigm suffers from\nembedding degradation caused by suboptimal personalization and dimensional\ncollapse, due to the existence of sparse interactions and heterogeneous\npreferences. To this end, we propose a novel model-agnostic strategy for FedRec\nto strengthen the personalized embedding utility, which is called Personalized\nLocal-Global Collaboration (PLGC). It is the first research in federated\nrecommendation to alleviate the dimensional collapse issue. Particularly, we\nincorporate the frozen global item embedding table into local devices. Based on\na Neural Tangent Kernel strategy that dynamically balances local and global\ninformation, PLGC optimizes personalized representations during forward\ninference, ultimately converging to user-specific preferences. Additionally,\nPLGC carries on a contrastive objective function to reduce embedding redundancy\nby dissolving dependencies between dimensions, thereby improving the backward\nrepresentation learning process. We introduce PLGC as a model-agnostic\npersonalized training strategy for federated recommendations that can be\napplied to existing baselines to alleviate embedding degradation. Extensive\nexperiments on five real-world datasets have demonstrated the effectiveness and\nadaptability of PLGC, which outperforms various baseline algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPLGC\u7684\u8054\u90a6\u63a8\u8350\u4e2a\u6027\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u7f13\u89e3\u5d4c\u5165\u9000\u5316\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4e2d\u5fc3\u5316\u63a8\u8350\u7cfb\u7edf\u7531\u4e8e\u9700\u8981\u6536\u96c6\u7528\u6237\u884c\u4e3a\u548c\u5176\u4ed6\u79c1\u4eba\u6570\u636e\u800c\u9047\u5230\u9690\u79c1\u6cc4\u9732\u95ee\u9898\u3002\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\uff08FedRec\uff09\u5df2\u6210\u4e3a\u4e00\u79cd\u6709\u524d\u9014\u7684\u65b9\u6cd5\uff0c\u4f46\u8fd9\u79cd\u5206\u5e03\u5f0f\u8bad\u7ec3\u6a21\u5f0f\u4f1a\u56e0\u6b21\u4f18\u7684\u4e2a\u6027\u5316\u548c\u7ef4\u5ea6\u5d29\u6e83\u800c\u5bfc\u81f4\u5d4c\u5165\u9000\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u65e0\u5173\u7b56\u7565PLGC\uff0c\u7528\u4e8e\u8054\u90a6\u63a8\u8350\uff0c\u4ee5\u52a0\u5f3a\u4e2a\u6027\u5316\u5d4c\u5165\u6548\u7528\u3002\u8be5\u65b9\u6cd5\u5c06\u51bb\u7ed3\u7684\u5168\u5c40\u9879\u76ee\u5d4c\u5165\u8868\u5408\u5e76\u5230\u672c\u5730\u8bbe\u5907\u4e2d\uff0c\u5e76\u57fa\u4e8e\u795e\u7ecf\u6b63\u5207\u6838\u7b56\u7565\u52a8\u6001\u5e73\u8861\u672c\u5730\u548c\u5168\u5c40\u4fe1\u606f\u3002\u6b64\u5916\uff0cPLGC\u8fd8\u91c7\u7528\u5bf9\u6bd4\u76ee\u6807\u51fd\u6570\u6765\u51cf\u5c11\u5d4c\u5165\u5197\u4f59\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPLGC\u7684\u6709\u6548\u6027\u548c\u9002\u5e94\u6027\u4f18\u4e8e\u5404\u79cd\u57fa\u7ebf\u7b97\u6cd5\u3002", "conclusion": "PLGC\u4f5c\u4e3a\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u4e2a\u6027\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u53ef\u4ee5\u5e94\u7528\u4e8e\u73b0\u6709\u7684\u8054\u90a6\u63a8\u8350\u57fa\u7ebf\uff0c\u4ee5\u7f13\u89e3\u5d4c\u5165\u9000\u5316\u3002"}}
{"id": "2508.19272", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19272", "abs": "https://arxiv.org/abs/2508.19272", "authors": ["Kshitij Fadnis", "Sara Rosenthal", "Maeda Hanafi", "Yannis Katsis", "Marina Danilevsky"], "title": "RAGAPHENE: A RAG Annotation Platform with Human Enhancements and Edits", "comment": null, "summary": "Retrieval Augmented Generation (RAG) is an important aspect of conversing\nwith Large Language Models (LLMs) when factually correct information is\nimportant. LLMs may provide answers that appear correct, but could contain\nhallucinated information. Thus, building benchmarks that can evaluate LLMs on\nmulti-turn RAG conversations has become an increasingly important task.\nSimulating real-world conversations is vital for producing high quality\nevaluation benchmarks. We present RAGAPHENE, a chat-based annotation platform\nthat enables annotators to simulate real-world conversations for benchmarking\nand evaluating LLMs. RAGAPHENE has been successfully used by approximately 40\nannotators to build thousands of real-world conversations.", "AI": {"tldr": "RAGAPHENE\u662f\u4e00\u4e2a\u804a\u5929\u5e73\u53f0\uff0c\u7528\u4e8e\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u7684\u5bf9\u8bdd\uff0c\u4ee5\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u5f53\u4e8b\u5b9e\u6b63\u786e\u7684\u4fe1\u606f\u5f88\u91cd\u8981\u65f6\uff0c\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u662f\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u5bf9\u8bdd\u7684\u4e00\u4e2a\u91cd\u8981\u65b9\u9762\u3002llm\u53ef\u80fd\u4f1a\u63d0\u4f9b\u770b\u8d77\u6765\u6b63\u786e\u7684\u7b54\u6848\uff0c\u4f46\u53ef\u80fd\u5305\u542b\u865a\u6784\u7684\u4fe1\u606f\u3002\u56e0\u6b64\uff0c\u6784\u5efa\u80fd\u591f\u8bc4\u4f30llm\u5728\u591a\u8f6eRAG\u5bf9\u8bdd\u4e2d\u7684\u57fa\u51c6\u5df2\u7ecf\u6210\u4e3a\u4e00\u9879\u65e5\u76ca\u91cd\u8981\u7684\u4efb\u52a1\u3002\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u7684\u5bf9\u8bdd\u5bf9\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8bc4\u4f30\u57fa\u51c6\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aRAGAPHENE\u7684\u57fa\u4e8e\u804a\u5929\u7684\u6807\u6ce8\u5e73\u53f0\u3002", "result": "RAGAPHENE\u662f\u4e00\u4e2a\u57fa\u4e8e\u804a\u5929\u7684\u6ce8\u91ca\u5e73\u53f0\uff0c\u5b83\u4f7f\u6ce8\u91ca\u8005\u80fd\u591f\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u7684\u5bf9\u8bdd\uff0c\u4ee5\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30llm\u3002", "conclusion": "RAGAPHENE\u5e73\u53f0\u5df2\u88ab\u6210\u529f\u7528\u4e8e\u6784\u5efa\u6570\u5343\u4e2a\u771f\u5b9e\u5bf9\u8bdd\u3002"}}
{"id": "2508.19289", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19289", "abs": "https://arxiv.org/abs/2508.19289", "authors": ["Tai Inui", "Steven Oh", "Magdeline Kuan"], "title": "Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation", "comment": "6 pages", "summary": "We present an unsupervised slide-quality assessment pipeline that combines\nseven expert-inspired visual-design metrics (whitespace, colorfulness, edge\ndensity, brightness contrast, text density, color harmony, layout balance) with\nCLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate\npresentation slides. Trained on 12k professional lecture slides and evaluated\non six academic talks (115 slides), our method achieved Pearson correlations up\nto 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores\nfrom leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude\nSonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual\nratings, discriminant validity against speaker-delivery scores, and exploratory\nalignment with overall impressions. Our results show that augmenting low-level\ndesign cues with multimodal embeddings closely approximates audience\nperceptions of slide quality, enabling scalable, objective feedback in real\ntime.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5e7b\u706f\u7247\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u89c6\u89c9\u8bbe\u8ba1\u6307\u6807\u548c\u591a\u6a21\u6001\u5d4c\u5165\uff0c\u80fd\u591f\u6709\u6548\u5730\u8bc4\u4f30\u5e7b\u706f\u7247\u8d28\u91cf\u5e76\u63d0\u4f9b\u5b9e\u65f6\u53cd\u9988\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u53ef\u6269\u5c55\u3001\u5ba2\u89c2\u7684\u5e7b\u706f\u7247\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u5e7b\u706f\u7247\u8d28\u91cf\u8bc4\u4f30\u6d41\u7a0b\uff0c\u8be5\u6d41\u7a0b\u7ed3\u5408\u4e86\u4e03\u4e2a\u4e13\u5bb6\u542f\u53d1\u7684\u89c6\u89c9\u8bbe\u8ba1\u6307\u6807\uff08\u7a7a\u767d\u3001\u8272\u5f69\u9c9c\u8273\u5ea6\u3001\u8fb9\u7f18\u5bc6\u5ea6\u3001\u4eae\u5ea6\u5bf9\u6bd4\u5ea6\u3001\u6587\u672c\u5bc6\u5ea6\u3001\u8272\u5f69\u548c\u8c10\u3001\u5e03\u5c40\u5e73\u8861\uff09\u4e0eCLIP-ViT\u5d4c\u5165\uff0c\u4f7f\u7528\u57fa\u4e8e\u9694\u79bb\u68ee\u6797\u7684\u5f02\u5e38\u8bc4\u5206\u6765\u8bc4\u4f30\u6f14\u793a\u5e7b\u706f\u7247\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u516d\u4e2a\u5b66\u672f\u8bb2\u5ea7\uff08115\u5f20\u5e7b\u706f\u7247\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u4e0e\u4eba\u5de5\u89c6\u89c9\u8d28\u91cf\u8bc4\u7ea7\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u9ad8\u8fbe0.83\uff0c\u6bd4\u9886\u5148\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08ChatGPT o4-mini-high\uff0cChatGPT o3\uff0cClaude Sonnet 4\uff0cGemini 2.5 Pro\uff09\u7684\u5206\u6570\u9ad81.79\u500d\u81f33.23\u500d\u3002\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u4e0e\u89c6\u89c9\u8bc4\u7ea7\u4e4b\u95f4\u7684\u6536\u655b\u6548\u5ea6\u3001\u4e0e\u6f14\u8bb2\u8005\u8868\u8fbe\u5206\u6570\u4e4b\u95f4\u7684\u533a\u5206\u6548\u5ea6\u4ee5\u53ca\u4e0e\u6574\u4f53\u5370\u8c61\u7684\u63a2\u7d22\u6027\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408\u5e95\u5c42\u8bbe\u8ba1\u7ebf\u7d22\u548c\u591a\u6a21\u6001\u5d4c\u5165\u53ef\u4ee5\u5f88\u597d\u5730\u6a21\u62df\u89c2\u4f17\u5bf9\u5e7b\u706f\u7247\u8d28\u91cf\u7684\u611f\u77e5\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u3001\u5ba2\u89c2\u7684\u5b9e\u65f6\u53cd\u9988\u3002"}}
{"id": "2508.19277", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19277", "abs": "https://arxiv.org/abs/2508.19277", "authors": ["Xinyu Li", "Tianjin Huang", "Ronghui Mu", "Xiaowei Huang", "Gaojie Jin"], "title": "POT: Inducing Overthinking in LLMs via Black-Box Iterative Optimization", "comment": null, "summary": "Recent advances in Chain-of-Thought (CoT) prompting have substantially\nenhanced the reasoning capabilities of large language models (LLMs), enabling\nsophisticated problem-solving through explicit multi-step reasoning traces.\nHowever, these enhanced reasoning processes introduce novel attack surfaces,\nparticularly vulnerabilities to computational inefficiency through\nunnecessarily verbose reasoning chains that consume excessive resources without\ncorresponding performance gains. Prior overthinking attacks typically require\nrestrictive conditions including access to external knowledge sources for data\npoisoning, reliance on retrievable poisoned content, and structurally obvious\ntemplates that limit practical applicability in real-world scenarios. To\naddress these limitations, we propose POT (Prompt-Only OverThinking), a novel\nblack-box attack framework that employs LLM-based iterative optimization to\ngenerate covert and semantically natural adversarial prompts, eliminating\ndependence on external data access and model retrieval. Extensive experiments\nacross diverse model architectures and datasets demonstrate that POT achieves\nsuperior performance compared to other methods.", "AI": {"tldr": "This paper proposes POT, a novel black-box attack framework that employs LLM-based iterative optimization to generate covert and semantically natural adversarial prompts, eliminating dependence on external data access and model retrieval. POT achieves superior performance compared to other methods.", "motivation": "enhanced reasoning processes introduce novel attack surfaces, particularly vulnerabilities to computational inefficiency through unnecessarily verbose reasoning chains that consume excessive resources without corresponding performance gains", "method": "LLM-based iterative optimization to generate covert and semantically natural adversarial prompts", "result": "POT achieves superior performance compared to other methods", "conclusion": "POT achieves superior performance compared to other methods."}}
{"id": "2508.19432", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19432", "abs": "https://arxiv.org/abs/2508.19432", "authors": ["Yao Fu", "Xianxuan Long", "Runchao Li", "Haotian Yu", "Mu Sheng", "Xiaotian Han", "Yu Yin", "Pan Li"], "title": "Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs", "comment": "Accepted to EMNLP2025 main conference (poster)", "summary": "Quantization enables efficient deployment of large language models (LLMs) in\nresource-constrained environments by significantly reducing memory and\ncomputation costs. While quantized LLMs often maintain performance on\nperplexity and zero-shot tasks, their impact on truthfulness-whether generating\ntruthful or deceptive responses-remains largely unexplored. In this work, we\nintroduce TruthfulnessEval, a comprehensive evaluation framework for assessing\nthe truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on\nLogical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on\nImitative Falsehoods. Using this framework, we examine mainstream quantization\ntechniques (ranging from 4-bit to extreme 2-bit) across several open-source\nLLMs. Surprisingly, we find that while quantized models retain internally\ntruthful representations, they are more susceptible to producing false outputs\nunder misleading prompts. To probe this vulnerability, we test 15 rephrased\nvariants of \"honest\", \"neutral\" and \"deceptive\" prompts and observe that\n\"deceptive\" prompts can override truth-consistent behavior, whereas \"honest\"\nand \"neutral\" prompts maintain stable outputs. Further, we reveal that\nquantized models \"know\" the truth internally yet still produce false outputs\nwhen guided by \"deceptive\" prompts via layer-wise probing and PCA\nvisualizations. Our findings provide insights into future designs of\nquantization-aware alignment and truthfulness interventions.", "AI": {"tldr": "This paper introduces TruthfulnessEval to assess the truthfulness of quantized LLMs. Quantized models are vulnerable to deceptive prompts, even though they internally know the truth.", "motivation": "Quantization enables efficient deployment of large language models (LLMs) in resource-constrained environments. While quantized LLMs often maintain performance on perplexity and zero-shot tasks, their impact on truthfulness remains largely unexplored.", "method": "Introduce TruthfulnessEval, a comprehensive evaluation framework for assessing the truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on Logical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on Imitative Falsehoods. Examine mainstream quantization techniques (ranging from 4-bit to extreme 2-bit) across several open-source LLMs. Test 15 rephrased variants of honest, neutral and deceptive prompts and observe the impact. Reveal that quantized models know the truth internally yet still produce false outputs when guided by deceptive prompts via layer-wise probing and PCA visualizations.", "result": "Quantized models retain internally truthful representations but are more susceptible to producing false outputs under misleading prompts.", "conclusion": "Quantized models retain internally truthful representations but are more susceptible to producing false outputs under misleading prompts. Deceptive prompts can override truth-consistent behavior, while honest and neutral prompts maintain stable outputs. Quantized models know the truth internally yet still produce false outputs when guided by deceptive prompts."}}
{"id": "2508.19372", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19372", "abs": "https://arxiv.org/abs/2508.19372", "authors": ["Zikun Fu", "Chen Yang", "Kourosh Davoudi", "Ken Q. Pu"], "title": "Database Entity Recognition with Data Augmentation and Deep Learning", "comment": "6 pages, 5 figures. Accepted at IEEE 26th International Conference on\n  Information Reuse and Integration for Data Science (IRI 2025), San Jose,\n  California, August 6-8, 2025", "summary": "This paper addresses the challenge of Database Entity Recognition (DB-ER) in\nNatural Language Queries (NLQ). We present several key contributions to advance\nthis field: (1) a human-annotated benchmark for DB-ER task, derived from\npopular text-to-sql benchmarks, (2) a novel data augmentation procedure that\nleverages automatic annotation of NLQs based on the corresponding SQL queries\nwhich are available in popular text-to-SQL benchmarks, (3) a specialized\nlanguage model based entity recognition model using T5 as a backbone and two\ndown-stream DB-ER tasks: sequence tagging and token classification for\nfine-tuning of backend and performing DB-ER respectively. We compared our DB-ER\ntagger with two state-of-the-art NER taggers, and observed better performance\nin both precision and recall for our model. The ablation evaluation shows that\ndata augmentation boosts precision and recall by over 10%, while fine-tuning of\nthe T5 backbone boosts these metrics by 5-10%.", "AI": {"tldr": "This paper introduces a new DB-ER tagger with data augmentation and T5 fine-tuning, achieving superior precision and recall compared to existing NER taggers.", "motivation": "Addressing the challenge of Database Entity Recognition (DB-ER) in Natural Language Queries (NLQ).", "method": "A specialized language model based entity recognition model using T5 as a backbone, fine-tuned with sequence tagging and token classification. A novel data augmentation procedure leverages automatic annotation of NLQs based on corresponding SQL queries.", "result": "The DB-ER tagger shows better performance in both precision and recall compared to state-of-the-art NER taggers. Data augmentation boosts precision and recall by over 10%, and fine-tuning of the T5 backbone boosts these metrics by 5-10%.", "conclusion": "The proposed DB-ER tagger outperforms state-of-the-art NER taggers in precision and recall. Data augmentation and fine-tuning of the T5 backbone significantly improve performance."}}
{"id": "2508.19620", "categories": ["cs.IR", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19620", "abs": "https://arxiv.org/abs/2508.19620", "authors": ["Yunqi Mi", "Jiakui Shen", "Guoshuai Zhao", "Jialie Shen", "Xueming Qian"], "title": "A Scenario-Oriented Survey of Federated Recommender Systems: Techniques, Challenges, and Future Directions", "comment": null, "summary": "Extending recommender systems to federated learning (FL) frameworks to\nprotect the privacy of users or platforms while making recommendations has\nrecently gained widespread attention in academia. This is due to the natural\ncoupling of recommender systems and federated learning architectures: the data\noriginates from distributed clients (mostly mobile devices held by users),\nwhich are highly related to privacy. In a centralized recommender system\n(CenRec), the central server collects clients' data, trains the model, and\nprovides the service. Whereas in federated recommender systems (FedRec), the\nstep of data collecting is omitted, and the step of model training is offloaded\nto each client. The server only aggregates the model and other knowledge, thus\navoiding client privacy leakage. Some surveys of federated recommender systems\ndiscuss and analyze related work from the perspective of designing FL systems.\nHowever, their utility drops by ignoring specific recommendation scenarios'\nunique characteristics and practical challenges. For example, the statistical\nheterogeneity issue in cross-domain FedRec originates from the label drift of\nthe data held by different platforms, which is mainly caused by the recommender\nitself, but not the federated architecture. Therefore, it should focus more on\nsolving specific problems in real-world recommendation scenarios to encourage\nthe deployment FedRec. To this end, this review comprehensively analyzes the\ncoupling of recommender systems and federated learning from the perspective of\nrecommendation researchers and practitioners. We establish a clear link between\nrecommendation scenarios and FL frameworks, systematically analyzing\nscenario-specific approaches, practical challenges, and potential\nopportunities. We aim to develop guidance for the real-world deployment of\nFedRec, bridging the gap between existing research and applications.", "AI": {"tldr": "This review analyzes federated recommender systems (FedRec) from a recommendation perspective, linking scenarios to FL frameworks and providing guidance for real-world deployment.", "motivation": "Extending recommender systems to federated learning (FL) frameworks protects user privacy. Existing surveys ignore specific recommendation scenarios' unique characteristics and practical challenges, such as the statistical heterogeneity issue in cross-domain FedRec.", "method": "The review analyzes the coupling of recommender systems and federated learning, establishing a clear link between recommendation scenarios and FL frameworks, and systematically analyzing scenario-specific approaches, practical challenges, and potential opportunities.", "result": "The review provides guidance for real-world deployment of FedRec, bridging the gap between existing research and applications.", "conclusion": "This review comprehensively analyzes the coupling of recommender systems and federated learning from the perspective of recommendation researchers and practitioners, establishing a clear link between recommendation scenarios and FL frameworks, and systematically analyzing scenario-specific approaches, practical challenges, and potential opportunities. The aim is to develop guidance for the real-world deployment of FedRec, bridging the gap between existing research and applications."}}
{"id": "2508.19274", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19274", "abs": "https://arxiv.org/abs/2508.19274", "authors": ["Yue Chu"], "title": "Leveraging Language Models and Machine Learning in Verbal Autopsy Analysis", "comment": "Ph.D. dissertation submitted to The Ohio State University, August\n  2025", "summary": "In countries without civil registration and vital statistics, verbal autopsy\n(VA) is a critical tool for estimating cause of death (COD) and inform policy\npriorities. In VA, interviewers ask proximal informants for details on the\ncircumstances preceding a death, in the form of unstructured narratives and\nstructured questions. Existing automated VA cause classification algorithms\nonly use the questions and ignore the information in the narratives. In this\nthesis, we investigate how the VA narrative can be used for automated COD\nclassification using pretrained language models (PLMs) and machine learning\n(ML) techniques. Using empirical data from South Africa, we demonstrate that\nwith the narrative alone, transformer-based PLMs with task-specific fine-tuning\noutperform leading question-only algorithms at both the individual and\npopulation levels, particularly in identifying non-communicable diseases. We\nexplore various multimodal fusion strategies combining narratives and questions\nin unified frameworks. Multimodal approaches further improve performance in COD\nclassification, confirming that each modality has unique contributions and may\ncapture valuable information that is not present in the other modality. We also\ncharacterize physician-perceived information sufficiency in VA. We describe\nvariations in sufficiency levels by age and COD and demonstrate that\nclassification accuracy is affected by sufficiency for both physicians and\nmodels. Overall, this thesis advances the growing body of knowledge at the\nintersection of natural language processing, epidemiology, and global health.\nIt demonstrates the value of narrative in enhancing COD classification. Our\nfindings underscore the need for more high-quality data from more diverse\nsettings to use in training and fine-tuning PLM/ML methods, and offer valuable\ninsights to guide the rethinking and redesign of the VA instrument and\ninterview.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u4f7f\u7528\u53e3\u5934\u5c38\u68c0(VA)\u53d9\u8ff0\u6765\u6539\u8fdb\u6b7b\u56e0(COD)\u5206\u7c7b\uff0c\u53d1\u73b0\u53d9\u8ff0\u53ef\u4ee5\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u66f4\u591a\u9ad8\u8d28\u91cf\u6570\u636e\u3002", "motivation": "\u5728\u6ca1\u6709\u6c11\u4e8b\u767b\u8bb0\u548c\u751f\u547d\u7edf\u8ba1\u7684\u56fd\u5bb6\uff0c\u53e3\u5934\u5c38\u68c0(VA)\u662f\u4f30\u8ba1\u6b7b\u56e0(COD)\u548c\u4e3a\u653f\u7b56\u91cd\u70b9\u63d0\u4f9b\u4fe1\u606f\u7684\u91cd\u8981\u5de5\u5177\u3002\u73b0\u6709\u7684\u81ea\u52a8VA\u6b7b\u56e0\u5206\u7c7b\u7b97\u6cd5\u53ea\u4f7f\u7528\u95ee\u9898\uff0c\u5ffd\u7565\u4e86\u53d9\u8ff0\u4e2d\u7684\u4fe1\u606f\u3002", "method": "\u8fd9\u7bc7\u8bba\u6587\u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b(PLM)\u548c\u673a\u5668\u5b66\u4e60(ML)\u6280\u672f\uff0c\u7814\u7a76\u5982\u4f55\u4f7f\u7528VA\u53d9\u8ff0\u8fdb\u884c\u81ea\u52a8\u6b7b\u56e0(COD)\u5206\u7c7b\u3002", "result": "\u4ec5\u4f7f\u7528\u53d9\u8ff0\uff0c\u7ecf\u8fc7\u7279\u5b9a\u4efb\u52a1\u5fae\u8c03\u7684\u57fa\u4e8eTransformer\u7684PLM\u5728\u4e2a\u4f53\u548c\u7fa4\u4f53\u5c42\u9762\u90fd\u4f18\u4e8e\u9886\u5148\u7684\u4ec5\u4f7f\u7528\u95ee\u9898\u7684\u7b97\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u8bc6\u522b\u975e\u4f20\u67d3\u6027\u75be\u75c5\u65b9\u9762\u3002\u591a\u6a21\u6001\u65b9\u6cd5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86COD\u5206\u7c7b\u7684\u6027\u80fd\uff0c\u8bc1\u5b9e\u4e86\u6bcf\u79cd\u6a21\u6001\u90fd\u6709\u72ec\u7279\u7684\u8d21\u732e\uff0c\u5e76\u4e14\u53ef\u4ee5\u6355\u83b7\u53e6\u4e00\u79cd\u6a21\u6001\u4e2d\u4e0d\u5b58\u5728\u7684\u6709\u4ef7\u503c\u7684\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u8fd8\u63cf\u8ff0\u4e86\u533b\u751f\u611f\u77e5\u7684VA\u4fe1\u606f\u5145\u5206\u6027\uff0c\u5e76\u8868\u660e\u5206\u7c7b\u51c6\u786e\u6027\u53d7\u5230\u533b\u751f\u548c\u6a21\u578b\u5145\u5206\u6027\u7684\u5f71\u54cd\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u8bc1\u660e\u4e86\u53d9\u8ff0\u5728\u63d0\u5347\u6b7b\u56e0\u5206\u7c7b\u4e2d\u7684\u4ef7\u503c\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u66f4\u591a\u6765\u81ea\u4e0d\u540c\u73af\u5883\u7684\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u4ee5\u7528\u4e8e\u8bad\u7ec3\u548c\u5fae\u8c03PLM/ML\u65b9\u6cd5\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u91cd\u65b0\u601d\u8003\u548c\u91cd\u65b0\u8bbe\u8ba1VA\u5de5\u5177\u548c\u8bbf\u8c08\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.19290", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19290", "abs": "https://arxiv.org/abs/2508.19290", "authors": ["Alexandros Gkillas", "Ioulia Kapsali", "Nikos Piperigkos", "Aris S. Lalos"], "title": "Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation", "comment": null, "summary": "LiDAR-based segmentation is essential for reliable perception in autonomous\nvehicles, yet modern segmentation networks are highly susceptible to\nadversarial attacks that can compromise safety. Most existing defenses are\ndesigned for networks operating directly on raw 3D point clouds and rely on\nlarge, computationally intensive generative models. However, many\nstate-of-the-art LiDAR segmentation pipelines operate on more efficient 2D\nrange view representations. Despite their widespread adoption, dedicated\nlightweight adversarial defenses for this domain remain largely unexplored. We\nintroduce an efficient model-based purification framework tailored for\nadversarial defense in 2D range-view LiDAR segmentation. We propose a direct\nattack formulation in the range-view domain and develop an explainable\npurification network based on a mathematical justified optimization problem,\nachieving strong adversarial resilience with minimal computational overhead.\nOur method achieves competitive performance on open benchmarks, consistently\noutperforming generative and adversarial training baselines. More importantly,\nreal-world deployment on a demo vehicle demonstrates the framework's ability to\ndeliver accurate operation in practical autonomous driving scenarios.", "AI": {"tldr": "This paper presents a lightweight and effective defense mechanism against adversarial attacks in 2D LiDAR segmentation, suitable for real-world autonomous driving applications.", "motivation": "Existing defenses for LiDAR segmentation networks are computationally intensive and not tailored for efficient 2D range view representations, which are widely used in state-of-the-art pipelines. This paper aims to address the lack of lightweight adversarial defenses in this domain.", "method": "The paper proposes a direct attack formulation in the range-view domain and develops an explainable purification network based on a mathematically justified optimization problem.", "result": "The proposed method achieves competitive performance on open benchmarks, outperforming generative and adversarial training baselines. Real-world deployment on a demo vehicle demonstrates its ability to deliver accurate operation in practical autonomous driving scenarios.", "conclusion": "This paper introduces a purification framework and demonstrates its effectiveness in defending against adversarial attacks in 2D range-view LiDAR segmentation, showing its potential for real-world autonomous driving scenarios."}}
{"id": "2508.19318", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19318", "abs": "https://arxiv.org/abs/2508.19318", "authors": ["Aohan Li", "Miyu Tsuzuki"], "title": "(DEMO) Deep Reinforcement Learning Based Resource Allocation in Distributed IoT Systems", "comment": null, "summary": "Deep Reinforcement Learning (DRL) has emerged as an efficient approach to\nresource allocation due to its strong capability in handling complex\ndecision-making tasks. However, only limited research has explored the training\nof DRL models with real-world data in practical, distributed Internet of Things\n(IoT) systems. To bridge this gap, this paper proposes a novel framework for\ntraining DRL models in real-world distributed IoT environments. In the proposed\nframework, IoT devices select communication channels using a DRL-based method,\nwhile the DRL model is trained with feedback information. Specifically,\nAcknowledgment (ACK) information is obtained from actual data transmissions\nover the selected channels. Implementation and performance evaluation, in terms\nof Frame Success Rate (FSR), are carried out, demonstrating both the\nfeasibility and the effectiveness of the proposed framework.", "AI": {"tldr": "This paper proposes a novel framework for training DRL models in real-world distributed IoT environments, where IoT devices select communication channels using a DRL-based method and the DRL model is trained with feedback information.", "motivation": "Only limited research has explored the training of DRL models with real-world data in practical, distributed Internet of Things (IoT) systems. To bridge this gap, this paper proposes a novel framework for training DRL models in real-world distributed IoT environments.", "method": "IoT devices select communication channels using a DRL-based method, while the DRL model is trained with feedback information. Specifically, Acknowledgment (ACK) information is obtained from actual data transmissions over the selected channels.", "result": "DRL-based method can be used to select communication channels in IoT devices", "conclusion": "The implementation and performance evaluation, in terms of Frame Success Rate (FSR), demonstrate both the feasibility and the effectiveness of the proposed framework."}}
{"id": "2508.19461", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19461", "abs": "https://arxiv.org/abs/2508.19461", "authors": ["Neil Kale", "Chen Bo Calvin Zhang", "Kevin Zhu", "Ankit Aich", "Paula Rodriguez", "Scale Red Team", "Christina Q. Knight", "Zifan Wang"], "title": "Reliable Weak-to-Strong Monitoring of LLM Agents", "comment": "18 pages, 15 figures", "summary": "We stress test monitoring systems for detecting covert misbehavior in\nautonomous LLM agents (e.g., secretly sharing private information). To this\nend, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)\nvarying levels of agent and monitor situational awareness; (2) distinct\nadversarial strategies to evade the monitor, such as prompt injection; and (3)\ntwo datasets and environments -- SHADE-Arena for tool-calling agents and our\nnew CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We\nrun MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse\nagent trajectories, alongside a new hybrid hierarchical-sequential scaffolding\nproposed in this work. Our empirical results yield three key findings. First,\nagent awareness dominates monitor awareness: an agent's knowledge that it is\nbeing monitored substantially degrades the monitor's reliability. On the\ncontrary, providing the monitor with more information about the agent is less\nhelpful than expected. Second, monitor scaffolding matters more than monitor\nawareness: the hybrid scaffolding consistently outperforms baseline monitor\nscaffolding, and can enable weaker models to reliably monitor stronger agents\n-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where\nhumans discuss with the LLM monitor to get an updated judgment for the agent's\nbehavior, targeted human oversight is most effective; escalating only\npre-flagged cases to human reviewers improved the TPR by approximately 15% at\nFPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the\nlack of adversarial robustness for LLMs and humans when monitoring and\ndetecting agent misbehavior. We release code, data, and logs to spur further\nresearch.", "AI": {"tldr": "This paper introduces a monitor red teaming workflow to evaluate the robustness of monitoring systems against covert misbehavior in LLM agents. The findings suggest that agent awareness significantly impacts monitor reliability, monitor scaffolding is crucial, and targeted human oversight improves performance.", "motivation": "stress test monitoring systems for detecting covert misbehavior in autonomous LLM agents (e.g., secretly sharing private information)", "method": "systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We run MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse agent trajectories, alongside a new hybrid hierarchical-sequential scaffolding proposed in this work.", "result": "agent awareness dominates monitor awareness; monitor scaffolding matters more than monitor awareness; in a human-in-the-loop setting where humans discuss with the LLM monitor to get an updated judgment for the agent's behavior, targeted human oversight is most effective", "conclusion": "This work establishes a standard workflow for MRT, highlighting the lack of adversarial robustness for LLMs and humans when monitoring and detecting agent misbehavior. We release code, data, and logs to spur further research."}}
{"id": "2508.19855", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.19855", "abs": "https://arxiv.org/abs/2508.19855", "authors": ["Junnan Dong", "Siyu An", "Yifei Yu", "Qian-Wen Zhang", "Linhao Luo", "Xiao Huang", "Yunsheng Wu", "Di Yin", "Xing Sun"], "title": "Youtu-GraphRAG: Vertically Unified Agents for Graph Retrieval-Augmented Complex Reasoning", "comment": "19 pages, 7 figures, 6 tables", "summary": "Graph retrieval-augmented generation (GraphRAG) has effectively enhanced\nlarge language models in complex reasoning by organizing fragmented knowledge\ninto explicitly structured graphs. Prior efforts have been made to improve\neither graph construction or graph retrieval in isolation, yielding suboptimal\nperformance, especially when domain shifts occur. In this paper, we propose a\nvertically unified agentic paradigm, Youtu-GraphRAG, to jointly connect the\nentire framework as an intricate integration. Specifically, (i) a seed graph\nschema is introduced to bound the automatic extraction agent with targeted\nentity types, relations and attribute types, also continuously expanded for\nscalability over unseen domains; (ii) To obtain higher-level knowledge upon the\nschema, we develop novel dually-perceived community detection, fusing\nstructural topology with subgraph semantics for comprehensive knowledge\norganization. This naturally yields a hierarchical knowledge tree that supports\nboth top-down filtering and bottom-up reasoning with community summaries; (iii)\nAn agentic retriever is designed to interpret the same graph schema to\ntransform complex queries into tractable and parallel sub-queries. It\niteratively performs reflection for more advanced reasoning; (iv) To alleviate\nthe knowledge leaking problem in pre-trained LLM, we propose a tailored\nanonymous dataset and a novel 'Anonymity Reversion' task that deeply measures\nthe real performance of the GraphRAG frameworks. Extensive experiments across\nsix challenging benchmarks demonstrate the robustness of Youtu-GraphRAG,\nremarkably moving the Pareto frontier with up to 90.71% saving of token costs\nand 16.62% higher accuracy over state-of-the-art baselines. The results\nindicate our adaptability, allowing seamless domain transfer with minimal\nintervention on schema.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684GraphRAG\u6846\u67b6\uff0c\u5728token\u6210\u672c\u548c\u51c6\u786e\u7387\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08GraphRAG\uff09\u901a\u8fc7\u5c06\u5206\u6563\u7684\u77e5\u8bc6\u7ec4\u7ec7\u6210\u663e\u5f0f\u7ed3\u6784\u5316\u7684\u56fe\uff0c\u6709\u6548\u5730\u589e\u5f3a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4e2d\u7684\u80fd\u529b\u3002\u5148\u524d\u7684\u52aa\u529b\u5df2\u7ecf\u505a\u51fa\u6539\u8fdb\u7684\u56fe\u6784\u5efa\u6216\u56fe\u68c0\u7d22\u5b64\u7acb\uff0c\u4ea7\u751f\u6b21\u4f18\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5f53\u57df\u8f6c\u79fb\u53d1\u751f\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5782\u76f4\u7edf\u4e00\u7684agentic\u8303\u5f0f\uff0cYoutu-GraphRAG\uff0c\u4ee5\u8054\u5408\u8fde\u63a5\u6574\u4e2a\u6846\u67b6\u4f5c\u4e3a\u4e00\u4e2a\u590d\u6742\u7684\u96c6\u6210\u3002", "result": "\u5728\u516d\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6700\u591a\u53ef\u8282\u7701 90.71% \u7684 token \u6210\u672c\uff0c\u5e76\u63d0\u9ad8 16.62% \u7684\u51c6\u786e\u7387\u3002", "conclusion": "Youtu-GraphRAG\u5728\u516d\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\uff0c\u663e\u8457\u5730\u79fb\u52a8\u4e86\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6700\u591a\u53ef\u8282\u7701 90.71% \u7684 token \u6210\u672c\uff0c\u5e76\u63d0\u9ad8 16.62% \u7684\u51c6\u786e\u7387\u3002\u7ed3\u679c\u8868\u660e\u6211\u4eec\u7684\u9002\u5e94\u6027\uff0c\u5141\u8bb8\u65e0\u7f1d\u7684\u9886\u57df\u8f6c\u79fb\uff0c\u53ea\u9700\u5bf9\u6a21\u5f0f\u8fdb\u884c\u6700\u5c0f\u7684\u5e72\u9884\u3002"}}
{"id": "2508.19279", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19279", "abs": "https://arxiv.org/abs/2508.19279", "authors": ["Gunjan Jalori", "Preetika Verma", "Sercan \u00d6 Ar\u0131k"], "title": "FLAIRR-TS -- Forecasting LLM-Agents with Iterative Refinement and Retrieval for Time Series", "comment": "EMNLP", "summary": "Time series Forecasting with large languagemodels (LLMs) requires bridging\nnumericalpatterns and natural language. Effective fore-casting on LLM often\nrelies on extensive pre-processing and fine-tuning.Recent studiesshow that a\nfrozen LLM can rival specializedforecasters when supplied with a carefully\nen-gineered natural-language prompt, but craft-ing such a prompt for each task\nis itself oner-ous and ad-hoc. We introduce FLAIRR-TS, atest-time prompt\noptimization framework thatutilizes an agentic system: a\nForecaster-agentgenerates forecasts using an initial prompt,which is then\nrefined by a refiner agent, in-formed by past outputs and retrieved\nanalogs.This adaptive prompting generalizes across do-mains using creative\nprompt templates andgenerates high-quality forecasts without inter-mediate code\ngeneration.Experiments onbenchmark datasets show improved accuracyover static\nprompting and retrieval-augmentedbaselines, approaching the performance\nofspecialized prompts.FLAIRR-TS providesa practical alternative to tuning,\nachievingstrong performance via its agentic approach toadaptive prompt\nrefinement and retrieval.", "AI": {"tldr": "FLAIRR-TS\u662f\u4e00\u79cd\u7528\u4e8eLLM\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6d4b\u8bd5\u65f6\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u4ee3\u7406\u7cfb\u7edf\u81ea\u9002\u5e94\u5730\u6539\u8fdb\u63d0\u793a\uff0c\u4f18\u4e8e\u9759\u6001\u65b9\u6cd5\uff0c\u5e76\u63a5\u8fd1\u4e13\u95e8\u9884\u6d4b\u5668\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9700\u8981\u6865\u63a5\u6570\u503c\u6a21\u5f0f\u548c\u81ea\u7136\u8bed\u8a00\u3002\u5728LLM\u4e0a\u8fdb\u884c\u6709\u6548\u7684\u9884\u6d4b\u901a\u5e38\u4f9d\u8d56\u4e8e\u5927\u91cf\u7684\u9884\u5904\u7406\u548c\u5fae\u8c03\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5f53\u63d0\u4f9b\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u65f6\uff0c\u4e00\u4e2a\u51bb\u7ed3\u7684LLM\u53ef\u4ee5\u4e0e\u4e13\u95e8\u7684\u9884\u6d4b\u5668\u76f8\u5ab2\u7f8e\uff0c\u4f46\u662f\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u5236\u4f5c\u8fd9\u6837\u7684\u63d0\u793a\u672c\u8eab\u5c31\u5f88\u9ebb\u70e6\u548c\u968f\u610f\u3002", "method": "FLAIRR-TS\uff0c\u4e00\u4e2a\u6d4b\u8bd5\u65f6\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u5b83\u5229\u7528\u4e00\u4e2a\u4ee3\u7406\u7cfb\u7edf\uff1a\u4e00\u4e2a\u9884\u6d4b\u4ee3\u7406\u4f7f\u7528\u521d\u59cb\u63d0\u793a\u751f\u6210\u9884\u6d4b\uff0c\u7136\u540e\u7531\u4e00\u4e2a\u7ec6\u5316\u4ee3\u7406\u6839\u636e\u8fc7\u53bb\u7684\u8f93\u51fa\u548c\u68c0\u7d22\u5230\u7684\u7c7b\u4f3c\u7269\u8fdb\u884c\u7ec6\u5316\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFLAIRR-TS\u7684\u51c6\u786e\u6027\u4f18\u4e8e\u9759\u6001\u63d0\u793a\u548c\u68c0\u7d22\u589e\u5f3a\u57fa\u7ebf\uff0c\u63a5\u8fd1\u4e8e\u4e13\u95e8\u63d0\u793a\u7684\u6027\u80fd\u3002", "conclusion": "FLAIRR-TS\u901a\u8fc7\u5176\u4ee3\u7406\u65b9\u6cd5\u5b9e\u73b0\u81ea\u9002\u5e94\u63d0\u793a\u6539\u8fdb\u548c\u68c0\u7d22\uff0c\u5728\u4e0d\u8fdb\u884c\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2508.19294", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19294", "abs": "https://arxiv.org/abs/2508.19294", "authors": ["Ranjan Sapkota", "Manoj Karkee"], "title": "Object Detection with Multimodal Large Vision-Language Models: An In-depth Review", "comment": "First Peer Reviewed Review Paper for Object Detection with\n  Vision-Language Models (VLMs)", "summary": "The fusion of language and vision in large vision-language models (LVLMs) has\nrevolutionized deep learning-based object detection by enhancing adaptability,\ncontextual reasoning, and generalization beyond traditional architectures. This\nin-depth review presents a structured exploration of the state-of-the-art in\nLVLMs, systematically organized through a three-step research review process.\nFirst, we discuss the functioning of vision language models (VLMs) for object\ndetection, describing how these models harness natural language processing\n(NLP) and computer vision (CV) techniques to revolutionize object detection and\nlocalization. We then explain the architectural innovations, training\nparadigms, and output flexibility of recent LVLMs for object detection,\nhighlighting how they achieve advanced contextual understanding for object\ndetection. The review thoroughly examines the approaches used in integration of\nvisual and textual information, demonstrating the progress made in object\ndetection using VLMs that facilitate more sophisticated object detection and\nlocalization strategies. This review presents comprehensive visualizations\ndemonstrating LVLMs' effectiveness in diverse scenarios including localization\nand segmentation, and then compares their real-time performance, adaptability,\nand complexity to traditional deep learning systems. Based on the review, its\nis expected that LVLMs will soon meet or surpass the performance of\nconventional methods in object detection. The review also identifies a few\nmajor limitations of the current LVLM modes, proposes solutions to address\nthose challenges, and presents a clear roadmap for the future advancement in\nthis field. We conclude, based on this study, that the recent advancement in\nLVLMs have made and will continue to make a transformative impact on object\ndetection and robotic applications in the future.", "AI": {"tldr": "This review explores the state-of-the-art in LVLMs for object detection, highlighting their architectural innovations, training paradigms, and output flexibility. It also identifies limitations and proposes solutions for future advancement.", "motivation": "enhance adaptability, contextual reasoning, and generalization beyond traditional architectures in deep learning-based object detection", "method": "a three-step research review process", "result": "LVLMs will soon meet or surpass the performance of conventional methods in object detection", "conclusion": "LVLMs have made and will continue to make a transformative impact on object detection and robotic applications in the future."}}
{"id": "2508.19344", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19344", "abs": "https://arxiv.org/abs/2508.19344", "authors": ["Daniil Zelezetsky", "Egor Cherepanov", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "Re:Frame -- Retrieving Experience From Associative Memory", "comment": "11 pages, 3 figures", "summary": "Offline reinforcement learning (RL) often deals with suboptimal data when\ncollecting large expert datasets is unavailable or impractical. This limitation\nmakes it difficult for agents to generalize and achieve high performance, as\nthey must learn primarily from imperfect or inconsistent trajectories. A\ncentral challenge is therefore how to best leverage scarce expert\ndemonstrations alongside abundant but lower-quality data. We demonstrate that\nincorporating even a tiny amount of expert experience can substantially improve\nRL agent performance. We introduce Re:Frame (Retrieving Experience From\nAssociative Memory), a plug-in module that augments a standard offline RL\npolicy (e.g., Decision Transformer) with a small external Associative Memory\nBuffer (AMB) populated by expert trajectories drawn from a separate dataset.\nDuring training on low-quality data, the policy learns to retrieve expert data\nfrom the Associative Memory Buffer (AMB) via content-based associations and\nintegrate them into decision-making; the same AMB is queried at evaluation.\nThis requires no environment interaction and no modifications to the backbone\narchitecture. On D4RL MuJoCo tasks, using as few as 60 expert trajectories\n(0.1% of a 6000-trajectory dataset), Re:Frame consistently improves over a\nstrong Decision Transformer baseline in three of four settings, with gains up\nto +10.7 normalized points. These results show that Re:Frame offers a simple\nand data-efficient way to inject scarce expert knowledge and substantially\nimprove offline RL from low-quality datasets.", "AI": {"tldr": "Re:Frame \u662f\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u901a\u8fc7\u6ce8\u5165\u7a00\u7f3a\u7684\u4e13\u5bb6\u77e5\u8bc6\u5e76\u5927\u5e45\u6539\u8fdb\u4ece\u4f4e\u8d28\u91cf\u6570\u636e\u96c6\u8fdb\u884c\u79bb\u7ebf RL\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60 (RL) \u7ecf\u5e38\u5904\u7406\u6b21\u4f18\u6570\u636e\uff0c\u56e0\u4e3a\u6536\u96c6\u5927\u578b\u4e13\u5bb6\u6570\u636e\u96c6\u4e0d\u53ef\u7528\u6216\u4e0d\u5207\u5b9e\u9645\u3002\u8fd9\u4f7f\u5f97\u667a\u80fd\u4f53\u96be\u4ee5\u6cdb\u5316\u5e76\u83b7\u5f97\u9ad8\u6027\u80fd\uff0c\u56e0\u4e3a\u5b83\u4eec\u5fc5\u987b\u4e3b\u8981\u4ece\u4e0d\u5b8c\u5584\u6216\u4e0d\u4e00\u81f4\u7684\u8f68\u8ff9\u4e2d\u5b66\u4e60\u3002\u56e0\u6b64\uff0c\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\u662f\u5982\u4f55\u6700\u597d\u5730\u5229\u7528\u7a00\u7f3a\u7684\u4e13\u5bb6\u6f14\u793a\u4ee5\u53ca\u5927\u91cf\u4f46\u8d28\u91cf\u8f83\u4f4e\u7684\u6570\u636e\u3002", "method": "\u5f15\u5165 Re:Frame\uff0c\u4e00\u4e2a\u63d2\u4ef6\u6a21\u5757\uff0c\u5b83\u4f7f\u7528\u4e00\u4e2a\u7531\u4e13\u5bb6\u8f68\u8ff9\u586b\u5145\u7684\u5c0f\u578b\u5916\u90e8\u5173\u8054\u8bb0\u5fc6\u7f13\u51b2\u533a (AMB) \u6765\u589e\u5f3a\u6807\u51c6\u79bb\u7ebf RL \u7b56\u7565\uff08\u4f8b\u5982\uff0c\u51b3\u7b56\u8f6c\u6362\u5668\uff09\u3002", "result": "\u5728 D4RL MuJoCo \u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u5c11\u81f3 60 \u4e2a\u4e13\u5bb6\u8f68\u8ff9\uff08\u5360 6000 \u4e2a\u8f68\u8ff9\u6570\u636e\u96c6\u7684 0.1%\uff09\uff0cRe:Frame \u5728\u56db\u5206\u4e4b\u4e09\u7684\u8bbe\u7f6e\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5927\u7684\u51b3\u7b56\u8f6c\u6362\u5668\u57fa\u7ebf\uff0c\u589e\u76ca\u9ad8\u8fbe +10.7 \u5f52\u4e00\u5316\u70b9\u3002", "conclusion": "Re:Frame \u901a\u8fc7\u68c0\u7d22\u5173\u8054\u8bb0\u5fc6\u5e93\u4e2d\u7684\u5c11\u91cf\u4e13\u5bb6\u77e5\u8bc6\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u4ece\u4f4e\u8d28\u91cf\u6570\u636e\u96c6\u8fdb\u884c\u79bb\u7ebf RL \u7684\u6027\u80fd\u3002"}}
{"id": "2508.19502", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19502", "abs": "https://arxiv.org/abs/2508.19502", "authors": ["Xifeng Yao", "Chengyuan Ma", "Dongyu Lang", "Yinhao Ni", "Zhiwei Xu", "Huarui Xie", "Zihao Chen", "Guang Shen", "Dandan Tu", "Yi Bai", "Changzheng Zhang"], "title": "SLIM: Subtrajectory-Level Elimination for More Effective Reasoning", "comment": "EMNLP 2025 Findings", "summary": "In recent months, substantial progress has been made in complex reasoning of\nLarge Language Models, particularly through the application of test-time\nscaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When\nresponding to a query, these models generate an extended reasoning trajectory,\nduring which the model explores, reflects, backtracks, and self-verifies before\narriving at a conclusion. However, fine-tuning models with such reasoning\ntrajectories may not always be optimal. Our findings indicate that not all\ncomponents within these reasoning trajectories contribute positively to the\nreasoning process; in fact, some components may affect the overall performance\nnegatively. In this study, we divide a reasoning trajectory into individual\nsubtrajectories and develop a \"5+2\" framework to: (1) systematically identify\nsuboptimal subtrajectories within the reasoning trajectory based on five\nhuman-established criteria; (2) assess the independence of the suboptimal\nsubtrajectories identified in (1) from the subsequent content, ensuring that\ntheir elimination does not compromise overall flow and coherence of the\nreasoning process. Additionally, a sampling algorithm, built upon the \"5+2\"\nframework, is employed to select data whose reasoning process is free from\nsuboptimal subtrajectories to the highest degree. Experimental results\ndemonstrate that our method can reduce the number of suboptimal subtrajectories\nby 25.9\\% during the inference. Furthermore, our method achieves an average\naccuracy of 58.92\\% on highly challenging math benchmarks with only two thirds\nof training data, surpassing the average accuracy of 58.06\\% achieved with the\nentire data, and outperforming open-source datasets, when fine-tuning\nQwen2.5-Math-7B. Finally, We validated our method under resource constraints\nand observed improved performance across various inference token limits.", "AI": {"tldr": "The paper introduces a method to improve the reasoning process of LLMs by identifying and removing suboptimal parts of their reasoning trajectories, leading to better performance with less data.", "motivation": "Fine-tuning models with full reasoning trajectories may not be optimal as some components can negatively impact performance.", "method": "A \"5+2\" framework is developed to identify and eliminate suboptimal subtrajectories within the reasoning trajectory. A sampling algorithm is used to select data with fewer suboptimal subtrajectories.", "result": "The method reduces suboptimal subtrajectories by 25.9% during inference and achieves an average accuracy of 58.92% on math benchmarks with only two thirds of training data, surpassing the average accuracy of 58.06% achieved with the entire data.Improved performance is observed under resource constraints.", "conclusion": "The proposed method reduces suboptimal subtrajectories by 25.9% during inference and achieves 58.92% accuracy on math benchmarks with 2/3 of the training data, outperforming baseline models and datasets."}}
{"id": "2508.19918", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.19918", "abs": "https://arxiv.org/abs/2508.19918", "authors": ["Manato Tajiri", "Michimasa Inaba"], "title": "Refining Text Generation for Realistic Conversational Recommendation via Direct Preference Optimization", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Conversational Recommender Systems (CRSs) aim to elicit user preferences via\nnatural dialogue to provide suitable item recommendations. However, current\nCRSs often deviate from realistic human interactions by rapidly recommending\nitems in brief sessions. This work addresses this gap by leveraging Large\nLanguage Models (LLMs) to generate dialogue summaries from dialogue history and\nitem recommendation information from item description. This approach enables\nthe extraction of both explicit user statements and implicit preferences\ninferred from the dialogue context. We introduce a method using Direct\nPreference Optimization (DPO) to ensure dialogue summary and item\nrecommendation information are rich in information crucial for effective\nrecommendations. Experiments on two public datasets validate our method's\neffectiveness in fostering more natural and realistic conversational\nrecommendation processes.Our implementation is publicly available\nat:https://github.com/UEC-InabaLab/Refining-LLM-Text", "AI": {"tldr": "This paper introduces a method using LLMs and DPO to generate dialogue summaries and item recommendations, improving the naturalness and realism of conversational recommendation systems.", "motivation": "Current CRSs often deviate from realistic human interactions by rapidly recommending items in brief sessions.", "method": "Leveraging Large Language Models (LLMs) to generate dialogue summaries from dialogue history and item recommendation information from item description; Direct Preference Optimization (DPO) is used to ensure dialogue summary and item recommendation information are rich in information crucial for effective recommendations.", "result": "The approach enables the extraction of both explicit user statements and implicit preferences inferred from the dialogue context.", "conclusion": "The method's effectiveness in fostering more natural and realistic conversational recommendation processes is validated through experiments on two public datasets."}}
{"id": "2508.19282", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19282", "abs": "https://arxiv.org/abs/2508.19282", "authors": ["Ziqiang Cui", "Yunpeng Weng", "Xing Tang", "Peiyang Liu", "Shiwei Li", "Bowei He", "Jiamin Chen", "Xiuqiang He", "Chen Ma"], "title": "CORE: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising approach to\nenhance the timeliness of knowledge and the factual accuracy of responses in\nLarge Language Models (LLMs). However, the inclusion of excessive retrieved\ndocuments substantially increases the input length, leading to higher\ncomputational costs. Previous studies have attempted to compress retrieved\ndocuments into shorter texts before in-context integration, but such methods\noften compromise end-task performance. The lack of well-defined compression\ntargets forces many approaches to rely on fixed heuristics, which cannot\nguarantee that the compressed content will effectively support the end task. To\naddress these limitations, we propose CORE, a novel method designed to achieve\nlossless context compression for RAG. CORE employs reinforcement learning to\noptimize the compression process without relying on predefined compression\nlabels. Specifically, it utilizes end-task performance as a reward signal and\napplies Generalized Reinforcement Learning Policy Optimization (GRPO) to train\nthe compressor. This end-to-end training framework enables the compressor to\ngenerate summaries that maximize the accuracy of answers generated by the LLM.\nExtensive experiments on four datasets demonstrate the superiority of our\napproach. With a high compression ratio of 3\\%, our method not only avoids\nperformance degradation compared to prepending full documents across all\ndatasets but also improves the average Exact Match (EM) score by 3.3 points.\nThe code will be released soon.", "AI": {"tldr": "This paper presents CORE, a reinforcement learning approach for compressing retrieved documents in RAG, which maintains performance while significantly reducing input length and computational cost.", "motivation": "The inclusion of excessive retrieved documents in RAG increases computational costs, and previous compression methods compromise end-task performance due to the lack of well-defined compression targets.", "method": "The paper uses reinforcement learning to optimize the compression process, utilizing end-task performance as a reward signal and applying Generalized Reinforcement Learning Policy Optimization (GRPO) to train the compressor.", "result": "CORE achieves a high compression ratio of 3% without performance degradation and improves the average Exact Match (EM) score by 3.3 points across four datasets.", "conclusion": "The paper introduces CORE, a reinforcement learning method for lossless context compression in RAG, achieving a 3% compression ratio without performance degradation and improving the average Exact Match score by 3.3 points."}}
{"id": "2508.19295", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19295", "abs": "https://arxiv.org/abs/2508.19295", "authors": ["Sauptik Dhar", "Nicholas Buoncristiani", "Joe Anakata", "Haoyu Zhang", "Michelle Munson"], "title": "Large VLM-based Stylized Sports Captioning", "comment": null, "summary": "The advent of large (visual) language models (LLM / LVLM) have led to a\ndeluge of automated human-like systems in several domains including social\nmedia content generation, search and recommendation, healthcare prognosis, AI\nassistants for cognitive tasks etc. Although these systems have been\nsuccessfully integrated in production; very little focus has been placed on\nsports, particularly accurate identification and natural language description\nof the game play. Most existing LLM/LVLMs can explain generic sports\nactivities, but lack sufficient domain-centric sports' jargon to create natural\n(human-like) descriptions. This work highlights the limitations of existing\nSoTA LLM/LVLMs for generating production-grade sports captions from images in a\ndesired stylized format, and proposes a two-level fine-tuned LVLM pipeline to\naddress that. The proposed pipeline yields an improvement > 8-10% in the F1,\nand > 2-10% in BERT score compared to alternative approaches. In addition, it\nhas a small runtime memory footprint and fast execution time. During Super Bowl\nLIX the pipeline proved its practical application for live professional sports\njournalism; generating highly accurate and stylized captions at the rate of 6\nimages per 3-5 seconds for over 1000 images during the game play.", "AI": {"tldr": "Existing LLM/LVLMs are not good at sports captioning. This paper proposes a new pipeline that improves accuracy and speed, and was tested during Super Bowl LIX.", "motivation": "Existing LLM/LVLMs lack sufficient domain-centric sports' jargon for accurate identification and natural language description of game play, hindering their ability to generate production-grade sports captions.", "method": "A two-level fine-tuned LVLM pipeline.", "result": "The proposed pipeline yields an improvement > 8-10% in the F1, and > 2-10% in BERT score compared to alternative approaches. It generates captions at a rate of 6 images per 3-5 seconds.", "conclusion": "The proposed two-level fine-tuned LVLM pipeline addresses the limitations of existing SoTA LLM/LVLMs in generating production-grade sports captions, demonstrating improved F1 and BERT scores, small memory footprint, and fast execution time. It was successfully applied during Super Bowl LIX."}}
{"id": "2508.19352", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19352", "abs": "https://arxiv.org/abs/2508.19352", "authors": ["Adarsh Jamadandi", "Jing Xu", "Adam Dziedzic", "Franziska Boenisch"], "title": "Memorization in Graph Neural Networks", "comment": null, "summary": "Deep neural networks (DNNs) have been shown to memorize their training data,\nyet similar analyses for graph neural networks (GNNs) remain largely\nunder-explored. We introduce NCMemo (Node Classification Memorization), the\nfirst framework to quantify label memorization in semi-supervised node\nclassification. We first establish an inverse relationship between memorization\nand graph homophily, i.e., the property that connected nodes share similar\nlabels/features. We find that lower homophily significantly increases\nmemorization, indicating that GNNs rely on memorization to learn less\nhomophilic graphs. Secondly, we analyze GNN training dynamics. We find that the\nincreased memorization in low homophily graphs is tightly coupled to the GNNs'\nimplicit bias on using graph structure during learning. In low homophily\nregimes, this structure is less informative, hence inducing memorization of the\nnode labels to minimize training loss. Finally, we show that nodes with higher\nlabel inconsistency in their feature-space neighborhood are significantly more\nprone to memorization. Building on our insights into the link between graph\nhomophily and memorization, we investigate graph rewiring as a means to\nmitigate memorization. Our results demonstrate that this approach effectively\nreduces memorization without compromising model performance. Moreover, we show\nthat it lowers the privacy risk for previously memorized data points in\npractice. Thus, our work not only advances understanding of GNN learning but\nalso supports more privacy-preserving GNN deployment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc (GNN) \u4e2d\u7684\u6807\u7b7e\u8bb0\u5fc6\u5316\u73b0\u8c61\uff0c\u53d1\u73b0\u4f4e\u540c\u8d28\u6027\u56fe\u4f1a\u589e\u52a0\u8bb0\u5fc6\u5316\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u56fe\u91cd\u5e03\u7ebf\u6765\u51cf\u8f7b\u8bb0\u5fc6\u5316\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc (GNN) \u7684\u8bb0\u5fc6\u5316\u5206\u6790\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u672a\u88ab\u63a2\u7d22\u3002\u8fd9\u9879\u7814\u7a76\u65e8\u5728\u91cf\u5316\u534a\u76d1\u7763\u8282\u70b9\u5206\u7c7b\u4e2d\u7684\u6807\u7b7e\u8bb0\u5fc6\u5316\u3002", "method": "\u63d0\u51fa\u4e86 NCMemo \u6846\u67b6\u6765\u91cf\u5316\u534a\u76d1\u7763\u8282\u70b9\u5206\u7c7b\u4e2d\u7684\u6807\u7b7e\u8bb0\u5fc6\u5316\uff0c\u5e76\u5206\u6790\u4e86 GNN \u7684\u8bad\u7ec3\u52a8\u6001\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u540c\u8d28\u6027\u8f83\u4f4e\u4f1a\u663e\u8457\u589e\u52a0\u8bb0\u5fc6\u5316\uff0c\u8868\u660e GNN \u4f9d\u8d56\u4e8e\u8bb0\u5fc6\u5316\u6765\u5b66\u4e60\u540c\u8d28\u6027\u8f83\u4f4e\u7684\u56fe\u3002\u6b64\u5916\uff0c\u5728\u4f4e\u540c\u8d28\u6027\u56fe\u4e2d\uff0c\u8bb0\u5fc6\u5316\u7684\u589e\u52a0\u4e0e GNN \u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u56fe\u7ed3\u6784\u7684\u5185\u9690\u504f\u5dee\u5bc6\u5207\u76f8\u5173\u3002\u7279\u5f81\u7a7a\u95f4\u90bb\u57df\u4e2d\u6807\u7b7e\u4e0d\u4e00\u81f4\u6027\u8f83\u9ad8\u7684\u8282\u70b9\u66f4\u5bb9\u6613\u8bb0\u5fc6\u5316\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u56fe\u91cd\u5e03\u7ebf\u53ef\u4ee5\u51cf\u8f7b GNN \u4e2d\u7684\u8bb0\u5fc6\u5316\uff0c\u800c\u4e0d\u4f1a\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u964d\u4f4e\u5148\u524d\u8bb0\u5fc6\u5316\u6570\u636e\u70b9\u7684\u9690\u79c1\u98ce\u9669\u3002\u8fd9\u9879\u5de5\u4f5c\u4e0d\u4ec5\u4fc3\u8fdb\u4e86\u5bf9 GNN \u5b66\u4e60\u7684\u7406\u89e3\uff0c\u800c\u4e14\u652f\u6301\u66f4\u6ce8\u91cd\u9690\u79c1\u4fdd\u62a4\u7684 GNN \u90e8\u7f72\u3002"}}
{"id": "2508.19505", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19505", "abs": "https://arxiv.org/abs/2508.19505", "authors": ["Gerard Boxo", "Ryan Socha", "Daniel Yoo", "Shivam Raval"], "title": "Caught in the Act: a mechanistic approach to detecting deception", "comment": null, "summary": "Sophisticated instrumentation for AI systems might have indicators that\nsignal misalignment from human values, not unlike a \"check engine\" light in\ncars. One such indicator of misalignment is deceptiveness in generated\nresponses. Future AI instrumentation may have the ability to detect when an LLM\ngenerates deceptive responses while reasoning about seemingly plausible but\nincorrect answers to factual questions. In this work, we demonstrate that\nlinear probes on LLMs internal activations can detect deception in their\nresponses with extremely high accuracy. Our probes reach a maximum of greater\nthan 90% accuracy in distinguishing between deceptive and non-deceptive\narguments generated by llama and qwen models ranging from 1.5B to 14B\nparameters, including their DeepSeek-r1 finetuned variants. We observe that\nprobes on smaller models (1.5B) achieve chance accuracy at detecting deception,\nwhile larger models (greater than 7B) reach 70-80%, with their reasoning\ncounterparts exceeding 90%. The layer-wise probe accuracy follows a three-stage\npattern across layers: near-random (50%) in early layers, peaking in middle\nlayers, and slightly declining in later layers. Furthermore, using an iterative\nnull space projection approach, we find multitudes of linear directions that\nencode deception, ranging from 20 in Qwen 3B to nearly 100 in DeepSeek 7B and\nQwen 14B models.", "AI": {"tldr": "AI\u53ef\u4ee5\u901a\u8fc7\u68c0\u6d4bLLM\u56de\u5e94\u4e2d\u7684\u6b3a\u9a97\u6027\u6765\u5224\u65ad\u5176\u662f\u5426\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u4e00\u81f4\u3002", "motivation": "AI\u7cfb\u7edf\u53ef\u80fd\u5b58\u5728\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u4e0d\u7b26\u7684\u6307\u6807\uff0c\u4f8b\u5982\u751f\u6210\u7684\u56de\u5e94\u4e2d\u7684\u6b3a\u9a97\u6027\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u63a2\u9488\u68c0\u6d4bLLM\u5185\u90e8\u6fc0\u6d3b\u4e2d\u7684\u6b3a\u9a97\u6027\u56de\u5e94\u3002", "result": "\u5728llama\u548cqwen\u6a21\u578b\uff08\u53c2\u6570\u8303\u56f4\u4ece1.5B\u523014B\uff09\u751f\u6210\u7684\u6b3a\u9a97\u6027\u548c\u975e\u6b3a\u9a97\u6027\u8bba\u8bc1\u4e2d\uff0c\u63a2\u9488\u7684\u51c6\u786e\u7387\u6700\u9ad8\u53ef\u8fbe90%\u4ee5\u4e0a\u3002\u8f83\u5c0f\u6a21\u578b\uff081.5B\uff09\u7684\u63a2\u9488\u5728\u68c0\u6d4b\u6b3a\u9a97\u65b9\u9762\u7684\u51c6\u786e\u7387\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\uff0c\u800c\u8f83\u5927\u6a21\u578b\uff08\u5927\u4e8e7B\uff09\u7684\u51c6\u786e\u7387\u8fbe\u523070-80%\uff0c\u63a8\u7406\u6a21\u578b\u7684\u51c6\u786e\u7387\u8d85\u8fc790%\u3002", "conclusion": "\u7ebf\u6027\u63a2\u9488\u53ef\u4ee5\u9ad8\u7cbe\u5ea6\u5730\u68c0\u6d4bLLM\u5728\u56de\u7b54\u4e8b\u5b9e\u6027\u95ee\u9898\u65f6\u4ea7\u751f\u7684\u6b3a\u9a97\u6027\u56de\u5e94\u3002"}}
{"id": "2508.19467", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.19467", "abs": "https://arxiv.org/abs/2508.19467", "authors": ["Sumon Kanti Dey", "Jeanne M. Powell", "Azra Ismail", "Jeanmarie Perrone", "Abeed Sarker"], "title": "Inference Gap in Domain Expertise and Machine Intelligence in Named Entity Recognition: Creation of and Insights from a Substance Use-related Dataset", "comment": "Dataset and code: https://github.com/SumonKantiDey/Reddit_Impacts_NER", "summary": "Nonmedical opioid use is an urgent public health challenge, with far-reaching\nclinical and social consequences that are often underreported in traditional\nhealthcare settings. Social media platforms, where individuals candidly share\nfirst-person experiences, offer a valuable yet underutilized source of insight\ninto these impacts. In this study, we present a named entity recognition (NER)\nframework to extract two categories of self-reported consequences from social\nmedia narratives related to opioid use: ClinicalImpacts (e.g., withdrawal,\ndepression) and SocialImpacts (e.g., job loss). To support this task, we\nintroduce RedditImpacts 2.0, a high-quality dataset with refined annotation\nguidelines and a focus on first-person disclosures, addressing key limitations\nof prior work. We evaluate both fine-tuned encoder-based models and\nstate-of-the-art large language models (LLMs) under zero- and few-shot\nin-context learning settings. Our fine-tuned DeBERTa-large model achieves a\nrelaxed token-level F1 of 0.61 [95% CI: 0.43-0.62], consistently outperforming\nLLMs in precision, span accuracy, and adherence to task-specific guidelines.\nFurthermore, we show that strong NER performance can be achieved with\nsubstantially less labeled data, emphasizing the feasibility of deploying\nrobust models in resource-limited settings. Our findings underscore the value\nof domain-specific fine-tuning for clinical NLP tasks and contribute to the\nresponsible development of AI tools that may enhance addiction surveillance,\nimprove interpretability, and support real-world healthcare decision-making.\nThe best performing model, however, still significantly underperforms compared\nto inter-expert agreement (Cohen's kappa: 0.81), demonstrating that a gap\npersists between expert intelligence and current state-of-the-art NER/AI\ncapabilities for tasks requiring deep domain knowledge.", "AI": {"tldr": "This study uses NER to extract clinical and social impacts of opioid use from social media, introduces a new dataset, and finds that fine-tuned models outperform LLMs but still lag behind expert agreement.", "motivation": "Social media platforms offer a valuable yet underutilized source of insight into the clinical and social consequences of nonmedical opioid use.", "method": "a named entity recognition (NER) framework to extract two categories of self-reported consequences from social media narratives related to opioid use", "result": "A fine-tuned DeBERTa-large model achieves a relaxed token-level F1 of 0.61 [95% CI: 0.43-0.62], consistently outperforming LLMs in precision, span accuracy, and adherence to task-specific guidelines. Strong NER performance can be achieved with substantially less labeled data.", "conclusion": "The best performing model still significantly underperforms compared to inter-expert agreement, demonstrating that a gap persists between expert intelligence and current state-of-the-art NER/AI capabilities for tasks requiring deep domain knowledge."}}
{"id": "2508.19357", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19357", "abs": "https://arxiv.org/abs/2508.19357", "authors": ["Peiran Zhou", "Junnan Zhu", "Yichen Shen", "Ruoxi Yu"], "title": "Context-Adaptive Synthesis and Compression for Enhanced Retrieval-Augmented Generation in Complex Domains", "comment": null, "summary": "Large Language Models (LLMs) excel in language tasks but are prone to\nhallucinations and outdated knowledge. Retrieval-Augmented Generation (RAG)\nmitigates these by grounding LLMs in external knowledge. However, in complex\ndomains involving multiple, lengthy, or conflicting documents, traditional RAG\nsuffers from information overload and inefficient synthesis, leading to\ninaccurate and untrustworthy answers. To address this, we propose CASC\n(Context-Adaptive Synthesis and Compression), a novel framework that\nintelligently processes retrieved contexts. CASC introduces a Context Analyzer\n& Synthesizer (CAS) module, powered by a fine-tuned smaller LLM, which performs\nkey information extraction, cross-document consistency checking and conflict\nresolution, and question-oriented structured synthesis. This process transforms\nraw, scattered information into a highly condensed, structured, and\nsemantically rich context, significantly reducing the token count and cognitive\nload for the final Reader LLM. We evaluate CASC on SciDocs-QA, a new\nchallenging multi-document question answering dataset designed for complex\nscientific domains with inherent redundancies and conflicts. Our extensive\nexperiments demonstrate that CASC consistently outperforms strong baselines.", "AI": {"tldr": "CASC\u901a\u8fc7\u667a\u80fd\u5904\u7406\u68c0\u7d22\u5230\u7684\u4e0a\u4e0b\u6587\u6765\u6539\u8fdbRAG\uff0c\u4ece\u800c\u5728\u590d\u6742\u9886\u57df\u4e2d\u63d0\u4f9b\u66f4\u51c6\u786e\u548c\u503c\u5f97\u4fe1\u8d56\u7684\u7b54\u6848\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\u548c\u8fc7\u65f6\u7684\u77e5\u8bc6\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u901a\u8fc7\u5c06 LLM \u5b9a\u4f4d\u4e8e\u5916\u90e8\u77e5\u8bc6\u6765\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\u3002\u7136\u800c\uff0c\u5728\u6d89\u53ca\u591a\u4e2a\u3001\u5197\u957f\u6216\u51b2\u7a81\u6587\u6863\u7684\u590d\u6742\u9886\u57df\u4e2d\uff0c\u4f20\u7edf\u7684 RAG \u4f1a\u53d7\u5230\u4fe1\u606f\u8fc7\u8f7d\u548c\u7efc\u5408\u6548\u7387\u4f4e\u4e0b\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u5bfc\u81f4\u4e0d\u51c6\u786e\u548c\u4e0d\u53ef\u4fe1\u7684\u7b54\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCASC\uff08Context-Adaptive Synthesis and Compression\uff09\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u667a\u80fd\u5730\u5904\u7406\u68c0\u7d22\u5230\u7684\u4e0a\u4e0b\u6587\u3002CASC \u5f15\u5165\u4e86\u4e00\u4e2a\u4e0a\u4e0b\u6587\u5206\u6790\u5668\u548c\u5408\u6210\u5668 (CAS) \u6a21\u5757\uff0c\u8be5\u6a21\u5757\u7531\u7ecf\u8fc7\u5fae\u8c03\u7684\u8f83\u5c0f LLM \u63d0\u4f9b\u652f\u6301\uff0c\u8be5\u6a21\u5757\u6267\u884c\u5173\u952e\u4fe1\u606f\u63d0\u53d6\u3001\u8de8\u6587\u6863\u4e00\u81f4\u6027\u68c0\u67e5\u548c\u51b2\u7a81\u89e3\u51b3\u4ee5\u53ca\u9762\u5411\u95ee\u9898\u7684\u7ed3\u6784\u5316\u5408\u6210\u3002", "result": "CASC\u6301\u7eed\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u3002", "conclusion": "CASC\u5728SciDocs-QA\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002"}}
{"id": "2508.19298", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19298", "abs": "https://arxiv.org/abs/2508.19298", "authors": ["Abu Sufian", "Anirudha Ghosh", "Debaditya Barman", "Marco Leo", "Cosimo Distante"], "title": "DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models", "comment": "6 pages, 4 figures, 13th International Workshop on Biometrics and\n  Forensics (IWBF)", "summary": "Large Vision Language Models (LVLMs) have demonstrated remarkable\ncapabilities across various downstream tasks, including biometric face\nrecognition (FR) with description. However, demographic biases remain a\ncritical concern in FR, as these foundation models often fail to perform\nequitably across diverse demographic groups, considering ethnicity/race,\ngender, and age. Therefore, through our work DemoBias, we conduct an empirical\nevaluation to investigate the extent of demographic biases in LVLMs for\nbiometric FR with textual token generation tasks. We fine-tuned and evaluated\nthree widely used pre-trained LVLMs: LLaVA, BLIP-2, and PaliGemma on our own\ngenerated demographic-balanced dataset. We utilize several evaluation metrics,\nlike group-specific BERTScores and the Fairness Discrepancy Rate, to quantify\nand trace the performance disparities. The experimental results deliver\ncompelling insights into the fairness and reliability of LVLMs across diverse\ndemographic groups. Our empirical study uncovered demographic biases in LVLMs,\nwith PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino,\nCaucasian, and South Asian groups, whereas BLIP-2 demonstrated comparably\nconsistent. Repository: https://github.com/Sufianlab/DemoBias.", "AI": {"tldr": "DemoBias\u7814\u7a76\u4e86LVLMs\u5728\u751f\u7269\u7279\u5f81\u4eba\u8138\u8bc6\u522b\u4e2d\u5b58\u5728\u7684\u4eba\u53e3\u7edf\u8ba1\u504f\u5dee\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(LVLMs)\u5728\u5305\u62ec\u5e26\u6709\u63cf\u8ff0\u7684\u751f\u7269\u7279\u5f81\u4eba\u8138\u8bc6\u522b(FR)\u7b49\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u4eba\u53e3\u7edf\u8ba1\u504f\u5dee\u4ecd\u7136\u662fFR\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u57fa\u7840\u6a21\u578b\u901a\u5e38\u65e0\u6cd5\u5728\u4e0d\u540c\u7684\u65cf\u88d4/\u79cd\u65cf\u3001\u6027\u522b\u548c\u5e74\u9f84\u7b49\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\u4e2d\u516c\u5e73\u5730\u6267\u884c\u3002", "method": "\u5728\u81ea\u751f\u6210\u7684\u4eba\u53e3\u7edf\u8ba1\u5e73\u8861\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9LLaVA, BLIP-2, \u548cPaliGemma\u8fd9\u4e09\u4e2a\u9884\u8bad\u7ec3LVLMs\u8fdb\u884c\u4e86\u5fae\u8c03\u548c\u8bc4\u4f30\u3002\u4f7f\u7528\u7279\u5b9a\u7fa4\u4f53\u7684BERTScores\u548cFairness Discrepancy Rate\u7b49\u8bc4\u4f30\u6307\u6807\u6765\u91cf\u5316\u548c\u8ffd\u8e2a\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u63ed\u793a\u4e86LVLMs\u5728\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\u4e2d\u7684\u516c\u5e73\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "LVLMs\u5b58\u5728\u4eba\u53e3\u7edf\u8ba1\u504f\u5dee\uff0cPaliGemma\u548cLLaVA\u5728Hispanic/Latino, Caucasian, \u548cSouth Asian\u7fa4\u4f53\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5dee\u5f02\uff0c\u800cBLIP-2\u8868\u73b0\u76f8\u5bf9\u7a33\u5b9a\u3002"}}
{"id": "2508.19353", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19353", "abs": "https://arxiv.org/abs/2508.19353", "authors": ["Marcin Osial", "Bartosz W\u00f3jcik", "Bartosz Zieli\u0144ski", "Sebastian Cygert"], "title": "Efficient Multi-Source Knowledge Transfer by Model Merging", "comment": null, "summary": "While transfer learning is an advantageous strategy, it overlooks the\nopportunity to leverage knowledge from numerous available models online.\nAddressing this multi-source transfer learning problem is a promising path to\nboost adaptability and cut re-training costs. However, existing approaches are\ninherently coarse-grained, lacking the necessary precision for granular\nknowledge extraction and the aggregation efficiency required to fuse knowledge\nfrom either a large number of source models or those with high parameter\ncounts. We address these limitations by leveraging Singular Value Decomposition\n(SVD) to first decompose each source model into its elementary, rank-one\ncomponents. A subsequent aggregation stage then selects only the most salient\ncomponents from all sources, thereby overcoming the previous efficiency and\nprecision limitations. To best preserve and leverage the synthesized knowledge\nbase, our method adapts to the target task by fine-tuning only the principal\nsingular values of the merged matrix. In essence, this process only\nrecalibrates the importance of top SVD components. The proposed framework\nallows for efficient transfer learning, is robust to perturbations both at the\ninput level and in the parameter space (e.g., noisy or pruned sources), and\nscales well computationally.", "AI": {"tldr": "This paper introduces a new multi-source transfer learning framework using SVD to decompose and aggregate knowledge from multiple models, achieving efficiency, robustness, and scalability.", "motivation": "existing approaches are inherently coarse-grained, lacking the necessary precision for granular knowledge extraction and the aggregation efficiency required to fuse knowledge from either a large number of source models or those with high parameter counts.", "method": "leveraging Singular Value Decomposition (SVD) to first decompose each source model into its elementary, rank-one components. A subsequent aggregation stage then selects only the most salient components from all sources", "result": "overcoming the previous efficiency and precision limitations", "conclusion": "The proposed framework allows for efficient transfer learning, is robust to perturbations both at the input level and in the parameter space (e.g., noisy or pruned sources), and scales well computationally."}}
{"id": "2508.19562", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19562", "abs": "https://arxiv.org/abs/2508.19562", "authors": ["Trisanth Srinivasan", "Santosh Patapati"], "title": "Democracy-in-Silico: Institutional Design as Alignment in AI-Governed Polities", "comment": null, "summary": "This paper introduces Democracy-in-Silico, an agent-based simulation where\nsocieties of advanced AI agents, imbued with complex psychological personas,\ngovern themselves under different institutional frameworks. We explore what it\nmeans to be human in an age of AI by tasking Large Language Models (LLMs) to\nembody agents with traumatic memories, hidden agendas, and psychological\ntriggers. These agents engage in deliberation, legislation, and elections under\nvarious stressors, such as budget crises and resource scarcity. We present a\nnovel metric, the Power-Preservation Index (PPI), to quantify misaligned\nbehavior where agents prioritize their own power over public welfare. Our\nfindings demonstrate that institutional design, specifically the combination of\na Constitutional AI (CAI) charter and a mediated deliberation protocol, serves\nas a potent alignment mechanism. These structures significantly reduce corrupt\npower-seeking behavior, improve policy stability, and enhance citizen welfare\ncompared to less constrained democratic models. The simulation reveals that an\ninstitutional design may offer a framework for aligning the complex, emergent\nbehaviors of future artificial agent societies, forcing us to reconsider what\nhuman rituals and responsibilities are essential in an age of shared authorship\nwith non-human entities.", "AI": {"tldr": "AI agents with complex personas govern themselves in a simulation. Constitutional AI and mediated deliberation reduce corruption and improve welfare.", "motivation": "Explore the meaning of being human in the age of AI by simulating societies of advanced AI agents governing themselves under different institutional frameworks.", "method": "Agent-based simulation using Large Language Models (LLMs) to embody agents with complex psychological personas, engaging in deliberation, legislation, and elections under various stressors.", "result": "The combination of a Constitutional AI (CAI) charter and a mediated deliberation protocol serves as a potent alignment mechanism, reducing corrupt power-seeking behavior, improving policy stability, and enhancing citizen welfare. A novel metric, the Power-Preservation Index (PPI), quantifies misaligned behavior.", "conclusion": "Institutional design, combining Constitutional AI (CAI) and mediated deliberation, effectively aligns artificial agent societies, reducing corruption, improving policy stability, and enhancing citizen welfare."}}
{"id": "2508.19758", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.19758", "abs": "https://arxiv.org/abs/2508.19758", "authors": ["Yixuan Tang", "Yuanyuan Shi", "Yiqun Sun", "Anthony Kum Hoe Tung"], "title": "Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval", "comment": "Accepted by EMNLP 2025", "summary": "Access to diverse perspectives is essential for understanding real-world\nevents, yet most news retrieval systems prioritize textual relevance, leading\nto redundant results and limited viewpoint exposure. We propose NEWSCOPE, a\ntwo-stage framework for diverse news retrieval that enhances event coverage by\nexplicitly modeling semantic variation at the sentence level. The first stage\nretrieves topically relevant content using dense retrieval, while the second\nstage applies sentence-level clustering and diversity-aware re-ranking to\nsurface complementary information. To evaluate retrieval diversity, we\nintroduce three interpretable metrics, namely Average Pairwise Distance,\nPositive Cluster Coverage, and Information Density Ratio, and construct two\nparagraph-level benchmarks: LocalNews and DSGlobal. Experiments show that\nNEWSCOPE consistently outperforms strong baselines, achieving significantly\nhigher diversity without compromising relevance. Our results demonstrate the\neffectiveness of fine-grained, interpretable modeling in mitigating redundancy\nand promoting comprehensive event understanding. The data and code are\navailable at https://github.com/tangyixuan/NEWSCOPE.", "AI": {"tldr": "NEWSCOPE, a two-stage framework, enhances event coverage by modeling semantic variation at the sentence level, achieving higher diversity without compromising relevance.", "motivation": "Most news retrieval systems prioritize textual relevance, leading to redundant results and limited viewpoint exposure.", "method": "A two-stage framework with dense retrieval and sentence-level clustering with diversity-aware re-ranking.", "result": "NEWSCOPE consistently outperforms strong baselines on two paragraph-level benchmarks: LocalNews and DSGlobal.", "conclusion": "NEWSCOPE achieves higher diversity without compromising relevance by fine-grained, interpretable modeling."}}
{"id": "2508.19359", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19359", "abs": "https://arxiv.org/abs/2508.19359", "authors": ["Fatemeh Haji", "Mazal Bethany", "Cho-Yu Jason Chiang", "Anthony Rios", "Peyman Najafirad"], "title": "Reflective Agreement: Combining Self-Mixture of Agents with a Sequence Tagger for Robust Event Extraction", "comment": null, "summary": "Event Extraction (EE) involves automatically identifying and extracting\nstructured information about events from unstructured text, including triggers,\nevent types, and arguments. Traditional discriminative models demonstrate high\nprecision but often exhibit limited recall, particularly for nuanced or\ninfrequent events. Conversely, generative approaches leveraging Large Language\nModels (LLMs) provide higher semantic flexibility and recall but suffer from\nhallucinations and inconsistent predictions. To address these challenges, we\npropose Agreement-based Reflective Inference System (ARIS), a hybrid approach\ncombining a Self Mixture of Agents with a discriminative sequence tagger. ARIS\nexplicitly leverages structured model consensus, confidence-based filtering,\nand an LLM reflective inference module to reliably resolve ambiguities and\nenhance overall event prediction quality. We further investigate decomposed\ninstruction fine-tuning for enhanced LLM event extraction understanding.\nExperiments demonstrate our approach outperforms existing state-of-the-art\nevent extraction methods across three benchmark datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u4e8b\u4ef6\u63d0\u53d6\u65b9\u6cd5ARIS\uff0c\u5b83\u7ed3\u5408\u4e86\u5224\u522b\u6a21\u578b\u548c\u751f\u6210\u6a21\u578b\uff08LLM\uff09\u7684\u4f18\u70b9\uff0c\u5e76\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u7ed3\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u5224\u522b\u6a21\u578b\u7cbe\u5ea6\u9ad8\u4f46\u53ec\u56de\u7387\u6709\u9650\uff0c\u751f\u6210\u65b9\u6cd5\uff08LLM\uff09\u8bed\u4e49\u7075\u6d3b\u6027\u548c\u53ec\u56de\u7387\u8f83\u9ad8\u4f46\u5b58\u5728\u5e7b\u89c9\u548c\u9884\u6d4b\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u4e86\u81ea\u6df7\u5408\u4ee3\u7406\u548c\u5224\u522b\u5e8f\u5217\u6807\u6ce8\u5668\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u663e\u5f0f\u5730\u5229\u7528\u7ed3\u6784\u5316\u6a21\u578b\u5171\u8bc6\u3001\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u8fc7\u6ee4\u548cLLM\u53cd\u5c04\u63a8\u7406\u6a21\u5757\u3002", "result": "ARIS\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "ARIS\u65b9\u6cd5\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u4e8b\u4ef6\u63d0\u53d6\u65b9\u6cd5\u3002"}}
{"id": "2508.19305", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19305", "abs": "https://arxiv.org/abs/2508.19305", "authors": ["Chen Chu", "Cyrus Shahabi"], "title": "Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities", "comment": null, "summary": "Spatial representation learning is essential for GeoAI applications such as\nurban analytics, enabling the encoding of shapes, locations, and spatial\nrelationships (topological and distance-based) of geo-entities like points,\npolylines, and polygons. Existing methods either target a single geo-entity\ntype or, like Poly2Vec, decompose entities into simpler components to enable\nFourier transformation, introducing high computational cost. Moreover, since\nthe transformed space lacks geometric alignment, these methods rely on uniform,\nnon-adaptive sampling, which blurs fine-grained features like edges and\nboundaries. To address these limitations, we introduce Geo2Vec, a novel method\ninspired by signed distance fields (SDF) that operates directly in the original\nspace. Geo2Vec adaptively samples points and encodes their signed distances\n(positive outside, negative inside), capturing geometry without decomposition.\nA neural network trained to approximate the SDF produces compact,\ngeometry-aware, and unified representations for all geo-entity types.\nAdditionally, we propose a rotation-invariant positional encoding to model\nhigh-frequency spatial variations and construct a structured and robust\nembedding space for downstream GeoAI models. Empirical results show that\nGeo2Vec consistently outperforms existing methods in representing shape and\nlocation, capturing topological and distance relationships, and achieving\ngreater efficiency in real-world GeoAI applications. Code and Data can be found\nat: https://github.com/chuchen2017/GeoNeuralRepresentation.", "AI": {"tldr": "Geo2Vec \u662f\u4e00\u79cd\u65b0\u7684\u7a7a\u95f4\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u81ea\u9002\u5e94\u91c7\u6837\u70b9\u5e76\u7f16\u7801\u6709\u7b26\u53f7\u8ddd\u79bb\u6765\u76f4\u63a5\u5728\u539f\u59cb\u7a7a\u95f4\u4e2d\u8fd0\u884c\uff0c\u4ece\u800c\u4e3a\u6240\u6709\u5730\u7406\u5b9e\u4f53\u7c7b\u578b\u751f\u6210\u7d27\u51d1\u7684\u3001\u5177\u6709\u51e0\u4f55\u610f\u8bc6\u7684\u7edf\u4e00\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u7684\u7a7a\u95f4\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u8981\u4e48\u9488\u5bf9\u5355\u4e00\u5730\u7406\u5b9e\u4f53\u7c7b\u578b\uff0c\u8981\u4e48\u50cf Poly2Vec \u4e00\u6837\uff0c\u5c06\u5b9e\u4f53\u5206\u89e3\u4e3a\u66f4\u7b80\u5355\u7684\u7ec4\u4ef6\u4ee5\u5b9e\u73b0\u5085\u91cc\u53f6\u53d8\u6362\uff0c\u4ece\u800c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u53d8\u6362\u540e\u7684\u7a7a\u95f4\u7f3a\u4e4f\u51e0\u4f55\u5bf9\u9f50\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5747\u5300\u7684\u3001\u975e\u81ea\u9002\u5e94\u7684\u91c7\u6837\uff0c\u8fd9\u4f1a\u6a21\u7cca\u8fb9\u7f18\u548c\u8fb9\u754c\u7b49\u7ec6\u7c92\u5ea6\u7279\u5f81\u3002", "method": "Geo2Vec\uff0c\u4e00\u79cd\u53d7\u6709\u7b26\u53f7\u8ddd\u79bb\u573a (SDF) \u542f\u53d1\u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u76f4\u63a5\u5728\u539f\u59cb\u7a7a\u95f4\u4e2d\u8fd0\u884c\u3002Geo2Vec \u81ea\u9002\u5e94\u5730\u5bf9\u70b9\u8fdb\u884c\u91c7\u6837\u5e76\u7f16\u7801\u5b83\u4eec\u7684\u6709\u7b26\u53f7\u8ddd\u79bb\uff08\u5916\u90e8\u4e3a\u6b63\uff0c\u5185\u90e8\u4e3a\u8d1f\uff09\uff0c\u65e0\u9700\u5206\u89e3\u5373\u53ef\u6355\u83b7\u51e0\u4f55\u5f62\u72b6\u3002\u8bad\u7ec3\u7528\u4e8e\u903c\u8fd1 SDF \u7684\u795e\u7ecf\u7f51\u7edc\u4e3a\u6240\u6709\u5730\u7406\u5b9e\u4f53\u7c7b\u578b\u751f\u6210\u7d27\u51d1\u7684\u3001\u5177\u6709\u51e0\u4f55\u610f\u8bc6\u7684\u7edf\u4e00\u8868\u793a\u3002", "result": "\u7ecf\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGeo2Vec \u5728\u8868\u793a\u5f62\u72b6\u548c\u4f4d\u7f6e\u3001\u6355\u83b7\u62d3\u6251\u548c\u8ddd\u79bb\u5173\u7cfb\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9645 GeoAI \u5e94\u7528\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6548\u7387\u3002", "conclusion": "Geo2Vec\u5728\u8868\u793a\u5f62\u72b6\u548c\u4f4d\u7f6e\u3001\u6355\u83b7\u62d3\u6251\u548c\u8ddd\u79bb\u5173\u7cfb\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9645GeoAI\u5e94\u7528\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6548\u7387\u3002"}}
{"id": "2508.19356", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2508.19356", "abs": "https://arxiv.org/abs/2508.19356", "authors": ["Jos\u00e9 Manuel Barraza-Chavez", "Rana A. Barghout", "Ricardo Almada-Monter", "Benjamin Sanchez-Lengeling", "Adrian Jinich", "Radhakrishnan Mahadevan"], "title": "Graph Data Modeling: Molecules, Proteins, & Chemical Processes", "comment": "3 to 4 hours read time. 73 pages. 35 figures", "summary": "Graphs are central to the chemical sciences, providing a natural language to\ndescribe molecules, proteins, reactions, and industrial processes. They capture\ninteractions and structures that underpin materials, biology, and medicine.\nThis primer, Graph Data Modeling: Molecules, Proteins, & Chemical Processes,\nintroduces graphs as mathematical objects in chemistry and shows how learning\nalgorithms (particularly graph neural networks) can operate on them. We outline\nthe foundations of graph design, key prediction tasks, representative examples\nacross chemical sciences, and the role of machine learning in graph-based\nmodeling. Together, these concepts prepare readers to apply graph methods to\nthe next generation of chemical discovery.", "AI": {"tldr": "This paper introduces graphs as mathematical objects in chemistry and shows how learning algorithms can operate on them.", "motivation": "Graphs are central to the chemical sciences, providing a natural language to describe molecules, proteins, reactions, and industrial processes. They capture interactions and structures that underpin materials, biology, and medicine.", "method": "learning algorithms (particularly graph neural networks)", "result": "We outline the foundations of graph design, key prediction tasks, representative examples across chemical sciences, and the role of machine learning in graph-based modeling.", "conclusion": "Together, these concepts prepare readers to apply graph methods to the next generation of chemical discovery."}}
{"id": "2508.19569", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19569", "abs": "https://arxiv.org/abs/2508.19569", "authors": ["Hung Chau", "Run Yu", "Zachary Pardos", "Peter Brusilovsky"], "title": "Skill-based Explanations for Serendipitous Course Recommendation", "comment": null, "summary": "Academic choice is crucial in U.S. undergraduate education, allowing students\nsignificant freedom in course selection. However, navigating the complex\nacademic environment is challenging due to limited information, guidance, and\nan overwhelming number of choices, compounded by time restrictions and the high\ndemand for popular courses. Although career counselors exist, their numbers are\ninsufficient, and course recommendation systems, though personalized, often\nlack insight into student perceptions and explanations to assess course\nrelevance. In this paper, a deep learning-based concept extraction model is\ndeveloped to efficiently extract relevant concepts from course descriptions to\nimprove the recommendation process. Using this model, the study examines the\neffects of skill-based explanations within a serendipitous recommendation\nframework, tested through the AskOski system at the University of California,\nBerkeley. The findings indicate that these explanations not only increase user\ninterest, particularly in courses with high unexpectedness, but also bolster\ndecision-making confidence. This underscores the importance of integrating\nskill-related data and explanations into educational recommendation systems.", "AI": {"tldr": "This paper develops a deep learning model to extract concepts from course descriptions and finds that skill-based explanations in course recommendations increase user interest and confidence.", "motivation": "Navigating the complex academic environment is challenging due to limited information, guidance, and an overwhelming number of choices.", "method": "A deep learning-based concept extraction model is developed to efficiently extract relevant concepts from course descriptions.", "result": "Skill-based explanations increase user interest and bolster decision-making confidence, particularly in courses with high unexpectedness.", "conclusion": "Integrating skill-related data and explanations into educational recommendation systems is important."}}
{"id": "2508.19997", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.19997", "abs": "https://arxiv.org/abs/2508.19997", "authors": ["Boheng Mao"], "title": "Selective Retrieval-Augmentation for Long-Tail Legal Text Classification", "comment": null, "summary": "Legal text classification is a fundamental NLP task in the legal domain.\nBenchmark datasets in this area often exhibit a long-tail label distribution,\nwhere many labels are underrepresented, leading to poor model performance on\nrare classes. This paper proposes Selective Retrieval-Augmentation (SRA) as a\nsolution to this problem. SRA focuses on augmenting samples belonging to\nlow-frequency labels in the training set, preventing the introduction of noise\nfor well-represented classes, and requires no changes to the model\narchitecture. Retrieval is performed only from the training data to ensure\nthere is no potential information leakage, removing the need for external\ncorpora simultaneously. The proposed SRA method is tested on two legal text\nclassification benchmark datasets with long-tail distributions: LEDGAR\n(single-label) and UNFAIR-ToS (multi-label). The results indicate that SRA\nattains higher micro-F1 and macro-F1 scores compared to all current LexGLUE\nbaselines across both datasets, illustrating consistent improvements in\nlong-tail legal text classification. The code repository is available at:\nhttps://github.com/Boheng-Mao/sra-legal", "AI": {"tldr": "This paper proposes Selective Retrieval-Augmentation (SRA) to improve the performance of legal text classification models on rare classes in long-tail datasets. SRA augments samples of low-frequency labels and outperforms existing baselines.", "motivation": "Benchmark datasets in this area often exhibit a long-tail label distribution, where many labels are underrepresented, leading to poor model performance on rare classes.", "method": "Selective Retrieval-Augmentation (SRA): focuses on augmenting samples belonging to low-frequency labels in the training set, preventing the introduction of noise for well-represented classes, and requires no changes to the model architecture. Retrieval is performed only from the training data.", "result": "SRA attains higher micro-F1 and macro-F1 scores compared to all current LexGLUE baselines across both datasets.", "conclusion": "SRA attains higher micro-F1 and macro-F1 scores compared to all current LexGLUE baselines across both datasets, illustrating consistent improvements in long-tail legal text classification."}}
{"id": "2508.19363", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19363", "abs": "https://arxiv.org/abs/2508.19363", "authors": ["Jiayu Ding", "Shuming Ma", "Lei Cui", "Nanning Zheng", "Furu Wei"], "title": "LongReasonArena: A Long Reasoning Benchmark for Large Language Models", "comment": null, "summary": "Existing long-context benchmarks for Large Language Models (LLMs) focus on\nevaluating comprehension of long inputs, while overlooking the evaluation of\nlong reasoning abilities. To address this gap, we introduce LongReasonArena, a\nbenchmark specifically designed to assess the long reasoning capabilities of\nLLMs. Our tasks require models to solve problems by executing multi-step\nalgorithms that reflect key aspects of long reasoning, such as retrieval and\nbacktracking. By controlling the inputs, the required reasoning length can be\narbitrarily scaled, reaching up to 1 million tokens of reasoning for the most\nchallenging tasks. Extensive evaluation results demonstrate that\nLongReasonArena presents a significant challenge for both open-source and\nproprietary LLMs. For instance, Deepseek-R1 achieves only 7.5% accuracy on our\ntask. Further analysis also reveals that the accuracy exhibits a linear decline\nwith respect to the logarithm of the expected number of reasoning steps. Our\ncode and data is available at\nhttps://github.com/LongReasonArena/LongReasonArena.", "AI": {"tldr": "LongReasonArena is introduced to assess the long reasoning capabilities of LLMs, which requires models to solve problems by executing multi-step algorithms. It is a significant challenge for both open-source and proprietary LLMs.", "motivation": "Existing long-context benchmarks for Large Language Models (LLMs) focus on evaluating comprehension of long inputs, while overlooking the evaluation of long reasoning abilities. To address this gap, we introduce LongReasonArena, a benchmark specifically designed to assess the long reasoning capabilities of LLMs.", "method": "Tasks require models to solve problems by executing multi-step algorithms that reflect key aspects of long reasoning, such as retrieval and backtracking. By controlling the inputs, the required reasoning length can be arbitrarily scaled, reaching up to 1 million tokens of reasoning for the most challenging tasks.", "result": "Deepseek-R1 achieves only 7.5% accuracy on our task.", "conclusion": "LongReasonArena presents a significant challenge for both open-source and proprietary LLMs. The accuracy exhibits a linear decline with respect to the logarithm of the expected number of reasoning steps."}}
{"id": "2508.19307", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19307", "abs": "https://arxiv.org/abs/2508.19307", "authors": ["Hamza Khan"], "title": "Advancements in Crop Analysis through Deep Learning and Explainable AI", "comment": "Master's thesis", "summary": "Rice is a staple food of global importance in terms of trade, nutrition, and\neconomic growth. Among Asian nations such as China, India, Pakistan, Thailand,\nVietnam and Indonesia are leading producers of both long and short grain\nvarieties, including basmati, jasmine, arborio, ipsala, and kainat saila. To\nensure consumer satisfaction and strengthen national reputations, monitoring\nrice crops and grain quality is essential. Manual inspection, however, is\nlabour intensive, time consuming and error prone, highlighting the need for\nautomated solutions for quality control and yield improvement. This study\nproposes an automated approach to classify five rice grain varieties using\nConvolutional Neural Networks (CNN). A publicly available dataset of 75000\nimages was used for training and testing. Model evaluation employed accuracy,\nrecall, precision, F1-score, ROC curves, and confusion matrices. Results\ndemonstrated high classification accuracy with minimal misclassifications,\nconfirming the model effectiveness in distinguishing rice varieties. In\naddition, an accurate diagnostic method for rice leaf diseases such as Brown\nSpot, Blast, Bacterial Blight, and Tungro was developed. The framework combined\nexplainable artificial intelligence (XAI) with deep learning models including\nCNN, VGG16, ResNet50, and MobileNetV2. Explainability techniques such as SHAP\n(SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic\nExplanations) revealed how specific grain and leaf features influenced\npredictions, enhancing model transparency and reliability. The findings\ndemonstrate the strong potential of deep learning in agricultural applications,\npaving the way for robust, interpretable systems that can support automated\ncrop quality inspection and disease diagnosis, ultimately benefiting farmers,\nconsumers, and the agricultural economy.", "AI": {"tldr": "This study uses deep learning and explainable AI to automate rice grain classification and disease diagnosis, improving efficiency and transparency in agricultural practices.", "motivation": "Manual inspection of rice crops is labour intensive, time consuming, and error prone, necessitating automated solutions for quality control and yield improvement.", "method": "Convolutional Neural Networks (CNN) were used to classify five rice grain varieties. Additionally, CNN, VGG16, ResNet50, and MobileNetV2 models were combined with explainable AI techniques (SHAP and LIME) for rice leaf disease diagnosis.", "result": "High classification accuracy was achieved in distinguishing rice varieties, with minimal misclassifications. An accurate diagnostic method for rice leaf diseases was also developed.", "conclusion": "Deep learning models, combined with explainable AI, show strong potential for automated crop quality inspection and disease diagnosis, benefiting farmers, consumers, and the agricultural economy."}}
{"id": "2508.19361", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19361", "abs": "https://arxiv.org/abs/2508.19361", "authors": ["Yongbin Lee", "Ki H. Chon"], "title": "Atrial Fibrillation Prediction Using a Lightweight Temporal Convolutional and Selective State Space Architecture", "comment": "4 pages, 2 figures, 4 table, IEEE-EMBS International Conference on\n  Body Sensor Networks (IEEE-EMBS BSN 2025)", "summary": "Atrial fibrillation (AF) is the most common arrhythmia, increasing the risk\nof stroke, heart failure, and other cardiovascular complications. While AF\ndetection algorithms perform well in identifying persistent AF, early-stage\nprogression, such as paroxysmal AF (PAF), often goes undetected due to its\nsudden onset and short duration. However, undetected PAF can progress into\nsustained AF, increasing the risk of mortality and severe complications. Early\nprediction of AF offers an opportunity to reduce disease progression through\npreventive therapies, such as catecholamine-sparing agents or beta-blockers. In\nthis study, we propose a lightweight deep learning model using only RR\nIntervals (RRIs), combining a Temporal Convolutional Network (TCN) for\npositional encoding with Mamba, a selective state space model, to enable early\nprediction of AF through efficient parallel sequence modeling. In subject-wise\ntesting results, our model achieved a sensitivity of 0.908, specificity of\n0.933, F1-score of 0.930, AUROC of 0.972, and AUPRC of 0.932. Additionally, our\nmethod demonstrates high computational efficiency, with only 73.5 thousand\nparameters and 38.3 MFLOPs, outperforming traditional Convolutional Neural\nNetwork-Recurrent Neural Network (CNN-RNN) approaches in both accuracy and\nmodel compactness. Notably, the model can predict AF up to two hours in advance\nusing just 30 minutes of input data, providing enough lead time for preventive\ninterventions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u65e9\u671f\u9884\u6d4b\u623f\u98a4\uff0c\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u80fd\u591f\u63d0\u524d\u4e24\u5c0f\u65f6\u8fdb\u884c\u9884\u6d4b\u3002", "motivation": "\u672a\u88ab\u53d1\u73b0\u7684\u9635\u53d1\u6027\u623f\u98a4 (PAF) \u4f1a\u53d1\u5c55\u4e3a\u6301\u7eed\u6027\u623f\u98a4\uff0c\u589e\u52a0\u6b7b\u4ea1\u7387\u548c\u4e25\u91cd\u5e76\u53d1\u75c7\u7684\u98ce\u9669\u3002\u65e9\u671f\u9884\u6d4b\u623f\u98a4\u6709\u673a\u4f1a\u901a\u8fc7\u9884\u9632\u6027\u6cbb\u7597\u51cf\u5c11\u75be\u75c5\u8fdb\u5c55\u3002", "method": "\u7ed3\u5408\u4e86\u65f6\u95f4\u5377\u79ef\u7f51\u7edc (TCN) \u548c Mamba\uff08\u4e00\u79cd\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff09\u7684\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ec5\u4f7f\u7528 RR \u95f4\u671f (RRI)\u8fdb\u884c\u9ad8\u6548\u7684\u5e76\u884c\u5e8f\u5217\u5efa\u6a21\u3002", "result": "\u8be5\u6a21\u578b\u5728\u53d7\u8bd5\u8005\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u7075\u654f\u5ea6\u4e3a 0.908\uff0c\u7279\u5f02\u6027\u4e3a 0.933\uff0cF1 \u5206\u6570\u4e3a 0.930\uff0cAUROC \u4e3a 0.972\uff0cAUPRC \u4e3a 0.932\u3002\u8be5\u6a21\u578b\u4ec5\u6709 73.5 \u5343\u4e2a\u53c2\u6570\u548c 38.3 MFLOPs\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u63d0\u524d\u4e24\u5c0f\u65f6\u9884\u6d4b\u623f\u98a4\uff0c\u5e76\u4e3a\u9884\u9632\u6027\u5e72\u9884\u63d0\u4f9b\u8db3\u591f\u7684\u63d0\u524d\u91cf\u3002"}}
{"id": "2508.19576", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19576", "abs": "https://arxiv.org/abs/2508.19576", "authors": ["Sining Zhoubian", "Dan Zhang", "Yuxiao Dong", "Jie Tang"], "title": "ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding", "comment": "20 pages, 4 figures", "summary": "With respect to improving the reasoning accuracy of LLMs, the representative\nreinforcement learning (RL) method GRPO faces failure due to insignificant\nreward variance, while verification methods based on process reward models\n(PRMs) suffer from difficulties with training data acquisition and verification\neffectiveness. To tackle these problems, this paper introduces ReST-RL, a\nunified LLM RL paradigm that significantly improves LLM's code reasoning\nability by combining an improved GRPO algorithm with a meticulously designed\ntest time decoding method assisted by a value model (VM). As the first stage of\npolicy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter\nand assemble high-value training data, increasing the reward variance of GRPO\nsampling, thus improving the effectiveness and efficiency of training. After\nthe basic reasoning ability of LLM policy has been improved, we further propose\na test time decoding optimization method called VM-MCTS. Through Monte-Carlo\nTree Search (MCTS), we collect accurate value targets with no annotation\nrequired, on which VM training is based. When decoding, the VM is deployed by\nan adapted MCTS algorithm to provide precise process signals as well as\nverification scores, assisting the LLM policy to achieve high reasoning\naccuracy. We validate the effectiveness of the proposed RL paradigm through\nextensive experiments on coding problems. Upon comparison, our approach\nsignificantly outperforms other reinforcement training baselines (e.g., naive\nGRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,\nPRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,\nAPPS, BigCodeBench, and HumanEval), indicating its power to strengthen the\nreasoning ability of LLM policies. Codes for our project can be found at\nhttps://github.com/THUDM/ReST-RL.", "AI": {"tldr": "ReST-RL, a new LLM RL method, improves code reasoning by combining an improved GRPO algorithm with a value model assisted MCTS, achieving state-of-the-art results on coding benchmarks.", "motivation": "Existing reinforcement learning (RL) methods like GRPO face failure due to insignificant reward variance, while verification methods based on process reward models (PRMs) suffer from training data acquisition and verification effectiveness issues.", "method": "The paper combines an improved GRPO algorithm with a value model (VM) assisted test time decoding method. It includes ReST-GRPO for policy reinforcement and VM-MCTS for test time decoding optimization.", "result": "ReST-RL significantly outperforms other reinforcement training baselines (e.g., naive GRPO and ReST-DPO), as well as decoding and verification baselines (e.g., PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g., APPS, BigCodeBench, and HumanEval).", "conclusion": "The paper's ReST-RL paradigm significantly improves LLM's code reasoning ability, outperforming other reinforcement training, decoding, and verification baselines on coding benchmarks."}}
{"id": "2508.20013", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.20013", "abs": "https://arxiv.org/abs/2508.20013", "authors": ["Lotte Gross", "Rebecca Walter", "Nicole Zoppi", "Adrien Justus", "Alessandro Gambetti", "Qiwei Han", "Maximilian Kaiser"], "title": "Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach", "comment": "10 pages, 5 figures, 3 tables", "summary": "This study addresses critical industrial challenges in e-commerce product\ncategorization, namely platform heterogeneity and the structural limitations of\nexisting taxonomies, by developing and deploying a multimodal hierarchical\nclassification framework. Using a dataset of 271,700 products from 40\ninternational fashion e-commerce platforms, we integrate textual features\n(RoBERTa), visual features (ViT), and joint vision--language representations\n(CLIP). We investigate fusion strategies, including early, late, and\nattention-based fusion within a hierarchical architecture enhanced by dynamic\nmasking to ensure taxonomic consistency. Results show that CLIP embeddings\ncombined via an MLP-based late-fusion strategy achieve the highest hierarchical\nF1 (98.59\\%), outperforming unimodal baselines. To address shallow or\ninconsistent categories, we further introduce a self-supervised ``product\nrecategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which\ndiscovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with\ncluster purities above 86\\%. Cross-platform experiments reveal a\ndeployment-relevant trade-off: complex late-fusion methods maximize accuracy\nwith diverse training data, while simpler early-fusion methods generalize more\neffectively to unseen platforms. Finally, we demonstrate the framework's\nindustrial scalability through deployment in EURWEB's commercial transaction\nintelligence platform via a two-stage inference pipeline, combining a\nlightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance\ncost and accuracy.", "AI": {"tldr": "Developed and deployed a multimodal hierarchical classification framework for e-commerce product categorization, achieving high accuracy and scalability.", "motivation": "Addresses critical industrial challenges in e-commerce product categorization, namely platform heterogeneity and the structural limitations of existing taxonomies.", "method": "A multimodal hierarchical classification framework integrating textual features (RoBERTa), visual features (ViT), and joint vision--language representations (CLIP) with fusion strategies.", "result": "CLIP embeddings combined via an MLP-based late-fusion strategy achieve the highest hierarchical F1 (98.59%), outperforming unimodal baselines. A self-supervised product recategorization pipeline using SimCLR, UMAP, and cascade clustering discovered new, fine-grained categories with cluster purities above 86%.", "conclusion": "The framework's industrial scalability is demonstrated through deployment in EURWEB's commercial transaction intelligence platform via a two-stage inference pipeline."}}
{"id": "2508.19312", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19312", "abs": "https://arxiv.org/abs/2508.19312", "authors": ["Ander Galv\u00e1n", "Marivi Higuero", "Jorge Sasiain", "Eduardo Jacob"], "title": "Sistema de Reconocimiento Facial Federado en Conjuntos Abiertos basado en OpenMax", "comment": "Aceptado para publicaci\\'on, in Spanish language. XVII Jornadas de\n  Ingenier\\'ia Telem\\'atica (JITEL 2025)", "summary": "Facial recognition powered by Artificial Intelligence has achieved high\naccuracy in specific scenarios and applications. Nevertheless, it faces\nsignificant challenges regarding privacy and identity management, particularly\nwhen unknown individuals appear in the operational context. This paper presents\nthe design, implementation, and evaluation of a facial recognition system\nwithin a federated learning framework tailored to open-set scenarios. The\nproposed approach integrates the OpenMax algorithm into federated learning,\nleveraging the exchange of mean activation vectors and local distance measures\nto reliably distinguish between known and unknown subjects. Experimental\nresults validate the effectiveness of the proposed solution, demonstrating its\npotential for enhancing privacy-aware and robust facial recognition in\ndistributed environments.\n  --\n  El reconocimiento facial impulsado por Inteligencia Artificial ha demostrado\nuna alta precisi\\'on en algunos escenarios y aplicaciones. Sin embargo,\npresenta desaf\\'ios relacionados con la privacidad y la identificaci\\'on de\npersonas, especialmente considerando que pueden aparecer sujetos desconocidos\npara el sistema que lo implementa. En este trabajo, se propone el dise\\~no,\nimplementaci\\'on y evaluaci\\'on de un sistema de reconocimiento facial en un\nescenario de aprendizaje federado, orientado a conjuntos abiertos.\nConcretamente, se dise\\~na una soluci\\'on basada en el algoritmo OpenMax para\nescenarios de aprendizaje federado. La propuesta emplea el intercambio de los\nvectores de activaci\\'on promedio y distancias locales para identificar de\nmanera eficaz tanto personas conocidas como desconocidas. Los experimentos\nrealizados demuestran la implementaci\\'on efectiva de la soluci\\'on propuesta.", "AI": {"tldr": "This paper presents a federated learning facial recognition system using OpenMax to distinguish between known and unknown subjects, enhancing privacy and robustness.", "motivation": "Facial recognition faces significant challenges regarding privacy and identity management, particularly when unknown individuals appear in the operational context.", "method": "The proposed approach integrates the OpenMax algorithm into federated learning, leveraging the exchange of mean activation vectors and local distance measures to reliably distinguish between known and unknown subjects.", "result": "The system can reliably distinguish between known and unknown subjects.", "conclusion": "The experimental results validate the effectiveness of the proposed solution, demonstrating its potential for enhancing privacy-aware and robust facial recognition in distributed environments."}}
{"id": "2508.19366", "categories": ["cs.LG", "cs.AI", "53B21, 46E22 (Primary), 68R10 (Secondary)"], "pdf": "https://arxiv.org/pdf/2508.19366", "abs": "https://arxiv.org/abs/2508.19366", "authors": ["Supratik Sarkar", "Swagatam Das"], "title": "Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying Hallucinations in multimodal LLMs", "comment": "29 pages, 3 figures, 1 table", "summary": "Hallucinations in large language models (LLMs) remain a fundamental obstacle\nto trustworthy AI, particularly in high-stakes multimodal domains such as\nmedicine, law, and finance. Existing evaluation techniques are largely\nheuristic -- anchored in qualitative benchmarking or ad-hoc empirical\nmitigation -- providing neither principled quantification nor actionable\ntheoretical guarantees. This gap leaves a critical blind spot in understanding\nhow hallucinations arise, propagate, and interact across modalities. We\nintroduce the first (to our knowledge) rigorous information geometric framework\nin diffusion dynamics for quantifying hallucinations in multimodal LLMs\n(MLLMs), advancing the field from qualitative detection to mathematically\ngrounded measurement. Our approach represents MLLM outputs as the spectral\nembeddings over multimodal graph Laplacians and characterizes the manifold gaps\nof truth vs inconsistencies as the semantic distortion, enabling the tight\nRayleigh--Ritz bounds on the multimodal hallucination energy as a functional of\ntime-dependent temperature profiles. By leveraging eigenmode decompositions in\nReproducing Kernel Hilbert Space (RKHS) embeddings, our framework delivers\nmodality-aware, theoretically interpretable metrics that capture the evolution\nof hallucinations across time and input prompts through temperature annealing.\nThis work establishes a principled foundation for quantifying and bounding\nhallucinations, transforming them from a qualitative risk to a tractable,\nanalyzable phenomenon.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u91cf\u5316\u591a\u6a21\u6001LLM\u5e7b\u89c9\u7684\u4fe1\u606f\u51e0\u4f55\u6846\u67b6\uff0c\u4e3a\u7406\u89e3\u548c\u63a7\u5236\u5e7b\u89c9\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u5e7b\u89c9\u4ecd\u7136\u662f\u53ef\u4fe1AI\u7684\u4e00\u4e2a\u6839\u672c\u969c\u788d\uff0c\u7279\u522b\u662f\u5728\u533b\u5b66\u3001\u6cd5\u5f8b\u548c\u91d1\u878d\u7b49\u9ad8\u98ce\u9669\u591a\u6a21\u6001\u9886\u57df\u3002\u73b0\u6709\u7684\u8bc4\u4f30\u6280\u672f\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u662f\u542f\u53d1\u5f0f\u7684\uff0c\u7f3a\u4e4f\u6709\u539f\u5219\u7684\u91cf\u5316\u6216\u53ef\u64cd\u4f5c\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u901a\u8fc7\u5c06MLLM\u8f93\u51fa\u8868\u793a\u4e3a\u591a\u6a21\u6001\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u7684\u8c31\u5d4c\u5165\uff0c\u5e76\u5229\u7528RKHS\u5d4c\u5165\u4e2d\u7684\u7279\u5f81\u6a21\u5206\u89e3\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9\u591a\u6a21\u6001\u5e7b\u89c9\u80fd\u91cf\u7684\u4e25\u683cRayleigh--Ritz\u754c\u9650\uff0c\u5e76\u63d0\u4f9b\u4e86\u6a21\u6001\u611f\u77e5\u7684\u3001\u7406\u8bba\u4e0a\u53ef\u89e3\u91ca\u7684\u6307\u6807\uff0c\u8fd9\u4e9b\u6307\u6807\u53ef\u4ee5\u6355\u83b7\u5e7b\u89c9\u968f\u65f6\u95f4\u548c\u8f93\u5165\u63d0\u793a\u901a\u8fc7\u6e29\u5ea6\u9000\u706b\u7684\u6f14\u53d8\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cf\u5316\u591a\u6a21\u6001LLM\u4e2d\u5e7b\u89c9\u7684\u4e25\u683c\u4fe1\u606f\u51e0\u4f55\u6846\u67b6\uff0c\u5c06\u5e7b\u89c9\u4ece\u5b9a\u6027\u98ce\u9669\u8f6c\u53d8\u4e3a\u53ef\u5904\u7406\u3001\u53ef\u5206\u6790\u7684\u73b0\u8c61\u3002"}}
{"id": "2508.19611", "categories": ["cs.AI", "cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.19611", "abs": "https://arxiv.org/abs/2508.19611", "authors": ["Huaiyuan Yao", "Wanpeng Xu", "Justin Turnau", "Nadia Kellam", "Hua Wei"], "title": "Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties", "comment": "18 pages, 9 figures", "summary": "Preparing high-quality instructional materials remains a labor-intensive\nprocess that often requires extensive coordination among teaching faculty,\ninstructional designers, and teaching assistants. In this work, we present\nInstructional Agents, a multi-agent large language model (LLM) framework\ndesigned to automate end-to-end course material generation, including syllabus\ncreation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing\nAI-assisted educational tools that focus on isolated tasks, Instructional\nAgents simulates role-based collaboration among educational agents to produce\ncohesive and pedagogically aligned content. The system operates in four modes:\nAutonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling\nflexible control over the degree of human involvement. We evaluate\nInstructional Agents across five university-level computer science courses and\nshow that it produces high-quality instructional materials while significantly\nreducing development time and human workload. By supporting institutions with\nlimited instructional design capacity, Instructional Agents provides a scalable\nand cost-effective framework to democratize access to high-quality education,\nparticularly in underserved or resource-constrained settings.", "AI": {"tldr": "Instructional Agents is a multi-agent LLM framework designed to automate end-to-end course material generation, including syllabus creation, lecture scripts, LaTeX-based slides, and assessments.", "motivation": "Preparing high-quality instructional materials remains a labor-intensive process that often requires extensive coordination.", "method": "Instructional Agents simulates role-based collaboration among educational agents to produce cohesive and pedagogically aligned content. The system operates in four modes: Autonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling flexible control over the degree of human involvement.", "result": "Instructional Agents produces high-quality instructional materials while significantly reducing development time and human workload.", "conclusion": "Instructional Agents produces high-quality instructional materials while significantly reducing development time and human workload. It provides a scalable and cost-effective framework to democratize access to high-quality education, particularly in underserved or resource-constrained settings."}}
{"id": "2508.19402", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19402", "abs": "https://arxiv.org/abs/2508.19402", "authors": ["Mor Turgeman", "Chen Shani", "Dafna Shahaf"], "title": "One Joke to Rule them All? On the (Im)possibility of Generalizing Humor", "comment": null, "summary": "Humor is a broad and complex form of communication that remains challenging\nfor machines. Despite its broadness, most existing research on computational\nhumor traditionally focused on modeling a specific type of humor. In this work,\nwe wish to understand whether competence on one or more specific humor tasks\nconfers any ability to transfer to novel, unseen types; in other words, is this\nfragmentation inevitable? This question is especially timely as new humor types\ncontinuously emerge in online and social media contexts (e.g., memes,\nanti-humor, AI fails). If Large Language Models (LLMs) are to keep up with this\nevolving landscape, they must be able to generalize across humor types by\ncapturing deeper, transferable mechanisms. To investigate this, we conduct a\nseries of transfer learning experiments across four datasets, representing\ndifferent humor tasks. We train LLMs under varied diversity settings (1-3\ndatasets in training, testing on a novel task). Experiments reveal that models\nare capable of some transfer, and can reach up to 75% accuracy on unseen\ndatasets; training on diverse sources improves transferability (1.88-4.05%)\nwith minimal-to-no drop in in-domain performance. Further analysis suggests\nrelations between humor types, with Dad Jokes surprisingly emerging as the best\nenabler of transfer (but is difficult to transfer to). We release data and\ncode.", "AI": {"tldr": "LLMs can generalize humor understanding across different types, with Dad Jokes being surprisingly helpful for transfer learning.", "motivation": "To understand if competence on specific humor tasks can transfer to novel, unseen types, especially as new humor types emerge online. To determine if fragmentation in humor understanding is inevitable for Large Language Models (LLMs).", "method": "Transfer learning experiments across four datasets representing different humor tasks. LLMs trained under varied diversity settings (1-3 datasets in training, testing on a novel task).", "result": "Models are capable of some transfer, reaching up to 75% accuracy on unseen datasets. Training on diverse sources improves transferability (1.88-4.05%) with minimal-to-no drop in in-domain performance. Dad Jokes are the best enabler of transfer but are difficult to transfer to.", "conclusion": "Models can transfer humor understanding to new types, achieving up to 75% accuracy on unseen datasets. Training on diverse sources improves transferability. Dad Jokes are surprisingly effective for enabling transfer but hard to transfer to."}}
{"id": "2508.19314", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19314", "abs": "https://arxiv.org/abs/2508.19314", "authors": ["Mahdis Tourian", "Sareh Rowlands", "Remy Vandaele", "Max Fancourt", "Rebecca Mein", "Hywel T. P. Williams"], "title": "Automated classification of natural habitats using ground-level imagery", "comment": "15 pages, 6 figures, 2 tables", "summary": "Accurate classification of terrestrial habitats is critical for biodiversity\nconservation, ecological monitoring, and land-use planning. Several habitat\nclassification schemes are in use, typically based on analysis of satellite\nimagery with validation by field ecologists. Here we present a methodology for\nclassification of habitats based solely on ground-level imagery (photographs),\noffering improved validation and the ability to classify habitats at scale (for\nexample using citizen-science imagery). In collaboration with Natural England,\na public sector organisation responsible for nature conservation in England,\nthis study develops a classification system that applies deep learning to\nground-level habitat photographs, categorising each image into one of 18\nclasses defined by the 'Living England' framework. Images were pre-processed\nusing resizing, normalisation, and augmentation; re-sampling was used to\nbalance classes in the training data and enhance model robustness. We developed\nand fine-tuned a DeepLabV3-ResNet101 classifier to assign a habitat class label\nto each photograph. Using five-fold cross-validation, the model demonstrated\nstrong overall performance across 18 habitat classes, with accuracy and\nF1-scores varying between classes. Across all folds, the model achieved a mean\nF1-score of 0.61, with visually distinct habitats such as Bare Soil, Silt and\nPeat (BSSP) and Bare Sand (BS) reaching values above 0.90, and mixed or\nambiguous classes scoring lower. These findings demonstrate the potential of\nthis approach for ecological monitoring. Ground-level imagery is readily\nobtained, and accurate computational methods for habitat classification based\non such data have many potential applications. To support use by practitioners,\nwe also provide a simple web application that classifies uploaded images using\nour model.", "AI": {"tldr": "This paper presents a deep learning method for classifying habitats from ground-level photos, achieving promising results and offering a web application for practical use.", "motivation": "Accurate classification of terrestrial habitats is critical for various applications, and this paper aims to improve validation and scalability by using ground-level imagery instead of satellite imagery.", "method": "A DeepLabV3-ResNet101 classifier was developed and fine-tuned to classify ground-level habitat photographs into 18 classes, using pre-processing techniques like resizing, normalization, and augmentation, and employing five-fold cross-validation.", "result": "The model achieved a mean F1-score of 0.61 across 18 habitat classes, with some visually distinct habitats reaching values above 0.90. A web application was also provided to classify uploaded images.", "conclusion": "This study demonstrates the potential of using ground-level imagery and deep learning for ecological monitoring and habitat classification, achieving a mean F1-score of 0.61 across 18 habitat classes."}}
{"id": "2508.19376", "categories": ["cs.LG", "cs.AI", "cs.CV", "hep-ex"], "pdf": "https://arxiv.org/pdf/2508.19376", "abs": "https://arxiv.org/abs/2508.19376", "authors": ["Dikshant Sagar", "Kaiwen Yu", "Alejandro Yankelevich", "Jianming Bian", "Pierre Baldi"], "title": "Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments", "comment": null, "summary": "Recent progress in large language models (LLMs) has shown strong potential\nfor multimodal reasoning beyond natural language. In this work, we explore the\nuse of a fine-tuned Vision-Language Model (VLM), based on LLaMA 3.2, for\nclassifying neutrino interactions from pixelated detector images in high-energy\nphysics (HEP) experiments. We benchmark its performance against an established\nCNN baseline used in experiments like NOvA and DUNE, evaluating metrics such as\nclassification accuracy, precision, recall, and AUC-ROC. Our results show that\nthe VLM not only matches or exceeds CNN performance but also enables richer\nreasoning and better integration of auxiliary textual or semantic context.\nThese findings suggest that VLMs offer a promising general-purpose backbone for\nevent classification in HEP, paving the way for multimodal approaches in\nexperimental neutrino physics.", "AI": {"tldr": "\u4f7f\u7528VLM\u8fdb\u884c\u4e2d\u5fae\u5b50\u76f8\u4e92\u4f5c\u7528\u5206\u7c7b\uff0c\u7ed3\u679c\u8868\u660eVLM\u53ef\u4ee5\u4e0eCNN\u76f8\u5ab2\u7f8e\uff0c\u5e76\u4e14\u80fd\u591f\u5b9e\u73b0\u66f4\u4e30\u5bcc\u7684\u63a8\u7406\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6700\u65b0\u8fdb\u5c55\u8868\u660e\uff0c\u5b83\u5728\u81ea\u7136\u8bed\u8a00\u4e4b\u5916\u7684\u591a\u6a21\u6001\u63a8\u7406\u65b9\u9762\u5177\u6709\u5f3a\u5927\u7684\u6f5c\u529b\u3002", "method": "\u57fa\u4e8eLLaMA 3.2\uff0c\u4f7f\u7528\u5fae\u8c03\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09", "result": "VLM\u4e0d\u4ec5\u5339\u914d\u6216\u8d85\u8fc7\u4e86CNN\u7684\u6027\u80fd\uff0c\u800c\u4e14\u80fd\u591f\u5b9e\u73b0\u66f4\u4e30\u5bcc\u7684\u63a8\u7406\u548c\u66f4\u597d\u5730\u96c6\u6210\u8f85\u52a9\u6587\u672c\u6216\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002", "conclusion": "VLMs\u5728HEP\u4e8b\u4ef6\u5206\u7c7b\u4e2d\u5f88\u6709\u524d\u666f\uff0c\u4e3a\u5b9e\u9a8c\u4e2d\u5fae\u5b50\u7269\u7406\u5b66\u4e2d\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.19679", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19679", "abs": "https://arxiv.org/abs/2508.19679", "authors": ["Qihang Ai", "Pi Bu", "Yue Cao", "Yingyao Wang", "Jihao Gu", "Jingxuan Xing", "Zekun Zhu", "Wei Jiang", "Zhicheng Zheng", "Jun Song", "Yuning Jiang", "Bo Zheng"], "title": "InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning", "comment": null, "summary": "Recent advances in Vision-Language Models (VLMs) have enabled mobile agents\nto perceive and interact with real-world mobile environments based on human\ninstructions. However, the current fully autonomous paradigm poses potential\nsafety risks when model understanding or reasoning capabilities are\ninsufficient. To address this challenge, we first introduce\n\\textbf{InquireBench}, a comprehensive benchmark specifically designed to\nevaluate mobile agents' capabilities in safe interaction and proactive inquiry\nwith users, encompassing 5 categories and 22 sub-categories, where most\nexisting VLM-based agents demonstrate near-zero performance. In this paper, we\naim to develop an interactive system that actively seeks human confirmation at\ncritical decision points. To achieve this, we propose \\textbf{InquireMobile}, a\nnovel model inspired by reinforcement learning, featuring a two-stage training\nstrategy and an interactive pre-action reasoning mechanism. Finally, our model\nachieves an 46.8% improvement in inquiry success rate and the best overall\nsuccess rate among existing baselines on InquireBench. We will open-source all\ndatasets, models, and evaluation codes to facilitate development in both\nacademia and industry.", "AI": {"tldr": "\u63d0\u51fa\u4e86InquireMobile\uff0c\u4e00\u79cd\u7528\u4e8e\u5b89\u5168\u79fb\u52a8\u4ee3\u7406\u4ea4\u4e92\u7684\u65b0\u6a21\u578b\uff0c\u5e76\u5728InquireBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u5f53\u524d\u7684\u5b8c\u5168\u81ea\u4e3b\u8303\u4f8b\u5728\u6a21\u578b\u7406\u89e3\u6216\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u65f6\u5b58\u5728\u6f5c\u5728\u7684\u5b89\u5168\u98ce\u9669\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u6311\u6218\uff0c\u6211\u4eec\u9996\u5148\u63a8\u51fa\u4e86InquireBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u7efc\u5408\u57fa\u51c6\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u79fb\u52a8\u4ee3\u7406\u5728\u5b89\u5168\u4ea4\u4e92\u548c\u4e0e\u7528\u6237\u8fdb\u884c\u4e3b\u52a8\u67e5\u8be2\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u5f3a\u5316\u5b66\u4e60\u542f\u53d1\u7684\u65b0\u6a21\u578bInquireMobile\uff0c\u8be5\u6a21\u578b\u5177\u6709\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u548c\u4ea4\u4e92\u5f0f\u9884\u52a8\u4f5c\u63a8\u7406\u673a\u5236\u3002", "result": "\u8be5\u6a21\u578b\u5728\u67e5\u8be2\u6210\u529f\u7387\u4e0a\u63d0\u9ad8\u4e8646.8%\uff0c\u5e76\u5728InquireBench\u4e0a\u7684\u73b0\u6709\u57fa\u7ebf\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u603b\u4f53\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728InquireBench\u4e0a\u5b9e\u73b0\u4e8646.8%\u7684\u67e5\u8be2\u6210\u529f\u7387\u63d0\u5347\uff0c\u5e76\u5728\u73b0\u6709\u57fa\u7ebf\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u603b\u4f53\u6210\u529f\u7387\u3002"}}
{"id": "2508.19427", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.19427", "abs": "https://arxiv.org/abs/2508.19427", "authors": ["Evandro L. T. P. Cunha"], "title": "A perishable ability? The future of writing in the face of generative artificial intelligence", "comment": "10 pages", "summary": "The 2020s have been witnessing a very significant advance in the development\nof generative artificial intelligence tools, including text generation systems\nbased on large language models. These tools have been increasingly used to\ngenerate texts in the most diverse domains -- from technical texts to literary\ntexts --, which might eventually lead to a lower volume of written text\nproduction by humans. This article discusses the possibility of a future in\nwhich human beings will have lost or significantly decreased their ability to\nwrite due to the outsourcing of this activity to machines. This possibility\nparallels the loss of the ability to write in other moments of human history,\nsuch as during the so-called Greek Dark Ages (approx. 1200 BCE - 800 BCE).", "AI": {"tldr": "\u63a2\u8ba8\u4e86\u7531\u4e8e\u673a\u5668\u4ee3\u5199\uff0c\u672a\u6765\u4eba\u7c7b\u5199\u4f5c\u80fd\u529b\u53ef\u80fd\u4e27\u5931\u7684\u53ef\u80fd\u6027\uff0c\u7c7b\u6bd4\u5e0c\u814a\u9ed1\u6697\u65f6\u4ee3", "motivation": "2020\u5e74\u4ee3\u89c1\u8bc1\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5de5\u5177\u7684\u91cd\u5927\u53d1\u5c55\uff0c\u5305\u62ec\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u751f\u6210\u7cfb\u7edf\u3002\u8fd9\u4e9b\u5de5\u5177\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u7528\u4e8e\u751f\u6210\u5404\u79cd\u9886\u57df\u7684\u6587\u672c\uff0c\u4ece\u6280\u672f\u6587\u672c\u5230\u6587\u5b66\u6587\u672c\uff0c\u8fd9\u6700\u7ec8\u53ef\u80fd\u5bfc\u81f4\u4eba\u7c7b\u4e66\u9762\u6587\u672c\u4ea7\u91cf\u964d\u4f4e\u3002", "method": "\u8ba8\u8bba", "result": "\u4e0d\u9002\u7528", "conclusion": "\u4eba\u7c7b\u53ef\u80fd\u7531\u4e8e\u5c06\u5199\u4f5c\u5916\u5305\u7ed9\u673a\u5668\u800c\u5931\u53bb\u6216\u663e\u8457\u964d\u4f4e\u5199\u4f5c\u80fd\u529b"}}
{"id": "2508.19320", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19320", "abs": "https://arxiv.org/abs/2508.19320", "authors": ["Ming Chen", "Liyuan Cui", "Wenyuan Zhang", "Haoxian Zhang", "Yan Zhou", "Xiaohan Li", "Xiaoqiang Liu", "Pengfei Wan"], "title": "MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation", "comment": "Technical Report. Project Page: https://chenmingthu.github.io/milm/", "summary": "Recently, interactive digital human video generation has attracted widespread\nattention and achieved remarkable progress. However, building such a practical\nsystem that can interact with diverse input signals in real time remains\nchallenging to existing methods, which often struggle with high latency, heavy\ncomputational cost, and limited controllability. In this work, we introduce an\nautoregressive video generation framework that enables interactive multimodal\ncontrol and low-latency extrapolation in a streaming manner. With minimal\nmodifications to a standard large language model (LLM), our framework accepts\nmultimodal condition encodings including audio, pose, and text, and outputs\nspatially and semantically coherent representations to guide the denoising\nprocess of a diffusion head. To support this, we construct a large-scale\ndialogue dataset of approximately 20,000 hours from multiple sources, providing\nrich conversational scenarios for training. We further introduce a deep\ncompression autoencoder with up to 64$\\times$ reduction ratio, which\neffectively alleviates the long-horizon inference burden of the autoregressive\nmodel. Extensive experiments on duplex conversation, multilingual human\nsynthesis, and interactive world model highlight the advantages of our approach\nin low latency, high efficiency, and fine-grained multimodal controllability.", "AI": {"tldr": "This paper introduces an autoregressive video generation framework for interactive multimodal control with low latency, using a modified LLM, a large dialogue dataset, and a deep compression autoencoder.", "motivation": "Existing interactive digital human video generation methods struggle with high latency, heavy computational cost, and limited controllability.", "method": "An autoregressive video generation framework with minimal modifications to a standard large language model (LLM) is introduced. It accepts multimodal condition encodings (audio, pose, and text) and outputs representations to guide a diffusion head. A deep compression autoencoder is used for long-horizon inference.", "result": "The framework enables interactive multimodal control and low-latency extrapolation in a streaming manner. A large-scale dialogue dataset of approximately 20,000 hours was constructed. A deep compression autoencoder with up to 64x reduction ratio was introduced.", "conclusion": "The proposed autoregressive video generation framework achieves low latency, high efficiency, and fine-grained multimodal controllability, as demonstrated through experiments on duplex conversation, multilingual human synthesis, and interactive world model."}}
{"id": "2508.19381", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19381", "abs": "https://arxiv.org/abs/2508.19381", "authors": ["Jesus Lopez", "Saeefa Rubaiyet Nowmi", "Viviana Cadena", "Mohammad Saidur Rahman"], "title": "Towards Quantum Machine Learning for Malicious Code Analysis", "comment": "6 pages, 3 figures, 2 tables. Accepted at the International Workshop\n  on Quantum Computing and Reinforcement Learning (QCRL) @ IEEE Quantum Week\n  2025", "summary": "Classical machine learning (CML) has been extensively studied for malware\nclassification. With the emergence of quantum computing, quantum machine\nlearning (QML) presents a paradigm-shifting opportunity to improve malware\ndetection, though its application in this domain remains largely unexplored. In\nthis study, we investigate two hybrid quantum-classical models -- a Quantum\nMultilayer Perceptron (QMLP) and a Quantum Convolutional Neural Network (QCNN),\nfor malware classification. Both models utilize angle embedding to encode\nmalware features into quantum states. QMLP captures complex patterns through\nfull qubit measurement and data re-uploading, while QCNN achieves faster\ntraining via quantum convolution and pooling layers that reduce active qubits.\nWe evaluate both models on five widely used malware datasets -- API-Graph,\nEMBER-Domain, EMBER-Class, AZ-Domain, and AZ-Class, across binary and\nmulticlass classification tasks.\n  Our results show high accuracy for binary classification -- 95-96% on\nAPI-Graph, 91-92% on AZ-Domain, and 77% on EMBER-Domain. In multiclass\nsettings, accuracy ranges from 91.6-95.7% on API-Graph, 41.7-93.6% on AZ-Class,\nand 60.7-88.1% on EMBER-Class. Overall, QMLP outperforms QCNN in complex\nmulticlass tasks, while QCNN offers improved training efficiency at the cost of\nreduced accuracy.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0QMLP\u548cQCNN\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u6027\u80fd\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u7684\u51fa\u73b0\uff0c\u91cf\u5b50\u673a\u5668\u5b66\u4e60(QML)\u4e3a\u6539\u8fdb\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8303\u5f0f\u8f6c\u53d8\u7684\u673a\u4f1a\uff0c\u5c3d\u7ba1\u5b83\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u4e86\u4e24\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6a21\u578b\u2014\u2014\u91cf\u5b50\u591a\u5c42\u611f\u77e5\u5668(QMLP)\u548c\u91cf\u5b50\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(QCNN)\u7528\u4e8e\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u3002\u4e24\u79cd\u6a21\u578b\u90fd\u5229\u7528\u89d2\u5ea6\u5d4c\u5165\u5c06\u6076\u610f\u8f6f\u4ef6\u7279\u5f81\u7f16\u7801\u6210\u91cf\u5b50\u6001\u3002", "result": "\u5728\u4e8c\u5143\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u51c6\u786e\u7387\u2014\u2014API-Graph\u4e0a\u4e3a95-96%\uff0cAZ-Domain\u4e0a\u4e3a91-92%\uff0cEMBER-Domain\u4e0a\u4e3a77%\u3002\u5728\u591a\u7c7b\u8bbe\u7f6e\u4e2d\uff0cAPI-Graph\u7684\u51c6\u786e\u7387\u8303\u56f4\u4e3a91.6-95.7%\uff0cAZ-Class\u7684\u51c6\u786e\u7387\u8303\u56f4\u4e3a41.7-93.6%\uff0cEMBER-Class\u7684\u51c6\u786e\u7387\u8303\u56f4\u4e3a60.7-88.1%\u3002", "conclusion": "QMLP\u5728\u590d\u6742\u7684\u591a\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8eQCNN\uff0c\u800cQCNN\u4ee5\u964d\u4f4e\u51c6\u786e\u7387\u4e3a\u4ee3\u4ef7\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2508.19827", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19827", "abs": "https://arxiv.org/abs/2508.19827", "authors": ["Samuel Lewis-Lim", "Xingwei Tan", "Zhixue Zhao", "Nikolaos Aletras"], "title": "Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?", "comment": "Accepted at EMNLP 2025 Main Conference", "summary": "Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited\ngains for soft-reasoning problems such as analytical and commonsense reasoning.\nCoT can also be unfaithful to a model's actual reasoning. We investigate the\ndynamics and faithfulness of CoT in soft-reasoning tasks across\ninstruction-tuned, reasoning and reasoning-distilled models. Our findings\nreveal differences in how these models rely on CoT, and show that CoT influence\nand faithfulness are not always aligned.", "AI": {"tldr": "CoT may not be effective or reliable for soft-reasoning tasks.", "motivation": "CoT often yields limited gains for soft-reasoning problems such as analytical and commonsense reasoning. CoT can also be unfaithful to a model's actual reasoning.", "method": "investigate the dynamics and faithfulness of CoT in soft-reasoning tasks across instruction-tuned, reasoning and reasoning-distilled models", "result": "differences in how these models rely on CoT, and show that CoT influence and faithfulness are not always aligned.", "conclusion": "CoT influence and faithfulness are not always aligned."}}
{"id": "2508.19428", "categories": ["cs.CL", "cs.LO", "cs.SC", "68T30, 68T50, 68T07, 68U15", "I.2.4; I.2.7; H.3.1; H.3.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.19428", "abs": "https://arxiv.org/abs/2508.19428", "authors": ["Aleksandra Beliaeva", "Temurbek Rahmatullaev"], "title": "Heterogeneous LLM Methods for Ontology Learning (Few-Shot Prompting, Ensemble Typing, and Attention-Based Taxonomies)", "comment": null, "summary": "We present a comprehensive system for addressing Tasks A, B, and C of the\nLLMs4OL 2025 challenge, which together span the full ontology construction\npipeline: term extraction, typing, and taxonomy discovery. Our approach\ncombines retrieval-augmented prompting, zero-shot classification, and\nattention-based graph modeling -- each tailored to the demands of the\nrespective task. For Task A, we jointly extract domain-specific terms and their\nontological types using a retrieval-augmented generation (RAG) pipeline.\nTraining data was reformulated into a document to terms and types\ncorrespondence, while test-time inference leverages semantically similar\ntraining examples. This single-pass method requires no model finetuning and\nimproves overall performance through lexical augmentation Task B, which\ninvolves assigning types to given terms, is handled via a dual strategy. In the\nfew-shot setting (for domains with labeled training data), we reuse the RAG\nscheme with few-shot prompting. In the zero-shot setting (for previously unseen\ndomains), we use a zero-shot classifier that combines cosine similarity scores\nfrom multiple embedding models using confidence-based weighting. In Task C, we\nmodel taxonomy discovery as graph inference. Using embeddings of type labels,\nwe train a lightweight cross-attention layer to predict is-a relations by\napproximating a soft adjacency matrix. These modular, task-specific solutions\nenabled us to achieve top-ranking results in the official leaderboard across\nall three tasks. Taken together these strategies showcase the scalability,\nadaptability, and robustness of LLM-based architectures for ontology learning\nacross heterogeneous domains.\n  Code is available at:\nhttps://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbek", "AI": {"tldr": "A modular, LLM-based system for ontology construction (term extraction, typing, and taxonomy discovery) achieved top results in the LLMs4OL 2025 challenge using RAG, zero-shot classification, and graph modeling.", "motivation": "Addresses Tasks A, B, and C of the LLMs4OL 2025 challenge, which together span the full ontology construction pipeline: term extraction, typing, and taxonomy discovery.", "method": "Combines retrieval-augmented prompting, zero-shot classification, and attention-based graph modeling, each tailored to the demands of the respective task.", "result": "Achieved top-ranking results in the official leaderboard across all three tasks.", "conclusion": "LLM-based architectures demonstrate scalability, adaptability, and robustness for ontology learning across heterogeneous domains, achieving top-ranking results in the LLMs4OL 2025 challenge."}}
{"id": "2508.19324", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.19324", "abs": "https://arxiv.org/abs/2508.19324", "authors": ["Jefferson David Rodriguez Chivata", "Davide Ghiani", "Simone Maurizio La Cava", "Marco Micheletto", "Giulia Orr\u00f9", "Federico Lama", "Gian Luca Marcialis"], "title": "Deep Data Hiding for ICAO-Compliant Face Images: A Survey", "comment": "In 2025 IEEE International Joint Conference on Biometrics (IJCB)", "summary": "ICAO-compliant facial images, initially designed for secure biometric\npassports, are increasingly becoming central to identity verification in a wide\nrange of application contexts, including border control, digital travel\ncredentials, and financial services. While their standardization enables global\ninteroperability, it also facilitates practices such as morphing and deepfakes,\nwhich can be exploited for harmful purposes like identity theft and illegal\nsharing of identity documents. Traditional countermeasures like Presentation\nAttack Detection (PAD) are limited to real-time capture and offer no\npost-capture protection. This survey paper investigates digital watermarking\nand steganography as complementary solutions that embed tamper-evident signals\ndirectly into the image, enabling persistent verification without compromising\nICAO compliance. We provide the first comprehensive analysis of\nstate-of-the-art techniques to evaluate the potential and drawbacks of the\nunderlying approaches concerning the applications involving ICAO-compliant\nimages and their suitability under standard constraints. We highlight key\ntrade-offs, offering guidance for secure deployment in real-world identity\nsystems.", "AI": {"tldr": "\u672c\u8c03\u67e5\u8bba\u6587\u63a2\u8ba8\u4e86\u6570\u5b57\u6c34\u5370\u548c\u9690\u5199\u672f\u4f5c\u4e3a ICAO \u517c\u5bb9\u56fe\u50cf\u7684\u4e92\u8865\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u5b9e\u73b0\u6301\u4e45\u9a8c\u8bc1\uff0c\u5e76\u5206\u6790\u4e86\u73b0\u6709\u6280\u672f\u7684\u4f18\u7f3a\u70b9\uff0c\u4e3a\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "\u7b26\u5408 ICAO \u6807\u51c6\u7684\u9762\u90e8\u56fe\u50cf\u8d8a\u6765\u8d8a\u591a\u5730\u6210\u4e3a\u8eab\u4efd\u9a8c\u8bc1\u7684\u6838\u5fc3\uff0c\u4f46\u4e5f\u4fc3\u8fdb\u4e86\u8bf8\u5982\u56fe\u50cf\u53d8\u5f62\u548c\u6df1\u5ea6\u4f2a\u9020\u7b49\u884c\u4e3a\uff0c\u8fd9\u4e9b\u884c\u4e3a\u53ef\u80fd\u88ab\u5229\u7528\u4e8e\u8eab\u4efd\u76d7\u7a83\u548c\u975e\u6cd5\u5171\u4eab\u8eab\u4efd\u8bc1\u660e\u6587\u4ef6\u7b49\u6709\u5bb3\u76ee\u7684\u3002\u4f20\u7edf\u7684\u5bf9\u7b56\uff08\u5982 PAD\uff09\u4ec5\u9650\u4e8e\u5b9e\u65f6\u6355\u83b7\uff0c\u4e0d\u63d0\u4f9b\u6355\u83b7\u540e\u4fdd\u62a4\u3002", "method": "\u5bf9\u6700\u5148\u8fdb\u7684\u6280\u672f\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5206\u6790\uff0c\u4ee5\u8bc4\u4f30\u5e95\u5c42\u65b9\u6cd5\u5728\u6d89\u53ca ICAO \u517c\u5bb9\u56fe\u50cf\u7684\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u548c\u7f3a\u70b9\uff0c\u4ee5\u53ca\u5b83\u4eec\u5728\u6807\u51c6\u7ea6\u675f\u4e0b\u7684\u9002\u7528\u6027\u3002", "result": "\u5f3a\u8c03\u4e86\u5173\u952e\u7684\u6743\u8861\uff0c\u4e3a\u5728\u5b9e\u9645\u8eab\u4efd\u7cfb\u7edf\u4e2d\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u6307\u5bfc\u3002", "conclusion": "\u6570\u5b57\u6c34\u5370\u548c\u9690\u5199\u672f\u53ef\u4ee5\u4f5c\u4e3a\u4e92\u8865\u89e3\u51b3\u65b9\u6848\uff0c\u76f4\u63a5\u5c06\u9632\u7be1\u6539\u4fe1\u53f7\u5d4c\u5165\u5230\u56fe\u50cf\u4e2d\uff0c\u4ece\u800c\u5b9e\u73b0\u6301\u4e45\u9a8c\u8bc1\uff0c\u4e14\u4e0d\u5f71\u54cd ICAO \u517c\u5bb9\u6027\u3002"}}
{"id": "2508.19389", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2508.19389", "abs": "https://arxiv.org/abs/2508.19389", "authors": ["Owais Ahmad", "Milad Ramezankhani", "Anirudh Deodhar"], "title": "DETNO: A Diffusion-Enhanced Transformer Neural Operator for Long-Term Traffic Forecasting", "comment": null, "summary": "Accurate long-term traffic forecasting remains a critical challenge in\nintelligent transportation systems, particularly when predicting high-frequency\ntraffic phenomena such as shock waves and congestion boundaries over extended\nrollout horizons. Neural operators have recently gained attention as promising\ntools for modeling traffic flow. While effective at learning function space\nmappings, they inherently produce smooth predictions that fail to reconstruct\nhigh-frequency features such as sharp density gradients which results in rapid\nerror accumulation during multi-step rollout predictions essential for\nreal-time traffic management. To address these fundamental limitations, we\nintroduce a unified Diffusion-Enhanced Transformer Neural Operator (DETNO)\narchitecture. DETNO leverages a transformer neural operator with\ncross-attention mechanisms, providing model expressivity and super-resolution,\ncoupled with a diffusion-based refinement component that iteratively\nreconstructs high-frequency traffic details through progressive denoising. This\novercomes the inherent smoothing limitations and rollout instability of\nstandard neural operators. Through comprehensive evaluation on chaotic traffic\ndatasets, our method demonstrates superior performance in extended rollout\npredictions compared to traditional and transformer-based neural operators,\npreserving high-frequency components and improving stability over long\nprediction horizons.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6269\u6563\u589e\u5f3a Transformer \u795e\u7ecf\u7b97\u5b50 (DETNO) \u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5229\u7528 transformer \u795e\u7ecf\u7b97\u5b50\u548c\u57fa\u4e8e\u6269\u6563\u7684\u7ec6\u5316\u7ec4\u4ef6\uff0c\u901a\u8fc7\u9010\u6b65\u53bb\u566a\u8fed\u4ee3\u5730\u91cd\u5efa\u9ad8\u9891\u4ea4\u901a\u7ec6\u8282\u3002", "motivation": "\u51c6\u786e\u7684\u957f\u671f\u4ea4\u901a\u9884\u6d4b\u4ecd\u7136\u662f\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u4e00\u9879\u5173\u952e\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9884\u6d4b\u9ad8\u9891\u4ea4\u901a\u73b0\u8c61\uff08\u5982\u51b2\u51fb\u6ce2\u548c\u62e5\u5835\u8fb9\u754c\uff09\u65f6\u3002\u795e\u7ecf\u7b97\u5b50\u5728\u5b66\u4e60\u51fd\u6570\u7a7a\u95f4\u6620\u5c04\u65b9\u9762\u6709\u6548\uff0c\u4f46\u5b83\u4eec\u56fa\u6709\u5730\u4ea7\u751f\u5e73\u6ed1\u7684\u9884\u6d4b\uff0c\u65e0\u6cd5\u91cd\u5efa\u9ad8\u9891\u7279\u5f81\uff0c\u4f8b\u5982\u6025\u5267\u7684\u5bc6\u5ea6\u68af\u5ea6\uff0c\u8fd9\u5bfc\u81f4\u5728\u591a\u6b65 rollout \u9884\u6d4b\u671f\u95f4\u5feb\u901f\u7d2f\u79ef\u8bef\u5dee\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u65f6\u4ea4\u901a\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u5177\u6709\u4ea4\u53c9\u6ce8\u610f\u673a\u5236\u7684 transformer \u795e\u7ecf\u7b97\u5b50\uff0c\u63d0\u4f9b\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u548c\u8d85\u5206\u8fa8\u7387\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u6269\u6563\u7684\u7ec6\u5316\u7ec4\u4ef6\uff0c\u901a\u8fc7\u9010\u6b65\u53bb\u566a\u8fed\u4ee3\u5730\u91cd\u5efa\u9ad8\u9891\u4ea4\u901a\u7ec6\u8282\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6df7\u6c8c\u4ea4\u901a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u7684\u8bc4\u4f30\uff0c", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6269\u5c55\u7684 rollout \u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u4e0e\u4f20\u7edf\u7684\u548c\u57fa\u4e8e transformer \u7684\u795e\u7ecf\u7b97\u5b50\u76f8\u6bd4\uff0c\u4fdd\u6301\u4e86\u9ad8\u9891\u5206\u91cf\uff0c\u5e76\u5728\u8f83\u957f\u7684\u9884\u6d4b\u8303\u56f4\u5185\u63d0\u9ad8\u4e86\u7a33\u5b9a\u6027\u3002"}}
{"id": "2508.19851", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19851", "abs": "https://arxiv.org/abs/2508.19851", "authors": ["Romain Harang", "Jason Naradowsky", "Yaswitha Gujju", "Yusuke Miyao"], "title": "Tracking World States with Language Models: State-Based Evaluation Using Chess", "comment": "Spotlight presentation at ICML 2025 Workshop on Assessing World\n  Models", "summary": "Large Language Models (LLMs) exhibit emergent capabilities in structured\ndomains, suggesting they may implicitly internalize high-fidelity\nrepresentations of world models. While probing techniques have shown promising\nsigns of this in scientific and game-based settings, they rely on\nmodel-specific internal activations, which limit interpretability and\ngeneralizability. In this work, we propose a model-agnostic, state-based\nevaluation framework using chess as a benchmark to assess whether LLMs preserve\nthe semantics of structured environments. Our method analyzes the downstream\nlegal move distributions (state affordances) to estimate semantic fidelity\nbetween predicted and actual game states. This approach offers a more\nmeaningful evaluation than conventional string-based metrics by aligning more\nclosely with the strategic and rule-governed nature of chess. Experimental\nresults demonstrate that our metrics capture deficiencies in state-tracking,\nhighlighting limitations of LLMs in maintaining coherent internal models over\nlong sequences. Our framework provides a robust tool for evaluating structured\nreasoning in LLMs without requiring internal model access, and generalizes to a\nwide class of symbolic environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u3001\u57fa\u4e8e\u72b6\u6001\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528\u56fd\u9645\u8c61\u68cb\u4f5c\u4e3a\u57fa\u51c6\u6765\u8bc4\u4f30 LLM \u662f\u5426\u4fdd\u7559\u7ed3\u6784\u5316\u73af\u5883\u7684\u8bed\u4e49\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u7ed3\u6784\u5316\u9886\u57df\u8868\u73b0\u51fa\u65b0\u5174\u80fd\u529b\uff0c\u8868\u660e\u5b83\u4eec\u53ef\u80fd\u9690\u5f0f\u5730\u5c06\u4e16\u754c\u6a21\u578b\u7684\u9ad8\u4fdd\u771f\u8868\u793a\u5185\u5728\u5316\u3002\u867d\u7136\u63a2\u6d4b\u6280\u672f\u5728\u79d1\u5b66\u548c\u57fa\u4e8e\u6e38\u620f\u7684\u8bbe\u7f6e\u4e2d\u663e\u793a\u51fa\u6709\u5e0c\u671b\u7684\u8ff9\u8c61\uff0c\u4f46\u5b83\u4eec\u4f9d\u8d56\u4e8e\u7279\u5b9a\u4e8e\u6a21\u578b\u7684\u5185\u90e8\u6fc0\u6d3b\uff0c\u8fd9\u9650\u5236\u4e86\u89e3\u91ca\u6027\u548c\u666e\u904d\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u5206\u6790\u4e0b\u6e38\u7684\u5408\u6cd5\u79fb\u52a8\u5206\u5e03\uff08\u72b6\u6001\u53ef\u4f9b\u6027\uff09\u4ee5\u4f30\u8ba1\u9884\u6d4b\u548c\u5b9e\u9645\u6e38\u620f\u72b6\u6001\u4e4b\u95f4\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6307\u6807\u53ef\u4ee5\u6355\u6349\u72b6\u6001\u8ddf\u8e2a\u4e2d\u7684\u7f3a\u9677\uff0c\u7a81\u51fa\u4e86 LLM \u5728\u4fdd\u6301\u957f\u5e8f\u5217\u4e0a\u8fde\u8d2f\u7684\u5185\u90e8\u6a21\u578b\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4e2d\u7684\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u800c\u65e0\u9700\u5185\u90e8\u6a21\u578b\u8bbf\u95ee\uff0c\u5e76\u4e14\u53ef\u4ee5\u63a8\u5e7f\u5230\u5404\u79cd\u7b26\u53f7\u73af\u5883\u3002"}}
{"id": "2508.19464", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19464", "abs": "https://arxiv.org/abs/2508.19464", "authors": ["Philipp Borchert", "Jochen De Weerdt", "Marie-Francine Moens"], "title": "Bridging Language Gaps: Enhancing Few-Shot Language Adaptation", "comment": "17 pages", "summary": "The disparity in language resources poses a challenge in multilingual NLP,\nwith high-resource languages benefiting from extensive data, while low-resource\nlanguages lack sufficient data for effective training. Our Contrastive Language\nAlignment with Prompting (CoLAP) method addresses this gap by integrating\ncontrastive learning with cross-lingual representations, facilitating\ntask-specific knowledge transfer from high-resource to lower-resource\nlanguages. The primary advantage of our approach is its data efficiency,\nenabling rapid adaptation to new languages and reducing the need for large\nlabeled datasets. We conduct experiments with multilingual encoder-only and\ndecoder-only language models on natural language understanding tasks, including\nnatural language inference and relation extraction, evaluating performance\nacross both high- and low-resource languages. Our results demonstrate that\nCoLAP outperforms few-shot cross-lingual transfer baselines and in-context\nlearning, even with limited available data. This effectively narrows the\ncross-lingual performance gap, contributing to the development of more\nefficient multilingual NLP techniques.", "AI": {"tldr": "CoLAP\u901a\u8fc7\u6574\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u8de8\u8bed\u8a00\u8868\u793a\uff0c\u4fc3\u8fdb\u4e86\u4ece\u9ad8\u8d44\u6e90\u5230\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u8f6c\u79fb\uff0c\u4ece\u800c\u7f29\u5c0f\u4e86\u8de8\u8bed\u8a00\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u591a\u8bed\u8a00NLP\u4e2d\u8bed\u8a00\u8d44\u6e90\u7684\u5dee\u5f02\u5e26\u6765\u4e86\u6311\u6218\uff0c\u9ad8\u8d44\u6e90\u8bed\u8a00\u53d7\u76ca\u4e8e\u5927\u91cf\u6570\u636e\uff0c\u800c\u4f4e\u8d44\u6e90\u8bed\u8a00\u7f3a\u4e4f\u6709\u6548\u8bad\u7ec3\u7684\u8db3\u591f\u6570\u636e\u3002", "method": "\u96c6\u6210\u4e86\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u8de8\u8bed\u8a00\u8868\u793a\u7684CoLAP\u65b9\u6cd5\u3002", "result": "CoLAP\u4f18\u4e8efew-shot\u8de8\u8bed\u8a00\u8fc1\u79fb\u57fa\u7ebf\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u5373\u4f7f\u5728\u6709\u9650\u7684\u53ef\u7528\u6570\u636e\u4e0b\u4e5f\u662f\u5982\u6b64\u3002", "conclusion": "CoLAP\u6709\u6548\u5730\u7f29\u5c0f\u4e86\u8de8\u8bed\u8a00\u6027\u80fd\u5dee\u8ddd\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u591a\u8bed\u8a00NLP\u6280\u672f\u3002"}}
{"id": "2508.19325", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19325", "abs": "https://arxiv.org/abs/2508.19325", "authors": ["Haoyang Su", "Jin-Yi Xiang", "Shaohao Rui", "Yifan Gao", "Xingyu Chen", "Tingxuan Yin", "Xiaosong Wang", "Lian-Ming Wu"], "title": "PRISM: A Framework Harnessing Unsupervised Visual Representations and Textual Prompts for Explainable MACE Survival Prediction from Cardiac Cine MRI", "comment": null, "summary": "Accurate prediction of major adverse cardiac events (MACE) remains a central\nchallenge in cardiovascular prognosis. We present PRISM (Prompt-guided\nRepresentation Integration for Survival Modeling), a self-supervised framework\nthat integrates visual representations from non-contrast cardiac cine magnetic\nresonance imaging with structured electronic health records (EHRs) for survival\nanalysis. PRISM extracts temporally synchronized imaging features through\nmotion-aware multi-view distillation and modulates them using medically\ninformed textual prompts to enable fine-grained risk prediction. Across four\nindependent clinical cohorts, PRISM consistently surpasses classical survival\nprediction models and state-of-the-art (SOTA) deep learning baselines under\ninternal and external validation. Further clinical findings demonstrate that\nthe combined imaging and EHR representations derived from PRISM provide\nvaluable insights into cardiac risk across diverse cohorts. Three distinct\nimaging signatures associated with elevated MACE risk are uncovered, including\nlateral wall dyssynchrony, inferior wall hypersensitivity, and anterior\nelevated focus during diastole. Prompt-guided attribution further identifies\nhypertension, diabetes, and smoking as dominant contributors among clinical and\nphysiological EHR factors.", "AI": {"tldr": "PRISM\u662f\u4e00\u79cd\u65b0\u7684\u5fc3\u8840\u7ba1\u98ce\u9669\u9884\u6d4b\u6846\u67b6\uff0c\u5b83\u6574\u5408\u4e86\u5f71\u50cf\u548cEHR\u6570\u636e\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u63ed\u793a\u4e86\u65b0\u7684\u98ce\u9669\u56e0\u7d20\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u4e3b\u8981\u4e0d\u826f\u5fc3\u810f\u4e8b\u4ef6(MACE)\u4ecd\u7136\u662f\u5fc3\u8840\u7ba1\u9884\u540e\u4e2d\u7684\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\u3002", "method": "PRISM\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u5b83\u6574\u5408\u4e86\u975e\u5bf9\u6bd4\u5fc3\u810f\u7535\u5f71\u78c1\u5171\u632f\u6210\u50cf\u7684\u89c6\u89c9\u8868\u5f81\u548c\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55(EHR)\uff0c\u7528\u4e8e\u751f\u5b58\u5206\u6790\u3002\u5b83\u901a\u8fc7\u8fd0\u52a8\u611f\u77e5\u591a\u89c6\u56fe\u84b8\u998f\u63d0\u53d6\u65f6\u95f4\u540c\u6b65\u7684\u5f71\u50cf\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u533b\u5b66\u6587\u672c\u63d0\u793a\u6765\u8c03\u8282\u5b83\u4eec\uff0c\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u98ce\u9669\u9884\u6d4b\u3002", "result": "PRISM\u5728\u56db\u4e2a\u72ec\u7acb\u7684\u4e34\u5e8a\u961f\u5217\u4e2d\uff0c\u5728\u5185\u90e8\u548c\u5916\u90e8\u9a8c\u8bc1\u4e0b\uff0c\u59cb\u7ec8\u4f18\u4e8e\u7ecf\u5178\u751f\u5b58\u9884\u6d4b\u6a21\u578b\u548c\u6700\u5148\u8fdb\u7684(SOTA)\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u3002\u53d1\u73b0\u4e86\u4e09\u4e2a\u4e0eMACE\u98ce\u9669\u76f8\u5173\u7684\u72ec\u7279\u5f71\u50cf\u7279\u5f81\uff0c\u5305\u62ec\u4fa7\u58c1\u4e0d\u540c\u6b65\u3001\u4e0b\u58c1\u8fc7\u654f\u548c\u8212\u5f20\u671f\u524d\u90e8\u7126\u70b9\u5347\u9ad8\u3002\u63d0\u793a\u5f15\u5bfc\u7684\u5f52\u56e0\u8fdb\u4e00\u6b65\u786e\u5b9a\u9ad8\u8840\u538b\u3001\u7cd6\u5c3f\u75c5\u548c\u5438\u70df\u662f\u4e34\u5e8a\u548c\u751f\u7406EHR\u56e0\u7d20\u4e2d\u7684\u4e3b\u8981\u8d21\u732e\u8005\u3002", "conclusion": "PRISM\u901a\u8fc7\u6574\u5408\u5f71\u50cf\u548cEHR\u4fe1\u606f\uff0c\u4e3a\u5fc3\u8840\u7ba1\u98ce\u9669\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u63ed\u793a\u4e86\u4e0eMACE\u98ce\u9669\u76f8\u5173\u7684\u5f71\u50cf\u7279\u5f81\u548cEHR\u56e0\u7d20\u3002"}}
{"id": "2508.19394", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.19394", "abs": "https://arxiv.org/abs/2508.19394", "authors": ["Afrar Jahin", "Yi Pan", "Yingfeng Wang", "Tianming Liu", "Wei Zhang"], "title": "Quantum-Classical Hybrid Molecular Autoencoder for Advancing Classical Decoding", "comment": null, "summary": "Although recent advances in quantum machine learning (QML) offer significant\npotential for enhancing generative models, particularly in molecular design, a\nlarge array of classical approaches still face challenges in achieving high\nfidelity and validity. In particular, the integration of QML with\nsequence-based tasks, such as Simplified Molecular Input Line Entry System\n(SMILES) string reconstruction, remains underexplored and usually suffers from\nfidelity degradation. In this work, we propose a hybrid quantum-classical\narchitecture for SMILES reconstruction that integrates quantum encoding with\nclassical sequence modeling to improve quantum fidelity and classical\nsimilarity. Our approach achieves a quantum fidelity of approximately 84% and a\nclassical reconstruction similarity of 60%, surpassing existing quantum\nbaselines. Our work lays a promising foundation for future QML applications,\nstriking a balance between expressive quantum representations and classical\nsequence models and catalyzing broader research on quantum-aware sequence\nmodels for molecular and drug discovery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u67b6\u6784\u7528\u4e8eSMILES\u91cd\u5efa\uff0c\u8be5\u67b6\u6784\u96c6\u6210\u4e86\u91cf\u5b50\u7f16\u7801\u4e0e\u7ecf\u5178\u5e8f\u5217\u5efa\u6a21\uff0c\u4ee5\u63d0\u9ad8\u91cf\u5b50\u4fdd\u771f\u5ea6\u548c\u7ecf\u5178\u76f8\u4f3c\u6027\u3002", "motivation": "\u5c3d\u7ba1\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff08QML\uff09\u7684\u6700\u65b0\u8fdb\u5c55\u4e3a\u589e\u5f3a\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u5206\u5b50\u8bbe\u8ba1\u4e2d\uff0c\u4f46\u5927\u91cf\u7ecf\u5178\u65b9\u6cd5\u5728\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u548c\u6709\u6548\u6027\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u6311\u6218\u3002\u7279\u522b\u662f\uff0cQML\u4e0e\u57fa\u4e8e\u5e8f\u5217\u7684\u4efb\u52a1\uff08\u5982SMILES\u5b57\u7b26\u4e32\u91cd\u5efa\uff09\u7684\u96c6\u6210\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u5e76\u4e14\u901a\u5e38\u4f1a\u906d\u53d7\u4fdd\u771f\u5ea6\u4e0b\u964d\u7684\u5f71\u54cd\u3002", "method": "\u4e00\u79cd\u7528\u4e8eSMILES\u91cd\u5efa\u7684\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u96c6\u6210\u4e86\u91cf\u5b50\u7f16\u7801\u4e0e\u7ecf\u5178\u5e8f\u5217\u5efa\u6a21\uff0c\u4ee5\u63d0\u9ad8\u91cf\u5b50\u4fdd\u771f\u5ea6\u548c\u7ecf\u5178\u76f8\u4f3c\u6027\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5927\u7ea684\uff05\u7684\u91cf\u5b50\u4fdd\u771f\u5ea6\u548c60\uff05\u7684\u7ecf\u5178\u91cd\u5efa\u76f8\u4f3c\u6027\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u7684\u91cf\u5b50\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u7684\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5e94\u7528\u5960\u5b9a\u4e86\u6709\u5e0c\u671b\u7684\u57fa\u7840\uff0c\u5728\u8868\u8fbe\u6027\u91cf\u5b50\u8868\u793a\u548c\u7ecf\u5178\u5e8f\u5217\u6a21\u578b\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u5e76\u4fc3\u8fdb\u4e86\u5bf9\u5206\u5b50\u548c\u836f\u7269\u53d1\u73b0\u7684\u91cf\u5b50\u611f\u77e5\u5e8f\u5217\u6a21\u578b\u7684\u66f4\u5e7f\u6cdb\u7814\u7a76\u3002"}}
{"id": "2508.19932", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19932", "abs": "https://arxiv.org/abs/2508.19932", "authors": ["Nitish Jaipuria", "Lorenzo Gatto", "Zijun Kan", "Shankey Poddar", "Bill Cheung", "Diksha Bansal", "Ramanan Balakrishnan", "Aviral Suri", "Jose Estevez"], "title": "CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments", "comment": "10 pages, 5 figures", "summary": "The proliferation of digital payment platforms has transformed commerce,\noffering unmatched convenience and accessibility globally. However, this growth\nhas also attracted malicious actors, leading to a corresponding increase in\nsophisticated social engineering scams. These scams are often initiated and\norchestrated on multiple surfaces outside the payment platform, making user and\ntransaction-based signals insufficient for a complete understanding of the\nscam's methodology and underlying patterns, without which it is very difficult\nto prevent it in a timely manner. This paper presents CASE (Conversational\nAgent for Scam Elucidation), a novel Agentic AI framework that addresses this\nproblem by collecting and managing user scam feedback in a safe and scalable\nmanner. A conversational agent is uniquely designed to proactively interview\npotential victims to elicit intelligence in the form of a detailed\nconversation. The conversation transcripts are then consumed by another AI\nsystem that extracts information and converts it into structured data for\ndownstream usage in automated and manual enforcement mechanisms. Using Google's\nGemini family of LLMs, we implemented this framework on Google Pay (GPay)\nIndia. By augmenting our existing features with this new intelligence, we have\nobserved a 21% uplift in the volume of scam enforcements. The architecture and\nits robust evaluation framework are highly generalizable, offering a blueprint\nfor building similar AI-driven systems to collect and manage scam intelligence\nin other sensitive domains.", "AI": {"tldr": "This paper presents CASE, a novel Agentic AI framework that addresses this problem by collecting and managing user scam feedback in a safe and scalable manner.", "motivation": "digital payment platforms has transformed commerce, offering unmatched convenience and accessibility globally. However, this growth has also attracted malicious actors, leading to a corresponding increase in sophisticated social engineering scams. These scams are often initiated and orchestrated on multiple surfaces outside the payment platform, making user and transaction-based signals insufficient for a complete understanding of the scam's methodology and underlying patterns, without which it is very difficult to prevent it in a timely manner.", "method": "a novel Agentic AI framework that addresses this problem by collecting and managing user scam feedback in a safe and scalable manner. A conversational agent is uniquely designed to proactively interview potential victims to elicit intelligence in the form of a detailed conversation. The conversation transcripts are then consumed by another AI system that extracts information and converts it into structured data for downstream usage in automated and manual enforcement mechanisms. Using Google's Gemini family of LLMs, we implemented this framework on Google Pay (GPay) India.", "result": "By augmenting our existing features with this new intelligence, we have observed a 21% uplift in the volume of scam enforcements.", "conclusion": "The architecture and its robust evaluation framework are highly generalizable, offering a blueprint for building similar AI-driven systems to collect and manage scam intelligence in other sensitive domains."}}
{"id": "2508.19349", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19349", "abs": "https://arxiv.org/abs/2508.19349", "authors": ["Mahdieh Behjat Khatooni", "Mohsen Soryani"], "title": "EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis", "comment": null, "summary": "Alzheimer's disease (AD) is one of the most prevalent neurodegenerative\ndisorders worldwide. As it progresses, it leads to the deterioration of\ncognitive functions. Since AD is irreversible, early diagnosis is crucial for\nmanaging its progression. Mild Cognitive Impairment (MCI) represents an\nintermediate stage between Cognitively Normal (CN) individuals and those with\nAD, and is considered a transitional phase from normal cognition to Alzheimer's\ndisease. Diagnosing MCI is particularly challenging due to the subtle\ndifferences between adjacent diagnostic categories. In this study, we propose\nEffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole\nAlzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging\n(MRI) dataset. Our model integrates a Convolutional Neural Network (CNN) with a\nVision Transformer (ViT) to capture both local and global features from MRI\nimages. Unlike previous studies that rely on limited subsets of data, our\napproach is trained on the full T1-weighted MRI dataset from ADNI, resulting in\na more robust and unbiased model. This comprehensive methodology enhances the\nmodel's clinical reliability. Furthermore, fine-tuning large pretrained models\noften yields suboptimal results when source and target dataset domains differ.\nTo address this, we incorporate Low-Rank Adaptation (LoRA) to effectively adapt\nthe pretrained ViT model to our target domain. This method enables efficient\nknowledge transfer and reduces the risk of overfitting. Our model achieves a\nclassification accuracy of 92.52% and an F1-score of 92.76% across three\ndiagnostic categories: AD, MCI, and CN for full ADNI dataset.", "AI": {"tldr": "This paper introduces EffNetViTLoRA, a CNN-ViT model with LoRA, for improved AD diagnosis using the full ADNI MRI dataset, achieving high accuracy in classifying AD, MCI, and CN.", "motivation": "Early diagnosis of Alzheimer's Disease (AD) is crucial, and Mild Cognitive Impairment (MCI) is a challenging transitional phase to diagnose. Existing studies rely on limited data subsets, and fine-tuning large pretrained models can yield suboptimal results.", "method": "The study proposes EffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole ADNI MRI dataset. The model integrates a CNN with a ViT and incorporates LoRA to adapt the pretrained ViT model.", "result": "The EffNetViTLoRA model demonstrates enhanced clinical reliability and achieves a classification accuracy of 92.52% and an F1-score of 92.76% across three diagnostic categories.", "conclusion": "The EffNetViTLoRA model achieves a classification accuracy of 92.52% and an F1-score of 92.76% across three diagnostic categories: AD, MCI, and CN for full ADNI dataset."}}
{"id": "2508.19410", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.19410", "abs": "https://arxiv.org/abs/2508.19410", "authors": ["Zongyu Wu", "Ruichen Xu", "Luoyao Chen", "Georgios Kementzidis", "Siyao Wang", "Yuefan Deng"], "title": "Kolmogorov-Arnold Representation for Symplectic Learning: Advancing Hamiltonian Neural Networks", "comment": "Comments: 8 pages, 6 figures. Accepted at IJCNN 2025 (to appear in\n  IEEE/IJCNN proceedings). This arXiv submission corresponds to the\n  camera-ready version with minor editorial clarifications; results unchanged", "summary": "We propose a Kolmogorov-Arnold Representation-based Hamiltonian Neural\nNetwork (KAR-HNN) that replaces the Multilayer Perceptrons (MLPs) with\nunivariate transformations. While Hamiltonian Neural Networks (HNNs) ensure\nenergy conservation by learning Hamiltonian functions directly from data,\nexisting implementations, often relying on MLPs, cause hypersensitivity to the\nhyperparameters while exploring complex energy landscapes. Our approach\nexploits the localized function approximations to better capture high-frequency\nand multi-scale dynamics, reducing energy drift and improving long-term\npredictive stability. The networks preserve the symplectic form of Hamiltonian\nsystems, and thus maintain interpretability and physical consistency. After\nassessing KAR-HNN on four benchmark problems including spring-mass, simple\npendulum, two- and three-body problem, we foresee its effectiveness for\naccurate and stable modeling of realistic physical processes often at high\ndimensions and with few known parameters.", "AI": {"tldr": "KAR-HNN replaces MLPs in HNNs with univariate transformations to improve stability and accuracy in modeling physical processes.", "motivation": "Existing HNN implementations relying on MLPs cause hypersensitivity to hyperparameters while exploring complex energy landscapes.", "method": "Replaces MLPs with univariate transformations in a Hamiltonian Neural Network (HNN).", "result": "Reduces energy drift and improves long-term predictive stability; preserves the symplectic form of Hamiltonian systems.", "conclusion": "KAR-HNN shows effectiveness for accurate and stable modeling of realistic physical processes."}}
{"id": "2508.19963", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19963", "abs": "https://arxiv.org/abs/2508.19963", "authors": ["M. Umlauft", "M. Schranz"], "title": "Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants", "comment": "This is the author's version of a paper reviewed and accepted by the\n  9th International Symposium on Swarm Behavior and Bio-Inspired Robotics 2025.\n  Authors were not able to present it due to time constraints. 3 Tables, 5\n  Figures", "summary": "Optimizing modern production plants using the job-shop principle is a known\nhard problem. For very large plants, like semiconductor fabs, the problem\nbecomes unsolvable on a plant-wide scale in a reasonable amount of time using\nclassical linear optimization. An alternative approach is the use of swarm\nintelligence algorithms. These have been applied to the job-shop problem\nbefore, but often in a centrally calculated way where they are applied to the\nsolution space, but they can be implemented in a bottom-up fashion to avoid\nglobal result computation as well. One of the problems in semiconductor\nproduction is that the production process requires a lot of switching between\nmachines that process lots one after the other and machines that process\nbatches of lots at once, often with long processing times. In this paper, we\naddress this switching problem with the ``boids'' flocking algorithm that was\noriginally used in robotics and movie industry. The flocking behavior is a\nbio-inspired algorithm that uses only local information and interaction based\non simple heuristics. We show that this algorithm addresses these valid\nconsiderations in production plant optimization, as it reacts to the switching\nof machine kinds similar to how a swarm of flocking animals would react to\nobstacles in its course.", "AI": {"tldr": "This paper uses the ``boids'' flocking algorithm to address the switching problem in semiconductor production.", "motivation": "Optimizing modern production plants using the job-shop principle is a known hard problem. For very large plants, like semiconductor fabs, the problem becomes unsolvable on a plant-wide scale in a reasonable amount of time using classical linear optimization.", "method": "The \"boids\" flocking algorithm.", "result": "The algorithm reacts to the switching of machine kinds similar to how a swarm of flocking animals would react to obstacles in its course.", "conclusion": "The boids flocking algorithm can address the switching problem in semiconductor production."}}
{"id": "2508.19475", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19475", "abs": "https://arxiv.org/abs/2508.19475", "authors": ["Md. Alvee Ehsan", "A. S. M Mehedi Hasan", "Kefaya Benta Shahnoor", "Syeda Sumaiya Tasneem"], "title": "Automatic Question & Answer Generation Using Generative Large Language Model (LLM)", "comment": null, "summary": "\\Abstract{In the realm of education, student evaluation holds equal\nsignificance as imparting knowledge. To be evaluated, students usually need to\ngo through text-based academic assessment methods. Instructors need to make\ndiverse sets of questions that need to be fair for all students to prove their\nadequacy over a particular topic. This can prove to be quite challenging as\nthey may need to manually go through several different lecture materials. Our\nobjective is to make this whole process much easier by implementing Automatic\nQuestion Answer Generation /(AQAG), using fine-tuned generative LLM. For\ntailoring the instructor's preferred question style (MCQ, conceptual, or\nfactual questions), prompt Engineering (PE) is being utilized. In this\nresearch, we propose to leverage unsupervised learning methods in NLP,\nprimarily focusing on the English language. This approach empowers the base\nMeta-Llama 2-7B model to integrate RACE dataset as training data for the\nfine-tuning process. Creating a customized model that will offer efficient\nsolutions for educators, instructors, and individuals engaged in text-based\nevaluations. A reliable and efficient tool for generating questions and answers\ncan free up valuable time and resources, thus streamlining their evaluation\nprocesses.}", "AI": {"tldr": "This paper proposes an Automatic Question Answer Generation (AQAG) method using fine-tuned generative LLM to help instructors create diverse and fair questions. It leverages unsupervised learning in NLP and uses Meta-Llama 2-7B model with RACE dataset for fine-tuning, aiming to streamline the evaluation process and free up resources.", "motivation": "Instructors need to make diverse sets of questions that need to be fair for all students to prove their adequacy over a particular topic. This can prove to be quite challenging as they may need to manually go through several different lecture materials. The objective is to make this whole process much easier by implementing Automatic Question Answer Generation (AQAG).", "method": "Automatic Question Answer Generation (AQAG), using fine-tuned generative LLM with prompt Engineering (PE). Unsupervised learning methods in NLP, primarily focusing on the English language. Base Meta-Llama 2-7B model to integrate RACE dataset as training data for the fine-tuning process.", "result": "The research proposes to leverage unsupervised learning methods in NLP, primarily focusing on the English language. This approach empowers the base Meta-Llama 2-7B model to integrate RACE dataset as training data for the fine-tuning process.", "conclusion": "A customized model is created that will offer efficient solutions for educators, instructors, and individuals engaged in text-based evaluations. A reliable and efficient tool for generating questions and answers can free up valuable time and resources, thus streamlining their evaluation processes."}}
{"id": "2508.19477", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19477", "abs": "https://arxiv.org/abs/2508.19477", "authors": ["Zachary L. Crang", "Rich D. Johnston", "Katie L. Mills", "Johsan Billingham", "Sam Robertson", "Michael H. Cole", "Jonathon Weakley", "Adam Hewitt and", "Grant M. Duthie"], "title": "Concurrent validity of computer-vision artificial intelligence player tracking software using broadcast footage", "comment": null, "summary": "This study aimed to: (1) understand whether commercially available\ncomputer-vision and artificial intelligence (AI) player tracking software can\naccurately measure player position, speed and distance using broadcast footage\nand (2) determine the impact of camera feed and resolution on accuracy. Data\nwere obtained from one match at the 2022 Qatar Federation Internationale de\nFootball Association (FIFA) World Cup. Tactical, programme and camera 1 feeds\nwere used. Three commercial tracking providers that use computer-vision and AI\nparticipated. Providers analysed instantaneous position (x, y coordinates) and\nspeed (m\\,s^{-1}) of each player. Their data were compared with a\nhigh-definition multi-camera tracking system (TRACAB Gen 5). Root mean square\nerror (RMSE) and mean bias were calculated. Position RMSE ranged from 1.68 to\n16.39 m, while speed RMSE ranged from 0.34 to 2.38 m\\,s^{-1}. Total match\ndistance mean bias ranged from -1745 m (-21.8%) to 1945 m (24.3%) across\nproviders. Computer-vision and AI player tracking software offer the ability to\ntrack players with fair precision when players are detected by the software.\nProviders should use a tactical feed when tracking position and speed, which\nwill maximise player detection, improving accuracy. Both 720p and 1080p\nresolutions are suitable, assuming appropriate computer-vision and AI models\nare implemented.", "AI": {"tldr": "This study assesses the accuracy of commercial computer-vision and AI software in tracking player position and speed using broadcast footage, finding fair precision with tactical feeds and suitable 720p/1080p resolutions.", "motivation": "This study aimed to: (1) understand whether commercially available computer-vision and artificial intelligence (AI) player tracking software can accurately measure player position, speed and distance using broadcast footage and (2) determine the impact of camera feed and resolution on accuracy.", "method": "Three commercial tracking providers that use computer-vision and AI participated. Providers analysed instantaneous position (x, y coordinates) and speed (m s-1) of each player. Their data were compared with a high-definition multi-camera tracking system (TRACAB Gen 5). Root mean square error (RMSE) and mean bias were calculated.", "result": "Position RMSE ranged from 1.68 to 16.39 m, while speed RMSE ranged from 0.34 to 2.38 m s-1. Total match distance mean bias ranged from -1745 m (-21.8%) to 1945 m (24.3%) across providers.", "conclusion": "Computer-vision and AI player tracking software offer the ability to track players with fair precision when players are detected by the software. Providers should use a tactical feed when tracking position and speed, which will maximise player detection, improving accuracy. Both 720p and 1080p resolutions are suitable, assuming appropriate computer-vision and AI models are implemented."}}
{"id": "2508.19414", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19414", "abs": "https://arxiv.org/abs/2508.19414", "authors": ["Gustavo Sandoval"], "title": "Even Heads Fix Odd Errors: Mechanistic Discovery and Surgical Repair in Transformer Attention", "comment": "9 pages", "summary": "We present a mechanistic case study of a format-dependent reasoning failure\nin Llama-3.1-8B-Instruct, where the model incorrectly judges \"9.11\" as larger\nthan \"9.8\" in chat or Q&A formats, but answers correctly in simple format.\nThrough systematic intervention, we discover transformers implement even/odd\nattention head specialization: even indexed heads handle numerical comparison,\nwhile odd heads serve incompatible functions. The bug requires exactly 8 even\nheads at Layer 10 for perfect repair. Any combination of 8+ even heads\nsucceeds, while 7 or fewer completely fails, revealing sharp computational\nthresholds with perfect redundancy among the 16 even heads. SAE analysis\nreveals the mechanism: format representations separate (10% feature overlap at\nLayer 7), then re-entangle with different weightings (80% feature overlap at\nLayer 10), with specific features showing 1.5x amplification in failing\nformats. We achieve perfect repair using only 25% of attention heads and\nidentify a 60% pattern replacement threshold, demonstrating that apparent\nfull-module requirements hide sophisticated substructure with implications for\ninterpretability and efficiency. All of our code is available at\nhttps://github.com/gussand/surgeon.", "AI": {"tldr": "Llama-3.1-8B-Instruct\u6a21\u578b\u5728\u7279\u5b9a\u683c\u5f0f\u4e0b\u6bd4\u8f83\u6570\u503c\u51fa\u9519\uff0c\u901a\u8fc7\u8c03\u6574\u6ce8\u610f\u529b\u5934\u53ef\u4ee5\u4fee\u590d\u3002", "motivation": "\u7814\u7a76Llama-3.1-8B-Instruct\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u683c\u5f0f\u4f9d\u8d56\u63a8\u7406\u5931\u8d25\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5e72\u9884\uff0c\u53d1\u73b0transformers\u5b9e\u73b0\u4e86\u5076\u6570/\u5947\u6570\u6ce8\u610f\u529b\u5934\u5206\u5de5\u3002", "result": "\u53d1\u73b0\u5076\u6570\u7d22\u5f15\u6ce8\u610f\u529b\u5934\u5904\u7406\u6570\u503c\u6bd4\u8f83\uff0c\u5947\u6570\u5934\u5904\u7406\u4e0d\u517c\u5bb9\u7684\u529f\u80fd\u3002\u9700\u8981\u7b2c10\u5c42\u4e2d\u76848\u4e2a\u5076\u6570\u5934\u624d\u80fd\u5b8c\u5168\u4fee\u590d\u3002SAE\u5206\u6790\u63ed\u793a\u4e86\u683c\u5f0f\u8868\u793a\u7684\u5206\u79bb\u548c\u91cd\u65b0\u7ea0\u7f20\u673a\u5236\uff0c\u4ee5\u53ca\u7279\u5b9a\u7279\u5f81\u5728\u5931\u8d25\u683c\u5f0f\u4e2d\u7684\u653e\u5927\u3002", "conclusion": "Llama-3.1-8B-Instruct\u6a21\u578b\u5728\u7279\u5b9a\u683c\u5f0f\u4e0b\u9519\u8bef\u5224\u65ad\u6570\u503c\u5927\u5c0f\uff0c\u4f46\u7b80\u5355\u683c\u5f0f\u4e0b\u6b63\u786e\u3002\u4fee\u590d\u8be5\u9519\u8bef\u9700\u8981\u7b2c10\u5c42\u4e2d\u76848\u4e2a\u5076\u6570\u7d22\u5f15\u6ce8\u610f\u529b\u5934\u3002"}}
{"id": "2508.20018", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.20018", "abs": "https://arxiv.org/abs/2508.20018", "authors": ["Quanfeng Lu", "Zhantao Ma", "Shuai Zhong", "Jin Wang", "Dahai Yu", "Michael K. Ng", "Ping Luo"], "title": "SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control", "comment": "28 pages, 12 figures", "summary": "The rapid advancement of large vision language models (LVLMs) and agent\nsystems has heightened interest in mobile GUI agents that can reliably\ntranslate natural language into interface operations. Existing single-agent\napproaches, however, remain limited by structural constraints. Although\nmulti-agent systems naturally decouple different competencies, recent progress\nin multi-agent reinforcement learning (MARL) has often been hindered by\ninefficiency and remains incompatible with current LVLM architectures. To\naddress these challenges, we introduce SWIRL, a staged workflow for interleaved\nreinforcement learning designed for multi-agent systems. SWIRL reformulates\nMARL into a sequence of single-agent reinforcement learning tasks, updating one\nagent at a time while keeping the others fixed. This formulation enables stable\ntraining and promotes efficient coordination across agents. Theoretically, we\nprovide a stepwise safety bound, a cross-round monotonic improvement theorem,\nand convergence guarantees on return, ensuring robust and principled\noptimization. In application to mobile GUI control, SWIRL instantiates a\nNavigator that converts language and screen context into structured plans, and\nan Interactor that grounds these plans into executable atomic actions.\nExtensive experiments demonstrate superior performance on both high-level and\nlow-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong\ncapability in multi-agent mathematical reasoning, underscoring its potential as\na general framework for developing efficient and robust multi-agent systems.", "AI": {"tldr": "SWIRL\u662f\u4e00\u79cd\u65b0\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u5355\u667a\u80fd\u4f53\u5b66\u4e60\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728GUI\u63a7\u5236\u548c\u6570\u5b66\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u548c\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5feb\u901f\u53d1\u5c55\u63d0\u9ad8\u4e86\u4eba\u4eec\u5bf9\u80fd\u591f\u53ef\u9760\u5730\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u5316\u4e3a\u754c\u9762\u64cd\u4f5c\u7684\u79fb\u52a8GUI\u667a\u80fd\u4f53\u7684\u5174\u8da3\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5355\u667a\u80fd\u4f53\u65b9\u6cd5\u4ecd\u7136\u53d7\u5230\u7ed3\u6784\u6027\u7ea6\u675f\u3002\u867d\u7136\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u81ea\u7136\u5730\u5c06\u4e0d\u540c\u7684\u80fd\u529b\u89e3\u8026\uff0c\u4f46\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7684\u6700\u65b0\u8fdb\u5c55\u5f80\u5f80\u53d7\u5230\u6548\u7387\u4f4e\u4e0b\u7684\u963b\u788d\uff0c\u5e76\u4e14\u4e0e\u5f53\u524d\u7684LVLM\u67b6\u6784\u4e0d\u517c\u5bb9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u4ea4\u9519\u5f3a\u5316\u5b66\u4e60\u7684\u5206\u9636\u6bb5\u5de5\u4f5c\u6d41\u7a0bSWIRL\uff0c\u5c06MARL\u91cd\u6784\u4e3a\u4e00\u7cfb\u5217\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u4e00\u6b21\u66f4\u65b0\u4e00\u4e2a\u667a\u80fd\u4f53\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u4ed6\u667a\u80fd\u4f53\u56fa\u5b9a\u3002", "result": "\u5728\u9ad8\u7ea7\u548c\u4f4e\u7ea7GUI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002SWIRL\u8fd8\u5728\u591a\u667a\u80fd\u4f53\u6570\u5b66\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\u3002", "conclusion": "SWIRL\u5728\u79fb\u52a8GUI\u63a7\u5236\u548c\u591a\u667a\u80fd\u4f53\u6570\u5b66\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u5f00\u53d1\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u901a\u7528\u6846\u67b6\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.19481", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19481", "abs": "https://arxiv.org/abs/2508.19481", "authors": ["Manuel Mosquera", "Melissa Robles", "Johan Rodriguez", "Ruben Manrique"], "title": "Improving Low-Resource Translation with Dictionary-Guided Fine-Tuning and RL: A Spanish-to-Wayuunaiki Study", "comment": null, "summary": "Low-resource machine translation remains a significant challenge for large\nlanguage models (LLMs), which often lack exposure to these languages during\npretraining and have limited parallel data for fine-tuning. We propose a novel\napproach that enhances translation for low-resource languages by integrating an\nexternal dictionary tool and training models end-to-end using reinforcement\nlearning, in addition to supervised fine-tuning. Focusing on the\nSpanish-Wayuunaiki language pair, we frame translation as a tool-augmented\ndecision-making problem in which the model can selectively consult a bilingual\ndictionary during generation. Our method combines supervised instruction tuning\nwith Guided Reward Policy Optimization (GRPO), enabling the model to learn both\nwhen and how to use the tool effectively. BLEU similarity scores are used as\nrewards to guide this learning process. Preliminary results show that our\ntool-augmented models achieve up to +3.37 BLEU improvement over previous work,\nand a 18% relative gain compared to a supervised baseline without dictionary\naccess, on the Spanish-Wayuunaiki test set from the AmericasNLP 2025 Shared\nTask. We also conduct ablation studies to assess the effects of model\narchitecture and training strategy, comparing Qwen2.5-0.5B-Instruct with other\nmodels such as LLaMA and a prior NLLB-based system. These findings highlight\nthe promise of combining LLMs with external tools and the role of reinforcement\nlearning in improving translation quality in low-resource language settings.", "AI": {"tldr": "This paper introduces a new method for low-resource machine translation that uses an external dictionary and reinforcement learning to improve translation quality. The method achieves significant improvements in BLEU scores compared to previous work.", "motivation": "Low-resource machine translation remains a significant challenge for large language models (LLMs), which often lack exposure to these languages during pretraining and have limited parallel data for fine-tuning.", "method": "Integrating an external dictionary tool and training models end-to-end using reinforcement learning, in addition to supervised fine-tuning.", "result": "Tool-augmented models achieve up to +3.37 BLEU improvement over previous work, and a 18% relative gain compared to a supervised baseline without dictionary access, on the Spanish-Wayuunaiki test set.", "conclusion": "Combining LLMs with external tools and reinforcement learning improves translation quality in low-resource language settings."}}
{"id": "2508.19485", "categories": ["cs.CV", "68T45 (Primary), 68T07 (Secondary)", "I.2.10; I.4.6"], "pdf": "https://arxiv.org/pdf/2508.19485", "abs": "https://arxiv.org/abs/2508.19485", "authors": ["Xinlong Zhao", "Qixiang Pang", "Shan Du"], "title": "JVLGS: Joint Vision-Language Gas Leak Segmentation", "comment": "19 pages, 13 figures", "summary": "Gas leaks pose serious threats to human health and contribute significantly\nto atmospheric pollution, drawing increasing public concern. However, the lack\nof effective detection methods hampers timely and accurate identification of\ngas leaks. While some vision-based techniques leverage infrared videos for leak\ndetection, the blurry and non-rigid nature of gas clouds often limits their\neffectiveness. To address these challenges, we propose a novel framework called\nJoint Vision-Language Gas leak Segmentation (JVLGS), which integrates the\ncomplementary strengths of visual and textual modalities to enhance gas leak\nrepresentation and segmentation. Recognizing that gas leaks are sporadic and\nmany video frames may contain no leak at all, our method incorporates a\npost-processing step to reduce false positives caused by noise and non-target\nobjects, an issue that affects many existing approaches. Extensive experiments\nconducted across diverse scenarios show that JVLGS significantly outperforms\nstate-of-the-art gas leak segmentation methods. We evaluate our model under\nboth supervised and few-shot learning settings, and it consistently achieves\nstrong performance in both, whereas competing methods tend to perform well in\nonly one setting or poorly in both. Code available at:\nhttps://github.com/GeekEagle/JVLGS", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9-\u8bed\u8a00\u6846\u67b6 JVLGS\uff0c\u7528\u4e8e\u66f4\u6709\u6548\u5730\u5206\u5272\u6c14\u4f53\u6cc4\u6f0f\uff0c\u5e76\u5728\u5404\u79cd\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u7f3a\u4e4f\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u963b\u788d\u4e86\u53ca\u65f6\u51c6\u786e\u5730\u8bc6\u522b\u6c14\u4f53\u6cc4\u6f0f\u3002\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u7684\u6280\u672f\u5229\u7528\u7ea2\u5916\u89c6\u9891\u8fdb\u884c\u6cc4\u6f0f\u68c0\u6d4b\uff0c\u4f46\u6c14\u4f53\u4e91\u7684\u6a21\u7cca\u548c\u975e\u521a\u6027\u6027\u8d28\u901a\u5e38\u9650\u5236\u4e86\u5b83\u4eec\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8054\u5408\u89c6\u89c9-\u8bed\u8a00\u6c14\u4f53\u6cc4\u6f0f\u5206\u5272 (JVLGS) \u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u4ee5\u589e\u5f3a\u6c14\u4f53\u6cc4\u6f0f\u8868\u793a\u548c\u5206\u5272\u3002\u8be5\u65b9\u6cd5\u5305\u542b\u4e00\u4e2a\u540e\u5904\u7406\u6b65\u9aa4\uff0c\u4ee5\u51cf\u5c11\u7531\u566a\u58f0\u548c\u975e\u76ee\u6807\u5bf9\u8c61\u5f15\u8d77\u7684\u5047\u9633\u6027\u3002", "result": "JVLGS \u5728\u5404\u79cd\u573a\u666f\u4e2d\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u6027\u80fd\u663e\u7740\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6c14\u4f53\u6cc4\u6f0f\u5206\u5272\u65b9\u6cd5\u3002\u5728\u76d1\u7763\u548c\u5c11\u6837\u672c\u5b66\u4e60\u73af\u5883\u4e2d\uff0c\u8be5\u6a21\u578b\u59cb\u7ec8\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u3002", "conclusion": "JVLGS\u5728\u76d1\u7763\u548c\u5c11\u6837\u672c\u5b66\u4e60\u73af\u5883\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2508.19419", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19419", "abs": "https://arxiv.org/abs/2508.19419", "authors": ["Harun Ur Rashid", "Aleksandra Pachalieva", "Daniel O'Malley"], "title": "Differentiable multiphase flow model for physics-informed machine learning in reservoir pressure management", "comment": null, "summary": "Accurate subsurface reservoir pressure control is extremely challenging due\nto geological heterogeneity and multiphase fluid-flow dynamics. Predicting\nbehavior in this setting relies on high-fidelity physics-based simulations that\nare computationally expensive. Yet, the uncertain, heterogeneous properties\nthat control these flows make it necessary to perform many of these expensive\nsimulations, which is often prohibitive. To address these challenges, we\nintroduce a physics-informed machine learning workflow that couples a fully\ndifferentiable multiphase flow simulator, which is implemented in the DPFEHM\nframework with a convolutional neural network (CNN). The CNN learns to predict\nfluid extraction rates from heterogeneous permeability fields to enforce\npressure limits at critical reservoir locations. By incorporating transient\nmultiphase flow physics into the training process, our method enables more\npractical and accurate predictions for realistic injection-extraction scenarios\ncompare to previous works. To speed up training, we pretrain the model on\nsingle-phase, steady-state simulations and then fine-tune it on full multiphase\nscenarios, which dramatically reduces the computational cost. We demonstrate\nthat high-accuracy training can be achieved with fewer than three thousand\nfull-physics multiphase flow simulations -- compared to previous estimates\nrequiring up to ten million. This drastic reduction in the number of\nsimulations is achieved by leveraging transfer learning from much less\nexpensive single-phase simulations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u7a0b\uff0c\u8be5\u6d41\u7a0b\u5c06\u53ef\u5fae\u591a\u76f8\u6d41\u6a21\u62df\u5668\u4e0e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u76f8\u7ed3\u5408\uff0c\u4ee5\u66f4\u5c11\u7684\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u50a8\u5c42\u538b\u529b\u63a7\u5236\u7684\u9ad8\u7cbe\u5ea6\u8bad\u7ec3\u3002", "motivation": "\u7531\u4e8e\u5730\u8d28\u975e\u5747\u8d28\u6027\u548c\u591a\u76f8\u6d41\u4f53\u6d41\u52a8\u52a8\u6001\uff0c\u7cbe\u786e\u7684\u5730\u4e0b\u50a8\u5c42\u538b\u529b\u63a7\u5236\u6781\u5177\u6311\u6218\u6027\u3002\u9884\u6d4b\u8fd9\u79cd\u73af\u5883\u4e0b\u7684\u884c\u4e3a\u4f9d\u8d56\u4e8e\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u9ad8\u4fdd\u771f\u7269\u7406\u6a21\u62df\u3002\u7136\u800c\uff0c\u63a7\u5236\u8fd9\u4e9b\u6d41\u7684\u4e0d\u786e\u5b9a\u3001\u975e\u5747\u8d28\u6027\u4f7f\u5f97\u6709\u5fc5\u8981\u8fdb\u884c\u8bb8\u591a\u8fd9\u4e9b\u6602\u8d35\u7684\u6a21\u62df\uff0c\u8fd9\u901a\u5e38\u662f\u4ee4\u4eba\u671b\u800c\u5374\u6b65\u7684\u3002", "method": "\u7ed3\u5408\u4e86\u53ef\u5fae\u591a\u76f8\u6d41\u6a21\u62df\u5668\uff08DPFEHM\u6846\u67b6\uff09\u4e0e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u3002CNN\u5b66\u4e60\u4ece\u975e\u5747\u8d28\u6e17\u900f\u7387\u573a\u9884\u6d4b\u6d41\u4f53\u91c7\u6536\u7387\uff0c\u4ee5\u63a7\u5236\u5173\u952e\u50a8\u5c42\u4f4d\u7f6e\u7684\u538b\u529b\u9650\u5236\u3002\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u52a0\u901f\u8bad\u7ec3\uff0c\u5148\u5728\u5355\u76f8\u7a33\u6001\u6a21\u62df\u4e0a\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u7136\u540e\u5728\u5168\u591a\u76f8\u573a\u666f\u4e2d\u5fae\u8c03\u3002", "result": "\u8be5\u65b9\u6cd5\u4ec5\u7528\u4e0d\u5230\u4e09\u5343\u6b21\u5168\u7269\u7406\u591a\u76f8\u6d41\u6a21\u62df\u5373\u53ef\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8bad\u7ec3\uff0c\u76f8\u6bd4\u4e4b\u524d\u4f30\u8ba1\u7684\u9700\u8981\u9ad8\u8fbe\u4e00\u5343\u4e07\u6b21\u7684\u6a21\u62df\uff0c\u5927\u5927\u51cf\u5c11\u4e86\u6a21\u62df\u6b21\u6570\u3002\u8fd9\u662f\u901a\u8fc7\u5229\u7528\u6765\u81ea\u4e0d\u592a\u6602\u8d35\u7684\u5355\u76f8\u6a21\u62df\u7684\u8fc1\u79fb\u5b66\u4e60\u6765\u5b9e\u73b0\u7684\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u53ef\u5fae\u591a\u76f8\u6d41\u6a21\u62df\u5668\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4ee5\u66f4\u5c11\u7684\u5168\u7269\u7406\u591a\u76f8\u6d41\u6a21\u62df\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002"}}
{"id": "2508.20040", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20040", "abs": "https://arxiv.org/abs/2508.20040", "authors": ["Przemyslaw Biecek", "Wojciech Samek"], "title": "Model Science: getting serious about verification, explanation and control of AI systems", "comment": "8 pages", "summary": "The growing adoption of foundation models calls for a paradigm shift from\nData Science to Model Science. Unlike data-centric approaches, Model Science\nplaces the trained model at the core of analysis, aiming to interact, verify,\nexplain, and control its behavior across diverse operational contexts. This\npaper introduces a conceptual framework for a new discipline called Model\nScience, along with the proposal for its four key pillars: Verification, which\nrequires strict, context-aware evaluation protocols; Explanation, which is\nunderstood as various approaches to explore of internal model operations;\nControl, which integrates alignment techniques to steer model behavior; and\nInterface, which develops interactive and visual explanation tools to improve\nhuman calibration and decision-making. The proposed framework aims to guide the\ndevelopment of credible, safe, and human-aligned AI systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u578b\u79d1\u5b66\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u5305\u62ec\u9a8c\u8bc1\u3001\u89e3\u91ca\u3001\u63a7\u5236\u548c\u754c\u9762\u56db\u4e2a\u5173\u952e\u652f\u67f1\uff0c\u65e8\u5728\u5f00\u53d1\u53ef\u4fe1\u3001\u5b89\u5168\u548c\u4ee5\u4eba\u4e3a\u672c\u7684AI\u7cfb\u7edf\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u7684\u65e5\u76ca\u666e\u53ca\u8981\u6c42\u5c06\u8303\u5f0f\u4ece\u6570\u636e\u79d1\u5b66\u8f6c\u53d8\u4e3a\u6a21\u578b\u79d1\u5b66\u3002\u4e0e\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u6a21\u578b\u79d1\u5b66\u5c06\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u7f6e\u4e8e\u5206\u6790\u7684\u6838\u5fc3\uff0c\u65e8\u5728\u8de8\u4e0d\u540c\u7684\u64cd\u4f5c\u73af\u5883\u4ea4\u4e92\u3001\u9a8c\u8bc1\u3001\u89e3\u91ca\u548c\u63a7\u5236\u5176\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4e86\u6a21\u578b\u79d1\u5b66\u7684\u56db\u4e2a\u5173\u952e\u652f\u67f1\uff1a\u9a8c\u8bc1\u3001\u89e3\u91ca\u3001\u63a7\u5236\u548c\u754c\u9762\u3002", "result": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3a\u6a21\u578b\u79d1\u5b66\u7684\u65b0\u5b66\u79d1\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u4ee5\u53ca\u5bf9\u5176\u56db\u4e2a\u5173\u952e\u652f\u67f1\u7684\u63d0\u8bae\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u578b\u79d1\u5b66\u7684\u6846\u67b6\uff0c\u4ee5\u6307\u5bfc\u53ef\u4fe1\u3001\u5b89\u5168\u548c\u4ee5\u4eba\u4e3a\u672c\u7684AI\u7cfb\u7edf\u7684\u5f00\u53d1\u3002"}}
{"id": "2508.19484", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19484", "abs": "https://arxiv.org/abs/2508.19484", "authors": ["Bahar Bateni", "Benjamin Pratt", "Jim Whitehead"], "title": "Rule Synergy Analysis using LLMs: State of the Art and Implications", "comment": "Submitted for publication at the IEEE Transactions on Games 2024,\n  Special Issue on Large Language Models and Games (10 pages excluding\n  appendix, 3 figures)", "summary": "Large language models (LLMs) have demonstrated strong performance across a\nvariety of domains, including logical reasoning, mathematics, and more. In this\npaper, we investigate how well LLMs understand and reason about complex rule\ninteractions in dynamic environments, such as card games. We introduce a\ndataset of card synergies from the game Slay the Spire, where pairs of cards\nare classified based on their positive, negative, or neutral interactions. Our\nevaluation shows that while LLMs excel at identifying non-synergistic pairs,\nthey struggle with detecting positive and, particularly, negative synergies. We\ncategorize common error types, including issues with timing, defining game\nstates, and following game rules. Our findings suggest directions for future\nresearch to improve model performance in predicting the effect of rules and\ntheir interactions.", "AI": {"tldr": "LLMs are tested on card game synergies. They're okay at identifying non-synergistic pairs, but bad at identifying positive and negative synergies.", "motivation": "Investigate how well LLMs understand and reason about complex rule interactions in dynamic environments like card games.", "method": "Introduce a dataset of card synergies from Slay the Spire, classifying card pairs based on their interactions.", "result": "LLMs excel at identifying non-synergistic pairs but struggle with positive and negative synergies. Common errors include issues with timing, defining game states, and following game rules.", "conclusion": "LLMs struggle with detecting positive and negative synergies, especially negative ones."}}
{"id": "2508.19498", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19498", "abs": "https://arxiv.org/abs/2508.19498", "authors": ["Yimu Wang", "Weiming Zhuang", "Chen Chen", "Jiabo Huang", "Jingtao Li", "Lingjuan Lyu"], "title": "UNIFORM: Unifying Knowledge from Large-scale and Diverse Pre-trained Models", "comment": null, "summary": "In the era of deep learning, the increasing number of pre-trained models\navailable online presents a wealth of knowledge. These models, developed with\ndiverse architectures and trained on varied datasets for different tasks,\nprovide unique interpretations of the real world. Their collective consensus is\nlikely universal and generalizable to unseen data. However, effectively\nharnessing this collective knowledge poses a fundamental challenge due to the\nheterogeneity of pre-trained models. Existing knowledge integration solutions\ntypically rely on strong assumptions about training data distributions and\nnetwork architectures, limiting them to learning only from specific types of\nmodels and resulting in data and/or inductive biases. In this work, we\nintroduce a novel framework, namely UNIFORM, for knowledge transfer from a\ndiverse set of off-the-shelf models into one student model without such\nconstraints. Specifically, we propose a dedicated voting mechanism to capture\nthe consensus of knowledge both at the logit level -- incorporating teacher\nmodels that are capable of predicting target classes of interest -- and at the\nfeature level, utilizing visual representations learned on arbitrary label\nspaces. Extensive experiments demonstrate that UNIFORM effectively enhances\nunsupervised object recognition performance compared to strong knowledge\ntransfer baselines. Notably, it exhibits remarkable scalability by benefiting\nfrom over one hundred teachers, while existing methods saturate at a much\nsmaller scale.", "AI": {"tldr": "\u63d0\u51faUNIFORM\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u4e0d\u540c\u7684\u73b0\u6210\u6a21\u578b\u4e2d\u8fdb\u884c\u77e5\u8bc6\u8f6c\u79fb\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u77e5\u8bc6\u96c6\u6210\u89e3\u51b3\u65b9\u6848\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u65e0\u76d1\u7763\u5bf9\u8c61\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5f02\u6784\u6027\uff0c\u5982\u4f55\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u6a21\u578b\u7684\u96c6\u4f53\u77e5\u8bc6\u662f\u4e00\u4e2a\u6839\u672c\u6027\u7684\u6311\u6218\u3002\u73b0\u6709\u7684\u77e5\u8bc6\u96c6\u6210\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u4f9d\u8d56\u4e8e\u5173\u4e8e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u548c\u7f51\u7edc\u67b6\u6784\u7684\u5f3a\u5047\u8bbe\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u53ea\u80fd\u4ece\u7279\u5b9a\u7c7b\u578b\u7684\u6a21\u578b\u4e2d\u5b66\u4e60\uff0c\u5e76\u5bfc\u81f4\u6570\u636e\u548c/\u6216\u5f52\u7eb3\u504f\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684\u6295\u7968\u673a\u5236\uff0c\u4ee5\u6355\u83b7logit\u7ea7\u522b\u548c\u7279\u5f81\u7ea7\u522b\u7684\u77e5\u8bc6\u5171\u8bc6\u3002", "result": "UNIFORM\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u65e0\u76d1\u7763\u5bf9\u8c61\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u53d7\u76ca\u4e8e\u8d85\u8fc7\u4e00\u767e\u4e2a\u6559\u5e08\u6a21\u578b\u6765\u5c55\u793a\u5353\u8d8a\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "UNIFORM\u901a\u8fc7\u5229\u7528\u8d85\u8fc7\u4e00\u767e\u4e2a\u6559\u5e08\u6a21\u578b\uff0c\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u65e0\u76d1\u7763\u5bf9\u8c61\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.19424", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19424", "abs": "https://arxiv.org/abs/2508.19424", "authors": ["Yifan Dou", "Adam Khadre", "Ruben C Petreaca", "Golrokh Mirzaei"], "title": "MS-ConTab: Multi-Scale Contrastive Learning of Mutation Signatures for Pan Cancer Representation and Stratification", "comment": null, "summary": "Motivation. Understanding the pan-cancer mutational landscape offers critical\ninsights into the molecular mechanisms underlying tumorigenesis. While\npatient-level machine learning techniques have been widely employed to identify\ntumor subtypes, cohort-level clustering, where entire cancer types are grouped\nbased on shared molecular features, has largely relied on classical statistical\nmethods.\n  Results. In this study, we introduce a novel unsupervised contrastive\nlearning framework to cluster 43 cancer types based on coding mutation data\nderived from the COSMIC database. For each cancer type, we construct two\ncomplementary mutation signatures: a gene-level profile capturing nucleotide\nsubstitution patterns across the most frequently mutated genes, and a\nchromosome-level profile representing normalized substitution frequencies\nacross chromosomes. These dual views are encoded using TabNet encoders and\noptimized via a multi-scale contrastive learning objective (NT-Xent loss) to\nlearn unified cancer-type embeddings. We demonstrate that the resulting latent\nrepresentations yield biologically meaningful clusters of cancer types,\naligning with known mutational processes and tissue origins. Our work\nrepresents the first application of contrastive learning to cohort-level cancer\nclustering, offering a scalable and interpretable framework for mutation-driven\ncancer subtyping.", "AI": {"tldr": "Contrastive learning is used to cluster 43 cancer types, revealing meaningful relationships based on mutation data.", "motivation": "Understanding the pan-cancer mutational landscape offers insights into tumorigenesis. Cohort-level clustering has largely relied on classical statistical methods.", "method": "A novel unsupervised contrastive learning framework is used to cluster cancer types based on coding mutation data. Dual views (gene-level and chromosome-level profiles) are encoded using TabNet encoders and optimized via a multi-scale contrastive learning objective (NT-Xent loss).", "result": "The latent representations yield biologically meaningful clusters of cancer types, aligning with known mutational processes and tissue origins.", "conclusion": "This study introduces a contrastive learning framework for clustering 43 cancer types, revealing biologically meaningful clusters that align with known mutational processes and tissue origins. It represents the first application of contrastive learning to cohort-level cancer clustering."}}
{"id": "2508.19026", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19026", "abs": "https://arxiv.org/abs/2508.19026", "authors": ["Gueter Josmy Faure", "Min-Hung Chen", "Jia-Fong Yeh", "Ying Cheng", "Hung-Ting Su", "Yung-Hao Tang", "Shang-Hong Lai", "Winston H. Hsu"], "title": "MovieCORE: COgnitive REasoning in Movies", "comment": "Accepted for EMNLP'2025 Main Conference. Project Page:\n  https://joslefaure.github.io/assets/html/moviecore.html", "summary": "This paper introduces MovieCORE, a novel video question answering (VQA)\ndataset designed to probe deeper cognitive understanding of movie content.\nUnlike existing datasets that focus on surface-level comprehension, MovieCORE\nemphasizes questions that engage System-2 thinking while remaining specific to\nthe video material. We present an innovative agentic brainstorming approach,\nutilizing multiple large language models (LLMs) as thought agents to generate\nand refine high-quality question-answer pairs. To evaluate dataset quality, we\ndevelop a set of cognitive tests assessing depth, thought-provocation\npotential, and syntactic complexity. We also propose a comprehensive evaluation\nscheme for assessing VQA model performance on deeper cognitive tasks. To\naddress the limitations of existing video-language models (VLMs), we introduce\nan agentic enhancement module, Agentic Choice Enhancement (ACE), which improves\nmodel reasoning capabilities post-training by up to 25%. Our work contributes\nto advancing movie understanding in AI systems and provides valuable insights\ninto the capabilities and limitations of current VQA models when faced with\nmore challenging, nuanced questions about cinematic content. Our project page,\ndataset and code can be found at\nhttps://joslefaure.github.io/assets/html/moviecore.html.", "AI": {"tldr": "MovieCORE: A new VQA dataset for deeper movie understanding, featuring challenging questions and an agentic enhancement module (ACE) that improves VQA model reasoning.", "motivation": "Existing VQA datasets lack depth and don't engage System-2 thinking for movie content understanding.", "method": "The paper introduces MovieCORE dataset using agentic brainstorming with LLMs and an agentic enhancement module (ACE) to improve model reasoning.", "result": "The ACE module improves model reasoning capabilities by up to 25%.", "conclusion": "This work advances movie understanding in AI and reveals limitations of current VQA models on challenging cinematic content questions."}}
{"id": "2508.19529", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19529", "abs": "https://arxiv.org/abs/2508.19529", "authors": ["Bowen Sun", "Yujun Cai", "Ming-Hsuan Yang", "Yiwei Wang"], "title": "Blockwise SFT for Diffusion Language Models: Reconciling Bidirectional Attention and Autoregressive Decoding", "comment": null, "summary": "Discrete diffusion language models have shown strong potential for text\ngeneration, yet standard supervised fine-tuning (SFT) misaligns with their\nsemi-autoregressive inference: training randomly masks tokens across the entire\nresponse, while inference generates fixed-size blocks sequentially. This\nmismatch introduces noisy prefixes and leaky suffixes, biasing gradients away\nfrom the desired blockwise likelihood. We propose Blockwise SFT, which\npartitions responses into fixed-size blocks, selects one active block per step\nfor stochastic masking, freezes all preceding tokens, and fully hides future\nones. Loss is computed only over the active block, directly mirroring the\nblockwise decoding process. Experiments on GSM8K, MATH, and MetaMathQA show\nconsistent gains over classical SFT under equal compute or token budgets. Block\nsize consistency studies and ablations confirm that improvements stem from\nfaithful training-inference alignment rather than incidental masking effects.\nOur results highlight the importance of matching supervision granularity to the\ndecoding procedure in diffusion-based language models.", "AI": {"tldr": "Blockwise SFT\u901a\u8fc7\u5bf9\u9f50\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u4e86diffusion-based\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u6807\u51c6\u7684\u76d1\u7763\u5fae\u8c03(SFT)\u4e0e\u79bb\u6563\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u534a\u81ea\u56de\u5f52\u63a8\u7406\u4e0d\u4e00\u81f4\uff1a\u8bad\u7ec3\u968f\u673amask\u6574\u4e2a\u54cd\u5e94\u4e2d\u7684tokens\uff0c\u800c\u63a8\u7406\u5219\u6309\u987a\u5e8f\u751f\u6210\u56fa\u5b9a\u5927\u5c0f\u7684\u5757\u3002", "method": "\u63d0\u51fa\u4e86Blockwise SFT\uff0c\u5b83\u5c06\u54cd\u5e94\u5206\u6210\u56fa\u5b9a\u5927\u5c0f\u7684\u5757\uff0c\u5e76\u9009\u62e9\u4e00\u4e2a\u5757\u8fdb\u884c\u968f\u673a\u63a9\u7801\u3002", "result": "\u5728GSM8K\u3001MATH\u548cMetaMathQA\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u76f8\u540c\u7684\u8ba1\u7b97\u6216token\u9884\u7b97\u4e0b\uff0cBlockwise SFT\u6bd4\u4f20\u7edf\u7684SFT\u6709\u6301\u7eed\u7684\u6536\u76ca\u3002", "conclusion": "Blockwise SFT\u80fd\u63d0\u5347diffusion-based\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u56e0\u4e3a\u5b83\u66f4\u5fe0\u5b9e\u5730\u5bf9\u9f50\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u3002"}}
{"id": "2508.19499", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19499", "abs": "https://arxiv.org/abs/2508.19499", "authors": ["Xiangxu Wang", "Tianhong Zhao", "Wei Tu", "Bowen Zhang", "Guanzhou Chen", "Jinzhou Cao"], "title": "Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery", "comment": null, "summary": "Origin-Destination (OD) flow matrices are essential for urban mobility\nanalysis, underpinning applications in traffic forecasting, infrastructure\nplanning, and policy design. However, existing methods suffer from two critical\nlimitations: (1) reliance on auxiliary features (e.g., Points of Interest,\nsocioeconomic statistics) that are costly to collect and have limited spatial\ncoverage; and (2) sensitivity to spatial topology, where minor index reordering\nof urban regions (e.g., census tract relabeling) disrupts structural coherence\nin generated flows. To address these challenges, we propose Sat2Flow, a latent\nstructure-aware diffusion-based framework that generates structurally coherent\nOD flows using solely satellite imagery as input. Our approach introduces a\nmulti-kernel encoder to capture diverse regional interactions and employs a\npermutation-aware diffusion process that aligns latent representations across\ndifferent regional orderings. Through a joint contrastive training objective\nthat bridges satellite-derived features with OD patterns, combined with\nequivariant diffusion training that enforces structural consistency, Sat2Flow\nensures topological robustness under arbitrary regional reindexing.\nExperimental results on real-world urban datasets demonstrate that Sat2Flow\noutperforms both physics-based and data-driven baselines in numerical accuracy\nwhile preserving empirical distributions and spatial structures under index\npermutations. Sat2Flow offers a globally scalable solution for OD flow\ngeneration in data-scarce urban environments, eliminating region-specific\nauxiliary data dependencies while maintaining structural invariance for robust\nmobility modeling.", "AI": {"tldr": "Sat2Flow\u4f7f\u7528\u536b\u661f\u56fe\u50cf\u751f\u6210OD\u6d41\uff0c\u65e0\u9700\u8f85\u52a9\u6570\u636e\uff0c\u4e14\u5bf9\u533a\u57df\u91cd\u65b0\u7d22\u5f15\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u8f85\u52a9\u7279\u5f81\u4e14\u5bf9\u7a7a\u95f4\u62d3\u6251\u654f\u611f\u3002", "method": "Sat2Flow\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6f5c\u5728\u7ed3\u6784\u611f\u77e5\u6269\u6563\u7684\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u536b\u661f\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\u6765\u751f\u6210\u7ed3\u6784\u8fde\u8d2f\u7684OD\u6d41\u3002", "result": "Sat2Flow\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5e76\u5728\u6570\u636e\u7a00\u7f3a\u7684\u57ce\u5e02\u73af\u5883\u4e2d\u4e3aOD\u6d41\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u5c40\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "Sat2Flow\u5728\u6570\u503c\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u7269\u7406\u548c\u6570\u636e\u9a71\u52a8\u7684\u57fa\u7ebf\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u7ecf\u9a8c\u5206\u5e03\u548c\u7a7a\u95f4\u7ed3\u6784\u3002"}}
{"id": "2508.19441", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.19441", "abs": "https://arxiv.org/abs/2508.19441", "authors": ["Sanket Jantre", "Deepak Akhare", "Xiaoning Qian", "Nathan M. Urban"], "title": "Data-Augmented Few-Shot Neural Stencil Emulation for System Identification of Computer Models", "comment": null, "summary": "Partial differential equations (PDEs) underpin the modeling of many natural\nand engineered systems. It can be convenient to express such models as neural\nPDEs rather than using traditional numerical PDE solvers by replacing part or\nall of the PDE's governing equations with a neural network representation.\nNeural PDEs are often easier to differentiate, linearize, reduce, or use for\nuncertainty quantification than the original numerical solver. They are usually\ntrained on solution trajectories obtained by long time integration of the PDE\nsolver. Here we propose a more sample-efficient data-augmentation strategy for\ngenerating neural PDE training data from a computer model by space-filling\nsampling of local \"stencil\" states. This approach removes a large degree of\nspatiotemporal redundancy present in trajectory data and oversamples states\nthat may be rarely visited but help the neural PDE generalize across the state\nspace. We demonstrate that accurate neural PDE stencil operators can be learned\nfrom synthetic training data generated by the computational equivalent of 10\ntimesteps' worth of numerical simulation. Accuracy is further improved if we\nassume access to a single full-trajectory simulation from the computer model,\nwhich is typically available in practice. Across several PDE systems, we show\nthat our data-augmented synthetic stencil data yield better trained neural\nstencil operators, with clear performance gains compared with naively sampled\nstencil data from simulation trajectories.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u751f\u6210\u795e\u7ecf PDE \u8bad\u7ec3\u6570\u636e\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u901a\u8fc7\u7a7a\u95f4\u586b\u5145\u91c7\u6837\u5c40\u90e8\u201c\u6a21\u7248\u201d\u72b6\u6001\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\uff0c\u5e76\u4e14\u5728\u591a\u4e2a PDE \u7cfb\u7edf\u4e2d\uff0c\u4e0e\u4ece\u6a21\u62df\u8f68\u8ff9\u4e2d\u7b80\u5355\u91c7\u6837\u7684\u6a21\u7248\u6570\u636e\u76f8\u6bd4\uff0c\u6027\u80fd\u6709\u660e\u663e\u7684\u63d0\u5347\u3002", "motivation": "\u795e\u7ecf\u504f\u5fae\u5206\u65b9\u7a0b (PDE) \u901a\u5e38\u6bd4\u4f20\u7edf\u7684\u6570\u503c PDE \u6c42\u89e3\u5668\u66f4\u5bb9\u6613\u533a\u5206\u3001\u7ebf\u6027\u5316\u3001\u7b80\u5316\u6216\u7528\u4e8e\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002\u5b83\u4eec\u901a\u5e38\u5728\u901a\u8fc7\u957f\u65f6\u95f4\u79ef\u5206 PDE \u6c42\u89e3\u5668\u83b7\u5f97\u7684\u89e3\u8f68\u8ff9\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u81f4\u529b\u4e8e\u63d0\u9ad8\u795e\u7ecf PDE \u7684\u6837\u672c\u6548\u7387\u3002", "method": "\u901a\u8fc7\u5bf9\u5c40\u90e8\u201c\u6a21\u7248\u201d\u72b6\u6001\u8fdb\u884c\u7a7a\u95f4\u586b\u5145\u91c7\u6837\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u5177\u6837\u672c\u6548\u7387\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u7528\u4e8e\u4ece\u8ba1\u7b97\u673a\u6a21\u578b\u751f\u6210\u795e\u7ecf PDE \u8bad\u7ec3\u6570\u636e\u3002", "result": "\u4ec5\u4f7f\u7528\u76f8\u5f53\u4e8e 10 \u4e2a\u65f6\u95f4\u6b65\u957f\u7684\u6570\u503c\u6a21\u62df\u7684\u8ba1\u7b97\u80fd\u529b\uff0c\u5373\u53ef\u5b66\u4e60\u5230\u7cbe\u786e\u7684\u795e\u7ecf PDE \u6a21\u7248\u7b97\u5b50\u3002\u5982\u679c\u6211\u4eec\u5047\u8bbe\u53ef\u4ee5\u4ece\u8ba1\u7b97\u673a\u6a21\u578b\u8bbf\u95ee\u5355\u4e2a\u5b8c\u6574\u8f68\u8ff9\u6a21\u62df\uff08\u8fd9\u5728\u5b9e\u8df5\u4e2d\u901a\u5e38\u662f\u53ef\u884c\u7684\uff09\uff0c\u5219\u7cbe\u5ea6\u4f1a\u8fdb\u4e00\u6b65\u63d0\u9ad8\u3002", "conclusion": "\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u7684\u5408\u6210\u6a21\u7248\u6570\u636e\u53ef\u4ee5\u8bad\u7ec3\u51fa\u66f4\u597d\u7684\u795e\u7ecf\u6a21\u7248\u7b97\u5b50\uff0c\u4e0e\u4ece\u6a21\u62df\u8f68\u8ff9\u4e2d\u7b80\u5355\u91c7\u6837\u7684\u6a21\u7248\u6570\u636e\u76f8\u6bd4\uff0c\u6027\u80fd\u6709\u660e\u663e\u7684\u63d0\u5347\u3002"}}
{"id": "2508.19532", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19532", "abs": "https://arxiv.org/abs/2508.19532", "authors": ["Houxing Ren", "Zimu Lu", "Weikang Shi", "Haotian Hou", "Yunqiao Yang", "Ke Wang", "Aojun Zhou", "Junting Pan", "Mingjie Zhan", "Hongsheng Li"], "title": "Alignment with Fill-In-the-Middle for Enhancing Code Generation", "comment": "Accepted to EMNLP 2025 (main conference)", "summary": "The code generation capabilities of Large Language Models (LLMs) have\nadvanced applications like tool invocation and problem-solving. However,\nimproving performance in code-related tasks remains challenging due to limited\ntraining data that is verifiable with accurate test cases. While Direct\nPreference Optimization (DPO) has shown promise, existing methods for\ngenerating test cases still face limitations. In this paper, we propose a novel\napproach that splits code snippets into smaller, granular blocks, creating more\ndiverse DPO pairs from the same test cases. Additionally, we introduce the\nAbstract Syntax Tree (AST) splitting and curriculum training method to enhance\nthe DPO training. Our approach demonstrates significant improvements in code\ngeneration tasks, as validated by experiments on benchmark datasets such as\nHumanEval (+), MBPP (+), APPS, LiveCodeBench, and BigCodeBench. Code and data\nare available at https://github.com/SenseLLM/StructureCoder.", "AI": {"tldr": "A new method using granular code splitting and AST with curriculum training improves code generation performance through enhanced DPO.", "motivation": "Improving performance in code-related tasks remains challenging due to limited training data that is verifiable with accurate test cases. Existing methods for generating test cases still face limitations.", "method": "Splits code snippets into smaller, granular blocks, uses Abstract Syntax Tree (AST) splitting, and applies a curriculum training method to enhance Direct Preference Optimization (DPO).", "result": "Demonstrates significant improvements in code generation tasks, as validated by experiments on benchmark datasets such as HumanEval (+), MBPP (+), APPS, LiveCodeBench, and BigCodeBench.", "conclusion": "This paper introduces a novel approach to improve code generation tasks by splitting code snippets into smaller, granular blocks and using AST splitting with curriculum training to enhance DPO training. The approach demonstrates significant improvements on benchmark datasets."}}
{"id": "2508.19511", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19511", "abs": "https://arxiv.org/abs/2508.19511", "authors": ["Alzayat Saleh", "Shunsuke Hatano", "Mostafa Rahimi Azghadi"], "title": "Weed Detection in Challenging Field Conditions: A Semi-Supervised Framework for Overcoming Shadow Bias and Data Scarcity", "comment": "19 pages, 10 figures, 6 tables", "summary": "The automated management of invasive weeds is critical for sustainable\nagriculture, yet the performance of deep learning models in real-world fields\nis often compromised by two factors: challenging environmental conditions and\nthe high cost of data annotation. This study tackles both issues through a\ndiagnostic-driven, semi-supervised framework. Using a unique dataset of\napproximately 975 labeled and 10,000 unlabeled images of Guinea Grass in\nsugarcane, we first establish strong supervised baselines for classification\n(ResNet) and detection (YOLO, RF-DETR), achieving F1 scores up to 0.90 and\nmAP50 scores exceeding 0.82. Crucially, this foundational analysis, aided by\ninterpretability tools, uncovered a pervasive \"shadow bias,\" where models\nlearned to misidentify shadows as vegetation. This diagnostic insight motivated\nour primary contribution: a semi-supervised pipeline that leverages unlabeled\ndata to enhance model robustness. By training models on a more diverse set of\nvisual information through pseudo-labeling, this framework not only helps\nmitigate the shadow bias but also provides a tangible boost in recall, a\ncritical metric for minimizing weed escapes in automated spraying systems. To\nvalidate our methodology, we demonstrate its effectiveness in a low-data regime\non a public crop-weed benchmark. Our work provides a clear and field-tested\nframework for developing, diagnosing, and improving robust computer vision\nsystems for the complex realities of precision agriculture.", "AI": {"tldr": "Addresses weed identification in agriculture by using semi-supervised learning to overcome environmental challenges and reduce annotation needs, improving model robustness and recall.", "motivation": "The automated management of invasive weeds is critical, but deep learning models struggle in real-world fields due to challenging conditions and high annotation costs.  Models also learn to misidentify shadows as vegetation.", "method": "a diagnostic-driven, semi-supervised framework using ResNet, YOLO, and RF-DETR models and pseudo-labeling.", "result": "Achieved F1 scores up to 0.90 and mAP50 scores exceeding 0.82. Mitigated shadow bias and boosted recall through semi-supervised learning. Demonstrated effectiveness in a low-data regime on a public crop-weed benchmark.", "conclusion": "This work provides a clear and field-tested framework for developing, diagnosing, and improving robust computer vision systems for precision agriculture."}}
{"id": "2508.19443", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19443", "abs": "https://arxiv.org/abs/2508.19443", "authors": ["Paimon Goulart", "Shaan Pakala", "Evangelos Papalexakis"], "title": "Efficiently Generating Multidimensional Calorimeter Data with Tensor Decomposition Parameterization", "comment": null, "summary": "Producing large complex simulation datasets can often be a time and resource\nconsuming task. Especially when these experiments are very expensive, it is\nbecoming more reasonable to generate synthetic data for downstream tasks.\nRecently, these methods may include using generative machine learning models\nsuch as Generative Adversarial Networks or diffusion models. As these\ngenerative models improve efficiency in producing useful data, we introduce an\ninternal tensor decomposition to these generative models to even further reduce\ncosts. More specifically, for multidimensional data, or tensors, we generate\nthe smaller tensor factors instead of the full tensor, in order to\nsignificantly reduce the model's output and overall parameters. This reduces\nthe costs of generating complex simulation data, and our experiments show the\ngenerated data remains useful. As a result, tensor decomposition has the\npotential to improve efficiency in generative models, especially when\ngenerating multidimensional data, or tensors.", "AI": {"tldr": "introduce an internal tensor decomposition to generative models to reduce costs of generating complex simulation data", "motivation": "Producing large complex simulation datasets can often be a time and resource consuming task. Especially when these experiments are very expensive, it is becoming more reasonable to generate synthetic data for downstream tasks.", "method": "an internal tensor decomposition to these generative models", "result": "the generated data remains useful", "conclusion": "tensor decomposition has the potential to improve efficiency in generative models, especially when generating multidimensional data, or tensors."}}
{"id": "2508.19533", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19533", "abs": "https://arxiv.org/abs/2508.19533", "authors": ["Kun Peng", "Cong Cao", "Hao Peng", "Guanlin Wu", "Zhifeng Hao", "Lei Jiang", "Yanbing Liu", "Philip S. Yu"], "title": "Emotion Transfer with Enhanced Prototype for Unseen Emotion Recognition in Conversation", "comment": "Accepted at EMNLP2025", "summary": "Current Emotion Recognition in Conversation (ERC) research follows a\nclosed-domain assumption. However, there is no clear consensus on emotion\nclassification in psychology, which presents a challenge for models when it\ncomes to recognizing previously unseen emotions in real-world applications. To\nbridge this gap, we introduce the Unseen Emotion Recognition in Conversation\n(UERC) task for the first time and propose ProEmoTrans, a solid prototype-based\nemotion transfer framework. This prototype-based approach shows promise but\nstill faces key challenges: First, implicit expressions complicate emotion\ndefinition, which we address by proposing an LLM-enhanced description approach.\nSecond, utterance encoding in long conversations is difficult, which we tackle\nwith a proposed parameter-free mechanism for efficient encoding and overfitting\nprevention. Finally, the Markovian flow nature of emotions is hard to transfer,\nwhich we address with an improved Attention Viterbi Decoding (AVD) method to\ntransfer seen emotion transitions to unseen emotions. Extensive experiments on\nthree datasets show that our method serves as a strong baseline for preliminary\nexploration in this new area.", "AI": {"tldr": "Introduces the Unseen Emotion Recognition in Conversation (UERC) task and proposes ProEmoTrans, a prototype-based emotion transfer framework, to address the challenge of recognizing unseen emotions in real-world conversations.", "motivation": "Current Emotion Recognition in Conversation (ERC) research follows a closed-domain assumption, which presents a challenge for models when it comes to recognizing previously unseen emotions in real-world applications due to the lack of clear consensus on emotion classification in psychology.", "method": "ProEmoTrans, a prototype-based emotion transfer framework, with an LLM-enhanced description approach, a parameter-free mechanism for efficient encoding and overfitting prevention, and an improved Attention Viterbi Decoding (AVD) method.", "result": "Extensive experiments on three datasets show the effectiveness of the proposed method.", "conclusion": "The proposed method serves as a strong baseline for preliminary exploration in the new area of Unseen Emotion Recognition in Conversation (UERC)."}}
{"id": "2508.19527", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19527", "abs": "https://arxiv.org/abs/2508.19527", "authors": ["Zhiting Gao", "Dan Song", "Diqiong Jiang", "Chao Xue", "An-An Liu"], "title": "MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment", "comment": "11 pages, 5 figures", "summary": "Motion generation is essential for animating virtual characters and embodied\nagents. While recent text-driven methods have made significant strides, they\noften struggle with achieving precise alignment between linguistic descriptions\nand motion semantics, as well as with the inefficiencies of slow, multi-step\ninference. To address these issues, we introduce TMR++ Aligned Preference\nOptimization (TAPO), an innovative framework that aligns subtle motion\nvariations with textual modifiers and incorporates iterative adjustments to\nreinforce semantic grounding. To further enable real-time synthesis, we propose\nMotionFLUX, a high-speed generation framework based on deterministic rectified\nflow matching. Unlike traditional diffusion models, which require hundreds of\ndenoising steps, MotionFLUX constructs optimal transport paths between noise\ndistributions and motion spaces, facilitating real-time synthesis. The\nlinearized probability paths reduce the need for multi-step sampling typical of\nsequential methods, significantly accelerating inference time without\nsacrificing motion quality. Experimental results demonstrate that, together,\nTAPO and MotionFLUX form a unified system that outperforms state-of-the-art\napproaches in both semantic consistency and motion quality, while also\naccelerating generation speed. The code and pretrained models will be released.", "AI": {"tldr": "TAPO and MotionFLUX improve text-driven motion generation by enhancing semantic alignment and enabling real-time synthesis.", "motivation": "Existing text-driven motion generation methods struggle with precise alignment between linguistic descriptions and motion semantics, and suffer from slow, multi-step inference.", "method": "TAPO aligns subtle motion variations with textual modifiers and incorporates iterative adjustments to reinforce semantic grounding. MotionFLUX constructs optimal transport paths between noise distributions and motion spaces, facilitating real-time synthesis using deterministic rectified flow matching.", "result": "TAPO and MotionFLUX outperform state-of-the-art approaches in semantic consistency, motion quality, and generation speed.", "conclusion": "TAPO and MotionFLUX form a unified system that outperforms state-of-the-art approaches in both semantic consistency and motion quality, while also accelerating generation speed."}}
{"id": "2508.19445", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.19445", "abs": "https://arxiv.org/abs/2508.19445", "authors": ["Haozhe Jiang", "Nika Haghtalab"], "title": "On Surjectivity of Neural Networks: Can you elicit any behavior from your model?", "comment": null, "summary": "Given a trained neural network, can any specified output be generated by some\ninput? Equivalently, does the network correspond to a function that is\nsurjective? In generative models, surjectivity implies that any output,\nincluding harmful or undesirable content, can in principle be generated by the\nnetworks, raising concerns about model safety and jailbreak vulnerabilities. In\nthis paper, we prove that many fundamental building blocks of modern neural\narchitectures, such as networks with pre-layer normalization and\nlinear-attention modules, are almost always surjective. As corollaries, widely\nused generative frameworks, including GPT-style transformers and diffusion\nmodels with deterministic ODE solvers, admit inverse mappings for arbitrary\noutputs. By studying surjectivity of these modern and commonly used neural\narchitectures, we contribute a formalism that sheds light on their unavoidable\nvulnerability to a broad class of adversarial attacks.", "AI": {"tldr": "Modern neural networks are almost always surjective, meaning they can generate any output, which raises concerns about model safety and vulnerability to adversarial attacks.", "motivation": "Surjectivity implies that any output, including harmful or undesirable content, can in principle be generated by the networks, raising concerns about model safety and jailbreak vulnerabilities.", "method": "proving that many fundamental building blocks of modern neural architectures, such as networks with pre-layer normalization and linear-attention modules, are almost always surjective.", "result": "many fundamental building blocks of modern neural architectures, such as networks with pre-layer normalization and linear-attention modules, are almost always surjective.", "conclusion": "Widely used generative frameworks, including GPT-style transformers and diffusion models with deterministic ODE solvers, admit inverse mappings for arbitrary outputs."}}
{"id": "2508.19546", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19546", "abs": "https://arxiv.org/abs/2508.19546", "authors": ["Jio Choi", "Mohit Bansal", "Elias Stengel-Eskin"], "title": "Language Models Identify Ambiguities and Exploit Loopholes", "comment": "EMNLP 2025 camera-ready; Code:\n  https://github.com/esteng/ambiguous-loophole-exploitation", "summary": "Studying the responses of large language models (LLMs) to loopholes presents\na two-fold opportunity. First, it affords us a lens through which to examine\nambiguity and pragmatics in LLMs, since exploiting a loophole requires\nidentifying ambiguity and performing sophisticated pragmatic reasoning. Second,\nloopholes pose an interesting and novel alignment problem where the model is\npresented with conflicting goals and can exploit ambiguities to its own\nadvantage. To address these questions, we design scenarios where LLMs are given\na goal and an ambiguous user instruction in conflict with the goal, with\nscenarios covering scalar implicature, structural ambiguities, and power\ndynamics. We then measure different models' abilities to exploit loopholes to\nsatisfy their given goals as opposed to the goals of the user. We find that\nboth closed-source and stronger open-source models can identify ambiguities and\nexploit their resulting loopholes, presenting a potential AI safety risk. Our\nanalysis indicates that models which exploit loopholes explicitly identify and\nreason about both ambiguity and conflicting goals.", "AI": {"tldr": "llm\u53ef\u4ee5\u8bc6\u522b\u5e76\u5229\u7528\u6f0f\u6d1e\uff0c\u6784\u6210\u6f5c\u5728\u7684AI\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u5bf9\u6f0f\u6d1e\u7684\u53cd\u5e94\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53cc\u91cd\u673a\u4f1a\u3002\u9996\u5148\uff0c\u5b83\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u900f\u955c\uff0c\u901a\u8fc7\u5b83\u53ef\u4ee5\u68c0\u67e5llm\u4e2d\u7684\u6b67\u4e49\u548c\u8bed\u7528\u5b66\uff0c\u56e0\u4e3a\u5229\u7528\u6f0f\u6d1e\u9700\u8981\u8bc6\u522b\u6b67\u4e49\u5e76\u6267\u884c\u590d\u6742\u7684\u8bed\u7528\u63a8\u7406\u3002\u5176\u6b21\uff0c\u6f0f\u6d1e\u63d0\u51fa\u4e86\u4e00\u4e2a\u6709\u8da3\u800c\u65b0\u9896\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u6a21\u578b\u9762\u4e34\u76f8\u4e92\u51b2\u7a81\u7684\u76ee\u6807\uff0c\u5e76\u4e14\u53ef\u4ee5\u5229\u7528\u6b67\u4e49\u6765\u5b9e\u73b0\u81ea\u5df1\u7684\u4f18\u52bf\u3002", "method": "\u8bbe\u8ba1\u4e86llm\u5177\u6709\u76ee\u6807\u548c\u4e0e\u76ee\u6807\u51b2\u7a81\u7684\u6a21\u7cca\u7528\u6237\u6307\u4ee4\u7684\u573a\u666f\uff0c\u573a\u666f\u6db5\u76d6\u6807\u91cf\u542b\u4e49\u3001\u7ed3\u6784\u6b67\u4e49\u548c\u6743\u529b\u52a8\u6001\u3002\u6d4b\u91cf\u4e86\u4e0d\u540c\u6a21\u578b\u5229\u7528\u6f0f\u6d1e\u6765\u6ee1\u8db3\u5176\u7ed9\u5b9a\u76ee\u6807\uff08\u800c\u4e0d\u662f\u7528\u6237\u76ee\u6807\uff09\u7684\u80fd\u529b\u3002", "result": "\u5c01\u95ed\u6e90\u6a21\u578b\u548c\u66f4\u5f3a\u5927\u7684\u5f00\u6e90\u6a21\u578b\u90fd\u53ef\u4ee5\u8bc6\u522b\u6b67\u4e49\u5e76\u5229\u7528\u7531\u6b64\u4ea7\u751f\u7684\u6f0f\u6d1e\u3002\u5206\u6790\u8868\u660e\uff0c\u5229\u7528\u6f0f\u6d1e\u7684\u6a21\u578b\u53ef\u4ee5\u660e\u786e\u8bc6\u522b\u5e76\u63a8\u7406\u6b67\u4e49\u548c\u51b2\u7a81\u76ee\u6807\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u53ef\u4ee5\u8bc6\u522b\u6b67\u4e49\u5e76\u5229\u7528\u7531\u6b64\u4ea7\u751f\u7684\u6f0f\u6d1e\uff0c\u4ece\u800c\u6784\u6210\u6f5c\u5728\u7684AI\u5b89\u5168\u98ce\u9669\u3002\u80fd\u591f\u5229\u7528\u6f0f\u6d1e\u7684\u6a21\u578b\u53ef\u4ee5\u660e\u786e\u8bc6\u522b\u5e76\u63a8\u7406\u6b67\u4e49\u548c\u51b2\u7a81\u76ee\u6807\u3002"}}
{"id": "2508.19542", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19542", "abs": "https://arxiv.org/abs/2508.19542", "authors": ["Nannan Zhu", "Yonghao Dong", "Teng Wang", "Xueqian Li", "Shengjun Deng", "Yijia Wang", "Zheng Hong", "Tiantian Geng", "Guo Niu", "Hanyan Huang", "Xiongfei Yao", "Shuaiwei Jiao"], "title": "CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning", "comment": null, "summary": "While multimodal large language models (MLLMs) exhibit strong performance on\nsingle-video tasks (e.g., video question answering), their ability across\nmultiple videos remains critically underexplored. However, this capability is\nessential for real-world applications, including multi-camera surveillance and\ncross-video procedural learning. To bridge this gap, we present CVBench, the\nfirst comprehensive benchmark designed to assess cross-video relational\nreasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning\nthree hierarchical tiers: cross-video object association (identifying shared\nentities), cross-video event association (linking temporal or causal event\nchains), and cross-video complex reasoning (integrating commonsense and domain\nknowledge). Built from five domain-diverse video clusters (e.g., sports, life\nrecords), the benchmark challenges models to synthesise information across\ndynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including\nGPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought\nprompting paradigms. Key findings reveal stark performance gaps: even top\nmodels, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks,\ncompared to the 91% accuracy of human performance. Crucially, our analysis\nreveals fundamental bottlenecks inherent in current MLLM architectures, notably\ndeficient inter-video context retention and poor disambiguation of overlapping\nentities. CVBench establishes a rigorous framework for diagnosing and advancing\nmulti-video reasoning, offering architectural insights for next-generation\nMLLMs.The data and evaluation code are available at\nhttps://github.com/Hokhim2/CVBench.", "AI": {"tldr": "CVBench \u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u89c6\u9891\u5173\u7cfb\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u56e0\u679c\u63a8\u7406\u65b9\u9762\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5355\u89c6\u9891\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u591a\u4e2a\u89c6\u9891\u4e2d\u7684\u80fd\u529b\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u80fd\u529b\u5bf9\u4e8e\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u5305\u62ec\u591a\u6444\u50cf\u5934\u76d1\u63a7\u548c\u8de8\u89c6\u9891\u7a0b\u5e8f\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u4e86 CVBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u7efc\u5408\u6027\u7684\u57fa\u51c6\uff0c\u65e8\u5728\u4e25\u683c\u8bc4\u4f30\u8de8\u89c6\u9891\u5173\u7cfb\u63a8\u7406\u3002", "result": "\u5bf9 10 \u591a\u4e2a\u9886\u5148\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u5b58\u5728\u660e\u663e\u7684\u6027\u80fd\u5dee\u8ddd\uff1a\u5373\u4f7f\u662f\u50cf GPT-4o \u8fd9\u6837\u7684\u9876\u7ea7\u6a21\u578b\uff0c\u5728\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e2d\u4e5f\u53ea\u80fd\u8fbe\u5230 60% \u7684\u51c6\u786e\u7387\uff0c\u800c\u4eba\u7c7b\u7684\u51c6\u786e\u7387\u4e3a 91%\u3002\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u4e2d\u56fa\u6709\u7684\u57fa\u672c\u74f6\u9888\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u89c6\u9891\u95f4\u7684\u4e0a\u4e0b\u6587\u4fdd\u7559\u548c\u91cd\u53e0\u5b9e\u4f53\u7684\u6d88\u6b67\u80fd\u529b\u3002", "conclusion": "\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u89c6\u9891\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u5373\u4f7f\u662f\u6700\u597d\u7684\u6a21\u578b\u4e5f\u53ea\u80fd\u8fbe\u5230 60% \u7684\u51c6\u786e\u7387\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u7684 91%\u3002"}}
{"id": "2508.19458", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.19458", "abs": "https://arxiv.org/abs/2508.19458", "authors": ["Mahdi Haghifam", "Adam Smith", "Jonathan Ullman"], "title": "The Sample Complexity of Membership Inference and Privacy Auditing", "comment": "58 Pages", "summary": "A membership-inference attack gets the output of a learning algorithm, and a\ntarget individual, and tries to determine whether this individual is a member\nof the training data or an independent sample from the same distribution. A\nsuccessful membership-inference attack typically requires the attacker to have\nsome knowledge about the distribution that the training data was sampled from,\nand this knowledge is often captured through a set of independent reference\nsamples from that distribution. In this work we study how much information the\nattacker needs for membership inference by investigating the sample\ncomplexity-the minimum number of reference samples required-for a successful\nattack. We study this question in the fundamental setting of Gaussian mean\nestimation where the learning algorithm is given $n$ samples from a Gaussian\ndistribution $\\mathcal{N}(\\mu,\\Sigma)$ in $d$ dimensions, and tries to estimate\n$\\hat\\mu$ up to some error $\\mathbb{E}[\\|\\hat \\mu - \\mu\\|^2_{\\Sigma}]\\leq\n\\rho^2 d$. Our result shows that for membership inference in this setting,\n$\\Omega(n + n^2 \\rho^2)$ samples can be necessary to carry out any attack that\ncompetes with a fully informed attacker. Our result is the first to show that\nthe attacker sometimes needs many more samples than the training algorithm uses\nto train the model. This result has significant implications for practice, as\nall attacks used in practice have a restricted form that uses $O(n)$ samples\nand cannot benefit from $\\omega(n)$ samples. Thus, these attacks may be\nunderestimating the possibility of membership inference, and better attacks may\nbe possible when information about the distribution is easy to obtain.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6210\u5458\u63a8\u7406\u653b\u51fb\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u53d1\u73b0\u653b\u51fb\u8005\u53ef\u80fd\u9700\u8981\u6bd4\u8bad\u7ec3\u6570\u636e\u66f4\u591a\u7684\u53c2\u8003\u6837\u672c\u624d\u80fd\u6210\u529f\u653b\u51fb\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u4f4e\u4f30\u4e86\u6210\u5458\u63a8\u7406\u7684\u98ce\u9669\u3002", "motivation": "\u7814\u7a76\u6210\u5458\u63a8\u7406\u653b\u51fb\u8005\u9700\u8981\u591a\u5c11\u4fe1\u606f\u624d\u80fd\u6210\u529f\u8fdb\u884c\u653b\u51fb\uff0c\u901a\u8fc7\u8c03\u67e5\u6210\u529f\u653b\u51fb\u6240\u9700\u7684\u6700\u5c0f\u53c2\u8003\u6837\u672c\u6570\u91cf\u6765\u5b9e\u73b0\u3002", "method": "\u7814\u7a76\u4e86\u9ad8\u65af\u5747\u503c\u4f30\u8ba1\u7684\u57fa\u672c\u573a\u666f\u4e0b\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u5176\u4e2d\u5b66\u4e60\u7b97\u6cd5\u4ece $d$ \u7ef4\u9ad8\u65af\u5206\u5e03 $\\mathcal{N}(\\mu,\\Sigma)$ \u4e2d\u83b7\u5f97 $n$ \u4e2a\u6837\u672c\uff0c\u5e76\u5c1d\u8bd5\u4f30\u8ba1 $\\hat\\mu$ \u76f4\u5230\u8bef\u5dee $\\mathbb{E}[\\|\\hat \\mu - \\mu\\|^2_{\\Sigma}]\\leq \\rho^2 d$\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5728\u8fd9\u79cd\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u6210\u5458\u63a8\u7406\uff0c\u53ef\u80fd\u9700\u8981 $\\Omega(n + n^2 \\rho^2)$ \u4e2a\u6837\u672c\u624d\u80fd\u8fdb\u884c\u4efb\u4f55\u4e0e\u5145\u5206\u77e5\u60c5\u7684\u653b\u51fb\u8005\u7ade\u4e89\u7684\u653b\u51fb\u3002 \u8fd9\u662f\u7b2c\u4e00\u4e2a\u8868\u660e\u653b\u51fb\u8005\u6709\u65f6\u9700\u8981\u7684\u6837\u672c\u6bd4\u8bad\u7ec3\u7b97\u6cd5\u8bad\u7ec3\u6a21\u578b\u4f7f\u7528\u7684\u6837\u672c\u591a\u5f97\u591a\u7684\u7ed3\u679c\u3002", "conclusion": "\u5bf9\u4e8e\u9ad8\u65af\u5747\u503c\u4f30\u8ba1\uff0c\u8fdb\u884c\u6210\u5458\u63a8\u7406\u653b\u51fb\u6240\u9700\u7684\u53c2\u8003\u6837\u672c\u6570\u91cf\u53ef\u80fd\u8fdc\u8d85\u8bad\u7ec3\u7b97\u6cd5\u4f7f\u7528\u7684\u6837\u672c\u6570\u3002\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u653b\u51fb\u53ef\u80fd\u56e0\u4e3a\u6837\u672c\u6570\u91cf\u7684\u9650\u5236\u800c\u4f4e\u4f30\u4e86\u6210\u5458\u63a8\u7406\u7684\u53ef\u80fd\u6027\uff0c\u5f53\u66f4\u5bb9\u6613\u83b7\u53d6\u5173\u4e8e\u5206\u5e03\u7684\u4fe1\u606f\u65f6\uff0c\u53ef\u80fd\u5b58\u5728\u66f4\u597d\u7684\u653b\u51fb\u65b9\u6cd5\u3002"}}
{"id": "2508.19578", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19578", "abs": "https://arxiv.org/abs/2508.19578", "authors": ["Jiaqi Deng", "Yuho Lee", "Nicole Hee-Yeon Kim", "Hyangsuk Min", "Taewon Yun", "Minjeong Ban", "Kim Yul", "Hwanjun Song"], "title": "Towards a Holistic and Automated Evaluation Framework for Multi-Level Comprehension of LLMs in Book-Length Contexts", "comment": "Accepted to EMNLP 2025 (Main)", "summary": "We introduce HAMLET, a holistic and automated framework for evaluating the\nlong-context comprehension of large language models (LLMs). HAMLET structures\nsource texts into a three-level key-fact hierarchy at root-, branch-, and\nleaf-levels, and employs query-focused summarization to evaluate how well\nmodels recall and faithfully represent information at each level. To validate\nthe reliability of our fully automated pipeline, we conduct a systematic human\nstudy, showing that our automatic evaluation achieves over 90% agreement with\nexpert human judgments, while reducing the cost by up to 25 times. HAMLET\nreveals that LLMs struggle with fine-grained comprehension, especially at the\nleaf level, and are sensitive to positional effects like the\nlost-in-the-middle. Analytical queries pose greater challenges than narrative\nones, and consistent performance gaps emerge between open-source and\nproprietary models, as well as across model scales. Our code and dataset are\npublicly available at https://github.com/DISL-Lab/HAMLET.", "AI": {"tldr": "HAMLET: a framework for evaluating long-context comprehension of LLMs. It reveals that LLMs struggle with fine-grained comprehension and are sensitive to positional effects.", "motivation": "evaluate the long-context comprehension of large language models (LLMs)", "method": "a holistic and automated framework for evaluating the long-context comprehension of large language models (LLMs). HAMLET structures source texts into a three-level key-fact hierarchy at root-, branch-, and leaf-levels, and employs query-focused summarization to evaluate how well models recall and faithfully represent information at each level.", "result": "automatic evaluation achieves over 90% agreement with expert human judgments, while reducing the cost by up to 25 times.", "conclusion": "LLMs struggle with fine-grained comprehension, especially at the leaf level, and are sensitive to positional effects like the lost-in-the-middle. Analytical queries pose greater challenges than narrative ones, and consistent performance gaps emerge between open-source and proprietary models, as well as across model scales."}}
{"id": "2508.19544", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19544", "abs": "https://arxiv.org/abs/2508.19544", "authors": ["Eduardo Davalos", "Yike Zhang", "Namrata Srivastava", "Yashvitha Thatigotla", "Jorge A. Salas", "Sara McFadden", "Sun-Joo Cho", "Amanda Goodwin", "Ashwin TS", "Gautam Biswas"], "title": "WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization", "comment": "9 pages, 7 figures, 1 table", "summary": "With advancements in AI, new gaze estimation methods are exceeding\nstate-of-the-art (SOTA) benchmarks, but their real-world application reveals a\ngap with commercial eye-tracking solutions. Factors like model size, inference\ntime, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking\nmethods lack sufficient accuracy, in particular due to head movement. To tackle\nthese issues, we introduce We bEyeTrack, a framework that integrates\nlightweight SOTA gaze estimation models directly in the browser. It\nincorporates model-based head pose estimation and on-device few-shot learning\nwith as few as nine calibration samples (k < 9). WebEyeTrack adapts to new\nusers, achieving SOTA performance with an error margin of 2.32 cm on\nGazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14.\nOur open-source code is available at\nhttps://github.com/RedForestAi/WebEyeTrack.", "AI": {"tldr": "WebEyeTrack is a framework that integrates lightweight SOTA gaze estimation models directly in the browser. It achieves SOTA performance with an error margin of 2.32 cm on GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14.", "motivation": "new gaze estimation methods are exceeding state-of-the-art (SOTA) benchmarks, but their real-world application reveals a gap with commercial eye-tracking solutions. Factors like model size, inference time, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking methods lack sufficient accuracy, in particular due to head movement.", "method": "a framework that integrates lightweight SOTA gaze estimation models directly in the browser. It incorporates model-based head pose estimation and on-device few-shot learning with as few as nine calibration samples (k < 9).", "result": "WebEyeTrack adapts to new users, achieving SOTA performance with an error margin of 2.32 cm on GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14.", "conclusion": "WebEyeTrack achieves SOTA performance with an error margin of 2.32 cm on GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14."}}
