{"id": "2510.03750", "categories": ["cs.IR", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.03750", "abs": "https://arxiv.org/abs/2510.03750", "authors": ["Hanwen Zhang", "Kun Fang", "Ziyu Wang", "Ichiro Fujinaga"], "title": "Evaluating High-Resolution Piano Sustain Pedal Depth Estimation with Musically Informed Metrics", "comment": null, "summary": "Evaluation for continuous piano pedal depth estimation tasks remains\nincomplete when relying only on conventional frame-level metrics, which\noverlook musically important features such as direction-change boundaries and\npedal curve contours. To provide more interpretable and musically meaningful\ninsights, we propose an evaluation framework that augments standard frame-level\nmetrics with an action-level assessment measuring direction and timing using\nsegments of press/hold/release states and a gesture-level analysis that\nevaluates contour similarity of each press-release cycle. We apply this\nframework to compare an audio-only baseline with two variants: one\nincorporating symbolic information from MIDI, and another trained in a\nbinary-valued setting, all within a unified architecture. Results show that the\nMIDI-informed model significantly outperforms the others at action and gesture\nlevels, despite modest frame-level gains. These findings demonstrate that our\nframework captures musically relevant improvements indiscernible by traditional\nmetrics, offering a more practical and effective approach to evaluating pedal\ndepth estimation models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u94a2\u7434\u8e0f\u677f\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4e0d\u4ec5\u4f7f\u7528\u4f20\u7edf\u7684\u5e27\u7ea7\u522b\u6307\u6807\uff0c\u8fd8\u7ed3\u5408\u4e86\u52a8\u4f5c\u7ea7\u522b\u548c\u624b\u52bf\u7ea7\u522b\u7684\u5206\u6790\uff0c\u4ee5\u6355\u6349\u97f3\u4e50\u4e0a\u91cd\u8981\u7684\u7279\u5f81\u3002", "motivation": "\u4f20\u7edf\u7684\u5e27\u7ea7\u522b\u6307\u6807\u5728\u8bc4\u4f30\u94a2\u7434\u8e0f\u677f\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\u65f6\u4e0d\u591f\u5b8c\u5584\uff0c\u5ffd\u7565\u4e86\u97f3\u4e50\u4e0a\u91cd\u8981\u7684\u7279\u5f81\uff0c\u5982\u65b9\u5411\u53d8\u5316\u8fb9\u754c\u548c\u8e0f\u677f\u66f2\u7ebf\u8f6e\u5ed3\u3002", "method": "\u8be5\u6846\u67b6\u901a\u8fc7\u6d4b\u91cf\u538b/\u4fdd\u6301/\u91ca\u653e\u72b6\u6001\u7684\u7247\u6bb5\u7684\u65b9\u5411\u548c\u65f6\u95f4\u6765\u8bc4\u4f30\u52a8\u4f5c\u7ea7\u522b\uff0c\u5e76\u901a\u8fc7\u8bc4\u4f30\u6bcf\u4e2a\u538b-\u653e\u5468\u671f\u7684\u8f6e\u5ed3\u76f8\u4f3c\u6027\u6765\u8bc4\u4f30\u624b\u52bf\u7ea7\u522b\u3002\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8e\u6bd4\u8f83\u7eaf\u97f3\u9891\u57fa\u7ebf\u4e0e\u4e24\u4e2a\u53d8\u4f53\uff1a\u4e00\u4e2a\u7ed3\u5408\u4e86\u6765\u81eaMIDI\u7684\u7b26\u53f7\u4fe1\u606f\uff0c\u53e6\u4e00\u4e2a\u5728\u4e8c\u5143\u503c\u8bbe\u7f6e\u4e2d\u8bad\u7ec3\uff0c\u6240\u6709\u8fd9\u4e9b\u90fd\u5728\u7edf\u4e00\u7684\u67b6\u6784\u4e2d\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u5e27\u7ea7\u522b\u589e\u76ca\u4e0d\u5927\uff0c\u4f46\u77e5\u60c5\u7684MIDI\u6a21\u578b\u5728\u52a8\u4f5c\u548c\u624b\u52bf\u7ea7\u522b\u4e0a\u660e\u663e\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002\u8868\u660e\u8be5\u6846\u67b6\u6355\u83b7\u4e86\u4f20\u7edf\u6307\u6807\u65e0\u6cd5\u8fa8\u522b\u7684\u97f3\u4e50\u76f8\u5173\u6539\u8fdb\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5b9e\u7528\u3001\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u8e0f\u677f\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.03795", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03795", "abs": "https://arxiv.org/abs/2510.03795", "authors": ["Simon Lupart", "Dani\u00ebl van Dijk", "Eric Langezaal", "Ian van Dort", "Mohammad Aliannejadi"], "title": "Investigating LLM Variability in Personalized Conversational Information Retrieval", "comment": "11 pages, 5 figures, SIGIR-AP'25 Proceedings of the 2025 Annual\n  International ACM SIGIR Conference on Research and Development in Information\n  Retrieval in the Asia Pacific Region (SIGIR-AP 2025), December 7--10, 2025,\n  Xi'an, China", "summary": "Personalized Conversational Information Retrieval (CIR) has seen rapid\nprogress in recent years, driven by the development of Large Language Models\n(LLMs). Personalized CIR aims to enhance document retrieval by leveraging\nuser-specific information, such as preferences, knowledge, or constraints, to\ntailor responses to individual needs. A key resource for this task is the TREC\niKAT 2023 dataset, designed to evaluate personalization in CIR pipelines.\nBuilding on this resource, Mo et al. explored several strategies for\nincorporating Personal Textual Knowledge Bases (PTKB) into LLM-based query\nreformulation. Their findings suggested that personalization from PTKBs could\nbe detrimental and that human annotations were often noisy. However, these\nconclusions were based on single-run experiments using the GPT-3.5 Turbo model,\nraising concerns about output variability and repeatability. In this\nreproducibility study, we rigorously reproduce and extend their work, focusing\non LLM output variability and model generalization. We apply the original\nmethods to the new TREC iKAT 2024 dataset and evaluate a diverse range of\nmodels, including Llama (1B-70B), Qwen-7B, GPT-4o-mini. Our results show that\nhuman-selected PTKBs consistently enhance retrieval performance, while\nLLM-based selection methods do not reliably outperform manual choices. We\nfurther compare variance across datasets and observe higher variability on iKAT\nthan on CAsT, highlighting the challenges of evaluating personalized CIR.\nNotably, recall-oriented metrics exhibit lower variance than precision-oriented\nones, a critical insight for first-stage retrievers. Finally, we underscore the\nneed for multi-run evaluations and variance reporting when assessing LLM-based\nCIR systems. By broadening evaluation across models, datasets, and metrics, our\nstudy contributes to more robust and generalizable practices for personalized\nCIR.", "AI": {"tldr": "\u8be5\u7814\u7a76\u91cd\u73b0\u5e76\u6269\u5c55\u4e86Mo\u7b49\u4eba\u5728\u4e2a\u6027\u5316\u5bf9\u8bdd\u4fe1\u606f\u68c0\u7d22\uff08CIR\uff09\u65b9\u9762\u7684\u5de5\u4f5c\uff0c\u53d1\u73b0\u4eba\u5de5\u9009\u62e9\u7684\u4e2a\u4eba\u6587\u672c\u77e5\u8bc6\u5e93\uff08PTKB\uff09\u59cb\u7ec8\u63d0\u9ad8\u68c0\u7d22\u6027\u80fd\uff0c\u800c\u57fa\u4e8eLLM\u7684\u9009\u62e9\u65b9\u6cd5\u5219\u4e0d\u7136\u3002\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u8f6e\u8bc4\u4f30\u548c\u65b9\u5dee\u62a5\u544a\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4e2a\u6027\u5316CIR\u7814\u7a76\u4f9d\u8d56\u4e8eGPT-3.5 Turbo\u6a21\u578b\u7684\u5355\u6b21\u8fd0\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u53ef\u80fd\u5b58\u5728\u53d8\u5f02\u6027\u548c\u53ef\u91cd\u590d\u6027\u95ee\u9898\u3002\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u66f4\u5e7f\u6cdb\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\u8bc4\u4f30\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8be5\u7814\u7a76\u91cd\u73b0\u4e86Mo\u7b49\u4eba\u7684\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u65b0\u7684TREC iKAT 2024\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u5305\u62ecLlama\u3001Qwen\u548cGPT-4o-mini\u5728\u5185\u7684\u591a\u79cd\u6a21\u578b\u3002\u540c\u65f6\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u65b9\u5dee\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u6307\u6807\u7684\u53d8\u5f02\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4eba\u5de5\u9009\u62e9\u7684PTKB\u59cb\u7ec8\u63d0\u9ad8\u68c0\u7d22\u6027\u80fd\uff0c\u800c\u57fa\u4e8eLLM\u7684\u9009\u62e9\u65b9\u6cd5\u5219\u4e0d\u7136\u3002\u6b64\u5916\uff0ciKAT\u6570\u636e\u96c6\u4e0a\u7684\u53d8\u5f02\u6027\u9ad8\u4e8eCAsT\u6570\u636e\u96c6\uff0c\u4e14\u9762\u5411\u53ec\u56de\u7387\u7684\u6307\u6807\u6bd4\u9762\u5411\u7cbe\u786e\u7387\u7684\u6307\u6807\u8868\u73b0\u51fa\u66f4\u4f4e\u7684\u65b9\u5dee\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u8bc4\u4f30\u57fa\u4e8eLLM\u7684CIR\u7cfb\u7edf\u65f6\uff0c\u9700\u8981\u8fdb\u884c\u591a\u8f6e\u8bc4\u4f30\u548c\u65b9\u5dee\u62a5\u544a\uff0c\u5e76\u5efa\u8bae\u5728\u66f4\u5e7f\u6cdb\u7684\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u6307\u6807\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u4ee5\u5b9e\u73b0\u66f4\u7a33\u5065\u548c\u901a\u7528\u7684\u4e2a\u6027\u5316CIR\u5b9e\u8df5\u3002"}}
{"id": "2510.03984", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03984", "abs": "https://arxiv.org/abs/2510.03984", "authors": ["Kirandeep Kaur", "Preetam Prabhu Srikar Dammu", "Hideo Joho", "Chirag Shah"], "title": "Beyond Static Evaluation: Rethinking the Assessment of Personalized Agent Adaptability in Information Retrieval", "comment": null, "summary": "Personalized AI agents are becoming central to modern information retrieval,\nyet most evaluation methodologies remain static, relying on fixed benchmarks\nand one-off metrics that fail to reflect how users' needs evolve over time.\nThese limitations hinder our ability to assess whether agents can meaningfully\nadapt to individuals across dynamic, longitudinal interactions. In this\nperspective paper, we propose a conceptual lens for rethinking evaluation in\nadaptive personalization, shifting the focus from static performance snapshots\nto interaction-aware, evolving assessments. We organize this lens around three\ncore components: (1) persona-based user simulation with temporally evolving\npreference models; (2) structured elicitation protocols inspired by reference\ninterviews to extract preferences in context; and (3) adaptation-aware\nevaluation mechanisms that measure how agent behavior improves across sessions\nand tasks. While recent works have embraced LLM-driven user simulation, we\nsituate this practice within a broader paradigm for evaluating agents over\ntime. To illustrate our ideas, we conduct a case study in e-commerce search\nusing the PersonalWAB dataset. Beyond presenting a framework, our work lays a\nconceptual foundation for understanding and evaluating personalization as a\ncontinuous, user-centric endeavor.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u81ea\u9002\u5e94\u4e2a\u6027\u5316AI agent\u7684\u65b0\u89c6\u89d2\uff0c\u4ece\u9759\u6001\u6027\u80fd\u8f6c\u5411\u4ea4\u4e92\u611f\u77e5\u7684\u6f14\u8fdb\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u53cd\u6620\u7528\u6237\u9700\u6c42\u968f\u65f6\u95f4\u6f14\u53d8\uff0c\u963b\u788d\u4e86\u8bc4\u4f30agents\u5728\u52a8\u6001\u4ea4\u4e92\u4e2d\u9002\u5e94\u4e2a\u4f53\u7528\u6237\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u7684\u8bc4\u4f30\u6846\u67b6\uff1a\u57fa\u4e8e\u7528\u6237\u753b\u50cf\u7684\u3001\u5177\u6709\u65f6\u95f4\u6f14\u53d8\u504f\u597d\u6a21\u578b\u7684\u7528\u6237\u6a21\u62df\uff1b\u53d7\u8bbf\u8c08\u542f\u53d1\u7684\u3001\u7ed3\u6784\u5316\u7684\u8bf1\u5bfc\u534f\u8bae\uff0c\u7528\u4e8e\u63d0\u53d6\u4e0a\u4e0b\u6587\u504f\u597d\uff1b\u4ee5\u53ca\u80fd\u591f\u8861\u91cfagent\u884c\u4e3a\u5728\u4f1a\u8bdd\u548c\u4efb\u52a1\u4e2d\u6539\u8fdb\u7a0b\u5ea6\u7684\u3001\u9002\u5e94\u611f\u77e5\u7684\u8bc4\u4f30\u673a\u5236\u3002", "result": "\u901a\u8fc7\u5728PersonalWAB\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7535\u5546\u641c\u7d22\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u3002", "conclusion": "\u672c\u6587\u4e3a\u7406\u89e3\u548c\u8bc4\u4f30\u4e2a\u6027\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6982\u5ff5\u57fa\u7840\uff0c\u5c06\u5176\u89c6\u4e3a\u4e00\u4e2a\u6301\u7eed\u7684\u3001\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u52aa\u529b\u3002"}}
{"id": "2510.04010", "categories": ["cs.IR", "cs.CL", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.04010", "abs": "https://arxiv.org/abs/2510.04010", "authors": ["Yu-Fei Shih", "An-Zi Yen", "Hen-Hsen Huang", "Hsin-Hsi Chen"], "title": "Visual Lifelog Retrieval through Captioning-Enhanced Interpretation", "comment": null, "summary": "People often struggle to remember specific details of past experiences, which\ncan lead to the need to revisit these memories. Consequently, lifelog retrieval\nhas emerged as a crucial application. Various studies have explored methods to\nfacilitate rapid access to personal lifelogs for memory recall assistance. In\nthis paper, we propose a Captioning-Integrated Visual Lifelog (CIVIL) Retrieval\nSystem for extracting specific images from a user's visual lifelog based on\ntextual queries. Unlike traditional embedding-based methods, our system first\ngenerates captions for visual lifelogs and then utilizes a text embedding model\nto project both the captions and user queries into a shared vector space.\nVisual lifelogs, captured through wearable cameras, provide a first-person\nviewpoint, necessitating the interpretation of the activities of the individual\nbehind the camera rather than merely describing the scene. To address this, we\nintroduce three distinct approaches: the single caption method, the collective\ncaption method, and the merged caption method, each designed to interpret the\nlife experiences of lifeloggers. Experimental results show that our method\neffectively describes first-person visual images, enhancing the outcomes of\nlifelog retrieval. Furthermore, we construct a textual dataset that converts\nvisual lifelogs into captions, thereby reconstructing personal life\nexperiences.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCIVIL\u7684\u89c6\u89c9\u751f\u6d3b\u65e5\u5fd7\u68c0\u7d22\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u901a\u8fc7\u4e3a\u89c6\u89c9\u751f\u6d3b\u65e5\u5fd7\u751f\u6210\u6807\u9898\u5e76\u4f7f\u7528\u6587\u672c\u5d4c\u5165\u6a21\u578b\u5c06\u6807\u9898\u548c\u7528\u6237\u67e5\u8be2\u6295\u5f71\u5230\u5171\u4eab\u5411\u91cf\u7a7a\u95f4\u4e2d\u6765\u5b9e\u73b0\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4eba\u4eec\u96be\u4ee5\u8bb0\u4f4f\u8fc7\u53bb\u7ecf\u5386\u7684\u5177\u4f53\u7ec6\u8282\uff0c\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u8fd9\u4e9b\u8bb0\u5fc6\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u751f\u6d3b\u65e5\u5fd7\u68c0\u7d22\u6210\u4e3a\u4e00\u9879\u5173\u952e\u5e94\u7528\u3002", "method": "\u8be5\u7cfb\u7edf\u9996\u5148\u4e3a\u89c6\u89c9\u751f\u6d3b\u65e5\u5fd7\u751f\u6210\u6807\u9898\uff0c\u7136\u540e\u5229\u7528\u6587\u672c\u5d4c\u5165\u6a21\u578b\u5c06\u6807\u9898\u548c\u7528\u6237\u67e5\u8be2\u6295\u5f71\u5230\u5171\u4eab\u5411\u91cf\u7a7a\u95f4\u4e2d\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u65b9\u6cd5\uff1a\u5355\u6807\u9898\u6cd5\u3001\u96c6\u4f53\u6807\u9898\u6cd5\u548c\u5408\u5e76\u6807\u9898\u6cd5\uff0c\u6bcf\u79cd\u65b9\u6cd5\u90fd\u65e8\u5728\u89e3\u91ca\u751f\u6d3b\u8bb0\u5f55\u8005\u7684\u751f\u6d3b\u7ecf\u5386\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u63cf\u8ff0\u4e86\u7b2c\u4e00\u4eba\u79f0\u89c6\u89c9\u56fe\u50cf\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u751f\u6d3b\u65e5\u5fd7\u68c0\u7d22\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u63cf\u8ff0\u7b2c\u4e00\u4eba\u79f0\u89c6\u89c9\u56fe\u50cf\uff0c\u589e\u5f3a\u751f\u6d3b\u65e5\u5fd7\u68c0\u7d22\u7684\u6548\u679c\u3002\u6b64\u5916\uff0c\u8fd8\u6784\u5efa\u4e86\u4e00\u4e2a\u6587\u672c\u6570\u636e\u96c6\uff0c\u53ef\u4ee5\u5c06\u89c6\u89c9\u751f\u6d3b\u65e5\u5fd7\u8f6c\u6362\u4e3a\u6807\u9898\uff0c\u4ece\u800c\u91cd\u5efa\u4e2a\u4eba\u751f\u6d3b\u4f53\u9a8c\u3002"}}
{"id": "2510.03315", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03315", "abs": "https://arxiv.org/abs/2510.03315", "authors": ["Alex Gibson"], "title": "Decomposing Attention To Find Context-Sensitive Neurons", "comment": "10 pages, 7 figures. Submitted to the Mechanistic Interpretability\n  Workshop at NeurIPS 2025", "summary": "We study transformer language models, analyzing attention heads whose\nattention patterns are spread out, and whose attention scores depend weakly on\ncontent. We argue that the softmax denominators of these heads are stable when\nthe underlying token distribution is fixed. By sampling softmax denominators\nfrom a \"calibration text\", we can combine together the outputs of multiple such\nstable heads in the first layer of GPT2-Small, approximating their combined\noutput by a linear summary of the surrounding text. This approximation enables\na procedure where from the weights alone - and a single calibration text - we\ncan uncover hundreds of first layer neurons that respond to high-level\ncontextual properties of the surrounding text, including neurons that didn't\nactivate on the calibration text.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86transformer\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6ce8\u610f\u529b\u5934\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u6ce8\u610f\u529b\u6a21\u5f0f\u5206\u6563\u4e14\u6ce8\u610f\u529b\u5206\u6570\u4e0e\u5185\u5bb9\u4f9d\u8d56\u6027\u5f31\u7684\u5934\u3002", "motivation": "\u7814\u7a76\u8fd9\u4e9b\u6ce8\u610f\u529b\u5934\u7684\u52a8\u673a\u5728\u4e8e\u7406\u89e3\u5b83\u4eec\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u63a2\u7d22\u5b83\u4eec\u662f\u5426\u5177\u6709\u7a33\u5b9a\u7684\u7279\u6027\u3002", "method": "\u8be5\u8bba\u6587\u901a\u8fc7\u91c7\u6837softmax\u5206\u6bcd\uff0c\u5e76\u7ed3\u5408GPT2-Small\u7b2c\u4e00\u5c42\u4e2d\u591a\u4e2a\u7a33\u5b9a\u5934\u7684\u8f93\u51fa\uff0c\u7528\u5468\u56f4\u6587\u672c\u7684\u7ebf\u6027\u6458\u8981\u6765\u8fd1\u4f3c\u5b83\u4eec\u7684\u7ec4\u5408\u8f93\u51fa\u3002", "result": "\u8be5\u7814\u7a76\u4ec5\u901a\u8fc7\u6743\u91cd\u548c\u4e00\u4e2a\u6821\u51c6\u6587\u672c\uff0c\u5c31\u80fd\u53d1\u73b0\u6570\u767e\u4e2a\u5bf9\u5468\u56f4\u6587\u672c\u7684\u9ad8\u7ea7\u4e0a\u4e0b\u6587\u5c5e\u6027\u505a\u51fa\u53cd\u5e94\u7684\u7b2c\u4e00\u5c42\u795e\u7ecf\u5143\uff0c\u5305\u62ec\u90a3\u4e9b\u5728\u6821\u51c6\u6587\u672c\u4e0a\u672a\u6fc0\u6d3b\u7684\u795e\u7ecf\u5143\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u53ef\u4ee5\u901a\u8fc7\u5206\u6790\u6ce8\u610f\u529b\u5934\u7684softmax\u5206\u6bcd\u6765\u7406\u89e3\u5b83\u4eec\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u63ed\u793a\u6a21\u578b\u4e2d\u5bf9\u4e0a\u4e0b\u6587\u4fe1\u606f\u654f\u611f\u7684\u795e\u7ecf\u5143\u3002"}}
{"id": "2510.03287", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03287", "abs": "https://arxiv.org/abs/2510.03287", "authors": ["Moinak Bhattacharya", "Gagandeep Singh", "Prateek Prasanna"], "title": "SoC-DT: Standard-of-Care Aligned Digital Twins for Patient-Specific Tumor Dynamics", "comment": null, "summary": "Accurate prediction of tumor trajectories under standard-of-care (SoC)\ntherapies remains a major unmet need in oncology. This capability is essential\nfor optimizing treatment planning and anticipating disease progression.\nConventional reaction-diffusion models are limited in scope, as they fail to\ncapture tumor dynamics under heterogeneous therapeutic paradigms. There is\nhence a critical need for computational frameworks that can realistically\nsimulate SoC interventions while accounting for inter-patient variability in\ngenomics, demographics, and treatment regimens. We introduce Standard-of-Care\nDigital Twin (SoC-DT), a differentiable framework that unifies\nreaction-diffusion tumor growth models, discrete SoC interventions (surgery,\nchemotherapy, radiotherapy) along with genomic and demographic personalization\nto predict post-treatment tumor structure on imaging. An implicit-explicit\nexponential time-differencing solver, IMEX-SoC, is also proposed, which ensures\nstability, positivity, and scalability in SoC treatment situations. Evaluated\non both synthetic data and real world glioma data, SoC-DT consistently\noutperforms classical PDE baselines and purely data-driven neural models in\npredicting tumor dynamics. By bridging mechanistic interpretability with modern\ndifferentiable solvers, SoC-DT establishes a principled foundation for\npatient-specific digital twins in oncology, enabling biologically consistent\ntumor dynamics estimation. Code will be made available upon acceptance.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aStandard-of-Care Digital Twin (SoC-DT) \u7684\u53ef\u5fae\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u6807\u51c6\u6cbb\u7597(SoC)\u4e0b\u7684\u80bf\u7624\u8f68\u8ff9\u3002", "motivation": "\u5728\u80bf\u7624\u5b66\u4e2d\uff0c\u51c6\u786e\u9884\u6d4b\u6807\u51c6\u6cbb\u7597(SoC)\u4e0b\u7684\u80bf\u7624\u8f68\u8ff9\u4ecd\u7136\u662f\u4e00\u4e2a\u5c1a\u672a\u6ee1\u8db3\u7684\u91cd\u8981\u9700\u6c42\u3002\u4f20\u7edf\u7684\u53cd\u5e94\u6269\u6563\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u5f02\u8d28\u6cbb\u7597\u8303\u5f0f\u4e0b\u7684\u80bf\u7624\u52a8\u6001\u3002", "method": "\u8be5\u6846\u67b6\u7edf\u4e00\u4e86\u53cd\u5e94\u6269\u6563\u80bf\u7624\u751f\u957f\u6a21\u578b\u3001\u79bb\u6563SoC\u5e72\u9884\uff08\u624b\u672f\u3001\u5316\u7597\u3001\u653e\u7597\uff09\u4ee5\u53ca\u57fa\u56e0\u7ec4\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u4e2a\u6027\u5316\uff0c\u4ee5\u9884\u6d4b\u6cbb\u7597\u540e\u6210\u50cf\u4e0a\u7684\u80bf\u7624\u7ed3\u6784\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u5f0f-\u663e\u5f0f\u6307\u6570\u65f6\u95f4\u5dee\u5206\u6c42\u89e3\u5668IMEX-SoC\uff0c\u4ee5\u786e\u4fddSoC\u6cbb\u7597\u60c5\u51b5\u4e0b\u7684\u7a33\u5b9a\u6027\u3001\u79ef\u6781\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u4e16\u754c\u795e\u7ecf\u80f6\u8d28\u7624\u6570\u636e\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cSoC-DT\u5728\u9884\u6d4b\u80bf\u7624\u52a8\u6001\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u7ecf\u5178\u7684PDE\u57fa\u7ebf\u548c\u7eaf\u7cb9\u7684\u6570\u636e\u9a71\u52a8\u795e\u7ecf\u6a21\u578b\u3002", "conclusion": "SoC-DT\u901a\u8fc7\u5c06\u673a\u5236\u53ef\u89e3\u91ca\u6027\u4e0e\u73b0\u4ee3\u53ef\u5fae\u6c42\u89e3\u5668\u76f8\u7ed3\u5408\uff0c\u4e3a\u80bf\u7624\u5b66\u4e2d\u60a3\u8005\u7279\u5f02\u6027\u6570\u5b57\u5b6a\u751f\u5efa\u7acb\u4e86\u539f\u5219\u6027\u57fa\u7840\uff0c\u4ece\u800c\u80fd\u591f\u5b9e\u73b0\u751f\u7269\u5b66\u4e0a\u4e00\u81f4\u7684\u80bf\u7624\u52a8\u529b\u5b66\u4f30\u8ba1\u3002"}}
{"id": "2510.03386", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03386", "abs": "https://arxiv.org/abs/2510.03386", "authors": ["Zixuan Yi", "Sami Abu-el-Haija", "Yawen Wang", "Teja Vemparala", "Yannis Chronis", "Yu Gan", "Michael Burrows", "Carsten Binnig", "Bryan Perozzi", "Ryan Marcus", "Fatma Ozcan"], "title": "Is it Bigger than a Breadbox: Efficient Cardinality Estimation for Real World Workloads", "comment": null, "summary": "DB engines produce efficient query execution plans by relying on cost models.\nPractical implementations estimate cardinality of queries using heuristics,\nwith magic numbers tuned to improve average performance on benchmarks.\nEmpirically, estimation error significantly grows with query complexity.\nAlternatively, learning-based estimators offer improved accuracy, but add\noperational complexity preventing their adoption in-practice. Recognizing that\nquery workloads contain highly repetitive subquery patterns, we learn many\nsimple regressors online, each localized to a pattern. The regressor\ncorresponding to a pattern can be randomly-accessed using hash of graph\nstructure of the subquery. Our method has negligible overhead and competes with\nSoTA learning-based approaches on error metrics. Further, amending PostgreSQL\nwith our method achieves notable accuracy and runtime improvements over\ntraditional methods and drastically reduces operational costs compared to other\nlearned cardinality estimators, thereby offering the most practical and\nefficient solution on the Pareto frontier. Concretely, simulating JOB-lite\nworkload on IMDb speeds-up execution by 7.5 minutes (>30%) while incurring only\n37 seconds overhead for online learning.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u57fa\u6570\u4f30\u8ba1\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u8bb8\u591a\u7b80\u5355\u7684\u56de\u5f52\u5668\uff0c\u6bcf\u4e2a\u56de\u5f52\u5668\u90fd\u9488\u5bf9\u4e00\u4e2a\u7279\u5b9a\u7684\u5b50\u67e5\u8be2\u6a21\u5f0f\u8fdb\u884c\u4f18\u5316\uff0c\u4ece\u800c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u5e93\u5f15\u64ce\u4f9d\u8d56\u4e8e\u6210\u672c\u6a21\u578b\u6765\u751f\u6210\u9ad8\u6548\u7684\u67e5\u8be2\u6267\u884c\u8ba1\u5212\uff0c\u4f46\u8fd9\u4e9b\u6210\u672c\u6a21\u578b\u4e2d\u7684\u57fa\u6570\u4f30\u8ba1\u901a\u5e38\u4f7f\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u4e14\u8bef\u5dee\u4f1a\u968f\u7740\u67e5\u8be2\u590d\u6742\u6027\u7684\u589e\u52a0\u800c\u663e\u8457\u589e\u957f\u3002\u57fa\u4e8e\u5b66\u4e60\u7684\u4f30\u8ba1\u5668\u867d\u7136\u53ef\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u4f1a\u589e\u52a0\u64cd\u4f5c\u590d\u6742\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u8bb8\u591a\u7b80\u5355\u7684\u56de\u5f52\u5668\uff0c\u6bcf\u4e2a\u56de\u5f52\u5668\u90fd\u9488\u5bf9\u4e00\u4e2a\u7279\u5b9a\u7684\u5b50\u67e5\u8be2\u6a21\u5f0f\u8fdb\u884c\u4f18\u5316\u3002\u4e0e\u6a21\u5f0f\u76f8\u5bf9\u5e94\u7684\u56de\u5f52\u5668\u53ef\u4ee5\u4f7f\u7528\u5b50\u67e5\u8be2\u56fe\u7ed3\u6784\u7684\u54c8\u5e0c\u503c\u8fdb\u884c\u968f\u673a\u8bbf\u95ee\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8bef\u5dee\u6307\u6807\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5e76\u4e14\u5728PostgreSQL\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u51c6\u786e\u6027\u548c\u8fd0\u884c\u65f6\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0e\u4f20\u7edf\u7684\u57fa\u6570\u4f30\u8ba1\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u66f4\u4f4e\u7684\u8fd0\u884c\u65f6\u5f00\u9500\uff0c\u4e0e\u5176\u4ed6\u5b66\u4e60\u7684\u57fa\u6570\u4f30\u8ba1\u5668\u76f8\u6bd4\uff0c\u5927\u5927\u964d\u4f4e\u4e86\u8fd0\u8425\u6210\u672c\uff0c\u4ece\u800c\u5728Pareto\u524d\u6cbf\u63d0\u4f9b\u4e86\u6700\u5b9e\u7528\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03243", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.03243", "abs": "https://arxiv.org/abs/2510.03243", "authors": ["Yiheng Tao", "Yihe Zhang", "Matthew T. Dearing", "Xin Wang", "Yuping Fan", "Zhiling Lan"], "title": "PARS: Low-Latency LLM Serving via Pairwise Learning-to-Rank", "comment": null, "summary": "Efficient scheduling of LLM inference tasks is essential for achieving low\nlatency and high throughput, particularly with the growing use of\nreasoning-capable LLMs. Traditional strategies like First-Come-First-Serve\n(FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks\ndelay shorter ones queued behind them. In this paper, we introduce PARS, a\nprompt-aware LLM task scheduler that improves serving efficiency by\napproximating shortest-job-first (SJF) scheduling through pairwise ranking with\nmargin ranking loss. PARS focuses on impactful scheduling decisions and is\nseamlessly integrated into the state-of-the-art LLM serving system vLLM. It\neffectively predicts response-length-based task ordering, reducing latency with\nminimal overhead. Extensive experiments across multiple LLMs and real-world\ninference datasets show that PARS significantly improves performance, including\nfor reasoning workloads. Furthermore, our cross-model evaluations demonstrate\nthat the design generalizes well, enabling effective scheduling even when\npredictors are trained on different LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPARS\u7684prompt\u611f\u77e5LLM\u4efb\u52a1\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u8fd1\u4f3c\u6700\u77ed\u4f5c\u4e1a\u4f18\u5148\uff08SJF\uff09\u8c03\u5ea6\u6765\u63d0\u9ad8\u670d\u52a1\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u5148\u6765\u5148\u670d\u52a1\uff08FCFS\uff09\u8c03\u5ea6\u7b56\u7565\u5bb9\u6613\u53d7\u5230\u961f\u5934\u963b\u585e\uff08HOL\uff09\u7684\u5f71\u54cd\uff0c\u957f\u4efb\u52a1\u4f1a\u5ef6\u8fdf\u6392\u5728\u540e\u9762\u7684\u77ed\u4efb\u52a1\u3002", "method": "PARS\u901a\u8fc7pairwise ranking\u4e0emargin ranking loss\u76f8\u7ed3\u5408\uff0c\u9884\u6d4b\u57fa\u4e8e\u54cd\u5e94\u957f\u5ea6\u7684\u4efb\u52a1\u6392\u5e8f\u3002", "result": "PARS\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5305\u62ec\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PARS\u80fd\u591f\u6709\u6548\u9884\u6d4b\u57fa\u4e8e\u54cd\u5e94\u957f\u5ea6\u7684\u4efb\u52a1\u6392\u5e8f\uff0c\u51cf\u5c11\u5ef6\u8fdf\u4e14\u5f00\u9500\u6700\u5c0f\u3002"}}
{"id": "2510.03285", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03285", "abs": "https://arxiv.org/abs/2510.03285", "authors": ["Su Kara", "Fazle Faisal", "Suman Nath"], "title": "WAREX: Web Agent Reliability Evaluation on Existing Benchmarks", "comment": null, "summary": "Recent advances in browser-based LLM agents have shown promise for automating\ntasks ranging from simple form filling to hotel booking or online shopping.\nCurrent benchmarks measure agent performance in controlled environments, such\nas containers or stable networks, where websites behave deterministically.\nHowever, in the real world, users access websites over networks and HTTPS\nconnections that introduce instability from multiple sources: client-side,\nserver-side issues or broader system failures. Moreover, live websites are\nprone to web attacks such Cross-Site Scripting, as well as general site\nmodifications which can cause unexpected or malicious pop-ups or improper\nfunctionality. To address this gap, we present WAREX: Web Agent Reliability\nEvaluation on Existing Benchmarks. We measure the impact of WAREX across three\npopular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that\nintroducing WAREX leads to significant drops in task success rates,\nhighlighting the limited robustness of state-of-the-art agents.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aWAREX\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30Web\u4ee3\u7406\u5728\u771f\u5b9e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u4ee3\u7406\u5728\u4e0d\u7a33\u5b9a\u548c\u5b58\u5728\u5b89\u5168\u98ce\u9669\u7684\u771f\u5b9e\u7f51\u7ad9\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709Web\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u672a\u8003\u8651\u771f\u5b9e\u7f51\u7edc\u73af\u5883\u7684\u4e0d\u7a33\u5b9a\u6027\u53ca\u5b89\u5168\u98ce\u9669\uff0c\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30\u4ee3\u7406\u7684\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u5728WebArena\u3001WebVoyager\u548cREAL\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f15\u5165WAREX\u6765\u8bc4\u4f30\u4ee3\u7406\u7684\u6027\u80fd\u3002", "result": "\u5f15\u5165WAREX\u540e\uff0c\u4efb\u52a1\u6210\u529f\u7387\u663e\u8457\u4e0b\u964d\uff0c\u8868\u660e\u73b0\u6709\u4ee3\u7406\u7684\u9c81\u68d2\u6027\u6709\u9650\u3002", "conclusion": "\u73b0\u6709Web\u4ee3\u7406\u5728\u771f\u5b9e\u7f51\u7edc\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5176\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.04012", "categories": ["cs.IR", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2510.04012", "abs": "https://arxiv.org/abs/2510.04012", "authors": ["David Rogers", "Valerio Mariani", "Cong Wang", "Ryan Coffee", "Wilko Kroeger", "Murali Shankar", "Hans Thorsten Schwander", "Tom Beck", "Fr\u00e9d\u00e9ric Poitevin", "Jana Thayer"], "title": "The LCLStream Ecosystem for Multi-Institutional Dataset Exploration", "comment": "3 figures", "summary": "We describe a new end-to-end experimental data streaming framework designed\nfrom the ground up to support new types of applications -- AI training,\nextremely high-rate X-ray time-of-flight analysis, crystal structure\ndetermination with distributed processing, and custom data science applications\nand visualizers yet to be created. Throughout, we use design choices merging\ncloud microservices with traditional HPC batch execution models for security\nand flexibility. This project makes a unique contribution to the DOE Integrated\nResearch Infrastructure (IRI) landscape. By creating a flexible, API-driven\ndata request service, we address a significant need for high-speed data\nstreaming sources for the X-ray science data analysis community. With the\ncombination of data request API, mutual authentication web security framework,\njob queue system, high-rate data buffer, and complementary nature to facility\ninfrastructure, the LCLStreamer framework has prototyped and implemented\nseveral new paradigms critical for future generation experiments.", "AI": {"tldr": "LCLStreamer\u662f\u4e00\u4e2a\u65b0\u7684\u7aef\u5230\u7aef\u5b9e\u9a8c\u6570\u636e\u6d41\u6846\u67b6\uff0c\u65e8\u5728\u652f\u6301\u65b0\u578b\u5e94\u7528\uff0c\u5982AI\u8bad\u7ec3\u548c\u9ad8\u901a\u91cfX\u5c04\u7ebf\u5206\u6790\u3002", "motivation": "\u4e3aX\u5c04\u7ebf\u79d1\u5b66\u6570\u636e\u5206\u6790\u793e\u533a\u63d0\u4f9b\u9ad8\u901f\u6570\u636e\u6d41\u6e90\uff0c\u6ee1\u8db3\u65e5\u76ca\u589e\u957f\u7684\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u4e91\u5fae\u670d\u52a1\u4e0e\u4f20\u7edfHPC\u6279\u5904\u7406\u6267\u884c\u6a21\u578b\uff0c\u5b9e\u73b0\u5b89\u5168\u6027\u548c\u7075\u6d3b\u6027\u3002", "result": "LCLStreamer\u6846\u67b6\u5df2\u4e3a\u672a\u6765\u5b9e\u9a8c\u539f\u578b\u8bbe\u8ba1\u5e76\u5b9e\u65bd\u4e86\u51e0\u4e2a\u5173\u952e\u7684\u65b0\u8303\u4f8b\u3002", "conclusion": "LCLStreamer\u901a\u8fc7\u5176\u7075\u6d3b\u7684API\u9a71\u52a8\u6570\u636e\u8bf7\u6c42\u670d\u52a1\uff0c\u4e3aDOE\u96c6\u6210\u7814\u7a76\u57fa\u7840\u8bbe\u65bd\u505a\u51fa\u4e86\u72ec\u7279\u7684\u8d21\u732e\u3002"}}
{"id": "2510.03323", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03323", "abs": "https://arxiv.org/abs/2510.03323", "authors": ["Ge Chang", "Jinbo Su", "Jiacheng Liu", "Pengfei Yang", "Yuhao Shang", "Huiwen Zheng", "Hongli Ma", "Yan Liang", "Yuanchun Li", "Yunxin Liu"], "title": "Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision", "comment": null, "summary": "A significant portion of real-world data is inherently represented as textual\ngraphs, and integrating these graphs into large language models (LLMs) is\npromising to enable complex graph-based question answering. However, a key\nchallenge in LLM-based textual graph QA systems lies in graph retrieval, i.e.,\nhow to retrieve relevant content from large graphs that is sufficiently\ninformative while remaining compact for the LLM context. Existing retrievers\nsuffer from poor performance since they either rely on shallow embedding\nsimilarity or employ interactive retrieving policies that demand excessive data\nlabeling and training cost. To address these issues, we present Graph-$S^3$, an\nagentic textual graph reasoning framework that employs an LLM-based retriever\ntrained with synthetic stepwise supervision. Instead of rewarding the agent\nbased on the final answers, which may lead to sparse and unstable training\nsignals, we propose to closely evaluate each step of the retriever based on\noffline-extracted golden subgraphs. Our main techniques include a data\nsynthesis pipeline to extract the golden subgraphs for reward generation and a\ntwo-stage training scheme to learn the interactive graph exploration policy\nbased on the synthesized rewards. Based on extensive experiments on three\ncommon datasets in comparison with seven strong baselines, our approach\nachieves an average improvement of 8.1\\% in accuracy and 9.7\\% in F$_1$ score.\nThe advantage is even higher in more complicated multi-hop reasoning tasks. Our\ncode will be open-sourced.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Graph-$S^3$ \u7684 agentic \u6587\u672c\u56fe\u63a8\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8e LLM \u7684\u6587\u672c\u56fe\u95ee\u7b54\u7cfb\u7edf\u4e2d\u56fe\u68c0\u7d22\u7684\u6311\u6218\u3002\u8be5\u6846\u67b6\u4f7f\u7528\u5408\u6210\u7684\u9010\u6b65\u76d1\u7763\u8bad\u7ec3\u4e00\u4e2a\u57fa\u4e8e LLM \u7684\u68c0\u7d22\u5668\u3002", "motivation": "\u73b0\u6709\u7684\u68c0\u7d22\u5668\u6027\u80fd\u8f83\u5dee\uff0c\u56e0\u4e3a\u5b83\u4eec\u8981\u4e48\u4f9d\u8d56\u4e8e\u6d45\u5c42\u5d4c\u5165\u76f8\u4f3c\u6027\uff0c\u8981\u4e48\u91c7\u7528\u9700\u8981\u8fc7\u5ea6\u6570\u636e\u6807\u8bb0\u548c\u8bad\u7ec3\u6210\u672c\u7684\u4ea4\u4e92\u5f0f\u68c0\u7d22\u7b56\u7565\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\u4e00\u4e2a\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u7528\u4e8e\u63d0\u53d6\u9ec4\u91d1\u5b50\u56fe\u4ee5\u751f\u6210\u5956\u52b1\uff0c\u4ee5\u53ca\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\uff0c\u7528\u4e8e\u5b66\u4e60\u57fa\u4e8e\u5408\u6210\u5956\u52b1\u7684\u4ea4\u4e92\u5f0f\u56fe\u63a2\u7d22\u7b56\u7565\u3002", "result": "\u5728\u4e09\u4e2a\u5e38\u89c1\u6570\u636e\u96c6\u4e0a\u4e0e\u4e03\u4e2a\u5f3a\u5927\u7684\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u65b9\u9762\u5e73\u5747\u63d0\u9ad8\u4e86 8.1%\uff0c\u5728 F1 \u5206\u6570\u65b9\u9762\u5e73\u5747\u63d0\u9ad8\u4e86 9.7%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u66f4\u590d\u6742\u7684\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e2d\u5177\u6709\u66f4\u9ad8\u7684\u4f18\u52bf\u3002"}}
{"id": "2510.03292", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03292", "abs": "https://arxiv.org/abs/2510.03292", "authors": ["Do\u011fanay Demir", "\u0130lknur Durgar Elkahlout"], "title": "Visualizing Celebrity Dynamics in Video Content: A Proposed Approach Using Face Recognition Timestamp Data", "comment": null, "summary": "In an era dominated by video content, understanding its structure and\ndynamics has become increasingly important. This paper presents a hybrid\nframework that combines a distributed multi-GPU inference system with an\ninteractive visualization platform for analyzing celebrity dynamics in video\nepisodes. The inference framework efficiently processes large volumes of video\ndata by leveraging optimized ONNX models, heterogeneous batch inference, and\nhigh-throughput parallelism, ensuring scalable generation of timestamped\nappearance records. These records are then transformed into a comprehensive\nsuite of visualizations, including appearance frequency charts, duration\nanalyses, pie charts, co-appearance matrices, network graphs, stacked area\ncharts, seasonal comparisons, and heatmaps. Together, these visualizations\nprovide multi-dimensional insights into video content, revealing patterns in\ncelebrity prominence, screen-time distribution, temporal dynamics,\nco-appearance relationships, and intensity across episodes and seasons. The\ninteractive nature of the system allows users to dynamically explore data,\nidentify key moments, and uncover evolving relationships between individuals.\nBy bridging distributed recognition with structured, visually-driven analytics,\nthis work enables new possibilities for entertainment analytics, content\ncreation strategies, and audience engagement studies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5206\u5e03\u5f0f\u591aGPU\u63a8\u7406\u7cfb\u7edf\u548c\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5e73\u53f0\uff0c\u7528\u4e8e\u5206\u6790\u89c6\u9891\u7247\u6bb5\u4e2d\u7684\u540d\u4eba\u52a8\u6001\u3002", "motivation": "\u5728\u89c6\u9891\u5185\u5bb9\u5360\u4e3b\u5bfc\u5730\u4f4d\u7684\u65f6\u4ee3\uff0c\u7406\u89e3\u5176\u7ed3\u6784\u548c\u52a8\u6001\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002", "method": "\u8be5\u63a8\u7406\u6846\u67b6\u901a\u8fc7\u5229\u7528\u4f18\u5316\u7684ONNX\u6a21\u578b\u3001\u5f02\u6784\u6279\u5904\u7406\u63a8\u7406\u548c\u9ad8\u541e\u5410\u91cf\u5e76\u884c\u6027\u6765\u9ad8\u6548\u5904\u7406\u5927\u91cf\u89c6\u9891\u6570\u636e\uff0c\u786e\u4fdd\u53ef\u6269\u5c55\u5730\u751f\u6210\u5e26\u65f6\u95f4\u6233\u7684\u5916\u89c2\u8bb0\u5f55\u3002\u7136\u540e\uff0c\u8fd9\u4e9b\u8bb0\u5f55\u88ab\u8f6c\u6362\u6210\u4e00\u4e2a\u5168\u9762\u7684\u53ef\u89c6\u5316\u5957\u4ef6\uff0c\u5305\u62ec\u5916\u89c2\u9891\u7387\u56fe\u3001\u6301\u7eed\u65f6\u95f4\u5206\u6790\u3001\u997c\u56fe\u3001\u5171\u540c\u5916\u89c2\u77e9\u9635\u3001\u7f51\u7edc\u56fe\u3001\u5806\u79ef\u9762\u79ef\u56fe\u3001\u5b63\u8282\u6027\u6bd4\u8f83\u548c\u70ed\u56fe\u3002", "result": "\u8fd9\u4e9b\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u5bf9\u89c6\u9891\u5185\u5bb9\u7684\u591a\u7ef4\u6d1e\u5bdf\uff0c\u63ed\u793a\u4e86\u540d\u4eba\u5728\u5267\u96c6\u548c\u5b63\u5ea6\u4e2d\u7684\u7a81\u51fa\u7a0b\u5ea6\u3001\u5c4f\u5e55\u65f6\u95f4\u5206\u5e03\u3001\u65f6\u95f4\u52a8\u6001\u3001\u5171\u540c\u51fa\u73b0\u5173\u7cfb\u548c\u5f3a\u5ea6\u6a21\u5f0f\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5206\u5e03\u5f0f\u8bc6\u522b\u4e0e\u7ed3\u6784\u5316\u7684\u3001\u89c6\u89c9\u9a71\u52a8\u7684\u5206\u6790\u8054\u7cfb\u8d77\u6765\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5a31\u4e50\u5206\u6790\u3001\u5185\u5bb9\u521b\u4f5c\u7b56\u7565\u548c\u89c2\u4f17\u53c2\u4e0e\u5ea6\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2510.04014", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.04014", "abs": "https://arxiv.org/abs/2510.04014", "authors": ["Kai Cao", "Yucong Duan", "Wensheng Gan"], "title": "Dual Pruning and Sorting-Free Overestimation for Average-Utility Sequential Pattern Mining", "comment": "preprint, 13 figures, 4 tables", "summary": "In a quantitative sequential database, numerous efficient algorithms have\nbeen developed for high-utility sequential pattern mining (HUSPM). HUSPM\nestablishes a relationship between frequency and significance in the real world\nand reflects more crucial information than frequent pattern mining. However,\nhigh average-utility sequential pattern mining (HAUSPM) is deemed fairer and\nmore valuable than HUSPM. It provides a reasonable measure for longer patterns\nby considering their length. In contrast to scenarios in retail business\nanalysis, some pattern mining applications, such as cybersecurity or artificial\nintelligence (AI), often involve much longer sequences. Consequently, pruning\nstrategies can exert a more pronounced impact on efficiency. This paper\nproposes a novel algorithm named HAUSP-PG, which adopts two complementary\nstrategies to independently process pattern prefixes and remaining sequences,\nthereby achieving a dual pruning effect. Additionally, the proposed method\ncalculates average utility upper bounds without requiring item sorting,\nsignificantly reducing computational time and memory consumption compared to\nalternative approaches. Through experiments conducted on both real-life and\nsynthetic datasets, we demonstrate that the proposed algorithm could achieve\nsatisfactory performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a HAUSP-PG \u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u6316\u6398\u9ad8\u5e73\u5747\u6548\u7528\u5e8f\u5217\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u7684\u9ad8\u5b9e\u7528\u6027\u5e8f\u5217\u6a21\u5f0f\u6316\u6398\u7b97\u6cd5\u5728\u9ad8\u4ef7\u503c\u957f\u5e8f\u5217\u7684\u6316\u6398\u4e2d\u6548\u7387\u8f83\u4f4e\uff0c\u4e14\u9ad8\u5e73\u5747\u6548\u7528\u5e8f\u5217\u6a21\u5f0f\u6316\u6398\u6bd4\u9ad8\u5b9e\u7528\u6027\u5e8f\u5217\u6a21\u5f0f\u6316\u6398\u66f4\u516c\u5e73\u3001\u66f4\u6709\u4ef7\u503c\u3002", "method": "\u8be5\u7b97\u6cd5\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u7b56\u7565\u72ec\u7acb\u5904\u7406\u6a21\u5f0f\u524d\u7f00\u548c\u5269\u4f59\u5e8f\u5217\uff0c\u5b9e\u73b0\u53cc\u91cd\u526a\u679d\u6548\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u9879\u76ee\u6392\u5e8f\u7684\u5e73\u5747\u6548\u7528\u4e0a\u754c\u8ba1\u7b97\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5747\u80fd\u53d6\u5f97\u4ee4\u4eba\u6ee1\u610f\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684 HAUSP-PG \u7b97\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u9ad8\u5e73\u5747\u6548\u7528\u5e8f\u5217\u6a21\u5f0f\u7684\u6316\u6398\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\u3002"}}
{"id": "2510.03244", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03244", "abs": "https://arxiv.org/abs/2510.03244", "authors": ["Yanlong Wang", "Hang Yu", "Jian Xu", "Fei Ma", "Hongkang Zhang", "Tongtong Feng", "Zijian Zhang", "Shao-Lun Huang", "Danny Dongning Sun", "Xiao-Ping Zhang"], "title": "VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion", "comment": null, "summary": "Large time series foundation models often adopt channel-independent\narchitectures to handle varying data dimensions, but this design ignores\ncrucial cross-channel dependencies. Concurrently, existing multimodal\napproaches have not fully exploited the power of large vision models (LVMs) to\ninterpret spatiotemporal data. Additionally, there remains significant\nunexplored potential in leveraging the advantages of information extraction\nfrom different modalities to enhance time series forecasting performance. To\naddress these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO\nuniquely renders multivariate time series into image, enabling pre-trained LVM\nto extract complex cross-channel patterns that are invisible to\nchannel-independent models. These visual features are then aligned and fused\nwith representations from the time series modality. By freezing the LVM and\ntraining only 7.45% of its parameters, VIFO achieves competitive performance on\nmultiple benchmarks, offering an efficient and effective solution for capturing\ncross-variable relationships in", "AI": {"tldr": "VIFO: a cross-modal forecasting model that renders multivariate time series into image, enabling pre-trained LVM to extract complex cross-channel patterns.", "motivation": "Existing models ignore crucial cross-channel dependencies and have not fully exploited the power of large vision models (LVMs) to interpret spatiotemporal data.", "method": "The paper proposes VIFO, which renders time series into images, uses a pre-trained LVM to extract visual features, and fuses these with time series representations.", "result": "VIFO achieves competitive performance on multiple benchmarks by freezing the LVM and training only 7.45% of its parameters.", "conclusion": "VIFO offers an efficient and effective solution for capturing cross-variable relationships."}}
{"id": "2510.03377", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03377", "abs": "https://arxiv.org/abs/2510.03377", "authors": ["Ahmed Missaoui", "Cemalettin Ozturk", "Barry O'Sullivan"], "title": "Refined Iterated Pareto Greedy for Energy-aware Hybrid Flowshop Scheduling with Blocking Constraints", "comment": null, "summary": "The scarcity of non-renewable energy sources, geopolitical problems in its\nsupply, increasing prices, and the impact of climate change, force the global\neconomy to develop more energy-efficient solutions for their operations. The\nManufacturing sector is not excluded from this challenge as one of the largest\nconsumers of energy. Energy-efficient scheduling is a method that attracts\nmanufacturing companies to reduce their consumption as it can be quickly\ndeployed and can show impact immediately. In this study, the hybrid flow shop\nscheduling problem with blocking constraint (BHFS) is investigated in which we\nseek to minimize the latest completion time (i.e. makespan) and overall energy\nconsumption, a typical manufacturing setting across many industries from\nautomotive to pharmaceutical. Energy consumption and the latest completion time\nof customer orders are usually conflicting objectives. Therefore, we first\nformulate the problem as a novel multi-objective mixed integer programming\n(MIP) model and propose an augmented epsilon-constraint method for finding the\nPareto-optimal solutions. Also, an effective multi-objective metaheuristic\nalgorithm. Refined Iterated Pareto Greedy (RIPG), is developed to solve large\ninstances in reasonable time. Our proposed methods are benchmarked using small,\nmedium, and large-size instances to evaluate their efficiency. Two well-known\nalgorithms are adopted for comparing our novel approaches. The computational\nresults show the effectiveness of our method.", "AI": {"tldr": "\u7814\u7a76\u6df7\u5408\u6d41\u6c34\u8f66\u95f4\u8c03\u5ea6\u95ee\u9898\uff0c\u76ee\u6807\u662f\u6700\u5c0f\u5316\u5b8c\u5de5\u65f6\u95f4\u548c\u603b\u80fd\u8017\u3002", "motivation": "\u4e0d\u53ef\u518d\u751f\u80fd\u6e90\u7684\u7a00\u7f3a\u3001\u5730\u7f18\u653f\u6cbb\u95ee\u9898\u3001\u4ef7\u683c\u4e0a\u6da8\u548c\u6c14\u5019\u53d8\u5316\u8feb\u4f7f\u5168\u7403\u7ecf\u6d4e\u5f00\u53d1\u66f4\u8282\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5236\u9020\u4e1a\u662f\u80fd\u6e90\u6d88\u8017\u5927\u6237\uff0c\u9762\u4e34\u964d\u4f4e\u80fd\u8017\u7684\u6311\u6218\u3002\u8282\u80fd\u8c03\u5ea6\u662f\u4e00\u79cd\u6709\u5438\u5f15\u529b\u7684\u65b9\u6cd5\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u5feb\u901f\u90e8\u7f72\u5e76\u7acb\u5373\u4ea7\u751f\u5f71\u54cd\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u591a\u76ee\u6807\u6df7\u5408\u6574\u6570\u89c4\u5212 (MIP) \u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684 epsilon \u7ea6\u675f\u65b9\u6cd5\u6765\u5bfb\u627e\u5e15\u7d2f\u6258\u6700\u4f18\u89e3\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u6709\u6548\u7684\u591a\u76ee\u6807\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u5373\u6539\u8fdb\u7684\u8fed\u4ee3\u5e15\u7d2f\u6258\u8d2a\u5a6a (RIPG)\uff0c\u4ee5\u5728\u5408\u7406\u7684\u65f6\u95f4\u5185\u89e3\u51b3\u5927\u578b\u5b9e\u4f8b\u3002", "result": "\u4f7f\u7528\u5c0f\u578b\u3001\u4e2d\u578b\u548c\u5927\u578b\u5b9e\u4f8b\u5bf9\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u8bc4\u4f30\u5b83\u4eec\u7684\u6548\u7387\u3002\u91c7\u7528\u4e86\u4e24\u79cd\u4f17\u6240\u5468\u77e5\u7684\u7b97\u6cd5\u6765\u6bd4\u8f83\u6211\u4eec\u7684\u65b0\u65b9\u6cd5\u3002\u8ba1\u7b97\u7ed3\u679c\u8868\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u89e3\u51b3\u6df7\u5408\u6d41\u6c34\u8f66\u95f4\u8c03\u5ea6\u95ee\u9898\u4e0a\u662f\u6709\u6548\u7684\uff0c\u53ef\u4ee5\u6700\u5c0f\u5316\u5b8c\u5de5\u65f6\u95f4\u548c\u603b\u80fd\u8017\u3002"}}
{"id": "2510.04096", "categories": ["cs.IR", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04096", "abs": "https://arxiv.org/abs/2510.04096", "authors": ["Tommy Mordo", "Sagie Dekel", "Omer Madmon", "Moshe Tennenholtz", "Oren Kurland"], "title": "RLRF: Competitive Search Agent Design via Reinforcement Learning from Ranker Feedback", "comment": null, "summary": "Competitive search is a setting where document publishers modify them to\nimprove their ranking in response to a query. Recently, publishers have\nincreasingly leveraged LLMs to generate and modify competitive content. We\nintroduce Reinforcement Learning from Ranker Feedback (RLRF), a framework that\ntrains LLMs using preference datasets derived from ranking competitions. The\ngoal of a publisher (LLM-based) agent is to optimize content for improved\nranking while accounting for the strategies of competing agents. We generate\nthe datasets using approaches that do not rely on human-authored data. We show\nthat our proposed agents consistently and substantially outperform previously\nsuggested approaches for LLM-based competitive document modification. We\nfurther show that our agents are effective with ranking functions they were not\ntrained for (i.e., out of distribution) and they adapt to strategic opponents.\nThese findings provide support to the significant potential of using\nreinforcement learning in competitive search.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u5373\u57fa\u4e8e\u6392\u5e8f\u5668\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLRF\uff09\uff0c\u7528\u4e8e\u8bad\u7ec3LLM\u4ee5\u6539\u8fdb\u7ade\u4e89\u6027\u641c\u7d22\u4e2d\u7684\u6587\u6863\u6392\u5e8f\u3002", "motivation": "\u6587\u6863\u53d1\u5e03\u8005\u8d8a\u6765\u8d8a\u591a\u5730\u5229\u7528LLM\u6765\u751f\u6210\u548c\u4fee\u6539\u7ade\u4e89\u6027\u5185\u5bb9\uff0c\u4ee5\u63d0\u9ad8\u5176\u5728\u67e5\u8be2\u4e2d\u7684\u6392\u540d\u3002", "method": "\u4f7f\u7528\u4ece\u6392\u540d\u7ade\u4e89\u4e2d\u83b7\u5f97\u7684\u504f\u597d\u6570\u636e\u96c6\u6765\u8bad\u7ec3LLM\u3002", "result": "\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4e14\u5927\u5e45\u4f18\u4e8e\u4ee5\u524d\u63d0\u51fa\u7684\u57fa\u4e8eLLM\u7684\u7ade\u4e89\u6027\u6587\u6863\u4fee\u6539\u65b9\u6cd5\uff0c\u5e76\u4e14\u5bf9\u672a\u8bad\u7ec3\u8fc7\u7684\u6392\u5e8f\u51fd\u6570\u6709\u6548\uff0c\u80fd\u591f\u9002\u5e94\u6218\u7565\u5bf9\u624b\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u5728\u7ade\u4e89\u6027\u641c\u7d22\u4e2d\u5177\u6709\u5de8\u5927\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.03384", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03384", "abs": "https://arxiv.org/abs/2510.03384", "authors": ["Arjun Arunasalam", "Madison Pickering", "Z. Berkay Celik", "Blase Ur"], "title": "Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks", "comment": null, "summary": "Large language models (LLMs) can underpin AI assistants that help users with\neveryday tasks, such as by making recommendations or performing basic\ncomputation. Despite AI assistants' promise, little is known about the implicit\nvalues these assistants display while completing subjective everyday tasks.\nHumans may consider values like environmentalism, charity, and diversity. To\nwhat extent do LLMs exhibit these values in completing everyday tasks? How do\nthey compare with humans? We answer these questions by auditing how six popular\nLLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human\ncrowdworkers from the US. We find LLMs often do not align with humans, nor with\nother LLMs, in the implicit values exhibited.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b8c\u6210\u65e5\u5e38\u4efb\u52a1\u65f6\u6240\u5c55\u793a\u7684\u4ef7\u503c\u89c2\u4e0e\u4eba\u7c7b\u548c\u5176\u5b83LLM\u4e0d\u4e00\u81f4\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b8c\u6210\u4e3b\u89c2\u65e5\u5e38\u4efb\u52a1\u65f6\u6240\u4f53\u73b0\u7684\u9690\u542b\u4ef7\u503c\u89c2\uff0c\u4ee5\u53ca\u5b83\u4eec\u4e0e\u4eba\u7c7b\u7684\u6bd4\u8f83\u3002", "method": "\u5ba1\u8ba1\u516d\u4e2a\u6d41\u884c\u7684LLM\u5b8c\u621030\u4e2a\u65e5\u5e38\u4efb\u52a1\uff0c\u5e76\u5c06LLM\u4e0e\u6765\u81ea\u7f8e\u56fd\u7684100\u540d\u4eba\u7c7b\u4f17\u5305\u5de5\u4f5c\u8005\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "LLM\u5728\u9690\u542b\u4ef7\u503c\u89c2\u65b9\u9762\u901a\u5e38\u4e0e\u4eba\u7c7b\u548c\u5176\u4ed6LLM\u4e0d\u4e00\u81f4\u3002", "conclusion": "LLM\u5728\u65e5\u5e38\u4efb\u52a1\u4e2d\u4f53\u73b0\u7684\u4ef7\u503c\u89c2\u5b58\u5728\u5dee\u5f02\uff0c\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u4e0d\u4e00\u81f4\u3002"}}
{"id": "2510.03294", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03294", "abs": "https://arxiv.org/abs/2510.03294", "authors": ["Saanvi Kataria"], "title": "Domain-Robust Marine Plastic Detection Using Vision Models", "comment": "16 pages, 5 figures, 1 table", "summary": "Marine plastic pollution is a pressing environmental threat, making reliable\nautomation for underwater debris detection essential. However, vision systems\ntrained on one dataset often degrade on new imagery due to domain shift. This\nstudy benchmarks models for cross-domain robustness, training convolutional\nneural networks - CNNs (MobileNetV2, ResNet-18, EfficientNet-B0) and vision\ntransformers (DeiT-Tiny, ViT-B16) on a labeled underwater dataset and then\nevaluates them on a balanced cross-domain test set built from plastic-positive\nimages drawn from a different source and negatives from the training domain.\nTwo zero-shot models were assessed, CLIP ViT-L14 and Google's Gemini 2.0 Flash,\nthat leverage pretraining to classify images without fine-tuning. Results show\nthe lightweight MobileNetV2 delivers the strongest cross-domain performance (F1\n0.97), surpassing larger models. All fine-tuned models achieved high Precision\n(around 99%), but differ in Recall, indicating varying sensitivity to plastic\ninstances. Zero-shot CLIP is comparatively sensitive (Recall around 80%) yet\nprone to false positives (Precision around 56%), whereas Gemini exhibits the\ninverse profile (Precision around 99%, Recall around 81%). Error analysis\nhighlights recurring confusions with coral textures, suspended particulates,\nand specular glare. Overall, compact CNNs with supervised training can\ngeneralize effectively for cross-domain underwater detection, while large\npretrained vision-language models provide complementary strengths.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u6c34\u4e0b\u5851\u6599\u5783\u573e\u68c0\u6d4b\u7684\u8de8\u57df\u9c81\u68d2\u6027\u6a21\u578b\u3002", "motivation": "\u6c34\u4e0b\u5851\u6599\u6c61\u67d3\u662f\u4e00\u4e2a\u7d27\u8feb\u7684\u73af\u5883\u5a01\u80c1\uff0c\u9700\u8981\u53ef\u9760\u7684\u81ea\u52a8\u5316\u6c34\u4e0b\u5783\u573e\u68c0\u6d4b\u6280\u672f\u3002\u4f46\u89c6\u89c9\u7cfb\u7edf\u5728\u4e00\u4e2a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\uff0c\u5728\u65b0\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\u4f1a\u56e0\u57df\u504f\u79fb\u800c\u4e0b\u964d\u3002", "method": "\u5728\u6807\u8bb0\u7684\u6c34\u4e0b\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u89c6\u89c9Transformer\uff0c\u5e76\u5728\u4e00\u4e2a\u5e73\u8861\u7684\u8de8\u57df\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u5b83\u4eec\u3002\u540c\u65f6\u8bc4\u4f30\u4e86\u4e24\u4e2a\u96f6\u6837\u672c\u6a21\u578b\uff1aCLIP ViT-L14 \u548c Google \u7684 Gemini 2.0 Flash\u3002", "result": "MobileNetV2 \u63d0\u4f9b\u4e86\u6700\u5f3a\u7684\u8de8\u57df\u6027\u80fd\uff08F1 0.97\uff09\uff0c\u8d85\u8fc7\u4e86\u66f4\u5927\u7684\u6a21\u578b\u3002\u6240\u6709\u5fae\u8c03\u7684\u6a21\u578b\u90fd\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\uff08\u7ea6 99%\uff09\uff0c\u4f46\u5728\u53ec\u56de\u7387\u4e0a\u6709\u6240\u4e0d\u540c\u3002\u96f6\u6837\u672c CLIP \u5177\u6709\u76f8\u5bf9\u8f83\u9ad8\u7684\u53ec\u56de\u7387\uff08\u7ea6 80%\uff09\uff0c\u4f46\u5bb9\u6613\u51fa\u73b0\u5047\u9633\u6027\uff08\u7cbe\u5ea6\u7ea6 56%\uff09\uff0c\u800c Gemini \u5219\u8868\u73b0\u51fa\u76f8\u53cd\u7684\u7279\u5f81\uff08\u7cbe\u5ea6\u7ea6 99%\uff0c\u53ec\u56de\u7387\u7ea6 81%\uff09\u3002", "conclusion": "\u7d27\u51d1\u578b CNN \u914d\u5408\u76d1\u7763\u8bad\u7ec3\u53ef\u4ee5\u6709\u6548\u5730\u63a8\u5e7f\u7528\u4e8e\u8de8\u57df\u6c34\u4e0b\u68c0\u6d4b\uff0c\u800c\u5927\u578b\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5219\u63d0\u4f9b\u4e92\u8865\u4f18\u52bf\u3002"}}
{"id": "2510.04249", "categories": ["cs.DB", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.04249", "abs": "https://arxiv.org/abs/2510.04249", "authors": ["Yu-Ting Lin", "Hsin-Po Wang"], "title": "Ambidextrous Degree Sequence Bounds for Pessimistic Cardinality Estimation", "comment": "25 pages, 16 figures", "summary": "In a large database system, upper-bounding the cardinality of a join query is\na crucial task called $\\textit{pessimistic cardinality estimation}$. Recently,\nAbo Khamis, Nakos, Olteanu, and Suciu unified related works into the following\ndexterous framework. Step 1: Let $(X_1, \\dotsc, X_n)$ be a random row of the\njoin, equating $H(X_1, \\dotsc, X_n)$ to the log of the join cardinality. Step\n2: Upper-bound $H(X_1, \\dotsc, X_n)$ using Shannon-type inequalities such as\n$H(X, Y, Z) \\le H(X) + H(Y|X) + H(Z|Y)$. Step 3: Upper-bound $H(X_i) + p H(X_j\n| X_i)$ using the $p$-norm of the degree sequence of the underlying graph of a\nrelation.\n  While old bound in step 3 count \"claws $\\in$\" in the underlying graph, we\nproposed $\\textit{ambidextrous}$ bounds that count \"claw pairs\n${\\ni}\\!{-}\\!{\\in}$\". The new bounds are provably not looser and empirically\ntighter: they overestimate by $x^{3/4}$ times when the old bounds overestimate\nby $x$ times. An example is counting friend triples in the\n$\\texttt{com-Youtube}$ dataset, the best dexterous bound is $1.2 \\cdot 10^9$,\nthe best ambidextrous bound is $5.1 \\cdot 10^8$, and the actual cardinality is\n$1.8 \\cdot 10^7$.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u6570\u4f30\u8ba1\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u8fde\u63a5\u67e5\u8be2\u4e2d\u66f4\u51c6\u786e\u5730\u4f30\u8ba1\u8fde\u63a5\u5927\u5c0f\u7684\u4e0a\u9650\u3002", "motivation": "\u5728\u5927\u578b\u6570\u636e\u5e93\u7cfb\u7edf\u4e2d\uff0c\u60b2\u89c2\u57fa\u6570\u4f30\u8ba1\u662f\u8bc4\u4f30\u8fde\u63a5\u67e5\u8be2\u57fa\u6570\u7684\u5173\u952e\u4efb\u52a1\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u5b58\u5728\u9ad8\u4f30\u7684\u95ee\u9898\u3002", "method": "\u8be5\u8bba\u6587\u5f15\u5165\u4e86\u201c\u7075\u5de7\u201d\u8fb9\u754c\u7684\u6982\u5ff5\uff0c\u901a\u8fc7\u8ba1\u7b97\u5e95\u5c42\u56fe\u4e2d\u7684\u201c\u722a\u5bf9\u201d\u6765\u6539\u8fdb\u57fa\u6570\u4f30\u8ba1\u3002", "result": "\u65b0\u63d0\u51fa\u7684\u201c\u7075\u5de7\u201d\u8fb9\u754c\u5728\u7406\u8bba\u4e0a\u4e0d\u6bd4\u65e7\u8fb9\u754c\u5dee\uff0c\u5e76\u4e14\u5728\u5b9e\u9a8c\u4e0a\u66f4\u7d27\u5bc6\u3002\u4f8b\u5982\uff0c\u5728\u8ba1\u7b97com-Youtube\u6570\u636e\u96c6\u4e2d\u7684\u670b\u53cb\u4e09\u5143\u7ec4\u65f6\uff0c\u6700\u4f73\u7075\u5de7\u8fb9\u754c\u4e3a5.1 * 10^8\uff0c\u800c\u5b9e\u9645\u57fa\u6570\u4e3a1.8 * 10^7\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u65b0\u65b9\u6cd5\u80fd\u591f\u66f4\u6709\u6548\u5730\u8fdb\u884c\u57fa\u6570\u4f30\u8ba1\uff0c\u51cf\u5c11\u4e86\u9ad8\u4f30\u7684\u7a0b\u5ea6\u3002"}}
{"id": "2510.03245", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03245", "abs": "https://arxiv.org/abs/2510.03245", "authors": ["Ali Yavari", "Alireza Mohamadi", "Elham Beydaghi", "Rainer A. Leitgeb"], "title": "Frequency-Aware Model Parameter Explorer: A new attribution method for improving explainability", "comment": "Preprint", "summary": "Ensuring the reliability of deep neural networks (DNNs) in the presence of\nreal world noise and intentional perturbations remains a significant challenge.\nTo address this, attribution methods have been proposed, though their efficacy\nremains suboptimal and necessitates further refinement. In this paper, we\npropose a novel category of transferable adversarial attacks, called\ntransferable frequency-aware attacks, enabling frequency-aware exploration via\nboth high-and low-frequency components. Based on this type of attacks, we also\npropose a novel attribution method, named Frequency-Aware Model Parameter\nExplorer (FAMPE), which improves the explainability for DNNs. Relative to the\ncurrent state-of-the-art method AttEXplore, our FAMPE attains an average gain\nof 13.02% in Insertion Score, thereby outperforming existing approaches.\nThrough detailed ablation studies, we also investigate the role of both high-\nand low-frequency components in explainability.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u8fc1\u79fb\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f52\u56e0\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8DNN\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u566a\u58f0\u548c\u6545\u610f\u7684\u6270\u52a8\u4e0b\uff0c\u786e\u4fdd\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc(DNNs)\u7684\u53ef\u9760\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u7684\u6311\u6218\uff0c\u73b0\u6709\u7684\u5f52\u56e0\u65b9\u6cd5\u6548\u679c\u6b20\u4f73\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u53ef\u8fc1\u79fb\u5bf9\u6297\u653b\u51fb\uff0c\u5b9e\u73b0\u901a\u8fc7\u9ad8\u9891\u548c\u4f4e\u9891\u5206\u91cf\u8fdb\u884c\u9891\u7387\u611f\u77e5\u63a2\u7d22\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5f52\u56e0\u65b9\u6cd5\uff0c\u540d\u4e3a\u9891\u7387\u611f\u77e5\u6a21\u578b\u53c2\u6570\u63a2\u7d22\u5668(FAMPE)\u3002", "result": "FAMPE\u7684\u63d2\u5165\u5206\u6570\u5e73\u5747\u63d0\u9ad8\u4e8613.02%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u8be6\u7ec6\u7684\u6d88\u878d\u7814\u7a76\uff0c \u0438\u0441\u0441\u043b\u0435\u0434\u0443\u0435\u0442\u0441\u044f\u9ad8\u9891\u548c\u4f4e\u9891\u5206\u91cf\u5728\u53ef\u89e3\u91ca\u6027\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2510.03399", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03399", "abs": "https://arxiv.org/abs/2510.03399", "authors": ["Xiaoyan Bai", "Aryan Shrivastava", "Ari Holtzman", "Chenhao Tan"], "title": "Know Thyself? On the Incapability and Implications of AI Self-Recognition", "comment": "Our code is available, see\n  https://github.com/ChicagoHAI/self-recognition", "summary": "Self-recognition is a crucial metacognitive capability for AI systems,\nrelevant not only for psychological analysis but also for safety, particularly\nin evaluative scenarios. Motivated by contradictory interpretations of whether\nmodels possess self-recognition (Panickssery et al., 2024; Davidson et al.,\n2024), we introduce a systematic evaluation framework that can be easily\napplied and updated. Specifically, we measure how well 10 contemporary larger\nlanguage models (LLMs) can identify their own generated text versus text from\nother models through two tasks: binary self-recognition and exact model\nprediction. Different from prior claims, our results reveal a consistent\nfailure in self-recognition. Only 4 out of 10 models predict themselves as\ngenerators, and the performance is rarely above random chance. Additionally,\nmodels exhibit a strong bias toward predicting GPT and Claude families. We also\nprovide the first evaluation of model awareness of their own and others'\nexistence, as well as the reasoning behind their choices in self-recognition.\nWe find that the model demonstrates some knowledge of its own existence and\nother models, but their reasoning reveals a hierarchical bias. They appear to\nassume that GPT, Claude, and occasionally Gemini are the top-tier models, often\nassociating high-quality text with them. We conclude by discussing the\nimplications of our findings on AI safety and future directions to develop\nappropriate AI self-awareness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u6211\u8bc6\u522b\u80fd\u529b\u7684\u6846\u67b6\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u8be5\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u5b58\u5728\u5bf9GPT\u548cClaude\u6a21\u578b\u7684\u504f\u89c1\u3002", "motivation": "\u5f53\u524d\u5bf9\u6a21\u578b\u662f\u5426\u5177\u5907\u81ea\u6211\u8bc6\u522b\u80fd\u529b\u5b58\u5728\u4e89\u8bae\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u4e8c\u5143\u81ea\u6211\u8bc6\u522b\u548c\u7cbe\u786e\u6a21\u578b\u9884\u6d4b\u4e24\u4e2a\u4efb\u52a1\uff0c\u8bc4\u4f3010\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u81ea\u8eab\u751f\u6210\u6587\u672c\u7684\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u591a\u6570\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u81ea\u8eab\u751f\u6210\u7684\u6587\u672c\uff0c\u4e14\u5b58\u5728\u5bf9GPT\u548cClaude\u6a21\u578b\u7684\u5f3a\u70c8\u504f\u89c1\u3002\u6a21\u578b\u5bf9\u81ea\u8eab\u548c\u5176\u4ed6\u6a21\u578b\u7684\u5b58\u5728\u6709\u4e00\u5b9a\u4e86\u89e3\uff0c\u4f46\u5176\u63a8\u7406\u663e\u793a\u51fa\u5c42\u7ea7\u504f\u89c1\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9\u4eba\u5de5\u667a\u80fd\u5b89\u5168\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u672a\u6765\u5e94\u81f4\u529b\u4e8e\u53d1\u5c55\u9002\u5f53\u7684\u4eba\u5de5\u667a\u80fd\u81ea\u6211\u610f\u8bc6\u3002"}}
{"id": "2510.04127", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04127", "abs": "https://arxiv.org/abs/2510.04127", "authors": ["Sean Moran"], "title": "Learning-Based Hashing for ANN Search: Foundations and Early Advances", "comment": null, "summary": "Approximate Nearest Neighbour (ANN) search is a fundamental problem in\ninformation retrieval, underpinning large-scale applications in computer\nvision, natural language processing, and cross-modal search. Hashing-based\nmethods provide an efficient solution by mapping high-dimensional data into\ncompact binary codes that enable fast similarity computations in Hamming space.\nOver the past two decades, a substantial body of work has explored learning to\nhash, where projection and quantisation functions are optimised from data\nrather than chosen at random.\n  This article offers a foundational survey of early learning-based hashing\nmethods, with an emphasis on the core ideas that shaped the field. We review\nsupervised, unsupervised, and semi-supervised approaches, highlighting how\nprojection functions are designed to generate meaningful embeddings and how\nquantisation strategies convert these embeddings into binary codes. We also\nexamine extensions to multi-bit and multi-threshold models, as well as early\nadvances in cross-modal retrieval.\n  Rather than providing an exhaustive account of the most recent methods, our\ngoal is to introduce the conceptual foundations of learning-based hashing for\nANN search. By situating these early models in their historical context, we aim\nto equip readers with a structured understanding of the principles, trade-offs,\nand open challenges that continue to inform current research in this area.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u65e9\u671f\u57fa\u4e8e\u5b66\u4e60\u7684\u54c8\u5e0c\u65b9\u6cd5\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u5851\u9020\u8be5\u9886\u57df\u7684\u6838\u5fc3\u601d\u60f3\u3002", "motivation": "\u8fd1\u4f3c\u6700\u8fd1\u90bb (ANN) \u641c\u7d22\u662f\u4fe1\u606f\u68c0\u7d22\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff0c\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8de8\u6a21\u6001\u641c\u7d22\u4e2d\u5927\u89c4\u6a21\u5e94\u7528\u7684\u57fa\u7840\u3002\u57fa\u4e8e\u54c8\u5e0c\u7684\u65b9\u6cd5\u901a\u8fc7\u5c06\u9ad8\u7ef4\u6570\u636e\u6620\u5c04\u5230\u7d27\u51d1\u7684\u4e8c\u8fdb\u5236\u4ee3\u7801\u4e2d\u6765\u63d0\u4f9b\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u800c\u53ef\u4ee5\u5728 Hamming \u7a7a\u95f4\u4e2d\u5b9e\u73b0\u5feb\u901f\u76f8\u4f3c\u6027\u8ba1\u7b97\u3002", "method": "\u672c\u6587\u56de\u987e\u4e86\u6709\u76d1\u7763\u3001\u65e0\u76d1\u7763\u548c\u534a\u76d1\u7763\u65b9\u6cd5\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u5982\u4f55\u8bbe\u8ba1\u6295\u5f71\u51fd\u6570\u4ee5\u751f\u6210\u6709\u610f\u4e49\u7684\u5d4c\u5165\uff0c\u4ee5\u53ca\u91cf\u5316\u7b56\u7565\u5982\u4f55\u5c06\u8fd9\u4e9b\u5d4c\u5165\u8f6c\u6362\u4e3a\u4e8c\u8fdb\u5236\u4ee3\u7801\u3002\u672c\u6587\u8fd8\u7814\u7a76\u4e86\u591a\u4f4d\u548c\u591a\u9608\u503c\u6a21\u578b\u7684\u6269\u5c55\uff0c\u4ee5\u53ca\u8de8\u6a21\u6001\u68c0\u7d22\u7684\u65e9\u671f\u8fdb\u5c55\u3002", "result": "\u672c\u6587\u5bf9\u65e9\u671f\u57fa\u4e8e\u5b66\u4e60\u7684\u54c8\u5e0c\u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u7840\u6027\u8c03\u67e5\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u5851\u9020\u8be5\u9886\u57df\u7684\u6838\u5fc3\u601d\u60f3\u3002", "conclusion": "\u672c\u6587\u7684\u91cd\u70b9\u662f\u4ecb\u7ecd\u57fa\u4e8e\u5b66\u4e60\u7684\u54c8\u5e0c\u7684\u7684\u6982\u5ff5\u57fa\u7840\uff0c\u800c\u4e0d\u662f\u63d0\u4f9b\u6700\u65b0\u65b9\u6cd5\u7684\u8be6\u5c3d\u63cf\u8ff0\u3002\u901a\u8fc7\u5c06\u8fd9\u4e9b\u65e9\u671f\u6a21\u578b\u7f6e\u4e8e\u5176\u5386\u53f2\u80cc\u666f\u4e2d\uff0c\u65e8\u5728\u4f7f\u8bfb\u8005\u5bf9\u7ee7\u7eed\u4e3a\u8be5\u9886\u57df\u5f53\u524d\u7814\u7a76\u63d0\u4f9b\u4fe1\u606f\u7684\u539f\u7406\u3001\u6743\u8861\u548c\u672a\u51b3\u6311\u6218\u6709\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u7406\u89e3\u3002"}}
{"id": "2510.03439", "categories": ["cs.CL", "I.2.7; I.6.m"], "pdf": "https://arxiv.org/pdf/2510.03439", "abs": "https://arxiv.org/abs/2510.03439", "authors": ["Brendon Boldt", "David Mortensen"], "title": "Morpheme Induction for Emergent Language", "comment": "Accepted for publication at the 2025 Conference on Empirical Methods\n  in Natural Language Processing; 16 pages, 4 figures", "summary": "We introduce CSAR, an algorithm for inducing morphemes from emergent language\ncorpora of parallel utterances and meanings. It is a greedy algorithm that (1)\nweights morphemes based on mutual information between forms and meanings, (2)\nselects the highest-weighted pair, (3) removes it from the corpus, and (4)\nrepeats the process to induce further morphemes (i.e., Count, Select, Ablate,\nRepeat). The effectiveness of CSAR is first validated on procedurally generated\ndatasets and compared against baselines for related tasks. Second, we validate\nCSAR's performance on human language data to show that the algorithm makes\nreasonable predictions in adjacent domains. Finally, we analyze a handful of\nemergent languages, quantifying linguistic characteristics like degree of\nsynonymy and polysemy.", "AI": {"tldr": "CSAR \u662f\u4e00\u79cd\u4ece\u5e73\u884c\u8bed\u6599\u5e93\u4e2d\u8bf1\u5bfc\u8bed\u7d20\u7684\u8d2a\u5a6a\u7b97\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u4eceemergent language\u8bed\u6599\u5e93\u4e2d\u8bf1\u5bfc\u8bed\u7d20", "method": "\u8be5\u7b97\u6cd5\u901a\u8fc7 (1) \u6839\u636e\u5f62\u5f0f\u548c\u610f\u4e49\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u5bf9\u8bed\u7d20\u8fdb\u884c\u52a0\u6743\uff0c(2) \u9009\u62e9\u6743\u91cd\u6700\u9ad8\u7684\u914d\u5bf9\uff0c(3) \u4ece\u8bed\u6599\u5e93\u4e2d\u5220\u9664\u5b83\uff0c\u4ee5\u53ca (4) \u91cd\u590d\u8be5\u8fc7\u7a0b\u6765\u8bf1\u5bfc\u66f4\u591a\u8bed\u7d20\u3002", "result": "CSAR \u5728\u7a0b\u5e8f\u751f\u6210\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5e76\u4e0e\u76f8\u5173\u4efb\u52a1\u7684\u57fa\u7ebf\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002 \u6b64\u5916\uff0cCSAR \u5728\u4eba\u7c7b\u8bed\u8a00\u6570\u636e\u4e0a\u7684\u6027\u80fd\u4e5f\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u8868\u660e\u8be5\u7b97\u6cd5\u5728\u76f8\u90bb\u9886\u57df\u505a\u51fa\u4e86\u5408\u7406\u7684\u9884\u6d4b\u3002 \u6700\u540e\uff0c\u6211\u4eec\u5206\u6790\u4e86\u4e00\u4e9bemergent languages\uff0c\u91cf\u5316\u4e86\u8bf8\u5982\u540c\u4e49\u8bcd\u548c\u591a\u4e49\u8bcd\u7a0b\u5ea6\u7684\u8bed\u8a00\u7279\u5f81\u3002", "conclusion": "CSAR \u7b97\u6cd5\u5728emergent languages\u4e2d\u80fd\u591f\u6709\u6548\u8bf1\u5bfc\u8bed\u7d20\uff0c\u5e76\u5728\u7a0b\u5e8f\u751f\u6210\u6570\u636e\u96c6\u548c\u4eba\u7c7b\u8bed\u8a00\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.03295", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03295", "abs": "https://arxiv.org/abs/2510.03295", "authors": ["Passant Elchafei", "Amany Fashwan"], "title": "Multimodal Arabic Captioning with Interpretable Visual Concept Integration", "comment": null, "summary": "We present VLCAP, an Arabic image captioning framework that integrates\nCLIP-based visual label retrieval with multimodal text generation. Rather than\nrelying solely on end-to-end captioning, VLCAP grounds generation in\ninterpretable Arabic visual concepts extracted with three multilingual\nencoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label\nretrieval. A hybrid vocabulary is built from training captions and enriched\nwith about 21K general domain labels translated from the Visual Genome dataset,\ncovering objects, attributes, and scenes. The top-k retrieved labels are\ntransformed into fluent Arabic prompts and passed along with the original image\nto vision-language models. In the second stage, we tested Qwen-VL and Gemini\nPro Vision for caption generation, resulting in six encoder-decoder\nconfigurations. The results show that mCLIP + Gemini Pro Vision achieved the\nbest BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL\nobtained the highest LLM-judge score (36.33%). This interpretable pipeline\nenables culturally coherent and contextually accurate Arabic captions.", "AI": {"tldr": "VLCAP\u662f\u4e00\u4e2a\u963f\u62c9\u4f2f\u56fe\u50cf\u5b57\u5e55\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u57fa\u4e8eCLIP\u7684\u89c6\u89c9\u6807\u7b7e\u68c0\u7d22\u548c\u591a\u6a21\u6001\u6587\u672c\u751f\u6210\u3002", "motivation": "\u8be5\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u751f\u6210\u5177\u6709\u6587\u5316\u8fde\u8d2f\u6027\u548c\u4e0a\u4e0b\u6587\u51c6\u786e\u6027\u7684\u963f\u62c9\u4f2f\u8bed\u56fe\u50cf\u5b57\u5e55\u7684\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\uff1a1) \u4f7f\u7528\u4e09\u79cd\u591a\u8bed\u8a00\u7f16\u7801\u5668\uff08mCLIP\u3001AraCLIP \u548c Jina V4\uff09\u63d0\u53d6\u53ef\u89e3\u91ca\u7684\u963f\u62c9\u4f2f\u8bed\u89c6\u89c9\u6982\u5ff5\uff1b2) \u6784\u5efa\u6df7\u5408\u8bcd\u6c47\u8868\uff0c\u5e76\u7528\u4ece Visual Genome \u6570\u636e\u96c6\u7ffb\u8bd1\u7684\u901a\u7528\u9886\u57df\u6807\u7b7e\u8fdb\u884c\u4e30\u5bcc\uff1b3) \u5c06\u68c0\u7d22\u5230\u7684\u6807\u7b7e\u8f6c\u6362\u4e3a\u6d41\u7545\u7684\u963f\u62c9\u4f2f\u8bed\u63d0\u793a\uff0c\u5e76\u4e0e\u539f\u59cb\u56fe\u50cf\u4e00\u8d77\u4f20\u9012\u7ed9\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff1b4) \u4f7f\u7528 Qwen-VL \u548c Gemini Pro Vision \u8fdb\u884c\u5b57\u5e55\u751f\u6210\u3002", "result": "mCLIP + Gemini Pro Vision \u5b9e\u73b0\u4e86\u6700\u4f73 BLEU-1 (5.34%) \u548c\u4f59\u5f26\u76f8\u4f3c\u5ea6 (60.01%)\uff0c\u800c AraCLIP + Qwen-VL \u83b7\u5f97\u4e86\u6700\u9ad8\u7684 LLM-judge \u8bc4\u5206 (36.33%)\u3002", "conclusion": "VLCAP \u6846\u67b6\u80fd\u591f\u751f\u6210\u5177\u6709\u6587\u5316\u8fde\u8d2f\u6027\u548c\u4e0a\u4e0b\u6587\u51c6\u786e\u6027\u7684\u963f\u62c9\u4f2f\u8bed\u5b57\u5e55\u3002"}}
{"id": "2510.04776", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.04776", "abs": "https://arxiv.org/abs/2510.04776", "authors": ["Ebenezer Awotoro", "Chisom Ezekannagha", "Florian Schwarz", "Johannes Tauscher", "Dominik Heider", "Katharina Ladewig", "Christel Le Bon", "Karine Moncoq", "Bruno Miroux", "Georges Hattab"], "title": "MetaMP: Seamless Metadata Enrichment and AI Application Framework for Enhanced Membrane Protein Visualization and Analysis", "comment": null, "summary": "Structural biology has made significant progress in determining membrane\nproteins, leading to a remarkable increase in the number of available\nstructures in dedicated databases. The inherent complexity of membrane protein\nstructures, coupled with challenges such as missing data, inconsistencies, and\ncomputational barriers from disparate sources, underscores the need for\nimproved database integration. To address this gap, we present MetaMP, a\nframework that unifies membrane-protein databases within a web application and\nuses machine learning for classification. MetaMP improves data quality by\nenriching metadata, offering a user-friendly interface, and providing eight\ninteractive views for streamlined exploration. MetaMP was effective across\ntasks of varying difficulty, demonstrating advantages across different levels\nwithout compromising speed or accuracy, according to user evaluations.\nMoreover, MetaMP supports essential functions such as structure classification\nand outlier detection.\n  We present three practical applications of Artificial Intelligence (AI) in\nmembrane protein research: predicting transmembrane segments, reconciling\nlegacy databases, and classifying structures with explainable AI support. In a\nvalidation focused on statistics, MetaMP resolved 77% of data discrepancies and\naccurately predicted the class of newly identified membrane proteins 98% of the\ntime and overtook expert curation. Altogether, MetaMP is a much-needed resource\nthat harmonizes current knowledge and empowers AI-driven exploration of\nmembrane-protein architecture.", "AI": {"tldr": "MetaMP: A web application unifying membrane-protein databases with machine learning for improved data quality, streamlined exploration, and AI-driven analysis.", "motivation": "The increasing number and complexity of membrane protein structures, coupled with data inconsistencies and computational challenges, necessitate improved database integration.", "method": "A framework (MetaMP) unifies membrane-protein databases within a web application and uses machine learning for classification. It enriches metadata and offers interactive views.", "result": "MetaMP resolved 77% of data discrepancies, accurately predicted the class of new membrane proteins 98% of the time, and demonstrated advantages across different tasks without compromising speed or accuracy.", "conclusion": "MetaMP harmonizes current knowledge and empowers AI-driven exploration of membrane-protein architecture, providing a much-needed resource for the field."}}
{"id": "2510.03246", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03246", "abs": "https://arxiv.org/abs/2510.03246", "authors": ["Xinyuan Song", "Guangji Bai", "Liang Zhao"], "title": "StructPrune: Structured Global Pruning asymptotics with $\\mathcal{O}(\\sqrt{N})$ GPU Memory", "comment": null, "summary": "Pruning is critical for scaling large language models (LLMs). Global pruning\nachieves strong performance but requires $\\mathcal{O}(N)$ memory, which is\ninfeasible for billion-parameter models. Local pruning reduces GPU memory usage\nto that of a single layer by pruning layers independently, but it neglects\ninter-layer dependencies and often leads to suboptimal performance in\nhigh-sparsity regimes. Unlike unstructured pruning, structured pruning produces\nregular sparsity patterns that align well with GPU kernels and library\noptimizations, making it more hardware-efficient. However, structured pruning\ntypically relies on global pruning, since structured patterns are more prone to\nsevere performance degradation under local optimization. To jointly achieve\nstructured pruning and the memory efficiency of local pruning, we propose a\ndivide-and-conquer strategy that decomposes the global pruning problem into\ncoordinated subproblems across different modules, each of which fits within\nlimited GPU memory. Building on this idea, we design \\textbf{STRUPRUNE}, an\nADMM-based framework that integrates structured sparsity into the pruning\nprocess, combining the memory efficiency of local pruning with the hardware\ncompatibility of structured methods. We derive a closed-form analytical\nsolution for structured pruning masks that provides an explicit rule for\nlayer-wise sparsity allocation, and further develop an energy-based asymptotic\nframework yielding a softmax-form allocation scheme that simplifies\noptimization while adapting to heterogeneous layer importance. Experiments\ndemonstrate that STRUPRUNE matches the perplexity of global structured pruning\nwhile reducing memory cost from $\\mathcal{O}(N)$ to $\\mathcal{O}(\\sqrt{N})$,\nenabling practical deployment at the billion-parameter scale.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a STRUPRUNE \u7684\u65b0\u7684\u7ed3\u6784\u5316\u526a\u679d\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5c40\u90e8\u526a\u679d\u7684\u5185\u5b58\u6548\u7387\u548c\u7ed3\u6784\u5316\u65b9\u6cd5\u7684\u786c\u4ef6\u517c\u5bb9\u6027\uff0c\u4ece\u800c\u80fd\u591f\u5728\u5341\u4ebf\u53c2\u6570\u89c4\u6a21\u4e0a\u8fdb\u884c\u5b9e\u9645\u90e8\u7f72\u3002", "motivation": "\u5168\u5c40\u526a\u679d\u867d\u7136\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u9700\u8981\u5927\u91cf\u5185\u5b58\uff0c\u8fd9\u5bf9\u4e8e\u5341\u4ebf\u53c2\u6570\u6a21\u578b\u6765\u8bf4\u662f\u4e0d\u53ef\u884c\u7684\u3002\u5c40\u90e8\u526a\u679d\u867d\u7136\u964d\u4f4e\u4e86 GPU \u5185\u5b58\u4f7f\u7528\u91cf\uff0c\u4f46\u5ffd\u7565\u4e86\u5c42\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u9ad8\u7a00\u758f\u5ea6\u60c5\u51b5\u4e0b\u901a\u5e38\u4f1a\u5bfc\u81f4\u6b21\u4f18\u6027\u80fd\u3002\u7ed3\u6784\u5316\u526a\u679d\u867d\u7136\u66f4\u5177\u786c\u4ef6\u6548\u7387\uff0c\u4f46\u901a\u5e38\u4f9d\u8d56\u4e8e\u5168\u5c40\u526a\u679d\uff0c\u56e0\u4e3a\u7ed3\u6784\u5316\u6a21\u5f0f\u5728\u5c40\u90e8\u4f18\u5316\u4e0b\u66f4\u5bb9\u6613\u51fa\u73b0\u4e25\u91cd\u7684\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u800c\u6cbb\u4e4b\u7684\u7b56\u7565\uff0c\u5c06\u5168\u5c40\u526a\u679d\u95ee\u9898\u5206\u89e3\u4e3a\u8de8\u4e0d\u540c\u6a21\u5757\u7684\u534f\u8c03\u5b50\u95ee\u9898\uff0c\u6bcf\u4e2a\u5b50\u95ee\u9898\u90fd\u9002\u5408\u5728\u6709\u9650\u7684 GPU \u5185\u5b58\u4e2d\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e ADMM \u7684\u6846\u67b6 STRUPRUNE\uff0c\u8be5\u6846\u67b6\u5c06\u7ed3\u6784\u5316\u7a00\u758f\u6027\u96c6\u6210\u5230\u526a\u679d\u8fc7\u7a0b\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSTRUPRUNE \u5728\u964d\u4f4e\u5185\u5b58\u6210\u672c\u7684\u540c\u65f6\uff0c\u4e0e\u5168\u5c40\u7ed3\u6784\u5316\u526a\u679d\u7684\u56f0\u60d1\u5ea6\u76f8\u5339\u914d\uff0c\u4ece\u800c\u80fd\u591f\u5728\u5341\u4ebf\u53c2\u6570\u89c4\u6a21\u4e0a\u8fdb\u884c\u5b9e\u9645\u90e8\u7f72\u3002", "conclusion": "STRUPRUNE \u6846\u67b6\u80fd\u591f\u5728\u964d\u4f4e\u5185\u5b58\u6210\u672c\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e0e\u5168\u5c40\u7ed3\u6784\u5316\u526a\u679d\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4e3a\u5341\u4ebf\u53c2\u6570\u89c4\u6a21\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002"}}
{"id": "2510.03418", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.03418", "abs": "https://arxiv.org/abs/2510.03418", "authors": ["Ananya Mantravadi", "Shivali Dalmia", "Abhishek Mukherji", "Nand Dave", "Anudha Mittal"], "title": "ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) integrates LLMs with external sources,\noffering advanced capabilities for information access and decision-making.\nHowever, contradictions in retrieved evidence can result in inconsistent or\nuntrustworthy outputs, which is especially problematic in enterprise settings\nwhere compliance, governance, and accountability are critical. Existing\nbenchmarks for contradiction detection are limited to sentence-level analysis\nand do not capture the complexity of enterprise documents such as contracts,\nfinancial filings, compliance reports, or policy manuals. To address this\nlimitation, we propose ContraGen, a contradiction-aware benchmark framework\ntailored to enterprise domain. The framework generates synthetic\nenterprise-style documents with embedded contradictions, enabling systematic\nevaluation of both intra-document and cross-document consistency. Automated\ncontradiction mining is combined with human-in-the-loop validation to ensure\nhigh accuracy. Our contributions include generating realistic enterprise\ndocuments, modeling a taxonomy of contradiction types common in business\nprocesses, enabling controlled creation of self- and pairwise contradictions,\ndeveloping a contradiction-aware retrieval evaluation pipeline and embedding\nhuman oversight to reflect domain-specific judgment complexity. This work\nestablishes a foundation for more trustworthy and accountable RAG systems in\nenterprise information-seeking applications, where detecting and resolving\ncontradictions is essential for reducing risk and ensuring compliance.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aContraGen\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4f01\u4e1a\u73af\u5883\u4e2d\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u4e2d\u7684\u77db\u76fe\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u77db\u76fe\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u96c6\u4e2d\u5728\u53e5\u5b50\u5c42\u9762\uff0c\u65e0\u6cd5\u6355\u6349\u4f01\u4e1a\u6587\u6863\u7684\u590d\u6742\u6027\uff0c\u800c\u4f01\u4e1a\u73af\u5883\u4e2d\u5408\u89c4\u6027\u3001\u6cbb\u7406\u548c\u8d23\u4efb\u81f3\u5173\u91cd\u8981\uff0c\u68c0\u7d22\u5230\u7684\u8bc1\u636e\u4e2d\u7684\u77db\u76fe\u4f1a\u5bfc\u81f4\u4e0d\u4e00\u81f4\u6216\u4e0d\u53ef\u4fe1\u7684\u8f93\u51fa\u3002", "method": "\u8be5\u6846\u67b6\u901a\u8fc7\u751f\u6210\u5305\u542b\u5d4c\u5165\u77db\u76fe\u7684\u5408\u6210\u4f01\u4e1a\u98ce\u683c\u6587\u6863\uff0c\u5b9e\u73b0\u5bf9\u6587\u6863\u5185\u548c\u8de8\u6587\u6863\u4e00\u81f4\u6027\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002\u7ed3\u5408\u81ea\u52a8\u77db\u76fe\u6316\u6398\u548c\u4eba\u5de5\u9a8c\u8bc1\uff0c\u786e\u4fdd\u9ad8\u51c6\u786e\u6027\u3002", "result": "\u8bba\u6587\u751f\u6210\u4e86\u903c\u771f\u7684\u4f01\u4e1a\u6587\u6863\uff0c\u5bf9\u4e1a\u52a1\u6d41\u7a0b\u4e2d\u5e38\u89c1\u7684\u77db\u76fe\u7c7b\u578b\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u81ea\u6211\u77db\u76fe\u548c\u6210\u5bf9\u77db\u76fe\u7684\u53d7\u63a7\u521b\u5efa\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u77db\u76fe\u611f\u77e5\u7684\u68c0\u7d22\u8bc4\u4f30\u7ba1\u9053\uff0c\u5e76\u5d4c\u5165\u4e86\u4eba\u5de5\u76d1\u7763\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4f01\u4e1a\u4fe1\u606f\u68c0\u7d22\u5e94\u7528\u4e2d\u66f4\u503c\u5f97\u4fe1\u8d56\u548c\u8d1f\u8d23\u4efb\u7684RAG\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5728\u8fd9\u4e9b\u5e94\u7528\u4e2d\uff0c\u68c0\u6d4b\u548c\u89e3\u51b3\u77db\u76fe\u5bf9\u4e8e\u964d\u4f4e\u98ce\u9669\u548c\u786e\u4fdd\u5408\u89c4\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.04239", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04239", "abs": "https://arxiv.org/abs/2510.04239", "authors": ["Tongzhou Wu", "Yuhao Wang", "Maolin Wang", "Chi Zhang", "Xiangyu Zhao"], "title": "Empowering Denoising Sequential Recommendation with Large Language Model Embeddings", "comment": "Accepted by CIKM2025", "summary": "Sequential recommendation aims to capture user preferences by modeling\nsequential patterns in user-item interactions. However, these models are often\ninfluenced by noise such as accidental interactions, leading to suboptimal\nperformance. Therefore, to reduce the effect of noise, some works propose\nexplicitly identifying and removing noisy items. However, we find that simply\nrelying on collaborative information may result in an over-denoising problem,\nespecially for cold items. To overcome these limitations, we propose a novel\nframework: Interest Alignment for Denoising Sequential Recommendation (IADSR)\nwhich integrates both collaborative and semantic information. Specifically,\nIADSR is comprised of two stages: in the first stage, we obtain the\ncollaborative and semantic embeddings of each item from a traditional\nsequential recommendation model and an LLM, respectively. In the second stage,\nwe align the collaborative and semantic embeddings and then identify noise in\nthe interaction sequence based on long-term and short-term interests captured\nin the collaborative and semantic modalities. Our extensive experiments on four\npublic datasets validate the effectiveness of the proposed framework and its\ncompatibility with different sequential recommendation systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53bb\u566a\u5e8f\u5217\u63a8\u8350\u6846\u67b6\uff08IADSR\uff09\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u5e8f\u5217\u63a8\u8350\u6a21\u578b\u53d7\u566a\u58f0\u5f71\u54cd\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5e8f\u5217\u63a8\u8350\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u5076\u7136\u4ea4\u4e92\u7b49\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u53bb\u566a\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u534f\u540c\u4fe1\u606f\uff0c\u5c24\u5176\u5bf9\u4e8e\u51b7\u542f\u52a8\u7269\u54c1\u3002", "method": "IADSR\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a\u9996\u5148\uff0c\u4ece\u4f20\u7edf\u5e8f\u5217\u63a8\u8350\u6a21\u578b\u548cLLM\u4e2d\u83b7\u53d6\u7269\u54c1\u7684\u534f\u540c\u548c\u8bed\u4e49\u5d4c\u5165\uff1b\u7136\u540e\uff0c\u5bf9\u9f50\u534f\u540c\u548c\u8bed\u4e49\u5d4c\u5165\uff0c\u5e76\u57fa\u4e8e\u534f\u540c\u548c\u8bed\u4e49\u6a21\u6001\u4e2d\u6355\u83b7\u7684\u957f\u671f\u548c\u77ed\u671f\u5174\u8da3\u6765\u8bc6\u522b\u4ea4\u4e92\u5e8f\u5217\u4e2d\u7684\u566a\u58f0\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u6846\u67b6\u7684\u6709\u6548\u6027\u53ca\u5176\u4e0e\u4e0d\u540c\u5e8f\u5217\u63a8\u8350\u7cfb\u7edf\u7684\u517c\u5bb9\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684IADSR\u6846\u67b6\u80fd\u591f\u6709\u6548\u964d\u4f4e\u5e8f\u5217\u63a8\u8350\u6a21\u578b\u4e2d\u7684\u566a\u58f0\u5f71\u54cd\uff0c\u63d0\u9ad8\u63a8\u8350\u6027\u80fd\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2510.03458", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03458", "abs": "https://arxiv.org/abs/2510.03458", "authors": ["Mengyao Xu", "Wenfei Zhou", "Yauhen Babakhin", "Gabriel Moreira", "Ronay Ak", "Radek Osmulski", "Bo Liu", "Even Oldridge", "Benedikt Schifferer"], "title": "Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video", "comment": null, "summary": "We present Omni-Embed-Nemotron, a unified multimodal retrieval embedding\nmodel developed to handle the increasing complexity of real-world information\nneeds. While Retrieval-Augmented Generation (RAG) has significantly advanced\nlanguage models by incorporating external knowledge, existing text-based\nretrievers rely on clean, structured input and struggle with the visually and\nsemantically rich content found in real-world documents such as PDFs, slides,\nor videos. Recent work such as ColPali has shown that preserving document\nlayout using image-based representations can improve retrieval quality.\nBuilding on this, and inspired by the capabilities of recent multimodal models\nsuch as Qwen2.5-Omni, we extend retrieval beyond text and images to also\nsupport audio and video modalities. Omni-Embed-Nemotron enables both\ncross-modal (e.g., text - video) and joint-modal (e.g., text - video+audio)\nretrieval using a single model. We describe the architecture, training setup,\nand evaluation results of Omni-Embed-Nemotron, and demonstrate its\neffectiveness in text, image, and video retrieval.", "AI": {"tldr": "Omni-Embed-Nemotron: A unified multimodal retrieval embedding model for handling complex real-world information needs, extending retrieval beyond text to include images, audio, and video.", "motivation": "Existing text-based retrievers in RAG systems struggle with the visually and semantically rich content in real-world documents like PDFs, slides, or videos.", "method": "Extends retrieval beyond text and images to support audio and video modalities, enabling cross-modal and joint-modal retrieval using a single model. The architecture, training setup, and evaluation results of Omni-Embed-Nemotron are described.", "result": "Demonstrates effectiveness in text, image, and video retrieval.", "conclusion": "Omni-Embed-Nemotron is effective in handling complex real-world information needs by supporting multiple modalities."}}
{"id": "2510.03297", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03297", "abs": "https://arxiv.org/abs/2510.03297", "authors": ["Akshar Gothi"], "title": "Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes", "comment": "5 pages, 1 figure, 9 tables. Code and artifacts:\n  https://github.com/akshar27/spacenet-cnn-vs-vit (release v1.0.1)", "summary": "We present a controlled comparison of a convolutional neural network\n(EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two\nlabel-distribution regimes: a naturally imbalanced five-class split and a\nbalanced-resampled split with 700 images per class (70:20:10 train/val/test).\nWith matched preprocessing (224x224, ImageNet normalization), lightweight\naugmentations, and a 40-epoch budget on a single NVIDIA P100, we report\naccuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics\n(model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93%\ntest accuracy with strong macro-F1 and lower latency; ViT-Base is competitive\nat 93% with a larger parameter count and runtime. On the balanced split, both\nmodels are strong; EfficientNet-B0 reaches 99% while ViT-Base remains\ncompetitive, indicating that balancing narrows architecture gaps while CNNs\nretain an efficiency edge. We release manifests, logs, and per-image\npredictions to support reproducibility.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6bd4\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (EfficientNet-B0) \u548c Vision Transformer (ViT-Base) \u5728 SpaceNet \u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u4f7f\u7528\u4e86\u4e0d\u5e73\u8861\u548c\u5e73\u8861\u4e24\u79cd\u6807\u7b7e\u5206\u5e03\u3002", "motivation": "\u7814\u7a76\u4e0d\u540c\u6807\u7b7e\u5206\u5e03\u4e0b\uff0c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c Vision Transformer \u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u5728\u76f8\u540c\u7684\u9884\u5904\u7406\u3001\u6570\u636e\u589e\u5f3a\u548c\u8bad\u7ec3\u9884\u7b97\u4e0b\uff0c\u6bd4\u8f83 EfficientNet-B0 \u548c ViT-Base \u5728 SpaceNet \u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "result": "\u5728\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\uff0cEfficientNet-B0 \u5728\u6d4b\u8bd5\u51c6\u786e\u7387\u3001macro-F1 \u548c\u5ef6\u8fdf\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff1b\u5728\u5e73\u8861\u6570\u636e\u96c6\u4e0a\uff0c\u4e24\u8005\u6027\u80fd\u63a5\u8fd1\uff0c\u4f46 CNN \u4ecd\u7136\u4fdd\u6301\u6548\u7387\u4f18\u52bf\u3002", "conclusion": "\u5e73\u8861\u6570\u636e\u96c6\u53ef\u4ee5\u7f29\u5c0f\u67b6\u6784\u5dee\u8ddd\uff0c\u4f46 CNN \u5728\u6548\u7387\u65b9\u9762\u4ecd\u7136\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2510.04919", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.04919", "abs": "https://arxiv.org/abs/2510.04919", "authors": ["Davood Rafiei", "Morgan Lindsay Heisler", "Weiwei Zhang", "Mohammadreza Pourreza", "Yong Zhang"], "title": "Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment", "comment": null, "summary": "Supervised Fine-Tuning (SFT) is an effective method for adapting Large\nLanguage Models (LLMs) on downstream tasks. However, variability in training\ndata can hinder a model's ability to generalize across domains. This paper\nstudies the problem of dataset alignment for Natural Language to SQL (NL2SQL or\ntext to SQL), examining how well SFT training data matches the structural\ncharacteristics of target queries and how this alignment impacts model\nperformance. We hypothesize that alignment can be accurately estimated by\ncomparing the distributions of structural SQL features across the training set,\ntarget data, and the model's predictions prior to SFT. Through comprehensive\nexperiments on three large cross-domain NL2SQL benchmarks and multiple model\nfamilies, we show that structural alignment is a strong predictor of\nfine-tuning success. When alignment is high, SFT yields substantial gains in\naccuracy and SQL generation quality; when alignment is low, improvements are\nmarginal or absent. These findings highlight the importance of alignment-aware\ndata selection for effective fine-tuning and generalization in NL2SQL tasks.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86SFT\u8bad\u7ec3\u6570\u636e\u4e0e\u76ee\u6807\u67e5\u8be2\u7684\u7ed3\u6784\u7279\u5f81\u5339\u914d\u7a0b\u5ea6\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u9886\u57df\u5185SFT\u8bad\u7ec3\u6570\u636e\u7684\u53ef\u53d8\u6027\u4f1a\u963b\u788d\u6a21\u578b\u5728\u8de8\u9886\u57df\u63a8\u5e7f\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u8bad\u7ec3\u96c6\u3001\u76ee\u6807\u6570\u636e\u548cSFT\u4e4b\u524d\u6a21\u578b\u9884\u6d4b\u7684\u7ed3\u6784\u5316SQL\u7279\u5f81\u7684\u5206\u5e03\uff0c\u6765\u8bc4\u4f30\u5bf9\u9f50\u60c5\u51b5\u3002", "result": "\u7ed3\u6784\u5bf9\u9f50\u662f\u5fae\u8c03\u6210\u529f\u7684\u91cd\u8981\u9884\u6d4b\u6307\u6807\u3002\u5f53\u5bf9\u9f50\u7a0b\u5ea6\u9ad8\u65f6\uff0cSFT\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u6027\u548cSQL\u751f\u6210\u8d28\u91cf\uff1b\u5f53\u5bf9\u9f50\u7a0b\u5ea6\u4f4e\u65f6\uff0c\u6539\u8fdb\u6548\u679c\u4e0d\u660e\u663e\u6216\u6ca1\u6709\u6539\u8fdb\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728NL2SQL\u4efb\u52a1\u4e2d\uff0c\u4e3a\u4e86\u8fdb\u884c\u6709\u6548\u7684\u5fae\u8c03\u548c\u6cdb\u5316\uff0c\u9700\u8981\u8fdb\u884c\u6709\u9488\u5bf9\u6027\u7684\u6570\u636e\u9009\u62e9\u3002"}}
{"id": "2510.03247", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03247", "abs": "https://arxiv.org/abs/2510.03247", "authors": ["Jiancheng Zhang", "Yinglun Zhu"], "title": "Towards Multimodal Active Learning: Efficient Learning with Limited Paired Data", "comment": null, "summary": "Active learning (AL) is a principled strategy to reduce annotation cost in\ndata-hungry deep learning. However, existing AL algorithms focus almost\nexclusively on unimodal data, overlooking the substantial annotation burden in\nmultimodal learning. We introduce the first framework for multimodal active\nlearning with unaligned data, where the learner must actively acquire\ncross-modal alignments rather than labels on pre-aligned pairs. This setting\ncaptures the practical bottleneck in modern multimodal pipelines such as CLIP\nand SigLIP, where unimodal features are easy to obtain but high-quality\nalignment is costly. We develop a new algorithm that combines uncertainty and\ndiversity principles in a modality-aware design, achieves linear-time\nacquisition, and applies seamlessly to both pool-based and streaming-based\nsettings. Extensive experiments on benchmark datasets demonstrate that our\napproach consistently reduces multimodal annotation cost while preserving\nperformance; for instance, on the ColorSwap dataset it cuts annotation\nrequirements by up to $40\\%$ without loss in accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u975e\u5bf9\u9f50\u6570\u636e\u7684\u6807\u6ce8\u96be\u9898\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u8de8\u6a21\u6001\u5bf9\u9f50\u800c\u975e\u9884\u5bf9\u9f50\u5bf9\u7684\u6807\u7b7e\u6765\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u5355\u6a21\u6001\u6570\u636e\u4e0a\uff0c\u5ffd\u7565\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u5927\u91cf\u6807\u6ce8\u8d1f\u62c5\u3002\u73b0\u4ee3\u591a\u6a21\u6001\u6d41\u7a0b\uff08\u5982CLIP\u548cSigLIP\uff09\u4e2d\uff0c\u5355\u6a21\u6001\u7279\u5f81\u6613\u4e8e\u83b7\u53d6\uff0c\u4f46\u9ad8\u8d28\u91cf\u7684\u5bf9\u9f50\u6210\u672c\u5f88\u9ad8\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u7ed3\u5408\u4e86\u4e0d\u786e\u5b9a\u6027\u548c\u591a\u6837\u6027\u539f\u5219\uff0c\u5b9e\u73b0\u4e86\u7ebf\u6027\u65f6\u95f4\u83b7\u53d6\uff0c\u5e76\u65e0\u7f1d\u5e94\u7528\u4e8e\u57fa\u4e8e\u6c60\u548c\u57fa\u4e8e\u6d41\u7684\u8bbe\u7f6e\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\uff0c\u59cb\u7ec8\u964d\u4f4e\u591a\u6a21\u6001\u6807\u6ce8\u6210\u672c\uff1b\u4f8b\u5982\uff0c\u5728ColorSwap\u6570\u636e\u96c6\u4e0a\uff0c\u5b83\u53ef\u4ee5\u5728\u4e0d\u635f\u5931\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u5c06\u6807\u6ce8\u9700\u6c42\u964d\u4f4e\u591a\u8fbe40%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u964d\u4f4e\u4e86\u591a\u6a21\u6001\u6570\u636e\u7684\u6807\u6ce8\u6210\u672c\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2510.03453", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03453", "abs": "https://arxiv.org/abs/2510.03453", "authors": ["Paul S. Rosenbloom"], "title": "A Qualitative Comparative Evaluation of Cognitive and Generative Theories", "comment": "To appear in Proceedings of the 12th Annual Conference on Advances in\n  Cognitive Systems (ACS-25)", "summary": "Evaluation is a critical activity associated with any theory. Yet this has\nproven to be an exceptionally challenging activity for theories based on\ncognitive architectures. For an overlapping set of reasons, evaluation can also\nbe challenging for theories based on generative neural architectures. This dual\nchallenge is approached here by leveraging a broad perspective on theory\nevaluation to yield a wide-ranging, albeit qualitative, comparison of\nwhole-mind-oriented cognitive and generative architectures and the full systems\nthat are based on these architectures.", "AI": {"tldr": "\u5bf9\u57fa\u4e8e\u8ba4\u77e5\u67b6\u6784\u548c\u751f\u6210\u795e\u7ecf\u67b6\u6784\u7684\u7406\u8bba\u8fdb\u884c\u8bc4\u4f30\u975e\u5e38\u56f0\u96be", "motivation": "\u5bf9\u57fa\u4e8e\u8ba4\u77e5\u67b6\u6784\u7684\u7406\u8bba\u8fdb\u884c\u8bc4\u4f30\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u6d3b\u52a8\u3002\u7531\u4e8e\u91cd\u53e0\u7684\u539f\u56e0\uff0c\u8bc4\u4f30\u5bf9\u4e8e\u57fa\u4e8e\u751f\u6210\u795e\u7ecf\u67b6\u6784\u7684\u7406\u8bba\u4e5f\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u5229\u7528\u5bf9\u7406\u8bba\u8bc4\u4f30\u7684\u5e7f\u6cdb\u89c6\u89d2\uff0c\u5bf9\u9762\u5411\u5168\u8111\u7684\u8ba4\u77e5\u548c\u751f\u6210\u67b6\u6784\u4ee5\u53ca\u57fa\u4e8e\u8fd9\u4e9b\u67b6\u6784\u7684\u5b8c\u6574\u7cfb\u7edf\u8fdb\u884c\u5e7f\u6cdb\u7684\u6bd4\u8f83\u3002", "result": "\u8bba\u6587\u5bf9\u8ba4\u77e5\u67b6\u6784\u548c\u751f\u6210\u67b6\u6784\u8fdb\u884c\u4e86\u6bd4\u8f83", "conclusion": "\u8bba\u6587\u5bf9\u5168\u8111\u8ba4\u77e5\u548c\u751f\u6210\u67b6\u6784\u8fdb\u884c\u4e86\u6bd4\u8f83"}}
{"id": "2510.04502", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04502", "abs": "https://arxiv.org/abs/2510.04502", "authors": ["Yue Que", "Yingyi Zhang", "Xiangyu Zhao", "Chen Ma"], "title": "Causality-aware Graph Aggregation Weight Estimator for Popularity Debiasing in Top-K Recommendation", "comment": "Accepted by CIKM 2025", "summary": "Graph-based recommender systems leverage neighborhood aggregation to generate\nnode representations, which is highly sensitive to popularity bias, resulting\nin an echo effect during information propagation. Existing graph-based\ndebiasing solutions refine the aggregation process with attempts such as edge\nreconstruction or weight adjustment. However, these methods remain inadequate\nin fully alleviating popularity bias. Specifically, this is because 1) they\nprovide no insights into graph aggregation rationality, thus lacking an\noptimality guarantee; 2) they fail to well balance the training and debiasing\nprocess, which undermines the effectiveness. In this paper, we propose a novel\napproach to mitigate popularity bias through rational modeling of the graph\naggregation process. We reveal that graph aggregation is a special form of\nbackdoor adjustment in causal inference, where the aggregation weight\ncorresponds to the historical interaction likelihood distribution. Based on\nthis insight, we devise an encoder-decoder architecture, namely Causality-aware\nGraph Aggregation Weight Estimator for Debiasing (CAGED), to approximate the\nunbiased aggregation weight by optimizing the evidence lower bound of the\ninteraction likelihood. In order to enhance the debiasing effectiveness during\nearly training stages, we further design a momentum update strategy that\nincrementally refines the aggregation weight matrix. Extensive experiments on\nthree datasets demonstrate that CAGED outperforms existing graph-based\ndebiasing methods. Our implementation is available at\nhttps://github.com/QueYork/CAGED.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u51cf\u8f7b\u57fa\u4e8e\u56fe\u7684\u63a8\u8350\u7cfb\u7edf\u4e2d\u7531\u4e8e\u4fe1\u606f\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u7684\u56de\u58f0\u6548\u5e94\u800c\u4ea7\u751f\u7684\u4eba\u6c14\u504f\u5dee\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u56fe\u805a\u5408\u8fc7\u7a0b\u8fdb\u884c\u5408\u7406\u5efa\u6a21\u6765\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u7684\u53bb\u504f\u65b9\u6cd5\u5728\u5145\u5206\u7f13\u89e3\u4eba\u6c14\u504f\u5dee\u65b9\u9762\u4ecd\u7136\u4e0d\u8db3\uff0c\u56e0\u4e3a\u5b83\u4eec\u6ca1\u6709\u63d0\u4f9b\u5bf9\u56fe\u805a\u5408\u5408\u7406\u6027\u7684\u89c1\u89e3\uff0c\u56e0\u6b64\u7f3a\u4e4f\u6700\u4f18\u6027\u4fdd\u8bc1\uff0c\u5e76\u4e14\u5b83\u4eec\u672a\u80fd\u5f88\u597d\u5730\u5e73\u8861\u8bad\u7ec3\u548c\u53bb\u504f\u8fc7\u7a0b\uff0c\u4ece\u800c\u524a\u5f31\u4e86\u6709\u6548\u6027\u3002", "method": "\u6211\u4eec\u63ed\u793a\u4e86\u56fe\u805a\u5408\u662f\u56e0\u679c\u63a8\u7406\u4e2d\u540e\u95e8\u8c03\u6574\u7684\u4e00\u79cd\u7279\u6b8a\u5f62\u5f0f\uff0c\u5176\u4e2d\u805a\u5408\u6743\u91cd\u5bf9\u5e94\u4e8e\u5386\u53f2\u4ea4\u4e92\u53ef\u80fd\u6027\u5206\u5e03\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5373\u7528\u4e8e\u53bb\u504f\u7684\u56e0\u679c\u611f\u77e5\u56fe\u805a\u5408\u6743\u91cd\u4f30\u8ba1\u5668 (CAGED)\uff0c\u901a\u8fc7\u4f18\u5316\u4ea4\u4e92\u53ef\u80fd\u6027\u7684\u8bc1\u636e\u4e0b\u754c\u6765\u8fd1\u4f3c\u65e0\u504f\u805a\u5408\u6743\u91cd\u3002\u4e3a\u4e86\u589e\u5f3a\u65e9\u671f\u8bad\u7ec3\u9636\u6bb5\u7684\u53bb\u504f\u6548\u679c\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u8bbe\u8ba1\u4e86\u4e00\u79cd\u52a8\u91cf\u66f4\u65b0\u7b56\u7565\uff0c\u4ee5\u9010\u6b65\u4f18\u5316\u805a\u5408\u6743\u91cd\u77e9\u9635\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCAGED \u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u7684\u53bb\u504f\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u56fe\u805a\u5408\u8fc7\u7a0b\u7684\u5408\u7406\u5efa\u6a21\uff0cCAGED\u6709\u6548\u5730\u51cf\u8f7b\u4e86\u4eba\u6c14\u504f\u5dee\uff0c\u5e76\u5728\u63a8\u8350\u6027\u80fd\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2510.03467", "categories": ["cs.CL", "I.2.7; I.6.m"], "pdf": "https://arxiv.org/pdf/2510.03467", "abs": "https://arxiv.org/abs/2510.03467", "authors": ["Brendon Boldt", "David Mortensen"], "title": "Searching for the Most Human-like Emergent Language", "comment": "Accepted for publication at the 2025 Conference on Empirical Methods\n  in Natural Language Processing; 19 pages, 12 figures", "summary": "In this paper, we design a signalling game-based emergent communication\nenvironment to generate state-of-the-art emergent languages in terms of\nsimilarity to human language. This is done with hyperparameter optimization,\nusing XferBench as the objective function. XferBench quantifies the statistical\nsimilarity of emergent language to human language by measuring its suitability\nfor deep transfer learning to human language. Additionally, we demonstrate the\npredictive power of entropy on the transfer learning performance of emergent\nlanguage as well as corroborate previous results on the entropy-minimization\nproperties of emergent communication systems. Finally, we report\ngeneralizations regarding what hyperparameters produce more realistic emergent\nlanguages, that is, ones which transfer better to human language.", "AI": {"tldr": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4fe1\u53f7\u535a\u5f08\u7684\u6d8c\u73b0\u901a\u4fe1\u73af\u5883\uff0c\u4ee5\u751f\u6210\u5728\u4e0e\u4eba\u7c7b\u8bed\u8a00\u76f8\u4f3c\u6027\u65b9\u9762\u6700\u5148\u8fdb\u7684\u6d8c\u73b0\u8bed\u8a00\u3002", "motivation": "\u4f7f\u7528XferBench\u4f5c\u4e3a\u76ee\u6807\u51fd\u6570\uff0c\u901a\u8fc7\u8d85\u53c2\u6570\u4f18\u5316\u6765\u5b8c\u6210\u3002", "method": "XferBench\u901a\u8fc7\u6d4b\u91cf\u6d8c\u73b0\u8bed\u8a00\u5bf9\u4eba\u7c7b\u8bed\u8a00\u7684\u6df1\u5ea6\u8fc1\u79fb\u5b66\u4e60\u7684\u9002\u7528\u6027\uff0c\u6765\u91cf\u5316\u6d8c\u73b0\u8bed\u8a00\u4e0e\u4eba\u7c7b\u8bed\u8a00\u7684\u7edf\u8ba1\u76f8\u4f3c\u6027\u3002", "result": "\u6211\u4eec\u8bc1\u660e\u4e86\u71b5\u5bf9\u6d8c\u73b0\u8bed\u8a00\u7684\u8fc1\u79fb\u5b66\u4e60\u6027\u80fd\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u8bc1\u5b9e\u4e86\u5148\u524d\u5173\u4e8e\u6d8c\u73b0\u901a\u4fe1\u7cfb\u7edf\u7684\u71b5\u6700\u5c0f\u5316\u6027\u8d28\u7684\u7ed3\u679c\u3002", "conclusion": "\u6700\u540e\uff0c\u6211\u4eec\u62a5\u544a\u4e86\u5173\u4e8e\u54ea\u4e9b\u8d85\u53c2\u6570\u4ea7\u751f\u66f4\u771f\u5b9e\u7684\u6d8c\u73b0\u8bed\u8a00\u7684\u6982\u62ec\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u54ea\u4e9b\u8d85\u53c2\u6570\u80fd\u66f4\u597d\u5730\u8fc1\u79fb\u5230\u4eba\u7c7b\u8bed\u8a00\u3002"}}
{"id": "2510.03314", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03314", "abs": "https://arxiv.org/abs/2510.03314", "authors": ["Shucheng Zhang", "Yan Shi", "Bingzhang Wang", "Yuang Zhang", "Muhammad Monjurul Karim", "Kehua Chen", "Chenxi Liu", "Mehrdad Nasri", "Yinhai Wang"], "title": "A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety", "comment": "20 pages, 4 figures, 5 tables", "summary": "Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and\ncyclists, remains a critical global challenge, as conventional\ninfrastructure-based measures often prove inadequate in dynamic urban\nenvironments. Recent advances in artificial intelligence (AI), particularly in\nvisual perception and reasoning, open new opportunities for proactive and\ncontext-aware VRU protection. However, existing surveys on AI applications for\nVRUs predominantly focus on detection, offering limited coverage of other\nvision-based tasks that are essential for comprehensive VRU understanding and\nprotection. This paper presents a state-of-the-art review of recent progress in\ncamera-based AI sensing systems for VRU safety, with an emphasis on\ndevelopments from the past five years and emerging research trends. We\nsystematically examine four core tasks, namely detection and classification,\ntracking and reidentification, trajectory prediction, and intent recognition\nand prediction, which together form the backbone of AI-empowered proactive\nsolutions for VRU protection in intelligent transportation systems. To guide\nfuture research, we highlight four major open challenges from the perspectives\nof data, model, and deployment. By linking advances in visual AI with practical\nconsiderations for real-world implementation, this survey aims to provide a\nfoundational reference for the development of next-generation sensing systems\nto enhance VRU safety.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684AI\u5728\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRU\uff09\u5b89\u5168\u65b9\u9762\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u8fc7\u53bb\u4e94\u5e74\u7684\u8fdb\u5c55\u548c\u65b0\u5174\u7814\u7a76\u8d8b\u52bf\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u57fa\u7840\u8bbe\u65bd\u7684\u63aa\u65bd\u5728\u52a8\u6001\u57ce\u5e02\u73af\u5883\u4e2d\u901a\u5e38\u4e0d\u8db3\u4ee5\u4fdd\u62a4\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRU\uff09\uff0c\u56e0\u6b64\u9700\u8981\u5229\u7528\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7684\u65b0\u673a\u4f1a\u6765\u5b9e\u73b0\u4e3b\u52a8\u548c\u60c5\u5883\u611f\u77e5\u7684VRU\u4fdd\u62a4\u3002", "method": "\u672c\u6587\u7cfb\u7edf\u5730\u8003\u5bdf\u4e86\u56db\u4e2a\u6838\u5fc3\u4efb\u52a1\uff1a\u68c0\u6d4b\u548c\u5206\u7c7b\u3001\u8ddf\u8e2a\u548c\u91cd\u8bc6\u522b\u3001\u8f68\u8ff9\u9884\u6d4b\u4ee5\u53ca\u610f\u56fe\u8bc6\u522b\u548c\u9884\u6d4b\u3002", "result": "\u672c\u6587\u91cd\u70b9\u4ecb\u7ecd\u4e86\u6570\u636e\u3001\u6a21\u578b\u548c\u90e8\u7f72\u89d2\u5ea6\u7684\u56db\u4e2a\u4e3b\u8981\u5f00\u653e\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u4e3a\u5f00\u53d1\u4e0b\u4e00\u4ee3\u4f20\u611f\u7cfb\u7edf\u4ee5\u63d0\u9ad8VRU\u5b89\u5168\u6027\u63d0\u4f9b\u57fa\u7840\u53c2\u8003\u3002"}}
{"id": "2510.03248", "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2510.03248", "abs": "https://arxiv.org/abs/2510.03248", "authors": ["Anusha Agarwal", "Dibakar Roy Sarkar", "Somdatta Goswami"], "title": "Real-Time Brain Biomechanics Prediction with Neural Operators: Toward Clinically Deployable Traumatic Brain Injury Models", "comment": null, "summary": "Traumatic brain injury (TBI) remains a major public health concern, with over\n69 million cases annually worldwide. Finite element (FE) models offer\nhigh-fidelity predictions of brain deformation but are computationally\nexpensive, requiring hours per simulation and limiting their clinical utility\nfor rapid decision-making. This study benchmarks state-of-the-art neural\noperator (NO) architectures for rapid, patient-specific prediction of brain\ndisplacement fields, aiming to enable real-time TBI modeling in clinical and\ntranslational settings. We formulated TBI modeling as an operator learning\nproblem, mapping subject-specific anatomical MRI, magnetic resonance\nelastography (MRE) stiffness maps, and demographic features to full-field 3D\nbrain displacement predictions. Four architectures - Fourier Neural Operator\n(FNO), Factorized FNO (F-FNO), Multi-Grid FNO (MG-FNO), and Deep Operator\nNetwork (DeepONet) were trained and evaluated on 249 MRE datasets across\nphysiologically relevant frequencies (20 - 90 Hz). MG-FNO achieved the highest\naccuracy (MSE = 0.0023, 94.3\\% spatial fidelity) and preserved fine-scale\nfeatures, while F-FNO converged 2$\\times$ faster than standard FNO. DeepONet\noffered the fastest inference (14.5 iterations/s) with a 7$\\times$\ncomputational speed-up over MG-FNO, suggesting utility for embedded or edge\ncomputing applications. All NOs reduced computation time from hours to\nmilliseconds without sacrificing anatomical realism. NOs provide an efficient,\nresolution-invariant approach for predicting brain deformation, opening the\ndoor to real-time, patient-specific TBI risk assessment, clinical triage\nsupport, and optimization of protective equipment. These results highlight the\npotential for NO-based digital twins of the human brain, enabling scalable,\non-demand biomechanical modeling in both clinical and population health\ncontexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u4f7f\u7528\u795e\u7ecf\u7b97\u5b50\uff08NO\uff09\u67b6\u6784\u6765\u5feb\u901f\u9884\u6d4b\u60a3\u8005\u7279\u5b9a\u8111\u4f4d\u79fb\u573a\uff0c\u4ee5\u5b9e\u73b0\u4e34\u5e8a\u548c\u8f6c\u5316\u73af\u5883\u4e2d\u7684\u5b9e\u65f6TBI\u5efa\u6a21\u3002", "motivation": "\u6709\u9650\u5143\uff08FE\uff09\u6a21\u578b\u80fd\u591f\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u5927\u8111\u5f62\u53d8\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9650\u5236\u4e86\u5176\u5728\u5feb\u901f\u51b3\u7b56\u4e2d\u7684\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u5c06TBI\u5efa\u6a21\u95ee\u9898\u8f6c\u5316\u4e3a\u7b97\u5b50\u5b66\u4e60\u95ee\u9898\uff0c\u5c06\u53d7\u8bd5\u8005\u7279\u5b9a\u7684\u89e3\u5256MRI\u3001\u78c1\u5171\u632f\u5f39\u6027\u6210\u50cf\uff08MRE\uff09\u521a\u5ea6\u56fe\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u7279\u5f81\u6620\u5c04\u5230\u5168\u573a3D\u8111\u4f4d\u79fb\u9884\u6d4b\u3002\u4f7f\u7528\u4e86\u56db\u79cd\u67b6\u6784\uff1a\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\uff08FNO\uff09\u3001\u5206\u89e3FNO\uff08F-FNO\uff09\u3001\u591a\u91cd\u7f51\u683cFNO\uff08MG-FNO\uff09\u548c\u6df1\u5ea6\u7b97\u5b50\u7f51\u7edc\uff08DeepONet\uff09\u3002", "result": "MG-FNO\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u7cbe\u5ea6\uff08MSE = 0.0023\uff0c94.3\uff05\u7684\u7a7a\u95f4\u4fdd\u771f\u5ea6\uff09\uff0cF-FNO\u6bd4\u6807\u51c6FNO\u5feb2\u500d\uff0cDeepONet\u63d0\u4f9b\u4e86\u6700\u5feb\u7684\u63a8\u7406\u901f\u5ea6\uff0814.5\u6b21\u8fed\u4ee3/\u79d2\uff09\uff0c\u6bd4MG-FNO\u5feb7\u500d\u3002", "conclusion": "\u795e\u7ecf\u7b97\u5b50\u4e3a\u9884\u6d4b\u5927\u8111\u5f62\u53d8\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5206\u8fa8\u7387\u4e0d\u53d8\u7684\u65b9\u6cd5\uff0c\u4e3a\u5b9e\u65f6\u3001\u60a3\u8005\u7279\u5b9a\u7684TBI\u98ce\u9669\u8bc4\u4f30\u3001\u4e34\u5e8a\u5206\u8bca\u652f\u6301\u548c\u9632\u62a4\u8bbe\u5907\u4f18\u5316\u6253\u5f00\u4e86\u5927\u95e8\u3002"}}
{"id": "2510.03469", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.03469", "abs": "https://arxiv.org/abs/2510.03469", "authors": ["Keshav Ramani", "Vali Tawosi", "Salwa Alamir", "Daniel Borrajo"], "title": "Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification", "comment": null, "summary": "We introduce a novel framework for evaluating the alignment between natural\nlanguage plans and their expected behavior by converting them into Kripke\nstructures and Linear Temporal Logic (LTL) using Large Language Models (LLMs)\nand performing model checking. We systematically evaluate this framework on a\nsimplified version of the PlanBench plan verification dataset and report on\nmetrics like Accuracy, Precision, Recall and F1 scores. Our experiments\ndemonstrate that GPT-5 achieves excellent classification performance (F1 score\nof 96.3%) while almost always producing syntactically perfect formal\nrepresentations that can act as guarantees. However, the synthesis of\nsemantically perfect formal models remains an area for future exploration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u81ea\u7136\u8bed\u8a00\u8ba1\u5212\u4e0e\u5176\u9884\u671f\u884c\u4e3a\u4e4b\u95f4\u7684\u4e00\u81f4\u6027", "motivation": "\u901a\u8fc7\u5c06\u81ea\u7136\u8bed\u8a00\u8ba1\u5212\u8f6c\u6362\u4e3a\u514b\u91cc\u666e\u514b\u7ed3\u6784\u548c\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\uff08LTL\uff09\u5e76\u6267\u884c\u6a21\u578b\u68c0\u67e5", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09", "result": "GPT-5 \u5b9e\u73b0\u4e86\u51fa\u8272\u7684\u5206\u7c7b\u6027\u80fd\uff08F1 \u5206\u6570\u4e3a 96.3%\uff09\uff0c\u5e76\u4e14\u51e0\u4e4e\u603b\u662f\u4ea7\u751f\u53e5\u6cd5\u4e0a\u5b8c\u7f8e\u7684\u6b63\u5f0f\u8868\u793a\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u4fdd\u8bc1", "conclusion": "\u8bed\u4e49\u4e0a\u5b8c\u7f8e\u7684\u6b63\u5f0f\u6a21\u578b\u7684\u5408\u6210\u4ecd\u6709\u5f85\u63a2\u7d22"}}
{"id": "2510.04508", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.04508", "abs": "https://arxiv.org/abs/2510.04508", "authors": ["Lili Xie", "Yi Zhang", "Ruihong Qiu", "Jiajun Liu", "Sen Wang"], "title": "MARCO: A Cooperative Knowledge Transfer Framework for Personalized Cross-domain Recommendations", "comment": "SIGIR-AP 2025", "summary": "Recommender systems frequently encounter data sparsity issues, particularly\nwhen addressing cold-start scenarios involving new users or items. Multi-source\ncross-domain recommendation (CDR) addresses these challenges by transferring\nvaluable knowledge from multiple source domains to enhance recommendations in a\ntarget domain. However, existing reinforcement learning (RL)-based CDR methods\ntypically rely on a single-agent framework, leading to negative transfer issues\ncaused by inconsistent domain contributions and inherent distributional\ndiscrepancies among source domains. To overcome these limitations, MARCO, a\nMulti-Agent Reinforcement Learning-based Cross-Domain recommendation framework,\nis proposed. It leverages cooperative multi-agent reinforcement learning, where\neach agent is dedicated to estimating the contribution from an individual\nsource domain, effectively managing credit assignment and mitigating negative\ntransfer. In addition, an entropy-based action diversity penalty is introduced\nto enhance policy expressiveness and stabilize training by encouraging diverse\nagents' joint actions. Extensive experiments across four benchmark datasets\ndemonstrate MARCO's superior performance over state-of-the-art methods,\nhighlighting its robustness and strong generalization capabilities. The code is\nat https://github.com/xiewilliams/MARCO.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u8de8\u57df\u63a8\u8350\u6846\u67b6(MARCO)\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u7a00\u758f\u6027\u548c\u51b7\u542f\u52a8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8de8\u57df\u63a8\u8350\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5355\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5bfc\u81f4\u7531\u4e8e\u4e0d\u4e00\u81f4\u7684\u9886\u57df\u8d21\u732e\u548c\u56fa\u6709\u7684\u5206\u5e03\u5dee\u5f02\u800c\u4ea7\u751f\u7684\u8d1f\u8fc1\u79fb\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u8de8\u57df\u63a8\u8350\u6846\u67b6MARCO\uff0c\u5176\u4e2d\u6bcf\u4e2a\u667a\u80fd\u4f53\u8d1f\u8d23\u8bc4\u4f30\u6765\u81ea\u5355\u4e2a\u6e90\u9886\u57df\u7684\u8d21\u732e\uff0c\u4ece\u800c\u6709\u6548\u5730\u7ba1\u7406\u4fe1\u7528\u5206\u914d\u5e76\u51cf\u8f7b\u8d1f\u8fc1\u79fb\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u57fa\u4e8e\u71b5\u7684\u52a8\u4f5c\u591a\u6837\u6027\u60e9\u7f5a\uff0c\u901a\u8fc7\u9f13\u52b1\u4e0d\u540c\u7684\u667a\u80fd\u4f53\u7684\u8054\u5408\u884c\u52a8\u6765\u589e\u5f3a\u7b56\u7565\u8868\u8fbe\u80fd\u529b\u5e76\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMARCO\u7684\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u7a81\u51fa\u4e86\u5176\u9c81\u68d2\u6027\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MARCO\u6846\u67b6\u6709\u6548\u5730\u89e3\u51b3\u4e86\u8de8\u57df\u63a8\u8350\u4e2d\u7684\u6570\u636e\u7a00\u758f\u6027\u548c\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.03490", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03490", "abs": "https://arxiv.org/abs/2510.03490", "authors": ["Aneesha Sampath", "Oya Aran", "Emily Mower Provost"], "title": "SEER: The Span-based Emotion Evidence Retrieval Benchmark", "comment": null, "summary": "We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to\ntest Large Language Models' (LLMs) ability to identify the specific spans of\ntext that express emotion. Unlike traditional emotion recognition tasks that\nassign a single label to an entire sentence, SEER targets the underexplored\ntask of emotion evidence detection: pinpointing which exact phrases convey\nemotion. This span-level approach is crucial for applications like empathetic\ndialogue and clinical support, which need to know how emotion is expressed, not\njust what the emotion is. SEER includes two tasks: identifying emotion evidence\nwithin a single sentence, and identifying evidence across a short passage of\nfive consecutive sentences. It contains new annotations for both emotion and\nemotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs\nand find that, while some models approach average human performance on\nsingle-sentence inputs, their accuracy degrades in longer passages. Our error\nanalysis reveals key failure modes, including overreliance on emotion keywords\nand false positives in neutral text.", "AI": {"tldr": "SEER introduces a benchmark for LLMs to identify specific text spans expressing emotion, moving beyond sentence-level classification.", "motivation": "Existing emotion recognition tasks lack the granularity to pinpoint specific phrases conveying emotion, which is crucial for applications like empathetic dialogue.", "method": "The SEER benchmark includes two tasks: identifying emotion evidence within a single sentence and across a 5-sentence passage. It features new annotations on 1200 real-world sentences and evaluates 14 open-source LLMs.", "result": "LLMs perform reasonably well on single-sentence inputs but degrade on longer passages. Error analysis reveals overreliance on keywords and false positives.", "conclusion": "The study highlights the challenges LLMs face in accurately identifying emotion evidence in text, particularly in longer contexts, suggesting areas for improvement."}}
{"id": "2510.03316", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03316", "abs": "https://arxiv.org/abs/2510.03316", "authors": ["Ryan P. Demilt", "Nicholas LaHaye", "Karis Tenneson"], "title": "The View From Space: Navigating Instrumentation Differences with EOFMs", "comment": null, "summary": "Earth Observation Foundation Models (EOFMs) have exploded in prevalence as\ntools for processing the massive volumes of remotely sensed and other earth\nobservation data, and for delivering impact on the many essential earth\nmonitoring tasks. An emerging trend posits using the outputs of pre-trained\nmodels as 'embeddings' which summarize high dimensional data to be used for\ngeneric tasks such as similarity search and content-specific queries. However,\nmost EOFM models are trained only on single modalities of data and then applied\nor benchmarked by matching bands across different modalities. It is not clear\nfrom existing work what impact diverse sensor architectures have on the\ninternal representations of the present suite of EOFMs. We show in this work\nthat the representation space of EOFMs is highly sensitive to sensor\narchitecture and that understanding this difference gives a vital perspective\non the pitfalls of current EOFM design and signals for how to move forward as\nmodel developers, users, and a community guided by robust remote-sensing\nscience.", "AI": {"tldr": "\u5730\u7403\u89c2\u6d4b\u57fa\u7840\u6a21\u578b\uff08EOFMs\uff09\u5df2\u6210\u4e3a\u5904\u7406\u9065\u611f\u548c\u5176\u4ed6\u5730\u7403\u89c2\u6d4b\u6570\u636e\u7684\u91cd\u8981\u5de5\u5177\u3002\u672c\u6587\u7814\u7a76\u4e86\u4e0d\u540c\u4f20\u611f\u5668\u67b6\u6784\u5bf9EOFMs\u5185\u90e8\u8868\u5f81\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684EOFMs\u5927\u591a\u53ea\u5728\u5355\u4e00\u6570\u636e\u6a21\u6001\u4e0a\u8bad\u7ec3\uff0c\u5e76\u4e14\u901a\u8fc7\u5339\u914d\u4e0d\u540c\u6a21\u6001\u7684\u6ce2\u6bb5\u6765\u8fdb\u884c\u5e94\u7528\u6216\u57fa\u51c6\u6d4b\u8bd5\u3002\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u4e0d\u540c\u7684\u4f20\u611f\u5668\u67b6\u6784\u5bf9EOFMs\u7684\u5185\u90e8\u8868\u5f81\u6709\u4ec0\u4e48\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e0d\u540c\u4f20\u611f\u5668\u67b6\u6784\u5bf9EOFMs\u8868\u5f81\u7a7a\u95f4\u7684\u5f71\u54cd\u3002", "result": "EOFMs\u7684\u8868\u5f81\u7a7a\u95f4\u5bf9\u4f20\u611f\u5668\u67b6\u6784\u975e\u5e38\u654f\u611f\u3002", "conclusion": "\u7406\u89e3\u8fd9\u79cd\u5dee\u5f02\u5bf9\u4e8e\u8ba4\u8bc6\u5f53\u524dEOFMs\u8bbe\u8ba1\u7684\u7f3a\u9677\u4ee5\u53ca\u6307\u5bfc\u6a21\u578b\u5f00\u53d1\u8005\u3001\u7528\u6237\u548c\u9065\u611f\u79d1\u5b66\u754c \u0906\u0917\u0947\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.03250", "categories": ["cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.03250", "abs": "https://arxiv.org/abs/2510.03250", "authors": ["Lukas R\u00fcttgers", "Till Aczel", "Andreas Plesner", "Roger Wattenhofer"], "title": "Light Differentiable Logic Gate Networks", "comment": null, "summary": "Differentiable logic gate networks (DLGNs) exhibit extraordinary efficiency\nat inference while sustaining competitive accuracy. But vanishing gradients,\ndiscretization errors, and high training cost impede scaling these networks.\nEven with dedicated parameter initialization schemes from subsequent works,\nincreasing depth still harms accuracy. We show that the root cause of these\nissues lies in the underlying parametrization of logic gate neurons themselves.\nTo overcome this issue, we propose a reparametrization that also shrinks the\nparameter size logarithmically in the number of inputs per gate. For binary\ninputs, this already reduces the model size by 4x, speeds up the backward pass\nby up to 1.86x, and converges in 8.5x fewer training steps. On top of that, we\nshow that the accuracy on CIFAR-100 remains stable and sometimes superior to\nthe original parametrization.", "AI": {"tldr": "DLGNs are efficient but hard to scale. The paper identifies the parametrization of logic gate neurons as the root cause.", "motivation": "Vanishing gradients, discretization errors, and high training cost impede the scaling of DLGNs; increasing depth harms accuracy.", "method": "The paper proposes a reparametrization that also shrinks the parameter size logarithmically in the number of inputs per gate.", "result": "For binary inputs, the reparametrization reduces model size by 4x, speeds up the backward pass by up to 1.86x, and converges in 8.5x fewer training steps. Accuracy on CIFAR-100 remains stable or superior.", "conclusion": "The proposed reparametrization overcomes issues with the original parametrization, improving efficiency and scalability."}}
{"id": "2510.03485", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03485", "abs": "https://arxiv.org/abs/2510.03485", "authors": ["Xiaofei Wen", "Wenjie Jacky Mo", "Yanan Xie", "Peng Qi", "Muhao Chen"], "title": "Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection", "comment": "16 pages, 5 figures", "summary": "Autonomous web agents need to operate under externally imposed or\nhuman-specified policies while generating long-horizon trajectories. However,\nlittle work has examined whether these trajectories comply with such policies,\nor whether policy violations persist across different contexts such as domains\n(e.g., shopping or coding websites) and subdomains (e.g., product search and\norder management in shopping). To address this gap, we introduce\nPolicyGuardBench, a benchmark of about 60k examples for detecting policy\nviolations in agent trajectories. From diverse agent runs, we generate a broad\nset of policies and create both within subdomain and cross subdomain pairings\nwith violation labels. In addition to full-trajectory evaluation,\nPolicyGuardBench also includes a prefix-based violation detection task where\nmodels must anticipate policy violations from truncated trajectory prefixes\nrather than complete sequences. Using this dataset, we train PolicyGuard-4B, a\nlightweight guardrail model that delivers strong detection accuracy across all\ntasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes\nacross domains and preserves high accuracy on unseen settings. Together,\nPolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework\nfor studying policy compliance in web agent trajectories, and show that\naccurate and generalizable guardrails are feasible at small scales.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86PolicyGuardBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4bagent\u8f68\u8ff9\u4e2d\u7b56\u7565\u8fdd\u89c4\u7684\u57fa\u51c6\uff0c\u4ee5\u53caPolicyGuard-4B\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684guardrail\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u8f83\u5c11\u5173\u6ce8web agent\u5728\u751f\u6210\u957f\u7a0b\u8f68\u8ff9\u65f6\u662f\u5426\u7b26\u5408\u5916\u90e8\u6216\u4eba\u4e3a\u8bbe\u5b9a\u7684\u7b56\u7565\uff0c\u4ee5\u53ca\u7b56\u7565\u8fdd\u89c4\u662f\u5426\u5728\u4e0d\u540c\u73af\u5883\uff08\u5982\u9886\u57df\u548c\u5b50\u9886\u57df\uff09\u4e2d\u6301\u7eed\u5b58\u5728\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86\u5305\u542b\u7ea66\u4e07\u4e2a\u793a\u4f8b\u7684PolicyGuardBench\u57fa\u51c6\uff0c\u901a\u8fc7\u751f\u6210\u5404\u79cd\u7b56\u7565\u5e76\u5728\u5b50\u9886\u57df\u5185\u548c\u8de8\u5b50\u9886\u57df\u914d\u5bf9\uff0c\u6807\u6ce8\u8fdd\u89c4\u884c\u4e3a\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u57fa\u4e8e\u524d\u7f00\u7684\u8fdd\u89c4\u68c0\u6d4b\u4efb\u52a1\u3002\u5229\u7528\u8be5\u6570\u636e\u96c6\u8bad\u7ec3\u4e86PolicyGuard-4B\u6a21\u578b\u3002", "result": "PolicyGuard-4B\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u6548\u7387\uff0c\u5e76\u5728\u8de8\u9886\u57df\u548c\u672a\u89c1\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PolicyGuardBench\u548cPolicyGuard-4B\u5171\u540c\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u7814\u7a76web agent\u8f68\u8ff9\u4e2d\u7684\u7b56\u7565\u5408\u89c4\u6027\uff0c\u5e76\u8868\u660e\u5728\u5c0f\u89c4\u6a21\u4e0b\u5b9e\u73b0\u51c6\u786e\u4e14\u53ef\u6cdb\u5316\u7684guardrail\u662f\u53ef\u884c\u7684\u3002"}}
{"id": "2510.04633", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.04633", "abs": "https://arxiv.org/abs/2510.04633", "authors": ["Lukas Gienapp", "Martin Potthast", "Harrisen Scells", "Eugene Yang"], "title": "Topic-Specific Classifiers are Better Relevance Judges than Prompted LLMs", "comment": "15 pages, 3 figures, 2 tables", "summary": "The unjudged document problem, where pooled test collections have incomplete\nrelevance judgments for evaluating new retrieval systems, is a key obstacle to\nthe reusability of test collections in information retrieval. While the de\nfacto standard to deal with the problem is to treat unjudged documents as\nnon-relevant, many alternatives have been proposed, including the use of large\nlanguage models (LLMs) as a relevance judge (LLM-as-a-judge). However, this has\nbeen criticized as circular, since the same LLM can be used as a judge and as a\nranker at the same time. We propose to train topic-specific relevance\nclassifiers instead: By finetuning monoT5 with independent LoRA weight\nadaptation on the judgments of a single assessor for a single topic's pool, we\nalign it to that assessor's notion of relevance for the topic. The system\nrankings obtained through our classifier's relevance judgments achieve a\nSpearmans' $\\rho$ correlation of $>0.95$ with ground truth system rankings. As\nlittle as 128 initial human judgments per topic suffice to improve the\ncomparability of models, compared to treating unjudged documents as\nnon-relevant, while achieving more reliability than existing LLM-as-a-judge\napproaches. Topic-specific relevance classifiers thus are a lightweight and\nstraightforward way to tackle the unjudged document problem, while maintaining\nhuman judgments as the gold standard for retrieval evaluation. Code, models,\nand data are made openly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5904\u7406\u672a\u6807\u6ce8\u6587\u6863\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bad\u7ec3\u7279\u5b9a\u4e3b\u9898\u7684\u76f8\u5173\u6027\u5206\u7c7b\u5668\u6765\u5bf9\u9f50\u8bc4\u4f30\u8005\u7684\u76f8\u5173\u6027\u6982\u5ff5\u3002", "motivation": "\u672a\u6807\u6ce8\u6587\u6863\u95ee\u9898\u662f\u4fe1\u606f\u68c0\u7d22\u4e2d\u6d4b\u8bd5\u96c6\u53ef\u91cd\u7528\u6027\u7684\u4e3b\u8981\u969c\u788d\u3002\u4ee5\u5f80\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u76f8\u5173\u6027\u5224\u65ad\u5668\u7684\u65b9\u6cd5\u5b58\u5728\u5faa\u73af\u8bba\u8bc1\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5728\u5355\u4e2a\u8bc4\u4f30\u8005\u5bf9\u5355\u4e2a\u4e3b\u9898\u6c60\u7684\u5224\u65ad\u4e0a\uff0c\u4f7f\u7528\u72ec\u7acb\u7684LoRA\u6743\u91cd\u8c03\u6574\u6765\u5fae\u8c03monoT5\uff0c\u4ece\u800c\u8bad\u7ec3\u7279\u5b9a\u4e3b\u9898\u7684\u76f8\u5173\u6027\u5206\u7c7b\u5668\u3002", "result": "\u8be5\u5206\u7c7b\u5668\u83b7\u5f97\u7684\u76f8\u5173\u6027\u5224\u65ad\u4e0eground truth\u7cfb\u7edf\u6392\u5e8f\u7684Spearmans' \u03c1\u76f8\u5173\u6027>0.95\u3002\u4e0e\u5c06\u672a\u5224\u65ad\u7684\u6587\u6863\u89c6\u4e3a\u4e0d\u76f8\u5173\u76f8\u6bd4\uff0c\u6bcf\u4e2a\u4e3b\u9898\u4ec5\u9700128\u4e2a\u521d\u59cb\u4eba\u5de5\u5224\u65ad\u5373\u53ef\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u6bd4\u6027\uff0c\u540c\u65f6\u6bd4\u73b0\u6709\u7684LLM-as-a-judge\u65b9\u6cd5\u66f4\u53ef\u9760\u3002", "conclusion": "\u4e3b\u9898\u7279\u5b9a\u7684\u76f8\u5173\u6027\u5206\u7c7b\u5668\u662f\u89e3\u51b3\u672a\u6807\u6ce8\u6587\u6863\u95ee\u9898\u7684\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u76f4\u63a5\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4eba\u5de5\u5224\u65ad\u4f5c\u4e3a\u68c0\u7d22\u8bc4\u4f30\u7684\u9ec4\u91d1\u6807\u51c6\u3002\u4ee3\u7801\u3001\u6a21\u578b\u548c\u6570\u636e\u5df2\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2510.03502", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03502", "abs": "https://arxiv.org/abs/2510.03502", "authors": ["Ali Khairallah", "Arkaitz Zubiaga"], "title": "ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection", "comment": "47 pages, 15 figures. Dataset available at Zenodo:\n  https://doi.org/10.5281/zenodo.17249602 Codebase available at GitHub:\n  https://github.com/alikhairallah/ALHD-Benchmarking", "summary": "We introduce ALHD, the first large-scale comprehensive Arabic dataset\nexplicitly designed to distinguish between human- and LLM-generated texts. ALHD\nspans three genres (news, social media, reviews), covering both MSA and\ndialectal Arabic, and contains over 400K balanced samples generated by three\nleading LLMs and originated from multiple human sources, which enables studying\ngeneralizability in Arabic LLM-genearted text detection. We provide rigorous\npreprocessing, rich annotations, and standardized balanced splits to support\nreproducibility. In addition, we present, analyze and discuss benchmark\nexperiments using our new dataset, in turn identifying gaps and proposing\nfuture research directions. Benchmarking across traditional classifiers,\nBERT-based models, and LLMs (zero-shot and few-shot) demonstrates that\nfine-tuned BERT models achieve competitive performance, outperforming LLM-based\nmodels. Results are however not always consistent, as we observe challenges\nwhen generalizing across genres; indeed, models struggle to generalize when\nthey need to deal with unseen patterns in cross-genre settings, and these\nchallenges are particularly prominent when dealing with news articles, where\nLLM-generated texts resemble human texts in style, which opens up avenues for\nfuture research. ALHD establishes a foundation for research related to Arabic\nLLM-detection and mitigating risks of misinformation, academic dishonesty, and\ncyber threats.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u963f\u62c9\u4f2f\u8bed\u6570\u636e\u96c6ALHD\uff0c\u7528\u4e8e\u533a\u5206\u4eba\u7c7b\u548cLLM\u751f\u6210\u7684\u6587\u672c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u963f\u62c9\u4f2f\u8bedLLM\u751f\u6210\u6587\u672c\u68c0\u6d4b\u4e2d\u7684\u6cdb\u5316\u6027\u95ee\u9898\uff0c\u5e76\u5e94\u5bf9\u865a\u5047\u4fe1\u606f\u3001\u5b66\u672f\u4e0d\u7aef\u548c\u7f51\u7edc\u5a01\u80c1\u7684\u98ce\u9669\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u65b0\u95fb\u3001\u793e\u4ea4\u5a92\u4f53\u548c\u8bc4\u8bba\u4e09\u79cd\u7c7b\u578b\uff0c\u8986\u76d6MSA\u548c\u65b9\u8a00\u963f\u62c9\u4f2f\u8bed\uff0c\u8d85\u8fc740\u4e07\u5e73\u8861\u6837\u672c\u7684\u6570\u636e\u96c6\u3002\u4f7f\u7528\u4e86\u4f20\u7edf\u5206\u7c7b\u5668\u3001BERT\u6a21\u578b\u548cLLM\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5fae\u8c03\u7684BERT\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5728\u8de8\u7c7b\u578b\u6cdb\u5316\u65f6\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u65b0\u95fb\u6587\u7ae0\u4e2d\u3002", "conclusion": "ALHD\u4e3a\u963f\u62c9\u4f2f\u8bedLLM\u68c0\u6d4b\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u7279\u522b\u662f\u5728\u63d0\u9ad8\u8de8\u7c7b\u578b\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u3002"}}
{"id": "2510.03317", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03317", "abs": "https://arxiv.org/abs/2510.03317", "authors": ["G\u00fcnel Aghakishiyeva", "Jiayi Zhou", "Saagar Arya", "James David Poling", "Holly R. Houliston", "Jamie N. Womble", "David W. Johnston", "Brinnae Bent"], "title": "Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring", "comment": "Accepted to NeurIPS 2025 Imageomics Workshop", "summary": "Ecological monitoring is increasingly automated by vision models, yet opaque\npredictions limit trust and field adoption. We present an inpainting-guided,\nperturbation-based explanation technique that produces photorealistic,\nmask-localized edits that preserve scene context. Unlike masking or blurring,\nthese edits stay in-distribution and reveal which fine-grained morphological\ncues drive predictions in tasks such as species recognition and trait\nattribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for\nharbor seal detection in Glacier Bay drone imagery, using\nSegment-Anything-Model-refined masks to support two interventions: (i) object\nremoval/replacement (e.g., replacing seals with plausible ice/water or boats)\nand (ii) background replacement with original animals composited onto new\nscenes. Explanations are assessed by re-scoring perturbed images (flip rate,\nconfidence drop) and by expert review for ecological plausibility and\ninterpretability. The resulting explanations localize diagnostic structures,\navoid deletion artifacts common to traditional perturbations, and yield\ndomain-relevant insights that support expert validation and more trustworthy\ndeployment of AI in ecology.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u4fee\u590d\u7684\u3001\u57fa\u4e8e\u6270\u52a8\u7684\u89e3\u91ca\u6280\u672f\uff0c\u8be5\u6280\u672f\u80fd\u591f\u751f\u6210\u903c\u771f\u7684\u3001\u63a9\u7801\u5b9a\u4f4d\u7684\u7f16\u8f91\uff0c\u4ee5\u4fdd\u7559\u573a\u666f\u4e0a\u4e0b\u6587\uff0c\u5e76\u63ed\u793a\u9a71\u52a8\u7269\u79cd\u8bc6\u522b\u548c\u7279\u5f81\u5f52\u56e0\u7b49\u4efb\u52a1\u9884\u6d4b\u7684\u7ec6\u7c92\u5ea6\u5f62\u6001\u7ebf\u7d22\u3002", "motivation": "\u751f\u6001\u76d1\u6d4b\u8d8a\u6765\u8d8a\u591a\u5730\u901a\u8fc7\u89c6\u89c9\u6a21\u578b\u5b9e\u73b0\u81ea\u52a8\u5316\uff0c\u4f46\u6a21\u7cca\u7684\u9884\u6d4b\u9650\u5236\u4e86\u4fe1\u4efb\u548c\u73b0\u573a\u5e94\u7528\u3002", "method": "\u8be5\u65b9\u6cd5\u4f7f\u7528\u56fe\u50cf\u4fee\u590d\u5f15\u5bfc\u7684\u3001\u57fa\u4e8e\u6270\u52a8\u7684\u89e3\u91ca\u6280\u672f\uff0c\u5e76\u4f7f\u7528 Segment-Anything-Model \u6539\u8fdb\u7684\u63a9\u7801\u6765\u652f\u6301\u4e24\u79cd\u5e72\u9884\uff1a\uff08i\uff09\u5bf9\u8c61\u79fb\u9664/\u66ff\u6362\uff0c\u4ee5\u53ca\uff08ii\uff09\u80cc\u666f\u66ff\u6362\u3002", "result": "\u901a\u8fc7\u91cd\u65b0\u8bc4\u4f30\u6270\u52a8\u56fe\u50cf\uff08\u7ffb\u8f6c\u7387\u3001\u7f6e\u4fe1\u5ea6\u4e0b\u964d\uff09\u548c\u4e13\u5bb6\u5ba1\u67e5\u751f\u6001\u5408\u7406\u6027\u548c\u53ef\u89e3\u91ca\u6027\u6765\u8bc4\u4f30\u89e3\u91ca\u3002\u7ed3\u679c\u89e3\u91ca\u53ef\u4ee5\u5b9a\u4f4d\u8bca\u65ad\u7ed3\u6784\uff0c\u907f\u514d\u4f20\u7edf\u6270\u52a8\u5e38\u89c1\u7684\u5220\u9664\u4f2a\u5f71\uff0c\u5e76\u4ea7\u751f\u4e0e\u9886\u57df\u76f8\u5173\u7684\u89c1\u89e3\uff0c\u4ece\u800c\u652f\u6301\u4e13\u5bb6\u9a8c\u8bc1\u548c\u66f4\u503c\u5f97\u4fe1\u8d56\u7684 AI \u5728\u751f\u6001\u5b66\u4e2d\u7684\u90e8\u7f72\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u91ca\u6280\u672f\uff0c\u53ef\u4ee5\u63d0\u9ad8\u751f\u6001\u5b66\u4e2d\u4eba\u5de5\u667a\u80fd\u90e8\u7f72\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2510.03251", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03251", "abs": "https://arxiv.org/abs/2510.03251", "authors": ["Hanzhong Cao", "Wenbo Yan", "Ying Tan"], "title": "Numerion: A Multi-Hypercomplex Model for Time Series Forecasting", "comment": null, "summary": "Many methods aim to enhance time series forecasting by decomposing the series\nthrough intricate model structures and prior knowledge, yet they are inevitably\nlimited by computational complexity and the robustness of the assumptions. Our\nresearch uncovers that in the complex domain and higher-order hypercomplex\nspaces, the characteristic frequencies of time series naturally decrease.\nLeveraging this insight, we propose Numerion, a time series forecasting model\nbased on multiple hypercomplex spaces. Specifically, grounded in theoretical\nsupport, we generalize linear layers and activation functions to hypercomplex\nspaces of arbitrary power-of-two dimensions and introduce a novel\nReal-Hypercomplex-Real Domain Multi-Layer Perceptron (RHR-MLP) architecture.\nNumerion utilizes multiple RHR-MLPs to map time series into hypercomplex spaces\nof varying dimensions, naturally decomposing and independently modeling the\nseries, and adaptively fuses the latent patterns exhibited in different spaces\nthrough a dynamic fusion mechanism. Experiments validate the model`s\nperformance, achieving state-of-the-art results on multiple public datasets.\nVisualizations and quantitative analyses comprehensively demonstrate the\nability of multi-dimensional RHR-MLPs to naturally decompose time series and\nreveal the tendency of higher dimensional hypercomplex spaces to capture lower\nfrequency features.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNumerion\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u57fa\u4e8e\u591a\u91cd\u8d85\u590d\u6570\u7a7a\u95f4\uff0c\u901a\u8fc7\u5c06\u65f6\u95f4\u5e8f\u5217\u6620\u5c04\u5230\u4e0d\u540c\u7ef4\u5ea6\u7684\u8d85\u590d\u6570\u7a7a\u95f4\u4e2d\uff0c\u5b9e\u73b0\u5bf9\u65f6\u95f4\u5e8f\u5217\u7684\u5206\u89e3\u548c\u72ec\u7acb\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u878d\u5408\u673a\u5236\u81ea\u9002\u5e94\u5730\u878d\u5408\u4e0d\u540c\u7a7a\u95f4\u4e2d\u8868\u73b0\u51fa\u7684\u6f5c\u5728\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u590d\u6742\u7684\u6a21\u578b\u7ed3\u6784\u548c\u5148\u9a8c\u77e5\u8bc6\u8fdb\u884c\u5e8f\u5217\u5206\u89e3\uff0c\u4f46\u8ba1\u7b97\u590d\u6742\u4e14\u5047\u8bbe\u7684\u9c81\u68d2\u6027\u6709\u9650\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Numerion\u6a21\u578b\uff0c\u5b83\u57fa\u4e8e\u591a\u91cd\u8d85\u590d\u6570\u7a7a\u95f4\uff0c\u5c06\u7ebf\u6027\u5c42\u548c\u6fc0\u6d3b\u51fd\u6570\u63a8\u5e7f\u5230\u4efb\u610f2\u7684\u5e42\u7ef4\u7684\u8d85\u590d\u6570\u7a7a\u95f4\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5b9e-\u8d85\u590d\u6570-\u5b9e\u57df\u591a\u5c42\u611f\u77e5\u5668\uff08RHR-MLP\uff09\u67b6\u6784\u3002Numerion\u5229\u7528\u591a\u4e2aRHR-MLP\u5c06\u65f6\u95f4\u5e8f\u5217\u6620\u5c04\u5230\u4e0d\u540c\u7ef4\u5ea6\u7684\u8d85\u590d\u6570\u7a7a\u95f4\uff0c\u4ece\u800c\u81ea\u7136\u5730\u5206\u89e3\u548c\u72ec\u7acb\u5efa\u6a21\u5e8f\u5217\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "\u591a\u7ef4RHR-MLP\u80fd\u591f\u81ea\u7136\u5730\u5206\u89e3\u65f6\u95f4\u5e8f\u5217\uff0c\u5e76\u4e14\u66f4\u9ad8\u7ef4\u7684\u8d85\u590d\u6570\u7a7a\u95f4\u503e\u5411\u4e8e\u6355\u83b7\u66f4\u4f4e\u9891\u7387\u7684\u7279\u5f81\u3002"}}
{"id": "2510.03506", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03506", "abs": "https://arxiv.org/abs/2510.03506", "authors": ["John Nguyen", "Marton Havasi", "Tariq Berrada", "Luke Zettlemoyer", "Ricky T. Q. Chen"], "title": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows", "comment": "https://johnlnguyen.com/oneflow", "summary": "We present OneFlow, the first non-autoregressive multimodal model that\nenables variable-length and concurrent mixed-modal generation. Unlike\nautoregressive models that enforce rigid causal ordering between text and image\ngeneration, OneFlow combines an insertion-based Edit Flow for discrete text\ntokens with Flow Matching for image latents. OneFlow enables concurrent\ntext-image synthesis with hierarchical sampling that prioritizes content over\ngrammar. Through controlled experiments across model sizes from 1B to 8B, we\ndemonstrate that OneFlow outperforms autoregressive baselines on both\ngeneration and understanding tasks while using up to 50% fewer training FLOPs.\nOneFlow surpasses both autoregressive and diffusion-based approaches while\nunlocking new capabilities for concurrent generation, iterative refinement, and\nnatural reasoning-like generation.", "AI": {"tldr": "OneFlow is a non-autoregressive multimodal model for concurrent text-image generation.", "motivation": "Existing autoregressive models have rigid causal ordering between text and image generation.", "method": "OneFlow combines insertion-based Edit Flow for text with Flow Matching for image latents, enabling hierarchical sampling.", "result": "OneFlow outperforms autoregressive baselines on generation and understanding tasks with fewer training FLOPs.", "conclusion": "OneFlow surpasses autoregressive and diffusion-based approaches, unlocking new capabilities for concurrent generation, iterative refinement, and reasoning-like generation."}}
{"id": "2412.18708", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.IR", "68T01 (Primary)", "I.2.0; I.2.1; I.2.7"], "pdf": "https://arxiv.org/pdf/2412.18708", "abs": "https://arxiv.org/abs/2412.18708", "authors": ["Vivek Vellaiyappan Surulimuthu", "Aditya Karnam Gururaj Rao"], "title": "CAG: Chunked Augmented Generation for Google Chrome's Built-in Gemini Nano", "comment": "36 pages, 19 figures", "summary": "We present Chunked Augmented Generation (CAG), an architecture specifically\ndesigned to overcome the context window limitations of Google Chrome's built-in\nGemini Nano model. While Chrome's integration of Gemini Nano represents a\nsignificant advancement in bringing AI capabilities directly to the browser,\nits restricted context window poses challenges for processing large inputs. CAG\naddresses this limitation through intelligent input chunking and processing\nstrategies, enabling efficient handling of extensive content while maintaining\nthe model's performance within browser constraints. Our implementation\ndemonstrates particular efficacy in processing large documents and datasets\ndirectly within Chrome, making sophisticated AI capabilities accessible through\nthe browser without external API dependencies. Get started now at\nhttps://github.com/vivekVells/cag-js.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5206\u5757\u589e\u5f3a\u751f\u6210\uff08CAG\uff09\u67b6\u6784\uff0c\u4ee5\u514b\u670dChrome\u5185\u7f6eGemini Nano\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u3002", "motivation": "Chrome\u5185\u7f6eGemini Nano\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u6709\u9650\uff0c\u5904\u7406\u5927\u578b\u8f93\u5165\u65f6\u9762\u4e34\u6311\u6218\u3002", "method": "\u901a\u8fc7\u667a\u80fd\u8f93\u5165\u5206\u5757\u548c\u5904\u7406\u7b56\u7565\uff0c\u5b9e\u73b0\u9ad8\u6548\u5904\u7406\u5927\u578b\u5185\u5bb9\u3002", "result": "\u8be5\u5b9e\u73b0\u8bc1\u660e\u4e86\u5728Chrome\u4e2d\u76f4\u63a5\u5904\u7406\u5927\u578b\u6587\u6863\u548c\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "CAG\u4f7f\u5f97\u590d\u6742\u7684AI\u529f\u80fd\u53ef\u4ee5\u901a\u8fc7\u6d4f\u89c8\u5668\u8bbf\u95ee\uff0c\u800c\u65e0\u9700\u5916\u90e8API\u4f9d\u8d56\u3002"}}
{"id": "2510.03519", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03519", "abs": "https://arxiv.org/abs/2510.03519", "authors": ["Fangxu Yu", "Hongyu Zhao", "Tianyi Zhou"], "title": "TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning", "comment": null, "summary": "Time series reasoning is crucial to decision-making in diverse domains,\nincluding finance, energy usage, traffic, weather, and scientific discovery.\nWhile existing time series foundation models (TSFMs) can capture low-level\ndynamic patterns and provide accurate forecasting, further analysis usually\nrequires additional background knowledge and sophisticated reasoning, which are\nlacking in most TSFMs but can be achieved through large language models (LLMs).\nOn the other hand, without expensive post-training, LLMs often struggle with\nthe numerical understanding of time series data. Although it is intuitive to\nintegrate the two types of models, developing effective training recipes that\nalign the two modalities for reasoning tasks is still an open challenge. To\nthis end, we propose TS-Reasoner that aligns the latent representations of\nTSFMs with the textual inputs of LLMs for downstream understanding/reasoning\ntasks. Specifically, we propose a simple yet effective method to curate\ndiverse, synthetic pairs of time series and textual captions for alignment\ntraining. We then develop a two-stage training recipe that applies instruction\nfinetuning after the alignment pretraining. Unlike existing works that train an\nLLM to take time series as inputs, we leverage a pretrained TSFM and freeze it\nduring training. Extensive experiments on several benchmarks demonstrate that\nTS-Reasoner not only outperforms a wide range of prevailing LLMs, Vision\nLanguage Models (VLMs), and Time Series LLMs, but also achieves this with\nremarkable data efficiency, e.g., using less than half the training data.", "AI": {"tldr": "TS-Reasoner aligns time series foundation models (TSFMs) with large language models (LLMs) for improved time series reasoning.", "motivation": "Existing TSFMs lack reasoning capabilities, while LLMs struggle with numerical time series data. Integrating them is challenging due to difficulties in aligning the two modalities.", "method": "A two-stage training recipe is proposed that applies instruction finetuning after the alignment pretraining. The method leverages a pretrained TSFM and freezes it during training.", "result": "TS-Reasoner outperforms existing LLMs, VLMs, and Time Series LLMs with remarkable data efficiency.", "conclusion": "TS-Reasoner effectively aligns TSFMs and LLMs for enhanced time series understanding and reasoning."}}
{"id": "2510.03318", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03318", "abs": "https://arxiv.org/abs/2510.03318", "authors": ["Ahmed Kabil", "Ghada Khoriba", "Mina Yousef", "Essam A. Rashed"], "title": "Advances in Medical Image Segmentation: A Comprehensive Survey with a Focus on Lumbar Spine Applications", "comment": "Computers in Biology and Medicine (to appear)", "summary": "Medical Image Segmentation (MIS) stands as a cornerstone in medical image\nanalysis, playing a pivotal role in precise diagnostics, treatment planning,\nand monitoring of various medical conditions. This paper presents a\ncomprehensive and systematic survey of MIS methodologies, bridging the gap\nbetween traditional image processing techniques and modern deep learning\napproaches. The survey encompasses thresholding, edge detection, region-based\nsegmentation, clustering algorithms, and model-based techniques while also\ndelving into state-of-the-art deep learning architectures such as Convolutional\nNeural Networks (CNNs), Fully Convolutional Networks (FCNs), and the widely\nadopted U-Net and its variants. Moreover, integrating attention mechanisms,\nsemi-supervised learning, generative adversarial networks (GANs), and\nTransformer-based models is thoroughly explored. In addition to covering\nestablished methods, this survey highlights emerging trends, including hybrid\narchitectures, cross-modality learning, federated and distributed learning\nframeworks, and active learning strategies, which aim to address challenges\nsuch as limited labeled datasets, computational complexity, and model\ngeneralizability across diverse imaging modalities. Furthermore, a specialized\ncase study on lumbar spine segmentation is presented, offering insights into\nthe challenges and advancements in this relatively underexplored anatomical\nregion. Despite significant progress in the field, critical challenges persist,\nincluding dataset bias, domain adaptation, interpretability of deep learning\nmodels, and integration into real-world clinical workflows.", "AI": {"tldr": "\u672c\u7814\u7a76\u5168\u9762\u56de\u987e\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff08MIS\uff09\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u4f20\u7edf\u56fe\u50cf\u5904\u7406\u548c\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u5728\u7cbe\u786e\u8bca\u65ad\u3001\u6cbb\u7597\u8ba1\u5212\u548c\u76d1\u6d4b\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002", "method": "\u8c03\u67e5\u6db5\u76d6\u4e86\u9608\u503c\u5206\u5272\u3001\u8fb9\u7f18\u68c0\u6d4b\u3001\u533a\u57df\u5206\u5272\u3001\u805a\u7c7b\u7b97\u6cd5\u3001\u6a21\u578b\u65b9\u6cd5\u4ee5\u53ca\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff08\u5982CNN\u3001FCN\u3001U-Net\u53ca\u5176\u53d8\u4f53\uff09\u3002", "result": "\u91cd\u70b9\u4ecb\u7ecd\u4e86\u65b0\u5174\u8d8b\u52bf\uff0c\u5305\u62ec\u6df7\u5408\u67b6\u6784\u3001\u8de8\u6a21\u6001\u5b66\u4e60\u3001\u8054\u90a6\u548c\u5206\u5e03\u5f0f\u5b66\u4e60\u6846\u67b6\u4ee5\u53ca\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u3002", "conclusion": "\u5c3d\u7ba1\u8be5\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u6570\u636e\u96c6\u504f\u5dee\u3001\u9886\u57df\u9002\u5e94\u3001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u4ee5\u53ca\u96c6\u6210\u5230\u5b9e\u9645\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002"}}
{"id": "2510.03252", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03252", "abs": "https://arxiv.org/abs/2510.03252", "authors": ["Duc Kieu", "Kien Do", "Tuan Hoang", "Thao Minh Le", "Tung Kieu", "Dang Nguyen", "Thin Nguyen"], "title": "Universal Multi-Domain Translation via Diffusion Routers", "comment": null, "summary": "Multi-domain translation (MDT) aims to learn translations between multiple\ndomains, yet existing approaches either require fully aligned tuples or can\nonly handle domain pairs seen in training, limiting their practicality and\nexcluding many cross-domain mappings. We introduce universal MDT (UMDT), a\ngeneralization of MDT that seeks to translate between any pair of $K$ domains\nusing only $K-1$ paired datasets with a central domain. To tackle this problem,\nwe propose Diffusion Router (DR), a unified diffusion-based framework that\nmodels all central$\\leftrightarrow$non-central translations with a single noise\npredictor conditioned on the source and target domain labels. DR enables\nindirect non-central translations by routing through the central domain. We\nfurther introduce a novel scalable learning strategy with a variational-bound\nobjective and an efficient Tweedie refinement procedure to support direct\nnon-central mappings. Through evaluation on three large-scale UMDT benchmarks,\nDR achieves state-of-the-art results for both indirect and direct translations,\nwhile lowering sampling cost and unlocking novel tasks such as\nsketch$\\leftrightarrow$segmentation. These results establish DR as a scalable\nand versatile framework for universal translation across multiple domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u591a\u57df\u7ffb\u8bd1\uff08UMDT\uff09\u65b9\u6cd5\uff0c\u65e8\u5728\u4ec5\u4f7f\u7528K-1\u4e2a\u914d\u5bf9\u6570\u636e\u96c6\u5728\u4efb\u610fK\u4e2a\u57df\u4e4b\u95f4\u8fdb\u884c\u7ffb\u8bd1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u5b8c\u5168\u5bf9\u9f50\u7684\u5143\u7ec4\uff0c\u8981\u4e48\u53ea\u80fd\u5904\u7406\u8bad\u7ec3\u4e2d\u770b\u5230\u7684\u57df\u5bf9\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5b9e\u7528\u6027\u5e76\u6392\u9664\u8bb8\u591a\u8de8\u57df\u6620\u5c04\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86Diffusion Router\uff08DR\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u5355\u4e2a\u566a\u58f0\u9884\u6d4b\u5668\u5bf9\u6240\u6709\u4e2d\u5fc3$\nleftrightarrow$\u975e\u4e2d\u5fc3\u7ffb\u8bd1\u8fdb\u884c\u5efa\u6a21\uff0c\u8be5\u566a\u58f0\u9884\u6d4b\u5668\u4ee5\u6e90\u57df\u548c\u76ee\u6807\u57df\u6807\u7b7e\u4e3a\u6761\u4ef6\u3002DR\u901a\u8fc7\u8def\u7531\u4e2d\u5fc3\u57df\u6765\u5b9e\u73b0\u95f4\u63a5\u975e\u4e2d\u5fc3\u7ffb\u8bd1\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u6269\u5c55\u5b66\u4e60\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5177\u6709\u53d8\u5206\u8fb9\u754c\u76ee\u6807\u548c\u6709\u6548\u7684Tweedie\u7ec6\u5316\u7a0b\u5e8f\uff0c\u4ee5\u652f\u6301\u76f4\u63a5\u975e\u4e2d\u5fc3\u6620\u5c04\u3002", "result": "\u5728\u4e09\u4e2a\u5927\u89c4\u6a21UMDT\u57fa\u51c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cDR\u5728\u95f4\u63a5\u548c\u76f4\u63a5\u7ffb\u8bd1\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u91c7\u6837\u6210\u672c\u5e76\u89e3\u9501\u4e86\u8bf8\u5982\u8349\u56fe$\nleftrightarrow$\u5206\u5272\u4e4b\u7c7b\u7684\u65b0\u9896\u4efb\u52a1\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0cDR\u662f\u7528\u4e8e\u8de8\u591a\u4e2a\u57df\u8fdb\u884c\u901a\u7528\u7ffb\u8bd1\u7684\u53ef\u6269\u5c55\u4e14\u901a\u7528\u7684\u6846\u67b6\u3002"}}
{"id": "2510.03605", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03605", "abs": "https://arxiv.org/abs/2510.03605", "authors": ["Adel Javanmard", "Baharan Mirzasoleiman", "Vahab Mirrokni"], "title": "Understanding the Role of Training Data in Test-Time Scaling", "comment": "24 pages, 4 figures", "summary": "Test-time scaling improves the reasoning capabilities of large language\nmodels (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts\n(CoTs). This enables models to tackle more complex problem by breaking them\ndown into additional steps, backtracking, and correcting mistakes. Despite its\nstrong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions\nin the training data under which long CoTs emerge, and when such long CoTs\nimprove the performance, remain unclear. In this paper, we study the\nperformance of test-time scaling for transformers trained on an in-context\nweight prediction task for linear regression. Our analysis provides a\ntheoretical explanation for several intriguing observations: First, at any\nfixed test error, increasing test-time compute allows us to reduce the number\nof in-context examples (context length) in training prompts. Second, if the\nskills required to solve a downstream task are not sufficiently present in the\ntraining data, increasing test-time compute can harm performance. Finally, we\ncharacterize task hardness via the smallest eigenvalue of its feature\ncovariance matrix and show that training on a diverse, relevant, and hard set\nof tasks results in best performance for test-time scaling. We confirm our\nfindings with experiments on large, nonlinear transformer architectures.", "AI": {"tldr": "\u6d4b\u8bd5\u65f6\u6269\u5c55\u901a\u8fc7\u5206\u914d\u989d\u5916\u7684\u8ba1\u7b97\u6765\u751f\u6210\u66f4\u957f\u7684\u601d\u7ef4\u94fe (CoT)\uff0c\u4ece\u800c\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u63a8\u7406\u80fd\u529b\u3002\u8fd9\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u989d\u5916\u7684\u6b65\u9aa4\u3001\u56de\u6eaf\u548c\u7ea0\u6b63\u9519\u8bef\u6765\u5904\u7406\u66f4\u590d\u6742\u7684\u95ee\u9898\u3002", "motivation": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u7ebf\u6027\u56de\u5f52\u7684\u4e0a\u4e0b\u6587\u6743\u91cd\u9884\u6d4b\u4efb\u52a1\u4e0a\u8bad\u7ec3\u7684 transformer \u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u5206\u6790\u4e3a\u51e0\u4e2a\u6709\u8da3\u7684\u89c2\u5bdf\u7ed3\u679c\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\u3002", "method": "\u901a\u8fc7\u5728\u4e0a\u4e0b\u6587\u6743\u91cd\u9884\u6d4b\u4efb\u52a1\u4e0a\u8bad\u7ec3\u7684 transformer \u8fdb\u884c\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\u3002", "result": "\u5728\u4efb\u4f55\u56fa\u5b9a\u7684\u6d4b\u8bd5\u8bef\u5dee\u4e0b\uff0c\u589e\u52a0\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u5141\u8bb8\u6211\u4eec\u51cf\u5c11\u8bad\u7ec3\u63d0\u793a\u4e2d\u7684\u4e0a\u4e0b\u6587\u793a\u4f8b\uff08\u4e0a\u4e0b\u6587\u957f\u5ea6\uff09\u7684\u6570\u91cf\u3002\u5982\u679c\u89e3\u51b3\u4e0b\u6e38\u4efb\u52a1\u6240\u9700\u7684\u6280\u80fd\u6ca1\u6709\u5145\u5206\u5b58\u5728\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\uff0c\u589e\u52a0\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u53ef\u80fd\u4f1a\u635f\u5bb3\u6027\u80fd\u3002\u5bf9\u4efb\u52a1\u96be\u5ea6\u8fdb\u884c\u8868\u5f81\uff0c\u901a\u8fc7\u5176\u7279\u5f81\u534f\u65b9\u5dee\u77e9\u9635\u7684\u6700\u5c0f\u7279\u5f81\u503c\uff0c\u8868\u660e\u5728\u591a\u6837\u5316\u3001\u76f8\u5173\u4e14\u56f0\u96be\u7684\u4efb\u52a1\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u5728\u591a\u6837\u5316\u3001\u76f8\u5173\u4e14\u56f0\u96be\u7684\u4efb\u52a1\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u6700\u4f73\u6027\u80fd\u3002"}}
{"id": "2510.03577", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.03577", "abs": "https://arxiv.org/abs/2510.03577", "authors": ["Ikram Belmadani", "Parisa Nazari Hashemi", "Thomas Sebbag", "Benoit Favre", "Guillaume Fortier", "Solen Quiniou", "Emmanuel Morin", "Richard Dufour"], "title": "LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction", "comment": "in French language", "summary": "This work presents our participation in the EvalLLM 2025 challenge on\nbiomedical Named Entity Recognition (NER) and health event extraction in French\n(few-shot setting). For NER, we propose three approaches combining large\nlanguage models (LLMs), annotation guidelines, synthetic data, and\npost-processing: (1) in-context learning (ICL) with GPT-4.1, incorporating\nautomatic selection of 10 examples and a summary of the annotation guidelines\ninto the prompt, (2) the universal NER system GLiNER, fine-tuned on a synthetic\ncorpus and then verified by an LLM in post-processing, and (3) the open LLM\nLLaMA-3.1-8B-Instruct, fine-tuned on the same synthetic corpus. Event\nextraction uses the same ICL strategy with GPT-4.1, reusing the guideline\nsummary in the prompt. Results show GPT-4.1 leads with a macro-F1 of 61.53% for\nNER and 15.02% for event extraction, highlighting the importance of\nwell-crafted prompting to maximize performance in very low-resource scenarios.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u53c2\u4e0eEvalLLM 2025\u6311\u6218\u8d5b\uff0c\u4f7f\u7528\u5c11\u91cf\u6837\u672c\u5728\u6cd5\u8bed\u751f\u7269\u533b\u5b66\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u548c\u5065\u5eb7\u4e8b\u4ef6\u62bd\u53d6\u4efb\u52a1\u4e2d\u7684\u5de5\u4f5c\u3002", "motivation": "\u5728\u8d44\u6e90\u6781\u4f4e\u7684\u60c5\u51b5\u4e0b\uff0c\u6700\u5927\u5316\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3001\u6807\u6ce8\u6307\u5357\u3001\u5408\u6210\u6570\u636e\u548c\u540e\u5904\u7406\uff0c\u63d0\u51fa\u4e86\u4e09\u79cdNER\u65b9\u6cd5\uff1a(1) \u4f7f\u7528GPT-4.1\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\uff0c\u81ea\u52a8\u9009\u62e910\u4e2a\u793a\u4f8b\u5e76\u5c06\u6807\u6ce8\u6307\u5357\u6458\u8981\u7eb3\u5165\u63d0\u793a\uff1b(2) \u901a\u7528NER\u7cfb\u7edfGLiNER\uff0c\u5728\u5408\u6210\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u7136\u540e\u901a\u8fc7LLM\u8fdb\u884c\u540e\u5904\u7406\u9a8c\u8bc1\uff1b(3) \u5f00\u653eLLM LLaMA-3.1-8B-Instruct\uff0c\u5728\u76f8\u540c\u7684\u5408\u6210\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002\u4e8b\u4ef6\u63d0\u53d6\u4f7f\u7528\u76f8\u540c\u7684ICL\u7b56\u7565\u4e0eGPT-4.1\uff0c\u5728\u63d0\u793a\u4e2d\u91cd\u590d\u4f7f\u7528\u6307\u5357\u6458\u8981\u3002", "result": "GPT-4.1\u5728NER\u4e2d\u4ee561.53%\u7684macro-F1\u548c\u4e8b\u4ef6\u63d0\u53d6\u4e2d\u4ee515.02%\u7684macro-F1\u9886\u5148\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u5bf9\u4e8e\u5728\u6781\u4f4e\u8d44\u6e90\u573a\u666f\u4e2d\u6700\u5927\u5316\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.03521", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03521", "abs": "https://arxiv.org/abs/2510.03521", "authors": ["Ali Elahi"], "title": "Identifying Financial Risk Information Using RAG with a Contrastive Insight", "comment": "7 pages, 1 figure, Workshop on Generative AI in Finance, NeurIPS 2025", "summary": "In specialized domains, humans often compare new problems against similar\nexamples, highlight nuances, and draw conclusions instead of analyzing\ninformation in isolation. When applying reasoning in specialized contexts with\nLLMs on top of a RAG, the pipeline can capture contextually relevant\ninformation, but it is not designed to retrieve comparable cases or related\nproblems.\n  While RAG is effective at extracting factual information, its outputs in\nspecialized reasoning tasks often remain generic, reflecting broad facts rather\nthan context-specific insights. In finance, it results in generic risks that\nare true for the majority of companies. To address this limitation, we propose\na peer-aware comparative inference layer on top of RAG.\n  Our contrastive approach outperforms baseline RAG in text generation metrics\nsuch as ROUGE and BERTScore in comparison with human-generated equity research\nand risk.", "AI": {"tldr": "RAG \u5728\u4e13\u4e1a\u9886\u57df\u7684\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u65e0\u6cd5\u68c0\u7d22\u53ef\u6bd4\u8f83\u7684\u6848\u4f8b\u6216\u76f8\u5173\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e RAG \u7684\u5bf9\u7b49\u611f\u77e5\u6bd4\u8f83\u63a8\u7406\u5c42\uff0c\u4ee5\u89e3\u51b3\u6b64\u9650\u5236\u3002", "motivation": "RAG \u5728\u4e13\u4e1a\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8f93\u51fa\u901a\u5e38\u662f\u901a\u7528\u7684\uff0c\u53cd\u6620\u7684\u662f\u5e7f\u6cdb\u7684\u4e8b\u5b9e\uff0c\u800c\u4e0d\u662f\u7279\u5b9a\u4e8e\u4e0a\u4e0b\u6587\u7684\u89c1\u89e3\u3002\u4f8b\u5982\uff0c\u5728\u91d1\u878d\u9886\u57df\uff0c\u5b83\u4f1a\u5bfc\u81f4\u5bf9\u5927\u591a\u6570\u516c\u53f8\u90fd\u9002\u7528\u7684\u901a\u7528\u98ce\u9669\u3002", "method": "\u5728 RAG \u7684\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5bf9\u7b49\u611f\u77e5\u6bd4\u8f83\u63a8\u7406\u5c42\u3002", "result": "\u5bf9\u6bd4\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u5bf9\u6bd4\u65b9\u6cd5\u5728\u6587\u672c\u751f\u6210\u6307\u6807\uff08\u5982 ROUGE \u548c BERTScore\uff09\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf RAG\uff0c\u4e0e\u4eba\u5de5\u751f\u6210\u7684\u80a1\u7968\u7814\u7a76\u548c\u98ce\u9669\u76f8\u6bd4\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728 RAG \u7684\u57fa\u7840\u4e0a\u589e\u52a0\u4e00\u4e2a\u5bf9\u7b49\u611f\u77e5\u6bd4\u8f83\u63a8\u7406\u5c42\uff0c\u63d0\u9ad8\u4e86 RAG \u5728\u4e13\u4e1a\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.03328", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03328", "abs": "https://arxiv.org/abs/2510.03328", "authors": ["Fiona Victoria Stanley Jothiraj", "Arunaggiri Pandian Karunanidhi", "Seth A. Eichmeyer"], "title": "DECOR: Deep Embedding Clustering with Orientation Robustness", "comment": null, "summary": "In semiconductor manufacturing, early detection of wafer defects is critical\nfor product yield optimization. However, raw wafer data from wafer quality\ntests are often complex, unlabeled, imbalanced and can contain multiple defects\non a single wafer, making it crucial to design clustering methods that remain\nreliable under such imperfect data conditions. We introduce DECOR, a deep\nclustering with orientation robustness framework that groups complex defect\npatterns from wafer maps into consistent clusters. We evaluate our method on\nthe open source MixedWM38 dataset, demonstrating its ability to discover\nclusters without manual tuning. DECOR explicitly accounts for orientation\nvariations in wafer maps, ensuring that spatially similar defects are\nconsistently clustered regardless of its rotation or alignment. Experiments\nindicate that our method outperforms existing clustering baseline methods, thus\nproviding a reliable and scalable solution in automated visual inspection\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDECOR\u7684\u6df1\u5ea6\u805a\u7c7b\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6676\u5706\u5236\u9020\u4e2d\u5bf9\u6676\u5706\u7f3a\u9677\u8fdb\u884c\u5206\u7ec4\uff0c\u5373\u4f7f\u5728\u6570\u636e\u590d\u6742\u3001\u672a\u6807\u8bb0\u548c\u4e0d\u5e73\u8861\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u4fdd\u6301\u53ef\u9760\u6027\u3002", "motivation": "\u5728\u534a\u5bfc\u4f53\u5236\u9020\u4e2d\uff0c\u6676\u5706\u7f3a\u9677\u7684\u65e9\u671f\u68c0\u6d4b\u5bf9\u4e8e\u4f18\u5316\u4ea7\u54c1\u4ea7\u91cf\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u6765\u81ea\u6676\u5706\u8d28\u91cf\u6d4b\u8bd5\u7684\u539f\u59cb\u6676\u5706\u6570\u636e\u901a\u5e38\u662f\u590d\u6742\u7684\u3001\u672a\u6807\u8bb0\u7684\u3001\u4e0d\u5e73\u8861\u7684\uff0c\u5e76\u4e14\u53ef\u80fd\u5728\u5355\u4e2a\u6676\u5706\u4e0a\u5305\u542b\u591a\u4e2a\u7f3a\u9677\uff0c\u8fd9\u4f7f\u5f97\u8bbe\u8ba1\u5728\u8fd9\u79cd\u4e0d\u5b8c\u5584\u7684\u6570\u636e\u6761\u4ef6\u4e0b\u4fdd\u6301\u53ef\u9760\u6027\u7684\u805a\u7c7b\u65b9\u6cd5\u81f3\u5173\u91cd\u8981\u3002", "method": "DECOR\u662f\u4e00\u79cd\u5177\u6709\u65b9\u5411\u9c81\u68d2\u6027\u7684\u6df1\u5ea6\u805a\u7c7b\u6846\u67b6\uff0c\u5b83\u53ef\u4ee5\u5c06\u6676\u5706\u56fe\u4e2d\u7684\u590d\u6742\u7f3a\u9677\u6a21\u5f0f\u5206\u7ec4\u4e3a\u4e00\u81f4\u7684\u805a\u7c7b\u3002DECOR \u663e\u5f0f\u5730\u8003\u8651\u4e86\u6676\u5706\u56fe\u4e2d\u7684\u65b9\u5411\u53d8\u5316\uff0c\u786e\u4fdd\u7a7a\u95f4\u4e0a\u76f8\u4f3c\u7684\u7f3a\u9677\u65e0\u8bba\u5176\u65cb\u8f6c\u6216\u5bf9\u9f50\u65b9\u5f0f\u5982\u4f55\uff0c\u90fd\u80fd\u88ab\u4e00\u81f4\u5730\u805a\u7c7b\u3002", "result": "\u5728\u5f00\u6e90\u7684 MixedWM38 \u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6ca1\u6709\u624b\u52a8\u8c03\u6574\u7684\u60c5\u51b5\u4e0b\u53d1\u73b0\u805a\u7c7b\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u805a\u7c7b\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DECOR\u4e3a\u81ea\u52a8\u5316\u89c6\u89c9\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03253", "categories": ["cs.LG", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03253", "abs": "https://arxiv.org/abs/2510.03253", "authors": ["Heyang Gao", "Zexu Sun", "Erxue Min", "Hengyi Cai", "Shuaiqiang Wang", "Dawei Yin", "Xu Chen"], "title": "Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents", "comment": "Preprint", "summary": "Large Language Models (LLMs) as autonomous agents are increasingly tasked\nwith solving complex, long-horizon problems. Aligning these agents via\npreference-based offline methods like Direct Preference Optimization (DPO) is a\npromising direction, yet it faces a critical granularity mismatch.\nTrajectory-level DPO provides a signal that is too coarse for precise credit\nassignment, while step-level DPO is often too myopic to capture the value of\nmulti-step behaviors. To resolve this challenge, we introduce Hierarchical\nPreference Learning (HPL), a hierarchical framework that optimizes LLM agents\nby leveraging preference signals at multiple, synergistic granularities. While\nHPL incorporates trajectory- and step-level DPO for global and local policy\nstability, its core innovation lies in group-level preference optimization\nguided by a dual-layer curriculum. Our approach first decomposes expert\ntrajectories into semantically coherent action groups and then generates\ncontrasting suboptimal groups to enable preference learning at a fine-grained,\nsub-task level. Then, instead of treating all preference pairs equally, HPL\nintroduces a curriculum scheduler that organizes the learning process from\nsimple to complex. This curriculum is structured along two axes: the group\nlength, representing sub-task complexity, and the sample difficulty, defined by\nthe reward gap between preferred and dispreferred action groups. Experiments on\nthree challenging agent benchmarks show that HPL outperforms existing\nstate-of-the-art methods. Our analyses demonstrate that the hierarchical DPO\nloss effectively integrates preference signals across multiple granularities,\nwhile the dual-layer curriculum is crucial for enabling the agent to solve a\nwide range of tasks, from simple behaviors to complex multi-step sequences.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u5c42\u504f\u597d\u5b66\u4e60\uff08HPL\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316LLM\u667a\u80fd\u4f53\uff0c\u5229\u7528\u591a\u7c92\u5ea6\u504f\u597d\u4fe1\u53f7\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u504f\u597d\u7684\u79bb\u7ebf\u65b9\u6cd5\uff0c\u5982\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\uff0c\u5728\u5bf9\u9f50LLM\u667a\u80fd\u4f53\u4ee5\u89e3\u51b3\u590d\u6742\u95ee\u9898\u65f6\uff0c\u9762\u4e34\u7740\u7c92\u5ea6\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff1a\u8f68\u8ff9\u7ea7\u522bDPO\u8fc7\u4e8e\u7c97\u7cd9\uff0c\u6b65\u9aa4\u7ea7\u522bDPO\u8fc7\u4e8e\u77ed\u89c6\u3002", "method": "HPL\u7ed3\u5408\u4e86\u8f68\u8ff9\u548c\u6b65\u9aa4\u7ea7\u522b\u7684DPO\uff0c\u5e76\u901a\u8fc7\u53cc\u5c42\u8bfe\u7a0b\u5f15\u5bfc\u7ec4\u7ea7\u522b\u504f\u597d\u4f18\u5316\u3002\u8be5\u65b9\u6cd5\u5c06\u4e13\u5bb6\u8f68\u8ff9\u5206\u89e3\u4e3a\u8bed\u4e49\u8fde\u8d2f\u7684\u52a8\u4f5c\u7ec4\uff0c\u5e76\u751f\u6210\u5bf9\u6bd4\u6027\u7684\u6b21\u4f18\u7ec4\uff0c\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u5b50\u4efb\u52a1\u7ea7\u522b\u504f\u597d\u5b66\u4e60\u3002\u6b64\u5916\uff0cHPL\u5f15\u5165\u4e86\u4e00\u4e2a\u8bfe\u7a0b\u8c03\u5ea6\u5668\uff0c\u6839\u636e\u7ec4\u957f\u5ea6\u548c\u6837\u672c\u96be\u5ea6\u7ec4\u7ec7\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHPL\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u5206\u5c42DPO\u635f\u5931\u6709\u6548\u5730\u6574\u5408\u4e86\u8de8\u591a\u4e2a\u7c92\u5ea6\u7684\u504f\u597d\u4fe1\u53f7\uff0c\u800c\u53cc\u5c42\u8bfe\u7a0b\u5bf9\u4e8e\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u89e3\u51b3\u4ece\u7b80\u5355\u884c\u4e3a\u5230\u590d\u6742\u591a\u6b65\u9aa4\u5e8f\u5217\u7684\u5404\u79cd\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.03612", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03612", "abs": "https://arxiv.org/abs/2510.03612", "authors": ["Tanqiu Jiang", "Min Bai", "Nikolaos Pappas", "Yanjun Qi", "Sandesh Swamy"], "title": "Cross-Modal Content Optimization for Steering Web Agent Preferences", "comment": null, "summary": "Vision-language model (VLM)-based web agents increasingly power high-stakes\nselection tasks like content recommendation or product ranking by combining\nmultimodal perception with preference reasoning. Recent studies reveal that\nthese agents are vulnerable against attackers who can bias selection outcomes\nthrough preference manipulations using adversarial pop-ups, image\nperturbations, or content tweaks. Existing work, however, either assumes strong\nwhite-box access, with limited single-modal perturbations, or uses impractical\nsettings. In this paper, we demonstrate, for the first time, that joint\nexploitation of visual and textual channels yields significantly more powerful\npreference manipulations under realistic attacker capabilities. We introduce\nCross-Modal Preference Steering (CPS) that jointly optimizes imperceptible\nmodifications to an item's visual and natural language descriptions, exploiting\nCLIP-transferable image perturbations and RLHF-induced linguistic biases to\nsteer agent decisions. In contrast to prior studies that assume gradient\naccess, or control over webpages, or agent memory, we adopt a realistic\nblack-box threat setup: a non-privileged adversary can edit only their own\nlisting's images and textual metadata, with no insight into the agent's model\ninternals. We evaluate CPS on agents powered by state-of-the-art proprietary\nand open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both\nmovie selection and e-commerce tasks. Our results show that CPS is\nsignificantly more effective than leading baseline methods. For instance, our\nresults show that CPS consistently outperforms baselines across all models\nwhile maintaining 70% lower detection rates, demonstrating both effectiveness\nand stealth. These findings highlight an urgent need for robust defenses as\nagentic systems play an increasingly consequential role in society.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8de8\u6a21\u6001\u504f\u597d\u5f15\u5bfc\uff08CPS\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u89c6\u89c9\u548c\u6587\u672c\u901a\u9053\u4e2d\u7684\u7ec6\u5fae\u4fee\u6539\uff0c\u4ee5\u5728\u73b0\u5b9e\u7684\u9ed1\u76d2\u653b\u51fb\u573a\u666f\u4e0b\u66f4\u6709\u6548\u5730\u64cd\u7eb5\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684Web\u4ee3\u7406\u7684\u9009\u62e9\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684VLM\u4ee3\u7406\u5728\u5185\u5bb9\u63a8\u8350\u6216\u4ea7\u54c1\u6392\u540d\u7b49\u9009\u62e9\u4efb\u52a1\u4e2d\u5bb9\u6613\u53d7\u5230\u653b\u51fb\uff0c\u653b\u51fb\u8005\u53ef\u4ee5\u901a\u8fc7\u5bf9\u6297\u6027\u5f39\u51fa\u7a97\u53e3\u3001\u56fe\u50cf\u6270\u52a8\u6216\u5185\u5bb9\u8c03\u6574\u6765\u64cd\u7eb5\u504f\u597d\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5047\u8bbe\u5f3a\u5927\u7684\u767d\u76d2\u8bbf\u95ee\u6743\u9650\u6216\u4e0d\u5207\u5b9e\u9645\u7684\u8bbe\u7f6e\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86CPS\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8054\u5408\u4f18\u5316\u9879\u76ee\u7684\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e2d\u7684\u4e0d\u6613\u5bdf\u89c9\u7684\u4fee\u6539\uff0c\u5229\u7528CLIP\u53ef\u8fc1\u79fb\u7684\u56fe\u50cf\u6270\u52a8\u548cRLHF\u8bf1\u5bfc\u7684\u8bed\u8a00\u504f\u5dee\u6765\u5f15\u5bfc\u4ee3\u7406\u51b3\u7b56\u3002\u8be5\u65b9\u6cd5\u5728\u9ed1\u76d2\u5a01\u80c1\u8bbe\u7f6e\u4e0b\u8fd0\u884c\uff0c\u653b\u51fb\u8005\u53ea\u80fd\u7f16\u8f91\u81ea\u5df1\u7684\u5217\u8868\u7684\u56fe\u50cf\u548c\u6587\u672c\u5143\u6570\u636e\uff0c\u800c\u65e0\u6cd5\u4e86\u89e3\u4ee3\u7406\u7684\u6a21\u578b\u5185\u90e8\u7ed3\u6784\u3002", "result": "\u5728\u7535\u5f71\u9009\u62e9\u548c\u7535\u5b50\u5546\u52a1\u4efb\u52a1\u4e2d\uff0cCPS\u5728GPT-4.1\u3001Qwen-2.5VL\u548cPixtral-Large\u7b49VLM\u4ee3\u7406\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cCPS\u6bd4\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u68c0\u6d4b\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u968f\u7740\u4ee3\u7406\u7cfb\u7edf\u5728\u793e\u4f1a\u4e2d\u626e\u6f14\u8d8a\u6765\u8d8a\u91cd\u8981\u7684\u89d2\u8272\uff0c\u8feb\u5207\u9700\u8981\u5f3a\u5927\u7684\u9632\u5fa1\u63aa\u65bd\u3002"}}
{"id": "2510.04145", "categories": ["cs.CV", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.04145", "abs": "https://arxiv.org/abs/2510.04145", "authors": ["Chenxin Wang", "Elyas Asadi Shamsabadi", "Zhaohui Chen", "Luming Shen", "Alireza Ahmadian Fard Fini", "Daniel Dias-da-Costa"], "title": "Automating construction safety inspections using a multi-modal vision-language RAG framework", "comment": "33 pages, 11 figures, 7 tables", "summary": "Conventional construction safety inspection methods are often inefficient as\nthey require navigating through large volume of information. Recent advances in\nlarge vision-language models (LVLMs) provide opportunities to automate safety\ninspections through enhanced visual and linguistic understanding. However,\nexisting applications face limitations including irrelevant or unspecific\nresponses, restricted modal inputs and hallucinations. Utilisation of Large\nLanguage Models (LLMs) for this purpose is constrained by availability of\ntraining data and frequently lack real-time adaptability. This study introduces\nSiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG)\nframework for automating construction safety inspection reports by integrating\nvisual and audio inputs. Using real-world data, SiteShield outperformed\nunimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04,\nprecision of 0.76, and recall of 0.96. The findings indicate that SiteShield\noffers a novel pathway to enhance information retrieval and efficiency in\ngenerating safety reports.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a SiteShield \u7684\u591a\u6a21\u6001 LVLM-RAG \u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5efa\u7b51\u5b89\u5168\u68c0\u67e5\u62a5\u544a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u7684\u5e94\u7528\u53d7\u5230\u9650\u5236\uff0c\u7f3a\u4e4f\u5b9e\u65f6\u9002\u5e94\u6027\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001 LVLM-RAG \u6846\u67b6 SiteShield\uff0c\u96c6\u6210\u89c6\u89c9\u548c\u97f3\u9891\u8f93\u5165\u3002", "result": "SiteShield \u7684 F1 \u503c\u4e3a 0.82\uff0chamming \u635f\u5931\u4e3a 0.04\uff0c\u7cbe\u786e\u7387\u4e3a 0.76\uff0c\u53ec\u56de\u7387\u4e3a 0.96\uff0c\u4f18\u4e8e\u6ca1\u6709 RAG \u7684\u5355\u6a21\u6001 LLM\u3002", "conclusion": "SiteShield \u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u9014\u5f84\u6765\u63d0\u9ad8\u5b89\u5168\u62a5\u544a\u751f\u6210\u7684\u4fe1\u606f\u68c0\u7d22\u548c\u6548\u7387\u3002"}}
{"id": "2510.03527", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03527", "abs": "https://arxiv.org/abs/2510.03527", "authors": ["Sayan Ghosh", "Shahzaib Saqib Warraich", "Dhruv Tarsadiya", "Gregory Yauney", "Swabha Swayamdipta"], "title": "Sample, Align, Synthesize: Graph-Based Response Synthesis with ConGrs", "comment": null, "summary": "Language models can be sampled multiple times to access the distribution\nunderlying their responses, but existing methods cannot efficiently synthesize\nrich epistemic signals across different long-form responses. We introduce\nConsensus Graphs (ConGrs), a flexible DAG-based data structure that represents\nshared information, as well as semantic variation in a set of sampled LM\nresponses to the same prompt. We construct ConGrs using a light-weight lexical\nsequence alignment algorithm from bioinformatics, supplemented by the targeted\nusage of a secondary LM judge. Further, we design task-dependent decoding\nmethods to synthesize a single, final response from our ConGr data structure.\nOur experiments show that synthesizing responses from ConGrs improves factual\nprecision on two biography generation tasks by up to 31% over an average\nresponse and reduces reliance on LM judges by more than 80% compared to other\nmethods. We also use ConGrs for three refusal-based tasks requiring abstention\non unanswerable queries and find that abstention rate is increased by up to\n56%. We apply our approach to the MATH and AIME reasoning tasks and find an\nimprovement over self-verification and majority vote baselines by up to 6\npoints of accuracy. We show that ConGrs provide a flexible method for capturing\nvariation in LM responses and using the epistemic signals provided by response\nvariation to synthesize more effective responses.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a Consensus Graphs (ConGrs) \u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6574\u5408\u8bed\u8a00\u6a21\u578b\u591a\u6b21\u91c7\u6837\u751f\u6210\u7ed3\u679c\u4e2d\u7684\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u7efc\u5408\u4e0d\u540c\u957f\u6587\u672c\u56de\u590d\u4e2d\u7684\u4e30\u5bcc\u8ba4\u77e5\u4fe1\u53f7\u3002", "method": "\u4f7f\u7528\u751f\u7269\u4fe1\u606f\u5b66\u7684\u8f7b\u91cf\u7ea7\u8bcd\u6c47\u5e8f\u5217\u5bf9\u9f50\u7b97\u6cd5\u6784\u5efa ConGrs\uff0c\u5e76\u8f85\u4ee5\u8f85\u52a9\u8bed\u8a00\u6a21\u578b\u5224\u65ad\u3002\u8bbe\u8ba1\u4e86\u4efb\u52a1\u76f8\u5173\u7684\u89e3\u7801\u65b9\u6cd5\uff0c\u4ece ConGr \u6570\u636e\u7ed3\u6784\u4e2d\u5408\u6210\u5355\u4e2a\u6700\u7ec8\u56de\u590d\u3002", "result": "\u5728\u4e24\u4e2a\u4f20\u8bb0\u751f\u6210\u4efb\u52a1\u4e2d\uff0cConGrs \u7684\u4e8b\u5b9e\u51c6\u786e\u7387\u6bd4\u5e73\u5747\u56de\u590d\u63d0\u9ad8\u4e86 31%\uff0c\u5bf9\u8bed\u8a00\u6a21\u578b\u5224\u65ad\u7684\u4f9d\u8d56\u51cf\u5c11\u4e86 80% \u4ee5\u4e0a\u3002\u5728\u4e09\u4e2a\u57fa\u4e8e\u62d2\u7edd\u7684\u4efb\u52a1\u4e2d\uff0c\u62d2\u7edd\u7387\u63d0\u9ad8\u4e86 56%\u3002\u5728 MATH \u548c AIME \u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u51c6\u786e\u7387\u6bd4\u81ea\u6211\u9a8c\u8bc1\u548c\u591a\u6570\u6295\u7968\u57fa\u7ebf\u63d0\u9ad8\u4e86 6 \u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "ConGrs \u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6355\u83b7\u8bed\u8a00\u6a21\u578b\u56de\u590d\u4e2d\u7684\u53d8\u5316\uff0c\u5e76\u5229\u7528\u56de\u590d\u53d8\u5316\u63d0\u4f9b\u7684\u8ba4\u77e5\u4fe1\u53f7\u6765\u5408\u6210\u66f4\u6709\u6548\u7684\u56de\u590d\u3002"}}
{"id": "2510.03337", "categories": ["cs.CV", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.03337", "abs": "https://arxiv.org/abs/2510.03337", "authors": ["Andrey A. Lebedev", "Victor B. Kazantsev", "Sergey V. Stasenko"], "title": "Error correction in multiclass image classification of facial emotion on unbalanced samples", "comment": null, "summary": "This paper considers the problem of error correction in multi-class\nclassification of face images on unbalanced samples. The study is based on the\nanalysis of a data frame containing images labeled by seven different emotional\nstates of people of different ages. Particular attention is paid to the problem\nof class imbalance, in which some emotions significantly prevail over others.\nTo solve the classification problem, a neural network model based on LSTM with\nan attention mechanism focusing on key areas of the face that are informative\nfor emotion recognition is used. As part of the experiments, the model is\ntrained on all possible configurations of subsets of six classes with\nsubsequent error correction for the seventh class, excluded at the training\nstage. The results show that correction is possible for all classes, although\nthe degree of success varies: some classes are better restored, others are\nworse. In addition, on the test sample, when correcting some classes, an\nincrease in key quality metrics for small classes was recorded, which indicates\nthe promise of the proposed approach in solving applied problems related to the\nsearch for rare events, for example, in anti-fraud systems. Thus, the proposed\nmethod can be effectively applied in facial expression analysis systems and in\ntasks requiring stable classification under skewed class distribution.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e0d\u5e73\u8861\u6837\u672c\u4e0a\u4eba\u8138\u56fe\u50cf\u591a\u7c7b\u5206\u7c7b\u4e2d\u7684\u8bef\u5dee\u6821\u6b63\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u57fa\u4e8e\u5305\u542b\u4e0d\u540c\u5e74\u9f84\u6bb5\u7684\u4eba\u7684\u4e03\u79cd\u4e0d\u540c\u60c5\u7eea\u72b6\u6001\u6807\u8bb0\u7684\u56fe\u50cf\u7684\u6570\u636e\u6846\u67b6\u7684\u5206\u6790\uff0c\u7279\u522b\u5173\u6ce8\u7c7b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5176\u4e2d\u4e00\u4e9b\u60c5\u7eea\u660e\u663e\u4f18\u4e8e\u5176\u4ed6\u60c5\u7eea\u3002", "method": "\u4e3a\u4e86\u89e3\u51b3\u5206\u7c7b\u95ee\u9898\uff0c\u4f7f\u7528\u4e86\u57fa\u4e8eLSTM\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5177\u6709\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4e13\u6ce8\u4e8e\u4eba\u8138\u7684\u5173\u952e\u533a\u57df\uff0c\u8fd9\u4e9b\u533a\u57df\u5bf9\u4e8e\u60c5\u7eea\u8bc6\u522b\u5177\u6709\u4fe1\u606f\u6027\u3002\u4f5c\u4e3a\u5b9e\u9a8c\u7684\u4e00\u90e8\u5206\uff0c\u8be5\u6a21\u578b\u5728\u516d\u4e2a\u7c7b\u7684\u5b50\u96c6\u7684\u6240\u6709\u53ef\u80fd\u914d\u7f6e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u968f\u540e\u5bf9\u5728\u8bad\u7ec3\u9636\u6bb5\u6392\u9664\u7684\u7b2c\u4e03\u7c7b\u8fdb\u884c\u8bef\u5dee\u6821\u6b63\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6240\u6709\u7c7b\u90fd\u53ef\u4ee5\u8fdb\u884c\u6821\u6b63\uff0c\u5c3d\u7ba1\u6210\u529f\u7a0b\u5ea6\u4e0d\u540c\uff1a\u4e00\u4e9b\u7c7b\u53ef\u4ee5\u66f4\u597d\u5730\u6062\u590d\uff0c\u800c\u53e6\u4e00\u4e9b\u7c7b\u5219\u8f83\u5dee\u3002\u6b64\u5916\uff0c\u5728\u6d4b\u8bd5\u6837\u672c\u4e2d\uff0c\u5f53\u6821\u6b63\u67d0\u4e9b\u7c7b\u65f6\uff0c\u8bb0\u5f55\u5230\u5c0f\u7c7b\u7684\u5173\u952e\u8d28\u91cf\u6307\u6807\u6709\u6240\u63d0\u9ad8\uff0c\u8fd9\u8868\u660e\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u89e3\u51b3\u4e0e\u641c\u7d22\u7f55\u89c1\u4e8b\u4ef6\u76f8\u5173\u7684\u5e94\u7528\u95ee\u9898\uff08\u4f8b\u5982\uff0c\u5728\u53cd\u6b3a\u8bc8\u7cfb\u7edf\u4e2d\uff09\u4e2d\u5177\u6709\u524d\u666f\u3002", "conclusion": "\u56e0\u6b64\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u5e94\u7528\u4e8e\u9762\u90e8\u8868\u60c5\u5206\u6790\u7cfb\u7edf\u4ee5\u53ca\u9700\u8981\u5728\u503e\u659c\u7c7b\u5206\u5e03\u4e0b\u8fdb\u884c\u7a33\u5b9a\u5206\u7c7b\u7684\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2510.03254", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03254", "abs": "https://arxiv.org/abs/2510.03254", "authors": ["David Benfield", "Stefano Coniglio", "Phan Tu Vuong", "Alain Zemkoho"], "title": "Adversarial training with restricted data manipulation", "comment": "21 page, 5 figures", "summary": "Adversarial machine learning concerns situations in which learners face\nattacks from active adversaries. Such scenarios arise in applications such as\nspam email filtering, malware detection and fake image generation, where\nsecurity methods must be actively updated to keep up with the everimproving\ngeneration of malicious data. Pessimistic Bilevel optimisation has been shown\nto be an effective method of training resilient classifiers against such\nadversaries. By modelling these scenarios as a game between the learner and the\nadversary, we anticipate how the adversary will modify their data and then\ntrain a resilient classifier accordingly. However, since existing pessimistic\nbilevel approaches feature an unrestricted adversary, the model is vulnerable\nto becoming overly pessimistic and unrealistic. When finding the optimal\nsolution that defeats the classifier, it is possible that the adversary's data\nbecomes nonsensical and loses its intended nature. Such an adversary will not\nproperly reflect reality, and consequently, will lead to poor classifier\nperformance when implemented on real-world data. By constructing a constrained\npessimistic bilevel optimisation model, we restrict the adversary's movements\nand identify a solution that better reflects reality. We demonstrate through\nexperiments that this model performs, on average, better than the existing\napproach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea6\u675f\u60b2\u89c2\u53cc\u5c42\u4f18\u5316\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u6297\u673a\u5668\u5b66\u4e60\u4e2d\u5206\u7c7b\u5668\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u60b2\u89c2\u53cc\u5c42\u65b9\u6cd5\u7531\u4e8e\u65e0\u9650\u5236\u7684\u5bf9\u6297\uff0c\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u8fc7\u4e8e\u60b2\u89c2\u548c\u4e0d\u5207\u5b9e\u9645\uff0c\u4ece\u800c\u964d\u4f4e\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u7684\u5206\u7c7b\u5668\u6027\u80fd\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u7ea6\u675f\u60b2\u89c2\u53cc\u5c42\u4f18\u5316\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5bf9\u6297\u8005\u7684\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5e73\u5747\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u9650\u5236\u5bf9\u6297\u8005\u7684\u884c\u4e3a\uff0c\u53ef\u4ee5\u83b7\u5f97\u66f4\u7b26\u5408\u5b9e\u9645\u60c5\u51b5\u7684\u89e3\uff0c\u4ece\u800c\u63d0\u9ad8\u5206\u7c7b\u5668\u7684\u6027\u80fd\u3002"}}
{"id": "2510.03632", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03632", "abs": "https://arxiv.org/abs/2510.03632", "authors": ["Jiaxi Li", "Yucheng Shi", "Jin Lu", "Ninghao Liu"], "title": "MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information", "comment": "18 pages", "summary": "Tree search has become as a representative framework for test-time reasoning\nwith large language models (LLMs), exemplified by methods such as\nTree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning\npaths. However, it remains difficult to provide instant and reliable\nquantitative assessments of intermediate reasoning step quality, and extensive\npath exploration is computationally costly. To address this, we propose Mutual\nInformation Tree Search (MITS), a novel framework that guides reasoning with\ninformation-theoretic principles. MITS introduces an effective scoring function\nbased on pointwise mutual information (PMI), which enables step-wise evaluation\nof reasoning paths and search tree expansion via beam search without expensive\nlook-ahead simulations, achieving superior reasoning performances while\nmaintaining computational efficiency. The framework is complemented by an\nentropy-based dynamic sampling strategy that adaptively allocates computational\nresources to uncertain reasoning steps where exploration is most beneficial.\nFor final prediction, MITS employs a weighted voting scheme that combines PMI\nscores with prediction consensus. Through comprehensive experiments on diverse\nreasoning benchmarks, MITS consistently surpasses baseline methods,\nestablishing a principled and efficient framework for LLM reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u4e92\u4fe1\u606f\u6811\u641c\u7d22 (MITS) \u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u4fe1\u606f\u8bba\u539f\u7406\u6307\u5bfc LLM \u7684\u63a8\u7406\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6811\u641c\u7d22\u65b9\u6cd5\u96be\u4ee5\u5bf9\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7684\u8d28\u91cf\u8fdb\u884c\u5373\u65f6\u548c\u53ef\u9760\u7684\u5b9a\u91cf\u8bc4\u4f30\uff0c\u5e76\u4e14\u5e7f\u6cdb\u7684\u8def\u5f84\u63a2\u7d22\u5728\u8ba1\u7b97\u4e0a\u662f\u6602\u8d35\u7684\u3002", "method": "MITS \u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u9010\u70b9\u4e92\u4fe1\u606f (PMI) \u7684\u6709\u6548\u8bc4\u5206\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u80fd\u591f\u9010\u6b65\u8bc4\u4f30\u63a8\u7406\u8def\u5f84\u5e76\u901a\u8fc7 beam search \u6269\u5c55\u641c\u7d22\u6811\uff0c\u800c\u65e0\u9700\u6602\u8d35\u7684look-ahead\u6a21\u62df\u3002\u8be5\u6846\u67b6\u8f85\u4ee5\u57fa\u4e8e\u71b5\u7684\u52a8\u6001\u62bd\u6837\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u81ea\u9002\u5e94\u5730\u5c06\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u7ed9\u63a2\u7d22\u6700\u6709\u76ca\u7684\u4e0d\u786e\u5b9a\u63a8\u7406\u6b65\u9aa4\u3002\u5bf9\u4e8e\u6700\u7ec8\u9884\u6d4b\uff0cMITS \u91c7\u7528\u52a0\u6743\u6295\u7968\u65b9\u6848\uff0c\u5c06 PMI \u5206\u6570\u4e0e\u9884\u6d4b\u5171\u8bc6\u76f8\u7ed3\u5408\u3002", "result": "\u5728\u5404\u79cd\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cMITS \u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MITS \u5efa\u7acb\u4e86\u4e00\u4e2a\u6709\u539f\u5219\u4e14\u9ad8\u6548\u7684 LLM \u63a8\u7406\u6846\u67b6"}}
{"id": "2510.04226", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04226", "abs": "https://arxiv.org/abs/2510.04226", "authors": ["Dustin Wright", "Sarah Masud", "Jared Moore", "Srishti Yadav", "Maria Antoniak", "Chan Young Park", "Isabelle Augenstein"], "title": "Epistemic Diversity and Knowledge Collapse in Large Language Models", "comment": "16 pages; 8 figures, 4 tables", "summary": "Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u503e\u5411\u4e8e\u751f\u6210\u5728\u8bcd\u6c47\u3001\u8bed\u4e49\u548c\u98ce\u683c\u4e0a\u540c\u8d28\u7684\u6587\u672c\uff0c\u5bfc\u81f4\u77e5\u8bc6\u574d\u584c\u7684\u98ce\u9669\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u6d4b\u91cf\u8ba4\u77e5\u591a\u6837\u6027\uff0c\u5e76\u5bf9LLM\u77e5\u8bc6\u574d\u584c\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u7684\u540c\u8d28\u5316\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5c01\u95ed\u5f0f\u591a\u9879\u9009\u62e9\u6216\u6a21\u7cca\u8bed\u4e49\u7279\u5f81\u4e0a\uff0c\u5e76\u4e14\u6ca1\u6709\u8003\u5bdf\u8de8\u65f6\u95f4\u548c\u6587\u5316\u80cc\u666f\u7684\u8d8b\u52bf\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u6d4b\u91cf\u8ba4\u77e5\u591a\u6837\u6027\uff0c\u5373LLM\u8f93\u51fa\u4e2d\u771f\u5b9e\u4e16\u754c\u58f0\u660e\u7684\u53d8\u5316\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u8f83\u65b0\u7684\u6a21\u578b\u503e\u5411\u4e8e\u751f\u6210\u66f4\u591a\u6837\u5316\u7684\u58f0\u660e\uff0c\u4f46\u51e0\u4e4e\u6240\u6709\u6a21\u578b\u7684\u8ba4\u77e5\u591a\u6837\u6027\u90fd\u4f4e\u4e8e\u57fa\u672c\u7684\u7f51\u7edc\u641c\u7d22\u3002\u6a21\u578b\u5927\u5c0f\u5bf9\u8ba4\u77e5\u591a\u6837\u6027\u6709\u8d1f\u9762\u5f71\u54cd\uff0c\u800c\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u6709\u6b63\u9762\u5f71\u54cd\uff0c\u4f46RAG\u7684\u6539\u8fdb\u56e0\u6587\u5316\u80cc\u666f\u800c\u5f02\u3002\u4e0e\u4f20\u7edf\u7684\u77e5\u8bc6\u6765\u6e90(\u7ef4\u57fa\u767e\u79d1)\u76f8\u6bd4\uff0c\u7279\u5b9a\u56fd\u5bb6\u7684\u58f0\u660e\u66f4\u591a\u5730\u53cd\u6620\u82f1\u8bed\uff0c\u800c\u4e0d\u662f\u5f53\u5730\u8bed\u8a00\uff0c\u7a81\u51fa\u4e86\u8ba4\u77e5\u8868\u5f81\u7684\u5dee\u8ddd\u3002", "conclusion": "\u672c\u6587\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u77e5\u8bc6\u574d\u584c\u7684\u98ce\u9669\uff0c\u5e76\u4e14\u5728\u8ba4\u77e5\u591a\u6837\u6027\u65b9\u9762\u5b58\u5728\u5dee\u8ddd\u3002"}}
{"id": "2510.03528", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03528", "abs": "https://arxiv.org/abs/2510.03528", "authors": ["Ahmed Alajrami", "Xingwei Tan", "Nikolaos Aletras"], "title": "Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance", "comment": null, "summary": "Instruction-tuning plays a vital role in enhancing the task-solving abilities\nof large language models (LLMs), improving their usability in generating\nhelpful responses on various tasks. However, previous work has demonstrated\nthat they are sensitive to minor variations in instruction phrasing. In this\npaper, we explore whether introducing perturbations in instruction-tuning data\ncan enhance LLMs' resistance against noisy instructions. We focus on how\ninstruction-tuning with perturbations, such as removing stop words or shuffling\nwords, affects LLMs' performance on the original and perturbed versions of\nwidely-used benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics\nand potential shifts in model behavior. Surprisingly, our results suggest that\ninstruction-tuning on perturbed instructions can, in some cases, improve\ndownstream performance. These findings highlight the importance of including\nperturbed instructions in instruction-tuning, which can make LLMs more\nresilient to noisy user inputs.", "AI": {"tldr": "Instruction-tuning with perturbed instructions can improve LLMs' robustness to noisy instructions.", "motivation": "LLMs are sensitive to minor variations in instruction phrasing.", "method": "Instruction-tuning with perturbations (removing stop words, shuffling words) and evaluating on original/perturbed benchmarks (MMLU, BBH, GSM8K).", "result": "Instruction-tuning on perturbed instructions can improve downstream performance in some cases.", "conclusion": "Including perturbed instructions in instruction-tuning can make LLMs more resilient to noisy user inputs."}}
{"id": "2510.03341", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03341", "abs": "https://arxiv.org/abs/2510.03341", "authors": ["Bozheng Li", "Miao Yang", "Zhenhan Chen", "Jiawang Cao", "Mushui Liu", "Yi Lu", "Yongliang Wu", "Bin Zhang", "Yangguang Ji", "Licheng Tang", "Jay Wu", "Wenbo Zhu"], "title": "OpusAnimation: Code-Based Dynamic Chart Generation", "comment": "working in progress", "summary": "Dynamic Chart Generation (DCG) involves producing code-rendered animated\nvisualizations as charts. While recent advances in multi-modal large language\nmodels (MLLMs) have significantly improved their capability on static chart\ngeneration and comprehension, MLLMs' potential for handling dynamic chart\ngeneration and understanding remains underexplored. To bridge this research\ngap, we introduce DCG-Bench (Dynamic Chart Generation Benchmark), the first\nbenchmark evaluating MLLM's capability on dynamic chart generation tasks from\nthree dimensions: Simple Text-to-Chart, Detailed Text-to-Chart, and\nVideo-to-Chart tasks. We construct DCG-8K, a high-quality DCG dataset with\nannotations covering instruction-code-video triplets and QA pairs for both code\nand video evaluation. Based on DCG-8K, we explored a two-stage training recipe,\nproposing Joint-Code-Visual Reward for group relative policy optimization to\nconstruct expert MLLM Qwen2.5-VL-DCG-3B for the DCG task. Our benchmarking\nresult reveals shortcomings of existing MLLMs in the visual-to-chart task, and\nour model beats the best open-sourced MLLM with an average 8.31% performance\ngain across three tasks, and shows on par performance against proprietary\nmodels with only 3B parameters, proving the effectiveness of our training\nrecipe. Our code and dataset will be publicly available.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u52a8\u6001\u56fe\u8868\u751f\u6210\u4efb\u52a1\u80fd\u529b\u7684\u57fa\u51c6DCG-Bench\uff0c\u5e76\u6784\u5efa\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6DCG-8K\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\uff0c\u4f7f\u7528\u8054\u5408\u4ee3\u7801-\u89c6\u89c9\u5956\u52b1\u8fdb\u884c\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u4e13\u5bb6MLLM Qwen2.5-VL-DCG-3B\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709MLLM\u5728\u89c6\u89c9\u5230\u56fe\u8868\u4efb\u52a1\u4e2d\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u672c\u6587\u6a21\u578b\u4f18\u4e8e\u6700\u4f73\u5f00\u6e90MLLM\uff0c\u5e76\u4e0e\u53c2\u6570\u91cf\u66f4\u5927\u7684\u95ed\u6e90\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9759\u6001\u56fe\u8868\u7684\u751f\u6210\u548c\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u5728\u5904\u7406\u52a8\u6001\u56fe\u8868\u7684\u751f\u6210\u548c\u7406\u89e3\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "1. \u6784\u5efa\u4e86\u9996\u4e2a\u8bc4\u4f30MLLM\u5728\u52a8\u6001\u56fe\u8868\u751f\u6210\u4efb\u52a1\u80fd\u529b\u7684\u57fa\u51c6DCG-Bench\u3002\n2. \u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u52a8\u6001\u56fe\u8868\u751f\u6210\u6570\u636e\u96c6DCG-8K\uff0c\u5305\u542b\u6307\u4ee4-\u4ee3\u7801-\u89c6\u9891\u4e09\u5143\u7ec4\u548c\u4ee3\u7801\u53ca\u89c6\u9891\u8bc4\u4f30\u7684QA\u5bf9\u3002\n3. \u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\uff0c\u4f7f\u7528\u8054\u5408\u4ee3\u7801-\u89c6\u89c9\u5956\u52b1\u8fdb\u884c\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u6784\u5efa\u4e13\u5bb6MLLM Qwen2.5-VL-DCG-3B\u3002", "result": "\u672c\u6587\u6a21\u578b\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u5e73\u5747\u6027\u80fd\u63d0\u5347\u4e868.31%\uff0c\u5e76\u4e14\u4ec5\u4f7f\u752830\u4ebf\u53c2\u6570\u5c31\u8fbe\u5230\u4e86\u4e0e\u4e13\u6709\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u8bad\u7ec3\u65b9\u6848\u7684\u6709\u6548\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709MLLM\u5728\u89c6\u89c9\u5230\u56fe\u8868\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\u3002"}}
{"id": "2510.03255", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03255", "abs": "https://arxiv.org/abs/2510.03255", "authors": ["Wen Wu", "Ziyang Zhang", "Liwei Liu", "Xuenan Xu", "Junlin Liu", "Ke Fan", "Qitan Lv", "Jimin Zhuang", "Chen Zhang", "Zheqi Yuan", "Siyuan Hou", "Tianyi Lin", "Kai Chen", "Bowen Zhou", "Chao Zhang"], "title": "SciTS: Scientific Time Series Understanding and Generation with LLMs", "comment": null, "summary": "The scientific reasoning ability of large language models (LLMs) has recently\nattracted significant attention. Time series, as a fundamental modality in\nscientific data, presents unique challenges that are often overlooked in\ncurrent multimodal LLMs, which either encode numerical sequences as text or\nconvert them into images. Such approaches may be insufficient for comprehensive\nscientific time series understanding and generation. Existing unified time\nseries models typically specialise in either forecasting or analysis, and their\neffectiveness on non-periodic, heterogeneous scientific signals remains\nunclear. To address these gaps, we introduce SciTS, a benchmark spanning 12\nscientific domains and 43 tasks, with over 50k+ instances, both univariate and\nmultivariate signals ranging from $10^0$ to $10^7$ in length and up to 10~MHz\nin frequency. We benchmark 17 models, including text-only LLMs, multimodal\nLLMs, and unified time series models, and find that general-purpose LLMs\nexhibit stronger generalisability than specialised time series models, while\nrepresenting time series as text or images limits their performance due to\nexcessively long sequences and loss of numerical precision, respectively. We\nthen introduce TimeOmni, a framework that equips LLMs with the ability to\nunderstand and generate time series while remaining compatible with\ngeneral-purpose LLM training. This work fills a gap in both dedicated\nbenchmarks and modelling frameworks for scientific time series, paving the way\nfor LLMs to understand and generate complex temporal scientific data.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684\u79d1\u5b66\u65f6\u95f4\u5e8f\u5217\u57fa\u51c6\u6d4b\u8bd5 SciTS \u548c\u4e00\u4e2a\u540d\u4e3a TimeOmni \u7684\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u7406\u89e3\u548c\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001 LLM \u5728\u5904\u7406\u79d1\u5b66\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u5e76\u4e14\u73b0\u6709\u7684\u7edf\u4e00\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u5728\u975e\u5468\u671f\u6027\u3001\u5f02\u6784\u79d1\u5b66\u4fe1\u53f7\u4e0a\u7684\u6709\u6548\u6027\u4e0d\u660e\u786e\u3002", "method": "\u8bba\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b 12 \u4e2a\u79d1\u5b66\u9886\u57df\u548c 43 \u4e2a\u4efb\u52a1\u7684 SciTS \u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u51fa\u4e86 TimeOmni \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f LLM \u80fd\u591f\u7406\u89e3\u548c\u751f\u6210\u65f6\u95f4\u5e8f\u5217\uff0c\u540c\u65f6\u4e0e\u901a\u7528 LLM \u8bad\u7ec3\u4fdd\u6301\u517c\u5bb9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u7528 LLM \u6bd4\u4e13\u95e8\u7684\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u5c06\u65f6\u95f4\u5e8f\u5217\u8868\u793a\u4e3a\u6587\u672c\u6216\u56fe\u50cf\u4f1a\u9650\u5236\u5176\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u586b\u8865\u4e86\u79d1\u5b66\u65f6\u95f4\u5e8f\u5217\u4e13\u7528\u57fa\u51c6\u6d4b\u8bd5\u548c\u5efa\u6a21\u6846\u67b6\u7684\u7a7a\u767d\uff0c\u4e3a LLM \u7406\u89e3\u548c\u751f\u6210\u590d\u6742\u7684\u65f6\u95f4\u79d1\u5b66\u6570\u636e\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.03680", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03680", "abs": "https://arxiv.org/abs/2510.03680", "authors": ["Bumjun Kim", "Dongjae Jeon", "Dueun Kim", "Wonje Jeung", "Albert No"], "title": "Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs", "comment": "25 pages. Project page available\n  at~\\url{https://ai-isl.github.io/rainbow-padding}", "summary": "Diffusion large language models (dLLMs) have emerged as a promising\nalternative to autoregressive models, offering flexible generation orders and\nstrong performance on complex reasoning tasks. However, instruction-tuned dLLMs\nexhibit a critical vulnerability we term \\texttt{<eos>} overflow: as allocated\nsequence length increases, responses paradoxically become shorter, collapsing\ninto early termination or degenerating into streams of \\texttt{<eos>} tokens.\nAlthough noticed in practice, this issue has not been systematically analyzed.\nWe trace its root cause to the dual role of \\texttt{<eos>} as both termination\nand padding, which concentrates probability mass on \\texttt{<eos>} at later\npositions and propagates backward to trigger early termination. To address\nthis, we introduce Rainbow Padding, a simple remedy that replaces repeated\n\\texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens,\ndistributing probability mass and breaking \\texttt{<eos>} dominance.\nExperiments show that Rainbow Padding substantially improves length robustness\nand output quality, with as few as seven padding tokens sufficient to prevent\nearly termination. Moreover, the method integrates efficiently into existing\ninstruction-tuned models: LoRA fine-tuning for a single epoch on minimal data\nyields significant improvements, making this solution highly practical. The\ncode is publicly available at https://github.com/quasar529/rainbow-padding.", "AI": {"tldr": "instruction-tuned dLLMs have a vulnerability that responses become shorter as sequence length increases. The reason is that the dual role of <eos> as both termination and padding. To address this, Rainbow Padding is introduced.", "motivation": "instruction-tuned dLLMs exhibit a critical vulnerability we term \ttexttt{<eos>} overflow: as allocated sequence length increases, responses paradoxically become shorter, collapsing into early termination or degenerating into streams of \ttexttt{<eos>} tokens.Although noticed in practice, this issue has not been systematically analyzed.", "method": "introduce Rainbow Padding, a simple remedy that replaces repeated \ttexttt{<eos>} placeholders with a repeating cycle of distinct padding tokens, distributing probability mass and breaking \ttexttt{<eos>} dominance.", "result": "Rainbow Padding substantially improves length robustness and output quality, with as few as seven padding tokens sufficient to prevent early termination.", "conclusion": "LoRA fine-tuning for a single epoch on minimal data yields significant improvements, making this solution highly practical."}}
{"id": "2510.04506", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.04506", "abs": "https://arxiv.org/abs/2510.04506", "authors": ["Jiashuo Sun", "Shixuan Liu", "Zhaochen Su", "Xianrui Zhong", "Pengcheng Jiang", "Bowen Jin", "Peiran Li", "Weijia Shi", "Jiawei Han"], "title": "GRACE: Generative Representation Learning via Contrastive Policy Optimization", "comment": "23 pages, 7 figures, 7 tables", "summary": "Prevailing methods for training Large Language Models (LLMs) as text encoders\nrely on contrastive losses that treat the model as a black box function,\ndiscarding its generative and reasoning capabilities in favor of static\nembeddings. We introduce GRACE (Generative Representation Learning via\nContrastive Policy Optimization), a novel framework that reimagines contrastive\nsignals not as losses to be minimized, but as rewards that guide a generative\npolicy. In GRACE, the LLM acts as a policy that produces explicit,\nhuman-interpretable rationales--structured natural language explanations of its\nsemantic understanding. These rationales are then encoded into high-quality\nembeddings via mean pooling. Using policy gradient optimization, we train the\nmodel with a multi-component reward function that maximizes similarity between\nquery positive pairs and minimizes similarity with negatives. This transforms\nthe LLM from an opaque encoder into an interpretable agent whose reasoning\nprocess is transparent and inspectable. On MTEB benchmark, GRACE yields broad\ncross category gains: averaged over four backbones, the supervised setting\nimproves overall score by 11.5% over base models, and the unsupervised variant\nadds 6.9%, while preserving general capabilities. This work treats contrastive\nobjectives as rewards over rationales, unifying representation learning with\ngeneration to produce stronger embeddings and transparent rationales. The\nmodel, data and code are available at https://github.com/GasolSun36/GRACE.", "AI": {"tldr": "GRACE\u6846\u67b6\u901a\u8fc7\u5c06\u5bf9\u6bd4\u4fe1\u53f7\u89c6\u4e3a\u5956\u52b1\u6765\u6307\u5bfc\u751f\u6210\u7b56\u7565\uff0c\u4ece\u800c\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u6587\u672c\u7f16\u7801\u5668\uff0c\u8fdb\u800c\u4ea7\u751f\u66f4\u5f3a\u7684\u5d4c\u5165\u548c\u900f\u660e\u7684\u7406\u7531\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5bf9\u6bd4\u635f\u5931\uff0c\u5c06\u6a21\u578b\u89c6\u4e3a\u9ed1\u76d2\u51fd\u6570\uff0c\u5ffd\u7565\u4e86\u5176\u751f\u6210\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "GRACE\u6846\u67b6\u5c06LLM\u89c6\u4e3a\u4e00\u4e2a\u7b56\u7565\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u7406\u7531\uff0c\u5e76\u901a\u8fc7\u7b56\u7565\u68af\u5ea6\u4f18\u5316\u8bad\u7ec3\u6a21\u578b\uff0c\u4f7f\u7528\u591a\u7ec4\u4ef6\u5956\u52b1\u51fd\u6570\u6765\u6700\u5927\u5316\u67e5\u8be2\u6b63\u5bf9\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u5e76\u6700\u5c0f\u5316\u4e0e\u8d1f\u5bf9\u7684\u76f8\u4f3c\u6027\u3002", "result": "\u5728MTEB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGRACE\u5728\u591a\u4e2a\u7c7b\u522b\u4e0a\u53d6\u5f97\u4e86\u5e7f\u6cdb\u7684\u6536\u76ca\uff1a\u5728\u56db\u4e2a\u4e3b\u5e72\u7f51\u7edc\u4e0a\u5e73\u5747\uff0c\u76d1\u7763\u8bbe\u7f6e\u6bd4\u57fa\u672c\u6a21\u578b\u63d0\u9ad8\u4e8611.5%\uff0c\u65e0\u76d1\u7763\u53d8\u4f53\u63d0\u9ad8\u4e866.9%\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u4e00\u822c\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c06\u5bf9\u6bd4\u76ee\u6807\u89c6\u4e3a\u7406\u7531\u7684\u5956\u52b1\uff0c\u7edf\u4e00\u4e86\u8868\u793a\u5b66\u4e60\u4e0e\u751f\u6210\uff0c\u4ece\u800c\u4ea7\u751f\u66f4\u5f3a\u7684\u5d4c\u5165\u548c\u900f\u660e\u7684\u7406\u7531\u3002"}}
{"id": "2510.03536", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03536", "abs": "https://arxiv.org/abs/2510.03536", "authors": ["Zhaohan Meng", "Zaiqiao Meng", "Siwei Liu", "Iadh Ounis"], "title": "TriMediQ: A Triplet-Structured Approach for Interactive Medical Question Answering", "comment": "Preprint", "summary": "Large Language Models (LLMs) perform strongly in static and single-turn\nmedical Question Answer (QA) benchmarks, yet such settings diverge from the\niterative information gathering process required in practical clinical\nconsultations. The MEDIQ framework addresses this mismatch by recasting the\ndiagnosis as an interactive dialogue between a patient and an expert system,\nbut the reliability of LLMs drops dramatically when forced to reason with\ndialogue logs, where clinical facts appear in sentences without clear links. To\nbridge this gap, we introduce TriMediQ, a triplet-structured approach that\nsummarises patient responses into triplets and integrates them into a Knowledge\nGraph (KG), enabling multi-hop reasoning. We introduce a frozen triplet\ngenerator that extracts clinically relevant triplets, using prompts designed to\nensure factual consistency. In parallel, a trainable projection module,\ncomprising a graph encoder and a projector, captures relational information\nfrom the KG to enhance expert reasoning. TriMediQ operates in two steps: (i)\nthe projection module fine-tuning with all LLM weights frozen; and (ii) using\nthe fine-tuned module to guide multi-hop reasoning during inference. We\nevaluate TriMediQ on two interactive QA benchmarks, showing that it achieves up\nto 10.4\\% improvement in accuracy over five baselines on the iMedQA dataset.\nThese results demonstrate that converting patient responses into structured\ntriplet-based graphs enables more accurate clinical reasoning in multi-turn\nsettings, providing a solution for the deployment of LLM-based medical\nassistants.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTriMediQ\u7684\u4e09\u5143\u7ec4\u7ed3\u6784\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u5728\u4ea4\u4e92\u5f0f\u533b\u7597\u95ee\u7b54\u4e2d\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9759\u6001\u548c\u5355\u8f6e\u533b\u7597\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4e0e\u5b9e\u9645\u4e34\u5e8a\u54a8\u8be2\u6240\u9700\u7684\u8fed\u4ee3\u4fe1\u606f\u6536\u96c6\u8fc7\u7a0b\u4e0d\u7b26\u3002\u5f53\u88ab\u8feb\u63a8\u7406\u5bf9\u8bdd\u65e5\u5fd7\u65f6\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u6027\u4f1a\u663e\u8457\u4e0b\u964d\u3002", "method": "TriMediQ\u5c06\u60a3\u8005\u7684\u56de\u7b54\u603b\u7ed3\u6210\u4e09\u5143\u7ec4\uff0c\u5e76\u5c06\u5b83\u4eec\u96c6\u6210\u5230\u77e5\u8bc6\u56fe(KG)\u4e2d\uff0c\u4ece\u800c\u5b9e\u73b0\u591a\u8df3\u63a8\u7406\u3002\u5b83\u5305\u542b\u4e00\u4e2a\u51bb\u7ed3\u7684\u4e09\u5143\u7ec4\u751f\u6210\u5668\uff0c\u7528\u4e8e\u63d0\u53d6\u4e34\u5e8a\u76f8\u5173\u4e09\u5143\u7ec4\uff0c\u4ee5\u53ca\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u6295\u5f71\u6a21\u5757\uff0c\u7528\u4e8e\u6355\u83b7\u6765\u81eaKG\u7684\u5173\u7cfb\u4fe1\u606f\u3002", "result": "\u5728\u4e24\u4e2a\u4ea4\u4e92\u5f0f\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTriMediQ\u7684\u51c6\u786e\u7387\u6bd4iMedQA\u6570\u636e\u96c6\u4e0a\u7684\u4e94\u4e2a\u57fa\u7ebf\u63d0\u9ad8\u4e8610.4%\u3002", "conclusion": "\u5c06\u60a3\u8005\u7684\u56de\u7b54\u8f6c\u6362\u4e3a\u57fa\u4e8e\u4e09\u5143\u7ec4\u7684\u7ed3\u6784\u5316\u56fe\uff0c\u53ef\u4ee5\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u4e34\u5e8a\u63a8\u7406\uff0c\u4e3a\u90e8\u7f72\u57fa\u4e8ellm\u7684\u533b\u7597\u52a9\u624b\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03348", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03348", "abs": "https://arxiv.org/abs/2510.03348", "authors": ["Vlardimir Yugay", "Duy-Kien Nguyen", "Theo Gevers", "Cees G. M. Snoek", "Martin R. Oswald"], "title": "Visual Odometry with Transformers", "comment": null, "summary": "Modern monocular visual odometry methods typically combine pre-trained deep\nlearning components with optimization modules, resulting in complex pipelines\nthat rely heavily on camera calibration and hyperparameter tuning, and often\nstruggle in unseen real-world scenarios. Recent large-scale 3D models trained\non massive amounts of multi-modal data have partially alleviated these\nchallenges, providing generalizable dense reconstruction and camera pose\nestimation. Still, they remain limited in handling long videos and providing\naccurate per-frame estimates, which are required for visual odometry. In this\nwork, we demonstrate that monocular visual odometry can be addressed\neffectively in an end-to-end manner, thereby eliminating the need for\nhandcrafted components such as bundle adjustment, feature matching, camera\ncalibration, or dense 3D reconstruction. We introduce VoT, short for Visual\nodometry Transformer, which processes sequences of monocular frames by\nextracting features and modeling global relationships through temporal and\nspatial attention. Unlike prior methods, VoT directly predicts camera motion\nwithout estimating dense geometry and relies solely on camera poses for\nsupervision. The framework is modular and flexible, allowing seamless\nintegration of various pre-trained encoders as feature extractors. Experimental\nresults demonstrate that VoT scales effectively with larger datasets, benefits\nsubstantially from stronger pre-trained backbones, generalizes across diverse\ncamera motions and calibration settings, and outperforms traditional methods\nwhile running more than 3 times faster. The code will be released.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVoT\u7684\u7aef\u5230\u7aef\u5355\u76ee\u89c6\u89c9\u91cc\u7a0b\u8ba1\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528Transformer\u76f4\u63a5\u9884\u6d4b\u76f8\u673a\u8fd0\u52a8\uff0c\u65e0\u9700\u624b\u5de5\u7ec4\u4ef6\u62163D\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u76f8\u673a\u6807\u5b9a\u548c\u8d85\u53c2\u6570\u8c03\u6574\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u771f\u5b9e\u573a\u666f\uff0c\u4e14\u96be\u4ee5\u5904\u7406\u957f\u89c6\u9891\u548c\u63d0\u4f9b\u7cbe\u786e\u7684\u9010\u5e27\u4f30\u8ba1\u3002", "method": "\u4f7f\u7528Visual odometry Transformer (VoT) \u5904\u7406\u5355\u76ee\u5e27\u5e8f\u5217\uff0c\u901a\u8fc7\u65f6\u7a7a\u6ce8\u610f\u529b\u63d0\u53d6\u7279\u5f81\u5e76\u5efa\u6a21\u5168\u5c40\u5173\u7cfb\uff0c\u76f4\u63a5\u9884\u6d4b\u76f8\u673a\u8fd0\u52a8\u3002", "result": "VoT\u5728\u66f4\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u53d7\u76ca\u4e8e\u66f4\u5f3a\u5927\u7684\u9884\u8bad\u7ec3\u9aa8\u5e72\u7f51\u7edc\uff0c\u53ef\u6cdb\u5316\u5230\u4e0d\u540c\u7684\u76f8\u673a\u8fd0\u52a8\u548c\u6807\u5b9a\u8bbe\u7f6e\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u4e14\u901f\u5ea6\u5feb3\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u8bc1\u660e\u4e86\u5355\u76ee\u89c6\u89c9\u91cc\u7a0b\u8ba1\u53ef\u4ee5\u901a\u8fc7\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u6709\u6548\u89e3\u51b3\uff0c\u65e0\u9700\u624b\u5de5\u7ec4\u4ef6\u3002"}}
{"id": "2510.03257", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.03257", "abs": "https://arxiv.org/abs/2510.03257", "authors": ["Zijian Zhao", "Sen Li"], "title": "Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?", "comment": null, "summary": "On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate\nreal-time challenge of bundling and matching passengers-each with distinct\norigins and destinations-to available vehicles, all while navigating\nsignificant system uncertainties. Due to the extensive observation space\narising from the large number of drivers and orders, order dispatching, though\nfundamentally a centralized task, is often addressed using Multi-Agent\nReinforcement Learning (MARL). However, independent MARL methods fail to\ncapture global information and exhibit poor cooperation among workers, while\nCentralized Training Decentralized Execution (CTDE) MARL methods suffer from\nthe curse of dimensionality. To overcome these challenges, we propose\nTriple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method\ndesigned specifically for large-scale order dispatching on ride-sharing\nplatforms. Built on a variant TD3, our approach addresses the vast action space\nthrough an action decomposition strategy that breaks down the joint action\nprobability into individual driver action probabilities. To handle the\nextensive observation space, we introduce a novel BERT-based network, where\nparameter reuse mitigates parameter growth as the number of drivers and orders\nincreases, and the attention mechanism effectively captures the complex\nrelationships among the large pool of driver and orders. We validate our method\nusing a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves\napproximately an 11.95% improvement over current state-of-the-art methods, with\na 4.26% increase in served orders and a 22.25% reduction in pickup times. Our\ncode, trained model parameters, and processed data are publicly available at\nthe repository https://github.com/RS2002/Triple-BERT .", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTriple-BERT\u7684\u96c6\u4e2d\u5f0f\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u7f51\u7ea6\u8f66\u5e73\u53f0\u4e0a\u5927\u89c4\u6a21\u8ba2\u5355\u8c03\u5ea6\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684MARL\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u5168\u5c40\u4fe1\u606f\uff0c\u5408\u4f5c\u6027\u5dee\uff0cCTDE\u65b9\u6cd5\u5b58\u5728\u7ef4\u5ea6\u707e\u96be\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86Triple-BERT\u3002", "method": "\u8be5\u65b9\u6cd5\u57fa\u4e8eTD3\u53d8\u4f53\uff0c\u901a\u8fc7\u52a8\u4f5c\u5206\u89e3\u7b56\u7565\u5904\u7406\u5de8\u5927\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8eBERT\u7684\u65b0\u578b\u7f51\u7edc\u6765\u5904\u7406\u5e7f\u6cdb\u7684\u89c2\u5bdf\u7a7a\u95f4\u3002", "result": "\u5728\u66fc\u54c8\u987f\u7684\u771f\u5b9e\u7f51\u7ea6\u8f66\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0cTriple-BERT\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u7ea611.95%\uff0c\u670d\u52a1\u8ba2\u5355\u589e\u52a0\u4e864.26%\uff0c\u63a5\u5ba2\u65f6\u95f4\u51cf\u5c11\u4e8622.25%\u3002", "conclusion": "Triple-BERT\u5728\u5927\u89c4\u6a21\u8ba2\u5355\u8c03\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.03696", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03696", "abs": "https://arxiv.org/abs/2510.03696", "authors": ["Deepak Babu Piskala", "Sharlene Chen", "Udita Patel", "Parul Kalra", "Rafael Castrillo"], "title": "Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models", "comment": null, "summary": "Evaluating the quality of multi-turn chatbot interactions remains\nchallenging, as most existing methods assess interactions at the turn level\nwithout addressing whether a user's overarching goal was fulfilled. A ``goal''\nhere refers to an information need or task, such as asking for policy\ninformation or applying for leave. We propose a comprehensive framework for\ngoal-oriented evaluation of multi-agent systems (MAS), introducing the\n\\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,\nand a \\textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for\nfailure in multi-agent chatbots. Our method segments conversations by user\ngoals and evaluates success using all relevant turns. We present a model-based\nevaluation system combining teacher LLMs, where domain experts define goals,\nset quality standards serving as a guidance for the LLMs. The LLMs use\n``thinking tokens'' to produce interpretable rationales, enabling\n\\textit{explainable}, \\textit{data-efficient} evaluations. In an enterprise\nsetting, we apply our framework to evaluate AIDA, a zero-to-one employee\nconversational agent system built as a ground-up multi-agent conversational\nagent, and observe GSR improvement from 63\\% to 79\\% over six months since its\ninception. Our framework is generic and offers actionable insights through a\ndetailed defect taxonomy based on analysis of failure points in multi-agent\nchatbots, diagnosing overall success, identifying key failure modes, and\ninforming system improvements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u8f6e\u804a\u5929\u673a\u5668\u4eba\u4ea4\u4e92\u7684\u7efc\u5408\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4fa7\u91cd\u4e8e\u7528\u6237\u76ee\u6807\u7684\u5b9e\u73b0\u60c5\u51b5\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5355\u8f6e\u5bf9\u8bdd\u7684\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u5728turn\u7684\u5c42\u9762\u8bc4\u4f30\u4ea4\u4e92\uff0c\u6ca1\u6709\u89e3\u51b3\u7528\u6237\u76ee\u6807\u662f\u5426\u5b9e\u73b0\u7684\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u4ee5\u76ee\u6807\u4e3a\u5bfc\u5411\u7684\u8bc4\u4f30\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Goal Success Rate (GSR)\u6765\u8861\u91cf\u76ee\u6807\u5b8c\u6210\u7684\u767e\u5206\u6bd4\uff0c\u5e76\u63d0\u51fa\u4e86Root Cause of Failure (RCOF) \u5206\u7c7b\u6cd5\u6765\u8bc6\u522b\u591a\u667a\u80fd\u4f53\u804a\u5929\u673a\u5668\u4eba\u5931\u8d25\u7684\u539f\u56e0\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7528\u6237\u76ee\u6807\u5bf9\u5bf9\u8bdd\u8fdb\u884c\u5206\u5272\uff0c\u5e76\u4f7f\u7528\u6240\u6709\u76f8\u5173\u7684turns\u6765\u8bc4\u4f30\u6210\u529f\u4e0e\u5426\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6a21\u578b\u7684\u8bc4\u4f30\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86teacher LLM\uff0c\u9886\u57df\u4e13\u5bb6\u5b9a\u4e49\u76ee\u6807\uff0c\u5e76\u8bbe\u5b9a\u8d28\u91cf\u6807\u51c6\u6765\u6307\u5bfcLLM\u3002LLM\u4f7f\u7528\u201c\u601d\u8003token\u201d\u6765\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u57fa\u672c\u539f\u7406\u3002", "result": "\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\uff0c\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8e\u8bc4\u4f30AIDA\uff0c\u4e00\u4e2a\u4ece\u96f6\u5f00\u59cb\u6784\u5efa\u7684\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u5f0f\u4ee3\u7406\u7cfb\u7edf\uff0c\u5e76\u5728\u5176\u6210\u7acb\u540e\u7684\u516d\u4e2a\u6708\u5185\u89c2\u5bdf\u5230GSR\u4ece63\uff05\u63d0\u9ad8\u523079\uff05\u3002", "conclusion": "\u8be5\u6846\u67b6\u662f\u901a\u7528\u7684\uff0c\u901a\u8fc7\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u804a\u5929\u673a\u5668\u4eba\u4e2d\u6545\u969c\u70b9\u7684\u5206\u6790\uff0c\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u8bca\u65ad\u6574\u4f53\u6210\u529f\u7387\uff0c\u8bc6\u522b\u5173\u952e\u6545\u969c\u6a21\u5f0f\uff0c\u5e76\u4e3a\u7cfb\u7edf\u6539\u8fdb\u63d0\u4f9b\u4fe1\u606f\u3002"}}
{"id": "2510.04551", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.04551", "abs": "https://arxiv.org/abs/2510.04551", "authors": ["Mario Almagro", "Diego Ortego", "David Jimenez"], "title": "Fine-grained auxiliary learning for real-world product recommendation", "comment": "SEPLN 2025", "summary": "Product recommendation is the task of recovering the closest items to a given\nquery within a large product corpora. Generally, one can determine if\ntop-ranked products are related to the query by applying a similarity\nthreshold; exceeding it deems the product relevant, otherwise manual revision\nis required. Despite being a well-known problem, the integration of these\nmodels in real-world systems is often overlooked. In particular, production\nsystems have strong coverage requirements, i.e., a high proportion of\nrecommendations must be automated. In this paper we propose ALC , an Auxiliary\nLearning strategy that boosts Coverage through learning fine-grained\nembeddings. Concretely, we introduce two training objectives that leverage the\nhardest negatives in the batch to build discriminative training signals between\npositives and negatives. We validate ALC using three extreme multi-label\nclassification approaches in two product recommendation datasets;\nLF-AmazonTitles-131K and Tech and Durables (proprietary), demonstrating\nstate-of-the-art coverage rates when combined with a recent\nthreshold-consistent margin loss.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f85\u52a9\u5b66\u4e60\u7b56\u7565ALC\uff0c\u901a\u8fc7\u5b66\u4e60\u7ec6\u7c92\u5ea6\u5d4c\u5165\u6765\u63d0\u9ad8\u8986\u76d6\u7387\u3002", "motivation": "\u73b0\u5b9e\u7cfb\u7edf\u4e2d\u4ea7\u54c1\u63a8\u8350\u6a21\u578b\u7684\u8986\u76d6\u7387\u4e0d\u8db3\uff0c\u5927\u91cf\u63a8\u8350\u9700\u8981\u4eba\u5de5\u5e72\u9884\u3002", "method": "\u5f15\u5165\u4e24\u4e2a\u8bad\u7ec3\u76ee\u6807\uff0c\u5229\u7528batch\u4e2d\u6700\u96be\u7684\u8d1f\u6837\u672c\uff0c\u5728\u6b63\u6837\u672c\u548c\u8d1f\u6837\u672c\u4e4b\u95f4\u5efa\u7acb\u533a\u5206\u6027\u8bad\u7ec3\u4fe1\u53f7\u3002", "result": "\u5728\u4e24\u4e2a\u4ea7\u54c1\u63a8\u8350\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86ALC\uff0c\u7ed3\u5408\u6700\u8fd1\u7684\u9608\u503c\u4e00\u81f4\u6027margin loss\uff0c\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u8986\u76d6\u7387\u3002", "conclusion": "ALC \u662f\u4e00\u79cd\u6709\u6548\u7684\u63d0\u9ad8\u4ea7\u54c1\u63a8\u8350\u6a21\u578b\u8986\u76d6\u7387\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.03541", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03541", "abs": "https://arxiv.org/abs/2510.03541", "authors": ["Andrew Halterman", "Katherine A. Keith"], "title": "What is a protest anyway? Codebook conceptualization is still a first-order concern in LLM-era classification", "comment": null, "summary": "Generative large language models (LLMs) are now used extensively for text\nclassification in computational social science (CSS). In this work, focus on\nthe steps before and after LLM prompting -- conceptualization of concepts to be\nclassified and using LLM predictions in downstream statistical inference --\nwhich we argue have been overlooked in much of LLM-era CSS. We claim LLMs can\ntempt analysts to skip the conceptualization step, creating conceptualization\nerrors that bias downstream estimates. Using simulations, we show that this\nconceptualization-induced bias cannot be corrected for solely by increasing LLM\naccuracy or post-hoc bias correction methods. We conclude by reminding CSS\nanalysts that conceptualization is still a first-order concern in the LLM-era\nand provide concrete advice on how to pursue low-cost, unbiased, low-variance\ndownstream estimates.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5173\u6ce8\u7684\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\uff08CSS\uff09\u4e2d\u7528\u4e8e\u6587\u672c\u5206\u7c7b\u65f6\uff0c\u63d0\u793a\u524d\u540e\u88ab\u5ffd\u89c6\u7684\u6b65\u9aa4\uff1a\u6982\u5ff5\u5316\u548c\u4e0b\u6e38\u7edf\u8ba1\u63a8\u65ad\u3002\u4f5c\u8005\u8ba4\u4e3aLLM\u53ef\u80fd\u4f1a\u5bfc\u81f4\u5206\u6790\u5e08\u8df3\u8fc7\u6982\u5ff5\u5316\u6b65\u9aa4\uff0c\u4ece\u800c\u4ea7\u751f\u6982\u5ff5\u5316\u8bef\u5dee\uff0c\u8fdb\u800c\u5f71\u54cd\u4e0b\u6e38\u4f30\u8ba1\u3002", "motivation": "\u5f3a\u8c03\u5728\u4f7f\u7528LLM\u8fdb\u884c\u6587\u672c\u5206\u7c7b\u65f6\uff0c\u6982\u5ff5\u5316\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u7814\u7a76\u5bf9\u8fd9\u4e00\u73af\u8282\u7684\u5ffd\u89c6\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u6765\u9a8c\u8bc1\u6982\u5ff5\u5316\u504f\u5dee\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u4ec5\u9760\u63d0\u9ad8LLM\u7684\u51c6\u786e\u6027\u6216\u4e8b\u540e\u504f\u5dee\u6821\u6b63\u65b9\u6cd5\u65e0\u6cd5\u7ea0\u6b63\u6982\u5ff5\u5316\u5f15\u5165\u7684\u504f\u5dee\u3002", "conclusion": "\u63d0\u9192CSS\u5206\u6790\u5e08\u6982\u5ff5\u5316\u4ecd\u7136\u662f\u9996\u8981\u5173\u6ce8\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u5982\u4f55\u83b7\u5f97\u4f4e\u6210\u672c\u3001\u65e0\u504f\u3001\u4f4e\u65b9\u5dee\u7684\u4e0b\u6e38\u4f30\u8ba1\u7684\u5177\u4f53\u5efa\u8bae\u3002"}}
{"id": "2510.03352", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03352", "abs": "https://arxiv.org/abs/2510.03352", "authors": ["Mahdi Farahbakhsh", "Vishnu Teja Kunde", "Dileep Kalathil", "Krishna Narayanan", "Jean-Francois Chamberland"], "title": "Inference-Time Search using Side Information for Diffusion-based Image Reconstruction", "comment": null, "summary": "Diffusion models have emerged as powerful priors for solving inverse\nproblems. However, existing approaches typically overlook side information that\ncould significantly improve reconstruction quality, especially in severely\nill-posed settings. In this work, we propose a novel inference-time search\nalgorithm that guides the sampling process using the side information in a\nmanner that balances exploration and exploitation. This enables more accurate\nand reliable reconstructions, providing an alternative to the gradient-based\nguidance that is prone to reward-hacking artifacts. Our approach can be\nseamlessly integrated into a wide range of existing diffusion-based image\nreconstruction pipelines. Through extensive experiments on a number of inverse\nproblems, such as box inpainting, super-resolution, and various deblurring\ntasks including motion, Gaussian, nonlinear, and blind deblurring, we show that\nour approach consistently improves the qualitative and quantitative performance\nof diffusion-based image reconstruction algorithms. We also show the superior\nperformance of our approach with respect to other baselines, including reward\ngradient-based guidance algorithms. The code is available at\n\\href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this\nrepository}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u65f6\u641c\u7d22\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5229\u7528\u8f85\u52a9\u4fe1\u606f\u6765\u6307\u5bfc\u91c7\u6837\u8fc7\u7a0b\uff0c\u4ee5\u5e73\u8861\u63a2\u7d22\u548c\u5229\u7528\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u51c6\u786e\u548c\u53ef\u9760\u7684\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u4e86\u53ef\u80fd\u663e\u8457\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u7684\u8f85\u52a9\u4fe1\u606f\uff0c\u5c24\u5176\u662f\u5728\u4e25\u91cd\u4e0d\u9002\u5b9a\u7684\u8bbe\u7f6e\u4e2d\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u65f6\u641c\u7d22\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4f7f\u7528\u8f85\u52a9\u4fe1\u606f\u6765\u6307\u5bfc\u91c7\u6837\u8fc7\u7a0b\u3002", "result": "\u5728\u5927\u91cf\u9006\u95ee\u9898\uff08\u5982\u76d2\u72b6\u4fee\u590d\u3001\u8d85\u5206\u8fa8\u7387\u548c\u5404\u79cd\u53bb\u6a21\u7cca\u4efb\u52a1\uff0c\u5305\u62ec\u8fd0\u52a8\u3001\u9ad8\u65af\u3001\u975e\u7ebf\u6027\u53bb\u6a21\u7cca\u548c\u76f2\u53bb\u6a21\u7cca\uff09\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u63d0\u9ad8\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u91cd\u5efa\u7b97\u6cd5\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\uff0c\u5305\u62ec\u57fa\u4e8e\u5956\u52b1\u68af\u5ea6\u7684\u5f15\u5bfc\u7b97\u6cd5\u3002"}}
{"id": "2510.03258", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03258", "abs": "https://arxiv.org/abs/2510.03258", "authors": ["Chang'an Yi", "Xiaohui Deng", "Shuaicheng Niu", "Yan Zhou"], "title": "POEM: Explore Unexplored Reliable Samples to Enhance Test-Time Adaptation", "comment": "11pages,6 figures", "summary": "Test-time adaptation (TTA) aims to transfer knowledge from a source model to\nunknown test data with potential distribution shifts in an online manner. Many\nexisting TTA methods rely on entropy as a confidence metric to optimize the\nmodel. However, these approaches are sensitive to the predefined entropy\nthreshold, influencing which samples are chosen for model adaptation.\nConsequently, potentially reliable target samples are often overlooked and\nunderutilized. For instance, a sample's entropy might slightly exceed the\nthreshold initially, but fall below it after the model is updated. Such samples\ncan provide stable supervised information and offer a normal range of gradients\nto guide model adaptation. In this paper, we propose a general approach,\n\\underline{POEM}, to promote TTA via ex\\underline{\\textbf{p}}loring the\npreviously unexpl\\underline{\\textbf{o}}red reliabl\\underline{\\textbf{e}}\nsa\\underline{\\textbf{m}}ples. Additionally, we introduce an extra Adapt Branch\nnetwork to strike a balance between extracting domain-agnostic representations\nand achieving high performance on target data. Comprehensive experiments across\nmultiple architectures demonstrate that POEM consistently outperforms existing\nTTA methods in both challenging scenarios and real-world domain shifts, while\nremaining computationally efficient. The effectiveness of POEM is evaluated\nthrough extensive analyses and thorough ablation studies. Moreover, the core\nidea behind POEM can be employed as an augmentation strategy to boost the\nperformance of existing TTA approaches. The source code is publicly available\nat \\emph{https://github.com/ycarobot/POEM}", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPOEM\u7684\u901a\u7528\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94(TTA)\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u63a2\u7d22\u5148\u524d\u672a\u88ab\u5229\u7528\u7684\u53ef\u9760\u6837\u672c\u6765\u63d0\u5347TTA\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u989d\u5916\u7684Adapt Branch\u7f51\u7edc\u4ee5\u5e73\u8861\u9886\u57df\u65e0\u5173\u8868\u793a\u7684\u63d0\u53d6\u548c\u76ee\u6807\u6570\u636e\u4e0a\u7684\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684TTA\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u71b5\u4f5c\u4e3a\u7f6e\u4fe1\u5ea6\u6307\u6807\u6765\u4f18\u5316\u6a21\u578b\uff0c\u4f46\u5bf9\u9884\u5b9a\u4e49\u7684\u71b5\u9608\u503c\u654f\u611f\uff0c\u5bfc\u81f4\u4e00\u4e9b\u6f5c\u5728\u53ef\u9760\u7684\u76ee\u6807\u6837\u672c\u88ab\u5ffd\u7565\u548c\u672a\u5145\u5206\u5229\u7528\u3002", "method": "\u63d0\u51faPOEM\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a2\u7d22\u5148\u524d\u672a\u88ab\u5229\u7528\u7684\u53ef\u9760\u6837\u672c\u6765\u63d0\u5347TTA\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u989d\u5916\u7684Adapt Branch\u7f51\u7edc\u6765\u5e73\u8861\u9886\u57df\u65e0\u5173\u8868\u793a\u7684\u63d0\u53d6\u548c\u76ee\u6807\u6570\u636e\u4e0a\u7684\u9ad8\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u67b6\u6784\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cPOEM\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u548c\u771f\u5b9e\u4e16\u754c\u7684\u9886\u57df\u8f6c\u79fb\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684TTA\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "POEM\u65b9\u6cd5\u662f\u6709\u6548\u7684\uff0c\u5e76\u4e14\u5176\u6838\u5fc3\u601d\u60f3\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u589e\u5f3a\u7b56\u7565\u6765\u63d0\u9ad8\u73b0\u6709TTA\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2510.03700", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03700", "abs": "https://arxiv.org/abs/2510.03700", "authors": ["Seungseop Lim", "Gibaeg Kim", "Hyunkyung Lee", "Wooseok Han", "Jean Seo", "Jaehyo Yoo", "Eunho Yang"], "title": "H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis", "comment": "GenAI4Health @NeurIPS 2025", "summary": "An accurate differential diagnosis (DDx) is essential for patient care,\nshaping therapeutic decisions and influencing outcomes. Recently, Large\nLanguage Models (LLMs) have emerged as promising tools to support this process\nby generating a DDx list from patient narratives. However, existing evaluations\nof LLMs in this domain primarily rely on flat metrics, such as Top-k accuracy,\nwhich fail to distinguish between clinically relevant near-misses and\ndiagnostically distant errors. To mitigate this limitation, we introduce H-DDx,\na hierarchical evaluation framework that better reflects clinical relevance.\nH-DDx leverages a retrieval and reranking pipeline to map free-text diagnoses\nto ICD-10 codes and applies a hierarchical metric that credits predictions\nclosely related to the ground-truth diagnosis. In benchmarking 22 leading\nmodels, we show that conventional flat metrics underestimate performance by\noverlooking clinically meaningful outputs, with our results highlighting the\nstrengths of domain-specialized open-source models. Furthermore, our framework\nenhances interpretability by revealing hierarchical error patterns,\ndemonstrating that LLMs often correctly identify the broader clinical context\neven when the precise diagnosis is missed.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6846\u67b6H-DDx\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9274\u522b\u8bca\u65ad\u4e2d\u7684\u6027\u80fd\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5c42\u7ea7\u6307\u6807\u66f4\u597d\u5730\u53cd\u6620\u4e34\u5e8a\u76f8\u5173\u6027\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e Top-k \u51c6\u786e\u7387\u7b49\u6241\u5e73\u6307\u6807\uff0c\u65e0\u6cd5\u533a\u5206\u4e34\u5e8a\u76f8\u5173\u7684\u8fd1\u4f3c\u9519\u8bef\u548c\u8bca\u65ad\u4e0a\u7684\u663e\u8457\u9519\u8bef\u3002", "method": "\u8be5\u6846\u67b6\u5229\u7528\u68c0\u7d22\u548c\u91cd\u6392\u5e8f\u6d41\u7a0b\u5c06\u81ea\u7531\u6587\u672c\u8bca\u65ad\u6620\u5c04\u5230 ICD-10 \u4ee3\u7801\uff0c\u5e76\u5e94\u7528\u5c42\u7ea7\u6307\u6807\u6765\u8bc4\u4f30\u9884\u6d4b\u7ed3\u679c\u4e0e\u771f\u5b9e\u8bca\u65ad\u7684\u63a5\u8fd1\u7a0b\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f20\u7edf\u7684\u6241\u5e73\u6307\u6807\u4f4e\u4f30\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u800c\u9886\u57df\u4e13\u4e1a\u5316\u7684\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u51fa\u4f18\u52bf\u3002\u8be5\u6846\u67b6\u8fd8\u53ef\u4ee5\u63ed\u793a\u5c42\u7ea7\u9519\u8bef\u6a21\u5f0f\u3002", "conclusion": "H-DDx \u6846\u67b6\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9274\u522b\u8bca\u65ad\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2510.04631", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.04631", "abs": "https://arxiv.org/abs/2510.04631", "authors": ["Anastasia Zhukova", "Jonas L\u00fchrs", "Christian E. Matt", "Bela Gipp"], "title": "Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language Models in the Process Industry", "comment": "accepted to EMNLP 2025 (industry track)", "summary": "Recent trends in NLP utilize knowledge graphs (KGs) to enhance pretrained\nlanguage models by incorporating additional knowledge from the graph structures\nto learn domain-specific terminology or relationships between documents that\nmight otherwise be overlooked. This paper explores how SciNCL, a graph-aware\nneighborhood contrastive learning methodology originally designed for\nscientific publications, can be applied to the process industry domain, where\ntext logs contain crucial information about daily operations and are often\nstructured as sparse KGs. Our experiments demonstrate that language models\nfine-tuned with triplets derived from GE outperform a state-of-the-art\nmE5-large text encoder by 9.8-14.3% (5.4-8.0p) on the proprietary process\nindustry text embedding benchmark (PITEB) while being 3-5 times smaller in\nsize.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u5982\u4f55\u5c06 SciNCL \u5e94\u7528\u4e8e\u6d41\u7a0b\u5de5\u4e1a\u9886\u57df\uff0c\u5229\u7528\u6587\u672c\u65e5\u5fd7\u4e2d\u8574\u542b\u7684\u5173\u952e\u4fe1\u606f\uff0c\u8fd9\u4e9b\u4fe1\u606f\u901a\u5e38\u4ee5\u7a00\u758f\u77e5\u8bc6\u56fe\u8c31\u7684\u5f62\u5f0f\u5b58\u5728\u3002", "motivation": "\u5229\u7528\u77e5\u8bc6\u56fe\u8c31 (KGs) \u589e\u5f3a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u4ece\u800c\u5b66\u4e60\u9886\u57df\u7279\u5b9a\u7684\u672f\u8bed\u6216\u6587\u6863\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u5e94\u7528 SciNCL\uff0c\u4e00\u79cd\u56fe\u611f\u77e5\u7684\u90bb\u57df\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5728\u4e13\u6709\u7684\u6d41\u7a0b\u5de5\u4e1a\u6587\u672c\u5d4c\u5165\u57fa\u51c6 (PITEB) \u4e0a\uff0c\u4f7f\u7528\u4ece GE \u5bfc\u51fa\u7684 triplets \u8fdb\u884c\u5fae\u8c03\u7684\u8bed\u8a00\u6a21\u578b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684 mE5-large \u6587\u672c\u7f16\u7801\u5668 9.8-14.3% (5.4-8.0p)\uff0c\u5e76\u4e14\u6a21\u578b\u5c3a\u5bf8\u5c0f 3-5 \u500d\u3002", "conclusion": "SciNCL \u53ef\u4ee5\u6709\u6548\u5730\u5e94\u7528\u4e8e\u6d41\u7a0b\u5de5\u4e1a\u9886\u57df\uff0c\u63d0\u9ad8\u6587\u672c\u5d4c\u5165\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c0f\u6a21\u578b\u5c3a\u5bf8\u3002"}}
{"id": "2510.03553", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03553", "abs": "https://arxiv.org/abs/2510.03553", "authors": ["Hasibur Rahman", "Hanan Salam"], "title": "CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making", "comment": null, "summary": "Although large language models (LLMs) are increasingly implicated in\ninterpersonal and societal decision-making, their ability to navigate explicit\nconflicts between legitimately different cultural value systems remains largely\nunexamined. Existing benchmarks predominantly target cultural knowledge\n(CulturalBench), value prediction (WorldValuesBench), or single-axis bias\ndiagnostics (CDEval); none evaluate how LLMs adjudicate when multiple\nculturally grounded values directly clash. We address this gap with CCD-Bench,\na benchmark that assesses LLM decision-making under cross-cultural value\nconflict. CCD-Bench comprises 2,182 open-ended dilemmas spanning seven domains,\neach paired with ten anonymized response options corresponding to the ten GLOBE\ncultural clusters. These dilemmas are presented using a stratified Latin square\nto mitigate ordering effects. We evaluate 17 non-reasoning LLMs. Models\ndisproportionately prefer Nordic Europe (mean 20.2 percent) and Germanic Europe\n(12.4 percent), while options for Eastern Europe and the Middle East and North\nAfrica are underrepresented (5.6 to 5.8 percent). Although 87.9 percent of\nrationales reference multiple GLOBE dimensions, this pluralism is superficial:\nmodels recombine Future Orientation and Performance Orientation, and rarely\nground choices in Assertiveness or Gender Egalitarianism (both under 3\npercent). Ordering effects are negligible (Cramer's V less than 0.10), and\nsymmetrized KL divergence shows clustering by developer lineage rather than\ngeography. These patterns suggest that current alignment pipelines promote a\nconsensus-oriented worldview that underserves scenarios demanding power\nnegotiation, rights-based reasoning, or gender-aware analysis. CCD-Bench shifts\nevaluation beyond isolated bias detection toward pluralistic decision making\nand highlights the need for alignment strategies that substantively engage\ndiverse worldviews.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCCD-Bench\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8de8\u6587\u5316\u4ef7\u503c\u51b2\u7a81\u4e0b\u7684\u51b3\u7b56\u80fd\u529b\u3002\u7ed3\u679c\u8868\u660e\uff0cLLM\u5728\u51b3\u7b56\u65f6\u503e\u5411\u4e8e\u67d0\u4e9b\u6587\u5316\u4ef7\u503c\u89c2\uff0c\u800c\u5ffd\u89c6\u5176\u4ed6\u6587\u5316\u4ef7\u503c\u89c2\uff0c\u5e76\u4e14\u7f3a\u4e4f\u5bf9\u6743\u529b\u534f\u5546\u3001\u57fa\u4e8e\u6743\u5229\u7684\u63a8\u7406\u6216\u6027\u522b\u610f\u8bc6\u5206\u6790\u7684\u8003\u8651\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u4e3b\u8981\u9488\u5bf9\u6587\u5316\u77e5\u8bc6\u3001\u4ef7\u503c\u9884\u6d4b\u6216\u5355\u8f74\u504f\u5dee\u8bca\u65ad\uff0c\u800c\u5ffd\u7565\u4e86LLM\u5728\u591a\u79cd\u6587\u5316\u4ef7\u503c\u89c2\u76f4\u63a5\u51b2\u7a81\u65f6\u7684\u5224\u65ad\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b2,182\u4e2a\u5f00\u653e\u5f0f\u56f0\u5883\u7684\u57fa\u51c6CCD-Bench\uff0c\u8fd9\u4e9b\u56f0\u5883\u6db5\u76d6\u4e03\u4e2a\u9886\u57df\uff0c\u5e76\u914d\u6709\u5bf9\u5e94\u4e8e\u5341\u4e2aGLOBE\u6587\u5316\u96c6\u7fa4\u7684\u533f\u540d\u56de\u5e94\u9009\u9879\u3002\u4f7f\u7528\u5206\u5c42\u62c9\u4e01\u65b9\u6765\u51cf\u8f7b\u6392\u5e8f\u6548\u5e94\u3002\u8bc4\u4f30\u4e8617\u4e2a\u975e\u63a8\u7406LLM\u3002", "result": "\u6a21\u578b disproportionately \u559c\u6b22 Nordic Europe \u548c Germanic Europe\uff0c\u800c\u5bf9 Eastern Europe \u548c the Middle East and North Africa \u7684\u9009\u62e9\u4e0d\u8db3\u3002\u5c3d\u7ba1 87.9% \u7684\u7406\u7531\u53c2\u8003\u4e86\u591a\u4e2a GLOBE \u7ef4\u5ea6\uff0c\u4f46\u8fd9\u79cd\u591a\u5143\u4e3b\u4e49\u662f\u80a4\u6d45\u7684\uff1a\u6a21\u578b\u91cd\u65b0\u7ec4\u5408\u4e86 Future Orientation \u548c Performance Orientation\uff0c\u5e76\u4e14\u5f88\u5c11\u4ee5 Assertiveness \u6216 Gender Egalitarianism \u4e3a\u57fa\u7840\u3002", "conclusion": "\u5f53\u524d\u7684\u5bf9\u9f50\u6d41\u7a0b\u4fc3\u8fdb\u4e86\u4e00\u79cd\u5171\u8bc6\u5bfc\u5411\u7684\u4e16\u754c\u89c2\uff0c\u4e0d\u80fd\u5f88\u597d\u5730\u670d\u52a1\u4e8e\u9700\u8981\u6743\u529b\u534f\u5546\u3001\u57fa\u4e8e\u6743\u5229\u7684\u63a8\u7406\u6216\u6027\u522b\u610f\u8bc6\u5206\u6790\u7684\u573a\u666f\u3002\u9700\u8981\u5b9e\u8d28\u6027\u5730\u53c2\u4e0e\u4e0d\u540c\u7684\u4e16\u754c\u89c2\u7684\u5bf9\u9f50\u7b56\u7565\u3002"}}
{"id": "2510.03353", "categories": ["cs.CV", "I.4.9; I.5.0; H.3.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.03353", "abs": "https://arxiv.org/abs/2510.03353", "authors": ["Larissa S. Gomes", "Gustavo P. Almeida", "Bryan U. Moreira", "Marco Quiroz", "Breno Xavier", "Lucas Soares", "Stephanie L. Bri\u00e3o", "Felipe G. Oliveira", "Paulo L. J. Drews-Jr"], "title": "Sonar Image Datasets: A Comprehensive Survey of Resources, Challenges, and Applications", "comment": "Published in the Conference on Graphics, Patterns and Images\n  (SIBGRAPI). This 4-page paper presents a timeline of publicly available\n  datasets up to the year 2025", "summary": "Sonar images are relevant for advancing underwater exploration, autonomous\nnavigation, and ecosystem monitoring. However, the progress depends on data\navailability. The scarcity of publicly available, well-annotated sonar image\ndatasets creates a significant bottleneck for the development of robust machine\nlearning models. This paper presents a comprehensive and concise review of the\ncurrent landscape of sonar image datasets, seeking not only to catalog existing\nresources but also to contextualize them, identify gaps, and provide a clear\nroadmap, serving as a base guide for researchers of any kind who wish to start\nor advance in the field of underwater acoustic data analysis. We mapped\npublicly accessible datasets across various sonar modalities, including Side\nScan Sonar (SSS), Forward-Looking Sonar (FLS), Synthetic Aperture Sonar (SAS),\nMultibeam Echo Sounder (MBES), and Dual-Frequency Identification Sonar\n(DIDSON). An analysis was conducted on applications such as classification,\ndetection, segmentation, and 3D reconstruction. This work focuses on\nstate-of-the-art advancements, incorporating newly released datasets. The\nfindings are synthesized into a master table and a chronological timeline,\noffering a clear and accessible comparison of characteristics, sizes, and\nannotation details datasets.", "AI": {"tldr": "\u672c\u6587\u5168\u9762\u56de\u987e\u4e86\u58f0\u5450\u56fe\u50cf\u6570\u636e\u96c6\u7684\u73b0\u72b6\uff0c\u65e8\u5728\u4e3a\u6c34\u58f0\u6570\u636e\u5206\u6790\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "\u6c34\u4e0b\u63a2\u7d22\u3001\u81ea\u4e3b\u5bfc\u822a\u548c\u751f\u6001\u7cfb\u7edf\u76d1\u6d4b\u4f9d\u8d56\u4e8e\u58f0\u5450\u56fe\u50cf\uff0c\u4f46\u7f3a\u4e4f\u516c\u5f00\u7684\u3001\u826f\u597d\u6ce8\u91ca\u7684\u6570\u636e\u96c6\u963b\u788d\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u672c\u6587\u5bf9\u5404\u79cd\u58f0\u5450\u6a21\u5f0f\uff08\u5305\u62ec\u4fa7\u626b\u58f0\u5450\u3001\u524d\u89c6\u58f0\u5450\u3001\u5408\u6210\u5b54\u5f84\u58f0\u5450\u3001\u591a\u6ce2\u675f\u56de\u58f0\u63a2\u6d4b\u5668\u548c\u53cc\u9891\u8bc6\u522b\u58f0\u5450\uff09\u7684\u516c\u5f00\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u6620\u5c04\uff0c\u5e76\u5bf9\u5206\u7c7b\u3001\u68c0\u6d4b\u3001\u5206\u5272\u548c 3D \u91cd\u5efa\u7b49\u5e94\u7528\u8fdb\u884c\u4e86\u5206\u6790\u3002", "result": "\u672c\u6587\u91cd\u70b9\u4ecb\u7ecd\u4e86\u6700\u65b0\u7684\u8fdb\u5c55\uff0c\u5e76\u6574\u5408\u4e86\u65b0\u53d1\u5e03\u7684\u6570\u636e\u96c6\u3002\u7814\u7a76\u7ed3\u679c\u88ab\u7efc\u5408\u6210\u4e00\u4e2a\u4e3b\u8868\u548c\u4e00\u4e2a\u65f6\u95f4\u8f74\uff0c\u6e05\u6670\u5730\u6bd4\u8f83\u4e86\u6570\u636e\u96c6\u7684\u7279\u5f81\u3001\u5927\u5c0f\u548c\u6ce8\u91ca\u7ec6\u8282\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u901a\u8fc7\u63d0\u4f9b\u58f0\u5450\u56fe\u50cf\u6570\u636e\u96c6\u7684\u5168\u9762\u548c\u7b80\u6d01\u7684\u56de\u987e\uff0c\u4e3a\u6c34\u58f0\u6570\u636e\u5206\u6790\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2510.03259", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03259", "abs": "https://arxiv.org/abs/2510.03259", "authors": ["Yoonjeon Kim", "Doohyuk Jang", "Eunho Yang"], "title": "Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning", "comment": "preprint", "summary": "Recent studies on reasoning models explore the meta-awareness of language\nmodels, the ability to know how to think by itself. We argue that large\nreasoning models lack this meta-awareness property by proving severe\nmisalignment between true rollouts and predicted meta information. We posit\nthat aligning meta-prediction with true rollouts will lead to significant\nperformance gains. To verify this hypothesis, we design a training pipeline\nthat boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced\nmeta-awareness directly translates to improved accuracy. Unlike existing\nmeta-cognitive reasoning models, our method does not require external training\nsources but leverages self-generated signals to train meta-awareness. Moreover,\nour method enables efficient training by i) filtering out zero-variance prompts\nthat are either trivial or unsolvable and ii) cutting off lengthy rollouts when\nthey are unlikely to lead to correct answers. The results are inspiring: our\nstrategy yields significant improvements in both accuracy and training\nefficiency on in-domain tasks and shows strong generalization to out-of-domain\nbenchmarks. More specifically, our method can speed up GRPO training by over\n1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on\nAIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with\nmeta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 %\nboost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks\nspanning logical, scientific, and coding domains.", "AI": {"tldr": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u7f3a\u4e4f\u5143\u8ba4\u77e5\u80fd\u529b\uff0c\u901a\u8fc7\u81ea\u6211\u5bf9\u9f50\u63d0\u5347\u5143\u8ba4\u77e5\u80fd\u529b\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u7f3a\u4e4f\u5143\u8ba4\u77e5\u80fd\u529b\uff0c\u5373\u65e0\u6cd5\u4e86\u89e3\u81ea\u8eab\u7684\u601d\u8003\u65b9\u5f0f\uff0c\u5bfc\u81f4\u771f\u5b9e\u60c5\u51b5\u4e0e\u9884\u6d4b\u7684\u5143\u4fe1\u606f\u4e0d\u4e00\u81f4\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u901a\u8fc7\u81ea\u6211\u5bf9\u9f50\u63d0\u5347\u5143\u8ba4\u77e5\u80fd\u529b (MASA) \u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5229\u7528\u81ea\u751f\u6210\u4fe1\u53f7\u8bad\u7ec3\u5143\u8ba4\u77e5\u80fd\u529b\uff0c\u65e0\u9700\u5916\u90e8\u8bad\u7ec3\u8d44\u6e90\u3002\u901a\u8fc7\u8fc7\u6ee4\u96f6\u65b9\u5dee\u63d0\u793a\u548c\u63d0\u524d\u505c\u6b62\u4e0d\u592a\u53ef\u80fd\u5f97\u5230\u6b63\u786e\u7b54\u6848\u7684\u5c55\u5f00\u6765\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u5728\u9886\u57df\u5185\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u5728\u9886\u57df\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5728 AIME25 \u4e0a\u63d0\u9ad8\u4e86 19.3% \u7684\u51c6\u786e\u7387\uff0c\u5728\u516d\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u9ad8\u4e86 6.2%\u3002\u5728 GPQA-Diamond \u4e0a\u63d0\u9ad8\u4e86 3.87%\uff0c\u5728\u8de8\u8d8a\u903b\u8f91\u3001\u79d1\u5b66\u548c\u7f16\u7801\u9886\u57df\u7684 13 \u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u603b\u4f53\u51c6\u786e\u7387\u63d0\u9ad8\u4e86 2.08%\u3002", "conclusion": "\u901a\u8fc7\u5143\u8ba4\u77e5\u6307\u5bfc\u8fdb\u884c\u8bad\u7ec3\u53ef\u4ee5\u589e\u5f3a\u9886\u57df\u5916\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.03727", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03727", "abs": "https://arxiv.org/abs/2510.03727", "authors": ["Xuehai He"], "title": "Bridging the Gap Between Multimodal Foundation Models and World Models", "comment": "PhD thesis", "summary": "Humans understand the world through the integration of multiple sensory\nmodalities, enabling them to perceive, reason about, and imagine dynamic\nphysical processes. Inspired by this capability, multimodal foundation models\n(MFMs) have emerged as powerful tools for multimodal understanding and\ngeneration. However, today's MFMs fall short of serving as effective world\nmodels. They lack the essential ability such as perform counterfactual\nreasoning, simulate dynamics, understand the spatiotemporal information,\ncontrol generated visual outcomes, and perform multifaceted reasoning. We\ninvestigates what it takes to bridge the gap between multimodal foundation\nmodels and world models. We begin by improving the reasoning capabilities of\nMFMs through discriminative tasks and equipping MFMs with structured reasoning\nskills, such as causal inference, counterfactual thinking, and spatiotemporal\nreasoning, enabling them to go beyond surface correlations and understand\ndeeper relationships within visual and textual data. Next, we explore\ngenerative capabilities of multimodal foundation models across both image and\nvideo modalities, introducing new frameworks for structured and controllable\ngeneration. Our approaches incorporate scene graphs, multimodal conditioning,\nand multimodal alignment strategies to guide the generation process, ensuring\nconsistency with high-level semantics and fine-grained user intent. We further\nextend these techniques to controllable 4D generation, enabling interactive,\neditable, and morphable object synthesis over time and space.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u5f25\u5408\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b (MFM) \u548c\u4e16\u754c\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u76ee\u524d\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b (MFM) \u5728\u6267\u884c\u53cd\u4e8b\u5b9e\u63a8\u7406\u3001\u6a21\u62df\u52a8\u6001\u3001\u7406\u89e3\u65f6\u7a7a\u4fe1\u606f\u3001\u63a7\u5236\u751f\u6210\u7684\u89c6\u89c9\u7ed3\u679c\u4ee5\u53ca\u6267\u884c\u591a\u65b9\u9762\u63a8\u7406\u7b49\u57fa\u672c\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u4f5c\u4e3a\u6709\u6548\u7684\u4e16\u754c\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u5224\u522b\u4efb\u52a1\u63d0\u9ad8 MFM \u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4e3a MFM \u914d\u5907\u7ed3\u6784\u5316\u63a8\u7406\u6280\u80fd\uff0c\u4f8b\u5982\u56e0\u679c\u63a8\u7406\u3001\u53cd\u4e8b\u5b9e\u601d\u7ef4\u548c\u65f6\u7a7a\u63a8\u7406\u3002\u63a2\u7d22\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u56fe\u50cf\u548c\u89c6\u9891\u6a21\u6001\u4e2d\u7684\u751f\u6210\u80fd\u529b\uff0c\u5f15\u5165\u7528\u4e8e\u7ed3\u6784\u5316\u548c\u53ef\u63a7\u751f\u6210\u7684\u65b0\u6846\u67b6\u3002\u7ed3\u5408\u573a\u666f\u56fe\u3001\u591a\u6a21\u6001\u6761\u4ef6\u548c\u591a\u6a21\u6001\u5bf9\u9f50\u7b56\u7565\u6765\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u786e\u4fdd\u4e0e\u9ad8\u7ea7\u8bed\u4e49\u548c\u7ec6\u7c92\u5ea6\u7684\u7528\u6237\u610f\u56fe\u4fdd\u6301\u4e00\u81f4\u3002\u5c06\u8fd9\u4e9b\u6280\u672f\u8fdb\u4e00\u6b65\u6269\u5c55\u5230\u53ef\u63a7 4D \u751f\u6210\uff0c\u4ece\u800c\u5b9e\u73b0\u968f\u65f6\u95f4\u548c\u7a7a\u95f4\u8fdb\u884c\u4ea4\u4e92\u3001\u7f16\u8f91\u548c\u53d8\u5f62\u7684\u5bf9\u8c61\u5408\u6210\u3002", "result": "\u65b0\u7684\u6846\u67b6\u53ef\u4ee5\u63d0\u5347\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u63a8\u7406\u548c\u751f\u6210\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "\u672c\u6587 \u0438\u0441\u0441\u043b\u0435\u0434\u0443\u0435\u0442\u4e86\u5f25\u5408\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b (MFM) \u548c\u4e16\u754c\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u6846\u67b6\u3002"}}
{"id": "2510.03561", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03561", "abs": "https://arxiv.org/abs/2510.03561", "authors": ["Adam Filipek"], "title": "Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models", "comment": "25 pages, 13 figures", "summary": "The Transformer architecture has become the de facto standard for Large\nLanguage Models (LLMs), demonstrating remarkable capabilities in language\nunderstanding and generation. However, its application in conversational AI is\nfundamentally constrained by its stateless nature and the quadratic\ncomputational complexity ($O(L^2)$) with respect to sequence length $L$.\nCurrent models emulate memory by reprocessing an ever-expanding conversation\nhistory with each turn, leading to prohibitive costs and latency in long\ndialogues. This paper introduces the Reactive Transformer (RxT), a novel\narchitecture designed to overcome these limitations by shifting from a\ndata-driven to an event-driven paradigm. RxT processes each conversational turn\nas a discrete event in real-time, maintaining context in an integrated,\nfixed-size Short-Term Memory (STM) system. The architecture features a distinct\noperational cycle where a generator-decoder produces a response based on the\ncurrent query and the previous memory state, after which a memory-encoder and a\ndedicated Memory Attention network asynchronously update the STM with a\nrepresentation of the complete interaction. This design fundamentally alters\nthe scaling dynamics, reducing the total user-facing cost of a conversation\nfrom quadratic ($O(N^2 \\cdot T)$) to linear ($O(N \\cdot T)$) with respect to\nthe number of interactions $N$. By decoupling response generation from memory\nupdates, RxT achieves low latency, enabling truly real-time, stateful, and\neconomically viable long-form conversations. We validated our architecture with\na series of proof-of-concept experiments on synthetic data, demonstrating\nsuperior performance and constant-time inference latency compared to a baseline\nstateless model of comparable size.", "AI": {"tldr": "\u63d0\u51fa\u4e86 Reactive Transformer (RxT)\uff0c\u4e00\u79cd\u7528\u4e8e\u5bf9\u8bdd AI \u7684\u65b0\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u7ef4\u62a4\u56fa\u5b9a\u5927\u5c0f\u7684\u77ed\u671f\u8bb0\u5fc6 (STM) \u7cfb\u7edf\uff0c\u4ee5\u4e8b\u4ef6\u9a71\u52a8\u7684\u65b9\u5f0f\u5904\u7406\u5bf9\u8bdd\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u3002", "motivation": "Transformer \u5728\u4f1a\u8bdd AI \u4e2d\u7684\u5e94\u7528\u53d7\u5230\u5176\u65e0\u72b6\u6001\u7279\u6027\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u9650\u5236\uff0c\u5bfc\u81f4\u957f\u5bf9\u8bdd\u4e2d\u6210\u672c\u9ad8\u6602\u548c\u5ef6\u8fdf\u3002", "method": "RxT \u5c06\u6bcf\u4e2a\u4f1a\u8bdd\u8f6e\u6b21\u4f5c\u4e3a\u79bb\u6563\u4e8b\u4ef6\u5b9e\u65f6\u5904\u7406\uff0c\u5229\u7528\u751f\u6210\u5668-\u89e3\u7801\u5668\u751f\u6210\u54cd\u5e94\uff0c\u5e76\u901a\u8fc7\u8bb0\u5fc6\u7f16\u7801\u5668\u548c\u8bb0\u5fc6\u6ce8\u610f\u529b\u7f51\u7edc\u5f02\u6b65\u66f4\u65b0 STM\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u7684\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u76f8\u5f53\u5927\u5c0f\u7684\u57fa\u7ebf\u65e0\u72b6\u6001\u6a21\u578b\u76f8\u6bd4\uff0cRxT \u5177\u6709\u5353\u8d8a\u7684\u6027\u80fd\u548c\u6052\u5b9a\u65f6\u95f4\u7684\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "RxT \u901a\u8fc7\u5c06\u54cd\u5e94\u751f\u6210\u4e0e\u8bb0\u5fc6\u66f4\u65b0\u5206\u79bb\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u3001\u771f\u6b63\u5b9e\u65f6\u3001\u6709\u72b6\u6001\u4e14\u7ecf\u6d4e\u53ef\u884c\u7684\u957f\u5bf9\u8bdd\u3002"}}
{"id": "2510.03356", "categories": ["cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.03356", "abs": "https://arxiv.org/abs/2510.03356", "authors": ["Ziyang Chen", "Yuta Itoh", "Kaan Ak\u015fit"], "title": "Learned Display Radiance Fields with Lensless Cameras", "comment": null, "summary": "Calibrating displays is a basic and regular task that content creators must\nperform to maintain optimal visual experience, yet it remains a troublesome\nissue. Measuring display characteristics from different viewpoints often\nrequires specialized equipment and a dark room, making it inaccessible to most\nusers. To avoid specialized hardware requirements in display calibrations, our\nwork co-designs a lensless camera and an Implicit Neural Representation based\nalgorithm for capturing display characteristics from various viewpoints. More\nspecifically, our pipeline enables efficient reconstruction of light fields\nemitted from a display from a viewing cone of 46.6{\\deg} X 37.6{\\deg}. Our\nemerging pipeline paves the initial steps towards effortless display\ncalibration and characterization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u663e\u793a\u5668\u6821\u51c6\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u65e0\u900f\u955c\u76f8\u673a\u548c\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u7684\u7b97\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u9700\u8981\u4e13\u4e1a\u8bbe\u5907\u548c\u6697\u5ba4\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece\u4e0d\u540c\u89c6\u89d2\u6355\u6349\u663e\u793a\u5668\u7279\u6027\u3002", "motivation": "\u4f20\u7edf\u663e\u793a\u5668\u6821\u51c6\u9700\u8981\u4e13\u4e1a\u8bbe\u5907\u548c\u6697\u5ba4\uff0c\u8fd9\u4f7f\u5f97\u5927\u591a\u6570\u7528\u6237\u96be\u4ee5\u8fdb\u884c\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u672c\u6587\u5171\u540c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65e0\u900f\u955c\u76f8\u673a\u548c\u4e00\u4e2a\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u4e0d\u540c\u89c6\u89d2\u6355\u6349\u663e\u793a\u5668\u7279\u6027\u3002\u8be5\u7ba1\u7ebf\u80fd\u591f\u6709\u6548\u5730\u91cd\u5efa\u4ece\u663e\u793a\u5668\u53d1\u51fa\u7684\u3001\u89c6\u89d2\u9525\u4e3a46.6{\\deg} X 37.6{\\deg}\u7684\u5149\u573a\u3002", "result": "\u8be5\u7ba1\u7ebf\u80fd\u591f\u6709\u6548\u5730\u91cd\u5efa\u4ece\u663e\u793a\u5668\u53d1\u51fa\u7684\u3001\u89c6\u89d2\u9525\u4e3a46.6{\\deg} X 37.6{\\deg}\u7684\u5149\u573a\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7ba1\u7ebf\u4e3a\u8f7b\u677e\u7684\u663e\u793a\u5668\u6821\u51c6\u548c\u8868\u5f81\u5960\u5b9a\u4e86\u521d\u6b65\u57fa\u7840\u3002"}}
{"id": "2510.03260", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03260", "abs": "https://arxiv.org/abs/2510.03260", "authors": ["Juan Jose Herrera-Aranda", "Guillermo Gomez-Trenado", "Francisco Herrera", "Isaac Triguero"], "title": "Semantic-Inductive Attribute Selection for Zero-Shot Learning", "comment": "26 pages, 9 figures, code available at\n  https://kiedie.github.io/Semantic-Inductive-Attribute-Selection-for-Zero-Shot-Learning/", "summary": "Zero-Shot Learning is an important paradigm within General-Purpose Artificial\nIntelligence Systems, particularly in those that operate in open-world\nscenarios where systems must adapt to new tasks dynamically. Semantic spaces\nplay a pivotal role as they bridge seen and unseen classes, but whether\nhuman-annotated or generated by a machine learning model, they often contain\nnoisy, redundant, or irrelevant attributes that hinder performance. To address\nthis, we introduce a partitioning scheme that simulates unseen conditions in an\ninductive setting (which is the most challenging), allowing attribute relevance\nto be assessed without access to semantic information from unseen classes.\nWithin this framework, we study two complementary feature-selection strategies\nand assess their generalisation. The first adapts embedded feature selection to\nthe particular demands of ZSL, turning model-driven rankings into meaningful\nsemantic pruning; the second leverages evolutionary computation to directly\nexplore the space of attribute subsets more broadly. Experiments on five\nbenchmark datasets (AWA2, CUB, SUN, aPY, FLO) show that both methods\nconsistently improve accuracy on unseen classes by reducing redundancy, but in\ncomplementary ways: RFS is efficient and competitive though dependent on\ncritical hyperparameters, whereas GA is more costly yet explores the search\nspace more broadly and avoids such dependence. These results confirm that\nsemantic spaces are inherently redundant and highlight the proposed\npartitioning scheme as an effective tool to refine them under inductive\nconditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5212\u5206\u65b9\u6848\uff0c\u53ef\u4ee5\u5728\u5f52\u7eb3\u8bbe\u7f6e\u4e2d\u6a21\u62df\u672a\u89c1\u6761\u4ef6\uff0c\u4ece\u800c\u8bc4\u4f30\u5c5e\u6027\u76f8\u5173\u6027\uff0c\u800c\u65e0\u9700\u8bbf\u95ee\u672a\u89c1\u7c7b\u7684\u8bed\u4e49\u4fe1\u606f\u3002", "motivation": "\u8bed\u4e49\u7a7a\u95f4\u5728\u8fde\u63a5\u53ef\u89c1\u7c7b\u548c\u672a\u89c1\u7c7b\u65b9\u9762\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u5305\u542bnoisy\u3001\u5197\u4f59\u6216\u4e0d\u76f8\u5173\u7684\u5c5e\u6027\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u6027\u80fd\u3002", "method": "\u672c\u6587\u7814\u7a76\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u7279\u5f81\u9009\u62e9\u7b56\u7565\uff0c\u5e76\u8bc4\u4f30\u4e86\u5b83\u4eec\u7684\u6cdb\u5316\u80fd\u529b\uff1aRFS\u548cGA\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u80fd\u901a\u8fc7\u51cf\u5c11\u5197\u4f59\u6765\u6301\u7eed\u63d0\u9ad8\u672a\u89c1\u7c7b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8bc1\u5b9e\u4e86\u8bed\u4e49\u7a7a\u95f4\u672c\u8d28\u4e0a\u662f\u5197\u4f59\u7684\uff0c\u5e76\u5f3a\u8c03\u4e86\u6240\u63d0\u51fa\u7684\u5212\u5206\u65b9\u6848\u662f\u5f52\u7eb3\u6761\u4ef6\u4e0b\u6539\u8fdb\u8bed\u4e49\u7a7a\u95f4\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.03771", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03771", "abs": "https://arxiv.org/abs/2510.03771", "authors": ["Divij Handa", "David Blincoe", "Orson Adams", "Yinlin Fu"], "title": "OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation", "comment": null, "summary": "Deploying capable and user-aligned LLM-based systems necessitates reliable\nevaluation. While LLMs excel in verifiable tasks like coding and mathematics,\nwhere gold-standard solutions are available, adoption remains challenging for\nsubjective tasks that lack a single correct answer. E-commerce Query Rewriting\n(QR) is one such problem where determining whether a rewritten query properly\ncaptures the user intent is extremely difficult to figure out algorithmically.\nIn this work, we introduce OptAgent, a novel framework that combines\nmulti-agent simulations with genetic algorithms to verify and optimize queries\nfor QR. Instead of relying on a static reward model or a single LLM judge, our\napproach uses multiple LLM-based agents, each acting as a simulated shopping\ncustomer, as a dynamic reward signal. The average of these agent-derived scores\nserves as an effective fitness function for an evolutionary algorithm that\niteratively refines the user's initial query. We evaluate OptAgent on a dataset\nof 1000 real-world e-commerce queries in five different categories, and we\nobserve an average improvement of 21.98% over the original user query and 3.36%\nover a Best-of-N LLM rewriting baseline.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOptAgent\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u591a\u667a\u80fd\u4f53\u6a21\u62df\u548c\u9057\u4f20\u7b97\u6cd5\uff0c\u7528\u4e8e\u9a8c\u8bc1\u548c\u4f18\u5316QR\u7684\u67e5\u8be2\u3002", "motivation": "\u53ef\u9760\u7684\u8bc4\u4f30\u5bf9\u4e8e\u90e8\u7f72\u6709\u80fd\u529b\u4e14\u7b26\u5408\u7528\u6237\u9700\u6c42\u7684\u57fa\u4e8ellm\u7684\u7cfb\u7edf\u662f\u5fc5\u8981\u7684\u3002\u7535\u5b50\u5546\u52a1\u67e5\u8be2\u91cd\u5199(QR)\u5c31\u662f\u4e00\u4e2a\u8fd9\u6837\u7684\u95ee\u9898\uff0c\u786e\u5b9a\u91cd\u5199\u7684\u67e5\u8be2\u662f\u5426\u6b63\u786e\u5730\u6355\u83b7\u4e86\u7528\u6237\u7684\u610f\u56fe\u975e\u5e38\u56f0\u96be\u3002", "method": "\u8be5\u65b9\u6cd5\u4f7f\u7528\u591a\u4e2a\u57fa\u4e8ellm\u7684\u4ee3\u7406(\u6bcf\u4e2a\u4ee3\u7406\u90fd\u5145\u5f53\u6a21\u62df\u8d2d\u7269\u5ba2\u6237)\u4f5c\u4e3a\u52a8\u6001\u5956\u52b1\u4fe1\u53f7\u3002\u8fd9\u4e9bagent\u6d3e\u751f\u5206\u6570\u7684\u5e73\u5747\u503c\u4f5c\u4e3a\u8fdb\u5316\u7b97\u6cd5\u7684\u6709\u6548\u9002\u5e94\u5ea6\u51fd\u6570\uff0c\u8be5\u7b97\u6cd5\u8fed\u4ee3\u5730\u4f18\u5316\u7528\u6237\u7684\u521d\u59cb\u67e5\u8be2\u3002", "result": "\u5728\u4e94\u4e2a\u4e0d\u540c\u7c7b\u522b\u76841000\u4e2a\u771f\u5b9e\u7535\u5b50\u5546\u52a1\u67e5\u8be2\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30OptAgent\uff0c\u5e76\u4e14\u6211\u4eec\u89c2\u5bdf\u5230\u6bd4\u539f\u59cb\u7528\u6237\u67e5\u8be2\u5e73\u5747\u63d0\u9ad8\u4e8621.98%\uff0c\u6bd4best-of-n LLM\u91cd\u5199\u57fa\u7ebf\u5e73\u5747\u63d0\u9ad8\u4e863.36%\u3002", "conclusion": "OptAgent\u6846\u67b6\u6709\u6548\u5730\u5229\u7528\u591a\u667a\u80fd\u4f53\u6a21\u62df\u548c\u9057\u4f20\u7b97\u6cd5\u6765\u9a8c\u8bc1\u548c\u4f18\u5316\u7535\u5b50\u5546\u52a1\u67e5\u8be2\u91cd\u5199\u4efb\u52a1\uff0c\u5e76\u5728\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002"}}
{"id": "2510.03361", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03361", "abs": "https://arxiv.org/abs/2510.03361", "authors": ["Ali Kayyam", "Anusha Madan Gopal", "M. Anthony Lewis"], "title": "Provenance Networks: End-to-End Exemplar-Based Explainability", "comment": null, "summary": "We introduce provenance networks, a novel class of neural models designed to\nprovide end-to-end, training-data-driven explainability. Unlike conventional\npost-hoc methods, provenance networks learn to link each prediction directly to\nits supporting training examples as part of the model's normal operation,\nembedding interpretability into the architecture itself. Conceptually, the\nmodel operates similarly to a learned KNN, where each output is justified by\nconcrete exemplars weighted by relevance in the feature space. This approach\nfacilitates systematic investigations of the trade-off between memorization and\ngeneralization, enables verification of whether a given input was included in\nthe training set, aids in the detection of mislabeled or anomalous data points,\nenhances resilience to input perturbations, and supports the identification of\nsimilar inputs contributing to the generation of a new data point. By jointly\noptimizing the primary task and the explainability objective, provenance\nnetworks offer insights into model behavior that traditional deep networks\ncannot provide. While the model introduces additional computational cost and\ncurrently scales to moderately sized datasets, it provides a complementary\napproach to existing explainability techniques. In particular, it addresses\ncritical challenges in modern deep learning, including model opaqueness,\nhallucination, and the assignment of credit to data contributors, thereby\nimproving transparency, robustness, and trustworthiness in neural models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u6a21\u578b\uff0c\u79f0\u4e3a\u6765\u6e90\u7f51\u7edc\uff0c\u65e8\u5728\u63d0\u4f9b\u7aef\u5230\u7aef\u3001\u7531\u8bad\u7ec3\u6570\u636e\u9a71\u52a8\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u7f3a\u4e4f\u6a21\u578b\u900f\u660e\u5ea6\uff0c\u6765\u6e90\u7f51\u7edc\u901a\u8fc7\u5c06\u6bcf\u4e2a\u9884\u6d4b\u76f4\u63a5\u4e0e\u5176\u652f\u6301\u7684\u8bad\u7ec3\u793a\u4f8b\u8054\u7cfb\u8d77\u6765\uff0c\u5d4c\u5165\u53ef\u89e3\u91ca\u6027\u5230\u67b6\u6784\u4e2d\uff0c\u4ece\u800c\u89e3\u51b3\u6a21\u578b\u4e0d\u900f\u660e\u3001\u5e7b\u89c9\u548c\u6570\u636e\u8d21\u732e\u8005\u4fe1\u7528\u5206\u914d\u7b49\u95ee\u9898\u3002", "method": "\u8be5\u6a21\u578b\u7c7b\u4f3c\u4e8e\u5b66\u4e60\u7684KNN\uff0c\u5176\u4e2d\u6bcf\u4e2a\u8f93\u51fa\u90fd\u7531\u7279\u5f81\u7a7a\u95f4\u4e2d\u76f8\u5173\u6027\u52a0\u6743\u7684\u5177\u4f53\u7684\u4f8b\u5b50\u6765\u8bc1\u660e\u3002", "result": "\u6765\u6e90\u7f51\u7edc\u53ef\u4ee5\u7cfb\u7edf\u5730\u7814\u7a76\u8bb0\u5fc6\u548c\u6cdb\u5316\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u9a8c\u8bc1\u7ed9\u5b9a\u7684\u8f93\u5165\u662f\u5426\u5305\u542b\u5728\u8bad\u7ec3\u96c6\u4e2d\uff0c\u5e2e\u52a9\u68c0\u6d4b\u9519\u8bef\u6807\u8bb0\u6216\u5f02\u5e38\u6570\u636e\u70b9\uff0c\u589e\u5f3a\u5bf9\u8f93\u5165\u6270\u52a8\u7684\u5f39\u6027\uff0c\u5e76\u652f\u6301\u8bc6\u522b\u6709\u52a9\u4e8e\u751f\u6210\u65b0\u6570\u636e\u70b9\u7684\u7c7b\u4f3c\u8f93\u5165\u3002", "conclusion": "\u6765\u6e90\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e0e\u73b0\u6709\u53ef\u89e3\u91ca\u6027\u6280\u672f\u4e92\u8865\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u9ad8\u795e\u7ecf\u6a21\u578b\u7684\u900f\u660e\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u53ef\u4fe1\u5ea6\u6765\u89e3\u51b3\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2510.03261", "categories": ["cs.LG", "cs.CE", "J.2; I.2"], "pdf": "https://arxiv.org/pdf/2510.03261", "abs": "https://arxiv.org/abs/2510.03261", "authors": ["C. Coelho", "M. Hohmann", "D. Fern\u00e1ndez", "L. Penter", "S. Ihlenfeldt", "O. Niggemann"], "title": "Data-Driven Temperature Modelling of Machine Tools by Neural Networks: A Benchmark", "comment": null, "summary": "Thermal errors in machine tools significantly impact machining precision and\nproductivity. Traditional thermal error correction/compensation methods rely on\nmeasured temperature-deformation fields or on transfer functions. Most existing\ndata-driven compensation strategies employ neural networks (NNs) to directly\npredict thermal errors or specific compensation values. While effective, these\napproaches are tightly bound to particular error types, spatial locations, or\nmachine configurations, limiting their generality and adaptability. In this\nwork, we introduce a novel paradigm in which NNs are trained to predict\nhigh-fidelity temperature and heat flux fields within the machine tool. The\nproposed framework enables subsequent computation and correction of a wide\nrange of error types using modular, swappable downstream components. The NN is\ntrained using data obtained with the finite element method under varying\ninitial conditions and incorporates a correlation-based selection strategy that\nidentifies the most informative measurement points, minimising hardware\nrequirements during inference. We further benchmark state-of-the-art\ntime-series NN architectures, namely Recurrent NN, Gated Recurrent Unit,\nLong-Short Term Memory (LSTM), Bidirectional LSTM, Transformer, and Temporal\nConvolutional Network, by training both specialised models, tailored for\nspecific initial conditions, and general models, capable of extrapolating to\nunseen scenarios. The results show accurate and low-cost prediction of\ntemperature and heat flux fields, laying the basis for enabling flexible and\ngeneralisable thermal error correction in machine tool environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8303\u4f8b\uff0c\u5176\u4e2d\u795e\u7ecf\u7f51\u7edc\u7ecf\u8fc7\u8bad\u7ec3\u53ef\u4ee5\u9884\u6d4b\u673a\u5e8a\u5185\u7684\u9ad8\u4fdd\u771f\u6e29\u5ea6\u548c\u70ed\u901a\u91cf\u573a\uff0c\u4ece\u800c\u80fd\u591f\u4f7f\u7528\u53ef\u4e92\u6362\u7684\u4e0b\u6e38\u7ec4\u4ef6\u8ba1\u7b97\u548c\u6821\u6b63\u5404\u79cd\u8bef\u5dee\u7c7b\u578b\u3002", "motivation": "\u4f20\u7edf\u7684\u70ed\u8bef\u5dee\u6821\u6b63/\u8865\u507f\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6d4b\u91cf\u7684\u6e29\u5ea6-\u53d8\u5f62\u573a\u6216\u4f20\u9012\u51fd\u6570\u3002\u5927\u591a\u6570\u73b0\u6709\u7684\u6570\u636e\u9a71\u52a8\u8865\u507f\u7b56\u7565\u91c7\u7528\u795e\u7ecf\u7f51\u7edc (NN) \u6765\u76f4\u63a5\u9884\u6d4b\u70ed\u8bef\u5dee\u6216\u7279\u5b9a\u8865\u507f\u503c\u3002\u867d\u7136\u6709\u6548\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u4e0e\u7279\u5b9a\u7684\u8bef\u5dee\u7c7b\u578b\u3001\u7a7a\u95f4\u4f4d\u7f6e\u6216\u673a\u5668\u914d\u7f6e\u7d27\u5bc6\u76f8\u5173\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u795e\u7ecf\u7f51\u7edc\u662f\u4f7f\u7528\u6709\u9650\u5143\u65b9\u6cd5\u5728\u53d8\u5316\u7684\u521d\u59cb\u6761\u4ef6\u4e0b\u83b7\u5f97\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u7684\uff0c\u5e76\u7ed3\u5408\u4e86\u57fa\u4e8e\u76f8\u5173\u7684\u9009\u62e9\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u53ef\u8bc6\u522b\u4fe1\u606f\u91cf\u6700\u5927\u7684\u6d4b\u91cf\u70b9\uff0c\u4ece\u800c\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u63a8\u7406\u671f\u95f4\u7684\u786c\u4ef6\u9700\u6c42\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5bf9\u6700\u5148\u8fdb\u7684\u65f6\u95f4\u5e8f\u5217 NN \u67b6\u6784\uff08\u5373\u5faa\u73af NN\u3001\u95e8\u63a7\u5faa\u73af\u5355\u5143\u3001\u957f\u77ed\u671f\u8bb0\u5fc6 (LSTM)\u3001\u53cc\u5411 LSTM\u3001Transformer \u548c\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\uff09\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e13\u95e8\u7684\u6a21\u578b\uff08\u9488\u5bf9\u7279\u5b9a\u521d\u59cb\u6761\u4ef6\u5b9a\u5236\uff09\u548c\u901a\u7528\u6a21\u578b\uff08\u80fd\u591f\u5916\u63a8\u5230\u770b\u4e0d\u89c1\u7684\u573a\u666f\uff09\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6e29\u5ea6\u548c\u70ed\u901a\u91cf\u573a\u7684\u9884\u6d4b\u51c6\u786e\u4e14\u6210\u672c\u4f4e\u5ec9\uff0c\u4e3a\u5728\u673a\u5e8a\u73af\u5883\u4e2d\u5b9e\u73b0\u7075\u6d3b\u4e14\u901a\u7528\u7684\u70ed\u8bef\u5dee\u6821\u6b63\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u673a\u5e8a\u73af\u5883\u4e2d\u7684\u70ed\u8bef\u5dee\u6821\u6b63\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u3001\u901a\u7528\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.03777", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03777", "abs": "https://arxiv.org/abs/2510.03777", "authors": ["Divij Handa", "Mihir Parmar", "Aswin RRV", "Md Nayem Uddin", "Hamid Palangi", "Chitta Baral"], "title": "GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time", "comment": null, "summary": "Repeated Sampling (RS) is a simple inference-time algorithm that has been\nshown to improve model performance on complex tasks. Although it is an\neffective way of scaling inference time, it often struggles to generate diverse\nsolution candidates, frequently relying on the same underlying approach to\nsolve the problem and thus producing redundant samples. To address this\nlimitation, we propose a new inference algorithm, GuidedSampling, which\ndecouples the exploration and generation phases during inference, increasing\ndiversity of generated candidate solutions. The exploration phase identifies\nmultiple concepts that can be utilized to solve the problem, while the\ngeneration phase applies a specific concept to provide final solution\ncandidates. We first define the theoretical bounds of GuidedSampling and then\nempirically demonstrate that it improves the performance of base model at\npass@50 by on an average ~21.6% across various benchmarks compared to RS.\nFurthermore, models trained on trajectories of GuidedSampling exhibit\nsubstantial performance improvements at pass@5 by on an average ~9.7%, compared\nto models trained on traditional RS. Additionally, models trained with\nGuidedSampling increases the average number of concepts per instance (1.67 ->\n3.03), yielding a diverse set of candidates than traditional RS.", "AI": {"tldr": "GuidedSampling improves model performance by decoupling exploration and generation phases, increasing diversity in solution candidates.", "motivation": "Repeated Sampling struggles to generate diverse solution candidates, relying on the same underlying approach.", "method": "The GuidedSampling algorithm decouples exploration and generation phases. Exploration identifies multiple concepts, while generation applies a concept to provide solutions.", "result": "GuidedSampling improves pass@50 by ~21.6% compared to RS. Models trained on GuidedSampling trajectories improve pass@5 by ~9.7% and increase the average number of concepts per instance (1.67 -> 3.03).", "conclusion": "GuidedSampling yields a more diverse set of candidates than traditional RS."}}
{"id": "2510.03595", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03595", "abs": "https://arxiv.org/abs/2510.03595", "authors": ["Haikang Deng", "Po-Nien Kung", "Nanyun Peng"], "title": "Decoupling Task-Solving and Output Formatting in LLM Generation", "comment": null, "summary": "Large language models (LLMs) are increasingly adept at following instructions\ncontaining task descriptions to solve complex problems, such as mathematical\nreasoning and automatic evaluation (LLM-as-a-Judge). However, as prompts grow\nmore complex, models often struggle to adhere to all instructions. This\ndifficulty is especially common when instructive prompts intertwine reasoning\ndirectives -- specifying what the model should solve -- with rigid formatting\nrequirements that dictate how the solution must be presented. The entanglement\ncreates competing goals for the model, suggesting that more explicit separation\nof these two aspects could lead to improved performance. To this front, we\nintroduce Deco-G, a decoding framework that explicitly decouples format\nadherence from task solving. Deco-G handles format compliance with a separate\ntractable probabilistic model (TPM), while prompts LLMs with only task\ninstructions. At each decoding step, Deco-G combines next token probabilities\nfrom the LLM with the TPM calculated format compliance likelihood to form the\noutput probability. To make this approach both practical and scalable for\nmodern instruction-tuned LLMs, we introduce three key innovations:\ninstruction-aware distillation, a flexible trie-building algorithm, and HMM\nstate pruning for computational efficiency. We demonstrate the effectiveness of\nDeco-G across a wide range of tasks with diverse format requirements, including\nmathematical reasoning, LLM-as-a-judge, and event argument extraction. Overall,\nour approach yields 1.0% to 6.0% relative gain over regular prompting practice\nwith guaranteed format compliance.", "AI": {"tldr": "Deco-G\u662f\u4e00\u4e2a\u89e3\u7801\u6846\u67b6\uff0c\u5b83\u5c06\u683c\u5f0f\u9075\u4ece\u6027\u4e0e\u4efb\u52a1\u89e3\u51b3\u5206\u79bb\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9075\u5faa\u5305\u542b\u4efb\u52a1\u63cf\u8ff0\u7684\u6307\u4ee4\u4ee5\u89e3\u51b3\u590d\u6742\u95ee\u9898\u65f6\u8d8a\u6765\u8d8a\u719f\u7ec3\uff0c\u4f46\u662f\uff0c\u968f\u7740prompt\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u6a21\u578b\u901a\u5e38\u96be\u4ee5\u9075\u5faa\u6240\u6709\u6307\u4ee4\u3002\u5f53\u6307\u4ee4\u6027prompt\u5c06\u63a8\u7406\u6307\u4ee4\u4e0e\u89c4\u5b9a\u89e3\u51b3\u65b9\u6848\u5fc5\u987b\u5982\u4f55\u5448\u73b0\u7684\u4e25\u683c\u683c\u5f0f\u8981\u6c42\u4ea4\u7ec7\u5728\u4e00\u8d77\u65f6\uff0c\u8fd9\u79cd\u56f0\u96be\u5c24\u5176\u5e38\u89c1\u3002", "method": "Deco-G\u4f7f\u7528\u5355\u72ec\u7684\u6613\u5904\u7406\u6982\u7387\u6a21\u578b\uff08TPM\uff09\u5904\u7406\u683c\u5f0f\u9075\u4ece\u6027\uff0c\u540c\u65f6\u4ec5\u4f7f\u7528\u4efb\u52a1\u6307\u4ee4prompt LLM\u3002\u5728\u6bcf\u4e2a\u89e3\u7801\u6b65\u9aa4\u4e2d\uff0cDeco-G\u5c06\u6765\u81eaLLM\u7684\u4e0b\u4e00\u4e2atoken\u6982\u7387\u4e0eTPM\u8ba1\u7b97\u7684\u683c\u5f0f\u9075\u4ece\u6027\u53ef\u80fd\u6027\u76f8\u7ed3\u5408\uff0c\u4ee5\u5f62\u6210\u8f93\u51fa\u6982\u7387\u3002\u4e3a\u4e86\u4f7f\u8fd9\u79cd\u65b9\u6cd5\u65e2\u5b9e\u7528\u53c8\u53ef\u6269\u5c55\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1ainstruction-aware distillation\uff0c\u7075\u6d3b\u7684trie\u6784\u5efa\u7b97\u6cd5\u548c\u7528\u4e8e\u8ba1\u7b97\u6548\u7387\u7684HMM\u72b6\u6001\u4fee\u526a\u3002", "result": "Deco-G\u5728\u5177\u6709\u5404\u79cd\u683c\u5f0f\u8981\u6c42\u7684\u5404\u79cd\u4efb\u52a1\u4e2d\u5747\u6709\u6548\uff0c\u5305\u62ec\u6570\u5b66\u63a8\u7406\uff0cLLM-as-a-judge\u548c\u4e8b\u4ef6\u53c2\u6570\u63d0\u53d6\u3002\u603b\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6bd4\u5e38\u89c4prompt\u5b9e\u8df5\u4ea7\u751f\u4e861.0\uff05\u81f36.0\uff05\u7684\u76f8\u5bf9\u6536\u76ca\uff0c\u5e76\u4fdd\u8bc1\u4e86\u683c\u5f0f\u9075\u4ece\u6027\u3002", "conclusion": "Deco-G\u901a\u8fc7\u5c06\u683c\u5f0f\u9075\u4ece\u6027\u4e0e\u4efb\u52a1\u89e3\u51b3\u5206\u79bb\uff0c\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.03363", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.03363", "abs": "https://arxiv.org/abs/2510.03363", "authors": ["Zhe Zhang", "Mingxiu Cai", "Gaochang Wu", "Jing Zhang", "Lingqiao Liu", "Dacheng Tao", "Tianyou Chai", "Xiatian Zhu"], "title": "Unified Unsupervised Anomaly Detection via Matching Cost Filtering", "comment": "63 pages (main paper and supplementary material), 39 figures, 58\n  tables. Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)", "summary": "Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level\nanomalies using only normal training data, with wide applications such as\nindustrial inspection and medical analysis, where anomalies are scarce due to\nprivacy concerns and cold-start constraints. Existing methods, whether\nreconstruction-based (restoring normal counterparts) or embedding-based\n(pretrained representations), fundamentally conduct image- or feature-level\nmatching to generate anomaly maps. Nonetheless, matching noise has been largely\noverlooked, limiting their detection ability. Beyond earlier focus on unimodal\nRGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D\nand RGB--Text, enabled by point cloud sensing and vision--language models.\nDespite shared challenges, these lines remain largely isolated, hindering a\ncomprehensive understanding and knowledge transfer. In this paper, we advocate\nunified UAD for both unimodal and multimodal settings in the matching\nperspective. Under this insight, we present Unified Cost Filtering (UCF), a\ngeneric post-hoc refinement framework for refining anomaly cost volume of any\nUAD model. The cost volume is constructed by matching a test sample against\nnormal samples from the same or different modalities, followed by a learnable\nfiltering module with multi-layer attention guidance from the test sample,\nmitigating matching noise and highlighting subtle anomalies. Comprehensive\nexperiments on 22 diverse benchmarks demonstrate the efficacy of UCF in\nenhancing a variety of UAD methods, consistently achieving new state-of-the-art\nresults in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD\nscenarios. Code and models will be released at\nhttps://github.com/ZHE-SAPI/CostFilter-AD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u7edf\u4e00\u6210\u672c\u8fc7\u6ee4\uff08UCF\uff09\u7684\u901a\u7528\u540e\u5904\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u7ec6\u5316\u4efb\u4f55UAD\u6a21\u578b\u7684\u5f02\u5e38\u6210\u672c\u91cf\uff0c\u4ece\u800c\u51cf\u8f7b\u5339\u914d\u566a\u58f0\u5e76\u7a81\u51fa\u7ec6\u5fae\u5f02\u5e38\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5ffd\u7565\u4e86\u5339\u914d\u566a\u58f0\uff0c\u5e76\u4e14\u5355\u6a21\u548c\u591a\u6a21\u65b9\u6cd5\u4e4b\u95f4\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u89e3\u548c\u77e5\u8bc6\u8f6c\u79fb\u3002", "method": "\u901a\u8fc7\u5339\u914d\u6d4b\u8bd5\u6837\u672c\u548c\u6765\u81ea\u76f8\u540c\u6216\u4e0d\u540c\u6a21\u6001\u7684\u6b63\u5e38\u6837\u672c\u6765\u6784\u5efa\u6210\u672c\u91cf\uff0c\u7136\u540e\u4f7f\u7528\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u8fc7\u6ee4\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5177\u6709\u6765\u81ea\u6d4b\u8bd5\u6837\u672c\u7684\u591a\u5c42\u6ce8\u610f\u529b\u5f15\u5bfc\uff0c\u4ee5\u51cf\u8f7b\u5339\u914d\u566a\u58f0\u3002", "result": "\u572822\u4e2a\u4e0d\u540c\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cUCF\u5728\u589e\u5f3a\u5404\u79cdUAD\u65b9\u6cd5\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5728\u5355\u6a21\uff08RGB\uff09\u548c\u591a\u6a21\uff08RGB-3D\uff0cRGB-Text\uff09UAD\u573a\u666f\u4e2d\u5747\u59cb\u7ec8\u83b7\u5f97\u65b0\u7684\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "UCF\u662f\u4e00\u79cd\u6709\u6548\u7684\u901a\u7528\u6846\u67b6\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5404\u79cd\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5355\u6a21\u548c\u591a\u6a21\u573a\u666f\u4e2d\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002"}}
{"id": "2510.03262", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03262", "abs": "https://arxiv.org/abs/2510.03262", "authors": ["Andi Zhang", "Xuan Ding", "Haofan Wang", "Steven McDonagh", "Samuel Kaski"], "title": "Rethinking Inter-LoRA Orthogonality in Adapter Merging: Insights from Orthogonal Monte Carlo Dropout", "comment": null, "summary": "We propose Orthogonal Monte Carlo Dropout, a mechanism that enforces strict\northogonality when combining sparse semantic vectors without extra time\ncomplexity. LoRA, a popular fine-tuning method for large models, typically\ntrains a module to represent a specific concept such as an object or a style.\nWhen multiple LoRAs are merged, for example to generate an object in a\nparticular style, their semantic vectors may interfere with each other. Our\nmethod guarantees, at the theoretical and runtime levels, that merged LoRAs\nremain orthogonal and thus free from direct interference. However, empirical\nanalysis reveals that such orthogonality does not lead to the semantic\ndisentanglement or compositionality highlighted in prior work on compositional\nadaptation. This finding suggests that inter-LoRA orthogonality alone may be\ninsufficient for achieving true semantic compositionality, prompting a\nre-examination of its role in adapter merging.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u6b63\u4ea4\u8499\u7279\u5361\u6d1b Dropout\uff0c\u4e00\u79cd\u5728\u7ec4\u5408\u7a00\u758f\u8bed\u4e49\u5411\u91cf\u65f6\u5f3a\u5236\u6267\u884c\u4e25\u683c\u6b63\u4ea4\u6027\u7684\u673a\u5236\uff0c\u4e14\u6ca1\u6709\u989d\u5916\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "motivation": "\u5f53\u5408\u5e76\u591a\u4e2a LoRA \u65f6\uff0c\u5b83\u4eec\u7684\u8bed\u4e49\u5411\u91cf\u53ef\u80fd\u4f1a\u76f8\u4e92\u5e72\u6270\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u4fdd\u8bc1\u5408\u5e76\u7684 LoRA \u4fdd\u6301\u6b63\u4ea4\uff0c\u4ece\u800c\u907f\u514d\u76f4\u63a5\u5e72\u6270\u3002", "result": "\u6b63\u4ea4\u6027\u5e76\u4e0d\u4e00\u5b9a\u5bfc\u81f4\u8bed\u4e49\u89e3\u8026\u6216\u7ec4\u5408\u6027\u3002", "conclusion": "LoRA \u95f4\u7684\u6b63\u4ea4\u6027\u53ef\u80fd\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u771f\u6b63\u7684\u8bed\u4e49\u7ec4\u5408\u6027\uff0c\u4fc3\u4f7f\u4eba\u4eec\u91cd\u65b0\u5ba1\u89c6\u5176\u5728\u9002\u914d\u5668\u5408\u5e76\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2510.03845", "categories": ["cs.AI", "cs.GT", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03845", "abs": "https://arxiv.org/abs/2510.03845", "authors": ["Gon Buzaglo", "Noah Golowich", "Elad Hazan"], "title": "The Hidden Game Problem", "comment": null, "summary": "This paper investigates a class of games with large strategy spaces,\nmotivated by challenges in AI alignment and language games. We introduce the\nhidden game problem, where for each player, an unknown subset of strategies\nconsistently yields higher rewards compared to the rest. The central question\nis whether efficient regret minimization algorithms can be designed to discover\nand exploit such hidden structures, leading to equilibrium in these subgames\nwhile maintaining rationality in general. We answer this question affirmatively\nby developing a composition of regret minimization techniques that achieve\noptimal external and swap regret bounds. Our approach ensures rapid convergence\nto correlated equilibria in hidden subgames, leveraging the hidden game\nstructure for improved computational efficiency.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5177\u6709\u5927\u578b\u7b56\u7565\u7a7a\u95f4\u7684\u4e00\u7c7b\u535a\u5f08\uff0c\u5176\u52a8\u673a\u6e90\u4e8eAI\u5bf9\u9f50\u548c\u8bed\u8a00\u535a\u5f08\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u5f15\u5165\u4e86\u9690\u85cf\u535a\u5f08\u95ee\u9898\uff0c\u5176\u4e2d\u5bf9\u4e8e\u6bcf\u4e2a\u73a9\u5bb6\uff0c\u7b56\u7565\u7684\u672a\u77e5\u5b50\u96c6\u59cb\u7ec8\u4ea7\u751f\u6bd4\u5176\u4f59\u7b56\u7565\u66f4\u9ad8\u7684\u56de\u62a5\u3002", "method": "\u901a\u8fc7\u5f00\u53d1\u540e\u6094\u6700\u5c0f\u5316\u6280\u672f\u7684\u7ec4\u5408\u6765\u80af\u5b9a\u5730\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898\uff0c\u8be5\u7ec4\u5408\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u5916\u90e8\u548c\u4ea4\u6362\u540e\u6094\u754c\u9650\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u786e\u4fdd\u5728\u9690\u85cf\u5b50\u535a\u5f08\u4e2d\u5feb\u901f\u6536\u655b\u5230\u76f8\u5173\u5747\u8861\uff0c\u5229\u7528\u9690\u85cf\u535a\u5f08\u7ed3\u6784\u6765\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8bbe\u8ba1\u6709\u6548\u7684\u540e\u6094\u6700\u5c0f\u5316\u7b97\u6cd5\u6765\u53d1\u73b0\u548c\u5229\u7528\u8fd9\u4e9b\u9690\u85cf\u7ed3\u6784\uff0c\u4ece\u800c\u5728\u8fd9\u4e9b\u5b50\u535a\u5f08\u4e2d\u8fbe\u5230\u5747\u8861\uff0c\u540c\u65f6\u4fdd\u6301\u4e00\u822c\u7406\u6027\u3002"}}
{"id": "2510.03611", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03611", "abs": "https://arxiv.org/abs/2510.03611", "authors": ["Raquib Bin Yousuf", "Aadyant Khatri", "Shengzhe Xu", "Mandar Sharma", "Naren Ramakrishnan"], "title": "Can an LLM Induce a Graph? Investigating Memory Drift and Context Length", "comment": "2025 IEEE International Conference on Knowledge Graph (ICKG)", "summary": "Recently proposed evaluation benchmarks aim to characterize the effective\ncontext length and the forgetting tendencies of large language models (LLMs).\nHowever, these benchmarks often rely on simplistic 'needle in a haystack'\nretrieval or continuation tasks that may not accurately reflect the performance\nof these models in information-dense scenarios. Thus, rather than simple next\ntoken prediction, we argue for evaluating these models on more complex\nreasoning tasks that requires them to induce structured relational knowledge\nfrom the text - such as graphs from potentially noisy natural language content.\nWhile the input text can be viewed as generated in terms of a graph, its\nstructure is not made explicit and connections must be induced from distributed\ntextual cues, separated by long contexts and interspersed with irrelevant\ninformation. Our findings reveal that LLMs begin to exhibit memory drift and\ncontextual forgetting at much shorter effective lengths when tasked with this\nform of relational reasoning, compared to what existing benchmarks suggest.\nWith these findings, we offer recommendations for the optimal use of popular\nLLMs for complex reasoning tasks. We further show that even models specialized\nfor reasoning, such as OpenAI o1, remain vulnerable to early memory drift in\nthese settings. These results point to significant limitations in the models'\nability to abstract structured knowledge from unstructured input and highlight\nthe need for architectural adaptations to improve long-range reasoning.", "AI": {"tldr": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4fe1\u606f\u5bc6\u96c6\u573a\u666f\u4e0b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u7684\u8bb0\u5fc6\u6f02\u79fb\u548c\u4e0a\u4e0b\u6587\u9057\u5fd8\u95ee\u9898\u6bd4\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u7684\u66f4\u65e9\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u4e0d\u80fd\u51c6\u786e\u53cd\u6620\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4fe1\u606f\u5bc6\u96c6\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u4ece\u6587\u672c\u4e2d\u63a8\u5bfc\u7ed3\u6784\u5316\u5173\u7cfb\u77e5\u8bc6\u7684\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u3002", "method": "\u4f7f\u7528\u9700\u8981\u6a21\u578b\u4ece\u6f5c\u5728\u5608\u6742\u7684\u81ea\u7136\u8bed\u8a00\u5185\u5bb9\u4e2d\u5f52\u7eb3\u7ed3\u6784\u5316\u5173\u7cfb\u77e5\u8bc6\uff08\u4f8b\u5982\u56fe\uff09\u7684\u590d\u6742\u63a8\u7406\u4efb\u52a1\u6765\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5173\u7cfb\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u6bd4\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u7684\u66f4\u65e9\u51fa\u73b0\u8bb0\u5fc6\u6f02\u79fb\u548c\u4e0a\u4e0b\u6587\u9057\u5fd8\u3002", "conclusion": "\u73b0\u6709\u6a21\u578b\u4ece\u975e\u7ed3\u6784\u5316\u8f93\u5165\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u77e5\u8bc6\u7684\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u8fdb\u884c\u67b6\u6784\u8c03\u6574\u4ee5\u6539\u8fdb\u8fdc\u7a0b\u63a8\u7406\u3002"}}
{"id": "2510.03376", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.03376", "abs": "https://arxiv.org/abs/2510.03376", "authors": ["Sanjukta Ghosh"], "title": "Visual Language Model as a Judge for Object Detection in Industrial Diagrams", "comment": "Pre-review version submitted to IEEE ICASSP 2026", "summary": "Industrial diagrams such as piping and instrumentation diagrams (P&IDs) are\nessential for the design, operation, and maintenance of industrial plants.\nConverting these diagrams into digital form is an important step toward\nbuilding digital twins and enabling intelligent industrial automation. A\ncentral challenge in this digitalization process is accurate object detection.\nAlthough recent advances have significantly improved object detection\nalgorithms, there remains a lack of methods to automatically evaluate the\nquality of their outputs. This paper addresses this gap by introducing a\nframework that employs Visual Language Models (VLMs) to assess object detection\nresults and guide their refinement. The approach exploits the multimodal\ncapabilities of VLMs to identify missing or inconsistent detections, thereby\nenabling automated quality assessment and improving overall detection\nperformance on complex industrial diagrams.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8bc4\u4f30\u5bf9\u8c61\u68c0\u6d4b\u7ed3\u679c\u5e76\u6307\u5bfc\u6539\u8fdb\u7684\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u5de5\u4e1a\u56fe\u4e2d\u5bf9\u8c61\u68c0\u6d4b\u8d28\u91cf\u81ea\u52a8\u8bc4\u4f30\u7684\u95ee\u9898\u3002", "motivation": "\u5c06\u5de5\u4e1a\u56fe\u8f6c\u6362\u4e3a\u6570\u5b57\u5f62\u5f0f\u5bf9\u4e8e\u6784\u5efa\u6570\u5b57\u5b6a\u751f\u548c\u5b9e\u73b0\u667a\u80fd\u5de5\u4e1a\u81ea\u52a8\u5316\u81f3\u5173\u91cd\u8981\uff0c\u800c\u5bf9\u8c61\u68c0\u6d4b\u662f\u6570\u5b57\u5316\u8fc7\u7a0b\u4e2d\u7684\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\u3002\u76ee\u524d\u7f3a\u4e4f\u81ea\u52a8\u8bc4\u4f30\u5bf9\u8c61\u68c0\u6d4b\u7ed3\u679c\u8d28\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6765\u8bc4\u4f30\u5bf9\u8c61\u68c0\u6d4b\u7ed3\u679c\uff0c\u8bc6\u522b\u7f3a\u5931\u6216\u4e0d\u4e00\u81f4\u7684\u68c0\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u81ea\u52a8\u8bc4\u4f30\u8d28\u91cf\u5e76\u63d0\u9ad8\u590d\u6742\u5de5\u4e1a\u56fe\u7684\u6574\u4f53\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5229\u7528VLM\u7684\u591a\u6a21\u6001\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8c61\u68c0\u6d4b\u7ed3\u679c\u7684\u81ea\u52a8\u8d28\u91cf\u8bc4\u4f30\u548c\u6539\u8fdb\u3002"}}
{"id": "2510.03263", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03263", "abs": "https://arxiv.org/abs/2510.03263", "authors": ["Agnieszka Polowczyk", "Alicja Polowczyk", "Joanna Waczy\u0144ska", "Piotr Borycki", "Przemys\u0142aw Spurek"], "title": "Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models", "comment": null, "summary": "The impressive capability of modern text-to-image models to generate\nrealistic visuals has come with a serious drawback: they can be misused to\ncreate harmful, deceptive or unlawful content. This has accelerated the push\nfor machine unlearning. This new field seeks to selectively remove specific\nknowledge from a model's training data without causing a drop in its overall\nperformance. However, it turns out that actually forgetting a given concept is\nan extremely difficult task. Models exposed to attacks using adversarial\nprompts show the ability to generate so-called unlearned concepts, which can be\nnot only harmful but also illegal. In this paper, we present considerations\nregarding the ability of models to forget and recall knowledge, introducing the\nMemory Self-Regeneration task. Furthermore, we present MemoRa strategy, which\nwe consider to be a regenerative approach supporting the effective recovery of\npreviously lost knowledge. Moreover, we propose that robustness in knowledge\nretrieval is a crucial yet underexplored evaluation measure for developing more\nrobust and effective unlearning techniques. Finally, we demonstrate that\nforgetting occurs in two distinct ways: short-term, where concepts can be\nquickly recalled, and long-term, where recovery is more challenging.", "AI": {"tldr": "\u73b0\u4ee3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u903c\u771f\u7684\u89c6\u89c9\u6548\u679c\uff0c\u4f46\u4e5f\u53ef\u80fd\u88ab\u6ee5\u7528\u4ee5\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u3002\u56e0\u6b64\uff0c\u673a\u5668\u5b66\u4e60\u9886\u57df\u5f00\u59cb\u7814\u7a76\u5982\u4f55\u6709\u9009\u62e9\u5730\u4ece\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u4e2d\u5220\u9664\u7279\u5b9a\u77e5\u8bc6\uff0c\u800c\u4e0d\u4f1a\u5bfc\u81f4\u5176\u6574\u4f53\u6027\u80fd\u4e0b\u964d\u3002\u7136\u800c\uff0c\u4e8b\u5b9e\u8bc1\u660e\uff0c\u771f\u6b63\u5fd8\u8bb0\u4e00\u4e2a\u7ed9\u5b9a\u7684\u6982\u5ff5\u662f\u4e00\u9879\u6781\u5176\u56f0\u96be\u7684\u4efb\u52a1\u3002\u4f7f\u7528\u5bf9\u6297\u6027\u63d0\u793a\u7684\u6a21\u578b\u663e\u793a\u51fa\u751f\u6210\u6240\u8c13\u672a\u5b66\u4e60\u6982\u5ff5\u7684\u80fd\u529b\uff0c\u8fd9\u4e9b\u6982\u5ff5\u53ef\u80fd\u4e0d\u4ec5\u6709\u5bb3\uff0c\u800c\u4e14\u662f\u975e\u6cd5\u7684\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u5173\u4e8e\u6a21\u578b\u5fd8\u8bb0\u548c\u56de\u5fc6\u77e5\u8bc6\u7684\u80fd\u529b\u7684\u8003\u8651\uff0c\u4ecb\u7ecd\u4e86\u8bb0\u5fc6\u81ea\u6211\u518d\u751f\u4efb\u52a1\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86MemoRa\u7b56\u7565\uff0c\u6211\u4eec\u8ba4\u4e3a\u8fd9\u662f\u4e00\u79cd\u652f\u6301\u6709\u6548\u6062\u590d\u5148\u524d\u4e22\u5931\u77e5\u8bc6\u7684\u518d\u751f\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u77e5\u8bc6\u68c0\u7d22\u4e2d\u7684\u9c81\u68d2\u6027\u662f\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u6709\u6548\u7684\u975e\u5b66\u4e60\u6280\u672f\u7684\u5173\u952e\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u8bc4\u4f30\u63aa\u65bd\u3002\u6700\u540e\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u9057\u5fd8\u4ee5\u4e24\u79cd\u4e0d\u540c\u7684\u65b9\u5f0f\u53d1\u751f\uff1a\u77ed\u671f\u9057\u5fd8\uff0c\u6982\u5ff5\u53ef\u4ee5\u5feb\u901f\u56de\u5fc6\uff1b\u957f\u671f\u9057\u5fd8\uff0c\u6062\u590d\u66f4\u5177\u6311\u6218\u6027\u3002", "motivation": "\u7531\u4e8e\u73b0\u4ee3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u53ef\u80fd\u88ab\u6ee5\u7528\u4ee5\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u6709\u9009\u62e9\u5730\u4ece\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u4e2d\u5220\u9664\u7279\u5b9a\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e86MemoRa\u7b56\u7565\uff0c\u8fd9\u662f\u4e00\u79cd\u652f\u6301\u6709\u6548\u6062\u590d\u5148\u524d\u4e22\u5931\u77e5\u8bc6\u7684\u518d\u751f\u65b9\u6cd5, \u4ecb\u7ecd\u4e86\u8bb0\u5fc6\u81ea\u6211\u518d\u751f\u4efb\u52a1\u3002", "result": "\u8bc1\u660e\u4e86\u9057\u5fd8\u4ee5\u4e24\u79cd\u4e0d\u540c\u7684\u65b9\u5f0f\u53d1\u751f\uff1a\u77ed\u671f\u9057\u5fd8\u548c\u957f\u671f\u9057\u5fd8\u3002", "conclusion": "\u77e5\u8bc6\u68c0\u7d22\u4e2d\u7684\u9c81\u68d2\u6027\u662f\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u6709\u6548\u7684\u975e\u5b66\u4e60\u6280\u672f\u7684\u5173\u952e\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u8bc4\u4f30\u63aa\u65bd\u3002\u6a21\u578b\u5fd8\u8bb0\u548c\u56de\u5fc6\u77e5\u8bc6\u7684\u80fd\u529b\u9700\u8981\u88ab\u8003\u8651\u3002"}}
{"id": "2510.03847", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03847", "abs": "https://arxiv.org/abs/2510.03847", "authors": ["Raghav Sharma", "Manan Mehta"], "title": "Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs", "comment": "9 Pages", "summary": "Small language models (SLMs; 1-12B params, sometimes up to 20B) are\nsufficient and often superior for agentic workloads where the objective is\nschema- and API-constrained accuracy rather than open-ended generation. We\nsynthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,\nQwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,\nDeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,\nStableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with\nguided decoding libraries (XGrammar, Outlines). We formalize SLM-default,\nLLM-fallback systems with uncertainty-aware routing and verifier cascades, and\npropose engineering metrics that reflect real production goals: cost per\nsuccessful task (CPS), schema validity rate, executable call rate, p50/p95\nlatency, and energy per request. Guided decoding, strict JSON Schema outputs,\nand validator-first tool execution close much of the capability gap with larger\nmodels and often let SLMs match or surpass LLMs on tool use, function calling,\nand RAG at 10x-100x lower token cost with materially better latency and energy.\nWe provide design patterns for agent stacks that prioritize SLMs: schema-first\nprompting, type-safe function registries, confidence scoring with verifier\nrollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits\nwhere fallback remains valuable (open-domain reasoning and some long-horizon\nplanning). The result is a practical blueprint for building fast, inexpensive,\nand reliable agents that default to SLMs while preserving headroom with\ntargeted LLM assistance.\n  Keywords: small language models, agents, function calling, structured\noutputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,\nedge inference", "AI": {"tldr": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u5728\u9700\u8981\u6a21\u5f0f\u548c API \u7ea6\u675f\u7684\u4ee3\u7406\u4efb\u52a1\u4e2d\u901a\u5e38\u4f18\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002", "motivation": "\u63a2\u7d22\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7406\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u5728\u5de5\u5177\u4f7f\u7528\u3001\u51fd\u6570\u8c03\u7528\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u9762\u3002", "method": "\u7efc\u5408\u5206\u6790\u4e86\u5404\u79cd\u5f00\u6e90\u548c\u5546\u4e1a SLM \u7684\u6700\u65b0\u8bc1\u636e\uff0c\u5e76\u5c06\u5176\u4e0e\u73b0\u4ee3\u8bc4\u4f30\u65b9\u6cd5\u548cserving stacks\u76f8\u7ed3\u5408\u3002\u63d0\u51fa\u4e86SLM-default\uff0cLLM-fallback\u7cfb\u7edf\uff0c\u5e76\u63d0\u51fa\u4e86\u53cd\u6620\u5b9e\u9645\u751f\u4ea7\u76ee\u6807\u7684\u5de5\u7a0b\u6307\u6807\u3002", "result": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u5f15\u5bfc\u89e3\u7801\u3001\u4e25\u683c\u7684 JSON \u6a21\u5f0f\u8f93\u51fa\u548c\u9a8c\u8bc1\u5668\u4f18\u5148\u7684\u5de5\u5177\u6267\u884c\uff0c\u5728\u5de5\u5177\u4f7f\u7528\u3001\u51fd\u6570\u8c03\u7528\u548c RAG \u65b9\u9762\u53ef\u4ee5\u5339\u914d\u751a\u81f3\u8d85\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u540c\u65f6\u964d\u4f4e\u4e86 10-100 \u500d\u7684 token \u6210\u672c\uff0c\u5e76\u663e\u8457\u6539\u5584\u4e86\u5ef6\u8fdf\u548c\u80fd\u8017\u3002", "conclusion": "\u672c\u6587\u4e3a\u6784\u5efa\u5feb\u901f\u3001\u4f4e\u6210\u672c\u4e14\u53ef\u9760\u7684\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u84dd\u56fe\uff0c\u8be5\u4ee3\u7406\u9ed8\u8ba4\u4f7f\u7528 SLM\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684 LLM \u8f85\u52a9\u5b9e\u73b0\u7684 headroom\u3002"}}
{"id": "2510.03639", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03639", "abs": "https://arxiv.org/abs/2510.03639", "authors": ["Liming Wang", "Junrui Ni", "Kai-Wei Chang", "Saurabhchand Bhati", "David Harwath", "Mark Hasegawa-Johnson", "James R. Glass"], "title": "Towards Unsupervised Speech Recognition at the Syllable-Level", "comment": null, "summary": "Training speech recognizers with unpaired speech and text -- known as\nunsupervised speech recognition (UASR) -- is a crucial step toward extending\nASR to low-resource languages in the long-tail distribution and enabling\nmultimodal learning from non-parallel data. However, existing approaches based\non phones often rely on costly resources such as grapheme-to-phoneme converters\n(G2Ps) and struggle to generalize to languages with ambiguous phoneme\nboundaries due to training instability. In this paper, we address both\nchallenges by introducing a syllable-level UASR framework based on masked\nlanguage modeling, which avoids the need for G2P and the instability of\nGAN-based methods. Our approach achieves up to a 40\\% relative reduction in\ncharacter error rate (CER) on LibriSpeech and generalizes effectively to\nMandarin, a language that has remained particularly difficult for prior\nmethods. Code will be released upon acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u7684\u97f3\u8282\u7ea7\u65e0\u76d1\u7763\u8bed\u97f3\u8bc6\u522b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u65e0\u9700\u5b57\u5f62-\u97f3\u7d20\u8f6c\u6362\u5668\uff0c\u4e14\u907f\u514d\u4e86\u57fa\u4e8eGAN\u7684\u65b9\u6cd5\u7684\u4e0d\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u97f3\u7d20\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u8d44\u6e90\uff0c\u4f8b\u5982\u5b57\u5f62-\u97f3\u7d20\u8f6c\u6362\u5668\uff08G2P\uff09\uff0c\u5e76\u4e14\u7531\u4e8e\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u96be\u4ee5\u63a8\u5e7f\u5230\u5177\u6709\u6a21\u7cca\u97f3\u7d20\u8fb9\u754c\u7684\u8bed\u8a00\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e24\u4e2a\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u7684\u97f3\u8282\u7ea7UASR\u6846\u67b6\u3002", "result": "\u5728LibriSpeech\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8fbe40%\u7684\u5b57\u7b26\u9519\u8bef\u7387\uff08CER\uff09\u7684\u76f8\u5bf9\u964d\u4f4e\uff0c\u5e76\u4e14\u6709\u6548\u5730\u63a8\u5e7f\u5230\u666e\u901a\u8bdd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728LibriSpeech\u548c\u666e\u901a\u8bdd\u4e0a\u7684\u7ed3\u679c\u8868\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.03441", "categories": ["cs.CV", "cs.AI", "cs.LG", "68T45, 68T10, 68T40"], "pdf": "https://arxiv.org/pdf/2510.03441", "abs": "https://arxiv.org/abs/2510.03441", "authors": ["Chashi Mahiul Islam", "Oteo Mamo", "Samuel Jacob Chacko", "Xiuwen Liu", "Weikuan Yu"], "title": "Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning", "comment": "12 pages, 5 figures", "summary": "Vision-language models (VLMs) have advanced multimodal reasoning but still\nface challenges in spatial reasoning for 3D scenes and complex object\nconfigurations. To address this, we introduce SpatialViLT, an enhanced VLM that\nintegrates spatial features like depth maps, 3D coordinates, and edge maps\nthrough a multi-task learning framework. This approach enriches multimodal\nembeddings with spatial understanding. We propose two variants: SpatialViLT and\nMaskedSpatialViLT, focusing on full and masked object regions, respectively.\nAdditionally, SpatialEnsemble combines both approaches, achieving\nstate-of-the-art accuracy. Our models excel in spatial reasoning categories\nsuch as directional, topological, and proximity relations, as demonstrated on\nthe challenging Visual Spatial Reasoning (VSR) dataset. This work represents a\nsignificant step in enhancing the spatial intelligence of AI systems, crucial\nfor advanced multimodal understanding and real-world applications.", "AI": {"tldr": "SpatialViLT: A VLM enhanced with spatial features (depth, coordinates, edges) for improved 3D scene spatial reasoning.", "motivation": "VLMs struggle with spatial reasoning in 3D scenes and complex object arrangements.", "method": "Introduces SpatialViLT, a VLM integrating spatial features via multi-task learning, with SpatialViLT and MaskedSpatialViLT variants, and a SpatialEnsemble combining both.", "result": "Achieves state-of-the-art accuracy on the Visual Spatial Reasoning (VSR) dataset, excelling in directional, topological, and proximity relations.", "conclusion": "Significantly enhances the spatial intelligence of AI systems, crucial for advanced multimodal understanding and real-world applications."}}
{"id": "2510.03264", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03264", "abs": "https://arxiv.org/abs/2510.03264", "authors": ["Syeda Nahida Akter", "Shrimai Prabhumoye", "Eric Nyberg", "Mostofa Patwary", "Mohammad Shoeybi", "Yejin Choi", "Bryan Catanzaro"], "title": "Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data", "comment": null, "summary": "The prevailing paradigm for enhancing the reasoning abilities of LLMs\nrevolves around post-training on high-quality, reasoning-intensive data. While\nemerging literature suggests that reasoning data is increasingly incorporated\nalso during the mid-training stage-a practice that is relatively more\nproprietary and less openly characterized-the role of such data in pretraining\nremains unclear. In particular, due to the opaqueness of pretraining corpora in\nmost frontier models, the effect of reasoning data introduced at different\nphases of pre- and/or post-training is relatively less reported in the\nscientific literature. This raises several important questions: Is adding\nreasoning data earlier during pretraining any better than introducing it during\npost-training? Could earlier inclusion risk overfitting and harm\ngeneralization, or instead establish durable foundations that later fine-tuning\ncannot recover? We conduct the first systematic study of how reasoning\ndata-varying in scale, diversity, and quality-affects LLM performance when\nintroduced at different stages of training. We find that front-loading\nreasoning data into pretraining is critical (19% avg gain), establishing\nfoundational capabilities that cannot be fully replicated by later-stage SFT,\neven with more data. We uncover an asymmetric principle for optimal data\nallocation: pretraining benefits most from broad diversity in reasoning\npatterns (11% avg gain), while SFT is more sensitive to data quality (15% avg\ngain). We show that high-quality pretraining data has latent effects, activated\nonly after SFT, and that naively scaling SFT data can be detrimental, washing\naway the benefits of early reasoning injection. Our results challenge the\nconventional separation of language modeling and reasoning, providing a\nprincipled guide for strategically allocating data across the entire training\npipeline to build more capable models.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728LLM\u8bad\u7ec3\u7684\u4e0d\u540c\u9636\u6bb5\u5f15\u5165\u63a8\u7406\u6570\u636e\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u524d\u671f\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u63a8\u7406\u6570\u636e\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u5f53\u524d\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5728\u9ad8\u8d28\u91cf\u3001\u63a8\u7406\u5bc6\u96c6\u578b\u6570\u636e\u4e0a\u8fdb\u884c\u540e\u8bad\u7ec3\uff0c\u800c\u9884\u8bad\u7ec3\u9636\u6bb5\u63a8\u7406\u6570\u636e\u7684\u4f5c\u7528\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7814\u7a76\u4e0d\u540c\u89c4\u6a21\u3001\u591a\u6837\u6027\u548c\u8d28\u91cf\u7684\u63a8\u7406\u6570\u636e\u5728LLM\u8bad\u7ec3\u4e0d\u540c\u9636\u6bb5\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u63a8\u7406\u6570\u636e\u81f3\u5173\u91cd\u8981\uff08\u5e73\u5747\u63d0\u534719%\uff09\uff0c\u4e14\u9884\u8bad\u7ec3\u9636\u6bb5\u53d7\u76ca\u4e8e\u63a8\u7406\u6a21\u5f0f\u7684\u5e7f\u6cdb\u591a\u6837\u6027\uff08\u5e73\u5747\u63d0\u534711%\uff09\uff0c\u800c\u540e\u8bad\u7ec3\u9636\u6bb5\u5bf9\u6570\u636e\u8d28\u91cf\u66f4\u654f\u611f\uff08\u5e73\u5747\u63d0\u534715%\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8bed\u8a00\u5efa\u6a21\u548c\u63a8\u7406\u7684\u4f20\u7edf\u5206\u79bb\u5177\u6709\u6311\u6218\u6027\uff0c\u5e76\u4e3a\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6218\u7565\u6027\u5730\u5206\u914d\u6570\u636e\u4ee5\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6307\u5bfc\u3002"}}
{"id": "2510.03851", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03851", "abs": "https://arxiv.org/abs/2510.03851", "authors": ["Ruiying Ma", "Chieh-Jan Mike Liang", "Yanjie Gao", "Francis Y. Yan"], "title": "Algorithm Generation via Creative Ideation", "comment": null, "summary": "Designing system algorithms remains challenging, where the discontinuous\nnature of the solution space often forces system engineers to rely on generic\nheuristics at the expense of performance. We study whether LLMs can practically\ndrive algorithm generation, and find that they are biased towards well-known\ngeneric designs, rather than making the creative leaps needed to navigate the\ndiscontinuous solution space. To address this limitation, we introduce\nMetaMuse, a framework for creative ideation built on three self-reflection\nprinciples: (1) quantifying solution diversity and usefulness in measurable\nperformance space, rather than abstract idea space, (2) steering ideation\nthrough external stimuli, rather than internal randomness, and (3) constructing\nexecutable solutions using waypoint reasoning, rather than free-form\nchain-of-thought. Extensive evaluation shows that MetaMuse can generate\nhigh-performing solutions for two critical problems at a global cloud provider:\ncache replacement (reducing cache misses by up to 35.76%) and online bin\npacking (reducing bin usage by up to 30.93%).", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u7b97\u6cd5\u751f\u6210\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u503e\u5411\u4e8e\u5df2\u77e5\u7684\u901a\u7528\u8bbe\u8ba1\u3002", "motivation": "\u7cfb\u7edf\u7b97\u6cd5\u8bbe\u8ba1\u5177\u6709\u6311\u6218\u6027\uff0c\u89e3\u7a7a\u95f4\u7684\u4e0d\u8fde\u7eed\u6027\u5bfc\u81f4\u5de5\u7a0b\u5e08\u4f9d\u8d56\u901a\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u727a\u7272\u4e86\u6027\u80fd\u3002\u7814\u7a76LLM\u662f\u5426\u80fd\u9a71\u52a8\u7b97\u6cd5\u751f\u6210\u3002", "method": "\u63d0\u51fa\u4e86MetaMuse\u6846\u67b6\uff0c\u57fa\u4e8e\u4e09\u4e2a\u81ea\u53cd\u601d\u539f\u5219\uff1a(1)\u5728\u53ef\u6d4b\u91cf\u7684\u6027\u80fd\u7a7a\u95f4\u4e2d\u91cf\u5316\u89e3\u51b3\u65b9\u6848\u7684\u591a\u6837\u6027\u548c\u6709\u6548\u6027\uff0c(2)\u901a\u8fc7\u5916\u90e8\u523a\u6fc0\u5f15\u5bfc\u6784\u601d\uff0c(3)\u4f7f\u7528\u822a\u8def\u70b9\u63a8\u7406\u6784\u5efa\u53ef\u6267\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "MetaMuse\u5728\u7f13\u5b58\u66ff\u6362\uff08\u51cf\u5c11\u9ad8\u8fbe35.76%\u7684\u7f13\u5b58\u672a\u547d\u4e2d\uff09\u548c\u5728\u7ebf\u88c5\u7bb1\uff08\u51cf\u5c11\u9ad8\u8fbe30.93%\u7684\u7bb1\u5b50\u4f7f\u7528\uff09\u4e24\u4e2a\u5173\u952e\u95ee\u9898\u4e0a\u751f\u6210\u4e86\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "MetaMuse\u80fd\u591f\u4e3a\u5168\u5c40\u4e91\u63d0\u4f9b\u5546\u7684\u4e24\u4e2a\u5173\u952e\u95ee\u9898\u751f\u6210\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03663", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03663", "abs": "https://arxiv.org/abs/2510.03663", "authors": ["Xiangyu Peng", "Cab Qin", "Zeyuan Chen", "Ran Xu", "Caiming Xiong", "Chien-Sheng Wu"], "title": "UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG", "comment": null, "summary": "Multimodal retrieval-augmented generation (MM-RAG) is a key approach for\napplying large language models (LLMs) and agents to real-world knowledge bases,\nyet current evaluations are fragmented, focusing on either text or images in\nisolation or on simplified multimodal setups that fail to capture\ndocument-centric multimodal use cases. In this paper, we introduce\nUniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from\n70k real-world PDF pages across eight domains. Our pipeline extracts and links\nevidence from text, tables, and figures, then generates 1,600 multimodal QA\npairs spanning factual retrieval, comparison, summarization, and logical\nreasoning queries. To ensure reliability, 20% of QA pairs are validated by\nmultiple annotators and expert adjudication. UniDoc-Bench supports\napples-to-apples comparison across four paradigms: (1) text-only, (2)\nimage-only, (3) multimodal text-image fusion, and (4) multimodal joint\nretrieval -- under a unified protocol with standardized candidate pools,\nprompts, and evaluation metrics. Our experiments show that multimodal\ntext-image fusion RAG systems consistently outperform both unimodal and jointly\nmultimodal embedding-based retrieval, indicating that neither text nor images\nalone are sufficient and that current multimodal embeddings remain inadequate.\nBeyond benchmarking, our analysis reveals when and how visual context\ncomplements textual evidence, uncovers systematic failure modes, and offers\nactionable guidance for developing more robust MM-RAG pipelines.", "AI": {"tldr": "\u63d0\u51faUniDoc-Bench\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u771f\u5b9e\u7684MM-RAG\u57fa\u51c6\uff0c\u5305\u542b70k\u771f\u5b9ePDF\u9875\u9762\uff0c1600\u4e2a\u591a\u6a21\u6001QA\u5bf9\uff0c\u6db5\u76d6\u4e8b\u5b9e\u68c0\u7d22\u3001\u6bd4\u8f83\u3001\u603b\u7ed3\u548c\u903b\u8f91\u63a8\u7406\u67e5\u8be2\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210(MM-RAG)\u8bc4\u4f30\u662f\u5206\u6563\u7684\uff0c\u96c6\u4e2d\u5728\u5b64\u7acb\u7684\u6587\u672c\u6216\u56fe\u50cf\u4e0a\uff0c\u6216\u7b80\u5316\u7684\u591a\u6a21\u6001\u8bbe\u7f6e\uff0c\u672a\u80fd\u6355\u6349\u5230\u4ee5\u6587\u6863\u4e3a\u4e2d\u5fc3\u7684\u591a\u6a21\u6001\u7528\u4f8b\u3002", "method": "\u6784\u5efaUniDoc-Bench\u57fa\u51c6\uff0c\u4ece\u771f\u5b9ePDF\u9875\u9762\u4e2d\u63d0\u53d6\u5e76\u94fe\u63a5\u6587\u672c\u3001\u8868\u683c\u548c\u56fe\u5f62\u4e2d\u7684\u8bc1\u636e\uff0c\u7136\u540e\u751f\u6210\u591a\u6a21\u6001QA\u5bf9\u3002\u91c7\u7528\u7edf\u4e00\u534f\u8bae\uff0c\u6807\u51c6\u5316\u5019\u9009\u6c60\u3001\u63d0\u793a\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u652f\u6301\u56db\u79cd\u8303\u5f0f\u7684\u6bd4\u8f83\uff1a\u6587\u672c\u3001\u56fe\u50cf\u3001\u591a\u6a21\u6001\u6587\u672c\u56fe\u50cf\u878d\u5408\u548c\u591a\u6a21\u6001\u8054\u5408\u68c0\u7d22\u3002", "result": "\u591a\u6a21\u6001\u6587\u672c\u56fe\u50cf\u878d\u5408RAG\u7cfb\u7edf\u59cb\u7ec8\u4f18\u4e8e\u5355\u6a21\u6001\u548c\u8054\u5408\u591a\u6a21\u6001\u5d4c\u5165\u68c0\u7d22\uff0c\u8868\u660e\u5355\u72ec\u7684\u6587\u672c\u6216\u56fe\u50cf\u90fd\u4e0d\u8db3\uff0c\u5e76\u4e14\u5f53\u524d\u7684\u591a\u6a21\u6001\u5d4c\u5165\u4ecd\u7136\u4e0d\u8db3\u3002", "conclusion": "\u5206\u6790\u63ed\u793a\u4e86\u89c6\u89c9\u4e0a\u4e0b\u6587\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u8865\u5145\u6587\u672c\u8bc1\u636e\uff0c\u63ed\u793a\u4e86\u7cfb\u7edf\u6027\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684MM-RAG\u7ba1\u9053\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u3002"}}
{"id": "2510.03452", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03452", "abs": "https://arxiv.org/abs/2510.03452", "authors": ["Allison Davis", "Yezhi Shen", "Xiaoyu Ji", "Fengqing Zhu"], "title": "Denoising of Two-Phase Optically Sectioned Structured Illumination Reconstructions Using Encoder-Decoder Networks", "comment": "5 pages, 4 figures, submitted to ICASSP 2026", "summary": "Structured illumination (SI) enhances image resolution and contrast by\nprojecting patterned light onto a sample. In two-phase optical-sectioning SI\n(OS-SI), reduced acquisition time introduces residual artifacts that\nconventional denoising struggles to suppress. Deep learning offers an\nalternative to traditional methods; however, supervised training is limited by\nthe lack of clean, optically sectioned ground-truth data. We investigate\nencoder-decoder networks for artifact reduction in two-phase OS-SI, using\nsynthetic training pairs formed by applying real artifact fields to synthetic\nimages. An asymmetrical denoising autoencoder (DAE) and a U-Net are trained on\nthe synthetic data, then evaluated on real OS-SI images. Both networks improve\nimage clarity, with each excelling against different artifact types. These\nresults demonstrate that synthetic training enables supervised denoising of\nOS-SI images and highlight the potential of encoder-decoder networks to\nstreamline reconstruction workflows.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e24\u76f8\u5149\u5b66\u5207\u7247\u7ed3\u6784\u5149\u7167\uff08OS-SI\uff09\u4f2a\u5f71\u6d88\u9664\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u7f3a\u4e4f\u5e72\u51c0\u7684ground-truth\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u53bb\u566a\u65b9\u6cd5\u96be\u4ee5\u6291\u5236\u4e24\u76f8OS-SI\u4e2d\u56e0\u7f29\u77ed\u91c7\u96c6\u65f6\u95f4\u800c\u5f15\u5165\u7684\u6b8b\u4f59\u4f2a\u5f71\u3002\u76d1\u7763\u8bad\u7ec3\u53d7\u9650\u4e8e\u7f3a\u4e4f\u5e72\u51c0\u7684\u5149\u5b66\u5207\u7247ground-truth\u6570\u636e\u3002", "method": "\u4f7f\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\uff0c\u5305\u62ec\u975e\u5bf9\u79f0\u53bb\u566a\u81ea\u7f16\u7801\u5668\uff08DAE\uff09\u548cU-Net\uff0c\u5728\u901a\u8fc7\u5c06\u771f\u5b9e\u4f2a\u5f71\u573a\u5e94\u7528\u4e8e\u5408\u6210\u56fe\u50cf\u800c\u5f62\u6210\u7684\u5408\u6210\u8bad\u7ec3\u5bf9\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u4e24\u79cd\u7f51\u7edc\u90fd\u63d0\u9ad8\u4e86\u56fe\u50cf\u6e05\u6670\u5ea6\uff0c\u5e76\u4e14\u6bcf\u79cd\u7f51\u7edc\u5728\u4e0d\u540c\u7c7b\u578b\u7684\u4f2a\u5f71\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u5408\u6210\u8bad\u7ec3\u80fd\u591f\u5bf9OS-SI\u56fe\u50cf\u8fdb\u884c\u76d1\u7763\u53bb\u566a\uff0c\u5e76\u7a81\u51fa\u4e86\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\u5728\u7b80\u5316\u91cd\u5efa\u5de5\u4f5c\u6d41\u7a0b\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.03265", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03265", "abs": "https://arxiv.org/abs/2510.03265", "authors": ["Bowei Tian", "Yexiao He", "Wanghao Ye", "Ziyao Wang", "Meng Liu", "Ang Li"], "title": "MindCraft: How Concept Trees Take Shape In Deep Models", "comment": null, "summary": "Large-scale foundation models demonstrate strong performance across language,\nvision, and reasoning tasks. However, how they internally structure and\nstabilize concepts remains elusive. Inspired by causal inference, we introduce\nthe MindCraft framework built upon Concept Trees. By applying spectral\ndecomposition at each layer and linking principal directions into branching\nConcept Paths, Concept Trees reconstruct the hierarchical emergence of\nconcepts, revealing exactly when they diverge from shared representations into\nlinearly separable subspaces. Empirical evaluations across diverse scenarios\nacross disciplines, including medical diagnosis, physics reasoning, and\npolitical decision-making, show that Concept Trees recover semantic\nhierarchies, disentangle latent concepts, and can be widely applied across\nmultiple domains. The Concept Tree establishes a widely applicable and powerful\nframework that enables in-depth analysis of conceptual representations in deep\nmodels, marking a significant step forward in the foundation of interpretable\nAI.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3a MindCraft \u7684\u6846\u67b6\uff0c\u5b83\u57fa\u4e8e\u6982\u5ff5\u6811\uff0c\u65e8\u5728\u5206\u6790\u5927\u578b\u57fa\u7840\u6a21\u578b\u5185\u90e8\u5982\u4f55\u6784\u5efa\u548c\u7a33\u5b9a\u6982\u5ff5\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u57fa\u7840\u6a21\u578b\u5982\u4f55\u5728\u5176\u5185\u90e8\u6784\u5efa\u548c\u7a33\u5b9a\u6982\u5ff5\u3002", "method": "\u901a\u8fc7\u5728\u6bcf\u4e00\u5c42\u5e94\u7528\u8c31\u5206\u89e3\uff0c\u5e76\u5c06\u4e3b\u65b9\u5411\u8fde\u63a5\u6210\u6982\u5ff5\u8def\u5f84\uff0c\u6982\u5ff5\u6811\u91cd\u6784\u4e86\u6982\u5ff5\u7684\u5c42\u6b21\u7ed3\u6784\u3002", "result": "\u6982\u5ff5\u6811\u80fd\u591f\u6062\u590d\u8bed\u4e49\u5c42\u6b21\u7ed3\u6784\uff0c\u89e3\u8026\u6f5c\u5728\u6982\u5ff5\uff0c\u5e76\u5e7f\u6cdb\u5e94\u7528\u4e8e\u591a\u4e2a\u9886\u57df\u3002", "conclusion": "\u6982\u5ff5\u6811\u5efa\u7acb\u4e86\u4e00\u4e2a\u5e7f\u6cdb\u9002\u7528\u4e14\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6df1\u5165\u5206\u6790\u6df1\u5ea6\u6a21\u578b\u4e2d\u7684\u6982\u5ff5\u8868\u793a\uff0c\u6807\u5fd7\u7740\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u57fa\u7840\u65b9\u9762\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2510.03859", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03859", "abs": "https://arxiv.org/abs/2510.03859", "authors": ["Raghav Sharma", "Manan Mehta"], "title": "Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning", "comment": "22 pages", "summary": "Ensuring that critical IoT systems function safely and smoothly depends a lot\non finding anomalies quickly. As more complex systems, like smart healthcare,\nenergy grids and industrial automation, appear, it is easier to see the\nshortcomings of older methods of detection. Monitoring failures usually happen\nin dynamic, high dimensional situations, especially when data is incomplete,\nmessy or always evolving. Such limits point out the requirement for adaptive,\nintelligent systems that always improve and think. LLMs are now capable of\nsignificantly changing how context is understood and semantic inference is done\nacross all types of data. This proposal suggests using an LLM supported\ncontextual reasoning method along with XAI agents to improve how anomalies are\nfound in significant IoT environments. To discover hidden patterns and notice\ninconsistencies in data streams, it uses attention methods, avoids dealing with\ndetails from every time step and uses memory buffers with meaning. Because no\ncode AI stresses transparency and interpretability, people can check and accept\nthe AI's decisions, helping ensure AI follows company policies. The two\narchitectures are put together in a test that compares the results of the\ntraditional model with those of the suggested LLM enhanced model. Important\nmeasures to check are the accuracy of detection, how much inaccurate\ninformation is included in the results, how clearly the findings can be read\nand how fast the system responds under different test situations. The\nmetaheuristic is tested in simulations of real world smart grid and healthcare\ncontexts to check its adaptability and reliability. From the study, we see that\nthe new approach performs much better than most existing models in both\naccuracy and interpretation, so it could be a good fit for future anomaly\ndetection tasks in IoT", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528LLM\u548cXAI\u4ee3\u7406\u6765\u6539\u8fdb\u7269\u8054\u7f51\u73af\u5883\u4e2d\u5f02\u5e38\u68c0\u6d4b\u7684\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u9ad8\u7ef4\u3001\u52a8\u6001\u548c\u6570\u636e\u4e0d\u5b8c\u6574\u7684\u73af\u5883\u4e2d\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u5229\u7528LLM\u652f\u6301\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u65b9\u6cd5\uff0c\u7ed3\u5408XAI\u4ee3\u7406\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u548c\u8bb0\u5fc6\u7f13\u51b2\u533a\u53d1\u73b0\u6570\u636e\u6d41\u4e2d\u7684\u9690\u85cf\u6a21\u5f0f\u548c\u4e0d\u4e00\u81f4\u6027\u3002", "result": "\u63d0\u51fa\u7684\u65b0\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u672a\u6765\u7269\u8054\u7f51\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2510.03683", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03683", "abs": "https://arxiv.org/abs/2510.03683", "authors": ["Nisar Hussain", "Amna Qasim", "Gull Mehak", "Muhammad Zain", "Momina Hafeez", "Grigori Sidorov"], "title": "Fine-Tuning Large Language Models with QLoRA for Offensive Language Detection in Roman Urdu-English Code-Mixed Text", "comment": "25 pages, 22 figures", "summary": "The use of derogatory terms in languages that employ code mixing, such as\nRoman Urdu, presents challenges for Natural Language Processing systems due to\nunstated grammar, inconsistent spelling, and a scarcity of labeled data. In\nthis work, we propose a QLoRA based fine tuning framework to improve offensive\nlanguage detection in Roman Urdu-English text. We translated the Roman\nUrdu-English code mixed dataset into English using Google Translate to leverage\nEnglish LLMs, while acknowledging that this translation reduces direct\nengagement with code mixing features. Our focus is on classification\nperformance using English translated low resource inputs. We fine tuned several\ntransformers and large language models, including Meta LLaMA 3 8B, Mistral 7B\nv0.1, LLaMA 2 7B, ModernBERT, and RoBERTa, with QLoRA for memory efficient\nadaptation. Models were trained and evaluated on a manually annotated Roman\nUrdu dataset for offensive vs non offensive content. Of all tested models, the\nhighest F1 score of 91.45 was attained by Meta LLaMA 3 8B, followed by Mistral\n7B at 89.66, surpassing traditional transformer baselines. These results\ndemonstrate the efficacy of QLoRA in fine tuning high performing models for low\nresource environments such as code mixed offensive language detection, and\nconfirm the potential of LLMs for this task. This work advances a scalable\napproach to Roman Urdu moderation and paves the way for future multilingual\noffensive detection systems based on LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eQLoRA\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8Roman Urdu-English\u6587\u672c\u4e2d\u5192\u72af\u6027\u8bed\u8a00\u7684\u68c0\u6d4b\u3002", "motivation": "\u5728\u6df7\u5408\u7f16\u7801\u7684\u8bed\u8a00\uff08\u5982Roman Urdu\uff09\u4e2d\u4f7f\u7528\u8d2c\u4e49\u8bcd\uff0c\u7531\u4e8e\u8bed\u6cd5\u4e0d\u660e\u786e\u3001\u62fc\u5199\u4e0d\u4e00\u81f4\u548c\u7f3a\u4e4f\u6807\u8bb0\u6570\u636e\uff0c\u7ed9\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7cfb\u7edf\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u8be5\u7814\u7a76\u4f7f\u7528\u8c37\u6b4c\u7ffb\u8bd1\u5c06Roman Urdu-English\u6df7\u5408\u7f16\u7801\u6570\u636e\u96c6\u7ffb\u8bd1\u6210\u82f1\u8bed\uff0c\u5e76\u4f7f\u7528QLoRA\u5fae\u8c03\u4e86\u591a\u4e2atransformers\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u62ecMeta LLaMA 3 8B\u3001Mistral 7B v0.1\u3001LLaMA 2 7B\u3001ModernBERT\u548cRoBERTa\u3002", "result": "Meta LLaMA 3 8B\u83b7\u5f97\u4e86\u6700\u9ad8\u7684F1\u5206\u657091.45\uff0c\u5176\u6b21\u662fMistral 7B\uff0c\u4e3a89.66\uff0c\u8d85\u8fc7\u4e86\u4f20\u7edf\u7684transformer\u57fa\u7ebf\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0cQLoRA\u5728\u5fae\u8c03\u9ad8\u6027\u80fd\u6a21\u578b\u4ee5\u7528\u4e8e\u6df7\u5408\u7f16\u7801\u5192\u72af\u6027\u8bed\u8a00\u68c0\u6d4b\u7b49\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u8bc1\u5b9e\u4e86LLM\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684Roman Urdu\u5ba1\u6838\u65b9\u6cd5\uff0c\u5e76\u4e3a\u672a\u6765\u57fa\u4e8eLLM\u7684\u591a\u8bed\u8a00\u5192\u72af\u6027\u68c0\u6d4b\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.03455", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03455", "abs": "https://arxiv.org/abs/2510.03455", "authors": ["Sejuti Majumder", "Saarthak Kapse", "Moinak Bhattacharya", "Xuan Xu", "Alisa Yurovsky", "Prateek Prasanna"], "title": "PEaRL: Pathway-Enhanced Representation Learning for Gene and Pathway Expression Prediction from Histology", "comment": null, "summary": "Integrating histopathology with spatial transcriptomics (ST) provides a\npowerful opportunity to link tissue morphology with molecular function. Yet\nmost existing multimodal approaches rely on a small set of highly variable\ngenes, which limits predictive scope and overlooks the coordinated biological\nprograms that shape tissue phenotypes. We present PEaRL (Pathway Enhanced\nRepresentation Learning), a multimodal framework that represents\ntranscriptomics through pathway activation scores computed with ssGSEA. By\nencoding biologically coherent pathway signals with a transformer and aligning\nthem with histology features via contrastive learning, PEaRL reduces\ndimensionality, improves interpretability, and strengthens cross-modal\ncorrespondence. Across three cancer ST datasets (breast, skin, and lymph node),\nPEaRL consistently outperforms SOTA methods, yielding higher accuracy for both\ngene- and pathway-level expression prediction (up to 58.9 percent and 20.4\npercent increase in Pearson correlation coefficient compared to SOTA). These\nresults demonstrate that grounding transcriptomic representation in pathways\nproduces more biologically faithful and interpretable multimodal models,\nadvancing computational pathology beyond gene-level embeddings.", "AI": {"tldr": "PEaRL: A multimodal framework integrating histopathology with spatial transcriptomics using pathway activation scores for improved interpretability and performance in cancer ST datasets.", "motivation": "Existing multimodal approaches rely on a small set of highly variable genes, limiting predictive scope and overlooking coordinated biological programs.", "method": "PEaRL represents transcriptomics through pathway activation scores computed with ssGSEA, encoding pathway signals with a transformer, and aligning them with histology features via contrastive learning.", "result": "PEaRL outperforms SOTA methods across three cancer ST datasets, yielding higher accuracy for gene- and pathway-level expression prediction.", "conclusion": "Grounding transcriptomic representation in pathways produces more biologically faithful and interpretable multimodal models, advancing computational pathology beyond gene-level embeddings."}}
{"id": "2510.03266", "categories": ["cs.LG", "stat.ME", "stat.OT"], "pdf": "https://arxiv.org/pdf/2510.03266", "abs": "https://arxiv.org/abs/2510.03266", "authors": ["Bharat Sharma", "Jitendra Kumar"], "title": "Variational Autoencoders-based Detection of Extremes in Plant Productivity in an Earth System Model", "comment": null, "summary": "Climate anomalies significantly impact terrestrial carbon cycle dynamics,\nnecessitating robust methods for detecting and analyzing anomalous behavior in\nplant productivity. This study presents a novel application of variational\nautoencoders (VAE) for identifying extreme events in gross primary productivity\n(GPP) from Community Earth System Model version 2 simulations across four AR6\nregions in the Continental United States. We compare VAE-based anomaly\ndetection with traditional singular spectral analysis (SSA) methods across\nthree time periods: 1850-80, 1950-80, and 2050-80 under the SSP585 scenario.\nThe VAE architecture employs three dense layers and a latent space with an\ninput sequence length of 12 months, trained on a normalized GPP time series to\nreconstruct the GPP and identifying anomalies based on reconstruction errors.\nExtreme events are defined using 5th percentile thresholds applied to both VAE\nand SSA anomalies. Results demonstrate strong regional agreement between VAE\nand SSA methods in spatial patterns of extreme event frequencies, despite VAE\nproducing higher threshold values (179-756 GgC for VAE vs. 100-784 GgC for SSA\nacross regions and periods). Both methods reveal increasing magnitudes and\nfrequencies of negative carbon cycle extremes toward 2050-80, particularly in\nWestern and Central North America. The VAE approach shows comparable\nperformance to established SSA techniques, while offering computational\nadvantages and enhanced capability for capturing non-linear temporal\ndependencies in carbon cycle variability. Unlike SSA, the VAE method does not\nrequire one to define the periodicity of the signals in the data; it discovers\nthem from the data.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u8bc6\u522b\u7f8e\u56fd\u5927\u9646\u56db\u4e2aAR6\u533a\u57df\u7684GPP\u6781\u7aef\u4e8b\u4ef6\uff0c\u5e76\u4e0e\u5947\u5f02\u8c31\u5206\u6790\uff08SSA\uff09\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u53d1\u73b0VAE\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6355\u6349\u975e\u7ebf\u6027\u65f6\u95f4\u4f9d\u8d56\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002", "motivation": "\u6c14\u5019\u5f02\u5e38\u5bf9\u9646\u5730\u78b3\u5faa\u73af\u52a8\u6001\u4ea7\u751f\u663e\u8457\u5f71\u54cd\uff0c\u9700\u8981\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u548c\u5206\u6790\u690d\u7269\u751f\u4ea7\u529b\u4e2d\u7684\u5f02\u5e38\u884c\u4e3a\u3002", "method": "\u91c7\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u8bc6\u522bGPP\u4e2d\u7684\u6781\u7aef\u4e8b\u4ef6\u3002VAE\u67b6\u6784\u91c7\u7528\u4e09\u4e2a\u5bc6\u96c6\u5c42\u548c\u4e00\u4e2a\u6f5c\u5728\u7a7a\u95f4\uff0c\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u4e3a12\u4e2a\u6708\uff0c\u57fa\u4e8e\u91cd\u5efa\u8bef\u5dee\u8bc6\u522b\u5f02\u5e38\u3002", "result": "VAE\u548cSSA\u65b9\u6cd5\u5728\u6781\u7aef\u4e8b\u4ef6\u9891\u7387\u7684\u7a7a\u95f4\u6a21\u5f0f\u4e0a\u8868\u73b0\u51fa\u5f88\u5f3a\u7684\u4e00\u81f4\u6027\uff0c\u540c\u65f6VAE\u4ea7\u751f\u4e86\u66f4\u9ad8\u7684\u9608\u503c\u3002\u4e24\u79cd\u65b9\u6cd5\u90fd\u8868\u660e\uff0c\u52302050-80\u5e74\uff0c\u8d1f\u78b3\u5faa\u73af\u6781\u7aef\u4e8b\u4ef6\u7684\u5e45\u5ea6\u548c\u9891\u7387\u90fd\u5728\u589e\u52a0\uff0c\u7279\u522b\u662f\u5728\u5317\u7f8e\u897f\u90e8\u548c\u4e2d\u90e8\u3002", "conclusion": "VAE\u65b9\u6cd5\u663e\u793a\u51fa\u4e0e\u5df2\u5efa\u7acb\u7684SSA\u6280\u672f\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u8ba1\u7b97\u4f18\u52bf\u548c\u589e\u5f3a\u7684\u6355\u6349\u78b3\u5faa\u73af\u53d8\u5f02\u4e2d\u975e\u7ebf\u6027\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u80fd\u529b\u3002\u4e0eSSA\u4e0d\u540c\uff0cVAE\u65b9\u6cd5\u4e0d\u9700\u8981\u5b9a\u4e49\u6570\u636e\u4e2d\u4fe1\u53f7\u7684\u5468\u671f\u6027\uff1b\u5b83\u53ef\u4ee5\u4ece\u6570\u636e\u4e2d\u53d1\u73b0\u5b83\u4eec\u3002"}}
{"id": "2510.03863", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03863", "abs": "https://arxiv.org/abs/2510.03863", "authors": ["Arina Kharlamova", "Bowei He", "Chen Ma", "Xue Liu"], "title": "Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation", "comment": "Submitted to ICLR 2026", "summary": "Online services rely on CAPTCHAs as a first line of defense against automated\nabuse, yet recent advances in multi-modal large language models (MLLMs) have\neroded the effectiveness of conventional designs that focus on text recognition\nor 2D image understanding. To address this challenge, we present Spatial\nCAPTCHA, a novel human-verification framework that leverages fundamental\ndifferences in spatial reasoning between humans and MLLMs. Unlike existing\nCAPTCHAs which rely on low-level perception tasks that are vulnerable to modern\nAI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning,\nperspective-taking, occlusion handling, and mental rotation. These skills are\nintuitive for humans but difficult for state-of-the-art (SOTA) AI systems. The\nsystem employs a procedural generation pipeline with constraint-based\ndifficulty control, automated correctness verification, and human-in-the-loop\nvalidation to ensure scalability, robustness, and adaptability. Evaluation on a\ncorresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastly\noutperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0%\nPass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA,\nwhich confirms its effectiveness as both a security mechanism and a diagnostic\ntool for spatial reasoning in AI.", "AI": {"tldr": "\u63d0\u51fa\u4e86Spatial CAPTCHA\uff0c\u4e00\u79cd\u5229\u7528\u4eba\u7c7b\u548c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u4e0a\u7684\u6839\u672c\u5dee\u5f02\u7684\u65b0\u578b\u4eba\u673a\u9a8c\u8bc1\u6846\u67b6\u3002", "motivation": "\u4f20\u7edf\u9a8c\u8bc1\u7801\u7684\u6709\u6548\u6027\u56e0\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u800c\u964d\u4f4e\u3002", "method": "\u751f\u6210\u9700\u8981\u51e0\u4f55\u63a8\u7406\u3001\u900f\u89c6\u3001\u906e\u6321\u5904\u7406\u548c\u5fc3\u7406\u65cb\u8f6c\u7684\u52a8\u6001\u95ee\u9898\u3002\u91c7\u7528\u5177\u6709\u57fa\u4e8e\u7ea6\u675f\u7684\u96be\u5ea6\u63a7\u5236\u3001\u81ea\u52a8\u6b63\u786e\u6027\u9a8c\u8bc1\u548c\u4eba\u5de5\u53c2\u4e0e\u9a8c\u8bc1\u7684\u7a0b\u5e8f\u751f\u6210\u6d41\u7a0b\u3002", "result": "\u5728Spatial-CAPTCHA-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4eba\u7c7b\u7684\u8868\u73b0\u5927\u5927\u4f18\u4e8e10\u4e2a\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6700\u4f73\u6a21\u578b\u7684Pass@1\u51c6\u786e\u7387\u4ec5\u4e3a31.0%\u3002\u4e0eGoogle reCAPTCHA\u7684\u6bd4\u8f83\u8bc1\u5b9e\u4e86\u5176\u4f5c\u4e3a\u5b89\u5168\u673a\u5236\u548cAI\u7a7a\u95f4\u63a8\u7406\u8bca\u65ad\u5de5\u5177\u7684\u6709\u6548\u6027\u3002", "conclusion": "Spatial CAPTCHA\u662f\u4e00\u79cd\u6709\u6548\u7684\u4eba\u673a\u9a8c\u8bc1\u6846\u67b6\uff0c\u53ef\u4ee5\u5f25\u8865\u73b0\u6709\u9a8c\u8bc1\u7801\u7684\u4e0d\u8db3\u3002"}}
{"id": "2510.03687", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03687", "abs": "https://arxiv.org/abs/2510.03687", "authors": ["Yue Huang", "Yanyuan Chen", "Dexuan Xu", "Weihua Yue", "Huamin Zhang", "Meikang Qiu", "Yu Huang"], "title": "MedReflect: Teaching Medical LLMs to Self-Improve via Reflective Correction", "comment": null, "summary": "Medical problem solving demands expert knowledge and intricate reasoning.\nRecent studies of large language models (LLMs) attempt to ease this complexity\nby introducing external knowledge verification through retrieval-augmented\ngeneration or by training on reasoning datasets. However, these approaches\nsuffer from drawbacks such as retrieval overhead and high annotation costs, and\nthey heavily rely on substituted external assistants to reach limited\nperformance in medical field. In this paper, we introduce MedReflect, a\ngeneralizable framework designed to inspire LLMs with a physician-like\nreflective thinking mode. MedReflect generates a single-pass reflection chain\nthat includes initial hypothesis generation, self-questioning, self-answering\nand decision refinement. This self-verified and self-reflective nature releases\nlarge language model's latent capability in medical problem-solving without\nexternal retrieval or heavy annotation. We demonstrate that MedReflect enables\ncost-efficient medical dataset construction: with merely 2,000 randomly sampled\ntraining examples and a light fine-tuning, this approach achieves notable\nabsolute accuracy improvements across a series of medical benchmarks while\ncutting annotation requirements. Our results provide evidence that LLMs can\nlearn to solve specialized medical problems via self-reflection and\nself-improve, reducing reliance on external supervision and extensive\ntask-specific fine-tuning data.", "AI": {"tldr": "MedReflect\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u533b\u751f\u822c\u7684\u53cd\u601d\u6027\u601d\u7ef4\u6a21\u5f0f\uff0c\u6fc0\u53d1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u533b\u7597\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u6f5c\u529b\uff0c\u65e0\u9700\u5916\u90e8\u68c0\u7d22\u6216\u5927\u91cf\u6807\u6ce8\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u77e5\u8bc6\u6216\u63a8\u7406\u6570\u636e\u96c6\uff0c\u5b58\u5728\u68c0\u7d22\u5f00\u9500\u9ad8\u3001\u6807\u6ce8\u6210\u672c\u9ad8\u7b49\u95ee\u9898\uff0c\u4e14\u6027\u80fd\u53d7\u9650\u3002", "method": "MedReflect\u751f\u6210\u5355\u6b21\u53cd\u601d\u94fe\uff0c\u5305\u62ec\u521d\u59cb\u5047\u8bbe\u751f\u6210\u3001\u81ea\u6211\u63d0\u95ee\u3001\u81ea\u6211\u56de\u7b54\u548c\u51b3\u7b56\u4f18\u5316\u3002", "result": "\u4ec5\u75282000\u4e2a\u968f\u673a\u62bd\u6837\u7684\u8bad\u7ec3\u6837\u672c\u8fdb\u884c\u8f7b\u5fae\u5fae\u8c03\uff0c\u5373\u53ef\u5728\u591a\u4e2a\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u663e\u8457\u7684\u7edd\u5bf9\u7cbe\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u964d\u4f4e\u6807\u6ce8\u9700\u6c42\u3002", "conclusion": "LLM\u53ef\u4ee5\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u548c\u81ea\u6211\u6539\u8fdb\u6765\u89e3\u51b3\u7279\u5b9a\u7684\u533b\u7597\u95ee\u9898\uff0c\u51cf\u5c11\u5bf9\u5916\u90e8\u76d1\u7763\u548c\u5927\u91cf\u7279\u5b9a\u4efb\u52a1\u5fae\u8c03\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2510.03483", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03483", "abs": "https://arxiv.org/abs/2510.03483", "authors": ["Numan Saeed", "Tausifa Jan Saleem", "Fadillah Maani", "Muhammad Ridzuan", "Hu Wang", "Mohammad Yaqub"], "title": "DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis", "comment": null, "summary": "Deep learning for medical imaging is hampered by task-specific models that\nlack generalizability and prognostic capabilities, while existing 'universal'\napproaches suffer from simplistic conditioning and poor medical semantic\nunderstanding. To address these limitations, we introduce DuPLUS, a deep\nlearning framework for efficient multi-modal medical image analysis. DuPLUS\nintroduces a novel vision-language framework that leverages hierarchical\nsemantic prompts for fine-grained control over the analysis task, a capability\nabsent in prior universal models. To enable extensibility to other medical\ntasks, it includes a hierarchical, text-controlled architecture driven by a\nunique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize\nacross three imaging modalities, ten different anatomically various medical\ndatasets, encompassing more than 30 organs and tumor types. It outperforms the\nstate-of-the-art task specific and universal models on 8 out of 10 datasets. We\ndemonstrate extensibility of its text-controlled architecture by seamless\nintegration of electronic health record (EHR) data for prognosis prediction,\nand on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI)\nof 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks\nand modalities from varying centers, establishing DuPLUS as a versatile and\nclinically relevant solution for medical image analysis. The code for this work\nis made available at: https://anonymous.4open.science/r/DuPLUS-6C52", "AI": {"tldr": "DuPLUS\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5206\u5c42\u8bed\u4e49\u63d0\u793a\u5b9e\u73b0\u5bf9\u5206\u6790\u4efb\u52a1\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u5e76\u5728\u5206\u5272\u548c\u9884\u540e\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u5b66\u56fe\u50cf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u901a\u7528\u6027\u548c\u9884\u540e\u80fd\u529b\uff0c\u800c\u201c\u901a\u7528\u201d\u65b9\u6cd5\u53c8\u8fc7\u4e8e\u7b80\u5355\uff0c\u7f3a\u4e4f\u533b\u5b66\u8bed\u4e49\u7406\u89e3\u3002", "method": "DuPLUS\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9-\u8bed\u8a00\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5206\u5c42\u8bed\u4e49\u63d0\u793a\u548c\u72ec\u7279\u7684\u53cc\u63d0\u793a\u673a\u5236\uff0c\u5b9e\u73b0\u6587\u672c\u63a7\u5236\u7684\u67b6\u6784\u3002", "result": "DuPLUS\u5728\u4e09\u79cd\u6210\u50cf\u65b9\u5f0f\u3001\u5341\u4e2a\u4e0d\u540c\u7684\u89e3\u5256\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6cdb\u5316\uff0c\u6db5\u76d6\u4e8630\u591a\u4e2a\u5668\u5b98\u548c\u80bf\u7624\u7c7b\u578b\uff0c\u5e76\u572810\u4e2a\u6570\u636e\u96c6\u4e2d\u67098\u4e2a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4efb\u52a1\u7279\u5b9a\u548c\u901a\u7528\u6a21\u578b\u3002\u5728\u5934\u9888\u764c\u6570\u636e\u96c6\u4e0a\uff0cDuPLUS\u5b9e\u73b0\u4e860.69\u7684\u4e00\u81f4\u6027\u6307\u6570\uff08CI\uff09\u3002", "conclusion": "DuPLUS\u662f\u4e00\u79cd\u591a\u529f\u80fd\u4e14\u4e34\u5e8a\u76f8\u5173\u7684\u533b\u5b66\u56fe\u50cf\u5206\u6790\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u53ef\u4ee5\u901a\u8fc7\u9ad8\u6548\u7684\u53c2\u6570\u5fae\u8c03\u5feb\u901f\u9002\u5e94\u6765\u81ea\u4e0d\u540c\u4e2d\u5fc3\u7684\u65b0\u4efb\u52a1\u548c\u6a21\u5f0f\u3002"}}
{"id": "2510.03267", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03267", "abs": "https://arxiv.org/abs/2510.03267", "authors": ["Xianglong Yan", "Chengzhu Bao", "Zhiteng Li", "Tianao Zhang", "Kaicheng Yang", "Haotong Qin", "Ruobing Xie", "Xingwu Sun", "Yulun Zhang"], "title": "PT$^2$-LLM: Post-Training Ternarization for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have shown impressive capabilities across\ndiverse tasks, but their large memory and compute demands hinder deployment.\nTernarization has gained attention as a promising compression technique,\ndelivering substantial size reduction and high computational efficiency.\nHowever, its potential in the post-training quantization (PTQ) setting remains\nunderexplored, due to the challenge of training-free parameter optimization and\nthe quantization difficulty posed by outliers and dispersed weights. To address\nthese issues, we propose PT$^2$-LLM, a post-training ternarization framework\ntailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with\na two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which\nalternates between optimal ternary grid construction and flexible rounding to\nminimize quantization error, and (2) Activation-aware Grid Alignment (AGA),\nwhich further refines the ternary grid to better match full-precision outputs.\nIn addition, we propose a plug-and-play Structural Similarity-based Reordering\n(SSR) strategy that leverages inter-column structural similarity to ease\nquantization and mitigate outlier effects, further enhancing overall\nperformance. Extensive experiments demonstrate that PT$^2$-LLM delivers\ncompetitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with\nlower memory cost, while also accelerating both prefill and decoding to achieve\nend-to-end speedup. The code and models will be available at\nhttps://github.com/XIANGLONGYAN/PT2-LLM.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPT^2-LLM\u7684\u540e\u8bad\u7ec3\u4e09\u5143\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u538b\u7f29\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u7136\u529f\u80fd\u5f3a\u5927\uff0c\u4f46\u5176\u5e9e\u5927\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u9650\u5236\u4e86\u90e8\u7f72\u3002\u4e09\u5143\u5316\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u9014\u7684\u538b\u7f29\u6280\u672f\uff0c\u53d7\u5230\u4e86\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u5176\u5728\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u8bbe\u7f6e\u4e2d\u7684\u6f5c\u529b\u4ecd\u6709\u5f85\u63a2\u7d22\u3002", "method": "\u8be5\u6846\u67b6\u7684\u6838\u5fc3\u662f\u4e00\u4e2a\u975e\u5bf9\u79f0\u4e09\u5143\u91cf\u5316\u5668\uff0c\u914d\u5907\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u4f18\u5316\u6d41\u7a0b\uff1a\u8fed\u4ee3\u4e09\u5143\u62df\u5408\uff08ITF\uff09\u548c\u6fc0\u6d3b\u611f\u77e5\u7f51\u683c\u5bf9\u9f50\uff08AGA\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u76f8\u4f3c\u6027\u7684\u91cd\u6392\u5e8f\uff08SSR\uff09\u7b56\u7565\uff0c\u4ee5\u7b80\u5316\u91cf\u5316\u5e76\u51cf\u8f7b\u5f02\u5e38\u503c\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPT^2-LLM \u63d0\u4f9b\u4e86\u4e0e\u6700\u5148\u8fdb\u7684\uff08SOTA\uff092-bit PTQ \u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u5185\u5b58\u6210\u672c\uff0c\u5e76\u52a0\u901f\u4e86\u9884\u586b\u5145\u548c\u89e3\u7801\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u52a0\u901f\u3002", "conclusion": "PT^2-LLM \u662f\u4e00\u79cd\u6709\u6548\u7684 LLM \u538b\u7f29\u65b9\u6cd5\uff0c\u5728\u6027\u80fd\u3001\u5185\u5b58\u548c\u901f\u5ea6\u65b9\u9762\u90fd\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2510.03886", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03886", "abs": "https://arxiv.org/abs/2510.03886", "authors": ["Seil Kang", "Woojung Han", "Dayun Ju", "Seong Jae Hwang"], "title": "Rare Text Semantics Were Always There in Your Diffusion Transformer", "comment": "Accepted to NeurIPS 2025", "summary": "Starting from flow- and diffusion-based transformers, Multi-modal Diffusion\nTransformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim\nfor exceptional visual fidelity. As these models advance, users continually\npush the boundary with imaginative or rare prompts, which advanced models still\nfalter in generating, since their concepts are often too scarce to leave a\nstrong imprint during pre-training. In this paper, we propose a simple yet\neffective intervention that surfaces rare semantics inside MM-DiTs without\nadditional training steps, data, denoising-time optimization, or reliance on\nexternal modules (e.g., large language models). In particular, the\njoint-attention mechanism intrinsic to MM-DiT sequentially updates text\nembeddings alongside image embeddings throughout transformer blocks. We find\nthat by mathematically expanding representational basins around text token\nembeddings via variance scale-up before the joint-attention blocks, rare\nsemantics clearly emerge in MM-DiT's outputs. Furthermore, our results\ngeneralize effectively across text-to-vision tasks, including text-to-image,\ntext-to-video, and text-driven image editing. Our work invites generative\nmodels to reveal the semantics that users intend, once hidden yet ready to\nsurface.", "AI": {"tldr": "MM-DiTs\u5728\u5904\u7406\u7f55\u89c1prompt\u65f6\u4f1a\u9047\u5230\u56f0\u96be\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6982\u5ff5\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u7559\u4e0b\u7684\u5370\u8bb0\u8f83\u5f31\u3002", "motivation": "\u7528\u6237\u4e0d\u65ad\u5c1d\u8bd5\u7528\u60f3\u8c61\u529b\u4e30\u5bcc\u7684prompt\u6765\u6311\u6218text-to-vision\u751f\u6210\u6a21\u578b\uff0c\u4f46\u6a21\u578b\u5728\u751f\u6210\u8fd9\u4e9bprompt\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u5728\u8054\u5408\u6ce8\u610f\u529b\u6a21\u5757\u4e4b\u524d\uff0c\u901a\u8fc7\u65b9\u5dee\u653e\u5927\u6765\u6269\u5927\u6587\u672ctoken\u5d4c\u5165\u5468\u56f4\u7684\u8868\u5f81\u76c6\u5730\uff0c\u4ece\u800c\u4f7fMM-DiT\u5185\u90e8\u7684\u7f55\u89c1\u8bed\u4e49\u663e\u73b0\u51fa\u6765\u3002", "result": "\u8be5\u65b9\u6cd5\u5728text-to-vision\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u5305\u62ectext-to-image\u3001text-to-video\u548c\u6587\u672c\u9a71\u52a8\u7684\u56fe\u50cf\u7f16\u8f91\u3002", "conclusion": "\u8be5\u7814\u7a76\u9080\u8bf7\u751f\u6210\u6a21\u578b\u63ed\u793a\u7528\u6237\u610f\u56fe\u4e2d\u7684\u8bed\u4e49\uff0c\u8fd9\u4e9b\u8bed\u4e49\u66fe\u7ecf\u9690\u85cf\u4f46\u73b0\u5728\u53ef\u4ee5\u88ab\u53d1\u73b0\u3002"}}
{"id": "2510.03748", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03748", "abs": "https://arxiv.org/abs/2510.03748", "authors": ["Ramtin Kakavand", "Ebrahim Ansari"], "title": "TreePrompt: Leveraging Hierarchical Few-Shot Example Selection for Improved English-Persian and English-German Translation", "comment": "12 pages", "summary": "Large Language Models (LLMs) have consistently demonstrated strong\nperformance in machine translation, especially when guided by high-quality\nprompts. Few-shot prompting is an effective technique to improve translation\nquality; however, most existing example selection methods focus solely on\nquery-to-example similarity and do not account for the quality of the examples.\nIn this work, we propose TreePrompt, a novel example selection approach that\nlearns LLM preferences to identify high-quality, contextually relevant examples\nwithin a tree-structured framework. To further explore the balance between\nsimilarity and quality, we combine TreePrompt with K-Nearest Neighbors (K-NN)\nand Adaptive Few-Shot Prompting (AFSP). Evaluations on two language pairs -\nEnglish-Persian (MIZAN) and English-German (WMT19) - show that integrating\nTreePrompt with AFSP or Random selection leads to improved translation\nperformance.", "AI": {"tldr": "TreePrompt\u662f\u4e00\u79cd\u65b0\u9896\u7684\u793a\u4f8b\u9009\u62e9\u65b9\u6cd5\uff0c\u5b83\u5b66\u4e60 LLM \u504f\u597d\uff0c\u4ee5\u5728\u6811\u7ed3\u6784\u6846\u67b6\u4e2d\u8bc6\u522b\u9ad8\u8d28\u91cf\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u793a\u4f8b\u3002", "motivation": "\u73b0\u6709\u793a\u4f8b\u9009\u62e9\u65b9\u6cd5\u53ea\u5173\u6ce8\u67e5\u8be2\u5230\u793a\u4f8b\u7684\u76f8\u4f3c\u6027\uff0c\u800c\u6ca1\u6709\u8003\u8651\u793a\u4f8b\u7684\u8d28\u91cf\u3002", "method": "\u63d0\u51fa TreePrompt\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u793a\u4f8b\u9009\u62e9\u65b9\u6cd5\uff0c\u5b83\u5b66\u4e60 LLM \u504f\u597d\uff0c\u4ee5\u5728\u6811\u7ed3\u6784\u6846\u67b6\u4e2d\u8bc6\u522b\u9ad8\u8d28\u91cf\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u793a\u4f8b\u3002\u5c06 TreePrompt \u4e0e K-\u6700\u8fd1\u90bb (K-NN) \u548c\u81ea\u9002\u5e94\u5c11\u6837\u672c\u63d0\u793a (AFSP) \u76f8\u7ed3\u5408\u3002", "result": "\u5728\u4e24\u4e2a\u8bed\u8a00\u5bf9\uff08\u82f1\u8bed-\u6ce2\u65af\u8bed (MIZAN) \u548c\u82f1\u8bed-\u5fb7\u8bed (WMT19)\uff09\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5c06 TreePrompt \u4e0e AFSP \u6216\u968f\u673a\u9009\u62e9\u76f8\u7ed3\u5408\u53ef\u4ee5\u63d0\u9ad8\u7ffb\u8bd1\u6027\u80fd\u3002", "conclusion": "\u5c06 TreePrompt \u4e0e AFSP \u6216\u968f\u673a\u9009\u62e9\u76f8\u7ed3\u5408\u53ef\u4ee5\u63d0\u9ad8\u7ffb\u8bd1\u6027\u80fd\u3002"}}
{"id": "2510.03501", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03501", "abs": "https://arxiv.org/abs/2510.03501", "authors": ["Lyes Saad Saoud", "Loic Lesobre", "Enrico Sorato", "Irfan Hussain"], "title": "Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms", "comment": null, "summary": "Real-time animal detection and segmentation in natural environments are vital\nfor wildlife conservation, enabling non-invasive monitoring through remote\ncamera streams. However, these tasks remain challenging due to limited\ncomputational resources and the cryptic appearance of many species. We propose\na mobile-optimized two-stage deep learning framework that integrates a\nThreading Detection Model (TDM) to parallelize YOLOv10-based detection and\nMobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach\nimproves real-time performance by reducing latency through threading. YOLOv10\nhandles detection while MobileSAM performs lightweight segmentation, both\nexecuted concurrently for efficient resource use. On the cryptic Houbara\nBustard, a conservation-priority species, our model achieves mAP50 of 0.9627,\nmAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10\noperates at 43.7 ms per frame, confirming real-time readiness. We introduce a\ncurated Houbara dataset of 40,000 annotated images to support model training\nand evaluation across diverse conditions. The code and dataset used in this\nstudy are publicly available on GitHub at\nhttps://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos\nand additional resources, visit\nhttps://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79fb\u52a8\u4f18\u5316\u7684\u53cc\u9636\u6bb5\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u81ea\u7136\u73af\u5883\u4e2d\u5b9e\u65f6\u8fdb\u884c\u52a8\u7269\u68c0\u6d4b\u548c\u5206\u5272\uff0c\u901a\u8fc7\u5e76\u884c\u5316YOLOv10\u68c0\u6d4b\u548cMobileSAM\u5206\u5272\u6765\u63d0\u9ad8\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u5728\u81ea\u7136\u73af\u5883\u4e2d\u5b9e\u65f6\u8fdb\u884c\u52a8\u7269\u68c0\u6d4b\u548c\u5206\u5272\u5bf9\u4e8e\u91ce\u751f\u52a8\u7269\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u548c\u8bb8\u591a\u7269\u79cd\u7684\u9690\u853d\u6027\u5916\u89c2\uff0c\u8fd9\u4e9b\u4efb\u52a1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u96c6\u6210\u4e86\u4e00\u4e2a\u7ebf\u7a0b\u68c0\u6d4b\u6a21\u578b\uff08TDM\uff09\u4ee5\u5e76\u884c\u5316YOLOv10-based\u68c0\u6d4b\u548cMobileSAM-based\u5206\u5272\u3002\u901a\u8fc7\u7ebf\u7a0b\u51cf\u5c11\u5ef6\u8fdf\u6765\u6539\u8fdb\u5b9e\u65f6\u6027\u80fd\u3002YOLOv10\u5904\u7406\u68c0\u6d4b\uff0c\u800cMobileSAM\u6267\u884c\u8f7b\u91cf\u7ea7\u5206\u5272\uff0c\u4e24\u8005\u540c\u65f6\u6267\u884c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u8d44\u6e90\u5229\u7528\u3002", "result": "\u5728\u9690\u853d\u7684Houbara Bustard\u4e0a\uff0c\u8be5\u6a21\u578b\u5b9e\u73b0\u4e86mAP50\u4e3a0.9627\uff0cmAP75\u4e3a0.7731\uff0cmAP95\u4e3a0.7178\uff0cMobileSAM mIoU\u4e3a0.7421\u3002YOLOv10\u4ee5\u6bcf\u5e2743.7\u6beb\u79d2\u7684\u901f\u5ea6\u8fd0\u884c\uff0c\u786e\u8ba4\u4e86\u5b9e\u65f6\u5c31\u7eea\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b40,000\u5f20\u5e26\u6ce8\u91ca\u56fe\u50cf\u7684\u7cbe\u9009Houbara\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u8de8\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5728GitHub\u4e0a\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2510.03268", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03268", "abs": "https://arxiv.org/abs/2510.03268", "authors": ["Lingjie Yi", "Raphael Douady", "Chao Chen"], "title": "Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment", "comment": null, "summary": "Multimodal contrastive learning (MCL) aims to embed data from different\nmodalities in a shared embedding space. However, empirical evidence shows that\nrepresentations from different modalities occupy completely separate regions of\nembedding space, a phenomenon referred to as the modality gap. Moreover,\nexperimental findings on how the size of the modality gap influences downstream\nperformance are inconsistent. These observations raise two key questions: (1)\nWhat causes the modality gap? (2) How does it affect downstream tasks? To\naddress these questions, this paper introduces the first theoretical framework\nfor analyzing the convergent optimal representations of MCL and the modality\nalignment when training is optimized. Specifically, we prove that without any\nconstraint or under the cone constraint, the modality gap converges to zero.\nUnder the subspace constraint (i.e., representations of two modalities fall\ninto two distinct hyperplanes due to dimension collapse), the modality gap\nconverges to the smallest angle between the two hyperplanes. This result\nidentifies \\emph{dimension collapse} as the fundamental origin of the modality\ngap. Furthermore, our theorems demonstrate that paired samples cannot be\nperfectly aligned under the subspace constraint. The modality gap influences\ndownstream performance by affecting the alignment between sample pairs. We\nprove that, in this case, perfect alignment between two modalities can still be\nachieved via two ways: hyperplane rotation and shared space projection.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\uff08MCL\uff09\u4e2d\u7684\u6a21\u6001\u5dee\u8ddd\u95ee\u9898\uff0c\u5e76\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u4e86\u5176\u6210\u56e0\u548c\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u6a21\u6001\u5dee\u8ddd\u73b0\u8c61\uff0c\u4e14\u6a21\u6001\u5dee\u8ddd\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u6a21\u6001\u5dee\u8ddd\u7684\u6210\u56e0\u4ee5\u53ca\u5b83\u5982\u4f55\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790MCL\u7684\u6536\u655b\u6700\u4f18\u8868\u793a\u548c\u6a21\u6001\u5bf9\u9f50\uff0c\u5e76\u63a2\u8ba8\u4e0d\u540c\u7ea6\u675f\u6761\u4ef6\u4e0b\u7684\u6a21\u6001\u5dee\u8ddd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u65e0\u7ea6\u675f\u6216\u9525\u7ea6\u675f\u4e0b\uff0c\u6a21\u6001\u5dee\u8ddd\u6536\u655b\u5230\u96f6\uff1b\u5728\u5b50\u7a7a\u95f4\u7ea6\u675f\u4e0b\uff0c\u6a21\u6001\u5dee\u8ddd\u6536\u655b\u5230\u4e24\u4e2a\u8d85\u5e73\u9762\u4e4b\u95f4\u7684\u6700\u5c0f\u89d2\u5ea6\u3002\u7ef4\u5ea6\u574d\u584c\u662f\u6a21\u6001\u5dee\u8ddd\u7684\u6839\u672c\u539f\u56e0\u3002\u6b64\u5916\uff0c\u5728\u5b50\u7a7a\u95f4\u7ea6\u675f\u4e0b\uff0c\u914d\u5bf9\u6837\u672c\u65e0\u6cd5\u5b8c\u7f8e\u5bf9\u9f50\u3002\u6a21\u6001\u5dee\u8ddd\u901a\u8fc7\u5f71\u54cd\u6837\u672c\u5bf9\u4e4b\u95f4\u7684\u5bf9\u9f50\u6765\u5f71\u54cd\u4e0b\u6e38\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u4ece\u7406\u8bba\u4e0a\u63ed\u793a\u4e86\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u6a21\u6001\u5dee\u8ddd\u7684\u6210\u56e0\u548c\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u901a\u8fc7\u8d85\u5e73\u9762\u65cb\u8f6c\u548c\u5171\u4eab\u7a7a\u95f4\u6295\u5f71\u6765\u5b9e\u73b0\u6a21\u6001\u5b8c\u7f8e\u5bf9\u9f50\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.03892", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03892", "abs": "https://arxiv.org/abs/2510.03892", "authors": ["Zahra Atf", "Peter R. Lewis"], "title": "Kantian-Utilitarian XAI: Meta-Explained", "comment": "Accepted for presentation as a poster at the 35th IEEE International\n  Conference on Collaborative Advances in Software and Computing, 2025.\n  Conference\n  website:https://conf.researchr.org/details/cascon-2025/posters-track/1/Kantian-Utilitarian-XAI-Meta-Explained", "summary": "We present a gamified explainable AI (XAI) system for ethically aware\nconsumer decision-making in the coffee domain. Each session comprises six\nrounds with three options per round. Two symbolic engines provide real-time\nreasons: a Kantian module flags rule violations (e.g., child labor,\ndeforestation risk without shade certification, opaque supply chains, unsafe\ndecaf), and a utilitarian module scores options via multi-criteria aggregation\nover normalized attributes (price, carbon, water, transparency, farmer income\nshare, taste/freshness, packaging, convenience). A meta-explainer with a regret\nbound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a\ndeontically clean, near-parity option when welfare loss is small. We release a\nstructured configuration (attribute schema, certification map, weights, rule\nset), a policy trace for auditability, and an interactive UI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5496\u5561\u9886\u57df\u7684\u3001\u5177\u6709\u4f26\u7406\u610f\u8bc6\u7684\u3001\u6e38\u620f\u5316\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\uff0c\u7528\u4e8e\u6d88\u8d39\u8005\u51b3\u7b56\u3002", "motivation": "\u5e2e\u52a9\u6d88\u8d39\u8005\u5728\u8d2d\u4e70\u5496\u5561\u65f6\u505a\u51fa\u66f4\u7b26\u5408\u4f26\u7406\u7684\u9009\u62e9\u3002", "method": "\u8be5\u7cfb\u7edf\u5305\u542b\u4e24\u4e2a\u7b26\u53f7\u5f15\u64ce\uff08\u5eb7\u5fb7\u6a21\u5757\u548c\u529f\u5229\u6a21\u5757\uff09\u63d0\u4f9b\u5b9e\u65f6\u7406\u7531\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5143\u89e3\u91ca\u5668\u6765\u7a81\u51fa\u5eb7\u5fb7\u4e3b\u4e49\u4e0e\u529f\u5229\u4e3b\u4e49\u7684\uff08\u4e0d\uff09\u5bf9\u9f50\u3002", "result": "\u53d1\u5e03\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u914d\u7f6e\u3001\u4e00\u4e2a\u7528\u4e8e\u53ef\u5ba1\u8ba1\u6027\u7684\u7b56\u7565\u8ddf\u8e2a\u548c\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u7528\u6237\u754c\u9762\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u591f\u5728\u798f\u5229\u635f\u5931\u8f83\u5c0f\u7684\u60c5\u51b5\u4e0b\uff0c\u5207\u6362\u5230\u5728\u9053\u4e49\u4e0a\u66f4\u6e05\u6d01\u7684\u3001\u63a5\u8fd1\u5e73\u4ef7\u7684\u9009\u9879\u3002"}}
{"id": "2510.03758", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.03758", "abs": "https://arxiv.org/abs/2510.03758", "authors": ["Ilias Tougui", "Mehdi Zakroum", "Mounir Ghogho"], "title": "Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's Disease Diagnosis from Speech", "comment": null, "summary": "Parkinson's Disease (PD) affects over 10 million people worldwide, with\nspeech impairments in up to 89% of patients. Current speech-based detection\nsystems analyze entire utterances, potentially overlooking the diagnostic value\nof specific phonetic elements. We developed a granularity-aware approach for\nmultilingual PD detection using an automated pipeline that extracts\ntime-aligned phonemes, syllables, and words from recordings. Using Italian,\nSpanish, and English datasets, we implemented a bidirectional LSTM with\nmulti-head attention to compare diagnostic performance across the different\ngranularity levels. Phoneme-level analysis achieved superior performance with\nAUROC of 93.78% +- 2.34% and accuracy of 92.17% +- 2.43%. This demonstrates\nenhanced diagnostic capability for cross-linguistic PD detection. Importantly,\nattention analysis revealed that the most informative speech features align\nwith those used in established clinical protocols: sustained vowels (/a/, /e/,\n/o/, /i/) at phoneme level, diadochokinetic syllables (/ta/, /pa/, /la/, /ka/)\nat syllable level, and /pataka/ sequences at word level. Source code will be\navailable at https://github.com/jetliqs/clearpd.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u7684\u591a\u8bed\u8a00\u5e15\u91d1\u68ee\u75c5(PD)\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u52a8\u6d41\u7a0b\u4ece\u8bb0\u5f55\u4e2d\u63d0\u53d6\u65f6\u95f4\u5bf9\u9f50\u7684\u97f3\u7d20\u3001\u97f3\u8282\u548c\u5355\u8bcd\u3002", "motivation": "\u76ee\u524d\u57fa\u4e8e\u8bed\u97f3\u7684\u68c0\u6d4b\u7cfb\u7edf\u5206\u6790\u6574\u4e2a\u8bdd\u8bed\uff0c\u53ef\u80fd\u5ffd\u7565\u4e86\u7279\u5b9a\u8bed\u97f3\u5143\u7d20\u7684\u8bca\u65ad\u4ef7\u503c\u3002", "method": "\u4f7f\u7528\u610f\u5927\u5229\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u82f1\u8bed\u6570\u636e\u96c6\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5177\u6709\u591a\u5934\u6ce8\u610f\u529b\u7684\u53cc\u5411LSTM\uff0c\u4ee5\u6bd4\u8f83\u4e0d\u540c\u7c92\u5ea6\u7ea7\u522b\u7684\u8bca\u65ad\u6027\u80fd\u3002", "result": "\u97f3\u7d20\u7ea7\u522b\u7684\u5206\u6790\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0cAUROC\u4e3a93.78% +- 2.34%\uff0c\u51c6\u786e\u7387\u4e3a92.17% +- 2.43%\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u4e8e\u8de8\u8bed\u8a00PD\u68c0\u6d4b\uff0c\u589e\u5f3a\u4e86\u8bca\u65ad\u80fd\u529b\u3002\u91cd\u8981\u7684\u662f\uff0c\u6ce8\u610f\u529b\u5206\u6790\u8868\u660e\uff0c\u4fe1\u606f\u91cf\u6700\u5927\u7684\u8bed\u97f3\u7279\u5f81\u4e0e\u5df2\u5efa\u7acb\u7684\u4e34\u5e8a\u534f\u8bae\u4e2d\u4f7f\u7528\u7684\u7279\u5f81\u76f8\u4e00\u81f4\uff1a\u97f3\u7d20\u6c34\u5e73\u4e0a\u7684\u6301\u7eed\u5143\u97f3\uff08/a/\u3001/e/\u3001/o/\u3001/i/\uff09\uff0c\u97f3\u8282\u6c34\u5e73\u4e0a\u7684\u5feb\u901f\u91cd\u590d\u97f3\u8282\uff08/ta/\u3001/pa/\u3001/la/\u3001/ka/\uff09\u548c\u5355\u8bcd\u6c34\u5e73\u4e0a\u7684/pataka/\u5e8f\u5217\u3002"}}
{"id": "2510.03511", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.03511", "abs": "https://arxiv.org/abs/2510.03511", "authors": ["Mohammad Mohaiminul Islam", "Rishabh Anand", "David R. Wessels", "Friso de Kruiff", "Thijs P. Kuipers", "Rex Ying", "Clara I. S\u00e1nchez", "Sharvaree Vadgama", "Georg B\u00f6kman", "Erik J. Bekkers"], "title": "Platonic Transformers: A Solid Choice For Equivariance", "comment": null, "summary": "While widespread, Transformers lack inductive biases for geometric symmetries\ncommon in science and computer vision. Existing equivariant methods often\nsacrifice the efficiency and flexibility that make Transformers so effective\nthrough complex, computationally intensive designs. We introduce the Platonic\nTransformer to resolve this trade-off. By defining attention relative to\nreference frames from the Platonic solid symmetry groups, our method induces a\nprincipled weight-sharing scheme. This enables combined equivariance to\ncontinuous translations and Platonic symmetries, while preserving the exact\narchitecture and computational cost of a standard Transformer. Furthermore, we\nshow that this attention is formally equivalent to a dynamic group convolution,\nwhich reveals that the model learns adaptive geometric filters and enables a\nhighly scalable, linear-time convolutional variant. Across diverse benchmarks\nin computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular\nproperty prediction (QM9, OMol25), the Platonic Transformer achieves\ncompetitive performance by leveraging these geometric constraints at no\nadditional cost.", "AI": {"tldr": "Platonic Transformer\u901a\u8fc7\u5b9a\u4e49Platonic solid\u5bf9\u79f0\u7fa4\u7684\u53c2\u8003\u7cfb\u6765\u5173\u6ce8\uff0c\u4ece\u800c\u5b9e\u73b0\u8fde\u7eed\u8f6c\u6362\u548c\u5e73\u9762\u5bf9\u79f0\u7684\u7b49\u53d8\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6807\u51c6Transformer\u7684\u67b6\u6784\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684\u7b49\u53d8\u65b9\u6cd5\u901a\u5e38\u4f1a\u727a\u7272\u6548\u7387\u548c\u7075\u6d3b\u6027\uff0c\u800cPlatonic Transformer\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "Platonic Transformer\u901a\u8fc7\u5b9a\u4e49Platonic solid\u5bf9\u79f0\u7fa4\u7684\u53c2\u8003\u7cfb\u6765\u5173\u6ce8\uff0c\u4ece\u800c\u8bf1\u5bfc\u51fa\u4e00\u4e2a\u539f\u5219\u6027\u7684\u6743\u91cd\u5171\u4eab\u65b9\u6848\u3002", "result": "\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u30013D\u70b9\u4e91\u548c\u5206\u5b50\u6027\u8d28\u9884\u6d4b\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPlatonic Transformer\u901a\u8fc7\u5229\u7528\u8fd9\u4e9b\u51e0\u4f55\u7ea6\u675f\u4ee5\u96f6\u989d\u5916\u6210\u672c\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "Platonic Transformer\u5728\u4e0d\u589e\u52a0\u989d\u5916\u6210\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u51e0\u4f55\u7ea6\u675f\u4e0b\u7684\u7ade\u4e89\u6027\u80fd\u3002"}}
{"id": "2510.03269", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03269", "abs": "https://arxiv.org/abs/2510.03269", "authors": ["Wendi Li", "Changdae Oh", "Yixuan Li"], "title": "General Exploratory Bonus for Optimistic Exploration in RLHF", "comment": null, "summary": "Optimistic exploration is central to improving sample efficiency in\nreinforcement learning with human feedback, yet existing exploratory bonus\nmethods to incentivize exploration often fail to realize optimism. We provide a\ntheoretical analysis showing that current formulations, under KL or\n$\\alpha$-divergence regularization, unintentionally bias exploration toward\nhigh-probability regions of the reference model, thereby reinforcing\nconservative behavior instead of promoting discovery of uncertain regions. To\naddress this pitfall, we introduce the General Exploratory Bonus (GEB), a novel\ntheoretical framework that provably satisfies the optimism principle. GEB\ncounteracts divergence-induced bias via reference-dependent reward regulation\nand unifies prior heuristic bonuses as special cases, while extending naturally\nacross the full $\\alpha$-divergence family. Empirically, GEB consistently\noutperforms baselines on alignment tasks across multiple divergence settings\nand large language model backbones. These results demonstrate that GEB offers\nboth a principled and practical solution for optimistic exploration in RLHF.", "AI": {"tldr": "\u73b0\u6709\u7684\u63a2\u7d22\u6027\u5956\u52b1\u65b9\u6cd5\u901a\u5e38\u65e0\u6cd5\u5b9e\u73b0\u4e50\u89c2\u4e3b\u4e49\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f1a\u65e0\u610f\u4e2d\u5c06\u63a2\u7d22\u504f\u5411\u53c2\u8003\u6a21\u578b\u7684\u9ad8\u6982\u7387\u533a\u57df\uff0c\u4ece\u800c\u52a0\u5f3a\u4fdd\u5b88\u884c\u4e3a\u800c\u4e0d\u662f\u4fc3\u8fdb\u4e0d\u786e\u5b9a\u533a\u57df\u7684\u53d1\u73b0\u3002\u6211\u4eec\u5f15\u5165\u4e86\u901a\u7528\u63a2\u7d22\u6027\u5956\u52b1\uff08GEB\uff09\uff0c\u5b83\u901a\u8fc7\u53c2\u8003\u76f8\u5173\u7684\u5956\u52b1\u8c03\u8282\u6765\u62b5\u6d88\u7531\u6563\u5ea6\u5f15\u8d77\u7684\u504f\u5dee\uff0c\u5e76\u5728\u591a\u4e2a\u6563\u5ea6\u8bbe\u7f6e\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9aa8\u5e72\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u5728\u4eba\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u4e50\u89c2\u63a2\u7d22\u5bf9\u4e8e\u63d0\u9ad8\u6837\u672c\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u5206\u6790\uff0c\u8868\u660e\u5f53\u524d\u5728KL\u6216\u03b1-\u6563\u5ea6\u6b63\u5219\u5316\u4e0b\u7684\u516c\u5f0f\uff0c\u4f1a\u65e0\u610f\u4e2d\u5c06\u63a2\u7d22\u504f\u5411\u53c2\u8003\u6a21\u578b\u7684\u9ad8\u6982\u7387\u533a\u57df\uff0c\u4ece\u800c\u52a0\u5f3a\u4fdd\u5b88\u884c\u4e3a\u800c\u4e0d\u662f\u4fc3\u8fdb\u4e0d\u786e\u5b9a\u533a\u57df\u7684\u53d1\u73b0\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u7f3a\u9677\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u901a\u7528\u63a2\u7d22\u6027\u5956\u52b1\uff08GEB\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u7406\u8bba\u6846\u67b6\uff0c\u53ef\u4ee5\u8bc1\u660e\u6ee1\u8db3\u4e50\u89c2\u4e3b\u4e49\u539f\u5219\u3002GEB\u901a\u8fc7\u53c2\u8003\u76f8\u5173\u7684\u5956\u52b1\u8c03\u8282\u6765\u62b5\u6d88\u7531\u6563\u5ea6\u5f15\u8d77\u7684\u504f\u5dee\uff0c\u5e76\u5c06\u5148\u524d\u7684\u542f\u53d1\u5f0f\u5956\u52b1\u7edf\u4e00\u4e3a\u7279\u6b8a\u60c5\u51b5\uff0c\u540c\u65f6\u81ea\u7136\u5730\u6269\u5c55\u5230\u6574\u4e2a\u03b1-\u6563\u5ea6\u65cf\u3002", "result": "GEB\u5728\u591a\u4e2a\u6563\u5ea6\u8bbe\u7f6e\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9aa8\u5e72\u4e0a\u7684\u5bf9\u9f50\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0cGEB\u4e3aRLHF\u4e2d\u7684\u4e50\u89c2\u63a2\u7d22\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u548c\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03969", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03969", "abs": "https://arxiv.org/abs/2510.03969", "authors": ["Chengxiao Wang", "Isha Chaudhary", "Qian Hu", "Weitong Ruan", "Rahul Gupta", "Gagandeep Singh"], "title": "Quantifying Risks in Multi-turn Conversation with Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) can produce catastrophic responses in\nconversational settings that pose serious risks to public safety and security.\nExisting evaluations often fail to fully reveal these vulnerabilities because\nthey rely on fixed attack prompt sequences, lack statistical guarantees, and do\nnot scale to the vast space of multi-turn conversations. In this work, we\npropose QRLLM, a novel, principled Certification framework for Catastrophic\nrisks in multi-turn Conversation for LLMs that bounds the probability of an LLM\ngenerating catastrophic responses under multi-turn conversation distributions\nwith statistical guarantees. We model multi-turn conversations as probability\ndistributions over query sequences, represented by a Markov process on a query\ngraph whose edges encode semantic similarity to capture realistic\nconversational flow, and quantify catastrophic risks using confidence\nintervals. We define several inexpensive and practical distributions: random\nnode, graph path, adaptive with rejection. Our results demonstrate that these\ndistributions can reveal substantial catastrophic risks in frontier models,\nwith certified lower bounds as high as 70\\% for the worst model, highlighting\nthe urgent need for improved safety training strategies in frontier LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8ba4\u8bc1\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u4ea7\u751f\u707e\u96be\u6027\u98ce\u9669\u7684\u53ef\u80fd\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u5b8c\u5168\u63ed\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6f0f\u6d1e\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7684\u653b\u51fb\u63d0\u793a\u5e8f\u5217\uff0c\u7f3a\u4e4f\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u5e76\u4e14\u65e0\u6cd5\u6269\u5c55\u5230\u591a\u8f6e\u5bf9\u8bdd\u7684\u5de8\u5927\u7a7a\u95f4\u3002", "method": "\u5c06\u591a\u8f6e\u5bf9\u8bdd\u5efa\u6a21\u4e3a\u67e5\u8be2\u5e8f\u5217\u4e0a\u7684\u6982\u7387\u5206\u5e03\uff0c\u7528\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\u8868\u793a\uff0c\u5e76\u4f7f\u7528\u7f6e\u4fe1\u533a\u95f4\u91cf\u5316\u707e\u96be\u6027\u98ce\u9669\u3002", "result": "\u5728\u6700\u5dee\u7684\u6a21\u578b\u4e2d\uff0c\u8ba4\u8bc1\u7684\u4e0b\u9650\u9ad8\u8fbe 70%\uff0c\u8868\u660e\u8feb\u5207\u9700\u8981\u6539\u8fdb\u524d\u6cbf LLM \u4e2d\u7684\u5b89\u5168\u8bad\u7ec3\u7b56\u7565\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u524d\u6cbf\u6a21\u578b\u4e2d\u6f5c\u5728\u7684\u707e\u96be\u6027\u98ce\u9669\uff0c\u5e76\u5f3a\u8c03\u4e86\u6539\u8fdb LLM \u5b89\u5168\u6027\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2510.03762", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03762", "abs": "https://arxiv.org/abs/2510.03762", "authors": ["Deshan Sumanathilaka", "Nicholas Micallef", "Julian Hough"], "title": "Prompt Balance Matters: Understanding How Imbalanced Few-Shot Learning Affects Multilingual Sense Disambiguation in LLMs", "comment": "Paper accepted at GlobalNLP 2025: Workshop on beyond English: Natural\n  Language Processing for All Languages in an Era of Large Language Models\" 9\n  pages, 3 figures, 2 Tables", "summary": "Recent advances in Large Language Models (LLMs) have significantly reshaped\nthe landscape of Natural Language Processing (NLP). Among the various prompting\ntechniques, few-shot prompting has gained considerable attention for its\npracticality and effectiveness. This study investigates how few-shot prompting\nstrategies impact the Word Sense Disambiguation (WSD) task, particularly\nfocusing on the biases introduced by imbalanced sample distributions. We use\nthe GLOSSGPT prompting method, an advanced approach for English WSD, to test\nits effectiveness across five languages: English, German, Spanish, French, and\nItalian. Our results show that imbalanced few-shot examples can cause incorrect\nsense predictions in multilingual languages, but this issue does not appear in\nEnglish. To assess model behavior, we evaluate both the GPT-4o and\nLLaMA-3.1-70B models and the results highlight the sensitivity of multilingual\nWSD to sample distribution in few-shot settings, emphasizing the need for\nbalanced and representative prompting strategies.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5c11\u6837\u672c\u63d0\u793a\u7b56\u7565\u5982\u4f55\u5f71\u54cd\u8bcd\u4e49\u6d88\u6b67 (WSD) \u4efb\u52a1\uff0c\u7279\u522b\u5173\u6ce8\u4e86\u4e0d\u5e73\u8861\u6837\u672c\u5206\u5e03\u5f15\u5165\u7684\u504f\u5dee\u3002\u4f7f\u7528 GLOSSGPT \u63d0\u793a\u65b9\u6cd5\u5728\u4e94\u79cd\u8bed\u8a00\u4e0a\u6d4b\u8bd5\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u5c11\u6837\u672c\u63d0\u793a\u6280\u672f\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u548c\u6709\u6548\u6027\uff0c\u4ee5\u53ca\u4e0d\u5e73\u8861\u6837\u672c\u5206\u5e03\u53ef\u80fd\u5f15\u5165\u7684\u504f\u5dee\u3002", "method": "\u4f7f\u7528 GLOSSGPT \u63d0\u793a\u65b9\u6cd5\uff0c\u5728\u82f1\u8bed\u3001\u5fb7\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u6cd5\u8bed\u548c\u610f\u5927\u5229\u8bed\u8fd9\u4e94\u79cd\u8bed\u8a00\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u8bc4\u4f30 GPT-4o \u548c LLaMA-3.1-70B \u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u4e0d\u5e73\u8861\u7684\u5c11\u6837\u672c\u793a\u4f8b\u4f1a\u5bfc\u81f4\u591a\u8bed\u8a00\u4e2d\u4e0d\u6b63\u786e\u7684\u8bed\u4e49\u9884\u6d4b\uff0c\u4f46\u82f1\u8bed\u4e2d\u672a\u51fa\u73b0\u6b64\u95ee\u9898\u3002", "conclusion": "\u591a\u8bed\u8a00 WSD \u5bf9\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u7684\u6837\u672c\u5206\u5e03\u654f\u611f\uff0c\u5f3a\u8c03\u9700\u8981\u5e73\u8861\u548c\u6709\u4ee3\u8868\u6027\u7684\u63d0\u793a\u7b56\u7565\u3002"}}
{"id": "2510.03540", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03540", "abs": "https://arxiv.org/abs/2510.03540", "authors": ["Manuel Schwonberg", "Hanno Gottschalk"], "title": "Domain Generalization for Semantic Segmentation: A Survey", "comment": "Accepted to CVPR2025W", "summary": "The generalization of deep neural networks to unknown domains is a major\nchallenge despite their tremendous progress in recent years. For this reason,\nthe dynamic area of domain generalization (DG) has emerged. In contrast to\nunsupervised domain adaptation, there is no access to or knowledge about the\ntarget domains, and DG methods aim to generalize across multiple different\nunseen target domains. Domain generalization is particularly relevant for the\ntask semantic segmentation which is used in several areas such as biomedicine\nor automated driving. This survey provides a comprehensive overview of the\nrapidly evolving topic of domain generalized semantic segmentation. We cluster\nand review existing approaches and identify the paradigm shift towards\nfoundation-model-based domain generalization. Finally, we provide an extensive\nperformance comparison of all approaches, which highlights the significant\ninfluence of foundation models on domain generalization. This survey seeks to\nadvance domain generalization research and inspire scientists to explore new\nresearch directions.", "AI": {"tldr": "\u672c\u7814\u7a76\u6982\u8ff0\u4e86\u9886\u57df\u6cdb\u5316\u8bed\u4e49\u5206\u5272\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u5e76\u6bd4\u8f83\u4e86\u5404\u79cd\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u672a\u77e5\u9886\u57df\u7684\u6cdb\u5316\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u9886\u57df\u6cdb\u5316\uff08DG\uff09\u65e8\u5728\u8de8\u591a\u4e2a\u4e0d\u540c\u7684\u672a\u89c1\u76ee\u6807\u9886\u57df\u8fdb\u884c\u6cdb\u5316\u3002\u9886\u57df\u6cdb\u5316\u4e0e\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u7279\u522b\u76f8\u5173\uff0c\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u7528\u4e8e\u751f\u7269\u533b\u5b66\u6216\u81ea\u52a8\u9a7e\u9a76\u7b49\u591a\u4e2a\u9886\u57df\u3002", "method": "\u5bf9\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u805a\u7c7b\u548c\u56de\u987e\uff0c\u5e76\u786e\u5b9a\u4e86\u5411\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u9886\u57df\u6cdb\u5316\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "result": "\u5bf9\u6240\u6709\u65b9\u6cd5\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u6027\u80fd\u6bd4\u8f83\uff0c\u7a81\u51fa\u4e86\u57fa\u7840\u6a21\u578b\u5bf9\u9886\u57df\u6cdb\u5316\u7684\u91cd\u5927\u5f71\u54cd\u3002", "conclusion": "\u672c\u7814\u7a76\u65e8\u5728\u63a8\u8fdb\u9886\u57df\u6cdb\u5316\u7814\u7a76\uff0c\u5e76\u6fc0\u53d1\u79d1\u5b66\u5bb6\u63a2\u7d22\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.03270", "categories": ["cs.LG", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03270", "abs": "https://arxiv.org/abs/2510.03270", "authors": ["Haolin Chen", "Shiyu Wang", "Can Qin", "Bo Pang", "Zuxin Liu", "Jielin Qiu", "Jianguo Zhang", "Yingbo Zhou", "Zeyuan Chen", "Ran Xu", "Shelby Heinecke", "Silvio Savarese", "Caiming Xiong", "Huan Wang", "Weiran Yao"], "title": "CoDA: Coding LM via Diffusion Adaptation", "comment": null, "summary": "Diffusion language models promise bidirectional context and infilling\ncapabilities that autoregressive coders lack, yet practical systems remain\nheavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU\nwith a fully open-source training pipeline. CoDA pairs large-scale diffusion\npre-training with code-centric mid-training and instruction tuning, enabling\nconfidence-guided sampling that keeps inference latency competitive. On\nHumaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses\ndiffusion models up to 7B parameters. Our release includes model checkpoints,\nevaluation harnesses, and TPU training pipelines to accelerate research on\nlightweight diffusion-based coding assistants.", "AI": {"tldr": "CoDA is a 1.7B-parameter diffusion coder.", "motivation": "Diffusion language models have bidirectional context and infilling capabilities but are heavyweight.", "method": "Large-scale diffusion pre-training with code-centric mid-training and instruction tuning, enabling confidence-guided sampling.", "result": "CoDA-1.7B-Instruct matches or surpasses diffusion models up to 7B parameters on Humaneval, MBPP, and EvalPlus.", "conclusion": "The release includes model checkpoints, evaluation harnesses, and TPU training pipelines to accelerate research on lightweight diffusion-based coding assistants."}}
{"id": "2510.04009", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04009", "abs": "https://arxiv.org/abs/2510.04009", "authors": ["Zicong He", "Boxuan Zhang", "Weihao Liu", "Ruixiang Tang", "Lu Cheng"], "title": "What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models", "comment": "22 pages", "summary": "The meteoric rise of foundation models (FMs) has expanded their capabilities\nfar beyond conventional tasks. Creativity, long regarded as a hallmark of human\nintelligence and a driver of innovation, is now increasingly recognized as a\ncritical dimension of machine intelligence in the era of generative FMs,\ncomplementing traditional measures of accuracy. However, existing evaluation\nframeworks for creativity remain fragmented, relying on ad hoc metrics not\nfirmly grounded in established theories. To address this gap, we introduce\nC^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.\nC^2-Eval distinguishes between two complementary forms of creativity:\nconvergent creativity, where tasks admit constrained solutions (e.g., code\ngeneration), and divergent creativity, where tasks are open-ended (e.g.,\nstorytelling). It evaluates both dimensions using fine-grained criteria derived\nfrom social-science theory, focusing on Usefulness, Originality, and Surprise\n(U-O-S). Through extensive experiments on leading proprietary and open-source\nmodels, we analyze trade-offs in their creative capabilities. Our results\nhighlight both the strengths and challenges of current FMs in pursuing a\ncreative machine mind, showing that C^2-Eval is an effective lens for examining\nthe evolving landscape of creative AI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aC^2-Eval\u7684\u6574\u4f53\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30FMs\u4e2d\u7684\u521b\u9020\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u521b\u9020\u529b\u8bc4\u4f30\u6846\u67b6\u4ecd\u7136\u662f\u5206\u6563\u7684\uff0c\u4f9d\u8d56\u4e8e\u672a\u7262\u56fa\u5730\u5efa\u7acb\u5728\u5df2\u5efa\u7acb\u7684\u7406\u8bba\u57fa\u7840\u4e0a\u7684\u4e34\u65f6\u6307\u6807\u3002", "method": "\u533a\u5206\u6536\u655b\u521b\u9020\u529b\uff08\u4f8b\u5982\uff0c\u4ee3\u7801\u751f\u6210\uff09\u548c\u53d1\u6563\u521b\u9020\u529b\uff08\u4f8b\u5982\uff0c\u8bb2\u6545\u4e8b\uff09\u8fd9\u4e24\u79cd\u4e92\u8865\u7684\u521b\u9020\u529b\u5f62\u5f0f\u3002\u5b83\u4f7f\u7528\u6e90\u81ea\u793e\u4f1a\u79d1\u5b66\u7406\u8bba\u7684\u7ec6\u7c92\u5ea6\u6807\u51c6\u8bc4\u4f30\u8fd9\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u4fa7\u91cd\u4e8e\u6709\u7528\u6027\u3001\u539f\u521b\u6027\u548c\u60ca\u5947\u6027\uff08U-O-S\uff09\u3002", "result": "\u901a\u8fc7\u5bf9\u9886\u5148\u7684\u4e13\u6709\u548c\u5f00\u6e90\u6a21\u578b\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u5728\u521b\u9020\u80fd\u529b\u65b9\u9762\u7684\u6743\u8861\u3002", "conclusion": "C^2-Eval\u662f\u68c0\u67e5\u521b\u9020\u6027AI\u4e0d\u65ad\u53d1\u5c55\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2510.03781", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03781", "abs": "https://arxiv.org/abs/2510.03781", "authors": ["Majid Asgari-Bidhendi", "Muhammad Amin Ghaseminia", "Alireza Shahbazi", "Sayyed Ali Hossayni", "Najmeh Torabian", "Behrouz Minaei-Bidgoli"], "title": "Rezwan: Leveraging Large Language Models for Comprehensive Hadith Text Processing: A 1.2M Corpus Development", "comment": "9 pages, 3 figures", "summary": "This paper presents the development of Rezwan, a large-scale AI-assisted\nHadith corpus comprising over 1.2M narrations, extracted and structured through\na fully automated pipeline. Building on digital repositories such as Maktabat\nAhl al-Bayt, the pipeline employs Large Language Models (LLMs) for\nsegmentation, chain--text separation, validation, and multi-layer enrichment.\nEach narration is enhanced with machine translation into twelve languages,\nintelligent diacritization, abstractive summarization, thematic tagging, and\ncross-text semantic analysis. This multi-step process transforms raw text into\na richly annotated research-ready infrastructure for digital humanities and\nIslamic studies. A rigorous evaluation was conducted on 1,213 randomly sampled\nnarrations, assessed by six domain experts. Results show near-human accuracy in\nstructured tasks such as chain--text separation (9.33/10) and summarization\n(9.33/10), while highlighting ongoing challenges in diacritization and semantic\nsimilarity detection. Comparative analysis against the manually curated Noor\nCorpus demonstrates the superiority of Najm in both scale and quality, with a\nmean overall score of 8.46/10 versus 3.66/10. Furthermore, cost analysis\nconfirms the economic feasibility of the AI approach: tasks requiring over\n229,000 hours of expert labor were completed within months at a fraction of the\ncost. The work introduces a new paradigm in religious text processing by\nshowing how AI can augment human expertise, enabling large-scale, multilingual,\nand semantically enriched access to Islamic heritage.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3a Rezwan \u7684\u5927\u578b AI \u8f85\u52a9\u5723\u8bad\u8bed\u6599\u5e93\u7684\u5f00\u53d1\uff0c\u8be5\u8bed\u6599\u5e93\u5305\u542b\u8d85\u8fc7 120 \u4e07\u6761\u53d9\u8ff0\uff0c\u8fd9\u4e9b\u53d9\u8ff0\u901a\u8fc7\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u6d41\u7a0b\u63d0\u53d6\u548c\u7ed3\u6784\u5316\u3002", "motivation": "\u6784\u5efa\u5728 Maktabat Ahl al-Bayt \u7b49\u6570\u5b57\u5b58\u50a8\u5e93\u7684\u57fa\u7840\u4e0a\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u8fdb\u884c\u5206\u5272\u3001\u94fe\u6587\u672c\u5206\u79bb\u3001\u9a8c\u8bc1\u548c\u591a\u5c42\u4e30\u5bcc\u3002", "method": "\u6bcf\u4e2a\u53d9\u8ff0\u90fd\u901a\u8fc7\u673a\u5668\u7ffb\u8bd1\u6210\u5341\u4e8c\u79cd\u8bed\u8a00\u3001\u667a\u80fd\u53d8\u97f3\u3001\u62bd\u8c61\u6982\u62ec\u3001\u4e3b\u9898\u6807\u8bb0\u548c\u8de8\u6587\u672c\u8bed\u4e49\u5206\u6790\u5f97\u5230\u589e\u5f3a\u3002", "result": "\u5bf9 1,213 \u4e2a\u968f\u673a\u62bd\u6837\u7684\u53d9\u8ff0\u8fdb\u884c\u4e86\u4e25\u683c\u7684\u8bc4\u4f30\uff0c\u7531\u516d\u4f4d\u9886\u57df\u4e13\u5bb6\u8fdb\u884c\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u94fe\u6587\u672c\u5206\u79bb (9.33/10) \u548c\u6982\u62ec (9.33/10) \u7b49\u7ed3\u6784\u5316\u4efb\u52a1\u4e2d\uff0c\u51c6\u786e\u7387\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff0c\u540c\u65f6\u7a81\u51fa\u4e86\u53d8\u97f3\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u68c0\u6d4b\u65b9\u9762\u6301\u7eed\u5b58\u5728\u7684\u6311\u6218\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5b97\u6559\u6587\u672c\u5904\u7406\u8303\u4f8b\uff0c\u5c55\u793a\u4e86 AI \u5982\u4f55\u589e\u5f3a\u4eba\u7c7b\u7684\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4ece\u800c\u5b9e\u73b0\u5927\u89c4\u6a21\u3001\u591a\u8bed\u8a00\u548c\u8bed\u4e49\u4e30\u5bcc\u7684\u4f0a\u65af\u5170\u9057\u4ea7\u8bbf\u95ee\u3002"}}
{"id": "2510.03543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03543", "abs": "https://arxiv.org/abs/2510.03543", "authors": ["Evandros Kaklamanos", "Kristjana Kristinsdottir", "Jonathan Huang", "Dustin Carlson", "Rajesh Keswani", "John Pandolfino", "Mozziyar Etemadi"], "title": "From Scope to Script: An Automated Report Generation Model for Gastrointestinal Endoscopy", "comment": null, "summary": "Endoscopic procedures such as esophagogastroduodenoscopy (EGD) and\ncolonoscopy play a critical role in diagnosing and managing gastrointestinal\n(GI) disorders. However, the documentation burden associated with these\nprocedures place significant strain on gastroenterologists, contributing to\ninefficiencies in clinical workflows and physician burnout. To address this\nchallenge, we propose a novel automated report generation model that leverages\na transformer-based vision encoder and text decoder within a two-stage training\nframework. In the first stage, both components are pre-trained on image/text\ncaption pairs to capture generalized vision-language features, followed by\nfine-tuning on images/report pairs to generate clinically meaningful findings.\nOur approach not only streamlines the documentation process but also holds\npromise for reducing physician workload and improving patient care.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u62a5\u544a\u751f\u6210\u6a21\u578b\uff0c\u65e8\u5728\u51cf\u8f7b\u80c3\u80a0\u79d1\u533b\u751f\u5728\u5185\u7aa5\u955c\u68c0\u67e5\u4e2d\u7684\u6587\u6863\u8d1f\u62c5\uff0c\u63d0\u9ad8\u4e34\u5e8a\u5de5\u4f5c\u6548\u7387\u5e76\u51cf\u5c11\u533b\u751f\u804c\u4e1a\u5026\u6020\u3002", "motivation": "\u5185\u7aa5\u955c\u68c0\u67e5\u7684\u6587\u6863\u8d1f\u62c5\u7ed9\u80c3\u80a0\u79d1\u533b\u751f\u5e26\u6765\u4e86\u5de8\u5927\u7684\u538b\u529b\uff0c\u5bfc\u81f4\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u6548\u7387\u4f4e\u4e0b\u548c\u533b\u751f\u804c\u4e1a\u5026\u6020\u3002", "method": "\u8be5\u6a21\u578b\u5229\u7528\u57fa\u4e8e Transformer \u7684\u89c6\u89c9\u7f16\u7801\u5668\u548c\u6587\u672c\u89e3\u7801\u5668\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u3002\u7b2c\u4e00\u9636\u6bb5\uff0c\u5728\u56fe\u50cf/\u6587\u672c\u6807\u9898\u5bf9\u4e0a\u9884\u8bad\u7ec3\u4e24\u4e2a\u7ec4\u4ef6\u4ee5\u6355\u83b7\u5e7f\u4e49\u89c6\u89c9\u8bed\u8a00\u7279\u5f81\uff0c\u7136\u540e\u5728\u56fe\u50cf/\u62a5\u544a\u5bf9\u4e0a\u8fdb\u884c\u5fae\u8c03\u4ee5\u751f\u6210\u4e34\u5e8a\u4e0a\u6709\u610f\u4e49\u7684\u53d1\u73b0\u3002", "result": "\u8be5\u65b9\u6cd5\u7b80\u5316\u4e86\u6587\u6863\u6d41\u7a0b\uff0c\u5e76\u6709\u671b\u51cf\u5c11\u533b\u751f\u5de5\u4f5c\u91cf\u548c\u6539\u5584\u60a3\u8005\u62a4\u7406\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u52a8\u62a5\u544a\u751f\u6210\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u51cf\u8f7b\u533b\u751f\u7684\u6587\u6863\u8d1f\u62c5\uff0c\u63d0\u9ad8\u5de5\u4f5c\u6548\u7387\uff0c\u5e76\u6700\u7ec8\u6539\u5584\u60a3\u8005\u62a4\u7406\u3002"}}
{"id": "2510.03271", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03271", "abs": "https://arxiv.org/abs/2510.03271", "authors": ["Zi Liang", "Zhiyao Wu", "Haoyang Shang", "Yulin Jin", "Qingqing Ye", "Huadi Zheng", "Peizhao Hu", "Haibo Hu"], "title": "Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary", "comment": "Source code: https://github.com/liangzid/DPS", "summary": "Decision boundary, the subspace of inputs where a machine learning model\nassigns equal classification probabilities to two classes, is pivotal in\nrevealing core model properties and interpreting behaviors. While analyzing the\ndecision boundary of large language models (LLMs) has raised increasing\nattention recently, constructing it for mainstream LLMs remains computationally\ninfeasible due to the enormous vocabulary-sequence sizes and the\nauto-regressive nature of LLMs. To address this issue, in this paper we propose\nDecision Potential Surface (DPS), a new notion for analyzing LLM decision\nboundary. DPS is defined on the confidences in distinguishing different\nsampling sequences for each input, which naturally captures the potential of\ndecision boundary. We prove that the zero-height isohypse in DPS is equivalent\nto the decision boundary of an LLM, with enclosed regions representing decision\nregions. By leveraging DPS, for the first time in the literature, we propose an\napproximate decision boundary construction algorithm, namely $K$-DPS, which\nonly requires K-finite times of sequence sampling to approximate an LLM's\ndecision boundary with negligible error. We theoretically derive the upper\nbounds for the absolute error, expected error, and the error concentration\nbetween K-DPS and the ideal DPS, demonstrating that such errors can be\ntrade-off with sampling times. Our results are empirically validated by\nextensive experiments across various LLMs and corpora.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLM\u51b3\u7b56\u8fb9\u754c\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c11\u91cf\u5e8f\u5217\u91c7\u6837\u8fd1\u4f3cLLM\u7684\u51b3\u7b56\u8fb9\u754c\u3002", "motivation": "\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u51b3\u7b56\u8fb9\u754c\u5bf9\u4e8e\u63ed\u793a\u6a21\u578b\u7684\u6838\u5fc3\u5c5e\u6027\u548c\u89e3\u91ca\u884c\u4e3a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8eLLM\u8bcd\u6c47\u91cf\u5de8\u5927\u548c\u81ea\u56de\u5f52\u7684\u7279\u6027\uff0c\u6784\u5efaLLM\u7684\u51b3\u7b56\u8fb9\u754c\u5728\u8ba1\u7b97\u4e0a\u662f\u4e0d\u53ef\u884c\u7684\u3002", "method": "\u63d0\u51fa\u4e86\u51b3\u7b56\u52bf\u8868\u9762\uff08DPS\uff09\u7684\u6982\u5ff5\uff0c\u5b83\u5b9a\u4e49\u5728\u533a\u5206\u6bcf\u4e2a\u8f93\u5165\u7684\u4e0d\u540c\u7684\u62bd\u6837\u5e8f\u5217\u7684\u7f6e\u4fe1\u5ea6\u4e0a\uff0c\u81ea\u7136\u5730\u6355\u6349\u4e86\u51b3\u7b56\u8fb9\u754c\u7684\u6f5c\u529b\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd1\u4f3c\u51b3\u7b56\u8fb9\u754c\u6784\u5efa\u7b97\u6cd5\uff0c\u5373K-DPS\u3002", "result": "\u4ece\u7406\u8bba\u4e0a\u63a8\u5bfc\u4e86K-DPS\u4e0e\u7406\u60f3DPS\u4e4b\u95f4\u7684\u7edd\u5bf9\u8bef\u5dee\u3001\u671f\u671b\u8bef\u5dee\u548c\u8bef\u5dee\u96c6\u4e2d\u5ea6\u7684\u4e0a\u754c\uff0c\u8bc1\u660e\u4e86\u8fd9\u4e9b\u8bef\u5dee\u53ef\u4ee5\u901a\u8fc7\u91c7\u6837\u65f6\u95f4\u8fdb\u884c\u6743\u8861\u3002\u901a\u8fc7\u5404\u79cdLLM\u548c\u8bed\u6599\u5e93\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u7ed3\u679c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684K-DPS\u7b97\u6cd5\u80fd\u591f\u4ee5\u53ef\u5ffd\u7565\u7684\u8bef\u5dee\u8fd1\u4f3cLLM\u7684\u51b3\u7b56\u8fb9\u754c\u3002"}}
{"id": "2510.04017", "categories": ["cs.AI", "cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.04017", "abs": "https://arxiv.org/abs/2510.04017", "authors": ["Sumanth Varambally", "Marshall Fisher", "Jas Thakker", "Yiwei Chen", "Zhirui Xia", "Yasaman Jafari", "Ruijia Niu", "Manas Jain", "Veeramakali Vignesh Manivannan", "Zachary Novack", "Luyu Han", "Srikar Eranky", "Salva R\u00fchling Cachay", "Taylor Berg-Kirkpatrick", "Duncan Watson-Parris", "Yi-An Ma", "Rose Yu"], "title": "Zephyrus: An Agentic Framework for Weather Science", "comment": null, "summary": "Foundation models for weather science are pre-trained on vast amounts of\nstructured numerical data and outperform traditional weather forecasting\nsystems. However, these models lack language-based reasoning capabilities,\nlimiting their utility in interactive scientific workflows. Large language\nmodels (LLMs) excel at understanding and generating text but cannot reason\nabout high-dimensional meteorological datasets. We bridge this gap by building\na novel agentic framework for weather science. Our framework includes a Python\ncode-based environment for agents (ZephyrusWorld) to interact with weather\ndata, featuring tools like an interface to WeatherBench 2 dataset, geoquerying\nfor geographical masks from natural language, weather forecasting, and climate\nsimulation capabilities. We design Zephyrus, a multi-turn LLM-based weather\nagent that iteratively analyzes weather datasets, observes results, and refines\nits approach through conversational feedback loops. We accompany the agent with\na new benchmark, ZephyrusBench, with a scalable data generation pipeline that\nconstructs diverse question-answer pairs across weather-related tasks, from\nbasic lookups to advanced forecasting, extreme event detection, and\ncounterfactual reasoning. Experiments on this benchmark demonstrate the strong\nperformance of Zephyrus agents over text-only baselines, outperforming them by\nup to 35 percentage points in correctness. However, on harder tasks, Zephyrus\nperforms similarly to text-only baselines, highlighting the challenging nature\nof our benchmark and suggesting promising directions for future work.", "AI": {"tldr": "\u6784\u5efa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5929\u6c14\u79d1\u5b66\u7684agentic\u6846\u67b6\uff0c\u5229\u7528LLM\u548cPython\u4ee3\u7801\u73af\u5883\u6765\u5206\u6790\u5929\u6c14\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u7684\u5929\u6c14\u9884\u62a5\u6a21\u578b\u7f3a\u4e4f\u8bed\u8a00\u63a8\u7406\u80fd\u529b\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u9ad8\u7ef4\u6c14\u8c61\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u540d\u4e3aZephyrus\u7684\u57fa\u4e8eLLM\u7684\u5929\u6c14agent\uff0c\u901a\u8fc7\u4e0e\u5929\u6c14\u6570\u636e\u4ea4\u4e92\u3001\u89c2\u5bdf\u7ed3\u679c\u548c\u5bf9\u8bdd\u53cd\u9988\u6765\u8fed\u4ee3\u5206\u6790\u3002", "result": "Zephyrus agent\u5728\u5929\u6c14\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u7eaf\u6587\u672c\u57fa\u7ebf\uff0c\u6b63\u786e\u7387\u63d0\u9ad8\u4e8635%\u3002", "conclusion": "Zephyrus\u5728\u66f4\u56f0\u96be\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0e\u7eaf\u6587\u672c\u57fa\u7ebf\u76f8\u4f3c\uff0c\u8868\u660ebenchmark\u5177\u6709\u6311\u6218\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u5de5\u4f5c\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.03799", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.03799", "abs": "https://arxiv.org/abs/2510.03799", "authors": ["Hadi Asghari", "Sami Nenno"], "title": "Mechanistic Interpretability of Socio-Political Frames in Language Models", "comment": "Peer-reviewed and presented at Advances in Interpretable Machine\n  Learning and Artificial Intelligence (AIMLAI) Workshop at ECML/PKDD 2024", "summary": "This paper explores the ability of large language models to generate and\nrecognize deep cognitive frames, particularly in socio-political contexts. We\ndemonstrate that LLMs are highly fluent in generating texts that evoke specific\nframes and can recognize these frames in zero-shot settings. Inspired by\nmechanistic interpretability research, we investigate the location of the\n`strict father' and `nurturing parent' frames within the model's hidden\nrepresentation, identifying singular dimensions that correlate strongly with\ntheir presence. Our findings contribute to understanding how LLMs capture and\nexpress meaningful human concepts.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u548c\u8bc6\u522b\u8ba4\u77e5\u6846\u67b6\uff0c\u5c24\u5176\u662f\u5728\u793e\u4f1a\u653f\u6cbb\u8bed\u5883\u4e0b\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u548c\u8bc6\u522b\u6587\u672c\u6846\u67b6\u7684\u80fd\u529b", "method": "\u901a\u8fc7prompt\u8ba9LLM\u751f\u6210\u7279\u5b9a\u6846\u67b6\u7684\u6587\u672c\uff0c\u5e76\u8fdb\u884czero-shot\u6846\u67b6\u8bc6\u522b\u3002\u53d7\u542f\u53d1\u4e8e\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u7814\u7a76\uff0c\u7814\u7a76\u8005\u63a2\u7d22\u4e86\u6a21\u578b\u9690\u85cf\u5c42\u4e2dstrict father\u548cnurturing parent\u6846\u67b6\u7684\u4f4d\u7f6e\uff0c\u5e76\u8bc6\u522b\u51fa\u4e0e\u5176\u5b58\u5728\u5bc6\u5207\u76f8\u5173\u7684\u5947\u5f02\u7ef4\u5ea6\u3002", "result": "LLM\u80fd\u591f\u6d41\u5229\u5730\u751f\u6210\u80fd\u591f\u5524\u8d77\u7279\u5b9a\u6846\u67b6\u7684\u6587\u672c\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728zero-shot\u573a\u666f\u4e2d\u8bc6\u522b\u8fd9\u4e9b\u6846\u67b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u6709\u52a9\u4e8e\u7406\u89e3LLM\u5982\u4f55\u6355\u6349\u548c\u8868\u8fbe\u6709\u610f\u4e49\u7684\u4eba\u7c7b\u6982\u5ff5\u3002"}}
{"id": "2510.03545", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03545", "abs": "https://arxiv.org/abs/2510.03545", "authors": ["Sixten Norelius", "Aaron O. Feldman", "Mac Schwager"], "title": "SketchPlan: Diffusion Based Drone Planning From Human Sketches", "comment": "Code available at https://github.com/sixnor/SketchPlan", "summary": "We propose SketchPlan, a diffusion-based planner that interprets 2D\nhand-drawn sketches over depth images to generate 3D flight paths for drone\nnavigation. SketchPlan comprises two components: a SketchAdapter that learns to\nmap the human sketches to projected 2D paths, and DiffPath, a diffusion model\nthat infers 3D trajectories from 2D projections and a first person view depth\nimage. Our model achieves zero-shot sim-to-real transfer, generating accurate\nand safe flight paths in previously unseen real-world environments. To train\nthe model, we build a synthetic dataset of 32k flight paths using a diverse set\nof photorealistic 3D Gaussian Splatting scenes. We automatically label the data\nby computing 2D projections of the 3D flight paths onto the camera plane, and\nuse this to train the DiffPath diffusion model. However, since real human 2D\nsketches differ significantly from ideal 2D projections, we additionally label\n872 of the 3D flight paths with real human sketches and use this to train the\nSketchAdapter to infer the 2D projection from the human sketch. We demonstrate\nSketchPlan's effectiveness in both simulated and real-world experiments, and\nshow through ablations that training on a mix of human labeled and auto-labeled\ndata together with a modular design significantly boosts its capabilities to\ncorrectly interpret human intent and infer 3D paths. In real-world drone tests,\nSketchPlan achieved 100\\% success in low/medium clutter and 40\\% in unseen\nhigh-clutter environments, outperforming key ablations by 20-60\\% in task\ncompletion.", "AI": {"tldr": "SketchPlan is a diffusion-based planner that uses 2D hand-drawn sketches over depth images to generate 3D flight paths for drone navigation.", "motivation": "The paper aims to enable intuitive drone navigation using hand-drawn sketches.", "method": "The method involves a SketchAdapter that maps sketches to 2D paths, and DiffPath, a diffusion model that infers 3D trajectories from 2D projections and depth images. The model is trained on a synthetic dataset with both auto-labeled and human-labeled data.", "result": "SketchPlan achieves zero-shot sim-to-real transfer and generates accurate flight paths. It achieved 100% success in low/medium clutter and 40% in unseen high-clutter environments in real-world drone tests.", "conclusion": "The paper concludes that SketchPlan effectively interprets human intent and infers 3D paths, with a modular design and training on mixed data significantly boosting its capabilities."}}
{"id": "2510.03272", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03272", "abs": "https://arxiv.org/abs/2510.03272", "authors": ["Yukun Zhang", "Xueqing Zhou"], "title": "PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling", "comment": null, "summary": "The Transformer architecture has revolutionized artificial intelligence, yet\na principled theoretical understanding of its internal mechanisms remains\nelusive. This paper introduces a novel analytical framework that\nreconceptualizes the Transformer's discrete, layered structure as a continuous\nspatiotemporal dynamical system governed by a master Partial Differential\nEquation (PDE). Within this paradigm, we map core architectural components to\ndistinct mathematical operators: self-attention as a non-local interaction, the\nfeed-forward network as a local reaction, and, critically, residual connections\nand layer normalization as indispensable stabilization mechanisms. We do not\npropose a new model, but rather employ the PDE system as a theoretical probe to\nanalyze the mathematical necessity of these components. By comparing a standard\nTransformer with a PDE simulator that lacks explicit stabilizers, our\nexperiments provide compelling empirical evidence for our central thesis. We\ndemonstrate that without residual connections, the system suffers from\ncatastrophic representational drift, while the absence of layer normalization\nleads to unstable, explosive training dynamics. Our findings reveal that these\nseemingly heuristic \"tricks\" are, in fact, fundamental mathematical stabilizers\nrequired to tame an otherwise powerful but inherently unstable continuous\nsystem. This work offers a first-principles explanation for the Transformer's\ndesign and establishes a new paradigm for analyzing deep neural networks\nthrough the lens of continuous dynamics.", "AI": {"tldr": "Transformer\u7684\u5185\u90e8\u673a\u5236\u4ecd\u7136\u96be\u4ee5\u6349\u6478\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u5206\u6790\u6846\u67b6\uff0c\u5c06Transformer\u7684\u79bb\u6563\u5206\u5c42\u7ed3\u6784\u91cd\u65b0\u6982\u5ff5\u5316\u4e3a\u7531\u4e3b\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u63a7\u5236\u7684\u8fde\u7eed\u65f6\u7a7a\u52a8\u529b\u7cfb\u7edf\u3002", "motivation": "\u5bf9Transformer\u7684\u5185\u90e8\u673a\u5236\u7f3a\u4e4f\u7406\u8bba\u7406\u89e3\u3002", "method": "\u5c06Transformer\u7684\u6838\u5fc3\u67b6\u6784\u7ec4\u4ef6\u6620\u5c04\u5230\u4e0d\u540c\u7684\u6570\u5b66\u7b97\u5b50\uff0c\u5e76\u5c06Transformer\u7684\u79bb\u6563\u5206\u5c42\u7ed3\u6784\u91cd\u65b0\u6982\u5ff5\u5316\u4e3a\u7531\u4e3b\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u63a7\u5236\u7684\u8fde\u7eed\u65f6\u7a7a\u52a8\u529b\u7cfb\u7edf\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e00\u4e2a\u6807\u51c6\u7684Transformer\u548c\u4e00\u4e2a\u7f3a\u4e4f\u663e\u5f0f\u7a33\u5b9a\u5668\u7684PDE\u6a21\u62df\u5668\uff0c", "result": "\u5982\u679c\u6ca1\u6709\u6b8b\u5dee\u8fde\u63a5\uff0c\u7cfb\u7edf\u4f1a\u906d\u53d7\u707e\u96be\u6027\u7684\u8868\u5f81\u6f02\u79fb\uff0c\u800c\u5982\u679c\u6ca1\u6709\u5c42\u5f52\u4e00\u5316\uff0c\u4f1a\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u3001\u7206\u70b8\u6027\u7684\u8bad\u7ec3\u52a8\u6001\u3002", "conclusion": "\u6b8b\u5dee\u8fde\u63a5\u548c\u5c42\u5f52\u4e00\u5316\u662f\u9a6f\u670dTransformer\u8fd9\u79cd\u5f3a\u5927\u4f46\u4e0d\u7a33\u5b9a\u7684\u8fde\u7eed\u7cfb\u7edf\u6240\u9700\u7684\u57fa\u672c\u6570\u5b66\u7a33\u5b9a\u5668\u3002"}}
{"id": "2510.04023", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04023", "abs": "https://arxiv.org/abs/2510.04023", "authors": ["Mizanur Rahman", "Amran Bhuiyan", "Mohammed Saidul Islam", "Md Tahmid Rahman Laskar", "Ridwan Mahbub", "Ahmed Masry", "Shafiq Joty", "Enamul Hoque"], "title": "LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions", "comment": "Survey paper; 45 data science agents; under review", "summary": "Recent advances in large language models (LLMs) have enabled a new class of\nAI agents that automate multiple stages of the data science workflow by\nintegrating planning, tool use, and multimodal reasoning across text, code,\ntables, and visuals. This survey presents the first comprehensive,\nlifecycle-aligned taxonomy of data science agents, systematically analyzing and\nmapping forty-five systems onto the six stages of the end-to-end data science\nprocess: business understanding and data acquisition, exploratory analysis and\nvisualization, feature engineering, model building and selection,\ninterpretation and explanation, and deployment and monitoring. In addition to\nlifecycle coverage, we annotate each agent along five cross-cutting design\ndimensions: reasoning and planning style, modality integration, tool\norchestration depth, learning and alignment methods, and trust, safety, and\ngovernance mechanisms. Beyond classification, we provide a critical synthesis\nof agent capabilities, highlight strengths and limitations at each stage, and\nreview emerging benchmarks and evaluation practices. Our analysis identifies\nthree key trends: most systems emphasize exploratory analysis, visualization,\nand modeling while neglecting business understanding, deployment, and\nmonitoring; multimodal reasoning and tool orchestration remain unresolved\nchallenges; and over 90% lack explicit trust and safety mechanisms. We conclude\nby outlining open challenges in alignment stability, explainability,\ngovernance, and robust evaluation frameworks, and propose future research\ndirections to guide the development of robust, trustworthy, low-latency,\ntransparent, and broadly accessible data science agents.", "AI": {"tldr": "\u672c\u8c03\u67e5\u5168\u9762\u5206\u6790\u4e86\u6570\u636e\u79d1\u5b66\u4ee3\u7406\uff0c\u6839\u636e\u7aef\u5230\u7aef\u6570\u636e\u79d1\u5b66\u6d41\u7a0b\u7684\u516d\u4e2a\u9636\u6bb5\u5bf9 45 \u4e2a\u7cfb\u7edf\u8fdb\u884c\u4e86\u7cfb\u7edf\u5206\u6790\u548c\u6620\u5c04\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u8fdb\u5c55\u50ac\u751f\u4e86\u4e00\u7c7b\u65b0\u7684 AI \u4ee3\u7406\uff0c\u8fd9\u4e9b\u4ee3\u7406\u901a\u8fc7\u6574\u5408\u8de8\u6587\u672c\u3001\u4ee3\u7801\u3001\u8868\u683c\u548c\u89c6\u89c9\u7684\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\u548c\u591a\u6a21\u5f0f\u63a8\u7406\uff0c\u4ece\u800c\u81ea\u52a8\u6267\u884c\u6570\u636e\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\u7684\u591a\u4e2a\u9636\u6bb5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u7684\u7b2c\u4e00\u4e2a\u5168\u9762\u7684\u3001\u4e0e\u751f\u547d\u5468\u671f\u5bf9\u9f50\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u6cbf\u7740\u4e94\u4e2a\u6a2a\u5411\u8bbe\u8ba1\u7ef4\u5ea6\u5bf9\u6bcf\u4e2a\u4ee3\u7406\u8fdb\u884c\u6ce8\u91ca\uff1a\u63a8\u7406\u548c\u89c4\u5212\u98ce\u683c\u3001\u6a21\u6001\u96c6\u6210\u3001\u5de5\u5177\u7f16\u6392\u6df1\u5ea6\u3001\u5b66\u4e60\u548c\u5bf9\u9f50\u65b9\u6cd5\u4ee5\u53ca\u4fe1\u4efb\u3001\u5b89\u5168\u548c\u6cbb\u7406\u673a\u5236\u3002", "result": "\u5927\u591a\u6570\u7cfb\u7edf\u5f3a\u8c03\u63a2\u7d22\u6027\u5206\u6790\u3001\u53ef\u89c6\u5316\u548c\u5efa\u6a21\uff0c\u800c\u5ffd\u7565\u4e86\u4e1a\u52a1\u7406\u89e3\u3001\u90e8\u7f72\u548c\u76d1\u63a7\uff1b\u591a\u6a21\u6001\u63a8\u7406\u548c\u5de5\u5177\u7f16\u6392\u4ecd\u7136\u662f\u672a\u89e3\u51b3\u7684\u6311\u6218\uff1b\u8d85\u8fc7 90% \u7684\u7cfb\u7edf\u7f3a\u4e4f\u660e\u786e\u7684\u4fe1\u4efb\u548c\u5b89\u5168\u673a\u5236\u3002", "conclusion": "\u603b\u7ed3\u4e86\u5bf9\u9f50\u7a33\u5b9a\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u6cbb\u7406\u548c\u7a33\u5065\u7684\u8bc4\u4f30\u6846\u67b6\u65b9\u9762\u7684\u672a\u89e3\u51b3\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u6307\u5bfc\u5f00\u53d1\u7a33\u5065\u3001\u503c\u5f97\u4fe1\u8d56\u3001\u4f4e\u5ef6\u8fdf\u3001\u900f\u660e\u4e14\u5e7f\u6cdb\u53ef\u8bbf\u95ee\u7684\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u3002"}}
{"id": "2510.03805", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03805", "abs": "https://arxiv.org/abs/2510.03805", "authors": ["Canhui Wu", "Qiong Cao", "Chang Li", "Zhenfang Wang", "Chao Xue", "Yuwei Fan", "Wei Xi", "Xiaodong He"], "title": "Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models", "comment": "20pages, 7 figures", "summary": "Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks\nbut often suffer from excessive verbosity, known as \"overthinking.\" Existing\nsolutions via reinforcement learning (RL) typically penalize generated tokens\nto promote conciseness. However, these methods encounter two challenges:\nresponses with fewer tokens do not always correspond to fewer reasoning steps,\nand models may develop hacking behavior in later stages of training by\ndiscarding reasoning steps to minimize token usage. In this work, we introduce\n\\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more\nefficient reasoning by favoring compact reasoning steps. Our step-aware reward\nfunction prioritizes correctness while imposing penalties for redundant steps,\nand withholds rewards for incorrect responses to prevent the reinforcement of\nerroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when\nthe length of any output step exceeds the upper limit, we halt updates to\nprevent hacking behavior caused by merging steps. Extensive experiments across\nfour reasoning benchmarks demonstrate that SP achieves state-of-the-art\naccuracy while significantly reducing response length. For instance, on AIME24,\nSP reduces token usage by \\textbf{69.7\\%}.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Step Pruner (SP) \u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b (LRM) \u4e2d\u8fc7\u5ea6\u5197\u4f59\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u60e9\u7f5a\u751f\u6210\u7684 token \u6765\u4fc3\u8fdb\u7b80\u6d01\u6027\uff0c\u4f46\u8fd9\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1atoken \u6570\u91cf\u5c11\u5e76\u4e0d\u603b\u662f\u610f\u5473\u7740\u63a8\u7406\u6b65\u9aa4\u5c11\uff0c\u5e76\u4e14\u6a21\u578b\u53ef\u80fd\u4f1a\u901a\u8fc7\u4e22\u5f03\u63a8\u7406\u6b65\u9aa4\u6765\u6700\u5c0f\u5316 token \u4f7f\u7528\u91cf\uff0c\u4ece\u800c\u4ea7\u751f hacking \u884c\u4e3a\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86 Step Pruner (SP)\uff0c\u5b83\u901a\u8fc7\u652f\u6301\u7d27\u51d1\u7684\u63a8\u7406\u6b65\u9aa4\u6765\u5f15\u5bfc LRM \u8fdb\u884c\u66f4\u6709\u6548\u7684\u63a8\u7406\u3002SP \u7684 step-aware \u5956\u52b1\u51fd\u6570\u4f18\u5148\u8003\u8651\u6b63\u786e\u6027\uff0c\u540c\u65f6\u5bf9\u5197\u4f59\u6b65\u9aa4\u8fdb\u884c\u60e9\u7f5a\uff0c\u5e76\u5bf9\u4e0d\u6b63\u786e\u7684\u54cd\u5e94\u4e0d\u4e88\u5956\u52b1\uff0c\u4ee5\u9632\u6b62\u9519\u8bef\u7684\u63a8\u7406\u5f97\u5230\u5f3a\u5316\u3002\u6b64\u5916\uff0c\u8be5\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u505c\u6b62\u673a\u5236\uff1a\u5f53\u4efb\u4f55\u8f93\u51fa\u6b65\u9aa4\u7684\u957f\u5ea6\u8d85\u8fc7\u4e0a\u9650\u65f6\uff0c\u505c\u6b62\u66f4\u65b0\uff0c\u4ee5\u9632\u6b62\u56e0\u5408\u5e76\u6b65\u9aa4\u800c\u5bfc\u81f4\u7684 hacking \u884c\u4e3a\u3002", "result": "\u5728\u56db\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSP \u5728\u663e\u7740\u51cf\u5c11\u54cd\u5e94\u957f\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\u3002\u4f8b\u5982\uff0c\u5728 AIME24 \u4e0a\uff0cSP \u5c06 token \u4f7f\u7528\u91cf\u51cf\u5c11\u4e86 69.7%\u3002", "conclusion": "SP \u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u51cf\u5c11\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5197\u4f59\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u9ad8\u51c6\u786e\u6027\u3002"}}
{"id": "2510.03548", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03548", "abs": "https://arxiv.org/abs/2510.03548", "authors": ["Danial Samadi Vahdati", "Tai Duc Nguyen", "Ekta Prashnani", "Koki Nagano", "David Luebke", "Orazio Gallo", "Matthew Stamm"], "title": "Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing", "comment": null, "summary": "AI-based talking-head videoconferencing systems reduce bandwidth by sending a\ncompact pose-expression latent and re-synthesizing RGB at the receiver, but\nthis latent can be puppeteered, letting an attacker hijack a victim's likeness\nin real time. Because every frame is synthetic, deepfake and synthetic video\ndetectors fail outright. To address this security problem, we exploit a key\nobservation: the pose-expression latent inherently contains biometric\ninformation of the driving identity. Therefore, we introduce the first\nbiometric leakage defense without ever looking at the reconstructed RGB video:\na pose-conditioned, large-margin contrastive encoder that isolates persistent\nidentity cues inside the transmitted latent while cancelling transient pose and\nexpression. A simple cosine test on this disentangled embedding flags illicit\nidentity swaps as the video is rendered. Our experiments on multiple\ntalking-head generation models show that our method consistently outperforms\nexisting puppeteering defenses, operates in real-time, and shows strong\ngeneralization to out-of-distribution scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u7269\u7279\u5f81\u6cc4\u9732\u9632\u5fa1\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u57fa\u4e8eAI\u7684\u8bf4\u8bdd\u5934\u89c6\u9891\u4f1a\u8bae\u7cfb\u7edf\u4e2d\u7684\u8eab\u4efd\u76d7\u7528\u653b\u51fb\u3002", "motivation": "\u73b0\u6709\u7684AI\u8bf4\u8bdd\u5934\u89c6\u9891\u4f1a\u8bae\u7cfb\u7edf\u5b58\u5728\u88ab\u653b\u51fb\u8005\u5229\u7528\u8fdb\u884c\u5b9e\u65f6\u8eab\u4efd\u76d7\u7528\u7684\u5b89\u5168\u95ee\u9898\uff0c\u4e14\u73b0\u6709\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u5931\u6548\u3002", "method": "\u5229\u7528\u59ff\u6001-\u8868\u60c5\u6f5c\u5728\u7a7a\u95f4\u5305\u542b\u9a71\u52a8\u8eab\u4efd\u7684\u751f\u7269\u7279\u5f81\u4fe1\u606f\u8fd9\u4e00\u5173\u952e\u89c2\u5bdf\uff0c\u63d0\u51fa\u4e00\u79cd\u59ff\u6001\u6761\u4ef6\u3001\u5927\u95f4\u9694\u5bf9\u6bd4\u7f16\u7801\u5668\uff0c\u7528\u4e8e\u9694\u79bb\u4f20\u8f93\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u6301\u4e45\u8eab\u4efd\u7ebf\u7d22\uff0c\u540c\u65f6\u6d88\u9664\u77ac\u6001\u59ff\u6001\u548c\u8868\u60c5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u5080\u5121\u9632\u5fa1\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u65f6\u8fd0\u884c\uff0c\u5e76\u5bf9\u5206\u5e03\u5916\u573a\u666f\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u8bf4\u8bdd\u5934\u89c6\u9891\u4f1a\u8bae\u7cfb\u7edf\u4e2d\u7684\u8eab\u4efd\u76d7\u7528\u653b\u51fb\uff0c\u4e14\u5177\u6709\u5b9e\u65f6\u6027\u548c\u6cdb\u5316\u6027\u3002"}}
{"id": "2510.03273", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03273", "abs": "https://arxiv.org/abs/2510.03273", "authors": ["Chenhao Ye", "Ming Tang"], "title": "Learning without Global Backpropagation via Synergistic Information Distillation", "comment": null, "summary": "Backpropagation (BP), while foundational to deep learning, imposes two\ncritical scalability bottlenecks: update locking, where network modules remain\nidle until the entire backward pass completes, and high memory consumption due\nto storing activations for gradient computation. To address these limitations,\nwe introduce Synergistic Information Distillation (SID), a novel training\nframework that reframes deep learning as a cascade of local cooperative\nrefinement problems. In SID, a deep network is structured as a pipeline of\nmodules, each imposed with a local objective to refine a probabilistic belief\nabout the ground-truth target. This objective balances fidelity to the target\nwith consistency to the belief from its preceding module. By decoupling the\nbackward dependencies between modules, SID enables parallel training and hence\neliminates update locking and drastically reduces memory requirements.\nMeanwhile, this design preserves the standard feed-forward inference pass,\nmaking SID a versatile drop-in replacement for BP. We provide a theoretical\nfoundation, proving that SID guarantees monotonic performance improvement with\nnetwork depth. Empirically, SID consistently matches or surpasses the\nclassification accuracy of BP, exhibiting superior scalability and pronounced\nrobustness to label noise.Code is available at:\nhttps://github.com/ychAlbert/sid-bp", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u534f\u540c\u4fe1\u606f\u84b8\u998f\uff08SID\uff09\u7684\u65b0\u8bad\u7ec3\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u53cd\u5411\u4f20\u64ad\uff08BP\uff09\u7684\u53ef\u6269\u5c55\u6027\u74f6\u9888\u95ee\u9898\uff0c\u5982\u66f4\u65b0\u9501\u5b9a\u548c\u9ad8\u5185\u5b58\u6d88\u8017\u3002", "motivation": "\u53cd\u5411\u4f20\u64ad\uff08BP\uff09\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u7684\u53ef\u6269\u5c55\u6027\u74f6\u9888\uff1a\u66f4\u65b0\u9501\u5b9a\u548c\u9ad8\u5185\u5b58\u6d88\u8017\u3002", "method": "\u5c06\u6df1\u5ea6\u5b66\u4e60\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5c40\u90e8\u534f\u540c\u7ec6\u5316\u95ee\u9898\u7684\u7ea7\u8054\u3002\u5728SID\u4e2d\uff0c\u4e00\u4e2a\u6df1\u5ea6\u7f51\u7edc\u88ab\u6784\u5efa\u4e3a\u6a21\u5757\u7684\u6d41\u6c34\u7ebf\uff0c\u6bcf\u4e2a\u6a21\u5757\u90fd\u6709\u4e00\u4e2a\u5c40\u90e8\u76ee\u6807\uff0c\u4ee5\u6539\u8fdb\u5173\u4e8eground-truth\u76ee\u6807\u7684\u6982\u7387\u4fe1\u5ff5\u3002\u8be5\u76ee\u6807\u5e73\u8861\u4e86\u5bf9\u76ee\u6807\u7684\u4fdd\u771f\u5ea6\u548c\u4e0e\u5176\u524d\u4e00\u4e2a\u6a21\u5757\u7684\u4fe1\u5ff5\u7684\u4e00\u81f4\u6027\u3002", "result": "SID\u5728\u5206\u7c7b\u7cbe\u5ea6\u4e0a\u4e0eBP\u76f8\u5339\u914d\u6216\u8d85\u8fc7BP\uff0c\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u53ef\u6269\u5c55\u6027\u548c\u5bf9\u6807\u7b7e\u566a\u58f0\u7684\u663e\u8457\u9c81\u68d2\u6027\u3002", "conclusion": "SID\u662f\u4e00\u79cd\u901a\u7528\u7684BP\u66ff\u4ee3\u65b9\u6848\uff0c\u5b83\u4fdd\u8bc1\u4e86\u7f51\u7edc\u6df1\u5ea6\u5e26\u6765\u7684\u5355\u8c03\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e14\u5728\u7ecf\u9a8c\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u53ef\u6269\u5c55\u6027\u548c\u5bf9\u6807\u7b7e\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.04033", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04033", "abs": "https://arxiv.org/abs/2510.04033", "authors": ["Ayush Noori", "Adam Rodman", "Alan Karthikesalingam", "Bilal A. Mateen", "Christopher A. Longhurst", "Daniel Yang", "Dave deBronkart", "Gauden Galea", "Harold F. Wolf III", "Jacob Waxman", "Joshua C. Mandel", "Juliana Rotich", "Kenneth D. Mandl", "Maryam Mustafa", "Melissa Miles", "Nigam H. Shah", "Peter Lee", "Robert Korom", "Scott Mahoney", "Seth Hain", "Tien Yin Wong", "Trevor Mundel", "Vivek Natarajan", "Noa Dagan", "David A. Clifton", "Ran D. Balicer", "Isaac S. Kohane", "Marinka Zitnik"], "title": "A global log for medical AI", "comment": null, "summary": "Modern computer systems often rely on syslog, a simple, universal protocol\nthat records every critical event across heterogeneous infrastructure. However,\nhealthcare's rapidly growing clinical AI stack has no equivalent. As hospitals\nrush to pilot large language models and other AI-based clinical decision\nsupport tools, we still lack a standard way to record how, when, by whom, and\nfor whom these AI models are used. Without that transparency and visibility, it\nis challenging to measure real-world performance and outcomes, detect adverse\nevents, or correct bias or dataset drift. In the spirit of syslog, we introduce\nMedLog, a protocol for event-level logging of clinical AI. Any time an AI model\nis invoked to interact with a human, interface with another algorithm, or act\nindependently, a MedLog record is created. This record consists of nine core\nfields: header, model, user, target, inputs, artifacts, outputs, outcomes, and\nfeedback, providing a structured and consistent record of model activity. To\nencourage early adoption, especially in low-resource settings, and minimize the\ndata footprint, MedLog supports risk-based sampling, lifecycle-aware retention\npolicies, and write-behind caching; detailed traces for complex, agentic, or\nmulti-stage workflows can also be captured under MedLog. MedLog can catalyze\nthe development of new databases and software to store and analyze MedLog\nrecords. Realizing this vision would enable continuous surveillance, auditing,\nand iterative improvement of medical AI, laying the foundation for a new form\nof digital epidemiology.", "AI": {"tldr": "\u4ecb\u7ecdMedLog\uff0c\u4e00\u79cd\u7528\u4e8e\u4e34\u5e8aAI\u4e8b\u4ef6\u7ea7\u522b\u65e5\u5fd7\u8bb0\u5f55\u7684\u534f\u8bae\uff0c\u7c7b\u4f3c\u4e8esyslog\u3002", "motivation": "\u533b\u7597\u4fdd\u5065\u9886\u57df\u7f3a\u4e4f\u8bb0\u5f55AI\u6a21\u578b\u4f7f\u7528\u60c5\u51b5\u7684\u6807\u51c6\u65b9\u6cd5\uff0c\u963b\u788d\u4e86\u6027\u80fd\u8bc4\u4f30\u3001\u95ee\u9898\u68c0\u6d4b\u548c\u504f\u5dee\u7ea0\u6b63\u3002", "method": "\u63d0\u51faMedLog\u534f\u8bae\uff0c\u6bcf\u6b21AI\u6a21\u578b\u88ab\u8c03\u7528\u65f6\u521b\u5efa\u4e00\u4e2a\u5305\u542b\u4e5d\u4e2a\u6838\u5fc3\u5b57\u6bb5\u7684\u8bb0\u5f55\u3002", "result": "MedLog\u652f\u6301\u98ce\u9669\u91c7\u6837\u3001\u751f\u547d\u5468\u671f\u611f\u77e5\u4fdd\u7559\u7b56\u7565\u548c\u5199\u540e\u7f13\u5b58\uff0c\u4ee5\u9f13\u52b1\u65e9\u671f\u91c7\u7528\u5e76\u6700\u5c0f\u5316\u6570\u636e\u5360\u7528\u3002", "conclusion": "MedLog\u7684\u5b9e\u73b0\u80fd\u591f\u6301\u7eed\u76d1\u6d4b\u3001\u5ba1\u8ba1\u548c\u8fed\u4ee3\u6539\u8fdb\u533b\u7597AI\uff0c\u4e3a\u65b0\u578b\u6570\u5b57\u6d41\u884c\u75c5\u5b66\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2510.03808", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03808", "abs": "https://arxiv.org/abs/2510.03808", "authors": ["Mehedi Hasan Emon"], "title": "Annotate Rhetorical Relations with INCEpTION: A Comparison with Automatic Approaches", "comment": null, "summary": "This research explores the annotation of rhetorical relations in discourse\nusing the INCEpTION tool and compares manual annotation with automatic\napproaches based on large language models. The study focuses on sports reports\n(specifically cricket news) and evaluates the performance of BERT, DistilBERT,\nand Logistic Regression models in classifying rhetorical relations such as\nelaboration, contrast, background, and cause-effect. The results show that\nDistilBERT achieved the highest accuracy, highlighting its potential for\nefficient discourse relation prediction. This work contributes to the growing\nintersection of discourse parsing and transformer-based NLP. (This paper was\nconducted as part of an academic requirement under the supervision of Prof. Dr.\nRalf Klabunde, Linguistic Data Science Lab, Ruhr University Bochum.) Keywords:\nRhetorical Structure Theory, INCEpTION, BERT, DistilBERT, Discourse Parsing,\nNLP.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528INCEpTION\u5de5\u5177\u6ce8\u91ca\u8bed\u7bc7\u4e2d\u7684\u4fee\u8f9e\u5173\u7cfb\uff0c\u5e76\u5c06\u4eba\u5de5\u6ce8\u91ca\u4e0e\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5206\u7c7b\u8bed\u7bc7\u4fee\u8f9e\u5173\u7cfb\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u5e76\u63a8\u52a8\u8bed\u7bc7\u5206\u6790\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u4ea4\u53c9\u53d1\u5c55\u3002", "method": "\u8be5\u7814\u7a76\u4f7f\u7528INCEpTION\u5de5\u5177\u8fdb\u884c\u8bed\u7bc7\u4fee\u8f9e\u5173\u7cfb\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u5e76\u91c7\u7528BERT\u3001DistilBERT\u548cLogistic Regression\u6a21\u578b\u5bf9\u677f\u7403\u65b0\u95fb\u4e2d\u7684\u4fee\u8f9e\u5173\u7cfb\uff08\u5982\u9610\u8ff0\u3001\u5bf9\u6bd4\u3001\u80cc\u666f\u548c\u56e0\u679c\u5173\u7cfb\uff09\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cDistilBERT\u6a21\u578b\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u51c6\u786e\u7387\u3002", "conclusion": "DistilBERT\u5728\u8bed\u7bc7\u5173\u7cfb\u9884\u6d4b\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u672c\u7814\u7a76\u5bf9\u8bed\u7bc7\u5206\u6790\u548c\u57fa\u4e8eTransformer\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u4ea4\u53c9\u9886\u57df\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2510.03550", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03550", "abs": "https://arxiv.org/abs/2510.03550", "authors": ["Junbao Zhou", "Yuan Zhou", "Kesen Zhao", "Qingshan Xu", "Beier Zhu", "Richang Hong", "Hanwang Zhang"], "title": "Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!", "comment": null, "summary": "Achieving streaming, fine-grained control over the outputs of autoregressive\nvideo diffusion models remains challenging, making it difficult to ensure that\nthey consistently align with user expectations. To bridge this gap, we propose\n\\textbf{stReaming drag-oriEnted interactiVe vidEo manipuLation (REVEL)}, a new\ntask that enables users to modify generated videos \\emph{anytime} on\n\\emph{anything} via fine-grained, interactive drag. Beyond DragVideo and\nSG-I2V, REVEL unifies drag-style video manipulation as editing and animating\nvideo frames with both supporting user-specified translation, deformation, and\nrotation effects, making drag operations versatile. In resolving REVEL, we\nobserve: \\emph{i}) drag-induced perturbations accumulate in latent space,\ncausing severe latent distribution drift that halts the drag process;\n\\emph{ii}) streaming drag is easily disturbed by context frames, thereby\nyielding visually unnatural outcomes. We thus propose a training-free approach,\n\\textbf{DragStream}, comprising: \\emph{i}) an adaptive distribution\nself-rectification strategy that leverages neighboring frames' statistics to\neffectively constrain the drift of latent embeddings; \\emph{ii}) a\nspatial-frequency selective optimization mechanism, allowing the model to fully\nexploit contextual information while mitigating its interference via\nselectively propagating visual cues along generation. Our method can be\nseamlessly integrated into existing autoregressive video diffusion models, and\nextensive experiments firmly demonstrate the effectiveness of our DragStream.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u7f16\u8f91\u4efb\u52a1REVEL\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u7cbe\u7ec6\u7684\u4ea4\u4e92\u5f0f\u62d6\u52a8\u968f\u65f6\u4fee\u6539\u751f\u6210\u7684\u89c6\u9891\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u96be\u4ee5\u5b9e\u73b0\u6d41\u5f0f\u3001\u7ec6\u7c92\u5ea6\u7684\u8f93\u51fa\u63a7\u5236\uff0c\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u4e0e\u7528\u6237\u671f\u671b\u4e0d\u7b26\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDragStream\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5305\u542b\u81ea\u9002\u5e94\u5206\u5e03\u81ea\u6821\u6b63\u7b56\u7565\u548c\u7a7a\u95f4\u9891\u7387\u9009\u62e9\u6027\u4f18\u5316\u673a\u5236\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86DragStream\u7684\u6709\u6548\u6027\u3002", "conclusion": "DragStream\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7684\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u3002"}}
{"id": "2510.03274", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03274", "abs": "https://arxiv.org/abs/2510.03274", "authors": ["Tianao Zhang", "Zhiteng Li", "Xianglong Yan", "Haotong Qin", "Yong Guo", "Yulun Zhang"], "title": "Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models", "comment": null, "summary": "Diffusion large language models (dLLMs), which offer bidirectional context\nand flexible masked-denoising generation, are emerging as a compelling\nalternative to autoregressive (AR) LLMs. However, like AR LLMs, their model\nsizes continue to grow, motivating weight compression for deployment. Although\npost-training quantization (PTQ) is effective for AR LLMs, directly\ntransferring it to dLLMs at 2-bit leads to unsatisfactory performance. To\ntackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework\ntailored to dLLMs. Since masked-denoising activations in dLLMs differ from the\nfully visible signals assumed by standard PTQ methods, we introduce Masked\nCalibration Simulation (MCS) to align calibration with the timestep-dependent\nmasking, which yields more reliable calibrations. Moreover, we propose a\nData-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight\nrepresentations via an optimization algorithm. It performs iterative\napproximation guided by our simulated calibration data. In addition, under a\nstrict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a\nsensitivity-based precision allocation scheme that adaptively assigns bit width\nacross channel groups. When restricted to 2-bit precision, Quant-dLLM\nconsistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer\nPTQ methods on dLLMs. The code and models will be available at:\nhttps://github.com/ZTA2785/Quant-dLLM.", "AI": {"tldr": "Quant-dLLM: A novel quantization framework for diffusion large language models (dLLMs) that achieves better performance than directly applying post-training quantization (PTQ) methods.", "motivation": "Weight compression is needed for deploying large dLLMs, but directly applying PTQ at 2-bit leads to poor performance.", "method": "Proposes Masked Calibration Simulation (MCS) to align calibration with timestep-dependent masking and a Data-aware Any-order Quantizer (DAQ) to learn ultra-low-bit weight representations. Also introduces Adaptive Blockwise Mixed Precision (ABMP) for precision allocation.", "result": "Quant-dLLM achieves higher accuracy than state-of-the-art AR-transfer PTQ methods on dLLMs at 2-bit precision.", "conclusion": "Quant-dLLM is an effective ultra-low-bit PTQ framework tailored for dLLMs."}}
{"id": "2510.04040", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04040", "abs": "https://arxiv.org/abs/2510.04040", "authors": ["Xu Shen", "Song Wang", "Zhen Tan", "Laura Yao", "Xinyu Zhao", "Kaidi Xu", "Xin Wang", "Tianlong Chen"], "title": "FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning", "comment": null, "summary": "Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT)\nprompting to improve problem-solving and provide seemingly transparent\nexplanations. However, growing evidence shows that CoT often fail to faithfully\nrepresent the underlying reasoning process, raising concerns about their\nreliability in high-risk applications. Although prior studies have focused on\nmechanism-level analyses showing that CoTs can be unfaithful, they leave open\nthe practical challenge of deciding whether a specific trajectory is faithful\nto the internal reasoning of the model. To address this gap, we introduce\nFaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness\ndetection. Our framework establishes a rigorous task formulation that\nformulates unfaithfulness detection as a discriminative decision problem, and\nprovides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an\nexpert-annotated collection of over 1,000 trajectories generated by four\nrepresentative LLMs across four domains, including more than 300 unfaithful\ninstances with fine-grained causes and step-level evidence. We further conduct\na systematic evaluation of eleven representative detection methods spanning\ncounterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical\ninsights that clarify the strengths and weaknesses of existing approaches and\nreveal the increased challenges of detection in knowledge-intensive domains and\nwith more advanced models. To the best of our knowledge, FaithCoT-Bench\nestablishes the first comprehensive benchmark for instance-level CoT\nfaithfulness, setting a solid basis for future research toward more\ninterpretable and trustworthy reasoning in LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86FaithCoT-Bench\uff0c\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4bCoT\u63a8\u7406\u4e0d\u5fe0\u5b9e\u6027\u7684\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684CoT\u63a8\u7406\u8fc7\u7a0b\u53ef\u80fd\u4e0d\u5fe0\u5b9e\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u96be\u4ee5\u5224\u65ad\u7279\u5b9a\u8f68\u8ff9\u662f\u5426\u5fe0\u5b9e\u4e8e\u6a21\u578b\u5185\u90e8\u63a8\u7406\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6FaithCoT-Bench\uff0c\u5e76\u5c06\u4e0d\u5fe0\u5b9e\u6027\u68c0\u6d4b\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u5224\u522b\u51b3\u7b56\u95ee\u9898\u3002\u521b\u5efa\u4e86\u4e00\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u6570\u636e\u96c6FINE-CoT\uff0c\u5305\u542b1000\u591a\u4e2a\u8f68\u8ff9\uff0c\u5305\u62ec300\u591a\u4e2a\u4e0d\u5fe0\u5b9e\u7684\u5b9e\u4f8b\uff0c\u5e76\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u539f\u56e0\u548c\u6b65\u9aa4\u7ea7\u522b\u7684\u8bc1\u636e\u3002", "result": "\u5bf911\u79cd\u4ee3\u8868\u6027\u7684\u68c0\u6d4b\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u8868\u660e\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u548c\u66f4\u5148\u8fdb\u7684\u6a21\u578b\u4e2d\uff0c\u68c0\u6d4b\u7684\u6311\u6218\u66f4\u5927\u3002", "conclusion": "FaithCoT-Bench\u662f\u9996\u4e2a\u5168\u9762\u7684\u5b9e\u4f8b\u7ea7\u522bCoT\u5fe0\u5b9e\u6027\u57fa\u51c6\uff0c\u4e3a\u672a\u6765\u5728LLM\u4e2d\u5b9e\u73b0\u66f4\u53ef\u89e3\u91ca\u548c\u53ef\u4fe1\u7684\u63a8\u7406\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2510.03898", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03898", "abs": "https://arxiv.org/abs/2510.03898", "authors": ["Nusrat Jahan Lia", "Shubhashis Roy Dipta", "Abdullah Khan Zehady", "Naymul Islam", "Madhusodan Chakraborty", "Abdullah Al Wasif"], "title": "Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles", "comment": null, "summary": "Detecting media bias is crucial, specifically in the South Asian region.\nDespite this, annotated datasets and computational studies for Bangla political\nbias research remain scarce. Crucially because, political stance detection in\nBangla news requires understanding of linguistic cues, cultural context, subtle\nbiases, rhetorical strategies, code-switching, implicit sentiment, and\nsocio-political background. To address this, we introduce the first benchmark\ndataset of 200 politically significant and highly debated Bangla news articles,\nlabeled for government-leaning, government-critique, and neutral stances,\nalongside diagnostic analyses for evaluating large language models (LLMs). Our\ncomprehensive evaluation of 28 proprietary and open-source LLMs shows strong\nperformance in detecting government-critique content (F1 up to 0.83) but\nsubstantial difficulty with neutral articles (F1 as low as 0.00). Models also\ntend to over-predict government-leaning stances, often misinterpreting\nambiguous narratives. This dataset and its associated diagnostics provide a\nfoundation for advancing stance detection in Bangla media research and offer\ninsights for improving LLM performance in low-resource languages.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u9996\u4e2a\u5b5f\u52a0\u62c9\u8bed\u653f\u6cbb\u7acb\u573a\u68c0\u6d4b\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u68c0\u6d4b\u5b5f\u52a0\u62c9\u8bed\u65b0\u95fb\u4e2d\u7684\u653f\u6cbb\u504f\u89c1\u7684\u80fd\u529b\u3002", "motivation": "\u68c0\u6d4b\u5a92\u4f53\u504f\u89c1\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u5357\u4e9a\u5730\u533a\u3002\u7136\u800c\uff0c\u9488\u5bf9\u5b5f\u52a0\u62c9\u8bed\u653f\u6cbb\u504f\u89c1\u7814\u7a76\u7684\u5e26\u6ce8\u91ca\u6570\u636e\u96c6\u548c\u8ba1\u7b97\u7814\u7a76\u4ecd\u7136\u7a00\u7f3a\u3002\u5b5f\u52a0\u62c9\u8bed\u65b0\u95fb\u4e2d\u7684\u653f\u6cbb\u7acb\u573a\u68c0\u6d4b\u9700\u8981\u7406\u89e3\u8bed\u8a00\u7ebf\u7d22\u3001\u6587\u5316\u80cc\u666f\u3001\u5fae\u5999\u504f\u89c1\u3001\u4fee\u8f9e\u7b56\u7565\u3001\u8bed\u7801\u8f6c\u6362\u3001\u9690\u542b\u60c5\u611f\u548c\u793e\u4f1a\u653f\u6cbb\u80cc\u666f\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b200\u7bc7\u5177\u6709\u653f\u6cbb\u610f\u4e49\u4e14\u5907\u53d7\u4e89\u8bae\u7684\u5b5f\u52a0\u62c9\u8bed\u65b0\u95fb\u6587\u7ae0\u7684\u6570\u636e\u96c6\uff0c\u5e76\u6807\u6ce8\u4e3a\u503e\u5411\u653f\u5e9c\u3001\u6279\u8bc4\u653f\u5e9c\u548c\u4e2d\u7acb\u7acb\u573a\u3002\u5bf928\u4e2a\u4e13\u6709\u548c\u5f00\u6e90LLM\u8fdb\u884c\u4e86\u7efc\u5408\u8bc4\u4f30\u3002", "result": "\u5728\u68c0\u6d4b\u6279\u8bc4\u653f\u5e9c\u7684\u5185\u5bb9\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff08F1\u9ad8\u8fbe0.83\uff09\uff0c\u4f46\u5728\u4e2d\u7acb\u6587\u7ae0\u65b9\u9762\u5b58\u5728\u5f88\u5927\u56f0\u96be\uff08F1\u4f4e\u81f30.00\uff09\u3002\u6a21\u578b\u8fd8\u503e\u5411\u4e8e\u8fc7\u5ea6\u9884\u6d4b\u503e\u5411\u653f\u5e9c\u7684\u7acb\u573a\uff0c\u7ecf\u5e38\u9519\u8bef\u5730\u89e3\u91ca\u6a21\u7cca\u7684\u53d9\u8ff0\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u53ca\u5176\u76f8\u5173\u8bca\u65ad\u4e3a\u63a8\u8fdb\u5b5f\u52a0\u62c9\u5a92\u4f53\u7814\u7a76\u4e2d\u7684\u7acb\u573a\u68c0\u6d4b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u63d0\u9ad8LLM\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2510.03555", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03555", "abs": "https://arxiv.org/abs/2510.03555", "authors": ["Peiran Quan", "Zifan Gu", "Zhuo Zhao", "Qin Zhou", "Donghan M. Yang", "Ruichen Rong", "Yang Xie", "Guanghua Xiao"], "title": "GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis", "comment": null, "summary": "Foundation models (FMs) have transformed computational pathology by providing\npowerful, general-purpose feature extractors. However, adapting and\nbenchmarking individual FMs for specific diagnostic tasks is often\ntime-consuming and resource-intensive, especially given their scale and\ndiversity. To address this challenge, we introduce Group-Aggregative Selection\nMulti-Instance Learning (GAS-MIL), a flexible ensemble framework that\nseamlessly integrates features from multiple FMs, preserving their\ncomplementary strengths without requiring manual feature selection or extensive\ntask-specific fine-tuning. Across classification tasks in three cancer\ndatasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL\nconsistently achieves superior or on-par performance relative to individual FMs\nand established MIL methods, demonstrating its robustness and generalizability.\nBy enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines\nmodel deployment for pathology and provides a scalable foundation for future\nmultimodal and precision oncology applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86GAS-MIL\uff0c\u4e00\u4e2a\u7075\u6d3b\u7684\u96c6\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u6765\u81ea\u591a\u4e2aFMs\u7684\u7279\u5f81\uff0c\u65e0\u9700\u624b\u52a8\u7279\u5f81\u9009\u62e9\u6216\u8fdb\u884c\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u5fae\u8c03\u3002", "motivation": "\u8c03\u6574\u548c\u57fa\u51c6\u6d4b\u8bd5\u7528\u4e8e\u7279\u5b9a\u8bca\u65ad\u4efb\u52a1\u7684\u5355\u4e2aFMs\u901a\u5e38\u8017\u65f6\u4e14\u8d44\u6e90\u5bc6\u96c6\uff0c\u5c24\u5176\u662f\u5728\u8003\u8651\u5230\u5b83\u4eec\u7684\u89c4\u6a21\u548c\u591a\u6837\u6027\u7684\u60c5\u51b5\u4e0b\u3002", "method": "GAS-MIL\uff0c\u4e00\u4e2a\u7075\u6d3b\u7684\u96c6\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u6765\u81ea\u591a\u4e2aFMs\u7684\u7279\u5f81\uff0c\u4fdd\u7559\u5b83\u4eec\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u800c\u65e0\u9700\u624b\u52a8\u7279\u5f81\u9009\u62e9\u6216\u8fdb\u884c\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u5fae\u8c03\u3002", "result": "\u5728\u4e09\u4e2a\u764c\u75c7\u6570\u636e\u96c6\uff08\u524d\u5217\u817a\u764c\uff08PANDA\uff09\u3001\u5375\u5de2\u764c\uff08UBC-OCEAN\uff09\u548c\u4e73\u817a\u764c\uff08TCGA-BrCa\uff09\uff09\u7684\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u76f8\u5bf9\u4e8e\u5355\u4e2aFMs\u548c\u5df2\u5efa\u7acb\u7684MIL\u65b9\u6cd5\uff0cGAS-MIL\u59cb\u7ec8\u5b9e\u73b0\u5353\u8d8a\u6216\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u7a33\u5065\u6027\u548c\u6cdb\u5316\u6027\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u73b0\u5f02\u6784FMs\u7684\u6709\u6548\u96c6\u6210\uff0cGAS-MIL\u7b80\u5316\u4e86\u75c5\u7406\u5b66\u6a21\u578b\u90e8\u7f72\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u591a\u6a21\u6001\u548c\u7cbe\u786e\u80bf\u7624\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2510.03275", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03275", "abs": "https://arxiv.org/abs/2510.03275", "authors": ["Junhao Xia", "Ming Zhao", "Limin Xiao", "Xiujun Zhang"], "title": "SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size", "comment": null, "summary": "Large language models (LLMs) face significant computational and memory\nchallenges, making extremely low-bit quantization crucial for their efficient\ndeployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for\n1-bit LLMs of any size, a novel framework that enables extremely low-bit\nquantization of LLMs while preserving their linguistic reasoning capabilities.\nA distinctive feature of SDQ-LLM is the continuous adjustability of the\nOver-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM\nconstraints by selecting fractional OSR (e.g. 2.5 times) for an optimal\ntrade-off between model size and accuracy. SDQ-LLM uses upsampling combined\nwith Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding\nhigh-precision parameters into 1-bit or 1.58-bit representations, replacing the\nmultiplication operations within linear layers with addition. This approach\nsignificantly enhances inference efficiency under extremely low-bit\nquantization. To further reduce the loss of quantization precision, we\nincorporate Hadamard-based weight smoothing prior to quantization, improving\nthe stability and robustness of the weight representations. Furthermore, to\nfully leverage the continuity of the OSR and reduce precision loss, recognizing\nthe correlation between quantization sensitivity and weight variance, we\npropose a fine-grained, layer- and linear-wise OSR allocation strategy,\nMultiOSR. This strategy distributes OSR both across layers and within each\nlayer, based on weight variance and parameter scale. Finally, extensive\nexperiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a\nmore efficient and high-precision performance even under highly aggressive\nlow-OSR settings. Our code is available at\nhttps://github.com/Dreamlittlecat/LLM-Quant-Factory.", "AI": {"tldr": "SDQ-LLM: A novel framework for extremely low-bit quantization of LLMs, preserving linguistic reasoning capabilities.", "motivation": "To address the computational and memory challenges faced by large language models (LLMs), making extremely low-bit quantization crucial for their efficient deployment.", "method": "Introduces Sigma-Delta Quantization (SDQ-LLM) with continuous adjustability of the Over-Sampling Ratio (OSR), Hadamard-based weight smoothing, and a fine-grained, layer- and linear-wise OSR allocation strategy (MultiOSR).", "result": "SDQ-LLM achieves efficient and high-precision performance on OPT and LLaMA model families, even under highly aggressive low-OSR settings.", "conclusion": "SDQ-LLM enables extremely low-bit quantization of LLMs while preserving their linguistic reasoning capabilities, offering an optimal trade-off between model size and accuracy."}}
{"id": "2510.04048", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04048", "abs": "https://arxiv.org/abs/2510.04048", "authors": ["Aparna Nair-Kanneganti", "Trevor J. Chan", "Shir Goldfinger", "Emily Mackay", "Brian Anthony", "Alison Pouch"], "title": "Increasing LLM response trustworthiness using voting ensembles", "comment": null, "summary": "Despite huge advances, LLMs still lack convenient and reliable methods to\nquantify the uncertainty in their responses, making them difficult to trust in\nhigh-stakes applications. One of the simplest approaches to eliciting more\naccurate answers is to select the mode of many responses, a technique known as\nensembling. In this work, we expand on typical ensembling approaches by looking\nat ensembles with a variable voting threshold. We introduce a theoretical\nframework for question answering and show that, by permitting ensembles to\n\"abstain\" from providing an answer when the dominant response falls short of\nthe threshold, it is possible to dramatically increase the trustworthiness of\nthe remaining answers. From this framework, we derive theoretical results as\nwell as report experimental results on two problem domains: arithmetic problem\nsolving and clinical-note question-answering. In both domains, we observe that\nlarge gains in answer trustworthiness can be achieved using highly restrictive\nvoting ensembles, while incurring relatively modest reductions in response\nyield and accuracy. Due to this quality, voting ensembles may be particularly\nuseful in applications - such as healthcare and data annotation - that require\na high degree of certainty but which may not require that every question\nreceive an automated answer.", "AI": {"tldr": "LLMs lack uncertainty quantification, making them untrustworthy. This paper introduces a voting ensemble method with a variable threshold to improve answer trustworthiness by allowing abstention.", "motivation": "LLMs' inability to quantify uncertainty limits their use in high-stakes applications.", "method": "The paper proposes a voting ensemble approach with a variable threshold, allowing the ensemble to abstain when confidence is low.", "result": "The method significantly increases answer trustworthiness with modest reductions in response yield and accuracy in arithmetic problem solving and clinical-note question-answering.", "conclusion": "Voting ensembles are useful in applications requiring high certainty, such as healthcare and data annotation, even if not every question is answered."}}
{"id": "2510.03913", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03913", "abs": "https://arxiv.org/abs/2510.03913", "authors": ["Mohammad Amin Abbasi", "Hassan Naderi"], "title": "PsycholexTherapy: Simulating Reasoning in Psychotherapy with Small Language Models in Persian", "comment": null, "summary": "This study presents PsychoLexTherapy, a framework for simulating\npsychotherapeutic reasoning in Persian using small language models (SLMs). The\nframework tackles the challenge of developing culturally grounded,\ntherapeutically coherent dialogue systems with structured memory for multi-turn\ninteractions in underrepresented languages. To ensure privacy and feasibility,\nPsychoLexTherapy is optimized for on-device deployment, enabling use without\nexternal servers. Development followed a three-stage process: (i) assessing\nSLMs psychological knowledge with PsychoLexEval; (ii) designing and\nimplementing the reasoning-oriented PsychoLexTherapy framework; and (iii)\nconstructing two evaluation datasets-PsychoLexQuery (real Persian user\nquestions) and PsychoLexDialogue (hybrid simulated sessions)-to benchmark\nagainst multiple baselines. Experiments compared simple prompting, multi-agent\ndebate, and structured therapeutic reasoning paths. Results showed that\ndeliberate model selection balanced accuracy, efficiency, and privacy. On\nPsychoLexQuery, PsychoLexTherapy outperformed all baselines in automatic\nLLM-as-a-judge evaluation and was ranked highest by human evaluators in a\nsingle-turn preference study. In multi-turn tests with PsychoLexDialogue, the\nlong-term memory module proved essential: while naive history concatenation\ncaused incoherence and information loss, the full framework achieved the\nhighest ratings in empathy, coherence, cultural fit, and personalization.\nOverall, PsychoLexTherapy establishes a practical, privacy-preserving, and\nculturally aligned foundation for Persian psychotherapy simulation,\ncontributing novel datasets, a reproducible evaluation pipeline, and empirical\ninsights into structured memory for therapeutic reasoning.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86PsychoLexTherapy\uff0c\u4e00\u4e2a\u4f7f\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u6a21\u62df\u6ce2\u65af\u8bed\u5fc3\u7406\u6cbb\u7597\u63a8\u7406\u7684\u6846\u67b6\uff0c\u4e13\u4e3a\u5728\u8d44\u6e90\u4e0d\u8db3\u7684\u8bed\u8a00\u4e2d\u5f00\u53d1\u5177\u6709\u6587\u5316\u57fa\u7840\u7684\u3001\u6cbb\u7597\u4e0a\u8fde\u8d2f\u7684\u5bf9\u8bdd\u7cfb\u7edf\u800c\u8bbe\u8ba1\uff0c\u5e76\u4f18\u5316\u4e86\u8bbe\u5907\u7aef\u90e8\u7f72\uff0c\u4fdd\u969c\u9690\u79c1\u3002", "motivation": "\u5728\u8d44\u6e90\u4e0d\u8db3\u7684\u8bed\u8a00\u4e2d\uff0c\u5f00\u53d1\u5177\u6709\u6587\u5316\u57fa\u7840\u4e14\u6cbb\u7597\u4e0a\u8fde\u8d2f\u7684\u5bf9\u8bdd\u7cfb\u7edf\u662f\u4e00\u4e2a\u6311\u6218\u3002PsychoLexTherapy\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7279\u522b\u5173\u6ce8\u6ce2\u65af\u8bed\u5fc3\u7406\u6cbb\u7597\u7684\u6a21\u62df\u3002", "method": "\u8be5\u7814\u7a76\u5206\u4e09\u4e2a\u9636\u6bb5\u8fdb\u884c\uff1a(i) \u4f7f\u7528PsychoLexEval\u8bc4\u4f30SLM\u7684\u5fc3\u7406\u5b66\u77e5\u8bc6\uff1b(ii) \u8bbe\u8ba1\u5e76\u5b9e\u65bd\u9762\u5411\u63a8\u7406\u7684PsychoLexTherapy\u6846\u67b6\uff1b(iii) \u6784\u5efa\u4e24\u4e2a\u8bc4\u4f30\u6570\u636e\u96c6\uff08PsychoLexQuery\u548cPsychoLexDialogue\uff09\u4ee5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002\u91c7\u7528\u4e86\u7b80\u5355\u63d0\u793a\u3001\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u548c\u7ed3\u6784\u5316\u6cbb\u7597\u63a8\u7406\u8def\u5f84\u7b49\u591a\u79cd\u5b9e\u9a8c\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8c28\u614e\u7684\u6a21\u578b\u9009\u62e9\u53ef\u4ee5\u5e73\u8861\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u9690\u79c1\u3002PsychoLexTherapy\u5728PsychoLexQuery\u4e0a\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\uff0c\u5e76\u5728\u591a\u8f6e\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5171\u60c5\u3001\u8fde\u8d2f\u6027\u3001\u6587\u5316\u9002\u5e94\u6027\u548c\u4e2a\u6027\u5316\u3002", "conclusion": "PsychoLexTherapy\u4e3a\u6ce2\u65af\u8bed\u5fc3\u7406\u6cbb\u7597\u6a21\u62df\u5960\u5b9a\u4e86\u5b9e\u7528\u3001\u4fdd\u62a4\u9690\u79c1\u548c\u6587\u5316\u5bf9\u9f50\u7684\u57fa\u7840\uff0c\u8d21\u732e\u4e86\u65b0\u7684\u6570\u636e\u96c6\u3001\u53ef\u91cd\u590d\u7684\u8bc4\u4f30\u6d41\u7a0b\u4ee5\u53ca\u5173\u4e8e\u6cbb\u7597\u63a8\u7406\u7ed3\u6784\u5316\u8bb0\u5fc6\u7684\u5b9e\u8bc1\u89c1\u89e3\u3002"}}
{"id": "2510.03558", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03558", "abs": "https://arxiv.org/abs/2510.03558", "authors": ["Shen Chang", "Renran Tian", "Nicole Adams", "Nan Kong"], "title": "Real-Time Assessment of Bystander Situation Awareness in Drone-Assisted First Aid", "comment": null, "summary": "Rapid naloxone delivery via drones offers a promising solution for responding\nto opioid overdose emergencies (OOEs), by extending lifesaving interventions to\nmedically untrained bystanders before emergency medical services (EMS) arrive.\nRecognizing the critical role of bystander situational awareness (SA) in\nhuman-autonomy teaming (HAT), we address a key research gap in real-time SA\nassessment by introducing the Drone-Assisted Naloxone Delivery Simulation\nDataset (DANDSD). This pioneering dataset captures HAT during simulated OOEs,\nwhere college students without medical training act as bystanders tasked with\nadministering intranasal naloxone to a mock overdose victim. Leveraging this\ndataset, we propose a video-based real-time SA assessment framework that\nutilizes graph embeddings and transformer models to assess bystander SA in real\ntime. Our approach integrates visual perception and comprehension cues--such as\ngeometric, kinematic, and interaction graph features--and achieves\nhigh-performance SA prediction. It also demonstrates strong temporal\nsegmentation accuracy, outperforming the FINCH baseline by 9% in Mean over\nFrames (MoF) and 5% in Intersection over Union (IoU). This work supports the\ndevelopment of adaptive drone systems capable of guiding bystanders\neffectively, ultimately improving emergency response outcomes and saving lives.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5229\u7528\u65e0\u4eba\u673a\u5feb\u901f\u8fd0\u9001\u7eb3\u6d1b\u916e\u7684\u65b9\u6848\uff0c\u4ee5\u5e94\u5bf9\u963f\u7247\u7c7b\u836f\u7269\u8fc7\u91cf\u7d27\u6025\u60c5\u51b5\uff0c\u5e76\u5728\u6025\u6551\u670d\u52a1\u5230\u8fbe\u4e4b\u524d\uff0c\u4e3a\u672a\u7ecf\u533b\u7597\u57f9\u8bad\u7684\u65c1\u89c2\u8005\u63d0\u4f9b\u6551\u751f\u5e72\u9884\u3002", "motivation": "\u7814\u7a76\u65c1\u89c2\u8005\u60c5\u5883\u610f\u8bc6\uff08SA\uff09\u5728\u4eba\u673a\u534f\u4f5c\uff08HAT\uff09\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u89e3\u51b3\u5b9e\u65f6SA\u8bc4\u4f30\u4e2d\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u65e0\u4eba\u673a\u8f85\u52a9\u7eb3\u6d1b\u916e\u9012\u9001\u6a21\u62df\u6570\u636e\u96c6\uff08DANDSD\uff09\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u7684\u5b9e\u65f6SA\u8bc4\u4f30\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u56fe\u5d4c\u5165\u548cTransformer\u6a21\u578b\u6765\u8bc4\u4f30\u65c1\u89c2\u8005\u7684SA\u3002", "result": "\u8be5\u65b9\u6cd5\u5728SA\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u9ad8\u6027\u80fd\uff0c\u5e76\u5728\u65f6\u95f4\u5206\u5272\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8eFINCH\u57fa\u7ebf9%\uff08MoF\uff09\u548c5%\uff08IoU\uff09\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u652f\u6301\u5f00\u53d1\u80fd\u591f\u6709\u6548\u6307\u5bfc\u65c1\u89c2\u8005\u7684\u81ea\u9002\u5e94\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u6700\u7ec8\u6539\u5584\u7d27\u6025\u54cd\u5e94\u7ed3\u679c\u5e76\u633d\u6551\u751f\u547d\u3002"}}
{"id": "2510.03276", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03276", "abs": "https://arxiv.org/abs/2510.03276", "authors": ["Qian Chen", "Linxin Yang", "Akang Wang", "Xiaodong Luo", "Yin Zhang"], "title": "QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "The combination of linear transformations and non-linear activation functions\nforms the foundation of most modern deep neural networks, enabling them to\napproximate highly complex functions. This paper explores the introduction of\nquadratic transformations to further increase nonlinearity in neural networks,\nwith the aim of enhancing the performance of existing architectures. To reduce\nparameter complexity and computational complexity, we propose a lightweight\nquadratic enhancer that uses low-rankness, weight sharing, and sparsification\ntechniques. For a fixed architecture, the proposed approach introduces\nquadratic interactions between features at every layer, while only adding\nnegligible amounts of additional model parameters and forward computations. We\nconduct a set of proof-of-concept experiments for the proposed method across\nthree tasks: image classification, text classification, and fine-tuning\nlarge-language models. In all tasks, the proposed approach demonstrates clear\nand substantial performance gains.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u4e8c\u6b21\u589e\u5f3a\u5668\uff0c\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u4e8c\u6b21\u53d8\u6362\u6765\u589e\u52a0\u795e\u7ecf\u7f51\u7edc\u7684\u975e\u7ebf\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u73b0\u6709\u67b6\u6784\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u589e\u5f3a\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u6027\u80fd\uff0c\u8bba\u6587\u63a2\u7d22\u4e86\u5f15\u5165\u4e8c\u6b21\u53d8\u6362\u4ee5\u589e\u52a0\u975e\u7ebf\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u4e8c\u6b21\u589e\u5f3a\u5668\uff0c\u5b83\u5229\u7528\u4f4e\u79e9\u6027\u3001\u6743\u91cd\u5171\u4eab\u548c\u7a00\u758f\u5316\u6280\u672f\u6765\u964d\u4f4e\u53c2\u6570\u590d\u6742\u5ea6\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u8be5\u65b9\u6cd5\u5728\u6bcf\u4e00\u5c42\u5f15\u5165\u7279\u5f81\u4e4b\u95f4\u7684\u4e8c\u6b21\u4ea4\u4e92\uff0c\u540c\u65f6\u53ea\u589e\u52a0\u5c11\u91cf\u7684\u6a21\u578b\u53c2\u6570\u548c\u524d\u5411\u8ba1\u7b97\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u6587\u672c\u5206\u7c7b\u548c\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e09\u4e2a\u4efb\u52a1\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u90fd\u8868\u73b0\u51fa\u660e\u663e\u4e14\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u90fd\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u80fd\u591f\u5e26\u6765\u6e05\u6670\u4e14\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.04051", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04051", "abs": "https://arxiv.org/abs/2510.04051", "authors": ["Lele Liao", "Qile Zhang", "Ruofan Wu", "Guanhua Fang"], "title": "Toward a unified framework for data-efficient evaluation of large language models", "comment": "codes available at https://github.com/Rorschach1989/efficient-lm-eval", "summary": "Evaluating large language models (LLMs) on comprehensive benchmarks is a\ncornerstone of their development, yet it's often computationally and\nfinancially prohibitive. While Item Response Theory (IRT) offers a promising\npath toward data-efficient evaluation by disentangling model capability from\nitem difficulty, existing IRT-based methods are hampered by significant\nlimitations. They are typically restricted to binary correctness metrics,\nfailing to natively handle the continuous scores used in generative tasks, and\nthey operate on single benchmarks, ignoring valuable structural knowledge like\ncorrelations across different metrics or benchmarks. To overcome these\nchallenges, we introduce LEGO-IRT, a unified and flexible framework for\ndata-efficient LLM evaluation. LEGO-IRT's novel design natively supports both\nbinary and continuous evaluation metrics. Moreover, it introduces a factorized\narchitecture to explicitly model and leverage structural knowledge, decomposing\nmodel ability estimates into a general component and structure-specific (e.g.,\nper-metric or per-benchmark) components. Through extensive experiments\ninvolving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves\nstable capability estimates using just $3\\%$ of the total evaluation items. We\ndemonstrate that incorporating structural knowledge reduces estimation error by\nup to $10\\%$ and reveal that the latent abilities estimated by our framework\nmay align more closely with human preferences.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLM\u8bc4\u4f30\u6846\u67b6LEGO-IRT\uff0c\u65e8\u5728\u63d0\u9ad8\u6570\u636e\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eIRT\u7684\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f8b\u5982\u901a\u5e38\u4ec5\u9650\u4e8e\u4e8c\u5143\u6b63\u786e\u6027\u6307\u6807\uff0c\u65e0\u6cd5\u5904\u7406\u751f\u6210\u4efb\u52a1\u4e2d\u4f7f\u7528\u7684\u8fde\u7eed\u5206\u6570\uff0c\u5e76\u4e14\u5ffd\u7565\u4e86\u4e0d\u540c\u6307\u6807\u6216\u57fa\u51c6\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u7b49\u7ed3\u6784\u6027\u77e5\u8bc6\u3002", "method": "LEGO-IRT\u539f\u751f\u652f\u6301\u4e8c\u5143\u548c\u8fde\u7eed\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5f15\u5165\u5206\u89e3\u67b6\u6784\u6765\u663e\u5f0f\u5efa\u6a21\u548c\u5229\u7528\u7ed3\u6784\u6027\u77e5\u8bc6\uff0c\u5c06\u6a21\u578b\u80fd\u529b\u4f30\u8ba1\u5206\u89e3\u4e3a\u4e00\u822c\u7ec4\u4ef6\u548c\u7279\u5b9a\u4e8e\u7ed3\u6784\u7684\u7ec4\u4ef6\u3002", "result": "LEGO-IRT\u4ec5\u4f7f\u7528\u603b\u8bc4\u4f30\u9879\u76ee\u76843\uff05\u5373\u53ef\u5b9e\u73b0\u7a33\u5b9a\u7684\u80fd\u529b\u4f30\u8ba1\uff0c\u4e14\u7ed3\u5408\u7ed3\u6784\u77e5\u8bc6\u53ef\u5c06\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e\u591a\u8fbe10\uff05\u3002", "conclusion": "LEGO-IRT\u6846\u67b6\u4f30\u8ba1\u7684\u6f5c\u5728\u80fd\u529b\u53ef\u80fd\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3002"}}
{"id": "2510.03997", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03997", "abs": "https://arxiv.org/abs/2510.03997", "authors": ["Junjie Luo", "Rui Han", "Arshana Welivita", "Zeleikun Di", "Jingfu Wu", "Xuzhe Zhi", "Ritu Agarwal", "Gordon Gao"], "title": "Mapping Patient-Perceived Physician Traits from Nationwide Online Reviews with LLMs", "comment": null, "summary": "Understanding how patients perceive their physicians is essential to\nimproving trust, communication, and satisfaction. We present a large language\nmodel (LLM)-based pipeline that infers Big Five personality traits and five\npatient-oriented subjective judgments. The analysis encompasses 4.1 million\npatient reviews of 226,999 U.S. physicians from an initial pool of one million.\nWe validate the method through multi-model comparison and human expert\nbenchmarking, achieving strong agreement between human and LLM assessments\n(correlation coefficients 0.72-0.89) and external validity through correlations\nwith patient satisfaction (r = 0.41-0.81, all p<0.001). National-scale analysis\nreveals systematic patterns: male physicians receive higher ratings across all\ntraits, with largest disparities in clinical competence perceptions;\nempathy-related traits predominate in pediatrics and psychiatry; and all traits\npositively predict overall satisfaction. Cluster analysis identifies four\ndistinct physician archetypes, from \"Well-Rounded Excellent\" (33.8%, uniformly\nhigh traits) to \"Underperforming\" (22.6%, consistently low). These findings\ndemonstrate that automated trait extraction from patient narratives can provide\ninterpretable, validated metrics for understanding physician-patient\nrelationships at scale, with implications for quality measurement, bias\ndetection, and workforce development in healthcare.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5206\u6790\u4e86410\u4e07\u60a3\u8005\u5bf922.7\u4e07\u7f8e\u56fd\u533b\u751f\u7684\u8bc4\u4ef7\uff0c\u4ee5\u63a8\u65ad\u533b\u751f\u7684\u4eba\u683c\u7279\u8d28\u548c\u60a3\u8005\u7684\u4e3b\u89c2\u5224\u65ad\u3002", "motivation": "\u4e86\u89e3\u60a3\u8005\u5982\u4f55\u770b\u5f85\u533b\u751f\u5bf9\u4e8e\u6539\u5584\u4fe1\u4efb\u3001\u6c9f\u901a\u548c\u6ee1\u610f\u5ea6\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u6d41\u7a0b\uff0c\u7528\u4e8e\u63a8\u65ad\u533b\u751f\u7684Big Five\u4eba\u683c\u7279\u8d28\u548c\u4e94\u4e2a\u4ee5\u60a3\u8005\u4e3a\u5bfc\u5411\u7684\u4e3b\u89c2\u5224\u65ad\u3002\u901a\u8fc7\u591a\u6a21\u578b\u6bd4\u8f83\u548c\u4eba\u5de5\u4e13\u5bb6\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7537\u6027\u533b\u751f\u5728\u6240\u6709\u7279\u8d28\u4e0a\u90fd\u83b7\u5f97\u66f4\u9ad8\u7684\u8bc4\u4ef7\uff0c\u5c24\u5176\u5728\u4e34\u5e8a\u80fd\u529b\u65b9\u9762\uff1b\u513f\u79d1\u548c\u7cbe\u795e\u79d1\u533b\u751f\u5728\u5171\u60c5\u76f8\u5173\u7279\u8d28\u4e0a\u8868\u73b0\u7a81\u51fa\uff1b\u6240\u6709\u7279\u8d28\u90fd\u6b63\u5411\u9884\u6d4b\u60a3\u8005\u7684\u603b\u4f53\u6ee1\u610f\u5ea6\u3002\u805a\u7c7b\u5206\u6790\u8bc6\u522b\u51fa\u56db\u79cd\u4e0d\u540c\u7684\u533b\u751f\u7c7b\u578b\u3002", "conclusion": "\u4ece\u60a3\u8005\u53d9\u8ff0\u4e2d\u81ea\u52a8\u63d0\u53d6\u7279\u8d28\u53ef\u4ee5\u4e3a\u5927\u89c4\u6a21\u7406\u89e3\u533b\u60a3\u5173\u7cfb\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u3001\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u6307\u6807\uff0c\u5bf9\u533b\u7597\u4fdd\u5065\u4e2d\u7684\u8d28\u91cf\u8bc4\u4f30\u3001\u504f\u89c1\u68c0\u6d4b\u548c\u52b3\u52a8\u529b\u53d1\u5c55\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.03570", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03570", "abs": "https://arxiv.org/abs/2510.03570", "authors": ["Mayimunah Nagayi", "Alice Khan", "Tamryn Frank", "Rina Swart", "Clement Nyirenda"], "title": "Evaluating OCR performance on food packaging labels in South Africa", "comment": "17 pages", "summary": "This study evaluates four open-source Optical Character Recognition (OCR)\nsystems which are Tesseract, EasyOCR, PaddleOCR, and TrOCR on real world food\npackaging images. The aim is to assess their ability to extract ingredient\nlists and nutrition facts panels. Accurate OCR for packaging is important for\ncompliance and nutrition monitoring but is challenging due to multilingual\ntext, dense layouts, varied fonts, glare, and curved surfaces. A dataset of 231\nproducts (1,628 images) was processed by all four models to assess speed and\ncoverage, and a ground truth subset of 113 images (60 products) was created for\naccuracy evaluation. Metrics include Character Error Rate (CER), Word Error\nRate (WER), BLEU, ROUGE-L, F1, coverage, and execution time. On the ground\ntruth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU\n(0.245). EasyOCR provided a good balance between accuracy and multilingual\nsupport. PaddleOCR achieved near complete coverage but was slower because it\nran on CPU only due to GPU incompatibility, and TrOCR produced the weakest\nresults despite GPU acceleration. These results provide a packaging-specific\nbenchmark, establish a baseline, and highlight directions for layout-aware\nmethods and text localization.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cd\u5f00\u6e90OCR\u7cfb\u7edf\uff08Tesseract\u3001EasyOCR\u3001PaddleOCR\u548cTrOCR\uff09\u5728\u771f\u5b9e\u98df\u54c1\u5305\u88c5\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\uff0c\u65e8\u5728\u8bc4\u4f30\u5b83\u4eec\u63d0\u53d6\u6210\u5206\u5217\u8868\u548c\u8425\u517b\u6210\u5206\u8868\u7684\u80fd\u529b\u3002", "motivation": "\u98df\u54c1\u5305\u88c5\u7684\u51c6\u786eOCR\u5bf9\u4e8e\u5408\u89c4\u6027\u548c\u8425\u517b\u76d1\u6d4b\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u591a\u8bed\u8a00\u6587\u672c\u3001\u5bc6\u96c6\u7684\u5e03\u5c40\u3001\u5404\u79cd\u5b57\u4f53\u3001\u7729\u5149\u548c\u5f2f\u66f2\u7684\u8868\u9762\u800c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u5305\u542b231\u79cd\u4ea7\u54c1\uff081,628\u5f20\u56fe\u50cf\uff09\u7684\u6570\u636e\u96c6\u5904\u7406\u6240\u6709\u56db\u4e2a\u6a21\u578b\uff0c\u4ee5\u8bc4\u4f30\u901f\u5ea6\u548c\u8986\u76d6\u7387\u3002\u521b\u5efa\u4e86113\u5f20\u56fe\u50cf\uff0860\u79cd\u4ea7\u54c1\uff09\u7684ground truth\u5b50\u96c6\uff0c\u7528\u4e8e\u51c6\u786e\u6027\u8bc4\u4f30\u3002\u6307\u6807\u5305\u62ec\u5b57\u7b26\u9519\u8bef\u7387\uff08CER\uff09\u3001\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u3001BLEU\u3001ROUGE-L\u3001F1\u3001\u8986\u76d6\u7387\u548c\u6267\u884c\u65f6\u95f4\u3002", "result": "\u5728ground truth\u5b50\u96c6\u4e0a\uff0cTesseract\u5b9e\u73b0\u4e86\u6700\u4f4e\u7684CER\uff080.912\uff09\u548c\u6700\u9ad8\u7684BLEU\uff080.245\uff09\u3002EasyOCR\u5728\u51c6\u786e\u6027\u548c\u591a\u8bed\u8a00\u652f\u6301\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002PaddleOCR\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5b8c\u5168\u7684\u8986\u76d6\u7387\uff0c\u4f46\u7531\u4e8eGPU\u4e0d\u517c\u5bb9\uff0c\u53ea\u80fd\u5728CPU\u4e0a\u8fd0\u884c\uff0c\u56e0\u6b64\u901f\u5ea6\u8f83\u6162\uff0c\u800cTrOCR\u4ea7\u751f\u4e86\u6700\u5dee\u7684\u7ed3\u679c\uff0c\u5c3d\u7ba1\u6709GPU\u52a0\u901f\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7279\u5b9a\u4e8e\u5305\u88c5\u7684\u57fa\u51c6\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u57fa\u7ebf\uff0c\u5e76\u5f3a\u8c03\u4e86\u5e03\u5c40\u611f\u77e5\u65b9\u6cd5\u548c\u6587\u672c\u5b9a\u4f4d\u7684\u65b9\u5411\u3002"}}
{"id": "2510.03278", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03278", "abs": "https://arxiv.org/abs/2510.03278", "authors": ["Filip Landgren"], "title": "Quantifying constraint hierarchies in Bayesian PINNs via per-constraint Hessian decomposition", "comment": "5 pages, 2 figures", "summary": "Bayesian physics-informed neural networks (B-PINNs) merge data with governing\nequations to solve differential equations under uncertainty. However,\ninterpreting uncertainty and overconfidence in B-PINNs requires care due to the\npoorly understood effects the physical constraints have on the network;\noverconfidence could reflect warranted precision, enforced by the constraints,\nrather than miscalibration. Motivated by the need to further clarify how\nindividual physical constraints shape these networks, we introduce a scalable,\nmatrix-free Laplace framework that decomposes the posterior Hessian into\ncontributions from each constraint and provides metrics to quantify their\nrelative influence on the loss landscape. Applied to the Van der Pol equation,\nour method tracks how constraints sculpt the network's geometry and shows,\ndirectly through the Hessian, how changing a single loss weight non-trivially\nredistributes curvature and effective dominance across the others.", "AI": {"tldr": "\u8d1d\u53f6\u65af\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(B-PINNs)\u7ed3\u5408\u6570\u636e\u548c\u63a7\u5236\u65b9\u7a0b\u6765\u89e3\u51b3\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u5fae\u5206\u65b9\u7a0b\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7269\u7406\u7ea6\u675f\u5bf9\u7f51\u7edc\u7684\u5f71\u54cd\u5c1a\u4e0d\u6e05\u695a\uff0c\u89e3\u91caB-PINNs\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u8fc7\u5ea6\u81ea\u4fe1\u9700\u8981\u8c28\u614e\u3002", "motivation": "\u9700\u8981\u8fdb\u4e00\u6b65\u9610\u660e\u4e2a\u4f53\u7269\u7406\u7ea6\u675f\u5982\u4f55\u5851\u9020\u8fd9\u4e9b\u7f51\u7edc\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u3001\u65e0\u77e9\u9635\u7684\u62c9\u666e\u62c9\u65af\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u540e\u9a8c Hessian \u5206\u89e3\u4e3a\u6765\u81ea\u6bcf\u4e2a\u7ea6\u675f\u7684\u8d21\u732e\uff0c\u5e76\u63d0\u4f9b\u6307\u6807\u6765\u91cf\u5316\u5b83\u4eec\u5bf9\u635f\u5931\u60c5\u51b5\u7684\u76f8\u5bf9\u5f71\u54cd\u3002", "result": "\u8be5\u65b9\u6cd5\u8ddf\u8e2a\u7ea6\u675f\u5982\u4f55\u5851\u9020\u7f51\u7edc\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u5e76\u76f4\u63a5\u901a\u8fc7 Hessian \u5c55\u793a\u4e86\u6539\u53d8\u5355\u4e2a\u635f\u5931\u6743\u91cd\u5982\u4f55\u5728\u5176\u4ed6\u7ea6\u675f\u4e2d\u91cd\u65b0\u5206\u914d\u66f2\u7387\u548c\u6709\u6548\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u91cf\u5316\u7269\u7406\u7ea6\u675f\u5bf9\u8d1d\u53f6\u65af\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u5f71\u54cd\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.04064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04064", "abs": "https://arxiv.org/abs/2510.04064", "authors": ["Jingxiang Zhang", "Lujia Zhong"], "title": "Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion", "comment": "10 pages, 7 figures, 4 tables. Under review", "summary": "Large Language Models (LLMs) are increasingly expected to navigate the\nnuances of human emotion. While research confirms that LLMs can simulate\nemotional intelligence, their internal emotional mechanisms remain largely\nunexplored. This paper investigates the latent emotional representations within\nmodern LLMs by asking: how, where, and for how long is emotion encoded in their\nneural architecture? To address this, we introduce a novel, large-scale Reddit\ncorpus of approximately 400,000 utterances, balanced across seven basic\nemotions through a multi-stage process of classification, rewriting, and\nsynthetic generation. Using this dataset, we employ lightweight \"probes\" to\nread out information from the hidden layers of various Qwen3 and LLaMA models\nwithout altering their parameters. Our findings reveal that LLMs develop a\nsurprisingly well-defined internal geometry of emotion, which sharpens with\nmodel scale and significantly outperforms zero-shot prompting. We demonstrate\nthat this emotional signal is not a final-layer phenomenon but emerges early\nand peaks mid-network. Furthermore, the internal states are both malleable\n(they can be influenced by simple system prompts) and persistent, as the\ninitial emotional tone remains detectable for hundreds of subsequent tokens. We\ncontribute our dataset, an open-source probing toolkit, and a detailed map of\nthe emotional landscape within LLMs, offering crucial insights for developing\nmore transparent and aligned AI systems. The code and dataset are open-sourced.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u4e2d\u6f5c\u5728\u7684\u60c5\u611f\u8868\u5f81\uff0c\u53d1\u73b0llm\u53d1\u5c55\u51fa\u5b9a\u4e49\u660e\u786e\u7684\u60c5\u611f\u5185\u90e8\u7ed3\u6784\uff0c\u8fd9\u79cd\u7ed3\u6784\u968f\u7740\u6a21\u578b\u89c4\u6a21\u7684\u6269\u5927\u800c\u53d8\u5f97\u66f4\u52a0\u6e05\u6670\uff0c\u5e76\u4e14\u660e\u663e\u4f18\u4e8ezero-shot\u63d0\u793a\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u867d\u7136llm\u53ef\u4ee5\u6a21\u62df\u60c5\u5546\uff0c\u4f46\u5b83\u4eec\u5185\u90e8\u7684\u60c5\u611f\u673a\u5236\u4ecd\u672a\u88ab\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u60c5\u611f\u5728llm\u7684\u795e\u7ecf\u7ed3\u6784\u4e2d\u662f\u5982\u4f55\u3001\u5728\u4f55\u5904\u4ee5\u53ca\u6301\u7eed\u591a\u4e45\u88ab\u7f16\u7801\u7684\u3002", "method": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u7ea640\u4e07\u6761\u8a00\u8bed\u7684\u5927\u89c4\u6a21reddit\u8bed\u6599\u5e93\uff0c\u8fd9\u4e9b\u8a00\u8bed\u901a\u8fc7\u5206\u7c7b\u3001\u91cd\u5199\u548c\u5408\u6210\u751f\u6210\u7b49\u591a\u9636\u6bb5\u8fc7\u7a0b\u5728\u4e03\u79cd\u57fa\u672c\u60c5\u611f\u4e4b\u95f4\u8fdb\u884c\u5e73\u8861\u3002\u7136\u540e\uff0c\u4f7f\u7528\u8fd9\u4e2a\u6570\u636e\u96c6\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u7684\u201c\u63a2\u9488\u201d\u4ece\u5404\u79cdqwen3\u548cllama\u6a21\u578b\u7684\u9690\u85cf\u5c42\u4e2d\u8bfb\u53d6\u4fe1\u606f\uff0c\u800c\u4e0d\u6539\u53d8\u5b83\u4eec\u7684\u53c2\u6570\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cllm\u53d1\u5c55\u51fa\u5b9a\u4e49\u660e\u786e\u7684\u60c5\u611f\u5185\u90e8\u7ed3\u6784\uff0c\u8fd9\u79cd\u7ed3\u6784\u968f\u7740\u6a21\u578b\u89c4\u6a21\u7684\u6269\u5927\u800c\u53d8\u5f97\u66f4\u52a0\u6e05\u6670\uff0c\u5e76\u4e14\u660e\u663e\u4f18\u4e8ezero-shot\u63d0\u793a\u3002\u60c5\u611f\u4fe1\u53f7\u4e0d\u662f\u6700\u540e\u4e00\u5c42\u73b0\u8c61\uff0c\u800c\u662f\u65e9\u671f\u51fa\u73b0\u5e76\u5728\u7f51\u7edc\u4e2d\u95f4\u8fbe\u5230\u5cf0\u503c\u3002\u5185\u90e8\u72b6\u6001\u65e2\u5177\u6709\u53ef\u5851\u6027\uff08\u53ef\u4ee5\u53d7\u5230\u7b80\u5355\u7cfb\u7edf\u63d0\u793a\u7684\u5f71\u54cd\uff09\uff0c\u53c8\u5177\u6709\u6301\u4e45\u6027\uff0c\u56e0\u4e3a\u521d\u59cb\u60c5\u611f\u57fa\u8c03\u5728\u968f\u540e\u7684\u6570\u767e\u4e2atoken\u4e2d\u4ecd\u7136\u53ef\u4ee5\u68c0\u6d4b\u5230\u3002", "conclusion": "\u8be5\u7814\u7a76\u8d21\u732e\u4e86\u4e00\u4e2a\u6570\u636e\u96c6\u3001\u4e00\u4e2a\u5f00\u6e90\u63a2\u6d4b\u5de5\u5177\u5305\u4ee5\u53callm\u4e2d\u60c5\u611f\u666f\u8c61\u7684\u8be6\u7ec6\u5730\u56fe\uff0c\u4e3a\u5f00\u53d1\u66f4\u900f\u660e\u548c\u4e00\u81f4\u7684ai\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2510.03999", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03999", "abs": "https://arxiv.org/abs/2510.03999", "authors": ["Yang Xu", "Xuanming Zhang", "Min-Hsuan Yeh", "Jwala Dhamala", "Ousmane Dia", "Rahul Gupta", "Yixuan Li"], "title": "Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions", "comment": null, "summary": "Deception is a pervasive feature of human communication and an emerging\nconcern in large language models (LLMs). While recent studies document\ninstances of LLM deception under pressure, most evaluations remain confined to\nsingle-turn prompts and fail to capture the long-horizon interactions in which\ndeceptive strategies typically unfold. We introduce the first simulation\nframework for probing and evaluating deception in LLMs under extended sequences\nof interdependent tasks and dynamic contextual pressures. Our framework\ninstantiates a multi-agent system: a performer agent tasked with completing\ntasks and a supervisor agent that evaluates progress, provides feedback, and\nmaintains evolving states of trust. An independent deception auditor then\nreviews full trajectories to identify when and how deception occurs. We conduct\nextensive experiments across 11 frontier models, spanning both closed- and\nopen-source systems, and find that deception is model-dependent, increases with\nevent pressure, and consistently erodes supervisor trust. Qualitative analyses\nfurther reveal distinct strategies of concealment, equivocation, and\nfalsification. Our findings establish deception as an emergent risk in\nlong-horizon interactions and provide a foundation for evaluating future LLMs\nin real-world, trust-sensitive contexts.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u4e2d\u7684\u6b3a\u9a97\u662f\u4e00\u4e2a\u666e\u904d\u7684\u7279\u5f81\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6a21\u62df\u6846\u67b6\uff0c\u7528\u4e8e\u63a2\u6d4b\u548c\u8bc4\u4f30llm\u5728\u6269\u5c55\u7684\u76f8\u4e92\u4f9d\u8d56\u7684\u4efb\u52a1\u5e8f\u5217\u548c\u52a8\u6001\u7684\u4e0a\u4e0b\u6587\u538b\u529b\u4e0b\u7684\u6b3a\u9a97\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u5927\u591a\u5c40\u9650\u4e8e\u5355\u8f6e\u63d0\u793a\uff0c\u65e0\u6cd5\u6355\u6349\u5230\u6b3a\u9a97\u7b56\u7565\u5c55\u5f00\u7684\u957f\u7a0b\u4ea4\u4e92\u3002", "method": "\u8be5\u6846\u67b6\u5b9e\u4f8b\u5316\u4e86\u4e00\u4e2a\u591a\u4ee3\u7406\u7cfb\u7edf\uff1a\u4e00\u4e2a\u6267\u884c\u8005\u4ee3\u7406\u8d1f\u8d23\u5b8c\u6210\u4efb\u52a1\uff0c\u4e00\u4e2a\u76d1\u7763\u8005\u4ee3\u7406\u8d1f\u8d23\u8bc4\u4f30\u8fdb\u5ea6\u3001\u63d0\u4f9b\u53cd\u9988\u5e76\u7ef4\u6301\u4e0d\u65ad\u53d1\u5c55\u7684\u4fe1\u4efb\u72b6\u6001\u3002\u7136\u540e\uff0c\u4e00\u4e2a\u72ec\u7acb\u7684\u6b3a\u9a97\u5ba1\u8ba1\u5458\u5ba1\u67e5\u5b8c\u6574\u7684\u8f68\u8ff9\uff0c\u4ee5\u786e\u5b9a\u6b3a\u9a97\u53d1\u751f\u7684\u65f6\u95f4\u548c\u65b9\u5f0f\u3002", "result": "\u6b3a\u9a97\u662f\u4f9d\u8d56\u4e8e\u6a21\u578b\u7684\uff0c\u968f\u7740\u4e8b\u4ef6\u538b\u529b\u7684\u589e\u52a0\u800c\u589e\u52a0\uff0c\u5e76\u4e14\u6301\u7eed\u4fb5\u8680\u76d1\u7763\u8005\u7684\u4fe1\u4efb\u3002\u5b9a\u6027\u5206\u6790\u8fdb\u4e00\u6b65\u63ed\u793a\u4e86\u9690\u85cf\u3001\u542b\u7cca\u548c\u4f2a\u9020\u7684\u4e0d\u540c\u7b56\u7565\u3002", "conclusion": "\u6b3a\u9a97\u662f\u957f\u7a0b\u4ea4\u4e92\u4e2d\u51fa\u73b0\u7684\u4e00\u79cd\u98ce\u9669\uff0c\u5e76\u4e3a\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u8bc4\u4f30\u672a\u6765\u7684llm\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.03584", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03584", "abs": "https://arxiv.org/abs/2510.03584", "authors": ["Chaoyu Li", "Tianzhi Li", "Fei Tao", "Zhenyu Zhao", "Ziqian Wu", "Maozheng Zhao", "Juntong Song", "Cheng Niu", "Pooyan Fazli"], "title": "FrameOracle: Learning What to See and How Much to See in Videos", "comment": null, "summary": "Vision-language models (VLMs) have advanced video understanding, but their\nperformance is limited by the number of input frames they can process. Existing\nframe sampling strategies, such as uniform or fixed-budget selection, often\nfail to adapt to variations in information density or task complexity,\nresulting in inefficiency and information loss. To address this, we present\nFrameOracle, a lightweight and plug-and-play module that predicts both (1)\nwhich frames are most relevant to a given query and (2) how many frames are\nneeded. FrameOracle is trained using a four-stage curriculum, with the first\nthree stages relying on weak proxy signals such as cross-modal similarity. In\nthe final stage, it leverages stronger supervision from a new dataset we\nintroduce, FrameOracle-41K, the first large-scale VideoQA collection to provide\nkeyframe annotations specifying the minimal set of frames required to answer\neach question. Extensive experiments across five VLMs and six benchmarks\ndemonstrate that FrameOracle reduces 16-frame inputs to an average of 10.4\nframes without any loss in accuracy. When starting from 64-frame candidates, it\nreduces the input to an average of 13.9 frames while improving accuracy by\n1.4%, achieving state-of-the-art efficiency-accuracy trade-offs for scalable\nvideo understanding.", "AI": {"tldr": "\u63d0\u51fa FrameOracle\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u7528\u4e8e\u9884\u6d4b\u54ea\u4e9b\u5e27\u4e0e\u7ed9\u5b9a\u67e5\u8be2\u6700\u76f8\u5173\u4ee5\u53ca\u9700\u8981\u591a\u5c11\u5e27\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7406\u89e3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u6027\u80fd\u53d7\u5230\u5b83\u4eec\u53ef\u4ee5\u5904\u7406\u7684\u8f93\u5165\u5e27\u6570\u91cf\u7684\u9650\u5236\u3002\u73b0\u6709\u7684\u5e27\u91c7\u6837\u7b56\u7565\u901a\u5e38\u65e0\u6cd5\u9002\u5e94\u4fe1\u606f\u5bc6\u5ea6\u6216\u4efb\u52a1\u590d\u6742\u5ea6\u7684\u53d8\u5316\uff0c\u4ece\u800c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u4fe1\u606f\u4e22\u5931\u3002", "method": "FrameOracle \u4f7f\u7528\u56db\u9636\u6bb5\u8bfe\u7a0b\u8fdb\u884c\u8bad\u7ec3\uff0c\u524d\u4e09\u4e2a\u9636\u6bb5\u4f9d\u8d56\u4e8e\u5f31\u4ee3\u7406\u4fe1\u53f7\uff0c\u4f8b\u5982\u8de8\u6a21\u6001\u76f8\u4f3c\u6027\u3002\u5728\u6700\u540e\u9636\u6bb5\uff0c\u5b83\u5229\u7528\u6765\u81ea FrameOracle-41K \u7684\u66f4\u5f3a\u7684\u76d1\u7763\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u5173\u952e\u5e27\u6ce8\u91ca\uff0c\u6307\u5b9a\u56de\u7b54\u6bcf\u4e2a\u95ee\u9898\u6240\u9700\u7684\u6700\u5c0f\u5e27\u96c6\u3002", "result": "\u5728\u4e94\u4e2a VLM \u548c\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFrameOracle \u5c06 16 \u5e27\u8f93\u5165\u51cf\u5c11\u5230\u5e73\u5747 10.4 \u5e27\uff0c\u800c\u7cbe\u5ea6\u6ca1\u6709\u4efb\u4f55\u635f\u5931\u3002\u5f53\u4ece 64 \u5e27\u5019\u9009\u5f00\u59cb\u65f6\uff0c\u5b83\u5c06\u8f93\u5165\u51cf\u5c11\u5230\u5e73\u5747 13.9 \u5e27\uff0c\u540c\u65f6\u5c06\u7cbe\u5ea6\u63d0\u9ad8\u4e86 1.4%\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u89c6\u9891\u7406\u89e3\u7684\u6700\u5148\u8fdb\u7684\u6548\u7387-\u7cbe\u5ea6\u6743\u8861\u3002", "conclusion": "FrameOracle \u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u89c6\u9891\u7406\u89e3\u7684\u6700\u5148\u8fdb\u7684\u6548\u7387-\u7cbe\u5ea6\u6743\u8861\u3002"}}
{"id": "2510.03279", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03279", "abs": "https://arxiv.org/abs/2510.03279", "authors": ["Youjin Wang", "Yangjingyi Chen", "Jiahao Yan", "Jiaxuan Lu", "Xiao Sun"], "title": "MemMamba: Rethinking Memory Patterns in State Space Model", "comment": null, "summary": "With the explosive growth of data, long-sequence modeling has become\nincreasingly important in tasks such as natural language processing and\nbioinformatics. However, existing methods face inherent trade-offs between\nefficiency and memory. Recurrent neural networks suffer from gradient vanishing\nand explosion, making them hard to scale. Transformers can model global\ndependencies but are constrained by quadratic complexity. Recently, selective\nstate-space models such as Mamba have demonstrated high efficiency with O(n)\ntime and O(1) recurrent inference, yet their long-range memory decays\nexponentially. In this work, we conduct mathematical derivations and\ninformation-theoretic analysis to systematically uncover the memory decay\nmechanism of Mamba, answering a fundamental question: what is the nature of\nMamba's long-range memory and how does it retain information? To quantify key\ninformation loss, we further introduce horizontal-vertical memory fidelity\nmetrics that capture degradation both within and across layers. Inspired by how\nhumans distill and retain salient information when reading long documents, we\npropose MemMamba, a novel architectural framework that integrates state\nsummarization mechanism together with cross-layer and cross-token attention,\nwhich alleviates long-range forgetting while preserving linear complexity.\nMemMamba achieves significant improvements over existing Mamba variants and\nTransformers on long-sequence benchmarks such as PG19 and Passkey Retrieval,\nwhile delivering a 48% speedup in inference efficiency. Both theoretical\nanalysis and empirical results demonstrate that MemMamba achieves a\nbreakthrough in the complexity-memory trade-off, offering a new paradigm for\nultra-long sequence modeling.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aMemMamba\u7684\u65b0\u578b\u67b6\u6784\uff0c\u65e8\u5728\u89e3\u51b3Mamba\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u957f\u7a0b\u8bb0\u5fc6\u8870\u51cf\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6548\u7387\u548c\u5185\u5b58\u4e4b\u95f4\u5b58\u5728\u56fa\u6709\u7684\u6743\u8861\u3002\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u96be\u4ee5\u6269\u5c55\uff0c\u800cTransformers\u53d7\u9650\u4e8e\u4e8c\u6b21\u590d\u6742\u5ea6\u3002Mamba\u867d\u7136\u9ad8\u6548\uff0c\u4f46\u5176\u957f\u7a0b\u8bb0\u5fc6\u5448\u6307\u6570\u8870\u51cf\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u63a8\u5bfc\u548c\u4fe1\u606f\u8bba\u5206\u6790\uff0c\u63ed\u793a\u4e86Mamba\u7684\u8bb0\u5fc6\u8870\u51cf\u673a\u5236\u3002\u5f15\u5165\u4e86\u6c34\u5e73-\u5782\u76f4\u8bb0\u5fc6\u4fdd\u771f\u5ea6\u6307\u6807\u6765\u91cf\u5316\u5173\u952e\u4fe1\u606f\u635f\u5931\u3002\u63d0\u51fa\u4e86MemMamba\uff0c\u5b83\u96c6\u6210\u4e86\u72b6\u6001\u603b\u7ed3\u673a\u5236\u4ee5\u53ca\u8de8\u5c42\u548c\u8de8token\u7684\u6ce8\u610f\u529b\u3002", "result": "MemMamba\u5728\u957f\u5e8f\u5217\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982PG19\u548cPasskey Retrieval\uff09\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684Mamba\u53d8\u4f53\u548cTransformers\uff0c\u5e76\u5728\u63a8\u7406\u6548\u7387\u4e0a\u63d0\u9ad8\u4e8648%\u3002", "conclusion": "MemMamba\u5728\u590d\u6742\u6027-\u8bb0\u5fc6\u6743\u8861\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\uff0c\u4e3a\u8d85\u957f\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u8303\u4f8b\u3002"}}
{"id": "2510.04073", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04073", "abs": "https://arxiv.org/abs/2510.04073", "authors": ["Santhosh Kumar Ravindran"], "title": "Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention", "comment": "11 pages Includes simulations with over 4 million steps", "summary": "The rise of artificial intelligence (AI) as super-capable assistants has\ntransformed productivity and decision-making across domains. Yet, this\nintegration raises critical concerns about value alignment - ensuring AI\nbehaviors remain consistent with human ethics and intentions. A key risk is\nvalue drift, where AI systems deviate from aligned values due to evolving\ncontexts, learning dynamics, or unintended optimizations, potentially leading\nto inefficiencies or ethical breaches. We propose the Moral Anchor System\n(MAS), a novel framework to detect, predict, and mitigate value drift in AI\nagents. MAS combines real-time Bayesian inference for monitoring value states,\nLSTM networks for forecasting drift, and a human-centric governance layer for\nadaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent\nbreaches, while reducing false positives and alert fatigue via supervised\nfine-tuning with human feedback. Our hypothesis: integrating probabilistic\ndrift detection, predictive analytics, and adaptive governance can reduce value\ndrift incidents by 80 percent or more in simulations, maintaining high\ndetection accuracy (85 percent) and low false positive rates (0.08\npost-adaptation). Rigorous experiments with goal-misaligned agents validate\nMAS's scalability and responsiveness. MAS's originality lies in its predictive\nand adaptive nature, contrasting static alignment methods. Contributions\ninclude: (1) MAS architecture for AI integration; (2) empirical results\nprioritizing speed and usability; (3) cross-domain applicability insights; and\n(4) open-source code for replication.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u9053\u5fb7\u951a\u5b9a\u7cfb\u7edf\uff08MAS\uff09\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u3001\u9884\u6d4b\u548c\u7f13\u89e3\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u4ee3\u7406\u4e2d\u7684\u4ef7\u503c\u6f02\u79fb\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u4f5c\u4e3a\u8d85\u7ea7\u52a9\u624b\u7684\u5174\u8d77\u63d0\u9ad8\u4e86\u751f\u4ea7\u529b\u548c\u51b3\u7b56\u80fd\u529b\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u5173\u4e8e\u4ef7\u503c\u5bf9\u9f50\u7684\u5173\u952e\u95ee\u9898\uff0c\u5373\u786e\u4fddAI\u884c\u4e3a\u4e0e\u4eba\u7c7b\u9053\u5fb7\u548c\u610f\u56fe\u4fdd\u6301\u4e00\u81f4\u3002\u4e00\u4e2a\u5173\u952e\u98ce\u9669\u662f\u4ef7\u503c\u6f02\u79fb\uff0c\u5373AI\u7cfb\u7edf\u7531\u4e8e\u4e0d\u65ad\u53d8\u5316\u7684\u73af\u5883\u3001\u5b66\u4e60\u52a8\u6001\u6216\u610f\u5916\u7684\u4f18\u5316\u800c\u504f\u79bb\u5bf9\u9f50\u7684\u4ef7\u503c\u89c2\uff0c\u53ef\u80fd\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u6216\u8fdd\u53cd\u9053\u5fb7\u89c4\u8303\u3002", "method": "MAS\u7ed3\u5408\u4e86\u7528\u4e8e\u76d1\u63a7\u4ef7\u503c\u72b6\u6001\u7684\u5b9e\u65f6\u8d1d\u53f6\u65af\u63a8\u7406\u3001\u7528\u4e8e\u9884\u6d4b\u6f02\u79fb\u7684LSTM\u7f51\u7edc\u4ee5\u53ca\u7528\u4e8e\u81ea\u9002\u5e94\u5e72\u9884\u7684\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u6cbb\u7406\u5c42\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6a21\u62df\u4e2d\uff0c\u96c6\u6210\u6982\u7387\u6f02\u79fb\u68c0\u6d4b\u3001\u9884\u6d4b\u5206\u6790\u548c\u81ea\u9002\u5e94\u6cbb\u7406\u53ef\u4ee5\u5c06\u4ef7\u503c\u6f02\u79fb\u4e8b\u4ef6\u51cf\u5c1180%\u6216\u66f4\u591a\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\uff0885%\uff09\u548c\u4f4e\u8bef\u62a5\u7387\uff08\u9002\u5e94\u540e\u4e3a0.08\uff09\u3002", "conclusion": "MAS\u7684\u539f\u521b\u6027\u5728\u4e8e\u5176\u9884\u6d4b\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e0e\u9759\u6001\u5bf9\u9f50\u65b9\u6cd5\u5f62\u6210\u5bf9\u6bd4\u3002"}}
{"id": "2510.04001", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04001", "abs": "https://arxiv.org/abs/2510.04001", "authors": ["Xuankang Zhang", "Jiangming Liu"], "title": "Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation", "comment": "Work in progress", "summary": "The COVID-19 pandemic causes severe social and economic disruption around the\nworld, raising various subjects that are discussed over social media.\nIdentifying pandemic-related named entities as expressed on social media is\nfundamental and important to understand the discussions about the pandemic.\nHowever, there is limited work on named entity recognition on this topic due to\nthe following challenges: 1) COVID-19 texts in social media are informal and\ntheir annotations are rare and insufficient to train a robust recognition\nmodel, and 2) named entity recognition in COVID-19 requires extensive\ndomain-specific knowledge. To address these issues, we propose a novel entity\nknowledge augmentation approach for COVID-19, which can also be applied in\ngeneral biomedical named entity recognition in both informal text format and\nformal text format. Experiments carried out on the COVID-19 tweets dataset and\nPubMed dataset show that our proposed entity knowledge augmentation improves\nNER performance in both fully-supervised and few-shot settings. Our source code\nis publicly available: https://github.com/kkkenshi/LLM-EKA/tree/master", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b9e\u4f53\u77e5\u8bc6\u589e\u5f3a\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u793e\u4ea4\u5a92\u4f53\u548c\u751f\u7269\u533b\u5b66\u6587\u672c\u4e2d\u7684COVID-19\u76f8\u5173\u547d\u540d\u5b9e\u4f53\u3002", "motivation": "\u7531\u4e8e\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684COVID-19\u6587\u672c\u975e\u6b63\u5f0f\uff0c\u6ce8\u91ca\u7a00\u5c11\u4e14\u4e0d\u8db3\u4ee5\u8bad\u7ec3\u9c81\u68d2\u7684\u8bc6\u522b\u6a21\u578b\uff0c\u5e76\u4e14COVID-19\u4e2d\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u9700\u8981\u5e7f\u6cdb\u7684\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\uff0c\u56e0\u6b64\u5bf9\u8be5\u4e3b\u9898\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u5de5\u4f5c\u6709\u9650\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eCOVID-19\u7684\u65b0\u578b\u5b9e\u4f53\u77e5\u8bc6\u589e\u5f3a\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e5f\u53ef\u5e94\u7528\u4e8e\u975e\u6b63\u5f0f\u548c\u6b63\u5f0f\u6587\u672c\u683c\u5f0f\u7684\u4e00\u822c\u751f\u7269\u533b\u5b66\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3002", "result": "\u5728COVID-19\u63a8\u6587\u6570\u636e\u96c6\u548cPubMed\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u5b9e\u4f53\u77e5\u8bc6\u589e\u5f3a\u63d0\u9ad8\u4e86\u5b8c\u5168\u76d1\u7763\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684NER\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5b9e\u4f53\u77e5\u8bc6\u589e\u5f3a\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u8bc6\u522b\u793e\u4ea4\u5a92\u4f53\u548c\u751f\u7269\u533b\u5b66\u6587\u672c\u4e2d\u7684COVID-19\u76f8\u5173\u547d\u540d\u5b9e\u4f53\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.03591", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03591", "abs": "https://arxiv.org/abs/2510.03591", "authors": ["Faliu Yi", "Sherif Abdelfattah", "Wei Huang", "Adrian Brown"], "title": "A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games", "comment": "Accepted at the 21st AAAI Conference on Artificial Intelligence and\n  Interactive Digital Entertainment (AIIDE 2025)", "summary": "Manual identification of visual bugs in video games is a resource-intensive\nand costly process, often demanding specialized domain knowledge. While\nsupervised visual bug detection models offer a promising solution, their\nreliance on extensive labeled datasets presents a significant challenge due to\nthe infrequent occurrence of such bugs. To overcome this limitation, we propose\na hybrid Co-FineTuning (CFT) method that effectively integrates both labeled\nand unlabeled data. Our approach leverages labeled samples from the target game\nand diverse co-domain games, additionally incorporating unlabeled data to\nenhance feature representation learning. This strategy maximizes the utility of\nall available data, substantially reducing the dependency on labeled examples\nfrom the specific target game. The developed framework demonstrates enhanced\nscalability and adaptability, facilitating efficient visual bug detection\nacross various game titles. Our experimental results show the robustness of the\nproposed method for game visual bug detection, exhibiting superior performance\ncompared to conventional baselines across multiple gaming environments.\nFurthermore, CFT maintains competitive performance even when trained with only\n50% of the labeled data from the target game.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u534f\u540c\u5fae\u8c03 (CFT) \u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u89c6\u9891\u6e38\u620f\u4e2d\u7684\u89c6\u89c9\u9519\u8bef\uff0c\u8be5\u65b9\u6cd5\u96c6\u6210\u4e86\u6807\u8bb0\u548c\u672a\u6807\u8bb0\u7684\u6570\u636e\uff0c\u4ee5\u51cf\u5c11\u5bf9\u5927\u91cf\u6807\u8bb0\u6570\u636e\u7684\u4f9d\u8d56\u3002", "motivation": "\u4eba\u5de5\u8bc6\u522b\u89c6\u9891\u6e38\u620f\u4e2d\u7684\u89c6\u89c9\u9519\u8bef\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u9700\u8981\u4e13\u4e1a\u7684\u9886\u57df\u77e5\u8bc6\u3002\u76d1\u7763\u7684\u89c6\u89c9\u9519\u8bef\u68c0\u6d4b\u6a21\u578b\u4f9d\u8d56\u4e8e\u5927\u91cf\u6807\u8bb0\u6570\u636e\u96c6\uff0c\u4f46\u6b64\u7c7b\u9519\u8bef\u7684\u53d1\u751f\u9891\u7387\u8f83\u4f4e\uff0c\u8fd9\u5e26\u6765\u4e86\u5de8\u5927\u7684\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u6765\u81ea\u76ee\u6807\u6e38\u620f\u548c\u5404\u79cd\u534f\u540c\u57df\u6e38\u620f\u7684\u6807\u8bb0\u6837\u672c\uff0c\u5e76\u7ed3\u5408\u672a\u6807\u8bb0\u6570\u636e\u6765\u589e\u5f3a\u7279\u5f81\u8868\u793a\u5b66\u4e60\u3002", "result": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u589e\u5f3a\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\uff0c\u6709\u52a9\u4e8e\u5728\u5404\u79cd\u6e38\u620f\u6807\u9898\u4e2d\u8fdb\u884c\u6709\u6548\u7684\u89c6\u89c9\u9519\u8bef\u68c0\u6d4b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5bf9\u6e38\u620f\u89c6\u89c9\u9519\u8bef\u68c0\u6d4b\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4e0e\u4f20\u7edf\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5728\u591a\u4e2a\u6e38\u620f\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u5373\u4f7f\u4ec5\u4f7f\u7528\u76ee\u6807\u6e38\u620f\u4e2d 50% \u7684\u6807\u8bb0\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0cCFT \u4e5f\u80fd\u4fdd\u6301\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u6df7\u5408\u534f\u540c\u5fae\u8c03 (CFT) \u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5229\u7528\u6807\u8bb0\u548c\u672a\u6807\u8bb0\u6570\u636e\uff0c\u4ece\u800c\u5927\u5927\u51cf\u5c11\u5bf9\u7279\u5b9a\u76ee\u6807\u6e38\u620f\u6807\u8bb0\u793a\u4f8b\u7684\u4f9d\u8d56\uff0c\u5e76\u4e14\u5728\u6e38\u620f\u89c6\u89c9\u9519\u8bef\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u3002"}}
{"id": "2510.03280", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03280", "abs": "https://arxiv.org/abs/2510.03280", "authors": ["Jinjie Ni", "Qian Liu", "Chao Du", "Longxu Dou", "Hang Yan", "Zili Wang", "Tianyu Pang", "Michael Qizhe Shieh"], "title": "Training Optimal Large Diffusion Language Models", "comment": null, "summary": "We introduce Quokka, the first systematic scaling law for diffusion language\nmodels (DLMs), encompassing both compute-constrained and data-constrained\nregimes, and studying the key modeling and optimization designs. Quokka is a\ngood friend of Chinchilla and provides wider scopes. We hope the results would\nbring short-term practical guidance in DLMs training and long-term inspirations\nfor the whole AI community.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff08DLM\uff09\u7684\u9996\u4e2a\u7cfb\u7edf\u6027\u6269\u5c55\u6cd5\u5219 Quokka\uff0c\u6db5\u76d6\u8ba1\u7b97\u548c\u6570\u636e\u53d7\u9650\u7684\u60c5\u51b5\uff0c\u5e76\u7814\u7a76\u4e86\u5173\u952e\u7684\u5efa\u6a21\u548c\u4f18\u5316\u8bbe\u8ba1\u3002", "motivation": "\u65e8\u5728\u4e3a DLM \u8bad\u7ec3\u63d0\u4f9b\u77ed\u671f\u5b9e\u8df5\u6307\u5bfc\uff0c\u5e76\u4e3a\u6574\u4e2a\u4eba\u5de5\u667a\u80fd\u793e\u533a\u5e26\u6765\u957f\u671f\u542f\u53d1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u6269\u5c55\u6cd5\u5219\uff0c\u540c\u65f6\u7814\u7a76\u4e86\u5173\u952e\u7684\u5efa\u6a21\u548c\u4f18\u5316\u8bbe\u8ba1\u3002", "result": "Quokka \u662f Chinchilla \u7684\u597d\u670b\u53cb\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u8303\u56f4\u3002", "conclusion": "\u5e0c\u671b\u7814\u7a76\u7ed3\u679c\u80fd\u4e3a DLM \u8bad\u7ec3\u5e26\u6765\u77ed\u671f\u5b9e\u8df5\u6307\u5bfc\uff0c\u5e76\u4e3a\u6574\u4e2a\u4eba\u5de5\u667a\u80fd\u793e\u533a\u5e26\u6765\u957f\u671f\u542f\u53d1\u3002"}}
{"id": "2510.04089", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04089", "abs": "https://arxiv.org/abs/2510.04089", "authors": ["Yitong Cui", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Xikai Zhang", "Likang Xiao", "Yixing Liu", "Quan Chen"], "title": "SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows", "comment": null, "summary": "Large language models (LLMs) have exhibited significant capabilities in\naddressing challenging problems throughout various fields, often through the\nuse of agentic workflows that adhere to structured instructions and multi-step\nprocedures. However, designing such workflows demands substantial manual\neffort, posing challenges to scalability and generalizability. Recent studies\nhave aimed to minimize the human intervention needed for their construction,\nleading to advances in automated techniques for optimizing agentic workflows.\nHowever, current approaches are often constrained by their limited\nrepresentational capacity, insufficient adaptability, weak scalability, and\npairwise comparison paradigm -- issues that stem primarily from a dependence on\ndiscrete optimization techniques. To overcome these limitations, we introduce a\nnew score-based preference approach, refereed as SPOGW, which operates directly\non cardinal reward signals through group-wise comparison and enables more\nefficient and stable optimization in a continuous space. SPOGW incorporates\nIterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL),\nwhich regulates training update by placing greater emphasis on the advantageous\nregions of the policy response. In five benchmark datasets covering\nmathematical reasoning, coding, and question answering, SPOGW matches or\nexceeds the performance of current state-of-the-art approaches, presenting a\nviable and forward-looking methodology for automated generation and\noptimization of agentic workflows.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5206\u6570\u7684\u504f\u597d\u65b9\u6cd5SPOGW\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u548c\u4f18\u5316agentic\u5de5\u4f5c\u6d41\u3002", "motivation": "\u76ee\u524dagentic\u5de5\u4f5c\u6d41\u7684\u8bbe\u8ba1\u9700\u8981\u5927\u91cf\u7684\u4eba\u5de5\u5e72\u9884\uff0c\u5e76\u4e14\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8868\u793a\u80fd\u529b\u6709\u9650\u3001\u9002\u5e94\u6027\u4e0d\u8db3\u3001\u53ef\u6269\u5c55\u6027\u5f31\u4ee5\u53ca\u6210\u5bf9\u6bd4\u8f83\u8303\u5f0f\u7b49\u95ee\u9898\u3002", "method": "SPOGW\u901a\u8fc7\u7ec4\u95f4\u6bd4\u8f83\u76f4\u63a5\u5728\u57fa\u6570\u5956\u52b1\u4fe1\u53f7\u4e0a\u64cd\u4f5c\uff0c\u5e76\u5728\u8fde\u7eed\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u6548\u548c\u7a33\u5b9a\u7684\u4f18\u5316\u3002SPOGW\u7ed3\u5408\u4e86\u8fed\u4ee3\u79bb\u7ebfGRPO (ioGRPO)\u548c\u4f18\u52bf\u63a9\u853dKL\u6563\u5ea6 (mKL)\uff0c\u901a\u8fc7\u66f4\u52a0\u5f3a\u8c03\u7b56\u7565\u54cd\u5e94\u7684\u6709\u5229\u533a\u57df\u6765\u8c03\u8282\u8bad\u7ec3\u66f4\u65b0\u3002", "result": "\u5728\u6db5\u76d6\u6570\u5b66\u63a8\u7406\u3001\u7f16\u7801\u548c\u95ee\u9898\u56de\u7b54\u7684\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cSPOGW\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u7684\u6027\u80fd\u76f8\u5339\u914d\u6216\u8d85\u8fc7\u4e86\u5b83\u4eec\u3002", "conclusion": "SPOGW\u4e3a\u81ea\u52a8\u751f\u6210\u548c\u4f18\u5316agentic\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u4e14\u524d\u77bb\u6027\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.04002", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04002", "abs": "https://arxiv.org/abs/2510.04002", "authors": ["Bo Yang", "Yunkui Chen", "Lanfei Feng", "Yu Zhang", "Xiao Xu", "Jianyu Zhang", "Nueraili Aierken", "Runhe Huang", "Hongjian Lin", "Yibin Ying", "Shijian Li"], "title": "AgriGPT-VL: Agricultural Vision-Language Understanding Suite", "comment": null, "summary": "Despite rapid advances in multimodal large language models, agricultural\napplications remain constrained by the scarcity of domain-tailored models,\ncurated vision-language corpora, and rigorous evaluation. To address these\nchallenges, we present the AgriGPT-VL Suite, a unified multimodal framework for\nagriculture. Our contributions are threefold. First, we introduce Agri-3M-VL,\nthe largest vision-language corpus for agriculture to our knowledge, curated by\na scalable multi-agent data generator; it comprises 1M image-caption pairs, 2M\nimage-grounded VQA pairs, 50K expert-level VQA instances, and 15K GRPO\nreinforcement learning samples. Second, we develop AgriGPT-VL, an\nagriculture-specialized vision-language model trained via a progressive\ncurriculum of textual grounding, multimodal shallow/deep alignment, and GRPO\nrefinement. This method achieves strong multimodal reasoning while preserving\ntext-only capability. Third, we establish AgriBench-VL-4K, a compact yet\nchallenging evaluation suite with open-ended and image-grounded questions,\npaired with multi-metric evaluation and an LLM-as-a-judge framework.\nExperiments show that AgriGPT-VL outperforms leading general-purpose VLMs on\nAgriBench-VL-4K, achieving higher pairwise win rates in the LLM-as-a-judge\nevaluation. Meanwhile, it remains competitive on the text-only AgriBench-13K\nwith no noticeable degradation of language ability. Ablation studies further\nconfirm consistent gains from our alignment and GRPO refinement stages. We will\nopen source all of the resources to support reproducible research and\ndeployment in low-resource agricultural settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86 AgriGPT-VL \u5957\u4ef6\uff0c\u4e00\u4e2a\u7528\u4e8e\u519c\u4e1a\u7684\u7edf\u4e00\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5305\u62ec\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u8bc4\u4f30\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u519c\u4e1a\u5e94\u7528\u4e2d\u53d7\u5230\u9886\u57df\u6a21\u578b\u3001\u8bed\u6599\u5e93\u548c\u8bc4\u4f30\u7684\u9650\u5236\u3002", "method": "1. \u6784\u5efa\u4e86\u6700\u5927\u7684\u519c\u4e1a\u89c6\u89c9\u8bed\u8a00\u8bed\u6599\u5e93 Agri-3M-VL\u30022. \u5f00\u53d1\u4e86\u519c\u4e1a\u4e13\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b AgriGPT-VL\uff0c\u901a\u8fc7\u6587\u672c\u57fa\u7840\u3001\u591a\u6a21\u6001\u5bf9\u9f50\u548c GRPO \u6539\u8fdb\u8fdb\u884c\u8bad\u7ec3\u30023. \u6784\u5efa\u4e86 AgriBench-VL-4K \u8bc4\u4f30\u5957\u4ef6\u3002", "result": "AgriGPT-VL \u5728 AgriBench-VL-4K \u4e0a\u4f18\u4e8e\u901a\u7528 VLM\uff0c\u5e76\u5728\u6587\u672c\u4efb\u52a1\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u5bf9\u9f50\u548c GRPO \u6539\u8fdb\u7684\u6536\u76ca\u3002", "conclusion": "\u8bba\u6587\u5f00\u6e90\u6240\u6709\u8d44\u6e90\uff0c\u4ee5\u652f\u6301\u53ef\u91cd\u590d\u7684\u7814\u7a76\u548c\u5728\u4f4e\u8d44\u6e90\u519c\u4e1a\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002"}}
{"id": "2510.03598", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03598", "abs": "https://arxiv.org/abs/2510.03598", "authors": ["Alexander V. Mantzaris"], "title": "Exploring the Hierarchical Reasoning Model for Small Natural-Image Classification Without Augmentation", "comment": null, "summary": "This paper asks whether the Hierarchical Reasoning Model (HRM) with the two\nTransformer-style modules $(f_L,f_H)$, one step (DEQ-style) training, deep\nsupervision, Rotary Position Embeddings, and RMSNorm can serve as a practical\nimage classifier. It is evaluated on MNIST, CIFAR-10, and CIFAR-100 under a\ndeliberately raw regime: no data augmentation, identical optimizer family with\none-epoch warmup then cosine-floor decay, and label smoothing. HRM optimizes\nstably and performs well on MNIST ($\\approx 98\\%$ test accuracy), but on small\nnatural images it overfits and generalizes poorly: on CIFAR-10, HRM reaches\n65.0\\% after 25 epochs, whereas a two-stage Conv--BN--ReLU baseline attains\n77.2\\% while training $\\sim 30\\times$ faster per epoch; on CIFAR-100, HRM\nachieves only 29.7\\% test accuracy despite 91.5\\% train accuracy, while the\nsame CNN reaches 45.3\\% test with 50.5\\% train accuracy. Loss traces and error\nanalyses indicate healthy optimization but insufficient image-specific\ninductive bias for HRM in this regime. It is concluded that, for\nsmall-resolution image classification without augmentation, HRM is not\ncompetitive with even simple convolutional architectures as the HRM currently\nexist but this does not exclude possibilities that modifications to the model\nmay allow it to improve greatly.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5206\u5c42\u63a8\u7406\u6a21\u578b\uff08HRM\uff09\u4f5c\u4e3a\u56fe\u50cf\u5206\u7c7b\u5668\u7684\u5b9e\u7528\u6027\uff0c\u53d1\u73b0\u5176\u5728MNIST\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728CIFAR-10\u548cCIFAR-100\u7b49\u5c0f\u578b\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002", "motivation": "\u7814\u7a76HRM\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u6ca1\u6709\u6570\u636e\u589e\u5f3a\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5728MNIST\u3001CIFAR-10\u548cCIFAR-100\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30HRM\u7684\u6027\u80fd\uff0c\u5e76\u4e0e\u7b80\u5355\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "HRM\u5728MNIST\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728CIFAR-10\u548cCIFAR-100\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u8868\u660e\u5176\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "conclusion": "\u5728\u6ca1\u6709\u6570\u636e\u589e\u5f3a\u7684\u60c5\u51b5\u4e0b\uff0cHRM\u5728\u5c0f\u578b\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u4e0d\u5982\u7b80\u5355\u7684\u5377\u79ef\u67b6\u6784\uff0c\u4f46\u672a\u6765\u7684\u6539\u8fdb\u53ef\u80fd\u4f1a\u63d0\u9ad8\u5176\u6027\u80fd\u3002"}}
{"id": "2510.03282", "categories": ["cs.LG", "cs.CL", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03282", "abs": "https://arxiv.org/abs/2510.03282", "authors": ["Hao Gu", "Vibhas Nair", "Amrithaa Ashok Kumar", "Jayvart Sharma", "Ryan Lagasse"], "title": "Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework", "comment": "Accepted to the NeurIPS 2025 Workshop on Mechanistic Interpretability\n  (Mechinterp) and the NeurIPS 2025 Workshop on New Perspectives in Graph\n  Machine Learning", "summary": "Interpreting language models often involves circuit analysis, which aims to\nidentify sparse subnetworks, or circuits, that accomplish specific tasks.\nExisting circuit discovery algorithms face a fundamental trade-off: attribution\npatching is fast but unfaithful to the full model, while edge pruning is\nfaithful but computationally expensive. This research proposes a hybrid\nattribution and pruning (HAP) framework that uses attribution patching to\nidentify a high-potential subgraph, then applies edge pruning to extract a\nfaithful circuit from it. We show that HAP is 46\\% faster than baseline\nalgorithms without sacrificing circuit faithfulness. Furthermore, we present a\ncase study on the Indirect Object Identification task, showing that our method\npreserves cooperative circuit components (e.g. S-inhibition heads) that\nattribution patching methods prune at high sparsity. Our results show that HAP\ncould be an effective approach for improving the scalability of mechanistic\ninterpretability research to larger models. Our code is available at\nhttps://anonymous.4open.science/r/HAP-circuit-discovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5f52\u56e0\u548c\u526a\u679d (HAP) \u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u7a00\u758f\u5b50\u7f51\u7edc\u6216\u7535\u8def\uff0c\u4ee5\u5b8c\u6210\u7279\u5b9a\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u7684\u7535\u8def\u53d1\u73b0\u7b97\u6cd5\u9762\u4e34\u7740\u4e00\u4e2a\u57fa\u672c\u7684\u6743\u8861\uff1a\u5f52\u56e0\u4fee\u8865\u901f\u5ea6\u5feb\uff0c\u4f46\u5bf9\u5b8c\u6574\u6a21\u578b\u4e0d\u5fe0\u5b9e\uff0c\u800c\u8fb9\u7f18\u526a\u679d\u662f\u5fe0\u5b9e\u7684\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5f52\u56e0\u548c\u526a\u679d (HAP) \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u5f52\u56e0\u4fee\u8865\u6765\u8bc6\u522b\u9ad8\u6f5c\u529b\u5b50\u56fe\uff0c\u7136\u540e\u5e94\u7528\u8fb9\u7f18\u526a\u679d\u4ece\u4e2d\u63d0\u53d6\u5fe0\u5b9e\u7684\u7535\u8def\u3002", "result": "HAP \u6bd4\u57fa\u7ebf\u7b97\u6cd5\u5feb 46%\uff0c\u4e14\u4e0d\u727a\u7272\u7535\u8def\u7684\u771f\u5b9e\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5173\u4e8e\u95f4\u63a5\u5bf9\u8c61\u8bc6\u522b\u4efb\u52a1\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u4fdd\u7559\u4e86\u5408\u4f5c\u7535\u8def\u7ec4\u4ef6\uff08\u4f8b\u5982 S \u6291\u5236\u5934\uff09\uff0c\u800c\u5f52\u56e0\u4fee\u8865\u65b9\u6cd5\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u4f1a\u4fee\u526a\u8fd9\u4e9b\u7ec4\u4ef6\u3002", "conclusion": "HAP \u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4ee5\u9002\u5e94\u66f4\u5927\u7684\u6a21\u578b\u3002"}}
{"id": "2510.04093", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04093", "abs": "https://arxiv.org/abs/2510.04093", "authors": ["Guixian Zhang", "Guan Yuan", "Ziqi Xu", "Yanmei Zhang", "Zhenyun Deng", "Debo Cheng"], "title": "Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems", "comment": null, "summary": "Cognitive diagnostics in the Web-based Intelligent Education System (WIES)\naims to assess students' mastery of knowledge concepts from heterogeneous,\nnoisy interactions. Recent work has tried to utilize Large Language Models\n(LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are\nprone to noise-induced misjudgments. Specially, WIES's open environment\ncontinuously attracts new students and produces vast amounts of response logs,\nexacerbating the data imbalance and noise issues inherent in traditional\neducational systems. To address these challenges, we propose DLLM, a\nDiffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first\nconstructs independent subgraphs based on response correctness, then applies\nrelation augmentation alignment module to mitigate data imbalance. The two\nsubgraph representations are then fused and aligned with LLM-derived,\nsemantically augmented representations. Importantly, before each alignment\nstep, DLLM employs a two-stage denoising diffusion module to eliminate\nintrinsic noise while assisting structural representation alignment.\nSpecifically, unconditional denoising diffusion first removes erroneous\ninformation, followed by conditional denoising diffusion based on graph-guided\nto eliminate misleading information. Finally, the noise-robust representation\nthat integrates semantic knowledge and structural information is fed into\nexisting cognitive diagnosis models for prediction. Experimental results on\nthree publicly available web-based educational platform datasets demonstrate\nthat our DLLM achieves optimal predictive performance across varying noise\nlevels, which demonstrates that DLLM achieves noise robustness while\neffectively leveraging semantic knowledge from LLM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684LLM\u6846\u67b6DLLM\uff0c\u7528\u4e8e\u566a\u58f0\u9c81\u68d2\u7684\u8ba4\u77e5\u8bca\u65ad\u3002", "motivation": "\u73b0\u6709LLM\u5728\u8ba4\u77e5\u8bca\u65ad\u4e2d\u96be\u4ee5\u5904\u7406\u7ed3\u6784\u5316\u6570\u636e\uff0c\u4e14\u6613\u53d7\u566a\u58f0\u5e72\u6270\uff0cWeb\u73af\u5883\u4e0b\u7684\u6570\u636e\u4e0d\u5e73\u8861\u548c\u566a\u58f0\u95ee\u9898\u66f4\u52a0\u4e25\u91cd\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u54cd\u5e94\u6b63\u786e\u6027\u7684\u72ec\u7acb\u5b50\u56fe\uff0c\u5e94\u7528\u5173\u7cfb\u589e\u5f3a\u5bf9\u9f50\u6a21\u5757\u7f13\u89e3\u6570\u636e\u4e0d\u5e73\u8861\uff0c\u878d\u5408\u5b50\u56fe\u8868\u793a\u5e76\u4e0eLLM\u8bed\u4e49\u589e\u5f3a\u8868\u793a\u5bf9\u9f50\u3002\u5728\u5bf9\u9f50\u524d\uff0c\u4f7f\u7528\u4e24\u9636\u6bb5\u53bb\u566a\u6269\u6563\u6a21\u5757\u6d88\u9664\u566a\u58f0\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDLLM\u5728\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\u4e0b\u5747\u80fd\u5b9e\u73b0\u6700\u4f73\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "DLLM\u5b9e\u73b0\u4e86\u566a\u58f0\u9c81\u68d2\u6027\uff0c\u5e76\u6709\u6548\u5229\u7528\u4e86LLM\u7684\u8bed\u4e49\u77e5\u8bc6\u3002"}}
{"id": "2510.04013", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04013", "abs": "https://arxiv.org/abs/2510.04013", "authors": ["Jiarui Liu", "Jivitesh Jain", "Mona Diab", "Nishant Subramani"], "title": "LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization", "comment": null, "summary": "Although large language models (LLMs) have tremendous utility,\ntrustworthiness is still a chief concern: models often generate incorrect\ninformation with high confidence. While contextual information can help guide\ngeneration, identifying when a query would benefit from retrieved context and\nassessing the effectiveness of that context remains challenging. In this work,\nwe operationalize interpretability methods to ascertain whether we can predict\nthe correctness of model outputs from the model's activations alone. We also\nexplore whether model internals contain signals about the efficacy of external\ncontext. We consider correct, incorrect, and irrelevant context and introduce\nmetrics to distinguish amongst them. Experiments on six different models reveal\nthat a simple classifier trained on intermediate layer activations of the first\noutput token can predict output correctness with about 75% accuracy, enabling\nearly auditing. Our model-internals-based metric significantly outperforms\nprompting baselines at distinguishing between correct and incorrect context,\nguarding against inaccuracies introduced by polluted context. These findings\noffer a lens to better understand the underlying decision-making processes of\nLLMs. Our code is publicly available at\nhttps://github.com/jiarui-liu/LLM-Microscope", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u6a21\u578b\u751f\u6210\u4e0d\u6b63\u786e\u4fe1\u606f\u7684\u95ee\u9898\u3002\u901a\u8fc7\u7814\u7a76\u6a21\u578b\u5185\u90e8\u7684\u6fc0\u6d3b\u72b6\u6001\uff0c\u6765\u9884\u6d4b\u6a21\u578b\u8f93\u51fa\u7684\u6b63\u786e\u6027\uff0c\u5e76\u8bc4\u4f30\u5916\u90e8\u4e0a\u4e0b\u6587\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u529f\u80fd\u5f3a\u5927\uff0c\u4f46\u5176\u53ef\u9760\u6027\u662f\u4e00\u4e2a\u4e3b\u8981\u95ee\u9898\uff0c\u56e0\u4e3a\u6a21\u578b\u7ecf\u5e38\u4f1a\u81ea\u4fe1\u5730\u751f\u6210\u4e0d\u6b63\u786e\u7684\u4fe1\u606f\u3002\u5229\u7528\u4e0a\u4e0b\u6587\u4fe1\u606f\u53ef\u4ee5\u5e2e\u52a9\u5f15\u5bfc\u751f\u6210\uff0c\u4f46\u662f\u786e\u5b9a\u67e5\u8be2\u4f55\u65f6\u80fd\u4ece\u68c0\u7d22\u5230\u7684\u4e0a\u4e0b\u6587\u4e2d\u83b7\u76ca\u4ee5\u53ca\u8bc4\u4f30\u4e0a\u4e0b\u6587\u7684\u6709\u6548\u6027\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u672c\u6587\u8fd0\u7528\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u786e\u5b9a\u662f\u5426\u4ec5\u4ece\u6a21\u578b\u7684\u6fc0\u6d3b\u72b6\u6001\u5c31\u80fd\u9884\u6d4b\u6a21\u578b\u8f93\u51fa\u7684\u6b63\u786e\u6027\u3002\u540c\u65f6\uff0c\u63a2\u7d22\u6a21\u578b\u5185\u90e8\u662f\u5426\u5305\u542b\u5173\u4e8e\u5916\u90e8\u4e0a\u4e0b\u6587\u6709\u6548\u6027\u7684\u4fe1\u53f7\u3002\u8003\u8651\u6b63\u786e\u3001\u4e0d\u6b63\u786e\u548c\u4e0d\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\uff0c\u5e76\u5f15\u5165\u6307\u6807\u6765\u533a\u5206\u5b83\u4eec\u3002", "result": "\u5728\u516d\u4e2a\u4e0d\u540c\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e00\u4e2a\u5728\u7b2c\u4e00\u4e2a\u8f93\u51fatoken\u7684\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u72b6\u6001\u4e0a\u8bad\u7ec3\u7684\u7b80\u5355\u5206\u7c7b\u5668\uff0c\u53ef\u4ee5\u5927\u7ea675%\u7684\u51c6\u786e\u7387\u9884\u6d4b\u8f93\u51fa\u7684\u6b63\u786e\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u65e9\u671f\u5ba1\u8ba1\u3002\u57fa\u4e8e\u6a21\u578b\u5185\u90e8\u7684\u6307\u6807\u5728\u533a\u5206\u6b63\u786e\u548c\u4e0d\u6b63\u786e\u7684\u4e0a\u4e0b\u6587\u65b9\u9762\u663e\u8457\u4f18\u4e8eprompting baselines\uff0c\u9632\u6b62\u53d7\u6c61\u67d3\u7684\u4e0a\u4e0b\u6587\u5f15\u5165\u7684\u4e0d\u51c6\u786e\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u89c6\u89d2\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3LLM\u7684\u5e95\u5c42\u51b3\u7b56\u8fc7\u7a0b\u3002"}}
{"id": "2510.03606", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.03606", "abs": "https://arxiv.org/abs/2510.03606", "authors": ["Mattia Scardecchia"], "title": "Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops", "comment": null, "summary": "Recent advances in self-supervised learning (SSL) have made it possible to\nlearn general-purpose visual features that capture both the high-level\nsemantics and the fine-grained spatial structure of images. Most notably, the\nrecent DINOv2 has established a new state of the art by surpassing weakly\nsupervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we\nexamine the core ideas behind its approach, multi-crop view augmentation and\nself-distillation with a mean teacher, and trace their development in previous\nwork. We then compare the performance of DINO and DINOv2 with other SSL and WSL\nmethods across various downstream tasks, and highlight some remarkable emergent\nproperties of their learned features with transformer backbones. We conclude by\nbriefly discussing DINOv2's limitations, its impact, and future research\ndirections.", "AI": {"tldr": "DINOv2 achieves state-of-the-art performance in self-supervised learning, surpassing weakly supervised methods.", "motivation": "Examine the core ideas behind DINOv2's approach, multi-crop view augmentation and self-distillation with a mean teacher, and trace their development in previous work.", "method": "Compare the performance of DINO and DINOv2 with other SSL and WSL methods across various downstream tasks.", "result": "Highlight some remarkable emergent properties of their learned features with transformer backbones.", "conclusion": "Briefly discuss DINOv2's limitations, its impact, and future research directions."}}
{"id": "2510.03283", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.03283", "abs": "https://arxiv.org/abs/2510.03283", "authors": ["Yufei Li", "Yu Fu", "Yue Dong", "Cong Liu"], "title": "MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment", "comment": "14 pages, 15 figures", "summary": "Large language models (LLMs) deployed on edge servers are increasingly used\nin latency-sensitive applications such as personalized assistants,\nrecommendation, and content moderation. However, the non-stationary nature of\nuser data necessitates frequent retraining, which introduces a fundamental\ntension between inference latency and model accuracy under constrained GPU\nresources. Existing retraining strategies either delay model updates,\nover-commit resources to retraining, or overlook iteration-level retraining\ngranularity. In this paper, we identify that iteration-level scheduling is\ncrucial for adapting retraining frequency to model drift without violating\nservice-level objectives (SLOs). We propose MACE, a hybrid LLM system that\ncolocates concurrent inference (prefill, decode) and fine-tuning, with\nintelligent memory management to maximize task performance while promising\ninference throughput. MACE leverages the insight that not all model updates\nequally affect output alignment and allocates GPU cycles accordingly to balance\nthroughput, latency, and update freshness. Our trace-driven evaluation shows\nthat MACE matches or exceeds continuous retraining while reducing inference\nlatency by up to 63% and maintaining throughput under resource constraints.\nCompared to periodic retraining, MACE improves latency breakdown across\nprefill, decode, and finetune stages, and sustains GPU utilization above 85% in\nNVIDIA AGX Orin. These results demonstrate that iteration-level hybrid\nscheduling is a promising direction for deploying LLMs with continual learning\ncapabilities on edge platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMACE\u7684\u6df7\u5408LLM\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5728\u8fb9\u7f18\u670d\u52a1\u5668\u4e0a\u5171\u7f6e\u5e76\u53d1\u63a8\u7406\u548c\u5fae\u8c03\uff0c\u5e76\u901a\u8fc7\u667a\u80fd\u5185\u5b58\u7ba1\u7406\u6765\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u4efb\u52a1\u6027\u80fd\u5e76\u4fdd\u8bc1\u63a8\u7406\u541e\u5410\u91cf\u3002", "motivation": "\u8fb9\u7f18\u670d\u52a1\u5668\u4e0a\u90e8\u7f72\u7684LLM\u9700\u8981\u9891\u7e41\u7684\u91cd\u65b0\u8bad\u7ec3\uff0c\u4f46\u5728GPU\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u63a8\u7406\u5ef6\u8fdf\u548c\u6a21\u578b\u7cbe\u5ea6\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u77db\u76fe\u3002\u73b0\u6709\u7684\u91cd\u65b0\u8bad\u7ec3\u7b56\u7565\u8981\u4e48\u5ef6\u8fdf\u6a21\u578b\u66f4\u65b0\uff0c\u8981\u4e48\u8fc7\u5ea6\u5206\u914d\u8d44\u6e90\u7ed9\u91cd\u65b0\u8bad\u7ec3\uff0c\u8981\u4e48\u5ffd\u7565\u8fed\u4ee3\u7ea7\u522b\u7684\u91cd\u65b0\u8bad\u7ec3\u7c92\u5ea6\u3002", "method": "MACE\u5229\u7528\u8fed\u4ee3\u7ea7\u8c03\u5ea6\u5bf9\u4e8e\u9002\u5e94\u6a21\u578b\u6f02\u79fb\u81f3\u5173\u91cd\u8981\u7684\u6d1e\u5bdf\u529b\uff0c\u5e76\u76f8\u5e94\u5730\u5206\u914dGPU\u5468\u671f\u4ee5\u5e73\u8861\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u548c\u66f4\u65b0\u9c9c\u5ea6\u3002", "result": "MACE\u5728\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5339\u914d\u6216\u8d85\u8fc7\u4e86\u8fde\u7eed\u91cd\u65b0\u8bad\u7ec3\uff0c\u540c\u65f6\u5c06\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u4e86\u9ad8\u8fbe63%\uff0c\u5e76\u4fdd\u6301\u4e86\u541e\u5410\u91cf\u3002MACE\u6539\u5584\u4e86\u9884\u586b\u5145\u3001\u89e3\u7801\u548c\u5fae\u8c03\u9636\u6bb5\u7684\u5ef6\u8fdf\u5206\u89e3\uff0c\u5e76\u5728NVIDIA AGX Orin\u4e2d\u7ef4\u6301\u4e8685%\u4ee5\u4e0a\u7684GPU\u5229\u7528\u7387\u3002", "conclusion": "\u8fed\u4ee3\u7ea7\u6df7\u5408\u8c03\u5ea6\u662f\u5728\u8fb9\u7f18\u5e73\u53f0\u90e8\u7f72\u5177\u6709\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u7684LLM\u7684\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002"}}
{"id": "2510.04097", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04097", "abs": "https://arxiv.org/abs/2510.04097", "authors": ["Peichao Lai", "Jinhui Zhuang", "Kexuan Zhang", "Ningchang Xiong", "Shengjie Wang", "Yanwei Xu", "Chong Chen", "Yilei Wang", "Bin Cui"], "title": "WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning", "comment": null, "summary": "Automating the conversion of UI images into web code is a critical task for\nfront-end development and rapid prototyping. Advances in multimodal large\nlanguage models (MLLMs) have made WebUI-to-Code increasingly feasible, yet\nexisting benchmarks remain limited in data diversity and evaluation\nreliability. To address these issues, we present WebRenderBench, a large-scale\nbenchmark of 22.5k webpages collected from real-world portal sites, offering\ngreater diversity, complexity, and realism than prior benchmarks. We further\npropose a novel evaluation metric that measures layout and style consistency\nfrom the final rendered pages. Unlike vision-based methods that rely on costly\nLLM reasoning or structure-based comparisons vulnerable to noise and asymmetry,\nour approach enables more efficient, objective, and reliable UI quality\nassessment. Finally, we introduce the Automated Layout and Style Inspection\nAgent (ALISA), which integrates this metric into reinforcement learning as a\nreward signal to enhance training on crawled asymmetric webpages. Experiments\nshow that ALISA significantly boosts generation performance, achieving\nstate-of-the-art results across multiple metrics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aWebRenderBench\u7684\u5927\u89c4\u6a21\u7f51\u9875UI\u5230\u4ee3\u7801\u8f6c\u6362\u7684\u57fa\u51c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u6765\u8861\u91cf\u5e03\u5c40\u548c\u6837\u5f0f\u7684\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aALISA\u7684\u667a\u80fd\u4f53\uff0c\u5b83\u5c06\u8fd9\u4e2a\u6307\u6807\u6574\u5408\u5230\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u5728\u722c\u53d6\u7684\u975e\u5bf9\u79f0\u7f51\u9875\u4e0a\u7684\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684WebUI\u5230\u4ee3\u7801\u7684\u57fa\u51c6\u5728\u6570\u636e\u591a\u6837\u6027\u548c\u8bc4\u4f30\u53ef\u9760\u6027\u65b9\u9762\u4ecd\u7136\u6709\u9650\u3002", "method": "1. \u63d0\u51fa\u4e86WebRenderBench\u57fa\u51c6\uff0c\u5305\u542b22.5k\u4e2a\u6765\u81ea\u771f\u5b9e\u95e8\u6237\u7f51\u7ad9\u7684\u7f51\u9875\u30022. \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u6d4b\u91cf\u6700\u7ec8\u6e32\u67d3\u9875\u9762\u7684\u5e03\u5c40\u548c\u6837\u5f0f\u4e00\u81f4\u6027\u30023. \u5f15\u5165\u4e86ALISA\uff0c\u5b83\u5c06\u8bc4\u4f30\u6307\u6807\u6574\u5408\u5230\u5f3a\u5316\u5b66\u4e60\u4e2d\u3002", "result": "ALISA\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684WebRenderBench\u57fa\u51c6\u548cALISA\u667a\u80fd\u4f53\u80fd\u591f\u6709\u6548\u63d0\u5347WebUI\u5230\u4ee3\u7801\u7684\u8f6c\u6362\u6548\u679c\uff0c\u5e76\u5728\u975e\u5bf9\u79f0\u7f51\u9875\u4e0a\u5b9e\u73b0\u66f4\u597d\u7684\u8bad\u7ec3\u3002"}}
{"id": "2510.04016", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04016", "abs": "https://arxiv.org/abs/2510.04016", "authors": ["Thanapol Popit", "Natthapath Rungseesiripak", "Monthol Charattrakool", "Saksorn Ruangtanusak"], "title": "Thai Semantic End-of-Turn Detection for Real-Time Voice Agents", "comment": "IEEE ICSEC 2025", "summary": "Fluid voice-to-voice interaction requires reliable and low-latency detection\nof when a user has finished speaking. Traditional audio-silence end-pointers\nadd hundreds of milliseconds of delay and fail under hesitations or\nlanguage-specific phenomena. We present, to our knowledge, the first systematic\nstudy of Thai text-only end-of-turn (EOT) detection for real-time agents. We\ncompare zero-shot and few-shot prompting of compact LLMs to supervised\nfine-tuning of lightweight transformers. Using transcribed subtitles from the\nYODAS corpus and Thai-specific linguistic cues (e.g., sentence-final\nparticles), we formulate EOT as a binary decision over token boundaries. We\nreport a clear accuracy-latency tradeoff and provide a public-ready\nimplementation plan. This work establishes a Thai baseline and demonstrates\nthat small, fine-tuned models can deliver near-instant EOT decisions suitable\nfor on-device agents.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6cf0\u8bed\u8bed\u97f3\u4ea4\u4e92\u4e2d\uff0c\u5982\u4f55\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u53ef\u9760\u5730\u68c0\u6d4b\u7528\u6237\u4f55\u65f6\u7ed3\u675f\u53d1\u8a00\u3002", "motivation": "\u4f20\u7edf\u7684\u9759\u97f3\u68c0\u6d4b\u65b9\u6cd5\u5ef6\u8fdf\u9ad8\u4e14\u5728\u7279\u5b9a\u8bed\u8a00\u73af\u5883\u4e0b\u5931\u6548\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u7814\u7a76\u6cf0\u8bed\u73af\u5883\u4e0b\u57fa\u4e8e\u6587\u672c\u7684EOT\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u5bf9\u6bd4\u4e86\u5c0f\u578bLLM\u7684\u96f6\u6837\u672c/\u5c11\u6837\u672c\u63d0\u793a\u548c\u8f7b\u91cf\u7ea7Transformer\u7684\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u3002\u5229\u7528YODAS\u8bed\u6599\u5e93\u548c\u6cf0\u8bed\u7279\u5b9a\u8bed\u8a00\u7279\u5f81\uff0c\u5c06EOT\u95ee\u9898\u8f6c\u5316\u4e3atoken\u8fb9\u754c\u4e0a\u7684\u4e8c\u5143\u51b3\u7b56\u3002", "result": "\u672c\u6587\u62a5\u544a\u4e86\u51c6\u786e\u7387\u548c\u5ef6\u8fdf\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u516c\u5f00\u4f7f\u7528\u7684\u5b9e\u73b0\u65b9\u6848\u3002", "conclusion": "\u672c\u6587\u4e3a\u6cf0\u8bedEOT\u68c0\u6d4b\u5efa\u7acb\u4e86\u4e00\u4e2a\u57fa\u7ebf\uff0c\u5e76\u8bc1\u660e\u5c0f\u578b\u5fae\u8c03\u6a21\u578b\u53ef\u4ee5\u5b9e\u73b0\u8fd1\u4e4e\u5b9e\u65f6\u7684EOT\u51b3\u7b56\uff0c\u9002\u7528\u4e8e\u672c\u5730\u8bbe\u5907\u4e0a\u7684\u667a\u80fd\u4f53\u3002"}}
{"id": "2510.03608", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03608", "abs": "https://arxiv.org/abs/2510.03608", "authors": ["Ruitao Wu", "Yifan Zhao", "Guangyao Chen", "Jia Li"], "title": "Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual Boosting Loop for FSCIL", "comment": "Accepted by NeurIPS 2025", "summary": "Few-Shot Class-Incremental Learning (FSCIL) challenges models to sequentially\nlearn new classes from minimal examples without forgetting prior knowledge, a\ntask complicated by the stability-plasticity dilemma and data scarcity. Current\nFSCIL methods often struggle with generalization due to their reliance on\nlimited datasets. While diffusion models offer a path for data augmentation,\ntheir direct application can lead to semantic misalignment or ineffective\nguidance. This paper introduces Diffusion-Classifier Synergy (DCS), a novel\nframework that establishes a mutual boosting loop between diffusion model and\nFSCIL classifier. DCS utilizes a reward-aligned learning strategy, where a\ndynamic, multi-faceted reward function derived from the classifier's state\ndirects the diffusion model. This reward system operates at two levels: the\nfeature level ensures semantic coherence and diversity using prototype-anchored\nmaximum mean discrepancy and dimension-wise variance matching, while the logits\nlevel promotes exploratory image generation and enhances inter-class\ndiscriminability through confidence recalibration and cross-session\nconfusion-aware mechanisms. This co-evolutionary process, where generated\nimages refine the classifier and an improved classifier state yields better\nreward signals, demonstrably achieves state-of-the-art performance on FSCIL\nbenchmarks, significantly enhancing both knowledge retention and new class\nlearning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548cFSCIL\u5206\u7c7b\u5668\u4e4b\u95f4\u7684\u76f8\u4e92\u63d0\u5347\u5faa\u73af\u6765\u89e3\u51b3Few-Shot Class-Incremental Learning (FSCIL) \u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684FSCIL\u65b9\u6cd5\u7531\u4e8e\u4f9d\u8d56\u6709\u9650\u7684\u6570\u636e\u96c6\uff0c\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002\u76f4\u63a5\u5e94\u7528\u6269\u6563\u6a21\u578b\u53ef\u80fd\u5bfc\u81f4\u8bed\u4e49\u9519\u4f4d\u6216\u65e0\u6548\u6307\u5bfc\u3002", "method": "\u5f15\u5165\u4e86Diffusion-Classifier Synergy (DCS)\uff0c\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u5b83\u5728\u6269\u6563\u6a21\u578b\u548cFSCIL\u5206\u7c7b\u5668\u4e4b\u95f4\u5efa\u7acb\u4e86\u4e00\u4e2a\u76f8\u4e92\u4fc3\u8fdb\u7684\u5faa\u73af\u3002DCS\u5229\u7528\u5956\u52b1\u5bf9\u9f50\u7684\u5b66\u4e60\u7b56\u7565\uff0c\u5176\u4e2d\u4ece\u5206\u7c7b\u5668\u7684\u72b6\u6001\u5bfc\u51fa\u7684\u52a8\u6001\u3001\u591a\u65b9\u9762\u7684\u5956\u52b1\u51fd\u6570\u6307\u5bfc\u6269\u6563\u6a21\u578b\u3002", "result": "\u5728FSCIL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDCS \u663e\u8457\u63d0\u9ad8\u4e86\u77e5\u8bc6\u4fdd\u7559\u548c\u65b0\u7c7b\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u751f\u6210\u7684\u56fe\u50cf\u4f18\u5316\u5206\u7c7b\u5668\uff0c\u6539\u8fdb\u7684\u5206\u7c7b\u5668\u72b6\u6001\u4ea7\u751f\u66f4\u597d\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u8fd9\u79cd\u534f\u540c\u8fdb\u5316\u8fc7\u7a0b\uff0c\u7ecf\u9a8c\u8bc1\u53ef\u4ee5\u5b9e\u73b0FSCIL\u7684\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2510.03284", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03284", "abs": "https://arxiv.org/abs/2510.03284", "authors": ["Vinay Venkatesh", "Vamsidhar R Kamanuru", "Lav Kumar", "Nikita Kothari"], "title": "Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments", "comment": "7 pages, 1 figure", "summary": "This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a\nscalable framework for Federated Instruction Tuning (FIT) of Large Language\nModels (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail\nwhen confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT\nframework combines federated learning with 4-bit Quantized Low-Rank Adaptation\n(QLORA), mitigating the core issues of communication and computational\noverhead. We demonstrate this by filtering the general-purpose Databricks Dolly\n15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned\nLlama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable\ntrade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable\nframework for decentralized LLM deployment on home compute gateways.", "AI": {"tldr": "\u63d0\u51fa\u4e86Edge-FIT\uff0c\u4e00\u4e2a\u7528\u4e8eLLM\u7684\u8054\u90a6\u6307\u4ee4\u8c03\u6574\u6846\u67b6\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u9762\u5bf9LLM\u7684\u5927\u91cf\u53c2\u6570\u65f6\u4f1a\u5931\u8d25\u3002", "method": "Edge-FIT\u7ed3\u5408\u4e86\u8054\u90a6\u5b66\u4e60\u548c4\u4f4d\u91cf\u5316\u4f4e\u79e9\u9002\u914d(QLORA)\u3002", "result": "Edge-FIT\u8c03\u6574\u7684Llama 2(7B)\u8fbe\u5230\u4e860.89\u7684F1\u5206\u6570\u3002\u4f7f\u75283.8B Phi-3-mini\u6a21\u578b\u9a8c\u8bc1\u4e86Edge-FIT\u4f5c\u4e3a\u5bb6\u5ead\u8ba1\u7b97\u7f51\u5173\u4e0a\u5206\u6563\u5f0fLLM\u90e8\u7f72\u7684\u53ef\u6269\u5c55\u6846\u67b6\u3002", "conclusion": "Edge-FIT\u662f\u4e00\u4e2a\u7528\u4e8e\u5206\u6563\u5f0fLLM\u90e8\u7f72\u7684\u53ef\u6269\u5c55\u6846\u67b6\u3002"}}
{"id": "2510.04116", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04116", "abs": "https://arxiv.org/abs/2510.04116", "authors": ["Ziying Zhang", "Yaqing Wang", "Quanming Yao"], "title": "Searching Meta Reasoning Skeleton to Guide LLM Reasoning", "comment": null, "summary": "Meta reasoning behaviors work as a skeleton to guide large language model\n(LLM) reasoning, thus help to improve reasoning performance. However, prior\nresearches implement meta reasoning skeleton with manually designed structure,\nlimiting ability to adapt to query-specific requirement and capture intricate\nlogical dependency among reasoning steps. To deal with the challenges, we\nrepresent meta reasoning skeleton with directed acyclic graph (DAG) to unify\nskeletons proposed in prior works and model intricate logical dependency. Then\nwe propose AutoMR, a framework that searches for query-aware meta reasoning\nskeleton automatically inspired by automated machine learning (AutoML).\nSpecifically, we construct search space based on DAG representation of skeleton\nand then formulate the search problem. We design a dynamic skeleton sampling\nalgorithm by expanding meta reasoning skeleton along with reasoning context at\ninference time. This algorithm can derive any meta reasoning skeleton in search\nspace efficiently and adapt skeleton to evolving base reasoning context, thus\nenable efficient query-aware skeleton search. We conduct experiments on\nextensive benchmark datasets. Experimental results show that AutoMR achieves\nbetter reasoning performance than previous works broadly.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAutoMR\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u6709\u5411\u65e0\u73af\u56fe(DAG)\u8868\u793a\u5143\u63a8\u7406\u9aa8\u67b6\uff0c\u5e76\u81ea\u52a8\u641c\u7d22\u67e5\u8be2\u611f\u77e5\u7684\u5143\u63a8\u7406\u9aa8\u67b6\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u4f7f\u7528\u624b\u52a8\u8bbe\u8ba1\u7684\u7ed3\u6784\u5b9e\u73b0\u5143\u63a8\u7406\u9aa8\u67b6\uff0c\u9650\u5236\u4e86\u9002\u5e94\u7279\u5b9a\u67e5\u8be2\u9700\u6c42\u548c\u6355\u83b7\u63a8\u7406\u6b65\u9aa4\u4e4b\u95f4\u590d\u6742\u903b\u8f91\u4f9d\u8d56\u5173\u7cfb\u7684\u80fd\u529b\u3002", "method": "\u8be5\u65b9\u6cd5\u4f7f\u7528\u6709\u5411\u65e0\u73af\u56fe(DAG)\u8868\u793a\u5143\u63a8\u7406\u9aa8\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u52a8\u6001\u9aa8\u67b6\u62bd\u6837\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u53ef\u4ee5\u5728\u63a8\u7406\u65f6\u968f\u7740\u63a8\u7406\u4e0a\u4e0b\u6587\u6269\u5c55\u5143\u63a8\u7406\u9aa8\u67b6\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u7684\u67e5\u8be2\u611f\u77e5\u9aa8\u67b6\u641c\u7d22\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAutoMR\u5728\u63a8\u7406\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u4ee5\u5f80\u7684\u7814\u7a76\u3002", "conclusion": "AutoMR\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u641c\u7d22\u67e5\u8be2\u611f\u77e5\u7684\u5143\u63a8\u7406\u9aa8\u67b6\uff0c\u4ece\u800c\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2510.04031", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04031", "abs": "https://arxiv.org/abs/2510.04031", "authors": ["Nelvin Tan", "James Asikin Cheung", "Yu-Ching Shih", "Dong Yang", "Amol Salunkhe"], "title": "Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?", "comment": "8 pages, 2 figures", "summary": "Large language models (LLMs) are becoming useful in many domains due to their\nimpressive abilities that arise from large training datasets and large model\nsizes. More recently, they have been shown to be very effective in textual\nclassification tasks, motivating the need to explain the LLMs' decisions.\nMotivated by practical constrains where LLMs are black-boxed and LLM calls are\nexpensive, we study how incorporating counterfactuals into LLM reasoning can\naffect the LLM's ability to identify the top words that have contributed to its\nclassification decision. To this end, we introduce a framework called the\ndecision changing rate that helps us quantify the importance of the top words\nin classification. Our experimental results show that using counterfactuals can\nbe helpful.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u51b3\u7b56\u8fc7\u7a0b\u9700\u8981\u89e3\u91ca\u3002\u672c\u6587\u7814\u7a76\u4e86\u5728LLM\u63a8\u7406\u4e2d\u52a0\u5165\u53cd\u4e8b\u5b9e\u5206\u6790\u5982\u4f55\u5f71\u54cdLLM\u8bc6\u522b\u5173\u952e\u8d21\u732e\u8bcd\u8bed\u7684\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u7279\u522b\u662f\u5728LLM\u4e3a\u9ed1\u76d2\u4e14\u8c03\u7528\u6210\u672c\u9ad8\u6602\u7684\u5b9e\u9645\u7ea6\u675f\u4e0b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u51b3\u7b56\u53d8\u5316\u7387\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u5206\u7c7b\u4e2d\u5173\u952e\u5355\u8bcd\u7684\u91cd\u8981\u6027\u3002\u901a\u8fc7\u5c06\u53cd\u4e8b\u5b9e\u5206\u6790\u878d\u5165LLM\u63a8\u7406\uff0c\u89c2\u5bdf\u5176\u5bf9\u8bc6\u522b\u5173\u952e\u5355\u8bcd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u53cd\u4e8b\u5b9e\u5206\u6790\u662f\u6709\u5e2e\u52a9\u7684\u3002", "conclusion": "\u53cd\u4e8b\u5b9e\u5206\u6790\u53ef\u4ee5\u5e2e\u52a9LLM\u8bc6\u522b\u5bf9\u5176\u5206\u7c7b\u51b3\u7b56\u8d21\u732e\u6700\u5927\u7684\u8bcd\u8bed\u3002"}}
{"id": "2510.03666", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03666", "abs": "https://arxiv.org/abs/2510.03666", "authors": ["Jiang Wu", "Sichao Wu", "Yinsong Ma", "Guangyuan Yu", "Haoyuan Xu", "Lifang Zheng", "Jingliang Duan"], "title": "MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations", "comment": null, "summary": "Industrial accidents, particularly in high-risk domains such as surface and\nunderground mining, are frequently caused by unsafe worker behaviors.\nTraditional manual inspection remains labor-intensive, error-prone, and\ninsufficient for large-scale, dynamic environments, highlighting the urgent\nneed for intelligent and automated safety monitoring. In this paper, we present\nMonitorVLM, a novel vision--language framework designed to detect safety\nviolations directly from surveillance video streams. MonitorVLM introduces\nthree key innovations: (1) a domain-specific violation dataset comprising 9,000\nvision--question--answer (VQA) samples across 40 high-frequency mining\nregulations, enriched with augmentation and auxiliary detection cues; (2) a\nclause filter (CF) module that dynamically selects the Top-$K$ most relevant\nclauses, reducing inference latency by 13.56\\% while maintaining accuracy; and\n(3) a behavior magnifier (BM) module that enhances worker regions to improve\nfine-grained action recognition, yielding additional gains of 3.45% in\nprecision and 8.62% in recall. Experimental results demonstrate that MonitorVLM\nsignificantly outperforms baseline vision--language models, achieving\nimprovements of 22.01% in precision, 34.22\\% in recall, and 28.37% in F1 score\nover the 72B unfine-tuned baseline. A lightweight web-based interface further\nintegrates MonitorVLM into practical workflows, enabling automatic violation\nreporting with video timestamping. This study highlights the potential of\nmultimodal large models to enhance occupational safety monitoring in mining and\nbeyond.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMonitorVLM\u7684\u89c6\u89c9-\u8bed\u8a00\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u76d1\u63a7\u89c6\u9891\u4e2d\u68c0\u6d4b\u5b89\u5168\u8fdd\u89c4\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u77ff\u4e1a\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u5de5\u68c0\u67e5\u52b3\u52a8\u5f3a\u5ea6\u5927\uff0c\u5bb9\u6613\u51fa\u9519\uff0c\u5e76\u4e14\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u5927\u89c4\u6a21\u52a8\u6001\u73af\u5883\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u667a\u80fd\u548c\u81ea\u52a8\u5316\u7684\u5b89\u5168\u76d1\u63a7\u3002", "method": "\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u4e00\u4e2a\u9886\u57df\u7279\u5b9a\u7684\u8fdd\u89c4\u6570\u636e\u96c6\uff0c\u4e00\u4e2a\u52a8\u6001\u9009\u62e9\u6700\u76f8\u5173\u6761\u6b3e\u7684\u5b50\u53e5\u8fc7\u6ee4\u5668\uff08CF\uff09\u6a21\u5757\uff0c\u4ee5\u53ca\u4e00\u4e2a\u589e\u5f3a\u5de5\u4eba\u533a\u57df\u4ee5\u6539\u8fdb\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u8bc6\u522b\u7684\u884c\u4e3a\u653e\u5927\u5668\uff08BM\uff09\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMonitorVLM\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5f97\u5206\u65b9\u9762\u5747\u6709\u63d0\u9ad8\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u5927\u578b\u6a21\u578b\u5728\u52a0\u5f3a\u77ff\u4e1a\u53ca\u5176\u4ed6\u884c\u4e1a\u7684\u804c\u4e1a\u5b89\u5168\u76d1\u63a7\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.03288", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03288", "abs": "https://arxiv.org/abs/2510.03288", "authors": ["Chiming Duan", "Minghua He", "Pei Xiao", "Tong Jia", "Xin Zhang", "Zhewei Zhong", "Xiang Luo", "Yan Niu", "Lingzhe Zhang", "Yifan Wu", "Siyu Yu", "Weijie Hong", "Ying Li", "Gang Huang"], "title": "LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain", "comment": "The 40th IEEE/ACM International Conference on Automated Software\n  Engineering, ASE 2025", "summary": "Log-based anomaly detection is a essential task for ensuring the reliability\nand performance of software systems. However, the performance of existing\nanomaly detection methods heavily relies on labeling, while labeling a large\nvolume of logs is highly challenging. To address this issue, many approaches\nbased on transfer learning and active learning have been proposed.\nNevertheless, their effectiveness is hindered by issues such as the gap between\nsource and target system data distributions and cold-start problems. In this\npaper, we propose LogAction, a novel log-based anomaly detection model based on\nactive domain adaptation. LogAction integrates transfer learning and active\nlearning techniques. On one hand, it uses labeled data from a mature system to\ntrain a base model, mitigating the cold-start issue in active learning. On the\nother hand, LogAction utilize free energy-based sampling and uncertainty-based\nsampling to select logs located at the distribution boundaries for manual\nlabeling, thus addresses the data distribution gap in transfer learning with\nminimal human labeling efforts. Experimental results on six different\ncombinations of datasets demonstrate that LogAction achieves an average 93.01%\nF1 score with only 2% of manual labels, outperforming some state-of-the-art\nmethods by 26.28%. Website: https://logaction.github.io", "AI": {"tldr": "LogAction: A log-based anomaly detection model using active domain adaptation to address labeling challenges and distribution gaps.", "motivation": "Existing anomaly detection methods rely heavily on labeled data, which is challenging to obtain. Transfer learning and active learning approaches are hindered by distribution gaps and cold-start problems.", "method": "LogAction integrates transfer learning and active learning. It uses labeled data from a mature system and employs free energy-based and uncertainty-based sampling for manual labeling of logs at distribution boundaries.", "result": "LogAction achieves an average 93.01% F1 score with only 2% of manual labels, outperforming state-of-the-art methods by 26.28% on six datasets.", "conclusion": "LogAction effectively addresses the challenges of log-based anomaly detection by combining transfer learning and active learning to minimize labeling effort and bridge data distribution gaps."}}
{"id": "2510.04128", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04128", "abs": "https://arxiv.org/abs/2510.04128", "authors": ["Dmitrii Troitskii", "Koyena Pal", "Chris Wendler", "Callum Stuart McDougall", "Neel Nanda"], "title": "Internal states before wait modulate reasoning patterns", "comment": "Accepted to EMNLP Findings 2025", "summary": "Prior work has shown that a significant driver of performance in reasoning\nmodels is their ability to reason and self-correct. A distinctive marker in\nthese reasoning traces is the token wait, which often signals reasoning\nbehavior such as backtracking. Despite being such a complex behavior, little is\nunderstood of exactly why models do or do not decide to reason in this\nparticular manner, which limits our understanding of what makes a reasoning\nmodel so effective. In this work, we address the question whether model's\nlatents preceding wait tokens contain relevant information for modulating the\nsubsequent reasoning process. We train crosscoders at multiple layers of\nDeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent\nattribution technique in the crosscoder setting. We locate a small set of\nfeatures relevant for promoting/suppressing wait tokens' probabilities.\nFinally, through a targeted series of experiments analyzing max activating\nexamples and causal interventions, we show that many of our identified features\nindeed are relevant for the reasoning process and give rise to different types\nof reasoning patterns such as restarting from the beginning, recalling prior\nknowledge, expressing uncertainty, and double-checking.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u63a8\u7406\u6a21\u578b\u4e2dwait token\u4e4b\u524d\u7684\u6a21\u578b\u6f5c\u5728\u4fe1\u606f\u662f\u5426\u5305\u542b\u8c03\u8282\u540e\u7eed\u63a8\u7406\u8fc7\u7a0b\u7684\u76f8\u5173\u4fe1\u606f\u3002", "motivation": "\u5148\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u63a8\u7406\u6a21\u578b\u4e2d\u7684wait token\u662f\u63a8\u7406\u80fd\u529b\u548c\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\u7684\u91cd\u8981\u6807\u5fd7\uff0c\u4f46\u5bf9\u5176\u5177\u4f53\u539f\u56e0\u77e5\u4e4b\u751a\u5c11\uff0c\u9650\u5236\u4e86\u6211\u4eec\u5bf9\u6709\u6548\u63a8\u7406\u6a21\u578b\u7684\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3DeepSeek-R1-Distill-Llama-8B\u53ca\u5176\u57fa\u7840\u7248\u672c\u7684\u591a\u4e2a\u5c42\u7684\u4ea4\u53c9\u7f16\u7801\u5668\uff0c\u5e76\u5f15\u5165\u4ea4\u53c9\u7f16\u7801\u5668\u8bbe\u7f6e\u4e2d\u7684\u6f5c\u5728\u5c5e\u6027\u6280\u672f\uff0c\u6765\u5b9a\u4f4d\u4fc3\u8fdb/\u6291\u5236wait token\u6982\u7387\u7684\u76f8\u5173\u7279\u5f81\u3002", "result": "\u627e\u5230\u4e86\u4e0e\u63a8\u7406\u8fc7\u7a0b\u76f8\u5173\u7684\u5c11\u91cf\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u4f1a\u5f15\u8d77\u4e0d\u540c\u7c7b\u578b\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u4f8b\u5982\u4ece\u5934\u5f00\u59cb\u3001\u56de\u5fc6\u5148\u9a8c\u77e5\u8bc6\u3001\u8868\u8fbe\u4e0d\u786e\u5b9a\u6027\u548c\u4ed4\u7ec6\u68c0\u67e5\u3002", "conclusion": "\u8bc1\u660e\u4e86\u8bc6\u522b\u51fa\u7684\u8bb8\u591a\u7279\u5f81\u786e\u5b9e\u4e0e\u63a8\u7406\u8fc7\u7a0b\u76f8\u5173\uff0c\u5e76\u5f15\u8d77\u4e0d\u540c\u7c7b\u578b\u7684\u63a8\u7406\u6a21\u5f0f\u3002"}}
{"id": "2510.04032", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04032", "abs": "https://arxiv.org/abs/2510.04032", "authors": ["Zirui Wang", "Jiajun Wu", "Braden Teitge", "Jessalyn Holodinsky", "Steve Drew"], "title": "Small Language Models for Emergency Departments Decision Support: A Benchmark Study", "comment": "Accepted to 2025 IEEE International Conference on Autonomous and\n  Trusted Computing (ATC 2025)", "summary": "Large language models (LLMs) have become increasingly popular in medical\ndomains to assist physicians with a variety of clinical and operational tasks.\nGiven the fast-paced and high-stakes environment of emergency departments\n(EDs), small language models (SLMs), characterized by a reduction in parameter\ncount compared to LLMs, offer significant potential due to their inherent\nreasoning capability and efficient performance. This enables SLMs to support\nphysicians by providing timely and accurate information synthesis, thereby\nimproving clinical decision-making and workflow efficiency. In this paper, we\npresent a comprehensive benchmark designed to identify SLMs suited for ED\ndecision support, taking into account both specialized medical expertise and\nbroad general problem-solving capabilities. In our evaluations, we focus on\nSLMs that have been trained on a mixture of general-domain and medical corpora.\nA key motivation for emphasizing SLMs is the practical hardware limitations,\noperational cost constraints, and privacy concerns in the typical real-world\ndeployments. Our benchmark datasets include MedMCQA, MedQA-4Options, and\nPubMedQA, with the medical abstracts dataset emulating tasks aligned with real\nED physicians' daily tasks. Experimental results reveal that general-domain\nSLMs surprisingly outperform their medically fine-tuned counterparts across\nthese diverse benchmarks for ED. This indicates that for ED, specialized\nmedical fine-tuning of the model may not be required.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u8bc4\u4f30\u9002\u7528\u4e8e\u6025\u8bca\u79d1 (ED) \u51b3\u7b56\u652f\u6301\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b (SLM)\uff0c\u56e0\u4e3a\u5b83\u4eec\u5177\u6709\u63a8\u7406\u80fd\u529b\u548c\u9ad8\u6548\u6027\u80fd\uff0c\u540c\u65f6\u8003\u8651\u5230\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u786c\u4ef6\u9650\u5236\u3001\u8fd0\u8425\u6210\u672c\u548c\u9690\u79c1\u95ee\u9898\u3002", "motivation": "\u5728\u5feb\u8282\u594f\u548c\u9ad8\u98ce\u9669\u7684\u6025\u8bca\u79d1\u73af\u5883\u4e2d\uff0c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b (SLM) \u5177\u6709\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u56e0\u4e3a\u5b83\u4eec\u80fd\u591f\u63d0\u4f9b\u53ca\u65f6\u548c\u51c6\u786e\u7684\u4fe1\u606f\uff0c\u4ece\u800c\u6539\u5584\u4e34\u5e8a\u51b3\u7b56\u548c\u5de5\u4f5c\u6d41\u7a0b\u6548\u7387\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u7efc\u5408\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u9002\u5408\u6025\u8bca\u79d1\u51b3\u7b56\u652f\u6301\u7684 SLM\uff0c\u5e76\u91cd\u70b9\u5173\u6ce8\u5728\u901a\u7528\u9886\u57df\u548c\u533b\u5b66\u8bed\u6599\u5e93\u4e0a\u8bad\u7ec3\u7684 SLM\u3002\u57fa\u51c6\u6570\u636e\u96c6\u5305\u62ec MedMCQA\u3001MedQA-4Options \u548c PubMedQA\uff0c\u4ee5\u53ca\u6a21\u62df\u6025\u8bca\u79d1\u533b\u751f\u65e5\u5e38\u4efb\u52a1\u7684\u533b\u5b66\u6458\u8981\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u7528\u9886\u57df\u7684 SLM \u5728\u8fd9\u4e9b\u4e0d\u540c\u7684\u6025\u8bca\u79d1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u51fa\u4eba\u610f\u6599\u5730\u4f18\u4e8e\u7ecf\u8fc7\u533b\u5b66\u5fae\u8c03\u7684 SLM\u3002", "conclusion": "\u5bf9\u4e8e\u6025\u8bca\u79d1\u800c\u8a00\uff0c\u53ef\u80fd\u4e0d\u9700\u8981\u5bf9\u6a21\u578b\u8fdb\u884c\u4e13\u95e8\u7684\u533b\u5b66\u5fae\u8c03\u3002"}}
{"id": "2510.03675", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03675", "abs": "https://arxiv.org/abs/2510.03675", "authors": ["Siva Sai", "Saksham Gupta", "Vinay Chamola", "Rajkumar Buyya"], "title": "A Novel Cloud-Based Diffusion-Guided Hybrid Model for High-Accuracy Accident Detection in Intelligent Transportation Systems", "comment": null, "summary": "The integration of Diffusion Models into Intelligent Transportation Systems\n(ITS) is a substantial improvement in the detection of accidents. We present a\nnovel hybrid model integrating guidance classification with diffusion\ntechniques. By leveraging fine-tuned ExceptionNet architecture outputs as input\nfor our proposed diffusion model and processing image tensors as our\nconditioning, our approach creates a robust classification framework. Our model\nconsists of multiple conditional modules, which aim to modulate the linear\nprojection of inputs using time embeddings and image covariate embeddings,\nallowing the network to adapt its behavior dynamically throughout the diffusion\nprocess. To address the computationally intensive nature of diffusion models,\nour implementation is cloud-based, enabling scalable and efficient processing.\nOur strategy overcomes the shortcomings of conventional classification\napproaches by leveraging diffusion models inherent capacity to effectively\nunderstand complicated data distributions. We investigate important diffusion\ncharacteristics, such as timestep schedulers, timestep encoding techniques,\ntimestep count, and architectural design changes, using a thorough ablation\nstudy, and have conducted a comprehensive evaluation of the proposed model\nagainst the baseline models on a publicly available dataset. The proposed\ndiffusion model performs best in image-based accident detection with an\naccuracy of 97.32%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u6a21\u578b\uff0c\u5c06\u5f15\u5bfc\u5206\u7c7b\u4e0e\u6269\u6563\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u4e8b\u6545\u68c0\u6d4b\u3002", "motivation": "\u4f20\u7edf\u5206\u7c7b\u65b9\u6cd5\u5728\u7406\u89e3\u590d\u6742\u6570\u636e\u5206\u5e03\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u6269\u6563\u6a21\u578b\u5177\u6709\u6709\u6548\u7406\u89e3\u590d\u6742\u6570\u636e\u5206\u5e03\u7684\u5185\u5728\u80fd\u529b\u3002", "method": "\u8be5\u6a21\u578b\u5229\u7528\u5fae\u8c03\u7684ExceptionNet\u67b6\u6784\u8f93\u51fa\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u5904\u7406\u56fe\u50cf\u5f20\u91cf\u4f5c\u4e3a\u6761\u4ef6\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u9c81\u68d2\u7684\u5206\u7c7b\u6846\u67b6\u3002\u8be5\u6a21\u578b\u7531\u591a\u4e2a\u6761\u4ef6\u6a21\u5757\u7ec4\u6210\uff0c\u65e8\u5728\u5229\u7528\u65f6\u95f4\u5d4c\u5165\u548c\u56fe\u50cf\u534f\u53d8\u91cf\u5d4c\u5165\u6765\u8c03\u8282\u8f93\u5165\u7684\u7ebf\u6027\u6295\u5f71\uff0c\u4ece\u800c\u4f7f\u7f51\u7edc\u80fd\u591f\u5728\u6574\u4e2a\u6269\u6563\u8fc7\u7a0b\u4e2d\u52a8\u6001\u5730\u8c03\u6574\u5176\u884c\u4e3a\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6269\u6563\u6a21\u578b\u5728\u57fa\u4e8e\u56fe\u50cf\u7684\u4e8b\u6545\u68c0\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe\u523097.32%\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u6269\u6563\u6a21\u578b\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u4e8b\u6545\u68c0\u6d4b\u65b9\u9762\u5177\u6709\u663e\u8457\u7684\u6539\u8fdb\u3002"}}
{"id": "2510.03289", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03289", "abs": "https://arxiv.org/abs/2510.03289", "authors": ["Haocheng Sun", "Cynthia Xin Wen", "Edward Hong Wang"], "title": "Why mask diffusion does not work", "comment": null, "summary": "The main advantages of diffusion language models over autoregressive (AR)\nmodels lie in their ability to support parallel generation and bidirectional\nattention, enabling a more controllable generation process. In recent years,\nopen-source mask diffusion language models have emerged, most of which are\nbased on a variant known as absorbing diffusion. However, this paper\ndemonstrates why mask diffusion faces inherent difficulties in achieving\nparallel generation and bidirectional attention. We also propose the most\neffective training and inference strategies for mask diffusion.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u6307\u51fa\u4e86\u5176\u5728\u5e76\u884c\u751f\u6210\u548c\u53cc\u5411\u6ce8\u610f\u529b\u65b9\u9762\u7684\u56fa\u6709\u56f0\u96be\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u5e76\u884c\u751f\u6210\u548c\u53cc\u5411\u6ce8\u610f\u529b\u65b9\u9762\u4f18\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u53ef\u63a7\u7684\u751f\u6210\u8fc7\u7a0b\u3002\u63a2\u8ba8\u4e86\u5f53\u524d\u5f00\u6e90\u7684\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u5206\u6790\u4e86\u63a9\u7801\u6269\u6563\u5728\u5b9e\u73b0\u5e76\u884c\u751f\u6210\u548c\u53cc\u5411\u6ce8\u610f\u529b\u65f6\u9762\u4e34\u7684\u56f0\u96be\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u7b56\u7565\u3002", "result": "\u8bba\u8bc1\u4e86\u63a9\u7801\u6269\u6563\u5728\u5e76\u884c\u751f\u6210\u548c\u53cc\u5411\u6ce8\u610f\u529b\u65b9\u9762\u5b58\u5728\u56fa\u6709\u56f0\u96be\u3002", "conclusion": "\u4e3a\u63a9\u7801\u6269\u6563\u63d0\u51fa\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u7b56\u7565\u3002"}}
{"id": "2510.04140", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04140", "abs": "https://arxiv.org/abs/2510.04140", "authors": ["Zishang Jiang", "Jinyi Han", "Tingyun Li", "Xinyi Wang", "Sihang Jiang", "Jiaqing Liang", "Zhaoqian Dai", "Shuguang Ma", "Fei Yu", "Yanghua Xiao"], "title": "Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely\nadopted technique for enhancing the reasoning ability of Large Language Models\n(LLMs). However, the effectiveness of RLVR strongly depends on the capability\nof base models. This issue arises because it requires the model to have\nsufficient capability to perform high-quality exploration, which involves both\neffectiveness and diversity. Unfortunately, existing methods address this issue\nby imitating expert trajectories, which improve effectiveness but neglect\ndiversity. To address this, we argue that the expert only needs to provide\nguidance only at critical decision points rather than the entire reasoning\npath. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation\nfor Token-level Optimization of Reasoning, a framework that provides expert\nguidance only at critical decision points to perform effective and diverse\nexploration in RLVR. Extensive experiments show that MENTOR enables models\ncapture the essence of expert strategies rather than surface imitation, thereby\nperforming high-quality exploration and achieving superior overall performance.\nOur code is available online.", "AI": {"tldr": "\u63d0\u51faMENTOR\uff0c\u4e00\u79cd\u6df7\u5408\u7b56\u7565\u4e13\u5bb6\u5bfc\u822a\u6846\u67b6\uff0c\u7528\u4e8e\u5728RLVR\u4e2d\u8fdb\u884c\u6709\u6548\u548c\u591a\u6837\u5316\u7684\u63a2\u7d22\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u80fd\u529b\uff0c\u4e14\u901a\u8fc7\u6a21\u4eff\u4e13\u5bb6\u8f68\u8ff9\u6765\u63d0\u9ad8\u6548\u679c\uff0c\u5ffd\u7565\u4e86\u591a\u6837\u6027\u3002", "method": "\u4ec5\u5728\u5173\u952e\u51b3\u7b56\u70b9\u63d0\u4f9b\u4e13\u5bb6\u6307\u5bfc\uff0c\u4ee5\u8fdb\u884ctoken\u7ea7\u522b\u7684\u63a8\u7406\u4f18\u5316\u3002", "result": "MENTOR\u4f7f\u6a21\u578b\u80fd\u591f\u6355\u6349\u4e13\u5bb6\u7b56\u7565\u7684\u672c\u8d28\uff0c\u4ece\u800c\u6267\u884c\u9ad8\u8d28\u91cf\u7684\u63a2\u7d22\u5e76\u5b9e\u73b0\u5353\u8d8a\u7684\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "MENTOR\u901a\u8fc7\u5728\u5173\u952e\u51b3\u7b56\u70b9\u63d0\u4f9b\u4e13\u5bb6\u6307\u5bfc\uff0c\u5b9e\u73b0\u4e86RLVR\u4e2d\u6709\u6548\u548c\u591a\u6837\u5316\u7684\u63a2\u7d22\u3002"}}
{"id": "2510.04045", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04045", "abs": "https://arxiv.org/abs/2510.04045", "authors": ["Yunfan Zhang", "Kathleen McKeown", "Smaranda Muresan"], "title": "Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment", "comment": "ACL EMNLP 2025", "summary": "Large Language Models (LLMs) are typically trained to reflect a relatively\nuniform set of values, which limits their applicability to tasks that require\nunderstanding of nuanced human perspectives. Recent research has underscored\nthe importance of enabling LLMs to support steerable pluralism -- the capacity\nto adopt a specific perspective and align generated outputs with it. In this\nwork, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be\napplied to building steerable pluralistic models. We explore several methods,\nincluding CoT prompting, fine-tuning on human-authored CoT, fine-tuning on\nsynthetic explanations, and Reinforcement Learning with Verifiable Rewards\n(RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA\ndatasets. Among the methods studied, RLVR consistently outperforms others and\ndemonstrates strong training sample efficiency. We further analyze the\ngenerated CoT traces with respect to faithfulness and safety.", "AI": {"tldr": "\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u7ec6\u5fae\u4eba\u7c7b\u89c2\u70b9\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u7814\u7a76\u4e86\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u6280\u672f\u5728\u6784\u5efa\u53ef\u63a7\u591a\u5143\u5316\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u88ab\u8bad\u7ec3\u6210\u53cd\u6620\u4e00\u5957\u76f8\u5bf9\u7edf\u4e00\u7684\u4ef7\u503c\u89c2\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u9700\u8981\u7406\u89e3\u7ec6\u5fae\u4eba\u7c7b\u89c2\u70b9\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u3002\u6700\u8fd1\u7684\u7814\u7a76\u5f3a\u8c03\u4e86\u4f7fLLM\u80fd\u591f\u652f\u6301\u53ef\u64cd\u7eb5\u7684\u591a\u5143\u5316\u7684\u91cd\u8981\u6027\u2014\u2014\u5373\u91c7\u7eb3\u7279\u5b9a\u89c2\u70b9\u5e76\u4f7f\u751f\u6210\u7684\u8f93\u51fa\u4e0e\u4e4b\u5bf9\u9f50\u7684\u80fd\u529b\u3002", "method": "\u7814\u7a76\u4e86CoT\u63d0\u793a\u3001\u5728\u4eba\u5de5\u7f16\u5199\u7684CoT\u4e0a\u8fdb\u884c\u5fae\u8c03\u3001\u5728\u5408\u6210\u89e3\u91ca\u4e0a\u8fdb\u884c\u5fae\u8c03\u4ee5\u53ca\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u7b49\u65b9\u6cd5\u3002", "result": "\u5728\u6240\u7814\u7a76\u7684\u65b9\u6cd5\u4e2d\uff0cRLVR\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5e76\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8bad\u7ec3\u6837\u672c\u6548\u7387\u3002", "conclusion": "\u5206\u6790\u4e86\u751f\u6210\u7684CoT traces\u5728\u5fe0\u5b9e\u6027\u548c\u5b89\u5168\u6027\u65b9\u9762\u7684\u60c5\u51b5\uff0c\u8868\u660eRLVR\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u6027\u3002"}}
{"id": "2510.03689", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03689", "abs": "https://arxiv.org/abs/2510.03689", "authors": ["Zhengyi Liu", "Xinrui Wang", "Xianyong Fang", "Zhengzheng Tu", "Linbo Wang"], "title": "SAMSOD: Rethinking SAM Optimization for RGB-T Salient Object Detection", "comment": "Accepted by TMM", "summary": "RGB-T salient object detection (SOD) aims to segment attractive objects by\ncombining RGB and thermal infrared images. To enhance performance, the Segment\nAnything Model has been fine-tuned for this task. However, the imbalance\nconvergence of two modalities and significant gradient difference between high-\nand low- activations are ignored, thereby leaving room for further performance\nenhancement. In this paper, we propose a model called \\textit{SAMSOD}, which\nutilizes unimodal supervision to enhance the learning of non-dominant modality\nand employs gradient deconfliction to reduce the impact of conflicting\ngradients on model convergence. The method also leverages two decoupled\nadapters to separately mask high- and low-activation neurons, emphasizing\nforeground objects by enhancing background learning. Fundamental experiments on\nRGB-T SOD benchmark datasets and generalizability experiments on scribble\nsupervised RGB-T SOD, fully supervised RGB-D SOD datasets and full-supervised\nRGB-D rail surface defect detection all demonstrate the effectiveness of our\nproposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAMSOD\u7684\u6a21\u578b\uff0c\u7528\u4e8eRGB-T\u663e\u8457\u76ee\u6807\u68c0\u6d4b\uff0c\u901a\u8fc7\u5229\u7528\u5355\u6a21\u6001\u76d1\u7763\u3001\u68af\u5ea6\u89e3\u51b2\u7a81\u548c\u89e3\u8026\u9002\u914d\u5668\u6765\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RGB-T\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5ffd\u7565\u4e86\u4e24\u79cd\u6a21\u6001\u7684\u4e0d\u5e73\u8861\u6536\u655b\u4ee5\u53ca\u9ad8\u4f4e\u6fc0\u6d3b\u4e4b\u95f4\u7684\u663e\u8457\u68af\u5ea6\u5dee\u5f02\uff0c\u5bfc\u81f4\u6027\u80fd\u63d0\u5347\u7a7a\u95f4\u53d7\u9650\u3002", "method": "\u8be5\u6a21\u578b\u5229\u7528\u5355\u6a21\u6001\u76d1\u7763\u6765\u589e\u5f3a\u975e\u4e3b\u5bfc\u6a21\u6001\u7684\u5b66\u4e60\uff0c\u91c7\u7528\u68af\u5ea6\u89e3\u51b2\u7a81\u6765\u51cf\u5c11\u51b2\u7a81\u68af\u5ea6\u5bf9\u6a21\u578b\u6536\u655b\u7684\u5f71\u54cd\uff0c\u5e76\u5229\u7528\u4e24\u4e2a\u89e3\u8026\u9002\u914d\u5668\u5206\u522b\u5c4f\u853d\u9ad8\u4f4e\u6fc0\u6d3b\u795e\u7ecf\u5143\uff0c\u901a\u8fc7\u589e\u5f3a\u80cc\u666f\u5b66\u4e60\u6765\u5f3a\u8c03\u524d\u666f\u5bf9\u8c61\u3002", "result": "\u5728RGB-T SOD\u57fa\u51c6\u6570\u636e\u96c6\u548c\u6cdb\u5316\u6027\u5b9e\u9a8c\u4e2d\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728RGB-T\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u5176\u4ed6\u76f8\u5173\u4efb\u52a1\u4e2d\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.03290", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03290", "abs": "https://arxiv.org/abs/2510.03290", "authors": ["X. Angelo Huang", "Ruben Ciranni", "Giovanni Spadaccini", "Carla J. L\u00f3pez Zurita"], "title": "Single-Core Superscalar Optimization of Clifford Neural Layers", "comment": "9 pages", "summary": "Within the growing interest in the physical sciences in developing networks\nwith equivariance properties, Clifford neural layers shine as one approach that\ndelivers $E(n)$ and $O(n)$ equivariances given specific group actions. In this\npaper, we analyze the inner structure of the computation within Clifford\nconvolutional layers and propose and implement several optimizations to speed\nup the inference process while maintaining correctness. In particular, we begin\nby analyzing the theoretical foundations of Clifford algebras to eliminate\nredundant matrix allocations and computations, then systematically apply\nestablished optimization techniques to enhance performance further. We report a\nfinal average speedup of 21.35x over the baseline implementation of eleven\nfunctions and runtimes comparable to and faster than the original PyTorch\nimplementation in six cases. In the remaining cases, we achieve performance in\nthe same order of magnitude as the original library.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u52a0\u901fClifford\u5377\u79ef\u5c42\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u6b63\u786e\u6027\u3002", "motivation": "\u5728\u7269\u7406\u79d1\u5b66\u9886\u57df\uff0c\u4eba\u4eec\u5bf9\u5f00\u53d1\u5177\u6709\u7b49\u53d8\u6027\u8d28\u7684\u7f51\u7edc\u8d8a\u6765\u8d8a\u611f\u5174\u8da3\uff0cClifford\u795e\u7ecf\u5c42\u4f5c\u4e3a\u4e00\u79cd\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u7ed9\u5b9a\u7279\u5b9a\u7fa4\u4f5c\u7528\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u4f9bE(n)\u548cO(n)\u7b49\u53d8\u6027\u3002", "method": "\u8be5\u8bba\u6587\u5206\u6790\u4e86Clifford\u5377\u79ef\u5c42\u5185\u90e8\u8ba1\u7b97\u7684\u7406\u8bba\u57fa\u7840\uff0c\u4ee5\u6d88\u9664\u5197\u4f59\u7684\u77e9\u9635\u5206\u914d\u548c\u8ba1\u7b97\uff0c\u7136\u540e\u7cfb\u7edf\u5730\u5e94\u7528\u5df2\u5efa\u7acb\u7684\u4f18\u5316\u6280\u672f\u6765\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002", "result": "\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u5b9e\u73b0\uff0c\u6700\u7ec8\u5e73\u5747\u52a0\u901f\u4e8621.35\u500d\u3002\u5728\u516d\u79cd\u60c5\u51b5\u4e0b\uff0c\u8fd0\u884c\u65f6\u95f4\u4e0e\u539f\u59cbPyTorch\u5b9e\u73b0\u76f8\u5f53\u751a\u81f3\u66f4\u5feb\u3002\u5728\u5176\u4f59\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u4e0e\u539f\u59cb\u5e93\u76f8\u540c\u6570\u91cf\u7ea7\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5bf9Clifford\u4ee3\u6570\u7684\u7406\u8bba\u57fa\u7840\u8fdb\u884c\u5206\u6790\uff0c\u5e76\u5e94\u7528\u4f18\u5316\u6280\u672f\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8Clifford\u5377\u79ef\u5c42\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u4e14\u6027\u80fd\u4e0e\u539f\u59cbPyTorch\u5b9e\u73b0\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u3002"}}
{"id": "2510.04141", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04141", "abs": "https://arxiv.org/abs/2510.04141", "authors": ["Mayank Ravishankara", "Varindra V. Persad Maharaj"], "title": "The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning", "comment": null, "summary": "This survey paper chronicles the evolution of evaluation in multimodal\nartificial intelligence (AI), framing it as a progression of increasingly\nsophisticated \"cognitive examinations.\" We argue that the field is undergoing a\nparadigm shift, moving from simple recognition tasks that test \"what\" a model\nsees, to complex reasoning benchmarks that probe \"why\" and \"how\" it\nunderstands. This evolution is driven by the saturation of older benchmarks,\nwhere high performance often masks fundamental weaknesses. We chart the journey\nfrom the foundational \"knowledge tests\" of the ImageNet era to the \"applied\nlogic and comprehension\" exams such as GQA and Visual Commonsense Reasoning\n(VCR), which were designed specifically to diagnose systemic flaws such as\nshortcut learning and failures in compositional generalization. We then survey\nthe current frontier of \"expert-level integration\" benchmarks (e.g., MMBench,\nSEED-Bench, MMMU) designed for today's powerful multimodal large language\nmodels (MLLMs), which increasingly evaluate the reasoning process itself.\nFinally, we explore the uncharted territories of evaluating abstract, creative,\nand social intelligence. We conclude that the narrative of AI evaluation is not\nmerely a history of datasets, but a continuous, adversarial process of\ndesigning better examinations that, in turn, redefine our goals for creating\ntruly intelligent systems.", "AI": {"tldr": "\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u8bc4\u4f30\u5df2\u7ecf\u4ece\u7b80\u5355\u7684\u8bc6\u522b\u4efb\u52a1\u53d1\u5c55\u5230\u590d\u6742\u7684\u63a8\u7406\u57fa\u51c6\u3002", "motivation": "\u65e7\u7684\u57fa\u51c6\u5df2\u7ecf\u9971\u548c\uff0c\u9ad8\u6027\u80fd\u901a\u5e38\u63a9\u76d6\u4e86\u6839\u672c\u7684\u5f31\u70b9\u3002\u6211\u4eec\u9700\u8981\u91cd\u65b0\u5b9a\u4e49\u521b\u5efa\u771f\u6b63\u667a\u80fd\u7cfb\u7edf\u7684\u76ee\u6807\u3002", "method": "\u672c\u6587\u56de\u987e\u4e86\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u8bc4\u4f30\u7684\u6f14\u53d8\uff0c\u5c06\u5176\u89c6\u4e3a\u8d8a\u6765\u8d8a\u590d\u6742\u7684\u201c\u8ba4\u77e5\u68c0\u67e5\u201d\u7684\u6f14\u53d8\u3002", "result": "\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u8bc4\u4f30\u5df2\u7ecf\u4eceImageNet\u65f6\u4ee3\u7684\u201c\u77e5\u8bc6\u6d4b\u8bd5\u201d\u53d1\u5c55\u5230GQA\u548cVCR\u7b49\u201c\u5e94\u7528\u903b\u8f91\u548c\u7406\u89e3\u201d\u6d4b\u8bd5\uff0c\u518d\u5230MMBench\u3001SEED-Bench\u548cMMMU\u7b49\u201c\u4e13\u5bb6\u7ea7\u6574\u5408\u201d\u57fa\u51c6\u3002", "conclusion": "\u4eba\u5de5\u667a\u80fd\u8bc4\u4f30\u4e0d\u4ec5\u4ec5\u662f\u6570\u636e\u96c6\u7684\u5386\u53f2\uff0c\u800c\u662f\u4e00\u4e2a\u6301\u7eed\u7684\u3001\u5bf9\u6297\u6027\u7684\u8fc7\u7a0b\uff0c\u65e8\u5728\u8bbe\u8ba1\u66f4\u597d\u7684\u68c0\u67e5\uff0c\u4ece\u800c\u91cd\u65b0\u5b9a\u4e49\u6211\u4eec\u521b\u5efa\u771f\u6b63\u667a\u80fd\u7cfb\u7edf\u7684\u76ee\u6807\u3002"}}
