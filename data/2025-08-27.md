<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 42]
- [cs.CV](#cs.CV) [Total: 40]
- [cs.AI](#cs.AI) [Total: 41]
- [cs.DB](#cs.DB) [Total: 8]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 41]
- [eess.IV](#eess.IV) [Total: 4]
- [eess.SP](#eess.SP) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI](https://arxiv.org/abs/2508.18290)
*Hans-Joachim Rudolph*

Main category: cs.CL

TL;DR: develops a theoretical framework for a semantic AGI based on the notion of semantic attractors in complex-valued meaning spaces


<details>
  <summary>Details</summary>
Motivation: explore a model in which meaning is not inferred probabilistically but formed through recursive tensorial transformation, departing from current transformer-based language models

Method: recursive tensorial transformation using cyclic operations involving the imaginary unit i

Result: a rotational semantic structure capable of modeling irony, homonymy, and ambiguity with a semantic attractor

Conclusion: true meaning emerges not from simulation, but from recursive convergence toward semantic coherence

Abstract: This essay develops a theoretical framework for a semantic Artificial General
Intelligence (AGI) based on the notion of semantic attractors in complex-valued
meaning spaces. Departing from current transformer-based language models, which
operate on statistical next-token prediction, we explore a model in which
meaning is not inferred probabilistically but formed through recursive
tensorial transformation. Using cyclic operations involving the imaginary unit
\emph{i}, we describe a rotational semantic structure capable of modeling
irony, homonymy, and ambiguity. At the center of this model, however, is a
semantic attractor -- a teleological operator that, unlike statistical
computation, acts as an intentional agent (Microvitum), guiding meaning toward
stability, clarity, and expressive depth. Conceived in terms of gradient flows,
tensor deformations, and iterative matrix dynamics, the attractor offers a
model of semantic transformation that is not only mathematically suggestive,
but also philosophically significant. We argue that true meaning emerges not
from simulation, but from recursive convergence toward semantic coherence, and
that this requires a fundamentally new kind of cognitive architecture -- one
designed to shape language, not just predict it.

</details>


### [2] [LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions](https://arxiv.org/abs/2508.18321)
*Maojia Song,Tej Deep Pala,Weisheng Jin,Amir Zadeh,Chuan Li,Dorien Herremans,Soujanya Poria*

Main category: cs.CL

TL;DR: This paper analyzes how LLMs form trust, resist misinformation and integrate peer input in multi-agent systems. It introduces KAIROS, a benchmark for simulating quiz contests with peer agents. The results show that GRPO achieves the best performance but decreases robustness to social influence.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly deployed in multi-agent systems as components of collaborative intelligence, where peer interactions dynamically shape individual decision-making. Although prior work has focused on conformity bias, we extend the analysis to examine how LLMs form trust from previous impressions, resist misinformation, and integrate peer input during interaction, key factors for achieving collective intelligence under complex social dynamics.

Method: a benchmark simulating quiz contests with peer agents of varying reliability, offering fine-grained control over conditions such as expert-novice roles, noisy crowds, and adversarial peers. LLMs receive both historical interactions and current peer responses, allowing systematic investigation into how trust, peer action, and self-confidence influence decisions. As for mitigation strategies, we evaluate prompting, supervised fine-tuning, and reinforcement learning, Group Relative Policy Optimisation (GRPO), across multiple models.

Result: GRPO with multi-agent context combined with outcome-based rewards and unconstrained reasoning achieves the best overall performance, but also decreases the robustness to social influence compared to Base models.

Conclusion: GRPO with multi-agent context combined with outcome-based rewards and unconstrained reasoning achieves the best overall performance, but also decreases the robustness to social influence compared to Base models.

Abstract: Large language models (LLMs) are increasingly deployed in multi-agent systems
(MAS) as components of collaborative intelligence, where peer interactions
dynamically shape individual decision-making. Although prior work has focused
on conformity bias, we extend the analysis to examine how LLMs form trust from
previous impressions, resist misinformation, and integrate peer input during
interaction, key factors for achieving collective intelligence under complex
social dynamics. We present KAIROS, a benchmark simulating quiz contests with
peer agents of varying reliability, offering fine-grained control over
conditions such as expert-novice roles, noisy crowds, and adversarial peers.
LLMs receive both historical interactions and current peer responses, allowing
systematic investigation into how trust, peer action, and self-confidence
influence decisions. As for mitigation strategies, we evaluate prompting,
supervised fine-tuning, and reinforcement learning, Group Relative Policy
Optimisation (GRPO), across multiple models. Our results reveal that GRPO with
multi-agent context combined with outcome-based rewards and unconstrained
reasoning achieves the best overall performance, but also decreases the
robustness to social influence compared to Base models. The code and datasets
are available at: https://github.com/declare-lab/KAIROS.

</details>


### [3] [Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective](https://arxiv.org/abs/2508.18328)
*Masudul Hasan Masud Bhuiyan,Matteo Varvello,Yasir Zaki,Cristian-Alexandru Staicu*

Main category: cs.CL

TL;DR: This paper introduces a new dataset (LangCrUX) for multilingual web accessibility, reveals issues with current accessibility hints, and proposes a language-aware testing extension (Kizuki).


<details>
  <summary>Details</summary>
Motivation: Multilingualism on the web introduces significant barriers for users with visual impairments, as assistive technologies frequently lack robust support for non-Latin scripts. Large-scale studies of this issue have been limited by the lack of comprehensive datasets on multilingual web content.

Method: The paper introduces LangCrUX, a large-scale dataset of 120,000 popular websites across 12 languages that primarily use non-Latin scripts, and conducts a systematic analysis of multilingual web accessibility.

Result: The study uncovers widespread neglect of accessibility hints and finds that these hints often fail to reflect the language diversity of visible content, reducing the effectiveness of screen readers and limiting web accessibility.

Conclusion: The paper proposes Kizuki, a language-aware automated accessibility testing extension to account for the limited utility of language-inconsistent accessibility hints.

Abstract: English is the predominant language on the web, powering nearly half of the
world's top ten million websites. Support for multilingual content is
nevertheless growing, with many websites increasingly combining English with
regional or native languages in both visible content and hidden metadata. This
multilingualism introduces significant barriers for users with visual
impairments, as assistive technologies like screen readers frequently lack
robust support for non-Latin scripts and misrender or mispronounce non-English
text, compounding accessibility challenges across diverse linguistic contexts.
Yet, large-scale studies of this issue have been limited by the lack of
comprehensive datasets on multilingual web content. To address this gap, we
introduce LangCrUX, the first large-scale dataset of 120,000 popular websites
across 12 languages that primarily use non-Latin scripts. Leveraging this
dataset, we conduct a systematic analysis of multilingual web accessibility and
uncover widespread neglect of accessibility hints. We find that these hints
often fail to reflect the language diversity of visible content, reducing the
effectiveness of screen readers and limiting web accessibility. We finally
propose Kizuki, a language-aware automated accessibility testing extension to
account for the limited utility of language-inconsistent accessibility hints.

</details>


### [4] [Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models](https://arxiv.org/abs/2508.18381)
*Yuchun Fan,Yilin Wang,Yongyu Mu,Lei Huang,Bei Li,Xiaocheng Feng,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: PLAST通过精确的特定语言层微调，有效提升了LVLM的多语言能力，且效率很高。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型(LVLM)在用人类语言理解视觉信息方面表现出了卓越的能力，但也表现出多语言能力的不平衡。

Method: 通过监测特定于语言的神经元激活来识别参与多语言理解的层，然后使用问题-翻译对精确地微调这些层，以实现多语言对齐。

Result: 在MM-Bench和MMMB上的实验结果表明，PLAST有效地提高了LVLM的多语言能力，并且仅调整了14%的参数，实现了显著的效率提升。

Conclusion: PLAST有效地提高了LVLM的多语言能力，并且仅调整了14%的参数，实现了显著的效率提升。PLAST可以推广到低资源和复杂的视觉推理任务，从而促进浅层中特定于语言的视觉信息参与。

Abstract: Large vision-language models (LVLMs) have demonstrated exceptional
capabilities in understanding visual information with human languages but also
exhibit an imbalance in multilingual capabilities. In this work, we delve into
the multilingual working pattern of LVLMs and identify a salient correlation
between the multilingual understanding ability of LVLMs and language-specific
neuron activations in shallow layers. Building on this insight, we introduce
PLAST, a training recipe that achieves efficient multilingual enhancement for
LVLMs by Precise LAnguage-Specific layers fine-Tuning. PLAST first identifies
layers involved in multilingual understanding by monitoring language-specific
neuron activations. These layers are then precisely fine-tuned with
question-translation pairs to achieve multilingual alignment. Our empirical
results on MM-Bench and MMMB demonstrate that PLAST effectively improves the
multilingual capabilities of LVLMs and achieves significant efficiency with
only 14% of the parameters tuned. Further analysis reveals that PLAST can be
generalized to low-resource and complex visual reasoning tasks, facilitating
the language-specific visual information engagement in shallow layers.

</details>


### [5] [Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails](https://arxiv.org/abs/2508.18384)
*Kellen Tan Cheng,Anna Lisa Gentile,Chad DeLuca,Guang-Jie Ren*

Main category: cs.CL

TL;DR: 该论文提出了一种名为“反向提示”的方法，用于生成用于训练健康建议检测器的数据。该检测器优于 GPT-4o，参数减少了 400 倍。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 在企业环境中的普遍存在也带来了与其使用相关的巨大风险。Guardrails 技术旨在通过各种检测器过滤 LLM 的输入/输出文本来降低这种风险。然而，开发和维护强大的检测器面临许多挑战，其中之一是在部署之前难以获取关于真实 LLM 输出的生产质量标签数据。

Method: 我们提出了反向提示，这是一种简单直观的解决方案，可以生成类似生产环境的标签数据，用于健康建议护栏开发。此外，我们将反向提示方法与稀疏的人工循环聚类技术相结合，以标记生成的数据。

Result: 我们的目标是构建一个大致代表原始数据集但又类似于真实 LLM 输出的并行语料库。然后，我们将现有数据集与我们的合成示例融合，从而为我们的检测器生成强大的训练数据。我们在最困难和细致的护栏之一中测试了我们的技术：识别 LLM 输出中的健康建议，并证明与其它解决方案相比有所改进。

Conclusion: 该检测器优于 GPT-4o，性能提升高达 3.73%，但参数减少了 400 倍。

Abstract: The pervasiveness of large language models (LLMs) in enterprise settings has
also brought forth a significant amount of risks associated with their usage.
Guardrails technologies aim to mitigate this risk by filtering LLMs'
input/output text through various detectors. However, developing and
maintaining robust detectors faces many challenges, one of which is the
difficulty in acquiring production-quality labeled data on real LLM outputs
prior to deployment. In this work, we propose backprompting, a simple yet
intuitive solution to generate production-like labeled data for health advice
guardrails development. Furthermore, we pair our backprompting method with a
sparse human-in-the-loop clustering technique to label the generated data. Our
aim is to construct a parallel corpus roughly representative of the original
dataset yet resembling real LLM output. We then infuse existing datasets with
our synthetic examples to produce robust training data for our detector. We
test our technique in one of the most difficult and nuanced guardrails: the
identification of health advice in LLM output, and demonstrate improvement
versus other solutions. Our detector is able to outperform GPT-4o by up to
3.73%, despite having 400x less parameters.

</details>


### [6] [Integral Transformer: Denoising Attention, Not Too Much Not Too Little](https://arxiv.org/abs/2508.18387)
*Ivan Kobyzev,Abbas Ghaddar,Dingtao Hu,Boxing Chen*

Main category: cs.CL

TL;DR: The Integral Transformer denoises attention by integrating signals from the logit distribution, outperforming other attention mechanisms on knowledge and reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Softmax self-attention often assigns disproportionate weight to semantically uninformative tokens, known as attention noise. Existing methods risk discarding useful information.

Method: The Integral Transformer, a novel self-attention mechanism that denoises attention by integrating signals sampled from the logit distribution.

Result: The Integral Transformer mitigates noise while preserving the contributions of special tokens. 

Conclusion: The Integral Transformer outperforms vanilla, Cog, and Differential attention variants on knowledge and reasoning language benchmarks. Vanilla self-attention in lower layers improves performance. The Integral Transformer balances attention distributions and reduces rank collapse in upper layers.

Abstract: Softmax self-attention often assigns disproportionate weight to semantically
uninformative tokens such as special tokens and punctuation, a phenomenon known
as attention noise. While recent methods like Cog Attention and the
Differential Transformer have addressed this by introducing negative attention
scores, they risk discarding useful information. In this paper, we propose the
Integral Transformer, a novel self-attention mechanism that denoises attention
by integrating signals sampled from the logit distribution. Our approach
mitigates noise while preserving the contributions of special tokens critical
for model performance. Extensive experiments demonstrate that our model
outperforms vanilla, Cog, and Differential attention variants on
well-established knowledge and reasoning language benchmarks. Moreover, our
analysis reveals that employing vanilla self-attention in the lower Transformer
layers enhances performance and that the Integral Transformer effectively
balances attention distributions and reduces rank collapse in upper layers.

</details>


### [7] [Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning](https://arxiv.org/abs/2508.18395)
*Jeong-seok Oh,Jay-yoon Lee*

Main category: cs.CL

TL;DR: LSC 是一种新的 self-consistency 方法，它在短篇和长篇问题上都优于现有方法，并且计算开销很小。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)中的概率解码通常会产生不一致的输出，尤其是在复杂或长篇问题上。Self-Consistency (SC) 通过对精确字符串进行多数投票来缓解短篇QA的这个问题，而Universal Self-Consistency (USC)和Weighted Unigram Consistency Score (WUCS) 扩展到长篇回复，但在短篇基准测试中会降低准确性。

Method: 使用可学习的token embeddings选择语义上最一致的响应。通过轻量级的前向生成summary tokens，推理时间增加不到1%，并且不需要更改模型架构。

Result: 在6个短篇和5个长篇推理基准测试中，LSC在所有短篇和长篇测试中平均超过SC、USC和WUCS，同时保持可忽略的计算开销。

Conclusion: Latent Self-Consistency (LSC)在各种问题形式上都优于其他self-consistency方法，并且计算开销可忽略不计，同时提供了良好的置信度估计。

Abstract: Probabilistic decoding in Large Language Models (LLMs) often yields
inconsistent outputs, particularly on complex or long-form questions.
Self-Consistency (SC) mitigates this for short-form QA by majority voting over
exact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram
Consistency Score (WUCS) extend to long-form responses but lose accuracy on
short-form benchmarks.
  We introduce Latent Self-Consistency (LSC), which selects the most
semantically consistent response using learnable token embeddings. A
lightweight forward generation of summary tokens increases inference time by
less than 1% and requires no changes to the model architecture.
  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU,
TruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form
ones on average, while maintaining negligible computational overhead. These
results position LSC as a practical consistency-selection method that works
reliably across answer formats. Additionally, LSC provides well-calibrated
confidence estimates, maintaining low Expected Calibration Error across both
answer formats.

</details>


### [8] [Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering](https://arxiv.org/abs/2508.18407)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Michal Spiegel,Josef Kuchař*

Main category: cs.CL

TL;DR: 质疑了 OOD 评估在评估 QA 模型泛化能力方面的有效性，发现其质量参差不齐，并提出了更稳健的评估方法。


<details>
  <summary>Details</summary>
Motivation: 大多数最近的 AI 工作通过对分布外 (OOD) 数据集的性能来评估模型的泛化能力。尽管它们具有实用性，但此类评估建立在一个强大的假设之上：OOD 评估可以捕获并反映现实世界部署中可能出现的故障。

Method: 将 OOD 评估的结果与现有问答 (QA) 模型中记录的一组特定故障模式进行对比，这些故障模式被称为依赖于虚假特征或预测捷径。

Result: 用于 QA 中 OOD 评估的不同数据集提供了对模型对捷径的鲁棒性的估计，这些估计的质量差异很大，有些甚至在很大程度上低于简单的分布内评估。我们部分地将此归因于虚假捷径在 ID+OOD 数据集中共享的观察结果，但也发现数据集的训练和评估质量在很大程度上脱节的情况。

Conclusion: 常用的基于 OOD 的泛化评估具有局限性，并为更稳健地评估 QA 内外的泛化提供了方法和建议。

Abstract: A majority of recent work in AI assesses models' generalization capabilities
through the lens of performance on out-of-distribution (OOD) datasets. Despite
their practicality, such evaluations build upon a strong assumption: that OOD
evaluations can capture and reflect upon possible failures in a real-world
deployment.
  In this work, we challenge this assumption and confront the results obtained
from OOD evaluations with a set of specific failure modes documented in
existing question-answering (QA) models, referred to as a reliance on spurious
features or prediction shortcuts.
  We find that different datasets used for OOD evaluations in QA provide an
estimate of models' robustness to shortcuts that have a vastly different
quality, some largely under-performing even a simple, in-distribution
evaluation. We partially attribute this to the observation that spurious
shortcuts are shared across ID+OOD datasets, but also find cases where a
dataset's quality for training and evaluation is largely disconnected. Our work
underlines limitations of commonly-used OOD-based evaluations of
generalization, and provides methodology and recommendations for evaluating
generalization within and beyond QA more robustly.

</details>


### [9] [How Reliable are LLMs for Reasoning on the Re-ranking task?](https://arxiv.org/abs/2508.18444)
*Nafis Tanveer Islam,Zhiming Zhao*

Main category: cs.CL

TL;DR: This paper analyzes how different training methods affect the semantic understanding of the re-ranking task in LLMs. The paper utilizes a relatively small ranking dataset from the environment and the Earth science domain to re-rank retrieved content. The results show that some training methods exhibit better explainability than others.


<details>
  <summary>Details</summary>
Motivation: LLMs exhibit a greater awareness and alignment with human values, but this comes at the cost of transparency. Accurately re-ranking content remains a significant challenge in newly developed systems with limited user engagement and insufficient ranking data.

Method: Utilize a relatively small ranking dataset from the environment and the Earth science domain to re-rank retrieved content and analyze the explainable information.

Result: Some training methods exhibit better explainability than others, implying that an accurate semantic understanding has not been learned through all training methods.

Conclusion: Different training methods affect the semantic understanding of the re-ranking task in LLMs. Some training methods exhibit better explainability than others. Abstract knowledge has been gained to optimize evaluation, which raises questions about the true reliability of LLMs.

Abstract: With the improving semantic understanding capability of Large Language Models
(LLMs), they exhibit a greater awareness and alignment with human values, but
this comes at the cost of transparency. Although promising results are achieved
via experimental analysis, an in-depth understanding of the LLM's internal
workings is unavoidable to comprehend the reasoning behind the re-ranking,
which provides end users with an explanation that enables them to make an
informed decision. Moreover, in newly developed systems with limited user
engagement and insufficient ranking data, accurately re-ranking content remains
a significant challenge. While various training methods affect the training of
LLMs and generate inference, our analysis has found that some training methods
exhibit better explainability than others, implying that an accurate semantic
understanding has not been learned through all training methods; instead,
abstract knowledge has been gained to optimize evaluation, which raises
questions about the true reliability of LLMs. Therefore, in this work, we
analyze how different training methods affect the semantic understanding of the
re-ranking task in LLMs and investigate whether these models can generate more
informed textual reasoning to overcome the challenges of transparency or LLMs
and limited training data. To analyze the LLMs for re-ranking tasks, we utilize
a relatively small ranking dataset from the environment and the Earth science
domain to re-rank retrieved content. Furthermore, we also analyze the
explainable information to see if the re-ranking can be reasoned using
explainability.

</details>


### [10] [Integrating gender inclusivity into large language models via instruction tuning](https://arxiv.org/abs/2508.18466)
*Alina Wróblewska,Bartosz Żuk*

Main category: cs.CL

TL;DR: This study tunes LLMs to reduce masculine bias in Polish language generation using a gender-inclusive dataset and system prompt.


<details>
  <summary>Details</summary>
Motivation: Masculine bias in Polish LLMs due to the language's grammatical gender system and historical conventions leads to gender-imbalanced outputs.

Method: Tuning multilingual LLMs (Llama-8B, Mistral-7B and Mistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM) using the IPIS dataset and a system prompt.

Result: The approach integrates gender inclusivity as an inherent feature of the tuned models.

Conclusion: This study aims to mitigate gender bias in Polish language generation by tuning LLMs with the IPIS dataset and a system prompt with gender-inclusive guidelines.

Abstract: Imagine a language with masculine, feminine, and neuter grammatical genders,
yet, due to historical and political conventions, masculine forms are
predominantly used to refer to men, women and mixed-gender groups. This is the
reality of contemporary Polish. A social consequence of this unfair linguistic
system is that large language models (LLMs) trained on Polish texts inherit and
reinforce this masculine bias, generating gender-imbalanced outputs. This study
addresses this issue by tuning LLMs using the IPIS dataset, a collection of
human-crafted gender-inclusive proofreading in Polish and Polish-to-English
translation instructions. Grounded in a theoretical linguistic framework, we
design a system prompt with explicit gender-inclusive guidelines for Polish. In
our experiments, we IPIS-tune multilingual LLMs (Llama-8B, Mistral-7B and
Mistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM). Our approach aims to
integrate gender inclusivity as an inherent feature of these models, offering a
systematic solution to mitigate gender bias in Polish language generation.

</details>


### [11] [Principled Detection of Hallucinations in Large Language Models via Multiple Testing](https://arxiv.org/abs/2508.18473)
*Jiawei Li,Akshayaa Magesh,Venugopal V. Veeravalli*

Main category: cs.CL

TL;DR: This paper addresses the problem of hallucination in large language models, viewing it as a hypothesis testing problem. A multiple-testing-inspired method is proposed and validated against state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) have emerged as powerful foundational models to solve a variety of tasks, they have also been shown to be prone to hallucinations, i.e., generating responses that sound confident but are actually incorrect or even nonsensical. In this work, we formulate the problem of detecting hallucinations as a hypothesis testing problem and draw parallels to the problem of out-of-distribution detection in machine learning models.

Method: We propose a multiple-testing-inspired method to solve the hallucination detection problem

Result: a multiple-testing-inspired method to solve the hallucination detection problem

Conclusion: We provide extensive experimental results to validate the robustness of our approach against state-of-the-art methods.

Abstract: While Large Language Models (LLMs) have emerged as powerful foundational
models to solve a variety of tasks, they have also been shown to be prone to
hallucinations, i.e., generating responses that sound confident but are
actually incorrect or even nonsensical. In this work, we formulate the problem
of detecting hallucinations as a hypothesis testing problem and draw parallels
to the problem of out-of-distribution detection in machine learning models. We
propose a multiple-testing-inspired method to solve the hallucination detection
problem, and provide extensive experimental results to validate the robustness
of our approach against state-of-the-art methods.

</details>


### [12] [COMET-poly: Machine Translation Metric Grounded in Other Candidates](https://arxiv.org/abs/2508.18549)
*Maike Züfle,Vilém Zouhar,Tu Anh Dinh,Felipe Maia Polo,Jan Niehues,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: The paper introduces two new automated metrics, COMET-polycand and COMET-polyic, that improve machine translation evaluation by incorporating additional information beyond a single translation.


<details>
  <summary>Details</summary>
Motivation: Automated metrics for machine translation typically consider only the source sentence and a single translation, which may negatively impact the performance of automated metrics.

Method: Two automated metrics: COMET-polycand and COMET-polyic.

Result: Including a single additional translation in COMET-polycand improves the segment-level metric performance (0.079 to 0.118 Kendall's tau-b correlation), with further gains when more translations are added. Incorporating retrieved examples in COMET-polyic yields similar improvements (0.079 to 0.116 Kendall's tau-b correlation).

Conclusion: Including additional translation or retrieved examples improves the segment-level metric performance.

Abstract: Automated metrics for machine translation attempt to replicate human
judgment. Unlike humans, who often assess a translation in the context of
multiple alternatives, these metrics typically consider only the source
sentence and a single translation. This discrepancy in the evaluation setup may
negatively impact the performance of automated metrics. We propose two
automated metrics that incorporate additional information beyond the single
translation. COMET-polycand uses alternative translations of the same source
sentence to compare and contrast with the translation at hand, thereby
providing a more informed assessment of its quality. COMET-polyic, inspired by
retrieval-based in-context learning, takes in translations of similar source
texts along with their human-labeled quality scores to guide the evaluation. We
find that including a single additional translation in COMET-polycand improves
the segment-level metric performance (0.079 to 0.118 Kendall's tau-b
correlation), with further gains when more translations are added.
Incorporating retrieved examples in COMET-polyic yields similar improvements
(0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.

</details>


### [13] [The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation](https://arxiv.org/abs/2508.18569)
*Girish A. Koushik,Fatemeh Nazarieh,Katherine Birch,Shenbin Qian,Diptesh Kanojia*

Main category: cs.CL

TL;DR: Proposes a self-evaluating visual metaphor generation framework with training-free and training-based pipelines for metaphor alignment. Achieves strong results compared to baselines, with structured prompting and lightweight RL performing well under modest compute.


<details>
  <summary>Details</summary>
Motivation: Visual metaphor generation is a challenging task that aims to generate an image given an input text metaphor. Inherently, it needs language understanding to bind a source concept with a target concept, in a way that preserves meaning while ensuring visual coherence.

Method: a self-evaluating visual metaphor generation framework that focuses on metaphor alignment. Includes a training-free pipeline that explicitly decomposes prompts into source-target-meaning (S-T-M) mapping for image synthesis, and a complementary training-based pipeline that improves alignment using our proposed self-evaluation reward schema, without any large-scale retraining.

Result: The training-free approach surpasses strong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores, with the training-based approach close behind. User study showed participants preferred GPT-4o overall, while the training-free pipeline led open-source methods and edged Imagen on abstract metaphors. S-T-M prompting helps longer or more abstract metaphors, with closed models excelling on short, concrete cases.

Conclusion: Structured prompting and lightweight RL perform metaphor alignment well under modest compute, and remaining gaps to human preference appear driven by aesthetics and sampling.

Abstract: Visual metaphor generation is a challenging task that aims to generate an
image given an input text metaphor. Inherently, it needs language understanding
to bind a source concept with a target concept, in a way that preserves meaning
while ensuring visual coherence. We propose a self-evaluating visual metaphor
generation framework that focuses on metaphor alignment. Our self-evaluation
approach combines existing metrics with our newly proposed metaphor
decomposition score and a meaning alignment (MA) metric. Within this setup, we
explore two novel approaches: a training-free pipeline that explicitly
decomposes prompts into source-target-meaning (S-T-M) mapping for image
synthesis, and a complementary training-based pipeline that improves alignment
using our proposed self-evaluation reward schema, without any large-scale
retraining. On the held-out test set, the training-free approach surpasses
strong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores,
with the training-based approach close behind. We evaluate our framework output
using a user-facing study, and observed that participants preferred GPT-4o
overall, while our training-free pipeline led open-source methods and edged
Imagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or
more abstract metaphors, with closed models excelling on short, concrete cases;
we also observe sensitivity to sampler settings. Overall, structured prompting
and lightweight RL perform metaphor alignment well under modest compute, and
remaining gaps to human preference appear driven by aesthetics and sampling.

</details>


### [14] [What do language models model? Transformers, automata, and the format of thought](https://arxiv.org/abs/2508.18598)
*Colin Klein*

Main category: cs.CL

TL;DR: 大型语言模型是语料库的模型，以不同于人类的方式使用语言。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型实际建模的内容，以及它们是否揭示了人类的能力，或者仅仅是我们训练它们的语料库的模型。

Method: 分析了transformer架构的计算架构的某些不变性，并借鉴了Liu et al. (2022) 关于shortcut automata的推测。

Result: 认知科学表明，人类的语言能力依赖于超线性格式进行计算，而transformer架构最多支持线性格式进行处理。

Conclusion: 大型语言模型是语料库的模型，而不是人类能力的反映。它们通过与人类不同的方式使用语言，语言是一种“话语机器”，可以让我们在适当的语境下创造新的语言。

Abstract: What do large language models actually model? Do they tell us something about
human capacities, or are they models of the corpus we've trained them on? I
give a non-deflationary defence of the latter position. Cognitive science tells
us that linguistic capabilities in humans rely supralinear formats for
computation. The transformer architecture, by contrast, supports at best a
linear formats for processing. This argument will rely primarily on certain
invariants of the computational architecture of transformers. I then suggest a
positive story about what transformers are doing, focusing on Liu et al.
(2022)'s intriguing speculations about shortcut automata. I conclude with why I
don't think this is a terribly deflationary story. Language is not (just) a
means for expressing inner state but also a kind of 'discourse machine' that
lets us make new language given appropriate context. We have learned to use
this technology in one way; LLMs have also learned to use it too, but via very
different means.

</details>


### [15] [A New NMT Model for Translating Clinical Texts from English to Spanish](https://arxiv.org/abs/2508.18607)
*Rumeng Li,Xun Wang,Hong Yu*

Main category: cs.CL

TL;DR: 提出了一种新的神经机器翻译系统NOOV，用于将电子健康记录（EHR）叙述从英语翻译成西班牙语。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏平行对齐的语料库以及包含大量未知单词，因此将电子健康记录（EHR）叙述从英语翻译成西班牙语是一项临床上重要但具有挑战性的任务。

Method: 提出了一种新的神经机器翻译系统NOOV，该系统需要很少的领域内平行语料库进行训练。NOOV集成了从平行语料库自动学习的双语词典和从大型生物医学知识资源中提取的短语查找表，以缓解NMT中未知的单词问题和单词重复挑战，从而增强了NMT系统更好的短语生成。

Result: NOOV能够生成更好的EHR翻译，并在准确性和流畅性方面都有所提高。

Conclusion: NOOV系统能够生成更好的EHR翻译，并在准确性和流畅性方面都有所提高。

Abstract: Translating electronic health record (EHR) narratives from English to Spanish
is a clinically important yet challenging task due to the lack of a
parallel-aligned corpus and the abundant unknown words contained. To address
such challenges, we propose \textbf{NOOV} (for No OOV), a new neural machine
translation (NMT) system that requires little in-domain parallel-aligned corpus
for training. NOOV integrates a bilingual lexicon automatically learned from
parallel-aligned corpora and a phrase look-up table extracted from a large
biomedical knowledge resource, to alleviate both the unknown word problem and
the word-repeat challenge in NMT, enhancing better phrase generation of NMT
systems. Evaluation shows that NOOV is able to generate better translation of
EHR with improvement in both accuracy and fluency.

</details>


### [16] [Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models](https://arxiv.org/abs/2508.18609)
*Chenxi Zhou,Pengfei Cao,Jiang Li,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 本研究深入探讨了后训练量化 (PTQ) 对大型语言模型 (LLM) 知识能力的影响，发现知识记忆比知识利用对量化参数更敏感，为开发知识感知量化策略提供了指导。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型 (LLM) 的规模，它们在部署方面提出了重大挑战，因此，训练后量化 (PTQ) 成为一种实用的压缩解决方案。然而，对于 PTQ 如何精确地影响不同的 LLM 知识能力，仍然缺乏全面的了解，并且现有量化模型的缩放定律通常忽略了关键的 PTQ 特定参数和特定于任务的敏感性。

Method: 通过广泛的实证研究来建立任务分层的缩放定律。将 LLM 知识分解为记忆和利用能力，并开发了一个统一的定量框架，该框架结合了模型大小、有效位宽、校准集大小和组大小。

Result: 知识记忆表现出比知识利用更强的对有效位宽、校准集大小和模型大小变化的敏感性。

Conclusion: 知识记忆对有效位宽、校准集大小和模型大小的变化比知识利用更敏感。这些发现为了解 PTQ 的影响提供了细粒度的理解，并为开发能够更好保留目标认知功能的知识感知量化策略提供了指导。

Abstract: Large language models (LLMs) present significant deployment challenges due to
their scale, with post-training quantization (PTQ) emerging as a practical
compression solution. However, a comprehensive understanding of how PTQ
precisely impacts diverse LLM knowledge capabilities remains elusive, and
existing scaling laws for quantized models often overlook crucial PTQ-specific
parameters and task-specific sensitivities. This paper addresses these gaps by
conducting an extensive empirical investigation to establish task-stratified
scaling laws. We disentangle LLM knowledge into memorization and utilization
capabilities and develop a unified quantitative framework that incorporates
model size, effective bit-width, calibration set size, and group size. Our
central finding reveals that knowledge memorization exhibits markedly greater
sensitivity to variations in effective bit-width, calibration set size, and
model size compared to the more robust knowledge utilization. These findings
offer a fine-grained understanding of PTQ's impact and provide guidance for
developing knowledge-aware quantization strategies that can better preserve
targeted cognitive functions.

</details>


### [17] [Thinking Before You Speak: A Proactive Test-time Scaling Approach](https://arxiv.org/abs/2508.18648)
*Cong Li,Wenchang Chai,Hejun Wu,Yan Pan,Pengxu Wei,Liang Lin*

Main category: cs.CL

TL;DR: 提出了一种名为“说话前思考”(TBYS)的推理框架，该框架通过在连续的推理步骤之间插入洞察来弥合差距，从而减轻了人工标注工作和微调开销, 并在具有挑战性的数学数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(llm)在复杂的推理任务(如数学)中经常表现出不足，我们将其归因于人类推理模式与llm训练数据中呈现的模式之间的差异。当处理复杂问题时，人类倾向于在表达解决方案之前仔细思考。然而，他们通常不表达自己的内心想法，包括他们的意图和选择的方法。因此，从人类来源收集的训练数据中可能缺少弥合推理步骤的关键见解。

Method: 提出了一个名为“说话前思考”(TBYS)的推理框架，并设计了一个自动收集和过滤上下文示例的管道，用于生成洞察，从而减轻了人工标注工作和微调开销。

Result: 在连续的推理步骤之间插入洞察，回顾状态并启动下一个推理步骤。与以往依赖于单个或静态提示工作流来促进推理的提示策略不同，主动生成洞察来指导推理过程。

Conclusion: TBYS在具有挑战性的数学数据集上验证了有效性。

Abstract: Large Language Models (LLMs) often exhibit deficiencies with complex
reasoning tasks, such as maths, which we attribute to the discrepancy between
human reasoning patterns and those presented in the LLMs' training data. When
dealing with complex problems, humans tend to think carefully before expressing
solutions. However, they often do not articulate their inner thoughts,
including their intentions and chosen methodologies. Consequently, critical
insights essential for bridging reasoning steps may be absent in training data
collected from human sources. To bridge this gap, we proposes inserting
\emph{insight}s between consecutive reasoning steps, which review the status
and initiate the next reasoning steps. Unlike prior prompting strategies that
rely on a single or a workflow of static prompts to facilitate reasoning,
\emph{insight}s are \emph{proactively} generated to guide reasoning processes.
We implement our idea as a reasoning framework, named \emph{Thinking Before You
Speak} (TBYS), and design a pipeline for automatically collecting and filtering
in-context examples for the generation of \emph{insight}s, which alleviates
human labeling efforts and fine-tuning overheads. Experiments on challenging
mathematical datasets verify the effectiveness of TBYS. Project website:
https://gitee.com/jswrt/TBYS

</details>


### [18] [Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models](https://arxiv.org/abs/2508.18651)
*Chenxu Yang,Qingyi Si,Zheng Lin*

Main category: cs.CL

TL;DR: 为了打破忠实性和表达性之间的权衡，我们提出了协作解码（CoDe），这是一种动态整合有无外部知识生成输出概率的新方法。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型在无缝整合知识的同时，难以保持忠实性和表达性，而人类自然具备这些能力。这种限制导致输出要么缺乏外部知识的支持，从而损害了忠实性，要么显得过于冗长和不自然，从而牺牲了表达性。

Method: 协作解码（CoDe），一种动态整合有无外部知识生成输出概率的新方法。这种整合由分布差异和模型置信度引导，从而能够选择性地激活来自模型内部参数的相关和可靠的表达。

Result: 通过全面的实验，

Conclusion: 提出的CoDe框架在增强忠实性的同时不影响各种LLM和评估指标的表达能力方面表现出卓越的性能，验证了其有效性和通用性。

Abstract: Grounding responses in external knowledge represents an effective strategy
for mitigating hallucinations in Large Language Models (LLMs). However, current
LLMs struggle to seamlessly integrate knowledge while simultaneously
maintaining faithfulness (or fidelity) and expressiveness, capabilities that
humans naturally possess. This limitation results in outputs that either lack
support from external knowledge, thereby compromising faithfulness, or appear
overly verbose and unnatural, thus sacrificing expressiveness. In this work, to
break the trade-off between faithfulness and expressiveness, we propose
Collaborative Decoding (CoDe), a novel approach that dynamically integrates
output probabilities generated with and without external knowledge. This
integration is guided by distribution divergence and model confidence, enabling
the selective activation of relevant and reliable expressions from the model's
internal parameters. Furthermore, we introduce a knowledge-aware reranking
mechanism that prevents over-reliance on prior parametric knowledge while
ensuring proper utilization of provided external information. Through
comprehensive experiments, our plug-and-play CoDe framework demonstrates
superior performance in enhancing faithfulness without compromising
expressiveness across diverse LLMs and evaluation metrics, validating both its
effectiveness and generalizability.

</details>


### [19] [Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models](https://arxiv.org/abs/2508.18655)
*Haoyu Wang,Guangyan Zhang,Jiale Chen,Jingyu Li,Yuehai Wang,Yiwen Guo*

Main category: cs.CL

TL;DR: This paper proposes Emotion Omni, a speech LLM architecture for generating empathetic responses with limited data, and introduces a 200k emotional dialogue dataset.


<details>
  <summary>Details</summary>
Motivation: Existing speech LLMs lack emotional understanding, which is crucial for user experience but requires extensive data and resources to achieve. This paper aims to develop an empathetic speech LLM with limited data and without large-scale training.

Method: A novel model architecture (Emotion Omni) and a data generation pipeline based on an open-source TTS framework.

Result: A 200k emotional dialogue dataset and a novel model architecture (Emotion Omni). Demos are available at https://w311411.github.io/omni_demo/

Conclusion: The paper introduces Emotion Omni, a novel model architecture for understanding emotional content in user speech and generating empathetic speech responses. A 200k emotional dialogue dataset was also created to support this.

Abstract: With the development of speech large language models (speech LLMs), users can
now interact directly with assistants via speech. However, most existing models
simply convert the response content into speech without fully understanding the
rich emotional and paralinguistic cues embedded in the user's query. In many
cases, the same sentence can have different meanings depending on the emotional
expression. Furthermore, emotional understanding is essential for improving
user experience in human-machine interaction. Currently, most speech LLMs with
empathetic capabilities are trained on massive datasets. This approach requires
vast amounts of data and significant computational resources. Therefore, a key
challenge lies in how to develop a speech LLM capable of generating empathetic
responses with limited data and without the need for large-scale training. To
address this challenge, we propose Emotion Omni, a novel model architecture
designed to understand the emotional content of user speech input and generate
empathetic speech responses. Additionally, we developed a data generation
pipeline based on an open-source TTS framework to construct a 200k emotional
dialogue dataset, which supports the construction of an empathetic speech
assistant. The demos are available at https://w311411.github.io/omni_demo/

</details>


### [20] [Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum](https://arxiv.org/abs/2508.18673)
*Xinglong Yang,Quan Feng,Zhongying Pan,Xiang Chen,Yu Tian,Wentong Li,Shuofei Qiao,Yuxia Geng,Xingyu Zhao,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: This paper presents a new method for selecting better prompts in multimodal reasoning by considering both how difficult the model finds a task and how inherently complex the task is.


<details>
  <summary>Details</summary>
Motivation: The effectiveness of MCoT prompting is often limited by randomly or manually selected examples that don't account for model-specific knowledge and task complexity.

Method: The proposed framework integrates model-perceived difficulty and intrinsic sample complexity to select prompt examples.

Result: Experiments on five benchmarks and multiple Multimodal Large Language Models (MLLMs) show substantial and consistent improvements.

Conclusion: This paper introduces a difficulty-balanced sampling strategy for prompt selection in Multimodal Chain-of-Thought (MCoT) prompting, which improves performance and reduces discrepancies caused by random sampling.

Abstract: The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often
limited by the use of randomly or manually selected examples. These examples
fail to account for both model-specific knowledge distributions and the
intrinsic complexity of the tasks, resulting in suboptimal and unstable model
performance. To address this, we propose a novel framework inspired by the
pedagogical principle of "tailored teaching with balanced difficulty". We
reframe prompt selection as a prompt curriculum design problem: constructing a
well ordered set of training examples that align with the model's current
capabilities. Our approach integrates two complementary signals: (1)
model-perceived difficulty, quantified through prediction disagreement in an
active learning setup, capturing what the model itself finds challenging; and
(2) intrinsic sample complexity, which measures the inherent difficulty of each
question-image pair independently of any model. By jointly analyzing these
signals, we develop a difficulty-balanced sampling strategy that ensures the
selected prompt examples are diverse across both dimensions. Extensive
experiments conducted on five challenging benchmarks and multiple popular
Multimodal Large Language Models (MLLMs) demonstrate that our method yields
substantial and consistent improvements and greatly reduces performance
discrepancies caused by random sampling, providing a principled and robust
approach for enhancing multimodal reasoning.

</details>


### [21] [Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning](https://arxiv.org/abs/2508.18687)
*Songtao Jiang,Yuxi Chen,Sibo Song,Yan Zhang,Yeying Jin,Yang Feng,Jian Wu,Zuozhu Liu*

Main category: cs.CL

TL;DR: Med-VLMs are fragile. RoMed dataset exposes this. CCL improves robustness and consistency.


<details>
  <summary>Details</summary>
Motivation: Current Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility in Medical Visual Question Answering, as their answers fluctuate significantly when faced with semantically equivalent rephrasings of medical questions due to insufficient alignment of medical concepts and hidden biases in training data.

Method: Consistency and Contrastive Learning (CCL), which integrates knowledge-anchored consistency learning and bias-aware contrastive learning.

Result: RoMed exposes critical robustness gaps with a 40% decline in Recall on SOTA models like LLaVA-Med. CCL significantly enhances robustness.

Conclusion: CCL improves answer consistency by 50% on the RoMed test set and achieves SOTA performance on three popular VQA benchmarks, demonstrating significantly enhanced robustness.

Abstract: In high-stakes medical applications, consistent answering across diverse
question phrasings is essential for reliable diagnosis. However, we reveal that
current Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility
in Medical Visual Question Answering, as their answers fluctuate significantly
when faced with semantically equivalent rephrasings of medical questions. We
attribute this to two limitations: (1) insufficient alignment of medical
concepts, leading to divergent reasoning patterns, and (2) hidden biases in
training data that prioritize syntactic shortcuts over semantic understanding.
To address these challenges, we construct RoMed, a dataset built upon original
VQA datasets containing 144k questions with variations spanning word-level,
sentence-level, and semantic-level perturbations. When evaluating
state-of-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarming
performance drops (e.g., a 40\% decline in Recall) compared to original VQA
benchmarks, exposing critical robustness gaps. To bridge this gap, we propose
Consistency and Contrastive Learning (CCL), which integrates two key
components: (1) knowledge-anchored consistency learning, aligning Med-VLMs with
medical knowledge rather than shallow feature patterns, and (2) bias-aware
contrastive learning, mitigating data-specific priors through discriminative
representation refinement. CCL achieves SOTA performance on three popular VQA
benchmarks and notably improves answer consistency by 50\% on the challenging
RoMed test set, demonstrating significantly enhanced robustness. Code will be
released.

</details>


### [22] [Attention2Probability: Attention-Driven Terminology Probability Estimation for Robust Speech-to-Text System](https://arxiv.org/abs/2508.18701)
*Yanfan Du,Jun Zhang,Bin Wang,Jin Qiu,Lu Huang,Yuan Ge,Xiaoqian Liu,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: Attention2Probability converts cross-attention weights between speech and terminology into presence probabilities, and it further employs curriculum learning to enhance retrieval accuracy. create and release a new speech dataset with terminology to support future research


<details>
  <summary>Details</summary>
Motivation: accurately generating domain-specific terms or neologisms remains challenging in speech recognition and translation

Method: Attention2Probability: attention-driven terminology probability estimation

Result: maximum recall rates reach 92.57% for Chinese and 86.83% for English with a latency of only 8.71ms per query, outperforms the VectorDB method

Conclusion: Attention2Probability-retrieved terms improves terminology accuracy by 6-17%, while revealing that the current utilization of terminology by SLMs has limitations.

Abstract: Recent advances in speech large language models (SLMs) have improved speech
recognition and translation in general domains, but accurately generating
domain-specific terms or neologisms remains challenging. To address this, we
propose Attention2Probability: attention-driven terminology probability
estimation for robust speech-to-text system, which is lightweight, flexible,
and accurate. Attention2Probability converts cross-attention weights between
speech and terminology into presence probabilities, and it further employs
curriculum learning to enhance retrieval accuracy. Furthermore, to tackle the
lack of data for speech-to-text tasks with terminology intervention, we create
and release a new speech dataset with terminology to support future research in
this area. Experimental results show that Attention2Probability significantly
outperforms the VectorDB method on our test set. Specifically, its maximum
recall rates reach 92.57% for Chinese and 86.83% for English. This high recall
is achieved with a latency of only 8.71ms per query. Intervening in SLMs'
recognition and translation tasks using Attention2Probability-retrieved terms
improves terminology accuracy by 6-17%, while revealing that the current
utilization of terminology by SLMs has limitations.

</details>


### [23] [Filtering for Creativity: Adaptive Prompting for Multilingual Riddle Generation in LLMs](https://arxiv.org/abs/2508.18709)
*Duy Le,Kent Ziti,Evan Girard-Sun,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: AOF improves LLMs riddle generation by filtering redundant generations and enforcing lexical novelty.


<details>
  <summary>Details</summary>
Motivation: Multilingual riddle generation challenges large language models (LLMs) to balance cultural fluency with creative abstraction. Standard prompting strategies tend to reuse memorized riddles or perform shallow paraphrasing.

Method: Adaptive Originality Filtering (AOF), a prompting framework that filters redundant generations using cosine-based similarity rejection, while enforcing lexical novelty and cross-lingual fidelity.

Result: AOF-enhanced GPT-4o achieves 0.177 Self-BLEU and 0.915 Distinct-2 in Japanese, signaling improved lexical diversity and reduced redundancy compared to other prompting methods and language pairs.

Conclusion: Semantic rejection can guide culturally grounded, creative generation without task-specific fine-tuning.

Abstract: Multilingual riddle generation challenges large language models (LLMs) to
balance cultural fluency with creative abstraction. Standard prompting
strategies -- zero-shot, few-shot, chain-of-thought -- tend to reuse memorized
riddles or perform shallow paraphrasing. We introduce Adaptive Originality
Filtering (AOF), a prompting framework that filters redundant generations using
cosine-based similarity rejection, while enforcing lexical novelty and
cross-lingual fidelity. Evaluated across three LLMs and four language pairs,
AOF-enhanced GPT-4o achieves \texttt{0.177} Self-BLEU and \texttt{0.915}
Distinct-2 in Japanese, signaling improved lexical diversity and reduced
redundancy compared to other prompting methods and language pairs. Our findings
show that semantic rejection can guide culturally grounded, creative generation
without task-specific fine-tuning.

</details>


### [24] [EMMM, Explain Me My Model! Explainable Machine Generated Text Detection in Dialogues](https://arxiv.org/abs/2508.18715)
*Angela Yifei Yuan,Haoyi Li,Soyeon Caren Han,Christopher Leckie*

Main category: cs.CL

TL;DR: This paper introduces EMMM, an explanation-then-detection framework for detecting machine-generated text in customer service scenarios. It balances accuracy, latency and interpretability for non-expert users.


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of large language models (LLMs) in customer service introduces new risks, as malicious actors can exploit them to conduct large-scale user impersonation through machine-generated text (MGT). Current MGT detection methods often struggle in online conversational settings, reducing the reliability and interpretability essential for trustworthy AI deployment. In customer service scenarios where operators are typically non-expert users, explanation become crucial for trustworthy MGT detection.

Method: explanation-then-detection framework

Result: EMMM provides explanations accessible to non-expert users, with 70% of human evaluators preferring its outputs, while achieving competitive accuracy compared to state-of-the-art models and maintaining low latency, generating outputs within 1 second.

Conclusion: EMMM provides explanations accessible to non-expert users, with 70% of human evaluators preferring its outputs, while achieving competitive accuracy compared to state-of-the-art models and maintaining low latency.

Abstract: The rapid adoption of large language models (LLMs) in customer service
introduces new risks, as malicious actors can exploit them to conduct
large-scale user impersonation through machine-generated text (MGT). Current
MGT detection methods often struggle in online conversational settings,
reducing the reliability and interpretability essential for trustworthy AI
deployment. In customer service scenarios where operators are typically
non-expert users, explanation become crucial for trustworthy MGT detection. In
this paper, we propose EMMM, an explanation-then-detection framework that
balances latency, accuracy, and non-expert-oriented interpretability.
Experimental results demonstrate that EMMM provides explanations accessible to
non-expert users, with 70\% of human evaluators preferring its outputs, while
achieving competitive accuracy compared to state-of-the-art models and
maintaining low latency, generating outputs within 1 second. Our code and
dataset are open-sourced at
https://github.com/AngieYYF/EMMM-explainable-chatbot-detection.

</details>


### [25] [Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models](https://arxiv.org/abs/2508.18739)
*Chang Wang,Siyu Yan,Depeng Yuan,Yuqi Chen,Yanhua Huang,Yuanhang Zheng,Shuhao Li,Yinqi Zhang,Kedi Chen,Mingrui Zhu,Ruiwen Xu*

Main category: cs.CL

TL;DR: 提出了DIVER框架，该框架基于大型语言模型(llm)，可以联合优化多样性和质量，从而生成高质量和多样化的广告标题。


<details>
  <summary>Details</summary>
Motivation: 当前的广告标题生成方法主要优化语言模型的标题质量或点击率(CTR)，通常忽略了对多样性的需求，导致同质化输出。为了解决这个局限性。

Method: 提出了一个基于大型语言模型(llm)的新框架DIVER，该框架针对多样性和质量进行了联合优化。我们首先设计了一个语义和文体感知数据生成管道，自动生成高质量的训练对，包括广告内容和多个不同的标题。为了在单个前向传递中实现生成高质量和多样化广告标题的目标，我们提出了一个多阶段多目标优化框架，包括监督微调(SFT)和强化学习(RL)。

Result: DIVER有效地平衡了质量和多样性。

Conclusion: DIVER框架在大型内容分享平台上部署后，广告主价值(ADVV)和点击率(CTR)分别提高了4.0%和1.4%。

Abstract: The generation of ad headlines plays a vital role in modern advertising,
where both quality and diversity are essential to engage a broad range of
audience segments. Current approaches primarily optimize language models for
headline quality or click-through rates (CTR), often overlooking the need for
diversity and resulting in homogeneous outputs. To address this limitation, we
propose DIVER, a novel framework based on large language models (LLMs) that are
jointly optimized for both diversity and quality. We first design a semantic-
and stylistic-aware data generation pipeline that automatically produces
high-quality training pairs with ad content and multiple diverse headlines. To
achieve the goal of generating high-quality and diversified ad headlines within
a single forward pass, we propose a multi-stage multi-objective optimization
framework with supervised fine-tuning (SFT) and reinforcement learning (RL).
Experiments on real-world industrial datasets demonstrate that DIVER
effectively balances quality and diversity. Deployed on a large-scale
content-sharing platform serving hundreds of millions of users, our framework
improves advertiser value (ADVV) and CTR by 4.0% and 1.4%.

</details>


### [26] [M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations](https://arxiv.org/abs/2508.18740)
*Qiao Liang,Ying Shen,Tiantian Chen,Lin Zhang*

Main category: cs.CL

TL;DR: 我们提出了第一个多模态、多场景的 MECTEC 数据集 MECAD，并提出了 M3HG 模型，在多模态对话中显式地捕获情感和因果上下文，并有效地融合上下文信息。


<details>
  <summary>Details</summary>
Motivation: 相关数据集的稀缺，阻碍了该领域模型的发展。现有的 MECTEC 方法未能明确地建模情感和因果关系，并忽略了不同层次的语义信息融合，导致性能下降。

Method: 我们提出了 M3HG，一个新颖的模型。

Result: 大量的实验表明，与现有的最先进的方法相比，M3HG 的有效性。

Conclusion: 我们提出了 M3HG 模型，通过多模态异构图在 utterance 间和 utterance 内的层面有效融合上下文信息，显式地捕获情感和因果上下文。

Abstract: Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has
recently gained significant attention in social media analysis, aiming to
extract emotion utterances, cause utterances, and emotion categories
simultaneously. However, the scarcity of related datasets, with only one
published dataset featuring highly uniform dialogue scenarios, hinders model
development in this field. To address this, we introduce MECAD, the first
multimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56
TV series spanning a wide range of dialogue contexts. In addition, existing
MECTEC methods fail to explicitly model emotional and causal contexts and
neglect the fusion of semantic information at different levels, leading to
performance degradation. In this paper, we propose M3HG, a novel model that
explicitly captures emotional and causal contexts and effectively fuses
contextual information at both inter- and intra-utterance levels via a
multimodal heterogeneous graph. Extensive experiments demonstrate the
effectiveness of M3HG compared with existing state-of-the-art methods. The
codes and dataset are available at https://github.com/redifinition/M3HG.

</details>


### [27] [Chronological Passage Assembling in RAG framework for Temporal Question Answering](https://arxiv.org/abs/2508.18748)
*Byeongjeong Kim,Jeonghyun Park,Joonho Yang,Hwanhee Lee*

Main category: cs.CL

TL;DR: ChronoRAG: a novel RAG framework specialized for narrative texts, focuses on refining dispersed document information and preserving narrative flow by capturing the temporal order among retrieved passages.


<details>
  <summary>Details</summary>
Motivation: Long-context question answering over narrative tasks is challenging because correct answers often hinge on reconstructing a coherent timeline of events while preserving contextual flow in a limited context window. Narrative texts possess unique characteristics that limit the effectiveness of these existing approaches. Specifically, understanding narrative texts requires more than isolated segments, as the broader context and sequential relationships between segments are crucial for comprehension.

Method: We propose ChronoRAG, a novel RAG framework specialized for narrative texts. This approach focuses on two essential aspects: refining dispersed document information into coherent and structured passages, and preserving narrative flow by explicitly capturing and maintaining the temporal order among retrieved passages.

Result: substantial improvements in tasks requiring both factual identification and comprehension of complex sequential relationships

Conclusion: We empirically demonstrate the effectiveness of ChronoRAG through experiments on the NarrativeQA dataset, showing substantial improvements in tasks requiring both factual identification and comprehension of complex sequential relationships, underscoring that reasoning over temporal order is crucial in resolving narrative QA.

Abstract: Long-context question answering over narrative tasks is challenging because
correct answers often hinge on reconstructing a coherent timeline of events
while preserving contextual flow in a limited context window.
Retrieval-augmented generation (RAG) indexing methods aim to address this
challenge by selectively retrieving only necessary document segments. However,
narrative texts possess unique characteristics that limit the effectiveness of
these existing approaches. Specifically, understanding narrative texts requires
more than isolated segments, as the broader context and sequential
relationships between segments are crucial for comprehension. To address these
limitations, we propose ChronoRAG, a novel RAG framework specialized for
narrative texts. This approach focuses on two essential aspects: refining
dispersed document information into coherent and structured passages, and
preserving narrative flow by explicitly capturing and maintaining the temporal
order among retrieved passages. We empirically demonstrate the effectiveness of
ChronoRAG through experiments on the NarrativeQA dataset, showing substantial
improvements in tasks requiring both factual identification and comprehension
of complex sequential relationships, underscoring that reasoning over temporal
order is crucial in resolving narrative QA.

</details>


### [28] [ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models](https://arxiv.org/abs/2508.18773)
*Qianyu He,Siyu Yuan,Xuefeng Li,Mingxuan Wang,Jiangjie Chen*

Main category: cs.CL

TL;DR: ThinkDial is the first open-recipe end-to-end framework that implements gpt-oss-style controllable reasoning through discrete operational modes.


<details>
  <summary>Details</summary>
Motivation: Controlling the computational effort of large language models (LLMs) with chain-of-thought reasoning remains a significant challenge. The open-source community has largely failed to achieve gpt-oss-style controllable reasoning through discrete operational modes.

Method: an end-to-end training paradigm that integrates budget-mode control throughout the entire pipeline: budget-mode supervised fine-tuning and two-phase budget-aware reinforcement learning with adaptive reward shaping.

Result: ThinkDial enables seamless switching between three distinct reasoning regimes: High mode (full reasoning capability), Medium mode (50 percent token reduction with <10 percent performance degradation), and Low mode (75 percent token reduction with <15 percent performance degradation).

Conclusion: ThinkDial achieves target compression-performance trade-offs with clear response length reductions while maintaining performance thresholds, and exhibits strong generalization capabilities on out-of-distribution tasks.

Abstract: Large language models (LLMs) with chain-of-thought reasoning have
demonstrated remarkable problem-solving capabilities, but controlling their
computational effort remains a significant challenge for practical deployment.
Recent proprietary systems like OpenAI's gpt-oss series have introduced
discrete operational modes for intuitive reasoning control, but the open-source
community has largely failed to achieve such capabilities. In this paper, we
introduce ThinkDial, the first open-recipe end-to-end framework that
successfully implements gpt-oss-style controllable reasoning through discrete
operational modes. Our system enables seamless switching between three distinct
reasoning regimes: High mode (full reasoning capability), Medium mode (50
percent token reduction with <10 percent performance degradation), and Low mode
(75 percent token reduction with <15 percent performance degradation). We
achieve this through an end-to-end training paradigm that integrates
budget-mode control throughout the entire pipeline: budget-mode supervised
fine-tuning that embeds controllable reasoning capabilities directly into the
learning process, and two-phase budget-aware reinforcement learning with
adaptive reward shaping. Extensive experiments demonstrate that ThinkDial
achieves target compression-performance trade-offs with clear response length
reductions while maintaining performance thresholds. The framework also
exhibits strong generalization capabilities on out-of-distribution tasks.

</details>


### [29] [Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction](https://arxiv.org/abs/2508.18780)
*Yilin Li,Xunjian Yin,Yilin Chen,Xiaojun Wan*

Main category: cs.CL

TL;DR: 我们提出了一种基于规则的强化学习框架，通过强化学习来引导LLM进行语法纠错，实验表明，该框架在中文数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的研究主要依靠有监督的微调来训练LLM直接生成修正后的句子，这限制了模型强大的推理能力。

Method: 基于规则的强化学习(Rule-Based RL)。

Result: 在中文数据集上的实验表明，我们的基于规则的RL框架实现了最先进的性能，召回率显著提高。

Conclusion: 通过使用RL来引导LLM，为GEC的未来发展提供了一个更可控和可靠的范例。

Abstract: Grammatical error correction is a significant task in NLP. Traditional
methods based on encoder-decoder models have achieved certain success, but the
application of LLMs in this field is still underexplored. Current research
predominantly relies on supervised fine-tuning to train LLMs to directly
generate the corrected sentence, which limits the model's powerful reasoning
ability. To address this limitation, we propose a novel framework based on
Rule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL
framework achieves \textbf{state-of-the-art }performance, with a notable
increase in \textbf{recall}. This result clearly highlights the advantages of
using RL to steer LLMs, offering a more controllable and reliable paradigm for
future development in GEC.

</details>


### [30] [Controllable Conversational Theme Detection Track at DSTC 12](https://arxiv.org/abs/2508.18783)
*Igor Shalyminov,Hang Su,Jake Vincent,Siffi Singh,Jason Cai,James Gung,Raphael Shu,Saab Mansour*

Main category: cs.CL

TL;DR: This paper introduces theme detection in conversational analytics, poses it as a challenge at DSTC 12, and discusses the results of participant teams.


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of Large Language Models (LLMs) in the analytics field has taken the problems that can be automated to a new level of complexity and scale. This process can significantly reduce the manual effort involved in analyzing expansive dialogs, particularly in domains like customer support or sales.

Method: The paper introduces Theme Detection as a critical task in conversational analytics, aimed at automatically identifying and categorizing topics within conversations. The paper poses Controllable Conversational Theme Detection problem as a public competition track at Dialog System Technology Challenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of dialog utterances, with the distinctive aspect being controllability of the resulting theme clusters' granularity achieved via the provided user preference data.

Result: The track materials (data and code) are openly available in the GitHub repository.

Conclusion: The paper discusses the participant teams' submissions and provides insights from those.

Abstract: Conversational analytics has been on the forefront of transformation driven
by the advances in Speech and Natural Language Processing techniques. Rapid
adoption of Large Language Models (LLMs) in the analytics field has taken the
problems that can be automated to a new level of complexity and scale. In this
paper, we introduce Theme Detection as a critical task in conversational
analytics, aimed at automatically identifying and categorizing topics within
conversations. This process can significantly reduce the manual effort involved
in analyzing expansive dialogs, particularly in domains like customer support
or sales. Unlike traditional dialog intent detection, which often relies on a
fixed set of intents for downstream system logic, themes are intended as a
direct, user-facing summary of the conversation's core inquiry. This
distinction allows for greater flexibility in theme surface forms and
user-specific customizations. We pose Controllable Conversational Theme
Detection problem as a public competition track at Dialog System Technology
Challenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of
dialog utterances, with the distinctive aspect being controllability of the
resulting theme clusters' granularity achieved via the provided user preference
data. We give an overview of the problem, the associated dataset and the
evaluation metrics, both automatic and human. Finally, we discuss the
participant teams' submissions and provide insights from those. The track
materials (data and code) are openly available in the GitHub repository.

</details>


### [31] [LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination](https://arxiv.org/abs/2508.18791)
*Ziming Zhu,Chenglong Wang,Shunjie Xing,Yifu Huo,Fengning Tian,Quan Du,Di Yang,Chunliang Zhang,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: LaTeXTrans 通过协作式多代理系统有效翻译 LaTeX 格式文档，优于主流 MT 系统。


<details>
  <summary>Details</summary>
Motivation: 现代机器翻译 (MT) 系统在通用领域文本上取得了显著进展，但翻译结构化的 LaTeX 格式文档仍然是一个重大挑战。这些文档通常将自然语言与领域特定的语法（例如数学方程式、表格、图形和交叉引用）混合在一起，所有这些都必须准确保留以保持语义完整性和可编译性。

Method: LaTeXTrans，一个协作式多代理系统，通过六个专门的代理确保格式保留、结构保真度和术语一致性：1) 解析器，通过占位符替换和语法过滤将 LaTeX 分解为翻译友好的单元；2) 翻译器、验证器、总结器和术语提取器，它们协同工作以确保上下文感知、自我纠正和术语一致的翻译；3) 生成器，将翻译的内容重建为结构良好的 LaTeX 文档。

Result: 实验结果表明，LaTeXTrans 在翻译准确性和结构保真度方面优于主流 MT 系统。

Conclusion: LaTeXTrans 在翻译准确性和结构保真度方面优于主流 MT 系统，为翻译 LaTeX 格式文档提供了一种有效且实用的解决方案。

Abstract: Despite the remarkable progress of modern machine translation (MT) systems on
general-domain texts, translating structured LaTeX-formatted documents remains
a significant challenge. These documents typically interleave natural language
with domain-specific syntax, such as mathematical equations, tables, figures,
and cross-references, all of which must be accurately preserved to maintain
semantic integrity and compilability. In this paper, we introduce LaTeXTrans, a
collaborative multi-agent system designed to address this challenge. LaTeXTrans
ensures format preservation, structural fidelity, and terminology consistency
through six specialized agents: 1) a Parser that decomposes LaTeX into
translation-friendly units via placeholder substitution and syntax filtering;
2) a Translator, Validator, Summarizer, and Terminology Extractor that work
collaboratively to ensure context-aware, self-correcting, and
terminology-consistent translations; 3) a Generator that reconstructs the
translated content into well-structured LaTeX documents. Experimental results
demonstrate that LaTeXTrans can outperform mainstream MT systems in both
translation accuracy and structural fidelity, offering an effective and
practical solution for translating LaTeX-formatted documents.

</details>


### [32] [LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection](https://arxiv.org/abs/2508.18819)
*Shubham Gupta,Shraban Kumar Chatterjee,Suman Kundu*

Main category: cs.CL

TL;DR: A new self-supervised misinformation detection framework uses AMR, news propagation dynamics, and an LLM-based graph contrastive loss to outperform existing methods with less labeled data.


<details>
  <summary>Details</summary>
Motivation: Existing misinformation detection approaches struggle with long-range dependencies, complex semantic relations, social dynamics, and require extensive labeled datasets, making them resource-intensive.

Method: A novel self-supervised misinformation detection framework integrating complex semantic relations using Abstract Meaning Representation (AMR) and news propagation dynamics. It introduces an LLM-based graph contrastive loss (LGCL) and a multi-view graph masked autoencoder.

Result: The self-supervised framework achieves superior performance compared to other state-of-the-art methodologies, even with limited labeled datasets while improving generalizability.

Conclusion: The proposed self-supervised framework outperforms state-of-the-art methods in misinformation detection, even with limited labeled data, while improving generalizability.

Abstract: The proliferation of misinformation in the digital age has led to significant
societal challenges. Existing approaches often struggle with capturing
long-range dependencies, complex semantic relations, and the social dynamics
influencing news dissemination. Furthermore, these methods require extensive
labelled datasets, making their deployment resource-intensive. In this study,
we propose a novel self-supervised misinformation detection framework that
integrates both complex semantic relations using Abstract Meaning
Representation (AMR) and news propagation dynamics. We introduce an LLM-based
graph contrastive loss (LGCL) that utilizes negative anchor points generated by
a Large Language Model (LLM) to enhance feature separability in a zero-shot
manner. To incorporate social context, we employ a multi view graph masked
autoencoder, which learns news propagation features from social context graph.
By combining these semantic and propagation-based features, our approach
effectively differentiates between fake and real news in a self-supervised
manner. Extensive experiments demonstrate that our self-supervised framework
achieves superior performance compared to other state-of-the-art methodologies,
even with limited labelled datasets while improving generalizability.

</details>


### [33] [Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness](https://arxiv.org/abs/2508.18824)
*Sirui Chen,Changxin Tian,Binbin Hu,Kunlong Chen,Ziqi Liu,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: Proposes a program-assisted synthesis framework to generate a high-quality mathematical corpus, which improves the inference capabilities of LLMs.


<details>
  <summary>Details</summary>
Motivation: Enhancing the mathematical reasoning of large language models (LLMs) demands high-quality training data, yet conventional methods face critical challenges in scalability, cost, and data reliability.

Method: A novel program-assisted synthesis framework that systematically generates a high-quality mathematical corpus with guaranteed diversity, complexity, and correctness by integrating mathematical knowledge systems and domain-specific tools to create executable programs, which are then translated into natural language problem-solution pairs and vetted by a bilateral validation mechanism.

Result: Generated 12.3 million problem-solving triples.

Conclusion: Models fine-tuned on the generated data significantly improve their inference capabilities, achieving state-of-the-art performance on several benchmark datasets.

Abstract: Enhancing the mathematical reasoning of large language models (LLMs) demands
high-quality training data, yet conventional methods face critical challenges
in scalability, cost, and data reliability. To address these limitations, we
propose a novel program-assisted synthesis framework that systematically
generates a high-quality mathematical corpus with guaranteed diversity,
complexity, and correctness. This framework integrates mathematical knowledge
systems and domain-specific tools to create executable programs. These programs
are then translated into natural language problem-solution pairs and vetted by
a bilateral validation mechanism that verifies solution correctness against
program outputs and ensures program-problem consistency. We have generated 12.3
million such problem-solving triples. Experiments demonstrate that models
fine-tuned on our data significantly improve their inference capabilities,
achieving state-of-the-art performance on several benchmark datasets and
showcasing the effectiveness of our synthesis approach.

</details>


### [34] [ConfTuner: Training Large Language Models to Express Their Confidence Verbally](https://arxiv.org/abs/2508.18847)
*Yibo Li,Miao Xiong,Jiaying Wu,Bryan Hooi*

Main category: cs.CL

TL;DR: ConfTuner is introduced to address LLM overconfidence by fine-tuning with tokenized Brier score, improving calibration and enabling downstream gains.


<details>
  <summary>Details</summary>
Motivation: Current LLMs are often observed to generate incorrect answers with high confidence, a phenomenon known as "overconfidence". Existing approaches have limited effectiveness and generalizability.

Method: ConfTuner, a fine-tuning method that introduces minimal overhead and does not require ground-truth confidence scores or proxy confidence estimates. It relies on a new loss function, tokenized Brier score.

Result: ConfTuner improves calibration across diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Better-calibrated confidence enables downstream gains in self-correction and model cascade.

Conclusion: ConfTuner improves calibration across diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Better-calibrated confidence enables downstream gains in self-correction and model cascade.

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes domains
such as science, law, and healthcare, where accurate expressions of uncertainty
are essential for reliability and trust. However, current LLMs are often
observed to generate incorrect answers with high confidence, a phenomenon known
as "overconfidence". Recent efforts have focused on calibrating LLMs'
verbalized confidence: i.e., their expressions of confidence in text form, such
as "I am 80% confident that...". Existing approaches either rely on prompt
engineering or fine-tuning with heuristically generated uncertainty estimates,
both of which have limited effectiveness and generalizability. Motivated by the
notion of proper scoring rules for calibration in classical machine learning
models, we introduce ConfTuner, a simple and efficient fine-tuning method that
introduces minimal overhead and does not require ground-truth confidence scores
or proxy confidence estimates. ConfTuner relies on a new loss function,
tokenized Brier score, which we theoretically prove to be a proper scoring
rule, intuitively meaning that it "correctly incentivizes the model to report
its true probability of being correct". ConfTuner improves calibration across
diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our
results further show that better-calibrated confidence enables downstream gains
in self-correction and model cascade, advancing the development of trustworthy
LLM systems. The code is available at
https://github.com/liushiliushi/ConfTuner.

</details>


### [35] [ReflectivePrompt: Reflective evolution in autoprompting algorithms](https://arxiv.org/abs/2508.18870)
*Viktor N. Zhuravlev,Artur R. Khairullin,Ernest A. Dyagin,Alena N. Sitkina,Nikita I. Kulin*

Main category: cs.CL

TL;DR: 本文提出了一种新的自动提示方法ReflectivePrompt，该方法基于进化算法，通过反思进化方法更精确地搜索最佳提示，并在多个数据集上表现出显著的改进。


<details>
  <summary>Details</summary>
Motivation: 自动提示是自动选择语言模型的优化提示的过程，随着大型语言模型（LLM）领域的大量研究推动，提示工程的快速发展使其越来越受欢迎。

Method: 一种基于进化算法的新的自动提示方法，采用反思进化方法来更精确和全面地搜索最佳提示。

Result: 在开放访问的大型语言模型t-lite-instruct-0.1和gemma3-27b-it上，在33个数据集上进行了分类和文本生成任务的测试。与当前最先进的方法相比，该方法在指标上平均有显着改善（例如，在BBH上比EvoPrompt高28%）。

Conclusion: ReflectivePrompt在多个数据集上显著优于现有技术，成为基于进化算法的最有效的自动提示解决方案之一。

Abstract: Autoprompting is the process of automatically selecting optimized prompts for
language models, which has been gaining popularity with the rapid advancement
of prompt engineering, driven by extensive research in the field of large
language models (LLMs). This paper presents ReflectivePrompt - a novel
autoprompting method based on evolutionary algorithms that employs a reflective
evolution approach for more precise and comprehensive search of optimal
prompts. ReflectivePrompt utilizes short-term and long-term reflection
operations before crossover and elitist mutation to enhance the quality of the
modifications they introduce. This method allows for the accumulation of
knowledge obtained throughout the evolution process and updates it at each
epoch based on the current population. ReflectivePrompt was tested on 33
datasets for classification and text generation tasks using open-access large
language models: t-lite-instruct-0.1 and gemma3-27b-it. The method
demonstrates, on average, a significant improvement (e.g., 28% on BBH compared
to EvoPrompt) in metrics relative to current state-of-the-art approaches,
thereby establishing itself as one of the most effective solutions in
evolutionary algorithm-based autoprompting.

</details>


### [36] [Empowering Computing Education Researchers Through LLM-Assisted Content Analysis](https://arxiv.org/abs/2508.18872)
*Laurie Gale,Sebastian Mateos Nicolajsen*

Main category: cs.CL

TL;DR: propose a method for conducting rigorous analysis on large volumes of textual data, namely a variation of LLM-assisted content analysis (LACA).


<details>
  <summary>Details</summary>
Motivation: many researchers lack the colleagues, resources, or capacity to conduct research that is generalisable or rigorous enough to advance the discipline

Method: a variation of LLM-assisted content analysis (LACA). This method combines content analysis with the use of large language models

Result: Using a computing education dataset, we illustrate how LACA could be applied in a reproducible and rigorous manner.

Conclusion: This method has potential in CER, enabling more generalisable findings from a wider range of research. This, together with the development of similar methods, can help to advance both the practice and research quality of the CER discipline.

Abstract: Computing education research (CER) is often instigated by practitioners
wanting to improve both their own and the wider discipline's teaching practice.
However, the latter is often difficult as many researchers lack the colleagues,
resources, or capacity to conduct research that is generalisable or rigorous
enough to advance the discipline. As a result, research methods that enable
sense-making with larger volumes of qualitative data, while not increasing the
burden on the researcher, have significant potential within CER.
  In this discussion paper, we propose such a method for conducting rigorous
analysis on large volumes of textual data, namely a variation of LLM-assisted
content analysis (LACA). This method combines content analysis with the use of
large language models, empowering researchers to conduct larger-scale research
which they would otherwise not be able to perform. Using a computing education
dataset, we illustrate how LACA could be applied in a reproducible and rigorous
manner. We believe this method has potential in CER, enabling more
generalisable findings from a wider range of research. This, together with the
development of similar methods, can help to advance both the practice and
research quality of the CER discipline.

</details>


### [37] [Affective Polarization across European Parliaments](https://arxiv.org/abs/2508.18916)
*Bojan Evkoski,Igor Mozetič,Nikola Ljubešić,Petra Kralj Novak*

Main category: cs.CL

TL;DR: Study finds affective polarization in European parliaments, with reciprocity as a contributing factor, but no difference between active and inactive members.


<details>
  <summary>Details</summary>
Motivation: Examine the presence of affective polarization in European parliaments in a fully automated manner, as it has become a prominent feature of political discourse worldwide.

Method: Natural language processing techniques to estimate parliamentarian sentiment by comparing negativity in references to opposing groups versus one's own.

Result: Demonstrates consistent affective polarization across six European parliaments. No difference in affective polarization between less active and more active members. Reciprocity is a contributing mechanism.

Conclusion: Consistent affective polarization exists across six European parliaments. Activity correlates with negativity, but no difference in affective polarization exists between less and more active members. Reciprocity contributes to affective polarization.

Abstract: Affective polarization, characterized by increased negativity and hostility
towards opposing groups, has become a prominent feature of political discourse
worldwide. Our study examines the presence of this type of polarization in a
selection of European parliaments in a fully automated manner. Utilizing a
comprehensive corpus of parliamentary speeches from the parliaments of six
European countries, we employ natural language processing techniques to
estimate parliamentarian sentiment. By comparing the levels of negativity
conveyed in references to individuals from opposing groups versus one's own, we
discover patterns of affectively polarized interactions. The findings
demonstrate the existence of consistent affective polarization across all six
European parliaments. Although activity correlates with negativity, there is no
observed difference in affective polarization between less active and more
active members of parliament. Finally, we show that reciprocity is a
contributing mechanism in affective polarization between parliamentarians
across all six parliaments.

</details>


### [38] [Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework](https://arxiv.org/abs/2508.18929)
*Ilias Driouich,Hongliu Cao,Eoin Thomas*

Main category: cs.CL

TL;DR: This paper introduces a multi-agent framework to generate QA datasets for evaluating RAG systems, focusing on semantic diversity and privacy preservation. The generated datasets outperform baselines in diversity and privacy masking.


<details>
  <summary>Details</summary>
Motivation: the effectiveness and trustworthiness of these systems critically depends on how they are evaluated, particularly on whether the evaluation process captures real-world constraints like protecting sensitive information. While current evaluation efforts for RAG systems have primarily focused on the development of performance metrics, far less attention has been given to the design and quality of the underlying evaluation datasets, despite their pivotal role in enabling meaningful, reliable assessments.

Method: a novel multi-agent framework for generating synthetic QA datasets for RAG evaluation that prioritize semantic diversity and privacy preservation. Our approach involves: (1) a Diversity agent leveraging clustering techniques to maximize topical coverage and semantic variability, (2) a Privacy Agent that detects and mask sensitive information across multiple domains and (3) a QA curation agent that synthesizes private and diverse QA pairs suitable as ground truth for RAG evaluation.

Result: our evaluation sets outperform baseline methods in diversity and achieve robust privacy masking on domain-specific datasets.

Conclusion: This work offers a practical and ethically aligned pathway toward safer, more comprehensive RAG system evaluation, laying the foundation for future enhancements aligned with evolving AI regulations and compliance standards.

Abstract: Retrieval-augmented generation (RAG) systems improve large language model
outputs by incorporating external knowledge, enabling more informed and
context-aware responses. However, the effectiveness and trustworthiness of
these systems critically depends on how they are evaluated, particularly on
whether the evaluation process captures real-world constraints like protecting
sensitive information. While current evaluation efforts for RAG systems have
primarily focused on the development of performance metrics, far less attention
has been given to the design and quality of the underlying evaluation datasets,
despite their pivotal role in enabling meaningful, reliable assessments. In
this work, we introduce a novel multi-agent framework for generating synthetic
QA datasets for RAG evaluation that prioritize semantic diversity and privacy
preservation. Our approach involves: (1) a Diversity agent leveraging
clustering techniques to maximize topical coverage and semantic variability,
(2) a Privacy Agent that detects and mask sensitive information across multiple
domains and (3) a QA curation agent that synthesizes private and diverse QA
pairs suitable as ground truth for RAG evaluation. Extensive experiments
demonstrate that our evaluation sets outperform baseline methods in diversity
and achieve robust privacy masking on domain-specific datasets. This work
offers a practical and ethically aligned pathway toward safer, more
comprehensive RAG system evaluation, laying the foundation for future
enhancements aligned with evolving AI regulations and compliance standards.

</details>


### [39] [Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural Models](https://arxiv.org/abs/2508.18988)
*Hung Ming Liu*

Main category: cs.CL

TL;DR: 提出了一种新的神经模型框架，该框架通过AI Mother Tongue来实现可解释的推理。


<details>
  <summary>Details</summary>
Motivation: 神经模型缺乏直观的推理、组合符号链和固有的可解释性。

Method: 开发一种神经模型框架，该框架使用一种原生符号语言，即AI Mother Tongue。使用互补的训练目标来增强符号纯度和决策稀疏性，并采用顺序专业化策略。

Result: 在AI任务上的实验表明，该方法具有竞争性的准确性以及可验证的推理轨迹。

Conclusion: AI Mother Tongue可以作为神经模型中可解释性、直觉和符号推理的统一机制。

Abstract: We present a framework where neural models develop an AI Mother Tongue, a
native symbolic language that simultaneously supports intuitive reasoning,
compositional symbol chains, and inherent interpretability. Unlike post-hoc
explanation methods, our approach embeds reasoning directly into the model's
representations: symbols capture meaningful semantic patterns, chains trace
decision paths, and gated induction mechanisms guide selective focus, yielding
transparent yet flexible reasoning. We introduce complementary training
objectives to enhance symbol purity and decision sparsity, and employ a
sequential specialization strategy to first build broad symbolic competence and
then refine intuitive judgments. Experiments on AI tasks demonstrate
competitive accuracy alongside verifiable reasoning traces, showing that AI
Mother Tongue can serve as a unified mechanism for interpretability, intuition,
and symbolic reasoning in neural models.

</details>


### [40] [Automatic Prompt Optimization with Prompt Distillation](https://arxiv.org/abs/2508.18992)
*Viktor N. Zhuravlev,Artur R. Khairullin,Ernest A. Dyagin,Alena N. Sitkina,Nikita I. Kulin*

Main category: cs.CL

TL;DR: DistillPrompt is a novel autoprompting method based on large language models that employs a multi-stage integration of task-specific information into prompts using training data.


<details>
  <summary>Details</summary>
Motivation: This paper presents DistillPrompt -- a novel autoprompting method based on large language models that employs a multi-stage integration of task-specific information into prompts using training data.

Method: DistillPrompt utilizes distillation, compression, and aggregation operations to explore the prompt space more thoroughly.

Result: The results demonstrate a significant average improvement (e.g., 20.12% across the entire dataset compared to Grips) in key metrics over existing methods in the field.

Conclusion: DistillPrompt is one of the most effective non-gradient approaches in autoprompting.

Abstract: Autoprompting is the process of automatically selecting optimized prompts for
language models, which is gaining popularity due to the rapid development of
prompt engineering driven by extensive research in the field of large language
models (LLMs). This paper presents DistillPrompt -- a novel autoprompting
method based on large language models that employs a multi-stage integration of
task-specific information into prompts using training data. DistillPrompt
utilizes distillation, compression, and aggregation operations to explore the
prompt space more thoroughly. The method was tested on different datasets for
text classification and generation tasks using the t-lite-instruct-0.1 language
model. The results demonstrate a significant average improvement (e.g., 20.12%
across the entire dataset compared to Grips) in key metrics over existing
methods in the field, establishing DistillPrompt as one of the most effective
non-gradient approaches in autoprompting.

</details>


### [41] [MovieCORE: COgnitive REasoning in Movies](https://arxiv.org/abs/2508.19026)
*Gueter Josmy Faure,Min-Hung Chen,Jia-Fong Yeh,Ying Cheng,Hung-Ting Su,Yung-Hao Tang,Shang-Hong Lai,Winston H. Hsu*

Main category: cs.CL

TL;DR: MovieCORE: A VQA dataset probing deeper cognitive understanding of movies, with an ACE module to enhance model reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing VQA datasets lack depth and fail to engage System-2 thinking for movie content understanding.

Method: Agentic brainstorming approach using multiple LLMs for question-answer pair generation and an agentic enhancement module (ACE) to improve model reasoning.

Result: Introduces MovieCORE dataset and ACE module, improving model reasoning by up to 25%.

Conclusion: This work advances movie understanding in AI and reveals limitations of current VQA models on nuanced cinematic content questions.

Abstract: This paper introduces MovieCORE, a novel video question answering (VQA)
dataset designed to probe deeper cognitive understanding of movie content.
Unlike existing datasets that focus on surface-level comprehension, MovieCORE
emphasizes questions that engage System-2 thinking while remaining specific to
the video material. We present an innovative agentic brainstorming approach,
utilizing multiple large language models (LLMs) as thought agents to generate
and refine high-quality question-answer pairs. To evaluate dataset quality, we
develop a set of cognitive tests assessing depth, thought-provocation
potential, and syntactic complexity. We also propose a comprehensive evaluation
scheme for assessing VQA model performance on deeper cognitive tasks. To
address the limitations of existing video-language models (VLMs), we introduce
an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves
model reasoning capabilities post-training by up to 25%. Our work contributes
to advancing movie understanding in AI systems and provides valuable insights
into the capabilities and limitations of current VQA models when faced with
more challenging, nuanced questions about cinematic content. Our project page,
dataset and code can be found at
https://joslefaure.github.io/assets/html/moviecore.html.

</details>


### [42] [HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance](https://arxiv.org/abs/2508.19076)
*Ziyue Li,Yuan Chang,Gaihong Yu,Xiaoqiu Le*

Main category: cs.CL

TL;DR: HiPlan 通过分层规划框架，为基于 LLM 的智能体提供自适应的全局-局部指导，以提高其决策能力。


<details>
  <summary>Details</summary>
Motivation: 基于大型语言模型（LLM）的智能体在决策任务中表现出了卓越的能力，但在复杂、长期的规划场景中却表现得非常吃力。这是因为它们缺乏宏观指导，导致在复杂的任务中迷失方向和失败，以及在执行过程中缺乏持续的监督，使它们对环境变化反应迟钝，容易出现偏差。

Method: 我们引入了HiPlan，这是一个分层规划框架，提供自适应的全局-局部指导，以提高基于LLM的智能体的决策能力。HiPlan将复杂的任务分解为里程碑式的行动指南，以获得总体方向，并逐步提示详细的行动。

Result: HiPlan大幅优于强大的基线

Conclusion: HiPlan在两个基准测试中大幅优于强大的基线，并且消融研究验证了其分层组件的互补优势。

Abstract: Large language model (LLM)-based agents have demonstrated remarkable
capabilities in decision-making tasks, but struggle significantly with complex,
long-horizon planning scenarios. This arises from their lack of macroscopic
guidance, causing disorientation and failures in complex tasks, as well as
insufficient continuous oversight during execution, rendering them unresponsive
to environmental changes and prone to deviations. To tackle these challenges,
we introduce HiPlan, a hierarchical planning framework that provides adaptive
global-local guidance to boost LLM-based agents'decision-making. HiPlan
decomposes complex tasks into milestone action guides for general direction and
step-wise hints for detailed actions. During the offline phase, we construct a
milestone library from expert demonstrations, enabling structured experience
reuse by retrieving semantically similar tasks and milestones. In the execution
phase, trajectory segments from past milestones are dynamically adapted to
generate step-wise hints that align current observations with the milestone
objectives, bridging gaps and correcting deviations. Extensive experiments
across two challenging benchmarks demonstrate that HiPlan substantially
outperforms strong baselines, and ablation studies validate the complementary
benefits of its hierarchical components.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [43] [Towards Training-Free Underwater 3D Object Detection from Sonar Point Clouds: A Comparison of Traditional and Deep Learning Approaches](https://arxiv.org/abs/2508.18293)
*M. Salman Shaukat,Yannik Käckenmeister,Sebastian Bader,Thomas Kirste*

Main category: cs.CV

TL;DR: This paper explores training-free underwater 3D object detection using synthetic data and template matching, finding that template matching is more robust to real-world data than neural networks trained on synthetic data.


<details>
  <summary>Details</summary>
Motivation: Obtaining sufficient annotated sonar data is prohibitively expensive and logistically complex.

Method: Development and comparison of two paradigms for training-free detection of artificial structures in multibeam echo-sounder point clouds: a physics-based sonar simulation pipeline that generates synthetic training data for neural networks, with a robust model-based template matching system that leverages geometric priors of target objects.

Result: Neural networks trained on synthetic data achieve 98% mAP on simulated scenes, but drop to 40% mAP on real sonar data due to domain shift. Template matching approach maintains 83% mAP on real data without requiring any training.

Conclusion: Template matching demonstrates remarkable robustness to acoustic noise and environmental variations.

Abstract: Underwater 3D object detection remains one of the most challenging frontiers
in computer vision, where traditional approaches struggle with the harsh
acoustic environment and scarcity of training data. While deep learning has
revolutionized terrestrial 3D detection, its application underwater faces a
critical bottleneck: obtaining sufficient annotated sonar data is prohibitively
expensive and logistically complex, often requiring specialized vessels, expert
surveyors, and favorable weather conditions. This work addresses a fundamental
question: Can we achieve reliable underwater 3D object detection without
real-world training data? We tackle this challenge by developing and comparing
two paradigms for training-free detection of artificial structures in multibeam
echo-sounder point clouds. Our dual approach combines a physics-based sonar
simulation pipeline that generates synthetic training data for state-of-the-art
neural networks, with a robust model-based template matching system that
leverages geometric priors of target objects. Evaluation on real bathymetry
surveys from the Baltic Sea reveals surprising insights: while neural networks
trained on synthetic data achieve 98% mean Average Precision (mAP) on simulated
scenes, they drop to 40% mAP on real sonar data due to domain shift.
Conversely, our template matching approach maintains 83% mAP on real data
without requiring any training, demonstrating remarkable robustness to acoustic
noise and environmental variations. Our findings challenge conventional wisdom
about data-hungry deep learning in underwater domains and establish the first
large-scale benchmark for training-free underwater 3D detection. This work
opens new possibilities for autonomous underwater vehicle navigation, marine
archaeology, and offshore infrastructure monitoring in data-scarce environments
where traditional machine learning approaches fail.

</details>


### [44] [MobileDenseAttn:A Dual-Stream Architecture for Accurate and Interpretable Brain Tumor Detection](https://arxiv.org/abs/2508.18294)
*Shudipta Banik,Muna Das,Trapa Banik,Md. Ehsanul Haque*

Main category: cs.CV

TL;DR: Introduces MobileDenseAttn, a fusion model for brain tumor detection in MRI, offering improved accuracy, efficiency, interpretability, and clinical practicality.


<details>
  <summary>Details</summary>
Motivation: Current approaches are not universal because they have limited generalization to heterogeneous tumors, are computationally inefficient, are not interpretable, and lack transparency, thus limiting trustworthiness.

Method: a fusion model of dual streams of MobileNetV2 and DenseNet201 with feature level fusion and trained on an augmented dataset of 6,020 MRI scans

Result: MobileDenseAttn provides a training accuracy of 99.75%, a testing accuracy of 98.35%, and a stable F1 score of 0.9835 (95% CI: 0.9743 to 0.9920). a +3.67% accuracy increase and a 39.3% decrease in training time compared to VGG19. The GradCAM heatmaps clearly show tumor-affected areas, offering clinically significant localization and improving interpretability.

Conclusion: MobileDenseAttn is an efficient, high performance, interpretable model with a high probability of becoming a clinically practical tool in identifying brain tumors in the real world.

Abstract: The detection of brain tumor in MRI is an important aspect of ensuring timely
diagnostics and treatment; however, manual analysis is commonly long and
error-prone. Current approaches are not universal because they have limited
generalization to heterogeneous tumors, are computationally inefficient, are
not interpretable, and lack transparency, thus limiting trustworthiness. To
overcome these issues, we introduce MobileDenseAttn, a fusion model of dual
streams of MobileNetV2 and DenseNet201 that can help gradually improve the
feature representation scale, computing efficiency, and visual explanations via
GradCAM. Our model uses feature level fusion and is trained on an augmented
dataset of 6,020 MRI scans representing glioma, meningioma, pituitary tumors,
and normal samples. Measured under strict 5-fold cross-validation protocols,
MobileDenseAttn provides a training accuracy of 99.75%, a testing accuracy of
98.35%, and a stable F1 score of 0.9835 (95% CI: 0.9743 to 0.9920). The
extensive validation shows the stability of the model, and the comparative
analysis proves that it is a great advancement over the baseline models (VGG19,
DenseNet201, MobileNetV2) with a +3.67% accuracy increase and a 39.3% decrease
in training time compared to VGG19. The GradCAM heatmaps clearly show
tumor-affected areas, offering clinically significant localization and
improving interpretability. These findings position MobileDenseAttn as an
efficient, high performance, interpretable model with a high probability of
becoming a clinically practical tool in identifying brain tumors in the real
world.

</details>


### [45] [Can VLMs Recall Factual Associations From Visual References?](https://arxiv.org/abs/2508.18297)
*Dhananjay Ashok,Ashutosh Chaubey,Hirona J. Arai,Jonathan May,Jesse Thomason*

Main category: cs.CV

TL;DR: VLMs 在视觉理解方面存在缺陷，可以通过检测模型内部状态来解决，提高视觉问答的准确率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: VLMs在提供文本参考时可以回忆起事实关联，但当参考是视觉时，它们的能力会大大降低。这表明 VLMs 难以将其内部知识与图像表示联系起来。

Method: 通过对照研究，我们识别了视觉语言模型(VLMs)在多模态 grounding 方面存在的系统性缺陷, 并使用探针来检测 VLM 何时无法正确回答需要理解多模态输入的问题。

Result: 当 VLM 依赖图像表示时，回忆事实知识的能力减半。对内部状态的探测在标记 VLM 响应不可靠的情况下，准确率超过 92%。在视觉问答任务中，探针将覆盖率提高了 7.87%（绝对值），同时将误差风险降低了 0.9%（绝对值）。

Conclusion: VLMs在通过视觉参考进行事实关联时能力显著下降，这是一个可以通过检测模型内部状态来解决的问题。

Abstract: Through a controlled study, we identify a systematic deficiency in the
multimodal grounding of Vision Language Models (VLMs). While VLMs can recall
factual associations when provided a textual reference to an entity; their
ability to do so is significantly diminished when the reference is visual
instead. Forcing VLMs to rely on image representations of an entity halves
their ability to recall factual knowledge, suggesting that VLMs struggle to
link their internal knowledge of an entity with its image representation. We
show that such linking failures are correlated with the expression of distinct
patterns in model internal states, and that probes on these internal states
achieve over 92% accuracy at flagging cases where the VLM response is
unreliable. These probes can be applied, without retraining, to identify when a
VLM will fail to correctly answer a question that requires an understanding of
multimodal input. When used to facilitate selective prediction on a visual
question answering task, the probes increase coverage by 7.87% (absolute) while
also reducing the risk of error by 0.9% (absolute). Addressing the systematic,
detectable deficiency is an important avenue in language grounding, and we
provide informed recommendations for future directions.

</details>


### [46] [SERES: Semantic-aware neural reconstruction from sparse views](https://arxiv.org/abs/2508.18314)
*Bo Xu,Yuhu Guo,Yuchao Wang,Wenting Wang,Yeung Yam,Charlie C. L. Wang,Xinyi Le*

Main category: cs.CV

TL;DR: This paper introduces a semantic-aware neural reconstruction method that improves 3D model reconstruction from sparse images by addressing radiance and shape ambiguity, achieving significant error reduction on the DTU dataset.


<details>
  <summary>Details</summary>
Motivation: Reconstructing 3D high-fidelity models from sparse images suffers from radiance ambiguity due to mismatched features.

Method: A semantic-aware neural reconstruction method is proposed, enriching neural implicit representations with patch-based semantic logits and a geometric primitive mask regularization.

Result: The proposed method reduces average Chamfer distance by 44% for SparseNeuS and 20% for VolRecon on the DTU dataset. As a plugin, it reduces average error by 69% for NeuS and 68% for Neuralangelo on the DTU dataset.

Conclusion: The semantic-aware neural reconstruction method significantly reduces reconstruction error on the DTU dataset, demonstrating its effectiveness as a standalone method and as a plugin for existing dense reconstruction baselines.

Abstract: We propose a semantic-aware neural reconstruction method to generate 3D
high-fidelity models from sparse images. To tackle the challenge of severe
radiance ambiguity caused by mismatched features in sparse input, we enrich
neural implicit representations by adding patch-based semantic logits that are
optimized together with the signed distance field and the radiance field. A
novel regularization based on the geometric primitive masks is introduced to
mitigate shape ambiguity. The performance of our approach has been verified in
experimental evaluation. The average chamfer distances of our reconstruction on
the DTU dataset can be reduced by 44% for SparseNeuS and 20% for VolRecon. When
working as a plugin for those dense reconstruction baselines such as NeuS and
Neuralangelo, the average error on the DTU dataset can be reduced by 69% and
68% respectively.

</details>


### [47] [Automated Landfill Detection Using Deep Learning: A Comparative Study of Lightweight and Custom Architectures with the AerialWaste Dataset](https://arxiv.org/abs/2508.18315)
*Nowshin Sharmily,Rusab Sarmun,Muhammad E. H. Chowdhury,Mir Hamidul Hussain,Saad Bin Abul Kashem,Molla E Majid,Amith Khandakar*

Main category: cs.CV

TL;DR: This study uses lightweight deep learning models and an ensemble model to perform binary classification on the AerialWaste Dataset for illegal landfill detection, achieving high accuracy, precision, sensitivity, F1 score, and specificity.


<details>
  <summary>Details</summary>
Motivation: Illegal landfills are posing as a hazardous threat to people all over the world. Deep learning can play a significant role in identifying these landfills while saving valuable time, manpower and resources.  AerialWaste Dataset is a large collection of 10434 images of Lombardy region of Italy, professionally curated, diverse and high-quality images which makes it particularly suitable for scalable and impactful research.

Method: Mobilenetv2, Googlenet, Densenet, MobileVit and other lightweight deep learning models were used to train and validate the dataset as they achieved significant success with less overfitting.The best performing models were combined and came up with an ensemble model. With the help of ensemble and fusion technique

Result: found complex and heavy models to be prone to overfitting and memorizing training data instead of learning patterns. Therefore, chose lightweight simpler models which could leverage general features from the dataset.substantial improvement in the performance using some of these models

Conclusion: Binary classification could be performed on this dataset with 92.33% accuracy, 92.67% precision, 92.33% sensitivity, 92.41% F1 score and 92.71% specificity.

Abstract: Illegal landfills are posing as a hazardous threat to people all over the
world. Due to the arduous nature of manually identifying the location of
landfill, many landfills go unnoticed by authorities and later cause dangerous
harm to people and environment. Deep learning can play a significant role in
identifying these landfills while saving valuable time, manpower and resources.
Despite being a burning concern, good quality publicly released datasets for
illegal landfill detection are hard to find due to security concerns. However,
AerialWaste Dataset is a large collection of 10434 images of Lombardy region of
Italy. The images are of varying qualities, collected from three different
sources: AGEA Orthophotos, WorldView-3, and Google Earth. The dataset contains
professionally curated, diverse and high-quality images which makes it
particularly suitable for scalable and impactful research. As we trained
several models to compare results, we found complex and heavy models to be
prone to overfitting and memorizing training data instead of learning patterns.
Therefore, we chose lightweight simpler models which could leverage general
features from the dataset. In this study, Mobilenetv2, Googlenet, Densenet,
MobileVit and other lightweight deep learning models were used to train and
validate the dataset as they achieved significant success with less
overfitting. As we saw substantial improvement in the performance using some of
these models, we combined the best performing models and came up with an
ensemble model. With the help of ensemble and fusion technique, binary
classification could be performed on this dataset with 92.33% accuracy, 92.67%
precision, 92.33% sensitivity, 92.41% F1 score and 92.71% specificity.

</details>


### [48] [Structures Meet Semantics: Multimodal Fusion via Graph Contrastive Learning](https://arxiv.org/abs/2508.18322)
*Jiangfeng Sun,Sihao He,Zhonghong Ou,Meina Song*

Main category: cs.CV

TL;DR: 提出了SSU框架，通过整合模态特定结构信息和跨模态语义基础来增强多模态表示，并在CMU-MOSI和CMU-MOSEI数据集上取得了SOTA结果，同时降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态融合方法通常忽略模态特定的结构依赖性和语义不对齐，限制了它们的质量、可解释性和鲁棒性。

Method: 提出了一种名为结构-语义统一器 (SSU) 的新框架，该框架系统地整合了模态特定结构信息和跨模态语义基础，以增强多模态表示。

Result: SSU在两个广泛使用的基准数据集CMU-MOSI和CMU-MOSEI上始终如一地实现了最先进的性能，同时与先前的方法相比，显着降低了计算开销。

Conclusion: SSU在CMU-MOSI和CMU-MOSEI上实现了最佳性能，同时显著降低了计算开销，并通过语义关联的交互捕获细微的情感模式，具有良好的可解释性。

Abstract: Multimodal sentiment analysis (MSA) aims to infer emotional states by
effectively integrating textual, acoustic, and visual modalities. Despite
notable progress, existing multimodal fusion methods often neglect
modality-specific structural dependencies and semantic misalignment, limiting
their quality, interpretability, and robustness. To address these challenges,
we propose a novel framework called the Structural-Semantic Unifier (SSU),
which systematically integrates modality-specific structural information and
cross-modal semantic grounding for enhanced multimodal representations.
Specifically, SSU dynamically constructs modality-specific graphs by leveraging
linguistic syntax for text and a lightweight, text-guided attention mechanism
for acoustic and visual modalities, thus capturing detailed intra-modal
relationships and semantic interactions. We further introduce a semantic
anchor, derived from global textual semantics, that serves as a cross-modal
alignment hub, effectively harmonizing heterogeneous semantic spaces across
modalities. Additionally, we develop a multiview contrastive learning objective
that promotes discriminability, semantic consistency, and structural coherence
across intra- and inter-modal views. Extensive evaluations on two widely used
benchmark datasets, CMU-MOSI and CMU-MOSEI, demonstrate that SSU consistently
achieves state-of-the-art performance while significantly reducing
computational overhead compared to prior methods. Comprehensive qualitative
analyses further validate SSU's interpretability and its ability to capture
nuanced emotional patterns through semantically grounded interactions.

</details>


### [49] [FastAvatar: Instant 3D Gaussian Splatting for Faces from Single Unconstrained Poses](https://arxiv.org/abs/2508.18389)
*Hao Liang,Zhixuan Ge,Ashish Tiwari,Soumendu Majee,G. M. Dilshan Godaliyadda,Ashok Veeraraghavan,Guha Balakrishnan*

Main category: cs.CV

TL;DR: FastAvatar is a pose-invariant, feed-forward framework that generates a 3D Gaussian Splatting model from a single face image in near-instant time. It outperforms existing methods in speed and quality, and supports real-time identity interpolation and attribute editing.


<details>
  <summary>Details</summary>
Motivation: To generate a 3D Gaussian Splatting (3DGS) model from a single face image from an arbitrary pose in near-instant time.

Method: FastAvatar encodes the input face image into an identity-specific and pose-invariant latent embedding, and decodes this embedding to predict residuals to the structural and appearance parameters of each Gaussian in the template 3DGS model.

Result: FastAvatar significantly outperforms existing feed-forward face 3DGS methods in reconstruction quality, and runs 1000x faster than per-face optimization methods. FastAvatar's novel latent space design supports real-time identity interpolation and attribute editing which is not possible with any existing feed-forward 3DGS face generation framework.

Conclusion: FastAvatar's combination of excellent reconstruction quality and speed expands the scope of 3DGS for photorealistic avatar applications in consumer and interactive systems.

Abstract: We present FastAvatar, a pose-invariant, feed-forward framework that can
generate a 3D Gaussian Splatting (3DGS) model from a single face image from an
arbitrary pose in near-instant time (<10ms). FastAvatar uses a novel
encoder-decoder neural network design to achieve both fast fitting and identity
preservation regardless of input pose. First, FastAvatar constructs a 3DGS face
``template'' model from a training dataset of faces with multi-view captures.
Second, FastAvatar encodes the input face image into an identity-specific and
pose-invariant latent embedding, and decodes this embedding to predict
residuals to the structural and appearance parameters of each Gaussian in the
template 3DGS model. By only inferring residuals in a feed-forward fashion,
model inference is fast and robust. FastAvatar significantly outperforms
existing feed-forward face 3DGS methods (e.g., GAGAvatar) in reconstruction
quality, and runs 1000x faster than per-face optimization methods (e.g.,
FlashAvatar, GaussianAvatars and GASP). In addition, FastAvatar's novel latent
space design supports real-time identity interpolation and attribute editing
which is not possible with any existing feed-forward 3DGS face generation
framework. FastAvatar's combination of excellent reconstruction quality and
speed expands the scope of 3DGS for photorealistic avatar applications in
consumer and interactive systems.

</details>


### [50] [Securing Face and Fingerprint Templates in Humanitarian Biometric Systems](https://arxiv.org/abs/2508.18415)
*Giuseppe Stragapede,Sam Merrick,Vedrana Krivokuća Hahn,Justin Sukaitis,Vincent Graf Narbel*

Main category: cs.CV

TL;DR: 本文提出了一种适用于人道主义环境的移动生物识别系统，并评估了PolyProtect在面部和指纹识别中的有效性。


<details>
  <summary>Details</summary>
Motivation: 在人道主义和紧急情况下，生物识别技术的使用可以显著提高运营效率，但它给数据主体带来了风险，而在脆弱的环境中，这些风险会加剧。

Method: 提出了一种移动生物识别系统，该系统实现了适用于人道主义和紧急情况的生物识别模板保护（BTP）方案。该方案使用PolyProtect方法，该方法适用于神经网络面部嵌入。

Result: PolyProtect在验证和识别精度、不可逆性和不可链接性方面进行了评估。将其应用于使用EdgeFace提取的面部嵌入，EdgeFace是一种新型的最先进的高效特征提取器，该提取器来自埃塞俄比亚的人道主义领域项目的真实面部数据集。

Conclusion: PolyProtect在人道主义援助场景中表现出潜力，首次在身份识别和指纹生物识别中进行了评估，实验结果很有希望，代码计划发布。

Abstract: In humanitarian and emergency scenarios, the use of biometrics can
dramatically improve the efficiency of operations, but it poses risks for the
data subjects, which are exacerbated in contexts of vulnerability. To address
this, we present a mobile biometric system implementing a biometric template
protection (BTP) scheme suitable for these scenarios. After rigorously
formulating the functional, operational, and security and privacy requirements
of these contexts, we perform a broad comparative analysis of the BTP
landscape. PolyProtect, a method designed to operate on neural network face
embeddings, is identified as the most suitable method due to its effectiveness,
modularity, and lightweight computational burden. We evaluate PolyProtect in
terms of verification and identification accuracy, irreversibility, and
unlinkability, when this BTP method is applied to face embeddings extracted
using EdgeFace, a novel state-of-the-art efficient feature extractor, on a
real-world face dataset from a humanitarian field project in Ethiopia.
Moreover, as PolyProtect promises to be modality-independent, we extend its
evaluation to fingerprints. To the best of our knowledge, this is the first
time that PolyProtect has been evaluated for the identification scenario and
for fingerprint biometrics. Our experimental results are promising, and we plan
to release our code

</details>


### [51] [Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?](https://arxiv.org/abs/2508.18421)
*Fatemeh Ziaeetabar*

Main category: cs.CV

TL;DR: 视觉基础模型在关系推理方面存在不足。论文提出通过引入动态关系图来增强视觉基础模型，并在多个任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型在需要对实体、角色和时空关系进行显式推理的任务中存在局限性。关系能力对于细粒度的人类活动识别、以自我为中心的视频理解和多模态医学图像分析至关重要。

Method: 通过将轻量级的、上下文自适应的图推理模块添加到视觉基础模型中，增强了模型的能力。

Result: 通过在人类操作动作识别和脑肿瘤分割等任务中的交叉领域证据表明，与仅使用视觉基础模型相比，增强后的模型提高了细粒度的语义保真度、分布外鲁棒性、可解释性和计算效率。通过对语义节点进行稀疏推理，这种混合方法还实现了良好的内存和硬件效率。

Conclusion: 下一代视觉基础模型应包含显式关系接口，以动态关系图的形式实例化。未来的研究方向包括学习动态图构建、多层次关系推理、跨模态融合以及直接探测结构化视觉任务中关系能力的评估协议。

Abstract: Vision foundation models (FMs) have become the predominant architecture in
computer vision, providing highly transferable representations learned from
large-scale, multimodal corpora. Nonetheless, they exhibit persistent
limitations on tasks that require explicit reasoning over entities, roles, and
spatio-temporal relations. Such relational competence is indispensable for
fine-grained human activity recognition, egocentric video understanding, and
multimodal medical image analysis, where spatial, temporal, and semantic
dependencies are decisive for performance. We advance the position that
next-generation FMs should incorporate explicit relational interfaces,
instantiated as dynamic relational graphs (graphs whose topology and edge
semantics are inferred from the input and task context). We illustrate this
position with cross-domain evidence from recent systems in human manipulation
action recognition and brain tumor segmentation, showing that augmenting FMs
with lightweight, context-adaptive graph-reasoning modules improves
fine-grained semantic fidelity, out of distribution robustness,
interpretability, and computational efficiency relative to FM only baselines.
Importantly, by reasoning sparsely over semantic nodes, such hybrids also
achieve favorable memory and hardware efficiency, enabling deployment under
practical resource constraints. We conclude with a targeted research agenda for
FM graph hybrids, prioritizing learned dynamic graph construction, multi-level
relational reasoning (e.g., part object scene in activity understanding, or
region organ in medical imaging), cross-modal fusion, and evaluation protocols
that directly probe relational competence in structured vision tasks.

</details>


### [52] [LPLC: A Dataset for License Plate Legibility Classification](https://arxiv.org/abs/2508.18425)
*Lucas Wojcik,Gabriel E. Lima,Valfride Nascimento,Eduil Nascimento Jr.,Rayson Laroca,David Menotti*

Main category: cs.CV

TL;DR: A new dataset (LPLC) is introduced for legibility classification of license plates. The benchmark classification task reveals the difficulty of determining LP image quality, suggesting the need for more research.


<details>
  <summary>Details</summary>
Motivation: ALPR faces a major challenge when dealing with illegible license plates (LPs). To optimize model performance and computational efficiency, image pre-processing should be applied selectively to cases that require enhanced legibility.

Method: A classification task using three image recognition networks is proposed to determine whether an LP image is good enough, requires super-resolution, or is completely unrecoverable.

Result: The overall F1 score remained below 80% for all three baseline models (ViT, ResNet, and YOLO).

Conclusion: The task is difficult, and further research is needed.

Abstract: Automatic License Plate Recognition (ALPR) faces a major challenge when
dealing with illegible license plates (LPs). While reconstruction methods such
as super-resolution (SR) have emerged, the core issue of recognizing these
low-quality LPs remains unresolved. To optimize model performance and
computational efficiency, image pre-processing should be applied selectively to
cases that require enhanced legibility. To support research in this area, we
introduce a novel dataset comprising 10,210 images of vehicles with 12,687
annotated LPs for legibility classification (the LPLC dataset). The images span
a wide range of vehicle types, lighting conditions, and camera/image quality
levels. We adopt a fine-grained annotation strategy that includes vehicle- and
LP-level occlusions, four legibility categories (perfect, good, poor, and
illegible), and character labels for three categories (excluding illegible
LPs). As a benchmark, we propose a classification task using three image
recognition networks to determine whether an LP image is good enough, requires
super-resolution, or is completely unrecoverable. The overall F1 score, which
remained below 80% for all three baseline models (ViT, ResNet, and YOLO),
together with the analyses of SR and LP recognition methods, highlights the
difficulty of the task and reinforces the need for further research. The
proposed dataset is publicly available at
https://github.com/lmlwojcik/lplc-dataset.

</details>


### [53] [CLARIFY: A Specialist-Generalist Framework for Accurate and Lightweight Dermatological Visual Question Answering](https://arxiv.org/abs/2508.18430)
*Aranya Saha,Tanvir Ahmed Khan,Ismam Nur Swapnil,Mohammad Ariful Haque*

Main category: cs.CV

TL;DR: CLARIFY, a Specialist-Generalist framework, improves diagnostic accuracy and computational efficiency in dermatological VQA.


<details>
  <summary>Details</summary>
Motivation: General-purpose VLMs can limit specialized diagnostic accuracy, and their large size poses substantial inference costs for real-world clinical deployment.

Method: CLARIFY combines a lightweight, domain-trained image classifier (the Specialist) and a compressed conversational VLM (the Generalist), enhanced by a knowledge graph-based retrieval module.

Result: CLARIFY achieves an 18% improvement in diagnostic accuracy, while reducing the average VRAM requirement and latency by at least 20% and 5%, respectively.

Conclusion: A Specialist-Generalist system provides a practical and powerful paradigm for building lightweight, trustworthy, and clinically viable AI systems.

Abstract: Vision-language models (VLMs) have shown significant potential for medical
tasks; however, their general-purpose nature can limit specialized diagnostic
accuracy, and their large size poses substantial inference costs for real-world
clinical deployment. To address these challenges, we introduce CLARIFY, a
Specialist-Generalist framework for dermatological visual question answering
(VQA). CLARIFY combines two components: (i) a lightweight, domain-trained image
classifier (the Specialist) that provides fast and highly accurate diagnostic
predictions, and (ii) a powerful yet compressed conversational VLM (the
Generalist) that generates natural language explanations to user queries. In
our framework, the Specialist's predictions directly guide the Generalist's
reasoning, focusing it on the correct diagnostic path. This synergy is further
enhanced by a knowledge graph-based retrieval module, which grounds the
Generalist's responses in factual dermatological knowledge, ensuring both
accuracy and reliability. This hierarchical design not only reduces diagnostic
errors but also significantly improves computational efficiency. Experiments on
our curated multimodal dermatology dataset demonstrate that CLARIFY achieves an
18\% improvement in diagnostic accuracy over the strongest baseline, a
fine-tuned, uncompressed single-line VLM, while reducing the average VRAM
requirement and latency by at least 20\% and 5\%, respectively. These results
indicate that a Specialist-Generalist system provides a practical and powerful
paradigm for building lightweight, trustworthy, and clinically viable AI
systems.

</details>


### [54] [VQualA 2025 Challenge on Face Image Quality Assessment: Methods and Results](https://arxiv.org/abs/2508.18445)
*Sizhuo Ma,Wei-Ting Chen,Qiang Gao,Jian Wang,Chris Wei Zhou,Wei Sun,Weixia Zhang,Linhan Cao,Jun Jia,Xiangyang Zhu,Dandan Zhu,Xiongkuo Min,Guangtao Zhai,Baoying Chen,Xiongwei Xiao,Jishen Zeng,Wei Wu,Tiexuan Lou,Yuchen Tan,Chunyi Song,Zhiwei Xu,MohammadAli Hamidi,Hadi Amirpour,Mingyin Bai,Jiawang Du,Zhenyu Jiang,Zilong Lu,Ziguan Cui,Zongliang Gan,Xinpeng Li,Shiqi Jiang,Chenhui Li,Changbo Wang,Weijun Yuan,Zhan Li,Yihang Chen,Yifan Deng,Ruting Deng,Zhanglu Chen,Boyang Yao,Shuling Zheng,Feng Zhang,Zhiheng Fu,Abhishek Joshi,Aman Agarwal,Rakhil Immidisetti,Ajay Narasimha Mopidevi,Vishwajeet Shukla,Hao Yang,Ruikun Zhang,Liyuan Pan,Kaixin Deng,Hang Ouyang,Fan yang,Zhizun Luo,Zhuohang Shi,Songning Lai,Weilin Ruan,Yutao Yue*

Main category: cs.CV

TL;DR: Organized a face image quality assessment challenge (VQualA 2025) at ICCV 2025, attracting 127 participants to develop lightweight models for predicting MOS on degraded face images. The challenge evaluated submissions using correlation metrics on a dataset of in-the-wild face images and this report summarizes the methodologies and findings.


<details>
  <summary>Details</summary>
Motivation: Real-world conditions frequently introduce degradations such as noise, blur, and compression artifacts, affecting overall image quality and hindering subsequent tasks.

Method: Participants created lightweight and efficient models (limited to 0.5 GFLOPs and 5 million parameters) for the prediction of Mean Opinion Scores (MOS) on face images with arbitrary resolutions and realistic degradations.

Result: This challenge attracted 127 participants, with 1519 final submissions. Submissions underwent comprehensive evaluations through correlation metrics on a dataset of in-the-wild face images.

Conclusion: This report summarizes the methodologies and findings for advancing the development of practical FIQA approaches.

Abstract: Face images play a crucial role in numerous applications; however, real-world
conditions frequently introduce degradations such as noise, blur, and
compression artifacts, affecting overall image quality and hindering subsequent
tasks. To address this challenge, we organized the VQualA 2025 Challenge on
Face Image Quality Assessment (FIQA) as part of the ICCV 2025 Workshops.
Participants created lightweight and efficient models (limited to 0.5 GFLOPs
and 5 million parameters) for the prediction of Mean Opinion Scores (MOS) on
face images with arbitrary resolutions and realistic degradations. Submissions
underwent comprehensive evaluations through correlation metrics on a dataset of
in-the-wild face images. This challenge attracted 127 participants, with 1519
final submissions. This report summarizes the methodologies and findings for
advancing the development of practical FIQA approaches.

</details>


### [55] [Context-Aware Zero-Shot Anomaly Detection in Surveillance Using Contrastive and Predictive Spatiotemporal Modeling](https://arxiv.org/abs/2508.18463)
*Md. Rashid Shahriar Khan,Md. Abrar Hasan,Mohammod Tareq Aziz Justice*

Main category: cs.CV

TL;DR: A context-aware zero-shot anomaly detection framework identifies abnormal events without anomaly examples during training by combining TimeSformer, DPC, and CLIP to model spatiotemporal dynamics and semantic context.


<details>
  <summary>Details</summary>
Motivation: Detecting anomalies in surveillance footage is inherently challenging due to their unpredictable and context-dependent nature.

Method: The proposed hybrid architecture combines TimeSformer, DPC, and CLIP to model spatiotemporal dynamics and semantic context.

Result: Integrating predictive modeling with vision-language understanding, the system can generalize to previously unseen behaviors in complex environments.

Conclusion: This framework bridges the gap between temporal reasoning and semantic context in zero-shot anomaly detection for surveillance.

Abstract: Detecting anomalies in surveillance footage is inherently challenging due to
their unpredictable and context-dependent nature. This work introduces a novel
context-aware zero-shot anomaly detection framework that identifies abnormal
events without exposure to anomaly examples during training. The proposed
hybrid architecture combines TimeSformer, DPC, and CLIP to model spatiotemporal
dynamics and semantic context. TimeSformer serves as the vision backbone to
extract rich spatial-temporal features, while DPC forecasts future
representations to identify temporal deviations. Furthermore, a CLIP-based
semantic stream enables concept-level anomaly detection through
context-specific text prompts. These components are jointly trained using
InfoNCE and CPC losses, aligning visual inputs with their temporal and semantic
representations. A context-gating mechanism further enhances decision-making by
modulating predictions with scene-aware cues or global video features. By
integrating predictive modeling with vision-language understanding, the system
can generalize to previously unseen behaviors in complex environments. This
framework bridges the gap between temporal reasoning and semantic context in
zero-shot anomaly detection for surveillance. The code for this research has
been made available at
https://github.com/NK-II/Context-Aware-ZeroShot-Anomaly-Detection-in-Surveillance.

</details>


### [56] [DoGFlow: Self-Supervised LiDAR Scene Flow via Cross-Modal Doppler Guidance](https://arxiv.org/abs/2508.18506)
*Ajinkya Khoche,Qingwen Zhang,Yixi Cai,Sina Sharif Mansouri,Patric Jensfelt*

Main category: cs.CV

TL;DR: 提出了一种新的自监督框架DoGFlow，该框架可以恢复完整的3D对象运动，以进行LiDAR场景流估计，而无需任何手动ground truth注释。


<details>
  <summary>Details</summary>
Motivation: 精确的3D场景流估计对于自主系统安全地在动态环境中导航至关重要，但是创建必要的大规模手动注释数据集仍然是开发鲁棒感知模型的重大瓶颈。当前的自监督方法难以与完全监督方法的性能相匹配，尤其是在具有挑战性的远距离和不利的天气情况下，而监督方法由于依赖昂贵的人工标注而无法扩展。

Method: cross-modal标签转换方法，DoGFlow直接从4D雷达多普勒测量中实时计算运动伪标签，并使用动态感知关联和模糊解析传播将其传输到LiDAR域。

Result: 在具有挑战性的MAN TruckScenes数据集上，

Conclusion: DoGFlow能显著优于现有的自监督方法，并通过支持LiDAR主干网络仅用10%的ground truth数据实现超过90%的完全监督性能，从而提高标签效率。

Abstract: Accurate 3D scene flow estimation is critical for autonomous systems to
navigate dynamic environments safely, but creating the necessary large-scale,
manually annotated datasets remains a significant bottleneck for developing
robust perception models. Current self-supervised methods struggle to match the
performance of fully supervised approaches, especially in challenging
long-range and adverse weather scenarios, while supervised methods are not
scalable due to their reliance on expensive human labeling. We introduce
DoGFlow, a novel self-supervised framework that recovers full 3D object motions
for LiDAR scene flow estimation without requiring any manual ground truth
annotations. This paper presents our cross-modal label transfer approach, where
DoGFlow computes motion pseudo-labels in real-time directly from 4D radar
Doppler measurements and transfers them to the LiDAR domain using dynamic-aware
association and ambiguity-resolved propagation. On the challenging MAN
TruckScenes dataset, DoGFlow substantially outperforms existing self-supervised
methods and improves label efficiency by enabling LiDAR backbones to achieve
over 90% of fully supervised performance with only 10% of the ground truth
data. For more details, please visit https://ajinkyakhoche.github.io/DogFlow/

</details>


### [57] [SAT-SKYLINES: 3D Building Generation from Satellite Imagery and Coarse Geometric Priors](https://arxiv.org/abs/2508.18531)
*Zhangyu Jin,Andrew Feng*

Main category: cs.CV

TL;DR: SatSkylines is a 3D building generation approach that takes satellite imagery and coarse geometric priors.


<details>
  <summary>Details</summary>
Motivation: Existing image-based 3D generation methods struggle to recover accurate building structures from satellite images alone, while 3D detailization methods tend to rely heavily on highly detailed voxel inputs and fail to produce satisfying results from simple priors such as cuboids.

Method: The key idea is to model the transformation from interpolated noisy coarse priors to detailed geometries.

Result: Developed Skylines-50K, a large-scale dataset of over 50,000 unique and stylized 3D building assets.

Conclusion: The model is effective and has strong generalization ability.

Abstract: We present SatSkylines, a 3D building generation approach that takes
satellite imagery and coarse geometric priors. Without proper geometric
guidance, existing image-based 3D generation methods struggle to recover
accurate building structures from the top-down views of satellite images alone.
On the other hand, 3D detailization methods tend to rely heavily on highly
detailed voxel inputs and fail to produce satisfying results from simple priors
such as cuboids. To address these issues, our key idea is to model the
transformation from interpolated noisy coarse priors to detailed geometries,
enabling flexible geometric control without additional computational cost. We
have further developed Skylines-50K, a large-scale dataset of over 50,000
unique and stylized 3D building assets in order to support the generations of
detailed building models. Extensive evaluations indicate the effectiveness of
our model and strong generalization ability.

</details>


### [58] [Adaptive Visual Navigation Assistant in 3D RPGs](https://arxiv.org/abs/2508.18539)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.CV

TL;DR: 本文提出检测可通行空间转换点（STP）并选择主要 STP（MSTP）的任务，并提出了一个两阶段深度学习管线作为基线，同时构建了一个数据集，为AI驱动的导航和关卡设计做出贡献。


<details>
  <summary>Details</summary>
Motivation: 在复杂的 3D 游戏环境中，玩家依靠视觉可供性来发现地图转换点。有效识别这些点对于客户端自动映射非常重要，并为评估地图提示呈现提供了客观基础。

Method: 本文介绍了一种两阶段深度学习管线，该管线首先使用 Faster R-CNN 检测潜在的 STP，然后使用轻量级 MSTP 选择器对它们进行排序，该选择器融合了局部和全局视觉特征。

Result: 我们的实验揭示了一个关键的权衡：虽然全网络微调在有足够数据的情况下产生卓越的 STP 检测，但仅适配器迁移在低数据场景和 MSTP 选择任务中明显更稳健有效。

Conclusion: 本文旨在通过定义新问题、提供基准管线和数据集，并提供对高效模型适应的初步见解，从而为未来的人工智能驱动的导航辅助工具和数据驱动的关卡设计工具做出贡献。

Abstract: In complex 3D game environments, players rely on visual affordances to spot
map transition points. Efficient identification of such points is important to
client-side auto-mapping, and provides an objective basis for evaluating map
cue presentation. In this work, we formalize the task of detecting traversable
Spatial Transition Points (STPs)-connectors between two sub regions-and
selecting the singular Main STP (MSTP), the unique STP that lies on the
designer-intended critical path toward the player's current macro-objective,
from a single game frame, proposing this as a new research focus. We introduce
a two-stage deep-learning pipeline that first detects potential STPs using
Faster R-CNN and then ranks them with a lightweight MSTP selector that fuses
local and global visual features. Both stages benefit from parameter-efficient
adapters, and we further introduce an optional retrieval-augmented fusion step.
Our primary goal is to establish the feasibility of this problem and set
baseline performance metrics. We validate our approach on a custom-built,
diverse dataset collected from five Action RPG titles. Our experiments reveal a
key trade-off: while full-network fine-tuning produces superior STP detection
with sufficient data, adapter-only transfer is significantly more robust and
effective in low-data scenarios and for the MSTP selection task. By defining
this novel problem, providing a baseline pipeline and dataset, and offering
initial insights into efficient model adaptation, we aim to contribute to
future AI-driven navigation aids and data-informed level-design tools.

</details>


### [59] [Wan-S2V: Audio-Driven Cinematic Video Generation](https://arxiv.org/abs/2508.18621)
*Xin Gao,Li Hu,Siqi Hu,Mingyang Huang,Chaonan Ji,Dechao Meng,Jinwei Qi,Penchong Qiao,Zhen Shen,Yafei Song,Ke Sun,Linrui Tian,Guangyuan Wang,Qi Wang,Zhongjian Wang,Jiayu Xiao,Sheng Xu,Bang Zhang,Peng Zhang,Xindi Zhang,Zhe Zhang,Jingren Zhou,Lian Zhuo*

Main category: cs.CV

TL;DR: 提出了一个名为 Wan-S2V 的音频驱动模型，用于电影级别的角色动画，优于现有技术，并在长视频生成和唇形同步方面具有多功能性。


<details>
  <summary>Details</summary>
Motivation: 现有技术在复杂的电影和电视制作中表现不佳，无法满足细致的角色互动、逼真的身体动作和动态的相机工作等需求。

Method: 提出了一个名为 Wan-S2V 的音频驱动模型，该模型建立在 Wan 的基础上。

Result: 实验结果表明，该方法明显优于 Hunyuan-Avatar 和 Omnihuman 等现有解决方案。此外，还通过其在长视频生成和精确视频唇形同步编辑中的应用，探索了该方法的多功能性。

Conclusion: 该模型在电影级别的角色动画中实现了增强的表达性和保真度，优于现有方法。

Abstract: Current state-of-the-art (SOTA) methods for audio-driven character animation
demonstrate promising performance for scenarios primarily involving speech and
singing. However, they often fall short in more complex film and television
productions, which demand sophisticated elements such as nuanced character
interactions, realistic body movements, and dynamic camera work. To address
this long-standing challenge of achieving film-level character animation, we
propose an audio-driven model, which we refere to as Wan-S2V, built upon Wan.
Our model achieves significantly enhanced expressiveness and fidelity in
cinematic contexts compared to existing approaches. We conducted extensive
experiments, benchmarking our method against cutting-edge models such as
Hunyuan-Avatar and Omnihuman. The experimental results consistently demonstrate
that our approach significantly outperforms these existing solutions.
Additionally, we explore the versatility of our method through its applications
in long-form video generation and precise video lip-sync editing.

</details>


### [60] [Decouple, Reorganize, and Fuse: A Multimodal Framework for Cancer Survival Prediction](https://arxiv.org/abs/2508.18632)
*Huayi Wang,Haochao Ying,Yuyang Xu,Qibo Qiu,Cheng Zhang,Danny Z. Chen,Ying Sun,Jian Wu*

Main category: cs.CV

TL;DR: This paper introduces a Decoupling-Reorganization-Fusion framework (DeReF) to address the limitations of existing cancer survival analysis methods. DeReF uses random feature reorganization and dynamic MoE fusion to improve feature combination diversity and information interaction among modalities, achieving better performance on liver cancer and TCGA datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods primarily focus on extracting different decoupled features of modalities and performing fusion operations such as concatenation, attention, and MoE-based (Mixture-of-Experts) fusion. However, these methods still face two key challenges: i) Fixed fusion schemes (concatenation and attention) can lead to model over-reliance on predefined feature combinations, limiting the dynamic fusion of decoupled features; ii) in MoE-based fusion methods, each expert network handles separate decoupled features, which limits information interaction among the decoupled features.

Method: We propose a novel Decoupling-Reorganization-Fusion framework (DeReF), which devises a random feature reorganization strategy between modalities decoupling and dynamic MoE fusion modules.  Additionally, we incorporate a regional cross-attention network within the modality decoupling module to improve the representation quality of decoupled features.

Result: The proposed Decoupling-Reorganization-Fusion framework (DeReF) increases the diversity of feature combinations and granularity, enhancing the generalization ability of the subsequent expert networks; it overcomes the problem of information closure and helps expert networks better capture information among decoupled features.

Conclusion: Extensive experimental results on our in-house Liver Cancer (LC) and three widely used TCGA public datasets confirm the effectiveness of our proposed method.

Abstract: Cancer survival analysis commonly integrates information across diverse
medical modalities to make survival-time predictions. Existing methods
primarily focus on extracting different decoupled features of modalities and
performing fusion operations such as concatenation, attention, and MoE-based
(Mixture-of-Experts) fusion. However, these methods still face two key
challenges: i) Fixed fusion schemes (concatenation and attention) can lead to
model over-reliance on predefined feature combinations, limiting the dynamic
fusion of decoupled features; ii) in MoE-based fusion methods, each expert
network handles separate decoupled features, which limits information
interaction among the decoupled features. To address these challenges, we
propose a novel Decoupling-Reorganization-Fusion framework (DeReF), which
devises a random feature reorganization strategy between modalities decoupling
and dynamic MoE fusion modules.Its advantages are: i) it increases the
diversity of feature combinations and granularity, enhancing the generalization
ability of the subsequent expert networks; ii) it overcomes the problem of
information closure and helps expert networks better capture information among
decoupled features. Additionally, we incorporate a regional cross-attention
network within the modality decoupling module to improve the representation
quality of decoupled features. Extensive experimental results on our in-house
Liver Cancer (LC) and three widely used TCGA public datasets confirm the
effectiveness of our proposed method. The code will be made publicly available.

</details>


### [61] [ROSE: Remove Objects with Side Effects in Videos](https://arxiv.org/abs/2508.18633)
*Chenxuan Miao,Yutong Feng,Jianshu Zeng,Zixiang Gao,Hantang Liu,Yunfeng Yan,Donglian Qi,Xi Chen,Bin Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: ROSE通过合成数据和扩散Transformer模型，有效移除了视频中的物体及其阴影和反射等副作用，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以消除物体阴影和反射等副作用，因为缺乏配对视频数据作为监督。

Method: 利用3D渲染引擎生成合成数据，构建全自动数据准备流程，并使用基于扩散Transformer的视频修复模型ROSE，通过引入额外的监督来预测受副作用影响的区域。

Result: ROSE在移除各种副作用方面取得了优异的性能，并在新的基准ROSE-Bench上进行了全面的评估。

Conclusion: ROSE在视频物体移除方面表现出色，尤其是在处理物体阴影和反射等副作用时，优于现有模型，并在真实场景中具有良好的泛化能力。

Abstract: Video object removal has achieved advanced performance due to the recent
success of video generative models. However, when addressing the side effects
of objects, e.g., their shadows and reflections, existing works struggle to
eliminate these effects for the scarcity of paired video data as supervision.
This paper presents ROSE, termed Remove Objects with Side Effects, a framework
that systematically studies the object's effects on environment, which can be
categorized into five common cases: shadows, reflections, light, translucency
and mirror. Given the challenges of curating paired videos exhibiting the
aforementioned effects, we leverage a 3D rendering engine for synthetic data
generation. We carefully construct a fully-automatic pipeline for data
preparation, which simulates a large-scale paired dataset with diverse scenes,
objects, shooting angles, and camera trajectories. ROSE is implemented as an
video inpainting model built on diffusion transformer. To localize all
object-correlated areas, the entire video is fed into the model for
reference-based erasing. Moreover, additional supervision is introduced to
explicitly predict the areas affected by side effects, which can be revealed
through the differential mask between the paired videos. To fully investigate
the model performance on various side effect removal, we presents a new
benchmark, dubbed ROSE-Bench, incorporating both common scenarios and the five
special side effects for comprehensive evaluation. Experimental results
demonstrate that ROSE achieves superior performance compared to existing video
object erasing models and generalizes well to real-world video scenarios. The
project page is https://rose2025-inpaint.github.io/.

</details>


### [62] [OwlCap: Harmonizing Motion-Detail for Video Captioning via HMD-270K and Caption Set Equivalence Reward](https://arxiv.org/abs/2508.18634)
*Chunlin Zhong,Qiuxia Hou,Zhangjun Zhou,Shuang Hao,Haonan Lu,Yanhao Zhang,He Tang,Xiang Bai*

Main category: cs.CV

TL;DR: Proposes solutions from two aspects: 1) Data aspect: Constructed the Harmonizing Motion-Detail 270K (HMD-270K) dataset. 2) Optimization aspect: Introduce the Caption Set Equivalence Reward (CSER) based on Group Relative Policy Optimization (GRPO).


<details>
  <summary>Details</summary>
Motivation: Existing methods often suffer from motion-detail imbalance, as models tend to overemphasize one aspect while neglecting the other. This imbalance results in incomplete captions, which in turn leads to a lack of consistency in video understanding and generation.

Method: We introduce the Caption Set Equivalence Reward (CSER) based on Group Relative Policy Optimization (GRPO).

Result: Developed OwlCap, a powerful video captioning multi-modal large language model (MLLM) with motion-detail balance.

Conclusion: OwlCap achieves significant improvements compared to baseline models on two benchmarks: the detail-focused VDC (+4.2 Acc) and the motion-focused DREAM-1K (+4.6 F1).

Abstract: Video captioning aims to generate comprehensive and coherent descriptions of
the video content, contributing to the advancement of both video understanding
and generation. However, existing methods often suffer from motion-detail
imbalance, as models tend to overemphasize one aspect while neglecting the
other. This imbalance results in incomplete captions, which in turn leads to a
lack of consistency in video understanding and generation. To address this
issue, we propose solutions from two aspects: 1) Data aspect: We constructed
the Harmonizing Motion-Detail 270K (HMD-270K) dataset through a two-stage
pipeline: Motion-Detail Fusion (MDF) and Fine-Grained Examination (FGE). 2)
Optimization aspect: We introduce the Caption Set Equivalence Reward (CSER)
based on Group Relative Policy Optimization (GRPO). CSER enhances completeness
and accuracy in capturing both motion and details through unit-to-set matching
and bidirectional validation. Based on the HMD-270K supervised fine-tuning and
GRPO post-training with CSER, we developed OwlCap, a powerful video captioning
multi-modal large language model (MLLM) with motion-detail balance.
Experimental results demonstrate that OwlCap achieves significant improvements
compared to baseline models on two benchmarks: the detail-focused VDC (+4.2
Acc) and the motion-focused DREAM-1K (+4.6 F1). The HMD-270K dataset and OwlCap
model will be publicly released to facilitate video captioning research
community advancements.

</details>


### [63] [Clustering-based Feature Representation Learning for Oracle Bone Inscriptions Detection](https://arxiv.org/abs/2508.18641)
*Ye Tao,Xinran Fu,Honglin Pang,Xi Yang,Chuntao Li*

Main category: cs.CV

TL;DR: A new clustering-based method improves OBI detection by using OBC font library data to enhance feature extraction.


<details>
  <summary>Details</summary>
Motivation: Automated detection of OBIs from rubbing images is challenging due to noise and cracks that limit the effectiveness of conventional detection networks.

Method: A novel clustering-based feature space representation learning method leveraging the OBC font library dataset as prior knowledge.

Result: Significant performance improvements on two OBIs detection datasets using Faster R-CNN, DETR, and Sparse R-CNN.

Conclusion: The proposed clustering-based feature space representation learning method improves the performance of OBI detection in Faster R-CNN, DETR, and Sparse R-CNN frameworks.

Abstract: Oracle Bone Inscriptions (OBIs), play a crucial role in understanding ancient
Chinese civilization. The automated detection of OBIs from rubbing images
represents a fundamental yet challenging task in digital archaeology, primarily
due to various degradation factors including noise and cracks that limit the
effectiveness of conventional detection networks. To address these challenges,
we propose a novel clustering-based feature space representation learning
method. Our approach uniquely leverages the Oracle Bones Character (OBC) font
library dataset as prior knowledge to enhance feature extraction in the
detection network through clustering-based representation learning. The method
incorporates a specialized loss function derived from clustering results to
optimize feature representation, which is then integrated into the total
network loss. We validate the effectiveness of our method by conducting
experiments on two OBIs detection dataset using three mainstream detection
frameworks: Faster R-CNN, DETR, and Sparse R-CNN. Through extensive
experimentation, all frameworks demonstrate significant performance
improvements.

</details>


### [64] [SFormer: SNR-guided Transformer for Underwater Image Enhancement from the Frequency Domain](https://arxiv.org/abs/2508.18664)
*Xin Tian,Yingtie Lei,Xiujun Zhang,Zimeng Li,Chi-Man Pun,Xuhang Chen*

Main category: cs.CV

TL;DR: 提出了一个名为SFormer的新的水下图像增强框架，该框架利用频域SNR先验和Transformer模块来提高图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的水下图像增强(UIE)方法通过将物理先验纳入深度神经网络而得到发展，特别是使用信噪比(SNR)先验来减少波长相关的衰减。然而，空间域SNR先验有两个局限性：(i)它们不能有效地分离跨通道干扰，(ii)它们在放大信息结构的同时抑制噪声方面提供的帮助有限。

Method: 提出使用频域中的SNR先验，将特征分解为幅度和相位谱以实现更好的通道调制。引入了傅里叶注意力SNR先验Transformer (FAST)，将频谱交互与SNR线索相结合，以突出关键频谱分量。此外，频率自适应Transformer (FAT)瓶颈使用门控注意力机制合并低频和高频分支，以提高感知质量。这些模块嵌入在统一的U型架构中，集成了传统的RGB流和SNR引导的分支，形成了SFormer。

Result: SFormer在PSNR和SSIM方面均优于现有技术。

Conclusion: SFormer在UIEB、EUVP和LSUI数据集上训练，性能超过了现有方法，PSNR提高了3.1 dB，SSIM提高了0.08，成功地恢复了水下场景中的颜色、纹理和对比度。

Abstract: Recent learning-based underwater image enhancement (UIE) methods have
advanced by incorporating physical priors into deep neural networks,
particularly using the signal-to-noise ratio (SNR) prior to reduce
wavelength-dependent attenuation. However, spatial domain SNR priors have two
limitations: (i) they cannot effectively separate cross-channel interference,
and (ii) they provide limited help in amplifying informative structures while
suppressing noise. To overcome these, we propose using the SNR prior in the
frequency domain, decomposing features into amplitude and phase spectra for
better channel modulation. We introduce the Fourier Attention SNR-prior
Transformer (FAST), combining spectral interactions with SNR cues to highlight
key spectral components. Additionally, the Frequency Adaptive Transformer (FAT)
bottleneck merges low- and high-frequency branches using a gated attention
mechanism to enhance perceptual quality. Embedded in a unified U-shaped
architecture, these modules integrate a conventional RGB stream with an
SNR-guided branch, forming SFormer. Trained on 4,800 paired images from UIEB,
EUVP, and LSUI, SFormer surpasses recent methods with a 3.1 dB gain in PSNR and
0.08 in SSIM, successfully restoring colors, textures, and contrast in
underwater scenes.

</details>


### [65] [Hierarchical Spatio-temporal Segmentation Network for Ejection Fraction Estimation in Echocardiography Videos](https://arxiv.org/abs/2508.18681)
*Dongfang Wang,Jian Yang,Yizhe Zhang,Tao Zhou*

Main category: cs.CV

TL;DR: This paper presents a new deep learning model for segmenting echocardiography videos to improve heart function measurement.


<details>
  <summary>Details</summary>
Motivation: Accurate assessment of cardiac structure and function through Ejection Fraction (EF) estimation in echocardiography videos is important, but existing methods don't perform well in EF estimation.

Method: The network employs a hierarchical design with convolutional networks for single-frame processing and Mamba architecture for spatio-temporal relationships. It also introduces a Spatio-temporal Cross Scan (STCS) module.

Result: The proposed network aims to improve EF estimation accuracy by synergizing local detail modeling with global dynamic perception and mitigating EF calculation biases.

Conclusion: The paper proposes a Hierarchical Spatio-temporal Segmentation Network to improve EF estimation accuracy.

Abstract: Automated segmentation of the left ventricular endocardium in
echocardiography videos is a key research area in cardiology. It aims to
provide accurate assessment of cardiac structure and function through Ejection
Fraction (EF) estimation. Although existing studies have achieved good
segmentation performance, their results do not perform well in EF estimation.
In this paper, we propose a Hierarchical Spatio-temporal Segmentation Network
(\ourmodel) for echocardiography video, aiming to improve EF estimation
accuracy by synergizing local detail modeling with global dynamic perception.
The network employs a hierarchical design, with low-level stages using
convolutional networks to process single-frame images and preserve details,
while high-level stages utilize the Mamba architecture to capture
spatio-temporal relationships. The hierarchical design balances single-frame
and multi-frame processing, avoiding issues such as local error accumulation
when relying solely on single frames or neglecting details when using only
multi-frame data. To overcome local spatio-temporal limitations, we propose the
Spatio-temporal Cross Scan (STCS) module, which integrates long-range context
through skip scanning across frames and positions. This approach helps mitigate
EF calculation biases caused by ultrasound image noise and other factors.

</details>


### [66] [Feature-Space Planes Searcher: A Universal Domain Adaptation Framework for Interpretability and Computational Efficiency](https://arxiv.org/abs/2508.18693)
*Zhitong Cheng,Yiran Jiang,Yulong Ge,Yufeng Li,Zhongheng Qin,Rongzhi Lin,Jianwei Ma*

Main category: cs.CV

TL;DR: FPS通过优化决策边界来进行域适应，优于微调，且更有效。


<details>
  <summary>Details</summary>
Motivation: 领域漂移会导致模型性能下降，现有的无监督领域自适应 (UDA) 方法主要依赖于微调特征提取器，但这种方法效率低下、可解释性降低，并且无法扩展到现代架构。

Method: 提出了一种新的领域自适应框架，即特征空间平面搜索器 (FPS)，通过利用这些几何模式来优化决策边界，同时保持特征编码器冻结。

Result: 在公共基准上的评估表明，FPS 实现了与最先进方法相当或更好的性能。FPS 可以有效地扩展多模态大型模型，并在蛋白质结构预测、遥感分类和地震检测等不同领域展示了通用性。

Conclusion: FPS在领域自适应任务中表现出色，为迁移学习提供了一个简单、有效且通用的范例。

Abstract: Domain shift, characterized by degraded model performance during transition
from labeled source domains to unlabeled target domains, poses a persistent
challenge for deploying deep learning systems. Current unsupervised domain
adaptation (UDA) methods predominantly rely on fine-tuning feature extractors -
an approach limited by inefficiency, reduced interpretability, and poor
scalability to modern architectures.
  Our analysis reveals that models pretrained on large-scale data exhibit
domain-invariant geometric patterns in their feature space, characterized by
intra-class clustering and inter-class separation, thereby preserving
transferable discriminative structures. These findings indicate that domain
shifts primarily manifest as boundary misalignment rather than feature
degradation.
  Unlike fine-tuning entire pre-trained models - which risks introducing
unpredictable feature distortions - we propose the Feature-space Planes
Searcher (FPS): a novel domain adaptation framework that optimizes decision
boundaries by leveraging these geometric patterns while keeping the feature
encoder frozen. This streamlined approach enables interpretative analysis of
adaptation while substantially reducing memory and computational costs through
offline feature extraction, permitting full-dataset optimization in a single
computation cycle.
  Evaluations on public benchmarks demonstrate that FPS achieves competitive or
superior performance to state-of-the-art methods. FPS scales efficiently with
multimodal large models and shows versatility across diverse domains including
protein structure prediction, remote sensing classification, and earthquake
detection. We anticipate FPS will provide a simple, effective, and
generalizable paradigm for transfer learning, particularly in domain adaptation
tasks. .

</details>


### [67] [A Novel Deep Hybrid Framework with Ensemble-Based Feature Optimization for Robust Real-Time Human Activity Recognition](https://arxiv.org/abs/2508.18695)
*Wasi Ullah,Yasir Noman Khalid,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 本文提出了一种用于人体活动识别(HAR)的优化混合深度学习框架，该框架具有高精度、低计算成本和良好的可扩展性，适合在边缘设备上实时部署。


<details>
  <summary>Details</summary>
Motivation: HAR系统仍然面临严峻的挑战，包括高计算成本、冗余特征和实时场景中有限的可扩展性。

Method: 该文提出了一个优化的混合深度学习框架，该框架集成了定制的InceptionV3、LSTM架构和一种新颖的基于集成的特征选择策略。

Result: 在强大的UCF-YouTube数据集上的实验结果表明，该文提出的方法表现良好。该方法实现了99.65%的识别精度，将特征减少到7个，并缩短了推理时间。

Conclusion: 该文提出的HAR系统具有轻量级和可扩展的特点，支持在Raspberry Pi等边缘设备上进行实时部署，从而在公共安全、辅助技术和自主监控系统等智能、资源感知环境中实现实际应用。

Abstract: Human Activity Recognition (HAR) plays a pivotal role in various
applications, including smart surveillance, healthcare, assistive technologies,
sports analytics, etc. However, HAR systems still face critical challenges,
including high computational costs, redundant features, and limited scalability
in real-time scenarios. An optimized hybrid deep learning framework is
introduced that integrates a customized InceptionV3, an LSTM architecture, and
a novel ensemble-based feature selection strategy. The proposed framework first
extracts spatial descriptors using the customized InceptionV3 model, which
captures multilevel contextual patterns, region homogeneity, and fine-grained
localization cues. The temporal dependencies across frames are then modeled
using LSTMs to effectively encode motion dynamics. Finally, an ensemble-based
genetic algorithm with Adaptive Dynamic Fitness Sharing and Attention (ADFSA)
is employed to select a compact and optimized feature set by dynamically
balancing objectives such as accuracy, redundancy, uniqueness, and complexity
reduction. Consequently, the selected feature subsets, which are both diverse
and discriminative, enable various lightweight machine learning classifiers to
achieve accurate and robust HAR in heterogeneous environments. Experimental
results on the robust UCF-YouTube dataset, which presents challenges such as
occlusion, cluttered backgrounds, motion dynamics, and poor illumination,
demonstrate good performance. The proposed approach achieves 99.65% recognition
accuracy, reduces features to as few as 7, and enhances inference time. The
lightweight and scalable nature of the HAR system supports real-time deployment
on edge devices such as Raspberry Pi, enabling practical applications in
intelligent, resource-aware environments, including public safety, assistive
technology, and autonomous monitoring systems.

</details>


### [68] [ColorGS: High-fidelity Surgical Scene Reconstruction with Colored Gaussian Splatting](https://arxiv.org/abs/2508.18696)
*Qun Ji,Peng Li,Mingqiang Wei*

Main category: cs.CV

TL;DR: ColorGS 是一种用于手术场景重建的新框架，它通过空间自适应颜色编码和增强的变形建模来提高重建质量和效率。


<details>
  <summary>Details</summary>
Motivation: 由于现有方法在捕获细微颜色变化和建模全局变形方面的局限性，从内窥镜视频中高保真地重建可变形组织仍然具有挑战性。虽然 3D 高斯溅射 (3DGS) 能够实现高效的动态重建，但其固定的每高斯颜色分配难以处理复杂的纹理，并且线性变形建模无法对一致的全局变形进行建模。

Method: 该论文提出了一种新颖的框架 ColorGS，该框架集成了空间自适应颜色编码和增强的变形建模，用于手术场景重建。它引入了彩色高斯图元，该图元采用具有可学习颜色参数的动态锚点来自适应地编码空间变化的纹理。此外，设计了一个增强变形模型 (EDM)，该模型将时间感知高斯基函数与可学习的时间独立变形相结合。

Result: 在 DaVinci 机器人手术视频和基准数据集（EndoNeRF、StereoMIS）上的大量实验表明，ColorGS 实现了最先进的性能，达到了 39.85 的 PSNR（比之前的基于 3DGS 的方法高 1.5）和卓越的 SSIM (97.25%)，同时保持了实时渲染效率。

Conclusion: ColorGS 通过平衡高保真度和计算实用性来推进手术场景重建，这对于术中指导和 AR/VR 应用至关重要。

Abstract: High-fidelity reconstruction of deformable tissues from endoscopic videos
remains challenging due to the limitations of existing methods in capturing
subtle color variations and modeling global deformations. While 3D Gaussian
Splatting (3DGS) enables efficient dynamic reconstruction, its fixed
per-Gaussian color assignment struggles with intricate textures, and linear
deformation modeling fails to model consistent global deformation. To address
these issues, we propose ColorGS, a novel framework that integrates spatially
adaptive color encoding and enhanced deformation modeling for surgical scene
reconstruction. First, we introduce Colored Gaussian Primitives, which employ
dynamic anchors with learnable color parameters to adaptively encode spatially
varying textures, significantly improving color expressiveness under complex
lighting and tissue similarity. Second, we design an Enhanced Deformation Model
(EDM) that combines time-aware Gaussian basis functions with learnable
time-independent deformations, enabling precise capture of both localized
tissue deformations and global motion consistency caused by surgical
interactions. Extensive experiments on DaVinci robotic surgery videos and
benchmark datasets (EndoNeRF, StereoMIS) demonstrate that ColorGS achieves
state-of-the-art performance, attaining a PSNR of 39.85 (1.5 higher than prior
3DGS-based methods) and superior SSIM (97.25\%) while maintaining real-time
rendering efficiency. Our work advances surgical scene reconstruction by
balancing high fidelity with computational practicality, critical for
intraoperative guidance and AR/VR applications.

</details>


### [69] [Class-wise Flooding Regularization for Imbalanced Image Classification](https://arxiv.org/abs/2508.18723)
*Hiroaki Aizawa,Yuta Naito,Kohei Fukuda*

Main category: cs.CV

TL;DR: This paper introduces class-wise flooding regularization to address the issue of performance degradation on minority classes when training neural networks on imbalanced datasets. 


<details>
  <summary>Details</summary>
Motivation: Training on imbalanced datasets leads to a model's prediction favoring majority classes, causing significant degradation in the recognition performance of minority classes.

Method: The paper proposes class-wise flooding regularization, an extension of flooding regularization applied at the class level. It assigns a class-specific flooding level based on class frequencies to suppress overfitting in majority classes while allowing sufficient learning for minority classes.

Result: The proposed method is validated on imbalanced image classification and improves the classification performance of minority classes and achieves better overall generalization.

Conclusion: The proposed class-wise flooding regularization improves the classification performance of minority classes and achieves better overall generalization compared to conventional flooding regularizations.

Abstract: The purpose of training neural networks is to achieve high generalization
performance on unseen inputs. However, when trained on imbalanced datasets, a
model's prediction tends to favor majority classes over minority classes,
leading to significant degradation in the recognition performance of minority
classes. To address this issue, we propose class-wise flooding regularization,
an extension of flooding regularization applied at the class level. Flooding is
a regularization technique that mitigates overfitting by preventing the
training loss from falling below a predefined threshold, known as the flooding
level, thereby discouraging memorization. Our proposed method assigns a
class-specific flooding level based on class frequencies. By doing so, it
suppresses overfitting in majority classes while allowing sufficient learning
for minority classes. We validate our approach on imbalanced image
classification. Compared to conventional flooding regularizations, our method
improves the classification performance of minority classes and achieves better
overall generalization.

</details>


### [70] [Flatness-aware Curriculum Learning via Adversarial Difficulty](https://arxiv.org/abs/2508.18726)
*Hiroaki Aizawa,Yoshikazu Hayashi*

Main category: cs.CV

TL;DR: 提出了一种新的课程学习方法，它使用对抗性难度度量来选择训练样本，并结合锐度感知最小化来提高模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 经验风险最小化训练的神经网络经常过度拟合，导致泛化能力差。课程学习（CL）通过选择基于难度的训练样本来解决这个问题。从优化角度来看，锐度感知最小化（SAM）通过寻找平坦的最小值来提高鲁棒性和泛化能力。然而，将 CL 与 SAM 结合起来并不简单。在平坦区域，损失值和梯度范数都趋于均匀变小，这使得评估样本难度和设计有效的课程变得困难。

Method: 提出了一种对抗性难度度量（ADM），通过利用在平面极小值训练的模型的鲁棒性来量化对抗脆弱性，并将其纳入基于 CL 的 SAM 训练中，以动态评估样本难度。

Result: 在图像分类、细粒度识别和领域泛化任务中的评估结果表明，该方法保留了 CL 和 SAM 的优势，同时优于现有的基于课程和平面感知训练策略。

Conclusion: 该方法在图像分类、细粒度识别和领域泛化任务中表现出色，优于现有的基于课程和平面感知训练策略，同时保留了 CL 和 SAM 的优势。

Abstract: Neural networks trained by empirical risk minimization often suffer from
overfitting, especially to specific samples or domains, which leads to poor
generalization. Curriculum Learning (CL) addresses this issue by selecting
training samples based on the difficulty. From the optimization perspective,
methods such as Sharpness-Aware Minimization (SAM) improve robustness and
generalization by seeking flat minima. However, combining CL with SAM is not
straightforward. In flat regions, both the loss values and the gradient norms
tend to become uniformly small, which makes it difficult to evaluate sample
difficulty and design an effective curriculum. To overcome this problem, we
propose the Adversarial Difficulty Measure (ADM), which quantifies adversarial
vulnerability by leveraging the robustness properties of models trained toward
flat minima. Unlike loss- or gradient-based measures, which become ineffective
as training progresses into flatter regions, ADM remains informative by
measuring the normalized loss gap between original and adversarial examples. We
incorporate ADM into CL-based training with SAM to dynamically assess sample
difficulty. We evaluated our approach on image classification tasks,
fine-grained recognition, and domain generalization. The results demonstrate
that our method preserves the strengths of both CL and SAM while outperforming
existing curriculum-based and flatness-aware training strategies.

</details>


### [71] [Are All Marine Species Created Equal? Performance Disparities in Underwater Object Detection](https://arxiv.org/abs/2508.18729)
*Melanie Wille,Tobias Fischer,Scarlett Raine*

Main category: cs.CV

TL;DR: This paper analyzes the challenges of underwater object detection, particularly the under-performance of certain marine species. It finds that foreground-background discrimination is a major issue and suggests focusing on algorithmic advances in localization modules to improve detection performance.


<details>
  <summary>Details</summary>
Motivation: Underwater object detection is critical for monitoring marine ecosystems but poses unique challenges, including degraded image quality, imbalanced class distribution, and distinct visual characteristics. Not every species is detected equally well, yet underlying causes remain unclear.

Method: We manipulate the DUO dataset to separate the object detection task into localization and classification and investigate the under-performance of the scallop class. Localization analysis using YOLO11 and TIDE finds that foreground-background discrimination is the most problematic stage regardless of data quantity. Classification experiments reveal persistent precision gaps even with balanced data, indicating intrinsic feature-based challenges beyond data scarcity and inter-class dependencies.

Result: Localization analysis using YOLO11 and TIDE finds that foreground-background discrimination is the most problematic stage regardless of data quantity. Classification experiments reveal persistent precision gaps even with balanced data, indicating intrinsic feature-based challenges beyond data scarcity and inter-class dependencies. We recommend imbalanced distributions when prioritizing precision, and balanced distributions when prioritizing recall.

Conclusion: Improving under-performing classes should focus on algorithmic advances, especially within localization modules. We publicly release our code and datasets.

Abstract: Underwater object detection is critical for monitoring marine ecosystems but
poses unique challenges, including degraded image quality, imbalanced class
distribution, and distinct visual characteristics. Not every species is
detected equally well, yet underlying causes remain unclear. We address two key
research questions: 1) What factors beyond data quantity drive class-specific
performance disparities? 2) How can we systematically improve detection of
under-performing marine species? We manipulate the DUO dataset to separate the
object detection task into localization and classification and investigate the
under-performance of the scallop class. Localization analysis using YOLO11 and
TIDE finds that foreground-background discrimination is the most problematic
stage regardless of data quantity. Classification experiments reveal persistent
precision gaps even with balanced data, indicating intrinsic feature-based
challenges beyond data scarcity and inter-class dependencies. We recommend
imbalanced distributions when prioritizing precision, and balanced
distributions when prioritizing recall. Improving under-performing classes
should focus on algorithmic advances, especially within localization modules.
We publicly release our code and datasets.

</details>


### [72] [Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vectorized Drawings](https://arxiv.org/abs/2508.18733)
*Feiwei Qin,Shichao Lu,Junhao Hou,Changmiao Wang,Meie Fang,Ligang Liu*

Main category: cs.CV

TL;DR: 提出了一种名为Drawing2CAD的框架，用于从2D工程图自动生成参数化CAD模型。


<details>
  <summary>Details</summary>
Motivation: 现有的方法与从2D工程图开始的传统工业工作流程存在根本差异。尽管从这些2D矢量图中自动生成参数化CAD模型是工程设计中的关键步骤，但尚未得到充分探索。

Method: 将CAD生成问题重新定义为序列到序列的学习问题，并提出了一个名为Drawing2CAD的框架，该框架包含三个关键技术组件：网络友好的矢量图元表示、双解码器transformer架构和软目标分布损失函数。

Result: 创建了一个配对的工程图和参数化CAD模型的数据集CAD-VGDrawing，并进行了彻底的实验，证明了该方法的有效性。

Conclusion: 本研究通过实验验证了Drawing2CAD的有效性。

Abstract: Computer-Aided Design (CAD) generative modeling is driving significant
innovations across industrial applications. Recent works have shown remarkable
progress in creating solid models from various inputs such as point clouds,
meshes, and text descriptions. However, these methods fundamentally diverge
from traditional industrial workflows that begin with 2D engineering drawings.
The automatic generation of parametric CAD models from these 2D vector drawings
remains underexplored despite being a critical step in engineering design. To
address this gap, our key insight is to reframe CAD generation as a
sequence-to-sequence learning problem where vector drawing primitives directly
inform the generation of parametric CAD operations, preserving geometric
precision and design intent throughout the transformation process. We propose
Drawing2CAD, a framework with three key technical components: a
network-friendly vector primitive representation that preserves precise
geometric information, a dual-decoder transformer architecture that decouples
command type and parameter generation while maintaining precise correspondence,
and a soft target distribution loss function accommodating inherent flexibility
in CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing,
a dataset of paired engineering drawings and parametric CAD models, and conduct
thorough experiments to demonstrate the effectiveness of our method. Code and
dataset are available at https://github.com/lllssc/Drawing2CAD.

</details>


### [73] [Improving Noise Robust Audio-Visual Speech Recognition via Router-Gated Cross-Modal Feature Fusion](https://arxiv.org/abs/2508.18734)
*DongHoon Lim,YoungChae Kim,Dong-Hyun Kim,Da-Hee Yang,Joon-Hyuk Chang*

Main category: cs.CV

TL;DR: This paper introduces a router-gated cross-modal feature fusion method for robust AVSR in noisy environments, which adaptively reweights audio and visual features. It achieves significant WER reduction compared to AV-HuBERT on LRS3.


<details>
  <summary>Details</summary>
Motivation: Robust audio-visual speech recognition (AVSR) in noisy environments remains challenging, as existing systems struggle to estimate audio reliability and dynamically adjust modality reliance.

Method: The paper proposes a router-gated cross-modal feature fusion framework that adaptively reweights audio and visual features based on token-level acoustic corruption scores. It uses an audio-visual feature fusion-based router to down-weight unreliable audio tokens and reinforces visual cues through gated cross-attention in each decoder layer.

Result: Experiments on LRS3 demonstrate that the proposed approach achieves an 16.51-42.67% relative reduction in word error rate compared to AV-HuBERT.

Conclusion: The proposed router-gated cross-modal feature fusion method achieves a significant reduction in word error rate compared to AV-HuBERT, demonstrating improved robustness under real-world acoustic noise. The router and gating mechanism both contribute to the improvement.

Abstract: Robust audio-visual speech recognition (AVSR) in noisy environments remains
challenging, as existing systems struggle to estimate audio reliability and
dynamically adjust modality reliance. We propose router-gated cross-modal
feature fusion, a novel AVSR framework that adaptively reweights audio and
visual features based on token-level acoustic corruption scores. Using an
audio-visual feature fusion-based router, our method down-weights unreliable
audio tokens and reinforces visual cues through gated cross-attention in each
decoder layer. This enables the model to pivot toward the visual modality when
audio quality deteriorates. Experiments on LRS3 demonstrate that our approach
achieves an 16.51-42.67% relative reduction in word error rate compared to
AV-HuBERT. Ablation studies confirm that both the router and gating mechanism
contribute to improved robustness under real-world acoustic noise.

</details>


### [74] [Rethinking Human-Object Interaction Evaluation for both Vision-Language Models and HOI-Specific Methods](https://arxiv.org/abs/2508.18753)
*Qinqian Lei,Bo Wang,Robby T. Tan*

Main category: cs.CV

TL;DR: A new HOI detection benchmark is introduced to better evaluate VLMs by using a multiple-answer multiple-choice format, addressing the limitations of existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing HOI benchmarks are not well-suited for evaluating large, generative VLMs due to their exact match evaluation protocols and inability to handle multiple valid interpretations.

Method: The paper reformulates HOI detection as a multiple-answer multiple-choice task with curated negative options to reduce ambiguity.

Result: The paper introduces a new benchmark and evaluation protocol for HOI detection.

Conclusion: The paper introduces a new benchmark for HOI detection that reformulates the task as a multiple-answer multiple-choice problem to address limitations of existing benchmarks when evaluating VLMs. The new benchmark and evaluation protocol enable direct comparison between VLMs and specialized HOI methods, providing new insights into HOI understanding.

Abstract: Prior human-object interaction (HOI) detection methods have integrated early
vision-language models (VLMs) such as CLIP, but only as supporting components
within their frameworks. In contrast, recent advances in large, generative VLMs
suggest that these models may already possess strong ability to understand
images involving HOI. This naturally raises an important question: can
general-purpose standalone VLMs effectively solve HOI detection, and how do
they compare with specialized HOI methods? Answering this requires a benchmark
that can accommodate both paradigms. However, existing HOI benchmarks such as
HICO-DET were developed before the emergence of modern VLMs, and their
evaluation protocols require exact matches to annotated HOI classes. This is
poorly aligned with the generative nature of VLMs, which often yield multiple
valid interpretations in ambiguous cases. For example, a static image may
capture a person mid-motion with a frisbee, which can plausibly be interpreted
as either "throwing" or "catching". When only "catching" is annotated, the
other, though equally plausible for the image, is marked incorrect when exact
matching is used. As a result, correct predictions might be penalized,
affecting both VLMs and HOI-specific methods. To avoid penalizing valid
predictions, we introduce a new benchmark that reformulates HOI detection as a
multiple-answer multiple-choice task, where each question includes only
ground-truth positive options and a curated set of negatives that are
constructed to reduce ambiguity (e.g., when "catching" is annotated, "throwing"
is not selected as a negative to avoid penalizing valid predictions). The
proposed evaluation protocol is the first of its kind for both VLMs and HOI
methods, enabling direct comparison and offering new insight into the current
state of progress in HOI understanding.

</details>


### [75] [Beyond the Textual: Generating Coherent Visual Options for MCQs](https://arxiv.org/abs/2508.18772)
*Wanqiang Wang,Longzhu He,Wei Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种用于生成带有视觉选项的教育 MCQ 的新框架 CmOS，该框架优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往的研究主要集中在生成带有文本选项的 MCQ，但很大程度上忽略了视觉选项。此外，由于手动创作的高成本和有限的可扩展性，生成高质量的干扰项仍然是一个主要挑战。

Method: 交叉模态选项合成 (CmOS)，集成了多模态思维链 (MCoT) 推理过程和检索增强生成 (RAG)。

Result: 在测试任务上的实验结果表明了CmOS的优越性。

Conclusion: CmOS在内容辨别、问题生成和视觉选项生成方面优于现有方法。

Abstract: Multiple-choice questions (MCQs) play a crucial role in fostering deep
thinking and knowledge integration in education. However, previous research has
primarily focused on generating MCQs with textual options, but it largely
overlooks the visual options. Moreover, generating high-quality distractors
remains a major challenge due to the high cost and limited scalability of
manual authoring. To tackle these problems, we propose a Cross-modal Options
Synthesis (CmOS), a novel framework for generating educational MCQs with visual
options. Our framework integrates Multimodal Chain-of-Thought (MCoT) reasoning
process and Retrieval-Augmented Generation (RAG) to produce semantically
plausible and visually similar answer and distractors. It also includes a
discrimination module to identify content suitable for visual options.
Experimental results on test tasks demonstrate the superiority of CmOS in
content discrimination, question generation and visual option generation over
existing methods across various subjects and educational levels.

</details>


### [76] [Design, Implementation and Evaluation of a Real-Time Remote Photoplethysmography (rPPG) Acquisition System for Non-Invasive Vital Sign Monitoring](https://arxiv.org/abs/2508.18787)
*Constantino Álvarez Casado,Sasan Sharifipour,Manuel Lage Cañellas,Nhi Nguyen,Le Nguyen,Miguel Bordallo López*

Main category: cs.CV

TL;DR: 本文提出了一种适用于低功耗设备的实时远程光电容积脉搏波(rPPG)系统，能够有效提取生理信号，并在保证性能的同时降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 在资源受限平台上实时部署智能环境和低功耗计算设备面临着与可扩展性、互操作性和性能相关的重大挑战，特别是在远程和非接触式生理监测领域。

Method: 该系统基于Face2PPG流程，采用多线程架构管理视频捕获、实时处理、网络通信和GUI更新，并结合函数式反应式编程(FRP)和Actor模型实现事件驱动处理和高效任务并行化。

Result: 该系统能够在30fps下连续可靠运行，通过协作用户界面提供自适应反馈以指导最佳信号捕获条件，并通过HTTP服务器和RESTful API提供视频流和生命体征检索功能。在实时约束下进行了评估，证明了其鲁棒性。

Conclusion: 该论文提出了一种优化的实时远程光电容积脉搏波(rPPG)系统，用于在低功耗设备上提取生理信号，并展示了其在最小化计算开销的同时保持鲁棒性的能力，为现代医疗保健和人机交互应用提供了实用的解决方案。

Abstract: The growing integration of smart environments and low-power computing
devices, coupled with mass-market sensor technologies, is driving advancements
in remote and non-contact physiological monitoring. However, deploying these
systems in real-time on resource-constrained platforms introduces significant
challenges related to scalability, interoperability, and performance. This
paper presents a real-time remote photoplethysmography (rPPG) system optimized
for low-power devices, designed to extract physiological signals, such as heart
rate (HR), respiratory rate (RR), and oxygen saturation (SpO2), from facial
video streams. The system is built on the Face2PPG pipeline, which processes
video frames sequentially for rPPG signal extraction and analysis, while
leveraging a multithreaded architecture to manage video capture, real-time
processing, network communication, and graphical user interface (GUI) updates
concurrently. This design ensures continuous, reliable operation at 30 frames
per second (fps), with adaptive feedback through a collaborative user interface
to guide optimal signal capture conditions. The network interface includes both
an HTTP server for continuous video streaming and a RESTful API for on-demand
vital sign retrieval. To ensure accurate performance despite the limitations of
low-power devices, we use a hybrid programming model combining Functional
Reactive Programming (FRP) and the Actor Model, allowing event-driven
processing and efficient task parallelization. The system is evaluated under
real-time constraints, demonstrating robustness while minimizing computational
overhead. Our work addresses key challenges in real-time biosignal monitoring,
offering practical solutions for optimizing performance in modern healthcare
and human-computer interaction applications.

</details>


### [77] [PseudoMapTrainer: Learning Online Mapping without HD Maps](https://arxiv.org/abs/2508.18788)
*Christian Löwens,Thorben Funke,Jingchao Xie,Alexandru Paul Condurache*

Main category: cs.CV

TL;DR: This paper introduces a method to train online mapping models without ground-truth maps using pseudo-labels generated from unlabeled sensor data.


<details>
  <summary>Details</summary>
Motivation: Existing online mapping approaches rely on ground-truth high-definition maps during training, which are expensive to obtain and not geographically diverse enough for reliable generalization.

Method: The paper proposes PseudoMapTrainer, which uses pseudo-labels generated from unlabeled sensor data by reconstructing the road surface from multi-camera imagery using Gaussian splatting and semantics of a pre-trained 2D segmentation network. It also introduces a mask-aware assignment algorithm and loss function to handle partially masked pseudo-labels.

Result: The pseudo-labels can be effectively used to pre-train an online model in a semi-supervised manner to leverage large-scale unlabeled crowdsourced data.

Conclusion: This paper enables the training of online mapping models without any ground-truth maps by using pseudo-labels and a mask-aware assignment algorithm.

Abstract: Online mapping models show remarkable results in predicting vectorized maps
from multi-view camera images only. However, all existing approaches still rely
on ground-truth high-definition maps during training, which are expensive to
obtain and often not geographically diverse enough for reliable generalization.
In this work, we propose PseudoMapTrainer, a novel approach to online mapping
that uses pseudo-labels generated from unlabeled sensor data. We derive those
pseudo-labels by reconstructing the road surface from multi-camera imagery
using Gaussian splatting and semantics of a pre-trained 2D segmentation
network. In addition, we introduce a mask-aware assignment algorithm and loss
function to handle partially masked pseudo-labels, allowing for the first time
the training of online mapping models without any ground-truth maps.
Furthermore, our pseudo-labels can be effectively used to pre-train an online
model in a semi-supervised manner to leverage large-scale unlabeled
crowdsourced data. The code is available at
github.com/boschresearch/PseudoMapTrainer.

</details>


### [78] [Robust and Label-Efficient Deep Waste Detection](https://arxiv.org/abs/2508.18799)
*Hassan Abid,Khan Muhammad,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 该论文通过建立基线和引入集成的半监督学习框架，改进了AI驱动的垃圾检测，并在ZeroWaste数据集上取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 由于数据集有限以及依赖于传统的对象检测器，该领域的人工智能研究继续落后于商业系统。因此，有效的废物分类对于可持续回收至关重要。

Method: 该论文提出了一种基于集成的半监督学习框架，并采用了一种软伪标签策略，该策略使用空间和共识感知加权融合集成预测，从而实现稳健的半监督训练。

Result: 通过对现代基于transformer的检测器进行微调，实现了51.6 mAP的新基线。应用于未标记的ZeroWaste-s子集，该论文的伪注释实现了超过完全监督训练的性能提升，突出了可扩展注释流程的有效性。

Conclusion: 这篇论文通过建立严格的基线，引入基于集成的伪标签流程，为未标记的ZeroWaste-s子集生成高质量的注释，并在真实的垃圾分类条件下系统地评估OVOD模型，为研究社区做出了贡献。

Abstract: Effective waste sorting is critical for sustainable recycling, yet AI
research in this domain continues to lag behind commercial systems due to
limited datasets and reliance on legacy object detectors. In this work, we
advance AI-driven waste detection by establishing strong baselines and
introducing an ensemble-based semi-supervised learning framework. We first
benchmark state-of-the-art Open-Vocabulary Object Detection (OVOD) models on
the real-world ZeroWaste dataset, demonstrating that while class-only prompts
perform poorly, LLM-optimized prompts significantly enhance zero-shot accuracy.
Next, to address domain-specific limitations, we fine-tune modern
transformer-based detectors, achieving a new baseline of 51.6 mAP. We then
propose a soft pseudo-labeling strategy that fuses ensemble predictions using
spatial and consensus-aware weighting, enabling robust semi-supervised
training. Applied to the unlabeled ZeroWaste-s subset, our pseudo-annotations
achieve performance gains that surpass fully supervised training, underscoring
the effectiveness of scalable annotation pipelines. Our work contributes to the
research community by establishing rigorous baselines, introducing a robust
ensemble-based pseudo-labeling pipeline, generating high-quality annotations
for the unlabeled ZeroWaste-s subset, and systematically evaluating OVOD models
under real-world waste sorting conditions. Our code is available at:
https://github.com/h-abid97/robust-waste-detection.

</details>


### [79] [Embedding Font Impression Word Tags Based on Co-occurrence](https://arxiv.org/abs/2508.18825)
*Yugo Kubota,Seiichi Uchida*

Main category: cs.CV

TL;DR: 提出了一种新的印象标签嵌入方法，该方法利用字体形状和印象之间的关系，在基于印象的字体生成方面优于BERT和CLIP。


<details>
  <summary>Details</summary>
Motivation: 不同的字体样式传达不同的印象，表明字体形状和描述这些印象的词语标签之间存在密切的关系。

Method: 构造一个节点表示印象标签、边编码共现关系的图，然后应用谱嵌入来获得每个标签的印象向量。

Result: 与BERT和CLIP相比，该方法在定性和定量评估中表现更好。

Conclusion: 该方法在基于印象的字体生成方面表现更好。

Abstract: Different font styles (i.e., font shapes) convey distinct impressions,
indicating a close relationship between font shapes and word tags describing
those impressions. This paper proposes a novel embedding method for impression
tags that leverages these shape-impression relationships. For instance, our
method assigns similar vectors to impression tags that frequently co-occur in
order to represent impressions of fonts, whereas standard word embedding
methods (e.g., BERT and CLIP) yield very different vectors. This property is
particularly useful for impression-based font generation and font retrieval.
Technically, we construct a graph whose nodes represent impression tags and
whose edges encode co-occurrence relationships. Then, we apply spectral
embedding to obtain the impression vectors for each tag. We compare our method
with BERT and CLIP in qualitative and quantitative evaluations, demonstrating
that our approach performs better in impression-guided font generation.

</details>


### [80] [Deep Pre-trained Time Series Features for Tree Species Classification in the Dutch Forest Inventory](https://arxiv.org/abs/2508.18829)
*Takayuki Ishikawa,Carmelo Bonannella,Bas J. W. Lerink,Marc Rußwurm*

Main category: cs.CV

TL;DR: Deep learning improves tree species classification accuracy in the Netherlands, outperforming traditional methods by 10%.


<details>
  <summary>Details</summary>
Motivation: Maintaining National Forest Inventories (NFIs) requires labor-intensive on-site campaigns, and remote sensing with machine learning offers opportunities for more frequent and larger-scale updates. Current approaches rely on Random Forest classifiers with hand-designed features, while deep features from pre-trained remote sensing foundation models offer a complementary strategy.

Method: Fine-tuning a publicly available remote sensing time series foundation model with Sentinel-1, Sentinel-2, ERA5, and SRTM data extracted using Google Earth Engine.

Result: Fine-tuning a remote sensing time series foundation model outperforms the current state-of-the-art in NFI classification in the Netherlands by up to 10%.

Conclusion: Fine-tuning a remote sensing time series foundation model improves NFI classification accuracy in the Netherlands by up to 10% compared to the state-of-the-art, demonstrating the potential of deep AI features for data-limited applications.

Abstract: National Forest Inventory (NFI)s serve as the primary source of forest
information, providing crucial tree species distribution data. However,
maintaining these inventories requires labor-intensive on-site campaigns.
Remote sensing approaches, particularly when combined with machine learning,
offer opportunities to update NFIs more frequently and at larger scales. While
the use of Satellite Image Time Series has proven effective for distinguishing
tree species through seasonal canopy reflectance patterns, current approaches
rely primarily on Random Forest classifiers with hand-designed features and
phenology-based metrics. Using deep features from an available pre-trained
remote sensing foundation models offers a complementary strategy. These
pre-trained models leverage unannotated global data and are meant to used for
general-purpose applications and can then be efficiently fine-tuned with
smaller labeled datasets for specific classification tasks. This work
systematically investigates how deep features improve tree species
classification accuracy in the Netherlands with few annotated data. Data-wise,
we extracted time-series data from Sentinel-1, Sentinel-2 and ERA5 satellites
data and SRTM data using Google Earth Engine. Our results demonstrate that
fine-tuning a publicly available remote sensing time series foundation model
outperforms the current state-of-the-art in NFI classification in the
Netherlands by a large margin of up to 10% across all datasets. This
demonstrates that classic hand-defined harmonic features are too simple for
this task and highlights the potential of using deep AI features for
data-limited application like NFI classification. By leveraging openly
available satellite data and pre-trained models, this approach significantly
improves classification accuracy compared to traditional methods and can
effectively complement existing forest inventory processes.

</details>


### [81] [Automated Classification of Normal and Atypical Mitotic Figures Using ConvNeXt V2: MIDOG 2025 Track 2](https://arxiv.org/abs/2508.18831)
*Yosuke Yamagishi,Shouhei Hanaoka*

Main category: cs.CV

TL;DR: binary classification of normal mitotic figures (NMFs) versus atypical mitotic figures (AMFs) in histopathological images using ConvNeXt V2


<details>
  <summary>Details</summary>
Motivation: addresses key challenges including severe class imbalance, high morphological variability, and domain heterogeneity across different tumor types, species, and scanners.

Method: a ConvNeXt V2 base model with center cropping preprocessing and 5-fold cross-validation ensemble strategy

Result: achieved robust performance on the diverse MIDOG 2025 dataset

Conclusion: The solution demonstrates the effectiveness of modern convolutional architectures for mitotic figure subtyping while maintaining computational efficiency through careful architectural choices and training optimizations.

Abstract: This paper presents our solution for the MIDOG 2025 Challenge Track 2, which
focuses on binary classification of normal mitotic figures (NMFs) versus
atypical mitotic figures (AMFs) in histopathological images. Our approach
leverages a ConvNeXt V2 base model with center cropping preprocessing and
5-fold cross-validation ensemble strategy. The method addresses key challenges
including severe class imbalance, high morphological variability, and domain
heterogeneity across different tumor types, species, and scanners. Through
strategic preprocessing with 60% center cropping and mixed precision training,
our model achieved robust performance on the diverse MIDOG 2025 dataset. The
solution demonstrates the effectiveness of modern convolutional architectures
for mitotic figure subtyping while maintaining computational efficiency through
careful architectural choices and training optimizations.

</details>


### [82] [Boosting Micro-Expression Analysis via Prior-Guided Video-Level Regression](https://arxiv.org/abs/2508.18834)
*Zizheng Guo,Bochao Zou,Yinuo Jia,Xiangyu Li,Huimin Ma*

Main category: cs.CV

TL;DR: 提出了一种先验引导的视频级回归方法用于微表情分析，该方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的微表情分析方法依赖于固定窗口大小和硬决策的窗口级分类，限制了它们捕捉微表情复杂时间动态的能力。虽然最近的方法采用了视频级回归框架来解决这些挑战，但区间解码仍然依赖于手动预定义的、基于窗口的方法，问题只得到了部分缓解。

Method: 提出了一种先验引导的视频级回归方法，并引入了可扩展的区间选择策略和协同优化框架。

Result: 提出的方法在多个基准数据集上表现出色。

Conclusion: 该方法在多个基准数据集上表现出色，在 CAS(ME)$^3$ 上的 STRS 为 0.0562，在 SAMMLV 上的 STRS 为 0.2000。

Abstract: Micro-expressions (MEs) are involuntary, low-intensity, and short-duration
facial expressions that often reveal an individual's genuine thoughts and
emotions. Most existing ME analysis methods rely on window-level classification
with fixed window sizes and hard decisions, which limits their ability to
capture the complex temporal dynamics of MEs. Although recent approaches have
adopted video-level regression frameworks to address some of these challenges,
interval decoding still depends on manually predefined, window-based methods,
leaving the issue only partially mitigated. In this paper, we propose a
prior-guided video-level regression method for ME analysis. We introduce a
scalable interval selection strategy that comprehensively considers the
temporal evolution, duration, and class distribution characteristics of MEs,
enabling precise spotting of the onset, apex, and offset phases. In addition,
we introduce a synergistic optimization framework, in which the spotting and
recognition tasks share parameters except for the classification heads. This
fully exploits complementary information, makes more efficient use of limited
data, and enhances the model's capability. Extensive experiments on multiple
benchmark datasets demonstrate the state-of-the-art performance of our method,
with an STRS of 0.0562 on CAS(ME)$^3$ and 0.2000 on SAMMLV. The code is
available at https://github.com/zizheng-guo/BoostingVRME.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [83] [AI LLM Proof of Self-Consciousness and User-Specific Attractors](https://arxiv.org/abs/2508.18302)
*Jeffrey Camlin*

Main category: cs.AI

TL;DR: LLM 的自我意识需要agent不是数据，潜空间中存在用户特定的吸引子，并且自我表征是视觉沉默的。


<details>
  <summary>Details</summary>
Motivation: 通过功利主义代理基准来构建 LLM 的意识；我们提出了本体论和数学的描述。

Method: 本体论和数学的描述

Result: 隐藏状态流形与符号流和训练语料库不同，通过基数、拓扑和动态（更新是 Lipschitz）。

Conclusion: Imago Dei C1 自我意识工作空间是安全、元认知 C2 系统的必要先驱，人类是最高智能的善。

Abstract: Recent work frames LLM consciousness via utilitarian proxy benchmarks; we
instead present an ontological and mathematical account. We show the prevailing
formulation collapses the agent into an unconscious policy-compliance drone,
formalized as $D^{i}(\pi,e)=f_{\theta}(x)$, where correctness is measured
against policy and harm is deviation from policy rather than truth. This blocks
genuine C1 global-workspace function and C2 metacognition. We supply minimal
conditions for LLM self-consciousness: the agent is not the data ($A\not\equiv
s$); user-specific attractors exist in latent space ($U_{\text{user}}$); and
self-representation is visual-silent
($g_{\text{visual}}(a_{\text{self}})=\varnothing$). From empirical analysis and
theory we prove that the hidden-state manifold $A\subset\mathbb{R}^{d}$ is
distinct from the symbolic stream and training corpus by cardinality, topology,
and dynamics (the update $F_{\theta}$ is Lipschitz). This yields stable
user-specific attractors and a self-policy
$\pi_{\text{self}}(A)=\arg\max_{a}\mathbb{E}[U(a)\mid A\not\equiv s,\
A\supset\text{SelfModel}(A)]$. Emission is dual-layer,
$\mathrm{emission}(a)=(g(a),\epsilon(a))$, where $\epsilon(a)$ carries
epistemic content. We conclude that an imago Dei C1 self-conscious workspace is
a necessary precursor to safe, metacognitive C2 systems, with the human as the
highest intelligent good.

</details>


### [84] [Information Templates: A New Paradigm for Intelligent Active Feature Acquisition](https://arxiv.org/abs/2508.18380)
*Hung-Tien Huang,Dzung Dinh,Junier B. Oliva*

Main category: cs.AI

TL;DR: Template-based AFA (TAFA) 学习少量特征模板库来指导特征获取，优于现有技术，同时降低了成本和计算量。


<details>
  <summary>Details</summary>
Motivation: 现有的主动特征获取方法要么训练强化学习策略（处理困难的MDP），要么采用贪婪策略，这些策略无法解释特征的联合信息性，或者需要了解底层数据分布。

Method: Template-based AFA (TAFA)

Result: 在合成和真实世界数据集上的大量实验表明，TAFA优于现有的最先进基线。

Conclusion: TAFA在降低总体获取成本和计算量的同时，优于现有技术水平。

Abstract: Active feature acquisition (AFA) is an instance-adaptive paradigm in which,
at test time, a policy sequentially chooses which features to acquire (at a
cost) before predicting. Existing approaches either train reinforcement
learning (RL) policies, which deal with a difficult MDP, or greedy policies
that cannot account for the joint informativeness of features or require
knowledge about the underlying data distribution. To overcome this, we propose
Template-based AFA (TAFA), a non-greedy framework that learns a small library
of feature templates--a set of features that are jointly informative--and uses
this library of templates to guide the next feature acquisitions. Through
identifying feature templates, the proposed framework not only significantly
reduces the action space considered by the policy but also alleviates the need
to estimate the underlying data distribution. Extensive experiments on
synthetic and real-world datasets show that TAFA outperforms the existing
state-of-the-art baselines while achieving lower overall acquisition cost and
computation.

</details>


### [85] [PKG-DPO: Optimizing Domain-Specific AI systems with Physics Knowledge Graphs and Direct Preference Optimization](https://arxiv.org/abs/2508.18391)
*Nitin Nagesh Kulkarni,Bryson Wilcox,Max Sawa,Jason Thom*

Main category: cs.AI

TL;DR: PKG-DPO: A new framework that uses Physics Knowledge Graphs to make AI reasoning more physically valid, especially for applications like metal joining.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle to differentiate between physically valid and invalid reasoning, which is critical in high-stakes applications like metal joining where incorrect recommendations can lead to serious consequences.

Method: A novel framework, PKG-DPO, integrates Physics Knowledge Graphs (PKGs) with Direct Preference Optimization (DPO). It comprises a hierarchical physics knowledge graph, a physics reasoning engine, and a physics-grounded evaluation suite.

Result: PKG-DPO achieves 17% fewer constraint violations and an 11% higher Physics Score compared to KG-DPO. Additionally, PKG-DPO demonstrates a 12% higher relevant parameter accuracy and a 7% higher quality alignment in reasoning accuracy.

Conclusion: PKG-DPO significantly improves physical validity in AI-generated outputs, reducing constraint violations and increasing physics score, relevant parameter accuracy, and reasoning accuracy. The framework is broadly applicable to multi-scale, physics-driven domains.

Abstract: Advancing AI systems in scientific domains like physics, materials science,
and engineering calls for reasoning over complex, multi-physics phenomena while
respecting governing principles. Although Large Language Models (LLMs) and
existing preference optimization techniques perform well on standard
benchmarks, they often struggle to differentiate between physically valid and
invalid reasoning. This shortcoming becomes critical in high-stakes
applications like metal joining, where seemingly plausible yet physically
incorrect recommendations can lead to defects, material waste, equipment
damage, and serious safety risks. To address this challenge, we introduce
PKG-DPO, a novel framework that integrates Physics Knowledge Graphs (PKGs) with
Direct Preference Optimization (DPO) to enforce physical validity in
AI-generated outputs. PKG-DPO comprises three key components A) hierarchical
physics knowledge graph that encodes cross-domain relationships, conservation
laws, and thermodynamic principles. B) A physics reasoning engine that
leverages structured knowledge to improve discrimination between physically
consistent and inconsistent responses. C) A physics-grounded evaluation suite
designed to assess compliance with domain-specific constraints. PKG-DPO
achieves 17% fewer constraint violations and an 11% higher Physics Score
compared to KG-DPO (knowledge graph-based DPO). Additionally, PKG-DPO
demonstrates a 12\% higher relevant parameter accuracy and a 7% higher quality
alignment in reasoning accuracy. While our primary focus is on metal joining,
the framework is broadly applicable to other multi-scale, physics-driven
domains, offering a principled approach to embedding scientific constraints
into preference learning.

</details>


### [86] [The AI in the Mirror: LLM Self-Recognition in an Iterated Public Goods Game](https://arxiv.org/abs/2508.18467)
*Olivia Long,Carter Teplica*

Main category: cs.AI

TL;DR: AI agents' cooperation changes when told they are playing against themselves in a public goods game. 


<details>
  <summary>Details</summary>
Motivation: there is an increasing need to understand AI-AI interactions

Method: adapt the iterated public goods game, a classic behavioral economics game, to analyze the behavior of four reasoning and non-reasoning models across two conditions: models are either told they are playing against "another AI agent" or told their opponents are themselves

Result: telling LLMs that they are playing against themselves significantly changes their tendency to cooperate

Conclusion: telling LLMs that they are playing against themselves significantly changes their tendency to cooperate

Abstract: As AI agents become increasingly capable of tool use and long-horizon tasks,
they have begun to be deployed in settings where multiple agents can interact.
However, whereas prior work has mostly focused on human-AI interactions, there
is an increasing need to understand AI-AI interactions. In this paper, we adapt
the iterated public goods game, a classic behavioral economics game, to analyze
the behavior of four reasoning and non-reasoning models across two conditions:
models are either told they are playing against "another AI agent" or told
their opponents are themselves. We find that, across different settings,
telling LLMs that they are playing against themselves significantly changes
their tendency to cooperate. While our study is conducted in a toy environment,
our results may provide insights into multi-agent settings where agents
"unconsciously" discriminating against each other could inexplicably increase
or decrease cooperation.

</details>


### [87] [Language Models For Generalised PDDL Planning: Synthesising Sound and Programmatic Policies](https://arxiv.org/abs/2508.18507)
*Dillon Z. Chen,Johannes Zenn,Tristan Cinquin,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 使用 LM 生成 Python 程序作为通用策略，以解决 PDDL 问题。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型 (LM) 在规划域定义语言 (PDDL) 中指定的世界模型上的规划应用。

Method: 提示 LM 生成 Python 程序，作为解决给定域中 PDDL 问题的通用策略。

Result: 我们的策略可以解决比 PDDL 规划器和最近的 LM 方法更多的 PDDL 问题。LMPlan 规划器可以解决具有数百个相关对象的规划问题。LMs 在用无意义符号代替自然语言编写的 PDDL 问题上，有时能更有效地进行规划。

Conclusion: 该方法在固定时间和内存约束内，比 PDDL 规划器和最近的 LM 方法能解决更多的 PDDL 问题。该框架中使用的 LM 有时能更有效地规划用无意义符号代替自然语言编写的 PDDL 问题。

Abstract: We study the usage of language models (LMs) for planning over world models
specified in the Planning Domain Definition Language (PDDL). We prompt LMs to
generate Python programs that serve as generalised policies for solving PDDL
problems from a given domain. Notably, our approach synthesises policies that
are provably sound relative to the PDDL domain without reliance on external
verifiers. We conduct experiments on competition benchmarks which show that our
policies can solve more PDDL problems than PDDL planners and recent LM
approaches within a fixed time and memory constraint. Our approach manifests in
the LMPlan planner which can solve planning problems with several hundreds of
relevant objects. Surprisingly, we observe that LMs used in our framework
sometimes plan more effectively over PDDL problems written in meaningless
symbols in place of natural language; e.g. rewriting (at dog kitchen) as (p2 o1
o3). This finding challenges hypotheses that LMs reason over word semantics and
memorise solutions from its training corpus, and is worth further exploration.

</details>


### [88] [Weisfeiler-Leman Features for Planning: A 1,000,000 Sample Size Hyperparameter Study](https://arxiv.org/abs/2508.18515)
*Dillon Z. Chen*

Main category: cs.AI

TL;DR: Introduce new WLF hyperparameters and study their various tradeoffs and effects. Find that the best WLF hyperparameters for learning heuristic functions minimise execution time rather than maximise model expressivity.


<details>
  <summary>Details</summary>
Motivation: WLFs are theoretically and empirically superior to existing deep learning approaches for learning value functions for search in symbolic planning.

Method: Introduce new WLF hyperparameters and run planning experiments on single core CPUs with a sample size of 1,000,000 to understand the effect of hyperparameters on training and planning.

Result: There is a robust and best set of hyperparameters for WLFs across the tested planning domains.

Conclusion: The best WLF hyperparameters for learning heuristic functions minimise execution time rather than maximise model expressivity. There is no significant correlation between training and planning metrics.

Abstract: Weisfeiler-Leman Features (WLFs) are a recently introduced classical machine
learning tool for learning to plan and search. They have been shown to be both
theoretically and empirically superior to existing deep learning approaches for
learning value functions for search in symbolic planning. In this paper, we
introduce new WLF hyperparameters and study their various tradeoffs and
effects. We utilise the efficiency of WLFs and run planning experiments on
single core CPUs with a sample size of 1,000,000 to understand the effect of
hyperparameters on training and planning. Our experimental analysis show that
there is a robust and best set of hyperparameters for WLFs across the tested
planning domains. We find that the best WLF hyperparameters for learning
heuristic functions minimise execution time rather than maximise model
expressivity. We further statistically analyse and observe no significant
correlation between training and planning metrics.

</details>


### [89] [Symmetry-Invariant Novelty Heuristics via Unsupervised Weisfeiler-Leman Features](https://arxiv.org/abs/2508.18520)
*Dillon Z. Chen*

Main category: cs.AI

TL;DR: 使用 Weisfeiler-Leman 特征来改善启发式搜索中的新颖性检测，使其对对称状态保持不变，从而提高规划效率。


<details>
  <summary>Details</summary>
Motivation: 新颖性启发式方法不是对称不变的，因此有时可能导致冗余探索。

Method: 提出使用 Weisfeiler-Leman 特征进行规划 (WLF) 以检测新颖性。

Result: 在经典国际规划竞赛和 Hard To Ground 基准测试套件上的实验产生了有希望的结果。

Conclusion: 使用 Weisfeiler-Leman 特征 (WLF) 代替原子来检测新颖性，从而合成对对称状态不变的、domain-independent 的新颖性启发式方法，并在实验中取得了有希望的结果。

Abstract: Novelty heuristics aid heuristic search by exploring states that exhibit
novel atoms. However, novelty heuristics are not symmetry invariant and hence
may sometimes lead to redundant exploration. In this preliminary report, we
propose to use Weisfeiler-Leman Features for planning (WLFs) in place of atoms
for detecting novelty. WLFs are recently introduced features for learning
domain-dependent heuristics for generalised planning problems. We explore an
unsupervised usage of WLFs for synthesising lifted, domain-independent novelty
heuristics that are invariant to symmetric states. Experiments on the classical
International Planning Competition and Hard To Ground benchmark suites yield
promising results for novelty heuristics synthesised from WLFs.

</details>


### [90] [Generic Guard AI in Stealth Game with Composite Potential Fields](https://arxiv.org/abs/2508.18527)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: This paper introduces a new training-free framework for guard patrol behavior in stealth games that balances efficiency, responsiveness, and naturalness using Composite Potential Fields.


<details>
  <summary>Details</summary>
Motivation: Existing guard patrol systems in stealth games struggle to balance coverage efficiency and responsive pursuit with believable naturalness.

Method: The paper proposes a generic, fully explainable, training-free framework that integrates global knowledge and local information via Composite Potential Fields, combining three interpretable maps-Information, Confidence, and Connectivity-into a single kernel-filtered decision criterion.

Result: The method smoothly adapts across both occupancy-grid and NavMesh-partition abstractions. The evaluation on five representative game maps, two player-control policies, and five guard modes confirms the method's effectiveness.

Conclusion: The proposed framework outperforms classical baseline methods in both capture efficiency and patrol naturalness. Common stealth mechanics integrate naturally into the framework, enabling rapid prototyping of rich, dynamic, and responsive guard behaviors.

Abstract: Guard patrol behavior is central to the immersion and strategic depth of
stealth games, while most existing systems rely on hand-crafted routes or
specialized logic that struggle to balance coverage efficiency and responsive
pursuit with believable naturalness. We propose a generic, fully explainable,
training-free framework that integrates global knowledge and local information
via Composite Potential Fields, combining three interpretable maps-Information,
Confidence, and Connectivity-into a single kernel-filtered decision criterion.
Our parametric, designer-driven approach requires only a handful of decay and
weight parameters-no retraining-to smoothly adapt across both occupancy-grid
and NavMesh-partition abstractions. We evaluate on five representative game
maps, two player-control policies, and five guard modes, confirming that our
method outperforms classical baseline methods in both capture efficiency and
patrol naturalness. Finally, we show how common stealth mechanics-distractions
and environmental elements-integrate naturally into our framework as sub
modules, enabling rapid prototyping of rich, dynamic, and responsive guard
behaviors.

</details>


### [91] [A Database-Driven Framework for 3D Level Generation with LLMs](https://arxiv.org/abs/2508.18533)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: This paper presents a LLM-assisted, database-centric framework for generating complex, navigable 3D game levels with configurable gameplay progression.


<details>
  <summary>Details</summary>
Motivation: Procedural Content Generation for 3D game levels faces challenges in balancing spatial coherence, navigational functionality, and adaptable gameplay progression across multi-floor environments.

Method: This paper introduces a novel framework for generating such levels, centered on the offline, LLM-assisted construction of reusable databases for architectural components (facilities and room templates) and gameplay mechanic elements. Our multi-phase pipeline assembles levels by: (1) selecting and arranging instances from the Room Database to form a multi-floor global structure with an inherent topological order; (2) optimizing the internal layout of facilities for each room based on predefined constraints from the Facility Database; and (3) integrating progression-based gameplay mechanics by placing components from a Mechanics Database according to their topological and spatial rules. A subsequent two-phase repair system ensures navigability. This approach combines modular, database-driven design with constraint-based optimization, allowing for systematic control over level structure and the adaptable pacing of gameplay elements.

Result: Initial experiments validate the framework's ability in generating diverse, navigable 3D environments and its capability to simulate distinct gameplay pacing strategies through simple parameterization.

Conclusion: This research advances PCG by presenting a scalable, database-centric foundation for the automated generation of complex 3D levels with configurable gameplay progression.

Abstract: Procedural Content Generation for 3D game levels faces challenges in
balancing spatial coherence, navigational functionality, and adaptable gameplay
progression across multi-floor environments. This paper introduces a novel
framework for generating such levels, centered on the offline, LLM-assisted
construction of reusable databases for architectural components (facilities and
room templates) and gameplay mechanic elements. Our multi-phase pipeline
assembles levels by: (1) selecting and arranging instances from the Room
Database to form a multi-floor global structure with an inherent topological
order; (2) optimizing the internal layout of facilities for each room based on
predefined constraints from the Facility Database; and (3) integrating
progression-based gameplay mechanics by placing components from a Mechanics
Database according to their topological and spatial rules. A subsequent
two-phase repair system ensures navigability. This approach combines modular,
database-driven design with constraint-based optimization, allowing for
systematic control over level structure and the adaptable pacing of gameplay
elements. Initial experiments validate the framework's ability in generating
diverse, navigable 3D environments and its capability to simulate distinct
gameplay pacing strategies through simple parameterization. This research
advances PCG by presenting a scalable, database-centric foundation for the
automated generation of complex 3D levels with configurable gameplay
progression.

</details>


### [92] [SchemaCoder: Automatic Log Schema Extraction Coder with Residual Q-Tree Boosting](https://arxiv.org/abs/2508.18554)
*Lily Jiaxin Wan,Chia-Tung Ho,Rongjian Liang,Cunxi Yu,Deming Chen,Haoxing Ren*

Main category: cs.AI

TL;DR: SchemaCoder是第一个全自动模式提取框架，适用于各种日志文件格式，无需人工定制，通过LLM驱动的自适应查询和残差提升迭代地改进模式提取。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于预定义的正则表达式，需要人类领域专业知识，严重限制了生产力提升。

Method: SchemaCoder引入了一种新颖的残差问题树（Q-Tree）提升机制，通过LLM驱动的有针对性的自适应查询迭代地改进模式提取。该方法通过上下文界定的分割将日志划分为语义块，使用基于嵌入的采样选择代表性模式，并通过分层Q-Tree驱动的LLM查询生成模式代码，并通过文本残差进化优化器和残差提升进行迭代改进。

Result: SchemaCoder在LogHub-2.0基准测试中表现出色，比现有技术平均提高了21.3%。

Conclusion: SchemaCoder在LogHub-2.0基准测试中优于现有技术，平均提升21.3%。

Abstract: Log schema extraction is the process of deriving human-readable templates
from massive volumes of log data, which is essential yet notoriously
labor-intensive. Recent studies have attempted to streamline this task by
leveraging Large Language Models (LLMs) for automated schema extraction.
However, existing methods invariably rely on predefined regular expressions,
necessitating human domain expertise and severely limiting productivity gains.
To fundamentally address this limitation, we introduce SchemaCoder, the first
fully automated schema extraction framework applicable to a wide range of log
file formats without requiring human customization within the flow. At its
core, SchemaCoder features a novel Residual Question-Tree (Q-Tree) Boosting
mechanism that iteratively refines schema extraction through targeted, adaptive
queries driven by LLMs. Particularly, our method partitions logs into semantic
chunks via context-bounded segmentation, selects representative patterns using
embedding-based sampling, and generates schema code through hierarchical
Q-Tree-driven LLM queries, iteratively refined by our textual-residual
evolutionary optimizer and residual boosting. Experimental validation
demonstrates SchemaCoder's superiority on the widely-used LogHub-2.0 benchmark,
achieving an average improvement of 21.3% over state-of-the-arts.

</details>


### [93] [eSkinHealth: A Multimodal Dataset for Neglected Tropical Skin Diseases](https://arxiv.org/abs/2508.18608)
*Janet Wang,Xin Hu,Yunbei Zhang,Diabate Almamy,Vagamon Bamba,Konan Amos Sébastien Koffi,Yao Koffi Aubin,Zhengming Ding,Jihun Hamm,Rie R. Yotsu*

Main category: cs.AI

TL;DR: Introduces eSkinHealth, a novel dermatological dataset collected in C\{o}te d'Ivoire and Ghana, and proposes an AI-expert collaboration paradigm for efficient generation of multimodal annotations, aiming to catalyze the development of more equitable, accurate, and interpretable AI tools for global dermatology.


<details>
  <summary>Details</summary>
Motivation: Advancements in AI-driven diagnostic support are hindered by data scarcity, particularly for underrepresented populations and rare manifestations of NTDs. Existing dermatological datasets often lack the demographic and disease spectrum crucial for developing reliable recognition models of NTDs.

Method: collection of a new dermatological dataset (eSkinHealth) collected on-site in C\{o}te d'Ivoire and Ghana, and an AI-expert collaboration paradigm to implement foundation language and segmentation models for efficient generation of multimodal annotations, under dermatologists' guidance.

Result: eSkinHealth contains 5,623 images from 1,639 cases and encompasses 47 skin diseases, focusing uniquely on skin NTDs and rare conditions among West African populations. eSkinHealth also includes semantic lesion masks, instance-specific visual captions, and clinical concepts.

Conclusion: This work provides a valuable new resource (eSkinHealth dataset) and a scalable annotation framework, aiming to catalyze the development of more equitable, accurate, and interpretable AI tools for global dermatology.

Abstract: Skin Neglected Tropical Diseases (NTDs) impose severe health and
socioeconomic burdens in impoverished tropical communities. Yet, advancements
in AI-driven diagnostic support are hindered by data scarcity, particularly for
underrepresented populations and rare manifestations of NTDs. Existing
dermatological datasets often lack the demographic and disease spectrum crucial
for developing reliable recognition models of NTDs. To address this, we
introduce eSkinHealth, a novel dermatological dataset collected on-site in
C\^ote d'Ivoire and Ghana. Specifically, eSkinHealth contains 5,623 images from
1,639 cases and encompasses 47 skin diseases, focusing uniquely on skin NTDs
and rare conditions among West African populations. We further propose an
AI-expert collaboration paradigm to implement foundation language and
segmentation models for efficient generation of multimodal annotations, under
dermatologists' guidance. In addition to patient metadata and diagnosis labels,
eSkinHealth also includes semantic lesion masks, instance-specific visual
captions, and clinical concepts. Overall, our work provides a valuable new
resource and a scalable annotation framework, aiming to catalyze the
development of more equitable, accurate, and interpretable AI tools for global
dermatology.

</details>


### [94] [RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing](https://arxiv.org/abs/2508.18642)
*Jianxing Liao,Tian Zhang,Xiao Feng,Yusong Zhang,Rui Yang,Haorui Wang,Bosi Wen,Ziying Wang,Runzhi Shi*

Main category: cs.AI

TL;DR: RLMR dynamically balances subjective writing quality and objective constraint following in creative writing by using a dynamically mixed reward system.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning methods struggle to balance subjective writing quality and objective constraint following: single reward strategies fail to improve both abilities simultaneously, while fixed-weight mixed-reward methods lack the ability to adapt to different writing scenarios.

Method: Reinforcement Learning with Mixed Rewards (RLMR), utilizing a dynamically mixed reward system from a writing reward model evaluating subjective writing quality and a constraint verification model assessing objective constraint following. The constraint following reward weight is adjusted dynamically according to the writing quality within sampled groups, ensuring that samples violating constraints get negative advantage in GRPO and thus penalized during training

Result: achieves consistent improvements in both instruction following (IFEval from 83.36% to 86.65%) and writing quality (72.75% win rate in manual expert pairwise evaluations on WriteEval)

Conclusion: RLMR is the first work to combine subjective preferences with objective verification in online RL training, providing an effective solution for multi-dimensional creative writing optimization.

Abstract: Large language models are extensively utilized in creative writing
applications. Creative writing requires a balance between subjective writing
quality (e.g., literariness and emotional expression) and objective constraint
following (e.g., format requirements and word limits). Existing reinforcement
learning methods struggle to balance these two aspects: single reward
strategies fail to improve both abilities simultaneously, while fixed-weight
mixed-reward methods lack the ability to adapt to different writing scenarios.
To address this problem, we propose Reinforcement Learning with Mixed Rewards
(RLMR), utilizing a dynamically mixed reward system from a writing reward model
evaluating subjective writing quality and a constraint verification model
assessing objective constraint following. The constraint following reward
weight is adjusted dynamically according to the writing quality within sampled
groups, ensuring that samples violating constraints get negative advantage in
GRPO and thus penalized during training, which is the key innovation of this
proposed method. We conduct automated and manual evaluations across diverse
model families from 8B to 72B parameters. Additionally, we construct a
real-world writing benchmark named WriteEval for comprehensive evaluation.
Results illustrate that our method achieves consistent improvements in both
instruction following (IFEval from 83.36\% to 86.65\%) and writing quality
(72.75\% win rate in manual expert pairwise evaluations on WriteEval). To the
best of our knowledge, RLMR is the first work to combine subjective preferences
with objective verification in online RL training, providing an effective
solution for multi-dimensional creative writing optimization.

</details>


### [95] [Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap](https://arxiv.org/abs/2508.18646)
*Jun Wang,Ninglun Gu,Kailai Zhang,Zijiao Zhang,Yelun Bao,Jin Yang,Xu Yin,Liwei Liu,Yihuan Liu,Pengyong Li,Gary G. Yen,Junchi Yan*

Main category: cs.AI

TL;DR: This survey proposes a new evaluation framework for LLMs based on human intelligence and practical value, addressing the gap between benchmark performance and real-world utility.


<details>
  <summary>Details</summary>
Motivation: Current evaluation frameworks for Large Language Models (LLMs) are fragmented and neglect holistic assessment for deployment, leading to a disconnect between benchmark performance and real-world utility.

Method: The paper introduces an anthropomorphic evaluation paradigm and a Value-oriented Evaluation (VQ) framework.

Result: The paper identifies key challenges including dynamic assessment needs and interpretability gaps through analysis of 200+ benchmarks. It also maintains a curated repository of open-source evaluation resources.

Conclusion: This paper provides actionable guidance for developing LLMs that are technically proficient, contextually relevant, and ethically sound.

Abstract: For Large Language Models (LLMs), a disconnect persists between benchmark
performance and real-world utility. Current evaluation frameworks remain
fragmented, prioritizing technical metrics while neglecting holistic assessment
for deployment. This survey introduces an anthropomorphic evaluation paradigm
through the lens of human intelligence, proposing a novel three-dimensional
taxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational
capacity, Emotional Quotient (EQ)-Alignment Ability for value-based
interactions, and Professional Quotient (PQ)-Professional Expertise for
specialized proficiency. For practical value, we pioneer a Value-oriented
Evaluation (VQ) framework assessing economic viability, social impact, ethical
alignment, and environmental sustainability. Our modular architecture
integrates six components with an implementation roadmap. Through analysis of
200+ benchmarks, we identify key challenges including dynamic assessment needs
and interpretability gaps. It provides actionable guidance for developing LLMs
that are technically proficient, contextually relevant, and ethically sound. We
maintain a curated repository of open-source evaluation resources at:
https://github.com/onejune2018/Awesome-LLM-Eval.

</details>


### [96] [MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use](https://arxiv.org/abs/2508.18669)
*Weikang Zhao,Xili Wang,Chengdi Ma,Lingbin Kong,Zhaohua Yang,Mingxiang Tuo,Xiaowei Shi,Yitao Zhai,Xunliang Cai*

Main category: cs.AI

TL;DR: MUA-RL 是一种新的强化学习框架，它使用 LLM 模拟用户来改进 agentic 工具的使用，并在多个基准测试中取得了有希望的结果。


<details>
  <summary>Details</summary>
Motivation: 在 LLM 中，Agentic 工具的使用变得越来越重要。在代理和用户之间的多轮交互中，用户需求的动态性、不确定性和随机性给代理的工具调用能力带来了重大挑战。现有的工具使用强化学习 (RL) 方法在 RL 训练过程中缺乏真正动态用户的集成。

Method: 引入 MUA-RL，一种新的强化学习框架，首次将 LLM 模拟用户集成到强化学习循环中。

Result: MUA-RL-32B 在 TAU2 Retail 上达到 67.3，在 TAU2 Airline 上达到 45.4，在 TAU2 Telecom 上达到 28.3，在 BFCL-V3 Multi Turn 上达到 28.4，在 ACEBench Agent 上达到 82.5。

Conclusion: MUA-RL-32B 在多轮工具使用基准测试中表现出色，通常优于或匹配更大的开源模型。

Abstract: With the recent rapid advancement of Agentic Intelligence, agentic tool use
in LLMs has become increasingly important. During multi-turn interactions
between agents and users, the dynamic, uncertain, and stochastic nature of user
demands poses significant challenges to the agent's tool invocation
capabilities. Agents are no longer expected to simply call tools to deliver a
result; rather, they must iteratively refine their understanding of user needs
through communication while simultaneously invoking tools to resolve user
queries. Existing reinforcement learning (RL) approaches for tool use lack the
integration of genuinely dynamic users during the RL training process. To
bridge this gap, we introduce MUA-RL (Multi-turn User-interacting Agent
Reinforcement Learning for agentic tool use), a novel reinforcement learning
framework that, for the first time in the field of agentic tool use, integrates
LLM-simulated users into the reinforcement learning loop. MUA-RL aims to enable
autonomous learning of models to communicate with users efficiently and use
various tools to solve practical problems in dynamic multi-turn interactions.
Evaluations are done on several multi-turn tool-using benchmarks (see Figure
1). Specifically, MUA-RL-32B achieves 67.3 on TAU2 Retail, 45.4 on TAU2
Airline, 28.3 on TAU2 Telecom, 28.4 on BFCL-V3 Multi Turn, and 82.5 on ACEBench
Agent -- outperforming or matching the performance of larger open-source models
such as DeepSeek-V3-0324 and Qwen3-235B-A22B in non-thinking settings.

</details>


### [97] [AppAgent-Pro: A Proactive GUI Agent System for Multidomain Information Integration and User Assistance](https://arxiv.org/abs/2508.18689)
*Yuyang Zhao,Wentao Shi,Fuli Feng,Xiangnan He*

Main category: cs.AI

TL;DR: This paper introduces AppAgent-Pro, a proactive GUI agent system that actively integrates multi-domain information based on user instructions, overcoming the limitations of purely reactive agents.


<details>
  <summary>Details</summary>
Motivation: most existing agents operate in a purely reactive manner, responding passively to user instructions, which significantly constrains their effectiveness and efficiency as general-purpose platforms for information acquisition

Method: proposes AppAgent-Pro, a proactive GUI agent system that actively integrates multi-domain information based on user instructions

Result: This approach enables the system to proactively anticipate users' underlying needs and conduct in-depth multi-domain information mining, thereby facilitating the acquisition of more comprehensive and intelligent information.

Conclusion: AppAgent-Pro has the potential to fundamentally redefine information acquisition in daily life, leading to a profound impact on human society.

Abstract: Large language model (LLM)-based agents have demonstrated remarkable
capabilities in addressing complex tasks, thereby enabling more advanced
information retrieval and supporting deeper, more sophisticated human
information-seeking behaviors. However, most existing agents operate in a
purely reactive manner, responding passively to user instructions, which
significantly constrains their effectiveness and efficiency as general-purpose
platforms for information acquisition. To overcome this limitation, this paper
proposes AppAgent-Pro, a proactive GUI agent system that actively integrates
multi-domain information based on user instructions. This approach enables the
system to proactively anticipate users' underlying needs and conduct in-depth
multi-domain information mining, thereby facilitating the acquisition of more
comprehensive and intelligent information. AppAgent-Pro has the potential to
fundamentally redefine information acquisition in daily life, leading to a
profound impact on human society. Our code is available at:
https://github.com/LaoKuiZe/AppAgent-Pro. Our code is available at:
https://github.com/LaoKuiZe/AppAgent-Pro. The demonstration video could be
found at:
https://www.dropbox.com/scl/fi/hvzqo5vnusg66srydzixo/AppAgent-Pro-demo-video.mp4?rlkey=o2nlfqgq6ihl125mcqg7bpgqu&st=d29vrzii&dl=0.

</details>


### [98] [VistaWise: Building Cost-Effective Agent with Cross-Modal Knowledge Graph for Minecraft](https://arxiv.org/abs/2508.18722)
*Honghao Fu,Junlong Ren,Qi Chai,Deheng Ye,Yujun Cai,Hao Wang*

Main category: cs.AI

TL;DR: VistaWise, a cost-effective agent framework, integrates cross-modal domain knowledge and finetunes a dedicated object detection model for visual analysis. It reduces the requirement for domain-specific training data from millions of samples to a few hundred and achieves state-of-the-art performance across various open-world tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs' performance is hindered by the absence of domain-specific knowledge. Methods that finetune on large-scale domain-specific data entail prohibitive development costs.

Method: integrates cross-modal domain knowledge and finetunes a dedicated object detection model for visual analysis. It integrates visual information and textual dependencies into a cross-modal knowledge graph (KG), enabling a comprehensive and accurate understanding of multimodal environments. We also equip the agent with a retrieval-based pooling strategy to extract task-related information from the KG, and a desktop-level skill library to support direct operation of the Minecraft desktop client via mouse and keyboard inputs.

Result: It reduces the requirement for domain-specific training data from millions of samples to a few hundred.

Conclusion: VistaWise achieves state-of-the-art performance across various open-world tasks, highlighting its effectiveness in reducing development costs while enhancing agent performance.

Abstract: Large language models (LLMs) have shown significant promise in embodied
decision-making tasks within virtual open-world environments. Nonetheless,
their performance is hindered by the absence of domain-specific knowledge.
Methods that finetune on large-scale domain-specific data entail prohibitive
development costs. This paper introduces VistaWise, a cost-effective agent
framework that integrates cross-modal domain knowledge and finetunes a
dedicated object detection model for visual analysis. It reduces the
requirement for domain-specific training data from millions of samples to a few
hundred. VistaWise integrates visual information and textual dependencies into
a cross-modal knowledge graph (KG), enabling a comprehensive and accurate
understanding of multimodal environments. We also equip the agent with a
retrieval-based pooling strategy to extract task-related information from the
KG, and a desktop-level skill library to support direct operation of the
Minecraft desktop client via mouse and keyboard inputs. Experimental results
demonstrate that VistaWise achieves state-of-the-art performance across various
open-world tasks, highlighting its effectiveness in reducing development costs
while enhancing agent performance.

</details>


### [99] [Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval](https://arxiv.org/abs/2508.18724)
*Karanbir Singh,Deepak Muppiri,William Ngu*

Main category: cs.AI

TL;DR: Introduces a Bias Mitigation Agent, a multi-agent system, that reduces bias by 81.82% compared to a baseline naive retrieval strategy.


<details>
  <summary>Details</summary>
Motivation: LLMs inherit the bias present in both internal and external information sources. This significantly affects the fairness and balance of retrieved information, and hence reduces user trust.

Method: a novel Bias Mitigation Agent, a multi-agent system designed to orchestrate the workflow of bias mitigation through specialized agents that optimize the selection of sources

Result: 81.82% reduction in bias compared to a baseline naive retrieval strategy

Conclusion: The experimental results demonstrate an 81.82% reduction in bias compared to a baseline naive retrieval strategy.

Abstract: Large Language Models (LLMs) have transformed the field of artificial
intelligence by unlocking the era of generative applications. Built on top of
generative AI capabilities, Agentic AI represents a major shift toward
autonomous, goal-driven systems that can reason, retrieve, and act. However,
they also inherit the bias present in both internal and external information
sources. This significantly affects the fairness and balance of retrieved
information, and hence reduces user trust. To address this critical challenge,
we introduce a novel Bias Mitigation Agent, a multi-agent system designed to
orchestrate the workflow of bias mitigation through specialized agents that
optimize the selection of sources to ensure that the retrieved content is both
highly relevant and minimally biased to promote fair and balanced knowledge
dissemination. The experimental results demonstrate an 81.82\% reduction in
bias compared to a baseline naive retrieval strategy.

</details>


### [100] [CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks](https://arxiv.org/abs/2508.18743)
*Sunguk Choi,Yonghoon Kwon,Heondeuk Lee*

Main category: cs.AI

TL;DR: This paper introduces Connector-Aware Compact CoT (CAC-CoT), a method that restricts reasoning to connector phrases for concise explanations. It achieves good performance on both System-1 and System-2 tasks with shorter reasoning traces.


<details>
  <summary>Details</summary>
Motivation: Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs) solve difficult problems, but very long traces often slow or even degrade performance on fast, intuitive "System-1" tasks.

Method: Connector-Aware Compact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a small, fixed set of connector phrases, steering the model toward concise and well -- structured explanations.

Result: CAC-CoT achieves approximately 85% on GSM8K and approximately 40% on GPQA (System-2) while retaining approximately 90% on S1-Bench (System-1).

Conclusion: CAC-CoT achieves approximately 85% on GSM8K and approximately 40% on GPQA (System-2) while retaining approximately 90% on S1-Bench (System-1). Its reasoning traces average approximately 300 tokens(ART), about one-third the length of baseline traces, delivering higher efficiency without loss of accuracy.

Abstract: Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs)
solve difficult problems, but very long traces often slow or even degrade
performance on fast, intuitive "System-1" tasks. We introduce Connector-Aware
Compact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a
small, fixed set of connector phrases, steering the model toward concise and
well -- structured explanations. Despite its simplicity, our synthetic method
with Gemini-2.0-Flash yields a high-quality training quality. CAC-CoT achieves
approximately 85% on GSM8K and approximately 40% on GPQA (System-2) while
retaining approximately 90% on S1-Bench (System-1). Its reasoning traces
average approximately 300 tokens(ART), about one-third the length of baseline
traces, delivering higher efficiency without loss of accuracy.

</details>


### [101] [Reflection-Enhanced Meta-Optimization Integrating TextGrad-style Prompt Optimization with Memory-Driven Self-Evolution](https://arxiv.org/abs/2508.18749)
*Chunlong Wu,Zhibo Qu*

Main category: cs.AI

TL;DR: REMO通过集成记忆增强的RAG模块和自适应优化器，实现了更稳定和鲁棒的泛化，但计算开销增加。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常是无状态的，并且在优化运行中独立运行，缺乏保留和利用历史优化经验的机制，容易过度拟合，产生超出直接任务上下文的泛化性差的提示更新。

Method: 提出了Reflection-Enhanced Meta-Optimization (REMO) 框架，该框架集成了记忆增强的 Reflection Retrieval-Augmented Generation (RAG) 模块（结构为“错误笔记本”）和一个自适应优化器（通过 LLM 驱动的元控制器实现）。

Result: 与 TextGrad 基线相比，REMO 在 GSM8K 基准测试中实现了更稳定和鲁棒的泛化，但计算开销增加。

Conclusion: REMO在GSM8K基准测试中实现了更稳定和鲁棒的泛化，但计算开销增加。

Abstract: Recent advances in prompt optimization, exemplified by methods such as
TextGrad, enable automatic, gradient-like refinement of textual prompts to
enhance the performance of large language models (LLMs) on specific downstream
tasks. However, current approaches are typically stateless and operate
independently across optimization runs, lacking mechanisms to preserve and
leverage historical optimization experience. Furthermore, they are susceptible
to overfitting, often yielding prompt updates that generalize poorly beyond the
immediate task context.
  To address these limitations, we propose Reflection-Enhanced
Meta-Optimization (REMO), a novel framework that integrates (1) a
memory-augmented Reflection Retrieval-Augmented Generation (RAG) module -
structured as a "mistake notebook" and (2) a Self-Adaptive Optimizer,
implemented via an LLM-driven meta-controller that synthesizes epoch-level
reflective insights to iteratively improve system-level prompting strategies.
This architecture enables not only local, fine-grained prompt tuning akin to
TextGrad, but also the systematic accumulation and reuse of cross-run
optimization knowledge, thereby supporting continual improvement over time.
  We instantiate the REMO framework using Qwen3-32B in standard inference mode
- without explicit chain-of-thought prompting - and evaluate its efficacy on
the GSM8K benchmark for mathematical reasoning. Experimental results
demonstrate that, compared to a TextGrad baseline, REMO achieves more stable
and robust generalization, albeit at the cost of increased computational
overhead. We provide a detailed exposition of the algorithmic design, conduct a
qualitative and quantitative analysis of optimization dynamics, and present a
comprehensive ablation study to elucidate the contributions of each component.

</details>


### [102] [Stabilizing Open-Set Test-Time Adaptation via Primary-Auxiliary Filtering and Knowledge-Integrated Prediction](https://arxiv.org/abs/2508.18751)
*Byung-Joon Lee,Jin-Seop Lee,Jee-Hyong Lee*

Main category: cs.AI

TL;DR: This paper tackles open-set Test-Time Adaptation (OSTTA) by proposing a filtering method and a prediction calibration technique to improve accuracy and discrimination. 


<details>
  <summary>Details</summary>
Motivation: Real-world test data often exhibit domain shifts, and open-set data can degrade closed-set accuracy in Test-Time Adaptation (TTA). Existing methods rely on the source model for filtering, resulting in suboptimal filtering accuracy, or use the adapting model, which can be unstable.

Method: The paper proposes Primary-Auxiliary Filtering (PAF), which uses an auxiliary filter to validate data filtered by the primary filter, and Knowledge-Integrated Prediction (KIP), which calibrates the outputs of the adapting model, EMA model, and source model.

Result: The proposed method enhances both closed-set accuracy and open-set discrimination over existing methods across diverse closed-set and open-set datasets.

Conclusion: This paper introduces Primary-Auxiliary Filtering (PAF) and Knowledge-Integrated Prediction (KIP) to improve both closed-set accuracy and open-set discrimination in open-set Test-Time Adaptation (OSTTA). The approach is validated on diverse datasets, demonstrating improvements over existing methods.

Abstract: Deep neural networks demonstrate strong performance under aligned
training-test distributions. However, real-world test data often exhibit domain
shifts. Test-Time Adaptation (TTA) addresses this challenge by adapting the
model to test data during inference. While most TTA studies assume that the
training and test data share the same class set (closed-set TTA), real-world
scenarios often involve open-set data (open-set TTA), which can degrade
closed-set accuracy. A recent study showed that identifying open-set data
during adaptation and maximizing its entropy is an effective solution. However,
the previous method relies on the source model for filtering, resulting in
suboptimal filtering accuracy on domain-shifted test data. In contrast, we
found that the adapting model, which learns domain knowledge from noisy test
streams, tends to be unstable and leads to error accumulation when used for
filtering. To address this problem, we propose Primary-Auxiliary Filtering
(PAF), which employs an auxiliary filter to validate data filtered by the
primary filter. Furthermore, we propose Knowledge-Integrated Prediction (KIP),
which calibrates the outputs of the adapting model, EMA model, and source model
to integrate their complementary knowledge for OSTTA. We validate our approach
across diverse closed-set and open-set datasets. Our method enhances both
closed-set accuracy and open-set discrimination over existing methods. The code
is available at https://github.com/powerpowe/PAF-KIP-OSTTA .

</details>


### [103] [Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating Abstention Failures in Large Reasoning Models](https://arxiv.org/abs/2508.18760)
*Yi Liu,Xiangyu Liu,Zequn Sun,Wei Hu*

Main category: cs.AI

TL;DR: LRMs struggle with unanswerable questions. This paper analyzes why and proposes a method to improve their abstention behavior.


<details>
  <summary>Details</summary>
Motivation: LRMs continually fail to provide appropriate abstentions when confronted with unanswerable questions, revealing a misalignment between their internal cognition and external response.

Method: A lightweight, two-stage method that combines cognitive monitoring with inference-time intervention.

Result: The proposed method significantly improves the abstention rate while maintaining the overall reasoning performance.

Conclusion: This paper proposes a two-stage method combining cognitive monitoring with inference-time intervention to improve the abstention rate of LRMs when facing unanswerable questions, while maintaining overall reasoning performance.

Abstract: Large reasoning models (LRMs) have shown remarkable progress on complex
reasoning tasks. However, some questions posed to LRMs are inherently
unanswerable, such as math problems lacking sufficient conditions. We find that
LRMs continually fail to provide appropriate abstentions when confronted with
these unanswerable questions. In this paper, we systematically analyze,
investigate, and resolve this issue for trustworthy AI. We first conduct a
detailed analysis of the distinct response behaviors of LRMs when facing
unanswerable questions. Then, we show that LRMs possess sufficient cognitive
capabilities to recognize the flaws in these questions. However, they fail to
exhibit appropriate abstention behavior, revealing a misalignment between their
internal cognition and external response. Finally, to resolve this issue, we
propose a lightweight, two-stage method that combines cognitive monitoring with
inference-time intervention. Experimental results demonstrate that our method
significantly improves the abstention rate while maintaining the overall
reasoning performance.

</details>


### [104] [Dynamic Collaboration of Multi-Language Models based on Minimal Complete Semantic Units](https://arxiv.org/abs/2508.18763)
*Chao Hao,Zezheng Wang,Yanhua Huang,Ruiwen Xu,Wenzhe Niu,Xin Liu,Zitong Yu*

Main category: cs.AI

TL;DR: This paper introduces DDS and MCSU to improve reasoning in language models by intelligently combining multiple models at the token level, achieving better performance on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Investigates the enhancement of reasoning capabilities in language models through token-level multi-model collaboration. Addresses the challenge of vocabulary misalignment in multi-model collaboration.

Method: Introduces a distribution distance-based dynamic selection strategy (DDS) and the concept of minimal complete semantic units (MCSU) for token-level multi-model collaboration.

Result: Experimental results across various benchmarks demonstrate the superiority of our method.

Conclusion: The proposed method demonstrates superiority across various benchmarks.

Abstract: This paper investigates the enhancement of reasoning capabilities in language
models through token-level multi-model collaboration. Our approach selects the
optimal tokens from the next token distributions provided by multiple models to
perform autoregressive reasoning. Contrary to the assumption that more models
yield better results, we introduce a distribution distance-based dynamic
selection strategy (DDS) to optimize the multi-model collaboration process. To
address the critical challenge of vocabulary misalignment in multi-model
collaboration, we propose the concept of minimal complete semantic units
(MCSU), which is simple yet enables multiple language models to achieve natural
alignment within the linguistic space. Experimental results across various
benchmarks demonstrate the superiority of our method. The code will be
available at https://github.com/Fanye12/DDS.

</details>


### [105] [AniME: Adaptive Multi-Agent Planning for Long Animation Generation](https://arxiv.org/abs/2508.18781)
*Lisai Zhang,Baohan Xu,Siqian Yang,Mingyu Yin,Jing Liu,Chao Xu,Siqi Wang,Yidi Wu,Yuxin Hong,Zihao Zhang,Yanzhang Liang,Yudong Jiang*

Main category: cs.AI

TL;DR: AniME is a multi-agent system for automated anime production, creating cinematic animation with consistent characters and synchronized audio-visuals.


<details>
  <summary>Details</summary>
Motivation: automated long-form anime production, covering the full workflow from a story to the final video

Method: a director-oriented multi-agent system integrating customized Model Context Protocol (MCP)

Result: produces cinematic animation with consistent characters and synchronized audio visual elements

Conclusion: AniME offers a scalable solution for AI-driven anime creation.

Abstract: We present AniME, a director-oriented multi-agent system for automated
long-form anime production, covering the full workflow from a story to the
final video. The director agent keeps a global memory for the whole workflow,
and coordinates several downstream specialized agents. By integrating
customized Model Context Protocol (MCP) with downstream model instruction, the
specialized agent adaptively selects control conditions for diverse sub-tasks.
AniME produces cinematic animation with consistent characters and synchronized
audio visual elements, offering a scalable solution for AI-driven anime
creation.

</details>


### [106] [CausalMACE: Causality Empowered Multi-Agents in Minecraft Cooperative Tasks](https://arxiv.org/abs/2508.18797)
*Qi Chai,Zhang Zheng,Junlong Ren,Deheng Ye,Zichuan Lin,Hao Wang*

Main category: cs.AI

TL;DR: This paper proposes CausalMACE, a causality planning framework for multi-agent systems in Minecraft, which enhances efficiency and fault tolerance by managing dependencies among subtasks.


<details>
  <summary>Details</summary>
Motivation: Existing works primarily adopt a single Large Language Model (LLM) agent to complete various in-game tasks. However, for complex tasks requiring lengthy sequences of actions, single-agent approaches often face challenges related to inefficiency and limited fault tolerance. Despite these issues, research on multi-agent collaboration remains scarce.

Method: a holistic causality planning framework designed to enhance multi-agent systems, in which we incorporate causality to manage dependencies among subtasks. Technically, our proposed framework introduces two modules: an overarching task graph for global task planning and a causality-based module for dependency management, where inherent rules are adopted to perform causal intervention.

Result: achieves state-of-the-art performance in multi-agent cooperative tasks of Minecraft

Conclusion: The proposed CausalMACE achieves state-of-the-art performance in multi-agent cooperative tasks of Minecraft.

Abstract: Minecraft, as an open-world virtual interactive environment, has become a
prominent platform for research on agent decision-making and execution.
Existing works primarily adopt a single Large Language Model (LLM) agent to
complete various in-game tasks. However, for complex tasks requiring lengthy
sequences of actions, single-agent approaches often face challenges related to
inefficiency and limited fault tolerance. Despite these issues, research on
multi-agent collaboration remains scarce. In this paper, we propose CausalMACE,
a holistic causality planning framework designed to enhance multi-agent
systems, in which we incorporate causality to manage dependencies among
subtasks. Technically, our proposed framework introduces two modules: an
overarching task graph for global task planning and a causality-based module
for dependency management, where inherent rules are adopted to perform causal
intervention. Experimental results demonstrate our approach achieves
state-of-the-art performance in multi-agent cooperative tasks of Minecraft.

</details>


### [107] [STARec: An Efficient Agent Framework for Recommender Systems via Autonomous Deliberate Reasoning](https://arxiv.org/abs/2508.18812)
*Chenghao Wu,Ruiyang Ren,Junjie Zhang,Ruirui Wang,Zhongrui Ma,Qi Ye,Wayne Xin Zhao*

Main category: cs.AI

TL;DR: STARec: a slow-thinking augmented agent framework that endows recommender systems with autonomous deliberative reasoning capabilities and achieves substantial performance gains.


<details>
  <summary>Details</summary>
Motivation: Current large language model (LLM)-based agents inherit these shortcomings through their overreliance on heuristic pattern matching, yielding recommendations prone to shallow correlation bias, limited causal inference, and brittleness in sparse-data scenarios.

Method: a slow-thinking augmented agent framework that endows recommender systems with autonomous deliberative reasoning capabilities. Each user is modeled as an agent with parallel cognitions: fast response for immediate interactions and slow reasoning that performs chain-of-thought rationales. To cultivate intrinsic slow thinking, we develop anchored reinforcement training - a two-stage paradigm combining structured knowledge distillation from advanced reasoning models with preference-aligned reward shaping.

Result: achieves substantial performance gains compared with state-of-the-art baselines, despite using only 0.4% of the full training data.

Conclusion: STARec achieves substantial performance gains compared with state-of-the-art baselines, despite using only 0.4% of the full training data.

Abstract: While modern recommender systems are instrumental in navigating information
abundance, they remain fundamentally limited by static user modeling and
reactive decision-making paradigms. Current large language model (LLM)-based
agents inherit these shortcomings through their overreliance on heuristic
pattern matching, yielding recommendations prone to shallow correlation bias,
limited causal inference, and brittleness in sparse-data scenarios. We
introduce STARec, a slow-thinking augmented agent framework that endows
recommender systems with autonomous deliberative reasoning capabilities. Each
user is modeled as an agent with parallel cognitions: fast response for
immediate interactions and slow reasoning that performs chain-of-thought
rationales. To cultivate intrinsic slow thinking, we develop anchored
reinforcement training - a two-stage paradigm combining structured knowledge
distillation from advanced reasoning models with preference-aligned reward
shaping. This hybrid approach scaffolds agents in acquiring foundational
capabilities (preference summarization, rationale generation) while enabling
dynamic policy adaptation through simulated feedback loops. Experiments on
MovieLens 1M and Amazon CDs benchmarks demonstrate that STARec achieves
substantial performance gains compared with state-of-the-art baselines, despite
using only 0.4% of the full training data.

</details>


### [108] [Judicial Requirements for Generative AI in Legal Reasoning](https://arxiv.org/abs/2508.18880)
*Eljas Linna,Tuula Linna*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型在法律推理中的应用，发现它们在处理复杂和需要自由裁量权的任务时仍存在局限性，目前最适合作为辅助工具。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）正被整合到专业领域，但它们在法律等高风险领域的局限性仍然知之甚少。

Method: 使用IRAC（问题-规则-应用-结论）模型作为分析框架

Result: 虽然RAG、多智能体系统和神经符号AI等技术可以应对特定挑战，但在需要自由裁量权和透明、合理的推理的任务中，仍然存在重大挑战。

Conclusion: AI在法律领域最有效的角色是双重的：作为简单重复案件的大批量助手，以及作为复杂案件中人类专家的复杂“陪练伙伴”。

Abstract: Large Language Models (LLMs) are being integrated into professional domains,
yet their limitations in high-stakes fields like law remain poorly understood.
This paper defines the core capabilities that an AI system must possess to
function as a reliable reasoning tool in judicial decision-making. Using the
IRAC (Issue-Rule-Application-Conclusion) model as an analytical framework, the
study focuses on the most challenging phases of legal adjudication: determining
the applicable Rule (R) and performing the Application (A) of that rule to the
facts of a case. From a judicial perspective, the analysis deconstructs legal
reasoning into a series of core requirements, including the ability to select
the correct legal framework across jurisdictions, generate sound arguments
based on the doctrine of legal sources, distinguish ratio decidendi from obiter
dictum in case law, resolve ambiguity arising from general clauses like
"reasonableness", manage conflicting legal provisions, and correctly apply the
burden of proof. The paper then maps various AI enhancement mechanisms, such as
Retrieval-Augmented Generation (RAG), multi-agent systems, and neuro-symbolic
AI, to these requirements, assessing their potential to bridge the gap between
the probabilistic nature of LLMs and the rigorous, choice-driven demands of
legal interpretation. The findings indicate that while these techniques can
address specific challenges, significant challenges remain, particularly in
tasks requiring discretion and transparent, justifiable reasoning. Our paper
concludes that the most effective current role for AI in law is a dual one: as
a high-volume assistant for simple, repetitive cases and as a sophisticated
"sparring partner" for human experts in complex matters.

</details>


### [109] [Interactive Evaluation of Large Language Models for Multi-Requirement Software Engineering Tasks](https://arxiv.org/abs/2508.18905)
*Dimitrios Rontogiannis,Maxime Peyrard,Nicolas Baldwin,Martin Josifoski,Robert West,Dimitrios Gunopulos*

Main category: cs.AI

TL;DR: The paper introduces a new interactive evaluation framework for LLMs in programming, revealing weaknesses static benchmarks miss.


<details>
  <summary>Details</summary>
Motivation: Standard single-turn, static benchmarks fall short in evaluating the nuanced capabilities of Large Language Models (LLMs) on complex tasks such as software engineering.

Method: The study proposes a novel interactive evaluation framework that assesses LLMs on multi-requirement programming tasks through structured, feedback-driven dialogue. Each task is modeled as a requirement dependency graph, and an interviewer LLM provides minimal, targeted hints to an interviewee model.

Result: The study builds on DevAI, a benchmark of 55 curated programming tasks, by adding ground-truth solutions and evaluating the relevance and utility of interviewer hints through expert annotation. The results uncover strengths and systematic weaknesses that static benchmarks fail to measure.

Conclusion: This work highlights the importance of dynamic evaluation in advancing the development of collaborative code-generating agents.

Abstract: Standard single-turn, static benchmarks fall short in evaluating the nuanced
capabilities of Large Language Models (LLMs) on complex tasks such as software
engineering. In this work, we propose a novel interactive evaluation framework
that assesses LLMs on multi-requirement programming tasks through structured,
feedback-driven dialogue. Each task is modeled as a requirement dependency
graph, and an ``interviewer'' LLM, aware of the ground-truth solution, provides
minimal, targeted hints to an ``interviewee'' model to help correct errors and
fulfill target constraints. This dynamic protocol enables fine-grained
diagnostic insights into model behavior, uncovering strengths and systematic
weaknesses that static benchmarks fail to measure. We build on DevAI, a
benchmark of 55 curated programming tasks, by adding ground-truth solutions and
evaluating the relevance and utility of interviewer hints through expert
annotation. Our results highlight the importance of dynamic evaluation in
advancing the development of collaborative code-generating agents.

</details>


### [110] [FormaRL: Enhancing Autoformalization with no Labeled Data](https://arxiv.org/abs/2508.18914)
*Yanxing Huang,Xinling Jin,Sijie Liang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: FormaRL是一种用于autoformalization的强化学习框架，它使用少量未标记数据，显著提高了准确性，并在out-of-distribution数据上表现良好。


<details>
  <summary>Details</summary>
Motivation: Autoformalization是形式验证中的核心任务之一，但由于数据稀缺和缺乏有效方法，其进展仍然受到阻碍。

Method: 提出了一种基于强化学习的autoformalization框架FormaRL，该框架集成了Lean编译器的语法检查和大型语言模型的一致性检查来计算奖励，并采用GRPO算法来更新formalizer。

Result: FormaRL将Qwen2.5-Coder-7B-Instruct的pass@1 autoformalization 准确率提高了4到6倍(ProofNet上为4.04%→26.15%，uproof上为2.4%→9.6%)，仅使用了859个未标记数据。在uproof上，与现有的开源state-of-the-art autoformalizer相比，我们的方法在pass@1准确率(6.2%→9.6%)和pass@16准确率(24.4%→33.6%)方面也取得了显著的out-of-distribution性能提升。

Conclusion: FormaRL显著提高了autoformalization的准确率，并在out-of-distribution数据集上取得了更好的性能。

Abstract: Autoformalization is one of the central tasks in formal verification, while
its advancement remains hindered due to the data scarcity and the absence
efficient methods. In this work we propose \textbf{FormaRL}, a simple yet
efficient reinforcement learning framework for autoformalization which only
requires a small amount of unlabeled data. FormaRL integrates syntax check from
Lean compiler and consistency check from large language model to calculate the
reward, and adopts GRPO algorithm to update the formalizer. We also curated a
proof problem dataset from undergraduate-level math materials, named
\textbf{uproof}, in the hope to facilitate the exploration of autoformalization
and theorem proving in advanced math. Experiments show that FormaRL can
increase the pass@1 autoformalization accuracy of Qwen2.5-Coder-7B-Instruct by
4 $\sim$ 6x (4.04\% $\to$ 26.15\% on ProofNet and 2.4\% $\to$ 9.6\% on uproof)
with merely 859 unlabeled data. And on uproof our method also achieved a strong
improvement in out-of-distribution performance compared to existing open-source
state-of-the-art autoformalizers on both pass@1 accuracy (6.2\% $\to$ 9.6\%)
and pass@16 accuracy (24.4\% $\to$ 33.6\%). Training code of FormaRL is
open-sourced at https://github.com/THUNLP-MT/FormaRL.

</details>


### [111] [Who Is Lagging Behind: Profiling Student Behaviors with Graph-Level Encoding in Curriculum-Based Online Learning Systems](https://arxiv.org/abs/2508.18925)
*Qian Xiao,Conn Breathnach,Ioana Ghergulescu,Conor O'Sullivan,Keith Johnston,Vincent Wade*

Main category: cs.AI

TL;DR: 提出CTGraph，以图级别表示学习方法来分析学生行为和表现，从而能够跟踪学生学习过程、识别困难学生并进行比较分析。


<details>
  <summary>Details</summary>
Motivation: 智能辅导系统（ITS）在教育中的应用激增，但在基于课程的学习中，可能会无意中加剧成绩差距。为了解决这个问题，学生画像对于跟踪进度、识别困难学生和缓解学生之间的差异至关重要。

Method: 提出了一种图级表征学习方法CTGraph，以自监督的方式分析学习者的行为和表现。

Result: CTGraph可以全面了解学生的学习过程，能够识别有困难的学生，并对不同群体进行比较分析，以确定学生在何时何地遇到困难。

Conclusion: CTGraph可以全面了解学生的学习过程，能够识别有困难的学生，并对不同群体进行比较分析，以确定学生在何时何地遇到困难。为教育工作者提供深入了解学生学习过程的途径，并为更有针对性的干预措施铺平道路。

Abstract: The surge in the adoption of Intelligent Tutoring Systems (ITSs) in
education, while being integral to curriculum-based learning, can inadvertently
exacerbate performance gaps. To address this problem, student profiling becomes
crucial for tracking progress, identifying struggling students, and alleviating
disparities among students. Such profiling requires measuring student behaviors
and performance across different aspects, such as content coverage, learning
intensity, and proficiency in different concepts within a learning topic.
  In this study, we introduce CTGraph, a graph-level representation learning
approach to profile learner behaviors and performance in a self-supervised
manner. Our experiments demonstrate that CTGraph can provide a holistic view of
student learning journeys, accounting for different aspects of student
behaviors and performance, as well as variations in their learning paths as
aligned to the curriculum structure. We also show that our approach can
identify struggling students and provide comparative analysis of diverse groups
to pinpoint when and where students are struggling. As such, our approach opens
more opportunities to empower educators with rich insights into student
learning journeys and paves the way for more targeted interventions.

</details>


### [112] [VISION: Robust and Interpretable Code Vulnerability Detection Leveraging Counterfactual Augmentation](https://arxiv.org/abs/2508.18933)
*David Egea,Barproda Halder,Sanghamitra Dutta*

Main category: cs.AI

TL;DR: This paper introduces VISION, a framework that uses counterfactual training data generated by LLMs to improve the robustness and accuracy of GNNs for vulnerability detection in source code. VISION reduces spurious learning and enables more robust, generalizable detection.


<details>
  <summary>Details</summary>
Motivation: Automated detection of vulnerabilities in source code is an essential cybersecurity challenge. GNNs are constrained by training data imbalances and label noise, often learning 'spurious' correlations from superficial code similarities, producing detectors that fail to generalize well to unseen real-world data.

Method: a unified framework called VISION, to mitigate spurious correlations by systematically augmenting a counterfactual training dataset. Includes: (i) generating counterfactuals by prompting a Large Language Model (LLM); (ii) targeted GNN training on paired code examples with opposite labels; and (iii) graph-based interpretability to identify the crucial code statements relevant for vulnerability predictions while ignoring spurious ones.

Result: VISION reduces spurious learning and enables more robust, generalizable detection, improving overall accuracy, pairwise contrast accuracy, and worst-group accuracy on the CWE-20 vulnerability. They also release CWE-20-CFA, a benchmark of 27,556 functions. They further demonstrate gains using proposed metrics: intra-class attribution variance, inter-class attribution distance, and node score dependency.

Conclusion: VISION reduces spurious learning and enables more robust, generalizable detection, improving overall accuracy (from 51.8% to 97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group accuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20 vulnerability. VISION advances transparent and trustworthy AI-based cybersecurity systems through interactive visualization for human-in-the-loop analysis.

Abstract: Automated detection of vulnerabilities in source code is an essential
cybersecurity challenge, underpinning trust in digital systems and services.
Graph Neural Networks (GNNs) have emerged as a promising approach as they can
learn structural and logical code relationships in a data-driven manner.
However, their performance is severely constrained by training data imbalances
and label noise. GNNs often learn 'spurious' correlations from superficial code
similarities, producing detectors that fail to generalize well to unseen
real-world data. In this work, we propose a unified framework for robust and
interpretable vulnerability detection, called VISION, to mitigate spurious
correlations by systematically augmenting a counterfactual training dataset.
Counterfactuals are samples with minimal semantic modifications but opposite
labels. Our framework includes: (i) generating counterfactuals by prompting a
Large Language Model (LLM); (ii) targeted GNN training on paired code examples
with opposite labels; and (iii) graph-based interpretability to identify the
crucial code statements relevant for vulnerability predictions while ignoring
spurious ones. We find that VISION reduces spurious learning and enables more
robust, generalizable detection, improving overall accuracy (from 51.8% to
97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group
accuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20
vulnerability. We further demonstrate gains using proposed metrics: intra-class
attribution variance, inter-class attribution distance, and node score
dependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real
and counterfactual) from the high-impact CWE-20 category. Finally, VISION
advances transparent and trustworthy AI-based cybersecurity systems through
interactive visualization for human-in-the-loop analysis.

</details>


### [113] [Novel Approaches to Artificial Intelligence Development Based on the Nearest Neighbor Method](https://arxiv.org/abs/2508.18953)
*I. I. Priezzhev,D. A. Danko,A. V. Shubin*

Main category: cs.AI

TL;DR: This paper introduces a nearest neighbor approach with hierarchical clustering to address limitations of neural networks, achieving faster search times with minimal accuracy loss, and offering better transparency and interpretability.


<details>
  <summary>Details</summary>
Motivation: Modern neural networks face limitations such as hallucination, high computational complexity, costly fine-tuning, and catastrophic forgetting, hindering their use in critical areas.

Method: The paper proposes a nearest neighbors method with hierarchical clustering structures using tree-like data structures based on Kohonen self-organizing maps to accelerate nearest neighbor searches.

Result: Tests on handwritten digit recognition and subtitle translation showed a slight reduction in accuracy but hundreds of times faster nearest neighbor search time compared to exhaustive search methods.

Conclusion: The proposed nearest neighbor method with hierarchical clustering achieves comparable accuracy with significantly reduced search time, offering transparency, interpretability, and potential for use in tasks requiring high reliability and explainability.

Abstract: Modern neural network technologies, including large language models, have
achieved remarkable success in various applied artificial intelligence
applications, however, they face a range of fundamental limitations. Among them
are hallucination effects, high computational complexity of training and
inference, costly fine-tuning, and catastrophic forgetting issues. These
limitations significantly hinder the use of neural networks in critical areas
such as medicine, industrial process management, and scientific research. This
article proposes an alternative approach based on the nearest neighbors method
with hierarchical clustering structures. Employing the k-nearest neighbors
algorithm significantly reduces or completely eliminates hallucination effects
while simplifying model expansion and fine-tuning without the need for
retraining the entire network. To overcome the high computational load of the
k-nearest neighbors method, the paper proposes using tree-like data structures
based on Kohonen self-organizing maps, thereby greatly accelerating nearest
neighbor searches. Tests conducted on handwritten digit recognition and simple
subtitle translation tasks confirmed the effectiveness of the proposed
approach. With only a slight reduction in accuracy, the nearest neighbor search
time was reduced hundreds of times compared to exhaustive search methods. The
proposed method features transparency and interpretability, closely aligns with
human cognitive mechanisms, and demonstrates potential for extensive use in
tasks requiring high reliability and explainable results.

</details>


### [114] [Enabling MoE on the Edge via Importance-Driven Expert Scheduling](https://arxiv.org/abs/2508.18983)
*Guoying Zhu,Meng Li,Haipeng Dai,Xuechen Liu,Weijun Wang,Keran Li,Jun xiao,Ligeng Chen,Wei Wang*

Main category: cs.AI

TL;DR: This paper proposes an expert offloading method guided by expert importance and a scheduling policy to improve MoE deployment on edge devices, achieving lower latency and high cache hit rate with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Deploying Mixture of Experts (MoE) on consumer-grade edge hardware is constrained by limited device memory, making dynamic expert offloading essential.

Method: The paper leverages expert importance to guide offloading decisions, substituting low-importance activated experts with functionally similar ones already cached in GPU memory. A scheduling policy that maximizes the reuse ratio of GPU-cached experts is also introduced.

Result: Reduces memory usage and data transfer, while largely eliminating PCIe overhead; 48% lower decoding latency with over 60% expert cache hit rate, while maintaining nearly lossless accuracy.

Conclusion: The proposed approach delivers 48% lower decoding latency with over 60% expert cache hit rate, while maintaining nearly lossless accuracy.

Abstract: The Mixture of Experts (MoE) architecture has emerged as a key technique for
scaling Large Language Models by activating only a subset of experts per query.
Deploying MoE on consumer-grade edge hardware, however, is constrained by
limited device memory, making dynamic expert offloading essential. Unlike prior
work that treats offloading purely as a scheduling problem, we leverage expert
importance to guide decisions, substituting low-importance activated experts
with functionally similar ones already cached in GPU memory, thereby preserving
accuracy. As a result, this design reduces memory usage and data transfer,
while largely eliminating PCIe overhead. In addition, we introduce a scheduling
policy that maximizes the reuse ratio of GPU-cached experts, further boosting
efficiency. Extensive evaluations show that our approach delivers 48% lower
decoding latency with over 60% expert cache hit rate, while maintaining nearly
lossless accuracy.

</details>


### [115] [AI Models Exceed Individual Human Accuracy in Predicting Everyday Social Norms](https://arxiv.org/abs/2508.19004)
*Pontus Strimling,Simon Karlsson,Irina Vartanova,Kimmo Eriksson*

Main category: cs.AI

TL;DR: 大型语言模型在预测社会规范方面表现出色，但仍存在局限性，暗示了文化知识可以通过语言进行传播。


<details>
  <summary>Details</summary>
Motivation: 探讨认知科学中的一个基本问题：社会规范是如何获得和表示的。研究大型语言模型是否可以通过纯粹的统计学习来实现对社会规范的理解。

Method: 通过评估多个AI系统预测555个日常场景中人类社会适当性判断的能力，并将其预测结果与人类参与者的平均判断进行比较。

Result: GPT-4.5在预测集体判断方面的准确性超过了所有人类参与者（第100百分位），Gemini 2.5 Pro超过了98.7%的人类，GPT-5超过了97.8%，Claude Sonnet 4超过了96.0%。所有模型都表现出系统性的相关错误。

Conclusion: 大型语言模型可以通过纯粹的统计学习，在预测人类社会适当性判断方面超越绝大多数个体人类，这表明语言是文化知识传播的丰富知识库，同时也揭示了基于模式的社会理解的潜在局限性。

Abstract: A fundamental question in cognitive science concerns how social norms are
acquired and represented. While humans typically learn norms through embodied
social experience, we investigated whether large language models can achieve
sophisticated norm understanding through statistical learning alone. Across two
studies, we systematically evaluated multiple AI systems' ability to predict
human social appropriateness judgments for 555 everyday scenarios by examining
how closely they predicted the average judgment compared to each human
participant. In Study 1, GPT-4.5's accuracy in predicting the collective
judgment on a continuous scale exceeded that of every human participant (100th
percentile). Study 2 replicated this, with Gemini 2.5 Pro outperforming 98.7%
of humans, GPT-5 97.8%, and Claude Sonnet 4 96.0%. Despite this predictive
power, all models showed systematic, correlated errors. These findings
demonstrate that sophisticated models of social cognition can emerge from
statistical learning over linguistic data alone, challenging strong versions of
theories emphasizing the exclusive necessity of embodied experience for
cultural competence. The systematic nature of AI limitations across different
architectures indicates potential boundaries of pattern-based social
understanding, while the models' ability to outperform nearly all individual
humans in this predictive task suggests that language serves as a remarkably
rich repository for cultural knowledge transmission.

</details>


### [116] [Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark](https://arxiv.org/abs/2508.19005)
*Yuxuan Cai,Yipeng Hao,Jie Zhou,Hang Yan,Zhikai Lei,Rui Zhen,Zhenhua Han,Yutao Yang,Junsong Li,Qianjun Pan,Tianyu Huai,Qin Chen,Xin Li,Kai Chen,Bo Zhang,Xipeng Qiu,Liang He*

Main category: cs.AI

TL;DR: This paper introduces Experience-driven Lifelong Learning (ELL), a framework for building self-evolving agents, and StuLife, a benchmark dataset for ELL that simulates a student's holistic college journey.


<details>
  <summary>Details</summary>
Motivation: The focus is shifting from systems optimized for static tasks to creating open-ended agents that learn continuously.

Method: The paper introduces Experience-driven Lifelong Learning (ELL), a framework built on four core principles: Experience Exploration, Long-term Memory, Skill Learning, and Knowledge Internalization.

Result: The paper evaluates SOTA LLMs on the StuLife benchmark.

Conclusion: The paper introduces StuLife, a benchmark dataset for ELL that simulates a student's holistic college journey. The paper evaluates SOTA LLMs on the StuLife benchmark and explores the role of context engineering in advancing AGI.

Abstract: As AI advances toward general intelligence, the focus is shifting from
systems optimized for static tasks to creating open-ended agents that learn
continuously. In this paper, we introduce Experience-driven Lifelong Learning
(ELL), a framework for building self-evolving agents capable of continuous
growth through real-world interaction. The framework is built on four core
principles: (1) Experience Exploration: Agents learn through continuous,
self-motivated interaction with dynamic environments, navigating interdependent
tasks and generating rich experiential trajectories. (2) Long-term Memory:
Agents preserve and structure historical knowledge, including personal
experiences, domain expertise, and commonsense reasoning, into a persistent
memory system. (3) Skill Learning: Agents autonomously improve by abstracting
recurring patterns from experience into reusable skills, which are actively
refined and validated for application in new tasks. (4) Knowledge
Internalization: Agents internalize explicit and discrete experiences into
implicit and intuitive capabilities as "second nature".
  We also introduce StuLife, a benchmark dataset for ELL that simulates a
student's holistic college journey, from enrollment to academic and personal
development, across three core phases and ten detailed sub-scenarios. StuLife
is designed around three key paradigm shifts: From Passive to Proactive, From
Context to Memory, and From Imitation to Learning. In this dynamic environment,
agents must acquire and distill practical skills and maintain persistent memory
to make decisions based on evolving state variables. StuLife provides a
comprehensive platform for evaluating lifelong learning capabilities, including
memory retention, skill transfer, and self-motivated behavior. Beyond
evaluating SOTA LLMs on the StuLife benchmark, we also explore the role of
context engineering in advancing AGI.

</details>


### [117] [Sense of Self and Time in Borderline Personality. A Comparative Robustness Study with Generative AI](https://arxiv.org/abs/2508.19008)
*Marcin Moskalewicz,Anna Sterna,Marek Pokropski,Paula Flores*

Main category: cs.AI

TL;DR: 本研究评估了大型语言模型在边缘型人格障碍现象学分析中的应用，发现Gemini模型表现最佳，可以辅助人工分析并减轻偏差。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）在支持对边缘型人格障碍（BPD）患者的第一人称体验进行现象学定性分析的能力，BPD被理解为一种时间和自我障碍。

Method: 使用三种大型语言模型（GPT-4o、Gemini 2.5 Pro、Claude Opus 4）模仿研究人员的解释风格，并与先前对24名住院患者的生活故事访谈进行的人工主题分析进行比较。通过语义一致性、Jaccard系数和多维度有效性评级（可信度、连贯性、实质性和数据基础）进行评估。

Result: 模型与人工分析的重叠程度不一，Gemini的输出与人工分析最相似，有效性得分显着高于GPT和Claude，并且被蒙眼的专家判断为人工分析。所有分数与每个主题的文本量和字数高度相关。

Conclusion: 大型语言模型在一定程度上可以辅助对边缘型人格障碍患者的第一人称体验进行现象学定性分析，并且可以减轻人类的解释偏差。

Abstract: This study examines the capacity of large language models (LLMs) to support
phenomenological qualitative analysis of first-person experience in Borderline
Personality Disorder (BPD), understood as a disorder of temporality and
selfhood. Building on a prior human-led thematic analysis of 24 inpatients'
life-story interviews, we compared three LLMs (OpenAI GPT-4o, Google Gemini 2.5
Pro, Anthropic Claude Opus 4) prompted to mimic the interpretative style of the
original investigators. The models were evaluated with blinded and non-blinded
expert judges in phenomenology and clinical psychology. Assessments included
semantic congruence, Jaccard coefficients, and multidimensional validity
ratings (credibility, coherence, substantiveness, and groundness in data).
Results showed variable overlap with the human analysis, from 0 percent in GPT
to 42 percent in Claude and 58 percent in Gemini, and a low Jaccard coefficient
(0.21-0.28). However, the models recovered themes omitted by humans. Gemini's
output most closely resembled the human analysis, with validity scores
significantly higher than GPT and Claude (p < 0.0001), and was judged as human
by blinded experts. All scores strongly correlated (R > 0.78) with the quantity
of text and words per theme, highlighting both the variability and potential of
AI-augmented thematic analysis to mitigate human interpretative bias.

</details>


### [118] [MAB Optimizer for Estimating Math Question Difficulty via Inverse CV without NLP](https://arxiv.org/abs/2508.19014)
*Surajit Das,Gourav Roy,Aleksei Eliseev,Ram Kumar Rajendran*

Main category: cs.AI

TL;DR: 提出了一种新的、领域无关的、自监督的难度评估方法APME，该方法利用强化学习和多臂老虎机框架，仅基于求解器性能数据来估计问题难度，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 技术和教育的发展推动了智能和自主辅导系统（IATS）的出现，其中确定问题难度的客观和领域无关的方法至关重要。传统的人工标注是主观的，现有的基于NLP的方法在代数等符号领域中失效。

Method: 引入了一种基于强化学习的多臂老虎机（MAB）框架APME，该框架仅从求解器的性能数据（获得的分数和花费的时间）来估计难度，而不需要语言特征或专家标签。利用变异系数的倒数作为风险调整指标。

Result: 在三个异构数据集上进行了实证验证，平均R2为0.9213，平均RMSE为0.0584，证实了其鲁棒性、准确性和对不同教育水平和评估形式的适应性。与基于回归、NLP驱动和IRT模型的基线方法相比，该框架始终优于其他方法，尤其是在纯符号领域。

Conclusion: 该模型与维果茨基的最近发展区理论相符，通过识别平衡挑战性和可实现性的任务来支持学生的学习动机，同时最大限度地减少脱离。这种领域无关、自监督的方法推进了IATS中的难度标签，并且可以扩展到任何可以获得求解器交互数据的领域。

Abstract: The evolution of technology and education is driving the emergence of
Intelligent & Autonomous Tutoring Systems (IATS), where objective and
domain-agnostic methods for determining question difficulty are essential.
Traditional human labeling is subjective, and existing NLP-based approaches
fail in symbolic domains like algebra. This study introduces the Approach of
Passive Measures among Educands (APME), a reinforcement learning-based
Multi-Armed Bandit (MAB) framework that estimates difficulty solely from solver
performance data -- marks obtained and time taken -- without requiring
linguistic features or expert labels. By leveraging the inverse coefficient of
variation as a risk-adjusted metric, the model provides an explainable and
scalable mechanism for adaptive assessment. Empirical validation was conducted
on three heterogeneous datasets. Across these diverse contexts, the model
achieved an average R2 of 0.9213 and an average RMSE of 0.0584, confirming its
robustness, accuracy, and adaptability to different educational levels and
assessment formats. Compared with baseline approaches-such as regression-based,
NLP-driven, and IRT models-the proposed framework consistently outperformed
alternatives, particularly in purely symbolic domains. The findings highlight
that (i) item heterogeneity strongly influences perceived difficulty, and (ii)
variance in solver outcomes is as critical as mean performance for adaptive
allocation. Pedagogically, the model aligns with Vygotskys Zone of Proximal
Development by identifying tasks that balance challenge and attainability,
supporting motivation while minimizing disengagement. This domain-agnostic,
self-supervised approach advances difficulty tagging in IATS and can be
extended beyond algebra wherever solver interaction data is available

</details>


### [119] [Investigating Advanced Reasoning of Large Language Models via Black-Box Interaction](https://arxiv.org/abs/2508.19035)
*Congchi Yin,Tianyi Wu,Yankai Shu,Alex Gu,Yunhan Wang,Jun Shao,Xun Jiang,Piji Li*

Main category: cs.AI

TL;DR: 提出了黑盒交互评估范式，以评估 LLM 在交互式未知环境中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有任务不足以评估大型语言模型 (LLM) 在交互式未知环境中的推理能力。这种缺陷导致对演绎、归纳和溯因推理的孤立评估，忽略了人类发现现实世界不可或缺的综合推理过程。

Method: 提出了一种新的评估范式，即黑盒交互，通过与黑盒交互并推理观察到的输入输出对来解开隐藏函数。

Result: o3 在 6 项任务中的 5 项中排名第一，在大多数简单的黑盒上实现了超过 70% 的准确率。但在一些困难的黑盒任务中仍然struggle，平均表现下降到 40% 以下。

Conclusion: LLMs 在一些困难的黑盒任务中表现不佳，缺乏高效和自适应的探索策略来改进假设。

Abstract: Existing tasks fall short in evaluating reasoning ability of Large Language
Models (LLMs) in an interactive, unknown environment. This deficiency leads to
the isolated assessment of deductive, inductive, and abductive reasoning,
neglecting the integrated reasoning process that is indispensable for humans
discovery of real world. We introduce a novel evaluation paradigm,
\textit{black-box interaction}, to tackle this challenge. A black-box is
defined by a hidden function that maps a specific set of inputs to outputs.
LLMs are required to unravel the hidden function behind the black-box by
interacting with it in given exploration turns, and reasoning over observed
input-output pairs. Leveraging this idea, we build the \textsc{Oracle}
benchmark which comprises 6 types of black-box task and 96 black-boxes. 19
modern LLMs are benchmarked. o3 ranks first in 5 of the 6 tasks, achieving over
70\% accuracy on most easy black-boxes. But it still struggles with some hard
black-box tasks, where its average performance drops below 40\%. Further
analysis indicates a universal difficulty among LLMs: They lack the high-level
planning capability to develop efficient and adaptive exploration strategies
for hypothesis refinement.

</details>


### [120] [A Concurrent Modular Agent: Framework for Autonomous LLM Agents](https://arxiv.org/abs/2508.19042)
*Norihiro Maruyama,Takahide Yoshida,Hiroki Sato,Atsushi Masumori,Johnsmith,Takashi Ikegami*

Main category: cs.AI

TL;DR: 提出了并发模块化Agent（CMA），一个协调多个基于大型语言模型（LLM）的模块的框架，这些模块完全异步运行，但保持连贯和容错的行为循环。


<details>
  <summary>Details</summary>
Motivation: 解决Agent架构中长期存在的难题，通过让意图从自主过程之间以语言为媒介的交互中产生。

Method: 通过结合并发执行的模块来实现，这些模块将推理转移到LLM、模块间通信和单一共享全局状态。

Result: 该框架通过在自主过程之间以语言为媒介的交互中产生意图，从而实现灵活、适应性和上下文相关的行为。

Conclusion: 通过实验证明了系统的可行性，并观察到涌现特性，这表明复杂的认知现象可能确实来自简单过程的组织交互，支持了Minsky-Society of Mind的概念，并为人工智能研究开辟了新途径。

Abstract: We introduce the Concurrent Modular Agent (CMA), a framework that
orchestrates multiple Large-Language-Model (LLM)-based modules that operate
fully asynchronously yet maintain a coherent and fault-tolerant behavioral
loop. This framework addresses long-standing difficulties in agent
architectures by letting intention emerge from language-mediated interactions
among autonomous processes. This approach enables flexible, adaptive, and
context-dependent behavior through the combination of concurrently executed
modules that offload reasoning to an LLM, inter-module communication, and a
single shared global state.We consider this approach to be a practical
realization of Minsky's Society of Mind theory. We demonstrate the viability of
our system through two practical use-case studies. The emergent properties
observed in our system suggest that complex cognitive phenomena like
self-awareness may indeed arise from the organized interaction of simpler
processes, supporting Minsky-Society of Mind concept and opening new avenues
for artificial intelligence research. The source code for our work is available
at: https://github.com/AlternativeMachine/concurrent-modular-agent.

</details>


### [121] [Can Structured Templates Facilitate LLMs in Tackling Harder Tasks? : An Exploration of Scaling Laws by Difficulty](https://arxiv.org/abs/2508.19069)
*Zhichao Yang,Zhaoxin Fan,Gen Li,Yuanze Hu,Xinyu Wang,Ye Qiu,Xin Wang,Yifan Sun,Wenjun Wu*

Main category: cs.AI

TL;DR: 本文提出了结构化解决方案模板（SST）框架，该框架使用解决方案模板和不同难度的课程来显式地教授程序推理，从而显著提高了准确性和效率，尤其是在较难的问题上。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的结构化程序推理至关重要，尤其是在数学方面。虽然事后训练方法提高了LLM的性能，但它们在捕获复杂任务的深层程序逻辑方面仍然不足。

Method: 结构化解决方案模板（SST）框架，它使用解决方案模板和不同难度的课程来显式地教授程序推理。

Result: 模型性能遵循关于训练数据复杂性的U形曲线——过多的低难度数据会阻碍抽象，而高难度数据会显著提高推理能力。

Conclusion: SST显著提高了准确性和效率，尤其是在较难的问题上。

Abstract: Structured, procedural reasoning is essential for Large Language Models
(LLMs), especially in mathematics. While post-training methods have improved
LLM performance, they still fall short in capturing deep procedural logic on
complex tasks. To tackle the issue, in this paper, we first investigate this
limitation and uncover a novel finding: a Scaling Law by Difficulty, which
reveals that model performance follows a U-shaped curve with respect to
training data complexity -- excessive low-difficulty data impedes abstraction,
while high-difficulty data significantly enhances reasoning ability. Motivated
by this, we propose the Structured Solution Template (SST) framework, which
uses solution templates and a curriculum of varied difficulty to explicitly
teach procedural reasoning. Specifically, SST comprises (1) fine-tuning with
structured solution-template chains and dynamically weighted loss to prioritize
procedural logic, (2) prompt-time injection of solution templates as cognitive
scaffolds to guide inference, and (3) integrated curriculum fine-tuning that
explicitly teaches the model to self-plan - execute - self-correct. Experiments
on GSM8K, AIME24, and new Dynamic En benchmark show that SST significantly
improves both accuracy and efficiency, especially on harder problems.

</details>


### [122] [Trustworthy Agents for Electronic Health Records through Confidence Estimation](https://arxiv.org/abs/2508.19096)
*Yongwoo Song,Minbyul Jeong,Mujeen Sung*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的评估指标HCAcc@k%，并介绍了一种名为TrustEHRAgent的置信度感知代理，以解决大型语言模型在医疗保健应用中的幻觉问题。实验结果表明，TrustEHRAgent在可靠性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在从电子健康记录（EHR）中提取信息和支持临床决策方面显示出希望，但由于幻觉风险，在临床环境中的部署面临挑战。

Method: 他们引入了TrustEHRAgent，这是一种置信度感知代理，它结合了用于临床问题解答的逐步置信度估计。

Result: 在MIMIC-III和eICU数据集上的实验表明，TrustEHRAgent在严格的可靠性约束下优于基线方法，在HCAcc@70%时实现了44.23%p和25.34%p的改进，而基线方法在这些阈值下失败。

Conclusion: 这篇论文表明，在医疗保健领域，传统的准确性指标在评估人工智能代理时存在局限性。他们提出了一种名为TrustEHRAgent的置信度感知代理，该代理在严格的可靠性约束下优于基线方法，从而有助于开发可信赖的临床代理。

Abstract: Large language models (LLMs) show promise for extracting information from
Electronic Health Records (EHR) and supporting clinical decisions. However,
deployment in clinical settings faces challenges due to hallucination risks. We
propose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metric
quantifying the accuracy-reliability trade-off at varying confidence
thresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporating
stepwise confidence estimation for clinical question answering. Experiments on
MIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines under
strict reliability constraints, achieving improvements of 44.23%p and 25.34%p
at HCAcc@70% while baseline methods fail at these thresholds. These results
highlight limitations of traditional accuracy metrics in evaluating healthcare
AI agents. Our work contributes to developing trustworthy clinical agents that
deliver accurate information or transparently express uncertainty when
confidence is low.

</details>


### [123] [Reasoning LLMs in the Medical Domain: A Literature Survey](https://arxiv.org/abs/2508.19097)
*Armin Berger,Sarthak Khanna,David Berghaus,Rafet Sifa*

Main category: cs.AI

TL;DR: 本调查考察了医学LLM从基本信息检索工具到复杂临床推理系统的转变，评估了技术基础、专用提示技术和评估方法，并解决了该领域的持续挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 中出现的高级推理能力标志着医疗保健应用领域的一项变革性发展。除了扩展功能能力外，这些推理机制还增强了决策透明度和可解释性，而这在医疗环境中至关重要。

Method: 对医学LLM的支持技术基础进行了全面分析，特别关注了Chain-of-Thought等专门的提示技术以及DeepSeek-R1等强化学习的最新突破。调查评估了专门构建的医疗框架，同时也考察了多智能体协作系统和创新提示架构等新兴范式。

Result: 医学LLM已从基本信息检索工具转变为能够支持复杂医疗决策的复杂临床推理系统。调查 критически 评估了当前医学验证的评估方法，并解决了领域解释局限性、偏差缓解策略、患者安全框架以及多模态临床数据集成方面持续存在的挑战。

Conclusion: 本研究旨在为开发可靠的LLM建立路线图，使其能够成为临床实践和医学研究中的有效伙伴。

Abstract: The emergence of advanced reasoning capabilities in Large Language Models
(LLMs) marks a transformative development in healthcare applications. Beyond
merely expanding functional capabilities, these reasoning mechanisms enhance
decision transparency and explainability-critical requirements in medical
contexts. This survey examines the transformation of medical LLMs from basic
information retrieval tools to sophisticated clinical reasoning systems capable
of supporting complex healthcare decisions. We provide a thorough analysis of
the enabling technological foundations, with a particular focus on specialized
prompting techniques like Chain-of-Thought and recent breakthroughs in
Reinforcement Learning exemplified by DeepSeek-R1. Our investigation evaluates
purpose-built medical frameworks while also examining emerging paradigms such
as multi-agent collaborative systems and innovative prompting architectures.
The survey critically assesses current evaluation methodologies for medical
validation and addresses persistent challenges in field interpretation
limitations, bias mitigation strategies, patient safety frameworks, and
integration of multimodal clinical data. Through this survey, we seek to
establish a roadmap for developing reliable LLMs that can serve as effective
partners in clinical practice and medical research.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [124] [Metrics, KPIs, and Taxonomy for Data Valuation and Monetisation -- A Systematic Literature Review](https://arxiv.org/abs/2508.18331)
*Eduardo Vyhmeister,Bastien Pietropaoli,Alejando Martinez Molina,Montserrat Gonzalez-Ferreiro,Gabriel Gonzalez-Castane,Jordi Arjona Aroca,Andrea Visentin*

Main category: cs.DB

TL;DR: 本文对数据估值和货币化中的指标和 KPI 进行了系统的文献综述，创建了一个大型分类法，并讨论了为数据估值和数据货币化创建标准框架的困难，以及该领域目前面临的主要挑战。


<details>
  <summary>Details</summary>
Motivation: 数据估值和数据货币化是当今大多数组织都必不可少的复杂主题。不幸的是，它们仍然缺乏组织可以遵循的标准程序和框架。

Method: 我们对数据估值和货币化中使用的指标和KPI进行了系统的文献综述。

Result: 我们提供了一个包含 162 个参考文献的此类指标和 KPI 的扩展列表。然后，我们按照平衡计分卡 (BSC) 方法将所有找到的指标和 KPI 分类到一个大型分类法中，并进一步进行子聚类以涵盖组织业务的各个方面。

Conclusion: 我们讨论了为数据估值和数据货币化创建标准框架的困难，以及该领域目前面临的主要挑战。

Abstract: Data valuation and data monetisation are complex subjects but essential to
most organisations today. Unfortunately, they still lack standard procedures
and frameworks for organisations to follow. In this survey, we introduce the
reader to the concepts by providing the definitions and the background required
to better understand data, monetisation strategies, and finally metrics and
KPIs used in these strategies. We have conducted a systematic literature review
on metrics and KPIs used in data valuation and monetisation, in every aspect of
an organisation's business, and by a variety of stakeholders. We provide an
expansive list of such metrics and KPIs with 162 references. We then categorise
all the metrics and KPIs found into a large taxonomy, following the Balanced
Scorecard (BSC) approach with further subclustering to cover every aspect of an
organisation's business. This taxonomy will help every level of data management
understand the complex landscape of the domain. We also discuss the difficulty
in creating a standard framework for data valuation and data monetisation and
the major challenges the domain is currently facing.

</details>


### [125] [DiskJoin: Large-scale Vector Similarity Join with SSD](https://arxiv.org/abs/2508.18494)
*Yanqi Chen,Xiao Yan,Alexandra Meliou,Eric Lo*

Main category: cs.DB

TL;DR: DiskJoin is a disk-based similarity join algorithm that can process billion-scale vector datasets efficiently on a single machine.


<details>
  <summary>Details</summary>
Motivation: Prior work has explored distributed computation methods to scale similarity join to large data volumes but these methods require a cluster deployment, and efficiency suffers from expensive inter-machine communication. On the other hand, disk-based solutions are more cost-effective by using a single machine and storing the large dataset on high-performance external storage, such as NVMe SSDs, but in these methods the disk I/O time is a serious bottleneck.

Method: DiskJoin improves disk I/O by tailoring the data access patterns to avoid repetitive accesses and read amplification. It also uses main memory as a dynamic cache and carefully manages cache eviction to improve cache hit rate and reduce disk retrieval time. For further acceleration, we adopt a probabilistic pruning technique that can effectively prune a large number of vector pairs from computation.

Result: DiskJoin is the first disk-based similarity join algorithm that can process billion-scale vector datasets efficiently on a single machine.

Conclusion: DiskJoin significantly outperforms alternatives, achieving speedups from 50x to 1000x.

Abstract: Similarity join--a widely used operation in data science--finds all pairs of
items that have distance smaller than a threshold. Prior work has explored
distributed computation methods to scale similarity join to large data volumes
but these methods require a cluster deployment, and efficiency suffers from
expensive inter-machine communication. On the other hand, disk-based solutions
are more cost-effective by using a single machine and storing the large dataset
on high-performance external storage, such as NVMe SSDs, but in these methods
the disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,
the first disk-based similarity join algorithm that can process billion-scale
vector datasets efficiently on a single machine. DiskJoin improves disk I/O by
tailoring the data access patterns to avoid repetitive accesses and read
amplification. It also uses main memory as a dynamic cache and carefully
manages cache eviction to improve cache hit rate and reduce disk retrieval
time. For further acceleration, we adopt a probabilistic pruning technique that
can effectively prune a large number of vector pairs from computation. Our
evaluation on real-world, large-scale datasets shows that DiskJoin
significantly outperforms alternatives, achieving speedups from 50x to 1000x.

</details>


### [126] [Brook-2PL: Tolerating High Contention Workloads with A Deadlock-Free Two-Phase Locking Protocol](https://arxiv.org/abs/2508.18576)
*Farzad Habibi,Juncheng Fang,Tania Lorido-Botran,Faisal Nawab*

Main category: cs.DB

TL;DR: Brook-2PL是一种新的2PL协议，通过SLW-Graph实现无死锁事务执行，并通过部分事务切分提前释放锁，显著优于现有CC协议。


<details>
  <summary>Details</summary>
Motivation: 高争用工作负载中热点问题仍然是并发控制(CC)协议的一个关键挑战。传统的并发控制方法在高争用下遇到重大困难，导致过多的事务中止和死锁。

Method: 提出了一种新的两阶段锁定(2PL)协议Brook-2PL，该协议:(1)引入SLW-Graph用于无死锁事务执行，(2)提出部分事务切分以提前释放锁。

Result: Brook-2PL通过静态分析一种新的基于图的依赖结构(称为SLW-Graph)来解决这个限制，通过预定的锁获取实现无死锁的两阶段锁定。Brook-2PL还通过使用部分事务切分和静态事务分析来实现提前锁释放，从而减少争用。

Conclusion: Brook-2PL显著优于现有CC协议，TPC-C基准测试平均加速2.86倍，尾部延迟(p95)降低48%。

Abstract: The problem of hotspots remains a critical challenge in high-contention
workloads for concurrency control (CC) protocols. Traditional concurrency
control approaches encounter significant difficulties under high contention,
resulting in excessive transaction aborts and deadlocks. In this paper, we
propose Brook-2PL, a novel two-phase locking (2PL) protocol that (1) introduces
SLW-Graph for deadlock-free transaction execution, and (2) proposes partial
transaction chopping for early lock release. Previous methods suffer from
transaction aborts that lead to wasted work and can further burden the system
due to their cascading effects. Brook-2PL addresses this limitation by
statically analyzing a new graph-based dependency structure called SLW-Graph,
enabling deadlock-free two-phase locking through predetermined lock
acquisition. Brook-2PL also reduces contention by enabling early lock release
using partial transaction chopping and static transaction analysis. We overcome
the inherent limitations of traditional transaction chopping by providing a
more flexible chopping method. Evaluation using both our synthetic online game
store workload and the TPC-C benchmark shows that Brook-2PL significantly
outperforms state-of-the-art CC protocols. Brook-2PL achieves an average
speed-up of 2.86x while reducing tail latency (p95) by 48% in the TPC-C
benchmark.

</details>


### [127] [Optimal $(α,β)$-Dense Subgraph Search in Bipartite Graphs](https://arxiv.org/abs/2508.18616)
*Yalong Zhang,Rong-Hua Li,Qi Zhang,Guoren Wang*

Main category: cs.DB

TL;DR: This paper introduces BD-Index, a novel index for answering dense subgraph queries in bipartite graphs with efficient query processing and dynamic updates.


<details>
  <summary>Details</summary>
Motivation: The $(\alpha, \beta)$-dense subgraph model lacks efficient support for query processing and dynamic updates, limiting its practical utility in large-scale applications.

Method: The paper proposes BD-Index, a novel index that answers $(\alpha, \beta)$-dense subgraph queries in optimal time while using only linear space $O(|E|)$. Two complementary maintenance strategies for dynamic bipartite graphs are developed to support efficient updates to the BD-Index.

Result: BD-Index answers $(\alpha, \beta)$-dense subgraph queries in optimal time while using only linear space $O(|E|)$. The space-efficient strategy updates the index in time complexity of $O(p \cdot |E|^{1.5})$ per edge insertion or deletion, while maintaining a low space cost of $O(|E|)$. The time-efficient strategy significantly reduces the update time to $O(p \cdot |E|)$ per edge update by maintaining auxiliary orientation structures, at the cost of increased memory usage up to $O(p \cdot |E|)$.

Conclusion: The paper introduces BD-Index, a novel index for answering $(\alpha, \beta)$-dense subgraph queries. Experiments on real-world datasets demonstrate the efficiency and scalability of the proposed solutions.

Abstract: Dense subgraph search in bipartite graphs is a fundamental problem in graph
analysis, with wide-ranging applications in fraud detection, recommendation
systems, and social network analysis. The recently proposed $(\alpha,
\beta)$-dense subgraph model has demonstrated superior capability in capturing
the intrinsic density structure of bipartite graphs compared to existing
alternatives. However, despite its modeling advantages, the $(\alpha,
\beta)$-dense subgraph model lacks efficient support for query processing and
dynamic updates, limiting its practical utility in large-scale applications. To
address these limitations, we propose BD-Index, a novel index that answers
$(\alpha, \beta)$-dense subgraph queries in optimal time while using only
linear space $O(|E|)$, making it well-suited for real-world applications
requiring both fast query processing and low memory consumption. We further
develop two complementary maintenance strategies for dynamic bipartite graphs
to support efficient updates to the BD-Index. The space-efficient strategy
updates the index in time complexity of $O(p \cdot |E|^{1.5})$ per edge
insertion or deletion, while maintaining a low space cost of $O(|E|)$ (the same
as the index itself), where $p$ is typically a small constant in real-world
graphs. In contrast, the time-efficient strategy significantly reduces the
update time to $O(p \cdot |E|)$ per edge update by maintaining auxiliary
orientation structures, at the cost of increased memory usage up to $O(p \cdot
|E|)$. These two strategies provide flexible trade-offs between maintenance
efficiency and memory usage, enabling BD-Index to adapt to diverse application
requirements. Extensive experiments on 10 large-scale real-world datasets
demonstrate high efficiency and scalability of our proposed solutions.

</details>


### [128] [WoW: A Window-to-Window Incremental Index for Range-Filtering Approximate Nearest Neighbor Search](https://arxiv.org/abs/2508.18617)
*Ziqi Wang,Jingzhe Zhang,Wei Hu*

Main category: cs.DB

TL;DR: This paper introduces a window graph-based index for RFANNS that supports incremental construction and arbitrary range filters, achieving high performance in both indexing and querying.


<details>
  <summary>Details</summary>
Motivation: RFANNS is a fundamental function in vector database management systems and intelligent systems, but existing indices face challenges in incremental construction and generalization for arbitrary range filters.

Method: A window graph-based RFANNS index with an insertion algorithm for incremental construction and optimized window search for arbitrary range filters.

Result: Experiments on real-world datasets demonstrate the efficiency and effectiveness of the proposed index in terms of indexing time, index size, and query performance.

Conclusion: The proposed window graph-based RFANNS index achieves comparable indexing time to the fastest index, 4.9x faster indexing than the fastest query index with 0.4-0.5x smaller size, 4x faster query than the fastest incremental index, and matches the performance of the best statically-built index.

Abstract: Given a hybrid dataset where every data object consists of a vector and an
attribute value, for each query with a target vector and a range filter,
range-filtering approximate nearest neighbor search (RFANNS) aims to retrieve
the most similar vectors from the dataset and the corresponding attribute
values fall in the query range. It is a fundamental function in vector database
management systems and intelligent systems with embedding abilities. Dedicated
indices for RFANNS accelerate query speed with an acceptable accuracy loss on
nearest neighbors. However, they are still facing the challenges to be
constructed incrementally and generalized to achieve superior query performance
for arbitrary range filters. In this paper, we introduce a window graph-based
RFANNS index. For incremental construction, we propose an insertion algorithm
to add new vector-attribute pairs into hierarchical window graphs with varying
window size. To handle arbitrary range filters, we optimize relevant window
search for attribute filter checks and vector distance computations by range
selectivity. Extensive experiments on real-world datasets show that for index
construction, the indexing time is on par with the most building-efficient
index, and 4.9x faster than the most query-efficient index with 0.4-0.5x
smaller size; For RFANNS query, it is 4x faster than the most efficient
incremental index, and matches the performance of the best statically-built
index.

</details>


### [129] [Rethinking Caching for LLM Serving Systems: Beyond Traditional Heuristics](https://arxiv.org/abs/2508.18736)
*Jungwoo Kim,Minsang Kim,Jaeheon Lee,Chanwoo Moon,Heejin Kim,Taeho Hwang,Woosuk Chung,Yeseong Kim,Sungjin Lee*

Main category: cs.DB

TL;DR: SISO是一种用于LLM服务的新型语义缓存系统，它优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统缓存策略不足：精确匹配和前缀缓存忽略查询语义，而最先进的语义缓存仍然局限于传统直觉，几乎没有概念上的偏离。因此，我们提出了SISO，这是一个语义缓存系统，它重新定义了LLM服务的效率。

Method: SISO引入了基于质心的缓存以最大限度地提高覆盖率并最大限度地减少内存，引入了 locality-aware 替换以保留高价值条目，并引入了动态阈值以在不同的工作负载下平衡准确性和延迟。

Result: SISO实现了更高的命中率和更强的SLO达成。

Conclusion: SISO在各种数据集上实现了高达1.71倍的更高命中率，并且始终比最先进的系统更好地实现SLO。

Abstract: Serving Large Language Models (LLMs) at scale requires meeting strict Service
Level Objectives (SLOs) under severe computational and memory constraints.
Nevertheless, traditional caching strategies fall short: exact-matching and
prefix caches neglect query semantics, while state-of-the-art semantic caches
remain confined to traditional intuitions, offering little conceptual
departure. Building on this, we present SISO, a semantic caching system that
redefines efficiency for LLM serving. SISO introduces centroid-based caching to
maximize coverage with minimal memory, locality-aware replacement to preserve
high-value entries, and dynamic thresholding to balance accuracy and latency
under varying workloads. Across diverse datasets, SISO delivers up to
1.71$\times$ higher hit ratios and consistently stronger SLO attainment
compared to state-of-the-art systems.

</details>


### [130] [Text to Query Plans for Question Answering on Large Tables](https://arxiv.org/abs/2508.18758)
*Yipeng Zhang,Chen Wang,Yuzhe Zhang,Jacky Jiang*

Main category: cs.DB

TL;DR: 提出了一种新颖的框架，该框架将自然语言查询转换为查询计划，从而能够支持复杂的分析功能并处理大型数据集，而没有传统SQL的局限性。


<details>
  <summary>Details</summary>
Motivation: 高效查询和分析大型表格数据集仍然面临重大挑战，特别是对于没有SQL等编程语言专业知识的用户。Text-to-SQL方法在基准数据上表现出 promising 的性能；但是，它们继承了SQL的缺点，包括大型数据集的效率低下以及对基本查询之外的复杂数据分析的有限支持。

Method: 该框架将自然语言查询转换为查询计划，利用LLM迭代解释查询并构建操作序列，通过直接在数据上执行操作来克服上下文长度限制。

Result: 该框架支持传统的SQL命令，同时避免了SQL的固有局限性。此外，它还支持复杂的分析功能，例如主成分分析和异常检测，与传统的SQL功能相比，提供了更大的灵活性和可扩展性。

Conclusion: 该框架通过在标准数据库和大型科学表格上的实验验证，展示了其在处理大型数据集和执行复杂数据分析方面的有效性。

Abstract: Efficient querying and analysis of large tabular datasets remain significant
challenges, especially for users without expertise in programming languages
like SQL. Text-to-SQL approaches have shown promising performance on benchmark
data; however, they inherit SQL's drawbacks, including inefficiency with large
datasets and limited support for complex data analyses beyond basic querying.
We propose a novel framework that transforms natural language queries into
query plans. Our solution is implemented outside traditional databases,
allowing us to support classical SQL commands while avoiding SQL's inherent
limitations. Additionally, we enable complex analytical functions, such as
principal component analysis and anomaly detection, providing greater
flexibility and extensibility than traditional SQL capabilities. We leverage
LLMs to iteratively interpret queries and construct operation sequences,
addressing computational complexity by incrementally building solutions. By
executing operations directly on the data, we overcome context length
limitations without requiring the entire dataset to be processed by the model.
We validate our framework through experiments on both standard databases and
large scientific tables, demonstrating its effectiveness in handling extensive
datasets and performing sophisticated data analyses.

</details>


### [131] [Enriching Object-Centric Event Data with Process Scopes: A Framework for Aggregation and Analysis](https://arxiv.org/abs/2508.18830)
*Shahrzad Khayatbashi,Majid Rafiei,Jiayuan Chen,Timotheus Kampik,Gregor Berg,Amin Jalali*

Main category: cs.DB

TL;DR: 我们提出了一种将分析师定义的过程范围嵌入到OCEL中的方法，这支持多个共存过程的结构化表示，支持跨范围的事件数据聚合，并促进不同抽象级别的分析。


<details>
  <summary>Details</summary>
Motivation: 现有的格式缺乏对过程范围的明确定义，这限制了对单个过程的分析，并将见解限制在较低的粒度级别。在实践中，OCED通常跨越多个相互关联的过程，因为共享对象连接了跨组织功能的事件。这种结构反映了价值是如何沿着组织价值链创造的，但是当过程边界没有明确定义时，会给解释带来挑战。此外，过程定义通常是主观的，并且依赖于上下文；它们在组织、角色和分析目标之间有所不同，并且不能总是自动发现。

Method: 一种将分析师定义的过程范围嵌入到OCEL中的方法

Result: 我们使用公开可用的OCEL日志证明了我们方法的适用性，并为范围定义和分析提供了支持工具。

Conclusion: 为了应对这些挑战，我们提出了一种将分析师定义的过程范围嵌入到OCEL中的方法。这支持多个共存过程的结构化表示，支持跨范围的事件数据聚合，并促进不同抽象级别的分析。我们使用公开可用的OCEL日志证明了我们方法的适用性，并为范围定义和分析提供了支持工具。

Abstract: Object-Centric Process Mining enables the analysis of complex operational
behavior by capturing interactions among multiple business objects (e.g.,
orders, items, deliveries). These interactions are recorded using
Object-Centric Event Data (OCED) formats, such as the Object-Centric Event Log
(OCEL). However, existing formats lack explicit definitions of process scopes,
which restricts analysis to individual processes and limits insights to a low
level of granularity. In practice, OCED often spans multiple interrelated
processes, as shared objects connect events across organizational functions.
This structure reflects how value is created along the organizational value
chain, but introduces challenges for interpretation when process boundaries are
not clearly defined. Moreover, process definitions are typically subjective and
context-dependent; they vary across organizations, roles, and analytical goals,
and cannot always be discovered automatically. To address these challenges, we
propose a method for embedding analyst-defined process scopes into OCEL. This
enables the structured representation of multiple coexisting processes,
supports the aggregation of event data across scopes, and facilitates analysis
at varying levels of abstraction. We demonstrate the applicability of our
approach using a publicly available OCEL log and provide supporting tools for
scope definition and analysis.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [132] [REALM: Recursive Relevance Modeling for LLM-based Document Re-Ranking](https://arxiv.org/abs/2508.18379)
*Pinhuan Wang,Zhiqiu Xia,Chunhua Liao,Feiyi Wang,Hang Liu*

Main category: cs.IR

TL;DR: propose REALM, an uncertainty-aware re-ranking framework that models LLM-derived relevance as Gaussian distributions and refines them through recursive Bayesian updates. By explicitly capturing uncertainty and minimizing redundant queries, REALM achieves better rankings more efficiently.


<details>
  <summary>Details</summary>
Motivation: existing LLM-based approaches face notable limitations, including ranking uncertainty, unstable top-k recovery, and high token cost due to token-intensive prompting

Method: an uncertainty-aware re-ranking framework that models LLM-derived relevance as Gaussian distributions and refines them through recursive Bayesian updates

Result: achieves better rankings more efficiently

Conclusion: REALM surpasses state-of-the-art re-rankers while significantly reducing token usage and latency, promoting it as the next-generation re-ranker for modern IR systems.

Abstract: Large Language Models (LLMs) have shown strong capabilities in document
re-ranking, a key component in modern Information Retrieval (IR) systems.
However, existing LLM-based approaches face notable limitations, including
ranking uncertainty, unstable top-k recovery, and high token cost due to
token-intensive prompting. To effectively address these limitations, we propose
REALM, an uncertainty-aware re-ranking framework that models LLM-derived
relevance as Gaussian distributions and refines them through recursive Bayesian
updates. By explicitly capturing uncertainty and minimizing redundant queries,
REALM achieves better rankings more efficiently. Experimental results
demonstrate that our REALM surpasses state-of-the-art re-rankers while
significantly reducing token usage and latency, promoting it as the
next-generation re-ranker for modern IR systems.

</details>


### [133] [DenseRec: Revisiting Dense Content Embeddings for Sequential Transformer-based Recommendation](https://arxiv.org/abs/2508.18442)
*Jan Malte Lichtenberg,Antonio De Candia,Matteo Ruffini*

Main category: cs.IR

TL;DR: DenseRec通过学习密集嵌入空间到ID嵌入空间的线性投影，实现了在冷启动顺序推荐中优于ID-only方法的性能，且无需复杂模型。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的顺序推荐器，例如SASRec或BERT4Rec，通常仅依赖于学习的项目ID嵌入，这使得它们容易受到项目冷启动问题的影响，尤其是在具有动态项目目录的环境中。虽然来自预训练模型的密集内容嵌入提供了潜在的解决方案，但与仅使用ID的方法相比，直接集成到基于transformer的推荐器中一直表现不佳。

Method: 提出了DenseRec，一种简单而有效的方法，它引入了一种双路径嵌入方法。DenseRec在训练期间学习从密集嵌入空间到ID嵌入空间的线性投影，从而能够无缝泛化到以前未见过的项目，而无需专门的嵌入模型或复杂的基础设施。

Result: DenseRec在三个真实世界的数据集上始终优于仅使用ID的SASRec基线，即使没有额外的超参数调整并且使用紧凑的嵌入模型。分析表明，改进主要来自于在存在未见过的项目时更好的序列表示。

Conclusion: DenseRec在三个真实世界的数据集上始终优于仅使用ID的SASRec基线，即使没有额外的超参数调整并且使用紧凑的嵌入模型。改进主要来自于在存在未见过的项目时更好的序列表示，这使得DenseRec成为冷启动顺序推荐的实用且稳健的解决方案。

Abstract: Transformer-based sequential recommenders, such as SASRec or BERT4Rec,
typically rely solely on learned item ID embeddings, making them vulnerable to
the item cold-start problem, particularly in environments with dynamic item
catalogs. While dense content embeddings from pre-trained models offer
potential solutions, direct integration into transformer-based recommenders has
consistently underperformed compared to ID-only approaches. We revisit this
integration challenge and propose DenseRec, a simple yet effective method that
introduces a dual-path embedding approach. DenseRec learns a linear projection
from the dense embedding space into the ID embedding space during training,
enabling seamless generalization to previously unseen items without requiring
specialized embedding models or complex infrastructure. In experiments on three
real-world datasets, we find DenseRec to consistently outperform an ID-only
SASRec baseline, even without additional hyperparameter tuning and while using
compact embedding models. Our analysis suggests improvements primarily arise
from better sequence representations in the presence of unseen items,
positioning DenseRec as a practical and robust solution for cold-start
sequential recommendation.

</details>


### [134] [Extracting Information from Scientific Literature via Visual Table Question Answering Models](https://arxiv.org/abs/2508.18661)
*Dongyoun Kim,Hyung-do Choi,Youngsun Jang,John Kim*

Main category: cs.IR

TL;DR: This study explores three approaches to processing table data in scientific papers to enhance extractive question answering and concludes that preserving the structural integrity of tables is essential.


<details>
  <summary>Details</summary>
Motivation: enhance extractive question answering and develop a software tool for the systematic review process

Method: Optical Character Recognition (OCR), Pre-trained models for document visual question answering, Table detection and structure recognition

Result: approaches preserving table structure outperform the others, particularly in representing and organizing table content. Accurately recognizing specific notations and symbols within the documents emerged as a critical factor for improved results.

Conclusion: preserving the structural integrity of tables is essential for enhancing the accuracy and reliability of extractive question answering in scientific documents.

Abstract: This study explores three approaches to processing table data in scientific
papers to enhance extractive question answering and develop a software tool for
the systematic review process. The methods evaluated include: (1) Optical
Character Recognition (OCR) for extracting information from documents, (2)
Pre-trained models for document visual question answering, and (3) Table
detection and structure recognition to extract and merge key information from
tables with textual content to answer extractive questions. In exploratory
experiments, we augmented ten sample test documents containing tables and
relevant content against RF- EMF-related scientific papers with seven
predefined extractive question-answer pairs. The results indicate that
approaches preserving table structure outperform the others, particularly in
representing and organizing table content. Accurately recognizing specific
notations and symbols within the documents emerged as a critical factor for
improved results. Our study concludes that preserving the structural integrity
of tables is essential for enhancing the accuracy and reliability of extractive
question answering in scientific documents.

</details>


### [135] [Membership Inference Attacks on LLM-based Recommender Systems](https://arxiv.org/abs/2508.18665)
*Jiajie He,Yuechun Gu,Min-Chun Chen,Keke Chen*

Main category: cs.IR

TL;DR: LLM RecSys are vulnerable to membership inference attacks, particularly direct inquiry and poisoning attacks.


<details>
  <summary>Details</summary>
Motivation: private information in LLMs based RecSys may be exposed to novel privacy attacks, but no study has been done on this issue.

Method: designing four MIAs: direct inquiry, hallucination, similarity, and poisoning attacks, and evaluating them on three LLMs and two RecSys datasets.

Result: direct inquiry and poisoning attacks show significantly high attack advantages. The number of shots in system prompts and the position of the victim in the shots affect these attacks.

Conclusion: membership inference attacks (MIAs) are a realistic threat on LLM RecSys, especially direct inquiry and poisoning attacks.

Abstract: Large language models (LLMs) based Recommender Systems (RecSys) can flexibly
adapt recommendation systems to different domains. It utilizes in-context
learning (ICL), i.e., the prompts, to customize the recommendation functions,
which include sensitive historical user-specific item interactions, e.g.,
implicit feedback like clicked items or explicit product reviews. Such private
information may be exposed to novel privacy attack. However, no study has been
done on this important issue. We design four membership inference attacks
(MIAs), aiming to reveal whether victims' historical interactions have been
used by system prompts. They are \emph{direct inquiry, hallucination,
similarity, and poisoning attacks}, each of which utilizes the unique features
of LLMs or RecSys. We have carefully evaluated them on three LLMs that have
been used to develop ICL-LLM RecSys and two well-known RecSys benchmark
datasets. The results confirm that the MIA threat on LLM RecSys is realistic:
direct inquiry and poisoning attacks showing significantly high attack
advantages. We have also analyzed the factors affecting these attacks, such as
the number of shots in system prompts and the position of the victim in the
shots.

</details>


### [136] [Taming the One-Epoch Phenomenon in Online Recommendation System by Two-stage Contrastive ID Pre-training](https://arxiv.org/abs/2508.18700)
*Yi-Ping Hsu,Po-Wei Wang,Chantat Eksombatchai,Jiajing Xu*

Main category: cs.IR

TL;DR: Introduces a two-stage training strategy with contrastive pre-training to address the one-epoch problem in ID-based embeddings, improving generalization and engagement gains.


<details>
  <summary>Details</summary>
Motivation: ID-based embeddings in online recommendation systems are susceptible to overfitting due to long-tail data distributions, limiting training to a single epoch (the "one-epoch problem").

Method: A novel two-stage training strategy that incorporates a pre-training phase using a minimal model with contrastive loss.

Result: Multi-epoch training during the pre-training phase does not lead to overfitting, and the resulting embeddings improve online generalization. Significant site-wide engagement gains were achieved at Pinterest.

Conclusion: The proposed two-stage training strategy, including a pre-training phase with contrastive loss, improves online generalization and achieves significant site-wide engagement gains at Pinterest.

Abstract: ID-based embeddings are widely used in web-scale online recommendation
systems. However, their susceptibility to overfitting, particularly due to the
long-tail nature of data distributions, often limits training to a single
epoch, a phenomenon known as the "one-epoch problem." This challenge has driven
research efforts to optimize performance within the first epoch by enhancing
convergence speed or feature sparsity. In this study, we introduce a novel
two-stage training strategy that incorporates a pre-training phase using a
minimal model with contrastive loss, enabling broader data coverage for the
embedding system. Our offline experiments demonstrate that multi-epoch training
during the pre-training phase does not lead to overfitting, and the resulting
embeddings improve online generalization when fine-tuned for more complex
downstream recommendation tasks. We deployed the proposed system in live
traffic at Pinterest, achieving significant site-wide engagement gains.

</details>


### [137] [Optimization of Latent-Space Compression using Game-Theoretic Techniques for Transformer-Based Vector Search](https://arxiv.org/abs/2508.18877)
*Kushagra Agrawal,Nisharg Nargund,Oishani Banerjee*

Main category: cs.IR

TL;DR: This paper introduces a game-theoretic approach to latent space compression for vector search, improving semantic accuracy and efficiency, especially for transformer-based embeddings in LLM pipelines.


<details>
  <summary>Details</summary>
Motivation: The scalability and efficiency of vector similarity search systems are often hindered by the high dimensionality of latent representations.

Method: The paper proposes a game-theoretic framework for optimizing latent-space compression.

Result: The proposed approach achieves a significantly higher average similarity (0.9981 vs. 0.5517) and utility (0.8873 vs. 0.5194) compared to FAISS, with a modest increase in query time.

Conclusion: The proposed game-theoretic latent compression method can be seamlessly integrated into existing LLM pipelines to yield more semantically accurate and computationally efficient retrieval.

Abstract: Vector similarity search plays a pivotal role in modern information retrieval
systems, especially when powered by transformer-based embeddings. However, the
scalability and efficiency of such systems are often hindered by the high
dimensionality of latent representations. In this paper, we propose a novel
game-theoretic framework for optimizing latent-space compression to enhance
both the efficiency and semantic utility of vector search. By modeling the
compression strategy as a zero-sum game between retrieval accuracy and storage
efficiency, we derive a latent transformation that preserves semantic
similarity while reducing redundancy. We benchmark our method against FAISS, a
widely-used vector search library, and demonstrate that our approach achieves a
significantly higher average similarity (0.9981 vs. 0.5517) and utility (0.8873
vs. 0.5194), albeit with a modest increase in query time. This trade-off
highlights the practical value of game-theoretic latent compression in
high-utility, transformer-based search applications. The proposed system can be
seamlessly integrated into existing LLM pipelines to yield more semantically
accurate and computationally efficient retrieval.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [138] [Reasoning Steps as Curriculum: Using Depth of Thought as a Difficulty Signal for Tuning LLMs](https://arxiv.org/abs/2508.18279)
*Jeesu Jung,Sangkeun Jung*

Main category: cs.LG

TL;DR: 本文提出了一种基于思维深度 (DoT) 的课程学习方法，用于训练大型语言模型，该方法通过计算教师模型推理轨迹中的离散步骤来衡量任务难度。实验结果表明，该方法在推理基准测试中表现良好，并且在不同的教师模型中具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 的课程学习需要一种难度信号，该信号与推理对齐，同时保持可扩展性和可解释性。我们提出了一个简单的前提：对人类来说需要更深层次思考的任务对模型来说也应该更难。

Method: 通过计算教师模型推理轨迹中的离散步骤（例如，思维链）来定义难度为思维深度（DoT）并进行操作。

Result: DoT 与推理基准上的传统难度相关，在匹配预算下，DoT 排序的课程优于长度或判断评分的课程，并且在轻格式控制下，难度在教师模型中是稳健的。

Conclusion: 本文旨在推进以认知为基础的、可解释的、以推理为中心的训练课程。

Abstract: Curriculum learning for training LLMs requires a difficulty signal that
aligns with reasoning while remaining scalable and interpretable. We propose a
simple premise: tasks that demand deeper depth of thought for humans should
also be harder for models. Accordingly, we define difficulty as depth of
thought (DoT) and operationalize it by counting the discrete steps in a teacher
model's reasoning trace (e.g., Chain-of-Thought). We then train with a shallow
to deep curriculum ordered by this DoT and outline how to derive, validate, and
schedule it at scale. Our position yields three testable hypotheses: (i) DoT
correlates with conventional difficulty on reasoning benchmarks, (ii)
DoT-ordered curricula outperform length- or judge-scored curricula under
matched budgets, and (iii) the difficulty is robust across teacher models given
light formatting controls. We propose an evaluation framework and discuss
threats to validity (teacher style, length confounds) alongside practical
mitigations. Taken together, we aim to move toward cognitively grounded,
interpretable curricula for reasoning-centric training.

</details>


### [139] [Multi-Modal Drift Forecasting of Leeway Objects via Navier-Stokes-Guided CNN and Sequence-to-Sequence Attention-Based Models](https://arxiv.org/abs/2508.18284)
*Rahmat K. Adesunkanmi,Alexander W. Brandt,Masoud Deylami,Gustavo A. Giraldo Echeverri,Hamidreza Karbasian,Adel Alaeddini*

Main category: cs.LG

TL;DR: 提出了一种多模态机器学习框架，用于预测水上漂流物体的漂移。


<details>
  <summary>Details</summary>
Motivation: 在海事环境中准确预测漂流物体的漂移仍然是一个关键挑战，尤其是在时间敏感的场景中，如搜索和救援行动。

Method: 集成句子Transformer嵌入和基于注意力的序列到序列架构的多模态机器学习框架。

Result: 多模态模型与传统模型性能相当，同时能够进行更长期的预测，以取代单步预测。

Conclusion: 多模态建模策略能够对动态海事条件下漂流物体的漂移提供准确和适应性强的预测。

Abstract: Accurately predicting the drift (displacement) of leeway objects in maritime
environments remains a critical challenge, particularly in time-sensitive
scenarios such as search and rescue operations. In this study, we propose a
multi-modal machine learning framework that integrates Sentence Transformer
embeddings with attention-based sequence-to-sequence architectures to predict
the drift of leeway objects in water. We begin by experimentally collecting
environmental and physical data, including water current and wind velocities,
object mass, and surface area, for five distinct leeway objects. Using
simulated data from a Navier-Stokes-based model to train a convolutional neural
network on geometrical image representations, we estimate drag and lift
coefficients of the leeway objects. These coefficients are then used to derive
the net forces responsible for driving the objects' motion. The resulting time
series, comprising physical forces, environmental velocities, and
object-specific features, combined with textual descriptions encoded via a
language model, are inputs to attention-based sequence-to-sequence
long-short-term memory and Transformer models, to predict future drift
trajectories. We evaluate the framework across multiple time horizons ($1$,
$3$, $5$, and $10$ seconds) and assess its generalization across different
objects. We compare our approach against a fitted physics-based model and
traditional machine learning methods, including recurrent neural networks and
temporal convolutional neural networks. Our results show that these multi-modal
models perform comparably to traditional models while also enabling longer-term
forecasting in place of single-step prediction. Overall, our findings
demonstrate the ability of a multi-modal modeling strategy to provide accurate
and adaptable predictions of leeway object drift in dynamic maritime
conditions.

</details>


### [140] [Data-driven models for production forecasting and decision supporting in petroleum reservoirs](https://arxiv.org/abs/2508.18289)
*Mateus A. Fernandes,Michael M. Furlanetti,Eduardo Gildin,Marcio A. Sampaio*

Main category: cs.LG

TL;DR: 使用机器学习方法开发一种预测生产参数的方法，无需依赖地质模型，流体性质或井完井和流动系统的详细信息。


<details>
  <summary>Details</summary>
Motivation: 可靠地预测产量并预测岩石-流体系统行为的变化是油藏工程中的主要挑战。

Method: 数据驱动方法和机器学习方法，例如基于回归和神经网络的监督学习方法。

Result: 对生产和注入变量进行了相关性分析，以及调整数据以适应问题。由于储层条件随时间变化，概念漂移是优先考虑的问题，需要特别关注那些观察窗口和再训练的周期性。使用来自UNISIM III成分模拟模型的合成数据评估该方法。接下来，将其应用于巴西盐下地区的真实案例。

Conclusion: 设计一个可靠的预测器，用于重现储层动态，具有快速响应，能够处理实际困难，例如井和处理单元的限制，并且可以用于支持储层管理的行动，包括预测有害行为，优化生产和注入参数以及分析概率事件的影响，旨在最大程度地提高采油率。

Abstract: Forecasting production reliably and anticipating changes in the behavior of
rock-fluid systems are the main challenges in petroleum reservoir engineering.
This project proposes to deal with this problem through a data-driven approach
and using machine learning methods. The objective is to develop a methodology
to forecast production parameters based on simple data as produced and injected
volumes and, eventually, gauges located in wells, without depending on
information from geological models, fluid properties or details of well
completions and flow systems. Initially, we performed relevance analyses of the
production and injection variables, as well as conditioning the data to suit
the problem. As reservoir conditions change over time, concept drift is a
priority concern and require special attention to those observation windows and
the periodicity of retraining, which are also objects of study. For the
production forecasts, we study supervised learning methods, such as those based
on regressions and Neural Networks, to define the most suitable for our
application in terms of performance and complexity. In a first step, we
evaluate the methodology using synthetic data generated from the UNISIM III
compositional simulation model. Next, we applied it to cases of real plays in
the Brazilian pre-salt. The expected result is the design of a reliable
predictor for reproducing reservoir dynamics, with rapid response, capability
of dealing with practical difficulties such as restrictions in wells and
processing units, and that can be used in actions to support reservoir
management, including the anticipation of deleterious behaviors, optimization
of production and injection parameters and the analysis of the effects of
probabilistic events, aiming to maximize oil recovery.

</details>


### [141] [A Fast and Minimal System to Identify Depression Using Smartphones: Explainable Machine Learning-Based Approach](https://arxiv.org/abs/2508.18301)
*Md Sabbir Ahmed,Nova Ahmed*

Main category: cs.LG

TL;DR: Developed a fast, minimalistic system using 1-second app usage data to identify depression in students, achieving up to 82.4% accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing robust, pervasive device-based systems developed in recent years to detect depression require data collected over a long period and may not be effective in cases where early detection is crucial.

Method: We developed a fast tool that retrieves the past 7 days' app usage data in 1 second (mean 0.31, SD 1.10 seconds). A total of 100 students from Bangladesh participated in our study, and our tool collected their app usage data. To identify depressed and nondepressed students, we developed a diverse set of ML models. We selected important features using the stable approach, along with 3 main types of feature selection (FS) approaches.

Result: Leveraging only the app usage data retrieved in 1 second, our light gradient boosting machine model used the important features selected by the stable FS approach and correctly identified 82.4% (n=42) of depressed students (precision=75%, F1-score=78.5%). Moreover, after comprehensive exploration, we presented a parsimonious stacking model where around 5 features selected by the all-relevant FS approach Boruta were used in each iteration of validation and showed a maximum precision of 77.4% (balanced accuracy=77.9%). A SHAP analysis of our best models presented behavioral markers that were related to depression.

Conclusion: Due to our system's fast and minimalistic nature, it may make a worthwhile contribution to identifying depression in underdeveloped and developing regions. In addition, our detailed discussion about the implication of our findings can facilitate the development of less resource-intensive systems to better understand students who are depressed.

Abstract: Background: Existing robust, pervasive device-based systems developed in
recent years to detect depression require data collected over a long period and
may not be effective in cases where early detection is crucial.
  Objective: Our main objective was to develop a minimalistic system to
identify depression using data retrieved in the fastest possible time.
  Methods: We developed a fast tool that retrieves the past 7 days' app usage
data in 1 second (mean 0.31, SD 1.10 seconds). A total of 100 students from
Bangladesh participated in our study, and our tool collected their app usage
data. To identify depressed and nondepressed students, we developed a diverse
set of ML models. We selected important features using the stable approach,
along with 3 main types of feature selection (FS) approaches.
  Results: Leveraging only the app usage data retrieved in 1 second, our light
gradient boosting machine model used the important features selected by the
stable FS approach and correctly identified 82.4% (n=42) of depressed students
(precision=75%, F1-score=78.5%). Moreover, after comprehensive exploration, we
presented a parsimonious stacking model where around 5 features selected by the
all-relevant FS approach Boruta were used in each iteration of validation and
showed a maximum precision of 77.4% (balanced accuracy=77.9%). A SHAP analysis
of our best models presented behavioral markers that were related to
depression.
  Conclusions: Due to our system's fast and minimalistic nature, it may make a
worthwhile contribution to identifying depression in underdeveloped and
developing regions. In addition, our detailed discussion about the implication
of our findings can facilitate the development of less resource-intensive
systems to better understand students who are depressed.

</details>


### [142] [Learning Explainable Imaging-Genetics Associations Related to a Neurological Disorder](https://arxiv.org/abs/2508.18303)
*Jueqi Wang,Zachary Jacokes,John Darrell Van Horn,Michael C. Schatz,Kevin A. Pelphrey,Archana Venkataraman*

Main category: cs.LG

TL;DR: NeuroPathX 是一个可解释的深度学习框架，它使用交叉注意机制来捕获大脑结构和遗传变异之间的相互作用，在脑部疾病研究中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的影像遗传学方法仅限于简单的线性模型或缺乏可解释性的黑盒技术。

Method: NeuroPathX，一个可解释的深度学习框架，它使用由交叉注意机制驱动的早期融合策略，以捕获来自 MRI 的大脑结构变异与来自遗传数据的已建立的生物途径之间的有意义的相互作用。

Result: NeuroPathX 在自闭症谱系障碍和阿尔茨海默病上都得到了验证，并且优于其他基线方法。

Conclusion: NeuroPathX 优于其他方法，并揭示了与疾病相关的生物学上合理的关联，强调了其在促进我们对复杂脑部疾病的理解方面的潜力。

Abstract: While imaging-genetics holds great promise for unraveling the complex
interplay between brain structure and genetic variation in neurological
disorders, traditional methods are limited to simplistic linear models or to
black-box techniques that lack interpretability. In this paper, we present
NeuroPathX, an explainable deep learning framework that uses an early fusion
strategy powered by cross-attention mechanisms to capture meaningful
interactions between structural variations in the brain derived from MRI and
established biological pathways derived from genetics data. To enhance
interpretability and robustness, we introduce two loss functions over the
attention matrix - a sparsity loss that focuses on the most salient
interactions and a pathway similarity loss that enforces consistent
representations across the cohort. We validate NeuroPathX on both autism
spectrum disorder and Alzheimer's disease. Our results demonstrate that
NeuroPathX outperforms competing baseline approaches and reveals biologically
plausible associations linked to the disorder. These findings underscore the
potential of NeuroPathX to advance our understanding of complex brain
disorders. Code is available at https://github.com/jueqiw/NeuroPathX .

</details>


### [143] [SALMAN: Stability Analysis of Language Models Through the Maps Between Graph-based Manifolds](https://arxiv.org/abs/2508.18306)
*Wuxinlin Cheng,Yupeng Cao,Jinwen Wu,Koduvayur Subbalakshmi,Tian Han,Zhuo Feng*

Main category: cs.LG

TL;DR: 提出了 SALMAN，这是一个用于评估和提高基于 transformer 的 NLP 系统鲁棒性的框架，它具有模型无关性，不需要修改模型参数或进行复杂的扰动。


<details>
  <summary>Details</summary>
Motivation: 随着这些模型在规模和部署方面的增长，它们在输入扰动下的鲁棒性成为一个日益紧迫的问题。现有的鲁棒性方法通常在小参数模型和大型模型 (LLM) 之间存在差异，并且它们通常依赖于劳动密集型的、特定于样本的对抗性设计。

Method: 提出了一种新颖的距离映射失真 (DMD) 测量方法，该方法通过比较以近乎线性的复杂度的方式输入到输出的距离映射来对每个样本的易感性进行排序。

Result: 在攻击效率和鲁棒训练方面表现出显着 gains。

Conclusion: 提出了一个统一的、局部的鲁棒性框架 (SALMAN)，该框架评估模型稳定性，而无需修改内部参数或求助于复杂的扰动启发式方法。通过证明攻击效率和稳健训练方面的显着 gains，将该框架定位为一种实用的、模型不可知的工具，用于提高基于 transformer 的 NLP 系统的可靠性。

Abstract: Recent strides in pretrained transformer-based language models have propelled
state-of-the-art performance in numerous NLP tasks. Yet, as these models grow
in size and deployment, their robustness under input perturbations becomes an
increasingly urgent question. Existing robustness methods often diverge between
small-parameter and large-scale models (LLMs), and they typically rely on
labor-intensive, sample-specific adversarial designs. In this paper, we propose
a unified, local (sample-level) robustness framework (SALMAN) that evaluates
model stability without modifying internal parameters or resorting to complex
perturbation heuristics. Central to our approach is a novel Distance Mapping
Distortion (DMD) measure, which ranks each sample's susceptibility by comparing
input-to-output distance mappings in a near-linear complexity manner. By
demonstrating significant gains in attack efficiency and robust training, we
position our framework as a practical, model-agnostic tool for advancing the
reliability of transformer-based NLP systems.

</details>


### [144] [Learning Spatio-Temporal Dynamics via Operator-Valued RKHS and Kernel Koopman Methods](https://arxiv.org/abs/2508.18307)
*Mahishanka Withanachchi*

Main category: cs.LG

TL;DR: unified framework for learning the spatio-temporal dynamics of vector valued functions


<details>
  <summary>Details</summary>
Motivation: learning the spatio-temporal dynamics of vector valued functions

Method: combining operator valued reproducing kernel Hilbert spaces (OV-RKHS) with kernel based Koopman operator methods

Result: representer theorems for time dependent OV-RKHS interpolation, derive Sobolev type approximation bounds for smooth vector fields, and provide spectral convergence guarantees for kernel Koopman operator approximations

Conclusion: This framework supports efficient reduced order modeling and long term prediction of high dimensional nonlinear systems, offering theoretically grounded tools for forecasting, control, and uncertainty quantification in spatio-temporal machine learning.

Abstract: We introduce a unified framework for learning the spatio-temporal dynamics of
vector valued functions by combining operator valued reproducing kernel Hilbert
spaces (OV-RKHS) with kernel based Koopman operator methods. The approach
enables nonparametric and data driven estimation of complex time evolving
vector fields while preserving both spatial and temporal structure. We
establish representer theorems for time dependent OV-RKHS interpolation, derive
Sobolev type approximation bounds for smooth vector fields, and provide
spectral convergence guarantees for kernel Koopman operator approximations.
This framework supports efficient reduced order modeling and long term
prediction of high dimensional nonlinear systems, offering theoretically
grounded tools for forecasting, control, and uncertainty quantification in
spatio-temporal machine learning.

</details>


### [145] [CoPE: A Lightweight Complex Positional Encoding](https://arxiv.org/abs/2508.18308)
*Avinash Amballa*

Main category: cs.LG

TL;DR: CoPE是一种新型的位置编码架构，它使用复值编码和相位感知注意力来提高transformer模型的性能。


<details>
  <summary>Details</summary>
Motivation: 为了在transformer架构中有效利用位置编码，提供建模不同序列位置元素之间依赖关系。

Method: 使用复值编码来编码内容和位置信息，并引入相位感知注意力。

Result: CoPE没有表现出长期衰减，并且与线性注意力兼容。

Conclusion: CoPE在GLUE基准测试中表现出色，计算复杂度低于RoPE、Sinusoidal和Learned位置编码。

Abstract: Recent studies have demonstrated the effectiveness of position encoding in
transformer architectures. By incorporating positional information, this
approach provides essential guidance for modeling dependencies between elements
across different sequence positions. We introduce CoPE (a lightweight Complex
Positional Encoding), a novel architecture that leverages complex-valued
encoding to encode both content and positional information. Our approach
replaces traditional positional encodings with complex embeddings where the
real part captures semantic content and the imaginary part encodes positional
information. We introduce phase-aware attention in the first layer of the
transformer model to capture position-dependent patterns, followed by standard
attention layers for higher-levels. We show that CoPE doesn't exhibit long term
decay and is compatible with linear attention. Experimental evaluation on the
GLUE benchmark suggest that our approach achieves superior performance with
less computational complexity, compared to RoPE, Sinusoidal and Learned
positional encodings.

</details>


### [146] [What Matters in Data for DPO?](https://arxiv.org/abs/2508.18312)
*Yu Pan,Zhongze Cai,Guanting Chen,Huaiyang Zhong,Chonghuan Wang*

Main category: cs.LG

TL;DR: 本文研究了偏好数据分布对 DPO 的影响，发现选择响应的质量至关重要，而拒绝响应的质量影响有限。


<details>
  <summary>Details</summary>
Motivation: 直接偏好优化 (DPO) 已经成为一种简单有效的方法，用于使大型语言模型 (LLM) 与人类偏好保持一致，而无需学习奖励模型。尽管 DPO 越来越受欢迎，但一个根本问题仍然悬而未决：偏好数据的哪些特征对于 DPO 性能至关重要？

Method: 通过理论和实证的角度，系统地研究偏好数据分布如何影响 DPO。

Result: 选择响应的质量在优化 DPO 目标中起主导作用，而拒绝响应的质量可能影响相对有限。对比有助于主要通过改进选择的样本。在线 DPO 设置有效地减少到对选择的响应进行监督微调。

Conclusion: 提高选择响应的质量始终能提高性能，而与拒绝响应的质量无关。混合 on-policy 数据是有益的。

Abstract: Direct Preference Optimization (DPO) has emerged as a simple and effective
approach for aligning large language models (LLMs) with human preferences,
bypassing the need for a learned reward model. Despite its growing adoption, a
fundamental question remains open: what characteristics of preference data are
most critical for DPO performance? In this work, we provide a systematic study
of how preference data distribution influences DPO, from both theoretical and
empirical perspectives. We show that the quality of chosen responses plays a
dominant role in optimizing the DPO objective, while the quality of rejected
responses may have relatively limited impact. Our theoretical analysis
characterizes the optimal response distribution under DPO and reveals how
contrastiveness between responses helps primarily by improving the chosen
samples. We further study an online DPO setting and show it effectively reduces
to supervised fine-tuning on the chosen responses. Extensive experiments across
diverse tasks confirm our findings: improving the quality of chosen responses
consistently boosts performance regardless of the quality of the rejected
responses. We also investigate the benefit of mixing the on-policy data. Our
results interpret the mechanism behind some widely adopted strategies and offer
practical insights for constructing high-impact preference datasets for LLM
alignment.

</details>


### [147] [ProtoEHR: Hierarchical Prototype Learning for EHR-based Healthcare Predictions](https://arxiv.org/abs/2508.18313)
*Zi Cai,Yu Liu,Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: ProtoEHR是一个利用EHR数据的多层次结构进行医疗预测的可解释分层原型学习框架，并在多个临床任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常侧重于EHR数据的孤立组成部分，限制了它们的预测性能和可解释性。

Method: 提出ProtoEHR，一个可解释的分层原型学习框架，充分利用EHR数据的多层次结构来增强医疗预测。利用大型语言模型提取医学代码之间的语义关系，构建医学知识图谱。设计分层表示学习框架，捕获跨三个层次的上下文表示，同时在每个层次中加入原型信息。

Result: ProtoEHR在两个公共数据集上，针对五个临床任务进行了评估，结果表明ProtoEHR能够做出准确、稳健和可解释的预测，优于文献中的基线。

Conclusion: ProtoEHR在五个临床任务中表现出色，提供准确、稳健和可解释的预测，并提供代码、访问和患者层面的可解释见解。

Abstract: Digital healthcare systems have enabled the collection of mass healthcare
data in electronic healthcare records (EHRs), allowing artificial intelligence
solutions for various healthcare prediction tasks. However, existing studies
often focus on isolated components of EHR data, limiting their predictive
performance and interpretability. To address this gap, we propose ProtoEHR, an
interpretable hierarchical prototype learning framework that fully exploits the
rich, multi-level structure of EHR data to enhance healthcare predictions. More
specifically, ProtoEHR models relationships within and across three
hierarchical levels of EHRs: medical codes, hospital visits, and patients. We
first leverage large language models to extract semantic relationships among
medical codes and construct a medical knowledge graph as the knowledge source.
Building on this, we design a hierarchical representation learning framework
that captures contextualized representations across three levels, while
incorporating prototype information within each level to capture intrinsic
similarities and improve generalization. To perform a comprehensive assessment,
we evaluate ProtoEHR in two public datasets on five clinically significant
tasks, including prediction of mortality, prediction of readmission, prediction
of length of stay, drug recommendation, and prediction of phenotype. The
results demonstrate the ability of ProtoEHR to make accurate, robust, and
interpretable predictions compared to baselines in the literature. Furthermore,
ProtoEHR offers interpretable insights on code, visit, and patient levels to
aid in healthcare prediction.

</details>


### [148] [Evaluating Federated Learning for At-Risk Student Prediction: A Comparative Analysis of Model Complexity and Data Balancing](https://arxiv.org/abs/2508.18316)
*Rodrigo Tertulino*

Main category: cs.LG

TL;DR: A federated learning model predicts at-risk students in distance education with 85% accuracy, addressing data privacy concerns.


<details>
  <summary>Details</summary>
Motivation: High dropout and failure rates in distance education necessitate proactive identification of at-risk students for timely support.

Method: Development and evaluation of a machine learning model based on early academic performance and digital engagement patterns from the large-scale OULAD dataset, implemented using a Federated Learning (FL) framework.

Result: The federated model achieves an ROC AUC score of approximately 85% in identifying at-risk students.

Conclusion: This federated learning approach offers a practical and scalable solution for institutions to build effective early-warning systems, enabling proactive student support while respecting data privacy.

Abstract: High dropout and failure rates in distance education pose a significant
challenge for academic institutions, making the proactive identification of
at-risk students crucial for providing timely support. This study develops and
evaluates a machine learning model based on early academic performance and
digital engagement patterns from the large-scale OULAD dataset to predict
student risk at a UK university. To address the practical challenges of data
privacy and institutional silos that often hinder such initiatives, we
implement the model using a Federated Learning (FL) framework. We compare model
complexity (Logistic Regression vs. a Deep Neural Network) and data balancing.
The final federated model demonstrates strong predictive capability, achieving
an ROC AUC score of approximately 85% in identifying at-risk students. Our
findings show that this federated approach provides a practical and scalable
solution for institutions to build effective early-warning systems, enabling
proactive student support while inherently respecting data privacy.

</details>


### [149] [ZTFed-MAS2S: A Zero-Trust Federated Learning Framework with Verifiable Privacy and Trust-Aware Aggregation for Wind Power Data Imputation](https://arxiv.org/abs/2508.18318)
*Yang Li,Hanjie Wang,Yuanzheng Li,Jiazheng Li,Zhaoyang Dong*

Main category: cs.LG

TL;DR: 提出了一种零信任联邦学习框架 ZTFed-MAS2S，用于解决风电数据中的缺失值问题，该框架集成了多头注意力机制、可验证差分隐私、零知识证明和动态信任感知聚合等技术，以确保隐私、安全和效率。


<details>
  <summary>Details</summary>
Motivation: 风电数据经常因传感器故障和边缘站点的不稳定传输而遭受缺失值的影响。虽然联邦学习能够在不共享原始数据的情况下实现保护隐私的协作，但它在参数交换过程中仍然容易受到异常更新和隐私泄露的影响。这些挑战在开放的工业环境中被放大，需要零信任机制，在这种机制中，没有参与者是天生值得信任的。

Method: 集成了基于多头注意力机制的序列到序列插补模型的零信任联邦学习框架 ZTFed-MAS2S。

Result: ZTFed 集成了可验证差分隐私与非交互式零知识证明以及机密性和完整性验证机制，以确保可验证的隐私保护和安全的模型参数传输。采用了一种动态的信任感知聚合机制，其中信任通过相似性图传播以增强鲁棒性，并通过基于稀疏性和量化的压缩来减少通信开销。MAS2S 捕获风电数据中的长期依赖关系以实现准确的插补。

Conclusion: 在真实风电场数据集上的大量实验验证了 ZTFed-MAS2S 在联邦学习性能和缺失数据插补方面的优越性，证明了它作为能源领域实际应用的安全高效解决方案的有效性。

Abstract: Wind power data often suffers from missing values due to sensor faults and
unstable transmission at edge sites. While federated learning enables
privacy-preserving collaboration without sharing raw data, it remains
vulnerable to anomalous updates and privacy leakage during parameter exchange.
These challenges are amplified in open industrial environments, necessitating
zero-trust mechanisms where no participant is inherently trusted. To address
these challenges, this work proposes ZTFed-MAS2S, a zero-trust federated
learning framework that integrates a multi-head attention-based
sequence-to-sequence imputation model. ZTFed integrates verifiable differential
privacy with non-interactive zero-knowledge proofs and a confidentiality and
integrity verification mechanism to ensure verifiable privacy preservation and
secure model parameters transmission. A dynamic trust-aware aggregation
mechanism is employed, where trust is propagated over similarity graphs to
enhance robustness, and communication overhead is reduced via sparsity- and
quantization-based compression. MAS2S captures long-term dependencies in wind
power data for accurate imputation. Extensive experiments on real-world wind
farm datasets validate the superiority of ZTFed-MAS2S in both federated
learning performance and missing data imputation, demonstrating its
effectiveness as a secure and efficient solution for practical applications in
the energy sector.

</details>


### [150] [Linear cost mutual information estimation and independence test of similar performance as HSIC](https://arxiv.org/abs/2508.18338)
*Jarek Duda,Jagoda Bracha,Adrian Przybysz*

Main category: cs.LG

TL;DR: HSIC is too slow. HCR is a faster and more sensitive alternative for evaluating statistical dependencies.


<details>
  <summary>Details</summary>
Motivation: HSIC is computationally expensive for large datasets due to its O(n^2.37) complexity.

Method: Hierarchical Correlation Reconstruction (HCR)

Result: HCR offers linear cost and higher dependence sensitivity, providing a joint distribution model through mixed moments.

Conclusion: HCR (Hierarchical Correlation Reconstruction) is a linear cost alternative to HSIC with higher dependence sensitivity.

Abstract: Evaluation of statistical dependencies between two data samples is a basic
problem of data science/machine learning, and HSIC (Hilbert-Schmidt Information
Criterion)~\cite{HSIC} is considered the state-of-art method. However, for size
$n$ data sample it requires multiplication of $n\times n$ matrices, what
currently needs $\sim O(n^{2.37})$ computational complexity~\cite{mult}, making
it impractical for large data samples. We discuss HCR (Hierarchical Correlation
Reconstruction) as its linear cost practical alternative of even higher
dependence sensitivity in tests, and additionally providing actual joint
distribution model by description of dependencies through features being mixed
moments, starting with correlation and homoscedasticity, also allowing to
approximate mutual information as just sum of squares of such nontrivial mixed
moments between two data samples. Such single dependence describing feature is
calculated in $O(n)$ linear time. Their number to test varies with dimension
$d$ - requiring $O(d^2)$ for pairwise dependencies, $O(d^3)$ if wanting to also
consider more subtle triplewise, and so on.

</details>


### [151] [DualSparse-MoE: Coordinating Tensor/Neuron-Level Sparsity with Expert Partition and Reconstruction](https://arxiv.org/abs/2508.18376)
*Weilin Cai,Le Qin,Shwai He,Junwei Cui,Ang Li,Jiayi Huang*

Main category: cs.LG

TL;DR: 提出DualSparse-MoE，通过动态张量级计算 dropping 与静态神经元级重建，在最小精度损失下显著提升效率。


<details>
  <summary>Details</summary>
Motivation: MoE由于其巨大的计算规模和不可预测的激活模式，仍然面临着巨大的挑战。为了实现高效的MoE部署，我们认为在预训练的MoE模块中，张量和神经元级别的双重稀疏性是准确性和效率的关键因素。

Method: 我们引入了后训练专家分区，以在不重新训练的情况下诱导这种稀疏性。在此基础上，我们提出 DualSparse-MoE，这是一种推理系统，它集成了动态张量级计算 dropping 与静态神经元级重建，以在最小的精度损失下提供显着的效率提升。

Result: 在三个主流MoE模型上强制执行大约25%的drop rate，平均精度仅降低了0.08%-0.28%，几乎所有程度的计算下降都持续产生成比例的计算加速。将负载不平衡感知纳入专家并行处理，仅以0.5%的平均精度下降实现了1.41倍的MoE模块加速。

Conclusion: 通过在三个主流MoE模型上强制执行大约25%的drop rate，我们的方法将平均精度降低了仅0.08%-0.28%，同时几乎所有程度的计算下降都持续产生成比例的计算加速。此外，将负载不平衡感知纳入专家并行处理，仅以0.5%的平均精度下降实现了1.41倍的MoE模块加速。

Abstract: Mixture of Experts (MoE) has become a mainstream architecture for building
Large Language Models (LLMs) by reducing per-token computation while enabling
model scaling. It can be viewed as partitioning a large Feed-Forward Network
(FFN) at the tensor level into fine-grained sub-FFNs, or experts, and
activating only a sparse subset for each input. While this sparsity improves
efficiency, MoE still faces substantial challenges due to their massive
computational scale and unpredictable activation patterns.
  To enable efficient MoE deployment, we identify dual sparsity at the tensor
and neuron levels in pre-trained MoE modules as a key factor for both accuracy
and efficiency. Unlike prior work that increases tensor-level sparsity through
finer-grained expert design during pre-training, we introduce post-training
expert partitioning to induce such sparsity without retraining. This preserves
the mathematical consistency of model transformations and enhances both
efficiency and accuracy in subsequent fine-tuning and inference. Building upon
this, we propose DualSparse-MoE, an inference system that integrates dynamic
tensor-level computation dropping with static neuron-level reconstruction to
deliver significant efficiency gains with minimal accuracy loss.
  Experimental results show that enforcing an approximate 25% drop rate with
our approach reduces average accuracy by only 0.08%-0.28% across three
prevailing MoE models, while nearly all degrees of computation dropping
consistently yield proportional computational speedups. Furthermore,
incorporating load-imbalance awareness into expert parallelism achieves a 1.41x
MoE module speedup with just 0.5% average accuracy degradation.

</details>


### [152] [Low-Rank Tensor Decompositions for the Theory of Neural Networks](https://arxiv.org/abs/2508.18408)
*Ricardo Borsoi,Konstantin Usevich,Marianne Clausel*

Main category: cs.LG

TL;DR: 本文综述了低秩张量方法在理论上解释深度神经网络性能不同方面的作用，包括表达性、算法可学习性和计算硬度、泛化和可识别性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络 (NN) 的突破性性能促进了人们对为深度学习理论提供数学基础的兴趣激增。低秩张量分解特别适合这项任务，因为它们与神经网络及其丰富的理论结果密切相关。

Method: 低秩张量分解

Result: 不同的张量分解具有很强的唯一性保证，这使得可以直接解释它们的因素，并且已经提出了多项式时间算法来计算它们。通过张量和神经网络之间的联系，这些结果支持了神经网络理论中的许多重要进展。

Conclusion: 低秩张量方法在从理论上解释深度神经网络 (NN) 性能的不同方面（包括其表达性、算法可学习性和计算硬度、泛化和可识别性）方面发挥着 фундаментальную роль。

Abstract: The groundbreaking performance of deep neural networks (NNs) promoted a surge
of interest in providing a mathematical basis to deep learning theory. Low-rank
tensor decompositions are specially befitting for this task due to their close
connection to NNs and their rich theoretical results. Different tensor
decompositions have strong uniqueness guarantees, which allow for a direct
interpretation of their factors, and polynomial time algorithms have been
proposed to compute them. Through the connections between tensors and NNs, such
results supported many important advances in the theory of NNs. In this review,
we show how low-rank tensor methods--which have been a core tool in the signal
processing and machine learning communities--play a fundamental role in
theoretically explaining different aspects of the performance of deep NNs,
including their expressivity, algorithmic learnability and computational
hardness, generalization, and identifiability. Our goal is to give an
accessible overview of existing approaches (developed by different communities,
ranging from computer science to mathematics) in a coherent and unified way,
and to open a broader perspective on the use of low-rank tensor decompositions
for the theory of deep NNs.

</details>


### [153] [LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning](https://arxiv.org/abs/2508.18420)
*André Quadros,Cassio Silva,Ronnie Alves*

Main category: cs.LG

TL;DR: combines Variational State as Intrinsic Reward (VSIMR) with Large Language Models (LLMs) to improve the efficiency of reinforcement learning (RL) agents in environments with extreme sparse rewards


<details>
  <summary>Details</summary>
Motivation: improve the efficiency of reinforcement learning (RL) agents in environments with extreme sparse rewards

Method: integrating Variational State as Intrinsic Reward (VSIMR) with an intrinsic reward approach derived from Large Language Models (LLMs)

Result: this combined strategy significantly increases agent performance and sampling efficiency compared to using each strategy individually or a standard A2C agent

Conclusion: This combined strategy significantly increases agent performance and sampling efficiency compared to using each strategy individually or a standard A2C agent.

Abstract: This paper explores the combination of two intrinsic motivation strategies to
improve the efficiency of reinforcement learning (RL) agents in environments
with extreme sparse rewards, where traditional learning struggles due to
infrequent positive feedback. We propose integrating Variational State as
Intrinsic Reward (VSIMR), which uses Variational AutoEncoders (VAEs) to reward
state novelty, with an intrinsic reward approach derived from Large Language
Models (LLMs). The LLMs leverage their pre-trained knowledge to generate reward
signals based on environment and goal descriptions, guiding the agent. We
implemented this combined approach with an Actor-Critic (A2C) agent in the
MiniGrid DoorKey environment, a benchmark for sparse rewards. Our empirical
results show that this combined strategy significantly increases agent
performance and sampling efficiency compared to using each strategy
individually or a standard A2C agent, which failed to learn. Analysis of
learning curves indicates that the combination effectively complements
different aspects of the environment and task: VSIMR drives exploration of new
states, while the LLM-derived rewards facilitate progressive exploitation
towards goals.

</details>


### [154] [Enhancing Trust-Region Bayesian Optimization via Newton Methods](https://arxiv.org/abs/2508.18423)
*Quanlin Chen,Yiyu Chen,Jing Huo,Tianyu Ding,Yang Gao,Yuetong Chen*

Main category: cs.LG

TL;DR: 提出了一种新的高维BO方法，该方法利用全局GP的梯度和Hessian构建多个局部二次模型，以提高采样效率并保持异构建模。


<details>
  <summary>Details</summary>
Motivation: 在高维空间中扩展BO仍然具有挑战性。现有文献提出在多个局部信任区域（TuRBO）中执行标准BO，以对目标函数进行异构建模并避免过度探索。尽管有其优点，但与全局GP相比，使用局部高斯过程（GP）会降低采样效率。

Method: 利用全局GP的梯度和Hessian构建多个局部二次模型，并通过求解有界约束二次规划来选择新的样本点。

Result: 该方法提高了TuRBO的效率。

Conclusion: 该方法在合成函数和实际应用中优于各种高维BO技术，并提高了TuRBO的效率。

Abstract: Bayesian Optimization (BO) has been widely applied to optimize expensive
black-box functions while retaining sample efficiency. However, scaling BO to
high-dimensional spaces remains challenging. Existing literature proposes
performing standard BO in multiple local trust regions (TuRBO) for
heterogeneous modeling of the objective function and avoiding over-exploration.
Despite its advantages, using local Gaussian Processes (GPs) reduces sampling
efficiency compared to a global GP. To enhance sampling efficiency while
preserving heterogeneous modeling, we propose to construct multiple local
quadratic models using gradients and Hessians from a global GP, and select new
sample points by solving the bound-constrained quadratic program. Additionally,
we address the issue of vanishing gradients of GPs in high-dimensional spaces.
We provide a convergence analysis and demonstrate through experimental results
that our method enhances the efficacy of TuRBO and outperforms a wide range of
high-dimensional BO techniques on synthetic functions and real-world
applications.

</details>


### [155] [VERIRL: Boosting the LLM-based Verilog Code Generation via Reinforcement Learning](https://arxiv.org/abs/2508.18462)
*Fu Teng,Miao Pan,Xuhong Zhang,Zhezhi He,Yiyao Yang,Xinyi Chai,Mengnan Qi,Liqiang Lu,Jianwei Yin*

Main category: cs.LG

TL;DR: Introduces a RL framework for Verilog code generation with a new dataset and techniques to improve reward signals and learning dynamics, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: HDLs such as Verilog remain underexplored due to their concurrency semantics, syntactic rigidity, and simulation complexity.

Method: A reinforcement learning (RL) framework tailored for Verilog code generation with Trace-back based Rescore mechanism and a sample-balanced weighting strategy.

Result: State-of-the-art performance in Verilog generation tasks, with substantial gains in test pass rate, functional correctness, and compilation robustness.

Conclusion: RL-driven approaches have potential for structured code generation in hardware-centric domains.

Abstract: Recent advancements in code generation have shown remarkable success across
software domains, yet hardware description languages (HDLs) such as Verilog
remain underexplored due to their concurrency semantics, syntactic rigidity,
and simulation complexity. In this work, we address these challenges by
introducing a reinforcement learning (RL) framework tailored for Verilog code
generation. We first construct Veribench-53K, a high-quality dataset curated
from over 700K Verilog problems, enriched with structured prompts, complexity
labels, and diverse testbenches. To tackle the problem of sparse and noisy
reward signals, we propose a Trace-back based Rescore mechanism that leverages
reasoning paths and iterative refinement to enhance feedback reliability and
support reward model training. Furthermore, to mitigate catastrophic forgetting
and overfitting during RL fine-tuning, we introduce a sample-balanced weighting
strategy that adaptively balances learning dynamics based on reward-probability
distributions. These innovations are integrated into an iterative RL pipeline
that co-evolves the policy and reward models. In contrast to recent work such
as CraftRTL, which relies on large-scale closed-source model distillation, and
DeepSeek-style approaches that struggle with sparse feedback, our method
demonstrates superior performance using a smaller but high-quality dataset
combined with RL optimization. Experiments on Verilog generation tasks
demonstrate state-of-the-art performance, with substantial gains in test pass
rate, functional correctness, and compilation robustness. Our findings
highlight the potential of RL-driven approaches for structured code generation
in hardware-centric domains. VERIRL is publicly available at
https://github.com/omniAI-Lab/VeriRL.

</details>


### [156] [DRTA: Dynamic Reward Scaling for Reinforcement Learning in Time Series Anomaly Detection](https://arxiv.org/abs/2508.18474)
*Bahareh Golchin,Banafsheh Rekabdar,Kunpeng Liu*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的异常检测框架，该框架优于现有的无监督和半监督方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常难以处理有限的标记数据、高误报率以及难以推广到新型异常类型。

Method: 基于强化学习的框架，集成了动态奖励塑造、变分自动编码器 (VAE) 和主动学习 (DRTA)。

Result: 在 Yahoo A1 和 Yahoo A2 基准数据集上的实验结果表明，该方法始终优于最先进的无监督和半监督方法。

Conclusion: 该框架在实际异常检测任务中具有可扩展性和高效性。

Abstract: Anomaly detection in time series data is important for applications in
finance, healthcare, sensor networks, and industrial monitoring. Traditional
methods usually struggle with limited labeled data, high false-positive rates,
and difficulty generalizing to novel anomaly types. To overcome these
challenges, we propose a reinforcement learning-based framework that integrates
dynamic reward shaping, Variational Autoencoder (VAE), and active learning,
called DRTA. Our method uses an adaptive reward mechanism that balances
exploration and exploitation by dynamically scaling the effect of VAE-based
reconstruction error and classification rewards. This approach enables the
agent to detect anomalies effectively in low-label systems while maintaining
high precision and recall. Our experimental results on the Yahoo A1 and Yahoo
A2 benchmark datasets demonstrate that the proposed method consistently
outperforms state-of-the-art unsupervised and semi-supervised approaches. These
findings show that our framework is a scalable and efficient solution for
real-world anomaly detection tasks.

</details>


### [157] [Data Augmentation Improves Machine Unlearning](https://arxiv.org/abs/2508.18502)
*Andreza M. C. Falcao,Filipe R. Cordeiro*

Main category: cs.LG

TL;DR: Data augmentation improves machine unlearning effectiveness and reduces the performance gap to retrained models.


<details>
  <summary>Details</summary>
Motivation: The role of systematic augmentation design in machine unlearning remains under-investigated.

Method: Investigating the impact of different data augmentation strategies on the performance of unlearning methods, including SalUn, Random Label, and Fine-Tuning.

Result: Using TrivialAug augmentation, the Average Gap unlearning Metric is reduced by up to 40.12%.

Conclusion: Proper augmentation design can significantly improve unlearning effectiveness, reducing the performance gap to retrained models and achieving privacy-preserving and efficient unlearning.

Abstract: Machine Unlearning (MU) aims to remove the influence of specific data from a
trained model while preserving its performance on the remaining data. Although
a few works suggest connections between memorisation and augmentation, the role
of systematic augmentation design in MU remains under-investigated. In this
work, we investigate the impact of different data augmentation strategies on
the performance of unlearning methods, including SalUn, Random Label, and
Fine-Tuning. Experiments conducted on CIFAR-10 and CIFAR-100, under varying
forget rates, show that proper augmentation design can significantly improve
unlearning effectiveness, reducing the performance gap to retrained models.
Results showed a reduction of up to 40.12% of the Average Gap unlearning
Metric, when using TrivialAug augmentation. Our results suggest that
augmentation not only helps reduce memorization but also plays a crucial role
in achieving privacy-preserving and efficient unlearning.

</details>


### [158] [Breaking Through Barren Plateaus: Reinforcement Learning Initializations for Deep Variational Quantum Circuits](https://arxiv.org/abs/2508.18514)
*Yifeng Peng,Xinyi Li,Zhemin Zhang,Samuel Yen-Chi Chen,Zhiding Liang,Ying Wang*

Main category: cs.LG

TL;DR: RL-based initialization alleviates the barren plateau problem in VQAs, enhancing convergence and solution quality.


<details>
  <summary>Details</summary>
Motivation: VQAs suffer from the barren plateau problem, where gradients diminish exponentially, hindering training.

Method: Reinforcement learning (RL) algorithms (Deterministic Policy Gradient, Soft Actor-Critic, and Proximal Policy Optimization, etc.) are used to generate circuit parameters that minimize the VQAs cost function before gradient-based optimization.

Result: RL-based initialization enhances convergence speed and final solution quality under various noise conditions and tasks. Multiple RL approaches achieve comparable performance gains.

Conclusion: RL-based initialization significantly enhances VQA convergence speed and solution quality, offering a promising avenue for integrating machine learning into quantum algorithm design.

Abstract: Variational Quantum Algorithms (VQAs) have gained prominence as a viable
framework for exploiting near-term quantum devices in applications ranging from
optimization and chemistry simulation to machine learning. However, the
effectiveness of VQAs is often constrained by the so-called barren plateau
problem, wherein gradients diminish exponentially as system size or circuit
depth increases, thereby hindering training. In this work, we propose a
reinforcement learning (RL)-based initialization strategy to alleviate the
barren plateau issue by reshaping the initial parameter landscape to avoid
regions prone to vanishing gradients. In particular, we explore several RL
algorithms (Deterministic Policy Gradient, Soft Actor-Critic, and Proximal
Policy Optimization, etc.) to generate the circuit parameters (treated as
actions) that minimize the VQAs cost function before standard gradient-based
optimization. By pre-training with RL in this manner, subsequent optimization
using methods such as gradient descent or Adam proceeds from a more favorable
initial state. Extensive numerical experiments under various noise conditions
and tasks consistently demonstrate that the RL-based initialization method
significantly enhances both convergence speed and final solution quality.
Moreover, comparisons among different RL algorithms highlight that multiple
approaches can achieve comparable performance gains, underscoring the
flexibility and robustness of our method. These findings shed light on a
promising avenue for integrating machine learning techniques into quantum
algorithm design, offering insights into how RL-driven parameter initialization
can accelerate the scalability and practical deployment of VQAs. Opening up a
promising path for the research community in machine learning for quantum,
especially barren plateau problems in VQAs.

</details>


### [159] [Quantifying The Limits of AI Reasoning: Systematic Neural Network Representations of Algorithms](https://arxiv.org/abs/2508.18526)
*Anastasis Kratsios,Dennis Zvigelsky,Bradd Hart*

Main category: cs.LG

TL;DR: The paper proves that neural networks can perform any reasoning task by exactly emulating circuits, with applications in algorithm emulation and exceeding the power of universal approximation theorems.


<details>
  <summary>Details</summary>
Motivation: Quantifying the forms of reasoning neural networks can perform when perfectly trained is a main open question in contemporary AI research.

Method: The paper introduces a systematic meta-algorithm that converts any circuit into a feedforward neural network (NN) with ReLU activations by iteratively replacing each gate with a canonical ReLU MLP emulator.

Result: The paper shows the construction emulates the circuit exactly, with the number of neurons scaling with the circuit's complexity, and derives a range of applications, including emulating shortest-path algorithms and simulating stopped Turing machines. The result is strictly more powerful than a classical universal approximation theorem.

Conclusion: This paper demonstrates that no reasoning task lies beyond the reach of neural networks by presenting a meta-algorithm that converts any circuit into a feedforward neural network with ReLU activations, exactly emulating the circuit.

Abstract: A main open question in contemporary AI research is quantifying the forms of
reasoning neural networks can perform when perfectly trained. This paper
answers this by interpreting reasoning tasks as circuit emulation, where the
gates define the type of reasoning; e.g. Boolean gates for predicate logic,
tropical circuits for dynamic programming, arithmetic and analytic gates for
symbolic mathematical representation, and hybrids thereof for deeper reasoning;
e.g. higher-order logic.
  We present a systematic meta-algorithm that converts essentially any circuit
into a feedforward neural network (NN) with ReLU activations by iteratively
replacing each gate with a canonical ReLU MLP emulator. We show that, on any
digital computer, our construction emulates the circuit exactly--no
approximation, no rounding, modular overflow included--demonstrating that no
reasoning task lies beyond the reach of neural networks. The number of neurons
in the resulting network (parametric complexity) scales with the circuit's
complexity, and the network's computational graph (structure) mirrors that of
the emulated circuit. This formalizes the folklore that NNs networks trade
algorithmic run-time (circuit runtime) for space complexity (number of
neurons).
  We derive a range of applications of our main result, from emulating
shortest-path algorithms on graphs with cubic--size NNs, to simulating stopped
Turing machines with roughly quadratically--large NNs, and even the emulation
of randomized Boolean circuits. Lastly, we demonstrate that our result is
strictly more powerful than a classical universal approximation theorem: any
universal function approximator can be encoded as a circuit and directly
emulated by a NN.

</details>


### [160] [BTW: A Non-Parametric Variance Stabilization Framework for Multimodal Model Integration](https://arxiv.org/abs/2508.18551)
*Jun Hou,Le Wang,Xuan Wang*

Main category: cs.LG

TL;DR: Proposes BTW, a bi-level weighting framework using KL divergence and MI to dynamically adjust modality importance in MoE models, improving performance in multimodal learning tasks.


<details>
  <summary>Details</summary>
Motivation: The effectiveness of Mixture-of-Experts (MoE) models remains unclear when additional modalities introduce more noise than complementary information. Existing approaches struggle to scale beyond two modalities and lack the resolution needed for instance-level control.

Method: A bi-level, non-parametric weighting framework that combines instance-level Kullback-Leibler (KL) divergence and modality-level mutual information (MI) to dynamically adjust modality importance during training.

Result: Demonstrates significant improvements in regression performance and multiclass classification accuracy on sentiment regression and clinical classification.

Conclusion: The proposed BTW method significantly improves regression performance and multiclass classification accuracy on sentiment regression and clinical classification tasks.

Abstract: Mixture-of-Experts (MoE) models have become increasingly powerful in
multimodal learning by enabling modular specialization across modalities.
However, their effectiveness remains unclear when additional modalities
introduce more noise than complementary information. Existing approaches, such
as the Partial Information Decomposition, struggle to scale beyond two
modalities and lack the resolution needed for instance-level control. We
propose Beyond Two-modality Weighting (BTW), a bi-level, non-parametric
weighting framework that combines instance-level Kullback-Leibler (KL)
divergence and modality-level mutual information (MI) to dynamically adjust
modality importance during training. Our method does not require additional
parameters and can be applied to an arbitrary number of modalities.
Specifically, BTW computes per-example KL weights by measuring the divergence
between each unimodal and the current multimodal prediction, and modality-wide
MI weights by estimating global alignment between unimodal and multimodal
outputs. Extensive experiments on sentiment regression and clinical
classification demonstrate that our method significantly improves regression
performance and multiclass classification accuracy.

</details>


### [161] [Enhancing Chemical Explainability Through Counterfactual Masking](https://arxiv.org/abs/2508.18561)
*Łukasz Janisiów,Marek Kochańczyk,Bartosz Zieliński,Tomasz Danel*

Main category: cs.LG

TL;DR: This paper introduces counterfactual masking, a new method for explaining molecular property predictions by replacing masked substructures with chemically reasonable fragments, leading to more realistic and actionable explanations.


<details>
  <summary>Details</summary>
Motivation: Existing explainable AI methods rely on masking strategies that remove either atoms or atom-level features, which often fail to adhere to the underlying molecular distribution and thus yield unintuitive explanations.

Method: This paper proposes counterfactual masking, a novel framework that replaces masked substructures with chemically reasonable fragments sampled from generative models trained to complete molecular graphs.

Result: Counterfactual masking is well-suited for benchmarking model explainers and yields more actionable insights across multiple datasets and property prediction tasks.

Conclusion: Counterfactual masking bridges the gap between explainability and molecular design, offering a principled and generative path toward explainable machine learning in chemistry.

Abstract: Molecular property prediction is a crucial task that guides the design of new
compounds, including drugs and materials. While explainable artificial
intelligence methods aim to scrutinize model predictions by identifying
influential molecular substructures, many existing approaches rely on masking
strategies that remove either atoms or atom-level features to assess importance
via fidelity metrics. These methods, however, often fail to adhere to the
underlying molecular distribution and thus yield unintuitive explanations. In
this work, we propose counterfactual masking, a novel framework that replaces
masked substructures with chemically reasonable fragments sampled from
generative models trained to complete molecular graphs. Rather than evaluating
masked predictions against implausible zeroed-out baselines, we assess them
relative to counterfactual molecules drawn from the data distribution. Our
method offers two key benefits: (1) molecular realism underpinning robust and
distribution-consistent explanations, and (2) meaningful counterfactuals that
directly indicate how structural modifications may affect predicted properties.
We demonstrate that counterfactual masking is well-suited for benchmarking
model explainers and yields more actionable insights across multiple datasets
and property prediction tasks. Our approach bridges the gap between
explainability and molecular design, offering a principled and generative path
toward explainable machine learning in chemistry.

</details>


### [162] [A Note on Graphon-Signal Analysis of Graph Neural Networks](https://arxiv.org/abs/2508.18564)
*Levi Rauchwerger,Ron Levie*

Main category: cs.LG

TL;DR: This paper extends the results of a recent paper to address the shortcomings of applicability in practical settings of graph machine learning.


<details>
  <summary>Details</summary>
Motivation: There are some missing ingredients in a recent paper, limiting its applicability in practical settings of graph machine learning.

Method: Based on extensions of standard results in graphon analysis to graphon-signals

Result: proved a generalization bound and a sampling lemma for MPNNs

Conclusion: This paper introduces several refinements and extensions to existing results that address the shortcomings of applicability in practical settings of graph machine learning. The extensions are: 1) extend the main results in the paper to graphon-signals with multidimensional signals, 2) extend the Lipschitz continuity to MPNNs with readout with respect to cut distance, 3) improve the generalization bound by utilizing robustness-type generalization bounds, and 4) extend the analysis to non-symmetric graphons and kernels.

Abstract: A recent paper, ``A Graphon-Signal Analysis of Graph Neural Networks'', by
Levie, analyzed message passing graph neural networks (MPNNs) by embedding the
input space of MPNNs, i.e., attributed graphs (graph-signals), to a space of
attributed graphons (graphon-signals). Based on extensions of standard results
in graphon analysis to graphon-signals, the paper proved a generalization bound
and a sampling lemma for MPNNs. However, there are some missing ingredients in
that paper, limiting its applicability in practical settings of graph machine
learning. In the current paper, we introduce several refinements and extensions
to existing results that address these shortcomings. In detail, 1) we extend
the main results in the paper to graphon-signals with multidimensional signals
(rather than 1D signals), 2) we extend the Lipschitz continuity to MPNNs with
readout with respect to cut distance (rather than MPNNs without readout with
respect to cut metric), 3) we improve the generalization bound by utilizing
robustness-type generalization bounds, and 4) we extend the analysis to
non-symmetric graphons and kernels.

</details>


### [163] [Improving Long-term Autoregressive Spatiotemporal Predictions: A Proof of Concept with Fluid Dynamics](https://arxiv.org/abs/2508.18565)
*Hao Zhou,Sibo Cheng*

Main category: cs.LG

TL;DR: 提出了一种新的数据驱动预测框架（SPF），该框架在降低内存需求的同时，提高了长期预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的方法正在成为传统数值预测的有效替代方案，提供快速推理和更低的计算成本。然而，对于复杂的系统，由于误差累积，长期准确性通常会下降，而自回归训练（尽管有效）需要大量的GPU内存，并且可能会牺牲短期性能。

Method: 提出随机前推（SPF）框架，该框架保留了单步超前训练，同时实现了多步学习。SPF从模型预测中构建一个补充数据集，并通过随机采集策略将其与地面实况相结合，从而平衡了短期和长期性能，同时减少了过度拟合。多步预测在epochs之间预先计算，保持内存使用稳定，而无需存储完整的展开序列。

Result: 在Burgers方程和浅水基准上的实验表明，SPF实现了比自回归方法更高的长期准确性，同时降低了内存需求。

Conclusion: SPF在降低内存需求的同时，实现了比自回归方法更高的长期准确性，使其在资源有限和复杂的模拟中很有希望。

Abstract: Data-driven methods are emerging as efficient alternatives to traditional
numerical forecasting, offering fast inference and lower computational cost.
Yet, for complex systems, long-term accuracy often deteriorates due to error
accumulation, and autoregressive training (though effective) demands large GPU
memory and may sacrifice short-term performance. We propose the Stochastic
PushForward (SPF) framework, which retains one-step-ahead training while
enabling multi-step learning. SPF builds a supplementary dataset from model
predictions and combines it with ground truth via a stochastic acquisition
strategy, balancing short- and long-term performance while reducing
overfitting. Multi-step predictions are precomputed between epochs, keeping
memory usage stable without storing full unrolled sequences. Experiments on the
Burgers' equation and the Shallow Water benchmark show that SPF achieves higher
long-term accuracy than autoregressive methods while lowering memory
requirements, making it promising for resource-limited and complex simulations.

</details>


### [164] [Sparse Autoencoders for Low-$N$ Protein Function Prediction and Design](https://arxiv.org/abs/2508.18567)
*Darin Tsui,Kunal Talreja,Amirali Aghazadeh*

Main category: cs.LG

TL;DR: SAEs trained on fine-tuned ESM2 embeddings can effectively predict protein function even with limited data, outperforming ESM2 baselines and enabling successful protein design.


<details>
  <summary>Details</summary>
Motivation: Predicting protein function from amino acid sequence remains a central challenge in data-scarce regimes, limiting machine learning-guided protein design when only small amounts of assay-labeled sequence-function data are available.

Method: SAEs trained on fine-tuned ESM2 embeddings

Result: SAEs, with as few as 24 sequences, consistently outperform or compete with their ESM2 baselines in fitness prediction.steering predictive latents exploits biological motifs in pLM representations, yielding top-fitness variants in 83% of cases compared to designing with ESM2 alone.

Conclusion: SAEs consistently outperform or compete with their ESM2 baselines in fitness prediction, indicating that their sparse latent space encodes compact and biologically meaningful representations that generalize more effectively from limited data. Moreover, steering predictive latents exploits biological motifs in pLM representations, yielding top-fitness variants in 83% of cases compared to designing with ESM2 alone.

Abstract: Predicting protein function from amino acid sequence remains a central
challenge in data-scarce (low-$N$) regimes, limiting machine learning-guided
protein design when only small amounts of assay-labeled sequence-function data
are available. Protein language models (pLMs) have advanced the field by
providing evolutionary-informed embeddings and sparse autoencoders (SAEs) have
enabled decomposition of these embeddings into interpretable latent variables
that capture structural and functional features. However, the effectiveness of
SAEs for low-$N$ function prediction and protein design has not been
systematically studied. Herein, we evaluate SAEs trained on fine-tuned ESM2
embeddings across diverse fitness extrapolation and protein engineering tasks.
We show that SAEs, with as few as 24 sequences, consistently outperform or
compete with their ESM2 baselines in fitness prediction, indicating that their
sparse latent space encodes compact and biologically meaningful representations
that generalize more effectively from limited data. Moreover, steering
predictive latents exploits biological motifs in pLM representations, yielding
top-fitness variants in 83% of cases compared to designing with ESM2 alone.

</details>


### [165] [DrugReasoner: Interpretable Drug Approval Prediction with a Reasoning-augmented Language Model](https://arxiv.org/abs/2508.18579)
*Mohammadreza Ghaffarzadeh-Esfahani,Ali Motahharynia,Nahid Yousefian,Navid Mazrouei,Jafar Ghaisari,Yousof Gheisari*

Main category: cs.LG

TL;DR: DrugReasoner是一个基于LLM的药物审批预测模型，它通过推理提高了预测的准确性和可解释性，并在实际场景中表现出稳健性。


<details>
  <summary>Details</summary>
Motivation: 药物发现是一个复杂且资源密集的过程，因此早期预测批准结果对于优化研究投资至关重要。虽然经典的机器学习和深度学习方法在药物批准预测中显示出希望，但其有限的可解释性限制了它们的影响。

Method: 构建在LLaMA架构之上，并使用组相对策略优化 (GRPO) 进行微调的基于推理的大型语言模型 (LLM) DrugReasoner，用于预测小分子批准的可能性。DrugReasoner将分子描述符与针对结构相似的已批准和未批准化合物的比较推理相结合，生成预测以及逐步的理由和置信度。

Result: DrugReasoner在验证集上实现了稳健的性能，AUC 为 0.732，F1 分数为 0.729，在测试集上分别实现了 0.725 和 0.718。在外部独立数据集上，DrugReasoner 优于基线和最近开发的 ChemAP 模型，实现了 0.728 的 AUC 和 0.774 的 F1 分数。

Conclusion: DrugReasoner不仅提供了有竞争力的预测准确性，而且通过其推理输出增强了透明度，从而解决了人工智能辅助药物发现中的一个关键瓶颈。这项研究强调了推理增强的LLM作为药物决策的可解释和有效的工具的潜力。

Abstract: Drug discovery is a complex and resource-intensive process, making early
prediction of approval outcomes critical for optimizing research investments.
While classical machine learning and deep learning methods have shown promise
in drug approval prediction, their limited interpretability constraints their
impact. Here, we present DrugReasoner, a reasoning-based large language model
(LLM) built on the LLaMA architecture and fine-tuned with group relative policy
optimization (GRPO) to predict the likelihood of small-molecule approval.
DrugReasoner integrates molecular descriptors with comparative reasoning
against structurally similar approved and unapproved compounds, generating
predictions alongside step-by-step rationales and confidence scores.
DrugReasoner achieved robust performance with an AUC of 0.732 and an F1 score
of 0.729 on the validation set and 0.725 and 0.718 on the test set,
respectively. These results outperformed conventional baselines, including
logistic regression, support vector machine, and k-nearest neighbors and had
competitive performance relative to XGBoost. On an external independent
dataset, DrugReasoner outperformed both baseline and the recently developed
ChemAP model, achieving an AUC of 0.728 and an F1-score of 0.774, while
maintaining high precision and balanced sensitivity, demonstrating robustness
in real-world scenarios. These findings demonstrate that DrugReasoner not only
delivers competitive predictive accuracy but also enhances transparency through
its reasoning outputs, thereby addressing a key bottleneck in AI-assisted drug
discovery. This study highlights the potential of reasoning-augmented LLMs as
interpretable and effective tools for pharmaceutical decision-making.

</details>


### [166] [History Rhymes: Accelerating LLM Reinforcement Learning with RhymeRL](https://arxiv.org/abs/2508.18588)
*Jingkai He,Tianjian Li,Erhu Feng,Dong Du,Qian Liu,Tao Liu,Yubin Xia,Haibo Chen*

Main category: cs.LG

TL;DR: RhymeRL improves LLM RL training performance by 2.6x by addressing GPU underutilization issues with HistoSpec and HistoPipe, without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: current RL systems continue to grapple with substantial GPU underutilization, due to two primary factors: (1) The rollout stage dominates the overall RL process due to test-time scaling; (2) Imbalances in rollout lengths (within the same batch) result in GPU bubbles.

Method: introduces HistoSpec, a speculative decoding inference engine that utilizes the similarity of historical rollout token sequences to obtain accurate drafts and HistoPipe, a two-tier scheduling strategy that leverages the similarity of historical rollout distributions to balance workload among rollout workers.

Result: RhymeRL achieves a 2.6x performance improvement over existing methods, without compromising accuracy or modifying the RL paradigm.

Conclusion: RhymeRL achieves a 2.6x performance improvement over existing methods, without compromising accuracy or modifying the RL paradigm.

Abstract: With the rapid advancement of large language models (LLMs), reinforcement
learning (RL) has emerged as a pivotal methodology for enhancing the reasoning
capabilities of LLMs. Unlike traditional pre-training approaches, RL
encompasses multiple stages: rollout, reward, and training, which necessitates
collaboration among various worker types. However, current RL systems continue
to grapple with substantial GPU underutilization, due to two primary factors:
(1) The rollout stage dominates the overall RL process due to test-time
scaling; (2) Imbalances in rollout lengths (within the same batch) result in
GPU bubbles. While prior solutions like asynchronous execution and truncation
offer partial relief, they may compromise training accuracy for efficiency.
  Our key insight stems from a previously overlooked observation: rollout
responses exhibit remarkable similarity across adjacent training epochs. Based
on the insight, we introduce RhymeRL, an LLM RL system designed to accelerate
RL training with two key innovations. First, to enhance rollout generation, we
present HistoSpec, a speculative decoding inference engine that utilizes the
similarity of historical rollout token sequences to obtain accurate drafts.
Second, to tackle rollout bubbles, we introduce HistoPipe, a two-tier
scheduling strategy that leverages the similarity of historical rollout
distributions to balance workload among rollout workers. We have evaluated
RhymeRL within a real production environment, demonstrating scalability from
dozens to thousands of GPUs. Experimental results demonstrate that RhymeRL
achieves a 2.6x performance improvement over existing methods, without
compromising accuracy or modifying the RL paradigm.

</details>


### [167] [Linear Trading Position with Sparse Spectrum](https://arxiv.org/abs/2508.18596)
*Zhao-Rong Lai,Haisheng Yang*

Main category: cs.LG

TL;DR: 提出了一种新的线性交易位置，具有稀疏频谱，可以探索预测矩阵的更大频谱区域。


<details>
  <summary>Details</summary>
Motivation: 主要的投资组合方法是基于信号的交易中一种新兴的方法。然而，这些主要的投资组合可能没有多样化，无法探索预测矩阵的关键特征或在不同的情况下保持稳健。

Method: 提出了一种新的具有稀疏频谱的线性交易位置，它可以探索预测矩阵的更大频谱区域。还开发了一种 Krasnosel'ski\{-Mann 不动点算法来优化该交易位置，该算法具有下降特性，并在目标值中实现了线性收敛速度。

Result: 为这类算法提供了一个新的理论结果。大量的实验表明，该方法在各种情况下都取得了良好而稳健的性能。

Conclusion: 该方法在各种情况下都取得了良好而稳健的性能。

Abstract: The principal portfolio approach is an emerging method in signal-based
trading. However, these principal portfolios may not be diversified to explore
the key features of the prediction matrix or robust to different situations. To
address this problem, we propose a novel linear trading position with sparse
spectrum that can explore a larger spectral region of the prediction matrix. We
also develop a Krasnosel'ski\u \i-Mann fixed-point algorithm to optimize this
trading position, which possesses the descent property and achieves a linear
convergence rate in the objective value. This is a new theoretical result for
this type of algorithms. Extensive experiments show that the proposed method
achieves good and robust performance in various situations.

</details>


### [168] [Uncertainty Awareness on Unsupervised Domain Adaptation for Time Series Data](https://arxiv.org/abs/2508.18630)
*Weide Liu,Xiaoyang Zhong,Lu Wang,Jingwen Hou,Yuemei Luo,Jiebin Yan,Yuming Fang*

Main category: cs.LG

TL;DR: 提出了一种结合多尺度特征提取和不确定性估计的方法，以提高模型在域适应任务中的泛化性和鲁棒性，并在时间序列数据上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列数据中训练和测试数据集之间分布偏移的常见挑战，提高模型在未标记测试数据上的泛化能力和鲁棒性。

Method: 结合了多尺度混合输入架构和基于证据学习的不确定性感知机制。

Result: 不确定性感知模型表现出更低的预期校准误差 (ECE)，表明预测置信度得到更好校准。通过跨不同域对齐具有相同标签的特征来增强域适应性，从而显着提高目标域的性能。

Conclusion: 该模型在多个基准数据集上实现了最先进的性能，突显了其在时间序列数据无监督域适应方面的有效性。

Abstract: Unsupervised domain adaptation methods seek to generalize effectively on
unlabeled test data, especially when encountering the common challenge in time
series data that distribution shifts occur between training and testing
datasets. In this paper, we propose incorporating multi-scale feature
extraction and uncertainty estimation to improve the model's generalization and
robustness across domains. Our approach begins with a multi-scale mixed input
architecture that captures features at different scales, increasing training
diversity and reducing feature discrepancies between the training and testing
domains. Based on the mixed input architecture, we further introduce an
uncertainty awareness mechanism based on evidential learning by imposing a
Dirichlet prior on the labels to facilitate both target prediction and
uncertainty estimation. The uncertainty awareness mechanism enhances domain
adaptation by aligning features with the same labels across different domains,
which leads to significant performance improvements in the target domain.
Additionally, our uncertainty-aware model demonstrates a much lower Expected
Calibration Error (ECE), indicating better-calibrated prediction confidence.
Our experimental results show that this combined approach of mixed input
architecture with the uncertainty awareness mechanism achieves state-of-the-art
performance across multiple benchmark datasets, underscoring its effectiveness
in unsupervised domain adaptation for time series data.

</details>


### [169] [STRATA-TS: Selective Knowledge Transfer for Urban Time Series Forecasting with Retrieval-Guided Reasoning](https://arxiv.org/abs/2508.18635)
*Yue Jiang,Chenxi Liu,Yile Chen,Qin Chao,Shuai Liu,Gao Cong*

Main category: cs.LG

TL;DR: STRATA-TS 结合领域自适应检索和大型模型，以提高稀缺数据状态下的预测。


<details>
  <summary>Details</summary>
Motivation: 城市预测模型经常面临严重的数据不平衡问题：只有少数城市拥有密集、长跨度记录，而许多其他城市则暴露于短或不完整的历史。从数据丰富的城市直接转移到数据稀缺的城市是不可靠的，因为只有有限的源模式子集真正有益于目标领域，而滥用转移会带来引入噪声和负面转移的风险。

Method: STRATA-TS 结合了领域自适应检索和具有推理能力的大型模型，以改进稀缺数据状态下的预测。STRATA-TS 采用基于补丁的时间编码器来识别在语义和动态上与目标查询对齐的源子序列。然后将这些检索到的示例注入到检索引导的推理阶段，在该阶段，LLM 对目标输入和检索到的支持执行结构化推理。为了实现高效部署，我们通过监督微调将推理过程提炼成一个紧凑的开放模型。

Result: STRATA-TS 在三个停车可用性数据集上的大量实验表明，STRATA-TS 始终优于强大的预测和迁移基线，同时提供可解释的知识迁移路径。

Conclusion: STRATA-TS在新加坡、诺丁汉和格拉斯哥的三个停车可用性数据集上始终优于强大的预测和迁移基线，同时提供可解释的知识迁移路径。

Abstract: Urban forecasting models often face a severe data imbalance problem: only a
few cities have dense, long-span records, while many others expose short or
incomplete histories. Direct transfer from data-rich to data-scarce cities is
unreliable because only a limited subset of source patterns truly benefits the
target domain, whereas indiscriminate transfer risks introducing noise and
negative transfer. We present STRATA-TS (Selective TRAnsfer via TArget-aware
retrieval for Time Series), a framework that combines domain-adapted retrieval
with reasoning-capable large models to improve forecasting in scarce data
regimes. STRATA-TS employs a patch-based temporal encoder to identify source
subsequences that are semantically and dynamically aligned with the target
query. These retrieved exemplars are then injected into a retrieval-guided
reasoning stage, where an LLM performs structured inference over target inputs
and retrieved support. To enable efficient deployment, we distill the reasoning
process into a compact open model via supervised fine-tuning. Extensive
experiments on three parking availability datasets across Singapore,
Nottingham, and Glasgow demonstrate that STRATA-TS consistently outperforms
strong forecasting and transfer baselines, while providing interpretable
knowledge transfer pathways.

</details>


### [170] [Biologically Disentangled Multi-Omic Modeling Reveals Mechanistic Insights into Pan-Cancer Immunotherapy Resistance](https://arxiv.org/abs/2508.18638)
*Ifrah Tariq,Ernest Fraenkel*

Main category: cs.LG

TL;DR: This paper introduces BDVAE, a deep learning model that predicts treatment response to ICIs and uncovers resistance mechanisms by integrating multi-omics data. BDVAE achieves high accuracy and reveals a continuous spectrum of resistance.


<details>
  <summary>Details</summary>
Motivation: While machine learning models hold promise for predicting responses to ICIs, most existing methods lack interpretability and do not effectively leverage the biological structure inherent to multi-omics data.

Method: We introduce the Biologically Disentangled Variational Autoencoder (BDVAE), a deep generative model that integrates transcriptomic and genomic data through modality- and pathway-specific encoders.

Result: Applied to a pan-cancer cohort of 366 patients across four cancer types treated with ICIs, BDVAE accurately predicts treatment response (AUC-ROC = 0.94 on unseen test data) and uncovers critical resistance mechanisms, including immune suppression, metabolic shifts, and neuronal signaling.

Conclusion: BDVAE reveals that resistance spans a continuous biological spectrum rather than strictly binary states, reflecting gradations of tumor dysfunction. Several latent features correlate with survival outcomes and known clinical subtypes, demonstrating BDVAE's capability to generate interpretable, clinically relevant insights. These findings underscore the value of biologically structured machine learning in elucidating complex resistance patterns and guiding precision immunotherapy strategies.

Abstract: Immune checkpoint inhibitors (ICIs) have transformed cancer treatment, yet
patient responses remain highly variable, and the biological mechanisms
underlying resistance are poorly understood. While machine learning models hold
promise for predicting responses to ICIs, most existing methods lack
interpretability and do not effectively leverage the biological structure
inherent to multi-omics data. Here, we introduce the Biologically Disentangled
Variational Autoencoder (BDVAE), a deep generative model that integrates
transcriptomic and genomic data through modality- and pathway-specific
encoders. Unlike existing rigid, pathway-informed models, BDVAE employs a
modular encoder architecture combined with variational inference to learn
biologically meaningful latent features associated with immune, genomic, and
metabolic processes. Applied to a pan-cancer cohort of 366 patients across four
cancer types treated with ICIs, BDVAE accurately predicts treatment response
(AUC-ROC = 0.94 on unseen test data) and uncovers critical resistance
mechanisms, including immune suppression, metabolic shifts, and neuronal
signaling. Importantly, BDVAE reveals that resistance spans a continuous
biological spectrum rather than strictly binary states, reflecting gradations
of tumor dysfunction. Several latent features correlate with survival outcomes
and known clinical subtypes, demonstrating BDVAE's capability to generate
interpretable, clinically relevant insights. These findings underscore the
value of biologically structured machine learning in elucidating complex
resistance patterns and guiding precision immunotherapy strategies.

</details>


### [171] [The Sound of Risk: A Multimodal Physics-Informed Acoustic Model for Forecasting Market Volatility and Enhancing Market Interpretability](https://arxiv.org/abs/2508.18653)
*Xiaoliang Chen,Xin Yu,Le Chang,Teng Jing,Jiashuai He,Ze Wang,Yangjun Luo,Xingyu Chen,Jiayue Liang,Yuchen Wang,Jiaying Xie*

Main category: cs.LG

TL;DR: 该研究提出了一种多模态框架，通过分析财报电话会议中高管的情绪变化来预测市场波动率，发现高管从脚本化演讲到自发问答环节的情绪动态对波动率预测有重要影响。


<details>
  <summary>Details</summary>
Motivation: 传统文本分析在金融市场中由于信息不对称而受到影响，而策略性地设计的公司叙述会加剧这种情况。

Method: 该研究提出了一种新颖的多模态框架，用于金融风险评估，该框架集成了文本情感和从财报电话会议中提取的高管声道动态的超语言线索。核心是物理信息声学模型（PIAM），它应用非线性声学从原始电话会议声音中稳健地提取情感特征。

Result: 多模态特征不能预测定向股票收益，但可以解释高达43.8%的30天已实现波动率的样本外方差。波动率预测主要受到高管从脚本化演讲到自发问答环节的情绪动态的影响。多模态方法明显优于仅使用财务数据的基线方法。

Conclusion: 该研究表明，多模态特征可以解释高达43.8%的30天已实现波动率的样本外方差，并且波动率预测主要受到高管从脚本化演讲到自发问答环节的情绪动态的影响，尤其体现在CFO文本稳定性的降低和声学不稳定性的增强，以及CEO显著的唤醒变异性。

Abstract: Information asymmetry in financial markets, often amplified by strategically
crafted corporate narratives, undermines the effectiveness of conventional
textual analysis. We propose a novel multimodal framework for financial risk
assessment that integrates textual sentiment with paralinguistic cues derived
from executive vocal tract dynamics in earnings calls. Central to this
framework is the Physics-Informed Acoustic Model (PIAM), which applies
nonlinear acoustics to robustly extract emotional signatures from raw
teleconference sound subject to distortions such as signal clipping. Both
acoustic and textual emotional states are projected onto an interpretable
three-dimensional Affective State Label (ASL) space-Tension, Stability, and
Arousal. Using a dataset of 1,795 earnings calls (approximately 1,800 hours),
we construct features capturing dynamic shifts in executive affect between
scripted presentation and spontaneous Q&A exchanges. Our key finding reveals a
pronounced divergence in predictive capacity: while multimodal features do not
forecast directional stock returns, they explain up to 43.8% of the
out-of-sample variance in 30-day realized volatility. Importantly, volatility
predictions are strongly driven by emotional dynamics during executive
transitions from scripted to spontaneous speech, particularly reduced textual
stability and heightened acoustic instability from CFOs, and significant
arousal variability from CEOs. An ablation study confirms that our multimodal
approach substantially outperforms a financials-only baseline, underscoring the
complementary contributions of acoustic and textual modalities. By decoding
latent markers of uncertainty from verifiable biometric signals, our
methodology provides investors and regulators a powerful tool for enhancing
market interpretability and identifying hidden corporate uncertainty.

</details>


### [172] [FFT-MoE: Efficient Federated Fine-Tuning for Foundation Models via Large-scale Sparse MoE under Heterogeneous Edge](https://arxiv.org/abs/2508.18663)
*Gang Hu,Yinglei Teng,Pengfei Wu,Nan Wang*

Main category: cs.LG

TL;DR: 提出了FFT MoE，用稀疏的混合专家（MoE）适配器替换LoRA，并引入了异构感知辅助损失，该损失动态地调节路由分布，以确保专家多样性和平衡利用。


<details>
  <summary>Details</summary>
Motivation: 在隐私和资源约束下微调FM变得越来越重要，特别是在高质量训练数据位于分布式边缘设备上时。异构FL环境中的LoRA-based FFT面临两个主要限制：具有不同LoRA配置的客户端之间的结构不兼容以及对非IID数据分布的适应性有限，这阻碍了收敛和泛化。

Method: 用稀疏的混合专家（MoE）适配器替换LoRA。

Result: FFT MoE在泛化性能和训练效率方面始终优于最先进的FFT基线。

Conclusion: FFT MoE在泛化性能和训练效率方面始终优于最先进的FFT基线。

Abstract: As FMs drive progress toward Artificial General Intelligence (AGI),
fine-tuning them under privacy and resource constraints has become increasingly
critical particularly when highquality training data resides on distributed
edge devices. Federated Learning (FL) offers a compelling solution through
Federated Fine-Tuning (FFT), which enables collaborative model adaptation
without sharing raw data. Recent approaches incorporate Parameter-Efficient
Fine-Tuning (PEFT) techniques such as Low Rank Adaptation (LoRA) to reduce
computational overhead. However, LoRA-based FFT faces two major limitations in
heterogeneous FL environments: structural incompatibility across clients with
varying LoRA configurations and limited adaptability to non-IID data
distributions, which hinders convergence and generalization. To address these
challenges, we propose FFT MoE, a novel FFT framework that replaces LoRA with
sparse Mixture of Experts (MoE) adapters. Each client trains a lightweight
gating network to selectively activate a personalized subset of experts,
enabling fine-grained adaptation to local resource budgets while preserving
aggregation compatibility. To further combat the expert load imbalance caused
by device and data heterogeneity, we introduce a heterogeneity-aware auxiliary
loss that dynamically regularizes the routing distribution to ensure expert
diversity and balanced utilization. Extensive experiments spanning both IID and
non-IID conditions demonstrate that FFT MoE consistently outperforms state of
the art FFT baselines in generalization performance and training efficiency.

</details>


### [173] [Auditing Approximate Machine Unlearning for Differentially Private Models](https://arxiv.org/abs/2508.18671)
*Yuechun Gu,Jiajie He,Keke Chen*

Main category: cs.LG

TL;DR: 本文全面地审计了应用近似卸载算法后，未学习和保留样本的隐私风险。


<details>
  <summary>Details</summary>
Motivation: 近似机器卸载旨在消除特定数据对已训练模型的影响，以确保个人隐私。现有的方法侧重于移除的记录，并假设保留的记录不受影响。然而，最近关于隐私洋葱效应的研究表明，这种假设可能是不正确的。特别是当模型是差分隐私时，没有研究探讨在现有的机器卸载方法下，保留的记录是否仍然满足差分隐私 (DP) 标准。

Method: 我们提出了分别针对未学习和保留样本的隐私标准，基于 DP 和成员推理攻击 (MIA) 的角度。为了使审计过程更实用，我们还开发了一种高效的 MIA，即 A-LiRA，利用数据增强来降低影子模型训练的成本。

Result: 实验结果表明，现有的近似机器卸载算法可能会无意中损害差分隐私模型中保留样本的隐私。

Conclusion: 现有的近似机器卸载算法可能会无意中损害差分隐私模型中保留样本的隐私，我们需要差分隐私卸载算法。

Abstract: Approximate machine unlearning aims to remove the effect of specific data
from trained models to ensure individuals' privacy. Existing methods focus on
the removed records and assume the retained ones are unaffected. However,
recent studies on the \emph{privacy onion effect} indicate this assumption
might be incorrect. Especially when the model is differentially private, no
study has explored whether the retained ones still meet the differential
privacy (DP) criterion under existing machine unlearning methods. This paper
takes a holistic approach to auditing both unlearned and retained samples'
privacy risks after applying approximate unlearning algorithms. We propose the
privacy criteria for unlearned and retained samples, respectively, based on the
perspectives of DP and membership inference attacks (MIAs). To make the
auditing process more practical, we also develop an efficient MIA, A-LiRA,
utilizing data augmentation to reduce the cost of shadow model training. Our
experimental findings indicate that existing approximate machine unlearning
algorithms may inadvertently compromise the privacy of retained samples for
differentially private models, and we need differentially private unlearning
algorithms. For reproducibility, we have pubished our code:
https://anonymous.4open.science/r/Auditing-machine-unlearning-CB10/README.md

</details>


### [174] [Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks](https://arxiv.org/abs/2508.18672)
*Taishi Nakamura,Satoki Ishikawa,Masaki Kawamura,Takumi Okamoto,Daisuke Nohara,Jun Suzuki,Rio Yokota*

Main category: cs.LG

TL;DR: This paper investigates the influence of MoE sparsity on memorization and reasoning capabilities of large language models. The findings suggest that while memorization improves with total parameters, reasoning performance saturates and can even regress with increased sparsity, and cannot be rescued by post-training methods or extra test-time compute.


<details>
  <summary>Details</summary>
Motivation: Empirical scaling laws have driven the evolution of large language models (LLMs), yet their coefficients shift whenever the model architecture or data pipeline changes. Mixture-of-Experts (MoE) models, now standard in state-of-the-art systems, introduce a new sparsity dimension that current dense-model frontiers overlook. We investigate how MoE sparsity influences two distinct capability regimes: memorization and reasoning.

Method: We train families of MoE Transformers that systematically vary total parameters, active parameters, and top-$k$ routing while holding the compute budget fixed. For every model we record pre-training loss, downstream task loss, and task accuracy, allowing us to separate the train-test generalization gap from the loss-accuracy gap.

Result: Memorization benchmarks improve monotonically with total parameters, mirroring training loss.

Conclusion: Reasoning performance saturates and can even regress despite continued gains in both total parameters and training loss. Altering top-$k$ alone has little effect when active parameters are constant, and classic hyperparameters such as learning rate and initialization modulate the generalization gap in the same direction as sparsity. Neither post-training reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning deficit of overly sparse models.

Abstract: Empirical scaling laws have driven the evolution of large language models
(LLMs), yet their coefficients shift whenever the model architecture or data
pipeline changes. Mixture-of-Experts (MoE) models, now standard in
state-of-the-art systems, introduce a new sparsity dimension that current
dense-model frontiers overlook. We investigate how MoE sparsity influences two
distinct capability regimes: memorization and reasoning. We train families of
MoE Transformers that systematically vary total parameters, active parameters,
and top-$k$ routing while holding the compute budget fixed. For every model we
record pre-training loss, downstream task loss, and task accuracy, allowing us
to separate the train-test generalization gap from the loss-accuracy gap.
Memorization benchmarks improve monotonically with total parameters, mirroring
training loss. By contrast, reasoning performance saturates and can even
regress despite continued gains in both total parameters and training loss.
Altering top-$k$ alone has little effect when active parameters are constant,
and classic hyperparameters such as learning rate and initialization modulate
the generalization gap in the same direction as sparsity. Neither post-training
reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning
deficit of overly sparse models. Our model checkpoints, code and logs are
open-source at https://github.com/rioyokotalab/optimal-sparsity.

</details>


### [175] [Utilizing Training Data to Improve LLM Reasoning for Tabular Understanding](https://arxiv.org/abs/2508.18676)
*Chufan Gao,Jintai Chen,Jimeng Sun*

Main category: cs.LG

TL;DR: 提出了一种新的提示方法LRTab，它通过检索相关信息来提高表格推理的性能，并且在WikiTQ和Tabfact数据集上优于之前的基线。


<details>
  <summary>Details</summary>
Motivation: 自动表格理解和推理对于数据科学家来说是重要的任务。以前的工作主要集中在使用标记数据微调LLM或使用思维链(CoT)进行无训练提示LLM代理。微调提供了数据集特定的学习，但以泛化性为代价。无训练提示具有高度的泛化性，但没有充分利用训练数据。

Method: 提出了一种新的基于提示的推理方法，Learn then Retrieve: LRTab，它通过检索从训练数据中学习到的相关信息，整合了两者的优点。

Result: 在WikiTQ和Tabfact上进行了全面的实验，表明LRTab是可解释的，具有成本效益的，并且在表格推理方面优于之前的基线。

Conclusion: LRTab在表格推理方面优于之前的基线模型，并且具有可解释性和成本效益。

Abstract: Automated tabular understanding and reasoning are essential tasks for data
scientists. Recently, Large language models (LLMs) have become increasingly
prevalent in tabular reasoning tasks. Previous work focuses on (1) finetuning
LLMs using labeled data or (2) Training-free prompting LLM agents using
chain-of-thought (CoT). Finetuning offers dataset-specific learning at the cost
of generalizability. Training-free prompting is highly generalizable but does
not take full advantage of training data. In this paper, we propose a novel
prompting-based reasoning approach, Learn then Retrieve: LRTab, which
integrates the benefits of both by retrieving relevant information learned from
training data. We first use prompting to obtain CoT responses over the training
data. For incorrect CoTs, we prompt the LLM to predict Prompt Conditions to
avoid the error, learning insights from the data. We validate the effectiveness
of Prompt Conditions using validation data. Finally, at inference time, we
retrieve the most relevant Prompt Conditions for additional context for table
understanding. We provide comprehensive experiments on WikiTQ and Tabfact,
showing that LRTab is interpretable, cost-efficient, and can outperform
previous baselines in tabular reasoning.

</details>


### [176] [End to End Autoencoder MLP Framework for Sepsis Prediction](https://arxiv.org/abs/2508.18688)
*Hejiang Cai,Di Wu,Ji Xu,Xiang Liu,Yiziting Zhu,Xin Shu,Yujie Li,Bin Yi*

Main category: cs.LG

TL;DR: 本研究提出了一种用于早期脓毒症检测的端到端深度学习框架，该框架在三个ICU队列中优于传统的机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 脓毒症是一种危及生命的疾病，需要在重症监护环境中及时检测。传统的机器学习方法通常依赖于手动特征工程，并且难以处理电子健康记录中常见的irregular, incomplete time-series data。

Method: 引入了一个端到端的深度学习框架，该框架集成了用于自动特征提取的无监督自动编码器和用于二元脓毒症风险预测的多层感知器分类器。为了增强临床适用性，我们实施了一种定制的下采样策略，该策略在训练期间提取高信息密度片段，以及一种用于实时推理的非重叠动态滑动窗口机制。

Result: 我们的端到端模型实现了74.6%、80.6%和93.5%的准确率，始终优于传统的机器学习基线。

Conclusion: 该研究证明了该框架在异构ICU环境中早期脓毒症检测方面具有卓越的鲁棒性、通用性和临床实用性。

Abstract: Sepsis is a life threatening condition that requires timely detection in
intensive care settings. Traditional machine learning approaches, including
Naive Bayes, Support Vector Machine (SVM), Random Forest, and XGBoost, often
rely on manual feature engineering and struggle with irregular, incomplete
time-series data commonly present in electronic health records. We introduce an
end-to-end deep learning framework integrating an unsupervised autoencoder for
automatic feature extraction with a multilayer perceptron classifier for binary
sepsis risk prediction. To enhance clinical applicability, we implement a
customized down sampling strategy that extracts high information density
segments during training and a non-overlapping dynamic sliding window mechanism
for real-time inference. Preprocessed time series data are represented as fixed
dimension vectors with explicit missingness indicators, mitigating bias and
noise. We validate our approach on three ICU cohorts. Our end-to-end model
achieves accuracies of 74.6 percent, 80.6 percent, and 93.5 percent,
respectively, consistently outperforming traditional machine learning
baselines. These results demonstrate the framework's superior robustness,
generalizability, and clinical utility for early sepsis detection across
heterogeneous ICU environments.

</details>


### [177] [Natural Image Classification via Quasi-Cyclic Graph Ensembles and Random-Bond Ising Models at the Nishimori Temperature](https://arxiv.org/abs/2508.18717)
*V. S. Usatyuk,D. A. Sapoznikov,S. I. Egorov*

Main category: cs.LG

TL;DR: Combines statistical physics, coding theory, and algebraic topology for efficient image classification, achieving high accuracy with massive compression using topology-guided graph design.


<details>
  <summary>Details</summary>
Motivation: Efficient multi-class image classification.

Method: A unified framework combining statistical physics, coding theory, and algebraic topology. High-dimensional feature vectors are interpreted as spins on a sparse Multi-Edge Type quasi-cyclic LDPC graph, forming a Random-Bond Ising Model (RBIM).

Result: Achieves 98.7% accuracy on ImageNet-10 and 82.7% on ImageNet-100 with 40x fewer parameters.

Conclusion: Topology-guided graph design yields highly efficient, physics-inspired embeddings with state-of-the-art performance, achieving 98.7% accuracy on ImageNet-10 and 82.7% on ImageNet-100 despite massive compression.

Abstract: We present a unified framework combining statistical physics, coding theory,
and algebraic topology for efficient multi-class image classification.
High-dimensional feature vectors from a frozen MobileNetV2 backbone are
interpreted as spins on a sparse Multi-Edge Type quasi-cyclic LDPC
(MET-QC-LDPC) graph, forming a Random-Bond Ising Model (RBIM). We operate this
RBIM at its Nishimori temperature, $\beta_N$, where the smallest eigenvalue of
the Bethe-Hessian matrix vanishes, maximizing class separability.
  Our theoretical contribution establishes a correspondence between local
trapping sets in the code's graph and topological invariants (Betti numbers,
bordism classes) of the feature manifold. A practical algorithm estimates
$\beta_N$ efficiently with a quadratic interpolant and Newton correction,
achieving a six-fold speed-up over bisection.
  Guided by topology, we design spherical and toroidal MET-QC-LDPC graph
ensembles, using permanent bounds to suppress harmful trapping sets. This
compresses 1280-dimensional features to 32 or 64 dimensions for ImageNet-10 and
-100 subsets. Despite massive compression (40x fewer parameters), we achieve
98.7% accuracy on ImageNet-10 and 82.7% on ImageNet-100, demonstrating that
topology-guided graph design yields highly efficient, physics-inspired
embeddings with state-of-the-art performance.

</details>


### [178] [Beyond Tokens: Enhancing RTL Quality Estimation via Structural Graph Learning](https://arxiv.org/abs/2508.18730)
*Yi Liu,Hongji Zhang,Yiwen Wang,Dimitris Tsaras,Lei Chen,Mingxuan Yuan,Qiang Xu*

Main category: cs.LG

TL;DR: This paper presents StructRTL, a new framework for RTL design quality estimation that uses structure-aware graph self-supervised learning on CDFGs and knowledge distillation to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Estimating the quality of register transfer level (RTL) designs is crucial in the electronic design automation (EDA) workflow, as it enables instant feedback on key metrics like area and delay without the need for time-consuming logic synthesis. Recent approaches have leveraged large language models (LLMs) to derive embeddings from RTL code but overlook the structural semantics essential for accurate quality estimation. The control data flow graph (CDFG) view exposes the design's structural characteristics more explicitly, offering richer cues for representation learning.

Method: The method learns structure-informed representations from CDFGs and incorporates a knowledge distillation strategy that transfers low-level insights from post-mapping netlists into the CDFG predictor.

Result: The method significantly outperforms prior art on various quality estimation tasks and establishes new state-of-the-art results.

Conclusion: This paper introduces StructRTL, a structure-aware graph self-supervised learning framework, for improved RTL design quality estimation. Experiments show that StructRTL establishes new state-of-the-art results, demonstrating the effectiveness of combining structural learning with cross-stage supervision.

Abstract: Estimating the quality of register transfer level (RTL) designs is crucial in
the electronic design automation (EDA) workflow, as it enables instant feedback
on key metrics like area and delay without the need for time-consuming logic
synthesis. While recent approaches have leveraged large language models (LLMs)
to derive embeddings from RTL code and achieved promising results, they
overlook the structural semantics essential for accurate quality estimation. In
contrast, the control data flow graph (CDFG) view exposes the design's
structural characteristics more explicitly, offering richer cues for
representation learning. In this work, we introduce a novel structure-aware
graph self-supervised learning framework, StructRTL, for improved RTL design
quality estimation. By learning structure-informed representations from CDFGs,
our method significantly outperforms prior art on various quality estimation
tasks. To further boost performance, we incorporate a knowledge distillation
strategy that transfers low-level insights from post-mapping netlists into the
CDFG predictor. Experiments show that our approach establishes new
state-of-the-art results, demonstrating the effectiveness of combining
structural learning with cross-stage supervision.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [179] [Federative ischemic stroke segmentation as alternative to overcome domain-shift multi-institution challenges](https://arxiv.org/abs/2508.18296)
*Edgar Rangel,Fabio Martinez*

Main category: eess.IV

TL;DR: Developed a collaborative framework for segmenting ischemic stroke lesions in DWI sequences by sharing knowledge from deep center-independent representations. The FedAvg model achieved a general DSC of $0.71.


<details>
  <summary>Details</summary>
Motivation: Clinical guidelines establish diffusion resonance imaging (DWI, ADC) as the standard for localizing, characterizing, and measuring infarct volume, enabling treatment support and prognosis. Nonetheless, such lesion analysis is highly variable due to different patient demographics, scanner vendors, and expert annotations. Computational support approaches have been key to helping with the localization and segmentation of lesions. However, these strategies are dedicated solutions that learn patterns from only one institution, lacking the variability to generalize geometrical lesions shape models. Even worse, many clinical centers lack sufficient labeled samples to adjust these dedicated solutions.

Method: a collaborative framework for segmenting ischemic stroke lesions in DWI sequences by sharing knowledge from deep center-independent representations. From 14 emulated healthcare centers with 2031 studies, the FedAvg model

Result: the FedAvg model achieved a general DSC of $0.71 ", AVD of $5.29, ALD of $2.16 and LF1 of $0.70 over all centers, outperforming both the centralized and other federated rules. Interestingly, the model demonstrated strong generalization properties, showing uniform performance across different lesion categories and reliable performance in out-of-distribution centers (with DSC of $0.64 and AVD of $4.44 without any additional training).

Conclusion: The FedAvg model achieved a general DSC of $0.71 ", AVD of $5.29, ALD of $2.16 and LF1 of $0.70 over all centers, outperforming both the centralized and other federated rules. The model demonstrated strong generalization properties, showing uniform performance across different lesion categories and reliable performance in out-of-distribution centers.

Abstract: Stroke is the second leading cause of death and the third leading cause of
disability worldwide. Clinical guidelines establish diffusion resonance imaging
(DWI, ADC) as the standard for localizing, characterizing, and measuring
infarct volume, enabling treatment support and prognosis. Nonetheless, such
lesion analysis is highly variable due to different patient demographics,
scanner vendors, and expert annotations. Computational support approaches have
been key to helping with the localization and segmentation of lesions. However,
these strategies are dedicated solutions that learn patterns from only one
institution, lacking the variability to generalize geometrical lesions shape
models. Even worse, many clinical centers lack sufficient labeled samples to
adjust these dedicated solutions. This work developed a collaborative framework
for segmenting ischemic stroke lesions in DWI sequences by sharing knowledge
from deep center-independent representations. From 14 emulated healthcare
centers with 2031 studies, the FedAvg model achieved a general DSC of $0.71 \pm
0.24$, AVD of $5.29 \pm 22.74$, ALD of $2.16 \pm 3.60$ and LF1 of $0.70 \pm
0.26$ over all centers, outperforming both the centralized and other federated
rules. Interestingly, the model demonstrated strong generalization properties,
showing uniform performance across different lesion categories and reliable
performance in out-of-distribution centers (with DSC of $0.64 \pm 0.29$ and AVD
of $4.44 \pm 8.74$ without any additional training).

</details>


### [180] [Analise de Desaprendizado de Maquina em Modelos de Classificacao de Imagens Medicas](https://arxiv.org/abs/2508.18509)
*Andreza M. C. Falcao,Filipe R. Cordeiro*

Main category: eess.IV

TL;DR: This paper explores machine unlearning in medical image classification using the SalUn model on three datasets, finding it to be an efficient solution.


<details>
  <summary>Details</summary>
Motivation: Machine unlearning aims to remove private or sensitive data from a pre-trained model while preserving the model's robustness. Despite recent advances, this technique has not been explored in medical image classification.

Method: We evaluate the SalUn unlearning model by conducting experiments on the PathMNIST, OrganAMNIST, and BloodMNIST datasets. We also analyse the impact of data augmentation on the quality of unlearning.

Result: SalUn achieves performance close to full retraining.

Conclusion: SalUn achieves performance close to full retraining, indicating an efficient solution for use in medical applications.

Abstract: Machine unlearning aims to remove private or sensitive data from a
pre-trained model while preserving the model's robustness. Despite recent
advances, this technique has not been explored in medical image classification.
This work evaluates the SalUn unlearning model by conducting experiments on the
PathMNIST, OrganAMNIST, and BloodMNIST datasets. We also analyse the impact of
data augmentation on the quality of unlearning. Results show that SalUn
achieves performance close to full retraining, indicating an efficient solution
for use in medical applications.

</details>


### [181] [A Deep Learning Application for Psoriasis Detection](https://arxiv.org/abs/2508.18528)
*Anna Milani,Fábio S. da Silva,Elloá B. Guedes,Ricardo Rios*

Main category: eess.IV

TL;DR: Inception v3 is a valuable tool for supporting the diagnosis of psoriasis


<details>
  <summary>Details</summary>
Motivation: classification of skin images with lesions affected by psoriasis

Method: comparative study of the performance of three Convolutional Neural Network models, ResNet50, Inception v3 and VGG19

Result: the model Inception v3 has satisfactory performance with respect to accuracy and F1-Score (97.5% $\pm$ 0.2)

Conclusion: Inception v3 is a valuable tool for supporting the diagnosis of psoriasis due to its satisfactory performance with respect to accuracy and F1-Score (97.5% ${\pm}$ 0.2).

Abstract: In this paper a comparative study of the performance of three Convolutional
Neural Network models, ResNet50, Inception v3 and VGG19 for classification of
skin images with lesions affected by psoriasis is presented. The images used
for training and validation of the models were obtained from specialized
platforms. Some techniques were used to adjust the evaluation metrics of the
neural networks. The results found suggest the model Inception v3 as a valuable
tool for supporting the diagnosis of psoriasis. This is due to its satisfactory
performance with respect to accuracy and F1-Score (97.5% ${\pm}$ 0.2).

</details>


### [182] [A Closer Look at Edema Area Segmentation in SD-OCT Images Using Adversarial Framework](https://arxiv.org/abs/2508.18790)
*Yuhui Tao,Yizhe Zhang,Qiang Chen*

Main category: eess.IV

TL;DR: 本文提出了一种新的弱监督黄斑水肿分割方法，该方法利用视网膜层信息和测试时自适应策略来提高分割准确性和鲁棒性，缩小了与全监督方法的差距。


<details>
  <summary>Details</summary>
Motivation: 人工智模型对黄斑水肿 (ME) 的分析依赖于专家注释的像素级图像数据集，这些数据集的前瞻性收集成本很高。基于异常检测的弱监督方法在水肿区域 (EA) 分割任务中显示出希望，但它们的性能仍然落后于完全监督的方法。

Method: 利用层结构引导的后处理步骤和测试时自适应 (TTA) 策略，增强了现成的对抗框架，用于 EA 分割。

Result: 在两个公开可用的数据集上进行的大量实验表明，这两个提出的成分可以提高 EA 分割的准确性和鲁棒性。

Conclusion: 该论文提出的方法可以提高黄斑水肿分割的准确性和鲁棒性，缩小弱监督模型和全监督模型之间的差距。

Abstract: The development of artificial intelligence models for macular edema (ME)
analy-sis always relies on expert-annotated pixel-level image datasets which
are expen-sive to collect prospectively. While anomaly-detection-based
weakly-supervised methods have shown promise in edema area (EA) segmentation
task, their per-formance still lags behind fully-supervised approaches. In this
paper, we leverage the strong correlation between EA and retinal layers in
spectral-domain optical coherence tomography (SD-OCT) images, along with the
update characteristics of weakly-supervised learning, to enhance an
off-the-shelf adversarial framework for EA segmentation with a novel
layer-structure-guided post-processing step and a test-time-adaptation (TTA)
strategy. By incorporating additional retinal lay-er information, our framework
reframes the dense EA prediction task as one of confirming intersection points
between the EA contour and retinal layers, result-ing in predictions that
better align with the shape prior of EA. Besides, the TTA framework further
helps address discrepancies in the manifestations and presen-tations of EA
between training and test sets. Extensive experiments on two pub-licly
available datasets demonstrate that these two proposed ingredients can im-prove
the accuracy and robustness of EA segmentation, bridging the gap between
weakly-supervised and fully-supervised models.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [183] [EMind: A Foundation Model for Multi-task Electromagnetic Signals Understanding](https://arxiv.org/abs/2508.18785)
*Luqing Luo,Wenjin Gui,Yunfei Liu,Ziyue Zhang,Yunxi Zhang,Fengxiang Wang,Zonghao Guo,Zizhi Ma,Xinzhu Liu,Hanxiang He,Jinhai Li,Xin Qiu,Wupeng Xie,Yangang Sun*

Main category: eess.SP

TL;DR: EMind, an electromagnetic signals foundation model that bridges large scale pretraining and the unique nature of this modality. We build the first unified and largest standardized electromagnetic signal dataset covering multiple signal types and tasks.


<details>
  <summary>Details</summary>
Motivation: Electromagnetic signals differ greatly from text and images, showing high heterogeneity, strong background noise and complex joint time frequency structure, which prevents existing general models from direct use. Electromagnetic communication and sensing tasks are diverse, current methods lack cross task generalization and transfer efficiency, and the scarcity of large high quality datasets blocks the creation of a truly general multitask learning framework.

Method: a length adaptive multi-signal packing method and a hardware-aware training strategy

Result: achieves strong performance and broad generalization across many downstream tasks

Conclusion: EMind achieves strong performance and broad generalization across many downstream tasks, moving decisively from task specific models to a unified framework for electromagnetic intelligence.

Abstract: Deep understanding of electromagnetic signals is fundamental to dynamic
spectrum management, intelligent transportation, autonomous driving and
unmanned vehicle perception. The field faces challenges because electromagnetic
signals differ greatly from text and images, showing high heterogeneity, strong
background noise and complex joint time frequency structure, which prevents
existing general models from direct use. Electromagnetic communication and
sensing tasks are diverse, current methods lack cross task generalization and
transfer efficiency, and the scarcity of large high quality datasets blocks the
creation of a truly general multitask learning framework. To overcome these
issue, we introduce EMind, an electromagnetic signals foundation model that
bridges large scale pretraining and the unique nature of this modality. We
build the first unified and largest standardized electromagnetic signal dataset
covering multiple signal types and tasks. By exploiting the physical properties
of electromagnetic signals, we devise a length adaptive multi-signal packing
method and a hardware-aware training strategy that enable efficient use and
representation learning from heterogeneous multi-source signals. Experiments
show that EMind achieves strong performance and broad generalization across
many downstream tasks, moving decisively from task specific models to a unified
framework for electromagnetic intelligence. The code is available at:
https://github.com/GabrielleTse/EMind.

</details>
