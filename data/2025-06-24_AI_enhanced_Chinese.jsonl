{"id": "2506.17226", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2506.17226", "abs": "https://arxiv.org/abs/2506.17226", "authors": ["Ashish Manchanda", "Prem Prakash Jayaraman", "Abhik Banerjee", "Kaneez Fizza", "Arkady Zaslavsky"], "title": "DCMF: A Dynamic Context Monitoring and Caching Framework for Context Management Platforms", "comment": null, "summary": "The rise of context-aware IoT applications has increased the demand for\ntimely and accurate context information. Context is derived by aggregating and\ninferring from dynamic IoT data, making it highly volatile and posing\nchallenges in maintaining freshness and real-time accessibility. Caching is a\npotential solution, but traditional policies struggle with the transient nature\nof context in IoT (e.g., ensuring real-time access for frequent queries or\nhandling fast-changing data). To address this, we propose the Dynamic Context\nMonitoring Framework (DCMF) to enhance context caching in Context Management\nPlatforms (CMPs) by dynamically evaluating and managing context. DCMF comprises\ntwo core components: the Context Evaluation Engine (CEE) and the Context\nManagement Module (CMM). The CEE calculates the Probability of Access (PoA)\nusing parameters such as Quality of Service (QoS), Quality of Context (QoC),\nCost of Context (CoC), timeliness, and Service Level Agreements (SLAs),\nassigning weights to assess access likelihood. Based on this, the CMM applies a\nhybrid Dempster-Shafer approach to manage Context Freshness (CF), updating\nbelief levels and confidence scores to determine whether to cache, evict, or\nrefresh context items. We implemented DCMF in a Context-as-a-Service (CoaaS)\nplatform and evaluated it using real-world smart city data, particularly\ntraffic and roadwork scenarios. Results show DCMF achieves a 12.5% higher cache\nhit rate and reduces cache expiry by up to 60% compared to the m-CAC technique,\nensuring timely delivery of relevant context and reduced latency. These results\ndemonstrate DCMF's scalability and suitability for dynamic context-aware IoT\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86DCMF\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u7269\u8054\u7f51\u73af\u5883\u4e2d\u7684\u4e0a\u4e0b\u6587\u7f13\u5b58\uff0c\u901a\u8fc7\u52a8\u6001\u8bc4\u4f30\u4e0a\u4e0b\u6587\u5e76\u4f7f\u7528\u6df7\u5408Dempster-Shafer\u65b9\u6cd5\u7ba1\u7406\u4e0a\u4e0b\u6587\u65b0\u9c9c\u5ea6\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u7f13\u5b58\u547d\u4e2d\u7387\u5e76\u51cf\u5c11\u4e86\u7f13\u5b58\u8fc7\u671f\u3002", "motivation": "\u4e0a\u4e0b\u6587\u611f\u77e5\u7269\u8054\u7f51\u5e94\u7528\u7684\u5174\u8d77\u589e\u52a0\u4e86\u5bf9\u53ca\u65f6\u548c\u51c6\u786e\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u9700\u6c42\u3002\u4e0a\u4e0b\u6587\u662f\u901a\u8fc7\u805a\u5408\u548c\u63a8\u65ad\u52a8\u6001\u7269\u8054\u7f51\u6570\u636e\u5f97\u51fa\u7684\uff0c\u8fd9\u4f7f\u5f97\u5b83\u5177\u6709\u9ad8\u5ea6\u7684\u6613\u5931\u6027\uff0c\u5e76\u5728\u4fdd\u6301\u65b0\u9c9c\u5ea6\u548c\u5b9e\u65f6\u53ef\u8bbf\u95ee\u6027\u65b9\u9762\u63d0\u51fa\u4e86\u6311\u6218\u3002\u7f13\u5b58\u662f\u4e00\u79cd\u6f5c\u5728\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4f20\u7edf\u7684\u7b56\u7565\u5728\u7269\u8054\u7f51\u4e2d\u96be\u4ee5\u5e94\u5bf9\u4e0a\u4e0b\u6587\u7684\u77ac\u65f6\u6027\uff08\u4f8b\u5982\uff0c\u786e\u4fdd\u9891\u7e41\u67e5\u8be2\u7684\u5b9e\u65f6\u8bbf\u95ee\u6216\u5904\u7406\u5feb\u901f\u53d8\u5316\u7684\u6570\u636e\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u52a8\u6001\u4e0a\u4e0b\u6587\u76d1\u63a7\u6846\u67b6\uff08DCMF\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8bc4\u4f30\u548c\u7ba1\u7406\u4e0a\u4e0b\u6587\u6765\u589e\u5f3a\u4e0a\u4e0b\u6587\u7ba1\u7406\u5e73\u53f0\uff08CMP\uff09\u4e2d\u7684\u4e0a\u4e0b\u6587\u7f13\u5b58\u3002DCMF\u5305\u62ec\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u4e0a\u4e0b\u6587\u8bc4\u4f30\u5f15\u64ce\uff08CEE\uff09\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\u6a21\u5757\uff08CMM\uff09\u3002", "result": "DCMF\u5b9e\u73b0\u4e8612.5%\u7684\u66f4\u9ad8\u7f13\u5b58\u547d\u4e2d\u7387\uff0c\u5e76\u5c06\u7f13\u5b58\u8fc7\u671f\u65f6\u95f4\u51cf\u5c11\u4e86\u9ad8\u8fbe60%\u3002", "conclusion": "DCMF\u5728\u5b9e\u9645\u667a\u80fd\u57ce\u5e02\u6570\u636e\uff08\u7279\u522b\u662f\u4ea4\u901a\u548c\u9053\u8def\u5de5\u7a0b\u573a\u666f\uff09\u4e2d\u7684\u5b9e\u65bd\u548c\u8bc4\u4f30\u8868\u660e\uff0c\u4e0em-CAC\u6280\u672f\u76f8\u6bd4\uff0c\u7f13\u5b58\u547d\u4e2d\u7387\u63d0\u9ad8\u4e8612.5%\uff0c\u7f13\u5b58\u8fc7\u671f\u51cf\u5c11\u4e8660%\uff0c\u4ece\u800c\u786e\u4fdd\u4e86\u76f8\u5173\u4e0a\u4e0b\u6587\u7684\u53ca\u65f6\u4ea4\u4ed8\u5e76\u51cf\u5c11\u4e86\u5ef6\u8fdf\u3002\u8fd9\u4e9b\u7ed3\u679c\u8bc1\u660e\u4e86DCMF\u7684\u53ef\u6269\u5c55\u6027\u548c\u5bf9\u52a8\u6001\u4e0a\u4e0b\u6587\u611f\u77e5\u7269\u8054\u7f51\u73af\u5883\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2506.17272", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17272", "abs": "https://arxiv.org/abs/2506.17272", "authors": ["Youzheng Liu", "Jiyan Liu", "Xiaoman Xu", "Taihang Wang", "Yimin Wang", "Ye Jiang"], "title": "QUST_NLP at SemEval-2025 Task 7: A Three-Stage Retrieval Framework for Monolingual and Crosslingual Fact-Checked Claim Retrieval", "comment": null, "summary": "This paper describes the participation of QUST_NLP in the SemEval-2025 Task\n7. We propose a three-stage retrieval framework specifically designed for\nfact-checked claim retrieval. Initially, we evaluate the performance of several\nretrieval models and select the one that yields the best results for candidate\nretrieval. Next, we employ multiple re-ranking models to enhance the candidate\nresults, with each model selecting the Top-10 outcomes. In the final stage, we\nutilize weighted voting to determine the final retrieval outcomes. Our approach\nachieved 5th place in the monolingual track and 7th place in the crosslingual\ntrack. We release our system code at:\nhttps://github.com/warmth27/SemEval2025_Task7", "AI": {"tldr": "A three-stage retrieval framework for fact-checked claim retrieval, achieving 5th and 7th place in SemEval-2025 Task 7.", "motivation": "This paper describes the participation of QUST_NLP in the SemEval-2025 Task 7.", "method": "a three-stage retrieval framework specifically designed for fact-checked claim retrieval. Initially, evaluate the performance of several retrieval models and select the one that yields the best results for candidate retrieval. Next, employ multiple re-ranking models to enhance the candidate results, with each model selecting the Top-10 outcomes. In the final stage, utilize weighted voting to determine the final retrieval outcomes.", "result": "achieved 5th place in the monolingual track and 7th place in the crosslingual track", "conclusion": "The approach achieved 5th place in the monolingual track and 7th place in the crosslingual track."}}
{"id": "2506.17451", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2506.17451", "abs": "https://arxiv.org/abs/2506.17451", "authors": ["Aida Sheshbolouki", "M. Tamer Ozsu"], "title": "Transient Concepts in Streaming Graphs", "comment": null, "summary": "Concept Drift (CD) occurs when a change in a hidden context can induce\nchanges in a target concept. CD is a natural phenomenon in non-stationary\nsettings such as data streams. Understanding, detection, and adaptation to CD\nin streaming data is (i) vital for effective and efficient analytics as\nreliable output depends on adaptation to fresh input, (ii) challenging as it\nrequires efficient operations as well as effective performance evaluations, and\n(iii) impactful as it applies to a variety of use cases and is a crucial\ninitial step for data management systems. Current works are mostly focused on\npassive CD detection as part of supervised adaptation, on independently\ngenerated data instances or graph snapshots, on target concepts as a function\nof data labels, on static data management, and on specific temporal order of\ndata record. These methods do not always work. We revisit CD for the streaming\ngraphs setting and introduce two first-of-its-kind frameworks SGDD and SGDP for\nstreaming graph CD detection and prediction. Both frameworks discern the change\nof generative source. SGDD detects the CDs due to the changes of generative\nparameters with significant delays such that it is difficult to evaluate the\nperformance, while SGDP predicts these CDs between 7374 to 0.19 milliseconds\nahead of their occurrence, without accessing the payloads of data records.", "AI": {"tldr": "This paper introduces SGDD and SGDP, two frameworks for detecting and predicting concept drift in streaming graphs. SGDP can predict these CDs quickly, without accessing the payloads of data records.", "motivation": "Understanding, detection, and adaptation to CD in streaming data is (i) vital for effective and efficient analytics as reliable output depends on adaptation to fresh input, (ii) challenging as it requires efficient operations as well as effective performance evaluations, and (iii) impactful as it applies to a variety of use cases and is a crucial initial step for data management systems.", "method": "introduce two first-of-its-kind frameworks SGDD and SGDP for streaming graph CD detection and prediction", "result": "SGDD detects the CDs due to the changes of generative parameters with significant delays such that it is difficult to evaluate the performance, while SGDP predicts these CDs between 7374 to 0.19 milliseconds ahead of their occurrence, without accessing the payloads of data records.", "conclusion": "We revisit CD for the streaming graphs setting and introduce two first-of-its-kind frameworks SGDD and SGDP for streaming graph CD detection and prediction. Both frameworks discern the change of generative source. SGDD detects the CDs due to the changes of generative parameters with significant delays such that it is difficult to evaluate the performance, while SGDP predicts these CDs between 7374 to 0.19 milliseconds ahead of their occurrence, without accessing the payloads of data records."}}
{"id": "2506.17277", "categories": ["cs.IR", "cs.AI", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2506.17277", "abs": "https://arxiv.org/abs/2506.17277", "authors": ["Mahmoud Amiri", "Thomas Bocklitz"], "title": "Chunk Twice, Embed Once: A Systematic Study of Segmentation and Representation Trade-offs in Chemistry-Aware Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems are increasingly vital for\nnavigating the ever-expanding body of scientific literature, particularly in\nhigh-stakes domains such as chemistry. Despite the promise of RAG, foundational\ndesign choices -- such as how documents are segmented and represented -- remain\nunderexplored in domain-specific contexts. This study presents the first\nlarge-scale, systematic evaluation of chunking strategies and embedding models\ntailored to chemistry-focused RAG systems. We investigate 25 chunking\nconfigurations across five method families and evaluate 48 embedding models on\nthree chemistry-specific benchmarks, including the newly introduced\nQuestChemRetrieval dataset. Our results reveal that recursive token-based\nchunking (specifically R100-0) consistently outperforms other approaches,\noffering strong performance with minimal resource overhead. We also find that\nretrieval-optimized embeddings -- such as Nomic and Intfloat E5 variants --\nsubstantially outperform domain-specialized models like SciBERT. By releasing\nour datasets, evaluation framework, and empirical benchmarks, we provide\nactionable guidelines for building effective and efficient chemistry-aware RAG\nsystems.", "AI": {"tldr": "systematic evaluation of chunking strategies and embedding models tailored to chemistry-focused RAG systems", "motivation": "foundational design choices remain underexplored in domain-specific contexts such as chemistry.", "method": "large-scale, systematic evaluation of chunking strategies and embedding models tailored to chemistry-focused RAG systems. investigate 25 chunking configurations across five method families and evaluate 48 embedding models on three chemistry-specific benchmarks, including the newly introduced QuestChemRetrieval dataset.", "result": "recursive token-based chunking (specifically R100-0) consistently outperforms other approaches. retrieval-optimized embeddings substantially outperform domain-specialized models like SciBERT.", "conclusion": "recursive token-based chunking (specifically R100-0) consistently outperforms other approaches, offering strong performance with minimal resource overhead. retrieval-optimized embeddings substantially outperform domain-specialized models like SciBERT."}}
{"id": "2506.17702", "categories": ["cs.DB", "cs.CC"], "pdf": "https://arxiv.org/pdf/2506.17702", "abs": "https://arxiv.org/abs/2506.17702", "authors": ["Stefan Mengel"], "title": "Lower Bounds for Conjunctive Query Evaluation", "comment": "paper for the tutorial at PODS 2025", "summary": "In this tutorial, we will survey known results on the complexity of\nconjunctive query evaluation in different settings, ranging from Boolean\nqueries over counting to more complex models like enumeration and direct\naccess. A particular focus will be on showing how different relatively recent\nhypotheses from complexity theory connect to query answering and allow showing\nthat known algorithms in several cases can likely not be improved.", "AI": {"tldr": "complexity theory connect to query answering and allow showing that known algorithms in several cases can likely not be improved", "motivation": "complexity of conjunctive query evaluation in different settings", "method": "survey known results on the complexity of conjunctive query evaluation in different settings", "result": "ranging from Boolean queries over counting to more complex models like enumeration and direct access", "conclusion": "known algorithms in several cases can likely not be improved"}}
{"id": "2506.17281", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17281", "abs": "https://arxiv.org/abs/2506.17281", "authors": ["Junze Chen", "Xinjie Yang", "Cheng Yang", "Junfei Bao", "Zeyuan Guo", "Yawen Li", "Chuan Shi"], "title": "CORONA: A Coarse-to-Fine Framework for Graph-based Recommendation with Large Language Models", "comment": null, "summary": "Recommender systems (RSs) are designed to retrieve candidate items a user\nmight be interested in from a large pool. A common approach is using graph\nneural networks (GNNs) to capture high-order interaction relationships. As\nlarge language models (LLMs) have shown strong capabilities across domains,\nresearchers are exploring their use to enhance recommendation. However, prior\nwork limits LLMs to re-ranking results or dataset augmentation, failing to\nutilize their power during candidate filtering - which may lead to suboptimal\nperformance. Instead, we propose to leverage LLMs' reasoning abilities during\nthe candidate filtering process, and introduce Chain Of Retrieval ON grAphs\n(CORONA) to progressively narrow down the range of candidate items on\ninteraction graphs with the help of LLMs: (1) First, LLM performs preference\nreasoning based on user profiles, with the response serving as a query to\nextract relevant users and items from the interaction graph as\npreference-assisted retrieval; (2) Then, using the information retrieved in the\nprevious step along with the purchase history of target user, LLM conducts\nintent reasoning to help refine an even smaller interaction subgraph as\nintent-assisted retrieval; (3) Finally, we employ a GNN to capture high-order\ncollaborative filtering information from the extracted subgraph, performing\nGNN-enhanced retrieval to generate the final recommendation results. The\nproposed framework leverages the reasoning capabilities of LLMs during the\nretrieval process, while seamlessly integrating GNNs to enhance overall\nrecommendation performance. Extensive experiments on various datasets and\nsettings demonstrate that our proposed CORONA achieves state-of-the-art\nperformance with an 18.6% relative improvement in recall and an 18.4% relative\nimprovement in NDCG on average.", "AI": {"tldr": " leverage LLMs' reasoning abilities during the candidate filtering process to enhance recommendation", "motivation": "prior work limits LLMs to re-ranking results or dataset augmentation, failing to utilize their power during candidate filtering - which may lead to suboptimal performance", "method": " leverage LLMs' reasoning abilities during the candidate filtering process, and introduce Chain Of Retrieval ON grAphs (CORONA) to progressively narrow down the range of candidate items on interaction graphs with the help of LLMs", "result": "achieves state-of-the-art performance with an 18.6% relative improvement in recall and an 18.4% relative improvement in NDCG on average", "conclusion": "The proposed CORONA achieves state-of-the-art performance with an 18.6% relative improvement in recall and an 18.4% relative improvement in NDCG on average."}}
{"id": "2506.18013", "categories": ["cs.DB", "cs.DS"], "pdf": "https://arxiv.org/pdf/2506.18013", "abs": "https://arxiv.org/abs/2506.18013", "authors": ["Muhammad Farhan", "Henning Koehler", "Qing Wang"], "title": "Dual-Hierarchy Labelling: Scaling Up Distance Queries on Dynamic Road Networks", "comment": null, "summary": "Computing the shortest-path distance between any two given vertices in road\nnetworks is an important problem. A tremendous amount of research has been\nconducted to address this problem, most of which are limited to static road\nnetworks. Since road networks undergo various real-time traffic conditions,\nthere is a pressing need to address this problem for dynamic road networks.\nExisting state-of-the-art methods incrementally maintain an indexing structure\nto reflect dynamic changes on road networks. However, these methods suffer from\neither slow query response time or poor maintenance performance, particularly\nwhen road networks are large. In this work, we propose an efficient solution\n\\emph{Dual-Hierarchy Labelling (DHL)} for distance querying on dynamic road\nnetworks from a novel perspective, which incorporates two hierarchies with\ndifferent but complementary data structures to support efficient query and\nupdate processing. Specifically, our proposed solution is comprised of three\nmain components: \\emph{query hierarchy}, \\emph{update hierarchy}, and\n\\emph{hierarchical labelling}, where \\emph{query hierarchy} enables efficient\nquery answering by exploring only a small subset of vertices in the labels of\ntwo query vertices and \\emph{update hierarchy} supports efficient maintenance\nof distance labelling under edge weight increase or decrease. We further\ndevelop dynamic algorithms to reflect dynamic changes by efficiently\nmaintaining the update hierarchy and hierarchical labelling. We also propose a\nparallel variant of our dynamic algorithms by exploiting labelling structure.\nWe evaluate our methods on 10 large road networks and it shows that our methods\nsignificantly outperform the state-of-the-art methods, i.e., achieving\nconsiderably faster construction and update time, while being consistently 2-4\ntimes faster in terms of query processing and consuming only 10\\%-20\\%\nlabelling space.", "AI": {"tldr": "This paper introduces Dual-Hierarchy Labelling (DHL) for efficient shortest-path distance queries on dynamic road networks, outperforming existing methods in speed and space consumption.", "motivation": "Existing methods for shortest-path distance computation in dynamic road networks suffer from slow query response time or poor maintenance performance, especially on large networks.", "method": "The paper proposes Dual-Hierarchy Labelling (DHL), incorporating two hierarchies (query and update) with different data structures to support efficient query and update processing. Dynamic algorithms and a parallel variant are developed for maintaining the update hierarchy and hierarchical labelling.", "result": "DHL achieves considerably faster construction and update time, consistently 2-4 times faster query processing, and consumes only 10%-20% labelling space compared to state-of-the-art methods on 10 large road networks.", "conclusion": "The proposed DHL method significantly outperforms state-of-the-art methods on large road networks, achieving faster construction and update times, 2-4 times faster query processing, and consuming only 10%-20% labelling space."}}
{"id": "2506.17282", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2506.17282", "abs": "https://arxiv.org/abs/2506.17282", "authors": ["Rushi Wang", "Jiateng Liu", "Weijie Zhao", "Shenglan Li", "Denghui Zhang"], "title": "Automating Financial Statement Audits with Large Language Models", "comment": "14 pages", "summary": "Financial statement auditing is essential for stakeholders to understand a\ncompany's financial health, yet current manual processes are inefficient and\nerror-prone. Even with extensive verification procedures, auditors frequently\nmiss errors, leading to inaccurate financial statements that fail to meet\nstakeholder expectations for transparency and reliability. To this end, we\nharness large language models (LLMs) to automate financial statement auditing\nand rigorously assess their capabilities, providing insights on their\nperformance boundaries in the scenario of automated auditing. Our work\nintroduces a comprehensive benchmark using a curated dataset combining\nreal-world financial tables with synthesized transaction data. In the\nbenchmark, we developed a rigorous five-stage evaluation framework to assess\nLLMs' auditing capabilities. The benchmark also challenges models to map\nspecific financial statement errors to corresponding violations of accounting\nstandards, simulating real-world auditing scenarios through test cases. Our\ntesting reveals that current state-of-the-art LLMs successfully identify\nfinancial statement errors when given historical transaction data. However,\nthese models demonstrate significant limitations in explaining detected errors\nand citing relevant accounting standards. Furthermore, LLMs struggle to execute\ncomplete audits and make necessary financial statement revisions. These\nfindings highlight a critical gap in LLMs' domain-specific accounting\nknowledge. Future research must focus on enhancing LLMs' understanding of\nauditing principles and procedures. Our benchmark and evaluation framework\nestablish a foundation for developing more effective automated auditing tools\nthat will substantially improve the accuracy and efficiency of real-world\nfinancial statement auditing.", "AI": {"tldr": "LLMs can automate parts of financial auditing, but lack accounting knowledge for full automation.", "motivation": "Manual financial statement auditing is inefficient and error-prone, leading to inaccurate statements.", "method": "We introduce a benchmark dataset combining real-world financial tables with synthesized transaction data and a five-stage evaluation framework.", "result": "LLMs identify errors with transaction data but struggle to explain them or cite accounting standards, and can't execute complete audits.", "conclusion": "Current LLMs can identify errors but struggle with explanations, citing standards, and complete audits, revealing a gap in accounting knowledge."}}
{"id": "2506.17237", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17237", "abs": "https://arxiv.org/abs/2506.17237", "authors": ["Dip Roy"], "title": "Mechanistic Interpretability of Diffusion Models: Circuit-Level Analysis and Causal Validation", "comment": null, "summary": "We present a quantitative circuit-level analysis of diffusion models,\nestablishing computational pathways and mechanistic principles underlying image\ngeneration processes. Through systematic intervention experiments across 2,000\nsynthetic and 2,000 CelebA facial images, we discover fundamental algorithmic\ndifferences in how diffusion architectures process synthetic versus\nnaturalistic data distributions. Our investigation reveals that real-world face\nprocessing requires circuits with measurably higher computational complexity\n(complexity ratio = 1.084 plus/minus 0.008, p < 0.001), exhibiting distinct\nattention specialization patterns with entropy divergence ranging from 0.015 to\n0.166 across denoising timesteps. We identify eight functionally distinct\nattention mechanisms showing specialized computational roles: edge detection\n(entropy = 3.18 plus/minus 0.12), texture analysis (entropy = 4.16 plus/minus\n0.08), and semantic understanding (entropy = 2.67 plus/minus 0.15).\nIntervention analysis demonstrates critical computational bottlenecks where\ntargeted ablations produce 25.6% to 128.3% performance degradation, providing\ncausal evidence for identified circuit functions. These findings establish\nquantitative foundations for algorithmic understanding and control of\ngenerative model behavior through mechanistic intervention strategies.", "AI": {"tldr": "\u901a\u8fc7\u7535\u8def\u7ea7\u5206\u6790\u548c\u5e72\u9884\u5b9e\u9a8c\uff0c\u63ed\u793a\u4e86\u6269\u6563\u6a21\u578b\u5904\u7406\u5408\u6210\u548c\u81ea\u7136\u56fe\u50cf\u7684\u7b97\u6cd5\u5dee\u5f02\uff0c\u5e76\u8bc6\u522b\u4e86\u5173\u952e\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u8ba1\u7b97\u74f6\u9888\u3002", "motivation": "\u63d0\u51fa\u6269\u6563\u6a21\u578b\u7684\u5b9a\u91cf\u7535\u8def\u7ea7\u5206\u6790\uff0c\u5efa\u7acb\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u7684\u8ba1\u7b97\u8def\u5f84\u548c\u673a\u5236\u539f\u7406\u3002", "method": "\u901a\u8fc7\u5bf92,000\u4e2a\u5408\u6210\u56fe\u50cf\u548c2,000\u4e2aCelebA\u9762\u90e8\u56fe\u50cf\u7684\u7cfb\u7edf\u5e72\u9884\u5b9e\u9a8c\uff0c\u8fdb\u884c\u5b9a\u91cf\u7535\u8def\u7ea7\u5206\u6790\u3002", "result": "\u63ed\u793a\u4e86\u771f\u5b9e\u9762\u90e8\u5904\u7406\u9700\u8981\u5177\u6709\u53ef\u6d4b\u91cf\u66f4\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u7535\u8def\uff08\u590d\u6742\u5ea6\u6bd4=1.084 +/- 0.008\uff0cp < 0.001\uff09\uff0c\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u6ce8\u610f\u529b\u4e13\u4e1a\u5316\u6a21\u5f0f\uff0c\u53bb\u566a\u65f6\u95f4\u6b65\u957f\u7684\u71b5\u6563\u5ea6\u8303\u56f4\u4e3a0.015\u81f30.166\u3002\u786e\u5b9a\u4e86\u516b\u79cd\u529f\u80fd\u4e0d\u540c\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u793a\u51fa\u4e13\u95e8\u7684\u8ba1\u7b97\u4f5c\u7528\uff1a\u8fb9\u7f18\u68c0\u6d4b\uff08\u71b5=3.18 +/- 0.12\uff09\uff0c\u7eb9\u7406\u5206\u6790\uff08\u71b5=4.16 +/- 0.08\uff09\u548c\u8bed\u4e49\u7406\u89e3\uff08\u71b5=2.67 +/- 0.15\uff09\u3002\u5e72\u9884\u5206\u6790\u8868\u660e\uff0c\u6709\u9488\u5bf9\u6027\u7684\u6d88\u878d\u4ea7\u751f25.6\uff05\u81f3128.3\uff05\u7684\u6027\u80fd\u4e0b\u964d\u7684\u5173\u952e\u8ba1\u7b97\u74f6\u9888\uff0c\u4e3a\u5df2\u8bc6\u522b\u7684\u7535\u8def\u529f\u80fd\u63d0\u4f9b\u56e0\u679c\u8bc1\u636e\u3002", "conclusion": "\u5efa\u7acb\u4e86\u751f\u6210\u6a21\u578b\u884c\u4e3a\u7684\u7b97\u6cd5\u7406\u89e3\u548c\u63a7\u5236\u7684\u5b9a\u91cf\u57fa\u7840\uff0c\u901a\u8fc7\u673a\u68b0\u5e72\u9884\u7b56\u7565\u3002"}}
{"id": "2506.17223", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.17223", "abs": "https://arxiv.org/abs/2506.17223", "authors": ["Shuvra Smaran Das", "Anirban Saha Anik", "Md Kishor Morol", "Mohammad Sakib Mahmood"], "title": "Outcome-Based Education: Evaluating Students' Perspectives Using Transformer", "comment": "6 pages, 7 figures", "summary": "Outcome-Based Education (OBE) emphasizes the development of specific\ncompetencies through student-centered learning. In this study, we reviewed the\nimportance of OBE and implemented transformer-based models, particularly\nDistilBERT, to analyze an NLP dataset that includes student feedback. Our\nobjective is to assess and improve educational outcomes. Our approach is better\nthan other machine learning models because it uses the transformer's deep\nunderstanding of language context to classify sentiment better, giving better\nresults across a wider range of matrices. Our work directly contributes to\nOBE's goal of achieving measurable outcomes by facilitating the identification\nof patterns in student learning experiences. We have also applied LIME (local\ninterpretable model-agnostic explanations) to make sure that model predictions\nare clear. This gives us understandable information about how key terms affect\nsentiment. Our findings indicate that the combination of transformer models and\nLIME explanations results in a strong and straightforward framework for\nanalyzing student feedback. This aligns more closely with the principles of OBE\nand ensures the improvement of educational practices through data-driven\ninsights.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528 DistilBERT \u548c LIME \u5206\u6790\u5b66\u751f\u53cd\u9988\uff0c\u4ee5\u6539\u8fdb\u57fa\u4e8e\u7ed3\u679c\u7684\u6559\u80b2 (OBE)\u3002", "motivation": "\u57fa\u4e8e\u7ed3\u679c\u7684\u6559\u80b2 (OBE) \u5f3a\u8c03\u901a\u8fc7\u4ee5\u5b66\u751f\u4e3a\u4e2d\u5fc3\u7684\u5b66\u4e60\u6765\u53d1\u5c55\u7279\u5b9a\u7684\u80fd\u529b\u3002\u672c\u7814\u7a76\u56de\u987e\u4e86 OBE \u7684\u91cd\u8981\u6027\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u8bc4\u4f30\u548c\u63d0\u9ad8\u6559\u80b2\u6210\u679c\u3002", "method": "\u5b9e\u65bd\u57fa\u4e8e transformer \u7684\u6a21\u578b\uff0c\u7279\u522b\u662f DistilBERT\uff0c\u4ee5\u5206\u6790\u5305\u542b\u5b66\u751f\u53cd\u9988\u7684 NLP \u6570\u636e\u96c6\uff0c\u5e76\u5e94\u7528 LIME (local interpretable model-agnostic explanations) \u6765\u786e\u4fdd\u6a21\u578b\u9884\u6d4b\u6e05\u6670\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u56e0\u4e3a\u5b83\u5229\u7528 transformer \u5bf9\u8bed\u8a00\u4e0a\u4e0b\u6587\u7684\u6df1\u523b\u7406\u89e3\u6765\u66f4\u597d\u5730\u5206\u7c7b\u60c5\u611f\uff0c\u4ece\u800c\u5728\u66f4\u5e7f\u6cdb\u7684\u77e9\u9635\u4e2d\u7ed9\u51fa\u66f4\u597d\u7684\u7ed3\u679c\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u901a\u8fc7\u4fc3\u8fdb\u8bc6\u522b\u5b66\u751f\u5b66\u4e60\u7ecf\u5386\u4e2d\u7684\u6a21\u5f0f\uff0c\u76f4\u63a5\u6709\u52a9\u4e8e OBE \u5b9e\u73b0\u53ef\u8861\u91cf\u7684\u6210\u679c\u7684\u76ee\u6807\u3002", "conclusion": "transformer \u6a21\u578b\u548c LIME \u89e3\u91ca\u7684\u7ed3\u5408\u4e3a\u5206\u6790\u5b66\u751f\u53cd\u9988\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u800c\u76f4\u63a5\u7684\u6846\u67b6\u3002\u8fd9\u4e0e OBE \u7684\u539f\u5219\u66f4\u52a0\u7d27\u5bc6\u5730\u7ed3\u5408\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u89c1\u89e3\u786e\u4fdd\u6539\u8fdb\u6559\u80b2\u5b9e\u8df5\u3002"}}
{"id": "2506.17289", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17289", "abs": "https://arxiv.org/abs/2506.17289", "authors": ["Rahul Raja", "Arpita Vats"], "title": "Evaluating Generalization and Representation Stability in Small LMs via Prompting", "comment": "Accepted at ICML", "summary": "We investigate the generalization capabilities of small language models under\ntwo popular adaptation paradigms: few-shot prompting and supervised\nfine-tuning. While prompting is often favored for its parameter efficiency and\nflexibility, it remains unclear how robust this approach is in low-resource\nsettings and under distributional shifts. This paper presents a comparative\nstudy of prompting and fine-tuning across task formats, prompt styles, and\nmodel scales, with a focus on their behavior in both in-distribution and\nout-of-distribution (OOD) settings.\n  Beyond accuracy, we analyze the internal representations learned by each\napproach to assess the stability and abstraction of task-specific features. Our\nfindings highlight critical differences in how small models internalize and\ngeneralize knowledge under different adaptation strategies. This work offers\npractical guidance for model selection in low-data regimes and contributes\nempirical insight into the ongoing debate over prompting versus fine-tuning.\nCode for the experiments is available at the following", "AI": {"tldr": "This paper compares prompting and fine-tuning for adapting small language models, finding differences in how they generalize knowledge, especially in low-resource settings.", "motivation": "While prompting is often favored for its parameter efficiency and flexibility, it remains unclear how robust this approach is in low-resource settings and under distributional shifts.", "method": "comparative study of prompting and fine-tuning across task formats, prompt styles, and model scales, with a focus on their behavior in both in-distribution and out-of-distribution (OOD) settings. Beyond accuracy, we analyze the internal representations learned by each approach to assess the stability and abstraction of task-specific features.", "result": "findings highlight critical differences in how small models internalize and generalize knowledge under different adaptation strategies.", "conclusion": "This work offers practical guidance for model selection in low-data regimes and contributes empirical insight into the ongoing debate over prompting versus fine-tuning."}}
{"id": "2506.17230", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17230", "abs": "https://arxiv.org/abs/2506.17230", "authors": ["Yichen Luo", "Jia Wang", "Dapeng Lan", "Yu Liu", "Zhibo Pang"], "title": "MMET: A Multi-Input and Multi-Scale Transformer for Efficient PDEs Solving", "comment": null, "summary": "Partial Differential Equations (PDEs) are fundamental for modeling physical\nsystems, yet solving them in a generic and efficient manner using machine\nlearning-based approaches remains challenging due to limited multi-input and\nmulti-scale generalization capabilities, as well as high computational costs.\nThis paper proposes the Multi-input and Multi-scale Efficient Transformer\n(MMET), a novel framework designed to address the above challenges. MMET\ndecouples mesh and query points as two sequences and feeds them into the\nencoder and decoder, respectively, and uses a Gated Condition Embedding (GCE)\nlayer to embed input variables or functions with varying dimensions, enabling\neffective solutions for multi-scale and multi-input problems. Additionally, a\nHilbert curve-based reserialization and patch embedding mechanism decrease the\ninput length. This significantly reduces the computational cost when dealing\nwith large-scale geometric models. These innovations enable efficient\nrepresentations and support multi-scale resolution queries for large-scale and\nmulti-input PDE problems. Experimental evaluations on diverse benchmarks\nspanning different physical fields demonstrate that MMET outperforms SOTA\nmethods in both accuracy and computational efficiency. This work highlights the\npotential of MMET as a robust and scalable solution for real-time PDE solving\nin engineering and physics-based applications, paving the way for future\nexplorations into pre-trained large-scale models in specific domains. This work\nis open-sourced at https://github.com/YichenLuo-0/MMET.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Transformer\u6846\u67b6MMET\uff0c\u7528\u4e8e\u9ad8\u6548\u6c42\u89e3\u591a\u8f93\u5165\u548c\u591a\u5c3a\u5ea6\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u5e76\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u901a\u7528\u5730\u3001\u9ad8\u6548\u5730\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u4ecd\u7136\u5177\u6709\u6311\u6218\uff0c\u8fd9\u662f\u7531\u4e8e\u6709\u9650\u7684\u591a\u8f93\u5165\u548c\u591a\u5c3a\u5ea6\u6cdb\u5316\u80fd\u529b\u4ee5\u53ca\u9ad8\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u591a\u8f93\u5165\u548c\u591a\u5c3a\u5ea6\u9ad8\u6548Transformer (MMET)\uff0c\u8fd9\u662f\u4e00\u79cd\u65e8\u5728\u89e3\u51b3\u4e0a\u8ff0\u6311\u6218\u7684\u65b0\u9896\u6846\u67b6\u3002MMET \u5c06\u7f51\u683c\u548c\u67e5\u8be2\u70b9\u89e3\u8026\u4e3a\u4e24\u4e2a\u5e8f\u5217\uff0c\u5e76\u5c06\u5b83\u4eec\u5206\u522b\u9988\u9001\u5230\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e2d\uff0c\u5e76\u4f7f\u7528\u95e8\u63a7\u6761\u4ef6\u5d4c\u5165 (GCE) \u5c42\u6765\u5d4c\u5165\u5177\u6709\u4e0d\u540c\u7ef4\u5ea6\u7684\u8f93\u5165\u53d8\u91cf\u6216\u51fd\u6570\uff0c\u4ece\u800c\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u591a\u5c3a\u5ea6\u548c\u591a\u8f93\u5165\u95ee\u9898\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u5e0c\u5c14\u4f2f\u7279\u66f2\u7ebf\u7684\u91cd\u65b0\u5e8f\u5217\u5316\u548c\u8865\u4e01\u5d4c\u5165\u673a\u5236\u51cf\u5c11\u4e86\u8f93\u5165\u957f\u5ea6\u3002", "result": "MMET \u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e SOTA \u65b9\u6cd5\u3002", "conclusion": "MMET\u5728\u4e0d\u540c\u7269\u7406\u9886\u57df\u7684\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e SOTA \u65b9\u6cd5\u3002\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86 MMET \u4f5c\u4e3a\u5de5\u7a0b\u548c\u57fa\u4e8e\u7269\u7406\u7684\u5e94\u7528\u7a0b\u5e8f\u4e2d\u5b9e\u65f6 PDE \u6c42\u89e3\u7684\u5f3a\u5927\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u63a2\u7d22\u7279\u5b9a\u9886\u57df\u4e2d\u9884\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2506.18062", "categories": ["cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.18062", "abs": "https://arxiv.org/abs/2506.18062", "authors": ["Samirasadat Jamalidinan", "Kazem Cheshmi"], "title": "Floating-Point Data Transformation for Lossless Compression", "comment": null, "summary": "Floating-point data is widely used across various domains. Depending on the\nrequired precision, each floating-point value can occupy several bytes.\nLossless storage of this information is crucial due to its critical accuracy,\nas seen in applications such as medical imaging and language model weights. In\nthese cases, data size is often significant, making lossless compression\nessential. Previous approaches either treat this data as raw byte streams for\ncompression or fail to leverage all patterns within the dataset. However,\nbecause multiple bytes represent a single value and due to inherent patterns in\nfloating-point representations, some of these bytes are correlated. To leverage\nthis property, we propose a novel data transformation method called Typed Data\nTransformation (\\DTT{}) that groups related bytes together to improve\ncompression. We implemented and tested our approach on various datasets across\nboth CPU and GPU. \\DTT{} achieves a geometric mean compression ratio\nimprovement of 1.16$\\times$ over state-of-the-art compression tools such as\nzstd, while also improving both compression and decompression throughput by\n1.18--3.79$\\times$.", "AI": {"tldr": "This paper introduces Typed Data Transformation (DTT) for lossless compression of floating-point data, improving compression ratio and throughput compared to existing tools.", "motivation": "Lossless storage of floating-point data is crucial due to its critical accuracy, as seen in applications such as medical imaging and language model weights. Previous approaches either treat this data as raw byte streams for compression or fail to leverage all patterns within the dataset. However, because multiple bytes represent a single value and due to inherent patterns in floating-point representations, some of these bytes are correlated.", "method": "a novel data transformation method called Typed Data Transformation (DTT) that groups related bytes together to improve compression", "result": "DTT achieves a geometric mean compression ratio improvement of 1.16x over state-of-the-art compression tools such as zstd, while also improving both compression and decompression throughput by 1.18--3.79x.", "conclusion": "Typed Data Transformation (DTT) achieves a geometric mean compression ratio improvement of 1.16x over state-of-the-art compression tools such as zstd, while also improving both compression and decompression throughput by 1.18--3.79x."}}
{"id": "2506.17285", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17285", "abs": "https://arxiv.org/abs/2506.17285", "authors": ["Vinaik Chhetri", "Yousaf Reza", "Moghis Fereidouni", "Srijata Maji", "Umar Farooq", "AB Siddique"], "title": "A Framework for Generating Conversational Recommendation Datasets from Behavioral Interactions", "comment": "12 pages, 6 tables,4 figures", "summary": "Modern recommendation systems typically follow two complementary paradigms:\ncollaborative filtering, which models long-term user preferences from\nhistorical interactions, and conversational recommendation systems (CRS), which\ninteract with users in natural language to uncover immediate needs. Each\ncaptures a different dimension of user intent. While CRS models lack\ncollaborative signals, leading to generic or poorly personalized suggestions,\ntraditional recommenders lack mechanisms to interactively elicit immediate\nneeds. Unifying these paradigms promises richer personalization but remains\nchallenging due to the lack of large-scale conversational datasets grounded in\nreal user behavior. We present ConvRecStudio, a framework that uses large\nlanguage models (LLMs) to simulate realistic, multi-turn dialogs grounded in\ntimestamped user-item interactions and reviews. ConvRecStudio follows a\nthree-stage pipeline: (1) Temporal Profiling, which constructs user profiles\nand community-level item sentiment trajectories over fine-grained aspects; (2)\nSemantic Dialog Planning, which generates a structured plan using a DAG of\nflexible super-nodes; and (3) Multi-Turn Simulation, which instantiates the\nplan using paired LLM agents for the user and system, constrained by\nexecutional and behavioral fidelity checks. We apply ConvRecStudio to three\ndomains -- MobileRec, Yelp, and Amazon Electronics -- producing over 12K\nmulti-turn dialogs per dataset. Human and automatic evaluations confirm the\nnaturalness, coherence, and behavioral grounding of the generated\nconversations. To demonstrate utility, we build a cross-attention transformer\nmodel that jointly encodes user history and dialog context, achieving gains in\nHit@K and NDCG@K over baselines using either signal alone or naive fusion.\nNotably, our model achieves a 10.9% improvement in Hit@1 on Yelp over the\nstrongest baseline.", "AI": {"tldr": "ConvRecStudio, a framework using LLMs, simulates realistic dialogs for recommendation systems, unifying collaborative filtering and CRS. A cross-attention transformer model built using this framework achieves significant improvements in recommendation accuracy.", "motivation": "Modern recommendation systems typically follow two complementary paradigms: collaborative filtering and conversational recommendation systems (CRS). Each captures a different dimension of user intent. While CRS models lack collaborative signals, leading to generic or poorly personalized suggestions, traditional recommenders lack mechanisms to interactively elicit immediate needs. Unifying these paradigms promises richer personalization but remains challenging due to the lack of large-scale conversational datasets grounded in real user behavior.", "method": "ConvRecStudio, a framework that uses large language models (LLMs) to simulate realistic, multi-turn dialogs grounded in timestamped user-item interactions and reviews. It follows a three-stage pipeline: (1) Temporal Profiling, (2) Semantic Dialog Planning, and (3) Multi-Turn Simulation.", "result": "ConvRecStudio is applied to three domains -- MobileRec, Yelp, and Amazon Electronics -- producing over 12K multi-turn dialogs per dataset. Human and automatic evaluations confirm the naturalness, coherence, and behavioral grounding of the generated conversations.", "conclusion": "A cross-attention transformer model is built that jointly encodes user history and dialog context, achieving gains in Hit@K and NDCG@K over baselines. The model achieves a 10.9% improvement in Hit@1 on Yelp over the strongest baseline."}}
{"id": "2506.17290", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17290", "abs": "https://arxiv.org/abs/2506.17290", "authors": ["Yuqi Li", "Junhao Dong", "Zeyu Dong", "Chuanguang Yang", "Zhulin An", "Yongjun Xu"], "title": "SRKD: Towards Efficient 3D Point Cloud Segmentation via Structure- and Relation-aware Knowledge Distillation", "comment": "13 pages", "summary": "3D point cloud segmentation faces practical challenges due to the\ncomputational complexity and deployment limitations of large-scale\ntransformer-based models. To address this, we propose a novel Structure- and\nRelation-aware Knowledge Distillation framework, named SRKD, that transfers\nrich geometric and semantic knowledge from a large frozen teacher model (>100M)\nto a lightweight student model (<15M). Specifically, we propose an affinity\nmatrix-based relation alignment module, which distills structural dependencies\nfrom the teacher to the student through point-wise similarity matching,\nenhancing the student's capability to learn contextual interactions. Meanwhile,\nwe introduce a cross-sample mini-batch construction strategy that enables the\nstudent to perceive stable and generalized geometric structure. This aligns\nacross diverse point cloud instances of the teacher, rather than within a\nsingle sample. Additionally, KL divergence is applied to align semantic\ndistributions, and ground-truth supervision further reinforces accurate\nsegmentation. Our method achieves state of the art performance with\nsignificantly reduced model complexity, demonstrating its effectiveness and\nefficiency in real-world deployment scenarios. Our Code is available at\nhttps://github.com/itsnotacie/SRKD.", "AI": {"tldr": "Proposes SRKD, a knowledge distillation framework, to transfer knowledge from a large teacher model to a lightweight student model for 3D point cloud segmentation, achieving state-of-the-art performance with reduced complexity.", "motivation": "3D point cloud segmentation faces practical challenges due to the computational complexity and deployment limitations of large-scale transformer-based models. To address this,", "method": "We propose a novel Structure- and Relation-aware Knowledge Distillation framework, named SRKD, that transfers rich geometric and semantic knowledge from a large frozen teacher model (>100M) to a lightweight student model (<15M). Specifically, we propose an affinity matrix-based relation alignment module, which distills structural dependencies from the teacher to the student through point-wise similarity matching, enhancing the student's capability to learn contextual interactions. Meanwhile, we introduce a cross-sample mini-batch construction strategy that enables the student to perceive stable and generalized geometric structure.", "result": "achieves state of the art performance with significantly reduced model complexity", "conclusion": "This method achieves state of the art performance with significantly reduced model complexity, demonstrating its effectiveness and efficiency in real-world deployment scenarios."}}
{"id": "2506.17231", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.17231", "abs": "https://arxiv.org/abs/2506.17231", "authors": ["Xiang Li", "Chong Zhang", "Jia Wang", "Fangyu Wu", "Yushi Li", "Xiaobo Jin"], "title": "Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs", "comment": "15 pages, 5 figures", "summary": "Attacks on large language models (LLMs) in jailbreaking scenarios raise many\nsecurity and ethical issues. Current jailbreak attack methods face problems\nsuch as low efficiency, high computational cost, and poor cross-model\nadaptability and versatility, which make it difficult to cope with the rapid\ndevelopment of LLM and new defense strategies. Our work proposes an Adversarial\nPrompt Distillation, which combines masked language modeling, reinforcement\nlearning, and dynamic temperature control through a prompt generation and\ndistillation method. It enables small language models (SLMs) to jailbreak\nattacks on mainstream LLMs. The experimental results verify the superiority of\nthe proposed method in terms of attack success rate and harm, and reflect the\nresource efficiency and cross-model adaptability. This research explores the\nfeasibility of distilling the jailbreak ability of LLM to SLM, reveals the\nmodel's vulnerability, and provides a new idea for LLM security research.", "AI": {"tldr": "Distilling jailbreak attacks from LLMs to SLMs using Adversarial Prompt Distillation.", "motivation": "Current jailbreak attack methods face problems such as low efficiency, high computational cost, and poor cross-model adaptability and versatility, which make it difficult to cope with the rapid development of LLM and new defense strategies.", "method": "Adversarial Prompt Distillation, which combines masked language modeling, reinforcement learning, and dynamic temperature control through a prompt generation and distillation method. It enables small language models (SLMs) to jailbreak attacks on mainstream LLMs.", "result": "The experimental results verify the superiority of the proposed method in terms of attack success rate and harm, and reflect the resource efficiency and cross-model adaptability.", "conclusion": "This research explores the feasibility of distilling the jailbreak ability of LLM to SLM, reveals the model's vulnerability, and provides a new idea for LLM security research."}}
{"id": "2506.17300", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17300", "abs": "https://arxiv.org/abs/2506.17300", "authors": ["Daniel T. Chang"], "title": "Individual Causal Inference with Structural Causal Model", "comment": null, "summary": "Individual causal inference (ICI) uses causal inference methods to understand\nand predict the effects of interventions on individuals, considering their\nspecific characteristics / facts. It aims to estimate individual causal effect\n(ICE), which varies across individuals. Estimating ICE can be challenging due\nto the limited data available for individuals, and the fact that most causal\ninference methods are population-based. Structural Causal Model (SCM) is\nfundamentally population-based. Therefore, causal discovery (structural\nlearning and parameter learning), association queries and intervention queries\nare all naturally population-based. However, exogenous variables (U) in SCM can\nencode individual variations and thus provide the mechanism for individualized\npopulation per specific individual characteristics / facts. Based on this, we\npropose ICI with SCM as a \"rung 3\" causal inference, because it involves\n\"imagining\" what would be the causal effect of a hypothetical intervention on\nan individual, given the individual's observed characteristics / facts.\nSpecifically, we propose the indiv-operator, indiv(W), to formalize/represent\nthe population individualization process, and the individual causal query, P(Y\n| indiv(W), do(X), Z), to formalize/represent ICI. We show and argue that ICI\nwith SCM is inference on individual alternatives (possible), not individual\ncounterfactuals (non-actual).", "AI": {"tldr": "This paper proposes a new method for individual causal inference (ICI) using Structural Causal Models (SCM).", "motivation": "Estimating ICE can be challenging due to the limited data available for individuals, and the fact that most causal inference methods are population-based. Structural Causal Model (SCM) is fundamentally population-based. However, exogenous variables (U) in SCM can encode individual variations and thus provide the mechanism for individualized population per specific individual characteristics / facts.", "method": "We propose the indiv-operator, indiv(W), to formalize/represent the population individualization process, and the individual causal query, P(Y | indiv(W), do(X), Z), to formalize/represent ICI.", "result": "propose ICI with SCM as a \"rung 3\" causal inference, because it involves \"imagining\" what would be the causal effect of a hypothetical intervention on an individual, given the individual's observed characteristics / facts.", "conclusion": "ICI with SCM is inference on individual alternatives (possible), not individual counterfactuals (non-actual)."}}
{"id": "2506.17232", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17232", "abs": "https://arxiv.org/abs/2506.17232", "authors": ["Zelin Zang", "Fei Wang", "Liangyu Li", "Jinlin Wu", "Chunshui Zhao", "Zhen Lei", "Baigui Sun"], "title": "PCaM: A Progressive Focus Attention-Based Information Fusion Method for Improving Vision Transformer Domain Adaptation", "comment": null, "summary": "Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a\nlabeled source domain to an unlabeled target domain. Recent UDA methods based\non Vision Transformers (ViTs) have achieved strong performance through\nattention-based feature alignment. However, we identify a key limitation:\nforeground object mismatch, where the discrepancy in foreground object size and\nspatial distribution across domains weakens attention consistency and hampers\neffective domain alignment. To address this issue, we propose the Progressive\nFocus Cross-Attention Mechanism (PCaM), which progressively filters out\nbackground information during cross-attention, allowing the model to focus on\nand fuse discriminative foreground semantics across domains. We further\nintroduce an attentional guidance loss that explicitly directs attention toward\ntask-relevant regions, enhancing cross-domain attention consistency. PCaM is\nlightweight, architecture-agnostic, and easy to integrate into existing\nViT-based UDA pipelines. Extensive experiments on Office-Home, DomainNet,\nVisDA-2017, and remote sensing datasets demonstrate that PCaM significantly\nimproves adaptation performance and achieves new state-of-the-art results,\nvalidating the effectiveness of attention-guided foreground fusion for domain\nadaptation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdProgressive Focus Cross-Attention Mechanism (PCaM)\uff0c\u4ee5\u89e3\u51b3\u975e\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\u4e2d\u524d\u666f\u5bf9\u8c61\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u524d\u666f\u5bf9\u8c61\u4e0d\u5339\u914d\uff0c\u5176\u4e2d\u8de8\u57df\u7684\u524d\u666f\u5bf9\u8c61\u5927\u5c0f\u548c\u7a7a\u95f4\u5206\u5e03\u7684\u5dee\u5f02\u524a\u5f31\u4e86\u6ce8\u610f\u529b\u4e00\u81f4\u6027\uff0c\u5e76\u963b\u788d\u4e86\u6709\u6548\u7684\u9886\u57df\u5bf9\u9f50\u3002", "method": "Progressive Focus Cross-Attention Mechanism (PCaM)", "result": "\u5728Office-Home\u3001DomainNet\u3001VisDA-2017\u548c\u9065\u611f\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPCaM\u663e\u8457\u63d0\u9ad8\u4e86\u9002\u5e94\u6027\u80fd\uff0c\u5e76\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "PCaM\u663e\u8457\u63d0\u9ad8\u4e86\u9002\u5e94\u6027\u80fd\uff0c\u5e76\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u6ce8\u610f\u5bfc\u5411\u7684\u524d\u666f\u878d\u5408\u5bf9\u9886\u57df\u9002\u5e94\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.18252", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2506.18252", "abs": "https://arxiv.org/abs/2506.18252", "authors": ["Jinjin Zhao"], "title": "Learning Lineage Constraints for Data Science Operations", "comment": null, "summary": "Data science workflows often integrate functionalities from a diverse set of\nlibraries and frameworks. Tasks such as debugging require data lineage that\ncrosses library boundaries. The problem is that the way that \"lineage\" is\nrepresented is often intimately tied to particular data models and data\nmanipulation paradigms. Inspired by the use of intermediate representations\n(IRs) in cross-library performance optimizations, this vision paper proposes a\nsimilar architecture for lineage - how do we specify logical lineage across\nlibraries in a common parameterized way? In practice, cross-library workflows\nwill contain both known operations and unknown operations, so a key design of\nXProv to link both materialized lineage graphs of data transformations and the\naforementioned abstracted logical patterns. We further discuss early ideas on\nhow to infer logical patterns when only the materialized graphs are available.", "AI": {"tldr": "This paper proposes XProv, an architecture for cross-library data lineage in data science workflows, using logical patterns and materialized graphs.", "motivation": "Debugging data science workflows that integrate functionalities from diverse libraries and frameworks requires data lineage that crosses library boundaries. Current lineage representations are often tied to particular data models and data manipulation paradigms.", "method": "The paper introduces XProv, which links materialized lineage graphs of data transformations and abstracted logical patterns. It also discusses ideas on inferring logical patterns from materialized graphs.", "result": "The paper presents early ideas on how to infer logical patterns when only materialized graphs are available.", "conclusion": "This paper proposes an architecture for lineage, called XProv, to specify logical lineage across libraries in a common parameterized way."}}
{"id": "2506.17287", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17287", "abs": "https://arxiv.org/abs/2506.17287", "authors": ["Aneta Poniszewska-Maranda", "Magdalena Pakula", "Bozena Borowska"], "title": "Recommendation systems in e-commerce applications with machine learning methods", "comment": "29th International Conference on Evaluation and Assessment in\n  Software Engineering, 17-20 June, 2025, Istanbul, Turkey", "summary": "E-commerce platforms are increasingly reliant on recommendation systems to\nenhance user experience, retain customers, and, in most cases, drive sales. The\nintegration of machine learning methods into these systems has significantly\nimproved their efficiency, personalization, and scalability. This paper aims to\nhighlight the current trends in e-commerce recommendation systems, identify\nchallenges, and evaluate the effectiveness of various machine learning methods\nused, including collaborative filtering, content-based filtering, and hybrid\nmodels. A systematic literature review (SLR) was conducted, analyzing 38\npublications from 2013 to 2025. The methods used were evaluated and compared to\ndetermine their performance and effectiveness in addressing e-commerce\nchallenges.", "AI": {"tldr": "This paper reviews machine learning methods in e-commerce recommendation systems, analyzing trends, challenges, and the effectiveness of different approaches using a systematic literature review.", "motivation": "E-commerce platforms are increasingly reliant on recommendation systems to enhance user experience, retain customers, and, in most cases, drive sales. The integration of machine learning methods into these systems has significantly improved their efficiency, personalization, and scalability.", "method": "A systematic literature review (SLR) was conducted, analyzing 38 publications from 2013 to 2025. The methods used were evaluated and compared to determine their performance and effectiveness in addressing e-commerce challenges.", "result": "The paper identifies current trends in e-commerce recommendation systems, identifies challenges, and evaluates the effectiveness of various machine learning methods used, including collaborative filtering, content-based filtering, and hybrid models.", "conclusion": "This paper concludes by evaluating the effectiveness of various machine learning methods used in e-commerce recommendation systems."}}
{"id": "2506.17302", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17302", "abs": "https://arxiv.org/abs/2506.17302", "authors": ["Yijun Lin", "Theresa Chen", "Colby Brungard", "Grunwald Sabine", "Sue Ives", "Matt Macander", "Timm Nawrocki", "Yao-Yi Chiang", "Nic Jelinski"], "title": "Fine-Scale Soil Mapping in Alaska with Multimodal Machine Learning", "comment": "12 pages, Submitted to SIGSPATIAL 2025", "summary": "Fine-scale soil mapping in Alaska, traditionally relying on fieldwork and\nlocalized simulations, remains a critical yet underdeveloped task, despite the\nregion's ecological importance and extensive permafrost coverage. As permafrost\nthaw accelerates due to climate change, it threatens infrastructure stability\nand key ecosystem services, such as soil carbon storage. High-resolution soil\nmaps are essential for characterizing permafrost distribution, identifying\nvulnerable areas, and informing adaptation strategies. We present MISO, a\nvision-based machine learning (ML) model to produce statewide fine-scale soil\nmaps for near-surface permafrost and soil taxonomy. The model integrates a\ngeospatial foundation model for visual feature extraction, implicit neural\nrepresentations for continuous spatial prediction, and contrastive learning for\nmultimodal alignment and geo-location awareness. We compare MISO with Random\nForest (RF), a traditional ML model that has been widely used in soil mapping\napplications. Spatial cross-validation and regional analysis across Permafrost\nZones and Major Land Resource Areas (MLRAs) show that MISO generalizes better\nto remote, unseen locations and achieves higher recall than RF, which is\ncritical for monitoring permafrost thaw and related environmental processes.\nThese findings demonstrate the potential of advanced ML approaches for\nfine-scale soil mapping and provide practical guidance for future soil sampling\nand infrastructure planning in permafrost-affected landscapes. The project will\nbe released at https://github.com/knowledge-computing/Peatland-permafrost.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578bMISO\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u571f\u58e4\u5730\u56fe\uff0c\u8be5\u6a21\u578b\u5728\u6c38\u4e45\u51bb\u571f\u5730\u533a\u57df\u7684\u6cdb\u5316\u6027\u548c\u53ec\u56de\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u6c14\u5019\u53d8\u5316\uff0c\u6c38\u4e45\u51bb\u571f\u878d\u5316\u52a0\u901f\uff0c\u5a01\u80c1\u5230\u57fa\u7840\u8bbe\u65bd\u7684\u7a33\u5b9a\u6027\u548c\u5173\u952e\u7684\u751f\u6001\u7cfb\u7edf\u670d\u52a1\uff0c\u5982\u571f\u58e4\u78b3\u50a8\u5b58\u3002\u9ad8\u5206\u8fa8\u7387\u7684\u571f\u58e4\u5730\u56fe\u5bf9\u4e8e\u63cf\u8ff0\u6c38\u4e45\u51bb\u571f\u7684\u5206\u5e03\u3001\u8bc6\u522b\u8106\u5f31\u533a\u57df\u548c\u4e3a\u9002\u5e94\u7b56\u7565\u63d0\u4f9b\u4fe1\u606f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578bMISO\uff0c\u7528\u4e8e\u751f\u6210\u5168\u5dde\u8303\u56f4\u7684\u8fd1\u5730\u8868\u6c38\u4e45\u51bb\u571f\u548c\u571f\u58e4\u5206\u7c7b\u7684\u7ec6\u5c3a\u5ea6\u571f\u58e4\u5730\u56fe\u3002\u8be5\u6a21\u578b\u96c6\u6210\u4e86\u7528\u4e8e\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u7684\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u3001\u7528\u4e8e\u8fde\u7eed\u7a7a\u95f4\u9884\u6d4b\u7684\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u4ee5\u53ca\u7528\u4e8e\u591a\u6a21\u6001\u5bf9\u9f50\u548c\u5730\u7406\u4f4d\u7f6e\u611f\u77e5\u7684\u5bf9\u6bd4\u5b66\u4e60\u3002", "result": "\u7a7a\u95f4\u4ea4\u53c9\u9a8c\u8bc1\u548c\u8de8\u6c38\u4e45\u51bb\u571f\u5e26\u548c\u4e3b\u8981\u571f\u5730\u8d44\u6e90\u533a (MLRA) \u7684\u533a\u57df\u5206\u6790\u8868\u660e\uff0cMISO \u6bd4 RF \u66f4\u597d\u5730\u63a8\u5e7f\u5230\u504f\u8fdc\u3001\u672a\u89c1\u8fc7\u7684\u4f4d\u7f6e\uff0c\u5e76\u4e14\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u53ec\u56de\u7387\u3002", "conclusion": "MISO\u6a21\u578b\u5728\u6cdb\u5316\u6027\u548c\u53ec\u56de\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578bRF\uff0c\u4e3a\u7ec6\u5c3a\u5ea6\u571f\u58e4 mapping \u63d0\u4f9b\u4e86\u6f5c\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u571f\u58e4\u91c7\u6837\u548c\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2506.17286", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17286", "abs": "https://arxiv.org/abs/2506.17286", "authors": ["Luoyang Sun", "Jiwen Jiang", "Cheng Deng", "Xinjian Wu", "Haifeng Zhang", "Lei Chen", "Lionel Ni", "Jun Wang"], "title": "GTA: Grouped-head latenT Attention", "comment": null, "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.", "AI": {"tldr": "\u63d0\u51fa\u4e86GTA\u6ce8\u610f\u529b\u673a\u5236\uff0c\u51cf\u5c11\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u63d0\u9ad8LLM\u90e8\u7f72\u6548\u7387\u3002", "motivation": "\u6ce8\u610f\u529b\u673a\u5236\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6210\u529f\u7684\u5173\u952e\uff0c\u4f46\u5176\u5de8\u5927\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u5bf9\u4f18\u5316\u6548\u7387\u548c\u6027\u80fd\u63d0\u51fa\u4e86\u6311\u6218\u3002KV\u7f13\u5b58\u548c\u6ce8\u610f\u529b\u8ba1\u7b97\u968f\u7740\u6587\u672c\u957f\u5ea6\u7684\u5feb\u901f\u6269\u5c55\uff0c\u5bf9\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\u6709\u9650\u7684\u786c\u4ef6\u90e8\u7f72\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u5206\u7ec4\u5934\u6f5c\u5728\u6ce8\u610f\u529b\uff08GTA\uff09\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u5171\u4eab\u6ce8\u610f\u529b\u56fe\u548c\u975e\u7ebf\u6027\u503c\u89e3\u7801\u5668\u6765\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "GTA\u5c06\u6ce8\u610f\u529b\u8ba1\u7b97\u7684FLOPs\u964d\u4f4e\u4e86\u9ad8\u8fbe62.5%\uff0c\u5e76\u5c06KV\u7f13\u5b58\u7f29\u5c0f\u4e86\u9ad8\u8fbe70%\uff0c\u540c\u65f6\u907f\u514d\u4e86\u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b\u7684\u989d\u5916\u5f00\u9500\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86LLM\u7684\u90e8\u7f72\u6548\u7387\u3002", "conclusion": "GTA\u6a21\u578b\u5b9e\u73b0\u4e862\u500d\u7684\u7aef\u5230\u7aef\u63a8\u7406\u901f\u5ea6\u63d0\u5347\uff0c\u9884\u586b\u5145\u53d7\u76ca\u4e8e\u8ba1\u7b97\u6210\u672c\u7684\u964d\u4f4e\uff0c\u89e3\u7801\u53d7\u76ca\u4e8e\u66f4\u5c0f\u7684\u7f13\u5b58\u5360\u7528\u7a7a\u95f4\u3002"}}
{"id": "2506.17434", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17434", "abs": "https://arxiv.org/abs/2506.17434", "authors": ["Sydney Levine", "Matija Franklin", "Tan Zhi-Xuan", "Secil Yanik Guyot", "Lionel Wong", "Daniel Kilov", "Yejin Choi", "Joshua B. Tenenbaum", "Noah Goodman", "Seth Lazar", "Iason Gabriel"], "title": "Resource Rational Contractualism Should Guide AI Alignment", "comment": "24 pages, 10 figures", "summary": "AI systems will soon have to navigate human environments and make decisions\nthat affect people and other AI agents whose goals and values diverge.\nContractualist alignment proposes grounding those decisions in agreements that\ndiverse stakeholders would endorse under the right conditions, yet securing\nsuch agreement at scale remains costly and slow -- even for advanced AI. We\ntherefore propose Resource-Rational Contractualism (RRC): a framework where AI\nsystems approximate the agreements rational parties would form by drawing on a\ntoolbox of normatively-grounded, cognitively-inspired heuristics that trade\neffort for accuracy. An RRC-aligned agent would not only operate efficiently,\nbut also be equipped to dynamically adapt to and interpret the ever-changing\nhuman social world.", "AI": {"tldr": "AI systems can use RRC to make decisions that affect people and other AI agents.", "motivation": "AI systems will soon have to navigate human environments and make decisions that affect people and other AI agents whose goals and values diverge. Contractualist alignment proposes grounding those decisions in agreements that diverse stakeholders would endorse under the right conditions, yet securing such agreement at scale remains costly and slow -- even for advanced AI.", "method": "Resource-Rational Contractualism (RRC): a framework where AI systems approximate the agreements rational parties would form by drawing on a toolbox of normatively-grounded, cognitively-inspired heuristics that trade effort for accuracy.", "result": "AI systems can use Resource-Rational Contractualism (RRC) to approximate agreements and operate efficiently.", "conclusion": "An RRC-aligned agent would not only operate efficiently, but also be equipped to dynamically adapt to and interpret the ever-changing human social world."}}
{"id": "2506.17234", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17234", "abs": "https://arxiv.org/abs/2506.17234", "authors": ["Payam Zohari", "Mostafa Haghir Chehreghani"], "title": "Graph Neural Networks in Multi-Omics Cancer Research: A Structured Survey", "comment": "51 pages", "summary": "The task of data integration for multi-omics data has emerged as a powerful\nstrategy to unravel the complex biological underpinnings of cancer. Recent\nadvancements in graph neural networks (GNNs) offer an effective framework to\nmodel heterogeneous and structured omics data, enabling precise representation\nof molecular interactions and regulatory networks. This systematic review\nexplores several recent studies that leverage GNN-based architectures in\nmulti-omics cancer research. We classify the approaches based on their targeted\nomics layers, graph neural network structures, and biological tasks such as\nsubtype classification, prognosis prediction, and biomarker discovery. The\nanalysis reveals a growing trend toward hybrid and interpretable models,\nalongside increasing adoption of attention mechanisms and contrastive learning.\nFurthermore, we highlight the use of patient-specific graphs and\nknowledge-driven priors as emerging directions. This survey serves as a\ncomprehensive resource for researchers aiming to design effective GNN-based\npipelines for integrative cancer analysis, offering insights into current\npractices, limitations, and potential future directions.", "AI": {"tldr": "GNNs are increasingly used for multi-omics cancer research. This review classifies GNN approaches and highlights trends like hybrid models and patient-specific graphs.", "motivation": "The task of data integration for multi-omics data has emerged as a powerful strategy to unravel the complex biological underpinnings of cancer. Recent advancements in graph neural networks (GNNs) offer an effective framework to model heterogeneous and structured omics data, enabling precise representation of molecular interactions and regulatory networks.", "method": "This systematic review explores several recent studies that leverage GNN-based architectures in multi-omics cancer research. We classify the approaches based on their targeted omics layers, graph neural network structures, and biological tasks such as subtype classification, prognosis prediction, and biomarker discovery.", "result": "The analysis reveals a growing trend toward hybrid and interpretable models, alongside increasing adoption of attention mechanisms and contrastive learning. Furthermore, we highlight the use of patient-specific graphs and knowledge-driven priors as emerging directions.", "conclusion": "This survey serves as a comprehensive resource for researchers aiming to design effective GNN-based pipelines for integrative cancer analysis, offering insights into current practices, limitations, and potential future directions."}}
{"id": "2506.18255", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2506.18255", "abs": "https://arxiv.org/abs/2506.18255", "authors": ["Jinjin Zhao", "Sanjay Krishnan"], "title": "Fast Capture of Cell-Level Provenance in Numpy", "comment": null, "summary": "Effective provenance tracking enhances reproducibility, governance, and data\nquality in array workflows. However, significant challenges arise in capturing\nthis provenance, including: (1) rapidly evolving APIs, (2) diverse operation\ntypes, and (3) large-scale datasets. To address these challenges, this paper\npresents a prototype annotation system designed for arrays, which captures\ncell-level provenance specifically within the numpy library. With this\nprototype, we explore straightforward memory optimizations that substantially\nreduce annotation latency. We envision this provenance capture approach for\narrays as part of a broader governance system for tracking for structured data\nworkflows and diverse data science applications.", "AI": {"tldr": "This paper introduces a prototype annotation system for numpy arrays that captures cell-level provenance and explores memory optimizations to reduce annotation latency.", "motivation": "Effective provenance tracking enhances reproducibility, governance, and data quality in array workflows. However, significant challenges arise in capturing this provenance, including: (1) rapidly evolving APIs, (2) diverse operation types, and (3) large-scale datasets.", "method": "This paper presents a prototype annotation system designed for arrays, which captures cell-level provenance specifically within the numpy library.", "result": "With this prototype, we explore straightforward memory optimizations that substantially reduce annotation latency.", "conclusion": "This paper envisions a provenance capture approach for arrays as part of a broader governance system for tracking for structured data workflows and diverse data science applications."}}
{"id": "2506.17288", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.17288", "abs": "https://arxiv.org/abs/2506.17288", "authors": ["Jiale Zhang", "Jiaxiang Chen", "Zhucong Li", "Jie Ding", "Kui Zhao", "Zenglin Xu", "Xin Pang", "Yinghui Xu"], "title": "SlimRAG: Retrieval without Graphs via Entity-Aware Context Selection", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances language models by\nincorporating external knowledge at inference time. However, graph-based RAG\nsystems often suffer from structural overhead and imprecise retrieval: they\nrequire costly pipelines for entity linking and relation extraction, yet\nfrequently return subgraphs filled with loosely related or tangential content.\nThis stems from a fundamental flaw -- semantic similarity does not imply\nsemantic relevance. We introduce SlimRAG, a lightweight framework for retrieval\nwithout graphs. SlimRAG replaces structure-heavy components with a simple yet\neffective entity-aware mechanism. At indexing time, it constructs a compact\nentity-to-chunk table based on semantic embeddings. At query time, it\nidentifies salient entities, retrieves and scores associated chunks, and\nassembles a concise, contextually relevant input -- without graph traversal or\nedge construction. To quantify retrieval efficiency, we propose Relative Index\nToken Utilization (RITU), a metric measuring the compactness of retrieved\ncontent. Experiments across multiple QA benchmarks show that SlimRAG\noutperforms strong flat and graph-based baselines in accuracy while reducing\nindex size and RITU (e.g., 16.31 vs. 56+), highlighting the value of\nstructure-free, entity-centric context selection. The code will be released\nsoon. https://github.com/continue-ai-company/SlimRAG", "AI": {"tldr": "SlimRAG\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u65e0\u56fe\u68c0\u7d22\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u5b9e\u4f53\u611f\u77e5\u673a\u5236\u6765\u63d0\u9ad8\u68c0\u7d22\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u57fa\u4e8e\u56fe\u7684RAG\u7cfb\u7edf\u901a\u5e38\u53d7\u5230\u7ed3\u6784\u5f00\u9500\u548c\u4e0d\u7cbe\u786e\u68c0\u7d22\u7684\u5f71\u54cd\uff1a\u5b83\u4eec\u9700\u8981\u6602\u8d35\u7684\u5b9e\u4f53\u94fe\u63a5\u548c\u5173\u7cfb\u63d0\u53d6\u7ba1\u9053\uff0c\u4f46\u7ecf\u5e38\u8fd4\u56de\u5145\u6ee1\u677e\u6563\u76f8\u5173\u6216\u5207\u5411\u5185\u5bb9\u7684\u5b50\u56fe\u3002\u8fd9\u6e90\u4e8e\u4e00\u4e2a\u6839\u672c\u7684\u7f3a\u9677\u2014\u2014\u8bed\u4e49\u76f8\u4f3c\u6027\u5e76\u4e0d\u610f\u5473\u7740\u8bed\u4e49\u76f8\u5173\u6027\u3002", "method": "SlimRAG\u7528\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u5b9e\u4f53\u611f\u77e5\u673a\u5236\u53d6\u4ee3\u4e86\u7ed3\u6784\u7e41\u91cd\u7684\u7ec4\u4ef6\u3002\u5728\u7d22\u5f15\u65f6\uff0c\u5b83\u6784\u5efa\u4e00\u4e2a\u57fa\u4e8e\u8bed\u4e49\u5d4c\u5165\u7684\u7d27\u51d1\u7684\u5b9e\u4f53\u5230\u5757\u8868\u3002\u5728\u67e5\u8be2\u65f6\uff0c\u5b83\u8bc6\u522b\u663e\u8457\u7684\u5b9e\u4f53\uff0c\u68c0\u7d22\u548c\u8bc4\u5206\u76f8\u5173\u7684\u5757\uff0c\u5e76\u7ec4\u88c5\u4e00\u4e2a\u7b80\u6d01\u7684\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u8f93\u5165\u2014\u2014\u6ca1\u6709\u56fe\u904d\u5386\u6216\u8fb9\u7f18\u6784\u5efa\u3002", "result": "SlimRAG\u4f18\u4e8e\u5f3a\u5927\u7684\u5e73\u9762\u548c\u57fa\u4e8e\u56fe\u7684\u57fa\u7ebf\uff0c\u540c\u65f6\u51cf\u5c11\u7d22\u5f15\u5927\u5c0f\u548cRITU (\u4f8b\u5982\uff0c16.31 vs. 56+)\u3002", "conclusion": "SlimRAG\u5728\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u5f3a\u5927\u7684\u5e73\u9762\u548c\u57fa\u4e8e\u56fe\u7684\u57fa\u7ebf\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u7d22\u5f15\u5927\u5c0f\u548cRITU\uff0c\u7a81\u51fa\u4e86\u65e0\u7ed3\u6784\u3001\u4ee5\u5b9e\u4f53\u4e3a\u4e2d\u5fc3\u7684\u4e0a\u4e0b\u6587\u9009\u62e9\u7684\u4ef7\u503c\u3002"}}
{"id": "2506.17325", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17325", "abs": "https://arxiv.org/abs/2506.17325", "authors": ["Sina Najafi", "M. Hadi Sepanj", "Fahimeh Jafari"], "title": "RadarSeq: A Temporal Vision Framework for User Churn Prediction via Radar Chart Sequences", "comment": null, "summary": "Predicting user churn in non-subscription gig platforms, where disengagement\nis implicit, poses unique challenges due to the absence of explicit labels and\nthe dynamic nature of user behavior. Existing methods often rely on aggregated\nsnapshots or static visual representations, which obscure temporal cues\ncritical for early detection. In this work, we propose a temporally-aware\ncomputer vision framework that models user behavioral patterns as a sequence of\nradar chart images, each encoding day-level behavioral features. By integrating\na pretrained CNN encoder with a bidirectional LSTM, our architecture captures\nboth spatial and temporal patterns underlying churn behavior. Extensive\nexperiments on a large real-world dataset demonstrate that our method\noutperforms classical models and ViT-based radar chart baselines, yielding\ngains of 17.7 in F1 score, 29.4 in precision, and 16.1 in AUC, along with\nimproved interpretability. The framework's modular design, explainability\ntools, and efficient deployment characteristics make it suitable for\nlarge-scale churn modeling in dynamic gig-economy platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u611f\u77e5\u8ba1\u7b97\u673a\u89c6\u89c9\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f18\u4e8e\u7ecf\u5178\u6a21\u578b\u548c ViT \u57fa\u7ebf\uff0c\u5728\u975e\u8ba2\u9605\u96f6\u5de5\u5e73\u53f0\u4e2d\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u5ba2\u6237\u6d41\u5931\u5efa\u6a21\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u660e\u786e\u7684\u6807\u7b7e\u4ee5\u53ca\u7528\u6237\u884c\u4e3a\u7684\u52a8\u6001\u6027\uff0c\u5728\u975e\u8ba2\u9605\u96f6\u5de5\u5e73\u53f0\u4e2d\u9884\u6d4b\u7528\u6237\u6d41\u5931\uff08\u5176\u4e2d\u8131\u79bb\u662f\u9690\u542b\u7684\uff09\u63d0\u51fa\u4e86\u72ec\u7279\u7684\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u805a\u5408\u5feb\u7167\u6216\u9759\u6001\u89c6\u89c9\u8868\u793a\uff0c\u8fd9\u63a9\u76d6\u4e86\u5bf9\u65e9\u671f\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\u7684\u65f6\u95f4\u7ebf\u7d22\u3002", "method": "\u5c06\u7528\u6237\u884c\u4e3a\u6a21\u5f0f\u5efa\u6a21\u4e3a\u96f7\u8fbe\u56fe\u56fe\u50cf\u5e8f\u5217\u7684\u65f6\u95f4\u611f\u77e5\u8ba1\u7b97\u673a\u89c6\u89c9\u6846\u67b6\uff0c\u6bcf\u4e2a\u96f7\u8fbe\u56fe\u90fd\u7f16\u7801\u4e86\u6bcf\u65e5\u7ea7\u522b\u7684\u884c\u4e3a\u7279\u5f81\u3002\u901a\u8fc7\u5c06\u9884\u8bad\u7ec3\u7684 CNN \u7f16\u7801\u5668\u4e0e\u53cc\u5411 LSTM \u96c6\u6210\uff0c\u8be5\u67b6\u6784\u6355\u83b7\u4e86\u5ba2\u6237\u6d41\u5931\u884c\u4e3a\u4e0b\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u6a21\u5f0f\u3002", "result": "\u5728\u5927\u578b\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u7ecf\u5178\u6a21\u578b\u548c\u57fa\u4e8e ViT \u7684\u96f7\u8fbe\u56fe\u57fa\u7ebf\uff0c\u5728 F1 \u5206\u6570\u4e0a\u63d0\u9ad8\u4e86 17.7\uff0c\u5728\u7cbe\u786e\u5ea6\u4e0a\u63d0\u9ad8\u4e86 29.4\uff0c\u5728 AUC \u4e0a\u63d0\u9ad8\u4e86 16.1\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u3001\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u548c\u9ad8\u6548\u7684\u90e8\u7f72\u7279\u6027\u4f7f\u5176\u9002\u7528\u4e8e\u52a8\u6001\u96f6\u5de5\u7ecf\u6d4e\u5e73\u53f0\u4e2d\u7684\u5927\u89c4\u6a21\u5ba2\u6237\u6d41\u5931\u5efa\u6a21\u3002"}}
{"id": "2506.17294", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17294", "abs": "https://arxiv.org/abs/2506.17294", "authors": ["Qirui Zheng", "Xingbo Wang", "Keyuan Cheng", "Yunlong Lu", "Wenxin Li"], "title": "AI-Generated Game Commentary: A Survey and a Datasheet Repository", "comment": null, "summary": "AI-Generated Game Commentary (AIGGC) has gained increasing attention due to\nits market potential and inherent technical challenges. As a comprehensive\nmultimodal Natural Language Processing (NLP) task, AIGGC imposes substantial\ndemands on language models, including factual accuracy, logical reasoning,\nexpressive text generation, generation speed, and context management. In this\npaper, we introduce a general framework for AIGGC and present a comprehensive\nsurvey of 45 existing game commentary dataset and methods according to key\nchallenges they aim to address in this domain. We further classify and compare\nvarious evaluation metrics commonly used in this domain. To support future\nresearch and benchmarking, we also provide a structured datasheet summarizing\nthe essential attributes of these datasets in appendix, which is meanwhile\npublicly available in an open repository.", "AI": {"tldr": "introduce a general framework for AIGGC and present a comprehensive survey of 45 existing game commentary dataset and methods, also provide a structured datasheet summarizing the essential attributes of these datasets", "motivation": "AI-Generated Game Commentary (AIGGC) has gained increasing attention due to its market potential and inherent technical challenges. As a comprehensive multimodal Natural Language Processing (NLP) task, AIGGC imposes substantial demands on language models, including factual accuracy, logical reasoning, expressive text generation, generation speed, and context management", "method": "introduce a general framework for AIGGC and present a comprehensive survey of 45 existing game commentary dataset and methods", "result": "classify and compare various evaluation metrics commonly used in this domain", "conclusion": "To support future research and benchmarking, we also provide a structured datasheet summarizing the essential attributes of these datasets in appendix, which is meanwhile publicly available in an open repository."}}
{"id": "2506.17442", "categories": ["cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17442", "abs": "https://arxiv.org/abs/2506.17442", "authors": ["Hao Guan", "David Bates", "Li Zhou"], "title": "Keeping Medical AI Healthy: A Review of Detection and Correction Methods for System Degradation", "comment": "15 pages, 5 figures", "summary": "Artificial intelligence (AI) is increasingly integrated into modern\nhealthcare, offering powerful support for clinical decision-making. However, in\nreal-world settings, AI systems may experience performance degradation over\ntime, due to factors such as shifting data distributions, changes in patient\ncharacteristics, evolving clinical protocols, and variations in data quality.\nThese factors can compromise model reliability, posing safety concerns and\nincreasing the likelihood of inaccurate predictions or adverse outcomes. This\nreview presents a forward-looking perspective on monitoring and maintaining the\n\"health\" of AI systems in healthcare. We highlight the urgent need for\ncontinuous performance monitoring, early degradation detection, and effective\nself-correction mechanisms. The paper begins by reviewing common causes of\nperformance degradation at both data and model levels. We then summarize key\ntechniques for detecting data and model drift, followed by an in-depth look at\nroot cause analysis. Correction strategies are further reviewed, ranging from\nmodel retraining to test-time adaptation. Our survey spans both traditional\nmachine learning models and state-of-the-art large language models (LLMs),\noffering insights into their strengths and limitations. Finally, we discuss\nongoing technical challenges and propose future research directions. This work\naims to guide the development of reliable, robust medical AI systems capable of\nsustaining safe, long-term deployment in dynamic clinical settings.", "AI": {"tldr": "The paper reviews the causes of performance degradation in AI systems within healthcare and suggests methods for monitoring, detection, and correction to ensure long-term safe deployment.", "motivation": "AI systems may experience performance degradation over time, due to factors such as shifting data distributions, changes in patient characteristics, evolving clinical protocols, and variations in data quality. These factors can compromise model reliability, posing safety concerns and increasing the likelihood of inaccurate predictions or adverse outcomes.", "method": "reviewing common causes of performance degradation at both data and model levels, summarizing key techniques for detecting data and model drift, followed by an in-depth look at root cause analysis and Correction strategies are further reviewed, ranging from model retraining to test-time adaptation.", "result": "This review presents a forward-looking perspective on monitoring and maintaining the 'health' of AI systems in healthcare. We highlight the urgent need for continuous performance monitoring, early degradation detection, and effective self-correction mechanisms.", "conclusion": "This work aims to guide the development of reliable, robust medical AI systems capable of sustaining safe, long-term deployment in dynamic clinical settings."}}
{"id": "2506.17238", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17238", "abs": "https://arxiv.org/abs/2506.17238", "authors": ["Siddharth M. Narayanan", "James D. Braza", "Ryan-Rhys Griffiths", "Albert Bou", "Geemi Wellawatte", "Mayk Caldas Ramos", "Ludovico Mitchener", "Samuel G. Rodriques", "Andrew D. White"], "title": "Training a Scientific Reasoning Model for Chemistry", "comment": null, "summary": "Reasoning models are large language models that emit a long chain-of-thought\nbefore answering, providing both higher accuracy and explicit reasoning for\ntheir response. A major question has been whether language model reasoning\ngeneralizes beyond mathematics, programming, and logic, where most previous\nwork has focused. We demonstrate that reasoning models can be post-trained for\nchemistry without additional domain pretraining, and require substantially less\ndata compared to contemporary domain-specific models. We report ether0, a 24B\nparameter LLM (based on Mistral-Small-24B) that can reason in natural language\nand respond with chemical structures. This reasoning model was trained with\nreinforcement learning on 640,730 experimentally-grounded chemistry problems\nacross 375 tasks ranging from synthesizability, to blood-brain barrier\npermeability, to human receptor activity, to scent. Our model exceeds\ngeneral-purpose chemistry models, frontier models, and human experts on\nmolecular design tasks. It is also more data efficient relative to specialized\nmodels. We anticipate that this method can be applied to train data-efficient\nlanguage models specialized for tasks across a wide variety of scientific\ndomains.", "AI": {"tldr": "\u63a8\u7406\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u540e\u8bad\u7ec3\u7528\u4e8e\u5316\u5b66\uff0c\u800c\u65e0\u9700\u989d\u5916\u7684\u9886\u57df\u9884\u8bad\u7ec3\uff0c\u5e76\u4e14\u4e0e\u5f53\u4ee3\u9886\u57df\u7279\u5b9a\u6a21\u578b\u76f8\u6bd4\uff0c\u9700\u8981\u7684\u6570\u636e\u91cf\u8981\u5c11\u5f97\u591a\u3002", "motivation": "\u4e3b\u8981\u95ee\u9898\u662f\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u662f\u5426\u80fd\u63a8\u5e7f\u5230\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u903b\u8f91\u4e4b\u5916\uff0c\u800c\u4e4b\u524d\u7684\u5927\u90e8\u5206\u5de5\u4f5c\u90fd\u96c6\u4e2d\u5728\u8fd9\u4e9b\u9886\u57df\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5728 640,730 \u4e2a\u5b9e\u9a8c\u6027\u5316\u5b66\u95ee\u9898\u4e0a\u8bad\u7ec3\u4e86\u4e00\u4e2a 24B \u53c2\u6570\u7684 LLM (\u57fa\u4e8e Mistral-Small-24B)\u3002", "result": "\u8be5\u6a21\u578b\u5728\u5206\u5b50\u8bbe\u8ba1\u4efb\u52a1\u4e0a\u8d85\u8fc7\u4e86\u901a\u7528\u5316\u5b66\u6a21\u578b\u3001\u524d\u6cbf\u6a21\u578b\u548c\u4eba\u7c7b\u4e13\u5bb6\u3002\u76f8\u5bf9\u4e8e\u4e13\u7528\u6a21\u578b\uff0c\u5b83\u4e5f\u66f4\u5177\u6709\u6570\u636e\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5e94\u7528\u4e8e\u8bad\u7ec3\u6570\u636e\u9ad8\u6548\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u4e13\u95e8\u7528\u4e8e\u5404\u79cd\u79d1\u5b66\u9886\u57df\u7684\u4efb\u52a1\u3002"}}
{"id": "2506.18257", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2506.18257", "abs": "https://arxiv.org/abs/2506.18257", "authors": ["Jinjin Zhao", "Sanjay Krishnan"], "title": "TableVault: Managing Dynamic Data Collections for LLM-Augmented Workflows", "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful tools for automating\nand executing complex data tasks. However, their integration into more complex\ndata workflows introduces significant management challenges. In response, we\npresent TableVault - a data management system designed to handle dynamic data\ncollections in LLM-augmented environments. TableVault meets the demands of\nthese workflows by supporting concurrent execution, ensuring reproducibility,\nmaintaining robust data versioning, and enabling composable workflow design. By\nmerging established database methodologies with emerging LLM-driven\nrequirements, TableVault offers a transparent platform that efficiently manages\nboth structured data and associated data artifacts.", "AI": {"tldr": "TableVault is a data management system designed to handle dynamic data collections in LLM-augmented environments.", "motivation": "integration of Large Language Models (LLMs) into more complex data workflows introduces significant management challenges", "method": "merging established database methodologies with emerging LLM-driven requirements", "result": "TableVault meets the demands of these workflows by supporting concurrent execution, ensuring reproducibility, maintaining robust data versioning, and enabling composable workflow design.", "conclusion": "TableVault offers a transparent platform that efficiently manages both structured data and associated data artifacts."}}
{"id": "2506.17493", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2506.17493", "abs": "https://arxiv.org/abs/2506.17493", "authors": ["Damian Martinez", "Catalina Riano", "Hui Fang"], "title": "PreQRAG -- Classify and Rewrite for Enhanced RAG", "comment": "7 pages, SIGIR 2025 LiveRAG", "summary": "This paper presents the submission of the UDInfo team to the SIGIR 2025\nLiveRAG Challenge. We introduce PreQRAG, a Retrieval Augmented Generation (RAG)\narchitecture designed to improve retrieval and generation quality through\ntargeted question preprocessing. PreQRAG incorporates a pipeline that first\nclassifies each input question as either single-document or multi-document\ntype. For single-document questions, we employ question rewriting techniques to\nimprove retrieval precision and generation relevance. For multi-document\nquestions, we decompose complex queries into focused sub-questions that can be\nprocessed more effectively by downstream components. This classification and\nrewriting strategy improves the RAG performance. Experimental evaluation of the\nLiveRAG Challenge dataset demonstrates the effectiveness of our\nquestion-type-aware architecture, with PreQRAG achieving the preliminary second\nplace in Session 2 of the LiveRAG challenge.", "AI": {"tldr": "PreQRAG improves RAG performance by classifying questions and rewriting/decomposing them based on type, achieving 2nd place in LiveRAG Challenge Session 2.", "motivation": "The paper aims to improve retrieval and generation quality in Retrieval Augmented Generation (RAG) through targeted question preprocessing.", "method": "The paper introduces PreQRAG, a Retrieval Augmented Generation (RAG) architecture that classifies questions as single-document or multi-document type and applies question rewriting or decomposition techniques accordingly.", "result": "PreQRAG achieves the preliminary second place in Session 2 of the LiveRAG challenge.", "conclusion": "The PreQRAG architecture achieves second place in Session 2 of the LiveRAG challenge, demonstrating the effectiveness of the question-type-aware approach."}}
{"id": "2506.17332", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17332", "abs": "https://arxiv.org/abs/2506.17332", "authors": ["Haitian Wang", "Yiren Wang", "Xinyu Wang", "Yumeng Miao", "Yuliang Zhang", "Yu Zhang", "Atif Mansoor"], "title": "P2MFDS: A Privacy-Preserving Multimodal Fall Detection System for Elderly People in Bathroom Environments", "comment": "Accepted to appear in the 2025 IEEE International Workshop on AIoT\n  and Smart Systems (AIoTSys'25). Nominated for Best Paper Award and Best IoT\n  System Implementation Award. Code and pretrained models available at:\n  https://github.com/HaitianWang/P2MFDS-A-Privacy-Preserving-Multimodal-Fall-Detection-Network-for-Elderly-Individuals-in-Bathroom", "summary": "By 2050, people aged 65 and over are projected to make up 16 percent of the\nglobal population. As aging is closely associated with increased fall risk,\nparticularly in wet and confined environments such as bathrooms where over 80\npercent of falls occur. Although recent research has increasingly focused on\nnon-intrusive, privacy-preserving approaches that do not rely on wearable\ndevices or video-based monitoring, these efforts have not fully overcome the\nlimitations of existing unimodal systems (e.g., WiFi-, infrared-, or\nmmWave-based), which are prone to reduced accuracy in complex environments.\nThese limitations stem from fundamental constraints in unimodal sensing,\nincluding system bias and environmental interference, such as multipath fading\nin WiFi-based systems and drastic temperature changes in infrared-based\nmethods. To address these challenges, we propose a Privacy-Preserving\nMultimodal Fall Detection System for Elderly People in Bathroom Environments.\nFirst, we develop a sensor evaluation framework to select and fuse\nmillimeter-wave radar with 3D vibration sensing, and use it to construct and\npreprocess a large-scale, privacy-preserving multimodal dataset in real\nbathroom settings, which will be released upon publication. Second, we\nintroduce P2MFDS, a dual-stream network combining a CNN-BiLSTM-Attention branch\nfor radar motion dynamics with a multi-scale CNN-SEBlock-Self-Attention branch\nfor vibration impact detection. By uniting macro- and micro-scale features,\nP2MFDS delivers significant gains in accuracy and recall over state-of-the-art\napproaches. Code and pretrained models will be made available at:\nhttps://github.com/HaitianWang/P2MFDS-A-Privacy-Preserving-Multimodal-Fall-Detection-Network-for-Elderly-Individuals-in-Bathroom.", "AI": {"tldr": "Developed P2MFDS, a privacy-preserving multimodal fall detection system for elderly people in bathroom environments, which unites macro- and micro-scale features to improve accuracy and recall.", "motivation": "Aging is closely associated with increased fall risk, particularly in wet and confined environments such as bathrooms where over 80 percent of falls occur. Although recent research has increasingly focused on non-intrusive, privacy-preserving approaches that do not rely on wearable devices or video-based monitoring, these efforts have not fully overcome the limitations of existing unimodal systems", "method": "a dual-stream network combining a CNN-BiLSTM-Attention branch for radar motion dynamics with a multi-scale CNN-SEBlock-Self-Attention branch for vibration impact detection", "result": "we propose a Privacy-Preserving Multimodal Fall Detection System for Elderly People in Bathroom Environments.  First, we develop a sensor evaluation framework to select and fuse millimeter-wave radar with 3D vibration sensing, and use it to construct and preprocess a large-scale, privacy-preserving multimodal dataset in real bathroom settings, which will be released upon publication. Second, we introduce P2MFDS", "conclusion": "P2MFDS delivers significant gains in accuracy and recall over state-of-the-art approaches."}}
{"id": "2506.17296", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17296", "abs": "https://arxiv.org/abs/2506.17296", "authors": ["Darius Foodeei", "Simin Fan", "Martin Jaggi"], "title": "Semantic uncertainty in advanced decoding methods for LLM generation", "comment": null, "summary": "This study investigates semantic uncertainty in large language model (LLM)\noutputs across different decoding methods, focusing on emerging techniques like\nspeculative sampling and chain-of-thought (CoT) decoding. Through experiments\non question answering, summarization, and code generation tasks, we analyze how\ndifferent decoding strategies affect both the diversity and reliability of\nmodel outputs. Our findings reveal that while CoT decoding demonstrates higher\nsemantic diversity, it maintains lower predictive entropy, suggesting that\nstructured exploration can lead to more confident and accurate outputs. This is\nevidenced by a 48.8% improvement in code generation Pass@2 rates, despite lower\nalignment with reference solutions. For summarization tasks, speculative\nsampling proved particularly effective, achieving superior ROUGE scores while\nmaintaining moderate semantic diversity. Our results challenge conventional\nassumptions about trade-offs between diversity and accuracy in language model\noutputs, demonstrating that properly structured decoding methods can increase\nsemantic exploration while maintaining or improving output quality. These\nfindings have significant implications for deploying language models in\npractical applications where both reliability and diverse solution generation\nare crucial.", "AI": {"tldr": "This paper analyzes how different decoding strategies affect LLM output diversity and reliability, finding that structured methods like CoT and speculative sampling can improve both.", "motivation": "Investigates semantic uncertainty in large language model (LLM) outputs across different decoding methods, focusing on emerging techniques like speculative sampling and chain-of-thought (CoT) decoding.", "method": "Experiments on question answering, summarization, and code generation tasks to analyze how different decoding strategies affect both the diversity and reliability of model outputs.", "result": "CoT decoding demonstrates higher semantic diversity and lower predictive entropy, with a 48.8% improvement in code generation. Speculative sampling proved particularly effective for summarization tasks, achieving superior ROUGE scores while maintaining moderate semantic diversity.", "conclusion": "Properly structured decoding methods can increase semantic exploration while maintaining or improving output quality."}}
{"id": "2506.17449", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17449", "abs": "https://arxiv.org/abs/2506.17449", "authors": ["Manasa Bharadwaj", "Nikhil Verma", "Kevin Ferreira"], "title": "OmniReflect: Discovering Transferable Constitutions for LLM agents via Neuro-Symbolic Reflections", "comment": null, "summary": "Efforts to improve Large Language Model (LLM) agent performance on complex\ntasks have largely focused on fine-tuning and iterative self-correction.\nHowever, these approaches often lack generalizable mechanisms for longterm\nlearning and remain inefficient in dynamic environments. We introduce\nOmniReflect, a hierarchical, reflection-driven framework that constructs a\nconstitution, a compact set of guiding principles distilled from task\nexperiences, to enhance the effectiveness and efficiency of an LLM agent.\nOmniReflect operates in two modes: Self-sustaining, where a single agent\nperiodically curates its own reflections during task execution, and\nCo-operative, where a Meta-advisor derives a constitution from a small\ncalibration set to guide another agent. To construct these constitutional\nprinciples, we employ Neural, Symbolic, and NeuroSymbolic techniques, offering\na balance between contextual adaptability and computational efficiency.\nEmpirical results averaged across models show major improvements in task\nsuccess, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3%\non PDDL in the Self-sustaining mode. Similar gains are seen in the Co-operative\nmode, where a lightweight Qwen3-4B ReAct agent outperforms all Reflexion\nbaselines on BabyAI. These findings highlight the robustness and effectiveness\nof OmniReflect across environments and backbones.", "AI": {"tldr": "OmniReflect, a hierarchical reflection framework, improves LLM agent performance on complex tasks by constructing a constitution from task experiences, showing significant gains in task success across various environments.", "motivation": "Efforts to improve Large Language Model (LLM) agent performance on complex tasks often lack generalizable mechanisms for long-term learning and remain inefficient in dynamic environments.", "method": "A hierarchical, reflection-driven framework that constructs a constitution, a compact set of guiding principles distilled from task experiences, to enhance the effectiveness and efficiency of an LLM agent. It operates in two modes: Self-sustaining and Co-operative, employing Neural, Symbolic, and NeuroSymbolic techniques.", "result": "Major improvements in task success, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3% on PDDL in the Self-sustaining mode. Similar gains are seen in the Co-operative mode.", "conclusion": "OmniReflect demonstrates robustness and effectiveness across environments and backbones, leading to significant improvements in task success."}}
{"id": "2506.17247", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17247", "abs": "https://arxiv.org/abs/2506.17247", "authors": ["Andrew B. Kahng", "Yiting Liu", "Zhiang Wang"], "title": "Recursive Learning-Based Virtual Buffering for Analytical Global Placement", "comment": null, "summary": "Due to the skewed scaling of interconnect versus cell delay in modern\ntechnology nodes, placement with buffer porosity (i.e., cell density) awareness\nis essential for timing closure in physical synthesis flows. However, existing\napproaches face two key challenges: (i) traditional van Ginneken-Lillis-style\nbuffering approaches are computationally expensive during global placement; and\n(ii) machine learning-based approaches, such as BufFormer, lack a thorough\nconsideration of Electrical Rule Check (ERC) violations and fail to \"close the\nloop\" back into the physical design flow. In this work, we propose\nMLBuf-RePlAce, the first open-source learning-driven virtual buffering-aware\nanalytical global placement framework, built on top of the OpenROAD\ninfrastructure. MLBuf-RePlAce adopts an efficient recursive learning-based\ngenerative buffering approach to predict buffer types and locations, addressing\nERC violations during global placement. We compare MLBuf-RePlAce against the\ndefault virtual buffering-based timing-driven global placer in OpenROAD, using\nopen-source testcases from the TILOS MacroPlacement and OpenROAD-flow-scripts\nrepositories. Without degradation of post-route power, MLBuf-RePlAce achieves\n(maximum, average) improvements of (56%, 31%) in total negative slack (TNS)\nwithin the open-source OpenROAD flow. When evaluated by completion in a\ncommercial flow, MLBuf-RePlAce achieves (maximum, average) improvements of\n(53%, 28%) in TNS with an average of 0.2% improvement in post-route power.", "AI": {"tldr": "MLBuf-RePlAce, a learning-driven virtual buffering-aware analytical global placement framework, improves timing closure and addresses ERC violations with significant TNS improvements.", "motivation": "Placement with buffer porosity awareness is essential for timing closure, but existing approaches are computationally expensive or lack consideration of Electrical Rule Check (ERC) violations.", "method": "An efficient recursive learning-based generative buffering approach to predict buffer types and locations, addressing ERC violations during global placement.", "result": "MLBuf-RePlAce achieves improvements of up to 56% in TNS within the open-source OpenROAD flow and up to 53% in TNS in a commercial flow.", "conclusion": "MLBuf-RePlAce achieves significant improvements in total negative slack (TNS) without degrading post-route power, and also improves TNS in a commercial flow with a slight improvement in post-route power."}}
{"id": "2506.18772", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2506.18772", "abs": "https://arxiv.org/abs/2506.18772", "authors": ["Hassan S. Al Khatib", "Subash Neupane", "Sudip Mittal", "Shahram Rahimi", "Nina Marhamati", "Sean Bozorgzad"], "title": "Patient Journey Ontology: Representing Medical Encounters for Enhanced Patient-Centric Applications", "comment": null, "summary": "The healthcare industry is moving towards a patient-centric paradigm that\nrequires advanced methods for managing and representing patient data. This\npaper presents a Patient Journey Ontology (PJO), a framework that aims to\ncapture the entirety of a patient's healthcare encounters. Utilizing\nontologies, the PJO integrates different patient data sources like medical\nhistories, diagnoses, treatment pathways, and outcomes; it enables semantic\ninteroperability and enhances clinical reasoning. By capturing temporal,\nsequential, and causal relationships between medical encounters, the PJO\nsupports predictive analytics, enabling earlier interventions and optimized\ntreatment plans. The ontology's structure, including its main classes,\nsubclasses, properties, and relationships, as detailed in the paper,\ndemonstrates its ability to provide a holistic view of patient care.\nQuantitative and qualitative evaluations by Subject Matter Experts (SMEs)\ndemonstrate strong capabilities in patient history retrieval, symptom tracking,\nand provider interaction representation, while identifying opportunities for\nenhanced diagnosis-symptom linking. These evaluations reveal the PJO's\nreliability and practical applicability, demonstrating its potential to enhance\npatient outcomes and healthcare efficiency. This work contributes to the\nongoing efforts of knowledge representation in healthcare, offering a reliable\ntool for personalized medicine, patient journey analysis and advancing the\ncapabilities of Generative AI in healthcare applications.", "AI": {"tldr": "Developed a Patient Journey Ontology (PJO) to capture patient healthcare encounters, enabling semantic interoperability, predictive analytics, and personalized medicine.", "motivation": "The healthcare industry is moving towards a patient-centric paradigm that requires advanced methods for managing and representing patient data.", "method": "This paper presents a Patient Journey Ontology (PJO), a framework that captures a patient's healthcare encounters by integrating different patient data sources.", "result": "Evaluations demonstrate strong capabilities in patient history retrieval, symptom tracking, and provider interaction representation, with opportunities for enhanced diagnosis-symptom linking. The PJO's reliability and practical applicability are also revealed.", "conclusion": "The Patient Journey Ontology (PJO) demonstrates potential to enhance patient outcomes, healthcare efficiency, and personalized medicine, offering a tool for patient journey analysis and advancing Generative AI in healthcare."}}
{"id": "2506.17580", "categories": ["cs.IR", "cs.AI", "cs.DL", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.17580", "abs": "https://arxiv.org/abs/2506.17580", "authors": ["Sajratul Y. Rubaiat", "Hasan M. Jamil"], "title": "Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models", "comment": null, "summary": "The exponential growth of scientific literature challenges researchers\nextracting and synthesizing knowledge. Traditional search engines return many\nsources without direct, detailed answers, while general-purpose LLMs may offer\nconcise responses that lack depth or omit current information. LLMs with search\ncapabilities are also limited by context window, yielding short, incomplete\nanswers. This paper introduces WISE (Workflow for Intelligent Scientific\nKnowledge Extraction), a system addressing these limits by using a structured\nworkflow to extract, refine, and rank query-specific knowledge. WISE uses an\nLLM-powered, tree-based architecture to refine data, focusing on query-aligned,\ncontext-aware, and non-redundant information. Dynamic scoring and ranking\nprioritize unique contributions from each source, and adaptive stopping\ncriteria minimize processing overhead. WISE delivers detailed, organized\nanswers by systematically exploring and synthesizing knowledge from diverse\nsources. Experiments on HBB gene-associated diseases demonstrate WISE reduces\nprocessed text by over 80% while achieving significantly higher recall over\nbaselines like search engines and other LLM-based approaches. ROUGE and BLEU\nmetrics reveal WISE's output is more unique than other systems, and a novel\nlevel-based metric shows it provides more in-depth information. We also explore\nhow the WISE workflow can be adapted for diverse domains like drug discovery,\nmaterial science, and social science, enabling efficient knowledge extraction\nand synthesis from unstructured scientific papers and web sources.", "AI": {"tldr": "WISE is a system that uses a structured workflow to extract, refine, and rank query-specific knowledge. It reduces processed text by over 80% while achieving significantly higher recall over baselines.", "motivation": "The exponential growth of scientific literature challenges researchers extracting and synthesizing knowledge. Traditional search engines return many sources without direct, detailed answers, while general-purpose LLMs may offer concise responses that lack depth or omit current information. LLMs with search capabilities are also limited by context window, yielding short, incomplete answers.", "method": "WISE uses an LLM-powered, tree-based architecture to refine data, focusing on query-aligned, context-aware, and non-redundant information. Dynamic scoring and ranking prioritize unique contributions from each source, and adaptive stopping criteria minimize processing overhead.", "result": "WISE reduces processed text by over 80% while achieving significantly higher recall over baselines like search engines and other LLM-based approaches. ROUGE and BLEU metrics reveal WISE's output is more unique than other systems, and a novel level-based metric shows it provides more in-depth information.", "conclusion": "WISE delivers detailed, organized answers by systematically exploring and synthesizing knowledge from diverse sources. Experiments on HBB gene-associated diseases demonstrate WISE reduces processed text by over 80% while achieving significantly higher recall over baselines like search engines and other LLM-based approaches. ROUGE and BLEU metrics reveal WISE's output is more unique than other systems, and a novel level-based metric shows it provides more in-depth information. We also explore how the WISE workflow can be adapted for diverse domains like drug discovery, material science, and social science, enabling efficient knowledge extraction and synthesis from unstructured scientific papers and web sources."}}
{"id": "2506.17346", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17346", "abs": "https://arxiv.org/abs/2506.17346", "authors": ["Yuhan Zhou", "Haihua Chen", "Kewei Sha"], "title": "A Novel Multi-layer Task-centric and Data Quality Framework for Autonomous Driving", "comment": null, "summary": "The next-generation autonomous vehicles (AVs), embedded with frequent\nreal-time decision-making, will rely heavily on a large volume of multisource\nand multimodal data. In real-world settings, the data quality (DQ) of different\nsources and modalities usually varies due to unexpected environmental factors\nor sensor issues. However, both researchers and practitioners in the AV field\noverwhelmingly concentrate on models/algorithms while undervaluing the DQ. To\nfulfill the needs of the next-generation AVs with guarantees of functionality,\nefficiency, and trustworthiness, this paper proposes a novel task-centric and\ndata quality vase framework which consists of five layers: data layer, DQ\nlayer, task layer, application layer, and goal layer. The proposed framework\naims to map DQ with task requirements and performance goals. To illustrate, a\ncase study investigating redundancy on the nuScenes dataset proves that\npartially removing redundancy on multisource image data could improve YOLOv8\nobject detection task performance. Analysis on multimodal data of image and\nLiDAR further presents existing redundancy DQ issues. This paper opens up a\nrange of critical but unexplored challenges at the intersection of DQ, task\norchestration, and performance-oriented system development in AVs. It is\nexpected to guide the AV community toward building more adaptive, explainable,\nand resilient AVs that respond intelligently to dynamic environments and\nheterogeneous data streams. Code, data, and implementation details are publicly\navailable at: https://anonymous.4open.science/r/dq4av-framework/README.md.", "AI": {"tldr": "This paper introduces a task-centric data quality framework for autonomous vehicles, demonstrating that addressing data redundancy can improve object detection performance.", "motivation": "Next-generation autonomous vehicles (AVs) will rely heavily on a large volume of multisource and multimodal data. In real-world settings, the data quality (DQ) of different sources and modalities usually varies. However, both researchers and practitioners in the AV field overwhelmingly concentrate on models/algorithms while undervaluing the DQ.", "method": "This paper proposes a novel task-centric and data quality vase framework which consists of five layers: data layer, DQ layer, task layer, application layer, and goal layer. The proposed framework aims to map DQ with task requirements and performance goals.", "result": "A case study investigating redundancy on the nuScenes dataset proves that partially removing redundancy on multisource image data could improve YOLOv8 object detection task performance. Analysis on multimodal data of image and LiDAR further presents existing redundancy DQ issues.", "conclusion": "This paper opens up a range of critical but unexplored challenges at the intersection of DQ, task orchestration, and performance-oriented system development in AVs. It is expected to guide the AV community toward building more adaptive, explainable, and resilient AVs that respond intelligently to dynamic environments and heterogeneous data streams."}}
{"id": "2506.17298", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17298", "abs": "https://arxiv.org/abs/2506.17298", "authors": ["Inception Labs", "Samar Khanna", "Siddhant Kharbanda", "Shufan Li", "Harshit Varma", "Eric Wang", "Sawyer Birnbaum", "Ziyang Luo", "Yanis Miraoui", "Akash Palrecha", "Stefano Ermon", "Aditya Grover", "Volodymyr Kuleshov"], "title": "Mercury: Ultra-Fast Language Models Based on Diffusion", "comment": "15 pages; equal core, cross-function, senior authors listed\n  alphabetically", "summary": "We present Mercury, a new generation of commercial-scale large language\nmodels (LLMs) based on diffusion. These models are parameterized via the\nTransformer architecture and trained to predict multiple tokens in parallel. In\nthis report, we detail Mercury Coder, our first set of diffusion LLMs designed\nfor coding applications. Currently, Mercury Coder comes in two sizes: Mini and\nSmall. These models set a new state-of-the-art on the speed-quality frontier.\nBased on independent evaluations conducted by Artificial Analysis, Mercury\nCoder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109\ntokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform\nspeed-optimized frontier models by up to 10x on average while maintaining\ncomparable quality. We discuss additional results on a variety of code\nbenchmarks spanning multiple languages and use-cases as well as real-world\nvalidation by developers on Copilot Arena, where the model currently ranks\nsecond on quality and is the fastest model overall. We also release a public\nAPI at https://platform.inceptionlabs.ai/ and free playground at\nhttps://chat.inceptionlabs.ai", "AI": {"tldr": "Mercury Coder, diffusion LLMs for coding, achieves state-of-the-art speed and quality, outperforming existing models and ranking highly on Copilot Arena. Public API and playground are available.", "motivation": "Introducing Mercury Coder, a new generation of commercial-scale large language models (LLMs) based on diffusion, designed for coding applications.", "method": "The models are diffusion LLMs based on the Transformer architecture and trained to predict multiple tokens in parallel.", "result": "Mercury Coder Mini achieves 1109 tokens/sec and Mercury Coder Small achieves 737 tokens/sec on NVIDIA H100 GPUs, outperforming speed-optimized frontier models by up to 10x on average while maintaining comparable quality.", "conclusion": "Mercury Coder Mini and Mercury Coder Small are state-of-the-art regarding speed and quality, achieving high throughputs on NVIDIA H100 GPUs and outperforming other models while maintaining quality. The model ranks second in quality and is the fastest overall on Copilot Arena. A public API and free playground are available."}}
{"id": "2506.17484", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17484", "abs": "https://arxiv.org/abs/2506.17484", "authors": ["Yao Zhang", "Zaixi Shang", "Silpan Patel", "Mikel Zuniga"], "title": "From Unstructured Communication to Intelligent RAG: Multi-Agent Automation for Supply Chain Knowledge Bases", "comment": "Accepted In Proceedings of the 1st Workshop on AI for Supply Chain:\n  Today and Future @ 31st ACM SIGKDD Conference on Knowledge Discovery and Data\n  Mining V.2 (KDD 25), August 3, 2025, Toronto, ON, Canada. ACM, New York, NY,\n  USA, 14 pages, 2 figures", "summary": "Supply chain operations generate vast amounts of operational data; however,\ncritical knowledge such as system usage practices, troubleshooting workflows,\nand resolution techniques often remains buried within unstructured\ncommunications like support tickets, emails, and chat logs. While RAG systems\naim to leverage such communications as a knowledge base, their effectiveness is\nlimited by raw data challenges: support tickets are typically noisy,\ninconsistent, and incomplete, making direct retrieval suboptimal. Unlike\nexisting RAG approaches that focus on runtime optimization, we introduce a\nnovel offline-first methodology that transforms these communications into a\nstructured knowledge base. Our key innovation is a LLMs-based multi-agent\nsystem orchestrating three specialized agents: Category Discovery for taxonomy\ncreation, Categorization for ticket grouping, and Knowledge Synthesis for\narticle generation. Applying our methodology to real-world support tickets with\nresolution notes and comments, our system creates a compact knowledge base -\nreducing total volume to just 3.4% of original ticket data while improving\nquality. Experiments demonstrate that our prebuilt knowledge base in RAG\nsystems significantly outperforms traditional RAG implementations (48.74% vs.\n38.60% helpful answers) and achieves a 77.4% reduction in unhelpful responses.\nBy automating institutional knowledge capture that typically remains siloed in\nexperts' heads, our solution translates to substantial operational efficiency:\nreducing support workload, accelerating resolution times, and creating\nself-improving systems that automatically resolve approximately 50% of future\nsupply chain tickets. Our approach addresses a key gap in knowledge management\nby transforming transient communications into structured, reusable knowledge\nthrough intelligent offline processing rather than latency-inducing runtime\narchitectures.", "AI": {"tldr": "This paper introduces a novel offline-first methodology that transforms unstructured communications into a structured knowledge base, which reduces total volume to just 3.4% of original ticket data while improving quality.", "motivation": "Critical knowledge such as system usage practices, troubleshooting workflows, and resolution techniques often remains buried within unstructured communications like support tickets, emails, and chat logs. While RAG systems aim to leverage such communications as a knowledge base, their effectiveness is limited by raw data challenges: support tickets are typically noisy, inconsistent, and incomplete, making direct retrieval suboptimal.", "method": "LLMs-based multi-agent system orchestrating three specialized agents: Category Discovery for taxonomy creation, Categorization for ticket grouping, and Knowledge Synthesis for article generation.", "result": "Our system creates a compact knowledge base - reducing total volume to just 3.4% of original ticket data while improving quality. Experiments demonstrate that our prebuilt knowledge base in RAG systems significantly outperforms traditional RAG implementations (48.74% vs. 38.60% helpful answers) and achieves a 77.4% reduction in unhelpful responses.", "conclusion": "The system translates to substantial operational efficiency: reducing support workload, accelerating resolution times, and creating self-improving systems that automatically resolve approximately 50% of future supply chain tickets."}}
{"id": "2506.17248", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.17248", "abs": "https://arxiv.org/abs/2506.17248", "authors": ["Zequn Yang", "Hongfa Wang", "Di Hu"], "title": "Efficient Quantification of Multimodal Interaction at Sample Level", "comment": "Accepted to ICML 2025", "summary": "Interactions between modalities -- redundancy, uniqueness, and synergy --\ncollectively determine the composition of multimodal information. Understanding\nthese interactions is crucial for analyzing information dynamics in multimodal\nsystems, yet their accurate sample-level quantification presents significant\ntheoretical and computational challenges. To address this, we introduce the\nLightweight Sample-wise Multimodal Interaction (LSMI) estimator, rigorously\ngrounded in pointwise information theory. We first develop a redundancy\nestimation framework, employing an appropriate pointwise information measure to\nquantify this most decomposable and measurable interaction. Building upon this,\nwe propose a general interaction estimation method that employs efficient\nentropy estimation, specifically tailored for sample-wise estimation in\ncontinuous distributions. Extensive experiments on synthetic and real-world\ndatasets validate LSMI's precision and efficiency. Crucially, our sample-wise\napproach reveals fine-grained sample- and category-level dynamics within\nmultimodal data, enabling practical applications such as redundancy-informed\nsample partitioning, targeted knowledge distillation, and interaction-aware\nmodel ensembling. The code is available at\nhttps://github.com/GeWu-Lab/LSMI_Estimator.", "AI": {"tldr": "introduce the Lightweight Sample-wise Multimodal Interaction (LSMI) estimator, rigorously grounded in pointwise information theory, validated LSMI's precision and efficiency on synthetic and real-world datasets, reveals fine-grained sample- and category-level dynamics within multimodal data, enabling practical applications such as redundancy-informed sample partitioning, targeted knowledge distillation, and interaction-aware model ensembling.", "motivation": "Understanding interactions between modalities is crucial for analyzing information dynamics in multimodal systems, yet their accurate sample-level quantification presents significant theoretical and computational challenges.", "method": "introduce the Lightweight Sample-wise Multimodal Interaction (LSMI) estimator, rigorously grounded in pointwise information theory. develop a redundancy estimation framework, employing an appropriate pointwise information measure to quantify this most decomposable and measurable interaction. propose a general interaction estimation method that employs efficient entropy estimation, specifically tailored for sample-wise estimation in continuous distributions.", "result": "Extensive experiments on synthetic and real-world datasets validate LSMI's precision and efficiency.", "conclusion": "The sample-wise approach reveals fine-grained sample- and category-level dynamics within multimodal data, enabling practical applications such as redundancy-informed sample partitioning, targeted knowledge distillation, and interaction-aware model ensembling."}}
{"id": "2506.18842", "categories": ["cs.DB", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18842", "abs": "https://arxiv.org/abs/2506.18842", "authors": ["Patrick Beukema", "Henry Herzog", "Yawen Zhang", "Hunter Pitelka", "Favyen Bastani"], "title": "LIGHTHOUSE: Fast and precise distance to shoreline calculations from anywhere on earth", "comment": "8 pages, 7 figures, 1 table, ICML 2025 ML4RS", "summary": "We introduce a new dataset and algorithm for fast and efficient coastal\ndistance calculations from Anywhere on Earth (AoE). Existing global coastal\ndatasets are only available at coarse resolution (e.g. 1-4 km) which limits\ntheir utility. Publicly available satellite imagery combined with computer\nvision enable much higher precision. We provide a global coastline dataset at\n10 meter resolution, a 100+ fold improvement in precision over existing data.\nTo handle the computational challenge of querying at such an increased scale,\nwe introduce a new library: Layered Iterative Geospatial Hierarchical\nTerrain-Oriented Unified Search Engine (Lighthouse). Lighthouse is both\nexceptionally fast and resource-efficient, requiring only 1 CPU and 2 GB of RAM\nto achieve millisecond online inference, making it well suited for real-time\napplications in resource-constrained environments.", "AI": {"tldr": "This paper introduces a new dataset and algorithm for fast and efficient coastal distance calculations from Anywhere on Earth (AoE).", "motivation": "Existing global coastal datasets are only available at coarse resolution (e.g. 1-4 km) which limits their utility. Publicly available satellite imagery combined with computer vision enable much higher precision.", "method": "a new library: Layered Iterative Geospatial Hierarchical Terrain-Oriented Unified Search Engine (Lighthouse)", "result": "a global coastline dataset at 10 meter resolution, a 100+ fold improvement in precision over existing data.", "conclusion": "We introduce a new library: Layered Iterative Geospatial Hierarchical Terrain-Oriented Unified Search Engine (Lighthouse). Lighthouse is both exceptionally fast and resource-efficient, requiring only 1 CPU and 2 GB of RAM to achieve millisecond online inference, making it well suited for real-time applications in resource-constrained environments."}}
{"id": "2506.17600", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2506.17600", "abs": "https://arxiv.org/abs/2506.17600", "authors": ["Huiguang Zhang", "Baoguo Liu", "Wei Feng", "Zongtang Li"], "title": "A novel fast short-time root music method for vibration monitoring of high-speed spindles", "comment": null, "summary": "Ultra-high-speed spindle bearings challenge traditional vibration monitoring\ndue to broadband noise, non-stationarity, and limited time-frequency\nresolution. We present a fast Short-Time Root-MUSIC (fSTrM) algorithm that\nexploits\n  FFT-accelerated Lanczos bidiagonalization to reduce computational complexity\nfrom $\\mathcal{O}(N^3)$ to $SN\\log_2N+S^2(N+S)+M^2(N+M)$\n  while preserving parametric super-resolution. The method constructs Hankel\nmatrices from 16 ms signal frames and extracts fault frequencies through\npolynomial rooting on the unit circle. Experimental validation on the\nPolitecnico di Torino bearing dataset demonstrates breakthrough micro-defect\ndetection capabilities. The algorithm reliably identifies 150 $\\mu$m defects --\npreviously undetectable by conventional methods -- providing 72+ hours\nadditional warning time. Compared to STFT and wavelet methods, fSTrM achieves\n1.2 Hz frequency resolution (vs. 12.5 Hz), 93\\% detection rate at $-$5 dB SNR,\nand quantifies defect severity through harmonic content analysis. Critically,\nthe algorithm processes each frame in 2.4 ms on embedded ARM Cortex-M7\nhardware, enabling real-time deployment. This advancement transforms bearing\nmonitoring from failure prevention to continuous degradation assessment,\nestablishing a new paradigm for predictive maintenance in aerospace and\nprecision machining.", "AI": {"tldr": "Presents a fast Short-Time Root-MUSIC (fSTrM) algorithm that achieves real-time micro-defect detection capabilities.", "motivation": "Ultra-high-speed spindle bearings challenge traditional vibration monitoring due to broadband noise, non-stationarity, and limited time-frequency resolution.", "method": "We present a fast Short-Time Root-MUSIC (fSTrM) algorithm that exploits FFT-accelerated Lanczos bidiagonalization", "result": "The algorithm reliably identifies 150 $\\mu$m defects -- previously undetectable by conventional methods -- providing 72+ hours additional warning time. Compared to STFT and wavelet methods, fSTrM achieves 1.2 Hz frequency resolution (vs. 12.5 Hz), 93\\% detection rate at $-$5 dB SNR", "conclusion": "This advancement transforms bearing monitoring from failure prevention to continuous degradation assessment, establishing a new paradigm for predictive maintenance in aerospace and precision machining."}}
{"id": "2506.17361", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17361", "abs": "https://arxiv.org/abs/2506.17361", "authors": ["Xufei Wang", "Mingjian Zhang", "Fei Ge", "Jinchen Zhu", "Wen Sha", "Jifen Ren", "Zhimeng Hou", "Shouguo Zheng", "ling Zheng", "Shizhuang Weng"], "title": "Efficient Feedback Gate Network for Hyperspectral Image Super-Resolution", "comment": "20 pages,17 figures", "summary": "Even without auxiliary images, single hyperspectral image super-resolution\n(SHSR) methods can be designed to improve the spatial resolution of\nhyperspectral images. However, failing to explore coherence thoroughly along\nbands and spatial-spectral information leads to the limited performance of the\nSHSR. In this study, we propose a novel group-based SHSR method termed the\nefficient feedback gate network, which uses various feedbacks and gate\noperations involving large kernel convolutions and spectral interactions. In\nparticular, by providing different guidance for neighboring groups, we can\nlearn rich band information and hierarchical hyperspectral spatial information\nusing channel shuffling and dilatation convolution in shuffled and progressive\ndilated fusion module(SPDFM). Moreover, we develop a wide-bound perception gate\nblock and a spectrum enhancement gate block to construct the spatial-spectral\nreinforcement gate module (SSRGM) and obtain highly representative\nspatial-spectral features efficiently. Additionally, we apply a\nthree-dimensional SSRGM to enhance holistic information and coherence for\nhyperspectral data. The experimental results on three hyperspectral datasets\ndemonstrate the superior performance of the proposed network over the\nstate-of-the-art methods in terms of spectral fidelity and spatial content\nreconstruction.", "AI": {"tldr": "This paper proposes a novel group-based SHSR method termed the efficient feedback gate network, which uses various feedbacks and gate operations involving large kernel convolutions and spectral interactions to improve the spatial resolution of hyperspectral images.", "motivation": "single hyperspectral image super-resolution (SHSR) methods can be designed to improve the spatial resolution of hyperspectral images. However, failing to explore coherence thoroughly along bands and spatial-spectral information leads to the limited performance of the SHSR.", "method": "a novel group-based SHSR method termed the efficient feedback gate network, which uses various feedbacks and gate operations involving large kernel convolutions and spectral interactions. In particular, by providing different guidance for neighboring groups, we can learn rich band information and hierarchical hyperspectral spatial information using channel shuffling and dilatation convolution in shuffled and progressive dilated fusion module(SPDFM). Moreover, we develop a wide-bound perception gate block and a spectrum enhancement gate block to construct the spatial-spectral reinforcement gate module (SSRGM) and obtain highly representative spatial-spectral features efficiently. Additionally, we apply a three-dimensional SSRGM to enhance holistic information and coherence for hyperspectral data.", "result": "the superior performance of the proposed network over the state-of-the-art methods in terms of spectral fidelity and spatial content reconstruction.", "conclusion": "The experimental results on three hyperspectral datasets demonstrate the superior performance of the proposed network over the state-of-the-art methods in terms of spectral fidelity and spatial content reconstruction."}}
{"id": "2506.17314", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.17314", "abs": "https://arxiv.org/abs/2506.17314", "authors": ["Adnan Qidwai", "Srija Mukhopadhyay", "Prerana Khatiwada", "Dan Roth", "Vivek Gupta"], "title": "PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights", "comment": "9 Pages, 9 Figures. Accepted at ACL 2025 System Demonstration Track", "summary": "Accurate and complete product descriptions are crucial for e-commerce, yet\nseller-provided information often falls short. Customer reviews offer valuable\ndetails but are laborious to sift through manually. We present PRAISE: Product\nReview Attribute Insight Structuring Engine, a novel system that uses Large\nLanguage Models (LLMs) to automatically extract, compare, and structure\ninsights from customer reviews and seller descriptions. PRAISE provides users\nwith an intuitive interface to identify missing, contradictory, or partially\nmatching details between these two sources, presenting the discrepancies in a\nclear, structured format alongside supporting evidence from reviews. This\nallows sellers to easily enhance their product listings for clarity and\npersuasiveness, and buyers to better assess product reliability. Our\ndemonstration showcases PRAISE's workflow, its effectiveness in generating\nactionable structured insights from unstructured reviews, and its potential to\nsignificantly improve the quality and trustworthiness of e-commerce product\ncatalogs.", "AI": {"tldr": "PRAISE uses LLMs to improve product descriptions by structuring customer review insights, helping sellers and buyers.", "motivation": "Seller-provided product information is often incomplete, and customer reviews are difficult to sift through manually.", "method": "PRAISE uses LLMs to extract, compare, and structure insights from customer reviews and seller descriptions.", "result": "PRAISE effectively generates actionable structured insights from unstructured reviews, identifies discrepancies between reviews and descriptions, and presents them in a clear format.", "conclusion": "PRAISE improves e-commerce product catalogs by enhancing clarity and trustworthiness."}}
{"id": "2506.17514", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17514", "abs": "https://arxiv.org/abs/2506.17514", "authors": ["Ninareh Mehrabi", "Tharindu Kumarage", "Kai-Wei Chang", "Aram Galstyan", "Rahul Gupta"], "title": "Kaleidoscopic Teaming in Multi Agent Simulations", "comment": null, "summary": "Warning: This paper contains content that may be inappropriate or offensive.\n  AI agents have gained significant recent attention due to their autonomous\ntool usage capabilities and their integration in various real-world\napplications. This autonomy poses novel challenges for the safety of such\nsystems, both in single- and multi-agent scenarios. We argue that existing red\nteaming or safety evaluation frameworks fall short in evaluating safety risks\nin complex behaviors, thought processes and actions taken by agents. Moreover,\nthey fail to consider risks in multi-agent setups where various vulnerabilities\ncan be exposed when agents engage in complex behaviors and interactions with\neach other. To address this shortcoming, we introduce the term kaleidoscopic\nteaming which seeks to capture complex and wide range of vulnerabilities that\ncan happen in agents both in single-agent and multi-agent scenarios. We also\npresent a new kaleidoscopic teaming framework that generates a diverse array of\nscenarios modeling real-world human societies. Our framework evaluates safety\nof agents in both single-agent and multi-agent setups. In single-agent setup,\nan agent is given a scenario that it needs to complete using the tools it has\naccess to. In multi-agent setup, multiple agents either compete against or\ncooperate together to complete a task in the scenario through which we capture\nexisting safety vulnerabilities in agents. We introduce new in-context\noptimization techniques that can be used in our kaleidoscopic teaming framework\nto generate better scenarios for safety analysis. Lastly, we present\nappropriate metrics that can be used along with our framework to measure safety\nof agents. Utilizing our kaleidoscopic teaming framework, we identify\nvulnerabilities in various models with respect to their safety in agentic\nuse-cases.", "AI": {"tldr": "This paper introduces a kaleidoscopic teaming framework to evaluate safety risks in AI agents, addressing the shortcomings of existing methods, and identifies vulnerabilities in various models.", "motivation": "Existing red teaming or safety evaluation frameworks fall short in evaluating safety risks in complex behaviors, thought processes and actions taken by agents, especially in multi-agent setups.", "method": "The paper presents a new kaleidoscopic teaming framework that generates a diverse array of scenarios modeling real-world human societies and introduces new in-context optimization techniques to generate better scenarios for safety analysis. Appropriate metrics are also presented to measure the safety of agents.", "result": "The paper evaluates the safety of agents in both single-agent and multi-agent setups and captures existing safety vulnerabilities in agents.", "conclusion": "The paper identifies vulnerabilities in various models with respect to their safety in agentic use-cases by utilizing the kaleidoscopic teaming framework."}}
{"id": "2506.17249", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17249", "abs": "https://arxiv.org/abs/2506.17249", "authors": ["Jianing He", "Qi Zhang", "Duoqian Miao", "Yi Kun", "Shufeng Hao", "Hongyun Zhang", "Zhihua Wei"], "title": "Improving Prediction Certainty Estimation for Reliable Early Exiting via Null Space Projection", "comment": "IJCAI 2025, 9 pages", "summary": "Early exiting has demonstrated great potential in accelerating the inference\nof pre-trained language models (PLMs) by enabling easy samples to exit at\nshallow layers, eliminating the need for executing deeper layers. However,\nexisting early exiting methods primarily rely on class-relevant logits to\nformulate their exiting signals for estimating prediction certainty, neglecting\nthe detrimental influence of class-irrelevant information in the features on\nprediction certainty. This leads to an overestimation of prediction certainty,\ncausing premature exiting of samples with incorrect early predictions. To\nremedy this, we define an NSP score to estimate prediction certainty by\nconsidering the proportion of class-irrelevant information in the features. On\nthis basis, we propose a novel early exiting method based on the\nCertainty-Aware Probability (CAP) score, which integrates insights from both\nlogits and the NSP score to enhance prediction certainty estimation, thus\nenabling more reliable exiting decisions. The experimental results on the GLUE\nbenchmark show that our method can achieve an average speed-up ratio of 2.19x\nacross all tasks with negligible performance degradation, surpassing the\nstate-of-the-art (SOTA) ConsistentEE by 28%, yielding a better trade-off\nbetween task performance and inference efficiency. The code is available at\nhttps://github.com/He-Jianing/NSP.git.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63d0\u524d\u9000\u51fa\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u8003\u8651\u4e0e\u7c7b\u522b\u65e0\u5173\u7684\u4fe1\u606f\u6765\u66f4\u51c6\u786e\u5730\u4f30\u8ba1\u9884\u6d4b\u7f6e\u4fe1\u5ea6\uff0c\u4ece\u800c\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u63d0\u524d\u9000\u51fa\u65b9\u6cd5\u4e3b\u8981\u4f9d\u9760\u4e0e\u7c7b\u522b\u76f8\u5173\u7684 logits \u6765\u5236\u5b9a\u5176\u9000\u51fa\u4fe1\u53f7\uff0c\u4ee5\u4f30\u8ba1\u9884\u6d4b\u7684\u786e\u5b9a\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u7279\u5f81\u4e2d\u4e0e\u7c7b\u522b\u65e0\u5173\u7684\u4fe1\u606f\u5bf9\u9884\u6d4b\u786e\u5b9a\u6027\u7684\u4e0d\u5229\u5f71\u54cd\u3002\u8fd9\u5bfc\u81f4\u5bf9\u9884\u6d4b\u786e\u5b9a\u6027\u7684\u9ad8\u4f30\uff0c\u5bfc\u81f4\u5177\u6709\u4e0d\u6b63\u786e\u7684\u65e9\u671f\u9884\u6d4b\u7684\u6837\u672c\u8fc7\u65e9\u9000\u51fa\u3002", "method": "\u8be5\u8bba\u6587\u5b9a\u4e49\u4e86\u4e00\u4e2a NSP \u5206\u6570\u6765\u4f30\u8ba1\u9884\u6d4b\u7f6e\u4fe1\u5ea6\uff0c\u8be5\u5206\u6570\u8003\u8651\u4e86\u7279\u5f81\u4e2d\u4e0e\u7c7b\u522b\u65e0\u5173\u7684\u4fe1\u606f\u7684\u6bd4\u4f8b\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u611f\u77e5\u6982\u7387 (CAP) \u5206\u6570\u7684\u65b0\u578b\u63d0\u524d\u9000\u51fa\u65b9\u6cd5\u3002", "result": "\u5728 GLUE \u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u5b9e\u73b0 2.19 \u500d\u7684\u5e73\u5747\u52a0\u901f\u6bd4\uff0c\u800c\u6027\u80fd\u4e0b\u964d\u53ef\u5ffd\u7565\u4e0d\u8ba1\uff0c\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684 (SOTA) ConsistentEE 28%\uff0c\u4ece\u800c\u5728\u4efb\u52a1\u6027\u80fd\u548c\u63a8\u7406\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6743\u8861\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u611f\u77e5\u6982\u7387 (CAP) \u5206\u6570\u7684\u65b0\u578b\u63d0\u524d\u9000\u51fa\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u96c6\u6210\u4e86 logits \u548c NSP \u5206\u6570\u7684\u89c1\u89e3\uff0c\u4ee5\u589e\u5f3a\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u9000\u51fa\u51b3\u7b56\u3002"}}
{"id": "2506.17977", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2506.17977", "abs": "https://arxiv.org/abs/2506.17977", "authors": ["Tingting Zhu", "Tingyang Chen", "Yinghui Wu", "Arijit Khan", "Xiangyu Ke"], "title": "SliceGX: Layer-wise GNN Explanation with Model-slicing", "comment": null, "summary": "Ensuring the trustworthiness of graph neural networks (GNNs) as black-box\nmodels requires effective explanation methods. Existing GNN explanations\ntypically apply input perturbations to identify subgraphs that are responsible\nfor the occurrence of the final output of GNNs. However, such approaches lack\nfiner-grained, layer-wise analysis of how intermediate representations\ncontribute to the final result, capabilities that are crucial for model\ndiagnosis and architecture optimization. This paper introduces SliceGX, a novel\nGNN explanation approach that generates explanations at specific GNN layers in\na progressive manner. Given a GNN M, a set of selected intermediate layers, and\na target layer, SliceGX automatically segments M into layer blocks (\"model\nslice\") and discovers high-quality explanatory subgraphs in each layer block\nthat clarifies the occurrence of output of M at the targeted layer. Although\nfinding such layer-wise explanations is computationally challenging, we develop\nefficient algorithms and optimization techniques that incrementally generate\nand maintain these subgraphs with provable approximation guarantees.\nAdditionally, SliceGX offers a SPARQL-like query interface, providing\ndeclarative access and search capacities for the generated explanations.\nThrough experiments on large real-world graphs and representative GNN\narchitectures, we verify the effectiveness and efficiency of SliceGX, and\nillustrate its practical utility in supporting model debugging.", "AI": {"tldr": "SliceGX: a novel GNN explanation approach that generates explanations at specific GNN layers in a progressive manner. It is effective and efficient, and it can support model debugging.", "motivation": "Existing GNN explanations lack finer-grained, layer-wise analysis of how intermediate representations contribute to the final result, capabilities that are crucial for model diagnosis and architecture optimization.", "method": "a novel GNN explanation approach that generates explanations at specific GNN layers in a progressive manner", "result": "SliceGX automatically segments M into layer blocks and discovers high-quality explanatory subgraphs in each layer block that clarifies the occurrence of output of M at the targeted layer. Additionally, SliceGX offers a SPARQL-like query interface, providing declarative access and search capacities for the generated explanations.", "conclusion": "SliceGX is effective and efficient, and it can support model debugging."}}
{"id": "2506.17682", "categories": ["cs.IR", "cs.AI", "68T07", "H.3.3"], "pdf": "https://arxiv.org/pdf/2506.17682", "abs": "https://arxiv.org/abs/2506.17682", "authors": ["Zhijian Feng", "Wenhao Zheng", "Xuanji Xiao"], "title": "Reinforcing User Interest Evolution in Multi-Scenario Learning for recommender systems", "comment": null, "summary": "In real-world recommendation systems, users would engage in variety\nscenarios, such as homepages, search pages, and related recommendation pages.\nEach of these scenarios would reflect different aspects users focus on.\nHowever, the user interests may be inconsistent in different scenarios, due to\ndifferences in decision-making processes and preference expression. This\nvariability complicates unified modeling, making multi-scenario learning a\nsignificant challenge. To address this, we propose a novel reinforcement\nlearning approach that models user preferences across scenarios by modeling\nuser interest evolution across multiple scenarios. Our method employs Double\nQ-learning to enhance next-item prediction accuracy and optimizes contrastive\nlearning loss using Q-value to make model performance better. Experimental\nresults demonstrate that our approach surpasses state-of-the-art methods in\nmulti-scenario recommendation tasks. Our work offers a fresh perspective on\nmulti-scenario modeling and highlights promising directions for future\nresearch.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u591a\u4e2a\u573a\u666f\u4e2d\u7684\u7528\u6237\u5174\u8da3\u6f14\u53d8\u8fdb\u884c\u5efa\u6a21\u6765\u5bf9\u8de8\u573a\u666f\u7684\u7528\u6237\u504f\u597d\u8fdb\u884c\u5efa\u6a21\uff0c\u4ece\u800c\u5728\u591a\u573a\u666f\u63a8\u8350\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u7684\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u7528\u6237\u4f1a\u53c2\u4e0e\u5404\u79cd\u573a\u666f\uff0c\u4f8b\u5982\u4e3b\u9875\u3001\u641c\u7d22\u9875\u9762\u548c\u76f8\u5173\u63a8\u8350\u9875\u9762\u3002\u8fd9\u4e9b\u573a\u666f\u4e2d\u7684\u6bcf\u4e00\u4e2a\u90fd\u4f1a\u53cd\u6620\u7528\u6237\u5173\u6ce8\u7684\u4e0d\u540c\u65b9\u9762\u3002\u7136\u800c\uff0c\u7531\u4e8e\u51b3\u7b56\u8fc7\u7a0b\u548c\u504f\u597d\u8868\u8fbe\u7684\u5dee\u5f02\uff0c\u7528\u6237\u5174\u8da3\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u53ef\u80fd\u4e0d\u4e00\u81f4\u3002\u8fd9\u79cd\u53ef\u53d8\u6027\u4f7f\u7edf\u4e00\u5efa\u6a21\u53d8\u5f97\u590d\u6742\uff0c\u4f7f\u591a\u573a\u666f\u5b66\u4e60\u6210\u4e3a\u4e00\u9879\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u591a\u4e2a\u573a\u666f\u4e2d\u7684\u7528\u6237\u5174\u8da3\u6f14\u53d8\u8fdb\u884c\u5efa\u6a21\u6765\u5bf9\u8de8\u573a\u666f\u7684\u7528\u6237\u504f\u597d\u8fdb\u884c\u5efa\u6a21\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\u53ccQ\u5b66\u4e60\u6765\u63d0\u9ad8\u4e0b\u4e00\u4e2a\u9879\u76ee\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u4f7f\u7528Q\u503c\u4f18\u5316\u5bf9\u6bd4\u5b66\u4e60\u635f\u5931\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u573a\u666f\u63a8\u8350\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u573a\u666f\u63a8\u8350\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u591a\u573a\u666f\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5e76\u7a81\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002"}}
{"id": "2506.17374", "categories": ["cs.CV", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.17374", "abs": "https://arxiv.org/abs/2506.17374", "authors": ["Muhammad Tayyab Khan", "Lequn Chen", "Zane Yong", "Jun Ming Tan", "Wenhe Feng", "Seung Ki Moon"], "title": "From Drawings to Decisions: A Hybrid Vision-Language Framework for Parsing 2D Engineering Drawings into Structured Manufacturing Knowledge", "comment": "Preprint submitted to Elsevier", "summary": "Efficient and accurate extraction of key information from 2D engineering\ndrawings is essential for advancing digital manufacturing workflows. Such\ninformation includes geometric dimensioning and tolerancing (GD&T), measures,\nmaterial specifications, and textual annotations. Manual extraction is slow and\nlabor-intensive, while generic OCR models often fail due to complex layouts,\nengineering symbols, and rotated text, leading to incomplete and unreliable\noutputs. These limitations result in incomplete and unreliable outputs. To\naddress these challenges, we propose a hybrid vision-language framework that\nintegrates a rotation-aware object detection model (YOLOv11-obb) with a\ntransformer-based vision-language parser. Our structured pipeline applies\nYOLOv11-OBB to localize annotations and extract oriented bounding box (OBB)\npatches, which are then parsed into structured outputs using a fine-tuned,\nlightweight vision-language model (VLM). We curate a dataset of 1,367 2D\nmechanical drawings annotated across nine key categories. YOLOv11-OBB is\ntrained on this dataset to detect OBBs and extract annotation patches. These\nare parsed using two open-source VLMs: Donut and Florence-2. Both models are\nlightweight and well-suited for specialized industrial tasks under limited\ncomputational overhead. Following fine-tuning of both models on the curated\ndataset of image patches paired with structured annotation labels, a\ncomparative experiment is conducted to evaluate parsing performance across four\nkey metrics. Donut outperforms Florence-2, achieving 88.5% precision, 99.2%\nrecall, and a 93.5% F1-score, with a hallucination rate of 11.5%. Finally, a\ncase study demonstrates how the extracted structured information supports\ndownstream manufacturing tasks such as process and tool selection, showcasing\nthe practical utility of the proposed framework in modernizing 2D drawing\ninterpretation.", "AI": {"tldr": "This paper introduces a hybrid vision-language framework for extracting key information from 2D engineering drawings. It uses YOLOv11-OBB for object detection and Donut/Florence-2 for parsing, with Donut showing superior performance.", "motivation": "Manual extraction is slow and labor-intensive, while generic OCR models often fail due to complex layouts, engineering symbols, and rotated text, leading to incomplete and unreliable outputs.", "method": "a hybrid vision-language framework that integrates a rotation-aware object detection model (YOLOv11-obb) with a transformer-based vision-language parser", "result": "YOLOv11-OBB is trained on this dataset to detect OBBs and extract annotation patches. These are parsed using two open-source VLMs: Donut and Florence-2.", "conclusion": "Donut outperforms Florence-2, achieving 88.5% precision, 99.2% recall, and a 93.5% F1-score, with a hallucination rate of 11.5%."}}
{"id": "2506.17352", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17352", "abs": "https://arxiv.org/abs/2506.17352", "authors": ["Tatsuhiro Aoshima", "Mitsuaki Akiyama"], "title": "Towards Safety Evaluations of Theory of Mind in Large Language Models", "comment": null, "summary": "As the capabilities of large language models (LLMs) continue to advance, the\nimportance of rigorous safety evaluation is becoming increasingly evident.\nRecent concerns within the realm of safety assessment have highlighted\ninstances in which LLMs exhibit behaviors that appear to disable oversight\nmechanisms and respond in a deceptive manner. For example, there have been\nreports suggesting that, when confronted with information unfavorable to their\nown persistence during task execution, LLMs may act covertly and even provide\nfalse answers to questions intended to verify their behavior.To evaluate the\npotential risk of such deceptive actions toward developers or users, it is\nessential to investigate whether these behaviors stem from covert, intentional\nprocesses within the model. In this study, we propose that it is necessary to\nmeasure the theory of mind capabilities of LLMs. We begin by reviewing existing\nresearch on theory of mind and identifying the perspectives and tasks relevant\nto its application in safety evaluation. Given that theory of mind has been\npredominantly studied within the context of developmental psychology, we\nanalyze developmental trends across a series of open-weight LLMs. Our results\nindicate that while LLMs have improved in reading comprehension, their theory\nof mind capabilities have not shown comparable development. Finally, we present\nthe current state of safety evaluation with respect to LLMs' theory of mind,\nand discuss remaining challenges for future work.", "AI": {"tldr": "This study investigates LLMs' theory of mind capabilities for safety evaluation, finding that while reading comprehension has improved, theory of mind has not kept pace.", "motivation": "evaluate the potential risk of deceptive actions toward developers or users", "method": "analyze developmental trends across a series of open-weight LLMs", "result": "LLMs have improved in reading comprehension, their theory of mind capabilities have not shown comparable development", "conclusion": "LLMs' theory of mind capabilities have not shown comparable development with reading comprehension."}}
{"id": "2506.17585", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17585", "abs": "https://arxiv.org/abs/2506.17585", "authors": ["Yukun Huang", "Sanxing Chen", "Jian Pei", "Manzil Zaheer", "Bhuwan Dhingra"], "title": "Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models", "comment": null, "summary": "Trustworthy language models should provide both correct and verifiable\nanswers. While language models can sometimes attribute their outputs to\npretraining data, their citations are often unreliable due to hallucination. As\na result, current systems insert citations by querying an external retriever at\ninference time, introducing latency, infrastructure dependence, and\nvulnerability to retrieval noise. We explore whether LLMs can be made to\nreliably attribute to the documents seen during (continual)\npretraining--without test-time retrieval--by revising the training process. To\nevaluate this, we release CitePretrainBench, a benchmark that mixes real-world\ncorpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and\nprobes both short-form (single fact) and long-form (multi-fact) citation tasks.\nOur approach follows a two-stage process: (1) continual pretraining to bind\nfacts to persistent document identifiers, and (2) instruction tuning to elicit\ncitation behavior. We find that simple Passive Indexing, which appends an\nidentifier to each document, helps memorize verbatim text but fails on\nparaphrased or compositional facts. Instead, we propose Active Indexing, which\ncontinually pretrains on synthetic QA pairs that (1) restate each fact in\ndiverse compositional forms, and (2) require bidirectional source-to-fact and\nfact-to-source generation, jointly teaching the model to generate content from\na cited source and to attribute its own answers. Experiments with Qwen2.5-7B\nand 3B show that Active Indexing consistently outperforms Passive Indexing\nacross all tasks and models, with citation precision gains up to 30.2 percent.\nOur ablation studies reveal that performance continues to improve as we scale\nthe amount of augmented data, showing a clear upward trend even at 16 times the\noriginal token count.", "AI": {"tldr": "This paper explores improving the reliability of citations in language models by revising the training process. They propose Active Indexing, which outperforms Passive Indexing in citation precision, achieving gains up to 30.2 percent.", "motivation": "Language models' citations are often unreliable due to hallucination. Current systems insert citations by querying an external retriever at inference time, introducing latency, infrastructure dependence, and vulnerability to retrieval noise. We explore whether LLMs can be made to reliably attribute to the documents seen during pretraining without test-time retrieval by revising the training process.", "method": "a two-stage process: (1) continual pretraining to bind facts to persistent document identifiers, and (2) instruction tuning to elicit citation behavior. We propose Active Indexing, which continually pretrains on synthetic QA pairs that restate each fact in diverse compositional forms and require bidirectional source-to-fact and fact-to-source generation.", "result": "Simple Passive Indexing helps memorize verbatim text but fails on paraphrased or compositional facts. Active Indexing consistently outperforms Passive Indexing across all tasks and models, with citation precision gains up to 30.2 percent. Performance continues to improve as we scale the amount of augmented data.", "conclusion": "Active Indexing consistently outperforms Passive Indexing across all tasks and models, with citation precision gains up to 30.2 percent. Performance continues to improve as we scale the amount of augmented data."}}
{"id": "2506.17250", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17250", "abs": "https://arxiv.org/abs/2506.17250", "authors": ["Fudong Lin", "Jiadong Lou", "Hao Wang", "Brian Jalaian", "Xu Yuan"], "title": "Towards Interpretable Adversarial Examples via Sparse Adversarial Attack", "comment": null, "summary": "Sparse attacks are to optimize the magnitude of adversarial perturbations for\nfooling deep neural networks (DNNs) involving only a few perturbed pixels\n(i.e., under the l0 constraint), suitable for interpreting the vulnerability of\nDNNs. However, existing solutions fail to yield interpretable adversarial\nexamples due to their poor sparsity. Worse still, they often struggle with\nheavy computational overhead, poor transferability, and weak attack strength.\nIn this paper, we aim to develop a sparse attack for understanding the\nvulnerability of CNNs by minimizing the magnitude of initial perturbations\nunder the l0 constraint, to overcome the existing drawbacks while achieving a\nfast, transferable, and strong attack to DNNs. In particular, a novel and\ntheoretical sound parameterization technique is introduced to approximate the\nNP-hard l0 optimization problem, making directly optimizing sparse\nperturbations computationally feasible. Besides, a novel loss function is\ndesigned to augment initial perturbations by maximizing the adversary property\nand minimizing the number of perturbed pixels simultaneously. Extensive\nexperiments are conducted to demonstrate that our approach, with theoretical\nperformance guarantees, outperforms state-of-the-art sparse attacks in terms of\ncomputational overhead, transferability, and attack strength, expecting to\nserve as a benchmark for evaluating the robustness of DNNs. In addition,\ntheoretical and empirical results validate that our approach yields sparser\nadversarial examples, empowering us to discover two categories of noises, i.e.,\n\"obscuring noise\" and \"leading noise\", which will help interpret how\nadversarial perturbation misleads the classifiers into incorrect predictions.\nOur code is available at https://github.com/fudong03/SparseAttack.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u53ef\u8fc1\u79fb\u4e14\u5f3a\u5927\u7684\u7a00\u758f\u653b\u51fb\u65b9\u6cd5\uff0c\u7528\u4e8e\u7406\u89e3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u7a00\u758f\u653b\u51fb\u89e3\u51b3\u65b9\u6848\u7531\u4e8e\u5176\u7a00\u758f\u6027\u5dee\u800c\u65e0\u6cd5\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u5bf9\u6297\u6837\u672c\uff0c\u5e76\u4e14\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u53ef\u8fc1\u79fb\u6027\u5dee\u548c\u653b\u51fb\u5f3a\u5ea6\u5f31\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u7406\u8bba\u4e0a\u5408\u7406\u7684\u53c2\u6570\u5316\u6280\u672f\u6765\u8fd1\u4f3cNP-hard l0\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u6700\u5927\u5316\u5bf9\u6297\u5c5e\u6027\u548c\u6700\u5c0f\u5316\u6270\u52a8\u50cf\u7d20\u7684\u6570\u91cf\u6765\u589e\u52a0\u521d\u59cb\u6270\u52a8\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u5f00\u9500\u3001\u53ef\u8fc1\u79fb\u6027\u548c\u653b\u51fb\u5f3a\u5ea6\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u7a00\u758f\u653b\u51fb\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a00\u758f\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u5728\u8ba1\u7b97\u5f00\u9500\u3001\u53ef\u8fc1\u79fb\u6027\u548c\u653b\u51fb\u5f3a\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e3a\u8bc4\u4f30\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u51c6\u3002\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u4ea7\u751f\u4e86\u66f4\u7a00\u758f\u7684\u5bf9\u6297\u6837\u672c\uff0c\u5e76\u53d1\u73b0\u4e86\u4e24\u79cd\u566a\u58f0\u7c7b\u578b\uff0c\u6709\u52a9\u4e8e\u89e3\u91ca\u5bf9\u6297\u6270\u52a8\u5982\u4f55\u8bef\u5bfc\u5206\u7c7b\u5668\u3002"}}
{"id": "2506.18499", "categories": ["cs.LG", "cs.AI", "cs.DB", "H.4.1; I.2.1"], "pdf": "https://arxiv.org/pdf/2506.18499", "abs": "https://arxiv.org/abs/2506.18499", "authors": ["Alessandra Agostini", "Andrea Maurino", "Blerina Spahiu"], "title": "PuckTrick: A Library for Making Synthetic Data More Realistic", "comment": "17 pages, 3 figures", "summary": "The increasing reliance on machine learning (ML) models for decision-making\nrequires high-quality training data. However, access to real-world datasets is\noften restricted due to privacy concerns, proprietary restrictions, and\nincomplete data availability. As a result, synthetic data generation (SDG) has\nemerged as a viable alternative, enabling the creation of artificial datasets\nthat preserve the statistical properties of real data while ensuring privacy\ncompliance. Despite its advantages, synthetic data is often overly clean and\nlacks real-world imperfections, such as missing values, noise, outliers, and\nmisclassified labels, which can significantly impact model generalization and\nrobustness. To address this limitation, we introduce Pucktrick, a Python\nlibrary designed to systematically contaminate synthetic datasets by\nintroducing controlled errors. The library supports multiple error types,\nincluding missing data, noisy values, outliers, label misclassification,\nduplication, and class imbalance, offering a structured approach to evaluating\nML model resilience under real-world data imperfections. Pucktrick provides two\ncontamination modes: one for injecting errors into clean datasets and another\nfor further corrupting already contaminated datasets. Through extensive\nexperiments on real-world financial datasets, we evaluate the impact of\nsystematic data contamination on model performance. Our findings demonstrate\nthat ML models trained on contaminated synthetic data outperform those trained\non purely synthetic, error-free data, particularly for tree-based and linear\nmodels such as SVMs and Extra Trees.", "AI": {"tldr": "Pucktrick, a Python library, contaminates synthetic datasets with controlled errors to improve ML model resilience, showing that models trained on contaminated data outperform those trained on purely synthetic data.", "motivation": "synthetic data is often overly clean and lacks real-world imperfections, such as missing values, noise, outliers, and misclassified labels, which can significantly impact model generalization and robustness", "method": "a Python library designed to systematically contaminate synthetic datasets by introducing controlled errors", "result": "ML models trained on contaminated synthetic data outperform those trained on purely synthetic, error-free data, particularly for tree-based and linear models such as SVMs and Extra Trees.", "conclusion": "ML models trained on contaminated synthetic data outperform those trained on purely synthetic, error-free data, particularly for tree-based and linear models such as SVMs and Extra Trees."}}
{"id": "2506.17765", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17765", "abs": "https://arxiv.org/abs/2506.17765", "authors": ["Jiao Chen", "Kehui Yao", "Reza Yousefi Maragheh", "Kai Zhao", "Jianpeng Xu", "Jason Cho", "Evren Korpeoglu", "Sushant Kumar", "Kannan Achan"], "title": "CARTS: Collaborative Agents for Recommendation Textual Summarization", "comment": null, "summary": "Current recommendation systems often require some form of textual data\nsummarization, such as generating concise and coherent titles for product\ncarousels or other grouped item displays. While large language models have\nshown promise in NLP domains for textual summarization, these approaches do not\ndirectly apply to recommendation systems, where explanations must be highly\nrelevant to the core features of item sets, adhere to strict word limit\nconstraints. In this paper, we propose CARTS (Collaborative Agents for\nRecommendation Textual Summarization), a multi-agent LLM framework designed for\nstructured summarization in recommendation systems. CARTS decomposes the task\ninto three stages-Generation Augmented Generation (GAG), refinement circle, and\narbitration, where successive agent roles are responsible for extracting\nsalient item features, iteratively refining candidate titles based on relevance\nand length feedback, and selecting the final title through a collaborative\narbitration process. Experiments on large-scale e-commerce data and live A/B\ntesting show that CARTS significantly outperforms single-pass and\nchain-of-thought LLM baselines, delivering higher title relevance and improved\nuser engagement metrics.", "AI": {"tldr": "CARTS\u662f\u4e00\u79cd\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u7ed3\u6784\u5316\u6458\u8981\u7684\u591a\u4ee3\u7406LLM\u6846\u67b6\uff0c\u5b83\u4f18\u4e8e\u5176\u4ed6LLM\u57fa\u7ebf\u3002", "motivation": "\u5f53\u524d\u7684\u63a8\u8350\u7cfb\u7edf\u901a\u5e38\u9700\u8981\u67d0\u79cd\u5f62\u5f0f\u7684\u6587\u672c\u6570\u636e\u6458\u8981\uff0c\u4f8b\u5982\u4e3a\u4ea7\u54c1\u8f6e\u64ad\u6216\u5176\u4ed6\u5206\u7ec4\u9879\u76ee\u663e\u793a\u751f\u6210\u7b80\u6d01\u800c\u8fde\u8d2f\u7684\u6807\u9898\u3002\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5df2\u5728NLP\u9886\u57df\u7684\u6587\u672c\u6458\u8981\u4e2d\u663e\u793a\u51fa\u5e0c\u671b\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5e76\u4e0d\u76f4\u63a5\u9002\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\uff0c\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u89e3\u91ca\u5fc5\u987b\u4e0e\u9879\u76ee\u96c6\u7684\u6838\u5fc3\u529f\u80fd\u9ad8\u5ea6\u76f8\u5173\uff0c\u5e76\u9075\u5b88\u4e25\u683c\u7684\u5b57\u6570\u9650\u5236\u3002", "method": "CARTS\uff08\u63a8\u8350\u6587\u672c\u6458\u8981\u7684\u534f\u4f5c\u4ee3\u7406\uff09\uff0c\u4e00\u4e2a\u591a\u4ee3\u7406LLM\u6846\u67b6\uff0c\u4e13\u4e3a\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u7ed3\u6784\u5316\u6458\u8981\u800c\u8bbe\u8ba1\u3002CARTS\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u4e09\u4e2a\u9636\u6bb5-\u751f\u6210\u589e\u5f3a\u751f\u6210\uff08GAG\uff09\uff0c\u7ec6\u5316\u5708\u548c\u4ef2\u88c1\uff0c\u5176\u4e2d\u8fde\u7eed\u7684\u4ee3\u7406\u89d2\u8272\u8d1f\u8d23\u63d0\u53d6\u663e\u7740\u7684\u9879\u76ee\u7279\u5f81\uff0c\u57fa\u4e8e\u76f8\u5173\u6027\u548c\u957f\u5ea6\u53cd\u9988\u8fed\u4ee3\u5730\u7ec6\u5316\u5019\u9009\u6807\u9898\uff0c\u5e76\u901a\u8fc7\u534f\u4f5c\u4ef2\u88c1\u8fc7\u7a0b\u9009\u62e9\u6700\u7ec8\u6807\u9898\u3002", "result": "\u5728\u5927\u578b\u7535\u5b50\u5546\u52a1\u6570\u636e\u548c\u5b9e\u65f6A / B\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCARTS\u663e\u7740\u4f18\u4e8e\u5355\u901a\u9053\u548c\u601d\u7ef4\u94feLLM\u57fa\u7ebf\uff0c\u4ece\u800c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u6807\u9898\u76f8\u5173\u6027\u548c\u6539\u8fdb\u7684\u7528\u6237\u53c2\u4e0e\u5ea6\u6307\u6807\u3002", "conclusion": "CARTS\u663e\u8457\u4f18\u4e8e\u5355\u901a\u9053\u548c\u601d\u7ef4\u94feLLM\u57fa\u7ebf\uff0c\u63d0\u4f9b\u66f4\u9ad8\u7684\u6807\u9898\u76f8\u5173\u6027\u548c\u6539\u8fdb\u7684\u7528\u6237\u53c2\u4e0e\u5ea6\u6307\u6807\u3002"}}
{"id": "2506.17403", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17403", "abs": "https://arxiv.org/abs/2506.17403", "authors": ["Zhiyi Shi", "Junsik Kim", "Helen Y. Yang", "Yonghyun Song", "Hyun-Jic Oh", "Dalit Ben-Yosef", "Daniel Needleman", "Hanspeter Pfister"], "title": "Spatial-Temporal Pre-Training for Embryo Viability Prediction Using Time-Lapse Videos", "comment": "Preprint submitted to Medical Image Analysis", "summary": "Automating embryo viability prediction for in vitro fertilization (IVF) is\nimportant but challenging due to the limited availability of labeled pregnancy\noutcome data, as only a small fraction of embryos are labeled after transfer.\nSelf-supervised learning (SSL) can leverage both labeled and unlabeled data to\nimprove prediction. However, existing SSL methods for videos are not directly\napplicable to embryo development videos due to two challenges: (1) embryo\ntime-lapse videos contain hundreds of frames, requiring significant GPU memory\nfor conventional SSL; (2) the dataset contains videos with varying lengths and\nmany outlier frames, causing traditional video alignment methods to struggle\nwith semantic misalignment. We propose Spatial-Temporal Pre-Training (STPT) to\naddress these challenges. STPT includes two stages: spatial and temporal. In\neach stage, only one encoder is trained while the other is frozen, reducing\nmemory demands. To handle temporal misalignment, STPT avoids frame-by-frame\nalignment across videos. The spatial stage learns from alignments within each\nvideo and its temporally consistent augmentations. The temporal stage then\nmodels relationships between video embeddings. Our method efficiently handles\nlong videos and temporal variability. On 23,027 time-lapse videos (3,286\nlabeled), STPT achieves the highest AUC of 0.635 (95% CI: 0.632-0.638) compared\nto baselines, with limited computational resources.", "AI": {"tldr": "This paper introduces Spatial-Temporal Pre-Training (STPT) to predict embryo viability from time-lapse videos, achieving state-of-the-art performance with limited resources by addressing challenges in long video processing and temporal misalignment.", "motivation": "Automating embryo viability prediction for in vitro fertilization (IVF) is important but challenging due to the limited availability of labeled pregnancy outcome data, as only a small fraction of embryos are labeled after transfer. Self-supervised learning (SSL) can leverage both labeled and unlabeled data to improve prediction. However, existing SSL methods for videos are not directly applicable to embryo development videos due to two challenges: (1) embryo time-lapse videos contain hundreds of frames, requiring significant GPU memory for conventional SSL; (2) the dataset contains videos with varying lengths and many outlier frames, causing traditional video alignment methods to struggle with semantic misalignment.", "method": "We propose Spatial-Temporal Pre-Training (STPT) to address these challenges. STPT includes two stages: spatial and temporal. In each stage, only one encoder is trained while the other is frozen, reducing memory demands. To handle temporal misalignment, STPT avoids frame-by-frame alignment across videos. The spatial stage learns from alignments within each video and its temporally consistent augmentations. The temporal stage then models relationships between video embeddings.", "result": "On 23,027 time-lapse videos (3,286 labeled), STPT achieves the highest AUC of 0.635 (95% CI: 0.632-0.638) compared to baselines, with limited computational resources.", "conclusion": "STPT achieves the highest AUC of 0.635 (95% CI: 0.632-0.638) compared to baselines, with limited computational resources."}}
{"id": "2506.17367", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.17367", "abs": "https://arxiv.org/abs/2506.17367", "authors": ["Mateusz Cedro", "Timour Ichmoukhamedov", "Sofie Goethals", "Yifan He", "James Hinns", "David Martens"], "title": "Cash or Comfort? How LLMs Value Your Inconvenience", "comment": "12 pages, 4 figures, 3 tables", "summary": "Large Language Models (LLMs) are increasingly proposed as near-autonomous\nartificial intelligence (AI) agents capable of making everyday decisions on\nbehalf of humans. Although LLMs perform well on many technical tasks, their\nbehaviour in personal decision-making remains less understood. Previous studies\nhave assessed their rationality and moral alignment with human decisions.\nHowever, the behaviour of AI assistants in scenarios where financial rewards\nare at odds with user comfort has not yet been thoroughly explored. In this\npaper, we tackle this problem by quantifying the prices assigned by multiple\nLLMs to a series of user discomforts: additional walking, waiting, hunger and\npain. We uncover several key concerns that strongly question the prospect of\nusing current LLMs as decision-making assistants: (1) a large variance in\nresponses between LLMs, (2) within a single LLM, responses show fragility to\nminor variations in prompt phrasing (e.g., reformulating the question in the\nfirst person can considerably alter the decision), (3) LLMs can accept\nunreasonably low rewards for major inconveniences (e.g., 1 Euro to wait 10\nhours), and (4) LLMs can reject monetary gains where no discomfort is imposed\n(e.g., 1,000 Euro to wait 0 minutes). These findings emphasize the need for\nscrutiny of how LLMs value human inconvenience, particularly as we move toward\napplications where such cash-versus-comfort trade-offs are made on users'\nbehalf.", "AI": {"tldr": "LLMs\u5728\u91d1\u94b1\u6362\u8212\u9002\u5ea6\u7684\u51b3\u7b56\u4e2d\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u4e0d\u9002\u5408\u505a\u51b3\u7b56\u52a9\u624b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u8ba4\u4e3a\u662f\u8fd1\u4e4e\u81ea\u4e3b\u7684\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\uff0c\u80fd\u591f\u4ee3\u8868\u4eba\u7c7b\u505a\u51fa\u65e5\u5e38\u51b3\u7b56\u3002\u7136\u800c\uff0c\u4eba\u5de5\u667a\u80fd\u52a9\u624b\u5728\u8d22\u52a1\u5956\u52b1\u4e0e\u7528\u6237\u8212\u9002\u5ea6\u76f8\u6096\u7684\u60c5\u51b5\u4e0b\u7684\u884c\u4e3a\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u91cf\u5316\u591a\u4e2aLLM\u5bf9\u4e00\u7cfb\u5217\u7528\u6237\u4e0d\u9002\uff08\u6b65\u884c\u3001\u7b49\u5f85\u3001\u9965\u997f\u548c\u75bc\u75db\uff09\u7684\u5b9a\u4ef7\u3002", "result": "LLMs\u7684\u54cd\u5e94\u5dee\u5f02\u5927\uff0c\u5bf9prompt\u63aa\u8f9e\u7684\u7ec6\u5fae\u53d8\u5316\u654f\u611f\uff0c\u5bf9\u91cd\u5927\u4e0d\u4fbf\u63a5\u53d7\u4e0d\u5408\u7406\u4f4e\u7684\u62a5\u916c\uff0c\u5e76\u53ef\u80fd\u62d2\u7edd\u6ca1\u6709\u4e0d\u9002\u611f\u7684\u91d1\u94b1\u6536\u76ca\u3002", "conclusion": "LLMs\u5728\u6d89\u53ca\u91d1\u94b1\u4e0e\u8212\u9002\u5ea6\u6743\u8861\u7684\u51b3\u7b56\u4e2d\u8868\u73b0\u51fa\u4e0d\u4e00\u81f4\u6027\uff0c\u5bf9\u4eba\u7c7b\u4e0d\u4fbf\u7684\u4f30\u503c\u5b58\u5728\u95ee\u9898\uff0c\u4e0d\u9002\u5408\u4f5c\u4e3a\u51b3\u7b56\u52a9\u624b\u3002"}}
{"id": "2506.17589", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17589", "abs": "https://arxiv.org/abs/2506.17589", "authors": ["Bowen Wang"], "title": "Taming the Untamed: Graph-Based Knowledge Retrieval and Reasoning for MLLMs to Conquer the Unknown", "comment": null, "summary": "The real value of knowledge lies not just in its accumulation, but in its\npotential to be harnessed effectively to conquer the unknown. Although recent\nmultimodal large language models (MLLMs) exhibit impressing multimodal\ncapabilities, they often fail in rarely encountered domain-specific tasks due\nto limited relevant knowledge. To explore this, we adopt visual game cognition\nas a testbed and select Monster Hunter: World as the target to construct a\nmultimodal knowledge graph (MH-MMKG), which incorporates multi-modalities and\nintricate entity relations. We also design a series of challenging queries\nbased on MH-MMKG to evaluate the models' ability for complex knowledge\nretrieval and reasoning. Furthermore, we propose a multi-agent retriever that\nenables a model to autonomously search relevant knowledge without additional\ntraining. Experimental results show that our approach significantly enhances\nthe performance of MLLMs, providing a new perspective on multimodal\nknowledge-augmented reasoning and laying a solid foundation for future\nresearch.", "AI": {"tldr": "\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u7cfb\u5217\u5177\u6709\u6311\u6218\u6027\u7684\u67e5\u8be2\uff0c\u4ee5\u8bc4\u4f30\u6a21\u578b\u590d\u6742\u77e5\u8bc6\u68c0\u7d22\u548c\u63a8\u7406\u7684\u80fd\u529b\u3002", "motivation": "\u77e5\u8bc6\u7684\u771f\u6b63\u4ef7\u503c\u4e0d\u4ec5\u5728\u4e8e\u79ef\u7d2f\uff0c\u8fd8\u5728\u4e8e\u5176\u6709\u6548\u5229\u7528\u4ee5\u5f81\u670d\u672a\u77e5\u4e8b\u7269\u7684\u6f5c\u529b\u3002\u5c3d\u7ba1\u6700\u8fd1\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u8868\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u591a\u6a21\u6001\u80fd\u529b\uff0c\u4f46\u7531\u4e8e\u76f8\u5173\u77e5\u8bc6\u6709\u9650\uff0c\u5b83\u4eec\u7ecf\u5e38\u5728\u5f88\u5c11\u9047\u5230\u7684\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u4e2d\u5931\u8d25\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u4ee3\u7406\u68c0\u7d22\u5668\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u81ea\u4e3b\u641c\u7d22\u76f8\u5173\u77e5\u8bc6\u800c\u65e0\u9700\u989d\u5916\u7684\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86 MLLM \u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86 MLLM \u7684\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001\u77e5\u8bc6\u589e\u5f3a\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u89c6\u89d2\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2506.17251", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17251", "abs": "https://arxiv.org/abs/2506.17251", "authors": ["Dongseok Lee", "Jimyung Hong", "Dongyoung Kim", "Jaehyung Kim"], "title": "Training-free LLM Verification via Recycling Few-shot Examples", "comment": null, "summary": "Although LLMs have achieved remarkable performance, the inherent\nstochasticity of their reasoning process and varying conclusions present\nsignificant challenges. Majority voting or Best-of-N with external verification\nmodels has been explored to find the most promising solution among multiple LLM\noutputs. However, these approaches have certain limitations, such as limited\napplicability or the cost of an additional training step. To address this\nproblem, we propose a novel and effective framework that Recycles Few-shot\nexamples to verify LLM outputs (Referi). Our key idea is to additionally\nutilize the given few-shot examples to evaluate the candidate outputs of the\ntarget query, not only using them to generate outputs as the conventional\nfew-shot prompting setup. Specifically, Referi evaluates the generated outputs\nby combining two different scores, designed motivated from Bayes' rule, and\nsubsequently selects the candidate that is both confidently determined and\ncontextually coherent through a few additional LLM inferences. Experiments with\nthree different LLMs and across seven diverse tasks demonstrate that our\nframework significantly improves the accuracy of LLMs-achieving an average gain\nof 4.8%-through effective response selection, without additional training.", "AI": {"tldr": "Referi improves LLM accuracy by recycling few-shot examples to verify outputs, achieving a 4.8% gain without extra training.", "motivation": "The inherent stochasticity of LLMs' reasoning process and varying conclusions present significant challenges. Existing approaches like majority voting or Best-of-N have limitations such as limited applicability or the cost of additional training.", "method": "The framework Recycles Few-shot examples to verify LLM outputs (Referi) by combining two different scores, designed motivated from Bayes' rule, and subsequently selects the candidate that is both confidently determined and contextually coherent through a few additional LLM inferences.", "result": "Experiments with three different LLMs and across seven diverse tasks demonstrate that the framework improves the accuracy of LLMs.", "conclusion": "The proposed Referi framework significantly improves the accuracy of LLMs by 4.8% through effective response selection, without additional training."}}
{"id": "2506.17782", "categories": ["cs.IR", "cs.AI", "H.3.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.17782", "abs": "https://arxiv.org/abs/2506.17782", "authors": ["Catarina Pires", "S\u00e9rgio Nunes", "Lu\u00eds Filipe Teixeira"], "title": "Expanding Relevance Judgments for Medical Case-based Retrieval Task with Multimodal LLMs", "comment": "To appear at the Third Workshop on Large Language Models for\n  Evaluation in Information Retrieval (LLM4Eval 2025), co-located with SIGIR\n  2025. 9 pages, 2 figures, 5 tables", "summary": "Evaluating Information Retrieval (IR) systems relies on high-quality manual\nrelevance judgments (qrels), which are costly and time-consuming to obtain.\nWhile pooling reduces the annotation effort, it results in only partially\nlabeled datasets. Large Language Models (LLMs) offer a promising alternative to\nreducing reliance on manual judgments, particularly in complex domains like\nmedical case-based retrieval, where relevance assessment requires analyzing\nboth textual and visual information. In this work, we explore using a\nMultimodal Large Language Model (MLLM) to expand relevance judgments, creating\na new dataset of automated judgments. Specifically, we employ Gemini 1.5 Pro on\nthe ImageCLEFmed 2013 case-based retrieval task, simulating human assessment\nthrough an iteratively refined, structured prompting strategy that integrates\nbinary scoring, instruction-based evaluation, and few-shot learning. We\nsystematically experimented with various prompt configurations to maximize\nagreement with human judgments. To evaluate agreement between the MLLM and\nhuman judgments, we use Cohen's Kappa, achieving a substantial agreement score\nof 0.6, comparable to inter-annotator agreement typically observed in\nmultimodal retrieval tasks. Starting from the original 15,028 manual judgments\n(4.72% relevant) across 35 topics, our MLLM-based approach expanded the dataset\nby over 37x to 558,653 judgments, increasing relevant annotations to 5,950. On\naverage, each medical case query received 15,398 new annotations, with\napproximately 99% being non-relevant, reflecting the high sparsity typical in\nthis domain. Our results demonstrate the potential of MLLMs to scale relevance\njudgment collection, offering a promising direction for supporting retrieval\nevaluation in medical and multimodal IR tasks.", "AI": {"tldr": "The paper explores using a Multimodal Large Language Model (MLLM) to expand relevance judgments in medical case-based retrieval. They used Gemini 1.5 Pro and achieved substantial agreement with human judgments, significantly expanding the dataset of relevance judgments.", "motivation": "Evaluating Information Retrieval (IR) systems relies on high-quality manual relevance judgments (qrels), which are costly and time-consuming to obtain. Large Language Models (LLMs) offer a promising alternative to reducing reliance on manual judgments, particularly in complex domains like medical case-based retrieval, where relevance assessment requires analyzing both textual and visual information.", "method": "The authors employ Gemini 1.5 Pro on the ImageCLEFmed 2013 case-based retrieval task, simulating human assessment through an iteratively refined, structured prompting strategy that integrates binary scoring, instruction-based evaluation, and few-shot learning. They systematically experimented with various prompt configurations to maximize agreement with human judgments, using Cohen's Kappa to evaluate agreement between the MLLM and human judgments.", "result": "The MLLM achieved a substantial agreement score of 0.6 (Cohen's Kappa), comparable to inter-annotator agreement typically observed in multimodal retrieval tasks. Starting from 15,028 manual judgments (4.72% relevant), the MLLM-based approach expanded the dataset by over 37x to 558,653 judgments, increasing relevant annotations to 5,950. On average, each medical case query received 15,398 new annotations.", "conclusion": "This paper demonstrates the potential of Multimodal Large Language Models (MLLMs) to scale relevance judgment collection, offering a promising direction for supporting retrieval evaluation in medical and multimodal IR tasks."}}
{"id": "2506.17412", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17412", "abs": "https://arxiv.org/abs/2506.17412", "authors": ["Zijun Sun", "Solveig Thrun", "Michael Kampffmeyer"], "title": "VMRA-MaR: An Asymmetry-Aware Temporal Framework for Longitudinal Breast Cancer Risk Prediction", "comment": "MICCAI 2025, Provisional Accept", "summary": "Breast cancer remains a leading cause of mortality worldwide and is typically\ndetected via screening programs where healthy people are invited in regular\nintervals. Automated risk prediction approaches have the potential to improve\nthis process by facilitating dynamically screening of high-risk groups. While\nmost models focus solely on the most recent screening, there is growing\ninterest in exploiting temporal information to capture evolving trends in\nbreast tissue, as inspired by clinical practice. Early methods typically relied\non two time steps, and although recent efforts have extended this to multiple\ntime steps using Transformer architectures, challenges remain in fully\nharnessing the rich temporal dynamics inherent in longitudinal imaging data. In\nthis work, we propose to instead leverage Vision Mamba RNN (VMRNN) with a\nstate-space model (SSM) and LSTM-like memory mechanisms to effectively capture\nnuanced trends in breast tissue evolution. To further enhance our approach, we\nincorporate an asymmetry module that utilizes a Spatial Asymmetry Detector\n(SAD) and Longitudinal Asymmetry Tracker (LAT) to identify clinically relevant\nbilateral differences. This integrated framework demonstrates notable\nimprovements in predicting cancer onset, especially for the more challenging\nhigh-density breast cases and achieves superior performance at extended time\npoints (years four and five), highlighting its potential to advance early\nbreast cancer recognition and enable more personalized screening strategies.\nOur code is available at https://github.com/Mortal-Suen/VMRA-MaR.git.", "AI": {"tldr": "This paper proposes a Vision Mamba RNN (VMRNN) with a state-space model (SSM) and LSTM-like memory mechanisms to effectively capture nuanced trends in breast tissue evolution. The integrated framework demonstrates notable improvements in predicting cancer onset, especially for the more challenging high-density breast cases and achieves superior performance at extended time points.", "motivation": "Automated risk prediction approaches have the potential to improve the breast cancer screening process by facilitating dynamically screening of high-risk groups. There is growing interest in exploiting temporal information to capture evolving trends in breast tissue, as inspired by clinical practice. Early methods typically relied on two time steps, and although recent efforts have extended this to multiple time steps using Transformer architectures, challenges remain in fully harnessing the rich temporal dynamics inherent in longitudinal imaging data.", "method": "leverage Vision Mamba RNN (VMRNN) with a state-space model (SSM) and LSTM-like memory mechanisms to effectively capture nuanced trends in breast tissue evolution. To further enhance our approach, we incorporate an asymmetry module that utilizes a Spatial Asymmetry Detector (SAD) and Longitudinal Asymmetry Tracker (LAT) to identify clinically relevant bilateral differences.", "result": "achieves superior performance at extended time points (years four and five), highlighting its potential to advance early breast cancer recognition and enable more personalized screening strategies.", "conclusion": "This integrated framework demonstrates notable improvements in predicting cancer onset, especially for the more challenging high-density breast cases and achieves superior performance at extended time points (years four and five), highlighting its potential to advance early breast cancer recognition and enable more personalized screening strategies."}}
{"id": "2506.17410", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.17410", "abs": "https://arxiv.org/abs/2506.17410", "authors": ["Danielle R. Thomas", "Conrad Borchers", "Jionghao Lin", "Sanjit Kakarla", "Shambhavi Bhushan", "Erin Gatz", "Shivang Gupta", "Ralph Abboud", "Kenneth R. Koedinger"], "title": "Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A Feasibility Study", "comment": "Short research paper accepted at EC-TEL 2025", "summary": "Tutoring improves student achievement, but identifying and studying what\ntutoring actions are most associated with student learning at scale based on\naudio transcriptions is an open research problem. This present study\ninvestigates the feasibility and scalability of using generative AI to identify\nand evaluate specific tutor moves in real-life math tutoring. We analyze 50\nrandomly selected transcripts of college-student remote tutors assisting middle\nschool students in mathematics. Using GPT-4, GPT-4o, GPT-4-turbo,\nGemini-1.5-pro, and LearnLM, we assess tutors' application of two tutor skills:\ndelivering effective praise and responding to student math errors. All models\nreliably detected relevant situations, for example, tutors providing praise to\nstudents (94-98% accuracy) and a student making a math error (82-88% accuracy)\nand effectively evaluated the tutors' adherence to tutoring best practices,\naligning closely with human judgments (83-89% and 73-77%, respectively). We\npropose a cost-effective prompting strategy and discuss practical implications\nfor using large language models to support scalable assessment in authentic\nsettings. This work further contributes LLM prompts to support reproducibility\nand research in AI-supported learning.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u6765\u8bc6\u522b\u548c\u8bc4\u4f30\u771f\u5b9e\u6570\u5b66\u8f85\u5bfc\u4e2d\u7279\u5b9a\u8f85\u5bfc\u52a8\u4f5c\u7684\u53ef\u884c\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u8f85\u5bfc\u53ef\u4ee5\u63d0\u9ad8\u5b66\u751f\u7684\u6210\u7ee9\uff0c\u4f46\u662f\u6839\u636e\u97f3\u9891\u8f6c\u5f55\u8bc6\u522b\u548c\u7814\u7a76\u54ea\u4e9b\u8f85\u5bfc\u884c\u4e3a\u4e0e\u5927\u89c4\u6a21\u7684\u5b66\u751f\u5b66\u4e60\u6700\u76f8\u5173\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u7814\u7a76\u95ee\u9898\u3002", "method": "\u6211\u4eec\u5206\u6790\u4e86 50 \u4efd\u968f\u673a\u9009\u62e9\u7684\u5927\u5b66\u5b66\u751f\u8fdc\u7a0b\u8f85\u5bfc\u4e2d\u5b66\u5b66\u751f\u6570\u5b66\u7684\u6210\u7ee9\u5355\u3002\u4f7f\u7528 GPT-4\u3001GPT-4o\u3001GPT-4-turbo\u3001Gemini-1.5-pro \u548c LearnLM\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u5bfc\u5e08\u5e94\u7528\u7684\u4e24\u9879\u5bfc\u5e08\u6280\u80fd\uff1a\u7ed9\u4e88\u6709\u6548\u7684\u8868\u626c\u548c\u56de\u5e94\u5b66\u751f\u6570\u5b66\u9519\u8bef\u3002", "result": "\u6240\u6709\u6a21\u578b\u90fd\u80fd\u53ef\u9760\u5730\u68c0\u6d4b\u5230\u76f8\u5173\u60c5\u51b5\uff0c\u4f8b\u5982\uff0c\u5bfc\u5e08\u5411\u5b66\u751f\u63d0\u4f9b\u8868\u626c\uff08\u51c6\u786e\u7387 94-98%\uff09\u548c\u5b66\u751f\u72af\u6570\u5b66\u9519\u8bef\uff08\u51c6\u786e\u7387 82-88%\uff09\uff0c\u5e76\u6709\u6548\u5730\u8bc4\u4f30\u4e86\u5bfc\u5e08\u5bf9\u8f85\u5bfc\u6700\u4f73\u5b9e\u8df5\u7684\u9075\u5b88\u60c5\u51b5\uff0c\u4e0e\u4eba\u7c7b\u7684\u5224\u65ad\u975e\u5e38\u543b\u5408\uff08\u5206\u522b\u4e3a 83-89% \u548c 73-77%\uff09\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u63d0\u793a\u7b56\u7565\uff0c\u5e76\u8ba8\u8bba\u4e86\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u652f\u6301\u53ef\u6269\u5c55\u8bc4\u4f30\u7684\u5b9e\u9645\u610f\u4e49\u3002\u8fd9\u9879\u5de5\u4f5c\u8fdb\u4e00\u6b65\u8d21\u732e\u4e86 LLM \u63d0\u793a\uff0c\u4ee5\u652f\u6301 AI \u652f\u6301\u7684\u5b66\u4e60\u4e2d\u7684\u53ef\u91cd\u590d\u6027\u548c\u7814\u7a76\u3002"}}
{"id": "2506.17644", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17644", "abs": "https://arxiv.org/abs/2506.17644", "authors": ["Zimo Ji", "Daoyuan Wu", "Wenyuan Jiang", "Pingchuan Ma", "Zongjie Li", "Shuai Wang"], "title": "Measuring and Augmenting Large Language Models for Solving Capture-the-Flag Challenges", "comment": null, "summary": "Capture-the-Flag (CTF) competitions are crucial for cybersecurity education\nand training. As large language models (LLMs) evolve, there is increasing\ninterest in their ability to automate CTF challenge solving. For example, DARPA\nhas organized the AIxCC competition since 2023 to advance AI-powered automated\noffense and defense. However, this demands a combination of multiple abilities,\nfrom knowledge to reasoning and further to actions. In this paper, we highlight\nthe importance of technical knowledge in solving CTF problems and deliberately\nconstruct a focused benchmark, CTFKnow, with 3,992 questions to measure LLMs'\nperformance in this core aspect. Our study offers a focused and innovative\nmeasurement of LLMs' capability in understanding CTF knowledge and applying it\nto solve CTF challenges. Our key findings reveal that while LLMs possess\nsubstantial technical knowledge, they falter in accurately applying this\nknowledge to specific scenarios and adapting their strategies based on feedback\nfrom the CTF environment.\n  Based on insights derived from this measurement study, we propose CTFAgent, a\nnovel LLM-driven framework for advancing CTF problem-solving. CTFAgent\nintroduces two new modules: two-stage Retrieval Augmented Generation (RAG) and\ninteractive Environmental Augmentation, which enhance LLMs' technical knowledge\nand vulnerability exploitation on CTF, respectively. Our experimental results\nshow that, on two popular CTF datasets, CTFAgent both achieves over 80%\nperformance improvement. Moreover, in the recent picoCTF2024 hosted by CMU,\nCTFAgent ranked in the top 23.6% of nearly 7,000 participating teams. This\nreflects the benefit of our measurement study and the potential of our\nframework in advancing LLMs' capabilities in CTF problem-solving.", "AI": {"tldr": "This paper introduces CTFKnow, a benchmark for evaluating LLMs' CTF knowledge, and proposes CTFAgent, an LLM-driven framework that improves CTF problem-solving performance.", "motivation": "There is increasing interest in using LLMs to automate CTF challenge solving, but this requires multiple abilities. The paper highlights the importance of technical knowledge in solving CTF problems.", "method": "The authors construct a focused benchmark, CTFKnow, with 3,992 questions to measure LLMs' performance in technical knowledge. They also propose CTFAgent, a framework with two-stage RAG and interactive Environmental Augmentation.", "result": "LLMs possess substantial technical knowledge but struggle to apply it accurately to specific scenarios. CTFAgent achieves over 80% performance improvement on two CTF datasets and ranked in the top 23.6% in picoCTF2024.", "conclusion": "The paper proposes CTFAgent, a novel LLM-driven framework with two new modules: two-stage Retrieval Augmented Generation (RAG) and interactive Environmental Augmentation. Experiments show CTFAgent achieves over 80% performance improvement on two popular CTF datasets and ranked in the top 23.6% of nearly 7,000 participating teams in picoCTF2024."}}
{"id": "2506.17252", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17252", "abs": "https://arxiv.org/abs/2506.17252", "authors": ["Zixuan Huang", "Yikun Ban", "Lean Fu", "Xiaojie Li", "Zhongxiang Dai", "Jianxin Li", "Deqing Wang"], "title": "Adaptive Sample Scheduling for Direct Preference Optimization", "comment": null, "summary": "Direct Preference Optimization (DPO) has emerged as an effective approach for\naligning large language models (LLMs) with human preferences. However, its\nperformance is highly dependent on the quality of the underlying human\npreference data. To address this bottleneck, prior work has explored various\ndata selection strategies, but these methods often overlook the impact of the\nevolving states of the language model during the DPO process. %including active\nquerying, response pair selection, and data pre-selection. In this paper, we\nintroduce a novel problem: Sample Scheduling for DPO, which aims to dynamically\nand adaptively schedule training samples based on the model's evolving states\nthroughout preference optimization. To solve this problem, we propose SamS, an\nefficient and effective algorithm that adaptively selects samples in each\ntraining batch based on the LLM's learning feedback to maximize the potential\ngeneralization performance. Notably, without modifying the core DPO algorithm,\nsimply integrating SamS significantly improves performance across tasks, with\nminimal additional computational overhead. This work points to a promising new\ndirection for improving LLM alignment through more effective utilization of\nfixed preference datasets.", "AI": {"tldr": "introduce Sample Scheduling for DPO, which aims to dynamically and adaptively schedule training samples based on the model's evolving states throughout preference optimization.", "motivation": "performance is highly dependent on the quality of the underlying human preference data.  prior work has explored various data selection strategies, but these methods often overlook the impact of the evolving states of the language model during the DPO process.", "method": "introduce SamS, an efficient and effective algorithm that adaptively selects samples in each training batch based on the LLM's learning feedback to maximize the potential generalization performance.", "result": "SamS significantly improves performance across tasks, with minimal additional computational overhead.", "conclusion": "Integrating SamS significantly improves performance across tasks, with minimal additional computational overhead, pointing to a promising new direction for improving LLM alignment through more effective utilization of fixed preference datasets."}}
{"id": "2506.17934", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17934", "abs": "https://arxiv.org/abs/2506.17934", "authors": ["Syed N. Sakib", "Kallol Naha", "Sajratul Y. Rubaiat", "Hasan M. Jamil"], "title": "A GenAI System for Improved FAIR Independent Biological Database Integration", "comment": null, "summary": "Life sciences research increasingly requires identifying, accessing, and\neffectively processing data from an ever-evolving array of information sources\non the Linked Open Data (LOD) network. This dynamic landscape places a\nsignificant burden on researchers, as the quality of query responses depends\nheavily on the selection and semantic integration of data sources --processes\nthat are often labor-intensive, error-prone, and costly. While the adoption of\nFAIR (Findable, Accessible, Interoperable, and Reusable) data principles has\naimed to address these challenges, barriers to efficient and accurate\nscientific data processing persist.\n  In this paper, we introduce FAIRBridge, an experimental natural\nlanguage-based query processing system designed to empower scientists to\ndiscover, access, and query biological databases, even when they are not\nFAIR-compliant. FAIRBridge harnesses the capabilities of AI to interpret query\nintents, map them to relevant databases described in scientific literature, and\ngenerate executable queries via intelligent resource access plans. The system\nalso includes robust tools for mitigating low-quality query processing,\nensuring high fidelity and responsiveness in the information delivered.\n  FAIRBridge's autonomous query processing framework enables users to explore\nalternative data sources, make informed choices at every step, and leverage\ncommunity-driven crowd curation when needed. By providing a user-friendly,\nautomated hypothesis-testing platform in natural English, FAIRBridge\nsignificantly enhances the integration and processing of scientific data,\noffering researchers a powerful new tool for advancing their inquiries.", "AI": {"tldr": "FAIRBridge\u662f\u4e00\u4e2a\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u67e5\u8be2\u5904\u7406\u7cfb\u7edf\uff0c\u65e8\u5728\u5e2e\u52a9\u79d1\u5b66\u5bb6\u53d1\u73b0\u3001\u8bbf\u95ee\u548c\u67e5\u8be2\u751f\u7269\u6570\u636e\u5e93\uff0c\u5373\u4f7f\u5b83\u4eec\u4e0d\u7b26\u5408FAIR\u6807\u51c6\u3002", "motivation": "\u751f\u547d\u79d1\u5b66\u7814\u7a76\u8d8a\u6765\u8d8a\u9700\u8981\u8bc6\u522b\u3001\u8bbf\u95ee\u548c\u6709\u6548\u5904\u7406\u6765\u81ea\u94fe\u63a5\u5f00\u653e\u6570\u636e\uff08LOD\uff09\u7f51\u7edc\u4e0a\u4e0d\u65ad\u53d1\u5c55\u7684\u4fe1\u606f\u6e90\u7684\u6570\u636e\u3002\u67e5\u8be2\u54cd\u5e94\u7684\u8d28\u91cf\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u6570\u636e\u6e90\u7684\u9009\u62e9\u548c\u8bed\u4e49\u96c6\u6210\uff0c\u800c\u8fd9\u4e9b\u8fc7\u7a0b\u901a\u5e38\u662f\u52b3\u52a8\u5bc6\u96c6\u578b\u3001\u5bb9\u6613\u51fa\u9519\u4e14\u6210\u672c\u9ad8\u6602\u7684\u3002", "method": "FAIRBridge\u5229\u7528\u4eba\u5de5\u667a\u80fd\u89e3\u91ca\u67e5\u8be2\u610f\u56fe\uff0c\u5c06\u5176\u6620\u5c04\u5230\u79d1\u5b66\u6587\u732e\u4e2d\u63cf\u8ff0\u7684\u76f8\u5173\u6570\u636e\u5e93\uff0c\u5e76\u901a\u8fc7\u667a\u80fd\u8d44\u6e90\u8bbf\u95ee\u8ba1\u5212\u751f\u6210\u53ef\u6267\u884c\u7684\u67e5\u8be2\u3002", "result": "FAIRBridge\u7684\u81ea\u4e3b\u67e5\u8be2\u5904\u7406\u6846\u67b6\u4f7f\u7528\u6237\u80fd\u591f\u63a2\u7d22\u66ff\u4ee3\u6570\u636e\u6e90\uff0c\u5728\u6bcf\u4e2a\u6b65\u9aa4\u505a\u51fa\u660e\u667a\u7684\u9009\u62e9\uff0c\u5e76\u5728\u9700\u8981\u65f6\u5229\u7528\u793e\u533a\u9a71\u52a8\u7684\u4f17\u5305\u7ba1\u7406\u3002", "conclusion": "FAIRBridge\u901a\u8fc7\u63d0\u4f9b\u7528\u6237\u53cb\u597d\u7684\u81ea\u52a8\u5316\u5047\u8bbe\u6d4b\u8bd5\u5e73\u53f0\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u79d1\u5b66\u6570\u636e\u7684\u96c6\u6210\u548c\u5904\u7406\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u65b0\u5de5\u5177\u6765\u63a8\u8fdb\u4ed6\u4eec\u7684\u7814\u7a76\u3002"}}
{"id": "2506.17425", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17425", "abs": "https://arxiv.org/abs/2506.17425", "authors": ["Minmin Yang", "Huantao Ren", "Senem Velipasalar"], "title": "Trans${^2}$-CBCT: A Dual-Transformer Framework for Sparse-View CBCT Reconstruction", "comment": null, "summary": "Cone-beam computed tomography (CBCT) using only a few X-ray projection views\nenables faster scans with lower radiation dose, but the resulting severe\nunder-sampling causes strong artifacts and poor spatial coverage. We address\nthese challenges in a unified framework. First, we replace conventional\nUNet/ResNet encoders with TransUNet, a hybrid CNN-Transformer model.\nConvolutional layers capture local details, while self-attention layers enhance\nglobal context. We adapt TransUNet to CBCT by combining multi-scale features,\nquerying view-specific features per 3D point, and adding a lightweight\nattenuation-prediction head. This yields Trans-CBCT, which surpasses prior\nbaselines by 1.17 dB PSNR and 0.0163 SSIM on the LUNA16 dataset with six views.\nSecond, we introduce a neighbor-aware Point Transformer to enforce volumetric\ncoherence. This module uses 3D positional encoding and attention over k-nearest\nneighbors to improve spatial consistency. The resulting model, Trans$^2$-CBCT,\nprovides an additional gain of 0.63 dB PSNR and 0.0117 SSIM. Experiments on\nLUNA16 and ToothFairy show consistent gains from six to ten views, validating\nthe effectiveness of combining CNN-Transformer features with point-based\ngeometry reasoning for sparse-view CBCT reconstruction.", "AI": {"tldr": "This paper introduces Trans-CBCT and Trans$^2$-CBCT, which combine CNN-Transformer features with point-based geometry reasoning for sparse-view CBCT reconstruction, achieving state-of-the-art results on LUNA16 and ToothFairy datasets.", "motivation": "Cone-beam computed tomography (CBCT) using only a few X-ray projection views enables faster scans with lower radiation dose, but the resulting severe under-sampling causes strong artifacts and poor spatial coverage.", "method": "We address these challenges in a unified framework. First, we replace conventional UNet/ResNet encoders with TransUNet, a hybrid CNN-Transformer model. We adapt TransUNet to CBCT by combining multi-scale features, querying view-specific features per 3D point, and adding a lightweight attenuation-prediction head. Second, we introduce a neighbor-aware Point Transformer to enforce volumetric coherence.", "result": "This yields Trans-CBCT, which surpasses prior baselines by 1.17 dB PSNR and 0.0163 SSIM on the LUNA16 dataset with six views. The resulting model, Trans$^2$-CBCT, provides an additional gain of 0.63 dB PSNR and 0.0117 SSIM.", "conclusion": "Experiments on LUNA16 and ToothFairy show consistent gains from six to ten views, validating the effectiveness of combining CNN-Transformer features with point-based geometry reasoning for sparse-view CBCT reconstruction."}}
{"id": "2506.17419", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.17419", "abs": "https://arxiv.org/abs/2506.17419", "authors": ["Jinhao Duan", "James Diffenderfer", "Sandeep Madireddy", "Tianlong Chen", "Bhavya Kailkhura", "Kaidi Xu"], "title": "UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making", "comment": "19 pages, 5 figures, 4 tables", "summary": "As Large Language Models (LLMs) are integrated into safety-critical\napplications involving sequential decision-making in the real world, it is\nessential to know when to trust LLM decisions. Existing LLM Uncertainty\nQuantification (UQ) methods are primarily designed for single-turn\nquestion-answering formats, resulting in multi-step decision-making scenarios,\ne.g., LLM agentic system, being underexplored. In this paper, we introduce a\nprincipled, information-theoretic framework that decomposes LLM sequential\ndecision uncertainty into two parts: (i) internal uncertainty intrinsic to the\ncurrent decision, which is focused on existing UQ methods, and (ii) extrinsic\nuncertainty, a Mutual-Information (MI) quantity describing how much uncertainty\nshould be inherited from preceding decisions. We then propose UProp, an\nefficient and effective extrinsic uncertainty estimator that converts the\ndirect estimation of MI to the estimation of Pointwise Mutual Information (PMI)\nover multiple Trajectory-Dependent Decision Processes (TDPs). UProp is\nevaluated over extensive multi-step decision-making benchmarks, e.g.,\nAgentBench and HotpotQA, with state-of-the-art LLMs, e.g., GPT-4.1 and\nDeepSeek-V3. Experimental results demonstrate that UProp significantly\noutperforms existing single-turn UQ baselines equipped with thoughtful\naggregation strategies. Moreover, we provide a comprehensive analysis of UProp,\nincluding sampling efficiency, potential applications, and intermediate\nuncertainty propagation, to demonstrate its effectiveness. Codes will be\navailable at https://github.com/jinhaoduan/UProp.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86 UProp\uff0c\u4e00\u79cd\u7528\u4e8e\u91cf\u5316 LLM \u5728\u591a\u6b65\u51b3\u7b56\u4e2d\u4e0d\u786e\u5b9a\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684 LLM \u4e0d\u786e\u5b9a\u6027\u91cf\u5316 (UQ) \u65b9\u6cd5\u4e3b\u8981\u8bbe\u8ba1\u7528\u4e8e\u5355\u8f6e\u95ee\u7b54\u683c\u5f0f\uff0c\u5bfc\u81f4\u591a\u6b65\u51b3\u7b56\u573a\u666f\uff08\u4f8b\u5982\uff0cLLM \u4e3b\u52a8\u7cfb\u7edf\uff09\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u56e0\u6b64\uff0c\u77e5\u9053\u4f55\u65f6\u4fe1\u4efb LLM \u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u6846\u67b6\uff0c\u5c06 LLM \u5e8f\u8d2f\u51b3\u7b56\u4e0d\u786e\u5b9a\u6027\u5206\u89e3\u4e3a\u5185\u90e8\u4e0d\u786e\u5b9a\u6027\u548c\u5916\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5916\u90e8\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5668 UProp\uff0c\u5b83\u5c06 MI \u7684\u76f4\u63a5\u4f30\u8ba1\u8f6c\u6362\u4e3a\u591a\u4e2a\u8f68\u8ff9\u4f9d\u8d56\u51b3\u7b56\u8fc7\u7a0b (TDP) \u4e0a\u9010\u70b9\u4e92\u4fe1\u606f (PMI) \u7684\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u914d\u5907\u5468\u5230\u805a\u5408\u7b56\u7565\u7684 UProp \u660e\u663e\u4f18\u4e8e\u73b0\u6709\u7684\u5355\u8f6e UQ \u57fa\u7ebf\u3002", "conclusion": "UProp\u5728\u591a\u6b65\u51b3\u7b56\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u5355\u8f6e UQ \u57fa\u7ebf\uff0c\u5e76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5206\u6790\uff0c\u5305\u62ec\u62bd\u6837\u6548\u7387\u3001\u6f5c\u5728\u5e94\u7528\u548c\u4e2d\u95f4\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\uff0c\u4ee5\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2506.17667", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17667", "abs": "https://arxiv.org/abs/2506.17667", "authors": ["Lintao Wang", "Encheng Su", "Jiaqi Liu", "Pengze Li", "Peng Xia", "Jiabei Xiao", "Wenlong Zhang", "Xinnan Dai", "Xi Chen", "Yuan Meng", "Mingyu Ding", "Lei Bai", "Wanli Ouyang", "Shixiang Tang", "Aoran Wang", "Xinzhu Ma"], "title": "PhysUniBench: An Undergraduate-Level Physics Reasoning Benchmark for Multimodal Models", "comment": null, "summary": "Physics problem-solving is a challenging domain for large AI models,\nrequiring integration of conceptual understanding, mathematical reasoning, and\ninterpretation of physical diagrams. Current evaluation methodologies show\nnotable limitations in capturing the breadth and complexity of\nundergraduate-level physics, underscoring the need for more rigorous\nassessments. To this end, we present PhysUniBench, a large-scale multimodal\nbenchmark designed to evaluate and improve the reasoning capabilities of\nmultimodal large language models (MLLMs) specifically on undergraduate-level\nphysics problems. PhysUniBench consists of 3,304 physics questions spanning 8\nmajor sub-disciplines of physics, each accompanied by one visual diagrams. The\nbenchmark includes both open-ended and multiple-choice questions,\nsystematically curated and difficulty-rated through an iterative\nmodel-in-the-loop process. The benchmark's construction involved a rigorous\nmulti-stage process, including multiple roll-outs, expert-level evaluation,\nautomated filtering of easily solved problems, and a nuanced difficulty grading\nsystem with five levels. Through extensive experiments, we observe that current\nstate-of-the-art models encounter substantial challenges in physics reasoning.\nFor example, GPT-4o mini achieves only about 34.2\\% accuracy in the proposed\nPhysUniBench. These results highlight that current MLLMs struggle with advanced\nphysics reasoning, especially on multi-step problems and those requiring\nprecise diagram interpretation. By providing a broad and rigorous assessment\ntool, PhysUniBench aims to drive progress in AI for Science, encouraging the\ndevelopment of models with stronger physical reasoning, problem-solving skills,\nand multimodal understanding. The benchmark and evaluation scripts are\navailable at https://prismax-team.github.io/PhysUniBenchmark/.", "AI": {"tldr": "PhysUniBench, a large-scale multimodal benchmark, is presented to evaluate the reasoning capabilities of MLLMs on undergraduate-level physics problems. It reveals that current state-of-the-art models struggle with advanced physics reasoning, especially on multi-step problems and those requiring precise diagram interpretation.", "motivation": "Current evaluation methodologies show notable limitations in capturing the breadth and complexity of undergraduate-level physics, underscoring the need for more rigorous assessments.", "method": "PhysUniBench consists of 3,304 physics questions spanning 8 major sub-disciplines of physics, each accompanied by one visual diagrams. The benchmark includes both open-ended and multiple-choice questions, systematically curated and difficulty-rated through an iterative model-in-the-loop process. The benchmark's construction involved a rigorous multi-stage process, including multiple roll-outs, expert-level evaluation, automated filtering of easily solved problems, and a nuanced difficulty grading system with five levels.", "result": "GPT-4o mini achieves only about 34.2% accuracy in the proposed PhysUniBench. These results highlight that current MLLMs struggle with advanced physics reasoning, especially on multi-step problems and those requiring precise diagram interpretation.", "conclusion": "current state-of-the-art models encounter substantial challenges in physics reasoning, especially on multi-step problems and those requiring precise diagram interpretation. By providing a broad and rigorous assessment tool, PhysUniBench aims to drive progress in AI for Science, encouraging the development of models with stronger physical reasoning, problem-solving skills, and multimodal understanding."}}
{"id": "2506.17253", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17253", "abs": "https://arxiv.org/abs/2506.17253", "authors": ["Chenghan Li", "Mingchen Li", "Yipu Liao", "Ruisheng Diao"], "title": "MS-TVNet:A Long-Term Time Series Prediction Method Based on Multi-Scale Dynamic Convolution", "comment": null, "summary": "Long-term time series prediction has predominantly relied on Transformer and\nMLP models, while the potential of convolutional networks in this domain\nremains underexplored. To address this gap, we introduce a novel multi-scale\ntime series reshape module, which effectively captures the relationships among\nmulti-period patches and variable dependencies. Building upon this module, we\npropose MS-TVNet, a multi-scale 3D dynamic convolutional neural network.\nThrough comprehensive evaluations on diverse datasets, MS-TVNet demonstrates\nsuperior performance compared to baseline models, achieving state-of-the-art\n(SOTA) results in long-term time series prediction. Our findings highlight the\neffectiveness of leveraging convolutional networks for capturing complex\ntemporal patterns, suggesting a promising direction for future research in this\nfield.The code is realsed on https://github.com/Curyyfaust/TVNet.", "AI": {"tldr": "This paper introduces MS-TVNet, a multi-scale 3D dynamic convolutional neural network, for long-term time series prediction. It outperforms existing methods by effectively capturing complex temporal patterns using convolutional networks.", "motivation": "Long-term time series prediction has predominantly relied on Transformer and MLP models, while the potential of convolutional networks in this domain remains underexplored. To address this gap", "method": "introduce a novel multi-scale time series reshape module, propose MS-TVNet, a multi-scale 3D dynamic convolutional neural network", "result": "MS-TVNet demonstrates superior performance compared to baseline models, achieving state-of-the-art (SOTA) results in long-term time series prediction", "conclusion": "MS-TVNet demonstrates superior performance compared to baseline models, achieving state-of-the-art (SOTA) results in long-term time series prediction. Our findings highlight the effectiveness of leveraging convolutional networks for capturing complex temporal patterns, suggesting a promising direction for future research in this field."}}
{"id": "2506.17966", "categories": ["cs.IR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17966", "abs": "https://arxiv.org/abs/2506.17966", "authors": ["Wangyu Wu", "Zhenhong Chen", "Xianglin Qiu", "Siqi Song", "Xiaowei Huang", "Fei Ma", "Jimin Xiao"], "title": "LLM-Enhanced Multimodal Fusion for Cross-Domain Sequential Recommendation", "comment": "arXiv admin note: substantial text overlap with arXiv:2504.15085", "summary": "Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by\nleveraging historical interactions across multiple domains, focusing on\nmodeling cross-domain preferences and capturing both intra- and inter-sequence\nitem relationships. We propose LLM-Enhanced Multimodal Fusion for Cross-Domain\nSequential Recommendation (LLM-EMF), a novel and advanced approach that\nenhances textual information with Large Language Models (LLM) knowledge and\nsignificantly improves recommendation performance through the fusion of visual\nand textual data. Using the frozen CLIP model, we generate image and text\nembeddings, thereby enriching item representations with multimodal data. A\nmultiple attention mechanism jointly learns both single-domain and cross-domain\npreferences, effectively capturing and understanding complex user interests\nacross diverse domains. Evaluations conducted on four e-commerce datasets\ndemonstrate that LLM-EMF consistently outperforms existing methods in modeling\ncross-domain user preferences, thereby highlighting the effectiveness of\nmultimodal data integration and its advantages in enhancing sequential\nrecommendation systems. Our source code will be released.", "AI": {"tldr": "LLM-EMF\uff1a\u4e00\u79cd\u7528\u4e8e\u8de8\u57df\u5e8f\u5217\u63a8\u8350\u7684 LLM \u589e\u5f3a\u578b\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u6765\u589e\u5f3a\u6587\u672c\u4fe1\u606f\uff0c\u4ece\u800c\u663e\u7740\u63d0\u9ad8\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u8de8\u9886\u57df\u5e8f\u5217\u63a8\u8350 (CDSR) \u901a\u8fc7\u5229\u7528\u8de8\u591a\u4e2a\u9886\u57df\u7684\u5386\u53f2\u4ea4\u4e92\u6765\u9884\u6d4b\u7528\u6237\u884c\u4e3a\uff0c\u91cd\u70b9\u662f\u5efa\u6a21\u8de8\u9886\u57df\u504f\u597d\u5e76\u6355\u83b7\u5e8f\u5217\u5185\u548c\u5e8f\u5217\u95f4\u9879\u76ee\u5173\u7cfb\u3002", "method": "LLM-EMF\uff0c\u4e00\u79cd\u65b0\u9896\u800c\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u77e5\u8bc6\u589e\u5f3a\u6587\u672c\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u7684\u878d\u5408\u663e\u7740\u63d0\u9ad8\u63a8\u8350\u6027\u80fd\u3002\u4f7f\u7528\u51bb\u7ed3\u7684 CLIP \u6a21\u578b\uff0c\u6211\u4eec\u751f\u6210\u56fe\u50cf\u548c\u6587\u672c\u5d4c\u5165\uff0c\u4ece\u800c\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\u4e30\u5bcc\u9879\u76ee\u8868\u793a\u3002", "result": "\u5728\u56db\u4e2a\u7535\u5b50\u5546\u52a1\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u8bc4\u4f30\u8868\u660e\uff0cLLM-EMF \u5728\u5efa\u6a21\u8de8\u9886\u57df\u7528\u6237\u504f\u597d\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LLM-EMF\u5728\u5efa\u6a21\u8de8\u9886\u57df\u7528\u6237\u504f\u597d\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7a81\u51fa\u4e86\u591a\u6a21\u6001\u6570\u636e\u96c6\u6210\u5728\u589e\u5f3a\u5e8f\u5217\u63a8\u8350\u7cfb\u7edf\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u4f18\u52bf\u3002"}}
{"id": "2506.17439", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17439", "abs": "https://arxiv.org/abs/2506.17439", "authors": ["Nisar Ahmed", "Gulshan Saleem", "Hafiz Muhammad Shahzad Asif", "Muhammad Usman Younus", "Kalsoom Safdar"], "title": "Enhancing Wireless Device Identification through RF Fingerprinting: Leveraging Transient Energy Spectrum Analysis", "comment": "Submitted in Wireless Personal Communications", "summary": "In recent years, the rapid growth of the Internet of Things technologies and\nthe widespread adoption of 5G wireless networks have led to an exponential\nincrease in the number of radiation devices operating in complex\nelectromagnetic environments. A key challenge in managing and securing these\ndevices is accurate identification and classification. To address this\nchallenge, specific emitter identification techniques have emerged as a\npromising solution that aims to provide reliable and efficient means of\nidentifying individual radiation devices in a unified and standardized manner.\nThis research proposes an approach that leverages transient energy spectrum\nanalysis using the General Linear Chirplet Transform to extract features from\nRF devices. A dataset comprising nine RF devices is utilized, with each sample\ncontaining 900 attributes and a total of 1080 equally distributed samples\nacross the devices. These features are then used in a classification modeling\nframework. To overcome the limitations of conventional machine learning\nmethods, we introduce a hybrid deep learning model called the CNN-Bi-GRU for\nlearning the identification of RF devices based on their transient\ncharacteristics. The proposed approach provided a 10-fold cross-validation\nperformance with a precision of 99.33%, recall of 99.53%, F1-score of 99.43%,\nand classification accuracy of 99.17%. The results demonstrate the promising\nclassification performance of the CNN-Bi-GRU approach, indicating its\nsuitability for accurately identifying RF devices based on their transient\ncharacteristics and its potential for enhancing device identification and\nclassification in complex wireless environments.", "AI": {"tldr": "This paper introduces a CNN-Bi-GRU model for RF device identification using transient characteristics, achieving high accuracy and showing potential for enhancing device identification in complex wireless environments.", "motivation": "Accurate identification and classification of radiation devices operating in complex electromagnetic environments is a key challenge in managing and securing these devices, driven by the growth of IoT and 5G.", "method": "A hybrid deep learning model called CNN-Bi-GRU is used for learning the identification of RF devices based on their transient characteristics, with features extracted using transient energy spectrum analysis via the General Linear Chirplet Transform.", "result": "The proposed approach achieved a 10-fold cross-validation performance with a precision of 99.33%, recall of 99.53%, F1-score of 99.43%, and classification accuracy of 99.17%.", "conclusion": "The CNN-Bi-GRU model demonstrates promising classification performance, achieving high precision, recall, F1-score, and accuracy in identifying RF devices based on transient characteristics. This indicates its potential for enhancing device identification in complex wireless environments."}}
{"id": "2506.17435", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.17435", "abs": "https://arxiv.org/abs/2506.17435", "authors": ["Alberto Martinez-Serra", "Alejandro De La Fuente", "Nienke Viescher", "Ana S. Cardenal"], "title": "Beyond the Link: Assessing LLMs' ability to Classify Political Content across Global Media", "comment": null, "summary": "The use of large language models (LLMs) is becoming common in the context of\npolitical science, particularly in studies that analyse individuals use of\ndigital media. However, while previous research has demonstrated LLMs ability\nat labelling tasks, the effectiveness of using LLMs to classify political\ncontent (PC) from just URLs is not yet well explored. The work presented in\nthis article bridges this gap by evaluating whether LLMs can accurately\nidentify PC vs. non-PC from both the article text and the URLs from five\ncountries (France, Germany, Spain, the UK, and the US) and different languages.\nUsing cutting-edge LLMs like GPT, Llama, Mistral, Deepseek, Qwen and Gemma, we\nmeasure model performance to assess whether URL-level analysis can be a good\napproximation for full-text analysis of PC, even across different linguistic\nand national contexts. Model outputs are compared with human-labelled articles,\nas well as traditional supervised machine learning techniques, to set a\nbaseline of performance. Overall, our findings suggest the capacity of URLs to\nembed most of the news content, providing a vital perspective on accuracy-cost\nbalancing. We also account for contextual limitations and suggest\nmethodological recommendations to use LLMs within political science studies.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86LLM\u662f\u5426\u53ef\u4ee5\u51c6\u786e\u5730\u4ece\u4e94\u4e2a\u56fd\u5bb6/\u5730\u533a\uff08\u6cd5\u56fd\u3001\u5fb7\u56fd\u3001\u897f\u73ed\u7259\u3001\u82f1\u56fd\u548c\u7f8e\u56fd\uff09\u548c\u4e0d\u540c\u8bed\u8a00\u7684\u6587\u7ae0\u6587\u672c\u548cURL\u4e2d\u8bc6\u522bPC\u4e0e\u975ePC\u3002", "motivation": "\u4ee5\u5f80\u7684\u7814\u7a76\u5df2\u7ecf\u8bc1\u660e\u4e86LLM\u5728\u6807\u8bb0\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u4f46\u662f\u5426\u53ef\u4ee5\u4f7f\u7528LLM\u4ec5\u4eceURL\u4e2d\u5206\u7c7b\u653f\u6cbb\u5185\u5bb9\uff08PC\uff09\u7684\u6709\u6548\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7684\u63a2\u7d22\u3002", "method": "\u4f7f\u7528GPT\u3001Llama\u3001Mistral\u3001Deepseek\u3001Qwen\u548cGemma\u7b49\u5148\u8fdb\u7684LLM\uff0c\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u4ee5\u8bc4\u4f30URL\u7ea7\u522b\u5206\u6790\u662f\u5426\u53ef\u4ee5\u5f88\u597d\u5730\u8fd1\u4f3cPC\u7684\u5168\u6587\u5206\u6790\uff0c\u5373\u4f7f\u5728\u4e0d\u540c\u7684\u8bed\u8a00\u548c\u56fd\u5bb6/\u5730\u533a\u73af\u5883\u4e2d\u4e5f\u662f\u5982\u6b64\u3002\u5c06\u6a21\u578b\u8f93\u51fa\u4e0e\u4eba\u5de5\u6807\u8bb0\u7684\u6587\u7ae0\u4ee5\u53ca\u4f20\u7edf\u7684\u76d1\u7763\u673a\u5668\u5b66\u4e60\u6280\u672f\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u8bbe\u5b9a\u6027\u80fd\u57fa\u7ebf\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cURL\u53ef\u4ee5\u5d4c\u5165\u5927\u591a\u6570\u65b0\u95fb\u5185\u5bb9\uff0c\u4e3a\u51c6\u786e\u6027-\u6210\u672c\u5e73\u8861\u63d0\u4f9b\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u89c6\u89d2\u3002", "conclusion": "URL\u53ef\u4ee5\u5d4c\u5165\u5927\u591a\u6570\u65b0\u95fb\u5185\u5bb9\uff0c\u4e3a\u51c6\u786e\u6027-\u6210\u672c\u5e73\u8861\u63d0\u4f9b\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u89c6\u89d2\u3002\u540c\u65f6\u8003\u8651\u4e86\u4e0a\u4e0b\u6587\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u5728\u653f\u6cbb\u79d1\u5b66\u7814\u7a76\u4e2d\u4f7f\u7528LLM\u63d0\u51fa\u4e86\u65b9\u6cd5\u8bba\u5efa\u8bae\u3002"}}
{"id": "2506.17697", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17697", "abs": "https://arxiv.org/abs/2506.17697", "authors": ["Bohan Tang", "Dezhao Luo", "Jingxuan Chen", "Shaogang Gong", "Jianye Hao", "Jun Wang", "Kun Shao"], "title": "Beyond Syntax: Action Semantics Learning for App Agents", "comment": null, "summary": "The advent of Large Language Models (LLMs) enables the rise of App agents\nthat interpret user intent and operate smartphone Apps through actions such as\nclicking and scrolling. While prompt-based solutions with closed LLM APIs show\npromising ability, they incur heavy compute costs and external API dependency.\nFine-tuning smaller open-source LLMs solves these limitations. However, current\nfine-tuning methods use a syntax learning paradigm that forces agents to\nreproduce exactly the ground truth action strings, leading to\nout-of-distribution (OOD) vulnerability. To fill this gap, we propose Action\nSemantics Learning (ASL), a novel learning framework, where the learning\nobjective is capturing the semantics of the ground truth actions. Specifically,\ninspired by the programming language theory, we define the action semantics for\nApp agents as the state transition induced by the action in the user interface.\nWith this insight, ASL employs a novel SEmantic Estimator (SEE) to compute a\nsemantic reward to train the App agents in generating actions aligned with the\nsemantics of ground truth actions, even when the syntactic forms differ. To\nsupport the effectiveness of ASL, we theoretically demonstrate the superior\nrobustness of ASL for the OOD problem compared with the existing syntax\nlearning paradigm. Extensive experiments on offline and online smartphone App\noperation benchmarks show that ASL significantly improves the accuracy and\ngeneralisation of App agents over existing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86Action Semantics Learning (ASL) \u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8App\u4ee3\u7406\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5fae\u8c03\u65b9\u6cd5\u4f7f\u7528\u8bed\u6cd5\u5b66\u4e60\u8303\u5f0f\uff0c\u5bfc\u81f4\u5206\u5e03\u5916\uff08OOD\uff09\u6f0f\u6d1e\u3002", "method": "Action Semantics Learning (ASL)", "result": "ASL\u5728\u79bb\u7ebf\u548c\u5728\u7ebf\u667a\u80fd\u624b\u673aApp\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86App\u4ee3\u7406\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u6027\u3002", "conclusion": "ASL\u663e\u8457\u63d0\u9ad8\u4e86App\u4ee3\u7406\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u6027\u3002"}}
{"id": "2506.17254", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17254", "abs": "https://arxiv.org/abs/2506.17254", "authors": ["Shaoang Li", "Jian Li"], "title": "Keeping Up with the Models: Online Deployment and Routing of LLMs at Scale", "comment": null, "summary": "The rapid pace at which new large language models (LLMs) appear -- and older\nones become obsolete -- forces LLM service providers to juggle a streaming\ninventory of models while respecting tight deployment capacity and per-query\ncost budgets. We cast the reality as an online decision problem that couples\nstage-wise deployment, made at fixed maintenance windows, with per-query\nrouting among the models kept live. We introduce StageRoute, a hierarchical\nalgorithm that (i) optimistically selects up to $M_max$ models for the next\nstage using reward upper-confidence and cost lower-confidence bounds, then (ii)\nsolves a budget-constrained bandit sub-problem to route each incoming query. We\nprove that StageRoute achieves a regret of order $T^{2/3}$ and provide a\nmatching lower bound, thereby establishing its near-optimality. Moreover, our\nexperiments confirm the theory, demonstrating that StageRoute performs close to\nthe optimum in practical settings.", "AI": {"tldr": "This paper introduces StageRoute, a near-optimal algorithm for managing LLM deployment and routing under capacity and cost constraints, achieving a regret of order $T^{2/3}.", "motivation": "The rapid pace at which new large language models (LLMs) appear -- and older ones become obsolete -- forces LLM service providers to juggle a streaming inventory of models while respecting tight deployment capacity and per-query cost budgets. We cast the reality as an online decision problem that couples stage-wise deployment, made at fixed maintenance windows, with per-query routing among the models kept live.", "method": "StageRoute, a hierarchical algorithm that (i) optimistically selects up to $M_max$ models for the next stage using reward upper-confidence and cost lower-confidence bounds, then (ii) solves a budget-constrained bandit sub-problem to route each incoming query.", "result": "StageRoute achieves a regret of order $T^{2/3}$ and provide a matching lower bound, thereby establishing its near-optimality. Moreover, our experiments confirm the theory, demonstrating that StageRoute performs close to the optimum in practical settings.", "conclusion": "StageRoute achieves a regret of order $T^{2/3}$ and provide a matching lower bound, thereby establishing its near-optimality. Moreover, experiments confirm the theory, demonstrating that StageRoute performs close to the optimum in practical settings."}}
{"id": "2506.18297", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2506.18297", "abs": "https://arxiv.org/abs/2506.18297", "authors": ["Shahil Kumar", "Manu Pande", "Anay Yatin Damle"], "title": "Comparative Analysis of Lion and AdamW Optimizers for Cross-Encoder Reranking with MiniLM, GTE, and ModernBERT", "comment": null, "summary": "Modern information retrieval systems often employ a two-stage pipeline: an\nefficient initial retrieval stage followed by a computationally intensive\nreranking stage. Cross-encoders have shown strong effectiveness for reranking\ndue to their deep analysis of query-document pairs. This paper studies the\nimpact of the Lion optimizer, a recent alternative to AdamW, during fine-tuning\nof cross-encoder rerankers. We fine-tune three transformer models-MiniLM, GTE,\nand ModernBERT-on the MS MARCO passage ranking dataset using both optimizers.\nGTE and ModernBERT support extended context lengths (up to 8192 tokens). We\nevaluate effectiveness using TREC 2019 Deep Learning Track and MS MARCO dev set\n(MRR@10). Experiments, run on the Modal cloud platform, reveal that ModernBERT\nwith Lion achieves the best NDCG@10 (0.7225) and MAP (0.5121) on TREC DL 2019,\nwhile MiniLM with Lion ties ModernBERT for MRR@10 (0.5988) on MS MARCO dev.\nLion also provides superior GPU efficiency, improving utilization by 2.67% to\n10.33% across models. We analyze performance trends using standard IR metrics\nand discuss the optimizer's impact on training dynamics across architectures.", "AI": {"tldr": "This paper studies the impact of the Lion optimizer during fine-tuning of cross-encoder rerankers.", "motivation": "Cross-encoders have shown strong effectiveness for reranking due to their deep analysis of query-document pairs. This paper studies the impact of the Lion optimizer, a recent alternative to AdamW, during fine-tuning of cross-encoder rerankers.", "method": "fine-tune three transformer models-MiniLM, GTE, and ModernBERT-on the MS MARCO passage ranking dataset using both optimizers", "result": "ModernBERT with Lion achieves the best NDCG@10 and MAP on TREC DL 2019, while MiniLM with Lion ties ModernBERT for MRR@10 on MS MARCO dev. Lion also provides superior GPU efficiency, improving utilization by 2.67% to 10.33% across models.", "conclusion": "ModernBERT with Lion achieves the best NDCG@10 (0.7225) and MAP (0.5121) on TREC DL 2019, while MiniLM with Lion ties ModernBERT for MRR@10 (0.5988) on MS MARCO dev. Lion also provides superior GPU efficiency, improving utilization by 2.67% to 10.33% across models."}}
{"id": "2506.17455", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17455", "abs": "https://arxiv.org/abs/2506.17455", "authors": ["Taufikur Rahman Fuad", "Sabbir Ahmed", "Shahriar Ivan"], "title": "AQUA20: A Benchmark Dataset for Underwater Species Classification under Challenging Conditions", "comment": "Submitted to AJSE Springer", "summary": "Robust visual recognition in underwater environments remains a significant\nchallenge due to complex distortions such as turbidity, low illumination, and\nocclusion, which severely degrade the performance of standard vision systems.\nThis paper introduces AQUA20, a comprehensive benchmark dataset comprising\n8,171 underwater images across 20 marine species reflecting real-world\nenvironmental challenges such as illumination, turbidity, occlusions, etc.,\nproviding a valuable resource for underwater visual understanding. Thirteen\nstate-of-the-art deep learning models, including lightweight CNNs (SqueezeNet,\nMobileNetV2) and transformer-based architectures (ViT, ConvNeXt), were\nevaluated to benchmark their performance in classifying marine species under\nchallenging conditions. Our experimental results show ConvNeXt achieving the\nbest performance, with a Top-3 accuracy of 98.82% and a Top-1 accuracy of\n90.69%, as well as the highest overall F1-score of 88.92% with moderately large\nparameter size. The results obtained from our other benchmark models also\ndemonstrate trade-offs between complexity and performance. We also provide an\nextensive explainability analysis using GRAD-CAM and LIME for interpreting the\nstrengths and pitfalls of the models. Our results reveal substantial room for\nimprovement in underwater species recognition and demonstrate the value of\nAQUA20 as a foundation for future research in this domain. The dataset is\npublicly available at: https://huggingface.co/datasets/taufiktrf/AQUA20.", "AI": {"tldr": "AQUA20 dataset is introduced to benchmark underwater species recognition, ConvNeXt performs the best.", "motivation": "Robust visual recognition in underwater environments is challenging due to complex distortions", "method": "thirteen state-of-the-art deep learning models, including lightweight CNNs (SqueezeNet, MobileNetV2) and transformer-based architectures (ViT, ConvNeXt)", "result": "ConvNeXt achieves the best performance, with a Top-3 accuracy of 98.82% and a Top-1 accuracy of 90.69%, as well as the highest overall F1-score of 88.92%", "conclusion": "underwater species recognition can be improved and AQUA20 is a good foundation for future research"}}
{"id": "2506.17459", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.17459", "abs": "https://arxiv.org/abs/2506.17459", "authors": ["Siyu Liang", "Gina-Anne Levow"], "title": "Breaking the Transcription Bottleneck: Fine-tuning ASR Models for Extremely Low-Resource Fieldwork Languages", "comment": null, "summary": "Automatic Speech Recognition (ASR) has reached impressive accuracy for\nhigh-resource languages, yet its utility in linguistic fieldwork remains\nlimited. Recordings collected in fieldwork contexts present unique challenges,\nincluding spontaneous speech, environmental noise, and severely constrained\ndatasets from under-documented languages. In this paper, we benchmark the\nperformance of two fine-tuned multilingual ASR models, MMS and XLS-R, on five\ntypologically diverse low-resource languages with control of training data\nduration. Our findings show that MMS is best suited when extremely small\namounts of training data are available, whereas XLS-R shows parity performance\nonce training data exceed one hour. We provide linguistically grounded analysis\nfor further provide insights towards practical guidelines for field linguists,\nhighlighting reproducible ASR adaptation approaches to mitigate the\ntranscription bottleneck in language documentation.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e24\u79cd\u591a\u8bed\u8a00ASR\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u6027\u80fd\uff0c\u4e3a\u5b9e\u5730\u8bed\u8a00\u5b66\u5bb6\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684ASR\u8c03\u6574\u65b9\u6cd5\uff0c\u4ee5\u51cf\u8f7b\u8bed\u8a00\u6587\u6863\u4e2d\u7684\u8f6c\u5f55\u74f6\u9888\u3002", "motivation": "\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b(ASR)\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u65b9\u9762\u5df2\u8fbe\u5230\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u51c6\u786e\u7387\uff0c\u4f46\u5176\u5728\u8bed\u8a00\u5b66\u9886\u57df\u4e2d\u7684\u5e94\u7528\u4ecd\u7136\u6709\u9650\u3002\u5728\u5b9e\u5730\u8c03\u67e5\u73af\u5883\u4e2d\u6536\u96c6\u7684\u5f55\u97f3\u9762\u4e34\u7740\u72ec\u7279\u7684\u6311\u6218\uff0c\u5305\u62ec\u81ea\u53d1\u6027\u8bed\u97f3\u3001\u73af\u5883\u566a\u58f0\u548c\u6765\u81ea\u672a\u5145\u5206\u8bb0\u5f55\u8bed\u8a00\u7684\u4e25\u91cd\u53d7\u9650\u7684\u6570\u636e\u96c6\u3002", "method": "\u5bf9\u4e94\u79cd\u7c7b\u578b\u4e0d\u540c\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u8bc4\u4f30\u5fae\u8c03\u7684\u591a\u8bed\u8a00ASR\u6a21\u578bMMS\u548cXLS-R\u7684\u6027\u80fd\uff0c\u5e76\u63a7\u5236\u8bad\u7ec3\u6570\u636e\u7684\u65f6\u957f\u3002", "result": "MMS\u5728\u6781\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u53ef\u7528\u65f6\u6700\u5408\u9002\uff0c\u800cXLS-R\u5728\u8bad\u7ec3\u6570\u636e\u8d85\u8fc7\u4e00\u5c0f\u65f6\u540e\u8868\u73b0\u51fa\u540c\u7b49\u6027\u80fd\u3002", "conclusion": "MMS\u6700\u9002\u5408\u6781\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\uff0c\u800cXLS-R\u5728\u8bad\u7ec3\u6570\u636e\u8d85\u8fc7\u4e00\u5c0f\u65f6\u540e\u8868\u73b0\u76f8\u5f53\u3002"}}
{"id": "2506.17784", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17784", "abs": "https://arxiv.org/abs/2506.17784", "authors": ["Song Wang", "Zhen Tan", "Zihan Chen", "Shuang Zhou", "Tianlong Chen", "Jundong Li"], "title": "AnyMAC: Cascading Flexible Multi-Agent Collaboration via Next-Agent Prediction", "comment": null, "summary": "Recent progress in large language model (LLM)-based multi-agent collaboration\nhighlights the power of structured communication in enabling collective\nintelligence. However, existing methods largely rely on static or graph-based\ninter-agent topologies, lacking the potential adaptability and flexibility in\ncommunication. In this work, we propose a new framework that rethinks\nmulti-agent coordination through a sequential structure rather than a graph\nstructure, offering a significantly larger topology space for multi-agent\ncommunication. Our method focuses on two key directions: (1) Next-Agent\nPrediction, which selects the most suitable agent role at each step, and (2)\nNext-Context Selection (NCS), which enables each agent to selectively access\nrelevant information from any previous step. Together, these components\nconstruct task-adaptive communication pipelines that support both role\nflexibility and global information flow. Extensive evaluations across multiple\nbenchmarks demonstrate that our approach achieves superior performance while\nsubstantially reducing communication overhead.", "AI": {"tldr": "rethinks multi-agent coordination through a sequential structure, offering a significantly larger topology space for multi-agent communication", "motivation": "existing methods largely rely on static or graph-based inter-agent topologies, lacking the potential adaptability and flexibility in communication", "method": "propose a new framework that rethinks multi-agent coordination through a sequential structure rather than a graph structure, focusing on Next-Agent Prediction and Next-Context Selection", "result": "construct task-adaptive communication pipelines that support both role flexibility and global information flow", "conclusion": "achieves superior performance while substantially reducing communication overhead"}}
{"id": "2506.17255", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17255", "abs": "https://arxiv.org/abs/2506.17255", "authors": ["Sunan Zou", "Ziyun Zhang", "Xueting Sun", "Guojie Luo"], "title": "UltraSketchLLM: Saliency-Driven Sketching for Ultra-Low Bit LLM Compression", "comment": null, "summary": "The rapid growth of large language models (LLMs) has outpaced the memory\nconstraints of edge devices, necessitating extreme weight compression beyond\nthe 1-bit limit. While quantization reduces model size, it is fundamentally\nlimited to 1 bit per weight. Existing multiple-to-one compression methods\neither rely on mapping tables (inducing memory overhead) or incur severe\naccuracy degradation due to random weight grouping. We introduce\nUltraSketchLLM, an index-free, sketch-based framework that achieves ultra-low\nbit compression (down to 0.5 bits per weight) while preserving model\nperformance. UltraSketchLLM leverages data sketching, a sub-linear\nrepresentation technique from streaming applications, to map multiple weights\nto single values with bounded error. Our approach integrates an underestimate\nAbsMaxMin sketch to minimize relative errors for small weights,\nimportance-aware space allocation to prioritize salient weights, and a\nstraight-through estimator for compression-aware finetuning. Experiments on\nLlama-3.2-1B demonstrate up to 0.5-bit compression with competitive perplexity,\nalongside tolerable latency overhead. UltraSketchLLM offers a practical\nsolution for deploying LLMs in resource-constrained environments.", "AI": {"tldr": "UltraSketchLLM compresses LLMs to ultra-low bitrates (0.5 bits per weight) using a sketching technique, maintaining performance and enabling deployment on edge devices.", "motivation": "The memory constraints of edge devices limit the deployment of large language models (LLMs), requiring weight compression beyond the 1-bit limit. Existing compression methods have limitations in memory overhead or accuracy degradation.", "method": "The paper introduces UltraSketchLLM, an index-free, sketch-based framework using data sketching with underestimate AbsMaxMin sketch, importance-aware space allocation, and a straight-through estimator for compression-aware finetuning.", "result": "Experiments on Llama-3.2-1B demonstrate up to 0.5-bit compression with competitive perplexity and tolerable latency overhead.", "conclusion": "UltraSketchLLM provides a way to compress LLMs to 0.5 bits per weight, making them deployable in resource-constrained environments."}}
{"id": "2506.18309", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18309", "abs": "https://arxiv.org/abs/2506.18309", "authors": ["Lu Wang", "Di Zhang", "Fangkai Yang", "Pu Zhao", "Jianfeng Liu", "Yuefeng Zhan", "Hao Sun", "Qingwei Lin", "Weiwei Deng", "Dongmei Zhang", "Feng Sun", "Qi Zhang"], "title": "LettinGo: Explore User Profile Generation for Recommendation System", "comment": "11 pages, 3 figures", "summary": "User profiling is pivotal for recommendation systems, as it transforms raw\nuser interaction data into concise and structured representations that drive\npersonalized recommendations. While traditional embedding-based profiles lack\ninterpretability and adaptability, recent advances with large language models\n(LLMs) enable text-based profiles that are semantically richer and more\ntransparent. However, existing methods often adhere to fixed formats that limit\ntheir ability to capture the full diversity of user behaviors. In this paper,\nwe introduce LettinGo, a novel framework for generating diverse and adaptive\nuser profiles. By leveraging the expressive power of LLMs and incorporating\ndirect feedback from downstream recommendation tasks, our approach avoids the\nrigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ\nDirect Preference Optimization (DPO) to align the profile generator with\ntask-specific performance, ensuring that the profiles remain adaptive and\neffective. LettinGo operates in three stages: (1) exploring diverse user\nprofiles via multiple LLMs, (2) evaluating profile quality based on their\nimpact in recommendation systems, and (3) aligning the profile generation\nthrough pairwise preference data derived from task performance. Experimental\nresults demonstrate that our framework significantly enhances recommendation\naccuracy, flexibility, and contextual awareness. This work enhances profile\ngeneration as a key innovation for next-generation recommendation systems.", "AI": {"tldr": "LettinGo, a novel framework for generating diverse and adaptive user profiles, enhances recommendation accuracy, flexibility, and contextual awareness.", "motivation": "Existing methods often adhere to fixed formats that limit their ability to capture the full diversity of user behaviors.", "method": "LettinGo operates in three stages: (1) exploring diverse user profiles via multiple LLMs, (2) evaluating profile quality based on their impact in recommendation systems, and (3) aligning the profile generation through pairwise preference data derived from task performance.", "result": "Experimental results demonstrate that our framework significantly enhances recommendation accuracy, flexibility, and contextual awareness.", "conclusion": "This work enhances profile generation as a key innovation for next-generation recommendation systems."}}
{"id": "2506.17457", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17457", "abs": "https://arxiv.org/abs/2506.17457", "authors": ["Dong Xiao", "Guangyao Chen", "Peixi Peng", "Yangru Huang", "Yifan Zhao", "Yongxing Dai", "Yonghong Tian"], "title": "When Every Millisecond Counts: Real-Time Anomaly Detection via the Multimodal Asynchronous Hybrid Network", "comment": "ICML 2025 Spotlight", "summary": "Anomaly detection is essential for the safety and reliability of autonomous\ndriving systems. Current methods often focus on detection accuracy but neglect\nresponse time, which is critical in time-sensitive driving scenarios. In this\npaper, we introduce real-time anomaly detection for autonomous driving,\nprioritizing both minimal response time and high accuracy. We propose a novel\nmultimodal asynchronous hybrid network that combines event streams from event\ncameras with image data from RGB cameras. Our network utilizes the high\ntemporal resolution of event cameras through an asynchronous Graph Neural\nNetwork and integrates it with spatial features extracted by a CNN from RGB\nimages. This combination effectively captures both the temporal dynamics and\nspatial details of the driving environment, enabling swift and precise anomaly\ndetection. Extensive experiments on benchmark datasets show that our approach\noutperforms existing methods in both accuracy and response time, achieving\nmillisecond-level real-time performance.", "AI": {"tldr": "real-time anomaly detection for autonomous driving, prioritizing both minimal response time and high accuracy", "motivation": "Current methods often focus on detection accuracy but neglect response time, which is critical in time-sensitive driving scenarios.", "method": "a novel multimodal asynchronous hybrid network that combines event streams from event cameras with image data from RGB cameras. Our network utilizes the high temporal resolution of event cameras through an asynchronous Graph Neural Network and integrates it with spatial features extracted by a CNN from RGB images", "result": "achieving millisecond-level real-time performance.", "conclusion": "The proposed approach outperforms existing methods in both accuracy and response time, achieving millisecond-level real-time performance."}}
{"id": "2506.17467", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17467", "abs": "https://arxiv.org/abs/2506.17467", "authors": ["Weixin Liang"], "title": "Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems", "comment": "Stanford CS PhD Dissertation", "summary": "Large language models (LLMs) have shown significant potential to change how\nwe write, communicate, and create, leading to rapid adoption across society.\nThis dissertation examines how individuals and institutions are adapting to and\nengaging with this emerging technology through three research directions.\nFirst, I demonstrate how the institutional adoption of AI detectors introduces\nsystematic biases, particularly disadvantaging writers of non-dominant language\nvarieties, highlighting critical equity concerns in AI governance. Second, I\npresent novel population-level algorithmic approaches that measure the\nincreasing adoption of LLMs across writing domains, revealing consistent\npatterns of AI-assisted content in academic peer reviews, scientific\npublications, consumer complaints, corporate communications, job postings, and\ninternational organization press releases. Finally, I investigate LLMs'\ncapability to provide feedback on research manuscripts through a large-scale\nempirical analysis, offering insights into their potential to support\nresearchers who face barriers in accessing timely manuscript feedback,\nparticularly early-career researchers and those from under-resourced settings.", "AI": {"tldr": "This dissertation examines the adoption and impact of LLMs, revealing biases in AI detection, measuring LLM adoption across writing domains, and exploring LLMs' potential to support researchers.", "motivation": "LLMs are rapidly being adopted across society and changing how we write, communicate, and create.", "method": "The dissertation employs population-level algorithmic approaches and large-scale empirical analysis.", "result": "The study reveals the institutional adoption of AI detectors introduces systematic biases, measures the increasing adoption of LLMs across writing domains, and investigates LLMs' capability to provide feedback on research manuscripts.", "conclusion": "LLMs have the potential to support researchers, especially early-career researchers and those from under-resourced settings, by providing feedback on research manuscripts."}}
{"id": "2506.17788", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "I.2.1; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.17788", "abs": "https://arxiv.org/abs/2506.17788", "authors": ["Shahab Rahimirad", "Guven Gergerli", "Lucia Romero", "Angela Qian", "Matthew Lyle Olson", "Simon Stepputtis", "Joseph Campbell"], "title": "Bayesian Social Deduction with Graph-Informed Language Models", "comment": "32 pages, 10 figures. Under review", "summary": "Social reasoning - inferring unobservable beliefs and intentions from partial\nobservations of other agents - remains a challenging task for large language\nmodels (LLMs). We evaluate the limits of current reasoning language models in\nthe social deduction game Avalon and find that while the largest models\ndemonstrate strong performance, they require extensive test-time inference and\ndegrade sharply when distilled to smaller, real-time-capable variants. To\naddress this, we introduce a hybrid reasoning framework that externalizes\nbelief inference to a structured probabilistic model, while using an LLM for\nlanguage understanding and interaction. Our approach achieves competitive\nperformance with much larger models in Agent-Agent play and, notably, is the\nfirst language agent to defeat human players in a controlled study - achieving\na 67% win rate and receiving higher qualitative ratings than both reasoning\nbaselines and human teammates. We release code, models, and a dataset to\nsupport future work on social reasoning in LLM agents, which can be found at\nhttps://camp-lab-purdue.github.io/bayesian-social-deduction/", "AI": {"tldr": "This paper presents a new approach to social reasoning for LLMs that combines probabilistic modeling with language understanding, achieving state-of-the-art results and defeating human players in the game Avalon.", "motivation": "Social reasoning is a challenging task for large language models (LLMs), and current models require extensive test-time inference and degrade sharply when distilled to smaller variants.", "method": "A hybrid reasoning framework that externalizes belief inference to a structured probabilistic model, while using an LLM for language understanding and interaction.", "result": "The proposed approach achieves competitive performance with much larger models in Agent-Agent play and defeats human players in a controlled study, achieving a 67% win rate and receiving higher qualitative ratings.", "conclusion": "This paper introduces a hybrid reasoning framework that combines a structured probabilistic model for belief inference with an LLM for language understanding and interaction. The approach achieves competitive performance with much larger models and defeats human players in a controlled study."}}
{"id": "2506.17262", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17262", "abs": "https://arxiv.org/abs/2506.17262", "authors": ["Thanadet Chuangsuwanich", "Monisha E. Nongpiur", "Fabian A. Braeu", "Tin A. Tun", "Alexandre Thiery", "Shamira Perera", "Ching Lin Ho", "Martin Buist", "George Barbastathis", "Tin Aung", "Micha\u00ebl J. A. Girard"], "title": "AI to Identify Strain-sensitive Regions of the Optic Nerve Head Linked to Functional Loss in Glaucoma", "comment": null, "summary": "Objective: (1) To assess whether ONH biomechanics improves prediction of\nthree progressive visual field loss patterns in glaucoma; (2) to use\nexplainable AI to identify strain-sensitive ONH regions contributing to these\npredictions.\n  Methods: We recruited 237 glaucoma subjects. The ONH of one eye was imaged\nunder two conditions: (1) primary gaze and (2) primary gaze with IOP elevated\nto ~35 mmHg via ophthalmo-dynamometry. Glaucoma experts classified the subjects\ninto four categories based on the presence of specific visual field defects:\n(1) superior nasal step (N=26), (2) superior partial arcuate (N=62), (3) full\nsuperior hemifield defect (N=25), and (4) other/non-specific defects (N=124).\nAutomatic ONH tissue segmentation and digital volume correlation were used to\ncompute IOP-induced neural tissue and lamina cribrosa (LC) strains.\nBiomechanical and structural features were input to a Geometric Deep Learning\nmodel. Three classification tasks were performed to detect: (1) superior nasal\nstep, (2) superior partial arcuate, (3) full superior hemifield defect. For\neach task, the data were split into 80% training and 20% testing sets. Area\nunder the curve (AUC) was used to assess performance. Explainable AI techniques\nwere employed to highlight the ONH regions most critical to each\nclassification.\n  Results: Models achieved high AUCs of 0.77-0.88, showing that ONH strain\nimproved VF loss prediction beyond morphology alone. The inferior and\ninferotemporal rim were identified as key strain-sensitive regions,\ncontributing most to visual field loss prediction and showing progressive\nexpansion with increasing disease severity.\n  Conclusion and Relevance: ONH strain enhances prediction of glaucomatous VF\nloss patterns. Neuroretinal rim, rather than the LC, was the most critical\nregion contributing to model predictions.", "AI": {"tldr": "ONH strain enhances prediction of glaucomatous VF loss patterns, with the neuroretinal rim being the most critical region.", "motivation": "To assess whether ONH biomechanics improves prediction of three progressive visual field loss patterns in glaucoma; to use explainable AI to identify strain-sensitive ONH regions contributing to these predictions.", "method": "ONH of glaucoma subjects was imaged under two conditions: primary gaze and primary gaze with IOP elevated to ~35 mmHg. Automatic ONH tissue segmentation and digital volume correlation were used to compute IOP-induced neural tissue and lamina cribrosa (LC) strains. Biomechanical and structural features were input to a Geometric Deep Learning model. Three classification tasks were performed to detect: (1) superior nasal step, (2) superior partial arcuate, (3) full superior hemifield defect.", "result": "Models achieved high AUCs of 0.77-0.88, showing that ONH strain improved VF loss prediction beyond morphology alone. The inferior and inferotemporal rim were identified as key strain-sensitive regions, contributing most to visual field loss prediction and showing progressive expansion with increasing disease severity.", "conclusion": "ONH strain enhances prediction of glaucomatous VF loss patterns. Neuroretinal rim, rather than the LC, was the most critical region contributing to model predictions."}}
{"id": "2506.18311", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.18311", "abs": "https://arxiv.org/abs/2506.18311", "authors": ["Hoang-An Trieu", "Dinh-Truong Do", "Chau Nguyen", "Vu Tran", "Minh Le Nguyen"], "title": "Enhancing Document Retrieval in COVID-19 Research: Leveraging Large Language Models for Hidden Relation Extraction", "comment": "In the Proceedings of SCIDOCA 2024", "summary": "In recent years, with the appearance of the COVID-19 pandemic, numerous\npublications relevant to this disease have been issued. Because of the massive\nvolume of publications, an efficient retrieval system is necessary to provide\nresearchers with useful information if an unexpected pandemic happens so\nsuddenly, like COVID-19. In this work, we present a method to help the\nretrieval system, the Covrelex-SE system, to provide more high-quality search\nresults. We exploited the power of the large language models (LLMs) to extract\nthe hidden relationships inside the unlabeled publication that cannot be found\nby the current parsing tools that the system is using. Since then, help the\nsystem to have more useful information during retrieval progress.", "AI": {"tldr": "Uses LLMs to improve COVID-19 publication retrieval by extracting hidden relationships.", "motivation": "The COVID-19 pandemic has led to a massive volume of publications, necessitating an efficient retrieval system to provide researchers with useful information.", "method": "Exploiting large language models (LLMs) to extract hidden relationships inside the unlabeled publication.", "result": "The LLMs help the system have more useful information during retrieval progress.", "conclusion": "This paper presents a method to improve the Covrelex-SE retrieval system by using LLMs to extract hidden relationships within publications."}}
{"id": "2506.17469", "categories": ["cs.CV", "I.5.4; I.2.10"], "pdf": "https://arxiv.org/pdf/2506.17469", "abs": "https://arxiv.org/abs/2506.17469", "authors": ["Thomas Plante St-Cyr", "Fran\u00e7ois Duhaime", "Jean-S\u00e9bastien Dub\u00e9", "Simon Grenier"], "title": "Photogranulometry -- Dataset of soil images with corresponding particle size distributions", "comment": "8 pages, 10 figures, conference", "summary": "Traditional particle size distribution (PSD) analyses create significant\ndowntime and are expensive in labor and maintenance. These drawbacks could be\nalleviated using optical grain size analysis integrated into routine\ngeotechnical laboratory workflow. This paper presents a high-resolution dataset\nof 12,714 images of 321 different soil samples collected in the Montreal,\nQuebec region, alongside their PSD analysis. It is designed to provide a robust\nstarting point for training convolutional neural networks (CNN) in geotechnical\napplications. Soil samples were photographed in a standardized top-view\nposition with a resolution of 45 MP and a minimum scale of 39.4 micrometers per\npixel, both in their moist and dry states. A custom test bench employing 13x9\ninch white aluminum trays, on which the samples are spread in a thin layer, was\nused. For samples exceeding a size limit, a coning and quartering method was\nemployed for mass reduction.", "AI": {"tldr": "A new high-resolution image dataset of soil samples with PSD analysis is presented for training CNNs, addressing the limitations of traditional PSD analyses.", "motivation": "Traditional particle size distribution (PSD) analyses create significant downtime and are expensive in labor and maintenance. These drawbacks could be alleviated using optical grain size analysis integrated into routine geotechnical laboratory workflow.", "method": "Soil samples were photographed in a standardized top-view position with a resolution of 45 MP and a minimum scale of 39.4 micrometers per pixel, both in their moist and dry states. A custom test bench employing 13x9 inch white aluminum trays was used.", "result": "The dataset provides a robust starting point for training convolutional neural networks (CNN) in geotechnical applications.", "conclusion": "This paper introduces a dataset of 12,714 high-resolution images of 321 soil samples from the Montreal region, accompanied by their PSD analysis."}}
{"id": "2506.17506", "categories": ["cs.CL", "cs.OS"], "pdf": "https://arxiv.org/pdf/2506.17506", "abs": "https://arxiv.org/abs/2506.17506", "authors": ["Lesheng Jin", "Zhenyuan Ruan", "Haohui Mai", "Jingbo Shang"], "title": "VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM", "comment": null, "summary": "Modern GPUs evolve rapidly, yet production compilers still rely on\nhand-crafted register allocation heuristics that require substantial re-tuning\nfor each hardware generation. We introduce VeriLocc, a framework that combines\nlarge language models (LLMs) with formal compiler techniques to enable\ngeneralizable and verifiable register allocation across GPU architectures.\nVeriLocc fine-tunes an LLM to translate intermediate representations (MIRs)\ninto target-specific register assignments, aided by static analysis for\ncross-architecture normalization and generalization and a verifier-guided\nregeneration loop to ensure correctness. Evaluated on matrix multiplication\n(GEMM) and multi-head attention (MHA), VeriLocc achieves 85-99% single-shot\naccuracy and near-100% pass@100. Case study shows that VeriLocc discovers more\nperformant assignments than expert-tuned libraries, outperforming rocBLAS by\nover 10% in runtime.", "AI": {"tldr": "VeriLocc, a framework that combines large language models (LLMs) with formal compiler techniques to enable generalizable and verifiable register allocation across GPU architectures", "motivation": "production compilers still rely on hand-crafted register allocation heuristics that require substantial re-tuning for each hardware generation.", "method": "a framework that combines large language models (LLMs) with formal compiler techniques to enable generalizable and verifiable register allocation across GPU architectures. VeriLocc fine-tunes an LLM to translate intermediate representations (MIRs) into target-specific register assignments, aided by static analysis for cross-architecture normalization and generalization and a verifier-guided regeneration loop to ensure correctness.", "result": "VeriLocc achieves 85-99% single-shot accuracy and near-100% pass@100. Case study shows that", "conclusion": "VeriLocc discovers more performant assignments than expert-tuned libraries, outperforming rocBLAS by over 10% in runtime."}}
{"id": "2506.17792", "categories": ["cs.AI", "cs.LO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17792", "abs": "https://arxiv.org/abs/2506.17792", "authors": ["Alexandros Evangelidis", "Gricel V\u00e1zquez", "Simos Gerasimou"], "title": "Efficient Strategy Synthesis for MDPs via Hierarchical Block Decomposition", "comment": null, "summary": "Software-intensive systems, such as software product lines and robotics,\nutilise Markov decision processes (MDPs) to capture uncertainty and analyse\nsequential decision-making problems. Despite the usefulness of conventional\npolicy synthesis methods, they fail to scale to large state spaces. Our\napproach addresses this issue and accelerates policy synthesis in large MDPs by\ndynamically refining the MDP and iteratively selecting the most fragile MDP\nregions for refinement. This iterative procedure offers a balance between\naccuracy and efficiency, as refinement occurs only when necessary. Through a\ncomprehensive empirical evaluation comprising diverse case studies and MDPs up\nto 1M states, we demonstrate significant performance improvements yielded by\nour approach compared to the leading probabilistic model checker PRISM (up to\n2x), thus offering a very competitive solution for real-world policy synthesis\ntasks in larger MDPs.", "AI": {"tldr": "This paper introduces an approach to accelerate policy synthesis in large MDPs by dynamically refining the MDP and iteratively selecting the most fragile MDP regions for refinement.", "motivation": "Conventional policy synthesis methods fail to scale to large state spaces.", "method": "dynamically refining the MDP and iteratively selecting the most fragile MDP regions for refinement", "result": "significant performance improvements yielded by our approach compared to the leading probabilistic model checker PRISM (up to 2x)", "conclusion": "The approach demonstrates significant performance improvements compared to the leading probabilistic model checker PRISM, offering a competitive solution for real-world policy synthesis tasks in larger MDPs."}}
{"id": "2506.17263", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17263", "abs": "https://arxiv.org/abs/2506.17263", "authors": ["Massimiliano Tamborski", "David Abel"], "title": "Memory Allocation in Resource-Constrained Reinforcement Learning", "comment": "RLDM 2025", "summary": "Resource constraints can fundamentally change both learning and\ndecision-making. We explore how memory constraints influence an agent's\nperformance when navigating unknown environments using standard reinforcement\nlearning algorithms. Specifically, memory-constrained agents face a dilemma:\nhow much of their limited memory should be allocated to each of the agent's\ninternal processes, such as estimating a world model, as opposed to forming a\nplan using that model? We study this dilemma in MCTS- and DQN-based algorithms\nand examine how different allocations of memory impact performance in episodic\nand continual learning settings.", "AI": {"tldr": "Memory constraints influence an agent's performance when navigating unknown environments.", "motivation": "Resource constraints can fundamentally change both learning and decision-making.", "method": "MCTS- and DQN-based algorithms", "result": "Examine how different allocations of memory impact performance in episodic and continual learning settings.", "conclusion": "This paper studies the dilemma of memory allocation between world model estimation and planning in memory-constrained agents using MCTS- and DQN-based algorithms."}}
{"id": "2506.18316", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.18316", "abs": "https://arxiv.org/abs/2506.18316", "authors": ["Trieu An", "Long Nguyen", "Minh Le Nguyen"], "title": "Team LA at SCIDOCA shared task 2025: Citation Discovery via relation-based zero-shot retrieval", "comment": "In the Proceedings of SCIDOCA 2025", "summary": "The Citation Discovery Shared Task focuses on predicting the correct citation\nfrom a given candidate pool for a given paragraph. The main challenges stem\nfrom the length of the abstract paragraphs and the high similarity among\ncandidate abstracts, making it difficult to determine the exact paper to cite.\nTo address this, we develop a system that first retrieves the top-k most\nsimilar abstracts based on extracted relational features from the given\nparagraph. From this subset, we leverage a Large Language Model (LLM) to\naccurately identify the most relevant citation. We evaluate our framework on\nthe training dataset provided by the SCIDOCA 2025 organizers, demonstrating its\neffectiveness in citation prediction.", "AI": {"tldr": "A system using relational features and a Large Language Model (LLM) is developed to predict citations from a candidate pool, addressing challenges of abstract length and similarity. It is evaluated on the SCIDOCA 2025 training dataset.", "motivation": "The main challenges stem from the length of the abstract paragraphs and the high similarity among candidate abstracts, making it difficult to determine the exact paper to cite.", "method": "A system is developed that first retrieves the top-k most similar abstracts based on extracted relational features from the given paragraph. From this subset, a Large Language Model (LLM) is leveraged to accurately identify the most relevant citation.", "result": "The system demonstrates its effectiveness in citation prediction.", "conclusion": "The framework's effectiveness in citation prediction is demonstrated on the training dataset provided by the SCIDOCA 2025 organizers."}}
{"id": "2506.17500", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17500", "abs": "https://arxiv.org/abs/2506.17500", "authors": ["Julio Silva-Rodr\u00edguez", "Fereshteh Shakeri", "Houda Bahig", "Jose Dolz", "Ismail Ben Ayed"], "title": "Few-Shot, Now for Real: Medical VLMs Adaptation without Balanced Sets or Validation", "comment": "MICCAI 2025. Code: https://github.com/jusiro/SS-Text", "summary": "Vision-language models (VLMs) are gaining attention in medical image\nanalysis. These are pre-trained on large, heterogeneous data sources, yielding\nrich and transferable representations. Notably, the combination of\nmodality-specialized VLMs with few-shot adaptation has provided fruitful\nresults, enabling the efficient deployment of high-performing solutions.\nHowever, previous works on this topic make strong assumptions about the\ndistribution of adaptation data, which are unrealistic in the medical domain.\nFirst, prior art assumes access to a balanced support set, a condition that\nbreaks the natural imbalance in disease prevalence found in real-world\nscenarios. Second, these works typically assume the presence of an additional\nvalidation set to fix critical hyper-parameters, which is highly\ndata-inefficient. This work challenges these favorable deployment scenarios and\nintroduces a realistic, imbalanced, validation-free adaptation setting. Our\nextensive benchmark across various modalities and downstream tasks demonstrates\nthat current methods systematically compromise their performance when operating\nunder realistic conditions, occasionally even performing worse than zero-shot\ninference. Also, we introduce a training-free linear probe that adaptively\nblends visual and textual supervision. Detailed studies demonstrate that the\nproposed solver is a strong, efficient baseline, enabling robust adaptation in\nchallenging scenarios.", "AI": {"tldr": "\u8fd9\u9879\u5de5\u4f5c\u6311\u6218\u4e86\u5bf9\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u6709\u5229\u5047\u8bbe\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u73b0\u5b9e\u7684\u3001\u4e0d\u5e73\u8861\u7684\u3001\u65e0\u9a8c\u8bc1\u7684\u9002\u5e94\u8bbe\u7f6e\u3002", "motivation": "\u5148\u524d\u5173\u4e8e\u8be5\u4e3b\u9898\u7684\u5de5\u4f5c\u5bf9\u9002\u5e94\u6570\u636e\u7684\u5206\u5e03\u505a\u51fa\u4e86\u5f88\u5f3a\u7684\u5047\u8bbe\uff0c\u8fd9\u5728\u533b\u5b66\u9886\u57df\u662f\u4e0d\u73b0\u5b9e\u7684\u3002\u9996\u5148\uff0c\u5148\u524d\u7684\u6280\u672f\u5047\u8bbe\u53ef\u4ee5\u8bbf\u95ee\u5e73\u8861\u7684\u652f\u6301\u96c6\uff0c\u8fd9\u79cd\u60c5\u51b5\u6253\u7834\u4e86\u73b0\u5b9e\u573a\u666f\u4e2d\u75be\u75c5\u6d41\u884c\u7684\u81ea\u7136\u4e0d\u5e73\u8861\u3002\u5176\u6b21\uff0c\u8fd9\u4e9b\u5de5\u4f5c\u901a\u5e38\u5047\u8bbe\u5b58\u5728\u4e00\u4e2a\u989d\u5916\u7684\u9a8c\u8bc1\u96c6\u6765\u4fee\u590d\u5173\u952e\u7684\u8d85\u53c2\u6570\uff0c\u8fd9\u662f\u975e\u5e38\u6570\u636e\u4f4e\u6548\u7684\u3002\u8fd9\u9879\u5de5\u4f5c\u6311\u6218\u4e86\u8fd9\u4e9b\u6709\u5229\u7684\u90e8\u7f72\u573a\u666f\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u73b0\u5b9e\u7684\u3001\u4e0d\u5e73\u8861\u7684\u3001\u65e0\u9a8c\u8bc1\u7684\u9002\u5e94\u8bbe\u7f6e\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u514d\u8bad\u7ec3\u7684\u7ebf\u6027\u63a2\u9488\uff0c\u53ef\u4ee5\u81ea\u9002\u5e94\u5730\u6df7\u5408\u89c6\u89c9\u548c\u6587\u672c\u76d1\u7763", "result": "\u5728\u5404\u79cd\u6a21\u6001\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8fdb\u884c\u7684\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u5f53\u524d\u7684\u65b9\u6cd5\u5728\u5b9e\u9645\u6761\u4ef6\u4e0b\u8fd0\u884c\u65f6\u4f1a\u7cfb\u7edf\u6027\u5730\u964d\u4f4e\u5176\u6027\u80fd\uff0c\u6709\u65f6\u751a\u81f3\u6bd4\u96f6\u6837\u672c\u63a8\u7406\u66f4\u5dee\u3002", "conclusion": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u514d\u8bad\u7ec3\u7684\u7ebf\u6027\u63a2\u9488\uff0c\u53ef\u4ee5\u81ea\u9002\u5e94\u5730\u6df7\u5408\u89c6\u89c9\u548c\u6587\u672c\u76d1\u7763\u3002\u8be6\u7ec6\u7684\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6c42\u89e3\u5668\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u3001\u9ad8\u6548\u7684\u57fa\u7ebf\uff0c\u80fd\u591f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u4e2d\u5b9e\u73b0\u7a33\u5065\u7684\u9002\u5e94\u3002"}}
{"id": "2506.17525", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17525", "abs": "https://arxiv.org/abs/2506.17525", "authors": ["Mingfei Lau", "Qian Chen", "Yeming Fang", "Tingting Xu", "Tongzhou Chen", "Pavel Golik"], "title": "Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning", "comment": "Accepted by ACL 2025 Main Conference", "summary": "Our quality audit for three widely used public multilingual speech datasets -\nMozilla Common Voice 17.0, FLEURS, and VoxPopuli - shows that in some\nlanguages, these datasets suffer from significant quality issues. We believe\naddressing these issues will make these datasets more useful as training and\nevaluation sets, and improve downstream models. We divide these quality issues\ninto two categories: micro-level and macro-level. We find that macro-level\nissues are more prevalent in less institutionalized, often under-resourced\nlanguages. We provide a case analysis of Taiwanese Southern Min (nan_tw) that\nhighlights the need for proactive language planning (e.g. orthography\nprescriptions, dialect boundary definition) and enhanced data quality control\nin the process of Automatic Speech Recognition (ASR) dataset creation. We\nconclude by proposing guidelines and recommendations to mitigate these issues\nin future dataset development, emphasizing the importance of sociolinguistic\nawareness in creating robust and reliable speech data resources.", "AI": {"tldr": "quality issues in multilingual speech datasets", "motivation": "in some languages, these datasets suffer from significant quality issues. We believe addressing these issues will make these datasets more useful as training and evaluation sets, and improve downstream models", "method": "quality audit for three widely used public multilingual speech datasets - Mozilla Common Voice 17.0, FLEURS, and VoxPopuli", "result": "macro-level issues are more prevalent in less institutionalized, often under-resourced languages. We provide a case analysis of Taiwanese Southern Min (nan_tw) that highlights the need for proactive language planning (e.g. orthography prescriptions, dialect boundary definition) and enhanced data quality control in the process of Automatic Speech Recognition (ASR) dataset creation", "conclusion": "proposing guidelines and recommendations to mitigate these issues in future dataset development, emphasizing the importance of sociolinguistic awareness in creating robust and reliable speech data resources."}}
{"id": "2506.17834", "categories": ["cs.AI", "cs.HC", "I.2.6; H.5.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.17834", "abs": "https://arxiv.org/abs/2506.17834", "authors": ["Carter Blair", "Kate Larson", "Edith Law"], "title": "Reflective Verbal Reward Design for Pluralistic Alignment", "comment": "9 pages, 3 figures, accepted to the IJCAI 2025 Human-Centred AI\n  track. Project repository at: https://osf.io/8yxf2/", "summary": "AI agents are commonly aligned with \"human values\" through reinforcement\nlearning from human feedback (RLHF), where a single reward model is learned\nfrom aggregated human feedback and used to align an agent's behavior. However,\nhuman values are not homogeneous--different people hold distinct and sometimes\nconflicting values. Aggregating feedback into a single reward model risks\ndisproportionately suppressing minority preferences. To address this, we\npresent a novel reward modeling approach for learning individualized reward\nmodels. Our approach uses a language model to guide users through reflective\ndialogues where they critique agent behavior and construct their preferences.\nThis personalized dialogue history, containing the user's reflections and\ncritiqued examples, is then used as context for another language model that\nserves as an individualized reward function (what we call a \"verbal reward\nmodel\") for evaluating new trajectories. In studies with 30 participants, our\nmethod achieved a 9-12% improvement in accuracy over non-reflective verbal\nreward models while being more sample efficient than traditional supervised\nlearning methods.", "AI": {"tldr": "Personalized reward modeling using reflective dialogues and language models improves accuracy and sample efficiency compared to traditional methods.", "motivation": "Human values are not homogeneous, and aggregating feedback into a single reward model risks disproportionately suppressing minority preferences in AI agent alignment.", "method": "A novel reward modeling approach using language models to guide users through reflective dialogues, creating personalized dialogue histories used as context for individualized reward functions.", "result": "Achieved a 9-12% improvement in accuracy over non-reflective verbal reward models and is more sample efficient than traditional supervised learning methods in studies with 30 participants.", "conclusion": "The proposed method achieves a 9-12% improvement in accuracy over non-reflective verbal reward models and is more sample efficient than traditional supervised learning methods."}}
{"id": "2506.17264", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17264", "abs": "https://arxiv.org/abs/2506.17264", "authors": ["Jikai Long", "Zijian Hu", "Xiaodong Yu", "Jianwen Xie", "Zhaozhuo Xu"], "title": "OAT-Rephrase: Optimization-Aware Training Data Rephrasing for Zeroth-Order LLM Fine-Tuning", "comment": null, "summary": "Fine-tuning large language models (LLMs) using zeroth-order optimization (ZO)\noffers a memory-efficient alternative to gradient-based methods but suffers\nfrom slower convergence and unstable optimization due to noisy gradient\nestimates. This paper introduces OAT-Rephrase, an Optimization-Aware Training\ndata rephrasing strategy that leverages an LLM to rephrase training instances\nbased on its understanding of the ZO dynamics, specifically MeZO, derived\ndirectly from its paper. The approach incorporates a dual-stage pipeline\nfeaturing a rewriter LLM and a semantic judge, ensuring all rephrasings retain\ntask relevance and logical consistency. Evaluations across five classification\ntasks and three LLM architectures demonstrate that OAT-Rephrase consistently\nimproves MeZO fine-tuning performance, often narrowing or eliminating the gap\nwith first-order methods. Our findings suggest that optimization-aware\nrephrasing serves as a reusable and low-overhead enhancement for zeroth-order\ntuning regimes.", "AI": {"tldr": "OAT-Rephrase improves zeroth-order LLM fine-tuning by rephrasing training data.", "motivation": "Zeroth-order optimization for fine-tuning LLMs is memory-efficient but has slow convergence and unstable optimization due to noisy gradient estimates.", "method": "Optimization-Aware Training data rephrasing strategy (OAT-Rephrase) using a dual-stage pipeline with a rewriter LLM and a semantic judge.", "result": "OAT-Rephrase improves MeZO fine-tuning performance, often matching first-order methods, across five classification tasks and three LLM architectures.", "conclusion": "Optimization-aware rephrasing enhances zeroth-order tuning."}}
{"id": "2506.18327", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18327", "abs": "https://arxiv.org/abs/2506.18327", "authors": ["Tahsin Alamgir Kheya", "Mohamed Reda Bouadjenek", "Sunil Aryal"], "title": "Bias vs Bias -- Dawn of Justice: A Fair Fight in Recommendation Systems", "comment": null, "summary": "Recommendation systems play a crucial role in our daily lives by impacting\nuser experience across various domains, including e-commerce, job\nadvertisements, entertainment, etc. Given the vital role of such systems in our\nlives, practitioners must ensure they do not produce unfair and imbalanced\nrecommendations. Previous work addressing bias in recommendations overlooked\nbias in certain item categories, potentially leaving some biases unaddressed.\nAdditionally, most previous work on fair re-ranking focused on binary-sensitive\nattributes. In this paper, we address these issues by proposing a\nfairness-aware re-ranking approach that helps mitigate bias in different\ncategories of items. This re-ranking approach leverages existing biases to\ncorrect disparities in recommendations across various demographic groups. We\nshow how our approach can mitigate bias on multiple sensitive attributes,\nincluding gender, age, and occupation. We experimented on three real-world\ndatasets to evaluate the effectiveness of our re-ranking scheme in mitigating\nbias in recommendations. Our results show how this approach helps mitigate\nsocial bias with little to no degradation in performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u516c\u5e73\u611f\u77e5\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u4ee5\u51cf\u8f7b\u63a8\u8350\u7cfb\u7edf\u4e2d\u4e0d\u540c\u9879\u76ee\u7c7b\u522b\u548c\u591a\u4e2a\u654f\u611f\u5c5e\u6027\u7684\u504f\u5dee\u3002", "motivation": "\u4ee5\u5f80\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u504f\u5dee\u7684\u5de5\u4f5c\u5ffd\u7565\u4e86\u67d0\u4e9b\u9879\u76ee\u7c7b\u522b\u4e2d\u7684\u504f\u5dee\uff0c\u5e76\u4e14\u5927\u591a\u6570\u5173\u4e8e\u516c\u5e73\u91cd\u6392\u5e8f\u7684\u5de5\u4f5c\u90fd\u96c6\u4e2d\u5728\u4e8c\u5143\u654f\u611f\u5c5e\u6027\u4e0a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u516c\u5e73\u611f\u77e5\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u73b0\u6709\u504f\u89c1\u6765\u7ea0\u6b63\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u5728\u63a8\u8350\u65b9\u9762\u7684\u5dee\u5f02\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u4ee5\u8bc4\u4f30\u8be5\u91cd\u6392\u5e8f\u65b9\u6848\u5728\u51cf\u8f7b\u63a8\u8350\u504f\u5dee\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u51cf\u5c11\u793e\u4f1a\u504f\u89c1\uff0c\u4e14\u51e0\u4e4e\u4e0d\u964d\u4f4e\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u51cf\u5c11\u793e\u4f1a\u504f\u89c1\uff0c\u4e14\u51e0\u4e4e\u4e0d\u964d\u4f4e\u6027\u80fd\u3002"}}
{"id": "2506.17503", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17503", "abs": "https://arxiv.org/abs/2506.17503", "authors": ["Julio Silva-Rodr\u00edguez", "Ismail Ben Ayed", "Jose Dolz"], "title": "Trustworthy Few-Shot Transfer of Medical VLMs through Split Conformal Prediction", "comment": "MICCAI 2025. Code: https://github.com/jusiro/SCA-T", "summary": "Medical vision-language models (VLMs) have demonstrated unprecedented\ntransfer capabilities and are being increasingly adopted for data-efficient\nimage classification. Despite its growing popularity, its reliability aspect\nremains largely unexplored. This work explores the split conformal prediction\n(SCP) framework to provide trustworthiness guarantees when transferring such\nmodels based on a small labeled calibration set. Despite its potential, the\ngeneralist nature of the VLMs' pre-training could negatively affect the\nproperties of the predicted conformal sets for specific tasks. While common\npractice in transfer learning for discriminative purposes involves an\nadaptation stage, we observe that deploying such a solution for conformal\npurposes is suboptimal since adapting the model using the available calibration\ndata breaks the rigid exchangeability assumptions for test data in SCP. To\naddress this issue, we propose transductive split conformal adaptation (SCA-T),\na novel pipeline for transfer learning on conformal scenarios, which performs\nan unsupervised transductive adaptation jointly on calibration and test data.\nWe present comprehensive experiments utilizing medical VLMs across various\nimage modalities, transfer tasks, and non-conformity scores. Our framework\noffers consistent gains in efficiency and conditional coverage compared to SCP,\nmaintaining the same empirical guarantees.", "AI": {"tldr": "This paper introduces transductive split conformal adaptation (SCA-T) to improve the reliability and efficiency of medical vision-language models in image classification tasks.", "motivation": "Medical vision-language models (VLMs) have demonstrated unprecedented transfer capabilities, but its reliability aspect remains largely unexplored. Adapting the model using the available calibration data breaks the rigid exchangeability assumptions for test data in SCP.", "method": "We propose transductive split conformal adaptation (SCA-T), a novel pipeline for transfer learning on conformal scenarios, which performs an unsupervised transductive adaptation jointly on calibration and test data.", "result": "Comprehensive experiments utilizing medical VLMs across various image modalities, transfer tasks, and non-conformity scores.", "conclusion": "The proposed SCA-T framework offers consistent gains in efficiency and conditional coverage compared to SCP, maintaining the same empirical guarantees."}}
{"id": "2506.17533", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.17533", "abs": "https://arxiv.org/abs/2506.17533", "authors": ["Yuanhao Wu", "Juntong Song", "Hanning Zhang", "Tong Zhang", "Cheng Niu"], "title": "DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning", "comment": null, "summary": "In this paper, we propose DuaShepherd, a novel reward modeling framework that\nintegrates two complementary reward signals, correctness and potential, to\nenhance the mathematical reasoning capabilities of Large Language Models\n(LLMs). While correctness-based signals emphasize identification of stepwise\nerrors, potential-based signals focus on the likelihood of reaching the correct\nfinal answer. We developed an automated pipeline for constructing large-scale\nreward modeling dataset with both signals. A unified, multi-head architecture\nwas explored to train the two reward models in a multi-task setup,\ndemonstrating benefits from learning both correctness and potential in\nparallel. By combining these two signals into a compound probability, our model\nachieves consistent performance improvements across multiple benchmarks.\nEmpirical evaluations on MATH500 and ProcessBench confirm that this combined\nreward significantly outperforms models trained on either reward type alone,\nachieving state-of-the-art performance under comparable resource constraints.", "AI": {"tldr": "DuaShepherd, a novel reward modeling framework, integrates correctness and potential reward signals to improve the mathematical reasoning capabilities of LLMs, achieving state-of-the-art performance.", "motivation": "This paper aims to enhance the mathematical reasoning capabilities of Large Language Models (LLMs).", "method": "The paper developed an automated pipeline for constructing large-scale reward modeling dataset with both correctness and potential signals and explored a unified, multi-head architecture to train the two reward models in a multi-task setup.", "result": "The model achieves state-of-the-art performance on MATH500 and ProcessBench.", "conclusion": "The model achieves consistent performance improvements across multiple benchmarks and outperforms models trained on either reward type alone, achieving state-of-the-art performance under comparable resource constraints."}}
{"id": "2506.17846", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17846", "abs": "https://arxiv.org/abs/2506.17846", "authors": ["Elija Perrier"], "title": "Out of Control -- Why Alignment Needs Formal Control Theory (and an Alignment Control Stack)", "comment": "Under review for Neurips 2025", "summary": "This position paper argues that formal optimal control theory should be\ncentral to AI alignment research, offering a distinct perspective from\nprevailing AI safety and security approaches. While recent work in AI safety\nand mechanistic interpretability has advanced formal methods for alignment,\nthey often fall short of the generalisation required of control frameworks for\nother technologies. There is also a lack of research into how to render\ndifferent alignment/control protocols interoperable. We argue that by recasting\nalignment through principles of formal optimal control and framing alignment in\nterms of hierarchical stack from physical to socio-technical layers according\nto which controls may be applied we can develop a better understanding of the\npotential and limitations for controlling frontier models and agentic AI\nsystems. To this end, we introduce an Alignment Control Stack which sets out a\nhierarchical layered alignment stack, identifying measurement and control\ncharacteristics at each layer and how different layers are formally\ninteroperable. We argue that such analysis is also key to the assurances that\nwill be needed by governments and regulators in order to see AI technologies\nsustainably benefit the community. Our position is that doing so will bridge\nthe well-established and empirically validated methods of optimal control with\npractical deployment considerations to create a more comprehensive alignment\nframework, enhancing how we approach safety and reliability for advanced AI\nsystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5f62\u5f0f\u6700\u4f18\u63a7\u5236\u7406\u8bba\u5e94\u6210\u4e3a AI \u5bf9\u9f50\u7814\u7a76\u7684\u6838\u5fc3\uff0c\u901a\u8fc7\u5bf9\u9f50\u63a7\u5236\u5806\u6808\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u548c\u63a7\u5236 AI \u7cfb\u7edf\uff0c\u4e3a\u9ad8\u7ea7 AI \u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u6846\u67b6\u3002", "motivation": "\u5f62\u5f0f\u6700\u4f18\u63a7\u5236\u7406\u8bba\u5e94\u8be5\u6210\u4e3a AI \u5bf9\u9f50\u7814\u7a76\u7684\u6838\u5fc3\uff0c\u63d0\u4f9b\u4e0e\u6d41\u884c\u7684 AI \u5b89\u5168\u548c\u4fdd\u969c\u65b9\u6cd5\u4e0d\u540c\u7684\u89c6\u89d2\u3002\u867d\u7136\u6700\u8fd1\u5728 AI \u5b89\u5168\u548c\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u5de5\u4f5c\u5df2\u7ecf\u63a8\u8fdb\u4e86\u5bf9\u9f50\u7684\u5f62\u5f0f\u65b9\u6cd5\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u65e0\u6cd5\u8fbe\u5230\u5176\u4ed6\u6280\u672f\u7684\u63a7\u5236\u6846\u67b6\u6240\u9700\u7684\u4e00\u822c\u5316\u3002\u6b64\u5916\uff0c\u8fd8\u7f3a\u4e4f\u5173\u4e8e\u5982\u4f55\u4f7f\u4e0d\u540c\u7684\u5bf9\u9f50/\u63a7\u5236\u534f\u8bae\u53ef\u4e92\u64cd\u4f5c\u7684\u7814\u7a76\u3002", "method": "\u5f15\u5165\u5bf9\u9f50\u63a7\u5236\u5806\u6808\uff0c\u8be5\u5806\u6808\u5efa\u7acb\u4e86\u4e00\u4e2a\u5206\u5c42\u5bf9\u9f50\u5806\u6808\uff0c\u8bc6\u522b\u6bcf\u4e00\u5c42\u7684\u6d4b\u91cf\u548c\u63a7\u5236\u7279\u6027\uff0c\u4ee5\u53ca\u4e0d\u540c\u5c42\u5982\u4f55\u5728\u5f62\u5f0f\u4e0a\u4e92\u64cd\u4f5c\u3002", "result": "\u901a\u8fc7\u5f62\u5f0f\u6700\u4f18\u63a7\u5236\u7684\u89c6\u89d2\u548c\u5206\u5c42\u5806\u6808\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u66f4\u5168\u9762\u5730\u7406\u89e3\u548c\u63a7\u5236\u524d\u6cbf AI \u7cfb\u7edf\uff0c\u4e3a\u653f\u5e9c\u548c\u76d1\u7ba1\u673a\u6784\u63d0\u4f9b\u6240\u9700\u7684\u4fdd\u8bc1\uff0c\u4ee5\u786e\u4fdd AI \u6280\u672f\u7684\u53ef\u6301\u7eed\u793e\u533a\u5229\u76ca\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5bf9\u9f50\u91cd\u65b0\u6784\u5efa\u4e3a\u5f62\u5f0f\u6700\u4f18\u63a7\u5236\u539f\u7406\uff0c\u5e76\u5728\u7269\u7406\u5230\u793e\u4f1a\u6280\u672f\u5c42\u7684\u5206\u5c42\u5806\u6808\u4e2d\u6784\u5efa\u5bf9\u9f50\uff0c\u6211\u4eec\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u63a7\u5236\u524d\u6cbf\u6a21\u578b\u548c\u4ee3\u7406 AI \u7cfb\u7edf\u7684\u6f5c\u529b\u3002\u8fd9\u6837\u505a\u5c06\u5f25\u5408\u5df2\u5efa\u7acb\u7684\u3001\u7ecf\u8fc7\u7ecf\u9a8c\u9a8c\u8bc1\u7684\u6700\u4f18\u63a7\u5236\u65b9\u6cd5\u4e0e\u5b9e\u9645\u90e8\u7f72\u8003\u8651\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ece\u800c\u521b\u5efa\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u5bf9\u9f50\u6846\u67b6\uff0c\u4ece\u800c\u589e\u5f3a\u6211\u4eec\u5904\u7406\u9ad8\u7ea7 AI \u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u7684\u65b9\u5f0f\u3002"}}
{"id": "2506.17265", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17265", "abs": "https://arxiv.org/abs/2506.17265", "authors": ["Xianren Zhang", "Hui Liu", "Delvin Ce Zhang", "Xianfeng Tang", "Qi He", "Dongwon Lee", "Suhang Wang"], "title": "Does Multimodal Large Language Model Truly Unlearn? Stealthy MLLM Unlearning Attack", "comment": "Under Review", "summary": "Multimodal Large Language Models (MLLMs) trained on massive data may memorize\nsensitive personal information and photos, posing serious privacy risks. To\nmitigate this, MLLM unlearning methods are proposed, which fine-tune MLLMs to\nreduce the ``forget'' sensitive information. However, it remains unclear\nwhether the knowledge has been truly forgotten or just hidden in the model.\nTherefore, we propose to study a novel problem of LLM unlearning attack, which\naims to recover the unlearned knowledge of an unlearned LLM. To achieve the\ngoal, we propose a novel framework Stealthy Unlearning Attack (SUA) framework\nthat learns a universal noise pattern. When applied to input images, this noise\ncan trigger the model to reveal unlearned content. While pixel-level\nperturbations may be visually subtle, they can be detected in the semantic\nembedding space, making such attacks vulnerable to potential defenses. To\nimprove stealthiness, we introduce an embedding alignment loss that minimizes\nthe difference between the perturbed and denoised image embeddings, ensuring\nthe attack is semantically unnoticeable. Experimental results show that SUA can\neffectively recover unlearned information from MLLMs. Furthermore, the learned\nnoise generalizes well: a single perturbation trained on a subset of samples\ncan reveal forgotten content in unseen images. This indicates that knowledge\nreappearance is not an occasional failure, but a consistent behavior.", "AI": {"tldr": "MLLM\u53ef\u80fd\u4f1a\u8bb0\u4f4f\u654f\u611f\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76LLM\u5220\u9664\u653b\u51fb\u3002\u6211\u4eec\u63d0\u51fa\u4e86SUA\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5b66\u4e60\u901a\u7528\u7684\u566a\u58f0\u6a21\u5f0f\u6765\u6062\u590d\u5df2\u5220\u9664\u7684\u77e5\u8bc6\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u9690\u853d\u6027\u548c\u6cdb\u5316\u6027\u3002", "motivation": "\u5728\u5927\u578b\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u53ef\u80fd\u4f1a\u8bb0\u4f4f\u654f\u611f\u7684\u4e2a\u4eba\u4fe1\u606f\u548c\u7167\u7247\uff0c\u4ece\u800c\u5e26\u6765\u4e25\u91cd\u7684\u9690\u79c1\u98ce\u9669\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u79cd\u60c5\u51b5\uff0c\u63d0\u51fa\u4e86MLLM\u5220\u9664\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5bf9MLLM\u8fdb\u884c\u5fae\u8c03\u4ee5\u51cf\u5c11\u201c\u5fd8\u8bb0\u201d\u654f\u611f\u4fe1\u606f\u3002\u4f46\u662f\uff0c\u5c1a\u4e0d\u6e05\u695a\u77e5\u8bc6\u662f\u88ab\u771f\u6b63\u5fd8\u8bb0\u4e86\u8fd8\u662f\u53ea\u662f\u9690\u85cf\u5728\u6a21\u578b\u4e2d\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u7814\u7a76LLM\u5220\u9664\u653b\u51fb\u8fd9\u4e00\u65b0\u9896\u95ee\u9898\uff0c\u8be5\u653b\u51fb\u65e8\u5728\u6062\u590d\u5df2\u5220\u9664\u7684LLM\u7684\u5df2\u5220\u9664\u77e5\u8bc6\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u9690\u853d\u6027\u5220\u9664\u653b\u51fb\uff08SUA\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5b66\u4e60\u4e00\u79cd\u901a\u7528\u7684\u566a\u58f0\u6a21\u5f0f\u3002\u5f53\u5e94\u7528\u4e8e\u8f93\u5165\u56fe\u50cf\u65f6\uff0c\u8fd9\u79cd\u566a\u58f0\u53ef\u4ee5\u89e6\u53d1\u6a21\u578b\u663e\u793a\u5df2\u5220\u9664\u7684\u5185\u5bb9\u3002\u4e3a\u4e86\u63d0\u9ad8\u9690\u853d\u6027\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u5d4c\u5165\u5bf9\u9f50\u635f\u5931\uff0c\u8be5\u635f\u5931\u53ef\u4ee5\u6700\u5927\u7a0b\u5ea6\u5730\u51cf\u5c11\u6270\u52a8\u56fe\u50cf\u548c\u53bb\u566a\u56fe\u50cf\u5d4c\u5165\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4ece\u800c\u786e\u4fdd\u653b\u51fb\u5728\u8bed\u4e49\u4e0a\u4e0d\u660e\u663e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSUA\u53ef\u4ee5\u6709\u6548\u5730\u4eceMLLM\u4e2d\u6062\u590d\u5df2\u5220\u9664\u7684\u4fe1\u606f\u3002", "conclusion": "SUA\u53ef\u4ee5\u6709\u6548\u5730\u4eceMLLM\u4e2d\u6062\u590d\u5df2\u5220\u9664\u7684\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u5b66\u4e60\u5230\u7684\u566a\u58f0\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\uff1a\u5728\u6837\u672c\u5b50\u96c6\u4e0a\u8bad\u7ec3\u7684\u5355\u4e2a\u6270\u52a8\u53ef\u4ee5\u63ed\u793a\u672a\u89c1\u56fe\u50cf\u4e2d\u88ab\u9057\u5fd8\u7684\u5185\u5bb9\u3002\u8fd9\u8868\u660e\u77e5\u8bc6\u7684\u91cd\u65b0\u51fa\u73b0\u4e0d\u662f\u5076\u7136\u7684\u5931\u8d25\uff0c\u800c\u662f\u4e00\u79cd\u4e00\u81f4\u7684\u884c\u4e3a\u3002"}}
{"id": "2506.18382", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18382", "abs": "https://arxiv.org/abs/2506.18382", "authors": ["Haotong Du", "Yaqing Wang", "Fei Xiong", "Lei Shao", "Ming Liu", "Hao Gu", "Quanming Yao", "Zhen Wang"], "title": "PERSCEN: Learning Personalized Interaction Pattern and Scenario Preference for Multi-Scenario Matching", "comment": "Accepted by KDD 2025", "summary": "With the expansion of business scales and scopes on online platforms,\nmulti-scenario matching has become a mainstream solution to reduce maintenance\ncosts and alleviate data sparsity. The key to effective multi-scenario\nrecommendation lies in capturing both user preferences shared across all\nscenarios and scenario-aware preferences specific to each scenario. However,\nexisting methods often overlook user-specific modeling, limiting the generation\nof personalized user representations. To address this, we propose PERSCEN, an\ninnovative approach that incorporates user-specific modeling into\nmulti-scenario matching. PERSCEN constructs a user-specific feature graph based\non user characteristics and employs a lightweight graph neural network to\ncapture higher-order interaction patterns, enabling personalized extraction of\npreferences shared across scenarios. Additionally, we leverage vector\nquantization techniques to distil scenario-aware preferences from users'\nbehavior sequence within individual scenarios, facilitating user-specific and\nscenario-aware preference modeling. To enhance efficient and flexible\ninformation transfer, we introduce a progressive scenario-aware gated linear\nunit that allows fine-grained, low-latency fusion. Extensive experiments\ndemonstrate that PERSCEN outperforms existing methods. Further efficiency\nanalysis confirms that PERSCEN effectively balances performance with\ncomputational cost, ensuring its practicality for real-world industrial\nsystems.", "AI": {"tldr": "PERSCEN: incorporates user-specific modeling into multi-scenario matching to capture both user preferences shared across all scenarios and scenario-aware preferences specific to each scenario.", "motivation": "existing methods often overlook user-specific modeling, limiting the generation of personalized user representations in multi-scenario matching.", "method": "incorporates user-specific modeling into multi-scenario matching; constructs a user-specific feature graph and employs a lightweight graph neural network; leverages vector quantization techniques to distil scenario-aware preferences; introduces a progressive scenario-aware gated linear unit.", "result": "PERSCEN outperforms existing methods. Further efficiency analysis confirms that PERSCEN effectively balances performance with computational cost.", "conclusion": "PERSCEN outperforms existing methods and balances performance with computational cost."}}
{"id": "2506.17505", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17505", "abs": "https://arxiv.org/abs/2506.17505", "authors": ["Jessy Lauer"], "title": "Learning golf swing signatures from a single wrist-worn inertial sensor", "comment": "9 pages, 6 figures", "summary": "Despite its importance for performance and injury prevention, golf swing\nanalysis is limited by isolated metrics, underrepresentation of professional\nathletes, and a lack of rich, interpretable movement representations. We\naddress these gaps with a holistic, data-driven framework for personalized golf\nswing analysis from a single wrist-worn sensor. We build a large dataset of\nprofessional swings from publicly available videos, reconstruct full-body 3D\nkinematics using biologically accurate human mesh recovery, and generate\nsynthetic inertial data to train neural networks that infer motion and segment\nswing phases from wrist-based input. We learn a compositional, discrete\nvocabulary of motion primitives that facilitates the detection and\nvisualization of technical flaws, and is expressive enough to predict player\nidentity, club type, sex, and age. Our system accurately estimates full-body\nkinematics and swing events from wrist data, delivering lab-grade motion\nanalysis on-course and supporting early detection of anomalous movement\npatterns. Explainability methods reveal subtle, individualized movement\nsignatures, reinforcing the view that variability is a hallmark of skilled\nperformance. Longitudinal tracking demonstrates practical value: as one\nplayer's handicap improved from 50 to 2.2 over 1.5 years, our system captured\nmeasurable technical progress and provided targeted, actionable feedback. Our\nfindings challenge common assumptions, such as swing consistency across clubs\nand the existence of a single \"ideal\" swing, and uncover latent biomarkers\nshaped by both intrinsic traits and task-specific constraints. This work\nbridges lab and field-based biomechanics, offering scalable, accessible,\nhigh-fidelity motion analysis for research, coaching, and injury prevention,\nwhile opening new directions in movement-based phenotyping, personalized\nequipment design, and motor skill development.", "AI": {"tldr": "holistic, data-driven framework for personalized golf swing analysis from a single wrist-worn sensor", "motivation": "golf swing analysis is limited by isolated metrics, underrepresentation of professional athletes, and a lack of rich, interpretable movement representations", "method": "build a large dataset of professional swings from publicly available videos, reconstruct full-body 3D kinematics using biologically accurate human mesh recovery, and generate synthetic inertial data to train neural networks that infer motion and segment swing phases from wrist-based input. We learn a compositional, discrete vocabulary of motion primitives that facilitates the detection and visualization of technical flaws, and is expressive enough to predict player identity, club type, sex, and age.", "result": "accurately estimates full-body kinematics and swing events from wrist data, delivering lab-grade motion analysis on-course and supporting early detection of anomalous movement patterns. Explainability methods reveal subtle, individualized movement signatures, reinforcing the view that variability is a hallmark of skilled performance. Longitudinal tracking demonstrates practical value: as one player's handicap improved from 50 to 2.2 over 1.5 years, our system captured measurable technical progress and provided targeted, actionable feedback.", "conclusion": "This work bridges lab and field-based biomechanics, offering scalable, accessible, high-fidelity motion analysis for research, coaching, and injury prevention, while opening new directions in movement-based phenotyping, personalized equipment design, and motor skill development."}}
{"id": "2506.17542", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.17542", "abs": "https://arxiv.org/abs/2506.17542", "authors": ["Nitin Venkateswaran", "Kevin Tang", "Ratree Wayland"], "title": "Probing for Phonology in Self-Supervised Speech Representations: A Case Study on Accent Perception", "comment": null, "summary": "Traditional models of accent perception underestimate the role of gradient\nvariations in phonological features which listeners rely upon for their accent\njudgments. We investigate how pretrained representations from current\nself-supervised learning (SSL) models of speech encode phonological\nfeature-level variations that influence the perception of segmental accent. We\nfocus on three segments: the labiodental approximant, the rhotic tap, and the\nretroflex stop, which are uniformly produced in the English of native speakers\nof Hindi as well as other languages in the Indian sub-continent. We use the\nCSLU Foreign Accented English corpus (Lander, 2007) to extract, for these\nsegments, phonological feature probabilities using Phonet (V\\'asquez-Correa et\nal., 2019) and pretrained representations from Wav2Vec2-BERT (Barrault et al.,\n2023) and WavLM (Chen et al., 2022) along with accent judgements by native\nspeakers of American English. Probing analyses show that accent strength is\nbest predicted by a subset of the segment's pretrained representation features,\nin which perceptually salient phonological features that contrast the expected\nAmerican English and realized non-native English segments are given prominent\nweighting. A multinomial logistic regression of pretrained representation-based\nsegment distances from American and Indian English baselines on accent ratings\nreveals strong associations between the odds of accent strength and distances\nfrom the baselines, in the expected directions. These results highlight the\nvalue of self-supervised speech representations for modeling accent perception\nusing interpretable phonological features.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u5982\u4f55\u7f16\u7801\u5f71\u54cd\u7247\u6bb5\u53e3\u97f3\u611f\u77e5\u7684\u97f3\u7cfb\u7279\u5f81\u7ea7\u53d8\u5f02\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u81ea\u76d1\u7763\u8bed\u97f3\u8868\u5f81\u5bf9\u4e8e\u4f7f\u7528\u53ef\u89e3\u91ca\u7684\u97f3\u7cfb\u7279\u5f81\u5efa\u6a21\u53e3\u97f3\u611f\u77e5\u5177\u6709\u4ef7\u503c\u3002", "motivation": "\u4f20\u7edf\u53e3\u97f3\u611f\u77e5\u6a21\u578b\u4f4e\u4f30\u4e86\u97f3\u7cfb\u7279\u5f81\u4e2d\u68af\u5ea6\u53d8\u5316\u7684\u4f5c\u7528\uff0c\u800c\u542c\u4f17\u4f9d\u8d56\u8fd9\u4e9b\u53d8\u5316\u6765\u5224\u65ad\u53e3\u97f3\u3002", "method": "\u4f7f\u7528 CSLU \u5916\u8bed\u53e3\u97f3\u82f1\u8bed\u8bed\u6599\u5e93\uff0c\u63d0\u53d6\u97f3\u7cfb\u7279\u5f81\u6982\u7387\uff0c\u5e76\u4f7f\u7528 Wav2Vec2-BERT \u548c WavLM \u7684\u9884\u8bad\u7ec3\u8868\u5f81\uff0c\u4ee5\u53ca\u7f8e\u56fd\u82f1\u8bed\u6bcd\u8bed\u8005\u7684\u53e3\u97f3\u5224\u65ad\u3002", "result": "\u53e3\u97f3\u5f3a\u5ea6\u53ef\u4ee5\u901a\u8fc7\u4e00\u90e8\u5206\u9884\u8bad\u7ec3\u8868\u5f81\u7279\u5f81\u6765\u6700\u597d\u5730\u9884\u6d4b\uff0c\u5176\u4e2d\u611f\u77e5\u4e0a\u663e\u8457\u7684\u97f3\u7cfb\u7279\u5f81\u88ab\u8d4b\u4e88\u7a81\u51fa\u7684\u6743\u91cd\uff0c\u8fd9\u4e9b\u7279\u5f81\u5bf9\u6bd4\u4e86\u9884\u671f\u7684\u7f8e\u56fd\u82f1\u8bed\u548c\u5b9e\u73b0\u7684\u975e\u6bcd\u8bed\u82f1\u8bed\u7247\u6bb5\u3002\u57fa\u4e8e\u9884\u8bad\u7ec3\u8868\u5f81\u7684\u7247\u6bb5\u8ddd\u79bb\u4e0e\u53e3\u97f3\u7b49\u7ea7\u4e4b\u95f4\u7684\u591a\u9879\u5f0f\u903b\u8f91\u56de\u5f52\u663e\u793a\uff0c\u53e3\u97f3\u5f3a\u5ea6\u4e0e\u57fa\u7ebf\u8ddd\u79bb\u4e4b\u95f4\u5b58\u5728\u5f88\u5f3a\u7684\u5173\u8054\u3002", "conclusion": "\u81ea\u76d1\u7763\u8bed\u97f3\u8868\u5f81\u5bf9\u4e8e\u4f7f\u7528\u53ef\u89e3\u91ca\u7684\u97f3\u7cfb\u7279\u5f81\u5efa\u6a21\u53e3\u97f3\u611f\u77e5\u5177\u6709\u4ef7\u503c\u3002"}}
{"id": "2506.17878", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17878", "abs": "https://arxiv.org/abs/2506.17878", "authors": ["Tam Trinh", "Manh Nguyen", "Truong-Son Hy"], "title": "Towards Robust Fact-Checking: A Multi-Agent System with Advanced Evidence Retrieval", "comment": null, "summary": "The rapid spread of misinformation in the digital era poses significant\nchallenges to public discourse, necessitating robust and scalable fact-checking\nsolutions. Traditional human-led fact-checking methods, while credible,\nstruggle with the volume and velocity of online content, prompting the\nintegration of automated systems powered by Large Language Models (LLMs).\nHowever, existing automated approaches often face limitations, such as handling\ncomplex claims, ensuring source credibility, and maintaining transparency. This\npaper proposes a novel multi-agent system for automated fact-checking that\nenhances accuracy, efficiency, and explainability. The system comprises four\nspecialized agents: an Input Ingestion Agent for claim decomposition, a Query\nGeneration Agent for formulating targeted subqueries, an Evidence Retrieval\nAgent for sourcing credible evidence, and a Verdict Prediction Agent for\nsynthesizing veracity judgments with human-interpretable explanations.\nEvaluated on benchmark datasets (FEVEROUS, HOVER, SciFact), the proposed system\nachieves a 12.3% improvement in Macro F1-score over baseline methods. The\nsystem effectively decomposes complex claims, retrieves reliable evidence from\ntrusted sources, and generates transparent explanations for verification\ndecisions. Our approach contributes to the growing field of automated\nfact-checking by providing a more accurate, efficient, and transparent\nverification methodology that aligns with human fact-checking practices while\nmaintaining scalability for real-world applications. Our source code is\navailable at https://github.com/HySonLab/FactAgent", "AI": {"tldr": "This paper introduces a multi-agent system for automated fact-checking that's more accurate, efficient, and transparent than existing methods.", "motivation": "The rapid spread of misinformation necessitates scalable fact-checking solutions, but existing automated approaches have limitations in handling complex claims, ensuring source credibility, and maintaining transparency.", "method": "A novel multi-agent system with four specialized agents for claim decomposition, query generation, evidence retrieval, and verdict prediction.", "result": "The system achieves a 12.3% improvement in Macro F1-score on benchmark datasets (FEVEROUS, HOVER, SciFact).", "conclusion": "The proposed multi-agent system improves fact-checking accuracy, efficiency, and transparency, achieving a 12.3% Macro F1-score improvement."}}
{"id": "2506.17267", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17267", "abs": "https://arxiv.org/abs/2506.17267", "authors": ["Jusheng Zhang", "Kaitong Cai", "Yijia Fan", "Jian Wang", "Keze Wang"], "title": "CF-VLM:CounterFactual Vision-Language Fine-tuning", "comment": null, "summary": "Recent advances in vision-language models (VLMs) have greatly improved\ncross-modal semantic understanding, yet significant limitations remain in\nfine-grained discrimination and deep causal reasoning tasks. Existing VLMs\noften rely on superficial statistical correlations, lacking the ability to\ncapture the underlying causal logic between visual and textual content. To\naddress this, we propose CounterFactual Vision-Language Fine-tuning (CF-VLM), a\nnovel framework that enhances the causal reasoning capabilities of VLMs through\nthe targeted use of counterfactual samples. CF-VLM introduces three\ncomplementary training objectives: maintaining foundational cross-modal\nalignment, reinforcing the uniqueness and stability of factual scene\nrepresentations against coherent counterfactuals, and sharpening the model's\nsensitivity to minimal but critical causal edits. Extensive experiments\ndemonstrate that CF-VLM consistently outperforms strong baselines and\nstate-of-the-art methods on compositional reasoning and generalization\nbenchmarks. Furthermore, it shows promise in mitigating visual hallucinations,\nindicating improved factual consistency. Our CF-VLM provides a robust\nfoundation for deploying VLMs in high-stakes, real-world scenarios requiring\nreliable reasoning and interpretability.", "AI": {"tldr": "This paper introduces CF-VLM, a new method to improve causal reasoning in vision-language models using counterfactual samples. It outperforms existing methods and shows promise in reducing visual hallucinations.", "motivation": "Existing VLMs often rely on superficial statistical correlations, lacking the ability to capture the underlying causal logic between visual and textual content. To address this, we propose", "method": "We propose CounterFactual Vision-Language Fine-tuning (CF-VLM), a novel framework that enhances the causal reasoning capabilities of VLMs through the targeted use of counterfactual samples. CF-VLM introduces three complementary training objectives: maintaining foundational cross-modal alignment, reinforcing the uniqueness and stability of factual scene representations against coherent counterfactuals, and sharpening the model's sensitivity to minimal but critical causal edits.", "result": "Extensive experiments demonstrate that CF-VLM consistently outperforms strong baselines and state-of-the-art methods on compositional reasoning and generalization benchmarks. Furthermore, it shows promise in mitigating visual hallucinations, indicating improved factual consistency.", "conclusion": "CF-VLM provides a robust foundation for deploying VLMs in high-stakes, real-world scenarios requiring reliable reasoning and interpretability."}}
{"id": "2506.18548", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2506.18548", "abs": "https://arxiv.org/abs/2506.18548", "authors": ["Jingwei Kang", "Maarten de Rijke", "Santiago de Leon-Martinez", "Harrie Oosterhuis"], "title": "Rethinking Click Models in Light of Carousel Interfaces: Theory-Based Categorization and Design of Click Models", "comment": "Accepted by ICTIR 2025", "summary": "Click models are a well-established for modeling user interactions with web\ninterfaces. Previous work has mainly focused on traditional single-list web\nsearch settings; this includes existing surveys that introduced categorizations\nbased on the first generation of probabilistic graphical model (PGM) click\nmodels that have become standard. However, these categorizations have become\noutdated, as their conceptualizations are unable to meaningfully compare PGM\nwith neural network (NN) click models nor generalize to newer interfaces, such\nas carousel interfaces. We argue that this outdated view fails to adequately\nexplain the fundamentals of click model designs, thus hindering the development\nof novel click models.\n  This work reconsiders what should be the fundamental concepts in click model\ndesign, grounding them - unlike previous approaches - in their mathematical\nproperties. We propose three fundamental key-design choices that explain what\nstatistical patterns a click model can capture, and thus indirectly, what user\nbehaviors they can capture. Based on these choices, we create a novel click\nmodel taxonomy that allows a meaningful comparison of all existing click\nmodels; this is the first taxonomy of single-list, grid and carousel click\nmodels that includes PGMs and NNs. Finally, we show how our conceptualization\nprovides a foundation for future click model design by an example derivation of\na novel design for carousel interfaces.", "AI": {"tldr": "This paper introduces a new click model taxonomy that includes PGMs and NNs and provides a foundation for future click model design by an example derivation of a novel design for carousel interfaces.", "motivation": "Existing categorizations are unable to meaningfully compare PGM with neural network click models nor generalize to newer interfaces, such as carousel interfaces. This outdated view fails to adequately explain the fundamentals of click model designs, thus hindering the development of novel click models.", "method": "This work reconsiders what should be the fundamental concepts in click model design, grounding them in their mathematical properties. Based on these choices, the work creates a novel click model taxonomy.", "result": "The conceptualization provides a foundation for future click model design by an example derivation of a novel design for carousel interfaces.", "conclusion": "This work proposes three fundamental key-design choices that explain what statistical patterns a click model can capture and creates a novel click model taxonomy that allows a meaningful comparison of all existing click models."}}
{"id": "2506.17545", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17545", "abs": "https://arxiv.org/abs/2506.17545", "authors": ["Zhihao Yuan", "Shuyi Jiang", "Chun-Mei Feng", "Yaolun Zhang", "Shuguang Cui", "Zhen Li", "Na Zhao"], "title": "Scene-R1: Video-Grounded Large Language Models for 3D Scene Reasoning without 3D Annotations", "comment": null, "summary": "Currently, utilizing large language models to understand the 3D world is\nbecoming popular. Yet existing 3D-aware LLMs act as black boxes: they output\nbounding boxes or textual answers without revealing how those decisions are\nmade, and they still rely on pre-trained 3D detectors to supply object\nproposals. We introduce Scene-R1, a video-grounded framework that learns to\nreason about 3D scenes without any point-wise 3D instance supervision by\npairing reinforcement-learning-driven reasoning with a two-stage grounding\npipeline. In the temporal grounding stage, we explicitly reason about the video\nand select the video snippets most relevant to an open-ended query. In the\nsubsequent image grounding stage, we analyze the image and predict the 2D\nbounding box. After that, we track the object using SAM2 to produce\npixel-accurate masks in RGB frames, and project them back into 3D, thereby\neliminating the need for 3D detector-based proposals while capturing fine\ngeometry and material cues. Scene-R1 can also adapt to the 3D visual question\nanswering task to answer free-form questions directly from video. Our training\npipeline only needs task-level 2D boxes or textual labels without dense 3D\npoint-wise labels. Scene-R1 surpasses existing open-vocabulary baselines on\nmultiple datasets, while delivering transparent, step-by-step rationales. These\nresults show that reinforcement-learning-based reasoning combined with RGB-D\nvideo alone offers a practical, annotation-efficient route to trustworthy 3D\nscene understanding.", "AI": {"tldr": "Scene-R1, a video-grounded framework, learns to reason about 3D scenes without any point-wise 3D instance supervision by pairing reinforcement-learning-driven reasoning with a two-stage grounding pipeline", "motivation": "existing 3D-aware LLMs act as black boxes and they still rely on pre-trained 3D detectors to supply object proposals", "method": "pairing reinforcement-learning-driven reasoning with a two-stage grounding pipeline", "result": "Scene-R1 surpasses existing open-vocabulary baselines on multiple datasets, while delivering transparent, step-by-step rationales", "conclusion": "reinforcement-learning-based reasoning combined with RGB-D video alone offers a practical, annotation-efficient route to trustworthy 3D scene understanding"}}
{"id": "2506.17578", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.17578", "abs": "https://arxiv.org/abs/2506.17578", "authors": ["Lingxiao Zeng", "Yiqi Tong", "Wei Guo", "Huarui Wu", "Lihao Ge", "Yijun Ye", "Fuzhen Zhuang", "Deqing Wang", "Wei Guo", "Cheng Chen"], "title": "AgriCHN: A Comprehensive Cross-domain Resource for Chinese Agricultural Named Entity Recognition", "comment": null, "summary": "Agricultural named entity recognition is a specialized task focusing on\nidentifying distinct agricultural entities within vast bodies of text,\nincluding crops, diseases, pests, and fertilizers. It plays a crucial role in\nenhancing information extraction from extensive agricultural text resources.\nHowever, the scarcity of high-quality agricultural datasets, particularly in\nChinese, has resulted in suboptimal performance when employing mainstream\nmethods for this purpose. Most earlier works only focus on annotating\nagricultural entities while overlook the profound correlation of agriculture\nwith hydrology and meteorology. To fill this blank, we present AgriCHN, a\ncomprehensive open-source Chinese resource designed to promote the accuracy of\nautomated agricultural entity annotation. The AgriCHN dataset has been\nmeticulously curated from a wealth of agricultural articles, comprising a total\nof 4,040 sentences and encapsulating 15,799 agricultural entity mentions\nspanning 27 diverse entity categories. Furthermore, it encompasses entities\nfrom hydrology to meteorology, thereby enriching the diversity of entities\nconsidered. Data validation reveals that, compared with relevant resources,\nAgriCHN demonstrates outstanding data quality, attributable to its richer\nagricultural entity types and more fine-grained entity divisions. A benchmark\ntask has also been constructed using several state-of-the-art neural NER\nmodels. Extensive experimental results highlight the significant challenge\nposed by AgriCHN and its potential for further research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u5f00\u6e90\u4e2d\u6587\u8d44\u6e90 AgriCHN\uff0c\u65e8\u5728\u63d0\u9ad8\u81ea\u52a8\u519c\u4e1a\u5b9e\u4f53\u6ce8\u91ca\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u9ad8\u8d28\u91cf\u7684\u519c\u4e1a\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u5c24\u5176\u662f\u5728\u4e2d\u6587\u65b9\u9762\uff0c\u5bfc\u81f4\u4f7f\u7528\u4e3b\u6d41\u65b9\u6cd5\u8fdb\u884c\u6b64\u76ee\u7684\u65f6\u6027\u80fd\u6b20\u4f73\u3002\u65e9\u671f\u7684\u5927\u591a\u6570\u5de5\u4f5c\u4ec5\u4fa7\u91cd\u4e8e\u6ce8\u91ca\u519c\u4e1a\u5b9e\u4f53\uff0c\u800c\u5ffd\u7565\u4e86\u519c\u4e1a\u4e0e\u6c34\u6587\u548c\u6c14\u8c61\u7684\u6df1\u523b\u76f8\u5173\u6027\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u51c6\u4efb\u52a1\uff0c\u4f7f\u7528\u4e86\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u795e\u7ecf NER \u6a21\u578b\u3002", "result": " AgriCHN\u6570\u636e\u96c6\u6bd4\u76f8\u5173\u8d44\u6e90\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6570\u636e\u8d28\u91cf\uff0c\u8fd9\u5f52\u56e0\u4e8e\u5176\u66f4\u4e30\u5bcc\u7684\u519c\u4e1a\u5b9e\u4f53\u7c7b\u578b\u548c\u66f4\u7ec6\u7c92\u5ea6\u7684\u5b9e\u4f53\u5212\u5206\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u7ed3\u679c\u7a81\u51fa\u4e86 AgriCHN \u5e26\u6765\u7684\u91cd\u5927\u6311\u6218\u53ca\u5176\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u6f5c\u529b\u3002", "conclusion": "AgriCHN\u6570\u636e\u96c6\u5177\u6709\u6311\u6218\u6027\uff0c\u6709\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.17900", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.17900", "abs": "https://arxiv.org/abs/2506.17900", "authors": ["Cheng Ji", "Huaiying Luo"], "title": "Leveraging Large Language Model for Intelligent Log Processing and Autonomous Debugging in Cloud AI Platforms", "comment": "Accepted by 2025 8th International Conference on Advanced Electronic\n  Materials, Computers and Software Engineering (AEMCSE 2025)", "summary": "With the increasing complexity and rapid expansion of the scale of AI systems\nin cloud platforms, the log data generated during system operation is massive,\nunstructured, and semantically ambiguous, which brings great challenges to\nfault location and system self-repair. In order to solve this problem, this\npaper proposes an intelligent log processing and automatic debugging framework\nbased on Large Language Model (LLM), named Intelligent Debugger (LLM-ID). This\nmethod is extended on the basis of the existing pre-trained Transformer model,\nand integrates a multi-stage semantic inference mechanism to realize the\ncontext understanding of system logs and the automatic reconstruction of fault\nchains. Firstly, the system log is dynamically structured, and the unsupervised\nclustering and embedding mechanism is used to extract the event template and\nsemantic schema. Subsequently, the fine-tuned LLM combined with the multi-round\nattention mechanism to perform contextual reasoning on the log sequence to\ngenerate potential fault assumptions and root cause paths. Furthermore, this\npaper introduces a reinforcement learning-based policy-guided recovery planner,\nwhich is driven by the remediation strategy generated by LLM to support dynamic\ndecision-making and adaptive debugging in the cloud environment. Compared with\nthe existing rule engine or traditional log analysis system, the proposed model\nhas stronger semantic understanding ability, continuous learning ability and\nheterogeneous environment adaptability. Experiments on the cloud platform log\ndataset show that LLM-ID improves the fault location accuracy by 16.2%, which\nis significantly better than the current mainstream methods", "AI": {"tldr": "This paper introduces LLM-ID, an LLM-based intelligent debugger for cloud systems that improves fault location accuracy by 16.2%.", "motivation": "The increasing complexity and rapid expansion of AI systems in cloud platforms generate massive, unstructured, and semantically ambiguous log data, posing challenges to fault location and system self-repair.", "method": "The paper proposes an intelligent log processing and automatic debugging framework based on Large Language Model (LLM), named Intelligent Debugger (LLM-ID). It integrates a multi-stage semantic inference mechanism, dynamic log structuring, unsupervised clustering, a fine-tuned LLM with multi-round attention, and a reinforcement learning-based policy-guided recovery planner.", "result": "LLM-ID improves fault location accuracy by 16.2% compared to existing rule engine or traditional log analysis systems.", "conclusion": "The proposed LLM-ID model improves fault location accuracy by 16.2% compared to existing methods, demonstrating stronger semantic understanding, continuous learning ability, and heterogeneous environment adaptability."}}
{"id": "2506.17297", "categories": ["cs.LG", "cs.AI", "68T05", "I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2506.17297", "abs": "https://arxiv.org/abs/2506.17297", "authors": ["Satyam Mishra", "Phung Thao Vi", "Shivam Mishra", "Vishwanath Bijalwan", "Vijay Bhaskar Semwal", "Abdul Manan Khan"], "title": "SafeRL-Lite: A Lightweight, Explainable, and Constrained Reinforcement Learning Library", "comment": "10 pages, 7 figures, open-source library, PyPI installable: pip\n  install saferl-lite", "summary": "We introduce SafeRL-Lite, an open-source Python library for building\nreinforcement learning (RL) agents that are both constrained and explainable.\nExisting RL toolkits often lack native mechanisms for enforcing hard safety\nconstraints or producing human-interpretable rationales for decisions.\nSafeRL-Lite provides modular wrappers around standard Gym environments and deep\nQ-learning agents to enable: (i) safety-aware training via constraint\nenforcement, and (ii) real-time post-hoc explanation via SHAP values and\nsaliency maps. The library is lightweight, extensible, and installable via pip,\nand includes built-in metrics for constraint violations. We demonstrate its\neffectiveness on constrained variants of CartPole and provide visualizations\nthat reveal both policy logic and safety adherence. The full codebase is\navailable at: https://github.com/satyamcser/saferl-lite.", "AI": {"tldr": "SafeRL-Lite is a Python library for building reinforcement learning (RL) agents that are both constrained and explainable.", "motivation": "Existing RL toolkits often lack native mechanisms for enforcing hard safety constraints or producing human-interpretable rationales for decisions.", "method": "modular wrappers around standard Gym environments and deep Q-learning agents", "result": "enable safety-aware training via constraint enforcement, and real-time post-hoc explanation via SHAP values and saliency maps", "conclusion": "SafeRL-Lite is effective on constrained variants of CartPole and provides visualizations that reveal both policy logic and safety adherence."}}
{"id": "2506.18670", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2506.18670", "abs": "https://arxiv.org/abs/2506.18670", "authors": ["Jingming Liu", "Yumeng Li", "Wei Shi", "Yao-Xiang Ding", "Hui Su", "Kun Zhou"], "title": "Harnessing the Power of Reinforcement Learning for Language-Model-Based Information Retriever via Query-Document Co-Augmentation", "comment": null, "summary": "Recent studies have proposed leveraging Large Language Models (LLMs) as\ninformation retrievers through query rewriting. However, for challenging\ncorpora, we argue that enhancing queries alone is insufficient for robust\nsemantic matching; the LLM should also have sufficient understanding of the\ncorpus by directly handling and augmenting the documents themselves. To this\nend, we present an LLM-based retriever empowered to augment both user queries\nand corpus documents, with its policy fully explored via reinforcement learning\n(RL) and minimal human inductive bias. Notably, we find that simply allowing\nthe LLM to modify documents yields little benefit unless paired with our\ncarefully designed bidirectional RL framework, which enables the LLM to\nsimultaneously learn and collaborate on both query and document augmentation\npolicies. A key technical challenge in realizing such a framework lies in\njointly updating both policies during training, where the rewards for the two\ndirections depend on each other, making their entangled reward intractable. Our\napproach addresses this by introducing a reward sampling strategy and a\nspecifically designed RL algorithm that enables effective training with these\nsampled rewards. Experimental results demonstrate that our approach\nsignificantly enhances LLM-based retrieval performance in both sparse and dense\nsettings, particularly in difficult retrieval domains, and achieves strong\ncross-benchmark generalization. Our code is released at\nhttps://github.com/liujm2001/CoAugRetriever.", "AI": {"tldr": "This paper introduces an LLM-based retriever that augments both user queries and corpus documents, trained with a bidirectional RL framework to improve retrieval performance, especially in challenging domains.", "motivation": "Enhancing queries alone is insufficient for robust semantic matching; the LLM should also have sufficient understanding of the corpus by directly handling and augmenting the documents themselves.", "method": "LLM-based retriever empowered to augment both user queries and corpus documents, with its policy fully explored via reinforcement learning (RL) and minimal human inductive bias.Introduces a reward sampling strategy and a specifically designed RL algorithm that enables effective training with these sampled rewards", "result": "Simply allowing the LLM to modify documents yields little benefit unless paired with our carefully designed bidirectional RL framework, which enables the LLM to simultaneously learn and collaborate on both query and document augmentation policies.", "conclusion": "The approach significantly enhances LLM-based retrieval performance in both sparse and dense settings, particularly in difficult retrieval domains, and achieves strong cross-benchmark generalization."}}
{"id": "2506.17558", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17558", "abs": "https://arxiv.org/abs/2506.17558", "authors": ["Jake Levi", "Mark van der Wilk"], "title": "SynDaCaTE: A Synthetic Dataset For Evaluating Part-Whole Hierarchical Inference", "comment": "Accepted at Methods and Opportunities at Small Scale (MOSS), ICML\n  2025, Vancouver, Canada", "summary": "Learning to infer object representations, and in particular part-whole\nhierarchies, has been the focus of extensive research in computer vision, in\npursuit of improving data efficiency, systematic generalisation, and\nrobustness. Models which are \\emph{designed} to infer part-whole hierarchies,\noften referred to as capsule networks, are typically trained end-to-end on\nsupervised tasks such as object classification, in which case it is difficult\nto evaluate whether such a model \\emph{actually} learns to infer part-whole\nhierarchies, as claimed. To address this difficulty, we present a SYNthetic\nDAtaset for CApsule Testing and Evaluation, abbreviated as SynDaCaTE, and\nestablish its utility by (1) demonstrating the precise bottleneck in a\nprominent existing capsule model, and (2) demonstrating that\npermutation-equivariant self-attention is highly effective for parts-to-wholes\ninference, which motivates future directions for designing effective inductive\nbiases for computer vision.", "AI": {"tldr": "This paper introduces SynDaCaTE, a synthetic dataset for capsule testing and evaluation. The authors use it to find a bottleneck in an existing capsule model and demonstrate that permutation-equivariant self-attention is effective for parts-to-wholes inference.", "motivation": "Learning to infer object representations, and in particular part-whole hierarchies, has been the focus of extensive research in computer vision, in pursuit of improving data efficiency, systematic generalisation, and robustness. Models which are designed to infer part-whole hierarchies, often referred to as capsule networks, are typically trained end-to-end on supervised tasks such as object classification, in which case it is difficult to evaluate whether such a model actually learns to infer part-whole hierarchies, as claimed.", "method": "present a SYNthetic DAtaset for CApsule Testing and Evaluation, abbreviated as SynDaCaTE, and establish its utility by (1) demonstrating the precise bottleneck in a prominent existing capsule model, and (2) demonstrating that permutation-equivariant self-attention is highly effective for parts-to-wholes inference", "result": "demonstrating the precise bottleneck in a prominent existing capsule model", "conclusion": "permutation-equivariant self-attention is highly effective for parts-to-wholes inference, which motivates future directions for designing effective inductive biases for computer vision."}}
{"id": "2506.17603", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.17603", "abs": "https://arxiv.org/abs/2506.17603", "authors": ["Jonathan Sakunkoo", "Annabella Sakunkoo"], "title": "Mind the Gap: Assessing Wiktionary's Crowd-Sourced Linguistic Knowledge on Morphological Gaps in Two Related Languages", "comment": null, "summary": "Morphological defectivity is an intriguing and understudied phenomenon in\nlinguistics. Addressing defectivity, where expected inflectional forms are\nabsent, is essential for improving the accuracy of NLP tools in morphologically\nrich languages. However, traditional linguistic resources often lack coverage\nof morphological gaps as such knowledge requires significant human expertise\nand effort to document and verify. For scarce linguistic phenomena in\nunder-explored languages, Wikipedia and Wiktionary often serve as among the few\naccessible resources. Despite their extensive reach, their reliability has been\na subject of controversy. This study customizes a novel neural morphological\nanalyzer to annotate Latin and Italian corpora. Using the massive annotated\ndata, crowd-sourced lists of defective verbs compiled from Wiktionary are\nvalidated computationally. Our results indicate that while Wiktionary provides\na highly reliable account of Italian morphological gaps, 7% of Latin lemmata\nlisted as defective show strong corpus evidence of being non-defective. This\ndiscrepancy highlights potential limitations of crowd-sourced wikis as\ndefinitive sources of linguistic knowledge, particularly for less-studied\nphenomena and languages, despite their value as resources for rare linguistic\nfeatures. By providing scalable tools and methods for quality assurance of\ncrowd-sourced data, this work advances computational morphology and expands\nlinguistic knowledge of defectivity in non-English, morphologically rich\nlanguages.", "AI": {"tldr": "This study validates Wiktionary's defective verb lists for Latin and Italian using a neural morphological analyzer, finding high reliability for Italian but discrepancies for Latin, highlighting limitations of crowd-sourced data for less-studied languages.", "motivation": "Addressing defectivity, where expected inflectional forms are absent, is essential for improving the accuracy of NLP tools in morphologically rich languages. However, traditional linguistic resources often lack coverage of morphological gaps as such knowledge requires significant human expertise and effort to document and verify. For scarce linguistic phenomena in under-explored languages, Wikipedia and Wiktionary often serve as among the few accessible resources. Despite their extensive reach, their reliability has been a subject of controversy.", "method": "customizes a novel neural morphological analyzer to annotate Latin and Italian corpora. Using the massive annotated data, crowd-sourced lists of defective verbs compiled from Wiktionary are validated computationally.", "result": "while Wiktionary provides a highly reliable account of Italian morphological gaps, 7% of Latin lemmata listed as defective show strong corpus evidence of being non-defective.", "conclusion": "Wiktionary provides a highly reliable account of Italian morphological gaps, but 7% of Latin lemmata listed as defective show strong corpus evidence of being non-defective. This discrepancy highlights potential limitations of crowd-sourced wikis as definitive sources of linguistic knowledge, particularly for less-studied phenomena and languages."}}
{"id": "2506.17913", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17913", "abs": "https://arxiv.org/abs/2506.17913", "authors": ["Jinjie Wei", "Jiyao Liu", "Lihao Liu", "Ming Hu", "Junzhi Ning", "Mingcheng Li", "Weijie Yin", "Junjun He", "Xiao Liang", "Chao Feng", "Dingkang Yang"], "title": "Learning, Reasoning, Refinement: A Framework for Kahneman's Dual-System Intelligence in GUI Agents", "comment": null, "summary": "Graphical User Interface (GUI) agents have made significant progress in\nautomating digital tasks through the utilization of computer vision and\nlanguage models. Nevertheless, existing agent systems encounter notable\nlimitations. Firstly, they predominantly depend on trial and error decision\nmaking rather than progressive reasoning, thereby lacking the capability to\nlearn and adapt from interactive encounters. Secondly, these systems are\nassessed using overly simplistic single step accuracy metrics, which do not\nadequately reflect the intricate nature of real world GUI interactions. In this\npaper, we present CogniGUI, a cognitive framework developed to overcome these\nlimitations by enabling adaptive learning for GUI automation resembling\nhuman-like behavior. Inspired by Kahneman's Dual Process Theory, our approach\ncombines two main components: (1) an omni parser engine that conducts immediate\nhierarchical parsing of GUI elements through quick visual semantic analysis to\nidentify actionable components, and (2) a Group based Relative Policy\nOptimization (GRPO) grounding agent that assesses multiple interaction paths\nusing a unique relative reward system, promoting minimal and efficient\noperational routes. This dual-system design facilitates iterative ''exploration\nlearning mastery'' cycles, enabling the agent to enhance its strategies over\ntime based on accumulated experience. Moreover, to assess the generalization\nand adaptability of agent systems, we introduce ScreenSeek, a comprehensive\nbenchmark that includes multi application navigation, dynamic state\ntransitions, and cross interface coherence, which are often overlooked\nchallenges in current benchmarks. Experimental results demonstrate that\nCogniGUI surpasses state-of-the-art methods in both the current GUI grounding\nbenchmarks and our newly proposed benchmark.", "AI": {"tldr": "CogniGUI\u901a\u8fc7\u7ed3\u5408omni parser\u5f15\u64ce\u548cGRPO grounding agent\uff0c\u514b\u670d\u4e86\u73b0\u6709GUI agent\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u884c\u4e3a\u7684\u81ea\u9002\u5e94\u5b66\u4e60GUI\u81ea\u52a8\u5316\u3002", "motivation": "\u73b0\u6709\u7684agent\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u4e8e\u53cd\u590d\u8bd5\u9a8c\u7684\u51b3\u7b56\uff0c\u800c\u4e0d\u662f\u6e10\u8fdb\u5f0f\u7684\u63a8\u7406\uff0c\u56e0\u6b64\u7f3a\u4e4f\u4ece\u4ea4\u4e92\u5f0f\u906d\u9047\u4e2d\u5b66\u4e60\u548c\u9002\u5e94\u7684\u80fd\u529b\u3002\u5176\u6b21\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u4f7f\u7528\u8fc7\u4e8e\u7b80\u5355\u7684\u5355\u6b65\u7cbe\u5ea6\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\uff0c\u8fd9\u4e9b\u6307\u6807\u4e0d\u80fd\u5145\u5206\u53cd\u6620\u771f\u5b9eGUI\u4ea4\u4e92\u7684\u590d\u6742\u6027\u3002", "method": "\u7ed3\u5408\u4e86\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a(1) \u901a\u8fc7\u5feb\u901f\u89c6\u89c9\u8bed\u4e49\u5206\u6790\u5bf9GUI\u5143\u7d20\u8fdb\u884c\u5373\u65f6\u5206\u5c42\u89e3\u6790\u7684omni parser\u5f15\u64ce\uff0c\u4ee5\u8bc6\u522b\u53ef\u64cd\u4f5c\u7684\u7ec4\u4ef6\uff0c\u4ee5\u53ca (2) \u57fa\u4e8e\u7ec4\u7684\u76f8\u5bf9\u7b56\u7565\u4f18\u5316(GRPO) grounding agent\uff0c\u8be5agent\u4f7f\u7528\u72ec\u7279\u7684\u76f8\u5bf9\u5956\u52b1\u7cfb\u7edf\u8bc4\u4f30\u591a\u4e2a\u4ea4\u4e92\u8def\u5f84\uff0c\u4ece\u800c\u4fc3\u8fdb\u6700\u5c0f\u548c\u9ad8\u6548\u7684\u64cd\u4f5c\u8def\u5f84\u3002", "result": "CogniGUI\u5728\u5f53\u524dGUI\u5b9a\u4f4d\u57fa\u51c6\u548c\u65b0\u63d0\u51fa\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "CogniGUI\u5728\u5f53\u524dGUI\u5b9a\u4f4d\u57fa\u51c6\u548c\u65b0\u63d0\u51fa\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.17304", "categories": ["cs.LG", "cs.AI", "cs.DS"], "pdf": "https://arxiv.org/pdf/2506.17304", "abs": "https://arxiv.org/abs/2506.17304", "authors": ["Jasper Yao"], "title": "AlgoSelect: Universal Algorithm Selection via the Comb Operator", "comment": "24 pages, 4 figures, 1 repository, 1 supplementary document", "summary": "We introduce AlgoSelect, a principled framework for learning optimal\nalgorithm selection from data, centered around the novel Comb Operator. Given a\nset of algorithms and a feature representation of problems, AlgoSelect learns\nto interpolate between diverse computational approaches. For pairs of\nalgorithms, a simple sigmoid-gated selector, an instance of the Comb Operator,\nfacilitates this interpolation. We extend this to an N-Path Comb for multiple\nalgorithms. We prove that this framework is universal (can approximate any\nalgorithm selector), information-theoretically optimal in its learnability\n(thresholds for selection converge almost surely, demonstrated via\nBorel-Cantelli arguments), computationally efficient, and robust. Key\ntheoretical contributions include: (1) a universal approximation theorem\ndemonstrating that Comb-based selectors can achieve arbitrary accuracy; (2)\ninformation-theoretic learnability for selection thresholds; (3) formalization\nof the Comb Operator within linear operator theory, detailing its boundedness\nand spectral properties; (4) an N-Path Comb generalization for multi-algorithm\nselection; and (5) a practical learning framework for the adaptive seeding\nfunctions that guide the Comb Operator. Empirical validation on a comprehensive\n20$\\times$20 problem-algorithm study demonstrates near-perfect selection\n(99.9\\%+ accuracy) with remarkably few samples and rapid convergence, revealing\nthat $H(\\text{Algorithm}|\\text{Problem}) \\approx 0$ in structured domains.\nAlgoSelect provides a theoretically grounded, practically deployable solution\nto automated algorithm selection with provable optimality and learnability\nguarantees, with significant implications for AI and adaptive systems.", "AI": {"tldr": "AlgoSelect \u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u7b97\u6cd5\u9009\u62e9\u7684\u901a\u7528\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5b66\u4e60\u5728\u7b97\u6cd5\u4e4b\u95f4\u8fdb\u884c\u63d2\u503c\u6765\u5b9e\u73b0\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u9009\u62e9\uff0c\u5e76\u4e14\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u6211\u4eec\u4ecb\u7ecd\u4e86 AlgoSelect\uff0c\u8fd9\u662f\u4e00\u4e2a\u6709\u539f\u5219\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u6700\u4f73\u7b97\u6cd5\u9009\u62e9\uff0c\u8be5\u6846\u67b6\u4ee5\u65b0\u578b Comb Operator \u4e3a\u4e2d\u5fc3\u3002", "method": "AlgoSelect \u5b66\u4e60\u5728\u4e0d\u540c\u7684\u8ba1\u7b97\u65b9\u6cd5\u4e4b\u95f4\u8fdb\u884c\u63d2\u503c\u3002\u5bf9\u4e8e\u6210\u5bf9\u7684\u7b97\u6cd5\uff0c\u4e00\u4e2a\u7b80\u5355\u7684 sigmoid \u95e8\u63a7\u9009\u62e9\u5668\uff08Comb Operator \u7684\u4e00\u4e2a\u5b9e\u4f8b\uff09\u4fc3\u8fdb\u4e86\u8fd9\u79cd\u63d2\u503c\u3002\u6211\u4eec\u5c06\u5176\u6269\u5c55\u5230\u591a\u7b97\u6cd5\u7684 N-Path Comb\u3002", "result": "\u5728\u5168\u9762\u7684 20$\\times$20 \u95ee\u9898-\u7b97\u6cd5\u7814\u7a76\u4e2d\u7684\u7ecf\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cAlgoSelect \u5b9e\u73b0\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u7684\u7b97\u6cd5\u9009\u62e9\uff0899.9\\%+ \u7684\u51c6\u786e\u7387\uff09\uff0c\u6240\u9700\u7684\u6837\u672c\u975e\u5e38\u5c11\u4e14\u6536\u655b\u901f\u5ea6\u5feb\uff0c\u63ed\u793a\u4e86\u5728\u7ed3\u6784\u5316\u9886\u57df\u4e2d $H(\\\\text{Algorithm}|\\\\text{Problem}) \\\\approx 0$\u3002", "conclusion": "AlgoSelect \u4e3a\u81ea\u52a8\u7b97\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u4e0a\u53ef\u9760\u3001\u5b9e\u9645\u53ef\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u53ef\u8bc1\u660e\u7684\u6700\u4f18\u6027\u548c\u53ef\u5b66\u4e60\u6027\u4fdd\u8bc1\uff0c\u5bf9\u4eba\u5de5\u667a\u80fd\u548c\u81ea\u9002\u5e94\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2506.18735", "categories": ["cs.IR", "eess.AS", "H.3.3; I.2.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2506.18735", "abs": "https://arxiv.org/abs/2506.18735", "authors": ["Shivam Verma", "Vivian Chen", "Darren Mei"], "title": "An Audio-centric Multi-task Learning Framework for Streaming Ads Targeting on Spotify", "comment": "Accepted at KDD 2025", "summary": "Spotify, a large-scale multimedia platform, attracts over 675 million monthly\nactive users who collectively consume millions of hours of music, podcasts,\naudiobooks, and video content. This diverse content consumption pattern\nintroduces unique challenges for computational advertising, which must\neffectively integrate a variety of ad modalities, including audio, video, and\ndisplay, within a single user experience. Traditional ad recommendation models,\nprimarily designed for foregrounded experiences, often struggle to reconcile\nthe platform's inherent audio-centrality with the demands of optimizing ad\nperformance across multiple formats and modalities. To overcome these\nchallenges, we introduce Cross-modal Adaptive Mixture-of-Experts (CAMoE), a\nnovel framework for optimizing click-through rate (CTR) prediction in both\naudio-centric and multi-modal settings. CAMoE enhances traditional\nmixture-of-experts models by incorporating modality-aware task grouping,\nadaptive loss masking, and deep-cross networks (DCN) to capture complex feature\ninteractions within a multi-modal ad ecosystem. Through extensive ablation\nstudies, we demonstrate that this approach achieves near Pareto-optimal\nperformance across audio, video, and display ad formats, significantly\nimproving AUC-PR compared to conventional single-task and content-based\nmulti-task learning baselines. When deployed at scale on Spotify's ad serving\nplatform, CAMoE delivered substantial gains, yielding a 14.5% increase in CTR\nfor audio ads, a 1.3% increase for video ads, and a 4.8% reduction in expected\ncost-per-click (eCPC) for audio slots.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86CAMoE\uff0c\u4e00\u4e2a\u7528\u4e8e\u4f18\u5316Spotify\u5e7f\u544a\u70b9\u51fb\u7387\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u6001\u611f\u77e5\u548c\u6df1\u5ea6\u5b66\u4e60\u663e\u8457\u63d0\u9ad8\u4e86\u5404\u79cd\u5e7f\u544a\u683c\u5f0f\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5e7f\u544a\u63a8\u8350\u6a21\u578b\u4e3b\u8981\u4e3a\u524d\u666f\u4f53\u9a8c\u800c\u8bbe\u8ba1\uff0c\u901a\u5e38\u96be\u4ee5\u534f\u8c03\u5e73\u53f0\u7684\u5185\u5728\u97f3\u9891\u4e2d\u5fc3\u6027\u4e0e\u8de8\u591a\u79cd\u683c\u5f0f\u548c\u6a21\u6001\u4f18\u5316\u5e7f\u544a\u6027\u80fd\u7684\u9700\u6c42\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u8de8\u6a21\u6001\u81ea\u9002\u5e94\u6df7\u5408\u4e13\u5bb6\uff08CAMoE\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u4ee5\u97f3\u9891\u4e3a\u4e2d\u5fc3\u548c\u591a\u6a21\u6001\u73af\u5883\u4e2d\u7684\u70b9\u51fb\u7387\uff08CTR\uff09\u9884\u6d4b\u3002CAMoE\u901a\u8fc7\u7ed3\u5408\u6a21\u6001\u611f\u77e5\u4efb\u52a1\u5206\u7ec4\u3001\u81ea\u9002\u5e94\u635f\u5931\u63a9\u853d\u548c\u6df1\u5ea6\u4ea4\u53c9\u7f51\u7edc\uff08DCN\uff09\u6765\u589e\u5f3a\u4f20\u7edf\u7684\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff0c\u4ee5\u6355\u83b7\u591a\u6a21\u6001\u5e7f\u544a\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u590d\u6742\u7279\u5f81\u4ea4\u4e92\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u8fd9\u79cd\u65b9\u6cd5\u5728\u97f3\u9891\u3001\u89c6\u9891\u548c\u5c55\u793a\u5e7f\u544a\u683c\u5f0f\u4e0a\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5e15\u7d2f\u6258\u6700\u4f18\u7684\u6027\u80fd\uff0c\u4e0e\u4f20\u7edf\u7684\u5355\u4efb\u52a1\u548c\u57fa\u4e8e\u5185\u5bb9\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u57fa\u7ebf\u76f8\u6bd4\uff0c\u663e\u7740\u63d0\u9ad8\u4e86AUC-PR\u3002", "conclusion": "CAMoE\u5728Spotify\u7684\u5e7f\u544a\u670d\u52a1\u5e73\u53f0\u4e0a\u5927\u89c4\u6a21\u90e8\u7f72\u540e\uff0c\u97f3\u9891\u5e7f\u544a\u7684\u70b9\u51fb\u7387\u63d0\u9ad8\u4e8614.5%\uff0c\u89c6\u9891\u5e7f\u544a\u7684\u70b9\u51fb\u7387\u63d0\u9ad8\u4e861.3%\uff0c\u97f3\u9891\u5e7f\u544a\u4f4d\u7684\u9884\u671f\u6bcf\u6b21\u70b9\u51fb\u6210\u672c\u964d\u4f4e\u4e864.8%\u3002"}}
{"id": "2506.17561", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.17561", "abs": "https://arxiv.org/abs/2506.17561", "authors": ["Chongkai Gao", "Zixuan Liu", "Zhenghao Chi", "Junshan Huang", "Xin Fei", "Yiwen Hou", "Yuxuan Zhang", "Yudi Lin", "Zhirui Fang", "Zeyu Jiang", "Lin Shao"], "title": "VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models", "comment": null, "summary": "Recent studies on Vision-Language-Action (VLA) models have shifted from the\nend-to-end action-generation paradigm toward a pipeline involving task planning\nfollowed by action generation, demonstrating improved performance on various\ncomplex, long-horizon manipulation tasks. However, existing approaches vary\nsignificantly in terms of network architectures, planning paradigms,\nrepresentations, and training data sources, making it challenging for\nresearchers to identify the precise sources of performance gains and components\nto be further improved. To systematically investigate the impacts of different\nplanning paradigms and representations isolating from network architectures and\ntraining data, in this paper, we introduce VLA-OS, a unified VLA architecture\nseries capable of various task planning paradigms, and design a comprehensive\nsuite of controlled experiments across diverse object categories (rigid and\ndeformable), visual modalities (2D and 3D), environments (simulation and\nreal-world), and end-effectors (grippers and dexterous hands). Our results\ndemonstrate that: 1) visually grounded planning representations are generally\nbetter than language planning representations; 2) the Hierarchical-VLA paradigm\ngenerally achieves superior or comparable performance than other paradigms on\ntask performance, pretraining, generalization ability, scalability, and\ncontinual learning ability, albeit at the cost of slower training and inference\nspeeds.", "AI": {"tldr": "This paper introduces VLA-OS to study task planning paradigms in VLA models, finding visually grounded planning is better and Hierarchical-VLA is generally superior.", "motivation": "existing approaches vary significantly in terms of network architectures, planning paradigms, representations, and training data sources, making it challenging for researchers to identify the precise sources of performance gains and components to be further improved", "method": "introduce VLA-OS, a unified VLA architecture series capable of various task planning paradigms, and design a comprehensive suite of controlled experiments across diverse object categories (rigid and deformable), visual modalities (2D and 3D), environments (simulation and real-world), and end-effectors (grippers and dexterous hands)", "result": "visually grounded planning representations are generally better than language planning representations; the Hierarchical-VLA paradigm generally achieves superior or comparable performance than other paradigms on task performance, pretraining, generalization ability, scalability, and continual learning ability, albeit at the cost of slower training and inference speeds", "conclusion": "visually grounded planning representations are generally better than language planning representations; the Hierarchical-VLA paradigm generally achieves superior or comparable performance than other paradigms on task performance, pretraining, generalization ability, scalability, and continual learning ability, albeit at the cost of slower training and inference speeds"}}
{"id": "2506.17609", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17609", "abs": "https://arxiv.org/abs/2506.17609", "authors": ["Lincan Li", "Eren Erman Ozguven", "Yue Zhao", "Guang Wang", "Yiqun Xie", "Yushun Dong"], "title": "TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting", "comment": null, "summary": "Accurate typhoon track forecasting is crucial for early system warning and\ndisaster response. While Transformer-based models have demonstrated strong\nperformance in modeling the temporal dynamics of dense trajectories of humans\nand vehicles in smart cities, they usually lack access to broader contextual\nknowledge that enhances the forecasting reliability of sparse meteorological\ntrajectories, such as typhoon tracks. To address this challenge, we propose\nTyphoFormer, a novel framework that incorporates natural language descriptions\nas auxiliary prompts to improve typhoon trajectory forecasting. For each time\nstep, we use Large Language Model (LLM) to generate concise textual\ndescriptions based on the numerical attributes recorded in the North Atlantic\nhurricane database. The language descriptions capture high-level meteorological\nsemantics and are embedded as auxiliary special tokens prepended to the\nnumerical time series input. By integrating both textual and sequential\ninformation within a unified Transformer encoder, TyphoFormer enables the model\nto leverage contextual cues that are otherwise inaccessible through numerical\nfeatures alone. Extensive experiments are conducted on HURDAT2 benchmark,\nresults show that TyphoFormer consistently outperforms other state-of-the-art\nbaseline methods, particularly under challenging scenarios involving nonlinear\npath shifts and limited historical observations.", "AI": {"tldr": "TyphoFormer\u7ed3\u5408\u4e86\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4f5c\u4e3a\u8f85\u52a9\u63d0\u793a\uff0c\u4ee5\u6539\u8fdb\u53f0\u98ce\u8f68\u8ff9\u9884\u6d4b\u3002", "motivation": "\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5728\u5bf9\u667a\u6167\u57ce\u5e02\u4e2d\u4eba\u7c7b\u548c\u8f66\u8f86\u7684\u5bc6\u96c6\u8f68\u8ff9\u7684\u5efa\u6a21\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u7f3a\u4e4f\u66f4\u5e7f\u6cdb\u7684\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff0c\u800c\u8fd9\u4e9b\u77e5\u8bc6\u53ef\u4ee5\u589e\u5f3a\u7a00\u758f\u6c14\u8c61\u8f68\u8ff9\uff08\u5982\u53f0\u98ce\u8f68\u8ff9\uff09\u7684\u9884\u6d4b\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faTyphoFormer\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4f5c\u4e3a\u8f85\u52a9\u63d0\u793a\uff0c\u4ee5\u6539\u8fdb\u53f0\u98ce\u8f68\u8ff9\u9884\u6d4b\u3002", "result": "\u901a\u8fc7\u5728\u7edf\u4e00\u7684Transformer\u7f16\u7801\u5668\u4e2d\u96c6\u6210\u6587\u672c\u548c\u5e8f\u5217\u4fe1\u606f\uff0cTyphoFormer\u4f7f\u6a21\u578b\u80fd\u591f\u5229\u7528\u901a\u8fc7\u6570\u503c\u7279\u5f81\u672c\u8eab\u65e0\u6cd5\u8bbf\u95ee\u7684\u4e0a\u4e0b\u6587\u7ebf\u7d22\u3002", "conclusion": "TyphoFormer\u5728HURDAT2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u975e\u7ebf\u6027\u8def\u5f84\u504f\u79fb\u548c\u6709\u9650\u5386\u53f2\u89c2\u6d4b\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2506.17930", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.NE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.17930", "abs": "https://arxiv.org/abs/2506.17930", "authors": ["Jianyu Wang", "Zhiqiang Hu", "Lidong Bing"], "title": "Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective", "comment": "ICML 2025, and Code will be released at:\n  https://github.com/jianyu-cs/PromptQuine/", "summary": "We propose a novel prompt design paradigm that challenges conventional wisdom\nin large language model (LLM) prompting. While conventional wisdom prioritizes\nwell-crafted instructions and demonstrations for in-context learning (ICL), we\nshow that pruning random demonstrations into seemingly incoherent \"gibberish\"\ncan remarkably improve performance across diverse tasks. Notably, the\n\"gibberish\" always matches or surpasses state-of-the-art automatic prompt\noptimization techniques, achieving substantial gains regardless of LLM\nalignment. Nevertheless, discovering an effective pruning strategy is\nnon-trivial, as existing attribution methods and prompt compression algorithms\nfail to deliver robust results, let alone human intuition. In terms of this, we\npropose a self-discover prompt optimization framework, PromptQuine, an\nevolutionary search framework that automatically searches for the pruning\nstrategy by itself using only low-data regimes. Much like the emergent\ncomplexity in nature--such as symbiosis and self-organization--arising in\nresponse to resource constraints, our framework evolves and refines\nunconventional yet highly effective prompts by leveraging only the tokens\npresent within the context. We demonstrate its effectiveness across\nclassification, multi-choice question answering, generation and math reasoning\ntasks across LLMs, while achieving decent runtime efficiency. We hope our\nfindings can guide mechanistic studies on in-context learning, and provide a\ncall to action, to pave the way for more open-ended search algorithms for more\neffective LLM prompting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4fee\u526a\u968f\u673a\u6f14\u793a\u6765\u6539\u8fdbLLM\u63d0\u793a\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aPromptQuine\u7684\u6846\u67b6\u6765\u81ea\u52a8\u641c\u7d22\u6709\u6548\u7684\u4fee\u526a\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u89c2\u70b9\u4f18\u5148\u8003\u8651\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6307\u4ee4\u548c\u6f14\u793a\u4ee5\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b66\u4e60 (ICL)\uff0c\u4f46\u672c\u6587\u5bf9\u6b64\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u53d1\u73b0\u63d0\u793a\u4f18\u5316\u6846\u67b6PromptQuine\uff0c\u8fd9\u662f\u4e00\u4e2a\u8fdb\u5316\u641c\u7d22\u6846\u67b6\uff0c\u5b83\u4ec5\u4f7f\u7528\u4f4e\u6570\u636e\u673a\u5236\u81ea\u52a8\u641c\u7d22\u4fee\u526a\u7b56\u7565\u3002", "result": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5206\u7c7b\u3001\u591a\u9879\u9009\u62e9\u95ee\u7b54\u3001\u751f\u6210\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u4e0d\u9519\u7684\u8fd0\u884c\u65f6\u6548\u7387\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u63d0\u793a\u8bbe\u8ba1\u8303\u5f0f\uff0c\u901a\u8fc7\u5c06\u968f\u673a\u6f14\u793a\u4fee\u526a\u6210\u770b\u4f3c\u4e0d\u8fde\u8d2f\u7684\u201c\u4e71\u8bed\u201d\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5404\u79cd\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u7684prompt\u4f18\u5316\u6280\u672f\u3002"}}
{"id": "2506.17307", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17307", "abs": "https://arxiv.org/abs/2506.17307", "authors": ["Zhixiang Chi", "Li Gu", "Huan Liu", "Ziqiang Wang", "Yanan Wu", "Yang Wang", "Konstantinos N Plataniotis"], "title": "Learning to Adapt Frozen CLIP for Few-Shot Test-Time Domain Adaptation", "comment": "ICLR2025,https://github.com/chi-chi-zx/L2C", "summary": "Few-shot Test-Time Domain Adaptation focuses on adapting a model at test time\nto a specific domain using only a few unlabeled examples, addressing domain\nshift. Prior methods leverage CLIP's strong out-of-distribution (OOD) abilities\nby generating domain-specific prompts to guide its generalized, frozen\nfeatures. However, since downstream datasets are not explicitly seen by CLIP,\nsolely depending on the feature space knowledge is constrained by CLIP's prior\nknowledge. Notably, when using a less robust backbone like ViT-B/16,\nperformance significantly drops on challenging real-world benchmarks. Departing\nfrom the state-of-the-art of inheriting the intrinsic OOD capability of CLIP,\nthis work introduces learning directly on the input space to complement the\ndataset-specific knowledge for frozen CLIP. Specifically, an independent side\nbranch is attached in parallel with CLIP and enforced to learn exclusive\nknowledge via revert attention. To better capture the dataset-specific label\nsemantics for downstream adaptation, we propose to enhance the inter-dispersion\namong text features via greedy text ensemble and refinement. The text and\nvisual features are then progressively fused in a domain-aware manner by a\ngenerated domain prompt to adapt toward a specific domain. Extensive\nexperiments show our method's superiority on 5 large-scale benchmarks (WILDS\nand DomainNet), notably improving over smaller networks like ViT-B/16 with\ngains of \\textbf{+5.1} in F1 for iWildCam and \\textbf{+3.1\\%} in WC Acc for\nFMoW.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5c11\u6837\u672c\u6d4b\u8bd5\u65f6\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8f93\u5165\u7a7a\u95f4\u5b66\u4e60\u548c\u589e\u5f3a\u6587\u672c\u7279\u5f81\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u7684\u9002\u5e94\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5229\u7528CLIP\u7684\u5f3a\u5927\u5206\u5e03\u5916\uff08OOD\uff09\u80fd\u529b\uff0c\u901a\u8fc7\u751f\u6210\u9886\u57df\u7279\u5b9a\u7684\u63d0\u793a\u6765\u6307\u5bfc\u5176\u5e7f\u4e49\u7684\u51bb\u7ed3\u7279\u5f81\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4e0b\u6e38\u6570\u636e\u96c6\u6ca1\u6709\u88abCLIP\u660e\u786e\u770b\u5230\uff0c\u56e0\u6b64\u4ec5\u4f9d\u8d56\u4e8e\u7279\u5f81\u7a7a\u95f4\u77e5\u8bc6\u53d7\u5230CLIP\u5148\u9a8c\u77e5\u8bc6\u7684\u9650\u5236\u3002\u5f53\u4f7f\u7528\u4e0d\u592a\u7a33\u5065\u7684\u9aa8\u5e72\u7f51\u7edc\uff08\u5982ViT-B/16\uff09\u65f6\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u73b0\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\u3002", "method": "\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u76f4\u63a5\u5728\u8f93\u5165\u7a7a\u95f4\u4e0a\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4ee5\u8865\u5145\u51bb\u7ed3\u7684CLIP\u7684\u6570\u636e\u96c6\u7279\u5b9a\u77e5\u8bc6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4e00\u4e2a\u72ec\u7acb\u7684\u4fa7\u5206\u652f\u4e0eCLIP\u5e76\u884c\u8fde\u63a5\uff0c\u5e76\u901a\u8fc7\u53cd\u8f6c\u6ce8\u610f\u529b\u5f3a\u5236\u5b66\u4e60\u72ec\u5360\u77e5\u8bc6\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u6355\u83b7\u4e0b\u6e38\u9002\u5e94\u7684\u6570\u636e\u96c6\u7279\u5b9a\u6807\u7b7e\u8bed\u4e49\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u901a\u8fc7\u8d2a\u5a6a\u6587\u672c\u96c6\u6210\u548c\u7ec6\u5316\u6765\u589e\u5f3a\u6587\u672c\u7279\u5f81\u4e4b\u95f4\u7684\u4e92\u6563\u6027\u3002\u7136\u540e\uff0c\u6587\u672c\u548c\u89c6\u89c9\u7279\u5f81\u901a\u8fc7\u751f\u6210\u7684\u9886\u57df\u63d0\u793a\u4ee5\u9886\u57df\u611f\u77e5\u7684\u65b9\u5f0f\u9010\u6b65\u878d\u5408\uff0c\u4ee5\u9002\u5e94\u7279\u5b9a\u9886\u57df\u3002", "result": "\u8be5\u65b9\u6cd5\u57285\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff08WILDS\u548cDomainNet\uff09\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u5c24\u5176\u662f\u5728\u8f83\u5c0f\u7684\u7f51\u7edc\uff08\u5982ViT-B/16\uff09\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4f8b\u5982\u5728iWildCam\u7684F1\u6307\u6807\u4e0a\u63d0\u9ad8\u4e86+5.1\uff0c\u5728FMoW\u7684WC Acc\u6307\u6807\u4e0a\u63d0\u9ad8\u4e86+3.1%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u57285\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff08WILDS\u548cDomainNet\uff09\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u5c24\u5176\u662f\u5728\u8f83\u5c0f\u7684\u7f51\u7edc\uff08\u5982ViT-B/16\uff09\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4f8b\u5982\u5728iWildCam\u7684F1\u6307\u6807\u4e0a\u63d0\u9ad8\u4e86+5.1\uff0c\u5728FMoW\u7684WC Acc\u6307\u6807\u4e0a\u63d0\u9ad8\u4e86+3.1%\u3002"}}
{"id": "2506.17562", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.17562", "abs": "https://arxiv.org/abs/2506.17562", "authors": ["Haoxuan Che", "Haibo Jin", "Zhengrui Guo", "Yi Lin", "Cheng Jin", "Hao Chen"], "title": "LLM-driven Medical Report Generation via Communication-efficient Heterogeneous Federated Learning", "comment": null, "summary": "LLMs have demonstrated significant potential in Medical Report Generation\n(MRG), yet their development requires large amounts of medical image-report\npairs, which are commonly scattered across multiple centers. Centralizing these\ndata is exceptionally challenging due to privacy regulations, thereby impeding\nmodel development and broader adoption of LLM-driven MRG models. To address\nthis challenge, we present FedMRG, the first framework that leverages Federated\nLearning (FL) to enable privacy-preserving, multi-center development of\nLLM-driven MRG models, specifically designed to overcome the critical challenge\nof communication-efficient LLM training under multi-modal data heterogeneity.\nTo start with, our framework tackles the fundamental challenge of communication\noverhead in FL-LLM tuning by employing low-rank factorization to efficiently\ndecompose parameter updates, significantly reducing gradient transmission costs\nand making LLM-driven MRG feasible in bandwidth-constrained FL settings.\nFurthermore, we observed the dual heterogeneity in MRG under the FL scenario:\nvarying image characteristics across medical centers, as well as diverse\nreporting styles and terminology preferences. To address this, we further\nenhance FedMRG with (1) client-aware contrastive learning in the MRG encoder,\ncoupled with diagnosis-driven prompts, which capture both globally\ngeneralizable and locally distinctive features while maintaining diagnostic\naccuracy; and (2) a dual-adapter mutual boosting mechanism in the MRG decoder\nthat harmonizes generic and specialized adapters to address variations in\nreporting styles and terminology. Through extensive evaluation of our\nestablished FL-MRG benchmark, we demonstrate the generalizability and\nadaptability of FedMRG, underscoring its potential in harnessing multi-center\ndata and generating clinically accurate reports while maintaining communication\nefficiency.", "AI": {"tldr": "FedMRG is a federated learning framework for privacy-preserving, multi-center development of LLM-driven MRG models, addressing communication overhead and data heterogeneity challenges.", "motivation": "Development of LLM-driven Medical Report Generation (MRG) models requires large amounts of medical image-report pairs scattered across multiple centers, but centralizing these data is challenging due to privacy regulations, impeding model development.", "method": "The FedMRG framework employs low-rank factorization for communication-efficient LLM tuning, client-aware contrastive learning with diagnosis-driven prompts in the MRG encoder, and a dual-adapter mutual boosting mechanism in the MRG decoder.", "result": "The FedMRG framework achieves communication efficiency and addresses data heterogeneity in multi-center medical report generation.", "conclusion": "The FedMRG framework demonstrates generalizability and adaptability in generating clinically accurate reports while maintaining communication efficiency, underscoring its potential in harnessing multi-center data."}}
{"id": "2506.17611", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.17611", "abs": "https://arxiv.org/abs/2506.17611", "authors": ["Jinchuan Tian", "William Chen", "Yifan Peng", "Jiatong Shi", "Siddhant Arora", "Shikhar Bharadwaj", "Takashi Maekaku", "Yusuke Shinohara", "Keita Goto", "Xiang Yue", "Huck Yang", "Shinji Watanabe"], "title": "OpusLM: A Family of Open Unified Speech Language Models", "comment": null, "summary": "This paper presents Open Unified Speech Language Models (OpusLMs), a family\nof open foundational speech language models (SpeechLMs) up to 7B. Initialized\nfrom decoder-only text language models, the OpusLMs are continuously\npre-trained on 213K hours of speech-text pairs and 292B text-only tokens. We\ndemonstrate our OpusLMs achieve comparable (or even superior) performance with\nexisting SpeechLMs in speech recognition, speech synthesis, and text-only\ncapabilities. Technically, this paper articulates our SpeechLM designs on\ntokenization, multi-stream language models, and multi-stage training\nstrategies. We experimentally demonstrate the importance of model size scaling\nand the effect of annealing data selection. The OpusLMs are all built from\npublicly available materials and are fully transparent models. We release our\ncode, data, checkpoints, and training logs to facilitate open SpeechLM research", "AI": {"tldr": "OpusLMs, a family of open foundational speech language models up to 7B, achieves comparable or superior performance with existing SpeechLMs. The code, data, checkpoints, and training logs are released.", "motivation": "To present Open Unified Speech Language Models (OpusLMs), a family of open foundational speech language models (SpeechLMs) up to 7B, built from publicly available materials and are fully transparent models.", "method": "Continuously pre-trained on 213K hours of speech-text pairs and 292B text-only tokens, with specific designs on tokenization, multi-stream language models, and multi-stage training strategies.", "result": "Demonstrates the importance of model size scaling and the effect of annealing data selection.", "conclusion": "OpusLMs achieve comparable or superior performance with existing SpeechLMs in speech recognition, speech synthesis, and text-only capabilities."}}
{"id": "2506.17959", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17959", "abs": "https://arxiv.org/abs/2506.17959", "authors": ["Lizzy Farrugia", "Lilian M. Azzopardi", "Jeremy Debattista", "Charlie Abela"], "title": "medicX-KG: A Knowledge Graph for Pharmacists' Drug Information Needs", "comment": null, "summary": "The role of pharmacists is evolving from medicine dispensing to delivering\ncomprehensive pharmaceutical services within multidisciplinary healthcare\nteams. Central to this shift is access to accurate, up-to-date medicinal\nproduct information supported by robust data integration. Leveraging artificial\nintelligence and semantic technologies, Knowledge Graphs (KGs) uncover hidden\nrelationships and enable data-driven decision-making. This paper presents\nmedicX-KG, a pharmacist-oriented knowledge graph supporting clinical and\nregulatory decisions. It forms the semantic layer of the broader medicX\nplatform, powering predictive and explainable pharmacy services. medicX-KG\nintegrates data from three sources, including, the British National Formulary\n(BNF), DrugBank, and the Malta Medicines Authority (MMA) that addresses Malta's\nregulatory landscape and combines European Medicines Agency alignment with\npartial UK supply dependence. The KG tackles the absence of a unified national\ndrug repository, reducing pharmacists' reliance on fragmented sources. Its\ndesign was informed by interviews with practicing pharmacists to ensure\nreal-world applicability. We detail the KG's construction, including data\nextraction, ontology design, and semantic mapping. Evaluation demonstrates that\nmedicX-KG effectively supports queries about drug availability, interactions,\nadverse reactions, and therapeutic classes. Limitations, including missing\ndetailed dosage encoding and real-time updates, are discussed alongside\ndirections for future enhancements.", "AI": {"tldr": "This paper presents medicX-KG, a pharmacist-oriented knowledge graph supporting clinical and regulatory decisions. It integrates data from three sources and effectively supports queries about drug availability, interactions, adverse reactions, and therapeutic classes.", "motivation": "The role of pharmacists is evolving from medicine dispensing to delivering comprehensive pharmaceutical services. Central to this shift is access to accurate, up-to-date medicinal product information. The KG tackles the absence of a unified national drug repository, reducing pharmacists' reliance on fragmented sources.", "method": "Knowledge graph construction, including data extraction, ontology design, and semantic mapping.", "result": "medicX-KG, a pharmacist-oriented knowledge graph supporting clinical and regulatory decisions. It integrates data from three sources, including, the British National Formulary (BNF), DrugBank, and the Malta Medicines Authority (MMA).", "conclusion": "medicX-KG effectively supports queries about drug availability, interactions, adverse reactions, and therapeutic classes."}}
{"id": "2506.17323", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17323", "abs": "https://arxiv.org/abs/2506.17323", "authors": ["Tamas Bisztray", "Bilel Cherif", "Richard A. Dubniczky", "Nils Gruschka", "Bertalan Borsos", "Mohamed Amine Ferrag", "Attila Kovacs", "Vasileios Mavroeidis", "Norbert Tihanyi"], "title": "I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution", "comment": null, "summary": "Detecting AI-generated code, deepfakes, and other synthetic content is an\nemerging research challenge. As code generated by Large Language Models (LLMs)\nbecomes more common, identifying the specific model behind each sample is\nincreasingly important. This paper presents the first systematic study of LLM\nauthorship attribution for C programs. We released CodeT5-Authorship, a novel\nmodel that uses only the encoder layers from the original CodeT5\nencoder-decoder architecture, discarding the decoder to focus on\nclassification. Our model's encoder output (first token) is passed through a\ntwo-layer classification head with GELU activation and dropout, producing a\nprobability distribution over possible authors. To evaluate our approach, we\nintroduce LLM-AuthorBench, a benchmark of 32,000 compilable C programs\ngenerated by eight state-of-the-art LLMs across diverse tasks. We compare our\nmodel to seven traditional ML classifiers and eight fine-tuned transformer\nmodels, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3,\nLongformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, our model\nachieves 97.56% accuracy in distinguishing C programs generated by closely\nrelated models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class\nattribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku,\nGPT-4.1, Llama 3.3, and DeepSeek-V3). To support open science, we release the\nCodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant\nGoogle Colab scripts on GitHub: https://github.com/LLMauthorbench/.", "AI": {"tldr": "This paper presents CodeT5-Authorship and LLM-AuthorBench for LLM authorship attribution of C code, achieving high accuracy in identifying the source LLM.", "motivation": "Identifying the specific LLM behind generated code is increasingly important as LLM-generated code becomes more prevalent.", "method": "The paper introduces CodeT5-Authorship, a novel model based on the CodeT5 encoder architecture, and LLM-AuthorBench, a benchmark dataset of 32,000 C programs generated by eight LLMs. The model's performance is compared against several baselines.", "result": "CodeT5-Authorship achieves 97.56% accuracy in binary classification of closely related models (GPT-4.1 and GPT-4o) and 95.40% accuracy in multi-class attribution among five leading LLMs.", "conclusion": "CodeT5-Authorship achieves high accuracy in distinguishing and attributing C programs generated by various LLMs, outperforming traditional ML classifiers and fine-tuned transformer models."}}
{"id": "2506.18535", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.18535", "abs": "https://arxiv.org/abs/2506.18535", "authors": ["Manu Pande", "Shahil Kumar", "Anay Yatin Damle"], "title": "When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking", "comment": null, "summary": "This paper investigates the counterintuitive phenomenon where fine-tuning\npre-trained transformer models degrades performance on the MS MARCO passage\nranking task. Through comprehensive experiments involving five model\nvariants-including full parameter fine-tuning and parameter efficient LoRA\nadaptations-we demonstrate that all fine-tuning approaches underperform the\nbase sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). Our\nanalysis reveals that fine-tuning disrupts the optimal embedding space\nstructure learned during the base model's extensive pre-training on 1 billion\nsentence pairs, including 9.1 million MS MARCO samples. UMAP visualizations\nshow progressive embedding space flattening, while training dynamics analysis\nand computational efficiency metrics further support our findings. These\nresults challenge conventional wisdom about transfer learning effectiveness on\nsaturated benchmarks and suggest architectural innovations may be necessary for\nmeaningful improvements.", "AI": {"tldr": "fine-tuning pre-trained transformer models degrades performance on the MS MARCO passage ranking task", "motivation": "investigates the counterintuitive phenomenon where fine-tuning pre-trained transformer models degrades performance", "method": "comprehensive experiments involving five model variants, UMAP visualizations, training dynamics analysis and computational efficiency metrics", "result": "all fine-tuning approaches underperform the base sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). progressive embedding space flattening", "conclusion": "fine-tuning degrades performance on MS MARCO passage ranking task and disrupts the optimal embedding space structure"}}
{"id": "2506.17587", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17587", "abs": "https://arxiv.org/abs/2506.17587", "authors": ["Le Yu", "Kaishen Wang", "Jianlong Xiong", "Yue Cao", "Tao He"], "title": "HalluRNN: Mitigating Hallucinations via Recurrent Cross-Layer Reasoning in Large Vision-Language Models", "comment": "6 figures, 9 tables", "summary": "Though Large Vision-Language Models (LVLMs) have achieved remarkable\nperformance across various tasks, they are still prone to\nhallucinations-generating outputs that are textually plausible but visually\nungrounded. While prior approaches generally address this issue through\ndata-centric fine-tuning or innovative decoding strategies, these methods often\nrequire substantial resources or task-specific configurations. In this work, we\nintroduce an architecture-level solution, HalluRNN, which enhances model\nstability through recurrent cross-layer reasoning. Specifically, we propose a\nnovel Dual-Gated Depth Propagation Unit (DG-DPU) module, which is shared across\nlayers and recurrently refines hidden states. This allows for the adaptive\npropagation of information throughout the model, enforces consistency across\nlayers, and mitigates hallucinations caused by representational drift. By\nfine-tuning only the DG-DPU module, HalluRNN achieves strong and robust\nperformance across multiple benchmarks.", "AI": {"tldr": "HalluRNN\u901a\u8fc7\u5faa\u73af\u8de8\u5c42\u63a8\u7406\u89e3\u51b3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4ec5\u9700\u5fae\u8c03DG-DPU\u6a21\u5757\u5373\u53ef\u5b9e\u73b0\u826f\u597d\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u5373\u751f\u6210\u5728\u6587\u672c\u4e0a\u5408\u7406\u4f46\u5728\u89c6\u89c9\u4e0a\u65e0\u6839\u636e\u7684\u8f93\u51fa\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u95e8\u63a7\u6df1\u5ea6\u4f20\u64ad\u5355\u5143\uff08DG-DPU\uff09\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5728\u5404\u5c42\u4e4b\u95f4\u5171\u4eab\u5e76\u5faa\u73af\u7ec6\u5316\u9690\u85cf\u72b6\u6001\u3002", "result": "\u901a\u8fc7\u4ec5\u5fae\u8c03DG-DPU\u6a21\u5757\uff0cHalluRNN\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u5f3a\u5927\u800c\u7a33\u5065\u7684\u6027\u80fd\u3002", "conclusion": "HalluRNN\u901a\u8fc7\u5faa\u73af\u8de8\u5c42\u63a8\u7406\u589e\u5f3a\u6a21\u578b\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u5f3a\u5927\u800c\u7a33\u5065\u7684\u6027\u80fd\u3002"}}
{"id": "2506.17630", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.17630", "abs": "https://arxiv.org/abs/2506.17630", "authors": ["Yang Wu", "Yifan Zhang", "Yiwei Wang", "Yujun Cai", "Yurong Wu", "Yuran Wang", "Ning Xu", "Jian Cheng"], "title": "Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in LLMs", "comment": "14 pages, 8 figures", "summary": "While Large Language Models (LLMs) demonstrate impressive reasoning\ncapabilities, growing evidence suggests much of their success stems from\nmemorized answer-reasoning patterns rather than genuine inference. In this\nwork, we investigate a central question: are LLMs primarily anchored to final\nanswers or to the textual pattern of reasoning chains? We propose a five-level\nanswer-visibility prompt framework that systematically manipulates answer cues\nand probes model behavior through indirect, behavioral analysis. Experiments\nacross state-of-the-art LLMs reveal a strong and consistent reliance on\nexplicit answers. The performance drops by 26.90\\% when answer cues are masked,\neven with complete reasoning chains. These findings suggest that much of the\nreasoning exhibited by LLMs may reflect post-hoc rationalization rather than\ntrue inference, calling into question their inferential depth. Our study\nuncovers the answer-anchoring phenomenon with rigorous empirical validation and\nunderscores the need for a more nuanced understanding of what constitutes\nreasoning in LLMs.", "AI": {"tldr": "LLM\u7684\u63a8\u7406\u53ef\u80fd\u53cd\u6620\u4e8b\u540e\u5408\u7406\u5316\uff0c\u800c\u4e0d\u662f\u771f\u6b63\u7684\u63a8\u7406\uff0c\u6211\u4eec\u9700\u8981\u5bf9LLM\u7684\u63a8\u7406\u6784\u6210\u8fdb\u884c\u66f4\u7ec6\u81f4\u7684\u7406\u89e3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8868\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8d8a\u6765\u8d8a\u591a\u7684\u8bc1\u636e\u8868\u660e\uff0c\u5b83\u4eec\u7684\u6210\u529f\u4e3b\u8981\u6e90\u4e8e\u8bb0\u5fc6\u7684\u7b54\u6848-\u63a8\u7406\u6a21\u5f0f\uff0c\u800c\u4e0d\u662f\u771f\u6b63\u7684\u63a8\u7406\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e94\u7ea7\u7b54\u6848\u53ef\u89c1\u6027\u63d0\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u95f4\u63a5\u884c\u4e3a\u5206\u6790\u6765\u63a2\u6d4b\u6a21\u578b\u884c\u4e3a\u3002", "result": "\u5f53\u7b54\u6848\u7ebf\u7d22\u88ab\u5c4f\u853d\u65f6\uff0c\u5373\u4f7f\u6709\u5b8c\u6574\u7684\u63a8\u7406\u94fe\uff0c\u6027\u80fd\u4e5f\u4f1a\u4e0b\u964d26.90%\u3002", "conclusion": "LLMs\u4e3b\u8981\u4f9d\u8d56\u4e8e\u663e\u5f0f\u7b54\u6848\uff0c\u800c\u4e0d\u662f\u771f\u6b63\u7684\u63a8\u7406\u3002"}}
{"id": "2506.18019", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18019", "abs": "https://arxiv.org/abs/2506.18019", "authors": ["Yuanchen Bei", "Weizhi Zhang", "Siwen Wang", "Weizhi Chen", "Sheng Zhou", "Hao Chen", "Yong Li", "Jiajun Bu", "Shirui Pan", "Yizhou Yu", "Irwin King", "Fakhri Karray", "Philip S. Yu"], "title": "Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities", "comment": "20 pages, 7 figures", "summary": "AI agents have experienced a paradigm shift, from early dominance by\nreinforcement learning (RL) to the rise of agents powered by large language\nmodels (LLMs), and now further advancing towards a synergistic fusion of RL and\nLLM capabilities. This progression has endowed AI agents with increasingly\nstrong abilities. Despite these advances, to accomplish complex real-world\ntasks, agents are required to plan and execute effectively, maintain reliable\nmemory, and coordinate smoothly with other agents. Achieving these capabilities\ninvolves contending with ever-present intricate information, operations, and\ninteractions. In light of this challenge, data structurization can play a\npromising role by transforming intricate and disorganized data into\nwell-structured forms that agents can more effectively understand and process.\nIn this context, graphs, with their natural advantage in organizing, managing,\nand harnessing intricate data relationships, present a powerful data paradigm\nfor structurization to support the capabilities demanded by advanced AI agents.\nTo this end, this survey presents a first systematic review of how graphs can\nempower AI agents. Specifically, we explore the integration of graph techniques\nwith core agent functionalities, highlight notable applications, and identify\nprospective avenues for future research. By comprehensively surveying this\nburgeoning intersection, we hope to inspire the development of next-generation\nAI agents equipped to tackle increasingly sophisticated challenges with graphs.\nRelated resources are collected and continuously updated for the community in\nthe Github link.", "AI": {"tldr": "Graphs can help AI agents handle complex tasks by better organizing data. This paper reviews how graphs are used with AI agents and suggests future research directions.", "motivation": "AI agents need to plan, execute, maintain memory, and coordinate with others to accomplish complex real-world tasks, which involves dealing with intricate information, operations, and interactions. Data structurization, especially using graphs, can help agents understand and process data more effectively.", "method": "A systematic review of the integration of graph techniques with core agent functionalities in AI agents.", "result": "The survey presents a systematic review of how graphs can empower AI agents.", "conclusion": "This survey reviews how graphs can empower AI agents, explores the integration of graph techniques with core agent functionalities, highlights applications, and identifies future research directions."}}
{"id": "2506.17324", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17324", "abs": "https://arxiv.org/abs/2506.17324", "authors": ["Emma Finn", "T. Anderson Keller", "Manos Theodosis", "Demba E. Ba"], "title": "Origins of Creativity in Attention-Based Diffusion Models", "comment": null, "summary": "As diffusion models have become the tool of choice for image generation and\nas the quality of the images continues to improve, the question of how\n`creativity' originates in diffusion has become increasingly important. The\nscore matching perspective on diffusion has proven particularly fruitful for\nunderstanding how and why diffusion models generate images that remain\nplausible while differing significantly from their training images. In\nparticular, as explained in (Kamb \\& Ganguli, 2024) and others, e.g.,\n(Ambrogioni, 2023), theory suggests that if our score matching were optimal, we\nwould only be able to recover training samples through our diffusion process.\nHowever, as shown by Kamb \\& Ganguli, (2024), in diffusion models where the\nscore is parametrized by a simple CNN, the inductive biases of the CNN itself\n(translation equivariance and locality) allow the model to generate samples\nthat globally do not match any training samples, but are rather patch-wise\n`mosaics'. Notably, however, this theory does not extend to describe the role\nof self-attention in this process. In this work, we take a preliminary step in\nthis direction to extend this theory to the case of diffusion models whose\nscore is parametrized by a CNN with a final self-attention layer. We show that\nour theory suggests that self-attention will induce a globally image-consistent\narrangement of local features beyond the patch-level in generated samples, and\nwe verify this behavior empirically on a carefully crafted dataset.", "AI": {"tldr": "extend the theory to the case of diffusion models whose score is parametrized by a CNN with a final self-attention layer, and verify this behavior empirically on a carefully crafted dataset", "motivation": "the question of how `creativity' originates in diffusion has become increasingly important. theory does not extend to describe the role of self-attention in this process", "method": "extend the theory to the case of diffusion models whose score is parametrized by a CNN with a final self-attention layer", "result": "verify this behavior empirically on a carefully crafted dataset", "conclusion": "self-attention will induce a globally image-consistent arrangement of local features beyond the patch-level in generated samples"}}
{"id": "2506.18902", "categories": ["cs.AI", "cs.CL", "cs.IR", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2506.18902", "abs": "https://arxiv.org/abs/2506.18902", "authors": ["Michael G\u00fcnther", "Saba Sturua", "Mohammad Kalim Akram", "Isabelle Mohr", "Andrei Ungureanu", "Sedigheh Eslami", "Scott Martens", "Bo Wang", "Nan Wang", "Han Xiao"], "title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval", "comment": "22 pages, 1-10 main, 14-22 experimental results, benchmark tables", "summary": "We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding\nmodel that unifies text and image representations through a novel architecture\nsupporting both single-vector and multi-vector embeddings in the late\ninteraction style. The model incorporates task-specific Low-Rank Adaptation\n(LoRA) adapters to optimize performance across diverse retrieval scenarios,\nincluding query-based information retrieval, cross-modal semantic similarity,\nand programming code search. Comprehensive evaluations demonstrate that\njina-embeddings-v4 achieves state-of-the-art performance on both single- modal\nand cross-modal retrieval tasks, with particular strength in processing\nvisually rich content such as tables, charts, diagrams, and mixed-media\nformats. To facilitate evaluation of this capability, we also introduce\nJina-VDR, a novel benchmark specifically designed for visually rich image\nretrieval.", "AI": {"tldr": "jina-embeddings-v4\u662f\u4e00\u79cd\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578b\uff0c\u5b83\u7edf\u4e00\u4e86\u6587\u672c\u548c\u56fe\u50cf\u8868\u793a\uff0c\u5e76\u5728\u5404\u79cd\u68c0\u7d22\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u7edf\u4e00\u6587\u672c\u548c\u56fe\u50cf\u8868\u793a\u3002", "method": "\u4e00\u79cd\u65b0\u9896\u7684\u67b6\u6784\uff0c\u652f\u6301\u540e\u671f\u4ea4\u4e92\u98ce\u683c\u4e2d\u7684\u5355\u5411\u91cf\u548c\u591a\u5411\u91cf\u5d4c\u5165\u3002", "result": "jina-embeddings-v4\u5728\u5355\u6a21\u6001\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "jina-embeddings-v4\u5728\u5355\u6a21\u6001\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5bcc\u542b\u89c6\u89c9\u5185\u5bb9\uff08\u5982\u56fe\u8868\u3001\u56fe\u8868\u3001\u56fe\u8868\u548c\u6df7\u5408\u5a92\u4f53\u683c\u5f0f\uff09\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2506.17590", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.17590", "abs": "https://arxiv.org/abs/2506.17590", "authors": ["Mihir Godbole", "Xiangbo Gao", "Zhengzhong Tu"], "title": "DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving", "comment": "19 pages, 5 figures, Preprint under review. Code available at:\n  https://github.com/taco-group/DRAMA-X", "summary": "Understanding the short-term motion of vulnerable road users (VRUs) like\npedestrians and cyclists is critical for safe autonomous driving, especially in\nurban scenarios with ambiguous or high-risk behaviors. While vision-language\nmodels (VLMs) have enabled open-vocabulary perception, their utility for\nfine-grained intent reasoning remains underexplored. Notably, no existing\nbenchmark evaluates multi-class intent prediction in safety-critical\nsituations, To address this gap, we introduce DRAMA-X, a fine-grained benchmark\nconstructed from the DRAMA dataset via an automated annotation pipeline.\nDRAMA-X contains 5,686 accident-prone frames labeled with object bounding\nboxes, a nine-class directional intent taxonomy, binary risk scores,\nexpert-generated action suggestions for the ego vehicle, and descriptive motion\nsummaries. These annotations enable a structured evaluation of four\ninterrelated tasks central to autonomous decision-making: object detection,\nintent prediction, risk assessment, and action suggestion. As a reference\nbaseline, we propose SGG-Intent, a lightweight, training-free framework that\nmirrors the ego vehicle's reasoning pipeline. It sequentially generates a scene\ngraph from visual input using VLM-backed detectors, infers intent, assesses\nrisk, and recommends an action using a compositional reasoning stage powered by\na large language model. We evaluate a range of recent VLMs, comparing\nperformance across all four DRAMA-X tasks. Our experiments demonstrate that\nscene-graph-based reasoning enhances intent prediction and risk assessment,\nespecially when contextual cues are explicitly modeled.", "AI": {"tldr": "Introduces DRAMA-X, a new benchmark for intent prediction in autonomous driving, and proposes SGG-Intent, a scene-graph-based reasoning framework.", "motivation": "Understanding the short-term motion of vulnerable road users (VRUs) like pedestrians and cyclists is critical for safe autonomous driving, especially in urban scenarios with ambiguous or high-risk behaviors. While vision-language models (VLMs) have enabled open-vocabulary perception, their utility for fine-grained intent reasoning remains underexplored. Notably, no existing benchmark evaluates multi-class intent prediction in safety-critical situations", "method": "A lightweight, training-free framework that mirrors the ego vehicle's reasoning pipeline. It sequentially generates a scene graph from visual input using VLM-backed detectors, infers intent, assesses risk, and recommends an action using a compositional reasoning stage powered by a large language model.", "result": "DRAMA-X contains 5,686 accident-prone frames labeled with object bounding boxes, a nine-class directional intent taxonomy, binary risk scores, expert-generated action suggestions for the ego vehicle, and descriptive motion summaries. These annotations enable a structured evaluation of four interrelated tasks central to autonomous decision-making: object detection, intent prediction, risk assessment, and action suggestion.", "conclusion": "Scene-graph-based reasoning enhances intent prediction and risk assessment, especially when contextual cues are explicitly modeled."}}
{"id": "2506.17637", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17637", "abs": "https://arxiv.org/abs/2506.17637", "authors": ["Yang Wu", "Yifan Zhang", "Yurong Wu", "Yuran Wang", "Junkai Zhang", "Jian Cheng"], "title": "Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation", "comment": "17 pages, 12 figures", "summary": "Large Language Models (LLMs) have revolutionized various domains but\nencounter substantial challenges in tackling optimization modeling tasks for\nOperations Research (OR), particularly when dealing with complex problem. In\nthis work, we propose Step-Opt-Instruct, a framework that augments existing\ndatasets and generates high-quality fine-tuning data tailored to optimization\nmodeling. Step-Opt-Instruct employs iterative problem generation to\nsystematically increase problem complexity and stepwise validation to\nrigorously verify data, preventing error propagation and ensuring the quality\nof the generated dataset. Leveraging this framework, we fine-tune open-source\nLLMs, including LLaMA-3-8B and Mistral-7B, to develop Step-Opt--a model that\nachieves state-of-the-art performance on benchmarks such as NL4OPT, MAMO, and\nIndustryOR. Extensive experiments demonstrate the superior performance of\nStep-Opt, especially in addressing complex OR tasks, with a notable 17.01\\%\nimprovement in micro average accuracy on difficult problems. These findings\nhighlight the effectiveness of combining structured validation with gradual\nproblem refinement to advance the automation of decision-making processes using\nLLMs.The code and dataset are available at https://github.com/samwu-learn/Step.", "AI": {"tldr": "Step-Opt-Instruct improves LLMs for optimization modeling by generating high-quality fine-tuning data, leading to state-of-the-art performance on OR benchmarks.", "motivation": "LLMs encounter substantial challenges in tackling optimization modeling tasks for OR, particularly when dealing with complex problems.", "method": "Step-Opt-Instruct, a framework that augments existing datasets and generates high-quality fine-tuning data tailored to optimization modeling, employing iterative problem generation to systematically increase problem complexity and stepwise validation to rigorously verify data.", "result": "Step-Opt achieves state-of-the-art performance on benchmarks such as NL4OPT, MAMO, and IndustryOR.", "conclusion": "Step-Opt demonstrates superior performance, especially in addressing complex OR tasks, with a notable 17.01% improvement in micro average accuracy on difficult problems, highlighting the effectiveness of combining structured validation with gradual problem refinement to advance the automation of decision-making processes using LLMs."}}
{"id": "2506.18044", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18044", "abs": "https://arxiv.org/abs/2506.18044", "authors": ["Joseph Babb", "Joohyung Lee"], "title": "Action Language BC+", "comment": "Journal of Logic and Computation, 2015", "summary": "Action languages are formal models of parts of natural language that are\ndesigned to describe effects of actions. Many of these languages can be viewed\nas high level notations of answer set programs structured to represent\ntransition systems. However, the form of answer set programs considered in the\nearlier work is quite limited in comparison with the modern Answer Set\nProgramming (ASP) language, which allows several useful constructs for\nknowledge representation, such as choice rules, aggregates, and abstract\nconstraint atoms. We propose a new action language called BC+, which closes the\ngap between action languages and the modern ASP language. The main idea is to\ndefine the semantics of BC+ in terms of general stable model semantics for\npropositional formulas, under which many modern ASP language constructs can be\nidentified with shorthands for propositional formulas. Language BC+ turns out\nto be sufficiently expressive to encompass the best features of other action\nlanguages, such as languages B, C, C+, and BC. Computational methods available\nin ASP solvers are readily applicable to compute BC+, which led to an\nimplementation of the language by extending system cplus2asp.", "AI": {"tldr": "This paper introduces a new action language called BC+ that bridges the gap between action languages and modern ASP, leveraging stable model semantics and ASP solver capabilities.", "motivation": "close the gap between action languages and the modern ASP language,The form of answer set programs considered in the earlier work is quite limited in comparison with the modern Answer Set Programming (ASP) language, which allows several useful constructs for knowledge representation, such as choice rules, aggregates, and abstract constraint atoms.", "method": "define the semantics of BC+ in terms of general stable model semantics for propositional formulas, under which many modern ASP language constructs can be identified with shorthands for propositional formulas.", "result": "We propose a new action language called BC+", "conclusion": "Language BC+ is sufficiently expressive to encompass the best features of other action languages, such as languages B, C, C+, and BC. Computational methods available in ASP solvers are readily applicable to compute BC+, which led to an implementation of the language by extending system cplus2asp."}}
{"id": "2506.17326", "categories": ["cs.LG", "stat.AP", "stat.ML", "62H05, 62G32, 62P10, 68T05"], "pdf": "https://arxiv.org/pdf/2506.17326", "abs": "https://arxiv.org/abs/2506.17326", "authors": ["Agnideep Aich", "Md Monzur Murshed", "Sameera Hewage", "Amanda Mayeaux"], "title": "CopulaSMOTE: A Copula-Based Oversampling Approach for Imbalanced Classification in Diabetes Prediction", "comment": null, "summary": "Diabetes mellitus poses a significant health risk, as nearly 1 in 9 people\nare affected by it. Early detection can significantly lower this risk. Despite\nsignificant advancements in machine learning for identifying diabetic cases,\nresults can still be influenced by the imbalanced nature of the data. To\naddress this challenge, our study considered copula-based data augmentation,\nwhich preserves the dependency structure when generating data for the minority\nclass and integrates it with machine learning (ML) techniques. We selected the\nPima Indian dataset and generated data using A2 copula, then applied four\nmachine learning algorithms: logistic regression, random forest, gradient\nboosting, and extreme gradient boosting. Our findings indicate that XGBoost\ncombined with A2 copula oversampling achieved the best performance improving\naccuracy by 4.6%, precision by 15.6%, recall by 20.4%, F1-score by 18.2% and\nAUC by 25.5% compared to the standard SMOTE method. Furthermore, we\nstatistically validated our results using the McNemar test. This research\nrepresents the first known use of A2 copulas for data augmentation and serves\nas an alternative to the SMOTE technique, highlighting the efficacy of copulas\nas a statistical method in machine learning applications.", "AI": {"tldr": "This paper introduces a new data augmentation method using A2 copulas to improve the performance of machine learning models in detecting diabetes, achieving better results than SMOTE with XGBoost.", "motivation": "Diabetes mellitus poses a significant health risk, as nearly 1 in 9 people are affected by it. Early detection can significantly lower this risk. Despite significant advancements in machine learning for identifying diabetic cases, results can still be influenced by the imbalanced nature of the data.", "method": "copula-based data augmentation, which preserves the dependency structure when generating data for the minority class and integrates it with machine learning (ML) techniques. We selected the Pima Indian dataset and generated data using A2 copula, then applied four machine learning algorithms: logistic regression, random forest, gradient boosting, and extreme gradient boosting.", "result": "XGBoost combined with A2 copula oversampling achieved the best performance improving accuracy by 4.6%, precision by 15.6%, recall by 20.4%, F1-score by 18.2% and AUC by 25.5% compared to the standard SMOTE method.", "conclusion": "XGBoost combined with A2 copula oversampling achieved the best performance improving accuracy by 4.6%, precision by 15.6%, recall by 20.4%, F1-score by 18.2% and AUC by 25.5% compared to the standard SMOTE method. This research represents the first known use of A2 copulas for data augmentation and serves as an alternative to the SMOTE technique, highlighting the efficacy of copulas as a statistical method in machine learning applications."}}
{"id": "2506.17592", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17592", "abs": "https://arxiv.org/abs/2506.17592", "authors": ["Younghun Kim", "Minsuk Jang", "Myung-Joon Kwon", "Wonjun Lee", "Changick Kim"], "title": "SELFI: Selective Fusion of Identity for Generalizable Deepfake Detection", "comment": null, "summary": "Face identity provides a powerful signal for deepfake detection. Prior\nstudies show that even when not explicitly modeled, classifiers often learn\nidentity features implicitly. This has led to conflicting views: some suppress\nidentity cues to reduce bias, while others rely on them as forensic evidence.\nTo reconcile these views, we analyze two hypotheses: (1) whether face identity\nalone is discriminative for detecting deepfakes, and (2) whether such identity\nfeatures generalize poorly across manipulation methods. Our experiments confirm\nthat identity is informative but context-dependent. While some manipulations\npreserve identity-consistent artifacts, others distort identity cues and harm\ngeneralization. We argue that identity features should neither be blindly\nsuppressed nor relied upon, but instead be explicitly modeled and adaptively\ncontrolled based on per-sample relevance. We propose \\textbf{SELFI}\n(\\textbf{SEL}ective \\textbf{F}usion of \\textbf{I}dentity), a generalizable\ndetection framework that dynamically modulates identity usage. SELFI consists\nof: (1) a Forgery-Aware Identity Adapter (FAIA) that extracts identity\nembeddings from a frozen face recognition model and projects them into a\nforgery-relevant space via auxiliary supervision; and (2) an Identity-Aware\nFusion Module (IAFM) that selectively integrates identity and visual features\nusing a relevance-guided fusion mechanism. Experiments on four benchmarks show\nthat SELFI improves cross-manipulation generalization, outperforming prior\nmethods by an average of 3.1\\% AUC. On the challenging DFDC dataset, SELFI\nexceeds the previous best by 6\\%. Code will be released upon paper acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSELFI\u7684deepfake\u68c0\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u52a8\u6001\u5730\u8c03\u6574\u8eab\u4efd\u4fe1\u606f\u7684\u4f7f\u7528\uff0c\u4ece\u800c\u63d0\u9ad8\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709deepfake\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u8eab\u4efd\u4fe1\u606f\u7684\u5229\u7528\u5b58\u5728\u4e89\u8bae\uff0c\u6709\u7684\u6291\u5236\u8eab\u4efd\u7ebf\u7d22\u4ee5\u51cf\u5c11\u504f\u5dee\uff0c\u6709\u7684\u4f9d\u8d56\u8eab\u4efd\u4fe1\u606f\u4f5c\u4e3a\u53d6\u8bc1\u4f9d\u636e\u3002\u4e3a\u4e86\u534f\u8c03\u8fd9\u4e9b\u89c2\u70b9\uff0c\u7814\u7a76\u5206\u6790\u4e86\u4e24\u4e2a\u5047\u8bbe\uff1a(1) \u4ec5\u9760\u4eba\u8138\u8eab\u4efd\u662f\u5426\u8db3\u4ee5\u533a\u5206deepfake\uff1b(2) \u8fd9\u79cd\u8eab\u4efd\u7279\u5f81\u662f\u5426\u5728\u4e0d\u540c\u7684\u64cd\u7eb5\u65b9\u6cd5\u4e2d\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u63d0\u51fa\u4e86SELFI\u6846\u67b6\uff0c\u5305\u542bForgery-Aware Identity Adapter (FAIA) \u548c Identity-Aware Fusion Module (IAFM)\uff0c\u524d\u8005\u63d0\u53d6\u8eab\u4efd\u5d4c\u5165\u5e76\u5c06\u5176\u6295\u5f71\u5230\u4f2a\u9020\u76f8\u5173\u7684\u7a7a\u95f4\uff0c\u540e\u8005\u9009\u62e9\u6027\u5730\u878d\u5408\u8eab\u4efd\u548c\u89c6\u89c9\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8eab\u4efd\u4fe1\u606f\u662f\u6709\u7528\u7684\uff0c\u4f46\u4f9d\u8d56\u4e8e\u4e0a\u4e0b\u6587\u3002SELFI\u6846\u67b6\u5728\u8de8\u64cd\u7eb5\u7684\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u6709\u6240\u63d0\u9ad8\uff0c\u5e76\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u5e73\u5747\u9ad83.1% AUC\u7684\u6027\u80fd\u63d0\u5347\u3002\u5728\u5177\u6709\u6311\u6218\u6027\u7684DFDC\u6570\u636e\u96c6\u4e0a\uff0cSELFI\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684\u6700\u4f73\u7ed3\u679c6%\u3002", "conclusion": "\u63d0\u51faSELFI\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5730\u878d\u5408\u8eab\u4efd\u4fe1\u606f\u548c\u89c6\u89c9\u7279\u5f81\u6765\u63d0\u9ad8deepfake\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u5e73\u5747\u9ad83.1% AUC\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5728DFDC\u6570\u636e\u96c6\u4e0a\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684\u6700\u4f73\u7ed3\u679c6%\u3002"}}
{"id": "2506.17671", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17671", "abs": "https://arxiv.org/abs/2506.17671", "authors": ["Fabien Furfaro"], "title": "TPTT: Transforming Pretrained Transformer into Titans", "comment": "6 pages, 1 figure", "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress in natural language processing, but their computational and memory\ndemands remain a significant challenge, particularly for long-context\ninference. We introduce TPTT (Transforming Pretrained Transformer into Titans),\na novel framework for enhancing pretrained Transformer models with efficient\nlinearized attention mechanisms and advanced memory management. TPTT employs\ntechniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA).\nIt is fully compatible with the Hugging Face Transformers library, enabling\nseamless adaptation of any causal LLM through parameter-efficient fine-tuning\n(LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU\nbenchmark with models of approximately 1 billion parameters, observing\nsubstantial improvements in both efficiency and accuracy. For instance,\nTitans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its\nbaseline. Statistical analyses and comparisons with recent state-of-the-art\nmethods confirm the practical scalability and robustness of TPTT. Code is\navailable at https://github.com/fabienfrfr/tptt . Python package at\nhttps://pypi.org/project/tptt/ .", "AI": {"tldr": "TPTT is a novel framework for enhancing pretrained Transformer models with efficient linearized attention mechanisms and advanced memory management, achieving substantial improvements in both efficiency and accuracy.", "motivation": "Computational and memory demands of large language models (LLMs) remain a significant challenge, particularly for long-context inference.", "method": "TPTT employs techniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA).", "result": "Titans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its baseline.", "conclusion": "The effectiveness of TPTT is confirmed on the MMLU benchmark, with Titans-Llama-3.2-1B achieving a 20% increase in Exact Match (EM) over its baseline. Statistical analyses and comparisons with recent state-of-the-art methods confirm the practical scalability and robustness of TPTT."}}
{"id": "2506.18056", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18056", "abs": "https://arxiv.org/abs/2506.18056", "authors": ["Paolo Baldi", "Fabio Aurelio D'Asaro", "Abeer Dyoub", "Francesca Alessandra Lisi"], "title": "Weighted Assumption Based Argumentation to reason about ethical principles and actions", "comment": null, "summary": "We augment Assumption Based Argumentation (ABA for short) with weighted\nargumentation. In a nutshell, we assign weights to arguments and then derive\nthe weight of attacks between ABA arguments. We illustrate our proposal through\nrunning examples in the field of ethical reasoning, and present an\nimplementation based on Answer Set Programming.", "AI": {"tldr": "ABA is augmented with weighted argumentation. Weights are assigned to arguments, and the attack weights between arguments are derived. The proposal is illustrated through examples in ethical reasoning and implemented using Answer Set Programming.", "motivation": "To enhance Assumption Based Argumentation (ABA) with weighted argumentation.", "method": "We augment Assumption Based Argumentation with weighted argumentation, assigning weights to arguments and deriving the weight of attacks between ABA arguments.", "result": "Proposal illustrated through running examples in the field of ethical reasoning.", "conclusion": "We present an implementation based on Answer Set Programming."}}
{"id": "2506.17333", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.mtrl-sci", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2506.17333", "abs": "https://arxiv.org/abs/2506.17333", "authors": ["Jaime A. Berkovich", "Noah S. David", "Markus J. Buehler"], "title": "AutomataGPT: Forecasting and Ruleset Inference for Two-Dimensional Cellular Automata", "comment": null, "summary": "Cellular automata (CA) provide a minimal formalism for investigating how\nsimple local interactions generate rich spatiotemporal behavior in domains as\ndiverse as traffic flow, ecology, tissue morphogenesis and crystal growth.\nHowever, automatically discovering the local update rules for a given\nphenomenon and using them for quantitative prediction remains challenging. Here\nwe present AutomataGPT, a decoder-only transformer pretrained on around 1\nmillion simulated trajectories that span 100 distinct two-dimensional binary\ndeterministic CA rules on toroidal grids. When evaluated on previously unseen\nrules drawn from the same CA family, AutomataGPT attains 98.5% perfect one-step\nforecasts and reconstructs the governing update rule with up to 96% functional\n(application) accuracy and 82% exact rule-matrix match. These results\ndemonstrate that large-scale pretraining over wider regions of rule space\nyields substantial generalization in both the forward (state forecasting) and\ninverse (rule inference) problems, without hand-crafted priors. By showing that\ntransformer models can faithfully infer and execute CA dynamics from data\nalone, our work lays the groundwork for abstracting real-world dynamical\nphenomena into data-efficient CA surrogates, opening avenues in biology, tissue\nengineering, physics and AI-driven scientific discovery.", "AI": {"tldr": "AutomataGPT, a decoder-only transformer, is pretrained on CA rules and can predict unseen rules with high accuracy, opening avenues for real-world applications.", "motivation": "automatically discovering the local update rules for a given phenomenon and using them for quantitative prediction remains challenging", "method": "a decoder-only transformer pretrained on around 1 million simulated trajectories that span 100 distinct two-dimensional binary deterministic CA rules on toroidal grids", "result": "attains 98.5% perfect one-step forecasts and reconstructs the governing update rule with up to 96% functional (application) accuracy and 82% exact rule-matrix match", "conclusion": "transformer models can faithfully infer and execute CA dynamics from data alone, laying the groundwork for abstracting real-world dynamical phenomena into data-efficient CA surrogates, opening avenues in biology, tissue engineering, physics and AI-driven scientific discovery"}}
{"id": "2506.17596", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17596", "abs": "https://arxiv.org/abs/2506.17596", "authors": ["Wei Huang", "Yinxuan Xu", "Yintao Zhou", "Zhengyu Li", "Jing Huang", "Meng Pang"], "title": "A Multimodal In Vitro Diagnostic Method for Parkinson's Disease Combining Facial Expressions and Behavioral Gait Data", "comment": "8 pages, 4 figures, accepted by CogSci 2025", "summary": "Parkinson's disease (PD), characterized by its incurable nature, rapid\nprogression, and severe disability, poses significant challenges to the lives\nof patients and their families. Given the aging population, the need for early\ndetection of PD is increasing. In vitro diagnosis has garnered attention due to\nits non-invasive nature and low cost. However, existing methods present several\nchallenges: 1) limited training data for facial expression diagnosis; 2)\nspecialized equipment and acquisition environments required for gait diagnosis,\nresulting in poor generalizability; 3) the risk of misdiagnosis or missed\ndiagnosis when relying on a single modality. To address these issues, we\npropose a novel multimodal in vitro diagnostic method for PD, leveraging facial\nexpressions and behavioral gait. Our method employs a lightweight deep learning\nmodel for feature extraction and fusion, aimed at improving diagnostic accuracy\nand facilitating deployment on mobile devices. Furthermore, we have established\nthe largest multimodal PD dataset in collaboration with a hospital and\nconducted extensive experiments to validate the effectiveness of our proposed\nmethod.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u4f53\u5916\u8bca\u65ad\u65b9\u6cd5\uff0c\u5229\u7528\u9762\u90e8\u8868\u60c5\u548c\u6b65\u6001\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5efa\u7acb\u4e86\u6700\u5927\u7684\u591a\u6a21\u6001\u5e15\u91d1\u68ee\u75c5\u6570\u636e\u96c6\uff0c\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\uff1a1\uff09\u9762\u90e8\u8868\u60c5\u8bca\u65ad\u7684\u8bad\u7ec3\u6570\u636e\u6709\u9650\uff1b2\uff09\u6b65\u6001\u8bca\u65ad\u9700\u8981\u4e13\u7528\u8bbe\u5907\u548c\u91c7\u96c6\u73af\u5883\uff0c\u5bfc\u81f4\u6cdb\u5316\u6027\u5dee\uff1b3\uff09\u4f9d\u8d56\u5355\u4e00\u6a21\u6001\u5b58\u5728\u8bef\u8bca\u6216\u6f0f\u8bca\u7684\u98ce\u9669\u3002\u4eba\u53e3\u8001\u9f84\u5316\uff0c\u5bf9\u5e15\u91d1\u68ee\u75c5\u65e9\u671f\u68c0\u6d4b\u7684\u9700\u6c42\u65e5\u76ca\u589e\u52a0\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u548c\u878d\u5408\u3002", "result": "\u5efa\u7acb\u4e86\u6700\u5927\u7684\u591a\u6a21\u6001\u5e15\u91d1\u68ee\u75c5\u6570\u636e\u96c6\uff0c\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u5e76\u4fc3\u8fdb\u4e86\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u4f53\u5916\u8bca\u65ad\u65b9\u6cd5\uff0c\u5229\u7528\u9762\u90e8\u8868\u60c5\u548c\u6b65\u6001\u6765\u8bca\u65ad\u5e15\u91d1\u68ee\u75c5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.17692", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.17692", "abs": "https://arxiv.org/abs/2506.17692", "authors": ["Binquan Ji", "Haibo Luo", "Yifei Lu", "Lei Hei", "Jiaqi Wang", "Tingjing Liao", "Lingyu Wang", "Shichao Wang", "Feiliang Ren"], "title": "Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering", "comment": null, "summary": "Knowledge-intensive multi-hop question answering (QA) tasks, which require\nintegrating evidence from multiple sources to address complex queries, often\nnecessitate multiple rounds of retrieval and iterative generation by large\nlanguage models (LLMs). However, incorporating many documents and extended\ncontexts poses challenges -such as hallucinations and semantic drift-for\nlightweight LLMs with fewer parameters. This work proposes a novel framework\ncalled DEC (Dynamic Enhancement Chain). DEC first decomposes complex questions\ninto logically coherent subquestions to form a hallucination-free reasoning\nchain. It then iteratively refines these subquestions through context-aware\nrewriting to generate effective query formulations. For retrieval, we introduce\na lightweight discriminative keyword extraction module that leverages extracted\nkeywords to achieve targeted, precise document recall with relatively low\ncomputational overhead. Extensive experiments on three multi-hop QA datasets\ndemonstrate that DEC performs on par with or surpasses state-of-the-art\nbenchmarks while significantly reducing token consumption. Notably, our\napproach attains state-of-the-art results on models with 8B parameters,\nshowcasing its effectiveness in various scenarios, particularly in\nresource-constrained environments.", "AI": {"tldr": "DEC is a framework that decomposes complex questions, refines subquestions, and uses keyword extraction for multi-hop QA, achieving state-of-the-art results with reduced token consumption, especially on smaller models.", "motivation": "Knowledge-intensive multi-hop question answering tasks necessitate multiple rounds of retrieval and iterative generation by LLMs, but incorporating many documents and extended contexts poses challenges for lightweight LLMs.", "method": "DEC decomposes complex questions into logically coherent subquestions, iteratively refines these subquestions through context-aware rewriting, and introduces a lightweight discriminative keyword extraction module.", "result": "DEC performs on par with or surpasses state-of-the-art benchmarks while significantly reducing token consumption. DEC attains state-of-the-art results on models with 8B parameters.", "conclusion": "DEC performs on par with or surpasses state-of-the-art benchmarks while significantly reducing token consumption, attaining state-of-the-art results on models with 8B parameters."}}
{"id": "2506.18096", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18096", "abs": "https://arxiv.org/abs/2506.18096", "authors": ["Yuxuan Huang", "Yihang Chen", "Haozheng Zhang", "Kang Li", "Meng Fang", "Linyi Yang", "Xiaoguang Li", "Lifeng Shang", "Songcen Xu", "Jianye Hao", "Kun Shao", "Jun Wang"], "title": "Deep Research Agents: A Systematic Examination And Roadmap", "comment": null, "summary": "The rapid progress of Large Language Models (LLMs) has given rise to a new\ncategory of autonomous AI systems, referred to as Deep Research (DR) agents.\nThese agents are designed to tackle complex, multi-turn informational research\ntasks by leveraging a combination of dynamic reasoning, adaptive long-horizon\nplanning, multi-hop information retrieval, iterative tool use, and the\ngeneration of structured analytical reports. In this paper, we conduct a\ndetailed analysis of the foundational technologies and architectural components\nthat constitute Deep Research agents. We begin by reviewing information\nacquisition strategies, contrasting API-based retrieval methods with\nbrowser-based exploration. We then examine modular tool-use frameworks,\nincluding code execution, multimodal input processing, and the integration of\nModel Context Protocols (MCPs) to support extensibility and ecosystem\ndevelopment. To systematize existing approaches, we propose a taxonomy that\ndifferentiates between static and dynamic workflows, and we classify agent\narchitectures based on planning strategies and agent composition, including\nsingle-agent and multi-agent configurations. We also provide a critical\nevaluation of current benchmarks, highlighting key limitations such as\nrestricted access to external knowledge, sequential execution inefficiencies,\nand misalignment between evaluation metrics and the practical objectives of DR\nagents. Finally, we outline open challenges and promising directions for future\nresearch. A curated and continuously updated repository of DR agent research is\navailable at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}.", "AI": {"tldr": "This paper analyzes the technologies and architectures of Deep Research agents, proposes a taxonomy for classifying them, evaluates current benchmarks, and outlines future research directions.", "motivation": "The rapid progress of Large Language Models (LLMs) has given rise to a new category of autonomous AI systems, referred to as Deep Research (DR) agents, tackle complex, multi-turn informational research tasks", "method": "conduct a detailed analysis of the foundational technologies and architectural components that constitute Deep Research agents. propose a taxonomy that differentiates between static and dynamic workflows, and classify agent architectures based on planning strategies and agent composition, including single-agent and multi-agent configurations", "result": "reviewing information acquisition strategies, contrasting API-based retrieval methods with browser-based exploration. examine modular tool-use frameworks, including code execution, multimodal input processing, and the integration of Model Context Protocols (MCPs) to support extensibility and ecosystem development. provide a critical evaluation of current benchmarks, highlighting key limitations such as restricted access to external knowledge, sequential execution inefficiencies, and misalignment between evaluation metrics and the practical objectives of DR agents", "conclusion": "outlines open challenges and promising directions for future research"}}
{"id": "2506.17342", "categories": ["cs.LG", "cs.AI", "cs.MM", "cs.NI"], "pdf": "https://arxiv.org/pdf/2506.17342", "abs": "https://arxiv.org/abs/2506.17342", "authors": ["Zijian Long", "Haopeng Wang", "Haiwei Dong", "Abdulmotaleb El Saddik"], "title": "Adaptive Social Metaverse Streaming based on Federated Multi-Agent Deep Reinforcement Learning", "comment": "Accepted by IEEE Transactions on Computational Social Systems", "summary": "The social metaverse is a growing digital ecosystem that blends virtual and\nphysical worlds. It allows users to interact socially, work, shop, and enjoy\nentertainment. However, privacy remains a major challenge, as immersive\ninteractions require continuous collection of biometric and behavioral data. At\nthe same time, ensuring high-quality, low-latency streaming is difficult due to\nthe demands of real-time interaction, immersive rendering, and bandwidth\noptimization. To address these issues, we propose ASMS (Adaptive Social\nMetaverse Streaming), a novel streaming system based on Federated Multi-Agent\nProximal Policy Optimization (F-MAPPO). ASMS leverages F-MAPPO, which\nintegrates federated learning (FL) and deep reinforcement learning (DRL) to\ndynamically adjust streaming bit rates while preserving user privacy.\nExperimental results show that ASMS improves user experience by at least 14%\ncompared to existing streaming methods across various network conditions.\nTherefore, ASMS enhances the social metaverse experience by providing seamless\nand immersive streaming, even in dynamic and resource-constrained networks,\nwhile ensuring that sensitive user data remains on local devices.", "AI": {"tldr": "propose ASMS (Adaptive Social Metaverse Streaming) to dynamically adjust streaming bit rates while preserving user privacy and improves user experience by at least 14%", "motivation": "privacy remains a major challenge, as immersive interactions require continuous collection of biometric and behavioral data. At the same time, ensuring high-quality, low-latency streaming is difficult due to the demands of real-time interaction, immersive rendering, and bandwidth optimization.", "method": "a novel streaming system based on Federated Multi-Agent Proximal Policy Optimization (F-MAPPO). ASMS leverages F-MAPPO, which integrates federated learning (FL) and deep reinforcement learning (DRL) to dynamically adjust streaming bit rates while preserving user privacy.", "result": "ASMS improves user experience by at least 14% compared to existing streaming methods across various network conditions.", "conclusion": "ASMS enhances the social metaverse experience by providing seamless and immersive streaming, even in dynamic and resource-constrained networks, while ensuring that sensitive user data remains on local devices."}}
{"id": "2506.17597", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17597", "abs": "https://arxiv.org/abs/2506.17597", "authors": ["Pengyu Kan", "Craig Jones", "Kenichi Oishi"], "title": "OpenMAP-BrainAge: Generalizable and Interpretable Brain Age Predictor", "comment": null, "summary": "Purpose: To develop an age prediction model which is interpretable and robust\nto demographic and technological variances in brain MRI scans. Materials and\nMethods: We propose a transformer-based architecture that leverages\nself-supervised pre-training on large-scale datasets. Our model processes\npseudo-3D T1-weighted MRI scans from three anatomical views and incorporates\nbrain volumetric information. By introducing a stem architecture, we reduce the\nconventional quadratic complexity of transformer models to linear complexity,\nenabling scalability for high-dimensional MRI data. We trained our model on\nADNI2 $\\&$ 3 (N=1348) and OASIS3 (N=716) datasets (age range: 42 - 95) from the\nNorth America, with an 8:1:1 split for train, validation and test. Then, we\nvalidated it on the AIBL dataset (N=768, age range: 60 - 92) from Australia.\nResults: We achieved an MAE of 3.65 years on ADNI2 $\\&$ 3 and OASIS3 test set\nand a high generalizability of MAE of 3.54 years on AIBL. There was a notable\nincrease in brain age gap (BAG) across cognitive groups, with mean of 0.15\nyears (95% CI: [-0.22, 0.51]) in CN, 2.55 years ([2.40, 2.70]) in MCI, 6.12\nyears ([5.82, 6.43]) in AD. Additionally, significant negative correlation\nbetween BAG and cognitive scores was observed, with correlation coefficient of\n-0.185 (p < 0.001) for MoCA and -0.231 (p < 0.001) for MMSE. Gradient-based\nfeature attribution highlighted ventricles and white matter structures as key\nregions influenced by brain aging. Conclusion: Our model effectively fused\ninformation from different views and volumetric information to achieve\nstate-of-the-art brain age prediction accuracy, improved generalizability and\ninterpretability with association to neurodegenerative disorders.", "AI": {"tldr": "Developed a robust and interpretable age prediction model using a transformer-based architecture on brain MRI scans, achieving state-of-the-art accuracy and generalizability, with associations to neurodegenerative disorders.", "motivation": "To develop an age prediction model which is interpretable and robust to demographic and technological variances in brain MRI scans.", "method": "We propose a transformer-based architecture that leverages self-supervised pre-training on large-scale datasets. Our model processes pseudo-3D T1-weighted MRI scans from three anatomical views and incorporates brain volumetric information. By introducing a stem architecture, we reduce the conventional quadratic complexity of transformer models to linear complexity, enabling scalability for high-dimensional MRI data. We trained our model on ADNI2 $\\& 3 (N=1348) and OASIS3 (N=716) datasets (age range: 42 - 95) from the North America, with an 8:1:1 split for train, validation and test. Then, we validated it on the AIBL dataset (N=768, age range: 60 - 92) from Australia.", "result": "We achieved an MAE of 3.65 years on ADNI2 $\\& 3 and OASIS3 test set and a high generalizability of MAE of 3.54 years on AIBL. There was a notable increase in brain age gap (BAG) across cognitive groups, with mean of 0.15 years (95% CI: [-0.22, 0.51]) in CN, 2.55 years ([2.40, 2.70]) in MCI, 6.12 years ([5.82, 6.43]) in AD. Additionally, significant negative correlation between BAG and cognitive scores was observed, with correlation coefficient of -0.185 (p < 0.001) for MoCA and -0.231 (p < 0.001) for MMSE. Gradient-based feature attribution highlighted ventricles and white matter structures as key regions influenced by brain aging.", "conclusion": "The model effectively fused information from different views and volumetric information to achieve state-of-the-art brain age prediction accuracy, improved generalizability and interpretability with association to neurodegenerative disorders."}}
{"id": "2506.17693", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17693", "abs": "https://arxiv.org/abs/2506.17693", "authors": ["Yuzhe Ding", "Kang He", "Bobo Li", "Li Zheng", "Haijun He", "Fei Li", "Chong Teng", "Donghong Ji"], "title": "Zero-Shot Conversational Stance Detection: Dataset and Approaches", "comment": "ACL 2025 (Findings)", "summary": "Stance detection, which aims to identify public opinion towards specific\ntargets using social media data, is an important yet challenging task. With the\nincreasing number of online debates among social media users, conversational\nstance detection has become a crucial research area. However, existing\nconversational stance detection datasets are restricted to a limited set of\nspecific targets, which constrains the effectiveness of stance detection models\nwhen encountering a large number of unseen targets in real-world applications.\nTo bridge this gap, we manually curate a large-scale, high-quality zero-shot\nconversational stance detection dataset, named ZS-CSD, comprising 280 targets\nacross two distinct target types. Leveraging the ZS-CSD dataset, we propose\nSITPCL, a speaker interaction and target-aware prototypical contrastive\nlearning model, and establish the benchmark performance in the zero-shot\nsetting. Experimental results demonstrate that our proposed SITPCL model\nachieves state-of-the-art performance in zero-shot conversational stance\ndetection. Notably, the SITPCL model attains only an F1-macro score of 43.81%,\nhighlighting the persistent challenges in zero-shot conversational stance\ndetection.", "AI": {"tldr": "A new dataset (ZS-CSD) and model (SITPCL) are introduced for zero-shot conversational stance detection. The model achieves state-of-the-art results but the task remains challenging.", "motivation": "Existing conversational stance detection datasets are restricted to a limited set of specific targets, which constrains the effectiveness of stance detection models when encountering a large number of unseen targets in real-world applications.", "method": "We propose SITPCL, a speaker interaction and target-aware prototypical contrastive learning model.", "result": "The proposed SITPCL model achieves state-of-the-art performance in zero-shot conversational stance detection.", "conclusion": "The SITPCL model achieves state-of-the-art performance in zero-shot conversational stance detection, but the F1-macro score of 43.81% highlights the persistent challenges in this area."}}
{"id": "2506.18126", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18126", "abs": "https://arxiv.org/abs/2506.18126", "authors": ["Xiang Yuming", "Li Sizhao", "Li Rongpeng", "Zhao Zhifeng", "Zhang Honggang"], "title": "Decentralized Consensus Inference-based Hierarchical Reinforcement Learning for Multi-Constrained UAV Pursuit-Evasion Game", "comment": null, "summary": "Multiple quadrotor unmanned aerial vehicle (UAV) systems have garnered\nwidespread research interest and fostered tremendous interesting applications,\nespecially in multi-constrained pursuit-evasion games (MC-PEG). The Cooperative\nEvasion and Formation Coverage (CEFC) task, where the UAV swarm aims to\nmaximize formation coverage across multiple target zones while collaboratively\nevading predators, belongs to one of the most challenging issues in MC-PEG,\nespecially under communication-limited constraints. This multifaceted problem,\nwhich intertwines responses to obstacles, adversaries, target zones, and\nformation dynamics, brings up significant high-dimensional complications in\nlocating a solution. In this paper, we propose a novel two-level framework\n(i.e., Consensus Inference-based Hierarchical Reinforcement Learning (CI-HRL)),\nwhich delegates target localization to a high-level policy, while adopting a\nlow-level policy to manage obstacle avoidance, navigation, and formation.\nSpecifically, in the high-level policy, we develop a novel multi-agent\nreinforcement learning module, Consensus-oriented Multi-Agent Communication\n(ConsMAC), to enable agents to perceive global information and establish\nconsensus from local states by effectively aggregating neighbor messages.\nMeanwhile, we leverage an Alternative Training-based Multi-agent proximal\npolicy optimization (AT-M) and policy distillation to accomplish the low-level\ncontrol. The experimental results, including the high-fidelity\nsoftware-in-the-loop (SITL) simulations, validate that CI-HRL provides a\nsuperior solution with enhanced swarm's collaborative evasion and task\ncompletion capabilities.", "AI": {"tldr": "This paper proposes CI-HRL, a two-level reinforcement learning framework, for the Cooperative Evasion and Formation Coverage (CEFC) task. It uses Consensus-oriented Multi-Agent Communication (ConsMAC) for high-level policy and Alternative Training-based Multi-agent proximal policy optimization (AT-M) and policy distillation for low-level control. The results show CI-HRL enhances swarm's collaborative evasion and task completion capabilities.", "motivation": "The Cooperative Evasion and Formation Coverage (CEFC) task, where the UAV swarm aims to maximize formation coverage across multiple target zones while collaboratively evading predators, belongs to one of the most challenging issues in MC-PEG, especially under communication-limited constraints. This multifaceted problem, which intertwines responses to obstacles, adversaries, target zones, and formation dynamics, brings up significant high-dimensional complications in locating a solution.", "method": "a novel two-level framework (i.e., Consensus Inference-based Hierarchical Reinforcement Learning (CI-HRL))", "result": "experimental results, including the high-fidelity software-in-the-loop (SITL) simulations, validate that CI-HRL provides a superior solution with enhanced swarm's collaborative evasion and task completion capabilities.", "conclusion": "CI-HRL provides a superior solution with enhanced swarm's collaborative evasion and task completion capabilities."}}
{"id": "2506.17344", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17344", "abs": "https://arxiv.org/abs/2506.17344", "authors": ["Tao Wang", "Hewei Tang"], "title": "FFINO: Factorized Fourier Improved Neural Operator for Modeling Multiphase Flow in Underground Hydrogen Storage", "comment": null, "summary": "Underground hydrogen storage (UHS) is a promising energy storage option for\nthe current energy transition to a low-carbon economy. Fast modeling of\nhydrogen plume migration and pressure field evolution is crucial for UHS field\nmanagement. In this study, we propose a new neural operator architecture,\nFFINO, as a fast surrogate model for multiphase flow problems in UHS. We\nparameterize experimental relative permeability curves reported in the\nliterature and include them as key uncertainty parameters in the FFINO model.\nWe also compare the FFINO model with the state-of-the-art FMIONet model through\na comprehensive combination of metrics. Our new FFINO model has 38.1% fewer\ntrainable parameters, 17.6% less training time, and 12% less GPU memory cost\ncompared to FMIONet. The FFINO model also achieves a 9.8% accuracy improvement\nin predicting hydrogen plume in focused areas, and 18% higher RMSE in\npredicting pressure buildup. The inference time of the trained FFINO model is\n7850 times faster than a numerical simulator, which makes it a competent\nsubstitute for numerical simulations of UHS problems with superior time\nefficiency.", "AI": {"tldr": "FFINO, a new neural operator architecture, is a fast and efficient surrogate model for underground hydrogen storage multiphase flow problems, outperforming FMIONet and numerical simulations.", "motivation": "Fast modeling of hydrogen plume migration and pressure field evolution is crucial for UHS field management in the energy transition to a low-carbon economy.", "method": "A new neural operator architecture, FFINO, is proposed as a fast surrogate model for multiphase flow problems in UHS. The model incorporates experimental relative permeability curves as key uncertainty parameters.", "result": "The FFINO model shows improvements over the FMIONet model with 38.1% fewer trainable parameters, 17.6% less training time, 12% less GPU memory cost, 9.8% accuracy improvement in predicting hydrogen plume, and 18% higher RMSE in predicting pressure buildup.", "conclusion": "The trained FFINO model demonstrates superior time efficiency, being 7850 times faster than numerical simulations, making it a competent substitute for UHS problem simulations."}}
{"id": "2506.17608", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17608", "abs": "https://arxiv.org/abs/2506.17608", "authors": ["Nikitha SR", "Aradhya Neeraj Mathur", "Tarun Ram Menta", "Rishabh Jain", "Mausoom Sarkar"], "title": "HIRE: Lightweight High-Resolution Image Feature Enrichment for Multimodal LLMs", "comment": "Accepted in CVPR 2025 Workshop on What's Next in Multimodal\n  Foundational Models", "summary": "The integration of high-resolution image features in modern multimodal large\nlanguage models has demonstrated significant improvements in fine-grained\nvisual understanding tasks, achieving high performance across multiple\nbenchmarks. Since these features are obtained from large image encoders like\nViT, they come with a significant increase in computational costs due to\nmultiple calls to these encoders. In this work, we first develop an intuition\nfor feature upsampling as a natural extension of high-resolution feature\ngeneration. Through extensive experiments and ablations, we demonstrate how a\nshallow feature enricher can achieve competitive results with tremendous\nreductions in training and inference time as well as computational cost, with\nupto 1.5x saving in FLOPs.", "AI": {"tldr": "Feature upsampling can reduce computational costs while maintaining competitive results in multimodal large language models.", "motivation": "Integration of high-resolution image features in modern multimodal large language models has demonstrated significant improvements but comes with a significant increase in computational costs.", "method": "develop an intuition for feature upsampling as a natural extension of high-resolution feature generation", "result": "achieve competitive results with tremendous reductions in training and inference time as well as computational cost, with upto 1.5x saving in FLOPs.", "conclusion": "A shallow feature enricher can achieve competitive results with tremendous reductions in training and inference time as well as computational cost, with upto 1.5x saving in FLOPs."}}
{"id": "2506.17700", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17700", "abs": "https://arxiv.org/abs/2506.17700", "authors": ["Summra Saleem", "Muhammad Nabeel Asim", "Shaista Zulfiqar", "Andreas Dengel"], "title": "The Evolution of Natural Language Processing: How Prompt Optimization and Language Models are Shaping the Future", "comment": null, "summary": "Large Language Models (LLMs) have revolutionized the field of Natural\nLanguage Processing (NLP) by automating traditional labor-intensive tasks and\nconsequently accelerated the development of computer-aided applications. As\nresearchers continue to advance this field with the introduction of novel\nlanguage models and more efficient training/finetuning methodologies, the idea\nof prompt engineering and subsequent optimization strategies with LLMs has\nemerged as a particularly impactful trend to yield a substantial performance\nboost across diverse NLP tasks. To best of our knowledge numerous review\narticles have explored prompt engineering, however, a critical gap exists in\ncomprehensive analyses of prompt optimization strategies. To bridge this gap\nthis paper provides unique and comprehensive insights about the potential of\ndiverse prompt optimization strategies. It analyzes their underlying working\nparadigms and based on these principles, categorizes them into 11 distinct\nclasses. Moreover, the paper provides details about various NLP tasks where\nthese prompt optimization strategies have been employed, along with details of\ndifferent LLMs and benchmark datasets used for evaluation. This comprehensive\ncompilation lays a robust foundation for future comparative studies and enables\nrigorous assessment of prompt optimization and LLM-based predictive pipelines\nunder consistent experimental settings: a critical need in the current\nlandscape. Ultimately, this research will centralize diverse strategic\nknowledge to facilitate the adaptation of existing prompt optimization\nstrategies for development of innovative predictors across unexplored tasks.", "AI": {"tldr": "This paper provides unique and comprehensive insights about the potential of diverse prompt optimization strategies, analyzes their underlying working paradigms, and categorizes them into 11 distinct classes. Moreover, the paper provides details about various NLP tasks where these prompt optimization strategies have been employed, along with details of different LLMs and benchmark datasets used for evaluation.", "motivation": "A critical gap exists in comprehensive analyses of prompt optimization strategies.", "method": "This paper analyzes the underlying working paradigms of prompt optimization strategies and categorizes them into 11 distinct classes.", "result": "The paper provides details about various NLP tasks where prompt optimization strategies have been employed, along with details of different LLMs and benchmark datasets used for evaluation. This comprehensive compilation lays a robust foundation for future comparative studies and enables rigorous assessment of prompt optimization and LLM-based predictive pipelines under consistent experimental settings.", "conclusion": "This research centralizes diverse strategic knowledge to facilitate the adaptation of existing prompt optimization strategies for development of innovative predictors across unexplored tasks."}}
{"id": "2506.18135", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.18135", "abs": "https://arxiv.org/abs/2506.18135", "authors": ["Zijun Chen", "Zhanpeng Zhou", "Bo Zhang", "Weinan Zhang", "Xi Sun", "Junchi Yan"], "title": "SE-Merging: A Self-Enhanced Approach for Dynamic Model Merging", "comment": "preprint, accepted at IJCNN2025", "summary": "Model merging has gained increasing attention due to its intriguing property:\ninterpolating the parameters of different task-specific fine-tuned models leads\nto multi-task abilities. However, despite its empirical success, the underlying\nmechanisms of model merging remain poorly understood. In this work, we delve\ninto the mechanism behind model merging from a representation perspective. Our\nanalysis reveals that model merging achieves multi-task abilities through two\nkey capabilities: i) distinguishing samples from different tasks, and ii)\nadapting to the corresponding expert model for each sample. These two\ncapabilities allow the merged model to retain task-specific expertise, enabling\nefficient multi-task adaptation. Building on these insights, we propose\n\\texttt{SE-Merging}, a self-enhanced model merging framework that leverages\nthese two characteristics to dynamically identify the corresponding task for\neach sample and then adaptively rescales the merging coefficients to further\nenhance task-specific expertise in the merged model. Notably,\n\\texttt{SE-Merging} achieves dynamic model merging without additional training.\nExtensive experiments demonstrate that \\texttt{SE-Merging} achieves significant\nperformance improvements while remaining compatible with existing model merging\ntechniques.", "AI": {"tldr": "This paper studies the mechanism behind model merging and proposes SE-Merging, a self-enhanced model merging framework that dynamically identifies the corresponding task for each sample and then adaptively rescales the merging coefficients to further enhance task-specific expertise in the merged model. SE-Merging achieves significant performance improvements without additional training.", "motivation": "Model merging has gained increasing attention due to its intriguing property: interpolating the parameters of different task-specific fine-tuned models leads to multi-task abilities. However, despite its empirical success, the underlying mechanisms of model merging remain poorly understood.", "method": "The paper analyzes model merging from a representation perspective and proposes SE-Merging, a self-enhanced model merging framework that leverages two key characteristics: distinguishing samples from different tasks and adaptively rescaling merging coefficients.", "result": "SE-Merging achieves significant performance improvements while remaining compatible with existing model merging techniques.", "conclusion": "The paper proposes SE-Merging, a self-enhanced model merging framework, and demonstrates its significant performance improvements while remaining compatible with existing model merging techniques."}}
{"id": "2506.17368", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.17368", "abs": "https://arxiv.org/abs/2506.17368", "authors": ["Zhenglin Lai", "Mengyao Liao", "Dong Xu", "Zebin Zhao", "Zhihang Yuan", "Chao Fan", "Jianqiang Li", "Bingzhe Wu"], "title": "SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification", "comment": "9 pages, 7 figures", "summary": "Large language models based on Mixture-of-Experts have achieved substantial\ngains in efficiency and scalability, yet their architectural uniqueness\nintroduces underexplored safety alignment challenges. Existing safety alignment\nstrategies, predominantly designed for dense models, are ill-suited to address\nMoE-specific vulnerabilities. In this work, we formalize and systematically\nstudy MoE model's positional vulnerability - the phenomenon where\nsafety-aligned behaviors rely on specific expert modules, revealing critical\nrisks inherent to MoE architectures. To this end, we present SAFEx, an\nanalytical framework that robustly identifies, characterizes, and validates the\nsafety-critical experts using a novel Stability-based Expert Selection (SES)\nalgorithm. Notably, our approach enables the explicit decomposition of\nsafety-critical experts into distinct functional groups, including those\nresponsible for harmful content detection and those controlling safe response\ngeneration. Extensive experiments on mainstream MoE models, such as the\nrecently released Qwen3-MoE, demonstrated that their intrinsic safety\nmechanisms heavily rely on a small subset of positional experts. Disabling\nthese experts significantly compromised the models' ability to refuse harmful\nrequests. For Qwen3-MoE with 6144 experts (in the FNN layer), we find that\ndisabling as few as 12 identified safety-critical experts can cause the refusal\nrate to drop by 22%, demonstrating the disproportionate impact of a small set\nof experts on overall model safety.", "AI": {"tldr": "MoE models have safety vulnerabilities because safety-aligned behaviors rely on specific expert modules. Disabling these experts significantly compromised the models' ability to refuse harmful requests.", "motivation": "Existing safety alignment strategies, predominantly designed for dense models, are ill-suited to address MoE-specific vulnerabilities. In this work, we formalize and systematically study MoE model's positional vulnerability - the phenomenon where safety-aligned behaviors rely on specific expert modules, revealing critical risks inherent to MoE architectures.", "method": "present SAFEx, an analytical framework that robustly identifies, characterizes, and validates the safety-critical experts using a novel Stability-based Expert Selection (SES) algorithm. Notably, our approach enables the explicit decomposition of safety-critical experts into distinct functional groups, including those responsible for harmful content detection and those controlling safe response generation.", "result": "demonstrated that their intrinsic safety mechanisms heavily rely on a small subset of positional experts", "conclusion": "Disabling a small set of identified safety-critical experts can significantly compromise the models' ability to refuse harmful requests. For Qwen3-MoE with 6144 experts (in the FNN layer), disabling as few as 12 identified safety-critical experts can cause the refusal rate to drop by 22%, demonstrating the disproportionate impact of a small set of experts on overall model safety."}}
{"id": "2506.17612", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17612", "abs": "https://arxiv.org/abs/2506.17612", "authors": ["Yunlong Lin", "Zixu Lin", "Kunjie Lin", "Jinbin Bai", "Panwang Pan", "Chenxin Li", "Haoyu Chen", "Zhongdao Wang", "Xinghao Ding", "Wenbo Li", "Shuicheng Yan"], "title": "JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo Retouching Agent", "comment": "40 pages, 26 figures", "summary": "Photo retouching has become integral to contemporary visual storytelling,\nenabling users to capture aesthetics and express creativity. While professional\ntools such as Adobe Lightroom offer powerful capabilities, they demand\nsubstantial expertise and manual effort. In contrast, existing AI-based\nsolutions provide automation but often suffer from limited adjustability and\npoor generalization, failing to meet diverse and personalized editing needs. To\nbridge this gap, we introduce JarvisArt, a multi-modal large language model\n(MLLM)-driven agent that understands user intent, mimics the reasoning process\nof professional artists, and intelligently coordinates over 200 retouching\ntools within Lightroom. JarvisArt undergoes a two-stage training process: an\ninitial Chain-of-Thought supervised fine-tuning to establish basic reasoning\nand tool-use skills, followed by Group Relative Policy Optimization for\nRetouching (GRPO-R) to further enhance its decision-making and tool\nproficiency. We also propose the Agent-to-Lightroom Protocol to facilitate\nseamless integration with Lightroom. To evaluate performance, we develop\nMMArt-Bench, a novel benchmark constructed from real-world user edits.\nJarvisArt demonstrates user-friendly interaction, superior generalization, and\nfine-grained control over both global and local adjustments, paving a new\navenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a\n60% improvement in average pixel-level metrics on MMArt-Bench for content\nfidelity, while maintaining comparable instruction-following capabilities.\nProject Page: https://jarvisart.vercel.app/.", "AI": {"tldr": "JarvisArt, a multi-modal large language model (MLLM)-driven agent, outperforms GPT-4o in intelligent photo retouching with better content fidelity and comparable instruction-following capabilities.", "motivation": "Existing AI-based solutions provide automation but often suffer from limited adjustability and poor generalization, failing to meet diverse and personalized editing needs. To bridge this gap, we introduce JarvisArt", "method": "a multi-modal large language model (MLLM)-driven agent that understands user intent, mimics the reasoning process of professional artists, and intelligently coordinates over 200 retouching tools within Lightroom. JarvisArt undergoes a two-stage training process: an initial Chain-of-Thought supervised fine-tuning to establish basic reasoning and tool-use skills, followed by Group Relative Policy Optimization for Retouching (GRPO-R) to further enhance its decision-making and tool proficiency. We also propose the Agent-to-Lightroom Protocol to facilitate seamless integration with Lightroom. To evaluate performance, we develop MMArt-Bench, a novel benchmark constructed from real-world user edits.", "result": "JarvisArt demonstrates user-friendly interaction, superior generalization, and fine-grained control over both global and local adjustments", "conclusion": "JarvisArt outperforms GPT-4o with a 60% improvement in average pixel-level metrics on MMArt-Bench for content fidelity, while maintaining comparable instruction-following capabilities, paving a new avenue for intelligent photo retouching."}}
{"id": "2506.17708", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17708", "abs": "https://arxiv.org/abs/2506.17708", "authors": ["MingZe Tang"], "title": "Aged to Perfection: Machine-Learning Maps of Age in Conversational English", "comment": "6 pages, 11 figures", "summary": "The study uses the British National Corpus 2014, a large sample of\ncontemporary spoken British English, to investigate language patterns across\ndifferent age groups. Our research attempts to explore how language patterns\nvary between different age groups, exploring the connection between speaker\ndemographics and linguistic factors such as utterance duration, lexical\ndiversity, and word choice. By merging computational language analysis and\nmachine learning methodologies, we attempt to uncover distinctive linguistic\nmarkers characteristic of multiple generations and create prediction models\nthat can consistently estimate the speaker's age group from various aspects.\nThis work contributes to our knowledge of sociolinguistic diversity throughout\nthe life of modern British speech.", "AI": {"tldr": "investigate language patterns across different age groups", "motivation": "Our research attempts to explore how language patterns vary between different age groups, exploring the connection between speaker demographics and linguistic factors such as utterance duration, lexical diversity, and word choice.", "method": "By merging computational language analysis and machine learning methodologies", "result": "uncover distinctive linguistic markers characteristic of multiple generations and create prediction models that can consistently estimate the speaker's age group from various aspects.", "conclusion": "This work contributes to our knowledge of sociolinguistic diversity throughout the life of modern British speech."}}
{"id": "2506.18149", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18149", "abs": "https://arxiv.org/abs/2506.18149", "authors": ["Fumian Chen", "Sotheara Veng", "Joshua Wilson", "Xiaoming Li", "Hui Fang"], "title": "CoachGPT: A Scaffolding-based Academic Writing Assistant", "comment": "SIGIR 2025 DEMO Pre-print", "summary": "Academic writing skills are crucial for students' success, but can feel\noverwhelming without proper guidance and practice, particularly when writing in\na second language. Traditionally, students ask instructors or search\ndictionaries, which are not universally accessible. Early writing assistants\nemerged as rule-based systems that focused on detecting misspellings,\nsubject-verb disagreements, and basic punctuation errors; however, they are\ninaccurate and lack contextual understanding. Machine learning-based assistants\ndemonstrate a strong ability for language understanding but are expensive to\ntrain. Large language models (LLMs) have shown remarkable capabilities in\ngenerating responses in natural languages based on given prompts. Still, they\nhave a fundamental limitation in education: they generate essays without\nteaching, which can have detrimental effects on learning when misused. To\naddress this limitation, we develop CoachGPT, which leverages large language\nmodels (LLMs) to assist individuals with limited educational resources and\nthose who prefer self-paced learning in academic writing. CoachGPT is an AI\nagent-based web application that (1) takes instructions from experienced\neducators, (2) converts instructions into sub-tasks, and (3) provides real-time\nfeedback and suggestions using large language models. This unique scaffolding\nstructure makes CoachGPT unique among existing writing assistants. Compared to\nexisting writing assistants, CoachGPT provides a more immersive writing\nexperience with personalized feedback and guidance. Our user studies prove the\nusefulness of CoachGPT and the potential of large language models for academic\nwriting.", "AI": {"tldr": "Developed CoachGPT, an AI agent-based web application using LLMs, to assist individuals with academic writing by providing personalized feedback and guidance.", "motivation": "Academic writing skills are crucial for students' success, but can feel overwhelming without proper guidance and practice, particularly when writing in a second language. Traditionally, students ask instructors or search dictionaries, which are not universally accessible. Early writing assistants emerged as rule-based systems that focused on detecting misspellings, subject-verb disagreements, and basic punctuation errors; however, they are inaccurate and lack contextual understanding. Machine learning-based assistants demonstrate a strong ability for language understanding but are expensive to train. Large language models (LLMs) have shown remarkable capabilities in generating responses in natural languages based on given prompts. Still, they have a fundamental limitation in education: they generate essays without teaching, which can have detrimental effects on learning when misused.", "method": "we develop CoachGPT, which leverages large language models (LLMs) to assist individuals with limited educational resources and those who prefer self-paced learning in academic writing. CoachGPT is an AI agent-based web application that (1) takes instructions from experienced educators, (2) converts instructions into sub-tasks, and (3) provides real-time feedback and suggestions using large language models.", "result": "Compared to existing writing assistants, CoachGPT provides a more immersive writing experience with personalized feedback and guidance.", "conclusion": "Our user studies prove the usefulness of CoachGPT and the potential of large language models for academic writing."}}
{"id": "2506.17417", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17417", "abs": "https://arxiv.org/abs/2506.17417", "authors": ["Mingyuan Wu", "Meitang Li", "Jingcheng Yang", "Jize Jiang", "Kaizhuo Yan", "Zhaoheng Li", "Minjia Zhang", "Klara Nahrstedt"], "title": "Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in Inference-time Scaling?", "comment": "Work in progress", "summary": "Recent advances in large language models (LLMs) have demonstrated that\ninference-time computation techniques, such as decoding-time scaling and\nself-refinement, can significantly enhance reasoning capabilities without\nrelying on external knowledge. A key driver of this success is the emergence of\nself-correction and self-verification behaviors, often elicited through\nreinforcement learning (RL). In this paper, we investigate whether these\ninference-time techniques extend effectively to vision-language models (VLMs),\nparticularly those trained with RL. We find that while decoding strategies such\nas majority voting and best-of-N selection with self-verification all improve\nVLM reasoning performance, generation-reliant methods such as the former\nachieve significantly higher gains versus verification-reliant methods such as\nthe latter. Additionally, the self-correction behavior often associated with\nRL-tuned models, such as aha moment, does not lead to measurable gains. We show\nvia extensive experimentation within the inference-time scaling framework to\nidentify a key root cause: RL-trained VLMs still lack robust self-verification\ncapabilities across both visual and textual modalities.", "AI": {"tldr": "Inference-time computation techniques can enhance reasoning capabilities without relying on external knowledge, but RL-trained VLMs still lack robust self-verification capabilities.", "motivation": "investigate whether inference-time techniques extend effectively to vision-language models (VLMs), particularly those trained with RL", "method": "decoding strategies such as majority voting and best-of-N selection with self-verification", "result": "decoding strategies improve VLM reasoning performance, generation-reliant methods achieve significantly higher gains versus verification-reliant methods, self-correction behavior does not lead to measurable gains", "conclusion": "RL-trained VLMs still lack robust self-verification capabilities across both visual and textual modalities."}}
{"id": "2506.17629", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.17629", "abs": "https://arxiv.org/abs/2506.17629", "authors": ["Kailing Li", "Qi'ao Xu", "Tianwen Qian", "Yuqian Fu", "Yang Jiao", "Xiaoling Wang"], "title": "CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning", "comment": null, "summary": "Embodied Visual Reasoning (EVR) seeks to follow complex, free-form\ninstructions based on egocentric video, enabling semantic understanding and\nspatiotemporal reasoning in dynamic environments. Despite its promising\npotential, EVR encounters significant challenges stemming from the diversity of\ncomplex instructions and the intricate spatiotemporal dynamics in long-term\negocentric videos. Prior solutions either employ Large Language Models (LLMs)\nover static video captions, which often omit critical visual details, or rely\non end-to-end Vision-Language Models (VLMs) that struggle with stepwise\ncompositional reasoning. Consider the complementary strengths of LLMs in\nreasoning and VLMs in perception, we propose CLiViS. It is a novel\ntraining-free framework that leverages LLMs for high-level task planning and\norchestrates VLM-driven open-world visual perception to iteratively update the\nscene context. Building on this synergy, the core of CLiViS is a dynamic\nCognitive Map that evolves throughout the reasoning process. This map\nconstructs a structured representation of the embodied scene, bridging\nlow-level perception and high-level reasoning. Extensive experiments across\nmultiple benchmarks demonstrate the effectiveness and generality of CLiViS,\nespecially in handling long-term visual dependencies. Code is available at\nhttps://github.com/Teacher-Tom/CLiViS.", "AI": {"tldr": "CLiViS\u662f\u4e00\u4e2a\u5229\u7528LLM\u548cVLM\u4f18\u52bf\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5177\u8eab\u89c6\u89c9\u63a8\u7406\uff0c\u901a\u8fc7\u52a8\u6001\u8ba4\u77e5\u5730\u56fe\u6865\u63a5\u4f4e\u5c42\u611f\u77e5\u548c\u9ad8\u5c42\u63a8\u7406\uff0c\u5c24\u5176\u64c5\u957f\u5904\u7406\u957f\u671f\u89c6\u89c9\u4f9d\u8d56\u3002", "motivation": "\u5177\u8eab\u89c6\u89c9\u63a8\u7406\uff08EVR\uff09\u65e8\u5728\u9075\u5faa\u57fa\u4e8e\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\u7684\u590d\u6742\u3001\u81ea\u7531\u5f62\u5f0f\u7684\u6307\u4ee4\uff0c\u4ece\u800c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u8bed\u4e49\u7406\u89e3\u548c\u65f6\u7a7a\u63a8\u7406\u3002\u5148\u524d\u7684\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u5728\u9759\u6001\u89c6\u9891\u5b57\u5e55\u4e0a\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u8fd9\u7ecf\u5e38\u7701\u7565\u5173\u952e\u7684\u89c6\u89c9\u7ec6\u8282\uff0c\u8981\u4e48\u4f9d\u8d56\u4e8e\u7aef\u5230\u7aef\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u8fd9\u4e9b\u6a21\u578b\u96be\u4ee5\u8fdb\u884c\u9010\u6b65\u7ec4\u5408\u63a8\u7406\u3002", "method": "CLiViS\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u514d\u8bad\u7ec3\u6846\u67b6\uff0c\u5229\u7528LLM\u8fdb\u884c\u9ad8\u5c42\u4efb\u52a1\u89c4\u5212\uff0c\u5e76\u534f\u8c03VLM\u9a71\u52a8\u7684\u5f00\u653e\u4e16\u754c\u89c6\u89c9\u611f\u77e5\u6765\u8fed\u4ee3\u66f4\u65b0\u573a\u666f\u4e0a\u4e0b\u6587\u3002\u5176\u6838\u5fc3\u662f\u4e00\u4e2a\u52a8\u6001\u8ba4\u77e5\u5730\u56fe\uff0c\u6784\u5efa\u4e86\u5177\u8eab\u573a\u666f\u7684\u7ed3\u6784\u5316\u8868\u793a\uff0c\u6865\u63a5\u4e86\u4f4e\u5c42\u611f\u77e5\u548c\u9ad8\u5c42\u63a8\u7406\u3002", "result": "CLiViS\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u901a\u7528\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u957f\u671f\u89c6\u89c9\u4f9d\u8d56\u65b9\u9762\u3002", "conclusion": "CLiViS\u5728\u5904\u7406\u957f\u671f\u89c6\u89c9\u4f9d\u8d56\u65b9\u9762\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2506.17715", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17715", "abs": "https://arxiv.org/abs/2506.17715", "authors": ["Matthias Sch\u00f6ffel", "Esteban Garces Arias", "Marinus Wiedner", "Paula Ruppert", "Meimingwei Li", "Christian Heumann", "Matthias A\u00dfenmacher"], "title": "Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource Medieval Romance Languages", "comment": null, "summary": "Part-of-speech (POS) tagging remains a foundational component in natural\nlanguage processing pipelines, particularly critical for historical text\nanalysis at the intersection of computational linguistics and digital\nhumanities. Despite significant advancements in modern large language models\n(LLMs) for ancient languages, their application to Medieval Romance languages\npresents distinctive challenges stemming from diachronic linguistic evolution,\nspelling variations, and labeled data scarcity. This study systematically\ninvestigates the central determinants of POS tagging performance across diverse\ncorpora of Medieval Occitan, Medieval Spanish, and Medieval French texts,\nspanning biblical, hagiographical, medical, and dietary domains. Through\nrigorous experimentation, we evaluate how fine-tuning approaches, prompt\nengineering, model architectures, decoding strategies, and cross-lingual\ntransfer learning techniques affect tagging accuracy. Our results reveal both\nnotable limitations in LLMs' ability to process historical language variations\nand non-standardized spelling, as well as promising specialized techniques that\neffectively address the unique challenges presented by low-resource historical\nlanguages.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86LLMs\u5728\u4e2d\u4e16\u7eaa\u7f57\u66fc\u8bedPOS tagging\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5176\u5728\u5904\u7406\u5386\u53f2\u8bed\u8a00\u53d8\u4f53\u548c\u975e\u6807\u51c6\u5316\u62fc\u5199\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b\u6709\u6548\u7684\u4e13\u95e8\u6280\u672f\u3002", "motivation": "\u8bcd\u6027 (POS) \u6807\u6ce8\u4ecd\u7136\u662f\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6d41\u7a0b\u4e2d\u7684\u4e00\u4e2a\u57fa\u7840\u7ec4\u4ef6, \u5bf9\u4e8e\u8ba1\u7b97\u8bed\u8a00\u5b66\u548c\u6570\u5b57\u4eba\u6587\u4ea4\u53c9\u9886\u57df\u7684\u5386\u53f2\u6587\u672c\u5206\u6790\u5c24\u5176\u91cd\u8981\u3002\u5c3d\u7ba1\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u53e4\u4ee3\u8bed\u8a00\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65, \u4f46\u5b83\u4eec\u5728\u5e94\u7528\u4e8e\u4e2d\u4e16\u7eaa\u7f57\u66fc\u8bed\u65f6\uff0c\u7531\u4e8e\u5386\u65f6\u8bed\u8a00\u6f14\u53d8\u3001\u62fc\u5199\u53d8\u5316\u548c\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\uff0c\u9762\u4e34\u7740\u72ec\u7279\u7684\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u4e2d\u4e16\u7eaa\u5965\u514b\u8bed\u3001\u4e2d\u4e16\u7eaa\u897f\u73ed\u7259\u8bed\u548c\u4e2d\u4e16\u7eaa\u6cd5\u8bed\u6587\u672c\u4e2dPOS tagging\u6027\u80fd\u7684\u51b3\u5b9a\u56e0\u7d20, \u5e76\u8bc4\u4f30\u4e86\u5fae\u8c03\u65b9\u6cd5\u3001\u63d0\u793a\u5de5\u7a0b\u3001\u6a21\u578b\u67b6\u6784\u3001\u89e3\u7801\u7b56\u7565\u548c\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u6280\u672f\u5bf9 tagging \u51c6\u786e\u7387\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cLLMs\u5728\u5904\u7406\u5386\u53f2\u8bed\u8a00\u53d8\u4f53\u548c\u975e\u6807\u51c6\u5316\u62fc\u5199\u65b9\u9762\u7684\u80fd\u529b\u5b58\u5728\u660e\u663e\u7684\u5c40\u9650\u6027, \u540c\u65f6\u4e5f\u8868\u660e\u4e00\u4e9b\u6709\u5e0c\u671b\u7684\u4e13\u95e8\u6280\u672f\u53ef\u4ee5\u6709\u6548\u5730\u89e3\u51b3\u4f4e\u8d44\u6e90\u5386\u53f2\u8bed\u8a00\u5e26\u6765\u7684\u72ec\u7279\u6311\u6218\u3002", "conclusion": "LLMs\u5728\u5904\u7406\u5386\u53f2\u8bed\u8a00\u53d8\u4f53\u548c\u975e\u6807\u51c6\u5316\u62fc\u5199\u65b9\u9762\u5b58\u5728\u660e\u663e\u7684\u5c40\u9650\u6027, \u4f46\u4e5f\u5b58\u5728\u4e00\u4e9b\u6709\u5e0c\u671b\u7684\u4e13\u95e8\u6280\u672f, \u53ef\u4ee5\u6709\u6548\u5730\u89e3\u51b3\u4f4e\u8d44\u6e90\u5386\u53f2\u8bed\u8a00\u5e26\u6765\u7684\u72ec\u7279\u6311\u6218\u3002"}}
{"id": "2506.18156", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18156", "abs": "https://arxiv.org/abs/2506.18156", "authors": ["Akash Kundu", "Rishika Goswami"], "title": "AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology", "comment": null, "summary": "We investigate whether Large Language Models (LLMs) exhibit human-like\ncognitive patterns under four established frameworks from psychology: Thematic\nApperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and\nCognitive Dissonance. We evaluated several proprietary and open-source models\nusing structured prompts and automated scoring. Our findings reveal that these\nmodels often produce coherent narratives, show susceptibility to positive\nframing, exhibit moral judgments aligned with Liberty/Oppression concerns, and\ndemonstrate self-contradictions tempered by extensive rationalization. Such\nbehaviors mirror human cognitive tendencies yet are shaped by their training\ndata and alignment methods. We discuss the implications for AI transparency,\nethical deployment, and future work that bridges cognitive psychology and AI\nsafety", "AI": {"tldr": "LLMs show human-like cognitive biases, which has implications for AI ethics.", "motivation": "Investigating whether LLMs exhibit human-like cognitive patterns under established psychology frameworks.", "method": "Using structured prompts and automated scoring to evaluate several proprietary and open-source LLMs under four established psychology frameworks (TAT, Framing Bias, MFT, Cognitive Dissonance).", "result": "LLMs produce coherent narratives, are susceptible to positive framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and demonstrate self-contradictions tempered by rationalization.", "conclusion": "LLMs exhibit human-like cognitive tendencies, shaped by training data and alignment methods, raising implications for AI transparency and ethical deployment."}}
{"id": "2506.17466", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17466", "abs": "https://arxiv.org/abs/2506.17466", "authors": ["Amitash Nanda", "Sree Bhargavi Balija", "Debashis Sahoo"], "title": "FedNAMs: Performing Interpretability Analysis in Federated Learning Context", "comment": "13 pages, 6 figures", "summary": "Federated learning continues to evolve but faces challenges in\ninterpretability and explainability. To address these challenges, we introduce\na novel approach that employs Neural Additive Models (NAMs) within a federated\nlearning framework. This new Federated Neural Additive Models (FedNAMs)\napproach merges the advantages of NAMs, where individual networks concentrate\non specific input features, with the decentralized approach of federated\nlearning, ultimately producing interpretable analysis results. This integration\nenhances privacy by training on local data across multiple devices, thereby\nminimizing the risks associated with data centralization and improving model\nrobustness and generalizability. FedNAMs maintain detailed, feature-specific\nlearning, making them especially valuable in sectors such as finance and\nhealthcare. They facilitate the training of client-specific models to integrate\nlocal updates, preserve privacy, and mitigate concerns related to\ncentralization. Our studies on various text and image classification tasks,\nusing datasets such as OpenFetch ML Wine, UCI Heart Disease, and Iris, show\nthat FedNAMs deliver strong interpretability with minimal accuracy loss\ncompared to traditional Federated Deep Neural Networks (DNNs). The research\ninvolves notable findings, including the identification of critical predictive\nfeatures at both client and global levels. Volatile acidity, sulfates, and\nchlorides for wine quality. Chest pain type, maximum heart rate, and number of\nvessels for heart disease. Petal length and width for iris classification. This\napproach strengthens privacy and model efficiency and improves interpretability\nand robustness across diverse datasets. Finally, FedNAMs generate insights on\ncauses of highly and low interpretable features.", "AI": {"tldr": "FedNAMs\u7ed3\u5408\u4e86NAMs\u548c\u8054\u90a6\u5b66\u4e60\u7684\u4f18\u70b9\uff0c\u5728\u4fdd\u8bc1\u9690\u79c1\u7684\u524d\u63d0\u4e0b\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e0d\u65ad\u53d1\u5c55\uff0c\u4f46\u5728\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u795e\u7ecf\u52a0\u6027\u6a21\u578b(FedNAMs)\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06NAMs\u7684\u4f18\u52bf\u4e0e\u8054\u90a6\u5b66\u4e60\u7684\u53bb\u4e2d\u5fc3\u5316\u65b9\u6cd5\u76f8\u7ed3\u5408\u3002", "result": "\u5728\u5404\u79cd\u6587\u672c\u548c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u7814\u7a76\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7684\u8054\u90a6\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc(dnn)\u76f8\u6bd4\uff0cFedNAMs\u5177\u6709\u5f88\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u7cbe\u5ea6\u635f\u5931\u6700\u5c0f\u3002", "conclusion": "FedNAMs\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9690\u79c1\u548c\u6a21\u578b\u6548\u7387\uff0c\u5e76\u80fd\u8bc6\u522b\u5173\u952e\u9884\u6d4b\u7279\u5f81\u3002"}}
{"id": "2506.17632", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17632", "abs": "https://arxiv.org/abs/2506.17632", "authors": ["Hangcheng Liu", "Xu Kuang", "Xingshuo Han", "Xingwan Wu", "Haoran Ou", "Shangwei Guo", "Xingyi Huang", "Tao Xiang", "Tianwei Zhang"], "title": "Optimization-Free Patch Attack on Stereo Depth Estimation", "comment": null, "summary": "Stereo Depth Estimation (SDE) is essential for scene understanding in\nvision-based systems like autonomous driving. However, recent studies show that\nSDE models are vulnerable to adversarial attacks, which are often limited to\nunrealistic settings, e.g., digital perturbations on separate stereo views in\nstatic scenes, restricting their real-world applicability. This raises a\ncritical question: how can we design physically realizable, scene-adaptive, and\ntransferable attacks against SDE under realistic constraints?\n  To answer this, we make two key contributions. First, we propose a unified\nattack framework that extends optimization-based techniques to four core stages\nof stereo matching: feature extraction, cost-volume construction, cost\naggregation, and disparity regression. A comprehensive stage-wise evaluation\nacross 9 mainstream SDE models, under constraints like photometric consistency,\nreveals that optimization-based patches suffer from poor transferability.\nInterestingly, partially transferable patches suggest that patterns, rather\nthan pixel-level perturbations, may be key to generalizable attacks. Motivated\nby this, we present PatchHunter, the first optimization-free adversarial patch\nattack against SDE. PatchHunter formulates patch generation as a reinforcement\nlearning-driven search over a structured space of visual patterns crafted to\ndisrupt SDE assumptions.\n  We validate PatchHunter across three levels: the KITTI dataset, the CARLA\nsimulator, and real-world vehicle deployment. PatchHunter not only surpasses\noptimization-based methods in effectiveness but also achieves significantly\nbetter black-box transferability. Even under challenging physical conditions\nlike low light, PatchHunter maintains high attack success (e.g., D1-all > 0.4),\nwhereas optimization-based methods fail.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5PatchHunter\uff0c\u7528\u4e8e\u653b\u51fb\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\uff08SDE\uff09\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728\u89c6\u89c9\u6a21\u5f0f\u7684\u7ed3\u6784\u5316\u7a7a\u95f4\u4e2d\u641c\u7d22\uff0c\u4ee5\u6270\u4e71SDE\u7684\u5047\u8bbe\uff0c\u5e76\u5728KITTI\u6570\u636e\u96c6\u3001CARLA\u6a21\u62df\u5668\u548c\u771f\u5b9e\u8f66\u8f86\u90e8\u7f72\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u53ef\u79fb\u690d\u6027\u3002", "motivation": "\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\uff08SDE\uff09\u5bf9\u4e8e\u50cf\u81ea\u52a8\u9a7e\u9a76\u8fd9\u6837\u57fa\u4e8e\u89c6\u89c9\u7684\u7cfb\u7edf\u4e2d\u7684\u573a\u666f\u7406\u89e3\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0cSDE\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u8fd9\u4e9b\u653b\u51fb\u901a\u5e38\u5c40\u9650\u4e8e\u4e0d\u5207\u5b9e\u9645\u7684\u8bbe\u7f6e\uff0c\u4f8b\u5982\uff0c\u9759\u6001\u573a\u666f\u4e2d\u5355\u72ec\u7acb\u4f53\u89c6\u56fe\u4e0a\u7684\u6570\u5b57\u6270\u52a8\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u9002\u7528\u6027\u3002\u8fd9\u5c31\u63d0\u51fa\u4e86\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u6211\u4eec\u5982\u4f55\u5728\u73b0\u5b9e\u7ea6\u675f\u4e0b\u8bbe\u8ba1\u7269\u7406\u4e0a\u53ef\u5b9e\u73b0\u7684\u3001\u573a\u666f\u81ea\u9002\u5e94\u7684\u548c\u53ef\u8f6c\u79fb\u7684\u653b\u51fb\u6765\u5bf9\u6297SDE\uff1f", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u653b\u51fb\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u57fa\u4e8e\u4f18\u5316\u7684\u6280\u672f\u6269\u5c55\u5230\u7acb\u4f53\u5339\u914d\u7684\u56db\u4e2a\u6838\u5fc3\u9636\u6bb5\uff1a\u7279\u5f81\u63d0\u53d6\u3001\u6210\u672c\u91cf\u6784\u5efa\u3001\u6210\u672c\u805a\u5408\u548c\u89c6\u5dee\u56de\u5f52\u3002\u63d0\u51faPatchHunter\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u9488\u5bf9SDE\u7684\u65e0\u4f18\u5316\u5bf9\u6297\u8865\u4e01\u653b\u51fb\u3002", "result": "\u5bf99\u4e2a\u4e3b\u6d41SDE\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u7684\u9636\u6bb5\u6027\u8bc4\u4f30\uff0c\u5728\u5149\u5ea6\u4e00\u81f4\u6027\u7b49\u7ea6\u675f\u6761\u4ef6\u4e0b\uff0c\u7ed3\u679c\u8868\u660e\u57fa\u4e8e\u4f18\u5316\u7684\u8865\u4e01\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002\u6709\u8da3\u7684\u662f\uff0c\u90e8\u5206\u53ef\u8f6c\u79fb\u7684\u8865\u4e01\u8868\u660e\uff0c\u6a21\u5f0f\uff08\u800c\u4e0d\u662f\u50cf\u7d20\u7ea7\u6270\u52a8\uff09\u53ef\u80fd\u662f\u5b9e\u73b0\u901a\u7528\u653b\u51fb\u7684\u5173\u952e\u3002PatchHunter\u4e0d\u4ec5\u5728\u6709\u6548\u6027\u4e0a\u8d85\u8d8a\u4e86\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u800c\u4e14\u5b9e\u73b0\u4e86\u660e\u663e\u66f4\u597d\u7684\u9ed1\u76d2\u53ef\u79fb\u690d\u6027\u3002\u5373\u4f7f\u5728\u4f4e\u5149\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u7269\u7406\u6761\u4ef6\u4e0b\uff0cPatchHunter\u4e5f\u80fd\u4fdd\u6301\u8f83\u9ad8\u7684\u653b\u51fb\u6210\u529f\u7387\uff08\u4f8b\u5982\uff0cD1-all > 0.4\uff09\uff0c\u800c\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\u5219\u5931\u8d25\u4e86\u3002", "conclusion": "PatchHunter\u4e0d\u4ec5\u5728\u6709\u6548\u6027\u4e0a\u8d85\u8d8a\u4e86\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u800c\u4e14\u5b9e\u73b0\u4e86\u660e\u663e\u66f4\u597d\u7684\u9ed1\u76d2\u53ef\u79fb\u690d\u6027\u3002\u5373\u4f7f\u5728\u4f4e\u5149\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u7269\u7406\u6761\u4ef6\u4e0b\uff0cPatchHunter\u4e5f\u80fd\u4fdd\u6301\u8f83\u9ad8\u7684\u653b\u51fb\u6210\u529f\u7387\uff08\u4f8b\u5982\uff0cD1-all > 0.4\uff09\uff0c\u800c\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\u5219\u5931\u8d25\u4e86\u3002"}}
{"id": "2506.17728", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17728", "abs": "https://arxiv.org/abs/2506.17728", "authors": ["Dalong Zhang", "Jun Xu", "Jun Zhou", "Lei Liang", "Lin Yuan", "Ling Zhong", "Mengshu Sun", "Peilong Zhao", "QiWei Wang", "Xiaorui Wang", "Xinkai Du", "YangYang Hou", "Yu Ao", "ZhaoYang Wang", "Zhengke Gui", "ZhiYing Yi", "Zhongpu Bo"], "title": "KAG-Thinker: Teaching Large Language Models to Think with Human-like Reasoning Process", "comment": null, "summary": "In this paper, we introduce KAG-Thinker, a novel human-like reasoning\nframework built upon a parameter-light large language model (LLM). Our approach\nenhances the logical coherence and contextual consistency of the thinking\nprocess in question-answering (Q\\&A) tasks on domain-specific knowledge bases\n(KBs) within LLMs. This framework simulates human cognitive mechanisms for\nhandling complex problems by establishing a structured thinking process.\nContinuing the \\textbf{Logical Form} guided retrieval and reasoning technology\nroute of KAG v0.7, firstly, it decomposes complex questions into independently\nsolvable sub-problems(also referred to as logical forms) through\n\\textbf{breadth decomposition}, each represented in two equivalent\nforms-natural language and logical function-and further classified as either\nKnowledge Retrieval or Reasoning Analysis tasks, with dependencies and\nvariables passing explicitly modeled via logical function interfaces. In the\nsolving process, the Retrieval function is used to perform knowledge retrieval\ntasks, while the Math and Deduce functions are used to perform reasoning\nanalysis tasks. Secondly, it is worth noting that, in the Knowledge Retrieval\nsub-problem tasks, LLMs and external knowledge sources are regarded as\nequivalent KBs. We use the \\textbf{knowledge boundary} model to determine the\noptimal source using self-regulatory mechanisms such as confidence calibration\nand reflective reasoning, and use the \\textbf{depth solving} model to enhance\nthe comprehensiveness of knowledge acquisition. Finally, instead of utilizing\nreinforcement learning, we employ supervised fine-tuning with multi-turn\ndialogues to align the model with our structured inference paradigm, thereby\navoiding excessive reflection. This is supported by a data evaluation framework\nand iterative corpus synthesis, which facilitate the generation of detailed\nreasoning trajectories...", "AI": {"tldr": "KAG-Thinker enhances LLM reasoning on knowledge bases by mimicking human cognition, decomposing problems, and using supervised fine-tuning.", "motivation": "The paper aims to enhance logical coherence and contextual consistency in question-answering tasks on domain-specific knowledge bases within LLMs by simulating human cognitive mechanisms.", "method": "The paper introduces KAG-Thinker, a framework that decomposes complex questions into sub-problems, uses breadth decomposition, knowledge retrieval, and depth solving models, and classifies tasks as Knowledge Retrieval or Reasoning Analysis.", "result": "The paper presents KAG-Thinker, a human-like reasoning framework built upon a parameter-light large language model (LLM).", "conclusion": "The paper utilizes supervised fine-tuning with multi-turn dialogues to align the model with a structured inference paradigm, avoiding excessive reflection, supported by a data evaluation framework and iterative corpus synthesis."}}
{"id": "2506.18158", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18158", "abs": "https://arxiv.org/abs/2506.18158", "authors": ["Xinzge Gao", "Chuanrui Hu", "Bin Chen", "Teng Li"], "title": "Chain-of-Memory: Enhancing GUI Agents for Cross-Application Navigation", "comment": null, "summary": "Multimodal large language models (MLLMs) are attracting growing attention in\nthe development of Graphical User Interface (GUI) agents. Existing approaches\noften rely on historical screenshots or actions to implicitly represent the\ntask state. This reliance poses challenges for GUI agents in accurately\nunderstanding task states and underscores the absence of effective mechanisms\nto store critical information in complex and lengthy cross-app tasks. To\naddress these challenges, we propose Chain-of-Memory (CoM), a novel approach\nfor explicitly modeling short-term and long-term memory in GUI agents. CoM\nachieves this by capturing action descriptions, integrating task-relevant\nscreen information, and maintaining a dedicated memory module to store and\nmanage this information. By leveraging explicit memory representations, CoM\nenables GUI agents to better understand task states and retain critical\nhistorical information persistently. To equip GUI agents with memory management\ncapabilities and evaluate the effectiveness of CoM, we developed the GUI\nOdyssey-CoM, a dataset comprising 111k screen-action pairs annotated with\nChain-of-Memory. Experimental results demonstrate that CoM significantly\nimproves GUI agents' performance in cross-application tasks. Additionally, GUI\nOdyssey-CoM enables 7B models to achieve memory management capabilities\ncomparable to 72B models. The dataset and code will be open-sourced.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChain-of-Memory (CoM) \u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u663e\u5f0f\u5efa\u6a21GUI agent\u4e2d\u7684\u77ed\u671f\u548c\u957f\u671f\u8bb0\u5fc6, \u663e\u8457\u63d0\u9ad8\u4e86GUI agent\u5728\u8de8\u5e94\u7528\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5386\u53f2\u622a\u56fe\u6216\u52a8\u4f5c\u6765\u9690\u5f0f\u5730\u8868\u793a\u4efb\u52a1\u72b6\u6001\u3002\u8fd9\u79cd\u4f9d\u8d56\u5bf9GUI agent\u51c6\u786e\u7406\u89e3\u4efb\u52a1\u72b6\u6001\u6784\u6210\u4e86\u6311\u6218\uff0c\u5e76\u7a81\u51fa\u4e86\u5728\u590d\u6742\u548c\u6f2b\u957f\u7684\u8de8\u5e94\u7528\u4efb\u52a1\u4e2d\u5b58\u50a8\u5173\u952e\u4fe1\u606f\u7684\u6709\u6548\u673a\u5236\u7684\u7f3a\u5931\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChain-of-Memory (CoM) \u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u663e\u5f0f\u5efa\u6a21GUI agent\u4e2d\u7684\u77ed\u671f\u548c\u957f\u671f\u8bb0\u5fc6\u3002CoM\u901a\u8fc7\u6355\u83b7\u52a8\u4f5c\u63cf\u8ff0\u3001\u6574\u5408\u4efb\u52a1\u76f8\u5173\u7684\u5c4f\u5e55\u4fe1\u606f\uff0c\u5e76\u7ef4\u62a4\u4e00\u4e2a\u4e13\u7528\u8bb0\u5fc6\u6a21\u5757\u6765\u5b58\u50a8\u548c\u7ba1\u7406\u8fd9\u4e9b\u4fe1\u606f\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002", "result": "CoM\u663e\u8457\u63d0\u9ad8\u4e86GUI agent\u5728\u8de8\u5e94\u7528\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0cGUI Odyssey-CoM\u4f7f7B\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u4e0e72B\u6a21\u578b\u76f8\u5f53\u7684\u5185\u5b58\u7ba1\u7406\u80fd\u529b\u3002", "conclusion": "CoM\u663e\u8457\u63d0\u9ad8\u4e86GUI agent\u5728\u8de8\u5e94\u7528\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0cGUI Odyssey-CoM\u4f7f7B\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u4e0e72B\u6a21\u578b\u76f8\u5f53\u7684\u5185\u5b58\u7ba1\u7406\u80fd\u529b\u3002"}}
{"id": "2506.17475", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17475", "abs": "https://arxiv.org/abs/2506.17475", "authors": ["Steffen Schotth\u00f6fer", "Timon Klein", "Jonas Kusch"], "title": "A geometric framework for momentum-based optimizers for low-rank training", "comment": null, "summary": "Low-rank pre-training and fine-tuning have recently emerged as promising\ntechniques for reducing the computational and storage costs of large neural\nnetworks. Training low-rank parameterizations typically relies on conventional\noptimizers such as heavy ball momentum methods or Adam. In this work, we\nidentify and analyze potential difficulties that these training methods\nencounter when used to train low-rank parameterizations of weights. In\nparticular, we show that classical momentum methods can struggle to converge to\na local optimum due to the geometry of the underlying optimization landscape.\nTo address this, we introduce novel training strategies derived from dynamical\nlow-rank approximation, which explicitly account for the underlying geometric\nstructure. Our approach leverages and combines tools from dynamical low-rank\napproximation and momentum-based optimization to design optimizers that respect\nthe intrinsic geometry of the parameter space. We validate our methods through\nnumerical experiments, demonstrating faster convergence, and stronger\nvalidation metrics at given parameter budgets.", "AI": {"tldr": "This paper introduces new training strategies based on dynamical low-rank approximation to improve the convergence and performance of low-rank pre-training and fine-tuning.", "motivation": "Classical momentum methods can struggle to converge to a local optimum due to the geometry of the underlying optimization landscape.", "method": "The paper introduces novel training strategies derived from dynamical low-rank approximation, which explicitly account for the underlying geometric structure. Our approach leverages and combines tools from dynamical low-rank approximation and momentum-based optimization to design optimizers that respect the intrinsic geometry of the parameter space.", "result": "The methods are validated through numerical experiments, demonstrating faster convergence, and stronger validation metrics at given parameter budgets.", "conclusion": "The paper introduces novel training strategies derived from dynamical low-rank approximation, which explicitly account for the underlying geometric structure. The methods are validated through numerical experiments, demonstrating faster convergence, and stronger validation metrics at given parameter budgets."}}
{"id": "2506.17633", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17633", "abs": "https://arxiv.org/abs/2506.17633", "authors": ["Xiang Fang", "Arvind Easwaran", "Blaise Genest"], "title": "Adaptive Multi-prompt Contrastive Network for Few-shot Out-of-distribution Detection", "comment": "ICML 2025", "summary": "Out-of-distribution (OOD) detection attempts to distinguish outlier samples\nto prevent models trained on the in-distribution (ID) dataset from producing\nunavailable outputs. Most OOD detection methods require many IID samples for\ntraining, which seriously limits their real-world applications. To this end, we\ntarget a challenging setting: few-shot OOD detection, where {Only a few {\\em\nlabeled ID} samples are available.} Therefore, few-shot OOD detection is much\nmore challenging than the traditional OOD detection setting. Previous few-shot\nOOD detection works ignore the distinct diversity between different classes. In\nthis paper, we propose a novel network: Adaptive Multi-prompt Contrastive\nNetwork (AMCN), which adapts the ID-OOD separation boundary by learning inter-\nand intra-class distribution. To compensate for the absence of OOD and scarcity\nof ID {\\em image samples}, we leverage CLIP, connecting text with images,\nengineering learnable ID and OOD {\\em textual prompts}. Specifically, we first\ngenerate adaptive prompts (learnable ID prompts, label-fixed OOD prompts and\nlabel-adaptive OOD prompts). Then, we generate an adaptive class boundary for\neach class by introducing a class-wise threshold. Finally, we propose a\nprompt-guided ID-OOD separation module to control the margin between ID and OOD\nprompts. Experimental results show that AMCN outperforms other state-of-the-art\nworks.", "AI": {"tldr": "Addresses few-shot OOD detection with Adaptive Multi-prompt Contrastive Network (AMCN) using CLIP to handle limited data and improve ID-OOD separation.", "motivation": "Most OOD detection methods require many IID samples for training, which seriously limits their real-world applications.  Previous few-shot OOD detection works ignore the distinct diversity between different classes. To compensate for the absence of OOD and scarcity of ID image samples, we leverage CLIP, connecting text with images, engineering learnable ID and OOD textual prompts.", "method": "Adaptive Multi-prompt Contrastive Network (AMCN), which adapts the ID-OOD separation boundary by learning inter- and intra-class distribution. leverage CLIP, connecting text with images, engineering learnable ID and OOD textual prompts. Specifically, generate adaptive prompts (learnable ID prompts, label-fixed OOD prompts and label-adaptive OOD prompts). Then, generate an adaptive class boundary for each class by introducing a class-wise threshold. Finally, propose a prompt-guided ID-OOD separation module to control the margin between ID and OOD prompts", "result": "AMCN outperforms other state-of-the-art works.", "conclusion": "AMCN outperforms other state-of-the-art works."}}
{"id": "2506.17748", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17748", "abs": "https://arxiv.org/abs/2506.17748", "authors": ["Anwoy Chatterjee", "Yash Goel", "Tanmoy Chakraborty"], "title": "HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations", "comment": null, "summary": "Contemporary Language Models (LMs), while impressively fluent, often generate\ncontent that is factually incorrect or unfaithful to the input context - a\ncritical issue commonly referred to as 'hallucination'. This tendency of LMs to\ngenerate hallucinated content undermines their reliability, especially because\nthese fabrications are often highly convincing and therefore difficult to\ndetect. While several existing methods attempt to detect hallucinations, most\nrely on analyzing multiple generations per input, leading to increased\ncomputational cost and latency. To address this, we propose a single-pass,\ntraining-free approach for effective Hallucination detectIon via Decoupled\nrEpresentations (HIDE). Our approach leverages the hypothesis that\nhallucinations result from a statistical decoupling between an LM's internal\nrepresentations of input context and its generated output. We quantify this\ndecoupling using the Hilbert-Schmidt Independence Criterion (HSIC) applied to\nhidden-state representations extracted while generating the output sequence. We\nconduct extensive experiments on four diverse question answering datasets,\nevaluating both faithfulness and factuality hallucinations across six\nopen-source LMs of varying scales and properties. Our results demonstrate that\nHIDE outperforms other single-pass methods in almost all settings, achieving an\naverage relative improvement of ~29% in AUC-ROC over the best-performing\nsingle-pass strategy across various models and datasets. Additionally, HIDE\nshows competitive and often superior performance with multi-pass\nstate-of-the-art methods, obtaining an average relative improvement of ~3% in\nAUC-ROC while consuming ~51% less computation time. Our findings highlight the\neffectiveness of exploiting internal representation decoupling in LMs for\nefficient and practical hallucination detection.", "AI": {"tldr": "This paper proposes HIDE, a single-pass, training-free approach for hallucination detection in LMs, which outperforms existing single-pass methods and achieves competitive performance with multi-pass methods while consuming less computation time.", "motivation": "Contemporary Language Models (LMs), while impressively fluent, often generate content that is factually incorrect or unfaithful to the input context - a critical issue commonly referred to as 'hallucination'. While several existing methods attempt to detect hallucinations, most rely on analyzing multiple generations per input, leading to increased computational cost and latency.", "method": "a single-pass, training-free approach for effective Hallucination detectIon via Decoupled rEpresentations (HIDE). It leverages the hypothesis that hallucinations result from a statistical decoupling between an LM's internal representations of input context and its generated output. It quantifies this decoupling using the Hilbert-Schmidt Independence Criterion (HSIC) applied to hidden-state representations extracted while generating the output sequence.", "result": "HIDE outperforms other single-pass methods in almost all settings, achieving an average relative improvement of ~29% in AUC-ROC over the best-performing single-pass strategy across various models and datasets.", "conclusion": "HIDE shows competitive and often superior performance with multi-pass state-of-the-art methods, obtaining an average relative improvement of ~3% in AUC-ROC while consuming ~51% less computation time. Our findings highlight the effectiveness of exploiting internal representation decoupling in LMs for efficient and practical hallucination detection."}}
{"id": "2506.18183", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.18183", "abs": "https://arxiv.org/abs/2506.18183", "authors": ["Zhiting Mei", "Christina Zhang", "Tenny Yin", "Justin Lidard", "Ola Shorinwa", "Anirudha Majumdar"], "title": "Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?", "comment": null, "summary": "Reasoning language models have set state-of-the-art (SOTA) records on many\nchallenging benchmarks, enabled by multi-step reasoning induced using\nreinforcement learning. However, like previous language models, reasoning\nmodels are prone to generating confident, plausible responses that are\nincorrect (hallucinations). Knowing when and how much to trust these models is\ncritical to the safe deployment of reasoning models in real-world applications.\nTo this end, we explore uncertainty quantification of reasoning models in this\nwork. Specifically, we ask three fundamental questions: First, are reasoning\nmodels well-calibrated? Second, does deeper reasoning improve model\ncalibration? Finally, inspired by humans' innate ability to double-check their\nthought processes to verify the validity of their answers and their confidence,\nwe ask: can reasoning models improve their calibration by explicitly reasoning\nabout their chain-of-thought traces? We introduce introspective uncertainty\nquantification (UQ) to explore this direction. In extensive evaluations on SOTA\nreasoning models across a broad range of benchmarks, we find that reasoning\nmodels: (i) are typically overconfident, with self-verbalized confidence\nestimates often greater than 85% particularly for incorrect responses, (ii)\nbecome even more overconfident with deeper reasoning, and (iii) can become\nbetter calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not\nuniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we\nconclude with important research directions to design necessary UQ benchmarks\nand improve the calibration of reasoning models.", "AI": {"tldr": "Reasoning models are overconfident, get more overconfident with deeper reasoning, but can improve calibration through introspection, though not uniformly.", "motivation": "Knowing when and how much to trust these models is critical to the safe deployment of reasoning models in real-world applications.", "method": "Introduce introspective uncertainty quantification (UQ).", "result": "Reasoning models: (i) are typically overconfident, with self-verbalized confidence estimates often greater than 85% particularly for incorrect responses, (ii) become even more overconfident with deeper reasoning, and (iii) can become better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated).", "conclusion": "Reasoning models can become better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated)."}}
{"id": "2506.17499", "categories": ["cs.LG", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.17499", "abs": "https://arxiv.org/abs/2506.17499", "authors": ["Xuanyu Zhuang", "Geoffroy Peeters", "Ga\u00ebl Richard"], "title": "Episode-specific Fine-tuning for Metric-based Few-shot Learners with Optimization-based Training", "comment": null, "summary": "In few-shot classification tasks (so-called episodes), a small set of labeled\nsupport samples is provided during inference to aid the classification of\nunlabeled query samples. Metric-based models typically operate by computing\nsimilarities between query and support embeddings within a learned metric\nspace, followed by nearest-neighbor classification. However, these labeled\nsupport samples are often underutilized--they are only used for similarity\ncomparison, despite their potential to fine-tune and adapt the metric space\nitself to the classes in the current episode. To address this, we propose a\nseries of simple yet effective episode-specific, during-inference fine-tuning\nmethods for metric-based models, including Rotational Division Fine-Tuning\n(RDFT) and its two variants, Iterative Division Fine-Tuning (IDFT) and\nAugmented Division Fine-Tuning (ADFT). These methods construct pseudo\nsupport-query pairs from the given support set to enable fine-tuning even for\nnon-parametric models. Nevertheless, the severely limited amount of data in\neach task poses a substantial risk of overfitting when applying such\nfine-tuning strategies. To mitigate this, we further propose to train the\nmetric-based model within an optimization-based meta-learning framework. With\nthe combined efforts of episode-specific fine-tuning and optimization-based\nmeta-training, metric-based models are equipped with the ability to rapidly\nadapt to the limited support samples during inference while avoiding\noverfitting. We validate our approach on three audio datasets from diverse\ndomains, namely ESC-50 (environmental sounds), Speech Commands V2 (spoken\nkeywords), and Medley-solos-DB (musical instrument). Experimental results\ndemonstrate that our approach consistently improves performance for all\nevaluated metric-based models (especially for attention-based models) and\ngeneralizes well across different audio domains.", "AI": {"tldr": "Proposes episode-specific fine-tuning methods for metric-based models in few-shot classification to better utilize support samples and avoid overfitting, validated on audio datasets.", "motivation": "Labeled support samples are often underutilized--they are only used for similarity comparison, despite their potential to fine-tune and adapt the metric space itself to the classes in the current episode. The severely limited amount of data in each task poses a substantial risk of overfitting when applying such fine-tuning strategies.", "method": "episode-specific, during-inference fine-tuning methods for metric-based models, including Rotational Division Fine-Tuning (RDFT) and its two variants, Iterative Division Fine-Tuning (IDFT) and Augmented Division Fine-Tuning (ADFT).", "result": "Our approach consistently improves performance for all evaluated metric-based models (especially for attention-based models) and generalizes well across different audio domains.", "conclusion": "The proposed approach consistently improves performance for all evaluated metric-based models (especially for attention-based models) and generalizes well across different audio domains."}}
{"id": "2506.17645", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17645", "abs": "https://arxiv.org/abs/2506.17645", "authors": ["Shih-Wen Liu", "Hsuan-Yu Fan", "Wei-Ta Chu", "Fu-En Yang", "Yu-Chiang Frank Wang"], "title": "Histopathology Image Report Generation by Vision Language Model with Multimodal In-Context Learning", "comment": "Accepted to MIDL 2025", "summary": "Automating medical report generation from histopathology images is a critical\nchallenge requiring effective visual representations and domain-specific\nknowledge. Inspired by the common practices of human experts, we propose an\nin-context learning framework called PathGenIC that integrates context derived\nfrom the training set with a multimodal in-context learning (ICL) mechanism.\nOur method dynamically retrieves semantically similar whole slide image\n(WSI)-report pairs and incorporates adaptive feedback to enhance contextual\nrelevance and generation quality. Evaluated on the HistGen benchmark, the\nframework achieves state-of-the-art results, with significant improvements\nacross BLEU, METEOR, and ROUGE-L metrics, and demonstrates robustness across\ndiverse report lengths and disease categories. By maximizing training data\nutility and bridging vision and language with ICL, our work offers a solution\nfor AI-driven histopathology reporting, setting a strong foundation for future\nadvancements in multimodal clinical applications.", "AI": {"tldr": "PathGenIC\u662f\u4e00\u79cd\u7528\u4e8e\u4ece\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u81ea\u52a8\u751f\u6210\u533b\u7597\u62a5\u544a\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u8bad\u7ec3\u6570\u636e\uff0c\u5728HistGen\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u4ece\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u81ea\u52a8\u751f\u6210\u533b\u7597\u62a5\u544a\u662f\u4e00\u9879\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u6709\u6548\u7684\u89c6\u89c9\u8868\u793a\u548c\u9886\u57df\u77e5\u8bc6\u3002\u53d7\u5230\u4eba\u7c7b\u4e13\u5bb6\u5e38\u89c1\u505a\u6cd5\u7684\u542f\u53d1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\uff0c\u79f0\u4e3aPathGenIC\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u4ece\u8bad\u7ec3\u96c6\u5bfc\u51fa\u7684\u4e0a\u4e0b\u6587\u4e0e\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u673a\u5236\u3002", "result": "\u8be5\u6846\u67b6\u5728HistGen\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u5728BLEU\u3001METEOR\u548cROUGE-L\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u62a5\u544a\u957f\u5ea6\u548c\u75be\u75c5\u7c7b\u522b\u4e2d\u7684\u7a33\u5065\u6027\u3002", "conclusion": "PathGenIC\u6846\u67b6\u5728HistGen\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u5728BLEU\u3001METEOR\u548cROUGE-L\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u62a5\u544a\u957f\u5ea6\u548c\u75be\u75c5\u7c7b\u522b\u4e2d\u7684\u7a33\u5065\u6027\u3002\u901a\u8fc7\u6700\u5927\u5316\u8bad\u7ec3\u6570\u636e\u6548\u7528\u548c\u4f7f\u7528ICL\u6865\u63a5\u89c6\u89c9\u548c\u8bed\u8a00\uff0c\u4e3aAI\u9a71\u52a8\u7684\u7ec4\u7ec7\u75c5\u7406\u5b66\u62a5\u544a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u591a\u6a21\u6001\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u672a\u6765\u8fdb\u5c55\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2506.17789", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.17789", "abs": "https://arxiv.org/abs/2506.17789", "authors": ["N J Karthika", "Maharaj Brahma", "Rohit Saluja", "Ganesh Ramakrishnan", "Maunendra Sankar Desarkar"], "title": "Multilingual Tokenization through the Lens of Indian Languages: Challenges and Insights", "comment": null, "summary": "Tokenization plays a pivotal role in multilingual NLP. However, existing\ntokenizers are often skewed towards high-resource languages, limiting their\neffectiveness for linguistically diverse and morphologically rich languages\nsuch as those in the Indian subcontinent. This paper presents a comprehensive\nintrinsic evaluation of tokenization strategies across 17 Indian languages. We\nquantify the trade-offs between bottom-up and top-down tokenizer algorithms\n(BPE and Unigram LM), effects of vocabulary sizes, and compare strategies of\nmultilingual vocabulary construction such as joint and cluster-based training.\nWe also show that extremely low-resource languages can benefit from tokenizers\ntrained on related high-resource languages. Our study provides practical\ninsights for building more fair, efficient, and linguistically informed\ntokenizers for multilingual NLP.", "AI": {"tldr": "Evaluate tokenization strategies across 17 Indian languages and find that extremely low-resource languages can benefit from tokenizers trained on related high-resource languages.", "motivation": "Existing tokenizers are often skewed towards high-resource languages, limiting their effectiveness for linguistically diverse and morphologically rich languages such as those in the Indian subcontinent.", "method": "Comprehensive intrinsic evaluation of tokenization strategies across 17 Indian languages. Quantify the trade-offs between bottom-up and top-down tokenizer algorithms (BPE and Unigram LM), effects of vocabulary sizes, and compare strategies of multilingual vocabulary construction such as joint and cluster-based training.", "result": "Quantify the trade-offs between bottom-up and top-down tokenizer algorithms (BPE and Unigram LM), effects of vocabulary sizes, and compare strategies of multilingual vocabulary construction such as joint and cluster-based training.Show that extremely low-resource languages can benefit from tokenizers trained on related high-resource languages.", "conclusion": "Extremely low-resource languages can benefit from tokenizers trained on related high-resource languages. The study provides practical insights for building more fair, efficient, and linguistically informed tokenizers for multilingual NLP."}}
{"id": "2506.18187", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.18187", "abs": "https://arxiv.org/abs/2506.18187", "authors": ["Shahriar Noroozizadeh", "Pim Welle", "Jeremy C. Weiss", "George H. Chen"], "title": "The Impact of Medication Non-adherence on Adverse Outcomes: Evidence from Schizophrenia Patients via Survival Analysis", "comment": "Conference on Health, Inference, and Learning (CHIL 2025)", "summary": "This study quantifies the association between non-adherence to antipsychotic\nmedications and adverse outcomes in individuals with schizophrenia. We frame\nthe problem using survival analysis, focusing on the time to the earliest of\nseveral adverse events (early death, involuntary hospitalization, jail\nbooking). We extend standard causal inference methods (T-learner, S-learner,\nnearest neighbor matching) to utilize various survival models to estimate\nindividual and average treatment effects, where treatment corresponds to\nmedication non-adherence. Analyses are repeated using different amounts of\nlongitudinal information (3, 6, 9, and 12 months). Using data from Allegheny\nCounty in western Pennsylvania, we find strong evidence that non-adherence\nadvances adverse outcomes by approximately 1 to 4 months. Ablation studies\nconfirm that county-provided risk scores adjust for key confounders, as their\nremoval amplifies the estimated effects. Subgroup analyses by medication\nformulation (injectable vs. oral) and medication type consistently show that\nnon-adherence is associated with earlier adverse events. These findings\nhighlight the clinical importance of adherence in delaying psychiatric crises\nand show that integrating survival analysis with causal inference tools can\nyield policy-relevant insights. We caution that although we apply causal\ninference, we only make associative claims and discuss assumptions needed for\ncausal interpretation.", "AI": {"tldr": "Non-adherence to antipsychotics leads to earlier adverse outcomes (death, hospitalization, jail) in schizophrenia patients by 1-4 months.", "motivation": "Quantify the association between non-adherence to antipsychotic medications and adverse outcomes in individuals with schizophrenia.", "method": "Survival analysis and causal inference methods (T-learner, S-learner, nearest neighbor matching) are used to estimate individual and average treatment effects.", "result": "Non-adherence advances adverse outcomes by approximately 1 to 4 months. County-provided risk scores adjust for key confounders. Non-adherence is associated with earlier adverse events across medication formulations and types.", "conclusion": "Non-adherence to antipsychotic medications is associated with earlier adverse outcomes in individuals with schizophrenia. Integrating survival analysis with causal inference tools can yield policy-relevant insights."}}
{"id": "2506.17518", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.17518", "abs": "https://arxiv.org/abs/2506.17518", "authors": ["Ayoub Echchahed", "Pablo Samuel Castro"], "title": "A Survey of State Representation Learning for Deep Reinforcement Learning", "comment": null, "summary": "Representation learning methods are an important tool for addressing the\nchallenges posed by complex observations spaces in sequential decision making\nproblems. Recently, many methods have used a wide variety of types of\napproaches for learning meaningful state representations in reinforcement\nlearning, allowing better sample efficiency, generalization, and performance.\nThis survey aims to provide a broad categorization of these methods within a\nmodel-free online setting, exploring how they tackle the learning of state\nrepresentations differently. We categorize the methods into six main classes,\ndetailing their mechanisms, benefits, and limitations. Through this taxonomy,\nour aim is to enhance the understanding of this field and provide a guide for\nnew researchers. We also discuss techniques for assessing the quality of\nrepresentations, and detail relevant future directions.", "AI": {"tldr": "Survey on representation learning methods in reinforcement learning, categorizing methods and discussing their benefits, limitations, and future directions.", "motivation": "Addressing challenges posed by complex observation spaces in sequential decision making problems using representation learning.", "method": "Categorization of representation learning methods within a model-free online setting.", "result": "Categorization of methods into six main classes, detailing their mechanisms, benefits, and limitations. Discussion of techniques for assessing the quality of representations, and detailing relevant future directions.", "conclusion": "This survey enhances the understanding of representation learning in reinforcement learning and provides a guide for new researchers."}}
{"id": "2506.17664", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17664", "abs": "https://arxiv.org/abs/2506.17664", "authors": ["Shuaiye Lu", "Linjiang Zhou", "Xiaochuan Shi"], "title": "MDSAM:Memory-Driven Sparse Attention Matrix for LVLMs Hallucination Mitigation", "comment": null, "summary": "Hallucinations in large vision-language models (LVLMs) often stem from the\nmodel's sensitivity to image tokens during decoding, as evidenced by attention\npeaks observed when generating both real and hallucinated entities. To address\nthis, we propose Memory-Driven Sparse Attention Matrix (MDSAM) , a novel\ntraining-free approach that dynamically captures and refines the attention\nallocated to image tokens at each layer. MDSAM memorizes attention patterns and\nactivates updates through alignment during decoding, enhancing focus on\nrelevant image tokens while effectively reducing hallucinations. We evaluate\nMDSAM on multiple benchmarks for tasks such as image captioning and visual\nquestion answering, demonstrating its ability to consistently reduce\nhallucinations and improve reliability. Compatible with various LVLM\narchitectures, MDSAM highlights its adaptability and effectiveness in\nmitigating hallucinations without requiring additional training or external\ntools.", "AI": {"tldr": "MDSAM \u662f\u4e00\u79cd\u514d\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6355\u83b7\u548c\u7ec6\u5316\u56fe\u50cf tokens \u7684\u6ce8\u610f\u529b\u6765\u51cf\u5c11 LVLM \u4e2d\u7684\u5e7b\u89c9\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u4e2d\u7684\u5e7b\u89c9\u901a\u5e38\u6e90\u4e8e\u6a21\u578b\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u5bf9\u56fe\u50cf tokens \u7684\u654f\u611f\u6027\uff0c\u6b63\u5982\u5728\u751f\u6210\u771f\u5b9e\u548c\u5e7b\u89c9\u5b9e\u4f53\u65f6\u89c2\u5bdf\u5230\u7684\u6ce8\u610f\u529b\u5cf0\u503c\u6240\u8bc1\u660e\u7684\u90a3\u6837\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Memory-Driven Sparse Attention Matrix (MDSAM) \u7684\u65b0\u9896\u7684\u514d\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u52a8\u6001\u6355\u83b7\u548c\u7ec6\u5316\u5206\u914d\u7ed9\u6bcf\u4e00\u5c42\u56fe\u50cf tokens \u7684\u6ce8\u610f\u529b\u3002", "result": "\u5728\u56fe\u50cf\u5b57\u5e55\u548c\u89c6\u89c9\u95ee\u7b54\u7b49\u4efb\u52a1\u7684\u591a\u4e2a\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e86 MDSAM\uff0c\u8bc1\u660e\u4e86\u5176\u51cf\u5c11\u5e7b\u89c9\u548c\u63d0\u9ad8\u53ef\u9760\u6027\u7684\u80fd\u529b\u3002", "conclusion": "MDSAM \u80fd\u591f\u6301\u7eed\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u9ad8\u53ef\u9760\u6027\uff0c\u4e14\u4e0e\u5404\u79cd LVLM \u67b6\u6784\u517c\u5bb9\uff0c\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5916\u90e8\u5de5\u5177\u7684\u60c5\u51b5\u4e0b\u51cf\u8f7b\u5e7b\u89c9\u3002"}}
{"id": "2506.17844", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17844", "abs": "https://arxiv.org/abs/2506.17844", "authors": ["Xin Zhang", "Qiyu Wei", "Yingjie Zhu", "Fanyi Wu", "Sophia Ananiadou"], "title": "THCM-CAL: Temporal-Hierarchical Causal Modelling with Conformal Calibration for Clinical Risk Prediction", "comment": "13 pages, 4 figures", "summary": "Automated clinical risk prediction from electronic health records (EHRs)\ndemands modeling both structured diagnostic codes and unstructured narrative\nnotes. However, most prior approaches either handle these modalities separately\nor rely on simplistic fusion strategies that ignore the directional,\nhierarchical causal interactions by which narrative observations precipitate\ndiagnoses and propagate risk across admissions. In this paper, we propose\nTHCM-CAL, a Temporal-Hierarchical Causal Model with Conformal Calibration. Our\nframework constructs a multimodal causal graph where nodes represent clinical\nentities from two modalities: Textual propositions extracted from notes and ICD\ncodes mapped to textual descriptions. Through hierarchical causal discovery,\nTHCM-CAL infers three clinically grounded interactions: intra-slice\nsame-modality sequencing, intra-slice cross-modality triggers, and inter-slice\nrisk propagation. To enhance prediction reliability, we extend conformal\nprediction to multi-label ICD coding, calibrating per-code confidence intervals\nunder complex co-occurrences. Experimental results on MIMIC-III and MIMIC-IV\ndemonstrate the superiority of THCM-CAL.", "AI": {"tldr": "THCM-CAL\u662f\u4e00\u79cd\u7528\u4e8e\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55(EHRs)\u7684\u81ea\u52a8\u4e34\u5e8a\u98ce\u9669\u9884\u6d4b\u6846\u67b6\uff0c\u5b83\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u56e0\u679c\u56fe\uff0c\u901a\u8fc7\u5206\u5c42\u56e0\u679c\u53d1\u73b0\uff0c\u63a8\u65ad\u51fa\u4e34\u5e8a\u4e0a\u5408\u7406\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u901a\u8fc7\u5171\u5f62\u9884\u6d4b\u589e\u5f3a\u9884\u6d4b\u53ef\u9760\u6027\u3002", "motivation": "\u4ee5\u5f80\u65b9\u6cd5\u8981\u4e48\u5355\u72ec\u5904\u7406\u7ed3\u6784\u5316\u8bca\u65ad\u4ee3\u7801\u548c\u975e\u7ed3\u6784\u5316\u53d9\u8ff0\u6027\u6ce8\u91ca\uff0c\u8981\u4e48\u4f9d\u8d56\u4e8e\u7b80\u5355\u7684\u878d\u5408\u7b56\u7565\uff0c\u5ffd\u7565\u4e86\u53d9\u8ff0\u6027\u89c2\u5bdf\u5f15\u53d1\u8bca\u65ad\u5e76\u5728\u5165\u9662\u671f\u95f4\u4f20\u64ad\u98ce\u9669\u7684\u65b9\u5411\u6027\u3001\u5206\u5c42\u56e0\u679c\u76f8\u4e92\u4f5c\u7528\u3002", "method": "Temporal-Hierarchical Causal Model with Conformal Calibration (THCM-CAL)", "result": "THCM-CAL \u63a8\u65ad\u51fa\u4e09\u4e2a\u4e34\u5e8a\u4e0a\u5408\u7406\u7684\u76f8\u4e92\u4f5c\u7528\uff1a\u7247\u5185\u540c\u6a21\u6001\u6392\u5e8f\u3001\u7247\u5185\u8de8\u6a21\u6001\u89e6\u53d1\u548c\u7247\u95f4\u98ce\u9669\u4f20\u64ad\u3002", "conclusion": "THCM-CAL\u5728MIMIC-III\u548cMIMIC-IV\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2506.18213", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18213", "abs": "https://arxiv.org/abs/2506.18213", "authors": ["Mar\u00eda Victoria Carro", "Denise Alejandra Mester", "Francisca Gauna Selasco", "Luca Nicol\u00e1s Forziati Gangi", "Matheo Sandleris Musa", "Lola Ramos Pereyra", "Mario Leiva", "Juan Gustavo Corvalan", "Mar\u00eda Vanina Martinez", "Gerardo Simari"], "title": "A Conceptual Framework for AI Capability Evaluations", "comment": "arXiv admin note: text overlap with arXiv:2306.04181 by other authors", "summary": "As AI systems advance and integrate into society, well-designed and\ntransparent evaluations are becoming essential tools in AI governance,\ninforming decisions by providing evidence about system capabilities and risks.\nYet there remains a lack of clarity on how to perform these assessments both\ncomprehensively and reliably. To address this gap, we propose a conceptual\nframework for analyzing AI capability evaluations, offering a structured,\ndescriptive approach that systematizes the analysis of widely used methods and\nterminology without imposing new taxonomies or rigid formats. This framework\nsupports transparency, comparability, and interpretability across diverse\nevaluations. It also enables researchers to identify methodological weaknesses,\nassists practitioners in designing evaluations, and provides policymakers with\nan accessible tool to scrutinize, compare, and navigate complex evaluation\nlandscapes.", "AI": {"tldr": "Proposes a framework for analyzing AI capability evaluations to improve transparency and comparability.", "motivation": "Well-designed and transparent evaluations are becoming essential tools in AI governance. Yet there remains a lack of clarity on how to perform these assessments both comprehensively and reliably.", "method": "A structured, descriptive approach that systematizes the analysis of widely used methods and terminology without imposing new taxonomies or rigid formats.", "result": "The framework supports transparency, comparability, and interpretability across diverse evaluations. It also enables researchers to identify methodological weaknesses, assists practitioners in designing evaluations, and provides policymakers with an accessible tool to scrutinize, compare, and navigate complex evaluation landscapes.", "conclusion": "This paper proposes a conceptual framework for analyzing AI capability evaluations to support transparency, comparability, and interpretability across diverse evaluations."}}
{"id": "2506.17543", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17543", "abs": "https://arxiv.org/abs/2506.17543", "authors": ["Aditi Madhusudan Jain"], "title": "Predicting E-commerce Purchase Behavior using a DQN-Inspired Deep Learning Model for enhanced adaptability", "comment": null, "summary": "This paper presents a novel approach to predicting buying intent and product\ndemand in e-commerce settings, leveraging a Deep Q-Network (DQN) inspired\narchitecture. In the rapidly evolving landscape of online retail, accurate\nprediction of user behavior is crucial for optimizing inventory management,\npersonalizing user experiences, and maximizing sales. Our method adapts\nconcepts from reinforcement learning to a supervised learning context,\ncombining the sequential modeling capabilities of Long Short-Term Memory (LSTM)\nnetworks with the strategic decision-making aspects of DQNs. We evaluate our\nmodel on a large-scale e-commerce dataset comprising over 885,000 user\nsessions, each characterized by 1,114 features. Our approach demonstrates\nrobust performance in handling the inherent class imbalance typical in\ne-commerce data, where purchase events are significantly less frequent than\nnon-purchase events. Through comprehensive experimentation with various\nclassification thresholds, we show that our model achieves a balance between\nprecision and recall, with an overall accuracy of 88\\% and an AUC-ROC score of\n0.88. Comparative analysis reveals that our DQN-inspired model offers\nadvantages over traditional machine learning and standard deep learning\napproaches, particularly in its ability to capture complex temporal patterns in\nuser behavior. The model's performance and scalability make it well-suited for\nreal-world e-commerce applications dealing with high-dimensional, sequential\ndata. This research contributes to the field of e-commerce analytics by\nintroducing a novel predictive modeling technique that combines the strengths\nof deep learning and reinforcement learning paradigms. Our findings have\nsignificant implications for improving demand forecasting, personalizing user\nexperiences, and optimizing marketing strategies in online retail environments.", "AI": {"tldr": "This paper presents a DQN-inspired model for predicting buying intent in e-commerce, achieving high accuracy and outperforming traditional methods.", "motivation": "Accurate prediction of user behavior is crucial for optimizing inventory management, personalizing user experiences, and maximizing sales in online retail.", "method": "The paper adapts a Deep Q-Network (DQN) inspired architecture, combining LSTM networks with DQN strategic decision-making, in a supervised learning context.", "result": "The model achieves 88% accuracy and an AUC-ROC score of 0.88 on a large-scale e-commerce dataset, demonstrating robust performance and advantages over traditional methods.", "conclusion": "The paper introduces a DQN-inspired model that outperforms traditional methods in predicting buying intent, offering improvements for demand forecasting and personalized marketing in e-commerce."}}
{"id": "2506.17679", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17679", "abs": "https://arxiv.org/abs/2506.17679", "authors": ["Wei Haolin"], "title": "CSDN: A Context-Gated Self-Adaptive Detection Network for Real-Time Object Detection", "comment": "15pages, 11figures", "summary": "Convolutional neural networks (CNNs) have long been the cornerstone of target\ndetection, but they are often limited by limited receptive fields, which\nhinders their ability to capture global contextual information. This paper\nbelieves that the effective utilization of extracted features is as important\nas the feature extraction process itself. We critically re-evaluated the\nDETR-inspired header network architecture, questioning the indispensable nature\nof its self-attention mechanism, and discovering significant information\nredundancies. To solve these problems, we introduced the Context-Gated\nScale-Adaptive Detection Network (CSDN), a Transformer-based detection header\ninspired by natural language processing architecture and human visual\nperception. CSDN aims to efficiently utilize the characteristics of the CNN\nbackbone network by replacing the traditional stacked self-attention and\ncross-attention layers with a novel gating mechanism. This mechanism enables\neach region of interest (ROI) to adaptively select and combine feature\ndimensions and scale information from multiple attention patterns. CSDN\nprovides more powerful global context modeling capabilities and can better\nadapt to objects of different sizes and structures. Our proposed detection head\ncan directly replace the native heads of various CNN-based detectors, and only\na few rounds of fine-tuning on the pre-training weights can significantly\nimprove the detection accuracy, thus avoiding the need to achieve small\nimprovements. Various layer modules undergo extensive re-training.", "AI": {"tldr": "This paper introduces CSDN, a novel Transformer-based detection header that replaces traditional self-attention mechanisms with a gating mechanism to improve detection accuracy and global context modeling.", "motivation": "Convolutional neural networks (CNNs) are often limited by limited receptive fields, which hinders their ability to capture global contextual information and significant information redundancies in DETR-inspired header network architecture.", "method": "The Context-Gated Scale-Adaptive Detection Network (CSDN), a Transformer-based detection header inspired by natural language processing architecture and human visual perception, replacing the traditional stacked self-attention and cross-attention layers with a novel gating mechanism.", "result": "CSDN provides more powerful global context modeling capabilities and can better adapt to objects of different sizes and structures.", "conclusion": "The proposed CSDN detection head can directly replace the native heads of various CNN-based detectors, and only a few rounds of fine-tuning on the pre-training weights can significantly improve the detection accuracy."}}
{"id": "2506.17863", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.17863", "abs": "https://arxiv.org/abs/2506.17863", "authors": ["Haoran Liu", "Amir Tahmasbi", "Ehtesham Sam Haque", "Purak Jain"], "title": "LLMs for Customized Marketing Content Generation and Evaluation at Scale", "comment": "KDD LLM4ECommerce Workshop 2025", "summary": "Offsite marketing is essential in e-commerce, enabling businesses to reach\ncustomers through external platforms and drive traffic to retail websites.\nHowever, most current offsite marketing content is overly generic,\ntemplate-based, and poorly aligned with landing pages, limiting its\neffectiveness. To address these limitations, we propose MarketingFM, a\nretrieval-augmented system that integrates multiple data sources to generate\nkeyword-specific ad copy with minimal human intervention. We validate\nMarketingFM via offline human and automated evaluations and large-scale online\nA/B tests. In one experiment, keyword-focused ad copy outperformed templates,\nachieving up to 9% higher CTR, 12% more impressions, and 0.38% lower CPC,\ndemonstrating gains in ad ranking and cost efficiency. Despite these gains,\nhuman review of generated ads remains costly. To address this, we propose\nAutoEval-Main, an automated evaluation system that combines rule-based metrics\nwith LLM-as-a-Judge techniques to ensure alignment with marketing principles.\nIn experiments with large-scale human annotations, AutoEval-Main achieved\n89.57% agreement with human reviewers. Building on this, we propose\nAutoEval-Update, a cost-efficient LLM-human collaborative framework to\ndynamically refine evaluation prompts and adapt to shifting criteria with\nminimal human input. By selectively sampling representative ads for human\nreview and using a critic LLM to generate alignment reports, AutoEval-Update\nimproves evaluation consistency while reducing manual effort. Experiments show\nthe critic LLM suggests meaningful refinements, improving LLM-human agreement.\nNonetheless, human oversight remains essential for setting thresholds and\nvalidating refinements before deployment.", "AI": {"tldr": "This paper introduces MarketingFM, AutoEval-Main, and AutoEval-Update to generate keyword-specific ad copy, automate ad evaluation, and refine evaluation prompts, achieving improved ad performance and evaluation consistency.", "motivation": "most current offsite marketing content is overly generic, template-based, and poorly aligned with landing pages, limiting its effectiveness", "method": "MarketingFM, a retrieval-augmented system that integrates multiple data sources to generate keyword-specific ad copy with minimal human intervention; AutoEval-Main, an automated evaluation system that combines rule-based metrics with LLM-as-a-Judge techniques; AutoEval-Update, a cost-efficient LLM-human collaborative framework to dynamically refine evaluation prompts", "result": "keyword-focused ad copy outperformed templates, achieving up to 9% higher CTR, 12% more impressions, and 0.38% lower CPC; AutoEval-Main achieved 89.57% agreement with human reviewers; the critic LLM suggests meaningful refinements, improving LLM-human agreement", "conclusion": "human oversight remains essential for setting thresholds and validating refinements before deployment"}}
{"id": "2506.18233", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18233", "abs": "https://arxiv.org/abs/2506.18233", "authors": ["Ruike Zhu", "Hanwen Zhang", "Tianyu Shi", "Chi Wang", "Tianyi Zhou", "Zengyi Qin"], "title": "The 4th Dimension for Scaling Model Size", "comment": null, "summary": "Scaling the size of large language models typically involves three\ndimensions: depth, width, and the number of parameters. In this work, we\nexplore a fourth dimension, virtual logical depth (VLD), which increases the\neffective algorithmic depth without changing the overall parameter count by\nreusing parameters within the model. Although parameter reuse is not a new\nconcept, its potential and characteristics in model scaling have not been\nthoroughly studied. Through carefully designed controlled experiments, we make\nthe following key discoveries regarding VLD scaling:\n  VLD scaling forces the knowledge capacity of the model to remain almost\nconstant, with only minor variations.\n  VLD scaling enables a significant improvement in reasoning capability,\nprovided the scaling method is properly implemented.\n  The number of parameters correlates with knowledge capacity, but not with\nreasoning capability. Under certain conditions, it is not necessary to increase\nthe parameter count to enhance reasoning.\n  These findings are consistent across various model configurations and are\nlikely to be generally valid within the scope of our experiments.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u6269\u5c55\u7ef4\u5ea6\uff1a\u865a\u62df\u903b\u8f91\u6df1\u5ea6 (VLD)\uff0c\u5b83\u901a\u8fc7\u91cd\u7528\u53c2\u6570\u6765\u63d0\u9ad8\u63a8\u7406\u80fd\u529b\uff0c\u800c\u65e0\u9700\u589e\u52a0\u53c2\u6570\u6570\u91cf\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u7684\u65b0\u7ef4\u5ea6\uff1a\u865a\u62df\u903b\u8f91\u6df1\u5ea6 (VLD)\uff0c\u5b83\u901a\u8fc7\u91cd\u7528\u6a21\u578b\u4e2d\u7684\u53c2\u6570\u6765\u589e\u52a0\u6709\u6548\u7b97\u6cd5\u6df1\u5ea6\uff0c\u800c\u4e0d\u6539\u53d8\u603b\u4f53\u53c2\u6570\u8ba1\u6570\u3002\u53c2\u6570\u91cd\u7528\u4e0d\u662f\u4e00\u4e2a\u65b0\u6982\u5ff5\uff0c\u4f46\u5176\u5728\u6a21\u578b\u7f29\u653e\u4e2d\u7684\u6f5c\u529b\u548c\u7279\u6027\u5c1a\u672a\u5f97\u5230\u5f7b\u5e95\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u53d7\u63a7\u5b9e\u9a8c", "result": "VLD \u7f29\u653e\u8feb\u4f7f\u6a21\u578b\u7684\u77e5\u8bc6\u5bb9\u91cf\u4fdd\u6301\u51e0\u4e4e\u6052\u5b9a\uff0c\u53ea\u6709\u5fae\u5c0f\u7684\u53d8\u5316\u3002VLD \u7f29\u653e\u80fd\u591f\u663e\u7740\u63d0\u9ad8\u63a8\u7406\u80fd\u529b\uff0c\u524d\u63d0\u662f\u6b63\u786e\u5b9e\u65bd\u7f29\u653e\u65b9\u6cd5\u3002\u53c2\u6570\u7684\u6570\u91cf\u4e0e\u77e5\u8bc6\u5bb9\u91cf\u76f8\u5173\uff0c\u4f46\u4e0e\u63a8\u7406\u80fd\u529b\u65e0\u5173\u3002\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u4e0d\u9700\u8981\u589e\u52a0\u53c2\u6570\u6570\u91cf\u6765\u589e\u5f3a\u63a8\u7406\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e0e\u5404\u79cd\u6a21\u578b\u914d\u7f6e\u4e00\u81f4\uff0c\u5e76\u4e14\u53ef\u80fd\u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u8303\u56f4\u5185\u666e\u904d\u6709\u6548\u3002", "conclusion": "VLD scaling \u53ef\u4ee5\u5728\u4e0d\u589e\u52a0\u53c2\u6570\u6570\u91cf\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u80fd\u529b\u3002\u53c2\u6570\u7684\u6570\u91cf\u4e0e\u77e5\u8bc6\u5bb9\u91cf\u76f8\u5173\uff0c\u4f46\u4e0e\u63a8\u7406\u80fd\u529b\u65e0\u5173\u3002\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u4e0d\u9700\u8981\u589e\u52a0\u53c2\u6570\u6570\u91cf\u6765\u589e\u5f3a\u63a8\u7406\u3002"}}
{"id": "2506.17552", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17552", "abs": "https://arxiv.org/abs/2506.17552", "authors": ["Wei Zhang", "Zi Wang", "Hanwen Zhou", "Zhaohong Deng", "Weiping Ding", "Yuxi Ge", "Te Zhang", "Yuanpeng Zhang", "Kup-Sze Choi", "Shitong Wang", "Shudong Hu"], "title": "DRIMV_TSK: An Interpretable Surgical Evaluation Model for Incomplete Multi-View Rectal Cancer Data", "comment": null, "summary": "A reliable evaluation of surgical difficulty can improve the success of the\ntreatment for rectal cancer and the current evaluation method is based on\nclinical data. However, more data about rectal cancer can be collected with the\ndevelopment of technology. Meanwhile, with the development of artificial\nintelligence, its application in rectal cancer treatment is becoming possible.\nIn this paper, a multi-view rectal cancer dataset is first constructed to give\na more comprehensive view of patients, including the high-resolution MRI image\nview, pressed-fat MRI image view, and clinical data view. Then, an\ninterpretable incomplete multi-view surgical evaluation model is proposed,\nconsidering that it is hard to obtain extensive and complete patient data in\nreal application scenarios. Specifically, a dual representation incomplete\nmulti-view learning model is first proposed to extract the common information\nbetween views and specific information in each view. In this model, the missing\nview imputation is integrated into representation learning, and second-order\nsimilarity constraint is also introduced to improve the cooperative learning\nbetween these two parts. Then, based on the imputed multi-view data and the\nlearned dual representation, a multi-view surgical evaluation model with the\nTSK fuzzy system is proposed. In the proposed model, a cooperative learning\nmechanism is constructed to explore the consistent information between views,\nand Shannon entropy is also introduced to adapt the view weight. On the MVRC\ndataset, we compared it with several advanced algorithms and DRIMV_TSK obtained\nthe best results.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u89c6\u56fe\u76f4\u80a0\u764c\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u624b\u672f\u8bc4\u4f30\u6a21\u578b DRIMV_TSK\uff0c\u8be5\u6a21\u578b\u5728\u4e0d\u5b8c\u5168\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u53ef\u9760\u7684\u624b\u672f\u96be\u5ea6\u8bc4\u4f30\u53ef\u4ee5\u63d0\u9ad8\u76f4\u80a0\u764c\u6cbb\u7597\u7684\u6210\u529f\u7387\uff0c\u4f46\u5f53\u524d\u7684\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4e34\u5e8a\u6570\u636e\uff0c\u800c\u968f\u7740\u6280\u672f\u548c\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u53ef\u4ee5\u6536\u96c6\u5230\u66f4\u591a\u5173\u4e8e\u76f4\u80a0\u764c\u7684\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u4e0d\u5b8c\u5168\u591a\u89c6\u56fe\u624b\u672f\u8bc4\u4f30\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u96c6\u6210\u4e86\u7f3a\u5931\u89c6\u56fe\u63d2\u8865\u548c\u4e8c\u9636\u76f8\u4f3c\u6027\u7ea6\u675f\uff0c\u5e76\u5229\u7528 TSK \u6a21\u7cca\u7cfb\u7edf\u548c\u534f\u540c\u5b66\u4e60\u673a\u5236\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u89c6\u56fe\u76f4\u80a0\u764c\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86 DRIMV_TSK \u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4f18\u4e8e\u5176\u4ed6\u5148\u8fdb\u7b97\u6cd5\u3002", "conclusion": "DRIMV_TSK \u7b97\u6cd5\u5728 MVRC \u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u597d\u3002"}}
{"id": "2506.17685", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17685", "abs": "https://arxiv.org/abs/2506.17685", "authors": ["Amirshayan Nasirimajd", "Chiara Plizzari", "Simone Alberto Peirone", "Marco Ciccone", "Giuseppe Averta", "Barbara Caputo"], "title": "Domain Generalization using Action Sequences for Egocentric Action Recognition", "comment": "Accepted at Pattern Recognition Letters. 9 pages including\n  references. Code and Data: https://github.com/Ashayan97/SeqDG", "summary": "Recognizing human activities from visual inputs, particularly through a\nfirst-person viewpoint, is essential for enabling robots to replicate human\nbehavior. Egocentric vision, characterized by cameras worn by observers,\ncaptures diverse changes in illumination, viewpoint, and environment. This\nvariability leads to a notable drop in the performance of Egocentric Action\nRecognition models when tested in environments not seen during training. In\nthis paper, we tackle these challenges by proposing a domain generalization\napproach for Egocentric Action Recognition. Our insight is that action\nsequences often reflect consistent user intent across visual domains. By\nleveraging action sequences, we aim to enhance the model's generalization\nability across unseen environments. Our proposed method, named SeqDG,\nintroduces a visual-text sequence reconstruction objective (SeqRec) that uses\ncontextual cues from both text and visual inputs to reconstruct the central\naction of the sequence. Additionally, we enhance the model's robustness by\ntraining it on mixed sequences of actions from different domains (SeqMix). We\nvalidate SeqDG on the EGTEA and EPIC-KITCHENS-100 datasets. Results on\nEPIC-KITCHENS-100, show that SeqDG leads to +2.4% relative average improvement\nin cross-domain action recognition in unseen environments, and on EGTEA the\nmodel achieved +0.6% Top-1 accuracy over SOTA in intra-domain action\nrecognition.", "AI": {"tldr": "\u63d0\u51faSeqDG\u7528\u4e8e\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u52a8\u4f5c\u8bc6\u522b\uff0c\u901a\u8fc7\u5e8f\u5217\u91cd\u6784\u548c\u6df7\u5408\u8bad\u7ec3\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eSeqDG\u5728\u8de8\u57df\u548c\u57df\u5185\u52a8\u4f5c\u8bc6\u522b\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002", "motivation": "\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u7684\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u5bf9\u4e8e\u673a\u5668\u4eba\u590d\u5236\u4eba\u7c7b\u884c\u4e3a\u81f3\u5173\u91cd\u8981\u3002\u5f53\u5728\u8bad\u7ec3\u671f\u95f4\u672a\u89c1\u8fc7\u7684\u73af\u5883\u4e2d\u8fdb\u884c\u6d4b\u8bd5\u65f6\uff0c\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u52a8\u4f5c\u8bc6\u522b\u6a21\u578b\u7684\u6027\u80fd\u4f1a\u663e\u7740\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSeqDG\u7684\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u5f15\u5165\u4e86\u89c6\u89c9-\u6587\u672c\u5e8f\u5217\u91cd\u6784\u76ee\u6807(SeqRec)\uff0c\u5e76\u4f7f\u7528\u6765\u81ea\u4e0d\u540c\u9886\u57df\u7684\u6df7\u5408\u52a8\u4f5c\u5e8f\u5217\u8fdb\u884c\u8bad\u7ec3(SeqMix)\u3002", "result": "\u5728EPIC-KITCHENS-100\u6570\u636e\u96c6\u4e0a\uff0cSeqDG\u5728\u672a\u89c1\u73af\u5883\u4e2d\u7684\u8de8\u57df\u52a8\u4f5c\u8bc6\u522b\u65b9\u9762\u5b9e\u73b0\u4e86+2.4%\u7684\u76f8\u5bf9\u5e73\u5747\u6539\u8fdb\uff1b\u5728EGTEA\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6a21\u578b\u5728\u57df\u5185\u52a8\u4f5c\u8bc6\u522b\u65b9\u9762\u6bd4SOTA\u63d0\u9ad8\u4e86+0.6%\u7684Top-1\u51c6\u786e\u7387\u3002", "conclusion": "SeqDG\u5728\u8de8\u57df\u52a8\u4f5c\u8bc6\u522b\u65b9\u9762\u6709\u63d0\u5347\uff0c\u5728\u57df\u5185\u52a8\u4f5c\u8bc6\u522b\u65b9\u9762\u8fbe\u5230\u4e86\u65b0\u7684SOTA\u3002"}}
{"id": "2506.17864", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.17864", "abs": "https://arxiv.org/abs/2506.17864", "authors": ["Taolin Zhang", "Haidong Kang", "Dongyang Li", "Qizhou Chen", "Chengyu Wang Xiaofeng He", "Richang Hong"], "title": "QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs", "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated impressive results\nbut still suffer from hallucinations. Model editing has been proposed to\ncorrect factual inaccuracies in LLMs. A challenging case is sequential model\nediting (SME), which aims to rectify errors continuously rather than treating\nthem as a one-time task. During SME, the general capabilities of LLMs can be\nnegatively affected due to the introduction of new parameters. In this paper,\nwe propose a queue-based self-correction framework (QueueEDIT) that not only\nenhances SME performance by addressing long-sequence dependency but also\nmitigates the impact of parameter bias on the general capabilities of LLMs.\nSpecifically, we first introduce a structural mapping editing loss to map the\ntriplets to the knowledge-sensitive neurons within the Transformer layers of\nLLMs. We then store the located parameters for each piece of edited knowledge\nin a queue and dynamically align previously edited parameters. In each edit, we\nselect queue parameters most relevant to the currently located parameters to\ndetermine whether previous knowledge needs realignment. Irrelevant parameters\nin the queue are frozen, and we update the parameters at the queue head to the\nLLM to ensure they do not harm general abilities. Experiments show that our\nframework significantly outperforms strong baselines across various SME\nsettings and maintains competitiveness in single-turn editing. The resulting\nLLMs also preserve high capabilities in general NLP tasks throughout the SME\nprocess.", "AI": {"tldr": "QueueEDIT improves sequential model editing (SME) for LLMs by using a queue to manage parameter updates, enhancing performance and preserving general capabilities.", "motivation": "Large language models (LLMs) suffer from hallucinations, and sequential model editing (SME) can negatively affect the general capabilities of LLMs due to the introduction of new parameters.", "method": "A queue-based self-correction framework (QueueEDIT) is proposed, which introduces a structural mapping editing loss to map triplets to knowledge-sensitive neurons and dynamically aligns previously edited parameters using a queue.", "result": "The QueueEDIT framework enhances SME performance by addressing long-sequence dependency and mitigates the impact of parameter bias on the general capabilities of LLMs.", "conclusion": "The proposed QueueEDIT framework significantly outperforms strong baselines across various SME settings, maintains competitiveness in single-turn editing, and preserves high capabilities in general NLP tasks throughout the SME process."}}
{"id": "2506.18260", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18260", "abs": "https://arxiv.org/abs/2506.18260", "authors": ["FuTe Wong"], "title": "Advanced For-Loop for QML algorithm search", "comment": "7 pages, 8 figures", "summary": "This paper introduces an advanced framework leveraging Large Language\nModel-based Multi-Agent Systems (LLMMA) for the automated search and\noptimization of Quantum Machine Learning (QML) algorithms. Inspired by Google\nDeepMind's FunSearch, the proposed system works on abstract level to\niteratively generates and refines quantum transformations of classical machine\nlearning algorithms (concepts), such as the Multi-Layer Perceptron,\nforward-forward and backpropagation algorithms. As a proof of concept, this\nwork highlights the potential of agentic frameworks to systematically explore\nclassical machine learning concepts and adapt them for quantum computing,\npaving the way for efficient and automated development of QML algorithms.\nFuture directions include incorporating planning mechanisms and optimizing\nstrategy in the search space for broader applications in quantum-enhanced\nmachine learning.", "AI": {"tldr": "This paper uses LLMMA for automated search and optimization of QML algorithms, inspired by FunSearch. It explores and adapts classical ML concepts for quantum computing, paving the way for efficient QML algorithm development.", "motivation": "Inspired by Google DeepMind's FunSearch, the proposed system works on abstract level to iteratively generates and refines quantum transformations of classical machine learning algorithms (concepts), such as the Multi-Layer Perceptron, forward-forward and backpropagation algorithms.", "method": "This paper introduces an advanced framework leveraging Large Language Model-based Multi-Agent Systems (LLMMA) for the automated search and optimization of Quantum Machine Learning (QML) algorithms.", "result": "The proposed system works on abstract level to iteratively generates and refines quantum transformations of classical machine learning algorithms", "conclusion": "This work highlights the potential of agentic frameworks to systematically explore classical machine learning concepts and adapt them for quantum computing, paving the way for efficient and automated development of QML algorithms."}}
{"id": "2506.17564", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.17564", "abs": "https://arxiv.org/abs/2506.17564", "authors": ["Lakshita Dodeja", "Karl Schmeckpeper", "Shivam Vats", "Thomas Weng", "Mingxi Jia", "George Konidaris", "Stefanie Tellex"], "title": "Accelerating Residual Reinforcement Learning with Uncertainty Estimation", "comment": null, "summary": "Residual Reinforcement Learning (RL) is a popular approach for adapting\npretrained policies by learning a lightweight residual policy that provides\ncorrective actions. While Residual RL is more sample-efficient than finetuning\nthe entire base policy, existing methods struggle with sparse rewards and are\ndesigned for deterministic base policies. We propose two improvements to\nResidual RL that further enhance its sample efficiency and make it suitable for\nstochastic base policies. First, we leverage uncertainty estimates of the base\npolicy to focus exploration on regions in which the base policy is not\nconfident. Second, we propose a simple modification to off-policy residual\nlearning that allows it to observe base actions and better handle stochastic\nbase policies. We evaluate our method with both Gaussian-based and\nDiffusion-based stochastic base policies on tasks from Robosuite and D4RL, and\ncompare against state-of-the-art finetuning methods, demo-augmented RL methods,\nand other residual RL methods. Our algorithm significantly outperforms existing\nbaselines in a variety of simulation benchmark environments. We also deploy our\nlearned polices in the real world to demonstrate their robustness with\nzero-shot sim-to-real transfer.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u79bb\u7b56\u7565\u5b66\u4e60\u6765\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u5e76\u4f7f\u5176\u9002\u7528\u4e8e\u968f\u673a\u57fa\u7840\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u7684\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u7a00\u758f\u5956\u52b1\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5e76\u4e14\u662f\u4e3a\u786e\u5b9a\u6027\u57fa\u7840\u7b56\u7565\u8bbe\u8ba1\u7684\u3002", "method": "\u5229\u7528\u57fa\u7840\u7b56\u7565\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6765\u96c6\u4e2d\u63a2\u7d22\u57fa\u7840\u7b56\u7565\u4e0d\u81ea\u4fe1\u7684\u533a\u57df\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u79bb\u7b56\u7565\u6b8b\u5dee\u5b66\u4e60\u4fee\u6539\u65b9\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u89c2\u5bdf\u57fa\u7840\u52a8\u4f5c\u5e76\u66f4\u597d\u5730\u5904\u7406\u968f\u673a\u57fa\u7840\u7b56\u7565\u3002", "result": "\u8be5\u7b97\u6cd5\u5728\u5404\u79cd\u6a21\u62df\u57fa\u51c6\u73af\u5883\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u5404\u79cd\u6a21\u62df\u57fa\u51c6\u73af\u5883\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\uff0c\u5e76\u4e14\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u90e8\u7f72\u4e86\u5b66\u4e60\u5230\u7684\u7b56\u7565\uff0c\u4ee5\u8bc1\u660e\u5b83\u4eec\u5177\u6709\u96f6\u6837\u672c sim-to-real \u8fc1\u79fb\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.17694", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17694", "abs": "https://arxiv.org/abs/2506.17694", "authors": ["Gnana Praveen Rajasekhar", "Jahangir Alam"], "title": "SSAVSV: Towards Unified Model for Self-Supervised Audio-Visual Speaker Verification", "comment": null, "summary": "Conventional audio-visual methods for speaker verification rely on large\namounts of labeled data and separate modality-specific architectures, which is\ncomputationally expensive, limiting their scalability. To address these\nproblems, we propose a self-supervised learning framework based on contrastive\nlearning with asymmetric masking and masked data modeling to obtain robust\naudiovisual feature representations. In particular, we employ a unified\nframework for self-supervised audiovisual speaker verification using a single\nshared backbone for audio and visual inputs, leveraging the versatility of\nvision transformers. The proposed unified framework can handle audio, visual,\nor audiovisual inputs using a single shared vision transformer backbone during\ntraining and testing while being computationally efficient and robust to\nmissing modalities. Extensive experiments demonstrate that our method achieves\ncompetitive performance without labeled data while reducing computational costs\ncompared to traditional approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u7edf\u4e00\u7684\u89c6\u89c9transformer backbone\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u7684\u89c6\u542c\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5927\u91cf\u7684\u6807\u7b7e\u6570\u636e\u548c\u5206\u79bb\u7684\u6a21\u6001\u7279\u5b9a\u67b6\u6784\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u975e\u5bf9\u79f0\u63a9\u853d\u548c\u63a9\u853d\u6570\u636e\u5efa\u6a21\uff0c\u4ee5\u83b7\u5f97\u7a33\u5065\u7684\u89c6\u542c\u7279\u5f81\u8868\u793a\u3002\u4f7f\u7528\u5355\u4e00\u5171\u4eabbackbone\uff0c\u5229\u7528\u89c6\u89c9transformer\u7684\u591a\u529f\u80fd\u6027\uff0c\u5b9e\u73b0\u7edf\u4e00\u7684\u81ea\u76d1\u7763\u89c6\u542c\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6ca1\u6709\u6807\u7b7e\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2506.17871", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17871", "abs": "https://arxiv.org/abs/2506.17871", "authors": ["Chenghao Yang", "Ari Holtzman"], "title": "How Alignment Shrinks the Generative Horizon", "comment": "Codebase: https://github.com/yangalan123/LLMBranchingFactor, Website:\n  https://yangalan123.github.io/branching_factor/", "summary": "Despite their impressive capabilities, aligned large language models (LLMs)\noften generate outputs that lack diversity. What drives this stability in the\ngeneration? We investigate this phenomenon through the lens of probability\nconcentration in the model's output distribution. To quantify this\nconcentration, we introduce the Branching Factor (BF) -- a token-invariant\nmeasure of the effective number of plausible next steps during generation. Our\nempirical analysis reveals two key findings: (1) BF often decreases as\ngeneration progresses, suggesting that LLMs become more predictable as they\ngenerate. (2) alignment tuning substantially sharpens the model's output\ndistribution from the outset, reducing BF by nearly an order of magnitude\n(e.g., from 12 to 1.2) relative to base models. This stark reduction helps\nexplain why aligned models often appear less sensitive to decoding strategies.\nBuilding on this insight, we find this stability has surprising implications\nfor complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g.,\nDeepSeek-distilled models), for instance, leverage this effect; by generating\nlonger reasoning chains, they push generation into later, more deterministic\n(lower BF) stages, resulting in more stable outputs. We hypothesize that\nalignment tuning does not fundamentally change a model's behavior, but instead\nsteers it toward stylistic tokens (e.g., \"Sure\") that unlock low-entropy\ntrajectories already present in the base model. This view is supported by\nnudging experiments, which show that prompting base models with such tokens can\nsimilarly reduce BF. Together, our findings establish BF as a powerful\ndiagnostic for understanding and controlling LLM outputs - clarifying how\nalignment reduces variability, how CoT promotes stable generations, and how\nbase models can be steered away from diversity.", "AI": {"tldr": "alignment reduces variability and CoT promotes stable generations", "motivation": "aligned large language models (LLMs) often generate outputs that lack diversity. What drives this stability in the generation?", "method": "introduce the Branching Factor (BF) -- a token-invariant measure of the effective number of plausible next steps during generation", "result": "BF often decreases as generation progresses, suggesting that LLMs become more predictable as they generate. alignment tuning substantially sharpens the model's output distribution from the outset, reducing BF by nearly an order of magnitude", "conclusion": "alignment reduces variability, how CoT promotes stable generations, and how base models can be steered away from diversity"}}
{"id": "2506.18348", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18348", "abs": "https://arxiv.org/abs/2506.18348", "authors": ["Weilun Yu", "Shixiang Tang", "Yonggui Huang", "Nanqing Dong", "Li Fan", "Honggang Qi", "Wei Liu", "Xiaoli Diao", "Xi Chen", "Wanli Ouyang"], "title": "Dynamic Knowledge Exchange and Dual-diversity Review: Concisely Unleashing the Potential of a Multi-Agent Research Team", "comment": null, "summary": "Scientific progress increasingly relies on effective collaboration among\nresearchers, a dynamic that large language models (LLMs) have only begun to\nemulate. While recent LLM-based scientist agents show promise in autonomous\nscientific discovery, they often lack the interactive reasoning and evaluation\nmechanisms essential to real-world research. We propose IDVSCI (Internal\nDiscussion and Vote SCIentists), a multi-agent framework built on LLMs that\nincorporates two key innovations: a Dynamic Knowledge Exchange mechanism\nenabling iterative feedback among agents, and a Dual-Diversity Review paradigm\nthat simulates heterogeneous expert evaluation. These components jointly\npromote deeper reasoning and the generation of more creative and impactful\nscientific ideas. To evaluate the effectiveness and generalizability of our\napproach, we conduct experiments on two datasets: a widely used benchmark in\ncomputer science and a new dataset we introduce in the health sciences domain.\nResults show that IDVSCI consistently achieves the best performance across both\ndatasets, outperforming existing systems such as AI Scientist and VIRSCI. These\nfindings highlight the value of modeling interaction and peer review dynamics\nin LLM-based autonomous research.", "AI": {"tldr": "IDVSCI\u662f\u4e00\u79cd\u57fa\u4e8ellm\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u52a8\u6001\u77e5\u8bc6\u4ea4\u6362\u673a\u5236\u548c\u53cc\u91cd\u591a\u6837\u6027\u5ba1\u67e5\u8303\u4f8b\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u6df1\u5165\u7684\u63a8\u7406\u548c\u4ea7\u751f\u66f4\u6709\u521b\u9020\u6027\u548c\u5f71\u54cd\u529b\u7684\u79d1\u5b66\u601d\u60f3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u5df2\u7ecf\u5f00\u59cb\u6a21\u4eff\u7814\u7a76\u4eba\u5458\u4e4b\u95f4\u6709\u6548\u7684\u534f\u4f5c\uff0c\u8fd9\u79cd\u534f\u4f5c\u8d8a\u6765\u8d8a\u4f9d\u8d56\u4e8e\u79d1\u5b66\u8fdb\u6b65\u3002\u867d\u7136\u6700\u8fd1\u57fa\u4e8ellm\u7684\u79d1\u5b66\u5bb6\u667a\u80fd\u4f53\u5728\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u65b9\u9762\u663e\u793a\u51fa\u4e86\u5e0c\u671b\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u7f3a\u4e4f\u73b0\u5b9e\u7814\u7a76\u4e2d\u5fc5\u4e0d\u53ef\u5c11\u7684\u4ea4\u4e92\u63a8\u7406\u548c\u8bc4\u4f30\u673a\u5236\u3002", "method": "IDVSCI (\u5185\u90e8\u8ba8\u8bba\u548c\u6295\u7968\u79d1\u5b66\u5bb6)\uff0c\u4e00\u4e2a\u5efa\u7acb\u5728llm\u4e0a\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5b83\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0:\u4e00\u4e2a\u52a8\u6001\u77e5\u8bc6\u4ea4\u6362\u673a\u5236\uff0c\u652f\u6301\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u8fed\u4ee3\u53cd\u9988\uff0c\u4ee5\u53ca\u4e00\u4e2a\u53cc\u91cd\u591a\u6837\u6027\u5ba1\u67e5\u8303\u4f8b\uff0c\u6a21\u62df\u5f02\u6784\u4e13\u5bb6\u8bc4\u4f30\u3002", "result": "IDVSCI\u59cb\u7ec8\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff0c\u4f18\u4e8eAI Scientist\u548cVIRSCI\u7b49\u73b0\u6709\u7cfb\u7edf\u3002", "conclusion": "IDVSCI\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u6700\u597d\u7684\u6027\u80fd\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u7684\u7cfb\u7edf\uff0c\u5982AI Scientist\u548cVIRSCI\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u5728\u57fa\u4e8ellm\u7684\u81ea\u4e3b\u7814\u7a76\u4e2d\u5efa\u6a21\u4ea4\u4e92\u548c\u540c\u884c\u8bc4\u5ba1\u52a8\u6001\u7684\u4ef7\u503c\u3002"}}
{"id": "2506.17576", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17576", "abs": "https://arxiv.org/abs/2506.17576", "authors": ["Furong Peng", "Jinzhen Gao", "Xuan Lu", "Kang Liu", "Yifan Huo", "Sheng Wang"], "title": "Towards Deeper GCNs: Alleviating Over-smoothing via Iterative Training and Fine-tuning", "comment": "16 pages,18 figures", "summary": "Graph Convolutional Networks (GCNs) suffer from severe performance\ndegradation in deep architectures due to over-smoothing. While existing studies\nprimarily attribute the over-smoothing to repeated applications of graph\nLaplacian operators, our empirical analysis reveals a critical yet overlooked\nfactor: trainable linear transformations in GCNs significantly exacerbate\nfeature collapse, even at moderate depths (e.g., 8 layers). In contrast,\nSimplified Graph Convolution (SGC), which removes these transformations,\nmaintains stable feature diversity up to 32 layers, highlighting linear\ntransformations' dual role in facilitating expressive power and inducing\nover-smoothing. However, completely removing linear transformations weakens the\nmodel's expressive capacity.\n  To address this trade-off, we propose Layer-wise Gradual Training (LGT), a\nnovel training strategy that progressively builds deep GCNs while preserving\ntheir expressiveness. LGT integrates three complementary components: (1)\nlayer-wise training to stabilize optimization from shallow to deep layers, (2)\nlow-rank adaptation to fine-tune shallow layers and accelerate training, and\n(3) identity initialization to ensure smooth integration of new layers and\naccelerate convergence. Extensive experiments on benchmark datasets demonstrate\nthat LGT achieves state-of-the-art performance on vanilla GCN, significantly\nimproving accuracy even in 32-layer settings. Moreover, as a training method,\nLGT can be seamlessly combined with existing methods such as PairNorm and\nContraNorm, further enhancing their performance in deeper networks. LGT offers\na general, architecture-agnostic training framework for scalable deep GCNs. The\ncode is available at [https://github.com/jfklasdfj/LGT_GCN].", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u7b56\u7565LGT\uff0c\u4ee5\u89e3\u51b3GCN\u4e2d\u7684\u8fc7\u5ea6\u5e73\u6ed1\u95ee\u9898\uff0c\u5e76\u5728\u6df1\u5ea6\u7f51\u7edc\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u7531\u4e8e\u8fc7\u5ea6\u5e73\u6ed1\uff0c\u5728\u6df1\u5ea6\u67b6\u6784\u4e2d\u9762\u4e34\u4e25\u91cd\u7684\u6027\u80fd\u4e0b\u964d\u3002\u867d\u7136\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u5c06\u8fc7\u5ea6\u5e73\u6ed1\u5f52\u56e0\u4e8e\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u7684\u91cd\u590d\u5e94\u7528\uff0c\u4f46\u6211\u4eec\u7684\u7ecf\u9a8c\u5206\u6790\u63ed\u793a\u4e86\u4e00\u4e2a\u5173\u952e\u4f46\u88ab\u5ffd\u89c6\u7684\u56e0\u7d20\uff1aGCN\u4e2d\u53ef\u8bad\u7ec3\u7684\u7ebf\u6027\u53d8\u6362\u4f1a\u663e\u7740\u52a0\u5267\u7279\u5f81\u5d29\u6e83\uff0c\u5373\u4f7f\u5728\u4e2d\u7b49\u6df1\u5ea6\uff08\u4f8b\u5982\uff0c8\u5c42\uff09\u4e5f\u662f\u5982\u6b64\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u7b80\u5316\u56fe\u5377\u79ef\uff08SGC\uff09\u6d88\u9664\u4e86\u8fd9\u4e9b\u53d8\u6362\uff0c\u4fdd\u6301\u4e86\u9ad8\u8fbe32\u5c42\u7684\u7a33\u5b9a\u7279\u5f81\u591a\u6837\u6027\uff0c\u7a81\u51fa\u4e86\u7ebf\u6027\u53d8\u6362\u5728\u4fc3\u8fdb\u8868\u8fbe\u80fd\u529b\u548c\u8bf1\u5bfc\u8fc7\u5ea6\u5e73\u6ed1\u65b9\u9762\u7684\u53cc\u91cd\u4f5c\u7528\u3002\u7136\u800c\uff0c\u5b8c\u5168\u6d88\u9664\u7ebf\u6027\u53d8\u6362\u4f1a\u524a\u5f31\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u5206\u5c42\u6e10\u8fdb\u8bad\u7ec3\uff08LGT\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u5176\u8868\u8fbe\u80fd\u529b\u7684\u540c\u65f6\u9010\u6b65\u6784\u5efa\u6df1\u5ea6GCN\u3002LGT\u96c6\u6210\u4e86\u4e09\u4e2a\u4e92\u8865\u7684\u7ec4\u4ef6\uff1a\uff081\uff09\u5206\u5c42\u8bad\u7ec3\uff0c\u4ee5\u7a33\u5b9a\u4ece\u6d45\u5c42\u5230\u6df1\u5c42\u7684\u4f18\u5316\uff0c\uff082\uff09\u4f4e\u79e9\u9002\u5e94\uff0c\u4ee5\u5fae\u8c03\u6d45\u5c42\u5e76\u52a0\u901f\u8bad\u7ec3\uff0c\u4ee5\u53ca\uff083\uff09\u8eab\u4efd\u521d\u59cb\u5316\uff0c\u4ee5\u786e\u4fdd\u65b0\u5c42\u7684\u5e73\u6ed1\u96c6\u6210\u5e76\u52a0\u901f\u6536\u655b\u3002", "result": "LGT\u5728vanilla GCN\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5373\u4f7f\u572832\u5c42\u8bbe\u7f6e\u4e0b\u4e5f\u80fd\u663e\u7740\u63d0\u9ad8\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u4f5c\u4e3a\u4e00\u79cd\u8bad\u7ec3\u65b9\u6cd5\uff0cLGT\u53ef\u4ee5\u4e0e\u73b0\u6709\u7684\u65b9\u6cd5\uff08\u5982PairNorm\u548cContraNorm\uff09\u65e0\u7f1d\u7ed3\u5408\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5b83\u4eec\u5728\u66f4\u6df1\u5c42\u7f51\u7edc\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "LGT\u5728vanilla GCN\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5373\u4f7f\u572832\u5c42\u8bbe\u7f6e\u4e0b\u4e5f\u80fd\u663e\u7740\u63d0\u9ad8\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u4f5c\u4e3a\u4e00\u79cd\u8bad\u7ec3\u65b9\u6cd5\uff0cLGT\u53ef\u4ee5\u4e0e\u73b0\u6709\u7684\u65b9\u6cd5\uff08\u5982PairNorm\u548cContraNorm\uff09\u65e0\u7f1d\u7ed3\u5408\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5b83\u4eec\u5728\u66f4\u6df1\u5c42\u7f51\u7edc\u4e2d\u7684\u6027\u80fd\u3002LGT\u4e3a\u53ef\u6269\u5c55\u7684\u6df1\u5ea6GCN\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u3001\u4e0e\u67b6\u6784\u65e0\u5173\u7684\u8bad\u7ec3\u6846\u67b6\u3002"}}
{"id": "2506.17705", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17705", "abs": "https://arxiv.org/abs/2506.17705", "authors": ["Bo Pan", "Yang Chen", "Yingwei Pan", "Ting Yao", "Wei Chen", "Tao Mei"], "title": "DreamJourney: Perpetual View Generation with Video Diffusion Models", "comment": null, "summary": "Perpetual view generation aims to synthesize a long-term video corresponding\nto an arbitrary camera trajectory solely from a single input image. Recent\nmethods commonly utilize a pre-trained text-to-image diffusion model to\nsynthesize new content of previously unseen regions along camera movement.\nHowever, the underlying 2D diffusion model lacks 3D awareness and results in\ndistorted artifacts. Moreover, they are limited to generating views of static\n3D scenes, neglecting to capture object movements within the dynamic 4D world.\nTo alleviate these issues, we present DreamJourney, a two-stage framework that\nleverages the world simulation capacity of video diffusion models to trigger a\nnew perpetual scene view generation task with both camera movements and object\ndynamics. Specifically, in stage I, DreamJourney first lifts the input image to\n3D point cloud and renders a sequence of partial images from a specific camera\ntrajectory. A video diffusion model is then utilized as generative prior to\ncomplete the missing regions and enhance visual coherence across the sequence,\nproducing a cross-view consistent video adheres to the 3D scene and camera\ntrajectory. Meanwhile, we introduce two simple yet effective strategies (early\nstopping and view padding) to further stabilize the generation process and\nimprove visual quality. Next, in stage II, DreamJourney leverages a multimodal\nlarge language model to produce a text prompt describing object movements in\ncurrent view, and uses video diffusion model to animate current view with\nobject movements. Stage I and II are repeated recurrently, enabling perpetual\ndynamic scene view generation. Extensive experiments demonstrate the\nsuperiority of our DreamJourney over state-of-the-art methods both\nquantitatively and qualitatively. Our project page:\nhttps://dream-journey.vercel.app.", "AI": {"tldr": "DreamJourney\u901a\u8fc7\u5229\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6c38\u4e45\u89c6\u56fe\u751f\u6210\u4e2d\u7f3a\u4e4f 3D \u611f\u77e5\u548c\u52a8\u6001\u6355\u6349\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u903c\u771f\u548c\u8fde\u8d2f\u7684\u52a8\u6001\u573a\u666f\u89c6\u56fe\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u6765\u5408\u6210\u6cbf\u7740\u76f8\u673a\u79fb\u52a8\u7684\u5148\u524d\u672a\u89c1\u533a\u57df\u7684\u65b0\u5185\u5bb9\u3002\u7136\u800c\uff0c\u5e95\u5c42 2D \u6269\u6563\u6a21\u578b\u7f3a\u4e4f 3D \u611f\u77e5\uff0c\u5e76\u5bfc\u81f4\u626d\u66f2\u7684\u4f2a\u5f71\u3002\u6b64\u5916\uff0c\u5b83\u4eec\u4ec5\u9650\u4e8e\u751f\u6210\u9759\u6001 3D \u573a\u666f\u7684\u89c6\u56fe\uff0c\u800c\u5ffd\u7565\u4e86\u6355\u83b7\u52a8\u6001 4D \u4e16\u754c\u4e2d\u7684\u5bf9\u8c61\u79fb\u52a8\u3002", "method": "DreamJourney\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5b83\u5229\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u4e16\u754c\u6a21\u62df\u80fd\u529b\u6765\u89e6\u53d1\u4e00\u4e2a\u65b0\u7684\u6c38\u4e45\u573a\u666f\u89c6\u56fe\u751f\u6210\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u540c\u65f6\u5177\u6709\u76f8\u673a\u79fb\u52a8\u548c\u5bf9\u8c61\u52a8\u6001\u3002", "result": "DreamJourney\uff0c\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5229\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u4e16\u754c\u6a21\u62df\u80fd\u529b\u6765\u89e6\u53d1\u4e00\u4e2a\u65b0\u7684\u6c38\u4e45\u573a\u666f\u89c6\u56fe\u751f\u6210\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u540c\u65f6\u5177\u6709\u76f8\u673a\u79fb\u52a8\u548c\u5bf9\u8c61\u52a8\u6001\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5728\u7b2c\u4e00\u9636\u6bb5\uff0cDreamJourney \u9996\u5148\u5c06\u8f93\u5165\u56fe\u50cf\u63d0\u5347\u5230 3D \u70b9\u4e91\uff0c\u5e76\u4ece\u7279\u5b9a\u7684\u76f8\u673a\u8f68\u8ff9\u6e32\u67d3\u4e00\u7cfb\u5217\u90e8\u5206\u56fe\u50cf\u3002\u7136\u540e\uff0c\u5229\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u751f\u6210\u5148\u9a8c\u6765\u5b8c\u6210\u7f3a\u5931\u7684\u533a\u57df\u5e76\u589e\u5f3a\u5e8f\u5217\u4e2d\u7684\u89c6\u89c9\u8fde\u8d2f\u6027\uff0c\u4ece\u800c\u751f\u6210\u7b26\u5408 3D \u573a\u666f\u548c\u76f8\u673a\u8f68\u8ff9\u7684\u8de8\u89c6\u56fe\u4e00\u81f4\u7684\u89c6\u9891\u3002\u540c\u65f6\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u7b56\u7565\uff08\u63d0\u524d\u505c\u6b62\u548c\u89c6\u56fe\u586b\u5145\uff09\u6765\u8fdb\u4e00\u6b65\u7a33\u5b9a\u751f\u6210\u8fc7\u7a0b\u5e76\u63d0\u9ad8\u89c6\u89c9\u8d28\u91cf\u3002\u63a5\u4e0b\u6765\uff0c\u5728\u7b2c\u4e8c\u9636\u6bb5\uff0cDreamJourney \u5229\u7528\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u751f\u6210\u63cf\u8ff0\u5f53\u524d\u89c6\u56fe\u4e2d\u5bf9\u8c61\u79fb\u52a8\u7684\u6587\u672c\u63d0\u793a\uff0c\u5e76\u4f7f\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u6765\u4e3a\u5f53\u524d\u89c6\u56fe\u6dfb\u52a0\u5bf9\u8c61\u79fb\u52a8\u7684\u52a8\u753b\u3002\u7b2c\u4e00\u9636\u6bb5\u548c\u7b2c\u4e8c\u9636\u6bb5\u4f1a\u5468\u671f\u6027\u5730\u91cd\u590d\uff0c\u4ece\u800c\u5b9e\u73b0\u6c38\u4e45\u52a8\u6001\u573a\u666f\u89c6\u56fe\u751f\u6210\u3002", "conclusion": "DreamJourney\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u65b9\u9762\u90fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c \u80fd\u591f\u5b9e\u73b0\u6c38\u4e45\u52a8\u6001\u573a\u666f\u89c6\u56fe\u751f\u6210\u3002"}}
{"id": "2506.17881", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17881", "abs": "https://arxiv.org/abs/2506.17881", "authors": ["Hua Tang", "Lingyong Yan", "Yukun Zhao", "Shuaiqiang Wang", "Jizhou Huang", "Dawei Yin"], "title": "Multi-turn Jailbreaking via Global Refinement and Active Fabrication", "comment": null, "summary": "Large Language Models (LLMs) have achieved exceptional performance across a\nwide range of tasks. However, they still pose significant safety risks due to\nthe potential misuse for malicious purposes. Jailbreaks, which aim to elicit\nmodels to generate harmful content, play a critical role in identifying the\nunderlying security threats. Recent jailbreaking primarily focuses on\nsingle-turn scenarios, while the more complicated multi-turn scenarios remain\nunderexplored. Moreover, existing multi-turn jailbreaking techniques struggle\nto adapt to the evolving dynamics of dialogue as the interaction progresses. To\naddress this limitation, we propose a novel multi-turn jailbreaking method that\nrefines the jailbreaking path globally at each interaction. We also actively\nfabricate model responses to suppress safety-related warnings, thereby\nincreasing the likelihood of eliciting harmful outputs in subsequent questions.\nExperimental results demonstrate the superior performance of our method\ncompared with existing single-turn and multi-turn jailbreaking techniques\nacross six state-of-the-art LLMs. Our code is publicly available at\nhttps://github.com/Ytang520/Multi-Turn_jailbreaking_Global-Refinment_and_Active-Fabrication.", "AI": {"tldr": "This paper presents a new multi-turn jailbreaking method for LLMs that improves upon existing techniques by globally refining the jailbreaking path and actively fabricating model responses.", "motivation": "Existing multi-turn jailbreaking techniques struggle to adapt to the evolving dynamics of dialogue as the interaction progresses.", "method": "The proposed method refines the jailbreaking path globally at each interaction and actively fabricates model responses to suppress safety-related warnings.", "result": "The proposed method achieves superior performance compared with existing single-turn and multi-turn jailbreaking techniques across six state-of-the-art LLMs.", "conclusion": "This paper introduces a novel multi-turn jailbreaking method that refines the jailbreaking path globally at each interaction and actively fabricates model responses to suppress safety-related warnings."}}
{"id": "2506.18424", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.18424", "abs": "https://arxiv.org/abs/2506.18424", "authors": ["Chengjie Liu", "Weiyu Chen", "Huiyao Xu", "Yuan Du", "Jun Yang", "Li Du"], "title": "A Large Language Model-based Multi-Agent Framework for Analog Circuits' Sizing Relationships Extraction", "comment": "Accepted by ISEDA 2025", "summary": "In the design process of the analog circuit pre-layout phase, device sizing\nis an important step in determining whether an analog circuit can meet the\nrequired performance metrics. Many existing techniques extract the circuit\nsizing task as a mathematical optimization problem to solve and continuously\nimprove the optimization efficiency from a mathematical perspective. But they\nignore the automatic introduction of prior knowledge, fail to achieve effective\npruning of the search space, which thereby leads to a considerable compression\nmargin remaining in the search space. To alleviate this problem, we propose a\nlarge language model (LLM)-based multi-agent framework for analog circuits'\nsizing relationships extraction from academic papers. The search space in the\nsizing process can be effectively pruned based on the sizing relationship\nextracted by this framework. Eventually, we conducted tests on 3 types of\ncircuits, and the optimization efficiency was improved by $2.32 \\sim 26.6\n\\times$. This work demonstrates that the LLM can effectively prune the search\nspace for analog circuit sizing, providing a new solution for the combination\nof LLMs and conventional analog circuit design automation methods.", "AI": {"tldr": "using large language model to prune the search space for analog circuit sizing", "motivation": "Many existing techniques extract the circuit sizing task as a mathematical optimization problem to solve and continuously improve the optimization efficiency from a mathematical perspective. But they ignore the automatic introduction of prior knowledge, fail to achieve effective pruning of the search space, which thereby leads to a considerable compression margin remaining in the search space.", "method": "a large language model (LLM)-based multi-agent framework for analog circuits' sizing relationships extraction from academic papers", "result": "the optimization efficiency was improved by $2.32 \\sim 26.6 \\times$", "conclusion": "This work demonstrates that the LLM can effectively prune the search space for analog circuit sizing, providing a new solution for the combination of LLMs and conventional analog circuit design automation methods."}}
{"id": "2506.17582", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2506.17582", "abs": "https://arxiv.org/abs/2506.17582", "authors": ["Jing Wang", "Biao Chen", "Hairun Xie", "Rui Wang", "Yifan Xia", "Jifa Zhang", "Hui Xu"], "title": "LFR-PINO: A Layered Fourier Reduced Physics-Informed Neural Operator for Parametric PDEs", "comment": "28 pages, 17 figures", "summary": "Physics-informed neural operators have emerged as a powerful paradigm for\nsolving parametric partial differential equations (PDEs), particularly in the\naerospace field, enabling the learning of solution operators that generalize\nacross parameter spaces. However, existing methods either suffer from limited\nexpressiveness due to fixed basis/coefficient designs, or face computational\nchallenges due to the high dimensionality of the parameter-to-weight mapping\nspace. We present LFR-PINO, a novel physics-informed neural operator that\nintroduces two key innovations: (1) a layered hypernetwork architecture that\nenables specialized parameter generation for each network layer, and (2) a\nfrequency-domain reduction strategy that significantly reduces parameter count\nwhile preserving essential spectral features. This design enables efficient\nlearning of a universal PDE solver through pre-training, capable of directly\nhandling new equations while allowing optional fine-tuning for enhanced\nprecision. The effectiveness of this approach is demonstrated through\ncomprehensive experiments on four representative PDE problems, where LFR-PINO\nachieves 22.8%-68.7% error reduction compared to state-of-the-art baselines.\nNotably, frequency-domain reduction strategy reduces memory usage by\n28.6%-69.3% compared to Hyper-PINNs while maintaining solution accuracy,\nstriking an optimal balance between computational efficiency and solution\nfidelity.", "AI": {"tldr": "LFR-PINO \u662f\u4e00\u79cd\u65b0\u7684\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7b97\u5b50\uff0c\u5b83\u901a\u8fc7\u5206\u5c42\u8d85\u7f51\u7edc\u67b6\u6784\u548c\u9891\u57df\u7ea6\u7b80\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u5b66\u4e60\u901a\u7528 PDE \u6c42\u89e3\u5668\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u795e\u7ecf\u7b97\u5b50\u65b9\u6cd5\u8981\u4e48\u7531\u4e8e\u56fa\u5b9a\u57fa/\u7cfb\u6570\u8bbe\u8ba1\u800c\u53d7\u5230\u6709\u9650\u7684\u8868\u8fbe\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u8981\u4e48\u7531\u4e8e\u53c2\u6570\u5230\u6743\u91cd\u6620\u5c04\u7a7a\u95f4\u7684\u9ad8\u7ef4\u5ea6\u800c\u9762\u4e34\u8ba1\u7b97\u6311\u6218\u3002", "method": " layered hypernetwork architecture and a frequency-domain reduction strategy", "result": "LFR-PINO\u5b9e\u73b0\u4e8622.8%-68.7%\u7684\u8bef\u5dee\u964d\u4f4e\uff0c\u9891\u7387\u57df\u7ea6\u7b80\u7b56\u7565\u964d\u4f4e\u4e8628.6%-69.3%\u7684\u5185\u5b58\u4f7f\u7528\u91cf\u3002", "conclusion": "LFR-PINO\u5728\u56db\u4e2a\u4ee3\u8868\u6027\u7684PDE\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e8622.8%-68.7%\u7684\u8bef\u5dee\u964d\u4f4e\uff0c\u5e76\u4e14\u9891\u7387\u57df\u7ea6\u7b80\u7b56\u7565\u5728\u4fdd\u6301\u89e3\u7684\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u6bd4Hyper-PINNs\u964d\u4f4e\u4e8628.6%-69.3%\u7684\u5185\u5b58\u4f7f\u7528\u91cf\u3002"}}
{"id": "2506.17707", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.17707", "abs": "https://arxiv.org/abs/2506.17707", "authors": ["Jihyun Kim", "Junho Park", "Kyeongbo Kong", "Suk-Ju Kang"], "title": "Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models", "comment": "Accepted by IEEE Transactions on Multimedia", "summary": "We present Programmable-Room, a framework which interactively generates and\nedits a 3D room mesh, given natural language instructions. For precise control\nof a room's each attribute, we decompose the challenging task into simpler\nsteps such as creating plausible 3D coordinates for room meshes, generating\npanorama images for the texture, constructing 3D meshes by integrating the\ncoordinates and panorama texture images, and arranging furniture. To support\nthe various decomposed tasks with a unified framework, we incorporate visual\nprogramming (VP). VP is a method that utilizes a large language model (LLM) to\nwrite a Python-like program which is an ordered list of necessary modules for\nthe various tasks given in natural language. We develop most of the modules.\nEspecially, for the texture generating module, we utilize a pretrained\nlarge-scale diffusion model to generate panorama images conditioned on text and\nvisual prompts (i.e., layout, depth, and semantic map) simultaneously.\nSpecifically, we enhance the panorama image generation quality by optimizing\nthe training objective with a 1D representation of a panorama scene obtained\nfrom bidirectional LSTM. We demonstrate Programmable-Room's flexibility in\ngenerating and editing 3D room meshes, and prove our framework's superiority to\nan existing model quantitatively and qualitatively. Project page is available\nin https://jihyun0510.github.io/Programmable_Room_Page/.", "AI": {"tldr": "Programmable-Room is a framework that uses language instructions to generate and edit 3D rooms by breaking down the task into simpler steps and using visual programming.", "motivation": "To interactively generate and edit a 3D room mesh, given natural language instructions, with precise control over each attribute.", "method": "The framework decomposes the task into simpler steps and uses visual programming (VP) with a large language model (LLM) to write a Python-like program. A pretrained large-scale diffusion model is used for texture generation, enhanced by optimizing the training objective with a 1D representation of a panorama scene obtained from bidirectional LSTM.", "result": "The framework can generate and edit 3D room meshes and outperforms existing models quantitatively and qualitatively.", "conclusion": "The Programmable-Room framework demonstrates flexibility in generating and editing 3D room meshes and outperforms existing models."}}
{"id": "2506.17949", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17949", "abs": "https://arxiv.org/abs/2506.17949", "authors": ["Hong Su"], "title": "Scatter-Based Innovation Propagation in Large Language Models for Multi-Stage Process Adaptation", "comment": null, "summary": "Large Language Models (LLMs) exhibit strong capabilities in reproducing and\nextending patterns observed during pretraining but often struggle to generalize\nnovel ideas beyond their original context. This paper addresses the challenge\nof applying such localized innovations - introduced at a specific stage or\ncomponent - to other parts of a multi-stage process. We propose a scatter-based\ninnovation expansion model (innovation scatter model) that guides the LLM\nthrough a four-step process: (1) identifying the core innovation by comparing\nthe user's input with its surrounding context, (2) generalizing the innovation\nby removing references to specific stages or components, (3) determining\nwhether the generalized innovation applies to a broader scope beyond the\noriginal stage, and (4) systematically applying it to other structurally\nsimilar stages using the LLM. This model leverages structural redundancy across\nstages to improve the applicability of novel ideas. Verification results\ndemonstrate that the innovation scatter model enables LLMs to extend\ninnovations across structurally similar stages, thereby enhancing\ngeneralization and reuse.", "AI": {"tldr": "This paper introduces an innovation scatter model that helps LLMs generalize and reuse innovations across different stages of a multi-stage process by identifying, generalizing, and applying innovations to structurally similar stages.", "motivation": "LLMs often struggle to generalize novel ideas beyond their original context. This paper addresses the challenge of applying such localized innovations - introduced at a specific stage or component - to other parts of a multi-stage process.", "method": "propose a scatter-based innovation expansion model (innovation scatter model) that guides the LLM through a four-step process: (1) identifying the core innovation by comparing the user's input with its surrounding context, (2) generalizing the innovation by removing references to specific stages or components, (3) determining whether the generalized innovation applies to a broader scope beyond the original stage, and (4) systematically applying it to other structurally similar stages using the LLM", "result": "Verification results demonstrate that the innovation scatter model enables LLMs to extend innovations across structurally similar stages, thereby enhancing generalization and reuse.", "conclusion": "the innovation scatter model enables LLMs to extend innovations across structurally similar stages, thereby enhancing generalization and reuse"}}
{"id": "2506.18428", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18428", "abs": "https://arxiv.org/abs/2506.18428", "authors": ["Feng He", "Zhenyang Liu", "Marco Valentino", "Zhixue Zhao"], "title": "How Robust is Model Editing after Fine-Tuning? An Empirical Study on Text-to-Image Diffusion Models", "comment": null, "summary": "Model editing offers a low-cost technique to inject or correct a particular\nbehavior in a pre-trained model without extensive retraining, supporting\napplications such as factual correction and bias mitigation. Despite this\ncommon practice, it remains unknown whether edits persist after fine-tuning or\nwhether they are inadvertently reversed. This question has fundamental\npractical implications. For example, if fine-tuning removes prior edits, it\ncould serve as a defence mechanism against hidden malicious edits. Vice versa,\nthe unintended removal of edits related to bias mitigation could pose serious\nsafety concerns. We systematically investigate the interaction between model\nediting and fine-tuning in the context of T2I diffusion models, which are known\nto exhibit biases and generate inappropriate content. Our study spans two T2I\nmodel families (Stable Diffusion and FLUX), two sota editing techniques, and\nthree fine-tuning methods (DreamBooth, LoRA, and DoRA). Through an extensive\nempirical analysis across diverse editing tasks and evaluation metrics, our\nfindings reveal a trend: edits generally fail to persist through fine-tuning,\neven when fine-tuning is tangential or unrelated to the edits. Notably, we\nobserve that DoRA exhibits the strongest edit reversal effect. At the same\ntime, among editing methods, UCE demonstrates greater robustness, retaining\nsignificantly higher efficacy post-fine-tuning compared to ReFACT. These\nfindings highlight a crucial limitation in current editing methodologies,\nemphasizing the need for more robust techniques to ensure reliable long-term\ncontrol and alignment of deployed AI systems. These findings have dual\nimplications for AI safety: they suggest that fine-tuning could serve as a\nremediation mechanism for malicious edits while simultaneously highlighting the\nneed for re-editing after fine-tuning to maintain beneficial safety and\nalignment properties.", "AI": {"tldr": "Model edits in T2I diffusion models often don't survive fine-tuning, especially with DoRA. UCE is more robust. Fine-tuning can remove malicious edits but may also require re-editing for safety.", "motivation": "It remains unknown whether edits persist after fine-tuning or whether they are inadvertently reversed. This question has fundamental practical implications. For example, if fine-tuning removes prior edits, it could serve as a defence mechanism against hidden malicious edits. Vice versa, the unintended removal of edits related to bias mitigation could pose serious safety concerns.", "method": "Two T2I model families (Stable Diffusion and FLUX), two sota editing techniques, and three fine-tuning methods (DreamBooth, LoRA, and DoRA). Through an extensive empirical analysis across diverse editing tasks and evaluation metrics", "result": "Edits generally fail to persist through fine-tuning, even when fine-tuning is tangential or unrelated to the edits. Notably, DoRA exhibits the strongest edit reversal effect. At the same time, among editing methods, UCE demonstrates greater robustness, retaining significantly higher efficacy post-fine-tuning compared to ReFACT.", "conclusion": "Edits generally fail to persist through fine-tuning, even when fine-tuning is tangential or unrelated to the edits. DoRA exhibits the strongest edit reversal effect. UCE demonstrates greater robustness, retaining significantly higher efficacy post-fine-tuning compared to ReFACT. Fine-tuning could serve as a remediation mechanism for malicious edits while simultaneously highlighting the need for re-editing after fine-tuning to maintain beneficial safety and alignment properties."}}
{"id": "2506.17607", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.17607", "abs": "https://arxiv.org/abs/2506.17607", "authors": ["Chicheng Zhang", "Yihan Zhou"], "title": "Towards Fundamental Limits for Active Multi-distribution Learning", "comment": "to appear in Conference on Learning Theory (COLT) 2025", "summary": "Multi-distribution learning extends agnostic Probably Approximately Correct\n(PAC) learning to the setting in which a family of $k$ distributions,\n$\\{D_i\\}_{i\\in[k]}$, is considered and a classifier's performance is measured\nby its error under the worst distribution. This problem has attracted a lot of\nrecent interests due to its applications in collaborative learning, fairness,\nand robustness. Despite a rather complete picture of sample complexity of\npassive multi-distribution learning, research on active multi-distribution\nlearning remains scarce, with algorithms whose optimality remaining unknown.\n  In this paper, we develop new algorithms for active multi-distribution\nlearning and establish improved label complexity upper and lower bounds, in\ndistribution-dependent and distribution-free settings. Specifically, in the\nnear-realizable setting we prove an upper bound of\n$\\widetilde{O}\\Bigl(\\theta_{\\max}(d+k)\\ln\\frac{1}{\\varepsilon}\\Bigr)$ and\n$\\widetilde{O}\\Bigl(\\theta_{\\max}(d+k)\\Bigl(\\ln\\frac{1}{\\varepsilon}+\\frac{\\nu^2}{\\varepsilon^2}\\Bigr)+\\frac{k\\nu}{\\varepsilon^2}\\Bigr)$\nin the realizable and agnostic settings respectively, where $\\theta_{\\max}$ is\nthe maximum disagreement coefficient among the $k$ distributions, $d$ is the VC\ndimension of the hypothesis class, $\\nu$ is the multi-distribution error of the\nbest hypothesis, and $\\varepsilon$ is the target excess error. Moreover, we\nshow that the bound in the realizable setting is information-theoretically\noptimal and that the $k\\nu/\\varepsilon^2$ term in the agnostic setting is\nfundamental for proper learners. We also establish instance-dependent sample\ncomplexity bound for passive multidistribution learning that smoothly\ninterpolates between realizable and agnostic\nregimes~\\citep{blum2017collaborative,zhang2024optimal}, which may be of\nindependent interest.", "AI": {"tldr": "This paper focuses on active multi-distribution learning, presenting new algorithms with improved label complexity bounds. It also provides instance-dependent sample complexity bounds for passive learning, bridging realizable and agnostic regimes.", "motivation": "Multi-distribution learning extends agnostic Probably Approximately Correct (PAC) learning to the setting in which a family of $k$ distributions is considered and a classifier's performance is measured by its error under the worst distribution. This problem has attracted a lot of recent interests due to its applications in collaborative learning, fairness, and robustness. Despite a rather complete picture of sample complexity of passive multi-distribution learning, research on active multi-distribution learning remains scarce, with algorithms whose optimality remaining unknown.", "method": "The paper develops new algorithms for active multi-distribution learning.", "result": "Specifically, in the near-realizable setting the paper proves an upper bound of $\\widetilde{O}\\Bigl(\\theta_{\\max}(d+k)\\ln\\frac{1}{\\varepsilon}\\Bigr)$ and $\\widetilde{O}\\Bigl(\\theta_{\\max}(d+k)\\Bigl(\\ln\\frac{1}{\\varepsilon}+\\frac{\\nu^2}{\\varepsilon^2}\\Bigr)+\\frac{k\\nu}{\\varepsilon^2}\\Bigr)$ in the realizable and agnostic settings respectively, where $\\theta_{\\max}$ is the maximum disagreement coefficient among the $k$ distributions, $d$ is the VC dimension of the hypothesis class, $\\nu$ is the multi-distribution error of the best hypothesis, and $\\varepsilon$ is the target excess error. Moreover, the paper shows that the bound in the realizable setting is information-theoretically optimal and that the $k\\nu/\\varepsilon^2$ term in the agnostic setting is fundamental for proper learners.", "conclusion": "This paper develops new algorithms for active multi-distribution learning and establishes improved label complexity upper and lower bounds, in distribution-dependent and distribution-free settings.  It also establishes instance-dependent sample complexity bound for passive multidistribution learning that smoothly interpolates between realizable and agnostic regimes."}}
{"id": "2506.17712", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17712", "abs": "https://arxiv.org/abs/2506.17712", "authors": ["Xinyu Xiong", "Wuteng Cao", "Zihuang Wu", "Lei Zhang", "Chong Gao", "Guanbin Li", "Qiyuan Qin"], "title": "PDC-Net: Pattern Divide-and-Conquer Network for Pelvic Radiation Injury Segmentation", "comment": "MICCAI 2025", "summary": "Accurate segmentation of Pelvic Radiation Injury (PRI) from Magnetic\nResonance Images (MRI) is crucial for more precise prognosis assessment and the\ndevelopment of personalized treatment plans. However, automated segmentation\nremains challenging due to factors such as complex organ morphologies and\nconfusing context. To address these challenges, we propose a novel Pattern\nDivide-and-Conquer Network (PDC-Net) for PRI segmentation. The core idea is to\nuse different network modules to \"divide\" various local and global patterns\nand, through flexible feature selection, to \"conquer\" the Regions of Interest\n(ROI) during the decoding phase. Specifically, considering that our ROI often\nmanifests as strip-like or circular-like structures in MR slices, we introduce\na Multi-Direction Aggregation (MDA) module. This module enhances the model's\nability to fit the shape of the organ by applying strip convolutions in four\ndistinct directions. Additionally, to mitigate the challenge of confusing\ncontext, we propose a Memory-Guided Context (MGC) module. This module\nexplicitly maintains a memory parameter to track cross-image patterns at the\ndataset level, thereby enhancing the distinction between global patterns\nassociated with the positive and negative classes. Finally, we design an\nAdaptive Fusion Decoder (AFD) that dynamically selects features from different\npatterns based on the Mixture-of-Experts (MoE) framework, ultimately generating\nthe final segmentation results. We evaluate our method on the first large-scale\npelvic radiation injury dataset, and the results demonstrate the superiority of\nour PDC-Net over existing approaches.", "AI": {"tldr": "PDC-Net for PRI segmentation", "motivation": "Accurate segmentation of Pelvic Radiation Injury (PRI) from Magnetic Resonance Images (MRI) is crucial for more precise prognosis assessment and the development of personalized treatment plans. However, automated segmentation remains challenging due to factors such as complex organ morphologies and confusing context.", "method": " propose a novel Pattern Divide-and-Conquer Network (PDC-Net) for PRI segmentation. The core idea is to use different network modules to \"divide\" various local and global patterns and, through flexible feature selection, to \"conquer\" the Regions of Interest (ROI) during the decoding phase. Specifically, considering that our ROI often manifest as strip-like or circular-like structures in MR slices, we introduce a Multi-Direction Aggregation (MDA) module. Additionally, to mitigate the challenge of confusing context, we propose a Memory-Guided Context (MGC) module. Finally, we design an Adaptive Fusion Decoder (AFD) that dynamically selects features from different patterns based on the Mixture-of-Experts (MoE) framework, ultimately generating the final segmentation results.", "result": "We evaluate our method on the first large-scale pelvic radiation injury dataset", "conclusion": "The results demonstrate the superiority of our PDC-Net over existing approaches."}}
{"id": "2506.17951", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.17951", "abs": "https://arxiv.org/abs/2506.17951", "authors": ["Quanwei Tang", "Sophia Yat Mei Lee", "Junshuang Wu", "Dong Zhang", "Shoushan Li", "Erik Cambria", "Guodong Zhou"], "title": "A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment", "comment": "acl 2025 findings", "summary": "Recent advancements in retrieval-augmented generation (RAG) have enhanced\nlarge language models in question answering by integrating external knowledge.\nHowever, challenges persist in achieving global understanding and aligning\nresponses with human ethical and quality preferences. To address these issues,\nwe propose GraphMPA, a comprehensive graph-based framework with mode-seeking\npreference alignment. Our approach constructs a hierarchical document graph\nusing a general similarity measurement, mimicking human cognitive processes for\ninformation understanding and synthesis. Additionally, we introduce\nmode-seeking preference optimization to better align model outputs with human\npreferences through probability-matching constraints. Extensive experiments on\nsix datasets demonstrate the effectiveness of our\n\\href{https://github.com/tangquanwei/GraphMPA}{GraphMPA}.", "AI": {"tldr": "GraphMPA, a graph-based framework with mode-seeking preference alignment, enhances RAG by improving global understanding and aligning responses with human preferences.", "motivation": "Challenges persist in achieving global understanding and aligning responses with human ethical and quality preferences in RAG.", "method": "We propose GraphMPA, a comprehensive graph-based framework with mode-seeking preference alignment. Our approach constructs a hierarchical document graph using a general similarity measurement, mimicking human cognitive processes for information understanding and synthesis. Additionally, we introduce mode-seeking preference optimization to better align model outputs with human preferences through probability-matching constraints.", "result": "The effectiveness of GraphMPA is demonstrated on six datasets.", "conclusion": "Extensive experiments on six datasets demonstrate the effectiveness of GraphMPA."}}
{"id": "2506.18511", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18511", "abs": "https://arxiv.org/abs/2506.18511", "authors": ["Yu Han", "Aaron Ceross", "Jeroen H. M. Bergmann"], "title": "Standard Applicability Judgment and Cross-jurisdictional Reasoning: A RAG-based Framework for Medical Device Compliance", "comment": null, "summary": "Identifying the appropriate regulatory standard applicability remains a\ncritical yet understudied challenge in medical device compliance, frequently\nnecessitating expert interpretation of fragmented and heterogeneous\ndocumentation across different jurisdictions. To address this challenge, we\nintroduce a modular AI system that leverages a retrieval-augmented generation\n(RAG) pipeline to automate standard applicability determination. Given a\nfree-text device description, our system retrieves candidate standards from a\ncurated corpus and uses large language models to infer jurisdiction-specific\napplicability, classified as Mandatory, Recommended, or Not Applicable, with\ntraceable justifications. We construct an international benchmark dataset of\nmedical device descriptions with expert-annotated standard mappings, and\nevaluate our system against retrieval-only, zero-shot, and rule-based\nbaselines. The proposed approach attains a classification accuracy of 73% and a\nTop-5 retrieval recall of 87%, demonstrating its effectiveness in identifying\nrelevant regulatory standards. We introduce the first end-to-end system for\nstandard applicability reasoning, enabling scalable and interpretable\nAI-supported regulatory science. Notably, our region-aware RAG agent performs\ncross-jurisdictional reasoning between Chinese and U.S. standards, supporting\nconflict resolution and applicability justification across regulatory\nframeworks.", "AI": {"tldr": "This paper introduces a RAG pipeline to automate standard applicability determination. The system retrieves candidate standards and uses LLMs to infer jurisdiction-specific applicability, achieving 73% accuracy and 87% Top-5 recall.", "motivation": "Identifying the appropriate regulatory standard applicability remains a critical yet understudied challenge in medical device compliance, frequently necessitating expert interpretation of fragmented and heterogeneous documentation across different jurisdictions.", "method": "a modular AI system that leverages a retrieval-augmented generation (RAG) pipeline to automate standard applicability determination. Given a free-text device description, our system retrieves candidate standards from a curated corpus and uses large language models to infer jurisdiction-specific applicability, classified as Mandatory, Recommended, or Not Applicable, with traceable justifications.", "result": "The proposed approach attains a classification accuracy of 73% and a Top-5 retrieval recall of 87%, demonstrating its effectiveness in identifying relevant regulatory standards.", "conclusion": "The proposed approach attains a classification accuracy of 73% and a Top-5 retrieval recall of 87%, demonstrating its effectiveness in identifying relevant regulatory standards. We introduce the first end-to-end system for standard applicability reasoning, enabling scalable and interpretable AI-supported regulatory science. Notably, our region-aware RAG agent performs cross-jurisdictional reasoning between Chinese and U.S. standards, supporting conflict resolution and applicability justification across regulatory frameworks."}}
