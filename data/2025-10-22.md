<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [DynaQuery: A Self-Adapting Framework for Querying Structured and Multimodal Data](https://arxiv.org/abs/2510.18029)
*Aymane Hassini*

Main category: cs.DB

TL;DR: DynaQuery is a framework for natural language querying over hybrid databases, using a Schema Introspection and Linking Engine (SILE) to improve query planning.


<details>
  <summary>Details</summary>
Motivation: Enable natural language querying over complex, hybrid databases by reasoning over structured schemas and unstructured content.

Method: Develop DynaQuery, a unified, self-adapting framework with SILE for schema linking as a query planning phase.

Result: DynaQuery reduces contextual failures like SCHEMA_HALLUCINATION compared to Retrieval-Augmented Generation (RAG).

Conclusion: DynaQuery provides a robust and adaptable architecture for natural language database interfaces.

Abstract: The rise of Large Language Models (LLMs) has accelerated the long-standing
goal of enabling natural language querying over complex, hybrid databases. Yet,
this ambition exposes a dual challenge: reasoning jointly over structured,
multi-relational schemas and the semantic content of linked unstructured
assets. To overcome this, we present DynaQuery - a unified, self-adapting
framework that serves as a practical blueprint for next-generation "Unbound
Databases." At the heart of DynaQuery lies the Schema Introspection and Linking
Engine (SILE), a novel systems primitive that elevates schema linking to a
first-class query planning phase. We conduct a rigorous, multi-benchmark
empirical evaluation of this structure-aware architecture against the prevalent
unstructured Retrieval-Augmented Generation (RAG) paradigm. Our results
demonstrate that the unstructured retrieval paradigm is architecturally
susceptible to catastrophic contextual failures, such as SCHEMA_HALLUCINATION,
leading to unreliable query generation. In contrast, our SILE-based design
establishes a substantially more robust foundation, nearly eliminating this
failure mode. Moreover, end-to-end validation on a complex, newly curated
benchmark uncovers a key generalization principle: the transition from pure
schema-awareness to holistic semantics-awareness. Taken together, our findings
provide a validated architectural basis for developing natural language
database interfaces that are robust, adaptable, and predictably consistent.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [2] [From AutoRecSys to AutoRecLab: A Call to Build, Evaluate, and Govern Autonomous Recommender-Systems Research Labs](https://arxiv.org/abs/2510.18104)
*Joeran Beel,Bela Gipp,Tobias Vente,Moritz Baumgart,Philipp Meister*

Main category: cs.IR

TL;DR: 这篇论文提出了一个从狭隘的 AutoRecSys 工具（专注于算法选择和超参数调整）到自主推荐系统研究实验室 (AutoRecLab) 的转变，该实验室集成了端到端自动化：问题构思、文献分析、实验设计和执行、结果解释、手稿起草和出处记录。


<details>
  <summary>Details</summary>
Motivation: 目前推荐系统研究主要集中在模型和评估的改进，而忽略了研究过程本身的自动化。

Method: 借鉴了自动科学（例如，多智能体 AI 科学家和 AI Co-Scientist 系统）的最新进展，为 RecSys 社区制定议程：(1) 构建开放的 AutoRecLab 原型，将 LLM 驱动的构思和报告与自动化实验相结合；(2) 建立基准和竞赛，评估智能体在以最少的人工输入生成可重复的 RecSys 发现方面的能力；(3) 为透明的 AI 生成的提交创建审查场所；(4) 通过详细的研究日志和元数据定义归属和可重复性的标准；(5) 促进关于自主研究中伦理、治理、隐私和公平的跨学科对话。

Result: 提高研究吞吐量，发现非显而易见的见解，并使 RecSys 能够为新兴的人工研究智能做出贡献。

Conclusion: 呼吁组织一次社区务虚会，以协调后续步骤并共同撰写负责任地整合自动化研究系统的指南。

Abstract: Recommender-systems research has accelerated model and evaluation advances,
yet largely neglects automating the research process itself. We argue for a
shift from narrow AutoRecSys tools -- focused on algorithm selection and
hyper-parameter tuning -- to an Autonomous Recommender-Systems Research Lab
(AutoRecLab) that integrates end-to-end automation: problem ideation,
literature analysis, experimental design and execution, result interpretation,
manuscript drafting, and provenance logging. Drawing on recent progress in
automated science (e.g., multi-agent AI Scientist and AI Co-Scientist systems),
we outline an agenda for the RecSys community: (1) build open AutoRecLab
prototypes that combine LLM-driven ideation and reporting with automated
experimentation; (2) establish benchmarks and competitions that evaluate agents
on producing reproducible RecSys findings with minimal human input; (3) create
review venues for transparently AI-generated submissions; (4) define standards
for attribution and reproducibility via detailed research logs and metadata;
and (5) foster interdisciplinary dialogue on ethics, governance, privacy, and
fairness in autonomous research. Advancing this agenda can increase research
throughput, surface non-obvious insights, and position RecSys to contribute to
emerging Artificial Research Intelligence. We conclude with a call to organise
a community retreat to coordinate next steps and co-author guidance for the
responsible integration of automated research systems.

</details>
